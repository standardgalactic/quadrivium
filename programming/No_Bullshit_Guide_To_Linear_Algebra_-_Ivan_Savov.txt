

No bullshit guide to linear algebra
Ivan Savov
April 30, 2016

No bullshit guide to linear algebra
by Ivan Savov
Copyright ¬© Ivan Savov, 2015, 2016.
All rights reserved.
Published by Minireference Co.
Montr√©al, Qu√©bec, Canada
minireference.com | @minireference | fb.me/noBSguide
For inquiries, contact the author at ivan.savov@gmail.com
Near-Ô¨Ånal release
v0.91
hg changeset: 314:578e9ac33e27
ISBN 978-0-9920010-1-8
10
9
8
7
6
5
4
3
2
1

Contents
Preface
vii
Introduction
1
1
Math fundamentals
9
1.1
Solving equations . . . . . . . . . . . . . . . . . . . . .
10
1.2
Numbers . . . . . . . . . . . . . . . . . . . . . . . . . .
12
1.3
Variables
. . . . . . . . . . . . . . . . . . . . . . . . .
16
1.4
Functions and their inverses . . . . . . . . . . . . . . .
18
1.5
Basic rules of algebra . . . . . . . . . . . . . . . . . . .
21
1.6
Solving quadratic equations . . . . . . . . . . . . . . .
25
1.7
The Cartesian plane . . . . . . . . . . . . . . . . . . .
29
1.8
Functions . . . . . . . . . . . . . . . . . . . . . . . . .
32
1.9
Function reference
. . . . . . . . . . . . . . . . . . . .
38
1.10 Polynomials . . . . . . . . . . . . . . . . . . . . . . . .
54
1.11 Trigonometry . . . . . . . . . . . . . . . . . . . . . . .
58
1.12 Trigonometric identities . . . . . . . . . . . . . . . . .
63
1.13 Geometry . . . . . . . . . . . . . . . . . . . . . . . . .
65
1.14 Circle
. . . . . . . . . . . . . . . . . . . . . . . . . . .
67
1.15 Solving systems of linear equations . . . . . . . . . . .
69
1.16 Set notation . . . . . . . . . . . . . . . . . . . . . . . .
73
1.17 Math problems . . . . . . . . . . . . . . . . . . . . . .
79
2
Vectors
89
2.1
Vectors
. . . . . . . . . . . . . . . . . . . . . . . . . .
90
2.2
Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
2.3
Vector products . . . . . . . . . . . . . . . . . . . . . .
99
2.4
Complex numbers
. . . . . . . . . . . . . . . . . . . .
102
2.5
Vectors problems . . . . . . . . . . . . . . . . . . . . .
107
3
Intro to linear algebra
111
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . .
111
3.2
Review of vector operations . . . . . . . . . . . . . . .
117
i

3.3
Matrix operations
. . . . . . . . . . . . . . . . . . . .
121
3.4
Linearity . . . . . . . . . . . . . . . . . . . . . . . . . .
126
3.5
Overview of linear algebra . . . . . . . . . . . . . . . .
131
3.6
Introductory problems . . . . . . . . . . . . . . . . . .
135
4
Computational linear algebra
137
4.1
Reduced row echelon form . . . . . . . . . . . . . . . .
138
4.2
Matrix equations . . . . . . . . . . . . . . . . . . . . .
150
4.3
Matrix multiplication . . . . . . . . . . . . . . . . . . .
154
4.4
Determinants . . . . . . . . . . . . . . . . . . . . . . .
158
4.5
Matrix inverse
. . . . . . . . . . . . . . . . . . . . . .
169
4.6
Computational problems . . . . . . . . . . . . . . . . .
176
5
Geometrical aspects of linear algebra
181
5.1
Lines and planes
. . . . . . . . . . . . . . . . . . . . .
181
5.2
Projections
. . . . . . . . . . . . . . . . . . . . . . . .
189
5.3
Coordinate projections . . . . . . . . . . . . . . . . . .
194
5.4
Vector spaces . . . . . . . . . . . . . . . . . . . . . . .
199
5.5
Vector space techniques
. . . . . . . . . . . . . . . . .
210
5.6
Geometrical problems
. . . . . . . . . . . . . . . . . .
220
6
Linear transformations
223
6.1
Linear transformations . . . . . . . . . . . . . . . . . .
223
6.2
Finding matrix representations . . . . . . . . . . . . .
234
6.3
Change of basis for matrices . . . . . . . . . . . . . . .
245
6.4
Invertible matrix theorem . . . . . . . . . . . . . . . .
249
6.5
Linear transformations problems
. . . . . . . . . . . .
256
7
Theoretical linear algebra
257
7.1
Eigenvalues and eigenvectors
. . . . . . . . . . . . . .
258
7.2
Special types of matrices . . . . . . . . . . . . . . . . .
271
7.3
Abstract vector spaces . . . . . . . . . . . . . . . . . .
277
7.4
Abstract inner product spaces . . . . . . . . . . . . . .
281
7.5
Gram-Schmidt orthogonalization . . . . . . . . . . . .
288
7.6
Matrix decompositions . . . . . . . . . . . . . . . . . .
292
7.7
Linear algebra with complex numbers
. . . . . . . . .
298
7.8
Theory problems . . . . . . . . . . . . . . . . . . . . .
313
8
Applications
317
8.1
Balancing chemical equations . . . . . . . . . . . . . .
318
8.2
Input-output models in economics . . . . . . . . . . .
320
8.3
Electric circuits . . . . . . . . . . . . . . . . . . . . . .
321
8.4
Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . .
327
8.5
Fibonacci sequence . . . . . . . . . . . . . . . . . . . .
330
8.6
Linear programming . . . . . . . . . . . . . . . . . . .
332

iii
8.7
Least squares approximate solutions
. . . . . . . . . .
333
8.8
Computer graphics . . . . . . . . . . . . . . . . . . . .
342
8.9
Cryptography . . . . . . . . . . . . . . . . . . . . . . .
354
8.10 Error correcting codes . . . . . . . . . . . . . . . . . .
366
8.11 Fourier analysis . . . . . . . . . . . . . . . . . . . . . .
375
8.12 Applications problems . . . . . . . . . . . . . . . . . .
389
9
Probability theory
391
9.1
Probability distributions . . . . . . . . . . . . . . . . .
391
9.2
Markov chains
. . . . . . . . . . . . . . . . . . . . . .
398
9.3
Google's PageRank algorithm . . . . . . . . . . . . . .
404
9.4
Probability problems . . . . . . . . . . . . . . . . . . .
410
10 Quantum mechanics
411
10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . .
412
10.2 Polarizing lenses experiment . . . . . . . . . . . . . . .
418
10.3 Dirac notation for vectors . . . . . . . . . . . . . . . .
425
10.4 Quantum information processing . . . . . . . . . . . .
431
10.5 Postulates of quantum mechanics . . . . . . . . . . . .
434
10.6 Polarizing lenses experiment revisited
. . . . . . . . .
448
10.7 Quantum physics is not that weird . . . . . . . . . . .
452
10.8 Quantum mechanics applications . . . . . . . . . . . .
457
10.9 Quantum mechanics problems . . . . . . . . . . . . . .
473
End matter
475
Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . .
475
Social stuÔ¨Ä
. . . . . . . . . . . . . . . . . . . . . . . . . . .
477
Acknowledgements . . . . . . . . . . . . . . . . . . . . . . .
477
General linear algebra links . . . . . . . . . . . . . . . . . .
477
A Answers and solutions
479
B Notation
499
Math notation
. . . . . . . . . . . . . . . . . . . . . . . . .
499
Set notation . . . . . . . . . . . . . . . . . . . . . . . . . . .
500
Vectors notation
. . . . . . . . . . . . . . . . . . . . . . . .
500
Complex numbers notation
. . . . . . . . . . . . . . . . . .
501
Vector space notation
. . . . . . . . . . . . . . . . . . . . .
501
Notation for matrices and matrix operations . . . . . . . . .
502
Notation for linear transformations . . . . . . . . . . . . . .
503
Matrix decompositions . . . . . . . . . . . . . . . . . . . . .
503
Abstract vector space notation
. . . . . . . . . . . . . . . .
504

Concept maps
Figure 1:
This concept map illustrates the prerequisite topics of high
school math covered in Chapter 1 and vectors covered in Chapter 2. Also
shown are the topics of computational and geometrical linear algebra cov-
ered in Chapters 4 and 5.
iv

Figure 2: Chapter 6 covers linear transformations and their properties.
Figure 3: Chapter 7 covers theoretical aspects of linear algebra.
v

Figure 4: Matrix operations and matrix computations play an important
role throughout this book. Matrices are used to implement linear transfor-
mations, systems of linear equations, and various geometrical computations.
Figure 5: The book concludes with three chapters on linear algebra ap-
plications. In Chapter 8 we'll discuss applications to science, economics,
business, computing, and signal processing. Chapter 9 on probability the-
ory and Chapter 10 on quantum mechanics serve as examples of advanced
subjects that you can access once you learn linear algebra.
vi

Preface
This book is about linear algebra and its applications. The material
is covered at the level of a Ô¨Årst-year university course with more ad-
vanced concepts also being presented. The book is written in a clean,
approachable style that gets to the point. Both practical and theo-
retical aspects of linear algebra are discussed, with extra emphasis on
explaining the connections between concepts and building on material
students are already familiar with.
Since it includes all necessary prerequisites, this book is suitable
for readers who don't feel "comfortable" with fundamental math con-
cepts, having never learned them well, or having forgotten them over
the years. The goal of this book is to give access to advanced
mathematical modelling tools to everyone interested in learning,
regardless of their academic background.
Why learn linear algebra?
Linear algebra is one of the most useful undergraduate math sub-
jects.
The practical skills like manipulating vectors and matrices
that students learn will come in handy for physics, computer science,
statistics, machine learning, and many other areas of science. Linear
algebra is essential for anyone pursuing studies in science.
In addition to being useful, learning linear algebra can also be a lot
of fun. Readers will experience knowledge buzz from understanding
the connections between concepts and seeing how they Ô¨Åt together.
Linear algebra is one of the most fundamental subjects in mathematics
and it's not uncommon to experience mind-expanding moments while
studying this subject.
The powerful concepts and tools of linear algebra form a bridge
toward more advanced areas of mathematics. For example, learning
about abstract vector spaces will help students recognize the common
"vector structure" in seemingly unrelated mathematical objects like
matrices, polynomials, and functions. Linear algebra techniques can
be applied not only to standard vectors, but to all mathematical
vii

viii
PREFACE
objects that are vector-like!
What's in this book?
Each section in this book is a self-contained tutorial that covers the
deÔ¨Ånitions, formulas, and explanations associated with a single topic.
Consult the concept maps on the preceding pages to see the topics
covered in the book and the connections between them.
The book begins with a review chapter on numbers, algebra, equa-
tions, functions, and trigonometry (Chapter 1) and a review chapter
on vectors (Chapter 2). Anyone who hasn't seen these concepts be-
fore, or who feels their math and vector skills are a little "rusty" should
read these chapters and work through the exercises and problems pro-
vided. Readers who feel conÔ¨Ådent in their high school math abilities
can jump straight to Chapter 3 where the linear algebra begins.
Chapters 4 through 7 cover the core topics of linear algebra: vec-
tors, bases, analytical geometry, matrices, linear transformations, ma-
trix representations, vector spaces, inner product spaces, eigenvectors,
and matrix decompositions.
Chapters 8, 9, and 10 discuss various applications of linear algebra.
Though not likely to appear on any linear algebra Ô¨Ånal exam, these
chapters serve to demonstrate the power of linear algebra techniques
and their relevance to many areas of science.
The mini-course on
quantum mechanics (Chapter 10) is unique to this book.
Is this book for you?
The quick pace and lively explanations in this book provide interesting
reading for students and non-students alike. Whether you're learning
linear algebra for a course, reviewing material as a prerequisite for
more advanced topics, or generally curious about the subject, this
guide will help you Ô¨Ånd your way in the land of linear algebra. The
short-tutorial format cuts to the chase: we're all busy adults with no
time to waste!
This book can be used as the main textbook for any university-
level linear algebra course. It contains everything students need to
know to prepare for a linear algebra Ô¨Ånal exam. Don't be fooled by
the book's small format: it's all in here. The text is compact because
it distills the essentials and removes the unnecessary cruft.
Publisher
The genesis of the no bullshit guide textbook series dates back to
my student days, when I was required to purchase expensive course
textbooks, which were long and tedious to read.
I said to myself

ix
that "something must be done," and started a textbook company to
produce textbooks that explain math and physics concepts clearly,
concisely, and aÔ¨Äordably.
The goal of Minireference Publishing is to Ô¨Åx the Ô¨Årst-year
science textbooks problem. Mainstream textbooks suck, so we're do-
ing something about it. We want to set the bar higher and redeÔ¨Åne
readers' expectations for what a textbook should be! Using print-on-
demand and digital distribution strategies allows us to provide readers
with high quality textbooks at reasonable prices.
About the author
I have been teaching math and physics for more than 15 years as a
private tutor. Through this experience, I learned to explain diÔ¨Écult
concepts by breaking complicated ideas into smaller chunks. An in-
teresting feedback loop occurs when students learn concepts in small
chunks: the knowledge buzz they experience when concepts "click"
into place motivates them to continue learning more. I know this from
Ô¨Årst-hand experience, both as a teacher and as a student. I completed
my undergraduate studies in electrical engineering, then stayed on to
earn a M.Sc. in physics, and a Ph.D. in computer science.
Linear algebra played a central role throughout my studies. With
this book, I want to share with you some of what I've learned about
this expansive subject.
Ivan Savov
Montreal, 2016


Introduction
There have been countless advances in science and technology in re-
cent years.
Modern science and engineering Ô¨Åelds have developed
advanced models for understanding the real world, predicting the out-
comes of experiments, and building useful technology. We're still far
from obtaining a theory of everything that can predict the future,
but we understand a lot about the natural world at many levels of de-
scription: physical, chemical, biological, ecological, psychological, and
social. Anyone interested in being part of scientiÔ¨Åc and technological
advances has no choice but to learn mathematics, since mathematical
models are used throughout all Ô¨Åelds of study. The linear algebra
techniques you'll learn in this book are some of the most powerful
mathematical modelling tools that exist.
At the core of linear algebra lies a very simple idea: linearity. A
function f is linear if it obeys the equation
f(ax1 + bx2) = af(x1) + bf(x2),
where x1 and x2 are any two inputs suitable for the function. We use
the term linear combination to describe any expression constructed
from a set of variables by multiplying each variable by a constant
and adding the results. In the above equation, the linear combination
ax1 + bx2 of the inputs x1 and x2 is transformed into the linear
combination af(x1) + bf(x2) of the outputs of the function f(x1)
and f(x2).
Linear functions transform linear combinations
of their inputs into the same linear combination of their
outputs. That's it, that's all! Now you know everything there is to
know about linear algebra. The rest of the book is just details.
A signiÔ¨Åcant proportion of the models used by scientists and en-
gineers describe linear relationships between quantities. Scientists,
engineers, statisticians, business folk, and politicians develop and use
linear models to make sense of the systems they study. In fact, linear
models are often used to model even nonlinear (more complicated)
phenomena. There are several good reasons for using linear models.
The Ô¨Årst reason is that linear models are very good at approximating
1

2
INTRODUCTION
the real world. Linear models for nonlinear phenomena are referred to
as linear approximations. If you've previously studied calculus, you'll
remember learning about tangent lines. The tangent line to a curve
f(x) at xo is given by the equation
T(x) = f ‚Ä≤(xo)
 x ‚àíxo

+ f(xo).
This line has slope f ‚Ä≤(xo) and passes through the point (xo, f(xo)).
The equation of the tangent line T(x) serves to approximate the func-
tion f(x) near xo. Using linear algebra techniques to model nonlinear
phenomena can be understood as a multivariable generalization of
this idea.
Linear models can also be combined with nonlinear transforma-
tions of the model's inputs or outputs to describe nonlinear phenom-
ena. These techniques are often employed in machine learning: kernel
methods are arbitrary non-linear transformations of the inputs of a
linear model, and the sigmoid activation curve is used to transform
a smoothly-varying output of a linear model into a hard yes or no
decision.
Perhaps the main reason linear models are widely used is because
they are easy to describe mathematically, and easy to "Ô¨Åt" to real-
world systems. We can obtain the parameters of a linear model for a
real-world system by analyzing its behaviour for relatively few inputs.
We'll illustrate this important point with an example.
Example
At an art event, you enter a room with a multimedia
setup.
A drawing canvas on a tablet computer is projected on a
giant screen. Anything you draw on the tablet will instantly appear
projected on the giant screen. The user interface on the tablet screen
doesn't give any indication about how to hold the tablet "right side
up." What is the fastest way to Ô¨Ånd the correct orientation of the
tablet so your drawing will not appear rotated or upside-down?
This situation is directly analogous to the tasks scientists face ev-
ery day when trying to model real-world systems. The canvas on the
tablet describes a two-dimensional input space, and the wall projec-
tion is a two-dimensional output space. We're looking for the unknown
transformation T that maps the pixels of the tablet screen (the input
space) to coloured dots on the wall (the output space). If the un-
known transformation T is a linear transformation, we can learn its
parameters very quickly.
Let's describe each pixel in the input space with a pair of coordi-
nates (x, y) and each point on the wall with another pair of coordi-
nates (x‚Ä≤, y‚Ä≤). The unknown transformation T describes the mapping
of pixel coordinates to wall coordinates:
(x, y)
T
‚àí‚Üí(x‚Ä≤, y‚Ä≤).

3
Figure 6: An unknown linear transformation T maps "tablet coordinates"
to "screen coordinates." How can we characterize T?
To uncover how T transforms (x, y)-coordinates to (x‚Ä≤, y‚Ä≤)-coordinates,
you can use the following three-step procedure. First put a dot in the
lower left corner of the tablet to represent the origin (0, 0) of the
xy-coordinate system. Observe the location where the dot appears
on the wall‚Äîwe'll call this location the origin of the x‚Ä≤y‚Ä≤-coordinate
system. Next, make a short horizontal swipe on the screen to repre-
sent the x-direction (1, 0) and observe the transformed T(1, 0) that
appears on the wall. As the Ô¨Ånal step, make a vertical swipe in the
y-direction (0, 1) and see the transformed T(0, 1) that appears on the
wall. By noting how the xy-coordinate system is mapped to the x‚Ä≤y‚Ä≤-
coordinate system, you can determine which orientation you must
hold the tablet for your drawing to appear upright when projected on
the wall. Knowing the outputs of a linear transformation T
for all "directions" in its inputs space allows us to completely
characterize T.
In the case of the multimedia setup at the art event, we're looking
for an unknown transformation T from a two-dimensional input space
to a two-dimensional output space. Since T is a linear transformation,
it's possible to completely describe T with only two swipes.
Let's
look at the math to see why this is true.
Can you predict what
will appear on the wall if you make an angled swipe in the (2, 3)-
direction? Observe that the point (2, 3) in the input space can be
obtained by moving 2 units in the x-direction and 3 units in the y-
direction: (2, 3) = (2, 0) + (0, 3) = 2(1, 0) + 3(0, 1). Using the fact
that T is a linear transformation, we can predict the output of the
transformation when the input is (2, 3):
T(2, 3) = T(2(1, 0) + 3(0, 1)) = 2T(1, 0) + 3T(0, 1).
The projection of the diagonal swipe in the (2, 3)-direction will have
a length equal to 2 times the unit x-direction output T(1, 0) plus 3
times the unit y-direction output T(0, 1). Knowledge of the outputs

4
INTRODUCTION
of the two swipes T(1, 0) and T(0, 1) is suÔ¨Écient to determine the
linear transformation's output for any input (a, b). Any input (a, b)
can be expressed as a linear combination: (a, b) = a(1, 0) + b(0, 1).
The corresponding output will be T(a, b) = aT(1, 0) + bT(0, 1). Since
we know T(1, 0) and T(0, 1), we can calculate T(a, b).
TL;DR
Linearity allows us to analyze multidimensional processes
and transformations by studying their eÔ¨Äects on a small set of in-
puts. This is the essential reason linear models are so prominent in
science. Probing a linear system with each "input direction" is enough
to completely characterize the system. Without this linear structure,
characterizing unknown input-output systems is a much harder task.
Linear algebra is the study of linear structure, in all its details. The
theoretical results and computational procedures of you'll learn apply
to all things linear and vector-like.
Linear transformations
You can think of linear transformations as "vector functions" and un-
derstand their properties in analogy with the properties of the regular
functions you're familiar with. The action of a function on a number
is similar to the action of a linear transformation on a vector:
function f : R ‚ÜíR ‚áîlinear transformation T : Rn ‚ÜíRm
input x ‚ààR ‚áîinput ‚Éóx ‚ààRn
output f(x) ‚ààR ‚áîoutput T(‚Éóx) ‚ààRm
inverse function f ‚àí1 ‚áîinverse transformation T ‚àí1
zeros of f ‚áîkernel of T
Studying linear algebra will expose you to many topics associated
with linear transformations. You'll learn about concepts like vector
spaces, projections, and orthogonalization procedures. Indeed, a Ô¨Årst
linear algebra course introduces many advanced, abstract ideas; yet
all the new ideas you'll encounter can be seen as extensions of ideas
you're already familiar with. Linear algebra is the vector-upgrade to
your high-school knowledge of functions.
Prerequisites
To understand linear algebra, you must have some preliminary knowl-
edge of fundamental math concepts like numbers, equations, and func-
tions. For example, you should be able to tell me the meaning of the
parameters m and b in the equation f(x) = mx + b. If you do not

5
feel conÔ¨Ådent about your basic math skills, don't worry. Chapter 1
is specially designed to help bring you quickly up to speed on the
material of high school math.
It's not a requirement, but it helps if you've previously used vectors
in physics. If you haven't taken a mechanics course where you saw ve-
locities and forces represented as vectors, you should read Chapter 2,
as it provides a short summary of vectors concepts usually taught in
the Ô¨Årst week of Physics 101. The last section in the vectors chapter
(Section 2.4) is about complex numbers. You should read that section
at some point because we'll use complex numbers in Section 7.7 later
in the book.
Executive summary
The book is organized into ten chapters. Chapters 3 through 7 are
the core of linear algebra. Chapters 8 through 10 contain "optional
reading" about linear algebra applications.
The concept maps on
pages iv, v, and vi illustrate the connections between the topics we'll
cover. I know the maps are teeming with concepts, but don't worry‚Äî
the book is split into tiny chunks, and we'll navigate the material step
by step. It will be like Mario World, but in n dimensions and with a
lot of bonus levels.
Chapter 3 is an introduction to the subject of linear algebra. Lin-
ear algebra is the math of vectors and matrices, so we'll start by
deÔ¨Åning the mathematical operations we can perform on vectors and
matrices.
In Chapter 4, we'll tackle the computational aspects of linear al-
gebra. By the end of this course, you will know how to solve systems
of equations, transform a matrix into its reduced row echelon form,
compute the product of two matrices, and Ô¨Ånd the determinant and
the inverse of a square matrix. Each of these computational tasks can
be tedious to carry out by hand and can require lots of steps. There
is no way around this; we must do the grunt work before we get to
the cool stuÔ¨Ä.
In Chapter 5, we'll review the properties and equations of basic
geometrical objects like points, lines, and planes.
We'll learn how
to compute projections onto vectors, projections onto planes, and
distances between objects. We'll also review the meaning of vector
coordinates, which are lengths measured with respect to a basis. We'll
learn about linear combinations of vectors, the span of a set of vectors,
and formally deÔ¨Åne what a vector space is. In Section 5.5 we'll learn
how to use the reduced row echelon form of a matrix, in order to
describe the fundamental spaces associated with the matrix.
Chapter 6 is about linear transformations. Armed with the com-

6
INTRODUCTION
putational tools from Chapter 4 and the geometrical intuition from
Chapter 5, we can tackle the core subject of linear algebra: linear
transformations. We'll explore in detail the correspondence between
linear transformations (vectors functions T : Rn ‚ÜíRm) and their
representation as m √ó n matrices. We'll also learn how the coeÔ¨É-
cients in a matrix representation depend on the choice of basis for
the input and output spaces of the transformation. Section 6.4 on
the invertible matrix theorem serves as a midway checkpoint for
your understanding of linear algebra. This theorem connects several
seemingly disparate concepts: reduced row echelon forms, matrix in-
verses, row spaces, column spaces, and determinants. The invertible
matrix theorem links all these concepts and highlights the proper-
ties of invertible linear transformations that distinguish them from
non-linear transformations. Invertible transformations are one-to-one
correspondences (bijections) between the vectors in the input space
and the vectors in the output space.
Chapter 7 covers more advanced theoretical topics of linear al-
gebra. We'll deÔ¨Åne the eigenvalues and the eigenvectors of a square
matrix. We'll see how the eigenvalues of a matrix tell us important in-
formation about the properties of the matrix. We'll learn about some
special names given to diÔ¨Äerent types of matrices, based on the prop-
erties of their eigenvalues. In Section 7.3 we'll learn about abstract
vector spaces. Abstract vectors are mathematical object that‚Äîlike
vectors‚Äîhave components and can be scaled, added, and subtracted
component-wise. Section 7.7 will discuss linear algebra with complex
numbers. Instead of working with vectors with real coeÔ¨Écients, we
can do linear algebra with vectors that have complex coeÔ¨Écients. This
section serves as a review of all the material in the book. We'll revisit
all the key concepts discussed in order to check how they are aÔ¨Äected
by the change to complex numbers.
In Chapter 8 we'll discuss the applications of linear algebra. If
you've done your job learning the material in the Ô¨Årst seven chapters,
you'll get to learn all the cool things you can do with linear algebra.
Chapter 9 will introduce the basic concepts of probability theory.
Chapter 10 contains an introduction to quantum mechanics.
The sections in the book are self-contained so you could read them
in any order. Feel free to skip ahead to the parts that you want to
learn Ô¨Årst. That being said, the material is ordered to provide an opti-
mal knowing-what-you-need-to-know-before-learning-what-you-want-
to-know experience. If you're new to linear algebra, it would be best
to read them in order. If you Ô¨Ånd yourself stuck on a concept at some
point, refer to the concept maps to see if you're missing some prereq-
uisites and Ô¨Çip to the section of the book that will help you Ô¨Åll in the
knowledge gaps accordingly.

7
DiÔ¨Éculty level
In terms of diÔ¨Éculty of content, I must prepare you to get ready for
some serious uphill pushes.
As your personal "trail guide" up the
"mountain" of linear algebra, it's my obligation to warn you about
the diÔ¨Éculties that lie ahead, so you can mentally prepare for a good
challenge.
Linear algebra is a diÔ¨Écult subject because it requires developing
your computational skills, your geometrical intuition, and your ab-
stract thinking. The computational aspects of linear algebra are not
particularly diÔ¨Écult, but they can be boring and repetitive. You'll
have to carry out hundreds of steps of basic arithmetic. The geomet-
rical problems you'll be exposed to in Chapter 5 can be diÔ¨Écult at
Ô¨Årst, but will get easier once you learn to draw diagrams and develop
your geometric reasoning. The theoretical aspects of linear algebra are
diÔ¨Écult because they require a new way of thinking, which resembles
what doing "real math" is like. You must not only understand and use
the material, but also know how to prove mathematical statements
using the deÔ¨Ånitions and properties of math objects.
In summary, much toil awaits you as you learn the concepts of
linear algebra, but the eÔ¨Äort is totally worth it. All the brain sweat
you put into understanding vectors and matrices will lead to mind-
expanding insights. You will reap the beneÔ¨Åts of your eÔ¨Äorts for the
rest of your life as your knowledge of linear algebra will open many
doors for you.


Chapter 1
Math fundamentals
In this chapter we'll review the fundamental ideas of mathematics
which are the prerequisites for learning linear algebra. We'll deÔ¨Åne
the diÔ¨Äerent types of numbers and the concept of a function, which is
a transformation that takes numbers as inputs and produces numbers
as outputs. Linear algebra is the extension of these ideas to many
dimensions: instead of "doing math" with numbers and functions, in
linear algebra we'll be "doing math" with vectors and linear transfor-
mations.
Figure 1.1: A concept map showing the mathematical topics covered in
this chapter. We'll learn about how to solve equations using algebra, how to
model the world using functions, and some important facts about geometry.
The material in this chapter is required for your understanding of the more
advanced topics in this book.
9

10
MATH FUNDAMENTALS
1.1
Solving equations
Most math skills boil down to being able to manipulate and solve
equations. Solving an equation means Ô¨Ånding the value of the un-
known in the equation.
Check this shit out:
x2 ‚àí4 = 45.
To solve the above equation is to answer the question "What is x?"
More precisely, we want to Ô¨Ånd the number that can take the place
of x in the equation so that the equality holds. In other words, we're
asking,
"Which number times itself minus four gives 45?"
That is quite a mouthful, don't you think?
To remedy this ver-
bosity, mathematicians often use specialized mathematical symbols.
The problem is that these specialized symbols can be very confusing.
Sometimes even the simplest math concepts are inaccessible if you
don't know what the symbols mean.
What are your feelings about math, dear reader? Are you afraid
of it? Do you have anxiety attacks because you think it will be too
diÔ¨Écult for you?
Chill!
Relax, my brothers and sisters.
There's
nothing to it. Nobody can magically guess what the solution to an
equation is immediately. To Ô¨Ånd the solution, you must break the
problem down into simpler steps.
To Ô¨Ånd x, we can manipulate the original equation, transforming
it into a diÔ¨Äerent equation (as true as the Ô¨Årst) that looks like this:
x = only numbers.
That's what it means to solve. The equation is solved because you
can type the numbers on the right-hand side of the equation into a
calculator and obtain the numerical value of x that you're seeking.
By the way, before we continue our discussion, let it be noted: the
equality symbol (=) means that all that is to the left of = is equal to
all that is to the right of =. To keep this equality statement true, for
every change you apply to the left side of the equation, you
must apply the same change to the right side of the equation.
To Ô¨Ånd x, we need to correctly manipulate the original equation
into its Ô¨Ånal form, simplifying it in each step. The only requirement
is that the manipulations we make transform one true equation into
another true equation. Looking at our earlier example, the Ô¨Årst sim-
plifying step is to add the number four to both sides of the equation:
x2 ‚àí4 + 4 = 45 + 4,

1.1
SOLVING EQUATIONS
11
which simpliÔ¨Åes to
x2 = 49.
The expression looks simpler, yes? How did I know to perform this
operation? I was trying to "undo" the eÔ¨Äects of the operation ‚àí4.
We undo an operation by applying its inverse.
In the case where
the operation is subtraction of some amount, the inverse operation is
the addition of the same amount. We'll learn more about function
inverses in Section 1.4 (page 18).
We're getting closer to our goal, namely to isolate x on one side of
the equation, leaving only numbers on the other side. The next step
is to undo the square x2 operation. The inverse operation of squaring
a number x2 is to take the square root ‚àö
so this is what we'll do
next. We obtain
‚àö
x2 =
‚àö
49.
Notice how we applied the square root to both sides of the equation?
If we don't apply the same operation to both sides, we'll break the
equality!
The equation
‚àö
x2 =
‚àö
49 simpliÔ¨Åes to
|x| = 7.
What's up with the vertical bars around x? The notation |x| stands
for the absolute value of x, which is the same as x except we ignore
the sign. For example |5| = 5 and | ‚àí5| = 5, too. The equation
|x| = 7 indicates that both x = 7 and x = ‚àí7 satisfy the equation
x2 = 49.
Seven squared is 49, and so is (‚àí7)2 = 49 because two
negatives cancel each other out.
We're done since we isolated x. The Ô¨Ånal solutions are
x = 7
or
x = ‚àí7.
Yes, there are two possible answers. You can check that both of the
above values of x satisfy the initial equation x2 ‚àí4 = 45.
If you are comfortable with all the notions of high school math
and you feel you could have solved the equation x2 ‚àí4 = 45 on your
own, then you should consider skipping ahead to Chapter 2. If on the
other hand you are wondering how the squiggle killed the power two,
then this chapter is for you! In the following sections we will review
all the essential concepts from high school math that you will need to
power through the rest of this book. First, let me tell you about the
diÔ¨Äerent kinds of numbers.

12
MATH FUNDAMENTALS
1.2
Numbers
In the beginning, we must deÔ¨Åne the main players in the world of
math: numbers.
DeÔ¨Ånitions
Numbers are the basic objects we use to calculate things. Mathe-
maticians like to classify the diÔ¨Äerent kinds of number-like objects
into sets:
‚Ä¢ The natural numbers: N = {0, 1, 2, 3, 4, 5, 6, 7, . . . }
‚Ä¢ The integers: Z = {. . . , ‚àí3, ‚àí2, ‚àí1, 0, 1, 2, 3, . . . }
‚Ä¢ The rational numbers: Q = { 5
3, 22
7 , 1.5, 0.125, ‚àí7, . . . }
‚Ä¢ The real numbers: R = {‚àí1, 0, 1,
‚àö
2, e, œÄ, 4.94 . . . , . . . }
‚Ä¢ The complex numbers: C = {‚àí1, 0, 1, i, 1 + i, 2 + 3i, . . . }
These categories of numbers should be somewhat familiar to you.
Think of them as neat classiÔ¨Åcation labels for everything that you
would normally call a number. Each item in the above list is a set.
A set is a collection of items of the same kind. Each collection has a
name and a precise deÔ¨Ånition. Note also that each of the sets in the
list contains all the sets above it. For now, we don't need to go into
the details of sets and set notation (page 73), but we do need to be
aware of the diÔ¨Äerent sets of numbers.
Why do we need so many diÔ¨Äerent sets of numbers? The answer
is partly historical and partly mathematical. Each set of numbers is
associated with more and more advanced mathematical problems.
The simplest numbers are the natural numbers N, which are suÔ¨É-
cient for all your math needs if all you are going to do is count things.
How many goats? Five goats here and six goats there so the total
is 11 goats. The sum of any two natural numbers is also a natural
number.
As soon as you start using subtraction (the inverse operation of ad-
dition), you start running into negative numbers, which are numbers
outside the set of natural numbers. If the only mathematical opera-
tions you will ever use are addition and subtraction, then the set of
integers Z = {. . . , ‚àí2, ‚àí1, 0, 1, 2, . . .} will be suÔ¨Écient. Think about
it. Any integer plus or minus any other integer is still an integer.
You can do a lot of interesting math with integers. There is an
entire Ô¨Åeld in math called number theory that deals with integers.
However, to restrict yourself solely to integers is somewhat limiting.
You can't use the notion of 2.5 goats for example.
The menu at
Rotisserie Romados, which oÔ¨Äers 1
4 of a chicken, would be completely
confusing.

1.2
NUMBERS
13
If you want to use division in your mathematical calculations,
you'll need the rationals Q. The rationals are the set of fractions of
integers:
Q =

all z such that z = x
y where x and y are in Z, and y Ã∏= 0

.
You can add, subtract, multiply, and divide rational numbers, and the
result will always be a rational number. However, even the rationals
are not enough for all of math!
In geometry, we can obtain irrational quantities like
‚àö
2 (the di-
agonal of a square with side 1) and œÄ (the ratio between a circle's cir-
cumference and its diameter). There are no integers x and y such that
‚àö
2 = x
y . Therefore,
‚àö
2 is not part of the set Q, and we say that
‚àö
2
is irrational. An irrational number has an inÔ¨Ånitely long decimal ex-
pansion that doesn't repeat. For example, œÄ = 3.141592653589793 . . .
where the dots indicate that the decimal expansion of œÄ continues all
the way to inÔ¨Ånity.
Adding the irrational numbers to the rationals gives us all the
useful numbers, which we call the set of real numbers R. The set R
contains the integers, the fractions Q, as well as irrational numbers like
‚àö
2 = 1.4142135 . . .. By using the reals you can compute pretty much
anything you want. From here on in the text, when I say number, I
mean an element of the set of real numbers R.
The only thing you can't do with the reals is take the square root
of a negative number‚Äîyou need the complex numbers C for that. We
defer the discussion on C until the end of Chapter 3.
Operations on numbers
Addition
You can add and subtract numbers. I will assume you are familiar
with this kind of stuÔ¨Ä:
2 + 5 = 7,
45 + 56 = 101,
65 ‚àí66 = ‚àí1,
9 999 + 1 = 10 000.
It can help visual learners to picture numbers as lengths measured out
on the number line. Adding numbers is like adding sticks together:
the resulting stick has a length equal to the sum of the lengths of the
constituent sticks.
Addition is commutative, which means that a + b = b + a. It is
also associative, which means that if you have a long summation like
a + b + c you can compute it in any order (a + b) + c or a + (b + c)
and you'll get the same answer.
Subtraction is the inverse operation of addition.

14
MATH FUNDAMENTALS
Multiplication
You can also multiply numbers together.
ab = a + a + ¬∑ ¬∑ ¬∑ + a
|
{z
}
b times
= b + b + ¬∑ ¬∑ ¬∑ + b
|
{z
}
a times
.
Note that multiplication can be deÔ¨Åned in terms of repeated addition.
The visual way to think about multiplication is as an area calcu-
lation. The area of a rectangle of base a and height b is equal to ab.
A rectangle with a height equal to its base is a square, and this is why
we call aa = a2 "a squared."
Multiplication of numbers is also commutative, ab = ba; and asso-
ciative, abc = (ab)c = a(bc). In modern notation, no special symbol
is used to denote multiplication; we simply put the two factors next
to each other and say the multiplication is implicit. Some other ways
to denote multiplication are a ¬∑ b, a √ó b, and, on computer systems,
a ‚àób.
Division
Division is the inverse operation of multiplication.
a/b = a
b = one bth of a.
Whatever a is, you need to divide it into b equal parts and take one
such part. Some texts denote division as a √∑ b.
Note that you cannot divide by 0. Try it on your calculator or
computer. It will say "error divide by zero" because this action
simply doesn't make sense. After all, what would it mean to divide
something into zero equal parts?
Exponentiation
Often an equation calls for us to multiply things together many times.
The act of multiplying a number by itself many times is called expo-
nentiation, and we denote this operation as a superscript:
ab = aaa ¬∑ ¬∑ ¬∑ a
|
{z
}
b times
.
We can also encounter negative exponents. The negative in the ex-
ponent does not mean "subtract," but rather "divide by":
a‚àíb = 1
ab =
1
aaa ¬∑ ¬∑ ¬∑ a
|
{z
}
b times
.

1.2
NUMBERS
15
Fractional exponents describe square-root-like operations:
a
1
2 ‚â°‚àöa ‚â°
2‚àöa,
a
1
3 ‚â°
3‚àöa,
a
1
4 ‚â°
4‚àöa = a
1
2
1
2 =

a
1
2
 1
2 =
q‚àöa.
Square root ‚àöx is the inverse operation of x2. Similarly, for any n we
deÔ¨Åne the function
n‚àöx (the nth root of x) to be the inverse function
of xn.
It's worth clarifying what "taking the nth root" means and under-
standing when to use this operation. The nth root of a is a number
which, when multiplied together n times, will give a. For example, a
cube root satisÔ¨Åes
3‚àöa 3‚àöa 3‚àöa =
 3‚àöa
3 = a =
3‚àö
a3.
Do you see why
3‚àöx and x3 are inverse operations?
The fractional exponent notation makes the meaning of roots much
more explicit. The nth root of a can be denoted in two equivalent
ways:
n‚àöa ‚â°a
1
n .
The symbol "‚â°" stands for "is equivalent to" and is used when two
mathematical objects are identical. Equivalence is a stronger relation
than equality. Writing
n‚àöa = a
1
n indicates we've found two mathe-
matical expressions (the left-hand side and the right-hand side of the
equality) that happen to be equal to each other. It is more mathe-
matically precise to write
n‚àöa ‚â°a
1
n , which tells us
n‚àöa and a
1
n are
two diÔ¨Äerent ways of denoting the same mathematical object.
The nth root of a is equal to one nth of a with respect to multi-
plication. To Ô¨Ånd the whole number, multiply the number a
1
n times
itself n times:
a
1
n a
1
n a
1
n a
1
n ¬∑ ¬∑ ¬∑ a
1
n a
1
n
|
{z
}
n times
=

a
1
n
n
= a
n
n = a1 = a.
The n-fold product of 1
n-fractional exponents of any number produces
that number with exponent one, therefore the inverse operation of
n‚àöx
is xn.
The commutative law of multiplication ab = ba implies that we
can see any fraction
a
b in two diÔ¨Äerent ways:
a
b = a 1
b =
1
ba.
We
multiply by a then divide the result by b, or Ô¨Årst we divide by b and
then multiply the result by a. Similarly, when we have a fraction in
the exponent, we can write the answer in two equivalent ways:
a
2
3 =
3‚àö
a2 = ( 3‚àöa)2,
a‚àí1
2 = 1
a
1
2 =
1
‚àöa,
a
m
n =
 n‚àöa
m =
n‚àö
am.
Make sure the above notation makes sense to you.
As an exer-
cise, try computing 5
4
3 on your calculator and check that you obtain
8.54987973 . . . as the answer.

16
MATH FUNDAMENTALS
Operator precedence
There is a standard convention for the order in which mathematical
operations must be performed. The basic algebra operations have the
following precedence:
1. Exponents and roots
2. Products and divisions
3. Additions and subtractions
For instance, the expression 5√ó32 +13 is interpreted as "Ô¨Årst Ô¨Ånd the
square of 3, then multiply it by 5, and then add 13." Parenthesis are
needed to carry out the operations in a diÔ¨Äerent order: to multiply
5 times 3 Ô¨Årst and then take the square, the equation should read
(5 √ó 3)2 + 13, where parenthesis indicate that the square acts on
(5 √ó 3) as a whole and not on 3 alone.
Other operations
We can deÔ¨Åne all kinds of operations on numbers. The above three are
special operations since they feel simple and intuitive to apply, but we
can also deÔ¨Åne arbitrary transformations on numbers. We call these
transformations functions. Before we learn about functions, let's Ô¨Årst
cover variables.
1.3
Variables
In math we use a lot of variables, which are placeholder names for
any number or unknown.
Example
Your friend invites you to a party and oÔ¨Äers you to drink
from a weirdly shaped shooter glass. You can't quite tell if it holds
25 ml of vodka or 50 ml or some amount in between.
Since it's a
mystery how much booze each shot contains, you shrug your shoulders
and say there's x ml in there. The night happens. So how much did
you drink? If you had three shots, then you drank 3x ml of vodka.
If you want to take it a step further, you can say you drank n shots,
making the total amount of alcohol you consumed nx ml.
Variables allow us to talk about quantities without knowing the
details. This is abstraction and it is very powerful stuÔ¨Ä: it allows you
to get drunk without knowing how drunk exactly!
Variable names
There are common naming patterns for variables:

1.3
VARIABLES
17
‚Ä¢ x: general name for the unknown in equations (also used to de-
note a function's input, as well as an object's position in physics
problems)
‚Ä¢ v: velocity in physics problems
‚Ä¢ Œ∏, œï: the Greek letters theta and phi are used to denote angles
‚Ä¢ xi, xf: denote an object's initial and Ô¨Ånal positions in physics
problems
‚Ä¢ X: a random variable in probability theory
‚Ä¢ C: costs in business along with P for proÔ¨Åt, and R for revenue
Variable substitution
We can often change variables and replace one unknown variable with
another to simplify an equation.
For example, say you don't feel
comfortable around square roots. Every time you see a square root,
you freak out until one day you Ô¨Ånd yourself taking an exam trying
to solve for x in the following equation:
6
5 ‚àí‚àöx = ‚àöx.
Don't freak out! In crucial moments like this, substitution can help
with your root phobia. Just write, "Let u = ‚àöx" on your exam, and
voila, you can rewrite the equation in terms of the variable u:
6
5 ‚àíu = u,
which contains no square roots.
The next step to solve for u is to undo the division operation.
Multiply both sides of the equation by (5 ‚àíu) to obtain
6
5 ‚àíu(5 ‚àíu) = u(5 ‚àíu),
which simpliÔ¨Åes to
6 = 5u ‚àíu2.
This can be rewritten as a quadratic equation, u2 ‚àí5u+6 = 0. Next,
we can factor the quadratic to obtain the equation (u‚àí2)(u‚àí3) = 0,
for which u1 = 2 and u2 = 3 are the solutions.
The last step is
to convert our u-answers into x answers by using u = ‚àöx, which
is equivalent to x = u2.
The Ô¨Ånal answers are x1 = 22 = 4 and
x2 = 32 = 9. Try plugging these x values into the original square root
equation to verify that they satisfy it.

18
MATH FUNDAMENTALS
Compact notation
Symbolic manipulation is a powerful tool because it allows us to man-
age complexity. Say you're solving a physics problem in which you're
told the mass of an object is m = 140 kg. If there are many steps in
the calculation, would you rather use the number 140 kg in each step,
or the shorter variable m? It's much easier in the long run to use the
variable m throughout your calculation, and wait until the last step
to substitute the value 140 kg when computing the Ô¨Ånal answer.
1.4
Functions and their inverses
As we saw in the section on solving equations, the ability to "undo"
functions is a key skill for solving equations.
Example
Suppose we're solving for x in the equation
f(x) = c,
where f is some function and c is some constant. Our goal is to isolate
x on one side of the equation, but the function f stands in our way.
By using the inverse function (denoted f ‚àí1) we "undo" the eÔ¨Äects
of f. Then we apply the inverse function f ‚àí1 to both sides of the
equation to obtain
f ‚àí1(f(x)) = x = f ‚àí1 (c) .
By deÔ¨Ånition, the inverse function f ‚àí1 performs the opposite action
of the function f so together the two functions cancel each other out.
We have f ‚àí1(f(x)) = x for any number x.
Provided everything is kosher (the function f ‚àí1 must be deÔ¨Åned
for the input c), the manipulation we made above is valid and we have
obtained the answer x = f ‚àí1(c).
The above example introduces the notation f ‚àí1 for denoting the
function's inverse. This notation is borrowed from the notion of in-
verse numbers: multiplication by the number a‚àí1 is the inverse op-
eration of multiplication by the number a: a‚àí1ax = 1x = x. In the
case of functions, however, the negative-one exponent does not re-
fer to "one over-f(x)" as in
1
f(x) = (f(x))‚àí1; rather, it refers to the
function's inverse. In other words, the number f ‚àí1(y) is equal to the
number x such that f(x) = y.
Be careful: sometimes applying the inverse leads to multiple so-
lutions. For example, the function f(x) = x2 maps two input values
(x and ‚àíx) to the same output value x2 = f(x) = f(‚àíx). The in-
verse function of f(x) = x2 is f ‚àí1(x) = ‚àöx, but both x = +‚àöc

1.4
FUNCTIONS AND THEIR INVERSES
19
and x = ‚àí‚àöc are solutions to the equation x2 = c. In this case,
this equation's solutions can be indicated in shorthand notation as
x = ¬±‚àöc.
Formulas
Here is a list of common functions and their inverses:
function f(x)
‚áî
inverse f ‚àí1(x)
x + 2
‚áî
x ‚àí2
2x
‚áî
1
2x
‚àíx
‚áî
‚àíx
x2
‚áî
¬±‚àöx
2x
‚áî
log2(x)
3x + 5
‚áî
1
3(x ‚àí5)
ax
‚áî
loga(x)
exp(x) ‚â°ex
‚áî
ln(x) ‚â°loge(x)
sin(x)
‚áî
sin‚àí1(x) ‚â°arcsin(x)
cos(x)
‚áî
cos‚àí1(x) ‚â°arccos(x)
The function-inverse relationship is reÔ¨Çexive‚Äîif you see a function
on one side of the above table (pick a side, any side), you'll Ô¨Ånd its
inverse on the opposite side.
Example
Let's say your teacher doesn't like you and right away, on the Ô¨Årst
day of class, he gives you a serious equation and tells you to Ô¨Ånd x:
log5

3 +
q
6‚àöx ‚àí7

= 34 + sin(5.5) ‚àíŒ®(1).
See what I mean when I say the teacher doesn't like you?
First, note that it doesn't matter what Œ® (the capital Greek letter
psi) is, since x is on the other side of the equation. You can keep
copying Œ®(1) from line to line, until the end, when you throw the
ball back to the teacher. "My answer is in terms of your variables,
dude. You go Ô¨Ågure out what the hell Œ® is since you brought it up in
the Ô¨Årst place!" By the way, it's not actually recommended to quote
me verbatim should a situation like this arise. The same goes with
sin(5.5). If you don't have a calculator handy, don't worry about it.
Keep the expression sin(5.5) instead of trying to Ô¨Ånd its numerical

20
MATH FUNDAMENTALS
value. In general, try to work with variables as much as possible and
leave the numerical computations for the last step.
Okay, enough beating about the bush. Let's just Ô¨Ånd x and get
it over with! On the right-hand side of the equation, we have the
sum of a bunch of terms with no x in them, so we'll leave them as
they are. On the left-hand side, the outermost function is a logarithm
base 5. Cool. Looking at the table of inverse functions we Ô¨Ånd the
exponential function is the inverse of the logarithm: ax ‚áîloga(x).
To get rid of log5, we must apply the exponential function base 5 to
both sides:
5
log5

3+‚àö
6‚àöx‚àí7

= 534+sin(5.5)‚àíŒ®(1),
which simpliÔ¨Åes to
3 +
q
6‚àöx ‚àí7 = 534+sin(5.5)‚àíŒ®(1),
since 5x cancels log5 x.
From here on, it is going to be as if Bruce Lee walked into a place
with lots of bad guys. Addition of 3 is undone by subtracting 3 on
both sides:
q
6‚àöx ‚àí7 = 534+sin(5.5)‚àíŒ®(1) ‚àí3.
To undo a square root we take the square:
6‚àöx ‚àí7 =

534+sin(5.5)‚àíŒ®(1) ‚àí3
2
.
Add 7 to both sides,
6‚àöx =

534+sin(5.5)‚àíŒ®(1) ‚àí3
2
+ 7,
divide by 6
‚àöx = 1
6

534+sin(5.5)‚àíŒ®(1) ‚àí3
2
+ 7

,
and square again to Ô¨Ånd the Ô¨Ånal answer:
x =
1
6

534+sin(5.5)‚àíŒ®(1) ‚àí3
2
+ 7
2
.
Did you see what I was doing in each step? Next time a function
stands in your way, hit it with its inverse so it knows not to challenge
you ever again.

1.5
BASIC RULES OF ALGEBRA
21
Discussion
The recipe I have outlined above is not universally applicable. Some-
times x isn't alone on one side. Sometimes x appears in several places
in the same equation. In these cases, you can't eÔ¨Äortlessly work your
way, Bruce Lee-style, clearing bad guys and digging toward x‚Äîyou
need other techniques.
The bad news is there's no general formula for solving complicated
equations. The good news is the above technique of "digging toward
x" is suÔ¨Écient for 80% of what you are going to be doing. You can get
another 15% if you learn how to solve the quadratic equation (page
25):
ax2 + bx + c = 0.
Solving third-degree polynomial equations like ax3 + bx2 + cx + d = 0
with pen and paper is also possible, but at this point you might as
well start using a computer to solve for the unknowns.
There are all kinds of other equations you can learn how to solve:
equations with multiple variables, equations with logarithms, equa-
tions with exponentials, and equations with trigonometric functions.
The principle of "digging" toward the unknown by applying inverse
functions is the key for solving all these types of equations, so be sure
to practice using it.
1.5
Basic rules of algebra
It's important that you know the general rules for manipulating num-
bers and variables, a process otherwise known as‚Äîyou guessed it‚Äî
algebra. This little refresher will cover these concepts to make sure
you're comfortable on the algebra front. We'll also review some impor-
tant algebraic tricks, like factoring and completing the square, which
are useful when solving equations.
When an expression contains multiple things added together, we
call those things terms. Furthermore, terms are usually composed of
many things multiplied together. When a number x is obtained as
the product of other numbers like x = abc, we say "x factors into a,
b, and c." We call a, b, and c the factors of x.
Given any four numbers a, b, c, and d, we can apply the following
algebraic properties:
1. Associative property: a + b + c = (a + b) + c = a + (b + c) and
abc = (ab)c = a(bc)
2. Commutative property: a + b = b + a and ab = ba
3. Distributive property: a(b + c) = ab + ac

22
MATH FUNDAMENTALS
We use the distributive property every time we expand brackets. For
example a(b + c + d) = ab + ac + ad.
The brackets, also known
as parentheses, indicate the expression (b + c + d) must be treated
as a whole: a factor that consists of three terms. Multiplying this
expression by a is the same as multiplying each term by a.
The opposite operation of expanding is called factoring, which
consists of rewriting the expression with the common parts taken out
in front of a bracket: ab + ac = a(b + c). In this section, we'll discuss
both of these operations and illustrate what they're capable of.
Expanding brackets
The distributive property is useful when dealing with polynomials:
(x + 3)(x + 2) = x(x + 2) + 3(x + 2) = x2 + x2 + 3x + 6.
We can use the commutative property on the second term x2 = 2x,
then combine the two x terms into a single term to obtain
(x + 3)(x + 2) = x2 + 5x + 6.
Let's look at this operation in its abstract form:
(x + a)(x + b) = x2 + (a + b)x + ab.
The product of two linear terms (expressions of the form x + ?) is
equal to a quadratic expression. Observe that the middle term on the
right-hand side contains the sum of the two constants on the left-hand
side (a + b), while the third term contains their product ab.
It is very common for people to confuse these terms. If you are ever
confused about an algebraic expression, go back to the distributive
property and expand the expression using a step-by-step approach. As
a second example, consider this slightly-more-complicated algebraic
expression and its expansion:
(x + a)(bx2 + cx + d) = x(bx2 + cx + d) + a(bx2 + cx + d)
= bx3 + cx2 + dx + abx2 + acx + ad
= bx3 + (c + ab)x2 + (d + ac)x + ad.
Note how all terms containing x2 are grouped into a one term, and all
terms containing x are grouped into another term. We use this pattern
when dealing with expressions containing diÔ¨Äerent powers of x.
Example
Suppose we are asked to solve for t in the equation
7(3 + 4t) = 11(6t ‚àí4).

1.5
BASIC RULES OF ALGEBRA
23
Since the unknown t appears on both sides of the equation, it is not
immediately obvious how to proceed.
To solve for t, we must bring all t terms to one side and all constant
terms to the other side. First, expand the two brackets to obtain
21 + 28t = 66t ‚àí44.
Then move things around to relocate all ts to the equation's right-
hand side and all constants to the left-hand side:
21 + 44 = 66t ‚àí28t.
We see t is contained in both terms on the right-hand side, so we can
rewrite the equation as
21 + 44 = (66 ‚àí28)t.
The answer is within close reach: t = 21+44
66‚àí28 = 65
38.
Factoring
Factoring involves taking out the common part(s) of a complicated
expression in order to make the expression more compact. Suppose
you're given the expression 6x2y +15x and must simplify it by taking
out common factors. The expression has two terms and each term
can be split into its constituent factors to obtain
6x2y + 15x = (3)(2)(x)(x)y + (5)(3)x.
Since factors x and 3 appear in both terms, we can factor them out
to the front like this:
6x2y + 15x = 3x(2xy + 5).
The expression on the right shows 3x is common to both terms.
Here's another example where factoring is used:
2x2y + 2x + 4x = 2x(xy + 1 + 2) = 2x(xy + 3).
Quadratic factoring
When dealing with a quadratic function, it is often useful to rewrite
the function as a product of two factors. Suppose you're given the
quadratic function f(x) = x2 ‚àí5x + 6 and asked to describe its prop-
erties. What are the roots of this function? In other words, for what
values of x is this function equal to zero? For which values of x is the
function positive, and for which x values is the function negative?

24
MATH FUNDAMENTALS
Factoring the expression x2 ‚àí5x+6 will help us see the properties
of the function more clearly. To factor a quadratic expression is to
express it as the product of two factors:
f(x) = x2 ‚àí5x + 6 = (x ‚àí2)(x ‚àí3).
We now see at a glance the solutions (roots) are x1 = 2 and x2 = 3.
We can also see for which x values the function will be overall positive:
for x > 3, both factors will be positive, and for x < 2 both factors
will be negative, and a negative times a negative gives a positive. For
values of x such that 2 < x < 3, the Ô¨Årst factor will be positive, and
the second factor negative, making the overall function negative.
For certain simple quadratics like the one above, you can simply
guess what the factors will be. For more complicated quadratic ex-
pressions, you'll need to use the quadratic formula (page 25), which
will be the subject of the next section. For now let us continue with
more algebra tricks.
Completing the square
Any quadratic expression Ax2 + Bx + C can be rewritten in the form
A(x ‚àíh)2 + k for some constants h and k.
This process is called
completing the square due to the reasoning we follow to Ô¨Ånd the value
of k.
The constants h and k can be interpreted geometrically as
the horizontal and vertical shifts in the graph of the basic quadratic
function.
The graph of the function f(x) = A(x ‚àíh)2 + k is the
same as the graph of the function f(x) = Ax2 except it is shifted h
units to the right and k units upward. We will discuss the geometrical
meaning of h and k in more detail in Section 1.9 (page 49). For now,
let's focus on the algebra steps.
Let's try to Ô¨Ånd the values of k and h needed to complete the
square in the expression x2 + 5x + 6. We start from the assumption
that the two expressions are equal, and then expand the bracket to
obtain
x2+5x+6 =A(x‚àíh)2+k =A(x2‚àí2hx+h2)+k =Ax2‚àí2Ahx+Ah2+k.
Observe the structure in the above equation. On both sides of the
equality there is one term which contains x2 (the quadratic term), one
term that contains x1 (the linear term), and some constant terms. By
focusing on the quadratic terms on both sides of the equation (they
are underlined) we see A = 1, so we can rewrite the equation as
x2 + 5x + 6 = x2‚àí2hx + h2 + k.
Next we look at the linear terms (underlined) and infer h = ‚àí2.5.
After rewriting, we obtain an equation in which k is the only unknown:
x2 + 5x + 6 = x2 ‚àí2(‚àí2.5)x + (‚àí2.5)2 + k.

1.6
SOLVING QUADRATIC EQUATIONS
25
We must pick a value of k that makes the constant terms equal:
k = 6‚àí(‚àí2.5)2 = 6‚àí(2.5)2 = 6‚àí
5
2
2
= 6√ó4
4‚àí25
4 = 24 ‚àí25
4
= ‚àí1
4 .
After completing the square we obtain
x2 + 5x + 6 = (x + 2.5)2 ‚àí1
4.
The right-hand side of the expression above tells us our function is
equivalent to the basic function x2, shifted 2.5 units to the left and
1
4 units down. This would be very useful information if you ever had
to draw the graph of this function‚Äîyou could simply plot the basic
graph of x2 and then shift it appropriately.
It is important you become comfortable with this procedure for
completing the square. It is not extra diÔ¨Écult, but it does require you
to think carefully about the unknowns h and k and to choose their
values appropriately. There is no general formula for Ô¨Ånding k, but
you can remember the following simple shortcut for Ô¨Ånding h. Given
an equation Ax2 + Bx + C = A(x ‚àíh)2 + k, we have h = ‚àíB
2A . Using
this shortcut will save you some time, but you will still have to go
through the algebra steps to Ô¨Ånd k.
Take out a pen and a piece of paper now (yes, right now!) and
verify that you can correctly complete the square in these expressions:
x2 ‚àí6x + 13 = (x ‚àí3)2 + 4 and x2 + 4x + 1 = (x + 2)2 ‚àí3.
1.6
Solving quadratic equations
What would you do if asked to solve for x in the quadratic equation
x2 = 45x + 23? This is called a quadratic equation since it contains
the unknown variable x squared. The name comes from the Latin
quadratus, which means square. Quadratic equations appear often,
so mathematicians created a general formula for solving them.
In
this section, we'll learn about this formula and use it to put some
quadratic equations in their place.
Before we can apply the formula, we need to rewrite the equation
we are trying to solve in the following form:
ax2 + bx + c = 0.
We reach this form‚Äîcalled the standard form of the quadratic equation‚Äî
by moving all the numbers and xs to one side and leaving only 0 on
the other side.
For example, to transform the quadratic equation
x2 = 45x + 23 into standard form, subtract 45x + 23 from both sides
of the equation to obtain x2 ‚àí45x ‚àí23 = 0. What are the values of
x that satisfy this formula?

26
MATH FUNDAMENTALS
Claim
The solutions to the equation ax2 + bx + c = 0 are
x1 = ‚àíb +
‚àö
b2 ‚àí4ac
2a
and
x2 = ‚àíb ‚àí
‚àö
b2 ‚àí4ac
2a
.
Let's see how these formulas are used to solve x2 ‚àí45x ‚àí23 = 0.
Finding the two solutions requires the simple mechanical task of iden-
tifying a = 1, b = ‚àí45, and c = ‚àí23 and plugging these values into
the formulas:
x1 = 45 +
p
452 ‚àí4(1)(‚àí23)
2
= 45.5054 . . . ,
x2 = 45 ‚àí
p
452 ‚àí4(1)(‚àí23)
2
= ‚àí0.5054 . . . .
Verify using your calculator that both of the values above satisfy the
original equation x2 = 45x + 23.
Proof of claim
This is an important proof. I want you to see how we can derive the
quadratic formula from Ô¨Årst principles because this knowledge will
help you understand the formula. The proof will use the completing-
the-square technique from the previous section.
Starting with ax2 + bx + c = 0, Ô¨Årst move c to the other side of
the equation:
ax2 + bx = ‚àíc.
Divide by a on both sides:
x2 + b
ax = ‚àíc
a.
Now complete the square on the left-hand side by asking, "What are
the values of h and k that satisfy the equation
(x ‚àíh)2 + k = x2 + b
ax ?"
To Ô¨Ånd the values for h and k, we'll expand the left-hand side to
obtain (x ‚àíh)2 + k = x2 ‚àí2hx + h2 + k. We can now identify h by
looking at the coeÔ¨Écients in front of x on both sides of the equation.
We have ‚àí2h = b
a and hence h = ‚àíb
2a.

1.6
SOLVING QUADRATIC EQUATIONS
27
Let's see what we have so far:

x + b
2a
2
=

x + b
2a

x + b
2a

= x2 + b
2ax + x b
2a + b2
4a2 = x2 + b
ax + b2
4a2 .
To determine k, we need to move that last term to the other side:

x + b
2a
2
‚àíb2
4a2 = x2 + b
ax.
We can continue with the proof where we left oÔ¨Ä:
x2 + b
ax = ‚àíc
a.
Replace the left-hand side with the complete-the-square expression
and obtain

x + b
2a
2
‚àíb2
4a2 = ‚àíc
a.
From here on, we can use the standard procedure for solving equations
(page 10). Arrange all constants on the right-hand side:

x + b
2a
2
= ‚àíc
a + b2
4a2 .
Next, take the square root of both sides. Since the square function
maps both positive and negative numbers to the same value, this step
yields two solutions:
x + b
2a = ¬±
r
‚àíc
a + b2
4a2 .
Let's take a moment to tidy up the mess under the square root:
r
‚àíc
a + b2
4a2 =
s
‚àí(4a)c
(4a)a + b2
4a2 =
r
‚àí4ac + b2
4a2
=
‚àö
b2 ‚àí4ac
2a
.
We obtain
x + b
2a = ¬±
‚àö
b2 ‚àí4ac
2a
,
which is just one step from the Ô¨Ånal answer,
x = ‚àíb
2a ¬±
‚àö
b2 ‚àí4ac
2a
= ‚àíb ¬±
‚àö
b2 ‚àí4ac
2a
.
This completes the proof.
‚ñ°

28
MATH FUNDAMENTALS
Alternative proof of claim
To have a proof we don't necessarily need to show the derivation of
the formula as outlined above. The claim states that x1 and x2 are
solutions. To prove the claim we can simply plug x1 and x2 into the
quadratic equation and verify the answers are zero. Verify this on
your own.
Applications
The Golden Ratio
The golden ratio is an essential proportion in geometry, art, aesthet-
ics, biology, and mysticism, and is usually denoted as œï = 1+
‚àö
5
2
=
1.6180339 . . .. This ratio is determined as the positive solution to the
quadratic equation
x2 ‚àíx ‚àí1 = 0.
Applying the quadratic formula to this equation yields two solutions,
x1 = 1 +
‚àö
5
2
= œï
and
x2 = 1 ‚àí
‚àö
5
2
= ‚àí1
œï .
You can learn more about the various contexts in which the golden
ratio appears from the Wikipedia article on the subject.
Explanations
Multiple solutions
Often, we are interested in only one of the two solutions to the
quadratic equation. It will usually be obvious from the context of
the problem which of the two solutions should be kept and which
should be discarded. For example, the time of Ô¨Çight of a ball thrown
in the air from a height of 3 metres with an initial velocity of 12 metres
per second is obtained by solving the equation (‚àí4.9)t2 +12t+3 = 0.
The two solutions of the quadratic equation are t1 = ‚àí0.229 and
t2 = 2.678. The Ô¨Årst answer t1 corresponds to a time in the past so
we reject it as invalid. The correct answer is t2. The ball will hit the
ground after t = 2.678 seconds.
Relation to factoring
In the previous section we discussed the quadratic factoring operation
by which we could rewrite a quadratic function as the product of two
terms f(x) = ax2 + bx + c = (x ‚àíx1)(x ‚àíx2). The two numbers x1

1.7
THE CARTESIAN PLANE
29
and x2 are called the roots of the function: these points are where the
function f(x) touches the x-axis.
You now have the ability to factor any quadratic equation. Use the
quadratic formula to Ô¨Ånd the two solutions, x1 and x2, then rewrite
the expression as (x ‚àíx1)(x ‚àíx2).
Some quadratic expressions cannot be factored, however. These
"unfactorable" expressions correspond to quadratic functions whose
graphs do not touch the x-axis. They have no solutions (no roots).
There is a quick test you can use to check if a quadratic function
f(x) = ax2 + bx + c has roots (touches or crosses the x-axis) or
doesn't have roots (never touches the x-axis). If b2 ‚àí4ac > 0 then
the function f has two roots. If b2 ‚àí4ac = 0, the function has only
one root, indicating the special case when the function touches the
x-axis at only one point. If b2 ‚àí4ac < 0, the function has no roots.
In this case the quadratic formula fails because it requires taking the
square root of a negative number, which is not allowed. Think about
it‚Äîhow could you square a number and obtain a negative number?
1.7
The Cartesian plane
Named after famous philosopher and mathematician Ren√© Descartes,
the Cartesian plane is a graphical representation for pairs of numbers.
Generally, we call the plane's horizontal axis "the x-axis" and its
vertical axis "the y-axis." We put notches at regular intervals on each
axis so we can measure distances.
Figure 1.2 is an example of an
empty Cartesian coordinate system. Think of the coordinate system
as an empty canvas. What can you draw on this canvas?
Vectors and points
A point P = (Px, Py) in the Cartesian plane has an x-coordinate and
a y-coordinate. To Ô¨Ånd this point, start from the origin‚Äîthe point
(0,0)‚Äîand move a distance Px on the x-axis, then move a distance
Py on the y-axis.
Similar to a point, a vector ‚Éóv = (vx, vy) is a pair of coordinates.
Unlike points, we don't necessarily start from the plane's origin when
mapping vectors.
We draw vectors as arrows that explicitly mark
where the vector starts and where it ends. Note that vectors ‚Éóv2 and
‚Éóv3 illustrated in Figure 1.3 are actually the same vector‚Äîthe "displace
left by 1 and down by 2" vector. It doesn't matter where you draw
this vector, it will always be the same whether it begins at the plane's
origin or elsewhere.

30
MATH FUNDAMENTALS
Figure 1.2:
The (x, y)-coordinate system, which is also known as the
Cartesian plane. Points P = (Px, Py), vectors ‚Éóv = (vx, vy), and graphs of
functions (x, f(x)) live here.
Figure 1.3: A Cartesian plane which shows the point P = (‚àí3, 2) and the
vectors ‚Éóv1 = (3, 1) and ‚Éóv2 = ‚Éóv3 = (‚àí1, ‚àí2).
Graphs of functions
The Cartesian plane is great for visualizing functions,
f : R ‚ÜíR.
You can think of a function as a set of input-output pairs (x, f(x)).
You can graph a function by letting the y-coordinate represent the
function's output value:
(x, y) = (x, f(x)).

1.7
THE CARTESIAN PLANE
31
For example, with the function f(x) = x2, we can pass a line through
the set of points
(x, y) = (x, x2),
and obtain the graph shown in Figure 1.4.
When plotting functions by setting y = f(x), we use a special
terminology for the two axes. The x-axis represents the independent
variable (the one that varies freely), and the y-axis represents the
dependent variable f(x), since f(x) depends on x.
Figure 1.4: The graph of the function f(x) = x2 consists of all pairs of
points (x, y) in the Cartesian plane that satisfy y = x2.
To draw the graph of any function f(x), use the following procedure.
Imagine making a sweep over all of the possible input values for the
function. For each input x, put a point at the coordinates (x, y) =
(x, f(x)) in the Cartesian plane. Using the graph of a function, you
can literally see what the function does: the "height" y of the graph
at a given x-coordinate tells you the value of the function f(x).
Discussion
To build mathematical intuition, it is essential you understand the
graphs of functions. Trying to memorize the deÔ¨Ånitions and the prop-
erties of functions is a diÔ¨Écult task. Remembering what the function
"looks like" is comparatively easier. You should spend some time fa-

32
MATH FUNDAMENTALS
miliarizing yourself with the graphs of the functions presented in the
next section.
1.8
Functions
We need to have a relationship talk. We need to talk about functions.
We use functions to describe the relationships between variables. In
particular, functions describe how one variable depends on another.
For example, the revenue R from a music concert depends on the
number of tickets sold n. If each ticket costs $25, the revenue from
the concert can be written as a function of n as follows: R(n) = 25n.
Solving for n in the equation R(n) = 7000 tells us the number of ticket
sales needed to generate $7000 in revenue. This is a simple model of
a function; as your knowledge of functions builds, you'll learn how
to build more detailed models of reality. For instance, if you need to
include a 5% processing charge for issuing the tickets, you can update
the revenue model to R(n) = 0.95 ¬∑ 25 ¬∑ n. If the estimated cost of
hosting the concert is C = $2000, then the proÔ¨Åt from the concert P
can be modelled as
P(n) = R(n) ‚àíC
= 0.95 ¬∑ $25 ¬∑ n ‚àí$2000
The function P(n) = 23.75n‚àí2000 models the proÔ¨Åt from the concert
as a function of the number of tickets sold. This is a pretty good model
already, and you can always update it later on as you Ô¨Ånd out more
information.
The more functions you know, the more tools you have for mod-
elling reality. To "know" a function, you must be able to understand
and connect several of its aspects. First you need to know the func-
tion's mathematical deÔ¨Ånition, which describes exactly what the
function does. Starting from the function's deÔ¨Ånition, you can use
your existing math skills to Ô¨Ånd the function's domain, its range, and
its inverse function. You must also know the graph of the function;
what the function looks like if you plot x versus f(x) in the Cartesian
plane (page 29). It's also a good idea to remember the values of the
function for some important inputs. Finally‚Äîand this is the part that
takes time‚Äîyou must learn about the function's relations to other
functions.

1.8
FUNCTIONS
33
DeÔ¨Ånitions
A function is a mathematical object that takes numbers as inputs and
gives numbers as outputs. We use the notation
f : A ‚ÜíB
to denote a function from the input set A to the output set B. In this
book, we mostly study functions that take real numbers as inputs and
give real numbers as outputs: f : R ‚ÜíR.
We now deÔ¨Åne some fancy technical terms used to describe the
input and output sets.
‚Ä¢ The domain of a function is the set of allowed input values.
‚Ä¢ The image or range of the function f is the set of all possible
output values of the function.
‚Ä¢ The codomain of a function describes the type of outputs the
function has.
To illustrate the subtle diÔ¨Äerence between the image of a function
and its codomain, consider the function f(x) = x2. The quadratic
function is of the form f : R ‚ÜíR. The function's domain is R (it
takes real numbers as inputs) and its codomain is R (the outputs are
real numbers too), however, not all outputs are possible. The image of
the function f(x) = x2 consists only of the nonnegative real numbers
[0, ‚àû‚â°{y ‚ààR | y ‚â•0}.
A function is not a number; rather, it is a mapping from numbers
to numbers. For any input x, the output value of f for that input is
denoted f(x).
Figure 1.5: An abstract representation of a function f from the set A to
the set B. The function f is the arrow which maps each input x in A to
an output f(x) in B. The output of the function f(x) is also denoted y.
We say "f maps x to f(x)," and use the following terminology to
classify the type of mapping that a function performs:

34
MATH FUNDAMENTALS
‚Ä¢ A function is one-to-one or injective if it maps diÔ¨Äerent inputs
to diÔ¨Äerent outputs.
‚Ä¢ A function is onto or surjective if it covers the entire output
set (in other words, if the image of the function is equal to the
function's codomain).
‚Ä¢ A function is bijective if it is both injective and surjective. In
this case, f is a one-to-one correspondence between the input
set and the output set: for each of the possible outputs y ‚ààY
(surjective part), there exists exactly one input x ‚ààX, such
that f(x) = y (injective part).
The term injective is an allusion from the 1940s inviting us to picture
the actions of injective functions as pipes through which numbers Ô¨Çow
like Ô¨Çuids. Since a Ô¨Çuid cannot be compressed, the output space must
be at least as large as the input space. A modern synonym for injective
functions is to say they are two-to-two. If we imagine two specks of
paint Ô¨Çoating around in the "input Ô¨Çuid," an injective function will
contain two distinct specks of paint in the "output Ô¨Çuid." In contrast,
non-injective functions can map several diÔ¨Äerent inputs to the same
output. For example f(x) = x2 is not injective since the inputs 2 and
‚àí2 are both mapped to the output value 4.
Function composition
We can combine two simple functions by
chaining them together to build a more
complicated function. This act of apply-
ing one function after another is called
function composition. Consider for exam-
ple the composition:
f ‚ó¶g (x) ‚â°f( g(x) ) = z.
The diagram on the right illustrates what
is going on. First, the function g : A ‚ÜíB acts on some input x
to produce an intermediary value y = g(x) in the set B. The inter-
mediary value y is then passed through the function f : B ‚ÜíC to
produce the Ô¨Ånal output value z = f(y) = f(g(x)) in the set C. We
can think of the composite function f ‚ó¶g as a function in its own
right. The function f ‚ó¶g : A ‚ÜíC is deÔ¨Åned through the formula
f ‚ó¶g (x) ‚â°f(g(x)).
Inverse function

1.8
FUNCTIONS
35
Recall that a bijective function is a
one-to-one correspondence between a
set of input values and a set of out-
put values. Given a bijective function
f : A ‚ÜíB, there exists an inverse
function f ‚àí1 : B ‚ÜíA, which per-
forms the inverse mapping of f. If you start from some x, apply f,
and then apply f ‚àí1, you'll arrive‚Äîfull circle‚Äîback to the original
input x:
f ‚àí1 f(x)

‚â°f ‚àí1‚ó¶f (x) = x.
This inverse function is represented abstractly as a backward arrow,
that puts the value f(x) back to the x it came from.
Function names
We use short symbols like +, ‚àí, √ó, and √∑ to denote most of the
important functions used in everyday life. We also use the weird surd
notation to denote nth root
n‚àöand superscripts to denote exponents.
All other functions are identiÔ¨Åed and denoted by their name.
If I
want to compute the cosine of the angle 60‚ó¶(a function describing
the ratio between the length of one side of a right-angle triangle and
the hypotenuse), I write cos(60‚ó¶), which means I want the value of
the cos function for the input 60‚ó¶.
Incidentally, the function cos has a nice output value for that spe-
ciÔ¨Åc angle: cos(60‚ó¶) ‚â°1
2. Therefore, seeing cos(60‚ó¶) somewhere in an
equation is the same as seeing 1
2. To Ô¨Ånd other values of the func-
tion, say cos(33.13‚ó¶), you'll need a calculator. A scientiÔ¨Åc calculator
features a convenient little cos button for this very purpose.
Handles on functions
When you learn about functions you learn about the diÔ¨Äerent "han-
dles" by which you can "grab" these mathematical objects. The main
handle for a function is its deÔ¨Ånition: it tells you the precise way to
calculate the output when you know the input. The function deÔ¨Åni-
tion is an important handle, but it is also important to "feel" what
the function does intuitively. How does one get a feel for a function?
Table of values
One simple way to represent a function is to look at a list of input-
output pairs:

{in = x1, out = f(x1)}, {in
=
x2, out = f(x2)},
{in = x3, out = f(x3)}, . . .
	
. A more compact notation for the input-
output pairs is {(x1, f(x1)), (x2, f(x2)), (x3, f(x3)), . . .}.
You can

36
MATH FUNDAMENTALS
make your own little table of values, pick some random inputs, and
record the output of the function in the second column:
input = x
‚Üí
f(x) = output
0
‚Üí
f(0)
1
‚Üí
f(1)
55
‚Üí
f(55)
x4
‚Üí
f(x4).
In addition to choosing random numbers for your table, it's also gen-
erally a good idea to check the function's values at x = 0, x = 1,
x = 100, x = ‚àí1, and any other important-looking x value.
Function graph
One of the best ways to feel a function is to look at its graph. A graph
is a line on a piece of paper that passes through all input-output pairs
of a function. Imagine you have a piece of paper, and on it you draw
a blank coordinate system as in Figure 1.6.
Figure 1.6: An empty (x, y)-coordinate system that you can use to plot
the graph of any function f(x). The graph of f(x) consists of all the points
for which (x, y) = (x, f(x)). See Figure 1.4 on page 31 for the graph of
f(x) = x2.
The horizontal axis, sometimes called the abscissa, is used to measure
x. The vertical axis is used to measure f(x). Because writing out f(x)
every time is long and tedious, we use a short, single-letter alias to
denote the output value of f as follows:
y ‚â°f(x) = output.

1.8
FUNCTIONS
37
Think of each input-output pair of the function f as a point (x, y) in
the coordinate system. The graph of a function is a representational
drawing of everything the function does. If you understand how to
interpret this drawing, you can infer everything there is to know about
the function.
Facts and properties
Another way to feel a function is by knowing the function's properties.
This approach boils down to learning facts about the function and its
relation to other functions.
An example of a mathematical fact is
sin(30‚ó¶) = 1
2. An example of a mathematical relation is the equation
sin2 x + cos2 x = 1, which indicates a link between the sin function
and the cos function.
The more you know about a function, the more "paths" your brain
builds to connect to that function. Real math knowledge is not mem-
orization; it requires establishing a graph of associations between dif-
ferent areas of information in your brain. Each concept is a node in
this graph, and each fact you know about this concept is an edge.
Mathematical thought is the usage of this graph to produce calcu-
lations and mathematical arguments called proofs. For example, by
connecting your knowledge of the fact sin(30‚ó¶) = 1
2 with the relation
sin2 x + cos2 x = 1, you can show that cos(30‚ó¶) =
‚àö
3
2 .
Note the
notation sin2(x) means (sin(x))2.
To develop mathematical skills, it is vital to practice path-building
between related concepts by solving exercises and reading and writing
mathematical proofs. With this book, I will introduce you to many
paths between concepts; it's up to you to reinforce these by using
what you've learned to solve problems.
Example
Consider the function f from the real numbers to the real numbers
(f : R ‚ÜíR) deÔ¨Åned by the quadratic expression
f(x) = x2 + 2x + 3.
The value of f when x = 1 is f(1) = 12 + 2(1) + 3 = 1 + 2 + 3 = 6.
When x = 2, the output is f(2) = 22 + 2(2) + 3 = 4 + 4 + 3 = 11.
What is the value of f when x = 0?
Example 2
Consider the exponential function with base 2:
f(x) = 2x.

38
MATH FUNDAMENTALS
This function is crucial to computer systems.
For instance, RAM
memory chips come in powers of two because the memory space is
exponential in the number of "address lines" used on the chip. When
x = 1, f(1) = 21 = 2. When x is 2 we have f(2) = 22 = 4. The
function is therefore described by the following input-output pairs:
(0, 1), (1, 2), (2, 4), (3, 8), (4, 16), (5, 32), (6, 64), (7, 128), (8, 256),
(9, 512), (10, 1024), (11, 2048), (12, 4096), etc. Recall that any number
raised to exponent 0 gives 1. Thus, the exponential function passes
through the point (0, 1). Recall also that negative exponents lead to
fractions: (‚àí1, 1
21 = 1
2), (‚àí2, 1
22 = 1
4), (‚àí3, 1
23 = 1
8), etc.
Discussion
In this section we talked a lot about functions in general, but we
haven't said much about any function speciÔ¨Åcally. There are many
useful functions out there, and we can't discuss them all here. In the
next section, we'll introduce 10 functions of strategic importance for
all of science. If you get a grip on these functions, you'll be able to
understand all of physics and calculus and handle any problem your
teacher may throw at you.
1.9
Function reference
Your function vocabulary determines how well you can express your-
self mathematically in the same way that your English vocabulary
determines how well you can express yourself in English. The follow-
ing pages aim to embiggen your function vocabulary so you won't be
caught with your pants down when the teacher tries to pull some trick
on you at the Ô¨Ånal.
If you are seeing these functions for the Ô¨Årst time, don't worry
about remembering all the facts and properties on the Ô¨Årst reading.
We will use these functions throughout the rest of the book so you will
have plenty of time to become familiar with them. Just remember to
come back to this section if you ever get stuck on a function.
Line
The equation of a line describes an input-output relationship where
the change in the output is proportional to the change in the input.
The equation of a line is
f(x) = mx + b.
The constant m describes the slope of the line. The constant b is
called the y-intercept and it corresponds to the value of the function

1.9
FUNCTION REFERENCE
39
when x = 0.
Graph
Figure 1.7: The graph of the function f(x) = 2x ‚àí3. The slope is m = 2.
The y-intercept of this line is at y = ‚àí3. The x-intercept is at x = 3
2.
Properties
‚Ä¢ Domain: x ‚ààR.
The function f(x) = mx+b is deÔ¨Åned for all input values x ‚ààR.
‚Ä¢ Image: x ‚ààR if m Ã∏= 0.
If m = 0 the function is constant
f(x) = b, so the image set contains only a single number {b}.
‚Ä¢ b/m: the x-intercept of f(x) = mx + b.
The x-intercept is
obtained by solving f(x) = 0.
‚Ä¢ A unique line passes through any two points (x1, y1) and (x2, y2)
if x1 Ã∏= x2.
‚Ä¢ The inverse to the line f(x) = mx + b is f ‚àí1(x) =
1
m(x ‚àíb),
which is also a line.
General equation
A line can also be described in a more symmetric form as a relation:
Ax + By = C.
This is known as the general equation of a line. The general equation
for the line shown in Figure 1.7 is 2x ‚àí1y = 3.
Given the general equation of a line Ax+By = C, you can convert
to the function form y = f(x) = mx + b using b = C
B and m = ‚àíA
B .

40
MATH FUNDAMENTALS
Square
The function x squared, is also called the quadratic function, or parabola.
The formula for the quadratic function is
f(x) = x2.
The name "quadratic" comes from the Latin quadratus for square,
since the expression for the area of a square with side length x is x2.
Figure 1.8: Plot of the quadratic function f(x) = x2. The graph of the
function passes through the following (x, y) coordinates: (‚àí2, 4), (‚àí1, 1),
(0, 0), (1, 1), (2, 4), (3, 9), etc.
Properties
‚Ä¢ Domain: x ‚ààR.
The function f(x) = x2 is deÔ¨Åned for all input values x ‚ààR.
‚Ä¢ Image: f(x) ‚àà[0, ‚àû.
The outputs are never negative: x2 ‚â•0, for all x ‚ààR.
‚Ä¢ The function x2 is the inverse of the square root function ‚àöx.
‚Ä¢ f(x) = x2 is two-to-one: it sends both x and ‚àíx to the same
output value x2 = (‚àíx)2.
‚Ä¢ The quadratic function is convex, meaning it curves upward.

1.9
FUNCTION REFERENCE
41
Square root
The square root function is denoted
f(x) = ‚àöx ‚â°x
1
2 .
The square root ‚àöx is the inverse function of the square function x2
for x ‚â•0. The symbol ‚àöc refers to the positive solution of x2 = c.
Note that ‚àí‚àöc is also a solution of x2 = c.
Graph
Figure 1.9: The graph of the function f(x) = ‚àöx. The domain of the
function is x ‚àà[0, ‚àû. You can't take the square root of a negative number.
Properties
‚Ä¢ Domain: x ‚àà[0, ‚àû.
The function f(x) = ‚àöx is only deÔ¨Åned for nonnegative inputs
x ‚â•0. There is no real number y such that y2 is negative, hence
the function f(x) = ‚àöx is not deÔ¨Åned for negative inputs x.
‚Ä¢ Image: f(x) ‚àà[0, ‚àû.
The outputs of the function f(x) = ‚àöx are never negative:
‚àöx ‚â•0, for all x ‚àà[0, ‚àû.
In addition to square root, there is also cube root f(x) =
3‚àöx ‚â°x
1
3 ,
which is the inverse function for the cubic function f(x) = x3. We
have
3‚àö
8 = 2 since 2 √ó 2 √ó 2 = 8. More generally, we can deÔ¨Åne the
nth-root function
n‚àöx as the inverse function of xn.

42
MATH FUNDAMENTALS
Absolute value
The absolute value function tells us the size of numbers without pay-
ing attention to whether the number is positive or negative. We can
compute a number's absolute value by ignoring the sign of the input
number. Thus, a number's absolute value corresponds to its distance
from the origin of the number line.
Another way of thinking about the absolute value function is to
say it multiplies negative numbers by ‚àí1 to "cancel" their negative
sign:
f(x) = |x| =

x
if x ‚â•0,
‚àíx
if x < 0.
Graph
Figure 1.10: The graph of the absolute value function f(x) = |x|.
Properties
‚Ä¢ Always returns a non-negative number
‚Ä¢ The combination of squaring followed by square-root is equiva-
lent to the absolute value function:
‚àö
x2 ‚â°|x|,
since squaring destroys the sign.

1.9
FUNCTION REFERENCE
43
Polynomial functions
The general equation for a polynomial function of degree n is written,
f(x) = a0 + a1x + a2x2 + a3x3 + ¬∑ ¬∑ ¬∑ + anxn.
The constants ai are known as the coeÔ¨Écients of the polynomial.
Parameters
‚Ä¢ n: the degree of the polynomial
‚Ä¢ a0: the constant term
‚Ä¢ a1: the linear coeÔ¨Écient, or Ô¨Årst-order coeÔ¨Écient
‚Ä¢ a2: the quadratic coeÔ¨Écient
‚Ä¢ a3: the cubic coeÔ¨Écient
‚Ä¢ an: the nth order coeÔ¨Écient
A polynomial of degree n has n + 1 coeÔ¨Écients: a0, a1, a2, . . . , an.
Properties
‚Ä¢ Domain: x ‚ààR. Polynomials are deÔ¨Åned for all inputs x ‚ààR.
‚Ä¢ Image: depends on the coeÔ¨Écients
‚Ä¢ The sum of two polynomials is also a polynomial.
Even and odd functions
The polynomials form an entire family of functions. Depending on the
choice of degree n and coeÔ¨Écients a0, a1, . . ., an, a polynomial func-
tion can take on many diÔ¨Äerent shapes. We'll study polynomials and
their properties in more detail in Section 1.10, but for now consider
the following observations about the symmetries of polynomials:
‚Ä¢ If a polynomial contains only even powers of x, like f(x) =
1 + x2 ‚àíx4 for example, we call this polynomial even. Even
polynomials have the property f(x) = f(‚àíx). The sign of the
input doesn't matter.
‚Ä¢ If a polynomial contains only odd powers of x, for example
g(x) = x + x3 ‚àíx9, we call this polynomial odd. Odd poly-
nomials have the property g(x) = ‚àíg(‚àíx).
‚Ä¢ If a polynomial has both even and odd terms then it is neither
even nor odd.
The terminology of odd and even applies to functions in general and
not just to polynomials. All functions that satisfy f(x) = f(‚àíx) are
called even functions, and all functions that satisfy f(x) = ‚àíf(‚àíx)
are called odd functions.

44
MATH FUNDAMENTALS
Sine
The sine function represents a fundamental unit of vibration. The
graph of sin(x) oscillates up and down and crosses the x-axis multiple
times. The shape of the graph of sin(x) corresponds to the shape of
a vibrating string. See Figure 1.11.
In the remainder of this book, we'll meet the function sin(x) many
times. We will deÔ¨Åne the function sin(x) more formally as a trigono-
metric ratio in Section 1.11 (page 58). In Chapter 2 we will use sin(x)
and cos(x) (another trigonometric ratio) to work out the components
of vectors. Later in Chapter ??, we will learn how the sine function
can be used to describe waves and periodic motion.
At this point in the book, however, we don't want to go into too
much detail about all these applications. Let's hold oÔ¨Äthe discussion
about vectors, triangles, angles, and ratios of lengths of sides and
instead just focus on the graph of the function f(x) = sin(x).
Graph
Figure 1.11: The graph of the function y = sin(x) passes through the fol-
lowing (x, y) coordinates: (0, 0), ( œÄ
6 , 1
2), ( œÄ
4 ,
‚àö
2
2 ), ( œÄ
3 ,
‚àö
3
2 ), ( œÄ
2 , 1), ( 2œÄ
3 ,
‚àö
3
2 ),
( 3œÄ
4 ,
‚àö
2
2 ), ( 5œÄ
6 , 1
2), and (œÄ, 0). For x ‚àà[œÄ, 2œÄ] the function has the same
shape as for x ‚àà[0, œÄ] but with negative values.
Let's start at x = 0 and follow the graph of
the function sin(x) as it goes up and down.
The graph starts from (0, 0) and smoothly in-
creases until it reaches the maximum value at
x = œÄ
2 .
Afterward, the function comes back
down to cross the x-axis at x = œÄ. After œÄ, the
function drops below the x-axis and reaches its

1.9
FUNCTION REFERENCE
45
minimum value of ‚àí1 at x = 3œÄ
2 . It then travels up again to cross the
x-axis at x = 2œÄ. This 2œÄ-long cycle repeats after x = 2œÄ. This is
why we call the function periodic‚Äîthe shape of the graph repeats.
Figure 1.12: The graph of sin(x) from x = 0 to x = 2œÄ repeats periodically
everywhere else on the number line.
Properties
‚Ä¢ Domain: x ‚ààR.
The function f(x) = sin(x) is deÔ¨Åned for all input values x ‚ààR.
‚Ä¢ Image: sin(x) ‚àà[‚àí1, 1].
The outputs of the sine function are always between ‚àí1 and 1.
‚Ä¢ Roots: [. . . , ‚àí3œÄ, ‚àí2œÄ, ‚àíœÄ, 0, œÄ, 2œÄ, 3œÄ, . . .].
The function sin(x) has roots at all multiples of œÄ.
‚Ä¢ The function is periodic, with period 2œÄ: sin(x) = sin(x + 2œÄ).
‚Ä¢ The sin function is odd: sin(x) = ‚àísin(‚àíx).
‚Ä¢ Relation to cos: sin2 x + cos2 x = 1
‚Ä¢ Relation to csc: csc(x) ‚â°
1
sin x (csc is read cosecant)
‚Ä¢ The inverse function of sin(x) is denoted as sin‚àí1(x), not to
be confused with (sin(x))‚àí1 =
1
sin(x) ‚â°csc(x). Sometimes the
function sin‚àí1(x) is denoted "arcsin(x)."
‚Ä¢ The number sin(Œ∏) is the length-ratio of the vertical side and the
hypotenuse in a right-angle triangle with angle Œ∏ at the base.
Links
[ See the Wikipedia page for nice illustrations ]
http://en.wikipedia.org/wiki/Sine

46
MATH FUNDAMENTALS
Cosine
The cosine function is the same as the sine function shifted by œÄ
2 to
the left: cos(x) = sin(x + œÄ
2 ). Thus everything you know about the
sine function also applies to the cosine function.
Graph
Figure 1.13: The graph of the function y = cos(x) passes through the fol-
lowing (x, y) coordinates: (0, 1), ( œÄ
6 ,
‚àö
3
2 ), ( œÄ
4 ,
‚àö
2
2 ), ( œÄ
3 , 1
2), ( œÄ
2 , 0), ( 2œÄ
3 , ‚àí1
2),
( 3œÄ
4 , ‚àí
‚àö
2
2 ), ( 5œÄ
6 , ‚àí
‚àö
3
2 ), and (œÄ, ‚àí1).
The cos function starts at cos(0) = 1, then drops down to cross the
x-axis at x = œÄ
2 . Cos continues until it reaches its minimum value at
x = œÄ. The function then moves upward, crossing the x-axis again at
x = 3œÄ
2 , and reaching its maximum value at x = 2œÄ.
Properties
‚Ä¢ Domain: x ‚ààR
‚Ä¢ Image: cos(x) ‚àà[‚àí1, 1]
‚Ä¢ Roots: [ . . . , ‚àí3œÄ
2 , ‚àíœÄ
2 , œÄ
2 , 3œÄ
2 , 5œÄ
2 , . . . ].
‚Ä¢ Relation to sin: sin2 x + cos2 x = 1
‚Ä¢ Relation to sec: sec(x) ‚â°
1
cos x (sec is read secant)
‚Ä¢ The inverse function of cos(x) is denoted cos‚àí1(x).
‚Ä¢ The cos function is even: cos(x) = cos(‚àíx).
‚Ä¢ The number cos(Œ∏) is the length-ratio of the horizontal side and
the hypotenuse in a right-angle triangle with angle Œ∏ at the base.

1.9
FUNCTION REFERENCE
47
Tangent
The tangent function is the ratio of the sine and cosine functions:
f(x) = tan(x) ‚â°sin(x)
cos(x).
Graph
Figure 1.14: The graph of the function f(x) = tan(x).
Properties
‚Ä¢ Domain: {x ‚ààR | x Ã∏= (2n+1)œÄ
2
for any n ‚ààZ}.
‚Ä¢ Range: x ‚ààR.
‚Ä¢ The function tan is periodic with period œÄ.
‚Ä¢ The tan function "blows up" at values of x where cos x = 0.
These are called asymptotes of the function and their locations
are x = . . . , ‚àí3œÄ
2 , ‚àíœÄ
2 , œÄ
2 , 3œÄ
2 , . . ..
‚Ä¢ Value at x = 0: tan(0) = 0
1 = 0, because sin(0) = 0.
‚Ä¢ Value at x = œÄ
4 : tan
  œÄ
4

= sin( œÄ
4 )
cos( œÄ
4 ) =
‚àö
2
2
‚àö
2
2
= 1.
‚Ä¢ The number tan(Œ∏) is the length-ratio of the vertical and the
horizontal sides in a right-angle triangle with angle Œ∏.
‚Ä¢ The inverse function of tan(x) is tan‚àí1(x).
‚Ä¢ The inverse tangent function is used to compute the angle at
the base in a right-angle triangle with horizontal side length ‚Ñìh
and vertical side length ‚Ñìv: Œ∏ = tan‚àí1
‚Ñìv
‚Ñìh

.

48
MATH FUNDAMENTALS
Exponential
The exponential function base e = 2.7182818 . . . is denoted
f(x) = ex ‚â°exp(x).
Graph
Figure 1.15:
The graph of the exponential function f(x) = ex passes
through the following (x, y) coordinates: (‚àí2, 1
e2 ), (‚àí1, 1
e), (0, 1), (1, e),
(2, e2), (3, e3 = 20.08 . . .), (5, 148.41 . . .), and (10, 22026.46 . . .).
Properties
‚Ä¢ Domain: x ‚ààR
‚Ä¢ Range: ex ‚àà(0, ‚àû
‚Ä¢ f(a)f(b) = f(a + b) since eaeb = ea+b.
‚Ä¢ The derivative (the slope of the graph) of the exponential func-
tion is the exponential function: f(x) = ex
‚áí
f ‚Ä≤(x) = ex.
A more general exponential function would be f(x) = AeŒ≥x, where
A is the initial value, and Œ≥ (the Greek letter gamma) is the rate of
the exponential. For Œ≥ > 0, the function f(x) is increasing, as in
Figure 1.15. For Œ≥ < 0, the function is decreasing and tends to zero
for large values of x. The case Œ≥ = 0 is special since e0 = 1, so f(x)
is a constant of f(x) = A1x = A.
Links
[ The exponential function 2x evaluated ]
http://www.youtube.com/watch?v=e4MSN6IImpI

1.9
FUNCTION REFERENCE
49
Natural logarithm
The natural logarithm function is denoted
f(x) = ln(x) = loge(x).
The function ln(x) is the inverse function of the exponential ex.
Graph
Figure 1.16:
The graph of the function ln(x) passes through the fol-
lowing (x, y) coordinates: ( 1
e2 , ‚àí2), ( 1
e, ‚àí1), (1, 0), (e, 1), (e2, 2), (e3, 3),
(148.41 . . . , 5), and (22026.46 . . . , 10).
Function transformations
Often, we're asked to adjust the shape of a function by scaling it or
moving it, so that it passes through certain points. For example, if
we wanted to make a function g with the same shape as the absolute
value function f(x) = |x|, but for which g(0) = 3, we would use the
function g(x) = |x| + 3.
In this section, we'll discuss the four basic transformations you
can perform on any function f to obtain a transformed function g:
‚Ä¢ Vertical translation: g(x) = f(x) + k
‚Ä¢ Horizontal translation: g(x) = f(x ‚àíh)
‚Ä¢ Vertical scaling: g(x) = Af(x)
‚Ä¢ Horizontal scaling: g(x) = f(ax)

50
MATH FUNDAMENTALS
By applying these transformations, we can move and stretch a generic
function to give it any desired shape.
The next couple of pages
illustrate all of the above
transformations on the func-
tion
f(x) = 6.75(x3 ‚àí2x2 + x).
We'll work with this func-
tion because it has distinc-
tive features in both the
horizontal and vertical di-
rections. By observing this
function's graph, we see its
x-intercepts are at x = 0
and x = 1.
We can conÔ¨Årm this mathematically by factoring the
expression:
f(x) = 6.75x(x2 ‚àí2x + 1) = 6.75x(x ‚àí1)2.
The function f(x) also has a local maximum at x = 1
3, and the value
of the function at that maximum is f( 1
3) = 1.
Vertical translations
To move a function f(x) up by k units,
add k to the function:
g(x) = f(x) + k.
The function g(x) will have exactly the
same shape as f(x), but it will be
translated (the mathematical term for
moved) upward by k units.
Recall the function f(x) = 6.75(x3‚àí
2x2 + x). To move the function up by
k = 2 units, we can write
g(x) = f(x)+2 = 6.75(x3‚àí2x2+x)+2,
and the graph of g(x) will be as it is shown to the right. Recall the
original function f(x) crosses the x-axis at x = 0. The transformed
function g(x) has the property g(0) = 2. The maximum at x = 1
3 has
similarly shifted in value from f( 1
3) = 1 to g( 1
3) = 3.

1.9
FUNCTION REFERENCE
51
Horizontal translation
We can move a function f to the
right by h units by subtracting h
from x and using (x ‚àíh) as the
function's input argument:
g(x) = f(x ‚àíh).
The
point
(0, f(0))
on
f(x)
now corresponds to the point
(h, g(h)) on g(x).
The graph to the right shows
the function f(x) = 6.75(x3 ‚àí
2x2 + x), as well as the function
g(x), which is shifted to the right
by h = 2 units:
g(x) = f(x ‚àí2) = 6.75

(x ‚àí2)3 ‚àí2(x ‚àí2)2 + (x ‚àí2)

.
The original function f gives us f(0) = 0 and f(1) = 0, so the new
function g(x) must give g(2) = 0 and g(3) = 0. The maximum at
x = 1
3 has similarly shifted by two units to the right, g(2 + 1
3) = 1.
Vertical scaling
To stretch or compress the shape of a function vertically, we can
multiply it by some constant A and obtain
g(x) = Af(x).
If |A| > 1, the function will be
stretched. If |A| < 1, the func-
tion will be compressed.
If A
is negative, the function will Ô¨Çip
upside down, which is a reÔ¨Çec-
tion through the x-axis.
There is an important diÔ¨Äer-
ence between vertical translation
and vertical scaling. Translation
moves all points of the function
by the same amount, whereas
scaling moves each point propor-
tionally to that point's distance
from the x-axis.

52
MATH FUNDAMENTALS
The function f(x) = 6.75(x3 ‚àí2x2 + x), when stretched vertically
by a factor of A = 2, becomes the function
g(x) = 2f(x) = 13.5(x3 ‚àí2x2 + x).
The x-intercepts f(0) = 0 and f(1) = 0 do not move, and remain at
g(0) = 0 and g(1) = 0. The maximum at x = 1
3 has doubled in value
as g( 1
3) = 2. Indeed, all values of f(x) have been stretched upward
by a factor of 2, as we can verify using the point f(1.5) = 2.5, which
has become g(1.5) = 5.
Horizontal scaling
To stretch or compress a function horizontally, we can multiply the
input value by some constant a to obtain:
g(x) = f(ax).
If |a| > 1, the function will be compressed. If |a| < 1, the function
will be stretched. Note that the behaviour here is the opposite of
vertical scaling. If a is a negative number, the function will also Ô¨Çip
horizontally, which is a reÔ¨Çection through the y-axis.
The graph on the right shows
f(x) = 6.75(x3 ‚àí2x2 + x), as
well as the function g(x), which
is f(x) compressed horizontally
by a factor of a = 2:
g(x) = f(2x)
= 6.75

(2x)3 ‚àí2(2x)2 + (2x)

.
The x-intercept f(0) = 0 does
not move since it is on the y-axis.
The x-intercept f(1) = 0 does
move, however, and we have g(0.5) = 0. The maximum at x =
1
3
moves to g( 1
6) = 1. All values of f(x) are compressed toward the
y-axis by a factor of 2.
General quadratic function
The general quadratic function takes the form
f(x) = A(x ‚àíh)2 + k,
where x is the input, and A, h, and k are the parameters.

1.9
FUNCTION REFERENCE
53
Parameters
‚Ä¢ A: the slope multiplier
‚ñ∑The larger the absolute value of A, the steeper the slope.
‚ñ∑If A < 0 (negative), the function opens downward.
‚Ä¢ h: the horizontal displacement of the function.
Notice that
subtracting a number inside the bracket ( )2 (positive h) makes
the function go to the right.
‚Ä¢ k: the vertical displacement of the function
Graph
The graph in Figure 1.17 illustrates a quadratic function with param-
eters A = 1, h = 1 (one unit shifted to the right), and k = ‚àí2 (two
units shifted down).
Figure 1.17: The graph of the function f(x) = (x ‚àí1)2 ‚àí2 is the same
as the basic function f(x) = x2, but shifted one unit to the right and two
units down.
If a quadratic crosses the x-axis, it can be written in factored form:
f(x) = A(x ‚àía)(x ‚àíb),
where a and b are the two roots. Another common way of writing a
quadratic function is f(x) = Ax2 + Bx + C.
Properties
‚Ä¢ There is a unique quadratic function that passes through any
three points (x1, y1), (x2, y2) and (x3, y3), if the points have
diÔ¨Äerent x-coordinates: x1 Ã∏= x2, x2 Ã∏= x3, and x1 Ã∏= x3.
‚Ä¢ The derivative of f(x) = Ax2 + Bx + C is f ‚Ä≤(x) = 2Ax + B.

54
MATH FUNDAMENTALS
General sine function
Introducing all possible parameters into the sine function gives us:
f(x) = A sin
  2œÄ
Œª x ‚àíœÜ

,
where A, Œª, and œÜ are the function's parameters.
Parameters
‚Ä¢ A: the amplitude describes the distance above and below the
x-axis that the function reaches as it oscillates.
‚Ä¢ Œª: the wavelength of the function:
Œª ‚â°{ the distance from one peak to the next }.
‚Ä¢ œÜ: is a phase shift, analogous to the horizontal shift h, which we
have seen. This number dictates where the oscillation starts.
The default sine function has zero phase shift (œÜ = 0), so it
passes through the origin with an increasing slope.
The "bare" sin function f(x) = sin(x) has wavelength 2œÄ and produces
outputs that oscillate between ‚àí1 and +1. When we multiply the bare
function by the constant A, the oscillations will range between ‚àíA
and A. When the input x is scaled by the factor 2œÄ
Œª , the wavelength
of the function becomes Œª.
1.10
Polynomials
The polynomials are a simple and useful family of functions.
For
example, quadratic polynomials of the form f(x) = ax2 +bx+c often
arise when describing physics phenomena.
DeÔ¨Ånitions
‚Ä¢ x: the variable
‚Ä¢ f(x): the polynomial. We sometimes denote polynomials P(x)
to distinguish them from generic function f(x).
‚Ä¢ Degree of f(x): the largest power of x that appears in the poly-
nomial
‚Ä¢ Roots of f(x): the values of x for which f(x) = 0
The most general Ô¨Årst-degree polynomial is a line f(x) = mx + b,
where m and b are arbitrary constants. The most general second-
degree polynomial is f(x) = a2x2 + a1x + a0, where again a0, a1, and

1.10
POLYNOMIALS
55
a2 are arbitrary constants. We call ak the coeÔ¨Écient of xk, since this
is the number that appears in front of xk. Following the pattern, a
third-degree polynomial will look like f(x) = a3x3 + a2x2 + a1x + a0.
In general, a polynomial of degree n has the equation
f(x) = anxn + an‚àí1xn‚àí1 + ¬∑ ¬∑ ¬∑ + a2x2 + a1x + a0.
Or, if we use summation notation, we can write the polynomial as
f(x) =
n
X
k=0
akxk.
The symbol Œ£ (the Greek letter sigma) stands for summation.
Solving polynomial equations
Very often in math, you will have to solve polynomial equations of
the form
A(x) = B(x),
where A(x) and B(x) are both polynomials. Recall from earlier that
to solve, we must Ô¨Ånd the value of x that makes the equality true.
Say the revenue of your company is a function of the number of
products sold x, and can be expressed as R(x) = 2x2 + 2x. Say also
the cost you incur to produce x objects is C(x) = x2 + 5x + 10. You
want to determine the amount of product you need to produce to
break even, that is, so that revenue equals cost: R(x) = C(x). To
Ô¨Ånd the break-even value x, solve the equation
2x2 + 2x = x2 + 5x + 10.
This may seem complicated since there are xs all over the place. No
worries! We can turn the equation into its "standard form," and then
use the quadratic formula. First, move all the terms to one side until
only zero remains on the other side:
2x2 + 2x
‚àíx2 = x2 + 5x + 10
‚àíx2
x2 + 2x
‚àí5x = 5x + 10
‚àí5x
x2 ‚àí3x
‚àí10 = 10
‚àí10
x2 ‚àí3x ‚àí10 = 0.
Remember, if we perform the same operations on both sides of the
equation, the equation remains true. Therefore, the values of x that
satisfy
x2 ‚àí3x ‚àí10 = 0,

56
MATH FUNDAMENTALS
namely x = ‚àí2 and x = 5, also satisfy
2x2 + 2x = x2 + 5x + 10,
which is the original problem we're trying to solve.
This "shuÔ¨Ñing of terms" approach will work for any polynomial
equation A(x) = B(x). We can always rewrite it as C(x) = 0, where
C(x) is a new polynomial with coeÔ¨Écients equal to the diÔ¨Äerence of
the coeÔ¨Écients of A and B. Don't worry about which side you move
all the coeÔ¨Écients to because C(x) = 0 and 0 = ‚àíC(x) have exactly
the same solutions. Furthermore, the degree of the polynomial C can
be no greater than that of A or B.
The form C(x) = 0 is the standard form of a polynomial, and we'll
explore several formulas you can use to Ô¨Ånd its solution(s).
Formulas
The formula for solving the polynomial equation P(x) = 0 depends
on the degree of the polynomial in question.
First
For a Ô¨Årst-degree polynomial equation, P1(x) = mx + b = 0, the
solution is x = ‚àíb
m : just move b to the other side and divide by m.
Second
For a second-degree polynomial,
P2(x) = ax2 + bx + c = 0,
the solutions are x1 = ‚àíb+
‚àö
b2‚àí4ac
2a
and x2 = ‚àíb‚àí
‚àö
b2‚àí4ac
2a
.
If b2 ‚àí4ac < 0, the solutions will involve taking the square root
of a negative number. In those cases, we say no real solutions exist.
Higher degrees
There is also a formula for polynomials of degree 3, but it is compli-
cated. For polynomials with order ‚â•5, there does not exist a general
analytical solution.
Using a computer
When solving real-world problems, you'll often run into much more
complicated equations. To Ô¨Ånd the solutions of anything more com-
plicated than the quadratic equation, I recommend using a computer
algebra system like SymPy: http://live.sympy.org.

1.10
POLYNOMIALS
57
To make the computer solve the equation x2 ‚àí3x + 2 = 0 for you,
type in the following:
>>> solve( x**2 - 3*x +2, x)
# usage: solve(expr, var)
[1, 2]
The function solve will Ô¨Ånd the roots of any equation of the form
expr = 0. Indeed, we can verify that x2 ‚àí3x + 2 = (x ‚àí1)(x ‚àí2),
so x = 1 and x = 2 are the two roots.
Substitution trick
Sometimes you can solve fourth-degree polynomials by using the quadratic
formula. Say you're asked to solve for x in
g(x) = x4 ‚àí3x2 ‚àí10 = 0.
Imagine this problem is on your exam, where you are not allowed the
use of a computer. How does the teacher expect you to solve for x?
The trick is to substitute y = x2 and rewrite the same equation as
g(y) = y2 ‚àí3y ‚àí10 = 0,
which you can solve by applying the quadratic formula. If you obtain
the solutions y = Œ± and y = Œ≤, then the solutions to the original
fourth-degree polynomial are x = ‚àöŒ± and x = ‚àöŒ≤, since y = x2.
Since we're not taking an exam right now, we are allowed to use
the computer to Ô¨Ånd the roots:
>>> solve(y**2 - 3*y -10, y)
[-2, 5]
>>> solve(x**4 - 3*x**2 -10 , x)
[sqrt(2)i, -sqrt(2)i, -sqrt(5) , sqrt(5) ]
Note how the second-degree polynomial has two roots, while the
fourth-degree polynomial has four roots (two of which are imaginary,
since we had to take the square root of a negative number to ob-
tain them). The imaginary roots contain the unit imaginary number
i ‚â°‚àö‚àí1.
If you see this kind of problem on an exam, you should report the
two real solutions as your answer‚Äîin this case ‚àí
‚àö
5 and
‚àö
5‚Äîwithout
mentioning the imaginary solutions because you are not supposed to
know about imaginary numbers yet. If you feel impatient and are
ready to know about the imaginary numbers right now, feel free to
skip ahead to the section on complex numbers (page 102).

58
MATH FUNDAMENTALS
1.11
Trigonometry
We can put any three lines together to make a triangle. What's more,
if one of the triangle's angles is equal to 90‚ó¶, we call this triangle a
right-angle triangle.
In this section we'll discuss right-angle triangles in great detail
and get to know their properties. We'll learn some fancy new terms
like hypotenuse, opposite, and adjacent, which are used to refer to the
diÔ¨Äerent sides of a triangle. We'll also use the functions sine, cosine,
and tangent to compute the ratios of lengths in right triangles.
Understanding triangles and their associated trigonometric func-
tions is of fundamental importance: you'll need this knowledge for
your future understanding of mathematical subjects like vectors and
complex numbers, as well as physics subjects like oscillations and
waves.
Figure 1.18: A right-angle triangle. The angle Œ∏ and the names of the
sides of the triangle are indicated.
Concepts
‚Ä¢ A, B, C: the three vertices of the triangle
‚Ä¢ Œ∏: the angle at the vertex C. Angles can be measured in degrees
or radians.
‚Ä¢ opp ‚â°AB: the length of the opposite side to Œ∏
‚Ä¢ adj ‚â°BC: the length of side adjacent to Œ∏
‚Ä¢ hyp ‚â°AC: the hypotenuse. This is the triangle's longest side.
‚Ä¢ h: the "height" of the triangle (in this case h = opp = AB)
‚Ä¢ sin Œ∏ ‚â°opp
hyp: the sine of theta is the ratio of the length of the
opposite side and the length of hypotenuse.
‚Ä¢ cos Œ∏ ‚â°
adj
hyp: the cosine of theta is the ratio of the adjacent
length and the hypotenuse length.
‚Ä¢ tan Œ∏ ‚â°
sin Œ∏
cos Œ∏ ‚â°
opp
adj : the tangent is the ratio of the opposite
length divided by the adjacent length.

1.11
TRIGONOMETRY
59
Pythagoras' theorem
In a right-angle triangle, the length of the hypotenuse squared is equal
to the sum of the squares of the lengths of the other sides:
|adj|2 + |opp|2 = |hyp|2.
If we divide both sides of the above equation by |hyp|2, we obtain
|adj|2
|hyp|2 + |opp|2
|hyp|2 = 1,
which can be rewritten as
cos2 Œ∏ + sin2 Œ∏ = 1.
This is a powerful trigonometric identity that describes an important
relationship between sin and cos.
Sin and cos
Meet the trigonometric functions, or trigs for short. These are your
new friends. Don't be shy now, say hello to them.
"Hello."
"Hi."
"Soooooo, you are like functions right?"
"Yep," sin and cos reply in chorus.
"Okay, so what do you do?"
"Who me?"
asks cos. "Well I tell the ratio. . . hmm. . . Wait, are
you asking what I do as a function or speciÔ¨Åcally what I do?"
"Both I guess?"
"Well, as a function, I take angles as inputs and I give ratios as
answers. More speciÔ¨Åcally, I tell you how 'wide' a triangle with that
angle will be," says cos all in one breath.
"What do you mean wide?" you ask.
"Oh yeah, I forgot to say, the triangle must have a hypotenuse of
length 1. What happens is there is a point P that moves around on
a circle of radius 1, and we imagine a triangle formed by the point
P, the origin, and the point on the x-axis located directly below the
point P."
"I am not sure I get it," you confess.
"Let me try explaining," says sin. "Look on the next page, and
you'll see a circle. This is the unit circle because it has a radius of 1.
You see it, yes?"
"Yes."
"This circle is really cool. Imagine a point P that starts from the
point P(0) = (1, 0) and moves along the circle of radius 1. The x and

60
MATH FUNDAMENTALS
y coordinates of the point P(Œ∏) = (Px(Œ∏), Py(Œ∏)) as a function of Œ∏
are
P(Œ∏) = (Px(Œ∏), Py(Œ∏)) = (cos Œ∏, sin Œ∏).
So, either you can think of us in the context of triangles, or you think
of us in the context of the unit circle."
"Cool. I kind of get it. Thanks so much," you say, but in reality
you are weirded out. Talking functions? "Well guys. It was nice to
meet you, but I have to get going, to Ô¨Ånish the rest of the book."
"See you later," says cos.
"Peace out," says sin.
The unit circle
The unit circle consists of all points (x, y) that satisfy the equation
x2 + y2 = 1. A point P = (Px, Py) on the unit circle has coordinates
(Px, Py) = (cos Œ∏, sin Œ∏), where Œ∏ is the angle P makes with the x-axis.
x
y
1
cos Œ∏
sin Œ∏
Œ∏
1
0
x
y
1
Œ∏
1
0
(Px, Py) = (cos Œ∏, sin Œ∏)
x2 + y2 = 1
Figure 1.19: The unit circle corresponds to the equation x2 +y2 = 1. The
coordinates of the point P on the unit circle are Px = cos Œ∏ and Py = sin Œ∏.
Œ∏
f(Œ∏)
0
1
œÄ
6
œÄ
3
œÄ
2
2œÄ
3
5œÄ
6
œÄ
f(Œ∏) = sin Œ∏
œÄ
6
œÄ
3
œÄ
2
1
1
x2 + y2 = 1
Œ∏
Figure 1.20: The function f(Œ∏) = sin Œ∏ describes the vertical position of a
point P that travels along the unit circle. The Ô¨Årst half of a cycle is shown.
You should be familiar with the values of sin and cos for all angles
that are multiples of œÄ
6 (30‚ó¶) or œÄ
4 (45‚ó¶). All of them are shown in

1.11
TRIGONOMETRY
61
Figure 1.21. For each angle, the x-coordinate (the Ô¨Årst number in the
bracket) is cos Œ∏, and the y-coordinate is sin Œ∏.
Figure 1.21: The unit circle. The coordinates of the point on the unit
circle (cos Œ∏, sin Œ∏) are indicated for several important values of the angle Œ∏.
Maybe you're thinking that's way too much to remember.
Don't
worry, you just have to memorize one fact:
sin(30‚ó¶) = sin
œÄ
6

= 1
2 .
Knowing this, you can determine all the other angles. Let's start with
cos(30‚ó¶). We know that at 30‚ó¶, point P on the unit circle has the
vertical coordinate 1
2 = sin(30‚ó¶). We also know the cos quantity we
are looking for is, by deÔ¨Ånition, the horizontal component:
P = (cos(30‚ó¶), sin(30‚ó¶)).
Key fact: all points on the unit circle are a distance of 1 from the
origin. Knowing that P is a point on the unit circle, and knowing the
value of sin(30‚ó¶), we can solve for cos(30‚ó¶). Start with the following
identity,
cos2 Œ∏ + sin2 Œ∏ = 1,
which is true for all angles Œ∏. Moving things around, we obtain
cos(30‚ó¶) =
q
1 ‚àísin2(30‚ó¶) =
r
1 ‚àí1
4 =
r
3
4 =
‚àö
3
2 .
To Ô¨Ånd the values of cos(60‚ó¶) and sin(60‚ó¶), observe the symmetry of
the circle. 60 degrees measured from the x-axis is the same as 30

62
MATH FUNDAMENTALS
degrees measured from the y-axis. From this, we know cos(60‚ó¶) =
sin(30‚ó¶) = 1
2. Therefore, sin(60‚ó¶) =
‚àö
3
2 .
To Ô¨Ånd the values of sin and cos for angles that are multiples of
45‚ó¶, we need to Ô¨Ånd the value a such that
a2 + a2 = 1,
since at 45‚ó¶, the horizontal and vertical coordinates will be the same.
Solving for a we Ô¨Ånd a =
1
‚àö
2, but people don't like to see square roots
in the denominator, so we write
‚àö
2
2
= cos(45‚ó¶) = sin(45‚ó¶).
All other angles in the circle behave like the three angles above, with
one diÔ¨Äerence: one or more of their components has a negative sign.
For example, 150‚ó¶is just like 30‚ó¶, except its x component is negative.
Don't memorize all the values of sin and cos; if you ever need to
determine their values, draw a little circle and use the symmetry of
the circle to Ô¨Ånd the sin and cos components.
Non-unit circles
Consider a point Q(Œ∏) at an angle of Œ∏ on a circle with radius r Ã∏= 1.
How can we Ô¨Ånd the x and y-coordinates of the point Q(Œ∏)?
We saw that the coeÔ¨Écients cos Œ∏ and sin Œ∏ correspond to the x
and y-coordinates of a point on the unit circle (r = 1). To obtain
the coordinates for a point on a circle of radius r, we must scale the
coordinates by a factor of r:
Q(Œ∏) = (Qx(Œ∏), Qy(Œ∏)) = (r cos Œ∏, r sin Œ∏).
The take-away message is that you can
use the functions cos Œ∏ and sin Œ∏ to Ô¨Ånd
the "horizontal" and "vertical" compo-
nents of any length r.
From this point on in the book,
we'll always talk about the length of
the adjacent side as rx = r cos Œ∏, and
the length of the opposite side as ry =
r sin Œ∏. It is extremely important you
get comfortable with this notation.
The reasoning behind the above calculations is as follows:
cos Œ∏ ‚â°adj
hyp = rx
r
‚áí
rx = r cos Œ∏,

1.12
TRIGONOMETRIC IDENTITIES
63
and
sin Œ∏ ‚â°opp
hyp = ry
r
‚áí
ry = r sin Œ∏.
Calculators
Make sure to set your calculator to the correct units for working with
angles. What should you type into your calculator to compute the
sine of 30 degrees? If your calculator is set to degrees, simply type:
30 , sin , = .
If your calculator is set to radians, you have two options:
1. Change the mode of the calculator so it works in degrees.
2. Convert 30‚ó¶to radians
30 [‚ó¶] √ó 2œÄ [rad]
360 [‚ó¶] = œÄ
6 [rad],
and type: œÄ , / , 6 , sin , = on your calculator.
Links
[ Unit-circle walkthrough and tricks by patrickJMT on YouTube ]
bit.ly/1mQg9Cj and bit.ly/1hvA702
1.12
Trigonometric identities
There are a number of important relationships between the values of
the functions sin and cos. Here are three of these relationships, known
as trigonometric identities. There about a dozen other identities that
are less important, but you should memorize these three.
The three identities to remember are:
1. Unit hypotenuse
sin2(Œ∏) + cos2(Œ∏) = 1.
The unit hypotenuse identity is true by the Pythagoras theorem and
the deÔ¨Ånitions of sin and cos. The sum of the squares of the sides of
a triangle is equal to the square of the hypotenuse.
2. sico + sico
sin(a + b) = sin(a) cos(b) + sin(b) cos(a).
The mnemonic for this identity is "sico + sico."

64
MATH FUNDAMENTALS
3. coco ‚àísisi
cos(a + b) = cos(a) cos(b) ‚àísin(a) sin(b).
The mnemonic for this identity is "coco - sisi." The negative sign is
there because it's not good to be a sissy.
Derived formulas
If you remember the above three formulas, you can derive pretty much
all the other trigonometric identities.
Double angle formulas
Starting from the sico-sico identity as explained above, and setting
a = b = x, we can derive the following identity:
sin(2x) = 2 sin(x) cos(x).
Starting from the coco-sisi identity, we obtain
cos(2x) = cos2(x) ‚àísin2(x)
= 2 cos2(x) ‚àí1 = 2
 1 ‚àísin2(x)

‚àí1 = 1 ‚àí2 sin2(x).
The formulas for expressing sin(2x) and cos(2x) in terms of sin(x)
and cos(x) are called double angle formulas.
If we rewrite the double-angle formula for cos(2x) to isolate the
sin2 or the cos2 term, we obtain the power-reduction formulas:
cos2(x) = 1
2 (1 + cos(2x)) ,
sin2(x) = 1
2 (1 ‚àícos(2x)) .
Self similarity
Sin and cos are periodic functions with period 2œÄ. Adding a multiple
of 2œÄ to the function's input does not change the function:
sin(x + 2œÄ) = sin(x + 124œÄ) = sin(x),
cos(x + 2œÄ) = cos(x).
Furthermore, sin and cos are self similar within each 2œÄ cycle:
sin(œÄ ‚àíx) = sin(x),
cos(œÄ ‚àíx) = ‚àícos(x).
Sin is cos, cos is sin
It shouldn't be surprising if I tell you that sin and cos are actually
œÄ
2 -shifted versions of each other:
cos(x) = sin

x+ œÄ
2

= sin
œÄ
2 ‚àíx

, sin(x) = cos

x‚àíœÄ
2

= cos
œÄ
2 ‚àíx

.

1.13
GEOMETRY
65
Sum formulas
sin(a) + sin(b) = 2 sin
1
2(a + b)

cos
1
2(a ‚àíb)

,
sin(a) ‚àísin(b) = 2 sin
1
2(a ‚àíb)

cos
1
2(a + b)

,
cos(a) + cos(b) = 2 cos
1
2(a + b)

cos
1
2(a ‚àíb)

,
cos(a) ‚àícos(b) = ‚àí2 sin
1
2(a + b)

sin
1
2(a ‚àíb)

.
Product formulas
sin(a) cos(b) = 1
2(sin (a + b) + sin (a ‚àíb)),
sin(a) sin(b) = 1
2(cos (a ‚àíb) ‚àícos (a + b)),
cos(a) cos(b) = 1
2(cos (a ‚àíb) + cos (a + b)).
Discussion
The above formulas will come in handy when you need to Ô¨Ånd some
unknown in an equation, or when you are trying to simplify a trigono-
metric expression. I am not saying you should necessarily memorize
them, but you should be aware that they exist.
1.13
Geometry
Triangles
The area of a triangle is equal to 1
2 times the
length of its base times its height:
A = 1
2aha.
Note that ha is the height of the triangle
relative to the side a.
The perimeter of a triangle is
P = a + b + c.

66
MATH FUNDAMENTALS
Consider a triangle with internal
angles Œ±, Œ≤ and Œ≥. The sum of
the inner angles in any triangle
is equal to two right angles: Œ± +
Œ≤ + Œ≥ = 180‚ó¶.
Sine rule
The sine law is
a
sin(Œ±) =
b
sin(Œ≤) =
c
sin(Œ≥),
where Œ± is the angle opposite to a, Œ≤ is the angle opposite to b, and
Œ≥ is the angle opposite to c.
Cosine rule
The cosine rules are
a2 = b2 + c2 ‚àí2bc cos(Œ±),
b2 = a2 + c2 ‚àí2ac cos(Œ≤),
c2 = a2 + b2 ‚àí2ab cos(Œ≥).
Sphere
A sphere is described by the equation
x2 + y2 + z2 = r2.
The surface area of a sphere is
A = 4œÄr2,
and the volume of a sphere is
V = 4
3œÄr3.
Cylinder
The surface area of a cylinder consists of the
top and bottom circular surfaces, plus the
area of the side of the cylinder:
A = 2
 œÄr2
+ (2œÄr)h.
The formula for the volume of a cylinder is
the product of the area of the cylinder's base
times its height:
V =
 œÄr2
h.

1.14
CIRCLE
67
Example
You open the hood of your car and see 2.0 L written on
top of the engine. The 2.0 L refers to the combined volume of the
four pistons, which are cylindrical in shape. The owner's manual tells
you the diameter of each piston (bore) is 87.5 mm, and the height of
each piston (stroke) is 83.1 mm. Verify that the total volume of the
cylinder displacement of your engine is indeed 1998789 mm3 ‚âà2 L.
1.14
Circle
The circle is a set of points located a constant distance from a centre
point. This geometrical shape appears in many situations.
DeÔ¨Ånitions
‚Ä¢ r: the radius of the circle
‚Ä¢ A: the area of the circle
‚Ä¢ C: the circumference of the circle
‚Ä¢ (x, y): a point on the circle
‚Ä¢ Œ∏: the angle (measured from the x-axis) of some point on the
circle
Formulas
A circle with radius r centred at the origin is described by the equation
x2 + y2 = r2.
All points (x, y) that satisfy this equation are part of the circle.
Rather than staying centred at the ori-
gin, the circle's centre can be located at any
point (p, q) on the plane:
(x ‚àíp)2 + (y ‚àíq)2 = r2.
Explicit function
The equation of a circle is a relation or an
implicit function involving x and y. To ob-
tain an explicit function y = f(x) for the
circle, we can solve for y to obtain
y =
p
r2 ‚àíx2,
‚àír ‚â§x ‚â§r,
and
y = ‚àí
p
r2 ‚àíx2,
‚àír ‚â§x ‚â§r.

68
MATH FUNDAMENTALS
The explicit expression is really two functions, because a vertical line
crosses the circle in two places.
The Ô¨Årst function corresponds to
the top half of the circle, and the second function corresponds to the
bottom half.
Polar coordinates
Circles are so common in mathematics that mathematicians developed
a special "circular coordinate system" in order to describe them more
easily.
It is possible to specify the coordinates
(x, y) of any point on the circle in terms of
the polar coordinates r‚à†Œ∏, where r measures
the distance of the point from the origin, and
Œ∏ is the angle measured from the x-axis.
To convert from the polar coordinates r‚à†Œ∏
to the (x, y) coordinates, use the trigonomet-
ric functions cos and sin:
x = r cos Œ∏
and
y = r sin Œ∏.
Parametric equation
We can describe all the points on the circle if we specify a Ô¨Åxed
radius r and vary the angle Œ∏ over all angles: Œ∏ ‚àà[0, 360‚ó¶). A para-
metric equation speciÔ¨Åes the coordinates (x(Œ∏), y(Œ∏)) for the points on
a curve, for all values of the parameter Œ∏. The parametric equation
for a circle of radius r is given by
{(x, y) ‚ààR2 | x = r cos Œ∏, y = r sin Œ∏, Œ∏ ‚àà[0, 360‚ó¶)}.
Try to visualize the curve traced by the point (x(Œ∏), y(Œ∏)) = (r cos Œ∏, r sin Œ∏)
as Œ∏ varies from 0‚ó¶to 360‚ó¶. The point will trace out a circle of radius
r.
If we let the parameter Œ∏ vary over a smaller interval, we'll obtain
subsets of the circle. For example, the parametric equation for the
top half of the circle is
{(x, y) ‚ààR2 | x = r cos Œ∏, y = r sin Œ∏, Œ∏ ‚àà[0, 180‚ó¶]}.
The top half of the circle is also described by {(x, y) ‚ààR2 | y =
‚àö
r2 ‚àíx2, x ‚àà[‚àír, r]}, where the parameter used is the x-coordinate.
Area
The area of a circle of radius r is A = œÄr2.

1.15
SOLVING SYSTEMS OF LINEAR EQUATIONS
69
Circumference and arc length
The circumference of a circle is
C = 2œÄr.
This is the total length you can measure by following the curve all
the way around to trace the outline of the entire circle.
O
r
‚Ñì
Œ∏ =57‚ó¶
What is the length of a part of the circle?
Say you have a piece of the circle, called an
arc, and that piece corresponds to the angle
Œ∏ = 57‚ó¶. What is the arc's length ‚Ñì?
If the circle's total length C = 2œÄr repre-
sents a full 360‚ó¶turn around the circle, then
the arc length ‚Ñìfor a portion of the circle cor-
responding to the angle Œ∏ is
‚Ñì= 2œÄr Œ∏
360 .
The arc length ‚Ñìdepends on r, the angle Œ∏, and a factor of
2œÄ
360.
Radians
Though degrees are commonly used as a measurement unit for angles,
it's much better to measure angles in radians, since radians are the
natural units for measuring angles.
The conversion ratio between
degrees and radians is
2œÄ[rad] = 360‚ó¶.
When measuring angles in radians, the arc length is given by:
‚Ñì= rŒ∏rad.
Measuring angles in radians is equivalent to measuring arc length on
a circle with radius r = 1.
1.15
Solving systems of linear equations
We know that solving equations with one unknown‚Äîlike 2x+4 = 7x,
for instance‚Äîrequires manipulating both sides of the equation until
the unknown variable is isolated on one side. For this instance, we
can subtract 2x from both sides of the equation to obtain 4 = 5x,
which simpliÔ¨Åes to x = 4
5.

70
MATH FUNDAMENTALS
What about the case when you are given two equations and must
solve for two unknowns? For example,
x + 2y = 5,
3x + 9y = 21.
Can you Ô¨Ånd values of x and y that satisfy both equations?
Concepts
‚Ä¢ x, y: the two unknowns in the equations
‚Ä¢ eq1, eq2: a system of two equations that must be solved simul-
taneously. These equations will look like
a1x + b1y = c1,
a2x + b2y = c2,
where as, bs, and cs are given constants.
Principles
If you have n equations and n unknowns, you can solve the equations
simultaneously and Ô¨Ånd the values of the unknowns. There are several
diÔ¨Äerent approaches for solving equations simultaneously. We'll learn
about three diÔ¨Äerent approaches in this section.
Solution techniques
When solving for two unknowns in two equations, the best approach
is to eliminate one of the variables from the equations. By combining
the two equations appropriately, we can simplify the problem to the
problem of Ô¨Ånding one unknown in one equation.
Solving by substitution
We want to solve the following system of equations:
x + 2y = 5,
3x + 9y = 21.
We can isolate x in the Ô¨Årst equation to obtain
x = 5 ‚àí2y,
3x + 9y = 21.

1.15
SOLVING SYSTEMS OF LINEAR EQUATIONS
71
Now substitute the expression for x from the top equation into the
bottom equation:
3(5 ‚àí2y) + 9y = 21.
We just eliminated one of the unknowns by substitution. Continuing,
we expand the bracket to Ô¨Ånd
15 ‚àí6y + 9y = 21,
or
3y = 6.
We Ô¨Ånd y = 2, but what is x? Easy. To solve for x, plug the value
y = 2 into any of the equations we started from. Using the equation
x = 5 ‚àí2y, we Ô¨Ånd x = 5 ‚àí2(2) = 1.
Solving by subtraction
Let's return to our set of equations to see another approach for solving:
x + 2y = 5,
3x + 9y = 21.
Observe that any equation will remain true if we multiply the whole
equation by some constant. For example, we can multiply the Ô¨Årst
equation by 3 to obtain an equivalent set of equations:
3x + 6y = 15,
3x + 9y = 21.
Why did I pick 3 as the multiplier? By choosing this constant, the x
terms in both equations now have the same coeÔ¨Écient.
Subtracting two true equations yields another true equation. Let's
subtract the top equation from the bottom one:

3x ‚àí
3x + 9y ‚àí6y = 21 ‚àí15
‚áí
3y = 6.
The 3x terms cancel. This subtraction eliminates the variable x be-
cause we multiplied the Ô¨Årst equation by 3. We Ô¨Ånd y = 2. To Ô¨Ånd
x, substitute y = 2 into one of the original equations:
x + 2(2) = 5,
from which we deduce that x = 1.

72
MATH FUNDAMENTALS
Solving by equating
There is a third way to solve the equations:
x + 2y = 5,
3x + 9y = 21.
We can isolate x in both equations by moving all other variables and
constants to the right-hand sides of the equations:
x = 5 ‚àí2y,
x = 1
3(21 ‚àí9y) = 7 ‚àí3y.
Though the variable x is unknown to us, we know two facts about
it: x is equal to 5 ‚àí2y and x is equal to 7 ‚àí3y. Therefore, we can
eliminate x by equating the right-hand sides of the equations:
5 ‚àí2y = 7 ‚àí3y.
We solve for y by adding 3y to both sides and subtracting 5 from both
sides. We Ô¨Ånd y = 2 then plug this value into the equation x = 5‚àí2y
to Ô¨Ånd x. The solutions are x = 1 and y = 2.
Discussion
The three elimination techniques presented here will allow you to
solve any system of n linear equations in n unknowns. Each time you
perform a substitution, a subtraction, or an elimination by equating,
you're simplifying the problem to a problem of Ô¨Ånding (n ‚àí1) un-
knowns in a system of (n ‚àí1) equations. There is actually an entire
course called linear algebra, in which you'll develop a more advanced,
systematic approach for solving systems of linear equations.
Exercises
E1.1 Solve the system of equations simultaneously for x and y:
2x + 4y = 16,
5x ‚àíy = 7.
E1.2 Solve the system of equations for the unknowns x, y, and z:
2x + y ‚àí4z = 28,
x + y + z = 8,
2x ‚àíy ‚àí6z = 22.

1.16
SET NOTATION
73
E1.3 Solve for p and q given the equations p + q = 10 and p ‚àíq = 4.
E1.4 Solve the following system of linear equations:
2x + 4y = 8,
4x + 3y = 11.
1.16
Set notation
A set is the mathematically precise notion for describing a group of
objects. You don't need to know about sets to perform simple math;
but more advanced topics require an understanding of what sets are
and how to denote set membership and subset relations between sets.
DeÔ¨Ånitions
‚Ä¢ set: a collection of mathematical objects
‚Ä¢ S, T: the usual variable names for sets
‚Ä¢ N, Z, Q, R: some important sets of numbers: the naturals, the
integers, the rationals, and the real numbers, respectively.
‚Ä¢ { deÔ¨Ånition }: the curly brackets surround the deÔ¨Ånition of a
set, and the expression inside the curly brackets describes what
the set contains.
Set operations:
‚Ä¢ S ‚à™T: the union of two sets. The union of S and T corresponds
to the elements in either S or T.
‚Ä¢ S ‚à©T: the intersection of the two sets. The intersection of S
and T corresponds to the elements in both S and T.
‚Ä¢ S \ T: set minus. The diÔ¨Äerence S \ T corresponds to the ele-
ments of S that are not in T.
Set relations:
‚Ä¢ ‚äÇ: is a subset of
‚Ä¢ ‚äÜ: is a subset of or equal to
Special mathematical shorthand symbols and their corresponding mean-
ings:
‚Ä¢ ‚àÄ: for all
‚Ä¢ ‚àÉ: there exists
‚Ä¢ ‚àÑ: there doesn't exist
‚Ä¢ | : such that
‚Ä¢ ‚àà: element of
‚Ä¢ /‚àà: not an element of

74
MATH FUNDAMENTALS
Sets
Much of math's power comes from abstraction: the ability to see the
bigger picture and think meta thoughts about the common relation-
ships between math objects. We can think of individual numbers like
3, ‚àí5, and œÄ, or we can talk about the set of all numbers.
It is often useful to restrict our attention to a speciÔ¨Åc subset of
the numbers as in the following examples.
Example 1: The nonnegative real numbers
DeÔ¨Åne R+ ‚äÇR (read "R+ a subset of R") to be the set of non-negative
real numbers:
R+ ‚â°{all x in R such that x ‚â•0},
or expressed more compactly,
R+ ‚â°{x ‚ààR | x ‚â•0}.
If we were to translate the above expression into plain English, it
would read "the set R+ is deÔ¨Åned as the set of all real numbers x such
that x is greater or equal to zero."
Example 2: Odd and even integers
DeÔ¨Åne the set of even integers as
E ‚â°{n ‚ààZ | n
2 ‚ààZ} = {. . . , ‚àí2, 0, 2, 4, 6, . . .}
and the set of odd integers as
O ‚â°{n ‚ààZ | n+1
2
‚ààZ} = {. . . , ‚àí3, ‚àí1, 1, 3, 5, . . .}.
In both of the above examples, we use the mathematical notation
{. . . | . . .} to deÔ¨Åne the sets. Inside the curly brackets we Ô¨Årst describe
the general kind of objects we are talking about, followed by the
symbol "|" (read "such that"), followed by the conditions that must
be satisÔ¨Åed by all elements of the set.
Number sets
Recall how we deÔ¨Åned the fundamental sets of numbers in the begin-
ning of this chapter. It is worthwhile to review them brieÔ¨Çy.
The natural numbers form the set derived when you start from 0
and add 1 any number of times:
N ‚â°{0, 1, 2, 3, 4, 5, 6, . . .}.

1.16
SET NOTATION
75
The integers are the numbers derived by adding or subtracting 1 some
number of times:
Z ‚â°{x | x = ¬±n, n ‚ààN}.
When we allow for divisions between integers, we get the rational
numbers:
Q ‚â°

z
 z = x
y where x and y are in Z, and y Ã∏= 0

.
The broader class of real numbers also includes all rationals as well
as irrational numbers like
‚àö
2 and œÄ:
R ‚â°{œÄ, e, ‚àí1.53929411 . . . , 4.99401940129401 . . . , . . .}.
Finally, we have the set of complex numbers:
C ‚â°{1, i, 1 + i, 2 + 3i, . . .}.
Note that the deÔ¨Ånitions of R and C are not very precise. Rather than
giving a precise deÔ¨Ånition of each set inside the curly brackets as we
did for Z and Q, we instead stated some examples of the elements in
the set. Mathematicians sometimes do this and expect you to guess
the general pattern for all the elements in the set.
The following inclusion relationship holds for the fundamental sets
of numbers:
N ‚äÇZ ‚äÇQ ‚äÇR ‚äÇC.
This relationship means every natural number is also an integer. Ev-
ery integer is a rational number. Every rational number is a real.
Every real number is also a complex number.
New vocabulary
The specialized notation used by mathematicians can be diÔ¨Écult to
get used to. You must learn how to read symbols like ‚àÉ, ‚äÇ, |, and
‚ààand translate their meaning in the sentence. Indeed, learning ad-
vanced mathematics notation is akin to learning a new language.
To help you practice the new vocabulary, we will look at an ancient
mathematical proof and express it in terms of modern mathematical
symbols.
Square-root of 2 is irrational
Claim:
‚àö
2 /‚ààQ. This means there do not exist numbers m ‚ààZ
and n ‚ààZ such that m/n =
‚àö
2.
The last sentence expressed in
mathematical notation would read,
‚àÑm ‚ààZ, n ‚ààZ
 m/n =
‚àö
2.

76
MATH FUNDAMENTALS
To prove the claim we'll use a technique called proof by contradiction.
We begin by assuming the opposite of what we want to prove: that
there exist numbers m ‚ààZ and n ‚ààZ such that m/n =
‚àö
2. We'll
then carry out some simple algebra steps and in the end we'll obtain
an equation that is not true‚Äîwe'll arrive at a contradiction. Arriving
at a contradiction means our original supposition is wrong: there are
no numbers m ‚ààZ and n ‚ààZ such that m/n =
‚àö
2.
Proof: Suppose there exist numbers m ‚ààZ and n ‚ààZ such that
m/n =
‚àö
2. We can assume the integers m and n have no common
factors. In particular, m and n cannot both be even, otherwise they
would both contain at least one factor of 2. Next, we'll investigate
whether m is an even number m ‚ààE, or an odd number m ‚ààO.
Look back to Example 2 for the deÔ¨Ånitions of the sets O and E.
Before we check for even and oddness, it will help to point out
the fact that the action of squaring an integer preserves its odd/even
nature. An even number times an even number gives an even number:
if e ‚ààE then e2 ‚ààE. Similarly, an odd number times an odd number
gives an odd number: if o ‚ààO then o2 ‚ààO.
We proceed with the proof. We assume m/n =
‚àö
2. Taking the
square of both sides of this equation, we obtain
m2
n2 = 2
‚áí
m2 = 2n2.
If we analyze the last equation in more detail, we can conclude that
m cannot be an odd number, or written "m /‚ààO" in math. If m is an
odd number then m2 will also be odd, but this would contradict the
above equation since the right-hand side of the equation contains the
factor 2 and every number containing a factor 2 is even, not odd. If
m is an integer (m ‚ààZ) and m is not odd (m /‚ààO) then it must be
that m is even (m ‚ààE).
If m is even, then it contains a factor of 2, so it can be written as
m = 2q where q is some other number q ‚ààZ. The exact value of q is
not important. Let's revisit the equation m2 = 2n2 once more, this
time substituting m = 2q into the equation:
(2q)2 = 2n2
‚áí
2q2 = n2.
By a similar reasoning as before, we can conclude n cannot be odd
(n /‚ààO) so n must be even (n ‚ààE). However, this statement contra-
dicts our initial assumption that m and n do not have any common
factors!
The fact that we arrived at a contradiction means we must have
made a mistake somewhere in our reasoning. Since each of the steps
we carried out were correct, the mistake must be in the original

1.16
SET NOTATION
77
premise, namely that "there exist numbers m ‚ààZ and n ‚ààZ such
that m/n =
‚àö
2." Rather, the opposite must be true: "there do not
exist numbers m ‚ààZ and n ‚ààZ such that m/n =
‚àö
2."
The last
statement is equivalent to saying
‚àö
2 is irrational, which is what we
wanted to prove.
‚ñ°
Set relations and operations
Figure 1.22 illustrates the notion of a set B that is strictly contained
in the set A. We say B ‚äÇA if ‚àÄb ‚ààB, we also have b ‚ààA, and
‚àÉa ‚ààA such that a /‚ààB. In other words, we write B ‚äÇA whenever
the set A contains B, but there exists at least one element in A that
is not an element of B.
Also illustrated in Figure 1.22 is the union of two sets A‚à™B, which
includes all the elements of both A and B. If e ‚ààA ‚à™B, then e ‚ààA
and/or e ‚ààB.
Figure 1.22: The left side of the Ô¨Ågure is an illustration of a set B which
is strictly contained in another set A, denoted B ‚äÇA. The right side of
the Ô¨Ågure illustrates the union of two sets A ‚à™B.
The set intersection A ‚à©B and set minus A \ B are illustrated in
Figure 1.23.
Figure 1.23: The left side of the Ô¨Ågure shows the intersection between the
sets A‚à©B. The intersection of two sets contains the elements that are part
of both sets. The right side of the Ô¨Ågure shows the set diÔ¨Äerence A \ B,
which consists of all the elements that are in A but not in B.

78
MATH FUNDAMENTALS
Sets related to functions
A function that takes real variables as inputs and produces real num-
bers as outputs is denoted f : R ‚ÜíR. The domain of a function is
the set of all possible inputs to the function that produce an output:
Dom(f) ‚â°{x ‚ààR | f(x) ‚ààR}.
Inputs for which the function is undeÔ¨Åned are not part of the domain.
For instance the function f(x) = ‚àöx is not deÔ¨Åned for negative inputs,
so we have Dom(f) = R+.
The image set of a function is the set of all possible outputs of the
function:
Im(f) ‚â°{y ‚ààR | ‚àÉx ‚ààR, y = f(x)}.
For example, the function f(x) = x2 has the image set Im(f) = R+
since the outputs it produces are always non-negative.
Discussion
Knowledge of the precise mathematical jargon introduced in this sec-
tion is not crucial to understanding the rest of this book. That being
said, I wanted to expose you to it here because this is the language
in which mathematicians think. Most advanced math textbooks will
assume you understand technical mathematical notation.

1.17
MATH PROBLEMS
79
1.17
Math problems
We've now reached the Ô¨Årst section of problems in this book. The
purpose of these problems is to give you a way to comprehensively
practice your math fundamentals.
In the real world, you'll rarely
have to solve equations by hand, however, knowing how to solve math
equations and manipulate math expressions will be very useful in later
chapters of this book. At times, honing your math chops might seem
like tough mental work, but at the end of each problem, you'll gain
a stronger foothold on all the subjects you've been learning about.
You'll also experience a small achievement buzz after each problem
you vanquish.
I have a special message to readers who are learning math just for
fun: you can either try the problems in this section or skip them. Since
you have no upcoming exam on this material, you could skip ahead
to the next chapter without any immediate consequences. However
(and it's a big however), those readers who don't take a crack at these
problems will be missing a signiÔ¨Åcant opportunity.
Sit down to do them later today, or another time when you're
properly caÔ¨Äeinated. If you take the initiative to make time for math,
you'll Ô¨Ånd yourself developing lasting comprehension and true math
Ô¨Çuency. Without the practice of solving problems, however, you're
extremely likely to forget most of what you've learned within a month,
simple as that. You'll still remember the big ideas, but the details
will be fuzzy and faded. Don't break the pace now: with math, it's
very much use it or lose it!
By solving some of the problems in this section, you'll remember a
lot more stuÔ¨Ä. Make sure you step away from the pixels while you're
solving problems. You don't need fancy technology to do math; grab a
pen and some paper from the printer and you'll be Ô¨Åne. Do yourself a
favour: put your phone in airplane-mode, close the lid of your laptop,
and move away from desktop computers. Give yourself some time
to think. Yes, I know you can look up the answer to any question
in Ô¨Åve seconds on the Internet, and you can use live.sympy.org to
solve any math problem, but that is like outsourcing the thinking.
Descartes, Leibniz, and Riemann did most of their work with pen
and paper and they did well. Spend some time with math the way
the masters did.
P1.1
Solve for x in the equation x2 ‚àí9 = 7.
P1.2
Solve for x in the equation cos‚àí1  x
A

‚àíœÜ = œât.
P1.3
Solve for x in the equation 1
x = 1
a + 1
b .

80
MATH FUNDAMENTALS
P1.4
Use a calculator to Ô¨Ånd the values of the following expressions:
(1)
4‚àö
33
(2) 210
(3) 7
1
4 ‚àí10
(4) 1
2 ln(e22)
P1.5
Compute the following expressions involving fractions:
(1) 1
2 + 1
4
(2) 4
7 ‚àí23
5
(3) 1 3
4 + 1 31
32
P1.6
Use the basic rules of algebra to simplify the following expressions:
(1) ab 1
a b2cb‚àí3
(2) abc
bca
(3)
27a2
‚àö
9abba
(4) a(b + c) ‚àíca
b
(5)
a
c
3‚àö
b
b
4
3
a2
(6)(x + a)(x + b)‚àíx(a + b)
P1.7
Expand the brackets in the following expressions:
(1) (x + a)(x ‚àíb)
(2) (2x + 3)(x ‚àí5)
(3) (5x ‚àí2)(2x + 7)
P1.8
Factor the following expressions as a product of linear terms:
(1) x2 ‚àí2x ‚àí8
(2) 3x3 ‚àí27x
(3) 6x2 + 11x ‚àí21
P1.9
Complete the square in the following quadratic expressions to obtain
expressions of the form A(x ‚àíh)2 + k.
(1) x2 ‚àí4x + 7
(2) 2x2 + 12x + 22
(3) 6x2 + 11x ‚àí21
P1.10
A golf club and a golf ball cost $1.10 together. The golf club costs
one dollar more than the ball. How much does the ball cost?
P1.11
An ancient artist drew scenes of hunting on the walls of a cave,
including 43 Ô¨Ågures of animals and people. There were 17 more Ô¨Ågures of
animals than people. How many Ô¨Ågures of people did the artist draw and
how many Ô¨Ågures of animals?
P1.12
A father is 35 years old and his son is 5 years old. In how many
years will the father's age be four times the son's age?
P1.13
A boy and a girl collected 120 nuts. The girl collected twice as
many nuts as the boy. How many nuts did each collect?
P1.14
Alice is 5 years older than Bob. The sum of their ages is 25 years.
How old is Alice?
P1.15
A publisher needs to bind 4500 books. One print shop can bind
these books in 30 days, another shop can do it in 45 days. How many days
are necessary to bind all the books if both shops work in parallel?
Hint: Find the books-per-day rate of each shop.
P1.16
A plane leaves Vancouver travelling at 600 km/h toward Montreal.
One hour later, a second plane leaves Vancouver heading for Montreal at
900 km/h. How long will it take for the second plane to overtake the Ô¨Årst?
Hint: Distance travelled is equal to velocity multiplied by time: d = vt.
P1.17
There are 26 sheep and 10 goats on a ship. How old is the captain?

1.17
MATH PROBLEMS
81
P1.18
The golden ratio, denoted œï, is the positive solution to the quadratic
equation x2 ‚àíx ‚àí1 = 0. Find the golden ratio.
P1.19
Solve for x in the equation 1
x +
2
1 ‚àíx = 4
x2 .
Hint: Multiply both sides of the equation by x2(1 ‚àíx).
P1.20
Use substitution to solve for x in the following equations:
(1) x6 ‚àí4x3 + 4 = 0
(2)
1
2 ‚àísin x = sin x
P1.21
Find the range of values of the parameter m for which the equation
2x2 ‚àímx + m = 0 has no real solutions.
Hint: Use the quadratic formula.
P1.22
Use the properties of exponents and logarithms to simplify
(1) exe‚àíxez
(2)
xy‚àí2z‚àí3
x2y3z‚àí4
‚àí3
(3) (8x6)‚àí2
3
(4) log4(
‚àö
2)
(5) log10(0.001)
(6)ln(x2 ‚àí1)‚àíln(x‚àí1)
P1.23
When representing numbers on a computer, the number of digits
of precision n in base b and the approximation error œµ are related by the
equation n = ‚àílogb(œµ). A float64 has 53 bits of precision (digits base
2). What is the approximation error œµ for a float64? How many digits of
precision does a float64 have in decimal (base 10)?
P1.24
Find the values of x that satisfy the following inequalities:
(1) 2x ‚àí5 > 3
(2) 5 ‚â§3x ‚àí4 ‚â§14
(3) 2x2 + x ‚â•1
P1.25
Two algorithms, P and Q, can be used to solve a certain problem.
The running time of Algorithm P as a function of the size of the problem n
is described by the function P(n) = 0.002n2. The running time of Algo-
rithm Q is described by Q(n) = 0.5n. For small problems, Algorithm P
runs faster. Starting from what n will Algorithm Q be faster?
P1.26
Consider a right-angle triangle in which the shorter sides are 8 cm
and 6 cm. What is the length of the triangle's longest side?
P1.27
A television screen measures 26 inches on the diagonal. The screen
height is 13 inches. How wide is the screen?
P1.28
A ladder of length 3.33 m leans against a wall and its foot is 1.44 m
from the wall. What is the height h where the ladder touches the wall?
h
1.44 m
3.33 m

82
MATH FUNDAMENTALS
P1.29
Kepler's triangle Consider a right-angle triangle in which the
hypotenuse has length œï =
‚àö
5+1
2
(the golden ratio) and the adjacent side
has length ‚àöœï. What is the length of the opposite side?
P1.30
Find the lengths x, y, and z in the Ô¨Ågure below.
x
30‚ó¶
A
B
C
D
E
q
3
2
q
3
2
z
y
45‚ó¶
P1.31
Given the angle and distance measurements labelled in Figure 1.24,
calculate the distance d and the height of the mountain peak h.
h
1000 m
800 m
d
20‚ó¶
25‚ó¶
Figure 1.24: Measuring the height of a mountain using angles.
Hint: Use the deÔ¨Ånition of tan Œ∏ to obtain two equations in two unknowns.
P1.32
You're observing a house from a blimp Ô¨Çying at an altitude of 2000
metres. From your point of view, the house appears at an angle 24‚ó¶below
the horizontal. What is the horizontal distance x between the blimp and
the house?

1.17
MATH PROBLEMS
83
24‚ó¶
2000
Œ∏
x
P1.33
Find x. Express your answer in terms of a, b, c and Œ∏.
a
b
c
x
Œ∏
Hint: Use Pythagoras' theorem twice and the tan function.
P1.34
An equilateral triangle is inscribed in a circle of radius 1. Find the
side length a and the area of the inscribed triangle A‚ñ≥.
1
a
Hint: Split the triangle into three sub-triangles.
P1.35
Use the power-reduction trigonometric identities (page 64) to ex-
press sin2 Œ∏ cos2 Œ∏ in terms of cos 4Œ∏.
P1.36
A circle of radius 1 is inscribed inside a regular octagon (a polygon
with eight sides of length b).
Calculate the octagon's perimeter and its
area.
1
b
b
b
b
b
b
b
b
Hint: Split the octagon into eight isosceles triangles.
P1.37
Find the length of side c in the triangle:

84
MATH FUNDAMENTALS
b
a = 10
c
A = 41‚ó¶
B
C = 75‚ó¶
Hint: Use the sine rule.
P1.38
Consider the obtuse triangle shown in Figure 1.25.
(a) Express h in terms of a and Œ∏.
(b) What is the area of this triangle?
(c) Express c in terms of the variables a, b, and Œ∏.
Hint: You can use the cosine rule for part (c).
h
c
a
b
A
B
C
Œ∏
Figure 1.25: A triangle with base b and height h.
P1.39
Find the measure of the angle B and deduce the measure of the
angle C. Find the length of side c.
b = 30
a = 18
c
A = 25‚ó¶
B
C
Hint: The sum of the internal angle measures of a triangle is 180‚ó¶.
P1.40
An observer on the ground measures an angle of inclination of
30‚ó¶to an approaching airplane, and 10 seconds later measures an angle of
inclination of 55‚ó¶. If the airplane is Ô¨Çying at a constant speed at an altitude
of 2000 m in a straight line directly over the observer, Ô¨Ånd the speed of the
airplane in kilometres per hour.

1.17
MATH PROBLEMS
85
v
v
2000 m
30‚ó¶
55‚ó¶
10 seconds pass
P1.41
Satoshi likes warm sak√©. He places 1 litre of water in a sauce pan
with diameter 17 cm. How much will the height of the water level rise when
Satoshi immerses a sak√© bottle with diameter 7.5 cm?
Hint: You'll need the volume conversion ratio 1 litre = 1000 cm3.
P1.42
Find the length x of the diagonal of the quadrilateral below.
4
8
7
x
12
Œ±1
Œ±2
11
Hint: Use the law of cosines once to Ô¨Ånd Œ±1 and Œ±2, and again to Ô¨Ånd x.
P1.43
Find the area of the shaded region.
2 cm
2 cm
2 cm
Hint: Find the area of the outer circle, subtract the area of missing centre
disk, then divide by two.
P1.44
In preparation for the shooting of a music video, you're asked to
suspend a wrecking ball hanging from a circular pulley. The pulley has a
radius of 50 cm. The other lengths are indicated in the Ô¨Ågure. What is the
total length of the rope required?

86
MATH FUNDAMENTALS
40‚ó¶
2m
4m
Hint: The total length of rope consists of two straight parts and the curved
section that wraps around the pulley.
P1.45
The length of a rectangle is c + 2 and its height is 5. What is the
area of the rectangle?
P1.46
A box of facial tissues has dimensions 10.5 cm by 7 cm by 22.3 cm.
What is the volume of the box in litres?
Hint: 1 L = 1000 cm3.
P1.47
What is the measure of the angle Œ∏ in the Ô¨Ågure below?
60‚ó¶
100‚ó¶
Œ∏
Hint: At the intersection of two lines, vertically opposite angles are equal.
P1.48
A large circle of radius R is surrounded by 12 smaller circles of
radius r. Find the ratio R
r rounded to four decimals.

1.17
MATH PROBLEMS
87
Hint: Draw an isosceles triangle with one vertex at the centre of the R-circle
and the other vertices at the centres of two adjacent r-circles.
P1.49
The area of a rectangular Ô¨Ågure is 35 cm2. If one side is 5 cm, how
long is the other side?
P1.50
A swimming pool has length ‚Ñì= 20 m, width w = 10 m, and depth
d = 1.5 m. Calculate the volume of water in the swimming pool in litres?
Hint: 1 m3 = 1000 L.
P1.51
How many litres of water remain in a tank that is 12 m long, 6 m
wide, and 3 m high, if 30% of its capacity is spent?
P1.52
A building has two water tanks, each with capacity 4000 L. One of
them is 1
4 full and the other contains three times more water. How many
litres of water does the building have in total?
P1.53
The rectangular lid of a box has length 40 cm and width 30 cm. A
rectangular hole with area 500 cm2 must be cut in the lid so that the hole's
sides are equal distances from the sides of the lid. What will the distance
be between the sides of the hole and the sides of the lid?
Hint: You'll need to deÔ¨Åne three variables to solve this problem.
P1.54
A man sells Ô¨Årewood. To make standard portions, he uses a stan-
dard length of rope ‚Ñìto surround a pack of logs. One day, a customer asks
him for a double portion of Ô¨Årewood. What length of rope should he use
to measure this order? Assume the packs of logs are circular in shape.
P1.55
How much pure water should be added to 10 litres of a solution
that is 60% acid to make a solution that is 20% acid?
P1.56
A tablet screen has a resolution of 768 pixels by 1024 pixels, and
the physical dimensions of the screen are 6 inches by 8 inches. One might
conclude that the best size of a PDF document for such a screen would
be 6 inches by 8 inches. At Ô¨Årst I thought so too, but I forgot to account
for the status bar, which is 20 pixels tall. The actual usable screen area is
only 768 pixels by 1004 pixels. Assuming the width of the PDF is chosen
to be 6 inches, what height should the PDF be so that it Ô¨Åts perfectly in
the content area of the tablet screen?

88
MATH FUNDAMENTALS
P1.57
Find the sum of the natural numbers 1 through 100.
Hint: Imagine pairing the biggest number with the smallest number in the
sum, the second biggest number with the second smallest number, etc.
P1.58
Solve for x and y simultaneously in the following system of equa-
tions: ‚àíx ‚àí2y = ‚àí2 and 3x + 3y = 0.
P1.59
Solve the following system of equations for the three unknowns:
1x + 2y + 3z = 14,
2x + 5y + 6z = 30,
‚àí1x + 2y + 3z = 12.
P1.60
A hotel oÔ¨Äers a 15% discount on rooms. Determine the original
price of a room if the discounted room price is $95.20.
P1.61
A set of kitchen tools normally retails for $450, but today it is
priced at the special oÔ¨Äer of $360. Calculate the percentage of the discount.
P1.62
You take out a $5000 loan at a nominal annual percentage rate
(nAPR) of 12% with monthly compounding. How much money will you
owe after 10 years?
P1.63
Plot the graphs of f(x) = 100e‚àíx/2 and g(x) = 100(1 ‚àíe‚àíx/2) by
evaluating the functions at diÔ¨Äerent values of x from 0 to 11.
P1.64
Starting from an initial quantity Qo of Exponentium at t = 0 s,
the quantity Q of Exponentium as a function of time varies according to
the expression Q(t) = Qo e‚àíŒªt, where Œª = 5.0 and t is measured in seconds.
Find the half-life of Exponentium, that is, the time it takes for the quantity
of Exponentium to reduce to half the initial quantity Qo.
P1.65
A hot body cools so that every 24 min its temperature decreases
by a factor of two. Deduce the time-constant and determine the time it
will take the body to reach 1% of its original temperature.
Hint: The temperature function is T(t)=Toe‚àít/œÑ and œÑ is the time constant.
P1.66
A capacitor of capacitance C = 4.0 √ó 10‚àí6 farads, charged to an
initial potential of Vo = 20 volts, is discharging through a resistance of
R = 10 000 ‚Ñ¶(read Ohms).
Find the potential V after 0.01 s and after
0.1 s, knowing the decreasing potential follows the rule V (t) = Voe‚àí
t
RC .
P1.67
Let B be the set of people who are bankers and C be the set of
crooks. Rewrite the math statement ‚àÉb ‚ààB | b /‚ààC in plain English.
P1.68
Let M denote the set of people who run Monsanto, and H denote
the people who ought to burn in hell for all eternity.
Write the math
statement ‚àÄp ‚ààM, p ‚ààH in plain English.
P1.69
When starting a business, one sometimes needs to Ô¨Ånd investors.
DeÔ¨Åne M to be the set of investors with money, and C to be the set of
investors with connections. Describe the following sets in words: (a) M \C,
(b) C \ M, and the most desirable set (c) M ‚à©C.

Chapter 2
Vectors
In this chapter we'll learn how to manipulate multi-dimensional ob-
jects called vectors. Vectors are the precise way to describe directions
in space. We need vectors in order to describe physical quantities like
the velocity of an object, its acceleration, and the net force acting on
the object.
Vectors
are
built
from
ordinary
numbers, which form the components
of the vector. You can think of a vec-
tor as a list of numbers, and vector al-
gebra as operations performed on the
numbers in the list.
Vectors can also
be manipulated as geometrical objects,
represented by arrows in space.
The
arrow that corresponds to the vector
‚Éóv = (vx, vy) starts at the origin (0, 0)
and ends at the point (vx, vy).
The
word vector comes from the Latin vehere, which means to carry. In-
deed, the vector ‚Éóv takes the point (0, 0) and carries it to the point
(vx, vy).
This chapter will introduce you to vectors, vector algebra, and
vector operations, which are useful for solving problems in many areas
of science. What you'll learn here applies more broadly to problems
in computer graphics, probability theory, machine learning, and other
Ô¨Åelds of science and mathematics. It's all about vectors these days,
so you better get to know them.
89

90
VECTORS
Figure 2.1: This Ô¨Ågure illustrates the new concepts related to vectors. As
you can see, there is quite a bit of new vocabulary to learn, but don't be
phased‚Äîall these terms are just fancy ways of talking about arrows.
2.1
Vectors
Vectors are extremely useful in all areas of life. In physics, for exam-
ple, we use a vector to describe the velocity of an object. It is not
suÔ¨Écient to say that the speed of a tennis ball is 20[m/s]: we must
also specify the direction in which the ball is moving. Both of the two
velocities
‚Éóv1 = (20, 0)
and
‚Éóv2 = (0, 20)
describe motion at the speed of 20[m/s]; but since one velocity points
along the x-axis, and the other points along the y-axis, they are com-
pletely diÔ¨Äerent velocities. The velocity vector contains information
about the object's speed and direction. The direction makes a big
diÔ¨Äerence. If it turns out that the tennis ball is coming your way, you
need to get out of the way!
This section's main idea is that vectors are not the same as
numbers. A vector is a special kind of mathematical object that is
made up of numbers. Before we begin any calculations with vectors,
we need to think about the basic mathematical operations that we
can perform on vectors. We will deÔ¨Åne vector addition ‚Éóu + ‚Éóv, vector
subtraction ‚Éóu ‚àí‚Éóv, vector scaling Œ±‚Éóv, and other operations. We will
also discuss two diÔ¨Äerent notions of vector product, which have useful
geometrical properties.

2.1
VECTORS
91
DeÔ¨Ånitions
The two-dimensional vector ‚Éóv ‚ààR2 is equivalent to a pair of numbers
‚Éóv ‚â°(vx, vy).
We call vx the x-component of ‚Éóv, and vy is the y-
component of ‚Éóv.
Vector representations
We'll use three equivalent ways to denote vectors:
‚Ä¢ ‚Éóv = (vx, vy): component notation, where the vector is repre-
sented as a pair of coordinates with respect to the x-axis and
the y-axis.
‚Ä¢ ‚Éóv = vxÀÜƒ±+vyÀÜÔöæ: unit vector notation, where the vector is expressed
in terms of the unit vectors ÀÜƒ± = (1, 0) and ÀÜÔöæ= (0, 1)
‚Ä¢ ‚Éóv = ‚à•‚Éóv‚à•‚à†Œ∏: length-and-direction notation, where the vector is
expressed in terms of its length ‚à•‚Éóv‚à•and the angle Œ∏ that the
vector makes with the x-axis.
These three notations describe diÔ¨Äerent aspects of vectors, and we will
use them throughout the rest of the book. We'll learn how to convert
between them‚Äîboth algebraically (with pen, paper, and calculator)
and intuitively (by drawing arrows).
Vector operations
Consider two vectors, ‚Éóu = (ux, uy) and ‚Éóv = (vx, vy), and assume that
Œ± ‚ààR is an arbitrary constant. The following operations are deÔ¨Åned
for these vectors:
‚Ä¢ Addition:
‚Éóu + ‚Éóv = (ux + vx, uy + vy)
‚Ä¢ Subtraction:
‚Éóu ‚àí‚Éóv = (ux ‚àívx, uy ‚àívy)
‚Ä¢ Scaling:
Œ±‚Éóu = (Œ±ux, Œ±uy)
‚Ä¢ Dot product:
‚Éóu ¬∑ ‚Éóv = uxvx + uyvy
‚Ä¢ Length:
‚à•‚Éóu‚à•=
‚àö
‚Éóu ¬∑ ‚Éóu =
q
u2x + u2y. We will also sometimes
simply use the letter u to denote the length of ‚Éóu.
‚Ä¢ Cross product:
‚Éóu √ó ‚Éóv = (uyvz ‚àíuzvy, uzvx ‚àíuxvz, uxvy ‚àí
uyvx). The cross product is only deÔ¨Åned for three-dimensional
vectors like ‚Éóu = (ux, uy, uz) and ‚Éóv = (vx, vy, vz).
Pay careful attention to the dot product and the cross product. Al-
though they're called products, these operations behave much diÔ¨Äer-
ently than taking the product of two numbers. Also note, there is no
notion of vector division.

92
VECTORS
Vector algebra
Addition and subtraction
Just like numbers, you can add vectors
‚Éóv + ‚Éów = (vx, vy) + (wx, wy) = (vx + wx, vy + wy),
subtract them
‚Éóv ‚àí‚Éów = (vx, vy) ‚àí(wx, wy) = (vx ‚àíwx, vy ‚àíwy),
and solve all kinds of equations where the unknown variable is a
vector.
This is not a formidably complicated new development in
mathematics. Performing arithmetic calculations on vectors simply
requires carrying out arithmetic operations on their compo-
nents. Given two vectors, ‚Éóv = (4, 2) and ‚Éów = (3, 7), their diÔ¨Äerence
is computed as ‚Éóv ‚àí‚Éów = (4, 2) ‚àí(3, 7) = (1, ‚àí5).
Scaling
We can also scale a vector by any number Œ± ‚ààR:
Œ±‚Éóv = (Œ±vx, Œ±vy),
where each component is multiplied by the scaling factor Œ±. Scaling
changes the length of a vector. If Œ± > 1 the vector will get longer, and
if 0 ‚â§Œ± < 1 then the vector will become shorter. If Œ± is a negative
number, the scaled vector will point in the opposite direction.
Length
A vector's length is obtained from Pythagoras' theorem.
Imagine a right-angle triangle with one side of length vx and the
other side of length vy; the length of the vector is equal to the length
of the triangle's hypotenuse:
‚à•‚Éóv‚à•2 = v2
x + v2
y
‚áí
‚à•‚Éóv‚à•=
q
v2x + v2y .
A common technique is to scale a vector ‚Éóv by one over its length
1
‚à•‚Éóv‚à•
to obtain a unit-length vector that points in the same direction as ‚Éóv:
ÀÜv ‚â°
‚Éóv
‚à•‚Éóv‚à•=
 vx
‚à•‚Éóv‚à•, vy
‚à•‚Éóv‚à•

.
Unit vectors (denoted with a hat instead of an arrow) are useful when
you want to describe only a direction in space without any speciÔ¨Åc
length in mind. Verify that ‚à•ÀÜv‚à•= 1.
Vector as arrows
So far, we described how to perform algebraic operations on vectors
in terms of their components. Vector operations can also be inter-
preted geometrically, as operations on two-dimensional arrows in the
Cartesian plane.

2.1
VECTORS
93
Vector addition
The sum of
two vectors corresponds to the
combined displacement of the
two vectors.
The diagram on
the right illustrates the addition
of two vectors, ‚Éóv1 = (3, 0) and
‚Éóv2 = (2, 2). The sum of the two
vectors is the vector ‚Éóv1 + ‚Éóv2 =
(3, 0) + (2, 2) = (5, 2).
Vector subtraction
Before we describe vector subtraction, note
that multiplying a vector by a scaling factor Œ± = ‚àí1 gives a vector of
the same length as the original, but pointing in the opposite direction.
This fact is useful if you want
to subtract two vectors using the
graphical approach. Subtracting
a vector is the same as adding
the negative of the vector:
‚Éów ‚àí‚Éóv1 = ‚Éów + (‚àí‚Éóv1) = ‚Éóv2.
The diagram on the right illus-
trates the graphical procedure
for subtracting the vector ‚Éóv1 = (3, 0) from the vector ‚Éów = (5, 2).
Subtraction of ‚Éóv1 = (3, 0) is the same as addition of ‚àí‚Éóv1 = (‚àí3, 0).
Scaling
The scaling operation acts
to change the length of a vector.
Suppose we want to obtain a vector
in the same direction as the vector
‚Éóv = (3, 2), but half as long. "Half as
long" corresponds to a scaling factor
of Œ± = 0.5. The scaled-down vector
is ‚Éów = 0.5‚Éóv = (1.5, 1).
Conversely, we can think of the
vector ‚Éóv as being twice as long as the
vector ‚Éów.
Length-and-direction representation
So far, we've seen how to represent a vector in terms of its components.
There is also another way of representing vectors: we can specify a
vector in terms of its length ||‚Éóv|| and its direction‚Äîthe angle it makes
with the x-axis. For example, the vector (1, 1) can also be written as

94
VECTORS
‚àö
2‚à†45‚ó¶. This magnitude-and-direction notation is useful because it
makes it easy to see the "size" of vectors. On the other hand, vector
arithmetic operations are much easier to carry out in the component
notation. We will use the following formulas for converting between
the two notations.
To
convert
the
length-and-direction
vector ‚à•‚Éór‚à•‚à†Œ∏ into an x-component and a
y-component (rx, ry), use the formulas
rx = ‚à•‚Éór‚à•cos Œ∏
and
ry = ‚à•‚Éór‚à•sin Œ∏.
To
convert
from
component
notation
(rx, ry) to length-and-direction ‚à•‚Éór‚à•‚à†Œ∏, use
r = ‚à•‚Éór‚à•=
q
r2x + r2y
and
Œ∏ = tan‚àí1
ry
rx

.
Note that the second part of the equation involves the inverse tangent
function. By convention, the function tan‚àí1 returns values between
œÄ/2 (90‚ó¶) and ‚àíœÄ/2 (‚àí90‚ó¶). You must be careful when Ô¨Ånding the Œ∏
of vectors with an angle outside of this range. SpeciÔ¨Åcally, for vectors
with vx < 0, you must add œÄ (180‚ó¶) to tan‚àí1(ry/rx) to obtain the
correct Œ∏.
Unit vector notation
As discussed above, we can think of a vector ‚Éóv = (vx, vy, vz) as a
command to "go a distance vx in the x-direction, a distance vy in the
y-direction, and vz in the z-direction."
To write this set of commands more explicitly, we can use multiples
of the vectors ÀÜƒ±, ÀÜÔöæ, and ÀÜk. These are the unit vectors pointing in the
x, y, and z directions, respectively:
ÀÜƒ± = (1, 0, 0),
ÀÜÔöæ= (0, 1, 0),
and
ÀÜk = (0, 0, 1).
Any number multiplied by ÀÜƒ± corresponds to a vector with that number
in the Ô¨Årst coordinate. For example, 3ÀÜƒ± ‚â°(3, 0, 0). Similarly, 4ÀÜÔöæ‚â°
(0, 4, 0) and 5ÀÜk ‚â°(0, 0, 5).
In physics, we tend to perform a lot of numerical calculations with
vectors; to make things easier, we often use unit vector notation:
vxÀÜƒ± + vyÀÜÔöæ+ vzÀÜk
‚áî
‚Éóv
‚áî
(vx, vy, vz).
The addition rule remains the same for the new notation:
2ÀÜƒ± + 3ÀÜÔöæ
| {z }
‚Éóv
+
5ÀÜƒ± ‚àí2ÀÜÔöæ
| {z }
‚Éów
= 7ÀÜƒ± + 1ÀÜÔöæ
| {z }
‚Éóv+‚Éów
.
It's the same story repeating all over again: we need to add ÀÜƒ±s with
ÀÜƒ±s, and ÀÜÔöæs with ÀÜÔöæs.

2.1
VECTORS
95
Examples
Simple example
Compute the sum ‚Éós = 4ÀÜƒ±+5‚à†30‚ó¶. Express your answer in the length-
and-direction notation.
Since we want to carry out an addition, and since addition is
performed in terms of components, our Ô¨Årst step is to convert 5‚à†30‚ó¶
into component notation: 5‚à†30‚ó¶= 5 cos 30‚ó¶ÀÜƒ±+5 sin 30‚ó¶ÀÜÔöæ= 5
‚àö
3
2 ÀÜƒ±+ 5
2ÀÜÔöæ.
We can now compute the sum:
‚Éós
=
4ÀÜƒ± + 5
‚àö
3
2 ÀÜƒ± + 5
2ÀÜÔöæ
=
(4 + 5
‚àö
3
2 )ÀÜƒ± + ( 5
2)ÀÜÔöæ.
The x-component of the sum is sx = (4 + 5
‚àö
3
2 ) and the y-component
of the sum is sy = ( 5
2). To express the answer as a length and a
direction, we compute the length ‚à•‚Éós‚à•=
q
s2x + s2y = 8.697 and the
direction tan‚àí1(sy/sx) = 16.7‚ó¶. The answer is ‚Éós = 8.697‚à†16.7‚ó¶.
Vector addition example
You're heading to physics class after a "safety meeting" with a friend,
and are looking forward to two hours of Ô¨Ånding absolute amazement
and awe in the laws of Mother Nature. As it turns out, there is no
enlightenment to be had that day because there is going to be an
in-class midterm. The Ô¨Årst question involves a block sliding down an
incline. You look at it, draw a little diagram, and then wonder how
the hell you are going to Ô¨Ånd the net force acting on the block. The
three forces acting on the block are ‚ÉóW = 300‚à†‚àí90‚ó¶, ‚ÉóN = 260‚à†120‚ó¶,
and ‚ÉóFf = 50‚à†30‚ó¶.
You happen to remember the net force formula:
X ‚ÉóF = ‚ÉóFnet = m‚Éóa
[ Newton's 2nd law ].
You get the feeling Newton's 2nd law is the answer to all your trou-
bles. You sense this formula is certainly the key because you saw the
keyword "net force" when reading the question, and notice "net force"
also appears in this very equation.
The net force is the sum of all forces acting on the block:
‚ÉóFnet =
X ‚ÉóF = ‚ÉóW + ‚ÉóN + ‚ÉóFf.
All that separates you from the answer is the addition of these vectors.
Vectors have components, and there is the whole sin/cos procedure
for decomposing length-and-direction vectors into their components.
If you have the vectors as components you'll be able to add them and
Ô¨Ånd the net force.

96
VECTORS
Okay, chill! Let's do this one step at a time. The net force must
have an x-component, which, according to the equation, must equal
the sum of the x-components of all the forces:
Fnet,x = Wx + Nx + Ff,x
= 300 cos(‚àí90‚ó¶) + 260 cos(120‚ó¶) + 50 cos(30‚ó¶)
= ‚àí86.7.
Now Ô¨Ånd the y-component of the net force using the sin of the angles:
Fnet,y = Wy + Ny + Ff,y
= 300 sin(‚àí90‚ó¶) + 260 sin(120‚ó¶) + 50 sin(30‚ó¶)
= ‚àí49.8.
Combining the two components of the vector, we get the Ô¨Ånal answer:
‚ÉóFnet ‚â°(Fnet,x, Fnet,y)
= (‚àí86.7, ‚àí49.8) = ‚àí86.7ÀÜƒ± ‚àí49.8ÀÜÔöæ
= 100‚à†209.9‚ó¶.
Bam! Just like that you're done, because you overstand them vectors!
Relative motion example
A boat can reach a top speed of 12 knots in calm seas. Instead of
cruising through a calm sea, however, the boat's crew is trying to sail
up the St-Laurence river. The speed of the current is 5 knots.
If the boat travels directly upstream at full throttle 12ÀÜƒ±, then the
speed of the boat relative to the shore will be
12ÀÜƒ± ‚àí5ÀÜƒ± = 7ÀÜƒ±,
since we have to "deduct" the speed of the current from the speed of
the boat relative to the water.
If the crew wants to cross the
river perpendicular to the current
Ô¨Çow, they can use some of the
boat's thrust to counterbalance the
current, and the remaining thrust
to push across. In what direction
should the boat sail to cross the
river? We are looking for the di-
rection of ‚Éóv the boat should take
such that, after adding in the ve-
locity of the current, the boat moves in a straight line between the
two banks (the ÀÜÔöædirection).

2.1
VECTORS
97
A picture is necessary: draw a river, then draw a triangle in the
river with its long leg perpendicular to the current Ô¨Çow. Make the
short leg of length 5. We will take the up-the-river component of the
speed ‚Éóv to be equal to 5ÀÜƒ±, so that it cancels exactly the ‚àí5ÀÜƒ± Ô¨Çow of
the river. Finally, label the hypotenuse with length 12, since this is
the speed of the boat relative to the surface of the water.
From all of this we can answer the question like professionals. You
want the angle? Well, we have that 12 sin(Œ∏) = 5, where Œ∏ is the angle
of the boat's course relative to the straight line between the two banks.
We can use the inverse-sin function to solve for the angle:
Œ∏ = sin‚àí1
 5
12

= 24.62‚ó¶.
The across-the-river component of the velocity can be calculated using
vy = 12 cos(Œ∏) = 10.91, or from Pythagoras' theorem if you prefer
vy =
p
‚à•‚Éóv‚à•2 ‚àív2x =
‚àö
122 ‚àí52 = 10.91.
Vector dimensions
The most common types of vectors are two-dimensional vectors (like
the ones in the Cartesian plane), and three-dimensional vectors (di-
rections in 3D space). 2D and 3D vectors are easier to work with
because we can visualize them and draw them in diagrams. In gen-
eral, vectors can exist in any number of dimensions. An example of a
n-dimensional vector is
‚Éóv = (v1, v2, . . . , vn) ‚ààRn.
The rules of vector algebra apply in higher dimensions, but our ability
to visualize stops at three dimensions.
Coordinate system
The geometrical interpretation of vectors depends on the coordinate
system in which the vectors are represented. Throughout this section
we have used the x, y, and z axes, and we've described vectors as
components along each of these directions. This is a very convenient
coordinate system; we have a set of three perpendicular axes, and a
set of three unit vectors {ÀÜƒ±, ÀÜÔöæ, ÀÜk} that point along each of the three
axis directions.
Every vector is implicitly deÔ¨Åned in terms of this
coordinate system. When you and I talk about the vector ‚Éóv = 3ÀÜƒ± +
4ÀÜÔöæ+ 2ÀÜk, we are really saying, "start from the origin (0, 0, 0), move
3 units in the x-direction, then move 4 units in the y-direction, and
Ô¨Ånally move 2 units in the z-direction." It is simpler to express these

98
VECTORS
directions as ‚Éóv = (3, 4, 2), while remembering that the numbers in the
bracket measure distances relative to the xyz-coordinate system.
It turns out, using the xyz-coordinate system and the vectors
{ÀÜƒ±, ÀÜÔöæ, ÀÜk} is just one of many possible ways we can represent vectors.
We can represent a vector ‚Éóv as coeÔ¨Écients (v1, v2, v3) with respect to
any basis {ÀÜe1, ÀÜe2, ÀÜe3} as follows: ‚Éóv = v1ÀÜe1 + v2ÀÜe2 + v3ÀÜe3. What is a
basis, you ask? I'm glad you asked, because this is the subject of the
next section.
2.2
Basis
One of the most important concepts in the study of vectors is the
concept of a basis. Consider the space of three-dimensional vectors
R3. A basis for R3 is a set of vectors {ÀÜe1, ÀÜe2, ÀÜe3} which can be used as
a coordinate system for R3. If the set of vectors {ÀÜe1, ÀÜe2, ÀÜe3} is a basis,
then you can represent any vector ‚Éóv ‚ààR3 as coeÔ¨Écients (v1, v2, v3)
with respect to that basis:
‚Éóv = v1ÀÜe1 + v2ÀÜe2 + v3ÀÜe3.
The vector ‚Éóv is obtained by measuring out a distance v1 in the ÀÜe1
direction, a distance v2 in the ÀÜe2 direction, and a distance v3 in the
ÀÜe3 direction.
You are already familiar with the standard basis {ÀÜƒ±, ÀÜÔöæ, ÀÜk}, which is
associated with the xyz-coordinate system. You know that any vector
‚Éóv ‚ààR3 can be expressed as a triplet (vx, vy, vz) with respect to the
basis {ÀÜƒ±, ÀÜÔöæ, ÀÜk} through the formula ‚Éóv = vxÀÜƒ±+vyÀÜÔöæ+vzÀÜk. In this section,
we'll discuss how to represent vectors with respect to other bases.
An analogy
Let's start with a simple example of a basis. If you look at the HTML
code behind any web page, you're sure to Ô¨Ånd at least one mention of
the colour stylesheet directive such as background-color:#336699;.
The numbers should be interpreted as a triplet of values (33, 66, 99),
each value describing the amount of red, green, and blue needed to
create a given colour. Let us call the colour described by the triplet
(33, 66, 99) CoolBlue.
This convention for colour representation is
called the RGB colour model and we can think of it as the RGB
basis.
A basis is a set of elements that can be combined together
to express something more complicated. In our case, the R, G, and
B elements are pure colours that can create any colour when mixed
appropriately. Schematically, we can write this mixing idea as
CoolBlue = (33, 66, 99)RGB = 33R + 66G + 99B,

2.3
VECTOR PRODUCTS
99
where the coeÔ¨Écients determine the strength of each colour compo-
nent. To create the colour, we combine its components as symbolized
by the + operation.
The cyan, magenta, and yellow (CMY) colour model is another
basis for representing colours. To express the "cool blue" colour in
the CMY basis, you will need the following coeÔ¨Écients:
(33, 66, 99)RGB = CoolBlue = (222, 189, 156)CMY = 222C+189M+156Y.
The same colour CoolBlue is represented by a diÔ¨Äerent set of coeÔ¨É-
cients when the CMY colour basis is used.
Note that a triplet of coeÔ¨Écients by itself does not mean anything
unless we know the basis being used.
For example, if we were to
interpret the triplet of coordinates (33, 66, 99) with respect to the
CMY basis, will would obtain a completely diÔ¨Äerent colour, which
would not be cool at all.
A basis is required to convert mathematical objects like the triplet
(a, b, c) into real-world ideas like colours. As exempliÔ¨Åed above, to
avoid any ambiguity we can use a subscript after the bracket to indi-
cate the basis associated with each triplet of coeÔ¨Écients.
Discussion
It's hard to over-emphasize the importance of the basis‚Äîthe coor-
dinate system you will use to describe vectors. The choice of coor-
dinate system is the bridge between real-world vector quantities and
their mathematical representation in terms of components.
Every
time you solve a problem with vectors, the Ô¨Årst thing you should
do is draw a coordinate system, and think of vector components
as measuring out a distance along this coordinate system.
2.3
Vector products
If addition of two vectors ‚Éóv and ‚Éów corresponds to the addition of their
components (vx +wx, vy +wy, vz +wz), you might logically think that
the product of two vectors will correspond to the product of their com-
ponents (vxwx, vywy, vzwz), however, this way of multiplying vectors
is not used in practice. Instead, we use the dot product and the cross
product.
The dot product tells you how similar two vectors are to each other:
‚Éóv ¬∑ ‚Éów ‚â°vxwx + vywy + vzwz ‚â°‚à•‚Éóv‚à•‚à•‚Éów‚à•cos(œï)
‚ààR,
where œï is the angle between the two vectors. The factor cos(œï) is
largest when the two vectors point in the same direction because the
angle between them will be œï = 0 and cos(0) = 1.

100
VECTORS
The exact formula for the cross product is more complicated so I
will not show it to you just yet. What is important to know is that
the cross product of two vectors is another vector:
‚Éóv √ó ‚Éów = { a vector perpendicular to both ‚Éóv and ‚Éów }
‚ààR3.
If you take the cross product of one vector pointing in the x-direction
with another vector pointing in the y-direction, the result will be a
vector in the z-direction.
Dot product
The dot product takes two vectors as inputs and produces a single real
number as an output:
¬∑ : R3 √ó R3
‚Üí
R.
The dot product between two vectors can be computed using either
the algebraic formula
‚Éóv ¬∑ ‚Éów ‚â°vxwx + vywy + vzwz,
or the geometrical formula
‚Éóv ¬∑ ‚Éów ‚â°‚à•‚Éóv‚à•‚à•‚Éów‚à•cos(œï),
where œï is the angle between the two vectors.
The dot product is also known as the inner product or scalar
product. The name scalar comes from the fact that the result of the
dot product is a scalar number‚Äîa number that does not change when
the basis changes.
We can combine the algebraic and the geometric formulas for the
dot product to obtain the formula
cos(œï) =
‚Éóv ¬∑ ‚Éów
‚à•‚Éóv‚à•‚à•‚Éów‚à•= vxwx + vywy + vzwz
‚à•‚Éóv‚à•‚à•‚Éów‚à•
and
œï = cos‚àí1(cos(œï)).
Thus, it's possible to Ô¨Ånd the angle between two vectors if we know
their components.
The geometric factor cos(œï) depends on the relative orientation of
the two vectors as follows:
‚Ä¢ If the vectors point in the same direction, then
cos(œï) = cos(0‚ó¶) = 1 and so ‚Éóv ¬∑ ‚Éów = ‚à•‚Éóv‚à•‚à•‚Éów‚à•.
‚Ä¢ If the vectors are perpendicular to each other, then
cos(œï) = cos(90‚ó¶) = 0 and so ‚Éóv ¬∑ ‚Éów = ‚à•‚Éóv‚à•‚à•‚Éów‚à•(0) = 0.
‚Ä¢ If the vectors point in exactly opposite directions, then
cos(œï) = cos(180‚ó¶) = ‚àí1 and so ‚Éóv ¬∑ ‚Éów = ‚àí‚à•‚Éóv‚à•‚à•‚Éów‚à•.

2.3
VECTOR PRODUCTS
101
Cross product
The cross product takes two vectors as inputs and produces another
vector as the output:
√ó : R3 √ó R3
‚Üí
R3.
Because the output of this operation is a vector, we sometimes refer
to the cross product as the vector product.
The cross products of individual basis elements are deÔ¨Åned as fol-
lows:
ÀÜƒ± √ó ÀÜÔöæ= ÀÜk,
ÀÜÔöæ√ó ÀÜk = ÀÜƒ±,
ÀÜk √ó ÀÜƒ± = ÀÜÔöæ.
The cross product is anti-symmetric in its inputs, which means swap-
ping the order of the inputs introduces a negative sign in the output:
ÀÜÔöæ√ó ÀÜƒ± = ‚àíÀÜk,
ÀÜk √ó ÀÜÔöæ= ‚àíÀÜƒ±,
ÀÜƒ± √ó ÀÜk = ‚àíÀÜÔöæ.
I bet you had never seen a product like this before. Most likely, the
products you've seen in math have been commutative, which means
the order of the inputs doesn't matter. The product of two numbers is
commutative ab = ba, and the dot product is commutative ‚Éóu¬∑‚Éóv = ‚Éóv¬∑‚Éóu,
but the cross product of two vectors is non-commutative ÀÜƒ± √ó ÀÜÔöæÃ∏= ÀÜÔöæ√óÀÜƒ±.
For two arbitrary vectors ‚Éóa = (ax, ay, az) and ‚Éób = (bx, by, bz), the
cross product is calculated as
‚Éóa √ó‚Éób = (aybz ‚àíazby, azbx ‚àíaxbz, axby ‚àíaybx) .
The cross product's output has a length that is proportional to the
sin of the angle between the vectors:
‚à•‚Éóa √ó‚Éób‚à•= ‚à•‚Éóa‚à•‚à•‚Éób‚à•sin(œï).
The direction of the vector (‚Éóa √ó‚Éób) is perpendicular to both ‚Éóa and ‚Éób.
The right-hand rule
Consider the plane formed by the vectors
‚Éóa and ‚Éób. There are actually two vectors
that are perpendicular to this plane: one
above the plane and one below the plane.
We use the right-hand rule to Ô¨Ågure out
which of these vectors corresponds to the
cross product ‚Éóa √ó‚Éób.
When your index Ô¨Ånger points in the
same direction as the vector ‚Éóa and your
middle Ô¨Ånger points in the direction of ‚Éób,
your thumb will point in the direction of ‚Éóa √ó ‚Éób.
The relationship
encoded in the right-hand rule matches the relationship between the
standard basis vectors: ÀÜƒ± √ó ÀÜÔöæ= ÀÜk.

102
VECTORS
Links
[ A nice illustration of the cross product ]
http://1ucasvb.tumblr.com/post/76812811092/
2.4
Complex numbers
By now, you've heard about complex numbers C. The word "complex"
is an intimidating word. Surely it must be a complex task to learn
about the complex numbers.
That may be true in general, but it
helps if you know about vectors. Complex numbers are similar to two-
dimensional vectors ‚Éóv ‚ààR2. We add and subtract complex numbers
like vectors. Complex numbers also have components, length, and
"direction." If you understand vectors, you will understand complex
numbers at almost no additional mental cost.
We'll begin with a practical problem.
Example
Suppose you're asked to solve the following quadratic equation:
x2 + 1 = 0.
You're looking for a number x, such that x2 = ‚àí1. If you are only
allowed to give real answers (the set of real numbers is denoted R),
then there is no answer to this question. In other words, this equation
has no solutions. Graphically speaking, this is because the quadratic
function f(x) = x2 + 1 does not cross the x-axis.
However, we're not taking no for an answer! If we insist on solving
for x in the equation x2 + 1 = 0, we can imagine a new number i that
satisÔ¨Åes i2 = ‚àí1. We call i the unit imaginary number. The solutions
to the equation are therefore x1 = i and x2 = ‚àíi. There are two
solutions because the equation was quadratic.
We can check that
i2 + 1 = ‚àí1 + 1 = 0 and also (‚àíi)2 + 1 = (‚àí1)2i2 + 1 = i2 + 1 = 0.
Thus, while the equation x2 + 1 = 0 has no real solutions, it does
have solutions if we allow the answers to be imaginary numbers.
DeÔ¨Ånitions
Complex numbers have a real part and an imaginary part:
‚Ä¢ i: the unit imaginary number i ‚â°‚àö‚àí1 or i2 = ‚àí1
‚Ä¢ bi: an imaginary number that is equal to b times i
‚Ä¢ R: the set of real numbers
‚Ä¢ C: the set of complex numbers C = {a + bi | a, b ‚ààR}

2.4
COMPLEX NUMBERS
103
‚Ä¢ z = a + bi: a complex number
‚Ä¢ Re{z} = a: the real part of z
‚Ä¢ Im{z} = b: the imaginary part of z
‚Ä¢ ¬Øz: the complex conjugate of z. If z = a + bi, then ¬Øz = a ‚àíbi.
The polar representation of complex numbers:
‚Ä¢ z = |z|‚à†œïz = |z| cos œïz + i|z| sin œïz
‚Ä¢ |z| = ‚àö¬Øzz =
‚àö
a2 + b2: the magnitude of z = a + bi
‚Ä¢ œïz = tan‚àí1(b/a): the phase or argument of z = a + bi
‚Ä¢ Re{z} = |z| cos œïz
‚Ä¢ Im{z} = |z| sin œïz
Formulas
Addition and subtraction
Just as we performed the addition of vectors component by compo-
nent, we perform addition on complex numbers by adding the real
parts together and adding the imaginary parts together:
(a + bi) + (c + di) = (a + c) + (b + d)i.
Polar representation
We can give a geometrical interpretation of the complex numbers by
extending the real number line into a two-dimensional plane called the
complex plane. The horizontal axis in the complex plane measures the
real part of the number. The vertical axis measures the imaginary
part. Complex numbers are vectors in the complex plane.
Re
Im
0
œïz
|z|
z = a + bi
a
b
It is possible to represent any
complex number z = a + bi in terms
of its magnitude and its phase:
z = |z|‚à†œïz = |z| cos œïz
|
{z
}
a
+ |z| sin œïz
|
{z
}
b
i.
The magnitude of a complex number
z = a + bi is
|z| =
p
a2 + b2.
This corresponds to the length of the
vector that represents the complex
number in the complex plane.
The formula is obtained by using
Pythagoras' theorem.

104
VECTORS
The phase, also known as the argument of the complex number
z = a + bi is
œïz ‚â°arg z = atan2(b,a) =
‚Ä† tan‚àí1(b/a).
The phase corresponds to the angle z forms with the real axis. Note
the equality labelled ‚Ä† is true only when a > 0, because the function
tan‚àí1 always returns numbers in the range [‚àíœÄ
2 , œÄ
2 ]. Manual correc-
tions of the output of tan‚àí1(b/a) are required for complex numbers
with a < 0.
Some programming languages provide the 2-input math function
atan2(y,x) that correctly computes the angle the vector (x, y) makes
with the x-axis in all four quadrants. Complex numbers behave like
2-dimensional vectors so you can use atan2 to compute their phase.
Complex numbers have vector-like properties like magnitude and phase,
but we can also do other operations with them that are not deÔ¨Åned
for vectors. The set of complex numbers C is a Ô¨Åeld. This means,
in addition to the addition and subtraction operations, we can also
perform multiplication and division with complex numbers.
Multiplication
The product of two complex numbers is computed using the usual
rules of algebra:
(a + bi)(c + di) = (ac ‚àíbd) + (ad + bc)i.
In the polar representation, the product formula is
(p‚à†œÜ)(q‚à†œà) = pq‚à†(œÜ + œà).
To multiply two complex numbers, multiply their magnitudes and add
their phases.
Division
Let's look at the procedure for dividing complex numbers:
(a + bi)
(c + di) = (a + bi)
(c + di)
(c ‚àídi)
(c ‚àídi) = (a + bi) (c ‚àídi)
(c2 + d2) = (a + bi) c + di
|c + di|2 .
In other words, to divide the number z by the complex number s,
compute ¬Øs and |s|2 = s¬Øs and then use
z/s = z ¬Øs
|s|2 .
You can think of
¬Øs
|s|2 as being equivalent to s‚àí1.

2.4
COMPLEX NUMBERS
105
Cardano's example
One of the earliest examples of reasoning in-
volving complex numbers was given by Gerolamo Cardano in his 1545
book Ars Magna. Cardano wrote, "If someone says to you, divide 10
into two parts, one of which multiplied into the other shall produce
40, it is evident that this case or question is impossible." We want to
Ô¨Ånd numbers x1 and x2 such that x1 + x2 = 10 and x1x2 = 40. This
sounds kind of impossible. Or is it?
"Nevertheless," Cardano said, "we shall solve it in this fashion:
x1 = 5 +
‚àö
15i and x2 = 5 ‚àí
‚àö
15i."
When you add x1 + x2 you obtain 10. When you multiply the two
numbers the answer is
x1x2 =

5 +
‚àö
15i

5 ‚àí
‚àö
15i

= 25 ‚àí5
‚àö
15i + 5
‚àö
15i ‚àí
‚àö
15
2i2 = 25 + 15 = 40.
Hence 5 +
‚àö
15i and 5 ‚àí
‚àö
15i are two numbers whose sum is 10 and
whose product is 40.
Example 2
Compute the product of i and ‚àí1. Both i and ‚àí1 have
a magnitude of 1 but diÔ¨Äerent phases. The phase of i is œÄ
2 (90‚ó¶), while
‚àí1 has phase œÄ (180‚ó¶). The product of these two numbers is
(i)(‚àí1) = (1‚à†œÄ
2 )(1‚à†œÄ) = 1‚à†( œÄ
2 + œÄ) = 1‚à†3œÄ
2
= ‚àíi.
Multiplication by i is eÔ¨Äectively a rotation by œÄ
2 (90‚ó¶) to the left.
Re
Im
0
r
1
3
z = ‚àí3 ‚àíi
œï
Example 3
Find the polar rep-
resentation of z = ‚àí3‚àíi and com-
pute z6.
Let's denote the polar repre-
sentation of z by z = r‚à†œï. We
Ô¨Ånd r =
‚àö
32 + 12 =
‚àö
10 and
œï = tan‚àí1( 1
3) + œÄ = 0.322 + œÄ.
Using the polar representa-
tion, we can easily compute z6:
z6 = r6‚à†(6œï) = (
‚àö
10)6‚à†6(0.322+œÄ) = 103‚à†1.932+6œÄ = 103‚à†1.932.
Note we can ignore multiples of 2œÄ in the phase. In component form,
z6 is equal to 1000 cos(1.932) + 1000 sin(1.932)i = ‚àí353.4 + 935.5i.

106
VECTORS
Fundamental theorem of algebra
The solutions to any polynomial equation a0 + a1x + ¬∑ ¬∑ ¬∑ + anxn = 0
are of the form
z = a + bi.
In other words, any polynomial P(x) of nth degree can be written as
P(x) = (x ‚àíz1)(x ‚àíz2) ¬∑ ¬∑ ¬∑ (x ‚àízn),
where zi ‚ààC are the polynomial's complex roots. Before today, you
might have said the equation x2 + 1 = 0 has no solutions. Now you
know its solutions are the complex numbers z1 = i and z2 = ‚àíi.
The theorem is "fundamental" because it tells us we won't ever
need to invent any "fancier" set of numbers to solve polynomial equa-
tions. Recall that each set of numbers is associated with a diÔ¨Äerent
class of equations. The natural numbers N appear as solutions of the
equation m + n = x, where m and n are natural numbers (denoted
m, n ‚ààN). The integers Z are the solutions to equations of the form
x + m = n, where m, n ‚ààN. The rational numbers Q are necessary
to solve for x in mx = n, with m, n ‚ààZ. To Ô¨Ånd the solutions of
x2 = 2, we need the real numbers R. The process of requiring new
types of numbers for solving more complicated types of equations
stops at C; any polynomial equation‚Äîno matter how complicated it
is‚Äîhas solutions that are complex numbers C.
Euler's formula
You already know cos Œ∏ is a shifted version of sin Œ∏, so it's clear these
two functions are related. It turns out the exponential function is also
related to sin and cos. Lo and behold, we have Euler's formula:
eiŒ∏ = cos Œ∏ + i sin Œ∏ .
Inputting an imaginary number to the exponential function outputs
a complex number that contains both cos and sin. Euler's formula
gives us an alternate notation for the polar representation of complex
numbers: z = |z|‚à†œïz = |z|eiœïz.
If you want to impress your friends with your math knowledge,
plug Œ∏ = œÄ into the above equation to Ô¨Ånd
eiœÄ = cos(œÄ) + i sin(œÄ) = ‚àí1,
which can be rearranged into the form, eœÄi + 1 = 0. This equation
shows a relationship between the Ô¨Åve most important numbers in all
of mathematics: Euler's number e = 2.71828 . . . , œÄ = 3.14159 . . ., the
imaginary number i, 1, and zero. It's kind of cool to see all these
important numbers reunited in one equation, don't you agree?

2.5
VECTORS PROBLEMS
107
De Moivre's theorem
By replacing Œ∏ in Euler's formula with nŒ∏, we obtain de Moivre's
theorem:
(cos Œ∏ + i sin Œ∏)n = cos nŒ∏ + i sin nŒ∏.
De Moivre's Theorem makes sense if you think of the complex number
z = eiŒ∏ = cos Œ∏ + i sin Œ∏, raised to the nth power:
(cos Œ∏ + i sin Œ∏)n = zn = (eiŒ∏)n = einŒ∏ = cos nŒ∏ + i sin nŒ∏.
Setting n = 2 in de Moivre's formula, we can derive the double angle
formulas (page 64) as the real and imaginary parts of the following
equation:
(cos2 Œ∏ ‚àísin2 Œ∏) + (2 sin Œ∏ cos Œ∏)i = cos(2Œ∏) + sin(2Œ∏)i.
Links
[ Mini tutorial on the complex numbers ]
http://paste.lisp.org/display/133628
2.5
Vectors problems
You learned a bunch of vector formulas and you saw some vector
diagrams, but did you really learn how to solve problems with vectors?
There is only one way to Ô¨Ånd out: test yourself by solving problems.
I've said it before and I don't want to repeat myself too much, but
it's worth saying again: the more problems you solve, the better you'll
understand the material. It's now time for you to try the following
vector problems to make sure you're on top of things.
P2.1
Express the following vectors in length-and-direction notation:
(a) ‚Éóu1 = (0, 5)
(b) ‚Éóu2 = (1, 2)
(c) ‚Éóu3 = (‚àí1, ‚àí2)
P2.2
Express the following vectors as components:
(a) ‚Éóv1 = 20‚à†30‚ó¶
(b) ‚Éóv2 = 10‚à†‚àí90‚ó¶
(c) ‚Éóv3 = 5‚à†150‚ó¶
P2.3
Express the following vectors in terms of unit vectors ÀÜƒ±, ÀÜÔöæ, and ÀÜk:
(a) ‚Éów1 = 10‚à†25‚ó¶
(b) ‚Éów2 = 7‚à†‚àí90‚ó¶
(c) ‚Éów3 = (3, ‚àí2, 3)
P2.4
Given the vectors ‚Éóv1 = (1, 1), ‚Éóv2 = (2, 3), and ‚Éóv3 = 5‚à†30‚ó¶, calculate
the following expressions:

108
VECTORS
(a) ‚Éóv1 + ‚Éóv2
(b) ‚Éóv2 ‚àí2‚Éóv1
(c) ‚Éóv1 + ‚Éóv2 + ‚Éóv3
P2.5
Starting from the point P = (2, 6), the three displacement vectors
shown in Figure 2.2 are applied to obtain the point Q.
What are the
coordinates of the point Q?
P = (2, 6)
(2, 2)
5
30‚ó¶
3
60‚ó¶
Q
Figure 2.2: A point P is displaced by three vectors to obtain point Q.
P2.6
Given the vectors ‚Éóu = (1, 1, 1), ‚Éóv = (2, 3, 1), and ‚Éów = (‚àí1, ‚àí1, 2),
compute the following products:
(1) ‚Éóu ¬∑ ‚Éóv
(2) ‚Éóu ¬∑ ‚Éów
(3) ‚Éóv ¬∑ ‚Éów
(4) ‚Éóu √ó ‚Éóv
(5) ‚Éóu √ó ‚Éów
(6) ‚Éóv √ó ‚Éów
P2.7
Find a unit-length vector that is perpendicular to both ‚Éóu = (1, 0, 1)
and ‚Éóv = (1, 2, 0).
Hint: Use the cross product.
P2.8
Find a vector that is orthogonal to both ‚Éóu1 = (1, 0, 1) and ‚Éóu2 =
(1, 3, 0) and whose dot product with the vector ‚Éóv = (1, 1, 0) is equal to 8.
P2.9
Compute the following expressions:
(a) ‚àö‚àí4
(b) 2 + 3i
2 + 2i
(c) e3i(2 + i)e‚àí3i
P2.10
Solve for x ‚ààC in the following equations:
(a) x2 = ‚àí4
(b) ‚àöx = 4i
(c) x2 + 2x + 2 = 0
(d) x4 + 4x2 + 3 = 0
Hint: To solve (d), use the substitution u = x2.
P2.11
Given the numbers z1 = 2+i, z2 = 2‚àíi, and z3 = ‚àí1‚àíi, compute
(a) |z1|
(b) z1
z3
(c) z1z2z3
P2.12
A real business is a business that is proÔ¨Åtable.
An imaginary
business is an idea that is just turning around in your head. We can model
the real-imaginary nature of a business project by representing the project
state as a complex number p ‚ààC. For example, a business idea is described
by the state po = 100i. In other words, it is 100% imaginary.

2.5
VECTORS PROBLEMS
109
To bring an idea from the imaginary into the real, you must work on it.
We'll model the work done on the project as a multiplication by the complex
number e‚àíiŒ±h, where h is the number of hours of work and Œ± is a constant
that depends on the project. After h hours of work, the initial state of the
project is transformed as follows: pf = e‚àíiŒ±hpi. Working on the project
for one hour "rotates" its state by ‚àíŒ±[rad], making it less imaginary and
more real.
If you start from an idea po = 100i and the cumulative number of hours
invested after t weeks of working on the project is h(t) = 0.2t2, how long
will it take for the project to become 100% real? Assume Œ± = 2.904√ó10‚àí3.
Hint: A project is 100% real if Re{p} = p.


Chapter 3
Intro to linear algebra
The Ô¨Årst two chapters reviewed core ideas of mathematics, along with
basic notions about vectors you may have been exposed to when learn-
ing physics. Now that we're done with the prerequisites, it's time to
dig into the main discussion of linear algebra: the study of vectors
and matrices.
3.1
Introduction
Vectors and matrices are the new objects of study in linear algebra,
and our Ô¨Årst step is to deÔ¨Åne the basic operations we can perform on
them.
We denote the set of n-dimensional vectors as Rn. A vector ‚Éóv ‚ààRn
is an n-tuple of real numbers.1
For example, a three-dimensional
vector is a triple of the form
‚Éóv = (v1, v2, v3)
‚àà
(R, R, R) ‚â°R3.
To specify the vector ‚Éóv, we must specify the values for its three com-
ponents: v1, v2, and v3.
A matrix A ‚ààRm√ón is a rectangular array of real numbers with
m rows and n columns. For example, a 3 √ó 2 matrix looks like this:
A =
Ô£Æ
Ô£∞
a11
a12
a21
a22
a31
a32
Ô£π
Ô£ª‚àà
Ô£Æ
Ô£∞
R
R
R
R
R
R
Ô£π
Ô£ª‚â°R3√ó2.
To specify the matrix A, we need to specify the values of its six
components, a11, a12, . . . , a32.
1The notation "s ‚ààS" is read "s element of S" or "s in S."
111

112
INTRO TO LINEAR ALGEBRA
In the remainder of this chapter we'll learn about the mathemat-
ical operations we can perform on vectors and matrices. Many prob-
lems in science, business, and technology can be described in terms of
vectors and matrices so it's important you understand how to work
with these math objects.
Context
To illustrate what is new about vectors and matrices, let's begin by
reviewing the properties of something more familiar: the set of real
numbers R. The basic operations on numbers are:
‚Ä¢ Addition (denoted +)
‚Ä¢ Subtraction, the inverse of addition (denoted ‚àí)
‚Ä¢ Multiplication (denoted implicitly)
‚Ä¢ Division, the inverse of multiplication (denoted by fractions)
You're familiar with these operations and know how to use them to
evaluate math expressions and solve equations.
You should also be familiar with functions that take real numbers
as inputs and give real numbers as outputs, denoted f : R ‚ÜíR.
Recall that, by deÔ¨Ånition, the inverse function f ‚àí1 undoes the eÔ¨Äect
of f. If you are given f(x) and want to Ô¨Ånd x, you can use the inverse
function as follows: f ‚àí1 (f(x)) = x. For example, the function f(x) =
ln(x) has the inverse f ‚àí1(x) = ex, and the inverse of g(x) = ‚àöx is
g‚àí1(x) = x2.
Vector operations
The operations we can perform on vectors are:
‚Ä¢ Addition (denoted +)
‚Ä¢ Subtraction, the inverse of addition (denoted ‚àí)
‚Ä¢ Scaling
‚Ä¢ Dot product (denoted ¬∑ )
‚Ä¢ Cross product (denoted √ó)
We'll discuss each of these vector operations in Section 3.2. Although
you should already be familiar with vectors and vector operations
from Chapter 2, it's worth revisiting and reviewing these concepts
because vectors are the foundation of linear algebra.

3.1
INTRODUCTION
113
Matrix operations
The mathematical operations deÔ¨Åned for matrices A and B are:
‚Ä¢ Addition (denoted A + B)
‚Ä¢ Subtraction, the inverse of addition (denoted A ‚àíB)
‚Ä¢ Scaling by a constant Œ± (denoted Œ±A)
‚Ä¢ Matrix product (denoted AB)
‚Ä¢ Matrix inverse (denoted A‚àí1)
‚Ä¢ Trace (denoted Tr(A))
‚Ä¢ Determinant (denoted det(A) or |A|)
We'll deÔ¨Åne each of these operations in Section 3.3. We'll learn about
the various computational, geometrical, and theoretical considera-
tions associated with these matrix operations in the remainder of the
book.
Let's now examine one important matrix operation in closer detail:
the matrix-vector product A‚Éóx.
Matrix-vector product
Consider the matrix A ‚ààRm√ón and the vector ‚Éóv ‚ààRn. The matrix-
vector product A‚Éóx produces a linear combination of the columns of
the matrix A with coeÔ¨Écients ‚Éóx. For example, the product of a 3 √ó 2
matrix A and a 2 √ó 1 vector ‚Éóx results in a 3 √ó 1 vector, which we'll
denote ‚Éóy:
‚Éóy = A‚Éóx,
Ô£Æ
Ô£∞
y1
y2
y3
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
a11
a12
a21
a22
a31
a32
Ô£π
Ô£ª
x1
x2

‚â°
Ô£Æ
Ô£∞
x1a11 + x2a12
x1a21 + x2a22
x1a31 + x2a32
Ô£π
Ô£ª‚â°x1
Ô£Æ
Ô£∞
a11
a21
a31
Ô£π
Ô£ª+ x2
Ô£Æ
Ô£∞
a12
a22
a32
Ô£π
Ô£ª
|
{z
}
column picture
.
The key thing to observe in the above formula is the interpretation of
the matrix-vector product in the column picture: ‚Éóy = A‚Éóx = x1A[:,1] +
x2A[:,2], where A[:,1] and A[:,2] are the Ô¨Årst and second columns of A.
For example, if you want to obtain the linear combination consisting
of 3 times the Ô¨Årst column of A and 4 times the second column of A,
you can multiply A by the vector ‚Éóx = [ 3
4 ].
Linear combinations as matrix products
Consider some set of vectors {‚Éóe1,‚Éóe2}, and a third vector ‚Éóy that is a
linear combination of the vectors ‚Éóe1 and ‚Éóe2:
‚Éóy = Œ±‚Éóe1 + Œ≤‚Éóe2.

114
INTRO TO LINEAR ALGEBRA
The numbers Œ±, Œ≤ ‚ààR are the coeÔ¨Écients in this linear combination.
The matrix-vector product is deÔ¨Åned expressly for the purpose
of studying linear combinations. We can describe the above linear
combination as the following matrix-vector product:
‚Éóy =
Ô£Æ
Ô£∞
|
|
‚Éóe1
‚Éóe2
|
|
Ô£π
Ô£ª
Œ±
Œ≤

= E‚Éóx.
The matrix E has ‚Éóe1 and ‚Éóe2 as columns. The dimensions of the matrix
E will be n √ó 2, where n is the dimension of the vectors ‚Éóe1, ‚Éóe2, and ‚Éóy.
Vector functions
Okay dear readers, we've reached the key notion in the study of linear
algebra. This is the crux. The essential Ô¨Åbre. The meat and potatoes.
The main idea. I know you're ready to handle it because you're famil-
iar with functions of a real variable f : R ‚ÜíR, and you just learned
the deÔ¨Ånition of the matrix-vector product (in which the variables
were chosen to subliminally remind you of the standard conventions
for the function input x and the function output y = f(x)). Without
further ado, I present to you the concept of a vector function.
The matrix-vector product corresponds to the abstract notion of a
linear transformation, which is one of the key notions in the study of
linear algebra. Multiplication by a matrix A ‚ààRm√ón can be thought
of as computing a linear transformation TA that takes n-vectors as
inputs and produces m-vectors as outputs:
TA : Rn ‚ÜíRm.
Instead of writing ‚Éóy = TA(‚Éóx) to denote the linear transformation TA
applied to the vector ‚Éóx, we can write ‚Éóy = A‚Éóx. Since the matrix A
has m rows, the result of the matrix-vector product is an m-vector.
Applying the linear transformation TA to the vector ‚Éóx corresponds to
the product of the matrix A and the column vector ‚Éóx. We say TA is
represented by the matrix A.
Inverse
When a matrix A is square and invertible, there exists an
inverse matrix A‚àí1 which undoes the eÔ¨Äect of A to restore the original
input vector:
A‚àí1(A(‚Éóx)) = A‚àí1A‚Éóx = ‚Éóx.
Using the matrix inverse A‚àí1 to undo the eÔ¨Äects of the matrix A is
analogous to using the inverse function f ‚àí1 to undo the eÔ¨Äects of the
function f.

3.1
INTRODUCTION
115
Example 1
Consider the linear transformation that multiplies the
Ô¨Årst components of input vectors by 3 and multiplies the second com-
ponents by 5, as described by the matrix
A =
3
0
0
5

,
A(‚Éóx) =
3
0
0
5
x1
x2

=
3x1
5x2

.
Its inverse is
A‚àí1 =
 1
3
0
0
1
5

,
A‚àí1(A(‚Éóx)) =
 1
3
0
0
1
5
3x1
5x2

=
x1
x2

= ‚Éóx.
The inverse matrix multiplies the Ô¨Årst component by 1
3 and the second
component by 1
5, which eÔ¨Äectively undoes what A did.
Example 2
Things get a little more complicated when matrices
mix the diÔ¨Äerent components of the input vector, as in the following
example:
B =
"
1
2
0
3
#
,
which acts as B(‚Éóx) =
"
1
2
0
3
#"
x1
x2
#
=
"
x1 + 2x2
3x2
#
.
Make sure you understand how to compute B(‚Éóx) ‚â°B‚Éóx using the
column picture of the matrix-vector product.
The inverse of the matrix B is the matrix
B‚àí1 =
"
1
‚àí2
3
0
1
3
#
.
Multiplication by the matrix B‚àí1 is the "undo action" for multiplica-
tion by B:
B‚àí1(B(‚Éóx))=
"
1
‚àí2
3
0
1
3
#"
1
2
0
3
#"
x1
x2
#
=
"
1
‚àí2
3
0
1
3
#"
x1 + 2x2
3x2
#
=
"
x1
x2
#
=‚Éóx.
By deÔ¨Ånition, the inverse A‚àí1 undoes the eÔ¨Äects of the matrix A. The
cumulative eÔ¨Äect of applying A‚àí1 after A is the identity matrix 1,
which has 1s on the diagonal and 0s everywhere else:
A‚àí1A‚Éóx = 1‚Éóx = ‚Éóx
‚áí
A‚àí1A =

1
0
0
1

= 1.
Note that 1‚Éóx = ‚Éóx for any vector ‚Éóx.
We'll discuss matrix inverses and how to compute them in more
detail later (Section 4.5). For now, it's important you know they exist.

116
INTRO TO LINEAR ALGEBRA
The fundamental idea of linear algebra
In the remainder of the book, we'll learn all about the properties of
vectors and matrices. Matrix-vector products play an important role
in linear algebra because of their relation to linear transformations.
Functions are transformations from an input space (the domain)
to an output space (the range). A linear transformation T : Rn ‚ÜíRm
is a "vector function" that takes n-vectors as inputs and produces m-
vectors as outputs.
If the vector function T is linear, the output
‚Éóy = T(‚Éóx) of T applied to ‚Éóx can be computed as the matrix-vector
product AT‚Éóx, for some matrix AT ‚ààRm√ón. We say T is represented
by the matrix AT . The matrix AT is a particular "implementation" of
the abstract linear transformation T. The coeÔ¨Écients of the matrix
AT depend on the basis for the input space and the basis for the
output space.
Equivalently, every matrix A ‚ààRm√ón corresponds to some linear
transformation TA : Rn ‚ÜíRm. What does TA do? We deÔ¨Åne the
action of TA on input ‚Éóx as the matrix-vector product A‚Éóx.
Given the equivalence between matrices and linear transforma-
tions, we can reinterpret the statement "linear algebra is about vectors
and matrices" by saying "linear algebra is about vectors and linear
transformations."
If high school math is about numbers and func-
tions, then linear algebra is about vectors and vector functions. The
action of a function on a number is similar to the action of a linear
transformation (matrix) on a vector:
function f : R ‚ÜíR ‚áîlinear transformation TA : Rn ‚ÜíRm
represented by the matrix A ‚ààRm√ón
input x ‚ààR ‚áîinput ‚Éóx ‚ààRn
output f(x) ‚ààR ‚áîoutput TA(‚Éóx) ‚â°A‚Éóx ‚ààRm
g ‚ó¶f (x) = g(f(x)) ‚áîTB(TA(‚Éóx)) ‚â°BA‚Éóx
function inverse f ‚àí1 ‚áîmatrix inverse A‚àí1
zeros of f ‚áîkernel of TA ‚â°null space of A ‚â°N(A)
image of f ‚áîimage of TA ‚â°column space of A ‚â°C(A)
The above table of correspondences serves as a roadmap for the rest
of the material in this book. There are several new concepts, but not
too many. You can do this!
A good strategy is to try adapting your existing knowledge about
functions to the world of linear transformations. For example, the
zeros of a function f(x) are the set of inputs for which the function's
output is zero. Similarly, the kernel of a linear transformation T is
the set of inputs that T sends to the zero vector. It's really the same
concept; we're just upgrading functions to vector inputs.

3.2
REVIEW OF VECTOR OPERATIONS
117
In Chapter 1, I explained why functions are useful tools for mod-
elling the real world. Well, linear algebra is the "vector upgrade" to
your real-world modelling skills. With linear algebra you'll be able to
model complex relationships between multivariable inputs and mul-
tivariable outputs. To build "modelling skills" you must Ô¨Årst develop
you geometrical intuition about lines, planes, vectors, bases, linear
transformations, vector spaces, vector subspaces, etc.
It's a lot of
work, but the eÔ¨Äort you invest will pay dividends.
Links
[ Linear algebra lecture series by Prof. Strang from MIT ]
bit.ly/1ayRcrj (original source of row and column pictures)
[ A system of equations in the row picture and column picture ]
https://www.youtube.com/watch?v=uNKDw46_Ev4
Exercises
What next?
Let's not get ahead of ourselves and bring geometry, vector spaces,
algorithms, and the applications of linear algebra into the mix all at
once. Instead, let's start with the basics. If linear algebra is about
vectors and matrices, then we'd better deÔ¨Åne vectors and matrices
precisely, and describe the math operations we can perform on them.
3.2
Review of vector operations
In Chapter 2 we described vectors from a practical point of view.
Vectors are useful for describing directional quantities like forces and
velocities in physics.
In this section, we'll describe vectors more
abstractly‚Äîas math objects. After deÔ¨Åning a new mathematical ob-
ject, the next step is to specify its properties and the operations we
can perform on it.
Formulas
Consider the vectors ‚Éóu = (u1, u2, u3) and ‚Éóv = (v1, v2, v3), and an
arbitrary constant Œ± ‚ààR. Vector algebra can be summarized as the
following operations:
‚Ä¢ Œ±‚Éóu ‚â°(Œ±u1, Œ±u2, Œ±u3)
‚Ä¢ ‚Éóu + ‚Éóv ‚â°(u1 + v1, u2 + v2, u3 + v3)

118
INTRO TO LINEAR ALGEBRA
‚Ä¢ ‚Éóu ‚àí‚Éóv ‚â°(u1 ‚àív1, u2 ‚àív2, u3 ‚àív3)
‚Ä¢ ||‚Éóu|| ‚â°
p
u2
1 + u2
2 + u2
3
‚Ä¢ ‚Éóu ¬∑ ‚Éóv ‚â°u1v1 + u2v2 + u3v3
‚Ä¢ ‚Éóu √ó ‚Éóv ‚â°(u2v3 ‚àíu3v2, u3v1 ‚àíu1v3, u1v2 ‚àíu2v1)
In the next few pages we'll see what these operations can do for us.
Notation
The set of real numbers is denoted R. An n-dimensional real vector
consists of n real numbers slapped together in a bracket. We denote
the set of 3-dimensional vectors as (R, R, R) ‚â°R3. Similarly, the set
of n-dimensional real vectors is denoted Rn.
Addition and subtraction
Addition and subtraction take pairs of vectors as inputs and produce
vectors as outputs:
+ : Rn √ó Rn ‚ÜíRn
and
‚àí: Rn √ó Rn ‚ÜíRn.
Addition and subtraction are performed component-wise:
‚Éów = ‚Éóu ¬± ‚Éóv
‚áî
wi = ui ¬± vi,
‚àÄi ‚àà[1, . . . , n].
Scaling by a constant
Scaling is an operation that takes a number and a vector as inputs
and produces a vector output:
scalar-mult : R √ó Rn ‚ÜíRn.
There is no symbol to denote scalar multiplication‚Äîwe just write the
scaling factor in front of the vector and the multiplication is implicit.
‚Éów = (3, 2)
‚Éóu = (1.5, 1)
The scaling factor Œ± multiplying
the vector ‚Éóu is equivalent to Œ± mul-
tiplying each component of the vec-
tor:
‚Éów = Œ±‚Éóu
‚áî
wi = Œ±ui.
For example, choosing Œ± = 2, we
obtain the vector ‚Éów = 2‚Éóu, which is
two times longer than the vector ‚Éóu:
‚Éów = (w1, w2, w3) = (2u1, 2u2, 2u3) = 2(u1, u2, u3) = 2‚Éóu.

3.2
REVIEW OF VECTOR OPERATIONS
119
Vector multiplication
The dot product takes pairs of vectors as inputs and produces real
numbers as outputs:
¬∑ : Rn √ó Rn ‚ÜíR,
‚Éóu ¬∑ ‚Éóv ‚â°
n
X
i=1
uivi.
The dot product is deÔ¨Åned for vectors of any dimension. As long as
two vectors have the same length, we can compute their dot product.
The dot product is the key tool for projections, decompositions,
and calculating orthogonality. It is also known as the scalar product or
the inner product. Applying the dot product to two vectors produces
a scalar number which carries information about how similar the two
vectors are. Orthogonal vectors are not similar at all: no part of one
vector goes in the same direction as the other vector, so their dot
product is zero. For example, ÀÜƒ±¬∑ ÀÜÔöæ= 0. Another notation for the inner
product is ‚ü®‚Éóu,‚Éóv‚ü©‚â°‚Éóu ¬∑ ‚Éóv.
The cross product takes pairs of three-dimensional vectors as
inputs and produces three-dimensional vectors as outputs:
√ó : R3 √ó R3 ‚ÜíR3,
‚Éów = ‚Éóu √ó ‚Éóv
‚áî
w1
=
u2v3 ‚àíu3v2,
w2
=
u3v1 ‚àíu1v3,
w3
=
u1v2 ‚àíu2v1.
The cross product, or vector product as it is sometimes called, is an
operation that computes a vector that is perpendicular to both input
vectors. For example: ÀÜƒ±√óÀÜÔöæ= ÀÜk. Note the cross product is only deÔ¨Åned
for three-dimensional vectors.
Length of a vector
The length of the vector ‚Éóu ‚ààRn is computed as follows:
‚à•‚Éóu‚à•=
q
u2
1 + u2
2 + ¬∑ ¬∑ ¬∑ + u2n =
‚àö
‚Éóu ¬∑ ‚Éóu.
The length of a vector is a nonnegative number that describes the
extent of the vector in space. The notion of length is an n-dimensional
extension of Pythagoras' formula for the length of the hypotenuse in
a right-angle triangle given the lengths of its two sides. The length of
a vector is sometimes called the magnitude or the norm of the vector.
The length of a vector ‚Éóu is denoted ‚à•‚Éóu‚à•or |‚Éóu|2 or sometimes simply u.
There are many mathematical concepts that correspond to the
intuitive notion of length. The formula above computes the Euclid-

120
INTRO TO LINEAR ALGEBRA
ian length (or Euclidian norm) of the vector. Another name for the
Euclidian length is the ‚Ñì2-norm (pronounced ell-two norm2).
Note a vector's length can be computed as the square root of the
dot product of the vector with itself: ‚à•‚Éóv‚à•=
‚àö
‚Éóv ¬∑ ‚Éóv. Indeed, there is a
deep mathematical connection between norms and inner products.
Unit vectors
Given a vector ‚Éóv of any length, we can build a unit vector in the same
direction by dividing ‚Éóv by its own length:
ÀÜv =
‚Éóv
||‚Éóv|| .
Unit vectors are useful in many contexts. When we want to specify a
direction in space, we use a unit vector in that direction.
Projection
Okay, pop-quiz time! Let's see if you remember anything from Chap-
ter 2. Suppose I give you a direction ÀÜd and some vector ‚Éóv, and ask
you how much of ‚Éóv is in the direction ÀÜd? To Ô¨Ånd the answer, you must
compute the dot product:
vd = ÀÜd ¬∑ ‚Éóv ‚â°‚à•ÀÜd‚à•‚à•‚Éóv‚à•cos Œ∏ = 1‚à•‚Éóv‚à•cos Œ∏,
where Œ∏ is the angle between ‚Éóv and ÀÜd. This formula is used in physics
to compute x-components of forces: Fx = ‚ÉóF ¬∑ ÀÜƒ± = ‚à•‚ÉóF‚à•cos Œ∏.
DeÔ¨Åne the projection of a vector ‚Éóv in the ÀÜd direction as follows:
Œ† ÀÜd(‚Éóv) = vd ÀÜd = ( ÀÜd ¬∑ ‚Éóv) ÀÜd.
(3.1)
If the direction is speciÔ¨Åed by a vector ‚Éód that is not of unit length,
then the projection formula becomes
Œ†‚Éód(‚Éóv) =
 ‚Éód ¬∑ ‚Éóv
‚à•‚Éód‚à•2
!
‚Éód.
(3.2)
Division by the length squared transforms the two appearances of the
vector ‚Éód into the unit vectors ÀÜd needed for the projection formula:
Œ† ÀÜd(‚Éóv) =
(‚Éóv ¬∑ ÀÜd)
| {z }
‚à•‚Éóv‚à•cos Œ∏
ÀÜd =
 
‚Éóv ¬∑
‚Éód
‚à•‚Éód‚à•
!
‚Éód
‚à•‚Éód‚à•
=
 
‚Éóv ¬∑ ‚Éód
‚à•‚Éód‚à•2
!
‚Éód = Œ†‚Éód(‚Éóv).
2The name ‚Ñì2-norm refers to the process of squaring each of the vector's com-
ponents, and then taking the square root. Another norm is the ‚Ñì4-norm, deÔ¨Åned
as the fourth root of the sum of the vector's components raised to the fourth
power: |‚Éóu|4 ‚â°
4q
u4
1 + u4
2 + u4
3.

3.3
MATRIX OPERATIONS
121
Remember these projection formulas well because we'll need to use
them several times in this book: when computing projections onto
planes (Section 5.2), when computing coordinates, and when describ-
ing the change-of-basis operation (Section 5.3).
Discussion
This section reviewed the properties of n-dimensional vectors, which
are ordered tuples (lists) of n coeÔ¨Écients. It is important to think of
vectors as whole mathematical objects and not as coeÔ¨Écients. Sure,
all the vector operations boil down to manipulations of their coeÔ¨É-
cients, but vectors are most useful (and best understood) if you think
of them as whole objects that have components, rather than focussing
on their components.
Exercises
3.3
Matrix operations
A matrix is a two-dimensional array (a table) of numbers. Consider
the m by n matrix A ‚ààRm√ón. We denote the matrix as a whole
A and refer to its individual entries as aij, where aij is the entry in
the ith row and the jth column of A. What are the mathematical
operations we can perform on this matrix?
Addition and subtraction
The matrix addition and subtraction operations take pairs of matrices
as inputs and produce matrices as outputs:
+ : Rm√ón √ó Rm√ón ‚ÜíRm√ón
and
‚àí: Rm√ón √ó Rm√ón ‚ÜíRm√ón.
Addition and subtraction are performed component-wise:
C = A ¬± B
‚áî
cij = aij ¬± bij, ‚àÄi ‚àà[1, . . . , m], j ‚àà[1, . . . , n].
For example, addition for 3 √ó 2 matrices is
Ô£Æ
Ô£∞
a11
a12
a21
a22
a31
a32
Ô£π
Ô£ª+
Ô£Æ
Ô£∞
b11
b12
b21
b22
b31
b32
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
a11 + b11
a12 + b12
a21 + b21
a22 + b22
a31 + b31
a32 + b32
Ô£π
Ô£ª.
Matrices must have the same dimensions to be added or subtracted.

122
INTRO TO LINEAR ALGEBRA
Multiplication by a constant
Given a number Œ± and a matrix A, we can scale A by Œ± as follows:
Œ±A = Œ±
Ô£Æ
Ô£∞
a11
a12
a21
a22
a31
a32
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
Œ±a11
Œ±a12
Œ±a21
Œ±a22
Œ±a31
Œ±a32
Ô£π
Ô£ª.
Matrix-vector multiplication
The result of the matrix-vector product between a matrix A ‚ààRm√ón
and a vector ‚Éóv ‚ààRn is an m-dimensional vector:
matrix-vector product : Rm√ón √ó Rn ‚ÜíRm
‚Éów = A‚Éóv
‚áî
wi =
n
X
j=1
aijvj,
‚àÄi ‚àà[1, . . . , m].
For example, the product of a 3 √ó 2 matrix A and the 2 √ó 1 column
vector ‚Éóv results in a 3 √ó 1 vector:
A‚Éóv =
Ô£Æ
Ô£∞
a11
a12
a21
a22
a31
a32
Ô£π
Ô£ª
v1
v2

= v1
Ô£Æ
Ô£∞
a11
a21
a31
Ô£π
Ô£ª+ v2
Ô£Æ
Ô£∞
a12
a22
a32
Ô£π
Ô£ª
|
{z
}
column picture
=
Ô£Æ
Ô£∞
(a11, a12) ¬∑ ‚Éóv
(a21, a22) ¬∑ ‚Éóv
(a31, a32) ¬∑ ‚Éóv
Ô£π
Ô£ª
Ô£º
Ô£Ω
Ô£æ
row picture
=
Ô£Æ
Ô£∞
a11v1 + a12v2
a21v1 + a22v2
a31v1 + a32v2
Ô£π
Ô£ª
‚ààR3√ó1.
Note the two diÔ¨Äerent ways to understand the matrix-vector product:
the column picture and the row picture. In the column picture, the
multiplication of the matrix A by the vector ‚Éóv is a linear combi-
nation of the columns of the matrix: A‚Éóv = v1A[:,1] + v2A[:,2],
where A[:,1] and A[:,2] are the two columns of the matrix A. In the
row picture, multiplication of the matrix A by the vector ‚Éóv produces
a column vector with coeÔ¨Écients equal to the dot products of the
rows of the matrix A with the vector ‚Éóv.
Matrix-matrix multiplication
The matrix product AB of matrices A ‚ààRm√ó‚Ñìand B ‚ààR‚Ñì√ón consists
of computing the dot product between each row of A and each column
of B:
matrix-product : Rm√ó‚Ñì√ó R‚Ñì√ón ‚ÜíRm√ón

3.3
MATRIX OPERATIONS
123
C = AB
‚áî
cij =
‚Ñì
X
k=1
aikbkj, ‚àÄi ‚àà[1, . . . , m], j ‚àà[1, . . . , n].
Ô£Æ
Ô£ØÔ£∞
a11
a12
a21
a22
a31
a32
Ô£π
Ô£∫Ô£ª
"
b11
b12
b21
b22
#
=
Ô£Æ
Ô£ØÔ£∞
a11b11 + a12b21
a11b12 + a12b22
a21b11 + a22b21
a21b12 + a22b22
a31b11 + a32b21
a31b12 + a32b22
Ô£π
Ô£∫Ô£ª‚ààR3√ó2.
Transpose
The transpose matrix AT is deÔ¨Åned by the formula aT
ij = aji. In other
words, we obtain the transpose by "Ô¨Çipping" the matrix through its
diagonal:
T : Rm√ón ‚ÜíRn√óm,
Œ±1
Œ±2
Œ±3
Œ≤1
Œ≤2
Œ≤3
T
=
Ô£Æ
Ô£∞
Œ±1
Œ≤1
Œ±2
Œ≤2
Œ±3
Œ≤3
Ô£π
Ô£ª.
Note that entries on the diagonal of the matrix are not aÔ¨Äected by
the transpose operation.
Properties of the transpose operation
‚Ä¢ (A + B)T = AT + BT
‚Ä¢ (AB)T = BTAT
‚Ä¢ (ABC)T = CTBTAT
‚Ä¢ (AT)‚àí1 = (A‚àí1)T
Vectors as matrices
A vector is a special type of matrix. You can treat a vector ‚Éóv ‚ààRn
either as a column vector (n √ó 1 matrix) or as a row vector (1 √ó n
matrix).
Inner product
Recall the deÔ¨Ånition of the dot product or inner product for vectors:
¬∑ : Rn √ó Rn ‚ÜíR
‚áî
‚Éóu ¬∑ ‚Éóv ‚â°
n
X
i=1
uivi.
If we think of these vectors as column vectors, we can write the dot
product in terms of the matrix transpose operation T and the standard

124
INTRO TO LINEAR ALGEBRA
rules of matrix multiplication:
‚Éóu ¬∑ ‚Éóv ‚â°‚ÉóuT‚Éóv =
u1
u2
u3

Ô£Æ
Ô£∞
v1
v2
v3
Ô£π
Ô£ª= u1v1 + u2v2 + u3v3.
The dot product for vectors is really a special case of matrix multi-
plication. Alternatively, we could say that matrix multiplication is
deÔ¨Åned in terms of the dot product.
Outer product
Consider again two column vectors ‚Éóu and ‚Éóv (n √ó 1 matrices). We
obtain the inner product if we apply the transpose to the Ô¨Årst vector
in the product: ‚ÉóuT‚Éóv ‚â°‚Éóu ¬∑ ‚Éóv. If we instead apply the transpose to
the second vector, we'll obtain the outer product of ‚Éóu and ‚Éóv. The
outer product operation takes pairs of vectors as inputs and produces
matrices as outputs:
outer-product : Rn √ó Rn ‚ÜíRn√ón.
For example, the outer product of two vectors in R3 is
‚Éóu‚ÉóvT =
Ô£Æ
Ô£∞
u1
u2
u3
Ô£π
Ô£ª
v1
v2
v3

=
Ô£Æ
Ô£∞
u1v1
u1v2
u1v3
u2v1
u2v2
u2v3
u3v1
u3v2
u3v3
Ô£π
Ô£ª
‚ààR3√ó3.
Observe that the matrix-matrix product between a 3 √ó 1 matrix and
a 1 √ó 3 matrix results in a 3 √ó 3 matrix.
In Section 5.2 we'll see how the outer product is used to build
projection matrices. For example, the matrix that corresponds to the
projection onto the x-axis is Mx ‚â°ÀÜƒ±ÀÜƒ±T ‚ààR3√ó3. The x-projection of
any vector ‚Éóv is computed as the matrix-vector product, Mx‚Éóv = ÀÜƒ±ÀÜƒ±T‚Éóv =
ÀÜƒ±(ÀÜƒ± ¬∑ ‚Éóv) = vxÀÜƒ±.
Matrix inverse
Multiplying an invertible matrix A by its inverse A‚àí1 produces the
identity matrix: AA‚àí1 = 1 = A‚àí1A.
The identity matrix obeys
1‚Éóv = ‚Éóv for all vectors ‚Éóv. The inverse matrix A‚àí1 undoes whatever A
did. The cumulative eÔ¨Äect of multiplying by A and A‚àí1 is equivalent
to the identity transformation,
A‚àí1(A(‚Éóv)) = (A‚àí1A)‚Éóv = 1‚Éóv = ‚Éóv.
We can think of "Ô¨Ånding the inverse" inv(A) = A‚àí1 as an operation
of the form,
inv : Rn√ón ‚ÜíRn√ón.

3.3
MATRIX OPERATIONS
125
Note that only invertible matrices have an inverse. Some matrices are
not invertible‚Äîthere is no "undo" operation for them. We'll postpone
the detailed discussion of invertibility until Section 6.4.
Properties of matrix inverse operation
‚Ä¢ (A + B)‚àí1 = A‚àí1 + B‚àí1
‚Ä¢ (AB)‚àí1 = B‚àí1A‚àí1
‚Ä¢ (ABC)‚àí1 = C‚àí1B‚àí1A‚àí1
‚Ä¢ (AT)‚àí1 = (A‚àí1)T
The matrix inverse plays the role of "division by the matrix A" in
matrix equations. We'll discuss matrix equations in Section 4.2.
Trace
The trace of an n√ón matrix is the sum of the n values on its diagonal:
Tr : Rn√ón ‚ÜíR,
Tr[A] ‚â°
n
X
i=1
aii.
Properties of the trace operation
‚Ä¢ Tr[Œ±A + Œ≤B] = Œ±Tr[A] + Œ≤Tr[B]
(linear property)
‚Ä¢ Tr[AB] = Tr[BA]
‚Ä¢ Tr[ABC] = Tr[CAB] = Tr[BCA]
(cyclic property)
‚Ä¢ Tr

AT
= Tr[A]
‚Ä¢ Tr[A] = Pn
i=1 Œªi,
where {Œªi} are the eigenvalues of A
Determinant
The determinant of a matrix is a calculation that involves all the
coeÔ¨Écients of the matrix, and whose output is a single number:
det : Rn√ón ‚ÜíR.
The determinant describes the relative geometry of the vectors that
make up the rows of the matrix. More speciÔ¨Åcally, the determinant
of a matrix A tells you the volume of a box with sides given by rows
of A.
The determinant of a 2 √ó 2 matrix is
det(A) = det
a
b
c
d

=

a
b
c
d
 = ad ‚àíbc.

126
INTRO TO LINEAR ALGEBRA
The quantity ad ‚àíbc corresponds to the area of the parallelogram
formed by the vectors (a, b) and (c, d). Observe that if the rows of A
point in the same direction, (a, b) = Œ±(c, d) for some Œ± ‚ààR, then the
area of the parallelogram will be zero. If the determinant of a matrix
is nonzero then the rows the matrix are linearly independent.
Properties of determinants
‚Ä¢ det(AB) = det(A) det(B)
‚Ä¢ det(A) = Qn
i=1 Œªi,
where {Œªi} are the eigenvalues of A
‚Ä¢ det
 AT
= det(A)
‚Ä¢ det
 A‚àí1
=
1
det(A)
Discussion
In the remainder of this book, you'll learn about various algebraic and
geometric interpretations of the matrix operations we deÔ¨Åned in this
section. Understanding vector and matrix operations is essential for
understanding more advanced theoretical topics and the applications
of linear algebra. Seeing all these deÔ¨Ånitions at the same time can
be overwhelming, I know, but the good news is that we've deÔ¨Åned all
the math operations and notation we'll use in the rest of the book. It
could be worse, right?
So far, we've deÔ¨Åned two of the main actors in linear algebra:
vectors and matrices. But the introduction to linear algebra won't be
complete until we introduce linearity. Linearity is the main thread
that runs through all the topics in this book.
Exercises
3.4
Linearity
What is linearity and why do we need to spend an entire course learn-
ing about it? Consider the following arbitrary function that contains
terms with diÔ¨Äerent powers of the input variable x:
f(x) =
a/x
|{z}
one-over-x
+
b
|{z}
constant
+
mx
|{z}
linear term
+
qx2
|{z}
quadratic
+ cx3
|{z}
cubic
.
The term mx is the linear term in this expression‚Äîit contains x to
the Ô¨Årst power. All other terms are nonlinear. In this section we'll
discuss the properties of expressions containing only linear terms.

3.4
LINEARITY
127
Introduction
A single-variable function takes as input a real number x and outputs
a real number y. The signature of this class of functions is
f : R ‚ÜíR.
The most general linear function from R to R looks like this:
y ‚â°f(x) = mx,
where m ‚ààR is called the coeÔ¨Écient of x. The action of a linear
function is to multiply the input by the constant m. So far so good.
Example of composition of linear functions
Given the linear
functions f(x) = 2x and g(y) = 3y, what is the equation of the
function h(x) ‚â°g‚ó¶f (x) = g(f(x))? The composition of the functions
f(x) = 2x and g(y) = 3y is the function h(x) = g(f(x)) = 3(2x) = 6x.
Note the composition of two linear functions is also a linear function.
The coeÔ¨Écient of h is equal to the product of the coeÔ¨Écients of f
and g.
DeÔ¨Ånition
A function f is linear if it satisÔ¨Åes the equation
f(Œ±x1 + Œ≤x2) = Œ±f(x1) + Œ≤f(x2),
for any two inputs x1 and x2 and all constants Œ± and Œ≤.
Linear
functions map a linear combination of inputs to the same linear com-
bination of outputs.
Lines are not linear functions!
Consider the equation of a line:
l(x) = mx + b,
where the constant m corresponds to the slope of the line, and the
constant b ‚â°f(0) is the y-intercept of the line. A line l(x) = mx + b
with b Ã∏= 0 is not a linear function. This logic may seem a bit weird,
but if you don't trust me, you can check for yourself:
l(Œ±x1+Œ≤x2) = m(Œ±x1+Œ≤x2)+b Ã∏= m(Œ±x1)+b+m(Œ≤x2)+b = Œ±l(x1)+Œ≤l(x2).
A function with a linear part plus a constant term is called an aÔ¨Éne
transformation. AÔ¨Éne transformations are cool but a bit oÔ¨Ä-topic,
since the focus in this book will be on linear transformations.

128
INTRO TO LINEAR ALGEBRA
Multivariable functions
The study of linear algebra is the study of all things linear. In par-
ticular, we'll learn how to work with functions that take multiple
variables as inputs. Consider the set of functions that take pairs of
real numbers as inputs and produce real numbers as outputs:
f : R √ó R ‚ÜíR.
The most general linear function of two variables is
f(x, y) = mxx + myy.
You can think of mx as the x-slope and my as the y-slope of the
function. We say mx is the x-coeÔ¨Écient and my the y-coeÔ¨Écient in
the linear expression mxx + myy.
Linear expressions
A linear expression in the variables x1, x2, and x3 has the form,
a1x1 + a2x2 + a3x3,
where a1, a2, and a3 are arbitrary constants.
Note the new ter-
minology, "expr is linear in v," which refers to an expression in
which the variable v is raised to the Ô¨Årst power.
The expression
1
ax1 + b6x2 + ‚àöc x3 contains nonlinear factors ( 1
a, b6, and ‚àöc) but is
a linear expression in the variables x1, x2, and x3.
Linear equations
A linear equation in the variables x1, x2, and x3 has the form
a1x1 + a2x2 + a3x3 = c.
This equation is linear because it contains only linear terms in the
variables x1, x2, and x3.
Example
Linear equations are very versatile. Suppose you know
the following equation describes some real-world phenomenon:
4k ‚àí2m + 8p = 10,
where k, m, and p correspond to three variables of interest. You can
interpret this equation as describing the variable m as a function of
the variables k and p, and rewrite the equation as
m(k, p) = 2k + 4p ‚àí5.

3.4
LINEARITY
129
Using this function, you can predict the value of m given the knowl-
edge of the quantities k and p.
Another option would be to interpret k as a function of m and p:
k(m, p) = 10
4 + m
2 ‚àí2p. This model would be useful if you know the
quantities m and p and want to predict the value of the variable k.
Applications
Geometrical interpretation of linear equations
The linear equation in x and y,
ax + by = c,
b Ã∏= 0,
corresponds to a line with the equation y(x) = mx + y0 in the Carte-
sian plane. The slope of the line is m = ‚àía/b and its y-intercept is
y0 = c/b. The special case when b = 0 corresponds to a vertical line
with equation x = c
a.
The most general linear equation in x, y, and z,
ax + by + cz = d,
corresponds to the equation of a plane in a three-dimensional space.
Assuming c Ã∏= 0, we can rewrite this equation so z (the "height") is a
function of the coordinates x and y: z(x, y) = z0 + mxx + myy. The
slope of the plane in the x-direction is mx = ‚àía
c while the slope in
the y-direction is my = ‚àíb
c. The z-intercept of this plane is z0 = d
c.
First-order approximations
When we use a linear function as a mathematical model for a non-
linear, real-world input-output process, we say the function represents
a linear model or a Ô¨Årst-order approximation for the process. Let's
analyze what this means in more detail, and see why linear models
are so popular in science.
In calculus, we learn that functions can be represented as inÔ¨Ånite
Taylor series:
f(x) = taylor(f(x)) = a0 + a1t + a2t2 + a3t3 + ¬∑ ¬∑ ¬∑ =
‚àû
X
n=0
anxn,
where the coeÔ¨Écient an depends on the nth derivative of f(x). The
Taylor series is only equal to the function f(x) if inÔ¨Ånitely many terms
in the series are calculated. If we sum together only a Ô¨Ånite number
of terms in the series, we obtain a Taylor series approximation. The
Ô¨Årst-order Taylor series approximation to f(x) is
f(x) ‚âàtaylor1(f(x)) = a0 + a1x = f(0) + f ‚Ä≤(0)x.

130
INTRO TO LINEAR ALGEBRA
The above equation describes the best approximation to f(x) near
x = 0, by a line of the form l(x) = mx+b. To build a linear model f(x)
of a real-world process, it is suÔ¨Écient to measure two parameters: the
initial value b ‚â°f(0) and the rate of change m ‚â°f ‚Ä≤(0).
Scientists routinely use linear models because this kind of model
allows for easy parametrization.
To build a linear model, the Ô¨Årst
step is to establish the initial value f(0) by inputting x = 0 to the
process and seeing what comes out. Next, we vary the input by some
amount ‚àÜx and observe the resulting change in the output ‚àÜf. The
rate of change parameter is equal to the change in the output divided
by the change in the input m = ‚àÜf
‚àÜx. Thus, we can obtain the param-
eters of a linear model in two simple steps. In contrast, Ô¨Ånding the
parametrization of nonlinear models is a more complicated task.
For a function F(x, y, z) that takes three variables as inputs, the
Ô¨Årst-order Taylor series approximation is
F(x, y, z) ‚âàb + mxx + myy + mzz.
Except for the constant term, the function has the form of a linear ex-
pression. The "Ô¨Årst-order approximation" to a function of n variables
F(x1, x2, . . . , xn) has the form b + m1x1 + m2x2 + ¬∑ ¬∑ ¬∑ + mnxn.
As in the single-variable case, Ô¨Ånding the parametrization of a mul-
tivariable linear model is a straightforward task. Suppose we want
to model some complicated real-world phenomenon that has n in-
put variables. First, we input only zeros to obtain the initial value
F(0, . . . , 0) ‚â°b.
Next, we go through each of the input variables
one-by-one and measure how a small change in each input ‚àÜxi aÔ¨Äects
the output ‚àÜf. The rate of change with respect to the input xi is
mi = ‚àÜf
‚àÜxi . By combining the knowledge of the initial value b and the
"slopes" with respect to each input parameter, we'll obtain a complete
linear model of the phenomenon.
Discussion
In the next three chapters, we'll learn about new mathematical objects
and mathematical operations. Linear algebra is the study vectors,
matrices, linear transformations, vector spaces, and more abstract
vector-like objects.
The mathematical operations we'll perform on
these objects will be linear: f(Œ±obj1+Œ≤obj2) = Œ±f(obj1)+Œ≤f(obj2).
Linearity is the core assumption of linear algebra.
Exercises
E3.1 Are these expressions linear in the variables x, y, and z?

3.5
OVERVIEW OF LINEAR ALGEBRA
131
a)2x+5y+‚àömz
b)10‚àöx+2(y+z)
c)42x+Œ±2 sin( œÄ
3 )y+z cos( œÄ
3 )
3.5
Overview of linear algebra
In linear algebra, you'll learn new computational techniques and de-
velop new ways of thinking about math. With these new tools, you'll
be able to use linear algebra techniques for many applications. Let's
now look at what lies ahead in the book.
Computational linear algebra
The Ô¨Årst steps toward understanding linear algebra will seem a little
tedious. In Chapter 4 you'll develop basic skills for manipulating vec-
tors and matrices. Matrices and vectors have many components and
performing operations on them involves many arithmetic steps‚Äîthere
is no way to circumvent this complexity. Make sure you understand
the basic algebra rules (how to add, subtract, and multiply vectors
and matrices) because they are a prerequisite for learning more ad-
vanced material.
The good news is, with the exception of your homework assign-
ments and Ô¨Ånal exam, you won't have to carry out matrix algebra by
hand. It is much more convenient to use a computer for large matrix
calculations. For small instances, like 4 √ó 4 matrices, you should be
able to perform all the matrix algebra operations with pen and paper.
The more you develop your matrix algebra skills, the deeper you'll be
able to delve into the advanced topics.
Geometrical linear algebra
So far, we described vectors and matrices as arrays of numbers. This
is Ô¨Åne for the purpose of doing algebra on vectors and matrices, but
this description is not suÔ¨Écient for understanding their geometrical
properties. The components of a vector ‚Éóv ‚ààRn can be thought of
as measuring distances along a coordinate system with n axes. The
vector ‚Éóv can therefore be said to "point" in a particular direction with
respect to the coordinate system. The fun part of linear algebra starts
when you learn about the geometrical interpretation of the algebraic
operations on vectors and matrices.
Consider some unit length vector that speciÔ¨Åes a direction of in-
terest ÀÜr. Suppose we're given some other vector ‚Éóv, and we're asked
to Ô¨Ånd how much of ‚Éóv is in the ÀÜr direction. The answer is computed
using the dot product: vr = ‚Éóv ¬∑ ÀÜr = ‚à•‚Éóv‚à•cos Œ∏, where Œ∏ is the angle
between ‚Éóv and ÀÜr. The technical term for the quantity vr is "the length
of the projection of ‚Éóv in the ÀÜr direction." By "projection," I mean we

132
INTRO TO LINEAR ALGEBRA
ignore all parts of ‚Éóv that are not in the ÀÜr direction. Projections are
used in mechanics to calculate the x- and y-components of forces in
force diagrams. In Section 5.2 we'll learn how to calculate all kinds
of projections using the dot product.
To further consider the geometrical aspects of vector operations,
imagine the following situation. Suppose I gave you two vectors ‚Éóu
and ‚Éóv, and asked you to Ô¨Ånd a third vector ‚Éów that is perpendicular
to both ‚Éóu and ‚Éóv. A priori this sounds like a complicated question to
answer, but in fact the required vector ‚Éów can easily be obtained by
computing the cross product ‚Éów = ‚Éóu √ó ‚Éóv.
In Section 5.1 we'll learn how to describe lines and planes in terms
of points, direction vectors, and normal vectors. Consider the follow-
ing geometric problem: given the equations of two planes in R3, Ô¨Ånd
the equation of the line where the two planes intersect. There is an
algebraic procedure called Gauss-Jordan elimination you can use to
Ô¨Ånd the solution.
The determinant of a matrix has a geometrical interpretation (Sec-
tion 4.4). The determinant tells us something about the relative ori-
entation of the vectors that make up the rows of the matrix. If the
determinant of a matrix is zero, it means the rows are not linearly
independent, in other words, at least one of the rows can be written
in terms of the other rows. Linear independence, as we'll see shortly,
is an important property for vectors to have. The determinant is a
convenient way to test whether a set of vectors are linearly indepen-
dent.
It is important that you visualize each new concept you learn
about.
Always keep a picture in your head of what is going on.
The relationships between two-dimensional vectors can be represented
in vector diagrams. Three-dimensional vectors can be visualized by
pointing pens and pencils in diÔ¨Äerent directions. Most of the intuitions
you build about vectors in two and three dimensions are applicable
to vectors with more dimensions.
Theoretical linear algebra
Linear algebra will teach you how to reason about vectors and ma-
trices in an abstract way. By thinking abstractly, you'll be able to
extend your geometrical intuition of two and three-dimensional prob-
lems to problems in higher dimensions. Much knowledge buzz awaits
you as you learn about new mathematical ideas and develop new ways
of thinking.
You're no doubt familiar with the normal coordinate system made
of two orthogonal axes: the x-axis and the y-axis. A vector ‚Éóv ‚ààR2
is speciÔ¨Åed in terms of its coordinates (vx, vy) with respect to these

3.5
OVERVIEW OF LINEAR ALGEBRA
133
axes. When we say ‚Éóv = (vx, vy), what we really mean is ‚Éóv = vxÀÜƒ±+vyÀÜÔöæ,
where ÀÜƒ± and ÀÜÔöæare unit vectors that point along the x- and y-axes. As
it turns out, we can use many other kinds of coordinate systems to
represent vectors. A basis for R2 is any set of two vectors {ÀÜe1, ÀÜe2}
that allows us to express all vectors ‚Éóv ‚ààR2 as linear combinations of
the basis vectors: ‚Éóv = v1ÀÜe1 + v2ÀÜe2. The same vector ‚Éóv corresponds
to two diÔ¨Äerent coordinate pairs, depending on which basis is used
for the description: ‚Éóv = (vx, vy) in the basis {ÀÜƒ±, ÀÜÔöæ} and ‚Éóv = (v1, v2)
in the basis {ÀÜe1, ÀÜe2}.
We'll learn about bases and their properties
in great detail in the coming chapters. The choice of basis plays a
fundamental role in all aspects of linear algebra.
Bases relate the
real-world to its mathematical representation in terms of vector and
matrix components.
In the text above, I explained that computing the product be-
tween a matrix and a vector A‚Éóx = ‚Éóy can be thought of as a linear
vector function, with input ‚Éóx and output ‚Éóy. Any linear transforma-
tion (Section 6.1) can be represented (Section 6.2) as a multiplication
by a matrix A. Conversely, every m √ó n matrix A ‚ààRm√ón can be
thought of as implementing some linear transformation (vector func-
tion): TA : Rn ‚ÜíRm. The equivalence between matrices and linear
transformations allows us to identify certain matrix properties with
properties of linear transformations. For example, the column space
C(A) of the matrix A (the set of vectors that can be written as a
combination of the columns of A) corresponds to the image space of
the linear transformation TA (the set of possible outputs of TA).
The eigenvalues and eigenvectors of matrices (Section 7.1) allow
us to describe the actions of matrices in a natural way. The set of
eigenvectors of a matrix are special input vectors for which the action
of the matrix is described as a scaling. When a matrix acts on one
of its eigenvectors, the output is a vector in the same direction as
the input vector scaled by a constant. The scaling constant is the
eigenvalue (own value) associated this eigenvector. By specifying all
the eigenvectors and eigenvalues of a matrix, it is possible to obtain a
complete description of what the matrix does. Thinking of matrices in
term of their eigenvalues and eigenvectors is a very powerful technique
for describing their properties and has many applications.
Part of what makes linear algebra so powerful is that linear algebra
techniques can be applied to all kinds of "vector-like" objects. The
abstract concept of a vector space (Section 7.3) captures precisely
what it means for some class of mathematical objects to be "vector-
like."
For example, the set of polynomials of degree at most two,
denoted P2(x), consists of all functions of the form f(x) = a0 + a1x +
a2x2.
Polynomials are vector-like because it's possible to describe
each polynomial in terms of its coeÔ¨Écients (a0, a1, a2). Furthermore,

134
INTRO TO LINEAR ALGEBRA
the sum of two polynomials and the multiplication of a polynomial by
a constant both correspond to vector-like calculations of coeÔ¨Écients.
Once you realize polynomials are vector-like, you'll be able to use
linear algebra concepts like linear independence, dimension, and basis
when working with polynomials.
Useful linear algebra
One of the most useful skills you'll learn in linear algebra is the ability
to solve systems of linear equations. Many real-world problems are
expressed as linear equations in multiple unknown quantities. You
can solve for n unknowns simultaneously if you have a set of n linear
equations that relate the unknowns. To solve this system of equations,
you can use basic techniques such as substitution, subtraction, and
elimination by equating to eliminate the variables one by one (see
Section 1.15), but the procedure will be slow and tedious for many
unknowns. If the system of equations is linear, it can be expressed as
an augmented matrix built from the coeÔ¨Écients in the equations. You
can then use the Gauss-Jordan elimination algorithm to solve for the
n unknowns (Section 4.1). The key beneÔ¨Åt of the augmented matrix
approach is that it allows you to focus on the coeÔ¨Écients without
worrying about the variable names. This saves time when you must
solve for many unknowns.
Another approach for solving n linear
equations in n unknowns is to express the system of equations as a
matrix equation (Section 4.2) and then solve the matrix equation by
computing the matrix inverse (Section 4.5).
In Section 7.6 you'll learn how to decompose a matrix into a prod-
uct of simpler matrices. Matrix decompositions are often performed
for computational reasons: certain problems are easier to solve on a
computer when the matrix is expressed in terms of its simpler con-
stituents. Other decompositions, like the decomposition of a matrix
into its eigenvalues and eigenvectors, give you valuable information
about the properties of the matrix. Google's original PageRank algo-
rithm for ranking webpages by "importance" can be explained as the
search for an eigenvector of a matrix. The matrix in question contains
information about all hyperlinks that exist between webpages. The
eigenvector we're looking for corresponds to a vector that describes
the relative importance of each page. So when I tell you eigenvec-
tors are valuable information, I am not kidding: a 350-billion dollar
company started as an eigenvector idea.
The techniques of linear algebra Ô¨Ånd applications in many areas of
science and technology. We'll discuss applications such as modelling
multidimensional real-world problems, Ô¨Ånding approximate solutions
to equations (curve Ô¨Åtting), solving constrained optimization prob-

3.6
INTRODUCTORY PROBLEMS
135
lems using linear programming, and many other in Chapter 8. As a
special bonus for readers interested in physics, a short introduction
to quantum mechanics can be found in Chapter 10; if you have a
good grasp of linear algebra, you can understand matrix quantum
mechanics at no additional mental cost.
Our journey of all things linear begins with the computational
aspects of linear algebra. In Chapter 4 we'll learn how to eÔ¨Éciently
solve large systems of linear equations, practice computing matrix
products, discuss matrix determinants, and compute matrix inverses.
3.6
Introductory problems
Before we continue with the new material in linear algebra, we want
to make sure you got the deÔ¨Ånitions down straight.
linearity, vector operations, matrix operations, etc.
P3.1
Find the sum of the vectors (1, 0, 1) and the vector (0, 2, 2).
Hint: Vector addition is performed element-wise.
P3.2
You friend is taking a physics class and needs some help with a
vector question. Can you help your friend answer this question: Let |a‚ü©=
1|0‚ü©+ 3|1‚ü©and |b‚ü©= 4|0‚ü©‚àí1|1‚ü©. Find |a‚ü©+ |b‚ü©.
Hint: The weird angle-bracket notation denotes basis vectors: |x‚ü©‚â°‚Éóex.
P3.3
Given ‚Éóv = (2, ‚àí1, 3) and ‚Éów = (1, 0, 1), compute the following vector
products: a) ‚Éóv ¬∑ ‚Éów; b) ‚Éóv √ó ‚Éów; c) ‚Éóv √ó ‚Éóv; d) ‚Éów √ó ‚Éów.
P3.4
Given unit vectors ÀÜƒ± = (1, 0, 0), ÀÜÔöæ= (0, 1, 0) and ÀÜk = (0, 0, 1). Find
the following cross products: a) ÀÜƒ± √ó ÀÜƒ±;
b) ÀÜƒ± √ó ÀÜÔöæ;
c) (‚àíÀÜƒ±) √ó ÀÜk + ÀÜÔöæ√ó ÀÜƒ±;
d) ÀÜk √ó ÀÜÔöæ+ ÀÜƒ± √ó ÀÜƒ± + ÀÜÔöæ√ó ÀÜk + ÀÜÔöæ√ó ÀÜƒ±.


Chapter 4
Computational linear
algebra
This chapter covers the computational aspects of performing matrix
calculations. Understanding matrix computations is important be-
cause all later chapters depend on them. Suppose we're given a huge
matrix A ‚ààRn√ón with n = 1000. Hidden behind the innocent-looking
mathematical notation of the matrix inverse A‚àí1, the matrix prod-
uct AA, and the matrix determinant |A|, lie monster computations
involving all the 1000 √ó 1000 = 1 million entries of the matrix A.
Millions of arithmetic operations must be performed... so I hope you
have at least a thousand pencils ready!
Okay, calm down. I won't actually make you calculate millions of
arithmetic operations. In fact, to learn linear algebra, it is suÔ¨Écient to
know how to carry out calculations with 3√ó3 and 4√ó4 matrices. Even
for such moderately sized matrices, computing products, inverses, and
determinants by hand are serious computational tasks. If you're ever
required to take a linear algebra Ô¨Ånal exam, you need to make sure
you can do these calculations quickly. Even if no exam looms in your
imminent future, it's important to practice matrix operations by hand
to get a feel for them.
This chapter will introduce you to the following computational
tasks involving matrices:
Gauss-Jordan elimination
Suppose we're trying to solve two equa-
tions in two unknowns x and y:
ax + by = c,
dx + ey = f.
137

138
COMPUTATIONAL LINEAR ALGEBRA
If we add Œ±-times the Ô¨Årst equation to the second equation, we obtain
an equivalent system of equations:
ax +
by =
c
(d + Œ±a)x + (e + Œ±b)y = f + Œ±c.
This is called a row operation: we added Œ±-times the Ô¨Årst row to the
second row. Row operations change the coeÔ¨Écients of the system of
equations, but leave the solution unchanged. Gauss-Jordan elimina-
tion is a systematic procedure for solving systems of linear equations
using row operations.
Matrix product
The product AB between matrices A ‚ààRm√ó‚Ñì
and B ‚ààR‚Ñì√ón is the matrix C ‚ààRm√ón whose coeÔ¨Écients cij are
deÔ¨Åned by the formula cij = P‚Ñì
k=1 aikbkj for all i ‚àà[1, . . . , m] and
j ‚àà[1, . . . , n]. In Section 4.3 we'll unpack this formula and learn about
its intuitive interpretation: that computing C = AB is computing all
the dot products between the rows of A and the columns of B.
Determinant
The determinant of a matrix A, denoted |A|, is an
operation that gives us useful information about the linear indepen-
dence of the rows of the matrix. The determinant is connected to
many notions of linear algebra: linear independence, geometry of vec-
tors, solving systems of equations, and matrix invertibility.
We'll
discuss these aspects of determinants in Section 4.4.
Matrix inverse
In Section 4.5 we'll build upon our knowledge of
Gauss-Jordan elimination, matrix products, and determinants to de-
rive three diÔ¨Äerent procedures for computing the matrix inverse A‚àí1.
4.1
Reduced row echelon form
In this section we'll learn to solve systems of linear equations using
the Gauss-Jordan elimination procedure. A system of equations can
be represented as a matrix of coeÔ¨Écients. The Gauss-Jordan elimina-
tion procedure converts any matrix into its reduced row echelon form
(RREF). We can easily Ô¨Ånd the solution (or solutions) of the system
of equations from the RREF.
Listen up: the material covered in this section requires your full-
on, caÔ¨Äeinated attention, as the procedures you'll learn are somewhat
tedious. Gauss-Jordan elimination involves many repetitive mathe-
matical manipulations of arrays of numbers. It's important you hang
in there and follow through the step-by-step manipulations, as well

4.1
REDUCED ROW ECHELON FORM
139
as verify each step I present on your own with pen and paper. Don't
just take my word for it‚Äîalways verify the steps!
Solving equations
Suppose you're asked to solve the following system of equations:
1x1 + 2x2
=
5
3x1 + 9x2
=
21.
The standard approach is to use one of the equation-solving tricks we
learned in Section 1.15 to combine the equations and Ô¨Ånd the values
of the two unknowns x1 and x2.
Observe that the names of the two unknowns are irrelevant to the
solution of the system of equations. Indeed, the solution (x1, x2) to
the above system of equations is the same as the solution (s, t) to the
system of equations
1s + 2t = 5
3s + 9t = 21.
The important parts of a system of linear equations are the coeÔ¨Écients
in front of the variables and the constants on the right-hand side of
each equation.
Augmented matrix
The system of linear equations can be written as an augmented matrix:
 1
2
5
3
9
21

.
The Ô¨Årst column corresponds to the coeÔ¨Écients of the Ô¨Årst variable,
the second column is for the second variable, and the last column
corresponds to the constants of the right-hand side. It is customary
to draw a vertical line where the equal signs in the equations would
normally appear. This line helps distinguish the coeÔ¨Écients of the
equations from the column of constants on the right-hand side.
Once we have the augmented matrix, we can simplify it by us-
ing row operations (which we'll discuss shortly) on its entries. After
simpliÔ¨Åcation by row operations, the augmented matrix will be trans-
formed to
 1
0
1
0
1
2

,

140
COMPUTATIONAL LINEAR ALGEBRA
which corresponds to the system of equations
x1
=
1
x2
=
2.
This is a trivial system of equations; there is nothing left to solve and
we can see the solutions are x1 = 1 and x2 = 2. This example illus-
trates the general idea of the Gauss-Jordan elimination procedure for
solving systems of equations by manipulating an augmented matrix.
Row operations
We can manipulate the rows of an augmented matrix without chang-
ing its solutions. We're allowed to perform the following three types
of row operations:
‚Ä¢ Add a multiple of one row to another row
‚Ä¢ Swap the position of two rows
‚Ä¢ Multiply a row by a constant
Let's trace the sequence of row operations needed to solve the system
of equations
1x1 + 2x2 = 5
3x1 + 9x2 = 21,
starting from its augmented matrix:
 1
2
5
3
9
21

.
1. As a Ô¨Årst step, we eliminate the Ô¨Årst variable in the second row
by subtracting three times the Ô¨Årst row from the second row:

1
2
5
0
3
6

.
We denote this row operation as R2 ‚ÜêR2 ‚àí3R1.
2. To simplify the second row, we divide it by 3 to obtain
 1
2
5
0
1
2

.
This row operation is denoted R2 ‚Üê1
3R2.

4.1
REDUCED ROW ECHELON FORM
141
3. The Ô¨Ånal step is to eliminate the second variable from the Ô¨Årst
row. We do this by subtracting two times the second row from
the Ô¨Årst row, R1 ‚ÜêR1 ‚àí2R2:
 1
0
1
0
1
2

.
We can now read oÔ¨Äthe solution: x1 = 1 and x2 = 2.
Note how we simpliÔ¨Åed the augmented matrix through a speciÔ¨Åc pro-
cedure: we followed the Gauss-Jordan elimination algorithm to bring
the matrix into its reduced row echelon form.
The reduced row echelon form (RREF) is the simplest form for an
augmented matrix. Each row contains a leading one (a numeral 1)
also known as a pivot. Each column's pivot is used to eliminate the
numbers that lie below and above it in the same column. The end
result of this procedure is the reduced row echelon form:
Ô£Æ
Ô£∞
1
0
‚àó
0
‚àó
0
1
‚àó
0
‚àó
0
0
0
1
‚àó
Ô£π
Ô£ª.
Note the matrix contains only zero entries below and above the pivots.
The asterisks ‚àódenote arbitrary numbers that could not be eliminated
because no leading one is present in these columns.
DeÔ¨Ånitions
‚Ä¢ The solution to a system of linear equations in the variables
x1, x2, . . . , xn is the set of values {(x1, x2, . . . , xn)} that satisfy
all the equations.
‚Ä¢ The pivot for row j of a matrix is the left-most nonzero entry
in the row j. Any pivot can be converted into a leading one by
an appropriate scaling of that row.
‚Ä¢ Gaussian elimination is the process of bringing a matrix into
row echelon form.
‚Ä¢ A matrix is said to be in row echelon form (REF) if all entries
below the leading ones are zero. This form can be obtained by
adding or subtracting the row with the leading one from the
rows below it.
‚Ä¢ Gaussian-Jordan elimination is the process of bringing a matrix
into reduced row echelon form.
‚Ä¢ A matrix is said to be in reduced row echelon form (RREF) if
all the entries below and above the pivots are zero. Starting
from the REF, we obtain the RREF by subtracting the row
containing the pivots from the rows above them.

142
COMPUTATIONAL LINEAR ALGEBRA
‚Ä¢ rank(A): the rank of the matrix A is the number of pivots in
the RREF of A.
Gauss-Jordan elimination algorithm
The Gauss-Jordan elimination algorithm proceeds in two phases: a
forward phase in which we move left to right, and a backward phase
in which we move right to left.
1. Forward phase (left to right):
1.1 Obtain a pivot (a leading one) in the leftmost column.
1.2 Subtract the row with the pivot from all rows below it to
obtain zeros in the entire column.
1.3 Look for a leading one in the next column and repeat.
2. Backward phase (right to left):
2.1 Find the rightmost pivot and use it to eliminate all num-
bers above the pivot in its column.
2.2 Move one column to the left and repeat.
Example
We're asked to solve the following system of equations:
1x + 2y + 3z = 14
2x + 5y + 6z = 30
‚àí1x + 2y + 3z = 12.
The Ô¨Årst step toward the solution is to build the augmented matrix
that corresponds to this system of equations:
Ô£Æ
Ô£∞
1
2
3
14
2
5
6
30
‚àí1
2
3
12
Ô£π
Ô£ª.
We can now start the left-to-right phase of the algorithm:
1. Conveniently, there is a leading one at the top of the leftmost
column.
If a zero were there instead, a row-swap operation
would be necessary to obtain a nonzero entry.
2. The next step is to clear the entries in the entire column below
this pivot. The row operations we'll use for this purpose are
R2 ‚ÜêR2 ‚àí2R1 and R3 ‚ÜêR3 + R1:
Ô£Æ
Ô£∞
1
2
3
14
0
1
0
2
0
4
6
26
Ô£π
Ô£ª.

4.1
REDUCED ROW ECHELON FORM
143
3. We now shift our attention to the second column. Using the
leading one for the second column, we set the number in the
column below it to zero using R3 ‚ÜêR3 ‚àí4R2. The result is
Ô£Æ
Ô£∞
1
2
3
14
0
1
0
2
0
0
6
18
Ô£π
Ô£ª.
4. Next, we move to the third column. Instead of a leading one,
we Ô¨Ånd it contains a "leading six," which we can convert to a
leading one using R3 ‚Üê1
6R3. We thus obtain
Ô£Æ
Ô£∞
1
2
3
14
0
1
0
2
0
0
1
3
Ô£π
Ô£ª.
The forward phase of the Gauss-Jordan elimination procedure is now
complete. We identiÔ¨Åed three pivots and used them to systematically
set all entries below each pivot to zero. The matrix is now in row
echelon form.
Next we start the backward phase of the Gauss-Jordan elimination
procedure, during which we'll work right-to-left to set all numbers
above each pivot to zero:
5. The Ô¨Årst row operation is R1 ‚ÜêR1 ‚àí3R3, and it leads to
Ô£Æ
Ô£∞
1
2
0
5
0
1
0
2
0
0
1
3
Ô£π
Ô£ª.
6. The Ô¨Ånal step is R1 ‚ÜêR1 ‚àí2R2, which gives
Ô£Æ
Ô£∞
1
0
0
1
0
1
0
2
0
0
1
3
Ô£π
Ô£ª.
The matrix is now in reduced row echelon form and we can see the
solution is x = 1, y = 2, and z = 3.
We've described the general idea of the Gauss-Jordan elimination
and explored some examples where the solutions to the system of
equations were unique. There are other possibilities for the solutions
of a system of linear equations. We'll describe these other possible
scenarios next.

144
COMPUTATIONAL LINEAR ALGEBRA
Number of solutions
A system of three linear equations in three variables could have:
‚Ä¢ One solution. If the RREF of a matrix has a pivot in each
row, we can read oÔ¨Äthe values of the solution by inspection:
Ô£Æ
Ô£∞
1
0
0
c1
0
1
0
c2
0
0
1
c3
Ô£π
Ô£ª.
The unique solution is x1 = c1, x2 = c2, and x3 = c3.
‚Ä¢ InÔ¨Ånitely many solutions 1. If one of the equations is redun-
dant, a row of zeros will appear when the matrix is brought to
the RREF. This happens when one of the original equations is a
linear combination of the other two. In such cases, we're really
solving two equations in three variables, so can't "pin down" one
of the unknown variables. We say the solution contains a free
variable. For example, consider the following RREF:
Ô£Æ
Ô£∞
1
0
a1
c1
0
1
a2
c2
0
0
0
0
Ô£π
Ô£ª.
The column that doesn't contain a leading one corresponds to
the free variable. To indicate that x3 is a free variable, we give
it a special label, x3 ‚â°t. The variable t could be any number
t ‚ààR. In other words, when we say t is free, it means t can
take on any value from ‚àí‚àûto +‚àû. The information in the
augmented matrix can now be used to express x1 and x2 in
terms of the right-hand constants and the free variable t:
Ô£±
Ô£≤
Ô£≥
x1 = c1 ‚àía1 t
x2 = c2 ‚àía2 t
x3 = t
,
‚àÄt ‚ààR
Ô£º
Ô£Ω
Ô£æ=
Ô£±
Ô£≤
Ô£≥
Ô£Æ
Ô£∞
c1
c2
0
Ô£π
Ô£ª+ t
Ô£Æ
Ô£∞
‚àía1
‚àía2
1
Ô£π
Ô£ª,
‚àÄt ‚ààR
Ô£º
Ô£Ω
Ô£æ.
The solution corresponds to the equation of a line passing through
the point (c1, c2, 0) with direction vector (‚àía1, ‚àía2, 1). We'll
discuss the geometry of lines in Chapter 5. For now, it's im-
portant you understand that a system of equations can have
more than one solution; any point on the line ‚Ñì‚â°{(c1, c2, 0) +
t(‚àía1, ‚àía2, 1), ‚àÄt ‚ààR} is a solution to the above system of
equations.
‚Ä¢ InÔ¨Ånitely many solutions 2. It's also possible to obtain a
two-dimensional solution space. This happens when two of the

4.1
REDUCED ROW ECHELON FORM
145
three equations are redundant.
In this case, there will be a
single leading one, and thus two free variables. For example, in
the RREF
Ô£Æ
Ô£∞
0
1
a2
c2
0
0
0
0
0
0
0
0
Ô£π
Ô£ª,
the variables x1 and x3 are free. As in the previous inÔ¨Ånitely-
many-solutions case, we deÔ¨Åne new labels for the free variables
x1 ‚â°s and x3 ‚â°t, where s ‚ààR and t ‚ààR are two arbitrary
numbers. The solution to this system of equations is
Ô£±
Ô£≤
Ô£≥
x1 = s
x2 = c2 ‚àía2t,
x3 = t
‚àÄs, t ‚ààR
Ô£º
Ô£Ω
Ô£æ=
Ô£±
Ô£≤
Ô£≥
Ô£Æ
Ô£∞
0
c2
0
Ô£π
Ô£ª+ s
Ô£Æ
Ô£∞
1
0
0
Ô£π
Ô£ª+ t
Ô£Æ
Ô£∞
0
‚àía2
1
Ô£π
Ô£ª, ‚àÄs, t ‚ààR
Ô£º
Ô£Ω
Ô£æ.
This solution set corresponds to the parametric equation of a
plane that contains the point (0, c2, 0) and the vectors (1, 0, 0)
and (0, ‚àía2, 1).
The general equation for the solution plane is 0x+1y+a2z = c2,
as can be observed from the Ô¨Årst row of the augmented matrix.
In Section 5.1 we'll learn more about the geometry of planes and
how to convert between their general and parametric forms.
‚Ä¢ No solutions. If there are no numbers (x1, x2, x3) that simul-
taneously satisfy all three equations, the system of equations
has no solution. An example of a system of equations with no
solution is the pair s + t = 4 and s + t = 44. There are no
numbers (s, t) that satisfy both of these equations.
A system of equations has no solution if its reduced row echelon
form contains a row of zero coeÔ¨Écients with a nonzero constant
in the right-hand side:
Ô£Æ
Ô£∞
1
0
0
c1
0
1
0
c2
0
0
0
c3
Ô£π
Ô£ª.
If c3 Ã∏= 0, this system of equations is impossible to satisfy. There
is no solution because there are no numbers (x1, x2, x3) such that
0x1 + 0x2 + 0x3 = c3.
Dear reader, we've reached the Ô¨Årst moment in this book where you'll
need to update your math vocabulary. The solution to an individual
equation is a Ô¨Ånite set of points. The solution to a system of equations
can be an entire space containing inÔ¨Ånitely many points, such as a
line or a plane. Please update the deÔ¨Ånition of the term solution to
include the new, more speciÔ¨Åc term solution set‚Äîthe set of points

146
COMPUTATIONAL LINEAR ALGEBRA
that satisfy the system of equations. The solution set of a system of
three linear equations in three unknowns could be either the empty
set {‚àÖ} (no solution), a set with one element {(x1, x2, x3)}, or a set
with inÔ¨Ånitely many elements like a line {po + t ‚Éóv, t ‚ààR} or a plane
{po + s‚Éóv + t ‚Éów, s, t ‚ààR}. Another possible solution set is all of R3;
every vector ‚Éóx ‚ààR3 is a solution to the equation:
Ô£Æ
Ô£∞
0
0
0
0
0
0
0
0
0
Ô£π
Ô£ª
Ô£Æ
Ô£∞
x1
x2
x3
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
0
0
0
Ô£π
Ô£ª.
Note the distinction between the three types of inÔ¨Ånite solution sets.
A line is one-dimensional, a plane is two-dimensional, and R3 is three-
dimensional.
Describing all points on a line requires one parame-
ter, describing all points on a plane takes two parameters, and‚Äîof
course‚Äîdescribing a point in R3 takes three parameters.
Geometric interpretation
We can gain some intuition about solution sets by studying the ge-
ometry of the intersections of lines in R2 and planes in R3.
Lines in two dimensions
Equations of the form ax + by = c correspond to lines in R2. Solving
systems of equations of the form
a1x + b1y
=
c1
a2x + b2y
=
c2
requires Ô¨Ånding the point (x, y) ‚ààR2 where these lines intersect.
There are three possibilities for the solution set:
‚Ä¢ One solution if the two lines intersect at a point.
‚Ä¢ InÔ¨Ånitely many solutions if the lines are superimposed.
‚Ä¢ No solution if the two lines are parallel and never intersect.
Planes in three dimensions
Equations of the form ax + by + cz = d correspond to planes in R3.
When solving three such equations,
a1x + b1y + c1z
=
d1,
a2x + b2y + c2z
=
d2,
a3x + b3y + c3z
=
d3,
we want to Ô¨Ånd a set of points (x, y, z) that satisfy all three equations
simultaneously. There are four possibilities for the solution set:

4.1
REDUCED ROW ECHELON FORM
147
‚Ä¢ One solution. Three non-parallel planes intersect at a point.
‚Ä¢ InÔ¨Ånitely many solutions 1. If one of the plane equations is
redundant, the solution corresponds to the intersection of two
planes. Two non-parallel planes intersect on a line.
‚Ä¢ InÔ¨Ånitely many solutions 2.
If two of the equations are
redundant, then the solution space is a two-dimensional plane.
‚Ä¢ No solution. If two (or more) of the planes are parallel, they
will never intersect.
Computer power
The computer algebra system at live.sympy.org can be used to
compute the reduced row echelon form of any matrix.
Here is an example of how to create a sympy Matrix object:
>>> from sympy.matrices import Matrix
>>> A = Matrix([[1, 2,
5],
# use SHIFT+ENTER for newline
[3, 9, 21]])
In Python, we deÔ¨Åne lists using the square brackets [ and ]. A matrix
is deÔ¨Åned as a list of lists.
To compute the reduced row echelon form of A, call its rref()
method:
>>> A.rref()
( [1, 0, 1] # RREF of A
# locations of pivots
[0, 1, 2],
[0, 1]
)
The rref() method returns a tuple containing the RREF of A and
an array that tells us the 0-based indices of the columns that contain
leading ones. Usually, we'll want to Ô¨Ånd the RREF of A and ignore
the pivots; to obtain the RREF without the pivots, select the Ô¨Årst
(index zero) element in the result of A.rref():
>>> Arref = A.rref()[0]
>>> Arref
[1, 0, 1]
[0, 1, 2]
Using the rref() method is the fastest way to obtain the reduced row
echelon form of a SymPy matrix. The computer will apply the Gauss-
Jordan elimination procedure for you and show you the answer. If
you want to see the intermediary steps of the elimination procedure,
you can also manually apply row operations to the matrix.

148
COMPUTATIONAL LINEAR ALGEBRA
Example
Let's compute the reduced row echelon form of the same
augmented matrix by using row operations:
>>> A = Matrix([[1, 2,
5],
[3, 9, 21]])
>>> A[1,:] = A[1,:] - 3*A[0,:]
>>> A
[1, 2, 5]
[0, 3, 6]
The notation A[i,:] is used to refer to entire rows of the matrix.
The Ô¨Årst argument speciÔ¨Åes the 0-based row index: the Ô¨Årst row of A
is A[0,:] and the second row is A[1,:]. The code example above
implements the row operation R2 ‚ÜêR2 ‚àí3R1.
To obtain the reduced row echelon form of the matrix A, we carry
out two more row operations, R2 ‚Üê1
3R2 and R1 ‚ÜêR1 ‚àí2R2, using
the following commands:
>>> A[1,:] = S(1)/3*A[1,:]
>>> A[0,:] = A[0,:] - 2*A[1,:]
>>> A
[1, 0, 1]
# the same result as A.rref()[0]
[0, 1, 2]
Note we represented the fraction
1
3 as S(1)/3 to obtain the exact
rational expression Rational(1,3). If we were to input 1
3 as 1/3,
Python will interpret this either as integer or Ô¨Çoating point division,
which is not what we want.
The single-letter helper function S is
an alias for the function sympify, which ensures a SymPy object is
produced. Another way to input the exact fraction 1
3 is S('1/3').
In case you need to swap two rows of a matrix, you can use the
standard Python tuple assignment syntax. To swap the position of
the Ô¨Årst and second rows, use
>>> A[0,:], A[1,:] = A[1,:], A[0,:]
>>> A
[0, 1, 2]
[1, 0, 1]
Using row operations to compute the reduced row echelon form of a
matrix allows you want to see the intermediary steps of a calcula-
tion; for instance, when checking the correctness of your homework
problems.
There are other applications of matrix methods that use row op-
erations (see Section 8.6), so it's good idea to know how to use SymPy
for this purpose.

4.1
REDUCED ROW ECHELON FORM
149
Discussion
In this section, we learned the Gauss-Jordan elimination procedure
for simplifying matrices, which just so happens to be one of the most
important computational tools of linear algebra.
Beyond being a
procedure for Ô¨Ånding solutions to systems of linear equations, the
Gauss-Jordan elimination algorithms can be used to solve a broad
range of other linear algebra problems. Later in the book, we'll use
the Gauss-Jordan elimination algorithm to compute inverse matrices
(Section 4.5) and to "distill" bases for vector spaces (Section 5.5).
Exercises
E4.1 Consider the systems of equations below and its augmented
matrix representation:
3x
+ 3y
=
6
2x
+
3
2y
=
5
‚áí
 3
3
6
2
3
2
5

.
Find the solution to this system of equations by bringing the aug-
mented matrix into reduced row echelon form.
E4.2 Repeat E4.1 using the calculator at http://live.sympy.org.
First deÔ¨Åne the augmented matrix using
>>> A = Matrix([
[3,
3,
6],
[2,
S(3)/2,
5]])
# note use of S(3)/2 to obtain 3/2
Perform row operations using SymPy to bring the matrix into RREF.
ConÔ¨Årm your answer using the direct method A.rref().
E4.3 Find the solutions to the systems of equations that correspond
to the following augmented matrices:
a)
 3
3
6
1
1
5

b)
 3
3
6
2
3
2
3

c)
 3
3
6
1
1
2

Hint: The third system of equations has a lot of solutions.
In this section we learned a practical computational algorithm for
solving systems of equations by a using row operations on an aug-
mented matrix. In the next section, we'll increase the level of ab-
straction: by "zooming out" one level, we can view the entire system
of equations as a matrix equation A‚Éóx = ‚Éób and solve the problem in
one step: ‚Éóx = A‚àí1‚Éób.

150
COMPUTATIONAL LINEAR ALGEBRA
4.2
Matrix equations
We can express the problem of solving a system of linear equations as
a matrix equation and obtain the solution using the matrix inverse.
Consider the following system of linear equations:
x1 + 2x2 = 5
3x2 + 9x2 = 21.
We can rewrite this system of equations using the matrix-vector prod-
uct:

1
2
3
9

x1
x2

=

5
21

,
or, more compactly, as
A‚Éóx = ‚Éób,
where A is a 2√ó2 matrix, ‚Éóx is the vector of unknowns (a 2√ó1 matrix),
and ‚Éób is a vector of constants (a 2 √ó 1 matrix).
We can solve for ‚Éóx in this matrix equation by multiplying both
sides of the equation by the inverse A‚àí1:
A‚àí1A‚Éóx = 1‚Éóx = ‚Éóx = A‚àí1‚Éób.
Thus, to solve a system of linear equations, we can Ô¨Ånd the inverse of
the matrix of coeÔ¨Écients, then compute the product:
‚Éóx =
x1
x2

= A‚àí1‚Éób =
 3
‚àí2
3
‚àí1
1
3
 5
21

=
1
2

.
The computational cost of Ô¨Ånding A‚àí1 is roughly equivalent to the
computational cost of bringing an augmented matrix [ A | ‚Éób ] to re-
duced row echelon form‚Äîit's not like we're given the solution for free
by simply rewriting the system of equations in matrix form. Never-
theless, expressing the system of equations as A‚Éóx = ‚Éób and its solution
as ‚Éóx = A‚àí1‚Éób is a useful level of abstraction that saves us from needing
to juggle dozens of individual coeÔ¨Écients. The same symbolic expres-
sion ‚Éóx = A‚àí1‚Éób applies whether A is a 2 √ó 2 matrix or a 1000 √ó 1000
matrix.
Introduction
It's time we had an important discussion about matrix equations and
how they diÔ¨Äer from regular equations with numbers. If a, b, and c
are three numbers, and I tell you to solve for a in the equation
ab = c,

4.2
MATRIX EQUATIONS
151
you'd know the answer is a = c/b = c 1
b = 1
bc, and that would be the
end of it.
Now suppose A, B, and C are matrices and you want to solve for
A in the matrix equation
AB = C.
The answer A = C/B is not allowed.
So far, we deÔ¨Åned matrix
multiplication and matrix inversion, but not matrix division. Instead
of dividing by B, we must multiply by B‚àí1, which, in eÔ¨Äect, plays
the same role as a "divide by B" operation. The product of B and
B‚àí1 gives the identity matrix,
BB‚àí1 = 1,
B‚àí1B = 1.
When applying the inverse matrix B‚àí1 to the equation, we must
specify whether we are multiplying from the left or from the right,
because matrix multiplication is not commutative. Can you determine
the correct answer for A in the above equations? Is it A = CB‚àí1 or
A = B‚àí1C?
To solve matrix equations, we employ the same technique we used
to solve equations in Chapter 1: undoing the operations that stand in
the way of the unknown. Recall that we must always do the same
thing to both sides of an equation for it to remain true.
With matrix equations, it's the same story all over again, but there
are two new things you need to keep in mind:
‚Ä¢ The order in which matrices are multiplied matters because ma-
trix multiplication is not a commutative operation AB Ã∏= BA.
The expressions ABC and BAC are diÔ¨Äerent despite the fact
that they are the product of the same three matrices.
‚Ä¢ When performing operations on matrix equations, you can act
either from the left or from the right on the equation.
The best way to familiarize yourself with the peculiarities of matrix
equations is to look at example calculations. Don't worry, there won't
be anything too mathematically demanding in this section; we'll just
look at some pictures.
Matrix times vector
Suppose we want to solve the equation A‚Éóx = ‚Éób, in which an n √ó n
matrix A multiplies the vector ‚Éóx to produce a vector ‚Éób. Recall, we
can think of vectors as "tall and skinny" n √ó 1 matrices.
The picture corresponding to the equation A‚Éóx = ‚Éób is

152
COMPUTATIONAL LINEAR ALGEBRA
.
Assuming A is invertible, we can multiply by the inverse A‚àí1 on the
left of both sides of the equation:
.
By deÔ¨Ånition, A‚àí1 times its inverse A is equal to the identity ma-
trix 1, which is a diagonal matrix with ones on the diagonal and
zeros everywhere else:
.
Any vector times the identity matrix remains unchanged, so
,
which is the Ô¨Ånal answer.
Note that the question "Solve for ‚Éóx in A‚Éóx = ‚Éób " sometimes arises
in situations where the matrix A is not invertible. If the system of
equations is under-speciÔ¨Åed (A is wider than it is tall), there will be
a whole subspace of acceptable solutions ‚Éóx.
Recall the cases with
inÔ¨Ånite solutions (lines and planes) we saw in the previous section.
Matrix times matrix
Let's look at some other matrix equations. Suppose we want to solve
for A in the equation AB = C:
.
To isolate A, we multiply by B‚àí1 from the right on both sides:
.

4.2
MATRIX EQUATIONS
153
When B‚àí1 hits B they cancel (BB‚àí1 = 1) and we obtain the answer:
.
Matrix times matrix variation
What if we want to solve for B in the same equation AB = C?
.
Again, we must do the same to both sides of the equation. To cancel
A, we need to multiply by A‚àí1 from the left:
.
After A‚àí1 cancels with A, we obtain the Ô¨Ånal result:
.
This completes our lightning tour of matrix equations. There is really
nothing new to learn here; just make sure you're aware that the order
in which matrices are multiplied matters, and remember the general
principle of "doing the same thing to both sides of the equation." Act-
ing according to this principle is essential in all of math, particularly
when manipulating matrices.
In the next section, we'll "zoom in" on matrix equations by ex-
amining the arithmetic operations performed on coeÔ¨Écients during
matrix multiplication.
Exercises
E4.4 Solve for X in the following matrix equations: (1) XA = B;
(2) ABCXD = E; (3) AC = XDC. Assume the matrices A, B, C,
D, and E are invertible.

154
COMPUTATIONAL LINEAR ALGEBRA
4.3
Matrix multiplication
Suppose we're given the matrices
A =
 a
b
c
d

and
B =
 e
f
g
h

,
and we want to compute the matrix product AB.
Unlike matrix addition and subtraction, matrix multiplication is
not performed element-wise:
 a
b
c
d
 e
f
g
h

Ã∏=
 ae
bf
cg
dh

.
Instead, the matrix product is computed by taking the dot product
between each row of the matrix on the left and each column of the
matrix on the right:
‚Éór1 ‚Üí
‚Éór2 ‚Üí
 a
b
c
d
 e
f
g
h

‚Üë
‚Üë
‚Éóc1
‚Éóc2
=
 ‚Éór1 ¬∑ ‚Éóc1
‚Éór1 ¬∑ ‚Éóc2
‚Éór2 ¬∑ ‚Éóc1
‚Éór2 ¬∑ ‚Éóc2

=
 ae + bg
af + bh
ce + dg
cf + dh

.
Recall the dot product between two vectors ‚Éóv and ‚Éów is computed
using the formula ‚Éóv ¬∑ ‚Éów ‚â°P
i viwi.
Now let's look at a picture that shows how to compute the prod-
uct of a matrix with four rows and a matrix with Ô¨Åve columns. To
compute the top left entry, take the dot product of the Ô¨Årst row of
the matrix on the left and the Ô¨Årst column of the matrix on the right:
Figure 4.1: Matrix multiplication is performed "row times column." The
Ô¨Årst-row, Ô¨Årst-column entry of the product is the dot product of r1 and c1.
Similarly, the entry on the third row and fourth column of the product
is computed by taking the dot product of the third row of the matrix
on the left and the fourth column of the matrix on the right:

4.3
MATRIX MULTIPLICATION
155
Figure 4.2: The third-row, fourth-column entry of the product is com-
puted by taking the dot product of r3 and c4.
For the matrix product to work, the rows of the matrix on the left
must have the same length as the columns of the matrix on the right.
Matrix multiplication rules
‚Ä¢ Matrix multiplication is associative:
(AB)C = A(BC) = ABC.
‚Ä¢ The "touching" dimensions of the matrices must be the same.
For the triple product ABC to exist, the number of columns of
A must equal to the number of rows of B, and the number of
columns of B must equal the number of rows of C.
‚Ä¢ Given two matrices A ‚ààRm√ón and B ‚ààRn√ók, the product AB
is an m √ó k matrix.
‚Ä¢ Matrix multiplication is not a commutative operation.
Figure 4.3: The order of multiplication matters for matrices: the product
AB does not equal the product BA.
Example
Consider the matrices A ‚ààR2√ó3 and B ‚ààR3√ó2. The
product AB = C ‚ààR2√ó2 is computed as
1
2
3
4
5
6

|
{z
}
A
Ô£Æ
Ô£∞
1
2
3
4
5
6
Ô£π
Ô£ª
| {z }
B
=
 1 + 6 + 15
2 + 8 + 18
4 + 15 + 30
8 + 20 + 36

=
22
28
49
64

| {z }
C
.
We can also compute the product BA = D ‚ààR3√ó3:
Ô£Æ
Ô£∞
1
2
3
4
5
6
Ô£π
Ô£ª
| {z }
B
1
2
3
4
5
6

|
{z
}
A
=
Ô£Æ
Ô£∞
1 + 8
2 + 10
3 + 12
3 + 16
6 + 20
9 + 24
5 + 24
10 + 30
15 + 36
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
9
12
15
19
26
33
29
40
51
Ô£π
Ô£ª
|
{z
}
D
.

156
COMPUTATIONAL LINEAR ALGEBRA
In each case, the "touching" dimensions of the two matrices in the
product are the same. Note that C = AB Ã∏= BA = D, and, in fact,
the products AB and BA are matrices with diÔ¨Äerent dimensions.
Applications
Why is matrix multiplication deÔ¨Åned the way it is deÔ¨Åned?
Composition of linear transformations
The long answer to this question will be covered in depth when we
reach the chapter on linear transformations (Section 6, page 223).
Since I don't want you to live in suspense until then, I'll give you the
short answer right now. We can think of multiplying a column vector
‚Éóx ‚ààRn by a matrix A ‚ààRm√ón as analogous to applying a "vector
function" A of the form:
A : Rn ‚ÜíRm.
Applying the vector function A to the input ‚Éóx is the same as comput-
ing the matrix-vector product A‚Éóx:
for all ‚Éóx ‚ààRn,
A(‚Éóx) ‚â°A‚Éóx.
Every linear transformation from Rn to Rm can be described as a
matrix product by some matrix A ‚ààRm√ón.
What happens when we apply two linear operations in succession?
When we do this to ordinary functions, we call it function composition,
and denote it with a little circle:
z = g(f(x)) = g ‚ó¶f (x),
where g ‚ó¶f (x) indicates we should Ô¨Årst apply f to x to obtain an
intermediary value y, then apply g to y to obtain the Ô¨Ånal output z.
The notation g ‚ó¶f is useful when we're interested in the overall func-
tional relationship between x and z, but don't want to talk about the
intermediate result y. We can refer to the composite function g ‚ó¶f
and discuss its properties.
With matrices, B ‚ó¶A (applying A then B) is equal to applying
the product matrix BA:
‚Éóz = B(A(‚Éóx)) = (BA)‚Éóx.
We can describe the overall map that transforms ‚Éóx to ‚Éóz by a single
entity BA, the product of matrices B and A. The matrix product
is deÔ¨Åned the way it is deÔ¨Åned to enable us to easily compose
linear transformations.

4.3
MATRIX MULTIPLICATION
157
Regarding matrices as linear transformations (vector functions)
helps explain why matrix multiplication is not commutative. In gen-
eral, BA Ã∏= AB: there's no reason to expect AB will equal BA, just as
there's no reason to expect that f ‚ó¶g will equal g ‚ó¶f for two arbitrary
functions.
Matrix multiplication is an extremely useful computational tool.
At the moment, your feelings about matrix multiplication might not
be so warm and fuzzy, given it can be tedious and repetitive.
Be
patient and stick with it.
Solve some exercises to make sure you
understand. Afterward, you can let computers multiply matrices for
you‚Äîthey're good at this kind of repetitive task.
Row operations as matrix products
There is an important connection between row operations and ma-
trix multiplication. Performing the row operation R on a matrix is
equivalent to a left multiplication by an elementary matrix ER:
A‚Ä≤ = R(A)
‚áî
A‚Ä≤ = ERA.
There are three types of elementary matrices that correspond to the
three types of row operations. Let's look at an example.
Example
The row operation of adding m times the 2nd row to the
1st row (R : R1 ‚ÜêR1+mR2) corresponds to the following elementary
matrix:
ER =
1
m
0
1

,
which acts as
1
m
0
1
a
b
c
d

=
a + mc
b + md
c
d

.
We'll discuss elementary matrices in more detail in Section 4.5.
We can also perform "column operations" on matrices if we mul-
tiply them by elementary matrices from the right.
Exercises
E4.5 Compute the following matrix products:
1
2
3
4
5
6
7
8

=


| {z }
P
,
3
1
2
2
0
2
‚àí2
1

Ô£Æ
Ô£ØÔ£ØÔ£∞
‚àí2
3
1
0
‚àí2 ‚àí2
2
2
Ô£π
Ô£∫Ô£∫Ô£ª=


| {z }
Q
.

158
COMPUTATIONAL LINEAR ALGEBRA
4.4
Determinants
What is the volume of a rectangular box of length 1 m, width 2 m,
and height 3 m? It's easy to compute the volume of this box because
its shape is a right rectangular prism. The volume of this rectangular
prism is V = ‚Ñì√ó w √ó h = 6 m3. What if the shape of the box was
a parallelepiped instead? A parallelepiped is a box whose opposite
faces are parallel but whose sides are slanted, as shown in Figure 4.7
on page 162. How do we compute the volume of a parallelepiped?
The determinant operation, speciÔ¨Åcally the 3 √ó 3 determinant, is the
perfect tool for this purpose.
The determinant operation takes square matrices as inputs and
produces numbers as outputs:
det : Rn√ón ‚ÜíR.
The determinant of a matrix, denoted det(A) or |A|, is a particular
way to multiply the entries of the matrix to produce a single number.
We use determinants for all kinds of tasks: to compute areas and
volumes, to solve systems of equations, to check whether a matrix is
invertible or not, etc.
We can interpret the determinant of a matrix intuitively as a ge-
ometrical calculation.
The determinant is the "volume" of the ge-
ometric shape whose edges are the rows of the matrix.
For 2 √ó 2
matrices, the determinant corresponds to the area of a parallelogram.
For 3 √ó 3 matrices, the determinant corresponds to the volume of a
parallelepiped. For dimensions d > 3, we say the determinant mea-
sures a d-dimensional hyper-volume.
Consider the linear transformation T : R2 ‚ÜíR2 deÔ¨Åned through
the matrix-vector product with a matrix AT : T(‚Éóx) ‚â°AT‚Éóx.
The
determinant of the matrix AT is the scale factor associated with the
linear transformation T. The scale factor of the linear transformation
T describes how the area of a unit square in the input space (a square
with dimensions 1√ó1) is transformed by T. After passing through T,
the unit square is transformed to a parallelogram with area det(AT ).
Linear transformations that "shrink" areas have det(AT ) < 1, while
linear transformations that "enlarge" areas have det(AT ) > 1. A linear
transformation that is area preserving has det(AT ) = 1.
The determinant is also used to check linear independence for a
given set of vectors. We construct a matrix using the vectors as the
matrix rows, and compute its determinant.
If the determinant is
nonzero, the vectors are linearly independent.
The determinant of a matrix tells us whether or not that matrix
is invertible. If det(A) Ã∏= 0, then A is invertible; if det(A) = 0, A is
not invertible.

4.4
DETERMINANTS
159
Figure 4.4: A square with side length 1 in the input space of T is trans-
formed to a parallelogram with area |AT | in the output space of T. The
determinant measures the scale factor by which the area changes.
The determinant shares a connection with the vector cross prod-
uct, and is also used in the deÔ¨Ånition of the eigenvalue equation.
In this section, we'll discuss all the applications of determinants.
As you read along, I encourage you to actively connect the geometric,
algebraic, and computational aspects of determinants. Don't worry
if it doesn't all click right away‚Äîyou can always review this section
once you've learned more about linear transformations, the geometry
of cross products, and eigenvalues.
Formulas
The determinant of a 2 √ó 2 matrix is
det
a11
a12
a21
a22

‚â°

a11
a12
a21
a22
 = a11a22 ‚àía12a21.
The formulas for the determinants of larger matrices are deÔ¨Åned re-
cursively. For example, the determinant of a 3 √ó 3 matrix is deÔ¨Åned
in terms of 2 √ó 2 determinants:

a11
a12
a13
a21
a22
a23
a31
a32
a33

=
= a11

a22
a23
a32
a33
 ‚àía12

a21
a23
a31
a33
 + a13

a21
a22
a31
a32

= a11(a22a33 ‚àía23a32) ‚àía12(a21a33 ‚àía23a31) + a13(a21a32 ‚àía22a31)
= a11a22a33
‚àía12a21a33
+ a13a21a32
‚àía11a23a32
+ a12a23a31
‚àía13a22a31.
There's a neat computational trick for computing 3 √ó 3 determinants
by hand. The trick consists of extending the matrix A into a 3 √ó 5

160
COMPUTATIONAL LINEAR ALGEBRA
array that contains copies of the columns of A: the 1st column of A is
copied to the 4th column of the extended array, and the 2nd column
of A is copied to the 5th column. The determinant is then computed
by summing the products of the entries on the three positive diagonals
(solid lines) and subtracting the products of the entries on the three
negative diagonals (dashed lines), as illustrated in Figure 4.5.
Figure 4.5: Computing the determinant using the extended-array trick.
The general formula for the determinant of an n √ó n matrix is
det(A) =
n
X
j=1
(‚àí1)1+ja1jM1j,
where Mij is called the minor associated with the entry aij. The
minor Mij is the determinant of the submatrix obtained by removing
the ith row and the jth column of the matrix A. Note the "alternating"
factor (‚àí1)i+j that changes value between +1 and ‚àí1 for diÔ¨Äerent
terms in the formula.
In the case of 3 √ó 3 matrices, applying the determinant formula
gives the expected expression,
det(A) = (+1)a11M11 + (‚àí1)a12M12 + (+1)a13M13
= a11

a22
a23
a32
a33
 ‚àía12

a21
a23
a31
a33
 + a13

a21
a22
a31
a32
.
The determinant of a 4 √ó 4 matrix B is
det(B) = b11M11 ‚àíb12M12 + b13M13 ‚àíb14M14.
The general formula for determinants det(A) = Pn
j=1 (‚àí1)1+ja1jM1j,
assumes we're expanding the determinant along the Ô¨Årst row of the
matrix. In fact, a determinant formula can be obtained by expanding
the determinant along any row or column of the matrix. For example,
expanding the determinant of a 3√ó3 matrix along the second column
produces the determinant formula
det(A) =
3
X
i=1
(‚àí1)i+2ai2Mi2 = (‚àí1)a12M12+(1)a22M22+(‚àí1)a32M32.

4.4
DETERMINANTS
161
The expand-along-any-row-or-column nature of determinants can be
very handy: if you need to calculate the determinant of a matrix with
one row (or column) containing many zero entries, it makes sense to
expand along that row since many of the terms in the formula will
be zero. If a matrix contains a row (or column) consisting entirely of
zeros, we can immediately tell its determinant is zero.
Geometrical interpretation
Area of a parallelogram
Suppose we're given vectors ‚Éóv = (v1, v2) and ‚Éów = (w1, w2) in R2 and
we construct a parallelogram with corner points (0, 0), ‚Éóv, ‚Éów, and ‚Éóv+ ‚Éów.
The area of this parallelogram is equal to the determinant of the
matrix that contains (v1, v2) and (w1, w2) as rows:
area =

v1
v2
w1
w2
 = v1w2 ‚àív2w1.
Figure 4.6: The determinant of a 2 √ó 2 matrix corresponds to the area of
the parallelogram constructed from the rows of the matrix.
Volume of a parallelepiped
Suppose we are given three vectors‚Äî‚Éóu = (u1, u2, u3), ‚Éóv = (v1, v2, v3),
and ‚Éów = (w1, w2, w3) in R3‚Äîand we construct the parallelepiped with
corner points (0, 0, 0), ‚Éóu, ‚Éóv, ‚Éów, ‚Éóv + ‚Éów, ‚Éóu + ‚Éów, ‚Éóu + ‚Éóv, and ‚Éóu + ‚Éóv + ‚Éów, as
illustrated in Figure 4.7.
The volume of this parallelepiped is equal to the determinant of
the matrix containing the vectors ‚Éóu, ‚Éóv, and ‚Éów as rows:
volume =

u1
u2
u3
v1
v2
v3
w1
w2
w3

= u1(v2w3 ‚àív3w2) ‚àíu2(v1w3 ‚àív3w1) + u3(v1w2 ‚àív2w1).

162
COMPUTATIONAL LINEAR ALGEBRA
Figure 4.7: The determinant of a 3 √ó 3 matrix corresponds to the volume
of the parallelepiped constructed from the rows of the matrix.
Sign and absolute value of the determinant
Calculating determinants can produce positive or negative numbers.
Consider the vectors ‚Éóv = (v1, v2) ‚ààR2 and ‚Éów = (w1, w2) ‚ààR2 and
the determinant
D ‚â°det
 v1
v2
w1
w2

= v1w2 ‚àív2w1.
Let's denote the value of the determinant by the variable D. The
absolute value of the determinant is equal to the area of the parallelo-
gram constructed by the vectors ‚Éóv and ‚Éów. The sign of the determinant
(positive, negative, or zero) tells us information about the relative ori-
entation of the vectors ‚Éóv and ‚Éów. If we let Œ∏ be the measure of the
angle from ‚Éóv toward ‚Éów, then
‚Ä¢ if Œ∏ is between 0 and œÄ[rad] (180[‚ó¶]), the determinant will be
positive D > 0. This is the case illustrated in Figure 4.6.
‚Ä¢ if Œ∏ is between œÄ (180[‚ó¶]) and 2œÄ[rad] (360[‚ó¶]), the determinant
will be negative D < 0.
‚Ä¢ when Œ∏ = 0 (the vectors point in the same direction), or when
Œ∏ = œÄ (the vectors point in opposite directions), the determinant
will be zero, D = 0.
The formula for the area of a parallelogram is A = b√óh, where b is the
length of the parallelogram's base, and h is the parallelogram's height.
In the case of the parallelogram in Figure 4.6, the length of the base is
‚à•‚Éóv‚à•and the height is ‚à•‚Éów‚à•sin Œ∏, where Œ∏ is the angle measure between
‚Éóv and ‚Éów. The geometrical interpretation of the 2 √ó 2 determinant is
described by the formula,
D ‚â°det
 v1
v2
w1
w2

‚â°‚à•‚Éóv‚à•
|{z}
b
‚à•‚Éów‚à•sin Œ∏
|
{z
}
h
.
Observe the "height" of the parallelogram is negative when Œ∏ is be-
tween œÄ and 2œÄ.

4.4
DETERMINANTS
163
Properties
Let A and B be two square matrices of the same dimension. The
determinant operation has the following properties:
‚Ä¢ det(AB) = det(A) det(B) = det(B) det(A) = det(BA)
‚Ä¢ If det(A) Ã∏= 0, the matrix is invertible and det(A‚àí1) =
1
det(A)
‚Ä¢ det
 AT
= det(A)
‚Ä¢ det(Œ±A) = Œ±n det(A), for an n √ó n matrix A
‚Ä¢ det(A) = Qn
i=1 Œªi, where {Œªi} = eig(A) are the eigenvalues of A
The eÔ¨Äects of row operations on determinants
Recall the three row operations we used for the Gauss-Jordan elimi-
nation procedure:
‚Ä¢ Add a multiple of one row to another row
‚Ä¢ Swap two rows
‚Ä¢ Multiply a row by a constant
We'll now describe the eÔ¨Äects of these row operations on the value of
the matrix determinant. In each case, we'll connect the eÔ¨Äects of the
row operation to the geometrical interpretation of the determinant
operation.
Add a multiple of one row to another row
Adding a multiple of one row of a matrix to another row does not
change the determinant of the matrix.
Figure 4.8: Row operations of the form RŒ± : Ri ‚ÜêRi + Œ±Rj do not
change the value of the matrix determinant.
This property follows from the fact that parallelepipeds with equal
base enclosed between two parallel planes have the same volume even
if they have diÔ¨Äerent slants. This is known as Cavalieri's principle.
It is easier to visualize Cavalieri's principle in two dimensions by
considering two parallelograms with base b and diÔ¨Äerent slants, en-
closed between two parallel lines. The area of both parallelograms is
the same A = b√óh, where h is the distance between the parallel lines.

164
COMPUTATIONAL LINEAR ALGEBRA
Swap rows
Swapping two rows of a matrix changes the sign of its determinant.
Figure 4.9: Row-swaps, RŒ≤ : Ri ‚ÜîRj, Ô¨Çip the sign of the determinant.
This property is a consequence of measuring signed volumes. Swap-
ping two rows changes the relative orientation of the vectors, and
hence the sign of the volume.
Multiply a row by a constant
Multiplying a row by a constant is equivalent to the constant multi-
plying the determinant.
Figure 4.10: Row operations of the form RŒ≥ : Ri ‚ÜêŒ±Ri scale the value
of the determinant by the factor Œ±.
The third property follows from the fact that making one side of
the parallelepiped Œ± times longer increases the volume of the paral-
lelepiped by a factor of Œ±.
When each entry of an n√ón matrix is multiplied by the constant Œ±,
each of the n rows is multiplied by Œ± so the determinant changes by
a factor of Œ±n: det(Œ±A) = Œ±n det(A).
Zero-vs-nonzero determinant property
There is an important distinction between matrices with zero deter-
minant and matrices with nonzero determinant. We can understand
this distinction geometrically by considering the 3 √ó 3 determinant
calculation.
Recall, the volume of the parallelepiped with sides ‚Éóu,
‚Éóv, and ‚Éów is equal to the determinant of the matrix containing the
vectors ‚Éóu, ‚Éóv, and ‚Éów as rows. If the determinant is zero, it means at
least one of the rows of the matrix is a linear combination of the other
rows. The volume associated with this determinant is zero because
the geometrical shape it corresponds to is a Ô¨Çattened, two-dimensional
parallelepiped, in other words, a parallelogram. We say the matrix is
"deÔ¨Åcient" if its determinant is zero.

4.4
DETERMINANTS
165
On the other hand, if the determinant of a matrix is nonzero, the
rows of the matrix are linearly independent. In this case, the deter-
minant calculation corresponds to the volume of a real parallelepiped.
We say the matrix is "full" if its determinant is nonzero.
The zero-vs-nonzero determinant property of a matrix does not
change when we perform row operations on the matrix. If a matrix
A has a nonzero determinant, we know its reduced row echelon form
will also have nonzero determinant. The number of nonzero rows in
the reduced row echelon form of the matrix is called the rank of the
matrix. We say a matrix A ‚ààRn√ón has full rank if its RREF contains
n pivots. If the RREF of the matrix A contains a row of zeros, then
A is not full rank and det(A) = 0. On the other hand, if det(A) Ã∏= 0,
we know that rref(A) = 1.
Applications
Apart from the geometric and invertibility-testing applications of de-
terminants described above, determinants are related to many other
topics in linear algebra. We'll brieÔ¨Çy cover some of these below.
Cross product as a determinant
We can compute the cross product of the vectors ‚Éóv = (v1, v2, v3) and
‚Éów = (w1, w2, w3) by computing the determinant of a special matrix.
We place the symbols ÀÜƒ±, ÀÜÔöæ, and ÀÜk in the Ô¨Årst row of the matrix, then
write the coeÔ¨Écients of ‚Éóv and ‚Éów in the second and third rows. After
expanding the determinant along the Ô¨Årst row, we obtain the cross
product:
‚Éóv √ó ‚Éów =

ÀÜƒ±
ÀÜÔöæ
ÀÜk
v1
v2
v3
w1
w2
w3

= ÀÜƒ±

v2
v3
w2
w3
 ‚àíÀÜÔöæ

v1
v3
w1
w3
 + ÀÜk

v1
v2
w1
w2

= (v2w3 ‚àív3w2)ÀÜƒ± ‚àí(v1w3 ‚àív3w1)ÀÜÔöæ+ (v1w2 ‚àív2w1)ÀÜk
= (v2w3 ‚àív3w2, v3w1 ‚àív1w3, v1w2 ‚àív2w1).
Observe that the anti-linear property of the vector cross product
‚Éóv √ó ‚Éów = ‚àí‚Éów √ó ‚Éóv corresponds to the swapping-rows-changes-the-sign
property of determinants.
The extended-array trick for computing 3 √ó 3 determinants (see
Figure 4.5) doubles as a trick for computing cross-products:

166
COMPUTATIONAL LINEAR ALGEBRA
Figure 4.11: We can quickly compute the cross product of two vectors
using the extended-array trick.
Using the correspondence between the cross-product and the deter-
minant, we can write the determinant of a 3 √ó 3 matrix in terms of
the dot product and cross product:

u1
u2
u3
v1
v2
v3
w1
w2
w3

= ‚Éóu ¬∑ (‚Éóv √ó ‚Éów).
Cramer's rule
Cramer's rule is an approach for solving systems of linear equations
using determinants. Consider the following system of equations and
its representation as a matrix equation:
a11x1 + a12x2 + a13x3 = b1
a21x1 + a22x2 + a23x3 = b2
‚áî
A‚Éóx = ‚Éób.
a31x1 + a32x2 + a33x3 = b3
We're looking for the vector ‚Éóx = (x1, x2, x3) that satisÔ¨Åes this system
of equations.
Begin by writing the system of equations as an augmented matrix:
Ô£Æ
Ô£∞
a11
a12
a13
b1
a21
a22
a23
b2
a31
a32
a33
b3
Ô£π
Ô£ª‚â°
Ô£Æ
Ô£∞
|
|
|
|
‚Éóa1
‚Éóa2
‚Éóa3
‚Éób
|
|
|
|
Ô£π
Ô£ª.
We use the notation ‚Éóaj to denote the jth column of coeÔ¨Écients in the
matrix A, and ‚Éób to denote the column of constants.
Cramer's rule requires computing ratios of determinants. To Ô¨Ånd x1,
the Ô¨Årst component of the solution vector ‚Éóx, we compute the following

4.4
DETERMINANTS
167
ratio of determinants:
x1 =

|
|
|
‚Éób
‚Éóa2
‚Éóa3
|
|
|


|
|
|
‚Éóa1
‚Éóa2
‚Éóa3
|
|
|

=

b1
a12
a13
b2
a22
a23
b3
a32
a33


a11
a12
a13
a21
a22
a23
a31
a32
a33

.
Basically, we replace the column that corresponds to the unknown we
want to solve for (in this case the Ô¨Årst column) with the vector of
constants ‚Éób, and compute the determinant before dividing by |A|. To
Ô¨Ånd x2, we'll need to compute the determinant of a matrix where ‚Éób
replaces the second column of A. Similarly, to Ô¨Ånd x3, we replace the
third column with ‚Éób.
Cramer's rule is a neat computational trick that might come in
handy if you ever want to solve for one particular coeÔ¨Écient in the
unknown vector ‚Éóx, without solving for the other coeÔ¨Écients.
Linear independence test
Suppose you're given a set of n, n-dimensional vectors {‚Éóv1,‚Éóv2, . . . ,‚Éóvn}
and asked to check whether the vectors are linearly independent.
You could use the Gauss-Jordan elimination procedure to accom-
plish this task. Write the vectors ‚Éóvi as the rows of a matrix M. Next,
use row operations to Ô¨Ånd the reduced row echelon form (RREF) of
the matrix M. Row operations do not change the linear independence
between the rows of a matrix, so you can tell whether the rows are
independent from the reduced row echelon form of the matrix M.
Alternatively, you can use the determinant test as a shortcut to
check whether the vectors are linearly independent. If det(M) is zero,
the vectors that form the rows of M are not linearly independent. On
the other hand, if det(M) Ã∏= 0, then the rows of M and linearly
independent.

168
COMPUTATIONAL LINEAR ALGEBRA
Eigenvalues
The determinant operation is used to deÔ¨Åne the characteristic poly-
nomial of a matrix. The characteristic polynomial of A is
pA(Œª) ‚â°det(A ‚àíŒª1)
=

a11 ‚àíŒª
a12
a21
a22 ‚àíŒª

= (a11 ‚àíŒª)(a22 ‚àíŒª) ‚àía12a21
= Œª2 ‚àí(a11 + a22)
|
{z
}
Tr(A)
Œª + (a11a22 ‚àía12a21)
|
{z
}
det(A)
.
The roots of the characteristic polynomial are the eigenvalues of the
matrix A. Observe the coeÔ¨Écient of the linear term in pA(Œª) is equal
to ‚àíTr(A) and the constant term equals det(A).
The name char-
acteristic polynomial is indeed appropriate since pA(Œª) encodes the
information about three important properties of the matrix A: its
eigenvalues (Œª1, Œª2), its trace Tr(A), and its determinant det(A).
At this point, we don't need to delve into a detailed discussion
about properties of the characteristic polynomial. We gave the def-
inition of pA(Œª) here because it involves the determinant, and we're
in the section on determinants. SpeciÔ¨Åcally, pA(Œª) is deÔ¨Åned as the
determinant of A with Œªs (the Greek letter lambda) subtracted from
the entries on the diagonal of A. We'll continue the discussion on the
characteristic polynomial and eigenvalues in Section 7.1.
Exercises
E4.6 Find the determinant of the following matrices:
A =
 1
2
3
4

,
B =
 3
4
1
2

,
C =
Ô£Æ
Ô£∞
1
1
1
1
2
3
1
2
1
Ô£π
Ô£ª,
D =
Ô£Æ
Ô£∞
1
2
3
0
0
0
1
3
4
Ô£π
Ô£ª.
Observe that the matrix B can be obtained from the matrix A by
swapping the Ô¨Årst and second rows of the matrix. We therefore expect
det(A) and det(B) to have the same absolute value but opposite signs.
E4.7 Find the volume of the parallelepiped whose sides are the vec-
tors ‚Éóu = (1, 2, 3), ‚Éóv = (2, ‚àí2, 4), and ‚Éów = (2, 2, 5).

4.5
MATRIX INVERSE
169
Links
[ More information about determinants from Wikipedia ]
http://en.wikipedia.org/wiki/Determinant
http://en.wikipedia.org/wiki/Minor_(linear_algebra)
4.5
Matrix inverse
In this section, we'll learn four diÔ¨Äerent approaches for computing the
inverse of a matrix. Since knowing how to compute matrix inverses
is a pretty useful skill, learning several approaches is hardly overkill.
Note that the matrix inverse is unique, so no matter which method
you use to Ô¨Ånd the inverse, you'll always obtain the same answer.
You can verify your calculations by computing the inverse in diÔ¨Äerent
ways and checking that the answers agree.
Existence of an inverse
Not all matrices are invertible. Given a matrix A ‚ààRn√ón, we can
check whether it is invertible or not by computing its determinant:
A‚àí1 exists
if and only if
det(A) Ã∏= 0.
Calculating the determinant of a matrix serves as an invertibility test.
The exact value of the determinant is not important; it could be big
or small, positive or negative; so long as the determinant is nonzero,
the matrix passes the invertibility test.
Using the adjugate matrix approach
The inverse of a 2 √ó 2 matrix can be computed as follows:
a
b
c
d
‚àí1
=
1
ad ‚àíbc
 d
‚àíb
‚àíc
a

.
This is the 2√ó2 version of a general formula for obtaining the inverse
based on the adjugate matrix:
A‚àí1 =
1
det(A)adj(A).
What is the adjugate matrix, you say? Ah, I'm glad you asked! The
adjugate matrix is kind of complicated, so let's proceed step by step.
We'll Ô¨Årst deÔ¨Åne a few prerequisite concepts.

170
COMPUTATIONAL LINEAR ALGEBRA
In the following example, we'll work on a matrix A ‚ààR3√ó3 and
refer to its entries as aij, where i is the row index and j is the column
index:
A =
Ô£Æ
Ô£∞
a11
a12
a13
a21
a22
a23
a31
a32
a33
Ô£π
Ô£ª.
We now deÔ¨Åne three concepts associated with determinants:
1. For each entry aij, the minor Mij is the determinant of the
matrix that remains after we remove the ith row and the jth
column of A. For example, the minor that corresponds to the
entry a12 is given by:
M12 ‚â°

√ó
√ó
√ó
a21
√ó
a23
a31
√ó
a33

=

a21
a23
a31
a33
 = a21a33 ‚àía23a31.
2. The sign of each entry aij is deÔ¨Åned as sign(aij) ‚â°(‚àí1)i+j. For
example, the signs of the diÔ¨Äerent entries in a 3 √ó 3 matrix are
Ô£´
Ô£≠
+
‚àí
+
‚àí
+
‚àí
+
‚àí
+
Ô£∂
Ô£∏.
3. The cofactor cij for the entry aij is the product of this entry's
sign and its minor:
cij ‚â°sign(aij)Mij = (‚àí1)i+jMij.
The above concepts should look somewhat familiar, since they pre-
viously appeared in the formula for computing determinants. If we
expand the determinant along the Ô¨Årst row of the matrix, we obtain
the formula
det(A) =
n
X
j=1
a1jsign(a1j)M1j =
n
X
j=1
a1jc1j.
Now you can see where the name cofactor comes from: the cofactor
cij is what multiplies the factor aij in the determinant formula.
Okay, now we're ready to describe the adjugate matrix. The adju-
gate matrix is deÔ¨Åned as the transpose of the matrix of cofactors C.
The matrix of cofactors is a matrix of the same dimensions as the
original matrix A that is constructed by replacing each entry aij by

4.5
MATRIX INVERSE
171
its cofactor cij. The matrix of cofactors for A is
C =
Ô£Æ
Ô£∞
c11
c12
c13
c21
c22
c23
c31
c32
c33
Ô£π
Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
+

a22
a23
a32
a33

‚àí

a21
a23
a31
a33

+

a21
a22
a31
a32

‚àí

a12
a13
a32
a33

+

a11
a13
a31
a33

‚àí

a11
a12
a31
a32

+

a12
a13
a22
a23

‚àí

a11
a13
a21
a23

+

a11
a12
a21
a22

Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
The adjugate matrix adj(A) is the transpose of the matrix of cofactors:
adj(A) ‚â°CT.
The formula for the inverse matrix is A‚àí1 =
1
det(A)adj(A). In the
3 √ó 3 case, the matrix inverse formula is
A‚àí1 =
1

a11
a12
a13
a21
a22
a23
a31
a32
a33

Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
+

a22
a23
a32
a33

‚àí

a12
a13
a32
a33

+

a12
a13
a22
a23

‚àí

a21
a23
a31
a33

+

a11
a13
a31
a33

‚àí

a11
a13
a21
a23

+

a21
a22
a31
a32

‚àí

a11
a12
a31
a32

+

a11
a12
a21
a22

Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
I know this looks complicated, but I wanted to show you the adjugate
matrix approach for computing the inverse because I think it's a nice
formula.
In practice, you'll rarely have to compute inverses using
this approach; nevertheless, the adjugate matrix formula represents
an important theoretical concept. Note the formula fails if |A| = 0
due to a divide-by-zero error.
Using row operations
Another way to obtain the inverse of a matrix is to record all the row
operations R1, R2, . . . needed to transform A into the identity matrix:
Rk(. . . R2(R1(A)) . . .) = 1.
Recall that we can think of the action of the matrix A as a vector
transformation: ‚Éów = A‚Éóv. By deÔ¨Ånition, the inverse A‚àí1 is the oper-
ation that undoes the eÔ¨Äect of A: A‚àí1 ‚Éów = ‚Éóv. The combination of A
followed by A‚àí1 is the identity transformation: A‚àí1A‚Éóv = 1‚Éóv ‚â°‚Éóv.
The cumulative eÔ¨Äect of the row operations required to transform
A to the identity matrix is equivalent to the "undo" action of A.

172
COMPUTATIONAL LINEAR ALGEBRA
Applying the sequence of row operations R1, R2, . . . , Rk has the same
eÔ¨Äect as multiplying by A‚àí1:
A‚àí1 ‚Éów ‚â°Rk(. . . R2(R1(‚Éów)) . . .).
If you think this method of Ô¨Ånding the inverse A‚àí1 seems more com-
plicated than useful, you'd be right‚Äîif it weren't for the existence of
a neat procedure for recording the row operations R1, R2,. . .,Rk that
makes everything simpler. We'll discuss this procedure next.
Begin by initializing an n√ó2n array with the entries of the matrix
A on the left side and the identity matrix on the right side: [ A | 1 ].
If we perform the Gauss-Jordan elimination procedure on this array,
we'll end up with the inverse A‚àí1 on the right-hand side of the array:
[ A | 1 ] ‚àíG-J elimination ‚Üí[ 1 | A‚àí1 ].
Example
Let's illustrate the procedure by computing the inverse
of the following matrix:
A =
1
2
3
9

.
We start by writing the matrix A next to the identity matrix 1:
 1
2
1
0
3
9
0
1

.
Next, we perform the Gauss-Jordan elimination procedure on the
resulting 2 √ó 4 matrix:
1. The Ô¨Årst step is to subtract three times the Ô¨Årst row from the
second row, written compactly as R1 : R2 ‚ÜêR2 ‚àí3R1, to
obtain
 1
2
1
0
0
3
‚àí3
1

.
2. We perform a second row operation R2 : R2 ‚Üê1
3R2 to obtain
a leading one in the second column:
"
1
2
1
0
0
1
‚àí1
1
3
#
.
3. Finally, we perform R3 : R1 ‚ÜêR1 ‚àí2R2 to clear the entry
above the leading one in the second column:
"
1
0
3
‚àí2
3
0
1
‚àí1
1
3
#
.

4.5
MATRIX INVERSE
173
The inverse of A now appears in the right-hand side of the array:
A‚àí1 =
"
3
‚àí2
3
‚àí1
1
3
#
.
This algorithm works because we identify the sequence of row opera-
tions R3(R2(R1( . ))) with the action of the inverse matrix A‚àí1:
R3(R2(R1(A))) = 1
‚áí
R3(R2(R1(1))) = A‚àí1.
The combined eÔ¨Äect of the three row operations is to "undo" the action
of A, and therefore this sequence of row operations has the same
eÔ¨Äect as the inverse operation A‚àí1. The right side of the 2 √ó 4 array
serves as a record of the cumulative eÔ¨Äect of this sequence of row
operations performed. Because the right side starts from a "blank"
identity matrix, it will contain A‚àí1 by the end of the procedure.
Using elementary matrices
Every row operation R performed on a matrix is equivalent to an
operation of left multiplication by an elementary matrix ER:
A‚Ä≤ = R(A)
‚áî
A‚Ä≤ = ERA.
There are three types of elementary matrices that correspond to the
three types of row operations. We'll illustrate the three types of ele-
mentary matrices with examples from the 2 √ó 2 case:
‚Ä¢ Adding m times the 2nd row to the 1st row corresponds to
RŒ± : R1 ‚ÜêR1 + mR2
‚áî
EŒ± =
1
m
0
1

.
‚Ä¢ Swapping the 1st and 2nd rows corresponds to
RŒ≤ : R1 ‚ÜîR2
‚áî
EŒ≤ =
0
1
1
0

.
‚Ä¢ Multiplying the 1st row by the constant k corresponds to
RŒ≥ : R1 ‚ÜêkR1
‚áî
EŒ≥ =
k
0
0
1

.
The general rule is simple: to Ô¨Ånd the elementary matrix that cor-
responds to a given row operation, apply that row operation to the
identity matrix 1.

174
COMPUTATIONAL LINEAR ALGEBRA
Recall the procedure we used to Ô¨Ånd the inverse in the previous
section.
We applied the sequence of row operations R1, R2, . . . to
transform the array [ A | 1 ] into the reduced row echelon form:
Rk(. . . R2(R1([ A | 1 ])) . . .)
=
[ 1 | A‚àí1 ].
If we represent each row operation as a multiplication by an elemen-
tary matrix, we obtain the equation
Ek ¬∑ ¬∑ ¬∑ E2E1[ A | 1 ]
=
[ 1 | A‚àí1 ].
Observe that Ek ¬∑ ¬∑ ¬∑ E2E1A = 1, so we have obtained an expression
for the inverse matrix A‚àí1 as a product of elementary matrices:
A‚àí1 = Ek ¬∑ ¬∑ ¬∑ E2E1.
We'll now illustrate how the formula A‚àí1 = Ek ¬∑ ¬∑ ¬∑ E2E1 applies in
the case of the matrix A discussed above
A =
1
2
3
9

.
Recall the row operations we applied in order to transform [ A | 1 ]
into [ 1 | A‚àí1 ]:
1. R1: R2 ‚ÜêR2 ‚àí3R1
2. R2: R2 ‚Üê1
3R2
3. R3: R1 ‚ÜêR1 ‚àí2R2
Let's revisit these row operations, representing each of them as a
multiplication by an elementary matrix:
1. The Ô¨Årst row operation R1 : R2 ‚ÜêR2 ‚àí3R1 corresponds to a
multiplication by the elementary matrix E1:
E1 =
 1
0
‚àí3
1

,
E1A =
 1
0
‚àí3
1
1
2
3
9

=
1
2
0
3

.
2. The second row operation R2 : R2 ‚Üê
1
3R2 corresponds to a
matrix E2:
E2 =
1
0
0
1
3

,
E2(E1A) =
1
0
0
1
3
1
2
0
3

=
1
2
0
1

.
3. The third row operation R3 : R1 ‚ÜêR1 ‚àí2R2 corresponds to
the elementary matrix E3:
E3 =

1 ‚àí2
0
1

,
E3(E2E1A) =

1 ‚àí2
0
1

1
2
0
1

=

1
0
0
1

.

4.5
MATRIX INVERSE
175
Note that E3E2E1A = 1, so the product E3E2E1 must equal A‚àí1:
A‚àí1 = E3E2E1 =
1
‚àí2
0
1
1
0
0
1
3
 1
0
‚àí3
1

=
 3
‚àí2
3
‚àí1
1
3

.
Verify the last equation by computing the product of the three ele-
mentary matrices.
Since we know A‚àí1 = E3E2E1, then A = (A‚àí1)‚àí1 = (E3E2E1)‚àí1 =
E‚àí1
1 E‚àí1
2 E‚àí1
3 . We can write A as a product of elementary matrices:
A = E‚àí1
1 E‚àí1
2 E‚àí1
3
=
1
0
3
1
1
0
0
3
1
2
0
1

.
The inverses of the elementary matrices are trivial to compute; they
correspond to elementary "undo" operations.
The elementary matrix approach teaches us that every invertible
matrix A can be decomposed as the product of elementary matrices.
The inverse matrix A‚àí1 consists of the product of the inverses of the
elementary matrices that make up A (in the reverse order).
Using a computer algebra system
You can use a computer algebra system to specify matrices and com-
pute their inverses. Let's illustrate how to Ô¨Ånd the matrix inverse
using the computer algebra system at live.sympy.org.
>>> from sympy.matrices import Matrix
>>> A = Matrix( [ [1,2],[3,9] ] )
# define a Matrix object
>>> A.inv()
# call the inv method on A
[ 3, -2/3]
[-1,
1/3]
Note SymPy returns an answer in terms of exact rational numbers.
This is in contrast with numerical computer algebra systems like
Octave and MATLAB, which are based on Ô¨Çoating point arithmetic.
You can use SymPy to check your answers on homework problems.
Discussion
We've explored several ways to compute matrix inverses. If you need
to Ô¨Ånd the inverse of a matrix using only pen and paper, on a Ô¨Ånal
exam for example, I recommend using the Gauss-Jordan elimination
procedure on the extended array:
[ A | 1 ] ‚àíG-J elimination ‚Üí[ 1 | A‚àí1 ],
since it is fairly easy to perform even for large matrices and it leverages
your experience with the Gauss-Jordan elimination procedure.
For 2 √ó 2 matrices, the formula

a
b
c
d
‚àí1
=
1
ad‚àíbc

d
‚àíb
‚àíc
a

is faster.

176
COMPUTATIONAL LINEAR ALGEBRA
Invertibility
Not all matrices are invertible.
Keep this in mind, since teachers
might try to trick you by asking you to Ô¨Ånd the inverse of a non-
invertible matrix. Let's analyze how each procedure for computing
the inverse fails when applied to a noninvertible matrix D. The in-
verse formula based on the determinant and the adjugate matrix is
D‚àí1 =
1
det(D)adj(D).
However, if the matrix D is not invertible,
then det(D) = 0 and the formula fails due to a divide-by-zero error.
The row operations approach to computing the inverse will also fail.
Starting from the extended array [ D | 1 ], you can apply all the row
operations you want, but you'll never be able to obtain the identity
matrix in the left-hand side of the array. This is because the reduced
row echelon form of a noninvertible matrix D has at least one row of
zeros: rref(D) Ã∏= 1. We'll discuss invertible matrices and their prop-
erties in Section 6.4. For now, be sure to remember the determinant
test for invertibility: if det(A) = 0, then A is noninvertible, and if
det(A) Ã∏= 0, then A is invertible.
Exercises
E4.8 Compute A‚àí1 where A =
1
1
1
2

.
E4.9 Implement the matrix inverse formula A‚àí1 =
1
det(A)adj(A) for
the case of 3 √ó 3 using a spreadsheet.
E4.10 Show that for an n √ó n invertible matrix A, the determinant
of the adjugate matrix is |adj(A)| = (|A|)n‚àí1.
Hint: Recall that |A‚àí1| =
1
|A| and |Œ±A| = Œ±n |A|.
4.6
Computational problems
P4.1
Mitchell wants to eat healthily. His target is to eat exactly 25 grams
of fat and 32 grams of protein for lunch today. There are two types of food
in the fridge, x and y. One serving of food x contains one gram of fat and
two grams of protein, while a serving of food y contains Ô¨Åve grams of fat
and one gram of protein. To Ô¨Ågure out how many servings of each type of
food he should eat, he comes up with the following system of equations:
x
+ 5y
= 25
2x
+
y
= 32
‚áí
 1
5
25
2
1
32

.
Can you help Mitchell Ô¨Ånd how many servings of x and y he should eat?
Hint: Find the reduced row echelon form of the augmented matrix.

4.6
COMPUTATIONAL PROBLEMS
177
P4.2
Alice, Bob, and Charlotte are solving this systems of equations:
3x
+3y
= 6
2x
+ 3
2y
= 5
‚áí
 3
3
6
2
3
2
5

.
Alice follows the "standard" procedure to obtain a leading one, by perform-
ing the row operation R1 ‚Üê1
3R1. Bob starts with a diÔ¨Äerent row operation,
applying R1 ‚ÜêR1 ‚àíR2 to obtain a leading one. Charlotte takes a third
approach by swapping the Ô¨Årst and second rows: R1 ‚ÜîR2.
a)
"
1
1
2
2
3
2
5
#
b)
"
1
3
2
1
2
3
2
5
#
c)
"
2
3
2
5
3
3
6
#
Help Alice, Bob, and Charlotte Ô¨Ånish the problems by writing the list of
remaining row operations each of them must perform to bring their version
of the augmented matrix into reduced row echelon form.
P4.3
Find the solutions to the systems of equations that correspond to
the following augmented matrices:
a)
 ‚àí1
‚àí2
‚àí2
3
3
0

b)
Ô£Æ
Ô£∞
1
‚àí1
‚àí2
1
‚àí2
3
3
‚àí1
‚àí1
0
1
2
Ô£π
Ô£ª
c)
Ô£Æ
Ô£∞
2
‚àí2
3
2
1
‚àí2
‚àí1
0
‚àí2
2
2
1
Ô£π
Ô£ª
P4.4
Find the solution set for the systems of equations described by the
following augmented matrices:
a)
 ‚àí1
‚àí2
‚àí2
3
6
6

b)
Ô£Æ
Ô£∞
1
‚àí1
‚àí2
1
‚àí2
3
3
‚àí1
‚àí1
2
1
0
Ô£π
Ô£ª
c)
Ô£Æ
Ô£∞
2
‚àí2
3
2
0
0
5
3
‚àí2
2
2
1
Ô£π
Ô£ª
P4.5
Find the solution to the following systems of equations:
a)
Ô£Æ
Ô£∞
2
1
‚àí1
0
0
1
1
0
4
2
‚àí2
0
Ô£π
Ô£ª
b)
Ô£Æ
Ô£∞
2
0
1
5
1
4
2
2
0
2
1
1
Ô£π
Ô£ª
c)
Ô£Æ
Ô£∞
2
1
‚àí1
0
0
1
1
0
4
2
‚àí2
0
Ô£π
Ô£ª
P4.6
Solve for C in the matrix equation ABCD = AD.
P4.7
Solve for the following matrix equations problems:
(a) Simplify the expression MNB‚àí1BK‚àí1KN ‚àí1M ‚àí2L‚àí1S‚àí1SMK2.
(b) Simplify J‚àí3K2G‚àí1GK‚àí3J2.
(c) Solve for A in the equation A‚àí1BNK = B2B‚àí1NK.
(d) Solve for Y in SUNNY = SUN.
You can assume all matrices are invertible.
P4.8
Solve for ‚Éóx in A‚Éóx = ‚Éób, where A =
h 1
0
‚àí3
2 ‚àí1
1
0
0
‚àí1
i
and ‚Éób = (2, 2, 3)T.
P4.9
Solve for ‚Éóx in the equation ‚Éóx = ‚Éód + A‚Éóx, where A =
h
0
0.05 0.3
0.01
0
0.01
0.1
0
0
i
,
and ‚Éód = (25, 10, 14)T. Use live.sympy.org to perform the calculations.
Hint: Rewrite as 1‚Éóx = ‚Éód + A‚Éóx then bring all the ‚Éóxs to one side.
P4.10
Given the following two matrices,
A =
Ô£Æ
Ô£∞
1
1
1
0
4
2
3
1
0
Ô£π
Ô£ª
and
B =
Ô£Æ
Ô£∞
3
1
2
0
1
1
Ô£π
Ô£ª,

178
COMPUTATIONAL LINEAR ALGEBRA
compute the matrix products a) AB, b) AA, c) BA, and d) BB.
P4.11
Compute the following product of three matrices:
2
10
‚àí5
0
0
0
1
3

Ô£Æ
Ô£ØÔ£ØÔ£∞
1
3
0
2
5
1
‚àí3
‚àí4
Ô£π
Ô£∫Ô£∫Ô£ª
1
1
1
1

.
P4.12
Given an unknown variable Œ± ‚ààR and the matrices
A=

cos(Œ±)
1
‚àí1
‚àísin(Œ±)

;
B =

sin(Œ±)
0
0
‚àísin(Œ±)

;
C =

1
‚àícos(Œ±)
sin(Œ±)
1

,
compute the value of a) A2 + B2, b) A2 + C, c) A2 + C ‚àíB2. Give you
answer in terms of Œ± and use double-angle formulas as needed.
P4.13
Find the determinants of the following matrices:
a)
2
1
3
0

b)
Ô£Æ
Ô£∞
0
5
3
0
1
1
0
1
0
Ô£π
Ô£ª
c)
Ô£Æ
Ô£∞
1
2
0
3
1
1
4
‚àí2
0
Ô£π
Ô£ª
P4.14
Find the determinants of these matrices
A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
3
‚àí1
5
2
0
2
2
‚àí3
0
0
4
0
0
0
0
‚àí2
Ô£π
Ô£∫Ô£∫Ô£ª;
B =
Ô£Æ
Ô£ØÔ£ØÔ£∞
2
‚àí1
0
‚àí3
2
0
1
1
3
0
1
4
0
0
‚àí1
3
‚àí2
3
1
0
‚àí1
0
‚àí1
0
2
Ô£π
Ô£∫Ô£∫Ô£ª.
P4.15
Find and area of a parallelogram which has vectors ‚Éóv = (3, ‚àí5)
and ‚Éów = (1, ‚àí1) as its sides.
Hint: Use the formula from Section 4.4 (page 161).
P4.16
Find volume of the parallelepiped who has the vectors ‚Éóu = (2, 0, 1),
‚Éóv = (1, ‚àí1, 1) and ‚Éów = (0, 2, 3) as sides.
P4.17
Given the matrix
A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
3
‚àí2
0
1
0
1
3
‚àí1
5
0
1
4
0
3
‚àí4
2
Ô£π
Ô£∫Ô£∫Ô£ª,
a) Find the determinant of A.
b) Find the determinant when you interchange the 1st and 3rd rows.
c) Find the determinant after multiplying the 2nd row by ‚àí2.
P4.18
Check whether the rows of the following matrices are linearly
independent or not:
A =
1
3
2
6

;
B =
Ô£Æ
Ô£∞
1
4
3
2
1
1
0
‚àí2
‚àí1
Ô£π
Ô£ª;

4.6
COMPUTATIONAL PROBLEMS
179
C =
Ô£Æ
Ô£ØÔ£ØÔ£∞
0
0
0
0
2
‚àí2
2
‚àí2
‚àí4
4
‚àí4
4
‚àí8
8
‚àí8
8
Ô£π
Ô£∫Ô£∫Ô£ª;
D =
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
1
1
1
2
0
‚àí2
0
‚àí1
2
1
‚àí2
1
‚àí1
‚àí1
1
Ô£π
Ô£∫Ô£∫Ô£ª.
Are the columns of these matrices linearly independent?
P4.19
The transformation from polar coordinates (r, Œ∏) to cartesian co-
ordinates (x, y) is given the equations x(r, Œ∏) = r cos Œ∏ and y(r, Œ∏) = r sin Œ∏.
Under this transformation area changes as dxdy = det(J)drdŒ∏, where
det(J) is the area scaling factor of the transformation. The matrix J con-
tains the partial derivates of x(r, Œ∏) and y(r, Œ∏), and is called the Jacobian
matrix of the transformation:
J =
 ‚àÇx
‚àÇr
‚àÇx
‚àÇŒ∏
‚àÇy
‚àÇr
‚àÇy
‚àÇŒ∏

.
Compute the value of det(J).
P4.20
Spherical coordinates (œÅ, Œ∏, œÜ) are described by
x = œÅ sin œÜ cos Œ∏,
y = œÅ sin œÜ sin Œ∏,
z = œÅ cos œÜ.
Small "volume chunks" transforms according to dxdydz = det(Js)dœÜdŒ∏dœÅ,
where det(Js) is the volume scaling factor, computed as the determinant of
the Jacobian matrix:
Js =
Ô£Æ
Ô£ØÔ£∞
‚àÇx
‚àÇœÅ
‚àÇx
‚àÇŒ∏
‚àÇx
‚àÇœÜ
‚àÇy
‚àÇœÅ
‚àÇy
‚àÇŒ∏
‚àÇy
‚àÇœÜ
‚àÇz
‚àÇœÅ
‚àÇz
‚àÇŒ∏
‚àÇz
‚àÇœÜ
Ô£π
Ô£∫Ô£ª.
Compute the absolute value of det(Js).
P4.21
Find the inverses for the following matrices:
a)
0
1
0
0

b)
1
2
2
5

c)
2
3
2
4

P4.22
Given the matrix equation AB = C, where A and C are 2 √ó 2
matrices, Ô¨Ånd the matrix B.
A =
1
4
2
7

C =
3
2
1
‚àí4

.
P4.23
Find an inverse of the following matrix:
A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
0
‚àí3
2
4
1
‚àí1
1
‚àí1
2
4
0
‚àí2
3
0
1
0
Ô£π
Ô£∫Ô£∫Ô£ª.
P4.24
Prove that the zero matrix A has no inverse.
P4.25
Obtain the matrices of cofactors for the following matrices:

180
COMPUTATIONAL LINEAR ALGEBRA
A=
h 1
4
3
2
1
1
0 ‚àí2 ‚àí1
i
;
B =
h 5
0
1
3 ‚àí1 ‚àí3
0 ‚àí4 ‚àí2
i
;
C =
 1
1
1
1
2
0
‚àí2
0
‚àí1
2
1
‚àí2
1
‚àí1 ‚àí1
1

.
P4.26
Find a, b, c and d in this equation:
 1
3
‚àí2
‚àí1
 a
b
c
d

=
3
‚àí5
4
0

.
P4.27
Given the constraints a = g, e = b = f, and c = d = h, Ô¨Ånd a
choice of the variables a, b, c, d, e, f, g, h that satisfy the matrix equation:
a
b
c
d
 e
f
g
h

=
‚àí2
‚àí3
0
2

.

Chapter 5
Geometrical aspects of
linear algebra
In this section we'll study geometrical objects like lines, planes, and
vector spaces. We'll use what we learned about vectors and matrices
in the previous chapters to perform geometrical calculations such as
projections and distance measurements.
Developing your intuition about the geometrical problems of linear
algebra is very important: of all the things you learn in this course,
your geometrical intuition will stay with you the longest. Years from
now, you may not recall the details of the Gauss-Jordan elimination
procedure, but you'll still remember that the solution to three linear
equations in three variables corresponds to the intersection of three
planes in R3.
5.1
Lines and planes
Points, lines, and planes are the basic building blocks of geometry. In
this section, we'll explore these geometric objects, the equations that
describe them, and their visual representations.
Concepts
‚Ä¢ p = (px, py, pz): a point in R3
‚Ä¢ ‚Éóv = (vx, vy, vz): a vector in R3
‚Ä¢ ÀÜv =
‚Éóv
‚à•‚Éóv‚à•: the unit vector in the same direction as the vector ‚Éóv
‚Ä¢ An inÔ¨Ånite line ‚Ñìis a one-dimensional space deÔ¨Åned in one of
several possible ways:
181

182
GEOMETRICAL ASPECTS OF LINEAR ALGEBRA
‚ñ∑‚Ñì: {po + t ‚Éóv, t ‚ààR}: a parametric equation of a line with
direction vector ‚Éóv passing through the point po
‚ñ∑‚Ñì:
n
x‚àípox
vx
= y‚àípoy
vy
= z‚àípoz
vz
o
: a symmetric equation
‚Ä¢ An inÔ¨Ånite plane P is a two-dimensional space deÔ¨Åned in one of
several possible ways:
‚ñ∑P : {Ax + By + Cz = D }: a general equation
‚ñ∑P : {po + s‚Éóv + t ‚Éów, s, t ‚ààR}: a parametric equation
‚ñ∑P : {‚Éón ¬∑ [(x, y, z) ‚àípo] = 0 }: a geometric equation of the
plane that contains point po and has normal vector ÀÜn
‚Ä¢ d(a, b): the shortest distance between geometric objects a and b
Points
We can specify a point in R3 by its coordinates p = (px, py, pz), which
is similar to how we specify vectors.
In fact, the two notions are
equivalent: we can either talk about the destination point p or the
vector ‚Éóp that takes us from the origin to the point p. This equivalence
lets us add and subtract vectors and points. For example, ‚Éód = q ‚àíp
denotes the displacement vector that takes the point p to the point q.
We can also specify a point as
the intersection of two lines.
As
an example in R2, let's deÔ¨Åne p =
(px, py) to be the intersection of the
lines x ‚àíy = ‚àí1 and 3x + y = 9.
We must solve the two equations
simultaneously to Ô¨Ånd the coordi-
nates of the point p.
We can use
the standard techniques for solving
equations to Ô¨Ånd the answer. The
intersection point is p = (2, 3). Note
that for two lines to intersect at a
point, the lines must not be paral-
lel.
Example 1
Find where the lines x + 2y = 5 and 3x + 9y = 21
intersect. To Ô¨Ånd the point of intersection, we solve these equations
simultaneously and obtain the point (x, y) that is contained in both
lines. The answer is the point p = (1, 2).

5.1
LINES AND PLANES
183
In three dimensions, a point can also
be speciÔ¨Åed as the intersection of three
planes. This is precisely what happens
when we solve equations of the form:
A1x + B1y + C1z = D1,
A2x + B2y + C2z = D2,
A3x + B3y + C3z = D3.
To solve this system of equations, we
must Ô¨Ånd the point (x, y, z) that sat-
isÔ¨Åes all three equations, which means
this point is contained in all three
planes.
Lines
A line ‚Ñìis a one-dimensional space that is inÔ¨Ånitely long. There are
several equivalent ways to specify a line in space.
The parametric equation of a line is ob-
tained as follows. Given a direction vector
‚Éóv and some point po on the line, we deÔ¨Åne
the line as the following set:
‚Ñì: {(x, y, z) ‚ààR3 | (x, y, z) = po+t‚Éóv, t ‚ààR}.
The line consists of all the points (x, y, z)
that can be reached starting from the point
po and adding any multiple of the direction
vector ‚Éóv. We say the line is parametrized by the variable t.
The symmetric equation is an equivalent way to describe a line that
does not require an explicit parametrization. Consider the equations
that correspond to each coordinate in the parametric equation of a
line:
x = pox + t vx,
y = poy + t vy,
z = poz + t vz.
When we solve for t in these equations and equate the results, we
obtain the symmetric equation of a line:
‚Ñì:
 x ‚àípox
vx
= y ‚àípoy
vy
= z ‚àípoz
vz

.
Note the parameter t does not appear. The symmetric equation spec-
iÔ¨Åes the line as the relationships between the x, y, and z coordinates
that hold for all points on the line.
You're probably most familiar with the symmetric equation of
lines in R2, which do not involve the variable z. For non-vertical lines

184
GEOMETRICAL ASPECTS OF LINEAR ALGEBRA
in R2 (vx Ã∏= 0), we can think of y as a function of x and write the
equation of the line in the equivalent form:
x ‚àípox
vx
= y ‚àípoy
vy
‚áí
y(x) = mx + b,
where m = vy
vx and b = poy ‚àívy
vx pox. The equation m = vy
vx makes
sense intuitively: the slope of a line m corresponds to how much the
line "moves" in the y-direction divided by how much the line "moves"
in the x-direction.
Another way to describe a line is
to specify two points that are part
of the line. The equation of a line
that contains the points p and q can
be obtained as follows:
‚Ñì: {‚Éóx = p + t (q ‚àíp), t ‚ààR},
where (q ‚àíp) plays the role of the
direction vector ‚Éóv for this line. Any
vector parallel to the line can be
used as the direction vector for the
line.
Example 2
Find the parametric equation of the line that passes
through the points p = (1, 1, 1) and q = (2, 3, 4). What is the sym-
metric equation of this line?
Using the direction vector ‚Éóv = q ‚àíp = (1, 2, 3) and the point
p on the line, we can write a parametric equation for the line as
{(x, y, z) ‚ààR3 | (x, y, z) = (1, 1, 1) + t(1, 2, 3), t ‚ààR}.
Note that
a parametric equation using the direction vector (‚àí1, ‚àí2, ‚àí3) would
be equally valid: {(1, 1, 1) + t(‚àí1, ‚àí2, ‚àí3), t ‚ààR}. The symmetric
equation of the line is x‚àí1
1
= y‚àí1
2
= z‚àí1
3 .
Lines as intersections of planes
In three dimensions, the intersection of two
non-parallel planes forms a line. For exam-
ple, the intersection of the xy-plane Pxy :
{(x, y, z) ‚ààR3 | z = 0} and the xz-plane
Pxz : {(x, y, z) ‚ààR3 | y = 0} is the x-axis:
{(x, y, z) ‚ààR3 | (0, 0, 0)+(1, 0, 0)t, t ‚ààR}.
For this simple case, we can imagine the
two planes (use your hands) and visually
establish that they intersect along the x-
axis.
Wouldn't it be nice if there was a
general procedure for Ô¨Ånding the line of intersection of two planes?

5.1
LINES AND PLANES
185
You already know such a procedure! The line of intersection be-
tween the planes A1x + B1y + C1z = D1 and A2x + B2y + C2z = D2
is the solution of the following set of linear equations:
A1x + B1y + C1z = D1,
A2x + B2y + C2z = D2.
Example 3
Find the intersection of the planes 0x+0y+1z = 0 and
0x + 1y + 1z = 0. We follow the standard Gauss-Jordan elimination
procedure: construct an augmented matrix, perform row operations
(denoted ‚àº), obtain the RREF, and interpret the solution:
 0
0
1
0
0
1
1
0

‚àº
 0
1
1
0
0
0
1
0

‚àº
 0
1
0
0
0
0
1
0

.
The Ô¨Årst column is a free variable t ‚ààR. The solution is the line
Ô£±
Ô£≤
Ô£≥
x = t
y = 0
z = 0
,
‚àÄt ‚ààR
Ô£º
Ô£Ω
Ô£æ=
Ô£±
Ô£≤
Ô£≥
Ô£Æ
Ô£∞
0
0
0
Ô£π
Ô£ª+ t
Ô£Æ
Ô£∞
1
0
0
Ô£π
Ô£ª,
‚àÄt ‚ààR
Ô£º
Ô£Ω
Ô£æ,
which corresponds to the x-axis.
Planes
A plane P in R3 is a two-dimensional space with inÔ¨Ånite extent. In
general, we specify a plane through a constraint equation that must
be satisÔ¨Åed by all points in the plane:
P : {(x, y, z) ‚ààR3 | Ax + By + Cz = D}.
The plane P is the set of all points (x, y, z) ‚ààR3 that satisfy the
equation Ax + By + Cz = D. The equation Ax + By + Cz = D is
called the general equation of the plane. This deÔ¨Ånition represents
the algebraic view of planes, which is useful for calculations.
There is an equally useful geometric
view of planes. A plane can be speciÔ¨Åed
by a normal vector ‚Éón and some point po
in the plane. The normal vector ‚Éón is per-
pendicular to the plane: it sticks out at
right angles to the plane like the normal
force between surfaces in physics problems.
All points in the plane P can be obtained
starting from the point po and moving in a
direction orthogonal to the normal vector
‚Éón. The geometric formula of a plane is
P :
‚Éón ¬∑ [(x, y, z) ‚àípo] = 0.

186
GEOMETRICAL ASPECTS OF LINEAR ALGEBRA
Recall that the dot product of two vectors is zero if and only if
these vectors are orthogonal. In the above equation, the expression
[(x, y, z)‚àípo] forms an arbitrary vector with one endpoint at po. From
all these vectors, we select only those that are perpendicular to ‚Éón and
thus we obtain all the points in the plane.
The geometric equation ‚Éón ¬∑ [(x, y, z) ‚àípo] = 0 is equivalent to the
general equation Ax + By + Cz = D. We can Ô¨Ånd the parameters A,
B, C, and D by calculating the dot product: A = nx, B = ny, C = nz,
and D = ‚Éón ¬∑ po = nxpox + nypoy + nypoz.
Observe that scaling the general equation of a plane by a constant
factor does not change the plane: the equations Ax + By + Cz = D
and Œ±Ax + Œ±By + Œ±Cz = Œ±D deÔ¨Åne the same plane. Similarly the
geometric equations ‚Éón ¬∑ [(x, y, z) ‚àípo] = 0 and Œ±‚Éón ¬∑ [(x, y, z) ‚àípo] = 0
deÔ¨Åne the same plane. In each case, the direction of the normal vector
‚Éón is important, but not its length.
We can also give a parametric equation of a plane P. If we know
a point po in the plane and two linearly independent vectors ‚Éóv and ‚Éów
that lie in the plane, then a parametric equation for the plane can be
obtained as follows:
P : {(x, y, z) ‚ààR3 | (x, y, z) = po + s‚Éóv + t ‚Éów, s, t ‚ààR}.
Since a plane is a two-dimensional space, we need two parameters (s
and t) to describe the location of arbitrary points in the plane.
Suppose we're given three points p, q,
and r that lie in the plane.
How can
we Ô¨Ånd the geometric equation for this
plane ‚Éón ¬∑ [(x, y, z) ‚àípo] = 0? We can use
the point p as the reference point po, but
how do we Ô¨Ånd the normal vector ‚Éón for
the plane? The trick is to use the cross
product. First we build two vectors that
are parallel to the plane, ‚Éóv = q ‚àíp and
‚Éów = r ‚àíp, and then compute their cross
product to Ô¨Ånd a vector that is perpen-
dicular to both of them, and hence normal to the plane.
‚Éón = ‚Éóv √ó ‚Éów = (q ‚àíp) √ó (r ‚àíp).
We can use the vector ‚Éón to write the geometric equation of the plane
‚Éón ¬∑ [(x, y, z) ‚àíp] = 0. The key property we used is the fact that the
cross product of two vectors is perpendicular to both vectors. The
cross product is the perfect tool for Ô¨Ånding normal vectors.

5.1
LINES AND PLANES
187
Example 4
Consider the plane that contains the points p = (1, 0, 0),
q = (0, 1, 0), and r = (0, 0, 1). Find a geometric equation, a general
equation, and a parametric equation for this plane.
We need a normal vector for the geometric equation.
We can
obtain a normal vector from the cross product of the vectors ‚Éóv =
q ‚àíp = (‚àí1, 1, 0) and ‚Éów = r ‚àíp = (‚àí1, 0, 1), which both lie in the
plane. We obtain the normal ‚Éón = ‚Éóv √ó ‚Éów = (1, 1, 1) and write the
geometric equation as (1, 1, 1) ¬∑ [(x, y, z) ‚àí(1, 0, 0)] = 0, using p as
the point po in the plane. To Ô¨Ånd the general equation for the plane,
we compute the dot product in the geometric equation and obtain
1x + 1y + 1z ‚àí1 = 0, which is the same as x + y + z = 1. The vectors
‚Éóv and ‚Éów obtained above can also be used in the parametric equation
of the plane: {(1, 0, 0) + s(‚àí1, 1, 0) + t(‚àí1, 0, 1), s, t ‚ààR}.
Distance formulas
We'll now discuss three formulas for calculating distances: the dis-
tance between two points, the closest distance between a line and the
origin, and the closest distance between a plane and the origin.
Distance between points
The distance between points p and q is equal to the length of the
vector that goes from p to q:
d(p, q) = ‚à•q ‚àíp‚à•=
q
(qx ‚àípx)2 + (qy ‚àípy)2 + (qz ‚àípz)2.
Distance between a line and the origin
The closest distance between the line with
equation ‚Ñì: {po + t‚Éóv, t ‚ààR} and the origin
O = (0, 0, 0) is given by the formula
d(‚Ñì, O) =
po ‚àípo ¬∑ ‚Éóv
‚à•‚Éóv‚à•2 ‚Éóv
 .
Example 5
The closest distance between
the line ‚Ñì: {(4, 5, 6) + t(1, 0, 1), t ‚ààR}
and the origin O = (0, 0, 0) is calculated as
follows:
d(‚Ñì, O) =
(4, 5, 6) ‚àí(4, 5, 6) ¬∑ (1, 0, 1)
12 + 02 + 12
(1, 0, 1)

=
(4, 5, 6) ‚àí4 + 0 + 6
2
(1, 0, 1)

= ‚à•(‚àí1, 5, 1)‚à•= 3
‚àö
3.

188
GEOMETRICAL ASPECTS OF LINEAR ALGEBRA
Distance between a plane and the origin
The closest distance between the plane with
geometric equation P : ‚Éón¬∑[(x, y, z)‚àípo] = 0
and the origin O is given by
d(P, O) = |‚Éón ¬∑ po|
‚à•‚Éón‚à•.
For example, the distance between the plane
P : (‚àí3, 0, ‚àí4) ¬∑ [(x, y, z) ‚àí(1, 2, 3)] = 0 and
the origin is computed as
d(P, O) = |(‚àí3, 0, ‚àí4) ¬∑ (1, 2, 3)|
‚à•(‚àí3, 0, ‚àí4)‚à•
= | ‚àí3 ‚àí12|
5
= 15
5
= 3.
Discussion
The distance formulas given above are complicated expressions that
involve calculating dot products and taking the length of vectors. To
understand the logic behind these distance formulas, we need to learn
a bit about projective geometry. The techniques of projective geome-
try allow us to measure distances between arbitrary points, lines, and
planes. No new math operations are required. Instead, we'll learn
how to use a combination of vector subtraction, vector length, and
the dot product to compute distances. Each distance function d(¬∑, ¬∑)
corresponds to an abstract procedure with one or two steps which can
be described using a vector diagram. Projections play a key role in
projective geometry, so we'll learn about them in detail in the next
section.
Exercises
E5.1 Find the closest distance between ‚Ñì: {(4, 5, 6)+t(7, 8, 9), t ‚ààR}
and the origin.
E5.2 Find the distance between the plane P with geometric equation
(1, 1, 1) ¬∑ [(x, y, z) ‚àí(4, 5, 6)] = 0 and the origin.
E5.3 Two nonparallel planes in R3 intersect at a line.
Two in-
tersecting nonparallel lines ‚Ñì1 and ‚Ñì2 in R3 deÔ¨Åne a unique plane
P : ‚Éón ¬∑ [(x, y, z) ‚àípo] = 0 that contains both lines. More generally,
a pair of nonintersecting nonparallel lines ‚Ñì1 and ‚Ñì2 in R3 deÔ¨Ånes a
whole family of planes {‚Éón ¬∑ (x, y, z) = d, ‚àÄd ‚ààR} that are parallel to
both lines, that is, they never intersect with the two lines.
Find the equation of the plane that contains the line of intersection
of the planes x + 2y + z = 1 and 2x ‚àíy ‚àíz = 2 and is parallel to the
line with parametric equation x = 1 + 2t, y = ‚àí2 + t, z = ‚àí1 ‚àít.

5.2
PROJECTIONS
189
Figure 5.1: The line of intersection ‚Ñì1 of two planes and another line ‚Ñì2.
In E5.3 we want to Ô¨Ånd the plane that contains ‚Ñì1 and does't intersect ‚Ñì2.
E5.4 Find the general equation of the line that passes through the
points (0, 5) and (6, ‚àí7) in R2.
5.2
Projections
In this section we'll learn to compute projections of vectors onto lines
and planes. Given an arbitrary vector, you'll need to Ô¨Ånd how much of
this vector points in a given direction (projection onto a line), or you'll
need to Ô¨Ånd the part of the vector that lies in some plane (projection
onto a plane). The dot product, ‚Éóu ¬∑ ‚Éóv ‚â°u1v1 + u2v2 + u3v3, will play
a central role in these calculations.
Each projection formula corresponds to a vector diagram.
Vector diagrams, also known as "picture proofs," are used to describe
the precise sequence of operations used to compute a projection. Fo-
cussing on the vector diagrams will make it much easier to understand
the projection and distance formulas. Indeed, the pictures in this sec-
tion are a heck of a lot more important than the formulas. Be sure you
understand each vector diagram well and don't worry about memoriz-
ing the corresponding formula. You can easily reproduce the formula
by starting from the vector diagram.
Concepts
‚Ä¢ S ‚äÜRn: S is a vector subspace of Rn.
In this chapter, we
assume S ‚äÜR3. The subspaces of R3 are lines ‚Ñìand planes P
that pass through the origin.
‚Ä¢ S‚ä•: the orthogonal space to S, S‚ä•‚â°{‚Éów ‚ààRn | ‚Éów ¬∑ S = 0}.
The symbol ‚ä•stands for perpendicular to.
‚Ä¢ Œ†S: the projection onto the subspace S
‚Ä¢ Œ†S‚ä•: the projection onto the orthogonal space S‚ä•

190
GEOMETRICAL ASPECTS OF LINEAR ALGEBRA
DeÔ¨Ånitions
Let S be a vector subspace of Rn, denoted S ‚äÜRn. In this section,
we'll focus on the subspaces of the space R3. The vector subspaces of
R3 are lines and planes that pass through the origin.
The projection operation onto the subspace S is a linear transfor-
mation that takes as inputs vectors in R3, and produces outputs in
the subspace S:
Œ†S : R3 ‚ÜíS.
The transformation Œ†S cuts oÔ¨Äall parts of the input that do not lie
within the subspace S. We can understand Œ†S by analyzing its action
for diÔ¨Äerent inputs:
‚Ä¢ If ‚Éóv ‚ààS, then Œ†S(‚Éóv) = ‚Éóv.
‚Ä¢ If ‚Éów ‚ààS‚ä•, then Œ†S(‚Éów) = ‚Éó0.
‚Ä¢ Linearity and the above two conditions imply that, for any vec-
tor ‚Éóu = Œ±‚Éóv + Œ≤ ‚Éów with ‚Éóv ‚ààS and ‚Éów ‚ààS‚ä•, we have
Œ†S(‚Éóu) = Œ†S(Œ±‚Éóv + Œ≤ ‚Éów) = Œ±‚Éóv.
The orthogonal subspace to S is the set of vectors that are perpendic-
ular to all vectors in S:
S‚ä•‚â°{ ‚Éów ‚ààR3 | ‚Éów ¬∑ ‚Éós = 0, ‚àÄ‚Éós ‚ààS }.
The operator Œ†S "projects" to the space S in the sense that, no matter
which vector ‚Éóu you start from, applying the projection Œ†S will result
in a vector that is in S:
‚àÄ‚Éóu ‚ààR3,
Œ†S(‚Éóu) ‚ààS.
All parts of ‚Éóu that were in the perp-space S‚ä•will be killed by Œ†S.
Meet Œ†S‚Äîthe S-perp killer.
We can split the set of all vectors R3 into two disjoint sets: vectors
entirely contained in S and vectors perpendicular to S. We say R3
decomposes into the direct sum of the subspaces S and S‚ä•:
R3 = S ‚äïS‚ä•.
Any vector ‚Éóu ‚ààR3 can be split into an S-part ‚Éóv = Œ†S(‚Éóu) and a
S‚ä•-part ‚Éów = Œ†S‚ä•(‚Éóu), such that
‚Éóu = ‚Éóv + ‚Éów.
A deÔ¨Åning property of projections is that they are idempotent opera-
tions, meaning it doesn't matter if you project a vector once, twice,
or a million times; the result will always be the same:
Œ†S(‚Éóu) = Œ†S(Œ†S(‚Éóu)) = Œ†S(Œ†S(Œ†S(‚Éóu))) = . . . .

5.2
PROJECTIONS
191
Once you project a vector onto the subspace S, any further projections
to S will have no eÔ¨Äect.
In the remainder of this section, we'll derive formulas for projections
onto lines and planes that pass through the origin.
Projection onto a line
Consider the line ‚Ñìpassing through the origin with direction vector ‚Éóv:
‚Ñì: {(x, y, z) ‚ààR3 | (x, y, z) = ‚Éó0 + t‚Éóv, t ‚ààR}.
The projection onto ‚Ñìfor an arbitrary
vector ‚Éóu ‚ààR3 is given by the formula
Œ†‚Ñì(‚Éóu) = ‚Éóu ¬∑ ‚Éóv
‚à•‚Éóv‚à•2‚Éóv.
The orthogonal space to the line ‚Ñìcon-
sists of all vectors perpendicular to
the direction vector ‚Éóv. Mathematically
speaking,
‚Ñì‚ä•:
{(x, y, z) ‚ààR3 | (x, y, z) ¬∑ ‚Éóv = 0}.
Recognize that the equation (x, y, z) ¬∑ ‚Éóv = 0 deÔ¨Ånes a plane.
The
orthogonal space for a line ‚Ñìwith direction vector ‚Éóv is a plane with
normal vector ‚Éóv. Makes sense, yes?
We can easily Ô¨Ånd the projection operation onto ‚Ñì‚ä•as well. Any
vector can be written as the sum of an ‚Ñìpart and a ‚Ñì‚ä•part: ‚Éóu = ‚Éóv+ ‚Éów,
where ‚Éóv = Œ†‚Ñì(‚Éóu) ‚àà‚Ñìand ‚Éów = Œ†‚Ñì‚ä•(‚Éóu) ‚àà‚Ñì‚ä•. To obtain Œ†‚Ñì‚ä•(‚Éóu) we
subtract the Œ†‚Ñìpart from the original vector ‚Éóu:
Œ†‚Ñì‚ä•(‚Éóu) = ‚Éów = ‚Éóu ‚àí‚Éóv = ‚Éóu ‚àíŒ†‚Ñì(‚Éóu) = ‚Éóu ‚àí‚Éóu ¬∑ ‚Éóv
‚à•‚Éóv‚à•2‚Éóv.
We can think of Œ†‚Ñì‚ä•(‚Éóu) = ‚Éów as the part of ‚Éóu that remains after we've
removed the ‚Ñì-part.
Example 1
Consider the line ‚ÑìdeÔ¨Åned by the parametric equation
{(x, y, z) ‚ààR3 | (x, y, z) = t(1, 2, 3), t ‚ààR}. Find the projection of
the vector ‚Éóu = (4, 5, 6) onto ‚Ñì. Find the projection of ‚Éóu onto ‚Ñì‚ä•and
verify that Œ†‚Ñì(‚Éóu) + Œ†‚Ñì‚ä•(‚Éóu) = ‚Éóu.
The direction vector of the line ‚Ñìis ‚Éóv = (1, 2, 3), so Œ†‚Ñì(‚Éóu) =
‚Éóu¬∑‚Éóv
‚à•‚Éóv‚à•2‚Éóv = 16
7 ‚Éóv = ( 16
7 , 32
7 , 48
7 ). Next, using the formula Œ†‚Ñì‚ä•(‚Éóu) = ‚Éóu ‚àí
‚Éóu¬∑‚Éóv
‚à•‚Éóv‚à•2‚Éóv, we Ô¨Ånd Œ†‚Ñì‚ä•(‚Éóu) = ( 12
7 , 3
7, ‚àí6
7 ). Observe that ( 12
7 , 3
7, ‚àí6
7 ) ¬∑ ‚Éóv = 0,
which shows the vector Œ†‚Ñì‚ä•(‚Éóu) is indeed perpendicular to ‚Ñì. Adding
the results of the two projections, we obtain the whole ‚Éóu: ( 16
7 , 32
7 , 48
7 )+
( 12
7 , 3
7, ‚àí6
7 ) = ( 28
7 , 35
7 , 42
7 ) = (4, 5, 6) = ‚Éóu.

192
GEOMETRICAL ASPECTS OF LINEAR ALGEBRA
Projection onto a plane
Now consider the two-dimensional plane P passing through the origin
with normal vector ‚Éón:
P :
{(x, y, z) ‚ààR3 | ‚Éón ¬∑ (x, y, z) = 0}.
The perpendicular space S‚ä•is a line with direction vector ‚Éón:
P ‚ä•: {(x, y, z) ‚ààR3 | (x, y, z) = ‚Éó0 + t‚Éón, t ‚ààR}.
Again, the vector space R3 decomposes into the direct sum of P and
P ‚ä•: R3 = P ‚äïP ‚ä•.
We want to Ô¨Ånd Œ†P , but it will actu-
ally be easier to Ô¨Ånd Œ†P ‚ä•Ô¨Årst and then
compute Œ†P (‚Éóu) using Œ†P (‚Éóu) = ‚Éóv = ‚Éóu ‚àí‚Éów,
where ‚Éów = Œ†P ‚ä•(‚Éóu).
Since P ‚ä•is a line, we know the formula
for projecting onto it is
Œ†P ‚ä•(‚Éóu) = ‚Éóu ¬∑ ‚Éón
‚à•‚Éón‚à•2‚Éón.
We can now obtain the formula for Œ†P :
Œ†P (‚Éóu) = ‚Éóv = ‚Éóu ‚àí‚Éów = ‚Éóu ‚àíŒ†P ‚ä•(‚Éóu) = ‚Éóu ‚àí‚Éóu ¬∑ ‚Éón
‚à•‚Éón‚à•2‚Éón.
Example 2
Consider the plane P deÔ¨Åned by the geometric equation
(1, 1, 1) ¬∑ [(x, y, z) ‚àí(0, 0, 0)] = 0. Find the projection of the vector
‚Éóu = (4, 5, 6) onto P and onto P ‚ä•. Verify that Œ†P (‚Éóu) + Œ†P ‚ä•(‚Éóu) = ‚Éóu.
Using the formula Œ†P (‚Éóu) = ‚Éóu‚àí‚Éóu¬∑‚Éón
‚à•‚Éón‚à•2‚Éón, we Ô¨Ånd Œ†P (‚Éóu) = (‚àí1, 0, 1).
We also Ô¨Ånd Œ†P ‚ä•(‚Éóu) =
‚Éóu¬∑‚Éón
‚à•‚Éón‚à•2‚Éón = (5, 5, 5), which is a vector in the same
direction as ‚Éón. Observe that the vector ‚Éóu can be reconstructed by
adding the two projections: Œ†P (‚Éóu)+Œ†P ‚ä•(‚Éóu) = (‚àí1, 0, 1)+(5, 5, 5) =
(4, 5, 6) = ‚Éóu.
Distances formulas revisited
Suppose you want to Ô¨Ånd the distance between the line ‚Ñì: {(x, y, z) ‚àà
R3 | (x, y, z) = po + t ‚Éóv, t ‚ààR} and the origin O = (0, 0, 0). This
problem is equivalent to Ô¨Ånding the distance between the line ‚Ñì‚Ä≤ :
{(x, y, z) ‚ààR3 | (x, y, z) = ‚Éó0+t‚Éóv, t ‚ààR} and the point po, the answer
to which is the length of the projection Œ†‚Ñì‚ä•(po):
d(‚Ñì‚Ä≤, po) = ‚à•Œ†‚Ñì‚ä•(po)‚à•=
po ‚àípo ¬∑ ‚Éóv
‚à•‚Éóv‚à•2 ‚Éóv
 .

5.2
PROJECTIONS
193
The distance between a plane P : ‚Éón¬∑[(x, y, z)‚àípo] = 0 and the origin
O is the same as the distance between the plane P ‚Ä≤ : ‚Éón ¬∑ (x, y, z) = 0
and the point po. We can obtain this distance by Ô¨Ånding the length
of the projection of po onto P ‚Ä≤‚ä•using the formula
d(P ‚Ä≤, po) = |‚Éón ¬∑ po|
‚à•‚Éón‚à•.
You should try drawing the pictures for the above two scenarios and
make sure the formulas make sense to you.
Projections matrices
Since projections are linear transformations, they can be expressed as
matrix-vector products:
‚Éóv = Œ†(‚Éóu)
‚áî
‚Éóv = MŒ†‚Éóu.
Multiplying the vector ‚Éóu by the matrix MŒ† is the same as applying
the projection Œ†.
We'll learn more about projection matrices later. For now, I'll
show you a simple example of a projection matrix in R3. Let Œ† be the
projection onto the xy-plane. This projection operation corresponds
to the following matrix-vector product:
Œ†(‚Éóu) = MŒ†‚Éóu =
Ô£Æ
Ô£∞
1
0
0
0
1
0
0
0
0
Ô£π
Ô£ª
Ô£Æ
Ô£∞
ux
uy
uz
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
ux
uy
0
Ô£π
Ô£ª.
Note how multiplying a vector by MŒ† results in selecting only the x-
and y-components of the vector while killing the z-component, which
is precisely what the projection onto the xy-plane is supposed to do.
Discussion
In the next section we'll talk about a particular set of projections
known as coordinate projections. We use coordinate projections to
Ô¨Ånd the components of vectors ‚Éóv with respect to a coordinate system:
vxÀÜƒ± ‚â°Œ†x(‚Éóv) = ‚Éóv ¬∑ ÀÜƒ±
‚à•ÀÜƒ±‚à•2ÀÜƒ± = (‚Éóv ¬∑ ÀÜƒ±)ÀÜƒ±,
vyÀÜÔöæ‚â°Œ†y(‚Éóv) = ‚Éóv ¬∑ ÀÜÔöæ
‚à•ÀÜÔöæ‚à•2 ÀÜÔöæ= (‚Éóv ¬∑ ÀÜÔöæ)ÀÜÔöæ,
vzÀÜk ‚â°Œ†z(‚Éóv) = ‚Éóv ¬∑ ÀÜk
‚à•ÀÜk‚à•2
ÀÜk = (‚Éóv ¬∑ ÀÜk)ÀÜk.
The coordinate projection Œ†x projects onto the x-axis, and similarly
Œ†y and Œ†z project onto the y- and z-axes.

194
GEOMETRICAL ASPECTS OF LINEAR ALGEBRA
Exercises
E5.5 Find the orthogonal projection of the vector ‚Éóv = (4, 5, 6) onto
the plane that contains the vectors (2, ‚àí4, 2) and (6, 1, ‚àí4).
5.3
Coordinate projections
In science, it's common to express vectors as components: (vx, vy, vz).
Thinking of vectors as arrays of numbers is Ô¨Åne for computational pur-
poses, since vector operations require manipulating the components
of vectors.
However, this way of thinking about vectors overlooks
an important concept‚Äîthe basis with respect to which the vector's
components are expressed.
It's not uncommon for students to have misconceptions about lin-
ear algebra due to an incomplete understanding of the fundamental
distinction between vectors and their components. Since I want you to
have a thorough understanding of linear algebra, we'll review‚Äîin full
detail‚Äîthe notion of a basis and how to compute vector components
with respect to diÔ¨Äerent bases.
Example
In physics we learn how to work with vectors in terms of
their x- and y-components. Given a standard xy-coordinate system,
we can decompose a force vector ‚ÉóF in terms of its components:
Fx = ‚à•‚ÉóF‚à•cos Œ∏,
Fy = ‚à•‚ÉóF‚à•sin Œ∏,
where Œ∏ is the angle the vector ‚ÉóF makes with the x-axis. We can
express the vector as coordinates with respect to the basis {ÀÜƒ±, ÀÜÔöæ} as
‚ÉóF = FxÀÜƒ± + FyÀÜÔöæ= (Fx, Fy)ÀÜƒ±ÀÜÔöæ.
The number Fx corresponds to the length of the projection of
the vector ‚ÉóF onto the x-axis. In the last section, we discussed the
projection operation and learned how to compute projections using
the dot product or as a matrix-vector product:
Fx ÀÜƒ± =
‚ÉóF ¬∑ ÀÜƒ±
‚à•ÀÜƒ±‚à•2ÀÜƒ± = Œ†x(‚ÉóF) =
1
0
0
0

| {z }
MŒ†x
Fx
Fy

|{z}
‚ÉóF
,
where Œ†x : R2 ‚ÜíR2 is the projection operator onto the x-axis (a
linear transformation) and MŒ†x is its matrix representation with re-
spect to the basis {ÀÜƒ±, ÀÜÔöæ}. Now we'll discuss the relationship between
vectors and their representation in terms of coordinates with respect
to diÔ¨Äerent bases.

5.3
COORDINATE PROJECTIONS
195
Concepts
We can deÔ¨Åne three diÔ¨Äerent types of bases for an n-dimensional vec-
tor space V :
‚Ä¢ A generic basis Bf = {‚Éóf1, ‚Éóf2, . . . , ‚Éófn} consists of any set of lin-
early independent vectors in V .
‚Ä¢ An orthogonal basis Be = {‚Éóe1,‚Éóe2, . . . ,‚Éóen} consists of n mutually
orthogonal vectors in V obeying ‚Éóei ¬∑ ‚Éóej = 0, ‚àÄi Ã∏= j.
‚Ä¢ An orthonormal basis BÀÜe = {ÀÜe1, ÀÜe2, . . . , ÀÜen} is an orthogonal
basis of unit length vectors: ÀÜei ¬∑ ÀÜej = 0, ‚àÄi Ã∏= j and ÀÜei ¬∑ ÀÜei =
‚à•ÀÜei‚à•= 1, ‚àÄi ‚àà{1, 2, . . . , n}.
A vector ‚Éóv is expressed as coordinates vi with respect to any basis B:
‚Éóv = v1‚Éóe1 + v2‚Éóe2 + ¬∑ ¬∑ ¬∑ + vn‚Éóen = (v1, v2, . . . , vn)B.
We can use two diÔ¨Äerent bases B and B‚Ä≤ to express the same vector:
‚Ä¢ ‚Éóv: a vector
‚Ä¢ [‚Éóv]B = (v1, v2, . . . , vn)B: the vector ‚Éóv expressed in the basis B
‚Ä¢ [‚Éóv]B‚Ä≤ = (v‚Ä≤
1, v‚Ä≤
2, . . . , v‚Ä≤
n)B‚Ä≤: the same vector ‚Éóv expressed in a dif-
ferent basis B‚Ä≤
‚Ä¢
[1]
B‚Ä≤
B: the change-of-basis matrix that converts from B coor-
dinates to B‚Ä≤ coordinates: [‚Éóv]B‚Ä≤ =
[1]
B‚Ä≤
B [‚Éóv]B.
Components with respect to a basis
A vector's components describe how much of the vector lies in a given
direction. For example, a vector ‚Éóv ‚ààR3 expressed as components
with respect to the standard orthonormal basis {ÀÜƒ±, ÀÜÔöæ, ÀÜk} is denoted ‚Éóv =
(vx, vy, vz)ÀÜƒ±ÀÜÔöæÀÜk. The components of a vector are also called coordinates
(in the context of a coordinate system) and coeÔ¨Écients (in the context
of a linear combination).
Don't be confused by this multitude of
terms: it's the same idea.
When the basis consists of a set of orthonormal vectors like the
vectors ÀÜƒ±, ÀÜÔöæ, and ÀÜk, we can compute vector components using the dot
product:
vx = ‚Éóv ¬∑ ÀÜƒ±,
vy = ‚Éóv ¬∑ ÀÜÔöæ,
vz = ‚Éóv ¬∑ ÀÜk.
In this section, we'll discuss how to Ô¨Ånd coordinates with respect to
three diÔ¨Äerent types of bases: orthonormal bases, orthogonal bases,
and generic bases. First, let's give a precise deÔ¨Ånition of what a basis
is.

196
GEOMETRICAL ASPECTS OF LINEAR ALGEBRA
DeÔ¨Ånition of a basis
A basis B = {‚Éóe1,‚Éóe2, . . . ,‚Éóen} for the vector space V has the following
two properties:
‚Ä¢ Spanning property. Any vector ‚Éóv ‚ààV can be expressed as a
linear combination of the basis elements:
‚Éóv = v1‚Éóe1 + v2‚Éóe2 + ¬∑ ¬∑ ¬∑ + vn‚Éóen.
This property guarantees that the vectors in the basis B are
suÔ¨Écient to represent any vector in V .
‚Ä¢ Linear independence property. The vectors that form the
basis B = {‚Éóe1,‚Éóe2, . . . ,‚Éóen} are linearly independent. The linear
independence of the vectors in the basis guarantees that none
of the vectors ‚Éóei is redundant.
If a set of vectors B = {‚Éóe1,‚Éóe2, . . . ,‚Éóen} satisÔ¨Åes both properties, we
say B is a basis for V . In other words, B can serve as a coordinate
system for V .
Coordinates with respect to an orthonormal basis
An orthonormal basis BÀÜe = {ÀÜe1, ÀÜe2, . . . , ÀÜen} consists of a set of mutu-
ally orthogonal, unit-length vectors:
ÀÜei ¬∑ ÀÜej =
(
1
if i = j,
0
if i Ã∏= j.
The vectors are mutually orthogonal since ÀÜei ¬∑ ÀÜei for all i Ã∏= j, and the
vectors have unit length since ÀÜei ¬∑ ÀÜei = 1 implies ‚à•ÀÜei‚à•= 1.
To compute the components of the vector ‚Éóa with respect to an
orthonormal basis BÀÜe we use the standard "prescription" similar to
the one we used for the {ÀÜƒ±, ÀÜÔöæ, ÀÜk} basis:
(a1, a2, . . . , an)BÀÜe ‚áî(‚Éóa ¬∑ ÀÜe1)
| {z }
a1
ÀÜe1 + (‚Éóa ¬∑ ÀÜe2)
| {z }
a2
ÀÜe2 + ¬∑ ¬∑ ¬∑ + (‚Éóa ¬∑ ÀÜen)
| {z }
an
ÀÜen.
Coordinates with respect to an orthogonal basis
Consider a basis that is orthogonal, but not orthonormal:
Be =
{‚Éóe1,‚Éóe2, . . . ,‚Éóen}.
We can compute the coordinates of any vector ‚Éób
with respect to the basis {‚Éóe1,‚Éóe2, . . . ,‚Éóen} as follows:
(b1, b2, . . . , bn)Be ‚áî
 ‚Éób ¬∑ ‚Éóe1
‚à•‚Éóe1‚à•2
!
‚Éóe1 +
 ‚Éób ¬∑ ‚Éóe2
‚à•‚Éóe2‚à•2
!
‚Éóe2 + ¬∑ ¬∑ ¬∑ +
 ‚Éób ¬∑ ‚Éóen
‚à•‚Éóen‚à•2
!
‚Éóen.

5.3
COORDINATE PROJECTIONS
197
To Ô¨Ånd the coeÔ¨Écients of the vector ‚Éób with respect to Be, we use the
general projection formula:
b1 =
‚Éób ¬∑ ‚Éóe1
‚à•‚Éóe1‚à•2 ,
b2 =
‚Éób ¬∑ ‚Éóe2
‚à•‚Éóe2‚à•2 ,
¬∑ ¬∑ ¬∑ ,
bn =
‚Éób ¬∑ ‚Éóen
‚à•‚Éóen‚à•2 .
Observe that each component of the vector can be computed indepen-
dently of the other components: to compute b1, all we need to know
is ‚Éób and ‚Éóe1; we don't need to know ‚Éóe2, ‚Éóe3, . . ., ‚Éóen, because we know
the other basis vectors are orthogonal to ‚Éóe1. The computation of the
coeÔ¨Écients correspond to n independent orthogonal projections. The
coeÔ¨Écient bi tells us how much of the basis vector ‚Éóei we need in the
linear combination to construct the vector ‚Éób.
Coordinates with respect to a generic basis
Suppose we're given a generic basis {‚Éóf1, ‚Éóf2, . . . , ‚Éófn} for a vector space.
To Ô¨Ånd the coeÔ¨Écients (c1, c2, . . . , cn) of a vector ‚Éóc with respect to this
basis, we need to solve the equation
c1 ‚Éóf1 + c2 ‚Éóf2 + ¬∑ ¬∑ ¬∑ + cn ‚Éófn = ‚Éóc
for the unknowns c1, c2, . . . , cn.
Because the vectors {‚Éófi} are not
orthogonal, the calculation of the coeÔ¨Écients c1, c2, . . . , cn must be
done simultaneously.
Example
Express the vector ‚Éóv = (5, 6)ÀÜƒ±ÀÜÔöæin terms of the basis Bf =
{‚Éóf1, ‚Éóf2} where ‚Éóf1 = (1, 1)ÀÜƒ±ÀÜÔöæand ‚Éóf2 = (3, 0)ÀÜƒ±ÀÜÔöæ.
We are looking for the coeÔ¨Écients v1 and v2 such that
(v1, v2)Bf = v1 ‚Éóf1 + v2 ‚Éóf2 = ‚Éóv = (5, 6)ÀÜƒ±ÀÜÔöæ.
To Ô¨Ånd the coeÔ¨Écients we need to solve the following system of equa-
tions simultaneously:
1v1 + 3v2 = 5
1v1 + 0 = 6.
From the second equation we Ô¨Ånd v1 = 6. We substitute this answer
into the Ô¨Årst equation and Ô¨Ånd v2 = ‚àí1
3 . Thus, the vector ‚Éóv written
with respect to the basis {‚Éóf1, ‚Éóf2} is ‚Éóv = 6‚Éóf1 ‚àí1
3 ‚Éóf2 =
 6, ‚àí1
3

Bf .
The general case of computing a vector's coordinates with respect
to a generic basis {‚Éóf1, ‚Éóf2, . . . , ‚Éófn} requires solving a system of n equa-
tions in n unknowns. You know how to do this, but it will take some

198
GEOMETRICAL ASPECTS OF LINEAR ALGEBRA
work. The take-home message is that orthogonal and orthonormal
bases are easiest to work with, since computing vector coordinates
requires only computing projections as opposed to solving systems of
equations.
Change of basis
We often identify a vector ‚Éóv with its components (vx, vy, vz).
It's
important to always keep in mind the basis with respect to which the
coeÔ¨Écients are taken, and if necessary specify the basis as a subscript
‚Éóv = (vx, vy, vz)ÀÜƒ±ÀÜÔöæÀÜk. When performing vector arithmetic operations like
‚Éóu + ‚Éóv, we don't really care what basis the vectors are expressed in so
long as the same basis is used for both ‚Éóu and ‚Éóv.
We sometimes need to use multiple bases, however. Consider the
basis B = {ÀÜe1, ÀÜe2, ÀÜe3} and another basis B‚Ä≤ = {ÀÜe‚Ä≤
1, ÀÜe‚Ä≤
2, ÀÜe‚Ä≤
3}. Suppose
we're given the coordinates v1, v2, v3 of some vector ‚Éóv in terms of the
basis B:
‚Éóv = (v1, v2, v3)B = v1ÀÜe1 + v2ÀÜe2 + v3ÀÜe3.
How can we Ô¨Ånd the coeÔ¨Écients of ‚Éóv in terms of the basis B‚Ä≤?
This is called a change-of-basis transformation and is performed
as a matrix multiplication with a change-of-basis matrix:
Ô£Æ
Ô£∞
v‚Ä≤
1
v‚Ä≤
2
v‚Ä≤
3
Ô£π
Ô£ª
B‚Ä≤
=
Ô£Æ
Ô£∞
ÀÜe‚Ä≤
1 ¬∑ ÀÜe1
ÀÜe‚Ä≤
1 ¬∑ ÀÜe2
ÀÜe‚Ä≤
1 ¬∑ ÀÜe3
ÀÜe‚Ä≤
2 ¬∑ ÀÜe1
ÀÜe‚Ä≤
2 ¬∑ ÀÜe2
ÀÜe‚Ä≤
2 ¬∑ ÀÜe3
ÀÜe‚Ä≤
3 ¬∑ ÀÜe1
ÀÜe‚Ä≤
3 ¬∑ ÀÜe2
ÀÜe‚Ä≤
3 ¬∑ ÀÜe3
Ô£π
Ô£ª
|
{z
}
[1]
B‚Ä≤
B
Ô£Æ
Ô£∞
v1
v2
v3
Ô£π
Ô£ª
B
.
The columns of the change-of-basis matrix describe the vectors of the
basis {ÀÜei} in terms of the basis {ÀÜe‚Ä≤
i}.
Note that multiplying a vector by the matrix
[1]
B‚Ä≤
B doesn't ac-
tually do anything since it doesn't change the vector. The change-
of-basis operation acts like the identity transformation, which is why
we use the notation
[1]
B‚Ä≤
B to describe it.
The vector ‚Éóv stays the
same‚Äîit is simply expressed in terms of a diÔ¨Äerent basis:
(v‚Ä≤
1, v‚Ä≤
2, v‚Ä≤
3)B‚Ä≤= v‚Ä≤
1ÀÜe‚Ä≤
1+v‚Ä≤
2ÀÜe‚Ä≤
2+v‚Ä≤
3ÀÜe‚Ä≤
3 = ‚Éóv = v1ÀÜe1+v2ÀÜe2+v3ÀÜe3 =(v1, v2, v3)B.
We say the vector ‚Éóv has two representations. The vector ‚Éóv corresponds
to the triple of coeÔ¨Écients (v1, v2, v3) with respect to the basis B and
to the triple (v‚Ä≤
1, v‚Ä≤
2, v‚Ä≤
3) with respect to the basis B‚Ä≤.
The matrix
[1]
B‚Ä≤
B contains the information about how each vector
of the old basis (B) is expressed in terms of the new basis (B‚Ä≤). For
example, the Ô¨Årst column of the change-of-basis matrix describes how
the vector ÀÜe1 is expressed in terms of the basis B‚Ä≤:
ÀÜe1 = (ÀÜe‚Ä≤
1 ¬∑ ÀÜe1) ÀÜe‚Ä≤
1 + (ÀÜe‚Ä≤
2 ¬∑ ÀÜe1) ÀÜe‚Ä≤
2 + (ÀÜe‚Ä≤
3 ¬∑ ÀÜe1) ÀÜe‚Ä≤
3 = (ÀÜe‚Ä≤
1 ¬∑ ÀÜe1, ÀÜe‚Ä≤
2 ¬∑ ÀÜe1, ÀÜe‚Ä≤
3 ¬∑ ÀÜe1)B‚Ä≤ .

5.4
VECTOR SPACES
199
Note this is the generic formula for expressing vectors in the basis B‚Ä≤.
To Ô¨Ånd the entries of the change-of-basis matrix
[1]
B‚Ä≤
B between
orthonormal bases B and B‚Ä≤, it's suÔ¨Écient to compute all the dot
products ÀÜe‚Ä≤
i ¬∑ ÀÜej. To compute the entries of a change-of-basis matrix
between bases B and B‚Ä≤ (which are orthogonal but not necessarily
orthonormal) we use (
[1]
B‚Ä≤
B)ij =
‚Éóe‚Ä≤
i¬∑‚Éóej
‚à•‚Éóe‚Ä≤
i‚à•‚à•‚Éóej‚à•. Computing the change-
of-basis matrix between nonorthogonal bases is more complicated.
Links
[ Khan Academy video on the change-of-basis operation ]
http://youtube.com/watch?v=meibWcbGqt4
Exercises
5.4
Vector spaces
Get ready‚Äîwe're about to shift gears from individual vectors to entire
sets of vectors.
We're entering the territory of vector spaces.
For
instance, the set of all possible three-dimensional vectors is denoted
R3, and is a type of vector space. A vector space consists of a set of
vectors and all linear combinations of these vectors. This means if the
vectors ‚Éóv1 and ‚Éóv2 are part of some vector space, then so is the vector
Œ±‚Éóv1 + Œ≤‚Éóv2 for any Œ± and Œ≤. A vector subspace consists of a subset of
all possible vectors. The vector subspaces of R3 are lines and planes
that pass through the origin.
Since vector spaces and subspaces play a central role in many areas
of linear algebra, you'll want to learn about the properties of vector
spaces and develop your vocabulary for describing them.
By using the language of vector spaces, you'll be able to describe
certain key properties of matrices. The fundamental subspaces associ-
ated with a matrix A are its column space C(A), its row space R(A),
its null space N(A), and its left null space N(AT). Let's now deÔ¨Åne
these vector spaces and discuss how they help us understand the solu-
tions to the matrix equation A‚Éóx = ‚Éób, and the properties of the linear
transformation TA(‚Éóx) ‚â°A‚Éóx.
DeÔ¨Ånitions
‚Ä¢ V : a vector space
‚Ä¢ ‚Éóv: a vector. We use the notation ‚Éóv ‚ààV to indicate the vector ‚Éóv
is part of the vector space V .
‚Ä¢ W: a vector subspace. We use the notation W ‚äÜV to indicate
the vector space W is a subspace of the vector space V .

200
GEOMETRICAL ASPECTS OF LINEAR ALGEBRA
‚Ä¢ span: the span of a set of vectors is the set of vectors that can
be constructed as linear combinations of these vectors:
span{‚Éóv1, . . . ,‚Éóvn} ‚â°{‚Éóv ‚ààV | ‚Éóv = Œ±1‚Éóv1 + ¬∑ ¬∑ ¬∑ + Œ±n‚Éóvn, Œ±i ‚ààR}.
For every matrix M ‚ààRm√ón, we deÔ¨Åne the following fundamental
vector spaces associated with the matrix M:
‚Ä¢ R(M) ‚äÜRn: the row space of the matrix M consists of all
possible linear combinations of the rows of the matrix M.
‚Ä¢ C(M) ‚äÜRm: the column space of the matrix M consists of all
possible linear combinations of the columns of the matrix M.
‚Ä¢ N(M) ‚äÜRn: the null space of M is the set of vectors that go to
the zero vector when multiplying M from the right: N(M) ‚â°
{‚Éóv ‚ààRn | M‚Éóv = ‚Éó0 }.
‚Ä¢ N(M T): the left null space of M is the set of vectors that go to
the zero vector when multiplying M from the left: N(M T) ‚â°
{‚Éów ‚ààRm | ‚ÉówTM = ‚Éó0}.
Vector space
A vector space V consists of a set of vectors and all possible lin-
ear combinations of these vectors. The notion of all possible linear
combinations is very powerful.
In particular, it implies two useful
properties. First, vector spaces are closed under addition: for all vec-
tors in that space, the sum of two vectors is also a vector in that
vector space. Mathematically, we write this as
‚àÄ‚Éóv1,‚Éóv2 ‚ààV,
‚Éóv1 + ‚Éóv2 ‚ààV.
Recall the symbol "‚àÄ" is math shorthand for the phrase "for all."
Second, vector spaces are closed under scalar multiplication:
‚àÄŒ± ‚ààR and ‚Éóv ‚ààV,
Œ±‚Éóv ‚ààV.
These two properties codify the essential nature of what a vector
space is: a space of vectors that can be added together and scaled by
constants.
Span
The span operator is a useful shorthand for denoting "the set of all
linear combinations" of some set of vectors. This may seem like a
weird notion at Ô¨Årst, but it will prove very useful for describing vector
spaces.

5.4
VECTOR SPACES
201
Let's now illustrate how to deÔ¨Åne vectors spaces using the span
operator through some examples. Given a vector ‚Éóv1 ‚ààV , deÔ¨Åne the
following vector space:
V1 ‚â°span{‚Éóv1} = {‚Éóv ‚ààV | ‚Éóv = Œ±‚Éóv1 for some Œ± ‚ààR}.
We say V1 is spanned by ‚Éóv1, which means any vector in V1 can be
written as a multiple of ‚Éóv1. The shape of V1 is an inÔ¨Ånite line.
Given two vectors ‚Éóv1,‚Éóv2 ‚ààV , we deÔ¨Åne the vector space spanned
by these vectors as follows:
V2 ‚â°span{‚Éóv1,‚Éóv2} = {‚Éóv ‚ààV | ‚Éóv = Œ±‚Éóv1 + Œ≤‚Éóv2 for some Œ±, Œ≤ ‚ààR}.
The vector space V2 contains all vectors that can be written as a linear
combination of ‚Éóv1 and ‚Éóv2. This is a two-dimensional vector space that
has the shape of an inÔ¨Ånite plane passing through the origin.
Now suppose we're given three vectors ‚Éóv1,‚Éóv2,‚Éóv3 ‚ààV , such that
‚Éóv3 = ‚Éóv1+‚Éóv2, and are asked to deÔ¨Åne the vector space V3 ‚â°span{‚Éóv1,‚Éóv2,‚Éóv3}.
The vector space V3 is actually the same as V2; adding the vector ‚Éóv3
to the span of ‚Éóv1 and ‚Éóv2 does not enlarge the vector space because
the vector ‚Éóv3 is a linear combination of ‚Éóv1 and ‚Éóv2.
Geometrically
speaking, the vector ‚Éóv3 lies in the same plane as ‚Éóv1 and ‚Éóv2.
Consider the vector space V ‚Ä≤
2 = span{‚Éóv1,‚Éóv‚Ä≤
2}, where ‚Éóv‚Ä≤
2 = Œ≥‚Éóv1, for
some Œ≥ ‚ààR. Since ‚Éóv‚Ä≤
2 is a linear combination of the vector ‚Éóv1, the
vector space V ‚Ä≤
2 is one-dimensional. In fact, V ‚Ä≤
2 is the same as the
vector space V1 deÔ¨Åned above: V ‚Ä≤
2 = span{‚Éóv1,‚Éóv‚Ä≤
2} = span{‚Éóv1} = V1.
Vector subspaces
We use the notation W ‚äÜV to indicate that W is a subspace of V . A
subspace is a subset of the vectors in the larger space that has a vector
space structure. In other words, W ‚äÜV if the following conditions
are satisÔ¨Åed:
‚Ä¢ W is contained in V : for all ‚Éów, if ‚Éów ‚ààW, then ‚Éów ‚ààV .
‚Ä¢ W is closed under addition: for all ‚Éów1, ‚Éów2 ‚ààW, ‚Éów1 + ‚Éów2 ‚ààW.
‚Ä¢ W is closed under scalar multiplication: for all ‚Éów ‚ààW, Œ±‚Éów ‚ààW.
Subspaces always contain the zero vector ‚Éó0. This is implied by the
third condition: any vector becomes the zero vector when multiplied
by the scalar Œ± = 0: Œ±‚Éów = 0‚Éów = ‚Éó0.
Subspaces speciÔ¨Åed by constraints
One way to deÔ¨Åne a vector subspace W is to start with a larger
space V and describe a constraint that is satisÔ¨Åed by all vectors in

202
GEOMETRICAL ASPECTS OF LINEAR ALGEBRA
the subspace W. For example, the xy-plane is deÔ¨Åned as the set of
vectors (x, y, z) ‚ààR3 that satisfy the constraint
(0, 0, 1) ¬∑ (x, y, z) = 0.
More formally, we deÔ¨Åne the xy-plane as
Pxy = {(x, y, z) ‚ààR3 | (0, 0, 1) ¬∑ (x, y, z) = 0}.
Since the vector ÀÜk ‚â°(0, 0, 1) is perpendicular to all vectors in the
xy-plane, we can describe the xy-plane as "the set of all vectors per-
pendicular to the vector ÀÜk."
Subspaces speciÔ¨Åed as a span
Another way to represent the xy-plane is to describe it as the span of
two linearly independent vectors in the plane:
Pxy = span{ (1, 0, 0), (0, 1, 0) },
which is equivalent to saying:
Pxy = {‚Éóv ‚ààR3 | ‚Éóv = Œ±(1, 0, 0) + Œ≤(0, 1, 0), ‚àÄŒ±, Œ≤ ‚ààR}.
This expression is a parametrization of the space Pxy with Œ± and Œ≤ as
the two parameters. Each point in the plane is described by a unique
pair of parameters (Œ±, Œ≤). The parametrization of an m-dimensional
vector space requires m parameters.
Subsets vs subspaces
Let's clarify the distinction between the terms subset and subspace.
Assume we're working in a vector space V . A subset of V can be
described in the form S = {‚Éóv ‚ààV | ‚ü®conditions‚ü©} and consists of all
vectors in V that satisfy the ‚ü®conditions‚ü©. A subspace W is a type
of subset with a vector space structure, meaning it is closed under
addition (for all ‚Éów1, ‚Éów2 ‚ààW,
‚Éów1 + ‚Éów2 ‚ààW), and closed under
scalar multiplication (for all ‚Éów ‚ààW, Œ±‚Éów ‚ààW). In linear algebra, the
terms subset and subspace are used somewhat interchangeably, and
the same symbol is used to denote both subset (S ‚äÜV ) and subspace
(W ‚äÜV ) relationships. When mathematicians refer to some subset
as a subspace, they're letting you know that you can take arbitrary
elements in the set, scale or add them together, and obtain an element
of the same set.
To illustrate the diÔ¨Äerence between a subset and a subspace, con-
sider the solution sets of a system of equations A‚Éóx = ‚Éób versus the

5.4
VECTOR SPACES
203
system of equations A‚Éóx = ‚Éó0. The solution set of A‚Éóx = ‚Éó0 is a vector
space that is called null space of A and denoted N(A). If ‚Éóx1 and ‚Éóx2
are two solutions to A‚Éóx = ‚Éó0, then Œ±‚Éóx1 + Œ≤‚Éóx2 is also a solution to
A‚Éóx = ‚Éó0. In contrast, the solution set of A‚Éóx = ‚Éób is {‚Éóc + ‚Éóvn}, for all
‚Éóvn ‚ààN(A), which is not a subspace of V unless ‚Éóc = ‚Éó0. Observe that
if ‚Éóx1 = ‚Éóc + ‚Éóv1 and ‚Éóx2 = ‚Éóc + ‚Éóv2 are two solutions to A‚Éóx = ‚Éób, their sum
is not a solution: ‚Éóx1 + ‚Éóx2 = 2‚Éóc + ‚Éóv1 + ‚Éóv2 /‚àà{‚Éóc + ‚Éóvn}.
A real-life situation
You walk into class one day and are caught completely oÔ¨Äguard
by a surprise quiz‚Äîwait, let's make it a mini-exam for emotional
eÔ¨Äect. Although you've read a chapter or two in the book, you've been
"busy" and are totally unprepared for this exam. The Ô¨Årst question
asks you to "Ô¨Ånd the solution of the homogenous system of equations
and the non-homogenous system of equations." You rack your brain,
but the only association with homogeny that comes to mind is the
homogenized milk you had for breakfast.
Wait, there's more‚Äîthe
question also asks you to "state whether each of the solutions obtained
is a vector space." Staring at the page, the words and equations begin
to blur as panic sets in.
Don't fear! Look at the problem again. You don't know what
the heck a homogenous system of equations is, but you sure as heck
know how to solve systems of equations. You solve the given system
of equations A‚Éóx = ‚Éób by building the augmented matrix [ A |‚Éób ] and
computing its reduced row echelon form using row operations. You
obtain the solution set ‚Éóx = {‚Éóv ‚ààV |‚Éóv = ‚Éóc + s‚Éóvn, ‚àÄs ‚ààR}, where ‚Éóc is
the particular solution and ‚Éóvn is a vector that spans the null space of
A.
Next, you ponder the "vector space" part of the question. You
notice the solution set to the system of equations A‚Éóx = ‚Éób isn't a
vector space since it doesn't pass through the origin. However, the
solution set to the equation A‚Éóx = ‚Éó0 is a vector space {‚Éóv ‚ààV |‚Éóv =
s‚Éóvn, ‚àÄs ‚ààR} = span{‚Éóvn}. Suddenly it clicks: a homogenous system
of equations must be the system of equations A‚Éóx = ‚Éó0, in which the
constants of the right-hand side are all zero. The term homogenous
kind of makes sense; all the constants of the right-hand side have the
same value b1 = b2 = ¬∑ ¬∑ ¬∑ = 0. The solution to the non-homogenous
system of equations A‚Éóx = ‚Éób is the set {‚Éóc + s‚Éóvn, ‚àÄs ‚ààR}, which is not
a vector space. The solution to the homogenous system of equations
A‚Éóx = ‚Éó0 is {‚Éóv ‚ààV |‚Éóv = s‚Éóvn, ‚àÄs ‚ààR}, which is a vector space. Well
done!

204
GEOMETRICAL ASPECTS OF LINEAR ALGEBRA
Matrix fundamental spaces
We now deÔ¨Åne four fundamental vector spaces associated with a ma-
trix M ‚ààRm√ón.
‚Ä¢ The column space C(M) is the span of the columns of the matrix.
The column space consists of all possible output vectors the
matrix can produce when multiplied by a vector from the right:
C(M) ‚â°{‚Éów ‚ààRm | ‚Éów = M‚Éóv for some ‚Éóv ‚ààRn}.
‚Ä¢ The null space N(M) of a matrix M ‚ààRm√ón consists of all
vectors the matrix M sends to the zero vector:
N(M) ‚â°{‚Éóv ‚ààRn | M‚Éóv = ‚Éó0}.
The null space is sometimes called the kernel of the matrix.
‚Ä¢ The row space R(M) is the span of the rows of the matrix.
We obtain linear combinations of the rows by multiplying the
matrix on the left with an m-dimensional vector:
R(M) ‚â°{‚Éóv ‚ààRn | ‚Éóv = ‚ÉówTM for some ‚Éów ‚ààRm}.
Note, we used the transpose T to transform ‚Éów to a row vector.
‚Ä¢ The left null space N(M T) of a matrix M ‚ààRm√ón consists of all
vectors the matrix M sends to the zero vector when multiplied
from the left:
N(M) ‚â°{‚Éów ‚ààRm | ‚ÉówTM = ‚Éó0}.
These vector spaces are called fundamental because they describe
important properties of the matrix M. Recall that matrix equations
can be used to represent systems of linear equations, and in connection
with linear transformations. A solid understanding of fundamental
spaces of a matrix leads to a solid understanding of linear equations
and linear transformations.
Matrices and systems of linear equations
The null space N(M) corresponds to the solution of the matrix equa-
tion M‚Éóx = ‚Éó0. If a matrix has a nonempty null space, the system
of equations corresponding to M‚Éóx = ‚Éób has an inÔ¨Ånite solution set.
Indeed, we can write the solution of M‚Éóx = ‚Éób as a particular solution
‚Éóc plus all possible vectors in the null space of M:
‚Éóx = ‚Éóc + span{‚Éóv1, . . . ,‚Éóvk},
where
span{‚Éóv1, . . . ,‚Éóvk} ‚â°N(M).

5.4
VECTOR SPACES
205
We can verify this claim as follows. Suppose ‚Éóx = ‚Éóc is a solution to
the equation M‚Éóx = ‚Éób. Consider the vector ‚Éóu ‚â°‚Éóc + Œ±‚Éóv1 + ¬∑ ¬∑ ¬∑ + Œ≥‚Éóvk,
which contains ‚Éóc and some arbitrary linear combinations of vectors
from the null space of M. Observe that ‚Éóu is also a solution to the
equation M‚Éóx = ‚Éób:
M‚Éóu = M(‚Éóc+Œ±‚Éóv1 +¬∑ ¬∑ ¬∑+Œ≥‚Éóvk) = M‚Éóc+Œ±
*0
M‚Éóv1 +¬∑ ¬∑ ¬∑+Œ≥
*0
M‚Éóvk = M‚Éóc = ‚Éób.
If the null space of M contains only the zero vector {‚Éó0}, then the
system of equations M‚Éóx = ‚Éób has a unique solution.
Matrices and linear transformations
Matrices can be used to represent linear transformations. We post-
pone the detailed discussion about linear transformations and their
representation as matrices until Chapter 6, but we'll discuss the sub-
ject here brieÔ¨Çy‚Äîmainly to introduce an important connection be-
tween the column space and the row space of a matrix, and to explain
why each matrix has two null spaces (what's up with that?).
Matrix-vector and vector-matrix products
A matrix M ‚ààRm√ón corresponds to not one but two linear trans-
formations. Up until now we've focused on the matrix-vector prod-
uct M‚Éóx = ‚Éóy, which implements a linear transformation of the form
TM : Rn ‚ÜíRm. In addition to the linear transformation implemented
by multiplication from the right, there is also the option of multiply-
ing M by a vector from the left: ‚ÉóaTM = ‚ÉóbT, where ‚ÉóaT (the input) is
an m-dimensional row vector, and‚ÉóbT (the output) is an n-dimensional
row vector.1
The vector-matrix product ‚ÉóaTM = ‚ÉóbT implements the linear trans-
formation of the form TM T : Rm ‚ÜíRn. We identify the output of
this linear transformation, ‚Éób = TM T(‚Éóa), with the result of the vector-
matrix product ‚ÉóbT ‚â°‚ÉóaTM. The linear transformation TM T : Rm ‚Üí
Rn is called the adjoint of the linear transformation TM : Rn ‚ÜíRm.
Adjoint linear transformations are represented by the same matrix M:
TM is deÔ¨Åned as the multiplication of M from the right, while TM T is
deÔ¨Åned as multiplication of M from the left.
Let's clarify why we used the notation TM T to denote the adjoint
operation of TM. We previously used the notation TA to describe the
linear transformation obtained by right multiplication by A: TA(‚Éóx) ‚â°
A‚Éóx. Instead of creating a new notation for left multiplication, we can
1Our convention is to assume vectors are column vectors by default.
The
transpose operation is required to obtain row vectors.

206
GEOMETRICAL ASPECTS OF LINEAR ALGEBRA
transform left multiplication into right multiplication by using the
transpose operation:
‚ÉóaTA = ‚ÉóbT
‚áí
 ‚ÉóaTA
T =

‚ÉóbTT
‚áí
AT‚Éóa = ‚Éób.
We can think of left multiplication by A as right multipli-
cation by AT. This correspondence also explains why we use the
notation N(M T) for the left null space of M; we can rewrite the
condition ‚ÉówTM = ‚Éó0T as M T ‚Éów = ‚Éó0, so the left null space of M is
equivalent to the right null space of M T.
Left and right input spaces
Let's call left space of M the set of vectors suitable for multiplying M
from the left. Similarly we'll call right space of M the set of vectors
suitable for multiplying M from the right. If M ‚ààRm√ón, the left
space is Rm and the right space is Rn.
By combining all the vectors in the row space of M and all the
vectors in its null space, we obtain the full right space:
R(M) ‚äïN(M) = Rn.
This means any vector ‚Éóv ‚ààRn can be written as a sum ‚Éóv = ‚Éóvr + ‚Éóvn,
such that ‚Éóvr ‚ààR(M) and ‚Éóvn ‚ààN(M). The symbol ‚äïstands for
orthogonal sum, which means we can pick ‚Éóvr and ‚Éóvn to be orthogonal
vectors, ‚Éóvr ¬∑ ‚Éóvn = 0.
If we consider the dimensions involved the above equation, we
obtain the following important relation between the dimension of the
row space and the null space of a matrix:
dim(R(M)) + dim(N(M)) = n = dim(Rn).
The n-dimensional right space splits into row-space dimensions and
null-space dimensions.
Similarly to the split in the right space, the left-space Rm decom-
poses into an orthogonal sum of the column space and the left null
space of the matrix:
C(M) ‚äïN(M T) = Rm.
If we count the dimensions in this equation, we obtain a relation
between the dimension of the column space and the left null space of
the matrix: dim(C(M)) + dim(N(M T)) = m.

5.4
VECTOR SPACES
207
Matrix rank
The column space and the row space of a matrix have the same di-
mension. We call this dimension the rank of the matrix:
rank(M) ‚â°dim (R(M)) = dim (C(M)) .
The rank of M is the number of linearly independent rows in M,
which is equal to the number of linearly independent columns in M.
The dimension of the null space of M is called the nullity of M:
nullity(M) ‚â°dim(N(M)).
Applying this new terminology, we can update our earlier obser-
vation about the dimensions of the right fundamental spaces of a
matrix:
rank(M) + nullity(M) = n = dim(Rn).
This formula is called the rank-nullity theorem, and can be used to
deduce the rank of a matrix given its nullity, or vice versa.
Summary
Together, R(M), N(M), C(M), and N(M T) describe all aspects of
the matrix M when multiplied by vectors from the left or the right.
Everything we've learned so far about how the matrix M maps vectors
between its left and right spaces can be summarized by the following
observations:
C(M)
M
‚Üê‚Üí
R(M),
‚Éó0
M
‚Üê‚àí
N(M),
N(M T)
M
‚àí‚Üí
‚Éó0.
Note the zero vector in the second row is ‚Éó0 ‚ààRm, while the zero
vector in the third row is ‚Éó0 ‚ààRn. In Section 6.1, we'll learn how
to interpret the fundamental vectors spaces of the matrix M as the
input and output spaces of the linear transformations TM : Rn ‚ÜíRm
and TM T : Rm ‚ÜíRn.
Throughout this section, we've referred to the notion of dimension
of a vector space. The dimension of a vector space is the number of
elements in a basis for that vector space. Before we can give a formal
deÔ¨Ånition of the dimension of a vector space, we must review and
solidify our understanding of the concept of a basis. We begin with a
formal deÔ¨Ånition of what it means for a set of vectors to be linearly
independent.

208
GEOMETRICAL ASPECTS OF LINEAR ALGEBRA
Linear independence
One of the most important ideas in linear algebra is the notion of
linear independence. We're often interested in Ô¨Ånding vectors that
cannot be written as linear combinations of others.
The set of vectors {‚Éóv1,‚Éóv2, . . . ,‚Éóvn} is linearly independent if the only
solution to the equation
Œ±1‚Éóv1 + Œ±2‚Éóv2 + ¬∑ ¬∑ ¬∑ + Œ±n‚Éóvn = ‚Éó0
is Œ±i = 0 for all i.
If ‚ÉóŒ± = ‚Éó0 is the only solution to the system of equations then none
of the vectors ‚Éóvi can be written as a linear combination of the other
vectors.
To understand the importance of the "all zero alphas" requirement,
let's consider an example where a nonzero solution ‚ÉóŒ± exists. Suppose
the set of vectors {‚Éóv1,‚Éóv2,‚Éóv3} satisfy Œ±1‚Éóv1 + Œ±2‚Éóv2 + Œ±3‚Éóv3 = 0, with
Œ±1 = ‚àí1, Œ±2 = 1, and Œ±3 = 2. Then we can write ‚Éóv1 = 1‚Éóv2 + 2‚Éóv3,
which shows that ‚Éóv1 is a linear combination of ‚Éóv2 and ‚Éóv3, hence the
vectors are not linearly independent.
The strange wording of the
deÔ¨Ånition in terms of an "all zero alphas" solution is required to make
the deÔ¨Ånition of linear independence symmetric. An all zero alphas
solution implies that no vector can be written as a linear combination
of the other vectors.
Basis
To carry out calculations with vectors in a vector space V , we need to
know a basis B = {‚Éóe1,‚Éóe2, . . . ,‚Éóen} for that space. Intuitively, a basis
for a vector space is any set of vectors that can serve as a coordinate
system for that vector space.
A basis for an n-dimensional vector
space V is a set of n linearly independent vectors in V .
The dimension of a vector space is deÔ¨Åned as the number of vectors
in a basis for that vector space. A basis for an n-dimensional vector
space contains exactly n vectors. Any set of less than n vectors would
not satisfy the spanning property; any set of more than n vectors
from V cannot be linearly independent (see page 196). To form a
basis for a vector space, a set of vectors must be "just right": it must
contain a suÔ¨Écient number of vectors (but not too many) so that the
coeÔ¨Écients of each vector will be uniquely determined.

5.4
VECTOR SPACES
209
The rank-nullity theorem
The relation between the rank and the nullity of a matrix are so im-
portant that it's worth formally stating the rank-nullity theorem
and showing its proof.
Rank-nullity theorem: For any matrix M ‚ààRm√ón, the following
statement holds:
rank(M) + nullity(M) = n,
where rank(M) ‚â°dim (R(M)) and nullity(M) ‚â°dim(N(M)).
It's not absolutely essential that you understand the proof of this
theorem, but it's a good idea to read it, as the proof will give you
some "exposure" to formal math language used in proofs.
Proof. We must show that the equation rank(M) + nullity(M) = n
holds.
Suppose rank(M) = k and Br = {‚Éóvr1, . . . ,‚Éóvrk} is a basis
for R(M).
Assume furthermore that nullity(M) = ‚Ñìand Bn =
{‚Éóvn1, . . . ,‚Éóvn‚Ñì} is a basis for the null space of M.
To prove the equation k + ‚Ñì= n, which relates the dimensions of
the row space and the null space to the dimension of Rn, we must
prove the equivalent statement about the number of vectors in the
bases Br and Bn. To prove the statement k + ‚Ñì= n, we must show
that the combination of the vectors of Br and Bn form a basis for Rn.
First, suppose that k+‚Ñì< n, and the combined basis vectors from
Br and Bn are insuÔ¨Écient to form a basis for Rn. This means there
must exist a vector ‚Éóv ‚ààRn which cannot be expressed in the form:
‚Éóv = vr1‚Éóvr1 + ¬∑ ¬∑ ¬∑ + vrk‚Éóvrk + vn1‚Éóvn1 + ¬∑ ¬∑ ¬∑ vn‚Ñì‚Éóvn‚Ñì.
Since every vector in R(M) can be expressed in the basis Br and
every vector in N(M) can be expressed in the basis Bn, it must be
that ‚Éóv /‚ààR(M) and ‚Éóv /‚ààN(M). However, ‚Éóv /‚ààR(M) implies M‚Éóv = ‚Éó0
since each ‚Éóv must be orthogonal to each of the rows of M. The fact
M‚Éóv = ‚Éó0 contradicts the fact ‚Éóv /‚ààN(M), therefore such a vector ‚Éóv must
not exist; every vector ‚Éóv ‚ààRn can be written as a linear combination
of the combination of the vectors in the bases Br and Bn.
Now let's analyze the other option k + ‚Ñì> n. The vectors in the
bases Br and Bn are linearly independent among themselves. All the
vectors in Br are orthogonal to all the vectors in Bn. Since orthogonal
implies linearly independent, the combined set of vectors Br ‚à™Bn is
a linearly independent set. Therefore the equation k + ‚Ñì> n is im-
possible because it would imply the existence of more than n linearly
independent vectors in an n-dimensional vector space.

210
GEOMETRICAL ASPECTS OF LINEAR ALGEBRA
The rank-nullity theorem is important because it "splits" the vec-
tors in the right space of the matrix into two categories: those that
lie in its row space and those that lie in its null space. We can use
the rank-nullity theorem to infer the dimension of the row space
of a matrix given the dimension of the null space and vice versa.
Example 1
Suppose the matrix M ‚ààRm√ón has a trivial null space
N(M) = {‚Éó0}; then nullity(M) ‚â°dim(N(M)) = 0 and we can con-
clude rank(M) ‚â°dim(R(M)) = n.
Example 2
Consider a matrix A ‚ààR3√ó6. After performing some
calculations, which we'll discuss in the next section, we Ô¨Ånd that one
of the rows of the matrix is a linear combination of the other two.
Therefore, the row space of A is two-dimensional and rank(A) = 2.
From this, we can infer nullity(A) = 6‚àí2 = 4, meaning the null space
of A is four-dimensional.
Distilling bases
A basis for an n-dimensional vector space V consist of exactly n vec-
tors. Any set of vectors {‚Éóe1,‚Éóe2, . . . ,‚Éóen} can serve as a basis as long as
the vectors are linearly independent and there is exactly n of them.
Sometimes an n-dimensional vector space V will be speciÔ¨Åed as
the span of more than n vectors:
V = span{‚Éóv1,‚Éóv2, . . . ,‚Éóvm},
m > n.
Since there are m > n of the ‚Éóv-vectors, they are too many to form a
basis. We say this set of vectors is over-complete. They cannot all be
linearly independent since there can be at most n linearly independent
vectors in an n-dimensional vector space.
If we want to Ô¨Ånd a basis for the space V , we'll have to reject some
of the vectors. Given the set of vectors {‚Éóv1,‚Éóv2, . . . ,‚Éóvm}, our task is
to distill a set of n linearly independent vectors {‚Éóe1,‚Éóe2, . . . ,‚Éóen} from
them. We'll learn how to do this in the next section.
Exercises
5.5
Vector space techniques
In this section, we'll learn how to "distill" a basis for any vector space;
an important procedure for characterizing vector spaces. Actually, the
procedure is not new‚Äîit's really an application of the Gauss-Jordan
elimination procedure we saw in Section 4.1.

5.5
VECTOR SPACE TECHNIQUES
211
Starting from a set of vectors that are not linearly independent,
we can write them as the rows of a matrix, and then perform row
operations on this matrix until we Ô¨Ånd the reduced row echelon form
of the matrix. Since row operations do not change the row space of
a matrix, the nonzero rows in the Ô¨Ånal RREF of the matrix span the
same space as the original set of vectors. The rows in the RREF of
the matrix will be linearly independent so they form a basis.
The ability to distill a basis is important when characterizing any
vector space. The basis serves as the coordinate system for that vector
space, and the number of vectors in a basis tells us the dimension of
the vector space. For this reason, we'll spend an entire section learning
how to distill bases for various vector spaces.
Finding a basis
Suppose the vector subspace V is deÔ¨Åned as the span of m vectors
{‚Éóu1, ‚Éóu2, . . . , ‚Éóum}, which are not necessarily linearly independent:
V ‚â°span{‚Éóu1, ‚Éóu2, . . . , ‚Éóum}.
Our task is to Ô¨Ånd a basis for V .
We're looking for an alternate
description of the vector space V as
V = span{‚Éóe1,‚Éóe2, . . . ,‚Éóen},
such that the vectors {‚Éóe1,‚Éóe2, . . . ,‚Éóen} will be linearly independent.
One way to accomplish this task is to write the vectors ‚Éóui as the
rows of a matrix M. By this construction, the space V corresponds
to the row space of the matrix M, denoted R(M).
We can then
use standard row operations to bring the matrix into its reduced row
echelon form. Applying row operations to a matrix does not change
its row space: R(M) = R(rref(M)). By transforming the matrix into
its RREF, we're able to see which of the rows are linearly independent
and can thus serve as basis vectors:
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
‚Äî
‚Éóu1
‚Äî
‚Äî
‚Éóu2
‚Äî
‚Äî
‚Éóu3
‚Äî
...
‚Äî
‚Éóum
‚Äî
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
‚àíG-J elimination ‚Üí
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
‚Äî
‚Éóe1
‚Äî
...
‚Äî
‚Éóen
‚Äî
0
0
0
0
0
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
The nonzero rows in the RREF of the matrix form a set of linearly
independent vectors {‚Éóe1,‚Éóe2, . . . ,‚Éóen} that span the vector space V .
The linearly dependent vectors have been reduced to rows of zeros.
The above process is called "Ô¨Ånding a basis" or "distilling a basis"
and it's important you understand how to carry out this procedure.

212
GEOMETRICAL ASPECTS OF LINEAR ALGEBRA
Even more important is that you understand why we'd want to distill a
basis in the Ô¨Årst place! By the end of the Gauss-Jordan procedure, we
obtain a description of the same vector space V in terms of a new set
of vectors. Why is it better to describe the vector space V in terms of
the vectors {‚Éóe1,‚Éóe2, . . . ,‚Éóen}, rather than in terms of {‚Éóu1, ‚Éóu2, . . . , ‚Éóum}?
I'll tell you exactly why. We prefer to use the basis {‚Éóe1,‚Éóe2, . . . ,‚Éóen}
to characterize the vector space V because there exists a one-to-
one correspondence between each vector ‚Éóv ‚ààV and the coeÔ¨Écients
v1, v2, . . . , vn in the linear combination
‚Éóv = v1‚Éóe1 + v2‚Éóe2 + ¬∑ ¬∑ ¬∑ + vn‚Éóen.
Using the basis B ‚â°{‚Éóe1,‚Éóe2, . . . ,‚Éóen} allows us to represent each vector
‚Éóv ‚ààV as a unique list of coeÔ¨Écients (v1, v2, . . . , vn)B.
This would not be possible if we used the vectors {‚Éóu1, ‚Éóu2, . . . , ‚Éóum}
to describe the vector space V . Since the vectors {‚Éóu1, ‚Éóu2, . . . , ‚Éóum} are
not linearly independent, the same vector ‚Éóv could be represented by
many diÔ¨Äerent linear combination of the form
‚Éóv = v‚Ä≤
1‚Éóu1 + v‚Ä≤
2‚Éóu2 + ¬∑ ¬∑ ¬∑ + v‚Ä≤
m‚Éóum.
We cannot identify ‚Éóv with a unique set of coeÔ¨Écients v‚Ä≤
1, v‚Ä≤
2, . . . , v‚Ä≤
m,
therefore vectors are not represented faithfully by their coeÔ¨Écients.
Another reason we prefer to describe V in terms of a basis is
because we can immediately see the vector space V is n-dimensional,
since there are n vectors in the basis for V .
DeÔ¨Ånitions
‚Ä¢ B = {‚Éóe1,‚Éóe2, . . . ,‚Éóen}. A basis for an n-dimensional vector space
S is a set of n linearly independent vectors that span S. Any
vector ‚Éóv ‚ààS can be written as a linear combination of the basis
elements:
‚Éóv = v1‚Éóe1 + v2‚Éóe2 + ¬∑ ¬∑ ¬∑ + vn‚Éóen.
A basis for an n-dimensional vector space contains exactly n
vectors.
‚Ä¢ dim(S): the dimension of the vector space S is equal to the
number of elements in a basis for S.
Recall the four fundamental spaces of a matrix M ‚ààRm√ón we deÔ¨Åned
in the previous section:
‚Ä¢ R(M) ‚äÜRn: the row space of the matrix M that consists of all
possible linear combinations of the rows of the matrix M.

5.5
VECTOR SPACE TECHNIQUES
213
‚Ä¢ N(M) ‚äÜRn: the null space of the matrix contains all the vec-
tors that become the zero vector when multiplied by M:
N(M) ‚â°{‚Éóv ‚ààRn | M‚Éóv = ‚Éó0 }.
‚Ä¢ C(M) ‚äÜRm: the column space of the matrix M that consists of
all possible linear combinations of the columns of the matrix M.
‚Ä¢ N(M T) ‚äÜRm: the left null space of the matrix contains all the
vectors that become the zero vector when multiplying M from
the left:
N(M T) ‚â°{‚Éów ‚ààRm | ‚ÉówTM = ‚Éó0T}.
Bases for the fundamental spaces of matrices
Performing the Gauss-Jordan elimination procedure on a matrix A
has the eÔ¨Äect of "distilling" a basis for its row space R(A). How do
we Ô¨Ånd bases for the other fundamental spaces of a matrix? In this
subsection we'll learn about a useful shortcut for computing bases for
the column space C(A) and the null space N(A) of a matrix, starting
from the reduced row echelon form of the matrix. Sorry, there is no
shortcut for Ô¨Ånding the left null space‚Äîwe'll have to use the transpose
operation to obtain AT and then Ô¨Ånd its null space N(AT).
Pay careful attention to the locations of the pivots (leading ones) in
the RREF of A because they play an important role in the procedures
described below.
Basis for the row space
The row space R(A) of a matrix A is deÔ¨Åned as the space of all vectors
that can be written as linear combinations of the rows of A. To Ô¨Ånd
a basis for R(A), we use the Gauss-Jordan elimination procedure:
1. Perform row operations to Ô¨Ånd the RREF of A.
2. The nonzero rows in the RREF of A form a basis for R(A).
Basis for the column space
To Ô¨Ånd a basis for the column space C(A) of a matrix A, we need to
determine which columns of A are linearly independent. To Ô¨Ånd the
linearly independent columns of A, follow these steps:
1. Perform row operations to Ô¨Ånd the RREF of A.
2. Identify the columns which contain the pivots (leading ones).
3. The corresponding columns in the original matrix A form a
basis for the column space of A.

214
GEOMETRICAL ASPECTS OF LINEAR ALGEBRA
This procedure works because elementary row operations do not change
the independence relations between the columns of the matrix.
If
two columns are linearly independent in the RREF of A, then these
columns were also linearly independent in the original matrix A.
Note that the column space of the matrix A corresponds to the
row space of the matrix transposed AT.
Thus, another procedure
for Ô¨Ånding a basis for the column space of a matrix A is to use the
Ô¨Ånd-a-basis-for-the-row-space procedure on AT.
Basis for the null space
The null space N(A) of a matrix A ‚ààRm√ón is
N(A) = {‚Éóx ‚ààRn | A‚Éóx = ‚Éó0 }.
The vectors in the null space are orthogonal to the row space of the
matrix A.
The null space of A is the solution of the equation A‚Éóx = ‚Éó0. You
should already be familiar with the procedure for Ô¨Ånding the solution
of systems of equations from Section 4.1. The steps of the procedure
are:
1. Perform row operations to Ô¨Ånd the RREF of A.
2. Identify the columns that do not contain a leading one. These
columns correspond to free variables of the solution. For exam-
ple, consider a matrix whose reduced row echelon form is
rref(A) =
Ô£Æ
Ô£∞
1
2
0
0
0
0
1
‚àí3
0
0
0
0
Ô£π
Ô£ª.
The second column and the fourth column do not contain lead-
ing ones (pivots), so they correspond to free variables, which are
customarily called s, t, r, etc. We're looking for a vector with
two free variables: (x1, s, x3, t)T.
3. Rewrite the null space problem as a system of equations:
Ô£Æ
Ô£∞
1
2
0
0
0
0
1
‚àí3
0
0
0
0
Ô£π
Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£∞
x1
s
x3
t
Ô£π
Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£∞
0
0
0
Ô£π
Ô£ª
‚áí
1x1 + 2s
=
0
1x3 ‚àí3t
=
0
0
=
0 .
We can express the unknowns x1 and x3 in terms of the free
variables s and t: x1 = ‚àí2s and x3 = 3t. The vectors in the
null space are of the form (‚àí2s, s, 3t, t)T, for all s, t ‚ààR. We

5.5
VECTOR SPACE TECHNIQUES
215
can rewrite this expression by splitting it into an s-part and a
t-part:
Ô£Æ
Ô£ØÔ£ØÔ£∞
x1
x2
x3
x3
Ô£π
Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚àí2s
s
3t
t
Ô£π
Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚àí2
1
0
0
Ô£π
Ô£∫Ô£∫Ô£ªs +
Ô£Æ
Ô£ØÔ£ØÔ£∞
0
0
3
1
Ô£π
Ô£∫Ô£∫Ô£ªt.
4. The direction vectors associated with each free variable form a
basis for the null space of the matrix A:
N(A) =
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚àí2s
s
3t
t
Ô£π
Ô£∫Ô£∫Ô£ª, ‚àÄs, t ‚ààR
Ô£º
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£æ
= span
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚àí2
1
0
0
Ô£π
Ô£∫Ô£∫Ô£ª,
Ô£Æ
Ô£ØÔ£ØÔ£∞
0
0
3
1
Ô£π
Ô£∫Ô£∫Ô£ª
Ô£º
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£æ
.
Verify that the matrix A multiplied by any vector from its null space
produces a zero vector.
Examples
Let's check out a couple examples that illustrate the procedures for
Ô¨Ånding bases for R(A), C(A), and N(A).
It's important that you
become proÔ¨Åcient at these "Ô¨Ånd a basis" tasks because they often
appear on homework assignments and exams.
Example 1
Find a basis for the row space, the column space, and
the null space of the matrix:
A =
Ô£Æ
Ô£∞
4
‚àí4
0
1
1
‚àí2
2
‚àí6
4
Ô£π
Ô£ª.
The Ô¨Årst steps toward Ô¨Ånding the row space, column space, and the
null space of a matrix all require calculating the RREF of the matrix,
so this is what we'll do Ô¨Årst.
‚Ä¢ Let's focus on the Ô¨Årst column. To create a pivot in the top left
corner, we divide the Ô¨Årst row by 4, denoted R1 ‚Üê1
4R1:
Ô£Æ
Ô£∞
1
‚àí1
0
1
1
‚àí2
2
‚àí6
4
Ô£π
Ô£ª.
‚Ä¢ We use this pivot to clear the numbers on the second and third
rows by performing R2 ‚ÜêR2 ‚àíR1 and R3 ‚ÜêR3 ‚àí2R1:
Ô£Æ
Ô£∞
1
‚àí1
0
0
2
‚àí2
0
‚àí4
4
Ô£π
Ô£ª.

216
GEOMETRICAL ASPECTS OF LINEAR ALGEBRA
‚Ä¢ We can create a pivot in the second row if we divide it by 2,
denoted R2 ‚Üê1
2R2:
Ô£Æ
Ô£∞
1
‚àí1
0
0
1
‚àí1
0
‚àí4
4
Ô£π
Ô£ª.
‚Ä¢ We now clear the entry below the pivot using R3 ‚ÜêR3 + 4R2:
Ô£Æ
Ô£∞
1
‚àí1
0
0
1
‚àí1
0
0
0
Ô£π
Ô£ª.
‚Ä¢ The Ô¨Ånal simpliÔ¨Åcation step is to clear the ‚àí1 in the Ô¨Årst row
using R1 ‚ÜêR1 + R2:
Ô£Æ
Ô£∞
1
0
‚àí1
0
1
‚àí1
0
0
0
Ô£π
Ô£ª.
Now that we have the RREF of the matrix, we can answer the ques-
tions like professionals.
Before we Ô¨Ånd bases for the fundamental spaces of A, let's Ô¨Årst do
some basic dimension counting. Observe that the matrix has just two
pivots. We say rank(A) = 2. This means both the row space and the
column spaces are two-dimensional.
Recall the equation n = rank(A)+nullity(A), which we saw in the
previous section. The right space R3 splits into two types of vectors:
those in the row space of A and those in the null space. Since we
know the row space is two-dimensional, we can deduce the dimension
of the null space: nullity(A) ‚â°dim(N(A)) = n‚àírank(A) = 3‚àí2 = 1.
Now let's answer the questions posed in the problem.
‚Ä¢ The row space of A consists of the two nonzero vectors in the
RREF of A:
R(A) = span{(1, 0, ‚àí1), (0, 1, ‚àí1)}.
‚Ä¢ To Ô¨Ånd the column space of A, observe that the Ô¨Årst and the
second columns contain the pivots in the RREF of A. If they
do, then the Ô¨Årst two columns of the original matrix A form a
basis for the column space of A:
C(A) = span
Ô£±
Ô£≤
Ô£≥
Ô£Æ
Ô£∞
4
1
2
Ô£π
Ô£ª,
Ô£Æ
Ô£∞
‚àí4
1
‚àí6
Ô£π
Ô£ª
Ô£º
Ô£Ω
Ô£æ.

5.5
VECTOR SPACE TECHNIQUES
217
‚Ä¢ Let's now Ô¨Ånd an expression for the null space of A.
First,
observe that the third column does not contain a pivot. No pivot
indicates that the third column corresponds to a free variable;
it can take on any value x3 = t,
t ‚ààR. We want to give a
description of all vectors (x1, x2, t)T that satisfy the system of
equations:
Ô£Æ
Ô£∞
1
0
‚àí1
0
1
‚àí1
0
0
0
Ô£π
Ô£ª
Ô£Æ
Ô£∞
x1
x2
t
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
0
0
0
Ô£π
Ô£ª
‚áí
1x1 ‚àí1t
=
0
1x2 ‚àí1t
=
0
0
=
0 .
We Ô¨Ånd x1 = t and x2 = t and obtain the following Ô¨Ånal expres-
sion for the null space:
N(A) =
Ô£±
Ô£≤
Ô£≥
Ô£Æ
Ô£∞
t
t
t
Ô£π
Ô£ª, t ‚ààR
Ô£º
Ô£Ω
Ô£æ= span
Ô£±
Ô£≤
Ô£≥
Ô£Æ
Ô£∞
1
1
1
Ô£π
Ô£ª
Ô£º
Ô£Ω
Ô£æ.
The null space of A is one-dimensional and consists of all mul-
tiples of the vector (1, 1, 1)T.
Example 2
Find a basis for the row space, column space, and null
space of the matrix:
B =
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
3
1
4
2
7
3
9
1
5
3
1
1
2
0
8
Ô£π
Ô£∫Ô£∫Ô£ª.
First we Ô¨Ånd the reduced row echelon form of the matrix B:
‚àº
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
3
1
4
0
1
1
1
0
2
2
‚àí3
0
‚àí1
‚àí1
4
Ô£π
Ô£∫Ô£∫Ô£ª‚àº
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
0
‚àí2
1
0
1
1
1
0
0
0
‚àí5
0
0
0
5
Ô£π
Ô£∫Ô£∫Ô£ª‚àº
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
0
‚àí2
0
0
1
1
0
0
0
0
1
0
0
0
0
Ô£π
Ô£∫Ô£∫Ô£ª.
As in the previous example, we begin by calculating the dimensions of
the subspaces. The rank of this matrix is three, so the column space
and the row space will be three-dimensional. Since the right space is
R4, this leaves one dimension for the null space. Next, let's Ô¨Ånd the
fundamental spaces for the matrix B.
‚Ä¢ The row space of B consists of the three nonzero vectors in the
RREF of B:
R(B) = span{ (1, 0, ‚àí2, 0), (0, 1, 1, 0), (0, 0, 0, 1)}.

218
GEOMETRICAL ASPECTS OF LINEAR ALGEBRA
‚Ä¢ The column space of B is spanned by the Ô¨Årst, second and fourth
columns of B since these columns contain the leading ones in
the RREF of B:
C(B) = span
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
2
1
1
Ô£π
Ô£∫Ô£∫Ô£ª,
Ô£Æ
Ô£ØÔ£ØÔ£∞
3
7
5
2
Ô£π
Ô£∫Ô£∫Ô£ª,
Ô£Æ
Ô£ØÔ£ØÔ£∞
4
9
1
8
Ô£π
Ô£∫Ô£∫Ô£ª
Ô£º
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£æ
.
‚Ä¢ The third column lacks a leading one, so it corresponds to a free
variable x3 = t, t ‚ààR. The null space of B is the set of vectors
(x1, x2, t, x4)T such that:
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
0
‚àí2
0
0
1
1
0
0
0
0
1
0
0
0
0
Ô£π
Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£∞
x1
x2
t
x4
Ô£π
Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£∞
0
0
0
0
Ô£π
Ô£∫Ô£∫Ô£ª
‚áí
1x1 ‚àí2t
=
0
1x2 + 1t
=
0
x4
=
0
0
=
0 .
We Ô¨Ånd the values of x1, x2, and x4 in terms of t and obtain
N(B) =
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
Ô£Æ
Ô£ØÔ£ØÔ£∞
2t
‚àít
t
0
Ô£π
Ô£∫Ô£∫Ô£ª, t ‚ààR
Ô£º
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£æ
= span
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
Ô£Æ
Ô£ØÔ£ØÔ£∞
2
‚àí1
1
0
Ô£π
Ô£∫Ô£∫Ô£ª
Ô£º
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£æ
.
Discussion
Dimensions
For an m √ó n matrix M ‚ààRm√ón the row space and the column space
consist of vectors with n components, while the column space and the
left null space consist of vectors with m components.
Don't confuse the number of components of vectors in a vector
space with the dimension of the space. Suppose we're given a matrix
with Ô¨Åve rows and ten columns M ‚ààR5√ó10 and the RREF of M
contains 3 pivots. We say the rank of the matrix is 3, which means
the row space of M is 3-dimensional. A basis for the row space of
M contains 3 vectors, each vector having 10 components. The null
space of the matrix is 7-dimensional (10 ‚àí3 = 7) and consist of
vectors with 10 components. The column space of the matrix is also
be 3-dimensional (R(M) = C(M)). A basis for the column space of
M consists of 3 vectors with 5 components. The left null space of
M is 2-dimensional (5 ‚àí3 = 2) and is spanned by vectors with 5
components.

5.5
VECTOR SPACE TECHNIQUES
219
Importance of bases
The procedures for identifying bases are somewhat technical and po-
tentially boring, but they are of great practical importance. To illus-
trate the importance of a basis, consider a scenario in which you're
given a description of the xy-plane Pxy as the span of three vectors:
Pxy = span{(1, 0, 0), (0, 1, 0), (1, 1, 0)}.
The above deÔ¨Ånition of Pxy says that any point p ‚ààPxy can be written
as a linear combination of the form
p = a(1, 0, 0) + b(0, 1, 0) + c(1, 1, 0),
for some coeÔ¨Écients (a, b, c). This representation of Pxy is misleading.
It might make us think (erroneously) that Pxy is three-dimensional,
since we need three coeÔ¨Écients (a, b, c) to describe points in Pxy.
Do we really need three coeÔ¨Écients to describe any p ‚ààPxy? No,
we don't. Two vectors are suÔ¨Écient: (1, 0, 0) and (0, 1, 0), for example.
The same point p described above can be written as:
p = (a + c)
| {z }
Œ±
(1, 0, 0) + (b + c)
| {z }
Œ≤
(0, 1, 0) = Œ±(1, 0, 0) + Œ≤(0, 1, 0).
Note the point is described in terms of two coeÔ¨Écients (Œ±, Œ≤). The
vector (1, 1, 0) is not necessary for the description of points in Pxy.
The vector (1, 1, 0) is redundant because it can be expressed in terms
of the vectors (1, 0, 0) and (0, 1, 0). By getting rid of the redundant
vector, we obtain a description of Pxy in terms of a basis:
Pxy = span{(1, 0, 0), (0, 1, 0)}.
Recall that the requirement for a basis B for a space V is that it be
made of linearly independent vectors and that it span the space V .
The vectors {(1, 0, 0), (0, 1, 0)} are suÔ¨Écient to represent any vector
in Pxy, and these vectors are linearly independent. We can correctly
conclude that the space Pxy is two-dimensional. If someone asks you
"how do you know that Pxy is two-dimensional?," say "because its
basis contains two vectors."
Exercises
E5.6 Consider the following matrix:
A =
Ô£Æ
Ô£∞
1
3
3
3
2
6
7
6
3
9
9
10
Ô£π
Ô£ª.
Find the RREF of A and bases for R(A), C(A), and N(A).

220
GEOMETRICAL ASPECTS OF LINEAR ALGEBRA
E5.7 Find the null space N(A) of the following matrix A:
A =
Ô£Æ
Ô£∞
1
0
‚àí1
0
4
0
0
‚àí2
0
2
‚àí2
‚àí1
Ô£π
Ô£ª.
5.6
Geometrical problems
We saw computations
now geometry prove you can imagine
P5.1
Find intersection of two lines: a) ‚Ñì1: 2x+y = 4 and ‚Ñì2: 3x‚àí2y = ‚àí1;
b) ‚Ñì1: y + x = 2 and ‚Ñì2: 2x + 2y = 4; c) ‚Ñì1: y + x = 2 and ‚Ñì2: y ‚àíx = 0.
P5.2
Find an intersection line between two planes: a) P1: 3x‚àí2y ‚àíz = 2
and P2: x + 2y + z = 0; d) P3: 2x + y ‚àíz = 0 and P4: x + 2y + x = 3.
P5.3
Find if the planes are parallel, perpendicular or neither: a) P1:
x ‚àíy ‚àíz = 0 and P2: 2z ‚àí2y ‚àí2z = 4; b) P3: 3x + 2y = 1 and P4:
y ‚àíz = 0; c) P5: x ‚àí2y + z = 5 and P6: x + y + z = 3.
P5.4
Find the distance from the point q = (2, 3, 5) to the plane P :
2x + y ‚àí2z = 0.
P5.5
Find distance between points: a) p = (4, 5, 3) and q = (1, 1, 1);
b) m = (4, ‚àí2, 0) and n = (0, 1, 0); c) r = (1, 0, 1) and s = (‚àí1, 1, ‚àí1);
d) p = (2, 1, 2) and j = (1, ‚àí2, ‚àí1).
P5.6
Find the general equation of the plane that passes through the
points q = (1, 3, 0), r = (0, 2, 1), and s = (1, 1, 1).
P5.7
Find the symmetric equation of the line ‚Ñìdescribed by the equation
x = 2t ‚àí3,
y = ‚àí4t + 1,
z = ‚àít.
P5.8
Given two vectors ‚Éóu = (2, 1, ‚àí1) and ‚Éóv = (1, 1, 1), Ô¨Ånd the projection
of ‚Éóv onto ‚Éóu, and then the projection of ‚Éóu onto ‚Éóv.
P5.9
Find a projection of ‚Éóv = (3, 4, 1) onto the plane P : 2x ‚àíy + 4z = 4.
P5.10
Find the component of the vector ‚Éóu = (‚àí2, 1, 1) that is perpendic-
ular to the plane P formed by the points m = (2, 4, 1), s = (1, ‚àí2, 4), and
r = (0, 4, 0).
P5.11
Find the distance between the line ‚Ñì: {x = 1+2t, y = ‚àí3+t, z = 2},
and the plane P : ‚àíx + 2y + 2z = 4.
P5.12
An m √ó n matrix A is upper triangular if all entries lying below
the main diagonal are zero, that is, if Aij = 0 whenever i > j. Prove that
upper triangular matrices form a subspace of Rm√ón.
P5.13
Prove that diagonal matrices are symmetric matrices.

5.6
GEOMETRICAL PROBLEMS
221
P5.14
Let ‚Éóu and ‚Éóv be distinct vectors from the vector space V . Show
that if {‚Éóu,‚Éóv} is a basis for V and a and b are nonzero scalars, then both
{‚Éóu + ‚Éóv, a‚Éóu} and {a‚Éóu, b‚Éóv} are also bases for V .
P5.15
Suppose that S = {‚Éóv1,‚Éóv2,‚Éóv3} is linearly independent and ‚Éów1 =
‚Éóv1 + ‚Éóv2 + ‚Éóv3, ‚Éów2 = ‚Éóv2 + ‚Éóv3 and ‚Éów3 = ‚Éóv3. Show that T = {‚Éów1, ‚Éów2, ‚Éów3} is
linearly independent.
P5.16
Compute the product matrix of the following thee matrices:
A =
cos œÜ
‚àísin œÜ
sin œÜ
cos œÜ
‚àö
x2 + 1 ‚àíx
0
0
‚àö
x2 + 1 + x
cos Œ∏
‚àísin Œ∏
sin Œ∏
cos Œ∏

,
where œÜ = ‚àíœÄ
4 ‚àí1
2 arctan x, and Œ∏ = œÄ
4 ‚àí1
2 arctan x.


Chapter 6
Linear transformations
Linear transformations are a central idea of linear algebra‚Äîthey form
the cornerstone that connects all the seemingly unrelated concepts
we've studied so far. We previously introduced linear transformations,
informally describing them as "vector functions." In this chapter, we'll
formally deÔ¨Åne linear transformations, describe their properties, and
discuss their applications.
In Section 6.2, we'll learn how matrices can be used to repre-
sent linear transformations. We'll show the matrix representations
of important types of linear transformations like projections, reÔ¨Çec-
tions, and rotations. Section 6.3 discusses the relation between bases
and matrix representations. We'll learn how the bases chosen for the
input and output spaces determine the coeÔ¨Écients of matrix repre-
sentations. The same linear transformation corresponds to diÔ¨Äerent
matrix representations, depending on the choice of bases for the input
and output spaces. Section 6.4 discusses and characterizes the class
of invertible linear transformations. This section will serve to connect
several topics we covered previously: linear transformations, matrix
representations, and the fundamental vector spaces of matrices.
6.1
Linear transformations
Linear transformations take vectors as inputs and produce vectors
as outputs.
A transformation T that takes n-dimensional vectors
as inputs and produces m-dimensional vectors as outputs is denoted
T : Rn ‚ÜíRm.
The class of linear transformations includes most of the useful
transformations of analytical geometry: stretchings, projections, re-
Ô¨Çections, rotations, and combinations of these. Since linear transfor-
mations describe and model many real-world phenomena in physics,
223

224
LINEAR TRANSFORMATIONS
chemistry, biology, and computer science, it's worthwhile to learn the
theory behind them.
Concepts
Linear transformations are mappings between vector inputs and vector
outputs. The following concepts describe the input and output spaces:
‚Ä¢ V : the input vector space of T
‚Ä¢ W: the output space of T
‚Ä¢ dim(U): the dimension of the vector space U
‚Ä¢ T : V ‚ÜíW: a linear transformation that takes vectors ‚Éóv ‚ààV as
inputs and produces vectors ‚Éów ‚ààW as outputs. The notation
T(‚Éóv) = ‚Éów describes T acting on ‚Éóv to produce the output ‚Éów.
Figure 6.1: An illustration of the linear transformation T : V ‚ÜíW.
‚Ä¢ Im(T): the image space of the linear transformation T is the
set of vectors that T can output for some input ‚Éóv ‚ààV . The
mathematical deÔ¨Ånition of the image space is
Im(T) ‚â°{‚Éów ‚ààW | ‚Éów = T(‚Éóv), for some ‚Éóv ‚ààV }
‚äÜ
W.
The image space is the vector equivalent of the image set of a
single-variable function Im(f) ‚â°{y ‚ààR | y = f(x), ‚àÄx ‚ààR}.
‚Ä¢ Ker(T): the kernel of the linear transformation T; the set of
vectors mapped to the zero vector by T.
The mathematical
deÔ¨Ånition of the kernel is
Ker(T) ‚â°{‚Éóv ‚ààV | T(‚Éóv) = ‚Éó0}
‚äÜ
V.
The kernel of a linear transformation is the vector equivalent of
the roots of a function: {x ‚ààR | f(x) = 0}.

6.1
LINEAR TRANSFORMATIONS
225
Figure 6.2: Two key properties of a linear transformation T : V ‚ÜíW; its
kernel Ker(T) ‚äÜV , and its image space Im(T) ‚äÜW.
Example
The linear transformation T : R2 ‚ÜíR3 is deÔ¨Åned by the
equation T(x, y) = (x, y, x+y). Applying T to the input vector (1, 0)
produces the output vector (1, 0, 1 + 0) = (1, 0, 1). Applying T to the
input vector (3, 4) produces the output vector (3, 4, 7).
The kernel of T contains only the zero vector Ker(T) = {‚Éó0}. The
image space of T is a two-dimensional subspace of the output space
R3, namely Im(T) = span{(1, 0, 1), (0, 1, 1)} ‚äÜR3.
Matrix representations
Given bases for the input and output spaces of a linear transformation
T, the transformation's action on vectors can be represented as a
matrix-vector product:
‚Ä¢ BV = {‚Éóe1,‚Éóe2, . . . ,‚Éóen}: a basis for the input vector space V
‚Ä¢ BW = {‚Éób1,‚Éób2, . . . ,‚Éóbm}: a basis for the output vector space W
‚Ä¢ MT ‚ààRm√ón: a matrix representation of the linear transforma-
tion T:
‚Éów = T(‚Éóv)
‚áî
‚Éów = MT‚Éóv.
To be precise, we denote the matrix representation as
[MT ]
BW
BV
to show it depends on the input and output bases.
‚Ä¢ C(MT ): the column space of a matrix MT
‚Ä¢ N(MT ): the null space a matrix MT
Properties of linear transformations
We'll start with the feature of linear transformations that makes them
suitable for modelling a wide range of phenomena in science, engineer-
ing, business, and computing.

226
LINEAR TRANSFORMATIONS
Linearity
The fundamental property of linear transformations is‚Äîyou guessed
it‚Äîtheir linearity. If ‚Éóv1 and ‚Éóv2 are two input vectors and Œ± and Œ≤ are
two constants, then
T(Œ±‚Éóv1 + Œ≤‚Éóv2) = Œ±T(‚Éóv1) + Œ≤T(‚Éóv2) = Œ±‚Éów1 + Œ≤ ‚Éów2,
where ‚Éów1 = T(‚Éóv1) and ‚Éów2 = T(‚Éóv2).
Figure 6.3: A linear transformation T maps the linear combination of
inputs 1
4‚Éóv1 + 3
4‚Éóv2 to the linear combination of outputs 1
4 ‚Éów1 + 3
4 ‚Éów2.
Linear transformations map a linear combination of inputs to the
same linear combination of outputs. If you know the outputs of T
for the inputs ‚Éóv1 and ‚Éóv2, you can deduce the output T for any linear
combination of the vectors ‚Éóv1 and ‚Éóv2 by computing the appropriate
linear combination of the outputs T(‚Éóv1) and T(‚Éóv2). This is perhaps
the most important idea in linear algebra. This is the linear that we're
referring to when we talk about linear algebra. Linear algebra is not
about lines, but about mathematical transformations that map linear
combination of inputs to the same linear combination of outputs.
In this chapter we'll study various aspects and properties of lin-
ear transformations, these abstract objects that map input vectors to
output vectors. The fact that linear transformations map linear com-
binations of inputs to corresponding linear combinations of its outputs
will be of central importance in many calculations and proofs. Make
a good note and store a mental image of the example shown in Fig-
ure 6.3.
Linear transformations as black boxes
Suppose someone gives you a black box that implements the linear
transformation T. While you can't look inside the box to see how

6.1
LINEAR TRANSFORMATIONS
227
T acts, you can probe the transformation by choosing various input
vectors and observing what comes out.
Assume the linear transformation T is of the form T : Rn ‚ÜíRm.
By probing this transformation with n vectors of a basis for the input
space and observing the outputs, you can characterize the transfor-
mation T completely.
To see why this is true, consider a basis {‚Éóe1,‚Éóe2, . . . ,‚Éóen} for the
n-dimensional input space V = Rn. To characterize T, input each of
the n basis vectors ‚Éóei into the black box and record the T(‚Éóei) that
comes out.
Any input vector ‚Éóv can be written as a linear combination of the
basis vectors:
‚Éóv = v1‚Éóe1 + v2‚Éóe2 + ¬∑ ¬∑ ¬∑ + vn‚Éóen.
Using these observations and the linearity of T, we can predict the
output of T for this vector:
T(‚Éóv) = v1T(‚Éóe1) + v2T(‚Éóe2) + ¬∑ ¬∑ ¬∑ + vnT(‚Éóen).
This black box model is used in many areas of science and is one of
the most important ideas in linear algebra. The transformation T
could be the description of a chemical process, an electrical circuit,
or some phenomenon in biology. As long as we know that T is (or
can be approximated by) a linear transformation, we can describe it
completely by "probing" it with a small number of inputs. This is
in contrast to characterizing non-linear transformations, which corre-
spond to arbitrarily complex input-output relationships and require
signiÔ¨Åcantly more probing.
Input and output spaces
Consider the linear transformation T from n-vectors to m-vectors:
T : Rn ‚ÜíRm.
The domain of the function T is Rn and its codomain is Rm.
The image space Im(T) consists of all possible outputs of the
transformation T. The image space is a subset of the output space,
Im(T) ‚äÜRm. A linear transformation T whose image space is equal
to its codomain (Im(T) = Rm) is called surjective or onto. Recall
that a function is surjective if it covers the entire output set.
The kernel of T is the subspace of the domain Rn that is mapped
to the zero vector by T: Ker(T) ‚â°{‚Éóv ‚ààRn | T(‚Éóv) = ‚Éó0}. A linear
transformation with an empty kernel Ker(T) = {‚Éó0} is called injective.
Injective transformations map diÔ¨Äerent inputs to diÔ¨Äerent outputs.

228
LINEAR TRANSFORMATIONS
If a linear transformation T is both injective and surjective it is
called bijective. In this case, T is a one-to-one correspondence between
the input vector space and the output vector space.
Note the terminology used to characterize linear transformations
(injective, surjective, and bijective) is the same as the terminology
used to characterize functions in Section 1.8. Indeed, linear trans-
formations are simply vector functions, so we can use the same ter-
minology. The concepts of image space and kernel are illustrated in
Figure 6.4.
Figure 6.4: Pictorial representations of the image space Im(T) and the
kernel Ker(T) of a linear transformation T : Rn ‚ÜíRm. The image space
is the set of all possible outputs of T. The kernel of T is the set of inputs
that T maps to the zero vector.
Observation
The dimensions of the input space and the output
space of a bijective linear transformation must be the same. Indeed, if
T : Rn ‚ÜíRm is bijective then it is both injective and surjective. Since
T is surjective, the input space must be at least as large as the output
space n ‚â•m. Since T is injective, the output space must be larger or
equal to the input space m ‚â•n. Combining these observations, we
Ô¨Ånd that if T : Rn ‚ÜíRm is bijective then m = n.
Example 2
Consider the linear transformation T : R3 ‚ÜíR2 deÔ¨Åned
by the equation T(x, y, z) = (x, z). Find the kernel and the image
space of T. Is T injective? Is T surjective?
The action of T is to delete the y-components of inputs.
Any
vector that has only a y-component will be sent to the zero vector.
We have Ker(T) = span{(0, 1, 0)}. The image space is Im(T) = R2.
The transformation T is not injective. As an explicit example proving
T is not injective, observe that T(0, 1, 0) = T(0, 2, 0) but (0, 1, 0) Ã∏=
(0, 2, 0). Since Im(T) is equal to the codomain R2, T is surjective.
Linear transformations as matrix multiplications
An important relationship exists between linear transformations and
matrices. If you Ô¨Åx a basis for the input vector space and a basis

6.1
LINEAR TRANSFORMATIONS
229
for the output vector space, a linear transformation T(‚Éóv) = ‚Éów can be
represented as matrix multiplication MT‚Éóv = ‚Éów for some matrix MT :
‚Éów = T(‚Éóv)
‚áî
‚Éów = MT‚Éóv.
Using this equivalence, we can re-interpret several properties of matri-
ces as properties of linear transformations. The equivalence is useful
in the other direction too, since it allows us to use the language of
linear transformations to talk about the properties of matrices.
The idea of representing the action of a linear transformation as a
matrix product is extremely important since it transforms the abstract
description of the transformation T into the concrete one: "take the
input vector ‚Éóv and multiply it from the right by the matrix MT ."
Example 3
We'll now illustrate the "linear transformation ‚áîmatrix-
product" equivalence with an example. DeÔ¨Åne Œ†Pxy to be the orthog-
onal projection onto the xy-plane Pxy. In words, the action of this
projection is to zero-out the z-component of input vectors. The ma-
trix that corresponds to this projection is
T
Ô£´
Ô£≠
Ô£Æ
Ô£∞
vx
vy
vz
Ô£π
Ô£ª
Ô£∂
Ô£∏=
Ô£Æ
Ô£∞
vx
vy
0
Ô£π
Ô£ª
‚áî
MT‚Éóv =
Ô£Æ
Ô£∞
1
0
0
0
1
0
0
0
0
Ô£π
Ô£ª
Ô£Æ
Ô£∞
vx
vy
vz
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
vx
vy
0
Ô£π
Ô£ª.
Finding the matrix
In order to Ô¨Ånd the matrix representation of any linear transformation
T : Rn ‚ÜíRm, it is suÔ¨Écient to probe T with the n vectors in the
standard basis for Rn:
ÀÜe1 ‚â°
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
0
...
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
ÀÜe2 ‚â°
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
0
1
...
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
. . . ,
ÀÜen ‚â°
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
0
...
0
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
To obtain MT , we combine the outputs T(ÀÜe1), T(ÀÜe2), . . ., T(ÀÜen) as
the columns of a matrix:
MT =
Ô£Æ
Ô£ØÔ£ØÔ£∞
|
|
|
T(ÀÜe1)
T(ÀÜe2)
. . .
T(ÀÜen)
|
|
|
Ô£π
Ô£∫Ô£∫Ô£ª.
Observe that the matrix constructed in this way has the right dimen-
sions: m √ó n. We have MT ‚ààRm√ón since we used n "probe vectors,"
and since the outputs of T are m-dimensional column vectors.

230
LINEAR TRANSFORMATIONS
To help visualize this new "column" idea, let's analyze what hap-
pens when we compute the product MT ÀÜe2. The probe vector ÀÜe2 ‚â°
(0, 1, 0, . . . , 0)T will "select" only the second column from MT , thus
we'll obtain the correct output: MT ÀÜe2 = T(ÀÜe2). Similarly, applying
MT to other basis vectors selects the other columns of MT .
Any input vector can be written as a linear combination of the
standard basis vectors ‚Éóv = v1ÀÜe1 + v2ÀÜe2 + ¬∑ ¬∑ ¬∑ + vnÀÜen. Therefore, by
linearity, we can compute the output T(‚Éóv) as follows:
T(‚Éóv) = v1T(ÀÜe1) + v2T(ÀÜe2) + ¬∑ ¬∑ ¬∑ + vnT(ÀÜen)
= v1
Ô£Æ
Ô£ØÔ£ØÔ£∞
|
T(ÀÜe1)
|
Ô£π
Ô£∫Ô£∫Ô£ª+ v2
Ô£Æ
Ô£ØÔ£ØÔ£∞
|
T(ÀÜe2)
|
Ô£π
Ô£∫Ô£∫Ô£ª+ ¬∑ ¬∑ ¬∑ + vn
Ô£Æ
Ô£ØÔ£ØÔ£∞
|
T(ÀÜen)
|
Ô£π
Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£∞
|
|
|
T(ÀÜe1)
T(ÀÜe2)
. . .
T(ÀÜen)
|
|
|
Ô£π
Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£∞
|
‚Éóv
|
Ô£π
Ô£∫Ô£∫Ô£ª
= MT [‚Éóv],
where [‚Éóv] = (v1, v2, . . . , vn)T is the coeÔ¨Écients vector of ‚Éóv, represented
as a column vector.
Input and output spaces
We can identify correspondences between the properties of a linear
transformation T and the properties of a matrix MT that imple-
ments T.
The outputs of the linear transformation T consist of all possi-
ble linear combinations of the columns of the matrix MT . Thus, we
can identify the image space of the linear transformation T with the
column space of the matrix MT :
Im(T) = {‚Éów ‚ààW | ‚Éów = T(‚Éóv), for some ‚Éóv ‚ààV } = C(MT ).
There is also an equivalence between the kernel of the linear transfor-
mation T and the null space of the matrix MT :
Ker(T) ‚â°{‚Éóv ‚ààRn|T(‚Éóv) = ‚Éó0} = {‚Éóv ‚ààRn|MT‚Éóv = ‚Éó0} ‚â°N(MT ).
The null space of a matrix N(MT ) consists of all vectors that are
orthogonal to the rows of the matrix MT . The vectors in the null
space of MT have a zero dot product with each of the rows of MT .
This orthogonality can also be phrased in the opposite direction. Any
vector in the row space R(MT ) of the matrix is orthogonal to the null
space N(MT ) of the matrix.

6.1
LINEAR TRANSFORMATIONS
231
These observations allow us to decompose the domain of the trans-
formation T as the orthogonal sum of the row space and the null space
of the matrix MT :
Rn = R(MT ) ‚äïN(MT ).
This split implies the conservation of dimensions formula:
dim(Rn) = n = dim(R(MT )) + dim(N(MT )).
The sum of the dimensions of the row space and the null space of a
matrix MT is equal to the total dimensions of the input space.
We can summarize everything we know about the input-output
relationship of the linear transformation T as follows:
T : R(MT ) ‚ÜíC(MT ),
T : N(MT ) ‚Üí{‚Éó0}.
Input vectors ‚Éóv ‚ààR(MT ) are mapped to output vectors ‚Éów ‚ààC(MT ) in
a one-to-one correspondence. Input vectors ‚Éóv ‚ààN(MT ) are mapped
to the zero vector ‚Éó0 ‚ààRm.
Composition of linear transformations
The consecutive application of two linear transformations T and S on
an input vector ‚Éóv corresponds to the following matrix product:
S ‚ó¶T(‚Éóv) = S(T(‚Éóv)) = MSMT‚Éóv.
The matrix MT "touches" the vector Ô¨Årst, followed by the matrix MS.
For this composition to be well-deÔ¨Åned, the dimension of the out-
put space of T must be the same as the dimension of the input space
of S. In terms of the matrices, this requirement corresponds to the
condition that the inner dimension in the matrix product MSMT
must be the same.
Importance of the choice of bases
Above, we assumed the standard basis was used both for inputs and
outputs of the linear transformation. Thus, we obtained the coeÔ¨É-
cients in the matrix MT with respect to the standard basis.
In particular, we assumed that the outputs of T were given as
column vectors in terms of the standard basis for Rm. If the outputs
were given in some other basis BW , the coeÔ¨Écients of the matrix MT
would have been with respect to BW .
Due to the dependence of matrix coeÔ¨Écients on the basis used,
it's wrong to say that a linear transformation is a matrix.
Indeed, the same linear transformation T will correspond to diÔ¨Äerent

232
LINEAR TRANSFORMATIONS
matrices if diÔ¨Äerent bases are used. We say the linear transformation
T corresponds to a matrix M for a given choice of input and output
bases. We write BW [MT ]BV to show the coeÔ¨Écients of the matrix MT
depend on the choice of left and right bases. Recall we can also use
the basis-in-a-subscript notation for vectors.
For example, writing
(vx, vy, vz)Bs shows the coeÔ¨Écients vx, vy, and vz are expressed in
terms of the standard basis Bs ‚â°{ÀÜƒ±, ÀÜÔöæ, ÀÜk}.
The choice of basis is an important technical detail: be aware of it,
but don't worry about it too much. Unless otherwise speciÔ¨Åed, assume
the standard basis is used for specifying vectors and matrices. When
you see the product A‚Éóv, it means
[A]
Bs
Bs [‚Éóv]Bs. The only time you
really need to pay attention to the choice of bases is when performing
change-of-basis transformations, which we'll discuss in Section 6.3.
Invertible transformations
We'll now revisit the properties of invertible matrices and connect
them to the notion of invertible transformations. Think of multipli-
cation by a matrix M as "doing" something to vectors, and multi-
plication by M ‚àí1 as doing the opposite thing, restoring the original
vector:
M ‚àí1M‚Éóv = ‚Éóv.
For example, the matrix
M =
2
0
0
1

corresponds to a stretching of space by a factor of 2 in the x-direction,
while the y-direction remains unchanged. The inverse transformation
corresponds to a shrinkage by a factor of 2 in the x-direction:
M ‚àí1 =
"
1
2
0
0
1
#
.
In general, it's hard to see exactly what the matrix M does, since it
performs some arbitrary linear combination of the coeÔ¨Écients of the
input vector.
If M is an invertible matrix, we can start from any output vector
‚Éów = M‚Éóv, and go back to Ô¨Ånd the input ‚Éóv that produced the output ‚Éów.
We do this by multiplying ‚Éów by the inverse: M ‚àí1 ‚Éów = M ‚àí1M‚Éóv = ‚Éóv.
A linear transformation T is invertible if there exist an inverse
transformation T ‚àí1 such that T ‚àí1(T(‚Éóv)) = ‚Éóv. By the correspondence
‚Éów = T(‚Éóv) ‚áî‚Éów = MT‚Éóv, we can identify the class of invertible
linear transformations with the class of invertible matrices.

6.1
LINEAR TRANSFORMATIONS
233
AÔ¨Éne transformations
An aÔ¨Éne transformation is a function A : Rn ‚ÜíRm that is the
combination of a linear transformation T followed by a translation by
a Ô¨Åxed vector ‚Éób:
‚Éóy = A(‚Éóx) = T(‚Éóx) +‚Éób.
By the T ‚áîMT equivalence we can write the formula for an aÔ¨Éne
transformation as
‚Éóy = A(‚Éóx) = MT‚Éóx +‚Éób.
To obtain the output, ‚Éóy, apply the linear transformation T (the
matrix-vector product MT‚Éóx), then add the vector‚Éób. This is the vector
generalization of a single-variable aÔ¨Éne function y = f(x) = mx + b.
Discussion
The most general linear transformation
In this section we learned that a linear transformation can be repre-
sented as matrix multiplication. Are there other ways to represent lin-
ear transformations? To study this question, let's analyze, from Ô¨Årst
principles, the most general form a linear transformation T : V ‚ÜíW
can take. We'll use V = R3 and W = R2 to keep things simple.
First consider the coeÔ¨Écient w1 of the output vector ‚Éów = T(‚Éóv)
when the input vector is ‚Éóv ‚ààR3. The fact that T is linear, means
that w1 can be an arbitrary mixture of the input vector coeÔ¨Écients
v1, v2, v3:
w1 = Œ±1v1 + Œ±2v2 + Œ±3v3.
Similarly, the coeÔ¨Écient w2 must be some arbitrary linear combina-
tion of the input coeÔ¨Écients w2 = Œ≤1v1 +Œ≤2v2 +Œ≤3v3. Thus, the most
general linear transformation T : R3 ‚ÜíR2 can be written as
w1 = Œ±1v1 + Œ±2v2 + Œ±3v3,
w2 = Œ≤1v1 + Œ≤2v2 + Œ≤3v3.
This is precisely the kind of expression that can be obtained as a
matrix product:
T(‚Éóv) =
w1
w2

=
Œ±1
Œ±2
Œ±3
Œ≤1
Œ≤2
Œ≤3
Ô£Æ
Ô£∞
v1
v2
v3
Ô£π
Ô£ª= MT‚Éóv.
Indeed, the matrix product is deÔ¨Åned as rows-dot-columns because it
allows us to easily describe linear transformations.

234
LINEAR TRANSFORMATIONS
Links
[ Examples of linear transformations from Wikibooks ]
http://wikibooks.org/wiki/Linear_Algebra/Linear_Transformations
Exercises
E6.1 Consider the transformation T(x, y, z) = (y + z, x + z, x + y).
Find the domain, codomain, kernel, and image space of T.
Is T
injective, surjective, or bijective?
6.2
Finding matrix representations
Every linear transformation T : Rn ‚ÜíRm can be represented as a
matrix MT ‚ààRm√ón. Suppose you're given the following description
of a linear transformation: "T is the counterclockwise rotation of all
points in the xy-plane by 30‚ó¶," and you want to Ô¨Ånd the matrix MT
that corresponds to this transformation.
Do you know how to Ô¨Ånd the matrix representation of T? This sec-
tion describes a simple and intuitive "probing procedure" for Ô¨Ånding
matrix representations. Don't worry; no alien technology is involved,
and we won't be probing any humans‚Äîonly linear transformations!
As you read, try to bridge your understanding between the general
speciÔ¨Åcation of a transformation T(‚Éóv) and its speciÔ¨Åc implementation
as a matrix-vector product MT‚Éóv. We'll use the "probing procedure"
to study various linear transformations and derive their matrix rep-
resentations.
Once we Ô¨Ånd the matrix representation of a given transformation,
we can eÔ¨Éciently apply that transformation to many vectors. This is
exactly how computers carry out linear transformations. For exam-
ple, a black-and-white image Ô¨Åle can be represented as a long list that
contains the coordinates of the image's black pixels: {‚Éóx1, ‚Éóx2, . . . , ‚Éóx‚Ñì}.
The image is obtained by starting with a white background and draw-
ing a black pixel in each of the locations ‚Éóxi on the screen.1 To rotate
the image, we can process the list of pixels using the matrix-vector
product ‚Éóyi = MT‚Éóxi, where MT is the matrix representation of the
desired rotation. The transformed list of pixels {‚Éóy1, ‚Éóy2, . . . , ‚Éóy‚Ñì} corre-
sponds to a rotated version of the image. This is essentially the eÔ¨Äect
of using the "rotate tool" in an image editing program‚Äîthe computer
multiplies the image by a rotation matrix.
1Location on a computer screen is denoted using pixel coordinates ‚Éóxi = (hi, vi).
The number hi describes a horizontal distance measured in pixels from the left
edge of the image, and vi measures the vertical distance from the top of the image.

6.2
FINDING MATRIX REPRESENTATIONS
235
Concepts
The previous section covered linear transformations and their matrix
representations:
‚Ä¢ T : Rn ‚ÜíRm: a linear transformation that takes inputs in Rn
and produces outputs in Rm
‚Ä¢ MT ‚ààRm√ón: the matrix representation of T
The action of the linear transformation T is equivalent to a multipli-
cation by the matrix MT :
‚Éów = T(‚Éóv)
‚áî
‚Éów = MT‚Éóv.
Theory
To Ô¨Ånd the matrix representation of the transformation T : Rn ‚ÜíRm,
it is suÔ¨Écient to "probe" T with the n vectors of the standard basis
for the input space Rn:
ÀÜe1 ‚â°
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
0
...
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
ÀÜe2 ‚â°
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
0
1
...
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
. . . ,
ÀÜen ‚â°
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
0
...
0
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
The matrix MT that corresponds to the action of T on the standard
basis is
MT =
Ô£Æ
Ô£ØÔ£ØÔ£∞
|
|
|
T(‚Éóe1)
T(‚Éóe2)
. . .
T(‚Éóen)
|
|
|
Ô£π
Ô£∫Ô£∫Ô£ª.
This is an m √ó n matrix whose columns are the outputs of T for the
n probe inputs.
The remainder of this section illustrates the "probing procedure"
for Ô¨Ånding matrix representations of linear transformations through
several examples.
Projections
We'll start with a class of linear transformations you're already famil-
iar with: projections. I hope you still remember what you learned in
Section 5.2 (page 189).
X projection

236
LINEAR TRANSFORMATIONS
The projection onto the x-axis is de-
noted Œ†x. The projection Œ†x acts on
any vector or point by leaving the x-
coordinate unchanged and setting the
y-coordinate to zero.
Let's analyze how the projection
Œ†x transforms the two vectors of the
standard basis:
Œ†x
1
0

=
1
0

,
Œ†x
0
1

=
0
0

.
The matrix representation of Œ†x is
therefore given by:
MŒ†x =

Œ†x

1
0

Œ†x

0
1

=

1
0
0
0

.
Y projection
Similar to Œ†x, Œ†y is deÔ¨Åned as the
projection onto the y-axis. Can you
guess what the matrix for the projec-
tion onto the y-axis will look like?
Use the standard approach to Ô¨Ånd
the matrix representation of Œ†y:
MŒ†y =

Œ†y

1
0

Œ†y

0
1

=

0
0
0
1

.
We can verify that the matrices MŒ†x
and MŒ†y do indeed select the appro-
priate coordinate from a general in-
put vector ‚Éóv = (vx, vy)T:
1
0
0
0
vx
vy

=
vx
0

,
0
0
0
1
vx
vy

=
 0
vy

.
Projection onto a vector
Recall that the general formula for the projection of a vector ‚Éóv onto
another vector ‚Éóa is obtained as:
Œ†‚Éóa(‚Éóv) =
 ‚Éóa ¬∑ ‚Éóv
‚à•‚Éóa‚à•2

‚Éóa.
To Ô¨Ånd the matrix representation of a projection onto an arbitrary
direction ‚Éóa, we compute
MŒ†‚Éóa =

Œ†‚Éóa

1
0

Œ†‚Éóa

0
1

.

6.2
FINDING MATRIX REPRESENTATIONS
237
Projection onto a plane
We can also compute the projection of the vector ‚Éóv ‚ààR3 onto the
plane P : ‚Éón ¬∑ ‚Éóx = nxx + nyy + nzz = 0 as follows:
Œ†P (‚Éóv) = ‚Éóv ‚àíŒ†‚Éón(‚Éóv).
How should we interpret the above formula? First compute the part
of the vector ‚Éóv that is perpendicular to the plane (in the ‚Éón direction),
then subtract this part from ‚Éóv to obtain the part that lies in the plane.
To obtain the matrix representation of Œ†P , calculate what it does
to the standard basis ÀÜƒ± ‚â°ÀÜe1 ‚â°(1, 0, 0)T, ÀÜÔöæ‚â°ÀÜe2 ‚â°(0, 1, 0)T, and
ÀÜk ‚â°ÀÜe3 ‚â°(0, 0, 1)T.
Projections as outer products
We can obtain the projection matrix onto any unit vector by comput-
ing the outer product of the vector with itself. As an example, we'll
Ô¨Ånd the matrix for the projection onto the x-axis Œ†x(‚Éóv) = (‚Éóv ¬∑ ÀÜƒ±)ÀÜƒ±.
Recall the inner product (dot product) between two column vectors
‚Éóu and ‚Éóv is equivalent to the matrix product ‚ÉóuT‚Éóv, while their outer
product is given by the matrix product ‚Éóu‚ÉóvT. The inner product is a
product between a 1 √ó n matrix and a n √ó 1 matrix, whose result is a
1 √ó 1 matrix‚Äîa single number. The outer product corresponds to an
n√ó1 matrix times a 1√ón matrix, making the answer an n√ón matrix.
The projection matrix corresponding to Œ†x is MŒ†x = ÀÜƒ±ÀÜƒ±T ‚ààRn√ón.
Where did that equation come from?
To derive the equation,
we x use the commutative law of scalar multiplication Œ±‚Éóv = ‚ÉóvŒ±, y
rewrite the dot product formula as a matrix product, and z use the
associative law of matrix multiplication A(BC) = (AB)C. Check it:
Œ†x(‚Éóv) = (ÀÜƒ± ¬∑ ‚Éóv) ÀÜƒ±
x= ÀÜƒ±(ÀÜƒ± ¬∑ ‚Éóv)
y= ÀÜƒ±(ÀÜƒ±T‚Éóv) =

1
0
1
0
vx
vy

z=
 ÀÜƒ±ÀÜƒ±T
‚Éóv =
1
0
1
0 vx
vy

= (M)‚Éóv =
1
0
0
0
vx
vy

=
vx
0

.
The outer product M ‚â°ÀÜƒ±ÀÜƒ±T corresponds to the projection matrix MŒ†x
we're looking for.
More generally, we obtain the projection matrix onto a line with
direction vector ‚Éóa by constructing the unit vector ÀÜa, and then calcu-
lating the outer product of ÀÜa with itself:
ÀÜa ‚â°
‚Éóa
‚à•‚Éóa‚à•,
MŒ†‚Éóa = ÀÜaÀÜaT.

238
LINEAR TRANSFORMATIONS
Example
Find the projection matrix Md ‚ààR2√ó2 for the projection
Œ†d onto the line with equation y = x (45‚ó¶diagonal line).
The line with equation y = x corresponds to the parametric equa-
tion {(x, y) ‚ààR2 | (x, y) = (0, 0) + t(1, 1), t ‚ààR}, so the direction
vector for this line is ‚Éóa = (1, 1). We want to Ô¨Ånd the matrix that
corresponds to the linear transformation Œ†d(‚Éóv) =

‚Éóv¬∑(1,1)
2

(1, 1)T.
The projection matrix onto ‚Éóa = (1, 1) is computed using the outer
product approach. First, compute a normalized direction vector ÀÜa =
( 1
‚àö
2,
1
‚àö
2); then compute Md using the outer product:
Md = ÀÜaÀÜaT =
" 1
‚àö
2
1
‚àö
2
# h
1
‚àö
2
1
‚àö
2
i
=
"
1
2
1
2
1
2
1
2
#
.
Usually, the idea of representing projections as outer products is not
covered in a Ô¨Årst linear algebra course, so don't worry about outer
products appearing on the exam. The purpose of introducing you to
the equivalence between projections onto ÀÜa and the outer product ÀÜaÀÜaT
is to illuminate this interesting connection between vectors and matri-
ces. This connection plays a fundamental role in quantum mechanics,
where projections in diÔ¨Äerent directions are frequently used.
If you're asked a matrix representation question on an exam, keep
things simple and stick to the "probing with the standard basis" ap-
proach, which gives the same answer as the one computed using the
outer product:
Md =

Œ†d
1
0

Œ†d
0
1

=
h
ÀÜƒ±¬∑‚Éóa
‚à•‚Éóa‚à•2

‚Éóa

ÀÜÔöæ¬∑‚Éóa
‚à•‚Éóa‚à•2

‚Éóa
i
=
"
1
2
1
2
1
2
1
2
#
.
Projections are idempotent
A projection matrix MŒ† satisÔ¨Åes MŒ†MŒ† = MŒ†. This is one of the
deÔ¨Åning properties of projections. The technical term for this is idem-
potence, meaning the operation can be applied multiple times without
changing the result beyond the initial application.
Subspaces
A projection acts diÔ¨Äerently on diÔ¨Äerent sets of input vectors. While
some input vectors remain unchanged, some input vectors are killed.
This is murder! Well, murder in a mathematical sense, which means
multiplication by zero.
Let Œ†S be the projection onto the space S, and S‚ä•be the orthog-
onal space to S deÔ¨Åned by S‚ä•‚â°{ ‚Éów ‚ààRn | ‚Éów ¬∑ ‚Éós = 0, ‚àÄ‚Éós ‚ààS }. The

6.2
FINDING MATRIX REPRESENTATIONS
239
action of Œ†S is completely diÔ¨Äerent for the vectors from S and S‚ä•.
All vectors ‚Éóv in S remain unchanged:
Œ†S(‚Éóv) = ‚Éóv,
whereas vectors ‚Éów in S‚ä•are killed:
Œ†S(‚Éów) = 0‚Éów = ‚Éó0.
The action of Œ†S on any vector from S‚ä•is equivalent to multiplication
by zero. This is why S‚ä•is called the null space of MŒ†S.
ReÔ¨Çections
We'll now compute matrix representations for simple reÔ¨Çection trans-
formations.
X reÔ¨Çection
ReÔ¨Çection through the x-axis leaves the
x-coordinate unchanged and Ô¨Çips the
sign of the y-coordinate. The Ô¨Ågure on
the right illustrates the eÔ¨Äect of reÔ¨Çect-
ing two points through the x-axis.
Using the usual probing procedure,
we obtain the matrix that corresponds
to this linear transformation:
MRx =

Rx
1
0

Rx
0
1

=
1
0
0 ‚àí1

.
This matrix sends (x, y)T to (x, ‚àíy)T as required.
Y reÔ¨Çection
The matrix associated with Ry, the re-
Ô¨Çection through the y-axis, is given by:
MRy =
‚àí1
0
0
1

.
The numbers in the above matrix
tell us to change the sign of the x-
coordinate of the input and leave its
y-coordinate unchanged.
In other
words, every point that starts on the
left of the y-axis moves to the right of

240
LINEAR TRANSFORMATIONS
the y-axis, and every point that starts
on the right of the y-axis moves to the
left.
Do you see the power and simplicity of the probing procedure for
Ô¨Ånding matrix representations? In the Ô¨Årst column, enter what you
want to happen to the ÀÜe1 vector; in the second column, enter what
you want to happen to the ÀÜe2 vector, and voila!
Diagonal reÔ¨Çection
Suppose we want to Ô¨Ånd the formula for the reÔ¨Çection through the
line y = x. We'll call this reÔ¨Çection Rd. This time, dear readers, it's
up to you to draw the diagram. In words, Rd is the transformation
that makes x and y "swap places."
Based on this notion of swapping places, the matrix for Rd is
MRd =
0
1
1
0

.
Alternatively, the usual "probing procedure" will lead us to the same
result.
Now I must point out an important property common to all re-
Ô¨Çections. The eÔ¨Äect of a reÔ¨Çection is described by one of two possi-
ble actions: some points remain unchanged by the reÔ¨Çection, while
other points Ô¨Çip into their exact negatives. For example, the invari-
ant points under Ry are the points that lie on the y-axis‚Äîthat is, the
multiples of (0, 1)T. The points that become their exact negatives are
those with only an x-component‚Äîthe multiples of (1, 0)T. The ac-
tion of Ry on all other points can be obtained as a linear combination
of the actions "leave unchanged" and "multiply by ‚àí1." We'll extent
this line of reasoning further at the end of this section, and again in
Section 7.1.
ReÔ¨Çections through lines and planes
What about Ô¨Ånding reÔ¨Çections through
an arbitrary line?
Consider the line
with parametric equation ‚Ñì: {‚Éó0 +
t‚Éóa, t ‚ààR}. We can Ô¨Ånd a formula for
the reÔ¨Çection through ‚Ñìin terms of the
projection formula we obtained earlier:
R‚Éóa(‚Éóv) = 2Œ†‚Éóa(‚Éóv) ‚àí‚Éóv.

6.2
FINDING MATRIX REPRESENTATIONS
241
Consider how we arrive at this formula. First, compute the projection
of ‚Éóv onto the line Œ†‚Éóa(‚Éóv). Then take two steps in that direction and
subtract ‚Éóv once. Annotate the Ô¨Ågure with a pencil so you can visualize
that the formula works.
Similarly, we can derive an expres-
sion for the reÔ¨Çection through an arbi-
trary plane P : ‚Éón ¬∑ ‚Éóx = 0:
RP (‚Éóv) = 2Œ†P (‚Éóv) ‚àí‚Éóv = ‚Éóv ‚àí2Œ†‚Éón(‚Éóv).
The Ô¨Årst form of the formula uses a
reasoning similar to the formula for
the reÔ¨Çection through a line.
The second form of the formula can be understood as computing
the shortest vector from the plane to ‚Éóv, subtracting that vector once
from ‚Éóv to reach a point in the plane, and subtracting it a second time
to move to the point RP (‚Éóv) on the other side of the plane.
Rotations
We'll now Ô¨Ånd the matrix representations for rotation transforma-
tions. The rotation in the counterclockwise by the angle Œ∏ is denoted
RŒ∏. Figure 6.5 illustrates the action of the rotation RŒ∏: the point A
is rotated around the origin to become the point B.
Figure 6.5: The linear transformation RŒ∏ rotates every point in the plane
by the angle Œ∏ in the counterclockwise direction. Note the eÔ¨Äect of RŒ∏ on
the basis vectors (1, 0) and (0, 1).
To Ô¨Ånd the matrix representation of RŒ∏, probe it with the standard
basis as usual:
MRŒ∏ =

RŒ∏
1
0

RŒ∏
0
1

.
To compute the values in the Ô¨Årst column, observe that RŒ∏ rotates the
vector (1, 0)T = 1‚à†0 to the vector 1‚à†Œ∏ = (cos Œ∏, sin Œ∏)T. The second

242
LINEAR TRANSFORMATIONS
input ÀÜe2 = (0, 1)T = 1‚à†œÄ
2 is rotated to 1‚à†( œÄ
2 + Œ∏) = (‚àísin Œ∏, cos Œ∏)T.
Therefore, the matrix for RŒ∏ is
MRŒ∏ =
Ô£Æ
Ô£∞
|
|
1‚à†Œ∏
1‚à†(œÄ
2+Œ∏)
|
|
Ô£π
Ô£ª=
"
cos Œ∏
‚àísin Œ∏
sin Œ∏
cos Œ∏
#
.
Finding the matrix representation of a linear transformation is like a
colouring-book activity for mathematicians. Filling in the columns is
just like colouring inside the lines‚Äînothing complicated.
Inverses
Can you determine the inverse matrix of MRŒ∏? You could use the
formula for Ô¨Ånding the inverse of a 2 √ó 2 matrix, or you could use
the [ A | I ]-and-RREF algorithm for Ô¨Ånding the inverse; but using
these approaches would be waaaaay too much work. Try to guess the
matrix representation of the inverse without doing any calculations.
If RŒ∏ rotates points by +Œ∏, can you tell me what the inverse operation
does? I'll leave a blank line here to give you some time to think. . . .
Think you have it? The inverse operation of RŒ∏ is R‚àíŒ∏, a rotation
by ‚àíŒ∏, which corresponds to the matrix
MR‚àíŒ∏ =

cos Œ∏
sin Œ∏
‚àísin Œ∏
cos Œ∏

.
Recall that cos is an even function, so cos(‚àíŒ∏) = cos(Œ∏), while sin is
an odd function, so sin(‚àíŒ∏) = ‚àísin(Œ∏).
For any vector ‚Éóv ‚ààR2 we have R‚àíŒ∏ (RŒ∏(‚Éóv)) = ‚Éóv = RŒ∏ (R‚àíŒ∏(‚Éóv)),
or in terms of matrices:
MR‚àíŒ∏MRŒ∏ = 1 = MRŒ∏MR‚àíŒ∏.
Cool, right? This is what representation really means: the abstract
notion of composition of linear transformations is represented as a
matrix product.
Here's another quiz question: what is the inverse operation of the
reÔ¨Çection through the x-axis Rx?
The "undo" action for Rx is to
apply Rx again. We say Rx is a self-inverse operation.
What is the inverse matrix of a projection Œ†S? Good luck Ô¨Ånding
that one‚Äîit's was a trick question. The projection Œ†S sends all input
vectors from the subspace S‚ä•to the zero vector.
Projections are
inherently many-to-one transformations and therefore not invertible.

6.2
FINDING MATRIX REPRESENTATIONS
243
Nonstandard-basis probing
At this point you should feel conÔ¨Ådent facing any linear transforma-
tion T : R2 ‚ÜíR2, and probing it with the standard basis to Ô¨Ånd its
matrix representation MT ‚ààR2√ó2. But what if you're not allowed
to probe T with the standard basis? What if you must Ô¨Ånd the ma-
trix of the transformation given the outputs of T for some other basis
{‚Éóv1 ‚â°(v1x, v1y)T,‚Éóv2 ‚â°(v2x, v2y)T}:
T
v1x
v1y

=
t1x
t1y

,
T
v2x
v2y

=
t2x
t2y

.
Let's test this idea. We're given the information v1x, v1y, t1x, t1y, and
v2x, v2y, t2x, t2y, and must Ô¨Ånd the matrix representation of T with
respect to the standard basis.
Because the vectors ‚Éóv1 and ‚Éóv2 form a basis, we can reconstruct the
information about the matrix MT from the input-output information
given. We're looking for four unknowns‚Äîm11, m12, m21, and m22‚Äî
that form the matrix representation of T:
MT =
m11
m12
m21
m22

.
We can write four equations with the input-output information pro-
vided:
m11v1x + m12v1y = t1x,
m21v1x + m22v1y = t1y,
m11v2x + m12v2y = t2x,
m21v2x + m22v2y = t2y.
Since there are four equations and four unknowns, we can solve for
the coeÔ¨Écients m11, m12, m21, and m22.
This system of equations diÔ¨Äers from ones we've seen before, so
we'll examine it in detail. Think of the entries of MT as a 4√ó1 vector
of unknowns ‚Éóm = (m11, m12, m21, m22)T. We can rewrite the four
equations as a matrix equation:
A‚Éóm = ‚Éót
‚áî
Ô£Æ
Ô£ØÔ£ØÔ£∞
v1x
v1y
0
0
0
0
v1x
v1y
v2x
v2y
0
0
0
0
v2x
v2y
Ô£π
Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£∞
m11
m12
m21
m22
Ô£π
Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£∞
t1x
t1y
t2x
t2y
Ô£π
Ô£∫Ô£∫Ô£ª.
Next, solve for ‚Éóm by computing ‚Éóm = A‚àí1‚Éót.
Finding the matrix representation by probing with a nonstandard
basis is more work than probing with the standard basis, but it's
totally doable.

244
LINEAR TRANSFORMATIONS
Eigenspaces
Probing the transformation T with any basis for the input space gives
suÔ¨Écient information to determine its matrix representation. We're
free to choose the "probing basis," so how do we decide which basis
to use? The standard basis is good for computing the matrix repre-
sentation, but perhaps there's a basis that allows us to simplify the
abstract description of T; a so-called natural basis for probing each
transformation.
Indeed, such a basis exists. Many linear transformations have a
basis {‚ÉóeŒª1,‚ÉóeŒª2, . . .} such that the action of T on the basis vector ‚ÉóeŒªi
is equivalent to the scaling of ‚ÉóeŒªi by the constant Œªi:
T(‚ÉóeŒªi) = Œªi‚ÉóeŒªi.
Recall how projections leave some vectors unchanged (multiply by
Œª = 1) and send other vectors to the zero vector (multiply by Œª = 0).
These subspaces of the input space are speciÔ¨Åc to each transformation,
and are called the eigenspaces (from the German "own spaces") of the
transformation T.
Consider the reÔ¨Çection Rx and its two eigenspaces:
‚Ä¢ The space of vectors that remain unchanged (the eigenspace
corresponding to Œª1 = 1) is spanned by the vector (1, 0):
Rx
1
0

= 1
1
0

.
‚Ä¢ The space of vectors that become the exact negatives of them-
selves (corresponding to Œª2 = ‚àí1) is spanned by (0, 1):
Rx

0
1

= ‚àí1
0
1

.
From a theoretical point of view, describing the action of a liner trans-
formation T in its natural basis is the best way to understand it. For
each of the eigenvectors in the various eigenspaces of T, the action of
T is a simple scalar multiplication. We defer the detailed discussion
on eigenvalues and eigenvectors until the next chapter.
Links
[ Rotation operation as the composition of three shear operations ]
http://datagenetics.com/blog/august32013/index.html

6.3
CHANGE OF BASIS FOR MATRICES
245
Exercises
6.3
Change of basis for matrices
Every linear transformation T : Rn ‚ÜíRm can be represented as a
matrix MT ‚ààRm√ón. The coeÔ¨Écients of the matrix MT depend on
the basis used to describe the input and output spaces. Note, this
dependence of matrix coeÔ¨Écients on the basis is directly analogous to
the dependence of vector coeÔ¨Écients on the basis.
In this section we'll learn how the choice of basis aÔ¨Äects the co-
eÔ¨Écients of matrix representations, and discuss how to carry out the
change-of-basis operation for matrices.
Concepts
You should already be familiar with the concepts of vector spaces,
bases, vector coeÔ¨Écients with respect to diÔ¨Äerent bases, and the change-
of-basis transformation:
‚Ä¢ V : an n-dimensional vector space
‚Ä¢ ‚Éóv: a vector in V
‚Ä¢ B = {ÀÜe1, ÀÜe2, . . . , ÀÜen}: an orthonormal basis for V
‚Ä¢ [‚Éóv]B = (v1, v2, . . . , vn)B: the vector ‚Éóv expressed in the basis B
‚Ä¢ B‚Ä≤ = {ÀÜe‚Ä≤
1, ÀÜe‚Ä≤
2, . . . , ÀÜe‚Ä≤
n}: another orthonormal basis for V
‚Ä¢ [‚Éóv]B‚Ä≤ = (v‚Ä≤
1, v‚Ä≤
2, . . . , v‚Ä≤
n)B‚Ä≤: the vector ‚Éóv expressed in the basis B‚Ä≤
‚Ä¢
[1]
B‚Ä≤
B: the change-of-basis matrix that converts from B coor-
dinates to B‚Ä≤ coordinates: [‚Éóv]B‚Ä≤ =
[1]
B‚Ä≤
B [‚Éóv]B
‚Ä¢
[1]
B
B‚Ä≤: the inverse change-of-basis matrix [‚Éóv]B =
[1]
B
B‚Ä≤ [‚Éóv]B‚Ä≤
(note that
[1]
B
B‚Ä≤ = (
[1]
B‚Ä≤
B)‚àí1)
We'll use the following concepts when describing a linear transforma-
tion T : V ‚ÜíW:
‚Ä¢ BV = {‚Éóe1,‚Éóe2, . . . ,‚Éóen}: a basis for the input vector space V
‚Ä¢ BW = {‚Éób1,‚Éób2, . . . ,‚Éóbm}: a basis for the output vector space W
‚Ä¢
[MT ]
BW
BV ‚ààRm√ón: a matrix representation of the linear trans-
formation T with respect to the bases BV and BW :
‚Éów = T(‚Éóv)
‚áî
[‚Éów]BW =
[MT ]
BW
BV [‚Éóv]BV .
By far, the most commonly used basis in linear algebra is the stan-
dard basis ÀÜe1 = (1, 0, 0, . . .)T, ÀÜe2 = (0, 1, 0, . . .)T, etc. It is therefore
customary to denote the matrix representation of a linear transfor-
mation T simply as MT , without an explicit reference to the input

246
LINEAR TRANSFORMATIONS
and output bases.
This simpliÔ¨Åed notation causes much confusion
and suÔ¨Äering when students later try to learn about change-of-basis
operations.
In order to really understand the connection between linear trans-
formations and their matrix representations, we need to have a little
talk about bases and matrix coeÔ¨Écients. It's a little complicated, but
the mental eÔ¨Äort you invest is worth the overall understanding you'll
gain. As the old Samurai saying goes, "Cry during training, laugh on
the battleÔ¨Åeld."
By the end of this section, you'll be able to handle any basis
question your teacher may throw at you.
Matrix components
Every linear transformation T can be represented as a matrix MT .
Consider the linear transformation T : V ‚ÜíW. Assume the input
vector space V is n-dimensional and let BV = {ÀÜe1, ÀÜe2, . . . , ÀÜen} be
a basis for V .
Assume the output space W is m-dimensional and
let BW = {‚Éób1,‚Éób2, . . . ,‚Éóbm} be a basis for output space of T.
The
coeÔ¨Écients of the matrix MT ‚ààRm√ón depend on the bases BV and
BW . We'll now analyze this dependence in detail.
To compute the matrix representation of T with respect to the
input basis BV = {ÀÜe1, ÀÜe2, . . . , ÀÜen}, we "probe" T with each of the
vectors in the basis and record the outputs as the columns of a matrix:
[MT ]BV =
Ô£Æ
Ô£ØÔ£ØÔ£∞
|
|
|
T(ÀÜe1)
T(ÀÜe2)
. . .
T(ÀÜen)
|
|
|
Ô£π
Ô£∫Ô£∫Ô£ª
BV
The subscript BV indicates the columns are built from outputs of the
basis BV . We can use the matrix [MT ]BV to compute T(‚Éóv) for a vector
‚Éóv expressed in the basis BV : ‚Éóv = (v1, v2, . . . , vn)T
BV .
The matrix-
vector product produces the correct linear combination of outputs:
[MT ]BV [‚Éóv]BV =
Ô£Æ
Ô£ØÔ£∞
|
|
|
T(ÀÜe1)
T(ÀÜe2)
. . .
T(ÀÜen)
|
|
|
Ô£π
Ô£∫Ô£ª
BV
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
v1
v2
...
vn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
BV
= v1T(ÀÜe1) + v2T(ÀÜe2) + ¬∑ ¬∑ ¬∑ + vnT(ÀÜen)
= T(v1ÀÜe1 + v2ÀÜe2 + ¬∑ ¬∑ ¬∑ + vnÀÜen)
= T(‚Éóv).

6.3
CHANGE OF BASIS FOR MATRICES
247
So far we've been treating the outputs of T as abstract vectors T(ÀÜej) ‚àà
W. Like all vectors in the space W, each output of T can be expressed
as a vector of coeÔ¨Écients with respect to the basis BW . For example,
the output T(ÀÜe1) can be expressed as
T(ÀÜe1) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
c11
c21
...
cm1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
BW
= c11‚Éób1 + c21‚Éób2 + ¬∑ ¬∑ ¬∑ + cm1‚Éóbm,
for some coeÔ¨Écients c11, c21, . . . , cm1. Similarly, the other output vec-
tors T(ÀÜej) can be expressed as coeÔ¨Écients with respect to the basis
BW , T(ÀÜej) = (c1j, c2j, . . . , cmj)T
BW .
We're now in a position to Ô¨Ånd the matrix representation
[MT ]
BW
BV
of the linear transformation T, with respect to the input basis BV and
the output basis BW :
[MT ]
BW
BV =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
c11
c12
¬∑ ¬∑ ¬∑
c1n
c21
c22
¬∑ ¬∑ ¬∑
c2n
...
...
cm1
cm2
¬∑ ¬∑ ¬∑
cmn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
BW
BV
‚ààRm√ón.
The action of T on a vector ‚Éóv is the same as the product of
[MT ]
BW
BV
and the vector of coeÔ¨Écients [‚Éóv]BV = (v1, v2, . . . , vn)T
BV :
[T(‚Éóv)]BW =
[MT ]
BW
BV [‚Éóv]BV .
You may feel this example has stretched the limits of your attention
span, but bear in mind, these nitty-gritty details hold the meaning of
matrix coeÔ¨Écients. If you can see how the positions of the coeÔ¨Écients
in the matrix encode the information about T and the choice of bases
BV and BW , you're well on your way to getting it. The coeÔ¨Écient
cij in the ith row and jth column in the matrix
[MT ]
BW
BV is the ith
component (with respect to BW ) of the output of T when the input
is ÀÜej.
Verify that the matrix representation
[MT ]
BW
BV correctly pre-
dicts the output of T for the input ‚Éóv = 5ÀÜe1 + 6ÀÜe2 = (5, 6, 0, . . .)T
BV .
Using the linearity of T, we know the correct output is T(‚Éóv) =
T(5ÀÜe1 +6ÀÜe2) = 5T(ÀÜe1)+6T(ÀÜe2). We can verify that the matrix-vector
product
[MT ]
BW
BV [‚Éóv]BV leads to the same answer:
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
c11
c12
¬∑ ¬∑ ¬∑
c1n
c21
c22
¬∑ ¬∑ ¬∑
c2n
...
...
cm1
cm2
¬∑ ¬∑ ¬∑
cmn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
5
6
0
...
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª= 5
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
c11
c21
...
cm1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª+ 6
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
c12
c22
...
cm1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª= 5T(ÀÜe1) + 6T(ÀÜe2).

248
LINEAR TRANSFORMATIONS
Change of basis for matrices
Given the matrix representation
[MT ]
BW
BV of the linear transforma-
tion T : V ‚ÜíW, you're asked to Ô¨Ånd the matrix representation of
T with respect to diÔ¨Äerent bases B‚Ä≤
V and B‚Ä≤
W . This is the change-of-
basis task for matrices.
We'll discuss the important special case where the input space
and the output space of the linear transformation are the same. Let
T : V ‚ÜíV be a linear transformation and let B = {ÀÜe1, ÀÜe2, . . . , ÀÜen}
and B‚Ä≤ = {ÀÜe‚Ä≤
1, ÀÜe‚Ä≤
2, . . . , ÀÜe‚Ä≤
n} be two bases for the vector space V .
Recall the change-of-basis matrix
[1]
B‚Ä≤
B that converts vectors
from B coordinates to B‚Ä≤ coordinates, and its inverse
[1]
B
B‚Ä≤, which
converts vectors from B‚Ä≤ coordinates to B coordinates:
[‚Éóv]B‚Ä≤ =
[1]
B‚Ä≤
B[‚Éóv]B
and
[‚Éóv]B =
[1]
B
B‚Ä≤[‚Éóv]B‚Ä≤ .
A clariÔ¨Åcation of notation is in order.
The change-of-basis matrix
[1]
B‚Ä≤
B is not equal to the identity matrix 1n. However, the change-
of-basis operation is logically equivalent to an identity transformation:
the vector ‚Éóv doesn't change‚Äîonly its coeÔ¨Écients. If you don't remem-
ber the change-of-basis operation for vectors, now's the time to Ô¨Çip
back to Section 5.3 (page 198) and review before continuing.
Given the matrix representation
[MT ]
B
B of the linear transforma-
tion T with respect to B, we want to Ô¨Ånd the matrix
[MT ]
B‚Ä≤
B‚Ä≤, which
is the representation of T with respect to the basis B‚Ä≤. The computa-
tion is straightforward. Perform the change-of-basis operation on the
input and output vectors:
[MT ]
B‚Ä≤
B‚Ä≤ =
[1]
B‚Ä≤
B
[MT ]
B
B
[1]
B
B‚Ä≤ .
This group of three matrices is interpreted as follows. Imagine an in-
put vector [‚Éóv]B‚Ä≤ multiplying the three matrices
[1]
B‚Ä≤
B
[MT ]
B
B
[1]
B
B‚Ä≤
from the right. In the Ô¨Årst step,
[1]
B
B‚Ä≤ converts the vector from the
basis B‚Ä≤ to the basis B so the matrix
[MT ]
B
B can be applied. In the
last step, the matrix B‚Ä≤[1]B converts the output of
[MT ]
B
B to the
basis B‚Ä≤. The combined eÔ¨Äect of multiplying by this speciÔ¨Åc arrange-
ment of three matrices is the same as applying T to the input vector
‚Éóv:
[1]
B‚Ä≤
B
[MT ]
B
B
[1]
B
B‚Ä≤[‚Éóv]B‚Ä≤ = [T(‚Éóv)]B‚Ä≤ ,
which means
[MT ]
B‚Ä≤
B‚Ä≤ ‚â°
[1]
B‚Ä≤
B
[MT ]
B
B
[1]
B
B‚Ä≤ .
This formula makes sense intuitively: to obtain a matrix with respect
to a diÔ¨Äerent basis, we must surround it by appropriate change-of-
basis matrices.

6.4
INVERTIBLE MATRIX THEOREM
249
Note the "touching dimensions" of the matrices are expressed with
respect to the same basis. Indeed, we can think of the change-of-basis
matrix as an "adaptor" we use to express vectors in a diÔ¨Äerent basis.
The change-of-basis operation for matrices requires two adaptors: one
for the input space and one for the output space of the matrix.
Similarity transformation
It's interesting to note the abstract mathematical properties of the
operation used above. Consider any matrix N ‚ààRn√ón and an in-
vertible matrix P ‚ààRn√ón.
DeÔ¨Åne M to be the result when N is
multiplied by P on the left and by the inverse P ‚àí1 on the right:
M = PNP ‚àí1.
We say matrices N and M are related by a similarity transformation.
Since the matrix P is invertible, its columns form a basis for the
vector space Rn. Thus, we can interpret P as a change-of-basis ma-
trix that converts the standard basis to the basis of the columns of P.
The matrix P ‚àí1 corresponds to the inverse change-of-basis matrix.
Using this interpretation, the matrix M corresponds to the same lin-
ear transformation as the matrix N, but is expressed with respect to
the basis P.
Similarity transformations preserve certain properties of matrices:
‚Ä¢ Trace: Tr(M) = Tr(N)
‚Ä¢ Determinant: det(M) = det(N)
‚Ä¢ Rank: rank(M) = rank(N)
‚Ä¢ Eigenvalues: eig(M) = eig(N)
Together, the trace, the determinant, the rank, and the eigenvalues of
a matrix are known as the invariant properties of the matrix because
they don't depend on the choice of basis.
Exercises
E6.2 Suppose you're given the matrix representation of a linear trans-
formation T with respect to the basis B‚Ä≤:
[MT ]
B‚Ä≤
B‚Ä≤. What formula
describes the matrix representation of T with respect to the basis B?
6.4
Invertible matrix theorem
So far, we discussed systems of linear equations, matrices, vector
spaces, and linear transformations. It's time to tie it all together!
We'll now explore connections between these diÔ¨Äerent contexts where

250
LINEAR TRANSFORMATIONS
matrices are used. Originally, we explored how matrices can solve sys-
tems of linear equations. Later, we studied geometrical properties of
matrices, including their row spaces, column spaces, and null spaces.
In Chapter 6, we learned about the connection between matrices and
linear transformations. In each of these domains, invertible matrices
play a particularly important role. Lucky for us, there's a theorem
that summarizes ten important facts about invertible matrices. One
theorem; ten facts. Now that's a good deal!
Invertible matrix theorem: For an n √ó n matrix A, the following
statements are equivalent:
(1) A is invertible
(2) The equation A‚Éóx = ‚Éób has exactly one solution for each ‚Éób ‚ààRn
(3) The null space of A contains only the zero vector N(A) = {‚Éó0}
(4) The equation A‚Éóx = ‚Éó0 has only the trivial solution ‚Éóx = ‚Éó0
(5) The columns of A form a basis for Rn:
‚Ä¢ The columns of A are linearly independent
‚Ä¢ The columns of A span Rn; C(A) = Rn
(6) The rank of the matrix A is n
(7) The RREF of A is the n √ó n identity matrix 1m
(8) The transpose matrix AT is invertible
(9) The rows of A form a basis for Rn:
‚Ä¢ The rows of A are linearly independent
‚Ä¢ The rows of A span Rn; R(A) = Rn
(10) The determinant of A is nonzero det(A) Ã∏= 0
These 10 statements are either all true or all false for a given matrix
A. We can split the set of n √ó n matrices into two disjoint subsets:
invertible matrices, for which all 10 statements are true, and non-
invertible matrices, for which all statements are false.
Proof of the invertible matrix theorem
It's essential you understand the details of this proof; the reasoning
that connects these 10 statements unites all the chunks of linear al-
gebra we've discussed. If you don't consider yourself a proof person,

6.4
INVERTIBLE MATRIX THEOREM
251
there's no excuse! Be sure to read and reread the proof, as it will help
to solidify your understanding of the material covered thus far.
Proofs by contradiction
Since our arrival at the invertible matrix theorem marks an im-
portant step, we'll Ô¨Årst quickly review some handy proof techniques,
just to make sure everyone's ready. A proof by contradiction starts by
assuming the opposite of the fact we want to prove, and after several
derivation steps arrives at a contradiction‚Äîa mathematical inconsis-
tency. Arriving at a contradiction implies our original premise is false,
which means the fact we want to prove is true. Thus, to show that
(A) implies (B)‚Äîdenoted (A)‚áí(B)‚Äîwe can show that not-(B) im-
plies not-(A). An example of a proof by contradiction is the proof of
‚àö
2 /‚ààQ (see page 75).
Review of deÔ¨Ånitions
To really make sure we're all on board before the train leaves the
station, it's wise to review some deÔ¨Ånitions from previous chapters.
The matrix A ‚ààRn√ón is invertible if there exists a matrix A‚àí1 such
that AA‚àí1 = 1n = A‚àí1A. The null space of A is the set of vectors
that become the zero vector when multiplying A from the right: {‚Éóv ‚àà
Rn | A‚Éóv = ‚Éó0}. The column space C(A) ‚äÜRn consists of all possible
linear combinations of the columns of the matrix A. Similarly, the
row space R(A) ‚äÜRn consists of all possible linear combinations
of the rows of A. The rank of a matrix, denoted rank(A), is equal
to the dimension of the row space and the column space rank(A) =
dim(C(A)) = dim(R(A)).
The rank of A is also equal to number
of pivots (leading ones) in the reduced row echelon form of A. The
determinant of A corresponds to the scale factor by which the linear
transformation TA(‚Éóx) ‚â°A‚Éóx transforms the n-dimensional volume of
the hyper-cube in the input space when it maps it to the output
space. If any of the columns of the matrix A are linearly dependent,
the determinant |A| will be zero. Phew!
Proof of the invertible matrix theorem. The moment has arrived:
we'll prove the equivalence of the 10 statements in the theorem by
showing a closed chain of implications between statements (1) through
(7). We'll separately show the equivalences (1) ‚áî(8) ‚áî(9) and
(5) ‚áî(10). Figure 6.6 shows an outline of the proof.
(1)‚áí(2): Assume A is invertible so there exists an inverse matrix A‚àí1
such that A‚àí1A = 1n. Therefore, for all ‚Éób ‚ààRn, the expression ‚Éóx =
A‚àí1‚Éób is a solution to A‚Éóx = ‚Éób. We must show the solution ‚Éóx = A‚àí1‚Éób

252
LINEAR TRANSFORMATIONS
(1)
(2)
(3)
(4)
(7)
(6)
(5)
(8)
(9)
(10)
Figure 6.6: The chain of implications used to prove the invertible ma-
trix theorem.
is the unique solution to this equation. Assume, for a contradiction,
that a diÔ¨Äerent solution ‚Éóy Ã∏= ‚Éóx exists, which also satisÔ¨Åes the equation
A‚Éóy = ‚Éób. Multiplying both sides of the equation A‚Éóy = ‚Éób by A‚àí1, we
obtain A‚àí1A‚Éóy = ‚Éóy = A‚àí1‚Éób. We see that ‚Éóy = A‚àí1‚Éób = ‚Éóx, which is
contrary to our assumption that ‚Éóy Ã∏= ‚Éóx. Thus, our assumption that a
second solution ‚Éóy Ã∏= ‚Éóx exists is false, and the equation A‚Éóx = ‚Éób has the
unique solution ‚Éóx = A‚àí1‚Éób.
(2)‚áí(3): We want to show that a unique solution to A‚Éóx = ‚Éób implies
the matrix A has a trivial null space N(A) = {‚Éó0}. We start by as-
suming the opposite is true: that N(A) contains at least one nonzero
vector ‚Éóy Ã∏= 0. If this were true, then ‚Éóx‚Ä≤ ‚â°‚Éóx + ‚Éóy would also be a solu-
tion to A‚Éóx = ‚Éób, since A‚Éóx‚Ä≤ = A(‚Éóx + ‚Éóy) = A‚Éóx + A‚Éóy = A‚Éóx +‚Éó0 = ‚Éób, since
A‚Éóy = ‚Éó0. The fact that two solutions (‚Éóx and ‚Éóx‚Ä≤ Ã∏= ‚Éóx) exist contradicts
the statement that A‚Éóx = ‚Éób has a unique solution. Thus, for A‚Éóx = ‚Éób to
have a unique solution, A must have a trivial null space N(A) = {‚Éó0}.
(3)‚áí(4) Statements (3) and (4) are equivalent by deÔ¨Ånition: the con-
dition that A's null space contains only the zero vector, N(A) = {‚Éó0},
is equivalent to the condition that the only solution to the equation
A‚Éóv = ‚Éó0 is ‚Éóv = ‚Éó0.
(4)‚áí(5): Analyze the equation A‚Éóv = ‚Éó0 in the column picture of ma-
trix multiplication, denoting the n columns of A as ‚Éóc1,‚Éóc2, . . . ,‚Éócn. We
obtain A‚Éóv = v1‚Éóc1+v2‚Éóc2+¬∑ ¬∑ ¬∑+vn‚Éócn = ‚Éó0. Since ‚Éóv = (v1, v2, . . . , vn) = ‚Éó0
is the only solution to this equation, we obtain the statement in the
deÔ¨Ånition of linear independence for a set of vectors. The fact that
A‚Éóv = ‚Éó0 has only ‚Éóv = ‚Éó0 as a solution implies the columns of A form a
linearly independent set. Furthermore, the columns of A form a basis
for Rn because they are a set of n linearly independent vectors in an
n-dimensional vector space.
(5)‚áí(6): We know rank(A) equals the number of linearly indepen-
dent columns in A. Since the n columns of A are linearly independent,
it follows that rank(A) = n.

6.4
INVERTIBLE MATRIX THEOREM
253
(6)‚áí(7): The rank is also equal to the number of leading ones (piv-
ots) in the RREF of A. Since A has rank n, its reduced row echelon
form must contain n pivots. The reduced row echelon form of an n√ón
matrix with n pivots is the identity matrix 1n.
(7)‚áí(1): We start from the assumption rref(A) = 1n. This means
it's possible to use a set of row operations R1, R2, . . . , Rk to transform
A to the identity matrix: Rk(¬∑ ¬∑ ¬∑ R2(R1(A)) ¬∑ ¬∑ ¬∑ ) = 1n. Consider the
elementary matrices E1, E2, . . . , Ek that correspond to the row opera-
tions R1, R2, . . . , Rk. Rewriting the equation Rk(¬∑ ¬∑ ¬∑ R2(R1(A)) ¬∑ ¬∑ ¬∑ ) =
1n in terms of these elementary matrices gives us Ek ¬∑ ¬∑ ¬∑ E2E1A = 1n.
This equation implies the inverse of A exists and is equal to the prod-
uct of elementary matrices A‚àí1 ‚â°Ek ¬∑ ¬∑ ¬∑ E2E1.
(1)‚áî(8): If A is invertible, there exists A‚àí1 such that AA‚àí1 = 1n.
If we apply the transpose operation to this equation, we obtain
(AA‚àí1)T = (1n)T
‚áí
(A‚àí1)TAT = 1n,
which shows the matrix (A‚àí1)T = (AT)‚àí1 exists and is the inverse of
AT. Therefore, A is invertible if and only if AT is invertible.
(8)‚áî(9): Statement (9) follows by a combination of statements (8)
and (5): if AT is invertible, its columns form a basis for Rn. Since the
columns of AT are the rows of A, it follows that the rows of A form a
basis for Rn.
(5)‚áî(10): The determinant of an n √ó n matrix is nonzero if and
only if the columns of the matrix are linearly independent. Thus, the
columns of the matrix A form a basis for Rn if and only if det(A) Ã∏= 0.
Nice work! By proving the chain of implications (1) ‚áí(2) ‚áí
¬∑ ¬∑ ¬∑ ‚áí(7) ‚áí(1), we've shown that the Ô¨Årst seven statements are
equivalent. If one of these statements is true, then all others are true‚Äî
just follow the arrows of implication. Alternatively, if one statement
is false, all statements are false, as we see by following the arrows of
implication in the backward direction.
We also "attached" statements (8), (9), and (10) to the main loop
of implications using "if an only if" statements. Thus, we've shown
the equivalence of all 10 statements, which completes the proof.
The steps of the proof shown above cover only a small selection of all
possible implications between the 10 statements. Coming up, you'll
be asked to prove several other implications as exercises.
It's im-
portant to practice these proofs! Obtaining proofs forces your brain
to truly grasp about linear algebra concepts and their deÔ¨Ånitions, as
well as directly apply their properties.
Note the crucial diÔ¨Äerence

254
LINEAR TRANSFORMATIONS
between "one-way" implications of the form (A) ‚áí(B) and if and
only if statements (A) ‚áî(B). The latter require you to prove both
directions of the implication: (A) ‚áí(B) and (A) ‚áê(B).
Invertible linear transformations
We can reinterpret the statements in the invertible matrix theo-
rem as a statement about invertible linear transformations:
T : Rn ‚ÜíRn is invertible
‚áî
MT ‚ààRn√ón is invertible.
The set of linear transformations splits into two disjoint subsets: in-
vertible linear transformations and non-invertible linear transforma-
tions.
Kernel and null space
The kernel of the linear transformation T is the same as the null
space of its matrix representation MT . Recall statement (3) of the
invertible matrix theorem: a matrix A is invertible if and only if its
null space contains only the zero vector N(A) = {‚Éó0}. The equivalent
condition for linear transformations is the zero kernel condition. A
linear transformation T is invertible if and only if its kernel contains
only the zero vector:
Ker(T) = {‚Éó0}
‚áî
T is invertible.
Invertible linear transformations T : Rn ‚ÜíRn map diÔ¨Äerent input
vectors ‚Éóx to diÔ¨Äerent output vectors ‚Éóy ‚â°T(‚Éóv); therefore it's possible
to build an inverse linear transformation T ‚àí1 : Rn ‚ÜíRn that restores
every ‚Éóy back to the ‚Éóx it came from.
In contrast, a non-invertible linear transformation S sends all vec-
tors ‚Éóx ‚ààKer(S) to the zero vector S(‚Éóx) = ‚Éó0. When this happens,
there is no way to undo the action of S since we can't determine the
original ‚Éóx that was sent to ‚Éó0.
Linear transformations as functions
In Section 1.8, we discussed the notion of invertibility for functions of
a real variable, f : R ‚ÜíR. In particular, we used the terms injective,
surjective, and bijective to describe how a function maps diÔ¨Äerent
inputs from its domain to outputs in its codomain (see page 33). Since
linear transformations are vector functions, we can apply the general
terminology for functions to describe how linear transformations map
diÔ¨Äerent inputs to outputs.

6.4
INVERTIBLE MATRIX THEOREM
255
A linear transformation is injective if it maps diÔ¨Äerent inputs to
diÔ¨Äerent outputs:
T(‚Éóv1) Ã∏= T(‚Éóv2) for all ‚Éóv1 Ã∏= ‚Éóv2
‚áî
T is injective.
A linear transformation T : Rn ‚ÜíRm is surjective if its image space
equals its codomain:
Im(T) = Rm
‚áî
T is surjective.
The surjective condition for linear transformations is equivalent to the
condition that the column space of the matrix MT spans the outputs
space: Im(T) ‚â°C(MT ) = Rm.
If a function is both injective and surjective then it is bijective.
Bijective functions are one-to-one correspondences between elements
in their input space and elements in their output space.
T(‚Éóx) = ‚Éóy has exactly one solution for each ‚Éóy
‚áî
T is bijective.
All bijective functions are invertible since each output ‚Éóy in the output
space corresponds to exactly one ‚Éóx in the input space.
Interestingly, for a linear transformation T : Rn ‚ÜíRn to be in-
vertible, the presence of either the injective or surjective property is
suÔ¨Écient.
If T is injective, it must have a Ker(T) = {‚Éó0} so it is
invertible. If T is surjective, its matrix representation MT has rank
n; the rank-nullity theorem (rank(MT ) + nullity(MT ) = n) tells
us MT has an empty null space N(MT ) ‚â°Ker(T) = {‚Éó0}, making T
invertible.
Links
[ See Section 2.3 of this page for a proof walkthrough ]
http://math.nyu.edu/~neylon/linalgfall04/project1/jja/group7.htm
[ Nice writeup about the invertible matrix theorem with proofs ]
http://bit.ly/InvMatThmProofs
[ Visualization of a two-dimensional linear transformations ]
http://ncase.me/matrix/
Exercises
E6.3 Prove that statement (5) of the invertible matrix theorem
implies statement (2).
E6.4 Prove that statement (2) implies statement (1).

256
LINEAR TRANSFORMATIONS
Discussion
In this chapter, we learned about linear transformations and their
matrix representations. The equivalence T(‚Éóx) ‚â°MT‚Éóx is important
because it forms a bridge between the abstract notion of a "vector
function" and its concrete implementation as a matrix-vector prod-
uct. Everything you know about matrices can be applied to linear
transformations, and everything you know about linear transforma-
tions can be applied to matrices. Which is mind-blowing, if you think
about it.
We say T is represented by the matrix
[MT ]
BW
BV with respect
to the basis BV for the input space and the basis BW for the out-
put space. In Section 6.2 we learned about the "probing procedure"
for Ô¨Ånding matrix representations with respect to the standard basis,
while Section 6.3 discussed the notion of change of basis for matrices.
Hold tight, because in the next chapter we'll learn about the eigenval-
ues and eigenvectors of matrices and discuss the eigendecomposition
of matrices, which is a type of change of basis.
Section 6.4 gave us the invertible matrix theorem along with a
taste of what it takes to prove formal math statements. It's extra im-
portant that you attempt some of the proofs in the exercise section on
page 255. Although proofs can be complicated, they're so worth your
time because they force you to clarify the deÔ¨Ånitions and properties
of all the math concepts you've encountered thus far. Attempting the
proofs in the problems section to Ô¨Ånd out if you're a linear algebra
amateur, or a linear algebra expert.
6.5
Linear transformations problems
P6.1
Find image space Im(T) for the linear transformation T : R2 ‚ÜíR3,
deÔ¨Åned by T(x, y) = (x, x ‚àíy, 2y). You are given the standard basis Bs =
{(1, 0), (0, 1)} .
P6.2
Let V be the vector space consisting of all functions of the form
Œ±e2x cos x + Œ≤e2x sin x.
Consider the following linear transformation L :
V ‚ÜíV , L(f) = f ‚Ä≤ + f. Find the matrix representing L with respect to the
basis {e2x cos x, e2x sin x}.
P6.3
Find the matrix representation of the derivative operatorDp ‚â°
d
dxp(x). Assume you're working in the vector space of polynomials of degree
three p(x) = a0+a1x+a2x2+a3x3, represented as coeÔ¨Écient (a0, a1, a2, a3).

Chapter 7
Theoretical linear algebra
Let's take a trip down memory lane: 150 pages ago, we embarked
on a mind-altering journey through the land of linear algebra. We
encountered vector and matrix operations.
We studied systems of
linear equations, solving them with row operations. We covered miles
of linear transformations and their matrix representations. With the
skills you've acquired to reach this point, you're ready to delve into
the abstract, theoretical aspects of linear algebra‚Äîthat is, since you
know all the useful stuÔ¨Ä, you can oÔ¨Écially move on to the cool stuÔ¨Ä.
The lessons in this chapter are less concerned with calculations and
more about mind expansion.
In math, we often use abstraction to Ô¨Ånd the commonalities be-
tween diÔ¨Äerent mathematical objects. These parallels give us a deeper
understanding of the mathematical structures we compare. This chap-
ter extends what we know about the vector space Rn to the realm
of abstract vector spaces of vector-like mathematical objects (Sec-
tion 7.3). We'll discuss linear independence, Ô¨Ånd bases, and count di-
mensions for these abstract vector spaces. Section 7.4 deÔ¨Ånes abstract
inner product operations and uses them to generalize the concept of
orthogonality for abstract vectors. Section 7.5 explores the Gram-
Schmidt orthogonalization procedure for distilling orthonormal bases
from non-orthonormal bases. The Ô¨Ånal section, Section 7.7, introduces
vectors and matrices with complex coeÔ¨Écients. This section also re-
views everything we've learned in this book, so be sure to read it even
if complex numbers are not required for your course. We'll also work
to develop a taxonomy for the diÔ¨Äerent types of matrices according
to their properties and applications (Section 7.2). Section 7.6 inves-
tigates matrix decompositions‚Äîtechniques for splitting matrices into
products of simpler matrices. We'll start the chapter by discussing
the most important decomposition technique: the eigendecomposition,
which is a way to uncover the "natural basis" of a matrix.
257

258
THEORETICAL LINEAR ALGEBRA
7.1
Eigenvalues and eigenvectors
The set of eigenvectors of a matrix is a special set of input vectors
for which the action of the matrix is described as a simple scaling.
In Section 6.2, we observed how linear transformations act diÔ¨Äerently
on diÔ¨Äerent input spaces. We also observed the special case of the
"zero eigenspace," called the null space of a matrix. The action of a
matrix on the vectors in its null space is equivalent to a multiplication
by zero. We'll now put these eigenvalues and eigenvectors under the
microscope and see what more there is to see.
Decomposing a matrix in terms of its eigenvalues and its eigenvec-
tors gives valuable insights into the properties of the matrix. Certain
matrix calculations, like computing the power of the matrix, become
much easier when we use the eigendecomposition of the matrix. For
example, suppose we're given a square matrix A and want to compute
A7. To make this example more concrete, we'll analyze the matrix
A =
 9
‚àí2
‚àí2
6

.
We want to compute
A7 =
 9
‚àí2
‚àí2
6
 9
‚àí2
‚àí2
6
 9
‚àí2
‚àí2
6
 9
‚àí2
‚àí2
6
 9
‚àí2
‚àí2
6
 9
‚àí2
‚àí2
6
 9
‚àí2
‚àí2
6

.
That's an awful lot of matrix multiplications! Now imagine how many
times we'd need to multiply the matrix if we wanted to Ô¨Ånd A17 or
A77‚Äîtoo many times, that's how many. Let's be smart about this.
Every matrix corresponds to some linear operation. This means it's
legit to ask, "what does the matrix A do?" Once we Ô¨Ågure out this
part, we can compute A77 by simply doing what A does 77 times.
The best way to see what a matrix does is to look inside it and
see what it's made of (you may need to gradually gain the matrix's
trust before it lets you do this). To understand the matrix A, you
must Ô¨Ånd its eigenvectors and its eigenvalues (eigen is the German
word for "self"). The eigenvectors of a matrix are a "natural basis" for
describing the action of the the matrix. The eigendecomposition is a
change-of-basis operation that expresses the matrix A with respect to
its eigenbasis (own-basis). The eigendecomposition of the matrix A
is a product of three matrices:
A =
"
9
‚àí2
‚àí2
6
#
=
"
1
2
2 ‚àí1
#
| {z }
Q
"
5
0
0
10
#
| {z }
Œõ
"
1
5
2
5
2
5
‚àí1
5
#
|
{z
}
Q‚àí1
= QŒõQ‚àí1.
You can multiply the three matrices QŒõQ‚àí1 to obtain A. Note that
the "middle matrix" Œõ (the capital Greek letter lambda) has entries

7.1
EIGENVALUES AND EIGENVECTORS
259
only on the diagonal. The diagonal matrix Œõ is sandwiched between
the matrix Q on the left and Q‚àí1 (the inverse of Q) on the right.
The eigendecomposition of A allows us to compute A7 in a civilized
manner:
A7
=
AAAAAAA
=
QŒõ Q‚àí1Q
| {z }
1
Œõ Q‚àí1Q
| {z }
1
Œõ Q‚àí1Q
| {z }
1
Œõ Q‚àí1Q
| {z }
1
Œõ Q‚àí1Q
| {z }
1
Œõ Q‚àí1Q
| {z }
1
ŒõQ‚àí1
=
QŒõ1Œõ1Œõ1Œõ1Œõ1Œõ1ŒõQ‚àí1
=
QŒõŒõŒõŒõŒõŒõŒõQ‚àí1
=
QŒõ7Q‚àí1.
All the inner Q‚àí1s cancel with the adjacent Qs.
How convenient!
Since the matrix Œõ is diagonal, it's easy to compute its seventh power:
Œõ7 =
5
0
0
10
7
=
57
0
0
107

=
78125
0
0
10 000 000

.
Thus we can express our calculation of A7 as
A7 =
"
9
‚àí2
‚àí2
6
#7
=
"
1
2
2 ‚àí1
#
| {z }
Q
"
78125
0
0
10 000 000
#"
1
5
2
5
2
5 ‚àí1
5
#
| {z }
Q‚àí1
.
We still need to multiply these three matrices together, but we've cut
the work from six matrix multiplications to two. The answer is
A7 = QŒõ7Q‚àí1 =
"
8015625
‚àí3968750
‚àí3968750
2062500
#
.
With this technique, we can compute A17 just as easily:
A17 = QŒõ17Q‚àí1 =
"
80000152587890625
‚àí39999694824218750
‚àí39999694824218750
20000610351562500
#
.
We could even compute A777 = QŒõ777Q‚àí1 if we wanted to. I hope by
now you get the point: once you express A in its eigenbasis, computed
powers of A requires computing powers of its eigenvalues, which is
much simpler than carrying out matrix multiplications.
DeÔ¨Ånitions
‚Ä¢ A: an n √ó n square matrix. The entries of A are denoted as aij.

260
THEORETICAL LINEAR ALGEBRA
‚Ä¢ eig(A) ‚â°(Œª1, Œª2, . . . , Œªn): the list of eigenvalues of A. Eigenval-
ues are usually denoted by the Greek letter lambda. Note that
some eigenvalues could be repeated in the list.
‚Ä¢ p(Œª) = det(A ‚àíŒª1): the characteristic polynomial of A. The
eigenvalues of A are the roots of the characteristic polynomial.
‚Ä¢ {‚ÉóeŒª1,‚ÉóeŒª2, . . . ,‚ÉóeŒªn}: the set of eigenvectors of A. Each eigenvec-
tor is associated with a corresponding eigenvalue.
‚Ä¢ Œõ ‚â°diag(Œª1, Œª2, . . . , Œªn): the diagonalized version of A. The
matrix Œõ contains the eigenvalues of A on the diagonal:
Œõ =
Ô£Æ
Ô£ØÔ£∞
Œª1
¬∑ ¬∑ ¬∑
0
...
...
0
0
0
Œªn
Ô£π
Ô£∫Ô£ª.
The matrix Œõ is the matrix A expressed in its eigenbasis.
‚Ä¢ Q: a matrix whose columns are eigenvectors of A:
Q ‚â°
Ô£Æ
Ô£ØÔ£∞
|
|
‚ÉóeŒª1
¬∑ ¬∑ ¬∑
‚ÉóeŒªn
|
|
Ô£π
Ô£∫Ô£ª=
[1]
Bs
BŒª .
The matrix Q corresponds to the change-of-basis matrix from
the eigenbasis BŒª = {‚ÉóeŒª1,‚ÉóeŒª2,‚ÉóeŒª3, . . .} to the standard basis
Bs = {ÀÜƒ±, ÀÜÔöæ, ÀÜk, . . .}.
‚Ä¢ A = QŒõQ‚àí1: the eigendecomposition of the matrix A
‚Ä¢ Œõ = Q‚àí1AQ: the diagonalization of the matrix A
Eigenvalues
The fundamental eigenvalue equation is
A‚ÉóeŒª = Œª‚ÉóeŒª,
where Œª is an eigenvalue and ‚ÉóeŒª is an eigenvector of the matrix A.
Multiply A by one of its eigenvectors ‚ÉóeŒª, and the result is the same
vector scaled by the constant Œª.
To Ô¨Ånd the eigenvalues of a matrix, start from the eigenvalue equa-
tion A‚ÉóeŒª = Œª‚ÉóeŒª, insert the identity 1, and rewrite the equation as a
null-space problem:
A‚ÉóeŒª = Œª1‚ÉóeŒª
‚áí
(A ‚àíŒª1)‚ÉóeŒª = ‚Éó0.
This equation has a solution whenever |A ‚àíŒª1| = 0. The eigenvalues
of A ‚ààRn√ón, denoted (Œª1, Œª2, . . . , Œªn), are the roots of the character-
istic polynomial:
p(Œª) ‚â°det(A ‚àíŒª1) = 0.

7.1
EIGENVALUES AND EIGENVECTORS
261
Calculate this determinant and we obtain an expression involving the
coeÔ¨Écients aij and the variable Œª.
If A is an n √ó n matrix, the
characteristic polynomial is a polynomial of degree n in Œª.
We denote the list of eigenvalues as eig(A) = (Œª1, Œª2, . . . , Œªn). If
Œªi is a repeated root of the characteristic polynomial p(Œª), it's called a
degenerate eigenvalue. For example, the identity matrix 1 ‚ààR2√ó2 has
the characteristic polynomial p1(Œª) = (Œª ‚àí1)2, which has a repeated
root at Œª = 1. We say the eigenvalue Œª = 1 is degenerate and has
algebraic multiplicity 2. It's important to keep track of degenerate
eigenvalues, so we specify the multiplicity of an eigenvalue by repeat-
edly including it in the list of eigenvalues: eig(1) = (Œª1, Œª2) = (1, 1).
Eigenvectors
The eigenvectors associated with eigenvalue Œªi of matrix A are the
vectors in the null space of the matrix (A ‚àíŒªi1).
To Ô¨Ånd the eigenvectors associated with the eigenvalue Œªi, you
need to solve for the components eŒª,x and eŒª,y of the vector ‚ÉóeŒª =
(eŒª,x, eŒª,y) that satisÔ¨Åes the equation
A‚ÉóeŒª = Œª‚ÉóeŒª,
or equivalently,
(A ‚àíŒª1)‚ÉóeŒª = 0
‚áî
a11 ‚àíŒª
a12
a21
a22 ‚àíŒª
 
eŒª,x
eŒª,y

=

0
0

.
You previously solved this type of problem when you learned how to
compute the null space of a matrix.
If Œªi is a repeated root (degenerate eigenvalue), the null space
of (A ‚àíŒªi1) could contain multiple eigenvectors. The dimension of
the null space of (A ‚àíŒªi1) is called the geometric multiplicity of the
eigenvalue Œªi.
Eigendecomposition
If an n√ón matrix A is diagonalizable (no, I did't make that word up)
this means we can Ô¨Ånd n eigenvectors for that matrix. The eigenvec-
tors that come from diÔ¨Äerent eigenspaces are guaranteed to be linearly
independent (see exercise E7.3). We can also pick a set of linearly
independent vectors within each of the degenerate eigenspaces. Com-
bining the eigenvectors from all the eigenspaces gives us a set of n
linearly independent eigenvectors, which form a basis for Rn. This is
the eigenbasis.

262
THEORETICAL LINEAR ALGEBRA
Let's place the n eigenvectors side by side as the columns of a
matrix:
Q ‚â°
Ô£Æ
Ô£ØÔ£∞
|
|
‚ÉóeŒª1
¬∑ ¬∑ ¬∑
‚ÉóeŒªn
|
|
Ô£π
Ô£∫Ô£ª.
We can decompose A in terms of its eigenvalues and its eigenvectors:
A = QŒõQ‚àí1 =
Ô£Æ
Ô£ØÔ£∞
|
|
‚ÉóeŒª1
¬∑ ¬∑ ¬∑
‚ÉóeŒªn
|
|
Ô£π
Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£∞
Œª1
¬∑ ¬∑ ¬∑
0
...
...
0
0
0
Œªn
Ô£π
Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£∞
Q‚àí1
Ô£π
Ô£∫Ô£ª.
The matrix Œõ is a diagonal matrix of eigenvalues, and the matrix Q
is the change-of-basis matrix that contains the corresponding eigen-
vectors as columns.
Note that only the direction of each eigenvector is important and
not the length. Indeed, if ‚ÉóeŒª is an eigenvector (with eigenvalue Œª), so
is any Œ±‚ÉóeŒª for all Œ± ‚ààR. Thus we're free to use any multiple of the
vectors ‚ÉóeŒªi as the columns of the matrix Q.
Example
Find the eigendecomposition of the matrix:
A =
Ô£Æ
Ô£∞
1
2
0
0
3
0
2
‚àí4
2
Ô£π
Ô£ª.
In decreasing order, the eigenvalues of the matrix are:
Œª1 = 3,
Œª2 = 2,
Œª3 = 1.
The eigenvalues of A are the values that appear on the diagonal of Œõ.
When a 3√ó3 matrix has three distinct eigenvalues, it is diagonaliz-
able, since it has the same number of linearly independent eigenvectors
as eigenvalues. We know the eigenvectors are linearly independent by
the following reasoning. The matrix A has three diÔ¨Äerent eigenval-
ues. Each eigenvalue is associated with at least one eigenvector, and
these eigenvectors are linearly independent (see E7.3 on page 270).
Recall that any set of n linearly independent vectors in Rn forms a
basis for Rn. Since the three eigenvectors of A are linearly indepen-
dent, we have enough columns to construct a change-of-basis matrix
of eigenvectors Q and use it to write A = QŒõQ‚àí1.
To Ô¨Ånd the eigenvectors of A, solve for the null space of the ma-
trices (A ‚àí31), (A ‚àí21), and (A ‚àí1) respectively:
‚ÉóeŒª1 =
Ô£Æ
Ô£∞
‚àí1
‚àí1
2
Ô£π
Ô£ª,
‚ÉóeŒª2 =
Ô£Æ
Ô£∞
0
0
1
Ô£π
Ô£ª,
‚ÉóeŒª3 =
Ô£Æ
Ô£∞
‚àí1
0
2
Ô£π
Ô£ª.

7.1
EIGENVALUES AND EIGENVECTORS
263
Check that A‚ÉóeŒªk = Œªk‚ÉóeŒªk for each of the vectors above. Let Q be the
matrix constructed with the eigenvectors as its columns:
Q =
Ô£Æ
Ô£∞
‚àí1
0
‚àí1
‚àí1
0
0
2
1
2
Ô£π
Ô£ª,
then compute
Q‚àí1 =
Ô£Æ
Ô£∞
0
‚àí1
0
2
0
1
‚àí1
1
0
Ô£π
Ô£ª.
Together with the matrix Œõ, these matrices form the eigendecompo-
sition of the matrix A:
A = QŒõQ‚àí1 =
Ô£Æ
Ô£∞
1
2
0
0
3
0
2 ‚àí4
2
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
‚àí1
0 ‚àí1
‚àí1
0
0
2
1
2
Ô£π
Ô£ª
Ô£Æ
Ô£∞
3
0
0
0
2
0
0
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
0
‚àí1
0
2
0
1
‚àí1
1
0
Ô£π
Ô£ª.
To Ô¨Ånd the diagonalization of A, move Q and Q‚àí1 to the other side of
the equation. More speciÔ¨Åcally, multiply the equation A = QŒõQ‚àí1 by
Q‚àí1 on the left and by Q on the right to obtain the diagonal matrix:
Œõ = Q‚àí1AQ =
Ô£Æ
Ô£∞
0
‚àí1
0
2
0
1
‚àí1
1
0
Ô£π
Ô£ª
Ô£Æ
Ô£∞
1
2
0
0
3
0
2 ‚àí4
2
Ô£π
Ô£ª
Ô£Æ
Ô£∞
‚àí1
0 ‚àí1
‚àí1
0
0
2
1
2
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
3
0
0
0
2
0
0
0
1
Ô£π
Ô£ª.
Have you noticed how time consuming it is to compute the eigende-
composition of a matrix? It's really incredible. Though we skipped
some details of the calculation (including Ô¨Ånding the solution of the
characteristic polynomial and the three null space calculations), Ô¨Ånd-
ing the eigendecomposition of a 3 √ó 3 matrix took more than a page
of work. Don't be surprised if it takes hours to compute eigendecom-
positions on homework problems; you're not doing anything wrong.
Eigendecompositions simply take a lot of work. We're dealing with a
seven-syllable word here, folks‚Äîdid you really expect the process to
be easy?
Well, actually, computing eigenvectors and eigenvalues will be-
come easier with practice. After eigendecomposing a dozen matrices
or so using only pen and paper, you'll be able to whip through the
steps for a 3 √ó 3 matrix in about 20 minutes. That's a really good
thing for your GPA, because the probability of seeing an eigenvalue
question on your linear algebra Ô¨Ånal exam is about 80%. It pays to
have a mathematical edge with this stuÔ¨Ä.
For readers learning linear algebra without the added motivational
bonus of exam stress, I recommend you eigendecompose at least one
matrix using only pen and paper to prove to yourself you can do it.
In all other circumstances, you're better oÔ¨Äusing SymPy to create
a Matrix object; then calling its eigenvals() method to Ô¨Ånd the
eigenvalues, or its eigenvects() method to Ô¨Ånd both its eigenvalues
and eigenvectors.

264
THEORETICAL LINEAR ALGEBRA
Explanations
Yes, we've got some explaining to do.
Eigenspaces
Recall the deÔ¨Ånition of the null space of a matrix A:
N(A) ‚â°{‚Éóv ‚ààRn | A‚Éóv = 0}.
The dimension of the null space is the number of linearly independent
vectors in the null space.
Example
If the matrix A sends exactly two linearly independent
vectors ‚Éóv and ‚Éów to the zero vector‚ÄîA‚Éóv = 0, A‚Éów = 0‚Äîthen its null
space is two-dimensional. We can always choose the vectors ‚Éóv and ‚Éów
to be orthogonal ‚Éóv ¬∑ ‚Éów = 0 and thus obtain an orthogonal basis for the
null space.
Each eigenvalue Œªi is associated with an eigenspace. The eigenspace
EŒªi is the null space of the matrix (A ‚àíŒªi1):
EŒªi ‚â°N (A ‚àíŒªi1) = {‚Éóv ‚ààRn | (A ‚àíŒªi1)‚Éóv = ‚Éó0 }.
Every eigenspace contains at least one nonzero eigenvector. For de-
generate eigenvalues (repeated roots of the characteristic polynomial)
the null space of (A ‚àíŒªi1) can contain multiple eigenvectors.
Change-of-basis matrix
The matrix Q is a change-of-basis matrix. Given a vector expressed
in the eigenbasis [‚Éóv]BŒª = (v‚Ä≤
1, v‚Ä≤
2, v‚Ä≤
3)T
BŒª = v‚Ä≤
1‚ÉóeŒª1 + v‚Ä≤
2‚ÉóeŒª3 + v‚Ä≤
3‚ÉóeŒª3, we
can use the matrix Q to convert it to coeÔ¨Écients in the standard basis
[‚Éóv]Bs = (v1, v2, v3)T
Bs = v1ÀÜƒ± + v2ÀÜÔöæ+ v3ÀÜk as follows:
[‚Éóv]Bs = Q[‚Éóv]BŒª =
[1]
Bs
BŒª[‚Éóv]BŒª.
The change of basis in the other direction is given by the inverse
matrix:
[‚Éóv]BŒª = Q‚àí1[‚Éóv]Bs =
[1]
BŒª
Bs[‚Éóv]Bs.
Interpretation
The eigendecomposition A = QŒõQ‚àí1 allows us to interpret the action
of A on an arbitrary input vector ‚Éóv as the following three steps:
[‚Éów]Bs =
[A]
Bs
Bs[‚Éóv]Bs = QŒõQ‚àí1[‚Éóv]Bs =
[1]
Bs
BŒª
[Œõ]
BŒª
BŒª
[1]
BŒª
Bs[‚Éóv]Bs
|
{z
}
1
|
{z
}
2
|
{z
}
3
.

7.1
EIGENVALUES AND EIGENVECTORS
265
1. In the Ô¨Årst step, we convert the vector ‚Éóv from the standard basis
to the eigenbasis of A.
2. In the second step, the action of A on vectors expressed with
respect to its eigenbasis corresponds to a multiplication by the
diagonal matrix Œõ.
3. In the third step, we convert the output ‚Éów from the eigenbasis
back to the standard basis.
Another way to interpret these three steps is to say that, deep down
inside, the matrix A is actually the diagonal matrix Œõ. To see the
diagonal form of the matrix, we must express the input vectors with
respect to the eigenbasis:
[‚Éów]BŒª =
[Œõ]
BŒª
BŒª[‚Éóv]BŒª.
It's extremely important you understand the meaning of the equation
A = QŒõQ‚àí1 intuitively in terms of the three-step procedure.
To
help understand the three-step procedure, we'll analyze in detail what
happens when we multiply A by one of its eigenvectors. Let's pick
‚ÉóeŒª1 and verify the equation A‚ÉóeŒª1 = QŒõQ‚àí1‚ÉóeŒª1 = Œª1‚ÉóeŒª1 by following
the vector through the three steps:
[A]
Bs
Bs[‚ÉóeŒª1]Bs = QŒõQ‚àí1[‚ÉóeŒª1]Bs
=
[1]
Bs
BŒª
[Œõ]
BŒª
BŒª
[1]
BŒª
Bs[‚ÉóeŒª1]Bs
|
{z
}
(1,0,...)T
BŒª
|
{z
}
(Œª1,0,...)T
BŒª
|
{z
}
Œª1[‚ÉóeŒª1]Bs
= Œª1[‚ÉóeŒª1]Bs.
In the Ô¨Årst step, we convert the vector [‚ÉóeŒª1]Bs to the eigenbasis and
obtain (1, 0, . . . , 0)T
BŒª. The second step results in (Œª1, 0, . . . , 0)T
BŒª, be-
cause multiplying Œõ by the vector (1, 0, . . . , 0)T
BŒª selects the value in
the Ô¨Årst column of Œõ. For the third step, we convert (Œª1, 0, . . . , 0)T
BŒª =
Œª1(1, 0, . . . , 0)T
BŒª back to the standard basis to obtain Œª1[‚ÉóeŒª1]Bs. Boom!
Invariant properties of matrices
The determinant and the trace of a matrix are strictly functions of the
eigenvalues. The determinant of A is the product of its eigenvalues:
det(A) ‚â°|A| =
Y
i
Œªi = Œª1Œª2 ¬∑ ¬∑ ¬∑ Œªn,

266
THEORETICAL LINEAR ALGEBRA
and the trace is the sum of the eigenvalues:
Tr(A) =
X
i
aii =
X
i
Œªi = Œª1 + Œª2 + ¬∑ ¬∑ ¬∑ Œªn.
The above equations are true because
|A| = |QŒõQ‚àí1| = |Q||Œõ||Q‚àí1| = |Q||Q‚àí1||Œõ| = |Q|
|Q||Œõ| = |Œõ| =
Y
i
Œªi,
and
Tr(A) = Tr(QŒõQ‚àí1) = Tr(ŒõQ‚àí1Q) = Tr(Œõ1) = Tr(Œõ) =
X
i
Œªi.
The Ô¨Årst equation follows from the properties of determinants: |AB| =
|A||B| and |A‚àí1| =
1
|A| (see page 125). The second equation follows
from the cyclic property of the trace operator Tr(ABC) = Tr(BCA)
(see page 125).
In fact, the above calculations are true for any similarity transfor-
mation. Recall that a similarity transformation is a change-of-basis
calculation in which a matrix A gets multiplied by an invertible matrix
P from the left and by the inverse P ‚àí1 from the right: A‚Ä≤ = PAP ‚àí1.
The determinant and the trace of a matrix are invariant properties
under similarity transformations‚Äîthey don't depend on the choice of
basis.
Relation to invertibility
Let's brieÔ¨Çy revisit three of the equivalent conditions we stated in the
invertible matrix theorem. For a matrix A ‚ààRn√ón, the following
statements are equivalent:
‚Ä¢ A is invertible
‚Ä¢ |A| Ã∏= 0
‚Ä¢ The null space contains only the zero vector N(A) = {‚Éó0}
The formula |A| = Œª1Œª2 ¬∑ ¬∑ ¬∑ Œªn reveals why the last two statements
are equivalent. If |A| Ã∏= 0, none of the Œªis are zero (if one of the
eigenvalues is zero, the whole product is zero). We know Œª = 0 is
not an eigenvalue of A, which means there exists no vector ‚Éóv such
that A‚Éóv = 0‚Éóv = ‚Éó0. Therefore, there are no vectors in the null space
N(A). We can also follow this reasoning in the other direction. If the
null space of A is empty, then there is no nonzero vector ‚Éóv such that
A‚Éóv = 0‚Éóv = ‚Éó0, which means Œª = 0 is not an eigenvalue of A, hence the
product Œª1Œª2 ¬∑ ¬∑ ¬∑ Œªn Ã∏= 0.
However, if there exists a nonzero vector ‚Éóv such that A‚Éóv = ‚Éó0, then
A has a non-empty null space, Œª = 0 is an eigenvalue of A, and thus
|A| = Œª1Œª2 ¬∑ ¬∑ ¬∑ Œªn = 0.

7.1
EIGENVALUES AND EIGENVECTORS
267
Eigendecomposition for normal matrices
A matrix A is normal if it satisÔ¨Åes the equation ATA = AAT. All
normal matrices are diagonalizable, and the change-of-basis matrix Q
can be chosen to be an orthogonal matrix O.
The eigenvectors corresponding to diÔ¨Äerent eigenvalues of a normal
matrix are orthogonal. Furthermore, we can choose the eigenvectors
within the same eigenspace to be orthogonal. By collecting the eigen-
vectors from all eigenspaces of the matrix A ‚ààRn√ón, it is possible to
obtain a basis {‚Éóe1,‚Éóe2, . . . ,‚Éóen} of orthogonal eigenvectors:
‚Éóei ¬∑ ‚Éóej =
 ‚à•‚Éóei‚à•2
if i = j,
0
if i Ã∏= j.
Normalizing these vectors gives a set of orthonormal eigenvectors
{ÀÜe1, ÀÜe2, . . . , ÀÜen} that form a basis for the space Rn:
ÀÜei ¬∑ ÀÜej =
 1
if i = j,
0
if i Ã∏= j.
Consider now the matrix O constructed by using these orthonormal
vectors as the columns:
O =
Ô£Æ
Ô£ØÔ£∞
|
|
ÀÜe1
¬∑ ¬∑ ¬∑
ÀÜen
|
|
Ô£π
Ô£∫Ô£ª.
The matrix O is an orthogonal matrix, meaning it satisÔ¨Åes OOT =
I = OTO. In other words, the inverse of O is obtained by taking the
transpose OT. To see how this works, consider the following product:
OTO =
Ô£Æ
Ô£ØÔ£∞
‚Äî
ÀÜe1 ‚Äî
...
‚Äî
ÀÜen ‚Äî
Ô£π
Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£∞
|
|
ÀÜe1
¬∑ ¬∑ ¬∑
ÀÜen
|
|
Ô£π
Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£∞
1
0
0
0
...
0
0
0
1
Ô£π
Ô£∫Ô£ª= 1.
Each of the ones on the diagonal arises from taking the dot product
of a unit-length eigenvector with itself. The oÔ¨Ä-diagonal entries are
zero because the vectors are orthogonal. By deÔ¨Ånition, the inverse
O‚àí1 is the matrix, which gives 1 when multiplied by O, so we have
O‚àí1 = OT.
Using the orthogonal matrix O and its inverse OT, we can write
the eigendecomposition of a matrix A as
A = OŒõO‚àí1 = OŒõOT =
Ô£Æ
Ô£ØÔ£∞
|
|
ÀÜe1
¬∑ ¬∑ ¬∑
ÀÜen
|
|
Ô£π
Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£∞
Œª1
¬∑ ¬∑ ¬∑
0
...
...
0
0
0
Œªn
Ô£π
Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£∞
‚Äî
ÀÜe1 ‚Äî
...
‚Äî
ÀÜen ‚Äî
Ô£π
Ô£∫Ô£ª.

268
THEORETICAL LINEAR ALGEBRA
The key advantage of using an orthogonal matrix O in the diagonal-
ization procedure is that computing its inverse becomes a trivial task:
O‚àí1 = OT. The class of normal matrices enjoy a special status by
virtue of being diagonalizable by orthogonal matrices.
Discussion
Non-diagonalizable matrices
Not all matrices are diagonalizable. For example, the matrix
B =
3
1
0
3

has Œª = 3 as a repeated eigenvalue, but the null space of the matrix
(B ‚àí31) contains only one eigenvector: (1, 0)T. The matrix B has a
single eigenvector in the eigenspace Œª = 3. To describe this situation
using precise mathematical terminology, we say the algebraic multi-
plicity of the eigenvalue Œª = 3 is two, but the geometric multiplicity
of the eigenvalue is one.
The matrix B is a 2 √ó 2 matrix with a single eigenvector. Since
we're one eigenvector short, we can't construct the diagonalizing change-
of-basis matrix Q. We say the matrix has deÔ¨Åcient geometric multi-
plicity, meaning it doesn't have a full set of eigenvectors. Therefore,
B is not diagonalizable.
Matrix power series
One of the most useful concepts of calculus is the idea that func-
tions can be represented as Taylor series. The Taylor series of the
exponential function f(x) = ex is
ex =
‚àû
X
k=0
xk
k! = 1 + x + x2
2 + x3
3! + x4
4! + x5
5! + ¬∑ ¬∑ ¬∑ .
Nothing stops us from using the same Taylor series expression to
deÔ¨Åne the exponential function of a matrix:
eA =
‚àû
X
k=0
Ak
k!
= 1 + A + A2
2 + A3
3! + A4
4! + A5
5! + ¬∑ ¬∑ ¬∑ .
Okay, there is one thing stopping us‚Äîwe need to compute an inÔ¨Ånite
sum of progressively larger matrix powers. Remember how we used
the diagonalization of A = QŒõQ‚àí1 to write A77 as QŒõ77Q‚àí1? We
can apply that trick here to obtain the exponential of a matrix in a
much simpler form:

7.1
EIGENVALUES AND EIGENVECTORS
269
eA =
‚àû
X
k=0
Ak
k! =
‚àû
X
k=0
(QŒõQ‚àí1)k
k!
=
‚àû
X
k=0
Q Œõk Q‚àí1
k!
= Q
" ‚àû
X
k=0
Œõk
k!
#
Q‚àí1
= Q

1 + Œõ + Œõ2
2 + Œõ3
3! + Œõ4
4! + . . .

Q‚àí1
= QeŒõQ‚àí1
=
Ô£Æ
Ô£ØÔ£∞
Q
Ô£π
Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£∞
eŒª1
¬∑ ¬∑ ¬∑
0
...
...
0
0
0
eŒªn
Ô£π
Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£∞
Q‚àí1
Ô£π
Ô£∫Ô£ª.
We can use this approach to deÔ¨Åne "matrix functions" of the form
F : Rn√ón ‚ÜíRn√ón
as Taylor series of matrices. Computing the matrix function F(A)
on an input matrix A = QŒõQ‚àí1 is equivalent to computing the func-
tion f of each of the eigenvalues of the matrix: F(A) = Q f(Œõ) Q‚àí1.
Review
We learned how to decompose matrices in terms of their eigenvalues
and eigenvectors. The fundamental equation is A‚ÉóeŒª = Œª‚ÉóeŒª, where the
vector ‚ÉóeŒª is an eigenvector of the matrix A, and the number Œª is an
eigenvalue of A. The word eigen is the German word for "self."
The characteristic polynomial comes about from a simple manip-
ulation of the eigenvalue equation:
A‚ÉóeŒª
=
Œª‚ÉóeŒª
A‚ÉóeŒª ‚àíŒª‚ÉóeŒª
=
0
(A ‚àíŒª1)‚ÉóeŒª
=
0.
For this equation to be satisÔ¨Åed, the vector ‚ÉóeŒª must be in the null
space of (A ‚àíŒª1). The problem of Ô¨Ånding the eigenvalues reduces to
Ô¨Ånding the values of Œª for which the matrix (A‚àíŒª1) has a non-empty
null space. Recall that a matrix has a non-empty null space if and
only if it is not invertible. The easiest way to check if a matrix is
invertible is to compute the determinant: |A ‚àíŒª1| = 0.

270
THEORETICAL LINEAR ALGEBRA
Because multiple eigenvalues and eigenvectors may satisfy this
equation, we keep a list of eigenvalues (Œª1, Œª2, . . . , Œªn) and correspond-
ing eigenvectors {‚ÉóeŒª1,‚ÉóeŒª2, . . .}. The eigendecomposition of the matrix
is A = QŒõQ‚àí1, where Q is the matrix with eigenvectors as columns
and Œõ contains the eigenvalues on the diagonal.
Applications
Many scientiÔ¨Åc methods use the eigendecomposition of a matrix as a
building block. For instance:
‚Ä¢ In statistics, the principal component analysis technique aims
to uncover the dominant cause of the variation in datasets by
eigendecomposing the covariance matrix‚Äîa matrix computed
from the dataset.
‚Ä¢ Google's original PageRank algorithm for ranking webpages by
"importance" can be explained as the search for an eigenvector
of a matrix.
The matrix contains information about all the
hyperlinks that exist between webpages (see Section 9.3).
‚Ä¢ In quantum mechanics, the energy of a system is described by
the Hamiltonian operator. The eigenvalues of the Hamiltonian
are the possible energy levels the system can have.
Analyzing a matrix in terms of its eigenvalues and its eigenvectors is a
powerful technique to "see inside" a matrix and understand what the
matrix does. In the next section, we'll analyze several diÔ¨Äerent types
of matrices and discuss their properties in terms of their eigenvalues.
Links
[ Good visual examples of eigenvectors from Wikipedia ]
http://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors
Exercises
E7.1 Explain why an n √ó n matrix A can have at most n diÔ¨Äerent
eigenvalues.
E7.2 Check out Problems 1 through 5 at the following URL:
en.wikibooks.org/wiki/Linear_Algebra/Eigenvalues_and_Eigenvectors
en.wikibooks.org/wiki/Linear_Algebra/Eigenvalues_and_Eigenvectors/Solutions
E7.3 Prove that the eigenvectors that correspond to diÔ¨Äerent eigen-
values are linearly independent.
E7.4 Let Œª be an eigenvalue of A and let ‚ÉóeŒª be the corresponding
eigenvector. Show that Œª2 is an eigenvalue of A2.

7.2
SPECIAL TYPES OF MATRICES
271
E7.5 Suppose Œª is an eigenvalue of the invertible matrix A with cor-
responding eigenvector ‚ÉóeŒª.
Show that Œª‚àí1 is an eigenvalue of the
inverse matrix A‚àí1.
E7.6 Find the values of Œ± and Œ≤ so the matrix A =

0
Œ±
1
Œ≤

will have
eigenvalues 1 and 3.
E7.7 Consider the matrix L =
3
2
4
1

. Which of the following vectors
are eigenvectors of L?
 1
1

,

1
‚àí1

,

1
‚àí2

,

2
‚àí1

.
7.2
Special types of matrices
Mathematicians just love to categorize things. Conveniently for us,
they've categorized certain types of matrices. Rather than embarking
on a verbose explanation of the properties of a matrix, such as
I have this matrix A whose rows are perpendicular vectors and
when you multiply any vector by this matrix it doesn't change
the length of the vector but just kind of rotates it---
it's much simpler to refer to the categorization by saying,
Let A be an orthogonal matrix.
Most advanced science textbooks and research papers routinely use
terminology like "diagonal matrix," "symmetric matrix," and "orthog-
onal matrix," so make sure you're familiar with these concepts.
This section will also review and reinforce what we learned about
linear transformations. Recall that we can think of the matrix-vector
product A‚Éóx as applying a linear transformation TA to an input vector
‚Éóx. Therefore, each of the special matrices discussed here also cor-
responds to a special type of linear transformation. Keep this dual
correspondence in mind because we'll use the same terminology to
describe matrices and linear transformations.
Notation
‚Ä¢ Rm√ón: the set of m √ó n matrices
‚Ä¢ A, B, C, O, P, Q, . . .: typical names for matrices
‚Ä¢ aij: the entry in the ith row and jth column of the matrix A
‚Ä¢ AT: the transpose of the matrix A

272
THEORETICAL LINEAR ALGEBRA
‚Ä¢ A‚àí1: the inverse of the matrix A
‚Ä¢ Œª1, Œª2, . . .: the eigenvalues of the matrix A. For each eigenvalue
Œªi there is at least one associated eigenvector ‚ÉóeŒªi that obeys
the equation A‚ÉóeŒªi = Œªi‚ÉóeŒªi. Multiplying the matrix A by its
eigenvectors ‚ÉóeŒªi is the same as scaling ‚ÉóeŒªi by Œªi.
Diagonal matrices
Diagonal matrices contain entries on the diagonal and zeros every-
where else. For example:
A =
Ô£Æ
Ô£∞
a11
0
0
0
a22
0
0
0
a33
Ô£π
Ô£ª.
A diagonal matrix A satisÔ¨Åes aij = 0, if i Ã∏= j. The eigenvalues of a
diagonal matrix are Œªi = aii.
Symmetric matrices
A matrix A is symmetric if and only if
AT = A,
or equivalently if
aij = aji, for all i, j.
The eigenvalues of symmetric matrices are real numbers, and the
eigenvectors can be chosen to be mutually orthogonal.
Given any matrix B ‚ààRm√ón, the product of B with its transpose
BTB is always a symmetric matrix.
Upper triangular matrices
Upper triangular matrices have zero entries below the main diagonal:
Ô£Æ
Ô£∞
a11
a12
a13
0
a22
a23
0
0
a33
Ô£π
Ô£ª,
aij = 0,
if i > j.
For a lower triangular matrix, all the entries above the diagonal are
zeros: aij = 0, if i < j.
Identity matrix
The identity matrix is denoted 1 or 1n ‚ààRn√ón and plays the role
of multiplication by the number 1 for matrices: 1A = A1 = A. The
identity matrix is diagonal with ones on the diagonal:
13 =
Ô£Æ
Ô£∞
1
0
0
0
1
0
0
0
1
Ô£π
Ô£ª.

7.2
SPECIAL TYPES OF MATRICES
273
Any vector ‚Éóv ‚ààR3 is an eigenvector of the identity matrix with eigen-
value Œª = 1.
Orthogonal matrices
A matrix O ‚ààRn√ón is orthogonal if it satisÔ¨Åes OOT = 1 = OTO.
In other words, the inverse of an orthogonal matrix O is obtained by
taking its transpose: O‚àí1 = OT.
Multiplication by an orthogonal matrix preserves lengths.
Consider the matrix-vector product O‚Éóv = ‚Éów. The length of a vector
before the multiplication is ‚à•‚Éóv‚à•=
‚àö
‚Éóv ¬∑ ‚Éóv. The length of a vector after
the multiplication is
‚à•‚Éów‚à•=
‚àö
‚Éów ¬∑ ‚Éów =
q
(O‚Éóv)T(O‚Éóv) =
‚àö
‚ÉóvTOTO‚Éóv.
The second equality follows from the interpretation of the dot product
as a matrix product ‚Éóu ¬∑ ‚Éóv = ‚ÉóuT‚Éóv. The third equality follows from the
properties of matrix transpose (AB)T = BTAT.
When O is an orthogonal matrix, we can substitute OTO = 1 in
the above expression to establish ‚à•‚Éów‚à•=
‚àö
‚ÉóvT1‚Éóv = ‚à•‚Éóv‚à•, which shows
that multiplication by an orthogonal matrix is a length preserving
operation.
The eigenvalues of an orthogonal matrix have unit length, but can
in general be complex numbers Œª = eiŒ∏ ‚ààC. The determinant of an
orthogonal matrix is either one or negative one |O| ‚àà{‚àí1, 1}.
You can visualize orthogonal matrices by thinking of their columns
as a set of vectors that form an orthonormal basis for Rn:
O =
Ô£Æ
Ô£ØÔ£∞
|
|
ÀÜe1
¬∑ ¬∑ ¬∑
ÀÜen
|
|
Ô£π
Ô£∫Ô£ª
such that
ÀÜei ¬∑ ÀÜej =

1
if i = j,
0
if i Ã∏= j.
You can verify the matrix O is orthogonal by computing OTO =
1.
The orthogonal matrix O is a change-of-basis matrix from the
standard basis to the "column basis" {ÀÜe1, ÀÜe2, . . . , ÀÜen}.
Everything stated above about multiplication by an orthogonal
matrix also applies to orthogonal transformations TO : Rn ‚ÜíRn
because of the equivalence O‚Éóv = ‚Éów ‚áîTO(‚Éóv) = ‚Éów.
The set of orthogonal matrices contains three special cases: rota-
tions matrices, reÔ¨Çection matrices, and permutation matrices.
Rotation matrices
A rotation matrix takes the standard basis {ÀÜƒ±, ÀÜÔöæ, ÀÜk} to a rotated ba-
sis {ÀÜe1, ÀÜe2, ÀÜe3}.
Consider an example in R2.
The counterclockwise

274
THEORETICAL LINEAR ALGEBRA
rotation by the angle Œ∏ is given by the matrix
RŒ∏ =
"
cos Œ∏
‚àísin Œ∏
sin Œ∏
cos Œ∏
#
.
The matrix RŒ∏ takes ÀÜƒ± = (1, 0)T to (cos Œ∏, sin Œ∏)T, and ÀÜÔöæ= (0, 1)T to
(‚àísin Œ∏, cos Œ∏)T.
As another example, consider the rotation by the angle Œ∏ around
the x-axis in R3:
Ô£Æ
Ô£∞
1
0
0
0
cos Œ∏
‚àísin Œ∏
0
sin Œ∏
cos Œ∏
Ô£π
Ô£ª.
This rotation is entirely in the yz-plane, so the x-component of a
vector multiplying this matrix remains unchanged.
The determinant of a rotation matrix is equal to one. The eigen-
values of rotation matrices are complex numbers with unit magnitude.
ReÔ¨Çections
If the determinant of an orthogonal matrix O is equal to negative one,
we say it is mirrored orthogonal. For example, the reÔ¨Çection through
the line with direction vector (cos Œ∏, sin Œ∏) is given by:
R =

cos(2Œ∏)
sin(2Œ∏)
sin(2Œ∏)
‚àícos(2Œ∏)

.
A reÔ¨Çection matrix always has at least one eigenvalue equal to negative
one, which corresponds to the direction perpendicular to the axis of
reÔ¨Çection.
Permutation matrices
Permutation matrices are another important class of orthogonal ma-
trices. The action of a permutation matrix is simply to change the
order of the coeÔ¨Écients of a vector. For example, the permutation
œÄ : ÀÜe1 ‚ÜíÀÜe‚Ä≤
1, ÀÜe2 ‚ÜíÀÜe‚Ä≤
3, ÀÜe3 ‚ÜíÀÜe‚Ä≤
2 can be represented as the matrix
MœÄ =
Ô£Æ
Ô£∞
1
0
0
0
0
1
0
1
0
Ô£π
Ô£ª.
An n √ó n permutation matrix contains n ones in n diÔ¨Äerent columns
and zeros everywhere else.
The sign of a permutation corresponds to the determinant det(MœÄ).
We say that permutation œÄ is even if det(MœÄ) = +1 and odd if
det(MœÄ) = ‚àí1.

7.2
SPECIAL TYPES OF MATRICES
275
Positive matrices
A matrix P ‚ààRn√ón is positive semideÔ¨Ånite if
‚ÉóvTP‚Éóv ‚â•0
for all ‚Éóv ‚ààRn.
The eigenvalues of a positive semideÔ¨Ånite matrix are all nonnegative
Œªi ‚â•0.
If instead the matrix P obeys the strict inequality ‚ÉóvTP‚Éóv > 0 for
all ‚Éóv ‚ààRn, we say the matrix P is positive deÔ¨Ånite. The eigenvalues
of positive deÔ¨Ånite matrices are strictly greater than zero Œªi > 0.
Projection matrices
The deÔ¨Åning property of a projection matrix is that it can be applied
multiple times without changing the result:
Œ† = Œ†2 = Œ†3 = Œ†4 = Œ†5 = ¬∑ ¬∑ ¬∑ .
A projection has two eigenvalues: one and zero. The space S that
is left invariant by the projection Œ†S corresponds to the eigenvalue
Œª = 1. The orthogonal complement S‚ä•corresponds to the eigenvalue
Œª = 0 and consists of vectors that get annihilated by Œ†S. The space
S‚ä•is the null space of Œ†S.
Normal matrices
The matrix A = Rn√ón is normal if it obeys ATA = AAT. If A is
normal, it has the following properties:
‚Ä¢ ‚Éóv is an eigenvector of A if and only if ‚Éóv is an eigenvector of AT.
‚Ä¢ For all vectors ‚Éóv and ‚Éów and a normal transformation A, we have
(A‚Éóv)¬∑(A‚Éów) = (A‚Éóv)T(A‚Éów) = ‚ÉóvTATA‚Éów = ‚ÉóvTAAT ‚Éów = (AT‚Éóv)¬∑(AT ‚Éów).
‚Ä¢ The matrix A has a full set of linearly independent eigenvectors.
Eigenvectors corresponding to distinct eigenvalues are orthogo-
nal and eigenvectors from the same eigenspace can be chosen to
be mutually orthogonal.
Every normal matrix is diagonalizable by an orthogonal matrix O.
The eigendecomposition of a normal matrix is written as A = OŒõOT,
where O is orthogonal and Œõ is diagonal.
Orthogonal (OTO = 1) and symmetric (AT = A) matrices are
normal matrices, since OTO = 1 = OOT and ATA = A2 = AAT.

276
THEORETICAL LINEAR ALGEBRA
Discussion
We've deÔ¨Åned several special categories of matrices and described
their properties. You're now equipped with some very precise ter-
minology for describing diÔ¨Äerent types of matrices.
Each of these
special matrices plays a role in certain applications.
This section also highlighted the importance of the eigenvalue de-
scription of matrices. Indeed, we can understand all special matrices
in terms of the constraints imposed on their eigenvalues. The concept
map in Figure 7.1 summarizes the relationships between the diÔ¨Äerent
special types of matrices. The map also refers to unitary and Hermi-
tian matrices, which extend the concepts of orthogonal and symmetric
matrices to describe matrices with complex coeÔ¨Écients.
Figure 7.1: This concept map illustrates the connections and relations
between special types of matrices. We can understand many special types
of matrices in connection with some constraint imposed on their eigenvalues
or their determinants.
This diagram shows only a subset of the many
connections between special matrices. Matrices with complex coeÔ¨Écients
will be discussed in Section 7.7.

7.3
ABSTRACT VECTOR SPACES
277
Exercises
E7.8 Is the matrix

1
2+i
3‚àíi
4

Hermitian?
E7.9 Find the determinants and inverses of the following triangular
matrices:
A =
Ô£Æ
Ô£∞
1
4
56
0
5
14
0
0
3
Ô£π
Ô£ª,
B =
x
0
y
z

,
C =
 1
5
0
0
5

.
7.3
Abstract vector spaces
You can apply your knowledge of vectors more generally to other
vector-like mathematical objects. For example, polynomials behave
similarly to vectors. To add two polynomials P(x) and Q(x), we add
together the coeÔ¨Écients of each power of x‚Äîthe the same way vectors
are added component by component.
In this section, we'll learn how to use the terminology and con-
cepts associated with vectors to study other mathematical objects. In
particular, we'll see that notions such as linear independence, basis,
and dimension can be applied to mathematical objects like matrices,
polynomials, and functions.
DeÔ¨Ånitions
An abstract vector space (V, F, +, ¬∑) consists of four things:
‚Ä¢ A set of vector-like objects V = {u, v, . . .}
‚Ä¢ A Ô¨Åeld F of scalar numbers, usually F = R
‚Ä¢ An addition operation "+" for elements of V that dictates how
to add vectors: u + v
‚Ä¢ A scalar multiplication operation "¬∑" for scaling a vector by an
element of the Ô¨Åeld. Scalar multiplication is usually denoted
implicitly Œ±u (without the dot).
A vector space satisÔ¨Åes the following eight axioms, for all scalars
Œ±, Œ≤ ‚ààF and all vectors u, v, w ‚ààV :
1. u + (v + w) = (u + v) + w (associativity of addition)
2. u + v = v + u (commutativity of addition)
3. There exists a zero vector 0 ‚ààV , such that u + 0 = 0 + u = u
for all u ‚ààV .
4. For every u ‚ààV , there exists an inverse element ‚àíu such that
u + (‚àíu) = u ‚àíu = 0.

278
THEORETICAL LINEAR ALGEBRA
5. Œ±(u + v) = Œ±u + Œ±v (distributivity I)
6. (Œ± + Œ≤)u = Œ±u + Œ≤u (distributivity II)
7. Œ±(Œ≤u) = (Œ±Œ≤)u (associativity of scalar multiplication)
8. There exists a unit scalar 1 such that 1u = u.
If you know anything about vectors, the above properties should be
familiar. Indeed, these are the standard properties for the vector space
Rn, where the Ô¨Åeld F is R, and for which standard vector addition
and scalar multiplication operations apply.
Theory
Believe it or not, we're actually done with all the theory for this
section.
Move along folks, there's nothing more to see here aside
from the deÔ¨Ånitions above‚Äîwhich are restatements of the properties
of vector addition and vector scaling that you've already seen before.
The only thing left to do is illustrate these concepts through some
examples.
Examples
Matrices, polynomials, and functions are vector-like math objects.
The following examples demonstrate how we can treat these math
objects as abstract vector spaces (V, F, +, ¬∑).
Matrices
Consider the vector space of m √ó n matrices over the real numbers
Rm√ón. The addition operation for two matrices A, B ‚ààRm√ón is the
usual rule of matrix addition: (A + B)ij = aij + bij.
This vector space is mn-dimensional, which can be seen by con-
structing a basis for the space. The standard basis for Rm√ón consists
of matrices with zero entries everywhere except for a single 1 in the
ith row and the jth column. Any matrix A ‚ààRm√ón can be written as
a linear combination of the matrices in the standard basis.
Example
The standard basis Bs for the vector space R2√ó2 is
e1 =
1
0
0
0

,
e2 =
0
1
0
0

,
e3 =
0
0
1
0

,
e4 =
0
0
0
1

.

7.3
ABSTRACT VECTOR SPACES
279
Any matrix A ‚ààR2√ó2 can be written as a linear combination of the
elements of Bs:
A =
a11
a12
a21
a22

= a11
1
0
0
0

+ a12
0
1
0
0

+ a21
0
0
1
0

+ a22
0
0
0
1

= a11e1 + a12e2 + a21e3 + a22e4.
In other words, A can be expressed as a vector of coeÔ¨Écients with
respect to the basis Bs: A = (a11, a12, a21, a22)Bs.
The abstract concept of a matrix A ‚ààR2√ó2 can be expressed as
two equivalent representations. We can think of A either as an array of
coeÔ¨Écients with two columns and two rows, or as a four-dimensional
vector of coeÔ¨Écients with respect to the basis Bs:
a11
a12
a21
a22

‚â°A ‚â°(a11, a12, a21, a22)Bs.
We've arrived at a major knowledge buzz milestone: matrices are
vectors! In precise mathematical terms, we just demonstrated the
existence of an isomorphism between the set of 2 √ó 2 matrices and
the set of four-dimensional vectors. We can add, subtract, and scale
2 √ó 2 matrices in their R4 representations. In the following exercises,
we'll see how to compute the matrix trace operation Tr(A) in terms
of the vector representation.
Symmetric 2x2 matrices
DeÔ¨Åne the vector space consisting of 2 √ó 2 symmetric matrices
S(2, 2) ‚â°{A ‚ààR2√ó2 | A = AT }
in combination with the usual matrix addition and scalar multiplica-
tion operations. We obtain an explicit basis for this space as follows:
v1 =

1
0
0
0

,
v2 =
0
1
1
0

,
v3 =
0
0
0
1

.
Any element of the vector space S ‚ààS(2, 2) can be written as a linear
combination of the basis elements:
S =
a
b
b
c

= a
1
0
0
0

+ b
0
1
1
0

+ c
0
0
0
1

= av1 + bv2 + cv3.
Since there are three vectors in a basis for S(2, 2), the vector space
S(2, 2) is three-dimensional.
Note how we count the dimensions in this case. The space of 2√ó2
matrices is four-dimensional in general, but imposing the symmetry
constraint a12 = a21 eliminates one parameter, so we're left with a
three-dimensional space.

280
THEORETICAL LINEAR ALGEBRA
Polynomials of degree n
DeÔ¨Åne the vector space Pn(t) of polynomials with real coeÔ¨Écients
and degree less than or equal to n. The "vectors" in this space are
polynomials of the form
p = a0 + a1x + a2x2 + ¬∑ ¬∑ ¬∑ + anxn,
where a0, a1, . . . , an are the coeÔ¨Écients of the polynomial p.
The addition of vectors p, q ‚ààPn(t) is performed component-wise:
p + q = (a0 + a1x + ¬∑ ¬∑ ¬∑ + anxn) + (b0 + b1x + ¬∑ ¬∑ ¬∑ + bnxn)
= (a0 + b0) + (a1 + b1)x + ¬∑ ¬∑ ¬∑ + (an + bn)xn.
Similarly, scalar multiplication acts as you would expect:
Œ±p = Œ± ¬∑ (a0 + a1x + . . . anxn) = (Œ±a0) + (Œ±a1)x + . . . (Œ±an)xn.
The space Pn(x) is (n + 1)-dimensional since each "vector" in this
space has n + 1 coeÔ¨Écients.
Functions
Another interesting vector space is the set of functions f : R ‚ÜíR
in combination with the point-wise addition and scaler multiplication
operations:
f + g = (f + g)(x) = f(x) + g(x),
Œ±f = (Œ±f)(x) = Œ±f(x).
The space of functions is inÔ¨Ånite-dimensional.
Discussion
We've talked about bases, components, and dimensions of abstract
vector spaces. Indeed, these notions are well-deÔ¨Åned for any vector-
like object. Though this section only discussed vector spaces with real
coeÔ¨Écients, we can apply the same techniques to vectors with coeÔ¨É-
cients from any Ô¨Åeld. The notion of a Ô¨Åeld describes any number-like
object for which the operations of addition, subtraction, multiplica-
tion, and division are deÔ¨Åned. An example of another Ô¨Åeld is the set
of complex numbers C. We'll discuss the linear algebra of vectors and
matrices with complex coeÔ¨Écients in Section 7.7.
In the next section, we'll deÔ¨Åne an abstract inner product operation
and use this deÔ¨Ånition to discuss concepts like orthogonality, length,
and distance in abstract vector spaces.

7.4
ABSTRACT INNER PRODUCT SPACES
281
Links
[ Further discussion and examples on Wikipedia ]
http://en.wikipedia.org/wiki/Vector_space
[ Examples of vector spaces ]
http://wikibooks.org/wiki/Linear_Algebra/Definition_and_Examples_of_Vector_Spaces
Exercises
E7.10 Consider an arbitrary matrix A ‚ààR2√ó2 and its representation
as a vector of coeÔ¨Écients with respect to Bs: ‚ÉóA = (a11, a12, a21, a22)Bs.
Suppose we want to compute the matrix trace operation in terms of
the vector dot product. What vector ‚Éóv ‚ààR4 makes this equation true
Tr(A) = ‚Éóv ¬∑ ‚ÉóA?
E7.11 Repeat the previous question, but now think of ‚ÉóA as a 4 √ó 1
matrix.
Find the matrix V that implements the trace operation:
Tr(A) = V ‚ÉóA. Assume the standard matrix-matrix product is used.
E7.12 Find the dimension of the vector space of functions that satisfy
f ‚Ä≤(t) + f(t) = 0.
Hint: Which function is equal to a multiple of its own derivative?
E7.13 Can every polynomial of degree at most 2 be written in the
form Œ±(1) + Œ≤(x ‚àí1) + Œ≥(x ‚àí1)2?
Hint: Try to express an arbitrary polynomial in this form.
7.4
Abstract inner product spaces
An inner product space is an abstract vector space (V, R, +, ¬∑) for
which we deÔ¨Åne an abstract inner product operation that takes pairs
of vectors as inputs and produces numbers as outputs:
‚ü®¬∑, ¬∑‚ü©: V √ó V ‚ÜíR.
We can use any inner product operation, as long as it satisÔ¨Åes the
following criteria for all u, v, v1, v2 ‚ààV and Œ±, Œ≤ ‚ààR. The inner
product operation must be:
‚Ä¢ Symmetric: ‚ü®u, v‚ü©= ‚ü®v, u‚ü©
‚Ä¢ Linear: ‚ü®u, Œ±v1 + Œ≤v2‚ü©= Œ±‚ü®u, v1‚ü©+ Œ≤‚ü®u, v2‚ü©
‚Ä¢ Positive semideÔ¨Ånite: ‚ü®u, u‚ü©‚â•0 for all u ‚ààV with ‚ü®u, u‚ü©= 0 if
and only if u = 0

282
THEORETICAL LINEAR ALGEBRA
These criteria are inspired by the properties of the standard inner
product (dot product) for vectors in Rn:
‚ü®‚Éóu,‚Éóv‚ü©‚â°‚Éóu ¬∑ ‚Éóv =
n
X
i=1
uivi = ‚ÉóuT‚Éóv.
In this section, the idea of dot product is generalized to abstract vec-
tors u, v ‚ààV by deÔ¨Åning an inner product operation ‚ü®u, v‚ü©appropri-
ate for the elements of V . We'll deÔ¨Åne an inner product operation for
matrices ‚ü®A, B‚ü©, polynomials ‚ü®p, q‚ü©, and functions ‚ü®f, g‚ü©. This inner
product will allow us to talk about orthogonality between abstract
vectors,
u and v are orthogonal
‚áî
‚ü®u, v‚ü©= 0,
the length of an abstract vector,
‚à•u‚à•‚â°
p
‚ü®u, u‚ü©,
and the distance between two abstract vectors,
d(u, v) ‚â°‚à•u ‚àív‚à•=
p
‚ü®(u ‚àív), (u ‚àív)‚ü©.
Let's get started.
DeÔ¨Ånitions
We'll work with vectors from an abstract vector space (V, R, +, ¬∑)
where:
‚Ä¢ V is the set of vectors in the vector space
‚Ä¢ R is the Ô¨Åeld of real numbers. The coeÔ¨Écients of the generalized
vectors are taken from this Ô¨Åeld.
‚Ä¢ + is the addition operation deÔ¨Åned for elements of V
‚Ä¢ ¬∑ is the scalar multiplication operation between an element of
the Ô¨Åeld Œ± ‚ààR and a vector u ‚ààV
We deÔ¨Åne a new operation called abstract inner product for that space:
‚ü®¬∑, ¬∑‚ü©: V √ó V ‚ÜíR.
The abstract inner product takes as inputs two vectors u, v ‚ààV and
produces real numbers as outputs: ‚ü®u, v‚ü©‚ààR.
We deÔ¨Åne the following related quantities in terms of the inner
product operation:
‚Ä¢ ‚à•u‚à•‚â°
p
‚ü®u, u‚ü©: the norm or length of an abstract vector
‚Ä¢ d(u, v) ‚â°‚à•u ‚àív‚à•: the distance between two vectors

7.4
ABSTRACT INNER PRODUCT SPACES
283
Orthogonality
Recall that two vectors ‚Éóu,‚Éóv ‚ààRn are said to be orthogonal if their
dot product is zero. This follows from the geometric interpretation of
the dot product:
‚Éóu ¬∑ ‚Éóv = ‚à•‚Éóu‚à•‚à•‚Éóv‚à•cos Œ∏,
where Œ∏ is the angle between ‚Éóu and ‚Éóv. Orthogonal means "at right
angle with." Indeed, if ‚Éóu ¬∑ ‚Éóv = 0, the angle between ‚Éóu and ‚Éóv must be
90‚ó¶or 270‚ó¶, since cos Œ∏ = 0 only for these two angles.
In analogy with the regular dot product, we deÔ¨Åne the notion of
orthogonality between abstract vectors in terms of the abstract inner
product:
u and v are orthogonal
‚áî
‚ü®u, v‚ü©= 0.
Translating the geometrical intuition of "at 90‚ó¶or 270‚ó¶angle with"
might not be possible for certain abstract vector spaces. For instance,
what is the "angle" between two polynomials? Nevertheless, the fun-
damental notion of "perpendicular to" exists in all abstract inner prod-
uct vector spaces.
Norm
Every deÔ¨Ånition of an inner product for an abstract vector space
(V, R, +, ¬∑) induces a norm for that vector space:
‚à•.‚à•: V ‚ÜíR.
The norm is deÔ¨Åned in terms of the inner product. The norm of a
vector is the square root of the inner product of the vector with itself:
‚à•u‚à•=
p
‚ü®u, u‚ü©.
The norm of a vector corresponds, in some sense, to the "length" of
the vector. All norms must satisfy the following criteria:
‚Ä¢ ‚à•v‚à•‚â•0 with equality if and only if v = 0
‚Ä¢ ‚à•kv‚à•= k‚à•v‚à•
‚Ä¢ The triangle inequality:
‚à•u + v‚à•‚â§‚à•u‚à•+ ‚à•v‚à•
‚Ä¢ Cauchy-Schwarz inequality:
|‚ü®x, y‚ü©| ‚â§‚à•x‚à•‚à•y‚à•,
with equality if and only if x and y are linearly dependent
Norms deÔ¨Åned in terms of a valid inner product automatically satisfy
these criteria.

284
THEORETICAL LINEAR ALGEBRA
Distance
The distance between two points p and q in Rn is equal to the norm of
the vector that goes from p to q: d(p, q) = ‚à•q ‚àíp‚à•. We can similarly
deÔ¨Åne a distance function between pairs of vectors in an abstract
vector space V :
d : V √ó V ‚ÜíR.
The distance between two abstract vectors is equal to the norm of
their diÔ¨Äerence:
d(u, v) ‚â°‚à•u ‚àív‚à•=
p
‚ü®(u ‚àív), (u ‚àív)‚ü©.
Distances deÔ¨Åned in terms of a valid norm obey the following criteria:
‚Ä¢ d(u, v) = d(v, u)
‚Ä¢ d(u, v) ‚â•0 with equality if and only if u = v
Examples
Let's deÔ¨Åne some inner product functions for the aforementioned ab-
stract vector spaces.
Matrix inner product
The Hilbert-Schmidt inner product for real matrices is deÔ¨Åned in
terms of the matrix transpose, matrix product, and matrix trace op-
erations:
‚ü®A, B‚ü©HS = Tr
 ATB

.
We can use this inner product to talk about orthogonality properties
of matrices. In the last section we deÔ¨Åned the set of 2 √ó 2 symmetric
matrices
S(2, 2) = {A ‚ààR2√ó2 | A = AT },
and gave an explicit basis for this space:
v1 =
1
0
0
0

,
v2 =
0
1
1
0

,
v3 =
0
0
0
1

.
It's easy to show that these vectors are all mutually orthogonal with
respect to the Hilbert-Schmidt inner product ‚ü®¬∑, ¬∑‚ü©HS:
‚ü®v1, v2‚ü©HS = 0,
‚ü®v1, v3‚ü©HS = 0,
‚ü®v2, v3‚ü©HS = 0.
Verify these three equations by computing each inner product. Try
this by hand on a piece of paper, like right now.
The three inner product calculations of the last equation indicate
that the set {v1, v2, v3} forms an orthogonal basis for the vector space
S(2, 2) with respect to the inner product ‚ü®¬∑, ¬∑‚ü©HS.

7.4
ABSTRACT INNER PRODUCT SPACES
285
Hilbert-Schmidt norm
The Hilbert-Schmidt inner product induces the Hilbert-Schmidt norm:
||A||HS ‚â°
p
‚ü®A, A‚ü©HS =
q
Tr(ATA) =
Ô£Æ
Ô£∞
n
X
i,j=1
|aij|2
Ô£π
Ô£ª
1
2
.
We can use this norm to describe the "length" of a matrix. Contin-
uing with the above example, we can obtain an orthonormal basis
{ÀÜv1, ÀÜv2, ÀÜv3} for S(2, 2) as follows:
ÀÜv1 = v1,
ÀÜv2 =
v2
‚à•v2‚à•HS
=
1
‚àö
2v2,
ÀÜv3 = v3.
Verify that ‚à•ÀÜv2‚à•HS = 1.
Function inner product
Consider two functions f = f(t) and g = g(t), and deÔ¨Åne their inner
product as follows:
‚ü®f, g‚ü©‚â°
Z ‚àû
‚àí‚àû
f(t)g(t) dt.
This formula is the continuous-variable version of the inner product
formula for vectors ‚Éóu ¬∑ ‚Éóv = P
i uivi. Instead of a summation, we have
an integral; otherwise the idea is the same: we measure the overlap
between f and g. The integral passes along the real line from ‚àí‚àû
until ‚àûlike a zipper that multiplies f(t) times g(t) at each point.
Example
Consider the function inner product on the interval [‚àí1, 1]
as deÔ¨Åned by the formula:
‚ü®f, g‚ü©=
Z 1
‚àí1
f(t)g(t) dt.
Verify that the following polynomials, known as the Legendre polyno-
mials Pn(x), are mutually orthogonal with respect to the above inner
product:
P0(x) = 1,
P1(x) = x,
P2(x) = 1
2(3x2 ‚àí1),
P3(x) = 1
2(5x3 ‚àí3x).

286
THEORETICAL LINEAR ALGEBRA
Generalized dot product
We can think of the regular dot product for vectors as the following
vector-matrix-vector product:
‚Éóu ¬∑ ‚Éóv = ‚ÉóuT‚Éóv = ‚ÉóuT1‚Éóv.
More generally, we can insert any symmetric, positive semideÔ¨Ånite
matrix M between the vectors and obtain a valid inner product:
‚ü®‚Éóx, ‚Éóy‚ü©M ‚â°‚ÉóxTM‚Éóy.
The matrix M is called the metric for this inner product, and it
encodes the relative contributions of the diÔ¨Äerent components of the
vectors to the inner product.
The requirement that M be symmetric stems from the symmetric
requirement for inner products: ‚ü®u, v‚ü©= ‚ü®v, u‚ü©. The requirement that
the matrix be positive semideÔ¨Ånite comes from the positive semidef-
inite requirement for inner products: ‚ü®u, u‚ü©= ‚ÉóuTM‚Éóu ‚â•0, for all
u ‚ààV .
We can always obtain a symmetric and positive semideÔ¨Ånite ma-
trix M by setting M = ATA for some matrix A. To understand why
we might want to construct M in this way, recall that each matrix
A implements some linear transformation TA(‚Éóu) = A‚Éóu.
An inner
product ‚ü®‚Éóu,‚Éóv‚ü©M can be interpreted as the regular dot product in the
output space of TA:
‚ü®‚Éóu,‚Éóv‚ü©M = ‚ÉóuTM‚Éóv = ‚ÉóuTATA‚Éóv = (A‚Éóu)T(A‚Éóv) = TA(‚Éóu) ¬∑ TA(‚Éóv).
This is a very powerful idea with many applications.
Example
Consider the task of computing the similarity between
two documents ‚Éówi and ‚Éówj. Each document is represented as a vector
of word counts in Rn.
The Ô¨Årst component of ‚Éówi represents how
many times the word aardvark appears in document i, the second
component is the count of abacus, and so on for the n words in the
dictionary.
Intuitively, two vectors are similar if their inner product is large.
We can compute the similarity between documents ‚Éówi and ‚Éówj by
calculating their inner product, normalized by their norms:
sim1(‚Éówi, ‚Éówj) ‚â°‚ü®‚Éówi, ‚Éówj‚ü©
‚à•‚Éówi‚à•‚à•‚Éówj‚à•=
‚ÉówT
i 1‚Éówj
‚à•‚Éówi‚à•‚à•‚Éówj‚à•.
This expression is called the cosine similarity between vectors be-
cause it corresponds to the cosine of the angle between the vectors

7.4
ABSTRACT INNER PRODUCT SPACES
287
‚Éówi and ‚Éówj. To make things more concrete, suppose we use a vocabu-
lary of 40 000 words to represent documents, making the word count
vectors for each document 40 000-dimensional vectors. That's a lot
of dimensions! Using the function sim1 to compute similarities be-
tween documents won't work too well because it's pretty rare that
two vectors in a 40 000-dimensional space point in the same direction.
Consider now the linear transformation T : Rn ‚ÜíRm that trans-
forms the word vectors into "topic" vectors, which are m-dimensional
vectors with m < n. Each dimension in "topic space" corresponds
to some topic: art, literature, science, etc., and we can assume
there are a small number of topics. Using T, we can obtain the topic
representation of each document ‚ÉóŒ∏i = T(‚Éówi), where the components
of ‚ÉóŒ∏i ‚ààRm tell us the proportions of the diÔ¨Äerent topics that appear
in document i.
An improved similarity measure for documents is to compute the
cosine similarity between their vector representations in topics space:
simT (‚Éówi, ‚Éówj) ‚â°‚ü®‚ÉóŒ∏i, ‚ÉóŒ∏j‚ü©
‚à•‚ÉóŒ∏i‚à•‚à•‚ÉóŒ∏j‚à•
= ‚ü®T(‚Éówi), T(‚Éówj)‚ü©
‚à•T(‚Éówi)‚à•‚à•T(‚Éówj)‚à•.
In words, simT measures the cosine similarity between documents
based on the topics they contain. Assuming we pick a small number
of topics, say m = 40, the similarity metric simT will lead to improved
similarity computations because the inner product will be computed
in a smaller vector space.
Let's focus on the numerator of the expression for simT . Recall
that every linear transformation T can be represented as a matrix-
vector product T(‚Éówi) = AT ‚Éówi, for some matrix AT . We can write
‚ü®T(‚Éówi), T(‚Éówj)‚ü©= ‚ü®AT ‚Éówi, AT ‚Éówj‚ü©= ‚ÉówT
i AT
T AT
| {z }
M
‚Éówj ‚â°‚ü®‚Éówi, ‚Éówj‚ü©M,
which shows the inner product in topic-space can be interpreted as a
generalized inner product for word-vectors with metric M = AT
T AT .
The notion of a generalized inner product with metric matrix M
deÔ¨Åned via ‚ü®‚Éóu,‚Éóv‚ü©M ‚â°‚ÉóuTM‚Éóv is an important concept that appears in
many advanced math topics like analysis and diÔ¨Äerential geometry.
Metrics also shows up in physics: when Einstein talks about how
masses cause space to become "curved," he's really talking about the
curvature of the metric of space-time.
Valid and invalid inner product spaces
A standard question profs like to ask on exams is to check whether
a given vector space and some weird deÔ¨Ånition of an inner product

288
THEORETICAL LINEAR ALGEBRA
operation form a valid inner product space. Recall that any operation
can be used as the inner product, as long as it satisÔ¨Åes the symmetry,
linearity, and positive semideÔ¨Ånite criteria. To prove an inner product
operations is valid, you must show it satisÔ¨Åes the three criteria.
Alternatively, you can prove the vector space (V, R, +, ¬∑) with inner
product ‚ü®u, v‚ü©is not a valid inner product space if you Ô¨Ånd an example
of one of more u, v ‚ààV which do not satisfy one of the axioms.
Discussion
This has been another one of those sections where we learn no new
linear algebra, but simply generalize notions we already know about
standard vectors ‚Éóv ‚ààRn to abstract vector-like objects v ‚ààV . You
can now talk about orthogonality and norms for matrices, polynomi-
als, and functions.
Exercises
7.5
Gram-Schmidt orthogonalization
Recall what we learned in Section 5.3 about the three "quality grades"
for bases: orthonormal, orthogonal, and generic, with orthonormal
bases being the most desirable of the three. In this section, we'll learn
how to take a generic basis for an n-dimensional space V ‚Äîthat is, a
set of n linearly independent vectors {v1, v2, . . . , vn}‚Äîand transform
it into an orthonormal basis {ÀÜe1, ÀÜe2, . . . , ÀÜen} that obeys:
‚ü®ÀÜei, ÀÜej‚ü©=
 1
if i = j,
0
if i Ã∏= j.
This procedure is known as Gram-Schmidt orthogonalization and is
based on a sequence of projection and subtraction operations.
The discussion and procedures in this section are described in
terms of vectors in an abstract inner product space. Thus, the Gram-
Schmidt algorithm applies to ordinary vectors ‚Éóv ‚ààRn, matrices A ‚àà
Rm√ón, and polynomials p ‚ààPn(x). Indeed, we can talk about orthog-
onality for any set of mathematical objects for which we've deÔ¨Åned
an inner product operation.
DeÔ¨Ånitions
‚Ä¢ V : an n-dimensional vector space
‚Ä¢ {v1, v2, . . . , vn}: a generic basis for the space V
‚Ä¢ {e1, e2, . . . , en}: an orthogonal basis for V . Each vector ei is
orthogonal to all other vectors: ei ¬∑ ej = 0, for i Ã∏= j.

7.5
GRAM-SCHMIDT ORTHOGONALIZATION
289
‚Ä¢ {ÀÜe1, ÀÜe2, . . . , ÀÜen}: an orthonormal basis for V . An orthonormal
basis is an orthogonal basis of unit-length vectors.
We assume the vector space V is equipped with an inner product
operation:
‚ü®¬∑, ¬∑‚ü©: V √ó V ‚ÜíR.
The following operations are deÔ¨Åned in terms of the inner product:
‚Ä¢ The length of a vector ‚à•v‚à•= ‚ü®v, v‚ü©
‚Ä¢ The projection operation. The projection of the vector u onto
the subspace spanned by the vector e is denoted Œ†e(u) and
computed using
Œ†e(u) = ‚ü®u, e‚ü©
‚à•e‚à•2 e.
‚Ä¢ The projection complement of the projection Œ†e(u) is the vec-
tor w that we must add to Œ†e(u) to recover the original vector u:
u = Œ†e(u) + w
‚áí
w = u ‚àíŒ†e(u).
The vector w is orthogonal to the vector e, ‚ü®w, e‚ü©= 0.
Orthonormal bases are nice
Recall that a basis for an n-dimensional vector space V is any set of
n linearly independent vectors in V . The choice of basis is a big deal
because we express the components of vectors and matrices with re-
spect to the basis. From a theoretical standpoint, all bases are equally
good; but from a practical standpoint, orthogonal and orthonormal
bases are much easier to work with.
An orthonormal basis B = {ÀÜe1, ÀÜe2, ÀÜe3} is the most useful kind of
basis because the coeÔ¨Écients c1, c2, and c3 of a vector c = (c1, c2, c3)B
with respect to B are obtained using three independent inner-product
calculations:
c1 = ‚ü®c, ÀÜe1‚ü©,
c2 = ‚ü®c, ÀÜe2‚ü©,
c3 = ‚ü®c, ÀÜe3‚ü©.
We can express any vector v as follows:
v = ‚ü®v, ÀÜe1‚ü©ÀÜe1 + ‚ü®v, ÀÜe2‚ü©ÀÜe2 + ‚ü®v, ÀÜe3‚ü©ÀÜe3.
This formula is a generalization of the usual formula for coeÔ¨Écients
with respect to the standard basis {ÀÜƒ±, ÀÜÔöæ, ÀÜk}: ‚Éóv = (‚Éóv¬∑ÀÜƒ±)ÀÜƒ±+(‚Éóv¬∑ÀÜÔöæ)ÀÜÔöæ+(‚Éóv¬∑ÀÜk)ÀÜk.

290
THEORETICAL LINEAR ALGEBRA
Orthogonalization
The "best" kind of basis for computational purposes is an orthonormal
basis like {ÀÜe1, ÀÜe2, . . . , ÀÜen}. How can we upgrade some general set of
n linearly independent vectors {v1, v2, . . . , vn} into an orthonormal
basis {ÀÜe1, ÀÜe2, . . . , ÀÜen}? The vectors {ÀÜei} must be linear combinations
of the vectors {vi}, but which linear combinations should we choose?
Note the vector space V remains the same:
span{v1, v2, . . . , vn} = V = span{ÀÜe1, ÀÜe2, . . . , ÀÜen}.
However, the basis {ÀÜe1, ÀÜe2, . . . , ÀÜem} is easier to work with.
The technical term for distilling a high-quality orthonormal basis
from a low-quality basis of arbitrary vectors is called orthogonaliza-
tion. Most of the work lies in obtaining the set of vectors {ei} that
are orthogonal to each other:
‚ü®ei, ej‚ü©= 0,
for all i Ã∏= j.
To convert an orthogonal basis into an orthonormal basis, divide each
vector by its length: ÀÜei =
ei
‚à•ei‚à•.
It's now time to see how orthogonalization works; get ready for
some Gram-Schmidting.
Gram-Schmidt orthogonalization procedure
The Gram-Schmidt orthogonalization procedure converts a basis of
arbitrary vectors {v1, . . . , vn} into an orthonormal basis {ÀÜe1, . . . , ÀÜen}.
The main idea is to take the vectors vi one at a time, each time
deÔ¨Åning a new vector ei as the orthogonal complement of vi to all
the previously chosen vectors e1, e2, . . ., ei‚àí1. Recall we can use the
projection formula Œ†ÀÜe(v) ‚â°‚ü®ÀÜe, v‚ü©ÀÜe to compute the component of any
vector v in the direction ÀÜe.
The orthogonalization algorithm consists of n steps:
e1 = v1
ÀÜe1 =
v1
‚à•v1‚à•,
e2 = v2 ‚àíŒ†ÀÜe1(v2),
ÀÜe2 =
e2
‚à•e2‚à•,
e3 = v3 ‚àíŒ†ÀÜe1(v3) ‚àíŒ†ÀÜe2(v3),
ÀÜe3 =
e3
‚à•e3‚à•,
e4 = v4 ‚àíŒ†ÀÜe1(v4) ‚àíŒ†ÀÜe2(v4), ‚àíŒ†ÀÜe3(v4),
ÀÜe4 =
e4
‚à•e4‚à•,
...
...
en = vn ‚àí
n‚àí1
X
i=1
Œ†ÀÜei(vn),
ÀÜen =
en
‚à•en‚à•.

7.5
GRAM-SCHMIDT ORTHOGONALIZATION
291
In the jth step of the procedure, we compute a vector ej by starting
from vj and subtracting all the projections of vj onto the previous
vectors ei for all i < j. In other words, ej is the part of vj that is
orthogonal to all the vectors e1, e2, . . . , ej‚àí1.
This procedure is known as orthogonalization because it splits the
vector space V into orthogonal subspaces V1, V2, . . . , Vn:
Vj = span
(
v ‚ààV | v =
j
X
i=1
Œ±ivi
)
\ span
(
v ‚ààV | v =
j‚àí1
X
i=1
Œ±ivi
)
.
Recall that the symbol \ denotes the set minus operation. The set
A \ B consists of all elements that are in set A but not in set B.
Observe the subspaces V1, V2, . . . , Vn are, by construction, mu-
tually orthogonal.
Given any vector u ‚ààVi and another vector
v ‚ààVj, j Ã∏= i, then u ¬∑ v = 0.
The vector space V is the sum of these subspaces:
V = V1 ‚äïV2 ‚äïV3 ‚äï¬∑ ¬∑ ¬∑ ‚äïVn.
The notation ‚äïmeans orthogonal sum. Each space Vj is spanned by
a vector ej which is orthogonal to all the Vis, for i < j.
Discussion
The main point you must remember about orthogonalization is simply
that it can be done.
Any "low-quality" basis (a set of n linearly
independent vectors {v1, v2, . . . , vn} in an n-dimensional space) can
be converted into a "high quality" orthonormal basis {ÀÜe1, ÀÜe2, . . . , ÀÜen}
using the Gram-Schmidt procedure.
You can also perceive the Gram-Schmidt procedure as a technique
for creating structure in an arbitrary vector space V .
The initial
description V = span{v1, v2, . . . , vn} lacks structure. It's just some
amorphous vector space spanned by an arbitrary set of vectors. After
the orthogonalization procedure, we obtain the equivalent description
V = V1‚äïV2‚äïV3‚äï¬∑ ¬∑ ¬∑‚äïVn that shows V is the direct sum of orthogonal
subspaces.
In the next section, we'll continue on our mathematical quest for
structure by discussing procedures that uncover hidden structure in
matrices. For example, when phrased in terms of matrices, the Gram-
Schmidt orthogonalization procedure is called QR decomposition‚Äî
stay tuned! And meanwhile, try the following exercises.
Exercises
E7.14 Convert the vectors ‚Éóv1 = (4, 2) and ‚Éóv2 = (1, 3) into an orthog-
onal basis for R2.

292
THEORETICAL LINEAR ALGEBRA
E7.15 Perform the Gram-Schmidt orthogonalization procedure on
the vectors ‚Éóv1 = (1, 1, 0), ‚Éóv2 = (1, 0, 1), and ‚Éóv3 = (0, 1, 1) to obtain an
orthonormal basis {ÀÜe1, ÀÜe2, ÀÜe3}.
E7.16 Consider the vector space P2(x) of polynomials of degree at
most two in combination with the inner product ‚ü®f, g‚ü©‚â°
R 1
‚àí1 f(x)g(x) dx.
The functions f1(x) = 1, f2(x) = x, and f3(x) = x2 are linearly inde-
pendent and form a basis for P2(x). Find an orthonormal basis for
P2(x).
7.6
Matrix decompositions
It's often useful to express a given matrix as the product of other,
simpler matrices. These matrix decompositions (also known as fac-
torizations) can help us understand the structure of matrices by re-
vealing their constituents. In this section, we'll discuss various matrix
factorizations and specify the types of matrices they apply to.
Most of the material covered here is not usually included in a Ô¨Årst-
year linear algebra course. Nevertheless, knowing about the diÔ¨Äerent
matrix decompositions is quite helpful, as many linear algebra appli-
cations depend on these decompositions. Got that? Good. Onward!
Eigendecomposition
The eigendecomposition breaks a matrix into its eigenvalues and eigen-
vectors. The eigenbasis, when it exists, is the most "natural" basis for
looking at a matrix. A diagonalizable matrix A can be written as
A = QŒõQ‚àí1,
where Q is a matrix whose columns are eigenvectors of A, and Œõ is a
diagonal matrix containing the eigenvalues of A.
The eigendecomposition of a matrix is a similarity transformation
(a change of basis) where the new basis matrix consists of eigenvectors
of the matrix.
If A is positive semideÔ¨Ånite then its eigenvalues are nonnegative.
If the matrix A is symmetric then its eigenvalues are real numbers.
When the matrix A is normal, meaning it satisÔ¨Åes AAT = ATA,
we can choose Q to be an orthogonal matrix O that satisÔ¨Åes OTO = 1.
Calculating the inverse of an orthogonal matrix is easy: O‚àí1 = OT.
The eigendecomposition for normal matrices is A = OŒõOT.
Singular value decomposition
We can generalize the concepts of eigenvalues and eigenvectors to non-
square matrices. Consider a matrix A ‚ààRm√ón. Since the matrix A is

7.6
MATRIX DECOMPOSITIONS
293
not a square matrix, we can't use the standard eigendecomposition.
However, there is a trick for turning a non-square matrix into a square
matrix while preserving some of its properties: multiply the matrix
by its transpose. The matrix AAT ‚ààRn√ón has the same column space
as the matrix A. Similarly, ATA ‚ààRm√óm has the same row space as
the matrix A.
The singular value decomposition breaks a matrix into the product
of three matrices: an m √ó m orthogonal matrix U which consists of
left singular vectors, an m √ó n matrix Œ£ with the singular values œÉi
on the diagonal, and an n √ó n orthogonal matrix V T of right singular
vectors:
A =
Ô£Æ
Ô£ØÔ£∞
|
|
ÀÜu1
¬∑ ¬∑ ¬∑
ÀÜum
|
|
Ô£π
Ô£∫Ô£ª
|
{z
}
U
Ô£Æ
Ô£ØÔ£∞
œÉ1
0
¬∑ ¬∑ ¬∑
0
œÉ2
¬∑ ¬∑ ¬∑
0
0
¬∑ ¬∑ ¬∑
Ô£π
Ô£∫Ô£ª
|
{z
}
Œ£
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
‚Äî
ÀÜv1
‚Äî
...
‚Äî
ÀÜvn
‚Äî
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
|
{z
}
V T
= UŒ£V T.
To Ô¨Ånd the matrices U, Œ£, and V , perform eigendecomposition on the
matrix products AAT and ATA.
First, consider Ô¨Årst the matrix AAT. Since AAT is a square ma-
trix, we can compute its eigendecomposition AAT = UŒõ‚ÑìU T. The
eigenvectors of AAT span the same space as the column space of the
matrix A. We call these vectors the left singular vectors of A.
The left singular vectors of A (the columns of U) are the eigen-
vectors of the matrix AAT:
U =
Ô£Æ
Ô£ØÔ£∞
|
|
ÀÜu1
¬∑ ¬∑ ¬∑
ÀÜum
|
|
Ô£π
Ô£∫Ô£ª,
where {(Œªi, ÀÜui)} = eigenvects(AAT).
To Ô¨Ånd the right singular vectors of A (the rows of V T), perform
the eigendecomposition on the matrix ATA, denoted ATA = V ŒõrV T.
Build the orthogonal matrix V T by stacking the eigenvectors of ATA
as rows:
V T =
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚Äî
ÀÜv1
‚Äî
...
‚Äî
ÀÜvn
‚Äî
Ô£π
Ô£∫Ô£∫Ô£ª,
where {(Œªi, ÀÜvi)} = eigenvects(ATA).
The eigenvalues of the matrix ATA are the same as the eigenvalues of
the matrix AAT. In both cases, the eigenvalues Œªi correspond to the
squares of the singular values of the matrix A.

294
THEORETICAL LINEAR ALGEBRA
On its diagonal, the matrix of singular values Œ£ ‚ààRm√ón con-
tains the singular values œÉi, which are the positive square roots of the
eigenvalues Œªi of the matrix AAT (or the matrix ATA):
œÉi =
p
Œªi ,
where {Œªi} = eigenvals(AAT) = eigenvals(ATA).
The singular value decomposition shows the inner structure of the
matrix A. We can interpret the operation ‚Éóy = A‚Éóx = UŒ£V T‚Éóx as a
three-step process:
1. Convert the input ‚Éóx to the basis of right singular vectors {‚Éóvi}.
2. Scale each component by the corresponding singular value œÉi.
3. Convert the output from the {‚Éóui} basis to the standard basis.
This three-step procedure is analogous to the three-step procedure
we used to understand the eigendecomposition of square matrices in
Section 7.1 (see page 264).
The singular value decomposition (SVD) has numerous applica-
tions in statistics, machine learning, and computer science. Applying
the SVD to a matrix is like looking inside it with X-ray vision, since
you can see its œÉis. The action of A = UŒ£V T occurs in n parallel
streams: the ith stream consists of multiplying the input vector by the
right singular vector ‚ÉóvT
i , scaling by the weight œÉi, and Ô¨Ånally multiply-
ing by the left singular vector ‚Éóui. Each singular value œÉi corresponds
to the "strength" of A on the ith subspace‚Äîthe subspace spanned by
its ith left and right singular vectors.
Example
Suppose you need to calculate the product M‚Éóv where
M ‚ààR1000√ó2000 and ‚Éóv ‚ààR2000. Suppose furthermore the matrix M
has only three large singular values, œÉ1 = 6, œÉ2 = 5, œÉ3 = 4, and
many small singular values, œÉ4 = 0.0002, œÉ5 = 0.0001, . . . , œÉ1000 =
1.1 √ó 10‚àí13. Observe that most of the "weight" of the matrix Œ£ is
concentrated in the Ô¨Årst three singular values, œÉ1, œÉ2, and œÉ3. We
can obtain a low-rank approximation to the matrix M by keeping
only the large singular values and their associated singular vectors.
Construct the matrix ÀúŒ£ ‚ààR3√ó3 which contains only the Ô¨Årst three
singular values, and surround ÀúŒ£ with matrices ÀúU ‚ààR1000√ó3 and ÀúV ‚Ä† ‚àà
R3√ó2000 that contain the singular vectors associated with the Ô¨Årst
three singular values. Despite the signiÔ¨Åcant reduction in the size of
the matrices used in the decomposition, the matrix
Àú
M ‚â°ÀúU ÀúŒ£ ÀúV ‚Ä† ‚àà
R1000√ó2000 represents a good approximation to the original matrix
M.
We cut some small insigniÔ¨Åcant singular values, which didn't
change the matrix too much. We can quantify the diÔ¨Äerence between

7.6
MATRIX DECOMPOSITIONS
295
the original M and its low-rank approximation Àú
M using the Hilbert-
Schmidt norm: ||M ‚àíÀú
M||HS =
qP1000
i=4 œÉ2
i . Since œÉ4, œÉ5, . . . , œÉ1000
are tiny numbers, we can say Àú
M ‚âàM.
Links
[ Singular value decomposition on Wikipedia ]
http://en.wikipedia.org/wiki/Singular_value_decomposition
[ Principal component analysis in statistics is based on SVD ]
http://en.wikipedia.org/wiki/Principal_component_analysis
[ Understanding the SVD and its applications ]
http://www.math.umn.edu/~lerman/math5467/svd.pdf
LU decomposition
Computing the inverse of a triangular matrix is far easier than com-
puting the inverse of a general matrix. Thus, it's sometimes useful to
write a matrix as the product of two triangular matrices for compu-
tational purposes. We call this factorization the LU decomposition:
A = LU,
where U is an upper triangular matrix and L is a lower triangular
matrix.
The main application of this decomposition is to obtain more ef-
Ô¨Åcient solutions to equations of the form A‚Éóx = ‚Éób. Because A = LU,
we can solve this equation in two steps. Starting from LU‚Éóx = ‚Éób, Ô¨Årst
multiply by L‚àí1 and then by U ‚àí1:
LU‚Éóx = ‚Éób
‚áí
L‚àí1LU‚Éóx = U‚Éóx = L‚àí1‚Éób
‚áí
‚Éóx = U ‚àí1L‚àí1‚Éób.
We've split the work of Ô¨Ånding the inverse A‚àí1 into two simpler sub-
tasks: Ô¨Ånding L‚àí1 and U ‚àí1, which are easier to compute.
The LU decomposition is mainly used for linear algebra calcula-
tions on computers, but it's also possible to Ô¨Ånd the LU decomposition
of a matrix by hand. Recall the algorithm for Ô¨Ånding the inverse of
a matrix in which we start from the array [ A | 1 ] and perform row
operations until we bring the array into reduced row echelon form
[ 1 | A‚àí1 ]. Consider the midpoint of the algorithm when the left-hand
side of the array is the row echelon form (REF). Since the matrix A
in its REF is upper triangular, the array will contain [ U | L‚àí1 ]. The
U part of the decomposition is on the left-hand side, and the L part
is obtained by Ô¨Ånding the inverse of the right-hand side of the array.
Note the LU decomposition exists only for matrices that can be
brought to RREF without using row-swap operations. If a matrix A

296
THEORETICAL LINEAR ALGEBRA
requires row-swap operations to be transformed to RREF, we can de-
compose it as A = PLU, where P is a permutation matrix. The PLU
decomposition has the same computational advantages of splitting the
inverse computation into three simpler subtasks: A‚àí1 = U ‚àí1L‚àí1P ‚àí1.
Cholesky decomposition
For a symmetric, positive semideÔ¨Ånite matrix A, the LU decomposi-
tion can take on a simpler form. Such matrices can be written as the
product of a triangular matrix and its transpose:
A = LLT
or
A = U TU,
where U is an upper triangular matrix and L is a lower triangular
matrix. This is called the Cholesky decomposition of a matrix, and like
the LU decomposition, it has applications for faster numerical linear
algebra calculations, non-linear optimization, and machine learning.
QR decomposition
Any real square matrix A ‚ààRn√ón can be decomposed as a product
of an orthogonal matrix O and an upper triangular matrix U:
A = OU.
For historical reasons, the orthogonal matrix is denoted Q instead
of O, and the upper triangular matrix is denoted R (think "right-
triangular" since it contains entries only to the right of main diagonal).
Using the conventional names, the decomposition becomes
A = QR,
which is why it's known as the QR decomposition.
The QR decomposition is equivalent to the Gram-Schmidt orthog-
onalization procedure on the columns of the matrix. The matrix Q
records the orthonormal basis while the matrix R contains the coeÔ¨É-
cients required to express the columns of A as linear combinations of
the columns of Q.
Example
Consider the decomposition
A =
Ô£Æ
Ô£∞
12
‚àí51
4
6
167
‚àí68
‚àí4
24
‚àí41
Ô£π
Ô£ª= QR.
We're looking for an orthogonal matrix Q and an upper triangular
matrix R such that A = QR. We can obtain the orthogonal matrix
Q by performing the Gram-Schmidt procedure on the columns of A.

7.6
MATRIX DECOMPOSITIONS
297
Let's illustrate the procedure by computing the factorization A.
Begin by changing the second column in A so it becomes orthogonal to
the Ô¨Årst (by subtracting a multiple of the Ô¨Årst column). Next, change
the third column in A so it is orthogonal to both of the Ô¨Årst columns
(by subtracting multiples of the Ô¨Årst two columns).
We obtain a
matrix with the same column space as A but which has orthogonal
columns:
Ô£Æ
Ô£∞
|
|
|
u1
u2
u3
|
|
|
Ô£π
Ô£ª=
Ô£Æ
Ô£ØÔ£∞
12
‚àí69
‚àí58
5
6
158
6
5
‚àí4
30
‚àí33
Ô£π
Ô£∫Ô£ª.
To obtain an orthogonal matrix, we must normalize each column to
be of unit length:
Q =
Ô£Æ
Ô£∞
|
|
|
u1
‚à•u1‚à•
u2
‚à•u2‚à•
u3
‚à•u3‚à•
|
|
|
Ô£π
Ô£ª=
Ô£Æ
Ô£ØÔ£∞
6
7
‚àí69
175
‚àí58
175
3
7
158
175
6
175
‚àí2
7
6
35
‚àí33
35
Ô£π
Ô£∫Ô£ª.
We can obtain the matrix R from QT and A:
QTA = QTQ
|{z}
1
R = R
‚áí
R = QTA =
Ô£Æ
Ô£∞
14
21
‚àí14
0
175
‚àí70
0
0
35
Ô£π
Ô£ª.
The columns of R contain the mixture of coeÔ¨Écients required to ob-
tain the columns of A from the columns of Q. For example, the second
column of A is equal to 21 u1
‚à•u1‚à•+ 175 u2
‚à•u2‚à•. Verify that QR equals A.
Discussion
The last several pages have only scratched the surface of matrix de-
compositions. There are countless applications for matrix methods,
and matrix factorizations play key roles in many of them.
Machine learning techniques often use matrix decompositions to
uncovers useful structure within data matrices. Two examples include
nonnegative matrix factorization (used for recommender systems) and
latent Dirichlet allocation (used for document classiÔ¨Åcation). I en-
courage you to research this subject further on your own‚Äîit's quite
an interesting wormhole to get sucked into.
Links
[ Cool retro video showing the steps of the SVD procedure ]
http://www.youtube.com/watch?v=R9UoFyqJca8

298
THEORETICAL LINEAR ALGEBRA
[ More info and examples on Wikipedia ]
http://en.wikipedia.org/wiki/Matrix_decomposition
http://en.wikipedia.org/wiki/Cholesky_decomposition
[ A detailed example of the QR factorization of a matrix ]
http://www.math.ucla.edu/~yanovsky/Teaching/Math151B/handouts/GramSchmidt.pdf
Exercises
E7.17 Compute the QR factorization of the matrix A =
Ô£Æ
Ô£∞
1
1
0
1
0
1
0
1
1
Ô£π
Ô£ª.
7.7
Linear algebra with complex numbers
So far we've discussed the math of vectors and matrices with real
coeÔ¨Écients. In fact, the linear algebra techniques you learned apply
to any Ô¨Åeld F. The term Ô¨Åeld applies to any mathematical object
for which the operations of addition, subtraction, multiplication, and
division are deÔ¨Åned.
Since the complex numbers C are a Ô¨Åeld, we can perform linear
algebra over the Ô¨Åeld of complex numbers. In this section, we'll deÔ¨Åne
vectors and matrices with complex coeÔ¨Écients, and discover that they
behave similarly to their real counterparts. You'll see that complex
linear algebra is no more complex than real linear algebra: it's the
same, in fact, except for one small diÔ¨Äerence: instead of matrix trans-
pose AT, we use the Hermitian transpose A‚Ä†, which is the combination
of the transpose and an entry-wise complex conjugate operation.
Complex vectors are not just an esoteric mathematical concept
intended for specialists.
Complex vectors can arise as answers for
problems involving ordinary real matrices. For example, the rotation
matrix
RŒ∏ =
cos Œ∏
‚àísin Œ∏
sin Œ∏
cos Œ∏

has complex eigenvalues Œª1 = eiŒ∏ and Œª2 = e‚àíiŒ∏ and its eigenvectors
have complex coeÔ¨Écients. If you want to know how to calculate the
eigenvalues and eigenvectors of rotation matrices, you need to under-
stand how to do linear algebra calculations with complex numbers.
This section serves as a review of all the important linear algebra
concepts we've learned in this book. I recommend you read this sec-
tion, even if you're not required to know about complex matrices for
your course. As your guide through the land of linear algebra, it's my
duty to make sure you understand linear algebra in the complex Ô¨Åeld.
It's good stuÔ¨Ä; I guarantee there's knowledge buzz to be had in this
section.

7.7
LINEAR ALGEBRA WITH COMPLEX NUMBERS
299
DeÔ¨Ånitions
Recall the basic notions of complex numbers introduced in Section 2.4:
‚Ä¢ i: the unit imaginary number; i ‚â°‚àö‚àí1 or i2 = ‚àí1
‚Ä¢ z = a + bi: a complex number z whose real part is a and whose
imaginary part is b
‚Ä¢ C: the set of complex numbers C = {a + bi | a, b ‚ààR}
‚Ä¢ Re{z} = a: the real part of z = a + bi
‚Ä¢ Im{z} = b: the imaginary part of z = a + bi
‚Ä¢ ¬Øz: the complex conjugate of z. If z = a + bi then ¬Øz = a ‚àíbi
‚Ä¢ |z| = ‚àö¬Øzz =
‚àö
a2 + b2: the magnitude or length of z = a + bi
‚Ä¢ arg(z) =1 tan‚àí1( b
a): the phase or argument of z = a + bi
Complex vectors
A complex vector ‚Éóv ‚ààCn is an array of n complex numbers:
‚Éóv = (v1, v2, . . . , vn) ‚àà(C, C, . . . , C) ‚â°Cn.
Complex matrices
A complex matrix A ‚ààCm√ón is a table of numbers with m rows and
n columns. An example of a 3 √ó 2 matrix with complex entries is
A =
Ô£Æ
Ô£∞
a11
a12
a21
a22
a31
a32
Ô£π
Ô£ª‚àà
Ô£Æ
Ô£∞
C
C
C
C
C
C
Ô£π
Ô£ª‚â°C3√ó2.
Hermitian transpose
The Hermitian transpose operation, denoted ‚Ä†, consists of the combi-
nation of the regular transpose (A ‚ÜíAT) and the complex conjuga-
tion of each entry in the matrix (aij ‚Üíaij):
A‚Ä† ‚â°(AT) = (A)T.
Expressed in terms of the entries of the matrix aij, the Hermitian
transpose corresponds to the transformation aij ‚Üíaji. There are
many mathematical terms that refer to this operation, including Her-
mitian conjugate, complex transpose, "dagger" operation, conjugate
transpose, and adjoint.
1Note that tan‚àí1( b
a) and arg(z) coincide only if a ‚â•0, and a manual correction
is necessary to the output of tan‚àí1( b
a) when a < 0. Alternatively, we can use the
function atan2(b,a) that computes the correct phase for all z = a + bi.

300
THEORETICAL LINEAR ALGEBRA
The term adjoint is preferred by mathematicians and the notation
A‚àóis used consistently in mathematics research papers. The dagger
notation ‚Ä† is preferred by physicists and engineers, but shunned by
mathematicians.
Mathematicians prefer to stick with the star su-
perscript because they feel they invented the concept. We use the
notation ‚Ä† in this book because at some point the author had to make
an allegiance with one of the two camps, and because the symbol ‚Ä†
looks a bit like the transpose symbol T.
The Hermitian transpose applied to a 3√ó2 matrix acts as follows:
if A =
Ô£Æ
Ô£∞
a11
a12
a21
a22
a31
a32
Ô£π
Ô£ª
then
A‚Ä† =
a11
a21
a31
a12
a22
a32

.
Recall that vectors are special types of a matrices. We can identify a
vector ‚Éóv ‚ààCn with a column matrix ‚Éóv ‚ààCn√ó1 or with a row matrix
‚Éóv ‚ààC1√ón. We apply the complex conjugation operation to transform
column vectors into conjugate row vectors:
‚Éóv‚Ä† ‚â°(‚ÉóvT) = (‚Éóv)T.
The Hermitian transpose of a column vector is a row vector in which
each coeÔ¨Écient has been complex-conjugated:
if ‚Éóv =
Ô£Æ
Ô£∞
Œ±
Œ≤
Œ≥
Ô£π
Ô£ª
then
‚Éóv‚Ä† =
Ô£Æ
Ô£∞
Œ±
Œ≤
Œ≥
Ô£π
Ô£ª
‚Ä†
=

Œ±
Œ≤
Œ≥

.
The Hermitian transpose for vectors is important because it's related
to the deÔ¨Ånition of the inner product for complex vectors.
Complex inner product
The inner product for vectors with complex coeÔ¨Écients ‚Éóu,‚Éóv ‚ààCn is
deÔ¨Åned as the following operation:
‚ü®‚Éóu,‚Éóv‚ü©‚â°
n
X
i=1
uivi ‚â°‚Éóu‚Ä†‚Éóv.
In this expression, complex conjugation is applied to each of the Ô¨Årst
vector's components. This corresponds to the notion of applying the
Hermitian transpose to the Ô¨Årst vector to turn it into a row vector of
complex conjugates, then using the matrix multiplication rule for a
1 √ó n matrix ‚Éóu‚Ä† times an n √ó 1 matrix ‚Éóv.
For real vectors ‚Éóu,‚Éóv ‚ààRn, the complex inner product formula
reduces to the inner product formula we used previously: ‚Éóu ¬∑‚Éóv = ‚ÉóuT‚Éóv.

7.7
LINEAR ALGEBRA WITH COMPLEX NUMBERS
301
Rather than thinking of the inner product for complex vectors as a
new operation, we can say the inner product has always been deÔ¨Åned
as ‚ü®‚Éóu,‚Éóv‚ü©‚â°‚Éóu‚Ä†‚Éóv‚Äîwe just never noticed until now because complex
conjugation has no eÔ¨Äect on vectors with real coeÔ¨Écients. SpeciÔ¨Åcally,
‚Éóu‚Ä† = ‚ÉóuT if all the uis are real numbers.
Linear algebra over the complex Ô¨Åeld
One of the fundamental linear algebra ideas we've learned is how to
use linear transformations to model input-output phenomena in which
input vectors ‚Éóv are linearly transformed to output vectors: ‚Éów = T(‚Éóv).
Linear transformations are vector functions of the form T : Rm ‚ÜíRn.
We can represent these Linear transformations as an m √ó n matrix
with respect to some choice of input and output bases.
These linear algebra ideas also apply to complex vectors and com-
plex matrices. For example, a linear transformation from T : C2 ‚ÜíC2
can be represented in terms of the matrix product

w1
w2

=

Œ±
Œ≤
Œ≥
Œ¥

v1
v2

.
Each linear transformation T : C2 ‚ÜíC2 corresponds to some 2 √ó 2
matrix
Ô£Æ
Ô£ØÔ£∞Œ±
Œ≤
Œ≥
Œ¥
Ô£π
Ô£∫Ô£ªwith coeÔ¨Écients Œ±, Œ≤, Œ≥, Œ¥ ‚ààC.
The change from real coeÔ¨Écients to complex coeÔ¨Écients has the
eÔ¨Äect of doubling the number of parameters required to describe the
transformation. A 2 √ó 2 complex matrix has eight parameters, not
four. Where are those eight parameters, you ask? Here:

Œ±
Œ≤
Œ≥
Œ¥

=

Re{Œ±} Re{Œ≤}
Re{Œ≥} Re{Œ¥}

+

Im{Œ±} Im{Œ≤}
Im{Œ≥} Im{Œ¥}

i.
Each of the four coeÔ¨Écients of the matrix has a real part and an
imaginary part, making for a total of eight parameters to "pick" when
specifying the matrix.
Similarly, to specify a vector ‚Éóv = C2 you need to specify four
parameters:
v1
v2

=
Re{v1}
Re{v2}

+
Im{v1}
Im{v2}

i.
In practice, this doubling of dimensions doesn't play a role in calcu-
lations because we usually perform algebra steps with the complex
coeÔ¨Écients and rarely split the matrices into their real and imaginary
parts.
All the linear algebra techniques you've learned also work with
complex numbers, as you'll see in the following examples.

302
THEORETICAL LINEAR ALGEBRA
Example 1: Solving systems of equations
Suppose you're solv-
ing a problem that involves complex numbers and a system of two
linear equations in two unknowns:
z1 + 2z2 = 3 + i
3z1 + (9 + i)z2 = 6 + 2i.
You're asked to Ô¨Ånd the values of the unknowns z1 and z2.
The solutions z1 and z2 will be complex numbers, but apart from
that, there's nothing special about this problem‚Äîkeep in mind, linear
algebra with complex numbers is the same as linear algebra with real
numbers, so the techniques you learned for real numbers work just as
well for complex numbers. Now let's solve this system of equations.
First observe that the system of equations can be written as a
matrix-vector product:
1
2
3
9 + i

|
{z
}
A
 z1
z2

|{z}
‚Éóz
=
 3 + i
6 + 2i

| {z }
‚Éób
.
We've expressed the system as a 2√ó2 matrix A multiplying the vector
of unknowns ‚Éóz (a 2 √ó 1 matrix) to produce a vector of constants ‚Éób
(another 2 √ó 1 matrix). We can solve for ‚Éóz by multiplying both sides
of the equation by the inverse matrix A‚àí1. The inverse matrix of A
is
A‚àí1 =
"
1 +
6
3+i
‚àí
2
3+i
‚àí
3
3+i
1
3+i
#
.
We can now compute the answer ‚Éóz using the equation ‚Éóz = A‚àí1‚Éób:
"
z1
z2
#
=
"
1 +
6
3+i
‚àí
2
3+i
‚àí
3
3+i
1
3+i
#"
3 + i
6 + 2i
#
=
"
3 + i + 6 ‚àí4
‚àí3 + 2
#
=
"
5 + i
‚àí1
#
.
Example 2: Finding the inverse
We learned several approaches
for computing matrix inverses in Section 4.5. Here we'll review the
procedure for computing the inverse using row operations.
Given the matrix
A =
1
2
3
9 + i

,
Ô¨Årst build a 2√ó4 array that contains A on the left side and the identity
matrix 1 on the right side:
 1
2
1
0
3
9 + i
0
1

.
We now perform the Gauss-Jordan elimination procedure on the re-
sulting 2 √ó 4 array.

7.7
LINEAR ALGEBRA WITH COMPLEX NUMBERS
303
1. Subtract three times the Ô¨Årst row from the second row (R2 ‚Üê
R2 ‚àí3R1) to obtain

1
2
1
0
0
3 + i
‚àí3
1

.
2. Perform R2 ‚Üê
1
3+iR2 to create a pivot in the second row:
 1
2
1
0
0
1
‚àí3
3+i
1
3+i

.
3. Finally, perform R1 ‚ÜêR1 ‚àí2R2 to obtain the RREF:
"
1
0
1 +
6
3+i
‚àí
2
3+i
0
1
‚àí3
3+i
1
3+i
#
.
The inverse of A appears on the right side of the array,
A‚àí1 =
"
1 +
6
3+i
‚àí
2
3+i
‚àí
3
3+i
1
3+i
#
.
Example 3: Linear transformations as matrices
The eÔ¨Äect of
multiplying a vector ‚Éóv ‚ààCn by a matrix M ‚ààCm√ón is the same as
applying a linear transformation TM : Cn ‚ÜíCm to the vector:
‚Éów = M‚Éóv
‚áî
‚Éów = TM(‚Éóv).
The opposite is also true‚Äîany linear transformation T can be repre-
sented as a matrix product with some matrix MT :
‚Éów = T(‚Éóv)
‚áî
‚Éów = MT‚Éóv.
We'll use a simple example to review the procedure for Ô¨Ånding the
matrix representation of a linear transformation.
Consider the linear transformation T : C2 ‚ÜíC2, which produces
the input-output pairs
T

1
0

=

3
2i

and
T
0
2

=

2
4 + 4i

.
How can we use the information provided above to Ô¨Ånd the matrix
representation of the linear transformation T?
To obtain the matrix representation of T with respect to a given
basis, we need to combine, as columns, the outputs of T for the n
elements of that basis:
MT =
Ô£Æ
Ô£ØÔ£∞
|
|
|
T(‚Éóe1)
T(‚Éóe2)
¬∑ ¬∑ ¬∑
T(‚Éóen)
|
|
|
Ô£π
Ô£∫Ô£ª,

304
THEORETICAL LINEAR ALGEBRA
where the set {ÀÜe1, ÀÜe2, . . . , ÀÜen} is a basis for the input space.
The problem statement gives us the information needed for the
Ô¨Årst column of MT , but we're not given the output of T for ÀÜe2. How-
ever, we can work around this limitation since we know T is linear.
The property T(Œ±‚Éóv) = Œ±T(‚Éóv) implies
T

2
0
1

= 2

1
2 + 2i

‚áí
T
0
1

=

1
2 + 2i

.
Combining the information for T(ÀÜe1) and T(ÀÜe2), we obtain the matrix
representation of T:
MT =
 3
1
2i
2 + 2i

.
Complex eigenvalues
The main reason why I want you to learn about linear algebra with
complex vectors is so we can complete the important task of classifying
the basic types of linear transformations in terms of their eigenvalues.
Recall that projections obey Œ† = Œ†2 and have eigenvalues zero or
one, and reÔ¨Çections have at least one eigenvalue equal to ‚àí1.
What are the eigenvalues of rotation matrices? The eigenvalues
of a matrix A are the roots of its characteristic polynomial pA(Œª) =
det(A ‚àíŒª1). To Ô¨Ånd the eigenvalues of the rotation matrix RŒ∏ we
deÔ¨Åned in Section 6.2 (page 241), we must Ô¨Ånd the solutions of the
equation pRŒ∏(Œª) = 0:
0 = pRŒ∏(Œª)
= det(RŒ∏ ‚àíŒª1)
= det
cos Œ∏ ‚àíŒª
‚àísin Œ∏
sin Œ∏
cos Œ∏ ‚àíŒª

= (cos Œ∏ ‚àíŒª)2 + sin2 Œ∏.
To solve for Œª, Ô¨Årst move sin2 Œ∏ to the other side of the equation
‚àísin2 Œ∏ = (cos Œ∏ ‚àíŒª)2,
then take the square root on both sides:
cos Œ∏ ‚àíŒª = ¬±
p
‚àísin2 Œ∏ = ¬±
‚àö
‚àí1 sin Œ∏ = ¬±i sin Œ∏.
The eigenvalues of RŒ∏ are Œª1 = cos Œ∏ + i sin Œ∏ and Œª2 = cos Œ∏ ‚àíi sin Œ∏.
Using Euler's formula (see page 106) we can express the eigenvalues
more compactly as Œª1 = eiŒ∏ and Œª2 = e‚àíiŒ∏. What's interesting here
is that complex numbers emerge as answers to a matrix problem that
was originally stated in terms of real variables.

7.7
LINEAR ALGEBRA WITH COMPLEX NUMBERS
305
This is not a coincidence: complex exponentials are in many ways
the natural way to talk about rotations, periodic motion, and waves.
If you pursue a career in math, physics, or engineering you'll use
complex numbers and Euler's equation on a daily basis.
Special types of matrices
We'll now deÔ¨Åne a few special types of matrices with complex co-
eÔ¨Écients. These matrices are analogous to the special matrices we
deÔ¨Åned in Section 7.2, but their deÔ¨Ånitions are adapted to use the
Hermitian conjugate operation ‚Ä†.
Unitary matrices
A matrix U is unitary if it obeys U ‚Ä†U = 1. The norm of the determi-
nant of a unitary matrix is 1, | det(U)| = 1. For an n √ó n matrix U,
the following statements are equivalent:
‚Ä¢ U is unitary.
‚Ä¢ The columns of U form an orthonormal basis.
‚Ä¢ The rows of U form an orthonormal basis.
‚Ä¢ The inverse of U is U ‚Ä†.
Unitary matrices are the complex analogues of orthogonal matrices.
Indeed, if a unitary matrix U has real coeÔ¨Écients, then U ‚Ä† = U T and
we have U TU = 1, which is the deÔ¨Ånition of an orthogonal matrix.
Hermitian matrices
A Hermitian matrix H is equal to its own Hermitian transpose:
H‚Ä† = H
‚áî
hij = hji,
for all i, j.
Hermitian matrices are complex-number analogues of symmetric ma-
trices.
A Hermitian matrix H can be freely moved from one side to the
other in a complex inner product:
‚ü®H‚Éóx, ‚Éóy‚ü©= (H‚Éóx)‚Ä†‚Éóy = ‚Éóx‚Ä†H‚Ä†‚Éóy = ‚Éóx‚Ä† (H‚Éóy) = ‚ü®‚Éóx, H‚Éóy‚ü©.
The eigenvalues of Hermitian matrices are real numbers.
Normal matrices
Previously, we deÔ¨Åned the set of real normal matrices to be matrices
that satisfy ATA = AAT. For matrices with complex coeÔ¨Écients, the
deÔ¨Ånition of a normal matrix uses the dagger: A‚Ä†A = AA‚Ä†.

306
THEORETICAL LINEAR ALGEBRA
Consulting the concept map in Figure 7.1 on page 276 will help
you see the parallels between the diÔ¨Äerent types of special matrices. I
realize there's a lot of new terminology to absorb all at once, so don't
worry about remembering everything. The main idea is to know these
special types of matrices exist‚Äînot to know everything about them.
Inner product for complex vectors
The complex inner product is an operation of the form:
‚ü®¬∑, ¬∑‚ü©: Cn √ó Cn ‚ÜíC.
The inner product ‚ü®‚Éóu,‚Éóv‚ü©for real vectors is equivalent to the matrix
product between the row vector ‚ÉóuT and the column vector ‚Éóv. Extend-
ing the notion of inner product to work with complex vectors requires
a modiÔ¨Åcation to the inner product formula. The inner product for
vectors ‚Éóu,‚Éóv ‚ààCn is deÔ¨Åned as
‚ü®‚Éóu,‚Éóv‚ü©‚â°
n
X
i=1
uivi ‚â°‚Éóu‚Ä†‚Éóv.
The formula is similar to the inner product formula for real vectors,
but uses the Hermitian transpose ‚Ä† instead of the regular transpose T.
The inner product of two vectors ‚Éóu,‚Éóv ‚ààC3 is
‚ü®‚Éóu,‚Éóv‚ü©= ¬Øu1v1 + ¬Øu2v2 + ¬Øu3v3 =

¬Øu1
¬Øu2
¬Øu3

Ô£Æ
Ô£∞
v1
v2
v3
Ô£π
Ô£ª= ‚Éóu‚Ä†‚Éóv.
This dagger thing is very important. Using the deÔ¨Ånition of the inner
product with a dagger on the Ô¨Årst entry ensures the complex inner
product will obey the positive semideÔ¨Ånite criterion (see page 281).
The inner product of a vector ‚Éóv ‚ààC3 with itself is
‚ü®‚Éóv,‚Éóv‚ü©‚â°‚Éóv‚Ä†‚Éóv =
¬Øv1
¬Øv2
¬Øv3

Ô£Æ
Ô£∞
v1
v2
v3
Ô£π
Ô£ª= |v1|2 + |v2|2 + |v3|2,
where |vi|2 = ¬Øvivi is the magnitude-squared of the coeÔ¨Écient vi ‚ààC.
The magnitudes of the complex coeÔ¨Écients are nonnegative real num-
bers, so the sum of their squares is also a nonnegative real number.
Therefore, the complex inner product satisÔ¨Åes the positive semideÔ¨Å-
nite requirement ‚ü®‚Éóv,‚Éóv‚ü©‚â•0 for inner products.
Length of a complex vector
The complex inner product induces the following complex norm:
‚à•‚Éóv‚à•‚â°
p
‚ü®‚Éóv,‚Éóv‚ü©=
‚àö
‚Éóv‚Ä†‚Éóv =
p
|v1|2 + |v2|2 + ¬∑ ¬∑ ¬∑ + |vn|2 .

7.7
LINEAR ALGEBRA WITH COMPLEX NUMBERS
307
The norm for complex vectors satisÔ¨Åes the positive semideÔ¨Ånite re-
quirement ‚à•‚Éóv‚à•‚â•0 for norms (see page 283).
Example
Calculate the norm of the vector ‚Éóv = (2 + i, 3, 5i).
The Hermitian transpose of the row vector ‚Éóv is the column vec-
tor ‚Éóv‚Ä† = (2 ‚àíi, 3, ‚àí5i)T.
The norm of ‚Éóv is equal to the square
root of ‚ü®‚Éóv,‚Éóv‚ü©so ‚à•‚Éóv‚à•=
p
‚ü®‚Éóv,‚Éóv‚ü©=
p
(2 ‚àíi)(2 + i) + 32 + (‚àí5i)(5i) =
‚àö4 + 1 + 9 + 25 =
‚àö
39.
Complex inner product spaces
A real inner product space is an abstract vector space (V, R, +, ¬∑) for
which we've deÔ¨Åned an inner product operation ‚ü®u, v‚ü©which obeys
(1) the symmetric property, (2) the linearity property, and (3) the
positive semideÔ¨Ånite property.
Similarly, a complex inner product space is an abstract vector
space (V, C, +, ¬∑) with an inner product operation ‚ü®u, v‚ü©that satisÔ¨Åes
the following criteria for all u, v, v1, v2 ‚ààV and Œ±, Œ≤ ‚ààC:
‚Ä¢ Conjugate symmetric: ‚ü®u, v‚ü©= ‚ü®v, u‚ü©
‚Ä¢ Linear: ‚ü®u, Œ±v1 + Œ≤v2‚ü©= Œ±‚ü®u, v1‚ü©+ Œ≤‚ü®u, v2‚ü©
‚Ä¢ Positive semideÔ¨Ånite: ‚ü®u, u‚ü©‚â•0 for all u ‚ààV with ‚ü®u, u‚ü©= 0 if
and only if u = 0
The conjugate symmetry property ‚ü®u, v‚ü©= ‚ü®v, u‚ü©ensures the inner
product of a vector with itself is a real number: ‚ü®u, u‚ü©= ‚ü®u, u‚ü©‚ààR.
Example
The Hilbert-Schmidt inner product for matrices A, B ‚àà
Cm√ón is deÔ¨Åned as
‚ü®A, B‚ü©HS ‚â°Tr
 A‚Ä†B

=
n
X
i=1
‚ü®A‚Éóei, B‚Éóei‚ü©.
The product A‚Éóei has the eÔ¨Äect of "selecting" the ith column of the
matrix A; we can consider the Hilbert-Schmidt inner product of ma-
trices A and B as the sum of the vector inner products of the columns
of the two matrices.
We can also deÔ¨Åne the Hilbert-Schmidt norm for matrices:
||A||HS ‚â°
p
‚ü®A, A‚ü©=
q
Tr(A‚Ä†A) =
Ô£Æ
Ô£∞
m
X
i=1
n
X
j=1
|aij|2
Ô£π
Ô£ª
1
2
.
The Hilbert-Schmidt norm is the square root of the sum of the squared-
magnitudes of the entries of the matrix.
The Hilbert-Schmidt inner product and norm are sometimes called
Frobenius inner product and Frobenius norm, respectively.

308
THEORETICAL LINEAR ALGEBRA
Singular value decomposition
The singular value decomposition we introduced for real matrices in
Section 7.6 also applies to matrices with complex entries.
The singular value decomposition of a matrix A ‚ààCm√ón is a way
to express A as the product of three matrices:
A = UŒ£V ‚Ä†.
The m √ó m unitary matrix U consists of left singular vectors of A.
The m √ó n matrix Œ£ contains the singular values œÉi on the diagonal.
The n √ó n unitary matrix V ‚Ä† consists of right singular vectors.
The singular values œÉi of A are the positive square roots of the
eigenvalues of the matrix AA‚Ä†. To Ô¨Ånd the matrix of left singular vec-
tors U, calculate the eigenvectors of AA‚Ä† and pack them as columns.
To Ô¨Ånd the matrix of right singular vectors V ‚Ä†, calculate the eigen-
vectors A‚Ä†A, pack them as columns in a matrix V , then take the
Hermitian transpose of this matrix.
The Hilbert-Schmidt norm of a matrix A ‚ààCm√ón is equal to the
square root of the sum of the squares of its singular values:
||A||HS =
q
Tr(A‚Ä†A) =
v
u
u
t
n
X
i=1
œÉ2
i .
This important fact about matrices shows a connection between the
"size" of a matrix and the size of its singular values. Each singular
value œÉi corresponds to the strength of the eÔ¨Äect of A on the ith
subspaces spanned by its left and right singular vectors.
The singular value decomposition is used in many algorithms and
procedures to uncover the inner structure of matrices. The machine
learning technique called principal component analysis (PCA) corre-
sponds to applying the SVD to a data matrix. Alternatively, you can
think of the PCA as applying an eigendecomposition of the covariance
matrix of the data.
Explanations
Complex eigenvectors
The characteristic polynomial of the rotation matrix RŒ∏ is p(Œª) =
(cos Œ∏‚àíŒª)2+sin2 Œ∏ = 0. The eigenvalues of RŒ∏ are Œª1 = cos Œ∏+i sin Œ∏ =
eiŒ∏ and Œª2 = cos Œ∏ ‚àíi sin Œ∏ = e‚àíiŒ∏. What are its eigenvectors?
Before we get into the eigenvector calculation, I want to show you
a useful trick for rewriting cos and sin expressions in terms of complex

7.7
LINEAR ALGEBRA WITH COMPLEX NUMBERS
309
exponential functions. Recall Euler's equation, eiŒ∏ = cos Œ∏ + i sin Œ∏.
Using this equation and the analogous expression for e‚àíiŒ∏, we can
obtain the following expressions for cos Œ∏ and sin Œ∏:
cos Œ∏ = 1
2
 eiŒ∏ + e‚àíiŒ∏
,
sin Œ∏ = 1
2i
 eiŒ∏ ‚àíe‚àíiŒ∏
.
Try calculating the right-hand side in each case to verify the accuracy
of each expression.
These formulas are useful because they allow
us to rewrite expressions of the form eiŒ∏ cos œÜ as eiŒ∏ 1
2
 eiœÜ + e‚àíiœÜ
=
1
2
 ei(Œ∏+œÜ) + ei(Œ∏‚àíœÜ)
, which is simpler.
Let's now see how to Ô¨Ånd the eigenvector ‚ÉóeŒª1 ‚â°(Œ±, Œ≤)T associ-
ated with the eigenvalue Œª1 = eiŒ∏. The eigenvalue equation for the
eigenvalue Œª1 = eiŒ∏ is
RŒ∏‚ÉóeŒª1 = eiŒ∏‚ÉóeŒª1
‚áî

cos Œ∏
‚àísin Œ∏
sin Œ∏
cos Œ∏

Œ±
Œ≤

= eiŒ∏

Œ±
Œ≤

.
We're looking for the coeÔ¨Écients Œ± and Œ≤.
Do you remember how to Ô¨Ånd eigenvectors? Don't worry if you've
forgotten‚Äîthis is why we have this review chapter! We'll go through
the problem in detail. Brace yourself though, because the calculation
is quite long.
The "Ô¨Ånding the eigenvector(s) of A for the eigenvalue Œª1" task is
carried out by Ô¨Ånding the null space of the matrix (A ‚àíŒª11). We
rewrite the eigenvalue equation stated above as
(RŒ∏ ‚àíeiŒ∏1)‚ÉóeŒª1 = 0
‚áî
cos Œ∏ ‚àíeiŒ∏
‚àísin Œ∏
sin Œ∏
cos Œ∏ ‚àíeiŒ∏
Œ±
Œ≤

=
0
0

.
It's now clear that the Ô¨Ånding-the-eigenvectors procedure corresponds
to a null space calculation.
Let's use the cos-rewriting trick to simplify cos Œ∏ ‚àíeiŒ∏:
cos Œ∏ ‚àíeiŒ∏ = 1
2
 eiŒ∏ + e‚àíiŒ∏
‚àíeiŒ∏
= 1
2eiŒ∏ + 1
2e‚àíiŒ∏ ‚àíeiŒ∏ = ‚àí1
2 eiŒ∏ + 1
2e‚àíiŒ∏ = ‚àí1
2
 eiŒ∏ ‚àíe‚àíiŒ∏
= ‚àíi 1
2i
 eiŒ∏ ‚àíe‚àíiŒ∏
= ‚àíi sin Œ∏.
We substitute this simpliÔ¨Åed expression in the two places where it
appears, and do some row operations to simplify the matrix:
‚àíi sin Œ∏
‚àísin Œ∏
sin Œ∏
‚àíi sin Œ∏

‚àº
sin Œ∏ ‚àíi sin Œ∏
sin Œ∏ ‚àíi sin Œ∏

‚àº
sin Œ∏ ‚àíi sin Œ∏
0
0

‚àº
1
‚àíi
0
0

.
We can now solve the null space problem. Observe that the second
column of the matrix does not contain a pivot, so Œ≤ is a free variable,

310
THEORETICAL LINEAR ALGEBRA
which we'll denote s ‚ààR. We thus obtain the equations:
1
‚àíi
0
0
Œ±
s

=
0
0

‚áí
1Œ± + (‚àíis)
=
0,
0
=
0.
Solving for Œ± in terms of s, we Ô¨Ånd Œ± = is, and therefore the solution
is (Œ±, Œ≤)T = (is, s). The eigenspace that corresponds to the eigenvalue
Œª1 = eiŒ∏ is the null space of the matrix (RŒ∏ ‚àíeiŒ∏1):
N(RŒ∏ ‚àíeiŒ∏1) =
is
s

,
‚àÄs ‚ààR

= span
i
1

.
After all this work, we've Ô¨Ånally obtained an eigenvector ‚ÉóeŒª1 = (i, 1)T
that corresponds to the eigenvalue Œª1 = eiŒ∏. Let's verify that the
vector we obtained satisÔ¨Åes the eigenvalue equation RŒ∏‚ÉóeŒª1 = eiŒ∏‚ÉóeŒª1:
cos Œ∏
‚àísin Œ∏
sin Œ∏
cos Œ∏
i
1

=
i cos Œ∏ ‚àísin Œ∏
i sin Œ∏ + cos Œ∏

=
i(cos Œ∏ + i sin Œ∏)
cos Œ∏ + i sin Œ∏

= eiŒ∏
i
1

.
The eigenvector for the eigenvalue Œª2 = e‚àíiŒ∏ is ‚ÉóeŒª2 = (i, ‚àí1)T. Verify
that it satisÔ¨Åes the eigenvalue equation RŒ∏‚ÉóeŒª2 = e‚àíiŒ∏‚ÉóeŒª2.
I know it was quite a struggle to Ô¨Ånd the eigenvectors of this
rotation matrix, but this the case in general when Ô¨Ånding eigenvec-
tors.
You must complete the null space calculation steps for each
eigenspace, and this takes a long time. Be sure to practice Ô¨Ånding
eigenvectors by hand‚ÄîI can pretty much guarantee you'll need this
skill on your linear algebra Ô¨Ånal. And don't forget to give yourself a
pat on the back when you're done!
Properties of the Hermitian transpose operation
The Hermitian transpose obeys the following properties:
‚Ä¢ (A + B)‚Ä† = A‚Ä† + B‚Ä†
‚Ä¢ (AB)‚Ä† = B‚Ä†A‚Ä†
‚Ä¢ (ABC)‚Ä† = C‚Ä†B‚Ä†A‚Ä†
‚Ä¢ (A‚Ä†)‚àí1 = (A‚àí1)‚Ä†
Note these are the same properties as the regular transpose operation
from Section 3.3 (see page 123).
Conjugate linearity in the Ô¨Årst input
The complex inner product we deÔ¨Åned is linear in the second entry
and conjugate-linear in the Ô¨Årst entry:
‚ü®‚Éóv, Œ±‚Éóa + Œ≤‚Éób‚ü©= Œ±‚ü®‚Éóv,‚Éóa‚ü©+ Œ≤‚ü®‚Éóv,‚Éób‚ü©,
‚ü®Œ±‚Éóa + Œ≤‚Éób, ‚Éów‚ü©= Œ±‚ü®‚Éóa, ‚Éów‚ü©+ Œ≤‚ü®‚Éób, ‚Éów‚ü©.

7.7
LINEAR ALGEBRA WITH COMPLEX NUMBERS
311
Keep this in mind every time you deal with complex inner products.
The complex inner product is not symmetric since it requires that
the complex conjugation be performed on the Ô¨Årst input. Remember,
instead of ‚ü®‚Éóv, ‚Éów‚ü©Ã∏= ‚ü®‚Éów,‚Éóv‚ü©, we have ‚ü®‚Éóv, ‚Éów‚ü©= ‚ü®‚Éów,‚Éóv‚ü©.
The choice of complex conjugation in the Ô¨Årst entry is a matter of
convention. In this text, we deÔ¨Åned the inner product ‚ü®¬∑, ¬∑‚ü©with the ‚Ä†
operation on the Ô¨Årst entry, which is known as the physics convention.
Some old mathematics texts deÔ¨Åne the inner product of complex vec-
tors using the complex conjugation on the second entry, which makes
the inner product linear in the Ô¨Årst entry and conjugate-linear in the
second entry. This convention is Ô¨Åne, too. The choice of convention
doesn't matter, as long as one of the entries is conjugated to ensure the
inner product obeys the positive semideÔ¨Ånite requirement ‚ü®‚Éóu, ‚Éóu‚ü©‚â•0.
Function inner product
In the section on inner product spaces, we discussed the notion of the
vector space of all real-valued functions of a real variable f : R ‚ÜíR,
and deÔ¨Åned an inner product between functions:
‚ü®f, g‚ü©=
Z ‚àû
‚àí‚àû
f(x)g(x) dx.
Suppose we have two complex-valued functions f(x) and g(x):
f : R ‚ÜíC,
g: R ‚ÜíC.
We deÔ¨Åne the inner product for complex-valued functions as
‚ü®f, g‚ü©=
Z ‚àû
‚àí‚àû
f(x)g(x) dx.
The complex conjugation of one of the functions ensures that the
inner product of a function with itself results in a real number. The
function inner product measures the overlap between f(x) and g(x).
Linear algebra over other Ô¨Åelds
We can carry out linear algebra calculations over any Ô¨Åeld. A Ô¨Åeld is
a set of numbers for which an addition, subtraction, multiplication,
and division operation are deÔ¨Åned. The addition and multiplication
operations we deÔ¨Åne must be associative and commutative, and mul-
tiplication must distribute over addition. Furthermore, a Ô¨Åeld must
contain an additive identity element (denoted 0) and a multiplicative
identity element (denoted 1). The properties of a Ô¨Åeld are essentially
all the properties of the numbers you're familiar with: Q, R, and C.

312
THEORETICAL LINEAR ALGEBRA
The focus of our discussion in this section was to show the linear
algebra techniques we learned for manipulating real numbers work
equally well with the complex numbers. This shouldn't be too sur-
prising since, after all, linear algebra manipulations boil down to arith-
metic manipulations of the coeÔ¨Écients of vectors and matrices. As
both real numbers and complex numbers can be added, subtracted,
multiplied, and divided, we can study linear algebra over both R and
C.
We can also perform linear algebra over Ô¨Ånite Ô¨Åelds. A Ô¨Ånite Ô¨Åeld
is a set Fq ‚â°{0, 1, 2, . . . , q ‚àí1}, where q is a prime number or the
power of a prime number. All the arithmetic operations in this Ô¨Åeld
are performed modulo the number q, which means all arithmetic op-
erations must result in answers in the set Fq ‚â°{0, 1, 2, . . . , q ‚àí1}. If
the result of an operation falls outside this set, we either add or sub-
tract q until the number falls in the set Fq. Consider the Ô¨Ånite Ô¨Åeld
F5 = {0, 1, 2, 3, 4}. To add two numbers in F5, proceed as follows:
(3 + 3) mod 5 = 6 mod 5
(too big, so subtract 5)
= 1 mod 5.
Similarly, for subtraction,
(1 ‚àí4) mod 5 = (‚àí3) mod 5
(too small, so add 5)
= 2 mod 5.
The Ô¨Åeld of binary numbers F2 ‚â°{0, 1} is an important Ô¨Ånite Ô¨Åeld
used in many areas of communications, engineering, and cryptogra-
phy. In the next chapter we'll discuss the one-time crypto system
which allows for secure communication of messages encoded in binary
(Section 8.9). We'll also discuss the error-correcting codes used that
enable the reliable transmission of information over noisy communi-
cation channels (Section 8.10). For example, the data packets that
your cell phone sends over the radio waves are Ô¨Årst linearly encoded
using a matrix-vector product operation carried out over the Ô¨Åeld F2.
At Ô¨Årst hand, thinking of linear algebra over the Ô¨Ånite Ô¨Åeld Fq ‚â°
{0, 1, 2, . . . , q ‚àí1} may seem complicated, but don't worry about it.
It's the same stuÔ¨Ä‚Äîyou just have to modq every arithmetic calcula-
tion. All you your intuition about dimensions and orthogonality, and
all the computational procedures you know are still applicable.
The Ô¨Åeld of rational numbers Q is another example of a Ô¨Åeld that's
often used in practice. Solving systems of equations using rational
numbers on computers is interesting because the answers obtained
are exact‚Äîusing rational numbers allows us to avoid many of the
numerical accuracy problems associated with Ô¨Çoating point numbers.

7.8
THEORY PROBLEMS
313
Discussion
The adjoint operator
Though we used the term Hermitian transpose and the notation A‚Ä†
throughout this section, it's worth commenting that mathematicians
prefer the term adjoint for the same operation, and denote it A‚àó.
Recall we previously discussed the concept of an adjoint linear trans-
formation TM T : Rm ‚ÜíRn, which corresponds to the multiplication
of a matrix M by a row vector from the left TM T(‚Éóa) ‚â°‚ÉóaTM (see
page 205). We didn't use the term "transpose" then because trans-
posing is something you do to matrices. Instead, we used the math
term adjoint, which precisely describes the notion of the "transpose
of a linear transformation." Since we're on the topic of math termi-
nology, it should be noted that some mathematicians use the term
adjoint operator instead of adjoint linear transformation, since they
call operators what we call linear transformations.
Matrix quantum mechanics
Guess what?
Understanding linear algebra over the complex Ô¨Åeld
means you understand quantum mechanics! Quantum mechanics un-
folds in a complex inner product space (called a Hilbert space). If you
understood the material in this section, you should be able to under-
stand the axioms of quantum mechanics at no additional mental cost.
If you're interested in this kind of stuÔ¨Äyou should read Chapter 10.
Exercises
E7.18 Calculate (a) (2 + 5i) ‚àí(3 + 4i), (b) (2 + 5i)(3 + 4i), and (c)
(2 + 5i)/(3 + 4i).
7.8
Theory problems
It's now time to test your understanding by solving some problems.
P7.1
Yuna wants to cheat on her exam, she needs your help.
Please
help her to compute eigenvalues for the following matrices and slip her the
piece of paper carefully so the teacher doesn't notice. Yuna will give you a
chocolate bar to thank you.
a)
 3
1
12
2

b)
Ô£Æ
Ô£∞
0
1
0
2
0
2
0
1
0
Ô£π
Ô£ª
P7.2
Compute the eigenvalues of the matrix A = [ 1 1
1 0 ].

314
THEORETICAL LINEAR ALGEBRA
P7.3
Show that the vector ‚Éóe1 = (1, 1
œï)T and ‚Éóe2 = (1, ‚àíœï)T are eigenvectors
of the matrix A = [ 1 1
1 0 ]. What are the eigenvalues associated with these
eigenvectors?
Hint: Compute A‚Éóe1 and see what happens. Use the fact that œï satisÔ¨Åes
the equation œï2 ‚àíœï ‚àí1 = 0 to simplify expressions.
P7.4
We can write the matrix A = [ 1 1
1 0 ] as the product of three ma-
trices QŒõX, where Q contains the eigenvectors of A, and Œõ contains its
eigenvalues:
1
1
1
0

=
1
1
1
œï
‚àíœï

|
{z
}
Q
œï
0
0
‚àí1
œï

|
{z
}
Œõ
 ?
?
?
?

| {z }
X
.
Find the matrix X.
P7.5
Find eigenvalues for the matrices below:
a)
4
2
0
5

b)
3
1
1
2

c)
Ô£Æ
Ô£∞
2
0
1
1
2
0
0
4
‚àí1
Ô£π
Ô£ª
d)
Ô£Æ
Ô£∞
‚àí3
0
0
4
1
0
2
1
‚àí1
Ô£π
Ô£ª
P7.6
Compute the eigenvalues and eigenvectors of these matrices:
a)
0
1
1
0

b)
Ô£Æ
Ô£∞
0
1
0
0
0
1
‚àí6
‚àí1
4
Ô£π
Ô£ª
P7.7
Given A =
2
2
5
‚àí1

, Ô¨Ånd A10.
P7.8
Consider the sequence of triples {(xn, yn, zn)}n=0,1,2,... produced
according to the formula:
Ô£Æ
Ô£∞
1
2
1
2
0
1
8
3
4
1
8
0
1
2
1
2
Ô£π
Ô£ª
|
{z
}
M
Ô£Æ
Ô£∞
xn
yn
zn
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
xn+1
yn+1
zn+1
Ô£π
Ô£ª
Give a formula for (x‚àû, y‚àû, z‚àû) in terms of (x0, y0, z0). See youtu.be/mX0NB9IyYpU
to see how this recurrence relation is related to "surface smoothing" algo-
rithms used in 3D graphics.
Hint: Compute the eigenvalues Œª1, Œª2, and Œª3 of the matrix M. What will
become of the eigenvalues if you raise them to the power ‚àû?
P7.9
Check if the following matrices are orthogonal or not:
a)
Ô£Æ
Ô£∞
‚àí1
0
1
0
1
0
0
0
‚àí1
Ô£π
Ô£ª
b)
Ô£Æ
Ô£∞
1
‚àí1
1
1
‚àí1
‚àí1
0
1
0
Ô£π
Ô£ª
c)
Ô£Æ
Ô£ØÔ£ØÔ£∞
0
0
0
1
0
0
1
0
1
0
0
0
0
1
0
0
Ô£π
Ô£∫Ô£∫Ô£ª
P7.10
Let V be the set of two dimensional vectors of real numbers,
with addition deÔ¨Åned as (a1, a2) + (b1, b2) = (a1 + b1, a2b2) and scalar
multiplication deÔ¨Åned as c ¬∑ (a1, a2) = (ca1, a2).
Is (V, R, +, ¬∑) a vector
space? Justify your answer.

7.8 THEORY PROBLEMS
315
P7.11
Let V = {(a1, a2)}, with a1, a2 ‚ààR. DeÔ¨Åne vector addition as
(a1, a2) + (b1, b2) = (a1 + 2b1, a2 + 3b2) and scalar multiplication as c ¬∑
(a1, a2) = (ca1, ca2). Is (V, R, +, ¬∑) a vector space? Justify your answer.
P7.12
Prove the Cauchy-Schwarz inequality |‚ü®u, v‚ü©| ‚â§‚à•u‚à•‚à•v‚à•.
Hint: It is true that ‚à•a‚à•> 0 for any vector a. Use this fact to expand the
expression ‚à•u ‚àícv‚à•> 0, choosing c = ‚ü®u,v‚ü©
‚ü®v,v‚ü©.
P7.13
Prove the triangle inequality ‚à•u + v‚à•‚â§‚à•u‚à•+ ‚à•v‚à•.
Hint: Compute ‚à•u + v‚à•as an inner product and simplify the expression
using the fact ‚ü®a, b‚ü©‚â§‚à•a‚à•‚à•b‚à•for all vectors a and b.
P7.14
Perform the Gram-Shmidt orthogonalization procedure on the
following basis for R2: {(0, 1), (‚àí1, 0)}.
P7.15
Perform Gram-Shmidt orthogonalization on vectors ‚Éóv1 = (1, 1)
and ‚Éóv2 = (0, 1) to obtain an orthonormal basis.
P7.16
Convert the vectors (3, 1) and (‚àí1, 1) into an orthonormal basis.
P7.17
Find the eigendecomposition of the following matrix:
A =
Ô£Æ
Ô£∞
2
0
‚àí5
0
2
0
0
0
‚àí3
Ô£π
Ô£ª.
P7.18
Compute the following expressions: a) |3i ‚àí4|; b) 2 ‚àí3i; c) (3i ‚àí
1) + 3 ‚àí2i; d) |‚àí3i ‚àí‚àí4i + 5|
P7.19
Given matrices A, B, and C below, Ô¨Ånd A + B, CB and (2 + i)B.
A =
 2 + i
‚àí1 + 2i
3 + 2i
‚àí2i

B =
2 ‚àíi
3 ‚àí2i
5 + i
‚àí5 + 5i

C =
Ô£Æ
Ô£∞
1 + 2i
i
3 ‚àíi
8
4 + 2i
1 ‚àíi
Ô£π
Ô£ª
P7.20
Find the eigenvalues of the following matrices:
a)
3
‚àí2
1
1

b)
3
‚àí9
4
‚àí3

c)
3
‚àí13
5
1

P7.21
Give a basis for the vector space of 3 √ó 3 diagonal matrices.
P7.22
What is the dimension of the vector space of 3 √ó 3 symmetric
matrices.
Hint: See page 272 for deÔ¨Ånition.
P7.23
How many elements are there in a basis for the vector space of
3 √ó 3 Hermitian matrices.
Hint: See page 305 for deÔ¨Ånition.
P7.24
A matrix is nilpotent if it becomes the zero matrix when repeatedly
multiplied by itself. We say A is nilpotent if Ak = 0 for some power k.
A nilpotent matrix has only the eigenvalue zero, and hence its trace and
determinant are zero. Are the following matrices nilpotent?

316
THEORETICAL LINEAR ALGEBRA
a)
‚àí2
4
‚àí1
2

b)
3
1
1
3

c)
Ô£Æ
Ô£∞
‚àí3
2
1
‚àí3
2
1
‚àí3
2
1
Ô£π
Ô£ª
d)
Ô£Æ
Ô£∞
1
1
4
3
0
‚àí1
5
2
7
Ô£π
Ô£ª
e)
Ô£Æ
Ô£∞
45
‚àí22
‚àí19
33
‚àí16
‚àí14
69
‚àí34
‚àí29
Ô£π
Ô£ª
f)
Ô£Æ
Ô£∞
5
‚àí3
2
15
‚àí9
6
10
‚àí6
4
Ô£π
Ô£ª
P7.25
Determine all the eigenvalues of A =
 1+i
1
2
1‚àíi

. For each eigenvalue
Œª of A, Ô¨Ånd the set of eigenvectors corresponding to Œª. Determine whether
or not A is diagonalizable and if so Ô¨Ånd an invertible matrix Q and a
diagonal matrix Œõ such that Q‚àí1AQ = Œõ.
P7.26
Let ‚Éóv1,‚Éóv2,‚Éóv3 be vectors in the inner product space V .
Given
‚ü®‚Éóv1,‚Éóv2‚ü©= 3, ‚ü®‚Éóv2,‚Éóv3‚ü©= 2, ‚ü®‚Éóv1,‚Éóv3‚ü©= 1, ‚ü®‚Éóv1,‚Éóv1‚ü©= 1, and ‚ü®‚Éóv2,‚Éóv1 + ‚Éóv2‚ü©= 13,
calculate:
a)‚ü®‚Éóv1, 2‚Éóv2 + 3‚Éóv3‚ü©
b)‚ü®2‚Éóv1 ‚àí‚Éóv2,‚Éóv1 + ‚Éóv3‚ü©
c)‚à•‚Éóv2‚à•
P7.27
Consider the basis Ba = {1 + ix, 1 + x + ix2, 1 + 2ix}.
(a) Show that Ba is a basis for the space of polynomials with complex
coeÔ¨Écients of degree at most 2.

Chapter 8
Applications
In this chapter we'll learn about applications of linear algebra. We'll
cover a wide range of topics from diÔ¨Äerent areas of science, business,
and technology to give you an idea of the spectrum of things you can
do using matrix and vector algebra. Don't worry if you're not able to
follow all the details in each section‚Äîwe're taking a shotgun approach
here, covering topics from many diÔ¨Äerent areas in the hope to hit some
that will be of interest to you. Note that most of the material covered
in this chapter is not likely to show up on your linear algebra Ô¨Ånal,
so no pressure on you, this is just for fun.
Before we start, I want to say a few words about scientiÔ¨Åc ethics.
Linear algebra is a powerful tool for solving problems and modelling
the real world. But with great power comes great responsibility. I
hope you'll make an eÔ¨Äort to think about the ethical implications
when you use linear algebra to solve problems. Certain applications
of linear algebra, like building weapons, interfering with crops, and
building mathematically-complicated Ô¨Ånancial scams are clearly evil,
so you should avoid them. Other areas where linear algebra can be
applied are not so clear cut: perhaps you're building a satellite lo-
calization service to Ô¨Ånd missing people in emergency situations, but
the same technology might end up being used by governments to spy
on and persecute your fellow citizens. Do you want to be the person
responsible for bringing about an Orwellian state? All I ask of you is
to make a quick "System check" before you set to work on a project.
Don't just say "It's my job" and go right ahead. If you Ô¨Ånd what
you're doing at work to be unethical then maybe you should Ô¨Ånd a
diÔ¨Äerent job. There are a lot of jobs out there for people who know
math, and if the bad guys can't hire quality people like you, their
power will decrease‚Äîand that's a good thing.
Onto the applications.
317

318
APPLICATIONS
8.1
Balancing chemical equations
Suppose you're given the chemical equation H2 + O2 ‚ÜíH2O, which
indicates that hydrogen molecules (H2) and oxygen molecules (O2)
can combine to produce water molecules (H2O). Chemical equations
describe how a set of reactants are transformed to a set of products.
In this case the reactants are hydrogen and oxygen molecules and the
products are water molecules.
The equation H2 +O2 ‚ÜíH2O is misleading since it doesn't tell us
the correct stoichiometric ratios: how much of each type of molecule
is consumed and produced. We say the equation is not balanced. To
balance the equation we must add coeÔ¨Écients in front of each reactant
and each product so the total number of atoms on both sides of the
reaction is the same: 2H2 + O2 ‚Üí2H2O. Two hydrogen molecules
are required for each oxygen molecule, since water molecules contain
one oxygen and two hydrogen atoms.
Let's look at another example. The combustion of methane gas is
described by the following chemical equation:
CH4 + O2 ‚ÜíCO2 + H2O .
We want to answer the following two questions. How many molecules
of oxygen will be consumed during the combustion of 1000 molecules
of methane? How many CO2 molecules will be produced as a result?
Before we can answer such questions, we must Ô¨Ånd the coeÔ¨Écients
a, b, c, and d that balance the methane-combustion equation:
aCH4 + bO2 ‚ÜícCO2 + dH2O.
For the equation to be balanced, the same number of atoms of each
type must appear on each side of the equation.
For the methane
combustion reaction to be balanced, the following equations must be
satisÔ¨Åed:
a = c
for C atoms to be balanced,
4a = 2d
for H atoms to be balanced,
2b = 2c + d
for O atoms to be balanced.
We can move the c and d terms to the left side of each equation and
rewrite the system of equations as a matrix equation:
a ‚àíc
=
0
4a ‚àí2d
=
0
2b ‚àí2c ‚àíd
=
0
‚áí
Ô£Æ
Ô£∞
1
0
‚àí1
0
4
0
0
‚àí2
0
2
‚àí2
‚àí1
Ô£π
Ô£ª
|
{z
}
A
Ô£Æ
Ô£ØÔ£ØÔ£∞
a
b
c
d
Ô£π
Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£∞
0
0
0
Ô£π
Ô£ª.

8.1
BALANCING CHEMICAL EQUATIONS
319
We're looking for the vector of coeÔ¨Écients ‚Éóx = (a, b, c, d) which is the
solution to the null space problem A‚Éóx = ‚Éó0. See E5.7 for details. The
RREF of A contains three pivots, one free variable, and the solution
to the null space problem is
{(a, b, c, d)} = span{( 1
2, 1 , 1
2 , 1)}.
The solution is a one-dimensional inÔ¨Ånite space spanned by the vector
( 1
2, 1 , 1
2 , 1). The solution space is inÔ¨Ånite since any balanced equation
will remain balanced if we double, or triple the amount of reactants
and products. Choosing the coeÔ¨Écients as suggested by the solution
to the null space problem gives 1
2CH4 + O2 ‚Üí1
2CO2 + H2O, which is
a balanced equation. The convention in chemistry is to choose integer
coeÔ¨Écients for reactants and products, so we'll multiply the equation
by two to obtain the Ô¨Ånal answer
CH4 + 2O2 ‚ÜíCO2 + 2H2O.
Balancing chemical equation may not seem like the most exciting
technique ever, but it's a very useful skill to have when calculating
things with chemistry. It's good to know that substances A and B
can transform into substances C and D, but it's better to know how
much of each reactant is consumed and how much of each product
is produced per "unit of reaction."
Once we've identiÔ¨Åed one "unit
of reaction," we can calculate other quantities in terms of it, like
measuring the energy released per unit of reaction. The combustion
of 1[mol] (= 6.022 √ó 1023 molecules) of methane produces 890[kJ] of
heat:
CH4 + 2O2 ‚ÜíCO2 + 2H2O
+ 890 [kJ/mol].
Now this is cool. If you're heating your chalet with methane gas, and
you know how much joules of heat you'll need, then the balancing
chemical equation will help you calculate how many litres of methane
you need to stock to survive this winter.
Exercises
The exercises below aren't diÔ¨Écult, so you should totally try to solve
them. Going through them will give you some extra practice with the
Gauss-Jordan elimination procedure. It's been ages since Chapter 4
so a refresher can't hurt.
E8.1 Balance the chemical equation Al + O2 ‚ÜíAl2O3.
E8.2 Balance the equation Fe(OH)3 + HCl ‚ÜíFeCl3 + H2O.

320
APPLICATIONS
8.2
Input-output models in economics
Suppose you're the top economic oÔ¨Écial of a small country and you
want to make a production plan for the coming year. For the sake of
simplicity, let's assume your country produces only three commodi-
ties: electric power, wood, and aluminum. Your job is to choose the
production rates of these commodities: xe, xw, and xa. The country
must produce enough to satisfy both the internal demand and the
external demand for these commodities. The problem is complicated
because the production rates in one industry may aÔ¨Äect the produc-
tion rates of other industries.
For instance, it takes some electric
power to produce each unit of aluminum, so your production plan
must account for both external demand for electric power, as well
as internal demand for electric power for aluminum production. If
there exist complex interdependences between the diÔ¨Äerent internal
industries, as is often the case, it can be diÔ¨Écult to pick the right
production rates.
In reality, most high-ranking government oÔ¨Écials base their deci-
sions about which industry to sponsor based on the dollar amounts of
the kickbacks and bribes they received during the previous year. Let's
ignore reality for a moment and assume you're an honest economist
interested in using math to do what is right for the country instead
of abusing his/her position of power like a blood thirsty leech.
Let's assume the electric production xe must satisfy an external
demand of 25 units, plus an additional 0.05 units for each unit of
wood produced (electricity needed for saw mill operations) and an
additional 0.3 units for each unit of aluminum produced. The wood
production must be 10 units plus additional small amounts that de-
pend on xe and xa (wood for construction). The production of alu-
minum must match 14 units of external demand plus an additional
0.1 units for each unit of electric power (for repairs of electric cables).
We can model the interdependence between the industries using the
following system of equations:
xe
=
25
+
0.05xw
+
0.3xa
xw
=
10
+
0.01xe
+
0.01xa
xa
=
14
|{z}
external demand
+
0.1xe
|
{z
}
internal demand
.
You can use linear algebra to solve this complicated industry interde-
pendence problem, and choose appropriate production rates. Express

8.3
ELECTRIC CIRCUITS
321
the system of equations as a matrix equation:
Ô£Æ
Ô£∞
xe
xw
xa
Ô£π
Ô£ª
|{z}
‚Éóx
=
Ô£Æ
Ô£∞
25
10
14
Ô£π
Ô£ª
|{z}
‚Éód
+
Ô£Æ
Ô£∞
0
0.05
0.3
0.01
0
0.01
0.1
0
0
Ô£π
Ô£ª
|
{z
}
A
Ô£Æ
Ô£∞
xe
xw
xa
Ô£π
Ô£ª
|{z}
‚Éóx
This is known as a Leontief input-output model in honour of Wassily
Leontief who Ô¨Årst thought about applying linear algebra techniques
to economics, and was awarded the Nobel prize for this contribution.
To Ô¨Ånd the appropriate production rates, we must solve for the
unknown ‚Éóx = (xe, xw, xa) in the above equation. The equation ‚Éóx =
‚Éód+A‚Éóx is a little unusual, but we can solve it using standard techniques:
1‚Éóx = ‚Éód + A‚Éóx
‚áí
(1 ‚àíA)‚Éóx = ‚Éód
‚áí
‚Éóx = (1 ‚àíA)‚àí1‚Éód.
For the case of the electricity, wood, and aluminum production sce-
nario, the solution is ‚Éóx = (xe, xw, xa) = (30.64, 10.48, 17.06).
See
P4.9 for the details of the solution.
Note the electricity production rate is signiÔ¨Åcantly higher than the
external demand in order to account for the internal demand of elec-
tricity for the aluminum production. I'm not a big fan of economics
and economists but I must admit this is a pretty neat procedure!
Links
[ The Wikipedia article provides some historical context ]
http://en.wikipedia.org/wiki/Input-output_model
8.3
Electric circuits
We can use Ohm's law to solve many circuit problems. Ohm's law,
V = IR, tells us the voltage V required to "push" a current I through a
resistor with resistance R. This simple equation is enough to solve for
the currents and voltages in all circuit problems that involve batteries
and resistors.
Since we're in "math land," the units of the circuit quantities won't
play a direct role in our analysis, but it is still a good idea to introduce
the units because units can help us perform dimensional analysis of the
equations. Voltages are measured in volts [V], currents are measured
in Amperes [A], and resistance is measured in Ohms [‚Ñ¶]. Intuitively,
the resistance of a resistor measures how diÔ¨Écult it is to "push" current
through it. Indeed, the units for resistance have the dimensions of Volt
per Ampere: [‚Ñ¶] ‚â°[V/A]. The equation V = RI tells us how much
current I[A] Ô¨Çows through a resistor with resistance R[‚Ñ¶] connected

322
APPLICATIONS
to a voltage source with potential V [V]. Alternatively, if we know
the current I[A] and the resistance R[‚Ñ¶], we can Ô¨Ånd V , the voltage
applied to the resistor. A third way to use the equation V = RI is to
solve for the resistance R in cases when we know both V and I.
Example
Your friend gives you a 121[‚Ñ¶] light bulb (a resistor) and
asks you to connect it outdoors on the backyard porch to provide
some extra lighting for a summer party. You run to the basement
and Ô¨Ånd three diÔ¨Äerent spools of electric cable: a green one rated for
currents of up to 0.5[A], a blue one rated for currents of up to 1[A],
and a red spool of wire rated for currents of up to 2[A]. Knowing
that the voltage coming out of the wall socket1 is 110[V], what is the
smallest-rating wire you can use to connect the light bulb?
A simple calculation using V = RI shows the current that will
Ô¨Çow in the wire is I = V
R = 110[V]
121[‚Ñ¶] = 0.909[A]. Thanks to your cal-
culation, you choose the blue wire rated for 1[A] knowing you won't
have problems with wires overheating and causing a Ô¨Åre. Done. Now
the only problem remaining is mosquito bites!
Given a complicated electric circuit in which several voltage sources
(batteries) and resistors (light bulbs) are connected, it can be diÔ¨Écult
to "solve for" all the voltages and currents in the circuit. Using the
equation V = RI for reach resistor leads to several equations that
must be solved simultaneously to Ô¨Ånd the unknowns. Did someone
say system of linear equations? Linear algebra to the rescue!
Knowing linear algebra
will enable you to solve even
the most complicated cir-
cuit
using
row
operations
(Gauss-Jordan elimination)
in one or two minutes. We'll
illustrate this application of
linear algebra by solving the
example circuit shown on the
right, which involves three
batteries (the parallel lines labelled + and ‚àí) and three resistors (the
wiggly lines).
1The voltage coming out of wall outlets is actually not a constant 110[V] but
a sine-wave oscillating between +155[V] and ‚àí155[V], but in this example we'll
treat the socket's output as a constant voltage of 110[V].

8.3
ELECTRIC CIRCUITS
323
Theory
Before we get started, let me introduce the minimum information you
need to know about circuits: KirchhoÔ¨Ä's voltage law and KirchhoÔ¨Ä's
current law.
The voltages in a circuit are related to the electric potential energy
of the electrons Ô¨Çowing in the circuit. The electric potential is anal-
ogous to the concept of gravitational potential: a battery raises the
electric potential of electrons like an elevator raises the gravitational
potential of objects by increasing their height.
Starting from this
heightened potential, electrons will Ô¨Çow in the circuit and lose poten-
tial when passing through resistors. KirchhoÔ¨Ä's voltage law (KVL)
states that the sum of the voltage gains and losses along any loop in
the circuit must sum to zero. Intuitively, you can think of KVL as
a manifestation of the conservation of energy principle: the potential
gained by electrons when they pass through batteries must be lost in
the resistors (in the form of heat). By the time the electrons complete
their journey around any loop in the circuit, they must come back to
their initial potential.
KirchhoÔ¨Ä's current law states that the total current Ô¨Çowing into a
wire junction must equal the total current Ô¨Çowing out of the junction.
You can think of this a manifestation of the conservation of charge
principle: the total charge coming into a junction equals the total
charge Ô¨Çowing out of the junction, because charges cannot be created
or destroyed.
Using branch currents to solve the circuit
To "solve" a circuit is to Ô¨Ånd the currents that Ô¨Çow in each wire and
the voltage across each resistor in the circuit.
The Ô¨Årst step is to
deÔ¨Åne variables for each of the quantities of interest in the circuit as
shown in Figure 8.1. We'll call I1 the current that Ô¨Çows down through
the middle wire of the circuit, which then splits into the current I2 in
the left branch and the current I3 going to the right. Next we follow
the currents in the circuit and label each resistor with "+" and "‚àí"
sides to indicate the direction of the voltage drop across it. The rule
to follow is simple: the label "+" goes on the side where the current
enters the resistor, and the label "‚àí" goes on the side where the
current leaves the resistor. This is because electric potential always
drops when passing through a resistor.
We're now in a position to apply KirchhoÔ¨Ä's voltage and current
laws to this circuit and obtain a set of equations that relate the un-
known currents. Let's Ô¨Årst apply KirchhoÔ¨Ä's voltage law to the loop
along the path A-B-C-D-A, computing the total of the voltage gains

324
APPLICATIONS
Figure 8.1: The circuit with branch currents labelled. Each resistor is
assigned a polarity relative to the current Ô¨Çowing through it.
along this path:
+10 ‚àíR1I2 + 5 ‚àíR2I1 = 0.
Each battery produces a gain in potential for the electrons Ô¨Çowing
through it. Each resistor leads to a drop in potential (negative gain)
proportional to the current Ô¨Çowing through the resistor (recall V =
RI).
Similarly, we obtain a second KVL equation by following the path
A-F-E-D-A in the circuit:
‚àí20 ‚àíR3I3 ‚àíR2II = 0.
Note we're measuring voltage gains along this loop so we count the
20[V] battery as a negative gain (a voltage drop) since it is connected
against the Ô¨Çow of current I3.
We obtain a third equation linking the unknown currents from
KirchhoÔ¨Ä's current law applied to junction A:
I1 = I2 + I3.
Combining the three circuit equations, we obtain a system of three
linear equations in three unknowns:
+10 ‚àíR1I2 + 5 ‚àíR2I1 = 0,
‚àí20 ‚àíR3I3 ‚àíR2I1 = 0,
I1 = I2 + I3.
Do you see where this is going? Perhaps rewriting the equations into
the standard form we discussed in Section 4.1 will help you see what
is going on:
‚àíR2I1 ‚àíR1I2
= ‚àí15,
‚àíR2I1
‚àíR3I3 = 20,
I1
‚àíI2
‚àíI3 = 0.

8.3
ELECTRIC CIRCUITS
325
Rewriting the system of equations as an augmented matrix we obtain:
Ô£Æ
Ô£∞
‚àíR2
‚àíR1
0
‚àí15
‚àíR2
0
‚àíR3
20
1
‚àí1
‚àí1
0
Ô£π
Ô£ª.
Assume the values of the resistors are R1 = 1[‚Ñ¶], R2 = 2[‚Ñ¶], and
R3 = 1[‚Ñ¶]. We substitute these values into the augmented matrix
then Ô¨Ånd its reduced row echelon form:
Ô£Æ
Ô£∞
‚àí2
‚àí1
0
‚àí15
‚àí2
0
‚àí1
20
1
‚àí1
‚àí1
0
Ô£π
Ô£ª
‚Äîrref‚Üí
Ô£Æ
Ô£∞
1
0
0
‚àí1
0
1
0
17
0
0
1
‚àí18
Ô£π
Ô£ª.
The currents are I1 = ‚àí1[A], I2 = 17[A], and I3 = ‚àí18[A]. The volt-
age drop across R1 is 17[V] with polarity as indicated in the circuit.
Negative currents indicate the current in the circuit Ô¨Çows in the op-
posite direction of the current label in the diagram. The voltage drop
across R2 and R3 are 1[V] and 18[V] respectively, but with reverse
polarity of the one indicated in the circuit. Use Figure 8.1 to verify
these currents and voltages are consistent with the KVL equations we
started from.
Using loop currents to solve the circuit
We'll now discuss an alternative approach for solving the circuit. In
the previous section, we deÔ¨Åned "branch current" variables and ob-
tained two KVL equations and one KCL equation. Let's now solve
the same circuit problem by deÔ¨Åning "loop current" variables ad illus-
trated in Figure 8.2. We now deÔ¨Åne I1 to be the current circulating
in the left loop of the circuit, in the clockwise direction. Similarly,
deÔ¨Åne I2 to be the current in the right loop of the circuit, also Ô¨Çowing
in the clockwise direction.
The analysis of the circuit using the loop currents is similar to
what we saw above. The only tricky part is the voltage drop across
the resistor R2 through which both I1 and I2 Ô¨Çow. The voltage drop
across the resistor is a linear function of the currents Ô¨Çowing through
the resistor (V = RI), so we can analyze the eÔ¨Äects of the two cur-
rents separately and then add the results. This is an instance of the
superposition principle which is often used in physics. In the KVL
equation for the left loop, the voltage drop across R2 as the super-
position of the voltage drop ‚àíR2I1 and the voltage gain R2I2, which
combine to give the term ‚àíR2(I1 ‚àíI2). The opposite eÔ¨Äects occur in
the KVL equation for the right loop: I2 causes a voltage drop ‚àíR2I2
while I1 causes a voltage gain R2I1.

326
APPLICATIONS
Figure 8.2: The circuit with "loop currents" labelled. Note the resistor
R2 has two currents Ô¨Çowing through it: I1 downward and I2 upward.
We thus obtain the following KVL equations:
+10 ‚àíR1I1 + 5 ‚àíR2(I1 ‚àíI2) = 0
+R2(I1 ‚àíI2) ‚àíR3I2 + 20 = 0.
We can rearrange this system of equations into the form:
(R1 + R2)I1 ‚àíR2I2 = 15
R2I1 ‚àí(R2 + R3)I2 = ‚àí20,
and then use standard linear algebra techniques to solve for the un-
knowns in a few seconds.
Using the same values for the resistors as given above (R1 = 1[‚Ñ¶],
R2 = 2[‚Ñ¶], and R3 = 1[‚Ñ¶]), we obtain the following solution:
 3
‚àí2
15
2
‚àí3
‚àí20

‚Äîrref‚Üí
 1
0
17
0
1
18

.
The loop currents are I1 = 17[A] and I2 = 18[A]. This result is
consistent with the result we obtained using branch currents.
Linear independence
The abstract notion of linear independence
manifests in an interesting way in electric circuit problems: we must
choose the KVL equations that describe the current Ô¨Çowing in linearly
independent loops. For example, there are actually three loops in the
circuit from which we can obtain KVL equations: in addition to the
two small loops we studied above, we can also apply KVL on the
perimeter of the circuit as a whole: +10 ‚àíR1I1 + 5 ‚àíR3I2 + 20 = 0.
It would seem then, that we have a system of three equations in two
unknowns. However, the three equations are not independent: the
KVM equation for the outer loop is equal to the sum of the KVL
equations of the two small loops.

8.4
GRAPHS
327
The procedures based on the "branch currents" and "loop cur-
rents" outlined above can be used to solve any electric circuit. For
complicated circuits there will be a lot of equations, but using matrix
methods you should be able to handle even the most complicated cir-
cuit. You can always use SymPy to do the RREF computation if it
ever becomes too hairy to handle by hand.
Other network Ô¨Çows
The approach described above for Ô¨Ånding the Ô¨Çow of currents in a
circuit can be applied to many other problems with Ô¨Çows: the Ô¨Çow of
cars on city streets, the Ô¨Çow of goods and services between economies,
and the Ô¨Çow of information (data packets) through the Internet. In
each case, a diÔ¨Äerent law describes the Ô¨Çow in the network, but the
same matrix techniques can be used to solve the resulting systems of
linear equations.
8.4
Graphs
A graph is an abstract mathematical model that describes connections
between a set of nodes. We call the nodes vertices and the connections
edges. The graph is deÔ¨Åned as pair of sets G = (V, E), where V is the
set of vertices and E is the set of edges in the graph. We can also
describe the edges by specifying the adjacency matrix of the graph.
Rather than deÔ¨Åne graphs formally and in detail, we'll look at
a simple graph example to give you an idea of main concepts and
introduce graph notation. Figure 8.3 shows a small graph with Ô¨Åve
vertices and seven edges. This abstract link structure could represent
many real-world scenarios: Ô¨Åve websites and the hyperlinks between
them, Ô¨Åve Twitter accounts and their "following" relationships, or
seven Ô¨Ånancial transactions between Ô¨Åve businesses.
1
2
3
4
5
Figure 8.3: A simple graph with Ô¨Åve vertices and seven edges.
The graph in Figure 8.3 is represented mathematically as G = (V, E),
where V = {1, 2, 3, 4, 5} is the set of vertices, and E = {(1, 2), (1, 3),
(2, 3), (3, 5), (4, 1), (4, 5), (5, 1)} is the set of edges. Note the edge
from vertex i to vertex j is represented as the pair (i, j).

328
APPLICATIONS
Adjacency matrix
The adjacency matrix representation of this graph in Figure 8.3 is a
5 √ó 5 matrix A that contains the information about the edges in the
graph. SpeciÔ¨Åcally, Aij = 1 if the edge (i, j) exists, otherwise Aij = 0
if the edge doesn't exist:
A =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
1
1
0
0
0
0
1
0
0
0
0
0
0
1
1
0
0
0
1
1
0
0
0
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Each row contains ones in the positions where edges exist. The adja-
cency matrix representation works in tandem with the integer labels
of the vertices‚ÄîVertex 1 corresponds to the Ô¨Årst row of A, Vertex 2
to the second row, and so on for the other rows. We don't need labels
for the vertices since the labels can be deduced from their position in
the matrix A.
Applications
The adjacency matrix of a graph can be used to answer certain ques-
tions about the graph's properties. For example, powers of the ad-
jacency matrix A tell us information about the connectivity in the
graph. The number ways to get from vertex i to vertex j in two steps
is (A2)ij‚Äîthe ijth entry of A2:
A2 =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
0
1
0
1
0
0
0
0
1
1
0
0
0
0
1
1
1
0
0
0
1
1
0
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
The number of ways to get from vertex i to vertex j in zero, one, or
two steps is 1 + A + A2:
1 + A + A2 =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
1
2
0
1
0
1
1
0
1
1
0
1
0
1
2
1
1
1
1
1
1
1
0
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Observe that most of the graph G is well connected, except for Ver-
tex 4, which has no inbound edges. The only way to get to Vertex 4
is if we start there.

8.4
GRAPHS
329
The fact we can discuss graph connectivity by doing matrix algebra
is amazing when you think about it! The entry (1+A+A2)41 = 2 tells
us there are two ways to get from Vertex 4 to Vertex 1 in two steps
or less.
Indeed we can either transition directly through the edge
(4, 1) in one step, or indirectly via Node 5 in two steps by passing
through the edges (4, 5) and (5, 1). Rather than manually counting
all possible paths between vertices, we can compute all possible paths
at once using matrix algebra on the adjacency matrix A.
Discussion
The analysis of connectivity between vertices is used in many domains.
Graphs are used to describe network Ô¨Çows, matching problems, social
networks, webpages, and many other. In all these domains, the adja-
cency matrix plays a key role in graph representation. In Section 9.3
we'll study the graph of all webpages on the Web and discuss the
Google's PageRank algorithm that uses the information in the adja-
cency matrix of the Web to compute an "importance" rank for each
webpage.
Links
[ Graph theory and its applications ]
https://en.wikipedia.org/wiki/Graph_theory
[ More details about adjacency matrices with examples ]
https://en.wikipedia.org/wiki/Adjacency_matrix
Exercises
E8.3 Find the adjacency matrix representation of the following graphs:
a)
b)
c)
E8.4 For each of the graphs in E8.3, Ô¨Ånd the number of ways to get
from vertex 1 to vertex 3 in two steps or less.
Hint: You can obtain the answer by inspection or by looking at the
appropriate entry of the matrix 1 + A + A2.

330
APPLICATIONS
8.5
Fibonacci sequence
We'll now look at a neat trick for computing the N th term in the Fi-
bonacci sequence (0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, . . .). The terms
in the Fibonacci sequence (a0, a1, a2, . . .) start with a0 = 0, a1 = 1,
and then each subsequent term is computed as the sum of the two
terms preceding it:
a0 = 0,
a1 = 1,
an = an‚àí1 + an‚àí2, for all n ‚â•2.
You can apply this formula (the technical term for this type of formula
is recurrence relation) to compute the N th term in the sequence aN.
Using the formula to compute the 1000th term in the sequence you'll
have to do about 1000 steps of arithmetic to obtain a1000. But do
we really need N steps to compute aN? In this section we'll learn
an eigenvalue trick that allows us to compute aN in just Ô¨Åve step of
symbolic computation, no matter how big N is!
First we express the recurrence relation as a matrix product:
an+1
=
an + an‚àí1
an
=
an
‚áí
an+1
an

| {z }
‚Éóan
=
1
1
1
0

| {z }
A
 an
an‚àí1

| {z }
‚Éóan‚àí1
.
We can compute the N th term in the Fibonacci sequence by starting
from the initial column vector ‚Éóa0 = (a1, a0)T, and repeatedly multi-
plying by the matrix A:
an+1
an

= AN
a1
a0

.
We can "extract" aN from the vector ‚ÉóaN by computing the dot product
of ‚ÉóaN with the vector (0, 1). This dot product operation has the eÔ¨Äect
of "selecting" the second entry of the vector ‚ÉóaN.
Thus, we obtain the following compact formula for computing the
N th term in the Fibonacci sequence in terms of the N th power of the
matrix A:
aN = (0, 1)AN(1, 0)T.
Do you remember the eigendecomposition trick for computing powers
of matrices by only computing powers of their eigenvalues? We can
use the eigendecomposition trick to compute AN very eÔ¨Éciently. The
Ô¨Årst step is to Ô¨Ånd the eigendecomposition of the matrix A:
1
1
1
0

=
1
1
1
œï
‚àíœï

|
{z
}
Q
œï
0
0
‚àí1
œï

|
{z
}
Œõ
"
5+
‚àö
5
10
‚àö
5
5
5‚àí
‚àö
5
10
‚àí
‚àö
5
5
#
|
{z
}
Q‚àí1
.

8.5
FIBONACCI SEQUENCE
331
Here Œª1 = œï = 1+
‚àö
5
2
‚âà1.618 . . . (the golden ratio) and Œª2 = ‚àí1
œï =
1‚àí
‚àö
5
2
‚âà‚àí0.618 . . . (the negative inverse of the golden ratio) are the
two eigenvalues of A. The columns of Q contain the corresponding
eigenvectors of A.
We can compute AN using the following formula:
AN = AA ¬∑ ¬∑ ¬∑ A
|
{z
}
N times
= QŒõQ‚àí1 QŒõQ‚àí1 ¬∑ ¬∑ ¬∑ QŒõQ‚àí1
|
{z
}
N times
= QŒõNQ‚àí1.
To compute AN it is suÔ¨Écient to compute the N th powers of the
eigenvalues Œª1 = œï and Œª2 =
‚àí1
œï .
For example, to compute a5,
the Ô¨Åfth element in the Fibonacci sequence, we compute A5 using
QŒõ5Q‚àí1, then use the formula a5 = (0, 1)A5(1, 0)T:
a5 =
0
1
1
1
1
0
5
1
0

=
0
1
Q
œï5
0
0
‚àí1
œï5

Q‚àí1

1
0

=
0
1
8
5
5
3

1
0

=
0
1
8
5

= 5.
We can just as easily compute A55:
A55 = QŒõ55Q‚àí1 =
225851433717
139583862445
139583862445
86267571272

,
then compute a55 using the formula
a55 = (0, 1)A55(1, 0)T = 139583862445.
Using the eigendecomposition trick allows us to take a "mathematical
shortcut" and obtain the answer aN in a constant number of math
operations‚Äîregardless of the size of N. The steps are: compute the
N th power of Œõ, then multiply ŒõN by Q‚àí1 and (1, 0)T on the right, and
by Q and (0, 1) on the left. This is interesting since other algorithms
for computing the Fibonacci numbers usually take a number of steps
proportional to the size of N.
There are some caveats to the approach outlined above. We as-
sumed computing œïN is a constant-time operation, which is not a
realistic assumption on any computer. Also, inÔ¨Ånite-precision (sym-
bolic) manipulations are not realistic either because computers work
with Ô¨Ånite-precision approximations to real numbers, so the eigen-
value trick will not work for large N. The eigenvalue trick is only a
theoretical result and can't be used in practical programs.

332
APPLICATIONS
Links
[ See the Wikipedia page for more on the Fibonacci numbers ]
https://en.wikipedia.org/wiki/Fibonacci_number
Exercises
E8.5 Describe what happens to the ratio an+1
an
as n ‚Üí‚àû.
Hint: Consider what happens to the powers of two eigenvalues Œªn
1 and
Œªn
2 for large n.
E8.6 Compute the matrix products in the expression (0, 1)QŒõNQ‚àí1(1, 0)T
to obtain a closed form expression for aN.
Hint:
8.6
Linear programming
In the early days of computing, computers were primarily used to
solve optimization problems so the term "programming" is often used
to describe optimization problems. Linear programming is the study
of linear optimization problems that involve linear constraints. This
type of optimization problems play an important role in business: the
whole point of corporations is to constantly optimize proÔ¨Åts, subject
to time, energy, and legal constraints.
Many optimization problems can be expressed as linear programs:
max
x1,x2,...,xn g(x1, x2, . . . , xn) = c1x1 + c2x2 + ¬∑ ¬∑ ¬∑ + cnxn,
subject to constraints:
a11x1 + a12x2 + ¬∑ ¬∑ ¬∑ + a1nxn ‚â§b1,
a21x1 + a22x2 + ¬∑ ¬∑ ¬∑ + a2nxn ‚â§b2,
...
am1x1 + am2x2 + ¬∑ ¬∑ ¬∑ + amnxn ‚â§bm,
x1 ‚â•0,
x2 ‚â•0,
. . . ,
xn ‚â•0.
For example, the variables x1, x2, . . . , xn could represent the produc-
tion rates of n diÔ¨Äerent products made by a company. The coeÔ¨É-
cients c1, c2, . . . , cn represent the price of selling each product and
g(x1, x2, . . . , xn) represents the overall revenue. The m inequalities
could represent various limitation of human resources, production ca-
pacity, or logistics constraints.
We want to choose the production
rates x1, x2, . . . , xn that maximize the revenue, subject to the con-
straints.

8.7
LEAST SQUARES APPROXIMATE SOLUTIONS
333
The simplex algorithm is a systematic procedure for Ô¨Ånding so-
lutions to linear programming problems. The simplex algorithm is
somewhat similar to the Gauss-Jordan elimination procedure since it
uses row operations on a matrix-like structure called a tableau. For
this reason, linear programming and the simplex algorithm are often
inÔ¨Çicted upon students taking a linear algebra course, especially busi-
ness students. I'm not going to lie to you and tell you the simplex
algorithm is very exciting, but it is very powerful so you should know
it exists, and develop a general intuition about how it works. Like
with all things corporate-related, it's worth learning about it so you'll
know the techniques of the enemy.
Since the details of the simplex algorithm might not be of inter-
est to all readers of the book, this topic was split out as a separate
tutorial, which you can read online at the link below.
[ Linear programming tutorial ]
https://minireference.github.io/linear_programming/tutorial.pdf
8.7
Least squares approximate solutions
An equation of the form A‚Éóx = ‚Éób could have exactly one solution (if A
is invertible), inÔ¨Ånitely many solutions (if A has a null space), or no
solution at all (if ‚Éób is not in the column space of A). In this section
we'll discuss the case with no solution and describe an approach for
computing an approximate solution ‚Éóx‚àósuch that the vector A‚Éóx‚àóis as
close as possible to ‚Éób.
We could jump right away to the formula for the least squares
approximate solution (‚Éóx‚àó= (ATA)‚àí1AT‚Éób), but this would hardly be
enlightening or useful for your understanding.
Instead, let's learn
about the least squares approximate solution in the context of a ma-
chine learning problem in which we'll try to predict some unknown
quantities based on a linear model "learned" from past observations.
This is called linear regression and is one of the most useful applica-
tions of linear algebra.
The database of current clients of your company contains all the
information about the frequency of purchases f, value of purchases
V , promptness of payment P, and other useful information.
You
know what is really useful information though? Knowing the customer
lifetime value (CLV)‚Äîthe total revenue this customer will generate
during their entire relationship with your company. You have data
on the CLVs of existing customers and you want to leverage this data
to predict the CLVs of new customers.
You're given the proÔ¨Åle parameters for N existing customers in the
form of a vector ‚Éóai ‚â°(fi, Vi, Pi, . . .) and calculated a customer life-

334
APPLICATIONS
time value (CLV) for each existing customer bi ‚â°CLV . The dataset
consists of observations ‚Éóai and outcomes bi:
D =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
‚Äî
‚Éóa1
‚Äî
‚Äî
‚Éóa2
‚Äî
‚Äî
‚Éóa3
‚Äî
...
‚Äî
‚ÉóaN
‚Äî
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
b1
b2
b3
...
bN
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£º
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£æ
= {A,‚Éób}.
The clients' observational data is stored in an N √ó n matrix A and
the corresponding CLVs are stored as a N √ó 1 column vector ‚Éób.
Statement of the problem
Given the ‚Éóak of a new customer, pre-
dict the customer's bk, based on the information in the dataset D.
Linear model
A simple way to model the dependence of the label bi on the obser-
vational data ‚Éóai = (ai1, ai2, . . . , ain) is to use a linear model with n
parameters m1, m2, . . . , mn:
y‚Éóm(x1, x2, . . . , xn) = m1x1 + m2x2 + ¬∑ ¬∑ ¬∑ + mnxn = ‚Éóm ¬∑ ‚Éóx.
If the model is good then y‚Éóm(‚Éóai) approximates bi well. But how should
we measure the quality of the approximation?
Enter the error term that describes how the model's prediction
y‚Éóm(‚Éóai) diÔ¨Äers form the observed value bi. The error term for the ith
customer is
ei(‚Éóm) = |y‚Éóm(‚Éóai) ‚àíbi|2.
The expression ei(‚Éóm) measures the squared distance between the
model's prediction and the known value.
Our goal is to choose a
model that makes the sum of all the error terms as small as possible:
S(‚Éóm) =
N
X
i=1
ei(‚Éóm) =
N
X
i=1
|y‚Éóm(‚Éóai) ‚àíbi|2 .
Intuitively, S(‚Éóm) is a good objective function to minimize because
S(‚Éóm) = 0 if the model perfectly predicts the data. Any model pre-
diction that overshoots or undershoots the correct bi will be "penal-
ized" by a large error term. Observe the objective function S(‚Éóm) is
a quadratic function, so the "penalties" grow quadratically with the
size of the discrepancy between the model and actual values.

8.7
LEAST SQUARES APPROXIMATE SOLUTIONS
335
Linear algebra formulation
When faced with an unfamiliar problem like Ô¨Ånding a quadratically-
penalized approximate solution to a system of equations A‚Éóx = ‚Éób with
no exact solution, you shouldn't be alarmed but stand your ground.
Try to relate the problem to something you're more familiar with.
Thinking about problems in terms of linear algebra can often unlock
your geometrical intuition and show you a path toward the solution.
Using a matrix-vector product we can express the "vector predic-
tion" of the model ‚Éóy‚Éóm for the whole dataset in one shot: ‚Éóy‚Éóm = A‚Éóm.
The "total squared error" function for the model ‚Éóm on the dataset
D = {A,‚Éób} can be written as the following expression:
S(‚Éóm) =
N
X
i=1
|y‚Éóm(‚Éóai) ‚àíbi|2
= ‚à•‚Éóy‚Éóm ‚àí‚Éób‚à•2
= ‚à•A‚Éóm ‚àí‚Éób‚à•2.
The total squared error for the model ‚Éóm on the dataset {A,‚Éób} is the
squared-length of the vector A‚Éóm ‚àí‚Éób.
In the ideal case when the model perfectly matches the observa-
tions, the total squared error is zero and the equation A‚Éóm = ‚Éób will
have a solution:
A‚Éóm ‚àí‚Éób = 0
‚áí
A‚Éóm = ‚Éób.
In practice, the model predictions A‚Éóm will never perfectly match the
data ‚Éób so we must be content with approximate solution ‚Éóm:
A‚Éóm ‚âà‚Éób.
The leasts-squares approximate solution to this equation is chosen so
as to minimize the total squared error function:
min
‚Éóm S(‚Éóm)
=
min
‚Éóm
A‚Éóm ‚àí‚Éób

2
.
In other words, of all the possible approximate solutions ‚Éóm, we must
pick the one that makes the length of the vector A‚Éóm ‚àí‚Éób the smallest.
Finding the least-squares approximate solution
There are two possible approaches for Ô¨Ånding the least-squares so-
lution, denoted ‚Éóm‚àó. We can either use calculus techniques to mini-
mize the total squared error S(‚Éóm), or geometry techniques to Ô¨Ånd the
shortest vector (A‚Éóm ‚àí‚Éób).

336
APPLICATIONS
Regardless of the approach chosen, the trick to Ô¨Ånding the least-
squares approximate solution to the equation A‚Éóm ‚âà‚Éób is to multiply
the equation by AT to obtain the equation:
ATA ‚Éóm = AT‚Éób.
The matrix ATA will be invertible if the columns of A are linearly
independent, which is the case for most tall-and-skinny matrices. We
can therefore solve for ‚Éóm by multiplying by the matrix (ATA)‚àí1:
‚Éóm = (ATA)‚àí1AT‚Éób.
Indeed, this expression is the least-squares solution to the optimiza-
tion problems we set out to solve in the beginning of this section:
‚Éóm‚àó= argmin
‚Éóm
S(‚Éóm) = argmin
‚Éóm
A‚Éóm ‚àí‚Éób

2
= (ATA)‚àí1AT‚Éób.
Pseudoinverse
The particular combination of A, its transpose AT,
and the inverse operation that we used to Ô¨Ånd the approximate solu-
tion ‚Éóm‚àóis called the Moore-Penrose pseudoinverse of the matrix A.
We use the shorthand notation A+ (not to be confused with A‚Ä†), for
the entire expression:
A+ ‚â°(ATA)‚àí1AT.
Applying A+ to both sides of the approximate equation A‚Éóm ‚âà‚Éób, is
analogous to "solving" the equation by applying the inverse:
A+A‚Éóm = A+‚Éób,
‚Éóm = A+‚Éób,
but the solution is approximate since A‚Éóm Ã∏= ‚Éób. The solution A+‚Éób is
optimal according to the total squared error criterion S(‚Éóm).
Example 1
Given the dataset with six samples (xi, bi): (105, 45),
(113, 63), (125, 86), (137, 118), (141, 112), (153, 169), what is the best
linear model ym(x) = mx for predicting b given x? The Ô¨Årst step
is to express the given samples in the standard form {A,‚Éób}, where
A ‚ààR6√ó1 and ‚Éób ‚ààR6√ó1. We then calculate the best parameter m
using the Moore-Penrose inverse formula:
{A,‚Éób} ‚â°
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
105
113
125
137
141
153
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
45
63
86
118
112
169
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£º
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£æ
‚áí
m‚àó= (ATA)‚àí1AT‚Éób = 0.792.

8.7
LEAST SQUARES APPROXIMATE SOLUTIONS
337
The best Ô¨Åt linear model to the samples is the equation y(x) = 0.792x.
For the details of the calculation, see bit.ly/leastsq_ex1.
Fig-
ure 8.4 shows a scatter plot of the dataset {(xi, bi)} and the graph of
the best Ô¨Åt linear model through the data.
Figure 8.4: The "best Ô¨Åt" line of the form ym(x) = mx through a set of
data points points (xi, bi) ‚ààR2. The plot shows a scatter plot of the data
points and the best Ô¨Åt line y(x) = 0.792x. The best Ô¨Åt line passes through
the middle of the dataset and minimizes the sum of the squared vertical
diÔ¨Äerences between the model output ym(xi) and the true value bi.
Geometric interpretation
The solution to the least squares optimization problem,
‚Éóm‚àó= argmin
‚Éóm
‚à•A‚Éóm ‚àí‚Éób‚à•2,
can be understood geometrically as the search for the vector in the
column space of A that is closest to the vector ‚Éób, as illustrated in
Figure 8.5. As we vary the parameter vector ‚Éóm, we obtain diÔ¨Äerent
vectors A‚Éóm ‚ààC(A). Of all the points A‚Éóm in the column space of A,
the point A‚Éóm‚àóis the closest to the point ‚Éób.
Let's deÔ¨Åne the "error vector" that corresponds to the diÔ¨Äerence
between the model prediction A‚Éóm and the actual value ‚Éób:
‚Éóe
‚â°
A‚Éóm ‚àí‚Éób.
Using the geometric intuition from Figure 8.5, we see that the optimal
solution A‚Éóm‚àóoccurs when the error vector is perpendicular to the col-
umn space of A. Recall the left-fundamental spaces of the matrix A:
its column space C(A) and its orthogonal complement, the left null
space N(AT). Thus, if we want an error vector that is perpendicular
to C(A), we must Ô¨Ånd an error vector that lies in the left null space

338
APPLICATIONS
of A: ‚Éóe ‚àó‚ààN(AT). Using the deÔ¨Ånition of left null space,
N(AT) ‚â°{‚Éów ‚ààRN | ‚ÉówTM = ‚Éó0},
we obtain the following equation that deÔ¨Ånes ‚Éóm‚àó:
(‚Éóe ‚àó)TA = 0
‚áí
(A‚Éóm‚àó‚àí‚Éób)TA = 0.
Taking the transpose of the last equation we obtain AT(A‚Éóm‚àó‚àí‚Éób) = 0,
which is equivalent to condition ATA‚Éóm‚àó= AT‚Éób used to Ô¨Ånd ‚Éóm‚àó.
Figure 8.5: Linear regression can be seen as a "search" problem restricted
to the column space of the data matrix A. The least-squares approximate
solution ‚Éóm‚àó= A+‚Éób corresponds to the point in C(A) that is closest to ‚Éób.
For this ‚Éóm‚àó, the error vector ‚Éóe ‚àóis perpendicular to the column space of A.
Using geometric intuition about vector spaces and orthogonal-
ity proves to useful for solving this complex optimization problem.
Choosing ‚Éóe ‚àóorthogonal to C(A), leads to the shortest vector A‚Éóm‚àó‚àí‚Éób,
and produces the smallest total squared error S(‚Éóm‚àó) = ‚à•A‚Éóm‚àó‚àí‚Éób‚à•2.
AÔ¨Éne models
The Moore-Penrose pseudoinverse formula can be used to Ô¨Åt more
complicated models. A simple extension of a linear model is the aÔ¨Éne
model y‚Éóm,c(‚Éóx) = ‚Éóm¬∑‚Éóx+c, which adds a constant term c to the output
of a linear model. The model parameters correspond to an (n + 1)-
vector ‚Éóm‚Ä≤ = (c, m1, m2, . . . , mn):
y‚Éóm‚Ä≤(x1, x2, . . . , xn) = m0 + m1x1 + m2x2 + ¬∑ ¬∑ ¬∑ + mnxn = ‚Éóm‚Ä≤ ¬∑ (1, ‚Éóx).
For uniformity of notation, we'll refer to the constant term as m0
instead of c. You can think of the parameter m0 as the y-intercept of
the model.

8.7
LEAST SQUARES APPROXIMATE SOLUTIONS
339
To accommodate this change in the model, we must preprocess
the dataset D = {A,‚Éób} to add a column of ones to the matrix A and
turn it into an N √ó(n+1) matrix A‚Ä≤. The new dataset looks like this:
D‚Ä≤ =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
‚Äî
‚Éóa1
‚Äî
1
‚Äî
‚Éóa2
‚Äî
1
‚Äî
‚Éóa3
‚Äî
...
...
1
‚Äî
‚ÉóaN
‚Äî
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
b0
b1
b2
...
bN
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£º
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£æ
= {A‚Ä≤,‚Éób}.
Except for the preprocessing step which commonly called "data-massaging,"
the remainder of the steps for Ô¨Ånding a least squares solution are the
same as in the case of linear regression. We Ô¨Ånd the optimal parameter
vector ‚Éóm‚Ä≤‚àóby applying the Moore-Penrose pseudoinverse formula:
‚Éóm‚Ä≤‚àó= (A‚Ä≤TA‚Ä≤)‚àí1A‚Ä≤T‚Éób.
Example 2
Find the best Ô¨Åt aÔ¨Éne model y‚Éóm‚Ä≤(x) = m0 + m1x to
the data points from Example 1 (page 336). To Ô¨Ånd the best param-
eter vector ‚Éóm‚Ä≤ = (m0, m1)T, we Ô¨Årst preprocess the dataset adding
a column of ones to the data matrix then apply the pseudoinverse
formula:
{A‚Ä≤,‚Éób} ‚â°
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
105
1
113
1
125
1
137
1
141
1
153
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
45
63
86
118
112
169
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£º
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£æ
‚áí
‚Éóm‚Ä≤‚àó= A‚Ä≤+‚Éób =
‚àí210.4
2.397

.
The best-Ô¨Åt aÔ¨Éne model is y(x) = ‚àí210.4 + 2.397x. We omit the
details of the matrix calculation for brevity, but you can verify ev-
erything is legit here: bit.ly/leastsq_ex2. See Figure 8.6 for the
graph of the best-Ô¨Åt aÔ¨Éne model.

340
APPLICATIONS
Figure 8.6: The best Ô¨Åt aÔ¨Éne model y ‚Éóm‚Ä≤(x) = m0+m1x through the data
points is the line y(x) = ‚àí210.4+2.397x. Allowing for one extra parameter
m0 (the y-intercept), leads to a much better Ô¨Åtting model to the data, as
compared to the Ô¨Åt in Figure 8.4.
Note the terms linear regression and linear least squares are used to
refer to Ô¨Åtting both linear models ‚Éóy‚Éóm = ‚Éóm ¬∑ ‚Éóx, and aÔ¨Éne models
‚Éóy‚Éóm ‚Ä≤ = ‚Éóm‚Ä≤ ¬∑(1, ‚Éóx). After all, an aÔ¨Éne model is just a linear model with
a y-intercept, so it makes sense to refer to them by the same name.
Quadratic models
Linear algebra techniques can also be used to Ô¨Ånd approximate so-
lutions for nonlinear models. For the sake of simplicity, let's assume
there are only two observed quantities in the dataset (n = 2). The
most general quadratic model for two variables is:
y‚Éóm(x, y) = m0 + m1x + m2y + m3xy + m4x2 + m5y2.
The parameter vector is six-dimensional: ‚Éóm = (m0, m1, m2, m3, m4, m5).
Assuming we start from the following dataset
D =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£≥
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
x1
y1
x2
y2
...
...
xN
yN
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
b1
b2
...
bN
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
Ô£º
Ô£¥
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£¥
Ô£æ
= {A,‚Éób}.
To use the Moore-Penrose pseudo-inverse formula, we must prepro-
cess A by adding several new columns:
D‚Ä≤ =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
x1
y1
x1y1
x2
1
y2
1
1
x2
y2
x2y2
x2
2
y2
2
...
...
...
...
...
...
1
xN
yN
xNyN
x2
N
y2
N
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
b1
b2
...
bN
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
Ô£º
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£æ
= {A‚Ä≤,‚Éób}.

8.7
LEAST SQUARES APPROXIMATE SOLUTIONS
341
This preprocessing step allows us to compute a vector prediction of the
model with parameters ‚Éóm on the entire dataset: ‚Éóy‚Éóm ‚â°A‚Ä≤ ‚Éóm. The total
squared error for this model is S(‚Éóm) = ‚à•‚Éóy‚Éóm ‚àí‚Éób‚à•2. The least squares
approximate solution ‚Éóm‚àóis obtained as usual: ‚Éóm‚àó= (A‚Ä≤TA‚Ä≤)‚àí1A‚Ä≤T‚Éób.
Note the number of parameters in a quadratic model grows‚Äî
surprise, surprise‚Äîquadratically. There were n = 2 variables in the
above example and the parameters vector ‚Éóm is six-dimensional. The
the number of parameters of a general quadratic model in n variables
is 1
2(n + 1)(n + 2) = 1
2(n2 + 3n + 2). The number of parameters we
need to "learn" is an important consideration to take into account
when Ô¨Åtting models to large datasets.
Example 3
Imagine you're a data-friendly business person and you
want to pick which model to Ô¨Åt to a dataset {A, b} with n = 1000
features (the columns of A).
You have the option of choosing an
aÔ¨Éne model ya(‚Éóx) = m0 + m1x1 + ¬∑ ¬∑ ¬∑ + mnxn, or a quadratic model
yq(‚Éóx) = m0 + m1x1 + ¬∑ ¬∑ ¬∑ + mnxn + mn+1x1y1 + mn+2x1y2 + ¬∑ ¬∑ ¬∑ .
To Ô¨Åt an aÔ¨Éne model, you'll need to preprocess A ‚ààRN√ó1000 into
A‚Ä≤ ‚ààRN√ó1001, then Ô¨Ånd the inverse of (A‚Ä≤TA‚Ä≤) ‚ààR1001√ó1001. That's
totally doable. Probably in your web browser.
In contrast, the number of parameters in a quadratic model with
n = 1000 dimensions is 1
2(1000 + 1)(1000 + 2) = 501501. To Ô¨Åt a
quadratic model you'll need to preprocess the data matrix to obtain
A‚Ä≤ ‚ààRN√ó501501, then Ô¨Ånd the inverse of (A‚Ä≤TA‚Ä≤) ‚ààR501501√ó501501.
That's a big matrix. You'll need one terabyte of memory just to store
all the entries of the matrix A‚Ä≤TA‚Ä≤, and computing its inverse will take
a really long time.
So the choice is made; aÔ¨Éne model it is. Who cares if the model is
less accurate than a quadratic model? If it works and provides value,
you should use it. This is the reason why scientists, engineers, statis-
ticians, and business folk are so crazy about building linear models,
even though more advanced models are available. Linear models are
the sweet spot: they have great modelling power, they're easy to im-
plement, and they lead to computational problems that are easy to
solve.
Links
[ Further discussion about least squares problems on Wikipedia ]
https://en.wikipedia.org/wiki/Linear_regression
https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)
[ More about the Moore-Penrose pseudoinverse ]
https://en.wikipedia.org/wiki/Moore-Penrose_pseudoinverse

342
APPLICATIONS
Exercises
E8.7 Calculate the total squared error S(m‚àó) = ‚à•Am‚àó‚àí‚Éób‚à•2 of the
best Ô¨Åt linear model obtained in Example 1 (page 336).
Hint: The Matrix method .norm() might come in handy.
E8.8 Revisit Example 2 (page 339) and Ô¨Ånd the total squared error
of the best-Ô¨Åt aÔ¨Éne model S(‚Éóm‚Ä≤‚àó) = ‚à•A‚Éóm‚Ä≤‚àó‚àí‚Éób‚à•2.
8.8
Computer graphics
Linear algebra is the mathematical language of computer graphics.
Whether you're building a simple two-dimensional game with stick
Ô¨Ågures or a fancy three-dimensional visualization, knowing linear al-
gebra will help you understand the graphics operations that draw
pixels on the screen.
In this section we'll discuss some basic computer graphics con-
cepts. In particular, we'll introduce homogenous coordinates which
are representations for vectors and matrices that use an extra di-
mension.
Homogenous coordinates allow us to represent all inter-
esting computer graphics transformations as matrix-vector products.
We've already seen that scalings, rotations, reÔ¨Çections, and orthogo-
nal projections can be represented as matrix-vector products; using
homogenous coordinates we'll be able to represent translations and
perspective projections as matrix products too. That's very conve-
nient, since it allow the entire computer graphics processing "pipeline"
to be understood in terms of a sequence of matrix multiplications.
Computer graphics is a vast subject and we don't have the space
here to go into depth.
To keep things simple, we'll focus on two-
dimensional graphics, and only brieÔ¨Çy touch upon three-dimensional
graphics. The goal is not to teach you the commands of computer
graphics APIs like OpenGL and WebGL, but to give you the basic
math tools you'll need to understand what is going under the hood.
AÔ¨Éne transformations
In Chapter 6 we studied various linear transformation and their rep-
resentation as matrices. We also brieÔ¨Çy discussed the class of aÔ¨Éne
transformations, which consist of a linear transformation followed by
a translation
‚Éów = T(‚Éóv) + ‚Éód.
In the above equation, the input vector ‚Éóv is Ô¨Årst acted upon by a
linear transformation T then the output of T is translated by the
displacement vector ‚Éód to produce the output vector ‚Éów.

8.8
COMPUTER GRAPHICS
343
In this section we'll use homogenous coordinates for vectors and
transformations, which allow us to express aÔ¨Éne transformations as
a matrix-vector product in a larger vector space:
‚Éów = T(‚Éóv) + ‚Éód
‚áî
‚ÉóW = A‚ÉóV .
If ‚Éóv is an n-dimensional vector, then its representation in homogenous
coordinates ‚ÉóV is an (n + 1)-dimensional vector. The (n + 1) √ó (n + 1)
matrix A contains the information about both the linear transforma-
tion T and the translation ‚Éód.
Homogenous coordinates
Instead of using a triple of cartesian coordinates to represent points
p = (x, y, z)c ‚ààR3, we'll use the quadruple P = (x, y, z, 1)h ‚ààR4,
which is a representation of the same point in homogenous coordi-
nates. Similarly, the vector ‚Éóv = (vx, vy, vz)c ‚ààR3, corresponds to
the four-vector ‚ÉóV = (vx, vy, vz, 1)h ‚ààR4 in homogenous coordinates.
Though there is no mathematical diÔ¨Äerence between points and vec-
tors, we'll stick to the language of points as it is more natural for
graphics problems. Homogenous coordinates of the form (x, y, z, 0)h
correspond to points at inÔ¨Ånity in the direction of (x, y, z)c.
An interesting property of homogenous coordinates is that they're
not unique. The vector ‚Éóv = (vx, vy, vz)c corresponds to a whole set
of point in homogenous coordinates: ‚ÉóV = {(Œ±vx, Œ±vy, Œ±vz, Œ±)h}, for
Œ± ‚ààR. This makes homogenous coordinates invariant to scaling:
Ô£Æ
Ô£∞
a
b
c
Ô£π
Ô£ª
c
‚áî
Ô£Æ
Ô£ØÔ£ØÔ£∞
a
b
c
1
Ô£π
Ô£∫Ô£∫Ô£ª
h
=
Ô£Æ
Ô£ØÔ£ØÔ£∞
5a
5b
5c
5
Ô£π
Ô£∫Ô£∫Ô£ª
h
=
Ô£Æ
Ô£ØÔ£ØÔ£∞
500a
500b
500c
500
Ô£π
Ô£∫Ô£∫Ô£ª
h
.
This is kind of weird, but this extra freedom to rescale vectors arbi-
trarily will lead to many useful applications.
To convert from homogenous coordinates (X, Y, Z, W)h = (a, b, c, d)h
to cartesian coordinates, we divide each component by the W-component
to obtain the equivalent vector (X, Y, Z, W)h = ( a
d, b
d, c
d, 1)h, which
corresponds to the point (x, y, z)c =
  a
d, b
d, c
d

c ‚ààR3.
In the case when the underlying cartesian space is two-dimensional,
the point p = (x, y)c ‚ààR2 is written as P = (X, Y, W)h = (x, y, 1)h in
homogenous coordinates. The homogenous coordinates (X, Y, W)h =
(a, b, d)h with d Ã∏= 0 represent the point (x, y)c =
  a
d, b
d

c ‚ààR2. You
can visualize the conversion Figure 8.7 illustrates how the plane w = 1
is used to obtain the cartesian coordinates.

344
APPLICATIONS
Figure 8.7:
Two-dimensional points and vectors correspond to inÔ¨Ånite
lines in homogenous coordinates. The cartesian coordinates (a, b) can be
identiÔ¨Åed with a "representative" point on the inÔ¨Ånite line.
By conven-
tion we represent this point as (a, b, 1) in homogenous coordinates, which
corresponds intersection of the inÔ¨Ånite line with plane w = 1.
To distinguish cartesian vectors from homogenous vectors, we'll
use a capital letters like P, ‚ÉóA, ‚ÉóB, . . . for points and vectors in homoge-
nous coordinates, and lowercase letters like p,‚Éóa,‚Éób, . . . when referring
to vectors in cartesian coordinates.
AÔ¨Éne transformations in homogenous coordinates
Consider
the aÔ¨Éne transformation that consists of the transformation T : R2 ‚Üí
R2 followed by a translation by ‚Éód = (d1, d2). If T is represented by
the matrix MT = [ m11 m12
m21 m22 ], then the aÔ¨Éne transformation as a whole
can be represented as follows:
x‚Ä≤
y‚Ä≤

=
m11
m12
m21
m22
x
y

+
d1
d2

‚áî
Ô£Æ
Ô£∞
x‚Ä≤
y‚Ä≤
1
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
m11
m12
d1
m21
m22
d2
0
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
x
y
1
Ô£π
Ô£ª.
As you can see, there's nothing fancy about homogenous coordinates;
we wanted to be able to add constants terms to each component of the
vector so we invented an extra "constant" component to each vector
and an extra column that "hits" these constants. Make sure you can
trace the above matrix-vector products and convince yourself there is
no new math‚Äîjust the good old matrix-vector product you're familiar
with.
Homogenous coordinates and projective geometry are powerful math-
ematical techniques with deep connections to many advanced math
subjects.
For the purpose of this appendix we can only give an
overview from an "engineering" perspective‚Äîwhat can you do with
matrix-vector products in homogenous coordinates?

8.8
COMPUTER GRAPHICS
345
Graphics transformations in 2D
This "extra dimension" in the homogenous coordinates representation
for vectors and points allows us to express most geometric transfor-
mation the form T : R2 ‚ÜíR2 in a very succinct manner. In addition
to the aÔ¨Éne transformations described above, homogenous coordi-
nates also allow us to perform perspective transformations, which are
of central importance in computer graphics. The most general trans-
formation we can perform using homogenous coordinates is
Ô£Æ
Ô£∞
x‚Ä≤
y‚Ä≤
w‚Ä≤
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
m11
m12
d1
m21
m22
d2
p1
p2
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
x
y
w
Ô£π
Ô£ª
where M = [ m11 m12
m21 m22 ] corresponds to any linear transformation, ‚Éód =
(d1, d2) corresponds to a translation, and (p1, p2) is a perspective
transformation.
Linear transformations
Let RŒ∏ be the clockwise rotation by the angle Œ∏ of all points in R2.
In homogenous coordinates, this rotation is represented as
p‚Ä≤ = RŒ∏(p)
‚áî
P ‚Ä≤ = MRŒ∏P,
where MRŒ∏ is the following 3 √ó 3 matrix:
Ô£Æ
Ô£∞
cos Œ∏
sin Œ∏
0
‚àísin Œ∏
cos Œ∏
0
0
0
1
Ô£π
Ô£ª.
Indeed the 2 √ó 2 top-left entries of matrices in homogenous coordi-
nates can be used to represent any linear transformation: projections,
reÔ¨Çections, scalings, and shear transformations. The following equa-
tion shows the homogenous matrices the reÔ¨Çection through the x-axis
MRx, an arbitrary scaling MS, and a shear along the x-axis MSHx:
MRx =
Ô£Æ
Ô£∞
1
0
0
0
‚àí1
0
0
0
1
Ô£π
Ô£ª,
MS =
Ô£Æ
Ô£∞
sx
0
0
0
sy
0
0
0
1
Ô£π
Ô£ª,
MSHx =
Ô£Æ
Ô£∞
1
a
0
0
1
0
0
0
1
Ô£π
Ô£ª.
Orthogonal projections
Projections can also be represented as 3 √ó 3 matrices. The projec-
tion onto the x-axis corresponds to the following representation in
homogenous coordinates:
p‚Ä≤ = Œ†x(p)
‚áî
Ô£Æ
Ô£∞
x‚Ä≤
0
1
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
1
0
0
0
0
0
0
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
x
y
1
Ô£π
Ô£ª.

346
APPLICATIONS
Translation
The translation by the displacement vector ‚Éód = (dx, dy) corresponds
to the matrix
MT‚Éó
d =
Ô£Æ
Ô£∞
1
0
dx
0
1
dy
0
0
1
Ô£π
Ô£ª.
Note the identity transformation in the top-left part the matrix: we
don't want any linear transformation performed‚Äîjust a translation.
Figure 8.8: Illustration of the diÔ¨Äerent transformations on a sample shape.
source: wikipedia File:2D_affine_transformation_matrix.svg
Rotations, reÔ¨Çections, shear transformations, and translations can all
be represented as multiplications by appropriate 3 √ó 3 matrices. Fig-
ure 8.8 shows examples of transformations that we can perform using
the matrix-vector product in homogenous coordinates. I hope by now
you're convinced that this idea of adding an extra "constant" dimen-
sion to vectors, is a useful thing to do.
But wait, there's more! We haven't even seen yet what happens
when we put coeÔ¨Écients in the last row of the matrix.

8.8
COMPUTER GRAPHICS
347
Perspective projections
The notion of perspective comes from the world of painting. For a
painting to look "realistic," objects' relative distance from the viewer
must be conveyed by their diÔ¨Äerent size. Distant objects in the scene
are drawn smaller than objects in the foreground. Rather than give
you the general formula for perspective transformations, we'll derive
the matrix representation for perspective transformations from Ô¨Årst
principles. Trust me, it will be a lot more interesting.
We can understand perspective transformations by tracing out
imaginary light rays that start from some object and go toward the
eye of an observer O. The image of the perspective projections is
where the light ray hits the projective plane. Since we're working in
R2, we can draw a picture of what is going on.
Figure 8.9: The perspective projection of a square onto a screen. Observe
that the side of the square that is closer to the observer appear longer than
the side that is farther.
Suppose we want to compute the perspective transformation to
the line with equation y = d for an observer placed at the origin
O = (0, 0). Under this perspective projection, every point p = (x, y)
in the plane maps to a point p‚Ä≤ = (x‚Ä≤, y‚Ä≤) on the line y = d. Figure 8.10
illustrates the situation.
x
y
p = (x, y)
p‚Ä≤
O
y = d
Figure 8.10: The point p‚Ä≤ = (x‚Ä≤, y‚Ä≤) is the projection of the point p = (x, y)
onto the plane with equation y = d. The ratio of lengths d/y must equal
the ratio of lengths x‚Ä≤/x.

348
APPLICATIONS
The only math prerequisite you need to remember is the general
principle for similar triangles.
If the triangle with sides a, b, c is
similar to a triangle with sides a‚Ä≤, b‚Ä≤, c‚Ä≤, this means the ratios of the
sides' lengths are equal:
a‚Ä≤
a
=
b‚Ä≤
b
=
c‚Ä≤
c .
Since the two triangles in Figure 8.10 are similar, we know x‚Ä≤
x = d
y
and therefore directly obtain the expression for (x‚Ä≤, y‚Ä≤) as follows:
x‚Ä≤ = d
y x,
y‚Ä≤ = d
y y = d.
This doesn't look very promising so far since the expression for x‚Ä≤
contains a division by y. The set of equations is not linear in y and
therefore cannot be express as a matrix-product. If only there was
some way to represent vectors and transformations that also allows
division by coeÔ¨Écients too.
Let's analyze the perspective transformation in terms of homoge-
nous coordinates, and see if something useful comes out:
Ô£Æ
Ô£ØÔ£∞
x‚Ä≤
y‚Ä≤
1
Ô£π
Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£∞
d
yx
d
1
Ô£π
Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£∞
x
y
y
d
Ô£π
Ô£∫Ô£ª.
The second equality holds because vectors in homogenous coordinates
are invariant to scalings: (a, b, c)h = Œ±(a, b, c)h for all Œ±. We can shift
the factor d
y as we please: ( d
yx, d, 1) = d
y(x, y, y
d) = (x, y, y
d). In the
alternate homogenous coordinates expression, we're no longer dividing
by y. This means we can represent the perspective transformation as
a matrix-vector product:
Ô£Æ
Ô£ØÔ£∞
x‚Ä≤
y‚Ä≤
1
Ô£π
Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£∞
x
y
y
d
Ô£π
Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£∞
1
0
0
0
1
0
0
1
d
0
Ô£π
Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£∞
x
y
1
Ô£π
Ô£∫Ô£ª.
Now that's interesting. By preparing the vector (X‚Ä≤, Y ‚Ä≤, W ‚Ä≤) with a
third component W ‚Ä≤ Ã∏= 1, we can force each coeÔ¨Écient to be scaled
by
1
W ‚Ä≤ , which is exactly what we need for perspective transforma-
tions. Depending on how the coeÔ¨Écient W ‚Ä≤ is constructed, diÔ¨Äerent
perspective transformations can be obtained.
A perspective projection transformation is a perspective transforma-
tion followed by an orthogonal projection that deletes some of the

8.8
COMPUTER GRAPHICS
349
vector's components. The perspective projection onto the line with
equation y = d is the composition of a perspective transformation
P : R3 ‚ÜíR3 followed by an orthogonal projection Œ†x : R3 ‚ÜíR2
which simply discards the y‚Ä≤ coordinate:
"
x‚Ä≤
1
#
=
"
1
0
0
0
0
1
#Ô£Æ
Ô£ØÔ£∞
1
0
0
0
1
0
0
1
d
0
Ô£π
Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£∞
x
y
1
Ô£π
Ô£∫Ô£ª
‚áí
"
x‚Ä≤
1
#
=
"
1
0
0
0
1
d
0
#
|
{z
}
Œ†xP
Ô£Æ
Ô£ØÔ£∞
x
y
1
Ô£π
Ô£∫Ô£ª.
Note only the x‚Ä≤ coordinate remains after the projection.
This is
the desired result since we want only the "local" coordinates of the
projection plane y = d to remain after the projection.
Certain textbooks on computer graphics discuss only the combined
perspective-plus-projection transformation Œ†xP, as described on the
right side of the above equation.
We prefer treat the perspective
transformation separately from the projection since it makes the math
easier to understand. Also, in many practical applications, keeping
the "depth information" of the diÔ¨Äerent objects which we're projecting
(the y-coordinates) can be useful for determining which objects appear
in front of others.
General perspective transformation
Let's now look at the general case of a perspective transformation
that projects arbitrary points p = (x, y) onto the line ax + by = d.
Again, we assume the observer is located at the origin O = (0, 0). We
want to calculate the coordinates of the projected point p‚Ä≤ = (x‚Ä≤, y‚Ä≤),
as illustrated in Figure 8.11.
The reasoning we'll use to obtain the general perspective trans-
formation is similar to the special case we considered above, and also
depends on the similar triangles. DeÔ¨Åne Œ± to the projection of the
point p onto the line with direction vector ‚Éón = (a, b) passing through
the origin. Using the general formula for distances (see Section 5.1),
we can obtain the length ‚Ñìfrom O to Œ±:
‚Ñì‚â°d(O, Œ±) = ‚Éón ¬∑ p
‚à•‚Éón‚à•= ax + by
‚à•‚Éón‚à•
.
Similarly, we deÔ¨Åne ‚Ñì‚Ä≤ to be the length of the projection of p‚Ä≤ onto ‚Éón:
‚Ñì‚Ä≤ ‚â°d(O, Œ±‚Ä≤) = ‚Éón ¬∑ p‚Ä≤
‚à•‚Éón‚à•= ax‚Ä≤ + by‚Ä≤
‚à•‚Éón‚à•
=
d
‚à•‚Éón‚à•.
The last equation holds because the point p‚Ä≤ is on the line with equa-
tion ax + by = d.

350
APPLICATIONS
x
y
p = (x, y)
p‚Ä≤
O
ax + by = d
Œ±‚Ä≤
Œ±
Figure 8.11: The point p‚Ä≤ = (x‚Ä≤, y‚Ä≤) is the projection of the point p = (x, y)
onto the line with equation ax + by = d. We deÔ¨Åne points Œ±‚Ä≤ and Œ± in the
direction of line's normal vector ‚Éón = (a, b). The distances from the origin
to these points are ‚Ñì‚Ä≤ and ‚Ñìrespectively. We have ‚Ñì‚Ä≤/‚Ñì= x‚Ä≤/x = y‚Ä≤/y.
By the similarity of triangles, we know the ratio of lengths x‚Ä≤/x
and y‚Ä≤/y must equal the ratio of orthogonal distances ‚Ñì‚Ä≤/‚Ñì:
x‚Ä≤
x = y‚Ä≤
y = ‚Ñì‚Ä≤
‚Ñì=
d
ax + by .
We can use this fact to express the coordinates x‚Ä≤ and y‚Ä≤ in terms of
the original x and y coordinates. As in the previous case, expressing
points in homogenous coordinates allows us to arbitrarily shift scaling
factors:
Ô£Æ
Ô£ØÔ£∞
x‚Ä≤
y‚Ä≤
1
Ô£π
Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£∞
‚Ñì‚Ä≤
‚Ñìx
‚Ñì‚Ä≤
‚Ñìy
1
Ô£π
Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£∞
x
y
‚Ñì
‚Ñì‚Ä≤
Ô£π
Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£∞
x
y
ax+by
d
Ô£π
Ô£∫Ô£ª.
The last expression is linear in the variables x and y, therefore it has
a matrix representation:
Ô£Æ
Ô£ØÔ£∞
x‚Ä≤
y‚Ä≤
1
Ô£π
Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£∞
X‚Ä≤
Y ‚Ä≤
W ‚Ä≤
Ô£π
Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£∞
1
0
0
0
1
0
a
d
b
d
0
Ô£π
Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£∞
x
y
1
Ô£π
Ô£∫Ô£ª.
This is the most general perspective transformation.
The "scaling
factor" W ‚Ä≤ is a linear combination of the input coordinates: W ‚Ä≤ =
a
dx + b
dy.
Observe that setting a = 0 and b = 1 we recover the
perspective transformation for the projection onto line y = d.

8.8
COMPUTER GRAPHICS
351
Graphics transformations in 3D
Everything we saw in the previous section about two-dimensional
transformations also applies to three-dimensional transformations. A
three-dimensional cartesian coordinate triple (x, y, z)c ‚ààR3 is repre-
sented as (x, y, z, 1)h ‚ààR4 in homogenous coordinates. Transforma-
tions in homogenous coordinates are represented by 4 √ó 4 matrices.
For example the most general aÔ¨Éne transformation corresponds to
A =
Ô£Æ
Ô£ØÔ£ØÔ£∞
m11
m12
m13
d1
m21
m22
m23
d2
m31
m32
m33
d3
0
0
0
1
Ô£π
Ô£∫Ô£∫Ô£ª,
and the perspective transformation onto the line ax + by + cz = d
with the observer at the origin is
P =
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
0
0
0
0
1
0
0
0
0
1
0
a
d
b
d
c
d
0
Ô£π
Ô£∫Ô£∫Ô£ª.
These should look somewhat familiar to you. Indeed, except for the
larger space and additional degrees of freedom, the mechanics of using
homogenous coordinates is the same in 3D as in 2D.
The best past of using the homogenous coordinates representation
for all transformations is that we can easily compose them together,
like LEGOs. We'll learn about this in the next section.
3D graphics programming
Inside every modern computer there is a special-purpose processor
dedicated to computer graphics operations called the graphics pro-
cessing unit (GPU). Modern GPUs can have thousands of individual
graphics processing units called shaders and each shader can perform
millions of linear algebra operations per second. Think about it, thou-
sands of processors working in parallel doing matrix-vector products
for you‚Äîthat's a lot of linear algebra calculating power!
The reason why we need so much processing power is because
3D models are made up of thousands of little polygons.
Drawing
a 3D scene, also known as rendering, involves performing linear al-
gebra manipulations on all these polygons. This is where the GPU
comes in. The job of the GPU is to translate, rotate, and scale the
polygons of the 3D models placing them into the scene, and then com-
pute what the scene looks like when projected to a two-dimensional
window through which you're observing the virtual world. This trans-
formation from the model coordinates, to world coordinates, and then

352
APPLICATIONS
to screen coordinates (pixels) is carried out in a graphics processing
pipeline.
object
space
Model
matrix
world
space
View
matrix
camera
space
Proj.
matrix
screen
space
Figure 8.12: A graphics processing pipeline for drawing 3D objects on
the screen. A 3D model is composed of polygons expressed with respect to
a coordinate system centred on the object. The model matrix positions the
object in the scene, the view matrix positions the camera in the scene, and
Ô¨Ånally the projection matrix computes what should appear on the screen.
We can understand the graphics processing pipeline as a sequence
of matrix transformations: the model matrix M, the view matrix V ,
and the projection matrix Œ†s.
The GPU applies this sequence of
operations to each of the object's vertices (x, y, z, 1)o, to obtain the
pixel coordinates of the vertices on the screen (x‚Ä≤, y‚Ä≤)s:

x‚Ä≤
y‚Ä≤

s
= Œ†sV M
Ô£Æ
Ô£ØÔ£ØÔ£∞
x
y
z
1
Ô£π
Ô£∫Ô£∫Ô£ª
m
‚áí
(x, y, z, 1)mM TV TŒ†T
s = (x‚Ä≤, y‚Ä≤)s.
It is customary to represent the graphics processing pipeline in the
"transpose picture" so that data Ô¨Çows from left to right. All vectors
are row vectors and we use the transpose of each operation in the
pipeline.
It is not necessary to compute three matrix-vector products for
each vertex. It is much more eÔ¨Écient to compute a combined trans-
formation matrix CT = M TV TŒ†T
s , then apply the combined matrix
CT to each of the coordinates of the 3D object. Similarly, when draw-
ing another object, only the model matrix needs to be modiÔ¨Åed, while
the view and projection matrices remain the same.
Practical considerations
We discussed homogenous coordinates and the linear algebra trans-
formations used for computer graphics. This is the essential "theory"
you'll need to get started with computer graphics programming. The
graphics pipeline used in modern 3D software has a few more steps
than the simpliÔ¨Åed version we showed in Figure 8.12. We'll now dis-
cuss some practical considerations in point form.
‚Ä¢ There are actually two graphics pipelines at work in the GPU.
The geometry pipeline handles the transformation of polygons

8.8
COMPUTER GRAPHICS
353
that make up the 3D objects. A separate texture pipeline con-
trols the graphics pattern that polygons will be Ô¨Ålled with. The
Ô¨Ånal step in the rendering process combines the outputs of the
two pipelines.
‚Ä¢ The Model and View matrices can be combined to form a Mod-
elView matrix that converts from object to camera coordinates.
‚Ä¢ Not all objects in the scene need to be rendered. We don't need
to render objects that fall outside of the camera's viewing angle.
Also we we can skip objects that are closer than the near plane
or farther than the far plane of the camera. Though the scene
could have inÔ¨Ånite extent, we're only interested in rendering the
subset of the scene that we want displayed on the screen, which
we call the view frustum. See the illustration in Figure 8.13.
Figure 8.13:
The perspective transformation used to project a three-
dimensional model onto the two-dimensional screen surface. Note only a
subset of the scene (the view frustum) is drawn, which is cut between the
near plane and the far plane within the Ô¨Åeld of view.
I encourage your to pursue the subject of computer graphics further
on your own. There are excellent free resources for learning OpenGL
and WebGL on the web.
Discussion
Homogenous coordinates have many other applications that we did
not have time to discuss. In this section we'll brieÔ¨Çy mention some of
these with the hope of inspiring you to research the subject further.
Homogenous coordinates are very convenient for representing planes
in R3. The plane with general equation ax+by+cz = d is represented
by the four-vector ‚ÉóN = (a, b, c, ‚àíd) in homogenous coordinates. This
is a natural thing to do‚Äîinstead of describing the plane's normal vec-
tor ‚Éón separately from the constant d, we can represent the plane by
an "enhanced" normal vector ‚ÉóN ‚ààR4:
Plane with ‚Éón = (a, b, c) and constant d
‚áî
‚ÉóN = (a, b, c, ‚àíd).

354
APPLICATIONS
This is really cool because we can now use the same representation
for both vectors and planes and perform vector operations between
them. Yet again we Ô¨Ånd a continuation of the "everything is a vector"
theme that we've seen throughout the book.
What good is representing planes in homogenous coordinates?
How about being able to "check" whether any point ‚ÉóP lies in the
plane ‚ÉóN by computing the dot product ‚ÉóN ¬∑ ‚ÉóP? The point ‚ÉóP lies inside
the plane with normal vector ‚ÉóN if and only if ‚ÉóN ¬∑ ‚ÉóP = 0. Consider
the point ‚ÉóP1 = (0, 0, d
c, 1) that lies in the plane ax + by + cz = d, we
can verify that
‚ÉóN ¬∑ ‚ÉóP1 = (a, b, c, ‚àíd) ¬∑ (0, 0, d
c, 1) = 0.
It's also possible to easily obtain the homogenous coordinates of the
plane ‚ÉóN that passes through any three points ‚ÉóP, ‚ÉóQ, and ‚ÉóR. We're
looking for a vector ‚ÉóN that is perpendicular to all three points. We can
obtain ‚ÉóN using a generalization of the the cross product, computed
using a four-dimensional determinant:
‚ÉóN =

ÀÜe1
ÀÜe2
ÀÜe3
ÀÜe4
px
py
pz
pw
qx
qy
qz
qw
rx
ry
rz
rw

,
where {ÀÜe1, ÀÜe2, ÀÜe3, ÀÜe4} is the standard basis for R4. Yes, seriously. I
bet you haven't seen a four-dimensional cross product before. This
stuÔ¨Äis wild! See the links below if you want to learn more about
homogenous coordinates, projective spaces, or computer graphics.
Links
[ Homogeneous coordinates lecture notes ]
http://web.cs.iastate.edu/~cs577/handouts/homogeneous-coords.pdf
[ Tutorials about OpenGL programming in C++ ]
http://www.songho.ca/opengl/index.html
[ Tutorials about WebGL from the Mozilla Developer Network ]
https://developer.mozilla.org/en-US/docs/Web/API/WebGL_API
[ Another detailed tutorial series on WebGL ]
https://github.com/greggman/webgl-fundamentals/
8.9
Cryptography
Cryptography is the study of secure communication. The two main
tasks that cryptographers aim to achieve are private communication

8.9
CRYPTOGRAPHY
355
(no eavesdroppers) and authenticated communication (no imperson-
ators). Using algebraic operations over Ô¨Ånite Ô¨Åelds Fq, it's possible to
achieve both of these goals. Math is the weapon for privacy!
The need for private communication between people has been
around since long before the development of modern mathematics.
Thanks to modern mathematical techniques, we can now perform
cryptographic operations with greater ease, and build crypto systems
whose security is guaranteed by mathematical proofs. In this section
we'll discuss the famous one-time pad encryption technique invented
by Claude Shannon. One-time pad encryption is a provably secure
crypto system. In order to understand what that means precisely,
we'll Ô¨Årst need some context about what is studied in cryptography.
Context
The secure communication scenarios we'll discuss in this section in-
volve three parties:
‚Ä¢ Alice is the message sender
‚Ä¢ Bob is the message receiver
‚Ä¢ Eve is the eavesdropper
Alice wants to send a private message to Bob, but Eve has the ability
to see all communication between Alice and Bob. You can think of
Eve as a facebook administrator, or a employee of the Orwellian,
privacy-invading web application du jour.
To defend against Eve,
Alice will encrypt her messages before sending them to Bob, using
a secret key only Alice and Bob have access to.
Eve will be able
to capture the encrypted messages (called ciphertexts) but they will
be unintelligible to her because of the encryption.
Assuming Bob
receives the messages from Alice, he'll be able to decrypt them using
his copy of the secret key. Using encryption allows Alice and Bob to
have private communication despite Eve's eavesdropping.
Cryptography is an interesting subject that is very pertinent in the
modern age of pervasive network surveillance. It's a bit like learning
kung fu for self defence. You don't need to go around beating up
people, but you should know how to defend yourself in case something
comes up. I encourage you to learn more about practical cryptography
for your communications. You don't need to stare at command-line
terminals with green letters scrolling on them like in the matrix, but
knowing some basics about passwords, secret keys, digital signatures,
and certiÔ¨Åcates will really help you keep up with the forces out there.
The material in this section only scratches the surface of all there
is to learn in cryptography and is only intended to illustrate the main

356
APPLICATIONS
concepts through a simple example: the one-time pad encryption pro-
tocol.
This simple encryption scheme is provably, unconditionally
secure, given some assumptions about the secret key ‚Éók.
DeÔ¨Ånitions
A cryptographic protocol consists of an encryption function, a de-
cryption function, and a procedure for generating the secret key ‚Éók.
For simplicity we assume messages and keys are all binary strings:
‚Ä¢ ‚Éóm ‚àà{0, 1}n: the message or plaintext is a bitstring of length n
‚Ä¢ ‚Éók ‚àà{0, 1}n: the key is a shared secret between Alice and Bob
‚Ä¢ ‚Éóc ‚àà{0, 1}n: the ciphertext is the encrypted message
‚Ä¢ Enc(‚Éóm,‚Éók): the encryption function that takes as input a mes-
sage ‚Éóm ‚àà{0, 1}n and the key ‚Éók ‚àà{0, 1}n and produces a ci-
phertext ‚Éóc ‚àà{0, 1}n as output
‚Ä¢ Dec(‚Éóc,‚Éók): the decryption function that takes as input a cipher-
text ‚Éóc ‚àà{0, 1}n and the key ‚Éók ‚àà{0, 1}n and produces the
decrypted message ‚Éóm ‚àà{0, 1}n as output
We consider the protocol to be secure if Eve cannot gain any infor-
mation about the messages ‚Éóm1, ‚Éóm2, . . . from the ciphertexts ‚Éóc1,‚Éóc2, . . .
she intercepts.
Before we describe the one-time pad encryption protocol and dis-
cuss its security, we must introduce some background about binary
numbers.
Binary
The cryptographic operations we'll discuss in this section apply to
information encoded in binary. A bit is an element of the binary Ô¨Åeld,
a Ô¨Ånite Ô¨Åeld with two elements F2 ‚â°{0, 1}. A bitstring of size n is
an n-dimensional vector of bits ‚Éóv ‚àà{0, 1}n. For notational simplicity,
we denote bitstrings as v1v2 ¬∑ ¬∑ ¬∑ vn instead of using the usual vector
notation (v1, v2, . . . , vn). For example, 0010 is a bitstring of length 4.
Information in modern digital systems is encoded in binary. Every
Ô¨Åle on your computer consists of long sequences of bits that are copied
between disk memory, RAM, caches, and CPU registers. For example,
using the ASCII encoding convention the character "a" is encoded as
the bitstring 01100001, and the character "b" is encoded as 01100010.
We can express a message that consists of several characters by con-
catenating the bitstrings that correspond to each character
"baba"
‚áî
01100010 01100001 01100010 01100001.

8.9
CRYPTOGRAPHY
357
For example, if the secret message ‚Éóm is the text "beer@18h", we'll
represent it as a sequence of eight chunks of eight bits each, making
for a bitstring of total length 64:
‚Éóm = 01100010
|
{z
}
b
01100101
|
{z
}
e
. . . . . . . . . . . . . . . 01101000
|
{z
}
h
‚àà
{0, 1}64.
Representing the text "beer@18h" as a 64-dimensional binary vector,
allows us to manipulate it using linear algebra operations. This may
sound like a little thing, a convenience at best, but it's actually a
big deal. Linear algebra is powerful stuÔ¨Ä, and if we can manipulate
information using the tools of linear algebra, then we can do power-
ful things with information. In this section we'll describe how basic
vector operations on bitstrings can be used for cryptography. In the
next section we'll learn about error correcting codes which use matrix
operations on binary information vectors to protect them from noise.
Put your seatbelt on and get ready.
The XOR operation
The XOR operation, usually denoted ‚äï, corresponds to addition in
the Ô¨Ånite Ô¨Åeld F2:
c = a ‚äïb
‚áî
c = (a + b)
mod 2.
Table 8.1 shows the results of the XOR operation on all possible
combinations of inputs.
‚äï
0
1
0
0
1
1
1
0
Table 8.1: The XOR operation ‚äïapplied to all possible binary input.
The XOR operation acting on two bitstrings of the same length is the
XOR operation applied to each of their elements:
‚Éóc = ‚Éóa ‚äï‚Éób
‚áî
ci = (ai + bi)
mod 2.
The XOR operation isn't anything new‚Äîit's just the normal vector
addition operation for vectors with coeÔ¨Écients in the Ô¨Ånite Ô¨Åeld F2.
Intuitively, you can think of XOR as a "conditional toggle" op-
eration. Computing the XOR of a bit a ‚ààF2 with 0 leaves the bit
unchanged, while computing the XOR with 1 toggles it, changing 0
to 1 or 1 to 0. To illustrate this "toggle" eÔ¨Äect, consider the bitstring

358
APPLICATIONS
‚Éóv = 0010 and the results of XOR-ing it with the all-zeros bitstring
0000, the all-ones bitstring 1111, and a random-looking bitstring 0101:
0010 ‚äï0000 = 0010,
0010 ‚äï1111 = 1101,
0010 ‚äï0101 = 0111.
In the Ô¨Årst case none of the bits are Ô¨Çipped, in the second case all the
bits are Ô¨Çipped, and in the third case only the second and fourth bits
are Ô¨Çipped.
An important property of the XOR operation is that it is its own
inverse: ‚Éóz ‚äï‚Éóz = ‚Éó0, for all ‚Éóz. In particular, if XOR-ing the bitstring
‚Éóx with some bitstring ‚Éók produces ‚Éóy, then XOR-ing ‚Éóy with ‚Éók recovers
the original vector ‚Éóx:
if ‚Éóx ‚äï‚Éók = ‚Éóy,
then ‚Éóy ‚äï‚Éók = ‚Éóx.
This statement follows from the self-inverse property of the XOR op-
eration and the associative law for vector addition:
‚Éóy ‚äï‚Éók = (‚Éóx ‚äï‚Éók) ‚äï‚Éók
= ‚Éóx ‚äï(‚Éók ‚äï‚Éók)
= ‚Éóx ‚äï‚Éó0
= ‚Éóx.
The self-inverse property of the XOR operation is the basis for the
one-time pad encryption and decryption operations.
One-time pad crypto system
The one-time pad encryption scheme is based on computing the XOR
of the message ‚Éóm and the secret key ‚Éók, which is of the same length as
the message. The security of the protocol depends crucially on how
the secret key ‚Éók is generated, and how it is used.
‚Ä¢ The key must be kept secret. Only Alice and Bob know ‚Éók.
‚Ä¢ The key has to be random: the value of each bit of the key
ki ‚àà‚Éók is random: ki = 1 with probability 50%, and ki = 0 with
probability 50%.
‚Ä¢ Alice and Bob must never reuse any of the secret key.
One-time pad encryption
To obtain the ciphertext ‚Éóc for the message ‚Éóm, Alice computes the
XOR of ‚Éóm and the secret key ‚Éók:
Enc(‚Éóm,‚Éók) ‚â°‚Éóm ‚äï‚Éók = ‚Éóc.

8.9
CRYPTOGRAPHY
359
One-time pad decryption
Upon receiving the ciphertext ‚Éóc, Bob decrypts it by computing the
XOR with the secret key:
Dec(‚Éóc,‚Éók) ‚â°‚Éóc ‚äï‚Éók = ‚Éóm.
From the self-inverse property of the XOR operation, we know XOR-
ing any bitstring with the secret key ‚Éók twice acts like the identity
operation: ‚Éóm ‚äï‚Éók ‚äï‚Éók = ‚Éóm.
Discussion
Observe that using this encryption method to send an n-bit message
requires n bits of secret key. Since bits of the secret key cannot be
reused, we say that sending n bits securely "consumes" n bits of se-
cret key. This notion of secret key as a "consumable resource" is an
important general theme in cryptography. Indeed, coming up with
encryption and decryption functions is considered the easy part of
cryptography, and the hard parts of cryptography are key manage-
ment and key distribution.
Consider a practical use of the one-time pad encryption scheme.
Sending multiple messages m1, m2, . . . will require multiple secret
keys ‚Éók1,‚Éók2, . . . that Alice would use when she wants to communicate
with Bob. Imagine a pad of pages, each page of the pad containing
one secret key ‚Éóki. Since keys cannot be reused, Alice will have to
keep Ô¨Çipping through the pad, using the key from each page only
once. This is where the name "one-time pad" comes from.
One-time pad security
Security deÔ¨Ånitions in cryptography are based on diÔ¨Äerent assump-
tions about the powers of the eavesdropper Eve. We need to formally
deÔ¨Åne what secure means, before we can evaluate the security of any
crypto system. Modern cryptography is a subÔ¨Åeld of mathematics,
so we shouldn't be surprised if the deÔ¨Ånition of security is stated in
mathematical terms.
DeÔ¨Ånition:
Indistinguishability under chosen-plaintext at-
tack (IND-CPA)
A cryptosystem is considered secure in terms of
indistinguishability if no Eve can distinguish the ciphertexts of two
messages ‚Éóma and ‚Éómb chosen by the eavesdropper, with probability
greater than guessing randomly.
This is one of the strongest type of security standards we can expect
from a crypto system. It is also the simplest to understand. This

360
APPLICATIONS
deÔ¨Ånition of security can be understood as a game played between
Alice and Eve. Suppose Eve can force Alice to send only one of two
possible messages ‚Éóma or ‚Éómb.
Every time Eve sees a ciphertext ‚Éóc,
she just has to guess whether Alice sent ‚Éóma or ‚Éómb.
We want to
compare an Eve that has access to ‚Éóc, with a "control Eve" that only
has access to a random string ‚Éór completely uncorrelated with the
message or the ciphertext. A crypto system is secure according to the
indistinguishability under chosen-plaintext security deÔ¨Ånition, if the
Eve that has access to ‚Éóc is no better than randomly guessing what ‚Éóm
is (the best that "control Eve" could do).
We won't go into the formal details of the math, but we need
to specify exactly what we mean by "with probability greater than
guessing randomly." Control Eve has a completely random string ‚Éór,
which contains no information about the message ‚Éóm, so control Eve
must guess randomly and her probability of success is
1
2.
An Eve
that can distinguishing the ciphertext ‚Éóca ‚â°Enc(‚Éóma) from the cipher-
text ‚Éócb ‚â°Enc(‚Éómb) with a probability signiÔ¨Åcantly greater than 1
2 is
considered to have an "advantage" in distinguishing the ciphertext.
Any such scheme is not considered secure in terms of IND-CPA. In-
tuitively, the IND-CPA deÔ¨Ånition of security captures the notion that
Eve should learn no information about the message ‚Éóm after seeing its
ciphertext ‚Éóc.
Sketch of security proof
The one-time pad encryption system is
secure according to IND-CPA because of the the assumption we make
about the shared secret key ‚Éók, namely that it is generated from the
random binary distribution. Each bit ki ‚àà‚Éók is 1 with probability
50%, and 0 with probability 50%.
Eve knows the plaintext of the two messages ma and mb, but she
can't tell which from the ciphertext ‚Éóc because she can't distinguish
between these two equally likely alternative scenarios. In Scenario 1
Alice sent ‚Éóma, the secret key is ‚Éóka, where ‚Éóc = ‚Éóma ‚äï‚Éóka. In Scenario 2
Alice sent ‚Éómb, the secret key is ‚Éókb, where ‚Éóc = ‚Éómb ‚äï‚Éókb. The XOR
operation "mixes" the randomness of ‚Éóm with the randomness in the
secret key ‚Éók, so trying to distinguish whether ‚Éóma or ‚Éómb was sent is
just as diÔ¨Écult as distinguishing ‚Éóka and ‚Éókb. Since ‚Éók is completely
random, Eve is forced to guess randomly. Thus the probability of
determining the correct message is no better than guessing, key pairs
which is precisely the requirement for the deÔ¨Ånition of security.
The randomness of the shared secret key is crucial to the security
of the one-time pad encryption scheme. In general we can think of
shared randomness (shared secret key) as a communication resource
that allows for mathematically-secure private communication between
two parties. But what if Alice and Bob don't have access to shared

8.9
CRYPTOGRAPHY
361
randomness (some shared secret)? In the next section we'll introduce
a diÔ¨Äerent type of crypto system which doesn't depend on a shared
secret between Alice and Bob.
Public key cryptography
Assume Alice and Bob are dissidents in two neighbouring countries
who want to communicate with each other, but they can't trust the
network that connects them. If the dissidents cannot meet in per-
son, obtaining a shared secret key to use for encryption will be dif-
Ô¨Åcult. They can't send the secret key ‚Éók over the network because
the eavesdropper will see it, and thus be able to decrypt all subse-
quent encrypted communications between the dissidents. This is a
major limitation of all symmetric-key crypto systems in which the
same secret is used to encrypt and decrypt messages.
Do not despair dear readers, the system hasn't won yet: the dis-
sidents can use public-key cryptography techniques, to share a secret
key over the untrusted network, in plain view of all state and non-state
sponsored eavesdroppers. The security of public-key crypto systems
comes from some clever math operations (in a Ô¨Ånite Ô¨Åeld), and the
computational diÔ¨Éculty of "reversing" these mathematical operations
for very large numbers.
For example, the security of the Rivest-
Shamir-Adleman (RSA) crypto system depends on the diÔ¨Éculty of
factoring integers. It is easy to compute the product of two integers
d and e, but given only the product de it is computationally diÔ¨Écult
to Ô¨Ånd the factors e and d. For large prime numbers e and d, even
the letter agencies will have a diÔ¨Écult time Ô¨Ånding the factor of de.
Citizens using math to defend against corporations spying and the
police state‚Äîthis is bound to be a central theme in the 21st century.
DeÔ¨Ånitions
In a public-key crypto system the secret key is actually a pair of keys
‚Éók ‚â°{e‚Éók, d‚Éók}, where e‚Éók is the public encryption key and d‚Éók is the
private decryption key. The same function Enc is used for encryption
and decryption, but with diÔ¨Äerent parts of the key.
To use public-key cryptography, each communicating party must
generate their own public-private key pairs.
We'll focus on Alice,
but assume Bob performs analogous steps. Alice generates a public-
private key pair {e‚Éók, d‚Éók}, then she must shares the public part of the
key with Bob. Note the public key can be shared in the open, and
it's not a problem if Eve intercepts it‚Äîthis is why it's called a public
key‚Äîeveryone is allowed to know it.

362
APPLICATIONS
Encryption
Bob will use Alice's public encryption key whenever he wants to send
Alice a secret message.
To encrypt message ‚Éóm, Bob will use the
function Enc and Alice's public encryption key e‚Éók as follows:
‚Éóc = Enc(‚Éóm, e‚Éók).
When Alice receives the ciphertext ‚Éóc, she'll use her private key d‚Éók
(that only she knows) to decrypt the message:
‚Éóm = Enc(‚Éóc, d‚Éók).
Observe that public-key crypto systems are inherently many-to-one:
anyone who knows Alice's public key e‚Éók can create encrypted messages
that only she can decode.
Digital signatures
Alice can also use her public-private key pair to broadcast one-to-
many authenticated statements ‚Éós, meaning receivers can be sure the
statements they receive were sent by Alice. The math is the same: we
just use the keys in the opposite order. Alice encrypts the statement
‚Éós to produce a ciphertext
‚Éóc = Enc(‚Éós, d‚Éók),
then posts the encrypted post ‚Éóc to her blog or on a public forum.
Everyone who know Alice's public key e‚Éók can decrypt the post ‚Éóc to
obtain the statement ‚Éós:
‚Éós = Enc(‚Éóc, e‚Éók).
The interesting property here is that we can be sure the statement
‚Éós was sent by Alice, since only she controls the private key d‚Éók. This
digital signature scheme makes it very diÔ¨Écult for any third parties
to impersonate Alice since they don't know Alice's private key d‚Éók.
This is the principle behind digital signatures used in the delivery of
software updates.
We don't have the space in this section to go any deeper into
public-key cryptography, but we'll illustrate the main ideas through
some practical examples.
Example 1: encrypting emails using GPG
By default all email communications happens in plaintext. When you
send and email, this email will be stored in plaintext at your email

8.9
CRYPTOGRAPHY
363
hosting provider, travel over the Internet, and then be stored in plain-
text at the receiver's hosting provider. There is basically zero privacy
when using email. That's why you should never send passwords or
other credentials by email. Email messages are also subject to imper-
sonation. Indeed, it's fairly easy to create email messages that appear
to originate from someone else. Spammers love this fact because they
know you're much more likely to open emails sent by your friends.
The GNU Privacy Guard (GPG) is a suite of cryptographic soft-
ware speciÔ¨Åcally tailored for email communication. Using the com-
mand line tool gpg or an appropriate plugin for your favourite email
client, it's possible to achieve private and authenticated communica-
tion. Using GPG is like a giant middle Ô¨Ånger raised toward the letter
agencies who want to snoop in on your communications.
We'll now discuss the basic steps to get started with using GPG.
The Ô¨Årst step is to install gpg tools for your operating system from the
GPG website https://www.gnupg.org/download/. The next step is
to generate the private-public key pairs that you'll use to encrypt and
sign your emails. This is accomplished using the following command:
$ gpg --gen-key
When prompted, choose the default option of RSA and RSA, key
size of 4096 (make the bastards work!), and validity of 0 so the key
won't ever expire. Enter your full name when the wizard prompts
you for "real name" and your email address at the next prompt. It's
important to choose a passphrase for your gpg keys, otherwise anyone
who gets a hold of your computer could use these keys to impersonate
you.
Once all this information is entered, the program will run for some
time and generate the keys. When the program is done, it will print
a bunch of information on the screen. You'll want to note the key id,
which is an eight-character long ASCII string that appears in place
of the question marks in "pub 4096R/????????."
The next step is to upload your public key the a key server. This
will allow people who want to communicate with you securely to get
a hold of your public keys, by searching for you based on your name
or your email. Use the following command to upload your public key
to the pgp.mit.edu, a very popular key server operated by MIT:
$ gpg --send-keys --keyserver pgp.mit.edu ????????
Be sure to replace ???????? with the hexadecimal key id you noted
when you generated your keys. With this step, you're done setting
up and publishing your keys.
If you want to communicate securely with a friend, you'll Ô¨Årst need
to obtain their public key from the keyserver. Using this command
to search the keyserver pgp.mit.edu based on your friend's name or
email:

364
APPLICATIONS
$ gpg --keyserver pgp.mit.edu --search-keys <friend's email>
If you Ô¨Ånd a key for your friend, you can follow the steps in the wizard
to import your friend's public key into your keychain. I've got my key
posted there (my key id is C0D34F08, with key Ô¨Ångerprint 8CBB AA5B
CDA6 ...), so if none of your friends are using PGP yet, you can add
me by searching for ivan savov. Run gpg --list-keys to see all
the keys you have on your keychain.
At this point we're done with all the preliminaries. Let's test the
gpg setup by sending an encrypted message to urfriend@host.com.
Use you favourite editor to write your message then save it as a text
Ô¨Åle called msg.txt. Next run the following command:
$ gpg --encrypt -r urfriend@host.com --sign --armor msg.txt
This will encrypt the contents of the msg.txt using the recipient's
(-r) public key, sign the message using your private key, and express
the output as an ascii-armoured plain text Ô¨Åle msg.txt.asc in the
same directory. Copy-paste the contents of this Ô¨Åle (including the
header) into a regular email and send it to your friend.
The procedure for verifying the signature and decrypting an email
you receive from your friend is much simpler. Copy-paste the contents
of the entire encrypted email into a temporary Ô¨Åle newemail.txt.asc
then you run the command
$ gpg newemail.txt.asc
This will conÔ¨Årm the email was really sent by your friend and decrypt
the contents of the message creating a new Ô¨Åle called newemail.txt.
There you have it: secure email comms that letter agencies can't read.
The steps described above involve calling manually the gpg command
from the command line, but in practice GPG is integrated into most
desktop email software. For example, installing GPGTools for Mac
integrates GPG with the Mail application, making sending encrypted
emails as simple as clicking a button. GPG is now a well established
technology, so it is likely your favourite desktop email program has a
GPG plugin that you can start using today.
Example 2: ssh keys for remote logins
Connecting to a remote host can be achieved using the secure shell
protocol. Running the command ssh user@remotehost.com will at-
tempt to login as user on the server remotehost.com, asking your to
provide a password. Passwords are the weakest form of authentica-
tion. Given enough eÔ¨Äort, it's always possible for an attacker to guess
your password. Also, people tend to often use the same password on

8.9
CRYPTOGRAPHY
365
diÔ¨Äerent services, which means if one service is compromised then all
services will be compromised. In this section we'll describe a more
secure approach for ssh logins based on public key authentication.
Setting up key-based authentication for ssh requires steps to be
performed on your machine and on the remote server. Commands
preÔ¨Åxed with laptop$ should be typed on your local machine, while
commands that start with remotehost$ are to be executed on the
remote server. Note these commands assume a UNIX environment.
If you're using a Windows machine, I recommend you install cygwin
to get the same functionality.
The Ô¨Årst step is to generate an ssh private-public key pair in the
directory called .ssh inside your home directory:
laptop$ cd ~
# go to you home dir ~
laptop$ mkdir .ssh
# create a .ssh subdir
laptop$ ssh-keygen -t rsa -b 4096
# generate an RSA key pair
Accept the defaults for the questions you're asked, and be sure to set
a password to protect your ssh private key. A new public key pair
will be created in the directory ~/.ssh/. The private key is contained
in the Ô¨Åle ~/.ssh/id_rsa and the corresponding public key is in the
Ô¨Åle ~/.ssh/id_rsa.pub. You can conÔ¨Årm these Ô¨Åles exist by running
the command ls .ssh. You can ensure these Ô¨Åles have restrictive
access permissions by issuing the command:
laptop$ chmod -R go-rwx .ssh
The above command has the eÔ¨Äect of recursively (-R) changing per-
missions users in the same group (g) as you and other (o) users, by
removing (-), read (r), write (w), and execute (x) permissions. Basi-
cally nobody should be allowed to touch these Ô¨Åles.
Another useful command to know is how to add the private key to
a "key chain," which is a temporary store of credentials tied to your
current login session on your laptop:
laptop$ ssh-add -K ~/.ssh/id_rsa
# add priv. key to keychain
This will prompt you to enter the password you used when generating
the ssh key pair. Now that the private key is on your keychain, you
won't be prompted for a password for the next little while.
Next, we want to copy the public key ~/.ssh/id_rsa.pub from
your laptop to the remote server. We can do this using the command
scp as follows:
laptop$ scp id_rsa.pub user@remotehost:~/key_from_laptop.pub
You'll need to substitute user and remotehost with the actual user-
name and hostname (or IP address) of the remote server for which
you want to setup ssh-key login.

366
APPLICATIONS
The Ô¨Ånal step is to place the the public key information in a special
Ô¨Åle called ~/.ssh/authorized_keys on the remote host. We can do
this using the following commands:
laptop$ ssh user@remotehost
remotehost$ cd ~
remotehost$ mkdir .ssh
remotehost$ cat key_from_laptop.pub >> .ssh/authorized_keys
remotehost$ chmod -R go-rwx .ssh
If you inspect the resulting Ô¨Åle you'll see it now contains a copy of the
public key from your laptop. If you now logout from the remote host
and try to reconnect to it using ssh user@remotehost.com, you'll
get logged in automatically without the need for password.
Once you've setup login for your server using ssh keys, it would
be a good idea to completely disable password logins. In general, this
is the best approach to follow and many hosting services already use
this approach.
Discussion
We'll conclude this section with important advice for developers who
want to use cryptography in their programs. The main thing to keep
in mind is not to try to roll your own crypto functions but to use
established libraries. There are many ways that crypto systems can
be attacked, and people have thought about defending against these
attacks. Libraries are good for you. Use them. Don't be a cowboy
programmer.
Links
[ Visual cryptography ]
http://www.datagenetics.com/blog/november32013/
[ Crypto advice for developers ]
http://daemonology.net/blog/2009-06-11-cryptographic-right-answers.html
[ Public-key cryptography general concepts ]
https://en.wikipedia.org/wiki/Public-key_cryptography
8.10
Error correcting codes
The raw information carrying capacity of a DVD is about 5.64GB,
which is about 20% more than the 4.7GB of data that your computer
will let you write to it. Why this overhead? Are DVD manufacturers
trying to cheat you? No, they're actually looking out for you; the

8.10
ERROR CORRECTING CODES
367
extra space is required for the error correcting code that is applied to
your data before writing it to the disk. Without the error correcting
code, even the tiniest scratch on the surface of the disk would make
the disk unreadable and destroy your precious data. In this section
we'll learn how error correcting codes work.
Error correcting codes play an essential part in the storage, the
transmission, and the processing of digital information.
Even the
slightest change to a computer program will make it crash‚Äîcomputer
programs simply don't like it when you Ô¨Åddle with their bits. Crashing
programs were the norm back in the 1940s as illustrated by this quote:
"Two weekends in a row I came in and found that all my stuÔ¨Ähad
been dumped and nothing was done. I was really annoyed because
I wanted those answers and two weekends had been lost. And so I
said, Dammit, if the machine can detect an error, why can't
it locate the position of the error and correct it?"
‚ÄîRichard Hamming
Richard Hamming, who was a researcher at Bell in the 1940s, ran into
the problem of digital data corruption and decided to do something to
Ô¨Åx it. He Ô¨Ågured out a clever way to encode k bits of information into
n bits of storage, such that it is possible to recover the information
even if some errors occur on the storage medium. An error correcting
code is a mathematical strategy for defending against erasures and
errors. Hamming's invention of error correcting codes was a prereq-
uisite for the modern age of computing: reliable computation is much
more useful than unreliable computation.
DeÔ¨Ånitions
An error correcting code is a prescription for encoding binary informa-
tion. Recall bits are the element of the Ô¨Ånite Ô¨Åeld with two elements
F2 = {0, 1}. A bitstring of length n is an n-dimensional vector of bits
‚Éóv ‚àà{0, 1}n. For example, 0010 is a bitstring of length 4.
We use several parameters to characterize error correcting codes:
‚Ä¢ k: the size of the messages for the code
‚Ä¢ ‚Éóxi ‚àà{0, 1}k: a message. Any bitstring of length k is a valid
message.
‚Ä¢ n: the size of the codewords in the code
‚Ä¢ ‚Éóci ‚àà{0, 1}n: the codeword that corresponds to message ‚Éóxi
‚Ä¢ A code consists of 2k codewords {‚Éóc1,‚Éóc2, . . .}, one for each of the
possible messages {‚Éóx1, ‚Éóx2, . . .}.
‚Ä¢ d(‚Éóci,‚Éócj): the Hamming distance between codewords ‚Éóci and ‚Éócj

368
APPLICATIONS
‚Ä¢ An (n, k, d) code is a procedure for encoding messages into code-
words Enc : {0, 1}k ‚Üí{0, 1}n which guarantees the minimum
distance between any two codewords is at least d.
The Hamming distance between two bitstrings ‚Éóx, ‚Éóy ‚àà{0, 1}n counts
the number of bits where the two bitstrings diÔ¨Äer:
d(‚Éóx, ‚Éóy) ‚â°
n
X
i=1
Œ¥(xi, yi),
where Œ¥(xi, yi) =
 0
if xi = yi,
1
if xi Ã∏= yi.
Intuitively, the Hamming distance between two bitstrings measures
the minimum number of substitutions required to transform one bit-
string into the other. For example, the the Hamming distance between
codewords ‚Éóc1 = 0010 and ‚Éóc2 = 0101 is d(‚Éóc1,‚Éóc2) = 3 because it takes
three substitutions (also called bit Ô¨Çips) to convert ‚Éóc1 to ‚Éóc2 or vice
versa.
An (n, k, d) code is deÔ¨Åned by an function Enc : {0, 1}k ‚Üí{0, 1}n
that encoding messages ‚Éóxi ‚àà{0, 1}k into codewords ‚Éóci ‚àà{0, 1}n.
Usually the encoding procedure Enc is paired with a decoding proce-
dure Dec : {0, 1}n ‚Üí{0, 1}k for recovering messages from (possibly
corrupted) codewords.
‚Éóx
‚àà{0, 1}k
Enc
‚Éóc
‚àà{0, 1}n
‚Éóc ‚Ä≤
‚àà{0, 1}n
 
Dec
‚Éóx ‚Ä≤
‚àà{0, 1}k
Figure 8.14:
An error correcting scheme using the encoding function
Enc and the decoding function Dec to protect against the eÔ¨Äect of noise
(denoted  ). Each message ‚Éóx is encoded into a codeword ‚Éóc. The codeword
‚Éóc is transmitted through a noisy channel that can corrupt the codeword
transforming it to another bitstring ‚Éóc ‚Ä≤.
The decoding function Dec will
look for a valid codeword ‚Éóc that is close in Hamming distance to ‚Éóc ‚Ä≤. If
the end-to-end protocol is successful, the decoded message will match the
transmitted message ‚Éóx ‚Ä≤ = ‚Éóx, despite the noise ( ).
Linear codes
A code is linear if its encoding function Enc is a linear transformation:
Enc(‚Éóxi + ‚Éóxj) = Enc(‚Éóxi) + Enc(‚Éóxj), for all messages ‚Éóxi, ‚Éóxj.
A linear code that encodes k-bit messages into n-bit codewords with
minimum inter-codeword distance d is denoted [n, k, d]. We use square
brackets to enclose the code parameters to indicate the code has a

8.10
ERROR CORRECTING CODES
369
linear structure. Linear codes are interesting because their encoding
function Enc can be implemented as a matrix multiplication.
‚Ä¢ G ‚ààFk√ón
2
: the generating matrix of the code. Each codeword ‚Éóci
is produced by multiplying the message ‚Éóxi by G from the right:
Enc(‚Éóxi) = ‚Éóci = ‚ÉóxiG.
‚Ä¢ R(G): the row space of the generator matrix is called the code
space. We say a codeword ‚Éóc is valid if ‚Éóc ‚ààR(G), which means
there exists some message ‚Éóx ‚àà{0, 1}k such that ‚ÉóxG = ‚Éóc.
‚Ä¢ H ‚ààF(n‚àík)√ón
2
: the parity check matrix of the code. The syn-
drome vector ‚Éós of any bitstring ‚Éóc ‚Ä≤ is obtained by multiplying
‚Éóc ‚Ä≤T by H from the left:
‚Éós = H‚Éóc ‚Ä≤T.
If ‚Éóc ‚Ä≤ is a valid codeword (no error occurred) then ‚Éós = ‚Éó0. If ‚Éós Ã∏= ‚Éó0,
we know an error has occurred. The syndrome information helps
us to correct the error.
We can understand linear codes in terms of the input and output
spaces of the encoding function Enc(‚Éóx) ‚â°‚ÉóxG. Left multiplication of
G by a k-dimensional row vector produces a linear combination of the
rows of G. Thus, the set of all possible codewords (called the code
space) corresponds to the row space of G.
Every vector in the null space of G is orthogonal to every codeword
‚Éóci. We can construct a parity check matrix H by choosing any basis
for the null space for G. We call H the orthogonal complement of G,
which means N(G) = R(H). Alternately, we can say the space of n-
dimensional bitstrings decomposes into orthogonal subspaces of valid
and invalid codewords: Fn
2 = R(G)‚äïR(H). We know H‚ÉócT = ‚Éó0 for all
valid codeword ‚Éóc. Furthermore, the syndrome obtained by multiplying
an invalid codeword ‚Éóc ‚Ä≤ with the parity check matrix ‚Éós = H‚Éóc ‚Ä≤T can
help us characterize the error that occurred, and correct it.
Coding theory
The general idea behind error correcting codes is to choose the 2k
codewords so they are placed far apart from each other in the space
{0, 1}n. If a code has minimum distance between codewords d ‚â•2,
then this code is robust to one-bit errors. To understand why, imagine
a bubble of radius 1 (in Hamming distance) around each codeword.
When a one-bit error occurs, a codeword will be displaced from its
position, but it will remain within the bubble of radius one. In other

370
APPLICATIONS
words, if a one-bit error occurs, we can still Ô¨Ånd the correct codeword
by looking for the closest valid codeword.
See Figure 8.15 for an
illustration of a set of codewords that are d > 2 distance apart. Any
bitstring that falls within one of the bubbles will be decoded as the
codeword at the centre of the bubble.
We cannot guarantee this
decoding procedure will succeed if more than one errors occur.
Figure 8.15: The rectangular region represents the space of binary strings
(bitstrings) of length n. Each codeword ci is denoted with a black dot. A
"bubble" of Hamming distance one around each codeword is shown. Observe
the distance between any two codewords is greater than two (d > 2). By
Observation 1, we know this code can correct any one-bit error (‚åäd
2‚åã‚â•1).
Observation 1
An (n, k, d)-code can correct up to ‚åäd
2‚åãerrors.
The notation ‚åäx‚åãdescribes the Ô¨Çoor function, which computes the
closest integer value smaller than x.
For example, ‚åä2‚åã= 2 and
‚åä3
2‚åã= ‚åä1.5‚åã= 1. We can visualize Observation 1 using Figure 8.15
by imagining the radius of each bubble is ‚åäd
2‚åãinstead of 1.
Repetition code
The simplest possible error correcting code is the repetition code,
which protects the information by recoding multiple copies of each
message bit.
For instance, we can construct a [3, 1, 3] code by re-
peating each message bit three times. The encoding procedure Enc is
deÔ¨Åned as follows:
Enc(0) = 000 ‚â°‚Éóc0,
Enc(1) = 111 ‚â°‚Éóc1.
Three bit Ô¨Çips are required to change the codeword ‚Éóc0 into the code-
word ‚Éóc1, and vice versa. The Hamming distance between the code-
words of this repetition code is d = 3.

8.10
ERROR CORRECTING CODES
371
Encoding a string of messages x1x2x3 = 010 will result in a string
of codewords 000111000. We can use the "majority vote" decoding
strategy using the following decoding function Dec deÔ¨Åned by
Dec(000) = 0, Dec(100) = 0, Dec(010) = 0, Dec(001) = 0,
Dec(111) = 1, Dec(011) = 1, Dec(101) = 1, Dec(110) = 1.
Observe that any one-bit error is corrected. For example, the message
x = 0 will be encoded as the codeword ‚Éóc = 000. If an error occurs
on the Ô¨Årst bit during transmission, the received codeword will be
‚Éóc ‚Ä≤ = 100, and majority-vote decoding will correctly output x = 0.
Since d > 2 for this repetition code, it can correct all one-bit errors.
The Hamming code
The [7, 4, 3] Hamming code is a linear code that encodes four-bit mes-
sages into seven-bit codewords with minimum Hamming distance of
d = 3 between any two codewords.
The generator matrix for the
Hamming code is
G =
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
0
0
1
0
1
1
0
1
0
1
0
1
0
0
0
1
1
0
0
1
0
0
0
0
1
1
1
Ô£π
Ô£∫Ô£∫Ô£ª.
Note that other possibilities for the matrix G exist. Any permutation
of the columns and rows of the matrix will be a generator matrix for a
[7, 4, 3] Hamming code. We have chosen this particular G because of
the useful structure in its parity check matrix H, which we'll discuss
shortly.
Encoding
We'll now look at how the generating matrix is used to encode four-bit
messages into seven-bit codewords. Recall that all arithmetic opera-
tions are performed in the Ô¨Ånite Ô¨Åeld F2. The message (0, 0, 0, 1) will
be encoded as the codeword
(0, 0, 0, 1)G = (0, 0, 0, 0, 1, 1, 1),
similarly (0, 0, 1, 0) will be encoded into (0, 0, 1, 0)G = (0, 0, 1, 1, 0, 0, 1).
Now consider the message (0, 0, 1, 1), which is a linear combination of
the messages (0, 0, 1, 0) and (0, 0, 0, 1). To obtain the codeword for
this message we can multiply it with G as usual to Ô¨Ånd (0, 0, 1, 1)G =
(0, 0, 1, 1, 1, 1, 0).
Another approach would be to use the linearity
of the code and add the codewords for the messages (0, 0, 1, 0) and
(0, 0, 0, 1): (0, 0, 1, 1, 0, 0, 1) + (0, 0, 0, 0, 1, 1, 1) = (0, 0, 1, 1, 1, 1, 0).

372
APPLICATIONS
Decoding with error correction
The minimum distance for this Hamming code is d = 3, which means
it can correct one-bit errors. In this section we'll look at some exam-
ples of bit Ô¨Åip errors that can occur, and discuss the decoding pro-
cedure we can follow to extract the message even from a corrupted
codeword ‚Éóc‚Ä≤.
The parity check matrix for the [7, 4, 3] Hamming code is
H =
Ô£Æ
Ô£∞
1
1
1
1
0
0
0
1
1
0
0
1
1
0
1
0
1
0
1
0
1
Ô£π
Ô£ª.
This matrix is the orthogonal complement of the generating matrix G.
Every valid codeword ‚Éóc is in the row space of G, since ‚Éóc = ‚ÉóxG for some
message ‚Éóx. Since the rows of H are orthogonal to R(G), the product
of H with any valid codeword will be zero: H‚ÉócT = 0.
On the other hand, if the codeword ‚Éóc ‚Ä≤ contains an error, then
multiplying it with H will produce a nonzero syndrome vector ‚Éós:
H‚Éóc ‚Ä≤T = ‚Éós Ã∏= 0.
The decoding procedure Dec can use the information in the syndrome
vector ‚Éós to correct the error. In general, the decoding function could
be a complex procedure that involving ‚Éós and vecc ‚Ä≤. In the case of the
the Hamming code, the decoding procedure is very simple because
the syndrome vector ‚Éós ‚àà{0, 1}3 contains the binary representation of
the location where the bit-Ô¨Çip error occurred. Let's look at how this
works through some examples.
Suppose we sent the message ‚Éóx = (0, 0, 1, 1) encoded as the code-
word ‚Éóc = (0, 0, 1, 1, 1, 1, 0). If an error on the last bit occurs in tran-
sit (bit one counting from the right), the received codeword will be
‚Éóc ‚Ä≤ = (0, 0, 1, 1, 1, 1, 1). Computing the syndrome for ‚Éóc ‚Ä≤ we obtain
‚Éós = H‚Éóc ‚Ä≤T =
Ô£Æ
Ô£∞
1
1
1
1
0
0
0
1
1
0
0
1
1
0
1
0
1
0
1
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
0
1
1
1
1
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£∞
0
0
1
Ô£π
Ô£ª.
The syndrome vector ‚Éós = (0, 0, 1) corresponds to the binary string
001, which is the number one. This is the location where the error
occurred‚Äîthe Ô¨Årst bit of the codeword (counting from the right).
After correcting the error on the Ô¨Årst bit we obtain the correct ‚Éóc =

8.10
ERROR CORRECTING CODES
373
(0, 0, 1, 1, 1, 1, 0), which decodes to the message ‚Éóx = (0, 0, 1, 1) that
was sent.
Let's check the error-correcting ability of the Hamming code with
another single-bit error. If a bit-Ô¨Çip error occurs on the fourth bit, the
received codeword will be ‚Éóc ‚Ä≤‚Ä≤ = (0, 0, 1, 0, 1, 1, 0). The syndrome for
‚Éóc ‚Ä≤‚Ä≤ is H‚Éóc ‚Ä≤‚Ä≤T = (1, 0, 0), which corresponds to the number four when
interpreted in binary. Again, we're able to obtain the position of error
from the syndrome.
The fact that the syndrome tells us where the error has occurred is
not a coincidence, but a consequence of deliberate construction of the
matrices G and H of the Hamming code. Let's analyze the possible
received codewords ‚Éóc ‚Ä≤, when the transmitted codeword is ‚Éóc ‚Ä≤:
‚Éóc‚Ä≤ =
(
‚Éóc
if no error occurs
‚Éóc + ‚Éóei
if bit-Ô¨Çip error occurs in position i
where ‚Éóei is vector that contains a single one in position i. Indeed a
bit Ô¨Çip in the ith position is the same as adding one modulo two in
that position.
In the case when no error occurs the syndrome will be zero H‚Éóc = ‚Éó0,
because H is deÔ¨Åned as the orthogonal complement of the code space
(the row space of G). In the case of a single error occurs, the syndrome
calculation only depends on the error:
‚Éós = H‚Éóc‚Ä≤T = H(‚Éóc + ‚Éóei)T = 
H‚Éóc + H‚Éóei = H‚Éóei.
If you look carefully at the structure in the parity check matrix H,
you'll see its columns contain the binary encoding of the numbers
between seven and one. With this clever choice of the matrix H, we're
able to obtain a syndrome that tells us the binary representation of
where the error has occurred.
Discussion
Throughout this section we referred to "the" [7,4,3] Hamming code,
but in fact there there is a lot of freedom when deÔ¨Åning a Hamming
code with these dimensions. For example, we're free to perform any
permutation of the columns of the generator matrix G and the parity
check matrix H, and the resulting code will have the same properties
as the one described above.
The term Hamming code actually applies to a whole family of
linear codes. For any r > 2, there exists a [2r ‚àí1, 2r ‚àír ‚àí1, 3] Ham-
ming code, that has similar structure and properties as the Hamming
[7, 4, 3] code described above.
The ability to "read" the location of the error directly from the
syndrome is truly a marvellous mathematical construction, that is

374
APPLICATIONS
particular to the Hamming code.
For other types error correcting
codes inferring the error from the syndrome vector ‚Éós may require a
more complicated procedure.
Note that Hamming codes all have minimum distance d = 3, which
means they allow us to correct ‚åä3
2‚åã= 1 bit errors. Hamming codes
are therefore not appropriate for use with communication channel on
which multi-bit errors are likely to occur.
There exist other code
families like the Reed-Muller codes and Reed-Solomon codes, which
can be used in more noisy scenarios. For example, Reed-Solomon
codes are used by NASA for deep-space communications and for error
correction on DVDs.
Error detecting codes
Another approach for dealing with errors is to focus on detecting the
errors and not trying to correct them.
Error detecting codes, like
the parity check code, are used in scenarios where it is possible to
retransmit messages; if the receiver detects a transmission error has
occurred, she can ask the sender to retransmit the corrupted message.
The receiver will be like "Yo, Sender, I got your message, but its parity
was odd, so I know there was an error and I want you to send that
message again." Error detect and retransmit is how Internet protocols
work (TCP/IP).
The parity check code is a simple example of an error detecting
code. The parity of a bitstring describes whether the number of 1s
in the string is odd or even. The bitstring 0010 has odd parity, while
the bitstring 1100 has even parity. We can compute the parity of any
bitstring by taking the sum of its bits, the sum being performed in
Ô¨Ånite Ô¨Åeld F2.
A simple [k+1, k, 2] parity check code can be created by appending
a single bit p (the parity bit) to end of the every message to indicate
the parity of the message bitstring x1x2 ¬∑ ¬∑ ¬∑ xk. We append p = 1 if the
message has odd parity, and p = 0 if the message has even parity. The
resultant message-plus-parity-check bitstring ‚Éóc = x1x2 ¬∑ ¬∑ ¬∑ xkp will al-
ways have even parity.
If a single bit-Ô¨Çip error occurs during transmission, the received
codeword ‚Éóc ‚Ä≤ will have odd parity, which tells us the message data has
been aÔ¨Äected by noise. More advanced error detecting schemes can
detect multiple errors, at the cost of appending more parity-check bits
at end of the messages.
Links
[ The Hamming distance between bitstrings ]
https://en.wikipedia.org/wiki/Hamming_distance

8.11
FOURIER ANALYSIS
375
[ More examples of linear codes on Wikipedia ]
https://en.wikipedia.org/wiki/Linear_code
https://en.wikipedia.org/wiki/Hamming_code
https://en.wikipedia.org/wiki/Reed-Muller_code
[ Details of error correcting codes used on optical disks ]
http://bat8.inria.fr/~lang/hotlist/cdrom/Documents/tech-summary.html
http://usna.edu/Users/math/wdj/_files/documents/reed-sol.htm
http://multimediadirector.com/help/technology/cd-rom/cdrom_spec.htm
Exercises
E8.9 Find the codeword ‚Éóc that corresponds to the message ‚Éóx =
(1, 0, 1, 1) for the [7, 4, 3] Hamming code whose generator matrix G
is given on page 371.
E8.10 Construct the encoding matrix for a [8, 7, 2] parity check code.
8.11
Fourier analysis
Way back in the 17th century, Isaac Newton carried out a famous ex-
periment using light beams and glass prisms. He showed that a beam
of white light splits into a rainbow of colours upon passing through a
prism: starting from red at one end, followed by orange, yellow, green,
blue, and Ô¨Ånally violet at the other end. This experiment showed that
white light is made up of components with diÔ¨Äerent colours. Using
the language of linear algebra, we can say that white light is a "linear
combination" of diÔ¨Äerent colours.
Today we know that the diÔ¨Äerent light colours correspond to elec-
tromagnetic waves with diÔ¨Äerent frequencies: red light has frequency
around 450 THz, while violet light has frequency around 730 THz.
We can therefore say that white light is made up of components with
diÔ¨Äerent frequencies. The notion of describing complex phenomena
in terms of components with diÔ¨Äerent frequencies is the main idea
behind Fourier analysis.
Fourier analysis is used to describe sounds, vibrations, electric
signals, radio signals, light signals, and many other phenomena. The
Fourier transform allows us to represent all these "signals" in terms of
components with diÔ¨Äerent frequencies. Indeed, the Fourier transform
can be understood as a change-of-basis operation from a time basis
to a frequency basis:
[v]t
‚áî
[v]f.

376
APPLICATIONS
For example, if v represents a musical vibration, then [v]t corresponds
to the vibration as a function of time, while [v]f corresponds to the
frequency content of the vibration. Depending on the properties of
the signal in the time domain and the choice of basis for the frequency
domain, diÔ¨Äerent Fourier transformations are possible.
We'll study three diÔ¨Äerent bases for the frequency domain based
on orthonormal sets of sinusoidal and complex exponential functions.
The Fourier series is a representation for continuous periodic func-
tions f(t) ‚àà{R ‚ÜíR}, that is functions that satisfy f(T + t) = f(t).
The Fourier basis used in the Fourier series is the set of sines and
cosines of the form sin( 2œÄn
T t) and cos( 2œÄn
T t), which form an orthogo-
nal set. The Fourier transform is the continuous version of the Fourier
series. Instead of a countable set of frequency components, the fre-
quency representation of the signal is described by a complex-valued
continuous function f(œâ) ‚àà{R ‚ÜíC}. Instead of a continuous time
parameter t ‚ààR, certain signals are described in terms of N samples
from the time signal: {f[t]}t‚àà[0,1,...,N‚àí1]. The Discrete Fourier trans-
form is a version of the Fourier transform for signals deÔ¨Åned at discrete
time samples. Table 8.2 shows a summary of these three Fourier-type
transformations. The table indicates the class of functions for which
the transform applies, the Fourier basis for the transform, and the
frequency-domain representation used.
Fourier transformations
Name
Time domain
Fourier basis
Frequency domain
FS
f(t) ‚àà{R ‚ÜíR}
1, {cos
  2œÄn
T

}n‚ààN+,
(a0, a1, b1, . . .)
s.t. f(t) = f(t + T)
{sin
  2œÄn
T

}n‚ààN+
FT
f(t) ‚àà{R ‚ÜíR}
{eiœât}œâ‚ààR
f(œâ) ‚àà{R ‚ÜíC}
s.t.
Z ‚àû
‚àí‚àû
|f(t)|2 dt < ‚àû
DFT
f[t] ‚àà{[N] ‚ÜíR}
{e
iwt
N }w‚àà[N]
f[w] ‚àà{[N] ‚ÜíC}
Table 8.2: Three important Fourier transformations. Observe the diÔ¨Äerent
time domain, Fourier basis, and frequency domains for each transform. The
Fourier series (FS) converts periodic continuous time signals into Fourier
coeÔ¨Écients. The Fourier transform (FT) converts Ô¨Ånite-power continuous
signal into continuous functions of frequency. The Discrete Fourier trans-
form (DFT) is the discretized version of the Fourier transform.
Fourier transforms are normally studied by physics and electrical
engineering students during their second or third year at university.
You, my dear readers, thanks to your understanding of linear algebra,

8.11
FOURIER ANALYSIS
377
can have a ten-page sneak-peek preview of the Fourier transformations
course right now. In this section, we'll study mathematics behind the
Fourier transforms and discuss how they're used in practical signal
processing applications. Before we jump into signal processing, let's
look at something much simpler‚Äîthe vibration of a guitar string.
Example 1: Describing the vibrations of a string
Imagine a string of length L that is tied at both ends, like a guitar
string. If you pluck this string it will start vibrating. We can de-
scribe the displacement of the string from its rest (straight) position
as a function f(x), where x ‚àà[0, L]. The longest vibration on the
string is called the fundamental, while the rest of the vibrations are
called overtones. See Figure 8.16 for an illustration. When you pluck
the guitar string you'll hear the fundamental and the overtones as a
single tone. The relative prominence of the frequencies varies among
instruments.
The way vibrations on a string work is that only certain modes
of vibration will remain in the long term. Any vibration is possible
to begin with, but after a while, the energy in the string "bounces
around," reÔ¨Çecting from the two Ô¨Åxed endpoints, and many vibrations
cancel out. The only vibrations remaining on the string will be the
following sin-like vibrations:
e1(x) ‚â°sin( œÄ
Lx),
e2(x) ‚â°sin( 2œÄ
L x),
e3(x) ‚â°sin( 3œÄ
L x),
. . . .
Vibrations of the form en(x) ‚â°sin( nœÄ
L x) persist because they are
stable, which means they satisfy the physics constraints imposed on
the string. One constraint on the string is that it's two endpoints are
clamped down: en(0) = 0 and en(L) = 0. Another physics constraint
requires that the vibration of the string f(x) must satisfy the equation
f(x) = f(L ‚àíx). Without going further into the physics of string
vibrations,2 let's just say that a "survival of the Ô¨Åttest" phenomenon
occurs, and only the Ô¨Ånite set of vibrations {en(x)}n‚ààN+ will remain
on the string in the long run.
The set of functions {en(x)}n‚ààN+ form a basis for the set of vibrations
that satisfy f(0) = 0, f(L) = 0, and f(x) = f(L ‚àíx) on [0, L]. Any
vibration on the string can be described in terms of a sequence of
coeÔ¨Écients (a1, a2, a3, . . .):
f(x) s.t. f(x) = f(L ‚àíx)
‚áî
(a1, a2, a3, a4, . . .).
The coeÔ¨Écients ai represent how much of the ith vibrations exists on
the string.
2You can learn more about the physics of standing waves from this tutorial:
http://www.phy.duke.edu/~rgb/Class/phy51/phy51/node34.html.

378
APPLICATIONS
Figure 8.16: Standing waves on a string with length L = 1. The longest
vibration is called the fundamental. Other vibrations are called overtones.
Depending on how you pluck the string, the shape of the vibrating
string f(x) will be some superposition (linear combination) of the
vibrations en(x):
f(x) = a1 sin( œÄ
Lx) + a2 sin( 2œÄ
L x) + a3 sin( 3œÄ
L x) + ¬∑ ¬∑ ¬∑
= a1 e1(x)
+ a2 e2(x)
+ a3 e3(x)
+ ¬∑ ¬∑ ¬∑
That's quite crazy if you think about it: rather than describing the
exact shape of the vibrating string f(x), it's suÔ¨Écient to specify a
list of coeÔ¨Écients to describe the same vibration. The coeÔ¨Écient a1
tells us the prominence of the fundamental vibration, and the other
coeÔ¨Écients tells us the prominence of the overtones. Since the laws of
physics restricts the possible vibrations to linear combinations of the
basis vibrations {en(x)}n‚ààN+, we can represent any vibration on the
string through its sequence of coeÔ¨Écients (a1, a2, a3, . . .).
The main idea
The above example shows it's possible to describe a complex real-
world system like a vibrating guitar string in terms of a succinct
description‚Äîthe coeÔ¨Écients (a1, a2, a3, . . .). This is general pattern
in science that occurs when solving problems stated in terms of dif-
ferential equations. The solutions can often be expressed as linear
combinations of an orthonormal basis of functions. Using this exam-
ple, we're now in a position to explain the "main idea" behind Fourier
transform, connecting it to the concept of inner product space which
we studied in Section 7.4.
The fundamental starting point of all Fourier-type reasoning is
to deÔ¨Åne the inner product space for the solution space, and a basis
of orthonormal functions for that space.
In the case of the sting

8.11
FOURIER ANALYSIS
379
vibrations, the space of string vibrations consists of functions f(x) on
the interval [0, L] that satisfy f(0) = 0, f(L) = 0, and f(x) = f(L‚àíx).
The inner product operation we'll use for this space is
‚ü®f(x), g(x)‚ü©‚â°
Z x=L
x=0
f(x)g(x) dx,
and the functions {en(x)}n‚ààN+ form an orthogonal basis for this
space. We can verify that ‚ü®em(x), en(x)‚ü©= 0 for all m Ã∏= n. See
exercise E8.11 for a hands-on experience.
Since {en(x)}n‚ààN+ forms a basis, we can express any function as
a linear combination of the basis functions. The complicated-looking
Fourier transform formulas that we'll get to shortly can therefore be
understood as applications of the general change-of-basis formula.
Change of basis review
Let's do a quick review of the change-of-
basis formula. You recall the change-of-basis operation, right? Given
two orthonormal bases B = {ÀÜe1, ÀÜe2, ÀÜe3} and B‚Ä≤ = {ÀÜe‚Ä≤
1, ÀÜe‚Ä≤
2, ÀÜe‚Ä≤
3}, you'd
know how to convert a vector [‚Éóv]B expressed with respect to basis B,
into coordinates with respect to basis B‚Ä≤. In case you've forgotten,
look back to page 198, where we deÔ¨Åne the change-of-basis matrix
[1]
B‚Ä≤
B that converts from B coordinates to B‚Ä≤ coordinates, and its
inverse
[1]
B
B‚Ä≤ that converts from B‚Ä≤ coordinates to B coordinates:
[‚Éóv]B‚Ä≤ =
[1]
B‚Ä≤
B[‚Éóv]B
‚áî
[‚Éóv]B =
[1]
B
B‚Ä≤[‚Éóv]B‚Ä≤.
When the bases are orthonormal, the change-of-basis operation de-
pends only on the inner product between the vectors of the two bases:
‚Éóv ‚Ä≤
i =
3
X
j=1
‚ü®ÀÜe‚Ä≤
i, ÀÜej‚ü©vj
‚áî
‚Éóvi =
3
X
j=1
‚ü®ÀÜei, ÀÜe‚Ä≤
j‚ü©v ‚Ä≤
j.
Orthonormal bases are nice like that‚Äîif you know how to compute
inner products, you can perform the change of basis.
To apply the general formulas for change of basis to the case of the
vibrating string, we must Ô¨Årst deÔ¨Åne the two bases and compute the
inner product between them. We can think of the function f(x) ‚àà
{[0, L] ‚ÜíR} as being described in the "default basis" ex, which is
equal to one at x and zero everywhere else. The Fourier basis for the
problem consists of the functions en ‚â°sin( nœÄ
L x) for n ‚ààN+. The
inner product between ex and en is
‚ü®ex, en‚ü©=
Z L
0
ex(y) sin( nœÄ
L y) dy = sin( nœÄ
L x),

380
APPLICATIONS
since the function ex(y) is equal to zero everywhere except at y = x.
We can obtain the change-of-basis transformations by extending
notion of matrix-vector appropriately.
We use the integral on the
interval [0, L] for the change-of-basis to the Fourier coeÔ¨Écients, and
the inÔ¨Ånite sum over coeÔ¨Écients to change from the space of Fourier
coeÔ¨Écients to functions:
an =
Z L
0
sin( nœÄ
L x)f(x) dx
‚áî
f(x) =
‚àû
X
n=1
sin( nœÄ
L x)an.
Compare these formulas with the basic change-of-basis formula be-
tween bases B and B‚Ä≤ discussed above.
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
f(0)
...
f(x)
...
f(L)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
¬∑ ¬∑ ¬∑
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
a1
a2
a3
a4
...
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Figure 8.17: Any string vibration f(x) can be represented as coeÔ¨Écients
(a1, a2, a3, a4, . . .) with respect to the basis of functions en(x) ‚â°sin( nœÄ
L x).
Figure 8.17 illustrates right hand side of the change-of-basis formula,
which transforms a vibration from the Fourier basis (a1, a2, a3, a4, . . .)
to a function f(x) ‚àà{[0, L] ‚ÜíR} in the default basis.
Analysis and synthesis
In the jargon of Fourier transformations, we refer to the change of
basis from the default basis to the Fourier basis ( [1]
f
x) as Fourier
analysis, and the transformation from the Fourier basis to the the de-
fault basis ( [1]
x
f) as Fourier synthesis. This terminology, in addition
to sounding extra fancy, gives us some useful intuition about the pur-
pose of the Fourier transform. Given a vibration on the string, we use
the Fourier transform to "analyze" the vibration, decomposing it into
its constituent vibrations. The Fourier analysis equations describe
how to calculate each Fourier coeÔ¨Écient.
The opposite direction‚Äîstarting from the Fourier coeÔ¨Écients and
combining them to obtain a vibration default basis is called synthesis,
in analogy with synthesizer that can generate any sound. The syn-
thesis equations describe how to calculate the values of the vibration
as a function of x based on its components in the frequency domain.
For example, you can pick any set of coeÔ¨Écients (a1, a2, a3, a4, . . .),
form the linear combination PN
n=1 an sin( 2œÄn
L x), and check what that

8.11
FOURIER ANALYSIS
381
vibration sounds like. This is how synthesizers work: they can repro-
duce the sound of any instrument by producing the right linear linear
combination of vibrations.
Fourier series
When discussing Fourier transformations, it is natural to work with
functions of time f(t), which we call signals. A signal in the "time
basis," is speciÔ¨Åed by its values f(t) for all times t. This is the "time
domain" representation for signals. The Fourier series corresponds
to the following change-of-basis operation:
f(t) s.t. f(t) = f(T + t)
‚áî
(a0, a1, b1, a2, b2, . . .).
The coeÔ¨Écients (a0, a1, b1, a2, b2, . . .) are called the Fourier coeÔ¨É-
cients of f(t).
The Fourier series applies to all signals f(t), that
satisfy the constraint f(t) = f(t + T) for all t ‚àà[0, T). We say such
signals are periodic with period T.
The basis used for the Fourier series consists of the set of cosine
and sine functions with frequencies that are multiples of 2œÄ
T :

sin
2œÄn
T t

n‚ààN+
and

cos
2œÄn
T t

n‚ààN
.
This family of functions forms an orthogonal set with respect to the
standard inner product:
‚ü®f(t), g(t)‚ü©= 1
T
Z t=T
t=0
f(t)g(t) dt.
Every periodic signal f(t) can be represented as a Fourier series of
the form:
f(t) = a0 +
‚àû
X
n=1
an cos( 2œÄn
T t) +
‚àû
X
n=1
bn sin( 2œÄn
T t).
(FSS)
The coeÔ¨Écient ai represent how much of the ith cos-like vibration is
contained in f(t), and the the coeÔ¨Écient bi represents how much of
the ith sin-like vibration is contained in f(t). See the illustrated in
Figure 8.18. This representation is directly analogous to the analysis
we performed for the vibrating string, but this time we use both sines
and cosines.
We compute the Fourier coeÔ¨Écients using the following formulas:
an ‚â°1
T
Z T
0
f(t) cos( 2œÄn
T t) dt,
bn ‚â°1
T
Z T
0
f(t) sin( 2œÄn
T t) dt.
(FSA)

382
APPLICATIONS
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
f(0)
...
f(t)
...
f(T)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
a0
a1
b1
a2
b2
a3
b3
...
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Figure 8.18: A periodic signal f(t) can be represented as a series of Fourier
coeÔ¨Écients (a0, a1, b1, a2, b2, a3, b3, . . .).
The Ô¨Årst column corresponds to
the constant component 1 = cos(0). The remaining columns correspond to
cosines and sines with diÔ¨Äerent frequencies.
These transformations correspond to the standard change of basis for
the function f(t) in the time domain to the Fourier basis of cosines
and sines.
For the Fourier series representation of a periodic time signal to
be exact, we must compute an inÔ¨Ånite number of coeÔ¨Écients in the
Fourier series (a0, a1, b1, a2, b2, . . .). However, we're often interested in
obtaining an approximation to a f(t) using only a Ô¨Ånite set of Fourier
coeÔ¨Écients:
f(t) ‚âàa0 +
N
X
n=1
an cos( 2œÄn
T t) +
N
X
n=1
bn sin( 2œÄn
T t).
This is called a Fourier series approximation since the frequency rep-
resentation does not contain the components with frequencies N+1
T ,
N+2
T , and higher. Nevertheless, these Ô¨Ånite-series approximations of
signals are used in many practical scenarios; it's much easier to com-
pute a Ô¨Ånite number of Fourier coeÔ¨Écients instead of an inÔ¨Ånite num-
ber.
Example
For an example calculation of the Fourier coeÔ¨Écients of
the square wave signal, see bit.ly/fourier_series_square_wave
by Joshua Vaughan. Note the square wave analyzed is an odd function
periodic function, so its coeÔ¨Écients an are all zero.
In the next section we'll describe the Fourier transform, which is
a continuous-frequency version of the Fourier series.
Fourier transform
The Fourier series representation applies only to periodic functions
f(t), but not every function of interest in signal processing is periodic.

8.11
FOURIER ANALYSIS
383
Figure 8.19: The square-wave signal can be approximated by a linear
combination of sine functions with diÔ¨Äerent frequencies.
The Fourier transform applies to all signals f(t) (periodic or not) that
obey the Ô¨Ånite-power constraint:
Z t=+‚àû
t=‚àí‚àû
|f(t)|2 dt ‚â§‚àû.
This class of functions includes most signals of practical importance
in communication scenarios.
The result of the Fourier transform is a complex-valued continuous
function in the frequency domain:
f(t) ‚àà{R ‚ÜíR}
‚áî
f(œâ) ‚àà{R ‚ÜíC}.
The basis used in the Fourier transforms is
eœâ ‚â°eiœât, for œâ ‚ààR.
parametrized by the continuous parameters œâ ‚ààR. The set of func-
tions {eœâ} form an orthogonal basis with respect to the inner product
‚ü®f(t), g(t)‚ü©=
1
‚àö
2œÄ
Z ‚àû
‚àí‚àû
f(t)g(t) dt.
The change-of-basis from the time domain to the frequency domain
is performed using the following integral:
f(œâ) ‚â°
Z
‚ü®eœâ, et‚ü©f(t) dt =
1
‚àö
2œÄ
Z ‚àû
‚àí‚àû
e‚àíiœâtf(t) dt.
(FTA)
We can understand this formula as an instance of the general change-
of-basis formula from the the "default basis" for the time domain
{et}t‚ààR, to the basis {eœâ}œâ‚ààR. The function f(œâ) is called the spec-
trum or Fourier transform of f(t); it tells us all the information about
the frequency content of the signal f(t). Note Fourier transform can

384
APPLICATIONS
be used as a verb when referring to the change-of-basis transforma-
tion, or as a noun when referring to the function f(œâ), which is the
result of the Fourier transform.
The Inverse Fourier transform is the change of basis from the
frequency domain back to the time domain.
Given the frequency
representation of a function f(œâ), we can reconstruct the time repre-
sentation of the signal using the integral:
f(t) ‚â°
Z
‚ü®et, eœâ‚ü©f(œâ) dœâ =
1
‚àö
2œÄ
Z ‚àû
‚àí‚àû
eiœâtf(œâ) dœâ.
(FTS)
Note the similarity between the formulas for the forward and the in-
verse Fourier transform formulas. Indeed, these are representation of
the general change-of-basis formula using the inner product for func-
tions. Compare the Fourier transform change-of-basis formulas and
the basic change-of-basis formulas between bases B and B‚Ä≤ discussed
above (see page 379). The conjugate-symmetric property of the inner
product ‚ü®et, eœâ‚ü©= ‚ü®eœâ, et‚ü©explains the change from e‚àíiœât to eiœât.
Further discussion about the Fourier transform and its many ap-
plications is beyond the scope of the current section. If you take a
signal processing course (the best course in the electrical engineering
curriculum), you'll learn all about the Fourier transform.
Discrete Fourier transform
Continuous-time signals are important in sound engineering, radio
communications, and other communication scenarios. Signals can also
be digitized and represented as discrete samples rather than contin-
uous functions. A continuous-time signal f(t) can be approximated
by taking a Ô¨Ånite set of N samples from the signal. This results of
this sampling is a discrete-time signal f[t] ‚àà{[N] ‚ÜíR}, where the
shorthand [N] denotes the sequence [0, 1, . . . , N ‚àí1]. We'll use square
brackets around the function input to distinguish the discrete-time
signal f[t] from its continuous-time counterpart f(t) ‚àà{R ‚ÜíR}.
The Discrete Fourier transform converts N samples of a discrete-
time signal, into a frequency representation with N frequency com-
ponents:
f[t] ‚ààR, for t ‚àà[N]
‚áî
f[w] ‚ààC, for w ‚àà[N].
The basis for the frequency domain consists of complex exponentials:
ew ‚â°ei 2œÄw
N t, for t ‚àà[N] and w ‚àà[N].
This set of functions form an orthonormal set with respect to the inner
product ‚ü®f[n], g[n]‚ü©‚â°
1
‚àö
N
PN‚àí1
n=0 f[n]g[n]. You can understand this

8.11
FOURIER ANALYSIS
385
inner product and this basis as discrete version of the inner product
and basis used for the Fourier transform, hence the name.
To convert from the time domain to the frequency domain, we use
the Discrete Fourier transform analysis equation:
f[w] ‚â°
N‚àí1
X
t=0
‚ü®ew, et‚ü©f[t] =
1
‚àö
N
N‚àí1
X
t=0
f[t]e‚àíi 2œÄw
N t.
(DFTA)
This is the usual change-of-basis formula from the standard basis in
the time domain {et} to the Discrete Fourier basis {ew}.
To convert a signal from the frequency domain back to the time
domain, we use the Discrete Fourier transform synthesis equation:
f[t] ‚â°
N‚àí1
X
w=0
‚ü®et, ew‚ü©f[w] =
1
‚àö
N
N‚àí1
X
w=0
f[w]ei 2œÄw
N t.
(DFTS)
Again, this inverse change-of-basis transformation can be understood
a special case of the standard change-of-basis formula that we learned
in Section 5.3 (see page 198).
Sampling signals
The sampling rate is an important practical con-
sideration to take into account when converting analog signals to dig-
ital form. Usually the samples of the continuous signal f(t) are taken
at uniform time intervals spaced by time ‚àÜt. The sampling rate or
sampling frequency fs is deÔ¨Åned as the inverse of the time between
samples: fs =
1
‚àÜt. For good performance, we must choose the sam-
pling frequency fs to be at least double that of the higher frequency
of interest in the signal we're sampling.
For example, the upper limit of the human hearing range is 20 kHz,
therefore CD digital audio recordings are sampled at fs = 44.1 kHz.
This means one second of audio recoding corresponds to 44 100 dis-
crete audio samples, which we'll denote as a sequence f[0], f[1], f[2],
. . ., f[44099]. The sample f[0] corresponds to the value of the signal
at t = 0, f[1] corresponds to the the value of f(t) at the next sampling
time t =
1
fs , and so on for the rest of the samples f[t] ‚â°f(t 1
fs ).
Digital signal processing
Fourier transforms play a central role in digital signal processing.
Many image compression and sound compression algorithms depend
on the Discrete Fourier transform.
For example mp3 compression
works by cutting up a song into short time segments, transforming
these segments to the frequency domain using the Discrete Fourier
transform, and then "forgetting" the high frequencies components.

386
APPLICATIONS
Below we give a high-level overview of the pipeline of transformations
used for mp3 encoding and playback. We'll assume we're starting from
a digital song recoding in the wav format. A wav Ô¨Åle describes the
song's sound intensity as a function of time, or we can say the song
is represented in the time basis [song]t.
The idea behind mp3 encoding is to cut out the frequency com-
ponents of a song that we cannot hear. The Ô¨Åeld of psychoacoustics
develops models of human hearing which can inform us of which fre-
quencies can be dropped without degrading the song's playback much.
Without going into details, we'll describe the human psychoacoustic
processing as a projection in the frequency domain [Œ†]
f
f. The overall
mp3 encoding and playback pipeline can be described as follows:
[song‚Ä≤]t = [1]
t
f
|
{z
}
mp3 playback
[Œ†]
f
f
[1]
f
t[song]t.
|
{z
}
mp3 encoding
The sound information Ô¨Çows from right to left in the above equa-
tion.
First we use the Fourier transform, denoted
[1]
f
t, to trans-
forms the song from the time-basis to the frequency-basis, where
the psychoacoustic projection is applied
[Œ†]
f
f.
An mp3 Ô¨Åle is the
frequency-domain representation of the song, after applying the pro-
jection: [song‚Ä≤]f = [Œ†]
f
f
[1]
f
t[song]t. Compression ratios up to a fac-
tor of 10 can be achieved using this approach: a 50 MB wav Ô¨Åle can be
compressed to a 5 MB mp3 Ô¨Åle. During playback, the inverse change-
of-basis [1]
t
f is used to transforms the song from the frequency-basis
back to the time-basis.
Obviously some level of sound degradation occurs in this process,
and the song you'll hear during playback [song‚Ä≤]t will be diÔ¨Äerent
from the original [song]t. If good models of human hearing are used,
the diÔ¨Äerences should be mostly imperceptible. Crucially to the mp3
encoding process, the psychoacoustic models are applied in the fre-
quency domain. This is an example where the Fourier transform as a
building block for a more complex procedure.
Discussion
In this section we learned about three diÔ¨Äerent Fourier transforma-
tions: the Fourier series, the Fourier transform, and the Discrete
Fourier transform. All the scary-looking formulas which we saw can
understood as special cases of the same idea: Fourier transformations
are diÔ¨Äerent types of change-of-basis to a set of orthogonal functions
of the form eiœât, which form a basis in the frequency domain.
It may seem like the Fourier series with its sine and cosine terms is
a diÔ¨Äerent kind of beast, but only until you recall Euler's formula eiŒ∏ =
cos Œ∏ + i sin Œ∏ (see page 106). Using Euler's formula we can express

8.11
FOURIER ANALYSIS
387
the Fourier series of a function as a sequence of complex coeÔ¨Écients
cn ‚ààC that contain the combined information of both cos- and sin-like
components of a periodic signal f(t). The complex Fourier coeÔ¨Écients
cn are obtained using the formula
cn =
Z T
0
D
ei 2œÄn
T
t, et
E
f(t) dt = 1
T
Z T
0
e‚àíi 2œÄn
T
tf(t) dt.
Problem P8.3 will ask you to verify the connection between the
Fourier series of complex exponentials and the Fourier series of sines
and cosines discussed above.
The Fourier change of basis is a general principle that can be ap-
plied in many diÔ¨Äerent contexts. The transforms discussed above used
the family of orthogonal functions eiœât, but there are other sets of or-
thogonal functions that can be used as alternate bases for diÔ¨Äerent
applications. To mention but a few, we have the Bessel functions JŒª,
the spherical harmonics Ymn(Œ∏, œà), the Legendre polynomials Pn(x),
the Hermite polynomials Hn(x), and the Laguerre polynomials Ln(x).
All these families of functions form orthogonal sets on diÔ¨Äerent inter-
vals and with respect to diÔ¨Äerent inner products.
Links
[ Visualizations of Fourier synthesis of a square wave signal ]
http://codepen.io/anon/pen/jPGJMK/
http://bgrawi.com/Fourier-Visualizations/
[ Excellent video tutorial about digital audio processing ]
http://xiph.org/video/vid2.shtml
[ Website with animations that explain signal processing concepts ]
http://jackschaedler.github.io/circles-sines-signals/
[ The Wikipedia pages of the three Fourier transforms described ]
https://en.wikipedia.org/wiki/Fourier_series
https://en.wikipedia.org/wiki/Fourier_transform
https://en.wikipedia.org/wiki/Discrete_Fourier_transform
[ A nice discussion on math.stackexchange.com ]
https://math.stackexchange.com/questions/1002/
[ Orthogonal polynomials and generalized Fourier series ]
http://math24.net/orthogonal-polynomials.html
Exercises
E8.11 Recall the functions en(x) ‚â°sin( nœÄ
L x) that can be used to
describe all vibrations of a guitar string of length L. Verify that e1(x)

388
APPLICATIONS
and e2(x) are orthogonal functions by computing the inner product
‚ü®e1(x), e2(x)‚ü©. Use the inner product deÔ¨Ånition from page 378.
Hint: You might Ô¨Ånd the double angle formula from page 64 useful.
E8.12 Explain why the Fourier series (a0, a1, b1, a2, b2, . . .) of a peri-
odic function f(t) contains a coeÔ¨Écient a0 but not a coeÔ¨Écient b0.
Discussion
We have only scratched the surface of all the possible problem domains
that can be modelled using linear algebra. The topics covered in this
chapter are a small sample of the range of scientiÔ¨Åc, computing, and
business activities that beneÔ¨Åt from the use of matrix methods and
understanding of vector spaces. Linear algebra allows you to perform
complex numerical procedures using a high level of abstraction.
Normally, a linear algebra class should end here. We've covered
all the information you need to know about vectors, matrices, lin-
ear transformations, vector spaces, and also discussed several appli-
cations. Feel free to close this book now, feeling content that your
valiant eÔ¨Äort to learn linear algebra is done. But perhaps you would
like to learn some further topics and make use of your linear algebra
skills? If this sounds interesting, I encourage you to keep on reading,
as there are two more chapters of cool "optional material" ahead.
Chapter 9 covers the basics of probability theory, Markov chains,
and the idea behind Google's PageRank algorithm for classifying web
pages.
Probability theory is not directly related to linear algebra,
but the applications we'll discuss make heavy use of linear algebra
concepts.
The laws of quantum mechanics govern physics phenomena at the
femto-scale‚Äîthink individual atoms. It's a common misconception
to assume quantum laws are somehow mysterious or counterintuitive,
and perhaps they are for people without the appropriate math back-
ground. Chapter 10 contains a concise introduction to the principles
of quantum mechanics speciÔ¨Åcally tailored to people who know linear
algebra. The material is adapted from lectures the author prepared
for a graduate-level introductory course, so no dumbing down or sim-
pliÔ¨Åcation will be done. With your background in linear algebra, you
can handle the real stuÔ¨Ä.

8.12
APPLICATIONS PROBLEMS
389
Links
You'll Ô¨Ånd the following resources useful if you want to learn more
about linear algebra applications.
[ Three compilations of linear algebra applications ]
http://aix1.uottawa.ca/~jkhoury/app.htm
https://medium.com/@jeremyjkun/633383d4153f
http://isites.harvard.edu/fs/docs/icb.topic1011412.files/applications.pdf
[ A document that describes many applications in detail ]
http://gwu.geverstine.com/linearalgebra.pdf
[ Thirty-three miniatures: algorithmic applications of linear algebra ]
http://kam.mff.cuni.cz/~matousek/stml-53-matousek-1.pdf
[ A book about linear algebra applications to data science ]
http://amazon.com/Data-Science-from-Scratch/dp/149190142X
8.12
Applications problems
P8.1
Find the adjacency matrix representation of the following graphs:
a)
b)
c)
P8.2
For each of the graphs in P8.1, Ô¨Ånd the number of ways to get from
vertex 1 to vertex 3 in two steps or less.
Hint: Obtain the answer by inspection or by looking at the appropriate
entry of the matrix 1 + A + A2.
P8.3
Consider the signal f(t) that is periodic with period T. The coeÔ¨É-
cients of the Complex Fourier series are deÔ¨Åned using the formula
cn = 1
T
Z T
0
f(t)e‚àíi 2œÄn
T
t dt.
Show that cn = an ‚àíibn where an and bn are the coeÔ¨Écients of the regular
Fourier series for f(t), deÔ¨Åned in terms of cosines and sines.
Hint: Obtain the real and imaginary parts of cn using Euler's formula.


Chapter 9
Probability theory
In this chapter we'll use linear algebra concepts to explore the world
of probability theory. Think of this as a "bonus" chapter because the
topics we'll discuss are not normally part of a linear algebra course.
Given the general usefulness of probabilistic reasoning and the fact
you have all the prerequisites, it would be a shame not to learn a bit
about probability theory and its applications, hence this chapter.
The structure of the chapter is as follows.
In Section 9.1 we'll
discuss probability distributions, which are mathematical models for
describing random events. In Section 9.2 we'll introduce the concept
of a Markov chain, which can be used to characterize the random tran-
sitions between diÔ¨Äerent states of a system. Of the myriad of topics in
probability theory, we've chosen to discuss probability distributions
and Markov chains because they correspond one-to-one with vectors
and matrices.
This means you should feel right at home.
In Sec-
tion 9.3 we'll show how to use matrices to represent links between
nodes in any network, and describe Google's PageRank algorithm for
ranking webpages.
9.1
Probability distributions
Many phenomena in the world around us are inherently unpredictable.
When you throw a six-sided die, one of the outcomes {1, 2, 3, 4, 5, 6}
will result, but you don't know which one. Similarly when you toss
a coin, you know the outcome will be either heads or tails but you
can't predict which outcome will result. Probabilities can be used to
model real-world systems. For example, we can build a probabilistic
model of hard drive failures using past observations. We can then
calculate the probability that your family photo albums will survive
the next Ô¨Åve or ten years. Backups my friends, backups.
391

392
PROBABILITY THEORY
Probabilistic models can help us better understand random events.
The fundamental concept in probability theory is the concept of a
probability distribution, which describes the likelihood of the diÔ¨Äerent
outcomes of a random event.
For example, the probability distri-
bution for the roll of a fair die is pX = ( 1
6, 1
6, 1
6, 1
6, 1
6, 1
6)T, and the
probability distribution for a coin toss is pY = ( 1
2, 1
2)T. Each entry
of a probability distribution corresponds to the probability mass of a
given outcome. A probability mass is a nonnegative number between
0 and 1. Furthermore, the sum of the entries in a probability distribu-
tion is 1. These two conditions are known as the Kolmogorov axioms
of probability.
Strictly speaking, understanding linear algebra is not required for
understanding probability theory. However, vector notation is very
useful for describing probability distributions.
Using your existing
knowledge of vectors and the rules for matrix multiplication will allow
you to understand many concepts in probability theory in a very short
time.
Probabilistic reasoning is a very useful so this digression is
totally worth it.
Random variables
A random variable X is associated with a probability distribution pX.
Before we formally deÔ¨Åne the notion of a probability distribution, we
must introduce some formalism. We denote by X (calligraphic X)
the set of possible outcomes of the random variable X. A discrete
random variable has a Ô¨Ånite set of possible outcomes. For example,
we can describe the outcome of rolling a six-sided die as the random
variable X ‚ààX. The set of possible outcomes is X = {1, 2, 3, 4, 5, 6}.
The number of possible outcomes is six: |X| = 6.
We can describe the randomness that results in a coin toss as a
random variable Y ‚àà{heads, tails}. The possible outcomes of the
coin toss are Y = {heads, tails}. The number of possible outcomes
is two: |Y| = 2.
In the case of the hard disk failure model, we can deÔ¨Åne the random
variable L ‚ààN that describes the years of lifetime of a hard disk before
it fails.
Using the random variable L we can describe interesting
scenarios and reason about them probabilistically. For example, the
condition that a hard disk will function correctly at least 8 years can
be described as L ‚â•8.
The set of all possible outcomes of a random variable is called
the sample space. The probability distribution of a random variable
speciÔ¨Åes the probability mass to each of the possible outcomes in the
sample space.

9.1
PROBABILITY DISTRIBUTIONS
393
Probability distributions
The probability distribution pX of a discrete random variable X ‚ààX
is a vector of |X| nonnegative numbers whose sum equals one. Using
mathematically precise notation, we write this deÔ¨Ånition as
pX ‚ààR|X|
such that
pX(x) ‚â•0, ‚àÄx ‚ààX
and
X
x‚ààX
pX(x) = 1.
A probability distribution is an ordinary vector in R|X| that satisÔ¨Åes
two special requirements: its entries must be nonnegative and the
sum of the entries must be one. Starting from these simple principles
for describing probabilistic phenomena, we can build a rich science
of random events, expectations, statistics, and develop methods for
predicting the likelihood of future events. In the remainder of this
section, we'll learn about probabilistic reasoning concepts through a
series of examples.
Example 1
Recall the random variables X that describes the out-
come of rolling a six-sided die. Assuming the die is fair, the probability
distribution of the random variable X is
pX =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
6
1
6
1
6
1
6
1
6
1
6
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
‚ÜêpX(1)
‚ÜêpX(2)
‚ÜêpX(3)
‚ÜêpX(4)
‚ÜêpX(5)
‚ÜêpX(6)
.
Note the unusual notation for referring to the entries of the probability
distribution pX:
the xth element of pX
‚â°
pX(x)
‚â°
Pr({X = x}).
Using the normal vector notation, we could denote pX(x) as pXx, but
using two subscripts might be confusing. When the random variable
X is clear from the context, we can use the lighter notation px to
denote the entry pX(x).
The notation Pr({X = x}) is the most precise. The number pX(x)
corresponds to the probability denoted Pr of the event that a ran-
dom draw from X results in the outcome x, denoted {X = x}. We
postpone the discussion about random events until the next section,
and continue with more examples of random variables and associated
probability distributions.

394
PROBABILITY THEORY
Example 2
The random variable Y which describes the outcome
of a coin toss is
pY =
"
1
2
1
2
#
‚ÜêpY(heads)
‚ÜêpY(tails)
Strictly speaking the probability distribution pY is a function of the
form pY : {heads, tails} ‚Üí[0, 1], but for the sake of notational
convenience, we associate the probability of heads and tails with
the Ô¨Årst and second entries of a two-dimensional vector pY. This is a
book about linear algebra, so everything must be turned into a vector!
Interpretations of probability theory
One approach for understanding probability theory is to think about
probabilities as describing relative frequencies of occurrence for the
diÔ¨Äerent possible outcomes. The quantity pX(a) represents the pro-
portion of outcomes of the event {X = a} among all possible outcomes
of X.
This suggest an approach for estimating the probability mass of
each of the possible outcomes. To characterize some real-world phe-
nomenon as the random variable X, we estimate the probability distri-
bution of X by repeating the real-world phenomenon n times, where
n grows to inÔ¨Ånity. The probability mass of each of outcome a ‚ààX
is deÔ¨Åned as the following limit:
pX(a) ‚â°N({X = a}, n)
n
,
n ‚Üí‚àû,
where N({X = a}, n) is number of times the outcome a occurs during
n repetitions. If I tell you the probability mass of outcome {X = a}
is 0.3, this tells you that if you take 1000 draws from the random
variable X, you can expect approximately 300 of those draws will
result in outcome {X = a}.
This is called the frequentist point of view about probabilities,
which is well suited for thinking about events that can be repeated
many times. In practice we never really have the leisure to repeat
events an inÔ¨Ånite number of times to obtain the exact probabili-
ties, so you have to think of this frequentist deÔ¨Ånition as a thought
experiment‚Äînot something you would do in practice.
Another way to think about probabilities is as the state of knowl-
edge or belief about the world. Instead of describing some objective
reality, the random variable X its probability distribution pX repre-
sents our state of knowledge about the real-world phenomenon that
X describes. Since pX represents our state of knowledge about the
random variable X, it makes sense to update the distribution pX as
we learn new facts. This is called the Bayesian point of view, named

9.1
PROBABILITY DISTRIBUTIONS
395
after Thomas Bayes who was an 18th century statistician. Consider
the following example of Bayesian-style reasoning. You're given a coin
and asked to come up with a probability distribution that describes
its chances of falling heads or tails. Initially you have no informa-
tion about the coin being biased either way, so it would make sense to
start win the initial belief that the coin is fair. This is called the prior
belief or simply prior. If you toss the coin a few times and obtain
much more heads than tails, you can then update your belief about
the coin's probability distribution to take into account the observed
data. The speciÔ¨Åc technique for updating the prior belief in the light
of new observations is called Bayes' rule.
Both the frequentist and Bayesian points of view lead to useful
techniques for modelling random events.
Frequentist methods are
concerned with drawing principled conclusions given a set of empirical
observations. Bayesian models are generally more Ô¨Çexible and allow us
to combine empirical observations with priors that encode the domain
knowledge provided by experts. The frequentist approach is useful
when you want to analyze data, prepare reports, and come up with
hard numbers about the data. The Bayesian approach is more useful
for building machine learning applications like voice recognition.
Conditional probability distributions
Probability theory allows us to model dependencies between random
variables. We use conditional probability distributions to describe sit-
uations where one random variable depends on another.
Consider
the random variable X whose random outcomes depend on another
variable Y . To describe this probability dependence we must specify
the probability distributions of X for each of the possible values of
Y . This information is expressed as a conditional probability distri-
bution:
pX|Y(x|y) ‚â°Pr({X = x} given {Y = y}).
The vertical bar is pronounced "given," and it is used to separate the
unknown random variable from the random variable whose value is
known. The distribution pX|Y satisÔ¨Åes the conditions for a probability
distribution for all y ‚ààY:
pX|Y ‚ààR|X|√ó|Y| s.t. pX|Y(x|y) ‚â•0 and
X
x‚ààX
pX|Y(x|y) = 1, ‚àÄy ‚ààY.
It is natural to represent pX|Y as a |X|√ó|Y| matrix, with each column
representing the probability of X for a given value of Y .
Example 3
Consider a probability distribution with six possible
outcomes obtained by rolling one of two diÔ¨Äerent dice. If we're given

396
PROBABILITY THEORY
the fair die, the probability distribution is pXf = ( 1
6, 1
6, 1
6, 1
6, 1
6, 1
6)T.
If we're given the biased die the probability distribution is pXb =
( 1
4, 1
4, 1
4, 1
4, 0, 0)T. Introducing the conditioning variable Y that de-
scribes which die is used, we can express the situation with the two
dice as the following conditional probability distribution:
pX|Y ‚â°
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
6
1
4
1
6
1
4
1
6
1
4
1
6
1
4
1
6
0
1
6
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
The Ô¨Årst Ô¨Årst column corresponds to the fair die Y = f, and the
second column corresponds to the biased die Y = b.
Combining the probability distributions for the two dice pXf and
pXb into a single conditional distribution pX|Y allows us to construct
more complex probabilistic structures, as illustrated in the next ex-
ample.
Example 4
Consider an experiment in which the outcome of a coin
toss Y decides which die we'll throw‚Äîthe fair die or the biased die.
Suppose the coin is biased with pY = ( 3
4, 1
4)T. If the outcome of the
coin toss is heads, we roll the fair die. If the coin toss gives tails, we
roll the biased die. We're interested in describing the random variable
X which corresponds to a die roll in which the fair die is used 3
4 of the
time and the biased die is used 1
4 of the time. What is the probability
distribution pX for X?
To model this situation, we combine the "which die" probability
distribution pY = ( 3
4, 1
4)T and the conditional probability distribution
pX|Y obtained in previous example:
pX(x) ‚â°
X
y‚ààY
pX|Y(x|y)pY(y)
= pX|Y(x|f)pY(heads) + pX|Y(x|b)pY(tails)
= pX|Y(x|f) 3
4
+ pX|Y(x|b) 1
4
= 3
4( 1
6, 1
6, 1
6, 1
6, 1
6, 1
6)T + 1
4( 1
4, 1
4, 1
4, 1
4, 0, 0)T
= ( 3
16, 3
16, 3
16, 3
16, 1
8, 1
8)T.
The probabilistic mixture of two random events corresponds to a lin-
ear combination. When you head "linear combination" you immedi-
ately think "matrix-vector product representation," right? Indeed, we

9.1
PROBABILITY DISTRIBUTIONS
397
can also express pX as a matrix-vector product between the condi-
tional probability distribution pX|Y (a matrix) and distribution pY (a
vector):
pX =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
6
1
4
1
6
1
4
1
6
1
4
1
6
1
4
1
6
0
1
6
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
" 3
4
1
4
#
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
3
16
3
16
3
16
3
16
1
8
1
8
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
You can verify that P
x pX(x) = 1 as expected.
Note how easy it is to compose conditional probability distribu-
tions to describe complicated random events in terms of the matrix-
vector product. The notion of a conditional probability distribution
is a fundamental building block of probability theory. Many mod-
ern machine learning techniques use probabilistic models constructed
from conditional probability distributions.
Discussion
We described the basic notions in probability theory like random vari-
ables, and discussed probability distributions and conditional prob-
ability distributions. These concepts are the bread-and-butter con-
cepts of probabilistic reasoning.
Let's take a minute to recap and
summarize the new material and link it back to vectors and linear
transformations, which are the main subjects of the book. The prob-
ability distribution of a discrete random variable is a vector of real
numbers. The probability distribution describing the outcome of a six
sided die is a six-dimensional vector pX ‚ààR6. Probability distribu-
tions are vectors that obey some extra constraints. Each entry must
be a positive number and the sum of the entries in the vector must
be one.
Conditional probability distributions are mappings that describe
how a set of "given" variables inÔ¨Çuence the probabilities of a set of
"outcome" random variables.
Conditional probability distributions
can be represented as matrices, where each column of the matrix con-
tains the outcome distribution for one of the values of the given ran-
dom variable. A conditional probability distribution with Ô¨Åve possible
outcomes, conditioned on a "given" variable with ten possible states
is represented as a 5 √ó 10 matrix, whose columns all sum to one.
Conditional probability distributions are a very powerful tool for
modelling real-world scenarios.
For example, you could describe a
noisy communication channel as a conditional probability distribution
of the channel outputs given each of the possible channel inputs. Many

398
PROBABILITY THEORY
machine learning algorithms involve the characterization, estimation,
and exploitation of conditional probability distributions‚Äîcollectively
called the model.
In the next section we'll learn about an application of conditional
probability distribution for describing the state of a system under-
going random transitions between a number of possible states. This
is an interesting special case of a conditional probability distribution
for which conditioning and outcome random variable are of the same
dimension. Indeed, the outcome variable and the conditional space
both represent states of the same system at diÔ¨Äerent times.
Links
[ Discussion on the Bayesian way of thinking about probabilities ]
https://en.wikipedia.org/wiki/Bayesian_probability
[ Detailed discussion about Bayes rule from Ô¨Årst principles ]
http://yudkowsky.net/rational/bayes
http://yudkowsky.net/rational/technical
Exercises
E9.1 Do the following vectors represent probability distributions:
a)
  1
2, 1
2, 1
2
T
b)
  1
4, 1
4, 1
4, 1
4
T
c)(0.3, 0.3, ‚àí0.1, 0.5)T
9.2
Markov chains
So far we talked about random events without any reference to the
Ô¨Çow of time. In this section we'll combine the idea of random variables
with the notion of time. A random process is a model of a system
that undergoes transitions between states over time. The state of the
system at time t is described by a random variable Xt. We assume
the state of the system depends on the previous states through a
conditional probability distribution:
pXt+1|XtXt‚àí1...X0(xt+1|xtxt‚àí1 . . . x0).
This means the random variable Xt+1 depends on the previous states
of the system: Xt, Xt‚àí1, Xt‚àí2, . . ., X0.
Studying such history-
dependent processes is a formidable task because of the myriad of
possible inÔ¨Çuences from past states. Faced with this complexity, we
can make a useful simplifying assumption and consider memoryless
random processes.

9.2
MARKOV CHAINS
399
A Markov process or Markov chain is a random process for which
the transition probabilities for the next state depend only on the
current state and not on states before it:
pXt+1|XtXt‚àí1...X0(xt+1|xtxt‚àí1 . . . x0) = pXt+1|Xt(xt+1|xt).
The next state Xt+1 depends only on the current Xt and not on the
pas history of the process: Xt‚àí1, Xt‚àí2, . . ., X0.
Thus, a Markov
chain can be fully described by a conditional probability distribution
pXt+1|Xt(xt+1|xt), which describes the probability of the next state of
the system given the current state.
Markov chains are an extremely versatile model for many real-
world systems. We don't have the space here to cover the topic in
full detail, but I plan to introduce the basic notions so you'll know
about them. First we'll show the connection between the evolution
of a Markov chain and the matrix product. Next we'll see how your
eigenvalue-Ô¨Ånding skills can be used to compute an important prop-
erty of Markov chains. Understanding Markov chains is also necessary
background material for understanding Google's PageRank algorithm,
which we'll discuss in Section 9.3.
Example
Consider three friends who are kicking around a football in the park:
Alice, Bob, and Charlie. When Alice gets the ball, the makes a pass
to Bob 40% of the time, a pass to Charlie 40% of the time, or holds
on to the ball 20% of the time. Bob is a bit of a greedy dude: when
he gets the ball he holds on to it 80% of the time, and is equally likely
to pass to Alice or Charlie in to remaining 20% of the time. When
Charlie gets the ball he's equally likely to pass the ball to Alice, Bob,
or keep it for himself. Assume these friends kick the ball around for
a very long time (hundreds of passes) and you observe them at some
point. What is the probability that each players will be in possession
of the ball at the instant when you observe them?
We can model the ball possession as a Markov process with three
possible states: X = {A, B, C}, each describing the "state" of the
football game when Alice, Bob, or Charlie has the ball. The transition
probabilities pXt+1|Xt(xt+1|xt) describe how the next ball possession
xt+1 depends on the previous state of the ball possession xt. The
transition matrix of the Markov chain in our current example is
pXt+1|Xt ‚â°
Ô£Æ
Ô£ØÔ£∞
0.2
0.1
0.3
0.4
0.8
0.3
0.4
0.1
0.3
Ô£π
Ô£∫Ô£ª‚â°M.

400
PROBABILITY THEORY
For consistency with the notation of conditional probability distribu-
tions, we refer to the coeÔ¨Écients of this matrix M as pXt+1|Xt(xt+1|xt).
The "given" variable xt selects the column of the matrix M and the
diÔ¨Äerent entries in this column represent the transition probabilities
for that state. Using the matrix M and some basic linear algebra tech-
niques we can calculate the probability of Ô¨Ånding the ball in any given
player after many iterations of the "pass the ball" Markov process.
C
A
B
0.2
0.4
0.4
0.8
0.1
0.1
0.3
0.3
0.3
Figure 9.1: Graphical representation of the transition probabilities be-
tween the three states: "Alice has the ball," "Bob has the ball," and "Charlie
has the ball."
Let's walk through an example calculation, in which we assume
the ball starts with Alice initially. Since we know for certain that Alice
has the ball at t = 0, the initial state of the system is described by
the probability distribution pX0 = (1, 0, 0)T, with 100% of the weight
on Alice. The probability of Ô¨Ånding the ball with each player after
one time step is obtained by multiplying the initial probability vector
pX0 by the matrix M:
pX1 = MpX0
=
Ô£Æ
Ô£ØÔ£∞
0.2
0.1
0.3
0.4
0.8
0.3
0.4
0.1
0.3
Ô£π
Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£∞
1
0
0
Ô£π
Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£∞
0.2
0.4
0.4
Ô£π
Ô£∫Ô£ª.
All the "where is the ball" probability mass started out on Alice, but
after one time step it spread to Bob and Charlie, according to Alice's
expected passing behaviour (the Ô¨Årst column in the transition matrix).
We can continue this process to obtain the probability of ball pos-
session after two time steps. We simply multiply the probability vec-
tor pX1 by the transition matrix M, which is the same as multiplying

9.2
MARKOV CHAINS
401
the initial state pX0 by M twice:
pX2 = MMpX0 = MpX1
=
Ô£Æ
Ô£ØÔ£∞
0.2
0.1
0.3
0.4
0.8
0.3
0.4
0.1
0.3
Ô£π
Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£∞
0.2
0.1
0.3
0.4
0.8
0.3
0.4
0.1
0.3
Ô£π
Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£∞
1
0
0
Ô£π
Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£∞
0.213
0.53
0.253
Ô£π
Ô£∫Ô£ª.
The probability that Alice is holding the ball after two time steps
is 0.213. This number represents the combination of three diÔ¨Äerent
"paths" of starting from the state x0 = A and coming to the state
x2 = A at t = 2.
Either Alice kept the ball for two time steps
(x1 = A), or Alice passed the ball to Bob (x1 = B) and then he
passed it back to her, or Alice passed the ball to Charlie (x1 = C)
and Charlie passed it back her. By computing the sum of the products
of the probabilities of the events on each path, we arrive at the value
pX2|X0(A|A) = pX2|X1(A|A) pX1|X0(A|A)
+ pX2|X1(A|B) pX1|X0(B|A)
+ pX2|X1(A|C) pX1|X0(C|A)
= 0.2 √ó 0.2 + 0.1 √ó 0.4 + 0.3 √ó 0.4
= 0.213.
Observe the comparative simplicity of the multiplication by the tran-
sition matrix, which "takes care" of these three diÔ¨Äerent paths auto-
matically. Do you have to manually think about all possible paths?
Nope. Just multiply the state by the transition matrix and you obtain
the state probabilities in the next time step.
Thanks to the power of the Markov chain construction, we can
carry the "ball possession" probabilities vector forward through time
as far as we want to. To calculate the random state pXt+1, multiply by
the previous state vector pXt by the Markov transition probabilities
matrix M. The probability distribution of ball possession after three
time steps is
pX3 = MpX2 = MMpX1 = MMMpX0 = M 3pX0 =
Ô£Æ
Ô£ØÔ£∞
0.1804
0.5964
0.2231
Ô£π
Ô£∫Ô£ª.
Continuing this process, we can obtain the probability state vector
after 10 and after 20 times steps of the Markov chain:
pX10= M 10pX0=
Ô£Æ
Ô£ØÔ£∞
0.161 . . .
0.645 . . .
0.193 . . .
Ô£π
Ô£∫Ô£ª
and
pX20= M 20pX0=
Ô£Æ
Ô£ØÔ£∞
0.1612903 . . .
0.6451612 . . .
0.193548 . . .
Ô£π
Ô£∫Ô£ª.

402
PROBABILITY THEORY
Observe the probability distribution seems to approach a steady state.
This is the "long term" probability question we're looking to answer
in this example.
Stationary distribution
If the evolution of a Markov chain continues for long enough, the prob-
ability vector will converge to a stable distribution pX‚àûthat remains
unchanged when multiplied by M:
MpX‚àû= pX‚àû.
This is called the stationary distribution of the Markov chain. Observe
that pX‚àûis an eigenvector of the matrix M with eigenvalue Œª = 1.
The convergence to a unique stationary distribution is a funda-
mental property of Markov chains. Assuming the Markov chain rep-
resented by M satisÔ¨Åes some technical conditions (that we won't go
into), it will converge to a stationary distribution pX‚àû. Thus, if we
want to Ô¨Ånd pX‚àûwe just have to keep repeatedly multiplying by M
until the distribution stabilizes:
pX‚àû=
Ô£Æ
Ô£ØÔ£∞
0.161290322580645 . . .
0.645161290322581 . . .
0.193548387096774 . . .
Ô£π
Ô£∫Ô£ª= M ‚àûpX0.
The Markov chain will converge to the same stationary distribution
pX‚àûregardless of the starting point pX0. The ball could have started
with Bob (0, 1, 0)T or with Charlie (0, 0, 1)T, and after running the
Markov chain for long enough we would still arrive at the stationary
distribution pX‚àû.
Since we know the stationary distribution is an eigenvector of M
with eigenvalue Œª = 1, we can use the usual eigenvector-Ô¨Ånding tech-
niques to obtain pX‚àûdirectly. We Ô¨Ånd the eigenvector by solving for
‚Éóv in (M ‚àí1)‚Éóv = ‚Éó0. The answer is pX‚àû= ( 5
31, 20
31, 6
31)T. A nice beneÔ¨Åt
of this approach is that we obtain the exact analytic expression for
the answer.
Discussion
Markov chains, despite being the simplest type of random process,
have countless applications in physics, speech recognition, information
processing, machine learning, and many other areas. Their simple
memoryless structure and their intuitive representation as matrices
make them easy to understand and easy to Ô¨Åt to many situations.

9.2
MARKOV CHAINS
403
In the next section we'll describe a Markov chain model for peo-
ple's Web browsing behaviour.
The stationary distribution of this
Markov chain serves to quantify the relative importance of webpages.
Links
[ Awesome visual representation of states and transitions ]
http://setosa.io/blog/2014/07/26/markov-chains/index.html
[ More details about applications from wikipedia ]
https://en.wikipedia.org/wiki/Markov_chain
Exercises
E9.2 After reading the section on Markov chains, you decide to re-
search the subject further and get a book on Markov chains from the
library. In the book's notation, Markov chains are represented using
right-multiplication of the state vector: ‚Éóv ‚Ä≤ = ‚ÉóvB, where ‚Éóv is the state
of the system at time t, ‚Éóv ‚Ä≤ is the state at time t + 1, and the matrix
B represents the Markov chain transition probabilities.
Find the matrix B that corresponds to the transition probabilities
discussed in Example 5. How is this matrix B related to the matrix
M, which we used in Example 5?
B = M T.
E9.3 Use the SymPy Matrix method .eigenvects() to conÔ¨Årm the
stationary probability distribution of the passing-the-ball Markov chain
is indeed
  5
31, 20
31, 6
31

.
Hint: To obtain the exact result, make sure you deÔ¨Åne the matrix A
using fractional notation:
>>> M = Matrix([[ 2/10, 1/10, 1/3 ],
[ 4/10, 8/10, 1/3 ],
[ 4/10, 1/10, 1/3 ]])
Use ev = M.eigenvects()[0][2][0] to extract the eigenvector that
correspond to the eigenvalue Œª = 1, then normalize the vector by its
1-norm to make it a probability distribution pinf = ev/ev.norm(1).
E9.4 Find the stationary probability distribution of the following
Markov chain:
C =
Ô£Æ
Ô£ØÔ£∞
0.8
0.3
0.2
0.1
0.2
0.6
0.1
0.5
0.2
Ô£π
Ô£∫Ô£ª.
E9.5 Repeat the previous problem using the nullspace method for
SymPy matrices to obtain pX‚àûby solving (C ‚àí1)‚Éóv = ‚Éó0.

404
PROBABILITY THEORY
9.3
Google's PageRank algorithm
Consider the information contained in the links between web pages.
Each link from Page A to Page B can be interpreted as a recommen-
dation, by Page A's author, for the contents of Page B. In web-speak
we say links from Page A to Page B are "sending eyeballs" to Page B,
presumably because there is something interesting to see on Page B.
These observations about "eyeball worthiness" are the inspiration be-
hind Google's PageRank algorithm. We Ô¨Ånd a good summary of the
idea behind PageRank in the 2001 patent application:
A method assigns importance ranks to nodes in [...] the
world wide web or any other hypermedia database. The
rank assigned to a document is calculated from the ranks
of documents citing it. In addition, the rank of a document
is calculated from a constant representing the probability
that a browser through the database will randomly jump
to the document.
‚Äî Patent US6285999
You may notice there is a weird-self referential thing going on in
the deÔ¨Ånition.
The rank of a document depends on the ranks of
documents that link to it, but how do you discover the ranks of these
documents? By calculating the ranks of documents that link to them,
and so on and so forth. Don't worry if all this sounds confusing, it
will all start to make sense very soon.
The random surfer model
Imagine someone who browses the Web by clicking on links randomly.
We'll call this person Randy. Every time Randy opens his web browser
he follows the following two strategies:
1. When visiting a webpage, he'll make a list of all the outbound
links on that page and click on one of the links at random.
2. Randy randomly selects a page on the Web and goes to it.
Randy follows Strategy 1 90% of the time and Strategy 2 the remain-
ing 10% of the time. In the unlikely event that he reaches a page
with no outbound links, he'll switches to Strategy 2 and jumps to a
random page.
You have to agree that Randy's behaviour is pretty random! This
is a very simple model for the behaviour of users on the Web that
assumes people either follow links blindly, or randomly jump to pages
on the web.
Simple as it be, this model allows us to capture an
important aspect of "relative importance" of diÔ¨Äerent webpages. In
the next section we'll describe the random surfer model using the
machinery of Markov chains.

9.3
GOOGLE'S PAGERANK ALGORITHM
405
The PageRank Markov chain
We want to construct a Markov chain that models the behaviour of
Randy, the random Web surfer. The state in the Markov chain corre-
sponds to the the webpage Randy is currently viewing, and the tran-
sition matrix will describes Randy's link following behaviour. This
probabilistic reformulation will help us calculate the PageRank vec-
tor, which describes the "importance" of each webpage on the Web.
We'll now revisit the two random strategies used by Randy to
browse the Web, and construct the appropriate Markov chain matrices
to describe them. Note the dimension of the matrices is n √ó n, where
n is the number of webpages on the Web.
1. Suppose Randy is visiting webpage j, which has a total of Nj
outbound links. Then the column for Page j should transition
probability
1
Nj toward each of the pages that Page j links to. In
the special case that Page j has zero outbound links, we'll Ô¨Åll
column j with the uniform distribution ( 1
n, 1
n, . . . , 1
n)T. Thus,
we can describe ijth entry of the transition matrix for Strategy 1
as follows:
(M1)ij =
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
1
Nj
if there's a link from Page j to Page i
1
n
if Page j has not outbound links
0
otherwise
2. The transition matrix for Strategy 2 is much simpler. No matter
which page Randy was visiting previously, we want him to jump
to a random page on the Web, thus each column of the transition
matrix M2 will contain the uniform distribution over all pages
( 1
n, 1
n, . . . , 1
n)T. We can express this succinctly as:
M2 = 1
n J,
where J is a matrix that has ones in all its entries.
Recall that Randy uses Strategy 1 90% of the time and Strategy 2
10% of the time. The Markov chain M that describes his behaviour
is a linear combination of the Markov chains for the two strategies:
M = (1 ‚àíŒ±)M1 + Œ±M2 = (1 ‚àíŒ±)M1 + Œ±
n J,
where Œ± = 0.1 if we want to have the 90-10 mix of strategies, but in
general Œ± is a parameter we can tune for diÔ¨Äerent applications.
The PageRank vector is the stationary distribution of the Markov
chain M, which is deÔ¨Åned through the equation MpX‚àû= pX‚àû. We can

406
PROBABILITY THEORY
obtain pX‚àûby Ô¨Ånding the eigenvector of M in the eigenspace Œª1 = 1,
which is equivalent to solving the null space problem (M ‚àí11)‚Éóe =
‚Éó0. We can also the stationary distribution pX‚àûby simply running
the Markov chain for many iterations, until we see the probability
distribution converge:
{ pX‚àû} = N(M ‚àí1)
‚áî
pX‚àû= M ‚àûpX0.
Both approaches for Ô¨Ånding pX‚àûare useful in diÔ¨Äerent contexts. Solv-
ing the null space problem gives us the answer directly, whereas the
second approach of iterating the Markov chain is more scalable for
large graphs.
The PageRank value of a webpage is the probability that Randy
will end up on a given page after running the Markov chain for a
suÔ¨Éciently long time. The PageRank vector pX‚àûcaptures an impor-
tant aspect of well-connectedness or importance of webpages. Let's
illustrate the entire procedure for calculating the PageRank vector in
a simple graph.
Example: micro-Web
We'll now study the micro-Web illustrated in Figure 9.2. This is a
simpliÔ¨Åed version of the link structure between webpages on the Web.
Like a vast simpliÔ¨Åcation! Instead of the billions of webpages of Web,
the micro-Web has only eight webpages {1, 2, 3, 4, 5, 6, 7, 8}. Instead
of the trillions of links between webpages, the micro-Web has only
fourteen links {(1, 2), (1, 5), (2, 3), (2, 5), (3, 1), (3, 5), (4, 5), (5, 6),
(5, 7), (6, 3), (6, 7), (7, 5), (7, 6), (7, 8)}.
Still, this simple example
is suÔ¨Écient to illustrate the main idea of the PageRank algorithm.
Scaling the solution from the case n = 8 to problem n = 12 000 000 is
left as an exercise for the reader.
We'll Ô¨Årst construct the transition matrix M1 that corresponds to
Strategy 1 (follow a random outbound link), then construct matrix
M2 for Strategy 2 (teleport to a random page), and construct a 90-
10-weighted linear combination of M1 and M2, which is the PageRank
Markov chain matrix.
The state of the Markov chain we want to constructs corresponds
to the probability distribution of Ô¨Ånding Randy on each of the eight
pages.
Let's say he is currently on Page 1, so the initial state of
the Markov chain is pX0 = (1, 0, 0, 0, 0, 0, 0, 0)T. Following Strategy 1,
Randy is supposed to go to either Page 2 or Page 5. Since there are
two possible outbound links, the probability mass of choosing one of
them is 1
2. Computing M1pX0 has the eÔ¨Äect of "selecting" the Ô¨Årst
column of M1, so we know what the Ô¨Årst column of M1 is. If we
then "probe" M1 with pX0 = (0, 1, 0, 0, 0, 0, 0, 0)T, which corresponds

9.3
GOOGLE'S PAGERANK ALGORITHM
407
Figure 9.2: Graph showing the links between the pages on the micro-Web.
Page 5 seems to be an important page because many pages link to it. Since
Page 5 links to pages 6 and 7, these pages will probably get a lot of eyeballs
too. Page 4 is the least important since there are no links to it. Page 8 is
an example of the "unlikely" case of a webpage with no outbound links.
to Randy following links randomly starting at Page 2, we'll know
what's in the second column of M1. Continuing with the standard
"probing" approach for building matrix representations, we Ô¨Ånd the
rest of the entries in the transition matrix for Strategy 1:
M1 =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
0
1
2
0
0
0
0
1
8
1
2
0
0
0
0
0
0
1
8
0
1
2
0
0
0
1
2
0
1
8
0
0
0
0
0
0
0
1
8
1
2
1
2
1
2
1
0
0
1
3
1
8
0
0
0
0
1
2
0
1
3
1
8
0
0
0
0
1
2
1
2
0
1
8
0
0
0
0
0
0
1
3
1
8
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
You should convince yourself M1 has the right entries by comparing
the rest of the columns with link structure in Figure 9.2.
Recall
Strategy 1 handles the exceptional case of a page with no outbound
links by jumping to a random page, since there are eight pages in
total on the micro-Web, the uniform distribution over all pages is
( 1
8, 1
8, 1
8, 1
8, 1
8, 1
8, 1
8, 1
8)T.
Do you recall the adjacency matrix representation for graphs we
discussed in Section 8.4? You can obtain M1 by taking the trans-
pose of the adjacency matrix A then normalizing the columns so they
become probability distributions.
The Markov chain for Strategy 2 is to jump to a random page on

408
PROBABILITY THEORY
the micro-Web:
M2 = 1
8 Jn =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Note the new notation Jn which corresponds to a n √ó n matrix with
ones in all its entries.
The PageRank Markov chain M is deÔ¨Åned in terms of the matrices
M1 and M2 and a mixture parameter, which we denote Œ±. Choosing
Œ± = 0.1 models a Randy who uses Strategy 1 90% of the time and
Strategy 2 10% of the time:
M = (1 ‚àíŒ±)M1 + Œ±
8 J
= 9
10 M1 + 1
80 J,
which is worth writing out in full detail:
M =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
80
1
80
37
80
1
80
1
80
1
80
1
80
1
8
37
80
1
80
1
80
1
80
1
80
1
80
1
80
1
8
1
80
37
80
1
80
1
80
1
80
37
80
1
80
1
8
1
80
1
80
1
80
1
80
1
80
1
80
1
80
1
8
37
80
37
80
37
80
73
80
1
80
1
80
5
16
1
8
1
80
1
80
1
80
1
80
37
80
1
80
5
16
1
8
1
80
1
80
1
80
1
80
37
80
37
80
1
80
1
8
1
80
1
80
1
80
1
80
1
80
1
80
5
16
1
8
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Notice the sprinkling of
1
80s to Ô¨Åll the regions which had only zeros in
M1. This additional mixing is important for technical reasons. If we
were to use just the Markov chain M1, the probability distribution
that describes Randy's state will not mix very well, but adding in
a little bit of Jn guarantees the Markov chain will converge to its
stationary distribution in the long term.

9.3
GOOGLE'S PAGERANK ALGORITHM
409
Having built the PageRank Markov chain for the micro-Web, we
Ô¨Ånd its stationary distribution pX‚àû, which is the PageRank vector:
pX‚àû= (0.08152, 0.05868, 0.1323, 0.02199, 0.2268, 0.1864, 0.2079, 0.08437)T
This vector was obtained by Ô¨Ånding the vector in the Œª1 = 1 eigenspace
of M. The author used live.sympy.org to obtain the result. See
bit.ly/microWebPR for the commands required to solve the null
space problem (M ‚àí1)‚Éóe = ‚Éó0.
We can now use the entires of the PageRank vector to sort the
pages of the micro-Web by their relative importance, as shown in
Table 9.2.
Page ID
PageRank
Page 5
0.22678
Page 7
0.20793
Page 6
0.18642
Page 3
0.13229
Page 8
0.08437
Page 1
0.08152
Page 2
0.05868
Page 4
0.02199
Table 9.2: The PageRanks of the pages from the graph in Figure 9.2.
According their PageRank score, the top two page in the micro-Web
are Page 5 with PageRank 0.22678 and Page 7 with PageRank 0.20793.
Page 6 is not far behind with PageRank 0.18642. Looking back at
Figure 9.2 we can conÔ¨Årm this ranking makes sense, since Page 5 has
the most links pointing to it and it links to Page 6 and Page 7. The
combined PageRank of these three pages is 0.62113, which means that
over 62% of the time Randy's eyeballs will be viewing one of these
three pages.
Discussion
The example we discussed above involved only eight webpages and
an 8 √ó 8 Markov chain matrix. Imagine the size of the matrix we'll
need to represent all the links between webpages on the Web. The
web has upward of a billions webpages and contains trillions of hyper-
links. Imagine an n√ón Markov chain matrix, where n = 1 000 000 000.
You'll need a lot of memory to store this matrix, and a lot of compute
power to work with it. The computation of the PageRank vector for
the entire Web can be performed using the power iteration method,

410
PROBABILITY THEORY
which Ô¨Ånds the eigenvector for the largest eigenvalue of the matrix.
Additionally, careful analysis of the data required to perform the al-
gorithm can avoid the need to construct the impossibly-large n √ó n
matrix. Thus the contribution of Larry Page and Sergey Brin go be-
yond the linear algebra insight, but also involve a clever approach for
splitting the computation onto a cluster of computers. This is some-
thing to keep in mind when you apply your linear algebra knowledge
to build stuÔ¨Ä. You need to cultivate a mix of intuition, backed by
mathematical tools, and engineering prowess to get things done. Go
Ô¨Ånd the right linear combination of these resources and build you own
$300B company.
Links
[ The original PageRank paper ]
http://ilpubs.stanford.edu/422/1/1999-66.pdf
[ Further discussion about the PageRank algorithm ]
https://en.wikipedia.org/wiki/PageRank
[ The power iteration method for Ô¨Ånding the PageRank eigenvector ]
https://en.wikipedia.org/wiki/Power_iteration
Exercises
E9.6 Compute the PageRank of the network of webpages shown in
Figure 8.3 (page 327).
Hint: Using live.sympy.org, you can deÔ¨Åne the stochastic matrix M
using Matrix, then mix it with an all-ones matrix ones(n,n). Use the
nullspace() matrix method to obtain the eigenvector in the Œª = 1
eigenspace.
9.4
Probability problems
P9.1
The probability of heads for a fair coin is p = 1
2. The probability
of getting heads n times in a row is given by the expression pn. What is
the probability of getting heads three times in a row?
P9.2
Consider the weather in a city which has "good" and "bad" years.
Suppose the weather conditions over the years form a Markov chain where
a good year is equally likely to be followed by a good or a bad year, while a
bad year is three times as likely to be followed by a bad year as by a good
year. Given that last year, call it Year 0, was a good weather year, Ô¨Ånd the
probability distribution that describes the weather in Year 1, Year 2, and
Year ‚àû.

Chapter 10
Quantum mechanics
Quantum mechanics was born out of need to explain certain observa-
tions in physics experiments, which could not be explained by previous
physics theories. During the Ô¨Årst half of the 20th century, in exper-
iment after experiment, quantum mechanics principles were used to
correctly predict the outcomes of many atom-scale experiments. Dur-
ing the second half of the 20th century, biologists, chemists, engineers,
and physicists applied quantum principles to all areas of science. This
process of "upgrading" classical models to quantum models leads to
better understanding of reality and better predictions. Scientists like
it that way.
In this chapter we'll introduce the fundamental principles of quan-
tum mechanics. This little excursion into physics-land will expose you
to the ideas developed by the likes of Bohr, Plank, Dirac, Heisenberg,
and Pauli. You have all the prerequisites to learn about some fasci-
nating 20th century discoveries. All it takes to understand quantum
mechanics is some knowledge of linear algebra (vectors, inner prod-
ucts, projections) and some probability theory (Chapter 9). Using
the skill set you've built in this book, you can learn the main ideas of
quantum mechanics at almost no additional mental cost.
This chapter is totally optional reading, reserved for readers who
insist on learning about the quantum world. If you're not interested
in quantum mechanics, it's okay to skip this chapter, but I recom-
mend you check out Section 10.3 on Dirac notation for vectors and
matrices. Learning Dirac notation serves as an excellent review of the
core concepts of linear algebra.
411

412
QUANTUM MECHANICS
10.1
Introduction
The fundamental principles of quantum mechanics can be explained
in the space on the back of an envelope. These simple principles have
far-reaching implications for many areas of science: physics, chem-
istry, biology, engineering, and many other Ô¨Åelds of study. Scientists
have adapted preexisting theories and models to incorporate quantum
principles. Each Ô¨Åeld of study has its own view on quantum mechan-
ics, and has developed a specialized language for describing quantum
concepts. Before we introduce quantum mechanics in the abstract,
let's look at some of the disciplines where quantum principles are
used.
Physics
Physicists use the laws of quantum mechanics as a tool-
box to understand and predict the outcomes of atomic-scale physics
experiments. By "upgrading" classical physics models to reÔ¨Çect the
ideas of quantum mechanics, physicists (and chemists) obtain more
accurate models that lead to better predictions.
For example, in a classical physics model, the motion of a particle
is described by its position x(t) and velocity v(t) as functions of time:
‚Ä¢ classical state: (x(t), v(t)), for all times t.
At any given time t, the particle is at position x(t) and moving with
velocity v(t). Using Newton's laws of motion and calculus, we can
predict the position and the velocity of a particle at all times.
In a quantum description of the motion of a particle in one-dimension,
the state of a particle is represented by a wave function |œà(x, t)‚ü©,
which is a complex-valued function of position x and time t:
‚Ä¢ quantum state: |œà(x, t)‚ü©, for all times t.
At any given time t, the state of the particle corresponds to a complex-
valued function of a real variable |œà(x)‚ü©‚àà{R ‚ÜíC}.
The wave
function |œà(x)‚ü©is also called the probability-amplitude function, since
the probability of Ô¨Ånding the particle at position xa is proportional
to the value of the squared-norm of the wave function:
Pr({particle position = xa})
‚àù
|œà(xa)‚ü©
2.
Instead of having a deÔ¨Ånite position x(t) as in the classical model, the
position of the particle in a quantum model is described by a proba-
bility distribution calculated from its wave function |œà(x)‚ü©. Instead of
having a deÔ¨Ånite momentum p(t), the momentum of a quantum par-
ticle is another function calculated based on its wave function |œà(x)‚ü©.

10.1
INTRODUCTION
413
Classical models provide accurate predictions for physics prob-
lems involving macroscopic objects, but fail to predict the physics of
atomic-scale phenomena. Much of the 20th century physics research
eÔ¨Äort has been dedicated to the study of quantum concepts like ground
states, measurements, spin angular momentum, polarization, uncer-
tainty, entanglement, and non-locality.
Computer science
Computer scientists understand quantum me-
chanics using principles of information. Quantum principles impose a
fundamental change to the "data types" used to represent information.
Classical information is represented as bits, elements of the Ô¨Ånite Ô¨Åeld
of size two Z2:
‚Ä¢ bit: x = 0
or
x = 1
(two-level system)
In the quantum world, the fundamental unit of information is the
qubit, which is a two-dimensional unit-length vector in a complex
inner product space:
‚Ä¢ qubit: |x‚ü©= Œ±|0‚ü©+ Œ≤|1‚ü©
(unit-length vector in C2)
This change to the underlying information model requires reconsider-
ations of the fundamental information processing tasks like computa-
tion, data compression, encryption, and communication.
Philosophy
Philosophers have also thought long and hard about
the laws of quantum mechanics and what they imply about our un-
derstanding of the ultimate nature of reality. A question of central
interest is how the deterministic laws of physics (clockwork-model of
the universe) must be adapted to the quantum paradigm, in which
experimental outcomes are inherently probabilistic. Another interest-
ing question to consider is whether the quantum state |œà‚ü©of physical
system really exists, or if |œà‚ü©is a representation of our knowledge
about the system.
Many scientists are also interested in the philosophical aspects of
quantum mechanics, but for the most part, professional scientists do
not focus on interpretations. Scientists care only about which models
lead to more accurate predictions. Since diÔ¨Äerent philosophical inter-
pretations of quantum phenomena cannot be tested experimentally,
these questions are considered outside the scope of physics research.
Psychology
There is even a psychology view on quantum mechan-
ics. Some people try to apply quantum principles to the human mind.
Neuroscience and quantum physics are two areas of science that are
not fully understood, so supposing there is a link between the two is

414
QUANTUM MECHANICS
not provably wrong, however current quantum psychology books oÔ¨Äer
nothing more than speculation. What is worse, it's speculation by
people who know less about quantum mechanics than you will know
by the end of this chapter.
Borrowing impressive-sounding words from physics can add an air
of respectability to pop-psychology books, but don't be fooled by such
tactics. We won't discuss this topic further in this book; I mentioned
quantum psychology only to give you a heads-up about this nonsense.
Next time you hear someone dropping the q-word in a context that
has nothing to do with atomic-scale phenomena, call "bullshit" on
them immediately and don't waste your time. They're just trying to
bamboozle you.
Physical models of the world
Before we talk about quantum mechanics, it's important to deÔ¨Åne
precisely the two conceptual "worlds" that we'll discuss:
‚Ä¢ The real world is where physical experiments are performed.
‚Ä¢ The mathematical world is a purely theoretical construct that
aims to model certain aspects of the real world.
If a mathematical model is good, its predictions correspond closely
to the behaviour of systems in the real world. Note there exist good
models and better models, but no physics model can ever be "true"
or "real" since, by it's very nature, it lives in the realm of formulas
and equations and not of atoms and lasers.
One physics model is
considered more "true" than another only if it's better at predicting
the outcomes of experiments. Coming up with physics models is a
lot of fun. Anyone can be a physicist! These are the rules for con-
structing physical models: you're allowed to come up with any crazy
mathematical model for describing nature, and if your model correctly
predicts the outcomes of experiments, physicists will start using it.
We can make a further distinction among mathematical models,
classifying them into two categories depending on the type of math
they use:
‚Ä¢ Classical models describe the world in terms of concepts like
positions and velocities and the way system states evolve from
one instant to the next is governed by deterministic laws.
‚Ä¢ Quantum models describe systems in terms of vectors in com-
plex vector spaces. The way systems evolve over time is gov-
erned by a deterministic laws, but we can only "observe" quan-
tum systems by performing measurements with nondeterminis-
tic outcomes.

10.1
INTRODUCTION
415
Table 10.1 compares the type of mathematical objects used in classical
and quantum models for the real world. In physics, classical models
describe the motion of particles using trajectories ‚Éór(t), whereas quan-
tum models use wave functions |œà(‚Éór, t)‚ü©. In computer science, classical
information is stored in bits i ‚àà{0, 1}, whereas quantum information
is stored in qubits |x‚ü©‚ààC2.
Real world:
‚Ä¢ The motion of a ball thrown in the air
‚Ä¢ The motion of an electron through space
‚Ä¢ The path of light particles moving through optical circuits
‚Ä¢ The electric current Ô¨Çowing though a superconducting loop
Classical models:
‚Ä¢ x(t) ‚àà{R ‚ÜíR}
‚Ä¢ ‚Éór(t) ‚àà{R ‚ÜíR3}
‚Ä¢ i ‚ààZ2 ‚â°{0, 1}
(a bit)
‚Ä¢ j ‚ààZd
Quantum models:
‚Ä¢ |œà(x, t)‚ü©‚àà{R √ó R ‚ÜíC}
‚Ä¢ |œà(‚Éór, t)‚ü©‚àà{R3 √ó R ‚ÜíC}
‚Ä¢ |x‚ü©‚ààC2
(a qubit)
‚Ä¢ |y‚ü©‚ààCd
Table 10.1: The real world and examples of classical and quantum math-
ematical models used to describe real-world phenomena.
Example
Let's analyze the diÔ¨Äerence between classical and quan-
tum models of the real world using an example. Consider a photon (a
particle of light) going through an optical circuit that consists of sev-
eral lenses, mirrors, and other optical instruments. A photon detector
is placed at position xf at the end of the circuit and the objective of
the experiment is to predict if the photon will arrive at the detector
and cause it to "click." The two possible outcomes of the experiment
are click (photon arrives at detector) or noclick (photon doesn't
arrive at detector).1
A classical model of the motion of the photon calculates the pho-
ton's position at all times x(t) and leads to the prediction i = 1
(click) if xf = x(t), for some t. On the other hand, if the detector
does not lie on the photon's trajectory, then the classical model will
predict i = 0 (noclick).
A quantum model would describe the photon's trajectory through
1We're assuming the detector has 100% eÔ¨Éciency (detects every photon that
arrives on it) and a zero noise (no false-positive clicks).

416
QUANTUM MECHANICS
the circuit as a linear combination of two diÔ¨Äerent possible paths:
|œà‚ü©= Œ±|0‚ü©+ Œ≤|1‚ü©
where |Œ±|2 + |Œ≤|2 = 1.
Here |1‚ü©describes paths that arrive at the detector, and |0‚ü©describes
paths that don't.
The coeÔ¨Écients Œ± and Œ≤ describe the relative
"weights" of the diÔ¨Äerent paths. Using the quantum model, we can
obtain a probabilistic prediction of whether the detector will click or
not:
Pr( noclick ) = |Œ±|2
and
Pr(click ) = |Œ≤|2.
Both the classical and the quantum models describe the same real-
world phenomenon, and the validity of both models can be tested by
comparing the predictions of the models with what happens in reality.
Note the assumptions about reality the two models make are very
diÔ¨Äerent. The classical model assumes the photon follows a single path
through the circuit, whereas the quantum model assumes the photon
can take multiple paths through the circuit. Despite the diÔ¨Äerence in
the mathematical "substrate" of the models, and their fundamentally
diÔ¨Äerent view of reality, we can compare the two models' predictions
on the same footing. We cannot say one model is more "real" than the
other. The only thing that is real is the photon in the optical circuit
and it doesn't care whether you use classical or quantum models to
describe its path.
Quantum model peculiarities
We'll now comment on the relative "intuitiveness" of classical and
quantum models and introduce the concept of quantum measurement,
which is of central importance in quantum mechanics.
Classical models have the advantage of being more intuitively un-
derstandable than quantum models. The variables in classical models
often correspond to measurable aspects of real-world systems. We can
identify the position variable in a classical model with the position of
a particle in the real world, as measured using measuring tape. Ve-
locity and momentum are harder to understand intuitively, but we
have some general intuition about motion and collisions from every-
day life. In general, classical models can be understood more easily
because it's easier for us to think about a mechanistic, clockwork-like
universe, in which objects push on each other, with clearly deÔ¨Åned
cause and eÔ¨Äect, and a clock that goes click, click, click.
Quantum models, in contrast, do not enjoy such intuitive interpre-
tations since we cannot come directly into contact with the quantum
states through any of our senses. Because of this indirect connection
between the states of quantum models and their predictions about
the real world, quantum models are often described as mysterious

10.1
INTRODUCTION
417
and counter intuitive. Quantum models are harder to understand in
part because they use complex vector quantities to represent systems,
and complex numbers are more diÔ¨Écult to visualize. For example,
"visualizing" the complex-valued state |œà‚ü©is diÔ¨Écult, since you have
to think about both the real part and the imaginary part of |œà‚ü©. Even
though we can't see what |œà‚ü©looks like, we can describe it using an
equation, and do mathematical calculations with it. In particular,
we can predict the outcomes of measurements performed in the real
world, and compare them with calculations obtained from the quan-
tum state |œà‚ü©.
The process of quantum measurement is how we map the predic-
tions of the quantum model to classical observable quantities. A quan-
tum measurement acts on the particle's wave function |œà‚ü©to produce
a classical outcome. You can think of measurements as asking
the particle questions, and the measurement outcomes as the
answers to these questions.
What is your position?
‚áî
position(|œà‚ü©) = x ‚ààR
What is your momentum?
‚áî
momentum(|œà‚ü©) = p ‚ààR
What is your spin momentum?
‚áî
spin‚Üë‚Üì(|œà‚ü©) = s ‚àà{‚Üë, ‚Üì}
Since measurement outcomes correspond to real-world quantities that
can be measured, we can judge the merits of quantum models the same
way we judge the merits of classical models‚Äîin terms of the quality
of their predictions.
Chapter overview
In the next section we'll describe a table-top experiment involving
lasers and polarization lenses whose outcome is diÔ¨Écult to explain
using classical physics. The remainder of the chapter will introduce
the tools necessary to explain the outcome of the experiment. We'll
start by introducing a special notation for vectors that is used when
describing quantum phenomena (Section 10.3). In Section 10.5 we'll
formally deÔ¨Åne the "rules" of quantum mechanics, also known as the
postulates of quantum mechanics. We'll focus on learning the "rules of
the game" using the simplest possible quantum systems (qubits), and
deÔ¨Åne how quantum systems are prepared, how we manipulate them
using quantum operations, and how we extract information from them
using quantum measurements. This part of the chapter is based on
the notes from the introductory lectures of a graduate-level quantum
information course, so don't think you'll be getting some watered-
down, hand-wavy version of quantum mechanics. You get the real
stuÔ¨Ä, because I know you can handle it. In Section 10.6 we'll use the

418
QUANTUM MECHANICS
rules of quantum mechanics to revisit the polarization lenses experi-
ment showing that a quantum model leads to the correct qualitative
and quantitative prediction for the observed outcome.
We'll close
the chapter with short explanations about diÔ¨Äerent applications of
quantum mechanics with pointers for further exploration about each
topic.
Throughout the chapter we'll focus on matrix quantum mechanics
and use computer science language for describing quantum phenom-
ena. Taking the computer science approach allows us to discuss the
fundamental aspects of quantum theory, without the need to intro-
duce all the physics background required to understands atoms.
Put on you safety goggle, because we're going to the lab!
10.2
Polarizing lenses experiment
We'll now describe a simple table-top experiment that illustrates the
limitations of classical, deterministic reasoning. The outcomes of the
experiment will highlight the need for more careful considerations of
the measurements used in scientiÔ¨Åc experiments.
We'll describe the experiment using words and diagrams, but the
equipment required to perform the experiment is fairly simple. You
could easily reproduce the experiment in your own "lab." I encourage
you to try for yourself. You'll need three polarizing lenses, a laser
pointer, and three binder clips for holding the lenses upright. You
can pick up polarizing lenses on the cheap from a second-hand camera
shop. Any polarizing lens will do.
Background
In photography, polarizing lenses are used to Ô¨Ålter-out certain unde-
sirable light reÔ¨Çections, like reÔ¨Çections that occur from water surfaces
or reÔ¨Çections from glass windows. To better understand the experi-
ment, we need to introduce some basic notions about the physics of
light, speciÔ¨Åcally, the concept of light polarization.
Light consists of photons. Photons are travelling pulses of elec-
tromagnetic energy. Electromagnetic energy can travel through space
in the form of a wave. Polarization refers to the orientation of the
electric Ô¨Åeld ‚ÉóE of a propagating electromagnetic wave.
Light is normally unpolarized, meaning it corresponds to a mix-
ture of photons whose electric and magnetic components have random
orientations. A light beam is polarized if all photon in it have the same
orientation of their electric Ô¨Åeld.

10.2
POLARIZING LENSES EXPERIMENT
419
Figure 10.1: A photon is a pulse of electromagnetic energy. The energy
of a photon travels in the form of a wave that has an electric component ‚ÉóE
and a magnetic component ‚ÉóB. The Ô¨Ågure shows a photon travelling in the
positive x-direction whose electric component is along the z-axis.
Light reÔ¨Çected from Ô¨Çat surfaces like the surface of a lake or a glass
window becomes polarized, which means the electric components of
all the reÔ¨Çected photons become aligned.
Photographers use this fact to selectively Ô¨Ålter out light with a
particular polarization. A polarizing Ô¨Ålter or polarizing lens has a
special coating which conducts electricity in one direction, but not in
the other. You can think of the surface of a polarizing Ô¨Ålter as being
covered by tiny conducing bands that interact with the electric com-
ponent of incoming light particles. Light rays that hit the Ô¨Ålter will
either pass through or be reÔ¨Çected depending on their polarization.
Light particles whose polarization is perpendicular to the conducting
lines will pass through, while light particles with polarization parallel
to the conducting lines are reÔ¨Çected. This is because the surface of
the Ô¨Ålter has diÔ¨Äerent conduction properties in diÔ¨Äerent directions.
Figure 10.2: The horizontal conducting bands of a polarizing Ô¨Ålter inter-
act with the horizontal component of the electric Ô¨Åeld of the incoming pho-
tons and reÔ¨Çect them. Vertically-polarized photons pass straight through
the Ô¨Ålter, because the conducing bands are perpendicular to their electric
Ô¨Åeld. Thus, a vertically polarizing Ô¨Ålter denoted V allows only vertically
polarized light to pass through.
Consider the illustration in Figure 10.3. The eÔ¨Äect of using a vertically
polarizing lens on a beam of light is to only allow vertically polarized
light to pass through.

420
QUANTUM MECHANICS
unpolarized light ‚ÜíV ‚Üívertically polarized light
Figure 10.3: A vertically polarizing lens (V ) allows only vertically polar-
ized light particles to pass through.
In Figure 10.4 we see another aspect of using polarizing lenses. If
the light is already vertically polarized, adding a second vertically
polarizing lens will not aÔ¨Äect the beam. All light that passed through
the Ô¨Årst lens will also pass through the second.
unpolarized light ‚ÜíV ‚Üívert. polarization ‚ÜíV ‚Üívert. polarization.
Figure 10.4: A second vertically polarizing lens has no further eÔ¨Äect since
light is already vertically polarized.
Taking a vertically polarizing lens and rotating it by 90 degrees turns
it into a horizontally polarizing lens. See Figure 10.5.
unpolarized light ‚ÜíH ‚Üíhorizontally polarized light.
Figure 10.5: A horizontally polarizing lens (H) allows only horizontally
polarized light particles to pass through.
Note that horizontally polarizing lenses and vertically polarizing lenses
are complementary. Vertically polarized light will not pass through
a horizontally polarizing lens.
This situation is illustrated in Fig-
ure 10.6.

10.2
POLARIZING LENSES EXPERIMENT
421
unpolarized light ‚ÜíV
‚Üívert. polarization ‚ÜíH ‚Üí‚àÖ.
Figure 10.6:
Placing a horizontally polarizing lens after the vertically
polarizing lens has the eÔ¨Äect of Ô¨Åltering all light. Zero photons make it
through both Ô¨Ålters, which we indicate with the empty set symbol ‚àÖ.
The above examples are meant to get you familiar with the properties
of polarizing lenses, in case you don't have some to play with. If you
have polarizing lenses at your disposal, you could try shining the laser
pointer through them and observe when light goes through and when
it gets Ô¨Åltered out. Use the paper clips to position the lenses on a Ô¨Çat
surface and try to reproduce the setup in Figure 10.4. Don't worry
about Ô¨Ånding the exact orientation for "vertical."
Any orientation
will do so long as the Ô¨Årst and the second polarizing lens have the
same polarization. Next, you can rotate the second lens by 90‚ó¶to
obtain the setup shown in Figure 10.6, where the second lens has the
complimentary orientation and thus rejects all light.
Example
Polarized sunglasses are another example where light po-
larization properties can be put to good use. When a beam of light
bounces from the surface of a lake, it becomes horizontally polarized.
This polarization eÔ¨Äect is due to the interaction of light's electric
Ô¨Åeld at the surface of the water. A person wearing vertically polar-
izing lenses (polarized sunglasses) will not be able to see the sun's
reÔ¨Çection from the water surface because the V -polarizing lenses will
Ô¨Ålter out the horizontally polarized reÔ¨Çection coming from the surface
of the lake. This is a useful eÔ¨Äect for people who are often out on the
water, as it reduces the blinding eÔ¨Äect of the sun's reÔ¨Çection.
Classical physics paradigm
Before we describe the outcome of the polarizing lenses experiment,
let's take a moment to describe the assumptions about the world that
19th century physicists held. Understanding this classical world view
will explain why the outcomes of the experiment you're about to see
are so surprising.
The classical laws of physics are deterministic, meaning they do
not allow randomness. The outcomes of experiments depend on deÔ¨Å-
nite variables like properties of the particles in the experiment. It is

422
QUANTUM MECHANICS
assumed that you can predict the outcome of any experiment given
the full knowledge of the properties of particles, and if some outcome
cannot be predicted, it's because you didn't know the value of some
property of the particles. Everything happens for a reason, in other
words. Another key assumption that a classical physicist would make
is that the photon's properties are immutable, meaning we cannot
change them.
Classical physicists assume their experiments corre-
spond to passive observations that don't aÔ¨Äect the system's proper-
ties.
The outcome of a polarizing lens experiment is whether photons
pass through a lens or get Ô¨Åltered. For a 19th century physicists, the
outcomes of such experiments depend on the polarization property of
photons. You can think of each photon as carrying a tag "H" or "V"
that describes its polarization. In the setup shown in Figure 10.3, each
photon that makes it through the lens must have tag="V", because
we know by deÔ¨Ånition that a V -polarizing lens only allows vertically
polarized photons to pass through. Readers familiar with SQL syn-
tax will recognize the action of the vertically polarizing lens as the
following query:
SELECT photon FROM photons WHERE tag="V";
In other words, from all the photons incident on the lens, only the
vertically polarized ones are allowed to pass through. Similarly, for
the H-polarizing lens shown in Figure 10.5, the Ô¨Åltering process can
be understood as the query:
SELECT photon FROM photons WHERE tag="H";
In both cases, classical physicists would assume that whether a photon
passes through a lens or not is predetermined, and it depends only on
the photon's tag.
For the purpose of the discussion here, we'll restrict our attention
to photons that either have horizontal (tag="H") or vertical (tag="V")
polarization. There are other possible polarization directions, but we
want to focus on the tags "H" and "V" because we know they are
mutually exclusive.
If a photon is horizontally polarized, then we
know it will be rejected by a vertically polarizing lens. Another thing
we can assert is that all photons that passed through an H-polarizing
lens are deÔ¨Ånitely not vertically polarized, otherwise they would have
been reÔ¨Çected and not made it through.
Polarizing lenses experiment
The physics experiment we'll describe consists of sending photons
through diÔ¨Äerent combinations of polarizing lenses and observing how

10.2
POLARIZING LENSES EXPERIMENT
423
many of them make it through the optical circuit. We'll describe the
number of photons that reach any point in the circuit qualitatively,
by referring to the optical power reaching that point, denoted P. We
keep track of the power that remains after passing through the Ô¨Ålters,
and we choose to call P = 1 (full brightness) the intensity of the beam
after the Ô¨Årst polarizing lens. You can think of optical power as telling
you how bright of a spot is visible if you insert a piece of paper at
that location. When power P = 1, the spot is fully bright. If P = 0.5
the spot is half as bright as when P = 1. The case P = 0 corresponds
to zero brightness and occurs when no photons are hitting the piece
of paper at all.
The starting point for the polarization experiment is an H-polarizing
lens followed by a V -polarizing lens, as shown in Figure 10.7.
light ‚àí‚ÜíH ‚àí‚ÜíP = 1 ‚àí‚ÜíV
‚àí‚ÜíP = 0
Figure 10.7: The initial setup for the polarizing lenses experiment consists
of an H-polarizing lens followed by a V -polarizing lens. Only photons that
have tag="H" can pass through the Ô¨Årst lens, so we know no photons with
tag="V" made it through the Ô¨Årst lens. No photons can make it through
both lenses, since the V -polarizing lens accepts only photons with tag="V".
We know the photons that make it through the Ô¨Årst Ô¨Ålter are hori-
zontally polarized, so it is not surprising to see that when this light
hits the second lens, none of the photons make it through, since a
V -polarizing lens will rejects H-polarized photons.
Adding a third lens
We now introduce a third lens in between the
Ô¨Årst two, choosing the orientation of the middle lens to be diÔ¨Äerent
from the other two, for example in the diagonal direction. The result
is shown in Figure 10.8. Suddenly light appears at the end of the
circuit! Wait, what just happened here? You tell me if this is crazy
or not. Intuitively, adding more Ô¨Åltering can only reduce the amount
of light that makes it through, yet the amount of light that makes
it through the circuit increases when we add the middle Ô¨Ålter. How
could adding more Ô¨Åltering increase light intensity? What is going
on?

424
QUANTUM MECHANICS
light ‚ÜíH ‚ÜíP = 1 ‚ÜíD ‚ÜíP = 0.5 ‚ÜíV
‚ÜíP = 0.25
Figure 10.8: Adding an additional polarizing Ô¨Ålter in the middle of the
setup causes light to appear at the end of the optical circuit.
We pick the middle lens to be a diagonally polarizing lens D.
A
diagonally polarizing lens is obtained by rotating any polarizing lens
by 45‚ó¶. The exact choice for the middle lens is not crucial for the
experiment to work; so long as it's diÔ¨Äerent from the H- and V -
polarizing lenses that surround it.
Classical analysis
The experimental observations illustrated in Figure 10.8 are diÔ¨Écult
to explain using the classical way of thinking of particle properties as
immutable tags, and experiments as passive observations.
Let's look at the evidence showing a particle's state can change
during measurements. We'll trace the path of the photons through
the optical circuit in Figure 10.8, keeping track of what we know
about them at each stage. All photons that passed through the Ô¨Årst
H-polarizing lens are known to be horizontally polarized (tag="H").
We're sure no V -polarized photons made it through, because an H-
polarizing lens is guaranteed to reject all V -polarized photons coming
to it. Yet, a little bit later (after passing through the D lens) the
same photons, when asked the question "are you vertical?"
at the
third lens, answer the question aÔ¨Érmatively. What kind of tagging is
this? Something must be wrong with the tagging system.
It seems the photons' tag states are aÔ¨Äected by the measurements
performed on them. This fact is diÔ¨Écult to explain for classical physi-
cists since they assume measurements correspond to passive observa-
tions of the systems. In the classical paradigm, measuring a photon's
D-polarization using the middle lens should not aÔ¨Äect its "H" and "V"
tags.
In Section 10.5 we'll revisit this experiment after having learning the
postulates of quantum mechanics. We'll see that vector-like state for
describing the photon's polarization explain well the outcome of the
polarizing lenses experiment and is even predicts the Ô¨Ånal light inten-
sity of P = 0.25, which is observed. Before we discuss the postulates

10.3
DIRAC NOTATION FOR VECTORS
425
of quantum mechanics (Section 10.5), we'll need to introduce some
new notation for describing quantum states.
10.3
Dirac notation for vectors
The Dirac notation for vectors |v‚ü©is an alternative to the usual no-
tations for vectors like ‚Éóv and v. This section is a quick primer on
Dirac notation, which is a precise and concise language for talking
about vectors and matrices. This new notation will look a little weird
initially, but once you get the hang of it, I guarantee you'll like it.
Learning Dirac notation serves as a very good review, since we'll re-
visit essential linear algebra concepts like bases, vectors, inner prod-
ucts, and matrices. Understanding Dirac notation is essential if you're
interested in learning quantum physics.
We'll now discuss several vector topics that you're familiar with,
comparing standard vector notation ‚Éóv with the equivalent Dirac no-
tation |v‚ü©.
The standard basis
Consider a d-dimensional complex vector space Cd. We refer to com-
plex vector spaces as Hilbert spaces, in honour of David Hilbert, who
did a lot of good things in math and physics.
Of central importance for understanding any vector space is to
construct a basis for the space. A natural choice for a basis is the
standard basis, which we'll denote {|0‚ü©, |1‚ü©, |2‚ü©, . . . , |d‚àí1‚ü©}. The basis
elements are deÔ¨Åned as follows:
|0‚ü©‚â°
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
0
...
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
|1‚ü©‚â°
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
0
1
...
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
. . . ,
|d ‚àí1‚ü©‚â°
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
0
...
0
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
Note the indices are shifted by one so the Ô¨Årst basis element has index
0, not index 1. This zero-based indexing is chosen to make certain
links between quantum theory and computer science more apparent.
The beneÔ¨Åts of Dirac notation is that it doesn't require writing
subscripts. To refer to a vector associated with properties a, b, and c,
we can write |a, b, c‚ü©, instead of the more convoluted expression ‚Éóva,b,c.
We'll now restrict attention to the two-dimensional complex vector
space C2. The results and deÔ¨Ånitions presented below apply equally
well to vectors in Cd as to vectors in C2.

426
QUANTUM MECHANICS
Vectors
In Dirac notation, a vector in C2 is denoted as a ket:
|v‚ü©= Œ±|0‚ü©+ Œ≤|1‚ü©
‚áî
 Œ±
Œ≤

= Œ±
 1
0

+ Œ≤
 0
1

.
where Œ± ‚ààC and Œ≤ ‚ààC are the coeÔ¨Écients of |v‚ü©and {|0‚ü©, |1‚ü©} is the
standard basis for C2:
|0‚ü©‚â°

1
0

,
|1‚ü©‚â°

0
1

.
Why do we call the angle-bracket thing a "ket" you ask? Let me tell
you about the bra, and then it will start to make sense.
The Hermitian transpose of the ket-vector |v‚ü©= Œ±|0‚ü©+ Œ≤|1‚ü©is the
bra-vector ‚ü®v|:
‚ü®v| = Œ±‚ü®0| + Œ≤‚ü®1|
‚áî
[Œ±, Œ≤] = Œ±[1, 0] + Œ≤[0, 1].
Recall that the Hermitian transpose (complex transpose) is the combi-
nation of the regular transpose (‚Éóv ‚Üí‚ÉóvT) and the complex conjugation
of each entry in the vector (vi ‚Üívi), and is denoted as the dagger
operator (‚Ä†). Observe how much simpler the bra notation for the Her-
mitian transpose of a vector is compared to the other notations we've
seen thus far:
‚ü®v|
‚áî
‚Éóv‚Ä† ‚â°(‚ÉóvT) ‚â°(‚Éóv)T.
But why call ‚ü®v| a "bra"? Is this some sort of sexist joke?
What happens when you put a bra ‚ü®v| next to a ket |w‚ü©? A braket
‚ü®v|w‚ü©, and a braket that looks very similar to the brackets used to
denote the inner product between two vectors. Observe how easy it is
to calculate the inner product between the vectors |v‚ü©= v0|0‚ü©+ v1|1‚ü©
and |w‚ü©= w0|0‚ü©+ w1|1‚ü©in Dirac notation:
‚ü®v|w‚ü©= v0w0 + v1w1
‚áî
‚Éóv‚Ä† ‚Éów ‚â°‚Éóv ¬∑ ‚Éów.
Complex conjugation was already applied to the coeÔ¨Écients of |v‚ü©
when transforming it into a bra-vector ‚ü®v|, thus we can simply "put
together" the bra and the ket to compute the inner product. The
bra notation ‚ü®v| contains the Hermitian transpose so it removes the
need for the dagger symbol. This trivial simpliÔ¨Åcation of the nota-
tion for inner product turns out to be very useful. Inner products are
the workhorse for calculating vector coeÔ¨Écients, Ô¨Ånding projections,
performing change-of-basis transformations, and in countless applica-
tions. The simpler notation for inner products will enable us to dig
deeper into each of these aspects of linear algebra, without getting
overwhelmed by notational complexity.

10.3
DIRAC NOTATION FOR VECTORS
427
The inner product of |v‚ü©= Œ±|0‚ü©+ Œ≤|1‚ü©with itself is
‚ü®v|v‚ü©=
 Œ±‚ü®0| + Œ≤‚ü®1|, Œ±|0‚ü©+ Œ≤|1‚ü©

= Œ±Œ±‚ü®0|0‚ü©+ Œ±Œ≤ ‚ü®0|1‚ü©
|{z}
0
+Œ≤Œ± ‚ü®1|0‚ü©
|{z}
0
+Œ≤Œ≤‚ü®1|1‚ü©
= |Œ±|2 + |Œ≤|2.
The ability to express vectors, inner products, and vector coeÔ¨Écients
in a concise and consistent manner is why everyone likes Dirac no-
tation. Dear readers, are you still with me? You can't tell me what
we've seen so far is too complicated, right? Let's continue then. The
main virtue of Dirac's bra-ket notation is that it is suÔ¨Éciently simple
as to be used in equations without the need to deÔ¨Åne new variables
vi for vector coeÔ¨Écients. We'll look at this more closely in the next
section.
Vector coeÔ¨Écients
The coeÔ¨Écients vi of a vector ‚Éóv with respect to an orthonormal ba-
sis {ÀÜei} are computed using the inner product vi ‚â°ÀÜe‚Ä†
i‚Éóv. In Dirac
notation, the coeÔ¨Écients of |v‚ü©with respect to the standard basis
{|0‚ü©, |1‚ü©} can be written as ‚ü®i|v‚ü©. We can write any vector |v‚ü©as a
linear combination of kets, with bra-kets as coeÔ¨Écients:
|v‚ü©= ‚ü®0|v‚ü©
|{z}
v0
|0‚ü©+ ‚ü®1|v‚ü©
|{z}
v1
|1‚ü©.
The expression ‚ü®i|v‚ü©is simple enough and it explicitly deÔ¨Ånes the ith
coeÔ¨Écient of |v‚ü©. We therefore don't need to deÔ¨Åne the variable vi.
Another basis for the vector space C2 is the Hadamard basis which
corresponds to the standard basis rotated by 45‚ó¶in the counter-
clockwise direction:
|+‚ü©‚â°
1
‚àö
2|0‚ü©+
1
‚àö
2|1‚ü©,
|‚àí‚ü©‚â°
1
‚àö
2|0‚ü©‚àí
1
‚àö
2|1‚ü©.
The Hadamard basis, henceforth denoted Bh ‚â°{|+‚ü©, |‚àí‚ü©}, is an or-
thonormal basis:
‚ü®+|+‚ü©= 1,
‚ü®+|‚àí‚ü©= 0,
‚ü®‚àí|+‚ü©= 0,
‚ü®‚àí|‚àí‚ü©= 1.
Since the Hadamard basis Bh is an orthonormal basis, we can express
any vector |v‚ü©‚ààC2 as a linear combination of kets:
|v‚ü©= ‚ü®+|v‚ü©
| {z }
v+
|+‚ü©+ ‚ü®‚àí|v‚ü©
| {z }
v‚àí
|‚àí‚ü©.

428
QUANTUM MECHANICS
Note the coeÔ¨Écients of the linear combination are computed using the
inner product with the corresponding basis vector ‚ü®+|v‚ü©and ‚ü®‚àí|v‚ü©.
The bra-ket notation allows us to refer to the coeÔ¨Écients of |v‚ü©with
respect to {|+‚ü©, |‚àí‚ü©} without the need to deÔ¨Åne variables v+ and v‚àí.
Vector coeÔ¨Écients with respect to diÔ¨Äerent bases are often used in
calculations. Using the usual vector notation, we must specify which
basis is being used as a subscript. For example, the same vector ‚Éóv
can be expressed as a coeÔ¨Écient vector ‚Éóv = (v0, v1)T
Bs with respect
to the standard basis Bs, or as a coeÔ¨Écient vector ‚Éóv = (v+, v‚àí)T
Bh
with respect to the Hadamard basis.
In the bra-ket notation, the
coeÔ¨Écients with respect to Bs are ‚ü®0|v‚ü©and ‚ü®1|v‚ü©, and the coeÔ¨Écients
with respect to Bh are ‚ü®+|v‚ü©and ‚ü®‚àí|v‚ü©, making the choice of basis
evident.
Change of basis
Consider the task of Ô¨Ånding the change-of-basis matrix
[1]
Bh
Bs that
converts vectors from the standard basis Bs = {(1, 0), (0, 1)}, to the
Hadamard basis Bh = {( 1
‚àö
2,
1
‚àö
2), ( 1
‚àö
2, ‚àí1
‚àö
2)}.
Using the standard approach for Ô¨Ånding change-of-basis matrices
discussed in Section 5.3 (page 198), we know the columns of
[1]
Bh
Bs
contains the coeÔ¨Écients of (1, 0) and (0, 1) as expressed with respect
the basis Bh:
[1]
Bh
Bs =
"
‚ü®+|0‚ü©
‚ü®+|1‚ü©
‚ü®‚àí|0‚ü©
‚ü®‚àí|1‚ü©
#
Bh
Bs
=
" 1
‚àö
2
1
‚àö
2
1
‚àö
2
‚àí1
‚àö
2
#
Bh
Bs
.
We can now compute the coeÔ¨Écients of any vector ‚Éóv ‚â°(v0, v1)T
Bs
with respect to the Hadamard basis, by multiplying (v0, v1)T
Bs by the
change-of-basis matrix:
 v+
v‚àí

Bh
=
[1]
Bh
Bs
 v0
v1

Bs
=
" 1
‚àö
2
1
‚àö
2
1
‚àö
2
‚àí1
‚àö
2
#
Bh
Bs
 v0
v1

Bs
=
"
1
‚àö
2
 v0 + v1

1
‚àö
2
 v0 ‚àív1

#
Bh
.
This is the usual approach for computing the coeÔ¨Écients of a vector
in one basis in terms of the coeÔ¨Écients of another basis using matrix
notation.
Consider now the same change-of-basis operation, but with calcu-
lations carried out in Dirac notation. Given the vector ‚Éóv = (v0, v1)Bs =

10.3
DIRAC NOTATION FOR VECTORS
429
v0|0‚ü©+v1|1‚ü©expressed as coeÔ¨Écients with respect to the standard ba-
sis Bs, we want to Ô¨Ånd ‚Éóv = (v+, v‚àí)Bh = v+|+‚ü©+ v‚àí|‚àí‚ü©. Starting
from the deÔ¨Ånition of v+ and v‚àí, we obtain
‚Éóv ‚â°(v+, v‚àí)T
Bh
‚â°‚ü®+|v‚ü©|+‚ü©+ ‚ü®‚àí|v‚ü©|‚àí‚ü©
= ‚ü®+|
 v0|0‚ü©+ v1|1‚ü©

|+‚ü©+ ‚ü®‚àí|
 v0|0‚ü©+ v1|1‚ü©

|‚àí‚ü©
=
 v0‚ü®+|0‚ü©+ v1‚ü®+|1‚ü©

|+‚ü©+
 v0‚ü®‚àí|0‚ü©+ v1‚ü®‚àí|1‚ü©

|‚àí‚ü©
=
1
‚àö
2
 v0 + v1

|
{z
}
v+
|+‚ü©+
1
‚àö
2
 v0 ‚àív1

|
{z
}
v‚àí
|‚àí‚ü©.
Working from the deÔ¨Ånitions of vectors and their components and
using only basic algebra rules we can perform the change-of-basis
operation, without explicitly constructing the change-of-basis matrix.
Outer products
Recall the outer product operation for vectors that we introduced in
Section 6.2 (see page 237). The expression |0‚ü©‚ü®0| is equivalent to the
projection onto the subspace spanned by the vector |0‚ü©:
|0‚ü©‚ü®0|
‚áî
1
0
1 0
=
1
0
0
0

.
We can verify this by considering the product of |0‚ü©‚ü®0| and an arbitrary
vector |v‚ü©= Œ±|0‚ü©+ Œ≤|1‚ü©:
|0‚ü©‚ü®0| (Œ±|0‚ü©+ Œ≤|1‚ü©) = Œ±|0‚ü©‚ü®0|0‚ü©
|{z}
1
+Œ≤|0‚ü©‚ü®0|1‚ü©
|{z}
0
= Œ±|0‚ü©.
The ability to easily express outer products is another win for Dirac
notation. For example, the projection onto the |+‚ü©-subspace is |+‚ü©‚ü®+|.
Matrices
Now get ready for some crazy stuÔ¨Ä. It turns out outer-product ex-
pressions are useful not only for projections, but can in fact represent
any matrix. Consider the linear operator A : C2 ‚ÜíC2 and its matrix
representation with respect to the standard basis:
[A]
Bs
Bs ‚â°
a00
a01
a10
a11

Bs
Bs
.
Instead of positioning the coeÔ¨Écients in an array, we can represent A
as a linear combination of outer-products:
A ‚â°a00|0‚ü©‚ü®0| + a01|0‚ü©‚ü®1| + a10|1‚ü©‚ü®0| + a11|1‚ü©‚ü®1|.

430
QUANTUM MECHANICS
The coeÔ¨Écient a10 describes the multiplier A applies between the
coeÔ¨Écient of |0‚ü©in its input and the coeÔ¨Écient of |1‚ü©in its output.
The expression a10|1‚ü©‚ü®0| is a concise description of the same story. The
‚ü®0| in this expression will "select" only the |0‚ü©coeÔ¨Écient of the input,
and the |1‚ü©indicates this term contributes to the |1‚ü©component of
the output.
The coeÔ¨Écients of the matrix representation
[A]
Bs
Bs depend on
the choice of bases for the input and output spaces. The value of the
coeÔ¨Écient aij in the matrix representation is computed by "probing"
the matrix with the jth basis element of the input basis, and observing
the value of the ith element in the resulting output. We can express
the entire "probing procedure" easily in Dirac notation: aij ‚â°‚ü®i|A|j‚ü©.
Thus, we can write the matrix coeÔ¨Écients as follows:

a00
a01
a10
a11

Bs
Bs
=

‚ü®0|A|0‚ü©
‚ü®0|A|1‚ü©
‚ü®1|A|0‚ü©
‚ü®1|A|1‚ü©

Bs
Bs
In fact, we don't need matrix notation or the coeÔ¨Écients aij at all.
Instead, we can express A as a linear combination of outer products,
with appropriately chosen coeÔ¨Écients:
A
‚â°‚ü®0|A|0‚ü©|0‚ü©‚ü®0| + ‚ü®0|A|1‚ü©|0‚ü©‚ü®1| + ‚ü®1|A|0‚ü©|1‚ü©‚ü®0| + ‚ü®1|A|1‚ü©|1‚ü©‚ü®1|.
Let's verify the formula for the a10 = ‚ü®1|A|0‚ü©-coeÔ¨Écient of A, to see
how this linear combination of outer products thing works. We start
from the deÔ¨Ånition A ‚â°a00|0‚ü©‚ü®0|+a01|0‚ü©‚ü®1|+a10|1‚ü©‚ü®0|+a11|1‚ü©‚ü®1|, and
multiply A by |0‚ü©from the right, and by ‚ü®1| from the left:
‚ü®1|A|0‚ü©= ‚ü®1|

a00|0‚ü©‚ü®0| + a01|0‚ü©‚ü®1| + a10|1‚ü©‚ü®0| + a11|1‚ü©‚ü®1|

|0‚ü©
= ‚ü®1|

a00|0‚ü©‚ü®0|0‚ü©
|{z}
1
+a01|0‚ü©‚ü®1|0‚ü©
|{z}
0
+a10|1‚ü©‚ü®0|0‚ü©
|{z}
1
+a11|1‚ü©‚ü®1|0‚ü©
|{z}
0

= ‚ü®1|

a00|0‚ü©+ a10|1‚ü©

= a00 ‚ü®1|0‚ü©
|{z}
0
+a10 ‚ü®1|1‚ü©
|{z}
1
= a10.
So indeed ‚ü®1|A|0‚ü©is the same as a10. In fact, we'll rarely use coef-
Ô¨Åcient notation a10, since ‚ü®1|A|0‚ü©is just as easy to write, and much
more intuitive: the a10-coeÔ¨Écient of A is what you obtain when you
"sandwich" the matrix A between the vectors ‚ü®1| on the left and |0‚ü©
on the right.
In Dirac notation, the basis appears explicitly in expressions for
coeÔ¨Écients of a matrix. We can deÔ¨Åne the coeÔ¨Écients of A in any

10.4
QUANTUM INFORMATION PROCESSING
431
other basis very easily and precisely. The representation of A with
respect to the Hadamard basis Bh = {|+‚ü©, |‚àí‚ü©} is
[A]
Bh
Bh ‚â°
‚ü®+|A|+‚ü©
‚ü®+|A|‚àí‚ü©
‚ü®‚àí|A|+‚ü©
‚ü®‚àí|A|‚àí‚ü©

Bh
Bh
,
or equivalently
A = ‚ü®+|A|+‚ü©|+‚ü©‚ü®+| + ‚ü®+|A|‚àí‚ü©|+‚ü©‚ü®‚àí| + ‚ü®‚àí|A|+‚ü©|‚àí‚ü©‚ü®+| + ‚ü®‚àí|A|‚àí‚ü©|‚àí‚ü©‚ü®‚àí|.
The coeÔ¨Écient ‚ü®+|A|+‚ü©is the a++-coeÔ¨Écient of the matrix represen-
tation of A with respect to the Hadamard basis Bh.
Summary
Dirac notation is a convenient way to represent linear al-
gebra concepts: vectors |v‚ü©and their Hermitian conjugates ‚ü®v|, vector
coeÔ¨Écients ‚ü®i|v‚ü©, inner products ‚ü®v|w‚ü©, outer products |v‚ü©‚ü®w|, and ma-
trix coeÔ¨Écients ‚ü®i|A|j‚ü©. Because of this expressiveness, Dirac notation
is widely used in modern chemistry, physics, and computer science,
when discussing quantum mechanical topics. In particular, Dirac no-
tation is a core component in the study of quantum physics. In fact,
we could say that if you understand Dirac notation, you understand
half of quantum mechanics already.
Exercises
E10.1 Consider the ket vector |u‚ü©= Œ±|0‚ü©+Œ≤|3‚ü©and the the bra vector
‚ü®v|, where |v‚ü©= a|1‚ü©+ b|2‚ü©. Express |u‚ü©and ‚ü®v| as four dimensional
vector of coeÔ¨Écients. Indicate whether your answers correspond to
row vectors or column vectors.
E10.2 Express the vector ‚Éów = (1, i, ‚àí1)T
Bs ‚ààC3 as a ket |w‚ü©and as
a bra ‚ü®w|. Compute its length ‚à•‚Éów‚à•=
p
‚ü®w|w‚ü©.
E10.3 Find the determinant of the following matrix:
A = 1|0‚ü©‚ü®0| + 2|0‚ü©‚ü®1| + 3|1‚ü©‚ü®0| + 4|1‚ü©‚ü®1|.
10.4
Quantum information processing
Digital technology is sought after because of the computational, stor-
age, and communication advantages that come with manipulating dig-
ital information instead of continuous-time signals. Similarly, quan-
tum technology is interesting because of the computational and com-
munication applications it enables. The purpose of this section is to
equip you with a mental model for thinking about quantum informa-
tion processing tasks, based on an analogy with digital information
processing pipelines, which may be more familiar to you.

432
QUANTUM MECHANICS
The use of quantum technology for information processing tasks is
no more mysterious than the use of digital technology for information
processing tasks. Playing a digital song recording on your mp3 player
involves a number of processing, conversion, and signal ampliÔ¨Åcation
steps. Similarly, using a quantum computer involves several conver-
sion, processing, and measurement steps. In both cases you input
some data into a machine, wait for the machine to process the data,
and then read out the answer. The use of digital and quantum tech-
nology can both be described operationally as black box processes,
whose internals are not directly accessible to us. In both cases the
intermediate representation of the data is in a format that is unin-
telligible: we can't "see" quantum states, but we can't "see" digital
information either. Think about it, is "seeing" the raw ones and ze-
ros of an mp3 Ô¨Åle really all that helpful? Can you tell which artist
plays the song 010100101010101000111 . . .? To understand informa-
tion processing in the digital and quantum worlds, we must study
the adaptors for converting between the internal representations and
world we can see.
In order to highlight the parallel structure between the digital in-
formation processing and quantum information processing, we'll now
look at an example of a digital information processing task.
Digital signal processing
A sound card is a computer component that converts between ana-
log signals that we can hear and digital signals that computers un-
derstand.
Sound is "digitized" using an analog-to-digital converter
(ADC). For music playback we use a digital-to-analog converter (DAC),
which transforms digital sounds into analog sound vibrations to be re-
produced by the speakers. The ADC corresponds to the microphone
jack on the sound card; the DAC output of the sound card goes to
the line-out and headphone jacks.
sound
ADC
.wav
‚ààZn
2
.mp3
‚ààZk
2
digital processing
DAC
sound ‚Ä≤
Figure 10.9: A digital information processing pipeline for sound record-
ing and playback.
Sound vibrations are captured by a microphone and
converted to digital form using an analog-to-digital converter (ADC). Next
the digital wav Ô¨Åle is converted to the more compact mp3 format using dig-
ital processing. In the last step, sound is converted back into analog sound
vibrations by a digital-to-analog converter (DAC).

10.4
QUANTUM INFORMATION PROCESSING
433
Figure 10.9 illustrates a digital information processing pipeline for
sound. We use an analog-to-digital converter (ADC) to transform the
analog sound into digital form, we then process it in the digital world,
and Ô¨Ånally use a digital-to-analog converter (DAC) to transform from
the digital world back to the analog world. If we're successful in the
music encoding, processing, and playback steps, the Ô¨Ånal output will
sound like the original sound recorded. The grey-shaded region in the
Ô¨Ågure corresponds to the digital world. We've chosen to show only
one, simple information processing task: the compression of a large
wav Ô¨Åle (n = 50[MB]) into a smaller mp3 Ô¨Åle (k = 5[MB]), which we
previously discussed in Section 8.11 (page 385).
The example of mp3 compression is just one many uses of digital
processing that are possible once we convert information into digi-
tal form. Digital information is very versatile, you can compress it,
encrypt it, encode it, transcode it, store it, and‚Äîmost important of
all‚Äîtransmit it over the Internet.
Quantum information processing
Quantum processing pipelines are analogous to digital information
processing pipelines. The process of state preparation corresponds to
the analog-to-digital conversion step. Quantum measurement corre-
sponds to the digital-to-analog conversion step. We use state prepara-
tion to put information into a quantum computer, and measurement
to read information out.
Figure 10.10 shows an example of a quantum information pro-
cessing pipeline. A classical bitstring x ‚ààZk
2 is supplied, and after
state preparation, quantum processing, and measurement steps, the
classical bitstring y ‚ààZ‚Ñì
2 is produced. The crucial diÔ¨Äerence with the
classical information processing, is that quantum information process-
ing is deÔ¨Åned as information processing with quantum states. Thus,
quantum computation is more general than a classical computation.
Instead of working with bits and using functions f : Zk
2 ‚ÜíZ‚Ñì
2, we're
working with qubits and use quantum operations T : Cm ‚ÜíCn to
perform computations.
The other big diÔ¨Äerence between classical and quantum computa-
tion, is that you can read out the output state |y‚ü©only once. You can
think of quantum measurement as asking a question about the
state |y‚ü©. You're free to choose to perform any measurement, but you
can ask only one question, since quantum measurement disrupt
the state of a system. Also the answer you obtain to your question
depends probabilistically on the state |y‚ü©.
In the next section, we'll discuss the components of the quantum in-
formation processing pipeline in more details.
We'll introduce the

434
QUANTUM MECHANICS
x
‚ààZk
2
State
preparation
|x‚ü©
‚ààCm
|y‚ü©
‚ààCn
quantum processing
Measurement
y
‚ààZ‚Ñì
2
Figure 10.10: A quantum information processing pipeline. A classical
bitstring x of length k is used as instructions for preparing an m-dimensional
quantum state |x‚ü©. Next, quantum operations are performed on the state
|x‚ü©to convert it to the output state |y‚ü©. Finally, the state |y‚ü©is measured
in order to obtain the classical bitstring y as output.
four postulates of quantum mechanics, which specify how quantum
systems are represented and what we can do with them. The pos-
tulates of quantum mechanics roughly correspond to the conversion
steps illustrated in Figure 10.10. One postulate deÔ¨Ånes how quantum
state are prepared, another postulate describe the types operations
we can perform on quantum states, and a third postulated formally
deÔ¨Ånes the process of quantum measurement.
The next section is
the "explanation of quantum mechanics in the space on the back of
an envelope," alluded to in the introduction. We've set the scene,
introduced Dirac notation, so we can Ô¨Ånally go into the good stuÔ¨Ä.
10.5
Postulates of quantum mechanics
The postulates of quantum mechanics dictate the rules for working
with the "quantum world." The four postulates of quantum mechanics
deÔ¨Åne
‚Ä¢ What quantum states are
‚Ä¢ Which quantum operations are allowed on quantum states
‚Ä¢ How to extract information from quantum systems by carrying
out measurements on them
‚Ä¢ How to represent composite quantum systems
The postulates of quantum mechanics specify the structure that all
quantum theories must have, and are common to all Ô¨Åelds that use
quantum mechanics: physics, chemistry, engineering, and quantum
information. Note the postulates are not provable or derivable from a
more basic theory: scientists simply take the postulates as facts and
make sure their theories embody these principles.

10.5
POSTULATES OF QUANTUM MECHANICS
435
Quantum states
Quantum states are modelled as special type of vectors. The state
of a d-dimensional quantum system is a unit-length vector |œà‚ü©, in a
d-dimensional complex inner-product vector space Cd, which we call
Hilbert space.
Postulate 1. To every isolated quantum system is associated a com-
plex inner product space (Hilbert space) called the state space. A
state is described by a unit-length vector in state space.
The following additional comments apply to states that describe quan-
tum systems:
‚Ä¢ The requirement that state vectors must have unit length will
become important when we discuss the probabilistic nature of
quantum measurements.
‚Ä¢ The global phase of a quantum state vector doesn't matter.
Thus |œà‚ü©‚â°eiŒ∏|œà‚ü©, ‚àÄŒ∏ ‚ààR. This means the vectors |œà‚ü©, ‚àí|œà‚ü©,
and i|œà‚ü©all represent the same quantum state.
‚Ä¢ Each physical system lives in its own Hilbert space, usually
denoted with the same label as the system, |œà‚ü©1 ‚ààV1 and
|œà‚ü©2 ‚ààV2.
In general, quantum states can be vectors in d-dimensional or some-
times even inÔ¨Ånitely-dimensional Hilbert spaces. We'll focus on two-
dimensional quantum systems.
The qubit
In analogy with the classical bits ‚àà{0, 1}, we call two-
dimensional quantum systems qubits ‚ààC2, which is short for quantum
bit. Many physical systems like the polarization of a photons and the
spin of electrons can all be represented as qubits. A qubit is a unit-
length vector in a two-dimensional Hilbert space C2:
|œà‚ü©= Œ±|0‚ü©+ Œ≤|1‚ü©,
where
Œ± ‚ààR,
(global phase doesn't matter)
Œ≤ ‚ààC,
|Œ±|2 + |Œ≤|2 = 1.
Recall |0‚ü©and |1‚ü©are the elements of standard basis for C2:
|0‚ü©‚â°
 1
0

,
|1‚ü©‚â°
 0
1

.

436
QUANTUM MECHANICS
The restriction to Œ± being a real number follows from the fact that
global phase of a quantum state can be ignored. The condition that
a quantum state must have unit length is equivalent to the constraint
|Œ±|2 + |Œ≤|2 = 1.
Figure 10.11: A photon encounters a half-silvered mirror. The photon
can take one of the two possible paths, so we describe it as the superposition
|Œ≥‚ü©=
1
‚àö
2|0‚ü©+
1
‚àö
2|1‚ü©, where |0‚ü©describes the photon passing through the
mirror, and |1‚ü©describes the photon being reÔ¨Çected.
The notion of a qubit is an abstract concept and many physical sys-
tems can embody it.
In Figure 10.11, the information of a qubit
is encoded in the path of the photon takes after it encounters the
half-silvered mirror. Qubits are a device-independent representation
of quantum information, similar to how classical bits are device-
independent representations for classical information. A bit is a bit,
regardless of whether it is transmitted over the network, stored in
RAM, or burned to a DVD. Similarly a qubit is a qubit, regardless
of whether it's encoded in the polarization of a photon, an electron's
spin, or in the direction of magnetic Ô¨Çux of a superconducting loop.
Quantum state preparation
The operation of encoding some
classical information into a quantum system is called state prepara-
tion. Imagine an apparatus that prepares quantum systems in one
of several possible quantum states, depending on the position of the
"control switch" x of the machine.
x ‚àí‚Üí
state preparation
‚àí‚Üí|x‚ü©
Figure 10.12: The classical input x is used to prepare a quantum system.
The state |x‚ü©produced when the input is x in one of several possible states.
An example of quantum state preparation is a machine that can pro-
duce photons in two diÔ¨Äerent polarizations |H‚ü©and |V ‚ü©. If the input
x = 0 is speciÔ¨Åed, the machine will produce the state |H‚ü©, and if x = 1
is speciÔ¨Åed as input, the machine will produce |V ‚ü©.

10.5
POSTULATES OF QUANTUM MECHANICS
437
Having deÔ¨Åned quantum states, it's now time to discus what we
can do with quantum states. What are the allowed quantum opera-
tions?
Quantum operations
The second deÔ¨Ånition that belongs on the back-of-the-enveloper ex-
planations, is the identiÔ¨Åcation of quantum operations with unitary
transformations acting on quantum states.
The following diagram
shows the the quantum operation U being applied to the input state
|œà‚ü©to produce the output state |œà‚Ä≤‚ü©.
|œà‚ü©‚àí‚Üí
U
‚àí‚Üí|œà‚Ä≤‚ü©
More generally, all quantum operations can perform on quantum
states are represented by unitary transformations as codiÔ¨Åed in the
second postulate of quantum mechanics.
Postulate 2. Time evolution of an isolated quantum system is uni-
tary.
If the state at time t1 is |œà1‚ü©and at time t2 is |œà2‚ü©then ‚àÉ
unitary U such that |œà2‚ü©= U|œà1‚ü©.
Recall that a unitary matrix U obeys U ‚Ä†U = UU ‚Ä† = 1. The second
postulate ensures that quantum states will maintain their unit-length
property after quantum operation are performed on them. Assume
the quantum system starts from a state |œà1‚ü©that has unit length
‚à•|œà1‚ü©‚à•2 = ‚ü®œà1|œà1‚ü©= 1. After the unitary U is applied, the state after
evolution will be |œà2‚ü©= U|œà1‚ü©and its norm is ‚à•|œà2‚ü©‚à•2 = ‚à•U|œà1‚ü©‚à•2 =
‚ü®œà1|U ‚Ä†U|œà1‚ü©= ‚ü®œà1|1|œà1‚ü©= ‚ü®œà1|œà1‚ü©= 1. In other words, quantum
operations are length-preserving.
We'll now deÔ¨Åne several quantum gates that perform useful unitary
operations on qubits.
Example 1: phase gate
The Z operator is deÔ¨Åned by its action
on the elements of the standard basis.
Z|0‚ü©= |0‚ü©,
Z|1‚ü©= ‚àí1|1‚ü©.
The Z operator leaves the |0‚ü©unchanged but Ô¨Çips the phase of |1‚ü©.
Given the knowledge of the actions of the Z operator on the vectors
of the standard basis, we can construct its matrix representations:
Z =
1
0
0
‚àí1

Bs
Bs
‚â°|0‚ü©‚ü®0| ‚àí|1‚ü©‚ü®1|.

438
QUANTUM MECHANICS
Example 2: NOT gate
The X operator is deÔ¨Åned by the following
actions on the elements of the standard basis:
X|0‚ü©= |1‚ü©,
X|1‚ü©= |0‚ü©.
The X operator acts as a "quantum NOT gate" changing |0‚ü©s into
|1‚ü©s, and |1‚ü©s into |0‚ü©s. The matrix representation of the X operator
is:
X =
0
1
1
0

Bs
Bs
‚â°|0‚ü©‚ü®1| + |1‚ü©‚ü®0|.
Example 3: Hadamard gate
The Hadamard operator takes the
elements of the standard basis to the elements of the Hadamard basis
|+‚ü©‚â°
1
‚àö
2(|0‚ü©+ |1‚ü©) and |‚àí‚ü©‚â°
1
‚àö
2(|0‚ü©‚àí|1‚ü©):
H|0‚ü©=
1
‚àö
2(|0‚ü©+ |1‚ü©) ‚â°|+‚ü©,
H|1‚ü©=
1
‚àö
2(|0‚ü©‚àí|1‚ü©) ‚â°|‚àí‚ü©.
You can also think of the H operator as a 45‚ó¶counter-clockwise ro-
tation. The matrix representation of the Hadamard gate is
H =
"
1
‚àö
2
1
‚àö
2
1
‚àö
2
‚àí1
‚àö
2
#
Bs
Bs
.
By linearity, we can deduce the eÔ¨Äects of the operators Z, X, and H
on an arbitrary qubit Œ±|0‚ü©+ Œ≤|1‚ü©:
Z(Œ±|0‚ü©+ Œ≤|1‚ü©) = Œ±|0‚ü©‚àíŒ≤|1‚ü©,
X(Œ±|0‚ü©+ Œ≤|1‚ü©) = Œ≤|0‚ü©+ Œ±|1‚ü©,
H(Œ±|0‚ü©+ Œ≤|1‚ü©) = Œ±+Œ≤
‚àö
2 |0‚ü©+ Œ±‚àíŒ≤
‚àö
2 |1‚ü©.
Example 4
The eÔ¨Äect of the the operator XZ corresponds to the
combination of the eÔ¨Äects of the Z and X operators. We can un-
derstand the action of XZ either by applying it to a arbitrary qubit
Œ±|0‚ü©+ Œ≤|1‚ü©:
XZ(Œ±|0‚ü©+ Œ≤|1‚ü©) = ‚àíŒ≤|0‚ü©+ Œ±|1‚ü©,
or by multiplying together the operator's matrix representations:
XZ =

0
1
1
0

Bs
Bs

1
0
0
‚àí1

Bs
Bs
=

0
‚àí1
1
0

Bs
Bs
.
In general, many possible other quantum operations and combinations
of operations are possible. The ones presented above are simply the
most commonly used. Note that unitary time evolution is invertible:
for every quantum gate G there exists an inverse gate G‚àí1 such that
G‚àí1G = 1.

10.5
POSTULATES OF QUANTUM MECHANICS
439
Exercises
E10.4 Compute XHHY (Œ±|0‚ü©+ Œ≤|1‚ü©.
E10.5 Compute XX, XY , and Y X.
We're now two-thirds down the back of the envelope, and it's time to
talk about the third fundamental idea in quantum mechanics: quan-
tum measurements.
Quantum measurements
A quantum measurement performed on a quantum system corre-
sponds to a collections of projection operators {Œ†i} that act on the
Hilbert space.
A measurement with n possible outcomes is repre-
sented by n projection operators {Œ†1, Œ†2, . . . , Œ†n}. The projection
operators form a decomposition of the identity, meaning their sum is
the identity matrix:
n
X
i=1
Œ†i = 1.
Intuitively, the n diÔ¨Äerent projection operators correspond to n diÔ¨Äer-
ent alternatives for the evolution of the quantum system. Performing
the measurement is like asking "which is it going to be?" and letting
the system decide which path it wants to take. Born's rule is used to
assign probabilities to diÔ¨Äerent measurement outcomes.
Postulate 3. A quantum measurement is modelled by a collection
of projection operators {Œ†i} that act on the state space of the system
being measured and satisfy P
i Œ†i = 1. The index i labels the diÔ¨Äerent
measurement outcomes.
The probability of outcome i when performing measurement {Œ†i}
on a quantum system in the state |œà‚ü©is given by the square-modulus
of the state after the applying the ith projection operator:
Pr({outcome i given state |œà‚ü©}) ‚â°
Œ†i|œà‚ü©

2
(Born's rule).
When outcome i occurs, the post-measurement state of the system is
|œà‚Ä≤
i‚ü©‚â°
Œ†i|œà‚ü©
‚à•Œ†i|œà‚ü©‚à•.
Let's unpack this deÔ¨Ånition to see what is going on.

440
QUANTUM MECHANICS
Born's rule
For the measurement deÔ¨Åned by the projection oper-
ators {Œ†1, Œ†2, . . . , Œ†n}, Born's rule states the probability of outcome
i is ‚ü®œà|Œ†i|œà‚ü©.
This expression for the square of the modulus of the overlap be-
tween |œà‚ü©and Œ†i can be written in several equivalent ways:
Œ†i|œà‚ü©

2
= (Œ†i|œà‚ü©, Œ†i|œà‚ü©) = ‚ü®œà|Œ†iŒ†i|œà‚ü©= ‚ü®œà|Œ†i|œà‚ü©.
where the last equality follows from idempotence property of projec-
tors Œ†i = Œ†2
i . The last expression with the projection operator "sand-
wiched" by two copies of the quantum state is the physicist's usual
way of expressing Born's rule, deÔ¨Åning Pr({outcome i}) ‚â°‚ü®œà|Œ†i|œà‚ü©.
For the class of projective measurements we're discussing here, the
two deÔ¨Ånitions are equivalent.
The set of projection {Œ†1, Œ†2, . . . , Œ†n} forms a decomposition of
the identity 1 = P
i Œ†i. This guarantees the probability distribution
of the diÔ¨Äerent outcomes is well normalized:
1 = ‚à•1|œà‚ü©‚à•2 = ‚à•(Œ†1 + ¬∑ ¬∑ ¬∑ + Œ†n)|œà‚ü©‚à•2
py= ‚à•Œ†1|œà‚ü©‚à•2 + ¬∑ ¬∑ ¬∑ + ‚à•Œ†n|œà‚ü©‚à•2
= Pr({outcome 1}) + ¬∑ ¬∑ ¬∑ + Pr({outcome n}) .
That's good to check, otherwise Kolmogorov would be angry with us.
Note the equality labelled
py= follows from Pythagoras' theorem; we're
using the fact that operators {Œ†i} are mutually orthogonal.
Post-measurement state
When outcome i occurs, Postulate 3
tells us the state of the quantum system becomes |œà‚Ä≤
i‚ü©‚â°
Œ†i|œà‚ü©
‚à•Œ†i|œà‚ü©‚à•,
which is the result of applying the projection Œ†i to obtain Œ†i|œà‚ü©, and
then normalizing the state so ‚à•|œà‚Ä≤
i‚ü©‚à•= 1.
Measurements are not passive observations! Quantum measure-
ment is an invasive procedure that typically changes the state of
the system being measured. In general |œà‚Ä≤
i‚ü©Ã∏= |œà‚ü©and we say the
state is disturbed by the measurement, thought it's also possible that
|œà‚Ä≤
i‚ü©= |œà‚ü©when the input state lives entirely within the image sub-
space of one of the projection operators: |œà‚Ä≤
i‚ü©= Œ†i|œà‚ü©= |œà‚ü©.
Another way to describe what happens during a quantum mea-
surement is to say the state |œà‚ü©collapses into the state |œà‚Ä≤
i‚ü©.
We
won't use this the terminology of "collapse" here because this termi-
nology often leads to magical thinking and whacky interpretations
of the mechanism behind quantum measurement. Of the four postu-
lates of quantum mechanics, Postulate 3 is the most debated one, with
various scientists denying it, or attaching free-for-all interpretations

10.5
POSTULATES OF QUANTUM MECHANICS
441
about how this "collapse" occurs. This is usually where the "human
observer" aspect of pop-psychology books connects. The reasoning
goes "physicists can't explain wave function collapse; neuroscientists
can't explain consciousness; therefore . . . " and then some nonsense.
A less magical way to think about quantum measurement is in
terms of interaction between the quantum state of a particle and a
classical measurement apparatus which can be in one of n possible
states. Because of the relative size of the two interacting systems, the
state |œà‚ü©is forced to "align" with one of the n possible states of the
measurement apparatus. A quantum measurement is an interaction
that creates classical information and destroys quantum information.
By measuring, we obtained the measurement outcome i, but disturbed
the initial state |œà‚ü©, forcing it into the "aligned with Œ†i"-state |œà‚Ä≤
i‚ü©. We
can still carry out more experiments with the post-measurement state
|œà‚Ä≤
i‚ü©, but it's not the same as the initial state |œà‚ü©. SpeciÔ¨Åcally, we've
lost all the information about |œà‚ü©that used to lie in the subspace
(1 ‚àíŒ†i).
Example 4
In Figure 10.13 a state vector |œà‚ü©= Œ±|0‚ü©+Œ≤|1‚ü©is being
measured with photo detectors modelled as projectors given by
Œ†0 = |0‚ü©‚ü®0|
P
j Œ†j = 1
Œ†1 = |1‚ü©‚ü®1|
Pr({0}|œà) ‚â°Pr({outcome 0 given state |œà‚ü©})
‚â°‚ü®œà|
Œ†0
|œà‚ü©
= ‚ü®œà|
|0‚ü©‚ü®0|
|œà‚ü©
= (¬ØŒ±‚ü®0| + ¬ØŒ≤‚ü®1|) |0‚ü©‚ü®0| (Œ±|0‚ü©+ Œ≤|1‚ü©)
= ¬ØŒ±Œ±
= |Œ±|2.
The probability of outcome 1 is Pr({1}|œà) ‚â°‚ü®œà|Œ†1|œà‚ü©= |Œ≤|2. The
state of the quantum system after the measurement is in one of two
possible states: |œà‚Ä≤
0‚ü©= |0‚ü©or |œà‚Ä≤
1‚ü©= |1‚ü©.
Example 5
Consider the measurement {Œ†+, Œ†‚àí} that consists of
the projectors onto the Hadamard basis:
Œ†+ = |+‚ü©‚ü®+|
Œ†+ + Œ†‚àí= 1
Œ†‚àí= |‚àí‚ü©‚ü®‚àí|

442
QUANTUM MECHANICS
Given the quantum state |œà‚ü©= Œ±|0‚ü©+Œ≤|1‚ü©, the probability of outcome
"+" is given by
Pr({+}|œà) ‚â°‚à•Œ†+|œà‚ü©‚à•2
= ‚à•|+‚ü©‚ü®+||œà‚ü©‚à•2
= ‚à•|+‚ü©‚ü®+|(Œ±|0‚ü©+ Œ≤|1‚ü©)‚à•2
= ‚à•|+‚ü©(Œ±‚ü®+|0‚ü©+ Œ≤‚ü®+|1‚ü©)‚à•2
=
|+‚ü©

Œ± 1
‚àö
2
+ Œ≤ 1
‚àö
2

2
= (Œ± + Œ≤)2
2
‚à•|+‚ü©‚à•2
= (Œ± + Œ≤)2
2
.
The probability of outcome "‚àí" is Pr({‚àí}|œà) = (Œ±‚àíŒ≤)2
2
. The state of
the quantum system after the measurement is in one of two possible
states: |œà‚Ä≤
+‚ü©= |+‚ü©or |œà‚Ä≤
‚àí‚ü©= |‚àí‚ü©.
Figure 10.13: The state of a photon after encountering a (1 ‚àíŒ±)-silvered
mirror is |Œ≥‚ü©= Œ±|0‚ü©+Œ≤|1‚ü©. The probability that the horizontal photodetec-
tor "clicks" is |Œ±|2, and is obtained by projecting |Œ≥‚ü©on the subspace |0‚ü©‚ü®0|.
The probability that the top photodetector clicks is equal to |Œ≤|2, and is
obtained by projecting |Œ≥‚ü©on the subspace |1‚ü©‚ü®1|.
The measurement process is a fundamental aspect of the quantum
models. You have to get used to the idea that measurements change
systems' states.
That's not magic, but simply due to the relative
size of the systems (tiny quantum particles and huge measurement
apparatus) and the fact we're forcing them to interact.
Composite quantum systems
So far we discussed state preparation, quantum operations, and quan-
tum measurements of individual qubits. It's now time to discuss quan-
tum models for systems made up of multiple qubits.

10.5
POSTULATES OF QUANTUM MECHANICS
443
Classically, if we have two bits b1 ‚àà{0, 1} and b2 ‚àà{0, 1}, we
can concatenate them to obtain a bit string b1b2 ‚àà{0, 1}2, which can
have one of four possible values: 00, 01, 10, and 11. The combined
state of two qubits |œï1‚ü©‚ààC2 and |œï2‚ü©‚ààC2 is the tensor product state
|œï1‚ü©‚äó|œï2‚ü©in the four-dimensional tensor product space C2‚äóC2 = C4.
A basis for the tensor product space can be obtained by taking all
possible combinations of the basis elements for the individual qubits:
{|0‚ü©‚äó|0‚ü©, |0‚ü©‚äó|1‚ü©, |1‚ü©‚äó|0‚ü©, |1‚ü©‚äó|1‚ü©}.
Postulate 4. The state space of a composite quantum system is the
tensor product of the state spaces of the individual systems. If you
have systems 1, 2, . . . , n in states |œï1‚ü©, |œï2‚ü©, ¬∑ ¬∑ ¬∑ , |œïn‚ü©, then the state
of the composite system is |œï1‚ü©‚äó|œï2‚ü©‚äó¬∑ ¬∑ ¬∑ ‚äó|œïn‚ü©.
Postulate 4 tells us how the state spaces of diÔ¨Äerent quantum systems
may be combined to give a description of the composite system. A lot
of the interesting quantum applications involve operations on multiple
qubits and are described by vectors in a tensor product space, so we
better look into this "‚äó"-thing more closely.
Tensor product space
You may not have heard of the tensor prod-
uct before, but don't worry about it. The only scary part is the symbol
"‚äó." A tensor product space consists of all possible combinations of
the basis vectors for the two subspaces. For example, consider two
qubits |œï1‚ü©‚ààV1 ‚â°C2 and |œï2‚ü©‚ààV2 ‚â°C2. We'll denote the stan-
dard basis for V1 as B1 ‚â°{|0‚ü©1, |1‚ü©1} and the standard basis for V2
as B2 ‚â°{|0‚ü©2, |1‚ü©2}.
The tensor product space B12 ‚â°V1 ‚äóV2 is
four-dimensional and this is a basis for it:
B12 ‚â°

|0‚ü©1‚äó|0‚ü©2, |0‚ü©1‚äó|1‚ü©2, |1‚ü©1‚äó|0‚ü©2, |1‚ü©1‚äó|1‚ü©2
	
.
This level of subscripts and the explicit use of the symbol ‚äóreally
hurts the eyes (and the hand if you have to use this notation when
solving problems). It is therefore customary to drop the subscripts
indicating which vector space the vector comes form, omitting the
tensor product symbol, and drawing a single ket which contains a
"string" of indices
|a‚ü©1‚äó|b‚ü©2 = |a‚ü©‚äó|b‚ü©= |a‚ü©|b‚ü©= |ab‚ü©.
The basis for the tensor product space B12 ‚â°V1 ‚äóV2 looks much
nicer using the simpliÔ¨Åed notation:
B12 ‚â°

|00‚ü©, |01‚ü©, |10‚ü©, |11‚ü©
	
.

444
QUANTUM MECHANICS
Tensor product of two vectors
Suppose we're given two qubits
whose states are described by the following vectors:
|œïa‚ü©1 = Œ±1|0‚ü©1 + Œ≤1|1‚ü©1,
|œïb‚ü©2 = Œ±2|0‚ü©2 + Œ≤2|1‚ü©2,
Note the subscripts indicate which system we're describing: |0‚ü©1 is
the state |0‚ü©for the Ô¨Årst qubit, while |0‚ü©2 is the state |0‚ü©of the second
qubit.
The state of the combined system is tensor product state |œïab‚ü©12 =
|œïa‚ü©1 ‚äó|œïb‚ü©2, which is computed by combining all possible combina-
tions of the coeÔ¨Écients of |œïa‚ü©1 and the coeÔ¨Écients of |œïb‚ü©2:
(Œ±1, Œ≤1)B1 ‚äó(Œ±2, Œ≤2)B2 = (Œ±1Œ±2, Œ±1Œ≤2, Œ≤1Œ±2, Œ≤1Œ≤2)B12 .
The notion of "all possible linear combinations" is easier to see by
considering the tensor product operation in terms of the basis vectors:
|œïab‚ü©12 = |œïa‚ü©1 ‚äó|œïb‚ü©2
= (Œ±1|0‚ü©1 + Œ≤1|1‚ü©1) ‚äó(Œ±2|0‚ü©2 + Œ≤2|1‚ü©2)
= Œ±1Œ±2|0‚ü©1|0‚ü©2 + Œ±1Œ≤2|0‚ü©1|1‚ü©2 + Œ≤1Œ±2|1‚ü©1|0‚ü©2 + Œ≤1Œ≤2|1‚ü©1|1‚ü©2
= Œ±1Œ±2|00‚ü©+ Œ±1Œ≤2|01‚ü©+ Œ≤1Œ±2|10‚ü©+ Œ≤1Œ≤2|11‚ü©
= (Œ±1Œ±2, Œ±1Œ≤2, Œ≤1Œ±2, Œ≤1Œ≤2)B12 .
Where B12 ‚â°{|00‚ü©, |01‚ü©, |10‚ü©, |11‚ü©} is the standard basis for the tensor
product space.
Dimension counting
A quantum state that consists of n qubits
can represent any unit length vector in C2n. That's an insanely big
state space‚Äîa huge 2n-dimensional playground.
In comparison, a
classical bitstring of length n can take on one of 2n values.
A very large state space does not necessarily make a model more
successful, but the large dimension of the tensor product space suggest
many new possibilities. Much of the recent excitement in the area of
quantum computing is based on the promise of using the qubits of a
quantum computer to perform computations in very large quantum
state spaces.
Think how much more powerful quantum states are
compared to binary strings. Adding an extra bit to a classical registers
doubles the number of states it can represent.
Adding a qubit to
a quantum register adds a whole two-dimensional subspace to the
quantum state space.
We shouldn't get carried away with enthusiasm, because with great
state space comes great noise! It is easy to imagine n qubits in a row,
but building a physical system which can store n qubits and protect
them from noise is a much more diÔ¨Écult task. Another bottleneck in
quantum computing is the diÔ¨Éculty of extracting information from

10.5
POSTULATES OF QUANTUM MECHANICS
445
quantum systems. The quantum state space of n qubits is C2n, but
projective measurements of the form {Œ†1, Œ†2, . . . , Œ†m} can only ob-
tain the one answer to a question with m possible classical outcomes
(m ‚â§2n). The information we learn about a quantum state through
measurement is directly proportional to the disturbance we cause to
the state. We'll learn more about theoretical and practical consider-
ations for quantum computing in Section 10.8.
Exercises
E10.6 Show that the quantum state |0‚ü©|1‚ü©‚àí|1‚ü©|0‚ü©is equal to the
quantum state |+‚ü©|‚àí‚ü©‚àí|‚àí‚ü©|+‚ü©.
Quantum entanglement
At the risk of veering further oÔ¨Ä-topic for a linear algebra book,
we'll now say a few words about quantum states which are entan-
gled. In particular we'll discuss the properties of the entangled state
|Œ®‚àí‚ü©‚â°
1
‚àö
2(|01‚ü©‚àí|10‚ü©). Some of the most fascinating results in quan-
tum information science make use of pre-shared entangled states. En-
tanglement is some really crazy stuÔ¨Ä, and I mean really crazy, not just
science-journalism crazy.
In Section 8.9 we discussed how shared secret key between two
parties, Alice and Bob, is a communication resource that can be used
to achieve private communication (using the one-time pad cryptosys-
tem). You can think of entanglement as another type of communica-
tion resource: a stronger-than-classical correlation between two parts
of a quantum system. One half of system is held by Alice, the other
half is held by Bob. When the collective state of a two-qubit quantum
systems is in the entangled state
1
‚àö
2(|01‚ü©‚àí|10‚ü©), measurements on
the individual qubits will produce anti-correlated results in any basis.
Example 7
The Einstein-Podolsky-Rosen (EPR) state is a two-
qubit quantum state with interesting nonlocal properties. We assume
that Alice holds one half of the quantum state, Bob holds the other
half of the following state:
|Œ®‚àí‚ü©AB ‚â°
1
‚àö
2|0‚ü©A|1‚ü©B ‚àí
1
‚àö
2|1‚ü©A|0‚ü©B.
Note the use of superscript to denote which party holds that part of
the system.
Let's analyze the results of diÔ¨Äerent measurement Alice and Bob
can perform on this state. If Alice chooses to measures her system in
the basis {|0‚ü©, |1‚ü©}, the projection operators that correspond to the

446
QUANTUM MECHANICS
two outcomes are
Œ†AB
0
= |0‚ü©‚ü®0|A ‚äó1B
and
Œ†AB
1
= |1‚ü©‚ü®1|A ‚äó1B.
Since we're measuring only Alice's half of the state, the measurement
acts like the identity operator on Bob's half of the state. There's a
50-50 chance of outcomes 0 and 1, and depending on the outcome,
the post-measurement state of the system will be either |0‚ü©A|1‚ü©B
or |1‚ü©A|0‚ü©B. If Bob then measures his half of the system, he'll be
sure to obtain the opposite outcome of Alice's. In other words, the
measurement outcomes that Alice and Bob obtain are perfectly anti-
correlated.
What if Alice and Bob choose to measure their respective halves
of the EPR state |Œ®‚àí‚ü©AB in the basis {|+‚ü©, |‚àí‚ü©}? Using some ba-
sic calculations (see Exercise E10.6), we can express the EPR state
|Œ®‚àí‚ü©AB in terms of the basis {|+‚ü©, |‚àí‚ü©} as follows:
1
‚àö
2|0‚ü©A|1‚ü©B ‚àí
1
‚àö
2|1‚ü©A|0‚ü©B = |Œ®‚àí‚ü©AB =
1
‚àö
2|+‚ü©A|‚àí‚ü©B ‚àí
1
‚àö
2|‚àí‚ü©A|+‚ü©B.
Observe the state |Œ®‚àí‚ü©AB has the same structure in the Hadamard
basis as in the standard basis. Thus, Alice and Bob's measurement
outcomes will also be perfectly anti-correlated when measuring in the
Hadamard basis.
A state |œà‚ü©AB is called entangled if it cannot be written as a tensor
product of quantum states |œÜ‚ü©A‚äó|œï‚ü©B, where |œÜ‚ü©A describes the state
held by Alice and |œï‚ü©B the state help by Bob. The EPR state |Œ®‚àí‚ü©AB
is entangled, which means it cannot be written as a tensor product of
the quantum states of individual qubits |œÜ‚ü©A ‚äó|œï‚ü©B:
|0‚ü©‚äó|1‚ü©‚àí|1‚ü©‚äó|0‚ü©Ã∏= (a|0‚ü©+ b|1‚ü©)‚äó(c|0‚ü©+ d|1‚ü©),
for any a, b, c, d ‚ààC. Since we cannot describe the EPR state |Œ®‚àí‚ü©AB
as the tensor product of two local states |œÜ‚ü©A and |œï‚ü©B, we say it
requires a nonlocal description, which is another way of saying |Œ®‚àí‚ü©AB
is entangled.
There is something strange about the EPR state. If Alice mea-
sures her half of the state and Ô¨Ånds |0‚ü©, then we know immediately
that Bob's state will be |1‚ü©. The collapse in the superposition on Al-
ice's side immediately causes a collapse of the superposition on Bob's
side. Note that Bob's collapse will occur immediately, no matter how
far Bob's system is from Alice's. This is what the authors Einstein,
Podolsky, and Rosen called "spooky action at a distance." How could
Bob's system "know" to always produce the opposite outcome even at
the other end of the universe?
Now imagine you have a whole bunch of physical systems prepared
in the EPR state. Alice has the left halves of the EPR pairs, Bob

10.5
POSTULATES OF QUANTUM MECHANICS
447
has all the right halves. This is a communication resource we call
shared entanglement.
Many of the quantum information protocols
make use of shared entanglement between Alice and Bob, to achieve
novel communication tasks.
So far we discussed entangled states from their measurement out-
comes as answers to questions. From the point of view of information,
a system is entangled whenever you know more about the system as
a whole than about its parts. But who has seen this entanglement?
Does |Œ®‚àí‚ü©really exist? How can we know that |Œ®‚àí‚ü©isn't just some
math scribble on a piece of paper? To quell your concerns, we'll give a
real-world physics example where we knowing more about the whole
system than about its constituent parts.
Physics example
We'll now describe a physical process that leads
to the creation of an entangled quantum state. Consider a quantum
particle p that decays into two quantum subparticles pa and pb. The
decay process obeys various physics conservation laws, in particular,
the total spin angular momentum before and after the decay must be
conserved. Suppose the particle p had zero spin angular momentum
before the decay, then the conservation of angular momentum princi-
ple dictates the subparticles pa and pb must have opposite spin. The
spin angular momentum of a each particle is in an unknown direc-
tion (up, down, left, right, in, or out), but whatever spin direction we
measure for pa, the spin angular momentum of pb will immediately
take on the opposite direction.
The general scenario discussed above describes what happens if a
Helium atom were to explode, and the two electrons in it's ground
state Ô¨Çy oÔ¨Äto distant sides of the universe. The two electrons will
have opposite spins, but we don't know the directions of the individual
spins. The only thing we know is their total spin is zero, since the
ground state of the Helium atom had spin zero. This is how we get
the "anti-correlation in any basis" aspect of quantum entanglement.
Summary
We can summarize the new concepts of quantum mechanics and relate
them to the standard concepts of linear algebra in the following table:
quantum state ‚áîvector |v‚ü©‚ààCd
evolution ‚áîunitary operations
measurement ‚áîprojections
composite system ‚áîtensor product

448
QUANTUM MECHANICS
The quantum formalism embodied in the four postulates discussed
above has been applied to describe many physical phenomena. Using
complex vectors to represent quantum states leads to useful models
and predictions for experimental outcomes. In the next section we'll
use the quantum formalism to analyze the outcomes of the polariza-
tion lenses experiment.
In addition to the applications of quantum principles, studying
the structure in quantum mechanics states and operations is an inter-
esting Ô¨Åeld on its own. An example of fundamentally new quantum
idea is existence of entangled quantum states, which are states of a
composite quantum system |Œ¶‚ü©12 ‚ààV1 ‚äóV2 that cannot be written as
a tensor product of local quantum states of the individual systems:
|Œ®‚ü©12 Ã∏= |œÜ‚ü©1 ‚äó|œï‚ü©2. Later on in this section discuss an interesting
application of quantum entanglement, as part of the quantum telepor-
tation protocol, illustrated in Figure 10.23 (page 468).
Exercises
E10.7 Compute probability of outcome "‚àí" for the measurement
{Œ†+, Œ†‚àí} = {|+‚ü©‚ü®+|, |‚àí‚ü©‚ü®‚àí|} performed on the quantum state |œà‚ü©=
Œ±|0‚ü©+ Œ≤|1‚ü©.
Links
[ Compact set of notes on QM written by a physicist ]
http://graybits.biz/notes/quantum_mechanics/preface
[ Lecture series on QM written by a computer scientist ]
http://scottaaronson.com/democritus/lec9.html
10.6
Polarizing lenses experiment revisited
Let's revisit the polarizing lenses experiment, this time modelling
the polarization states of photons as two-dimensional complex vec-
tors. We can deÔ¨Åne the state of a horizontally polarized photon as
|H‚ü©‚â°(1, 0)T and the vertical polarization state as |V ‚ü©‚â°(0, 1)T.
This choice corresponds to the observation that horizontal and verti-
cal polarizations are complementary, which we model as orthogonal
state vectors. Starting with unpolarized light, and passing it through
a polarizing lens allows us to prepare photons in a chosen state:

10.6
POLARIZING LENSES EXPERIMENT REVISITED
449
light ‚ÜíH ‚Üí
 1
0

‚â°|H‚ü©,
light ‚ÜíV ‚Üí
 0
1

‚â°|V ‚ü©.
Figure 10.14: State preparation procedure for photons with horizontal or
vertical polarization.
Placing a polarizing lens in the path of a photon corresponds to the
measurement {Œ†‚Üê, Œ†‚Üí} of the photon's polarization state. The two
possible outcomes of the measurements are: "photon goes through"
and "photon is reÔ¨Çected." Each photon that hits a polarizing lens is
asked the question "are you horizontally or vertically polarized?" and
forced to decide.
Figure 10.15 shows the projection operators that correspond to a
measurement using a horizontally polarizing lens. The outcome "goes
through" corresponds to the projection operator Œ†‚Üí= |H‚ü©‚ü®H| ‚â°
[ 1 0
0 0 ]. The outcome "is reÔ¨Çected" corresponds to the projection matrix
Œ†‚Üê= |V ‚ü©‚ü®V | ‚â°[ 0 0
0 1 ].
|Œ≥‚ü©
‚àí‚Üí
H
|{z}
Ô£±
Ô£≤
Ô£≥Œ†‚Üê‚â°
Ô£Æ
Ô£∞0
0
0
1
Ô£π
Ô£ª
,
Œ†‚Üí‚â°
Ô£Æ
Ô£∞1
0
0
0
Ô£π
Ô£ª
Ô£º
Ô£Ω
Ô£æ
‚àí‚Üí
Figure 10.15: A horizontally polarizing lens corresponds to the quantum
measurement {Œ†‚Üê, Œ†‚Üí}.
An incoming photon in the state |Œ≥‚ü©is asked
to choose one of the two alternative paths. With probability ‚à•Œ†‚Üí|Œ≥‚ü©‚à•2,
the photon will go through the H-polarizing lens and become horizontally
polarized |Œ≥‚Ä≤‚ü©= (1, 0)T. With probability ‚à•Œ†‚Üê|Œ≥‚ü©‚à•2, it will be reÔ¨Çected.
Recall that the probability of outcome i in a quantum measurement
{Œ†i} is given by the expression ‚à•Œ†i|Œ≥‚ü©‚à•2, where |Œ≥‚ü©is the state of the
incoming photon. Knowing the projection operators that correspond
to the "goes through" and "is reÔ¨Çected" outcomes allows us to predict
the probability that photons with a given state will make it though
the lens. The setup shown in Figure 10.16 illustrates a simple two-
step experiment in which states prepared in the horizontally polarized
state |H‚ü©= (1, 0)T arrive at a V -polarizing lens. The probability of

450
QUANTUM MECHANICS
photons making it through the V -polarizing lens is
Pr({pass through V lens given state |H‚ü©}) = ‚à•|V ‚ü©‚ü®V ||H‚ü©‚à•2
=

0
0
0
1
 1
0

2
=

0
0

2
= 0.
Indeed, this is what we observe in the lab‚Äîall |H‚ü©photons are are
reject by the V -polarizing lens.
light ‚àí‚Üí
H
| {z }
("0
0
0
1
#
,
"1
0
0
0
#)
‚àí‚Üí
|
{z
}
state preparation

1
0

‚àí‚Üí
V
|{z}
("1
0
0
0
#
,
"0
0
0
1
#)
‚àí‚Üí‚àÖ
|
{z
}
measurement
Figure 10.16: Photons prepared in the state |H‚ü©= (1, 0)T are rejected
by the V -polarizing lens because their states have zero overlap with the
projector Œ†‚Üí= |V ‚ü©‚ü®V |.
Let's now use the quantum formalism to analyze the results of the
three-lenses experiment which we saw earlier in the chapter.
Fig-
ure 10.17 shows the optical circuit which consists of a state prepara-
tion step and two measurement steps. The diagonally polarizing lens
placed in the middle of the circuit only allows through photons that
have 45‚ó¶-diagonal polarization: |D‚ü©‚â°
  1
‚àö
2,
1
‚àö
2
T. The projection op-
erator associated with the "goes through" outcome of the diagonal
polarization measurement is
Œ†‚Üí= |D‚ü©‚ü®D| =
" 1
‚àö
2
1
‚àö
2
# h
1
‚àö
2
1
‚àö
2
i
=
" 1
2
1
2
1
2
1
2
#
.
The post-measurement state of photons that make it through the
diagonally polarizing lens will be |D‚ü©‚â°
  1
‚àö
2,
1
‚àö
2
T.

10.6
POLARIZING LENSES EXPERIMENT REVISITED
451
light ‚Üí
H
| {z }
("0
0
0
1
#
,
"1
0
0
0
#)
‚Üí
|
{z
}
state preparation

1
0

‚Üí
D
| {z }
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
2
‚àí1
2
‚àí1
2
1
2
Ô£π
Ô£∫Ô£∫Ô£ª,
Ô£Æ
Ô£ØÔ£∞
1
2
1
2
1
2
1
2
Ô£π
Ô£∫Ô£ª
Ô£º
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£æ
‚Üí
|
{z
}
measurement 1
Ô£Æ
Ô£∞
1
‚àö
2
1
‚àö
2
Ô£π
Ô£ª‚Üí
V
|{z}
("1
0
0
0
#
,
"0
0
0
1
#)
‚Üí
|
{z
}
measurement 2

0
1

Figure 10.17: Photons prepared in the state |H‚ü©are subjected to two
sequential measurements: a diagonal polarizing measurement D followed
by a vertical polarization measurement V . The projection operators for "is
reÔ¨Çected" and "goes though" outcomes are indicated in each step.
The photons that made it through the middle lens are in the state
|D‚ü©. Let's analyze what probability of photons making it through the
V -polarizing lens:
Pr({pass through V lens given state |D‚ü©}) = ‚à•|V ‚ü©‚ü®V ||D‚ü©‚à•2
=

0
0
0
1
 " 1
‚àö
2
1
‚àö
2
#
2
=

 0
1
‚àö
2

2
= 02 +

1
‚àö
2
2
= 1
2.
The overall probability of a photon making it through until the end
is 1
4.
This probability is consistent with the light intensity observations
we see in Figure 10.8 (page 424).
Observe the diÔ¨Äerence in inter-
pretation: we previously referred to the optical power P = 0.5 after
the Ô¨Årst measurement, and optical power P = 0.25 after the second
measurement, whereas the quantum formalism refers to probabilities
p = 0.5 and p = 0.25 for the photons to reach diÔ¨Äerent parts of the
circuit.
Discussion
The three-lenses experiment serves to illustrates some key aspects
of the quantum formalism. When performed with a beam of light,
the outcomes of the experiment can be explained using the classical
theory of electromagnetic waves. The light beam consists of so many

452
QUANTUM MECHANICS
photons that it behaves like continuous quantity that can be split:
part of the wave goes through the lens, and another part gets reÔ¨Çected.
The projections to the "goes through" orientation performed by the
polarizing lenses can easily be understood if we model the light beam
as a classical wave. Therefore, the table-top experiment we performed
cannot be seen as a proof that quantum mechanics description of
reality is necessary.
The same experiment can be reproduced with single photon sources,
which behave like really really weak laser pointers, emitting only one
photon at a time.
The classical explanation runs into trouble ex-
plaining what happens, since a single photon cannot be subdivided
into parts. Rather than seeing this as a "failure" of classical wave the-
ory, we can see it as a resounding success, since understanding waves
allows you to understand quantum theory better.
The polarizing lenses experiment is inspired by the famous Stern-
Gerlach experiment. The demonstration and reasoning about the ob-
served outcomes is very similar, but the experiment is performed with
the magnetic spin of silver atoms. I encourage you to learn more about
the original Stern-Gerlach experiment.
https://en.wikipedia.org/wiki/Stern-Gerlach_experiment
https://youtube.com/watch?v=rg4Fnag4V-E
10.7
Quantum physics is not that weird
Without a doubt, you have previously heard that quantum mechanics
is mysterious, counterintuitive, and generally "magical." It's not that
magical, unless, of course, vector operations count as magic. In this
section we'll single out three so called "weird" aspects of quantum
mechanics: superposition, interference, and the fact that quantum
measurements aÔ¨Äect the states of systems being measured. We'll see
these aspect of quantum mechanics are actually not that weird. This
is a necessary "demystiÔ¨Åcation" step so we can get all the quantum
sensationalism out of the way.
Quantum superposition
Classical binary variables (bits) can have one of two possible values:
0 or 1. Examples of physical systems that behave like bits are electric
switches that can be either open or closed, digital transistors that
either conduct or don't conduct electricity, and capacitors that are
either charged or discharged.
A quantum bit (qubit), can be both 0 and 1 at the same time.
Wow! Said this way, this surely sounds impressive and mystical, no?

10.7
QUANTUM PHYSICS IS NOT THAT WEIRD
453
But if we use the term linear combination instead of "at the same
time," the quantum reality won't seem so foreign. A quantum state
is a linear combination of the basis states. This isn't so crazy, right?
The superposition principle is a general notion in physics that is not
speciÔ¨Åc to quantum phenomena, but applies to all systems described
by diÔ¨Äerential equations. Indeed, superpositions exist in many classi-
cal physics problems too.
Example
Consider a mass attached to a spring that undergoes sim-
ple harmonic motion. The diÔ¨Äerential equation that governs the mo-
tion of the mass is x‚Ä≤‚Ä≤(t)+œâ2x(t) = 0. This equation has two solutions:
x0(t) = sin(œât) and x1(t) = cos(œât), corresponding to two diÔ¨Äerent
starting points of the oscillation. Since both x0(t) and x1(t) satisfy
the equation x‚Ä≤‚Ä≤(t) + œâ2x(t) = 0, any linear combination of x0(t) and
x1(t) is also a solution. Thus, the most general solution to the diÔ¨Äer-
ential equation is of the form:
x(t) = Œ±x0(t) + Œ≤x1(t) = Œ± sin(œât) + Œ≤ cos(œât).
Usually we combine the sin and cos terms and describe the equation
of motion for the mass-spring system in the equivalent form x(t) =
A cos(œât+œÜ), where A and œÜ are computed from Œ± and Œ≤. In science-
journalist speak however, the mass-spring system would be described
as undergoing both sin motion and cos motion at the same time! Do
you see how ridiculous this sounds?
The notion of quantum superposition is simply a consequence of the
general superposition principle for diÔ¨Äerential equations. If the quan-
tum states |0‚ü©and |1‚ü©both represent valid solutions to a quantum
diÔ¨Äerential equation, then the state of the system can be described as
a linear combination of these two solutions:
|œà‚ü©= Œ±|0‚ü©+ Œ≤|1‚ü©.
The observation that "|œà‚ü©is both |0‚ü©and |1‚ü©at the same time" is not
wrong, it's just not very useful. It's much more precise to describe
the quantum state |œà‚ü©as a linear combination.
Interference
Unlike particles that bounce oÔ¨Äeach other, waves can co-exist in the
same place. The resulting wave pattern is the sum of the constituent
waves. Quantum particles behave similarly to waves in certain exper-
iments, and this can lead to interference between quantum systems.
The prototypal example of interference is Young's double-split ex-
periment, in which particles passing through two thin slits interact

454
QUANTUM MECHANICS
with each other causing an interference pattern of alternating bright
and dark spots on a screen. Classical physics models assume particles
behave like tiny point-like balls that bounce oÔ¨Äeach other whenever
they come in contact. The prediction of a classical model is that par-
ticles will appear on the screen in two bright peaks, directly facing
the two slits.
In contrast, the quantum model of a particle describes it as a trav-
elling energy pulse that exhibits wave-like properties.2 In a quantum
model, the particles coming through the slits behave like waves and
can combine constructively or destructively, depending on the rela-
tive distances travelled by the particles. Similar interference patterns
occur whenever waves combine, as in the example of waves on the
surface of a liquid, or sound waves.
Figure 10.18: The waves emitted by two synchronized sources form an
interference pattern. Observe the stripes of destructive interference where
the waves meat "out of sync" (peak to troughs) and cancel each other out.
Performing the experiment reveals a pattern of bright and dark
stripes (called fringes) on the screen in support of the quantum model.
The locations of the dark fringes corresponds exactly to the places
where particles coming from the two slits arrive "out of sync," and
combine destructively:
|œà‚ü©‚àí|œà‚ü©= 0.
This case corresponds to the dark fringes on the screen, where no
particles arrive.
The idea that one wave can cancel another wave is not new. What
is new is the observation that particles behave like waves and can in-
terfere with each other. That's deÔ¨Ånitely new. Interference was one of
2This is where the name wave function comes from.

10.7
QUANTUM PHYSICS IS NOT THAT WEIRD
455
the Ô¨Årst puzzling eÔ¨Äect of quantum systems that was observed. Ob-
servations from interference experiments forced physicists to attribute
wave-like properties even to particles.
[ Video demonstration of Young's double-split experiment ]
https://youtube.com/watch?v=qCmtegdqOOA
Measurement of a system aÔ¨Äects the system's state
Another "weird" aspect of quantum mechanics is the notion that quan-
tum measurements can aÔ¨Äect the states of the systems being mea-
sured. This is due to the energy scale and the size of systems where
quantum physics comes into play, and not due to some sort of "quan-
tum magic." Let's see why.
When we think about physical systems on the scale of individual
atoms, we can't continue to see ourselves (and our physical measure-
ment apparatuses) as a passive observers of the systems. We need
to take into account the interactions between quantum systems and
the measurement apparatus. For example, in order to see a particle,
we must bounce oÔ¨Äat least one photon from the particle and then
collect the rebounding photon at a detector. Because the momen-
tum and the energy of a single particle are tiny, the collision with the
photon as part of the experiment will make the particle recoil. The
particle recoils from the collision with the observing photon because
of the conservation of momentum principle. Thus, the measurement
might send the particle Ô¨Çying oÔ¨Äin a completely diÔ¨Äerent direction.
This goes to show that measurements aÔ¨Äecting the sate of systems
being measured is not some magical process but simply due to tiny
energies of the particles being observed.
Wave functions
The quantum mechanics techniques we discussed in this chapter are
good for modelling physical systems that have discrete sets of states.
In matrix quantum mechanics, quantum states are described by a
vectors in Ô¨Ånite-dimensional complex inner product spaces.
Other
physics problems require the use of wave function quantum mechanics
in which quantum states are represented as complex-valued functions
of space coordinates ‚Éór = (x, y, z).
Instead of the dot product be-
tween vectors, the inner product for wave functions is ‚ü®f(‚Éór), g(‚Éór)‚ü©=
RRR
R3 f(‚Éór)g(‚Éór) d3‚Éór. This may seem like a totally new ball game, but
actually calculations using wave functions are not too diÔ¨Äerent from
inner product calculations we used to compute Fourier transforma-
tions in Section 8.11.

456
QUANTUM MECHANICS
It's beyond the scope of the current presentation to discuss wave
functions in detail, but I want to show you an example of a calculation
with wave functions, so you won't say that I didn't show you some
proper physics stuÔ¨Ä. The ground state of the Hydrogen atom is de-
scribed by the wave function œà(‚Éór) =
1
‚àö
œÄa3 exp(‚àír/a), where r = ‚à•‚Éór‚à•.
The probability of Ô¨Ånding the electron at position ‚Éór from the proton
is described by the inner product œà(‚Éór)œà(‚Éór) ‚â°|œà(‚Éór)|2:
Pr({Ô¨Ånding electron at ‚Éór }) = |œà(‚Éór)|2.
Since œà(‚Éór) depends only on the distance r, we know the wave function
has a spherically symmetric shape, as illustrated in Figure 10.19.
Figure 10.19: The s orbital of an electron is spherically symmetric.
We'll now check if |œà(‚Éór)|2 is a properly normalized probability density
function. Integrating the probability density function |œà(‚Éór)|2 over all
of R3 should give one. We'll use spherical coordinates (r, œÜ, Œ∏) to solve
this problem instead of cartesian coordinates (x, y, z). In spherical
coordinates, the volume of a thin slice from the surface of a sphere
of width dŒ∏, height dœÜ, and thickness dr is given by r2 sin œÜ dœÜ dŒ∏ dr.
If you haven't seen spherical coordinates before, don't worry about
this expression too much. The conversion factor r2 sin œÜ is just some
trickery needed to convert the "small piece of volume" d3‚Éór = dx dy dz
to an equivalent small piece of volume in spherical coordinates d3‚Éór =
r2 sin œÜ dœÜ dŒ∏ dr. See P4.20 for the derivation.
We'll split the triple integral into two parts: an integral that de-
pends only on the radius r, and double integral over all angles œÜ and
Œ∏:
ptotal =
ZZZ
R3 |œà(‚Éór)|2 d3‚Éór
=
Z ‚àû
0
Z 2œÄ
0
Z œÄ
0
|œà(r)|2 r2 sin œÜ dœÜ dŒ∏ dr
=
Z ‚àû
0
1
œÄa3 exp(‚àí2r/a) r2 dr
 Z 2œÄ
0
Z œÄ
0
sin œÜ dœÜ dŒ∏


10.8
QUANTUM MECHANICS APPLICATIONS
457
=
Z ‚àû
0
1
œÄa3 exp(‚àí2r/a) r2 dr

(4œÄ)
=
Z ‚àû
0
4
a3 exp
2r
a

r2
|
{z
}
p(r)
dr.
The expression p(r) =
4
a3 exp(‚àí2r/a)r2 describes the probability of
Ô¨Ånding the electron at a distance r from the centre of the nucleus.
We can complete the calculation of total probability by taking the
integral of p(r) from r = 0 to r = ‚àû:
ptotal =
Z ‚àû
0
p(r) dr
= 4
a3
Z ‚àû
0
exp
2r
a

r2 dr
= 1.
The purpose of working through this wave function calculation is to
give you an idea of the complex calculations physicists have to do on
a day to day basis using the wave function formalism. In compari-
son, the matrix formalism for quantum mechanics is much simpler,
involving only basic linear algebra calculations.
10.8
Quantum mechanics applications
What can we do using quantum physics that we couldn't do using
classical physics? What can we compute with qubits that we couldn't
compute with bits? After learning the quantum formalism, it's time
to see if it's useful for something or not. Below we present some areas
of physics and computer science that wouldn't exist without the laws
of quantum mechanics.
Particle physics
The quantum mechanics formalism we discussed above is not well
suited for describing the behaviour of high energy particles. The best
current model for to describing high energy physics is called quantum
Ô¨Åeld theory, and models each fundamental particle as a disturbance in
a particle Ô¨Åeld. Particles (disturbances) and antiparticles (negatives
of disturbances) can be created and destroyed, and interact with other
particle Ô¨Åelds. It's a bit like chemistry where diÔ¨Äerent combinations of
atoms get transformed into other combinations of atoms, but instead
of atoms we have elementary particles like quarks and leptons. The
same way Mendeleev's periodic table gives us a catalogue of all the

458
QUANTUM MECHANICS
available atoms, the Standard model of particle physics gives us the
catalogue of elementary particles, which combine to form particles
like protons and neutrons and transform into other combinations of
particles in high-energy physics experiments.
As the name suggests, high energy physics is one extreme of an en-
ergy continuum. At the low energy scale of the continuum the rules of
chemistry rule. Chemical reactions describe how molecules transform
into other molecules, which basically represent diÔ¨Äerent ways elec-
trons can be shared between a bunch of atoms. At higher energies,
atoms get "stripped" of their electrons because they have so much en-
ergy that they're not bound to the nucleus anymore. At this point the
laws of chemistry stop being relevant, since electrons are freely Ô¨Çying
around and the molecules are no longer together. Exit chemistry; en-
ter nuclear physics. Nuclear physics studies the diÔ¨Äerent combinations
of protons and neutrons that can form the nuclei of diÔ¨Äerent atoms.
A nuclear reaction is like a chemical reaction, but instead of chemi-
cal molecules the reactant and products are diÔ¨Äerent types of nuclei.
An example of nuclear reaction is the fusion of two heavy hydrogen
nuclei to form a helium nucleus. At higher energy still, even protons
and neutrons can break apart, and the analysis shifts to interactions
between elementary particles like leptons, bosons, neutrinos, quarks,
and photons. This is the domain of high energy physics.
The basic postulates of quantum mechanics still apply in quan-
tum Ô¨Åeld theory, but the models become more complicated since we
assume even the interactions between particles are quantized. You
can think of the basic quantum mechanics described in this chapter
as learning the alphabet, and quantum Ô¨Åeld theory as studying Shake-
speare, including the invention of new words. Studying quantum Ô¨Åeld
theory requires new math tools like path integrals, new intuitions
like symmetry observations, and new computational tricks like renor-
malization. The essential way of thinking about photons, electrons,
and the interactions between them can obtained by reading Richard
Feynman's short book titled QED, which stands for quantum elec-
trodynamics. In this tiny book, Feynman uses an analogy of a "tiny
clock" attached to each particle to explain the phase eiŒ∏ of its wave
function. Starting from this simple analogy the author works his way
up to explaining concepts from graduate-level quantum Ô¨Åeld theory
like path integrals. I highly recommended this book. It's your chance
to learn from one of the great scientists in the Ô¨Åeld and one of the
best physics teachers of all times.
[ The Standard Model of particle physics ]
https://en.wikipedia.org/wiki/Standard_Model
[ Nuclear fusion is the way energy is generated inside stars ]
https://en.wikipedia.org/wiki/Nuclear_fusion

10.8
QUANTUM MECHANICS APPLICATIONS
459
[BOOK] Richard P. Feynman. QED: The strange theory of light and
matter. Princeton University Press, 2006, ISBN 0691125759.
Solid state physics
Physicists have been on a quest to understand the inner structure
of materials ever since the Ô¨Årst days of physics. Some examples of
applications developed based on this understanding include semicon-
ductors, lasers, photovoltaic batteries (solar panels), light emitting
diodes (LEDs), and many other. These applications all depend on spe-
cially engineered conductivity properties of materials. Indeed, think-
ing about the conductivity of materials can give us a very good picture
of their other properties. We can classify materials into the follow-
ing general groups: insulators, metals, and semi-conductors. These
three categories correspond to materials with diÔ¨Äerent energy band
structure.
Insulators are the most boring type of material. A good example of
an insulator is glass‚Äîjust an amorphous clump of silica (SiO2). The
term glass is used more generally in physics to describe any material
whose molecules are randomly oriented and don't have any speciÔ¨Åc
crystal structure.
Conductors are more interesting.
A hand wavy
explanation of conductivity would be to say the electrons in conduc-
tors like aluminum and copper are "free to move around." Solid state
physics allows for a more precise understanding of the phenomenon.
Using quantum mechanical models we can determine the energy lev-
els that electrons can occupy, and predict how many electrons will be
available to conduct electricity.
Semiconductors are the most interesting type of material since
they can switch between conductive and non-conductive states. The
transistor, the magical invention that makes all electronics possible,
consists of a sandwich of three diÔ¨Äerent types of semiconductors. The
voltage applied to the middle section of a transistor is called the gate
voltage, and it controls the amount of current that can Ô¨Çow through
the transistor. If the gate voltage is set to ON (think 1 in binary) then
semiconducting material is biased so free electrons are available in its
conduction band and current can Ô¨Çow through. If the gate voltage
is set to OFF (think 0 in binary) then conduction band is depleted
and the transistor won't conduct electricity. The improvements in
semiconductor technologies, speciÔ¨Åcally the ability to pack billions of
transistors into a tiny microprocessor chip is behind the computers
revolution that's been ongoing, pretty much since the transistors were
Ô¨Årst commercialized. In summary, no solid state physics, no mobile
phones.
Quantum mechanics is used in all areas of solid state physics, in

460
QUANTUM MECHANICS
fact we could even say "applied quantum physics" would be another
suitable label for the Ô¨Åeld.
[ Simple explanation of the energy band structure and conductivity ]
https://wikipedia.org/wiki/Electrical_resistivity_and_conductivity
Superconductors
Certain materials exhibit surprising physical properties at very low
temperature. When we say "low" temperatures, we mean really low
like ‚àí272‚ó¶C. You'd exhibit surprising properties too if your were
placed in an environment this cold! For example there are regular
conductors with small resistance, and high-end conductors which have
less resistance, and then there are superconductors which have zero
resistance. Superconductors are an example of a purely quantum phe-
nomenon that cannot be explained by classical physics.
Some of the most iconic landmarks of modern scientiÔ¨Åc progress
like magnetic resonance imaging (MRI) machines, and magnetically
levitating bullet trains are made possible by superconductor technol-
ogy. Superconductors oÔ¨Äer zero resistance to electric current, which
means they can support much stronger currents than regular conduc-
tors like copper and aluminum.
[ Superconductivity ]
https://en.wikipedia.org/wiki/Superconductivity
Quantum optics
Classical optics deals with beams of light that contain quintillions of
photons. A quintillion is 1018, which is a lot. For experiments with
this many photons, it is possible to model light beams as a contin-
uous electromagnetic waves and use classical electromagnetic theory
and optics to understand experiments. Quantum optics comes into
play when we perform optics experiments using much fewer photons,
including experiments that involve single photons.
When a single
photon travels through an optical circuit it cannot "split" like a con-
tinuous wave. For example, when a beam of light hits a half-silvered
mirror, we say the beam is "partially" reÔ¨Çected. We can't say the same
thing for a single photon since the photon cannot be split. Instead the
photon goes into a superposition of "passed through" and "reÔ¨Çected"
states, as shown in Figure 10.11 (page 436).
An example of quantum optics eÔ¨Äect is the spontaneous downcon-
version eÔ¨Äect in which a single photon is absorbed by a material and
then reemitted as two photons whose polarization state is entangled:
|Œ®‚àí‚ü©=
1
‚àö
2|H‚ü©|V ‚ü©‚àí
1
‚àö
2|V ‚ü©|H‚ü©.

10.8
QUANTUM MECHANICS APPLICATIONS
461
Because of the properties of the crystal, we know one of the two emit-
ted photons will have horizontal polarization and the other will have
vertical polarization, but we don't know which is which. Such entan-
gled photons can be used as a starting point for other experiments
that use entanglement. Another interesting aspect of quantum optics
are the so called squeezed states that can be detected more accurately
than regular (unsqueezed) photons.
Quantum optics is a Ô¨Åeld of active research. Scientists in academia
and industry study exotic photon generation, advanced photon detec-
tion schemes, and in general explore how photons can be used most
eÔ¨Éciently to carry information.
[ Basic principles in physics of light ]
https://materialford.wordpress.com/introduction-to-research-light/
Quantum cryptography
Performing a quantum measurement on the state |œà‚ü©tends to disturb
the state. From the point of view of experimental physics, this is an
obstacle since it gives us a limited one-time access to the quantum
state |œà‚ü©, making studying quantum states more diÔ¨Écult. From the
point of view of cryptography however, the state-disturbing aspect of
quantum measurement is an interesting property. If Alice transmit
a secret message to Bob encoded in the state of a quantum system,
then it would be impossible for an eavesdropper Eve to "listen in"
on the state unnoticed because Eve's measurement will disturb the
state. The BB84 protocol, named after its inventors Charles Bennett
and Gilles Brassard, is based on this principle.
The standard basis Bs = {|0‚ü©, |1‚ü©} and the Hadamard basis Bh =
{|+‚ü©, |‚àí‚ü©} are a mutually unbiased bases, which means basis vector
from one basis lie exactly "in between" the vectors from the other
basis. The use of mutually unbiased bases is central to the security
of the BB84 protocol, which we'll describe in point form:
1. Alice transmits 2n "candidate" bits of secret key to Bob. She
chooses one of the bases Bs or Bh at random when encoding
each bit of information she wants to send. Bob chooses to per-
form his measurement randomly, either in the standard basis
or in the Hadamard basis. The information will be transmit-
ted faithfully whenever Bob happens to pick the same basis as
Alice, which happens about half the time. If the basis used in
Bob' measurement is diÔ¨Äerent from the basis used by Alice for
the encoding, Bob's output will be completely random.
2. Alice and Bob publicly announce the basis they used for each
transmission and discard the bits where diÔ¨Äerent bases were

462
QUANTUM MECHANICS
used. This leaves Alice and Bob with roughly n candidate bits
of secret key.
3. Alice and Bob then publicly reveal Œ±n of the candidate bits,
which we'll call the check bits. Assuming the quantum commu-
nication channel between Alice and Bob does not introduce any
noise, Alice and Bob's copies of the check bits should be iden-
tical, since the same basis was used. If too many of these check
bits disagree, they abort the protocol.
4. If the Œ±n check bits agree, then Alice and Bob can be sure the
remaining (1 ‚àíŒ±)n bits they share are only known to them.
Consider what happens if the eavesdropper Eve tries introduce herself
between Alice and Bob by measuring the quantum state |œà‚ü©send by
Alice, then forwarding to Bob the post measurement state |œà‚Ä≤‚ü©. Eve
is forced to choose a basis for the measurement she performs and her
measurement will disturb the state |œà‚ü©whenever she picks a basis
diÔ¨Äerent from the one used by Alice. Since |œà‚Ä≤‚ü©Ã∏= |œà‚ü©Alice and Bob
will be able to detect this eavesdropping has occurred because some
of the check bits they compare in Step 3 will disagree. The BB84 does
not use quantum mechanics to prevent eavesdropping, but rather gives
Alice and Bob the ability to detect when an eavesdropper is present.
The BB84 protocol established the beginning of a new Ô¨Åeld at the
intersection of computer science and physics which studies quantum
key distribution protocols. The Ô¨Åeld has developed rapidly from the-
ory, to research, and today there are even commercial quantum cryp-
tography systems. It's interesting to compare the quantum crypto
with the public key cryptography systems discussed in Section 8.9.
The security of the RSA public-key encryption is based on the compu-
tational diÔ¨Éculty of factoring large numbers. The security of quantum
cryptography is guaranteed by the laws of quantum mechanics.
[ Bennett-Brassard quantum cryptography protocol from 1984 ]
en.wikipedia.org/wiki/BB84
[ Using quantum phenomena to distribute secret key ]
https://en.wikipedia.org/wiki/Quantum_key_distribution
Quantum computing
The idea of quantum computing‚Äîin one form or another‚Äîhas been
around since the early days of quantum physics. Richard Feynman
originally proposed the idea of a quantum simulator, which is a quan-
tum apparatus that can simulate the quantum behaviour of another
physical system. Imagine a physical system that is diÔ¨Écult and expen-
sive to build, but whose behaviour can be simulated on the quantum

10.8
QUANTUM MECHANICS APPLICATIONS
463
simulator. A quantum simulator will be much better at simulating
quantum phenomena than any simulation of quantum physics on a
classical computer.
Another possible application of a quantum simulator would be to
encode classical mathematical optimization problems as constraints in
a quantum system, then use let the quantum evolution of the system
"search" for good solutions. Using a quantum simulator in this way, it
might be possible to Ô¨Ånd solutions to a optimization problems much
faster than any classical optimization algorithm would able to.
Once computer scientists started thinking about quantum com-
puting, they were not satisÔ¨Åed just with studying only optimization
problems, but set out to qualify and quantify all the computational
tasks that are possible with qubits. A quantum computer stores and
manipulates information that is encoded as quantum states. It is pos-
sible to perform certain computational tasks on in the quantum world
much faster than on any classical computer.
Quantum circuits
Computer scientists like to think quantum com-
puting tasks as series of "quantum gates," in analogy with the logic
gates used to construct classical computers. Figure 10.20 shows an
example of a quantum circuit that takes two qubits as inputs and
produces two qubits as outputs.
H
‚áî
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
‚àö
2
0
0
1
‚àö
2
0
1
‚àö
2
1
‚àö
2
0
1
‚àö
2
0
0
‚àí1
‚àö
2
0
1
‚àö
2
‚àí1
‚àö
2
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Figure 10.20: A quantum circuit that applies the Hadamard gate to the
Ô¨Årst qubit then applies the control-NOT gate from the Ô¨Årst qubit to the
second qubit.
This circuit in Figure 10.20 is the combination of two quantum gates.
The Ô¨Årst operation is to apply the Hadamard gate H on the Ô¨Årst qubit,
leaving the second qubit untouched. This operation is equivalent to
multiplying the input state by the matrix H‚äó1. The second operation
is called the control-NOT (or control-X) gate, which applies the X
operator (also known as the NOT gate) to the second qubit whenever
the Ô¨Årst qubit is |1‚ü©, and does nothing otherwise:
CNOT(|0‚ü©‚äó|œï‚ü©) = |0‚ü©‚äó|œï‚ü©,
CNOT(|1‚ü©‚äó|œï‚ü©) = |1‚ü©‚äóX|œï‚ü©.

464
QUANTUM MECHANICS
The circuit illustrated in Figure 10.20 can be used to create entan-
gled quantum states. If we input the quantum state |00‚ü©‚â°|0‚ü©‚äó|0‚ü©
into the circuit, we obtain the maximally entangled state |Œ¶+‚ü©‚â°
1
‚àö
2 (|00‚ü©+ |11‚ü©) as output:
|0‚ü©
H
|0‚ü©
|Œ¶+‚ü©
Figure 10.21: Inputting |0‚ü©‚äó|0‚ü©into the circuit produces an EPR state
|Œ¶+‚ü©‚â°
1
‚àö
2 (|00‚ü©+ |11‚ü©) on the two output wires of the circuit.
Quantum measurements can also be represented in quantum circuits.
Figure 10.22 shows how a quantum measurement in the standard basis
is represented.
Œ±|0‚ü©+ Œ≤|1‚ü©
Bs
0 or 1
Figure 10.22: Measurement in the standard basis Bs = {|0‚ü©, |1‚ü©}. The
projectors of this measurement are Œ†0 = |0‚ü©‚ü®0| and Œ†1 = |1‚ü©‚ü®1|.
We use double lines to represent the Ô¨Çow of classical information in
the circuit.
Quantum registers
Consider a quantum computer with a single
register |R‚ü©that consists of three qubits. The quantum state of this
quantum register is a vector in C2 ‚äóC2 ‚äóC2:
|R‚ü©=
(Œ±1|0‚ü©+ Œ≤1|1‚ü©) ‚äó(Œ±2|0‚ü©+ Œ≤2|1‚ü©) ‚äó(Œ±3|0‚ü©+ Œ≤3|1‚ü©) ,
where the tensor product ‚äóis used to combine the quantum states
of the individual qubits. We'll call this the "physical representation"
of the register and use 0-based indexing for the qubits. Borrowing
language from classical computing, we'll call the rightmost qubit the
least signiÔ¨Åcant qubit, and the leftmost qubit the most signiÔ¨Åcant
qubit.
The tensor product of three vectors with dimension two is a vector
with dimension eight. The quantum register |R‚ü©is thus a vector in an
eight-dimensional vector space. The quantum state of a three-qubit
register can be written as:
|R‚ü©= a0|0‚ü©+ a1|1‚ü©+ a2|2‚ü©+ a3|3‚ü©+ a4|4‚ü©+ a5|5‚ü©+ a6|6‚ü©+ a7|7‚ü©,

10.8
QUANTUM MECHANICS APPLICATIONS
465
where ai are complex coeÔ¨Écients. We'll call this eight-dimensional
vector space the "logical representation" of the quantum register. Part
of the excitement about quantum computing is the huge size of the
"logical space" where quantum computations take place. The logi-
cal space of a 10-qubit quantum register has dimension 210 = 1024.
That's 1024 complex coeÔ¨Écients we're talking about. That's a big
state space for just a ten-qubit quantum register. Compare this with
a 10-bit classical register which can store just one of 210 = 1024 dis-
crete values.
We cannot discuss quantum computing further but I still want to
show you some examples of single-qubit quantum operations and their
eÔ¨Äect on the tensor product space, so you'll have an idea of what kind
of craziness is possible.
Quantum gates
Let's say you've managed to construct a quantum
register, what can you do with it? Recall the single-qubit quantum
operations Z, X, and H we described earlier. We can apply any of
these operations on individual qubits in the quantum register. For
example applying the X = [ 0 1
1 0 ] gate to the Ô¨Årst (most signiÔ¨Åcant)
qubit of the quantum register corresponds to the following quantum
operation:
X
‚áî
X ‚äó1 ‚äó1 =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
The operator X ‚äó1‚äó1 "toggles" the Ô¨Årst qubit in the register while
leaving all other qubits unchanged.
Yes, I know the tensor product operation is a bit crazy, but that's
the representation of composite quantum systems and operations so
you'll have to get used to it. What if we apply the X operator applied
on the middle qubit?
X
‚áî
1 ‚äóX ‚äó1 =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.

466
QUANTUM MECHANICS
Compare the structure of the operators X ‚äó1‚äó1 and 1‚äóX ‚äó1. Do
you see how the action of X aÔ¨Äect diÔ¨Äerent dimensions in the tensor
product space C8?
To complete the picture, let's also see the eÔ¨Äects of applying the
X gate to the third (least signiÔ¨Åcant) qubit in the register:
X
‚áî
1 ‚äó1 ‚äóX =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Crazy stuÔ¨Ä, right? Don't worry you'll get used to the whole space-
within-a-space structure with time.
Okay so what?
We discussed quantum registers and quantum operations, but we still
haven't said what quantum computing is good for, if anything. Quan-
tum computers give us access to a very large state space. The funda-
mental promise of quantum computing is that a small set of simple
quantum operations (quantum gates) can be used to perform interest-
ing computational tasks. Sure it's diÔ¨Écult to interact with quantum
systems (state preparation and measurement), but damn the space is
big so it's worth checking out what kind of computing you can do in
there. It turns out there are already several useful things you can do
using a quantum computer. The two Ô¨Çagship applications for quan-
tum computing are Grover's search algorithm and Shor's factoring
algorithm.
Grover's search algorithm
Suppose you're given an unsorted list
of n items and you want to Ô¨Ånd a particular item in that list. This
is called and unstructured search problem. This is a hard problem
to solve for a classical computer since the algorithm would have to
go through the entire list, which will take on the order of n steps. In
contrast, the unstructured problem can be solved in roughly ‚àön steps
on a quantum computer using Grover's algorithm.
This "quantum speedup" for doing unstructured search is a nice-
to-have, but the real money maker for the Ô¨Åeld of quantum computing
has been Shor's factoring algorithm for factoring biprime numbers.

10.8
QUANTUM MECHANICS APPLICATIONS
467
Shor's factoring algorithm
The security of the RSA crypto sys-
tem we discussed in Section 8.9 is based on the assumption that fac-
toring large biprime numbers is computationally intractable. Given
the product de of two unknown prime numbers d and e, it is compu-
tationally diÔ¨Écult to Ô¨Ånd the factors e and d. No classical algorithm
is known that can factor large numbers; even the letter agencies will
have a hard time Ô¨Ånding the factors of de when d and e are chosen to
be suÔ¨Éciently large prime numbers. Thus, if an algorithm that could
quickly factor large numbers existed, attackers would be able to break
many of current security systems. Shor's factoring algorithms Ô¨Åts the
bill, theoretically speaking.
Shor's algorithm reduces the factoring problem to the problem
of period Ô¨Ånding, which can be solved eÔ¨Éciently using the quantum
Fourier transform.
Shor's algorithm can factor large numbers eÔ¨É-
ciently (in polynomial time). This means RSA encryption would be
easily "hackable" using Shor's algorithm running on suÔ¨Éciently large,
and suÔ¨Éciently reliable quantum computer. The letter agencies really
got really excited about this development since they'd love to be able
to hack all of present-day cryptography. Can you imagine not being
able to login securely to any website because Eve is listening in and
hacking your crypto using her quantum computer?
Shor's algorithms is currently only a theoretical concern. Despite
considerable eÔ¨Äort, no quantum computers exist today that would
allow us to manipulate quantum registers with thousands of qubits.
It seems that the letter agencies didn't get the memo on this, and
they think that quantum computers will be easy to build. Perhaps
quantum researchers funded by various military agencies deserve a
Nobel peace prize for all the funds they have diverted from actual
weapons research and into research on fundamental science.
Discussion
Quantum computing certainly opens some interesting possibilities,
but we shouldn't get ahead of ourselves and imagine a quantum com-
puting revolution is just around the corner. As with startup ideas,
the implementation is what counts‚Äînot the idea. The current status
of quantum computing as a technology is mixed. On one hand certain
quantum algorithms performed in logical space are very powerful; on
the other hand the diÔ¨Éculty of building a quantum computer is not
to be missunderestimated.
It's also important to keep in mind that quantum computers are
not better at solving arbitrary computational problems. The problems
for which there exists a quantum speedup have a particular structure,
which can be tackled with a choreographed pattern of constructive and

468
QUANTUM MECHANICS
destructive interference in quantum registers. Not all computationally
hard problems have this structure. Quantum computing technology is
at a cross road: it could turn out to be a revolutionary development,
or it could turn out that building a large-scale quantum computer is
too much of an engineering challenge. Basically, don't go out thinking
quantum is the big new thing. It's cool that we can do certain tasks
faster on a quantum computer, but don't throw out classical computer
just yet.
Even if the quest to build a quantum computer doesn't pan out in
the end, we'll have learned many interesting things about fundamental
physics along the way. Indeed learning about the fundamental nature
quantum information is more scientiÔ¨Åcally valuable than focussing on
how to hack into people's email. In the next section we'll discuss an
example of a fundamental results of quantum information science.
Quantum teleportation
Figure 10.23 illustrates one of the sur-
prising aspects of quantum information: we can "teleport" a quantum
state |œà‚ü©from one lab to another using one maximally entangled state
shared between Alice's and Bob's labs and two bits of classical com-
munication from Alice to Bob. The quantum state |œà‚ü©starts oÔ¨Äin
the Ô¨Årst qubit of the register, which we assume is held by Alice, and
ends up in the third qubit which is Bob's lab. We can express the the
quantum teleportations protocol as a quantum circuit.
|œà‚ü©1
H
X Z
|œà‚ü©3
|Œ¶+‚ü©23
Figure 10.23: The Ô¨Årst two qubits are assumed to be in Alice's lab. The
state of the Ô¨Årst qubit |œà‚ü©1 is transferred into the third qubit |œà‚ü©3, which is
held by Bob. We say œà is "teleported" from Alice's lab to Bob's lab because
only classical information was used to transfer the state. The information
about the results of the two measurements are classical bits, which are easy
to send over.
The quantum teleportation protocol requires that Alice and Bob pre-
share a maximally-entangled state |Œ¶+‚ü©. This could be achieved if
Alice and Bob get together in a central location and produce an en-
tangled state using the circuit shown in Figure 10.21. Alice and Bob
then bring their respective halves of the entangled state to their labs.
Note Bob's lab could be very far away from Alice's lab, in another
building, in another city, or even at the other end of the world.

10.8
QUANTUM MECHANICS APPLICATIONS
469
The initial state for the quantum teleportation protocol is
|œà‚ü©1 ‚äó|Œ¶+‚ü©23 =

Œ±|0‚ü©1 + Œ≤|1‚ü©1

‚äó

1
‚àö
2|00‚ü©23 +
1
‚àö
2|11‚ü©23

.
Alice has two qubits in her lab, the state |œà‚ü©1 = Œ±|0‚ü©1 + Œ≤|1‚ü©1 and
half of the entangled state, Bob has the third qubit, which is the
other half of the entangled state.
At the end of the teleportation
protocol the information about the state œà will appear in Bob's lab:
|œà‚ü©3 = Œ±|0‚ü©3 + Œ≤|1‚ü©3.
Without quantum communication it seems impossible for Alice to
communicate the coeÔ¨Écients Œ± and Œ≤ to Bob. The pre-shared entan-
glement between Alice and Bob somehow enables this feat. As soon
as Alice performs the measurement of her two qubits, the quantum
information about the sate œà becomes available in Bob's lab, up to a
"correction factor" of 1, X, Z, or X followed by Z, that Bob has to
apply on the third qubit. Bob will not have the state information until
he learns which of the four recovery operations he must perform. The
"which correction" information can be transmitted by classical means:
Alice can shout the result to Bob if he's next door, tell him the results
on the phone, or send him a text message. After applying the needed
recovery gate(s), Bob ends up with the state |œà‚ü©3 = Œ±|0‚ü©3 + Œ≤|1‚ü©3.
We can describe this protocol as "quantum teleportation" because
Alice seems to "beam up" the state to Bob by sending him only clas-
sical information. That's true, but the more interesting part is the
pre-shared entanglement they used, which really makes this work. I
have complained enough about science journalists by this point in the
chapter, so I won't repeat myself, but you can imagine they had a
Ô¨Åeld trip with this one.
The need for pre-shared entanglement |Œ¶+‚ü©between Alice and Bob
is analogous to how Alice and Bob needed to pre-share a secret key ‚Éók in
order to use the one-time pad encryption protocol. Indeed, pre-shared
entangled states are a prime resource in quantum information science.
The superdense coding protocol is another surprising application of
quantum entanglement. Using this protocol Alice can communicate
two bits of classical information to Bob by sending him a single qubit
and consuming one pre-shared entangled state.
Links
[ Quantum simulators and practical implementations ]
https://en.wikipedia.org/wiki/Quantum_simulator
[ Some data about the diÔ¨Éculty of RSA factoring ]
https://en.wikipedia.org/wiki/RSA_numbers

470
QUANTUM MECHANICS
[ An introduction to quantum computing ]
http://arxiv.org/abs/0708.0261v1/
[ Video tutorials on quantum computing by Michael Nielsen ]
http://michaelnielsen.org/blog/quantum-computing-for-the-determined/
[ Grover's algorithm for unstructured search ]
https://en.wikipedia.org/wiki/Grover's_algorithm
[ Shor's algorithm for factoring biprime integers ]
https://en.wikipedia.org/wiki/Shor's_algorithm
Quantum error correcting codes
Quantum states are Ô¨Ånicky things. Every interaction that a qubit
has with its environment corrupts the quantum information that it
stores. In the previous section we talked about quantum computing
in the abstract, assuming the existence of an ideal noiseless quantum
computer. The real world is a noisy place, so constructing a practical
quantum computer in the presence of noise is a much more diÔ¨Écult
challenge.
Recall that errors caused by noise were also a problem for classical
computers, and we solved that problem using error correcting codes.
Can we use error correcting codes on quantum computers too? Indeed
it's possible to use quantum error correcting codes to defend against
the eÔ¨Äect of quantum noise. Keep in mind, quantum error correcting
codes are more complicated to build than their classical counterparts,
so it's not an "obvious" thing to do, but it can be done.
We don't want to go into too much details, but it's worth pointing
out the following interesting fact about quantum error correction.
Building quantum error correcting codes that can defend agains a
Ô¨Ånite set of errors is suÔ¨Écient to defend against all possible types
of errors. The use of quantum error correcting schemes is analogous
to the classical error correcting schemes we saw in Section 8.10. We
encode k qubits of data that we want to protect from noise into a
larger n-qubit state. The encoded state can support some number of
errors before losing the data. The error correction procedure involves
a syndrome measurement on a portion of the state, and "correction"
operators applied to the remaining part. I encourage your to follow
the links provided below to learn more about the topic.
Building reliable "quantum gates" is a formidably diÔ¨Écult task
because of the fundamental diÔ¨Éculty of protecting qubits from noise,
and at the same time enabling quantum operations and strong in-
teractions between qubits. It is the author's opinion that Feynman's
original idea of building quantum simulators for physical systems will
be the Ô¨Årst useful applications in quantum computing.

10.8
QUANTUM MECHANICS APPLICATIONS
471
[ More on quantum error correcting codes ]
https://en.wikipedia.org/wiki/Quantum_error_correction
Quantum information theory
Classical information theory studies problems like the compression
of information and the transmission of information through noisy
communication channels. Quantum information theory studies the
analogous problems of compression of quantum information and the
communication over noisy quantum channels.
The appearance of the word "theory" in "quantum information the-
ory" should give you a hint that this is mostly a theoretical area of
research in which problems are studied in the abstract. The main re-
sults in information theory are abstract theorems that may not have
direct bearing on practical communication scenarios. Applications of
quantum information theory are still far into the future, but that's
how it is with theory subjects in general. The classical information
theory theorems proved in the 1970s probably looked like "useless the-
ory" too, but these theorems serve as the basis of all modern wireless
communications.
Perhaps 40 years form now, the purely theoreti-
cal quantum information theorems of today will be used to solve the
practical communication problems of the future.
Current eÔ¨Äorts in quantum information theory aim to established
capacity results for quantum channels. Some of the existing results
are directly analogous to classical capacity results. Other problems
in quantum information theory, like the use of entanglement-assisted
codes, have no classical counterparts and require completely new way
of thinking about communication problems. The book by Wilde is an
excellent guide to the Ô¨Åeld.
Recently quantum theory has been applied to novel communica-
tion systems, and there is a growing interest from industry to develop
applications that push optical communication channels to their the-
oretical eÔ¨Éciency bounds. Essentially, quantum networks are being
invented in parallel with quantum computers, so when we Ô¨Ånally build
quantum computers, we'll be able to connect them together, presum-
ably so they can share funny cat videos. What else?
[BOOK] Mark M. Wilde. From Classical to Quantum Shannon The-
ory, Cambridge, ISBN 1107034256, arxiv.org/abs/1106.1445.
Conclusion
Thank you for sticking around until the end.
I hope this chapter
helped you see the basic principles of quantum mechanics and de-
mystiÔ¨Åed some of the sensationalist aspects of the quantum world.

472
QUANTUM MECHANICS
There's nothing too counterintuitive in quantum mechanics, it's just
linear algebra, right? With this chapter, I wanted to bring you into
the fold about this fascinating subject. Above all, I hope you'll never
let any science journalist ever mystify you with a hand-wavy quan-
tum mechanics story. I also expect you to call bullshit on any writer
that claims quantum mechanics is related to your thought patterns,
or‚Äîmore ridiculously still‚Äîthat your thoughts cause reality to exist.
It could be, but prove it!
I would like to conclude with an optimistic outcome about what is
to come. A couple of hundred years ago quantum mechanics was seen
as a foreign thing not to be trusted, but with time physicists developed
good models, found better ways to explain things, wrote good books,
so that now the subject is taught to undergraduate physics students.
This gives me hope for humanity that we can handle even the most
complex and uncertain topics, when we put our minds into it.
Today we face many complex problems like consolidated corporate
control of innovation, cartels, corruption, eroding democratic govern-
ment systems, the militarization of everything, and conÔ¨Çicts of ide-
ologies. We have Sunni and Shia brothers Ô¨Åghting each other for no
good reason. We have all kinds of other bullshit divisions between
us. Let's hope that one hundred years from now we have a Ô¨Årm hold
on these aspects of human nature, so we can realize the potential of
every child, born anywhere in the world.

10.9
QUANTUM MECHANICS PROBLEMS
473
10.9
Quantum mechanics problems
P10.1
You work in a quantum computing startup and your boss asks you
to implement the quantum gate Q =
1
‚àö
2[ 1 1
1 1 ]. Can you do it?
Hint: Recall the requirements for quantum gates.
P10.2
The Hadamard gate is deÔ¨Åned as H =
1
‚àö
2
 1
1
1 ‚àí1

. Compute the
eÔ¨Äect of the operator HH on the elements of the standard basis {|0‚ü©, |1‚ü©}.
P10.3
Specifying an arbitrary vector Œ±|0‚ü©+ Œ≤|1‚ü©‚ààC2 requires four pa-
rameters: the real and imaginary parts of Œ± and Œ≤. Thus one might think
that quantum states in C2 have four degrees of freedom. However, the unit-
length requirement and the fact that global phase of a qubit can be ignored
correspond to additional constraints that reduce the number of degrees of
freedom. How many parameters are required to specify a general quantum
state |œà‚ü©‚ààC2?
P10.4
We can write any qubit using only two real parameters
|œà‚ü©= Œ±|0‚ü©+
p
1 ‚àíŒ±2eiœï|1‚ü©,
where Œ± ‚ààR and œï ‚ààR. What are the ranges of values for Œ± and œï such
that all qubits can be represented?
P10.5
Another choice of parametrization for qubits is to use two angles
Œ∏ and œï:
|œà‚ü©= cos(Œ∏/2) |0‚ü©+ sin(Œ∏/2) eiœï|1‚ü©.
What are the ranges of values for Œ∏ and œï such that all qubits can be
represented?
P10.6
Calculate the joint quantum state of the three qubits at each
step in the quantum teleportation circuit in Figure 10.23. Denote |R0‚ü©=
(Œ±|0‚ü©+ Œ≤|1‚ü©)1 ‚äó|Œ¶+‚ü©23 the initial state, |R1‚ü©the state after the control
swap gate, |R2‚ü©after the H gate, and the Ô¨Ånal state after the correction
operator has been applied is |R3‚ü©.
P10.7
Consider a one-dimensional model of a particle caught between two
ideal mirrors. If we assume the two mirrors are placed at a distance of 1 unit
apart, the wave function description of the system is œà(x), where x ‚àà[0, 1].
Find the probability of observing x between 0 and 0.1 for the following
wave functions: (a) Uniform distribution on [0, 1], (b) œàb(x) = 2x ‚àí1, (c)
œàc(x) = 6x2 ‚àí6x + 1.
P10.8
Show that the functions œàb(x) = 2x ‚àí1 and œàc(x) = 6x2 ‚àí6x + 1
are orthogonal with respect to the inner product ‚ü®f, g‚ü©=
R 1
0 f(x)g(x) dx.


End matter
Conclusion
By surviving the linear algebra math covered in this book you've
proven you can handle abstraction. Good job! That's precisely the
skill needed for understanding more advanced math concepts, building
scientiÔ¨Åc models, and proÔ¨Åting from useful applications. Understand-
ing concepts like dimensions, orthogonality, and length in abstract
vector spaces is very useful. Congratulations on your Ô¨Årst steps to-
wards mathematical enlightenment.
Let's review where we stand in terms of math modelling tools. The
Ô¨Årst step when learning math modelling is to understand basic math
concepts such as numbers, equations, and functions f : R ‚ÜíR. Once
you know about functions, you can start to use diÔ¨Äerent formulas f(x)
to represent, model, and predict the values of real-world quantities.
Using functions is the Ô¨Årst modelling superpower conferred on people
who become knowledgeable in math. Mathematical models emerge
as a naturally-useful common core for all of science. For example, by
understanding the properties of the function f(x) = Ae‚àíx/B in the
abstract will enable you to describe the expected number of atoms re-
maining in a radioactive reaction N(t) = Noe‚àíŒ≥t, predict the voltage
of a discharging capacitor over time v(t) = Voe‚àí
t
RC , and understand
the exponential probability distribution pX(x) = Œªe‚àíŒªx.
The second step toward math modelling enlightenment is to gen-
eralize the inputs x, the outputs y, and functions f to other contexts.
In linear algebra, we studied functions T : Rn ‚ÜíRm, which are linear:
T(Œ±‚Éóx1 + Œ≤‚Éóx2) = Œ±T(‚Éóx1) + Œ≤T(‚Éóx2).
The linear property enables us to calculate things, solve equations
involving linear transformations, and build models with interesting
structure (recall the applications discussed in Chapter 8). The math-
ematical structure of a linear transformation T : Rn ‚ÜíRm is faith-
fully represented as a multiplication by a matrix MT ‚ààRm√ón. The
notion of matrix representations (T ‚áîMT ) played a central role in
475

476
END MATTER
this book. Seeing the parallels and similarities between the abstract
math notion of a linear transformation and its concrete representation
as a matrix is essential for the second step of math enlightenment.
If you didn't skip the sections on abstract vector spaces, you also
know about the parallels between the vector space R4, and the ab-
stract vector spaces of third-degree polynomials a0+a1x+a2x2+a3x3
and 2√ó2 matrices
 a b
c d

. This is another step on your way up the lad-
der of abstraction, though I don't think it counts as a third step since
there is no new math‚Äîjust some basic observations and cataloguing
of math objects that have vector structure.
The computational skills you learned in Chapter 4 are also use-
ful, but you probably won't be solving any problems by hand using
row operations from this point forward, since computers outclass you
many-to-one on matrix arithmetic tasks. Good riddance. Until now
you did all the work and used SymPy to check your answers, from now
on you can let SymPy do all the calculations and your job will be to
chill.
It was a great pleasure for me to be your guide through the topics
of linear algebra. I hope you walk away from this book with a solid
understanding of how the concepts of linear algebra Ô¨Åt together. In
the introduction to the book, I likened linear algebra to playing with
legos. Indeed, if you feel comfortable with manipulating vectors and
matrices, performing change-of-basis operations, and using the ma-
trix decomposition techniques to see inside matrices, you'll be able to
"play" with all kinds of complex systems and problems. For example,
consider the linear transformation T that you want to apply to an
input vector ‚Éóv. Suppose the linear transformation T is most easily
described in the basis B‚Ä≤, but the vector ‚Éóv is expressed with respect
to the basis B. "No problem," you say, and proceed straight away
to build the following chain of matrices that will compute the output
vector ‚Éów:
[‚Éów]B =
[1]
B
B‚Ä≤
[AT ]
B‚Ä≤
B‚Ä≤
[1]
B‚Ä≤
B[‚Éóv]B.
Do you see how matrices and vectors Ô¨Åt together neatly like legos?
I can't tell what the next step on your journey will be.
With
linear algebra modelling skills under your belt, you have a thousand
doors open to you, and you must explore and choose. Will you go
on to learn how to code and start a software company? Maybe you
can use your analytical skills to go to wall street and destroy the
System from the inside?
Or perhaps you'll apply your modelling
skills to revolutionize energy generation, thus making human progress
sustainable. Regardless of your choice of career, I want you to stay
on good terms with math and keep learning when you have a chance.
Good luck with your studies!

SOCIAL STUFF
477
Social stuÔ¨Ä
Be sure to send me feedback if you liked or hated this book. Feedback
is crucial so I know how to adjust the writing, the content, and the
attitude of the book for future readers. I'm sure you found some parts
of the book that were not explained well or otherwise weak. Please
take the time to drop me a line and let me know. This way I'll know
which sections to Ô¨Åx in the next revision. You can reach me by email
at ivan.savov@gmail.com.
Another thing you could do to help me is to write a review of the
book on amazon.com, goodreads, google books, or otherwise spread
the word about the No bullshit guide series of books. Talk to your
friends and let them in on the math buzz.
If you want to know what Minireference Co. has been up to,
check out the company blog at minireference.com/blog/. The blog
is a mix of 30% technology talk, 50% book business talk, and 20%
announcements. Checking the blog is the easiest way to check the
progress of the revolution in the textbook industry.
You can also
connect via twitter @minireference and facebook fb.me/noBSguide.
Acknowledgements
This book would not have been possible without the help of my par-
ents, and my teachers: Paul Kenton, Benoit Larose, Ioannis Psaromiligkos,
and Patrick Hayden. It's from them that I learned to trick students
into learning advanced material. Many thanks also to David Avis,
Arlo Breault, Michael Hilke, Igor Khavkine, Juan Pablo Di Lelle, Ivo
Panayotov, and Mark M. Wilde for their support with the book.
The errorlessness and consistency of the text would not have been
possible without the help of my editor Sandy Gordon, who did a
great job at polishing the text until it Ô¨Çowed. Truly no bullshit is
allowed into the book when Sandy's on watch. Many thanks to Polina
Anis'kina who helped me to create the problem sets for the book.
General linear algebra links
Below are some useful links for you to learn more about linear algebra.
We covered a lot of ground, but linear algebra is endless. Don't sit
on your laurels now having completed this book and the problem sets
and think you're the boss. You have all the tools, but you need to
practice using them. Try reading up on the same topics from some
other sources. See if you can do the problem sets in another linear

478
END MATTER
algebra textbook.
Try to make some use of linear algebra in the
coming year to solidify your understanding of the material.
[ Video lectures of Gilbert Strang's Linear algebra class at MIT ]
http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010
[ Lecture notes by Terrence Tao ]
http://www.math.ucla.edu/~tao/resource/general/115a.3.02f/
[ Wikipedia overview on matrices ]
https://en.wikipedia.org/wiki/Matrix_(mathematics)
[ Linear algebra wikibook (with solved problems) ]
https://en.wikibooks.org/wiki/Linear_Algebra
[ Proofs involving linear algebra ]
http://proofwiki.org/wiki/Category:Linear_Transformations
http://proofwiki.org/wiki/Category:Linear_Algebra
http://proofwiki.org/wiki/Category:Matrix_Algebra

Appendix A
Answers and solutions
Chapter 1 solutions
Answers to exercises
E1.1 x = 2, y = 3.
E1.2 x = 5, y = 6, and z = ‚àí3.
E1.3 p = 7 and q = 3.
E1.4 x = 2, y = 1.
Answers to problems
P1.1 x = ¬±4.
P1.2 x = A cos(œât + œÜ).
P1.3 x =
ab
a+b.
P1.4 (1) 2.2795.
(2) 1024. (3) ‚àí8.373. (4) 11.
P1.5 (1) 3
4 . (2) ‚àí141
35 . (3) 3 23
32 .
P1.6 (1) c. (2) 1.
(3) 9a
b . (4) a. (5)
b
ac . (6) x2 +ab.
P1.7 (1) x2 +(a‚àíb)x‚àíab. (2) 2x2 ‚àí7x‚àí15.
(3) 10x2+31x‚àí14.
P1.8 (1) (x‚àí4)(x+2). (2) 3x(x‚àí3)(x+3). (3) (x+3)(6x‚àí7).
P1.9 (1) (x ‚àí2)2 + 3. (2) 2(x + 3)2 + 4. (3) 6
 x + 11
12
2 ‚àí625
24 .
P1.10 $0.05.
P1.11 13 people, 30 animals.
P1.12 5 years later.
P1.13 girl = 80 nuts, boy
= 40 nuts.
P1.14 Alice is 15.
P1.15 18 days.
P1.16 After 2 hours.
P1.18
œï =
1+
‚àö
5
2
.
P1.19 x =
‚àí5¬±
‚àö
41
2
.
P1.20 (1) x =
3‚àö
2. (2) x = ( œÄ
2 + 2œÄn)
for n ‚ààZ.
P1.21 No real solutions if 0 < m < 8.
P1.22 (1) ez. (2) x3y15
z3
.
(3)
1
4x4 . (4) 1
4 . (5) ‚àí3. (6) ln(x + 1).
P1.23 œµ = 1.110 √ó 10‚àí16; n = 15.95
in decimal.
P1.24 (1) x ‚àà(4, ‚àû.
(2) x ‚àà[3, 6].
(3) x ‚àà‚àí‚àû, ‚àí1] ‚à™[ 1
2 , ‚àû.
P1.25 For n > 250, Algorithm Q is faster.
P1.26 10 cm.
P1.27 22.52 in.
P1.28 h =
‚àö
3.332 ‚àí1.442 = 3 m.
P1.29 The opposite side has length 1.
P1.30 x =
‚àö
3, y = 1, and z = 2.
P1.31 d =
1800 tan 20‚ó¶‚àí800 tan 25‚ó¶
tan 25‚ó¶‚àítan 20‚ó¶
, h =
1658.46 m.
P1.32 x =
2000
tan 24‚ó¶.
P1.33 x = tan Œ∏
‚àö
a2 + b2 + c2.
P1.34
a =
‚àö
3, A‚ñ≥= 3
‚àö
3
4 .
P1.35 sin2 Œ∏ cos2 Œ∏ = 1‚àícos 4Œ∏
8
.
P1.36 P8 = 16 tan(22.5‚ó¶),
A8 = 8 tan(22.5‚ó¶).
P1.37 c =
a sin 75‚ó¶
sin 41‚ó¶
‚âà14.7.
P1.38 (a) h = a sin Œ∏.
(b) A =
1
2 ba sin Œ∏.
(c) c =
p
a2 + b2 ‚àí2ab cos(180 ‚àíŒ∏).
P1.39 B = 44.8‚ó¶,
C = 110.2‚ó¶.
c =
a sin 110.2‚ó¶
sin 25‚ó¶
‚âà39.97.
P1.40 v = 742.92 km/h.
P1.41
1.33 cm.
P1.42 x = 9.55.
P1.43
1
2 (œÄ42 ‚àíœÄ22) = 18.85 cm2.
P1.44
‚Ñìrope = 7.83 m.
P1.45 Arect = 5c + 10.
P1.46 Vbox = 1.639 L.
P1.47
Œ∏ = 120‚ó¶.
P1.48 R
r = 1‚àísin 15‚ó¶
sin 15‚ó¶
= 2.8637.
P1.49 7 cm.
P1.50 V = 300 000 L.
P1.51 315 000 L.
P1.52 4000 L.
P1.53 d =
1
2 (35 ‚àí5
‚àö
21).
P1.54 A rope
479

480
ANSWERS AND SOLUTIONS
of length
‚àö
2‚Ñì.
P1.55 20 L of water.
P1.56 h = 7.84375 inches.
P1.57
1 + 2 + ¬∑ ¬∑ ¬∑ + 100 = 50 √ó 101 = 5050.
P1.58 x = ‚àí2 and y = 2.
P1.59
x = 1, y = 2, and z = 3.
P1.60 $112.
P1.61 20%.
P1.62 $16501.93.
P1.64 0.14 s.
P1.65 œÑ = 34.625 min, 159.45 min.
P1.66 V (0.01) = 15.58 volts.
V (0.1) = 1.642 volts.
Solutions to selected problems
P1.5 For (3), 1 3
4 + 1 31
32 = 7
4 + 63
32 = 56
32 + 63
32 = 119
32 = 3 23
32 .
P1.9 The solutions for (1) and (2) are fairly straightforward. To solve (3), we Ô¨Årst
factor out 6 from the Ô¨Årst two terms to obtain 6(x2+ 11
6 x)‚àí21. Next we choose half
of the coeÔ¨Écient of the linear term to go inside the square and add the appropriate
correction to maintain equality: 6[x2 + 11
6 x] ‚àí21 = 6[(x + 11
12 )2 ‚àí
  11
12
2] ‚àí21.
After expanding the rectangular brackets and simplifying, we obtain the Ô¨Ånal
expression: 6
 x + 11
12
2 ‚àí625
24 .
P1.11 Let p denote the number of people and a denote the number of animals.
We are told p + a = 43 and a = p + 17. Substituting the second equation into the
Ô¨Årst, we Ô¨Ånd p + (p + 17) = 43, which is equivalent to 2p = 26 or p = 13. There
are 13 Ô¨Ågures of people and 30 Ô¨Ågures of animals.
P1.12 We must solve for x in 35 + x = 4(5 + x). We obtain 35 + x = 20 + 4x,
then 15 = 3x, so x = 5.
P1.13 Let's consider x =the number of nuts collected by a boy.
Therefore
2x =the number of nuts collected by a girl. We get x + 2x = 120, so x = 40.
The number of nuts collected by a boy= 40, The number of nuts collected by a
girl= 80.
P1.14 Let A be Alice's age and B be Bob's age.
We're told A = B + 5 and
A+ B = 25. Substituting the Ô¨Årst equation into the second we Ô¨Ånd (B + 5) + B =
25, which is the same as 2B = 20, so Bob is 10 years old. Alice is 15 years old.
P1.15 The Ô¨Årst shop can bind 4500/30 = 150 books per day. The second shop
can bind 4500/45 = 100 books per day. The combined production capacity rate
is 150 + 100 = 250 books per day. It will take 4500/250 = 18 days to bind the
books when the two shops work in parallel.
P1.16 Let x denote the distance the slower plane will travel before the two planes
meet. Let tmeet denote the time when they meet, as measured from the moment
the second plane departs. The slower plane must travel x km in tmeet hours, so
we have tmeet =
x
600 . The faster plane is 600 km behind when it departs. It must
travel a distance (x + 600) km in the same time so tmeet = x+600
900 . Combining
the two equations we Ô¨Ånd
x
600 = x+600
900 . After cross-multiplying we Ô¨Ånd 900x =
600x + 6002, which has solution x = 1200 km. The time when the planes meet is
tmeet = 2 hours after the departure of the second plane.
P1.17 This is a funny nonsensical problem that showed up on a school exam. I'm
just checking to make sure you're still here.
P1.21 Using the quadratic formula, we Ô¨Ånd x = m¬±‚àö
m2‚àí8m
4
. If m2 ‚àí8m ‚â•0,
the solutions are real. If m2 ‚àí8m < 0, the solutions will be complex numbers.
Factoring the expressions and plugging in some numbers, we observe that m2 ‚àí
8m = m(m ‚àí8) < 0 for all m ‚àà(0, 8).
P1.23 See bit.ly/float64prec for the calculations.
P1.24 For (3), 1 3
4 + 1 31
32 = 7
4 + 63
32 = 56
32 + 63
32 = 119
32 = 3 23
32 .

481
P1.25 The running time of Algorithm Q grows linearly with the size of the prob-
lem, whereas Algorithm P's running time grows quadratically. To Ô¨Ånd the size
of the problem when the algorithms take the same time, we solve P(n) = Q(n),
which is 0.002n2 = 0.5n. The solution is n = 250. For n > 250, the linear-time
algorithm (Algorithm Q) will take less time.
P1.29 Solve for b in Pythagoras' formula c2 = a2 + b2 with c = œï, and a = ‚àöœï.
The triangle with sides 1, ‚àöœï, and œï is called Kepler's triangle.
P1.30 Use Pythagoras' theorem to Ô¨Ånd x. Then use cos(30‚ó¶) =
‚àö
3
2
= x
z to Ô¨Ånd
z. Finally use sin(30‚ó¶) = 1
2 = y
z to Ô¨Ånd y.
P1.31 Observe the two right-angle triangles drawn in Figure 1.24.
From the
triangle with angle 25‚ó¶we know tan 25‚ó¶=
h
800+d. From the triangle with angle
20‚ó¶we know tan 20‚ó¶=
h
1800+d. We isolate h in both equations and eliminate
h by equating (1800 + d) tan 25‚ó¶= tan 20‚ó¶(800 + d). Solving for d we Ô¨Ånd d =
1800 tan 20‚ó¶‚àí800 tan 25‚ó¶
tan 25‚ó¶‚àítan 20‚ó¶
= 2756.57 m.
Finally we use tan 25‚ó¶=
h
800+d again to
obtain h = tan 25‚ó¶(800 + d) = 1658.46 m.
P1.32 Consider the right-angle triangle with base x and opposite side 2000.
Looking at the diagram we see that Œ∏ = 24‚ó¶.
We can then use the relation
tan 24‚ó¶= 2000
x
and solve for x.
P1.34 The internal angles of an equilateral triangle are all 60‚ó¶. Draw three radial
lines that connect the centre of the circle to each vertex of the triangle.
The
equilateral triangle is split into three obtuse triangles with angle measures 30‚ó¶,
30‚ó¶, and 120‚ó¶. Split each of these obtuse sub-triangles down the middle to obtain
six right-angle triangles with hypotenuse 1. The side of the equilateral triangle is
equal to two times the base of the right-angle triangles a = 2 cos(30‚ó¶) =
‚àö
3. To
Ô¨Ånd the area, we use A‚ñ≥= 1
2 ah, where h = 1 + sin(30‚ó¶).
P1.35 We know sin2(Œ∏) =
1
2 (1 ‚àícos(2Œ∏)) and cos2(Œ∏) =
1
2 (1 + cos(2Œ∏)), so
their product is
1
4 (1 ‚àícos(2Œ∏) cos(2Œ∏)).
Note cos(2Œ∏) cos(2Œ∏) = cos2(2Œ∏).
Us-
ing the power-reduction formula on the term cos2(2Œ∏) leads to the Ô¨Ånal answer
sin2 Œ∏ cos2 Œ∏ = 1
4
 1 ‚àí1
2 (1 + cos(4Œ∏))

.
P1.36 Split the octagon into eight isosceles triangles. The height of each triangle
will be 1, and its angle measure at the centre will be
360‚ó¶
8
= 45‚ó¶.
Split each
of these triangles into two halves down the middle.
The octagon is now split
into 16 similar right-angle triangles with angle measure 22.5‚ó¶at the centre. In
a right-angle triangle with angle 22.5‚ó¶and adjacent side 1, what is the length of
the opposite side? The opposite side of each of the 16 triangles is b
2 = tan(22.5‚ó¶),
so the perimeter of the octagon is P8 = 16 tan(22.5‚ó¶).
In general, if a unit
circle is inscribed inside an n-sided regular polygon, the perimeter of the polygon
is Pn = 2n tan

360‚ó¶
2n

.
To Ô¨Ånd the area of the octagon, we use the formula
A‚ñ≥=
1
2 bh, with b = 2 tan(22.5‚ó¶) and h = 1 to Ô¨Ånd the area of each isosceles
triangle. The area of the octagon is A8 = 8 ¬∑ 1
2 (2 tan(22.5‚ó¶))(1) = 8 tan(22.5‚ó¶).
For an n-sided regular polygon the area formula is An = n tan

360‚ó¶
2n

. Bonus
points if you can tell me what happens to the formulas for Pn and An as n goes
to inÔ¨Ånity (see bit.ly/1jGU1Kz).
P1.40 Initially the horizontal distance between the observer and the plane is
d1 =
2000
tan 30‚ó¶m.
After 10 seconds, the distance is d2 =
2000
tan 55‚ó¶m.
Velocity is
change in distance divided by the time v = d1‚àíd2
10
= 206.36 m/s. To convert m/s
into km/h, we must multiply by the appropriate conversion factors: 206.36 m/s √ó
1 km
1000 m √ó 3600 s
1 h
= 742.92 km/h.

482
ANSWERS AND SOLUTIONS
P1.41 The volume of the water stays constant and is equal to 1000 cm3. Initially
the height of the water h1 can be obtained from the formula for the volume of a
cylinder 1000 cm3 = h1œÄ(17 cm)2, so h1 = 5.51 cm. After the bottle is inserted,
the volume of water has the shape of a cylinder from which a cylindrical part is
missing and 1000 cm3 = h2
 œÄ(17 cm)2 ‚àíœÄ(7.5 cm)2
. We Ô¨Ånd h2 = 6.84 cm. The
change in water height is h2 ‚àíh1 = 1.33 cm.
P1.42 Using the law of cosines for the angles Œ±1 and Œ±2, we obtain the equations
72 = 82 + 122 ‚àí2(8)(12) cos Œ±1 and 112 = 42 + 122 ‚àí2(4)(12) cos Œ±2 from which
we Ô¨Ånd Œ±1 = 34.09‚ó¶and Œ±2 = 66.03‚ó¶. In the last step we use the law of cosines
again to obtain x2 = 82 + 42 ‚àí2(8)(4) cos(34.09‚ó¶+ 66.03‚ó¶).
P1.44 The length of the horizontal part of the rope is ‚Ñìh = 4 sin 40. The circular
portion of the rope that hugs the pulley has length
1
4 of the circumference of
a circle with radius r = 50 cm = 0.5 m.
Using the formula C = 2œÄr, we Ô¨Ånd
‚Ñìc = 1
4 œÄ(0.5)2 =
œÄ
16 . The vertical part of the rope has length ‚Ñìv = 4 cos 40 + 2.
The total length of rope is ‚Ñìh + ‚Ñìc + ‚Ñìv = 7.83 m.
P1.45 The rectangle's area is equal to its length times its height Arect = ‚Ñìh.
P1.46 The box's volume is V = w √ó h √ó ‚Ñì= 10.5 √ó 7 √ó 22.3 = 1639 cm3=1.639 L.
P1.47 We didn't really cover these concepts in the book, but since we're on the
topic let's deÔ¨Åne some vocabulary. The complement of an acute angle is its defect
from a right angle; that is, the angle by which it falls short of a right angle. (i)
Two angles are complementary when their sum is 90‚ó¶.
The supplement of an
angle is its defect from two right angles, that is, the angle by which it falls short
of 180‚ó¶. (ii) Two angles are supplementary when their sum is 180‚ó¶. Angles that
are complementary or supplementary to the same angle are equal to one another.
We'll now use these facts and the diagram below to Ô¨Ånd the angle Œ∏.
60‚ó¶
Œ±
100‚ó¶
Œ≤
Œ≥
Œ≥‚Ä≤
Œ∏
The angle Œ± is vertically opposite to the angle 60‚ó¶so Œ± = 60‚ó¶. The angle Œ≤ is
supplementary to the angle 100‚ó¶so Œ≤ = 180‚àí100 = 80‚ó¶. The sum of the angles in
a triangle is 180‚ó¶so Œ≥ = 180‚ó¶‚àíŒ± ‚àíŒ≤ = 40‚ó¶. The two horizontal lines are parallel
so the diagonally cutting line makes the same angle with them: Œ≥‚Ä≤ = Œ≥ = 40‚ó¶.
The angle Œ∏ is supplementary to the angle Œ≥‚Ä≤ so Œ∏ = 180 ‚àí40 = 120‚ó¶.
P1.48 The base of this triangle has length 2r and each side has length R+r. If you
split this triangle through the middle, each half is a right triangle with an angle at
the centre 360‚ó¶
24
= 15‚ó¶, hypotenuse R + r, and opposite side r. We therefore have
sin 15‚ó¶=
r
R+r . After rearranging this equation, we Ô¨Ånd R
r = 1‚àísin 15‚ó¶
sin 15‚ó¶
= 2.8637.
P1.51 The tank's total capacity is 15 √ó 6 √ó 5 = 450 m3. If 30% of its capacity is
spent, then 70% of the capacity remains: 315 m3. Knowing that 1 m3 = 1000 L,
we Ô¨Ånd there are 315 000 L in the tank.

483
P1.52 The Ô¨Årst tank contains 1
4 √ó4000 = 1000 L. The second tank contains three
times more water, so 3000 L. The total is 4000 L.
P1.53 Let's deÔ¨Åne w and h to be the width and the height of the hole. DeÔ¨Åne d
to be the distance from the hole to the sides of the lid. The statement of the
problem dictates the following three equations must be satisÔ¨Åed: w + 2d = 40,
h + 2d = 30, and wh = 500. After some manipulations, we Ô¨Ånd w = 5(1 +
‚àö
21),
h = 5(
‚àö
21 ‚àí1) and d = 1
2 (35 ‚àí5
‚àö
21).
P1.54 The amount of wood in a pack of wood is proportional to the area of a
circle A = œÄr2. The circumference of this circle is equal to the length of the rope
C = ‚Ñì.
Note the circumference is proportional to the radius C = 2œÄr.
If we
want double the area, we need the circle to have radius
‚àö
2r, which means the
circumference needs to be
‚àö
2 times larger. If we want a pack with double the
wood, we need to use a rope of length
‚àö
2‚Ñì.
P1.55 In 10 L of a 60% acid solution there are 6 L of acid and 4 L of water. A
20% acid solution will contain four times as much water as it contains acid, so 6 L
acid and 24 L water. Since the 10 L we start from already contains 4 L of water,
we must add 20 L.
P1.56 The document must have a 768/1004 aspect ratio, so its height must be
6 √ó 1004
768 = 7.84375 inches.
P1.57 If we rewrite 1 + 2 + 3 + ¬∑ ¬∑ ¬∑ + 98 + 99 + 100 by pairing numbers, we obtain
the sum (1 + 100) + (2 + 99) + (3 + 98) + ¬∑ ¬∑ ¬∑ . This list has 50 terms and each term
has the value 101. Therefore 1 + 2 + 3 + ¬∑ ¬∑ ¬∑ + 100 = 50 √ó 101 = 5050.
P1.62 An nAPR of 12% means the monthly interest rate is 12%
12
= 1%. After 10
years you'll owe $5000(1.01)120 = $16501.93. Yikes!
P1.63 The graphs of the functions are shown in Figure A.1. Observe that f(x)
decreases to 37% of its initial value when x = 2. The increasing exponential g(x)
reaches 63% of its maximum value at x = 2.
(a) The graph of f(x).
(b) The graph of g(x).
Figure A.1: The graphs of the two functions from P1.63.
P1.64 We're looking for the time t such that Q(t)/Qo = 1
2 , which is the same as
e‚àí5t = 0.5. Taking logarithms of both sides we Ô¨Ånd ‚àí5t = ln(0.5), and solving
for t we Ô¨Ånd t = 0.14 s.

484
ANSWERS AND SOLUTIONS
P1.65 We're told T(24)/To =
1
2 = e‚àí24/œÑ, which we can rewrite as ln( 1
2 ) =
‚àí24/œÑ. Solving for œÑ, we Ô¨Ånd œÑ =
24
ln 2 = 34.625 min. To Ô¨Ånd the time the body
takes to reach 1% of its initial temperature, we must solve for t in T(t)/To =
0.01 = e‚àít/34.625. We Ô¨Ånd t = 159.45 min.
P1.67 There exists at least one banker who is not a crook. Another way of saying
the same thing is "not all bankers are crooks"‚Äîjust most of them.
P1.68 Everyone steering the ship at Monsanto ought to burn in hell, forever.
P1.69 (a) Investors with money but without connections.
(b) Investors with
connections but no money. (c) Investors with both money and connections.
Chapter 2 solutions
Answers to problems
P2.1 (a) ‚Éóu1 = 5‚à†90‚ó¶. (b) ‚Éóu2 =
‚àö
5‚à†63.4‚ó¶. (c) ‚Éóu3 =
‚àö
5‚à†243.4‚ó¶or
‚àö
5‚à†‚àí116.6‚ó¶.
P2.2 (a) ‚Éóv1 = (17.32, 10).
(b) ‚Éóv2 = (0, ‚àí10).
(c) ‚Éóv3 = (‚àí4.33, 2.5).
P2.3
(a) ‚Éów1 = 9.06ÀÜƒ± + 4.23ÀÜÔöæ. (b) ‚Éów2 = ‚àí7ÀÜÔöæ. (c) ‚Éów3 = 3ÀÜƒ± ‚àí2ÀÜÔöæ+ 3ÀÜk.
P2.4 (a) (3, 4).
(b) (0, 1). (c) (7.33, 6.5).
P2.5 Q = (5.73, 4).
P2.6 (1) 6. (2) 0. (3) ‚àí3.
(4) (‚àí2, 1, 1). (5) (3, ‚àí3, 0). (6) (7, ‚àí5, 1).
P2.7 (‚àí2
3 , 1
3 , 2
3 ) or ( 2
3 , ‚àí1
3 , ‚àí2
3 ).
P2.8 (12, ‚àí4, ‚àí12).
P2.9 (a) 2i. (b) 1
4 (5 + i). (c) 2 + i.
P2.10 (a) x = ¬±2i.
(b) x = ‚àí16. (c) x = ‚àí1 ‚àíi and x = ‚àí1 + i. (d) x = i, x = ‚àíi, x =
‚àö
3i, and
x = ‚àí
‚àö
3i.
P2.11 (a)
‚àö
5. (b) 1
2 (‚àí3 + i). (c) ‚àí5 ‚àí5i.
P2.12 t = 52 weeks.
Solutions to selected problems
P2.7 See bit.ly/1cOa8yo for calculations.
P2.8 Any multiple of the vector ‚Éóu1 √ó ‚Éóu2 = (‚àí3, 1, 3) is perpendicular to both
‚Éóu1 and ‚Éóu2. We must Ô¨Ånd a multiplier t ‚ààR such that t(‚àí3, 1, 3) ¬∑ (1, 1, 0) = 8.
Computing the dot product we Ô¨Ånd ‚àí3t + t = 8, so t = ‚àí4. The vector we're
looking for is (12, ‚àí4, ‚àí12). See bit.ly/1nmYH8T for calculations.
P2.12 We want the Ô¨Ånal state of the project to be 100% real: pf = 100. Given
that we start from pi = 100i, the rotation required is e‚àíiŒ±h(t) = e‚àíi œÄ
2 , which
means Œ±h(t) = œÄ
2 . We can rewrite this equation as h(t) = 0.2t2 =
œÄ
2Œ± and solving
for t we Ô¨Ånd t =
q
œÄ
2(0.002904)(0.2) = 52 weeks.
Chapter 3 solutions
Answers to problems
P3.1 (1, 2, 3).
P3.2 |a‚ü©+ |b‚ü©= 5|0‚ü©+ 2|1‚ü©.
P3.3 a) 5; b) (‚àí1, 1, 1); c) (0, 0, 0);
d) (0, 0, 0).
P3.4 a) 0; b) ÀÜk = (0, 0, 1); c) ÀÜÔöæ+ (‚àíÀÜk) = (0, 1, ‚àí1); d) (‚àíÀÜk) =
(0, 0, ‚àí1).
Chapter 4 solutions
Answers to exercises
E4.1 x = 4, y = ‚àí2.
E4.3 (a) no solution; (b) x = 0, y = 2; (c) {(1, 1)+s(‚àí1, 1), ‚àÄs ‚ààR}.
E4.4 (1) X = BA‚àí1; (2) X = C‚àí1B‚àí1A‚àí1ED‚àí1; (3) X = AD‚àí1.
E4.5

485
P =

19
22
43
50

, Q =

‚àí5
9
8
6

.
E4.7 V = 2.
E4.8 A‚àí1 =

2
‚àí1
‚àí1
1

.
E4.9 Link
to .ods .xls Ô¨Åles.
Solutions to selected exercises
E4.2 The row operations required to bring A to reduced row echelon form are:
R1 ‚Üê1
3 R1, R2 ‚ÜêR2 ‚àí2R1, R2 ‚Üê‚àí2R2, R1 ‚ÜêR1 ‚àíR2. Using SymPy these
operations are implemented as follows:
>>> A[0,:] = A[0,:]/3
>>> A[1,:] = A[1,:] - 2*A[0,:]
>>> A[1,:] = -2*A[1,:]
>>> A[0,:] = A[0,:] - A[1,:]
Try displaying the matrix A after each operation to watch the progress.
E4.7 See bit.ly/181ugMm for calculations.
Answers to problems
P4.1 x = 15 and y = 2.
P4.2 (a) R2 ‚ÜêR2 ‚àí2R1, R2 ‚Üê‚àí2R2, R1 ‚ÜêR1 ‚àíR2;
(b) R2 ‚ÜêR2‚àí2R1, R2 ‚Üê‚àí2
3 R2, R1 ‚ÜêR1‚àí3
2 R2; (c) R1 ‚Üê1
2 R1, R2 ‚ÜêR2‚àí3R1,
R2 ‚Üê4
3 R2, R1 ‚ÜêR1‚àí3
4 R2.
P4.3 (a) (‚àí2, 2); (b) (‚àí4, ‚àí1, ‚àí2); (c) ( ‚àí2
5 , ‚àí1
2 , 3
5 ).
P4.4 (a) {(2, 0)+s(‚àí2, 1), ‚àÄs ‚ààR}; (b) {(2, 1, 0)+t(3, 1, 1), ‚àÄt ‚ààR}; (c) {( 1
10 , 3
5 , 0)+Œ±(1, 1, 0), ‚àÄ
P4.5
(a)
h x1
x2
x3
i
‚àà
h 0
0
0
i
+ t
 ‚àía1
‚àí1
‚àía2

, ‚àÄt ‚ààR

.
(b)
h x1
x2
x3
i
=
h 0
‚àí2
5
i
.
P4.6
C = B‚àí1.
P4.7 (a) M‚àí1L‚àí1MK; (b) J‚àí3K‚àí1J2; (c) A = 1; (d) Y = N‚àí1.
P4.8 ‚Éóx = (‚àí7, ‚àí19, ‚àí3)T.
P4.9 ‚Éóx = (30.64, 10.48, 17.06).
P4.10 a) AB =
h 6 2
10 2
11 3
i
. b) AA =
h 4 6 3
6 18 8
3 7 5
i
. c) BA doesn't exist. d) BB doesn't exist.
P4.11
h
‚àí2
‚àí2
‚àí15 ‚àí15
i
.
P4.12 a)
h
0
cos(Œ±)‚àísin(Œ±)
sin(Œ±)‚àícos(Œ±)
‚àícos(2Œ±)
i
; b)
h cos2(Œ±)
sin(Œ±)
‚àícos(Œ±) sin2(Œ±)
i
; c)
h cos(2Œ±) sin(Œ±)
‚àícos(Œ±)
0
i
.
P4.13 a) ‚àí3. b) 0. c) 10.
P4.14 det(A) = ‚àí48; det(B) = 13.
P4.15 Area
= 2.
P4.16 Volume = 8.
P4.17 a) 86. b) ‚àí86. c) ‚àí172.
P4.18 For both rows
and columns: A: not independent; B: independent; C: not independent; D: not
independent.
P4.19 det(J) = r.
P4.20 | det(Js)| = œÅ2 sin œÜ.
P4.21 a) The
inverse doesn't exist. b)
h
5
‚àí2
‚àí2
1
i
. c)
h
2
‚àí3
2
‚àí1
1
i
.
P4.22 B =
 ‚àí17 ‚àí30
5
8

.
P4.23
A‚àí1 =
1
21
Ô£Æ
Ô£∞
‚àí3 ‚àí5 ‚àí21
6
11
3
‚àí2
7
‚àí4
9
15
21
2
‚àí12
3
‚àí9
0
3
Ô£π
Ô£ª.
P4.25 CA =
 1
2
‚àí4
‚àí2 ‚àí1
2
1
5
‚àí7

; CB =
 ‚àí10
6
‚àí12
‚àí4 ‚àí10
20
1
18
‚àí5

;
CC =
" 0
0
0
0
2
‚àí2
2
‚àí2
‚àí4
4
‚àí4
4
‚àí8
8
‚àí8
8
#
.
P4.26 a = ‚àí3, b = 1, c = 2, d = ‚àí2.
P4.27
 1 ‚àí1
2
2
 ‚àí1 ‚àí1
1
2

=
 ‚àí2 ‚àí3
0
2

.
Solutions to selected problems
P4.6 First simplify the equation by multiplying with A‚àí1 from the left, and with
D‚àí1 form the right, to obtain BC = 1. Now we can isolate C by multiplying
with B‚àí1 from the left. We obtain B‚àí1 = C.
P4.9 Start by rewriting the matrix equations as (1 ‚àíA)‚Éóx = ‚Éód, then solve for
‚Éóx by hitting the equation with the appropriate inverse: ‚Éóx = (1 ‚àíA)‚àí1 ‚Éód.
See
bit.ly/1hg44Ys for the details of the calculation.

486
ANSWERS AND SOLUTIONS
P4.17 The answers in a) and b) have diÔ¨Äerent signs because interchanging rows
in a matrix changes the sign of the determinant. For part c), we use the fact that
multiplying one row of a matrix by a constant has the eÔ¨Äect of multiplying the
determinant by the same constant.
P4.18 To check if the rows/columns of a matrix are independent, we can calculate
the determinant of the matrix. If the determinant is not zero then vectors are
independent, else vectors are not independent. The columns of a square matrix
form a linearly independent set if and only if the rows of the matrix form a linearly
independent set.
P4.19 The determinant of J is

cos Œ∏
‚àír sin Œ∏
sin Œ∏
r cos Œ∏
 = r cos2 Œ∏ + r sin2 Œ∏ = r.
P4.20 The determinant of Js is
det(Js) =

sin œÜ cos Œ∏
‚àíœÅ sin œÜ sin Œ∏
œÅ cos œÜ cos Œ∏
sin œÜ sin Œ∏
œÅ sin œÜ cos Œ∏
œÅ cos œÜ sin Œ∏
cos œÜ
0
‚àíœÅ sin œÜ

= œÅ2  cos2 œÜ sin œÜ(‚àí1) ‚àísin3 œÜ(1)

= ‚àíœÅ2 sin œÜ
 cos2 œÜ + sin2 œÜ

= ‚àíœÅ2 sin œÜ.
Since we're only interested in Ô¨Ånding the volume factor we can ignore the sign of
the Jacobian's determinant: | det(Js)| = œÅ2 sin œÜ.
P4.22 To solve for the matrix B in the equation AB = C, we must get rid of the
matrix A on the left side. We do this by multiplying the equation AB = C by
the inverse A‚àí1. We Ô¨Ånd the inverse of A by starting from the array [ A | 1 ],
and performing the row operations R1 : R2 ‚ÜêR2 ‚àí2R1, R2 : R2 ‚Üê‚àíR2, and
R3 : R1 ‚ÜêR1 ‚àí4R2, to Ô¨Ånd the matrix A‚àí1 =
h
‚àí7
4
2
‚àí1
i
. Applying A‚àí1 to both
sides of the equation we Ô¨Ånd B = A‚àí1C =
 ‚àí17 ‚àí30
5
8

.
P4.24 Zero matrix has the det(A) = 0.
We have A‚àí1 =
1
det(A)adj(A).
We
cannot divide by zero, so the zero matrix has no inverse.
P4.26 Multiply two matrices and create systems of four equations: a + 3c = 3,
‚àí2a ‚àíc = 4, b + 3d = ‚àí5, ‚àí2b ‚àíd = 0. Combine the Ô¨Årst two equations and the
last two to Ô¨Ånd the variables by using multiplication and addition.
P4.27
h a b
c d
ih
e f
g h
i
=
h
ae+bg af+bh
ce+dg df+dh
i
=
h
2ae
ae+ec
ce+ca c2+ec
i
. So we have 2ae = ‚àí2,
then ae = ‚àí1; ae + ec = ‚àí3, then ec = ‚àí2; ec + ca = 0, then ca = 2; ce + c2=2,
then c = 2. Therefore c = d = h = 2, a = g = 1 and e = b = f = ‚àí1.
Chapter 5 solutions
Answers to exercises
E5.1 d(‚Ñì, O) =
3
‚àö
291
97
‚âà0.5276.
E5.2 d(P, O) = 5
‚àö
3 ‚âà8.66.
E5.3
(‚àí2, 11, 7)¬∑[(x, y, z)‚àí(1, 0, 0)] = 0.
E5.4 2x+y = 5.
E5.5 ( 30
53 , 5
53 , ‚àí20
53 ).
E5.6
R(A) = span{(1, 3, 0, 0), (0, 0, 1, 0), (0, 0, 0, 1)}, C(A) = span{(1, 2, 3)T, (3, 7, 9)T,
(3, 6, 10)T}, and N(A) = span{(‚àí3, 1, 0, 0)T}.
E5.7 N(A) = span{( 1
2 , 1 , 1
2 , 1)}.

487
Solutions to selected exercises
E5.1 Using the formula from page 187, we Ô¨Ånd
d(‚Ñì, O) =
(4, 5, 6) ‚àí(4, 5, 6) ¬∑ (7, 8, 9)
72 + 82 + 92
(7, 8, 9)

=
(4, 5, 6) ‚àí122
194 (7, 8, 9)

=

 ‚àí39
97 , ‚àí3
97 , 33
97
 = 3
‚àö
291
97
.
E5.3 We use the reduced row echelon procedure to Ô¨Ånd the intersection of the
two planes. The line of intersection is ‚Ñì1 : {(1, 0, 0) + (1, ‚àí3, 5)t | ‚àÄt ‚ààR}, where
(1, 0, 0) is a point on the line of intersection and ‚Éóv1 = (1, ‚àí3, 5) is its direction
vector.
We want to Ô¨Ånd the equation of a plane ‚Éón ¬∑ [(x, y, z) ‚àípo] = 0 whose
normal vector is perpendicular to both ‚Éóv1 = (1, ‚àí3, 5) and ‚Éóv2 = (2, 1, ‚àí1). To
Ô¨Ånd a normal vector use ‚Éón = ‚Éóv1 √ó ‚Éóv2 = (‚àí2, 11, 7). The point po = (1, 0, 0) is on
the line ‚Ñì1, so the equation of the plane is (‚àí2, 11, 7) ¬∑ [(x, y, z) ‚àí(1, 0, 0)] = 0.
E5.5 The vectors (2, ‚àí4, 2) and (6, 1, ‚àí4) deÔ¨Åne a plane P. We're looking for
the projection of ‚Éóv on P. First we use the cross product to Ô¨Ånd a normal vector
to the plane P: ‚Éón = (2, ‚àí4, 2) √ó (6, 1, ‚àí4) = (14, 20, 26). Then we compute the
projection using the formula Œ†P (‚Éóv) = ‚Éóv ‚àíŒ†‚Éón(‚Éóv) = ( 30
53 , 5
53 , ‚àí20
53 ).
E5.7 The null space of A consists of all vectors ‚Éóx = (x1, x2, x3, x4)T that satisfy
A‚Éóx = ‚Éó0. To solve this equation we Ô¨Årst compute the RREF of A:
rref(A) =
Ô£Æ
Ô£ØÔ£∞
1
0
0
‚àí1
2
0
1
0
‚àí1
0
0
1
‚àí1
2
Ô£π
Ô£∫Ô£ª,
and observe it pivots in the Ô¨Årst three columns, while x4 = s is a free variable.
The null space of A is
Ô£Æ
Ô£ØÔ£∞
1
0
0
‚àí1
2
0
1
0
‚àí1
0
0
1
‚àí1
2
Ô£π
Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£∞
x1
x2
x3
s
Ô£π
Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£∞
0
0
0
Ô£π
Ô£ª
‚áí
1x1 ‚àí1
2 s
=
0
1x2 ‚àís
=
0
x3 ‚àí1
2 s
=
0
The null space is N(A) = span{( 1
2 , 1 , 1
2 , 1)}.
Answers to problems
P5.1 a) q = (1, 2); b) inÔ¨Ånite intersection points; c) m = (1, 1).
P5.2 a) ‚Ñì1:
1(x) ‚àí1(y) + 1(z) = 0; b) ‚Ñì2:
0(x) + 1(y) + 1(z) = 0.
P5.3 a) parallel;
b) neither; c) perpendicular.
P5.4 d(q, P) = 1.
P5.5 a) d(p, q) = 6;
b) d(m, n) = 5; c) d(r, s) = 3; d) d(p, j) =
‚àö
19.
P5.6 x + y + 2z = 4.
P5.7
‚Ñì:
n
x+3
2
= y‚àí1
‚àí4 =
z
‚àí1
o
.
P5.8 Œ†‚Éóu(‚Éóv) =
‚Éóv¬∑‚Éóu
‚à•‚Éóu‚à•2 ‚Éóu =
(1,1,1)¬∑(2,1,‚àí1)
22+12+(‚àí1)2 (2, 1, ‚àí1) =
1
3 (2, 1, ‚àí1); Œ†‚Éóv(‚Éóu) =
‚Éóv¬∑‚Éóu
‚à•‚Éóv‚à•2 ‚Éóv = (1,1,1)¬∑(2,1,‚àí1)
12+12+12
(1, 1, 1) = 2
3 (1, 1, 1).
P5.9 Œ†P (‚Éóv) =
1
7 (17, 30, ‚àí1).
P5.10 Œ†P ‚ä•(‚Éóu) = ‚àí1
41 (6, ‚àí5, 12).
P5.11 d(‚Ñì, P) = 7
3 .
P5.16
A =
 1
2x
0
1

. This matrix A corresponds to a shear on the x-axis.
Solutions to selected problems
P5.1 To Ô¨Ånd an intersection point, you have to isolate a variable in one equation
and replace it in the second equation, and then Ô¨Ånd a variable.

488
ANSWERS AND SOLUTIONS
P5.3 a) Find a normal for each plane n1 = (1, ‚àí1, ‚àí1) and n2 = (2, ‚àí2, ‚àí2) =
2(1, ‚àí1, ‚àí1). These places are multiples of each other so the planes are parallel.
b) n1 = (3, 2, 0), n2 = (0, 1, ‚àí1). n1 ¬∑ n2 = 2.
Planes are neither parallel or
perpendicular. c) n1 = (1, ‚àí2, 1), n2 = (1, 1, 1). n1 ¬∑ n2 = 0. Therefore two planes
are perpendicular.
P5.4 d(q, P) = |2(2)+1(3)‚àí2(5)|
‚àö
22+12+(‚àí2)2 = 3
3 = 1.
P5.6 First we Ô¨Ånd vectors in the place, for example ‚Éóu ‚â°r ‚àíq = (‚àí1, ‚àí1, 1) and
‚Éóv ‚â°s ‚àíq = (0, ‚àí2, 1). Then we need to Ô¨Ånd the normal vector ‚Éón, ‚Éón = ‚Éóu √ó ‚Éóv =
(‚àí1, ‚àí1, 1) √ó (0, ‚àí2, 1) = (1, 1, 2). We can use any of the three points as the point
po in the geometric equation ‚Éón ¬∑ [(x, y, z) ‚àípo] = 0. Using q = (1, 3, 0), we obtain
the equation (1, 1, 2)¬∑[(x, y, z)‚àí(1, 3, 0)] = 1(x‚àí1)+1(y‚àí3)+2z = 0. Simplifying
the expression gives x ‚àí1 + y ‚àí3 + 2z = 0 and we end up with x + y + 2z = 4.
P5.9 Let's Ô¨Årst Ô¨Ånd Œ†P ‚ä•(‚Éóv) =
‚Éóv¬∑‚Éón
‚à•‚Éón‚à•2 ‚Éón = (3,4,1)¬∑(2,‚àí1,4)
22+(‚àí1)2+42 (2, ‚àí1, 4) = 2
7 (2, ‚àí1, 4).
Then Œ†P (‚Éóv) = ‚Éóv ‚àíŒ†P ‚ä•(‚Éóv) = (3, 4, 1)‚àí2
7 (2, ‚àí1, 4) = 1
7 (17, 30, ‚àí1). We can verify
Œ†P (‚Éóv) + Œ†P ‚ä•(‚Éóv) = 1
7 (17, 30, ‚àí1) + 2
7 (2, ‚àí1, 4) = (3, 4, 1) = ‚Éóv. This shows that
the projection we found is correct.
P5.10 First we Ô¨Ånd the normal ‚Éón of the plane P using the cross-product trick
‚Éón = (s ‚àím) √ó (r ‚àím).
Since s ‚àím = (‚àí1, ‚àí6, 3) and r ‚àím = (‚àí2, 0, ‚àí1),
we Ô¨Ånd ‚Éón = (‚àí1, ‚àí6, 3) √ó (‚àí2, 0, ‚àí1) = (6, ‚àí5, 12).
Now we want to Ô¨Ånd the
projection of ‚Éóu onto the space perpendicular to P, which is Œ†P ‚ä•(‚Éóu) =
‚Éóv¬∑‚Éón
‚à•‚Éón‚à•2 ‚Éón =
(‚àí2,1,1)¬∑(6,‚àí5,12)
62+(‚àí5)2+122
(6, ‚àí5, 12) = ‚àí1
41 (6, ‚àí5, 12).
P5.11 We'll compute the distance by Ô¨Ånding a vector ‚Éóv that connects an arbitrary
point on the plane P with an arbitrary point on the line ‚Ñìand then computing the
component of ‚Éóv that is perpendicular to the plane. The point that lies on the line
is p‚Ñì= (1, ‚àí3, 2) and the point on the plane is qP = (0, 1, 1). The vector between
them is ‚Éóv = (0, 1, 1) ‚àí(1, ‚àí3, 2) = (1, ‚àí4, 1). To compute d(‚Ñì, P) we must Ô¨Ånd
|‚Éón¬∑‚Éóv|
‚à•‚Éón‚à•= (1,‚àí4,1)¬∑(‚àí1,2,2)
‚àö
(‚àí1)2+22+22 = 7
3 .
P5.12 Directly check that the sum of two upper triangular matrices and scalar
multiplication result in upper triangular matrices. And of course the zero matrix
is upper triangular.
P5.13 If A is a diagonal matrix, we have Aij = 0 = Aji when i Ã∏= j. Therefore
diagonal matrices are symmetric.
P5.14 If {‚Éóu,‚Éóv} is a basis then the dimension of V would be two. So it is enough
to check both {‚Éóu + ‚Éóv, a‚Éóu} and {a‚Éóu, b‚Éóv} are linearly independent. Assuming s(‚Éóu +
‚Éóv) + ta‚Éóu = (s + ta)‚Éóu + s‚Éóv = 0 we have s + ta = s = 0 and hence s = t = 0.
Assuming sa‚Éóu + tb‚Éóv = 0 we have sa = tb = 0 and hence s = t = 0.
P5.15 Let us assume that {‚Éóv3,‚Éóv2 + ‚Éóv3,‚Éóv1 + ‚Éóv2 + ‚Éóv3} is not linearly independent.
Then x1‚Éóv3 + x2(‚Éóv2 + ‚Éóv3) + x3(‚Éóv1 + ‚Éóv2 + ‚Éóv3) = 0 has a nontrivial solution. Let us
say this solution is (c1, c2, c3) where c1 Ã∏= 0 or c2 Ã∏= 0 or c3 Ã∏= 0. Then we see that
0 = c1‚Éóv3 + c2(‚Éóv2 + ‚Éóv3) + c3(‚Éóv1 + ‚Éóv2 + ‚Éóv3)
= c1‚Éóv3 + c2‚Éóv2 + c2‚Éóv3 + c3‚Éóv1 + c3‚Éóv2 + c3‚Éóv3
= (c1 + c2 + c3)‚Éóv3 + (c2 + c3)‚Éóv2 + c3‚Éóv1.
If c1 Ã∏= 0, then c1 +c2 +c3 Ã∏= 0. If c2 Ã∏= 0, then c2 +c3 Ã∏= 0. If c3 Ã∏= 0, then c3 Ã∏= 0.
This means that x1‚Éóv1 +x2‚Éóv2 +x3‚Éóv3 = 0 has a nontrivial solution. In other words,
we deduce that {‚Éóv1,‚Éóv2,‚Éóv3} is linearly dependent. It is a contradiction. Because
we are given that {‚Éóv1,‚Éóv2,‚Éóv3} is linearly independent. This contradiction is coming
from the assumption that {‚Éóv3,‚Éóv2 + ‚Éóv3,‚Éóv1 + ‚Éóv2 + ‚Éóv3} is not linearly independent.
Therefore, {‚Éóv3,‚Éóv2 + ‚Éóv3,‚Éóv1 + ‚Éóv2 + ‚Éóv3} must be linearly independent.

489
Chapter 6 solutions
Answers to exercises
E6.1 Dom(T) = R3, Co(T) = R3, Ker(T) = {‚Éó0}, Im(T) = R3. T is injective and
surjective, and therefore bijective.
E6.2
[MT ]
B
B =
[1]
B
B‚Ä≤
[MT ]
B‚Ä≤
B‚Ä≤
[1]
B‚Ä≤
B.
Solutions to selected exercises
E6.2 Start from the formula for
[MT ]
B‚Ä≤
B‚Ä≤ and multiply it by
[1]
B
B‚Ä≤ from the
left and by
[1]
B‚Ä≤
B from the right:
[1]
B
B‚Ä≤
[MT ]
B‚Ä≤
B‚Ä≤
[1]
B‚Ä≤
B =
[1]
B
B‚Ä≤
[1]
B‚Ä≤
B
[MT ]
B
B
[1]
B
B‚Ä≤
[1]
B‚Ä≤
B
=
[MT ]
B
B .
Recall that
[1]
B
B‚Ä≤ is the inverse matrix of
[1]
B‚Ä≤
B so the two matrices cancel out.
E6.3 Let ‚Éóc1,‚Éóc2, . . . ,‚Éócn be the n columns of A. Since the columns of A form a
basis for Rn, any vector ‚Éób ‚ààRn can be written as a unique linear combination of
the columns of A: ‚Éób = x1‚Éóc1 +x2‚Éóc2 +¬∑ ¬∑ ¬∑+xn‚Éócn for some coeÔ¨Écients x1, x2, . . . , xn.
Reinterpreting the last equation as a matrix product in the column picture, we
conclude that A‚Éóx = ‚Éób has a unique solution ‚Éóx.
E6.4 Let Bs = {ÀÜe1, ÀÜe2, . . . , ÀÜen} be the standard basis for Rn.
Since A‚Éóx = ‚Éób
has a solution ‚Éóx for every possible ‚Éób, it is possible to Ô¨Ånd the solutions ‚Éóxi in
n equations of the form A‚Éóxi = ÀÜei, for i ‚àà{1, 2, . . . , n}.
Now construct the
matrix B that contains the solutions ‚Éóxi as columns: B = [‚Éóx1, . . . , ‚Éóxn]. Observe
that AB = A[‚Éóx1, . . . , ‚Éóxn] = [A‚Éóx1, . . . , A‚Éóxn] = [ÀÜe1, . . . , ÀÜen] = 1n. The equation
BA = 1n implies B ‚â°A‚àí1 and thus A is invertible.
Answers to problems
P6.1 Im(T) = span {(1, 1, 0), (0, ‚àí1, 2)}.
P6.2
 3
1
‚àí1 3

.
P6.3 D =
 0 1 0 0
0 0 2 0
0 0 0 3
0 0 0 0

.
Solutions to selected problems
P6.1 Applying T to the input vector (1, 0) produces (1, 1 ‚àí0, 2 ¬∑ 0) = (1, 1, 0),
and the input vector (0, 1) produces the output (0, 0 ‚àí1, 2 ¬∑ 1) = (0, ‚àí1, 2). Thus,
Im(T) = span {(1, 1, 0), (0, ‚àí1, 2)} ‚äÜR3.
P6.2 We apply L to both e2x cos x and e2x sin x
L(e2x cos x) = 2e2x cos x ‚àíe2x sin x + e2x cos x = 3e2x cos x ‚àíe2x sin x
L(e2x sin x) = 2e2x sin x + e2x cos x + e2x sin x = e2x cos x + 3e2x sin x.
The Ô¨Årst corresponds to the vector [3, ‚àí1],with respect to the given basis, and
the second corresponds to the vector [1, 3] with respect to the given basis. These
vectors are columns of the matrix representing L. Thus, the matrix representing
L with respect to the given basis is
 3
1
‚àí1 3

.
Chapter 7 solutions
Answers to exercises
E7.3 See math.stackexchange.com/a/29374/46349.
E7.6 Œ± = ‚àí3 and Œ≤ = 4.
E7.7 (1, 1)T and (1, ‚àí2)T are eigenvectors of L.
E7.8 No.
E7.9 det(A) = 15,

490
ANSWERS AND SOLUTIONS
A‚àí1 =
Ô£Æ
Ô£∞
1 ‚àí4
5 ‚àí11
3
0
1
5
‚àí1
3
0
0
1
3
Ô£π
Ô£ª; det(B) = xz, B‚àí1 =

1
x
0
‚àíy
xz
1
z

; det(C) = 1, C‚àí1 =
h 5 0
0 1
5
i
.
E7.10 ‚Éóv = (1, 0, 0, 1).
E7.11 V = [1, 0, 0, 1].
E7.12 The solution
space is one-dimensional and spanned by f(t) = e‚àít.
E7.13 Yes.
E7.14
‚Éóe1 = (4, 2), ‚Éóe2 = (‚àí1, 2).
E7.15 ÀÜe1 = ( 1
‚àö
2 ,
1
‚àö
2 , 0), ÀÜe2 = ( 1
‚àö
6 , ‚àí1
‚àö
6 ,
2
‚àö
6 ), and
ÀÜe3 = (‚àí1
‚àö
3 ,
1
‚àö
3 ,
1
‚àö
3 ).
E7.16 ÀÜe1 =
1
‚àö
2 , ÀÜe2 =
p
3/2x, and ÀÜe3 =
‚àö
5
‚àö
8 (3x2 ‚àí1).
E7.17 Q =
Ô£Æ
Ô£ØÔ£∞
1
‚àö
2
1
‚àö
6
‚àí1
‚àö
3
1
‚àö
2
‚àí1
‚àö
6
1
‚àö
3
0
2
‚àö
6
1
‚àö
3
Ô£π
Ô£∫Ô£ª, and R =
Ô£Æ
Ô£ØÔ£∞
‚àö
2
1
‚àö
2
1
‚àö
2
0
3
‚àö
6
1
‚àö
6
0
0
2
‚àö
3
Ô£π
Ô£∫Ô£ª.
E7.18 (a)
‚àí1 + i; (b) ‚àí14 + 23i; (c)
1
25 (26 + 7i).
Solutions to selected exercises
E7.1 The characteristic polynomial of the matrix has degree n, and an nth-degree
polynomial has at most n distinct roots.
E7.3 Proof by contradiction.
Assume you have n distinct eigenvalues Œªi and
eigenvectors {‚Éóei} which are linearly dependent: Pn
i=1 Œ±i‚Éóei = ‚Éó0 with some Œ±i Ã∏= 0.
If a nonzero combination of Œ±i could give the zero vector as a linear combination,
the equation (A ‚àíŒªnI) (P Œ±i‚Éóei) = (A ‚àíŒªnI)‚Éó0 = ‚Éó0 would be true. However, if
you expand the expression on the left, you'll see it's not equal to zero.
E7.4 Multiply both sides of the eigenvalue equation A‚ÉóeŒª = Œª‚ÉóeŒª by A to obtain
AA‚ÉóeŒª = ŒªA‚ÉóeŒª = Œª2‚ÉóeŒª. Thus Œª2 is an eigenvalue of A2.
E7.5 Multiply both sides of the eigenvalue equation A‚ÉóeŒª = Œª‚ÉóeŒª by A‚àí1 to obtain
A‚àí1A‚ÉóeŒª = ŒªA‚àí1‚ÉóeŒª.
After A‚àí1 cancels with A, we're left with the equation
‚ÉóeŒª = ŒªA‚àí1‚ÉóeŒª. Dividing both sides of the equation by Œª we obtain 1
Œª‚ÉóeŒª = A‚àí1‚ÉóeŒª,
which shows that Œª‚àí1 is an eigenvalue of A‚àí1.
E7.6 The characteristic polynomial of A is pA(Œª) = x2 ‚àíŒ≤x ‚àíŒ±. If we want the
eigenvalues of A to be 1 and 3, we must choose Œ± and Œ≤ so that pA(Œª) = (Œª ‚àí
1)(Œª‚àí3). Expanding the factored expression, we Ô¨Ånd (Œª‚àí1)(Œª‚àí3) = Œª2 ‚àí4Œª+3,
so Œ± = ‚àí3 and Œ≤ = 4.
E7.7 First we compute L(1, 1)T = (5, 5)T = 5(1, 1)T so (1, 1)T is an eigenvector,
with eigenvalue Œª = 5.
Next we compute L(1, ‚àí1)T = (1, 3)T Ã∏= Œ±(1, ‚àí1)T so
(1, ‚àí1)T is not an eigenvector.
We can also compute L(1, ‚àí2)T = (‚àí1, 2)T =
‚àí1(1, ‚àí2)T, which implies (1, ‚àí2)T is an eigenvector with eigenvalue ‚àí1. Since
2√ó2 matrix can have at most two eigenvectors, we don't need to check (2, ‚àí1)T‚Äî
we know its not an eigenvector.
E7.8 A matrix A is Hermitian if it satisÔ¨Åes A‚Ä† = A, which is not the case for the
given matrix.
E7.12 The solutions to the diÔ¨Äerential equation f‚Ä≤(t) + f(t) = 0 are of the form
f(t) = Ce‚àít, where C is an arbitrary constant. Since any solution in the solution
space can be written as a multiple of the function e‚àít, we say the solution space
is spanned by e‚àít. Since one function is suÔ¨Écient to span the solution space, the
solution space is one-dimensional.
E7.13 Consider an arbitrary second-degree polynomial p(x) = a0 + a1x + a2x2.
We can rewrite it as
p(x) = a0 + a1[(x ‚àí1) + 1] + a2[(x ‚àí1) + 1]2
= a0 + a1(x ‚àí1) + a1 + a2[(x ‚àí1)2 + 2(x ‚àí1) + 1]
= (a0 + a1 + a2)1 + (a1 + 2a2)(x ‚àí1) + a2(x ‚àí1)2.

491
Since we've expressed an arbitrary polynomial of degree 2 in the desired form, the
answer is yes. In other words, the set {1, x ‚àí1, (x ‚àí1)2} is a basis for the vector
space of polynomials of degree at most 2.
E7.14 The exercise does not require us to normalize the vectors, so we can leave
the Ô¨Årst vector as is ‚Éóv1 = ‚Éóe1 = (4, 2). Next, we calculate ‚Éóe2 using the formula
‚Éóe2 = ‚Éóv2 ‚àíŒ†‚Éóe1(‚Éóv2), which corresponds to removing the component of ‚Éóv2 that
lies in the direction of ‚Éóe1.
Using the projection formula we obtain Œ†‚Éóe1(‚Éóv2) ‚â°
(4,2)¬∑(1,3)
‚à•(4,2)‚à•2 (4, 2) =
4+6
16+4 (4, 2) =
1
2 (4, 2) = (2, 1).
Thus ‚Éóe2 = ‚Éóv2 ‚àíŒ†‚Éóe1(‚Éóv2) =
(1, 3) ‚àí(2, 1) = (‚àí1, 2). Verify that ‚Éóe1 ¬∑ ‚Éóe2 = 0.
Answers to problems
P7.1
(a) Œª1 = 6, Œª2 = ‚àí1; (b) Œª1 = ‚àí2, Œª2 = 2, Œª3 = 0.
P7.2 Œª1 =
œï ‚â°1+
‚àö
5
2
= 1.6180339 . . .; Œª2 = ‚àí1
œï = 1‚àí
‚àö
5
2
= ‚àí0.6180339 . . ..
P7.3 Œª1 = œï
and Œª2 = ‚àí1
œï.
P7.4 X = Q‚àí1 =
"
5+
‚àö
5
10
‚àö
5
5
5‚àí
‚àö
5
10
‚àí
‚àö
5
5
#
.
P7.5
(a) Œª1 = 5,
Œª2 = 4; (b) Œª1 =
1
2 (5 +
p
(5)), Œª2 =
1
2 (5 ‚àí
p
(5)); (c) Œª1 = 3, Œª2 = Œª3 = 0
and (d) Œª1 = ‚àí3, Œª2 = ‚àí1, Œª3 = 1.
P7.6
(a) Œª1 = 1, ‚ÉóeŒª1 = (1, 1)T,
Œª2 = ‚àí1, ‚ÉóeŒª2 = (1, ‚àí1)T; (b) Œª1 = 3, ‚ÉóeŒª1 = (1, 3, 9)T, Œª2 = 2, ‚ÉóeŒª2 = (1, 2, 4)T,
Œª3 = ‚àí1, ‚ÉóeŒª3 = (1, ‚àí1, 1)T.
P7.7 A10 =
h 2
2
5 ‚àí1
i10
=
h 765854 282722
706805 341771
i
.
P7.8 (x‚àû, y‚àû, z‚àû)T = 1
6 (x0 + 4y0 + z0, x0 + 4y0 + z0, x0 + 4y0 + z0)T.
P7.9
(a) Yes; (b) No; (c) Yes.
P7.10 No.
P7.11 No.
P7.14 ÀÜe1 = (0, 1),
ÀÜe2 = (‚àí1, 0).
P7.15 ÀÜe1 = ( 1
‚àö
2 ,
1
‚àö
2 ), ÀÜe2 = (‚àí1
‚àö
2 ,
1
‚àö
2 ).
P7.16 ÀÜe1 = (
3
‚àö
10 ,
1
‚àö
10 ),
ÀÜe2 = ( ‚àí1
‚àö
10 ,
3
‚àö
10 ).
P7.17 A = QŒõQ‚àí1 =

2 0 ‚àí5
0 2
0
0 0 ‚àí3

=
h 1 0 1
0 1 0
1 0 0
i h ‚àí3 0 0
0
2 0
0
0 2
i h 0 0
1
0 1
0
1 0 ‚àí1
i
.
P7.18 a) 5;b) 2 + 3i; c) 2 + 5i; d)
‚àö
26.
P7.19 A + B =

4
2
8+3i ‚àí5+3i

; CB =
 3+8i
2‚àíi
45+3i ‚àí33+31i
16‚àí4i
16+8i

; (2 + i)B =
h
5
8‚àíi
9+7i ‚àí15+5i
i
.
P7.20
(a) Œª1 = 2 + i and
Œª2 = 2 ‚àíi; (b) Œª1 = 3
‚àö
3i and Œª2 = 3
‚àö
3i; (c) Œª1 = 2 + 8i and Œª2 = 2 ‚àí8i.
P7.21
n
e1 =
 1 0 0
0 0 0
0 0 0

, e2 =
 0 0 0
0 1 0
0 0 0

, e3 =
 0 0 0
0 0 0
0 0 1
o
.
P7.22 d = 6.
P7.23
d = 9.
P7.24 (a) Yes. (b) No. (c) Yes. (d) No. (e) Yes. (f) Yes.
P7.25
Q =
 ‚àí1+i
1+i
2
2

, Œõ =
 0 0
0 2

.
P7.26 (a) 9; (b) 3; (c)
‚àö
10.
Solutions to selected problems
P7.2 To Ô¨Ånd the eigenvalues of the matrix A we must Ô¨Ånd the roots of it's char-
acteristic polynomial
p(Œª) = det(A ‚àíŒª1) = det
1
1
1
0

‚àí
Œª
0
0
Œª

= det
1 ‚àíŒª
1
1
‚àíŒª

.
Using the determinant formula det
  a b
c d

= ad ‚àíbc, we Ô¨Ånd the characteristic
polynomial of A is p(Œª) = Œª2 ‚àíŒª ‚àí1. The eigenvalues of A are the roots Œª1 and
Œª2 of this equation, which we can Ô¨Ånd using the formula for solving quadratic
equations we saw in Section 1.6 (see page 28).
P7.3 The vector ‚Éóe1 is an eigenvector of A because A‚Éóe1 =
h 1 1
1 0
i 1
1
œï

=

1+ 1
œï
1

.
Now observe the following interesting fact:
1
œï(1 + 1
œï) =
1
œï +
1
œï2 =
œï+1
œï2
= 1.
This means we can write A‚Éóe1 =
h 1 1
1 0
i 1
1
œï

= œï
 1
1
œï

, which shows that ‚Éóe1 is
an eigenvector of A and it corresponds to eigenvalue Œª1 = œï. Similar reasoning

492
ANSWERS AND SOLUTIONS
shows A‚Éóe2 = ‚àí1
œï‚Éóe2 so ‚Éóe2 is an eigenvector of A that corresponds to eigenvalue
Œª2 = ‚àí1
œï.
P7.4 The eigendecomposition of matrix A is A = QŒõQ‚àí1. The unknown matrix
X is the inverse matrix of the matrix Q =
h 1
1
1
œï ‚àíœï
i
=

1
1
2
1+
‚àö
5
‚àí1+
‚àö
5
2

. To Ô¨Ånd
Q‚àí1 we can start from the array [ Q | 1 ] and perform row operations until we
obtain [ 1 | Q‚àí1 ].
P7.6 (a) First we obtain the characteristic polynomial.
(b) The characteristic polynomial is
P7.7 First we decompose A as the product of three matrices A = QŒõQ‚àí1, where
Q is a matrix of eigenvectors, and Œõ contains the eigenvalues of A. A =
 2
2
5 ‚àí1

=
 ‚àí2 1
5
1
 ‚àí3 0
0
4
 ‚àí1
7
1
7
5
7
2
7

. Since the matrix Œõ is diagonal, we can compute it's Ô¨Åfth
power. Œõ10 =
h
59049
0
0
1048576
i
Thus expressing the calculation of A10.
A10 =
‚àí2
1
5
1
 59049
0
0
1048576
  ‚àí1
7
1
7
5
7
2
7

=
765854
282722
706805
341771

.
P7.8 The eigenvalues of M are 1
4 , 1
2 , and 1, and its eigendecomposition is M =
QŒõQ‚àí1. We can compute (x‚àû, y‚àû, z‚àû)T using M‚àû(x0, y0, z0)T. To compute
M‚àû, we can compute Œõ‚àû. The 1
4 and 1
2 eigenspaces will disappear, so we'll be
left only with the subspace of the eigenvalue 1. M‚àû= QŒõ‚àûQ‚àí1, and each row
of this matrix has the form [ 1
6 , 4
6 , 1
6 ]. See bit.ly/eigenex001 for details.
P7.9 To check if the matrix is orthogonal the transpose of that matrix should act
as an inverse such that OT O = 1.
P7.10 A vector space would obey 0 ¬∑ (a1, a2) = (0, 0) (the zero vector), but we
have 0 ¬∑ (a1, a2) = (0, a2) Ã∏= (0, 0), so (V, R, +, ¬∑) is not a vector space.
P7.11 The vector addition operation is not associative:
we have ((a1, a2) +
(b1, b2)) + (c1, c2) = (a1 + 2b1 + 2c1, a2 + 3b2 + 3c2) but (a1, a2) + ((b1, b2) +
(c1, c2)) = (a1 + 2b1 + 4c1, a2 + 3b2 + 9c2).
P7.12 If v = 0, then the inequality holds trivially. If v Ã∏= 0, we can start from
the following which holds for any c ‚ààC:
0 ‚â§‚ü®u ‚àícv, u ‚àícv‚ü©
= ‚ü®u, u ‚àícv‚ü©‚àíc‚ü®v, u ‚àícv‚ü©
= ‚ü®u, u‚ü©‚àíc‚ü®u, v‚ü©‚àíc‚ü®v, u‚ü©+ |c|2‚ü®v, v‚ü©.
This is true in particular when c = ‚ü®u,v‚ü©
‚ü®v,v‚ü©, so we continue
0 ‚â§‚ü®u, u‚ü©‚àí‚ü®u, v‚ü©‚ü®u, v‚ü©
‚ü®v, v‚ü©
‚àí‚ü®u, v‚ü©‚ü®u, v‚ü©
‚ü®v, v‚ü©
+ |‚ü®u, v‚ü©|2‚ü®v, v‚ü©
‚ü®v, v‚ü©2
0 ‚â§‚ü®u, u‚ü©‚àí|‚ü®u, v‚ü©|2
‚ü®v, v‚ü©
‚àí|‚ü®u, v‚ü©|2
‚ü®v, v‚ü©
+ |‚ü®u, v‚ü©|2
‚ü®v, v‚ü©
0 ‚â§‚ü®u, u‚ü©‚àí|‚ü®u, v‚ü©|2
‚ü®v, v‚ü©
0 ‚â§‚ü®u, u‚ü©‚ü®v, v‚ü©‚àí|‚ü®u, v‚ü©|2,
from which we conclude |‚ü®u, v‚ü©|2 ‚â§‚à•u‚à•2‚à•v‚à•2. Taking the square root on both
sides we obtain the the Cauchy-Schwarz inequality |‚ü®u, v‚ü©| ‚â§‚à•u‚à•‚à•v‚à•.

493
P7.13 We proceed using the following chain of inequalities:
‚à•u + v‚à•2 = ‚ü®u + v, u + v‚ü©
= ‚ü®u, u‚ü©+ ‚ü®u, v‚ü©+ ‚ü®v, u‚ü©+ ‚ü®v, v‚ü©
‚â§‚à•u‚à•2 + 2‚ü®u, v‚ü©+ ‚à•v‚à•2
‚â§‚à•u‚à•2 + 2‚à•u‚à•‚à•v‚à•+ ‚à•v‚à•2
= (‚à•u‚à•+ ‚à•v‚à•)2
Therefore we obtain the equation ‚à•u + v‚à•2 ‚â§(‚à•u‚à•+ ‚à•v‚à•)2, and since ‚à•u‚à•and
‚à•v‚à•are non-negative numbers, we can take the square root on both sides of the
equation to obtain ‚à•u + v‚à•‚â§‚à•u‚à•+ ‚à•v‚à•.
P7.14 Let ‚Éóv1 = (0, 1) and ‚Éóv2 = (‚àí1, 0). We don't really need to perform the
Gram-Shmidt procedure since ‚Éóv2 is already perpendicular to ‚Éóv1 and both vectors
have unit length.
P7.15 We're given the vectors ‚Éóv1 = (1, 1) and ‚Éóv2 = (0, 1) and want to perform
the Gram-Shmidt procedure. We pick ‚Éóe1 = ‚Éóv1 = (1, 1) and after normalization
we have ÀÜe1 = ( 1
‚àö
2 ,
1
‚àö
2 ). Next we compute ‚Éóe2 = ‚Éóv2 ‚àíŒ†ÀÜe1(‚Éóv2) = ‚Éóv2 ‚àí(ÀÜe1 ¬∑ ‚Éóv2)ÀÜe1 =
( ‚àí1
2 , 1
2 ). Normalizing ‚Éóe2 we obtain ÀÜe2 = (‚àí1
‚àö
2 ,
1
‚àö
2 ).
P7.16 Let ‚Éóv1 = (3, 1) and ‚Éóv2 = (‚àí1, 1). We start by identifying ‚Éóe1 = ‚Éóv1, then
perform Gram-Shmidt process to Ô¨Ånd ‚Éóe2 from ‚Éóv2:
‚Éóe2 = ‚Éóv2 ‚àíŒ†ÀÜe1(‚Éóv2)ÀÜe1 = ‚Éóv2 ‚àí
 ‚Éóe1
‚à•‚Éóe1‚à•¬∑ ‚Éóv2
 ‚Éóe1
‚à•‚Éóe1‚à•=
 ‚àí2
5 , 6
5

.
Now we have two orthogonal vectors and we can normalize them to make them
unit length. We obtain the vectors (
3
‚àö
10 ,
1
‚àö
10 ) and ( ‚àí1
‚àö
10 ,
3
‚àö
10 ) which form an
orthogonal basis.
P7.17 First you have to Ô¨Ånd eigenvalues of the given matrix.1,1 Then Ô¨Ånd an
eigenvector for each eigenvalue. Then construct a matrix Q composed of the 3
eigenvectors. Finally, write the eigendecomposition of the form A = QŒõQ‚àí1.
P7.18 a)
‚àö
32 + 42 = 5; b) Only the sign of imaginary part changes so it becomes
2 + 3i; c) 3i ‚àí1 + 3 + 2i = 2 + 5i; d) |3i ‚àí4i ‚àí5| = | ‚àí5 ‚àíi| =
‚àö
26.
P7.21 First of all we must determine dimensionality of the vector space in ques-
tion. The general vector space of 3 √ó 3 matrices has 9 dimensions, but a diagonal
matrix A satisÔ¨Åes aij = 0 for all i Ã∏= j, which corresponds to the following six
constraints {a12 = 0, a13 = 0, a21 = 0, a23 = 0, a31 = 0, a32 = 0}. The vector
space of diagonal matrices is therefore three dimensional. The answer given is the
standard basis. Other answers are possible so long as they span the same space,
and there are three of them.
P7.22 A matrix A ‚ààR3√ó3 is symmetric if and only if AT = A. This means we
can pick the entries on the diagonal arbitrarily, but the symmetry requirement
leads to the constraints a12 = a21, a13 = a31, and a23 = a32. Thus space of 3 √ó 3
symmetric matrices is six dimensional.
P7.23 A Hermitian matrix H is a matrix with complex coeÔ¨Écients that satisÔ¨Åes
H = H‚Ä†, or equivalently hij = hji, for all i, j.
A priori the space of 3 √ó 3
matrices with complex coeÔ¨Écients is 18 dimensional, for the real and imaginary
parts of each of the nine coeÔ¨Écients. The Hermitian property imposes two types
of constraints. Diagonal elements must be real if we want hii = hii to be true,
which introduces three constraints. Once we pick the real and imaginary part of
an oÔ¨Ä-diagonal element aij, we're forced to choose aji = aij, which are another 6
constraints. Thus, the vector space of 3 √ó 3 Hermitian matrices is 18 ‚àí3 ‚àí6 = 9
dimensional.

494
ANSWERS AND SOLUTIONS
P7.24 To prove a matrix is nilpotent we can either compute its powers to see if
we get the zero matrix. To prove a matrix is not nilpotent, you can show it has a
non-zero eigenvalue.
(a) This matrix is nilpotent because its square is the zero matrix.
(b) The matrix is not nilpotent because its characteristic polynomial p(Œª) = Œª2 ‚àí
6Œª + 8 is diÔ¨Äerent from the all-zero eigenvalues characteristic polynomial p(Œª) =
(Œª ‚àí0)(Œª ‚àí0) = Œª2.
(c) This matrix is nilpotent because it squares to the zero matrix.
(d) The matrix is not nilpotent because it has non-zero eigenvalues.
(e) Yes, the cube of this matrix is the zero matrix.
(f) Yes, the square of this matrix is the zero matrix.
P7.25 The characteristic polynomial of A is pA(Œª) = Œª2 ‚àí2Œª = Œª(Œª ‚àí2) so
the eigenvalues are 0 and 2. The eigenvector for Œª = 0 is ‚Éóe0 = (‚àí1 + i, 2)T. The
eigenvector for Œª = 2 is ‚Éóe2 = (1+i, 2)T. As A has two one-dimensional eigenspaces,
an eigenbasis is given by {(‚àí1 + i, 2)T, (1 + i, 2)T}. So A is diagonalizable with
Q =
 ‚àí1+i 1+i
2
2

and Œõ =
 0 0
0 2

.
P7.26 (a) Using linearity: ‚ü®‚Éóv1, 2‚Éóv2 + 3‚Éóv3‚ü©= 2‚ü®‚Éóv1,‚Éóv2‚ü©+ 3‚ü®‚Éóv1,‚Éóv3‚ü©= 6 + 3 = 9.
(b) Using linearity in both entries: ‚ü®2‚Éóv1 ‚àí‚Éóv2,‚Éóv1 + ‚Éóv3‚ü©= 2‚ü®‚Éóv1,‚Éóv1‚ü©+ 2‚ü®‚Éóv1,‚Éóv3‚ü©‚àí
‚ü®‚Éóv2,‚Éóv1‚ü©‚àí‚ü®‚Éóv2,‚Éóv3‚ü©= 2‚ü®‚Éóv1,‚Éóv1‚ü©+ 2‚ü®‚Éóv1,‚Éóv3‚ü©‚àí‚ü®‚Éóv1,‚Éóv2‚ü©‚àí‚ü®‚Éóv2,‚Éóv3‚ü©= 2 + 2 ‚àí3 + 2 = 3.
(c) We start with 13 = ‚ü®‚Éóv2,‚Éóv1 + ‚Éóv2‚ü©= ‚ü®‚Éóv2,‚Éóv1‚ü©+ ‚ü®‚Éóv2,‚Éóv2‚ü©, and since we know
‚ü®‚Éóv1,‚Éóv2‚ü©= 3, we obtain 3 + ‚à•‚Éóv2‚à•2 = 13, so ‚à•‚Éóv2‚à•2 = 10 and ‚à•‚Éóv2‚à•=
‚àö
10.
P7.27 (a) There are several ways to answer this question. One way is to note
that the dimension of P2 is 3 and so if Ba is a linearly independent set, then it is
a basis as it has 3 elements. If a(1 + ix) + b(1 + x + ix2) + c(1 + 2ix) = 0 then
(a + b + c)1 + (ia + b + 2ic)x + ibx2 = 0. But the standard basis of P2 is linearly
independent and so we must have a + b + c = 0, ia + b + 2ic = 0 and ib = 0. The
last equation implies b = 0 and then the Ô¨Årst two imply both a and c are zero. As
the only linear combination of distinct elements of Ba which sums to zero is the
trivial sum, Ba is linearly independent.
Chapter 8 solutions
Answers to exercises
E8.1 4Al + 3O2 ‚Üí2Al2O3.
E8.2 Fe(OH)3 + 3HCl ‚ÜíFeCl3 + 3H2O.
E8.3
(a) A =
h 0 1 0
0 0 1
1 1 0
i
; (b) A =
h 0 1 0
0 1 1
0 0 0
i
; (c) A =
h 0 1 1
1 0 1
1 1 0
i
.
E8.4
(a) 1; (b) 1;
(c) 2.
E8.5 limn‚Üí‚àû
an+1
an
= Œª1 = œï = 1+
‚àö
5
2
.
E8.6 aN =
1
‚àö
5

1+
‚àö
5
2
N
‚àí
1
‚àö
5

1‚àí
‚àö
5
2
N
.
E8.7 S(m‚àó) = 4704.63.
E8.8 S(‚Éóm‚Ä≤‚àó) = 433.54.
E8.9 ‚Éóc =
‚ÉóxG = (1, 0, 1, 0, 1, 0, 1).
E8.10 A 7 √ó 7 identity matrix next to an all-ones
vectors.
Solutions to selected exercises
E8.3 The ith row of the adjacency matrix contains the information of the outgoing
edges for vertex i in the graph. If the edge (i, j) exists, then Aij = 1, otherwise
Aij = 0.
E8.6 Another expression is Fn = œïn‚àí(‚àíœï)‚àín
‚àö
5
.
E8.11 To Ô¨Ånd the inner product ‚ü®e1(x), e2(x)‚ü©, we must compute the integral
R L
0 sin( œÄ
L x) sin( 2œÄ
L x) dx. We can change variables y =
œÄ
L x to simplify the inte-
gral, changing also dx ‚Üí
L
œÄ dy and the limits.
We thus have ‚ü®e1(x), e2(x)‚ü©=

495
k
R œÄ
0 sin(y) sin(2y) dy, where k =
L
œÄ .
Using the double angle formula we Ô¨Ånd
sin(y) sin(2y) = sin(y)2 sin(y) cos(y) = 2 sin2(y) cos(y). We proceed using the sub-
stitution u = sin(y), du = cos(y)dy to obtain 2k
R
sin2(y) cos(y) dy = 2k
R
u2du =
2k
3

u3
=
2k
3 sin3(y).
Finally, we evaluating the expression at the endpoints:
‚ü®e1(x), e2(x)‚ü©= 2k
3

sin3(œÄ) ‚àísin3(0)

= 0, conÔ¨Årming orthogonality.
E8.12 The coeÔ¨Écient b0 corresponds to the basis function sin( 0œÄ
T t), which is zero
everywhere. Thus, the integral b0 =
1
T
R T
0 f(t) sin( 2œÄ0
T t) dt, always produces a
zero result b0 = 0. On the other hand, the zero-frequency cos function is constant
cos( 2œÄ0
T t) = 1, and the coeÔ¨Écient a0 = 1
T
R T
0 f(t)1 dt corresponds to the average
value of the function f(t) on the interval [0, T].
Answers to problems
P8.1
(a) A =
" 0 1 0 0 1
1 0 1 1 0
0 1 0 1 0
0 0 1 0 1
1 0 0 1 0
#
; (b) A =
" 0 1 0 0 1
0 0 1 0 1
0 0 0 1 1
1 0 0 0 1
0 0 0 0 0
#
; (c) A =
" 0 1 1 1 1
1 0 1 0 0
0 1 0 1 0
0 0 1 0 1
1 0 0 1 0
#
.
P8.2
(a) 1; (b) 1; (c) 3.
Solutions to selected problems
P8.1 The ith row of the adjacency matrix contains the information of the outgoing
edges for vertex i in the graph. If the edge (i, j) exists, then Aij = 1, otherwise
Aij = 0.
P8.3 Decompose the formula for cn into a its real part and imaginary parts:
cn =
Z T
0
D
ei 2œÄn
T
t, et
E
f(t) dt
= 1
T
Z T
0
f(t)e‚àíi 2œÄn
T
t dt
= Re

1
T
Z T
0
f(t)e‚àíi 2œÄn
T
t dt

+ Im

1
T
Z T
0
f(t)e‚àíi 2œÄn
T
t dt

i
= 1
T
Z T
0
f(t)Re
n
e‚àíi 2œÄn
T
to
dt + 1
T
Z T
0
f(t)Im
n
e‚àíi 2œÄn
T
to
i dt
= 1
T
Z T
0
f(t) cos
  2œÄn
T t

dt ‚àí1
T
Z T
0
f(t) sin
  2œÄn
T t

i dt.
We can recognize the real part of cn as the cosine coeÔ¨Écients an of the Fourier
series, and the imaginary part of cn as the negative of the sine coeÔ¨Écients bn of
the Fourier series:
Re {cn} = Re

1
T
Z T
0
f(t)e‚àíi 2œÄn
T
t dt

= 1
T
Z T
0
f(t) cos
  2œÄn
T t

dt = an,
Im {cn} = Im

1
T
Z T
0
f(t)e‚àíi 2œÄn
T
t dt

= 1
T
Z T
0
f(t) sin
 ‚àí2œÄn
T t

dt = ‚àíbn.
Thus we have shown cn = an ‚àíibn.
Using the simple deÔ¨Ånition of the coeÔ¨Écients cn above, the synthesis equation
for the complex Fourier transform is a somewhat awkward expression f(t) = c0 +
1
2
P‚àû
n=1 e‚àíi 2œÄn
T
tcn + 1
2
P‚àí1
n=‚àí‚àûe‚àíi 2œÄn
T
tcn. Many textbooks describe complex
Fourier series in terms of the two-sided coeÔ¨Écients c‚Ä≤
n, n ‚ààZ deÔ¨Åned as
c‚Ä≤
0 = c0,
c‚Ä≤
n = 1
2 cn = 1
2 (an ‚àíibn)
for n ‚â•1,
c‚Ä≤
‚àín = 1
2 cn = 1
2 (an + ibn)
for n ‚â•1.

496
ANSWERS AND SOLUTIONS
Using the coeÔ¨Écients c‚Ä≤
n, the synthesis equation for the complex Fourier series is
the simpler expression f(t) = P‚àû
n=‚àí‚àûe‚àíi 2œÄn
T
tc‚Ä≤
n.
Chapter 9 solutions
Answers to exercises
E9.1
(a) No; weights don't add to one. (b) Yes. (c) No; contains a negative
number.
E9.4 pX‚àû=
  34
61 , 14
61 , 13
61

.
E9.5 pX‚àû=
  34
61 , 14
61 , 13
61

.
E9.6
pX‚àû= (0.2793, 0.1457, 0.2768, 0.02, 0.2781)T.
Solutions to selected exercises
E9.4 Use ev = C.eigenvects()[0][2][0] to extract the eigenvector that corre-
spond to the eigenvalue Œª = 1, then normalize the vector by its 1-norm to make
it a probability distribution pinf = ev/ev.norm(1) =
  34
61 , 14
61 , 13
61

.
E9.5 DeÔ¨Åne the matrix C then use ev = (C-eye(3)).nullspace()[0] to obtain
an eigenvector that corresponds the eigenvalue Œª = 1. To obtain a probability dis-
tribution, compute ev/ev.norm(1) =
  34
61 , 14
61 , 13
61

. Using the nullspace method
is a more targeted approach for Ô¨Ånding stationary distributions than using the
eigenvects method. We don't need to compute all the eigenvectors of C, just the
one we're interested in.
E9.6 You can see the solution at this URL: bit.ly/21GOUCe.
Answers to problems
P9.1 Pr({heads, heads, heads}) =
1
8 .
P9.2 pX1 = ( 1
2 , 1
2 )T, pX2 = ( 3
8 , 5
8 )T,
pX‚àû= ( 1
3 , 2
3 )T.
Solutions to selected problems
P9.1 Substitute p = 1
2 and n = 3 into the expression pn.
P9.2 DeÔ¨Åned Xi to be probability distribution of the weather in Year i. The
transition matrix for the weather Markov chain is M =
 1
2
1
4
1
2
3
4

. We obtain the
weather in Year 1 using pX1 = M(1, 0)T = ( 1
2 , 1
2 )T. The weather in Year 2 is
pX2 = M2(1, 0)T = ( 3
8 , 5
8 )T. The long term (stationary) distribution is pX‚àû=
M‚àû(1, 0)T = ( 1
3 , 2
3 )T.
Chapter 10 solutions
Answers to exercises
E10.1 ‚Éóu = (Œ±, 0, 0Œ≤)T (column vector), ‚ÉóvT = (0, a, b, 0) (row vector).
E10.2
|w‚ü©= 1|0‚ü©+ i|1‚ü©‚àí|2‚ü©, ‚ü®w| = 1‚ü®0| ‚àíi‚ü®1| ‚àí‚ü®2|, ‚à•‚Éów‚à•=
‚àö
3.
E10.3 det(A) = ‚àí2.
E10.6 See solution.
E10.7 Pr({‚àí}|œà) = (Œ±‚àíŒ≤)2
2
.

497
Solutions to selected exercises
E10.5 Note XY is not the same as Y X.
E10.6 We can rewrite the deÔ¨Ånition of |+‚ü©and |‚àí‚ü©in order to obtain expressions
for the elements of the standard basis |0‚ü©=
1
‚àö
2 |+‚ü©+
1
‚àö
2 |‚àí‚ü©, and |1‚ü©=
1
‚àö
2 |+‚ü©‚àí
1
‚àö
2 |‚àí‚ü©. The remainder of the procedure is straightforward:
|01‚ü©‚àí|10‚ü©= |0‚ü©|1‚ü©‚àí|1‚ü©|0‚ü©
=

1
‚àö
2 |+‚ü©+
1
‚àö
2 |‚àí‚ü©

1
‚àö
2 |+‚ü©‚àí
1
‚àö
2 |‚àí‚ü©

‚àí

1
‚àö
2 |+‚ü©‚àí
1
‚àö
2 |‚àí‚ü©

1
‚àö
2 |+‚ü©+
1
‚àö
2 |‚àí‚ü©

= 1
2


|+‚ü©|+‚ü©‚àí|+‚ü©|‚àí‚ü©+ |‚àí‚ü©|+‚ü©‚àí
|‚àí‚ü©|‚àí‚ü©
‚àí
|+‚ü©|+‚ü©‚àí|+‚ü©|‚àí‚ü©+ |‚àí‚ü©|+‚ü©+
|‚àí‚ü©|‚àí‚ü©

= 1
2

‚àí2|+‚ü©|‚àí‚ü©+ 2|‚àí‚ü©|+‚ü©

= ‚àí(|+‚ü©|‚àí‚ü©‚àí|‚àí‚ü©|+‚ü©)
= |+‚ü©|‚àí‚ü©‚àí|‚àí‚ü©|+‚ü©.
The last equality holds because the global phase of the states doesn't matter.
Answers to problems
P10.1 No, since Q is not unitary.
P10.2 HH|0‚ü©= |0‚ü©and HH|1‚ü©= |1‚ü©.
P10.3
2 parameters.
P10.4 Œ± ‚àà[0, 1] and œï ‚àà[0, 2œÄ].
P10.5 Œ∏ ‚àà[0, œÄ] and œï ‚àà[0, 2œÄ].
P10.6 |R3‚ü©= |œà‚ü©3, since the state was teleported to Bob's register.
Solutions to selected problems
P10.1 Quantum operators are unitary. Since QQ‚Ä† Ã∏= 1, Q is not unitary and it
cannot be implemented by any physical device. The boss is not always right!
P10.2 Let's Ô¨Årst see what happens to |0‚ü©when we apply the operator HH. The
result of the Ô¨Årst H applied to |0‚ü©is H|0‚ü©= |+‚ü©=
1
‚àö
2 (|0‚ü©+ |1‚ü©). Then apply-
ing the second H operator we get HH|0‚ü©= H|+‚ü©=
1
‚àö
2 (H|0‚ü©+ H|1‚ü©). Apply-
ing the H operation gives
1
‚àö
2

1
‚àö
2 (|0‚ü©+ |1‚ü©) +
1
‚àö
2 (|0‚ü©‚àí|1‚ü©)

, which simpliÔ¨Åes to
1
‚àö
2

2
‚àö
2 |0‚ü©

= |0‚ü©. So HH|0‚ü©= |0‚ü©. A similar calculation shows HH|1‚ü©= |1‚ü©.
P10.3 Starting from the four degrees of freedom for a general two-dimensional
complex vectors, we must subtract one degree of freedom for each of the con-
straints: one because we're ignoring global phase, and one because we require
|Œ±|2 + |Œ≤|2 = 1:
4 d.f. ‚àíŒ± real ‚àí{‚à•|œà‚ü©‚à•= 1} = 2 d.f.
A qubit |œà‚ü©has only two degrees of freedom. In other words two parameters are
suÔ¨Écient to describe any qubit.
P10.5 These are the angles on the Bloch sphere.
P10.6 Check this video for a sketch of the solution: youtu.be/3wZ35c3oYUE.


Appendix B
Notation
This appendix contains a summary of the notation used in this book.
Math notation
Expression
Read as
Used to denote
a, b, x, y
variables
=
is equal to
expressions that have the same value
‚â°
is deÔ¨Åned as
new variable deÔ¨Ånitions
a + b
a plus b
the combined lengths of a and b
a ‚àíb
a minus b
the diÔ¨Äerence in lengths between a and b
a √ó b ‚â°ab
a times b
the area of a rectangle
a2 ‚â°aa
a squared
the area of a square of side length a
a3 ‚â°aaa
a cubed
the volume of a cube of side length a
an
a exponent n
a multiplied by itself n times
‚àöa ‚â°a
1
2
square root of a
the side length of a square of area a
3‚àöa ‚â°a
1
3
cube root of a
the side length of a cube with volume a
a/b ‚â°a
b
a divided by b
a parts of a whole split into b parts
a‚àí1 ‚â°1
a
one over a
division by a
f(x)
f of x
the function f applied to input x
f ‚àí1
f inverse
the inverse function of f(x)
f ‚ó¶g
f compose g
function composition; f ‚ó¶g(x) ‚â°f(g(x))
ex
e to the x
the exponential function base e
ln(x)
natural log of x
the logarithm base e
ax
a to the x
the exponential function base a
loga(x)
log base a of x
the logarithm base a
Œ∏, œÜ
theta, phi
angles
sin, cos, tan
sin, cos, tan
trigonometric ratios
%
percent
proportions of a total; a% ‚â°
a
100
499

500
NOTATION
Set notation
You don't need a lot of fancy notation to do math, but it really helps
if you know a little bit of set notation.
Symbol
Read as
Denotes
{ . . . }
the set . . .
deÔ¨Ånition of a set
|
such that
describe or restrict the elements of a set
N
the naturals
the set N ‚â°{0, 1, 2, . . .}. Also N+‚â°N\{0}.
Z
the integers
the set Z ‚â°{. . . , ‚àí2, ‚àí1, 0, 1, 2, 3, . . .}
Q
the rationals
the set of fractions of integers
R
the reals
the set of real numbers
C
the set of complex numbers
Fq
Ô¨Ånite Ô¨Åeld
the set {0, 1, 2, 3, . . . , q ‚àí1}
‚äÇ
subset
one set strictly contained in another
‚äÜ
subset or equal
containment or equality
‚à™
union
the combined elements from two sets
‚à©
intersection
the elements two sets have in common
S \ T
S set minus T
the elements of S that are not in T
a ‚ààS
a in S
a is an element of set S
a /‚ààS
a not in S
a is not an element of set S
‚àÄx
for all x
a statement that holds for all x
‚àÉx
there exists x
an existence statement
‚àÑx
there doesn't exist x
a non-existence statement
An example of a condensed math statement that uses set notation
is "‚àÑm, n ‚ààZ such that
m
n =
‚àö
2," which reads "there don't exist
integers m and n whose fraction equals
‚àö
2." Since we identify the set
of fraction of integers with the rationals, this statement is equivalent
to the shorter "
‚àö
2 /‚ààQ," which reads "
‚àö
2 is irrational."
Vectors notation
Expression
Denotes
Rn
the set of n-dimensional real vectors
‚Éóv
a vector
(vx, vy)
vector in component notation
vxÀÜƒ± + vyÀÜÔöæ
vector in unit vector notation
‚à•‚Éóv‚à•‚à†Œ∏
vector in length-and-direction notation
‚à•‚Éóv‚à•
length of the vector ‚Éóv
Œ∏
angle the vector ‚Éóv makes with the x-axis
ÀÜv ‚â°
‚Éóv
‚à•‚Éóv‚à•
unit length vector in the same direction as ‚Éóv
‚Éóu ¬∑ ‚Éóv
dot product of the vectors ‚Éóu and ‚Éóv
‚Éóu √ó ‚Éóv
cross product of the vectors ‚Éóu and ‚Éóv

COMPLEX NUMBERS NOTATION
501
Complex numbers notation
Expression
Denotes
C
the set of complex numbers C ‚â°{a + bi | a, b ‚ààR}
i
the unit imaginary number i ‚â°‚àö‚àí1 or i2 = ‚àí1
Re{z} = a
real part of z = a + bi
Im{z} = b
imaginary part of z = a + bi
|z|‚à†œïz
polar representation of z = |z| cos œïz + i|z| sin œïz
|z| =
‚àö
a2 + b2
magnitude of z = a + bi
œïz = tan‚àí1(b/a)
phase or argument of z = a + bi
¬Øz = a ‚àíbi
complex conjugate of z = a + bi
Cn
the set of n-dimensional complex vectors
Vector space notation
Expression
Denotes
U, V, W
vector spaces
W ‚äÜV
vector space W subspace of vector space V
{‚Éóv ‚ààV | ‚ü®cond‚ü©}
subspace of vectors in V satisfying condition ‚ü®cond‚ü©
span{‚Éóv1, . . . ,‚Éóvn}
span of vectors ‚Éóv1, . . . ,‚Éóvn
dim(U)
dimension of vector space U
R(M)
row space of M
N(M)
null space of M
C(M)
column space of M
N(M T)
left null space of M
rank(M)
rank of M; rank(M) ‚â°dim(R(M)) = dim(C(M))
nullity(M)
nullity of M; nullity(M) ‚â°dim(N(M))
Bs
the standard basis
{‚Éóe1, . . . ,‚Éóen}
an orthogonal basis
{ÀÜe1, . . . , ÀÜen}
an orthonormal basis
[1]
B‚Ä≤
B
the change-of-basis matrix from basis B to basis B‚Ä≤
Œ†S
projection onto subspace S
Œ†S‚ä•
projection onto the orthogonal complement of S

502
NOTATION
Notation for matrices and matrix operations
Expression
Denotes
Rm√ón
the set of m √ó n matrices with real coeÔ¨Écients
A
a matrix
aij
entry in the ith row and jth column of A
|A|
determinant of A, also denoted det(A)
A‚àí1
matrix inverse
AT
matrix transpose
1
identity matrix; 1A = A1 = A and 1‚Éóv = ‚Éóv
AB
matrix-matrix product
A‚Éóv
matrix-vector product
‚ÉówTA
vector-matrix product
‚ÉóuT‚Éóv
vector-vector inner product; ‚ÉóuT‚Éóv ‚â°‚Éóu ¬∑ ‚Éóv
‚Éóu‚ÉóvT
vector-vector outer product
ref(A)
row echelon form of A
rref(A)
reduced row echelon form of A
rank(A)
rank of A ‚â°number of pivots in rref(A)
A ‚àºA‚Ä≤
matrix A‚Ä≤ obtained from matrix A by row operations
R1, R2, . . .
row operations, of which there are three types:
‚ÜíRi ‚ÜêRi + kRj: add k-times row j to row i
‚ÜíRi ‚ÜîRj: swap rows i and j
‚ÜíRi ‚ÜêmRi: multiply row i by constant m
ER
elementary matrix that corresponds R; R(M)‚â°ERM
[ A | ‚Éób ]
augmented matrix containing matrix A and vector ‚Éób
[ A | B ]
augmented matrix array containing matrices A and B
Mij
minor associated with entry aij. See page 169.
adj(A)
adjugate matrix of A. See page 171.
(ATA)‚àí1AT
generalized inverse of A. See page 333.
Cm√ón
the set of m √ó n matrices with complex coeÔ¨Écients
A‚Ä†
Hermitian transpose; A‚Ä† ‚â°(A)T

NOTATION FOR LINEAR TRANSFORMATIONS
503
Notation for linear transformations
Expression
Denotes
T : Rn ‚ÜíRm
linear transformation T (a vector function)
MT ‚ààRm√ón
matrix representation of T
Dom(T) ‚â°Rn
domain of T
CoDom(T) ‚â°Rm
codomain of T
Im(T) ‚â°C(MT )
the image space of T
Ker(T) ‚â°N(MT )
the kernel of T
S ‚ó¶T(‚Éóx)
composition of linear transformations;
S ‚ó¶T(‚Éóx) ‚â°S(T(‚Éóx)) ‚â°MSMT‚Éóx
M ‚ààRm√ón
an m √ó n matrix
TM : Rn ‚ÜíRm
the linear transformation deÔ¨Åned as TM(‚Éóv) ‚â°M‚Éóv
TMT : Rm ‚ÜíRn
the adjoint linear transformation TMT(‚Éóa) ‚â°‚ÉóaTM
Matrix decompositions
Expression
Denotes
A ‚ààRn√ón
a matrix (assume diagonalizable)
pA(Œª)‚â°|A ‚àíŒª1|
characteristic polynomial of A
Œª1, . . . , Œªn
eigenvalues of A = roots of pA(Œª) ‚â°Qn
i=1(Œª ‚àíŒªi)
Œõ ‚ààRn√ón
diagonal matrix of eigenvalues of A
‚ÉóeŒª1, . . . ,‚ÉóeŒªn
eigenvectors of A
Q ‚ààRn√ón
matrix whose columns are eigenvectors of A
A = QŒõQ‚àí1
eigendecomposition of A
A = OŒõOT
eigendecomposition of a normal matrix
B ‚ààRm√ón
a generic matrix
œÉ1, œÉ2, . . .
singular values of B
Œ£ ‚ààRm√ón
matrix of singular values of B
‚Éóu1, . . . , ‚Éóum
left singular vectors of B
U ‚ààRm√óm
matrix of left singular vectors of B
‚Éóv1, . . . ,‚Éóvn
right singular vectors of B
V ‚ààRn√ón
matrix of right singular vectors of B
B = UŒ£V T
singular value decomposition of B

504
NOTATION
Abstract vector space notation
Expression
Denotes
(V, F, +, ¬∑)
abstract vector space of vectors from the set V , whose
coeÔ¨Écients are from the Ô¨Åeld F, addition operation "+"
and scalar-multiplication operation "¬∑ "
u, v, w
abstract vectors
‚ü®u, v‚ü©
inner product of vectors u and v
‚à•u‚à•
norm of u
d(u, v)
distance between u and v


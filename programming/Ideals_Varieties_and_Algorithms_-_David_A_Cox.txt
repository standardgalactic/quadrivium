Undergraduate Texts in Mathematics
David A. Cox
John Little
Donal O'Shea
Ideals, 
Varieties, and 
Algorithms
An Introduction to Computational 
Algebraic Geometry and Commutative 
Algebra
 Fourth Edition 

Undergraduate Texts in Mathematics

Undergraduate Texts in Mathematics
Series Editors:
Sheldon Axler
San Francisco State University, San Francisco, CA, USA
Kenneth Ribet
University of California, Berkeley, CA, USA
Advisory Board:
Colin Adams, Williams College
David A. Cox, Amherst College
Pamela Gorkin, Bucknell University
Roger E. Howe, Yale University
Michael Orrison, Harvey Mudd College
Jill Pipher, Brown University
Fadil Santosa, University of Minnesota
Undergraduate Texts in Mathematics are generally aimed at third- and fourth-
year undergraduate mathematics students at North American universities. These
texts strive to provide students and teachers with new perspectives and novel
approaches. The books include motivation that guides the reader to an apprecia-
tion of interrelations among different aspects of the subject. They feature examples
that illustrate key concepts as well as exercises that strengthen understanding.
More information about this series at http://www.springer.com/series/666

David A. Cox
• John Little
• Donal O'Shea
Ideals, Varieties,
and Algorithms
An Introduction to Computational Algebraic
Geometry and Commutative Algebra
Fourth Edition
123

David A. Cox
Department of Mathematics
Amherst College
Amherst, MA, USA
Donal O'Shea
President's Ofﬁce
New College of Florida
Sarasota, FL, USA
John Little
Department of Mathematics
and Computer Science
College of the Holy Cross
Worcester, MA, USA
ISSN 0172-6056
ISSN 2197-5604
(electronic)
Undergraduate Texts in Mathematics
ISBN 978-3-319-16720-6
ISBN 978-3-319-16721-3
(eBook)
DOI 10.1007/978-3-319-16721-3
Library of Congress Control Number: 2015934444
Mathematics Subject Classiﬁcation (2010): 14-01, 13-01, 13Pxx
Springer Cham Heidelberg New York Dordrecht London
© Springer International Publishing Switzerland 1998, 2005, 2007, 2015
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, express or implied, with respect to the material contained herein or for any
errors or omissions that may have been made.
Printed on acid-free paper
Springer International Publishing AG Switzerland is part of Springer Science+Business Media (www.
springer.com)

To Elaine,
for her love and support.
D.A.C.
To the memory of my parents.
J.B.L.
To Mary and my children.
D.O'S.


Preface
We wrote this book to introduce undergraduates to some interesting ideas in
algebraic geometry and commutative algebra. For a long time, these topics involved
a lot of abstract mathematics and were only taught at the graduate level. Their com-
putational aspects, dormant since the nineteenth century, re-emerged in the 1960s
with Buchberger's work on algorithms for manipulating systems of polynomial
equations. The development of computers fast enough to run these algorithms has
made it possible to investigate complicated examples that would be impossible to do
by hand, and has changed the practice of much research in algebraic geometry and
commutative algebra. This has also enhanced the importance of the subject for com-
puter scientists and engineers, who now regularly use these techniques in a whole
range of problems.
It is our belief that the growing importance of these computational techniques
warrants their introduction into the undergraduate (and graduate) mathematics cur-
riculum. Many undergraduates enjoy the concrete, almost nineteenth century, ﬂavor
that a computational emphasis brings to the subject. At the same time, one can do
some substantial mathematics, including the Hilbert Basis Theorem, Elimination
Theory, and the Nullstellensatz.
Prerequisites
The mathematical prerequisites of the book are modest: students should have had a
course in linear algebra and a course where they learned how to do proofs. Examples
of the latter sort of course include discrete math and abstract algebra. It is important
to note that abstract algebra is not a prerequisite. On the other hand, if all of the
students have had abstract algebra, then certain parts of the course will go much
more quickly.
The book assumes that the students will have access to a computer algebra sys-
tem. Appendix C describes the features of MapleTM, Mathematica®, Sage, and other
computer algebra systems that are most relevant to the text. We do not assume any
prior experience with computer science. However, many of the algorithms in the
vii

viii
Preface
book are described in pseudocode, which may be unfamiliar to students with no
background in programming.Appendix B contains a careful description of the pseu-
docode that we use in the text.
How to Use the Book
In writing the book, we tried to structure the material so that the book could be used
in a variety of courses, and at a variety of different levels. For instance, the book
could serve as a basis of a second course in undergraduate abstract algebra, but we
think that it just as easily could provide a credible alternative to the ﬁrst course.
Although the book is aimed primarily at undergraduates, it could also be used in
various graduate courses, with some supplements. In particular, beginning graduate
courses in algebraic geometry or computational algebra may ﬁnd the text useful. We
hope, of course, that mathematicians and colleagues in other disciplines will enjoy
reading the book as much as we enjoyed writing it.
The ﬁrst four chapters form the core of the book. It should be possible to cover
them in a 14-week semester, and there may be some time left over at the end to
explore other parts of the text. The following chart explains the logical dependence
of the chapters:
1
2
§9
3
§5
§6
4
§1
§7
6
8
§7
5
7
9
10

Preface
ix
The table of contents describes what is covered in each chapter. As the chart in-
dicates, there are a variety of ways to proceed after covering the ﬁrst four chapters.
The three solid arcs and one dashed arc in the chart correspond to special dependen-
cies that will be explained below. Also, a two-semester course could be designed
that covers the entire book. For instructors interested in having their students do an
independent project, we have included a list of possible topics in Appendix D.
Features of the New Edition
This fourth edition incorporates several substantial changes. In some cases, topics
have been reorganized and/or augmented using results of recent work. Here is a
summary of the major changes to the original nine chapters of the book:
• Chapter 2: We now deﬁne standard representations (implicit in earlier editions)
and lcm representations (new to this edition). Theorem 6 from §9 plays an im-
portant role in the book, as indicated by the solid arcs in the dependence chart on
the previous page.
• Chapter 3: We now give two proofs of the Extension Theorem (Theorem 3 in §1).
The resultant proof from earlier editions now appears in §6, and a new Gröbner
basis proof inspired by SCHAUENBURG (2007) is presented in §5. This makes it
possible for instructors to omit resultants entirely if they choose. However, resul-
tants are used in the proof of Bezout's Theorem in Chapter 8, §7, as indicated by
the dashed arc in the dependence chart.
• Chapter 4: There are several important changes:
- In §1 we present a Gröbner basis proof of the Weak Nullstellensatz using ideas
from GLEBSKY (2012).
- In §4 we now cover saturations I : J∞in addition to ideal quotients I : J.
- In §7 we use Gröbner bases to prove the Closure Theorem (Theorem 3 in
Chapter 3, §2) following SCHAUENBURG (2007).
• Chapter 5: We have added a new §6 on Noether normalization and relative ﬁnite-
ness. Unlike the previous topics, the proofs involved in this case are quite classi-
cal. But having this material to draw on provides another illuminating viewpoint
in the study of the dimension of a variety in Chapter 9.
• Chapter 6: The discussion of the behavior of Gröbner bases under specialization
in §3 has been supplemented by a brief presentation of the recently developed
concept of a Gröbner cover from MONTES and WIBMER (2010). We would like
to thank Antonio Montes for the Gröbner cover calculation reported in §3.
In the biggest single change, we have added a new Chapter 10 presenting some
of the progress of the past 25 years in methods for computing Gröbner bases (i.e.,
since the improved Buchberger algorithm discussed in Chapter 2, §10). We present
Traverso's Hilbert driven Buchberger algorithm for homogeneous ideals, Faugère's
F4 algorithm, and a brief introduction to the signature-based family of algorithms
including Faugère's F5. These new algorithmic approaches make use of several in-
teresting ideas from previous chapters and lead the reader toward some of the next
steps in commutative algebra (modules, syzygies, etc.). We chose to include this

x
Preface
topic in part because it illustrates so clearly the close marriage between theory and
practice in this part of mathematics.
Since software for the computations discussed in our text has also undergone ma-
jor changes since 1992, Appendix C has been completely rewritten. We now discuss
Maple, Mathematica, Sage, CoCoA, Macaulay2, and Singular in some detail and
list several other systems that can be used in courses based on our text. Appendix D
has also been substantially updated with new ideas for student projects. Some of the
wide range of applications developed since our ﬁrst edition can be seen from the
new topics there. Finally, the bibliography has been updated and expanded to reﬂect
some of the continuous and rapid development of our subjects.
Acknowledgments
When we began writing the ﬁrst edition of Ideals, Varieties, and Algorithms in 1989,
major funding was provided by the New England Consortium for Undergraduate
Science Education (and its parent organization, the Pew Charitable Trusts). This
project would have been impossible without their support. Various aspects of our
work were also aided by grants from IBM and the Sloan Foundation, the Alexan-
der von Humboldt Foundation, the Department of Education's FIPSE program, the
Howard Hughes Foundation, and the National Science Foundation. We are grateful
to all of these organizations for their help.
We also wish to thank colleagues and students at Amherst College, George Ma-
son University, College of the Holy Cross, Massachusetts Institute of Technology,
Mount Holyoke College, Smith College, and the University of Massachusetts who
participated in courses based on early versions of the manuscript. Their feedback
improved the book considerably.
We want to give special thanks to David Bayer and Monique Lejeune-Jalabert,
whose thesis BAYER (1982) and notes LEJEUNE-JALABERT (1985) ﬁrst acquainted
us with this wonderful subject. We are also grateful to Bernd Sturmfels, whose book
STURMFELS (2008) was the inspiration for Chapter 7, and Frances Kirwan, whose
book KIRWAN (1992) convinced us to include Bezout's Theorem in Chapter 8. We
would also like to thank Steven Kleiman, Michael Singer, and A. H. M. Levelt for
important contributions to the second and third editions of the book.
We are extremely grateful to the many, many individuals (too numerous to list
here) who reported typographical errors and gave us feedback on the earlier editions.
Thank you all!
Corrections, comments, and suggestions for improvement are welcome!
Amherst, MA, USA
David Cox
Worcester, MA, USA
John Little
Sarasota, FL, USA
Donal O'Shea
January 2015

Contents
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii
Notation for Sets and Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xv
Chapter 1.
Geometry, Algebra, and Algorithms . . . . . . . . . . . . . . . . . . . . . .
1
§1.
Polynomials and Afﬁne Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
§2.
Afﬁne Varieties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
§3.
Parametrizations of Afﬁne Varieties . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
§4.
Ideals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
§5.
Polynomials of One Variable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
Chapter 2.
Gröbner Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
§1.
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
§2.
Orderings on the Monomials in k[x1, . . . , xn] . . . . . . . . . . . . . . . . . . . . 54
§3.
A Division Algorithm in k[x1, . . . , xn] . . . . . . . . . . . . . . . . . . . . . . . . . . 61
§4.
Monomial Ideals and Dickson's Lemma. . . . . . . . . . . . . . . . . . . . . . . . 70
§5.
The Hilbert Basis Theorem and Gröbner Bases . . . . . . . . . . . . . . . . . . 76
§6.
Properties of Gröbner Bases. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
§7.
Buchberger's Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
§8.
First Applications of Gröbner Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
§9.
Reﬁnements of the Buchberger Criterion . . . . . . . . . . . . . . . . . . . . . . . 104
§10. Improvements on Buchberger's Algorithm. . . . . . . . . . . . . . . . . . . . . . 109
Chapter 3.
Elimination Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
§1.
The Elimination and Extension Theorems . . . . . . . . . . . . . . . . . . . . . . 121
§2.
The Geometry of Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
§3.
Implicitization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
§4.
Singular Points and Envelopes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
§5.
Gröbner Bases and the Extension Theorem . . . . . . . . . . . . . . . . . . . . . 155
§6.
Resultants and the Extension Theorem . . . . . . . . . . . . . . . . . . . . . . . . . 161
xi

xii
Contents
Chapter 4.
The Algebra-Geometry Dictionary. . . . . . . . . . . . . . . . . . . . . . . 175
§1.
Hilbert's Nullstellensatz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
§2.
Radical Ideals and the Ideal-Variety Correspondence. . . . . . . . . . . . . 181
§3.
Sums, Products, and Intersections of Ideals . . . . . . . . . . . . . . . . . . . . . 189
§4.
Zariski Closures, Ideal Quotients, and Saturations . . . . . . . . . . . . . . . 199
§5.
Irreducible Varieties and Prime Ideals. . . . . . . . . . . . . . . . . . . . . . . . . . 206
§6.
Decomposition of a Variety into Irreducibles . . . . . . . . . . . . . . . . . . . . 212
§7.
Proof of the Closure Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
§8.
Primary Decomposition of Ideals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
§9.
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
Chapter 5.
Polynomial and Rational Functions on a Variety . . . . . . . . . . . 233
§1.
Polynomial Mappings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
§2.
Quotients of Polynomial Rings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
§3.
Algorithmic Computations in k[x1, . . . , xn]/I . . . . . . . . . . . . . . . . . . . . 248
§4.
The Coordinate Ring of an Afﬁne Variety . . . . . . . . . . . . . . . . . . . . . . 257
§5.
Rational Functions on a Variety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
§6.
Relative Finiteness and Noether Normalization . . . . . . . . . . . . . . . . . . 277
Chapter 6.
Robotics and Automatic Geometric Theorem Proving . . . . . . 291
§1.
Geometric Description of Robots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
§2.
The Forward Kinematic Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
§3.
The Inverse Kinematic Problem and Motion Planning . . . . . . . . . . . . 304
§4.
Automatic Geometric Theorem Proving . . . . . . . . . . . . . . . . . . . . . . . . 319
§5.
Wu's Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
Chapter 7.
Invariant Theory of Finite Groups . . . . . . . . . . . . . . . . . . . . . . . 345
§1.
Symmetric Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
§2.
Finite Matrix Groups and Rings of Invariants . . . . . . . . . . . . . . . . . . . 355
§3.
Generators for the Ring of Invariants . . . . . . . . . . . . . . . . . . . . . . . . . . 364
§4.
Relations Among Generators and the Geometry of Orbits . . . . . . . . . 373
Chapter 8.
Projective Algebraic Geometry . . . . . . . . . . . . . . . . . . . . . . . . . . 385
§1.
The Projective Plane . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
§2.
Projective Space and Projective Varieties . . . . . . . . . . . . . . . . . . . . . . . 396
§3.
The Projective Algebra-Geometry Dictionary . . . . . . . . . . . . . . . . . . . 406
§4.
The Projective Closure of an Afﬁne Variety . . . . . . . . . . . . . . . . . . . . . 415
§5.
Projective Elimination Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422
§6.
The Geometry of Quadric Hypersurfaces . . . . . . . . . . . . . . . . . . . . . . . 436
§7.
Bezout's Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451
Chapter 9.
The Dimension of a Variety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469
§1.
The Variety of a Monomial Ideal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469
§2.
The Complement of a Monomial Ideal . . . . . . . . . . . . . . . . . . . . . . . . . 473
§3.
The Hilbert Function and the Dimension of a Variety . . . . . . . . . . . . . 486
§4.
Elementary Properties of Dimension. . . . . . . . . . . . . . . . . . . . . . . . . . . 498

Contents
xiii
§5.
Dimension and Algebraic Independence . . . . . . . . . . . . . . . . . . . . . . . 506
§6.
Dimension and Nonsingularity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515
§7.
The Tangent Cone . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 525
Chapter 10. Additional Gröbner Basis Algorithms . . . . . . . . . . . . . . . . . . . . 539
§1.
Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 539
§2.
Hilbert Driven Buchberger Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 550
§3.
The F4 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 567
§4.
Signature-based Algorithms and F5 . . . . . . . . . . . . . . . . . . . . . . . . . . . 576
Appendix A.
Some Concepts from Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . 593
§1.
Fields and Rings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 593
§2.
Unique Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 594
§3.
Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 595
§4.
Determinants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 596
Appendix B.
Pseudocode. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 599
§1.
Inputs, Outputs, Variables, and Constants. . . . . . . . . . . . . . . . . . . . . . . 600
§2.
Assignment Statements. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 600
§3.
Looping Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 600
§4.
Branching Structures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 602
§5.
Output Statements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 602
Appendix C.
Computer Algebra Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603
§1.
General Purpose Systems: Maple, Mathematica, Sage . . . . . . . . . . . . 604
§2.
Special Purpose Programs: CoCoA, Macaulay2, Singular . . . . . . . . . 611
§3.
Other Systems and Packages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 617
Appendix D.
Independent Projects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 619
§1.
General Comments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 619
§2.
Suggested Projects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 620
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 627
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 635


Notation for Sets and Functions
In this book, a set is a collection of mathematical objects. We say that x is an element
of A, written x ∈A, when x is one of objects in A. Then x /∈A means that x is not an
element of A. Commonly used sets are:
Z = {. . . , −2, −1, 0, 1, 2, . . .}, the set of integers,
Z≥0 = {0, 1, 2, . . .}, the set of nonnegative integers,
Q = the set of rational numbers (fractions),
R = the set of real numbers,
C = the set of complex numbers.
Sets will often be speciﬁed by listing the elements in the set, such as A = {0, 1, 2},
or by set-builder notation, such as
[0, 1] = {x ∈R | 0 ≤x ≤1}.
The empty set is the set with no elements, denoted ∅. We write
A ⊆B
when every element of A is also an element of B; we say that A is contained in B
and A is a subset of B. When in addition A ̸= B, we write
A ⊊B
and say that A is a proper subset of B. Basic operations on sets are
A ∪B = {x | x ∈A or x ∈B}, the union of A and B,
A ∩B = {x | x ∈A and x ∈B}, the intersection of A and B,
A \ B = {x | x ∈A and x /∈B}, the difference of A and B,
A × B = {(x, y) | x ∈A and y ∈B}, the cartesian product of A and B.
xv

xvi
Notation for Sets and Functions
We say that f is a function from A to B, written
f : A −→B,
when for every x ∈A, there is a unique f(x) ∈B. We sometimes write the function
f as
x 
−→f(x).
Given any set A, an important function is the identity function
idA : A −→A
deﬁned by x 
→x for all x ∈A. Given functions f : A →B and g : B →C, their
composition
g ◦f : A −→C
is deﬁned by x 
→g(f(x)) for x ∈A.
A function f : A →B is one-to-one if f(x) = f(y) implies x = y whenever x, y ∈A.
A function f : A →B is onto if for all y ∈B, there is x ∈A with f(x) = y. If f is
one-to-one and onto, then f has an inverse function
f −1 : B −→A
deﬁned by f −1(y) = x when f(x) = y. The inverse function satisﬁes
f −1 ◦f = idA
and
f ◦f −1 = idB.

Chapter 1
Geometry, Algebra, and Algorithms
This chapter will introduce some of the basic themes of the book. The geometry
we are interested in concerns afﬁne varieties, which are curves and surfaces (and
higher dimensional objects) deﬁned by polynomial equations. To understand afﬁne
varieties, we will need some algebra, and in particular, we will need to study ideals
in the polynomial ring k[x1, . . . , xn]. Finally, we will discuss polynomials in one
variable to illustrate the role played by algorithms.
§1
Polynomials and Afﬁne Space
To link algebra and geometry, we will study polynomials over a ﬁeld. We all know
what polynomials are, but the term ﬁeld may be unfamiliar. The basic intuition is
that a ﬁeld is a set where one can deﬁne addition, subtraction, multiplication, and
division with the usual properties. Standard examples are the real numbers R and
the complex numbers C, whereas the integers Z are not a ﬁeld since division fails
(3 and 2 are integers, but their quotient 3/2 is not). A formal deﬁnition of ﬁeld may
be found in Appendix A.
One reason that ﬁelds are important is that linear algebra works over any ﬁeld.
Thus, even if your linear algebra course restricted the scalars to lie in R or C, most
of the theorems and techniques you learned apply to an arbitrary ﬁeld k. In this
book, we will employ different ﬁelds for different purposes. The most commonly
used ﬁelds will be:
• The rational numbers Q: the ﬁeld for most of our computer examples.
• The real numbers R: the ﬁeld for drawing pictures of curves and surfaces.
• The complex numbers C: the ﬁeld for proving many of our theorems.
On occasion, we will encounter other ﬁelds, such as ﬁelds of rational functions
(which will be deﬁned later). There is also a very interesting theory of ﬁnite ﬁelds—
see the exercises for one of the simpler examples.
© Springer International Publishing Switzerland 2015
D.A. Cox et al., Ideals, Varieties, and Algorithms, Undergraduate Texts
in Mathematics, DOI 10.1007/978-3-319-16721-3_1
1

2
Chapter 1 Geometry, Algebra, and Algorithms
We can now deﬁne polynomials. The reader certainly is familiar with polynomi-
als in one and two variables, but we will need to discuss polynomials in n variables
x1, . . . , xn with coefﬁcients in an arbitrary ﬁeld k. We start by deﬁning monomials.
Deﬁnition 1. A monomial in x1, . . . , xn is a product of the form
xα1
1 · xα2
2 · · · xαn
n ,
where all of the exponents α1, . . . , αn are nonnegative integers. The total degree of
this monomial is the sum α1 + · · · + αn.
We can simplify the notation for monomials as follows: let α = (α1, . . . , αn) be
an n-tuple of nonnegative integers. Then we set
xα = xα1
1 · xα2
2 · · · xαn
n .
When α = (0, . . . , 0), note that xα = 1. We also let |α| = α1 + · · · + αn denote the
total degree of the monomial xα.
Deﬁnition 2. A polynomial f in x1, . . . , xn with coefﬁcients in a ﬁeld k is a ﬁnite
linear combination (with coefﬁcients in k) of monomials. We will write a polynomial
f in the form
f =

α
aαxα,
aα ∈k,
where the sum is over a ﬁnite number of n-tuples α = (α1, . . . , αn). The set of all
polynomials in x1, . . . , xn with coefﬁcients in k is denoted k[x1, . . . , xn].
When dealing with polynomials in a small number of variables, we will usually
dispense with subscripts. Thus, polynomials in one, two, and three variables lie in
k[x], k[x, y], and k[x, y, z], respectively. For example,
f = 2x3y2z + 3
2y3z3 −3xyz + y2
is a polynomial in Q[x, y, z]. We will usually use the letters f, g, h, p, q, r to refer to
polynomials.
We will use the following terminology in dealing with polynomials.
Deﬁnition 3. Let f = 
α aαxα be a polynomial in k[x1, . . . , xn].
(i) We call aα the coefﬁcient of the monomial xα.
(ii) If aα ̸= 0, then we call aαxα a term of f.
(iii) The total degree of f ̸= 0, denoted deg( f ), is the maximum |α| such that the
coefﬁcient aα is nonzero. The total degree of the zero polynomial is undeﬁned.
As an example, the polynomial f = 2x3y2z + 3
2y3z3 −3xyz + y2 given above
has four terms and total degree six. Note that there are two terms of maximal total
degree, which is something that cannot happen for polynomials of one variable.
In Chapter 2, we will study how to order the terms of a polynomial.

§1 Polynomials and Afﬁne Space
3
The sum and product of two polynomials is again a polynomial. We say that a
polynomial f divides a polynomial g provided that g = f h for some polynomial
h ∈k[x1, . . . , xn].
One can show that, under addition and multiplication, k[x1, . . . , xn] satisﬁes all
of the ﬁeld axioms except for the existence of multiplicative inverses (because, for
example, 1/x1 is not a polynomial). Such a mathematical structure is called a com-
mutative ring (see Appendix A for the full deﬁnition), and for this reason we will
refer to k[x1, . . . , xn] as a polynomial ring.
The next topic to consider is afﬁne space.
Deﬁnition 4. Given a ﬁeld k and a positive integer n, we deﬁne the n-dimensional
afﬁne space over k to be the set
kn = {(a1, . . . , an) | a1, . . . , an ∈k}.
For an example of afﬁne space, consider the case k = R. Here we get the familiar
space Rn from calculus and linear algebra. In general, we call k1 = k the afﬁne line
and k2 the afﬁne plane.
Let us next see how polynomials relate to afﬁne space. The key idea is that a
polynomial f = 
α aαxα ∈k[x1, . . . , xn] gives a function
f : kn −→k
deﬁned as follows: given (a1, . . . , an) ∈kn, replace every xi by ai in the expres-
sion for f. Since all of the coefﬁcients also lie in k, this operation gives an element
f(a1, . . . , an) ∈k. The ability to regard a polynomial as a function is what makes it
possible to link algebra and geometry.
This dual nature of polynomials has some unexpected consequences. For exam-
ple, the question "is f = 0?" now has two potential meanings: is f the zero polyno-
mial?, which means that all of its coefﬁcients aα are zero, or is f the zero function?,
which means that f(a1, . . . , an) = 0 for all (a1, . . . , an) ∈kn. The surprising fact is
that these two statements are not equivalent in general. For an example of how they
can differ, consider the set consisting of the two elements 0 and 1. In the exercises,
we will see that this can be made into a ﬁeld where 1 + 1 = 0. This ﬁeld is usually
called F2. Now consider the polynomial x2 −x = x(x −1) ∈F2[x]. Since this
polynomial vanishes at 0 and 1, we have found a nonzero polynomial which gives
the zero function on the afﬁne space F1
2. Other examples will be discussed in the
exercises.
However, as long as k is inﬁnite, there is no problem.
Proposition 5. Let k be an inﬁnite ﬁeld and let f ∈k[x1, . . . , xn]. Then f = 0 in
k[x1, . . . , xn] if and only if f : kn →k is the zero function.
Proof. One direction of the proof is obvious since the zero polynomial clearly gives
the zero function. To prove the converse, we need to show that if f(a1, . . . , an) = 0
for all (a1, . . . , an) ∈kn, then f is the zero polynomial. We will use induction on the
number of variables n.

4
Chapter 1 Geometry, Algebra, and Algorithms
When n = 1, it is well known that a nonzero polynomial in k[x] of degree m
has at most m distinct roots (we will prove this fact in Corollary 3 of §5). For our
particular f ∈k[x], we are assuming f(a) = 0 for all a ∈k. Since k is inﬁnite, this
means that f has inﬁnitely many roots, and, hence, f must be the zero polynomial.
Now assume that the converse is true for n −1, and let f ∈k[x1, . . . , xn] be a
polynomial that vanishes at all points of kn. By collecting the various powers of xn,
we can write f in the form
f =
N

i=0
gi(x1, . . . , xn−1)xi
n,
where gi ∈k[x1, . . . , xn−1]. We will show that each gi is the zero polynomial in n−1
variables, which will force f to be the zero polynomial in k[x1, . . . , xn].
If we ﬁx (a1, . . . , an−1) ∈kn−1, we get the polynomial f(a1, . . . , an−1, xn) ∈
k[xn]. By our hypothesis on f, this vanishes for every an ∈k. It follows from the case
n = 1 that f(a1, . . . , an−1, xn) is the zero polynomial in k[xn]. Using the above for-
mula for f, we see that the coefﬁcients of f(a1, . . . , an−1, xn) are gi(a1, . . . , an−1),
and thus, gi(a1, . . . , an−1) = 0 for all i. Since (a1, . . . , an−1) was arbitrarily cho-
sen in kn−1, it follows that each gi ∈k[x1, . . . , xn−1] gives the zero function on
kn−1. Our inductive assumption then implies that each gi is the zero polynomial in
k[x1, . . . , xn−1]. This forces f to be the zero polynomial in k[x1, . . . , xn] and com-
pletes the proof of the proposition.
□
Note that in the statement of Proposition 5, the assertion "f = 0 in k[x1, . . . , xn]"
means that f is the zero polynomial, i.e., that every coefﬁcient of f is zero. Thus, we
use the same symbol "0" to stand for the zero element of k and the zero polynomial
in k[x1, . . . , xn]. The context will make clear which one we mean.
As a corollary, we see that two polynomials over an inﬁnite ﬁeld are equal pre-
cisely when they give the same function on afﬁne space.
Corollary 6. Let k be an inﬁnite ﬁeld, and let f, g ∈k[x1, . . . , xn]. Then f = g in
k[x1, . . . , xn] if and only if f : kn →k and g : kn →k are the same function.
Proof. To prove the nontrivial direction, suppose that f, g ∈k[x1, . . . , xn] give the
same function on kn. By hypothesis, the polynomial f −g vanishes at all points of kn.
Proposition 5 then implies that f −g is the zero polynomial. This proves that f = g
in k[x1, . . . , xn].
□
Finally, we need to record a special property of polynomials over the ﬁeld of
complex numbers C.
Theorem 7. Every nonconstant polynomial f ∈C[x] has a root in C.
Proof. This is the Fundamental Theorem of Algebra, and proofs can be found
in most introductory texts on complex analysis (although many other proofs are
known).
□

§2 Afﬁne Varieties
5
We say that a ﬁeld k is algebraically closed if every nonconstant polynomial in
k[x] has a root in k. Thus R is not algebraically closed (what are the roots of x2+1?),
whereas the above theorem asserts that C is algebraically closed. In Chapter 4 we
will prove a powerful generalization of Theorem 7 called the Hilbert Nullstellensatz.
EXERCISES FOR §1
1. Let F2 = {0, 1}, and deﬁne addition and multiplication by 0 + 0 = 1 + 1 = 0, 0 + 1 =
1 + 0 = 1, 0 · 0 = 0 · 1 = 1 · 0 = 0 and 1 · 1 = 1. Explain why F2 is a ﬁeld. (You need
not check the associative and distributive properties, but you should verify the existence
of identities and inverses, both additive and multiplicative.)
2. Let F2 be the ﬁeld from Exercise 1.
a. Consider the polynomial g(x, y) = x2y + y2x ∈F2[x, y]. Show that g(x, y) = 0 for
every (x, y) ∈F2
2, and explain why this does not contradict Proposition 5.
b. Find a nonzero polynomial in F2[x, y, z] which vanishes at every point of F3
2. Try to
ﬁnd one involving all three variables.
c. Find a nonzero polynomial in F2[x1, . . . , xn] which vanishes at every point of Fn
2. Can
you ﬁnd one in which all of x1, . . . , xn appear?
3. (Requires abstract algebra). Let p be a prime number. The ring of integers modulo p is a
ﬁeld with p elements, which we will denote Fp.
a. Explain why Fp \ {0} is a group under multiplication.
b. Use Lagrange's Theorem to show that ap−1 = 1 for all a ∈Fp \ {0}.
c. Prove that ap = a for all a ∈Fp. Hint: Treat the cases a = 0 and a ̸= 0 separately.
d. Find a nonzero polynomial in Fp[x] that vanishes at all points of Fp. Hint: Use part (c).
4. (Requires abstract algebra.) Let F be a ﬁnite ﬁeld with q elements. Adapt the argument of
Exercise 3 to prove that xq −x is a nonzero polynomial in F[x] which vanishes at every
point of F. This shows that Proposition 5 fails for all ﬁnite ﬁelds.
5. In the proof of Proposition 5, we took f ∈k[x1, . . . , xn] and wrote it as a polynomial in xn
with coefﬁcients in k[x1, . . . , xn−1]. To see what this looks like in a speciﬁc case, consider
the polynomial
f (x, y, z) = x5y2z −x4y3 + y5 + x2z −y3z + xy + 2x −5z + 3.
a. Write f as a polynomial in x with coefﬁcients in k[y, z].
b. Write f as a polynomial in y with coefﬁcients in k[x, z].
c. Write f as a polynomial in z with coefﬁcients in k[x, y].
6. Inside of Cn, we have the subset Zn, which consists of all points with integer coordinates.
a. Prove that if f ∈C[x1, . . . , xn] vanishes at every point of Zn, then f is the zero polyno-
mial. Hint: Adapt the proof of Proposition 5.
b. Let f ∈C[x1, . . . , xn], and let M be the largest power of any variable that appears in f .
Let Zn
M+1 be the set of points of Zn, all coordinates of which lie between 1 and M + 1,
inclusive. Prove that if f vanishes at all points of Zn
M+1, then f is the zero polynomial.
§2
Afﬁne Varieties
We can now deﬁne the basic geometric objects studied in this book.
Deﬁnition 1. Let k be a ﬁeld, and let f1, . . . , fs be polynomials in k[x1, . . . , xn]. Then
we set

6
Chapter 1 Geometry, Algebra, and Algorithms
V( f1, . . . , fs) = {(a1, . . . , an) ∈kn | fi(a1, . . . , an) = 0 for all 1 ≤i ≤s}.
We call V( f1, . . . , fs) the afﬁne variety deﬁned by f1, . . . , fs.
Thus, an afﬁne variety V( f1, . . . , fs) ⊆kn is the set of all solutions of the system
of equations f1(x1, . . . , xn) = · · · = fs(x1, . . . , xn) = 0. We will use the letters V, W,
etc. to denote afﬁne varieties. The main purpose of this section is to introduce the
reader to lots of examples, some new and some familiar. We will use k = R so that
we can draw pictures.
We begin in the plane R2 with the variety V(x2 + y2 −1), which is the circle of
radius 1 centered at the origin:
1
1
x
y
The conic sections studied in school (circles, ellipses, parabolas, and hyperbolas)
are afﬁne varieties. Likewise, graphs of polynomial functions are afﬁne varieties
[the graph of y = f(x) is V(y −f(x))]. Although not as obvious, graphs of rational
functions are also afﬁne varieties. For example, consider the graph of y = x3−1
x :
-4
-2
2
4
x
-20
-10
10
20
30
y
It is easy to check that this is the afﬁne variety V(xy −x3 + 1).
Next, let us look in the 3-dimensional space R3. A nice afﬁne variety is given by
paraboloid of revolution V(z −x2 −y2), which is obtained by rotating the parabola

§2 Afﬁne Varieties
7
z = x2 about the z-axis (you can check this using polar coordinates). This gives us
the picture:
z
y
x
You may also be familiar with the cone V(z2 −x2 −y2):
y
x
z
A much more complicated surface is given by V(x2 −y2z2 + z3):
y
x
z
In these last two examples, the surfaces are not smooth everywhere: the cone has
a sharp point at the origin, and the last example intersects itself along the whole

8
Chapter 1 Geometry, Algebra, and Algorithms
y-axis. These are examples of singular points, which will be studied later in the
book.
An interesting example of a curve in R3 is the twisted cubic, which is the variety
V(y −x2, z −x3). For simplicity, we will conﬁne ourselves to the portion that lies in
the ﬁrst octant. To begin, we draw the surfaces y = x2 and z = x3 separately:
O
y
x
z
O
y
x
z
Then their intersection gives the twisted cubic:
O
y
x
z
Notice that when we had one equation in R2, we got a curve, which is a
1-dimensional object. A similar situation happens in R3: one equation in R3 usu-
ally gives a surface, which has dimension 2. Again, dimension drops by one. But
now consider the twisted cubic: here, two equations in R3 give a curve, so that di-
mension drops by two. Since each equation imposes an extra constraint, intuition
suggests that each equation drops the dimension by one. Thus, if we started in R4,
one would hope that an afﬁne variety deﬁned by two equations would be a surface.
Unfortunately, the notion of dimension is more subtle than indicated by the above

§2 Afﬁne Varieties
9
examples. To illustrate this, consider the variety V(xz, yz). One can easily check that
the equations xz = yz = 0 deﬁne the union of the (x, y)-plane and the z-axis:
x
y
z
Hence, this variety consists of two pieces which have different dimensions, and one
of the pieces (the plane) has the "wrong" dimension according to the above intuition.
We next give some examples of varieties in higher dimensions. A familiar case
comes from linear algebra. Namely, ﬁx a ﬁeld k, and consider a system of m linear
equations in n unknowns x1, . . . , xn with coefﬁcients in k:
(1)
a11x1 + · · · + a1nxn = b1,
...
am1x1 + · · · + amnxn = bm.
The solutions of these equations form an afﬁne variety in kn, which we will call a
linear variety. Thus, lines and planes are linear varieties, and there are examples
of arbitrarily large dimension. In linear algebra, you learned the method of row re-
duction (also called Gaussian elimination), which gives an algorithm for ﬁnding all
solutions of such a system of equations. In Chapter 2, we will study a generalization
of this algorithm which applies to systems of polynomial equations.
Linear varieties relate nicely to our discussion of dimension. Namely, if V ⊆kn is
the linear variety deﬁned by (1), then V need not have dimension n−m even though
V is deﬁned by m equations. In fact, when V is nonempty, linear algebra tells us that
V has dimension n−r, where r is the rank of the matrix (aij). So for linear varieties,
the dimension is determined by the number of independent equations. This intuition
applies to more general afﬁne varieties, except that the notion of "independent" is
more subtle.
Some complicated examples in higher dimensions come from calculus. Sup-
pose, for example, that we wanted to ﬁnd the minimum and maximum values of
f(x, y, z) = x3 + 2xyz −z2 subject to the constraint g(x, y, z) = x2 + y2 + z2 = 1.
The method of Lagrange multipliers states that ∇f
= λ∇g at a local mini-

10
Chapter 1 Geometry, Algebra, and Algorithms
mum or maximum [recall that the gradient of f is the vector of partial derivatives
∇f = ( fx, fy, fz)]. This gives us the following system of four equations in four
unknowns, x, y, z, λ, to solve:
(2)
3x2 + 2yz = 2xλ,
2xz = 2yλ,
2xy −2z = 2zλ,
x2 + y2 + z2 = 1.
These equations deﬁne an afﬁne variety in R4, and our intuition concerning dimen-
sion leads us to hope it consists of ﬁnitely many points (which have dimension 0)
since it is deﬁned by four equations. Students often ﬁnd Lagrange multipliers dif-
ﬁcult because the equations are so hard to solve. The algorithms of Chapter 2 will
provide a powerful tool for attacking such problems. In particular, we will ﬁnd all
solutions of the above equations.
We should also mention that afﬁne varieties can be the empty set. For example,
when k = R, it is obvious that V(x2 + y2 + 1) = ∅since x2 + y2 = −1 has
no real solutions (although there are solutions when k = C). Another example is
V(xy, xy −1), which is empty no matter what the ﬁeld is, for a given x and y cannot
satisfy both xy = 0 and xy = 1. In Chapter 4 we will study a method for determining
when an afﬁne variety over C is nonempty.
To give an idea of some of the applications of afﬁne varieties, let us consider a
simple example from robotics. Suppose we have a robot arm in the plane consisting
of two linked rods of lengths 1 and 2, with the longer rod anchored at the origin:
(x,y)
(z,w)
The "state" of the arm is completely described by the coordinates (x, y) and (z, w)
indicated in the ﬁgure. Thus the state can be regarded as a 4-tuple (x, y, z, w) ∈R4.
However, not all 4-tuples can occur as states of the arm. In fact, it is easy to see that
the subset of possible states is the afﬁne variety in R4 deﬁned by the equations

§2 Afﬁne Varieties
11
x2 + y2 = 4,
(x −z)2 + (y −w)2 = 1.
Notice how even larger dimensions enter quite easily: if we were to consider the
same arm in 3-dimensional space, then the variety of states would be deﬁned by two
equations in R6. The techniques to be developed in this book have some important
applications to the theory of robotics.
So far, all of our drawings have been over R. Later in the book, we will consider
varieties over C. Here, it is more difﬁcult (but not impossible) to get a geometric
idea of what such a variety looks like.
Finally, let us record some basic properties of afﬁne varieties.
Lemma 2. If V, W ⊆kn are afﬁne varieties, then so are V ∪W and V ∩W.
Proof. Suppose that V = V( f1, . . . , fs) and W = V(g1, . . . , gt). Then we claim that
V ∩W = V( f1, . . . , fs, g1, . . . , gt),
V ∪W = V( figj | 1 ≤i ≤s, 1 ≤j ≤t).
The ﬁrst equality is trivial to prove: being in V ∩W means that both f1, . . . , fs and
g1, . . . , gt vanish, which is the same as f1, . . . , fs, g1, . . . , gt vanishing.
The second equality takes a little more work. If (a1, . . . , an) ∈V, then all
of the fi's vanish at this point, which implies that all of the figj's also vanish at
(a1, . . . , an). Thus, V ⊆V( figj), and W ⊆V( figj) follows similarly. This proves
that V ∪W ⊆V( figj). Going the other way, choose (a1, . . . , an) ∈V( figj). If this
lies in V, then we are done, and if not, then fi0(a1, . . . , an) ̸= 0 for some i0. Since
fi0gj vanishes at (a1, . . . , an) for all j, the gj's must vanish at this point, proving that
(a1, . . . , an) ∈W. This shows that V( figj) ⊆V ∪W.
□
This lemma implies that ﬁnite intersections and unions of afﬁne varieties are
again afﬁne varieties. It turns out that we have already seen examples of unions
and intersections. Concerning unions, consider the union of the (x, y)-plane and the
z-axis in afﬁne 3-space. By the above formula, we have
V(z) ∪V(x, y) = V(zx, zy).
This, of course, is one of the examples discussed earlier in the section. As for inter-
sections, notice that the twisted cubic was given as the intersection of two surfaces.
The examples given in this section lead to some interesting questions concerning
afﬁne varieties. Suppose that we have f1, . . . , fs ∈k[x1, . . . , xn]. Then:
• (Consistency) Can we determine if V( f1, . . . , fs) ̸= ∅, i.e., do the equations f1 =
· · · = fs = 0 have a common solution?
• (Finiteness) Can we determine if V( f1, . . . , fs) is ﬁnite, and if so, can we ﬁnd all
of the solutions explicitly?
• (Dimension) Can we determine the "dimension" of V( f1, . . . , fs)?

12
Chapter 1 Geometry, Algebra, and Algorithms
The answer to these questions is yes, although care must be taken in choosing
the ﬁeld k that we work over. The hardest is the one concerning dimension, for it
involves some sophisticated concepts. Nevertheless, we will give complete solutions
to all three problems.
EXERCISES FOR §2
1. Sketch the following afﬁne varieties in R2:
a. V(x2 + 4y2 + 2x −16y + 1).
b. V(x2 −y2).
c. V(2x + y −1, 3x −y + 2).
In each case, does the variety have the dimension you would intuitively expect it to have?
2. In R2, sketch V(y2 −x(x −1)(x −2)). Hint: For which x's is it possible to solve for y?
How many y's correspond to each x? What symmetry does the curve have?
3. In the plane R2, draw a picture to illustrate
V(x2 + y2 −4) ∩V(xy −1) = V(x2 + y2 −4, xy −1),
and determine the points of intersection. Note that this is a special case of Lemma 2.
4. Sketch the following afﬁne varieties in R3:
a. V(x2 + y2 + z2 −1).
b. V(x2 + y2 −1).
c. V(x + 2, y −1.5, z).
d. V(xz2 −xy). Hint: Factor xz2 −xy.
e. V(x4 −zx, x3 −yx).
f. V(x2 + y2 + z2 −1, x2 + y2 + (z −1)2 −1).
In each case, does the variety have the dimension you would intuitively expect it to have?
5. Use the proof of Lemma 2 to sketch V((x −2)(x2 −y), y(x2 −y), (z + 1)(x2 −y)) in R3.
Hint: This is the union of which two varieties?
6. Let us show that all ﬁnite subsets of kn are afﬁne varieties.
a. Prove that a single point (a1, . . . , an) ∈kn is an afﬁne variety.
b. Prove that every ﬁnite subset of kn is an afﬁne variety. Hint: Lemma 2 will be useful.
7. One of the prettiest examples from polar coordinates is the four-leaved rose
-.75
-.5
-.25
.25
.5
.75
-.75
-.5
-.25
.25
.5
.75
This curve is deﬁned by the polar equation r = sin(2θ). We will show that this curve is
an afﬁne variety.

§2 Afﬁne Varieties
13
a. Using r2 = x2 + y2, x = r cos(θ) and y = r sin(θ), show that the four-leaved rose is
contained in the afﬁne variety V((x2+y2)3−4x2y2). Hint: Use an identity for sin(2θ).
b. Now argue carefully that V((x2 + y2)3 −4x2y2) is contained in the four-leaved rose.
This is trickier than it seems since r can be negative in r = sin(2θ).
Combining parts (a) and (b), we have proved that the four-leaved rose is the afﬁne variety
V((x2 + y2)3 −4x2y2).
8. It can take some work to show that something is not an afﬁne variety. For example,
consider the set
X = {(x, x) | x ∈R, x ̸= 1} ⊆R2,
which is the straight line x = y with the point (1, 1) removed. To show that X is not
an afﬁne variety, suppose that X = V( f1, . . . , fs). Then each fi vanishes on X, and if
we can show that fi also vanishes at (1, 1), we will get the desired contradiction. Thus,
here is what you are to prove: if f ∈R[x, y] vanishes on X, then f (1, 1) = 0. Hint: Let
g(t) = f (t, t), which is a polynomial R[t]. Now apply the proof of Proposition 5 of §1.
9. Let R = {(x, y) ∈R2 | y > 0} be the upper half plane. Prove that R is not an afﬁne
variety.
10. Let Zn ⊆Cn consist of those points with integer coordinates. Prove that Zn is not an
afﬁne variety. Hint: See Exercise 6 from §1.
11. So far, we have discussed varieties over R or C. It is also possible to consider varieties
over the ﬁeld Q, although the questions here tend to be much harder. For example, let n
be a positive integer, and consider the variety Fn ⊆Q2 deﬁned by
xn + yn = 1.
Notice that there are some obvious solutions when x or y is zero. We call these trivial
solutions. An interesting question is whether or not there are any nontrivial solutions.
a. Show that Fn has two trivial solutions if n is odd and four trivial solutions if n is even.
b. Show that Fn has a nontrivial solution for some n ≥3 if and only if Fermat's Last
Theorem were false.
Fermat's Last Theorem states that, for n ≥3, the equation
xn + yn = zn
has no solutions where x, y, and z are nonzero integers. The general case of this conjecture
was proved by Andrew Wiles in 1994 using some very sophisticated number theory. The
proof is extremely difﬁcult.
12. Find a Lagrange multipliers problem in a calculus book and write down the correspond-
ing system of equations. Be sure to use an example where one wants to ﬁnd the minimum
or maximum of a polynomial function subject to a polynomial constraint. This way the
equations deﬁne an afﬁne variety, and try to ﬁnd a problem that leads to complicated
equations. Later we will use Gröbner basis methods to solve these equations.
13. Consider a robot arm in R2 that consists of three arms of lengths 3, 2, and 1, respectively.
The arm of length 3 is anchored at the origin, the arm of length 2 is attached to the free
end of the arm of length 3, and the arm of length 1 is attached to the free end of the arm
of length 2. The "hand" of the robot arm is attached to the end of the arm of length 1.
a. Draw a picture of the robot arm.
b. How many variables does it take to determine the "state" of the robot arm?
c. Give the equations for the variety of possible states.
d. Using the intuitive notion of dimension discussed in this section, guess what the
dimension of the variety of states should be.
14. This exercise will study the possible "hand" positions of the robot arm described in
Exercise 13.
a. If (u, v) is the position of the hand, explain why u2 + v2 ≤36.

14
Chapter 1 Geometry, Algebra, and Algorithms
b. Suppose we "lock" the joint between the length 3 and length 2 arms to form a straight
angle, but allow the other joint to move freely. Draw a picture to show that in these
conﬁgurations, (u, v) can be any point of the annulus 16 ≤u2 + v2 ≤36.
c. Draw a picture to show that (u, v) can be any point in the disk u2 + v2 ≤36. Hint:
Consider 16 ≤u2 + v2 ≤36, 4 ≤u2 + v2 ≤16, and u2 + v2 ≤4 separately.
15. In Lemma 2, we showed that if V and W are afﬁne varieties, then so are their union V ∪W
and intersection V ∩W. In this exercise we will study how other set-theoretic operations
affect afﬁne varieties.
a. Prove that ﬁnite unions and intersections of afﬁne varieties are again afﬁne varieties.
Hint: Induction.
b. Give an example to show that an inﬁnite union of afﬁne varieties need not be an
afﬁne variety. Hint: By Exercises 8-10, we know some subsets of kn that are not
afﬁne varieties. Surprisingly, an inﬁnite intersection of afﬁne varieties is still an afﬁne
variety. This is a consequence of the Hilbert Basis Theorem, which will be discussed
in Chapters 2 and 4.
c. Give an example to show that the set-theoretic difference V \W of two afﬁne varieties
need not be an afﬁne variety.
d. Let V ⊆kn and W ⊆km be two afﬁne varieties, and let
V × W = {(x1, . . . , xn, y1, . . . , ym) ∈kn+m | (x1, . . . , xn) ∈V, (y1, . . . , ym) ∈W}
be their Cartesian product. Prove that V × W is an afﬁne variety in kn+m. Hint: If V is
deﬁned by f1, . . . , fs ∈k[x1, . . . , xn], then we can regard f1, . . . , fs as polynomials in
k[x1, . . . , xn, y1, . . . , ym], and similarly for W. Show that this gives deﬁning equations
for the Cartesian product.
§3
Parametrizations of Afﬁne Varieties
In this section, we will discuss the problem of describing the points of an afﬁne
variety V( f1, . . . , fs). This reduces to asking whether there is a way to "write down"
the solutions of the system of polynomial equations f1 = · · · = fs = 0. When there
are ﬁnitely many solutions, the goal is simply to list them all. But what do we do
when there are inﬁnitely many? As we will see, this question leads to the notion of
parametrizing an afﬁne variety.
To get started, let us look at an example from linear algebra. Let the ﬁeld be R,
and consider the system of equations
(1)
x + y + z = 1,
x + 2y −z = 3.
Geometrically, this represents the line in R3 which is the intersection of the planes
x + y + z = 1 and x + 2y −z = 3. It follows that there are inﬁnitely many solutions.
To describe the solutions, we use row operations on equations (1) to obtain the
equivalent equations
x + 3z = −1,
y −2z = 2.

§3 Parametrizations of Afﬁne Varieties
15
Letting z = t, where t is arbitrary, this implies that all solutions of (1) are given by
x = −1 −3t,
y = 2 + 2t,
(2)
z = t
as t varies over R. We call t a parameter, and (2) is, thus, a parametrization of the
solutions of (1).
To see if the idea of parametrizing solutions can be applied to other afﬁne vari-
eties, let us look at the example of the unit circle
(3)
x2 + y2 = 1.
A common way to parametrize the circle is using trigonometric functions:
x = cos(t),
y = sin(t).
There is also a more algebraic way to parametrize this circle:
(4)
x = 1 −t2
1 + t2 ,
y =
2t
1 + t2 .
You should check that the points deﬁned by these equations lie on the circle (3). It is
also interesting to note that this parametrization does not describe the whole circle:
since x = 1−t2
1+t2 can never equal −1, the point (−1, 0) is not covered. At the end of
the section, we will explain how this parametrization was obtained.
Notice that equations (4) involve quotients of polynomials. These are examples
of rational functions, and before we can say what it means to parametrize a variety,
we need to deﬁne the general notion of rational function.
Deﬁnition 1. Let k be a ﬁeld. A rational function in t1, . . . , tm with coefﬁcients in
k is a quotient f/g of two polynomials f, g ∈k[t1, . . . , tm], where g is not the zero
polynomial. Furthermore, two rational functions f/g and f ′/g′ are equal provided
that g′f = gf ′ in k[t1, . . . , tm]. Finally, the set of all rational functions in t1, . . . , tm
with coefﬁcients in k is denoted k(t1, . . . , tm).
It is not difﬁcult to show that addition and multiplication of rational functions
are well deﬁned and that k(t1, . . . , tm) is a ﬁeld. We will assume these facts without
proof.
Now suppose that we are given a variety V = V( f1, . . . , fs) ⊆kn. Then a ra-
tional parametric representation of V consists of rational functions r1, . . . , rn ∈
k(t1, . . . , tm) such that the points given by

16
Chapter 1 Geometry, Algebra, and Algorithms
x1 = r1(t1, . . . , tm),
x2 = r2(t1, . . . , tm),
...
xn = rn(t1, . . . , tm)
lie in V. We also require that V be the "smallest" variety containing these points. As
the example of the circle shows, a parametrization may not cover all points of V. In
Chapter 3, we will give a more precise deﬁnition of what we mean by "smallest."
In many situations, we have a parametrization of a variety V, where r1, . . . , rn
are polynomials rather than rational functions. This is what we call a polynomial
parametric representation of V.
By contrast, the original deﬁning equations f1 = · · · = fs = 0 of V are called
an implicit representation of V. In our previous examples, note that equations (1)
and (3) are implicit representations of varieties, whereas (2) and (4) are parametric.
One of the main virtues of a parametric representation of a curve or surface is
that it is easy to draw on a computer. Given the formulas for the parametrization,
the computer evaluates them for various values of the parameters and then plots the
resulting points. For example, in §2 we viewed the surface V(x2 −y2z2 + z3):
y
x
z
This picture was not plotted using the implicit representation x2 −y2z2 + z3 = 0.
Rather, we used the parametric representation given by
x = t(u2 −t2),
y = u,
(5)
z = u2 −t2.
There are two parameters t and u since we are describing a surface, and the above
picture was drawn using the range −1 ≤t, u ≤1. In the exercises, we will derive
this parametrization and check that it covers the entire surface V(x2 −y2z2 + z3).
At the same time, it is often useful to have an implicit representation of a variety.
For example, suppose we want to know whether or not the point (1, 2, −1) is on
the above surface. If all we had was the parametrization (5), then, to decide this
question, we would need to solve the equations

§3 Parametrizations of Afﬁne Varieties
17
1 = t(u2 −t2),
2 = u,
(6)
−1 = u2 −t2
for t and u. On the other hand, if we have the implicit representation x2 −y2z2 +z3 =
0, then it is simply a matter of plugging into this equation. Since
12 −22(−1)2 + (−1)3 = 1 −4 −1 = −4 ̸= 0,
it follows that (1, 2, −1) is not on the surface [and, consequently, equations (6) have
no solution].
The desirability of having both types of representations leads to the following
two questions:
• (Parametrization) Does every afﬁne variety have a rational parametric represen-
tation?
• (Implicitization) Given a parametric representation of an afﬁne variety, can we
ﬁnd the deﬁning equations (i.e., can we ﬁnd an implicit representation)?
The answer to the ﬁrst question is no. In fact, most afﬁne varieties cannot be
parametrized in the sense described here. Those that can are called unirational. In
general, it is difﬁcult to tell whether a given variety is unirational or not. The situa-
tion for the second question is much nicer. In Chapter 3, we will see that the answer
is always yes: given a parametric representation, we can always ﬁnd the deﬁning
equations.
Let us look at an example of how implicitization works. Consider the parametric
representation
(7)
x = 1 + t,
y = 1 + t2.
This describes a curve in the plane, but at this point, we cannot be sure that it lies on
an afﬁne variety. To ﬁnd the equation we are looking for, notice that we can solve
the ﬁrst equation for t to obtain
t = x −1.
Substituting this into the second equation yields
y = 1 + (x −1)2 = x2 −2x + 2.
Hence the parametric equations (7) describe the afﬁne variety V(y −x2 + 2x −2).
In the above example, notice that the basic strategy was to eliminate the variable
t so that we were left with an equation involving only x and y. This illustrates the
role played by elimination theory, which will be studied in much greater detail in
Chapter 3.
We will next discuss two examples of how geometry can be used to parametrize
varieties. Let us start with the unit circle x2 + y2 = 1, which was parametrized in (4)
via

18
Chapter 1 Geometry, Algebra, and Algorithms
x = 1 −t2
1 + t2 ,
y =
2t
1 + t2 .
To see where this parametrization comes from, notice that each nonvertical line
through (−1, 0) will intersect the circle in a unique point (x, y):
1
1
(x,y)
(0,t)
(−1,0)
x
y
Each nonvertical line also meets the y-axis, and this is the point (0, t) in the above
picture.
This gives us a geometric parametrization of the circle: given t, draw the line con-
necting (−1, 0) to (0, t), and let (x, y) be the point where the line meets x2 + y2 = 1.
Notice that the previous sentence really gives a parametrization: as t runs from −∞
to ∞on the vertical axis, the corresponding point (x, y) traverses all of the circle
except for the point (−1, 0).
It remains to ﬁnd explicit formulas for x and y in terms of t. To do this, consider
the slope of the line in the above picture. We can compute the slope in two ways,
using either the points (−1, 0) and (0, t), or the points (−1, 0) and (x, y). This gives
us the equation
t −0
0 −(−1) =
y −0
x −(−1),
which simpliﬁes to become
t =
y
x + 1.
Thus, y = t(x + 1). If we substitute this into x2 + y2 = 1, we get
x2 + t2(x + 1)2 = 1,
which gives the quadratic equation
(8)
(1 + t2)x2 + 2t2x + t2 −1 = 0.

§3 Parametrizations of Afﬁne Varieties
19
This equation gives the x-coordinates of where the line meets the circle, and it is
quadratic since there are two points of intersection. One of the points is −1, so that
x+1 is a factor of (8). It is now easy to ﬁnd the other factor, and we can rewrite (8) as
(x + 1)((1 + t2)x −(1 −t2)) = 0.
Since the x-coordinate we want is given by the second factor, we obtain
x = 1 −t2
1 + t2 .
Furthermore, y = t(x + 1) easily leads to
y =
2t
1 + t2
(you should check this), and we have now derived the parametrization given earlier.
Note how the geometry tells us exactly what portion of the circle is covered.
For our second example, let us consider the twisted cubic V(y −x2, z −x3) from
§2. This is a curve in 3-dimensional space, and by looking at the tangent lines to the
curve, we will get an interesting surface. The idea is as follows. Given one point on
the curve, we can draw the tangent line at that point:
Now imagine taking the tangent lines for all points on the twisted cubic. This gives
us the following surface:
This picture shows several of the tangent lines. The above surface is called the tan-
gent surface of the twisted cubic.

20
Chapter 1 Geometry, Algebra, and Algorithms
To convert this geometric description into something more algebraic, notice that
setting x = t in y −x2 = z −x3 = 0 gives us a parametrization
x = t,
y = t2,
z = t3
of the twisted cubic. We will write this as r(t) = (t, t2, t3). Now ﬁx a particular value
of t, which gives us a point on the curve. From calculus, we know that the tangent
vector to the curve at the point given by r(t) is r′(t) = (1, 2t, 3t2). It follows that the
tangent line is parametrized by
r(t) + ur′(t) = (t, t2, t3) + u(1, 2t, 3t2) = (t + u, t2 + 2tu, t3 + 3t2u),
where u is a parameter that moves along the tangent line. If we now allow t to vary,
then we can parametrize the entire tangent surface by
x = t + u,
y = t2 + 2tu,
z = t3 + 3t2u.
The parameters t and u have the following interpretations: t tells where we are on
the curve, and u tells where we are on the tangent line. This parametrization was
used to draw the picture of the tangent surface presented earlier.
A ﬁnal question concerns the implicit representation of the tangent surface: how
do we ﬁnd its deﬁning equation? This is a special case of the implicitization problem
mentioned earlier and is equivalent to eliminating t and u from the above parametric
equations. In Chapters 2 and 3, we will see that there is an algorithm for doing
this, and, in particular, we will prove that the tangent surface to the twisted cubic is
deﬁned by the equation
x3z −(3/4)x2y2 −(3/2)xyz + y3 + (1/4)z2 = 0.
We will end this section with an example from Computer Aided Geometric De-
sign (CAGD). When creating complex shapes like automobile hoods or airplane
wings, design engineers need curves and surfaces that are varied in shape, easy to
describe, and quick to draw. Parametric equations involving polynomial and rational
functions satisfy these requirements; there is a large body of literature on this topic.
For simplicity, let us suppose that a design engineer wants to describe a curve
in the plane. Complicated curves are usually created by joining together simpler
pieces, and for the pieces to join smoothly, the tangent directions must match up
at the endpoints. Thus, for each piece, the designer needs to control the following
geometric data:
• the starting and ending points of the curve;
• the tangent directions at the starting and ending points.

§3 Parametrizations of Afﬁne Varieties
21
The Bézier cubic, introduced by Renault auto designer P. Bézier, is especially well
suited for this purpose. A Bézier cubic is given parametrically by the equations
(9)
x = (1 −t)3x0 + 3t(1 −t)2x1 + 3t2(1 −t)x2 + t3x3,
y = (1 −t)3y0 + 3t(1 −t)2y1 + 3t2(1 −t)y2 + t3y3
for 0 ≤t ≤1, where x0, y0, x1, y1, x2, y2, x3, y3 are constants speciﬁed by the design
engineer. Let us see how these constants correspond to the above geometric data.
If we evaluate the above formulas at t = 0 and t = 1, then we obtain
(x(0), y(0)) = (x0, y0),
(x(1), y(1)) = (x3, y3).
As t varies from 0 to 1, equations (9) describe a curve starting at (x0, y0) and ending
at (x3, y3). This gives us half of the needed data. We will next use calculus to ﬁnd
the tangent directions when t = 0 and 1. We know that the tangent vector to (9)
when t = 0 is (x′(0), y′(0)). To calculate x′(0), we differentiate the ﬁrst line of (9)
to obtain
x′ = −3(1 −t)2x0 + 3((1 −t)2 −2t(1 −t))x1 + 3(2t(1 −t) −t2)x2 + 3t2x3.
Then substituting t = 0 yields
x′(0) = −3x0 + 3x1 = 3(x1 −x0),
and from here, it is straightforward to show that
(10)
(x′(0), y′(0)) = 3(x1 −x0, y1 −y0),
(x′(1), y′(1)) = 3(x3 −x2, y3 −y2).
Since (x1 −x0, y1 −y0) = (x1, y1) −(x0, y0), it follows that (x′(0), y′(0)) is three
times the vector from (x0, y0) to (x1, y1). Hence, by placing (x1, y1), the designer
can control the tangent direction at the beginning of the curve. In a similar way, the
placement of (x2, y2) controls the tangent direction at the end of the curve.
The points (x0, y0), (x1, y1), (x2, y2) and (x3, y3) are called the control points of
the Bézier cubic. They are usually labeled P0, P1, P2, and P3, and the convex quadri-
lateral they determine is called the control polygon. Here is a picture of a Bézier
curve together with its control polygon:

22
Chapter 1 Geometry, Algebra, and Algorithms
In the exercises, we will show that a Bézier cubic always lies inside its control
polygon.
The data determining a Bézier cubic is thus easy to specify and has a strong
geometric meaning. One issue not resolved so far is the length of the tangent vectors
(x′(0), y′(0)) and (x′(1), y′(1)). According to (10), it is possible to change the points
(x1, y1) and (x2, y2) without changing the tangent directions. For example, if we keep
the same directions as in the previous picture, but lengthen the tangent vectors, then
we get the following curve:
Thus, increasing the velocity at an endpoint makes the curve stay close to the
tangent line for a longer distance. With practice and experience, a designer can
become proﬁcient in using Bézier cubics to create a wide variety of curves. It is
interesting to note that the designer may never be aware of equations (9) that are
used to describe the curve.
Besides CAGD, we should mention that Bézier cubics are also used in the page
description language PostScript. The curveto command in PostScript has the coor-
dinates of the control points as input and the Bézier cubic as output. This is how the
above Bézier cubics were drawn—each curve was speciﬁed by a single curveto
instruction in a PostScript ﬁle.
EXERCISES FOR §3
1. Parametrize all solutions of the linear equations
x + 2y −2z + w = −1,
x + y + z −w = 2.
2. Use a trigonometric identity to show that
x = cos (t),
y = cos (2t)
parametrizes a portion of a parabola. Indicate exactly what portion of the parabola is
covered.

§3 Parametrizations of Afﬁne Varieties
23
3. Given f ∈k[x], ﬁnd a parametrization of V(y −f (x)).
4. Consider the parametric representation
x =
t
1 + t ,
y = 1 −1
t2 .
a. Find the equation of the afﬁne variety determined by the above parametric equations.
b. Show that the above equations parametrize all points of the variety found in part
(a) except for the point (1, 1).
5. This problem will be concerned with the hyperbola x2 −y2 = 1.
-2
-1.5
-1
-.5
.5
1
1.5
2
-2
-1.5
-1
-.5
.5
1
1.5
2
a. Just as trigonometric functions are used to parametrize the circle, hyperbolic func-
tions are used to parametrize the hyperbola. Show that the point
x = cosh(t),
y = sinh(t)
always lies on x2 −y2 = 1. What portion of the hyperbola is covered?
b. Show that a straight line meets a hyperbola in 0, 1, or 2 points, and illustrate your
answer with a picture. Hint: Consider the cases x = a and y = mx + b separately.
c. Adapt the argument given at the end of the section to derive a parametrization of the
hyperbola. Hint: Consider nonvertical lines through the point (−1, 0) on the hyper-
bola.
d. The parametrization you found in part (c) is undeﬁned for two values of t. Explain
how this relates to the asymptotes of the hyperbola.
6. The goal of this problem is to show that the sphere x2 + y2 + z2 = 1 in 3-dimensional
space can be parametrized by
x =
2u
u2 + v2 + 1,
y =
2v
u2 + v2 + 1,
z = u2 + v2 −1
u2 + v2 + 1.

24
Chapter 1 Geometry, Algebra, and Algorithms
The idea is to adapt the argument given at the end of the section to 3-dimensional space.
a. Given a point (u, v, 0) in the (x, y)-plane, draw the line from this point to the "north
pole" (0, 0, 1) of the sphere, and let (x, y, z) be the other point where the line meets
the sphere. Draw a picture to illustrate this, and argue geometrically that mapping
(u, v) to (x, y, z) gives a parametrization of the sphere minus the north pole.
b. Show that the line connecting (0, 0, 1) to (u, v, 0) is parametrized by (tu, tv, 1 −t),
where t is a parameter that moves along the line.
c. Substitute x = tu, y = tv and z = 1−t into the equation for the sphere x2+y2+z2 = 1.
Use this to derive the formulas given at the beginning of the problem.
7. Adapt the argument of the previous exercise to parametrize the "sphere" x2
1+· · ·+x2
n = 1
in n-dimensional afﬁne space. Hint: There will be n −1 parameters.
8. Consider the curve deﬁned by y2 = cx2 −x3, where c is some constant. Here is a picture
of the curve when c > 0:
c
x
y
Our goal is to parametrize this curve.
a. Show that a line will meet this curve at either 0, 1, 2, or 3 points. Illustrate your answer
with a picture. Hint: Let the equation of the line be either x = a or y = mx + b.
b. Show that a nonvertical line through the origin meets the curve at exactly one other
point when m2 ̸= c. Draw a picture to illustrate this, and see if you can come up with
an intuitive explanation as to why this happens.
c. Now draw the vertical line x = 1. Given a point (1, t) on this line, draw the line
connecting (1, t) to the origin. This will intersect the curve in a point (x, y). Draw a
picture to illustrate this, and argue geometrically that this gives a parametrization of
the entire curve.
d. Show that the geometric description from part (c) leads to the parametrization
x = c −t2,
y = t(c −t2).
9. The strophoid is a curve that was studied by various mathematicians, including Isaac
Barrow (1630-1677), Jean Bernoulli (1667-1748), and Maria Agnesi (1718-1799).
A trigonometric parametrization is given by

§3 Parametrizations of Afﬁne Varieties
25
x = a sin(t),
y = a tan(t)(1 + sin(t))
where a is a constant. If we let t vary in the range −4.5 ≤t ≤1.5, we get the picture
shown here.
a
x
a
-
y
a. Find the equation in x and y that describes the strophoid. Hint: If you are sloppy, you
will get the equation (a2 −x2)y2 = x2(a + x)2. To see why this is not quite correct,
see what happens when x = −a.
b. Find an algebraic parametrization of the strophoid.
10. Around 180 B.C.E., Diocles wrote the book On Burning-Glasses. One of the curves he
considered was the cissoid and he used it to solve the problem of the duplication of the
cube [see part (c) below]. The cissoid has the equation y2(a + x) = (a −x)3, where a is
a constant. This gives the following curve in the plane:
a
a
−
x
a
y

26
Chapter 1 Geometry, Algebra, and Algorithms
a. Find an algebraic parametrization of the cissoid.
b. Diocles described the cissoid using the following geometric construction. Given a
circle of radius a (which we will take as centered at the origin), pick x between a and
−a, and draw the line L connecting (a, 0) to the point P = (−x,
√
a2 −x2) on the
circle. This determines a point Q = (x, y) on L:
a
x
−x
a
−a
P
Q
L
↑
Prove that the cissoid is the locus of all such points Q.
c. The duplication of the cube is the classical Greek problem of trying to construct
3√
2
using ruler and compass. It is known that this is impossible given just a ruler and
compass. Diocles showed that if in addition, you allow the use of the cissoid, then
one can construct
3√
2. Here is how it works. Draw the line connecting (−a, 0) to
(0, a/2). This line will meet the cissoid at a point (x, y). Then prove that
2 =
a −x
y
3
,
which shows how to construct
3√
2 using ruler, compass, and cissoid.
11. In this problem, we will derive the parametrization
x = t(u2 −t2),
y = u,
z = u2 −t2,
of the surface x2 −y2z2 + z3 = 0 considered in the text.
a. Adapt the formulas in part (d) of Exercise 8 to show that the curve x2 = cz2 −z3 is
parametrized by
z = c −t2,
x = t(c −t2).
b. Now replace the c in part (a) by y2, and explain how this leads to the above paramet-
rization of x2 −y2z2 + z3 = 0.
c. Explain why this parametrization covers the entire surface V(x2 −y2z2 + z3). Hint:
See part (c) of Exercise 8.
12. Consider the variety V = V(y −x2, z −x4) ⊆R3.
a. Draw a picture of V.

§3 Parametrizations of Afﬁne Varieties
27
b. Parametrize V in a way similar to what we did with the twisted cubic.
c. Parametrize the tangent surface of V.
13. The general problem of ﬁnding the equation of a parametrized surface will be studied in
Chapters 2 and 3. However, when the surface is a plane, methods from calculus or linear
algebra can be used. For example, consider the plane in R3 parametrized by
x = 1 + u −v,
y = u + 2v,
z = −1 −u + v.
Find the equation of the plane determined this way. Hint: Let the equation of the plane
be ax + by + cz = d. Then substitute in the above parametrization to obtain a sys-
tem of equations for a, b, c, d. Another way to solve the problem would be to write the
parametrization in vector form as (1, 0, −1) + u(1, 1, −1) + v(−1, 2, 1). Then one can
get a quick solution using the cross product.
14. This problem deals with convex sets and will be used in the next exercise to show that
a Bézier cubic lies within its control polygon. A subset C ⊆R2 is convex if for all
P, Q ∈C, the line segment joining P to Q also lies in C.
a. If P =
 x
y

and Q =
 z
w

lie in a convex set C, then show that
t

x
y

+ (1 −t)

z
w

∈C
when 0 ≤t ≤1.
b. If Pi =
 xi
yi

lies in a convex set C for 1 ≤i ≤n, then show that
n

i=1
ti

xi
yi

∈C
wherever t1, . . . , tn are nonnegative numbers such that n
i=1 ti = 1. Hint: Use induc-
tion on n.
15. Let a Bézier cubic be given by
x = (1 −t)3x0 + 3t(1 −t)2x1 + 3t2(1 −t)x2 + t3x3,
y = (1 −t)3y0 + 3t(1 −t)2y1 + 3t2(1 −t)y2 + t3y3.
a. Show that the above equations can be written in vector form

x
y

= (1 −t)3

x0
y0

+ 3t(1 −t)2

x1
y1

+ 3t2(1 −t)

x2
y2

+ t3

x3
y3

.
b. Use the previous exercise to show that a Bézier cubic always lies inside its control
polygon. Hint: In the above equations, what is the sum of the coefﬁcients?
16. One disadvantage of Bézier cubics is that curves like circles and hyperbolas cannot be
described exactly by cubics. In this exercise, we will discuss a method similar to exam-
ple (4) for parametrizing conic sections. Our treatment is based on BALL (1987) [see also
GOLDMAN (2003), Section 5.7].
A conic section is a curve in the plane deﬁned by a second degree equation of the
form ax2 + bxy + cy2 + dx + ey + f = 0. Conic sections include the familiar examples
of circles, ellipses, parabolas, and hyperbolas. Now consider the curve parametrized by

28
Chapter 1 Geometry, Algebra, and Algorithms
x = (1 −t)2x1 + 2t(1 −t)wx2 + t2x3
(1 −t)2 + 2t(1 −t)w + t2
,
y = (1 −t)2y1 + 2t(1 −t)wy2 + t2y3
(1 −t)2 + 2t(1 −t)w + t2
for 0 ≤t ≤1. The constants w, x1, y1, x2, y2, x3, y3 are speciﬁed by the design engi-
neer, and we will assume that w ≥0. In Chapter 3, we will show that these equations
parametrize a conic section. The goal of this exercise is to give a geometric interpretation
for the quantities w, x1, y1, x2, y2, x3, y3.
a. Show that our assumption w ≥0 implies that the denominator in the above formulas
never vanishes.
b. Evaluate the above formulas at t = 0 and t = 1. This should tell you what x1, y1, x3, y3
mean.
c. Now compute (x′(0), y′(0)) and (x′(1), y′(1)). Use this to show that (x2, y2) is the
intersection of the tangent lines at the start and end of the curve. Explain why
(x1, y1), (x2, y2), and (x3, y3) are called the control points of the curve.
d. Deﬁne the control polygon (it is actually a triangle in this case), and prove that the
curve deﬁned by the above equations always lies in its control polygon. Hint: Adapt
the argument of the previous exercise. This gives the following picture:
(x1,y1)
(x2,y2)
(x3,y3)
It remains to explain the constant w, which is called the shape factor. A hint should
come from the answer to part (c), for note that w appears in the formulas for the
tangent vectors when t = 0 and 1. So w somehow controls the "velocity," and a larger
w should force the curve closer to (x2, y2). In the last two parts of the problem, we
will determine exactly what w does.
e. Prove that
x( 1
2)
y( 1
2)

=
1
1 + w
1
2

x1
y1

+ 1
2

x3
y3

+
w
1 + w

x2
y2

.
Use this formula to show that (x( 1
2), y( 1
2)) lies on the line segment connecting (x2, y2)
to the midpoint of the line between (x1, y1) and (x3, y3).
a
b
(x1,y1)
(x2,y2)
(x3,y3)

§4 Ideals
29
f. Notice that (x( 1
2), y( 1
2)) divides this line segment into two pieces, say of lengths a
and b as indicated in the above picture. Then prove that
w = a
b,
so that w tells us exactly where the curve crosses this line segment. Hint: Use the
distance formula.
17. Use the formulas of the previous exercise to parametrize the arc of the circle x2 + y2 = 1
from (1, 0) to (0, 1). Hint: Use part (f) of Exercise 16 to show that w = 1/
√
2.
§4
Ideals
We next deﬁne the basic algebraic objects studied in this book.
Deﬁnition 1. A subset I ⊆k[x1, . . . , xn] is an ideal if it satisﬁes:
(i) 0 ∈I.
(ii) If f, g ∈I, then f + g ∈I.
(iii) If f ∈I and h ∈k[x1, . . . , xn], then hf ∈I.
The goal of this section is to introduce the reader to some naturally occurring
ideals and to see how ideals relate to afﬁne varieties. The real importance of ideals
is that they will give us a language for computing with afﬁne varieties.
The ﬁrst natural example of an ideal is the ideal generated by a ﬁnite number of
polynomials.
Deﬁnition 2. Let f1, . . . , fs be polynomials in k[x1, . . . , xn]. Then we set
⟨f1, . . . , fs⟩=

s

i=1
hi fi
 h1, . . . , hs ∈k[x1, . . . , xn]

.
The crucial fact is that ⟨f1, . . . , fs⟩is an ideal.
Lemma 3. If f1, . . . , fs ∈k[x1, . . . , xn], then ⟨f1, . . . , fs⟩is an ideal of k[x1, . . . , xn].
We will call ⟨f1, . . . , fs⟩the ideal generated by f1, . . . , fs.
Proof. First, 0 ∈⟨f1, . . . , fs⟩since 0 = s
i=1 0 · fi. Next, suppose that f =
s
i=1 pi fi and g = s
i=1 qi fi, and let h ∈k[x1, . . . , xn]. Then the equations
f + g =
s

i=1
(pi + qi)fi,
hf =
s

i=1
(hpi)fi
complete the proof that ⟨f1, . . . , fs⟩is an ideal.
□

30
Chapter 1 Geometry, Algebra, and Algorithms
The ideal ⟨f1, . . . , fs⟩has a nice interpretation in terms of polynomial equations.
Given f1, . . . , fs ∈k[x1, . . . , xn], we get the system of equations
f1 = 0,
...
fs = 0.
From these equations, one can derive others using algebra. For example, if we mul-
tiply the ﬁrst equation by h1 ∈k[x1, . . . , xn], the second by h2 ∈k[x1, . . . , xn], etc.,
and then add the resulting equations, we obtain
h1 f1 + h2 f2 + · · · + hs fs = 0,
which is a consequence of our original system. Notice that the left-hand side of
this equation is exactly an element of the ideal ⟨f1, . . . , fs⟩. Thus, we can think of
⟨f1, . . . , fs⟩as consisting of all "polynomial consequences" of the equations f1 =
f2 = · · · = fs = 0.
To see what this means in practice, consider the example from §3 where we took
x = 1 + t,
y = 1 + t2
and eliminated t to obtain
y = x2 −2x + 2
[see the discussion following equation (7) in §3]. Let us redo this example using the
above ideas. We start by writing the equations as
(1)
x −1 −t = 0,
y −1 −t2 = 0.
To cancel the terms involving t, we multiply the ﬁrst equation by x −1 + t and the
second by −1:
(x −1)2 −t2 = 0,
−y + 1 + t2 = 0,
and then add to obtain
(x −1)2 −y + 1 = x2 −2x + 2 −y = 0.
In terms of the ideal generated by equations (1), we can write this as
x2 −2x + 2 −y = (x −1 + t)(x −1 −t) + (−1)(y −1 −t2)
∈⟨x −1 −t, y −1 −t2⟩.

§4 Ideals
31
Similarly, any other "polynomial consequence" of (1) leads to an element of this
ideal.
We say that an ideal I is ﬁnitely generated if there exist f1, . . . , fs ∈k[x1, . . . , xn]
such that I = ⟨f1, . . . , fs⟩, and we say that f1, . . . , fs, are a basis of I. In Chapter 2,
we will prove the amazing fact that every ideal of k[x1, . . . , xn] is ﬁnitely generated
(this is known as the Hilbert Basis Theorem). Note that a given ideal may have many
different bases. In Chapter 2, we will show that one can choose an especially useful
type of basis, called a Gröbner basis.
There is a nice analogy with linear algebra that can be made here. The deﬁnition
of an ideal is similar to the deﬁnition of a subspace: both have to be closed un-
der addition and multiplication, except that, for a subspace, we multiply by scalars,
whereas for an ideal, we multiply by polynomials. Further, notice that the ideal gen-
erated by polynomials f1, . . . , fs is similar to the span of a ﬁnite number of vectors
v1, . . . , vs. In each case, one takes linear combinations, using ﬁeld coefﬁcients for
the span and polynomial coefﬁcients for the ideal. Relations with linear algebra are
explored further in Exercise 6.
Another indication of the role played by ideals is the following proposition,
which shows that a variety depends only on the ideal generated by its deﬁning
equations.
Proposition 4. If f1, . . . , fs and g1, . . . , gt are bases of the same ideal in k[x1, . . . , xn],
so that ⟨f1, . . . , fs⟩= ⟨g1, . . . , gt⟩, then we have V( f1, . . . , fs) = V(g1, . . . , gt).
Proof. The proof is very straightforward and is left as an exercise.
□
As an example, consider the variety V(2x2 + 3y2 −11, x2 −y2 −3). It is easy to
show that ⟨2x2 + 3y2 −11, x2 −y2 −3⟩= ⟨x2 −4, y2 −1⟩(see Exercise 3), so that
V(2x2 + 3y2 −11, x2 −y2 −3) = V(x2 −4, y2 −1) = {(±2, ±1)}
by the above proposition. Thus, by changing the basis of the ideal, we made it easier
to determine the variety.
The ability to change the basis without affecting the variety is very important.
Later in the book, this will lead to the observation that afﬁne varieties are determined
by ideals, not equations. (In fact, the correspondence between ideals and varieties
is the main topic of Chapter 4.) From a more practical point of view, we will also
see that Proposition 4, when combined with the Gröbner bases mentioned above,
provides a powerful tool for understanding afﬁne varieties.
We will next discuss how afﬁne varieties give rise to an interesting class of ideals.
Suppose we have an afﬁne variety V = V( f1, . . . , fs) ⊆kn deﬁned by f1, . . . , fs ∈
k[x1, . . . , xn]. We know that f1, . . . , fs vanish on V, but are these the only ones? Are
there other polynomials that vanish on V? For example, consider the twisted cubic
studied in §2. This curve is deﬁned by the vanishing of y −x2 and z −x3. From the
parametrization (t, t2, t3) discussed in §3, we see that z−xy and y2−xz are two more
polynomials that vanish on the twisted cubic. Are there other such polynomials?
How do we ﬁnd them all?

32
Chapter 1 Geometry, Algebra, and Algorithms
To study this question, we will consider the set of all polynomials that vanish on
a given variety.
Deﬁnition 5. Let V ⊆kn be an afﬁne variety. Then we set
I(V) = {f ∈k[x1, . . . , xn] | f(a1, . . . , an) = 0 for all (a1, . . . , an) ∈V}.
The crucial observation is that I(V) is an ideal.
Lemma 6. If V ⊆kn is an afﬁne variety, then I(V) ⊆k[x1, . . . , xn] is an ideal. We
will call I(V) the ideal of V.
Proof. It is obvious that 0 ∈I(V) since the zero polynomial vanishes on all of
kn, and so, in particular it vanishes on V. Next, suppose that f, g ∈I(V) and h ∈
k[x1, . . . , xn].
Let (a1, . . . , an) be an arbitrary point of V. Then
f(a1, . . . , an) + g(a1, . . . , an) = 0 + 0 = 0,
h(a1, . . . , an) f(a1, . . . , an) = h(a1, . . . , an) · 0 = 0,
and it follows that I(V) is an ideal.
□
For an example of the ideal of a variety, consider the variety {(0, 0)} consisting
of the origin in k2. Then its ideal I({(0, 0)}) consists of all polynomials that vanish
at the origin, and we claim that
I({(0, 0)}) = ⟨x, y⟩.
One direction of proof is trivial, for any polynomial of the form A(x, y)x + B(x, y)y
obviously vanishes at the origin. Going the other way, suppose that f = 
i,j aijxiyj
vanishes at the origin. Then a00 = f(0, 0) = 0 and, consequently,
f = a00 +

i,j̸=0,0
aijxi yj
= 0 +
 
i,j
i>0
aijxi−1yj

x +
 
j>0
a0jyj−1

y ∈⟨x, y⟩.
Our claim is now proved.
For another example, consider the case when V is all of kn. Then I(kn) consists
of polynomials that vanish everywhere, and, hence, by Proposition 5 of §1, we have
I(kn) = {0}
when k is inﬁnite.
(Here, "0" denotes the zero polynomial in k[x1, . . . , xn].) Note that Proposition 5
of §1 is equivalent to the above statement. In the exercises, we will discuss what
happens when k is a ﬁnite ﬁeld.

§4 Ideals
33
A more interesting example is given by the twisted cubic V = V(y −x2, z −x3)
in R3. We claim that
I(V) = ⟨y −x2, z −x3⟩.
To prove this, we will ﬁrst show that given a polynomial f ∈R[x, y, z], we can write
f in the form
(2)
f = h1(y −x2) + h2(z −x3) + r,
where h1, h2 ∈R[x, y, z] and r is a polynomial in the variable x alone. First, consider
the case when f is a monomial xαyβzγ. Then the binomial theorem tells us that
xαyβzγ = xα(x2 + (y −x2))β(x3 + (z −x3))γ
= xα(x2β + terms involving y −x2)(x3γ + terms involving z −x3),
and multiplying this out shows that
xαyβzγ = h1(y −x2) + h2(z −x3) + xα+2β+3γ
for some polynomials h1, h2 ∈R[x, y, z]. Thus, (2) is true in this case. Since an
arbitrary f ∈R[x, y, z] is an R-linear combination of monomials, it follows that (2)
holds in general.
We can now prove I(V) = ⟨y −x2, z −x3⟩. First, by the deﬁnition of the twisted
cubic V, we have y −x2, z −x3 ∈I(V), and since I(V) is an ideal, it follows that
h1(y −x2) + h2(z −x3) ∈I(V). This proves that ⟨y −x2, z −x3⟩⊆I(V). To prove
the opposite inclusion, let f ∈I(V) and let
f = h1(y −x2) + h2(z −x3) + r
be the expression given by (2). To prove that r is zero, we will use the parametriza-
tion (t, t2, t3) of the twisted cubic. Since f vanishes on V, we obtain
0 = f(t, t2, t3) = 0 + 0 + r(t)
(recall that r is a polynomial in x alone). Since t can be any real number, r ∈R[x]
must be the zero polynomial by Proposition 5 of §1. But r = 0 shows that f has the
desired form, and I(V) = ⟨y −x2, z −x3⟩is proved.
What we did in (2) is reminiscent of the division of polynomials, except that we
are dividing by two polynomials instead of one. In fact, (2) is a special case of the
generalized division algorithm to be studied in Chapter 2.
A nice corollary of the above example is that given a polynomial f ∈R[x, y, z],
we have f ∈⟨y −x2, z −x3⟩if and only if f(t, t2, t3) is identically zero. This gives
us an algorithm for deciding whether a polynomial lies in the ideal. However, this
method is dependent on the parametrization (t, t2, t3). Is there a way of deciding
whether f ∈⟨y−x2, z−x3⟩without using the parametrization? In Chapter 2, we will
answer this question positively using Gröbner bases and the generalized division
algorithm.

34
Chapter 1 Geometry, Algebra, and Algorithms
The example of the twisted cubic is very suggestive. We started with the poly-
nomials y −x2 and z −x3, used them to deﬁne an afﬁne variety, took all functions
vanishing on the variety, and got back the ideal generated by the two polynomials.
It is natural to wonder if this happens in general. So take f1, . . . , fs ∈k[x1, . . . , xn].
This gives us
polynomials
variety
ideal
f1, . . . , fs
−→V( f1, . . . , fs) −→I(V( f1, . . . , fs)),
and the natural question to ask is whether I(V( f1, . . . , fs)) = ⟨f1, . . . , fs⟩? The an-
swer, unfortunately, is not always yes. Here is the best answer we can give at this
point.
Lemma 7. Let f1, . . . , fs ∈k[x1, . . . , xn]. Then ⟨f1, . . . , fs⟩⊆I(V( f1, . . . , fs)), al-
though equality need not occur.
Proof. Let f ∈⟨f1, . . . , fs⟩, which means that f = s
i=1 hi fi for some polyno-
mials h1, . . . , hs ∈k[x1, . . . , xn]. Since f1, . . . , fs vanish on V( f1, . . . , fs), so must
s
i=1 hi fi. Thus, f vanishes on V( f1, . . . , fs), which proves f ∈I(V( f1, . . . , fs)).
For the second part of the lemma, we need an example where I(V( f1, . . . , fs)) is
strictly larger than ⟨f1, . . . , fs⟩. We will show that the inclusion
⟨x2, y2⟩⊆I(V(x2, y2))
is not an equality. We ﬁrst compute I(V(x2, y2)). The equations x2 = y2 = 0 imply
that V(x2, y2) = {(0, 0)}. But an earlier example showed that the ideal of {(0, 0)}
is ⟨x, y⟩, so that I(V(x2, y2)) = ⟨x, y⟩. To see that this is strictly larger than ⟨x2, y2⟩,
note that x /∈⟨x2, y2⟩since for polynomials of the form h1(x, y)x2 +h2(x, y)y2, every
monomial has total degree at least two.
□
For arbitrary ﬁelds, the relationship between ⟨f1, . . . , fs⟩and I(V( f1, . . . , fs))
can be rather subtle (see the exercises for some examples). However, over an alge-
braically closed ﬁeld like C, there is a straightforward relation between these ideals.
This will be explained when we prove the Nullstellensatz in Chapter 4.
Although for a general ﬁeld, I(V( f1, . . . , fs)) may not equal ⟨f1, . . . , fs⟩, the ideal
of a variety always contains enough information to determine the variety uniquely.
Proposition 8. Let V and W be afﬁne varieties in kn. Then:
(i) V ⊆W if and only if I(V) ⊇I(W).
(ii) V = W if and only if I(V) = I(W).
Proof. We leave it as an exercise to show that (ii) is an immediate consequence of
(i). To prove (i), ﬁrst suppose that V ⊆W. Then any polynomial vanishing on W
must vanish on V, which proves I(W) ⊆I(V). Next, assume that I(W) ⊆I(V). We
know that W is the variety deﬁned by some polynomials g1, . . . , gt ∈k[x1, . . . , xn].
Then g1, . . . , gt ∈I(W) ⊆I(V), and hence the gi's vanish on V. Since W consists
of all common zeros of the gi's, it follows that V ⊆W.
□

§4 Ideals
35
There is a rich relationship between ideals and afﬁne varieties; the material pre-
sented so far is just the tip of the iceberg. We will explore this relation further in
Chapter 4. In particular, we will see that theorems proved about ideals have strong
geometric implications. For now, let us list three questions we can pose concerning
ideals in k[x1, . . . , xn]:
• (Ideal Description) Can every ideal I ⊆k[x1, . . . , xn] be written as ⟨f1, . . . , fs⟩for
some f1, . . . , fs ∈k[x1, . . . , xn]?
• (Ideal Membership) If f1, . . . , fs ∈k[x1, . . . , xn], is there an algorithm to decide
whether a given f ∈k[x1, . . . , xn] lies in ⟨f1, . . . , fs⟩?
• (Nullstellensatz) Given f1, . . . , fs ∈k[x1, . . . , xn], what is the exact relation be-
tween ⟨f1, . . . , fs⟩and I(V( f1, . . . , fs))?
In the chapters that follow, we will solve these problems completely (and we will
explain where the name Nullstellensatz comes from), although we will need to be
careful about which ﬁeld we are working over.
EXERCISES FOR §4
1. Consider the equations
x2 + y2 −1 = 0,
xy −1 = 0
which describe the intersection of a circle and a hyperbola.
a. Use algebra to eliminate y from the above equations.
b. Show how the polynomial found in part (a) lies in ⟨x2 + y2 −1, xy −1⟩. Your answer
should be similar to what we did in (1). Hint: Multiply the second equation by xy+ 1.
2. Let I ⊆k[x1, . . . , xn] be an ideal, and let f1, . . . , fs ∈k[x1, . . . , xn]. Prove that the follow-
ing statements are equivalent:
(i) f1, . . . , fs ∈I.
(ii) ⟨f1, . . . , fs⟩⊆I.
This fact is useful when you want to show that one ideal is contained in another.
3. Use the previous exercise to prove the following equalities of ideals in Q[x, y]:
a. ⟨x + y, x −y⟩= ⟨x, y⟩.
b. ⟨x + xy, y + xy, x2, y2⟩= ⟨x, y⟩.
c. ⟨2x2 + 3y2 −11, x2 −y2 −3⟩= ⟨x2 −4, y2 −1⟩.
This illustrates that the same ideal can have many different bases and that different bases
may have different numbers of elements.
4. Prove Proposition 4.
5. Show that V(x + xy, y + xy, x2, y2) = V(x, y). Hint: See Exercise 3.
6. The word "basis" is used in various ways in mathematics. In this exercise, we will see
that "a basis of an ideal," as deﬁned in this section, is quite different from "a basis of a
subspace," which is studied in linear algebra.
a. First, consider the ideal I = ⟨x⟩⊆k[x]. As an ideal, I has a basis consisting of the
one element x. But I can also be regarded as a subspace of k[x], which is a vector
space over k. Prove that any vector space basis of I over k is inﬁnite. Hint: It sufﬁces
to ﬁnd one basis that is inﬁnite. Thus, allowing x to be multiplied by elements of k[x]
instead of just k is what enables ⟨x⟩to have a ﬁnite basis.

36
Chapter 1 Geometry, Algebra, and Algorithms
b. In linear algebra, a basis must span and be linearly independent over k, whereas for
an ideal, a basis is concerned only with spanning—there is no mention of any sort of
independence. The reason is that once we allow polynomial coefﬁcients, no indepen-
dence is possible. To see this, consider the ideal ⟨x, y⟩⊆k[x, y]. Show that zero can
be written as a linear combination of y and x with nonzero polynomial coefﬁcients.
c. More generally, suppose that f1, . . . , fs is the basis of an ideal I ⊆k[x1, . . . , xn]. If
s ≥2 and fi ̸= 0 for all i, then show that for any i and j, zero can be written as a linear
combination of fi and fj with nonzero polynomial coefﬁcients.
d. A consequence of the lack of independence is that when we write an element f ∈
⟨f1, . . . , fs⟩as f = s
i=1 hi fi, the coefﬁcients hi are not unique. As an example,
consider f = x2 + xy + y2 ∈⟨x, y⟩. Express f as a linear combination of x and y in
two different ways. (Even though the hi's are not unique, one can measure their lack
of uniqueness. This leads to the interesting topic of syzygies.)
e. A basis f1, . . . , fs of an ideal I is said to be minimal if no proper subset of f1, . . . , fs is
a basis of I. For example, x, x2 is a basis of an ideal, but not a minimal basis since x
generates the same ideal. Unfortunately, an ideal can have minimal bases consisting
of different numbers of elements. To see this, show that x and x + x2, x2 are minimal
bases of the same ideal of k[x]. Explain how this contrasts with the situation in linear
algebra.
7. Show that I(V(xn, ym)) = ⟨x, y⟩for any positive integers n and m.
8. The ideal I(V) of a variety has a special property not shared by all ideals. Speciﬁcally,
we deﬁne an ideal I to be radical if whenever a power f m of a polynomial f is in I, then
f itself is in I. More succinctly, I is radical when f ∈I if and only if f m ∈I for some
positive integer m.
a. Prove that I(V) is always a radical ideal.
b. Prove that ⟨x2, y2⟩is not a radical ideal. This implies that ⟨x2, y2⟩̸= I(V) for any
variety V ⊆k2.
Radical ideals will play an important role in Chapter 4. In particular, the Nullstellensatz
will imply that there is a one-to-one correspondence between varieties in Cn and radical
ideals in C[x1, . . . , xn].
9. Let V = V(y −x2, z −x3) be the twisted cubic. In the text, we showed that I(V) =
⟨y −x2, z −x3⟩.
a. Use the parametrization of the twisted cubic to show that y2 −xz ∈I(V).
b. Use the argument given in the text to express y2 −xz as a combination of y −x2 and
z −x3.
10. Use the argument given in the discussion of the twisted cubic to show that I(V(x−y)) =
⟨x −y⟩. Your argument should be valid for any inﬁnite ﬁeld k.
11. Let V ⊆R3 be the curve parametrized by (t, t3, t4).
a. Prove that V is an afﬁne variety.
b. Adapt the method used in the case of the twisted cubic to determine I(V).
12. Let V ⊆R3 be the curve parametrized by (t2, t3, t4).
a. Prove that V is an afﬁne variety.
b. Determine I(V).
This problem is quite a bit more challenging than the previous one—ﬁguring out the
proper analogue of equation (2) is not easy. Once we study the division algorithm in
Chapter 2, this exercise will become much easier.
13. In Exercise 2 of §1, we showed that x2y+y2x vanishes at all points of F2
2. More generally,
let I ⊆F2[x, y] be the ideal of all polynomials that vanish at all points of F2
2. The goal of
this exercise is to show that I = ⟨x2 −x, y2 −y⟩.
a. Show that ⟨x2 −x, y2 −y⟩⊆I.

§5 Polynomials of One Variable
37
b. Show that every f ∈F2[x, y] can be written as f = A(x2 −x) + B(y2 −y) + axy
+ bx + cy + d, where A, B ∈F2[x, y] and a, b, c, d ∈F2. Hint: Write f in the form

i pi(x)yi and use the division algorithm (Proposition 2 of §5) to divide each pi by
x2 −x. From this, you can write f = A(x2 −x) + q1(y)x + q2(y). Now divide q1
and q2 by y2 −y. Again, this argument will become vastly simpler once we know the
division algorithm from Chapter 2.
c. Show that axy + bx + cy + d ∈I if and only if a = b = c = d = 0.
d. Using parts (b) and (c), complete the proof that I = ⟨x2 −x, y2 −y⟩.
e. Express x2y + y2x as a combination of x2 −x and y2 −y. Hint: Remember that
2 = 1 + 1 = 0 in F2.
14. This exercise is concerned with Proposition 8.
a. Prove that part (ii) of the proposition follows from part (i).
b. Prove the following corollary of the proposition: if V and W are afﬁne varieties in kn,
then V ⊊W if and only if I(V) ⊋I(W).
15. In the text, we deﬁned I(V) for a variety V ⊆kn. We can generalize this as follows: if
S ⊆kn is any subset, then we set
I(S) = {f ∈k[x1, . . . , xn] | f (a1, . . . , an) = 0 for all (a1, . . . , an) ∈S}.
a. Prove that I(S) is an ideal.
b. Let X = {(a, a) ∈R2 | a ̸= 1}. By Exercise 8 of §2, we know that X is not an afﬁne
variety. Determine I(X). Hint: What you proved in Exercise 8 of §2 will be useful.
See also Exercise 10 of this section.
c. Let Zn be the points of Cn with integer coordinates. Determine I(Zn). Hint: See Ex-
ercise 6 of §1.
16. Here is more practice with ideals. Let I be an ideal in k[x1, . . . , xn].
a. Prove that 1 ∈I if and only if I = k[x1, . . . , xn].
b. More generally, prove that I contains a nonzero constant if and only if I = k[x1, . . . , xn].
c. Suppose f , g ∈k[x1, . . . , xn] satisfy f 2, g2 ∈I. Prove that (f + g)3 ∈I. Hint: Expand
(f + g)3 using the Binomial Theorem.
d. Now suppose f , g ∈k[x1, . . . , xn] satisfy f r, gs ∈I. Prove that (f + g)r+s−1 ∈I.
17. In the proof of Lemma 7, we showed that x /∈⟨x2, y2⟩in k[x, y].
a. Prove that xy /∈⟨x2, y2⟩.
b. Prove that 1, x, y, xy are the only monomials not contained in ⟨x2, y2⟩.
18. In the text, we showed that I({(0, 0)}) = ⟨x, y⟩in k[x, y].
a. Generalize this by proving that the origin 0 = (0, . . . , 0) ∈kn has the property that
I({0}) = ⟨x1, . . . , xn⟩in k[x1, . . . , xn].
b. What does part (a) say about polynomials in k[x1, . . . , xn] with zero constant term?
19. One of the key ideas of this section is that a system of equations f1 = · · · = fs = 0 gives
the ideal I = ⟨f1, . . . , fs⟩of polynomial consequences. Now suppose that the system
has a consequence of the form f = g and we take the mth power of each side to obtain
f m = gm. In terms of the ideal I, this means that f −g ∈I should imply f m −gm ∈I.
Prove this by factoring f m −gm.
§5
Polynomials of One Variable
In this section, we will discuss polynomials of one variable and study the division
algorithm from high school algebra. This simple algorithm has some surprisingly
deep consequences—for example, we will use it to determine the structure of ideals

38
Chapter 1 Geometry, Algebra, and Algorithms
of k[x] and to explore the idea of a greatest common divisor. The theory developed
will allow us to solve, in the special case of polynomials in k[x], most of the prob-
lems raised in earlier sections. We will also begin to understand the important role
played by algorithms.
By this point in their mathematics careers, most students have already seen a
variety of algorithms, although the term "algorithm" may not have been used. In-
formally, an algorithm is a speciﬁc set of instructions for manipulating symbolic
or numerical data. Examples are the differentiation formulas from calculus and the
method of row reduction from linear algebra. An algorithm will have inputs, which
are objects used by the algorithm, and outputs, which are the results of the algo-
rithm. At each stage of execution, the algorithm must specify exactly what the next
step will be.
When we are studying an algorithm, we will usually present it in "pseudocode,"
which will make the formal structure easier to understand. Pseudocode is similar to
many common computer languages, and a brief discussion is given in Appendix B.
Another reason for using pseudocode is that it indicates how the algorithm could
be programmed on a computer. We should also mention that most of the algorithms
in this book are implemented in Maple, Mathematica, and many other computer
algebra systems. Appendix C has more details concerning these programs.
We begin by discussing the division algorithm for polynomials in k[x]. A crucial
component of this algorithm is the notion of the "leading term" of a polynomial in
one variable. The precise deﬁnition is as follows.
Deﬁnition 1. Given a nonzero polynomial f ∈k[x], let
f = c0xm + c1xm−1 + · · · + cm,
where ci ∈k and c0 ̸= 0 [thus, m = deg(f)]. Then we say that c0xm is the leading
term of f, written LT( f) = c0xm.
For example, if f = 2x3 −4x + 3, then LT( f) = 2x3. Notice also that if f and g
are nonzero polynomials, then
(1)
deg( f) ≤deg(g) ⇐⇒LT( f) divides LT(g).
We can now describe the division algorithm.
Proposition 2 (The Division Algorithm). Let k be a ﬁeld and let g be a nonzero
polynomial in k[x]. Then every f ∈k[x] can be written as
f = qg + r,
where q, r ∈k[x], and either r = 0 or deg(r) < deg(g). Furthermore, q and r are
unique, and there is an algorithm for ﬁnding q and r.

§5 Polynomials of One Variable
39
Proof. Here is the algorithm for ﬁnding q and r, presented in pseudocode:
Input : g, f
Output : q, r
q := 0; r := f
WHILE r ̸= 0 AND LT(g) divides LT(r) DO
q := q + LT(r)/LT(g)
r := r −(LT(r)/LT(g)) g
RETURN q, r
The WHILE...DO statement means doing the indented operations until the expres-
sion between the WHILE and DO becomes false. The statements q := . . . and
r := . . . indicate that we are deﬁning or redeﬁning the values of q and r. Both q and
r are variables in this algorithm—they change value at each step. We need to show
that the algorithm terminates and that the ﬁnal values of q and r have the required
properties. (For a fuller discussion of pseudocode, see Appendix B.)
To see why this algorithm works, ﬁrst note that f = qg + r holds for the initial
values of q and r, and that whenever we redeﬁne q and r, the equality f = qg + r
remains true. This is because of the identity
f = qg + r = (q + LT(r)/LT(g)) g + (r −(LT(r)/LT(g)) g).
Next, note that the WHILE...DO statement terminates when "r ̸= 0 and LT(g) di-
vides LT(r)" is false, i.e., when either r = 0 or LT(g) does not divide LT(r). By (5),
this last statement is equivalent to deg(r) < deg(g). Thus, when the algorithm ter-
minates, it produces q and r with the required properties.
We are not quite done; we still need to show that the algorithm terminates,
i.e., that the expression between the WHILE and DO eventually becomes false
(otherwise, we would be stuck in an inﬁnite loop). The key observation is that
r −(LT(r)/LT(g)) g is either 0 or has smaller degree than r. To see why, suppose
that
r = c0xm + · · · + cm,
LT(r) = c0xm,
g = d0xℓ+ · · · + dℓ,
LT(g) = d0xℓ,
and suppose that m ≥ℓ. Then
r −(LT(r)/LT(g)) g = (c0xm + · · · ) −(c0/d0)xm−ℓ(d0xℓ+ · · · ),
and it follows that the degree of r must drop (or the whole expression may vanish).
Since the degree is ﬁnite, it can drop at most ﬁnitely many times, which proves that
the algorithm terminates.
To see how this algorithm corresponds to the process learned in high school,
consider the following partially completed division:

40
Chapter 1 Geometry, Algebra, and Algorithms
2x + 1
	
x3 + 2x2 + x + 1
1
2x2
x3 + 1
2x2
3
2x2 + x + 1
Here, f and g are given by f = x3+2x2+x+1 and g = 2x+1, and more importantly,
the current (but not ﬁnal) values of q and r are q = 1
2x2 and r = 3
2x2 + x + 1. Now
notice that the statements
q : = q + LT(r)/LT(g),
r : = r −(LT(r)/LT(g))g
in the WHILE...DO loop correspond exactly to the next step in the above division.
The ﬁnal step in proving the proposition is to show that q and r are unique. So
suppose that f = qg + r = q′g + r′ where both r and r′ have degree less than g
(unless one or both are 0). If r ̸= r′, then deg(r′ −r) < deg(g). On the other hand,
since
(2)
(q −q′)g = r′ −r,
we would have q −q′ ̸= 0, and consequently,
deg(r′ −r) = deg((q −q′)g) = deg(q −q′) + deg(g) ≥deg(g).
This contradiction forces r = r′, and then (2) shows that q = q′. This completes the
proof of the proposition.
□
Most computer algebra systems implement the above algorithm [with some
modiﬁcations—see VON ZUR GATHEN and GERHARD (2013)] for dividing poly-
nomials.
A useful corollary of the division algorithm concerns the number of roots of a
polynomial in one variable.
Corollary 3. If k is a ﬁeld and f ∈k[x] is a nonzero polynomial, then f has at most
deg( f) roots in k.
Proof. We will use induction on m = deg( f). When m = 0, f is a nonzero constant,
and the corollary is obviously true. Now assume that the corollary holds for all
polynomials of degree m −1, and let f have degree m. If f has no roots in k, then
we are done. So suppose a is a root in k. If we divide f by x −a, then Proposition 2
tells us that f = q(x −a) + r, where r ∈k since x −a has degree one. To determine
r, evaluate both sides at x = a, which gives 0 = f(a) = q(a)(a −a) + r = r. It
follows that f = q(x −a). Note also that q has degree m −1.
We claim that any root of f other than a is also a root of q. To see this, let b ̸= a
be a root of f. Then 0 = f(b) = q(b)(b −a) implies that q(b) = 0 since k is a ﬁeld.
Since q has at most m −1 roots by our inductive assumption, f has at most m roots
in k. This completes the proof.
□

§5 Polynomials of One Variable
41
Corollary 3 was used to prove Proposition 5 in §1, which states that I(kn) =
{0} whenever k is inﬁnite. This is an example of how a geometric fact can be the
consequence of an algorithm.
We can also use Proposition 2 to determine the structure of all ideals of k[x].
Corollary 4. If k is a ﬁeld, then every ideal of k[x] can be written as ⟨f⟩for some
f ∈k[x]. Furthermore, f is unique up to multiplication by a nonzero constant in k.
Proof. Take an ideal I ⊆k[x]. If I = {0}, then we are done since I = ⟨0⟩. Other-
wise, let f be a nonzero polynomial of minimum degree contained in I. We claim that
⟨f⟩= I. The inclusion ⟨f⟩⊆I is obvious since I is an ideal. Going the other way,
take g ∈I. By division algorithm (Proposition 2), we have g = q f + r, where either
r = 0 or deg(r) < deg( f). Since I is an ideal, q f ∈I and, thus, r = g −q f ∈I.
If r were not zero, then deg(r) < deg( f), which would contradict our choice of f.
Thus, r = 0, so that g = q f ∈⟨f⟩. This proves that I = ⟨f⟩.
To study uniqueness, suppose that ⟨f ⟩= ⟨g⟩. Then f ∈⟨g⟩implies that f = hg
for some polynomial h. Thus,
(3)
deg( f) = deg(h) + deg(g),
so that deg( f) ≥deg(g). The same argument with f and g interchanged shows
deg( f) ≤deg(g), and it follows that deg( f) = deg(g). Then (3) implies deg(h) =
0, so that h is a nonzero constant.
□
In general, an ideal generated by one element is called a principal ideal. In view
of Corollary 4, we say that k[x] is a principal ideal domain, abbreviated PID.
The proof of Corollary 4 tells us that the generator of an ideal in k[x] is the
nonzero polynomial of minimum degree contained in the ideal. This description is
not useful in practice, for it requires that we check the degrees of all polynomials
(there are inﬁnitely many) in the ideal. Is there a better way to ﬁnd the generator?
For example, how do we ﬁnd a generator of the ideal
⟨x4 −1, x6 −1⟩⊆k[x]?
The tool needed to solve this problem is the greatest common divisor.
Deﬁnition 5. A greatest common divisor of polynomials f, g ∈k[x] is a polyno-
mial h such that:
(i) h divides f and g.
(ii) If p is another polynomial which divides f and g, then p divides h. When h has
these properties, we write h = gcd( f, g).
Here are the main properties of gcd's.
Proposition 6. Let f, g ∈k[x]. Then:
(i) gcd( f, g) exists and is unique up to multiplication by a nonzero constant in k.
(ii) gcd( f, g) is a generator of the ideal ⟨f, g⟩.
(iii) There is an algorithm for ﬁnding gcd( f, g).

42
Chapter 1 Geometry, Algebra, and Algorithms
Proof. Consider the ideal ⟨f, g⟩. Since every ideal of k[x] is principal (Corollary 4),
there exists h ∈k[x] such that ⟨f, g⟩= ⟨h⟩. We claim that h is the gcd of f, g. To
see this, ﬁrst note that h divides f and g since f, g ∈⟨h⟩. Thus, the ﬁrst part of
Deﬁnition 5 is satisﬁed. Next, suppose that p ∈k[x] divides f and g. This means that
f = Cp and g = Dp for some C, D ∈k[x]. Since h ∈⟨f, g⟩, there are A, B such that
Af + Bg = h. Substituting, we obtain
h = Af + Bg = ACp + BDp = (AC + BD)p,
which shows that p divides h. Thus, h = gcd( f, g).
This proves the existence of the gcd. To prove uniqueness, suppose that h′ was
another gcd of f and g. Then, by the second part of Deﬁnition 5, h and h′ would
each divide the other. This easily implies that h is a nonzero constant multiple of h′.
Thus, part (i) of the corollary is proved, and part (ii) follows by the way we found h
in the above paragraph.
The existence proof just given is not useful in practice. It depends on our ability
to ﬁnd a generator of ⟨f, g⟩. As we noted in the discussion following Corollary 4,
this involves checking the degrees of inﬁnitely many polynomials. Fortunately, there
is a classic algorithm, known as the Euclidean Algorithm, which computes the gcd
of two polynomials in k[x]. This is what part (iii) of the proposition is all about.
We will need the following notation. Let f, g ∈k[x], where g ̸= 0, and write
f = qg + r, where q and r are as in Proposition 2. Then we set r = remainder( f, g).
We can now state the Euclidean Algorithm for ﬁnding gcd( f, g):
Input : f, g
Output : h = gcd( f, g)
h := f
s := g
WHILE s ̸= 0 DO
rem := remainder(h, s)
h := s
s := rem
RETURN h
To see why this algorithm computes the gcd, write f = qg + r as in Proposition 2.
We claim that
(4)
gcd( f, g) = gcd( f −qg, g) = gcd(r, g).
To prove this, by part (ii) of the proposition, it sufﬁces to show that the ideals ⟨f, g⟩
and ⟨f −qg, g⟩are equal. We will leave this easy argument as an exercise.
We can write (4) in the form
gcd( f, g) = gcd(g, r).

§5 Polynomials of One Variable
43
Notice that deg(g) > deg(r) or r = 0. If r ̸= 0, we can make things yet smaller by
repeating this process. Thus, we write g = q′r + r′ as in Proposition 2, and arguing
as above, we obtain
gcd(g, r) = gcd(r, r′),
where deg(r) > deg(r′) or r′ = 0. Continuing in this way, we get
(5)
gcd( f, g) = gcd(g, r) = gcd(r, r′) = gcd(r′, r′′) = · · · ,
where either the degrees drop
deg(g) > deg(r) > deg(r′) > deg(r′′) > · · · ,
or the process terminates when one of r, r′, r′′, . . . becomes 0.
We can now explain how the Euclidean Algorithm works. The algorithm has
variables h and s, and we can see these variables in equation (5): the values of
h are the ﬁrst polynomial in each gcd, and the values of s are the second. You
should check that in (5), going from one gcd to the next is exactly what is done
in the WHILE...DO loop of the algorithm. Thus, at every stage of the algorithm,
gcd(h, s) = gcd( f, g).
The algorithm must terminate because the degree of s keeps dropping, so that
at some stage, s = 0. When this happens, we have gcd(h, 0) = gcd( f, g), and
since ⟨h, 0⟩obviously equals ⟨h⟩, we have gcd(h, 0) = h. Combining these last two
equations, it follows that h = gcd( f, g) when s = 0. This proves that h is the gcd
of f and g when the algorithm terminates, and the proof of Proposition 6 is now
complete.
□
We should mention that there is also a version of the Euclidean Algorithm for
ﬁnding the gcd of two integers. Most computer algebra systems have a command
for ﬁnding the gcd of two polynomials (or integers) that uses a modiﬁed form of
the Euclidean Algorithm [see VON ZUR GATHEN and GERHARD (2013) for more
details].
For an example of how the Euclidean Algorithm works, let us compute the gcd
of x4 −1 and x6 −1. First, we use the division algorithm:
x4 −1 = 0(x6 −1) + x4 −1,
x6 −1 = x2(x4 −1) + x2 −1,
x4 −1 = (x2 + 1)(x2 −1) + 0.
Then, by equation (5), we have
gcd(x4 −1, x6 −1) = gcd(x6 −1, x4 −1)
= gcd(x4 −1, x2 −1) = gcd(x2 −1, 0) = x2 −1.
Note that this gcd computation answers our earlier question of ﬁnding a generator
for the ideal ⟨x4 −1, x6 −1⟩. Namely, Proposition 6 and gcd(x4 −1, x6 −1) = x2 −1
imply that

44
Chapter 1 Geometry, Algebra, and Algorithms
⟨x4 −1, x6 −1⟩= ⟨x2 −1⟩.
At this point, it is natural to ask what happens for an ideal generated by three or
more polynomials. How do we ﬁnd a generator in this case? The idea is to extend
the deﬁnition of gcd to more than two polynomials.
Deﬁnition 7. A greatest common divisor of polynomials f1, . . . , fs ∈k[x] is a poly-
nomial h such that:
(i) h divides f1, . . . , fs.
(ii) If p is another polynomial which divides f1, . . . , fs, then p divides h.
When h has these properties, we write h = gcd( f1, . . . , fs).
Here are the main properties of these gcd's.
Proposition 8. Let f1, . . . , fs ∈k[x],where s ≥2. Then:
(i) gcd( f1, . . . , fs) exists and is unique up to multiplication by a nonzero constant
in k.
(ii) gcd( f1, . . . , fs) is a generator of the ideal ⟨f1, . . . , fs⟩.
(iii) If s ≥3, then gcd( f1, . . . , fs) = gcd( f1, gcd( f2, . . . , fs)).
(iv) There is an algorithm for ﬁnding gcd( f1, . . . , fs).
Proof. The proofs of parts (i) and (ii) are similar to the proofs given in Proposition 6
and will be omitted. To prove part (iii), let h = gcd( f2, . . . , fs). We leave it as an
exercise to show that
⟨f1, h ⟩= ⟨f1, f2, . . . , fs ⟩.
By part (ii) of this proposition, we see that
⟨gcd( f1, h)⟩= ⟨gcd( f1, . . . , fs)⟩.
Then gcd( f1, h) = gcd( f1, . . . , fs) follows from the uniqueness part of Corollary 4,
which proves what we want.
Finally, we need to show that there is an algorithm for ﬁnding gcd( f1, . . . , fs).
The basic idea is to combine part (iii) with the Euclidean Algorithm. For example,
suppose that we wanted to compute the gcd of four polynomials f1, f2, f3, f4. Using
part (iii) of the proposition twice, we obtain
(6)
gcd( f1, f2, f3, f4) = gcd( f1, gcd( f2, f3, f4))
= gcd( f1, gcd( f2, gcd( f3, f4))).
Then if we use the Euclidean Algorithm three times [once for each gcd in the second
line of (6)], we get the gcd of f1, f2, f3, f4. In the exercises, you will be asked to write
pseudocode for an algorithm that implements this idea for an arbitrary number of
polynomials. Proposition 8 is proved.
□
The gcd command in most computer algebra systems only handles two polyno-
mials at a time. Thus, to work with more than two polynomials, you will need to
use the method described in the proof of Proposition 8. For an example, consider
the ideal

§5 Polynomials of One Variable
45
⟨x3 −3x + 2, x4 −1, x6 −1⟩⊆k[x].
We know that gcd(x3 −3x + 2, x4 −1, x6 −1) is a generator. Furthermore, you can
check that
gcd(x3 −3x + 2, x4 −1, x6 −1) = gcd(x3 −3x + 2, gcd(x4 −1, x6 −1))
= gcd(x3 −3x + 2, x2 −1) = x −1.
It follows that
⟨x3 −3x + 2, x4 −1, x6 −1⟩= ⟨x −1⟩.
More generally, given f1, . . . , fs ∈k[x], it is clear that we now have an algorithm for
ﬁnding a generator of ⟨f1, . . . , fs⟩.
For another application of the algorithms developed here, consider the ideal
membership problem from §4: Given f1, . . . , fs ∈k[x], is there an algorithm for
deciding whether a given polynomial f ∈k[x] lies in the ideal ⟨f1, . . . , fs⟩? The an-
swer is yes, and the algorithm is easy to describe. The ﬁrst step is to use gcd's to ﬁnd
a generator h of ⟨f1, . . . , fs⟩. Then, since f ∈⟨f1, . . . , fs⟩is equivalent to f ∈⟨h⟩, we
need only use the division algorithm to write f = qh + r, where deg(r) < deg(h). It
follows that f is in the ideal if and only if r = 0. For example, suppose we wanted
to know whether
x3 + 4x2 + 3x −7 ∈⟨x3 −3x + 2, x4 −1, x6 −1⟩.
We saw above that x −1 is a generator of this ideal so that our question can be
rephrased as the question whether
x3 + 4x2 + 3x −7 ∈⟨x −1⟩.
Dividing, we ﬁnd that
x3 + 4x2 + 3x −7 = (x2 + 5x + 8)(x −1) + 1.
Hence x3 +4x2 +3x−7 is not in the ideal ⟨x3 −3x+2, x4 −1, x6 −1⟩. In Chapter 2,
we will solve the ideal membership problem for polynomials in k[x1, . . . , xn] using
a similar strategy. We will ﬁrst ﬁnd a nice basis of the ideal (called a Gröbner basis)
and then we will use a generalized division algorithm to determine whether or not a
polynomial is in the ideal.
In the exercises, we will see that in the one-variable case, other problems posed
in earlier sections can be solved algorithmically using the methods discussed here.
EXERCISES FOR §5
1. Over the complex numbers C, Corollary 3 can be stated in a stronger form. Namely,
prove that if f ∈C[x] is a polynomial of degree n > 0, then f can be written in the form
f = c(x −a1) · · · (x −an), where c, a1, . . . , an ∈C and c ̸= 0. Hint: Use Theorem 7 of
§1. Note that this result holds for any algebraically closed ﬁeld.

46
Chapter 1 Geometry, Algebra, and Algorithms
2. Although Corollary 3 is simple to prove, it has some nice consequences. For example,
consider the n × n Vandermonde determinant determined by a1, . . . , an in a ﬁeld k:
det
⎛
⎜
⎜
⎜
⎝
1
a1
a2
1
· · ·
an−1
1
1
a2
a2
2
· · ·
an−1
2
...
...
...
...
1
an
a2
n
· · ·
an−1
n
⎞
⎟
⎟
⎟
⎠.
Prove that this determinant is nonzero when the ai's are distinct. Hint: If the determi-
nant is zero, then the columns are linearly dependent. Show that the coefﬁcients of the
linear relation determine a polynomial of degree ≤n −1 which has n roots. Then use
Corollary 3.
3. The fact that every ideal of k[x] is principal (generated by one element) is special to the
case of polynomials in one variable. In this exercise we will see why. Namely, consider
the ideal I = ⟨x, y⟩⊆k[x, y]. Prove that I is not a principal ideal. Hint: If x = fg, where
f , g ∈k[x, y], then prove that f or g is a constant. It follows that the treatment of gcd's
given in this section applies only to polynomials in one variable. One can compute gcd's
for polynomials of ≥2 variables, but the theory involved is more complicated [see VON
ZUR GATHEN and GERHARD (2013), Chapter 6].
4. If h is the gcd of f , g ∈k[x], then prove that there are A, B ∈k[x] such that Af + Bg = h.
5. If f , g ∈k[x], then prove that ⟨f −qg, g⟩= ⟨f , g⟩for any q in k[x]. This will prove
equation (4) in the text.
6. Given f1, . . . , fs ∈k[x], let h = gcd( f2, . . . , fs). Then use the equality ⟨h⟩= ⟨f2, . . . , fs⟩
to show that ⟨f1, h⟩= ⟨f1, f2, . . . , fs⟩. This equality is used in the proof of part (iii) of
Proposition 8.
7. If you are allowed to compute the gcd of only two polynomials at a time (which is true
for some computer algebra systems), give pseudocode for an algorithm that computes
the gcd of polynomials f1, . . . , fs ∈k[x], where s > 2. Prove that your algorithm works.
Hint: See (6). This will complete the proof of part (iv) of Proposition 8.
8. Use a computer algebra system to compute the following gcd's:
a. gcd(x4 + x2 + 1, x4 −x2 −2x −1, x3 −1).
b. gcd(x3 + 2x2 −x −2, x3 −2x2 −x + 2, x3 −x2 −4x + 4).
9. Use the method described in the text to decide whether x2 −4 is an element of the ideal
⟨x3 + x2 −4x −4, x3 −x2 −4x + 4, x3 −2x2 −x + 2⟩.
10. Give pseudocode for an algorithm that has input f , g ∈k[x] and output h, A, B ∈k[x]
where h = gcd( f , g) and Af + Bg = h. Hint: The idea is to add variables A, B, C, D to
the algorithm so that Af + Bg = h and Cf + Dg = s remain true at every step of the
algorithm. Note that the initial values of A, B, C, D are 1, 0, 0, 1, respectively. You may
ﬁnd it useful to let quotient( f , g) denote the quotient of f on division by g, i.e., if the
division algorithm yields f = qg + r, then q = quotient( f , g).
11. In this exercise we will study the one-variable case of the consistency problem from §2.
Given f1, . . . , fs ∈k[x], this asks if there is an algorithm to decide whether V( f1, . . . , fs)
is nonempty. We will see that the answer is yes when k = C.
a. Let f ∈C[x] be a nonzero polynomial. Then use Theorem 7 of §1 to show that
V( f ) = ∅if and only if f is constant.
b. If f1, . . . , fs ∈C[x], prove V( f1, . . . , fs) = ∅if and only if gcd( f1, . . . , fs) = 1.
c. Describe (in words, not pseudocode) an algorithm for determining whether or not
V( f1, . . . , fs) is nonempty.
When k = R, the consistency problem is much more difﬁcult. It requires giving an
algorithm that tells whether a polynomial f ∈R[x] has a real root.
12. This exercise will study the one-variable case of the Nullstellensatz problem from §4,
which asks for the relation between I(V( f1, . . . , fs)) and ⟨f1, . . . , fs⟩when f1, . . . ,

§5 Polynomials of One Variable
47
fs ∈C[x]. By using gcd's, we can reduce to the case of a single generator. So, in this
problem, we will explicitly determine I(V( f )) when f ∈C[x] is a nonconstant polyno-
mial. Since we are working over the complex numbers, we know by Exercise 1 that f
factors completely, i.e.,
f = c(x −a1)r1 · · · (x −al)rl,
where a1, . . . , al ∈C are distinct and c ∈C \ {0}. Deﬁne the polynomial
fred = c(x −a1) · · · (x −al).
The polynomials f and fred have the same roots, but their multiplicities may differ. In
particular, all roots of fred have multiplicity one. We call fred the reduced or square-free
part of f . The latter name recognizes that fred is the square-free factor of f of largest
degree.
a. Show that V( f ) = {a1, . . . , al}.
b. Show that I(V( f )) = ⟨fred⟩.
Whereas part (b) describes I(V( f )), the answer is not completely satisfactory because
we need to factor f completely to ﬁnd fred. In Exercises 13, 14, and 15 we will show how
to determine fred without any factoring.
13. We will study the formal derivative of f = c0xn + c1xn−1 + · · · + cn−1x + cn ∈C[x]. The
formal derivative is deﬁned by the usual formulas from calculus:
f ′ = nc0xn−1 + (n −1)c1xn−2 + · · · + cn−1 + 0.
Prove that the following rules of differentiation apply:
(af )′ = af ′
when a ∈C,
( f + g)′ = f ′ + g′,
( fg)′ = f ′g + fg′.
14. In this exercise we will use the differentiation properties of Exercise 13 to compute
gcd( f , f ′) when f ∈C[x].
a. Suppose f = (x −a)r h in C[x], where h(a) ̸= 0. Then prove that f ′ = (x −a)r−1 h1,
where h1 ∈C[x] does not vanish at a. Hint: Use the product rule.
b. Let f = c(x−a1)r1 · · · (x−al)rl be the factorization of f , where a1, . . . , al are distinct.
Prove that f ′ is a product f ′ = (x −a1)r1−1 · · · (x −al)rl−1 H, where H ∈C[x] is a
polynomial vanishing at none of a1, . . . , al.
c. Prove that gcd( f , f ′) = (x −a1)r1−1 · · · (x −al)rl−1.
15. Consider the square-free part fred of a polynomial f ∈C[x] deﬁned in Exercise 12.
a. Use Exercise 14 to prove that fred is given by the formula
fred =
f
gcd( f , f ′).
The virtue of this formula is that it allows us to ﬁnd the square-free part without
factoring f . This allows for much quicker computations.
b. Use a computer algebra system to ﬁnd the square-free part of the polynomial
x11 −x10 + 2x8 −4x7 + 3x5 −3x4 + x3 + 3x2 −x −1.
16. Use Exercises 12 and 15 to describe (in words, not pseudocode) an algorithm whose
input consists of polynomials f1, . . . , fs ∈C[x] and whose output consists of a basis of
I(V( f1, . . . , fs)). It is more difﬁcult to construct such an algorithm when dealing with
polynomials of more than one variable.
17. Find a basis for the ideal I(V(x5 −2x4 + 2x2 −x, x5 −x4 −2x3 + 2x2 + x −1)).

Chapter 2
Gröbner Bases
§1
Introduction
In Chapter 1, we have seen how the algebra of the polynomial rings k[x1, . . . , xn] and
the geometry of afﬁne algebraic varieties are linked. In this chapter, we will study the
method of Gröbner bases, which will allow us to solve problems about polynomial
ideals in an algorithmic or computational fashion. The method of Gröbner bases is
also used in several powerful computer algebra systems to study speciﬁc polynomial
ideals that arise in applications. In Chapter 1, we posed many problems concerning
the algebra of polynomial ideals and the geometry of afﬁne varieties. In this chapter
and the next, we will focus on four of these problems.
Problems
a. The IDEAL DESCRIPTION PROBLEM: Does every ideal I ⊆k[x1, . . . , xn] have a
ﬁnite basis? In other words, can we write I = ⟨f1, . . . , fs⟩for fi ∈k[x1, . . . , xn]?
b. The IDEAL MEMBERSHIP PROBLEM: Given f ∈k[x1, . . . , xn] and an ideal
I = ⟨f1, . . . , fs⟩, determine if f ∈I. Geometrically, this is closely related to
the problem of determining whether V( f1, . . . , fs) lies on the variety V( f).
c. The PROBLEM OF SOLVING POLYNOMIAL EQUATIONS: Find all common solu-
tions in kn of a system of polynomial equations
f1(x1, . . . , xn) = · · · = fs(x1, . . . , xn) = 0.
This is the same as asking for the points in the afﬁne variety V( f1, . . . , fs).
d. The IMPLICITIZATION PROBLEM: Let V ⊆kn be given parametrically as
x1 = g1(t1, . . . , tm),
...
xn = gn(t1, . . . , tm).
© Springer International Publishing Switzerland 2015
D.A. Cox et al., Ideals, Varieties, and Algorithms, Undergraduate Texts
in Mathematics, DOI 10.1007/978-3-319-16721-3_2
49

50
Chapter 2 Gröbner Bases
If the gi are polynomials (or rational functions) in the variables tj, then V will be
an afﬁne variety or part of one. Find a system of polynomial equations (in the xi)
that deﬁnes the variety.
Some comments are in order. Problem (a) asks whether every polynomial ideal
has a ﬁnite description via generators. Many of the ideals we have seen so far do
have such descriptions—indeed, the way we have speciﬁed most of the ideals we
have studied has been to give a ﬁnite generating set. However, there are other ways
of constructing ideals that do not lead directly to this sort of description. The main
example we have seen is the ideal of a variety, I(V). It will be useful to know that
these ideals also have ﬁnite descriptions. On the other hand, in the exercises, we will
see that if we allow inﬁnitely many variables to appear in our polynomials, then the
answer to Problem (a) is no.
Note that Problems (c) and (d) are, so to speak, inverse problems. In Problem (c),
we ask for the set of solutions of a given system of polynomial equations. In Prob-
lem (d), on the other hand, we are given the solutions, and the problem is to ﬁnd a
system of equations with those solutions.
To begin our study of Gröbner bases, let us consider some special cases in which
you have seen algorithmic techniques to solve the problems given above.
Example 1. When n = 1, we solved the ideal description problem in §5 of
Chapter 1. Namely, given an ideal I ⊆k[x], we showed that I = ⟨g⟩for some
g ∈k[x] (see Corollary 4 of Chapter 1, §5). So ideals have an especially simple
description in this case.
We also saw in §5 of Chapter 1 that the solution of the ideal membership problem
follows easily from the division algorithm: given f ∈k[x], to check whether f ∈I =
⟨g⟩, we divide g into f:
f = q · g + r,
where q, r ∈k[x] and r = 0 or deg(r) < deg(g). Then we proved that f ∈I if and
only if r = 0. Thus, we have an algorithmic test for ideal membership in the case
n = 1.
Example 2. Next, let n (the number of variables) be arbitrary, and consider the prob-
lem of solving a system of polynomial equations:
(1)
a11x1 + · · · + a1nxn + b1 = 0,
...
am1x1 + · · · + amnxn + bm = 0,
where each polynomial is linear (total degree 1).
For example, consider the system
2x1 + 3x2 −x3 = 0,
x1 + x2 −1 = 0,
(2)
x1 + x3 −3 = 0.

§1 Introduction
51
We row-reduce the matrix of the system to reduced row echelon form:
⎛
⎝
1
0
1
3
0
1 −1 −2
0
0
0
0
⎞
⎠.
The form of this matrix shows that x3 is a free variable, and setting x3 = t
(any element of k), we have
x1 = −t + 3,
x2 = t −2,
x3 = t.
These are parametric equations for a line L in k3. The original system of equa-
tions (2) presents L as an afﬁne variety.
In the general case, one performs row operations on the matrix of (1)
⎛
⎜
⎝
a11 · · · a1n −b1
...
...
...
am1 · · · amn −bm
⎞
⎟
⎠
until it is in reduced row echelon form (where the ﬁrst nonzero entry on each row
is 1, and all other entries in the column containing a leading 1 are zero). Then we
can ﬁnd all solutions of the original system (1) by substituting values for the free
variables in the reduced row echelon form system. In some examples there may
be only one solution, or no solutions. This last case will occur, for instance, if the
reduced row echelon matrix contains a row (0 . . . 0 1), corresponding to the incon-
sistent equation 0 = 1.
Example 3. Again, take n arbitrary, and consider the subset V of kn parametrized by
(3)
x1 = a11t1 + · · · + a1mtm + b1,
...
xn = an1t1 + · · · + anmtm + bn.
We see that V is an afﬁne linear subspace of kn since V is the image of the
mapping F : km →kn deﬁned by
F(t1, . . . , tm) = (a11t1 + · · · + a1mtm + b1, . . . , an1t1 + · · · + anmtm + bn).
This is a linear mapping, followed by a translation. Let us consider the impliciti-
zation problem in this case. In other words, we seek a system of linear equations
[as in (1)] whose solutions are the points of V.
For example, consider the afﬁne linear subspace V ⊆k4 deﬁned by

52
Chapter 2 Gröbner Bases
x1 = t1 + t2 + 1,
x2 = t1 −t2 + 3,
x3 = 2t1 −2,
x4 = t1 + 2t2 −3.
We rewrite the equations by subtracting the xi terms and constants from both sides
and apply the row reduction algorithm to the corresponding matrix:
⎛
⎜
⎜
⎝
1
1 −1
0
0
0 −1
1 −1
0 −1
0
0 −3
2
0
0
0 −1
0
2
1
2
0
0
0 −1
3
⎞
⎟
⎟
⎠
(where the coefﬁcients of the xi have been placed after the coefﬁcients of the tj in
each row). We obtain the reduced row echelon form:
⎛
⎜
⎜
⎝
1
0
0
0 −1/2
0
1
0
1
0
0
1/4 −1/2
1
0
0
1
0 −1/4 −1/2
3
0
0
0
1 −3/4
1/2
3
⎞
⎟
⎟
⎠.
Because the entries in the ﬁrst two columns of rows 3 and 4 are zero, the last two
rows of this matrix correspond to the following two equations with no tj terms:
x1 −(1/4)x3 −(1/2)x4 −3 = 0,
x2 −(3/4)x3 + (1/2)x4 −3 = 0.
(Note that this system is also in echelon form.) These two equations deﬁne V in k4.
The same method can be applied to ﬁnd implicit equations for any afﬁne linear
subspace V given parametrically as in (3): one computes the reduced row echelon
form of (3), and the rows involving only x1, . . . , xn give the equations for V. We thus
have an algorithmic solution to the implicitization problem in this case.
Our goal in this chapter will be to develop extensions of the methods used in
these examples to systems of polynomial equations of any degrees in any number
of variables. What we will see is that a sort of "combination" of row-reduction and
division of polynomials—the method of Gröbner bases mentioned at the outset—
allows us to handle all these problems.
EXERCISES FOR §1
1. Determine whether the given polynomial is in the given ideal I ⊆R[x] using the method
of Example 1.
a. f (x) = x2 −3x + 2, I = ⟨x −2⟩.
b. f (x) = x5 −4x + 1, I = ⟨x3 −x2 + x⟩.
c. f (x) = x2 −4x + 4, I = ⟨x4 −6x2 + 12x −8, 2x3 −10x2 + 16x −8⟩.
d. f (x) = x3 −1, I = ⟨x9 −1, x5 + x3 −x2 −1⟩.

§1 Introduction
53
2. Find parametrizations of the afﬁne varieties deﬁned by the following sets of equations.
a. In R3 or C3:
2x + 3y −z = 9,
x −y = 1,
3x + 7y −2z = 17.
b. In R4 or C4:
x1 + x2 −x3 −x4 = 0,
x1 −x2 + x3 = 0.
c. In R3 or C3:
y −x3 = 0,
z −x5 = 0.
3. Find implicit equations for the afﬁne varieties parametrized as follows.
a. In R3 or C3:
x1 = t −5,
x2 = 2t + 1,
x3 = −t + 6.
b. In R4 or C4:
x1 = 2t −5u,
x2 = t + 2u,
x3 = −t + u,
x4 = t + 3u.
c. In R3 or C3:
x = t,
y = t4,
z = t7.
4. Let x1, x2, x3, . . . be an inﬁnite collection of independent variables indexed by the natural
numbers. A polynomial with coefﬁcients in a ﬁeld k in the xi is a ﬁnite linear combination
of (ﬁnite) monomials xe1
i1 . . . xen
in . Let R denote the set of all polynomials in the xi. Note that
we can add and multiply elements of R in the usual way. Thus, R is the polynomial ring
k[x1, x2, . . .] in inﬁnitely many variables.
a. Let I = ⟨x1, x2, x3, . . .⟩be the set of polynomials of the form xt1 f1 +· · ·+xtm fm, where
fj ∈R. Show that I is an ideal in the ring R.
b. Show, arguing by contradiction, that I has no ﬁnite generating set. Hint: It is not enough
only to consider subsets of {xi | i ≥1}.
5. In this problem you will show that all polynomial parametric curves in k2 are contained in
afﬁne algebraic varieties.
a. Show that the number of distinct monomials xayb of total degree ≤m in k[x, y] is equal
to (m + 1)(m + 2)/2. [Note: This is the binomial coefﬁcient
m+2
2

.]
b. Show that if f (t) and g(t) are polynomials of degree ≤n in t, then for m large enough,
the "monomials"
[ f (t)]a[g(t)]b
with a + b ≤m are linearly dependent.

54
Chapter 2 Gröbner Bases
c. Deduce from part (b) that if C is a curve in k2 given parametrically by x = f (t), y =
g(t) for f (t), g(t) ∈k[t], then C is contained in V(F) for some nonzero F ∈k[x, y].
d. Generalize parts (a), (b), and (c) to show that any polynomial parametric surface
x = f (t, u),
y = g(t, u),
z = h(t, u)
is contained in an algebraic surface V(F), where F ∈k[x, y, z] is nonzero.
§2
Orderings on the Monomials in k[x1, . . . , xn]
If we examine the division algorithm in k[x] and the row-reduction (Gaussian elimi-
nation) algorithm for systems of linear equations (or matrices) in detail, we see that a
notion of ordering of terms in polynomials is a key ingredient of both (though this is
not often stressed). For example, in dividing f(x) = x5−3x2+1 by g(x) = x2−4x+7
by the standard method, we would:
• Write the terms in the polynomials in decreasing order by degree in x.
• At the ﬁrst step, the leading term (the term of highest degree) in f is x5 = x3 · x2 =
x3 · (leading term in g). Thus, we would subtract x3 · g(x) from f to cancel the
leading term, leaving 4x4 −7x3 −3x2 + 1.
• Then, we would repeat the same process on f(x) −x3 · g(x), etc., until we obtain
a polynomial of degree less than 2.
For the division algorithm on polynomials in one variable, we are dealing with the
degree ordering on the one-variable monomials:
(1)
· · · > xm+1 > xm > · · · > x2 > x > 1.
The success of the algorithm depends on working systematically with the leading
terms in f and g, and not removing terms "at random" from f using arbitrary terms
from g.
Similarly, in the row-reduction algorithm on matrices, in any given row, we sys-
tematically work with entries to the left ﬁrst—leading entries are those nonzero en-
tries farthest to the left on the row. On the level of linear equations, this is expressed
by ordering the variables x1, . . . , xn as follows:
(2)
x1 > x2 > · · · > xn.
We write the terms in our equations in decreasing order. Furthermore, in an echelon
form system, the equations are listed with their leading terms in decreasing order.
(In fact, the precise deﬁnition of an echelon form system could be given in terms of
this ordering—see Exercise 8.)
From the above evidence, we might guess that a major component of any exten-
sion of division and row-reduction to arbitrary polynomials in several variables will
be an ordering on the terms in polynomials in k[x1, . . . , xn]. In this section, we will
discuss the desirable properties such an ordering should have, and we will construct

§2 Orderings on the Monomials in k[x1, . . . , xn]
55
several different examples that satisfy our requirements. Each of these orderings
will be useful in different contexts.
First, we note that we can reconstruct the monomial xα = xα1
1 · · · xαn
n
from
the n-tuple of exponents α = (α1, . . . , αn) ∈Zn
≥0. This observation establishes a
one-to-one correspondence between the monomials in k[x1, . . . , xn] and Zn
≥0. Fur-
thermore, any ordering > we establish on the space Zn
≥0 will give us an ordering on
monomials: if α > β according to this ordering, we will also say that xα > xβ.
There are many different ways to deﬁne orderings on Zn
≥0. For our purposes,
most of these orderings will not be useful, however, since we will want our orderings
to be compatible with the algebraic structure of polynomial rings.
To begin, since a polynomial is a sum of monomials, we would like to be able
to arrange the terms in a polynomial unambiguously in descending (or ascending)
order. To do this, we must be able to compare every pair of monomials to establish
their proper relative positions. Thus, we will require that our orderings be linear or
total orderings. This means that for every pair of monomials xα and x β, exactly one
of the three statements
xα > x β,
xα = x β,
x β > xα
should be true. A total order is also required to be transitive, so that xα > x β and
x β > xγ always imply xα > xγ.
Next, we must take into account the effect of the sum and product operations
on polynomials. When we add polynomials, after combining like terms, we may
simply rearrange the terms present into the appropriate order, so sums present no
difﬁculties. Products are more subtle, however. Since multiplication in a polynomial
ring distributes over addition, it sufﬁces to consider what happens when we multiply
a monomial times a polynomial. If doing this changed the relative ordering of terms,
signiﬁcant problems could result in any process similar to the division algorithm in
k[x], in which we must identify the leading terms in polynomials. The reason is that
the leading term in the product could be different from the product of the monomial
and the leading term of the original polynomial.
Hence, we will require that all monomial orderings have the following additional
property. If xα > xβ and xγ is any monomial, then we require that xαxγ > xβxγ. In
terms of the exponent vectors, this property means that if α > β in our ordering on
Zn
≥0, then, for all γ ∈Zn
≥0, α + γ > β + γ.
With these considerations in mind, we make the following deﬁnition.
Deﬁnition 1. A monomial ordering > on k[x1, . . . , xn] is a relation > on Zn
≥0, or
equivalently, a relation on the set of monomials xα, α ∈Zn
≥0, satisfying:
(i) > is a total (or linear) ordering on Zn
≥0.
(ii) If α > β and γ ∈Zn
≥0, then α + γ > β + γ.
(iii) > is a well-ordering on Zn
≥0. This means that every nonempty subset of Zn
≥0
has a smallest element under >. In other words, if A ⊆Zn
≥0 is nonempty, then
there is α ∈A such that β > α for every β ̸= α in A.
Given a monomial ordering >, we say that α ≥β when either α > β or α = β.

56
Chapter 2 Gröbner Bases
The following lemma will help us understand what the well-ordering condition
of part (iii) of the deﬁnition means.
Lemma 2. An order relation > on Zn
≥0 is a well-ordering if and only if every strictly
decreasing sequence in Zn
≥0
α(1) > α(2) > α(3) > · · ·
eventually terminates.
Proof. We will prove this in contrapositive form: > is not a well-ordering if and
only if there is an inﬁnite strictly decreasing sequence in Zn
≥0.
If > is not a well-ordering, then some nonempty subset S ⊆Zn
≥0 has no least
element. Now pick α(1) ∈S. Since α(1) is not the least element, we can ﬁnd
α(1) > α(2) in S. Then α(2) is also not the least element, so that there is α(2) >
α(3) in S. Continuing this way, we get an inﬁnite strictly decreasing sequence
α(1) > α(2) > α(3) > · · · .
Conversely, given such an inﬁnite sequence, then {α(1), α(2), α(3), . . .} is a non-
empty subset of Zn
≥0 with no least element, and thus, > is not a well-ordering.
□
The importance of this lemma will become evident in what follows. It will be
used to show that various algorithms must terminate because some term strictly
decreases (with respect to a ﬁxed monomial order) at each step of the algorithm.
In §4, we will see that given parts (i) and (ii) in Deﬁnition 1, the well-ordering
condition of part (iii) is equivalent to α ≥0 for all α ∈Zn
≥0.
For a simple example of a monomial order, note that the usual numerical order
· · · > m + 1 > m > · · · > 3 > 2 > 1 > 0
on the elements of Z≥0 satisﬁes the three conditions of Deﬁnition 1. Hence, the
degree ordering (1) on the monomials in k[x] is a monomial ordering, unique by
Exercise 13.
Our ﬁrst example of an ordering on n-tuples will be lexicographic order (or lex
order, for short).
Deﬁnition 3 (Lexicographic Order). Let α = (α1, . . . , αn) and β = (β1, . . . , βn)
be in Zn
≥0. We say α >lex β if the leftmost nonzero entry of the vector difference
α −β ∈Zn is positive. We will write xα >lex xβ if α >lex β.
Here are some examples:
a. (1, 2, 0) >lex (0, 3, 4) since α −β = (1, −1, −4).
b. (3, 2, 4) >lex (3, 2, 1) since α −β = (0, 0, 3).
c. The variables x1, . . . , xn are ordered in the usual way [see (2)] by the lex ordering:
(1, 0, . . . , 0) >lex (0, 1, 0, . . . , 0) >lex · · · >lex (0, . . . , 0, 1).
so x1 >lex x2 >lex · · · >lex xn.

§2 Orderings on the Monomials in k[x1, . . . , xn]
57
In practice, when we work with polynomials in two or three variables, we will
call the variables x, y, z rather than x1, x2, x3. We will also assume that the alphabet-
ical order x > y > z on the variables is used to deﬁne the lexicographic ordering
unless we explicitly say otherwise.
Lex order is analogous to the ordering of words used in dictionaries (hence the
name). We can view the entries of an n-tuple α ∈Zn
≥0 as analogues of the letters in
a word. The letters are ordered alphabetically:
a > b > · · · > y > z.
Then, for instance,
arrow >lex arson
since the third letter of "arson" comes after the third letter of "arrow" in alphabetical
order, whereas the ﬁrst two letters are the same in both. Since all elements α ∈Zn
≥0
have length n, this analogy only applies to words with a ﬁxed number of letters.
For completeness, we must check that the lexicographic order satisﬁes the three
conditions of Deﬁnition 1.
Proposition 4. The lex ordering on Zn
≥0 is a monomial ordering.
Proof. (i) That >lex is a total ordering follows directly from the deﬁnition and the
fact that the usual numerical order on Z≥0 is a total ordering.
(ii) If α >lex β, then we have that the leftmost nonzero entry in α−β, say αi−βi,
is positive. But xα · xγ = xα+γ and xβ · xγ = xβ+γ. Then in (α + γ) −(β + γ) =
α −β, the leftmost nonzero entry is again αi −βi > 0.
(iii) Suppose that >lex were not a well-ordering. Then by Lemma 2, there would
be an inﬁnite strictly descending sequence
α(1) >lex α(2) >lex α(3) >lex · · ·
of elements of Zn
≥0. We will show that this leads to a contradiction.
Consider the ﬁrst entries of the vectors α(i) ∈Zn
≥0. By the deﬁnition of the
lex order, these ﬁrst entries form a nonincreasing sequence of nonnegative integers.
Since Z≥0 is well-ordered, the ﬁrst entries of the α(i) must "stabilize" eventually.
In other words, there exists an ℓsuch that all the ﬁrst entries of the α(i) with i ≥ℓ
are equal.
Beginning at α(ℓ), the second and subsequent entries come into play in deter-
mining the lex order. The second entries of α(ℓ), α(ℓ+ 1), . . . form a nonincreasing
sequence. By the same reasoning as before, the second entries "stabilize" eventually
as well. Continuing in the same way, we see that for some m, the α(m), α(m+1), . . .
all are equal. This contradicts the fact that α(m) >lex α(m + 1).
□
It is important to realize that there are many lex orders, corresponding to how the
variables are ordered. So far, we have used lex order with x1 > x2 > · · · > xn. But
given any ordering of the variables x1, . . . , xn, there is a corresponding lex order.
For example, if the variables are x and y, then we get one lex order with x > y and

58
Chapter 2 Gröbner Bases
a second with y > x. In the general case of n variables, there are n! lex orders. In
what follows, the phrase "lex order" will refer to the one with x1 > · · · > xn unless
otherwise stated.
In lex order, notice that a variable dominates any monomial involving only
smaller variables, regardless of its total degree. Thus, for the lex order with x >
y > z, we have x >lex y5z3. For some purposes, we may also want to take the total
degrees of the monomials into account and order monomials of bigger degree ﬁrst.
One way to do this is the graded lexicographic order (or grlex order).
Deﬁnition 5 (Graded Lex Order). Let α, β ∈Zn
≥0. We say α >grlex β if
|α| =
n

i=1
αi > |β| =
n

i=1
βi,
or
|α| = |β| and α >lex β.
We see that grlex orders by total degree ﬁrst, then "break ties" using lex order.
Here are some examples:
a. (1, 2, 3) >grlex (3, 2, 0) since |(1, 2, 3)| = 6 > |(3, 2, 0)| = 5.
b. (1, 2, 4) >grlex (1, 1, 5) since |(1, 2, 4)| = |(1, 1, 5)| and (1, 2, 4) >lex (1, 1, 5).
c. The variables are ordered according to the lex order, i.e., x1 >grlex · · · >grlex xn.
We will leave it as an exercise to show that the grlex ordering satisﬁes the three
conditions of Deﬁnition 1. As in the case of lex order, there are n! grlex orders
on n variables, depending on how the variables are ordered.
Another (somewhat less intuitive) order on monomials is the graded reverse lexi-
cographical order (or grevlex order). Even though this ordering "takes some getting
used to," it has been shown that for some operations, the grevlex ordering is the most
efﬁcient for computations.
Deﬁnition 6 (Graded Reverse Lex Order). Let α, β ∈Zn
≥0. We say α >grevlex β if
|α| =
n

i=1
αi > |β| =
n

i=1
βi, or |α| = |β| and the rightmost nonzero entry
of α −β ∈Zn is negative.
Like grlex, grevlex orders by total degree, but it "breaks ties" in a different way.
For example:
a. (4, 7, 1) >grevlex (4, 2, 3) since |(4, 7, 1)| = 12 > |(4, 2, 3)| = 9.
b. (1, 5, 2) >grevlex (4, 1, 3) since |(1, 5, 2)| = |(4, 1, 3)| and (1, 5, 2) −(4, 1, 3) =
(−3, 4, −1).
You will show in the exercises that the grevlex ordering gives a monomial ordering.
Note also that lex and grevlex give the same ordering on the variables. That is,
(1, 0, . . . , 0) >grevlex (0, 1, . . . , 0) >grevlex · · · >grevlex (0, . . . , 0, 1)
or
x1 >grevlex x2 >grevlex · · · >grevlex xn

§2 Orderings on the Monomials in k[x1, . . . , xn]
59
Thus, grevlex is really different from the grlex order with the variables rearranged
(as one might be tempted to believe from the name).
To explain the relation between grlex and grevlex, note that both use total degree
in the same way. To break a tie, grlex uses lex order, so that it looks at the leftmost
(or largest) variable and favors the larger power. In contrast, when grevlex ﬁnds
the same total degree, it looks at the rightmost (or smallest) variable and favors
the smaller power. In the exercises, you will check that this amounts to a "double-
reversal" of lex order. For example,
x5yz >grlex x4yz2,
since both monomials have total degree 7 and x5yz >lex x4yz2. In this case, we also
have
x5yz >grevlex x4yz2,
but for a different reason: x5yz is larger because the smaller variable z appears to a
smaller power.
As with lex and grlex, there are n! grevlex orderings corresponding to how the n
variables are ordered.
There are many other monomial orders besides the ones considered here. Some
of these will be explored in the exercises for §4. Most computer algebra systems
implement lex order, and most also allow other orders, such as grlex and grevlex.
Once such an order is chosen, these systems allow the user to specify any of the n!
orderings of the variables. As we will see in §8 of this chapter and in later chapters,
this facility becomes very useful when studying a variety of questions.
We will end this section with a discussion of how a monomial ordering can be
applied to polynomials. If f = 
α aαxα is a nonzero polynomial in k[x1, . . . , xn]
and we have selected a monomial ordering >, then we can order the monomials of
f in an unambiguous way with respect to >. For example, consider the polynomial
f = 4xy2z + 4z2 −5x3 + 7x2z2 ∈k[x, y, z]. Then:
• With respect to lex order, we would reorder the terms of f in decreasing order as
f = −5x3 + 7x2z2 + 4xy2z + 4z2.
• With respect to grlex order, we would have
f = 7x2z2 + 4xy2z −5x3 + 4z2.
• With respect to grevlex order, we would have
f = 4xy2z + 7x2z2 −5x3 + 4z2.
We will use the following terminology.
Deﬁnition 7. Let f = 
α aαxα be a nonzero polynomial in k[x1, . . . , xn] and let >
be a monomial order.

60
Chapter 2 Gröbner Bases
(i) The multidegree of f is
multideg( f) = max(α ∈Zn
≥0 | aα ̸= 0)
(the maximum is taken with respect to >).
(ii) The leading coefﬁcient of f is
LC( f) = amultideg( f) ∈k.
(iii) The leading monomial of f is
LM( f) = xmultideg( f)
(with coefﬁcient 1).
(iv) The leading term of f is
LT( f) = LC( f) · LM( f).
To illustrate, let f = 4xy2z + 4z2 −5x3 + 7x2z2 as before and let > denote lex
order. Then
multideg( f) = (3, 0, 0),
LC( f) = −5,
LM( f) = x3,
LT( f) = −5x3.
In the exercises, you will show that the multidegree has the following useful
properties.
Lemma 8. Let f, g ∈k[x1, . . . , xn] be nonzero polynomials. Then:
(i) multideg( fg) = multideg( f) + multideg(g).
(ii) If f + g ̸= 0, then multideg( f + g) ≤max(multideg( f), multideg(g)). If, in
addition, multideg( f) ̸= multideg(g), then equality occurs.
Some books use different terminology. In EISENBUD (1999), the leading term
LT( f) becomes the initial term in>( f). A more substantial difference appears in
BECKER and WEISPFENNING (1993), where the meanings of "monomial" and
"term" are interchanged. For them, the leading term LT( f) is the head monomial
HM( f), while the leading monomial LM( f) is the head term HT( f). Page 10 of
KREUZER and ROBBIANO (2000) has a summary of the terminology used in differ-
ent books. Our advice when reading other texts is to check the deﬁnitions carefully.
EXERCISES FOR §2
1. Rewrite each of the following polynomials, ordering the terms using the lex order, the
grlex order, and the grevlex order, giving LM( f ), LT( f ), and multideg( f ) in each case.

§3 A Division Algorithm in k[x1, . . . , xn]
61
a. f (x, y, z) = 2x + 3y + z + x2 −z2 + x3.
b. f (x, y, z) = 2x2y8 −3x5yz4 + xyz3 −xy4.
2. Each of the following polynomials is written with its monomials ordered according to
(exactly) one of lex, grlex, or grevlex order. Determine which monomial order was used
in each case.
a. f (x, y, z) = 7x2y4z −2xy6 + x2y2.
b. f (x, y, z) = xy3z + xy2z2 + x2z3.
c. f (x, y, z) = x4y5z + 2x3y2z −4xy2z4.
3. Repeat Exercise 1 when the variables are ordered z > y > x.
4. Show that grlex is a monomial order according to Deﬁnition 1.
5. Show that grevlex is a monomial order according to Deﬁnition 1.
6. Another monomial order is the inverse lexicographic or invlex order deﬁned by the
following: for α, β ∈Zn
≥0, α >invlex β if and only if the rightmost nonzero entry of
α −β is positive. Show that invlex is equivalent to the lex order with the variables
permuted in a certain way. (Which permutation?)
7. Let > be any monomial order.
a. Show that α ≥0 for all α ∈Zn
≥0. Hint: Proof by contradiction.
b. Show that if xα divides xβ, then α ≤β. Is the converse true?
c. Show that if α ∈Zn
≥0, then α is the smallest element of α + Zn
≥0 = {α + β | β ∈
Zn
≥0}.
8. Write a precise deﬁnition of what it means for a system of linear equations to be in
echelon form, using the ordering given in equation (2).
9. In this exercise, we will study grevlex in more detail. Let >invlex, be the order given in
Exercise 6, and deﬁne >rinvlex to be the reversal of this ordering, i.e., for α, β ∈Zn
≥0.
α >rinvlex β ⇐⇒β >invlex α.
Notice that rinvlex is a "double reversal" of lex, in the sense that we ﬁrst reverse the
order of the variables and then we reverse the ordering itself.
a. Show that α >grevlex β if and only if |α| > |β|, or |α| = |β| and α >rinvlex β.
b. Is rinvlex a monomial ordering according to Deﬁnition 1? If so, prove it; if not, say
which properties fail.
10. In Z≥0 with the usual ordering, between any two integers, there are only a ﬁnite number
of other integers. Is this necessarily true in Zn
≥0 for a monomial order? Is it true for the
grlex order?
11. Let > be a monomial order on k[x1, . . . , xn].
a. Let f ∈k[x1, . . . , xn] and let m be a monomial. Show that LT(m · f ) = m · LT( f ).
b. Let f , g ∈k[x1, . . . , xn]. Is LT( f · g) necessarily the same as LT( f ) · LT(g)?
c. If fi, gi ∈k[x1, . . . , xn], 1 ≤i ≤s, is LM(s
i=1 figi) necessarily equal to LM( fi) ·
LM(gi) for some i?
12. Lemma 8 gives two properties of the multidegree.
a. Prove Lemma 8. Hint: The arguments used in Exercise 11 may be relevant.
b. Suppose that multideg( f ) = multideg(g) and f + g ̸= 0. Give examples to show that
multideg( f + g) may or may not equal max(multideg( f ), multideg(g)).
13. Prove that 1 < x < x2 < x3 < · · · is the unique monomial order on k[x].
§3
A Division Algorithm in k[x1, . . . , xn]
In §1, we saw how the division algorithm could be used to solve the ideal mem-
bership problem for polynomials of one variable. To study this problem when
there are more variables, we will formulate a division algorithm for polynomials

62
Chapter 2 Gröbner Bases
in k[x1, . . . , xn] that extends the algorithm for k[x]. In the general case, the goal is
to divide f ∈k[x1, . . . , xn] by f1, . . . , fs ∈k[x1, . . . , xn]. As we will see, this means
expressing f in the form
f = q1 f1 + · · · + qs fs + r,
where the "quotients" q1, . . . , qs and remainder r lie in k[x1, . . . , xn]. Some care will
be needed in deciding how to characterize the remainder. This is where we will
use the monomial orderings introduced in §2. We will then see how the division
algorithm applies to the ideal membership problem.
The basic idea of the algorithm is the same as in the one-variable case: we want to
cancel the leading term of f (with respect to a ﬁxed monomial order) by multiplying
some fi by an appropriate monomial and subtracting. Then this monomial becomes
a term in the corresponding qi. Rather than state the algorithm in general, let us ﬁrst
work through some examples to see what is involved.
Example 1. We will ﬁrst divide f = xy2 + 1 by f1 = xy + 1 and f2 = y + 1, using
lex order with x > y. We want to employ the same scheme as for division of one-
variable polynomials, the difference being that there are now several divisors and
quotients. Listing the divisors f1, f2 and the quotients q1, q2 vertically, we have the
following setup:
q1 :
q2 :
xy + 1
y + 1

xy2 + 1
The leading terms LT( f1) = xy and LT( f2) = y both divide the leading term LT( f) =
xy2. Since f1 is listed ﬁrst, we will use it. Thus, we divide xy into xy2, leaving y, and
then subtract y · f1 from f:
q1 :
y
q2 :
xy + 1
y + 1

xy2 + 1
xy2 + y
−y + 1
Now we repeat the same process on −y+1. This time we must use f2 since LT( f1) =
xy does not divide LT(−y + 1) = −y. We obtain:
q1 :
y
q2 : −1
xy + 1
y + 1

xy2 + 1
xy2 + y
−y + 1
−y −1
2

§3 A Division Algorithm in k[x1, . . . , xn]
63
Since LT( f1) and LT( f2) do not divide 2, the remainder is r = 2 and we are done.
Thus, we have written f = xy2 + 1 in the form
xy2 + 1 = y · (xy + 1) + (−1) · (y + 1) + 2.
Example 2. In this example, we will encounter an unexpected subtlety that can
occur when dealing with polynomials of more than one variable. Let us divide
f = x2y + xy2 + y2 by f1 = xy −1 and f2 = y2 −1. As in the previous exam-
ple, we will use lex order with x > y. The ﬁrst two steps of the algorithm go as
usual, giving us the following partially completed division (remember that when
both leading terms divide, we use f1):
q1 :
x + y
q2 :
xy −1
y2 −1

x2y + xy2 + y2
x2y −x
xy2 + x + y2
xy2 −y
x + y2 + y
Note that neither LT( f1) = xy nor LT( f2) = y2 divides LT(x+y2 +y) = x. However,
x + y2 + y is not the remainder since LT( f2) divides y2. Thus, if we move x to the
remainder, we can continue dividing. (This is something that never happens in the
one-variable case: once the leading term of the divisor no longer divides the leading
term of what is at the bottom of the division, the algorithm terminates.)
To implement this idea, we create a remainder column r, to the right of the divi-
sion, where we put the terms belonging to the remainder. Also, we call the polyno-
mial at the bottom of division the intermediate dividend. Then we continue dividing
until the intermediate dividend is zero. Here is the next step, where we move x to
the remainder column (as indicated by the arrow):
q1 :
x + y
q2 :
xy −1
y2 −1

x2y + xy2 + y2
xy2 −x
xy2 + x + y2
x2y −y
x + y2 + y
y2 + y
r
−→
x
Now we continue dividing. If we can divide by LT( f1) or LT( f2), we proceed as
usual, and if neither divides, we move the leading term of the intermediate dividend
to the remainder column.

64
Chapter 2 Gröbner Bases
Here is the rest of the division:
q1 :
x + y
q2 :
1
xy −1
y2 −1

x2y + xy2 + y2
x2y −x
xy2 + x + y2
xy2 −y
x + y2 + y
y2 + y
r
−→
x
y2 −1
y + 1
1
−→
x + y
0
−→
x + y + 1
Thus, the remainder is x + y + 1, and we obtain
(1)
x2y + xy2 + y2 = (x + y) · (xy −1) + 1 · (y2 −1) + x + y + 1.
Note that the remainder is a sum of monomials, none of which is divisible by the
leading terms LT( f1) or LT( f2).
The above example is a fairly complete illustration of how the division algorithm
works. It also shows us what property we want the remainder to have: none of its
terms should be divisible by the leading terms of the polynomials by which we are
dividing. We can now state the general form of the division algorithm.
Theorem 3 (Division Algorithm in k[x1, . . . , xn]). Let > be a monomial order on
Zn
≥0, and let F = ( f1, . . . , fs) be an ordered s-tuple of polynomials in k[x1, . . . , xn].
Then every f ∈k[x1, . . . , xn] can be written as
f = q1 f1 + · · · + qs fs + r,
where qi, r ∈k[x1, . . . , xn], and either r = 0 or r is a linear combination, with coef-
ﬁcients in k, of monomials, none of which is divisible by any of LT( f1), . . . , LT( fs).
We call r a remainder of f on division by F. Furthermore, if qi fi ̸= 0, then
multideg( f) ≥multideg(qi fi).
Proof. We prove the existence of q1, . . . , qs and r by giving an algorithm for their
construction and showing that it operates correctly on any given input. We recom-
mend that the reader review the division algorithm in k[x] given in Proposition 2 of
Chapter 1, §5 before studying the following generalization:

§3 A Division Algorithm in k[x1, . . . , xn]
65
Input : f1, . . . , fs, f
Output : q1, . . . , qs, r
q1 := 0; . . . ; qs := 0; r := 0
p := f
WHILE p ̸= 0 DO
i := 1
divisionoccurred := false
WHILE i ≤s AND divisionoccurred = false DO
IF LT( fi) divides LT(p) THEN
qi := qi + LT(p)/LT( fi)
p := p −(LT(p)/LT( fi))fi
divisionoccurred := true
ELSE
i := i + 1
IF divisionoccurred = false THEN
r := r + LT(p)
p := p −LT(p)
RETURN q1, . . . , qs, r
We can relate this algorithm to the previous example by noting that the variable
p represents the intermediate dividend at each stage, the variable r represents the
column on the right-hand side, and the variables q1, . . . , qs are the quotients listed
above the division. Finally, the boolean variable "divisionoccurred" tells us when
some LT( fi) divides the leading term of the intermediate dividend. You should check
that each time we go through the main WHILE ...DO loop, precisely one of two
things happens:
• (Division Step) If some LT( fi) divides LT(p), then the algorithm proceeds as in
the one-variable case.
• (Remainder Step) If no LT( fi) divides LT(p), then the algorithm adds LT(p) to
the remainder.
These steps correspond exactly to what we did in Example 2.
To prove that the algorithm works, we will ﬁrst show that
(2)
f = q1 f1 + · · · + qs fs + p + r
holds at every stage. This is clearly true for the initial values of q1, . . . , qs, p, and r.
Now suppose that (2) holds at one step of the algorithm. If the next step is a Division
Step, then some LT( fi) divides LT(p), and the equality
qi fi + p = (qi + LT(p)/LT( fi)) fi + (p −(LT(p)/LT( fi)) fi)

66
Chapter 2 Gröbner Bases
shows that qi fi+p is unchanged. Since all other variables are unaffected, (2) remains
true in this case. On the other hand, if the next step is a Remainder Step, then p and
r will be changed, but the sum p + r is unchanged since
p + r = (p −LT(p)) + (r + LT(p)).
As before, equality (2) is still preserved.
Next, notice that the algorithm comes to a halt when p = 0. In this situation, (2)
becomes
f = q1 f1 + · · · + qs fs + r.
Since terms are added to r only when they are divisible by none of the LT( fi), it fol-
lows that q1, . . . , qs and r have the desired properties when the algorithm terminates.
Finally, we need to show that the algorithm does eventually terminate. The key
observation is that each time we redeﬁne the variable p, either its multidegree drops
(relative to our term ordering) or it becomes 0. To see this, ﬁrst suppose that during
a Division Step, p is redeﬁned to be
p′ = p −
LT(p)
LT( fi) fi.
By Lemma 8 of §2, we have
LT
 LT(p)
LT( fi) fi

=
LT(p)
LT( fi)
LT( fi) = LT(p),
so that p and (LT(p)/LT( fi)) fi have the same leading term. Hence, their difference
p′ must have strictly smaller multidegree when p′ ̸= 0. Next, suppose that during a
Remainder Step, p is redeﬁned to be
p′ = p −LT(p).
Here, it is obvious that multideg(p′) < multideg(p) when p′ ̸= 0. Thus, in ei-
ther case, the multidegree must decrease. If the algorithm never terminated, then we
would get an inﬁnite decreasing sequence of multidegrees. The well-ordering prop-
erty of >, as stated in Lemma 2 of §2, shows that this cannot occur. Thus p = 0
must happen eventually, so that the algorithm terminates after ﬁnitely many steps.
It remains to study the relation between multideg( f) and multideg(qi fi). Every
term in qi is of the form LT(p)/LT( fi) for some value of the variable p. The algorithm
starts with p = f, and we just ﬁnished proving that the multidegree of p decreases.
This shows that LT(p) ≤LT( f), and then it follows easily [using condition (ii) of the
deﬁnition of a monomial order] that multideg(qi fi) ≤multideg( f) when qi fi ̸= 0
(see Exercise 4). This completes the proof of the theorem.
□
The algebra behind the division algorithm is very simple (there is nothing beyond
high school algebra in what we did), which makes it surprising that this form of the
algorithm was ﬁrst isolated and exploited only within the past 50 years.

§3 A Division Algorithm in k[x1, . . . , xn]
67
We will conclude this section by asking whether the division algorithm has the
same nice properties as the one-variable version. Unfortunately, the answer is not
pretty—the examples given below will show that the division algorithm is far from
perfect. In fact, the algorithm achieves its full potential only when coupled with the
Gröbner bases studied in §§5 and 6.
A ﬁrst important property of the division algorithm in k[x] is that the remainder is
uniquely determined. To see how this can fail when there is more than one variable,
consider the following example.
Example 4. Let us divide f = x2y + xy2 + y2 by f1 = y2 −1 and f2 = xy - 1. We
will use lex order with x > y. This is the same as Example 2, except that we have
changed the order of the divisors. For practice, we suggest that the reader should do
the division. You should get the following answer:
q1 :
x + 1
q2 :
x
y2 −1
xy −1

x2y + xy2 + y2
x2y −x
xy2 + x + y2
xy2 −x
2x + y2
y2
r
−→
2x
y2 −1
1
0
−→
2x + 1
This shows that
(3)
x2y + xy2 + y2 = (x + 1) · (y2 −1) + x · (xy −1) + 2x + 1.
If you compare this with equation (1), you will see that the remainder is different
from what we got in Example 2.
This shows that the remainder r is not uniquely characterized by the require-
ment that none of its terms be divisible by LT( f1), . . . , LT( fs). The situation is not
completely chaotic: if we follow the algorithm precisely as stated [most importantly,
testing LT(p) for divisibility by LT( f1), LT( f2), . . . in that order], then q1, . . . , qs and
r are uniquely determined. (See Exercise 11 for a more detailed discussion of how
to characterize the output of the algorithm.) However, Examples 2 and 4 show that
the ordering of the s-tuple of polynomials ( f1, . . . , fs) deﬁnitely matters, both in the
number of steps the algorithm will take to complete the calculation and in the re-
sults. The qi and r can change if we simply rearrange the fi. (The qi and r may also
change if we change the monomial ordering, but that is another story.)
One nice feature of the division algorithm in k[x] is the way it solves the ideal
membership problem—recall Example 1 from §1. Do we get something similar

68
Chapter 2 Gröbner Bases
for several variables? One implication is an easy corollary of Theorem 3: if after
division of f by F = ( f1, . . . , fs) we obtain a remainder r = 0, then
f = q1 f1 + · · · + qs fs,
so that f ∈⟨f1, . . . , fs⟩. Thus r = 0 is a sufﬁcient condition for ideal membership.
However, as the following example shows, r = 0 is not a necessary condition for
being in the ideal.
Example 5. Let f1 = xy −1, f2 = y2 −1 ∈k[x, y] with the lex order. Dividing
f = xy2 −x by F = ( f1, f2), the result is
xy2 −x = y · (xy −1) + 0 · (y2 −1) + (−x + y).
With F = ( f2, f1), however, we have
xy2 −x = x · (y2 −1) + 0 · (xy −1) + 0.
The second calculation shows that f ∈⟨f1, f2⟩. Then the ﬁrst calculation shows that
even if f ∈⟨f1, f2⟩, it is still possible to obtain a nonzero remainder on division by
F = ( f1, f2).
Thus, we must conclude that the division algorithm given in Theorem 3 is an
imperfect generalization of its one-variable counterpart. To remedy this situation,
we turn to one of the lessons learned in Chapter 1. Namely, in dealing with a col-
lection of polynomials f1, . . . , fs ∈k[x1, . . . , xn], it is frequently desirable to pass
to the ideal I they generate. This allows the possibility of going from f1, . . . , fs to a
different generating set for I. So we can still ask whether there might be a "good"
generating set for I. For such a set, we would want the remainder r on division by
the "good" generators to be uniquely determined and the condition r = 0 should be
equivalent to membership in the ideal. In §6, we will see that Gröbner bases have
exactly these "good" properties.
In the exercises, you will experiment with a computer algebra system to try to
discover for yourself what properties a "good" generating set should have. We will
give a precise deﬁnition of "good" in §5 of this chapter.
EXERCISES FOR §3
1. Compute the remainder on division of the given polynomial f by the ordered set F (by
hand). Use the grlex order, then the lex order in each case.
a. f = x7y2 + x3y2 −y + 1, F = (xy2 −x, x −y3).
b. Repeat part (a) with the order of the pair F reversed.
2. Compute the remainder on division:
a. f = xy2z2 + xy −yz, F = (x −y2, y −z3, z2 −1).
b. Repeat part (a) with the order of the set F permuted cyclically.
3. Using a computer algebra system, check your work from Exercises 1 and 2. (You may
need to consult documentation to learn whether the system you are using has an explicit
polynomial division command or you will need to perform the individual steps of the
algorithm yourself.)

§3 A Division Algorithm in k[x1, . . . , xn]
69
4. Let f = q1 f1 + · · · + qs fs + r be the output of the division algorithm.
a. Complete the proof begun in the text that multideg( f ) ≥multideg(qi fi) provided that
qi fi ̸= 0.
b. Prove that multideg( f ) ≥multideg(r) when r ̸= 0.
The following problems investigate in greater detail the way the remainder computed by
the division algorithm depends on the ordering and the form of the s-tuple of divisors F =
( f1, . . . , fs). You may wish to use a computer algebra system to perform these calculations.
5. We will study the division of f = x3 −x2y −x2z + x by f1 = x2y −z and f2 = xy −1.
a. Compute using grlex order:
r1 = remainder of f on division by ( f1, f2).
r2 = remainder of f on division by ( f2, f1).
Your results should be different. Where in the division algorithm did the difference
occur? (You may need to do a few steps by hand here.)
b. Is r = r1 −r2 in the ideal ⟨f1, f2⟩? If so, ﬁnd an explicit expression r = Af1 + Bf2. If
not, say why not.
c. Compute the remainder of r on division by ( f1, f2). Why could you have predicted
your answer before doing the division?
d. Find another polynomial g ∈⟨f1, f2⟩such that the remainder on division of g by
( f1, f2) is nonzero. Hint: (xy + 1) · f2 = x2y2 −1, whereas y · f1 = x2y2 −yz.
e. Does the division algorithm give us a solution for the ideal membership problem for
the ideal ⟨f1, f2⟩? Explain your answer.
6. Using the grlex order, ﬁnd an element g of ⟨f1, f2⟩= ⟨2xy2 −x, 3x2y −y −1⟩⊆R[x, y]
whose remainder on division by ( f1, f2) is nonzero. Hint: You can ﬁnd such a g where
the remainder is g itself.
7. Answer the question of Exercise 6 for ⟨f1, f2, f3⟩= ⟨x4y2 −z, x3y3 −1, x2y4 −2z⟩
⊆R[x, y, z]. Find two different polynomials g (not constant multiples of each other).
8. Try to formulate a general pattern that ﬁts the examples in Exercises 5(c)(d), 6, and 7.
What condition on the leading term of the polynomial g = A1 f1 + · · · + As fs would
guarantee that there was a nonzero remainder on division by ( f1, . . . , fs)? What does
your condition imply about the ideal membership problem?
9. The discussion around equation (2) of Chapter 1, §4 shows that every polynomial f ∈
R[x, y, z] can be written as
f = h1(y −x2) + h2(z −x3) + r,
where r is a polynomial in x alone and V(y−x2, z−x3) is the twisted cubic curve in R3.
a. Give a proof of this fact using the division algorithm. Hint: You need to specify
carefully the monomial ordering to be used.
b. Use the parametrization of the twisted cubic to show that z2 −x4y vanishes at every
point of the twisted cubic.
c. Find an explicit representation
z2 −x4y = h1(y −x2) + h2(z −x3)
using the division algorithm.
10. Let V ⊆R3 be the curve parametrized by (t, tm, tn), n, m ≥2.
a. Show that V is an afﬁne variety.
b. Adapt the ideas in Exercise 9 to determine I(V).

70
Chapter 2 Gröbner Bases
11. In this exercise, we will characterize completely the expression
f = q1 f1 + · · · + qs fs + r
that is produced by the division algorithm (among all the possible expressions for f of
this form). Let LM( fi) = xα(i) and deﬁne
Δ1 = α(1) + Zn
≥0,
Δ2 = (α(2) + Zn
≥0) \ Δ1,
...
Δs = (α(s) + Zn
≥0) \
 s−1

i=1
Δi

,
Δ = Zn
≥0 \

s
i=1
Δi

.
(Note that Zn
≥0 is the disjoint union of the Δi and Δ.)
a. Show that β ∈Δi if and only if xα(i) divides xβ and no xα(j) with j < i divides xβ.
b. Show that γ ∈Δ if and only if no xα(i) divides xγ.
c. Show that in the expression f = q1 f1 + · · · + qs fs + r computed by the division
algorithm, for every i, every monomial xβ in qi satisﬁes β + α(i) ∈Δi, and every
monomial xγ in r satisﬁes γ ∈Δ.
d. Show that there is exactly one expression f = q1 f1 + · · · + qs fs + r satisfying the
properties given in part (c).
12. Show that the operation of computing remainders on division by F = ( f1, . . . , fs) is
linear over k. That is, if the remainder on division of gi by F is ri, i = 1, 2, then, for any
c1, c2 ∈k, the remainder on division of c1g1 +c2g2 is c1r1 +c2r2. Hint: Use Exercise 11.
§4
Monomial Ideals and Dickson's Lemma
In this section, we will consider the ideal description problem of §1 for the special
case of monomial ideals. This will require a careful study of the properties of these
ideals. Our results will also have an unexpected application to monomial orderings.
To start, we deﬁne monomial ideals in k[x1, . . . , xn].
Deﬁnition 1. An ideal I ⊆k[x1, . . . , xn] is a monomial ideal if there is a subset
A ⊆Zn
≥0 (possibly inﬁnite) such that I consists of all polynomials which are ﬁnite
sums of the form 
α∈A hαxα, where hα ∈k[x1, . . . , xn]. In this case, we write
I = ⟨xα | α ∈A⟩.
An example of a monomial ideal is given by I = ⟨x4y2, x3y4, x2y5⟩⊆k[x, y].
More interesting examples of monomial ideals will be given in §5.
We ﬁrst need to characterize all monomials that lie in a given monomial ideal.
Lemma 2. Let I = ⟨xα | α ∈A⟩be a monomial ideal. Then a monomial xβ lies in I
if and only if xβ is divisible by xα for some α ∈A.

§4 Monomial Ideals and Dickson's Lemma
71
Proof. If xβ is a multiple of xα for some α ∈A, then xβ ∈I by the deﬁnition of
ideal. Conversely, if xβ ∈I, then xβ = s
i=1 hi xα(i), where hi ∈k[x1, . . . , xn] and
α(i) ∈A. If we expand each hi as a sum of terms, we obtain
xβ =
s

i=1
hixα(i) =
s

i=1
 
j
ci,jxβ(i,j)
xα(i) =

i,j
ci,j xβ(i,j)xα(i).
After collecting terms of the same multidegree, every term on the right side of the
equation is divisible by some xα(i). Hence, the left side xβ must have the same
property.
□
Note that xβ is divisible by xα exactly when xβ = xα ·xγ for some γ ∈Zn
≥0. This
is equivalent to β = α + γ. Thus, the set
α + Zn
≥0 = {α + γ | γ ∈Zn
≥0}
consists of the exponents of all monomials divisible by xα. This observation and
Lemma 2 allows us to draw pictures of the monomials in a given monomial ideal.
For example, if I = ⟨x4y2, x3y4, x2y5⟩, then the exponents of the monomials in I
form the set
((4, 2) + Z2
≥0) ∪((3, 4) + Z2
≥0) ∪((2, 5) + Z2
≥0).
We can visualize this set as the union of the integer points in three translated copies
of the ﬁrst quadrant in the plane:
n
m
(m,n) ←→ xm yn
(2,5)
(3,4)
(4,2)
Let us next show that whether a given polynomial f lies in a monomial ideal can
be determined by looking at the monomials of f.
Lemma 3. Let I be a monomial ideal, and let f ∈k[x1, . . . , xn]. Then the following
are equivalent:
(i) f ∈I.
(ii) Every term of f lies in I.
(iii) f is a k-linear combination of the monomials in I.

72
Chapter 2 Gröbner Bases
Proof. The implications (iii) ⇒(ii) ⇒(i) and (ii) ⇒(iii) are trivial. The proof of
(i) ⇒(ii) is similar to what we did in Lemma 2 and is left as an exercise.
□
An immediate consequence of part (iii) of the lemma is that a monomial ideal is
uniquely determined by its monomials. Hence, we have the following corollary.
Corollary 4. Two monomial ideals are the same if and only if they contain the same
monomials.
The main result of this section is that all monomial ideals of k[x1, . . . , xn] are
ﬁnitely generated.
Theorem 5 (Dickson's Lemma). Let I = ⟨xα | α ∈A⟩⊆k[x1, . . . , xn] be a
monomial ideal. Then I can be written in the form I = ⟨xα(1), . . . , xα(s)⟩, where
α(1), . . . , α(s) ∈A. In particular, I has a ﬁnite basis.
Proof. (By induction on n, the number of variables.) If n = 1, then I is generated by
the monomials xα
1 , where α ∈A ⊆Z≥0. Let β be the smallest element of A ⊆Z≥0.
Then β ≤α for all α ∈A, so that xβ
1 divides all other generators xα
1 . From here,
I = ⟨xβ
1 ⟩follows easily.
Now assume that n > 1 and that the theorem is true for n −1. We will write the
variables as x1, . . . , xn−1, y, so that monomials in k[x1, . . . , xn−1, y] can be written as
xαym, where α = (α1, . . . , αn−1) ∈Zn−1
≥0 and m ∈Z≥0.
Suppose that I ⊆k[x1, . . . , xn−1, y] is a monomial ideal. To ﬁnd generators for
I, let J be the ideal in k[x1, . . . , xn−1] generated by the monomials xα for which
xαym ∈I for some m ≥0. Since J is a monomial ideal in k[x1, . . . , xn−1],
our inductive hypothesis implies that ﬁnitely many of the xα's generate J, say
J = ⟨xα(1), . . . , xα(s)⟩. The ideal J can be understood as the "projection" of I into
k[x1, . . . , xn−1].
For each i between 1 and s, the deﬁnition of J tells us that xα(i)ymi ∈I for some
mi ≥0. Let m be the largest of the mi. Then, for each ℓbetween 0 and m −1,
consider the ideal Jℓ⊆k[x1, . . . , xn−1] generated by the monomials xβ such that
xβyℓ∈I. One can think of Jℓas the "slice" of I generated by monomials containing
y exactly to the ℓth power. Using our inductive hypothesis again, Jℓhas a ﬁnite
generating set of monomials, say Jℓ= ⟨xαℓ(1), . . . , xαℓ(sℓ)⟩.
We claim that I is generated by the monomials in the following list:
from J : xα(1)ym, . . . , xα(s)ym,
from J0 : xα0(1), . . . , xα0(s0),
from J1 : xα1(1)y, . . . , xα1(s1)y,
...
from Jm−1 : xαm−1(1)ym−1, . . . , xαm−1(sm−1)ym−1.
First note that every monomial in I is divisible by one on the list. To see why, let
xαyp ∈I. If p ≥m, then xαyp is divisible by some xα(i)ym by the construction of J.
On the other hand, if p ≤m −1, then xαyp is divisible by some xαp(j)yp by the

§4 Monomial Ideals and Dickson's Lemma
73
construction of Jp. It follows from Lemma 2 that the above monomials generate an
ideal having the same monomials as I. By Corollary 4, this forces the ideals to be
the same, and our claim is proved.
To complete the proof, we need to show that the ﬁnite set of generators can be
chosen from a given set of generators for the ideal. If we switch back to writing the
variables as x1, . . . , xn, then our monomial ideal is I = ⟨xα | α ∈A⟩⊆k[x1, . . . , xn].
We need to show that I is generated by ﬁnitely many of the xα's, where α ∈A. By
the previous paragraph, we know that I = ⟨xβ(1), . . . , xβ(s)⟩for some monomials
xβ(i) in I. Since xβ(i) ∈I = ⟨xα : α ∈A⟩, Lemma 2 tells us that each xβ(i) is divisible
by xα(i) for some α(i) ∈A. From here, it is easy to show that I = ⟨xα(1), . . . , xα(s)⟩
(see Exercise 6 for the details). This completes the proof.
□
To better understand how the proof of Theorem 5 works, let us apply it to the
ideal I = ⟨x4y2, x3y4, x2y5⟩discussed earlier in the section. From the picture of the
exponents, you can see that the "projection" is J = ⟨x2⟩⊆k[x]. Since x2y5 ∈I,
we have m = 5. Then we get the "slices" Jℓ, 0 ≤ℓ≤4 = m −1, generated by
monomials containing yℓ:
J0 = J1 = {0},
J2 = J3 = ⟨x4⟩,
J4 = ⟨x3⟩.
These "slices" are easy to see using the picture of the exponents. Then the proof of
Theorem 5 gives I = ⟨x2y5, x4y2, x4y3, x3y4⟩.
Theorem 5 solves the ideal description problem for monomial ideals, for it tells
that such an ideal has a ﬁnite basis. This, in turn, allows us to solve the ideal mem-
bership problem for monomial ideals. Namely, if I = ⟨xα(1), . . . , xα(s)⟩, then one
can easily show that a given polynomial f is in I if and only if the remainder of f on
division by xα(1), . . . , xα(s) is zero. See Exercise 8 for the details.
We can also use Dickson's Lemma to prove the following important fact about
monomial orderings in k[x1, . . . , xn].
Corollary 6. Let > be a relation on Zn
≥0 satisfying:
(i) > is a total ordering on Zn
≥0.
(ii) If α > β and γ ∈Zn
≥0, then α + γ > β + γ.
Then > is well-ordering if and only if α ≥0 for all α ∈Zn
≥0.
Proof. ⇒: Assuming > is a well-ordering, let α0 be the smallest element of Zn
≥0. It
sufﬁces to show α0 ≥0. This is easy: if 0 > α0, then by hypothesis (ii), we can add
α0 to both sides to obtain α0 > 2α0, which is impossible since α0 is the smallest
element of Zn
≥0.
⇐: Assuming that α ≥0 for all α ∈Zn
≥0, let A ⊆Zn
≥0 be nonempty. We need
to show that A has a smallest element. Since I = ⟨xα | α ∈A⟩is a monomial
ideal, Dickson's Lemma gives us α(1), . . . , α(s) ∈A so that I = ⟨xα(1), . . . , xα(s)⟩.
Relabeling if necessary, we can assume that α(1) < α(2) < . . . < α(s). We claim
that α(1) is the smallest element of A. To prove this, take α ∈A. Then xα ∈I =

74
Chapter 2 Gröbner Bases
⟨xα(1), . . . , xα(s)⟩, so that by Lemma 2, xα is divisible by some xα(i). This tells us
that α = α(i) + γ for some γ ∈Zn
≥0. Then γ ≥0 and hypothesis (ii) imply that
α = α(i) + γ ≥α(i) + 0 = α(i) ≥α(1).
Thus, α(1) is the least element of A.
□
As a result of this corollary, the deﬁnition of monomial ordering given in Deﬁ-
nition 1 of §2 can be simpliﬁed. Conditions (i) and (ii) in the deﬁnition would be
unchanged, but we could replace (iii) by the simpler condition that α ≥0 for all
α ∈Zn
≥0. This makes it much easier to verify that a given ordering is actually a
monomial ordering. See Exercises 9-11 for some examples.
Among all bases of a monomial ideal, there is one that is better than the others.
Proposition 7. A monomial ideal I ⊆k[x1, . . . , xn] has a basis xα(1), . . . , xα(s) with
the property that xα(i) does not divide xα(j) for i ̸= j. Furthermore, this basis is
unique and is called the minimal basis of I.
Proof. By Theorem 5, I has a ﬁnite basis consisting of monomials. If one monomial
in this basis divides another, then we can discard the other and still have a basis.
Doing this repeatedly proves the existence of a minimal basis xα(1), . . . , xα(s).
For uniqueness, assume that xβ(1), . . . , xβ(t) is a second minimal basis of I. Then
xα(1) ∈I and Lemma 2 imply that xβ(i) | xα(1) for some i. Switching to the other
basis, xβ(i) ∈I implies that xα(j) | xβ(i) for some j. Thus xα(j) | xα(1), which
by minimality implies j = 1, and xα(1) = xβ(i) follows easily. Continuing in this
way, we see that the ﬁrst basis is contained in the second. Then equality follows by
interchanging the two bases.
□
EXERCISES FOR §4
1. Let I ⊆k[x1, . . . , xn] be an ideal with the property that for every f =  cαxα ∈I, every
monomial xα appearing in f is also in I. Show that I is a monomial ideal.
2. Complete the proof of Lemma 3 begun in the text.
3. Let I = ⟨x6, x2y3, xy7⟩⊆k[x, y].
a. In the (m, n)-plane, plot the set of exponent vectors (m, n) of monomials xmyn ap-
pearing in elements of I.
b. If we apply the division algorithm to an element f ∈k[x, y], using the generators of I
as divisors, what terms can appear in the remainder?
4. Let I ⊆k[x, y] be the monomial ideal spanned over k by the monomials xβ corresponding
to β in the shaded region shown at the top of the next page.
a. Use the method given in the proof of Theorem 5 to ﬁnd an ideal basis for I.
b. Find a minimal basis for I in the sense of Proposition 7.
5. Suppose that I = ⟨xα | α ∈A⟩is a monomial ideal, and let S be the set of all exponents
that occur as monomials of I. For any monomial order >, prove that the smallest element
of S with respect to > must lie in A.

§4 Monomial Ideals and Dickson's Lemma
75
n
m
(m,n) ←→ xm yn
(3,6)
(5,4)
(6,0)
6. Let I = ⟨xα | α ∈A⟩be a monomial ideal, and assume that we have a ﬁnite basis
I = ⟨xβ(1), . . . , xβ(s)⟩. In the proof of Dickson's Lemma, we observed that each xβ(i) is
divisible by xα(i) for some α(i) ∈A. Prove that I = ⟨xα(1), . . . , xα(s)⟩.
7. Prove that Dickson's Lemma (Theorem 5) is equivalent to the following statement: given
a nonempty subset A ⊆Zn
≥0, there are ﬁnitely many elements α(1), . . . , α(s) ∈A such
that for every α ∈A, there exists some i and some γ ∈Zn
≥0 such that α = α(i) + γ.
8. If I = ⟨xα(1), . . . , xα(s)⟩is a monomial ideal, prove that a polynomial f is in I if and
only if the remainder of f on division by xα(1), . . . , xα(s) is zero. Hint: Use Lemmas 2
and 3.
9. Suppose we have the polynomial ring k[x1, . . . , xn, y1, . . . , ym]. Let us deﬁne a mono-
mial order >mixed on this ring that mixes lex order for x1, . . . xn, with grlex order for
y1, . . . , ym. If we write monomials in the n + m variables as xα yβ, where α ∈Zn
≥0 and
β ∈Zm
≥0, then we deﬁne
xα yβ >mixed xγ yδ ⇐⇒xα >lex xγ
or
xα = xγ
and
yβ >grlex yδ.
Use Corollary 6 to prove that >mixed is a monomial order. This is an example of what
is called a product order. It is clear that many other monomial orders can be created by
this method.
10. In this exercise we will investigate a special case of a weight order. Let u = (u1, . . . , un)
be a vector in Rn such that u1, . . . , un are positive and linearly independent over Q. We
say that u is an independent weight vector. Then, for α, β ∈Zn
≥0, deﬁne
α >u β ⇐⇒u · α > u · β,
where the centered dot is the usual dot product of vectors. We call >u the weight order
determined by u.
a. Use Corollary 6 to prove that >u is a monomial order. Hint: Where does your argu-
ment use the linear independence of u1, . . . , un?
b. Show that u = (1,
√
2) is an independent weight vector, so that >u is a weight order
on Z2
≥0.
c. Show that u = (1,
√
2,
√
3) is an independent weight vector, so that >u is a weight
order on Z3
≥0.
11. Another important weight order is constructed as follows. Let u = (u1, . . . , un) be in
Zn
≥0, and ﬁx a monomial order >σ (such as >lex or >grevlex) on Zn
≥0. Then, for α, β ∈
Zn
≥0, deﬁne α >u,σ β if and only if

76
Chapter 2 Gröbner Bases
u · α > u · β
or
u · α = u · β
and
α >σ β.
We call >u,σ the weight order determined by u and >σ.
a. Use Corollary 6 to prove that >u,σ is a monomial order.
b. Find u ∈Zn
≥0 so that the weight order >u,lex is the grlex order >grlex.
c. In the deﬁnition of >u,σ, the order >σ is used to break ties, and it turns out that ties
will always occur when n ≥2. More precisely, prove that given u ∈Zn
≥0, there are
α ̸= β in Zn
≥0 such that u · α = u · β. Hint: Consider the linear equation u1a1 +
· · · + unan = 0 over Q. Show that there is a nonzero integer solution (a1, . . . , an),
and then show that (a1, . . . , an) = α −β for some α, β ∈Zn
≥0.
d. A useful example of a weight order is the elimination order introduced by BAYER
and STILLMAN (1987b). Fix an integer 1 ≤l ≤n and let u = (1, . . . , 1, 0, . . . , 0),
where there are l 1's and n −l 0's. Then the l-th elimination order >l is the weight
order >u,grevlex. Prove that >l has the following property: if xα is a monomial in
which one of x1, . . . , xl appears, then xα >l xβ for any monomial involving only
xl+1, . . . , xn. Elimination orders play an important role in elimination theory, which
we will study in the next chapter.
The weight orders described in Exercises 10 and 11 are only special cases of weight orders.
In general, to determine a weight order, one starts with a vector u1 ∈Rn, whose entries may
not be linearly independent over Q. Then α > β if u1 · α > u1 · β. But to break ties, one uses
a second weight vector u2 ∈Rn. Thus, α > β also holds if u1 ·α = u1 ·β and u2 ·α > u2 ·β.
If there are still ties (when u1 · α = u1 · β and u2 · α = u2 · β), then one uses a third
weight vector u3, and so on. It can be proved that every monomial order on Zn
≥0 arises in this
way. For a detailed treatment of weight orders and their relation to monomial orders, consult
ROBBIANO (1986). See also Tutorial 10 of KREUZER and ROBBIANO (2000) or Section 1.2
of GREUEL and PFISTER (2008).
§5
The Hilbert Basis Theorem and Gröbner Bases
In this section, we will give a complete solution of the ideal description problem
from §1. Our treatment will also lead to ideal bases with "good" properties relative
to the division algorithm introduced in §3. The key idea we will use is that once we
choose a monomial ordering, each nonzero f ∈k[x1, . . . , xn] has a unique leading
term LT( f). Then, for any ideal I, we can deﬁne its ideal of leading terms as follows.
Deﬁnition 1. Let I ⊆k[x1, . . . , xn] be an ideal other than {0}, and ﬁx a monomial
ordering on k[x1, . . . , xn]. Then:
(i) We denote by LT(I) the set of leading terms of nonzero elements of I. Thus,
LT(I) = {cxα | there exists f ∈I \ {0} with LT( f) = cxα}.
(ii) We denote by ⟨LT(I)⟩the ideal generated by the elements of LT(I).
We have already seen that leading terms play an important role in the divi-
sion algorithm. This brings up a subtle but important point concerning ⟨LT(I)⟩.
Namely, if we are given a ﬁnite generating set for I, say I = ⟨f1, . . . , fs⟩, then
⟨LT( f1), . . . , LT( fs)⟩and ⟨LT(I)⟩may be different ideals. It is true that LT( fi) ∈

§5 The Hilbert Basis Theorem and Gröbner Bases
77
LT(I) ⊆⟨LT(I)⟩by deﬁnition, which implies ⟨LT( f1), . . . , LT( fs)⟩⊆⟨LT(I)⟩. How-
ever, ⟨LT(I)⟩can be strictly larger. To see this, consider the following example.
Example 2. Let I = ⟨f1, f2⟩, where f1 = x3 −2xy and f2 = x2y −2y2 + x, and use
the grlex ordering on monomials in k[x, y]. Then
x · (x2y −2y2 + x) −y · (x3 −2xy) = x2,
so that x2 ∈I. Thus x2 = LT(x2) ∈⟨LT(I)⟩. However x2 is not divisible by LT( f1) =
x3 or LT( f2) = x2y, so that x2 /∈⟨LT( f1), LT( f2)⟩by Lemma 2 of §4.
In the exercises for §3, you computed other examples of ideals I = ⟨f1, . . . , fs⟩,
where ⟨LT(I)⟩was strictly bigger than ⟨LT( f1), . . . , LT( fs)⟩. The exercises at the end
of the section will explore what this implies about the ideal membership problem.
We will now show that ⟨LT(I)⟩is a monomial ideal. This will allow us to apply
the results of §4. In particular, it will follow that ⟨LT(I)⟩is generated by ﬁnitely
many leading terms.
Proposition 3. Let I ⊆k[x1, . . . , xn] be an ideal different from {0}.
(i) ⟨LT(I)⟩is a monomial ideal.
(ii) There are g1, . . . , gt ∈I such that ⟨LT(I)⟩= ⟨LT(g1), . . . , LT(gt)⟩.
Proof. (i) The leading monomials LM(g) of elements g ∈I \ {0} generate the
monomial ideal ⟨LM(g) | g ∈I \ {0}⟩. Since LM(g) and LT(g) differ by a nonzero
constant, this ideal equals ⟨LT(g) | g ∈I \ {0}⟩= ⟨LT(I)⟩(see Exercise 4). Thus,
⟨LT(I)⟩is a monomial ideal.
(ii) Since ⟨LT(I)⟩is generated by the monomials LM(g) for g ∈I\{0}, Dickson's
Lemma from §4 tells us that ⟨LT(I)⟩= ⟨LM(g1), . . . , LM(gt)⟩for ﬁnitely many
g1, . . . , gt ∈I. Since LM(gi) differs from LT(gi) by a nonzero constant, it follows
that ⟨LT(I)⟩= ⟨LT(g1), . . . , LT(gt)⟩. This completes the proof.
□
We can now use Proposition 3 and the division algorithm to prove the existence
of a ﬁnite generating set of every polynomial ideal, thus giving an afﬁrmative answer
to the ideal description problem from §1.
Theorem 4 (Hilbert Basis Theorem). Every ideal I ⊆k[x1, . . . , xn] has a ﬁnite
generating set. In other words, I = ⟨g1, . . . , gt⟩for some g1, . . . , gt ∈I.
Proof. If I = {0}, we take our generating set to be {0}, which is certainly ﬁnite.
If I contains some nonzero polynomial, then a generating set g1, . . . , gt for I can be
constructed as follows.
We ﬁrst select one particular monomial order to use in the division algorithm
and in computing leading terms. Then I has an ideal of leading terms ⟨LT(I)⟩. By
Proposition 3, there are g1, . . . , gt ∈I such that ⟨LT(I)⟩= ⟨LT(g1), . . . , LT(gt)⟩. We
claim that I = ⟨g1, . . . , gt⟩.
It is clear that ⟨g1, . . . , gt⟩⊆I since each gi ∈I. Conversely, let f ∈I be any
polynomial. If we apply the division algorithm from §3 to divide f by (g1, . . . , gt),
then we get an expression of the form

78
Chapter 2 Gröbner Bases
f = q1g1 + · · · + qtgt + r
where no term of r is divisible by any of LT(g1), . . . , LT(gt). We claim that r = 0.
To see this, note that
r = f −q1g1 −· · · −qtgt ∈I.
If r ̸= 0, then LT(r) ∈⟨LT(I)⟩= ⟨LT(g1), . . . , LT(gt)⟩, and by Lemma 2 of §4,
it follows that LT(r) must be divisible by some LT(gi). This contradicts what it
means to be a remainder, and, consequently, r must be zero. Thus,
f = q1g1 + · · · + qtgt + 0 ∈⟨g1, . . . , gt⟩,
which shows that I ⊆⟨g1, . . . , gt⟩. This completes the proof.
□
Besides answering the ideal description question, the basis {g1, . . . , gt} used in
the proof of Theorem 4 has the special property that ⟨LT(I)⟩= ⟨LT(g1), . . . , LT(gt)⟩.
As we saw in Example 2, not all bases of an ideal behave this way. We will give these
special bases the following name.
Deﬁnition 5. Fix a monomial order on the polynomial ring k[x1, . . . , xn]. A ﬁnite
subset G = {g1, . . . , gt} of an ideal I ⊆k[x1, . . . , xn] different from {0} is said to
be a Gröbner basis (or standard basis) if
⟨LT(g1), . . . , LT(gt)⟩= ⟨LT(I)⟩.
Using the convention that ⟨∅⟩= {0}, we deﬁne the empty set ∅to be the Gröbner
basis of the zero ideal {0}.
Equivalently, but more informally, a set {g1, . . . , gt} ⊆I is a Gröbner basis of I
if and only if the leading term of any element of I is divisible by one of the LT(gi)
(this follows from Lemma 2 of §4—see Exercise 5). The proof of Theorem 4 also
establishes the following result.
Corollary 6. Fix a monomial order. Then every ideal I ⊆k[x1, . . . , xn] has a Gröb-
ner basis. Furthermore, any Gröbner basis for an ideal I is a basis of I.
Proof. Given a nonzero ideal, the set G = {g1, . . . , gt} constructed in the proof
of Theorem 4 is a Gröbner basis by deﬁnition. For the second claim, note that if
⟨LT(I)⟩= ⟨LT(g1), . . . , LT(gt)⟩, then the argument given in Theorem 4 shows that
I = ⟨g1, . . . , gt⟩, so that G is a basis for I. (A slightly different proof is given in
Exercise 6.)
□
In §6 we will study the properties of Gröbner bases in more detail, and, in partic-
ular, we will see how they give a solution of the ideal membership problem. Gröbner
bases are the "good" generating sets we hoped for at the end of §3.
For some examples of Gröbner bases, ﬁrst consider the ideal I from Example 2,
which had the basis { f1, f2} = {x3 −2xy, x2y −2y2 + x}. Then { f1, f2} is not
a Gröbner basis for I with respect to grlex order since we saw in Example 2 that

§5 The Hilbert Basis Theorem and Gröbner Bases
79
x2 ∈⟨LT(I)⟩, but x2 /∈⟨LT( f1), LT( f2)⟩. In §7 we will learn how to ﬁnd a Gröbner
basis of I.
Next, consider the ideal J = ⟨g1, g2⟩= ⟨x+z, y−z⟩. We claim that g1 and g2 form
a Gröbner basis using lex order in R[x, y, z]. Thus, we must show that the leading
term of every nonzero element of J lies in the ideal ⟨LT(g1), LT(g2)⟩= ⟨x, y⟩. By
Lemma 2 of §4, this is equivalent to showing that the leading term of any nonzero
element of J is divisible by either x or y.
To prove this, consider any f = Ag1 + Bg2 ∈J. Suppose on the contrary that f
is nonzero and LT( f) is divisible by neither x nor y. Then by the deﬁnition of lex
order, f must be a polynomial in z alone. However, f vanishes on the linear subspace
L = V(x+z, y−z) ⊆R3 since f ∈J. It is easy to check that (x, y, z) = (−t, t, t) ∈L
for any real number t. The only polynomial in z alone that vanishes at all of these
points is the zero polynomial, which is a contradiction. It follows that ⟨g1, g2⟩is a
Gröbner basis for J. In §6, we will learn a more systematic way to detect when a
basis is a Gröbner basis.
Note, by the way, that the generators for the ideal J come from a row echelon
matrix of coefﬁcients:

1
0
1
0
1 −1

.
This is no accident: for ideals generated by linear polynomials, a Gröbner basis
for lex order is determined by the row echelon form of the matrix made from the
coefﬁcients of the generators (see Exercise 9).
Gröbner bases for ideals in polynomial rings were introduced by B. Buchberger
in his PhD thesis BUCHBERGER (1965) and named by him in honor of W. Gröbner
(1899-1980), Buchberger's thesis adviser. The closely related concept of "standard
bases" for ideals in power series rings was discovered independently by H. Hiron-
aka in HIRONAKA (1964). As we will see later in this chapter, Buchberger also
developed the fundamental algorithms for working with Gröbner bases. Sometimes
one sees the alternate spelling "Groebner bases," since this is how the command is
spelled in some computer algebra systems.
We conclude this section with two applications of the Hilbert Basis Theorem.
The ﬁrst is an algebraic statement about the ideals in k[x1, . . . , xn]. An ascending
chain of ideals is a nested increasing sequence:
I1 ⊆I2 ⊆I3 ⊆· · · .
For example, the sequence
(1)
⟨x1⟩⊆⟨x1, x2⟩⊆· · · ⊆⟨x1, . . . , xn⟩
forms a (ﬁnite) ascending chain of ideals. If we try to extend this chain by including
an ideal with further generator(s), one of two alternatives will occur. Consider the
ideal ⟨x1, . . . , xn, f⟩where f ∈k[x1, . . . , xn]. If f ∈⟨x1, . . . , xn⟩, then we obtain
⟨x1, . . . , xn⟩again and nothing has changed. If, on the other hand, f /∈⟨x1, . . . , xn⟩,
then we claim ⟨x1, . . . , xn, f⟩= k[x1, . . . , xn]. We leave the proof of this claim to

80
Chapter 2 Gröbner Bases
the reader (Exercise 11 of this section). As a result, the ascending chain (1) can
be continued in only two ways, either by repeating the last ideal ad inﬁnitum or
by appending k[x1, . . . , xn] and then repeating it ad inﬁnitum. In either case, the
ascending chain will have "stabilized" after a ﬁnite number of steps, in the sense
that all the ideals after that point in the chain will be equal. Our next result shows
that the same phenomenon occurs in every ascending chain of ideals in k[x1, . . . , xn].
Theorem 7 (The Ascending Chain Condition). Let
I1 ⊆I2 ⊆I3 ⊆· · ·
be an ascending chain of ideals in k[x1, . . . , xn]. Then there exists an N ≥1 such
that
IN = IN+1 = IN+2 = · · · .
Proof. Given the ascending chain I1 ⊆I2 ⊆I3 ⊆· · · , consider the set I = ∞
i=1 Ii.
We begin by showing that I is also an ideal in k[x1, . . . , xn]. First, 0 ∈I since 0 ∈Ii
for every i. Next, if f, g ∈I, then, by deﬁnition, f ∈Ii, and g ∈Ij for some i
and j (possibly different). However, since the ideals Ii form an ascending chain, if
we relabel so that i ≤j, then both f and g are in Ij. Since Ij is an ideal, the sum
f + g ∈Ij, hence, ∈I. Similarly, if f ∈I and r ∈k[x1, . . . , xn], then f ∈Ii for some
i, and r · f ∈Ii ⊆I. Hence, I is an ideal.
By the Hilbert Basis Theorem, the ideal I must have a ﬁnite generating set: I =
⟨f1, . . . , fs⟩. But each of the generators is contained in some one of the Ij, say fi ∈Iji
for some ji, i = 1, . . . , s. We take N to be the maximum of the ji. Then by the
deﬁnition of an ascending chain fi ∈IN for all i. Hence we have
I = ⟨f1, . . . , fs⟩⊆IN ⊆IN+1 ⊆· · · ⊆I.
As a result the ascending chain stabilizes with IN. All the subsequent ideals in the
chain are equal.
□
The statement that every ascending chain of ideals in k[x1, . . . , xn] stabilizes is
often called the ascending chain condition, or ACC for short. In Exercise 12 of
this section, you will show that if we assume the ACC as hypothesis, then it follows
that every ideal is ﬁnitely generated. Thus, the ACC is actually equivalent to the
conclusion of the Hilbert Basis Theorem. We will use the ACC in a crucial way in
§7, when we give Buchberger's algorithm for constructing Gröbner bases. We will
also use the ACC in Chapter 4 to study the structure of afﬁne varieties.
Our second consequence of the Hilbert Basis Theorem will be geometric. Up to
this point, we have considered afﬁne varieties as the sets of solutions of speciﬁc
ﬁnite sets of polynomial equations:
V( f1, . . . , fs) = {(a1, . . . , an) ∈kn | fi(a1, . . . , an) = 0 for all i}.
The Hilbert Basis Theorem shows that, in fact, it also makes sense to speak of the
afﬁne variety deﬁned by an ideal I ⊆k[x1, . . . , xn].

§5 The Hilbert Basis Theorem and Gröbner Bases
81
Deﬁnition 8. Let I ⊆k[x1, . . . , xn] be an ideal. We will denote by V(I) the set
V(I) = {(a1, . . . , an) ∈kn | f(a1, . . . , an) = 0 for all f ∈I}.
Even though a nonzero ideal I always contains inﬁnitely many different polyno-
mials, the set V(I) can still be deﬁned by a ﬁnite set of polynomial equations.
Proposition 9. V(I) is an afﬁne variety. In particular, if I = ⟨f1, . . . , fs⟩, then
V(I) = V( f1, . . . , fs).
Proof. By the Hilbert Basis Theorem, I = ⟨f1, . . . , fs⟩for some ﬁnite generating
set. We claim that V(I) = V( f1, . . . , fs). First, since the fi ∈I, if f(a1, . . . , an) = 0
for all f ∈I, then fi(a1, . . . , an) = 0, so V(I) ⊆V( f1, . . . , fs). On the other hand,
let (a1, . . . , an) ∈V( f1, . . . , fs) and let f ∈I. Since I = ⟨f1, . . . , fs⟩, we can write
f =
s

i=1
hi fi
for some hi ∈k[x1, . . . , xn]. But then
f(a1, . . . , an) =
s

i=1
hi(a1, . . . , an) fi(a1, . . . , an)
=
s

i=1
hi(a1, . . . , an) · 0 = 0.
Thus, V( f1, . . . , fs) ⊆V(I) and, hence, they are equal.
□
The most important consequence of this proposition is that varieties are de-
termined by ideals. For example, in Chapter 1, we proved that V( f1, . . . , fs) =
V(g1, . . . , gt) whenever ⟨f1, . . . , fs⟩= ⟨g1, . . . , gt⟩(see Proposition 4 of Chapter 1,
§4). This proposition is an immediate corollary of Proposition 9. The relation be-
tween ideals and varieties will be explored in more detail in Chapter 4.
In the exercises, we will exploit Proposition 9 by showing that by using the right
generating set for an ideal I, we can gain a better understanding of the variety V(I).
EXERCISES FOR §5
1. Let I = ⟨g1, g2, g3⟩⊆R[x, y, z], where g1 = xy2 −xz + y, g2 = xy −z2 and
g3 = x −yz4. Using the lex order, give an example of g ∈I such that LT(g) /∈
⟨LT(g1), LT(g2), LT(g3)⟩.
2. For the ideals and generators given in Exercises 5, 6, and 7 of §3, show that LT(I) is
strictly bigger than ⟨LT( f1), . . . , LT( fs)⟩. Hint: This should follow directly from what
you did in those exercises.
3. To generalize the situation of Exercises 1 and 2, suppose that I = ⟨f1, . . . , fs⟩is an ideal
such that ⟨LT( f1), . . . , LT( fs)⟩is strictly smaller than ⟨LT(I)⟩.
a. Prove that there is some f ∈I whose remainder on division by f1, . . . , fs is nonzero.
Hint: First show that LT( f ) /∈⟨LT( f1), . . . , LT( fs)⟩for some f ∈I. Then use
Lemma 2 of §4.

82
Chapter 2 Gröbner Bases
b. What does part (a) say about the ideal membership problem?
c. How does part (a) relate to the conjecture you were asked to make in Exercise 8 of
§3?
4. If I ⊆k[x1, . . . , xn] is an ideal, prove that ⟨LT(g) | g ∈I\{0}⟩= ⟨LM(g) | g ∈I\{0}⟩.
5. Let I be an ideal of k[x1, . . . , xn]. Show that G = {g1, . . . , gt} ⊆I is a Gröbner basis of
I if and only if the leading term of any element of I is divisible by one of the LT(gi).
6. Corollary 6 asserts that a Gröbner basis is a basis, i.e., if G = {g1, . . . , gt} ⊆I satisﬁes
⟨LT(I)⟩= ⟨LT(g1), . . . , LT(gt)⟩, then I = ⟨g1, . . . , gt⟩. We gave one proof of this in the
proof of Theorem 4. Complete the following sketch to give a second proof. If f ∈I, then
divide f by (g1, . . . , gt). At each step of the division algorithm, the leading term of the
polynomial under the division will be in ⟨LT(I)⟩and, hence, will be divisible by one of
the LT(gi). Hence, terms are never added to the remainder, so that f = t
i=1 ai gi when
the algorithm terminates.
7. If we use grlex order with x > y > z, is {x4y2 −z5, x3y3 −1, x2y4 −2z} a Gröbner basis
for the ideal generated by these polynomials? Why or why not?
8. Repeat Exercise 7 for I = ⟨x −z2, y −z3⟩using the lex order. Hint: The difﬁcult part of
this exercise is to determine exactly which polynomials are in ⟨LT(I)⟩.
9. Let A = (aij) be an m × n matrix with real entries in row echelon form and let J ⊆
R[x1, . . . , xn] be an ideal generated by the linear polynomials n
j=1 aijxj for 1 ≤i ≤m.
Show that the given generators form a Gröbner basis for J with respect to a suitable
lexicographic order. Hint: Order the variables corresponding to the leading 1's before
the other variables.
10. Let I ⊆k[x1, . . . , xn] be a principal ideal (that is, I is generated by a single f ∈I—
see §5 of Chapter 1). Show that any ﬁnite subset of I containing a generator for I is a
Gröbner basis for I.
11. Let f ∈k[x1, . . . , xn]. If f /∈⟨x1, . . . , xn⟩, then show ⟨x1, . . . , xn, f ⟩= k[x1, . . . , xn].
12. Show that if we take as hypothesis that every ascending chain of ideals in k[x1, . . . , xn]
stabilizes, then the conclusion of the Hilbert Basis Theorem is a consequence. Hint: Ar-
gue by contradiction, assuming that some ideal I ⊆k[x1, . . . , xn] has no ﬁnite generating
set. The arguments you gave in Exercise 12 should not make any special use of proper-
ties of polynomials. Indeed, it is true that in any commutative ring R, the following two
statements are equivalent:
(i) Every ideal I ⊆R is ﬁnitely generated.
(ii) Every ascending chain of ideals of R stabilizes.
13. Let
V1 ⊇V2 ⊇V3 ⊇· · ·
be a descending chain of afﬁne varieties. Show that there is some N ≥1 such that
VN = VN+1 = VN+2 = · · · . Hint: Use the ACC and Exercise 14 of Chapter 1, §4.
14. Let f1, f2, . . . ∈k[x1, . . . , xn] be an inﬁnite collection of polynomials. Prove that there is
an integer N such that fi ∈⟨f1, . . . , fN⟩for all i ≥N + 1. Hint: Use f1, f2, . . . to create
an ascending chain of ideals.
15. Given polynomials f1, f2, . . . ∈k[x1, . . . , xn], let V( f1, f2, . . .) ⊆kn be the solutions of
the inﬁnite system of equations f1 = f2 = · · · = 0. Show that there is some N such that
V( f1, f2, . . .) = V( f1, . . . , fN).
16. In Chapter 1, §4, we deﬁned the ideal I(V) of a variety V ⊆kn. In this section, we
deﬁned the variety of any ideal (see Deﬁnition 8). In particular, this means that V(I(V))
is a variety. Prove that V(I(V)) = V. Hint: See the proof of Lemma 7 of Chapter 1, §4.
17. Consider the variety V = V(x2 −y, y + x2 −4) ⊆C2. Note that V = V(I), where
I = ⟨x2 −y, y + x2 −4⟩.
a. Prove that I = ⟨x2 −y, x2 −2⟩.
b. Using the basis from part (a), prove that V(I) = {(±
√
2, 2)}.

§6 Properties of Gröbner Bases
83
One reason why the second basis made V easier to understand was that x2 −2 could
be factored. This implied that V "split" into two pieces. See Exercise 18 for a general
statement.
18. When an ideal has a basis where some of the elements can be factored, we can use the
factorization to help understand the variety.
a. Show that if g ∈k[x1, . . . , xn] factors as g = g1g2, then for any f , we have V( f , g) =
V( f , g1) ∪V( f , g2).
b. Show that in R3, V(y −x2, xz −y2) = V(y −x2, xz −x4).
c. Use part (a) to describe and/or sketch the variety from part (b).
§6
Properties of Gröbner Bases
As shown in §5, every nonzero ideal I ⊆k[x1, . . . , xn] has a Gröbner basis. In this
section, we will study the properties of Gröbner bases and learn how to detect when
a given basis is a Gröbner basis. We begin by showing that the undesirable behavior
of the division algorithm in k[x1, . . . , xn] noted in §3 does not occur when we divide
by the elements of a Gröbner basis.
Let us ﬁrst prove that the remainder is uniquely determined when we divide by a
Gröbner basis.
Proposition 1. Let I ⊆k[x1, . . . , xn] be an ideal and let G = {g1, . . . , gt} be a
Gröbner basis for I. Then given f ∈k[x1, . . . , xn], there is a unique r ∈k[x1, . . . , xn]
with the following two properties:
(i) No term of r is divisible by any of LT(g1), . . . , LT(gt).
(ii) There is g ∈I such that f = g + r.
In particular, r is the remainder on division of f by G no matter how the elements of
G are listed when using the division algorithm.
Proof. The division algorithm gives f = q1g1 + · · · + qtgt + r, where r satisﬁes
(i). We can also satisfy (ii) by setting g = q1g1 + · · · + qtgt ∈I. This proves the
existence of r.
To prove uniqueness, suppose f = g+r = g′+r′ satisfy (i) and (ii). Then r−r′ =
g′ −g ∈I, so that if r ̸= r′, then LT(r −r′) ∈⟨LT(I)⟩= ⟨LT(g1), . . . , LT(gt)⟩.
By Lemma 2 of §4, it follows that LT(r −r′) is divisible by some LT(gi). This is
impossible since no term of r, r′ is divisible by one of LT(g1), . . . , LT(gt). Thus r−r′
must be zero, and uniqueness is proved.
The ﬁnal part of the proposition follows from the uniqueness of r.
□
The remainder r is sometimes called the normal form of f, and its uniqueness
properties will be explored in Exercises 1 and 4. In fact, Gröbner bases can be char-
acterized by the uniqueness of the remainder—see Theorem 5.35 of BECKER and
WEISPFENNING (1993).
Although the remainder r is unique, even for a Gröbner basis, the "quotients" qi
produced by the division algorithm f = q1g1 + · · · + qtgt + r can change if we list
the generators in a different order. See Exercise 2 for an example.
As a corollary of Proposition 1, we get the following criterion for when a given
polynomial lies in an ideal.

84
Chapter 2 Gröbner Bases
Corollary 2. Let G = {g1, . . . , gt} be a Gröbner basis for an ideal I ⊆k[x1, . . . , xn]
and let f ∈k[x1, . . . , xn]. Then f ∈I if and only if the remainder on division of f by
G is zero.
Proof. If the remainder is zero, then we have already observed that f ∈I. Con-
versely, given f ∈I, then f = f + 0 satisﬁes the two conditions of Proposition 1. It
follows that 0 is the remainder of f on division by G.
□
The property given in Corollary 2 is sometimes taken as the deﬁnition of a Gröb-
ner basis, since one can show that it is true if and only if ⟨LT(g1), . . . , LT(gt)⟩=
⟨LT(I)⟩(see Exercise 3). For this and similar conditions equivalent to being a Gröb-
ner basis, see Proposition 5.38 of BECKER and WEISPFENNING (1993).
Using Corollary 2, we get an algorithm for solving the ideal membership problem
from §1, provided that we know a Gröbner basis G for the ideal in question—we
only need to compute a remainder with respect to G to determine whether f ∈I. In
§7, we will learn how to ﬁnd Gröbner bases, and we will give a complete solution
of the ideal membership problem in §8.
We will use the following notation for the remainder.
Deﬁnition 3. We will write f F for the remainder on division of f by the ordered
s-tuple F = ( f1, . . . , fs). If F is a Gröbner basis for ⟨f1, . . . , fs⟩, then we can regard
F as a set (without any particular order) by Proposition 1.
For instance, with F = (x2y −y2, x4y2 −y2) ⊆k[x, y], using the lex order, we
have
x5y
F = xy3
since the division algorithm yields
x5y = (x3 + xy)(x2y −y2) + 0 · (x4y2 −y2) + xy3.
We will next discuss how to tell whether a given generating set of an ideal is
a Gröbner basis. As we have indicated, the "obstruction" to {f1, . . . , fs} being a
Gröbner basis is the possible occurrence of polynomial combinations of the fi whose
leading terms are not in the ideal generated by the LT( fi). One way this can occur is
if the leading terms in a suitable combination
axαfi −bxβfj
cancel, leaving only smaller terms. On the other hand, axα fi −bxβ fj ∈I, so its
leading term is in ⟨LT(I)⟩. You should check that this is what happened in Example 2
of §5. To study this cancellation phenomenon, we introduce the following special
combinations.
Deﬁnition 4. Let f, g ∈k[x1, . . . , xn] be nonzero polynomials.
(i) If multideg( f) = α and multideg(g) = β, then let γ = (γ1, . . . , γn), where
γi = max(αi, βi) for each i. We call xγ the least common multiple of LM( f)
and LM(g), written xγ = lcm(LM( f), LM(g)).

§6 Properties of Gröbner Bases
85
(ii) The S-polynomial of f and g is the combination
S( f, g) =
xγ
LT( f) · f −
xγ
LT(g) · g.
(Note that we are inverting the leading coefﬁcients here as well.)
For example, let f = x3y2 −x2y3 + x and g = 3x4y + y2 in R[x, y] with the grlex
order. Then γ = (4, 2) and
S( f, g) = x4y2
x3y2 · f −x4y2
3x4y · g
= x · f −(1/3) · y · g
= −x3y3 + x2 −(1/3)y3.
An S-polynomial S( f, g) is "designed" to produce cancellation of leading terms.
See Exercise 7 for a precise description of the cancellation that occurs.
The following lemma shows that every cancellation of leading terms among
polynomials of the same multidegree comes from the cancellation that occurs for
S-polynomials.
Lemma 5. Suppose we have a sum s
i=1 pi, where multideg(pi) = δ ∈Zn
≥0 for
all i. If multideg(s
i=1 pi) < δ, then s
i=1 pi is a linear combination, with coef-
ﬁcients in k, of the S-polynomials S(pj, pl) for 1 ≤j, l ≤s. Furthermore, each
S(pj, pl) has multidegree < δ.
Proof. Let di = LC(pi), so that dixδ is the leading term of pi. Since the sum s
i=1 pi
has strictly smaller multidegree, it follows easily that s
i=1 di = 0.
Next observe that since pi and pj have the same leading monomial, their
S-polynomial reduces to
(1)
S(pi, pj) = 1
di
pi −1
dj
pj.
It follows that
(2)
s−1

i=1
diS(pi, ps) = d1
 1
d1
p1 −1
ds
ps

+ d2
 1
d2
p2 −1
ds
ps

+ · · ·
= p1 + p2 + · · · + ps−1 −1
ds
(d1 + · · · + ds−1)ps.
However, s
i=1 di = 0 implies d1 + · · · + ds−1 = −ds, so that (2) reduces to
s−1

i=1
diS(pi, ps) = p1 + · · · + ps−1 + ps.

86
Chapter 2 Gröbner Bases
Thus, s
i=1 pi is a sum of S-polynomials of the desired form, and equation (1) makes
it easy to see that S(pi, pj) has multidegree < δ. The lemma is proved.
□
When p1, . . . , ps satisfy the hypothesis of Lemma 5, we get an equation of the
form
s

i=1
pi =

j,l
cjlS(pj, pl).
Let us consider where the cancellation occurs. In the sum on the left, every summand
pi has multidegree δ, so the cancellation occurs only after adding them up. However,
in the sum on the right, each summand cjlS(pj, pl) has multidegree < δ, so that the
cancellation has already occurred. Intuitively, this means that all cancellation can be
accounted for by S-polynomials.
Using S-polynomials and Lemma 5, we can now prove the following criterion of
Buchberger for when a basis of an ideal is a Gröbner basis.
Theorem 6 (Buchberger's Criterion). Let I be a polynomial ideal. Then a basis
G = {g1, . . . , gt} of I is a Gröbner basis of I if and only if for all pairs i ̸= j, the
remainder on division of S(gi, gj) by G (listed in some order) is zero.
Proof. ⇒: If G is a Gröbner basis, then since S(gi, gj) ∈I, the remainder on division
by G is zero by Corollary 2.
⇐: Let f ∈I be nonzero. We will show that LT( f) ∈⟨LT(g1), . . . , LT(gt)⟩as
follows. Write
f =
t

i=1
higi,
hi ∈k[x1, . . . , xn].
From Lemma 8 of §2, it follows that
(3)
multideg( f) ≤max(multideg(higi) | higi ̸= 0).
The strategy of the proof is to pick the most efﬁcient representation of f, meaning
that among all expressions f = t
i=1 higi, we pick one for which
δ = max(multideg(higi) | higi ̸= 0)
is minimal. The minimal δ exists by the well-ordering property of our monomial
ordering. By (3), it follows that multideg( f) ≤δ.
If equality occurs, then multideg( f) = multideg(higi) for some i. This easily
implies that LT( f) is divisible by LT(gi). Then LT( f) ∈⟨LT(g1), . . . , LT(gt)⟩, which
is what we want to prove.
It remains to consider the case when the minimal δ satisﬁes multideg( f) < δ.
We will use S(gi, gj)
G = 0 for i ̸= j to ﬁnd a new expression for f that decreases δ.
This will contradict the minimality of δ and complete the proof.
Given an expression f = t
i=1 higi with minimal δ, we begin by isolating the
part of the sum where multidegree δ occurs:

§6 Properties of Gröbner Bases
87
(4)
f =

multideg(higi)=δ
higi +

multideg(higi)<δ
higi
=

multideg(higi)=δ
LT(hi)gi +

multideg(higi)=δ
(hi −LT(hi))gi +

multideg(higi)<δ
higi.
The monomials appearing in the second and third sums on the second line all have
multidegree < δ. Then multideg( f) < δ means that the ﬁrst sum on the second line
also has multidegree < δ.
The key to decreasing δ is to rewrite the ﬁrst sum in two stages: use Lemma 5
to rewrite the ﬁrst sum in terms of S-polynomials, and then use S(gi, gj)
G = 0 to
rewrite the S-polynomials without cancellation.
To express the ﬁrst sum on the second line of (4) using S-polynomials, note that
(5)

multideg(higi)=δ
LT(hi)gi
satisﬁes the hypothesis of Lemma 5 since each pi = LT(hi)gi has multidegree δ
and the sum has multidegree < δ. Hence, the ﬁrst sum is a linear combination with
coefﬁcients in k of the S-polynomials S(pi, pj). In Exercise 8, you will verify that
S(pi, pj) = xδ−γijS(gi, gj),
where xγij = lcm(LM(gi), LM(gj)). It follows that the ﬁrst sum (5) is a linear com-
bination of xδ−γijS(gi, gj) for certain pairs (i, j).
Consider one of these S-polynomials S(gi, gj). Since S(gi, gj)
G = 0, the division
algorithm (Theorem 3 of §3) gives an expression
(6)
S(gi, gj) =
t

l=1
Algl,
where Al ∈k[x1, . . . , xn] and
(7)
multideg(Algl) ≤multideg(S(gi, gj))
when Algl ̸= 0. Now multiply each side of (6) by xδ−γij to obtain
(8)
xδ−γijS(gi, gj) =
t

l=1
Blgl,
where Bl = xδ−γijAl. Then (7) implies that when Blgl ̸= 0, we have
(9)
multideg(Blgl) ≤multideg(xδ−γijS(gi, gj)) < δ
since LT(S(gi, gj)) < lcm(LM(gi), LM(gj)) = xγij by Exercise 7.

88
Chapter 2 Gröbner Bases
It follows that the ﬁrst sum (5) is a linear combination of certain xδ−γijS(gi, gj),
each of which satisﬁes (8) and (9). Hence we can write the ﬁrst sum as
(10)

multideg(higi)=δ
LT(hi)gi =
t

l=1
˜Blgl
with the property that when ˜Blgl ̸= 0, we have
(11)
multideg(˜Blgl) < δ.
Substituting (10) into the second line of (4) gives an expression for f as a polynomial
combination of the gi's where all terms have multidegree < δ. This contradicts the
minimality of δ and completes the proof of the theorem.
□
The Buchberger criterion given in Theorem 6 is one of the key results about
Gröbner bases. We have seen that Gröbner bases have many nice properties, but, so
far, it has been difﬁcult to determine if a basis of an ideal is a Gröbner basis (the
examples we gave in §5 were rather trivial). Using the Buchberger criterion, also
called the S-pair criterion, it is easy to show whether a given basis is a Gröbner
basis. Furthermore, in §7, we will see that the S-pair criterion also leads naturally to
an algorithm for computing Gröbner bases.
As an example of how to use Theorem 6, consider the ideal I = ⟨y −x2, z −x3⟩
of the twisted cubic in R3. We claim that G = {y −x2, z −x3} is a Gröbner basis
for lex order with y > z > x. To prove this, consider the S-polynomial
S(y −x2, z −x3) = yz
y (y −x2) −yz
z (z −x3) = −zx2 + yx3.
Using the division algorithm, one ﬁnds that
−zx2 + yx3 = x3 · (y −x2) + (−x2) · (z −x3) + 0,
so that S(y −x2, z −x3)
G = 0. Thus, by Theorem 6, G is a Gröbner basis for I.
You can also check that G is not a Gröbner basis for lex order with x > y > z (see
Exercise 9).
EXERCISES FOR §6
1. Show that Proposition 1 can be strengthened slightly as follows. Fix a monomial order-
ing and let I ⊆k[x1, . . . , xn] be an ideal. Suppose that f ∈k[x1, . . . , xn].
a. Show that f can be written in the form f = g + r, where g ∈I and no term of r is
divisible by any element of LT(I).
b. Given two expressions f = g + r = g′ + r′ as in part (a), prove that r = r′. Thus, r
and g are uniquely determined.
This result shows once a monomial order is ﬁxed, we can deﬁne a unique "remainder of
f on division by I." We will exploit this idea in Chapter 5.
2. In §5, we showed that G = {x + z, y −z} is a Gröbner basis for lex order. Let us use
this basis to study the uniqueness of the division algorithm.
a. Divide xy by x + z, y −z.

§6 Properties of Gröbner Bases
89
b. Now interchange the two polynomials and divide xy by y −z, x + z.
You should get the same remainder (as predicted by Proposition 1), but the "quotients"
should be different for the two divisions. This shows that the uniqueness of the remainder
is the best one can hope for.
3. In Corollary 2, we showed that if I = ⟨g1, . . . , gt⟩and if G = {g1, . . . , gt} is a Gröbner
basis for I, then f G = 0 for all f ∈I. Prove the converse of this statement. Namely, show
that if G is a basis for I with the property that f G = 0 for all f ∈I, then G is a Gröbner
basis for I.
4. Let G and G′ be Gröbner bases for an ideal I with respect to the same monomial order
in k[x1, . . . , xn]. Show that f G = f G′
for all f ∈k[x1, . . . , xn]. Hence, the remainder on
division by a Gröbner basis is even independent of which Gröbner basis we use, as long
as we use one particular monomial order. Hint: See Exercise 1.
5. Compute S( f , g) using the lex order.
a. f = 4x2z −7y2, g = xyz2 + 3xz4.
b. f = x4y −z2, g = 3xz2 −y.
c. f = x7y2z + 2ixyz, g = 2x7y2z + 4.
d. f = xy + z3, g = z2 −3z.
6. Does S( f , g) depend on which monomial order is used? Illustrate your assertion with
examples.
7. Prove that multideg(S( f , g)) < γ, where xγ = lcm(LM( f ), LM(g)). Explain why this
inequality is a precise version of the claim that S-polynomials are designed to produce
cancellation.
8. As in the proof of Theorem 6, suppose that cixα(i)gi and cjxα(j)gj have multidegree δ.
Prove that
S(xα(i)gi, xα(j)gj) = xδ−γijS(gi, gj),
where xγij = lcm(LM(gi), LM(gj)).
9. Show that {y −x2, z −x3} is not a Gröbner basis for lex order with x > y > z.
10. Using Theorem 6, determine whether the following sets G are Gröbner bases for the
ideal they generate. You may want to use a computer algebra system to compute the
S-polynomials and remainders.
a. G = {x2 −y, x3 −z} for grlex order.
b. G = {x2 −y, x3 −z} for invlex order (see Exercise 6 of §2).
c. G = {xy2 −xz + y, xy −z2, x −yz4} for lex order.
11. Let f , g ∈k[x1, . . . , xn] be polynomials such that LM( f ) and LM(g) are relatively prime
monomials and LC( f ) = LC(g) = 1. Assume that f or g has at least two terms.
a. Show that S( f , g) = −(g −LT(g))f + ( f −LT( f ))g.
b. Deduce that S( f , g) ̸= 0 and that the leading monomial of S( f , g) is a multiple of
either LM( f ) or LM(g) in this case.
12. Let f , g ∈k[x1, . . . , xn] be nonzero and xα, xβ be monomials. Verify that
S(xαf , xβg) = xγS( f , g)
where
xγ = lcm(xαLM( f ), xβ LM(g))
lcm(LM( f ), LM(g))
.
Be sure to prove that xγ is a monomial. Also explain how this relates to Exercise 8.
13. Let I ⊆k[x1, . . . , xn] be an ideal, and let G be a Gröbner basis of I.
a. Show that f G = gG if and only if f −g ∈I. Hint: See Exercise 1.
b. Use Exercise 1 to show that
f + gG = f G + gG.

90
Chapter 2 Gröbner Bases
c. Deduce that
fgG = f G · gG
G
.
We will return to an interesting consequence of these facts in Chapter 5.
§7
Buchberger's Algorithm
In Corollary 6 of §5, we saw that every ideal in k[x1, . . . , xn] has a Gröbner basis.
Unfortunately, the proof given was nonconstructive in the sense that it did not tell
us how to produce the Gröbner basis. So we now turn to the question: given an ideal
I ⊆k[x1, . . . , xn], how can we actually construct a Gröbner basis for I? To see the
main ideas behind the method we will use, we return to the ideal of Example 2 from
§5 and proceed as follows.
Example 1. Consider the ring Q[x, y] with grlex order, and let I = ⟨f1, f2⟩=
⟨x3 −2xy, x2y −2y2 + x⟩. Recall that {f1, f2} is not a Gröbner basis for I since
LT(S( f1, f2)) = −x2 /∈⟨LT( f1), LT( f2)⟩.
To produce a Gröbner basis, one natural idea is to try ﬁrst to extend the original
generating set to a Gröbner basis by adding more polynomials in I. In one sense,
this adds nothing new, and even introduces an element of redundancy. However, the
extra information we get from a Gröbner basis more than makes up for this.
What new generators should we add? By what we have said about the
S-polynomials in §6, the following should come as no surprise. We have S( f1, f2) =
−x2 ∈I, and its remainder on division by F = ( f1, f2) is −x2, which is nonzero.
Hence, we should include that remainder in our generating set, as a new generator
f3 = −x2. If we set F = ( f1, f2, f3), we can use Theorem 6 of §6 to test if this new
set is a Gröbner basis for I. We compute
S( f1, f2) = f3, so
S( f1, f2)
F = 0,
S( f1, f3) = (x3 −2xy) −(−x)(−x2) = −2xy, but
S( f1, f3)
F = −2xy ̸= 0.
Thus, we must add f4 = −2xy to our generating set. If we let F = ( f1, f2, f3, f4),
then by Exercise 12 we have
S( f1, f2)
F = S( f1, f3)
F = 0,
S( f1, f4) = y(x3 −2xy) −(−1/2)x2(−2xy) = −2xy2 = yf4, so
S( f1, f4)
F = 0,
S( f2, f3) = (x2y −2y2 + x) −(−y)(−x2) = −2y2 + x, but
S( f2, f3)
F = −2y2 + x ̸= 0.

§7 Buchberger's Algorithm
91
Hence, we must also add f5 = −2y2 + x to our generating set. Setting F =
( f1, f2, f3, f4, f5), one can compute that
S( fi, fj)
F = 0 for all 1 ≤i < j ≤5.
By Theorem 6 of §6, it follows that a grlex Gröbner basis for I is given by
{ f1, f2, f3, f4, f5} = {x3 −2xy, x2y −2y2 + x, −x2, −2xy, −2y2 + x}.
The above example suggests that in general, one should try to extend a basis F to
a Gröbner basis by successively adding nonzero remainders S( fi, fj)
F to F. This idea
is a natural consequence of the S-pair criterion from §6 and leads to the following
algorithm due to Buchberger for computing a Gröbner basis.
Theorem 2 (Buchberger's Algorithm). Let I = ⟨f1, . . . , fs⟩̸= {0} be a polyno-
mial ideal. Then a Gröbner basis for I can be constructed in a ﬁnite number of steps
by the following algorithm:
Input : F = ( f1, . . . , fs)
Output : a Gröbner basis G = (g1, . . . , gt) for I, with F ⊆G
G := F
REPEAT
G′ := G
FOR each pair {p, q}, p ̸= q in G′ DO
r := S(p, q)
G′
IF r ̸= 0 THEN G := G ∪{r}
UNTIL G = G′
RETURN G
Proof. We begin with some frequently used notation. If G = {g1, . . . , gt}, then ⟨G⟩
and ⟨LT(G)⟩will denote the following ideals:
⟨G⟩= ⟨g1, . . . , gt⟩,
⟨LT(G)⟩= ⟨LT(g1), . . . , LT(gt)⟩.
Turning to the proof of the theorem, we ﬁrst show that G ⊆I holds at every stage of
the algorithm. This is true initially, and whenever we enlarge G, we do so by adding
the remainder r = S(p, q)
G′
for p, q ∈G′ ⊆G. Thus, if G ⊆I, then p, q and, hence,
S(p, q) are in I, and since we are dividing by G′ ⊆I, we get G ∪{r} ⊆I. We also
note that G contains the given basis F of I, so that G is actually a basis of I.
The algorithm terminates when G = G′, which means that r = S(p, q)
G′
= 0 for
all p, q ∈G. Hence G is a Gröbner basis of ⟨G⟩= I by Theorem 6 of §6.

92
Chapter 2 Gröbner Bases
It remains to prove that the algorithm terminates.We need to consider what hap-
pens after each pass through the main loop. The set G consists of G′ (the old G)
together with the nonzero remainders of S-polynomials of elements of G′. Then
(1)
⟨LT(G′)⟩⊆⟨LT(G)⟩
since G′ ⊆G. Furthermore, if G′ ̸= G, we claim that ⟨LT(G′)⟩is strictly smaller
than ⟨LT(G)⟩. To see this, suppose that a nonzero remainder r of an S-polynomialhas
been adjoined to G. Since r is a remainder on division by G′, LT(r) is not divisible
by the leading terms of elements of G′, and thus LT(r) /∈⟨LT(G′)⟩by Lemma 2
of §4. Yet LT(r) ∈⟨LT(G)⟩, which proves our claim.
By (1), the ideals ⟨LT(G′)⟩from successive iterations of the loop form an ascend-
ing chain of ideals in k[x1, . . . , xn]. Thus, the ACC (Theorem 7 of §5) implies that af-
ter a ﬁnite number of iterations the chain will stabilize, so that ⟨LT(G′)⟩= ⟨LT(G)⟩
must happen eventually. By the previous paragraph, this implies that G′ = G, so
that the algorithm must terminate after a ﬁnite number of steps.
□
Taken together, the Buchberger criterion (Theorem 6 of §6) and the Buchberger
algorithm (Theorem 2 above) provide an algorithmic basis for the theory of Gröbner
bases. These contributions of Buchberger are central to the development of the sub-
ject. In §8, we will get our ﬁrst hints of what can be done with these methods, and a
large part of the rest of the book will be devoted to exploring their ramiﬁcations.
We should also point out the algorithm presented in Theorem 2 is only a rudi-
mentary version of the Buchberger algorithm. It was chosen for what we hope will
be its clarity for the reader, but it is not a very practical way to do the computation.
Note (as a ﬁrst improvement) that once a remainder S(p, q)
G′
= 0, that remainder
will stay zero even if we adjoin further elements to the generating set G′. Thus, there
is no reason to recompute those remainders on subsequent passes through the main
loop. Indeed, if we add our new generators fj one at a time, the only remainders that
need to be checked are S( fi, fj)
G′
, where i ≤j −1. It is a good exercise to revise
the algorithm to take this observation into account. Other improvements of a deeper
nature can also be made, but we will postpone considering them until §10.
Gröbner bases computed using the algorithm of Theorem 2 are often bigger than
necessary. We can eliminate some unneeded generators by using the following fact.
Lemma 3. Let G be a Gröbner basis of I ⊆k[x1, . . . , xn]. Let p ∈G be a polynomial
such that LT(p) ∈⟨LT(G \ {p})⟩. Then G \ {p} is also a Gröbner basis for I.
Proof. We know that ⟨LT(G)⟩= ⟨LT(I)⟩. If LT(p) ∈⟨LT(G \ {p})⟩, then we have
⟨LT(G \ {p})⟩= ⟨LT(G)⟩. By deﬁnition, it follows that G \ {p} is also a Gröbner
basis for I.
□
By adjusting constants to make all leading coefﬁcients equal to 1 and removing
any p with LT(p) ∈⟨LT(G \ {p})⟩from G, we arrive at what we will call a minimal
Gröbner basis. We can construct a minimal Gröbner basis for a given nonzero ideal
by applying the algorithm of Theorem 2 and then using Lemma 3 to eliminate any
unneeded generators that might have been included.

§7 Buchberger's Algorithm
93
To illustrate this procedure, we return to the ideal I studied in Example 1. Using
grlex order, we found the Gröbner basis
f1 = x3 −2xy,
f2 = x2y −2y2 + x,
f3 = −x2,
f4 = −2xy,
f5 = −2y2 + x.
Since some of the leading coefﬁcients are different from 1, the ﬁrst step is to
multiply the generators by suitable constants to make this true. Then note that
LT( f1) = x3 = −x · LT( f3). By Lemma 3, we can dispense with f1 in the mini-
mal Gröbner basis. Similarly, since LT( f2) = x2y = −(1/2)x · LT( f4), we can also
eliminate f2. There are no further cases where the leading term of a generator divides
the leading term of another generator. Hence,
˜f3 = x2,
˜f4 = xy,
˜f5 = y2 −(1/2)x
is a minimal Gröbner basis for I.
When G is a minimal Gröbner basis, the leading terms LT(p), p ∈G, form the
unique minimal basis of ⟨LT(I)⟩by Proposition 7 of §4 (see Exercise 6). Unfortu-
nately, the original ideal I may have many minimal Gröbner bases. For example, in
the ideal I considered above, it is easy to check that
(2)
ˆf3 = x2 + axy,
˜f4 = xy,
˜f5 = y2 −(1/2)x
is also a minimal Gröbner basis, where a ∈Q is any constant. Thus, we can produce
inﬁnitely many minimal Gröbner bases. Fortunately, we can single out one minimal
basis that is better than the others. The deﬁnition is as follows.
Deﬁnition 4. A reduced Gröbner basis for a polynomial ideal I is a Gröbner basis
G for I such that:
(i) LC(p) = 1 for all p ∈G.
(ii) For all p ∈G, no monomial of p lies in ⟨LT(G \ {p})⟩.
Note that for the Gröbner bases given in (2), only the one with a = 0 is reduced.
In general, reduced Gröbner bases have the following nice property.
Theorem 5. Let I ̸= {0} be a polynomial ideal. Then, for a given monomial order-
ing, I has a reduced Gröbner basis, and the reduced Gröbner basis is unique.
Proof. As noted above, all minimal Gröbner bases for I have the same leading
terms. Now let G be a minimal Gröbner basis for I. We say that g ∈G is fully
reduced for G provided that no monomial of g is in ⟨LT(G \ {p})⟩. Observe that g is
fully reduced for any other minimal Gröbner basis G′ of I that contains g since G′
and G have the same leading terms.

94
Chapter 2 Gröbner Bases
Next, given g ∈G, let g′ = gG\{g} and set G′ = (G \ {g}) ∪{g′}. We claim that
G′ is a minimal Gröbner basis for I. To see this, ﬁrst note that LT(g′) = LT(g), for
when we divide g by G \ {g}, LT(g) goes to the remainder since it is not divisible
by any element of LT(G \ {g}). This shows that ⟨LT(G′)⟩= ⟨LT(G)⟩. Since G′ is
clearly contained in I, we see that G′ is a Gröbner basis, and minimality follows.
Finally, note that g′ is fully reduced for G′ by construction.
Now, take the elements of G and apply the above process until they are all fully
reduced. The Gröbner basis may change each time we do the process, but our earlier
observation shows that once an element is fully reduced, it stays fully reduced since
we never change the leading terms. Thus, we end up with a reduced Gröbner basis.
Finally, to prove uniqueness, suppose that G and G are reduced Gröbner bases
for I. Then in particular, G and G are minimal Gröbner bases, and hence have the
same leading terms, i.e., LT(G) = LT( G). Thus, given g ∈G, there is ˜g ∈G such
that LT(g) = LT(˜g). If we can show that g = ˜g, it will follow that G = G, and
uniqueness will be proved.
To show g = ˜g, consider g −˜g. This is in I, and since G is a Gröbner basis,
it follows that g −˜gG = 0. But we also know LT(g) = LT(˜g). Hence, these terms
cancel in g −˜g, and the remaining terms are divisible by none of LT(G) = LT( G)
since G and G are reduced. This shows that g −˜gG = g −˜g, and then g −˜g = 0
follows. This completes the proof.
□
Many computer algebra systems implement a version of Buchberger's algo-
rithm for computing Gröbner bases. These systems always compute a Gröbner basis
whose elements are constant multiples of the elements in a reduced Gröbner basis.
This means that they will give essentially the same answers for a given problem.
Thus, answers can be easily checked from one system to the next.
Another consequence of the uniqueness in Proposition 5 is that we have an
ideal equality algorithm for seeing when two sets of polynomials { f1, . . . , fs} and
{g1, . . . , gt} generate the same ideal: simply ﬁx a monomial order and compute a
reduced Gröbner basis for ⟨f1, . . . , fs⟩and ⟨g1, . . . , gt⟩. Then the ideals are equal if
and only if the Gröbner bases are the same.
To conclude this section, we will indicate brieﬂy some of the connections be-
tween Buchberger's algorithm and the row-reduction (Gaussian elimination) al-
gorithm for systems of linear equations. The interesting fact here is that the row-
reduction algorithm is essentially a special case of the general algorithm we have
described. For concreteness, we will discuss the special case corresponding to the
system of linear equations
3x −6y −2z
= 0,
2x −4y
+ 4w = 0,
x −2y −
z −
w = 0.
If we use row operations on the coefﬁcient matrix to put it in row echelon form
(which means that the leading 1's have been identiﬁed), then we get the matrix

§7 Buchberger's Algorithm
95
(3)
⎛
⎝
1
−2
−1
−1
0
0
1
3
0
0
0
0
⎞
⎠.
To get a reduced row echelon matrix, we need to make sure that each leading 1 is
the only nonzero entry in its column. This leads to the matrix
(4)
⎛
⎝
1
−2
0
2
0
0
1
3
0
0
0
0
⎞
⎠.
To translate these computations into algebra, let I be the ideal
I = ⟨3x −6y −2z, 2x −4y + 4w, x −2y −z −w⟩⊆k[x, y, z, w]
corresponding to the original system of equations.We will use lex order with x >
y > z > w. Then, in the exercises, you will verify that the linear forms determined
by the row echelon matrix (3) give a minimal Gröbner basis
I = ⟨x −2y −z −w, z + 3w⟩,
and you will also check that the reduced row echelon matrix (4) gives the reduced
Gröbner basis
I = ⟨x −2y + 2w, z + 3w⟩.
Recall from linear algebra that every matrix can be put in reduced row echelon form
in a unique way. This can be viewed as a special case of the uniqueness of reduced
Gröbner bases.
In the exercises, you will examine the relation between Buchberger's algorithm
and the Euclidean Algorithm for ﬁnding the generator for the ideal ⟨f, g⟩⊆k[x].
EXERCISES FOR §7
1. Check that S( fi, fj)
F = 0 for all pairs 1 ≤i < j ≤5 in Example 1.
2. Use the algorithm given in Theorem 2 to ﬁnd a Gröbner basis for each of the following
ideals. You may wish to use a computer algebra system to compute the S-polynomials
and remainders. Use the lex, then the grlex order in each case, and then compare your
results.
a. I = ⟨x2y −1, xy2 −x⟩.
b. I = ⟨x2 + y, x4 + 2x2y + y2 + 3⟩. [What does your result indicate about the variety
V(I)?]
c. I = ⟨x −z4, y −z5⟩.
3. Find reduced Gröbner bases for the ideals in Exercise 2 with respect to lex and grlex.
4. Use the result of Exercise 7 of §4 to give an alternate proof that Buchberger's algorithm
will always terminate after a ﬁnite number of steps.
5. Let G be a Gröbner basis of an ideal I with the property that LC(g) = 1 for all g ∈G.
Prove that G is a minimal Gröbner basis if and only if no proper subset of G is a Gröbner
basis of I.

96
Chapter 2 Gröbner Bases
6. The minimal basis of a monomial ideal was introduced in Proposition 7 of §4. Show that
a Gröbner basis G of I is minimal if and only if LC(g) = 1 for all g ∈G and LT(G) is
the minimal basis of the monomial ideal ⟨LT(I)⟩.
7. Fix a monomial order, and let G and G be minimal Gröbner bases for the ideal I.
a. Prove that LT(G) = LT( G).
b. Conclude that G and G have the same number of elements.
8. Develop an algorithm that produces a reduced Gröbner basis (see Deﬁnition 4) for an
ideal I, given as input an arbitrary Gröbner basis for I. Prove that your algorithm works.
9. Consider the ideal
I = ⟨3x −6y −2z, 2x −4y + 4w, x −2y −z −w⟩⊆k[x, y, z, w]
mentioned in the text. We will use lex order with x > y > z > w.
a. Show that the linear polynomials determined by the row echelon matrix (3) give a
minimal Gröbner basis I = ⟨x −2y −z −w, z + 3w⟩. Hint: Use Theorem 6 of §6.
b. Show that the linear polynomials from the reduced row echelon matrix (4) give the
reduced Gröbner basis I = ⟨x −2y + 2w, z + 3w⟩.
10. Let A = (aij) be an n × m matrix with entries in k and let fi = ai1x1 + · · · + aimxm be the
linear polynomials in k[x1, . . . , xm] determined by the rows of A. Then we get the ideal
I = ⟨f1, . . . , fn⟩. We will use lex order with x1 > · · · > xm. Now let B = (bij) be the
reduced row echelon matrix determined by A and let g1, . . . , gt be the linear polynomials
coming from the nonzero rows of B (so that t ≤n). We want to prove that g1, . . . , gt
form the reduced Gröbner basis of I.
a. Show that I = ⟨g1, . . . , gt⟩. Hint: Show that the result of applying a row operation to
A gives a matrix whose rows generate the same ideal.
b. Use Theorem 6 of §6 to show that g1, . . . , gt form a Gröbner basis of I. Hint: If the
leading 1 in the ith row of B is in the sth column, we can write gi = xs + C, where C
is a linear polynomial involving none of the variables corresponding to leading 1's.
If gj = xt + D is written similarly, then you need to divide S(gi, gj) = xtC −xsD by
g1, . . . , gt. Note that you will use only gi and gj in the division.
c. Explain why g1, . . . , gt form the reduced Gröbner basis of I.
11. Show that the result of applying the Euclidean Algorithm in k[x] to any pair of polyno-
mials f , g is a reduced Gröbner basis for ⟨f , g⟩(after dividing by a constant to make the
leading coefﬁcient equal to 1). Explain how the steps of the Euclidean Algorithm can be
seen as special cases of the operations used in Buchberger's algorithm.
12. Fix F = {f1, . . . , fs} and let r = f F. Since dividing f by F gives r as remainder, adding
r to the polynomials we divide by should reduce the remainder to zero. In other words,
we should have f F∪{r} = 0 when r comes last. Prove this as follows.
a. When you divide f by F∪{r}, consider the ﬁrst place in the division algorithm where
the intermediate dividend p is not divisible by any LT( fi). Explain why LT(p) =
LT(r) and why the next intermediate dividend is p −r.
b. From here on in the division algorithm, explain why the leading term of the inter-
mediate dividend is always divisible by one of the LT( fi). Hint: If this were false,
consider the ﬁrst time it fails. Remember that the terms of r are not divisible by any
LT( fi).
c. Conclude that the remainder is zero, as desired.
d. (For readers who did Exercise 11 of §3.) Give an alternate proof of f F∪{r} = 0 using
Exercise 11 of §3.
13. In the discussion following the proof of Theorem 2, we commented that if S( f , g)
G′
= 0,
then remainder stays zero when we enlarge G′. More generally, if f F = 0 and F′ is
obtained from F by adding elements at the end, then f F′
= 0. Prove this.

§8 First Applications of Gröbner Bases
97
14. Suppose we have n points V = {(a1, b1), . . . , (an, bn)} ⊆k2 where a1, . . . , an are
distinct. This exercise will study the Lagrange interpolation polynomial deﬁned by
h(x) =
n

i=1
bi

j̸=i
xj −aj
ai −aj ∈k[x].
We will also explain how h(x) relates to the reduced Gröbner basis of I(V) ⊆k[x, y].
a. Show that h(ai) = bi for i = 1, . . . , n and explain why h has degree ≤n −1.
b. Prove that h(x) is the unique polynomial of degree ≤n −1 satisfying h(ai) = bi for
i = 1, . . . , n.
c. Prove that I(V) = ⟨f (x), y −h(x)⟩, where f (x) = n
i=1(x −ai). Hint: Divide
g ∈I(V) by f (x), y −h(x) using lex order with y > x.
d. Prove that {f (x), y −h(x)} is the reduced Gröbner basis for I(V) ⊆k[x, y] for lex
order with y > x.
§8
First Applications of Gröbner Bases
In §1, we posed four problems concerning ideals and varieties. The ﬁrst was the
ideal description problem, which was solved by the Hilbert Basis Theorem in §5.
Let us now consider the three remaining problems and see to what extent we can
solve them using Gröbner bases.
The Ideal Membership Problem
If we combine Gröbner bases with the division algorithm, we get the following ideal
membership algorithm: given an ideal I = ⟨f1, . . . , fs⟩, we can decide whether a
given polynomial f lies in I as follows. First, using a Gröbner basis algorithm (for
instance, the one in Theorem 2 of §7), ﬁnd a Gröbner basis G = {g1, . . . , gt} for I.
Then Corollary 2 of §6 implies that
f ∈I if and only if f G = 0.
Example 1. Let I = ⟨f1, f2⟩= ⟨xz−y2, x3 −z2⟩∈C[x, y, z], and use the grlex order.
Let f = −4x2y2z2 + y6 + 3z5. We want to know if f ∈I.
The generating set given is not a Gröbner basis of I because LT(I) also contains
polynomials such as LT(S( f1, f2)) = LT(−x2y2 + z3) = −x2y2 that are not in the
ideal ⟨LT( f1), LT( f2)⟩= ⟨xz, x3⟩. Hence, we begin by computing a Gröbner basis
for I. Using a computer algebra system, we ﬁnd a Gröbner basis
G = {f1, f2, f3, f4, f5} = {xz −y2, x3 −z2, x2y2 −z3, xy4 −z4, y6 −z5}.
Note that this is a reduced Gröbner basis.

98
Chapter 2 Gröbner Bases
We may now test polynomials for membership in I. For example, dividing f
above by G, we ﬁnd
f = (−4xy2z −4y4) · f1 + 0 · f2 + 0 · f3 + 0 · f4 + (−3) · f5 + 0.
Since the remainder is zero, we have f ∈I.
For another example, consider f = xy −5z2 + x. Even without completely
computing the remainder on division by G, we can see from the form of the ele-
ments in G that f /∈I. The reason is that LT( f) = xy is clearly not in the ideal
⟨LT(G)⟩= ⟨xz, x3, x2y2, xy4, y6⟩. Hence, f G ̸= 0, so that f /∈I.
This last observation illustrates the way the properties of an ideal are revealed by
the form of the elements of a Gröbner basis.
The Problem of Solving Polynomial Equations
Next, we will investigate how the Gröbner basis technique can be applied to solve
systems of polynomial equations in several variables. Let us begin by looking at
some speciﬁc examples.
Example 2. Consider the equations
x2 + y2 + z2 = 1,
x2 + z2 = y,
(1)
x = z
in C3. These equations determine I = ⟨x2+y2+z2−1, x2+z2−y, x−z⟩⊆C[x, y, z],
and we want to ﬁnd all points in V(I). Proposition 9 of §5 implies that we can
compute V(I) using any basis of I. So let us see what happens when we use a
Gröbner basis.
Though we have no compelling reason as of yet to do so, we will compute a
reduced Gröbner basis on I with respect to the lex order. The basis is
g1 = x −z,
g2 = y −2z2,
g3 = z4 + (1/2)z2 −1/4.
If we examine these polynomials closely, we ﬁnd something remarkable. First, the
polynomial g3 depends on z alone. To ﬁnd its roots, we solve for z2 by the quadratic
formula and take square roots. This gives four values of z:
z = ±1
2

±
√
5 −1.
Next, when these values of z are substituted into the equations g2 = 0 and g1 = 0,
those two equations can be solved uniquely for y and x, respectively. Thus, there are

§8 First Applications of Gröbner Bases
99
four solutions altogether of g1 = g2 = g3 = 0, two real and two complex. Since
V(I) = V(g1, g2, g3) by Proposition 9 of §5, we have found all solutions of the
original equations (1).
Example 3. Next, we will consider the system of polynomial equations (2) from
Chapter 1, §2, obtained by applying Lagrange multipliers to ﬁnd the minimum and
maximum values of x3 + 2xyz −z2 subject to the constraint x2 + y2 + z2 = 1:
3x2 + 2yz −2xλ = 0,
2xz −2yλ = 0,
2xy −2z −2zλ = 0,
x2 + y2 + z2 −1 = 0.
Again, we follow our general hunch and begin by computing a Gröbner basis for
the ideal in R[x, y, z, λ] generated by the left-hand sides of the four equations, using
the lex order with λ > x > y > z. We ﬁnd a Gröbner basis:
(2)
λ −3
2x −3
2yz −167616
3835 z6 + 36717
590 z4 −134419
7670 z2,
x2 + y2 + z2 −1,
xy −19584
3835 z5 + 1999
295 z3 −6403
3835z,
xz + yz2 −1152
3835z5 −108
295z3 + 2556
3835z,
y3 + yz2 −y −9216
3835z5 + 906
295z3 −2562
3835z,
y2z −6912
3835z5 + 827
295z3 −3839
3835z,
yz3 −yz −576
59 z6 + 1605
118 z4 −453
118z2,
z7 −1763
1152z5 + 655
1152z3 −11
288z.
At ﬁrst glance, this collection of polynomials looks horrendous. (The coefﬁcients
of the elements of Gröbner basis can be signiﬁcantly messier than the coefﬁcients of
the original generating set.) However, on further observation, we see that once again
the last polynomial depends only on the variable z. We have "eliminated" the other
variables in the process of ﬁnding the Gröbner basis. (Miraculously) the equation
obtained by setting this polynomial equal to zero has the roots
z = 0, ±1, ±2/3, ±
√
11/8
√
2.
If we set z equal to each of these values in turn, the remaining equations can then be
solved for y, x (and λ, though its values are essentially irrelevant for our purposes).
We obtain the following solutions:

100
Chapter 2 Gröbner Bases
z = 0;
y = 0;
x = ±1,
z = 0;
y = ±1;
x = 0,
z = ±1;
y = 0;
x = 0,
z = 2/3;
y = 1/3;
x = −2/3,
z = −2/3;
y = −1/3;
x = −2/3,
z =
√
11/8
√
2;
y = −3
√
11/8
√
2;
x = −3/8,
z = −
√
11/8
√
2;
y = 3
√
11/8
√
2;
x = −3/8.
From here, it is easy to determine the minimum and maximum values.
Examples 2 and 3 indicate that ﬁnding a Gröbner basis for an ideal with respect
to the lex order simpliﬁes the form of the equations considerably. In particular, we
seem to get equations where the variables are eliminated successively. Also, note
that the order of elimination seems to correspond to the ordering of the variables.
For instance, in Example 3, we had variables λ > x > y > z, and if you look back
at the Gröbner basis (2), you will see that λ is eliminated ﬁrst, x second, and so on.
A system of equations in this form is easy to solve, especially when the last
equation contains only one variable. We can apply one-variable techniques to try and
ﬁnd its roots, then substitute back into the other equations in the system and solve
for the other variables, using a procedure similar to the above examples. The reader
should note the analogy between this procedure for solving polynomial systems and
the method of "back-substitution" used to solve a linear system in triangular form.
We will study the process of elimination of variables from systems of polynomial
equations intensively in Chapter 3. In particular, we will see why lex order gives a
Gröbner basis that successively eliminates the variables.
The Implicitization Problem
Suppose that the parametric equations
(3)
x1 = f1(t1, . . . , tm),
...
xn = fn(t1, . . . , tm),
deﬁne a subset of an algebraic variety V in kn. For instance, this will always be
the case if the fi are rational functions in t1, . . . , tm, as we will show in Chapter 3.
How can we ﬁnd polynomial equations in the xi that deﬁne V? This problem can be
solved using Gröbner bases, though a complete proof that this is the case will come
only with the results of Chapter 3.
For simplicity, we will restrict our attention to the case where the fi are actually
polynomials. We begin with the afﬁne variety in km+n deﬁned by (3), namely
x1 −f1(t1, . . . , tm) = 0,
...
xn −fn(t1, . . . , tm) = 0.

§8 First Applications of Gröbner Bases
101
The basic idea is to eliminate the variables t1, . . . , tm from these equations. This
should give us the equations for V.
Given what we saw in Examples 2 and 3, it makes sense to use a Gröbner basis
to eliminate variables. We will take the lex order in k[t1, . . . , tm, x1, . . . , xn] deﬁned
by the variable ordering
t1 > · · · > tm > x1 > · · · > xn.
Now suppose we have a Gröbner basis of the ideal ˜I = ⟨x1 −f1, . . . , xn −fn⟩.
Since we are using lex order, we expect the Gröbner basis to have polynomials that
eliminate variables, and t1, . . . , tm should be eliminated ﬁrst since they are biggest
in our monomial order. Thus, the Gröbner basis for ˜I should contain polynomials
that only involve x1, . . . , xn. These are our candidates for the equations of V.
The ideas just described will be explored in detail when we study elimination
theory in Chapter 3. For now, we will content ourselves with some examples to see
how this process works.
Example 4. Consider the parametric curve V:
x = t4,
y = t3,
z = t2
in C3. We compute a Gröbner basis G of I = ⟨x −t4, y −t3, z −t2⟩with respect to
the lex order in C[t, x, y, z], and we ﬁnd
G = {t2 −z, ty −z2, tz −y, x −z2, y2 −z3}.
The last two polynomials depend only on x, y, z, so they deﬁne an afﬁne variety
of C3 containing our curve V. By the intuition on dimensions that we developed
in Chapter 1, we would guess that two equations in C3 would deﬁne a curve (a 1-
dimensional variety). The remaining question to answer is whether V is the entire
intersection of the two surfaces
x −z2 = 0,
y2 −z3 = 0.
Might there be other curves (or even surfaces) in the intersection? We will be able to
show that the answer is no when we have established the general results in Chapter 3.
Example 5. Now consider the tangent surface of the twisted cubic in R3, which we
studied in Chapter 1. This surface is parametrized by
x = t + u,
y = t2 + 2tu,
z = t3 + 3t2u.

102
Chapter 2 Gröbner Bases
We compute a Gröbner basis G for this ideal relative to the lex order deﬁned by
t > u > x > y > z, and we ﬁnd that G has 6 elements altogether. If you make the
calculation, you will see that only one contains only x, y, z terms:
(4)
x3z −(3/4)x2y2 −(3/2)xyz + y3 + (1/4)z2 = 0.
The variety deﬁned by this equation is a surface containing the tangent surface to
the twisted cubic. However, it is possible that the surface given by (4) is strictly
bigger than the tangent surface: there may be solutions of (4) that do not correspond
to points on the tangent surface. We will return to this example in Chapter 3.
To summarize our ﬁndings in this section, we have seen that Gröbner bases and
the division algorithm give a complete solution of the ideal membership problem.
Furthermore, we have seen ways to produce solutions of systems of polynomial
equations and to produce equations of parametrically given subsets of afﬁne space.
Our success in the examples given earlier depended on the fact that Gröbner bases,
when computed using lex order, seem to eliminate variables in a very nice fashion.
In Chapter 3, we will prove that this is always the case, and we will explore other
aspects of what is called elimination theory.
EXERCISES FOR §8
In the following exercises, a computer algebra system should be used to perform the necessary
calculations. (Most of the calculations would be very arduous if carried out by hand.)
1. Determine whether f = xy3 −z2 + y5 −z3 is in the ideal I = ⟨−x3 + y, x2y −z⟩.
2. Repeat Exercise 1 for f = x3z −2y2 and I = ⟨xz −y, xy + 2z2, y −z⟩.
3. By the method of Examples 2 and 3, ﬁnd the points in C3 on the variety
V(x2 + y2 + z2 −1, x2 + y2 + z2 −2x, 2x −3y −z).
4. Repeat Exercise 3 for V(x2y −z3, 2xy −4z −1, z −y2, x3 −4zy).
5. Recall from calculus that a critical point of a differentiable function f (x, y) is a point
where the partial derivatives
∂f
∂x and ∂f
∂y vanish simultaneously. When f ∈R[x, y], it
follows that the critical points can be found by applying our techniques to the system of
polynomial equations
∂f
∂x = ∂f
∂y = 0.
To see how this works, consider the function
f (x, y) = (x2 + y2 −4)(x2 + y2 −1) + (x −3/2)2 + (y −3/2)2.
a. Find all critical points of f (x, y).
b. Classify your critical points as local maxima, local minima, or saddle points. Hint:
Use the second derivative test.
6. Fill in the details of Example 5. In particular, compute the required Gröbner basis, and
verify that this gives us (up to a constant multiple) the polynomial appearing on the
left-hand side of equation (4).

§8 First Applications of Gröbner Bases
103
7. Let the surface S in R3 be formed by taking the union of the straight lines joining pairs
of points on the lines
⎧
⎨
⎩
x = t
y = 0
z = 1
⎫
⎬
⎭,
⎧
⎨
⎩
x = 0
y = 1
z = t
⎫
⎬
⎭
with the same parameter value (i.e., the same t). (This is a special example of a class of
surfaces called ruled surfaces.)
a. Show that the surface S can be given parametrically as
x = ut,
y = 1 −u,
z = t + u(1 −t).
b. Using the method of Examples 4 and 5, ﬁnd an (implicit) equation of a variety V
containing the surface S.
c. Show V = S (that is, show that every point of the variety V can be obtained by
substituting some values for t, u in the equations of part (a). Hint: Try to "solve" the
implicit equation of V for one variable as a function of the other two.
8. Some parametric curves and surfaces are algebraic varieties even when the given
parametrizations involve transcendental functions such as sin and cos. In this problem,
we will see that the parametric surface T,
x = (2 + cos(t)) cos(u),
y = (2 + cos(t)) sin(u),
z = sin(t),
lies on an afﬁne variety in R3.
a. Draw a picture of T. Hint: Use cylindrical coordinates.
b. Let a = cos(t), b = sin(t), c = cos(u), d = sin(u), and rewrite the above equations
as polynomial equations in a, b, c, d, x, y, z.
c. The pairs a, b and c, d in part (b) are not independent since there are additional poly-
nomial identities
a2 + b2 −1 = 0,
c2 + d2 −1 = 0
stemming from the basic trigonometric identity. Form a system of ﬁve equations by
adjoining the above equations to those from part (b) and compute a Gröbner basis for
the corresponding ideal. Use the lex monomial ordering and the variable order
a > b > c > d > x > y > z.
There should be exactly one polynomial in your basis that depends only on x, y, z.
This is the equation of a variety containing T.
9. Consider the parametric curve K ⊆R3 given by
x = (2 + cos(2s))cos(3s),
y = (2 + cos(2s))sin(3s),
z = sin(2s).
a. Express the equations of K as polynomial equations in x, y, z, a = cos(s), b = sin(s).
Hint: Trig identities.
b. By computing a Gröbner basis for the ideal generated by the equations from part (a)
and a2+b2−1 as in Exercise 8, show that K is (a subset of) an afﬁne algebraic curve.
Find implicit equations for a curve containing K.

104
Chapter 2 Gröbner Bases
c. Show that the equation of the surface from Exercise 8 is contained in the ideal gener-
ated by the equations from part (b). What does this result mean geometrically? (You
can actually reach the same conclusion by comparing the parametrizations of T and
K, without calculations.)
10. Use the method of Lagrange multipliers to ﬁnd the point(s) on the surface deﬁned by
x4 + y2 + z2 −1 = 0 that are closest to the point (1, 1, 1) in R3. Hint: Proceed as in
Example 3. (You may need to "fall back" on a numerical method to solve the equations
you get.)
11. Suppose we have numbers a, b, c which satisfy the equations
a + b + c = 3,
a2 + b2 + c2 = 5,
a3 + b3 + c3 = 7.
a. Prove that a4 + b4 + c4 = 9. Hint: Regard a, b, c as variables and show carefully that
a4 + b4 + c4 −9 ∈⟨a + b + c −3, a2 + b2 + c2 −5, a3 + b3 + c3 −7⟩.
b. Show that a5 + b5 + c5 ̸= 11.
c. What are a5 + b5 + c5 and a6 + b6 + c6? Hint: Compute remainders.
§9
Reﬁnements of the Buchberger Criterion
The Buchberger criterion (Theorem 6 of §6) states that a basis G = {g1, . . . , gt} of
a polynomial ideal is a Gröbner basis provided that S(gi, gj)
G = 0 for all gi, gj ∈G.
In other words, if each of these S-polynomials has a representation
S(gi, gj) =
t

l=1
qlgl + 0
produced by the division algorithm, then G is a Gröbner basis of the ideal it gener-
ates. The goal of this section is to give two versions of the Buchberger criterion that
allow more ﬂexibility in how the S-polynomials are represented.
Standard Representations
We ﬁrst give a more general view of what it means to have zero remainder. The
deﬁnition is as follows.
Deﬁnition 1. Fix a monomial order and let G = {g1, . . . , gt} ⊆k[x1, . . . , xn]. Given
f ∈k[x1, . . . , xn], we say that f reduces to zero modulo G, written
f →G 0,
if f has a standard representation
f = A1g1 + · · · + Atgt, Ai ∈k[x1, . . . , xn],

§9 Reﬁnements of the Buchberger Criterion
105
which means that whenever Aigi ̸= 0, we have
multideg( f) ≥multideg(Aigi).
To understand the relation between Deﬁnition 1 and the division algorithm, we
have the following lemma.
Lemma 2. Let G = (g1, . . . , gt) be an ordered set of elements of k[x1, . . . , xn] and
ﬁx f ∈k[x1, . . . , xn]. Then f G = 0 implies f →G 0, though the converse is false in
general.
Proof. If f G = 0, then the division algorithm implies
f = q1g1 + · · · + qtgt + 0,
and by Theorem 3 of §3, whenever qigi ̸= 0, we have
multideg( f) ≥multideg(qigi).
This shows that f →G 0. To see that the converse may fail, consider Example 5 from
§3. If we divide f = xy2 −x by G = (xy + 1, y2 −1), the division algorithm gives
xy2 −x = y · (xy + 1) + 0 · (y2 −1) + (−x −y),
so that f G = −x −y ̸= 0. Yet we can also write
xy2 −x = 0 · (xy + 1) + x · (y2 −1),
and since
multideg(xy2 −x) ≥multideg(x · (y2 −1))
(in fact, they are equal), it follows that f →G 0.
□
As an example of how Deﬁnition 1 can be used, let us state a more general version
of the Gröbner basis criterion from §6.
Theorem 3. A basis G = {g1, . . . , gt} for an ideal I is a Gröbner basis if and only
if S(gi, gj) →G 0 for all i ̸= j.
Proof. If G is a Gröbner basis, then S(gi, gj) ∈I has zero remainder on division by
G, hence S(gi, gj) →G 0 by Lemma 2. For the converse, Theorem 6 of §6 implies
that G is a Gröbner basis when S(gi, gj)
G = 0 for all i ̸= j. But if you examine the
proof, you will see that all we used was
S(gi, gj) =
t

l=1
Algl,

106
Chapter 2 Gröbner Bases
where
multideg(Algl) ≤multideg(S(gi, gj))
when Algl ̸= 0 (see (6) and (7) from §6). This is exactly what S(gi, gj) →G 0 means,
and the theorem follows.
□
By Lemma 2, notice that Theorem 6 of §6 is a special case of Theorem 3. Using
the notion of "standard representation" from Deﬁnition 1, Theorem 3 says that a
basis for an ideal I is a Gröbner basis if and only if all of its S-polynomials have
standard representations.
There are some situations where an S-polynomialis guaranteed to have a standard
representation.
Proposition 4. Given a ﬁnite set G ⊆k[x1, . . . , xn], suppose that we have f, g ∈G
such that the leading monomials of f and g are relatively prime. Then S( f, g) →G 0.
Proof. For simplicity, we assume that f, g have been multiplied by appropriate con-
stants to make LC( f) = LC(g) = 1. Write f = LM( f) + p, g = LM(g) + q.
Since LM( f) and LM(g) are relatively prime, we know that lcm(LM( f), LM(g)) =
LM( f) · LM(g). Hence, the S-polynomial S( f, g) can be written
(1)
S( f, g) = LM(g) · f −LM( f) · g
= (g −q) · f −( f −p) · g
= g · f −q · f −f · g + p · g
= p · g −q · f.
We claim that
(2)
multideg(S( f, g)) = max(multideg(p · g), multideg(q · f)).
Note that (1) and (2) imply S( f, g) →G 0 since f, g ∈G. To prove (2), observe that
in the last polynomial of (1), the leading monomials of p·g and q·f are distinct and,
hence, cannot cancel. For if the leading monomials were the same, we would have
LM(p) · LM(g) = LM(q) · LM( f).
However this is impossible if LM( f), LM(g) are relatively prime: from the last equa-
tion, LM(g) would have to divide LM(q), which is absurd since LM(g) > LM(q). □
For an example of how this proposition works, let G = (yz + y, x3 + y, z4) and
use grlex order on k[x, y, z]. Since x3 and z4 are relatively prime, we have
S(x3 + y, z4) →G 0
by Proposition 4. However, using the division algorithm, it is easy to check that
S(x3 + y, z4) = yz4 = (z3 −z2 + z −1)(yz + y) + 0 · (x3 + y) + 0 · z4 + y.

§9 Reﬁnements of the Buchberger Criterion
107
so that
S(x3 + y, z4)
G = y ̸= 0.
This explains why we need Deﬁnition 1: Proposition 4 is false if we use the notion
of zero remainder coming from the division algorithm.
Another example of Proposition 4 is given by the ideal I = ⟨y −x2, z −x3⟩. It
is easy to check that the given generators f = y −x2 and g = z −x3 do not form a
Gröbner basis for lex order with x > y > z. But if we switch to lex with z > y > x,
then the leading monomials are LM( f) = y and LM(g) = z. Setting G = {f, g},
Proposition 4 implies S( f, g) →G 0, so that G is a Gröbner basis of I by Theorem 3.
In §10, we see that Proposition 4 is part of a more efﬁcient version of the Buchberger
algorithm.
LCM Representations
Our second version of the Buchberger criterion allows a yet more general way of
presenting S-polynomials. Recall from Exercise 7 of §6 that an S-polynomialS( f, g)
has leading term that is guaranteed to be strictly less than lcm(LM( f), LM(g)).
Deﬁnition 5. Given nonzero polynomials F = ( f1, . . . , fs), we say that
S( fi, fj) =
s

l=1
Al fl
is an lcm representation provided that
lcm(LM( fi), LM( fj)) > LT(Al fl) whenever Al fl ̸= 0.
To understand how lcm representations relate to standard representations, write
S( fi, fj) = s
l=1 Al fl and take l with Al fl ̸= 0. Then consider the inequalities
lcm(LM( fi), LM( fj)) > LT(S( fi, fj)),
(3)
lcm(LM( fi), LM( fj)) > LT(Al fl).
(4)
Note that (3) is true by the deﬁnition of S-polynomial. In a standard representation,
we have (3) ⇒(4) since LT(S( fi, fj)) ≥LT(Al fl). In an lcm representation, on the
other hand, we have (4), but we make no assumption about how LT(S( fi, fj)) and
LT(Al fl) relate to each other.
The above discussion shows that every standard representation is also an lcm
representation. For an example of how the converse may fail, let f1 = xz + 1, f2 =
yz + 1, f3 = xz + y −z + 1. Using lex order with x > y > z, one can write
S( f1, f2) = (−1) · f1 + 0 · f2 + 1 · f3.
In Exercise 1, you will check that this is an lcm representation but not a standard
representation.

108
Chapter 2 Gröbner Bases
Here is a version of the Buchberger criterion that uses lcm representations.
Theorem 6. A basis G = (g1, . . . , gt) for an ideal I is a Gröbner basis if and only
if for every i ̸= j, the S-polynomial S(gi, gj) has an lcm representation.
Proof. If G is a Gröbner basis, then every S-polynomial has a standard representa-
tion, hence an lcm representation. For the converse, we will look closely at the proof
of Theorem 6 of §6, just as we did for Theorem 3.
We are assuming that S(gi, gj) has an lcm representation
S(gi, gj) =
t

l=1
Algl
with xγij > LT(Algl) when Algl ̸= 0. Here, xγij = lcm(LM(gj), LM(gl)). If we set
Bl = xδ−γijAl, then
xδ−γijS(gi, gj) =
t

l=1
Blgl,
where
multideg(Blgl) = multideg(xδ−γij) + multideg(Algl) < (δ −γij) + γij = δ.
This gives the same inequality as (9) in the proof of Theorem 6 of §6. From here,
the rest of the proof is identical to what we did in §6, and the theorem follows.
□
We noted above any standard representation of S(gi, gj) is an lcm representation.
Thus Theorem 6 of §6 and Theorem 3 of this section follow from Theorem 6, since
S(gi, gj) has an lcm representation whenever it satisﬁes either S(gi, gj)
G = 0 or
S(gi, gj) →G 0. We will consider a further generalization of the Buchberger criterion
in §10.
The ideas of this section are useful in elimination theory, which we will study
in Chapter 3. Two of the central results are the Extension Theorem and the Closure
Theorem. Standard representations appear in the proof of the Extension Theorem
given in Chapter 3, §5, and lcm representations are used in the proof of the Closure
Theorem given in Chapter 4, §7. We will also use Theorem 6 in the proof of the
Nullstellensatz given in Chapter 4, §1.
EXERCISES FOR §9
1. Let f1 = xz + 1, f2 = yz + 1, and f3 = xz + y −z + 1. For lex order with x > y > z, show
that
S( f1, f2) = (−1) · f1 + 0 · f2 + 1 · f3.
Also show that this is an lcm representation but not a standard representation.
2. Consider the ideal I = ⟨x2 + y + z −1, x + y2 + z −1, x + y + z2 −1⟩⊆Q[x, y, z].
a. Show that the generators of I fail to be Gröbner basis for any lex order.
b. Find a monomial order for which the leading terms of the generators are relatively
prime.

§10 Improvements on Buchberger's Algorithm
109
c. Explain why the generators automatically form a Gröbner basis for the monomial order
you found in part (b).
3. The result of the previous exercise can be generalized as follows. Suppose that I =
⟨f1, . . . , fs⟩where LM( fi) and LM( fj) are relatively prime for all indices i ̸= j. Prove
that {f1, . . . , fs} is a Gröbner basis of I.
§10
Improvements on Buchberger's Algorithm
In designing useful mathematical software, attention must be paid not only to the
correctness of the algorithms employed, but also to their efﬁciency. In this section,
we will discuss two improvements on the basic Buchberger algorithm for com-
puting Gröbner bases that can greatly speed up the calculations. Some version of
these improvements has been built into most of the computer algebra systems that
use Gröbner basis methods. The section will conclude with a brief discussion of
the complexity of computing Gröbner bases. This is still an active area of research
though, and there are as yet no deﬁnitive results in this direction.
The Buchberger algorithm presented in §7 computes remainders S( f, g)
G and
adds them to G when they are nonzero. As you learned from doing examples by
hand, these polynomial divisions are the most computationally intensive part of
Buchberger's algorithm. Hence, one way to improve the efﬁciency of the algorithm
would be to show that fewer S-polynomials S( f, g) need to be considered. Any re-
duction of the number of divisions that need to be performed is all to the good.
Theorem 3 of §9 tells us that when checking for a Gröbner basis, we can replace
S( f, g)
G = 0 with S( f, g) →G 0. Thus, if we can predict in advance that certain
S-polynomials are guaranteed to reduce to zero, then we can ignore them in the
Buchberger algorithm.
We have already seen one example where reduction to zero is guaranteed, namely
Proposition 4 of §9. This proposition is sufﬁciently important that we restate it here.
Proposition 1. Given a ﬁnite set G ⊆k[x1, . . . , xn], suppose that we have f, g ∈G
such that
lcm(LM( f), LM(g)) = LM( f) · LM(g).
This means that the leading monomials of f and g are relatively prime. Then
S( f, g) →G 0.
Note that Proposition 1 gives a more efﬁcient version of Theorem 3 of §9: to
test for a Gröbner basis, we need only have S(gi, gj) →G 0 for those i < j where
LM(gi) and LM(gj) are not relatively prime. But before we apply this to improving
Buchberger's algorithm, let us explore a second way to improve Theorem 3 of §9.
The basic idea is to better understand the role played by S-polynomials in the
proof of Theorem 6 of §6. Since S-polynomials were constructed to cancel leading
terms, this means we should study cancellation in greater generality. Hence, we will
introduce the notion of a syzygy on the leading terms of F = ( f1, . . . , fs). This word

110
Chapter 2 Gröbner Bases
is used in astronomy to indicate an alignment of three planets or other heavenly bod-
ies. The root is a Greek word meaning "yoke." In an astronomical syzygy, planets
are "yoked together"; in a mathematical syzygy, it is polynomials that are "yoked."
Deﬁnition 2. Let F = ( f1, . . . , fs). A syzygy on the leading terms LT( f1), . . . , LT( fs)
of F is an s-tuple of polynomials S = (h1, . . . , hs) ∈(k[x1, . . . , xn])s such that
s

i=1
hi · LT( fi) = 0.
We let S(F) be the subset of (k[x1, . . . , xn])s consisting of all syzygies on the leading
terms of F.
For an example of a syzygy, consider F = (x, x2 + z, y + z). Then using the lex
order, S = (−x + y, 1, −x) ∈(k[x, y, z])3 deﬁnes a syzygy in S(F) since
(−x + y) · LT(x) + 1 · LT(x2 + z) + (−x) · LT(y + z) = 0.
Let ei = (0, . . . , 0, 1, 0, . . ., 0) ∈(k[x1, . . . , xn])s, where the 1 is in the ith place.
Then a syzygy S ∈S(F) can be written as S = s
i=1 hiei. For an example of how
to use this notation, consider the syzygies that come from S-polynomials. Namely,
given a pair {fi, fj} ⊆F where i < j, let xγ = lcm(LM( fi), LM( fj)). Then
(1)
Sij =
xγ
LT( fi)ei −
xγ
LT( fj)ej
gives a syzygy on the leading terms of F. In fact, the name S-polynomial is actually
an abbreviation for "syzygy polynomial."
It is straightforward to check that the set of syzygies is closed under coordinate-
wise sums, and under coordinate-wise multiplication by polynomials (see Exer-
cise 1). An especially nice fact about S(F) is that it has a ﬁnite basis—there is a
ﬁnite collection of syzygies such that every other syzygy is a linear combination
with polynomial coefﬁcients of the basis syzygies.
However, before we can prove this, we need to learn a bit more about the structure
of S(F). We ﬁrst deﬁne the notion of a homogeneous syzygy.
Deﬁnition 3. An element S ∈S(F) is homogeneous of multidegree α, where α ∈
Zn
≥0, provided that
S = (c1xα(1), . . . , csxα(s)),
where ci ∈k and α(i) + multideg( fi) = α whenever ci ̸= 0.
You should check that the syzygy Sij given in (1) is homogeneous of multidegree
γ (see Exercise 4). We can decompose syzygies into homogeneous ones as follows.
Lemma 4. Every element of S(F) can be written uniquely as a sum of homogeneous
elements of S(F).

§10 Improvements on Buchberger's Algorithm
111
Proof. Let S = (h1, . . . , hs) ∈S(F). Fix an exponent α ∈Zn
≥0, and let hiα
be the term of hi (if any) such that hiα fi has multidegree α. Then we must have
s
i=1 hiαLT( fi) = 0 since the hiαLT( fi) are the terms of multidegree α in the sum
s
i=1 hiLT( fi) = 0. Then Sα = (h1α, . . . , hsα) is a homogeneous element of S(F)
of degree α and S = 
α Sα.
The proof of uniqueness will be left to the reader (see Exercise 5).
□
We can now prove that the Sij's form a basis of all syzygies on the leading terms.
Proposition 5. Given F = ( f1, . . . , fs), every syzygy S ∈S(F) can be written as
S =

i<j
uij Sij,
where uij ∈k[x1, . . . , xn] and the syzygy Sij is deﬁned as in (1).
Proof. By Lemma 4, we can assume that S is homogeneous of multidegree α. Then
S must have at least two nonzero components, say cixα(i) and cjxα(j), where i < j.
Then α(i) + multideg( fi) = α(j) + multideg( fj) = α, which implies that xγ =
lcm(LM( fi), LM( fj)) divides xα. Since
Sij =
xγ
LT( fi)ei −
xγ
LT( fj)ej,
an easy calculation shows that the ith component of
S −ciLC( fi)xα−γSij
must be zero, and the only other component affected is the jth. Hence we have
produced a homogeneous syzygy with fewer nonzero components. Since a nonzero
syzygy must have at least two nonzero components, continuing in this way will
eventually enable us to write S as a combination of the Sij's, and we are done.
□
This proposition explains our observation in §6 that S-polynomials account for
all possible cancellation of leading terms.
We are now ready to state a more reﬁned version of our algorithmic criterion for
Gröbner bases.
Theorem 6. A basis G = (g1, . . . , gt) for an ideal I is a Gröbner basis if and only if
for every element S = (H1, . . . , Ht) in a homogeneous basis for the syzygies S(G),
S · G = t
i=1 Higi can be written
(2)
S · G =
t

i=1
Aigi,
where the multidegree α of S satisﬁes
(3)
α > multideg(Aigi) whenever Aigi ̸= 0.

112
Chapter 2 Gröbner Bases
Proof. First assume that G is a Gröbner basis. Since S is a syzygy, it satisﬁes
α > multideg(S · G), and then any standard representation S · G = t
i=1 Aigi
has the desired property. For the converse, we will use the strategy (and nota-
tion) of the proof of Theorem 6 of §6. We start with f
= t
i=1 higi, where
δ = max(multideg(higi)) is minimal among all ways of writing f in terms of G.
As before, we need to show that multideg( f) < δ leads to a contradiction.
By (4) in §6, multideg( f) < δ implies that 
multideg(higi)=δ LT(hi)gi has strictly
smaller multidegree. This therefore means that 
multideg(higi)=δ LT(hi)LT(gi) = 0,
so that
S =

multideg(higi)=δ
LT(hi)ei
is a syzygy in S(G). Note also that S is homogeneous of multidegree δ. Our hypoth-
esis then gives us a homogeneous basis S1, . . . , Sm of S(G) with the nice property
that Sj · G satisﬁes (2) and (3) for all j. We can write S in the form
(4)
S = u1S1 + · · · + umSm.
By writing the uj's as sums of terms and expanding, we see that (4) expresses S as a
sum of homogeneous syzygies. Since S is homogeneous of multidegree δ, it follows
from the uniqueness of Lemma 4 that we can discard all syzygies not of multidegree
δ. Thus, in (4), we can assume that, for each j, either
uj = 0, or ujSj is homogeneous of multidegree δ.
Suppose that Sj has multidegree γj. If uj ̸= 0, then it follows that uj can be written
in the form uj = cjxδ−γj for some cj ∈k. Thus, (4) can be written
S =

j
cjxδ−γjSj,
where the sum is over those j's with uj ̸= 0. If we take the dot product of each side
with G, we obtain
(5)

multideg(higi)=δ
LT(hi)gi = S · G =

j
cjxδ−γjSj · G.
Since Sj has multidegree γj, our hypothesis implies that Sj · G = t
i=1 Aijgi, where
multideg(Aijgi) < γj when Aijgi ̸= 0.
It follows that if we set Bij = xδ−γjAij, then we have
xδ−γjSj · G =
t

i=1
Bijgi

§10 Improvements on Buchberger's Algorithm
113
where multideg(Bijgi) < δ when Bijgi ̸= 0. Using this and (5), we can write the sum

multideg(higi)=δ LT(hi)gi as

multideg(higi)=δ
LT(hi)gi =
t

l=1
˜Blgl,
where multideg(˜Blgl) < δ when ˜Blgl ̸= 0. This is exactly what we proved in (10)
and (11) from §6. From here, the remainder of the proof is identical to what we did
in §6. The theorem is proved.
□
Note that Theorem 3 of §9 is a special case of this result. Namely, if we use
the basis {Sij} for the syzygies S(G), then the polynomials Sij · G to be tested are
precisely the S-polynomials S(gi, gj).
A homogeneous syzygy S with the property that S · G →G 0 is easily seen to
satisfy (2) and (3) (Exercise 6). This gives the following corollary of Theorem 6.
Corollary 7. A basis G = (g1, . . . , gt) for an ideal I is a Gröbner basis if and only
if for every element S = (H1, . . . , Ht) in a homogeneous basis for the syzygies S(G),
S · G →G 0.
To exploit the power of Theorem 6 and Corollary 7, we need to learn how to make
small bases of S(G). For an example of how a basis can be smaller than expected,
consider G = (x2y2 + z, xy2 −y, x2y + yz) and use lex order in k[x, y, z]. The basis
formed by the three syzygies corresponding to the S-polynomials consists of
S12 = (1, −x, 0),
S13 = (1, 0, −y),
S23 = (0, x, −y).
However, we see that S23 = S13 −S12. Thus S23 is redundant in the sense that
it can be obtained from S12, S13 by a linear combination. (Here, the coefﬁcients are
constants; in general, relations between syzygies may have polynomial coefﬁcients.)
It follows that {S12, S13} is a smaller basis of S(G).
We will show next that starting with the basis {Sij | i < j}, there is a systematic
way to predict when elements can be omitted.
Proposition 8. Given G = (g1, . . . , gt), suppose that S ⊆{Sij | 1 ≤i < j ≤t} is
a basis of S(G). In addition, suppose we have distinct elements gi, gj, gl ∈G such
that
LT(gl) divides lcm(LT(gi), LT(gj)).
If Sil, Sjl ∈S, then S \ {Sij} is also a basis of S(G). (Note: If i > j, we set Sij = Sji.)
Proof. For simplicity, assume that i < j < l. Set xγij = lcm(LM(gi), LM(gj)) and let
xγil and xγjl be deﬁned similarly. Then our hypothesis implies that xγil and xγjl both
divide xγij . In Exercise 7, you will verify that

114
Chapter 2 Gröbner Bases
Sij = xγij
xγil Sil −xγij
xγjl Sjl,
and the proposition is proved.
□
To incorporate this proposition into an algorithm for creating Gröbner bases, we
will use the ordered pairs (i, j) with i < j to keep track of which syzygies we want.
Since we sometimes will have an i ̸= j where we do not know which is larger, we
will use the following notation: given i ̸= j, deﬁne
[i, j] =

(i, j)
if i < j
(j, i)
if i > j.
We can now state an improved version of Buchberger's algorithm that takes into
account the results proved so far.
Theorem 9. Let I = ⟨f1, . . . , fs⟩be a polynomial ideal. Then a Gröbner basis of I
can be constructed in a ﬁnite number of steps by the following algorithm:
Input : F = ( f1, . . . , fs)
Output : a Gröbner basis G for I = ⟨f1, . . . , fs⟩
B := {(i, j) | 1 ≤i < j ≤s}
G := F
t := s
WHILE B ̸= ∅DO
Select (i, j) ∈B
IF lcm(LT( fi), LT( fj)) ̸= LT( fi)LT( fj) AND
Criterion( fi, fj, B) = false THEN
r := S( fi, fj)
G
IF r ̸= 0 THEN
t := t + 1; ft := r
G := G ∪{ ft}
B := B ∪{(i, t) | 1 ≤i ≤t −1}
B := B \ {(i, j)}
RETURN G
Here, Criterion( fi, fj, B) is true provided that there is some l /∈{i, j} for which the
pairs [i, l] and [j, l] are not in B and LT( fl) divides lcm(LT( fi), LT( fj)). (Note that
this criterion is based on Proposition 8.)
Proof. The basic idea of the algorithm is that B records the pairs (i, j) that remain to
be considered. Furthermore, we only compute the remainder of those S-polynomials
S(gi, gj) for which neither Proposition 1 nor Proposition 8 applies.

§10 Improvements on Buchberger's Algorithm
115
To prove that the algorithm works, we ﬁrst observe that at every stage of the
algorithm, B has the property that if 1 ≤i < j ≤t and (i, j) /∈B, then
(6)
S( fi, fj) →G 0
or
Criterion( fi, fj, B) holds.
Initially, this is true since B starts off as the set of all possible pairs. We must show
that if (6) holds for some intermediate value of B, then it continues to hold when B
changes, say to B′.
To prove this, assume that (i, j) /∈B′. If (i, j) ∈B, then an examination of the
algorithm shows that B′ = B \ {(i, j)}. Now look at the step before we remove (i, j)
from B. If lcm(LT( fi)), LT( fj)) = LT( fi)LT( fj), then S( fi, fj) →G 0 by Proposi-
tion 1, and (6) holds. Also if Criterion( fi, fj, B) is true, then (6) clearly holds. Now
suppose that both of these fail. In this case, the algorithm computes the remainder
r = S( fi, fj)
G. If r = 0, then S( fi, fj) →G 0 by Lemma 2, as desired. Finally, if
r ̸= 0, then we enlarge G to be G′ = G ∪{r}, and we leave it as an exercise to show
that S( fi, fj) →G′ 0.
It remains to study the case when (i, j) /∈B. Here, (6) holds for B, and in Exer-
cise 9, you will show that this implies that (6) also holds for B′.
Next, we need to show that G is a Gröbner basis when B = ∅. To prove this, let
t be the length of G, and consider the set I consisting of all pairs (i, j) for 1 ≤i <
j ≤t where Criterion( fi, fj, B) was false when (i, j) was selected in the algorithm.
We claim that S = {Sij | (i, j) ∈I} is a basis of S(G) with the property that
Sij · G = S( fi, fj) →G 0 for all Sij ∈S. This claim and Corollary 7 will prove that G
is a Gröbner basis.
To prove our claim, note that B = ∅implies that (6) holds for all pairs (i, j) for
1 ≤i < j ≤t. It follows that S( fi, fj) →G 0 for all (i, j) ∈I. It remains to show
that S is a basis of S(G). To prove this, ﬁrst notice that we can order the pairs (i, j)
according to when they were removed from B in the algorithm (see Exercise 10
for the details of this ordering). Now go through the pairs in reverse order, starting
with the last removed, and delete the pairs (i, j) for which Criterion( fi, fj, B) was
true at that point in the algorithm. After going through all pairs, those that remain
are precisely the elements of I. Let us show that at every stage of this process, the
syzygies corresponding to the pairs (i, j) not yet deleted form a basis of S(G). This is
true initially because we started with all of the Sij's, which we know to be a basis.
Further, if at some point we delete (i, j), then the deﬁnition of Criterion( fi, fj, B)
implies that there is some l where LT( fl) satisﬁes the lcm condition and [i, l], [ j, l] /∈
B. Thus, [i, l] and [ j, l] were removed earlier from B, and hence Sil and Sjl are still
in the set we are creating because we are going in reverse order. If follows from
Proposition 8 that we still have a basis even after deleting Sij.
Finally, we need to show that the algorithm terminates. As in the proof of the
original algorithm (Theorem 2 of §7), G is always a basis of our ideal, and each
time we enlarge G, the monomial ideal ⟨LT(G)⟩gets strictly larger. By the ACC,
it follows that at some point, G must stop growing, and thus, we eventually stop
adding elements to B. Since every pass through the WHILE...DO loop removes an
element of B, we must eventually get B = ∅, and the algorithm comes to an end. □

116
Chapter 2 Gröbner Bases
The algorithm given above is still not optimal, and several strategies have been
found to improve its efﬁciency further. For example, in the division algorithm (Theo-
rem 3 of §3), we allowed the divisors f1, . . . , fs to be listed in any order. In practice,
some effort could be saved on average if we arranged the fi so that their leading
terms are listed in increasing order with respect to the chosen monomial ordering.
Since the smaller LT( fi) are more likely to be used during the division algorithm,
listing them earlier means that fewer comparisons will be required. A second strat-
egy concerns the step where we choose (i, j) ∈B in the algorithm of Theorem 9.
BUCHBERGER (1985) suggests that there will often be some savings if we pick
(i, j) ∈B such that lcm(LM( fi), LM( fj)) is as small as possible. The corresponding
S-polynomials will tend to yield any nonzero remainders (and new elements of the
Gröbner basis) sooner in the process, so there will be more of a chance that subse-
quent remainders S( fi, fj)
G will be zero. This normal selection strategy is discussed
in more detail in BECKER and WEISPFENNING (1993), BUCHBERGER (1985) and
GEBAUER and MÖLLER (1988). Finally, there is the idea of sugar, which is a re-
ﬁnement of the normal selection strategy. Sugar and its variant double sugar can be
found in GIOVINI, MORA, NIESI, ROBBIANO and TRAVERSO (1991).
In another direction, one can also modify the algorithm so that it will automati-
cally produce a reduced Gröbner basis (as deﬁned in §7). The basic idea is to sys-
tematically reduce G each time it is enlarged. Incorporating this idea also generally
lessens the number of S-polynomials that must be divided in the course of the algo-
rithm. For a further discussion of this idea, consult BUCHBERGER (1985).
We will discuss further ideas for computing Gröbner bases in Chapter 10.
Complexity Issues
We will end this section with a short discussion of the complexity of computing
Gröbner bases. Even with the best currently known versions of the algorithm, it is
still easy to generate examples of ideals for which the computation of a Gröbner
basis takes a tremendously long time and/or consumes a huge amount of storage
space. There are several reasons for this:
• The total degrees of intermediate polynomials that must be generated as the al-
gorithm proceeds can be quite large.
• The coefﬁcients of the elements of a Gröbner basis can be quite complicated
rational numbers, even when the coefﬁcients of the original ideal generators were
small integers. See Example 3 of §8 or Exercise 13 of this section for some
instances of this phenomenon.
For these reasons, a large amount of theoretical work has been done to try to es-
tablish uniform upper bounds on the degrees of the intermediate polynomials in
Gröbner basis calculations when the degrees of the original generators are given. For
some speciﬁc results in this area, see DUBÉ (1990) and MÖLLER and MORA (1984).
The idea is to measure to what extent the Gröbner basis method will continue to be
tractable as larger and larger problems are attacked.

§10 Improvements on Buchberger's Algorithm
117
The bounds on the degrees of the generators in a Gröbner basis are quite large,
and it has been shown that large bounds are necessary. For instance, MAYR and
MEYER (1982) give examples where the construction of a Gröbner basis for an
ideal generated by polynomials of degree less than or equal to some d can involve
polynomials of degree proportional to 22d. As d →∞, 22d grows very rapidly. Even
when grevlex order is used (which often gives the smallest Gröbner bases—see
below), the degrees can be quite large. For example, consider the polynomials
xn+1 −yzn−1w,
xyn−1 −zn,
xnz −ynw.
If we use grevlex order with x > y > z > w, then Mora [see LAZARD (1983)]
showed that the reduced Gröbner basis contains the polynomial
zn2+1 −yn2w.
The results led for a time to some pessimism concerning the ultimate practicality
of the Gröbner basis method as a whole. Further work has shown, however, that
for ideals in two and three variables, much more reasonable upper degree bounds
are available [see, for example, LAZARD (1983) and WINKLER (1984)]. Further-
more, in any case the running time and storage space required by the algorithm
seem to be much more manageable "on average" (and this tends to include most
cases of geometric interest) than in the worst cases. There is also a growing real-
ization that computing "algebraic" information (such as the primary decomposition
of an ideal—see Chapter 4) should have greater complexity than computing "geo-
metric" information (such as the dimension of a variety—see Chapter 9). A good
reference for this is GIUSTI and HEINTZ (1993), and a discussion of a wide variety
of complexity issues related to Gröbner bases can be found in BAYER and MUM-
FORD (1993). See also pages 616-619 of VON ZUR GATHEN and GERHARD (2013)
for further discussion and references.
Finally, experimentation with changes of variables and varying the ordering of
the variables often can reduce the difﬁculty of the computation drastically. BAYER
and STILLMAN (1987a) have shown that in most cases, the grevlex order should
produce a Gröbner basis with polynomials of the smallest total degree. In a different
direction, it is tempting to consider changing the monomial ordering as the algo-
rithm progresses in order to produce a Gröbner basis more efﬁciently. This idea was
introduced in GRITZMANN and STURMFELS (1993) and has been taken up again in
CABOARA and PERRY (2014).
EXERCISES FOR §10
1. Let S = (c1, . . . , cs) and T = (d1, . . . , ds) ∈(k[x1, . . . , xn])s be syzygies on the leading
terms of F = ( f1, . . . , fs).
a. Show that S + T = (c1 + d1, . . . , cs + ds) is also a syzygy.
b. Show that if g ∈k[x1, . . . , xn], then g · S = (gc1, . . . , gcs) is also a syzygy.
2. Given any G = (g1, . . . , gs) ∈(k[x1, . . . , xn])s , we can deﬁne a syzygy on G to be an s-
tuple S = (h1, . . . , hs) ∈(k[x1, . . . , xn])s such that 
i higi = 0. [Note that the syzygies
we studied in the text are syzygies on LT(G) = (LT(g1), . . . , LT(gs)).]

118
Chapter 2 Gröbner Bases
a. Show that if G = (x2 −y, xy −z, y2 −xz), then (z, −y, x) deﬁnes a syzygy on G.
b. Find another syzygy on G from part (a).
c. Show that if S, T are syzygies on G, and g ∈k[x1, . . . , xn], then S+T and gS are also
syzygies on G.
3. Let M be an m × (m + 1) matrix of polynomials in k[x1, . . . , xn]. Let I be the ideal gen-
erated by the determinants of all the m × m submatrices of M (such ideals are examples
of determinantal ideals).
a. Find a 2×3 matrix M such that the associated determinantal ideal of 2×2 submatrices
is the ideal with generators G as in part (a) of Exercise 2.
b. Explain the syzygy given in part (a) of Exercise 2 in terms of your matrix.
c. Give a general way to produce syzygies on the generators of a determinantal ideal.
Hint: Find ways to produce (m + 1) × (m + 1) matrices containing M, whose deter-
minants are automatically zero.
4. Prove that the syzygy Sij deﬁned in (1) is homogeneous of multidegree γ.
5. Complete the proof of Lemma 4 by showing that the decomposition into homogeneous
components is unique. Hint: First show that if S = 
α S′
α, where S′
α has multidegree
α, then, for a ﬁxed i, the ith components of the S′
α are either 0 or have multidegree equal
to α −multideg( fi) and, hence, give distinct terms as α varies.
6. Suppose that S is a homogeneous syzygy of multidegree α in S(G).
a. Prove that S · G has multidegree < α.
b. Use part (a) to show that Corollary 7 follows from Theorem 6.
7. Complete the proof of Proposition 8 by proving the formula expressing Sij in terms of
Sil and Sjl.
8. Let G be a ﬁnite subset of k[x1, . . . , xn] and let f ∈⟨G⟩. If f G = r ̸= 0, then show that
f →G′ 0, where G′ = G ∪{r}. This fact is used in the proof of Theorem 9.
9. In the proof of Theorem 9, we claimed that for every value of B, if 1 ≤i < j ≤t and
(i, j) /∈B, then condition (6) was true. To prove this, we needed to show that if the
claim held for B, then it held when B changed to some B′. The case when (i, j) /∈B′ but
(i, j) ∈B was covered in the text. It remains to consider when (i, j) /∈B′ ∪B. In this
case, prove that (6) holds for B′. Hint: Note that (6) holds for B. There are two cases
to consider, depending on whether B′ is bigger or smaller than B. In the latter situation,
B′ = B \ {(l, m)} for some (l, m) ̸= (i, j).
10. In this exercise, we will study the ordering on the set {(i, j) | 1 ≤i < j ≤t} described
in the proof of Theorem 9. Assume that B = ∅, and recall that t is the length of G when
the algorithm stops.
a. Show that any pair (i, j) with 1 ≤i < j ≤t was a member of B at some point during
the algorithm.
b. Use part (a) and B = ∅to explain how we can order the set of all pairs according to
when a pair was removed from B.
11. Consider f1 = x3 −2xy and f2 = x2y−2y2 +x and use grlex order on k[x, y]. These poly-
nomials are taken from Example 1 of §7, where we followed Buchberger's algorithm
to show how a Gröbner basis was produced. Redo this example using the algorithm of
Theorem 9 and, in particular, keep track of how many times you have to use the division
algorithm.
12. Consider the polynomials
xn+1 −yzn−1w,
xyn−1 −zn,
xnz −ynw,
and use grevlex order with x > y > z > w. Mora [see LAZARD (1983)] showed that the
reduced Gröbner basis contains the polynomial
zn2+1 −yn2w.
Prove that this is true when n is 3, 4, or 5. How big are the Gröbner bases?

§10 Improvements on Buchberger's Algorithm
119
13. In this exercise, we will look at some examples of how the term order can affect the
length of a Gröbner basis computation and the complexity of the answer.
a. Compute a Gröbner basis for I = ⟨x5 + y4 + z3 −1, x3 + y2 + z2 −1⟩using lex and
grevlex orders with x > y > z. You will see that the Gröbner basis is much simpler
when using grevlex.
b. Compute a Gröbner basis for I = ⟨x5 + y4 + z3 −1, x3 + y3 + z2 −1⟩using lex
and grevlex orders with x > y > z. This differs from the previous example by a
single exponent, but the Gröbner basis for lex order is signiﬁcantly nastier (one of its
polynomials has 282 terms, total degree 25, and a largest coefﬁcient of 170255391).
c. Let I = ⟨x4 −yz2w, xy2 −z3, x3z −y3w⟩be the ideal generated by the polynomials
of Exercise 12 with n = 3. Using lex and grevlex orders with x > y > z > w, show
that the resulting Gröbner bases are the same. So grevlex is not always better than
lex, but in practice, it is usually a good idea to use grevlex whenever possible.

Chapter 3
Elimination Theory
This chapter will study systematic methods for eliminating variables from systems
of polynomial equations. The basic strategy of elimination theory will be given in
two main theorems: the Elimination Theorem and the Extension Theorem. We will
prove these results using Gröbner bases and the classic theory of resultants. The
geometric interpretation of elimination will also be explored when we discuss the
Closure Theorem. Of the many applications of elimination theory, we will treat two
in detail: the implicitization problem and the envelope of a family of curves.
§1
The Elimination and Extension Theorems
To get a sense of how elimination works, let us look at an example similar to those
discussed at the end of Chapter 2. We will solve the system of equations
x2 + y + z = 1,
x + y2 + z = 1,
(1)
x + y + z2 = 1.
If we let I be the ideal
(2)
I = ⟨x2 + y + z −1, x + y2 + z −1, x + y + z2 −1⟩,
then a Gröbner basis for I with respect to lex order is given by the four polynomials
(3)
g1 = x + y + z2 −1,
g2 = y2 −y −z2 + z,
g3 = 2yz2 + z4 −z2,
g4 = z6 −4z4 + 4z3 −z2.
© Springer International Publishing Switzerland 2015
D.A. Cox et al., Ideals, Varieties, and Algorithms, Undergraduate Texts
in Mathematics, DOI 10.1007/978-3-319-16721-3_3
121

122
Chapter 3 Elimination Theory
It follows that equations (1) and (3) have the same solutions. However, since
g4 = z6 −4z4 + 4z3 −z2 = z2(z −1)2(z2 + 2z −1)
involves only z, we see that the possible z's are 0, 1, and −1±
√
2. Substituting these
values into g2 = y2 −y −z2 + z = 0 and g3 = 2yz2 + z4 −z2 = 0, we can determine
the possible y's, and then ﬁnally g1 = x + y + z2 −1 = 0 gives the corresponding
x's. In this way, one can check that equations (1) have exactly ﬁve solutions:
(1, 0, 0), (0, 1, 0), (0, 0, 1),
(−1 +
√
2, −1 +
√
2, −1 +
√
2),
(−1 −
√
2, −1 −
√
2, −1 −
√
2).
What enabled us to ﬁnd these solutions? There were two things that made our
success possible:
• (Elimination Step) We could ﬁnd a consequence g4 = z6 −4z4 + 4z3 −z2 = 0
of our original equations which involved only z, i.e., we eliminated x and y from
the system of equations.
• (Extension Step) Once we solved the simpler equation g4 = 0 to determine the
values of z, we could extend these solutions to solutions of the original equations.
The basic idea of elimination theory is that both the Elimination Step and the
Extension Step can be done in great generality.
To see how the Elimination Step works, notice that our observation concerning
g4 can be written as
g4 ∈I ∩C[z],
where I is the ideal given in equation (2). In fact, I ∩C[z] consists of all conse-
quences of our equations which eliminate x and y. Generalizing this idea leads to
the following deﬁnition.
Deﬁnition 1. Given I = ⟨f1, . . . , fs⟩⊆k[x1, . . . , xn], the l-th elimination ideal Il is
the ideal of k[xl+1, . . . , xn] deﬁned by
Il = I ∩k[xl+1, . . . , xn].
Thus, Il consists of all consequences of f1 = · · · = fs = 0 which elimi-
nate the variables x1, . . . , xl. In the exercises, you will verify that Il is an ideal of
k[xl+1, . . . , xn]. Note that I = I0 is the 0-th elimination ideal. Also observe that
different orderings of the variables lead to different elimination ideals.
Using this language, we see that eliminating x1, . . . , xl means ﬁnding nonzero
polynomials in the l-th elimination ideal Il. Thus a solution of the Elimination Step
means giving a systematic procedure for ﬁnding elements of Il. With the proper term
ordering, Gröbner bases allow us to do this instantly.
Theorem 2 (The Elimination Theorem). Let I ⊆k[x1, . . . , xn] be an ideal and let
G be a Gröbner basis of I with respect to lex order where x1 > x2 > · · · > xn. Then,
for every 0 ≤l ≤n, the set

§1 The Elimination and Extension Theorems
123
Gl = G ∩k[xl+1, . . . , xn]
is a Gröbner basis of the l-th elimination ideal Il.
Proof. Fix l between 0 and n. Since Gl ⊆Il by construction, it sufﬁces to show that
⟨LT(Il)⟩= ⟨LT(Gl)⟩
by the deﬁnition of Gröbner basis. One inclusion is obvious, and to prove the other
inclusion ⟨LT(Il)⟩⊆⟨LT(Gl)⟩, we need only to show that the leading term LT( f),
for an arbitrary f ∈Il, is divisible by LT(g) for some g ∈Gl.
To prove this, note that f also lies in I, which tells us that LT( f) is divisible by
LT(g) for some g ∈G since G is a Gröbner basis of I. Since f ∈Il, this means that
LT(g) involves only the variables xl+1, . . . , xn. Now comes the crucial observation:
since we are using lex order with x1 > · · · > xn, any monomial involving x1, . . . , xl
is greater than all monomials in k[xl+1, . . . , xn], so that LT(g) ∈k[xl+1, . . . , xn] im-
plies g ∈k[xl+1, . . . , xn]. This shows that g ∈Gl, and the theorem is proved.
□
For an example of how this theorem works, let us return to example (1) from the
beginning of the section. Here, I = ⟨x2 + y + z −1, x + y2 + z −1, x + y + z2 −1⟩,
and a Gröbner basis with respect to lex order is given in (3). It follows from the
Elimination Theorem that
I1 = I ∩C[y, z] = ⟨y2 −y −z2 + z, 2yz2 + z4 −z2, z6 −4z4 + 4z3 −z2⟩,
I2 = I ∩C[z] = ⟨z6 −4z4 + 4z3 −z2⟩.
Thus, g4 = z6 −4z4 + 4z3 −z2 is not just some random way of eliminating x and y
from our equations—it is the best possible way to do so since any other polynomial
that eliminates x and y is a multiple of g4.
The Elimination Theorem shows that a Gröbner basis for lex order eliminates not
only the ﬁrst variable, but also the ﬁrst two variables, the ﬁrst three variables, and
so on. In some cases (such as the implicitization problem to be studied in §3), we
only want to eliminate certain variables, and we do not care about the others. In such
a situation, it is a bit of overkill to compute a Gröbner basis using lex order. This
is especially true since lex order can lead to some very unpleasant Gröbner bases
(see Exercise 13 of Chapter 2, §10 for an example). In the exercises, you will study
versions of the Elimination Theorem that use more efﬁcient monomial orderings
than lex.
We next discuss the Extension Step. Suppose we have an ideal I ⊆k[x1, . . . , xn].
As in Chapter 2, we have the afﬁne variety
V(I) = {(a1, . . . , an) ∈kn | f(a1, . . . , an) = 0 for all f ∈I}.
To describe points of V(I), the basic idea is to build up solutions one coordinate
at a time. Fix some l between 1 and n. This gives us the elimination ideal Il, and
we will call a solution (al+1, . . . , an) ∈V(Il) a partial solution of the original
system of equations. To extend (al+1, . . . , an) to a complete solution in V(I), we

124
Chapter 3 Elimination Theory
ﬁrst need to add one more coordinate to the solution. This means ﬁnding al so that
(al, al+1, . . . , an) lies in the variety V(Il−1) of the next elimination ideal. More con-
cretely, suppose that Il−1 = ⟨g1, . . . , gr⟩in k[xl, xl+1, . . . , xn]. Then we want to ﬁnd
solutions xl = al of the equations
g1(xl, al+1, . . . , an) = · · · = gr(xl, al+1, . . . , an) = 0.
Here we are dealing with polynomials of one variable xl, and it follows that the
possible al's are just the roots of the gcd of the above r polynomials.
The basic problem is that the above polynomials may not have a common root,
i.e., there may be some partial solutions which do not extend to complete solutions.
For a simple example, consider the equations
(4)
xy = 1,
xz = 1.
Here, I = ⟨xy −1, xz −1⟩, and an easy application of the Elimination Theorem
shows that y −z generates the ﬁrst elimination ideal I1. Thus, the partial solutions
are given by (a, a), and these all extend to complete solutions (1/a, a, a) except for
the partial solution (0, 0). To see what is going on geometrically, note that y = z
deﬁnes a plane in 3-dimensional space. Then the variety (4) is a hyperbola lying in
this plane:
x
z
y
←the plane y = z
←the solutions
←the partial
solutions
It is clear that the variety deﬁned by (4) has no points lying over the partial
solution (0, 0). Pictures such as the one here will be studied in more detail in §2
when we study the geometric interpretation of eliminating variables. For now, our
goal is to see if we can determine in advance which partial solutions extend to
complete solutions.
Let us restrict our attention to the case where we eliminate just the ﬁrst variable
x1. Thus, we want to know if a partial solution (a2, . . . , an) ∈V(I1) can be extended
to a solution (a1, a2, . . . , an) ∈V(I). The following theorem tells us when this can
be done.

§1 The Elimination and Extension Theorems
125
Theorem 3 (The Extension Theorem). Let I = ⟨f1, . . . , fs⟩⊆C[x1, . . . , xn] and
let I1 be the ﬁrst elimination ideal of I. For each 1 ≤i ≤s, write fi in the form
fi = ci(x2, . . . , xn) xNi
1 + terms in which x1 has degree < Ni,
where Ni ≥0 and ci ∈C[x2, . . . , xn] is nonzero. Suppose that we have a partial
solution (a2, . . . , an) ∈V(I1). If (a2, . . . , an) /∈V(c1, . . . , cs), then there exists
a1 ∈C such that (a1, a2, . . . , an) ∈V(I).
We will give two proofs of this theorem, one using Gröbner bases in §5 and
the other using resultants in §6. For the rest of the section, we will explain the
Extension Theorem and discuss its consequences. A geometric interpretation will
be given in §2.
A ﬁrst observation is that the theorem is stated only for the ﬁeld k = C. To see
why C is important, assume that k = R and consider the equations
(5)
x2 = y,
x2 = z.
Eliminating x gives y = z, so that we get the partial solutions (a, a) for all a ∈R.
Since the leading coefﬁcients of x in x2 −y and x2 −z never vanish, the Extension
Theorem guarantees that (a, a) extends, provided we work over C. Over R, the
situation is different. Here, x2 = a has no real solutions when a is negative, so that
only those partial solutions with a ≥0 extend to real solutions of (5). This shows
that the Extension Theorem is false over R.
Turning to the hypothesis (a2, . . . , an) /∈V(c1, . . . , cs), note that the ci's are the
leading coefﬁcients with respect to x1 of the fi's. Thus, (a2, . . . , an) /∈V(c1, . . . , cs)
says that the leading coefﬁcients do not vanish simultaneously at the partial solution.
To see why this condition is necessary, let us look at example (4). Here, the equations
xy = 1,
xz = 1
have the partial solutions (y, z) = (a, a). The only one that does not extend is (0, 0),
which is the partial solution where the leading coefﬁcients y and z of x vanish. The
Extension Theorem tells us that the Extension Step can fail only when the leading
coefﬁcients vanish simultaneously.
Finally, we should mention that the variety V(c1, . . . , cs) where the leading
coefﬁcients vanish depends on the basis { f1, . . . , fs} of I: changing to a different
basis may cause V(c1, . . . , cs) to change. In Chapter 8, we will learn how to choose
( f1, . . . , fs) so that V(c1, . . . , cs) is as small as possible. We should also point out
that if one works in projective space (to be deﬁned in Chapter 8), then one can show
that all partial solutions extend.
Although the Extension Theorem is stated only for the case of eliminating the
ﬁrst variable x1, it can be used when eliminating any number of variables. For
example, consider the equations

126
Chapter 3 Elimination Theory
(6)
x2 + y2 + z2 = 1,
xyz = 1.
A Gröbner basis for I = ⟨x2 + y2 + z2 −1, xyz −1⟩with respect to lex order is
g1 = y4z2 + y2z4 −y2z2 + 1,
g2 = x + y3z + yz3 −yz.
By the Elimination Theorem, we obtain
I1 = I ∩C[y, z] = ⟨g1⟩,
I2 = I ∩C[z] = {0}.
Since I2 = {0}, we have V(I2) = C, and, thus, every c ∈C is a partial solution. So
we ask:
Which partial solutions c ∈C = V(I2) extend to (a, b, c) ∈V(I)?
The idea is to extend c one coordinate at a time: ﬁrst to (b, c), then to (a, b, c). To
control which solutions extend, we will use the Extension Theorem at each step.
The crucial observation is that I2 is the ﬁrst elimination ideal of I1. This is easy
to see here, and the general case is covered in the exercises. Thus, we will use the
Extension Theorem once to go from c ∈V(I2) to (b, c) ∈V(I1), and a second time
to go to (a, b, c) ∈V(I). This will tell us exactly which c's extend.
To start, we apply the Extension Theorem to go from I2 to I1 = ⟨g1⟩. The coefﬁ-
cient of y4 in g1 is z2, so that c ∈C = V(I2) extends to (b, c) whenever c ̸= 0. Note
also that g1 = 0 has no solution when c = 0. The next step is to go from I1 to I;
that is, to ﬁnd a so that (a, b, c) ∈V(I ). If we substitute (y, z) = (b, c) into (6), we
get two equations in x, and it is not obvious that there is a common solution x = a.
This is where the Extension Theorem shows its power. The leading coefﬁcients of x
in x2 + y2 + z2 −1 and xyz −1 are 1 and yz, respectively. Since 1 never vanishes,
the Extension Theorem guarantees that a always exists. We have thus proved that
all partial solutions c ̸= 0 extend to V(I).
The Extension Theorem is especially easy to use when one of the leading coef-
ﬁcients is constant. This case is sufﬁciently useful that we will state it as a separate
corollary.
Corollary 4. Let I = ⟨f1, . . . , fs⟩⊆C[x1, . . . , xn], and assume that for some i, fi is
of the form
fi = cixNi
1 + terms in which x1 has degree < Ni,
where ci ∈C is nonzero and Ni > 0. If I1 is the ﬁrst elimination ideal of I and
(a2, . . . , an) ∈V(I1), then there is a1 ∈C such that (a1, a2, . . . , an) ∈V(I).
Proof. This follows immediately from the Extension Theorem: since ci ̸= 0 in
C implies V(c1, . . . , cs) = ∅, we have (a2, . . . , an) /∈V(c1, . . . , cs) for all partial
solutions (a2, . . . , an).
□

§1 The Elimination and Extension Theorems
127
We will end this section with an example of a system of equations that does not
have nice solutions. Consider the equations
xy = 4,
y2 = x3 −1.
Using lex order with x > y, the Gröbner basis is given by
g1 = 16x −y2 −y4,
g2 = y5 + y3 −64,
but if we proceed as usual, we discover that y5+y3−64 has no rational roots (in fact,
it is irreducible over Q, a concept we will discuss in Chapter 4, §2). One option is to
compute the roots numerically. A variety of methods (such as the Newton-Raphson
method) are available, and for y5 + y3 −64 = 0, one obtains
y = 2.21363, −1.78719 ± 1.3984i, or 0.680372 ± 2.26969i.
These solutions can then be substituted into g1 = 16x −y2 −y4 = 0 to determine
the values of x. Thus, unlike the previous examples, we can only ﬁnd numerical
approximations to the solutions. See VON ZUR GATHEN and GERHARD (2013) for
an introduction to ﬁnding the roots of a polynomial of one variable.
There are many interesting problems that arise when one tries to ﬁnd numerical
solutions of systems of polynomial equations. For further reading on this topic, we
recommend LAZARD (1993) and MANOCHA (1994). The reader may also wish to
consult COX, LITTLE and O'SHEA (2005) and DICKENSTEIN and EMIRIS (2005).
EXERCISES FOR §1
1. Let I ⊆k[x1, . . . , xn] be an ideal.
a. Prove that Il = I ∩k[xl+1, . . . , xn] is an ideal of k[xl+1, . . . , xn].
b. Prove that the ideal Il+1 ⊆k[xl+2, . . . , xn] is the ﬁrst elimination ideal of Il ⊆
k[xl+1, . . . , xn]. This observation allows us to use the Extension Theorem multiple
times when eliminating more than one variable.
2. Consider the system of equations
x2 + 2y2 = 3,
x2 + xy + y2 = 3.
a. If I is the ideal generated by these equations, ﬁnd bases of I ∩k[x] and I ∩k[y].
b. Find all solutions of the equations.
c. Which of the solutions are rational, i.e., lie in Q2?
d. What is the smallest ﬁeld k containing Q such that all solutions lie in k2?
3. Determine all solutions (x, y) ∈Q2 of the system of equations
x2 + 2y2 = 2,
x2 + xy + y2 = 2.
Also determine all solutions in C2.

128
Chapter 3 Elimination Theory
4. Find bases for the elimination ideals I1 and I2 for the ideal I determined by the equations:
x2 + y2 + z2 = 4,
x2 + 2y2 = 5,
xz = 1.
How many rational (i.e., in Q3) solutions are there?
5. In this exercise, we will prove a more general version of the Elimination Theorem. Fix an
integer 1 ≤l ≤n. We say that a monomial order > on k[x1, . . . , xn] is of l-elimination
type provided that any monomial involving one of x1, . . . , xl is greater than all monomials
in k[xl+1, . . . , xn]. Prove the following generalized Elimination Theorem. If I is an ideal
in k[x1, . . . , xn] and G is a Gröbner basis of I with respect to a monomial order of l-
elimination type, then G ∩k[xl+1, . . . , xn] is a Gröbner basis of the l-th elimination ideal
Il = I ∩k[xl+1, . . . , xn].
6. To exploit the generalized Elimination Theorem of Exercise 5, we need some interesting
examples of monomial orders of l-elimination type. We will consider two such orders.
a. Fix an integer 1 ≤l ≤n, and deﬁne the order >l as follows: if α, β ∈Zn
≥0, then
α >l β if
α1 + · · · + αl > β1 + · · · + βl, or α1 + · · · + αl = β1 + · · · + βl and α >grevlex β.
This is the l-th elimination order of BAYER and STILLMAN (1987b). Prove that >l is a
monomial order and is of l-elimination type. Hint: If you did Exercise 11 of Chapter 2,
§4, then you have already done this problem.
b. In Exercise 9 of Chapter 2, §4, we considered an example of a product order that mixed
lex and grlex orders on different sets of variables. Explain how to create a product order
that induces grevlex on both k[x1, . . . , xl] and k[xl+1, . . . , xn] and show that this order
is of l-elimination type.
c. If G is a Gröbner basis for I ⊆k[x1, . . . , xn] for either of the monomial orders of parts
(a) or (b), explain why G ∩k[xl+1, . . . , xn] is a Gröbner basis with respect to grevlex.
7. Consider the equations
t2 + x2 + y2 + z2 = 0,
t2 + 2x2 −xy −z2 = 0,
t + y3 −z3 = 0.
We want to eliminate t. Let I = ⟨t2 + x2 + y2 + z2, t2 + 2x2 −xy −z2, t + y3 −z3⟩be the
corresponding ideal.
a. Using lex order with t > x > y > z, compute a Gröbner basis for I, and then ﬁnd a ba-
sis for I ∩Q[x, y, z]. You should get four generators, one of which has total degree 12.
b. Compute a grevlex Gröbner basis for I ∩Q[x, y, z]. You will get a simpler set of two
generators.
c. Combine the answer to part (b) with the polynomial t + y3 −z3 and show that this
gives a Gröbner basis for I with respect to the elimination order >1 (this is >l with
l = 1) of Exercise 6. Note that this Gröbner basis is much simpler than the one found
in part (a). If you have access to a computer algebra system that knows elimination
orders, then check your answer.
8. In equation (6), we showed that z ̸= 0 could be speciﬁed arbitrarily. Hence, z can be
regarded as a "parameter." To emphasize this point, show that there are formulas for x and
y in terms of z. Hint: Use g1 and the quadratic formula to get y in terms of z. Then use xyz =
1 to get x. The formulas you obtain give a "parametrization" of V(I) which is different
from those studied in §3 of Chapter 1. Namely, in Chapter 1, we used parametrizations by

§2 The Geometry of Elimination
129
rational functions, whereas here, we have what is called a parametrization by algebraic
functions. Note that x and y are not uniquely determined by z.
9. Consider the system of equations given by
x5 + 1
x5 = y,
x + 1
x = z.
Let I be the ideal in C[x, y, z] determined by these equations.
a. Find a basis of I1 ⊆C[y, z] and show that I2 = {0}.
b. Use the Extension Theorem to prove that each partial solution c ∈V(I2) = C extends
to a solution in V(I) ⊆C3.
c. Which partial solutions (b, c) ∈V(I1) ⊆R2 extend to solutions in V(I) ⊆R3?
Explain why your answer does not contradict the Extension Theorem.
d. If we regard z as a "parameter" (see the previous problem), then solve for x and y as
algebraic functions of z to obtain a "parametrization" of V(I).
§2
The Geometry of Elimination
In this section, we will give a geometric interpretation of the theorems from §1.
The main idea is that elimination corresponds to projecting a variety onto a lower
dimensional subspace. We will also discuss the Closure Theorem, which describes
the relation between partial solutions and elimination ideals. For simplicity, we will
work over the ﬁeld k = C.
Let us start by deﬁning the projection of an afﬁne variety. Suppose that we are
given V = V( f1, . . . , fs) ⊆Cn. To eliminate the ﬁrst l variables x1, . . . , xl, we will
consider the projection map
πl : Cn −→Cn−l
which sends (a1, . . . , an) to (al+1, . . . , an). If we apply πl to V ⊆Cn, then we get
πl(V) ⊆Cn−l. We can relate πl(V) to the l-th elimination ideal as follows.
Lemma 1. With the above notation, let Il = ⟨f1, . . . , fs⟩∩C[xl+1, . . . , xn] be the
l-th elimination ideal. Then, in Cn−l, we have
πl(V) ⊆V(Il).
Proof. Fix a polynomial f ∈Il. If (a1, . . . , an) ∈V, then f vanishes at (a1, . . . , an)
since f ∈⟨f1, . . . , fs⟩. But f involves only xl+1, . . . , xn, so that we can write
f(al+1, . . . , an) = f(πl(a1, . . . , an)) = 0.
This shows that f vanishes at all points of πl(V).
□
As in §1, points of V(Il) will be called partial solutions. Using the lemma, we
can write πl(V) as follows:

130
Chapter 3 Elimination Theory
πl(V) = {(al+1, . . . , an) ∈V(Il) | there exist a1, . . . , al ∈C such
that (a1, . . . , al, al+1, . . . , an) ∈V}.
Thus, πl(V) consists exactly of the partial solutions that extend to complete solu-
tions. For an example, consider the variety V deﬁned by equations (4) from §1:
(1)
xy = 1,
xz = 1.
Here, we have the following picture that simultaneously shows the solutions and the
partial solutions:
x
z
y
←the plane y = z
←the solutions
←the partial
solutions
↓
↓
↑
↑
the arrows ↑, ↓
indicate the
projection π1
Note that V(I1) is the line y = z in the (y, z)-plane, and that
π1(V) = {(a, a) ∈C2 | a ̸= 0}.
In particular, π1(V) is not an afﬁne variety—it is missing the point (0, 0).
The basic tool to understand the missing points is the Extension Theorem from
§1. It only deals with π1 (i.e., eliminating x1), but gives us a good picture of what
happens in this case. Stated geometrically, here is what the Extension Theorem says.
Theorem 2 (The Geometric Extension Theorem). Given V = V( f1, . . . , fs) ⊆
Cn, let ci ∈C[x2, . . . , xn] be as in the Extension Theorem from §1. If I1 is the ﬁrst
elimination ideal of ⟨f1, . . . , fs⟩, then we have the equality in Cn−1
V(I1) = π1(V) ∪(V(c1, . . . , cs) ∩V(I1)),
where π1 : Cn →Cn−1 is projection onto the last n −1 coordinates.
Proof. The proof follows from Lemma 1 and the Extension Theorem. The details
will be left as an exercise.
□

§2 The Geometry of Elimination
131
This theorem tells us that π1(V) ﬁlls up the afﬁne variety V(I1), except possibly
for a part that lies in V(c1, . . . , cs). Unfortunately, it is not clear how big this part is,
and sometimes V(c1, . . . , cs) is unnaturally large. For example, one can show that
the equations
(2)
(y −z)x2 + xy = 1,
(y −z)x2 + xz = 1
generate the same ideal as equations (1). Since c1 = c2 = y −z generate the elimi-
nation ideal I1, the Geometric Extension Theorem tells us nothing about the size of
π1(V) in this case.
Nevertheless, we can still make the following strong statements about the relation
between πl(V) and V(Il).
Theorem 3 (The Closure Theorem). Let V = V( f1, . . . , fs) ⊆Cn and let Il be the
l-th elimination ideal of ⟨f1, . . . , fs⟩. Then:
(i) V(Il) is the smallest afﬁne variety containing πl(V) ⊆Cn−l.
(ii) When V ̸= ∅, there is an afﬁne variety W ⊊V(Il) such that V(Il) \ W ⊆πl(V).
When we say "smallest variety" in part (i), we mean "smallest with respect to
set-theoretic inclusion." Thus, V(Il) being smallest means two things:
• πl(V) ⊆V(Il)
• If Z is any other afﬁne variety in Cn−l containing πl(V), then V(Il) ⊆Z.
In Chapter 4, we will express this by saying that V(Il) is the Zariski closure of
πl(V). This is where the theorem gets its name. Part (ii) of the theorem says that
although πl(V) may not equal V(Il), it ﬁlls up "most" of V(Il) in the sense that what
is missing lies in a strictly smaller afﬁne variety.
We cannot yet prove the Closure Theorem, for it requires the Nullstellensatz and
other tools from Chapter 4. The proof will be deferred until then. We will also say
more about the variety W ⊊V(Il) of part (ii) in Chapter 4.
The Closure Theorem gives us a partial description of πl(V) since it ﬁlls up
V(Il), except for some missing points that lie in a variety strictly smaller than V(Il).
Unfortunately, the missing points might not ﬁll up all of the smaller variety. The
precise structure of πl(V) can be described as follows: there are afﬁne varieties
Zi ⊆Wi ⊆Cn−l for 1 ≤i ≤m such that
πl(V) =
m

i=1
(Wi \ Zi).
In general, a set of this form is called constructible. We will prove this in Chapter 4.
In §1, we saw that the nicest case of the Extension Theorem was when one of
the leading coefﬁcients ci was a nonzero constant. Then the ci's can never simulta-
neously vanish at a point (a2, . . . , an), and, consequently, partial solutions always
extend in this case. Thus, we have the following geometric version of Corollary 4
of §1.

132
Chapter 3 Elimination Theory
Corollary 4. Let V = V( f1, . . . , fs) ⊆Cn, and assume that for some i, fi is of the
form
fi = cixNi
1
+ terms in which x1 has degree < Ni,
where ci ∈C is nonzero and Ni > 0. If I1 is the ﬁrst elimination ideal, then in Cn−1,
π1(V) = V(I1),
where π1 is the projection on the last n −1 coordinates.
A ﬁnal point to make concerns ﬁelds. The Extension Theorem and the Closure
Theorem (and their corollaries) are stated for the ﬁeld of complex numbers C. In §§5
and 6, we will see that the Extension Theorem actually holds for any algebraically
closed ﬁeld k, and in Chapter 4, we will show that the same is true for the Closure
Theorem.
EXERCISES FOR §2
1. Prove the Geometric Extension Theorem (Theorem 2) using the Extension Theorem and
Lemma 1.
2. In example (2), verify carefully that ⟨(y−z)x2+xy−1, (y−z)x2+xz−1⟩= ⟨xy−1, xz−1⟩.
Also check that y −z vanishes at all partial solutions in V(I1).
3. In this problem, we will prove part (ii) of Theorem 3 in the special case when I =
⟨f1, f2, f3⟩, where
f1 = yx3 + x2,
f2 = y3x2 + y2,
f3 = yx4 + x2 + y2.
a. Find a Gröbner basis for I and show that I1 = ⟨y2⟩.
b. Let ci be the coefﬁcient of the highest power of x in fi. Then explain why W =
V(c1, c2, c3) ∩V(I1) does not satisfy part (ii) of Theorem 3.
c. Let ˜I = ⟨f1, f2, f3, c1, c2, c3⟩. Show that V(I) = V(˜I) and V(I1) = V(˜I1).
d. Let xNi be the highest power of x appearing in fi and set ˜f i = fi −cixNi. Show that
˜I = ⟨˜f 1, ˜f 2, ˜f 3, c1, c2, c3⟩.
e. Repeat part (b) for ˜I using the generators from part (d) to ﬁnd W ⊊V(I1) that satisﬁes
part (ii) of Theorem 3.
4. To see how the Closure Theorem can fail over R, consider the ideal
I = ⟨x2 + y2 + z2 + 2, 3x2 + 4y2 + 4z2 + 5⟩.
Let V = V(I), and let π1 be the projection taking (x, y, z) to (y, z).
a. Working over C, prove that V(I1) = π1(V).
b. Working over R, prove that V = ∅and that V(I1) is inﬁnite. Thus, V(I1) may be much
larger than the smallest variety containing π1(V) when the ﬁeld is not algebraically
closed.
5. Suppose that I ⊆C[x, y] is an ideal such that I1 ̸= {0}. Prove that V(I1) = π1(V),
where V = V(I) and π1 is the projection onto the y-axis. Hint: Use part (i) of the Closure
Theorem. Also, the only varieties contained in C are either C or ﬁnite subsets of C.

§3 Implicitization
133
§3
Implicitization
In Chapter 1, we saw that a variety V can sometimes be described using para-
metric equations. The basic idea of the implicitization problem is to convert the
parametrization into deﬁning equations for V. The name "implicitization" comes
from Chapter 1, where the equations deﬁning V were called an "implicit representa-
tion" of V. However, some care is required in giving a precise formulation of implic-
itization. The problem is that the parametrization need not ﬁll up all of the variety
V—an example is given by equation (4) from Chapter 1, §3. So the implicitization
problem really asks for the equations deﬁning the smallest variety V containing the
parametrization. In this section, we will use the elimination theory developed in §§1
and 2 to give a complete solution of the implicitization problem.
Furthermore, once the smallest variety V has been found, two other interesting
questions arise. First, does the parametrization ﬁll up all of V? Second, if there
are missing points, how do we ﬁnd them? As we will see, Gröbner bases and the
Extension Theorem give us powerful tools for studying this situation.
To illustrate these issues in a speciﬁc case, let us look at the tangent surface to the
twisted cubic in R3, ﬁrst studied in Chapter 1, §3. Recall that this surface is given
parametrically by
x = t + u,
y = t2 + 2tu,
(1)
z = t3 + 3t2u.
In §8 of Chapter 2, we used these equations to show that the tangent surface lies on
the variety V in R3 deﬁned by
x3z −(3/4)x2y2 −(3/2)xyz + y3 + (1/4)z2 = 0.
However, we do not know if V is the smallest variety containing the tangent surface
and, thus, we cannot claim to have solved the implicitization problem. Furthermore,
even if V is the smallest variety, we still do not know if the tangent surface ﬁlls it up
completely. So there is a lot of work to do.
We begin our solution of the implicitization problem with the case of a polyno-
mial parametrization, which is speciﬁed by the data
x1 = f1(t1, . . . , tm),
...
(2)
xn = fn(t1, . . . , tm).
Here, f1, . . . , fn are polynomials in k[t1, . . . , tm]. We can think of this geometrically
as the function
F : km −→kn
deﬁned by

134
Chapter 3 Elimination Theory
F(t1, . . . , tm) = ( f1(t1, . . . , tm), . . . , fn(t1, . . . , tm)).
Then F(km) ⊆kn is the subset of kn parametrized by equations (2). Since F(km)
may not be an afﬁne variety (examples will be given in the exercises), a solution of
the implicitization problem means ﬁnding the smallest afﬁne variety that contains
F(km).
We can relate implicitization to elimination as follows. Equations (2) deﬁne a
variety V = V(x1 −f1, . . . , xn −fn) ⊆km+n. Points of V can be written in the form
(t1, . . . , tm, f1(t1, . . . , tm), . . . , fn(t1, . . . , tm)),
which shows that V can be regarded as the graph of the function F. We also have
two other functions
i : km −→km+n,
πm : km+n −→kn
deﬁned by
i(t1, . . . , tm) = (t1, . . . , tm, f1(t1, . . . , tm), . . . , fn(t1, . . . , tm))
and
πm(t1, . . . , tm, x1, . . . , xn) = (x1, . . . , xn),
respectively. This gives us the following diagram of sets and maps:
(3)
km+n
πm
D
D
D
D
D
D
D
D
km
i
z
z
z
z
z
z
z
z
F
 kn
Note that F is then the composition F = πm ◦i. It is also straightforward to show
that i(km) = V. Thus, we obtain
(4)
F(km) = πm(i(km)) = πm(V).
In more concrete terms, this says that the image of the parametrization is the pro-
jection of its graph. We can now use elimination theory to ﬁnd the smallest variety
containing F(km).
Theorem 1 (Polynomial Implicitization). If k is an inﬁnite ﬁeld, let F : km →kn
be the function determined by the polynomial parametrization (2). Let I be the ideal
I = ⟨x1 −f1, . . . , xn −fn⟩⊆k[t1, . . . , tm, x1, . . . , xn] and let Im = I ∩k[x1, . . . , xn]
be the m-th elimination ideal. Then V(Im) is the smallest variety in kn containing
F(km).
Proof. By equation (4) above and Lemma 1 of §2, F(km) = πm(V) ⊆V(Im).
Thus V(Im) is a variety containing F(km). To show it is the smallest, suppose that

§3 Implicitization
135
h ∈k[x1, . . . , xn] vanishes on F(km). We show that h ∈Im as follows. If we regard h
as a polynomial in k[t1, . . . , tm, x1, . . . , xn], then we can divide h by x1−f1, . . . , xn−fn
using lex order with x1 > · · · > xn > t1 > · · · > tm. This gives an equation
(5)
h(x1, . . . , xn) = q1 · (x1 −f1) + · · · + qn · (xn −fn) + r(t1, . . . , tm)
since LT(xj −fj) = xj. Given any a = (a1, . . . , am) ∈km, we substitute ti = ai and
xi = fi(a) into the above equation to obtain
0 = h( f1(a), . . . , fn(a)) = 0 + · · · + 0 + r(a).
It follows that r(a) = 0 for all a ∈km. Since k is inﬁnite, Proposition 5 of Chapter 1,
§1 implies that r(t1, . . . , tm) is the zero polynomial. Thus we obtain
h(x1, . . . , xn) = q1 · (x1 −f1) + · · · + qn · (xn −fn) ∈I ∩k[x1, . . . , xn] = Im
since I = ⟨x1 −f1, . . . , xn −fn⟩.
Now suppose that Z = V(h1, . . . , hs) ⊆kn is variety of kn such that F(km) ⊆Z.
Then each hi vanishes on F(km) and hence lies in Im by the previous paragraph. Thus
V(Im) ⊆V(h1, . . . , hs) = Z.
This proves that V(Im) is the smallest variety of kn containing F(km).
□
Theorem 1 gives the following implicitization algorithm for polynomial
parametrizations: if we are given xi = fi(t1, . . . , tm) for polynomials f1, . . . , fn ∈
k[t1, . . . , tm], consider the ideal I = ⟨x1 −f1, . . . , xn −fn⟩and compute a Gröbner
basis with respect to a lexicographic ordering where every ti is greater than every
xi. By the Elimination Theorem, the elements of the Gröbner basis not involving
t1, . . . , tm form a basis of Im, and by Theorem 1, they deﬁne the smallest variety in
kn containing the parametrization.
For an example of how this algorithm works, let us look at the tangent surface to
the twisted cubic in R3, which is given by the polynomial parametrization (1). Thus,
we need to consider the ideal
I = ⟨x −t −u, y −t2 −2tu, z −t3 −3t2u⟩⊆R[t, u, x, y, z].
Using lex order with t > u > x > y > z, a Gröbner basis for I is given by
g1 = t + u −x,
g2 = u2 −x2 + y,
g3 = ux2 −uy −x3 + (3/2)xy −(1/2)z,
g4 = uxy −uz −x2y −xz + 2y2,
g5 = uxz −uy2 + x2z −(1/2)xy2 −(1/2)yz,
g6 = uy3 −uz2 −2x2yz + (1/2)xy3 −xz2 + (5/2)y2z,
g7 = x3z −(3/4)x2y2 −(3/2)xyz + y3 + (1/4)z2.

136
Chapter 3 Elimination Theory
The Elimination Theorem tells us that I2 = I ∩R[x, y, z] = ⟨g7⟩, and thus by Theo-
rem 1, V(g7) solves the implicitization problem for the tangent surface of the twisted
cubic. The equation g7 = 0 is exactly the one given at the start of this section, but
now we know it deﬁnes the smallest variety in R3 containing the tangent surface.
But we still do not know if the tangent surface ﬁlls up all of V(g7) ⊆R3. To
answer this question, we must see whether all partial solutions (x, y, z) ∈V(g7) =
V(I2) extend to (t, u, x, y, z) ∈V(I). We will ﬁrst work over C so that we can use
the Extension Theorem. As usual, our strategy will be to add one coordinate at a
time.
Let us start with (x, y, z) ∈V(I2) = V(g7). In §1, we observed that I2 is the
ﬁrst elimination ideal of I1. Further, the Elimination Theorem tells us that I1 =
⟨g2, . . . , g7⟩. Then the Extension Theorem, in the form of Corollary 4 of §1, implies
that (x, y, z) always extends to (u, x, y, z) ∈V(I1) since I1 has a generator with a
constant leading coefﬁcient of u (we leave it to you to ﬁnd which of g2, . . . , g7 has
this property). Going from (u, x, y, z) ∈V(I1) to (t, u, x, y, z) ∈V(I) is just as easy:
using Corollary 4 of §1 again, we can always extend since g1 = t + u −x has a
constant leading coefﬁcient of t. We have thus proved that the tangent surface to the
twisted cubic equals V(g7) in C3.
It remains to see what happens over R. If we start with a real solution (x, y, z) ∈
R3 of g7 = 0, we know that it extends to (t, u, x, y, z) ∈V(I) ⊆R5. But are the
parameters t and u real? This is not immediately obvious. However, if you look at
the above Gröbner basis, you can check that t and u are real when (x, y, z) ∈R3 (see
Exercise 4). It follows that the tangent surface to the twisted cubic in R3 equals the
variety deﬁned by
x3z −(3/4)x2y2 −(3/2)xyz + y3 + (1/4)z2 = 0.
In general, the question of whether a parametrization ﬁlls up all of its variety can
be difﬁcult to answer. Each case has to be analyzed separately. But as indicated by
the example just completed, the combination of Gröbner bases and the Extension
Theorem can shed considerable light on what is going on.
In our discussion of implicitization, we have thus far only considered polynomial
parametrizations. The next step is to see what happens when we have a parametriza-
tion by rational functions. To illustrate the difﬁculties that can arise, consider the
following rational parametrization:
(6)
x = u2
v ,
y = v2
u ,
z = u.
It is easy to check that the point (x, y, z) always lies on the surface x2y = z3. Let
us see what happens if we clear denominators in the above equations and apply the
polynomial implicitization algorithm. We get the ideal

§3 Implicitization
137
I = ⟨vx −u2, uy −v2, z −u⟩⊆k[u, v, x, y, z],
and we leave it as an exercise to show that I2 = I ∩k[x, y, z] is given by I2 =
⟨z(x2y −z3)⟩. This implies that
V(I2) = V(x2y −z3) ∪V(z),
and, in particular, V(I2) is not the smallest variety containing the parametrization.
So the above ideal I is not what we want—simply "clearing denominators" is too
naive. To ﬁnd an ideal that works better, we will need to be more clever.
In the general situation of a rational parametrization, we have
x1 = f1(t1, . . . , tm)
g1(t1, . . . , tm),
...
(7)
xn = fn(t1, . . . , tm)
gn(t1, . . . , tm),
where f1, g1, . . . , fn, gn are polynomials in k[t1, . . . , tm]. The map F from km to kn
given by (7) may not be deﬁned on all of km because of the denominators. But if we
let W = V(g1g2 · · · gn) ⊆km, then it is clear that
F(t1, . . . , tm) =
 f1(t1, . . . , tm)
g1(t1, . . . , tm), . . . , fn(t1, . . . , tm)
gn(t1, . . . , tm)

deﬁnes a map
F : km \ W −→kn.
To solve the implicitization problem, we need to ﬁnd the smallest variety of kn
containing F(km \ W).
We can adapt diagram (3) to this case by writing
(8)
km+n
πm
C
C
C
C
C
C
C
C
C
km \ W
i
v
v
v
v
v
v
v
v
v
F
 kn
It is easy to check that i(km \ W) ⊆V(I), where I = ⟨g1x1 −f1, . . . , gnxn −fn⟩is the
ideal obtained by "clearing denominators." The problem is that V(I) may not be the
smallest variety containing i(km \ W). In the exercises, you will see that (6) is such
an example.
To avoid this difﬁculty, we will alter the ideal I slightly by using a new variable to
control the denominators. Consider the polynomial ring k[y, t1, . . . , tm, x1, . . . , xn],
which gives us the afﬁne space k1+m+n. Let g be the product g = g1 · g2 · · · gn, so
that W = V(g). Then consider the ideal

138
Chapter 3 Elimination Theory
J = ⟨g1x1 −f1, . . . , gnxn −fn, 1 −gy⟩⊆k[y, t1, . . . , tm, x1, . . . , xn].
Note that the equation 1 −gy = 0 means that the denominators g1, . . . , gn never
vanish on V(J). To adapt diagram (8) to this new situation, consider the maps
j : km \ W −→k1+m+n,
π1+m : k1+m+n −→kn
deﬁned by
j(t1, . . . , tm) =

1
g(t1, . . . , tm), t1, . . . , tm, f1(t1, . . . , tm)
g1(t1, . . . , tm), . . . , fn(t1, . . . , tm)
gn(t1, . . . , tm)

and
π1+m(y, t1, . . . , tm, x1, . . . , xn) = (x1, . . . , xn),
respectively. Then we get the diagram
k1+m+n
π1+m
E
E
E
E
E
E
E
E
E
km \ W
j
t
t
t
t
t
t
t
t
t
t
F
 kn
As before, we have F = π1+m ◦j. The surprise is that j(km \ W) = V(J) in
k1+m+n. To see this, note that j(km \ W) ⊆V(J) follows easily from the deﬁni-
tions of j and J. Going the other way, if (y, t1, . . . , tm, x1, . . . , xn) ∈V(J), then
g(t1, . . . , tm)y = 1 implies that none of the gi's vanish at (t1, . . . , tm) and, thus,
gi(t1, . . . , tm)xi = fi(t1, . . . , tm) can be solved for xi = fi(t1, . . . , tm)/gi(t1, . . . , tm).
Since y = 1/g(t1, . . . , tm), it follows that our point lies in j(km \ W). This proves
that V(J) ⊆j(km \ W).
From F = π1+m ◦j and j(km \ W) = V(J), we obtain
(9)
F(km \ W) = π1+m( j(km \ W)) = π1+m(V(J)).
Thus, the image of the parametrization is the projection of the variety V(J). As with
the polynomial case, we can now use elimination theory to solve the implicitization
problem.
Theorem 2 (Rational Implicitization). If k is an inﬁnite ﬁeld, let F : km \ W →kn
be the function determined by the rational parametrization (7). Let J be the ideal
J = ⟨g1x1 −f1, . . . , gnxn −fn, 1 −gy⟩⊆k[y, t1, . . . , tm, x1, . . . , xn], where g =
g1 · g2 · · · gn and W = V(g). Also let J1+m = J ∩k[x1, . . . , xn] be the (1 + m)-th
elimination ideal. Then V(J1+m) is the smallest variety in kn containing F(km \ W).
Proof. We will show that if h ∈k[x1, . . . , xn] vanishes on F(km\W), then h ∈J1+m.
In the proof of Theorem 1, we divided h by xi −fi to obtain (5) . Here, we divide
h by gixi −fi, except that we need to multiply h by a (possibly large) power of
g = g1 · · · gn to make this work. In the exercises, you will show that (5) gets replaced

§3 Implicitization
139
with an equation
(10)
gNh(x1, . . . , xn) = q1 · (g1x1 −f1) + · · · + qn · (gnxn −fn) + r(t1, . . . , tm)
in k[t1, . . . , tm, x1, . . . , xn], where N is a sufﬁciently large integer. Then, given any
a = (a1, . . . , am) ∈km \ W, we have gi(a) ̸= 0 for all i. Hence we can substitute
ti = ai and xi = fi(a)/gi(a) into (10) to obtain
0 = g(a)Nh( f1(a)/g1(a)), . . . , fn(a)/gn(a)) = 0 + · · · + 0 + r(a).
Thus r(a) = 0 for all a ∈km \ W. Since k is inﬁnite, this implies that r(t1, . . . , tm)
is the zero polynomial, as you will prove in the exercises. Hence
gNh(x1, . . . , xn) = q1 · (g1x1 −f1) + · · · + qn · (gnxn −fn),
so that gNh ∈⟨g1x1 −f1, . . . , gnxn −fn⟩⊆k[t1, . . . , tm, x1, . . . , xn]. Combining this
with the identity
h = gNyNh + h(1 −gNyN) = yN(gNh) + h(1 + gy + · · · + gN−1yN−1)(1 −gy),
we see that in the larger ring k[y, t1, . . . , tm, x1, . . . , xn], we have
h(x1, . . . , xn) ∈⟨g1x1 −f1, . . . , gnxn −fn, 1 −gy⟩∩k[x1, . . . , xn] = J1+m
by the deﬁnition of J. From here, the proof is similar to what we did in Theorem 1.
The exercises at the end of the section will take you through the details.
□
The interpretation of Theorem 2 is very nice: given the rational parametriza-
tion (7), consider the equations
g1x1 = f1,
...
gnxn = fn,
g1g2 · · · gny = 1.
These equations are obtained from (7) by "clearing denominators" and adding a
ﬁnal equation (in the new variable y) to prevent the denominators from vanishing.
Then eliminating y, t1, . . . , tm gives us the equations we want.
Theorem 2 implies the following implicitization algorithm for rational param-
etrizations: if we have xi = fi/gi for polynomials f1, g1, . . . , fn, gn ∈k[t1, . . . , tm],
consider the new variable y and set J = ⟨g1x1 −f1, . . . , gnxn −fn, 1 −gy⟩, where
g = g1 · · · gn. Compute a Gröbner basis with respect to a lexicographic ordering
where y and every ti are greater than every xi. Then the elements of the Gröb-
ner basis not involving y, t1, . . . , tm deﬁne the smallest variety in kn containing the
parametrization. This algorithm is due to KALKBRENER (1990).
Let us see how this algorithm works for example (6). Let w be the new variable,
so that
J = ⟨vx −u2, uy −v2, z −u, 1 −uvw⟩⊆k[w, u, v, x, y, z].

140
Chapter 3 Elimination Theory
One easily calculates that J3 = J ∩k[x, y, z] = ⟨x2y −z3⟩, so that V(x2y −z3) is the
variety determined by the parametrization (6). In the exercises, you will study how
much of V(x2y −z3) is ﬁlled up by the parametrization.
We should also mention that in practice, resultants are often used to solve the
implicitization problem. Implicitization for curves and surfaces is discussed in AN-
DERSON, GOLDMAN and SEDERBERG (1984a) and (1984b). Another reference is
CANNY and MANOCHA (1992), which shows how implicitization of parametric
surfaces can be done using multipolynomial resultants. A more recent reference is
DICKENSTEIN and EMIRIS (2005).
EXERCISES FOR §3
1. In diagram (3) in the text, prove carefully that F = πm ◦i and i(km) = V.
2. When k = C, the conclusion of Theorem 1 can be strengthened. Namely, one can show
that there is a variety W ⊊V(Im) such that V(Im) \ W ⊆F(Cm). Prove this using the
Closure Theorem.
3. Give an example to show that Exercise 2 is false over R. Hint: t2 is always positive.
4. In the text, we proved that over C, the tangent surface to the twisted cubic is deﬁned by
the equation
g7 = x3z −(3/4)x2y2 −(3/2)xyz + y3 + (1/4)z2 = 0.
We want to show that the same is true over R. If (x, y, z) is a real solution of the above
equation, then we proved (using the Extension Theorem) that there are t, u ∈C such that
x = t + u,
y = t2 + 2tu,
z = t3 + 3t2u.
Use the Gröbner basis given in the text to show that t and u are real. This will prove that
(x, y, z) is on the tangent surface in R3. Hint: First show that u is real.
5. In the parametrization of the tangent surface of the twisted cubic, show that the parame-
ters t and u are uniquely determined by x, y, and z. Hint: The argument is similar to what
you did in Exercise 4.
6. Let S be the parametric surface deﬁned by
x = uv,
y = u2,
z = v2.
a. Find the equation of the smallest variety V that contains S.
b. Over C, use the Extension Theorem to prove that S = V. Hint: The argument is
similar to what we did for the tangent surface of the twisted cubic.
c. Over R, show that S only covers "half" of V. What parametrization would cover the
other "half"?
7. Let S be the parametric surface
x = uv,
y = uv2,
z = u2.

§3 Implicitization
141
a. Find the equation of the smallest variety V that contains S.
b. Over C, show that V contains points which are not on S. Determine exactly which
points of V are not on S. Hint: Use lexicographic order with u > v > x > y > z.
8. The Enneper surface is deﬁned parametrically by
x = 3u + 3uv2 −u3,
y = 3v + 3u2v −v3,
z = 3u2 −3v2.
a. Find the equation of the smallest variety V containing the Enneper surface. It will be
a very complicated equation!
b. Over C, use the Extension Theorem to prove that the above equations parametrize
the entire surface V. Hint: There are a lot of polynomials in the Gröbner basis. Keep
looking—you will ﬁnd what you need.
9. The Whitney umbrella surface is given parametrically by
x = uv,
y = v,
z = u2.
A picture of this surface is:
a. Find the equation of the smallest variety containing the Whitney umbrella.
b. Show that the parametrization ﬁlls up the variety over C but not over R. Over R,
exactly what points are omitted?
c. Show that the parameters u and v are not always uniquely determined by x, y, and z.
Find the points where uniqueness fails and explain how your answer relates to the
picture.
10. Consider the curve in Cn parametrized by xi = fi(t), where f1, . . . , fn are polynomials in
C[t]. This gives the ideal
I = ⟨x1 −f1(t), . . . , xn −fn(t)⟩⊆C[t, x1, . . . , xn].
a. Prove that the parametric equations ﬁll up all of the variety V(I1) ⊆Cn.
b. Show that the conclusion of part (a) may fail if we let f1 . . . , fn be rational functions.
Hint: See §3 of Chapter 1.
c. Even if all of the fi's are polynomials, show that the conclusion of part (a) may fail if
we work over R.

142
Chapter 3 Elimination Theory
11. This problem is concerned with the proof of Theorem 2.
a. Take h ∈k[x1, . . . , xn] and let fi, gi be as in the theorem with g = g1 · · · gn. Show
that if N is sufﬁciently large, then there is F ∈k[t1, . . . , tm, x1, . . . , xn] such that
gNh = F(t1, . . . , tm, g1x1, . . . , gnxn).
b. Divide F from part (a) by x1 −f1, . . . , xn −fn. Then, in this division, replace xi with
gixi to obtain (10).
c. Let k be an inﬁnite ﬁeld and let f , g ∈k[t1, . . . , tm]. Assume that g ̸= 0 and that f
vanishes on km \ V(g). Prove that f is the zero polynomial. Hint: Consider fg.
d. Complete the proof of Theorem 2 using ideas from the proof of Theorem 1.
12. Consider the parametrization (6) given in the text. For simplicity, let k = C. Also let
I = ⟨vx −u2, uy −v2, z −u⟩be the ideal obtained by "clearing denominators."
a. Show that I2 = ⟨z(x2y −z3)⟩.
b. Show that the smallest variety in C5 containing i(C2 \ W) [see diagram (8)] is the
variety V(vx −u2, uy −v2, z −u, x2y −z3, vz −xy). Hint: Show that i(C2 \ W) =
π1(V(J)), and then use the Closure Theorem.
c. Show that {(0, 0, x, y, 0) | x, y ∈C} ⊆V(I) and conclude that V(I) is not the
smallest variety containing i(C2 \ W).
d. Determine exactly which portion of x2y = z3 is parametrized by (6).
13. Given a rational parametrization as in (7), there is one case where the naive ideal I =
⟨g1x1 −f1, . . . , gnxn −fn⟩obtained by "clearing denominators" gives the right answer.
Suppose that xi = fi(t)/gi(t) where there is only one parameter t. We can assume that for
each i, fi(t) and gi(t) are relatively prime in k[t] (so in particular, they have no common
roots). If I ⊆k[t, x1, . . . , xn] is as above, then prove that V(I1) is the smallest variety
containing F(k \ W), where as usual g = g1 · · · gn ∈k[t] and W = V(g) ⊆k. Hint: In
diagram (8), show that i(k \ W) = V(I), and adapt the proof of Theorem 2.
14. The folium of Descartes can be parametrized by
x =
3t
1 + t3 ,
y =
3t2
1 + t3 .
a. Find the equation of the folium. Hint: Use Exercise 13.
b. Over C or R, show that the above parametrization covers the entire curve.
15. In Exercise 16 to §3 of Chapter 1, we studied the parametric equations over R
x = (1 −t)2x1 + 2t(1 −t)wx2 + t2x3
(1 −t)2 + 2t(1 −t)w + t2
,
y = (1 −t)2y1 + 2t(1 −t)wy2 + t2y3
(1 −t)2 + 2t(1 −t)w + t2
,
where w, x1, y1, x2, y2, x3, y3 are constants and w > 0. By eliminating t, show that these
equations describe a portion of a conic section. Recall that a conic section is described
by an equation of the form
ax2 + bxy + cy2 + dx + ey + f = 0.
Hint: In most computer algebra systems, the Gröbner basis command allows polynomials
to have coefﬁcients involving symbolic constants like w, x1, y1, x2, y2, x3, y3.

§4 Singular Points and Envelopes
143
§4
Singular Points and Envelopes
In this section, we will discuss two topics from geometry:
• the singular points on a curve,
• the envelope of a family of curves.
Our goal is to show how geometry provides interesting equations that can be solved
by the techniques studied in §§1 and 2.
We will introduce some of the basic ideas concerning singular points and en-
velopes, but our treatment will be far from complete. One could write an entire
book on these topics [see, for example, BRUCE and GIBLIN (1992)]. Also, our dis-
cussion of envelopes will not be fully rigorous. We will rely on some ideas from
calculus to justify what is going on.
Singular Points
Suppose that we have a curve in the plane k2 deﬁned by f(x, y) = 0, where f ∈
k[x, y]. We expect that V( f) will have a well-deﬁned tangent line at most points,
although this may fail where the curve crosses itself or has a kink. Here are two
examples:
x
y
y2 = x3
x
y
y2 = x2(1 + x)
If we demand that a tangent line be unique and follow the curve on both sides of the
point, then each of these curves has a point where there is no tangent. Intuitively, a
singular point of V( f) is a point such as above where the tangent line fails to exist.
To make this notion more precise, we ﬁrst must give an algebraic deﬁnition of
tangent line. We will use the following approach. Given a point (a, b) ∈V( f), a
line L through (a, b) is given parametrically by
(1)
x = a + ct,
y = b + dt.

144
Chapter 3 Elimination Theory
This line goes through (a, b) when t = 0. Notice also that (c, d) ̸= (0, 0) is a vector
parallel to the line. Thus, by varying (c, d), we get all lines through (a, b). But how
do we ﬁnd the one that is tangent to V( f)? Can we ﬁnd it without using calculus?
Let us look at an example. Consider the line L
(2)
x = 1 + ct,
y = 1 + dt,
through the point (1, 1) on the parabola y = x2:
x
y
(1,1)
↓
L
← tangent line
y = x2
From calculus, we know that the tangent line has slope 2, which corresponds to the
line with d = 2c in the above parametrization. To ﬁnd this line by algebraic means,
we will study the polynomial that describes how the line meets the parabola. If we
substitute (2) into the left-hand side of y −x2 = 0, we get the polynomial
(3)
g(t) = 1 + dt −(1 + ct)2 = −c2t2 + (d −2c)t = t(−c2t + d −2c).
The roots of g determine where the line intersects the parabola (be sure you under-
stand this). If d ̸= 2c, then g has two distinct roots when c ̸= 0 and one root when
c = 0. But if d = 2c, then g has a root of multiplicity 2. Thus, we can detect when
the line (2) is tangent to the parabola by looking for a multiple root.
Based on this example, let us make the following deﬁnition.
Deﬁnition 1. Let m be a positive integer. Suppose that we have a point (a, b) ∈V( f)
and let L be the line through (a, b). Then L meets V( f) with multiplicity m at (a, b)
if L can be parametrized as in (1) so that t = 0 is a root of multiplicity m of the
polynomial g(t) = f(a + ct, b + dt).
In this deﬁnition, note that g(0) = f(a, b) = 0, so that t = 0 is a root of g. Also,
recall that t = 0 is a root of multiplicity m when g = tmh, where h(0) ̸= 0. One
ambiguity with this deﬁnition is that a given line has many different parametriza-
tions. So we need to check that the notion of multiplicity is independent of the
parametrization. This will be covered in the exercises.

§4 Singular Points and Envelopes
145
For an example of how this deﬁnition works, consider the line given by (2) above.
It should be clear from (3) that the line meets the parabola y = x2 with multiplicity
1 at (1, 1) when d ̸= 2c and with multiplicity 2 when d = 2c. Other examples will
be given in the exercises.
We will use the notion of multiplicity to pick out the tangent line. To make this
work, we will need the gradient vector of f, which is deﬁned to be
∇f =
 ∂f
∂x , ∂f
∂y

.
We can now state our result.
Proposition 2. Let f ∈k[x, y], and let (a, b) ∈V( f).
(i) If ∇f(a, b) ̸= (0, 0), then there is a unique line through (a, b) which meets V( f)
with multiplicity ≥2.
(ii) If ∇f(a, b) = (0, 0), then every line through (a, b) meets V( f) with multiplicity
≥2.
Proof. Let a line L through (a, b) be parametrized as in equation (1) and let g(t) =
f(a + ct, b + dt). Since (a, b) ∈V( f), it follows that t = 0 is a root of g. The
following observation will be proved in the exercises:
(4)
t = 0 is a root of g of multiplicity ≥2 ⇐⇒g′(0) = 0.
Using the chain rule, one sees that
g′(t) = ∂f
∂x (a + ct, b + dt) · c + ∂f
∂y (a + ct, b + dt) · d,
and thus
g′(0) = ∂f
∂x (a, b) · c + ∂f
∂y (a, b) · d.
If ∇f(a, b) = (0, 0), then the above equation shows that g′(0) always equals 0.
By (4), it follows that L always meets V( f) at (a, b) with multiplicity ≥2. This
proves the second part of the proposition. Turning to the ﬁrst part, suppose that
∇f(a, b) ̸= (0, 0). We know that g′(0) = 0 if and only if
(5)
∂f
∂x (a, b) · c + ∂f
∂y (a, b) · d = 0.
This is a linear equation in the unknowns c and d. Since the coefﬁcients ∂
∂x f(a, b)
and ∂
∂y f(a, b) are not both zero, the solution space is 1-dimensional. Thus, there is
(c0, d0) ̸= (0, 0) such that (c, d) satisﬁes the above equation if and only if (c, d) =
λ(c0, d0) for some λ ∈k. It follows that the (c, d)'s giving g′(0) = 0 all parametrize
the same line L. This shows that there is a unique line which meets V( f) at (a, b)
with multiplicity ≥2. Proposition 2 is proved.
□

146
Chapter 3 Elimination Theory
Using Proposition 2, it is now obvious how to deﬁne the tangent line. From the
second part of the proposition, it is also clear what a singular point should be.
Deﬁnition 3. Let f ∈k[x, y] and let (a, b) ∈V( f).
(i) If ∇f(a, b) ̸= (0, 0), then the tangent line of V( f) at (a, b) is the unique line
through (a, b) which meets V( f) with multiplicity ≥2. We say that (a, b) is a
nonsingular point of V( f).
(ii) If ∇f(a, b) = (0, 0), then we say that (a, b) is a singular point of V( f).
Over R, the tangent line and the gradient have the following geometric interpre-
tation. If the tangent to V( f) at (a, b) is parametrized by (1), then the vector (c, d) is
parallel to the tangent line. But we also know from equation (5) that the dot product
∇f(a, b) · (c, d) is zero, which means that ∇f(a, b) is perpendicular to (c, d). Thus,
we have an algebraic proof of the theorem from calculus that the gradient ∇f(a, b)
is perpendicular to the tangent line of V( f) at (a, b).
For any given curve V( f), we can compute the singular points as follows. The
gradient ∇f is zero when
∂
∂x f and ∂
∂y f vanish simultaneously. Since we also have
to be on V( f), we also need f = 0. It follows that the singular points of V( f) are
determined by the equations
f = ∂f
∂x = ∂f
∂y = 0.
As an example, consider the curve y2 = x2(1+x) shown earlier. To ﬁnd the singular
points, we must solve
f = y2 −x2 −x3 = 0,
∂f
∂x = −2x −3x2 = 0,
∂f
∂y = 2y = 0.
From these equations, it is easy to see that (0, 0) is the only singular point of V( f).
This agrees with the earlier picture.
Using the methods learned in §§1 and 2, we can tackle much more complicated
problems. For example, later in this section we will determine the singular points of
the curve deﬁned by the sixth degree equation
0 = −1156 + 688x2 −191x4 + 16x6 + 544y + 30x2y −40x4y
+ 225y2 −96x2y2 + 16x4y2 −136y3 −32x2y3 + 16y4.
The exercises will explore some other aspects of singular points. In Chapter 9, we
will study singular and nonsingular points on an arbitrary afﬁne variety.

§4 Singular Points and Envelopes
147
Envelopes
In our discussion of envelopes, we will work over R to make the geometry easier to
see. The best way to explain what we mean by envelope is to compute an example.
Let t ∈R, and consider the circle in R2 deﬁned by the equation
(6)
(x −t)2 + (y −t2)2 = 4.
Since (t, t2) parametrizes a parabola, we can think of equation (6) as describing the
family of circles of radius 2 in R2 whose centers lie on the parabola y = x2. The
picture is as follows:
A Family of Circles in the Plane
x
y
Note that the "boundary" curve is simultaneously tangent to all the circles in the
family. This is a special case of the envelope of a family of curves. The basic idea is
that the envelope of a family of curves is a single curve that is tangent to all of the
curves in the family. Our goal is to study envelopes and learn how to compute them.
In particular, we want to ﬁnd the equation of the envelope in the above example.
Before we can give a more careful deﬁnition of envelope, we must ﬁrst under-
stand the concept of a family of curves in R2.
Deﬁnition 4. Given a polynomial F ∈R[x, y, t], ﬁx a real number t ∈R. Then the
variety in R2 deﬁned by F(x, y, t) = 0 is denoted V(Ft), and the family of curves
determined by F consists of the varieties V(Ft) as t varies over R.
In this deﬁnition, we think of t as a parameter that tells us which curve in the
family we are looking at. Strictly speaking, we should say "family of varieties"
rather than "family of curves," but we will use the latter to emphasize the geometry
of the situation.
For another example of a family and its envelope, consider the curves deﬁned by
(7)
F(x, y, t) = (x −t)2 −y + t = 0.

148
Chapter 3 Elimination Theory
Writing this as y −t = (x −t)2, we see in the picture below that (7) describes
the family V(Ft) of parabolas obtained by translating the standard parabola y = x2
along the straight line y = x. In this case, the envelope is clearly the straight line
that just touches each parabola in the family. This line has slope 1, and from here, it
is easy to check that the envelope is given by y = x −1/4 (the details are left as an
exercise).
Not all envelopes are so easy to describe. The remarkable fact is that we can
characterize the envelope in the following completely algebraic way.
A Family of Parabolas in the Plane
x
y
Deﬁnition 5. Given a family V(Ft) of curves in R2, its envelope consists of all
points (x, y) ∈R2 with the property that
F(x, y, t) = 0,
∂F
∂t (x, y, t) = 0
for some t ∈R.
We need to explain how this deﬁnition corresponds to the intuitive idea of en-
velope. The argument given below is not rigorous, but it does explain where the
condition on
∂
∂tF comes from. A complete treatment of envelopes requires a fair
amount of theoretical machinery. We refer the reader to Chapter 5 of BRUCE and
GIBLIN (1992) for more details.
Given a family V(Ft), we think of the envelope as a curve C with the property
that at each point on the curve, C is tangent to one of the curves V(Ft) in the family.
Suppose that C is parametrized by
x = f(t),
y = g(t).
We will assume that at time t, the point ( f(t), g(t)) is on the curve V(Ft). This
ensures that C meets all the members of the family. Algebraically, this means that

§4 Singular Points and Envelopes
149
(8)
F( f(t), g(t), t) = 0
for all t ∈R.
But when is C tangent to V(Ft) at ( f(t), g(t))? This is what is needed for C
to be the envelope of the family. We know from calculus that the tangent vector
to C is ( f ′(t), g′(t)). As for V(Ft), we have the gradient ∇F =
 ∂
∂xF, ∂
∂yF
	
, and
from the ﬁrst part of this section, we know that ∇F is perpendicular to the tangent
line to V(Ft). Thus, for C to be tangent to V(Ft), the tangent ( f ′(t), g′(t)) must be
perpendicular to the gradient ∇F. In terms of dot products, this means that ∇F ·
( f ′(t), g′(t)) = 0 or, equivalently,
(9)
∂F
∂x ( f(t), g(t), t) · f ′(t) + ∂F
∂y ( f(t), g(t), t) · g′(t) = 0.
We have thus shown that the envelope is determined by conditions (8) and (9).
To relate this to Deﬁnition 5, differentiate (8) with respect to t. Using the chain rule,
we get
∂F
∂x ( f(t), g(t), t) · f ′(t) + ∂F
∂y ( f(t), g(t), t) · g′(t) + ∂F
∂t ( f(t), g(t), t) = 0.
If we subtract equation (9) from this, we obtain
(10)
∂F
∂t ( f(t), g(t), t) = 0.
From (8) and (10), it follows that (x, y) = ( f(t), g(t)) has exactly the property
described in Deﬁnition 5.
As we will see later in the section, the above discussion of envelopes is rather
naive. For us, the main consequence of Deﬁnition 5 is that the envelope of V(Ft) is
determined by the equations
F(x, y, t) = 0,
∂F
∂t (x, y, t) = 0.
Note that x and y tell us where we are on the envelope and t tells us which curve in
the family we are tangent to. Since these equations involve x, y, and t, we need to
eliminate t to ﬁnd the equation of the envelope. Thus, we can apply the theory from
§§1 and 2 to study the envelope of a family of curves.
Let us see how this works in example (6). Here, F = (x −t)2 + (y −t2)2 −4, so
that the envelope is described by the equations
(11)
F = (x −t)2 + (y −t2)2 −4 = 0,
∂F
∂t = −2(x −t) −4t(y −t2) = 0.
Using lexicographic order with t > x > y, a Gröbner basis is given by the ﬁve
polynomials

150
Chapter 3 Elimination Theory
g1 = −1156 + 688x2 −191x4 + 16x6 + 544y + 30x2y −40x4y
+ 225y2 −96x2y2 + 16x4y2 −136y3 −32x2y3 + 16y4,
g2 = (7327 −1928y −768y2 −896y3 + 256y4)t + 6929x −2946x3
+ 224x5 + 2922xy −1480x3y + 128x5y −792xy2 −224x3y2
−544xy3 + 128x3y3 −384xy4,
g3 = (431x −12xy −48xy2 −64xy3)t + 952 −159x2 −16x4 + 320y
−214x2y + 32x4y −366y2 −32x2y2 −80y3 + 32x2y3 + 32y4,
g4 = (697 −288x2 + 108y −336y2 + 64y3)t + 23x −174x3
+ 32x5 + 322xy −112x3y + 32xy2 + 32x3y2 −96xy3,
g5 = 135t2 + (26x + 40xy + 32xy2)t −128 + 111x2
−16x4 + 64y + 8x2y + 32y2 −16x2y2 −16y3.
We have written the Gröbner basis as polynomials in t with coefﬁcients in R[x, y].
The Elimination Theorem tells us that g1 generates the ﬁrst elimination ideal. Thus,
the envelope lies on the curve g1 = 0. Here is a picture of V(g1) together with the
parabola y = x2:
−6
−4
−2
0
2
4
6
−2
0
2
4
6
8
10
The surprise is the "triangular" portion of the graph that was somewhat unclear in
the earlier picture of the family. By drawing some circles centered on the parabola,
you can see how the triangle is still part of the envelope.
We have proved that the envelope lies on V(g1), but the two may not be equal. In
fact, there are two interesting questions to ask at this point:
• Is every point of V(g1) on the envelope? This is the same as asking if every partial
solution (x, y) of (11) extends to a complete solution (x, y, t).
• Given a point on the envelope, how many curves in the family are tangent to the
envelope at the point? This asks how many t's are there for which (x, y) extends
to (x, y, t).

§4 Singular Points and Envelopes
151
Since the leading coefﬁcient of t in g5 is the constant 135, the Extension Theorem
(in the form of Corollary 4 of §1) guarantees that every partial solution extends, pro-
vided we work over the complex numbers. Thus, t exists, but it might be complex.
This illustrates the power and limitation of the Extension Theorem: it can guarantee
that there is a solution, but it might lie in the wrong ﬁeld.
In spite of this difﬁculty, the equation g5 = 0 does have something useful to tell
us: it is quadratic in t, so that a given (x, y) extends in at most two ways to a complete
solution. Thus, a point on the envelope of (6) is tangent to at most two circles in the
family. Can you see any points where there are two tangent circles?
To get more information on what is happening, let us look at the other polyno-
mials in the Gröbner basis. Note that g2, g3, and g4 involve t only to the ﬁrst power.
Thus, we can write them in the form
gi = Ai(x, y)t + Bi(x, y),
i = 2, 3, 4.
If Ai does not vanish at (x, y) for one of i = 2, 3, 4, then we can solve Ait + Bi = 0
to obtain
t = −Bi(x, y)
Ai(x, y).
Thus, we see that t is real whenever x and y are. More importantly, this formula
shows that t is uniquely determined when Ai(x, y) ̸= 0. Thus, a point on the envelope
of (6) not in V(A2, A3, A4) is tangent to exactly one circle in the family.
It remains to understand where A2, A3, and A4 vanish simultaneously. These poly-
nomials might look complicated, but, using the techniques of §1, one can show that
the real solutions of A2 = A3 = A4 = 0 are
(12)
(x, y) = (0, 17/4) and (±0.936845, 1.63988).
Looking back at the picture of V(g1), it appears that these are the singular points of
V(g1). Can you see the two circles tangent at these points? From the ﬁrst part of this
section, we know that the singular points of V(g1) are determined by the equations
g1 =
∂
∂xg1 =
∂
∂yg1 = 0. Thus, to say that the singular points coincide with (12)
means that
(13)
V(A2, A3, A4) = V

g1, ∂g1
∂x , ∂g1
∂y

.
To prove this, we will show that
(14)
g1, ∂g1
∂x , ∂g1
∂y ∈

A2, A3, A4

,
A2
2, A2
3, A2
4 ∈

g1, ∂g1
∂x , ∂g1
∂y

.
The proof of (14) is a straightforward application of the ideal membership algo-
rithm discussed in Chapter 2. For the ﬁrst line, one computes a Gröbner basis of

152
Chapter 3 Elimination Theory
⟨A2, A3, A4⟩and then applies the algorithm for the ideal membership problem to
each of g1, ∂
∂xg1, ∂
∂yg1 (see §8 of Chapter 2). The second line of (14) is treated
similarly—the details will be left as an exercise.
Since (13) follows immediately from (14), we have proved that a nonsingular
point on V(g1) is in the envelope of (6) and, at such a point, the envelope is tangent
to exactly one circle in the family. Also note that the singular points of V(g1) are
the most interesting points in the envelope, for they are the ones where there are two
tangent circles. This last observation shows that singular points are not always bad—
they can be a useful indication that something unusual is happening. An important
part of algebraic geometry is devoted to the study of singular points.
In this example, equations (11) for the envelope were easy to write down. But to
understand the equations, we had to use a Gröbner basis and the Elimination and
Extension Theorems. Even though the Gröbner basis looked complicated, it told us
exactly which points on the envelope were tangent to more than one circle. This
illustrates nicely the power of the theory we have developed so far.
As we said earlier, our treatment of envelopes has been a bit naive. Evidence
of this comes from the above example, which shows that the envelope can have
singularities. How can the envelope be "tangent" to a curve in the family at a singular
point? In the exercises, we will indicate another reason why our discussion has been
too simple. We have also omitted the fascinating relation between the family of
curves V(Ft) ⊆R2 and the surface V(F) ⊆R3 deﬁned by F(x, y, t) = 0. We refer
the reader to Chapter 5 of BRUCE and GIBLIN (1992) for a more complete treatment
of these aspects of envelopes.
EXERCISES FOR §4
1. Let C be the curve in k2 deﬁned by x3 −xy + y2 = 1 and note that (1, 1) ∈C. Now
consider the straight line parametrized by
x = 1 + ct,
y = 1 + dt.
Compute the multiplicity of this line when it meets C at (1, 1). What does this tell you
about the tangent line? Hint: There will be two cases to consider.
2. In Deﬁnition 1, we need to show that the notion of multiplicity is independent of how
the line is parametrized.
a. Show that two parametrizations
x = a + ct,
x = a + c′t,
y = b + dt,
y = b + d′t,
correspond to the same line if and only if there is a nonzero number λ ∈k such that
(c, d) = λ(c′, d′). Hint: In the parametrization x = a + ct, y = b + dt of a line L,
recall that L is parallel to the vector (c, d).
b. Suppose that the two parametrizations of part (a) correspond to the same line L that
meets V( f ) at (a, b). Prove that the polynomials g(t) = f (a+ ct, b+ dt) and g′(t) =
f (a + c′t, b + d′t) have the same multiplicity at t = 0. Hint: Use part (a) to relate
g and g′. This will prove that the multiplicity of how L meets V( f ) at (a, b) is well
deﬁned.

§4 Singular Points and Envelopes
153
3. Consider the straight lines
x = t,
y = b + t.
These lines have slope 1 and y-intercept b. For which values of b is the line tangent
to the circle x2 + y2 = 2? Draw a picture to illustrate your answer. Hint: Consider
g(t) = t2 + (b + t)2 −2. The roots of this quadratic determine the values of t where the
line meets the circle.
4. If (a, b) ∈V( f ) and ∇f (a, b) ̸= (0, 0), prove that the tangent line of V( f ) at (a, b) is
deﬁned by the equation
∂
∂x f (a, b) · (x −a) + ∂
∂y f (a, b) · (y −b) = 0.
5. Let g ∈k[t] be a polynomial such that g(0) = 0. Assume that Q ⊆k.
a. Prove that t = 0 is a root of multiplicity ≥2 of g if and only if g′(0) = 0. Hint:
Write g(t) = th(t) and use the product rule.
b. More generally, prove that t = 0 is a root of multiplicity ≥m if and only if g′(0) =
g′′(0) = · · · = g(m−1)(0) = 0.
6. As in the Deﬁnition 1, let a line L be parametrized by (1), where (a, b) ∈V( f ). Also
let g(t) = f (a + ct, b + dt). Prove that L meets V( f ) with multiplicity m if and only
if g′(0) = g′′(0) = · · · = g(m−1)(0) = 0 but g(m)(0) ̸= 0. Hint: Use the previous
exercise.
7. In this exercise, we will study how a tangent line can meet a curve with multiplicity
greater than 2. Let C be the curve deﬁned by y = f (x), where f ∈k[x]. Thus, C is just
the graph of f .
a. Give an algebraic proof that the tangent line to C at (a, f (a)) is parametrized by
x = a + t,
y = f (a) + f ′(a)t.
Hint: Consider g(t) = f (a) + f ′(a)t −f (a + t).
b. Show that the tangent line at (a, f (a)) meets the curve with multiplicity ≥3 if and
only if f ′′(a) = 0. Hint: Use the previous exercise.
c. Show that the multiplicity is exactly 3 if and only if f ′′(a) = 0 but f ′′′(a) ̸= 0.
d. Over R, a point of inﬂection is deﬁned to be a point where f ′′(x) changes sign. Prove
that if the multiplicity is 3, then (a, f (a)) is a point of inﬂection.
8. In this problem, we will compute some singular points.
a. Show that (0, 0) is the only singular point of y2 = x3.
b. In Exercise 8 of §3 of Chapter 1, we studied the curve y2 = cx2 −x3, where c is some
constant. Find all singular points of this curve and explain how your answer relates
to the picture of the curve given in Chapter 1.
c. Show that the circle x2 + y2 = a2 in R2 has no singular points when a > 0.
9. One use of multiplicities is to show that one singularity is "worse" than another.
a. For the curve y2 = x3, show that most lines through the origin meet the curve with
multiplicity exactly 2.
b. For x4 + 2xy2 + y3 = 0, show that all lines through the origin meet the curve with
multiplicity ≥3.
This suggests that the singularity at the origin is "worse" on the second curve. Using
the ideas behind this exercise, one can deﬁne the notion of the multiplicity of a singular
point.
10. We proved in the text that (0, 0) is a singular point of the curve C deﬁned by y2 =
x2(1 + x). But in the picture of C, it looks like there are two "tangent" lines through the
origin. Can we use multiplicities to pick these out?

154
Chapter 3 Elimination Theory
a. Show that with two exceptions, all lines through the origin meet C with multiplicity
2. What are the lines that have multiplicity 3?
b. Explain how your answer to part (a) relates to the picture of C in the text. Why should
the "tangent" lines have higher multiplicity?
11. The four-leaved rose is deﬁned in polar coordinates by the equation r = sin(2θ):
-.75
-.5
-.25
.25
.5
.75
-.75
-.5
-.25
.25
.5
.75
In Cartesian coordinates, this curve is deﬁned by the equation (x2 + y2)3 = 4x2y2.
a. Show that most lines through the origin meet the rose with multiplicity 4 at the origin.
Can you give a geometric explanation for this number?
b. Find the lines through the origin that meet the rose with multiplicity > 4. Give a
geometric explanation for the numbers you get.
12. Consider a surface V( f ) ⊆k3 deﬁned by f ∈k[x, y, z].
a. Deﬁne what it means for (a, b, c) ∈V( f ) to be a singular point.
b. Determine all singular points of the sphere x2 + y2 + z2 = 1. Does your answer make
sense?
c. Determine all singular points on the surface V(x2 −y2z2+z3). How does your answer
relate to the picture of the surface drawn in §2 of Chapter 1?
13. Consider the family of curves given by F = xy −t ∈R[x, y, t]. Draw several of the
curves V(Ft). Be sure to include a picture of V(F0). What is the envelope of this family?
14. This problem will study the envelope of the family F = (x −t)2 −y + t considered in
example (7).
a. It is obvious that the envelope is a straight line of slope 1. Use elementary calculus to
show that the line is y = x −1/4.
b. Use Deﬁnition 5 to compute the envelope.
c. Find a parametrization of the envelope so that at time t, the point ( f (t), g(t)) is on the
parabola V(Ft). Note that this is the kind of parametrization used in our discussion
of Deﬁnition 5.
15. This problem is concerned with the envelope of example (6).
a. Copy the picture in the text onto a sheet of paper and draw in the two tangent circles
for each of the points in (12).
b. For the point (0, 4.25) = (0, 17/4), ﬁnd the exact values of the t's that give the two
tangent circles.
c. Show that the exact values of the points (12) are given by

0, 17
4

and

±

15 + 6
3√
2 −12
3√
4, 1
4(−1 + 6
3√
2)

.
Hint: Most computer algebra systems have commands to factor polynomials and
solve cubic equations.

§5 Gröbner Bases and the Extension Theorem
155
16. Consider the family determined by F = (x −t)2 + y2 −(1/2)t2.
a. Compute the envelope of this family.
b. Draw a picture to illustrate your answer.
17. Consider the family of circles deﬁned by (x −t)2 + (y −t2)2 = t2 in the plane R2.
a. Compute the equation of the envelope of this family and show that the envelope is
the union of two varieties.
b. Use the Extension Theorem and a Gröbner basis to determine, for each point in the
envelope, how many curves in the family are tangent to it. Draw a picture to illustrate
your answer. Hint: You will use a different argument for each of the two curves
making up the envelope.
18. Prove (14) using the hints given in the text. Also show that A2 /∈

g1, ∂
∂xg1, ∂
∂yg1

. This
shows that the ideals

g1, ∂
∂xg1, ∂
∂yg1

and ⟨A2, A3, A4⟩are not equal, even though they
deﬁne the same variety.
19. In this exercise, we will show that our deﬁnition of envelope is too naive.
a. Given a family of circles of radius 1 with centers lying on the x-axis, draw a picture
to show that the envelope consists of the lines y = ±1.
b. Use Deﬁnition 5 to compute the envelope of the family given by F = (x−t)2+y2−1.
Your answer should not be surprising.
c. Use Deﬁnition 5 to ﬁnd the envelope when the family is F = (x −t3)2 + y2 −1.
Note that one of the curves in the family is part of the answer. This is because using
t3 allows the curves to "bunch up" near t = 0, which forces V(F0) to appear in the
envelope.
In our intuitive discussion of envelope, recall that we assumed we could parametrize the
envelope so that ( f (t), g(t)) was in V(Ft) at time t. This presumes that the envelope is
tangent to different curves in the family. Yet in the example given in part (c), part of the
envelope lies in the same curve in the family. Thus, our treatment of envelope was too
simple.
20. Suppose we have a family of curves in R2 determined by F ∈R[x, y, t]. Some of the
curves V(Ft) may have singular points whereas others may not. Can we ﬁnd the ones
that have a singularity?
a. By considering the equations F =
∂
∂xF =
∂
∂yF = 0 in R3 and using elimination
theory, describe a procedure for determining those t's corresponding to curves in the
family which have a singular point.
b. Apply the method of part (a) to ﬁnd the curves in the family of Exercise 13 that have
singular points.
§5
Gröbner Bases and the Extension Theorem
The ﬁnal task of Chapter 3 is to prove the Extension Theorem. We give two proofs,
one using Gröbner bases in this section and a second using resultants in the next.
The proofs are very different, which means that §§5 and 6 can be read independently
of each other.
In the Extension Theorem, we have an ideal I ⊆k[x1, . . . , xn] with elimination
ideal I1 = I ∩k[x2, . . . , xn]. Given a partial solution a = (a2, . . . , an) ∈V(I1), the
goal is to ﬁnd a1 ∈k such that (a1, a) ∈V(I), i.e, to extend a to a solution of the
original system.
Before beginning the proof, we introduce some notation. If f ∈k[x1, . . . , xn] is
nonzero, we write f in the form

156
Chapter 3 Elimination Theory
f = cf(x2, . . . , xn)xN
1 + terms in which x1 has degree < N,
where N ≥0 and cf ∈k[x2, . . . , xn] is nonzero. We also deﬁne deg( f, x1) = N and
set cf = 0 when f = 0.
Here are two properties of deg( f, x1) and cf that we will need. The proof will be
left to the reader (Exercise 1).
Lemma 1. Assume that f = t
j=1 Ajgj is a standard representation for lex order
with x1 > · · · > xn. Then:
(i) deg( f, x1) ≥deg(Ajgj, x1) whenever Ajgj ̸= 0.
(ii) cf = 
deg(Ajgj,x1)=N cAjcgj, where N = deg( f, x1).
The ﬁrst main result of this section tells us how lex Gröbner bases interact with
nicely behaved partial solutions.
Theorem 2. Let G = {g1, . . . , gt} be a Gröbner basis of I ⊆k[x1, . . . , xn] for lex
order with x1 > · · · > xn. For each 1 ≤j ≤t, let cj = cgj, so that
gj = cj(x2, . . . , xn)xNj
1 + terms in which x1 has degree < Nj,
where Nj ≥0 and cj ∈k[x2, . . . , xn] is nonzero. Assume a = (a2, . . . , an) ∈V(I1) is
a partial solution with the property that a /∈V(c1, . . . , ct). Then
(1)
{f(x1, a) | f ∈I} = ⟨go(x1, a)⟩⊆k[x1],
where go ∈G satisﬁes co(a) ̸= 0 and go has minimal x1-degree among all elements
gj ∈G with cj(a) ̸= 0. Furthermore:
(i) deg(go(x1, a)) > 0.
(ii) If go(a1, a) = 0 for a1 ∈k, then (a1, a) ∈V(I).
In this theorem, we use the letter "o" for the index to indicate that go is optimal
with respect to the partial solution a ∈V(I1).
Proof. Choose an optimal go ∈G. We ﬁrst prove (i). If deg(go(x1, a)) = 0, then
deg(go, x1) = 0 since co(a) ̸= 0. Thus go ∈I1 and co = go. But then a ∈V(I1)
implies co(a) = go(a) = 0, a contradiction. This proves that deg(go(x1, a)) > 0.
Next observe that (ii) is an immediate consequence of (1) since go(a1, a) = 0
and (1) imply that f(a1, a) = 0 for all f ∈I, proving that (a1, a) ∈V(I).
It remains to prove (1), which is the hardest part of the proof. We begin with the
function
(2)
k[x1, . . . , xn] −→k[x1]
deﬁned by evaluation at a, i.e., f(x1, x2, . . . , xn) 
→f(x1, a). Since evaluation is
compatible with addition and multiplication of polynomials, (2) is what we will call
a ring homomorphism in Chapter 5, §2. In Exercise 2, you will show that the image
of I under the ring homomorphism (2) is an ideal of k[x1]. Furthermore, since I is

§5 Gröbner Bases and the Extension Theorem
157
generated by the gj ∈G, Exercise 3 implies that the image of I is generated by the
gj(x1, a). Hence (1) will follow once we show gj(x1, a) ∈⟨go(x1, a)⟩for all gj ∈G.
We will use a clever argument given in SCHAUENBURG (2007). The proof has
two steps:
Step 1: Prove that gj(x1, a) = 0 when gj ∈G satisﬁes deg(gj, x1) < deg(go, x1).
Step 2: Prove that gj(x1, a) ∈⟨go(x1, a)⟩by induction on deg(gj, x1), with Step 1
as the base case.
For Step 1, we set do = deg(go, x1). Our choice of go implies that go does not
drop x1-degree when evaluated at a, but any gj ∈G with deg(gj, x1) < do either
drops x1-degree or vanishes identically when evaluated at a.
Suppose there are some gj ∈G with deg(gj, x1) < do and gj(x1, a) ̸= 0. Among
all such gj's, let gb be one that minimizes the degree drop when evaluated at a. We
use the letter "b" for the index to indicate that gb is bad with respect to evaluation.
Our goal is to show the existence of this bad gb leads to a contradiction.
If we set δ = deg(gb, x1) −deg(gb(x1, a)), then gb drops degree by δ when
evaluated at a, and any other gj ∈G with deg(gj, x1) < do either vanishes identically
or drops degree by at least δ when evaluated at a.
Let db = deg(gb, x1), so that deg(gb(x1, a)) = db −δ. Then consider
S = coxdo−db
1
gb −cbgo ∈I
= coxdo−db
1
(cbxdb
1 + · · · ) −cb(coxdo
1 + · · · ).
The second line makes it clear that deg(S, x1) < do. We will compute deg(S(x1, a))
in two ways and show that this leads to a contradiction.
The ﬁrst way to compute the degree is to evaluate directly at a. This gives
S(x1, a) = co(a) xdo−db
1
gb(x1, a) −cb(a) go(x1, a) = co(a) xdo−db
1
gb(x1, a)
since cb(a) = 0 (gb drops degree when evaluated at a). Then co(a) ̸= 0 (by the
deﬁnition of go) implies that
(3)
deg(S(x1, a)) = do −db + deg(gb(x1, a)) = do −db + db −δ = do −δ.
The second way begins with a standard representation S = t
j=1 Bjgj. When we
combine part (i) of Lemma 1 with deg(S, x1) < do, we get the inequality
(4)
deg(Bj, x1) + deg(gj, x1) = deg(Bjgj, x1) ≤deg(S, x1) < do
when Bj ̸= 0. Thus the gj's that appear satisfy deg(gj, x1) < do, so that either
gj(x1, a) = 0 or the x1-degree of gj drops by at least δ when we evaluate at a. Hence
deg(Bj(x1, a)) + deg(gj(x1, a)) ≤deg(Bj, x1) + deg(gj, x1) −δ < do −δ,
where the ﬁnal inequality uses (4). It follows that when we evaluate S = t
j=1 Bjgj
at a, we obtain
deg(S(x1, a)) ≤max(deg(Bj(x1, a)) + deg(gj(x1, a))) < do −δ.

158
Chapter 3 Elimination Theory
This contradicts (3), and Step 1 is proved.
For Step 2, we prove by induction on deg(gj, x1) that gj(x1, a) ∈⟨go(x1, a)⟩
for all gj ∈G. The base case is when deg(gj, x1) < do. Here, Step 1 implies that
gj(x1, a) = 0, which obviously lies in ⟨go(x1, a)⟩.
For the inductive step, ﬁx d ≥do and assume that the assertion is true for all
gj ∈G with deg(gj, x1) < d. Then take gj ∈G with deg(gj, x1) = d and consider
the polynomial
S = cogj −cjxd−do
1
go ∈I
= co(cjxd
1 + · · · ) −cjxd−do
1
(coxdo
1 + · · · ).
The second line makes it clear that deg(S, x1) < d.
Taking a standard representation S = t
ℓ=1 Bℓgℓand arguing as in (4) shows
that deg(gℓ, x1) < d when Bℓ̸= 0. By our inductive hypothesis, we have gℓ(x1, a) ∈
⟨go(x1, a)⟩when Bℓ̸= 0. Then
cogj = cjxd−do
1
go + S = cjxd−do
1
go +
t

ℓ=1
Bℓgℓ
implies that
co(a)gj(x1, a) = cj(a)xd−do
1
go(x1, a) +
t

ℓ=1
Bℓ(x1, a)gℓ(x1, a) ∈⟨go(x1, a)⟩.
Since co(a) ̸= 0, we conclude that gj(x1, a) ∈⟨go(x1, a)⟩. This proves the inductive
step and completes the proof of Step 2.
□
In concrete terms, equation (1) of Theorem 2 has the following interpretation.
Suppose that I = ⟨f1, . . . , fs⟩, so that V(I) is deﬁned by f1 = · · · = fs = 0. If
a ∈V(I1) is a partial solution satisfying the hypothesis of the theorem, then when
we evaluate the system at a, it reduces to the single equation go(x1, a) = 0, and we
can ﬁnd go by computing a lex Gröbner basis of I.
For an example of how this works, consider I = ⟨x2y + xz + 1, xy −xz2 + z −1⟩
in C[x, y, z]. A Gröbner basis G of I for lex order with x > y > z consists of the four
polynomials
(5)
g1 = x2z2 + x + 1,
g2 = xy −xz2 + z −1 = (y −z2)x + z −1,
g3 = xz3 −xz2 −y + z2 + z −1 = (z3 −z2)x −y + z2 + z −1,
g4 = y2 −2yz2 −yz + y + 2z4 −z3.
Thus I1 = ⟨g4⟩= ⟨y2 −2yz2 −yz + y + 2z4 −z3⟩⊆C[y, z]. In the notation of
Theorem 2,
c1 = z2, c2 = y −z2, c3 = z2(z −1), c4 = g4.

§5 Gröbner Bases and the Extension Theorem
159
Since
V(c1, c2, c3, c4) = V(z2, y −z2, z2(z −1), y2 −2yz2 −yz + y + 2z4 −z3) = {(0, 0)},
we see that if (b, c) ∈V(I1) ⊆C2 is a partial solution different from (0, 0), then
{f(x, b, c) | f ∈I} = ⟨go(x, b, c)⟩⊆C[x],
where go ∈G is the polynomial from Theorem 2.
Let us compute go for (1, 1) ∈V(I1). Here, c1(1, 1) ̸= 0, while c2, c3, c4 all
vanish at (1, 1). Hence go = g1. Note that
g2(x, 1, 1) = g3(x, 1, 1) = g4(x, 1, 1) = 0,
as predicted by Step 1 of the proof of Theorem 2 since deg(gi, x) < 2 = deg(g1, x)
for i = 2, 3, 4. Since go = g1 = x2z2 + x + 1, we obtain
{f(x, 1, 1) | f ∈I} = ⟨g1(x, 1, 1)⟩= ⟨x2 + x + 1⟩.
For the partial solution (0, 1
2) ∈V(I1), the situation is a bit different. Here, (5) gives
g1(x, 0, 1
2) = 1
4x2 + x + 1,
g2(x, 0, 1
2) = −1
4x −1
2,
g3(x, 0, 1
2) = −1
8x −1
4,
g4(x, 0, 1
2) = 0.
The third polynomial is a constant multiple of the second, so that go can be taken to
be g2 or g3. Then part (ii) of Theorem 2 tells us that when trying to extend (0, 1
2) ∈
V(I1) to a solution of the original system, there is only one equation to solve, namely
go(x, 0, 1
2) = 0. In particular, we can ignore the quadratic polynomial g1(x, 0, 1
2)
since it is a multiple of go(x, 0, 1
2) by equation (1) of Theorem 2.
We are now ready to give our ﬁrst proof of the Extension Theorem. In the original
statement of the theorem, we worked over the ﬁeld C. Here we prove a more general
version that holds for any algebraically closed ﬁeld.
Theorem 3 (The Extension Theorem). Let I = ⟨f1, . . . , fs⟩⊆k[x1, . . . , xn] and let
I1 be the ﬁrst elimination ideal of I. For each 1 ≤i ≤s, write fi in the form
fi = ci(x2, . . . , xn)xNi
1 + terms in which x1 has degree < Ni,
where Ni ≥0 and ci ∈k[x2, . . . , xn] is nonzero. Suppose that we have a partial
solution (a2, . . . , an) ∈V(I1). If (a2, . . . , an) /∈V(c1, . . . , cs) and k is algebraically
closed, then there exists a1 ∈k such that (a1, . . . , an) ∈V(I).
Proof. Let G = {g1, . . . , gt} be a lex Gröbner basis of I with x1 > · · · > xn and set
a = (a2, . . . , an). We ﬁrst show that there is gj ∈G such that cgj(a) ̸= 0. Here, we
are using the notation introduced in the discussion leading up to Lemma 1.

160
Chapter 3 Elimination Theory
Our hypothesis implies that ci(a) ̸= 0 for some i. Take a standard representation
fi = t
j=1 Ajgj of fi. Since ci = cfi and Ni = deg( fi, x1), part (ii) of Lemma 1 yields
ci =

deg(Ajgj,x1)=Ni
cAjcgj.
Then ci(a) ̸= 0 implies that cgj(a) ̸= 0 for at least one gj appearing in the sum.
Hence we can apply Theorem 2. This gives go ∈G with deg(go(x1, a)) > 0
by part (i) of the theorem. Then there is a1 ∈k with go(a1, a) = 0 since k is
algebraically closed. By part (ii) of Theorem 2, (a1, a) ∈V(I), and the Extension
Theorem is proved.
□
EXERCISES FOR §5
1. As in Lemma 1, let f = t
j=1 Ajgj be a standard representation and set N = deg( f , x1).
a. Prove that N ≥deg(Ajgj, x1) when Ajgj ̸= 0. Hint: Recall that multideg( f ) ≥
multideg(Ajgj) when Ajgj ̸= 0. Then explain why the ﬁrst entry in multideg( f ) is
deg( f , x1).
b. Prove that
cf =

deg(Ajgj,x1)=N
cAjcgj.
Hint: Use part (a) and compare the coefﬁcients of xN
1 in f = t
j=1 Ajgj.
2. Suppose that k is a ﬁeld and ϕ : k[x1, . . . , xn] →k[x1] is a ring homomorphism that is the
identity on k and maps x1 to x1. Given an ideal I ⊆k[x1, . . . , xn], prove that ϕ(I) ⊆k[x1]
is an ideal. (In the proof of Theorem 2, we use this result when ϕ is the map that evaluates
xi at ai for 2 ≤i ≤n.)
3. In the proof of Theorem 2, show that (1) follows from the assertion that gj(x1, a) ∈
⟨go(x1, a)⟩for all gj ∈G.
4. This exercise will explore the example I = ⟨x2y + xz + 1, xy −xz2 + z −1⟩discussed in
the text.
a. Show that the partial solution (b, c) = (0, 0) does not extend to a solution (a, 0, 0) ∈
V(I).
b. In the text, we showed that go = g1 for the partial solution (1, 1). Show that go = g3
works for all partial solutions different from (1, 1) and (0, 0).
5. Evaluation at a is sometimes called specialization. Given I ⊆k[x1, . . . , xn] with lex Gröb-
ner basis G = {g1, . . . , gt}, we get the specialized basis {g1(x1, a), . . . , gt(x1, a)}. Dis-
carding the polynomials the specialize to zero, we get G′ = {gj(x1, a) | gj(x1, a) ̸= 0}.
a. Show that G′ is a basis of the ideal {f (x1, a) | f ∈I} ⊆k[x1].
b. If in addition a ∈V(I1) is a partial solution satisfying the hypothesis of Theorem 2,
prove that G′ is a Gröbner basis of {f (x1, a) | f ∈I}.
The result of part (b) is an example of a specialization theorem for Gröbner bases. We will
study the specialization of Gröbner bases in more detail in Chapter 6.
6. Show that Theorem 2 remains true if we replace lex order for x1 > · · · > xn with any
monomial order for which x1 is greater than all monomials in x2, . . . , xn. This is an order
of 1-elimination type in the terminology of Exercise 5 of §1. Hint: You will need to show
that Lemma 1 of this section holds for such monomial orders.
7. Use the strategy explained in the discussion following Theorem 2 to ﬁnd all solutions of
the system of equations given in Example 3 of Chapter 2, §8.

§6 Resultants and the Extension Theorem
161
§6
Resultants and the Extension Theorem
So far, we have presented elimination theory from the point of view of Gröbner
bases. Here, we will prove the Extension Theorem using a classical approach to
elimination theory based on the theory of resultants.
We introduce the concept of resultant by asking when two polynomials in k[x]
have a common factor. This might seem far removed from elimination, but we will
see the connection by the end of the section, where we use resultants to construct
elements of elimination ideals and give a second proof of the Extension Theorem.
Resultants will reappear in §7 of Chapter 8 when we prove Bezout's Theorem.
This section can be read independently of §5.
Resultants
We begin with the question of whether two polynomials f, g ∈k[x] have a common
factor, which means a polynomial h ∈k[x] of degree > 0 that divides f and g. One
approach would be to compute the gcd of f and g using the Euclidean Algorithm
from Chapter 1. A drawback is that the Euclidean Algorithm requires divisions in
the ﬁeld k. As we will see later, this is something we want to avoid when doing
elimination. Is there a way of determining whether a common factor exists without
doing any divisions in k? Here is a ﬁrst answer.
Lemma 1. Let f, g ∈k[x] be polynomials of degrees l > 0 and m > 0, respectively.
Then f and g have a common factor in k[x] if and only if there are polynomials
A, B ∈k[x] such that:
(i) A and B are not both zero.
(ii) A has degree at most m −1 and B has degree at most l −1.
(iii) Af + Bg = 0.
Proof. First, assume that f and g have a common factor h ∈k[x]. Then f = hf 1 and
g = hg1, where f1, g1 ∈k[x]. Note that f1 has degree at most l −1, and similarly
deg(g1) ≤m −1. Then
g1 · f + (−f1) · g = g1 · hf 1 −f1 · hg1 = 0.
and, thus, A = g1 and B = −f1 have the required properties.
Conversely, suppose that A and B have the above three properties. By (i), we may
assume B ̸= 0. If f and g have no common factor, then their gcd is 1, so we can ﬁnd
polynomials ˜A, ˜B ∈k[x] such that ˜Af + ˜Bg = 1 (see Proposition 6 of Chapter 1, §5).
Now multiply by B and use Bg = −Af:
B = B(˜Af + ˜Bg) = ˜ABf + ˜BBg = ˜ABf + ˜B(−Af) = (˜AB −˜BA)f.
Since B is nonzero, this equation shows that B has degree at least l, contradicting (ii).
Hence, A and B must have a common factor of positive degree.
□

162
Chapter 3 Elimination Theory
The answer given by Lemma 1 may not seem very satisfactory, for we still need
to decide whether the required A and B exist. Remarkably, we can use linear algebra
to answer this last question. The idea is to turn Af + Bg = 0 into a system of linear
equations. We begin by writing
A = u0xm−1 + · · · + um−1,
B = v0xl−1 + · · · + vl−1,
where for now we will regard the l + m coefﬁcients u0, . . . , um−1, v0, . . . , vl−1 as
unknowns. Our goal is to ﬁnd ui, vi ∈k, not all zero, so that the equation
(1)
Af + Bg = 0
holds. Note that this will automatically give us A and B as required in Lemma 1.
To get a system of linear equations, let us also write out f and g:
f = c0xl + · · · + cl,
c0 ̸= 0,
g = d0xm + · · · + dm,
d0 ̸= 0,
where ci, di ∈k. If we substitute the formulas for f, g, A, and B into equation (1) and
compare the coefﬁcients of powers of x, then we get the following system of linear
equations with unknowns ui, vi and coefﬁcients ci, di in k:
(2)
c0u0
+
d0v0
= 0
coefﬁcient of xl+m−1
c1u0 + c0u1
+
d1v0 + d0v1
= 0
coefﬁcient of xl+m−2
...
...
...
...
clum−1
+
dmvl−1 = 0
coefﬁcient of x0.
Since there are l + m linear equations and l + m unknowns, we know from linear
algebra that there is a nonzero solution if and only if the coefﬁcient matrix has zero
determinant. This leads to the following deﬁnition.
Deﬁnition 2. Given nonzero polynomials f, g ∈k[x] of degree l, m, respectively,
write them as
f = c0xl + · · · + cl,
c0 ̸= 0,
g = d0xm + · · · + dm,
d0 ̸= 0.
If l, m > 0, then the Sylvester matrix of f and g with respect to x, denoted
Syl( f, g, x), is the coefﬁcient matrix of the system of equations given in (2). Thus,
Syl( f, g, x) is the following (l + m) × (l + m) matrix:

§6 Resultants and the Extension Theorem
163
Syl( f, g, x) =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
c0
c1
c0
c2
c1
...
...
...
c0
...
c1
cl
cl
...
...
cl



m columns
d0
d1
d0
d2
d1
...
...
...
d0
...
d1
dm
dm
...
...
dm
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠



l columns
,
where the empty spaces are ﬁlled by zeros. The resultant of f and g with respect to
x, denoted Res( f, g, x), is the determinant of the Sylvester matrix. Thus,
Res( f, g, x) = det(Syl( f, g, x)).
Finally, when f, g do not both have positive degree, we deﬁne:
(3)
Res(c0, g, x) = cm
0 , when c0 ∈k \ {0}, m > 0,
Res( f, d0, x) = dl
0, when d0 ∈k \ {0}, l > 0,
Res(c0, d0, x) = 1, when c0, d0 ∈k \ {0}.
To understand the formula for Res(c0, g, x), notice that Syl(c0, g, x) reduces to an
m × m diagonal matrix with c0's on the diagonal, and similarly for Res( f, d0, x).
From this deﬁnition, we get the following properties of the resultant. A polyno-
mial is called an integer polynomial provided that all of its coefﬁcients are integers.
Proposition 3. Given nonzero f, g ∈k[x], the resultant Res( f, g, x) ∈k is an integer
polynomial in the coefﬁcients of f and g. Furthermore, f and g have a common factor
in k[x] if and only if Res( f, g, x) = 0.
Proof. The determinant of an s × s matrix A = (aij)l≤i,j≤s is given by the formula
det(A) =

σ a permutation
of {1,...,s}
sgn(σ) a1σ(1) · a2σ(2) · · · asσ(s),
where sgn(σ) is +1 if σ interchanges an even number of pairs of elements of
{1, . . ., s} and −1 if σ interchanges an odd number of pairs (see Appendix A, §4
for more details). This shows that the determinant is an integer polynomial (in fact,
the coefﬁcients are ±1) in its entries, and the ﬁrst statement of the proposition then
follows immediately from the deﬁnition of resultant when f, g have positive degree.
The formulas (3) take care of the remaining cases.

164
Chapter 3 Elimination Theory
The second statement is easy to prove when f, g have positive degree: the re-
sultant is zero ⇔the coefﬁcient matrix of equations (2) has zero determinant ⇔
equations (2) have a nonzero solution. We observed earlier that this is equivalent to
the existence of A and B as in Lemma 1, and then Lemma 1 completes the proof in
this case.
When f or g is a nonzero constant, Res( f, g, x) ̸= 0 by (3), and f and g cannot
have a common factor since by deﬁnition common factors have positive degree. □
As an example, let us see if f = 2x2 +3x+1 and g = 7x2 +x+3 have a common
factor in Q[x]. One computes that
Res( f, g, x) = det
⎛
⎜
⎜
⎝
2
0
7
0
3
2
1
7
1
3
3
1
0
1
0
3
⎞
⎟
⎟
⎠= 153 ̸= 0,
so that there is no common factor.
Here is an important consequence of Proposition 3.
Corollary 4. If f, g ∈C[x] are nonzero, then Res( f, g, x) = 0 if and only if f and g
have a common root in C. Furthermore, we can replace C with any algebraically
closed ﬁeld k.
Proof. Since C is algebraically closed, f and g have a common factor in C[x] of
positive degree if and only if they have a common root in C. Then we are done by
Proposition 3. This argument clearly works over any algebraically closed ﬁeld.
□
One disadvantage to using resultants is that large determinants are hard to com-
pute. In the exercises, we will explain an alternate method for computing resultants
that is similar to the Euclidean Algorithm. Most computer algebra systems have a
resultant command that implements this algorithm.
To link resultants to elimination, let us compute the resultant of the polynomials
f = xy −1 and g = x2 + y2 −4. Regarding f and g as polynomials in x whose
coefﬁcients are polynomials in y, we get
Res( f, g, x) = det
⎛
⎝
y
0
1
−1
y
0
0
−1
y2 −4
⎞
⎠= y4 −4y2 + 1.
This eliminates x, but how does this relate to elimination that we did in §§1 and 2?
In particular, is Res( f, g, x) = y4−4y2+1 in the ﬁrst elimination ideal ⟨f, g⟩∩k[y]?
The full answer will come later in the section and will use the following result.
Proposition 5. Given nonzero f, g ∈k[x], there are polynomials A, B ∈k[x] such
that
Af + Bg = Res( f, g, x).
Furthermore, if at least one of f, g has positive degree, then the coefﬁcients of A and
B are integer polynomials in the coefﬁcients of f and g.

§6 Resultants and the Extension Theorem
165
Proof. The deﬁnition of resultant was based on the equation Af + Bg = 0. In this
proof, we will apply the same methods to the equation
(4)
˜Af + ˜Bg = 1.
The reason for using ˜A, ˜B rather than A, B will soon become apparent.
The proposition is trivially true if Res( f, g, x) = 0 (simply choose A = B = 0).
If f = c0 ∈k and m = deg(g) > 0, then by (3), we have
Res( f, g, x) = cm
0 = cm−1
0
· f + 0 · g,
and the case l = deg( f) > 0, g = d0 ∈k is handled similarly.
Hence we may assume that f, g have positive degree and satisfy Res( f, g, x) ̸= 0.
Now let
f = c0xl + · · · + cl,
c0 ̸= 0,
g = d0xm + · · · + dm,
d0 ̸= 0,
˜A = u0xm−1 + · · · + um−1,
˜B = v0xl−1 + · · · + vl−1,
where u0, . . . , um−1, v0, . . . , vl−1 are unknowns in k. Equation (4) holds if and only
if substituting these formulas into (4) gives an equality of polynomials. Compar-
ing coefﬁcients of powers of x, we conclude that (4) is equivalent to the following
system of linear equations with unknowns ui, vi and coefﬁcients ci, di in k:
(5)
c0u0
+
d0v0
= 0
coefﬁcient of xl+m−1
c1u0 + c0u1
+
d1v0 + d0v1
= 0
coefﬁcient of xl+m−2
...
...
...
...
clum−1
+
dmvl−1 = 1
coefﬁcient of x0.
These equations are the same as (2) except for the 1 on the right-hand side of the last
equation. Thus, the coefﬁcient matrix is the Sylvester matrix of f and g, and then
Res( f, g, x) ̸= 0 guarantees that (5) has a unique solution in k.
In this situation, we use Cramer's Rule to give a formula for the unique solution.
Cramer's Rule states that the ith unknown is a ratio of two determinants, where the
denominator is the determinant of the coefﬁcient matrix and the numerator is the
determinant of the matrix where the ith column of the coefﬁcient matrix has been
replaced by the right-hand side of the equation. For a more precise statement of
Cramer's rule, the reader should consult Appendix A, §4. In our case, Cramer's rule
gives formulas for the ui's and vi's. For example, the ﬁrst unknown u0 is given by
the formula

166
Chapter 3 Elimination Theory
u0 =
1
Res( f, g, x) det
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
d0
0
c0
...
...
...
...
...
...
d0
0
cl
c0
dm
...
...
...
...
...
...
1
cl
dm
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Since a determinant is an integer polynomial in its entries, it follows that
u0 = an integer polynomial in ci, di
Res( f, g, x)
.
All of the ui's and vi's can be written this way. Since ˜A = u0xm−1 + · · · + um−1, we
can pull out the common denominator Res( f, g, x) and write ˜A in the form
˜A =
1
Res( f, g, x) A,
where A ∈k[x] and the coefﬁcients of A are integer polynomials in ci, di. Similarly,
we can write
˜B =
1
Res( f, g, x) B,
where B ∈k[x] has the same properties as A. Since ˜A and ˜B satisfy ˜Af + ˜Bg = 1,
we can multiply through by Res( f, g, x) to obtain
Af + Bg = Res( f, g, x).
Since A and B have the required kind of coefﬁcients, the proposition is proved.
□
Most courses in linear algebra place little emphasis on Cramer's rule, mainly
because Gaussian elimination is much more efﬁcient than Cramer's rule from a
computational point of view. But for theoretical uses, where one needs to worry
about the form of the solution, Cramer's rule is very important.
We can now explain the relation between the resultant and the gcd. Given f, g ∈
k[x], Res( f, g, x) ̸= 0 tells us that f and g have no common factor, and hence their
gcd is 1. Then Proposition 6 of Chapter 1, §5 says that there are ˜A and ˜B such that
˜Af + ˜Bg = 1. As the above formulas for ˜A and ˜B make clear, the coefﬁcients of ˜A
and ˜B have a denominator given by the resultant. Then clearing these denominators
leads to Af + Bg = Res( f, g, x).
To see this more explicitly, let us return to the case of f = xy −1 and g = x2 +
y2 −4. If we regard these as polynomials in x, then we computed that Res( f, g, x) =
y4 −4y2 + 1 ̸= 0. Thus, their gcd is 1, and we leave it as an exercise to check that
−

y
y4 −4y2 + 1x +
1
y4 −4y2 + 1

f +
y2
y4 −4y2 + 1g = 1.

§6 Resultants and the Extension Theorem
167
This equation takes place in k(y)[x], i.e., the coefﬁcients are rational functions in y,
because the gcd theory from Chapter 1, §5 requires ﬁeld coefﬁcients. If we want to
work in k[x, y], we must clear denominators, which leads to the equation
(6)
−(yx + 1)f + y2g = y4 −4y2 + 1.
This, of course, is just a special case of Proposition 5. Hence, we can regard the
resultant as a "denominator-free" version of the gcd.
The Extension Theorem
Our ﬁnal task is to use resultants to give a second proof of the Extension Theorem.
We begin with nonzero polynomials f, g ∈k[x1, . . . , xn] of degree l, m in x1, written
(7)
f = c0xl
1 + · · · + cl,
c0 ̸= 0,
g = d0xm
1 + · · · + dm,
d0 ̸= 0,
where now ci, di ∈k[x2, . . . , xn]. We set x1-deg( f) = l and x1-deg(g) = m. By
Proposition 3,
Res( f, g, x1) = det(Syl( f, g, x1))
is a polynomial in k[x2, . . . , xn] since ci, di ∈k[x2, . . . , xn]. If at least one of f, g has
positive x1-degree, then Proposition 5 implies that
Res( f, g, x1) = Af + Bg
with A, B ∈k[x1, . . . , xn]. It follows that
(8)
h = Res( f, g, x1) ∈⟨f, g⟩∩k[x2, . . . , xn].
Hence Res( f, g, x1) lies in the ﬁrst elimination ideal of ⟨f, g⟩. In particular, this
answers the question posed before Proposition 5.
We will prove the Extension Theorem by studying the interaction between re-
sultants and partial solutions. Substituting a = (a2, . . . , an) ∈kn−1 into h =
Res( f, g, x1) gives a specialization of the resultant. However, this need not equal
the resultant of the specialized polynomials f(x1, a) and g(x1, a). See Exercises 12
and 13 for some examples. Fortunately, there is one situation where the relation
between the two resultants is easy to state.
Proposition 6. Let nonzero f, g ∈k[x1, . . . , xn] have x1-degree l, m, respectively,
and assume that a = (a2, . . . , an) ∈kn−1 satisﬁes the following:
(i) f(x1, a) ∈k[x1] has degree l.
(ii) g(x1, a) ∈k[x1] is nonzero of degree p ≤m.
If c0 is as in (7), then the polynomial h = Res( f, g, x1) ∈k[x2, . . . , xn] satisﬁes
h(a) = c0(a)m−p Res( f(x1, a), g(x1, a), x1).

168
Chapter 3 Elimination Theory
Proof. We ﬁrst consider the case when l, m > 0. If we substitute a = (a2, . . . , an)
for x2, . . . , xn in the determinantal formula for h = Res( f, g, x1), we obtain
(9)
h(a) = det
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
c0(a)
...
...
...
c0(a)
cl(a)
...
...
...
cl(a)



m columns
d0(a)
...
...
...
d0(a)
dm(a)
...
...
...
dm(a)
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠



l columns
.
If g(x1, a) has degree p = m, then our assumptions imply that
f(x1, a) = c0(a)xl
1 + · · · + cl(a),
c0(a) ̸= 0,
g(x1, a) = d0(a)xm
1 + · · · + dm(a),
d0(a) ̸= 0.
Hence the above determinant is the resultant of f(x1, a) and g(x1, a), so that
h(a) = Res( f(x1, a), g(x1, a), x1).
This proves the proposition when p = m. When p < m, the determinant (9) is no
longer the resultant of f(x1, a) and g(x1, a) (it has the wrong size). Here, we get the
desired resultant by repeatedly expanding by minors along the ﬁrst row. We leave
the details to the reader (see Exercise 14).
Next suppose that l = 0 and m > 0. Then f(x1, a) = c0(a) ̸= 0, so by (3),
Res( f(x1, a), g(x1, a), x1) =

c0(a)p
p > 0
1
p = 0.
Since the determinant (9) reduces to h(a) = c0(a)m when l = 0, the desired equation
holds in this case. The proof when l > 0 and m = 0 is similar. Finally, when
l = m = 0, our hypotheses imply that c0(a), d0(a) ̸= 0, and then our desired
equation reduces to 1 = 1 by (3).
□
Over the complex numbers (or, more generally, over any algebraically closed
ﬁeld), we have the following corollary of Proposition 6.
Corollary 7. Let nonzero f, g ∈C[x1, . . . , xn] have x1-degree l, m, respectively, and
assume that a = (a2, . . . , an) ∈Cn−1 satisﬁes the following:
(i) f(x1, a) ∈k[x1] has degree l or g(x1, a) ∈k[x1] has degree m.
(ii) Res( f, g, x1)(a) = 0.
Then there exists a1 ∈C such that f(a1, a) = g(a1, a) = 0. Furthermore, we can
replace C with any algebraically closed ﬁeld k.

§6 Resultants and the Extension Theorem
169
Proof. We give the proof in the case of an algebraically closed ﬁeld k. Switching f
and g if necessary, we may assume that f(x1, a) has degree l, so that c0(a) ̸= 0. If
g(x1, a) ̸= 0, then Proposition 6 implies that
0 = Res( f, g, x1)(a) = c0(a)m−pRes( f(x1, a), g(x1, a), x1),
where p is the degree of g(x1, a). Thus Res( f(x1, a), g(x1, a), x1) = 0. Since
f(x1, a), g(x1, a) are nonzero, the desired a1 ∈k exists by Corollary 4.
It remains to study what happens when g(x1, a) = 0. If l > 0, then f(x1, a) has
positive degree, so any root a1 ∈k of f(x1, a) will work. On the other hand, if l = 0,
then (3) implies that Res( f, g, x1) = cm
0 , which does not vanish at a since c0(a) ̸= 0.
Hence this case cannot occur.
□
We can now prove the Extension Theorem. In §1, we stated the result over C.
Here is a more general version that holds over any algebraically closed ﬁeld k.
Theorem 8 (The Extension Theorem). Let I = ⟨f1, . . . , fs⟩⊆k[x1, . . . , xn] and let
I1 be the ﬁrst elimination ideal of I. For each 1 ≤i ≤s, write fi in the form
fi = ci(x2, . . . , xn) xNi
1 + terms in which x1 has degree < Ni,
where Ni ≥0 and ci ∈k[x2, . . . , xn] is nonzero. Suppose that we have a partial
solution (a2, . . . , an) ∈V(I1). If (a2, . . . , an) /∈V(c1, . . . , cs) and k is algebraically
closed, then there exists a1 ∈k such that (a1, a2, . . . , an) ∈V(I).
Proof. The ﬁrst part of the argument repeats material from the proof of Theorem 2
of §5. We reproduce it verbatim for the convenience of readers who skipped §5.
Set a = (a2, . . . , an) and consider the function
(10)
k[x1, . . . , xn] −→k[x1]
deﬁned by evaluation at a, i.e., f(x1, x2, . . . , xn) 
→f(x1, a). Since evaluation is
compatible with addition and multiplication of polynomials, (10) is what we will
call a ring homomorphism in Chapter 5, §2. In Exercise 15, you will show that the
image of I under the ring homomorphism (10) is an ideal of k[x1]. By Corollary 4
of Chapter 1, §5, it follows that the image of I is generated by a single polynomial
u(x1) ∈k[x1]. Thus
{f(x1, a) | f ∈I} = ⟨u(x1)⟩.
In particular, there is f ∗∈I such that f ∗(x1, a) = u1(x1). Then we can rewrite the
above equation as
(11)
{f(x1, a) | f ∈I} = ⟨f ∗(x1, a)⟩.
Since a /∈V(c1, . . . , cs), we have ci(a) ̸= 0 for some i, and then fi(x1, a) has
positive x1-degree since a ∈V(I1). By (11), it follows that f ∗(x1, a) is nonzero.
We now apply our results to the polynomials fi, f ∗∈I. Since fi(x1, a) has positive
x1-degree, (8) implies that

170
Chapter 3 Elimination Theory
h = Res( fi, f ∗, x1) ∈⟨fi, f∗⟩∩k[x2, . . . , xn] ⊆I ∩k[x2, . . . , xn] = I1.
It follows that h(a) = 0 since a ∈V(I1). Since k is algebraically closed and fi
does not drop degree in x1 when evaluated at a, Corollary 7 gives us a1 ∈k with
fi(a1, a) = f ∗(a1, a) = 0. By (11), we see that f(a1, a) = 0 for all f ∈I, so
that (a1, a) = (a1, a2, . . . , an) ∈V(I). This completes the proof of the Extension
Theorem.
□
In addition to the resultant of two polynomials discussed here, the resultant of
three or more polynomials can be deﬁned. Readers interested in multipolynomial re-
sultants should consult MACAULAY (1902) or VAN DER WAERDEN (1931). A mod-
ern introduction to this theory can be found in COX, LITTLE and O'SHEA (2005). A
more sophisticated treatment of resultants is presented in JOUANOLOU (1991), and
a vast generalization of the concept of resultant is discussed in GELFAND, KAPRA-
NOV and ZELEVINSKY (1994).
EXERCISES FOR §6
1. Compute the resultant of x5 −3x4 −2x3 + 3x2 + 7x + 6 and x4 + x2 + 1. Do these
polynomials have a common factor in Q[x]? Explain your reasoning.
2. If f , g ∈Z[x], explain why Res( f , g, x) ∈Z.
3. Assume that f has degree l and g has degree m. Here are some properties of the resultant.
We assume that at least one of l, m is positive.
a. Prove that the resultant has the symmetry property
Res( f , g, x) = (−1)lmRes(g, f , x).
Be sure to take the cases l = 0 or m = 0 into account. Hint: A determinant changes
sign if you switch two columns.
b. If λ, μ ∈k are nonzero, then prove that
Res(λf , μg, x) = λmμl Res( f , g, x).
Again, be sure to consider the cases when l = 0 or m = 0.
c. When l = m = 0, show that part (a) is still true but part (b) can fail.
4. In §3, we mentioned that resultants are sometimes used to solve implicitization problems.
For a simple example of how this works, consider the curve parametrized by
u =
t2
1 + t2 ,
v =
t3
1 + t2 .
To get an implicit equation, form the equations
u(1 + t2) −t2 = 0,
v(1 + t2) −t3 = 0
and use an appropriate resultant to eliminate t. Then compare your result to the answer
obtained by the methods of §3. (Note that Exercise 13 of §3 is relevant.)
5. Consider the polynomials f = 2x2 + 3x + 1 and g = 7x2 + x + 3.
a. Use the Euclidean Algorithm (by hand, not computer) to ﬁnd the gcd of these poly-
nomials.
b. Find polynomials ˜A, ˜B ∈Q[x] such that ˜Af + ˜Bg = 1. Hint: Use the calculations you
made in part (a).

§6 Resultants and the Extension Theorem
171
c. In the equation you found in part (b), clear the denominators. How does this answer
relate to the resultant?
6. Let f = xy −1 and g = x2 + y2 −4. We will regard f and g as polynomials in x with
coefﬁcients in k(y).
a. With f and g as above, set up the system of equations (5) that describes ˜Af + ˜Bg = 1.
Hint: ˜A is linear and ˜B is constant. Thus, you should have three equations in three
unknowns.
b. Use Cramer's rule to solve the system of equations obtained in part (a). Hint: The
denominator is the resultant.
c. What equation do you get when you clear denominators in part (b)? Hint: See equa-
tion (6) in the text.
7. The last part of Proposition 5 requires that at least one of f , g ∈k[x] have positive degree
in x. Working in Q[x], explain why Res(2, 2, x) shows that this requirement is necessary.
8. The discussion of resultants in the text assumes that the polynomials f , g are nonzero.
For some purposes, however, it is useful to let f = 0 or g = 0. We deﬁne
Res(0, g, x) = Res( f , 0, x) = 0
for any f , g ∈k[x]. This deﬁnition will play an important role in Exercises 10 and 11. On
the other hand, it is not compatible with some of the results proved in the text. Explain
why Res(1, 0, x) = 0 implies that the "nonzero" hypothesis is essential in Corollary 4.
9. Let f = c0xl + · · · + cl and g = d0xm + · · · + dm be nonzero polynomials in k[x], and
assume that c0, d0 ̸= 0 and l ≥m.
a. Let ˜f = f −(c0/d0)xl−mg, so that deg( ˜f) ≤l −1. If deg( ˜f ) = l −1, then prove that
Res( f , g, x) = (−1)md0Res( ˜f, g, x).
Hint: Use column operations on the Sylvester matrix. You will subtract c0/d0 times
the ﬁrst m columns in the g part from the columns in the f part. Then expand by
minors along the ﬁrst row. [See Section 11.4 of DUMMIT and FOOTE (2004) for a
description of expansion by minors, also called cofactor expansion.]
b. Let ˜f be as in part (a), but this time we allow the possibility that the degree of ˜f could
be strictly smaller than l −1. If ˜f ̸= 0, prove that
Res( f , g, x) = (−1)m(l−deg( ˜f ))dl−deg( ˜f )
0
Res( ˜f, g, x).
Hint: The exponent l −deg( ˜f ) tells you how many times to expand by minors.
c. Now use the division algorithm to write f = qg + r in k[x], where deg(r) < deg(g)
or r = 0. In the former case, use part (b) to prove that
Res( f , g, x) = (−1)m(l−deg(r))dl−deg(r)
0
Res(r, g, x).
d. If the remainder r vanishes, Exercise 8 implies that Res(r, g, x) = Res(0, g, x) = 0.
Explain why the formula of part (c) correctly computes Res( f , g, x) in this case. Hint:
Proposition 3.
10. In this exercise and the next, we will modify the Euclidean Algorithm to give an algo-
rithm for computing resultants. The basic idea is the following: to ﬁnd the gcd of f and
g, we used the division algorithm to write f = qg + r, g = q′r + r′, etc. In equation (5)
of Chapter 1, §5, the equalities
gcd( f , g) = gcd(g, r) = gcd(r, r′) = · · ·
enabled us to compute the gcd since the degrees were decreasing. Use Exercises 3 and 9
to prove the following "resultant" version of the ﬁrst two equalities above:

172
Chapter 3 Elimination Theory
Res( f , g, x) = (−1)deg(g)(deg( f)−deg(r))ddeg( f)−deg(r)
0
Res(r, g, x)
= (−1)deg( f)deg(g)ddeg( f)−deg(r)
0
Res(g, r, x)
= (−1)deg( f)deg(g)+deg(r)(deg(g)−deg(r′))ddeg( f)−deg(r)
0
d′deg(g)−deg(r′)
0
Res(r′, r, x)
= (−1)deg( f)deg(g)+deg(g)deg(r)ddeg( f)−deg(r)
0
d′deg(g)−deg(r′)
0
Res(r, r′, x)
where d0 (resp. d′
0) is the leading coefﬁcient of g (resp. r). Continuing in this way, we
can reduce to the case where the second polynomial is constant, and then we can use (3)
to compute the resultant.
11. To turn the previous exercises into an algorithm, we will use pseudocode and two func-
tions: let r = remainder( f , g) be the remainder on division of f by g and let LC( f ) be
the leading coefﬁcient of f . We can now state the algorithm for ﬁnding Res( f , g, x):
Input : f , g ∈k[x] \ {0}
Output : res = Res( f , g, x)
h := f
s := g
res := 1
WHILE deg(s) > 0 DO
r := remainder(h, s)
res := (−1)deg(h)deg(s) LC(s)deg(h)−deg(r)res
h := s
s := r
IF h = 0 or s = 0 THEN res := 0 ELSE
IF deg(h) > 0 THEN res := sdeg(h)res
RETURN res
Prove that this algorithm computes the resultant of f and g. Hint: Use (3) and Exercises 3,
8, 9, and 10, and follow the proof of Proposition 6 of Chapter 1, §5.
12. In the discussion leading up to Proposition 6, we claimed that the specialization of a
resultant need not be the resultant of the specialized polynomials. Let us work out some
examples.
a. Let f = x2y + 3x −1 and g = 6x2 + y2 −4. Compute h = Res( f , g, x) and show
that h(0) = −180. But if we set y = 0 in f and g, we get the polynomials 3x −1 and
6x2 −4. Check that Res(3x −1, 6x2 −4, x) = −30. Thus, h(0) is not a resultant—it
is off by a factor of 6. Note why equality fails: h(0) is a 4 × 4 determinant, whereas
Res(3x −1, 6x2 −4, x) is a 3 × 3 determinant.
b. Now let f = x2y + 3xy −1 and g = 6x2 + y2 −4. Compute h = Res( f , g, x) and
verify that h(0) = 36. Setting y = 0 in f and g gives polynomials −1 and 6x2 −4.
Use (3) to show that the resultant of these polynomials is 1. Thus, h(0) is off by a
factor of 36.
When the degree of f drops by 1 [in part (a)], we get an extra factor of 6, and when it
drops by 2 [in part (b)], we get an extra factor of 36 = 62. And the leading coefﬁcient of
x in g is 6. In Exercise 14, we will see that this is no accident.
13. Let f = x2y + x −1 and g = x2y + x + y2 −4. If h = Res( f , g, x) ∈C[y], show that
h(0) = 0. But if we substitute y = 0 into f and g, we get x −1 and x −4. Show that
these polynomials have a nonzero resultant. Thus, h(0) is not a resultant.
14. In this problem you will complete the proof of Proposition 6 by determining what hap-
pens to a resultant when specializing causes the degree of one of the polynomials to

§6 Resultants and the Extension Theorem
173
drop. Let f , g ∈k[x1, . . . , xn] and set h = Res( f , g, x1). If a = (a2, . . . , an) ∈kn−1,
let f (x1, a) be the polynomial in k[x1] obtained by substituting in a. As in (7), let
c0, d0 ∈k[x2, . . . , xn] be the leading coefﬁcients of x1 in f , g, respectively. We will
assume that c0(a) ̸= 0 and d0(a) = 0, and our goal is to see how h(a) relates to
Res( f (x1, a), g(x1, a), x1).
a. First suppose that the degree of g drops by exactly 1, which means that d1(a) ̸= 0. In
this case, prove that
h(a) = c0(a) · Res( f (x1, a), g(x1, a), x1).
Hint: h(a) is given by the following determinant:
h(a) = det
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
c0(a)
c1(a)
c0(a)
c1(a)
...
...
...
c0(a)
...
c1(a)
cl(a)
cl(a)
...
...
cl(a)


 
m columns
0
d1(a)
0
d1(a)
...
...
...
0
...
d1(a)
dm(a)
dm(a)
...
...
dm(a)
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠


 
l columns
.
The determinant is the wrong size to be the resultant of f (x1, a) and g(x1, a).
If you expand by minors along the ﬁrst row [see Section 11.4 of DUMMIT and
FOOTE (2004)], the desired result will follow.
b. Now let us do the general case. Suppose that the degree of g(x1, a) is m −p, where
p ≥1. Then prove that
h(a) = c0(a)p · Res( f (x1, a), g(x1, a), x1).
Hint: Expand by minors p times. Note how this formula explains Exercise 12.
15. Suppose that k is a ﬁeld and ϕ : k[x1, . . . , xn] →k[x1] is a ring homomorphism that is the
identity on k and maps x1 to x1. Given an ideal I ⊆k[x1, . . . , xn], prove that ϕ(I) ⊆k[x1]
is an ideal. (In the proof of Theorem 8, we use this result when ϕ is the map that evaluates
xi at ai for 2 ≤i ≤n.)
16. If f = c0xl + · · · + cl ∈k[x], where c0 ̸= 0 and l > 0, then the discriminant of f is
disc( f ) = (−1)l(l−1)/2
c0
Res( f , f ′, x),
where f ′ is the formal derivative of f from Exercise 13 of Chapter 1, §5. When k = C,
prove that f has a multiple root if and only if disc( f ) = 0. Hint: See Exercise 14 of
Chapter 1, §5.
17. Use the previous exercise to determine whether or not 6x4 −23x3 + 32x2 −19x + 4 has
a multiple root in C. What is the multiple root?
18. Compute the discriminant of the quadratic polynomial f = ax2 + bx + c. Explain how
your answer relates to the quadratic formula, and, without using Exercise 16, prove that
f has a multiple root if and only if its discriminant vanishes.

174
Chapter 3 Elimination Theory
19. Suppose that f , g ∈C[x] are polynomials of positive degree. The goal of this problem is
to construct a polynomial whose roots are all sums of a root of f plus a root of g.
a. Show that a complex number γ ∈C can be written γ = α + β, where f (α) =
g(β) = 0, if and only if the equations f (x) = g(y −x) = 0 have a solution with
y = γ.
b. Using Proposition 6, show that γ is a root of Res( f (x), g(y −x), x) if and only if
γ = α + β, where f (α) = g(β) = 0.
c. Construct a polynomial with coefﬁcients in Q which has
√
2 +
√
3 as a root. Hint:
What are f and g in this case?
d. Modify your construction to create a polynomial whose roots are all differences of a
root of f minus a root of g.
20. Suppose that f , g ∈C[x] are polynomials of positive degree. If all of the roots of f are
nonzero, adapt the argument of Exercise 19 to construct a polynomial whose roots are
all products of a root of f times a root of g.
21. Suppose that I = ⟨f , g⟩⊆C[x, y] and assume that Res( f , g, x) ̸= 0. Prove that V(I1) =
π1(V), where V = V(I) and π1 is projection onto the y-axis. Hint: See Exercise 5 of §2.
This exercise is due to GALLET, RAHKOOY and ZAFEIRAKOPOULOS (2013).

Chapter 4
The Algebra-Geometry Dictionary
In this chapter, we will explore the correspondence between ideals and varieties.
In §§1 and 2, we will prove the Nullstellensatz, a celebrated theorem which iden-
tiﬁes exactly which ideals correspond to varieties. This will allow us to construct a
"dictionary" between geometry and algebra, whereby any statement about varieties
can be translated into a statement about ideals (and conversely). We will pursue this
theme in §§3 and 4, where we will deﬁne a number of natural algebraic operations
on ideals and study their geometric analogues. In keeping with the computational
emphasis of the book, we will develop algorithms to carry out the algebraic op-
erations. In §§5 and 6, we will study the more important algebraic and geometric
concepts arising out of the Hilbert Basis Theorem: notably the possibility of decom-
posing a variety into a union of simpler varieties and the corresponding algebraic
notion of writing an ideal as an intersection of simpler ideals. In §7, we will prove
the Closure Theorem from Chapter 3 using the tools developed in this chapter.
§1
Hilbert's Nullstellensatz
In Chapter 1, we saw that a variety V ⊆kn can be studied by passing to the ideal
I(V) = {f ∈k[x1, . . . , xn] | f(a) = 0 for all a ∈V}
of all polynomials vanishing on V. Hence, we have a map
afﬁne varieties
V
−→
ideals
I(V).
Conversely, given an ideal I ⊆k[x1, . . . , xn], we can deﬁne the set
V(I) = {a ∈kn | f(a) = 0 for all f ∈I}.
© Springer International Publishing Switzerland 2015
D.A. Cox et al., Ideals, Varieties, and Algorithms, Undergraduate Texts
in Mathematics, DOI 10.1007/978-3-319-16721-3_4
175

176
Chapter 4 The Algebra-Geometry Dictionary
The Hilbert Basis Theorem assures us that V(I) is actually an afﬁne variety, for
it tells us that there exists a ﬁnite set of polynomials f1, . . . , fs ∈I such that I =
⟨f1, . . . , fs⟩, and we proved in Proposition 9 of Chapter 2, §5 that V(I) is the set of
common roots of these polynomials. Thus, we have a map
ideals
I
−→
afﬁne varieties
V(I).
These two maps give us a correspondence between ideals and varieties. In this chap-
ter, we will explore the nature of this correspondence.
The ﬁrst thing to note is that this correspondence (more precisely, the map V) is
not one-to-one: different ideals can give the same variety. For example, ⟨x⟩and ⟨x2⟩
are different ideals in k[x] which have the same variety V(x) = V(x2) = {0}. More
serious problems can arise if the ﬁeld k is not algebraically closed. For example,
consider the three polynomials 1, 1 + x2, and 1 + x2 + x4 in R[x]. These generate
different ideals
I1 = ⟨1⟩= R[x],
I2 = ⟨1 + x2⟩,
I3 = ⟨1 + x2 + x4⟩,
but each polynomial has no real roots, so that the corresponding varieties are all
empty:
V(I1) = V(I2) = V(I3) = ∅.
Examples of polynomials in two variables without real roots include 1+x2 +y2 and
1 + x2 + y4. These give different ideals in R[x, y] which correspond to the empty
variety.
Does this problem of having different ideals represent the empty variety go away
if the ﬁeld k is algebraically closed? It does in the one-variable case when the ring is
k[x]. To see this, recall from §5 of Chapter 1 that any ideal I in k[x] can be generated
by a single polynomial because k[x] is a principal ideal domain. So we can write
I = ⟨f⟩for some polynomial f ∈k[x]. Then V(I) is the set of roots of f; i.e.,
the set of a ∈k such that f(a) = 0. But since k is algebraically closed, every
nonconstant polynomial in k[x] has a root. Hence, the only way that we could have
V(I) = ∅would be to have f be a nonzero constant. In this case, 1/f ∈k. Thus,
1 = (1/f) · f ∈I, which means that g = g · 1 ∈I for all g ∈k[x]. This shows
that I = k[x] is the only ideal of k[x] that represents the empty variety when k is
algebraically closed.
A wonderful thing now happens: the same property holds when there is more than
one variable. In any polynomial ring, algebraic closure is enough to guarantee that
the only ideal which represents the empty variety is the entire polynomial ring itself.
This is the Weak Nullstellensatz, which is the basis of (and is equivalent to) one
of the most celebrated mathematical results of the late nineteenth century, Hilbert's
Nullstellensatz. Such is its impact that, even today, one customarily uses the original
German name Nullstellensatz: a word formed, in typical German fashion, from three
simpler words: Null (= Zero), Stellen (= Places), Satz (= Theorem).

§1 Hilbert's Nullstellensatz
177
Theorem 1 (The Weak Nullstellensatz). Let k be an algebraically closed ﬁeld and
let I ⊆k[x1, . . . , xn] be an ideal satisfying V(I) = ∅. Then I = k[x1, . . . , xn].
Proof. Our proof is inspired by GLEBSKY (2012). We will prove the theorem in
contrapositive form:
I ⊊k[x1, . . . , xn] =⇒V(I) ̸= ∅.
We will make frequent use of the standard equivalence I = k[x1, . . . , xn] ⇔1 ∈I.
This is part (a) of Exercise 16 from Chapter 1, §4.
Given a ∈k and f ∈k[x1, . . . , xn], let ¯f = f(x1, . . . , xn−1, a) ∈k[x1, . . . , xn−1].
Similar to Exercise 2 of Chapter 3, §5 and Exercise 15 of Chapter 3, §6, the set
Ixn=a = {¯f | f ∈I}
is an ideal of k[x1, . . . , xn−1]. The key step in the proof is the following claim.
Claim. If k is algebraically closed and I ⊊k[x1, . . . , xn] is a proper ideal, then there
is a ∈k such that Ixn=a ⊊k[x1, . . . , xn−1].
Once we prove the claim, an easy induction gives elements a1, . . . , an ∈k such
that Ixn=an,...,x1=a1 ⊊k. But the only ideals of k are {0} and k (Exercise 3), so that
Ixn=an,...,x1=a1 = {0}. This implies (a1, . . . , an) ∈V(I). We conclude that V(I) ̸= ∅,
and the theorem will follow.
To prove the claim, there are two cases, depending on the size of I ∩k[xn].
Case 1. I ∩k[xn] ̸= {0}. Let f ∈I ∩k[xn] be nonzero, and note that f is nonconstant,
since otherwise 1 ∈I ∩k[xn] ⊆I, contradicting I ̸= k[x1, . . . , xn].
Since k is algebraically closed, f = c  r
i=1(xn −bi)mi where c, b1, . . . , br ∈k and
c ̸= 0. Suppose that Ixn=bi = k[x1, . . . , xn−1] for all i. Then for all i there is Bi ∈I
with Bi(x1, . . . , xn−1, bi) = 1. This implies that
1 = Bi(x1, . . . , xn−1, bi) = Bi(x1, . . . , xn−1, xn −(xn −bi)) = Bi + Ai(xn −bi)
for some Ai ∈k[x1, . . . , xn]. Since this holds for i = 1, . . . , r, we obtain
1 =
r!
i=1
(Ai(xn −bi) + Bi)mi = A
r!
i=1
(xn −bi)mi + B,
where A =  r
i=1 Ami
i
and B ∈I. This and  r
i=1(xn −bi)mi = c−1f ∈I imply that
1 ∈I, which contradicts I ̸= k[x1, . . . , xn]. Thus Ixn=bi ̸= k[x1, . . . , xn−1] for some i.
This bi is the desired a.
Case 2. I ∩k[xn] = {0}. Let {g1, . . . , gt} be a Gröbner basis of I for lex order with
x1 > · · · > xn and write
(1)
gi = ci(xn)xαi + terms < xαi,
where ci(xn) ∈k[xn] is nonzero and xαi is a monomial in x1, . . . , xn−1.

178
Chapter 4 The Algebra-Geometry Dictionary
Now pick a ∈k such that ci(a) ̸= 0 for all i. This is possible since algebraically
closed ﬁelds are inﬁnite by Exercise 4. It is easy to see that the polynomials
¯gi = gi(x1, . . . , xn−1, a)
form a basis of Ixn=a (Exercise 5). Substituting xn = a into equation (1), one easily
sees that LT(¯gi) = ci(a)xαi since ci(a) ̸= 0. Also note that xαi ̸= 1, since otherwise
gi = ci ∈I ∩k[xn] = {0}, yet ci ̸= 0. This shows that LT(¯gi) is nonconstant for all i.
We claim that the ¯gi form a Gröbner basis of Ixn=a. Assuming the claim, it follows
that 1 /∈Ixn=a since no LT(¯gi) can divide 1. Thus Ixn=a ̸= k[x1, . . . , xn−1], which is
what we want to show.
To prove the claim, take gi, gj ∈G and consider the polynomial
S = cj(xn) xγ
xαi gi −ci(xn) xγ
xαj gj,
where xγ = lcm(xαi, xαj). By construction, xγ > LT(S) (be sure you understand
this). Since S ∈I, it has a standard representation S = t
l=1 Algl. Then evaluating
at xn = a gives
cj(a) xγ
xαi ¯gi −ci(a) xγ
xαj ¯gj = S = t
l=1 ¯Al¯gl.
Since LT(¯gi) = ci(a)xαi, we see that S is the S-polynomial S(¯gi, ¯gj) up to the
nonzero constant ci(a)cj(a). Then
xγ > LT(S) ≥LT(Algl),
Algl ̸= 0
implies that
xγ > LT(¯Al¯gl),
¯Al¯gl ̸= 0
(Exercise 6). Since xγ = lcm(LM(¯gi), LM(¯gj)), it follows that S(¯gi, ¯gj) has an lcm
representation for all i, j and hence is a Gröbner basis by Theorem 6 of Chapter 2,
§9. This proves the claim and completes the proof of the theorem.
□
In the special case when k = C, the Weak Nullstellensatz may be thought of
as the "Fundamental Theorem of Algebra for multivariable polynomials"—every
system of polynomials that generates an ideal strictly smaller than C[x1, . . . , xn] has
a common zero in Cn.
The Weak Nullstellensatz also allows us to solve the consistency problem from
§2 of Chapter 1. Recall that this problem asks whether a system
f1 = 0,
f2 = 0,
...
fs = 0

§1 Hilbert's Nullstellensatz
179
of polynomial equations has a common solution in Cn. The polynomials fail to have
a common solution if and only if V( f1, . . . , fs) = ∅. By the Weak Nullstellensatz, the
latter holds if and only if 1 ∈⟨f1, . . . , fs⟩. Thus, to solve the consistency problem,
we need to be able to determine whether 1 belongs to an ideal. This is made easy
by the observation that for any monomial ordering, {1} is the only reduced Gröbner
basis of the ideal ⟨1⟩= k[x1, . . . , xn].
To see this, let {g1, . . . , gt} be a Gröbner basis of I = ⟨1⟩. Thus, 1 ∈⟨LT(I)⟩=
⟨LT(g1), . . . , LT(gt)⟩, and then Lemma 2 of Chapter 2, §4 implies that 1 is divisible
by some LT(gi), say LT(g1). This forces LT(g1) to be constant. Then every other
LT(gi) is a multiple of that constant, so that g2, . . . , gt can be removed from the
Gröbner basis by Lemma 3 of Chapter 2, §7. Finally, since LT(g1) is constant, g1
itself is constant since every nonconstant monomial is > 1 (see Corollary 6 of
Chapter 2, §4). We can multiply by an appropriate constant to make g1 = 1. Our
reduced Gröbner basis is thus {1}.
To summarize, we have the following consistency algorithm: if we have poly-
nomials f1, . . . , fs ∈C[x1, . . . , xn], we compute a reduced Gröbner basis of the ideal
they generate with respect to any ordering. If this basis is {1}, the polynomials have
no common zero in Cn; if the basis is not {1}, they must have a common zero. Note
that this algorithm works over any algebraically closed ﬁeld.
If we are working over a ﬁeld k which is not algebraically closed, then the con-
sistency algorithm still works in one direction: if {1} is a reduced Gröbner basis of
⟨f1, . . . , fs⟩, then the equations f1 = · · · = fs = 0 have no common solution. The
converse is not true, as shown by the examples preceding the statement of the Weak
Nullstellensatz.
Inspired by the Weak Nullstellensatz, one might hope that the correspondence
between ideals and varieties is one-to-one provided only that one restricts to alge-
braically closed ﬁelds. Unfortunately, our earlier example V(x) = V(x2) = {0}
works over any ﬁeld. Similarly, the ideals ⟨x2, y⟩and ⟨x, y⟩(and, for that matter,
⟨xn, ym⟩where n and m are integers greater than one) are different but deﬁne the
same variety: namely, the single point {(0, 0)} ⊆k2. These examples illustrate a
basic reason why different ideals can deﬁne the same variety (equivalently, that the
map V can fail to be one-to-one): namely, a power of a polynomial vanishes on the
same set as the original polynomial. The Hilbert Nullstellensatz states that over an
algebraically closed ﬁeld, this is the only reason that different ideals can give the
same variety: if a polynomial f vanishes at all points of some variety V(I), then
some power of f must belong to I.
Theorem 2 (Hilbert's Nullstellensatz). Let k be an algebraically closed ﬁeld. If
f, f1, . . . , fs ∈k[x1, . . . , xn], then f ∈I(V( f1, . . . , fs)) if and only if
f m ∈⟨f1, . . . , fs⟩
for some integer m ≥1.
Proof. Given a nonzero polynomial f which vanishes at every common zero of
the polynomials f1, . . . , fs, we must show that there exists an integer m ≥1 and

180
Chapter 4 The Algebra-Geometry Dictionary
polynomials A1, . . . , As such that
f m =
s

i=1
Ai fi.
The most direct proof is based on an ingenious trick. Consider the ideal
˜I = ⟨f1, . . . , fs , 1 −yf ⟩⊆k[x1, . . . , xn, y],
where f, f1, . . . , fs are as above. We claim that
V(˜I) = ∅.
To see this, let (a1, . . . , an, an+1) ∈kn+1. Either
• (a1, . . . , an) is a common zero of f1, . . . , fs, or
• (a1, . . . , an) is not a common zero of f1, . . . , fs.
In the ﬁrst case f(a1, . . . , an) = 0 since f vanishes at any common zero of f1, . . . , fs.
Thus, the polynomial 1 −yf takes the value 1 −an+1f(a1, . . . , an) = 1 ̸= 0 at
the point (a1, . . . , an, an+1). In particular, (a1, . . . , an, an+1) /∈V(˜I). In the second
case, for some i, 1 ≤i ≤s, we must have fi(a1, . . . , an) = 0. Thinking of fi as
a function of n + 1 variables which does not depend on the last variable, we have
fi(a1, . . . , an, an+1) ̸= 0. In particular, we again conclude that (a1, . . . , an, an+1) /∈
V(˜I). Since (a1, . . . , an, an+1) ∈kn+1 was arbitrary, we obtain V(˜I) = ∅, as claimed.
Now apply the Weak Nullstellensatz to conclude that 1 ∈˜I. Hence
(2)
1 =
s

i=1
pi(x1, . . . , xn, y)fi + q(x1, . . . , xn, y)(1 −yf )
for some polynomials pi, q ∈k[x1, . . . , xn, y]. Now set y = 1/f(x1, . . . , xn). Then
relation (2) above implies that
(3)
1 =
s

i=1
pi(x1, . . . , xn, 1/f)fi.
Multiply both sides of this equation by a power f m, where m is chosen sufﬁciently
large to clear all the denominators. This yields
(4)
f m =
s

i=1
Ai fi,
for some polynomials Ai ∈k[x1, . . . , xn], which is what we had to show.
□

§2 Radical Ideals and the Ideal-Variety Correspondence
181
EXERCISES FOR §1
1. Recall that V(y −x2, z −x3) is the twisted cubic in R3.
a. Show that V((y −x2)2 + (z −x3)2) is also the twisted cubic.
b. Show that any variety V(I) ⊆Rn, I ⊆R[x1, . . . , xn], can be deﬁned by a single
equation (and hence by a principal ideal).
2. Let J = ⟨x2 + y2 −1, y −1⟩. Find f ∈I(V(J)) such that f /∈J.
3. Prove that {0} and k are the only ideals of a ﬁeld k.
4. Prove that an algebraically closed ﬁeld k must be inﬁnite. Hint: Given n elements
a1, . . . , an of a ﬁeld k, can you write down a nonconstant polynomial f ∈k[x] with
the property that f (ai) = 1 for all i?
5. In the proof of Theorem 1, prove that Ixn=a = ⟨¯g1, . . . , ¯gt⟩.
6. In the proof of Theorem 1, let xδ be a monomial in x1, . . . , xn−1 satisfying xδ > LT( f )
for some f ∈k[x1, . . . , xn]. Prove that xδ > LT(¯f), where ¯f = f (x1, . . . , xn−1, a).
7. In deducing Hilbert's Nullstellensatz from the Weak Nullstellensatz, we made the sub-
stitution y = 1/f (x1, . . . , xn) to deduce relations (3) and (4) from (2). Justify this rigor-
ously. Hint: In what set is 1/f contained?
8. The purpose of this exercise is to show that if k is any ﬁeld that is not algebraically closed,
then any variety V ⊆kn can be deﬁned by a single equation.
a. If g = a0xn + a1xn−1 + · · · + an−1x + an is a polynomial of degree n in x, deﬁne
the homogenization gh of g with respect to some variable y to be the polynomial
gh = a0xn + a1xn−1y + · · · + an−1xyn−1 + anyn. Show that g has a root in k if and
only if there is (a, b) ∈k2 such that (a, b) ̸= (0, 0) and gh(a, b) = 0. Hint: Show that
gh(a, b) = bngh(a/b, 1) when b ̸= 0.
b. If k is not algebraically closed, show that there exists f ∈k[x, y] such that the variety
deﬁned by f = 0 consists of just the origin (0, 0) ∈k2. Hint: Choose a polynomial in
k[x] with no root in k and consider its homogenization.
c. If k is not algebraically closed, show that for each integer l > 0 there exists f ∈
k[x1, . . . , xl] such that the only solution of f = 0 is the origin (0, . . . , 0) ∈kl. Hint:
Use induction on l and part (b) above.
d. If W = V(g1, . . . , gs) is any variety in kn, where k is not algebraically closed, then
show that W can be deﬁned by a single equation. Hint: Consider the polynomial
f (g1, . . . , gs) where f is as in part (c).
9. Let k be an arbitrary ﬁeld and let S be the subset of all polynomials in k[x1, . . . , xn] that
have no zeros in kn. If I is any ideal in k[x1, . . . , xn] such that I ∩S = ∅, show that
V(I) ̸= ∅. Hint: When k is not algebraically closed, use the previous exercise.
10. In Exercise 1, we encountered two ideals in R[x, y] that give the same nonempty variety.
Show that one of these ideals is contained in the other. Can you ﬁnd two ideals in R[x, y],
neither contained in the other, which give the same nonempty variety? Can you do the
same for R[x]?
§2
Radical Ideals and the Ideal-Variety Correspondence
To further explore the relation between ideals and varieties, it is natural to recast
Hilbert's Nullstellensatz in terms of ideals. Can we characterize the kinds of ideals
that appear as the ideal of a variety? In other words, can we identify those ideals that
consist of all polynomials which vanish on some variety V? The key observation is
contained in the following simple lemma.

182
Chapter 4 The Algebra-Geometry Dictionary
Lemma 1. Let V be a variety. If f m ∈I(V), then f ∈I(V).
Proof. Let a ∈V. If f m ∈I(V), then ( f(a))m = 0. But this can happen only if
f(a) = 0. Since a ∈V was arbitrary, we must have f ∈I(V).
□
Thus, an ideal consisting of all polynomials which vanish on a variety V has the
property that if some power of a polynomial belongs to the ideal, then the polyno-
mial itself must belong to the ideal. This leads to the following deﬁnition.
Deﬁnition 2. An ideal I is radical if f m ∈I for some integer m ≥1 implies that
f ∈I.
Rephrasing Lemma 1 in terms of radical ideals gives the following statement.
Corollary 3. I(V) is a radical ideal.
On the other hand, Hilbert's Nullstellensatz tells us that the only way that an
arbitrary ideal I can fail to be the ideal of all polynomials vanishing on V(I) is for
I to contain powers f m of polynomials f which are not in I—in other words, for I
to fail to be a radical ideal. This suggests that there is a one-to-one correspondence
between afﬁne varieties and radical ideals. To clarify this and get a sharp statement,
it is useful to introduce the operation of taking the radical of an ideal.
Deﬁnition 4. Let I ⊆k[x1, . . . , xn] be an ideal. The radical of I, denoted
√
I, is the
set
{f | f m ∈I for some integer m ≥1}.
Note that we always have I ⊆
√
I since f ∈I implies f 1 ∈I and, hence, f ∈
√
I
by deﬁnition. It is an easy exercise to show that an ideal I is radical if and only if
I =
√
I. A somewhat more surprising fact is that the radical of an ideal is always an
ideal. To see what is at stake here, consider, for example, the ideal J = ⟨x2, y3⟩⊆
k[x, y]. Although neither x nor y belongs to J, it is clear that x ∈
√
J and y ∈
√
J.
Note that (x · y)2 = x2y2 ∈J since x2 ∈J; thus, x · y ∈
√
J. It is less obvious that
x + y ∈
√
J. To see this, observe that
(x + y)4 = x4 + 4x3y + 6x2y2 + 4xy3 + y4 ∈J
because x4, 4x3y, 6x2y2 ∈J (they are all multiples of x2) and 4xy3, y4 ∈J (because
they are multiples of y3). Thus, x + y ∈
√
J. By way of contrast, neither xy nor x + y
belong to J.
Lemma 5. If I is an ideal in k[x1, . . . , xn], then
√
I is an ideal in k[x1, . . . , xn] con-
taining I. Furthermore,
√
I is a radical ideal.
Proof. We have already shown that I ⊆
√
I. To show
√
I is an ideal, suppose f, g ∈
√
I. Then there are positive integers m and l such that f m, gl ∈I. In the binomial
expansion of ( f + g)m+l−1 every term has a factor f igj with i + j = m + l −1. Since
either i ≥m or j ≥l, either f i or gj is in I, whence f igj ∈I and every term in the

§2 Radical Ideals and the Ideal-Variety Correspondence
183
binomial expansion is in I. Hence, ( f + g)m+l−1 ∈I and, therefore, f + g ∈
√
I.
Finally, suppose f ∈
√
I and h ∈k[x1, . . . , xn]. Then f m ∈I for some integer m ≥1.
Since I is an ideal, we have (h · f)m = hmf m ∈I. Hence, hf ∈
√
I. This shows that
√
I is an ideal. In Exercise 4, you will show that
√
I is a radical ideal.
□
We are now ready to state the ideal-theoretic form of the Nullstellensatz.
Theorem 6 (The Strong Nullstellensatz). Let k be an algebraically closed ﬁeld. If
I is an ideal in k[x1, . . . , xn], then
I(V(I)) =
√
I.
Proof. We certainly have
√
I ⊆I(V(I)) because f ∈
√
I implies that f m ∈I for
some m. Hence, f m vanishes on V(I), which implies that f vanishes on V(I). Thus,
f ∈I(V(I)).
Conversely, take f ∈I(V(I)). Then, by deﬁnition, f vanishes on V(I). By
Hilbert's Nullstellensatz, there exists an integer m ≥1 such that f m ∈I. But this
means that f ∈
√
I. Since f was arbitrary, I(V(I)) ⊆
√
I, and we are done.
□
It has become a custom, to which we shall adhere, to refer to Theorem 6 as the
Nullstellensatz with no further qualiﬁcation. The most important consequence of
the Nullstellensatz is that it allows us to set up a "dictionary" between geometry and
algebra. The basis of the dictionary is contained in the following theorem.
Theorem 7 (The Ideal-Variety Correspondence). Let k be an arbitrary ﬁeld.
(i) The maps
afﬁne varieties
I
−→ideals
and
ideals
V
−→afﬁne varieties
are inclusion-reversing, i.e., if I1 ⊆I2 are ideals, then V(I1) ⊇V(I2) and,
similarly, if V1 ⊆V2 are varieties, then I(V1) ⊇I(V2).
(ii) For any variety V,
V(I(V)) = V,
so that I is always one-to-one. On the other hand, any ideal I satisﬁes
V(
√
I) = V(I).
(iii) If k is algebraically closed, and if we restrict to radical ideals, then the maps
afﬁne varieties
I
−→radical ideals
and
radical ideals
V
−→afﬁne varieties
are inclusion-reversing bijections which are inverses of each other.

184
Chapter 4 The Algebra-Geometry Dictionary
Proof. (i) The proof will be covered in the exercises.
(ii) Let V = V( f1, . . . , fs) be an afﬁne variety in kn. Since every f ∈I(V)
vanishes on V, the inclusion V ⊆V(I(V)) follows directly from the deﬁnition
of V. Going the other way, note that f1, . . . , fs ∈I(V) by the deﬁnition of I,
and, thus, ⟨f1, . . . , fs⟩⊆
I(V). Since V is inclusion-reversing, it follows that
V(I(V)) ⊆V(⟨f1, . . . , fs⟩) = V. This proves that V(I(V)) = V, and, consequently,
I is one-to-one since it has a left inverse. The ﬁnal assertion of part (ii) is left as an
exercise.
(iii) Since I(V) is radical by Corollary 3, we can think of I as a function which
takes varieties to radical ideals. Furthermore, we already know V(I(V)) = V for
any variety V. It remains to prove I(V(I)) = I whenever I is a radical ideal. This is
easy: the Nullstellensatz tells us I(V(I)) =
√
I, and I being radical implies
√
I = I
(see Exercise 4). This gives the desired equality. Hence, V and I are inverses of
each other and, thus, deﬁne bijections between the set of radical ideals and afﬁne
varieties. The theorem is proved.
□
As a consequence of this theorem, any question about varieties can be rephrased
as an algebraic question about radical ideals (and conversely), provided that we are
working over an algebraically closed ﬁeld. This ability to pass between algebra and
geometry will give us considerable power.
In view of the Nullstellensatz and the importance it assigns to radical ideals, it is
natural to ask whether one can compute generators for the radical from generators
of the original ideal. In fact, there are three questions to ask concerning an ideal
I = ⟨f1, . . . , fs⟩:
• (Radical Generators) Is there an algorithm which produces a set {g1, . . . , gm} of
polynomials such that
√
I = ⟨g1, . . . , gm⟩?
• (Radical Ideal) Is there an algorithm which will determine whether I is radical?
• (Radical Membership) Given f ∈k[x1, . . . , xn], is there an algorithm which will
determine whether f ∈
√
I?
The existence of these algorithms follows from the work of HERMANN (1926)
[see also MINES, RICHMAN, and RUITENBERG (1988) and SEIDENBERG (1974,
1984) for more modern expositions]. More practical algorithms for ﬁnding radicals
follow from the work of GIANNI, TRAGER and ZACHARIAS (1988), KRICK and
LOGAR (1991), and EISENBUD, HUNEKE and VASCONCELOS (1992). These algo-
rithms have been implemented in CoCoA, Singular, and Macaulay2, among others.
See, for example, Section 4.5 of GREUEL and PFISTER (2008).
For now, we will settle for solving the more modest radical membership problem.
To test whether f ∈
√
I, we could use the ideal membership algorithm to check
whether f m ∈I for all integers m > 0. This is not satisfactory because we might have
to go to very large powers of m, and it will never tell us if f /∈
√
I (at least, not until
we work out a priori bounds on m). Fortunately, we can adapt the proof of Hilbert's
Nullstellensatz to give an algorithm for determining whether f ∈
"
⟨f1, . . . , fs⟩.
Proposition 8 (Radical Membership). Let k be an arbitrary ﬁeld and let I =
⟨f1, . . . , fs⟩⊆k[x1, . . . , xn] be an ideal. Then f ∈
√
I if and only if the constant

§2 Radical Ideals and the Ideal-Variety Correspondence
185
polynomial 1 belongs to the ideal ˜I = ⟨f1, . . . , fs, 1 −yf⟩⊆k[x1, . . . , xn, y], in
which case ˜I = k[x1, . . . , xn, y].
Proof. From equations (2), (3), and (4) in the proof of Hilbert's Nullstellensatz in
§1, we see that 1 ∈˜I implies f m ∈I for some m, which, in turn, implies f ∈
√
I.
Going the other way, suppose that f ∈
√
I. Then f m ∈I ⊆˜I for some m. But we
also have 1 −yf ∈˜I, and, consequently,
1 = ymf m + (1 −ymf m) = ym · f m + (1 −yf) · (1 + yf + · · · + ym−1f m−1) ∈˜I,
as desired.
□
Proposition 8, together with our earlier remarks on determining whether 1 be-
longs to an ideal (see the discussion of the consistency problem in §1), imme-
diately leads to the following radical membership algorithm: to determine if
f ∈
"
⟨f1, . . . , fs⟩⊆k[x1, . . . , xn], we compute a reduced Gröbner basis of the ideal
⟨f1, . . . , fs, 1 −yf⟩⊆k[x1, . . . , xn, y] with respect to some ordering. If the result is
{1}, then f ∈
√
I. Otherwise, f /∈
√
I.
As an example, consider the ideal I = ⟨xy2 + 2y2, x4 −2x2 + 1⟩in k[x, y]. Let us
test if f = y −x2 + 1 lies in
√
I. Using lex order on k[x, y, z], one checks that the
ideal
˜I = ⟨xy2 + 2y2, x4 −2x2 + 1, 1 −z(y −x2 + 1)⟩⊆k[x, y, z]
has reduced Gröbner basis {1}. It follows that y −x2 + 1 ∈
√
I by Proposition 8.
Using the division algorithm, we can check what power of y −x2 + 1 lies in I:
y −x2 + 1
G = y −x2 + 1,
(y −x2 + 1)2G = −2x2y + 2y,
(y −x2 + 1)3G = 0,
where G = {x4−2x2+1, y2} is a Gröbner basis of I with respect to lex order and pG
is the remainder of p on division by G. As a consequence, we see that (y−x2+1)3 ∈
I, but no lower power of y −x2 + 1 is in I (in particular, y −x2 + 1 /∈I).
We can also see what is happening in this example geometrically. As a set,
V(I) = {(±1, 0)}, but (speaking somewhat imprecisely) every polynomial in I
vanishes to order at least 2 at each of the two points in V(I). This is visible from the
form of the generators of I if we factor them:
xy2 + 2y2 = y2(x + 2)
and
x4 −2x2 + 1 = (x2 −1)2.
Even though f = y −x2 + 1 also vanishes at (±1, 0), f only vanishes to order 1
there. We must take a higher power of f to obtain an element of I.
We will end this section with a discussion of the one case where we can compute
the radical of an ideal, which is when we are dealing with a principal ideal I =
⟨f⟩. A nonconstant polynomial f is said to be irreducible if it has the property that
whenever f = g · h for some polynomials g and h, then either g or h is a constant.
As noted in §2 of Appendix A, any nonconstant polynomial f can always be written

186
Chapter 4 The Algebra-Geometry Dictionary
as a product of irreducible polynomials. By collecting the irreducible polynomials
which differ by constant multiples of one another, we can write f in the form
f = cf a1
1 · · · f ar
r ,
c ∈k,
where the fi's, 1 ≤i ≤r, are distinct irreducible polynomials, meaning that fi and fj
are not constant multiples of one another whenever i ̸= j. Moreover, this expression
for f is unique up to reordering the fi's and up to multiplying the fi's by constant
multiples. (This unique factorization is Theorem 2 from Appendix A, §2.)
If we have f expressed as a product of irreducible polynomials, then it is easy to
write down the radical of the principal ideal generated by f.
Proposition 9. Let f ∈k[x1, . . . , xn] and I = ⟨f⟩be the principal ideal generated
by f. If f = cf a1
1 · · · f ar
r
is the factorization of f into a product of distinct irreducible
polynomials, then
√
I =
"
⟨f⟩= ⟨f1f2 · · · fr⟩.
Proof. We ﬁrst show that f1f2 · · · fr belongs to
√
I. Let N be an integer strictly
greater than the maximum of a1, . . . , ar. Then
( f1f2 · · · fr)N = f N−a1
1
f N−a2
2
· · · f N−ar
r
f
is a polynomial multiple of f. This shows that ( f1f2 · · · fr)N ∈I, which implies that
f1f2 · · · fr ∈
√
I. Thus ⟨f1f2 · · · fr⟩⊆
√
I.
Conversely, suppose that g ∈
√
I. Then there exists a positive integer M such
that gM ∈I = ⟨f⟩, so that gM is a multiple of f and hence a multiple of each
irreducible factor fi of f. Thus, fi is an irreducible factor of gM. However, the unique
factorization of gM into distinct irreducible polynomials is the Mth power of the
factorization of g. It follows that each fi is an irreducible factor of g. This implies
that g is a polynomial multiple of f1f2 · · · fr and, therefore, g is contained in the ideal
⟨f1f2 · · · fr⟩. The proposition is proved.
□
In view of Proposition 9, we make the following deﬁnition:
Deﬁnition 10. If f ∈k[x1, . . . , xn] is a polynomial, we deﬁne the reduction of f,
denoted fred, to be the polynomial such that ⟨fred⟩=
"
⟨f⟩. A polynomial is said to
be reduced (or square-free) if f = fred.
Thus, fred is the polynomial f with repeated factors "stripped away." So, for ex-
ample, if f = (x + y2)3(x −y), then fred = (x + y2)(x −y). Note that fred is only
unique up to a constant factor in k.
The usefulness of Proposition 9 is mitigated by the requirement that f be factored
into irreducible factors. We might ask if there is an algorithm to compute fred from f
without factoring f ﬁrst. It turns out that such an algorithm exists.
To state the algorithm, we will need the notion of a greatest common divisor of
two polynomials.
Deﬁnition 11. Let f, g ∈k[x1, . . . , xn]. Then h ∈k[x1, . . . , xn] is called a greatest
common divisor of f and g, and denoted h = gcd( f, g), if

§2 Radical Ideals and the Ideal-Variety Correspondence
187
(i) h divides f and g.
(ii) If p is any polynomial that divides both f and g, then p divides h.
It is easy to show that gcd( f, g) exists and is unique up to multiplication by a
nonzero constant in k (see Exercise 9). Unfortunately, the one-variable algorithm
for ﬁnding the gcd (i.e., the Euclidean Algorithm) does not work in the case of
several variables. To see this, consider the polynomials xy and xz in k[x, y, z]. Clearly,
gcd(xy, xz) = x. However, no matter what term ordering we use, dividing xy by xz
gives 0 plus remainder xy and dividing xz by xy gives 0 plus remainder xz. As a
result, neither polynomial "reduces" with respect to the other and there is no next
step to which to apply the analogue of the Euclidean Algorithm.
Nevertheless, there is an algorithm for calculating the gcd of two polynomials
in several variables. We defer a discussion of it until the next section after we have
studied intersections of ideals. For the purposes of our discussion here, let us assume
that we have such an algorithm. We also remark that given polynomials f1, . . . , fs ∈
k[x1, . . . , xn], one can deﬁne gcd( f1, f2, . . . , fs) exactly as in the one-variable case.
There is also an algorithm for computing gcd( f1, f2, . . . , fs).
Using this notion of gcd, we can now give a formula for computing the radical
of a principal ideal.
Proposition 12. Suppose that k is a ﬁeld containing the rational numbers Q and let
I = ⟨f⟩be a principal ideal in k[x1, . . . , xn] . Then
√
I = ⟨fred⟩, where
fred =
f
gcd

f, ∂f
∂x1 , ∂f
∂x2 , . . . , ∂f
∂xn
	.
Proof. Writing f as in Proposition 9, we know that
√
I = ⟨f1f2 · · · fr⟩. Thus, it
sufﬁces to show that
(1)
gcd

f, ∂f
∂x1
, . . . , ∂f
∂xn

= f a1−1
1
f a2−1
2
· · · f ar−1
r
.
We ﬁrst use the product rule to note that
∂f
∂xj
= f a1−1
1
f a2−1
2
· · · f ar−1
r

a1
∂f1
∂xj
f2 · · · fr + · · · + arf1 · · · fr−1
∂fr
∂xj

.
This proves that f a1−1
1
f a2−1
2
· · · f ar−1
r
divides the gcd. It remains to show that for
each i, there is some ∂f
∂xj which is not divisible by f ai
i .
Write f = f ai
i hi, where hi is not divisible by fi. Since fi is nonconstant, some
variable xj must appear in fi. The product rule gives us
∂f
∂xj
= f ai−1
i

a1
∂fi
∂xj
hi + fi
∂hi
∂xj

.
If this expression is divisible by f ai
i , then ∂fi
∂xj hi must be divisible by fi. Since fi is
irreducible and does not divide hi, this forces fi to divide ∂fi
∂xj . In Exercise 13, you

188
Chapter 4 The Algebra-Geometry Dictionary
will show that ∂fi
∂xj is nonzero since Q ⊆k and xj appears in fi. As ∂fi
∂xj also has
smaller total degree than fi, it follows that fi cannot divide ∂fi
∂xj . Consequently, ∂f
∂xj is
not divisible by f ai
i , which proves (1), and the proposition follows.
□
It is worth remarking that for ﬁelds which do not contain Q, the above formula
for fred may fail (see Exercise 13).
EXERCISES FOR §2
1. Given a ﬁeld k (not necessarily algebraically closed), show that
!
⟨x2, y2⟩= ⟨x, y⟩and,
more generally, show that
!
⟨xn, ym⟩= ⟨x, y⟩for any positive integers n and m.
2. Let f and g be distinct nonconstant polynomials in k[x, y] and let I = ⟨f 2, g3⟩. Is it
necessarily true that
√
I = ⟨f , g⟩? Explain.
3. Show that ⟨x2 + 1⟩⊆R[x] is a radical ideal, but that V(x2 + 1) is the empty variety.
4. Let I be an ideal in k[x1, . . . , xn], where k is an arbitrary ﬁeld.
a. Show that
√
I is a radical ideal.
b. Show that I is radical if and only if I =
√
I.
c. Show that
!√
I =
√
I.
5. Prove that I and V are inclusion-reversing and that V(
√
I) = V(I) for any ideal I.
6. Let I be an ideal in k[x1, . . . , xn].
a. In the special case when
√
I = ⟨f1, f2⟩, with f mi
i
∈I, prove that f m1+m2−1 ∈I for all
f ∈
√
I.
b. Now prove that for any I, there exists a single integer m such that f m ∈I for all
f ∈
√
I. Hint: Write
√
I = ⟨f1, . . . , fs⟩.
7. Determine whether the following polynomials lie in the following radicals. If the answer
is yes, what is the smallest power of the polynomial that lies in the ideal?
a. Is x + y ∈
!
⟨x3, y3, xy(x + y)⟩?
b. Is x2 + 3xz ∈
!
⟨x + z, x2y, x −z2⟩?
8. Let f1 = y2 + 2xy −1 and f2 = x2 + 1. Prove that ⟨f1, f2⟩is not a radical ideal. Hint:
What is f1 + f2?
9. Given f , g ∈k[x1, . . . , xn], use unique factorization to prove that gcd( f , g) exists. Also
prove that gcd( f , g) is unique up to multiplication by a nonzero constant of k.
10. Prove the following ideal-theoretic characterization of gcd( f , g): given polynomials
f , g, h in k[x1, . . . , xn], then h = gcd( f , g) if and only if h is a generator of the small-
est principal ideal containing ⟨f , g⟩(i.e., if ⟨h⟩⊆J, whenever J is a principal ideal such
that J ⊇⟨f , g⟩).
11. Find a basis for the ideal
!
⟨x5 −2x4 + 2x2 −x, x5 −x4 −2x3 + 2x2 + x −1⟩.
Compare with Exercise 17 of Chapter 1, §5.
12. Let f = x5+3x4y+3x3y2−2x4y2+x2y3−6x3y3−6x2y4+x3y4−2xy5+3x2y5+3xy6+y7 ∈
Q[x, y]. Compute
!
⟨f ⟩.
13. A ﬁeld k has characteristic zero if it contains the rational numbers Q; otherwise, k has
positive characteristic.
a. Let k be the ﬁeld F2 from Exercise 1 of Chapter 1, §1. If f = x2
1 + · · · + x2
n ∈
F2[x1, . . . , xn], then show that
∂f
∂xi = 0 for all i. Conclude that the formula given in
Proposition 12 may fail when the ﬁeld is F2.

§3 Sums, Products, and Intersections of Ideals
189
b. Let k be a ﬁeld of characteristic zero and let f ∈k[x1, . . . , xn] be nonconstant. If the
variable xj appears in f , then prove that
∂f
∂xj ̸= 0. Also explain why
∂f
∂xj has smaller
total degree than f .
14. Let J = ⟨xy, (x −y)x⟩. Describe V(J) and show that
√
J = ⟨x⟩.
15. Prove that I = ⟨xy, xz, yz⟩is a radical ideal. Hint: If you divide f ∈k[x, y, z] by xy, xz, yz,
what does the remainder look like? What does f m look like?
16. Let I ⊆k[x1, . . . , xn] be an ideal. Assume that I has a Gröbner basis G = {g1, . . . , gt}
such that for all i, LT(gi) is square-free in the sense of Deﬁnition 10.
a. If f ∈
√
I, prove that LT( f ) is divisible by LT(gi) for some i. Hint: f m ∈I.
b. Prove that I is radical. Hint: Use part (a) to show that G is a Gröbner basis of
√
I.
17. This exercise continues the line of thought begun in Exercise 16.
a. Prove that a monomial ideal in k[x1, . . . , xn] is radical if and only if its minimal gen-
erators are square-free.
b. Given an ideal I ⊆k[x1, . . . , xn], prove that if ⟨LT(I)⟩is radical, then I is radical.
c. Give an example to show that the converse of part (b) can fail.
§3
Sums, Products, and Intersections of Ideals
Ideals are algebraic objects and, as a result, there are natural algebraic operations
we can deﬁne on them. In this section, we consider three such operations: sum,
intersection, and product. These are binary operations: to each pair of ideals, they
associate a new ideal. We shall be particularly interested in two general questions
which arise in connection with each of these operations. The ﬁrst asks how, given
generators of a pair of ideals, one can compute generators of the new ideals which
result on applying these operations. The second asks for the geometric signiﬁcance
of these algebraic operations. Thus, the ﬁrst question ﬁts the general computational
theme of this book; the second, the general thrust of this chapter. We consider each
of the operations in turn.
Sums of Ideals
Deﬁnition 1. If I and J are ideals of the ring k[x1, . . . , xn], then the sum of I and J,
denoted I + J, is the set
I + J = {f + g | f ∈I and g ∈J}.
Proposition 2. If I and J are ideals in k[x1, . . . , xn], then I + J is also an ideal in
k[x1, . . . , xn]. In fact, I + J is the smallest ideal containing I and J. Furthermore, if
I = ⟨f1, . . . , fr⟩and J = ⟨g1, . . . , gs⟩, then I + J = ⟨f1, . . . , fr, g1, . . . , gs⟩.
Proof. Note ﬁrst that 0 = 0+0 ∈I +J. Suppose h1, h2 ∈I +J. By the deﬁnition of
I + J, there exist f1, f2 ∈I and g1, g2 ∈J such that h1 = f1 + g1, h2 = f2 + g2. Then,
after rearranging terms slightly, h1 + h2 = ( f1 + f2) + (g1 + g2). But f1 + f2 ∈I
because I is an ideal and, similarly, g1 + g2 ∈J, whence h1 + h2 ∈I + J. To
check closure under multiplication, let h ∈I + J and p ∈k[x1, . . . , xn] be any

190
Chapter 4 The Algebra-Geometry Dictionary
polynomial. Then, as above, there exist f ∈I and g ∈J such that h = f + g. But
then p · h = p · ( f + g) = p · f + p · g. Now p · f ∈I and p · g ∈J because I and J
are ideals. Consequently, p · h ∈I + J. This shows that I + J is an ideal.
If H is an ideal which contains I and J, then H must contain all elements f ∈I
and g ∈J. Since H is an ideal, H must contain all f + g, where f ∈I, g ∈J. In
particular, H ⊇I + J. Therefore, every ideal containing I and J contains I + J and,
thus, I + J must be the smallest such ideal.
Finally, if I = ⟨f1, . . . , fr⟩and J = ⟨g1, . . . , gs⟩, then ⟨f1, . . . , fr, g1, . . . , gs⟩is
an ideal containing I and J, so that I + J ⊆⟨f1, . . . , fr, g1, . . . , gs⟩. The reverse
inclusion is obvious, so that I + J = ⟨f1, . . . , fr, g1, . . . , gs⟩.
□
The following corollary is an immediate consequence of Proposition 2.
Corollary 3. If f1, . . . , fr ∈k[x1, . . . , xn], then
⟨f1, . . . , fr⟩= ⟨f1⟩+ · · · + ⟨fr⟩.
To see what happens geometrically, let I = ⟨x2 + y⟩and J = ⟨z⟩be ideals in
R[x, y, z]. We have sketched V(I) and V(J) on the next page. Then I+J = ⟨x2+y, z⟩
contains both x2 + y and z. Thus, the variety V(I + J) must consist of those points
where both x2 + y and z vanish, i.e., it must be the intersection of V(I) and V(J).
z
y
x
↓
V(z)
←V(x2+y)
←
V(x2+y,z)
The same line of reasoning generalizes to show that addition of ideals corresponds
geometrically to taking intersections of varieties.
Theorem 4. If I and J are ideals in k[x1, . . . , xn], then V(I + J) = V(I) ∩V(J).
Proof. If a ∈V(I +J), then a ∈V(I) because I ⊆I +J; similarly, a ∈V(J). Thus,
a ∈V(I) ∩V(J) and we conclude that V(I + J) ⊆V(I) ∩V(J).
To get the opposite inclusion, suppose a ∈V(I)∩V(J). Let h be any polynomial
in I + J. Then there exist f ∈I and g ∈J such that h = f + g. We have f(a) = 0
because a ∈V(I) and g(a) = 0 because a ∈V(J). Thus, h(a) = f(a) + g(a) =
0 + 0 = 0. Since h was arbitrary, we conclude that a ∈V(I + J). Hence, V(I + J) ⊇
V(I) ∩V(J).
□

§3 Sums, Products, and Intersections of Ideals
191
An analogue of Theorem 4 stated in terms of generators was given in Lemma 2
of Chapter 1, §2.
Products of Ideals
In Lemma 2 of Chapter 1, §2, we encountered the fact that an ideal generated by the
products of the generators of two other ideals corresponds to the union of varieties:
V( f1, . . . , fr) ∪V(g1, . . . , gs) = V( figj, 1 ≤i ≤r, 1 ≤j ≤s).
Thus, for example, the variety V(xz, yz) corresponding to an ideal generated by the
product of the generators of the ideals, ⟨x, y⟩and ⟨z⟩in k[x, y, z] is the union of
V(x, y) (the z-axis) and V(z) [the (x, y)-plane]. This suggests the following deﬁni-
tion.
Deﬁnition 5. If I and J are two ideals in k[x1, . . . , xn], then their product, denoted
I · J, is deﬁned to be the ideal generated by all polynomials f · g where f ∈I and
g ∈J.
Thus, the product I · J of I and J is the set
I · J = {f1g1 + · · · + frgr | f1, . . . , fr ∈I, g1, . . . , gr ∈J, r a positive integer}.
To see that this is an ideal, note that 0 = 0 · 0 ∈I · J. Moreover, it is clear that
h1, h2 ∈I · J implies that h1 + h2 ∈I · J. Finally, if h = f1g1 + · · · + frgr ∈I · J and
p is any polynomial, then
ph = (pf1)g1 + · · · + (pfr)gr ∈I · J
since pfi ∈I for all i, 1 ≤i ≤r. Note that the set of products would not be an
ideal because it would not be closed under addition. The following easy proposition
shows that computing a set of generators for I · J given sets of generators for I and
J is completely straightforward.
Proposition 6. Let I = ⟨f1, . . . , fr⟩and J = ⟨g1, . . . , gs⟩. Then I · J is generated by
the set of all products of generators of I and J:
I · J = ⟨figj | 1 ≤i ≤r, 1 ≤j ≤s⟩.
Proof. It is clear that the ideal generated by products figj of the generators is con-
tained in I · J. To establish the opposite inclusion, note that any polynomial in I · J
is a sum of polynomials of the form fg with f ∈I and g ∈J. But we can write f and
g in terms of the generators f1, . . . , fr and g1, . . . , gs, respectively, as
f = a1f1 + · · · + arfr,
g = b1g1 + · · · + bsgs
for appropriate polynomials a1, . . . , ar, b1, . . . , bs. Thus, fg, and consequently any
sum of polynomials of this form, can be written as a sum 
ij cijfigj, where cij ∈
k[x1, . . . , xn].
□

192
Chapter 4 The Algebra-Geometry Dictionary
The following proposition guarantees that the product of ideals does indeed cor-
respond geometrically to the operation of taking the union of varieties.
Theorem 7. If I and J are ideals in k[x1, . . . , xn], then V(I · J) = V(I) ∪V(J).
Proof. Let a ∈V(I · J). Then g(a)h(a) = 0 for all g ∈I and all h ∈J. If g(a) = 0
for all g ∈I, then a ∈V(I). If g(a) ̸= 0 for some g ∈I, then we must have h(a) = 0
for all h ∈J. In either event, a ∈V(I) ∪V(J).
Conversely, suppose a ∈V(I) ∪V(J). Either g(a) = 0 for all g ∈I or h(a) = 0
for all h ∈J. Thus, g(a)h(a) = 0 for all g ∈I and h ∈J. Thus, f(a) = 0 for all
f ∈I · J and, hence, a ∈V(I · J).
□
In what follows, we will often write the product of ideals as IJ rather than I · J.
Intersections of Ideals
The operation of forming the intersection of two ideals is, in some ways, even more
primitive than the operations of addition and multiplication.
Deﬁnition 8. The intersection I ∩J of two ideals I and J in k[x1, . . . , xn] is the set
of polynomials which belong to both I and J.
As in the case of sums, the set of ideals is closed under intersections.
Proposition 9. If I and J are ideals in k[x1, . . . , xn], then I ∩J is also an ideal.
Proof. Note that 0 ∈I ∩J since 0 ∈I and 0 ∈J. If f, g ∈I ∩J, then f + g ∈I
because f, g ∈I. Similarly, f + g ∈J and, hence, f + g ∈I ∩J. Finally, to check
closure under multiplication, let f ∈I ∩J and h be any polynomial in k[x1, . . . , xn].
Since f ∈I and I is an ideal, we have h · f ∈I. Similarly, h · f ∈J and, hence,
h · f ∈I ∩J.
□
Note that we always have IJ ⊆I∩J since elements of IJ are sums of polynomials
of the form fg with f ∈I and g ∈J. But the latter belongs to both I (since f ∈I)
and J (since g ∈J). However, IJ can be strictly contained in I ∩J. For example,
if I = J = ⟨x, y⟩, then IJ = ⟨x2, xy, y2⟩is strictly contained in I ∩J = I = ⟨x, y⟩
(x ∈I ∩J, but x /∈IJ).
Given two ideals and a set of generators for each, we would like to be able to
compute a set of generators for the intersection. This is much more delicate than the
analogous problems for sums and products of ideals, which were entirely straight-
forward. To see what is involved, suppose I is the ideal in Q[x, y] generated by the
polynomial f = (x + y)4(x2 + y)2(x −5y) and let J be the ideal generated by the
polynomial g = (x + y)(x2 + y)3(x + 3y). We leave it as an (easy) exercise to check
that
I ∩J = ⟨(x + y)4(x2 + y)3(x −5y)(x + 3y)⟩.

§3 Sums, Products, and Intersections of Ideals
193
This computation is easy precisely because we were given factorizations of f and g
into irreducible polynomials. In general, such factorizations may not be available. So
any algorithm which allows one to compute intersections will have to be powerful
enough to circumvent this difﬁculty.
Nevertheless, there is a nice trick that reduces the computation of intersections
to computing the intersection of an ideal with a subring (i.e., eliminating variables),
a problem which we have already solved. To state the theorem, we need a little no-
tation: if I is an ideal in k[x1, . . . , xn] and f(t) ∈k[t] a polynomial in the single
variable t, then f(t)I denotes the ideal in k[x1, . . . , xn, t] generated by the set of poly-
nomials {f(t)·h | h ∈I}. This is a little different from our usual notion of product in
that the ideal I and the ideal generated by f(t) in k[t] lie in different rings: in fact, the
ideal I ⊆k[x1, . . . , xn] is not an ideal in k[x1, . . . , xn, t] because it is not closed un-
der multiplication by t. When we want to stress that a polynomial h ∈k[x1, . . . , xn]
involves only the variables x1, . . . , xn, we write h = h(x). Along the same lines, if
we are considering a polynomial g in k[x1, . . . , xn, t] and we want to emphasize that
it can involve the variables x1, . . . , xn as well as t, we will write g = g(x, t). In terms
of this notation, f(t)I = ⟨f(t)h(x) | h(x) ∈I⟩. So, for example, if f(t) = t2 −t and
I = ⟨x, y⟩, then the ideal f(t)I in k[x, y, t] contains (t2 −t)x and (t2 −t)y. In fact, it is
not difﬁcult to see that f(t)I is generated as an ideal by (t2 −t)x and (t2 −t)y. This
is a special case of the following assertion.
Lemma 10.
(i) If I is generated as an ideal in k[x1, . . . , xn] by p1(x), . . . , pr(x), then f(t)I is
generated as an ideal in k[x1, . . . , xn, t] by f(t) · p1(x), . . . , f(t) · pr(x).
(ii) If g(x, t) ∈f(t)I and a is any element of the ﬁeld k, then g(x, a) ∈I.
Proof. To prove the ﬁrst assertion, note that any polynomial g(x, t) ∈f(t)I can be
expressed as a sum of terms of the form h(x, t) · f(t) · p(x) for h ∈k[x1, . . . , xn, t]
and p ∈I. But because I is generated by p1, . . . , pr the polynomial p(x) can be
expressed as a sum of terms of the form qi(x)pi(x), 1 ≤i ≤r. In other words,
p(x) =
r

i=1
qi(x)pi(x).
Hence,
h(x, t) · f(t) · p(x) =
r

i=1
h(x, t)qi(x)f(t)pi(x).
Now, for each i, 1 ≤i ≤r, h(x, t) · qi(x) ∈k[x1, . . . , xn, t]. Thus, h(x, t) · f(t) · p(x)
belongs to the ideal in k[x1, . . . , xn, t] generated by f(t)·p1(x), . . . , f(t)·pr(x). Since
g(x, t) is a sum of such terms,
g(x, t) ∈⟨f(t) · p1(x), . . . , f(t) · pr(x)⟩,
which establishes (i). The second assertion follows immediately upon substituting
a ∈k for t.
□

194
Chapter 4 The Algebra-Geometry Dictionary
Theorem 11. Let I, J be ideals in k[x1, . . . , xn]. Then
I ∩J = (tI + (1 −t)J) ∩k[x1, . . . , xn].
Proof. Note that tI + (1 −t)J is an ideal in k[x1, . . . , xn, t]. To establish the desired
equality, we use the usual strategy of proving containment in both directions.
Suppose f ∈I ∩J. Since f ∈I, we have t · f ∈tI. Similarly, f ∈J implies
(1 −t) · f ∈(1 −t)J. Thus, f = t · f + (1 −t) · f ∈tI + (1 −t)J. Since I, J ⊆
k[x1, . . . , xn], we have f ∈(tI + (1 −t)J) ∩k[x1, . . . , xn]. This shows that I ∩J ⊆
(tI + (1 −t)J) ∩k[x1, . . . , xn].
To establish the opposite containment, take f ∈(tI + (1 −t)J) ∩k[x1, . . . , xn].
Then f(x) = g(x, t) + h(x, t), where g(x, t) ∈tI and h(x, t) ∈(1 −t)J. First
set t = 0. Since every element of tI is a multiple of t, we have g(x, 0) = 0. Thus,
f(x) = h(x, 0) and hence, f(x) ∈J by Lemma 10. On the other hand, set t = 1 in the
relation f(x) = g(x, t)+h(x, t). Since every element of (1−t)J is a multiple of 1−t,
we have h(x, 1) = 0. Thus, f(x) = g(x, 1) and, hence, f(x) ∈I by Lemma 10. Since
f belongs to both I and J, we have f ∈I∩J. Thus, I∩J ⊇(tI+(1−t)J)∩k[x1, . . . , xn]
and this completes the proof.
□
The above result and the Elimination Theorem (Theorem 2 of Chapter 3, §1)
lead to the following algorithm for computing intersections of ideals: if I =
⟨f1, . . . , fr⟩and J = ⟨g1, . . . , gs⟩are ideals in k[x1, . . . , xn], we consider the ideal
⟨tf1, . . . , tfr, (1 −t)g1, . . . , (1 −t)gs⟩⊆k[x1, . . . , xn, t]
and compute a Gröbner basis with respect to lex order in which t is greater than
the xi. The elements of this basis which do not contain the variable t will form a
basis (in fact, a Gröbner basis) of I ∩J. For more efﬁcient calculations, one could
also use one of the orders described in Exercises 5 and 6 of Chapter 3, §1. An
algorithm for intersecting three or more ideals is described in Proposition 6.19 of
BECKER and WEISPFENNING (1993).
As a simple example of the above procedure, suppose we want to compute the
intersection of the ideals I = ⟨x2y⟩and J = ⟨xy2⟩in Q[x, y]. We consider the ideal
tI + (1 −t)J = ⟨tx2y, (1 −t)xy2⟩= ⟨tx2y, txy2 −xy2⟩
in Q[t, x, y]. Computing the S-polynomial of the generators, we obtain tx2y2 −
(tx2y2 −x2y2) = x2y2. It is easily checked that {tx2y, txy2 −xy2, x2y2} is a Gröbner
basis of tI + (1 −t)J with respect to lex order with t > x > y. By the Elimination
Theorem, {x2y2} is a (Gröbner) basis of (tI + (1 −t)J) ∩Q[x, y]. Thus,
I ∩J = ⟨x2y2⟩.
As another example, we invite the reader to apply the algorithm for computing in-
tersections of ideals to give an alternate proof that the intersection I ∩J of the ideals
I = ⟨(x + y)4(x2 + y)2(x −5y)⟩
and
J = ⟨(x + y)(x2 + y)3(x + 3y)⟩

§3 Sums, Products, and Intersections of Ideals
195
in Q[x, y] is
I ∩J = ⟨(x + y)4(x2 + y)3(x −5y)(x + 3y)⟩.
These examples above are rather simple in that our algorithm applies to ideals which
are not necessarily principal, whereas the examples given here involve intersections
of principal ideals. We shall see a somewhat more complicated example in the exer-
cises.
We can generalize both of the examples above by introducing the following def-
inition.
Deﬁnition 12. A polynomial h ∈k[x1, . . . , xn] is called a least common multiple
of f, g ∈k[x1, . . . , xn] and denoted h = lcm( f, g) if
(i) f divides h and g divides h.
(ii) If f and g both divide a polynomial p, then h divides p.
For example,
lcm(x2y, xy2) = x2y2
and
lcm((x + y)4(x2 + y)2(x −5y), (x + y)(x2 + y)3(x + 3y))
= (x + y)4(x2 + y)3(x −5y)(x + 3y).
More generally, suppose f, g ∈k[x1, . . . , xn] and let f = cf a1
1 . . . f ar
r
and g =
c′gb1
1 . . . gbss be their factorizations into distinct irreducible polynomials. It may hap-
pen that some of the irreducible factors of f are constant multiples of those of g. In
this case, let us suppose that we have rearranged the order of the irreducible poly-
nomials in the expressions for f and g so that for some l, 1 ≤l ≤min(r, s), fi is a
constant (nonzero) multiple of gi for 1 ≤i ≤l and for all i, j > l, fi is not a constant
multiple of gj. Then it follows from unique factorization that
(1)
lcm( f, g) = f max(a1,b1)
1
· · · f max(al,bl)
l
· gbl+1
l+1 · · · gbs
s · f al+1
l+1 · · · f ar
r .
[In the case that f and g share no common factors, we have lcm( f, g) = f · g.] This,
in turn, implies the following result.
Proposition 13.
(i) The intersection I ∩J of two principal ideals I, J ⊆k[x1, . . . , xn] is a principal
ideal.
(ii) If I = ⟨f⟩, J = ⟨g⟩and I ∩J = ⟨h⟩, then
h = lcm( f, g).
Proof. The proof will be left as an exercise.
□
This result, together with our algorithm for computing the intersection of two
ideals immediately gives an algorithm for computing the least common multiple
of two polynomials: to compute the least common multiple of two polynomials

196
Chapter 4 The Algebra-Geometry Dictionary
f, g ∈k[x1, . . . , xn], we compute the intersection ⟨f⟩∩⟨g⟩using our algorithm for
computing the intersection of ideals. Proposition 13 assures us that this intersection
is a principal ideal (in the exercises, we ask you to prove that the intersection of
principal ideals is principal) and that any generator of it is a least common multiple
of f and g.
This algorithm for computing least common multiples allows us to clear up a
point which we left unﬁnished in §2: namely, the computation of the greatest com-
mon divisor of two polynomials f and g. The crucial observation is the following.
Proposition 14. Let f, g ∈k[x1, . . . , xn]. Then
lcm( f, g) · gcd( f, g) = fg.
Proof. This follows by expressing f and g as products of distinct irreducibles and
then using the remarks preceding Proposition 13, especially equation (1). You will
provide the details in Exercise 5.
□
It follows immediately from Proposition 14 that
(2)
gcd( f, g) =
f · g
lcm( f, g).
This gives an algorithm for computing the greatest common divisor of two poly-
nomials f and g. Namely, we compute lcm( f, g) using our algorithm for the least
common multiple and divide it into the product of f and g using the division algo-
rithm.
We should point out that the gcd algorithm just described is rather cumber-
some. In practice, more efﬁcient algorithms are used [see DAVENPORT, SIRET and
TOURNIER (1993)].
Having dealt with the computation of intersections, we now ask what operation
on varieties corresponds to the operation of intersection on ideals. The following
result answers this question.
Theorem 15. If I and J are ideals in k[x1, . . . , xn], then V(I ∩J) = V(I) ∪V(J).
Proof. Let a ∈V(I) ∪V(J). Then a ∈V(I) or a ∈V(J). This means that either
f(a) = 0 for all f ∈I or f(a) = 0 for all f ∈J. Thus, certainly, f(a) = 0 for all
f ∈I ∩J. Hence, a ∈V(I ∩J). Hence, V(I) ∪V(J) ⊆V(I ∩J).
On the other hand, note that since IJ ⊆I ∩J, we have V(I ∩J) ⊆V(IJ).
But V(IJ) = V(I) ∪V(J) by Theorem 7, and we immediately obtain the reverse
inequality.
□
Thus, the intersection of two ideals corresponds to the same variety as the prod-
uct. In view of this and the fact that the intersection is much more difﬁcult to com-
pute than the product, one might legitimately question the wisdom of bothering with
the intersection at all. The reason is that intersection behaves much better with re-
spect to the operation of taking radicals: the product of radical ideals need not be
a radical ideal (consider IJ where I = J), but the intersection of radical ideals is
always a radical ideal. The latter fact is a consequence of the next proposition.

§3 Sums, Products, and Intersections of Ideals
197
Proposition 16. If I, J are any ideals, then
√
I ∩J =
√
I ∩
√
J.
Proof. If f ∈
√
I ∩J, then f m ∈I ∩J for some integer m > 0. Since f m ∈I, we
have f ∈
√
I. Similarly, f ∈
√
J. Thus,
√
I ∩J ⊆
√
I ∩
√
J.
For the reverse inclusion, take f ∈
√
I ∩
√
J. Then, there exist integers m, p > 0
such that f m ∈I and f p ∈J. Thus f m+p = f mf p ∈I ∩J, so f ∈
√
I ∩J.
□
EXERCISES FOR §3
1. Show that in Q[x, y], we have
⟨(x+y)4(x2+y)2(x−5y)⟩∩⟨(x+y)(x2+y)3(x +3y)⟩= ⟨(x+y)4(x2+y)3(x −5y)(x+3y)⟩.
2. Prove formula (1) for the least common multiple of two polynomials f and g.
3. Prove assertion (i) of Proposition 13. In other words, show that the intersection of two
principal ideals is principal.
4. Prove assertion (ii) of Proposition 13. In other words, show that the least common multi-
ple of two polynomials f and g in k[x1, . . . , xn] is the generator of the ideal ⟨f ⟩∩⟨g⟩.
5. Prove Proposition 14. In other words, show that the least common multiple of two poly-
nomials times the greatest common divisor of the same two polynomials is the product
of the polynomials. Hint: Use the remarks following the statement of Proposition 14.
6. Let I1, . . . , Ir and J be ideals in k[x1, . . . , xn]. Show the following:
a. (I1 + I2)J = I1J + I2J.
b. (I1 · · · Ir)m = Im
1 · · · Im
r .
7. Let I and J be ideals in k[x1, . . . , xn], where k is an arbitrary ﬁeld. Prove the following:
a. If Iℓ⊆J for some integer ℓ> 0, then
√
I ⊆
√
J.
b. √I + J =
!√
I +
√
J.
8. Let
f = x4 + x3y + x3z2 −x2y2 + x2yz2 −xy3 −xy2z2 −y3z2
and
g = x4 + 2x3z2 −x2y2 + x2z4 −2xy2z2 −y2z4.
a. Use a computer algebra program to compute generators for ⟨f ⟩∩⟨g⟩and
!
⟨f ⟩⟨g⟩.
b. Use a computer algebra program to compute gcd( f , g).
c. Let p = x2 + xy+ xz+ yz and q = x2 −xy−xz+ yz. Use a computer algebra program
to calculate ⟨f , g⟩∩⟨p, q⟩.
9. For an arbitrary ﬁeld, show that
√
IJ =
√
I ∩J. Give an example to show that the product
of radical ideals need not be radical. Also give an example to show that
√
IJ can differ
from
√
I
√
J.
10. If I is an ideal in k[x1, . . . , xn] and ⟨f (t)⟩is an ideal in k[t], show that the ideal f (t)I
deﬁned in the text is the product of the ideal ˜I generated by all elements of I in
k[x1, . . . , xn, t] and the ideal ⟨f (t)⟩generated by f (t) in k[x1, . . . , xn, t].
11. Two ideals I and J of k[x1, . . . , xn] are said to be comaximal if and only if I + J =
k[x1, . . . , xn].
a. Show that if k = C, then I and J are comaximal if and only if V(I) ∩V(J) = ∅. Give
an example to show that this is false in general.
b. Show that if I and J are comaximal, then IJ = I ∩J.

198
Chapter 4 The Algebra-Geometry Dictionary
c. Is the converse to part (b) true? That is, if IJ = I ∩J, does it necessarily follow that I
and J are comaximal? Proof or counterexample?
d. If I and J are comaximal, show that I and J2 are comaximal. In fact, show that Ir and
Js are comaximal for all positive integers r and s.
e. Let I1, . . . , Ir be ideals in k[x1, . . . , xn] and suppose that Ii and Ji = "
j̸=i Ij are comax-
imal for all i. Show that
Im
1 ∩· · · ∩Im
r = (I1 · · · Ir)m = (I1 ∩· · · ∩Ir)m
for all positive integers m.
12. Let I, J be ideals in k[x1, . . . , xn] and suppose that I ⊆
√
J. Show that Im ⊆J for some
integer m > 0. Hint: You will need to use the Hilbert Basis Theorem.
13. Let A be an m × n constant matrix and suppose that x = Ay, where we are thinking of
x ∈km and y ∈kn as column vectors of variables. Deﬁne a map
αA : k[x1, . . . , xm] −→k[y1, . . . , yn]
by sending f ∈k[x1, . . . , xm] to αA( f ) ∈k[y1, . . . , yn], where αA( f ) is the polynomial
deﬁned by αA( f )(y) = f (Ay).
a. Show that αA is k-linear, i.e., show that αA(rf +sg) = rαA( f )+sαA(g) for all r, s ∈k
and all f , g ∈k[x1, . . . , xn].
b. Show that αA( f · g) = αA( f ) · αA(g) for all f , g ∈k[x1, . . . , xn]. (As we will see
in Deﬁnition 8 of Chapter 5, §2, a map between rings which preserves addition and
multiplication and also preserves the multiplicative identity is called a ring homomor-
phism. Since it is clear that αA(1) = 1, this shows that αA is a ring homomorphism.)
c. Show that the set {f ∈k[x1, . . . , xm] | αA( f ) = 0} is an ideal in k[x1, . . . , xm]. [This
set is called the kernel of αA and denoted ker(αA).]
d. If I is an ideal in k[x1, . . . , xn], show that the set αA(I) = {αA( f ) | f ∈I} need
not be an ideal in k[y1, . . . , yn]. [We will often write ⟨αA(I)⟩to denote the ideal in
k[y1, . . . , yn] generated by the elements of αA(I)—it is called the extension of I to
k[y1, . . . , yn].]
e. If I′ is an ideal in k[y1, . . . , yn], set α−1
A (I′) = {f ∈k[x1, . . . , xm] | αA( f ) ∈I′}.
Show that α−1
A (I′) is an ideal in k[x1, . . . , xm] (often called the contraction of I′).
14. Let A and αA be as above and let K = ker(αA). Let I and J be ideals in k[x1, . . . , xm].
Show that:
a. I ⊆J implies ⟨αA(I)⟩⊆⟨αA(J)⟩.
b. ⟨αA(I + J)⟩= ⟨αA(I)⟩+ ⟨αA(J)⟩.
c. ⟨αA (IJ)⟩= ⟨αA(I)⟩⟨αA(J)⟩.
d. ⟨αA(I ∩J)⟩⊆⟨αA(I)⟩∩⟨αA(J)⟩, with equality if I ⊇K or J ⊇K and αA is onto.
e. ⟨αA(
√
I)⟩⊆
!
⟨αA(I)⟩with equality if I ⊇K and αA is onto.
15. Let A, αA, and K = ker(αA) be as above. Let I′ and J′ be ideals in k[y1, . . . , yn]. Show
that:
a. I′ ⊆J′ implies α−1
A (I′) ⊆α−1
A (J′).
b. α−1
A (I′ + J′) ⊇α−1
A (I′) + α−1
A (J′), with equality if αA is onto.
c. α−1
A (I′J′) ⊇(α−1
A (I′))(α−1
A (J′)), with equality if αA is onto and the right-hand side
contains K.
d. α−1
A (I′ ∩J′) = α−1
A (I′) ∩α−1
A (J′).
e. α−1
A (
√
I′) =

α−1
A (I′).

§4 Zariski Closures, Ideal Quotients, and Saturations
199
§4
Zariski Closures, Ideal Quotients, and Saturations
We have already encountered a number of examples of sets which are not varieties.
Such sets arose very naturally in Chapter 3, where we saw that the projection of a
variety need not be a variety, and in the exercises in Chapter 1, where we saw that
the (set-theoretic) difference of varieties can fail to be a variety.
Whether or not a set S ⊆kn is an afﬁne variety, the set
I(S) = {f ∈k[x1, . . . , xn] | f(a) = 0 for all a ∈S}
is an ideal in k[x1, . . . , xn] (check this!). In fact, it is radical. By the ideal-variety cor-
respondence, V(I(S)) is a variety. The following proposition states that this variety
is the smallest variety that contains the set S.
Proposition 1. If S ⊆kn, the afﬁne variety V(I(S)) is the smallest variety that
contains S [in the sense that if W ⊆kn is any afﬁne variety containing S, then
V(I(S)) ⊆W].
Proof. If W ⊇S, then I(W) ⊆I(S) because I is inclusion-reversing. But then
V(I(W)) ⊇V(I(S)) because V also reverses inclusions. Since W is an afﬁne variety,
V(I(W)) = W by Theorem 7 from §2, and the result follows.
□
This proposition leads to the following deﬁnition.
Deﬁnition 2. The Zariski closure of a subset of afﬁne space is the smallest afﬁne
algebraic variety containing the set. If S ⊆kn, the Zariski closure of S is denoted S
and is equal to V(I(S)).
We note the following properties of Zariski closure.
Lemma 3. Let S and T be subsets of kn. Then:
(i) I(S) = I(S).
(ii) If S ⊆T, then S ⊆T.
(iii) S ∪T = S ∪T.
Proof. For (i), the inclusion I(S) ⊆I(S) follows from S ⊆S. Going the other way,
f ∈I(S) implies S ⊆V( f). Then S ⊆S ⊆V( f) by Deﬁnition 2, so that f ∈I(S).
The proofs of (ii) and (iii) will be covered in the exercises.
□
A natural example of Zariski closure is given by elimination ideals. We can now
prove the ﬁrst assertion of the Closure Theorem (Theorem 3 of Chapter 3, §2).
Theorem 4 (The Closure Theorem, ﬁrst part). Assume k is algebraically closed.
Let V = V( f1, . . . , fs) ⊆kn, and let πl : kn →kn−l be projection onto the last n −l
coordinates. If Il is the l-th elimination ideal Il = ⟨f1, . . . , fs⟩∩k[xl+1, . . . , xn], then
V(Il) is the Zariski closure of πl(V).

200
Chapter 4 The Algebra-Geometry Dictionary
Proof. In view of Proposition 1, we must show that V(Il) = V(I(πl(V))). By
Lemma 1 of Chapter 3, §2, we have πl(V) ⊆V(Il). Since V(I(πl(V))) is the small-
est variety containing πl(V), it follows immediately that V(I(πl(V))) ⊆V(Il).
To get the opposite inclusion, suppose f ∈I(πl(V)), i.e., f(al+1, . . . , an) = 0 for
all (al+1, . . . , an) ∈πl(V). Then, considered as an element of k[x1, x2, . . . , xn], we
certainly have f(a1, a2, . . . , an) = 0 for all (a1, . . . , an) ∈V. By Hilbert's Nullstel-
lensatz, f N ∈⟨f1, . . . , fs⟩for some integer N. Since f does not depend on x1, . . . , xl,
neither does f N, and we have f N ∈⟨f1, . . . , fs⟩∩k[xl+1, . . . , xn] = Il. Thus, f ∈
√
Il,
which implies I(πl(V)) ⊆
√
Il. It follows that V(Il) = V(
√
Il) ⊆V(I(πl(V))), and
the theorem is proved.
□
The conclusion of Theorem 4 can be stated as V(Il) = πl(V). In general, if V is a
variety, then we say that a subset S ⊆V is Zariski dense in V if V = S, i.e., V is the
Zariski closure of S. is Thus Theorem 4 tells us that πl(V) is Zariski dense in V(Il)
when the ﬁeld is algebraically closed.
One context in which we encountered sets that were not varieties was in taking
the difference of varieties. For example, let V = V(I) where I ⊆k[x, y, z] is the
ideal ⟨xz, yz⟩and W = V(J) where J = ⟨z⟩. Then we have already seen that V is
the union of the (x, y)-plane and the z-axis. Since W is the (x, y)-plane, V \ W is the
z-axis with the origin removed [because the origin also belongs to the (x, y)-plane].
We have seen in Chapter 1 that this is not a variety. The z-axis [i.e., V(x, y)] is the
Zariski closure of V \ W.
We could ask if there is a general way to compute the ideal corresponding to
the Zariski closure V \ W of the difference of two varieties V and W. The answer
is afﬁrmative, but it involves two new algebraic constructions on ideals called ideal
quotients and saturations.
We begin with the ﬁrst construction.
Deﬁnition 5. If I, J are ideals in k[x1, . . . , xn], then I :J is the set
{f ∈k[x1, . . . , xn] | fg ∈I for all g ∈J}
and is called the ideal quotient (or colon ideal) of I by J.
So, for example, in k[x, y, z] we have
⟨xz, yz⟩: ⟨z⟩= {f ∈k[x, y, z] | f · z ∈⟨xz, yz⟩}
= {f ∈k[x, y, z] | f · z = Axz + Byz}
= {f ∈k[x, y, z] | f = Ax + By}
= ⟨x, y⟩.
Proposition 6. If I, J are ideals in k[x1, . . . , xn], then the ideal quotient I : J is an
ideal in k[x1, . . . , xn] and I : J contains I.
Proof. To show I :J contains I, note that because I is an ideal, if f ∈I, then fg ∈I
for all g ∈k[x1, . . . , xn] and, hence, certainly fg ∈I for all g ∈J. To show that I : J

§4 Zariski Closures, Ideal Quotients, and Saturations
201
is an ideal, ﬁrst note that 0 ∈I : J because 0 ∈I. Let f1, f2 ∈I :J. Then f1g and f2g
are in I for all g ∈J. Since I is an ideal ( f1+f2)g = f1g+f2g ∈I for all g ∈J. Thus,
f1 + f2 ∈I : J. To check closure under multiplication is equally straightforward: if
f ∈I : J and h ∈k[x1, . . . , xn], then fg ∈I and, since I is an ideal, hfg ∈I for all
g ∈J, which means that hf ∈I :J.
□
The algebraic properties of ideal quotients and methods for computing them will
be discussed later in the section. For now, we want to explore the relation between
ideal quotients and the Zariski closure of a difference of varieties.
Proposition 7.
(i) If I and J are ideals in k[x1, . . . , xn], then
V(I) = V(I + J) ∪V(I :J).
(ii) If V and W are varieties kn, then
V = (V ∩W) ∪(V \ W).
(iii) In the situation of (i), we have
V(I) \ V(J) ⊆V(I : J).
Proof. We begin with (ii). Since V contains V \ W and V is a variety, the smallest
variety containing V\W must be contained in V. Hence, V \ W ⊆V. Since V∩W ⊆
V, we have (V ∩W) ∪(V \ W) ⊆V.
To get the reverse containment, note that V = (V ∩W)∪(V \W). Since V \W ⊆
V \ W, the desired inclusion V ⊆(V ∩W) ∪V \ W follows immediately.
For (iii), we ﬁrst claim that I : J ⊆I(V(I) \ V(J)). For suppose that f ∈I : J and
a ∈V(I) \ V(J). Then fg ∈I for all g ∈J. Since a ∈V(I), we have f(a)g(a) = 0
for all g ∈J. Since a /∈V(J), there is some g ∈J such that g(a) ̸= 0. Hence,
f(a) = 0 for all a ∈V(I) \ V(J). Thus, f ∈I(V(I) \ V(J)), which proves the claim.
Since V reverses inclusions, we have V(I : J) ⊇V(I(V(I) \ V(J))) = V(I) \ V(J).
Finally, for (i), note that V(I + J) = V(I) ∩V(J) by Theorem 4 of §3. Then
applying (ii) with V = V(I) and W = V(J) gives
V(I) = V(I + J) ∪V(I) \ V(J) ⊆V(I + J) ∪V(I : J),
where the inclusion follows from (iii). But I ⊆I + J and I ⊆I : J imply that
V(I + J) ⊆V(I)
and
V(I : J) ⊆V(I).
These inclusions give V(I + J) ∪V(I : J) ⊆V(I), and then we are done.
□
In Proposition 7, note that V(I + J) from part (i) matches up with V ∩W in
part (ii) since V(I + J) = V(I) ∩V(J). So it is natural to ask if V(I : J) in part
(i) matches up with V \ W in part (ii). This is equivalent to asking if the inclusion
V(I) \ V(J) ⊆V(I : J) in part (iii) is an equality.

202
Chapter 4 The Algebra-Geometry Dictionary
Unfortunately, this can fail, even when the ﬁeld is algebraically closed. To see
what can go wrong, let I = ⟨x2(y −1)⟩and J = ⟨x⟩in the polynomial ring C[x, y].
Then one easily checks that
V(I) = V(x) ∪V(y −1) = V(J) ∪V(y −1) ⊆C2,
which is the union of the y-axis and the line y = 1. It follows without difﬁculty that
V(I) \ V(J) = V(y −1). However, the ideal quotient is
I : J = ⟨x2(y −1)⟩: ⟨x⟩= {f ∈C[x, y] | f · x = Ax2(y −1)}
= {f ∈C[x, y] | f = Ax(y −1)} = ⟨x(y −1)⟩.
Then V(I : J) = V(x(y −1)) = V(x) ∪V(y −1), which is strictly bigger than
V(I) \ V(J) = V(y −1). In other words, the inclusion in part (iii) of Proposition 7
can be strict, even over an algebraically closed ﬁeld.
However, if we replace J with J2, then a computation similar to the above gives
I : J2 = ⟨y −1⟩, so that V(I : J2) = V(I) \ V(J). In general, higher powers may be
required, which leads to our second algebraic construction on ideals.
Deﬁnition 8. If I, J are ideals in k[x1, . . . , xn], then I :J∞is the set
{f ∈k[x1, . . . , xn] | for all g ∈J, there is N ≥0 such that fgN ∈I}
and is called the saturation of I with respect to J.
Proposition 9. If I, J are ideals in k[x1, . . . , xn], then the saturation I : J∞is an ideal
in k[x1, . . . , xn]. Furthermore:
(i) I ⊆I : J ⊆I : J∞.
(ii) I :J∞= I : JN for all sufﬁciently large N.
(iii)
√
I :J∞=
√
I :J.
Proof. First observe that J1 ⊆J2 implies I :J2 ⊆I :J1. Since JN+1 ⊆JN for all N,
we obtain the ascending chain of ideals
(1)
I ⊆I : J ⊆I :J2 ⊆I : J3 ⊆· · · .
By the ACC, there is N such that I : JN = I : JN+1 = · · · . We claim that I :J∞=
I : JN. One inclusion is easy, for if f ∈I : JN and g ∈J, then gN ∈JN. Hence,
fgN ∈I, proving that f ∈I :J∞. For the other inclusion, take f ∈I : J∞and let
J = ⟨g1, . . . , gs⟩. By Deﬁnition 8, f times a power of gi lies in I. If M is the largest
such power, then fgM
i ∈I for i = 1, . . . , s. In the exercises, you will show that
JsM ⊆⟨gM
1 , . . . , gM
s ⟩.
This implies f JsM ⊆I, so that f ∈I : JsM. Then f ∈I : JN since (1) stabilizes at N.
Part (ii) follows from the claim just proved, and I :J∞= I : JN implies that I :J∞
is an ideal by Proposition 6. Note also that part (i) follows from (1) and part (ii) .

§4 Zariski Closures, Ideal Quotients, and Saturations
203
For part (iii), we ﬁrst show
√
I :J∞⊆
√
I : J. This is easy, for f ∈
√
I :J∞
implies f m ∈I : J∞for some m. Given g ∈J, it follows that f mgN ∈I for some N.
Then (fg)M ∈I for M = max(m, N), so that fg ∈
√
I. Since this holds for all g ∈J,
we conclude that f ∈
√
I : J.
For the opposite inclusion, take f ∈
√
I : J and write J = ⟨g1, . . . , gs⟩. Then
fgi ∈
√
I, so we can ﬁnd M with f MgM
i ∈I for all i. The argument from (ii) implies
f MJsM ⊆I, so
f M ∈I : JsM ⊆I : J∞.
It follows that f ∈
√
I :J∞, and the proof is complete.
□
Later in the section we will discuss further algebraic properties of saturations and
how to compute them. For now, we focus on their relation to geometry.
Theorem 10. Let I and J be ideals in k[x1, . . . , xn]. Then:
(i) V(I) = V(I + J) ∪V(I :J∞).
(ii) V(I) \ V(J) ⊆V(I :J∞).
(iii) If k is algebraically closed, then V(I : J∞) = V(I) \ V(J).
Proof. In the exercises, you will show that (i) and (ii) follow by easy modiﬁcations
of the proofs of parts (i) and (iii) of Proposition 7.
For (iii), suppose that k is algebraically closed. We ﬁrst show that
(2)
I(V(I) \ V(J)) ⊆
√
I :J.
Let f ∈I(V(I) \ V(J)). If g ∈J, then fg vanishes on V(I) because f vanishes on
V(I) \ V(J) and g on V(J). Thus, fg ∈I(V(I)), so fg ∈
√
I by the Nullstellensatz.
Since this holds for all g ∈J, we have f ∈
√
I : J, as claimed.
Since V is inclusion-reversing, (2) implies
V(
√
I : J) ⊆V(I(V(I) \ V(J))) = V(I) \ V(J).
However, we also have
V(I :J∞) = V(
√
I : J∞) = V(
√
I :J),
where the second equality follows from part (iii) of Proposition 9. Combining the
last two displays, we obtain
V(I :J∞) ⊆V(I) \ V(J).
Then (iii) follows immediately from this inclusion and (ii).
□
When k is algebraically closed, Theorem 10 and Theorem 4 of §3 imply that the
decomposition
V(I) = V(I + J) ∪V(I :J∞)
is precisely the decomposition

204
Chapter 4 The Algebra-Geometry Dictionary
V(I) = (V(I) ∩V(J)) ∪(V(I) \ V(J))
from part (ii) of Proposition 7. This shows that the saturation I : J∞is the ideal-
theoretic analog of the Zariski closure V(I) \ V(J).
In some situations, saturations can be replaced with ideal quotients. For example,
the proof of Theorem 10 yields the following corollary when the ideal I is radical.
Corollary 11. Let I and J be ideals in k[x1, . . . , xn]. If k is algebraically closed and
I is radical, then
V(I :J) = V(I) \ V(J).
You will prove this in the exercises. Another nice fact (also covered in the exer-
cises) is that if k is arbitrary and V and W are varieties in kn, then
I(V): I(W) = I(V \ W).
The following proposition takes care of some simple properties of ideal quotients
and saturations.
Proposition 12. Let I and J be ideals in k[x1, . . . , xn]. Then:
(i) I :k[x1, . . . , xn] = I : k[x1, . . . , xn]∞= I.
(ii) J ⊆I if and only if I :J = k[x1, . . . , xn].
(iii) J ⊆
√
I if and only if I :J∞= k[x1, . . . , xn].
Proof. The proof is left as an exercise.
□
When the ﬁeld is algebraically closed, the reader is urged to translate parts (i)
and (iii) of the proposition into terms of varieties (upon which they become clear).
The following proposition will help us compute ideal quotients and saturations.
Proposition 13. Let I and J1, . . . , Jr be ideals in k[x1, . . . , xn]. Then:
I :

r

i=1
Ji

=
r#
i=1

I : Ji
	
,
(3)
I :

r

i=1
Ji
∞=
r#
i=1

I : J∞
i
	
.
(4)
Proof. We again leave the (straightforward) proofs to the reader.
□
If f is a polynomial and I an ideal, we will often write I : f instead of I : ⟨f⟩, and
similarly I : f ∞instead of I : ⟨f⟩∞. Note that (3) and (4) imply that
(5)
I :⟨f1, f2, . . . , fr⟩=
r#
i=1
(I : fi) and I :⟨f1, f2, . . . , fr⟩∞=
r#
i=1
(I : f ∞
i
).
We now turn to the question of how to compute generators of the ideal quotient
I : J and saturation I :J∞, given generators of I and J. Inspired by (5), we begin with
the case when J is generated by a single polynomial.

§4 Zariski Closures, Ideal Quotients, and Saturations
205
Theorem 14. Let I be an ideal and g an element of k[x1, . . . , xn]. Then:
(i) If {h1, . . . , hp} is a basis of the ideal I ∩⟨g⟩, then {h1/g, . . ., hp/g} is a basis of
I :g.
(ii) If {f1, . . . , fs} is a basis of I and ˜I = ⟨f1, . . . , fs, 1−yg⟩⊆k[x1, . . . , xn, y], where
y is a new variable, then
I : g∞= ˜I ∩k[x1, . . . , xn].
Furthermore, if G is a lex Gröbner basis of ˜I for y > x1 > · · · > xn, then
G ∩k[x1, . . . , xn] is a basis of I : g∞.
Proof. For (i), observe that if h ∈⟨g⟩, then h = bg for some polynomial b ∈
k[x1, . . . , xn]. Thus, if f ∈⟨h1/g, . . . , hp/g⟩, then
hf = bgf ∈⟨h1, . . . , hp⟩= I ∩⟨g⟩⊆I.
Thus, f ∈I : g. Conversely, suppose f ∈I :g. Then fg ∈I. Since fg ∈⟨g⟩, we
have fg ∈I ∩⟨g⟩. If I ∩⟨g⟩= ⟨h1, . . . , hp⟩, this means fg =  rihi for some
polynomials ri. Since each hi ∈⟨g⟩, each hi/g is a polynomial, and we conclude
that f =  ri(hi/g), whence f ∈⟨h1/g, . . . , hp/g⟩.
The ﬁrst assertion of (ii) is left as an exercise. Then the Elimination Theorem
from Chapter 3, §1 implies that G ∩k[x1, . . . , xn] is a Gröbner basis of I : g∞.
□
This theorem, together with our procedure for computing intersections of ideals
and equation (5), immediately leads to an algorithm for computing a basis of an
ideal quotient: given I = ⟨f1, . . . , fr⟩and J = ⟨g1, . . . , gs⟩, to compute a basis
of I :J, we ﬁrst compute a basis for I : gi for each i. In view of Theorem 14, this
means computing a basis {h1, . . . , hp} of ⟨f1, . . . , fr⟩∩⟨gi⟩. Recall that we do this
via the algorithm for computing intersections of ideals from §3. Using the division
algorithm, we divide each of basis element hj by gi to get a basis for I : gi by part (i)
of Theorem 14. Finally, we compute a basis for I :J by applying the intersection
algorithm s−1 times, computing ﬁrst a basis for I : ⟨g1, g2⟩= (I :g1)∩(I : g2), then
a basis for I : ⟨g1, g2, g3⟩= (I : ⟨g1, g2⟩) ∩(I : g3), and so on.
Similarly, we have an algorithm for computing a basis of a saturation: given
I = ⟨f1, . . . , fr⟩and J = ⟨g1, . . . , gs⟩, to compute a basis of I : J∞, we ﬁrst compute
a basis for I :g∞
i
for each i using the method described in part (ii) of Theorem 14.
Then by (5), we need to intersect the ideals I :g∞
i , which we do as above by applying
the intersection algorithm s −1 times.
EXERCISES FOR §4
1. Find the Zariski closure of the following sets:
a. The projection of the hyperbola V(xy −1) in R2 onto the x-axis.
b. The boundary of the ﬁrst quadrant in R2.
c. The set {(x, y) ∈R2 | x2 + y2 ≤4}.
2. Complete the proof of Lemma 3. Hint: For part (iii), use Lemma 2 from Chapter 1, §2.
3. Let f = (x + y)2(x −y)(x + z2) and g = (x + z2)3(x −y)(z + y). Compute generators
for ⟨f ⟩: ⟨g⟩.

206
Chapter 4 The Algebra-Geometry Dictionary
4. Let I and J be ideals in k[x1, . . . , xn]. Show that if I is radical, then I : J is radical and
I : J = I :
√
J = I : J∞.
5. As in the proof of Proposition 9, assume J = ⟨g1, . . . , gs⟩. Prove that JsM ⊆⟨gM
1 , . . . , gM
s ⟩.
Hint: See the proof of Lemma 5 of §2.
6. Prove parts (i) and (ii) of Theorem 10. Hint: Adapt the proofs of parts (i) and (iii) of
Proposition 7.
7. Prove Corollary 11. Hint: Combine Theorem 10 and the Exercise 4. Another approach
would be look closely at the proof of Theorem 10 when I is radical.
8. Let V, W ⊆kn be varieties. Prove that I(V): I(W) = I(V \ W).
9. Prove Proposition 12 and ﬁnd geometric interpretations of parts (i) and (iii)
10. Prove Proposition 13 and ﬁnd a geometric interpretation of (4).
11. Prove I : g∞= ˜I ∩k[x1, . . . , xn] from part (ii) of Theorem 14. Hint: See the proof of
Proposition 8 of §2.
12. Show that Proposition 8 of §2 is a corollary of Proposition 12 and Theorem 14.
13. An example mentioned in the text used I = ⟨x2(y −1)⟩and J = ⟨x⟩. Compute I : J∞
and explain how your answer relates to the discussion in the text.
14. Let I, J ⊆k[x1, . . . , xn] be ideals. Prove that I : J∞= I : J N if and only if I : J N =
I : J N+1. Then use this to describe an algorithm for computing the saturation I : J∞based
on the algorithm for computing ideal quotients.
15. Show that N can be arbitrarily large in I : J∞= I : J N. Hint: Look at I = ⟨xN(y −1)⟩.
16. Let I, J, K ⊆k[x1, . . . , xn] be ideals. Prove the following:
a. IJ ⊆K if and only if I ⊆K : J.
b. (I : J) : K = I : JK.
17. Given ideals I1, . . . , Ir, J ⊆k[x1, . . . , xn], prove that
 "r
i=1 Ii

: J = "r
i=1(Ii : J). Then
prove a similar result for saturations and give a geometric interpretation.
18. Let A be an m × n constant matrix and suppose that x = Ay. where we are thinking of
x ∈km and y ∈kn as column vectors of variables. As in Exercise 13 of §3, deﬁne a map
αA : k[x1, . . . , xm] −→k[y1, . . . , yn]
by sending f ∈k[x1, . . . , xm] to αA( f ) ∈k[y1, . . . , yn], where αA( f ) is the polynomial
deﬁned by αA( f )(y) = f (Ay).
a. Show that αA(I : J) ⊆αA(I) : αA(J) with equality if I ⊇ker(αA) and αA is onto.
b. Show that α−1
A (I′ : J′) = α−1
A (I′) : α−1
A (J′) when αA is onto.
§5
Irreducible Varieties and Prime Ideals
We have already seen that the union of two varieties is a variety. For example, in
Chapter 1 and in the last section, we considered V(xz, yz), which is the union of
a line and a plane. Intuitively, it is natural to think of the line and the plane as
"more fundamental" than V(xz, yz). Intuition also tells us that a line or a plane are
"irreducible" or "indecomposable" in some sense: they do not obviously seem to be
a union of ﬁnitely many simpler varieties. We formalize this notion as follows.
Deﬁnition 1. An afﬁne variety V ⊆kn is irreducible if whenever V is written in
the form V = V1 ∪V2, where V1 and V2 are afﬁne varieties, then either V1 = V or
V2 = V.

§5 Irreducible Varieties and Prime Ideals
207
Thus, V(xz, yz) is not an irreducible variety. On the other hand, it is not com-
pletely clear when a variety is irreducible. If this deﬁnition is to correspond to our
geometric intuition, it is clear that a point, a line, and a plane ought to be irreducible.
For that matter, the twisted cubic V(y −x2, z −x3) in R3 appears to be irreducible.
But how do we prove this? The key is to capture this notion algebraically: if we can
characterize ideals which correspond to irreducible varieties, then perhaps we stand
a chance of establishing whether a variety is irreducible.
The following notion turns out to be the right one.
Deﬁnition 2. An ideal I ⊆k[x1, . . . , xn] is prime if whenever f, g ∈k[x1, . . . , xn]
and fg ∈I, then either f ∈I or g ∈I.
If we have set things up right, an irreducible variety will correspond to a prime
ideal and conversely. The following theorem assures us that this is indeed the case.
Proposition 3. Let V ⊆kn be an afﬁne variety. Then V is irreducible if and only if
I(V) is a prime ideal.
Proof. First, assume that V is irreducible and let fg ∈I(V). Set V1 = V ∩V( f) and
V2 = V ∩V(g); these are afﬁne varieties because an intersection of afﬁne varieties is
a variety. Then fg ∈I(V) easily implies that V = V1 ∪V2. Since V is irreducible, we
have either V = V1 or V = V2. Say the former holds, so that V = V1 = V ∩V( f).
This implies that f vanishes on V, so that f ∈I(V). Thus, I(V) is prime.
Next, assume that I(V) is prime and let V = V1 ∪V2. Suppose that V ̸= V1. We
claim that I(V) = I(V2). To prove this, note that I(V) ⊆I(V2) since V2 ⊆V. For
the opposite inclusion, ﬁrst note that I(V) ⊊I(V1) since V1 ⊊V. Thus, we can pick
f ∈I(V1) \ I(V). Now take any g ∈I(V2). Since V = V1 ∪V2, it follows that fg
vanishes on V, and, hence, fg ∈I(V). But I(V) is prime, so that f or g lies in I(V).
We know that f /∈I(V) and, thus, g ∈I(V). This proves I(V) = I(V2), whence
V = V2 because I is one-to-one. Thus, V is an irreducible variety.
□
It is an easy exercise to show that every prime ideal is radical. Then, using the
ideal-variety correspondence between radical ideals and varieties, we get the fol-
lowing corollary of Proposition 3.
Corollary 4. When k is algebraically closed, the functions I and V induce a one-
to-one correspondence between irreducible varieties in kn and prime ideals in
k[x1, . . . , xn].
As an example of how to use Proposition 3, let us prove that the ideal I(V) of the
twisted cubic is prime. Suppose that fg ∈I(V). Since the curve is parametrized by
(t, t2, t3), it follows that, for all t,
f(t, t2, t3)g(t, t2, t3) = 0.
This implies that f(t, t2, t3) or g(t, t2, t3) must be the zero polynomial, so that f or
g vanishes on V. Hence, f or g lies in I(V), proving that I(V) is a prime ideal.

208
Chapter 4 The Algebra-Geometry Dictionary
By the proposition, the twisted cubic is an irreducible variety in R3. One proves
that a straight line is irreducible in the same way: ﬁrst parametrize it, then apply the
above argument.
In fact, the above argument holds much more generally.
Proposition 5. If k is an inﬁnite ﬁeld and V ⊆kn is a variety deﬁned parametrically
x1 = f1(t1, . . . , tm),
...
xn = fn(t1, . . . , tm),
where f1, . . . , fn are polynomials in k[t1, . . . , tm], then V is irreducible.
Proof. As in §3 of Chapter 3, we let F : km →kn be deﬁned by
F(t1, . . . , tm) = ( f1(t1, . . . , tm), . . . , fn(t1, . . . , tm)).
Saying that V is deﬁned parametrically by the above equations means that V is the
Zariski closure of F(km). In particular, I(V) = I(F(km)).
For any polynomial g ∈k[x1, . . . , xn], the function g ◦F is a polynomial in
k[t1, . . . , tm]. In fact, g ◦F is the polynomial obtained by "plugging the polynomials
f1, . . . , fn into g":
g ◦F = g( f1(t1, . . . , tm), . . . , fn(t1, . . . , tm)).
Because k is inﬁnite, I(V) = I(F(km)) is the set of polynomials in k[x1, . . . , xn]
whose composition with F is the zero polynomial in k[t1, . . . , tm]:
I(V) = {g ∈k[x1, . . . , xn] | g ◦F = 0}.
Now suppose that gh ∈I(V). Then (gh) ◦F = (g ◦F)(h ◦F) = 0. (Make sure you
understand this.) But, if the product of two polynomials in k[t1, . . . , tm] is the zero
polynomial, one of them must be the zero polynomial. Hence, either g ◦F = 0 or
h ◦F = 0. This means that either g ∈I(V) or h ∈I(V). This shows that I(V) is a
prime ideal and, therefore, that V is irreducible.
□
With a little care, the above argument extends still further to show that any variety
deﬁned by a rational parametrization is irreducible.
Proposition 6. If k is an inﬁnite ﬁeld and V is a variety deﬁned by the rational
parametrization
x1 = f1(t1, . . . , tm)
g1(t1, . . . , tm),
...
xn = fn(t1, . . . , tm)
gn(t1, . . . , tm),
where f1, . . . , fn, g1, . . . , gn ∈k[t1, . . . , tm], then V is irreducible.

§5 Irreducible Varieties and Prime Ideals
209
Proof. Set W = V(g1g2 · · · gn) and let F : km \ W →kn deﬁned by
F(t1, . . . , tm) =
 f1(t1, . . . , tm)
g1(t1, . . . , tm), . . . , fn(tn, . . . , tm)
gn(t1, . . . , tm)

.
Then V is the Zariski closure of F(km \ W), which implies that I(V) is the set of
h ∈k[x1, . . . , xn] such that the function h ◦F is zero for all (t1, . . . , tm) ∈km \ W.
The difﬁculty is that h ◦F need not be a polynomial, and we, thus, cannot directly
apply the argument in the latter part of the proof of Proposition 5.
We can get around this difﬁculty as follows. Let h ∈k[x1, . . . , xn]. Since
g1(t1, . . . , tm)g2(t1, . . . , tm) · · · gn(t1, . . . , tm) ̸= 0
for any (t1, . . . , tm) ∈km \ W, the function (g1g2 · · · gn)N(h ◦F) is equal to zero
at precisely those values of (t1, . . . , tm) ∈km \ W for which h ◦F is equal to zero.
Moreover, if we let N be the total degree of h ∈k[x1, . . . , xn], then we leave it as
an exercise to show that (g1g2 · · · gn)N(h ◦F) is a polynomial in k[t1, . . . , tm]. We
deduce that h ∈I(V) if and only if (g1g2 · · · gn)N(h ◦F) is zero for all t ∈km \ W.
But, by Exercise 11 of Chapter 3, §3, this happens if and only if (g1g2 · · · gn)N(h◦F)
is the zero polynomial in k[t1, . . . , tm]. Thus, we have shown that
h ∈I(V)
if and only if
(g1g2 · · · gn)N(h ◦F) = 0 ∈k[t1, . . . , tm].
Now, we continue our proof that I(V) is prime. Suppose p, q ∈k[x1, . . . , xn]
satisfy p · q ∈I(V). If the total degrees of p and q are M and N, respectively, then
the total degree of p·q is M +N. Thus, (g1g2 · · · gn)M+N(p◦F)·(q◦F) = 0. But the
former is a product of the polynomials (g1g2 · · · gn)M(p◦F) and (g1g2 · · · gn)N(q◦F)
in k[t1, . . . , tm]. Hence one of them must be the zero polynomial. In particular, either
p ∈I(V) or q ∈I(V). This shows that I(V) is a prime ideal and, therefore, that V is
an irreducible variety.
□
The simplest variety in kn given by a parametrization consists of a single point,
{(a1, . . . , an)}. In the notation of Proposition 5, it is given by the parametrization in
which each fi is the constant polynomial fi(t1, . . . , tm) = ai, 1 ≤i ≤n. It is clearly
irreducible and it is easy to check that I({(a1, . . . , an)}) = ⟨x1−a1, . . . , xn−an⟩(see
Exercise 7), which implies that the latter is prime. The ideal ⟨x1 −a1, . . . , xn −an⟩
has another distinctive property: it is maximal in the sense that the only ideal which
strictly contains it is the whole ring k[x1, . . . , xn]. Such ideals are important enough
to merit special attention.
Deﬁnition 7. An ideal I ⊆k[x1, . . . , xn] is said to be maximal if I ̸= k[x1, . . . , xn]
and any ideal J containing I is such that either J = I or J = k[x1, . . . , xn].
In order to streamline statements, we make the following deﬁnition.
Deﬁnition 8. An ideal I ⊆k[x1, . . . , xn] is said to be proper if I is not equal to
k[x1, . . . , xn].

210
Chapter 4 The Algebra-Geometry Dictionary
Thus, an ideal is maximal if it is proper and no other proper ideal strictly con-
tains it. We now show that any ideal of the form ⟨x1 −a1, . . . , xn −an⟩is maximal.
Proposition 9. If k is any ﬁeld, an ideal I ⊆k[x1, . . . , xn] of the form
I = ⟨x1 −a1, . . . , xn −an⟩,
where a1, . . . , an ∈k, is maximal.
Proof. Suppose that J is some ideal strictly containing I. Then there must exist
f ∈J such that f /∈I. We can use the division algorithm to write f as A1(x1 −a1) +
· · · + An(xn −an) + b for some b ∈k. Since A1(x1 −a1) + · · · + An(xn −an) ∈I
and f /∈I, we must have b ̸= 0. However, since f ∈J and since A1(x1 −a1) + · · · +
An(xn −an) ∈I ⊆J, we also have
b = f −(A1(x1 −a1) + · · · + An(xn −an)) ∈J.
Since b is nonzero, 1 = 1/b · b ∈J, so J = k[x1, . . . , xn].
□
Since
V(x1 −a1, . . . , xn −an) = {(a1, . . . , an)},
every point (a1, . . . , an) ∈kn corresponds to a maximal ideal of k[x1, . . . , xn],
namely ⟨x1 −a1, . . . , xn −an⟩. The converse does not hold if k is not algebraically
closed. In the exercises, we ask you to show that ⟨x2 + 1⟩is maximal in R[x]. The
latter does not correspond to a point of R. The following result, however, holds in
any polynomial ring.
Proposition 10. If k is any ﬁeld, a maximal ideal in k[x1, . . . , xn] is prime.
Proof. Suppose that I is a proper ideal which is not prime and let fg ∈I, where
f /∈I and g /∈I. Consider the ideal ⟨f⟩+ I. This ideal strictly contains I because
f /∈I. Moreover, if we were to have ⟨f⟩+I = k[x1, . . . , xn], then 1 = cf +h for some
polynomial c and some h ∈I. Multiplying through by g would give g = cfg+hg ∈I
which would contradict our choice of g. Thus, I + ⟨f⟩is a proper ideal containing
I, so that I is not maximal.
□
Note that Propositions 9 and 10 together imply that ⟨x1−a1, . . . , xn−an⟩is prime
in k[x1, . . . , xn] even if k is not inﬁnite. Over an algebraically closed ﬁeld, it turns
out that every maximal ideal corresponds to some point of kn.
Theorem 11. If k is an algebraically closed ﬁeld, then every maximal ideal of
k[x1, . . . , xn] is of the form ⟨x1 −a1, . . . , xn −an⟩for some a1, . . . , an ∈k.
Proof. Let I ⊆k[x1, . . . , xn] be maximal. Since I ̸= k[x1, . . . , xn], we have
V(I) ̸= ∅by the Weak Nullstellensatz (Theorem 1 of §1). Hence, there is some

§5 Irreducible Varieties and Prime Ideals
211
point (a1, . . . , an) ∈V(I). This means that every f ∈I vanishes at (a1, . . . , an), so
that f ∈I({(a1, . . . , an)}). Thus, we can write
I ⊆I({(a1, . . . , an)}).
We have already observed that I({(a1, . . . , an)}) = ⟨x1 −a1, . . . , xn −an⟩(see
Exercise 7), and, thus, the above inclusion becomes
I ⊆⟨x1 −a1, . . . , xn −an⟩⊊k[x1, . . . , xn].
Since I is maximal, it follows that I = ⟨x1 −a1, . . . , xn −an⟩.
□
Note the proof of Theorem 11 uses the Weak Nullstellensatz. It is not difﬁcult to
see that it is, in fact, equivalent to the Weak Nullstellensatz.
We have the following easy corollary of Theorem 11.
Corollary 12. If k is an algebraically closed ﬁeld, then there is a one-to-one corre-
spondence between points of kn and maximal ideals of k[x1, . . . , xn].
Thus, we have extended our algebra-geometry dictionary. Over an algebraically
closed ﬁeld, every nonempty irreducible variety corresponds to a proper prime ideal,
and conversely. Every point corresponds to a maximal ideal, and conversely.
We can use Zariski closure to characterize when a variety is irreducible.
Proposition 13. A variety V is irreducible if and only if for every variety W ⊊V,
the difference V \ W is Zariski dense in V.
Proof. First assume that V is irreducible and take W ⊊V. Then Proposition 7 of §4
gives the decomposition V = W ∪V \ W. Since V is irreducible and V ̸= W, this
forces V = V \ W.
For the converse, suppose that V = V1 ∪V2. If V1 ⊊V, then V \ V1 = V. But
V \ V1 ⊆V2, so that V \ V1 ⊆V2. This implies V ⊆V2, and V = V2 follows.
□
Let us make a ﬁnal comment about terminology. Some references, such as
HARTSHORNE (1977), use the term "variety" for what we call an irreducible variety
and say "algebraic set" instead of variety. When reading other books on algebraic
geometry, be sure to check the deﬁnitions!
EXERCISES FOR §5
1. If h ∈k[x1, . . . , xn] has total degree N and if F is as in Proposition 6, show that
(g1g2 . . . gn)N(h ◦F) is a polynomial in k[t1, . . . , tm].
2. Show that a prime ideal is radical.
3. Show that an ideal I is prime if and only if for any ideals J and K such that JK ⊆I, either
J ⊆I or K ⊆I.
4. Let I1, . . . , In be ideals and P a prime ideal containing "n
i=1 Ii. Then prove that P ⊇Ii for
some i. Further, if P = "n
i=1 Ii, show that P = Ii for some i.
5. Express f = x2z −6y4 + 2xy3z in the form f = f1(x, y, z)(x + 3) + f2(x, y, z)(y −1) +
f3(x, y, z)(z −2) for some f1, f2, f3 ∈k[x, y, z].

212
Chapter 4 The Algebra-Geometry Dictionary
6. Let k be an inﬁnite ﬁeld.
a. Show that any straight line in kn is irreducible.
b. Prove that any linear subspace of kn is irreducible. Hint: Parametrize and use Propo-
sition 5.
7. Show that
I({(a1, . . . , an)}) = ⟨x1 −a1, . . . , xn −an⟩.
8. Show the following:
a. ⟨x2 + 1⟩is maximal in R[x].
b. If I ⊆R[x1, . . . , xn] is maximal, show that V(I) is either empty or a point in Rn. Hint:
Examine the proof of Theorem 11.
c. Give an example of a maximal ideal I in R[x1, . . . , xn] for which V(I) = ∅. Hint:
Consider the ideal ⟨x2
1 + 1, x2, . . . , xn⟩.
9. Suppose that k is a ﬁeld which is not algebraically closed.
a. Show that if I ⊆k[x1, . . . , xn] is maximal, then V(I) is either empty or a point in kn.
Hint: Examine the proof of Theorem 11.
b. Show that there exists a maximal ideal I in k[x1, . . . , xn] for which V(I) = ∅. Hint:
See the previous exercise.
c. Conclude that if k is not algebraically closed, there is always a maximal ideal of
k[x1, . . . , xn] which is not of the form ⟨x1 −a1, . . . , xn −an⟩.
10. Prove that Theorem 11 implies the Weak Nullstellensatz.
11. If f ∈C[x1, . . . , xn] is irreducible, then V( f ) is irreducible. Hint: Show that ⟨f ⟩is prime.
12. Prove that if I is any proper ideal in C[x1, . . . , xn], then
√
I is the intersection of all
maximal ideals containing I. Hint: Use Theorem 11.
13. Let f1, . . . , fn ∈k[x1] be polynomials of one variable and consider the ideal
I = ⟨f1(x1), x2 −f2(x1), . . . , xn −fn(x1)⟩⊆k[x1, . . . , xn].
We also assume that deg( f1) = m > 0.
a. Show that every f ∈k[x1, . . . , xn] can be written uniquely as f = q + r where q ∈I
and r ∈k[x1] with either r = 0 or deg(r) < m. Hint: Use lex order with x1 last.
b. Let f ∈k[x1]. Use part (a) to show that f ∈I if and only if f is divisible by f1 in k[x1].
c. Prove that I is prime if and only if f1 ∈k[x1] is irreducible.
d. Prove that I is radical if and only if f1 ∈k[x1] is square-free.
e. Prove that
√
I = ⟨( f1)red⟩+ I, where (f1)red is deﬁned in §2.
§6
Decomposition of a Variety into Irreducibles
In the last section, we saw that irreducible varieties arise naturally in many contexts.
It is natural to ask whether an arbitrary variety can be built up out of irreducibles. In
this section, we explore this and related questions.
We begin by translating the ascending chain condition (ACC) for ideals (see §5
of Chapter 2) into the language of varieties.
Proposition 1 (The Descending Chain Condition). Any descending chain of va-
rieties
V1 ⊇V2 ⊇V3 ⊇· · ·
in kn must stabilize, meaning that there exists a positive integer N such that VN =
VN+1 = · · · .

§6 Decomposition of a Variety into Irreducibles
213
Proof. Passing to the corresponding ideals gives an ascending chain of ideals
I(V1) ⊆I(V2) ⊆I(V3) ⊆· · · .
By the ascending chain condition for ideals (see Theorem 7 of Chapter 2, §5), there
exists N such that I(VN) = I(VN+1) = · · · . Since V(I(V)) = V for any variety V,
we have VN = VN+1 = · · · .
□
We can use Proposition 1 to prove the following basic result about the structure
of afﬁne varieties.
Theorem 2. Let V ⊆kn be an afﬁne variety. Then V can be written as a ﬁnite union
V = V1 ∪· · · ∪Vm,
where each Vi is an irreducible variety.
Proof. Assume that V is an afﬁne variety which cannot be written as a ﬁnite union
of irreducibles. Then V is not irreducible, so that V = V1 ∪V′
1, where V ̸= V1 and
V ̸= V′
1. Further, one of V1 and V′
1 must not be a ﬁnite union of irreducibles, for
otherwise V would be of the same form. Say V1 is not a ﬁnite union of irreducibles.
Repeating the argument just given, we can write V1 = V2∪V′
2, where V1 ̸= V2, V1 ̸=
V′
2, and V2 is not a ﬁnite union of irreducibles. Continuing in this way gives us an
inﬁnite sequence of afﬁne varieties
V ⊇V1 ⊇V2 ⊇· · ·
with
V ̸= V1 ̸= V2 ̸= · · · .
This contradicts Proposition 1.
□
As a simple example of Theorem 2, consider the variety V(xz, yz) which is a
union of a line (the z-axis) and a plane [the (x, y)-plane], both of which are irre-
ducible by Exercise 6 of §5. For a more complicated example of the decomposition
of a variety into irreducibles, consider the variety
V = V(xz −y2, x3 −yz).
A sketch of this variety appears at the top of the next page. The picture suggests that
this variety is not irreducible. It appears to be a union of two curves. Indeed, since
both xz −y2 and x3 −yz vanish on the z-axis, it is clear that the z-axis V(x, y) is
contained in V. What about the other curve V \ V(x, y)?

214
Chapter 4 The Algebra-Geometry Dictionary
By Corollary 11 of §4, this suggests looking at the ideal quotient
⟨xz −y2, x3 −yz⟩: ⟨x, y⟩.
(At the end of the section we will see that ⟨xz −y2, x3 −yz⟩is a radical ideal.) We
can compute this quotient using our algorithm for computing ideal quotients (make
sure you review this algorithm). By equation (5) of §4, the above is equal to
(I : x) ∩(I : y),
where I = ⟨xz −y2, x3 −yz⟩. To compute I :x, we ﬁrst compute I ∩⟨x⟩using our
algorithm for computing intersections of ideals. Using lex order with z > y > x, we
obtain
I ∩⟨x⟩= ⟨x2z −xy2, x4 −xyz, x3y −xz2, x5 −xy3⟩.
We can omit x5 −xy3 since it is a combination of the ﬁrst and second elements in
the basis. Hence
(1)
I :x =
x2z −xy2
x
, x4 −xyz
x
, x3y −xz2
x

= ⟨xz −y2, x3 −yz, x2y −z2⟩
= I + ⟨x2y −z2⟩.
Similarly, to compute I : ⟨y⟩, we compute
I ∩⟨y⟩= ⟨xyz −y3, x3y −y2z, x2y2 −yz2⟩,

§6 Decomposition of a Variety into Irreducibles
215
which gives
I :y =
xyz −y3
y
, x3y −y2z
y
, x2y2 −yz2
y

= ⟨xz −y2, x3 −yz, x2y −z2⟩
= I + ⟨x2y −z2⟩
= I :x.
(Do the computations using a computer algebra system.) Since I : x = I :y, we have
I : ⟨x, y⟩= ⟨xz −y2, x3 −yz, x2y −z2⟩.
The variety W = V(xz−y2, x3 −yz, x2y−z2) turns out to be an irreducible curve. To
see this, note that it can be parametrized as (t3, t4, t5) [it is clear that (t3, t4, t5) ∈W
for any t—we leave it as an exercise to show every point of W is of this form], so
that W is irreducible by Proposition 5 of the last section. It then follows easily that
V = V(I) = V(x, y) ∪V(I :⟨x, y⟩) = V(x, y) ∪W
(see Proposition 7 of §4), which gives decomposition of V into irreducibles.
Both in the above example and the case of V(xz, yz), it appears that the decom-
position of a variety into irreducible pieces is unique. It is natural to ask whether
this is true in general. It is clear that, to avoid trivialities, we must rule out decom-
positions in which the same irreducible piece appears more than once, or in which
one irreducible piece contains another. This is the aim of the following deﬁnition.
Deﬁnition 3. Let V ⊆kn be an afﬁne variety. A decomposition
V = V1 ∪· · · ∪Vm,
where each Vi is an irreducible variety, is called a minimal decomposition (or,
sometimes, an irredundant union) if Vi ̸⊆Vj for i ̸= j. Also, we call the Vi the
irreducible components of V.
With this deﬁnition, we can now prove the following uniqueness result.
Theorem 4. Let V ⊆kn be an afﬁne variety. Then V has a minimal decomposition
V = V1 ∪· · · ∪Vm
(so each Vi is an irreducible variety and Vi ̸⊆Vj for i ̸= j). Furthermore, this
minimal decomposition is unique up to the order in which V1, . . . , Vm are written.
Proof. By Theorem 2, V can be written in the form V = V1 ∪· · · ∪Vm, where each
Vi is irreducible. Further, if a Vi lies in some Vj for i ̸= j, we can drop Vi, and V
will be the union of the remaining Vj's for j ̸= i. Repeating this process leads to a
minimal decomposition of V.

216
Chapter 4 The Algebra-Geometry Dictionary
To show uniqueness, suppose that V = V′
1 ∪· · · ∪V′
l is another minimal decom-
position of V. Then, for each Vi in the ﬁrst decomposition, we have
Vi = Vi ∩V = Vi ∩(V′
1 ∪· · · ∪V′
l ) = (Vi ∩V′
1) ∪· · · ∪(Vi ∩V′
l ).
Since Vi is irreducible, it follows that Vi = Vi∩V′
j for some j, i.e., Vi ⊆V′
j . Applying
the same argument to V′
j (using the Vi's to decompose V) shows that V′
j ⊆Vk for
some k, and, thus,
Vi ⊆V′
j ⊆Vk.
By minimality, i = k, and it follows that Vi = V′
j . Hence, every Vi appears in
V = V′
1 ∪· · · ∪V′
l , which implies m ≤l. A similar argument proves l ≤m, and
m = l follows. Thus, the V′
i 's are just a permutation of the Vi's, and uniqueness is
proved.
□
The uniqueness part of Theorem 4 guarantees that the irreducible components
of V are well-deﬁned. We remark that the uniqueness is false if one does not insist
that the union be ﬁnite. (A plane P is the union of all the points on it. It is also the
union of some line in P and all the points not on the line—there are inﬁnitely many
lines in P with which one could start.) This should alert the reader to the fact that
although the proof of Theorem 4 is easy, it is far from vacuous: one makes subtle
use of ﬁniteness (which follows, in turn, from the Hilbert Basis Theorem).
Here is a result that relates irreducible components to Zariski closure.
Proposition 5. Let V, W be varieties with W ⊊V. Then V \ W is Zariski dense in V
if and only if W contains no irreducible component of V.
Proof. Suppose that V = V1 ∪· · · ∪Vm as in Theorem 4 and that Vi ̸⊆W for all i.
This implies Vi ∩W ⊊Vi, and since Vi is irreducible, we deduce Vi \ (Vi ∩W) = Vi
by Proposition 13 of §5. Then
V \ W = (V1 ∪· · · ∪Vm) \ W = (V1 \ (V1 ∩W)) ∪· · · ∪(Vm \ (Vm ∩W))
= V1 \ (V1 ∩W) ∪· · · ∪Vm \ (Vm ∩W)
= V1 ∪· · · ∪Vm = V,
where the second line uses Lemma 3 of §4. The other direction of the proof will be
covered in the exercises.
□
Theorems 2 and 4 can also be expressed purely algebraically using the one-to-one
correspondence between radical ideals and varieties.
Theorem 6. If k is algebraically closed, then every radical ideal in k[x1, . . . , xn]
can be written uniquely as a ﬁnite intersection of prime ideals P1 ∩· · · ∩Pr, where
Pi ̸⊆Pj for i ̸= j. (As in the case of varieties, we often call such a presentation of a
radical ideal a minimal decomposition or an irredundant intersection).
Proof. Theorem 6 follows immediately from Theorems 2 and 4 and the ideal-
variety correspondence.
□

§6 Decomposition of a Variety into Irreducibles
217
We can also use ideal quotients from §4 to describe the prime ideals that appear
in the minimal representation of a radical ideal.
Theorem 7. If k is algebraically closed and I is a proper radical ideal such that
I =
r#
i=1
Pi
is its minimal decomposition as an intersection of prime ideals, then the Pi's are
precisely the proper prime ideals that occur in the set {I : f | f ∈k[x1, . . . , xn]}.
Proof. First, note that since I is proper, each Pi is also a proper ideal (this follows
from minimality).
For any f ∈k[x1, . . . , xn], we have
I : f =

r#
i=1
Pi

: f =
r#
i=1
(Pi : f)
by Exercise 17 of §4. Note also that for any prime ideal P, either f ∈P, in which
case P: f = ⟨1⟩, or f /∈P, in which case P: f = P (see Exercise 3).
Now suppose that I : f is a proper prime ideal. By Exercise 4 of §5, the above
formula for I : f implies that I : f = Pi : f for some i. Since Pi :f = Pi or ⟨1⟩by the
above observation, it follows that I : f = Pi.
To see that every Pi can arise in this way, ﬁx i and pick f ∈
 $r
j̸=i Pj
	
\ Pi; such
an f exists because $r
i=1 Pi is minimal. Then Pi : f = Pi and Pj : f = ⟨1⟩for j ̸= i. If
we combine this with the above formula for I : f, then it follows that I : f = Pi.
□
We should mention that Theorems 6 and 7 hold for any ﬁeld k, although the
proofs in the general case are different (see Corollary 10 of §8).
For an example of these theorems, consider the ideal I = ⟨xz−y2, x3−yz⟩. Recall
that the variety V = V(I) was discussed earlier in this section. For the time being,
let us assume that I is radical (eventually we will see that this is true). Can we write
I as an intersection of prime ideals?
We start with the geometric decomposition
V = V(x, y) ∪W
proved earlier, where W = V(xz −y2, x3 −yz, x2y −z2). This suggests that
I = ⟨x, y⟩∩⟨xz −y2, x3 −yz, x2y −z2⟩,
which is straightforward to prove by the techniques we have learned so far (see
Exercise 4). Also, from equation (1), we know that I :x = ⟨xz−y2, x3−yz, x2y−z2⟩.
Thus,
I = ⟨x, y⟩∩(I : x).

218
Chapter 4 The Algebra-Geometry Dictionary
To represent ⟨x, y⟩as an ideal quotient of I, let us think geometrically. The idea is
to remove W from V. Of the three equations deﬁning W, the ﬁrst two give V. So it
makes sense to use the third one, x2y −z2, and one can check that I :(x2y −z2) =
⟨x, y⟩(see Exercise 4). Thus,
(2)
I = (I :(x2y −z2)) ∩(I : x).
It remains to show that I : (x2y −z2) and I :x are prime ideals. The ﬁrst is easy
since I : (x2y −z2) = ⟨x, y⟩is obviously prime. As for the second, we have already
seen that W = V(xz −y2, x3 −yz, x2y −z2) is irreducible and, in the exercises,
you will show that I(W) = ⟨xz −y2, x3 −yz, x2y −z2⟩= I :x. It follows from
Proposition 3 of §5 that I : x is a prime ideal. This completes the proof that (2) is the
minimal representation of I as an intersection of prime ideals. Finally, since I is an
intersection of prime ideals, we see that I is a radical ideal (see Exercise 1).
The arguments used in this example are special to the case I = ⟨xz −y2, x3 −yz⟩.
It would be nice to have more general methods that could be applied to any ideal.
Theorems 2, 4, 6, and 7 tell us that certain decompositions exist, but the proofs give
no indication of how to ﬁnd them. The problem is that the proofs rely on the Hilbert
Basis Theorem, which is intrinsically nonconstructive. Based on what we have seen
in §§5 and 6, the following questions arise naturally:
• (Primality) Is there an algorithm for deciding if a given ideal is prime?
• (Irreducibility) Is there an algorithm for deciding if a given afﬁne variety is irre-
ducible?
• (Decomposition) Is there an algorithm for ﬁnding the minimal decomposition of
a given variety or radical ideal?
The answer to all three questions is yes, and descriptions of the algorithms can
be found in the works of HERMANN (1926), MINES, RICHMAN, and RUITEN-
BERG (1988), and SEIDENBERG (1974, 1984). As in §2, the algorithms in these ar-
ticles are very inefﬁcient. However, the work of GIANNI, TRAGER and ZACHARIAS
(1988) and EISENBUD, HUNEKE and VASCONCELOS (1992) has led to more efﬁ-
cient algorithms. See also Chapter 8 of BECKER and WEISPFENNING (1993) and
§4.4 of ADAMS and LOUSTAUNAU (1994).
EXERCISES FOR §6
1. Show that the intersection of any collection of prime ideals is radical.
2. Show that an irredundant intersection of at least two prime ideals is never prime.
3. If P ⊆k[x1, . . . , xn] is a prime ideal, then prove that P : f = P if f /∈P and P : f = ⟨1⟩if
f ∈P.
4. Let I = ⟨xz −y2, x3 −yz⟩.
a. Show that I : (x2y −z2) = ⟨x, y⟩.
b. Show that I : (x2y −z2) is prime.
c. Show that I = ⟨x, y⟩∩⟨xz −y2, x3 −yz, z2 −x2y⟩.
5. Let J = ⟨xz −y2, x3 −yz, z2 −x2y⟩⊆k[x, y, z], where k is inﬁnite.
a. Show that every point of W = V(J) is of the form (t3, t4, t5) for some t ∈k.

§7 Proof of the Closure Theorem
219
b. Show that J = I(W). Hint: Compute a Gröbner basis for J using lex order with
z > y > x and show that every f ∈k[x, y, z] can be written in the form
f = g + a + bz + xA(x) + yB(x) + y2C(x),
where g ∈J, a, b ∈k and A, B, C ∈k[x].
6. Complete the proof of Proposition 5. Hint: Vi ⊆W implies V \ W ⊆V \ Vi.
7. Translate Theorem 7 and its proof into geometry.
8. Let I = ⟨xz −y2, z3 −x5⟩⊆Q[x, y, z].
a. Express V(I) as a ﬁnite union of irreducible varieties. Hint: The parametrizations
(t3, t4, t5) and (t3, −t4, t5) will be useful.
b. Express I as an intersection of prime ideals which are ideal quotients of I and conclude
that I is radical.
9. Let V, W be varieties in kn with V ⊆W. Show that each irreducible component of V is
contained in some irreducible component of W.
10. Let f ∈C[x1, . . . , xn] and let f = f a1
1 f a2
2 · · · f ar
r
be the decomposition of f into irreducible
factors. Show that V( f ) = V( f1) ∪· · · ∪V( fr) is the decomposition of V( f ) into irre-
ducible components and I(V( f )) = ⟨f1f2 · · · fr⟩. Hint: See Exercise 11 of §5.
§7
Proof of the Closure Theorem
This section will complete the proof of the Closure Theorem from Chapter 3, §2. We
will use many of the tools introduced in this chapter, including the Nullstellensatz,
Zariski closures, saturations, and irreducible components.
We begin by recalling the basic situation. Let k be an algebraically closed ﬁeld,
and let πl : kn →kn−l is projection onto the last n −l components. If V = V(I) is
an afﬁne variety in kn, then we get the l-th elimination ideal Il = I ∩k[xl+1, . . . , xn].
The ﬁrst part of the Closure Theorem, which asserts that V(Il) is the Zariski closure
of πl(V) in kn−l, was proved earlier in Theorem 4 of §4.
The remaining part of the Closure Theorem tells us that πl(V) ﬁlls up "most" of
V(Il). Here is the precise statement.
Theorem 1 (The Closure Theorem, second part). Let k be algebraically closed,
and let V = V(I) ⊆kn. Then there is an afﬁne variety W ⊆V(Il) such that
V(Il) \ W ⊆πl(V) and V(Il) \ W = V(Il).
This is slightly different from the Closure Theorem stated in §2 of Chapter 3.
There, we assumed V ̸= ∅and asserted that V(Il)\W ⊆πl(V) for some W ⊊V(Il).
In Exercise 1 you will prove that Theorem 1 implies the version in Chapter 3.
The proof of Theorem 1 will use the following notation. Rename xl+1, . . . , xn as
yl+1, . . . , yn and write k[x1, . . . , xl, yl+1, . . . , yn] as k[x, y] for x = (x1, . . . , xl) and
y = (yl+1, . . . , yn). Also ﬁx a monomial order > on k[x, y] with the property that
xα > xβ implies xα > xβyγ for all γ. The product order described in Exercise 9 of
Chapter 2, §4 is an example of such a monomial order. Another example is given by
lex order with x1 > · · · > xl > yl+1 > · · · > yn.

220
Chapter 4 The Algebra-Geometry Dictionary
An important tool in proving Theorem 1 is the following result.
Theorem 2. Fix a ﬁeld k. Let I ⊆k[x, y] be an ideal and let G = {g1, . . . , gt} be a
Gröbner basis for I with respect to a monomial order as above. For 1 ≤i ≤t with
gi /∈k[y], write gi in the form
(1)
gi = ci(y)xαi + terms < xαi.
Finally, assume that b = (al+1, . . . , an) ∈V(Il) ⊆kn−l is a partial solution such
that ci(b) ̸= 0 for all gi /∈k[y]. Then:
(i) The set
G = {gi(x, b) | gi /∈k[y]} ⊆k[x]
is a Gröbner basis of the ideal {f(x, b) | f ∈I}.
(ii) If k is algebraically closed, then there exists a = (a1, . . . , al) ∈kl such that
(a, b) ∈V = V(I).
Proof. Given f ∈k[x, y], we set
¯f = f(x, b) ∈k[x].
In this notation, G = {¯gi | gi /∈k[y]}. Also observe that ¯gi = 0 when gi ∈k[y] since
b ∈V(Il). If we set ¯I = {¯f | f ∈I}, then it is an easy exercise to show that
¯I = ⟨G⟩⊆k[x].
In particular, ¯I is an ideal of k[x].
To prove that G is a Gröbner basis of ¯I, take gi, gj ∈G \ k[y] and consider the
polynomial
S = cj(y) xγ
xαi gi −ci(y) xγ
xαj gj,
where xγ = lcm(xαi, xαj). Our chosen monomial order has the property that
LT(gi) = LT(ci(y))xαi, and it follows easily that xγ > LT(S). Since S ∈I, it has a
standard representation S = t
k=1 Akgk. Then evaluating at b gives
cj(b) xγ
xαi ¯gi −ci(b) xγ
xαj ¯gj = S = 
¯gk∈G Ak¯gk
since ¯gi = 0 for gi ∈k[y].
Then ci(b), cj(b) ̸= 0 imply that S is the S-polynomial S(¯gi, ¯gj) up to the nonzero
constant ci(b)cj(b). Since
xγ > LT(S) ≥LT(Akgk),
Akgk ̸= 0,
it follows that
xγ > LT(¯Ak¯gk),
¯Ak¯gk ̸= 0,

§7 Proof of the Closure Theorem
221
by Exercise 3 of Chapter 2, §9. Hence S(¯gi, ¯gj) has an lcm representation as deﬁned
in Chapter 2, §9. By Theorem 6 of that section, we conclude that G is a Gröbner
basis of ¯I, as claimed.
For part (ii), note that by construction, every element of G has positive total
degree in the x variables, so that ¯gi is nonconstant for every i. It follows that 1 /∈¯I
since G is a Gröbner basis of ¯I. Hence ¯I ⊊k[x], so that by the Nullstellensatz,
there exists a ∈kl such that ¯gi(a) = 0 for all ¯gi ∈G, i.e., gi(a, b) = 0 for all
gi ∈G\k[y]. Since ¯gi = 0 when gi ∈k[y], it follows that gi(a, b) = 0 for all gi ∈G.
Hence (a, b) ∈V = V(I).
□
Part (ii) of Theorem 2 is related to the Extension Theorem from Chapter 3. Com-
pared to the Extension theorem, part (ii) is simultaneously stronger (the Extension
Theorem assumes l = 1, i.e., just one variable is eliminated) and weaker [part (ii)
requires the nonvanishing of all relevant leading coefﬁcients, while the Extension
Theorem requires just one].
For our purposes, Theorem 2 has the following important corollary.
Corollary 3. With the same notation as Theorem 2, we have
V(Il) \ V
 
gi∈G\k[y] ci
	
⊆πl(V).
Proof. Take b ∈V(Il) \ V
 
gi∈G\k[y] ci
	
. Then b ∈V(Il) and ci(b) ̸= 0 for all
gi ∈G \ k[y]. By Theorem 2, there is a ∈kl such that (a, b) ∈V = V(I). In other
words, b ∈πl(V), and the corollary follows.
□
Since A \ B = A \ (A ∩B), Corollary 3 implies that the intersection
W = V(Il) ∩V
 
gi∈G\k[y] ci
	
⊆V(Il)
has the property that V(Il) \ W ⊆πl(V). If V(Il) \ W is also Zariski dense in V(Il),
then W ⊆V(Il) satisﬁes the conclusion of the Closure Theorem.
Hence, to complete the proof of the Closure Theorem, we need to explore what
happens when the difference V(Il) \ V
 
gi∈G\k[y] ci
	
is not Zariski dense in V(Il).
The following proposition shows that in this case, the original variety V = V(I)
decomposes into varieties coming from strictly bigger ideals.
Proposition 4. Assume that k is algebraically closed and the Gröbner basis G is
reduced. If V(Il) \ V
 
gi∈G\k[y] ci
	
is not Zariski dense in V(Il), then there is some
gi ∈G \ k[y] whose ci has the following two properties:
(i) V = V(I + ⟨ci⟩) ∪V(I : c∞
i ).
(ii) I ⊊I + ⟨ci⟩and I ⊊I :c∞
i .
Proof. For (i), we have V = V(I) = V(I + ⟨ci⟩) ∪V(I : c∞
i ) by Theorem 10 of §4.
For (ii), we ﬁrst show that I ⊊I + ⟨ci⟩for all gi ∈G \ k[y]. To see why, suppose
that ci ∈I for some i. Since G is a Gröbner basis of I, LT(ci) is divisible by some
LT(gj), and then gj ∈k[y] since the monomial order eliminates the x variables.

222
Chapter 4 The Algebra-Geometry Dictionary
Hence gj ̸= gi. But then (1) implies that LT(gj) divides LT(gi) = LT(ci)xαi, which
contradicts our assumption that G is reduced. Hence ci /∈I, and I ⊊I +⟨ci⟩follows.
Now suppose that I = I :c∞
i
for all i with gi ∈G \ k[y]. In Exercise 4, you will
show that this implies Il : c∞
i
= Il for all i. Hence
V(Il) = V(Il :c∞
i ) = V(Il) \ V(ci) = V(Il) \ (V(Il) ∩V(ci)),
where the second equality uses Theorem 10 of §4. If follows that V(Il) ∩V(ci)
contains no irreducible component of V(Il) by Proposition 5 of §6. Since this holds
for all i, the ﬁnite union

gi∈G\k[y]V(Il) ∩V(ci) = V(Il) ∩
gi∈G\k[y]V(ci) = V(Il) ∩V
 
gi∈G\k[y] ci
	
also contains no irreducible component of V(Il) (see Exercise 5). By the same
proposition from §6, we conclude that the difference
V(Il) \

V(Il) ∩V
 
gi∈G\k[y] ci
		
= V(Il) \ V
 
gi∈G\k[y] ci
	
is Zariski dense in V(Il). This contradiction shows that I ⊊I :c∞
i
for some i and
completes the proof of the proposition.
□
In the situation of Proposition 4, we have a decomposition of V into two pieces.
The next step is to show that if we can ﬁnd a W that works for each piece, then we
can ﬁnd a W what works for V. Here is the precise result.
Proposition 5. Let k be algebraically closed. Suppose that a variety V = V(I) can
be written V = V(I(1)) ∪V(I(2)) and that we have varieties
W1 ⊆V(I(1)
l
)
and
W2 ⊆V(I(2)
l
)
such that V(I(i)
l ) \ Wi = V(I(i)
l ) and V(I(i)
l ) \ Wi ⊆πl(V(I(i)) for i = 1, 2. Then
W = W1 ∪W2 is a variety contained in V that satisﬁes
V(Il) \ W = V(Il)
and
V(Il) \ W ⊆πl(V).
Proof. For simplicity, set Vi = V(I(i)), so that V = V1 ∪V2. The ﬁrst part of the
Closure Theorem proved in §4 implies that V(Il) = πl(V) and V(I(i)
l ) = πl(Vi).
Hence
V(Il) = πl(V) = πl(V1 ∪V2) = πl(V1) ∪πl(V2) = πl(V1) ∪πl(V2)
= V(I(1)
l
) ∪V(I(2)
l
),
where the last equality of the ﬁrst line uses Lemma 3 of §4.
Now let Wi ⊆V(I(i)
l ) be as in the statement of the proposition. By Proposition 5
of §6, we know that Wi contains no irreducible component of V(I(i)
l ). As you will
prove in Exercise 5, this implies that the union W = W1∪W2 contains no irreducible

§7 Proof of the Closure Theorem
223
component of V(Il) = V(I(1)
l
)∪V(I(2)
l
). Using Proposition 5 of §6 again, we deduce
that V(Il) \ W is Zariski dense in V(Il). Since we also have
V(Il) \ W =

V(I(1)
l
) ∪V(I(2)
l
)
	
\ (W1 ∪W2) ⊆

V(I(1)
l
) \ W1
	
∪

V(I(2)
l
) \ W2
	
⊆πl(V1) ∪πl(V2) = πl(V),
the proof of the proposition is complete.
□
The ﬁnal ingredient we need for the proof of the Closure Theorem is the follow-
ing maximum principle for ideals.
Proposition 6 (Maximum Principle for Ideals). Given a nonempty collection of
ideals {Iα}α∈A in a polynomial ring k[x1, . . . , xn], there exists α0 ∈A such that for
all β ∈A, we have
Iα0 ⊆Iβ =⇒Iα0 = Iβ.
In other words, Iα0 is maximal with respect to inclusion among the Iα for α ∈A.
Proof. This is an easy consequence of the ascending chain condition (Theorem 7
of Chapter 2, §5). The proof will be left as an exercise.
□
We are now ready to prove the second part of the Closure Theorem.
Proof of Theorem 1. Suppose the theorem fails for some ideal I ⊆k[x1, . . . , xn],
i.e., there is no afﬁne variety W ⊊V(I) such that
V(Il) \ W ⊆πl(V(I)) and V(Il) \ W = V(Il).
Our goal is to derive a contradiction.
Among all ideals for which the theorem fails, the maximum principle of Propo-
sition 6 guarantees that there is a maximal such ideal, i.e., there is an ideal I such
that the theorem fails for I but holds for every strictly larger ideal I ⊊J.
Let us apply our results to I. By Corollary 3, we know that
V(Il) \ V
 
gi∈G\k[y] ci
	
⊆πl(V).
Since the theorem fails for I, V(I) \ V
 
gi∈G\k[y] ci
	
cannot be Zariski dense in
V(Il). Therefore, by Proposition 4, there is some i such that
I ⊊I(1) = I + ⟨ci⟩,
I ⊊I(2) = I : c∞
i
and
V(I) = V(I(1)) ∪V(I(2)).
Our choice of I guarantees that the theorem holds for the strictly larger ideals I(1)
and I(2). The resulting afﬁne varieties Wi ⊆V(I(i)
l ), i = 1, 2, satisfy the hypothesis
of Proposition 5, and then the proposition implies that W = W1 ∪W2 ⊆V(I)
satisﬁes the theorem for I. This contradicts our choice of I, and we are done.
□

224
Chapter 4 The Algebra-Geometry Dictionary
The proof of the Closure Theorem just given is nonconstructive. Fortunately, in
practice it is straightforward to ﬁnd W ⊆V(Il) with the required properties. We will
give two examples and then describe a general procedure.
The ﬁrst example is very simple. Consider the ideal
I = ⟨yx2 + yx + 1⟩⊆C[x, y].
We use lex order with x > y, and I1 = {0} since g1 = yx2+yx+1 is a Gröbner basis
for I. In the notation of Theorem 2, we have c1 = y, and then Corollary 3 implies
that
V(I1) \ V(c1) = C \ V(y) = C \ {0} ⊆π1(V(I)) = C.
Hence, we can take W = {0} in Theorem 1 since C \ {0} is Zariski dense in C.
The second example, taken from SCHAUENBURG (2007), uses the ideal
I = ⟨xz + y −1, w + y + z −2, z2⟩⊆C[w, x, y, z].
It is straightforward to check that V = V(I) is the line V = V(w−1, y−1, z) ⊆C4,
which projects to the point π2(V) = V(y −1, z) ⊆C2 when we eliminate w and x.
Thus, W = ∅satisﬁes Theorem 1 in this case.
Here is a systematic way to discover that W = ∅. A lex Gröbner basis of I for
w > x > y > z consists of
g1 = w + y + z −2, g2 = xz + y −1, g3 = y2 −2y + 1, g4 = yz −z, g5 = z2.
Eliminating w and x gives I2 = ⟨g3, g4, g5⟩, and one sees easily that
V(I2) = V(y −1, z).
Since g1 = 1 · w + y + z −2 and g2 = z · x + y −1, we have c1 = 1 and c2 = z. If
we set
J = ⟨c1c2⟩= ⟨z⟩,
then Corollary 3 implies that V(I2) \ V(J) ⊆π2(V). However, V(I2) \ V(J) = ∅, so
the difference is not Zariski dense in V(I2).
In this situation, we use the decomposition of V(I) guaranteed to exist by Propo-
sition 4. Note that I = I : c∞
1 since c1 = 1. Hence we use c2 = z in the proposition.
This gives the two ideals
I(1) = I + ⟨c2⟩= ⟨xz + y −1, w + y + z −2, z2, z⟩= ⟨w −1, y −1, z⟩,
I(2) = I : c∞
2 = I :z∞= ⟨1⟩since z2 ∈I.
Now we start over with I(1) and I(2).

§7 Proof of the Closure Theorem
225
For I(1), observe that {w −1, y −1, z} is a Gröbner basis of I(1), and only g(1)
1
=
w −1 /∈C[y, z]. The coefﬁcient of w is c(1)
1
= 1, and then Corollary 3 applied to
I(1) gives
V(I(1)
2 ) \ V(1) ⊆π2(V(I(1))).
Since V(1) = ∅, we can pick W1 = ∅for I(1) in Theorem 1.
Applying the same systematic process to I(2) = ⟨1⟩, we see that there are no gi /∈
C[y, z]. Thus Corollary 3 involves the product over the empty set. By convention
(see Exercise 7) the empty product is 1. Then Corollary 3 tells us that we can pick
W2 = ∅for I(2) in Theorem 1. By Proposition 5, it follows that Theorem 1 holds for
the ideal I with
W = W1 ∪W2 = ∅∪∅= ∅.
To do this in general, we use the following recursive algorithm to produce the
desired subset W:
Input : an ideal I ⊆k[x, y] with variety V = V(I)
Output : FindW(I) = W ⊆V(Il) with V(Il) \ W ⊆πl(V), V(Il) \ W = V(Il)
G := reduced Gröbner basis for I for a monomial order as in Theorem 2
ci := coefﬁcient in gi = ci(y)xαi + terms < xαi when gi ∈G \ k[y]
Il := I ∩k[y] = ⟨G ∩k[y]⟩
J :=
 
gi∈G\k[y] ci

IF V(Il) \ V(J) = V(Il) THEN
FindW(I) := V(Il) ∩V(J)
ELSE
Select gi ∈G \ k[y] with I ⊊I :c∞
i
FindW(I) := FindW(I + ⟨ci⟩) ∪FindW(I : c∞
i )
RETURN FindW(I)
The function FindW takes the input ideal I and computes the ideals Il and J =
 
gi∈G\k[y] ci

. The IF statement asks whether V(Il)\V(J) is Zariski dense in V(Il).
If the answer is yes, then V(Il)∩V(J) has the desired property by Corollary 3, which
is why FindW(I) = V(Il) ∩V(J) in this case. In the exercises, you will describe an
algorithm for determining whether V(Il) \ V(J) = V(Il).
When V(Il) \ V(J) fails to be Zariski dense in V(Il), Proposition 4 guarantees
that we can ﬁnd ci such that the ideals
I(1) = I + ⟨ci⟩and I(2) = I : c∞
i
are strictly larger than I and satisfy V = V(I) = V(I(1)) ∪V(I(2)). Then, as in
the second example above, we repeat the process on the two new ideals, which
means computing FindW(I(1)) and FindW(I(2)). By Proposition 5, the union of
these varieties works for I, which explains the last line of FindW.

226
Chapter 4 The Algebra-Geometry Dictionary
We say that FindW is recursive since it calls itself. We leave it as an exercise
to show that the maximum principle from Proposition 6 implies that FindW always
terminates in ﬁnitely many steps. When it does, correctness follows from the above
discussion.
We end this section by using the Closure Theorem to give a precise description
of the projection πl(V) ⊆kn−l of an afﬁne variety V ⊆kn.
Theorem 7. Let k be algebraically closed and let V ⊆kn be an afﬁne variety. Then
there are afﬁne varieties Zi ⊆Wi ⊆kn−l for 1 ≤i ≤p such that
πl(V) =
p
i=1
(Wi \ Zi).
Proof. We assume V ̸= ∅. First let W1 = V(Il). By the Closure Theorem, there is a
variety Z1 ⊊W1 such that W1 \ Z1 ⊆πl(V). Then, back in kn, consider the set
V1 = V ∩{(a1, . . . , an) ∈kn | (al+1, . . . , an) ∈Z1}.
One easily checks that V1 is an afﬁne variety (see Exercise 10), and furthermore,
V1 ⊊V since otherwise we would have πl(V) ⊆Z1, which would imply W1 ⊆Z1
by Zariski closure. Moreover, you will check in Exercise 10 that
(2)
πl(V) = (W1 \ Z1) ∪πl(V1).
If V1 = ∅, then we are done. If V1 is nonempty, let W2 be the Zariski closure of
πl(V1). Applying the Closure Theorem to V1, we get Z2 ⊊W2 with W2\Z2 ⊂πl(V1).
Then, repeating the above construction, we get the variety
V2 = V1 ∩{(a1, . . . , an) ∈kn | (al+1, . . . , an) ∈Z2}
such that V2 ⊊V1 and
πl(V) = (W1 \ Z1) ∪(W2 \ Z2) ∪πl(V2).
If V2 = ∅, we are done, and if not, we repeat this process again to obtain W3, Z3
and V3 ⊊V2. Continuing in this way, we must eventually have VN = ∅for some N,
since otherwise we would get an inﬁnite descending chain of varieties
V ⊋V1 ⊋V2 ⊋· · · ,
which would contradict Proposition 1 of §6. Once we have VN = ∅, the desired
formula for πl(V) follows easily.
□
In general, a set of the form described in Theorem 7 is called constructible.
As a simple example of Theorem 7, consider I = ⟨xy + z −1, y2z2⟩⊆C[x, y, z]
and set V = V(I) ⊆C3. We leave it as an exercise to show that
V(I1) = V(z) ∪V(y, z −1) = V(z) ∪{(0, 1)}

§7 Proof of the Closure Theorem
227
and that W = V(y, z) = {(0, 0)} satisﬁes V(I1) \ W ⊆π1(V). However, we also
have π1(V) ⊆V(I1), and since xy + z −1 ∈I, no point of V has vanishing y and z
coordinates. It follows that π1(V) ⊆V(I1) \ {(0, 0)}. Hence
π1(V) = V(I1) \ {(0, 0)} = (V(z) \ {(0, 0)}) ∪{(0, 1)}.
This gives an explicit representation of π1(V) as a constructible set. You will work
out another example of Theorem 7 in the exercises. More substantial examples can
be found in SCHAUENBURG (2007), which also describes an algorithm for writing
πl(V) as a constructible set. Another approach is described in ULLRICH (2006).
EXERCISES FOR §7
1. Prove that Theorem 3 of Chapter 3, §2 follows from Theorem 1 of this section. Hint:
Show that the W from Theorem 1 satisﬁes W ⊊V(Il) when V ̸= ∅.
2. In the notation of Theorem 2, prove that ¯I = ⟨G⟩for ¯I = {¯f | f ∈I}.
3. Given sets A and B, prove that A \ B = A \ (A ∩B).
4. In the proof of Proposition 4, prove that I = I : c∞
i
implies that Il = Il : c∞
i .
5. This exercise will explore some properties of irreducible components needed in the
proofs of Propositions 4 and 5.
a. Let W1, . . . , Wr be afﬁne varieties contained in a variety V and assume that for each
1 ≤i ≤r, no irreducible component of V is contained in Wi. Prove that the same is
true for #r
i=1 Wi. (This fact is used in the proof of Proposition 4.)
b. Let Wi ⊆Vi be afﬁne varieties for i = 1, 2 such that Wi contains no irreducible
component of Vi. Prove that W = W1 ∪W2 contains no irreducible component of
V = V1 ∪V2. (This fact is used in the proof of Proposition 5.)
6. Prove Proposition 6. Hint: Assume that the proposition is false for some nonempty col-
lection of ideals {Iα}α∈A and show that this leads to a contradiction of the ascending
chain condition.
7. In this exercise we will see why it is reasonable to make the convention that the empty
product is 1. Let R be a commutative ring with 1 and let A be a ﬁnite set such that for
every α ∈A, we have rα ∈R. Then we get the product

α∈Arα.
Although A is unordered, the product is well-deﬁned since R is commutative.
a. Assume B is ﬁnite and disjoint from A such that for every β ∈B, we have rβ ∈R.
Prove that

γ∈A∪Brγ =

α∈Arα

β∈Brβ

.
b. It is likely that the proof you gave in part (a) assumed that A and B are nonempty.
Explain why 
α∈∅rα = 1 makes the above formula work in all cases.
c. In a similar way, deﬁne 
α∈A rα and explain why 
α∈∅rα = 0 is needed to make
the analog of part (a) true for sums.
8. The goal of this exercise is to describe an algorithm for deciding whether V(I) \ V(g) =
V(I) when the ﬁeld k is algebraically closed.
a. Prove that V(I) \ V(g) = V(I) is equivalent to I : g∞⊆
√
I. Hint: Use the Nullstel-
lensatz and Theorem 10 of §4. Also remember that I ⊆I : g∞.
b. Use Theorem 14 of §4 and the Radical Membership Algorithm from §2 to describe
an algorithm for deciding whether I : g∞⊆
√
I.

228
Chapter 4 The Algebra-Geometry Dictionary
9. Give a proof of the termination of FindW that uses the maximum principle stated in
Proposition 6. Hint: Consider the set of all ideals in k[x, y] for which FindW does not
terminate.
10. This exercise is concerned with the proof of Theorem 7.
a. Verify that V1 = V ∩{(a1, . . . , an) ∈kn | (al+1, . . . , an) ∈Z1} is an afﬁne variety.
b. Verify that πl(V) = (W1 \ Z1) ∪πl(V1).
11. As in the text, let V = V(I) for I = ⟨xy + z −1, y2z2⟩⊆C[x, y, z]. Show that
V(I1) = V(z) ∪V(y, z −1) = V(z) ∪{(0, 1)}
and that W = V(y, z) = {(0, 0)} satisﬁes V(I1) \ W ⊆π1(V).
12. Let V = V(y−xz) ⊆C3. Theorem 7 tells us that π1(V) ⊆C2 is a constructible set. Find
an explicit decomposition of π1(V) of the form given by Theorem 7. Hint: Your answer
will involve W1, Z1 and W2.
13. Proposition 6 is the maximum principle for ideals. The geometric analog is the minimum
principle for varieties, which states that among any nonempty collection of varieties in
kn, there is a variety in the collection which is minimal with respect to inclusion. More
precisely, this means that if we are given varieties Vα, α ∈A, where A is a nonempty
set, then there is some α0 ∈A with the property that for all β ∈A, Vβ ⊆Vα0 implies
Vβ = Vα0. Prove the minimum principle. Hint: Use Proposition 1 of §6.
14. Apply the minimum principle of Exercise 13 to give a different proof of Theorem 7. Hint:
Consider the collection of all varieties V ⊆kn for which πl(V) is not constructible. By
the minimum principle, there is a variety V such that πl(V) is not constructible but πl(W)
is constructible for every variety W ⊊V. Show how the proof of Theorem 7 up to (2)
can be used to obtain a contradiction and thereby prove the theorem.
§8
Primary Decomposition of Ideals
In view of the decomposition theorem proved in §6 for radical ideals, it is natural
to ask whether an arbitrary ideal I (not necessarily radical) can be represented as
an intersection of simpler ideals. In this section, we will prove the Lasker-Noether
decomposition theorem, which describes the structure of I in detail.
There is no hope of writing an arbitrary ideal I as an intersection of prime ideals
(since an intersection of prime ideals is always radical). The next thing that suggests
itself is to write I as an intersection of powers of prime ideals. This does not quite
work either: consider the ideal I = ⟨x, y2⟩in C[x, y]. Any prime ideal containing I
must contain x and y and, hence, must equal ⟨x, y⟩(since ⟨x, y⟩is maximal). Thus, if
I were to be an intersection of powers of prime ideals, it would have to be a power
of ⟨x, y⟩(see Exercise 1 for the details).
The concept we need is a bit more subtle.
Deﬁnition 1. An ideal I in k[x1, . . . , xn] is primary if fg ∈I implies either f ∈I or
some power gm ∈I for some m > 0.
It is easy to see that prime ideals are primary. Also, you can check that the ideal
I = ⟨x, y2⟩discussed above is primary (see Exercise 1).
Lemma 2. If I is a primary ideal, then
√
I is prime and is the smallest prime ideal
containing I.

§8 Primary Decomposition of Ideals
229
Proof. See Exercise 2.
□
In view of this lemma, we make the following deﬁnition.
Deﬁnition 3. If I is primary and
√
I = P, then we say that I is P-primary.
We can now prove that every ideal is an intersection of primary ideals.
Theorem 4. Every ideal I ⊆k[x1, . . . , xn] can be written as a ﬁnite intersection of
primary ideals.
Proof. We ﬁrst deﬁne an ideal I to be irreducible if I = I1 ∩I2 implies that I = I1
or I = I2. We claim that every ideal is an intersection of ﬁnitely many irreducible
ideals. The argument is an "ideal" version of the proof of Theorem 2 from §6. One
uses the ACC rather than the DCC—we leave the details as an exercise.
Next we claim that an irreducible ideal is primary. Note that this will prove the
theorem. To see why the claim is true, suppose that I is irreducible and that fg ∈I
with f /∈I. We need to prove that some power of g lies in I. Consider the saturation
I : g∞. By Proposition 9 of §4, we know that I :g∞= I :gN once N is sufﬁciently
large. We will leave it as an exercise to show that (I + ⟨gN⟩) ∩(I + ⟨f⟩) = I. Since
I is irreducible, it follows that I = I + ⟨gN⟩or I = I + ⟨f⟩. The latter cannot occur
since f /∈I, so that I = I + ⟨gN⟩. This proves that gN ∈I.
□
As in the case of varieties, we can deﬁne what it means for a decomposition to
be minimal.
Deﬁnition 5. A primary decomposition of an ideal I is an expression of I as an
intersection of primary ideals: I = $r
i=1 Qi. It is called minimal or irredundant if
the √Qi are all distinct and Qi ̸⊇$
j̸=i Qj.
To prove the existence of a minimal decomposition, we will need the following
lemma, the proof of which we leave as an exercise.
Lemma 6. If I, J are primary and
√
I =
√
J, then I ∩J is primary.
We can now prove the ﬁrst part of the Lasker-Noether decomposition theorem.
Theorem 7 (Lasker-Noether). Every ideal I ⊆k[x1, . . . , xn] has a minimal primary
decomposition.
Proof. By Theorem 4, we know that there is a primary decomposition I = $r
i=1 Qi.
Suppose that Qi and Qj have the same radical for some i ̸= j. Then, by Lemma 6,
Q = Qi ∩Qj is primary, so that in the decomposition of I, we can replace Qi and
Qj by the single ideal Q. Continuing in this way, eventually all of the Qi's will have
distinct radicals.
Next, suppose that some Qi contains $
j̸=i Qj. Then we can omit Qi, and I will
be the intersection of the remaining Qj's for j ̸= i. Continuing in this way, we can
reduce to the case where Qi ̸⊇$
j̸=i Qj for all i.
□

230
Chapter 4 The Algebra-Geometry Dictionary
Unlike the case of varieties (or radical ideals), a minimal primary decomposition
need not be unique. In the exercises, you will verify that the ideal ⟨x2, xy⟩⊆k[x, y]
has the two distinct minimal decompositions
⟨x2, xy⟩= ⟨x⟩∩⟨x2, xy, y2⟩= ⟨x⟩∩⟨x2, y⟩.
Although ⟨x2, xy, y2⟩and ⟨x2, y⟩are distinct, note that they have the same radical.
To prove that this happens in general, we will use ideal quotients from §4. We start
by computing some ideal quotients of a primary ideal.
Lemma 8. If I is primary with
√
I = P and f ∈k[x1, . . . , xn], then:
(i) If f ∈I, then I : f = ⟨1⟩.
(ii) If f /∈I, then I : f is P-primary.
(iii) If f /∈P, then I : f = I.
Proof. See Exercise 7.
□
The second part of the Lasker-Noether theorem tells us that the radicals of the
ideals in a minimal decomposition are uniquely determined.
Theorem 9 (Lasker-Noether). Let I = $r
i=1 Qi be a minimal primary decomposi-
tion of a proper ideal I ⊆k[x1, . . . , xn] and let Pi = √Qi. Then the Pi are precisely
the proper prime ideals occurring in the set {√I : f | f ∈k[x1, . . . , xn]}.
Remark. In particular, the Pi are independent of the primary decomposition of I.
We say that the Pi belong to I.
Proof. The proof is very similar to the proof of Theorem 7 from §6. The details are
covered in Exercises 8-10.
□
In §6, we proved a decomposition theorem for radical ideals over an algebraically
closed ﬁeld. Using the Lasker-Noether theorems, we can now show that these re-
sults hold over an arbitrary ﬁeld k.
Corollary 10. Let I = $r
i=1 Qi be a minimal primary decomposition of a proper
radical ideal I ⊆k[x1, . . . , xn]. Then the Qi are prime and are precisely the proper
prime ideals occurring in the set {I : f | f ∈k[x1, . . . , xn]}.
Proof. See Exercise 12.
□
The two Lasker-Noether theorems do not tell the full story of a minimal primary
decomposition I = $r
i=1 Qi. For example, if Pi is minimal in the sense that no Pj
is strictly contained in Pi, then one can show that Qi is uniquely determined. Thus
there is a uniqueness theorem for some of the Qi's [see Chapter 4 of ATIYAH and
MACDONALD (1969) for the details]. We should also mention that the conclusion of
Theorem 9 can be strengthened: one can show that the Pi's are precisely the proper
prime ideals in the set {I : f | f ∈k[x1, . . . , xn]} [see Chapter 7 of ATIYAH and
MACDONALD (1969)].

§8 Primary Decomposition of Ideals
231
Finally, it is natural to ask if a primary decomposition can be done constructively.
More precisely, given I = ⟨f1, . . . , fs⟩, we can ask the following:
• (Primary Decomposition) Is there an algorithm for ﬁnding bases for the primary
ideals Qi in a minimal primary decomposition of I?
• (Associated Primes) Can we ﬁnd bases for the associated primes Pi = √Qi?
If you look in the references given at the end of §6, you will see that the answer
to these questions is yes. Primary decomposition has been implemented in CoCoA,
Macaulay2, Singular, and Maple.
EXERCISES FOR §9
1. Consider the ideal I = ⟨x, y2⟩⊆C[x, y].
a. Prove that ⟨x, y⟩2 ⊊I ⊊⟨x, y⟩, and conclude that I is not a prime power.
b. Prove that I is primary.
2. Prove Lemma 2.
3. This exercise is concerned with the proof of Theorem 4. Let I ⊆k[x1, . . . , xn] be an ideal.
a. Using the hints given in the text, prove that I is a ﬁnite intersection of irreducible
ideals.
b. Suppose that fg ∈I and I : g∞= I : gN. Then prove that (I + ⟨gN⟩) ∩(I + ⟨f ⟩) = I.
Hint: Elements of (I + ⟨gN⟩) ∩(I + ⟨f ⟩) can be written as a + bgN = c + df , where
a, c ∈I and b, d ∈k[x1, . . . , xn]. Now multiply through by g and use I : gN = I : gN+1.
4. In the proof of Theorem 4, we showed that every irreducible ideal is primary. Surpris-
ingly, the converse is false. Let I be the ideal ⟨x2, xy, y2⟩⊆k[x, y].
a. Show that I is primary.
b. Show that I = ⟨x2, y⟩∩⟨x, y2⟩and conclude that I is not irreducible.
5. Prove Lemma 6. Hint: Proposition 16 from §3 will be useful.
6. Let I be the ideal ⟨x2, xy⟩⊆Q[x, y].
a. Prove that
I = ⟨x⟩∩⟨x2, xy, y2⟩= ⟨x⟩∩⟨x2, y⟩
are two distinct minimal primary decompositions of I.
b. Prove that for any a ∈Q,
I = ⟨x⟩∩⟨x2, y −ax⟩
is a minimal primary decomposition of I. Thus I has inﬁnitely many distinct minimal
primary decompositions.
7. Prove Lemma 8.
8. Prove that an ideal is proper if and only if its radical is.
9. Use Exercise 8 to show that the primes belonging to a proper ideal are also proper.
10. Prove Theorem 9. Hint: Adapt the proof of Theorem 7 from §6. The extra ingredient is
that you will need to take radicals. Proposition 16 from §3 will be useful. You will also
need to use Exercise 9 and Lemma 8.
11. Let P1, . . . , Pr be the prime ideals belonging to I.
a. Prove that
√
I = "r
i=1 Pi. Hint: Use Proposition 16 from §3.
b. Show that
√
I = "r
i=1 Pi need not be a minimal decomposition of
√
I. Hint: Exer-
cise 4.
12. Prove Corollary 10. Hint: Use Proposition 9 of §4 to show that I : f is radical.

232
Chapter 4 The Algebra-Geometry Dictionary
§9
Summary
The table on the next page summarizes the results of this chapter. In the table, it is
supposed that all ideals are radical and that the ﬁeld is algebraically closed.
ALGEBRA
GEOMETRY
radical ideals
varieties
I
−→
V(I)
I(V)
←−
V
addition of ideals
intersection of varieties
I + J
−→
V(I) ∩V(J)
"
I(V) + I(W)
←−
V ∩W
product of ideals
union of varieties
IJ
−→
V(I) ∪V(J)
"
I(V)I(W)
←−
V ∪W
intersection of ideals
union of varieties
I ∩J
−→
V(I) ∪V(J)
I(V) ∩I(W)
←−
V ∪W
ideal quotients
difference of varieties
I : J
−→
V(I) \ V(J)
I(V): I(W)
←−
V \ W
elimination of variables
projection of varieties
I ∩k[xl+1, . . . , xn]
←→
πl(V(I))
prime ideal
←→
irreducible variety
minimal decomposition
minimal decomposition
I = P1 ∩· · · ∩Pm
−→
V(I) = V(P1) ∪· · · ∪V(Pm)
I(V) = I(V1) ∩· · · ∩I(Vm)
←−
V = V1 ∪· · · ∪Vm
maximal ideal
←→
point of afﬁne space
ascending chain condition
←→
descending chain condition

Chapter 5
Polynomial and Rational Functions on a Variety
One of the unifying themes of modern mathematics is that in order to understand
any class of mathematical objects, one should also study mappings between those
objects, and especially the mappings which preserve some property of interest. For
instance, in linear algebra after studying vector spaces, you also studied the prop-
erties of linear mappings between vector spaces (mappings that preserve the vector
space operations of sum and scalar product).
This chapter will consider mappings between varieties, and the results of our inv-
estigation will form another chapter of the "algebra-geometry dictionary" that we
started in Chapter 4. The algebraic properties of polynomial and rational functions
on a variety yield many insights into the geometric properties of the variety itself.
This chapter will also serve as an introduction to (and motivation for) the idea of a
quotient ring. The chapter will end with a discussion of Noether normalization.
§1
Polynomial Mappings
We will begin our study of functions between varieties by reconsidering two
examples that we have encountered previously. First, recall the tangent surface of
the twisted cubic curve in R3. As in equation (1) of Chapter 3, §3, we describe this
surface parametrically:
x = t + u,
y = t2 + 2tu,
(1)
z = t3 + 3t2u.
In functional language, giving the parametric representation (1) is equivalent to
deﬁning a mapping
φ : R2 −→R3
© Springer International Publishing Switzerland 2015
D.A. Cox et al., Ideals, Varieties, and Algorithms, Undergraduate Texts
in Mathematics, DOI 10.1007/978-3-319-16721-3_5
233

234
Chapter 5 Polynomial and Rational Functions on a Variety
by
(2)
φ(t, u) = (t + u, t2 + 2tu, t3 + 3t2u).
The domain of φ is an afﬁne variety V = R2 and the image of φ is the tangent
surface S.
We saw in §3 of Chapter 3 that S is the same as the afﬁne variety
W = V(x3z −(3/4)x2y2 −(3/2)xyz + y3 + (1/4)z2).
Hence, our parametrization gives what we might call a polynomial mapping between
V and W. (The adjective "polynomial" refers to the fact that the components of φ
are polynomials in t and u.)
Second, in the discussion of the geometry of elimination of variables from sys-
tems of equations in §2 of Chapter 3, we considered the projection mappings
πl : Cn −→Cn−l
deﬁned by
πl(a1, . . . , an) = (al+1, . . . , an).
If we have a variety V = V(I) ⊆Cn, then we can also restrict πl to V and, as
we know, πl(V) will be contained in the afﬁne variety W = V(Il), where Il =
I ∩C[xl+1, . . . , xn], the l-th elimination ideal of I. Hence, we can consider πl :
V →W as a mapping of varieties. Here too, by the deﬁnition of πl we see that the
components of πl are polynomials in the coordinates in the domain.
Deﬁnition 1. Let V ⊆km, W ⊆kn be varieties. A function φ : V →W is said to be a
polynomial mapping (or regular mapping) if there exist polynomials f1, . . . , fn ∈
k[x1, . . . , xm] such that
φ(a1, . . . , am) = ( f1(a1, . . . , am), . . . , fn(a1, . . . , am))
for all (a1, . . . , am) ∈V. We say that the n-tuple of polynomials
( f1, . . . , fn) ∈(k[x1, . . . , xm])n
represents φ. The fi are the components of this representation.
To say that φ is a polynomial mapping from V ⊆km to W ⊆kn repre-
sented by ( f1, . . . , fn) means that ( f1(a1, . . . , am), . . . , fn(a1, . . . , am)) must satisfy
the deﬁning equations of W for all points (a1, . . . , am) ∈V. For example, con-
sider V = V(y −x2, z −x3) ⊆k3 (the twisted cubic) and W = V(y3 −z2) ⊆k2.
Then the projection π1 : k3 →k2 represented by (y, z) gives a polynomial mapping
π1 : V →W. This is true because every point in π1(V) = {(a2, a3) | a ∈k} satisﬁes
the deﬁning equation of W.
Of particular interest is the case W = k, where φ simply becomes a scalar polyno-
mial function deﬁned on the variety V. One reason to consider polynomial functions
from V to k is that a general polynomial mapping φ : V →kn is constructed by using

§1 Polynomial Mappings
235
any n polynomial functions φ : V →k as the components. Hence, if we understand
functions φ : V →k, we understand how to construct all mappings φ : V →kn as
well.
To begin our study of polynomial functions, note that, for V ⊆km, Deﬁnition 1
says that a mapping φ : V →k is a polynomial function if there exists a polynomial
f ∈k[x1, . . . , xm] representing φ. In fact, we usually specify a polynomial function
by giving an explicit polynomial representative. Thus, ﬁnding a representative is
not actually the key issue. What we will see next, however, is that the cases where
a representative is uniquely determined are very rare. For example, consider the
variety V = V(y −x2) ⊆R2. The polynomial f = x3 + y3 represents a polynomial
function from V to R. However, g = x3 +y3 +(y−x2), h = x3 +y3 +(x4y−x6), and
F = x3 + y3 + A(x, y)(y −x2) for any A(x, y) deﬁne the same polynomial function
on V. Indeed, since I(V) is the set of polynomials which are zero at every point of
V, adding any element of I(V) to f does not change the values of the polynomial at
the points of V. The general pattern is the same.
Proposition 2. Let V ⊆km be an afﬁne variety. Then:
(i) f and g ∈k[x1, . . . , xm] represent the same polynomial function on V if and only
if f −g ∈I(V).
(ii) ( f1, . . . , fn) and (g1, . . . , gn) represent the same polynomial mapping from V to
kn if and only if fi −gi ∈I(V) for each i, 1 ≤i ≤n.
Proof. (i) If f −g = h ∈I(V), then for any point p = (a1, . . . , am) ∈V, f(p) −
g(p) = h(p) = 0. Hence, f and g represent the same function on V. Conversely, if
f and g represent the same function, then, at every p ∈V, f(p) −g(p) = 0. Thus,
f −g ∈I(V) by deﬁnition. Part (ii) follows directly from (i).
□
It follows that the correspondence between polynomials in k[x1, . . . , xm] and
polynomial functions V →k is one-to-one only in the case where I(V) = {0}.
In Exercise 7, you will show that I(V) = {0} if and only if k is inﬁnite and V = km.
There are two ways of dealing with this potential ambiguity in describing poly-
nomial functions on a variety:
• In rough terms, we can "lump together" all the polynomials f ∈k[x1, . . . , xm] that
represent the same function on V and think of that collection as a "new object" in
its own right. We can then take the collection of polynomials as our description
of the function on V.
• Alternatively, we can systematically look for the simplest possible individual
polynomial that represents each function on V and work with those "standard
representative" polynomials exclusively.
Each of these approaches has its own advantages, and we will consider both of them
in detail in later sections of this chapter. We will conclude this section by looking at
two further examples to show the kinds of properties of varieties that can be revealed
by considering polynomial functions.
Deﬁnition 3. We denote by k[V] the collection of polynomial functions φ : V →k.
Since k is a ﬁeld, we can deﬁne a sum and a product function for any pair of
functions φ, ψ : V →k by adding and multiplying images. For each p ∈V,

236
Chapter 5 Polynomial and Rational Functions on a Variety
(φ + ψ)(p) = φ(p) + ψ(p),
(φ · ψ)(p) = φ(p) · ψ(p).
Furthermore, if we pick speciﬁc representatives f, g ∈k[x1, . . . , xm] for φ, ψ, res-
pectively, then by deﬁnition, the polynomial sum f + g represents φ + ψ and the
polynomial product f ·g represents φ·ψ. It follows that φ+ψ and φ·ψ are polynomial
functions on V.
Thus, we see that k[V] has sum and product operations constructed using the
sum and product operations in k[x1, . . . , xm]. All of the usual properties of sums
and products of polynomials also hold for functions in k[V]. Thus, k[V] is another
example of a commutative ring. (See Appendix A for the precise deﬁnition.) We
will return to this point in §2.
Now we are ready to start exploring what k[V] can tell us about the geometric
properties of a variety V. First, recall from §5 of Chapter 4 that a variety V ⊆km
is said to be reducible if it can be written as the union of two nonempty proper
subvarieties: V = V1 ∪V2, where V1 ̸= V and V2 ̸= V. For example, the variety
V = V(x3 + xy2 −xz, yx2 + y3 −yz) in k3 is reducible since, from the factorizations
of the deﬁning equations, we can decompose V as V = V(x2 + y2 −z) ∪V(x, y).
We would like to demonstrate that geometric properties such as reducibility can be
"read off" from a sufﬁciently good algebraic description of k[V]. To see this, let
(3)
f = x2 + y2 −z, g = 2x2 −3y4z ∈k[x, y, z]
and let φ, ψ be the corresponding elements of k[V].
Note that neither φ nor ψ is identically zero on V. For example, at (0, 0, 5) ∈
V, φ(0, 0, 5) = f(0, 0, 5) = −5 ̸= 0. Similarly, at (1, 1, 2) ∈V, ψ(1, 1, 2) =
g(1, 1, 2) = −4 ̸= 0. However, the product function φ · ψ is zero at every point of
V. The reason is that
f · g = (x2 + y2 −z)(2x2 −3y4z)
= 2x(x3 + xy2 −xz) −3y3z(x2y + y3 −yz)
∈⟨x3 + xy2 −xz, x2y + y3 −yz⟩.
Hence f ·g ∈I(V), so the corresponding polynomial function φ·ψ on V is identically
zero.
The product of two nonzero elements of a ﬁeld or of two nonzero polynomials in
k[x1, . . . , xn] is never zero. In general, a commutative ring R is said to be an integral
domain if whenever a · b = 0 in R, either a = 0 or b = 0. Hence, for the variety V
in the above example, we see that k[V] is not an integral domain. Furthermore, the
existence of φ ̸= 0 and ψ ̸= 0 in k[V] such that φ · ψ = 0 is a direct consequence
of the reducibility of V, since f in (3) is zero on V1 = V(x2 + y2 −z) but not on
V2 = V(x, y), and similarly g is zero on V2 but not on V1. This is why f · g = 0
at every point of V = V1 ∪V2. Hence, we see a relation between the geometric
properties of V and the algebraic properties of k[V].
The general case of this relation can be stated as follows.

§1 Polynomial Mappings
237
Proposition 4. Let V ⊆kn be an afﬁne variety. The following are equivalent:
(i) V is irreducible.
(ii) I(V) is a prime ideal.
(iii) k[V] is an integral domain.
Proof. (i) ⇔(ii) is Proposition 3 of Chapter 4, §5.
To show (iii) ⇒(i), suppose that k[V] is an integral domain but that V is reducible.
By Deﬁnition 1 of Chapter 4, §5, this means that we can write V = V1 ∪V2, where
V1 and V2 are proper, nonempty subvarieties of V. Let f1 ∈k[x1, . . . , xn] be a polyno-
mial that vanishes on V1 but not identically on V2 and, similarly, let f2 be identically
zero on V2, but not on V1. (Such polynomials must exist since V1 and V2 are varieties
and neither is contained in the other.) Hence, neither f1 nor f2 represents the zero
function in k[V]. However, the product f1 · f2 vanishes at all points of V1 ∪V2 = V.
Hence, the product function is zero in k[V]. This is a contradiction to our hypothesis
that k[V] was an integral domain. Hence, V is irreducible.
Finally, for (i) ⇒(iii), suppose that k[V] is not an integral domain. Then there
must be polynomials f, g ∈k[x1, . . . , xn] such that neither f nor g vanishes iden-
tically on V but their product does. In Exercise 9, you will check that we get a
decomposition of V as a union of subvarieties:
V = (V ∩V( f)) ∪(V ∩V(g)).
You will also show in Exercise 9 that, under these hypotheses, neither V ∩V( f) nor
V ∩V(g) is all of V. This contradicts our assumption that V is irreducible.
□
Next we will consider another example of the kind of information about varieties
revealed by polynomial mappings. The variety V ⊆C3 deﬁned by
x2 + 2xz + 2y2 + 3y = 0,
xy + 2x + z = 0,
(4)
xz + y2 + 2y = 0
is the intersection of three quadric surfaces.
To study V, we compute a Gröbner basis for the ideal generated by the polyno-
mials in (4) using lex order with y > z > x. The result is
(5)
g1 = y −x2,
g2 = z + x3 + 2x.
Geometrically, by the results of Chapter 3, §2, we know that the projection of V on
the x-axis is onto since the two polynomials in (5) have constant leading coefﬁcients.
Furthermore, for each value of x in C, there are unique y, z satisfying equations (4).
We can rephrase this observation using the maps
π : V −→C,
(x, y, z) 
−→x,
φ : C −→V,
x 
−→(x, x2, −x3 −2x).

238
Chapter 5 Polynomial and Rational Functions on a Variety
Note that (5) guarantees that φ takes values in V. Both φ and π are visibly poly-
nomial mappings. We claim that these maps establish a one-to-one correspondence
between the points of the variety V and the points of the variety C.
Our claim will follow if we can show that π and φ are inverses of each other. To
verify this last claim, we ﬁrst check that π ◦φ = idC. This is actually quite clear
since
(π ◦φ)(x) = π(x, x2, −x3 −2x) = x.
On the other hand, if (x, y, z) ∈V, then
(φ ◦π)(x, y, z) = (x, x2, −x3 −2x).
By (5), we have y−x2, z+x3 +2x ∈I(V) and it follows that φ ◦π deﬁnes the same
mapping on V as idV(x, y, z) = (x, y, z).
The conclusion we draw from this example is that V ⊆C3 and C are "isomor-
phic" varieties in the sense that there is a one-to-one, onto, polynomial mapping
from V to C, with a polynomial inverse. Even though our two varieties are deﬁned
by different equations and are subsets of different ambient spaces, they are "the
same" in a certain sense. In addition, the Gröbner basis calculation leading to equa-
tion (5) shows that C[V] = C[x], in the sense that every ψ ∈C[V] can be (uniquely)
expressed by substituting for y and z from (5) to yield a polynomial in x alone. Of
course, if we use x as the coordinate on W = C, then C[W] = C[x] as well, and we
obtain the same collection of functions on our two isomorphic varieties.
Thus, the collection of polynomial functions on an afﬁne variety can detect ge-
ometric properties such as reducibility or irreducibility. In addition, knowing the
structure of k[V] can also furnish information leading toward the beginnings of a
classiﬁcation of varieties, a topic we have not broached before. We will return to
these questions later in the chapter, once we have developed several different tools
to analyze the algebraic properties of k[V].
EXERCISES FOR §1
1. Let V be the twisted cubic in R3 and let W = V(v−u−u2) in R2. Show that φ(x, y, z) =
(xy, z + x2y2) deﬁnes a polynomial mapping from V to W. Hint: The easiest way is to
use a parametrization of V.
2. Let V = V(y −x) in R2 and let φ : R2 →R3 be the polynomial mapping represented by
φ(x, y) = (x2 −y, y2, x −3y2). The image of V under φ is a variety in R3. Find a system
of equations deﬁning the image of φ.
3. Given a polynomial function φ : V →k, we deﬁne a level set or ﬁber of φ to be
φ−1(c) = {(a1, . . . , am) ∈V | φ(a1, . . . , am) = c},
where c ∈k is ﬁxed. In this exercise, we will investigate how level sets can be used to
analyze and reconstruct a variety. We will assume that k = R, and we will work with the
surface
V = V(x2 −y2z2 + z3) ⊆R3.
a. Let φ be the polynomial function represented by f (x, y, z) = z. The image of φ is
all of R in this case. For each c ∈R, explain why the level set φ−1(c) is the afﬁne
variety deﬁned by the equations:

§1 Polynomial Mappings
239
x2 −y2z2 + z3 = 0,
z −c = 0.
b. Eliminate z between these equations to ﬁnd the equation of the intersection of V with
the plane z = c. Explain why your equation deﬁnes a hyperbola in the plane z = c if
c ̸= 0, and the y-axis if c = 0. (Refer to the sketch of V in §3 of Chapter 1, and see if
you can visualize the way these hyperbolas lie on V.)
c. Let π : V →R be the polynomial mapping π(x, y, z) = x. Describe the level sets
π−1(c) in V geometrically for c = −1, 0, 1.
d. Do the same for the level sets of σ : V →R given by σ(x, y, z) = y.
e. Construct a one-to-one polynomial mapping ψ : R →V and identify the image as a
subvariety of V. Hint: The y-axis.
4. Let V = V(z2 −(x2 + y2 −1)(4 −x2 −y2)) in R3 and let π : V →R2 be the vertical
projection π(x, y, z) = (x, y).
a. What is the maximum number of points in π−1(a, b) for (a, b) ∈R2?
b. For which subsets R ⊆R2 does (a, b) ∈R imply π−1(a, b) consists of two points,
one point, no points?
c. Using part (b) describe and/or sketch V.
5. Show that φ1(x, y, z) = (2x2+y2, z2−y3+3xz) and φ2(x, y, z) = (2y+xz, 3y2) represent
the same polynomial mapping from the twisted cubic in R3 to R2.
6. Consider the mapping φ : R2 →R5 deﬁned by φ(u, v) = (u, v, u2, uv, v2).
a. The image of φ is a variety S known as an afﬁne Veronese surface. Find implicit
equations for S.
b. Show that the projection π : S →R2 deﬁned by π(x1, x2, x3, x4, x5) = (x1, x2) is the
inverse mapping of φ : R2 →S. What does this imply about S and R2?
7. This problem characterizes the varieties for which I(V) = {0}.
a. Show that if k is an inﬁnite ﬁeld and V ⊆kn is a variety, then I(V) = {0} if and only
if V = kn.
b. On the other hand, show that if k is ﬁnite, then I(V) is never equal to {0}. Hint: See
Exercise 4 of Chapter 1, §1.
8. Let V = V(xy, xz) ⊆R3.
a. Show that neither of the polynomial functions f = y2 + z3, g = x2 −x is identically
zero on V, but that their product is identically zero on V.
b. Find V1 = V ∩V( f ) and V2 = V ∩V(g) and show that V = V1 ∪V2.
9. Let V be an irreducible variety and let φ, ψ be functions in k[V] represented by polyno-
mials f , g, respectively. Assume that φ · ψ = 0 in k[V] and that neither φ nor ψ is the
zero function on V.
a. Show that V = (V ∩V( f )) ∪(V ∩V(g)).
b. Show that neither V ∩V( f ) nor V ∩V(g) is all of V and deduce a contradiction.
10. In this problem, we will see that there are no nonconstant polynomial mappings from
V = R to W = V(y2 −x3 + x) ⊆R2. Thus, these varieties are not isomorphic (i.e., they
are not "the same" in the sense introduced in this section).
a. Suppose φ : R →W is a polynomial mapping represented by φ(t) = (a(t), b(t))
where a(t), b(t) ∈R[t]. Explain why it must be true that b(t)2 = a(t)(a(t)2 −1).
b. Explain why the two factors on the right of the equation in part (a) must be relatively
prime in R[t].
c. Using the unique factorizations of a and b into products of powers of irreducible
polynomials, show that b2 = ac2 for some polynomial c ∈R[t] relatively prime to a.
d. From part (c) it follows that c2 = a2 −1. Deduce from this equation that c, a, and,
hence, b must be constant polynomials. Hint: a2 −c2 = 1.

240
Chapter 5 Polynomial and Rational Functions on a Variety
§2
Quotients of Polynomial Rings
The construction of k[V] given in §1 is a special case of what is called the quotient
of k[x1, . . . , xn] modulo an ideal I. From the word quotient, you might guess that
the issue is to deﬁne a division operation, but this is not the case. Instead, forming
the quotient will indicate the sort of "lumping together" of polynomials that we
mentioned in §1 when describing the elements φ ∈k[V]. The quotient construction
is a fundamental tool in commutative algebra and algebraic geometry, so if you
pursue these subjects further, the acquaintance you make with quotient rings here
will be valuable.
To begin, we introduce some new terminology.
Deﬁnition 1. Let I ⊆k[x1, . . . , xn] be an ideal, and let f, g ∈k[x1, . . . , xn]. We say f
and g are congruent modulo I, written
f ≡g mod I,
if f −g ∈I.
For instance, if I = ⟨x2 −y2, x + y3 + 1⟩⊆k[x, y], then f = x4 −y4 + x and
g = x + x5 + x4y3 + x4 are congruent modulo I since
f −g = x4 −y4 −x5 −x4y3 −x4
= (x2 + y2)(x2 −y2) −(x4)(x + y3 + 1) ∈I.
The most important property of the congruence relation is given by the following
proposition.
Proposition 2. Let I ⊆k[x1, . . . , xn] be an ideal. Then congruence modulo I is an
equivalence relation on k[x1, . . . , xn].
Proof. Congruence modulo I is reﬂexive since f −f = 0 ∈I for every f ∈
k[x1, . . . , xn]. To prove symmetry, suppose that f ≡g mod I. Then f −g ∈I, which
implies that g −f = (−1)( f −g) ∈I as well. Hence, g ≡f mod I also. Finally, we
need to consider transitivity. If f ≡g mod I and g ≡h mod I, then f −g, g−h ∈I.
Since I is closed under addition, we have f −h = f −g + g −h ∈I as well. Hence,
f ≡h mod I.
□
An equivalence relation on a set S partitions S into a collection of disjoint subsets
called equivalence classes. For any f ∈k[x1, . . . , xn], the class of f is the set
[ f] = {g ∈k[x1, . . . , xn] | g ≡f mod I}.
The deﬁnition of congruence modulo I and Proposition 2 makes sense for every
ideal I ⊆k[x1, . . . , xn]. In the special case that I = I(V) is the ideal of the variety
V, then by Proposition 2 of §1, it follows that f ≡g mod I(V) if and only if f
and g deﬁne the same function on V. In other words, the "lumping together" of

§2 Quotients of Polynomial Rings
241
polynomials that deﬁne the same function on a variety V is accomplished by passing
to the equivalence classes for the congruence relation modulo I(V). More formally,
we have the following proposition.
Proposition 3. The distinct polynomial functions φ : V →k are in one-to-one
correspondence with the equivalence classes of polynomials under congruence
modulo I(V).
Proof. This is a corollary of Proposition 2 of §1 and the (easy) proof is left to the
reader as an exercise.
□
We are now ready to introduce the quotients mentioned in the title of this section.
Deﬁnition 4. The quotient of k[x1, . . . , xn] modulo I, written k[x1, . . . , xn]/I, is the
set of equivalence classes for congruence modulo I:
k[x1, . . . , xn]/I = {[ f] | f ∈k[x1, . . . , xn]}.
For instance, take k = R, n = 1, and I = ⟨x2 −2⟩. We may ask whether there
is some way to describe all the equivalence classes for congruence modulo I. By
the division algorithm, every f ∈R[x] can be written as f = q · (x2 −2) + r,
where r = ax + b for some a, b ∈R. By the deﬁnition, f ≡r mod I since f −r =
q · (x2 −2) ∈I. Thus, every element of R[x] belongs to one of the equivalence
classes [ax + b], and R[x]/I = {[ax + b] | a, b ∈R}. In §3, we will extend the idea
used in this example to a method for dealing with k[x1, . . . , xn]/I in general.
Because k[x1, . . . , xn] is a ring, given any two classes [ f], [g] ∈k[x1, . . . , xn]/I,
we can attempt to deﬁne sum and product operations on classes by using the corre-
sponding operations on elements of k[x1, . . . , xn]. That is, we can try to deﬁne
(1)
[ f] + [g] = [ f + g]
(sum in k[x1, . . . , xn]),
[ f] · [g] = [ f · g]
(product in k[x1, . . . , xn]).
We must check, however, that these formulas actually make sense. We need to show
that if we choose different f ′ ∈[ f] and g′ ∈[g], then the class [ f ′ + g′] is the same
as the class [ f + g]. Similarly, we need to check that [ f ′ · g′] = [ f · g].
Proposition 5. The operations deﬁned in equations (1) yield the same classes in
k[x1, . . . , xn]/I on the right-hand sides no matter which f ′ ∈[ f] and g′ ∈[g] we use.
(We say that the operations on classes given in (1) are well-deﬁned on classes.)
Proof. If f ′ ∈[ f] and g′ ∈[g], then f ′ = f + A and g′ = g + B, where A, B ∈I.
Hence,
f ′ + g′ = ( f + A) + (g + B) = ( f + g) + (A + B).
Since we also have A + B ∈I (I is an ideal), it follows that f ′ + g′ ≡f + g mod I,
so [ f ′ + g′] = [ f + g]. Similarly,
f ′ · g′ = ( f + A) · (g + B) = fg + Ag + fB + AB.

242
Chapter 5 Polynomial and Rational Functions on a Variety
Since A, B ∈I, we have Ag+fB+AB ∈I. Thus, f ′ ·g′ ≡f ·g mod I, which implies
that [ f ′ · g′] = [ f · g].
□
To illustrate this result, consider the sum and product operations in R[x]/⟨x2−2⟩.
As we saw earlier, the classes [ax+b], a, b ∈R form a complete list of the elements
of R[x]/⟨x2 −2⟩. The sum operation is deﬁned by [ax + b] + [cx + d] = [(a + c)x +
(b + d)]. Note that this amounts to the usual vector sum on ordered pairs of real
numbers. The product operation is also easily understood. We have
[ax + b] · [cx + d] = [acx2 + (ad + bc)x + bd]
= [(ad + bc)x + (bd + 2ac)],
as we can see by dividing the quadratic polynomial in the ﬁrst line by x2 −2 and
using the remainder as our representative of the class of the product.
Once we know that the operations in (1) are well-deﬁned, it follows immediately
that all of the axioms for a commutative ring are satisﬁed in k[x1, . . . , xn]/I. This is
so because the sum and product in k[x1, . . . , xn]/I are deﬁned in terms of the corre-
sponding operations in k[x1, . . . , xn], where we know that the axioms do hold. For
example, to check that sums are associative in k[x1, . . . , xn]/I, we argue as follows:
if [ f], [g], [h] ∈k[x1, . . . , xn]/I, then
([ f] + [g]) + [h] = [ f + g] + [h]
= [( f + g) + h]
[by (1)]
= [ f + (g + h)]
(by associativity in k[x1, . . . , xn])
= [ f] + [g + h]
= [ f] + ([g] + [h]).
Similarly, commutativity of addition, associativity, and commutativity of multipli-
cation, and the distributive law all follow because polynomials satisfy these prop-
erties. The additive identity is [0] ∈k[x1, . . . , xn]/I, and the multiplicative identity
is [1] ∈k[x1, . . . , xn]/I. To summarize, we have sketched the proof of the following
theorem.
Theorem 6. Let I be an ideal in k[x1, . . . , xn]. The quotient k[x1, . . . , xn]/I is a com-
mutative ring under the sum and product operations given in (1).
Next, given a variety V, let us relate the quotient ring k[x1, . . . , xn]/I(V) to the
ring k[V] of polynomial functions on V. It turns out that these two rings are "the
same" in the following sense.
Theorem 7. The one-to-one correspondence between the elements of k[V] and the
elements of k[x1, . . . , xn]/I(V) given in Proposition 3 preserves sums and products.
Proof. Let Φ : k[x1, . . . , xn]/I(V) →k[V] be the mapping deﬁned by Φ([ f]) = φ,
where φ is the polynomial function represented by f. Since every element of k[V] is

§2 Quotients of Polynomial Rings
243
represented by some polynomial, we see that Φ is onto. To see that Φ is also one-to-
one, suppose that Φ([ f]) = Φ([g]). Then by Proposition 3, f ≡g mod I(V). Hence,
[ f] = [g] in k[x1, . . . , xn]/I(V).
To study sums and products, let [ f], [g] ∈k[x1, . . . , xn]/I(V). Then Φ([ f]+[g]) =
Φ([ f +g]) by the deﬁnition of sum in the quotient ring. If f represents the polynomial
function φ and g represents ψ, then f + g represents φ + ψ. Hence,
Φ([ f + g]) = φ + ψ = Φ([ f]) + Φ([g]).
Thus, Φ preserves sums. Similarly,
Φ([ f] · [g]) = Φ([ f · g]) = φ · ψ = Φ([ f]) · Φ([g]).
Thus, Φ preserves products as well.
The inverse correspondence Ψ = Φ−1 also preserves sums and products by a
similar argument, and the theorem is proved.
□
The result of Theorem 7 illustrates a basic notion from abstract algebra. The
following deﬁnition tells us what it means for two rings to be essentially the same.
Deﬁnition 8. Let R, S be commutative rings.
(i) A mapping Φ : R →S is said to be a ring isomorphism if:
a. Φ preserves sums: Φ(r + r′) = Φ(r) + Φ(r′) for all r, r′ ∈R.
b. Φ preserves products: Φ(r · r′) = Φ(r) · Φ(r′) for all r, r′ ∈R.
c. Φ is one-to-one and onto.
(ii) Two rings R, S are isomorphic if there exists an isomorphism Φ : R →S. We
write R ∼= S to denote that R is isomorphic to S.
(iii) A mapping Φ : R →S is a ring homomorphism if Φ satisﬁes properties (a)
and (b) of (i), but not necessarily property (c), and if, in addition, Φ maps the
multiplicative identity 1 ∈R to 1 ∈S.
In general, a "homomorphism" is a mapping that preserves algebraic structure.
A ring homomorphism Φ : R →S is a mapping that preserves the addition and
multiplication operations in the ring R.
From Theorem 7, we get a ring isomorphism k[V] ∼= k[x1, . . . , xn]/I(V). A nat-
ural question to ask is what happens if we replace I(V) by some other ideal I
which deﬁnes V. [From Chapter 4, we know that there are lots of ideals I such
that V = V(I).] Could it be true that all the quotient rings k[x1, . . . , xn]/I are iso-
morphic to k[V]? The following example shows that the answer to this question is
no. Let V = {(0, 0)}. We saw in Chapter 1, §4 that I(V) = I({(0, 0)}) = ⟨x, y⟩. By
Theorem 7, we know that k[x, y]/I(V) ∼= k[V].
Our ﬁrst claim is that the quotient ring k[x, y]/I(V) is isomorphic to the ﬁeld k.
The easiest way to see this is to note that a polynomial function on the one-point set
{(0, 0)} can be represented by a constant since the function will have only one func-
tion value. Alternatively, we can derive the same fact algebraically by constructing
a mapping

244
Chapter 5 Polynomial and Rational Functions on a Variety
Φ : k[x, y]/I(V) −→k
by setting Φ([ f]) = f(0, 0) (the constant term of the polynomial). We will leave it
as an exercise to show that Φ is a well-deﬁned ring isomorphism.
Now, let I = ⟨x3+y2, 3y4⟩⊆k[x, y]. It is easy to check that V(I) = {(0, 0)} = V.
We ask whether k[x, y]/I is also isomorphic to k. A moment's thought shows that
this is not so. For instance, consider the class [y] ∈k[x, y]/I. Note that y /∈I, a fact
which can be checked by ﬁnding a Gröbner basis for I (use any monomial order)
and computing a remainder. In the ring k[x, y]/I, this shows that [y] ̸= [0]. But we
also have [y]4 = [y4] = [0] since y4 ∈I. Thus, there is an element of k[x, y]/I
which is not zero itself, but whose fourth power is zero. In a ﬁeld, this is impossible.
We conclude that k[x, y]/I is not a ﬁeld. But this says that k[x, y]/I(V) and k[x, y]/I
cannot be isomorphic rings since one is a ﬁeld and the other is not. (See Exercise 8.)
In a commutative ring R, an element a ∈R such that an = 0 for some n ≥1
is said to be nilpotent. The example just given is actually quite representative of
the kind of difference that can appear when we compare k[x1, . . . , xn]/I(V) with
k[x1, . . . , xn]/I for another ideal I with V(I) = V. If I is not a radical ideal, there will
be elements f ∈
√
I which are not in I itself. Thus, in k[x1, . . . , xn]/I, we will have
[ f] ̸= [0], whereas [ f]n = [0] for the n > 1 such that f n ∈I. The ring k[x1, . . . , xn]/I
will have nonzero nilpotent elements, whereas k[x1, . . . , xn]/I(V) never does: I(V)
is always a radical ideal, so [ f]n = 0 if and only if [ f] = 0.
Since a quotient k[x1, . . . , xn]/I is a commutative ring in its own right, we can
study other facets of its ring structure as well, and, in particular, we can consider
ideals in k[x1, . . . , xn]/I. The deﬁnition is the same as the deﬁnition of ideals in
k[x1, . . . , xn].
Deﬁnition 9. A subset I of a commutative ring R is said to be an ideal in R if it
satisﬁes
(i) 0 ∈I (where 0 is the zero element of R).
(ii) If a, b ∈I, then a + b ∈I.
(iii) If a ∈I and r ∈R, then r · a ∈I.
There is a close relation between ideals in the quotient k[x1, . . . , xn]/I and ideals
in k[x1, . . . , xn].
Proposition 10. Let I be an ideal in k[x1, . . . , xn]. The ideals in the quotient ring
k[x1, . . . , xn]/I are in one-to-one correspondence with the ideals of k[x1, . . . , xn]
containing I (i.e., the ideals J satisfying I ⊆J ⊆k[x1, . . . , xn]).
Proof. First, we give a way to produce an ideal in k[x1, . . . , xn]/I corresponding
to each J containing I in k[x1, . . . , xn]. Given an ideal J in k[x1, . . . , xn] containing
I, let J/I denote the set {[ j] ∈k[x1, . . . , xn]/I | j ∈J}. We claim that J/I is an
ideal in k[x1, . . . , xn]/I. To prove this, ﬁrst note that [0] ∈J/I since 0 ∈J. Next, let
[ j], [k] ∈J/I. Then [ j] + [k] = [ j + k] by the deﬁnition of the sum in k[x1, . . . , xn]/I.
Since j, k ∈J, we have j + k ∈J as well. Hence, [ j] + [k] ∈J/I. Finally, if [ j] ∈J/I
and [r] ∈k[x1, . . . , xn]/I, then [r] · [ j] = [r · j] by the deﬁnition of the product in

§2 Quotients of Polynomial Rings
245
k[x1, . . . , xn]/I. But r · j ∈J since J is an ideal in k[x1, . . . , xn]. Hence, [r] · [ j] ∈J/I.
As a result, J/I is an ideal in k[x1, . . . , xn]/I.
If ˜J ⊆k[x1, . . . , xn]/I is an ideal, we next show how to produce an ideal J ⊆
k[x1, . . . , xn] which contains I. Let J = { j ∈k[x1, . . . , xn] | [ j] ∈˜J}. Then we have
I ⊆J since [i] = [0] ∈˜J for any i ∈I. It remains to show that J is an ideal of
k[x1, . . . , xn]. First note that 0 ∈I ⊆J. Furthermore, if j, k ∈J, then [ j], [k] ∈˜J
implies that [ j] + [k] = [ j + k] ∈˜J. It follows that j + k ∈J. Finally, if j ∈J and
r ∈k[x1, . . . , xn], then [ j] ∈˜J, so [r][ j] = [rj] ∈˜J. But this says rj ∈J, and, hence,
J is an ideal in k[x1, . . . , xn].
We have thus shown that there are correspondences between the two collections
of ideals:
{J | I ⊆J ⊆k[x1, . . . , xn]}
{˜J ⊆k[x1, . . . , xn]/I}
J −→J/I = {[ j] | j ∈J}
(2)
J = { j | [ j] ∈˜J} ←−˜J.
We leave it as an exercise to prove that each of these arrows is the inverse of the
other. This gives the desired one-to-one correspondence.
□
For example, consider the ideal I = ⟨x2 −4x + 3⟩in R = R[x]. We know from
Chapter 1 that R is a principal ideal domain, i.e., every ideal in R is generated by
a single polynomial. The ideals containing I are precisely the ideals generated by
polynomials that divide x2 −4x + 3 = (x −1)(x −3). Hence, the quotient ring R/I
has exactly four ideals in this case:
ideals in R/I
ideals in R containing I
{[0]}
I
⟨[x −1]⟩
⟨x −1⟩
⟨[x −3]⟩
⟨x −3⟩
R/I
R
As in another example earlier in this section, we can compute in R/I by computing
remainders with respect to x2 −4x + 3.
As a corollary of Proposition 10, we deduce the following result about ideals in
quotient rings, parallel to the Hilbert Basis Theorem from Chapter 2.
Corollary 11. Every ideal in the quotient ring k[x1, . . . , xn]/I is ﬁnitely generated.
Proof. Let ˜J be any ideal in k[x1, . . . , xn]/I. By Proposition 10, ˜J = {[ j] | j ∈J}
for an ideal J in k[x1, . . . , xn] containing I. Then the Hilbert Basis Theorem implies
that J = ⟨f1, . . . , fs⟩for some fi ∈k[x1, . . . , xn]. But then for any j ∈J, we have
j = h1 f1 + · · · + hs fs for some hi ∈k[x1, . . . , xn]. Hence,
[ j] = [h1f1 + · · · + hsfs]
= [h1][ f1] + · · · + [hs][ fs].
As a result, the classes [ f1], . . . , [ fs] generate ˜J in k[x1, . . . , xn]/I.
□

246
Chapter 5 Polynomial and Rational Functions on a Variety
In the next section, we will discuss a more constructive method to study the
quotient rings k[x1, . . . , xn]/I and their algebraic properties.
EXERCISES FOR §2
1. Let I = ⟨f1, . . . , fs⟩⊆k[x1, . . . , xn]. Describe an algorithm for determining whether
f ≡g mod I using techniques from Chapter 2.
2. Prove Proposition 3.
3. Prove Theorem 6. This means showing that the other axioms for a commutative ring are
satisﬁed by k[x1, . . . , xn]/I.
4. In this problem, we will give an algebraic construction of a ﬁeld containing Q in which
2 has a square root. Note that the ﬁeld of real numbers is one such ﬁeld. However, our
construction will not make use of the limit process necessary, for example, to make sense
of an inﬁnite decimal expansion such as the usual expansion
√
2 = 1.414 . . .. Instead,
we will work with the polynomial x2 −2.
a. Show that every f ∈Q[x] is congruent modulo the ideal I = ⟨x2 −2⟩⊆Q[x] to a
unique polynomial of the form ax + b, where a, b ∈Q.
b. Show that the class of x in Q[x]/I is a square root of 2 in the sense that [x]2 = [2] in
Q[x]/I.
c. Show that F = Q[x]/I is a ﬁeld. Hint: Using Theorem 6, the only thing left to prove
is that every nonzero element of F has a multiplicative inverse in F.
d. Find a subﬁeld of F isomorphic to Q.
5. In this problem, we will consider the addition and multiplication operations in the quo-
tient ring R[x]/⟨x2 + 1⟩.
a. Show that every f ∈R[x] is congruent modulo I = ⟨x2 + 1⟩to a unique polynomial
of the form ax + b, where a, b ∈R.
b. Construct formulas for the addition and multiplication rules in R[x]/⟨x2 + 1⟩using
these polynomials as the standard representatives for classes.
c. Do we know another way to describe the ring R[x]/(x2 + 1) (that is, another well-
known ring isomorphic to this one)? Hint: What is [x]2?
6. Show that R[x]/⟨x2 −4x + 3⟩is not an integral domain.
7. One can deﬁne a quotient ring R/I for any ideal I in a commutative ring R. The general
construction is the same as what we did for k[x1, . . . , xn]/I. Here is a simple example.
a. Let I = ⟨p⟩in R = Z, where p is a prime number. Show that the relation of congru-
ence modulo p, deﬁned by
m ≡n mod p ⇐⇒p divides m −n
is an equivalence relation on Z, and list the different equivalence classes. We will
denote the set of equivalence classes by Z/⟨p⟩.
b. Construct sum and product operations in Z/⟨p⟩by the analogue of equation (1) and
then prove that they are well-deﬁned by adapting the proof of Proposition 5.
c. Explain why Z/⟨p⟩is a commutative ring under the operations you deﬁned in part
(b).
d. Show that the ﬁnite ﬁeld Fp introduced in Chapter 1 is isomorphic as a ring to Z/⟨p⟩.
8. In this problem, we study how ring homomorphisms interact with multiplicative inverses.
a. Show that every ring isomorphism Φ : R →S takes the multiplicative identity in R
to the multiplicative identity in S, i.e., Φ(1) = 1.
b. Show that if r ∈R has a multiplicative inverse, then for any ring homomorphism
Φ : R →S, Φ(r−1) is a multiplicative inverse for Φ(r) in the ring S.
c. Show that if R and S are isomorphic as rings and R is a ﬁeld, then S is also a ﬁeld.
9. Prove that the map f →f (0, 0) induces a ring isomorphism k[x, y]/⟨x, y⟩∼= k. Hint: An
efﬁcient proof can be given using Exercise 16.

§2 Quotients of Polynomial Rings
247
10. This problem illustrates one important use of nilpotent elements in rings. Let R = k[x, t]
and let I = ⟨t2⟩⊆R.
a. Show that [t] is a nilpotent element in R/I and ﬁnd the smallest power of [t] which is
equal to zero.
b. Show that every class in R/I has a unique representative of the form a + bε, where
a, b ∈k[x] and ε is shorthand for [t].
c. Given a + bε ∈R/I and f (x) ∈k[x], we can deﬁne an element f (a + bε) ∈R/I by
substituting x = a + bε into f (x). For instance, with a + bε = 2 + ε and f (x) = x2,
we obtain (2 + ε)2 = 4 + 4ε + ε2 = 4 + 4ε. Show that
f (a + bε) = f (a) + f ′(a) · bε,
where f ′ is the formal derivative of f deﬁned in Exercise 13 of Chapter 1, §5.
d. Suppose ε = [t] ∈R/⟨t3⟩and 1
2 ∈k. Find an analogous formula for f (a + bε).
11. Let R be a commutative ring. Show that the set of nilpotent elements of R forms an
ideal in R. Hint: To show that the sum of two nilpotent elements is also nilpotent, use the
binomial expansion of (a + b)m for a suitable exponent m.
12. This exercise will show that the two mappings given in (2) are inverses of each other.
a. If I ⊆J is an ideal of k[x1, . . . , xn], show that J = { f ∈k[x1, . . . , xn] | [ f ] ∈J/I},
where J/I = {[ j] | j ∈J}. Explain how your proof uses the assumption I ⊆J.
b. If ˜J is an ideal of k[x1, . . . , xn]/I, show that ˜J = {[ f ] ∈k[x1, . . . , xn]/I | f ∈J},
where J = { j | [ j] ∈˜J}.
13. Let R and S be commutative rings and let Φ : R →S be a ring homomorphism.
a. If J ⊆S is an ideal, show that Φ−1(J) is an ideal in R.
b. If Φ is an isomorphism of rings, show that there is a one-to-one, inclusion-preserving
correspondence between the ideals of R and the ideals of S.
14. This problem studies the ideals in some quotient rings.
a. Let I = ⟨x3 −x⟩⊆R = R[x]. Determine the ideals in the quotient ring R/I using
Proposition 10. Draw a diagram indicating the inclusions among these ideals.
b. How does your answer change if I = ⟨x3 + x⟩?
15. This problem considers some special quotient rings of R[x, y].
a. Let I = ⟨x2, y2⟩⊆R[x, y]. Describe the ideals in R[x, y]/I. Hint: Use Proposition 10.
b. Is R[x, y]/⟨x3, y⟩isomorphic to R[x, y]/⟨x2, y2⟩?
16. Let Φ : k[x1, . . . , xn] →S be a ring homomorphism. The set {r ∈k[x1, . . . , xn] | Φ(r) =
0 ∈S} is called the kernel of Φ, written ker(Φ).
a. Show that ker(Φ) is an ideal in k[x1, . . . , xn].
b. Show that the mapping ν from k[x1, . . . , xn]/ker(Φ) to S deﬁned by ν([r]) = Φ(r) is
well-deﬁned in the sense that ν([r]) = ν([r′]) whenever r ≡r′ mod ker(Φ).
c. Show that ν is a ring homomorphism.
d. (The Isomorphism Theorem) Assume that Φ is onto. Show that ν is a one-to-one
and onto ring homomorphism. As a result, we have S ∼= k[x1, . . . , xn]/ker(Φ) when
Φ : k[x1, . . . , xn] →S is onto.
17. Use Exercise 16 to give a more concise proof of Theorem 7. Consider the mapping
Φ : k[x1, . . . , xn] →k[V] that takes a polynomial to the element of k[V] that it represents.
Hint: What is the kernel of Φ?
18. Prove that k[x1, . . . , xn]/I has no nonzero nilpotent elements if and only if I is radical.
19. An m × n constant matrix A gives a map αA : k[x1, . . . , xm] →k[y1, . . . , yn] deﬁned by
αA( f )(y) = f (Ay) for f ∈k[x1, . . . , xm]. Exercise 13 of Chapter 4, §3 shows that αA is a
ring homomorphism. If m = n and A is invertible, prove that αA is a ring isomorphism.

248
Chapter 5 Polynomial and Rational Functions on a Variety
§3
Algorithmic Computations in k[x1, . . . , xn]/I
In this section, we will use the division algorithm to produce simple representatives
of equivalence classes for congruence modulo I, where I ⊆k[x1, . . . , xn] is an ideal.
These representatives will enable us to develop an explicit method for computing the
sum and product operations in a quotient ring k[x1, . . . , xn]/I. As an added dividend,
we will derive an easily checked criterion to determine when a system of polynomial
equations over C has only ﬁnitely many solutions. We will also describe a strategy
for ﬁnding the solutions of such a system.
The basic idea that we will use is a direct consequence of the fact that the remain-
der on division of a polynomial f by a Gröbner basis G for an ideal I is uniquely
determined by the polynomial f. (This was Proposition 1 of Chapter 2, §6.) Fur-
thermore, we have the following basic observations reinterpreting the result of the
division and the form of the remainder.
Proposition 1. Fix a monomial ordering on k[x1, . . . , xn] and let I ⊆k[x1, . . . , xn]
be an ideal. As in Chapter 2, §5, ⟨LT(I)⟩will denote the ideal generated by the
leading terms of elements of I.
(i) Every f ∈k[x1, . . . , xn] is congruent modulo I to a unique polynomial r which is
a k-linear combination of the monomials in the complement of ⟨LT(I)⟩.
(ii) The elements of {xα | xα /∈⟨LT(I)⟩} are "linearly independent modulo I," i.e.,
if we have

α
cαxα ≡0 mod I,
where the xα are all in the complement of ⟨LT(I)⟩, then cα = 0 for all α.
Proof. (i) Let G be a Gröbner basis for I and let f ∈k[x1, . . . , xn]. By the division
algorithm, the remainder r = f G satisﬁes f = q + r, where q ∈I. Hence, f −r =
q ∈I, so f ≡r mod I. The division algorithm also tells us that r is a k-linear
combination of the monomials xα /∈⟨LT(I)⟩. The uniqueness of r follows from
Proposition 1 of Chapter 2, §6.
(ii) The argument to establish this part of the proposition is essentially the same
as the proof of the uniqueness of the remainder in Proposition 1 of Chapter 2, §6.
We leave it to the reader to carry out the details.
□
Historically, this was the ﬁrst application of Gröbner bases. Buchberger's thesis
BUCHBERGER (1965) concerned the question of ﬁnding "standard sets of represen-
tatives" for the classes in quotient rings. We also note that if I = I(V) for a variety V,
Proposition 1 gives standard representatives for the polynomial functions φ ∈k[V].
Example 2. Let I = ⟨xy3 −x2, x3y2 −y⟩in R[x, y] and use graded lex order. We ﬁnd
that
G = {x3y2 −y, x4 −y2, xy3 −x2, y4 −xy}
is a Gröbner basis for I. Hence, ⟨LT(I)⟩= ⟨x3y2, x4, xy3, y4⟩. As in Chapter 2, §4,
we can draw a diagram in Z2
≥0 to represent the exponent vectors of the monomials

§3 Algorithmic Computations in k[x1, . . . , xn]/I
249
in ⟨LT(I)⟩and its complement as follows. The vectors
α(1) = (3, 2),
α(2) = (4, 0),
α(3) = (1, 3),
α(4) = (0, 4)
are the exponent vectors of the generators of ⟨LT(I)⟩. Thus, the elements of
((3, 2) + Z2
≥0) ∪((4, 0) + Z2
≥0) ∪((1, 3) + Z2
≥0) ∪((0, 4) + Z2
≥0)
are the exponent vectors of monomials in ⟨LT(I)⟩. As a result, we can represent the
monomials in ⟨LT(I)⟩by the integer points in the shaded region in Z2
≥0 given below:
n
m
(m,n) ←→ xm yn
(0,4)
(1,3)
(3,2)
(4,0)
Given any f ∈R[x, y], Proposition 1 implies that the remainder f G will be a
R-linear combination of the 12 monomials 1, x, x2, x3, y, xy, x2y, x3y, y2, xy2, x2y2, y3
not contained in the shaded region. Note that in this case the remainders all belong
to a ﬁnite-dimensional vector subspace of R[x, y].
We may also ask what happens if we use a different monomial order in R[x, y]
with the same ideal. If we use lex order instead of grlex, with the variables ordered
y > x, we ﬁnd that a Gröbner basis in this case is
G = {y −x7, x12 −x2}.
Hence, for this monomial order, ⟨LT(I)⟩= ⟨y, x12⟩, and ⟨LT(I)⟩contains all the
monomials with exponent vectors in the shaded region shown below:
n
m
(m,n) ←→ xm yn
(0,1)
(12,0)
Thus, for every f ∈R[x, y], we see that f G ∈Span(1, x, x2, . . . , x11).

250
Chapter 5 Polynomial and Rational Functions on a Variety
Note that ⟨LT(I)⟩and the remainders can be completely different depending on
which monomial order we use. In both cases, however, the possible remainders form
the elements of a 12-dimensional vector space. The fact that the dimension is the
same in both cases is no accident, as we will soon see. No matter what monomial
order we use, for a given ideal I, we will always ﬁnd the same number of monomials
in the complement of ⟨LT(I)⟩(in the case that this number is ﬁnite).
Example 3. For the ideal considered in Example 2, there were only ﬁnitely many
monomials in the complement of ⟨LT(I)⟩. This is actually a very special situation.
For instance, consider I = ⟨x −z2, y −z3⟩⊆k[x, y, z]. Using lex order, the given
generators for I already form a Gröbner basis, so that ⟨LT(I)⟩= ⟨x, y⟩. The set
of possible remainders modulo I is thus the set of all k-linear combinations of the
powers of z. In this case, we recognize I as the ideal of a twisted cubic curve in k3. As
a result of Proposition 1, we see that every polynomial function on the twisted cubic
can be uniquely represented by a polynomial in k[z]. Hence, the space of possible
remainders is not ﬁnite-dimensional and V(I) is a curve. What can you say about
V(I) for the ideal in Example 2?
In any case, we can use Proposition 1 in the following way to describe a portion
of the algebraic structure of the quotient ring k[x1, . . . , xn]/I.
Proposition 4. Let I ⊆k[x1, . . . , xn] be an ideal. Then k[x1, . . . , xn]/I is isomorphic
as a k-vector space to S = Span(xα | xα /∈⟨LT(I)⟩).
Proof. By Proposition 1, the mapping Φ : k[x1, . . . , xn]/I
→S deﬁned by
Φ([ f]) = f G deﬁnes a one-to-one correspondence between the classes in k[x1, . . . ,
xn]/I and the elements of S. Hence, it remains to check that Φ preserves the vector
space operations. Consider the sum operation in k[x1, . . . , xn]/I introduced in §2.
If [ f], [g] are elements of k[x1, . . . , xn]/I, then using Proposition 1, we can "stan-
dardize" our polynomial representatives by computing remainders with respect to a
Gröbner basis G for I. By Exercise 13 of Chapter 2, §6, we have f + gG = f G + gG,
so that if
f G =

α
cαxα
and
gG =

α
dαxα,
where the sum is over those α with xα /∈⟨LT(I)⟩, then
(1)
f + gG =

α
(cα + dα)xα.
It follows that with the standard representatives, the sum operation in k[x1, . . . , xn]/I
is the same as the vector sum in the k-vector space S = Span(xα | xα /∈⟨LT(I)⟩).
Further, if c ∈k, we leave it as an exercise to prove that c · f G = c · f G (this is an
easy consequence of the uniqueness part of Proposition 1). It follows that
c · f G =

α
ccαxα,

§3 Algorithmic Computations in k[x1, . . . , xn]/I
251
which shows that multiplication by c in k[x1, . . . , xn]/I is the same as scalar multi-
plication in S. Thus Φ is linear and hence is a vector space isomorphism.
□
The product operation in k[x1, . . . , xn]/I is slightly less straightforward. The rea-
son for this is clear, however, if we consider an example. Let I be the ideal
I = ⟨y + x2 −1, xy −2y2 + 2y⟩⊆R[x, y].
If we compute a Gröbner basis for I using lex order with x > y, then we get
(2)
G = {x2 + y −1, xy −2y2 + 2y, y3 −(7/4)y2 + (3/4)y}.
Thus, ⟨LT(I)⟩= ⟨x2, xy, y3⟩, and {1, x, y, y2} forms a basis for the vector space of
remainders modulo I. Consider the classes of f = 3y2 +x and g = x−y in R[x, y]/I.
The product of [ f] and [g] is represented by f ·g = 3xy2+x2−3y3−xy. However, this
polynomial cannot be the standard representative of the product function because it
contains monomials that are in ⟨LT(I)⟩. Hence, we should divide by G, and the
remainder f · gG will be the standard representative of the product. We have
3xy2 + x2 −3y3 −xy
G = (−11/4)y2 −(5/4)y + 1,
which is in Span(1, x, y, y2) as we expect.
The above discussion gives a completely algorithmic way to handle computations
in k[x1, . . . , xn]/I. To summarize, we have proved the following result.
Proposition 5. Let I be an ideal in k[x1, . . . , xn] and let G be a Gröbner basis of
I with respect to any monomial order. For each [ f] ∈k[x1, . . . , xn]/I, we get the
standard representative f = f G in S = Span(xα | xα /∈⟨LT(I)⟩). Then:
(i) [ f] + [g] is represented by f + g.
(ii) [ f] · [g] is represented by f · g ∈S.
We will conclude this section by using the ideas we have developed to give an
algorithmic criterion to determine when a variety in Cn contains only a ﬁnite number
of points or, equivalently, to determine when a system of polynomial equations has
only a ﬁnite number of solutions in Cn. Here is a general version of the result.
Theorem 6 (Finiteness Theorem). Let I ⊆k[x1, . . . , xn] be an ideal and ﬁx a
monomial ordering on k[x1, . . . , xn]. Consider the following ﬁve statements:
(i) For each i, 1 ≤i ≤n, there is some mi ≥0 such that xmi
i
∈⟨LT(I)⟩.
(ii) Let G be a Gröbner basis for I. Then for each i, 1 ≤i ≤n, there is some mi ≥0
such that xmi
i
= LM(g) for some g ∈G.
(iii) The set {xα | xα /∈⟨LT(I)⟩} is ﬁnite.
(iv) The k-vector space k[x1, . . . , xn]/I is ﬁnite-dimensional.
(v) V(I) ⊆kn is a ﬁnite set.
Then (i)-(iv) are equivalent and they all imply (v). Furthermore, if k is algebraically
closed, then (i)-(v) are all equivalent.

252
Chapter 5 Polynomial and Rational Functions on a Variety
Proof. We prove that (i)-(iv) are equivalent by showing (i) ⇔(ii), (i) ⇔(iii), and
(iii) ⇔(iv).
(i) ⇔(ii) Assume xmi
i
∈⟨LT(I)⟩. Since G is a Gröbner basis of I, ⟨LT(I)⟩=
⟨LT(g) | g ∈G⟩. By Lemma 2 of Chapter 2, §4, there is some g ∈G, such that
LT(g) divides xmi
i . But this implies that LM(g) is a power of xi, as claimed. The
opposite implication follows directly from the deﬁnition of ⟨LT(I)⟩since G ⊆I.
(i) ⇔(iii) If some power xmi
i
∈⟨LT(I)⟩for each i, then the monomials xα1
1 · · · xαn
n
for which some αi ≥mi are all in ⟨LT(I)⟩. The monomials in the complement of
⟨LT(I)⟩must have 0 ≤αi ≤mi −1 for each i. As a result, the number of monomials
in the complement of ⟨LT(I)⟩is at most m1 · m2 · · · mn. For the opposite implication,
assume that the complement consists of N < ∞monomials. Then, for each i, at least
one of the N + 1 monomials 1, xi, x2
i , . . . , xN
i must lie in ⟨LT(I)⟩.
(iii) ⇔(iv) follows from Proposition 4.
To complete the proof, we will show (iv) ⇒(v) and, assuming k is algebraically
closed, (v) ⇒(i).
(iv) ⇒(v) To show that V = V(I) is ﬁnite, it sufﬁces to show that for each i there
can be only ﬁnitely many distinct i-th coordinates for the points of V. Fix i and con-
sider the classes [x j
i ] in k[x1, . . . , xn]/I, where j = 0, 1, 2, . . .. Since k[x1, . . . , xn]/I
is ﬁnite-dimensional, the [x j
i ] must be linearly dependent in k[x1, . . . , xn]/I. Thus,
there exist constants cj (not all zero) and some m such that
m

j=0
cj[x j
i ] =
%
m

j=0
cjx j
i
&
= [0].
However, this implies that m
j=0 cjx j
i ∈I. Since a nonzero polynomial can have
only ﬁnitely many roots in k, this shows that the points of V have only ﬁnitely many
different i-th coordinates.
(v) ⇒(i) Assume k is algebraically closed and V = V(I) is ﬁnite. If V = ∅, then
1 ∈I by the Weak Nullstellensatz. In this case, we can take mi = 0 for all i. If V
is nonempty, then for a ﬁxed i, let a1, . . . , am ∈k be the distinct i-th coordinates of
points in V. Form the one-variable polynomial
f(xi) =
m
!
j=1
(xi −aj).
By construction, f vanishes at every point in V, so f ∈I(V). By the Nullstellensatz,
there is some N ≥1 such that f N ∈I. But this says that the leading monomial of f N
is in ⟨LT(I)⟩. Examining our expression for f, we see that xmN
i
∈⟨LT(I)⟩.
□
Over an algebraically closed ﬁeld k, Theorem 6 shows how we can characterize
"zero-dimensional" varieties (varieties containing only ﬁnitely many points) using
the properties of k[x1, . . . , xn]/I. In Chapter 9, we will take up the question of assign-
ing a dimension to a general variety, and some of the ideas introduced in Theorem 6
will be useful. The same holds for Example 3.

§3 Algorithmic Computations in k[x1, . . . , xn]/I
253
A judicious choice of monomial ordering can sometimes lead to a very easy
determination that a variety is ﬁnite. For example, consider the ideal
I = ⟨x5 + y3 + z2 −1, x2 + y3 + z −1, x4 + y5 + z6 −1⟩.
Using grlex, we see that x5, y3, z6 ∈⟨LT(I)⟩since those are the leading monomials
of the three generators. By (i) ⇒(v) of Theorem 6, we know that V(I) is ﬁnite
(even without computing a Gröbner basis). If we actually wanted to determine which
points were in V(I), we would need to do elimination, for instance, by computing a
lexicographic Gröbner basis. We will say more about ﬁnding solutions below.
We also have a quantitative estimate for the number of solutions when the criteria
of Theorem 6 are satisﬁed.
Proposition 7. Let I ⊆k[x1, . . . , xn] be an ideal such that for each i, some power
xmi
i
∈⟨LT(I)⟩, and set V = V(I). Then:
(i) The number of points of V is at most dim k[x1, . . . , xn]/I (where "dim" means
dimension as a vector space over k).
(ii) The number of points of V is at most m1 · m2 · · · mn.
(iii) If I is radical and k is algebraically closed, then equality holds in part (i), i.e.,
the number of points in V is exactly dim k[x1, . . . , xn]/I.
Proof. We ﬁrst show that given distinct points p1, . . . , pm ∈kn, there is a polyno-
mial f1 ∈k[x1, . . . , xn] with f1(p1) = 1 and f1(p2) = · · · = f1(pm) = 0. To prove
this, note that if a ̸= b ∈kn, then they must differ at some coordinate, say the j-
th, and it follows that g = (xj −bj)/(aj −bj) satisﬁes g(a) = 1, g(b) = 0. If we
apply this observation to each pair p1 ̸= pi, i ≥2, we get polynomials gi such that
gi(p1) = 1 and gi(pi) = 0 for i ≥2. Then f1 = g2·g3 · · · gm has the desired property.
In this argument, there is nothing special about p1. If we apply the same argument
with p1 replaced by each of p1, . . . , pm in turn, we get polynomials f1, . . . , fm such
that fi(pi) = 1 and fi(pj) = 0 for i ̸= j.
Now we can prove the proposition. By Theorem 6, we know that V is ﬁnite. Write
V = {p1, . . . , pm}, where the pi are distinct.
(i) Let f1, . . . , fm be as above. If we can prove that [ f1], . . . , [ fm] ∈k[x1, . . . , xn]/I
are linearly independent, then
(3)
m ≤dim k[x1, . . . , xn]/I
will follow, proving (i).
To prove linear independence, suppose that m
i=1 ai[ fi] = [0] in k[x1, . . . , xn]/I,
where ai ∈k. Back in k[x1, . . . , xn], this means that g = m
i=1 ai fi ∈I, so that g
vanishes at all points of V = {p1, . . . , pm}. Then, for 1 ≤j ≤m, we have
0 = g(pj) =
m

i=1
ai fi(pj) = 0 + aj fj(pj) = aj,
and linear independence follows.

254
Chapter 5 Polynomial and Rational Functions on a Variety
(ii) Proposition 4 implies that dim k[x1, . . . , xn]/I is the number of monomials in
the set {xα | xα /∈⟨LT(I)⟩}. The proof of (i) ⇔(iii) from Theorem 6 shows that this
number is at most m1 · · · mn. Then the desired bound follows from (i).
(iii) Now assume that k is algebraically closed and I is a radical ideal. To prove
that equality holds in (3), it sufﬁces to show that [ f1], . . . , [ fm] form a basis of
k[x1, . . . , xn]/I. Since we just proved linear independence, we only need to show that
they span. Thus, let [g] ∈k[x1, . . . , xn]/I be arbitrary, and set ai = g(pi). Then con-
sider h = g −m
i=1 aifi. One easily computes h(pj) = 0 for all j, so that h ∈I(V).
By the Nullstellensatz, I(V) = I(V(I)) =
√
I since k is algebraically closed, and
since I is radical, we conclude that h ∈I. Thus [h] = [0] in k[x1, . . . , xn]/I, which
implies [g] = m
i=1 ai[ fi]. The proposition is now proved.
□
For an example of Proposition 7, consider the ideal I = ⟨xy3 −x2, x3y2 −y⟩
in R[x, y] from Example 2 at the beginning of this section. Using grlex, we found
x4, y4 ∈⟨LT(I)⟩, so that V(I) ⊆R2 has at most 4 · 4 = 16 points by part (ii) of
Proposition 7. Yet Example 2 also shows that R[x, y]/I has dimension 12 over R.
Thus part (i) of the proposition gives the better bound of 12. Note that the bound of
12 also holds if we work over C.
In Example 2, we discovered that switching to lex order with y > x gives
the Gröbner basis G = {y −x7, x12 −x2} for I. Then part (ii) of Proposition 7
gives the bound of 1 · 12 = 12, which agrees with the bound given by part (i). By
solving the equations from G, we see that V(I) actually contains only 11 distinct
points in this case:
(4)
V(I) = {(0, 0)} ∪{(ζ, ζ7) | ζ10 = 1}.
(Recall that there are 10 distinct 10-th roots of unity in C.)
For any ideal I, we have V(I) = V(
√
I). Thus, when V(I) is ﬁnite, Proposi-
tion 7 shows how to ﬁnd the exact number of solutions over an algebraically closed
ﬁeld, provided we know
√
I. Computing radicals was discussed in §2 of Chapter 4,
though when I satisﬁes the conditions of Theorem 6,
√
I is simple to compute. See
Proposition (2.7) of COX, LITTLE and O'SHEA (2005) for the details.
Thus far in this section, we have studied the ﬁniteness of solutions in Theorem 6
and the number of solutions in Proposition 7. We now combine this with earlier
results to describe a method for ﬁnding the solutions. For simplicity, we work over
the complex numbers C. Consider a system of equations
(5)
f1 = · · · = fs = 0
whose ideal I ⊆C[x1, . . . , xn] satisﬁes the conditions of Theorem 6. There are no
solutions when I = C[x1, . . . , xn], so we will assume that I is a proper ideal.
Let G be a reduced Gröbner basis of I for lex order with x1 > · · · > xn.
By assumption, for each i, there is g ∈G such that LT(g) = xmi
i . Note that
g ∈C[xi, . . . , xn] since we are using lex order. By Theorem 6, it follows that (5)
has only ﬁnitely many solutions. We will ﬁnd the solutions by working one variable
at a time, using the "back-substitution" method discussed earlier in the book.

§3 Algorithmic Computations in k[x1, . . . , xn]/I
255
We start with G ∩C[xn], which is nonempty by the previous paragraph. Since G
is reduced, this intersection consists of a single polynomial, say G ∩C[xn] = {g}.
The ﬁrst step in solving (5) is to ﬁnd the solutions of g(xn) = 0, i.e., the roots of g.
These exist since C is algebraically closed. This gives partial solutions of (5).
Now we work backwards. Suppose that a = (ai+1, . . . , an) is a partial solution
of (5). This means that a ∈V(Ii), Ii = I ∩C[xi+1, . . . , xn]. The goal to ﬁnd all
ai ∈C such that (ai, a) = (ai, ai+1, . . . , an) is a partial solution in V(Ii−1) for Ii−1 =
I ∩C[xi, . . . , xn]. By the Elimination Theorem, Gi−1 = G ∩C[xi, . . . , xn] is a basis
of Ii−1. Since Gi−1 contains a polynomial with xmi
i
as leading term, the Extension
Theorem (speciﬁcally, Corollary 4 of Chapter 3, §1) implies that the partial solution
a extends to (ai, a) = (ai, ai+1, . . . , an) ∈V(Ii−1).
We can ﬁnd the possible ai's as follows. Let g1, . . . , gℓbe the polynomials Gi−1
in which xi actually appears. When we evaluate Gi−1 at a, any polynomial lying in
C[xi+1, . . . , xn] will vanish identically, leaving us with the equations
(6)
g1(xi, a) = · · · = gℓ(xi, a) = 0.
The desired ai's are the solutions of this system of equations.
Now comes a key observation: there is only one polynomial in (6) to worry about.
To ﬁnd this polynomial, write each gj in the form
gj = cj(xi+1, . . . , xn)xNj
i + terms in which xi has degree < Nj,
where Nj ≥0 and cj ∈k[xi+1, . . . , xn] is nonzero. Among all gj for which cj(a) ̸= 0,
choose g∗with minimal degree in xi. Then Theorem 2 of Chapter 3, §5 implies that
the system (6) is equivalent to the single equation
g∗(xi, a) = 0.
Thus the roots of g∗(xi, a) give all possible ways of extending a to a partial solution
in V(Ii−1). Continuing in this way, we get all solutions of the original system (5).
It is very satisfying to see how our theory gives a complete answer to solving
equations in the zero-dimensional case. In practice, things are more complicated
because of complexity issues related to computing Gröbner bases and numerical
issues related to ﬁnding roots of univariate polynomials.
EXERCISES FOR §3
1. Complete the proof of part (ii) of Proposition 1.
2. In Proposition 5, we stated a method for computing [ f ] · [g] in k[x1, . . . , xn]/I. Could we
simply compute f · gG rather than ﬁrst computing the remainders of f and g separately?
3. Let I = ⟨x4y −z6, x2 −y3z, x3z2 −y3⟩in k[x, y, z].
a. Using lex order, ﬁnd a Gröbner basis G of I and a collection of monomials that spans
the space of remainders modulo G.
b. Repeat part (a) for grlex order. How do your sets of monomials compare?
4. Use the division algorithm and the uniqueness part of Proposition 1 to prove that
c · f G = c · f G whenever f ∈k[x1, . . . , xn] and c ∈k.

256
Chapter 5 Polynomial and Rational Functions on a Variety
5. Let I = ⟨y + x2 −1, xy −2y2 + 2y⟩⊆R[x, y]. This is the ideal used in the example
following Proposition 4.
a. Construct a vector space isomorphism R[x, y]/I ∼= R4.
b. Using the lexicographic Gröbner basis given in (2), compute a "multiplication table"
for the elements {[1], [x], [y], [y2]} in R[x, y]/I. Hint: Express each product as a linear
combination of these four classes.
c. Is R[x, y]/I a ﬁeld? Why or why not?
d. Compute V(I). Hint: It has four points.
e. The lex Gröbner basis computed in (2) shows that m1 = 2 and m2 = 3 are the smallest
powers of x and y, respectively, contained in ⟨LT(I)⟩. What bound does part (ii) of
Proposition 7 give for the number of points in V? Does part (i) of the proposition give
a better bound?
6. Let V = V(x3 −x2
1, x4 −x1x2, x2x4 −x1x5, x2
4 −x3x5) ⊆C5.
a. Using any convenient monomial order, determine a collection of monomials span-
ning the space of remainders modulo a Gröbner basis for the ideal generated by the
deﬁning equations of V.
b. For which i is there some mi ≥0 such that xmi
i
∈⟨LT(I)⟩?
c. Is V a ﬁnite set? Why or why not?
7. Let I be any ideal in k[x1, . . . , xn].
a. Suppose that the set {xα | xα /∈⟨LT(I)⟩} is ﬁnite with d elements for some choice
of monomial order. Show that the dimension of k[x1, . . . , xn]/I as a k-vector space is
equal to d.
b. Deduce from part (a) that the number of monomials in the complement of ⟨LT(I)⟩is
independent of the choice of the monomial order, when that number is ﬁnite.
8. Suppose that I ⊆C[x1, . . . , xn] is a radical ideal with a Gröbner basis {g1, . . . , gn} such
that LT(gi) = xmi
i
for each i. Prove that V(I) contains exactly m1 · m2 · · · mn points.
9. Most computer algebra systems contain routines for simplifying radical expressions. For
example, instead of writing
r =
1
x +
√
2 +
√
3
,
most systems would allow you to rationalize the denominator and rewrite r as a quotient
of polynomials in x, where
√
2 and
√
3 appear in the coefﬁcients only in the numerator.
The idea behind one method used here is as follows.
a. Explain why r can be seen as a rational function in x, whose coefﬁcients are elements
of the quotient ring R = Q[y1, y2]/⟨y2
1 −2, y2
2 −3⟩. Hint: See Exercise 4 from §2 of
this chapter.
b. Compute a Gröbner basis G for I = ⟨y2
1 −2, y2
2 −3⟩and construct a multiplication
table for the classes of the monomials spanning the possible remainders modulo G
(which should be {[1], [y1], [y2], [y1y2]}).
c. Now, to rationalize the denominator of r, we can try to solve the following equation
(x[1] + [y1] + [y2]) · (a0[1] + a1[y1] + a2[y2] + a3[y1y2]) = [1],
where a0, a1, a2, a3 are rational functions of x with rational number coefﬁcients. Mul-
tiply out the above equation using your table from part (b), match coefﬁcients, and
solve the resulting linear equations for a0, a1, a2, a3. Then
a0[1] + a1[y1] + a2[y2] + a3[y1y2]
gives the rationalized expression for r.
10. In this problem, we will establish a fact about the number of monomials of total de-
gree less than or equal to d in k[x1, . . . , xn] and relate this to the intuitive notion of the
dimension of the variety V = kn.

§4 The Coordinate Ring of an Afﬁne Variety
257
a. Explain why every monomial in k[x1, . . . , xn] is in the complement of ⟨LT( I(V))⟩for
V = kn.
b. Show that for all d, n ≥0, the number of distinct monomials of total degree less
than or equal to d in k[x1, . . . , xn] is the binomial coefﬁcient
n+d
n

. (This generalizes
part (a) of Exercise 5 in Chapter 2, §1.)
c. When n is ﬁxed, explain why this number of monomials grows like dn as d →∞.
Note that the exponent n is the same as the intuitive dimension of the variety V = kn,
for which k[V] = k[x1, . . . , xn].
11. In this problem, we will compare what happens with the monomials not in ⟨LT(I)⟩in
two examples where V(I) is not ﬁnite, and one where V(I) is ﬁnite.
a. Consider the variety V(I) ⊆C3, where I = ⟨x2 + y, x −y2 + z2, xy −z⟩. Compute
a Gröbner basis for I using lex order, and, for 1 ≤d ≤10, tabulate the number
of monomials of total degree ≤d that are not in ⟨LT(I)⟩. Note that by Theorem 6,
V(I) is a ﬁnite subset of C3. Hint: It may be helpful to try to visualize or sketch a
3-dimensional analogue of the diagrams in Example 2 for this ideal.
b. Repeat the calculations of part (a) for J = ⟨x2 + y, x −y2 + z2⟩. Here, V(J) is not
ﬁnite. How does the behavior of the number of monomials of total degree ≤d in the
complement of ⟨LT(J)⟩(as a function of d) differ from the behavior in part (a)?
c. Let H(d) be the number of monomials of total degree ≤d in the complement of
⟨LT(J)⟩. Can you guess a power ℓsuch that H(d) will grow roughly like dℓas d
grows?
d. Now repeat parts (b) and (c) for the ideal K = ⟨x2 + y⟩.
e. Using the intuitive notion of the dimension of a variety that we developed in Chap-
ter 1, can you see a pattern here? We will return to these questions in Chapter 9.
12. Let k be any ﬁeld, and suppose I ⊆k[x1, . . . , xn] has the property that k[x1, . . . , xn]/I is
a ﬁnite-dimensional vector space over k.
a. Prove that dim k[x1, . . . , xn]/
√
I ≤dim k[x1, . . . , xn]/I. Hint: Show that I ⊆
√
I
induces a map of quotient rings k[x1, . . . , xn]/I →k[x1, . . . , xn]/
√
I which is onto.
b. Show that the number of points in V(I) is at most dim k[x1, . . . , xn]/
√
I.
c. Give an example to show that equality need not hold in part (b) when k is not alge-
braically closed.
d. Assume that k is algebraically closed. Strengthen part (iii) of Proposition 7 by show-
ing that I is radical if and only if dim k[x1, . . . , xn]/I equals the number of points in
V(I). Hint: Use part (a).
13. A polynomial in k[x1, . . . , xn] of the form xα −xβ is called a binomial, and an ideal
generated by binomials is a binomial ideal. Let I ⊆C[x1, . . . , xn] be a binomial ideal.
a. Prove that I has a reduced lex Gröbner basis consisting of binomials. Hint: What is
the S-polynomial of two binomials?
b. Assume that V(I) ⊆Cn is ﬁnite. Prove that every coordinate of a solution is either
zero or a root of unity. Hint: Follow the solution procedure described at the end of the
section. Note that a root of a root of unity is again a root of unity.
c. Explain how part (b) relates to equation (4) in the text.
§4
The Coordinate Ring of an Afﬁne Variety
In this section, we will apply the algebraic tools developed in §§2 and 3 to study the
ring k[V] of polynomial functions on an afﬁne variety V ⊆kn. Using the isomor-
phism k[V] ∼= k[x1, . . . , xn]/I(V) from §2, we will frequently identify k[V] with the
quotient ring k[x1, . . . , xn]/I(V). Thus, given a polynomial f ∈k[x1, . . . , xn], we let
[ f] denote the polynomial function in k[V] represented by f.

258
Chapter 5 Polynomial and Rational Functions on a Variety
In particular, each variable xi gives a polynomial function [xi] : V →k whose
value at a point p ∈V is the i-th coordinate of p. We call [xi] ∈k[V] the i-th
coordinate function on V. Then the isomorphism k[V] ∼= k[x1, . . . , xn]/I(V) shows
that the coordinate functions generate k[V] in the sense that any polynomial function
on V is a k-linear combination of products of the [xi]. This explains the following
terminology.
Deﬁnition 1. The coordinate ring of an afﬁne variety V ⊆kn is the ring k[V].
Many results from previous sections of this chapter can be rephrased in terms of
the coordinate ring. For example:
• Proposition 4 from §1: A variety is irreducible if and only if its coordinate ring
is an integral domain.
• Theorem 6 from §3: Over an algebraically closed ﬁeld k, a variety is ﬁnite if and
only if its coordinate ring is ﬁnite-dimensional as a k-vector space.
In the "algebra-geometry" dictionary of Chapter 4, we related varieties in kn to
ideals in k[x1, . . . , xn]. One theme of Chapter 5 is that this dictionary still works if
we replace kn and k[x1, . . . , xn] by a general variety V and its coordinate ring k[V].
For this purpose, we introduce the following deﬁnitions.
Deﬁnition 2. Let V ⊆kn be an afﬁne variety.
(i) For any ideal J = ⟨φ1, . . . , φs⟩⊆k[V], we deﬁne
VV(J) = {(a1, . . . , an) ∈V | φ(a1, . . . , an) = 0 for all φ ∈J}.
We call VV(J) a subvariety of V.
(ii) For each subset W ⊆V, we deﬁne
IV(W) = {φ ∈k[V] | φ(a1, . . . , an) = 0 for all (a1, . . . , an) ∈W}.
For instance, let V = V(z −x2 −y2) ⊆R3. If we take J = ⟨[x]⟩⊆R[V], then
W = VV(J) = ⟨(0, a, a2) | a ∈R⟩⊆V
is a subvariety of V. Note that this is the same as V(z −x2 −y2, x) in R3. Similarly,
if we let W = {(1, 1, 2)} ⊆V, then we leave it as an exercise to show that
IV(W) = ⟨[x −1], [y −1]⟩.
Given a ﬁxed afﬁne variety V, we can use IV and VV to relate subvarieties of V
to ideals in k[V]. The ﬁrst result we get is the following.
Proposition 3. Let V ⊆kn be an afﬁne variety.
(i) For each ideal J ⊆k[V], W = VV(J) is an afﬁne variety in kn contained in V.
(ii) For each subset W ⊆V, IV(W) is an ideal of k[V].
(iii) If J ⊆k[V] is an ideal, then J ⊆
√
J ⊆IV(VV(J)).
(iv) If W ⊆V is a subvariety, then W = VV(IV(W)).

§4 The Coordinate Ring of an Afﬁne Variety
259
Proof. To prove (i), we will use the one-to-one correspondence of Proposition 10
of §2 between the ideals of k[V] and the ideals in k[x1, . . . , xn] containing I(V). Let
˜J = { f ∈k[x1, . . . , xn] | [ f] ∈J} ⊆k[x1, . . . , xn] be the ideal corresponding to
J ⊆k[V]. Then V(˜J) ⊆V, since I(V) ⊆˜J. But we also have V(˜J) = VV(J)
by deﬁnition since the elements of ˜J represent the functions in J on V. Thus, W
(considered as a subset of kn) is an afﬁne variety in its own right.
The proofs of (ii), (iii), and (iv) are similar to arguments given in earlier chapters
and the details are left as an exercise. Note that the deﬁnition of the radical of an
ideal is the same in k[V] as it is in k[x1, . . . , xn].
□
We can also show that the radical ideals in k[V] correspond to the radical ideals
in k[x1, . . . , xn] containing I(V).
Proposition 4. An ideal J ⊆k[V] is radical if and only if the corresponding ideal
˜J = { f ∈k[x1, . . . , xn] | [ f] ∈J} ⊆k[x1, . . . , xn] is radical.
Proof. Assume J is radical, and let f ∈k[x1, . . . , xn] satisfy f m ∈˜J for some m ≥1.
Then [ f m] = [ f]m ∈J. Since J is a radical ideal, this implies that [ f] ∈J. Hence,
f ∈˜J, so ˜J is also a radical ideal. Conversely, if ˜J is radical and [ f]m ∈J, then
[ f m] ∈J, so f m ∈˜J. Since ˜J is radical, this shows that f ∈˜J. Hence, [ f] ∈J and J
is radical.
□
Rather than discuss the complete "ideal-variety" correspondence (as we did in
Chapter 4), we will conﬁne ourselves to the following result which highlights some
of the important properties of the correspondence.
Theorem 5. Let k be an algebraically closed ﬁeld and let V ⊆kn be an afﬁne
variety.
(i) (The Nullstellensatz in k[V]) If J is any ideal in k[V], then
IV(VV(J)) =
√
J = {[ f] ∈k[V] | [ f]m ∈J}.
(ii) The correspondences
 afﬁne subvarieties
W ⊆V

IV
−→
VV
←−
 radical ideals
J ⊆k[V]

are inclusion-reversing bijections and are inverses of each other.
(iii) Under the correspondence given in (ii), points of V correspond to maximal
ideals of k[V].
Proof. (i) Let J be an ideal of k[V]. By the correspondence of Proposition 10 of
§2, J corresponds to the ideal ˜J ⊆k[x1, . . . , xn] as in the proof of Proposition 4,
where V(˜J) = VV(J). As a result, if [ f] ∈IV(VV(J)), then f ∈I(V(˜J)). By the
Nullstellensatz in kn, I(V(˜J)) =
"
˜J, so f m ∈˜J for some m ≥1. But then, [ f m] =
[ f]m ∈J, so [ f] ∈
√
J in k[V]. We have shown that IV(VV(J)) ⊆
√
J. Since the
opposite inclusion holds for any ideal, our Nullstellensatz in k[V] is proved.

260
Chapter 5 Polynomial and Rational Functions on a Variety
(ii) follows from (i) as in Chapter 4.
(iii) is proved in the same way as Theorem 11 of Chapter 4, §5.
□
When k is algebraically closed, the Weak Nullstellensatz also holds in k[V]. You
will prove this in Exercise 16.
Next, we return to the general topic of a classiﬁcation of varieties that we posed
in §1. What should it mean for two afﬁne varieties to be "isomorphic"? One reason-
able answer is given in the following deﬁnition.
Deﬁnition 6. Let V ⊆km and W ⊆kn be afﬁne varieties. We say that V and W are
isomorphic if there exist polynomial mappings α : V →W and β : W →V such
that α ◦β = idW and β ◦α = idV. (For any variety V, we write idV for the identity
mapping from V to itself. This is always a polynomial mapping.)
Intuitively, varieties that are isomorphic should share properties such as irre-
ducibility, dimension, etc. In addition, subvarieties of V should correspond to subva-
rieties of W, and so forth. For instance, saying that a variety W ⊆kn is isomorphic to
V = km implies that there is a one-to-one and onto polynomial mapping α : km →W
with a polynomial inverse. Thus, we have a polynomial parametrization of W with
especially nice properties! Here is an example, inspired by a technique used in geo-
metric modeling, which illustrates the usefulness of this idea.
Example 7. Let us consider the two surfaces
Q1 = V(x2 −xy −y2 + z2) = V( f1),
Q2 = V(x2 −y2 + z2 −z) = V( f2)
in R3. (These might be boundary surfaces of a solid region in a shape we were
designing, for example.) To study the intersection curve C = V( f1, f2) of the two
surfaces, we could proceed as follows. Neither Q1 nor Q2 is an especially simple
surface, so the intersection curve is fairly difﬁcult to visualize directly. However, as
usual, we are not limited to using the particular equations f1, f2 to deﬁne the curve!
It is easy to check that C = V( f1, f1 +cf2), where c ∈R is any nonzero real number.
Hence, the surfaces Fc = V( f1 + cf2) also contain C. These surfaces, together with
Q2, are often called the elements of the pencil of surfaces determined by Q1 and Q2.
(A pencil of varieties is a one-parameter family of varieties, parametrized by the
points of k. In the above case, the parameter is c ∈R.)
If we can ﬁnd a value of c making the surface Fc particularly simple, then under-
standing the curve C will be correspondingly easier. Here, if we take c = −1, then
F−1 is deﬁned by
0 = f1 −f2
= z −xy.
The surface Q = F−1 = V(z −xy) is much easier to understand because it is
isomorphic as a variety to R2 [as is the graph of any polynomial function f(x, y)].
To see this, note that we have polynomial mappings:

§4 The Coordinate Ring of an Afﬁne Variety
261
α : R2 −→Q,
(x, y) 
−→(x, y, xy),
π : Q −→R2,
(x, y, z) 
−→(x, y),
which satisfy α ◦π = idQ and π ◦α = idR2.
Hence, curves on Q can be reduced to plane curves in the following way. To study
C, we can project to the curve π(C) ⊂R2, and we obtain the equation
x2y2 + x2 −xy −y2 = 0
for π(C) by substituting z = xy in either f1 or f2. Note that π and α restrict to
give isomorphisms between C and π(C), so we have not really lost anything by
projecting in this case.
x
y
In particular, each point (a, b) on π(C) corresponds to exactly one point (a, b, ab)
on C. In the exercises, you will show that π(C) can also be parametrized as
(1)
x = −t2 + t + 1
t2 + 1
,
y = −t2 + t + 1
t(t + 2)
.
From this we can also obtain a parametrization of C via the mapping α.
Given the above example, it is natural to ask how we can tell whether two vari-
eties are isomorphic. One way is to consider the relation between their coordinate
rings

262
Chapter 5 Polynomial and Rational Functions on a Variety
k[V] ∼= k[x1, . . . , xm]/I(V)
and
k[W] ∼= k[y1, . . . , yn]/I(W).
The fundamental observation is that if we have a polynomial mapping α : V →W,
then every polynomial function φ : W →k in k[W] gives us another polynomial
function φ ◦α : V →k in k[V]. This will give us a map from k[W] to k[V] with the
following properties.
Proposition 8. Let V and W be varieties (possibly in different afﬁne spaces).
(i) Let α : V →W be a polynomial mapping. Then for every polynomial function
φ : W →k, the composition φ ◦α : V →k is also a polynomial function.
Furthermore, the map α∗: k[W] →k[V] deﬁned by α∗(φ) = φ ◦α is a ring
homomorphism which is the identity on the constant functions k ⊆k[W]. (Note
that α∗"goes in the opposite direction" from α since α∗maps functions on W to
functions on V. For this reason we call α∗the pullback mapping on functions.)
(ii) Conversely, let Φ : k[W] →k[V] be a ring homomorphism which is the identity
on constants. Then there is a unique polynomial mapping α : V →W such that
Φ = α∗.
Proof. (i) Suppose that V ⊆km has coordinates x1, . . . , xm and W ⊆kn has coordi-
nates y1, . . . , yn. Then φ : W →k can be represented by a polynomial f(y1, . . . , yn),
and α : V →W can be represented by an n-tuple of polynomials:
α(x1, . . . , xm) = (h1(x1, . . . , xm), . . . , hn(x1, . . . , xm)).
We compute φ ◦α by substituting α(x1, . . . , xm) into φ. Thus,
(φ ◦α)(x1, . . . , xm) = f(h1(x1, . . . , xm), . . . , hn(x1, . . . , xm)),
which is a polynomial in x1, . . . , xm. Hence, φ ◦α is a polynomial function on V.
It follows that we can deﬁne α∗: k[W] →k[V] by the formula α∗(φ) = φ◦α. To
show that α∗is a ring homomorphism,let ψ be another element of k[W], represented
by a polynomial g(y1, . . . , yn). Then
(α∗(φ + ψ))(x1, . . . , xm) = f(h1(x1, . . . , xm), . . . , hn(x1, . . . , xm)) +
g(h1(x1, . . . , xm), . . . , hn(x1, . . . , xm))
= α∗(φ)(x1, . . . , xm) + α∗(ψ)(x1, . . . , xm).
Hence, α∗(φ + ψ) = α∗(φ) + α∗(ψ), and α∗(φ · ψ) = α∗(φ) · α∗(ψ) is proved
similarly. Thus, α∗is a ring homomorphism.
Finally, consider [a] ∈k[W] for some a ∈k. Then [a] is a constant function on
W with value a, and it follows that α∗([a]) = [a] ◦α is constant on V, again with
value a. Thus, α∗([a]) = [a], so that α∗is the identity on constants.
(ii) Now let Φ : k[W] →k[V] be a ring homomorphism which is the identity
on the constants. We need to show that Φ comes from some polynomial mapping
α : V →W. Since W ⊆kn has coordinates y1, . . . , yn, we get coordinate functions
[yi] ∈k[W]. Then Φ([yi]) ∈k[V], and since V ⊆km has coordinates x1, . . . , xm, we

§4 The Coordinate Ring of an Afﬁne Variety
263
can write Φ([yi]) = [hi(x1, . . . , xm)] ∈k[V] for some polynomial hi ∈k[x1, . . . , xm].
Then consider the polynomial mapping
α = (h1(x1, . . . , xm), . . . , hn(x1, . . . , xm)).
We need to show that α maps V to W and that Φ = α∗.
Given any polynomial F ∈k[y1, . . . , yn], we ﬁrst claim that
(2)
[F ◦α] = Φ([F])
in k[V]. To prove this, note that
[F ◦α] = [F(h1, . . . , hn)] = F([h1], . . . , [hn]) = F(Φ([y1]), . . . , Φ([yn])),
where the second equality follows from the deﬁnition of sum and product in k[V],
and the third follows from [hi] = Φ([yi]). But [F] = [F(y1, . . . , yn)] is a k-linear
combination of products of the [yi], so that
F(Φ([y1]), . . . , Φ([yn])) = Φ([F(y1, . . . , yn)]) = Φ([F])
since Φ is a ring homomorphism which is the identity on k (see Exercise 10).
Equation (2) follows immediately.
We can now prove that α maps V to W. Given a point (c1, . . . , cm) ∈V, we must
show that α(c1, . . . , cm) ∈W. If F ∈I(W), then [F] = 0 in k[W], and since Φ is a
ring homomorphism, we have Φ([F]) = 0 in k[V]. By (2), this implies that [F ◦α]
is the zero function on V. In particular,
[F ◦α](c1, . . . , cm) = F(α(c1, . . . , cm)) = 0.
Since F was an arbitrary element of I(W), this shows α(c1, . . . , cm) ∈W, as desired.
Once we know α maps V to W, equation (2) implies that [F] ◦α = Φ([F]) for
any [F] ∈k[W]. Since α∗([F]) = [F] ◦α, this proves Φ = α∗. It remains to show
that α is uniquely determined. So suppose we have β : V →W such that Φ = β∗.
If β is represented by
β(x1, . . . , xm) = (˜h1(x1, . . . , xm), . . . , ˜hn(x1, . . . , xm)),
then note that β∗([yi]) = [yi] ◦β = [˜hi(x1, . . . , xm)]. A similar computation gives
α∗([yi]) = [hi(x1, . . . , xm)], and since α∗= Φ = β∗, we have [hi] = [˜hi]
for all i. Then hi and ˜hi give the same polynomial function on V, and, hence,
α = (h1, . . . , hn) and β = (˜h1, . . . , ˜hn) deﬁne the same mapping on V. This shows
α = β, and uniqueness is proved.
□
Now suppose that α : V →W and β : W →V are inverse polynomial mappings.
Then α ◦β = idW, where idW : W →W is the identity map. By general properties
of functions, this implies (α ◦β)∗(φ) = id∗
W(φ) = φ ◦idW = φ for all φ ∈k[W].
However, we also have

264
Chapter 5 Polynomial and Rational Functions on a Variety
(3)
(α ◦β)∗(φ) = φ ◦(α ◦β) = (φ ◦α) ◦β
= α∗(φ) ◦β = β∗(α∗(φ)) = (β∗◦α∗)(φ).
Hence, (α◦β)∗= β∗◦α∗= idk[W] as a mapping from k[W] to itself. Similarly, one
can show that (β ◦α)∗= α∗◦β∗= idk[V]. This proves the ﬁrst half of the following
theorem.
Theorem 9. Two afﬁne varieties V ⊆km and W ⊆kn are isomorphic if and only
if there is an isomorphism k[V] ∼= k[W] of coordinate rings which is the identity on
constant functions.
Proof. The above discussion shows that if V and W are isomorphic varieties, then
k[V] ∼= k[W] as rings. Proposition 8 shows that the isomorphism is the identity on
constants.
For the converse, we must show that if we have a ring isomorphism Φ : k[W] →
k[V] which is the identity on k, then Φ and Φ−1 "come from" inverse polynomial
mappings between V and W. By part (ii) of Proposition 8, we know that Φ = α∗for
some α : V →W and Φ−1 = β∗for β : W →V. We need to show that α and β are
inverse mappings. First consider the composite map α ◦β : W →W. This is clearly
a polynomial map, and, using the argument from (3), we see that for any φ ∈k[W],
(4)
(α ◦β)∗(φ) = β∗(α∗(φ)) = Φ−1(Φ(φ)) = φ.
Since the identity map idW : W →W is a polynomial map on W, and we saw above
that id∗
W(φ) = φ for all φ ∈k[W], from (4), we conclude that (α ◦β)∗= id∗
W, and
then α◦β = idW follows from the uniqueness statement of part (ii) of Proposition 8.
In a similar way, one proves that β ◦α = idV, and hence α and β are inverse
mappings. This completes the proof of the theorem.
□
We conclude with several examples to illustrate isomorphisms of varieties and
the corresponding isomorphisms of their coordinate rings.
Let A be an invertible n × n matrix with entries in k and consider the linear
mapping LA : kn →kn deﬁned by LA(x) = Ax, where Ax is matrix multiplication.
This is easily seen to be an isomorphism of varieties by considering LA−1. (An iso-
morphism of a variety with itself is often called an automorphism of the variety.)
It follows from Theorem 9 that L∗
A : k[x1, . . . , xn] →k[x1, . . . , xn] is a ring isomor-
phism. In Exercise 9, you will show that if V is any subvariety of kn, then LA(V)
is a subvariety of kn isomorphic to V since LA restricts to give an isomorphism of
V onto LA(V). For example, the curve we studied in the ﬁnal example of §1 of this
chapter was obtained from the "standard" twisted cubic curve in C3 by an invertible
linear mapping. Refer to equation (5) of §1 and see if you can identify the mapping
LA that was used.
Next, let f(x, y) ∈k[x, y] and consider the graph of the polynomial function on
k2 given by f [that is, the variety V = V(z −f(x, y)) ⊆k3]. Generalizing what we
said concerning the variety V(z −xy) in analyzing the curve given in Example 7, it
will always be the case that a graph V is isomorphic as a variety to k2. The reason
is that the projection on the (x, y)-plane π : V →k2, and the parametrization of

§4 The Coordinate Ring of an Afﬁne Variety
265
the graph given by α : k2 →V, α(x, y) = (x, y, f(x, y)) are inverse mappings. The
isomorphism of coordinate rings corresponding to α just consists of substituting
z = f(x, y) into every polynomial function F(x, y, z) on V.
Finally, consider the curve V = V(y5 −x2) in R2.
x
y
We claim that V is not isomorphic to R as a variety, even though there is a one-to-
one polynomial mapping from V to R given by projecting V onto the x-axis. The
reason lies in the coordinate ring of V, R[V] = R[x, y]/⟨y5 −x2⟩. If there were an
isomorphism α : R →V, then the "pullback" α∗: R[V] →R[u] would be a ring
isomorphism given by
α∗([x]) = g(u),
α∗([y]) = h(u),
where g(u), h(u) ∈R[u] are polynomials. Since y5 −x2 represents the zero function
on V, we must have α∗([y5 −x2]) = (h(u))5 −(g(u))2 = 0 in R[u].
We may assume that g(0) = h(0) = 0 since the parametrization α can be "ar-
ranged" so that α(0) = (0, 0) ∈V. But then let us examine the possible polynomial
solutions
g(u) = c1u + c2u2 + · · · ,
h(u) = d1u + d2u2 + · · ·
of the equation (g(u))2 = (h(u))5. Since (h(u))5 contains no power of u lower than
u5, the same must be true of (g(u))2. However,
(g(u))2 = c2
1u2 + 2c1c2u3 + (c2
2 + 2c1c3)u4 + 2(c1c4 + c2c3)u5 + · · · .
The coefﬁcient of u2 must be zero, which implies c1 = 0. The coefﬁcient of u4 must
also be zero, which implies c2 = 0 as well. Since c1, c2 = 0, the smallest power of
u that can appear in (g(u))2 is u6, which implies that d1 = 0 also.
It follows that u cannot be in the image of α∗since the image of α∗consists of
polynomials in g(u) and h(u). This is a contradiction since α∗was supposed to be
a ring isomorphism onto R[u]. Thus, our two varieties are not isomorphic. In the
exercises, you will derive more information about R[V] by the method of §3 to yield
another proof that R[V] is not isomorphic to a polynomial ring in one variable.

266
Chapter 5 Polynomial and Rational Functions on a Variety
EXERCISES FOR §4
1. Let C be the twisted cubic curve in k3.
a. Show that C is a subvariety of the surface S = V(xz −y2).
b. Find an ideal J ⊆k[S] such that C = VS(J).
2. Let V ⊆Cn be a nonempty afﬁne variety.
a. Let φ ∈C[V]. Show that VV(φ) = ∅if and only if φ is invertible in C[V] (which
means that there is some ψ ∈C[V] such that φψ = [1] in C[V]).
b. Is the statement of part (a) true if we replace C by R? If so, prove it; if not, give a
counterexample.
3. Prove parts (ii), (iii), and (iv) of Proposition 3.
4. Let V = V(y −xn, z −xm), where m, n are any integers ≥1. Show that V is isomorphic
as a variety to k by constructing explicit inverse polynomial mappings α : k →V and
β : V →k.
5. Show that any surface in k3 with a deﬁning equation of the form x −f (y, z) = 0 or
y −g(x, z) = 0 is isomorphic as a variety to k2.
6. Let V be a variety in kn deﬁned by a single equation of the form xn−f (x1, . . . , xn−1) = 0.
Show that V is isomorphic as a variety to kn−1.
7. In this exercise, we will derive the parametrization (1) for the projected curve π(C) from
Example 7.
a. Show that every hyperbola in R2 whose asymptotes are horizontal and vertical and
which passes through the points (0, 0) and (1, 1) is deﬁned by an equation of the form
xy + tx −(t + 1)y = 0
for some t ∈R.
b. Using a computer algebra system, compute a Gröbner basis for the ideal generated
by the equation of π(C), and the above equation of the hyperbola. Use lex order with
the variables ordered x > y > t.
c. The Gröbner basis will contain one polynomial depending on y, t only. By collecting
powers of y and factoring, show that this polynomial has y = 0 as a double root,
y = 1 as a single root, and one root which depends on t, namely y = −t2+t+1
t(t+2) .
d. Now consider the other elements of the basis and show that for the "movable" root
from part (c) there is a unique corresponding x value given by the ﬁrst equation in (1).
The method sketched in Exercise 7 probably seems exceedingly ad hoc, but it is an ex-
ample of a general pattern that can be developed with some more machinery concerning
algebraic curves. Using the complex projective plane to be introduced in Chapter 8, it can
be shown that π(C) is contained in a projective algebraic curve with three singular points
similar to the one at (0, 0) in the sketch. Using the family of conics passing through all
three singular points and any one additional point, we can give a rational parametrization
for any irreducible quartic curve with three singular points as in this example. However,
one can show that nonsingular quartic curves have no such parametrizations.
8. Let Q1 = V(x2 + y2 + z2 −1), and Q2 = V((x −1/2)2 −3y2 −2z2) in R3.
a. Using the idea of Example 7 and Exercise 5, ﬁnd a surface in the pencil deﬁned by
Q1 and Q2 that is isomorphic as a variety to R2.
b. Describe and/or sketch the intersection curve Q1 ∩Q2.
9. Let α : V →W and β : W →V be inverse polynomial mappings between two
isomorphic varieties V and W. Let U = VV(I) for some ideal I ⊆k[V]. Show that α(U)
is a subvariety of W and explain how to ﬁnd an ideal J ⊆k[W] such that α(U) = VW(J).
10. Let Φ : k[V] →k[W] be a ring homomorphism of coordinate rings which is the identity
on constants. Suppose that V ⊆km with coordinates x1, . . . , xm. If F ∈k[x1, . . . , xm],
then prove that Φ([F]) = F(Φ([x1]), . . . , Φ([xm])). Hint: Express [F] as a k-linear com-
bination of products of the [xi].

§4 The Coordinate Ring of an Afﬁne Variety
267
11. Recall the example following Deﬁnition 2 where V = V(z −x2 −y2) ⊆R3.
a. Show that the subvariety W = {(1, 1, 2)} ⊆V is equal to VV([x−1], [y−1]). Explain
why this implies that ⟨[x −1], [y −1]⟩⊆IV(W).
b. Prove that ⟨[x −1], [y −1]⟩= IV(W). Hint: Show that V is isomorphic to R2 and use
Exercise 9.
12. Let V = V(y2 −3x2z + 2) ⊆R3 and let LA be the linear mapping on R3 deﬁned by the
matrix
A =
⎛
⎝
2
0
1
1
1
0
0
1
1
⎞
⎠.
a. Verify that LA is an isomorphism from R3 to R3.
b. Find the equation of the image of V under LA.
13. In this exercise, we will rotate the twisted cubic in R3.
a. Find the matrix A of the linear mapping on R3 that rotates every point through an
angle of π/6 counterclockwise about the z-axis.
b. What are the equations of the image of the standard twisted cubic curve under the
linear mapping deﬁned by the rotation matrix A?
14. This exercise will outline another proof that V = V(y5 −x2) ⊆R2 is not isomorphic
to R as a variety. This proof will use the algebraic structure of R[V]. We will show that
there is no ring isomorphism from R[V] to R[t]. (Note that R[t] is the coordinate ring
of R.)
a. Using the techniques of §3, explain how each element of R[V] can be uniquely rep-
resented by a polynomial of the form a(y) + b(y)x, where a, b ∈R[y].
b. Express the product (a + bx)(a′ + b′x) in R[V] in the form given in part (a).
c. Aiming for a contradiction, suppose that there were some ring isomorphism Φ :
R[t] →R[V]. Since Φ is assumed to be onto, x = Φ( f (t)) and y = Φ(g(t)) for
some polynomials f , g. Using the unique factorizations of f , g and the product for-
mula from part (b), deduce a contradiction.
15. Let V ⊆R3 be the tangent surface of the twisted cubic curve.
a. Show that the usual parametrization of V sets up a one-to-one correspondence bet-
ween the points of V and the points of R2. Hint: Recall the discussion of V in Chap-
ter 3, §3. In light of part (a), it is natural to ask whether V is isomorphic to R2. We
will show that the answer to this question is no.
b. Show that V is singular at each point on the twisted cubic curve by using the method
of Exercise 12 of Chapter 3, §4. (The tangent surface has what is called a "cuspidal
edge" along this curve.)
c. Show that if α : R2 →V is any polynomial parametrization of V, and α(a, b) is
contained in the twisted cubic itself, then the derivative matrix of α must have rank
strictly less than 2 at (a, b) (in other words, the columns of the derivative matrix
must be linearly dependent there). (Note: α need not be the standard parametrization,
although the statement will be true also for that parametrization.)
d. Now suppose that the polynomial parametrization α has a polynomial inverse map-
ping β : V →R2. Using the chain rule from multivariable calculus, show that part (c)
gives a contradiction if we consider (a, b) such that α(a, b) is on the twisted cubic.
16. Let k be algebraically closed. Prove the Weak Nullstellensatz for k[V], which asserts that
for any ideal J ⊆k[V], VV(J) = ∅if and only if J = k[V]. Also explain how this relates
to Exercise 2 when J = ⟨φ⟩.
17. Here is some practice with isomorphisms.
a. Let f : R →S be a ring isomorphism. Prove that R is an integral domain if and only
if S is an integral domain.
b. Let φ : V →W be an isomorphism of afﬁne varieties. Prove that V is irreducible if
and only if W is irreducible. Hint: Combine part (a) with Theorem 9 of this section
and Proposition 4 of §1.

268
Chapter 5 Polynomial and Rational Functions on a Variety
18. Let A be an invertible n × n matrix with entries in k and consider the map LA : kn →kn
from the discussion following Theorem 9.
a. Prove that L∗
A is the ring homomorphism denoted αA in Exercise 13 of Chapter 4, §3.
b. Explain how the discussion in the text gives a proof of Exercise 19 of §2.
§5
Rational Functions on a Variety
The ring of integers can be embedded in many ﬁelds. The smallest of these is the
ﬁeld of rational numbers Q because Q is formed by constructing fractions m
n , where
m, n ∈Z and n ̸= 0. Nothing more than integers was used. Similarly, the polynomial
ring k[x1, . . . , xn] is included as a subring in the ﬁeld of rational functions
k(x1, . . . , xn) =
' f(x1, . . . , xn)
g(x1, . . . , xn)
 f, g ∈k[x1, . . . , xn], g ̸= 0
(
.
Generalizing these examples, if R is any integral domain, then we can form what
is called the ﬁeld of fractions, or quotient ﬁeld of R, denoted FF(R). The elements
of FF(R) are thought of as "fractions" r/s, where r, s ∈R and s ̸= 0. Two of
these fractions r/s and r′/s′ represent the same element in the ﬁeld of fractions if
rs′ = r′s. We add and multiply elements of FF(R) as we do rational numbers or
rational functions:
r/s + t/u = (ru + ts)/su
and
r/s · t/u = rt/su.
The assumption that R is an integral domain ensures that the denominators of the
sum and product will be nonzero. You will check in Exercise 1 that these operations
are well-deﬁned and that FF(R) satisﬁes all the axioms of a ﬁeld. Furthermore,
FF(R) contains the subset {r/1 | r ∈R}, which is a subring isomorphic to R itself.
Hence, the terminology "ﬁeld of fractions, or quotient ﬁeld of R" is fully justiﬁed.
Now if V ⊆kn is an irreducible variety, then we have seen in §1 that the coor-
dinate ring k[V] is an integral domain. The ﬁeld of fractions FF(k[V]) is given the
following name.
Deﬁnition 1. Let V be an irreducible afﬁne variety in kn. We call FF(k[V]) the func-
tion ﬁeld (or ﬁeld of rational functions) of V, and we denote this ﬁeld by k(V).
Note the consistency of our notation. We use k[x1, . . . , xn] for a polynomial ring
and k[V] for the coordinate ring of V. Similarly, we use k(x1, . . . , xn) for a rational
function ﬁeld and k(V) for the function ﬁeld of V.
We can write the function ﬁeld k(V) of V ⊆kn explicitly as
k(V) = {φ/ψ | φ, ψ ∈k[V], ψ ̸= 0}
= {[ f]/[g] | f, g ∈k[x1, . . . , xn], g /∈I(V)}.

§5 Rational Functions on a Variety
269
As with any rational function, we must be careful to avoid zeros of the denominator
if we want a well-deﬁned function value in k. Thus, an element φ/ψ ∈k(V) deﬁnes
a function only on the complement of VV(ψ).
The most basic example of the function ﬁeld of a variety is given by V = kn. In
this case, we have k[V] = k[x1, . . . , xn] and, hence,
k(V) = k(x1, . . . , xn).
We next consider some more complicated examples.
Example 2. In §4, we showed that the curve
V = V(y5 −x2) ⊆R2
is not isomorphic to R because the coordinate rings of V and R are not isomorphic.
Let us see what we can say about the function ﬁeld of V. To begin, note that by
the method of §3, we can represent the elements of R[V] by remainders modulo
G = {y5 −x2}, which is a Gröbner basis for I(V) with respect to lex order with
x > y in R[x, y]. Then R[V] = {a(y) + xb(y) | a, b ∈R[y]} as a real vector space,
and multiplication is deﬁned by
(1)
(a + xb) · (c + xd) = (ac + y5 · bd) + x(ad + bc).
In Exercise 2, you will show that V is irreducible, so that R[V] is an integral domain.
Now, using this description of R[V], we can also describe the function ﬁeld R(V)
as follows. If c + xd ̸= 0 in R[V], then in the function ﬁeld we can write
a + xb
c + xd = a + xb
c + xd · c −xd
c −xd
= (ac −y5bd) + x(bc −ad)
c2 −y5d2
= ac −y5bd
c2 −y5d2 + x bc −ad
c2 −y5d2 .
This is an element of R(y) + xR(y). Conversely, it is clear that every element of
R(y) + xR(y) deﬁnes an element of R(V). Hence, the ﬁeld R(V) can be identiﬁed
with the set of functions R(y) + xR(y), where the addition and multiplication oper-
ations are deﬁned as before in R[V], only using rational functions of y rather than
polynomials.
Now consider the mappings:
α : V −→R,
(x, y) 
−→x/y2,
β : R −→V,
u 
−→(u5, u2).
Note that α is deﬁned except at (0, 0) ∈V, whereas β is a polynomial parametriza-
tion of V. As in §4, we can use α and β to deﬁne mappings "going in the opposite
direction" on functions. However, since α itself is deﬁned as a rational function, we

270
Chapter 5 Polynomial and Rational Functions on a Variety
will not stay within R[V] if we compose α with a function in R[u]. Hence, we will
consider the maps
α∗: R(u) −→R(V),
f(u) 
−→f(x/y2),
β∗: R(V) −→R(u),
a(y) + xb(y) 
−→a(u2) + u5b(u2).
We claim that α∗and β∗are inverse ring isomorphisms. That α∗and β∗preserve
sums and products follows by the argument given in the proof of Proposition 8 from
§4. To check that α∗and β∗are inverses, ﬁrst we have that for any f(u) ∈R(u),
α∗( f) = f(x/y2). Hence, β∗(α∗( f)) = f(u5/(u2)2) = f(u). Therefore, β∗◦α∗
is the identity on R(u). Similarly, if a(y) + xb(y) ∈R(V), then β∗(a + xb) =
a(u2) + u5b(u2), so
α∗(β∗(a + xb)) = a((x/y2)2) + (x/y2)5b((x/y2)2)
= a(x2/y4) + (x5/y10)b(x2/y4).
However, in R(V), x2 = y5, so x2/y4 = y, and x5/y10 = xy10/y10 = x. Hence,
α∗◦β∗is the identity on R(V). Thus, α∗, β∗deﬁne ring isomorphisms between the
function ﬁelds R(V) and R(u).
Example 2 shows that it is possible for two varieties to have the same (i.e., iso-
morphic) function ﬁelds, even when they are not isomorphic. It also gave us an
example of a rational mapping between two varieties. Before we give a precise def-
inition of a rational mapping, let us look at another example.
Example 3. Let Q = V(x2 + y2 −z2 −1), a hyperboloid of one sheet in R3, and let
W = V(x + 1), the plane x = −1. Let p = (1, 0, 0) ∈Q. For any q ∈Q \ {p}, we
construct the line Lq joining p and q, and we deﬁne a mapping φ to W by setting
φ(q) = Lq ∩W
if the line intersects W. (If the line does not intersect W, then φ(q) is undeﬁned.)
We can ﬁnd an algebraic formula for φ as follows. If q = (x0, y0, z0) ∈Q, then Lq
is given in parametric form by
x = 1 + t(x0 −1),
y = ty0,
(2)
z = tz0.
At φ(q) = Lq ∩W, we must have 1 + t(x0 −1) = −1, so t =
−2
x0−1. From (2), it
follows that
(3)
φ(q) =

−1, −2y0
x0 −1, −2z0
x0 −1

.
This shows that φ is deﬁned on all of Q except for the points on the two lines
VQ(x −1) = Q ∩V(x −1) = {(1, t, t) | t ∈R} ∪{(1, t, −t) | t ∈R}.

§5 Rational Functions on a Variety
271
We will call φ : Q\ VQ(x−1) →W a rational mapping on Q since the components
of φ are rational functions. [We can think of them as elements of R(Q) if we like.]
Going in the other direction, if (−1, a, b) ∈W, then the line L through p =
(1, 0, 0) and (−1, a, b) can be parametrized by
x = 1 −2t,
y = ta,
z = tb,
Computing the intersections with Q, we ﬁnd
L ∩Q =
'
(1, 0, 0),
a2 −b2 −4
a2 −b2 + 4,
4a
a2 −b2 + 4,
4b
a2 −b2 + 4
(
.
Thus, if we let H denote the hyperbola VW(a2−b2+4), then we can deﬁne a second
rational mapping
ψ : W \ H −→Q
by
(4)
ψ(−1, a, b) =
a2 −b2 −4
a2 −b2 + 4,
4a
a2 −b2 + 4,
4b
a2 −b2 + 4

.
From the geometric descriptions of φ and ψ, φ ◦ψ is the identity mapping on the
subset W \ H ⊆W. Similarly, we see that ψ ◦φ is the identity on Q \ VQ(x −1).
Also, using the formulas from equations (3) and (4), it can be checked that φ∗◦ψ∗
and ψ∗◦φ∗are the identity mappings on the function ﬁelds. (We should mention
that as in the second example, Q and W are not isomorphic varieties. However, this
is not an easy fact to prove given what we know.)
We now introduce some general terminology that was implicit in the above
examples.
Deﬁnition 4. Let V ⊆km and W ⊆kn be irreducible afﬁne varieties. A rational
mapping from V to W is a function φ represented by
(5)
φ(x1, . . . , xm) =
 f1(x1, . . . , xm)
g1(x1, . . . , xm), . . . , fn(x1, . . . , xm)
gn(x1, . . . , xm)

,
where fi/gi ∈k(x1, . . . , xm) satisfy:
(i) φ is deﬁned at some point of V.
(ii) For every (a1, . . . , am) ∈V where φ is deﬁned, φ(a1, . . . , am) ∈W.
Note that a rational mapping φ from V to W may fail to be a function from V to
W in the usual sense because, as we have seen in the examples, φ may not be deﬁned
everywhere on V. For this reason, many authors use a special notation to indicate a
rational mapping:
φ : V  W.

272
Chapter 5 Polynomial and Rational Functions on a Variety
We will follow this convention as well. By condition (i), the set of points of V when
the rational mapping φ in (5) is deﬁned includes V \ VV(g1 · · · gn) = V \ (VV(g1) ∪
· · · ∪VV(gn)), where VV(g1 · · · gn) is a proper subvariety of V.
Because rational mappings are not deﬁned everywhere on their domains, we must
exercise some care in studying them. In particular, we will need the following pre-
cise deﬁnition of when two rational mappings are to be considered equal.
Deﬁnition 5. Let φ, ψ : V  W be rational mappings represented by
φ =
 f1
g1
, . . . , fn
gn

and
ψ =
 f ′
1
g′
1
, . . . , f ′
n
g′n

.
Then we say that φ = ψ if for each i, 1 ≤i ≤n,
fig′
i −f ′
i gi ∈I(V).
We have the following geometric criterion for the equality of rational mappings.
Proposition 6. Two rational mappings φ, ψ : V  W are equal if and only if
there is a proper subvariety V′ ⊆V such that φ and ψ are deﬁned on V \ V′ and
φ(p) = ψ(p) for all p ∈V \ V′.
Proof. We will assume that φ = ( f1/g1, . . . , fn/gn) and ψ = (f ′
1/g′
1, . . . , f ′
n/g′
n).
First, suppose that φ and ψ are equal as in Deﬁnition 5 and let V1 = VV(g1 · · · gn)
and V2 = VV(g′
1 · · · g′
n). By hypothesis, V1 and V2 are proper subvarieties of V, and
since V is irreducible, it follows that V′ = V1 ∪V2 is also a proper subvariety of V.
Then φ and ψ are deﬁned on V \ V′, and since fig′
i −f ′
i gi ∈I(V), it follows that fi/gi
and f ′
i /g′
i give the same function on V \ V′. Hence, the same is true for φ and ψ.
Conversely, suppose that φ and ψ are deﬁned and equal (as functions) on V \ V′.
This implies that for each i, we have fi/gi = f ′
i /g′
i on V \V′. Then fig′
i −f ′
i gi vanishes
on V \ V′, which shows that V = V( fig′
i −f ′
i gi) ∪V′. Since V is irreducible and V′
is a proper subvariety, this forces V = V( fig′
i −f ′
i gi). Thus, fig′
i −f ′
i gi ∈I(V), as
desired.
□
As an example, recall from Example 3 that we had rational maps φ : Q  W
and ψ : W  Q such that φ ◦ψ was the identity on W \ H ⊆W. By Proposition 6,
this proves that φ ◦ψ equals the identity map idW in the sense of Deﬁnition 5.
We also need to be careful in dealing with the composition of rational mappings.
Deﬁnition 7. Given φ : V  W and ψ : W  Z, we say that ψ ◦φ is deﬁned if
there is a point p ∈V such that φ is deﬁned at p and ψ is deﬁned at φ(p).
When a composition ψ ◦φ is deﬁned, it gives us a rational mapping as follows.
Proposition 8. Let φ : V  W and ψ : W  Z be rational mappings such that
ψ ◦φ is deﬁned. Then there is a proper subvariety V′ ⊊V such that:
(i) φ is deﬁned on V \ V′ and ψ is deﬁned on φ(V \ V′).
(ii) ψ ◦φ : V  Z is a rational mapping deﬁned on V \ V′.

§5 Rational Functions on a Variety
273
Proof. Suppose that φ and ψ are represented by
φ(x1, . . . , xm) =
 f1(x1, . . . , xm)
g1(x1, . . . , xm), . . . , fn(x1, . . . , xm)
gn(x1, . . . , xm)

.
ψ(y1, . . . , yn) =
 f ′
1(y1, . . . , yn)
g′
1(y1, . . . , yn), . . . , f ′
l (y1, . . . , yn)
g′
l(y1, . . . , yn)

.
Then the j-th component of ψ ◦φ is
f ′
j ( f1/g1, . . . , fn/gn)
g′
j( f1/g1, . . . , fn/gn),
which is clearly a rational function in x1, . . . , xm. To get a quotient of polynomials,
we can write this as
Pj
Qj
= (g1 · · · gn)Mf ′
j ( f1/g1 · · · , fn/gn)
(g1 . . . gn)Mg′
j( f1/g1, · · · , fn/gn),
when M is sufﬁciently large.
Now set
V′ = VV([Q1 · · · Qlg1 · · · gn]) ⊆V.
It should be clear that φ is deﬁned on V \V′ and ψ is deﬁned on φ(V \V′). It remains
to show that V′ ̸= V. But by assumption, there is p ∈V such that φ(p) and ψ(φ(p))
are deﬁned. This means that gi(p) ̸= 0 for 1 ≤i ≤n and
g′
j( f1(p)/g1(p), . . . , fn(p)/gn(p)) ̸= 0
for 1 ≤j ≤l. It follows that Qj(p) ̸= 0 and consequently, p ∈V \ V′.
□
In the exercises, you will work out an example to show how ψ ◦φ can fail to be
deﬁned. Basically, this happens when the domain of deﬁnition of ψ lies outside the
image of φ.
Examples 2 and 3 illustrate the following alternative to the notion of isomorphism
of varieties.
Deﬁnition 9. (i) Two irreducible varieties V ⊆km and W ⊆kn are birationally
equivalent if there exist rational mappings φ : V  W and ψ : W  V such
that φ ◦ψ is deﬁned (as in Deﬁnition 7) and equal to the identity map idW (as in
Deﬁnition 5), and similarly for ψ ◦φ.
(ii) A rational variety is a variety that is birationally equivalent to kn for some n.
Just as isomorphism of varieties can be detected from the coordinate rings, bira-
tional equivalence can be detected from the function ﬁelds.
Theorem 10. Two irreducible varieties V and W are birationally equivalent if and
only if there is an isomorphism of function ﬁelds k(V) ∼= k(W) which is the identity
on k. (By deﬁnition, two ﬁelds are isomorphic if they are isomorphic as commutative
rings.)

274
Chapter 5 Polynomial and Rational Functions on a Variety
Proof. The proof is similar to what we did in Theorem 9 of §4. Suppose ﬁrst that V
and W are birationally equivalent via φ : V  W and ψ : W  V. We will deﬁne
a pullback mapping φ∗: k(W) →k(V) by the rule φ∗( f) = f ◦φ and show that φ∗
is an isomorphism. Unlike the polynomial case, it is not obvious that φ∗( f) = f ◦φ
exists for all f ∈k(W)—we need to prove that f ◦φ is deﬁned at some point of V.
We ﬁrst show that our assumption φ ◦ψ = idW implies the existence of a proper
subvariety W′ ⊆W such that
ψ is deﬁned on W \ W′,
φ is deﬁned on ψ(W \ W′),
(6)
φ ◦ψ is the identity function on W \ W′.
To prove this, we ﬁrst use Proposition 8 to ﬁnd a proper subvariety W1 ⊆W such
that ψ is deﬁned on W\W1 and φ is deﬁned on ψ(W\W1). Also, from Proposition 6,
we get a proper subvariety W2 ⊆W such that φ◦ψ is the identity function on W\W2.
Since W is irreducible, W′ = W1 ∪W2 is a proper subvariety, and it follows easily
that (6) holds for this choice of W′.
Given f ∈k(W), we can now prove that f ◦φ is deﬁned. If f is deﬁned on
W \W′′ ⊆W, then we can pick q ∈W \(W′∪W′′) since W is irreducible. From (6),
we get p = ψ(q) ∈V such that φ(p) is deﬁned, and since φ(p) = q /∈W′′, we also
know that f is deﬁned at φ(p), i.e., f ◦φ is deﬁned at p. By Deﬁnition 4, φ∗( f) = f ◦φ
exists as an element of k(V).
This proves that we have a map φ∗: k(W) →k(V), and φ∗is a ring homomor-
phism by the proof of Proposition 8 from §4. Similarly, we get a ring homomor-
phism ψ∗: k(V) →k(W). To show that these maps are inverses of each other, let
us look at
(ψ∗◦φ∗)( f) = f ◦φ ◦ψ
for f ∈k(W). Using the above notation, we see that f ◦φ ◦ψ equals f as a function
on W \ (W′ ∪W′′), so that f ◦φ ◦ψ = f in k(W) by Proposition 6. This shows that
ψ∗◦φ∗is the identity on k(W), and a similar argument shows that φ∗◦ψ∗= idk(V).
Thus, φ∗: k(W) →k(V) is an isomorphism of ﬁelds. We leave it to the reader to
show that φ∗is the identity on the constant functions k ⊆k(W).
The proof of the converse implication is left as an exercise for the reader. Once
again the idea is basically the same as in the proof of Theorem 9 of §4.
□
In the exercises, you will prove that two irreducible varieties are birationally
equivalent if there are "big" subsets (complements of proper subvarieties) that can
be put in one-to-one correspondence by rational mappings. For example, the curve
V = V(y5 −x2) from Example 2 is birationally equivalent to W = R. You should
check that V \ {(0, 0)} and W \ {0} are in a one-to-one correspondence via the
rational mappings f and g from equation (1). The birational equivalence between
the hyperboloid and the plane in Example 3 works similarly. This example also
shows that outside of the "big" subsets, birationally equivalent varieties may be
quite different (you will check this in Exercise 14).

§5 Rational Functions on a Variety
275
As we see from these examples, birational equivalence of irreducible varieties is
a weaker equivalence relation than isomorphism. By this we mean that the set of
varieties birationally equivalent to a given variety will contain many different non-
isomorphic varieties. Nevertheless, in the history of algebraic geometry, the clas-
siﬁcation of varieties up to birational equivalence has received more attention than
classiﬁcation up to isomorphism, perhaps because constructing rational functions
on a variety is easier than constructing polynomial functions. There are reasonably
complete classiﬁcations of irreducible varieties of dimensions 1 and 2 up to bira-
tional equivalence, and, recently, signiﬁcant progress has been made in dimension
≥3 with the so-called minimal model program. The birational classiﬁcation of va-
rieties remains an area of active research in algebraic geometry.
EXERCISES FOR §5
1. Let R be an integral domain, and let FF(R) be the ﬁeld of fractions of R as described in
the text.
a. Show that addition is well-deﬁned in FF(R). This means that if r/s = r′/s′ and t/u =
t′/u′, then you must show that (ru + ts)/su = (r′u′ + t′s′)/s′u′. Hint: Remember
what it means for two elements of FF(R) to be equal.
b. Show that multiplication is well-deﬁned in FF(R).
c. Show that the ﬁeld axioms are satisﬁed for FF(R).
2. As in Example 2, let V = V(y5 −x2) ⊆R2.
a. Show that y5 −x2 is irreducible in R[x, y] and prove that I(V) = ⟨y5 −x2⟩.
b. Conclude that R[V] is an integral domain.
3. Show that the singular cubic curve V(y2−x3) is a rational variety (birationally equivalent
to k) by adapting what we did in Example 2.
4. Consider the singular cubic curve Vc = V(y2 −cx2 + x3) studied in Exercise 8 of
Chapter 1, §3. Using the parametrization given there, prove that Vc is a rational variety
and ﬁnd subvarieties V′
c ⊆Vc and W ⊆R such that your rational mappings deﬁne
a one-to-one correspondence between Vc \ V′
c and R \ W. Hint: Recall that t in the
parametrization of Vc is the slope of a line passing through (0, 0).
5. Verify that the curve π(C) from Exercise 7 of §4 is a rational variety. Hint: To deﬁne a
rational inverse of the parametrization we derived in that exercise, you need to solve for
t as a function of x and y on the curve. The equation of the hyperbola may be useful.
6. In Example 3, verify directly that (3) and (4) deﬁne inverse rational mappings from the
hyperboloid of the one sheet to the plane.
7. Let S = V(x2 + y2 + z2 −1) in R3 and let W = V(z) be the (x, y)-plane. In this exercise,
we will show that S and W are birationally equivalent varieties, via an explicit mapping
called stereographic projection. See also Exercise 6 of Chapter 1, §3.
a. Derive parametric equations as in (2) for the line Lq in R3 passing through the north
pole (0, 0, 1) of S and a general point q = (x0, y0, z0) ̸= (0, 0, 1) in S.
b. Using the line from part (a) show that φ(q) = Lq ∩W deﬁnes a rational mapping
φ : S  R2. This mapping is the stereographic projection mentioned above.
c. Show that the rational parametrization of S given in Exercise 6 of Chapter 1, §3 is the
inverse mapping of φ.
d. Deduce that S and W are birationally equivalent varieties and ﬁnd subvarieties S′ ⊆S
and W′ ⊆W such that φ and ψ put S\S′ and W \W′ into one-to-one correspondence.
8. In Exercise 10 of §1, you showed that there were no nonconstant polynomial mappings
from R to V = V(y2 −x3 + x). In this problem, you will show that there are no noncon-
stant rational mappings either, so V is not birationally equivalent to R. In the process, we

276
Chapter 5 Polynomial and Rational Functions on a Variety
will need to consider polynomials with complex coefﬁcients, so the proof will actually
show that V(y2 −x3 + x) ⊆C2 is not birationally equivalent to C either. The proof will
be by contradiction.
a. Start by assuming that α : R  V is a nonconstant rational mapping deﬁned
by α(t) = (a(t)/b(t), c(t)/d(t)) with a and b relatively prime, c and d relatively
prime, and b, d monic. By substituting into the equation of V, show that b3 = d2 and
c2 = a3 −ab2.
b. Deduce that a, b, a+b, and a−b are all squares of polynomials in C[t]. In other words,
show that a = A2, b = B2, a + b = C2 and a −b = D2 for some A, B, C, D ∈C[t].
c. Show that the polynomials A, B ∈C[t] from part (b) are nonconstant and relatively
prime and that A4 −B4 is the square of a polynomial in C[t].
d. The key step of the proof is to show that such polynomials cannot exist using inﬁnite
descent. Suppose that A, B ∈C[t] satisfy the conclusions of part (c). Prove that there
are polynomials A1, B1, C1 ∈C[t] such that
A −B = A2
1
A + B = B2
1
A2 + B2 = C2
1.
e. Prove that the polynomials A1, B1 from part (d) are relatively prime and nonconstant
and that their degrees satisfy
max(deg(A1), deg(B1)) ≤1
2 max(deg(A), deg(B)).
Also show that A4
1 −(
√
i B1)4 = A4
1 + B4
1 is the square of a polynomial in C[t].
Conclude that A1,
√
i B1 satisfy the conclusions of part (c).
f. Conclude that if such a pair A, B exists, then one can repeat parts (d) and (e) inﬁnitely
many times with decreasing degrees at each step (this is the "inﬁnite descent"). Ex-
plain why this is impossible and conclude that our original polynomials a, b, c, d must
be constant.
9. Let V be an irreducible variety and let f ∈k(V). If we write f = φ/ψ, where φ, ψ ∈
k[V], then we know that f is deﬁned on V \ VV(ψ). What is interesting is that f might
make sense on a larger set. In this exercise, we will work out how this can happen on the
variety V = V(xz −yw) ⊆C4.
a. Prove that xz −yw ∈C[x, y, z, w] is irreducible. Hint: Look at the total degrees of its
factors.
b. Use unique factorization in C[x, y, z, w] to prove that ⟨xz −yw⟩is a prime ideal.
c. Conclude that V is irreducible and that I(V) = ⟨xz −yw⟩.
d. Let f = [x]/[y] ∈C(V) so that f is deﬁned on V \ VV([y]). Show that VV([y]) is the
union of planes {(0, 0, z, w) | z, w ∈C} ∪{(x, 0, 0, w) | x, w ∈C}.
e. Show that f = [w]/[z] and conclude that f is deﬁned everywhere outside of the plane
{(x, 0, 0, w) | x, w ∈C}.
Note that what made this possible was that we had two fundamentally different ways of
representing the rational function f . This is part of why rational functions are subtle to
deal with.
10. Consider the rational mappings φ : R  R3 and ψ : R3  R deﬁned by
φ(t) = (t, 1/t, t2)
and
ψ(x, y, z) = x + yz
x −yz.
Show that ψ ◦φ is not deﬁned.

§6 Relative Finiteness and Noether Normalization
277
11. Complete the proof of Theorem 10 by showing that if V and W are irreducible varieties
and k(V) ∼= k(W) is an isomorphism of their function ﬁelds which is the identity on
constants, then there are inverse rational mappings φ : V  W and ψ : W  V. Hint:
Follow the proof of Theorem 9 from §4.
12. Suppose that φ : V  W is a rational mapping deﬁned on V \ V′. If W′ ⊆W is a
subvariety, then prove that
V′′ = V′ ∪{p ∈V \ V′ | φ(p) ∈W′}
is a subvariety of V. Hint: Find equations for V′′ by substituting the rational functions
representing φ into the equations for W′ and setting the numerators of the resulting
functions equal to zero.
13. Suppose that V and W are birationally equivalent varieties via φ : V  W and ψ :
W  V. As mentioned in the text after the proof of Theorem 10, this means that V and
W have "big" subsets that are the same. More precisely, there are proper subvarieties
V1 ⊆V and W1 ⊆W such that φ and ψ induce inverse bijections between subsets V \V1
and W \ W1. Note that Exercises 4 and 7 involved special cases of this result.
a. Let V′ ⊆V be the subvariety that satisﬁes the properties given in (6) for φ ◦ψ.
Similarly, we get W′ ⊆W that satisﬁes the analogous properties for ψ ◦φ. Let
V = {p ∈V \ V′ | φ(p) ∈W \ W′},
W = {q ∈W \ W′ | ψ(q) ∈V \ V′}.
Show that we have bijections φ : V →W and ψ : W →V which are inverses of
each other.
b. Use Exercise 12 to prove that V = V \ V1 and W = W \ W1 for proper subvarieties
V1 and W1.
Parts (a) and (b) give the desired one-to-one correspondence between "big" subsets of V
and W.
14. In Example 3, we had rational mappings φ : Q  W and ψ : W  Q.
a. Show that φ and ψ induce inverse bijections φ : Q \ VQ(x −1) →W \ H and
ψ : W \ H →Q \ VQ(x −1), where H = VW(a2 −b2 + 4).
b. Show that H and VQ(x −1) are very different varieties that are neither isomorphic
nor birationally equivalent.
§6
Relative Finiteness and Noether Normalization
A major theme of this book is the relation between geometric objects, speciﬁcally
afﬁne varieties, and algebraic objects, notably ideals in polynomial rings. In this
chapter, we learned about other algebraic objects, namely the coordinate ring k[V]
of a variety V and the quotient ring k[x1, . . . , xn]/I of an ideal I ⊆k[x1, . . . , xn]. But
the rings k[V] and k[x1, . . . , xn]/I have an additional structure that deserves explicit
recognition.
Deﬁnition 1. A k-algebra is a ring which contains the ﬁeld k as a subring. Also:
(i) A k-algebra is ﬁnitely generated if it contains ﬁnitely many elements such that
every element can be expressed as a polynomial (with coefﬁcients in k) in these
ﬁnitely many elements.

278
Chapter 5 Polynomial and Rational Functions on a Variety
(ii) A homomorphism of k-algebras is a ring homomorphism which is the identity
on elements of k.
In addition to being a ring, a k-algebra is a vector space over k (usually inﬁnite
dimensional) where addition is deﬁned by the addition in the ring, and scalar multi-
plication is just multiplication by elements of the subring k. Examples of k-algebras
include the coordinate ring of a nonempty variety or the quotient ring of a proper
polynomial ideal. We need to assume V ̸= ∅in order to guarantee that k ⊆k[V],
and similarly I ⊊k[x1, . . . , xn] ensures that k[x1, . . . , xn]/I contains a copy of k.
The k-algebra k[x1, . . . , xn]/I is ﬁnitely generated, and every ﬁnitely generated
k-algebra is isomorphic (as a k-algebra) to the quotient of a polynomial ring by
an ideal (see Exercise 1). We can also characterize which k-algebras correspond
to coordinate rings of varieties. A k-algebra is said to be reduced if it contains no
nilpotent elements (i.e., no nonzero element r such that rm = 0 for some integer
m > 0). The coordinate ring of a variety is reduced, because the ideal of a variety
is radical (see Exercise 18 of §2). In the exercises, you will prove that when k is
algebraically closed, every reduced, ﬁnitely generated k-algebra is isomorphic to
the coordinate ring of a variety.
One advantage of k-algebras over ideals is that two varieties are isomorphic if
and only if their coordinate rings are isomorphic as k-algebras (Theorem 9 of §4).
When we study afﬁne varieties up to isomorphism, it no longer makes sense to talk
about the ideal of a variety, since isomorphic varieties may live in afﬁne spaces
of different dimensions and hence have ideals in polynomial rings with different
numbers of variables. Yet the coordinate rings of these varieties are essentially the
same, since they are isomorphic as k-algebras.
In this section, we will explore two related aspects of the structure of ﬁnitely
generated k-algebras.
Relative Finiteness
Given an ideal I ⊆k[x1, . . . , xn], the Finiteness Theorem (Theorem 6 of §3) char-
acterizes when the k-algebra k[x1, . . . , xn]/I has ﬁnite dimension as a vector space
over k. To generalize this result, suppose that we allow the ideal to depend on a set
of parameters y1, . . . , ym. Thus, we consider an ideal
I ⊆k[x1, . . . , xn, y1, . . . , ym],
and we assume that I ∩k[y1, . . . , ym] = {0} since we do not the ideal to impose
any restrictions on the parameters. One consequence of this assumption is that the
natural map
k[y1, . . . , ym] −→k[x1, . . . , xn, y1, . . . , ym]/I
is easily seen to be one-to-one. This allows us to regard k[y1, . . . , ym] as a subring of
the quotient ring k[x1, . . . , xn, y1, . . . , ym]/I.

§6 Relative Finiteness and Noether Normalization
279
To adapt the Finiteness Theorem to this situation, we need to know what it means
for k[x1, . . . , xn, y1, . . . , ym]/I to be "ﬁnite" over the subring k[y1, . . . , ym]. We have
the following general notion of ﬁniteness for a subring of a commutative ring.
Deﬁnition 2. Given a commutative ring S and a subring R ⊆S, we say that S is
ﬁnite over R if there are ﬁnitely many elements s1, . . . , sℓ∈S such that every s ∈S
can be written in the form
s = a1s1 + · · · + aℓsℓ,
a1, . . . , aℓ∈R.
For example, consider k[x, y]/⟨x2 −y2⟩. It is easy to see that k[y] ∩⟨x2 −y2⟩=
{0}, so that we can consider k[y] as a subring of k[x, y]/⟨x2 −y2⟩. Moreover,
k[x, y]/⟨x2 −y2⟩is ﬁnite over k[y] because every element can be expressed as a
k[y]-linear combination of the images of 1 and x in k[x, y]/⟨x2 −y2⟩.
On the other hand, if we consider the quotient ring k[x, y]/⟨xy⟩, we again
have k[y] ∩⟨xy⟩= {0}, so that we can again consider k[y] as a subring of
k[x, y]/⟨xy⟩. However, k[x, y]/⟨xy⟩is not ﬁnite over k[y] because none of the im-
ages of 1, x, x2, x3, . . . can be expressed as a k[y]-linear combination of the others.
Similarly, in the exercises we ask you to show that k[x, y]/⟨xy −1⟩is not ﬁnite over
the subring k[y].
Here is an important consequence of ﬁniteness.
Proposition 3. Assume that S is ﬁnite over R. Then every s ∈S satisﬁes an equation
of the form
(1)
sℓ+ a1sℓ−1 + · · · + aℓ= 0,
a1, . . . , aℓ∈R.
Proof. By assumption, there exist s1, . . . sℓ∈S such that every s ∈S can be written
in the form
s = a1s1 + · · · + aℓsℓ,
ai ∈R.
Now ﬁx an element s ∈S. Thinking of the set s1, . . . , sℓas akin to a basis, we
can deﬁne an ℓ× ℓmatrix A = (aij) with entries in the ring R that represents
multiplication by s on s1, . . . , sℓ:
s · si = ai1s1 + · · · + aiℓsiℓ,
1 ≤i ≤ℓ, aij ∈R.
If we write v for the transpose (s1, . . . , sℓ)t, then A has the property that Av = sv.
The characteristic polynomial of A is det(A −xIℓ), where Iℓis the ℓ× ℓidentity
matrix. The coefﬁcient of x is (−1)ℓ, which allows us to write
det(A −xIℓ) = (−1)ℓ(xℓ+ a1xℓ−1 + · · · + aℓ).
Since A has entries in R and the determinant of a matrix is a polynomial in its entries
(see §4 of Appendix A), it follows that ai ∈R for all i. Hence the proposition will
follow once we prove that det(A −sIℓ) = 0. If we were doing linear algebra over a
ﬁeld, then this would follow immediately since Av = sv would imply that s was an
eigenvalue of A.

280
Chapter 5 Polynomial and Rational Functions on a Variety
However, standard linear algebra does not apply here since we are working with
rings R ⊆S. Fortunately, the argument is not difﬁcult. Let B = A −sIℓand let C
be the transpose of the matrix of cofactors of B as deﬁned in Appendix A, §4. The
formula given there shows that C has entries in R and satisﬁes CB = det(B)Iℓ.
The equation Av = sv implies that Bv = (A −sIℓ)v = 0, where 0 is the ℓ× 1
column vector with all entries 0. Then
det(B)v = (det(B)Iℓ)v = (CB)v = C(Bv) = C0 = 0.
Thus det(B)si = 0 for all 1 ≤i ≤ℓ. Every element of S, in particular 1 ∈S, is
a linear combination of the si with coefﬁcients in R. Thus 1 = b1s1 + · · · + bℓsℓ,
bi ∈R. Multiplying this equation by det(B), we obtain det(B) = det(B) · 1 = 0, and
the proposition follows.
□
The proof above is often called the "determinant trick." It is a good example of
an elementary proof that is not obvious.
In general, an element s ∈S is integral over a subring R if s satisﬁes an equation
of the form (1). Thus Proposition 3 can be restated as saying that if S is ﬁnite over
R, then every element of S is integral over R.
We can now state the relative version of the Finiteness Theorem from §3.
Theorem 4 (Relative Finiteness Theorem). Let I ⊆k[x1, . . . , xn, y1, . . . , ym] be
an ideal such that I ∩k[y1, . . . , ym] = {0} and ﬁx an ordering of n-elimination type
(as in Exercise 5 of Chapter 3, §1). Then the following statements are equivalent:
(i) For each i, 1 ≤i ≤n, there is some mi ≥0 such that xmi
i
∈⟨LT(I)⟩.
(ii) Let G be a Gröbner basis for I. Then for each i, 1 ≤i ≤n, there is some mi ≥0
such that xmi
i
= LM(g) for some g ∈G.
(iii) The set {xα | there is β ∈Zm
≥0 such that xαyβ /∈⟨LT(I)⟩} is ﬁnite.
(iv) The ring k[x1, . . . , xn, y1, . . . , ym]/I is ﬁnite over the subring k[y1, . . . , ym].
Proof. (i) ⇔(ii) The proof is identical to the proof of (i) ⇔(ii) in Theorem 6 of §3.
(ii) ⇒(iii) If some power xmi
i
∈⟨LT(I)⟩for i = 1, . . . , n, then any monomial
xαyβ = xα1
1 · · · xαn
n yβ for which some αi ≥mi is in ⟨LT(I)⟩. Hence a monomial in
the complement of ⟨LT(I)⟩must have αi ≤mi −1 for all 1 ≤i ≤n. As a result,
there are at most m1 · m2 · · · mn monomials xα such that xαyβ /∈⟨LT(I)⟩.
(iii) ⇒(iv) Take f ∈k[x1, . . . , xn, y1, . . . , ym] and divide f by G using the division
algorithm. This allows us to write f in the form f = g + r, where g ∈I and r is
a linear combination of monomials xαyβ /∈⟨LT(I)⟩. By assumption, only ﬁnitely
many different xα's appear in these monomials, say xα1, . . . , xαℓ. By collecting the
terms of r that share the same xαj, we can write f in the form
f = g + B1xα1 + · · · + Bℓxαℓ,
Bj ∈k[y1, . . . , ym].
Let [ f] denote the equivalence class of f in the quotient ring k[x1, . . . , xn, y1, . . .
, ym]/I. Since g ∈I, the above equation implies that
[ f] = B1[xα1] + · · · + Bℓ[xαℓ]

§6 Relative Finiteness and Noether Normalization
281
in k[x1, . . . , xn, y1, . . . , ym]/I. Thus [xα1], . . . , [xαℓ] satisfy Deﬁnition 2, which proves
that k[x1, . . . , xn, y1, . . . , ym]/I is ﬁnite over the subring k[y1, . . . , ym].
(iv) ⇒(i) Fix i with 1 ≤i ≤n. By (iv) and Proposition 3, there is an equation
[xi]d + A1[xi]d−1 + · · · + Ad = 0,
Aj ∈k[y1, . . . , ym],
in k[x1, . . . , xn, y1, . . . , ym]/I. Back in k[x1, . . . , xn, y1, . . . , ym], this means that
xd
i + A1xd−1
i
+ · · · + Ad ∈I.
Since we are using an n-elimination order, xi is greater than any monomial in
y1, . . . , ym. Since Aj ∈k[y1, . . . , ym], this implies xd
i > LT(Ajxd−j
i
) for j = 1, . . . , d.
It follows that xd
i = LT(xd
i + A1xd−1
i
+ · · · + Ad) ∈⟨LT(I)⟩, and we are done.
□
The Finiteness Theorem from §3 has parts (i)-(v), where (i)-(iv) were equiva-
lent over any ﬁeld k. Do you see how the Relative Finiteness Theorem proved here
generalizes (i)-(iv) of the Finiteness Theorem?
The ﬁnal part (v) of the Finiteness Theorem states that V(I) is ﬁnite and is equiv-
alent to the other parts of the theorem when k is algebraically closed. Hence, it is
natural to ask if the Relative Finiteness Theorem has a similar geometric meaning
in the algebraically closed case.
We begin with the geometric meaning of an ideal I ⊆k[x1, . . . , xn, y1, . . . , ym].
The inclusion of k-algebras k[y1, . . . , ym] ⊆k[x1, . . . , xn, y1, . . . , ym] corresponds to
the projection kn+m →km that sends a point (a, b) = (a1, . . . , an, b1, . . . , bm) in
kn+m to its last m coordinates b ∈km. The ideal I gives a variety V(I) ⊆kn+m.
Composing this inclusion with the projection gives a function
(2)
π : V(I) −→km.
Then a point b = (b1, . . . , bm) ∈km gives two objects:
• (Algebraic) The ideal Ib ⊆k[x1, . . . , xn], which is obtained by setting yi = bi in
all elements of the ideal I.
• (Geometric) The ﬁber π−1(b) = V(I) ∩(kn × {b}), which consists of all points
of V(I) whose last m coordinates are given by b.
The relation between these (which you will establish in the exercises) is that
π−1(b) = V(Ib) × {b}.
Earlier, we said that I ⊆k[x1, . . . , xn, y1, . . . , ym] is an ideal depending on param-
eters y1, . . . , ym. We now see what this means: I gives the family of ideals Ib ⊆
k[x1, . . . , xn], parametrized by b ∈km, with corresponding varieties V(Ib) ⊆kn.
Combining these for all possible parameter values gives the disjoint union
(3)

b∈km
V(Ib) × {b} =

b∈km
π−1(b) = V(I) ⊆kn+m.
This explains nicely how V(I) relates to the family of ideals {Ib}b∈km.

282
Chapter 5 Polynomial and Rational Functions on a Variety
For an example of this, let us return to our example k[y] ⊆k[x, y]/⟨x2 −y2⟩,
which corresponds to the projection V(x2 −y2) →k onto the second coordinate. If
we set k = R and pick b ∈R, then for I = ⟨x2 −y2⟩⊆R[x, y], we get the picture:
y
x
y
π
↓
↓
b
V(Ib)×{b}
←
←
V(I)
←
←
The y-axis is shown horizontally since we want to write the projection π vertically.
Note that V(Ib) = V(x2 −b2) = {±b}. Also, the union of the ﬁbers π−1(b) =
V(Ib) × {b} is V(I) ⊆R2, as predicted by (3).
The next step is to interpret our assumption that I ∩k[y1, . . . , ym] = {0}. The
intersection is the n-th elimination ideal In of I from Chapter 3. When k is alge-
braically closed, the Closure Theorem (Theorem 3 of Chapter 3, §2) tells us that
V(In) ⊆km the Zariski closure of the image of the variety V(I) under the projection
π. Thus, to say that I ∩k[y1, . . . , ym] = {0} is to say that the Zariski closure of
the image π(V(I)) is equal to the entire afﬁne space km. The Closure Theorem also
implies that there is a proper variety W ⊊km such that
km \ W ⊆π(V(I)).
It follows that for "most" b ∈km (i.e., outside of W), the ﬁber π−1(b) = V(Ib)×{b}
is nonempty. Thus, I ∩k[y1, . . . , ym] = {0} means V(Ib) ̸= ∅for most b ∈km.
We now offer the geometric interpretation of the Relative Finiteness Theorem.
Theorem 5 (Geometric Relative Finiteness Theorem).
Let k be algebraically
closed and I ⊆k[x1, . . . , xn, y1, . . . , ym] be an ideal such that I∩k[y1, . . . , ym] = {0}.
If, in addition, k[x1, . . . , xn, y1, . . . , ym]/I is ﬁnite over k[y], then:
(i) The projection map π : V(I) →km is onto and has ﬁnite ﬁbers.
(ii) For each b ∈km, the variety V(Ib) ⊆kn is ﬁnite and nonempty.
Proof. Note that (i) can be restated as saying that π−1(b) is ﬁnite and nonempty for
all b ∈km. Since π−1(b) = V(Ib) × {b}, it follows that (i) and (ii) are equivalent.
We will prove the theorem by showing that V(Ib) is ﬁnite and π−1(b) is nonempty.

§6 Relative Finiteness and Noether Normalization
283
Set x = (x1, . . . , xn) and y = (y1, . . . , ym), and let G = {g1, . . . , gt} be a reduced
Gröbner basis for I ⊆k[x, y] for lex order x1 > . . . > xn > y1 > . . . > ym. Since
G is reduced, the Relative Finiteness Theorem tells us that for each 1 ≤i ≤n,
there exists g ∈G such that LT(g) = xNi
i . Given b ∈km, we have g(x, b) ∈Ib, and
the lex order we are using implies that LT(g(x, b)) = xNi
i
in k[x] for lex order with
x1 > · · · > xn. Since this holds for all 1 ≤i ≤n, the Finiteness Theorem from §3
implies that the variety V(Ib) is ﬁnite.
It remains to show that the ﬁbers of π : V(I) →km are nonempty. First observe
that the n-th elimination ideal of I is In = I∩k[y]. By assumption, this equals {0}, so
that V(In) = V(0) = km. Thus, every b ∈km is a partial solution. Since π−1(b) ̸= ∅
if and only if there exists a ∈kn such that (a, b) ∈V(I), we need only show that
the partial solution b ∈km extends to a solution (a, b) ∈V(I) ⊆kn+m. We do
this by applying the Extension Theorem (Theorem 3 of Chapter 3, §1) n times. To
successively eliminate x1, . . . , xn, we use the elimination ideals
I0 = I ∩k[x1, . . . , xn, y1, . . . , ym] = I ∩k[x, y] = I
I1 = I ∩k[x2 . . . , xn, y]
...
In−1 = I ∩k[xn, y]
In = I ∩k[y] = {0}.
By the Elimination Theorem (Theorem 2 of Chapter 3, §1), the intersection
Gi = G ∩k[xi+1, . . . , xn, y] is a Gröbner basis for Ii for 1 ≤i ≤n −1. Note
also that Ii is the ﬁrst elimination ideal of Ii−1 for 1 ≤i ≤n −1. As in the previ-
ous paragraph, we have g ∈G with LT(g) = xNi
i . The lex order we are using then
implies g ∈G ∩k[xi, . . . , xn, y] = Gi−1. Now write g as
g = xNi
i
+ terms in which xi has degree < Ni.
Since k is algebraically closed, the version of the Extension Theorem given in Corol-
lary 4 of Chapter 3, §1 tells us that any partial solution (ai+1, . . . , an, b) ∈V(Ij)
extends to a solution (ai, ai+1, . . . , an, b) ∈V(Ii−1) for each 1 ≤i ≤n. So, now we
are done: start with any b ∈V(In) = km and apply Corollary 4 successively with
i = n, n −1, . . . , 1 to ﬁnd (a, b) = (a1, . . . , an, b) ∈V(I) ⊆kn+m.
□
We remark that the converse of the Geometric Relative Finiteness Theorem is not
true. There are ideals I ⊆k[x, y] for which the projection of V(I) →km is onto with
ﬁnite ﬁbers, but k[x, y]/I is not ﬁnite over k[y]. In the exercises, you will show that
the ideal ⟨x(xy −1)⟩⊆k[x, y] provides an example.
Noether Normalization
When I ⊆k[x, y] satisﬁes the Relative Finiteness Theorem, the k-algebra A =
k[x, y]/I is ﬁnite over the polynomial ring k[y] ⊆A. This seems like a very special

284
Chapter 5 Polynomial and Rational Functions on a Variety
situation. The surprise that when k is inﬁnite, any ﬁnitely generated k-algebra is
ﬁnite over a polynomial subring. This is the Noether Normalization Theorem.
Before stating the theorem precisely, we need one bit of terminology: elements
u1, . . . , um in a k-algebra A are algebraically independent over k when the only
polynomial f with coefﬁcients in k satisfying f(u1, . . . , um) = 0 in A is the zero
polynomial. When this condition is satisﬁed, the subring k[u1, . . . , um] ⊆A is iso-
morphic to a polynomial ring in m variables.
Theorem 6 (Noether Normalization). Let k be an inﬁnite ﬁeld and suppose that
we are given a ﬁnitely generated k-algebra A. Then:
(i) There are algebraically independently elements u1, . . . , um ∈A such that A is
ﬁnite over k[u1, . . . , um].
(ii) If A is generated by s1, . . . , sℓas a k-algebra, then m ≤ℓand u1, . . . , um can be
chosen to be k-linear combinations of s1, . . . , sℓ.
The proof will involve the following ﬁniteness result.
Lemma 7. Let R ⊆S be rings, and assume that there is s ∈S such that S = R[s],
meaning that every element of S can be written as a polynomial in s with coefﬁcients
in R. If, in addition, s satisﬁes an equation
sd + r1sd−1 + · · · + rd = 0
with r1, . . . , rd ∈R, then S is ﬁnite over R.
Proof. Let R[x] be the ring of polynomials in x with coefﬁcients in R. Also set
f = xd +r1xd−1 +· · ·+rd ∈R[x]. By hypothesis, any element of S can be written as
g(s) for some g ∈R[x]. Dividing g by f gives a, b ∈R[x] such that g = af + b and
either deg(b) < d or b = 0. The division algorithm presented in §5 of Chapter 1
requires ﬁeld coefﬁcients, but for more general ring coefﬁcients, the algorithm still
works, provided one divides by a monic polynomial such as f. You will check this
in the exercises.
Since f(s) = 0, we obtain g(s) = a(s)f(s) + b(s) = a(s) · 0 + b(s) = b(s).
Since deg(b) < d, we see that any element of S can be expressed as an R-linear
combination of 1, s,..., sd−1. Hence S is ﬁnite over R.
□
We are now ready to prove the Noether Normalization Theorem.
Proof of Theorem 6. We proceed by induction on the number ℓof generators. First
suppose that ℓ= 1. Then A is a k-algebra with a single generator s1, so A = k[s1].
There are two cases to consider. If there is no nonzero polynomial f with f(s1) = 0,
then s1 is algebraically independent over k. So the theorem holds with m = 1 and
u1 = s1 since A is ﬁnite over k[s1] = A. It remains to consider the case when there
is a nonzero polynomial f ∈k[x] with f(s1) = 0. We may assume that f is monic.
Since f has coefﬁcients in k, Lemma 7 implies A is ﬁnite over the subring k. So the
theorem holds in this case with m = 0.
Now, let ℓ> 1 and suppose that the theorem is true for ﬁnitely generated
k-algebras with fewer than ℓgenerators. If there is no nonzero polynomial f in ℓ

§6 Relative Finiteness and Noether Normalization
285
variables such that f(s1, . . . , sℓ) = 0, then s1, . . . , sℓare algebraically independent
and the theorem is true with m = ℓand ui = si for 1 ≤i ≤ℓsince A is trivially
ﬁnite over k[s1, . . . , sℓ] = A.
Otherwise, choose f nonzero with f(s1, . . . , sℓ) = 0. Let ˜s1 = s1 −a1sℓ, ˜s2 =
s2 −a2sℓ, . . . ,˜sℓ−1 = sℓ−1 −aℓ−1sℓfor some a1, . . . , aℓ−1 ∈k that we shall choose
momentarily. Thus,
(4)
s1 = ˜s1 + a1sℓ, s2 = ˜s2 + a2sℓ, . . . , sℓ−1 = ˜sℓ−1 + aℓ−1sℓ.
Replacing si with ˜si + aisℓfor 1 ≤i ≤ℓ−1 in f and expanding, we obtain
(5)
0 = f(s1, . . . , sℓ) = f(˜s1 + a1sℓ, . . . ,˜sℓ−1 + aℓ−1sℓ, sℓ)
= c(a1, . . . , aℓ−1)sd
ℓ+ terms in which sℓhas degree < d,
where d is the total degree of f. We will leave it as an exercise for the reader to show
that c(a1, . . . , aℓ−1) is a nonzero polynomial expression in a1, . . . , aℓ−1. Since the
ﬁeld k is inﬁnite, we can choose a1, . . . , aℓ−1 ∈k with c(a1, . . . , aℓ−1) ̸= 0 by
Proposition 5 of Chapter 1, §1.
For this choice of a1, . . . , aℓ−1, let B = k[˜s1, . . . ,˜sℓ−1] ⊆A be the k-algebra
generated by ˜s1, . . . ,˜sℓ−1. We prove that A is ﬁnite over B as follows. Dividing each
side of (5) by c(a1, . . . , aℓ−1) ̸= 0 gives an equation
0 = sd
ℓ+ b1sd−1
ℓ
+ · · · + bd,
where b1, . . . , bd ∈B. Since A is generated as k-algebra by s1, . . . , sℓ, (4) implies
that the same is true for˜s1, . . . ,˜sℓ−1, sℓ. It follows that A = B[sℓ]. This and the above
equation imply that A is ﬁnite over B by Lemma 7.
Our inductive hypothesis applies to B since it has ℓ−1 algebra generators. Hence
there are m ≤ℓ−1 algebraically independent elements u1, . . . , um ∈B such that
B is ﬁnite over k[u1, . . . , um]. Since A is ﬁnite over B and "being ﬁnite" is transitive
(see Exercise 14), it follows that A is ﬁnite over k[u1, . . . , um]. Furthermore, since we
may assume that u1, . . . , um are k-linear combinations of ˜s1 = s1 −a1sℓ, . . . ,˜sℓ−1 =
sℓ−1 −aℓ−1sℓ, the ui are also k-linear combinations of s1, . . . , sℓ. This completes the
proof of the theorem.
□
The inclusion k[u1, . . . , um] ⊆A constructed in Theorem 6 is called a Noether
normalization of A. Part (i) of the theorem still holds when k is ﬁnite, though the
proof is different—see Theorem 30 in Section 15.3 of DUMMIT and FOOTE (2004).
We next relate Noether normalizations to the Relative Finiteness Theorem. Let
k[u1, . . . , um] ⊆A be a Noether normalization of A. In the proof of Theorem 8
below, we will see that u1, . . . , um can be extended to a set of algebra generators
v1, . . . , vn, u1, . . . , um of A over k. This gives an onto k-algebra homomorphism
k[x1, . . . , xn, y1, . . . , ym] −→A
that maps xi to vi and yi to ui. Let I be the kernel of this map. Then we have a
k-algebra isomorphism

286
Chapter 5 Polynomial and Rational Functions on a Variety
(6)
k[x1, . . . , xn, y1, . . . , ym]/I ∼= A.
Furthermore, I ∩k[y1, . . . , ym] = {0}, since a nonzero f in this intersection would
satisfy f(u1, . . . , um) = 0, contradicting the algebraic independence of u1, . . . , um.
As earlier in the section, I ∩k[y1, . . . , ym] = {0} gives an inclusion
k[y1, . . . , ym] ⊆k[x1, . . . , xn, y1, . . . , ym]/I
which under the isomorphism (6) corresponds to the inclusion
k[u1, . . . , um] ⊆A.
Since A is ﬁnite over k[u1, . . . , um], it follows that k[x1, . . . , xn, y1, . . . , ym]/I is ﬁnite
over k[y1, . . . , ym], just as in Theorem 4. In other words, with a suitable choice of
algebra generators, the Noether normalization of any ﬁnitely generated k-algebra
becomes an instance of the Relative Finiteness Theorem.
Our ﬁnal topic is the geometric interpretation of Noether normalization. We will
assume that k is algebraically closed, hence inﬁnite by Exercise 4 of Chapter 4, §1.
The coordinate ring k[V] of an afﬁne variety V ⊆kℓhas a Noether normalization
k[u1, . . . , um] ⊆k[V] such that the ui are algebraically independent. If y1, . . . , ym are
variables, then mapping yi to ui gives a k-algebra homomorphism
k[y1, . . . , ym] −→k[V],
which by Proposition 8 of §4 corresponds to a polynomial map of varieties
π : V −→km.
The following theorem records the properties of this map.
Theorem 8 (Geometric Noether Normalization). Let V ⊆kℓbe a variety with k
algebraically closed. Then a Noether normalization of k[V] can be chosen so that
the above map π : V →km has the following properties:
(i) π is the composition of the inclusion V ⊆kℓwith a linear map kℓ→km.
(ii) π is onto with ﬁnite ﬁbers.
Proof. Let φi : V →k be the coordinate function that map (a1, . . . , aℓ) ∈V to
ai ∈k for i = 1, . . . , ℓ. If t1, . . . , tℓare coordinates on kℓ, then the isomorphism
k[t1, . . . , tℓ]/I(V) ∼= k[V]
from Proposition 3 of §2 takes the equivalence class [ti] to φi. Note that φ1, . . . , φℓ
are algebra generators of k[V] over k by Exercise 2.
Since each φi is the restriction to V of a linear map kℓ→k, the same is true
for any k-linear combination of the φi. By Theorem 6, we can choose u1, . . . , um to
be linear combinations of the φi. The resulting map (u1, . . . , um) : V →km is the
composition of V ⊆kℓwith a linear map kℓ→km. Since (u1, . . . , um) is the map π
described above, part (i) of the theorem follows.

§6 Relative Finiteness and Noether Normalization
287
For part (ii), note that u1, . . . , um ∈k[V] are linearly independent over k since
they are algebraically independent. When we write uj = ℓ
i=1 cijφi for 1 ≤j ≤m,
the resulting ℓ× m matrix C = (cij) has rank m. In the exercises, you will use
standard facts from linear algebra to show that there is an invertible ℓ× ℓmatrix D
whose last m columns are the matrix C. Set n = ℓ−m, so that ℓ= n + m.
Using D, we get new coordinates x1, . . . , xn, y1, . . . , ym on kℓ, where the xi (resp.
the yi) are the linear combinations of t1, . . . , tℓthat use the ﬁrst n (resp. last m) rows
of D. With these new coordinates on kℓ, we have an isomorphism
k[x1, . . . , xn, y1, . . . , ym]/I(V) ∼= k[V]
where [yi] maps to ui by the construction of D, and [xi] maps to an element we will
denote by vi. With this setup, we are in the situation of Theorems 4 and 5, and with
our new coordinates, the map π : kℓ= kn+m →km is projection onto the last m
coordinates. Then Theorem 5 implies that π is onto with ﬁnite ﬁbers.
□
This theorem, while nice, does not capture the full geometric meaning of Noether
normalization. For this, one needs to study what are called ﬁnite morphisms, as
deﬁned in Section II.3 of HARTSHORNE (1977). We will give a brief hint of the
geometry of ﬁnite morphisms in Exercise 17.
So, we have two ways to regard the Noether Normalization Theorem. From an
algebraic perspective, it is a structure theorem for ﬁnitely generated k-algebras over
an inﬁnite ﬁeld, where one ﬁnds algebraically independent elements such that the
algebra is ﬁnite over the subalgebra generated by these elements. From a geometric
point of view, the Noether Normalization Theorem asserts that for a variety over
an algebraically closed ﬁeld, there is a number m such that the variety maps onto
an m-dimensional afﬁne space with ﬁnite ﬁbers. As we will see in Chapter 9, the
number m is in fact the dimension of the variety.
EXERCISES FOR §6
1. Show that if a ring R is a ﬁnitely generated k-algebra, then R is isomorphic to the quotient
ring k[x1, . . . , xn]/I for some n and some ideal I ⊆k[x1, . . . , xn]. Hint: If r1, . . . , rn ∈R
have the property described in part (i) of Deﬁnition 1, then map k[x1, . . . , xn] to R by
sending xi to ri, and consider the ideal in k[x1, . . . , xn] that is the kernel of this map.
2. a. Prove that the coordinate ring k[V] of a variety V ⊆kn is a reduced, ﬁnitely generated
k-algebra. Also explain why the algebra generators can be chosen to be the coordinate
functions φi that map (a1, . . . , an) ∈V to ai ∈k for i = 1, . . . , n.
b. Conversely, suppose we are given a reduced, ﬁnitely generated k-algebra R. Show
that R is isomorphic to k[x1, . . . , xn]/I where I is radical. Hint: Combine the previous
exercise with Exercise 18 of §2.
c. Conclude if k is algebraically closed, then every reduced ﬁnitely generated k-algebra
is isomorphic to the coordinate ring of an afﬁne variety. Hint: The Nullstellensatz.
3. Let I ⊆k[x1, . . . , xn, y1, . . . , ym] be an ideal satisfying I ∩k[y1, . . . , ym] = {0}. Prove
that the natural map k[y1, . . . , ym] →k[x1, . . . , xn, y1, . . . , ym]/I is one-to-one.
4. Show that a one-to-one map of k-algebras k[W] →k[V] corresponds to a dominant
map V →W (i.e., a map such that W is the Zariski closure of the image of V). Hint:
φ : V →W gives V →φ(V) ⊆W.

288
Chapter 5 Polynomial and Rational Functions on a Variety
5. Let I ⊆k[x1. . . . , xn] be an ideal, and consider the quotient ring k[x1, . . . , xn]/I with sub-
ring k = k·1 consisting of all constant multiples of the identity. Show that k[x1, . . . , xn]/I
is ﬁnite over k in the sense of Deﬁnition 2 if and only if it has ﬁnite dimension as a vec-
tor space over k. Hint: A vector space has ﬁnite dimension if and only if it has a ﬁnite
spanning set.
6. Show that k[x, y]/⟨xa −yb⟩is ﬁnite both over k[x] and k[y], but that k[x, y]/⟨xa+1 −xyb⟩
is ﬁnite over k[y] but not k[x]. Interpret what this means geometrically.
7. a. Carefully verify that k[x, y]/⟨xy⟩is not ﬁnite over k[y].
b. However, show that k[x, y]/⟨x2 −y2⟩(which is ﬁnite over k[y]) is isomorphic as a
k-algebra to k[x, y]/⟨xy⟩. You may assume that 2 ̸= 0 in k. Hint: Use the invertible
linear (hence, polynomial) map of k2 to itself that takes (x, y) to (x −y, x + y).
8. Show that the k-algebras k[x, y]/⟨xy⟩and k[x, y]/⟨xy −1⟩are not isomorphic.
9. Given rings R ⊆S, we say that S is ﬁnitely generated over R provided that there exist
s1, . . . , sm ∈S such that every element of S can be expressed as a polynomial with
coefﬁcients in R in s1, . . . , sm. Thus a k-algebra is ﬁnitely generated in the sense of
Deﬁnition 1 exactly when it is ﬁnitely generated over the subring k.
a. Given a ring R, explain why the polynomial ring R[x] is ﬁnitely generated over R but
not ﬁnite over R in the sense of Deﬁnition 2. Hint: Proposition 3 will be useful.
b. Now assume that S is ﬁnitely generated over R. Strengthen Proposition 3 by showing
that S is ﬁnite over R if and only if every s ∈S satisﬁes an equation of the form (1),
i.e., every s ∈S is integral over R as deﬁned in the text. Hint: For the converse, each
si satisﬁes an equation of the form sℓi
i + · · · = 0. Then consider the ﬁnitely many
elements of S given by sa1
1 · · · sam
m where 0 ≤ai ≤ℓi for all i.
c. Suppose that we have ﬁnitely generated k-algebras R ⊆S. Use part (b) to prove that
S is ﬁnite over R if and only if every element of S is integral over R.
10. Consider the map π : V(I) →km deﬁned in equation (2) in the text. Given a point
b ∈km, prove that π−1(b) = V(Ib) × {b} in kn+m.
11. Suppose that we have a function f : X →Y for sets X and Y. The ﬁber of y ∈Y is the set
f −1(y) = {x ∈X | f (x) = y}. Prove that we have a disjoint union X = #
y∈Y f −1(y).
12. Consider the ideal I = ⟨x(xy−1)⟩= ⟨x2y−x⟩⊆k[x, y], where k is algebraically closed.
a. Prove that the projection π : V(I) →k to the y-axis is onto with ﬁnite ﬁbers.
b. Show that the k-algebra k[x, y]/I is not ﬁnite over k[y].
This shows that the converse of the Geometric Relative Finiteness Theorem is not true.
13. We want to show that the polynomial c(a1, . . . , aℓ−1) in (4) is a nonzero polynomial in
a1, . . . , aℓ−1.
a. Suppose f has total degree d and write f = fd + fd−1 + . . . + f0, where fi is the sum
of all terms of f of total degree i, 0 ≤i ≤d, Show that after the substitution (5), the
coefﬁcient c(a1, . . . , aℓ−1) of sd
ℓin is fd(a1, . . . , aℓ−1, 1).
b. A polynomial h(z1, . . . , zℓ) is homogeneous of total degree N if each monomial in
h has total degree N. (For example, the polynomial fi from part (a) is homogeneous
of total degree i.) Show that h is the zero polynomial in k[z1, . . . , zℓ] if and only if
h(z1, . . . , zℓ−1, 1) is the zero polynomial in k[z1, . . . , zℓ−1].
c. Conclude that c(a1, . . . , aℓ−1) is not the zero polynomial in a1, . . . , aℓ−1.
14. Let R ⊆S ⊆T be rings where S is ﬁnite over R via s1, . . . , sM ∈S and T is
ﬁnite over S via t1, . . . , tN ∈T. Prove that T is ﬁnite over R via the products sitj,
1 ≤i ≤M, 1 ≤j ≤N.
15. Let R[x] be the ring of polynomials in x with coefﬁcients in a ring R. Given a monic
polynomial f ∈R[x], adapt the division algorithm (Proposition 2 of Chapter 1, §5)
to show that any g ∈R[x] can be written g = af + b where a, b ∈R[x] and either
deg(b) < d or b = 0.
16. Let C be an ℓ× m matrix of rank m with entries in a ﬁeld k. Explain why m ≤ℓand
prove that there is an invertible ℓ× ℓmatrix D whose last m columns are the columns of

§6 Relative Finiteness and Noether Normalization
289
C. Hint: The columns of C give m linearly independent vectors in kℓ. Extend these to a
basis of kℓ.
17. In the situation of Theorem 5, we showed that π : V(I) →km is onto with ﬁnite ﬁbers.
Here we explore some further properties of π.
a. Let J ⊆k[x, y] be an ideal containing I, so that V(J) ⊆V(I). Prove that π(V(J))⊆km
is a variety. Hint: Let Jn = J ∩k[y] and note that π(V(J)) = V(Jn) by the Closure
Theorem. Then adapt the proof of Theorem 5 to show that all partial solutions in
V(Jn) extend to solutions in V(J) and conclude that φ(V(J)) = V(Jn).
b. In general, a polynomial map φ : V →W of varieties is closed if the image under φ
of any subvariety of V is a subvariety of W. Thus part (a) implies that π : V(I) →km
is closed. Prove more generally that the map ˜π : V(I) × kN →km × kN deﬁned
by ˜π(u, v) = (π(u), v) is also closed. Hint: All you are doing is adding N more y
variables in Theorem 5.
The text mentioned the notion of a ﬁnite morphism, which for a polynomial map
φ : V →W means that k[V] is ﬁnite over φ∗(k[W]). Thus the map π : V(I) →km
from Theorem 5 is ﬁnite. Over an algebraically closed ﬁeld, a ﬁnite map has ﬁnite ﬁbers
and a property called universally closed. We will not deﬁne this concept here [see Sec-
tion II.4 of HARTSHORNE (1977)], but we note that part (b) follows from being univer-
sally closed.
18. This exercise will compute the Noether normalization of the k-algebra A = k[x] × k[x],
where we use coordinate-wise multiplication. We assume k is algebraically closed.
a. A k-algebra contains a copy of the ﬁeld k. For A, show that k ∼= {(a, a) | a ∈k} ⊆A.
Also show that A is reduced.
b. Prove that A is generated by s1 = (1, 0), s2 = (0, 1) and s3 = (x, x) as a k-algebra.
Hint: Show that ( f (x), g(x)) = f (s3)s1 + g(s3)s2.
c. Prove that k[s3] ⊆A is a Noether normalization of A.
d. Deﬁne a k-algebra homomorphism k[x1, x2, y] →A by x1 →s1, x2 →s2, and y →s3.
Prove that the kernel of this homomorphism is I = ⟨x1 + x2 −1, x2
2 −x2⟩.
e. In the afﬁne space k3 with coordinates x1, x2, y, show that V = V(I) is a disjoint union
of two lines. Then use this to explain why k[V] is isomorphic to k[x] × k[x] = A.

Chapter 6
Robotics and Automatic Geometric Theorem
Proving
In this chapter we will consider two applications of concepts and techniques from
algebraic geometry in areas of computer science. First, continuing a theme intro-
duced in several examples in Chapter 1, we will develop a systematic approach that
uses algebraic varieties to describe the space of possible conﬁgurations of mechan-
ical linkages such as robot "arms." We will use this approach to solve the forward
and inverse kinematic problems of robotics for certain types of robots.
Second, we will apply the algorithms developed in earlier chapters to the study
of automatic geometric theorem proving, an area that has been of interest to res-
earchers in artiﬁcial intelligence. When the hypotheses of a geometric theorem can
be expressed as polynomial equations relating the Cartesian coordinates of points
in the Euclidean plane, the geometrical propositions deducible from the hypotheses
will include all the statements that can be expressed as polynomials in the ideal
generated by the hypotheses.
§1
Geometric Description of Robots
To treat the space of conﬁgurations of a robot geometrically, we need to make some
simplifying assumptions about the components of our robots and their mechanical
properties. We will not try to address many important issues in the engineering of
actual robots (such as what types of motors and mechanical linkages would be used
to achieve what motions, and how those motions would be controlled). Thus, we
will restrict ourselves to highly idealized robots. However, within this framework,
we will be able to indicate the types of problems that actually arise in robot motion
description and planning.
We will always consider robots constructed from rigid links or segments, con-
nected by joints of various types. For simplicity, we will consider only robots in
which the segments are connected in series, as in a human limb. One end of our
robot "arm" will usually be ﬁxed in position. At the other end will be the "hand" or
© Springer International Publishing Switzerland 2015
D.A. Cox et al., Ideals, Varieties, and Algorithms, Undergraduate Texts
in Mathematics, DOI 10.1007/978-3-319-16721-3_6
291

292
Chapter 6 Robotics and Automatic Geometric Theorem Proving
"effector," which will sometimes be considered as a ﬁnal segment of the robot. In
actual robots, this "hand" might be provided with mechanisms for grasping objects
or with tools for performing some task. Thus, one of the major goals is to be able to
describe and specify the position and orientation of the "hand."
Since the segments of our robots are rigid, the possible motions of the entire
robot assembly are determined by the motions of the joints. Many actual robots are
constructed using
• planar revolute joints, and
• prismatic joints.
A planar revolute joint permits a rotation of one segment relative to another. We
will assume that both of the segments in question lie in one plane and all motions of
the joint will leave the two segments in that plane. (This is the same as saying that
the axis of rotation is perpendicular to the plane in question.)
a revolute joint
A prismatic joint permits one segment of a robot to move by sliding or translation
along an axis. The following sketch shows a schematic view of a prismatic joint
between two segments of a robot lying in a plane. Such a joint permits translational
motion along a line in the plane.
←  retracted
a prismatic joint
←partially
extended

§1 Geometric Description of Robots
293
If there are several joints in a robot, we will assume for simplicity that the joints
all lie in the same plane, that the axes of rotation of all revolute joints are perpendic-
ular to that plane, and, in addition, that the translation axes for the prismatic joints
all lie in the plane of the joints. Thus, all motion will take place in one plane. Of
course, this leads to a very restricted class of robots. Real robots must usually be
capable of 3-dimensional motion. To achieve this, other types and combinations of
joints are used. These include "ball" joints allowing rotation about any axis pass-
ing through some point in R3 and helical or "screw" joints combining rotation and
translation along the axis of rotation in R3. It would also be possible to connect
several segments of a robot with planar revolute joints, but with nonparallel axes of
rotation. All of these possible conﬁgurations can be treated by methods similar to
the ones we will present, but we will not consider them in detail. Our purpose here
is to illustrate how afﬁne varieties can be used to describe the geometry of robots,
not to present a treatise on practical robotics. The planar robots provide a class of
relatively uncomplicated but illustrative examples for us to consider.
Example 1. Consider the following planar robot "arm" with three revolute joints
and one prismatic joint. All motions of the robot take place in the plane of the page.
← segment 1
joint 1 →
↓
segment 2
↓
joint 2
segment 3 →
joint 3 →
← segment 4
←joint 4
(fully extended)
←segment 5
(the hand)
For easy reference, we number the segments and joints of a robot in increasing order
out from the ﬁxed end to the hand. Thus, in the above ﬁgure, segment 2 connects
joints 1 and 2, and so on. Joint 4 is prismatic, and we will regard segment 4 as
having variable length, depending on the setting of the prismatic joint. In this robot,
the hand of the robot comprises segment 5.
In general, the position or setting of a revolute joint between segments i and i+1
can be described by measuring the angle θ (counterclockwise) from segment i to
segment i + 1. Thus, the totality of settings of such a joint can be parametrized by
a circle S1 or by the interval [0, 2π] with the endpoints identiﬁed. (In some cases,
a revolute joint may not be free to rotate through a full circle, and then we would
parametrize the possible settings by a subset of S1.)
Similarly, the setting of a prismatic joint can be speciﬁed by giving the distance
the joint is extended or, as in Example 1, by the total length of the segment (i.e., the
distance between the end of the joint and the previous joint). Either way, the settings
of a prismatic joint can be parametrized by a ﬁnite interval of real numbers.

294
Chapter 6 Robotics and Automatic Geometric Theorem Proving
If the joint settings of our robot can be speciﬁed independently, then the possible
settings of the whole collection of joints in a planar robot with r revolute joints and
p prismatic joints can be parametrized by the Cartesian product
J = S1 × · · · × S1 × I1 × · · · × Ip,
where there is one S1 factor for each revolute joint, and Ij gives the settings of the
j-th prismatic joint. We will call J the joint space of the robot.
We can describe the space of possible conﬁgurations of the "hand" of a planar
robot as follows. Fixing a Cartesian coordinate system in the plane, we can repre-
sent the possible positions of the "hand" by the points (a, b) of a region U ⊆R2.
Similarly, we can represent the orientation of the "hand" by giving a unit vector
aligned with some speciﬁc feature of the hand. Thus, the possible hand orientations
are parametrized by vectors u in V = S1. For example, if the "hand" is attached to
a revolute joint, then we have the following picture of the hand conﬁguration:
the point (a,b)
specifies the
hand position 
(a,b) →
the unit vector u
specifies the
hand orientation
u
We will call C = U × V the conﬁguration space or operational space of the robot's
hand.
Since the robot's segments are assumed to be rigid, each collection of joint
settings will place the "hand" in a uniquely determined location, with a uniquely
determined orientation. Thus, we have a function or mapping
f : J −→C
which encodes how the different possible joint settings yield different hand conﬁg-
urations.
The two basic problems we will consider can be described succinctly in terms of
the mapping f : J →C described above:
• (Forward Kinematic Problem) Can we give an explicit description or formula for
f in terms of the joint settings (our coordinates on J ) and the dimensions of the
segments of the robot "arm"?
• (Inverse Kinematic Problem) Given c ∈C, can we determine one or all the j ∈J
such that f(j) = c?
In §2, we will see that the forward problem is relatively easily solved. Deter-
mining the position and orientation of the "hand" from the "arm" joint settings is
mainly a matter of being systematic in describing the relative positions of the seg-
ments on either side of a joint. Thus, the forward problem is of interest mainly as

§1 Geometric Description of Robots
295
a preliminary to the inverse problem. We will show that the mapping f : J →C
giving the "hand" conﬁguration as a function of the joint settings may be written as
a polynomial mapping as in Chapter 5, §1.
The inverse problem is somewhat more subtle since our explicit formulas will
not be linear if revolute joints are present. Thus, we will need to use the general
results on systems of polynomial equations to solve the equation
(1)
f(j) = c.
One feature of nonlinear systems of equations is that there can be several different
solutions, even when the entire set of solutions is ﬁnite. We will see in §3 that this
is true for a planar robot arm with three (or more) revolute joints. As a practical
matter, the potential nonuniqueness of the solutions of the systems (1) is sometimes
very desirable. For instance, if our real world robot is to work in a space containing
physical obstacles or barriers to movement in certain directions, it may be the case
that some of the solutions of (1) for a given c ∈C correspond to positions that are
not physically reachable:
←  barrier
To determine whether it is possible to reach a given position, we might need to
determine all solutions of (1), then see which one(s) are feasible given the con-
straints of the environment in which our robot is to work.
EXERCISES FOR §1
1. Give descriptions of the joint space J and the conﬁguration space C for the planar robot
picture in Example 1 in the text. For your description of C, determine a bounded subset of
U ⊆R2 containing all possible hand positions. Hint: The description of U will depend on
the lengths of the segments.
2. Consider the mapping f : J →C for the robot pictured in Example 1 in the text. On geo-
metric grounds, do you expect f to be a one-to-one mapping? Can you ﬁnd two different
ways to put the hand in some particular position with a given orientation? Are there more
than two such positions?
The text discussed the joint space J and the conﬁguration space C for planar robots. In the
following problems, we consider what J and C look like for robots capable of motion in
three dimensions.
3. What would the conﬁguration space C look like for a 3-dimensional robot? In particular,
how can we describe the possible hand orientations?

296
Chapter 6 Robotics and Automatic Geometric Theorem Proving
4. A "ball" joint at point B allows segment 2 in the robot pictured below to rotate by any
angle about any axis in R3 passing through B. (Note: The motions of this joint are similar
to those of the "joystick" found in some computer games.)
a ball joint
←segment 2
rotates freely in
three dimensions
a. Describe the set of possible joint settings for this joint mathematically. Hint: The dis-
tinct joint settings correspond to the possible direction vectors of segment 2.
b. Construct a one-to-one correspondence between your set of joint settings in part (a)
and the unit sphere S2 ⊆R3. Hint: One simple way to do this is to use the spherical
angular coordinates φ, θ on S2.
5. A helical or "screw" joint at point H allows segment 2 of the robot pictured below to
extend out from H along the line L in the direction of segment 1, while rotating about the
axis L.
a helical or "screw" joint
The rotation angle θ (measured from the original, unextended position of segment 2) is
given by θ = l ·α, where l ∈[0, m] gives the distance from H to the other end of segment
2 and α is a constant angle. Give a mathematical description of the space of joint settings
for this joint.
6. Give a mathematical description of the joint space J for a 3-dimensional robot with two
"ball" joints and one helical joint.

§2 The Forward Kinematic Problem
297
§2
The Forward Kinematic Problem
In this section, we will present a standard method for solving the forward kinematic
problem for a given robot "arm." As in §1, we will only consider robots in R2,
which means that the "hand" will be constrained to lie in the plane. Other cases will
be studied in the exercises.
All of our robots will have a ﬁrst segment that is anchored, or ﬁxed in position.
In other words, there is no movable joint at the initial endpoint of segment 1. With
this convention, we will use a standard rectangular coordinate system in the plane
to describe the position and orientation of the "hand." The origin of this coordinate
system is placed at joint 1 of the robot arm, which is also ﬁxed in position since all
of segment 1 is. For example:
←
anchor
←  segment 1
y1
x1
joint 1
→
In addition to the global (x1, y1) coordinate system, we introduce a local rectan-
gular coordinate system at each of the revolute joints to describe the relative posi-
tions of the segments meeting at that joint. Naturally, these coordinate systems will
change as the position of the "arm" varies.
At a revolute joint i, we introduce an (xi+1, yi+1) coordinate system in the follow-
ing way. The origin is placed at joint i. We take the positive xi+1-axis to lie along
the direction of segment i + 1 (in the robot's current position). Then the positive
yi+1-axis is determined to form a normal right-handed rectangular coordinate sys-
tem. Note that for each i ≥2, the (xi, yi) coordinates of joint i are (li, 0), where li is
the length of segment i.

298
Chapter 6 Robotics and Automatic Geometric Theorem Proving
joint i −1 →
↑
joint i
θi
yi
xi
segment i
yi +1
xi +1
segment i +1
Our ﬁrst goal is to relate the (xi+1, yi+1) coordinates of a point with the (xi, yi)
coordinates of that point. Let θi be the counterclockwise angle from the xi-axis to
the xi+1-axis. This is the same as the joint setting angle θi described in §1. From the
diagram above, we see that if a point q has (xi+1, yi+1) coordinates
q = (ai+1, bi+1),
then to obtain the (xi, yi) coordinates of q, say
q = (ai, bi),
we rotate by the angle θi (to align the xi- and xi+1-axes), and then translate by the
vector (li, 0) (to make the origins of the coordinate systems coincide). In the exe-
rcises, you will show that rotation by θi is accomplished by multiplying by the rota-
tion matrix

cosθi −sinθi
sinθi
cosθi

.
It is also easy to check that translation is accomplished by adding the vector (li, 0).
Thus, we get the following relation between the (xi, yi) and (xi+1, yi+1) coordinates
of q:
ai
bi

=
cosθi −sinθi
sinθi
cosθi

·
ai+1
bi+1

+
li
0

.
This coordinate transformation is also commonly written in a shorthand form using
a 3 × 3 matrix and 3-component vectors:
(1)
⎛
⎝
ai
bi
1
⎞
⎠=
⎛
⎝
cosθi −sinθi
li
sinθi
cosθi 0
0
0
1
⎞
⎠·
⎛
⎝
ai+1
bi+1
1
⎞
⎠= Ai ·
⎛
⎝
ai+1
bi+1
1
⎞
⎠.
This allows us to combine the rotation by θi with the translation along segment i
into a single 3 × 3 matrix Ai.

§2 The Forward Kinematic Problem
299
Example 1. With this notation in hand, let us next consider a general plane robot
"arm" with three revolute joints:
length l1
θ 1
length l2
length l3
θ 2
θ 3
We will think of the hand as segment 4, which is attached via the revolute joint 3
to segment 3. As before, li will denote the length of segment i. We have
A1 =
⎛
⎝
cosθ1 −sinθ1
0
sinθ1
cosθ1 0
0
0
1
⎞
⎠
since the origin of the (x2, y2) coordinate system is also placed at joint 1. We also
have matrices A2 and A3 as in formula (1). The key observation is that the global
coordinates of any point can be obtained by starting in the (x4, y4) coordinate system
and working our way back to the global (x1, y1) system one joint at a time. In other
words, we multiply the (x4, y4) coordinate vector of the point by A3, A2, A1 in turn:
⎛
⎝
x1
y1
1
⎞
⎠= A1A2A3
⎛
⎝
x4
y4
1
⎞
⎠.
Using the trigonometric addition formulas, this equation can be written as
⎛
⎝
x1
y1
1
⎞
⎠=
⎛
⎝
cos(θ1 + θ2 + θ3) −sin(θ1 + θ2 + θ3) l3cos(θ1 + θ2) + l2cosθ1
sin(θ1 + θ2 + θ3) cos(θ1 + θ2 + θ3) l3sin(θ1 + θ2) + l2sinθ1
0
0
1
⎞
⎠
⎛
⎝
x4
y4
1
⎞
⎠.
Since the (x4, y4) coordinates of the hand are (0, 0) (because the hand is attached
directly to joint 3), we obtain the (x1, y1) coordinates of the hand by setting x4 =
y4 = 0 and computing the matrix product above. The result is
(2)
⎛
⎝
x1
y1
1
⎞
⎠=
⎛
⎝
l3cos(θ1 + θ2) + l2cosθ1
l3sin(θ1 + θ2) + l2sinθ1
1
⎞
⎠.

300
Chapter 6 Robotics and Automatic Geometric Theorem Proving
The hand orientation is determined if we know the angle between the x4-axis and
the direction of any particular feature of interest to us on the hand. For instance,
we might simply want to use the direction of the x4-axis to specify this orientation.
From our computations, we know that the angle between the x1-axis and the x4-axis
is simply θ1 + θ2 + θ3. Knowing the θi allows us to also compute this angle.
If we combine this fact about the hand orientation with the formula (2) for the
hand position, we get an explicit description of the mapping f : J →C introduced
in §1. As a function of the joint angles θi, the conﬁguration of the hand is given by
(3)
f(θ1 + θ2 + θ3) =
⎛
⎝
l3cos(θ1 + θ2) + l2cosθ1
l3sin(θ1 + θ2) + l2sinθ1
θ1 + θ2 + θ3
⎞
⎠.
The same ideas will apply when any number of planar revolute joints are present.
You will study the explicit form of the function f in these cases in Exercise 7.
Example 2. Prismatic joints can also be handled within this framework. For ins-
tance, let us consider a planar robot whose ﬁrst three segments and joints are the
same as those of the robot in Example 1, but which has an additional prismatic joint
between segment 4 and the hand. Thus, segment 4 will have variable length and
segment 5 will be the hand.
length l1
θ 1
length l2
length l3
θ 2
θ 3
length l4
The translation axis of the prismatic joint lies along the direction of segment 4.
We can describe such a robot as follows. The three revolute joints allow us exactly
the same freedom in placing joint 3 as in the robot studied in Example 1. However,
the prismatic joint allows us to change the length of segment 4 to any value between
l4 = m1 (when retracted) and l4 = m2 (when fully extended). By the reasoning given
in Example 1, if the setting l4 of the prismatic joint is known, then the position of
the hand will be given by multiplying the product matrix A1A2A3 times the (x4, y4)
coordinate vector of the hand, namely (l4, 0). It follows that the conﬁguration of the
hand is given by

§2 The Forward Kinematic Problem
301
(4)
g(θ1, θ2, θ3, l4) =
⎛
⎝
l4cos(θ1 + θ2 + θ3) + l3cos(θ1 + θ2) + l2cosθ1
l4sin(θ1 + θ2 + θ3) + l3sin(θ1 + θ2) + l2sinθ1
θ1 + θ2 + θ3
⎞
⎠.
As before, l2 and l3 are constant, but l4 ∈[m1, m2] is now another variable. The hand
orientation will be given by θ1 + θ2 + θ3 as before since the setting of the prismatic
joint will not affect the direction of the hand.
We will next discuss how formulas such as (3) and (4) may be converted into
representations of f and g as polynomial or rational mappings in suitable variables.
The joint variables for revolute and for prismatic joints are handled differently. For
the revolute joints, the most direct way of converting to a polynomial set of equa-
tions is to use an idea we have seen several times before, for example, in Exercise 8
of Chapter 2, §8. Even though cosθ and sinθ are transcendental functions, they give
a parametrization
x = cosθ,
y = sinθ
of the algebraic variety V(x2 + y2 −1) in the plane. Thus, we can write the compo-
nents of the right-hand side of (3) or, equivalently, the entries of the matrix A1A2A3
in (2) as functions of
ci = cosθi,
si = sinθi,
subject to the constraints
(5)
c2
i + s2
i −1 = 0
for i = 1, 2, 3. Note that the variety deﬁned by these three equations in R6 is a
concrete realization of the joint space J for this type of robot. Geometrically, this
variety is just a Cartesian product of three copies of the circle.
Explicitly, we obtain from (3) an expression for the hand position as a function
of the variables c1, s1, c2, s2, c3, s3. Using the trigonometric addition formulas, we
can write
cos(θ1 + θ2) = cosθ1cosθ2 −sinθ1sinθ2 = c1c2 −s1s2.
Similarly,
sin(θ1 + θ2) = sinθ1cosθ2 + sinθ2cosθ1 = s1c2 + s2c1.
Thus, the (x1, y1) coordinates of the hand position are:
(6)

l3(c1c2 −s1s2) + l2c1
l3(s1c2 + s2c1) + l2s1

.

302
Chapter 6 Robotics and Automatic Geometric Theorem Proving
In the language of Chapter 5, we have deﬁned a polynomial mapping from the vari-
ety J = V(x2
1 + y2
1 −1, x2
2 + y2
2 −1, x2
3 + y2
3 −1) to R2. Note that the hand position
does not depend on θ3. That angle enters only in determining the hand orientation.
Since the hand orientation depends directly on the angles θi themselves, it is not
possible to express the orientation itself as a polynomial in ci = cosθi and si = sinθi.
However, we can handle the orientation in a similar way. See Exercise 3.
Similarly, from the mapping g in Example 2, we obtain the polynomial form
(7)

l4(c1(c2c3 −s2s3) −s1(c2s3 + c3s2)) + l3(c1c2 −s1s2) + l2c1
l4(s1(c2c3 −s2s3) + c1(c2s3 + c3s2)) + l3(s1c2 + s2c1) + l2s1

for the (x1, y1) coordinates of the hand position. Here J is the subset V × [m1, m2]
of the variety V × R, where V = V(x2
1 + y2
1 −1, x2
2 + y2
2 −1, x2
3 + y2
3 −1). The
length l4 is treated as another ordinary variable in (7), so our component functions
are polynomials in l4, and the ci and si.
A second way to write formulas (3) and (4) is based on the rational
parametrization
(8)
x = 1 −t2
1 + t2 ,
y =
2t
1 + t2
of the circle introduced in §3 of Chapter 1. [In terms of the trigonometric parame-
trization, t = tan(θ/2).] This allows us to express the mapping (3) in terms of three
variables ti = tan(θi/2). We will leave it as an exercise for the reader to work out
this alternate explicit form of the mapping f : J →C in Example 1. In the language
of Chapter 5, the variety J for the robot in Example 1 is birationally equivalent to
R3. We can construct a rational parametrization ρ : R3 →J using three copies
of the parametrization (8). Hence, we obtain a rational mapping from R3 to R2,
expressing the hand coordinates of the robot arm as functions of t1, t2, t3 by taking
the composition of ρ with the hand coordinate mapping in the form (6).
Both of these forms have certain advantages and disadvantages for practical use.
For the robot of Example 1, one immediately visible advantage of the rational map-
ping obtained from (8) is that it involves only three variables rather than the six
variables si, ci, i = 1, 2, 3, needed to describe the full mapping f as in Exercise 3.
In addition, we do not need the three extra constraint equations (5). However, the ti
values corresponding to joint positions with θi close to π are awkwardly large, and
there is no ti value corresponding to θi = π. We do not obtain every theoretically
possible hand position in the image of the mapping f when it is expressed in this
form. Of course, this might not actually be a problem if our robot is constructed so
that segment i+1 is not free to fold back onto segment i (i.e., the joint setting θi = π
is not possible). The polynomial form (6) is more unwieldy, but since it comes from
the trigonometric (unit-speed) parametrization of the circle, it does not suffer from
the potential shortcomings of the rational form. It would be somewhat better suited
for revolute joints that can freely rotate through a full circle.

§2 The Forward Kinematic Problem
303
EXERCISES FOR §2
1. Consider the plane R2 with an orthogonal right-handed coordinate system (x1, y1). Now
introduce a second coordinate system (x2, y2) by rotating the ﬁrst counterclockwise by an
angle θ. Suppose that a point q has (x1, y1) coordinates (a1, b1) and (x2, y2) coordinates
(a2, b2). We claim that

a1
b1

=

cosθ −sinθ
sinθ
cosθ

·

a2
b2

.
To prove this, ﬁrst express the (x2, y2) coordinates of q in polar form as
q = (a2, b2) = (rcosα, rsinα).
a. Show that the (x1, y1) coordinates of q are given by
q = (a1, b1) = (rcos(α + θ), rsin(α + θ)).
b. Now use trigonometric identities to prove the desired formula.
2. In Examples 1 and 2, we used a 3 × 3 matrix A to represent each of the changes of coo-
rdinates from one local system to another. Those changes of coordinates were rotations,
followed by translations. These are special types of afﬁne transformations.
a. Show that any afﬁne transformation in the plane
x′ = ax + by + e,
y′ = cx + dy + f
can be represented in a similar way:
⎛
⎝
x′
y′
1
⎞
⎠=
⎛
⎝
a b e
c d
f
0 0 1
⎞
⎠·
⎛
⎝
x
y
1
⎞
⎠.
b. Give a similar representation for afﬁne transformations of R3 using 4 × 4 matrices.
3. In this exercise, we will reconsider the hand orientation for the robots in Examples 1
and 2. Namely, let α = θ1 +θ2 +θ3 be the angle giving the hand orientation in the (x1, y1)
coordinate system.
a. Using the trigonometric addition formulas, show that
c = cosα, s = sinα
can be expressed as polynomials in ci = cosθi and si = sinθi. Thus, the whole map-
ping f can be expressed in polynomial form, at the cost of introducing an extra coor-
dinate function for C.
b. Express c and s using the rational parametrization (8) of the circle.
4. Consider a planar robot with a revolute joint 1, segment 2 of length l2, a prismatic joint 2
with settings l3 ∈[0, m3], and a revolute joint 3, with segment 4 being the hand.
a. What are the joint and conﬁguration spaces J and C for this robot?
b. Using the method of Examples 1 and 2, construct an explicit formula for the mapping
f : J →C in terms of the trigonometric functions of the joint angles.
c. Convert the function f into a polynomial mapping by introducing suitable new coordi-
nates.
5. Rewrite the mappings f and g in Examples 1 and 2, respectively, using the rational param-
etrization (8) of the circle for each revolute joint. Show that in each case the hand position

304
Chapter 6 Robotics and Automatic Geometric Theorem Proving
and orientation are given by rational mappings on Rn. (The value of n will be different in
the two examples.)
6. Rewrite the mapping f for the robot from Exercise 4, using the rational parametrization (8)
of the circle for each revolute joint.
7. Consider a planar robot with a ﬁxed segment 1 as in our examples in this section and with
n revolute joints linking segments of length l2, . . . , ln. The hand is segment n+1, attached
to segment n by joint n.
a. What are the joint and conﬁguration spaces for this robot?
b. Show that the mapping f : J →C for this robot has the form
f (θ1, . . . , θn) =
⎛
⎜
⎝
n−1
i=1 li+1cos
 i
j=1 θj

n−1
i=1 li+1sin
 i
j=1 θj

n
i=1 θi
⎞
⎟
⎠.
Hint: Argue by induction on n.
8. Another type of 3-dimensional joint is a "spin" or nonplanar revolute joint that allows one
segment to rotate or spin in the plane perpendicular to the other segment. In this exercise,
we will study the forward kinematic problem for a 3-dimensional robot containing two
"spin" joints. As usual, segment 1 of the robot will be ﬁxed, and we will pick a global
coordinate system (x1, y1, z1) with the origin at joint 1 and segment 1 on the z1-axis. Joint
1 is a "spin" joint with rotation axis along the z1-axis, so that segment 2 rotates in the
(x1, y1)-plane. Then segment 2 has length l2 and joint 2 is a second "spin" joint connecting
segment 2 to segment 3. The axis for joint 2 lies along segment 2, so that segment 3 always
rotates in the plane perpendicular to segment 2.
a. Construct a local right-handed orthogonal coordinate system (x2, y2, z2) with origin at
joint 1, with the x2-axis in the direction of segment 2 and the y2-axis in the (x1, y1)-
plane. Give an explicit formula for the (x1, y1, z1) coordinates of a general point, in
terms of its (x2, y2, z2) coordinates and of the joint angle θ1.
b. Express your formula from part (a) in matrix form, using the 4×4 matrix representation
for afﬁne space transformations given in part (b) of Exercise 2.
c. Now, construct a local orthogonal coordinate system (x3, y3, z3) with origin at joint 2,
the x3-axis in the direction of segment 3, and the z3-axis in the direction of segment
2. Give an explicit formula for the (x2, y2, z2) coordinates of a point in terms of its
(x3, y3, z3) coordinates and the joint angle θ2.
d. Express your formula from part (c) in matrix form.
e. Give the transformation relating the (x3, y3, z3) coordinates of a general point to its
(x1, y1, z1) coordinates in matrix form. Hint: This will involve suitably multiplying the
matrices found in parts (b) and (d).
9. Consider the robot from Exercise 8.
a. Using the result of part (c) of Exercise 8, give an explicit formula for the mapping
f : J →C for this robot.
b. Express the hand position for this robot as a polynomial function of the variables
ci = cosθi and si = sinθi.
c. The orientation of the hand (the end of segment 3) of this robot can be expressed by
giving a unit vector in the direction of segment 3, expressed in the global coordinate
system. Find an expression for the hand orientation.
§3
The Inverse Kinematic Problem and Motion Planning
In this section, we will continue the discussion of the robot kinematic problems
introduced in §1. To begin, we will consider the inverse kinematic problem for the

§3 The Inverse Kinematic Problem and Motion Planning
305
planar robot arm with three revolute joints studied in Example 1 of §2. Given a
point (x1, y1) = (a, b) ∈R2 and an orientation, we wish to determine whether it
is possible to place the hand of the robot at that point with that orientation. If it is
possible, we wish to ﬁnd all combinations of joint settings that will accomplish this.
In other words, we want to determine the image of the mapping f : J →C for this
robot; for each c in the image of f, we want to determine the inverse image f −1(c).
It is quite easy to see geometrically that if l3 = l2 = l, the hand of our robot can
be placed at any point of the closed disk of radius 2l centered at joint 1—the origin of
the (x1, y1) coordinate system. On the other hand, if l3 ̸= l2, then the hand positions
ﬁll out a closed annulus centered at joint 1. (See, for example, the ideas used in
Exercise 14 of Chapter 1, §2.) We will also be able to see this using the solution of
the forward problem derived in equation (7) of §2. In addition, our solution will give
explicit formulas for the joint settings necessary to produce a given hand position.
Such formulas could be built into a control program for a robot of this kind.
For this robot, it is also easy to control the hand orientation. Since the setting of
joint 3 is independent of the settings of joints 1 and 2, we see that, given any θ1
and θ2, it is possible to attain any desired orientation α = θ1 + θ2 + θ3 by setting
θ3 = α −(θ1 + θ2) accordingly.
To simplify our solution of the inverse kinematic problem, we will use the above
observation to ignore the hand orientation. Thus, we will concentrate on the position
of the hand, which is a function of θ1 and θ2 alone. From equation (6) of §2, we
see that the possible ways to place the hand at a given point (x1, y1) = (a, b) are
described by the following system of polynomial equations:
(1)
a = l3(c1c2 −s1s2) + l2c1,
b = l3(c1s2 + c2s1) + l2s1,
0 = c2
1 + s2
1 −1,
0 = c2
2 + s2
2 −1
for c1, s1, c2, s2. To solve these equations, we ﬁrst compute a grevlex Gröbner basis
with
c1 > s1 > c2 > s2.
Our solutions will depend on the values of a, b, l2, l3, which appear as symbolic
parameters in the coefﬁcients of the Gröbner basis:
(2)
c1 −
2bl2l3
2l2(a2 + b2)s2 −a(a2 + b2 + l2
2 −l2
3)
2l2(a2 + b2)
,
s1 +
2al2l3
2l2(a2 + b2)s2 + b(a2 + b2 + l2
2 −l2
3)
2l2(a2 + b2)
,
c2 −a2 + b2 −l2
2 −l2
3
2l2l3
,
s2
2 + (a2 + b2)2 −2(a2 + b2)(l2
2 + l2
3) + (l2
2 −l2
3)2
4l2
2l2
3
.

306
Chapter 6 Robotics and Automatic Geometric Theorem Proving
In algebraic terms, this is the reduced Gröbner basis for the ideal I generated by
the polynomials in (1) in the ring R(a, b, l2, l3)[c1, s1, c2, s2]. Note that we allow
denominators that depend only on the parameters a, b, l2, l3.
This is the ﬁrst time we have computed a Gröbner basis over a ﬁeld of rational
functions, and one has to be a bit careful about how to interpret (2). Working over
R(a, b, l2, l3) means that a, b, l2, l3 are abstract variables over R, and, in particular,
they are algebraically independent [i.e., if p is a polynomial with real coefﬁcients
such that p(a, b, l2, l3) = 0, then p must be the zero polynomial]. Yet, in practice,
we want a, b, l2, l3 to be certain speciﬁc real numbers. When we make such a sub-
stitution, the polynomials (1) generate an ideal ¯I ⊆R[c1, s1, c2, s2] corresponding to
a speciﬁc hand position of a robot with speciﬁc segment lengths. The key question
is whether (2) remains a Gröbner basis for ¯I under this substitution. In general, the
replacement of variables by speciﬁc values in a ﬁeld is called specialization, and the
question is how a Gröbner basis behaves under specialization.
A ﬁrst observation is that we expect problems when a specialization causes any of
the denominators in (2) to vanish. This is typical of how specialization works: things
usually behave nicely for most (but not all) values of the variables. In the exercises,
you will prove that there is a proper subvariety W ⊆R4 such that (2) specializes to
a Gröbner basis of ¯I whenever a, b, l2, l3 take values in R4 \ W. We also will see that
there is an algorithm for ﬁnding W. The subtle point is that, in general, the vanishing
of denominators is not the only thing that can go wrong (you will work out some
examples in the exercises). Fortunately, in the example we are considering, it can be
shown that W is, in fact, deﬁned by the vanishing of the denominators. This means
that if we choose values l2 ̸= 0, l3 ̸= 0, and a2 + b2 ̸= 0, then (2) still gives a
Gröbner basis of (1). The details of the argument will be given in Exercise 9.
Given such a specialization, two observations follow immediately from the lead-
ing terms of the Gröbner basis (2). First, any zero s2 of the last polynomial can be
extended uniquely to a full solution of the system. Second, the set of solutions of (1)
is a ﬁnite set for this choice of a, b, l2, l3. Indeed, since the last polynomial in (2) is
quadratic in s2, there can be at most two distinct solutions. It remains to see which
a, b yield real values for s2 (the relevant solutions for the geometry of our robot).
To simplify the formulas somewhat, we will specialize to the case l2 = l3 = 1. In
Exercise 1, you will show that by either substituting l2 = l3 = 1 directly into (2) or
setting l2 = l3 = 1 in (1) and recomputing a Gröbner basis in R(a, b)[c1, s1, c2, s2],
we obtain the same result:
(3)
c1 −
2b
2(a2 + b2)s2 −a
2,
s1 +
2a
2(a2 + b2)s2 + b
2,
c2 −a2 + b2 −2
2
,
s2
2 + (a2 + b2)(a2 + b2 −4)
4
.

§3 The Inverse Kinematic Problem and Motion Planning
307
Other choices for l2 and l3 will be studied in Exercise 4. [Although (2) remains a
Gröbner basis for any nonzero values of l2 and l3, the geometry of the situation
changes rather dramatically if l2 ̸= l3.]
It follows from our earlier remarks that (3) is a Gröbner basis for (1) for all
specializations of a and b where a2 + b2 ̸= 0, which over R happens whenever the
hand is not at the origin. Solving the last equation in (3), we ﬁnd that
s2 = ±1
2

(a2 + b2)(4 −(a2 + b2)).
Note that the solution(s) of this equation are real if and only if a2 + b2 ≤4, and
when a2 + b2 = 4, we have a double root. From the geometry of the system, that is
exactly what we expect. The distance from joint 1 to joint 3 is at most l2 + l3 = 2,
and positions with l2 + l3 = 2 can be reached in only one way—by setting θ2 = 0
so that segment 3 and segment 2 point in the same direction.
Given s2, the other elements of the Gröbner basis (3) give exactly one value for
each of c1, s1, c2. Further, since c2
1 + s2
1 −1 and c2
2 + s2
2 −1 are in the ideal generated
by (3), the values we get for c1, s1, c2, s2 uniquely determine the joint angles θ1 and
θ2. Thus, the case where a2 + b2 ̸= 0 is fully understood.
It remains to study s1, c1, s2, c2 when a = b = 0. Geometrically, this means that
joint 3 is placed at the origin of the (x1, y1) system—at the same point as joint 1. The
ﬁrst two polynomials in our basis (3) are undeﬁned when we substitute a = b = 0 in
their coefﬁcients. This is a case where specialization fails. In fact, setting l2 = l3 = 1
and a = b = 0 into the original system (1) yields the grevlex Gröbner basis
(4)
c2
1 + s2
1 −1,
c2 + 1,
s2.
With a little thought, the geometric reason for this is visible. There are actually
inﬁnitely many different possible conﬁgurations that will place joint 3 at the origin
since segments 2 and 3 have equal lengths. The angle θ1 can be speciﬁed arbitrarily,
and then setting θ2 = π will fold segment 3 back along segment 2, placing joint
3 at (0, 0). These are the only joint settings placing the hand at (a, b) = (0, 0). In
Exercise 3 you will verify that this analysis is fully consistent with (4).
Note that the form of the specialized Gröbner basis (4) is different from the gen-
eral form (2). The equation for s2 now has degree 1, and the equation for c1 (rather
than the equation for s2) has degree 2. Below we will say more about how Gröbner
bases can change under specialization.
This completes the analysis of our robot arm. To summarize, given any (a, b) in
(x1, y1) coordinates, to place joint 3 at (a, b) when l2 = l3 = 1, there are
• inﬁnitely many distinct settings of joint 1 when a2 + b2 = 0,
• two distinct settings of joint 1 when a2 + b2 < 4,
• one setting of joint 1 when a2 + b2 = 4,
• no possible settings of joint 1 when a2 + b2 > 4.

308
Chapter 6 Robotics and Automatic Geometric Theorem Proving
The cases a2 + b2 = 0, 4 are examples of what are known as kinematic singularities
for this robot. We will give a precise deﬁnition of this concept and discuss some of
its meaning below.
In the exercises, you will consider the robot arm with three revolute joints and
one prismatic joint introduced in Example 2 of §2. There are more restrictions here
for the hand orientation. For example, if l4 lies in the interval [0, 1], then the hand can
be placed in any position in the closed disk of radius 3 centered at (x1, y1) = (0, 0).
However, an interesting difference is that points on the boundary circle can only be
reached with one hand orientation.
Specialization of Gröbner Bases
Before continuing our discussion of robotics, let us make some further comments
about specialization. The general version of the approach presented here involves
computing the Gröbner basis G of an ideal I ⊆k(t1, . . . , tm)[x1, . . . , xn], where
t1, . . . , tm are the parameters. In Exercises 7 and 8, you will show how to ﬁnd a
proper variety W ⊆km such that G remains a Gröbner basis under all specializa-
tions (t1, . . . , tm) 
→(a1, . . . , am) ∈km \ W.
Another approach is to turn the parameters into variables. In other words, we
work in k[x1, . . . , xm, t1, . . . , tm], which we write as k[x, t] for x = (x1, . . . , xn) and
t = (t1, . . . , tm). Assume as in (1) that our system of equations is f1 = · · · = fs = 0,
where fi ∈k[x, t] for i = 1, . . . , s. Also ﬁx a monomial order on k[x, t] with the
property that xα > xβ implies xα > xβtγ for all γ. As noted in Chapter 4, §7,
examples of such monomial orders include product orders or lex order with x1 >
· · · > xn > t1 > · · · > tm.
We will also assume that the ideal ˜I = ⟨f1, . . . , fs⟩⊆k[x, t] satisﬁes
(5)
˜I ∩k[t] = {0}.
This tells us that the equations put no constraints on the parameters. Hence we can
assign the parameters independently. Also, if the intersection (5) contains a nonzero
polynomial h(t), then the fi generate the unit ideal ⟨1⟩in k(t)[x] since h(t) ̸= 0 is
invertible in k(t).
With this setup, we have the following result which not only computes a Gröbner
basis in k(t)[x] but also makes it easy to ﬁnd a variety W ⊆km such that the Gröbner
basis specializes nicely away from W.
Proposition 1. Assume ˜I = ⟨f1, . . . , fs⟩⊆k[x, t] satisﬁes (5) and ﬁx a monomial
order as above. If G = {g1, . . . , gt} is a Gröbner basis for ˜I, then:
(i) G is a Gröbner basis for the ideal of k(t)[x] generated by the fi with respect to
the induced monomial order.
(ii) For i = 1, . . . , t, write gi ∈G in the form
gi = hi(t)xαi + terms < xαi,

§3 The Inverse Kinematic Problem and Motion Planning
309
where hi(t) ∈k[t] is nonzero. If we set W = V(h1 · · · ht) ⊆km, then for any
specialization t 
→a ∈km \ W, the gi(x, a) form a Gröbner basis with respect
to the induced monomial order for the ideal generated by the fi(x, a) in k[x].
Proof. You will prove part (i) in Exercise 10, and part (ii) follows immediately from
Theorem 2 of Chapter 4, §7 since G ∩k[t] = ∅by (5).
□
Proposition 1 can make it easy to ﬁnd specializations that preserve the Gröbner
basis. Unfortunately, the G produced by the proposition may be rather inefﬁcient as
a Gröbner basis in k(t)[x], and the corresponding W ⊆km may be too big.
To illustrate these problems, let us apply Proposition 1 to the main example of
this section. Take the ideal generated by (1) in R[c1, s1, c2, s2, a, b, l2, l3]. For the
product order built from grevlex with c1 > s1 > c2 > s2 followed by grevlex with
a > b > l2 > l3, the resulting Gröbner basis G has 13 polynomials, as opposed to
the Gröbner basis (2) over R(a, b, l2, l3)[c1, s1, c2, s2], which has four polynomials.
Also, you will compute in Exercise 11 that for G, the polynomials hi ∈R[a, b, l2, l3]
in Proposition 1 are given by
2l2l3, a2 + b2, b, a, a2 + b2 −l2
2 −l2
3, l2
2 −l2
3, 2l2l3, 1, l3, −l2, l3, l2, 1.
Then Proposition 1 implies that G remains a Gröbner basis for any specialization
where a, b, l2, l3, a2 +b2 and a2 +b2 −l2
2 −l2
3 are nonzero. In fact, we showed earlier
that we only need l2, l3 and a2 + b2 to be nonzero, though that analysis required
computations over a function ﬁeld.
Our discussion of specialization will end with the problem of ﬁnding Gröbner
bases of all possible specializations of a system of equations with parameters. This
question led to the idea of a comprehensive Gröbner basis [see, for example, the
appendix to BECKER and WEISPFENNING (1993)] and has since been reﬁned to
concept of a Gröbner cover, as deﬁned by MONTES and WIBMER (2010).
Roughly speaking, a Gröbner cover of an ideal in k[x, t] consists of pairs (Gi, Si)
for i = 1, . . . , N with the following properties:
• The Gi are ﬁnite subsets of k[x, t].
• The segments Si form a constructible partition of the parameter space km with
coordinates t = (t1, . . . , tm). More precisely, this means:
- Each Si is a constructible subset of km, as deﬁned in Chapter 4, §7.
- Si ∩Sj = ∅for i ̸= j.
- S1 ∪· · · ∪SN = km.
• The specialization Gi of Gi for t 
→a ∈Si is a Gröbner basis of the corresponding
specialization of the ideal, and Gi has the same leading terms as Gi.
The discussion of Gröbner covers in MONTES and WIBMER (2010) is more precise
and in particular addresses the issues of uniqueness and minimality. Here, we are
just trying to give the ﬂavor of what it means to be Gröbner cover.
For the robot described by (1), the Gröbner cover package in Singular partitions
R4 into 12 segments. Five segments have nontrivial Gi; the remaining seven have
Gi = {1}. Lumping these together, we get the following table:

310
Chapter 6 Robotics and Automatic Geometric Theorem Proving
Leading monomials
Segment
Comments
1
c1, s1, c2, s2
2
R4 \ V(l2l3(a2 + b2))
See (2)
2
c1, s1, c2
2

V(l2, a2 + b2 −l2
3) \ V(l3)
	
∪

V(l3, a2 + b2 −l2
2) \ V(l2)
	
See Exercise 12
3
c2
1, c2
2
V(a, b, l2, l3)
See Exercise 13
4
c1, s1, c2, s2
V(a2 + b2) \ (· · · ∪V(a, b))
Empty over R
5
c2
1, c2, s2

V(a, b, l2 −l3) \ V(l3, l2)
	
∪

V(a, b, l2 + l3) \ V(l3, l2)
	
See (4) and
Exercise 14
6
1
Everything else
No solutions
To save space, we only give the leading terms of each Gi. Segment 1 corresponds
to the Gröbner basis (2). By the analysis given earlier in the section, this remains a
Gröbner basis under specializations with l2l3(a2 + b2) ̸= 0, exactly as predicted by
Segment 1. Earlier, we also computed the Gröbner basis (4) for the specialization
a = b = 0, l2 = l3 = 1. In Exercise 14 you will relate this to Segment 5. Other
segments will be studied in the exercises.
Kinematic Singularities
We return to the geometry of robots with a discussion of kinematic singularities and
the issues they raise in robot motion planning. Our treatment will use some ideas
from multivariable calculus that we have not encountered before.
Let f : J →C be the function expressing the hand conﬁguration as a function of
the joint settings. In the explicit parametrizations of the space J that we have used,
each component of f is a differentiable function of the variables θi. For example,
this is clearly true for the mapping f for a planar robot with three revolute joints:
(6)
f(θ1, θ2, θ3) =
⎛
⎝
l3cos(θ1 + θ2) + l2cosθ1
l3sin(θ1 + θ2) + l2sinθ1
θ1 + θ2 + θ3
⎞
⎠.
Hence, we can compute the Jacobian matrix (or matrix of partial derivatives) of f
with respect to the variables θ1, θ2, θ3. We write fi for the i-th component function
of f. Then, by deﬁnition, the Jacobian matrix is
Jf (θ1, θ2, θ3) =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
∂f1
∂θ1
∂f1
∂θ2
∂f1
∂θ3
∂f2
∂θ1
∂f2
∂θ2
∂f2
∂θ3
∂f3
∂θ1
∂f3
∂θ2
∂f3
∂θ3
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.

§3 The Inverse Kinematic Problem and Motion Planning
311
For example, the mapping f in (6) has the Jacobian matrix
(7)
Jf(θ1, θ2, θ3) =
⎛
⎝
−l3sin(θ1 + θ2) −l2sinθ1 −l3sin(θ1 + θ2)
0
l3cos(θ1 + θ2) + l2cosθ1
l3cos(θ1 + θ2) 0
1
1
1
⎞
⎠.
From the matrix of functions Jf , we obtain matrices with constant entries by
substituting particular values j = (θ1, θ2, θ3). We will write Jf (j) for the substituted
matrix, which plays an important role in multivariable calculus. Its key property is
that Jf (j) deﬁnes a linear mapping which is the best linear approximation of the
function f at j ∈J . This means that near j, the function f and the linear function
given by Jf (j) have roughly the same behavior. In this sense, Jf(j) represents the
derivative of the mapping f at j ∈J .
To deﬁne what is meant by a kinematic singularity, we need ﬁrst to assign dimen-
sions to the joint space J and the conﬁguration space C for our robot, to be denoted
by dim(J ) and dim(C), respectively. We will do this in a very intuitive way. The
dimension of J , for example, will be simply the number of independent "degrees
of freedom" we have in setting the joints. Each planar joint (revolute or prismatic)
contributes 1 dimension to J . Note that this yields a dimension of 3 for the joint
space of the plane robot with three revolute joints. Similarly, dim(C) will be the
number of independent degrees of freedom we have in the conﬁguration (position
and orientation) of the hand. For our planar robot, this dimension is also 3.
In general, suppose we have a robot with dim(J ) = m and dim(C) = n. Then
differentiating f as before, we will obtain an n × m Jacobian matrix of functions. If
we substitute in j ∈J , we get the linear map Jf (j) : Rm →Rn that best approxi-
mates f near j. An important invariant of a matrix is its rank, which is the maximal
number of linearly independent columns (or rows). The exercises will review some
of the properties of the rank. Since Jf (j) is an n × m matrix, its rank will always be
less than or equal to min(m, n). For instance, consider our planar robot with three
revolute joints and l2 = l3 = 1. If we let j = (0, π
2 , π
3 ), then formula (7) gives us
Jf

0, π
2 , π
3

=
⎛
⎝
−1 −1 0
1
0
0
1
1
1
⎞
⎠.
This matrix has rank exactly 3 (the largest possible in this case).
We say that Jf (j) has maximal rank if its rank is min(m, n) (the largest possible
value), and, otherwise, Jf (j) has deﬁcient rank. When a matrix has deﬁcient rank,
its kernel is larger and image smaller than one would expect (see Exercise 19).
Since Jf(j) closely approximates f, Jf (j) having deﬁcient rank should indicate some
special or "singular" behavior of f itself near the point j. Hence, we introduce the
following deﬁnition.
Deﬁnition 2. A kinematic singularity for a robot is a point j ∈J such that Jf (j)
has rank strictly less than min(dim(J ), dim(C)).

312
Chapter 6 Robotics and Automatic Geometric Theorem Proving
For example, the kinematic singularities of the 3-revolute joint robot occur ex-
actly when the matrix (7) has rank ≤2. For square n × n matrices, having deﬁcient
rank is equivalent to the vanishing of the determinant. We have
0 = det(Jf ) = sin(θ1 + θ2)cosθ1 −cos(θ1 + θ2)sinθ1
= sin((θ1 + θ2) −θ1) = sinθ2
if and only if θ2 = 0 or θ2 = π. Note that θ2 = 0 corresponds to a position in
which segment 3 extends past segment 2 along the positive x2-axis, whereas θ2 = π
corresponds to a position in which segment 3 is folded back along segment 2. These
are exactly the two special conﬁgurations we found earlier in which there are not
exactly two joint settings yielding a particular hand conﬁguration.
Kinematic singularities are essentially unavoidable for planar robot arms with
three or more revolute joints.
Proposition 3. Let f : J →C be the conﬁguration mapping for a planar robot with
n ≥3 revolute joints. Then there exist kinematic singularities j ∈J .
Proof. By Exercise 7 of §2, we know that f has the form
f(θ1, . . . , θn) =
⎛
⎜
⎜
⎝
n−1
i=1 li+1cos
 i
j=1 θj
	
n−1
i=1 li+1sin
 i
j=1 θj
	
n
i=1 θi
⎞
⎟
⎟
⎠.
Hence, the Jacobian matrix Jf will be the 3 × n matrix
⎛
⎜
⎜
⎝
−n−1
i=1 li+1sin
 i
j=1 θj
	
−n−1
i=2 li+1sin
 i
j=1 θj
	
· · · −lnsin(θn−1) 0
n−1
i=1 li+1cos
 i
j=1 θj
	
n−1
i=2 li+1cos
 i
j=1 θj
	
· · ·
lncos(θn−1) 0
1
1
· · ·
1
1
⎞
⎟
⎟
⎠.
Since we assume n ≥3, by the deﬁnition, a kinematic singularity is a point where
the rank of Jf is ≤2. If j ∈J is a point where all θi ∈{0, π}, then every entry of
the ﬁrst row of Jf(j) is zero. Hence, rank Jf (j) ≤2 for those j.
□
Descriptions of the possible motions of robots such as the ones we have devel-
oped are used in an essential way in planning the motions of the robot needed to
accomplish the tasks that are set for it. The methods we have sketched are suitable
(at least in theory) for implementation in programs to control robot motion automat-
ically. The main goal of such a program would be to instruct the robot what joint
setting changes to make in order to take the hand from one position to another. The
basic problems to be solved here would be ﬁrst, to ﬁnd a parametrized path c(t) ∈C
starting at the initial hand conﬁguration and ending at the new desired conﬁguration,
and second, to ﬁnd a corresponding path j(t) ∈J such that f(j(t)) = c(t) for all t.
In addition, we might want to impose extra constraints on the paths used such as the
following:

§3 The Inverse Kinematic Problem and Motion Planning
313
1. If the conﬁguration space path c(t) is closed (i.e., if the starting and ﬁnal con-
ﬁgurations are the same), we might also want path j(t) to be a closed path. This
would be especially important for robots performing a repetitive task such as
making a certain weld on an automobile body. Making certain the joint space
path is closed means that the whole cycle of joint setting changes can simply be
repeated to perform the task again.
2. In any real robot, we would want to limit the joint speeds necessary to perform the
prescribed motion. Overly fast (or rough) motions could damage the mechanisms.
3. We would want to do as little total joint movement as possible to perform each
motion.
Kinematic singularities have an important role to play in motion planning. To see the
undesirable behavior that can occur, suppose we have a conﬁguration space path c(t)
such that the corresponding joint space path j(t) passes through or near a kinematic
singularity. Using the multivariable chain rule, we can differentiate c(t) = f(j(t))
with respect to t to obtain
(8)
c′(t) = Jf (j(t)) · j′(t).
We can interpret c′(t) as the velocity of our conﬁguration space path, whereas j′(t)
is the corresponding joint space velocity. If at some time t0 our joint space path
passes through a kinematic singularity for our robot, then, because Jf(j(t0)) is a
matrix of deﬁcient rank, equation (8) may have no solution for j′(t0), which means
there may be no smooth joint paths j(t) corresponding to conﬁguration paths that
move in certain directions. As an example, consider the kinematic singularities with
θ2 = π for our planar robot with three revolute joints. If θ1 = 0, then segments 2
and 3 point along the x1-axis:
segment 1
θ 1 = 0
segment 2
segment 3
θ 2 = π
Can the hand move
in the x1-direction?
At a kinematic singularity
With segment 3 folded back along segment 2, there is no way to move the hand in
the x1-direction. More precisely, suppose that we have a conﬁguration path such that
c′(t0) is in the direction of the x1-axis. Then, using formula (7) for Jf , equation (8)
becomes
c′(t0) = Jf(t0) · j′(t0) =
⎛
⎝
0
0
0
0 −1
0
1
1
1
⎞
⎠· j′(t0).

314
Chapter 6 Robotics and Automatic Geometric Theorem Proving
Because the top row of Jf (t0) is identically zero, this equation has no solution for
j′(t0) since we want the x1 component of c′(t0) to be nonzero. Thus, c(t) is a con-
ﬁguration path for which there is no corresponding smooth path in joint space. This
is typical of what can go wrong at a kinematic singularity.
For j(t0) near a kinematic singularity, we may still have bad behavior since
Jf(j(t0)) may be close to a matrix of deﬁcient rank. Using techniques from num-
erical linear algebra, it can be shown that in (8), if Jf (j(t0)) is close to a matrix of
deﬁcient rank, very large joint space velocities may be needed to achieve a small
conﬁguration space velocity. For a simple example of this phenomenon, again con-
sider the kinematic singularities of our planar robot with 3 revolute joints with
θ2 = π (where segment 3 is folded back along segment 2). As the diagram below
suggests, in order to move from position A to position B, both near the origin, a large
change in θ1 will be needed to move the hand a short distance.
Δθ 1
Δx1
Near a kinematic singularity
A
B
To avoid undesirable situations such as this, care must be taken in specifying
the desired conﬁguration space path c(t). The study of methods for doing this in a
systematic way is an active ﬁeld of current research in robotics, and unfortunately
beyond the scope of this text. For readers who wish to pursue this topic further,
a standard basic reference on robotics is the text by PAUL (1981). The survey in
BUCHBERGER (1985) contains another discussion of Gröbner basis methods for
the inverse kinematic problem. A readable introduction to the inverse kinematic
problem and motion control, with references to the original research papers, is given
in BAILLIEUL ET AL. (1990).
EXERCISES FOR §3
1. Consider the specialization of the Gröbner basis (2) to the case l2 = l3 = 1.
a. First, substitute l2 = l3 = 1 directly into (2) and simplify to obtain (3).
b. Now, set l2 = l3 = 1 in (1) and compute a Gröbner basis for the "specialized" ideal
generated by (1), again using grevlex order with c1 > s1 > c2 > s2. The result should
be (3), just as in part (a).
2. This exercise studies the geometry of the planar robot with three revolute joints discussed
in the text with the dimensions specialized to l2 = l3 = 1.
a. Draw a diagram illustrating the two solutions of the inverse kinematic problem for the
robot in the general case 0 < a2 + b2 < 4.

§3 The Inverse Kinematic Problem and Motion Planning
315
b. Explain geometrically why the equations for c2 and s2 in (3) do not involve c1 and s1.
Hint: Use the geometry of the diagram from part (a) to determine how the two values
of θ2 are related.
3. Consider the robot arm discussed in the text. Setting l2 = l3 = 1 and a = b = 0 in (1)
gives the Gröbner basis (4). How is this basis different from the basis (3), which only
assumes l2 = l3 = 1? How does this difference explain the properties of the kinematic
singularity at (0, 0)?
4. In this exercise, you will study the geometry of the robot discussed in the text when l2 ̸= l3.
a. Set l2 = 1, l3 = 2 and solve the system (2) for c1, s1, c2, s2. Interpret your results
geometrically, identifying and explaining all special cases. How is this case different
from the case l2 = l3 = 1 done in the text?
b. Now, set l2 = 2, l3 = 1 and answer the same questions as in part (a).
As we know from the examples in the text, the form of a Gröbner basis for an ideal can
change if symbolic parameters appearing in the coefﬁcients are specialized. In Exercises 5-9,
we will study some further examples of this phenomenon and prove some general results.
5. We begin with another example of how denominators in a Gröbner basis can cause
problems under specialization. Consider the ideal I = ⟨f , g⟩, where f = x2 −y, g =
(y −tx)(y −t) = −txy + t2x + y2 −ty, and t is a symbolic parameter. We will use lex
order with x > y.
a. Compute a reduced Gröbner basis for I in R(t)[x, y]. What polynomials in t appear in
the denominators in this basis?
b. Now set t = 0 in f , g and recompute a Gröbner basis. How is this basis different from
the one in part (a)? What if we clear denominators in the basis from part (a) and set
t = 0?
c. How do the points in the variety V(I) ⊆R2 depend on the choice of t ∈R. Is it
reasonable that t = 0 is a special case?
d. The ﬁrst step of Buchberger's algorithm to compute a Gröbner basis for I would be to
compute the S-polynomial S( f , g). Compute this S-polynomial by hand in R(t)[x, y].
Note that the special case t = 0 is already distinguished at this step.
6. This exercise will explore a more subtle example of what can go wrong during a spe-
cialization. Consider the ideal I = ⟨x + ty, x + y⟩⊆R(t)[x, y], where t is a symbolic
parameter. We will use lex order with x > y.
a. Show that {x, y} is a reduced Gröbner basis of I. Note that neither the original basis
nor the Gröbner basis have any denominators.
b. Let t = 1 and show that {x+y} is a Gröbner basis for the specialized ideal ¯I ⊆R[x, y].
c. To see why t = 1 is special, express the Gröbner basis {x, y} in terms of the original
basis {x + ty, x + y}. What denominators do you see? In the next problem, we will
explore the general case of what is happening here.
7. In this exercise, we will derive a condition under which the form of a Gröbner basis does
not change under specialization. Consider the ideal
I = ⟨f1(x, t), . . . , fs(x, t)⟩⊆k(t)[x],
where t = (t1, . . . , tm) and x = (x1, . . . , xn). We think of the ti as symbolic parameters
appearing in the coefﬁcients of f1, . . . , fs. Also ﬁx a monomial order. By dividing each fi
by its leading coefﬁcient [which lies in k(t)], we may assume that the leading coefﬁcients
of the fi are equal to 1. Then let {g1, . . . , gt} be a reduced Gröbner basis for I. Thus the
leading coefﬁcients of the gi are also 1. Finally, let t →a ∈km be a specialization of the
parameters such that none of the denominators of the fi or gi vanish at a.
a. If we use the division algorithm to ﬁnd Aij ∈k(t)[x] such that

316
Chapter 6 Robotics and Automatic Geometric Theorem Proving
fi =
t

j=1
Aijgj,
then show that none of the denominators of Aij vanish at a.
b. We also know that gj can be written
gj =
s

i=1
Bji fi,
for some Bij ∈k(t)[x]. As Exercise 6 shows, the Bji may introduce new denominators.
So assume, in addition, that none of the denominators of the Bji vanish under the
specialization t →a. Let ¯I denote the ideal in k[x] generated by the specialized fi.
Under these assumptions, prove that the specialized gj form a basis of ¯I.
c. Show that the specialized gj form a Gröbner basis for ¯I. Hint: The monomial order
used to compute ¯I only deals with terms in the variables xj. The parameters tj are
"constants" as far as the ordering is concerned.
d. Let d1, . . . , dM ∈k[t] be all denominators that appear among fi, gj, and Bji, and let
W = V(d1 · d2 · · · dM) ⊆km. Conclude that the gj remain a Gröbner basis for the fi
under all specializations t →a ∈km \ W.
8. We next describe an algorithm for ﬁnding which specializations preserve a Gröbner ba-
sis. We will use the notation of Exercise 7. Thus, we want an algorithm for ﬁnding the
denominators d1, . . . , dM appearing in the fi, gj, and Bji. This is easy to do for the fi and
gj, but the Bji are more difﬁcult. The problem is that since the fi are not a Gröbner basis,
we cannot use the division algorithm to ﬁnd the Bji. Fortunately, we only need the de-
nominators. The idea is to work in the ring k[x, t]. If we multiply the fi and gj by suitable
polynomials in k[t], we get
˜fi, ˜gj ∈k[x, t].
Let ˜I ⊆k(t)[x] be the ideal generated by the ˜fi.
a. Suppose gj = s
i=1 Bji fi in k(t)[x] and let d ∈k[t] be a polynomial that clears all
denominators for the gj, the fi, and the Bji. Then prove that
d ∈(˜I : ˜gj) ∩k[t],
where ˜I : ˜gj is the ideal quotient as deﬁned in §4 of Chapter 4.
b. Give an algorithm for computing (˜I : ˜gj) ∩k[t] and use this to describe an algorithm
for ﬁnding the subset W ⊆km described in part (d) of Exercise 7.
9. The algorithm described in Exercise 8 can lead to lengthy calculations which may be too
much for some computer algebra systems. Fortunately, quicker methods are available in
some cases. Let fi, gj ∈k(t)[x] be as in Exercises 7 and 8, and suppose we suspect that the
gj will remain a Gröbner basis for the fi under all specializations where the denominators
of the fi and gj do not vanish. How can we check this quickly?
a. Let d ∈k[t] be the least common multiple of all denominators in the fi and gj and let
˜fi, ˜gj ∈k[x, t] be the polynomials we get by clearing denominators. Finally, let ˜I be
the ideal in k[x, t] generated by the ˜fi. If d˜gj ∈˜I for all j, then prove that specialization
works for all t →a ∈km \ V(d).
b. Describe an algorithm for checking the criterion given in part (a). For efﬁciency, what
monomial order should be used?
c. Apply the algorithm of part (b) to equations (1) in the text. This will prove that (2)
remains a Gröbner basis for (1) under all specializations where l2 ̸= 0, l3 ̸= 0, and
a2 + b2 ̸= 0.

§3 The Inverse Kinematic Problem and Motion Planning
317
10. This exercise is concerned with part (i) of Proposition 1. Fix a monomial on order k[x, t]
such that xα > xβ implies xα > xβtγ for all γ. When restricted to monomials involving
only the xi, we get a monomial order on k(t)[x] called the induced order.
a. A nonzero polynomial f ∈k[x, t] has leading monomial LM( f ) = xαtβ. If we regard
f as an element of k(t)[x], prove that its leading monomial with respect to the induced
order is xα.
b. Use part (a) to prove part (i) of Proposition 1.
11. Consider the ideal generated by (1) in R[c1, s1, c2, s2, a, b, l2, l3].
a. Compute a Gröbner basis G using the product order built from grevlex with c1 >
s1 > c2 > s2 followed by grevlex with a > b > l2 > l3.
b. For each gi ∈G, compute the corresponding polynomial ci ∈R[a, b, l2, l3].
12. In the text, we discussed the Gröbner cover for the system (1). In this exercise, we will
explore Segment 2, where the set G2 consists of the polynomials
(l2
2 + l2
3)c1 −al3c2 −bl3s2 −al2,
(l2
2 + l2
3)s1 −bl3c2 + al3s2 −bl2,
c2
2 + s2
2 −1.
a. On the ﬁrst part of Segment 2, we have l2 = 0, a2 + b2 = l2
3 ̸= 0. Simplify the
above polynomials and conﬁrm that the leading monomials are c1, s1, c2
2. Also explain
geometrically why the resulting equations have inﬁnitely many solutions.
b. Do a similar analysis on the second part of Segment 2, where l3 = 0, a2+b2 = l2
2 ̸= 0.
13. The most degenerate specialization of (1) is where a = b = l2 = l3 = 0. What system
of equations do we get in this case and how does it relate to Segment 3 in the Gröbner
cover of this system?
14. Explain how (4) relates to Segment 5 of the Gröbner cover of (1). Also explain why the
second part of the segment is not relevant to this robot arm.
15. Consider the planar robot with two revolute joints and one prismatic joint described in
Exercise 4 of §2.
a. Given a desired hand position and orientation, set up a system of equations as in (1)
of this section whose solutions give the possible joint settings to reach that hand
conﬁguration. Take the length of segment 2 to be 1.
b. Using a computer algebra system, solve your equations by computing a Gröbner basis
for the ideal generated by the equations from part (a) with respect to a suitable lex
order. Note: Some experimentation may be necessary to ﬁnd a reasonable variable
order.
c. What is the solution of the inverse kinematic problem for this robot. In other words,
which hand positions and orientations are possible? How many different joint settings
yield a given hand conﬁguration? (Do not forget that the setting of the prismatic joint
is limited to a ﬁnite interval in [0, m3] ⊆R.)
d. Does this robot have any kinematic singularities according to Deﬁnition 2? If so,
describe them.
16. Consider the planar robot with three joints and one prismatic joint that we studied in
Example 2 of §2. We will assume that segments 2 and 3 have length 1, and that segment
4 varies in length between 1 and 2. The hand conﬁguration consists of a position (a, b)
and an orientation θ. We will study this robot using approach suggested by Exercise 3 of
§2, which involves parameters c = cosθ and s = sinθ.
a. Set up a system of equations as in (1) of this section whose solutions give the possible
joint settings to reach the hand position (a, b) and orientation (c, s). Hint: If the joint
angles are θ1, θ2, θ3, then recall that θ = θ1 + θ2 + θ3.
b. Compute a Gröbner basis over R(a, b, c, s)[c1, s1, c2, s2, c3, s3, l4]. Your answer should
be {1}.

318
Chapter 6 Robotics and Automatic Geometric Theorem Proving
c. Show that a Gröbner basis over R[c1, s1, c2, s2, c3, s3, l4, a, b, c, s] for a suitable mono-
mial order contains c2 + s2 −1 and use this to explain the result of part (b).
17. Here we take a different approach to the robot arm of Exercise 16. The idea is to make
c and s variables and write the equations in terms of the cosine and sine of θ1, θ2 and
θ = θ1 + θ2 + θ3. Also, by rotating the entire conﬁguration, we can assume that the hand
position is (a, 0).
a. Given the hand position (a, 0), set up a system of equations for the possible joint
settings. The equations will have a as a parameter and variables c1, s1, c2, s2, c, s, l4.
b. Solve your equations by computing a Gröbner basis for a suitable monomial order.
c. What is the solution of the inverse kinematic problem for this robot? In other words,
which hand positions and orientations are possible? How does the set of possible hand
orientations vary with the position? (Do not forget that the setting l4 of the prismatic
joint is limited to the ﬁnite interval in [1, 2] ⊆R.)
d. How many different joint settings yield a given hand conﬁguration in general? Are
there special cases?
e. Does this robot have any kinematic singularities according to Deﬁnition 2? If so,
describe the corresponding robot conﬁgurations and relate them to part (d).
18. Consider the 3-dimensional robot with two "spin" joints from Exercise 8 of §2.
a. Given a desired hand position and orientation, set up a system of equations as in (1)
of this section whose solutions give the possible joint settings to reach that hand
conﬁguration. Take the length of segment 2 to be 4, and the length of segment 3 to be
2, if you like.
b. Solve your equations by computing a Gröbner basis for the ideal generated by your
equations with respect to a suitable lex order. Note: In this case there will be an ele-
ment of the Gröbner basis that depends only on the hand position coordinates. What
does this mean geometrically? Is your answer reasonable in terms of the geometry of
this robot?
c. What is the solution of the inverse kinematic problem for this robot? That is, which
hand positions and orientations are possible?
d. How many different joint settings yield a given hand conﬁguration in general? Are
there special cases?
e. Does this robot have any kinematic singularities according to Deﬁnition 2?
19. Let A be an m × n matrix with real entries. We will study the rank of A, which is the
maximal number of linearly independent columns (or rows) in A. Multiplication by A
gives a linear map LA : Rn →Rm, and from linear algebra, we know that the rank of
A is the dimension of the image of LA. As in the text, A has maximal rank if its rank is
min(m, n). To understand what maximal rank means, there are three cases to consider.
a. If m = n, show that A has maximal rank ⇔det(A) ̸= 0 ⇔LA is an isomorphism of
vector spaces.
b. If m < n, show that A has maximal rank ⇔the equation A · x = b has a solution for
all b ∈Rm ⇔LA is an onto mapping.
c. If m > n, show that A has maximal rank ⇔the equation A · x = b has at most one
solution for all b ∈Rm ⇔LA is a one-to-one mapping.
20. A robot is said to be kinematically redundant if the dimension of its joint space J is
larger than the dimension of its conﬁguration space C.
a. Which of the robots considered in this section (in the text and in Exercises 15-18
above) are kinematically redundant?
b. (This part requires knowledge of the Implicit Function Theorem.) Suppose we have a
kinematically redundant robot and j ∈J is not a kinematic singularity. What can be
said about the inverse image f −1( f (j)) in J ? In particular, how many different ways
are there to put the robot in the conﬁguration given by f (j)?
21. Verify the chain rule formula (8) explicitly for the planar robot with three revolute joints.
Hint: Substitute θi = θi(t) and compute the derivative of the conﬁguration space path
f (θ1(t), θ2(t), θ3(t)) with respect to t.

§4 Automatic Geometric Theorem Proving
319
§4
Automatic Geometric Theorem Proving
The geometric descriptions of robots and robot motion we studied in the ﬁrst three
sections of this chapter were designed to be used as tools by a control program to
help plan the motions of the robot to accomplish a given task. In the process, the
control program could be said to be "reasoning" about the geometric constraints
given by the robot's design and its environment and to be "deducing" a feasible
solution to the given motion problem. In this section and in the next, we will exa-
mine a second subject which has some of the same ﬂavor—automated geometric
reasoning in general. We will give two algorithmic methods for determining the
validity of general statements in Euclidean geometry. Such methods are of interest
to researchers both in artiﬁcial intelligence (AI) and in geometric modeling because
they have been used in the design of programs that, in effect, can prove or disprove
conjectured relationships between, or theorems about, plane geometric objects.
Few people would claim that such programs embody an understanding of the
meaning of geometric statements comparable to that of a human geometer. Indeed,
the whole question of whether a computer is capable of intelligent behavior is one
that is still completely unresolved. However, it is interesting to note that a number
of new (i.e., apparently previously unknown) theorems have been veriﬁed by these
methods. In a limited sense, these "theorem provers" are capable of "reasoning"
about geometric conﬁgurations, an area often considered to be solely the domain of
human intelligence.
The basic idea underlying the methods we will consider is that once we introduce
Cartesian coordinates in the Euclidean plane, the hypotheses and the conclusions
of a large class of geometric theorems can be expressed as polynomial equations
between the coordinates of collections of points speciﬁed in the statements. Here is
a simple but representative example.
Example 1. Let A, B, C, D be the vertices of a parallelogram in the plane, as in the
ﬁgure below.
A
B
C
D
N
It is a standard geometric theorem that the two diagonals AD and BC of any paral-
lelogram intersect at a point (N in the ﬁgure) which bisects both diagonals. In other
words, AN = DN and BN = CN, where, as usual, XY denotes the length of the line

320
Chapter 6 Robotics and Automatic Geometric Theorem Proving
segment XY joining the two points X and Y. The usual proof from geometry is based
on showing that the triangles ΔANC and ΔBND are congruent. See Exercise 1.
To relate this theorem to algebraic geometry, we will show how the conﬁgura-
tion of the parallelogram and its diagonals (the hypotheses of the theorem) and the
statement that the point N bisects the diagonals (the conclusion of the theorem) can
be expressed in polynomial form.
The properties of parallelograms are unchanged under translations and rotations
in the plane. Hence, we may begin by translating and rotating the parallelogram
to place it in any position we like, or equivalently, by choosing our coordinates in
any convenient fashion. The simplest way to proceed is as follows. We place the
vertex A at the origin and align the side AB with the horizontal coordinate axis. In
other words, we can take A = (0, 0) and B = (u1, 0) for some u1 ̸= 0 ∈R. In
what follows we will think of u1 as an indeterminate or variable whose value can be
chosen arbitrarily in R \ {0}. The vertex C of the parallelogram can be at any point
C = (u2, u3), where u2, u3 are new indeterminates independent of u1, and u3 ̸= 0.
The remaining vertex D is now completely determined by the choice of A, B, C.
It will always be true that when constructing the geometric conﬁguration des-
cribed by a theorem, some of the coordinates of some points will be arbitrary,
whereas the remaining coordinates of points will be determined (possibly up to a
ﬁnite number of choices) by the arbitrary ones. To indicate arbitrary coordinates, we
will consistently use variables ui, whereas the other coordinates will be denoted xj.
It is important to note that this division of coordinates into two subsets is in no way
uniquely speciﬁed by the hypotheses of the theorem. Different constructions of a
ﬁgure, for example, may lead to different sets of arbitrary variables and to different
translations of the hypotheses into polynomial equations.
Since D is determined by A, B, and C, we will write D = (x1, x2). One hypothesis
of our theorem is that the quadrilateral ABDC is a parallelogram or, equivalently,
that the opposite pairs of sides are parallel and, hence, have the same slope. Using
the slope formula for a line segment, we see that one translation of these statements
is as follows:
AB ∥CD : 0 = x2 −u3
x1 −u2
,
AC ∥BD : u3
u2
=
x2
x1 −u1
.
Clearing denominators, we obtain the polynomial equations
(1)
h1 = x2 −u3 = 0,
h2 = (x1 −u1)u3 −x2u2 = 0.
(Below, we will discuss another way to get equations for x1 and x2.)
Next, we construct the intersection point of the diagonals of the parallelogram.
Since the coordinates of the intersection point N are determined by the other data,
we write N = (x3, x4). Saying that N is the intersection of the diagonals is equiva-
lent to saying that N lies on both of the lines AD and BC, or to saying that the triples

§4 Automatic Geometric Theorem Proving
321
A, N, D and B, N, C are collinear. The latter form of the statement leads to the sim-
plest formulation of these hypotheses. Using the slope formula again, we have the
following relations:
A, N, D collinear : x4
x3
= x2
x1
,
B, N, C collinear :
x4
x3 −u1
=
u3
u2 −u1
.
Clearing denominators again, we have the polynomial equations
(2)
h3 = x4x1 −x3x2 = 0,
h4 = x4(u2 −u1) −(x3 −u1)u3 = 0.
The system of four equations formed from (1) and (2) gives one translation of the
hypotheses of our theorem.
The conclusions can be written in polynomial form by using the distance formula
for two points in the plane (the Pythagorean Theorem) and squaring:
AN = ND : x2
3 + x2
4 = (x3 −x1)2 + (x4 −x2)2,
BN = NC : (x3 −u1)2 + x2
4 = (x3 −u2)2 + (x4 −u3)2.
Canceling like terms, the conclusions can be written as
(3)
g1 = x2
1 −2x1x3 −2x4x2 + x2
2 = 0,
g2 = 2x3u1 −2x3u2 −2x4u3 −u2
1 + u2
2 + u2
3 = 0.
Our translation of the theorem states that the two equations in (3) should hold when
the hypotheses in (1) and (2) hold.
As we noted earlier, different translations of the hypotheses and conclusions of a
theorem are possible. For instance, see Exercise 2 for a different translation of this
theorem based on a different construction of the parallelogram (i.e., a different col-
lection of arbitrary coordinates). There is also a great deal of freedom in the way that
hypotheses can be translated. For example, the way we represented the hypothesis
that ABDC is a parallelogram in (1) is typical of the way a computer program might
translate these statements, based on a general method for handling the hypothesis
AB ∥CD. But there is an alternate translation based on the observation that, from the
parallelogram law for vector addition, the coordinate vector of the point D should
simply be the vector sum of the coordinate vectors B = (u1, 0) and C = (u2, u3).
Writing D = (x1, x2), this alternate translation would be
(4)
h′
1 = x1 −u1 −u2 = 0,
h′
2 = x2 −u3 = 0.
These equations are much simpler than the ones in (1). If we wanted to design a
geometric theorem-prover that could translate the hypothesis "ABDC is a parallelo-

322
Chapter 6 Robotics and Automatic Geometric Theorem Proving
gram" directly (without reducing it to the equivalent form "AB ∥CD and AC ∥BD"),
the translation (4) would be preferable to (1).
Further, we could also use h′
2 to eliminate the variable x2 from the hypotheses and
conclusions, yielding an even simpler system of equations. In fact, with complicated
geometric constructions, preparatory simpliﬁcations of this kind can sometimes be
necessary. They often lead to much more tractable systems of equations.
The following proposition lists some of the most common geometric statements
that can be translated into polynomial equations.
Proposition 2. Let A, B, C, D, E, F be points in the plane. Each of the following
geometric statements can be expressed by one or more polynomial equations:
(i) AB is parallel to CD.
(ii) AB is perpendicular to CD.
(iii) A, B, C are collinear.
(iv) The distance from A to B is equal to the distance from C to D: AB = CD.
(v) C lies on the circle with center A and radius AB.
(vi) C is the midpoint of AB.
(vii) The acute angle ∠ABC is equal to the acute angle ∠DEF
(viii) BD bisects the angle ∠ABC.
Proof. General methods for translating statements (i), (iii), and (iv) were illustrated
in Example 1; the general cases are exactly the same. Statement (v) is equivalent
to AC = AB. Hence, it is a special case of (iv) and can be treated in the same
way. Statement (vi) can be reduced to a conjunction of two statements: A, C, B are
collinear, and AC = CB. We, thus, obtain two equations from (iii) and (iv). Finally,
(ii), (vii), and (viii) are left to the reader in Exercise 4.
□
Exercise 3 gives several other types of statements that can be translated into
polynomial equations. We will say that a geometric theorem is admissible if both its
hypotheses and its conclusions admit translations into polynomial equations. There
are always many different equivalent formulations of an admissible theorem; the
translation will never be unique.
Correctly translating the hypotheses of a theorem into a system of polynomial
equations can be accomplished most readily if we think of constructing a ﬁgure
illustrating the conﬁguration in question point by point. This is exactly the process
used in Example 1 and in the following example.
Example 3. We will use Proposition 2 to translate the following beautiful result into
polynomial equations.
Theorem (The Circle Theorem of Apollonius). Let ΔABC be a right triangle in
the plane, with right angle at A. The midpoints of the three sides and the foot of the
altitude drawn from A to BC all lie on one circle.
The theorem is illustrated in the following ﬁgure:

§4 Automatic Geometric Theorem Proving
323
A
B
C
H
M1
M2
M3
In Exercise 1, you will give a conventional geometric proof of the Circle Theorem.
Here we will make the translation to polynomial form, showing that the Circle The-
orem is admissible. We begin by constructing the triangle. Placing A at (0,0) and B
at (u1, 0), the hypothesis that ∠CAB is a right angle says C = (0, u2). (Of course,
we are taking a shortcut here; we could also make C a general point and add the
hypothesis CA ⊥AB, but that would lead to more variables and more equations.)
Next, we construct the three midpoints of the sides. These points have coordi-
nates M1 = (x1, 0), M2 = (0, x2), and M3 = (x3, x4). As in Example 1, we use the
convention that u1, u2 are to be arbitrary, whereas the xj are determined by the values
of u1, u2. Using part (vi) of Proposition 2, we obtain the equations
(5)
h1 = 2x1 −u1 = 0,
h2 = 2x2 −u2 = 0,
h3 = 2x3 −u1 = 0,
h4 = 2x4 −u2 = 0.
The next step is to construct the point H = (x5, x6), the foot of the altitude drawn
from A. We have two hypotheses here:
(6)
B, H, C collinear : h5 = u2x5 + u1x6 −u1u2 = 0,
AH ⊥BC : h6 = u1x5 −u2x6 = 0.
Finally, we must consider the statement that M1, M2, M3, H lie on a circle. A gen-
eral collection of four points in the plane lies on no single circle (this is why the
statement of the Circle Theorem is interesting). But three noncollinear points always
do lie on a circle (the circumscribed circle of the triangle they form). Thus, our
conclusion can be restated as follows: if we construct the circle containing the non-
collinear triple M1, M2, M3, then H must lie on this circle also. To apply part (v) of
Proposition 2, we must know the center of the circle, so this is an additional point
that must be constructed. We call the center O = (x7, x8) and derive two additional
hypotheses:

324
Chapter 6 Robotics and Automatic Geometric Theorem Proving
(7)
M1O = M2O : h7 = (x1 −x7)2 + x2
8 −x2
7 −(x8 −x2)2 = 0,
M1O = M3O : h8 = (x1 −x7)2 + (0 −x8)2 −(x3 −x7)2 −(x4 −x8)2 = 0.
Our conclusion is HO = M1O, which takes the form
(8)
g = (x5 −x7)2 + (x6 −x8)2 −(x1 −x7)2 −x2
8 = 0.
We remark that both here and in Example 1, the number of hypotheses and the
number of dependent variables xj are the same. This is typical of properly posed
geometric hypotheses. We expect that given values for the ui, there should be at
most ﬁnitely many different combinations of xj satisfying the equations.
We now consider the typical form of an admissible geometric theorem. We will
have some number of arbitrary coordinates, or independent variables in our con-
struction, denoted by u1, . . . , um. In addition, there will be some collection of dep-
endent variables x1, . . . , xn. The hypotheses of the theorem will be represented by
a collection of polynomial equations in the ui, xj. As we noted in Example 3, it is
typical of a properly posed theorem that the number of hypotheses is equal to the
number of dependent variables, so we will write the hypotheses as
h1(u1, . . . , um, x1, . . . , xn) = 0,
...
(9)
hn(u1, . . . , um, x1, . . . , xn) = 0.
The conclusions of the theorem will also be expressed as polynomials in the ui, xj.
It sufﬁces to consider the case of one conclusion since if there are more, we can
simply treat them one at a time. Hence, we will write the conclusion as
g(u1, . . . , um, x1, . . . , xn) = 0.
The question to be addressed is: how can the fact that g follows from h1, . . . , hn
be deduced algebraically? The basic idea is that we want g to vanish whenever
h1, . . . , hn do. We observe that the hypotheses (9) are equations that deﬁne a variety
V = V(h1, . . . , hn) ⊆Rm+n.
This leads to the following deﬁnition.
Deﬁnition 4. The conclusion g follows strictly from the hypotheses h1, . . . , hn if
g ∈I(V) ⊆R[u1, . . . , um, x1, . . . , xn], where V = V(h1, . . . , hn).
Although this deﬁnition seems reasonable, we will see later that it is too strict.
Most geometric theorems have some "degenerate" cases that Deﬁnition 4 does not
take into account. But for the time being, we will use the above notion of "follows
strictly."

§4 Automatic Geometric Theorem Proving
325
One drawback of Deﬁnition 4 is that because we are working over R, we do
not have an effective method for determining I(V). But we still have the following
useful criterion.
Proposition 5. If g ∈
"
⟨h1, . . . , hn⟩, then g follows strictly from h1, . . . , hn.
Proof. The hypothesis g ∈
"
⟨h1, . . . , hn⟩implies that gs ∈⟨h1, . . . , hn⟩for some s.
Thus, gs = n
i=1 Aihi, where Ai ∈R[u1, . . . , um, x1, . . . , xn]. Then gs, and, hence, g
itself, must vanish whenever h1, . . . , hn do.
□
Note that the converse of this proposition fails whenever
"
⟨h1, . . . , hn⟩⊊I(V),
which can easily happen when working over R. Nevertheless, Proposition 5 is still
useful because we can test whether g ∈
"
⟨h1, . . . , hn⟩using the radical mem-
bership algorithm from Chapter 4, §2. Let ¯I = ⟨h1, . . . , hn, 1 −yg⟩in the ring
R[u1, . . . , um, x1, . . . , xn, y]. Then Proposition 8 of Chapter 4, §2 implies that
g ∈
"
⟨h1, . . . , hn⟩⇐⇒{1} is the reduced Gröbner basis of ¯I.
If this condition is satisﬁed, then g follows strictly from h1, . . . , hn.
If we work over C, we can get a better sense of what g ∈
"
⟨h1, . . . , hn⟩means.
By allowing solutions in C, the hypotheses h1, . . . , hn deﬁne a variety VC ⊆Cm+n.
Then, in Exercise 9, you will use the Strong Nullstellensatz to show that
g ∈
"
⟨h1, . . . , hn⟩⊆R[u1, . . . , um, x1, . . . , xn]
⇐⇒g ∈I(VC) ⊆C[u1, . . . , um, x1, . . . , xn].
Thus, g ∈
"
⟨h1, . . . , hn⟩means that g "follows strictly over C" from h1, . . . , hn.
Let us apply these concepts to an example. This will reveal why Deﬁnition 4 is
too strong.
Example 1 (continued). To see what can go wrong if we proceed as above, con-
sider the theorem on the diagonals of a parallelogram from Example 1, taking as
hypotheses the four polynomials from (1) and (2):
h1 = x2 −u3,
h2 = (x1 −u1)u3 −u2x2,
h3 = x4x1 −x3x2,
h4 = x4(u2 −u1) −(x3 −u1)u3.
We will take as conclusion the ﬁrst polynomial from (3):
g = x2
1 −2x1x3 −2x4x2 + x2
2.
To apply Proposition 5, we must compute a Gröbner basis for
¯I = ⟨h1, h2, h3, h4, 1 −yg⟩⊆R[u1, u2, u3, x1, x2, x3, x4, y].

326
Chapter 6 Robotics and Automatic Geometric Theorem Proving
Surprisingly enough, we do not ﬁnd {1}. (You will use a computer algebra system
in Exercise 10 to verify this.) Since the statement is a true geometric theorem, we
must try to understand why our proposed method failed in this case.
The reason can be seen by computing a Gröbner basis for I = ⟨h1, h2, h3, h4⟩in
R[u1, u2, u3, x1, x2, x3, x4], using lex order with x1 > x2 > x3 > x4 > u1 > u2 > u3.
The result is
f1 = x1x4 + x4u1 −x4u2 −u1u3,
f2 = x1u3 −u1u3 −u2u3,
f3 = x2 −u3,
f4 = x3u3 + x4u1 + x4u2 −u1u3,
f5 = x4u2
1 −x4u1u2 −1
2u2
1u3 + 1
2u1u2u3,
f6 = x4u1u3 −1
2u1u2
3.
The variety V = V(h1, h2, h3, h4) = V( f1, . . . , f6) in R7 deﬁned by the hypotheses
is actually reducible. To see this, note that f2 factors as (x1 −u1 −u2)u3, which
implies that
V = V( f1, x1 −u1 −u2, f3, f4, f5, f6) ∪V( f1, u3, f3, f4, f5, f6).
Since f5 and f6 also factor, we can continue this decomposition process. Things
simplify dramatically if we recompute the Gröbner basis at each stage, and, in the
exercises, you will show that this leads to the decomposition
V = V′ ∪U1 ∪U2 ∪U3
into irreducible varieties, where
V′ = V

x1 −u1 −u2, x2 −u3, x3 −u1 + u2
2
, x4 −u3
2

,
U1 = V(x2, x4, u3),
U2 = V(x1, x2, u1 −u2, u3),
U3 = V(x1 −u2, x2 −u3, x3u3 −x4u2, u1).
You will also show that none of these varieties are contained in the others, so that
V′, U1, U2, U3 are the irreducible components of V.
The problem becomes apparent when we interpret the components U1,
U2, U3 ⊆V in terms of the parallelogram ABDC. On U1 and U2, we have u3 = 0.
This is troubling since u3 was supposed to be arbitrary. Further, when u3 = 0, the
vertex C of our parallelogram lies on AB and, hence we do not have a parallelogram
at all. This is a degenerate case of our conﬁguration, which we intended to rule out
by the hypothesis that ABDC was an honest parallelogram in the plane. Similarly,
we have u1 = 0 on U3, which again is a degenerate conﬁguration.
You can also check that on U1 = V(x2, x4, u3), our conclusion g becomes g =
x2
1 −2x1x3, which is not zero since x1 and x3 are arbitrary on U1. This explains

§4 Automatic Geometric Theorem Proving
327
why our ﬁrst attempt failed to prove the theorem. Once we exclude the degenerate
cases U1, U2, U3, the above method easily shows that g vanishes on V′. We leave
the details as an exercise.
Our goal is to develop a general method that can be used to decide the val-
idity of a theorem, taking into account any degenerate special cases that may
need to be excluded. To begin, we use Theorem 2 of Chapter 4, §6 to write
V = V(h1, . . . , hn) ⊆Rm+n as a ﬁnite union of irreducible varieties,
(10)
V = V1 ∪· · · ∪Vk.
As we saw in the continuation of Example 1, it may be the case that some poly-
nomial equation involving only the ui holds on one or more of these irreducible
components of V. Since our intent is that the ui should be essentially independent,
we want to exclude these components from consideration if they are present. We
introduce the following terminology.
Deﬁnition 6. Let W be an irreducible variety in the afﬁne space Rm+n with coordi-
nates u1, . . . , um, x1, . . . , xn. We say that the functions u1, . . . , um are algebraically
independent on W if no nonzero polynomial in the ui alone vanishes identically
on W.
Equivalently, Deﬁnition 6 states that u1, . . . , um are algebraically independent on
W if I(W) ∩R[u1, . . . , um] = {0}.
Thus, in the decomposition of the variety V given in (10), we can regroup the
irreducible components in the following way:
(11)
V = W1 ∪· · · ∪Wp ∪U1 ∪· · · ∪Uq,
where u1, . . . , um are algebraically independent on the components Wi and are not
algebraically independent on the components Uj. Thus, the Uj, represent "degen-
erate" cases of the hypotheses of our theorem. To ensure that the variables ui are
actually arbitrary in the geometric conﬁgurations we study, we should consider only
the subvariety
V′ = W1 ∪· · · ∪Wp ⊆V.
Given a conclusion g ∈R[u1, . . . , um, x1, . . . , xn] we want to prove, we are not
interested in how g behaves on the degenerate cases. This leads to the following
deﬁnition.
Deﬁnition 7. The conclusion g follows generically from the hypotheses h1, . . . , hn
if g ∈I(V′) ⊆R[u1, . . . , um, x1, . . . , xn], where, as above, V′ ⊆Rm+n is the
union of the components of the variety V = V(h1, . . . , hn) on which the ui are
algebraically independent.
Saying a geometric theorem is "true" in the usual sense means precisely that its
conclusion(s) follow generically from its hypotheses. The question becomes, given
a conclusion g: can we determine when g ∈I(V′)? In other words, can we dev-
elop a criterion that determines whether g vanishes on every component of V on

328
Chapter 6 Robotics and Automatic Geometric Theorem Proving
which the ui are algebraically independent, ignoring what happens on the possible
"degenerate" components?
Determining the decomposition of a variety into irreducible components is not
always easy, so we would like a method to determine whether a conclusion follows
generically from a set of hypotheses that does not require knowledge of the decom-
position (11). Further, even if we could ﬁnd V′, we would still have the problem of
computing I(V′).
Fortunately, it is possible to show that g follows generically from h1, . . . , hn with-
out knowing the decomposition of V given in (11). We have the following result.
Proposition 8. In the above situation, g follows generically from h1, . . . , hn when-
ever there is some nonzero polynomial c(u1, . . . , um) ∈R[u1, . . . , um] such that
c · g ∈
√
H,
where H is the ideal generated by the hypotheses hi in R[u1, . . . , um, x1, . . . , xn].
Proof. Let Vj be one of the irreducible components of V′. Since c · g ∈
√
H, we see
that c · g vanishes on V and, hence, on Vj. Thus, the product c · g is in I(Vj). But Vj
is irreducible, so that I(Vj) is a prime ideal by Proposition 3 of Chapter 4, §5. Thus,
c · g ∈I(Vj) implies either c or g is in I(Vj). We know c ̸∈I(Vj) since no nonzero
polynomial in the ui alone vanishes on this component. Hence, g ∈I(Vj), and since
this is true for each component of V′, it follows that g ∈I(V′).
□
For Proposition 8 to give a practical way of determining whether a conclusion
follows generically from a set of hypotheses, we need a criterion for deciding when
there is a nonzero polynomial c with c · g ∈
√
H. This is actually quite easy to do.
By the deﬁnition of the radical, we know that c · g ∈
√
H if and only if
(c · g)s =
n

j=1
Ajhj
for some Aj ∈R[u1, . . . , um, x1, . . . , xn] and s ≥1. If we divide both sides of this
equation by cs, we obtain
gs =
n

j=1
Aj
cs hj,
which shows that g is in the radical of the ideal H generated by h1, . . . , hn over the
ring R(u1, . . . , um)[x1, . . . , xn] (in which we allow denominators depending only on
the ui). Conversely, if g ∈
"
H, then
gs =
n

j=1
Bjhj,
where the Bj ∈R(u1, . . . , um)[x1, . . . , xn]. If we ﬁnd a least common denominator c
for all terms in all the Bj and multiply both sides by cs (clearing denominators in the
process), we obtain

§4 Automatic Geometric Theorem Proving
329
(c · g)s =
n

j=1
B′
jhj,
where B′
j ∈R[u1, . . . , um, x1, . . . , xn] and c depends only on the ui. As a result,
c · g ∈
√
H. These calculations and the radical membership algorithm from §2 of
Chapter 4 establish the following corollary of Proposition 8.
Corollary 9. In the situation of Proposition 8, the following are equivalent:
(i) There is a nonzero polynomial c ∈R[u1, . . . , um] such that c · g ∈
√
H.
(ii) g ∈
"
H, where H is the ideal generated by the hj in R(u1, . . . , um)[x1, . . . , xn].
(iii) {1} is the reduced Gröbner basis of the ideal
⟨h1, . . . , hn, 1 −yg⟩⊆R(u1, . . . , um)[x1, . . . , xn, y].
If we combine part (iii) of this corollary with Proposition 8, we get an algorithmic
method for proving that a conclusion follows generically from a set of hypotheses.
We will call this the Gröbner basis method in geometric theorem proving.
To illustrate the use of this method, we will consider the theorem on parallelo-
grams from Example 1 once more. We ﬁrst compute a Gröbner basis of the ideal
⟨h1, h2, h3, h4, 1 −yg⟩in the ring R(u1, u2, u3)[x1, x2, x3, x4, y]. This computation
does yield {1} as we expect. Making u1, u2, u3 invertible by passing to R(u1, u2, u3)
as our ﬁeld of coefﬁcients in effect removes the degenerate cases encountered above,
and the conclusion does follow generically from the hypotheses. Moreover, in Ex-
ercise 12, you will see that g itself (and not some higher power) actually lies in
⟨h1, h2, h3, h4⟩⊆R(u1, u2, u3)[x1, x2, x3, x4].
Note that the Gröbner basis method does not tell us what the degenerate cases are.
The information about these cases is contained in the polynomial c ∈R[u1, . . . , um],
for c · g ∈
√
H tells us that g follows from h1, . . . , hn whenever c does not vanish
(this is because c · g vanishes on V). In Exercise 14, we will give an algorithm for
ﬁnding c.
Over C, we can think of Corollary 9 in terms of the variety VC = V(h1, . . . , hn) ⊆
Cm+n as follows. Decomposing VC as in (11), let V′
C ⊆VC be the union of those
components where the ui are algebraically independent. Then Exercise 15 will use
the Nullstellensatz to prove that
∃c ̸= 0 in R[u1, . . . , um] with c · g ∈
"
⟨h1, . . . , hn⟩⊆R[u1, . . . , um, x1, . . . , xu]
⇐⇒g ∈I(V′
C) ⊆C[u1, . . . , um, x1, . . . , xn].
Thus, the conditions of Corollary 9 mean that g "follows generically over C" from
the hypotheses h1, . . . , hn.
This interpretation points out what is perhaps the main limitation of the Gröbner
basis method in geometric theorem proving: it can only prove theorems where the
conclusions follow generically over C, even though we are only interested in what
happens over R. In particular, there are theorems which are true over R but not

330
Chapter 6 Robotics and Automatic Geometric Theorem Proving
over C [see STURMFELS (1989) for an example]. Our methods will fail for such
theorems.
When using Corollary 9, it is often unnecessary to consider the radical of H.
In many cases, the ﬁrst power of the conclusion is in H already. So most theorem
proving programs in effect use an ideal membership algorithm ﬁrst to test if g ∈H,
and only go on to the radical membership test if that initial step fails.
To illustrate this, we continue with the Circle Theorem of Apollonius from
Example 3. Our hypotheses are the eight polynomials hi from (5)-(7). We begin
by computing a Gröbner basis (using lex order) for the ideal H, which yields
(12)
f1 = x1 −u1/2,
f2 = x2 −u2/2,
f3 = x3 −u1/2,
f4 = x4 −u2/2,
f5 = x5 −
u1u2
2
u2
1 + u2
2
,
f6 = x6 −
u2
1u2
u2
1 + u2
2
,
f7 = x7 −u1/4,
f8 = x8 −u2/4.
We leave it as an exercise to show that the conclusion (8) reduces to zero on division
by this Gröbner basis. Thus, g itself is in H, which shows that g follows generically
from h1, . . . , h8. Note that we must have either u1 ̸= 0 or u2 ̸= 0 in order to solve for
x5 and x6. The equations u1 = 0 and u2 = 0 describe degenerate right "triangles" in
which the three vertices are not distinct, so we certainly wish to rule these cases out.
It is interesting to note, however, that if either u1 or u2 is nonzero, the conclusion
is still true. For instance, if u1 ̸= 0 but u2 = 0, then the vertices C and A coincide.
From (5) and (6), the midpoints M1 and M3 coincide, M2 coincides with A, and H
coincides with A as well. As a result, there is a circle (inﬁnitely many of them in
fact) containing M1, M2, M3, and H in this degenerate case. In Exercise 16, you will
study what happens when u1 = u2 = 0.
We conclude this section by noting that there is one further subtlety that can occur
when we use this method to prove or verify a theorem. Namely, there are cases
where the given statement of a geometric theorem conceals one or more unstated
"extra" hypotheses. These may very well not be included when we make a direct
translation to a system of polynomial equations. This often results in a situation
where the variety V′ is reducible or, equivalently, where p ≥2 in (11). In this case,
it may be true that the intended conclusion is zero only on some of the reducible
components of V′, so that any method based on Corollary 9 would fail. We will study
an example of this type in Exercise 17. If this happens, we may need to reformulate
our hypotheses to exclude the extraneous, unwanted components of V′.

§4 Automatic Geometric Theorem Proving
331
In a related development, the Gröbner covers mentioned in §3 have been used
to discover the hypotheses under which certain geometric conﬁgurations have nice
properties. See MONTES and RECIO (2014) for an example.
EXERCISES FOR §4
1. This exercise asks you to give geometric proofs of the theorems that we studied in
Examples 1 and 3.
a. Give a standard Euclidean proof of the theorem of Example 1. Hint: Show ΔANC ∼=
ΔBND.
b. Give a standard Euclidean proof of the Circle Theorem of Apollonius from Example 3.
Hint: First show that AB and M2M3 are parallel.
2. This exercise shows that it is possible to give translations of a theorem based on different
collections of arbitrary coordinates. Consider the parallelogram ABDC from Example 1
and begin by placing A at the origin.
a. Explain why it is also possible to consider both of the coordinates of D as arbitrary
variables: D = (u1, u2).
b. With this choice, explain why we can specify the coordinates of B as B = (u3, x1), i.e.,
the x-coordinate of B is arbitrary, but the y-coordinate is determined by the choices of
u1, u2, u3.
c. Complete the translation of the theorem based on this choice of coordinates.
3. Let A, B, C, D, E, F, G, H be points in the plane.
a. Show that the statement AB is tangent to the circle through A, C, D can be expressed
by polynomial equations. Hint: Construct the center of the circle ﬁrst. Then, what is
true about the tangent and the radius of a circle at a given point?
b. Show that the statement AB · CD = EF · GH can be expressed by one or more
polynomial equations.
c. Show that the statement
AB
CD =
EF
GH can be expressed by one or more polynomial
equations.
d. The cross ratio of the ordered 4-tuple of distinct collinear points (A, B, C, D) is
deﬁned to be the real number
AC · BD
AD · BC.
Show that the statement "The cross ratio of (A, B, C, D) is equal to ρ ∈R" can be
expressed by one or more polynomial equations.
4. In this exercise, you will complete the proof of Proposition 2 in the text.
a. Prove part (ii).
b. Show that if α, β are acute angles, then α = β if and only if tanα = tanβ. Use this
fact and part (c) of Exercise 3 to prove part (vii) of Proposition 2. Hint: To compute
the tangent of an angle, you can construct an appropriate right triangle and compute
a ratio of side lengths.
c. Prove part (viii).
5. Let ΔABC be a triangle in the plane. Recall that the altitude from A is the line segment
from A meeting the opposite side BC at a right angle. (We may have to extend BC here to
ﬁnd the intersection point.) A standard geometric theorem asserts that the three altitudes
of a triangle meet at a single point H, often called the orthocenter of the triangle. Give a
translation of the hypotheses and conclusion of this theorem as a system of polynomial
equations.
6. Let ΔABC be a triangle in the plane. It is a standard theorem that if we let M1 be the mid-
point of BC, M2 be the midpoint of AC and M3 be the midpoint of AB, then the segments
AM1, BM2 and CM3 meet at a single point M, often called the centroid of the triangle.
Give a translation of the hypotheses and conclusion of this theorem as a system of poly-
nomial equations.

332
Chapter 6 Robotics and Automatic Geometric Theorem Proving
7. Let ΔABC be a triangle in the plane. It is a famous theorem of Euler that the circum-
center (the center of the circumscribed circle), the orthocenter (from Exercise 5), and
the centroid (from Exercise 6) are always collinear. Translate the hypotheses and con-
clusion of this theorem into a system of polynomial equations. (The line containing the
three "centers" of the triangle is called the Euler line of the triangle.)
8. A beautiful theorem ascribed to Pappus concerns two collinear triples of points A, B, C
and A′, B′, C′. Let
P = AB′ ∩A′B,
Q = AC′ ∩A′C,
R = BC′ ∩B′C
be as in the ﬁgure:
C
B
A
A¢
B¢
C¢
P
Q
R
Then it is always the case that P, Q, R are collinear points. Give a translation of the
hypotheses and conclusion of this theorem as a system of polynomial equations.
9. Given h1, . . . , hn ∈R[u1, . . . , um, x1, . . . , xn], let VC = V(h1, . . . , hn) ⊆Cm+n. If
g ∈R[u1, . . . , um, x1, . . . , xn], the goal of this exercise is to prove that
g ∈
!
⟨h1, . . . , hn⟩⊆R[u1, . . . , um, x1, . . . , xn]
⇐⇒g ∈I(VC) ⊆C[u1, . . . , um, x1, . . . , xn].
a. Prove the ⇒implication.
b. Use the Strong Nullstellensatz to show that if g ∈I(VC), then there are polynomials
Aj ∈C[u1, . . . , um, x1, . . . , xn] such that gs = n
j=1 Ajhj for some s ≥1.
c. Explain why Aj can be written Aj = A′
j + iA′′
j , where A′
j, A′′
j are polynomials with real
coefﬁcients. Use this to conclude that gs = n
j=1 A′
jhj, which will complete the proof
of the ⇐implication. Hint: g and h1, . . . , hn have real coefﬁcients.
10. Verify the claim made in Example 1 that {1} is not the unique reduced Gröbner basis for
the ideal ¯I = ⟨h1, h2, h3, h4, 1 −yg⟩.
11. This exercise will study the decomposition into reducible components of the variety
deﬁned by the hypotheses of the theorem from Example 1.
a. Verify the claim made in the continuation of Example 1 that
V = V( f1, x1 −u1 −u2, f3, . . . , f6) ∪V( f1, u3, f3, . . . , f6) = V1 ∪V2.
b. Compute Gröbner bases for the deﬁning equations of V1 and V2. Some of the polyno-
mials should factor and use this to decompose V1 and V2.
c. By continuing this process, show that V is the union of the varieties V′, U1, U2, U3
deﬁned in the text.
d. Prove that V′, U1, U2, U3 are irreducible and that none of them is contained in the
union of the others. This shows that V′, U1, U2, U3 are the reducible components of V.

§4 Automatic Geometric Theorem Proving
333
e. On which irreducible component of V is the conclusion of the theorem valid?
f. Suppose we take as hypotheses the four polynomials in (4) and (2). Is the variety
W = V(h′
1, h′
2, h3, h4) reducible? How many components does it have?
12. Verify the claim made in Example 1 that the conclusion g itself (and not some higher
power) is in the ideal generated by h1, h2, h3, h4 in R(u1, u2, u3)[x1, x2, x3, x4].
13. By applying part (iii) of Corollary 9, verify that g follows generically from the hj for
each of the following theorems. What is the lowest power of g which is contained in the
ideal H in each case?
a. The theorem on the orthocenter of a triangle (Exercise 5).
b. The theorem on the centroid of a triangle (Exercise 6).
c. The theorem on the Euler line of a triangle (Exercise 7).
d. Pappus's Theorem (Exercise 8).
14. In this exercise, we will give an algorithm for ﬁnding a nonzero c ∈R[u1, . . . , um] such
that c · g ∈
√
H, assuming that such a c exists. We will work with the ideal
H = ⟨h1, . . . , hn, 1 −yg⟩⊆R[u1, . . . , um, x1, . . . , xn, y].
a. Show that the conditions of Corollary 9 are equivalent to H ∩R[u1, . . . , um] ̸= {0}.
Hint: Use condition (iii) of the corollary.
b. If c ∈H ∩R[u1, . . . , um], prove that c · g ∈
√
H. Hint: Adapt the argument used in
equations (2)-(4) in the proof of Hilbert's Nullstellensatz in Chapter 4, §1.
c. Describe an algorithm for computing H ∩R[u1, . . . , um]. For maximum efﬁciency,
what monomial order should you use?
Parts (a)-(c) give an algorithm which decides if there is a nonzero c with c · g ∈
√
H
and simultaneously produces the required c. Parts (d) and (e) below give some inter-
esting properties of the ideal H ∩R[u1, . . . , um].
d. Show that if the conclusion g fails to hold for some choice of u1, . . . , um, then
(u1, . . . , um) ∈W = V(H ∩R[u1, . . . , um]) ⊆Rm. Thus, W records the degener-
ate cases where g fails.
e. Show that

H ∩R[u1, . . . , um] gives all c's for which c·g ∈
√
H. Hint: One direction
follows from part (a). If c · g ∈
√
H, note that H contains (c · g)'s and 1 −gy. Now
adapt the argument given in Proposition 8 of Chapter 4, §2 to show that cs ∈H.
15. As in Exercise 9, suppose that we have h1, . . . , hn ∈R[u1, . . . , um, x1, . . . , xn]. Then
we get VC = V(h1, . . . , hn) ⊆Cm+n. As we did with V, let V′
C be the union of the
irreducible components of VC where u1, . . . , un are algebraically independent. Given
g ∈R[u1, . . . , um, x1, . . . , xn], we want to show that
∃c ̸= 0 in R[u1, . . . , um] with c · g ∈
!
⟨h1, . . . , hn⟩⊆R[u1, . . . , um, x1, . . . , xn]
⇐⇒g ∈I(V′
C) ⊆C[u1, . . . , um, x1, . . . , xn].
a. Prove the ⇒implication. Hint: See the proof of Proposition 8.
b. Show that if g ∈I(V′
C), then there is a nonzero polynomial c ∈C[u1, . . . , um] such
that c · g ∈I(VC). Hint: Write VC = V′
C ∪U′
1 ∪· · · ∪U′
q, where u1, . . . , um are
algebraically dependent on each U′
j . This means there is a nonzero polynomial cj ∈
C[u1, . . . , um] which vanishes on U′
j .
c. Show that the polynomial c of part b can be chosen to have real coefﬁcients. Hint: If
c is the polynomial obtained from c by taking the complex conjugates of the coefﬁ-
cients, show that c has real coefﬁcients.
d. Once we have c ∈R[u1, . . . , um] with c · g ∈I(VC), use Exercise 9 to complete the
proof of the ⇐implication.

334
Chapter 6 Robotics and Automatic Geometric Theorem Proving
16. This exercise deals with the Circle Theorem of Apollonius from Example 3.
a. Show that the conclusion (8) reduces to 0 on division by the Gröbner basis (12) given
in the text.
b. Discuss the case u1 = u2 = 0 in the Circle Theorem. Does the conclusion follow in
this degenerate case?
c. Note that in the diagram in the text illustrating the Circle Theorem, the circle is shown
passing through the vertex A in addition to the three midpoints and the foot of the
altitude drawn from A. Does this conclusion also follow from the hypotheses?
17. In this exercise, we will study a case where a direct translation of the hypotheses of
a "true" theorem leads to extraneous components on which the conclusion is actually
false. Let ΔABC be a triangle in the plane. We construct three new points A′, B′, C' such
that the triangles ΔA′BC, ΔAB′C, ΔABC′ are equilateral. The intended construction is
illustrated in the ﬁgure below.
B
A
C
B¢
A¢
C¢
S
Our theorem is that the three line segments AA′, BB′, CC′ all meet in a single point S. (We
call S the Fermat point of the triangle. If no angle of the original triangle was greater than
2π
3 , it can be shown that the three segments AS, BS, CS form a Steiner tree, the network
of shortest total length connecting the points A, B, C.)
a. Give a conventional geometric proof of the theorem, assuming the construction is
done as in the ﬁgure.
b. Now, translate the hypotheses and conclusion of this theorem directly into a set of
polynomial equations.
c. Apply the test based on Corollary 9 to determine whether the conclusion follows
generically from the hypotheses. The test should fail. Note: This computation may
require a great deal of ingenuity to push through on some computer algebra systems.
This is a complicated system of polynomials.
d. (The key point) Show that there are other ways to construct a ﬁgure which is con-
sistent with the hypotheses as stated, but which do not agree with the ﬁgure above.
Hint: Are the points A′, B′, C′ uniquely determined by the hypotheses as stated? Is
the statement of the theorem valid for these alternate constructions of the ﬁgure? Use
this to explain why part (c) did not yield the expected result. (These alternate con-
structions correspond to points in different components of the variety deﬁned by the
hypotheses.)

§5 Wu's Method
335
This example is discussed in detail in pages 69-72 of CHOU (1988). After decompos-
ing the variety deﬁned by the hypotheses, Chou shows that the conclusion follows on
a component including the case pictured above and the ﬂipped case where the point A′
lies in the half plane determined by BC that contains the triangle, and similarly for B′
and C′. These two cases are characterized by the fact that certain rational functions of
the coordinates of the points take only positive values. Chou is able to treat this alge-
braically showing that these cases are where the rational functions in question coincide
with certain sums of squares.
§5
Wu's Method
In this section, we will study a second algorithmic method for proving theorems
in Euclidean geometry based on systems of polynomial equations. This method,
introduced by the Chinese mathematician Wu Wen-Tsün, was developed before the
Gröbner basis method given in §4. It is also more commonly used than the Gröbner
basis method in practice because it is usually more efﬁcient.
Both the elementary version of Wu's method that we will present, and the more
reﬁned versions, use an interesting variant of the division algorithm for multivariable
polynomials introduced in Chapter 2, §3. The idea here is to follow the one-variable
polynomial division algorithm as closely as possible, and we obtain a result known
as the pseudodivision algorithm. To describe the ﬁrst step in the process, we consider
two polynomials in the ring k[x1, . . . , xn, y], written in the form
(1)
f = cpyp + · · · + c1y + c0,
g = dmym + · · · + d1y + d0,
where the coefﬁcients ci, dj are polynomials in x1, . . . , xn. Assume that m ≤p.
Proceeding as in the one-variable division algorithm for polynomials in y, we can
attempt to remove the leading term cpyp in f by subtracting a multiple of g. However,
this is not possible directly unless dm divides cp in k[x1, . . . , xn]. In pseudodivision,
we ﬁrst multiply f by dm to ensure that the leading coefﬁcient is divisible by dm, then
proceed as in one-variable division. We can state the algorithm formally as follows.
Proposition 1. Let f, g ∈k[x1, . . . , xn, y] be as in (1) and assume m ≤p and dm ̸= 0.
(i) There is an equation
ds
m f = qg + r,
where q, r ∈k[x1, . . . , xn, y], s ≥0, and either r = 0 or the degree of r in y is
less than m.
(ii) r ∈⟨f, g⟩in the ring k[x1, . . . , xn, y].
Proof. (i) Polynomials q, r satisfying the conditions of the proposition can be con-
structed by the following algorithm, called pseudodivision with respect to y. We
use the notation deg(h, y) for the degree of the polynomial h in the variable y and
LC(h, y) for the leading coefﬁcient of h as a polynomial in y—i.e., the coefﬁcient of
ydeg(h,y) in h.

336
Chapter 6 Robotics and Automatic Geometric Theorem Proving
Input : f, g
Output : q, r
m := deg(g, y); d := LC(g, y)
r := f; q := 0
WHILE r ̸= 0 AND deg(r, y) ≥m DO
r := dr −LC(r, y)gydeg(r,y)−m
q := dq + LC(r, y)ydeg(r,y)−m
RETURN q, r
Note that if we follow this procedure, the body of the WHILE loop will be executed
at most p −m + 1 times. Thus, the power s in dsf = qg + r can be chosen so that
s ≤p −m + 1. We leave the rest of proof of (i) to the reader as Exercise 1.
From dsf = qg + r, it follows that r = dsf −qg ∈⟨f, g⟩. Since d = dm, we also
have ds
m f = qg + r, and the proof of the proposition is complete.
□
The polynomials q, r are known as a pseudoquotient and a pseudoremainder of
f on pseudodivision by g, with respect to the variable y. We will use the notation
Rem( f, g, y) for the pseudoremainder produced by the algorithm given in the proof
of Proposition 1. For example, if we pseudodivide f = x2y3 −y by g = x3y −2 with
respect to y by the algorithm above, we obtain the equation
(x3)3f = (x8y2 + 2x5y + 4x2 −x6)g + 8x2 −2x6.
In particular, the pseudoremainder is Rem( f, g, y) = 8x2 −2x6.
We note that there is a second, "slicker" way to understand what is happening
in this algorithm. The same idea of allowing denominators that we exploited in §4
shows that pseudodivision is the same as
• ordinary one-variable polynomial division for polynomials in y, with coefﬁcients
in the rational function ﬁeld K = k(x1, . . . , xn), followed by
• clearing denominators. You will establish this claim in Exercise 2, based on the
observation that the only term that needs to be inverted in division of polynomi-
als in K[y] (K any ﬁeld) is the leading coefﬁcient dm of the divisor g. Thus, the
denominators introduced in the process of dividing f by g can all be cleared
by multiplying by a suitable power ds
m, and we get an equation of the form
ds
m f = qg + r.
In this second form, or directly, pseudodivision can be readily implemented in most
computer algebra systems. Indeed, some systems include pseudodivision as one of
the built-in operations on polynomials.
We recall the situation studied in §4, in which the hypotheses and conclusion of
a theorem in Euclidean plane geometry are translated into a system of polynomials
in variables u1, . . . , um, x1, . . . , xn, with h1, . . . , hn representing the hypotheses and
g giving the conclusion. As in equation (11) of §4, we can group the irreducible
components of the variety V = V(h1, . . . , hn) ⊆Rm+n as

§5 Wu's Method
337
V = V′ ∪U,
where V′ is the union of the components on which the ui are algebraically indepen-
dent. Our goal is to prove that g vanishes on V′.
The elementary version of Wu's method that we will discuss is tailored for the
case where V′ is irreducible. We note, however, that Wu's method can be extended
to the more general reducible case also. The main algebraic tool needed (Ritt's dec-
omposition algorithm based on characteristic sets for prime ideals) would lead us
too far aﬁeld, though, so we will not discuss it. Note that, in practice, we usually
do not know in advance whether V′ is irreducible or not. Thus, reliable "theorem-
provers" based on Wu's method should include these more general techniques too.
Our simpliﬁed version of Wu's method uses the pseudodivision algorithm in two
ways in the process of determining whether the equation g = 0 follows from hj = 0.
• Step 1 of Wu's method uses pseudodivision to reduce the hypotheses to a system
of polynomials fj that are in triangular form in the variables x1, . . . , xn. In other
words, we seek
(2)
f1 = f1(u1, . . . , um, x1),
f2 = f2(u1, . . . , um, x1, x2),
...
fn = fn(u1, . . . , um, x1, . . . , xn)
such that V( f1, . . . , fn) again contains the irreducible variety V′, on which the ui
are algebraically independent.
• Step 2 of Wu's method uses successive pseudodivision of the conclusion g with
respect to each of the variables xj to determine whether g ∈I(V′). We compute
Rn−1 = Rem(g, fn, xn),
Rn−2 = Rem(Rn−1, fn−1, xn−1),
...
(3)
R1 = Rem(R2, f2, x2),
R0 = Rem(R1, f1, x1).
• Then R0 = 0 implies that g follows from the hypotheses hj under an additional
condition, to be made precise in Theorem 4.
To explain how Wu's method works, we need to explain each of these steps,
beginning with the reduction to triangular form.
Step 1. Reduction to Triangular Form
In practice, this reduction can almost always be accomplished using a procedure
very similar to Gaussian elimination for systems of linear equations. We will not

338
Chapter 6 Robotics and Automatic Geometric Theorem Proving
state any general theorems concerning our procedure, however, because there are
some exceptional cases in which it might fail. (See the comments below.) A com-
pletely general procedure for accomplishing this kind of reduction may be found in
CHOU (1988).
The elementary version is performed as follows. We work one variable at a time,
beginning with xn.
1.1. Among the hj, ﬁnd all the polynomials containing the variable xn. Call the set
of such polynomials S. (If there are no such polynomials, the translation of
our geometric theorem is most likely incorrect since it would allow xn to be
arbitrary.)
1.2. If there is only one polynomial in S, then we can rename the polynomials,
making that one polynomial f ′
n, and our system of polynomials will have the
form
(4)
f ′
1 = f ′
1(u1, . . . , um, x1, . . . , xn−1),
...
f ′
n−1 = f ′
n−1(u1, . . . , um, x1, . . . , xn−1),
f ′
n = f ′
n(u1, . . . , um, x1, . . . , xn).
1.3. If there is more than one polynomial in S, but some element of S has degree 1 in
xn, then we can take f ′
n as that polynomial and replace all the other hypotheses
in S by their pseudoremainders on division by f ′
n with respect to xn. [One of
these pseudoremainders could conceivably be zero, but this would mean that
f ′
n would divide dsh, where h is one of the other hypothesis polynomials and
d = LC( f ′
n, xn). This is unlikely since V′ is assumed to be irreducible.] We
obtain a system in the form (4) again. By part (ii) of Proposition 1, all the f ′
j
are in the ideal generated by the hj.
1.4. If there are several polynomials in S, but none has degree 1 in xn, then pick
a, b ∈S where 0 < deg(b, xn) ≤deg(a, xn) and compute the pseudoremainder
r = Rem(a, b, xn). Then:
a. If deg(r, xn) ≥1, then replace S by (S \ {a}) ∪{r} (leaving the hypothe-
ses not in S unchanged) and repeat either 1.4 (if deg(r, xn) ≥2) or 1.3
(if deg(r, xn) = 1).
b. If deg(r, xn) = 0, then replace S by S \ {a} (adding r to the hypotheses not
in S) and repeat either 1.4 (if the new S has ≥2 elements) or 1.2 (if the new
S has only one element).
Eventually we are reduced to a system of polynomials of the form (4) again.
Since the degree in xn are reduced each time we compute a pseudoremainder, we
will eventually remove the xn terms from all but one of our polynomials. More-
over, by part (ii) of Proposition 1, each of the resulting polynomials is contained
in the ideal generated by the hj. Again, it is conceivable that we could obtain a
zero pseudoremainder at some stage here. This would usually, but not always, im-
ply reducibility, so it is unlikely. We then apply the same process to the polynomials
f ′
1, . . . , f ′
n−1 in (4) to remove the xn−1 terms from all but one polynomial. Continuing

§5 Wu's Method
339
in this way, we will eventually arrive at a system of equations in triangular form as
in (2) above.
Once we have the triangular equations, we can relate them to the original hyp-
otheses as follows.
Proposition 2. Suppose that f1 = · · · = fn = 0 are the triangular equations
obtained from h1 = · · · = hn = 0 by the above reduction algorithm. Then
V′ ⊆V = V(h1, . . . , hn) ⊆V( f1, . . . , fn).
Proof. As we noted above, all the fj are contained in the ideal generated by the hj.
Thus, ⟨f1, . . . , fn⟩⊆⟨h1, . . . , hn⟩and hence, V = V(h1, . . . , hn) ⊆V( f1, . . . , fn)
follows immediately. Since V′ ⊆V, we are done.
□
Example 3. To illustrate the operation of this triangulation procedure, we will apply
it to the hypotheses of the Circle Theorem of Apollonius from §4. Referring back
to (5)-(7) of §4, we have
h1 = 2x1 −u1,
h2 = 2x2 −u2,
h3 = 2x3 −u1,
h4 = 2x4 −u2,
h5 = u2x5 + u1x6 −u1u2,
h6 = u1x5 −u2x6,
h7 = x2
1 −x2
2 −2x1x7 + 2x2x8,
h8 = x2
1 −2x1x7 −x2
3 + 2x3x7 −x2
4 + 2x4x8.
Note that this system is very nearly in triangular form in the xj. In fact, this is often
true, especially in the cases where each step of constructing the geometric conﬁgu-
ration involves adding one new point.
In Step 1 of the triangulation procedure, we see that h7, h8 are the only polyno-
mials in our set containing x8. Even better, h8 has degree 1 in x8. Hence, we proceed
as in 1.3 of the triangulation procedure, making f8 = h8, and replacing h7 by
f7 = Rem(h7, h8, x8)
= (2x1x2 −2x2x3 −2x1x4)x7 −x2
1x2 + x2x2
3 + x2
1x4 −x2
2x4 + x2x2
4.
As this example indicates, we often ignore numerical constants when computing
remainders. Only f7 contains x7, so nothing further needs to be done there. Both h6
and h5 contain x6, but we are in the situation of 1.3 in the procedure again. We make
f6 = h6 and replace h5 by
f5 = Rem(h5, h6, x6) = (u2
1 + u2
2)x5 −u1u2
2.
The remaining four polynomials are in triangular form already, so we take fi = hi
for i = 1, 2, 3, 4.

340
Chapter 6 Robotics and Automatic Geometric Theorem Proving
Step 2. Successive Pseudodivision
The key step in Wu's method is the successive pseudodivision operation given in
equation (3) computing the ﬁnal remainder R0. The usefulness of this operation is
indicated by the following theorem.
Theorem 4. Consider the set of hypotheses and the conclusion for a geometric
theorem. Let R0 be the ﬁnal remainder computed by the successive pseudodivision
of g as in (3), using the system of polynomials f1, . . . , fn in triangular form (2). Let
dj be the leading coefﬁcient of fj as a polynomial in xj (so dj is a polynomial in
u1, . . . , um and x1, . . . , xj−1). Then:
(i) There are nonnegative integers s1, . . . , sn and polynomials A1, . . . , An in the ring
R[u1, . . . , um, x1, . . . , xn] such that
ds1
1 · · · dsn
n g = A1 f1 + · · · + An fn + R0.
(ii) If R0 is the zero polynomial, then g is zero at every point of V′ \V(d1d2 · · · dn) ⊆
Rm+n.
Proof. Part (i) follows by applying Proposition 1 repeatedly. Pseudodividing g by
fn with respect to xn, we have
Rn−1 = dsn
n g −qn fn.
Hence, when we pseudodivide again with respect to xn−1:
Rn−2 = dsn−1
n−1 (dsn
n g −qn fn) −qn−1 fn−1
= dsn−1
n−1 dsn
n g −qn−1 fn−1 −dsn−1
n−1 qn fn.
Continuing in the same way, we will eventually obtain an expression of the form
R0 = ds1
1 · · · dsn
n g −(A1 f1 + · · · + An fn),
which is what we wanted to show.
(ii) By the result of part (i), if R0 = 0, then at every point of the variety W =
V( f1, . . . , fn), either g or one of the dsj
j is zero. By Proposition 2, the variety V′ is
contained in W, so the same is true on V′. The assertion follows.
□
Even though they are not always polynomial relations in the ui alone, the equa-
tions dj = 0, where dj is the leading coefﬁcient of fj, can often be interpreted as loci
deﬁning degenerate special cases of our geometric conﬁguration.
Example 3 (continued). For instance, let us complete the application of Wu's
method to the Circle Theorem of Apollonius. Our goal is to show that
g = (x5 −x7)2 + (x6 −x8)2 −(x1 −x7)2 −x2
8 = 0

§5 Wu's Method
341
is a consequence of the hypotheses h1 = · · · = h8 = 0 (see (8) of §4). Using
f1, . . . , f8 computed above, we set R8 = g and compute the successive remainders
Ri−1 = Rem(Ri, fi, xi)
as i decreases from 8 to 1. When computing these remainders, we always use the
minimal exponent s in Proposition 1, and in some cases, we ignore constant factors
of the remainder. We obtain the following remainders.
R7 = x4x2
5 −2x4x5x7 + x4x2
6 −x4x2
1 + 2x4x1x7 + x6x2
1 −2x6x1x7
−x6x2
3 + 2x6x3x7 −x6x2
4,
R6 = x2
4x1x2
5 −x2
4x2
1x5 −x4x1x6x2
3 + x2
4x1x2
6 −x3
4x1x6 + x2
4x2
2x5
−x2
4x2
2x1 −x2x3
4x5 + x2x3
4x1 −x2x1x4x2
5 −x2x1x4x2
6
+x2x3x4x2
5 + x2x3x4x2
6 −x2x3x4x2
1 + x4x2
1x6x3 + x4x2
2x6x1
−x4x2
2x6x3 + x2x2
1x4x5 −x2x2
3x4x5 + x2x2
3x4x1,
R5 = u2
2x2
4x1x2
5 −u2
2x2
4x2
1x5 + u2
2x2
4x2
2x5 −u2
2x2
4x2
2x1 −u2
2x2x3
4x5
+u2
2x2x3
4x1 −x4u2
2x2x1x2
5 + x4u2
2x2x3x2
5 −x4u2
2x2x3x2
1
+x4u2
2x2x2
1x5 −x4u2
2x2x2
3x5 + x4u2
2x2x2
3x1 −u1x5u2x3
4x1
+x4u1x5u2x2
2x1 −x4u1x5u2x1x2
3 −x4u1x5u2x2
2x3
+x4x1x5u2x2
1x3 + u2
1x2
5x2
4x1 −x4u2
1x2
5x2x1 + x4u2
1x2
5x2x3,
R4 = −u4
2x4x2x3x2
1 −u4
2x2
4x2
2x1 + u4
2x4x2x2
3x1 + u4
2x3
4x2x1
−u2
2x4u2
1x2x3x2
1 −u2
2x2
4u2
1x2
2x1 + u2
2x4u2
1x2x2
3x1
+u2
2x3
4u2
1x2x1 −u4
2x3
4u1x2 −u3
2x3
4u2
1x1 + u4
2x2
4u1x2
2
−u4
2x2
4u1x2
1 + u3
2x4u2
1x2
2x1 −u3
2x4u2
1x1x2
3 −u4
2x4u1x2x2
3
+u4
2x4u1x2x2
1 −u3
2x4u2
1x2
2x3 + u3
2x4u2
1x2
1x3 + u4
2x2
4u2
1x1
−u4
2x4u2
1x2x1 + u4
2x4u2
1x2x3,
R3 = 4u5
2x2x2
3x1 −4u5
2u1x2x2
3 + 4u5
2u1x2x2
1 −4u5
2x2x3x2
1
−3u5
2u2
1x2x1 + 4u5
2u2
1x2x3 −4u4
2u2
1x1x2
3 −4u4
2u2
1x2
2x3
+2u4
2u2
1x2
2x1 + 4u4
2u2
1x2
1x3 −4u3
2u2
1x2x3x2
1 + 4u3
2u2
1x2x2
3x1
−2u6
2x2
2x1 −2u6
2u1x2
1 + 2u6
2u1x2
2 + u6
2u2
1x1 + u7
2x2x1
−u7
2u1x2,
R2 = 2u5
2u1x2x2
1 −2u5
2u2
1x2x1 + 2u4
2u2
1x2
2x1 −2u6
2x2
2x1
−2u6
2u1x2
1 + 2u6
2u1x2
2 + u6
2u2
1x1 + u7
2x2x1 −u7
2u1x2
+u5
2u3
1x2 −2u4
2u3
1x2
2 + 2u4
2u3
1x2
1 −2u3
2u3
1x2x2
1 + u3
2u4
1x2x1
−u4
2u4
1x1,
R1 = −2u6
2u1x2
1 −u4
2u4
1x1 + u6
2u2
1x1 + 2u4
2u3
1x2
1,
R0 = 0.

342
Chapter 6 Robotics and Automatic Geometric Theorem Proving
By Theorem 4, Wu's method conﬁrms that the Circle Theorem is valid when
none of the leading coefﬁcients of the fj is zero. The nontrivial conditions here are
d5 = u2
1 + u2
2 ̸= 0,
d6 = u2 ̸= 0,
d7 = 2x1x2 −2x2x3 −2x1x4 ̸= 0,
d8 = 2x4 ̸= 0.
The second condition in this list is u2 ̸= 0, which says that the vertices A and C of the
right triangle ΔABC are distinct [recall we chose coordinates so that A = (0, 0) and
C = (0, u2) in Example 3 of §4]. This also implies the ﬁrst condition since u1 and
u2 are real. The condition 2x4 ̸= 0 is equivalent to u2 ̸= 0 by the hypothesis h4 = 0.
Finally, d7 ̸= 0 says that the vertices of the triangle are distinct (see Exercise 5).
From this analysis, we see that the Circle Theorem actually follows generically from
its hypotheses as in §4.
The elementary version of Wu's method only gives g = 0 under the side condi-
tions dj ̸= 0. In particular, note that in a case where V′ is reducible, it is entirely
conceivable that one of the dj could vanish on an entire component of V′. If this
happened, there would be no conclusion concerning the validity of the theorem for
geometric conﬁgurations corresponding to points in that component.
Indeed, a much stronger version of Theorem 4 is known when the subvariety V′
for a given set of hypotheses is irreducible. With the extra algebraic tools we have
omitted (Ritt's decomposition algorithm), it can be proved that there are special
triangular form sets of fj (called characteristic sets) with the property that R0 = 0
is a necessary and sufﬁcient condition for g to lie in I(V′). In particular, it is never
the case that one of the leading coefﬁcients of the fj is identically zero on V′ so that
R0 = 0 implies that g must vanish on all of V′. We refer the interested reader to
CHOU (1988) for the details. Other treatments of characteristic sets and the Wu-Ritt
algorithm can be found in MISHRA (1993) and WANG (2001).
Finally, we will brieﬂy compare Wu's method with the method based on Gröb-
ner bases introduced in §4. These two methods apply to exactly the same class of
geometric theorems and they usually yield equivalent results. Both make essential
use of a division algorithm to determine whether a polynomial is in a given ideal
or not. However, as we can guess from the triangulation procedure described above,
the basic version of Wu's method at least is likely to be much quicker on a given
problem. The reason is that simply triangulating a set of polynomials usually req-
uires much less effort than computing a Gröbner basis for the ideal they generate, or
for the ideal H = ⟨h1, . . . , hn, 1 −yg⟩. This pattern is especially pronounced when
the original polynomials themselves are nearly in triangular form, which is often
the case for the hypotheses of a geometric theorem. In a sense, this superiority of
Wu's method is only natural since Gröbner bases contain much more information
than triangular form sets. Note that we have not claimed anywhere that the trian-
gular form set of polynomials even generates the same ideal as the hypotheses in
either R[u1, . . . , um, x1, . . . , xn] or R(u1, . . . , um)[x1, . . . , xn]. In fact, this is not true
in general (Exercise 4). Wu's method is an example of a technique tailored to solve a

§5 Wu's Method
343
particular problem. Such techniques can often outperform general techniques (such
as computing Gröbner bases) that do many other things besides.
Readers interested in pursuing this topic should consult CHOU (1988), the sec-
ond half of which is an annotated collection of 512 geometric theorems proved by
Chou's program implementing Wu's method. WU (1983) is a reprint of the original
paper that introduced these ideas.
EXERCISES FOR §5
1. This problem completes the proof of Proposition 1 begun in the text.
a. Complete the proof of (i) of the proposition.
b. Show that q, r in the equation ds
m f = qg+r in the proposition are deﬁnitely not unique
if no condition is placed on the exponent s.
2. Establish the claim stated after Proposition 1 that pseudodivision is equivalent to ordinary
polynomial division in the ring K[y], where K = k(x1, . . . , xn).
3. Show that there is a unique minimal s ≤p −m + 1 in Proposition 1 for which the
equation ds
m f = qg + r exists, and that q and r are unique when s is minimal. Hint: Use
the uniqueness of the quotient and remainder for division in k(x1, . . . , xn)[y].
4. Show by example that applying the triangulation procedure described in this section to two
polynomials h1, h2 ∈k[x1, x2] can yield polynomials f1, f2 that generate an ideal strictly
smaller than ⟨h1, h2⟩. The same can be true for larger sets of polynomials as well.
5. Show that the nondegeneracy condition d7 ̸= 0 for the Circle Theorem is automatically
satisﬁed if u1 and u2 are nonzero.
6. Use Wu's method to verify each of the following theorems. In each case, state the condi-
tions dj ̸= 0 under which Theorem 4 implies that the conclusion follows from the hyp-
otheses. If you also did the corresponding exercises in §4, try to compare the time and/or
effort involved with each method.
a. The theorem on the diagonals of a parallelogram (Example 1 of §4).
b. The theorem on the orthocenter of a triangle (Exercise 5 of §4).
c. The theorem on the centroid of a triangle (Exercise 6 of §4).
d. The theorem on the Euler line of a triangle (Exercise 7 of §4).
e. Pappus's Theorem (Exercise 8 of §4).
7. Consider the theorem from Exercise 17 of §4 (for which V′ is reducible according to
a direct translation of the hypotheses). Apply Wu's method to this problem. (Your ﬁnal
remainder should be nonzero here.)

Chapter 7
Invariant Theory of Finite Groups
Invariant theory has had a profound effect on the development of algebraic geometry.
For example, the Hilbert Basis Theorem and Hilbert Nullstellensatz, which play a
central role in the earlier chapters in this book, were proved by Hilbert in the course
of his investigations of invariant theory.
In this chapter, we will study the invariants of ﬁnite groups. The basic goal is to
describe all polynomials that are unchanged when we change variables according to
a given ﬁnite group of matrices. Our treatment will be elementary and by no means
complete. In particular, we do not presume a prior knowledge of group theory.
§1
Symmetric Polynomials
Symmetric polynomials arise naturally when studying the roots of a polynomial.
For example, consider the cubic f = x3 +bx2 +cx+d and let its roots be α1, α2, α3.
Then
x3 + bx2 + cx + d = (x −α1)(x −α2)(x −α3).
If we expand the right-hand side, we obtain
x3 + bx2 + cx + d = x3 −(α1 + α2 + α3)x2 + (α1α2 + α1α3 + α2α3)x −α1α2α3,
and thus,
b = −(α1 + α2 + α3),
c = α1α2 + α1α3 + α2α3,
(1)
d = −α1α2α3.
This shows that the coefﬁcients of f are polynomials in its roots. Further, since
changing the order of the roots does not affect f, it follows that the polynomials
expressing b, c, d in terms of α1, α2, α3 are unchanged if we permute α1, α2, α3.
Such polynomials are said to be symmetric. The general concept is deﬁned as
follows.
© Springer International Publishing Switzerland 2015
D.A. Cox et al., Ideals, Varieties, and Algorithms, Undergraduate Texts
in Mathematics, DOI 10.1007/978-3-319-16721-3_7
345

346
Chapter 7 Invariant Theory of Finite Groups
Deﬁnition 1. A polynomial f ∈k[x1, . . . , xn] is symmetric if
f(xi1, . . . , xin) = f(x1, . . . , xn)
for every possible permutation xi1, . . . , xin of the variables x1, . . . , xn.
For example, if the variables are x, y, and z, then x2+y2+z2 and xyz are obviously
symmetric. The following symmetric polynomials will play an important role in our
discussion.
Deﬁnition 2. Given variables x1, . . . , xn, we deﬁne the elementary symmetric
polynomials σ1, . . . , σn ∈k[x1, . . . , xn] by the formulas
σ1 = x1 + · · · + xn,
...
σr =

i1<i2<···<ir
xi1xi2 · · · xir,
...
σn = x1x2 · · · xn.
Thus, σr is the sum of all monomials that are products of r distinct variables.
In particular, every term of σr has total degree r. To see that these polynomials
are indeed symmetric, we will generalize observation (1). Namely, introduce a new
variable X and consider the polynomial
(2)
f(X) = (X −x1)(X −x2) · · · (X −xn)
with roots x1, . . . , xn. If we expand the right-hand side, it is straightforward to show
that
f(X) = Xn −σ1Xn−1 + σ2Xn−2 + · · · + (−1)n−1σn−1X + (−1)nσn
(we leave the details of the proof as an exercise). Now suppose that we rearrange
x1, . . . , xn. This changes the order of the factors on the right-hand side of (2), but f
itself will be unchanged. Thus, the coefﬁcients (−1)rσr of f are symmetric polyno-
mials.
One corollary is that for any polynomial with leading coefﬁcient 1, the other
coefﬁcients are the elementary symmetric polynomials of its roots (up to a factor of
±1). The exercises will explore some interesting consequences of this fact.
From the elementary symmetric polynomials, we can construct other symmetric
polynomials by taking polynomials in σ1, . . . , σn. Thus, for example, if n = 3,
σ2
2 −σ1σ3 = x2y2 + x2yz + x2z2 + xy2z + xyz2 + y2z2
is a symmetric polynomial. What is more surprising is that all symmetric polyno-
mials can be represented in this way.

§1 Symmetric Polynomials
347
Theorem 3 (The Fundamental Theorem of Symmetric Polynomials).
Every
symmetric polynomial in k[x1, . . . , xn] can be written uniquely as a polynomial in
the elementary symmetric polynomials σ1, . . . , σn.
Proof. We will use lex order with x1 > x2 > · · · > xn. Given a nonzero symmetric
polynomial f ∈k[x1, . . . , xn], let LT( f) = axα. If α = (α1, . . . , αn), we ﬁrst claim
that α1 ≥α2 ≥· · · ≥αn. To prove this, suppose that αi < αi+1 for some i. Let
β be the exponent vector obtained from α by switching αi and αi+1. We will write
this as β = (. . . , αi+1, αi, . . .). Since axα is a term of f, it follows that axβ is a term
of f(. . . , xi+1, xi, . . .). But f is symmetric, so that f(. . . , xi+1, xi, . . .) = f, and thus,
axβ is a term of f. This is impossible since β > α under lex order, and our claim is
proved.
Now let
h = σα1−α2
1
σα2−α3
2
· · · σαn−1−αn
n−1
σαn
n .
To compute the leading term of h, ﬁrst note that LT(σr) = x1x2 · · · xr for 1 ≤r ≤n.
Hence,
(3)
LT(h) = LT(σα1−α2
1
σα2−α3
2
· · · σαn
n )
= LT(σ1)α1−α2 LT(σ2)α2−α3 · · · LT(σn)αn
= xα1−α2
1
(x1x2)α2−α3 · · · (x1 . . . xn)αn
= xα1
1 xα2
2 · · · xαn
n = xα.
It follows that f and ah have the same leading term, and thus,
multideg( f −ah) < multideg( f)
whenever f −ah ̸= 0.
Now set f1 = f −ah and note that f1 is symmetric since f and ah are. Hence,
if f1 ̸= 0, we can repeat the above process to form f2 = f1 −a1h1, where a1 is
a constant and h1 is a product of σ1, . . . , σn to various powers. Further, we know
that LT( f2) < LT( f1) when f2 ̸= 0. Continuing in this way, we get a sequence of
polynomials f, f1, f2, . . . with
multideg( f) > multideg( f1) > multideg( f2) > · · · .
Since lex order is a well-ordering, the sequence must be ﬁnite. But the only way the
process terminates is when ft+1 = 0 for some t. Then it follows easily that
f = ah + a1h1 + · · · + atht,
which shows that f is a polynomial in the elementary symmetric polynomials.
It remains to prove uniqueness. Suppose that we have a symmetric polynomial f
which can be written as
f = g1(σ1, . . . , σn) = g2(σ1, . . . , σn).

348
Chapter 7 Invariant Theory of Finite Groups
Here, g1 and g2 are polynomials in n variables, say y1, . . . , yn. We need to prove that
g1 = g2 in k[y1, . . . , yn].
If we set g = g1 −g2, then g(σ1, . . . , σn) = 0 in k[x1, . . . , xn]. Uniqueness will
be proved if we can show that g = 0 in k[y1, . . . , yn]. So suppose that g ̸= 0. If
we write g = 
β aβyβ, then g(σ1, . . . , σn) is a sum of the polynomials gβ =
aβσβ1
1 σβ2
2 · · · σβn
n , where β = (β1, . . . , βn). Furthermore, the argument used in (3)
above shows that
LT(gβ) = aβxβ1+···+βn
1
xβ2+···+βn
2
· · · xβn
n .
It is an easy exercise to show that the map
(β1, . . . βn) 
−→(β1 + · · · + βn, β2 + · · · + βn, . . . , βn)
is one-to-one. Thus, the gβ's have distinct leading terms. In particular, if we pick
β such that LT(gβ) > LT(gγ) for all γ ̸= β, then LT(gβ) will be greater than
all terms of the gγ's. It follows that there is nothing to cancel LT(gβ) and, thus,
g(σ1, . . . , σn) cannot be zero in k[x1, . . . , xn]. This contradiction completes the proof
of the theorem.
□
The proof just given is due to Gauss, who needed the properties of symmetric
polynomials for his second proof (dated 1816) of the fundamental theorem of alge-
bra. Here is how Gauss states lex order: "Then among the two terms
Maαbβcγ · · ·
and
Maα′bβ′cγ′ · · ·
superior order is attributed to the ﬁrst rather than the second, if
either α > α′, or α = α′ and β > β′, or α = α′, β = β′ and γ > γ′, etc."
[see p. 36 of GAUSS (1876)]. This is the earliest known explicit statement of lex
order.
Note that the proof of Theorem 3 gives an algorithm for writing a symmetric
polynomial in terms of σ1, . . . , σn. For an example of how this works, consider
f = x3y + x3z + xy3 + xz3 + y3z + yz3 ∈k[x, y, z].
The leading term of f is x3y = LT(σ2
1σ), which gives
f1 = f −σ2
1σ2 = −2x2y2 −5x2yz −2x2z2 −5xy2z −5xyz2 −2y2z2.
The leading term is now −2x2y2 = −2LT(σ2
2), and thus,
f2 = f −σ2
1σ2 + 2σ2
2 = −x2yz −xy2z −xyz2.
Then one easily sees that
f3 = f −σ2
1σ2 + 2σ2
2 + σ1σ3 = 0

§1 Symmetric Polynomials
349
and hence,
f = σ2
1σ2 −2σ2
2 −σ1σ3
is the unique expression of f in terms of the elementary symmetric polynomials.
Surprisingly, we do not need to write a general algorithm for expressing a sym-
metric polynomial in σ1, . . . , σn, for we can do this process using the division algo-
rithm from Chapter 2. We can even use the division algorithm to check for symme-
try. The precise method is as follows.
Proposition 4. In the ring k[x1, . . . , xn, y1, . . . , yn], ﬁx a monomial order where any
monomial involving one of x1, . . . , xn is greater than all monomials in k[y1, . . . , yn].
Let G be a Gröbner basis of ⟨σ1 −y1, . . . , σn −yn⟩⊆k[x1, . . . , xn, y1, . . . , yn]. Given
f ∈k[x1, . . . , xn], let g = f G be the remainder of f on division by G. Then:
(i) f is symmetric if and only if g ∈k[y1, . . . , yn].
(ii) If f is symmetric, then f = g(σ1, . . . , σn) is the unique expression of f as a
polynomial in the elementary symmetric polynomials σ1, . . . , σn.
Proof. As above, we have f ∈k[x1, . . . , xn], and g ∈k[x1, . . . , xn, y1, . . . , yn] is its
remainder on division by G = {g1, . . . , gt}. This means that
f = A1g1 + · · · + Atgt + g,
where A1, . . . , At ∈k[x1, . . . xn, y1, . . . , yn]. We can assume that gi ̸= 0 for all i.
To prove (i), ﬁrst suppose that g ∈k[y1, . . . , yn]. Then for each i, substitute σi for
yi in the above formula for f. This will not affect f since it involves only x1, . . . , xn.
The crucial observation is that under this substitution, every polynomial in the ideal
⟨σ1 −y1, . . . , σn −yn⟩goes to zero. Since g1, . . . , gt lie in this ideal, it follows that
f = g(σ1, . . . , σn).
Hence, f is symmetric.
Conversely, suppose that f ∈k[x1, . . . , xn] is symmetric. Then f = g(σ1, . . . , σn)
for some g ∈k[y1, . . . , yn]. We want to show that g is the remainder of f on divi-
sion by G. To prove this, ﬁrst note that in k[x1, . . . , xn, y1, . . . , yn], a monomial in
σ1, . . . , σn can be written as follows:
σα1
1 · · · σαn
n
= (y1 + (σ1 −y1))α1 · · · (yn + (σn −yn))αn
= yα1
1 · · · yαn
n + B1 · (σ1 −y1) + · · · + Bn · (σn −yn)
for some B1, . . . , Bn ∈k[x1, . . . , xn, y1, . . . , yn]. Multiplying by an appropriate con-
stant and adding over the exponents appearing in g, it follows that
g(σ1, . . . , σn) = g(y1, . . . , yn) + C1 · (σ1 −y1) + · · · + Cn · (σn −yn),
where C1, . . . , Cn ∈k[x1, . . . , xn, y1, . . . , yn]. Since f = g(σ1, . . . , σn), we can write
this as
(4)
f = C1 · (σ1 −y1) + · · · + Cn · (σn −yn) + g(y1, . . . , yn).

350
Chapter 7 Invariant Theory of Finite Groups
We want to show that g is the remainder of f on division by G.
The ﬁrst step is to show that no term of g is divisible by an element of LT(G).
If this were false, then there would be gi ∈G such that LT(gi) divides some term
of g. Hence, LT(gi) would involve only y1, . . . , yn since g ∈k[y1, . . . , yn]. By our
hypothesis on the ordering, it would follow that gi ∈k[y1, . . . , yn]. Now replace
every yi with the correspondingσi. Since gi ∈⟨σ1−y1, . . . , σn−yn⟩, we have already
observed that gi goes to zero under the substitution yi 
→σi. Then gi ∈k[y1, . . . , yn]
would mean gi(σ1, . . . , σn) = 0. By the uniqueness part of Theorem 3, this would
imply gi = 0, which is impossible since gi ̸= 0. This proves our claim. It follows
that in (4), no term of g is divisible by an element of LT(G), and since G is a Gröbner
basis, Proposition 1 of Chapter 2, §6 tells us that g is the remainder of f on division
by G. This proves that the remainder lies in k[y1, . . . , yn] when f is symmetric.
Part (ii) of the proposition follows immediately from the above arguments, and
we are done.
□
A seeming drawback to the above proposition is the necessity to compute a Gröb-
ner basis for ⟨σ1 −y1, . . . , σn −yn⟩. However, when we use lex order, it is quite
simple to write down a Gröbner basis for this ideal. We ﬁrst need some notation.
Given variables u1, . . . , us, let
hj(u1, . . . , us) =

|α|=j
uα
be the sum of all monomials of total degree j in u1, . . . , us. Then we get the following
Gröbner basis.
Proposition 5. Fix lex order on the polynomial ring k[x1, . . . , xn, y1, . . . , yn] with
x1 > · · · > xn > y1 > · · · > yn. Then the polynomials
gj = hj(xj, . . . , xn) +
j

i=1
(−1)ihj−i(xj, . . . , xn)yi,
j = 1, . . . , n,
form a Gröbner basis for the ideal ⟨σ1 −y1, . . . , σn −yn⟩.
Proof. We will sketch the proof, leaving most of the details for the exercises. The
ﬁrst step is to note the polynomial identity
(5)
0 = hj(xj, . . . , xn) +
j

i=1
(−1)ihj−i(xj, . . . , xn)σi.
The proof will be covered in Exercises 10 and 11.
The next step is to show that g1, . . . , gn form a basis of ⟨σ1 −y1, . . . , σn −yn⟩. If
we subtract the identity (5) from the deﬁnition of gj, we obtain
(6)
gj =
j

i=1
(−1)ihj−i(xj, . . . , xn)(yi −σi),

§1 Symmetric Polynomials
351
which proves that ⟨g1, . . . , gn⟩⊆⟨σ1 −y1, . . . , σn −yn⟩. To prove the opposite
inclusion, note that since h0 = 1, we can write (6) as
(7)
gj = (−1)j(yj −σj) +
j−1

i=1
(−1)ihj−i(xj, . . . , xn)(yi −σi).
Then induction on j shows that ⟨σ1 −y1, . . . , σn −yn⟩⊆⟨g1, . . . , gn⟩(see Exer-
cise 12).
Finally, we need to show that we have a Gröbner basis. In Exercise 12, we will
ask you to prove that
LT(gj) = x j
j .
This is where we use lex order with x1 > · · · > xn > y1 > · · · > yn. Thus the
leading terms of g1, . . . , gn are relatively prime, and using the theory developed
in §§9 and 10 of Chapter 2, it is easy to show that we have a Gröbner basis (see
Exercise 12 for the details). This completes the proof.
□
In dealing with symmetric polynomials, it is often convenient to work with ones
that are homogeneous. Here is the deﬁnition.
Deﬁnition 6. A polynomial f ∈k[x1, . . . , xn] is homogeneous of total degree m
provided that every term appearing in f has total degree m.
As an example, note that the i-th elementary symmetric polynomial σi is homo-
geneous of total degree i. An important fact is that every polynomial can be written
uniquely as a sum of homogeneous polynomials. Namely, given f ∈k[x1, . . . , xn],
let fm be the sum of all terms of f of total degree m. Then each fm is homogeneous
and f = 
m fm. We call fm the m-th homogeneous component of f.
We can understand symmetric polynomials in terms of their homogeneous com-
ponents as follows.
Proposition 7. A polynomial f ∈k[x1, . . . , xn] is symmetric if and only if all of its
homogeneous components are symmetric.
Proof. Assume that f is symmetric and let xi1, . . . , xin be a permutation of x1, . . . , xn.
This permutation takes a term of f of total degree m to one of the same total degree.
Since f(xi1, . . . , xin) = f(x1, . . . , xn), it follows that the m-th homogeneous compo-
nent must also be symmetric. The converse is trivial and the proposition follows. □
Proposition 7 tells us that when working with a symmetric polynomial, we can
assume that it is homogeneous. In the exercises, we will explore what this implies
about how the polynomial is expressed in terms of σ1, . . . , σn.
The ﬁnal topic we will explore is a different way of writing symmetric polyno-
mials. Speciﬁcally, we will consider the power sums
sj = x j
1 + x j
2 + · · · + x j
n.
Note that sj is symmetric. Then we can write an arbitrary symmetric polynomial in
terms of s1, . . . , sn as follows.

352
Chapter 7 Invariant Theory of Finite Groups
Theorem 8. If k is a ﬁeld containing the rational numbers Q, then every symmet-
ric polynomial in k[x1, . . . , xn] can be written as a polynomial in the power sums
s1, . . . , sn.
Proof. Since every symmetric polynomial is a polynomial in the elementary sym-
metric polynomials (by Theorem 3), it sufﬁces to prove that σ1, . . . , σn are polyno-
mials in s1, . . . , sn. For this purpose, we will use the Newton identities, which state
that
sj −σ1sj−1 + · · · + (−1)j−1σj−1s1 + (−1)jjσj = 0,
1 ≤j ≤n,
sj −σ1sj−1 + · · · + (−1)n−1σn−1sj−n+1 + (−1)nσnsj−n = 0,
j > n.
The proof of these identities will be given in the exercises.
We now prove by induction on j that σj is a polynomial in s1, . . . , sn. This is true
for j = 1 since σ1 = s1. If the claim is true for 1, 2, . . . , j −1, then the Newton
identities imply that
σj = (−1)j−1 1
j

sj −σ1sj−1 + · · · + (−1)j−1σj−1s1
	
.
We can divide by the integer j because Q is contained in the coefﬁcient ﬁeld (see
Exercise 16 for an example of what can go wrong when Q ̸⊆k). Then our inductive
assumption and the above equation show that σj is a polynomial in s1, . . . , sn.
□
As a consequence of Theorems 3 and 8, every elementary symmetric polynomial
can be written in terms of power sums, and vice versa. For example,
s2 = σ2
1 −2σ2 ←→σ2 = 1
2(s2
1 −s2),
s3 = σ3
1 −3σ1σ2 + 3σ3 ←→σ3 = 1
6(s3
1 −3s1s2 + 2s3).
Power sums will be unexpectedly useful in §3 when we give an algorithm for ﬁnding
the invariant polynomials for a ﬁnite group.
EXERCISES FOR §1
1. Prove that f ∈k[x, y, z] is symmetric if and only if f (x, y, z) = f (y, x, z) = f (y, z, x).
2. (Requires abstract algebra) Prove that f ∈k[x1, . . . , xn] is symmetric if and only if
f (x1, x2, x3, . . . , xn) = f (x2, x1, x3, . . . , xn) = f (x2, x3, . . . xn, x1). Hint: Show that the
cyclic permutations (1, 2) and (1, 2, . . . , n) generate the symmetric group Sn. See Exer-
cise 4 in Section 3.5 of DUMMIT and FOOTE (2004).
3. Let σ(i)
j
denote the j-th elementary symmetric polynomial in x1, . . . , xi−1, xi+1 . . . , xn
for j < n. The superscript "(i)" tells us to omit the variable xi. Also set σ(i)
n
= 0 and
σ0 = 1. Prove that σj = σ(i)
j
+ xiσ(i)
j−1 for all i, j. This identity is useful in induction
arguments involving elementary symmetric polynomials.
4. As in (2), let f (X) = (X −x1)(X −x2) · · · (X −xn). Prove that f = Xn −σ1Xn−1 +
σ2Xn−2 + · · · + (−1)n−1σn−1X + (−1)nσn. Hint: You can give an induction proof using
the identities of Exercise 3.

§1 Symmetric Polynomials
353
5. Consider the polynomial
f = (x2 + y2)(x2 + z2)(y2 + z2) ∈k[x, y, z].
a. Use the method given in the proof of Theorem 3 to write f as a polynomial in the
elementary symmetric polynomials σ1, σ2, σ3.
b. Use the method described in Proposition 4 to write f in terms of σ1, σ2, σ3.
You can use a computer algebra system for both parts of the exercise. Note that by
stripping off the coefﬁcients of powers of X in the polynomial (X −x)(X −y)(X −z),
you can get the computer to generate the elementary symmetric polynomials.
6. If the variables are x1, . . . , xn, show that 
i̸=j x2
i xj = σ1σ2 −3σ3. Hint: If you get stuck,
see Exercise 13. Note that a computer algebra system cannot help here!
7. Let f = xn + a1xn−1 + · · · + an ∈k[x] have roots α1, . . . , αn, which lie in some bigger
ﬁeld K containing k.
a. Prove that any symmetric polynomial g(α1, . . . , αn) in the roots of f can be expressed
as a polynomial in the coefﬁcients a1, . . . , an of f .
b. In particular, if the symmetric polynomial g has coefﬁcients in k, conclude that
g(α1, . . . , αn) ∈k.
8. As in Exercise 7, let f = xn + a1xn−1 + · · · + an ∈k[x] have roots α1, . . . , αn, which lie
in some bigger ﬁeld K containing k. The discriminant of f is deﬁned to be
D( f ) =

i̸=j
(αi −αj)
a. Use Exercise 7 to show that D( f ) is a polynomial in a1, . . . , an.
b. When n = 2, express D( f ) in terms of a1 and a2. Does your result look familiar?
c. When n = 3, express D( f ) in terms of a1, a2, a3.
d. Explain why a cubic polynomial x3 + a1x2 + a2x + a3 has a multiple root if and only
if −4a3
1a3 + a2
1a2
2 + 18a1a2a3 −4a3
2 −27a2
3 = 0.
9. Given a cubic polynomial f = x3 +a1x2 +a2x+a3, what condition must the coefﬁcients
of f satisfy in order for one of its roots to be the average of the other two? Hint: If α1 is
the average of the other two, then 2α1 −α2 −α3 = 0. But it could happen that α2 or
α3 is the average of the other two. Hence, you get a condition stating that the product of
three expressions similar to 2α1 −α2 −α3 is equal to zero. Now use Exercise 7.
10. As in Proposition 5, let hj(x1, . . . , xn) be the sum of all monomials of total degree j in
x1, . . . , xn. Also, let σ0 = 1 and σi = 0 if i > n. The goal of this exercise is to show that
if j > 0, then
0 =
j

i=0
(−1)ihj−i(x1, . . . , xn)σi(x1, . . . , xn).
In Exercise 11, we will use this to prove the closely related identity (5) that appears in
the text. To prove the above identity, we will compute the coefﬁcients of the monomials
xα that appear in hj−iσi. Since every term in hj−iσi has total degree j, we can assume that
xα has total degree j. We will let a denote the number of variables that actually appear
in xα.
a. If xα appears in hj−iσi, show that i ≤a. Hint: How many variables appear in each
term of σi?
b. If i ≤a, show that exactly
a
i

terms of σi involve only variables that appear in xα.
Note that all of these terms have total degree i.
c. If i ≤a, show that xα appears in hj−iσi with coefﬁcient
a
i

. Hint: This follows from
part (b) because hj−i is the sum of all monomials of total degree j −i, and each
monomial has coefﬁcient 1.

354
Chapter 7 Invariant Theory of Finite Groups
d. Conclude that the coefﬁcient of xα in j
i=0(−1)ihj−iσi is a
i=0(−1)ia
i

. Then use
the binomial theorem to show that the coefﬁcient of xα is zero. This will complete
the proof of our identity.
11. In this exercise, we will prove the identity
0 = hj(xj, . . . , xn) +
j

i=1
(−1)ihj−i(xk, . . . , xn)σi(x1, . . . , xn)
used in the proof of Proposition 5. As in Exercise 10, let σ0 = 1, so that the identity can
be written more compactly as
0 =
j

i=0
(−1)ihj−i(xj, . . . , xn)σi(x1, . . . , xn)
The idea is to separate out the variables x1, . . . , xj−1. To this end, if S ⊆{1, . . . , j −1},
let xS be the product of the corresponding variables and let |S| denote the number of
elements in S.
a. Prove that
σi(x1, . . . , xn) =

S⊆{1,...,j−1}
xSσi−|S|(xj, . . . , xn),
where we set σm = 0 if m < 0.
b. Prove that
j

i=0
(−1)ihj−i(xj, . . . , xn)σi(x1, . . . , xn)
=

S⊂{1,...,j−1}
xS

j

i=|S|
(−1)ihj−i(xj, . . . , xn)σi−|S|(xj, . . . , xn)

.
c. Use Exercise 10 to conclude that the sum inside the parentheses is zero for every S.
This proves the desired identity. Hint: Let ℓ= i −|S|.
12. This exercise is concerned with the proof of Proposition 5. Let gj be as deﬁned in the
statement of the proposition.
a. Use equation (7) to prove that ⟨σ1 −y1, . . . , σn −yn⟩⊆⟨g1, . . . , gn⟩.
b. Prove that LT(gj) = x j
j .
c. Combine part (b) with Theorem 3 of Chapter 2, §9 and Proposition 1 of Chapter 2,
§10 to prove that g1, . . . , gn form a Gröbner basis.
13. Let f be a homogeneous symmetric polynomial of total degree d.
a. Show that f can be written as a linear combination (with coefﬁcients in k) of polyno-
mials of the form σi1
1 σi2
2 · · · σinn where d = i1 + 2i2 + · · · + nin.
b. Let m be the maximum degree of x1 that appears in f . By symmetry, m is the maxi-
mum degree in f of any variable. If σi1
1 σi2
2 · · · σinn appears in the expression of f from
part (a), then prove that i1 + i2 + · · · + in ≤m.
c. Show that the symmetric polynomial 
i̸=j x2
i xj can be written as aσ1σ2 + bσ3 for
some constants a and b. Then determine a and b. Compare this to what you did in
Exercise 6.
14. In this exercise, you will prove the Newton identities used in the proof of Theorem 8.
Let the variables be x1, . . . , xn.
a. Set σ0 = 1 and σi = 0 whenever i < 0 or i > n. Then show that the Newton
identities are equivalent to

§2 Finite Matrix Groups and Rings of Invariants
355
sj −σ1sj−1 + · · · + (−1)j−1σj−1s1 + (−1)jjσj = 0,
j ≥1.
b. Prove the identity of part (a) by induction on n. Hint: Similar to Exercise 3, let s(n)
j
be
the jth power sum of x1, . . . , xn−1, i.e., all variables except xn. Then use Exercise 3
and note that sj = s(n)
j
+ x j
n.
15. This exercise will use the identity (5) to prove the following nonsymmetric Newton iden-
tities:
x j
i −σ1x j−1
i
+ · · · + (−1)j−1σk−1xi + (−1)jσk = (−1)jσ(i)
j ,
1 ≤j < n,
x j
i −σ1x j−1
i
+ · · · + (−1)n−1σn−1x j−n+1
i
+ (−1)nσnx j−n
i
= 0,
j ≥n,
where as in Exercise 3, σ(i)
j
= σj(x1, . . . , xi−1, xi+1, . . . , xn) is the j-th elementary sym-
metric polynomial of all variables except xi. We will then give a second proof of the
Newton identities.
a. Show that the nonsymmetric Newton identity for j = n follows from (5). Then prove
that this implies the nonsymmetric Newton identities for j ≥n. Hint: Treat the case
i = n ﬁrst.
b. Show that the nonsymmetric Newton identity for j = n −1 follows from the one for
j = n. Hint: σn = xiσ(i)
n−1.
c. Prove the nonsymmetric Newton identity for j < n by decreasing induction on j.
Hint: By Exercise 3, σj = σ(i)
j
+ xiσ(i)
j−1.
d. Prove that n
i=1 σ(i)
j
= (n −j)σj. Hint: A term xi1 · · · xij, where 1 ≤i1 < · · · < ij ≤
n, appears in how many of the σ(i)
j 's?
e. Prove the Newton identities.
16. Consider the ﬁeld F2 = {0, 1} consisting of two elements. Show that it is impossible
to express the symmetric polynomial xy ∈F2[x, y] as a polynomial in s1 and s2 with
coefﬁcients F2. Hint: Show that s2 = s2
1!
17. Express s4 as a polynomial in σ1, . . . , σ4 and express σ4 as a polynomial in s1 . . . , s4.
18. We can use the division algorithm to automate the process of writing a polynomial
g(σ1, . . . , σn) in terms of s1, . . . , sn. Namely, regard σ1, . . . , σn, s1, . . . , sn as variables
and consider the polynomials
gj = sj = σ1sj−1 + · · · + (−1)j−1σj−1s1 + (−1)jjσj,
1 ≤j ≤n.
Show that if we use the correct lex order, the remainder of g(σ1, . . . , σn) on division by
g1, . . . , gn will be a polynomial h(s1, . . . , sn) such that g(σ1, . . . , σn) = h(s1, . . . , sn).
Hint: The lex order you need is not σ1 > σ2 > · · · > σn > s1 > · · · > sn.
§2
Finite Matrix Groups and Rings of Invariants
In this section, we will give some basic deﬁnitions for invariants of ﬁnite matrix
groups and we will compute some examples to illustrate what questions the general
theory should address. For the rest of this chapter, we will always assume that our
ﬁeld k contains the rational numbers Q. Such ﬁelds are said to be of characteristic
zero.
Deﬁnition 1. Let GL(n, k) be the set of all invertible n × n matrices with entries in
the ﬁeld k.

356
Chapter 7 Invariant Theory of Finite Groups
If A and B are invertible n × n matrices, then linear algebra implies that the
product AB and inverse A−1 are also invertible (see Exercise 1). Also, recall that the
n × n identity matrix In has the properties that A · In = In · A = A and A · A−1 = In
for all A ∈GL(n, k). In the terminology of Appendix A, we say that GL(n, k) is a
group.
Note that A ∈GL(n, k) gives an invertible linear map LA : kn →kn via matrix
multiplication. Since every linear map from kn to itself arises in this way, it is cus-
tomary to call GL(n, k) the general linear group.
We will be most interested in the following subsets of GL(n, k).
Deﬁnition 2. A ﬁnite subset G ⊆GL(n, k) is called a ﬁnite matrix group provided
it is nonempty and closed under matrix multiplication. The number of elements of
G is called the order of G and is denoted |G|.
Let us look at some examples of ﬁnite matrix groups.
Example 3. Suppose that A ∈GL(n, k) is a matrix such that Am = In for some
positive integer m. If m is the smallest such integer, then it is easy to show that
Cm = {In, A, . . . , Am−1} ⊆GL(n, k)
is closed under multiplication (see Exercise 2) and, hence, is a ﬁnite matrix group.
We call Cm a cyclic group of order m. An example is given by
A =

0 −1
1
0

∈GL(2, k).
One can check that A4 = I2, so that C4 = {I2, A, A2, A3} is a cyclic matrix group of
order 4 in GL(2, k).
Example 4. An important example of a ﬁnite matrix group comes from the
permutations of variables discussed in §1. Let τ denote a permutation xi1, . . . , xin
of x1, . . . , xn. Since τ is determined by what it does to the subscripts, we will set
i1 = τ(1), i2 = τ(2), . . . , in = τ(n). Then the corresponding permutation of vari-
ables is xτ(1), . . . , xτ(n).
We can create a matrix from τ as follows. Consider the linear map that takes
(x1, . . . , xn) to (xτ(1), . . . , xτ(n)). The matrix representing this linear map is denoted
Mτ and is called a permutation matrix. Thus, Mτ has the property that under matrix
multiplication, it permutes the variables according to τ:
Mτ ·
⎛
⎜
⎝
x1
...
xn
⎞
⎟
⎠=
⎛
⎜
⎝
xτ(1)
...
xτ(n)
⎞
⎟
⎠.
We leave it as an exercise to show that Mτ is obtained from the identity matrix by
permuting its columns according to τ. More precisely, the τ(i)-th column of Mτ is

§2 Finite Matrix Groups and Rings of Invariants
357
the i-th column of In. As an example, consider the permutation τ that takes (x, y, z)
to (y, z, x). Here, τ(1) = 2, τ(2) = 3, and τ(3) = 1, and one can check that
Mτ ·
⎛
⎝
x
y
z
⎞
⎠=
⎛
⎝
0 1 0
0 0 1
1 0 0
⎞
⎠
⎛
⎝
x
y
z
⎞
⎠=
⎛
⎝
y
z
x
⎞
⎠.
Since there are n! ways to permute the variables, we get n! permutation matrices.
Furthermore, this set is closed under matrix multiplication, for it is easy to show that
Mτ · Mν = Mντ,
where ντ is the permutation that takes i to ν(τ(i)) (see Exercise 4). Thus, the per-
mutation matrices form a ﬁnite matrix group in GL(n, k). We will denote this matrix
group by Sn. (Strictly speaking, the group of permutation matrices is only isomor-
phic to Sn in the sense of group theory. We will ignore this distinction.)
Example 5. Another important class of ﬁnite matrix groups comes from the sym-
metries of regular polyhedra. For example, consider a cube in R3 centered at the
origin. The set of rotations of R3 that take the cube to itself is clearly ﬁnite and
closed under multiplication. Thus, we get a ﬁnite matrix group in GL(3, R). In gen-
eral, all ﬁnite matrix groups in GL(3, R) have been classiﬁed, and there is a rich
geometry associated with such groups (see Exercises 5-9 for some examples). To
pursue this topic further, the reader should consult BENSON and GROVE (1985),
KLEIN (1884), or COXETER (1973).
Finite matrix groups have the following useful properties.
Proposition 6. Let G ⊆GL(n, k) be a ﬁnite matrix group. Then:
(i) In ∈G.
(ii) If A ∈G, then Am = In for some positive integer m.
(iii) If A ∈G, then A−1 ∈G.
Proof. Take A ∈G. Then {A, A2, A3, . . .} ⊆G since G is closed under multipli-
cation. The ﬁniteness of G then implies that Ai = Aj for some i > j, and since A
is invertible, we can multiply each side by A−j to conclude that Am = In, where
m = i −j > 0. This proves (ii).
To prove (iii), note that (ii) implies In = Am = A · Am−1 = Am−1 · A. Thus,
A−1 = Am−1 ∈G since G is closed under multiplication. As for (i), since G ̸= ∅,
we can pick A ∈G, and then by (ii), In = Am ∈G.
□
We next observe that elements of GL(n, k) act on polynomials in k[x1, . . . , xn].
To see how this works, let A = (aij) ∈GL(n, k) and f ∈k[x1, . . . , xn]. Then
(1)
g(x1, . . . , xn) = f(a11x1 + · · · + a1nxn, . . . , an1x1 + · · · + annxn)
is again a polynomial in k[x1, . . . , xn]. To express this more compactly, let x denote
the column vector of the variables x1, . . . , xn. Thus,

358
Chapter 7 Invariant Theory of Finite Groups
x =
⎛
⎜
⎝
x1
...
xn
⎞
⎟
⎠.
Then we can use matrix multiplication to express equation (1) as
g(x) = f(A · x).
If we think of A as a change of basis matrix, then g is simply f viewed using the new
coordinates.
For an example of how this works, let f(x, y) = x2 + xy + y2 ∈R[x, y] and
A =
1
√
2
1 −1
1
1

∈GL(2, R).
Then
g(x, y) = f(A · x) = f
x −y
√
2
, x + y
√
2

=
x −y
√
2
2
+ x −y
√
2
· x + y
√
2
+
x + y
√
2
2
= 3
2x2 + 1
2y2.
Geometrically, this shows that we can eliminate the xy term of f by rotating the
coordinate axes 45◦.
A remarkable fact is that sometimes this process gives back the same polynomial
we started with. For example, if we let h(x, y) = x2 + y2 and use the above matrix
A, then one can check that
h(x) = h(A · x).
In this case, we say that h is invariant under A.
This leads to the following fundamental deﬁnition.
Deﬁnition 7. Let G ⊆GL(n, k) be a ﬁnite matrix group. Then a polynomial f(x) ∈
k[x1, . . . , xn] is invariant under G if
f(x) = f(A · x)
for all A ∈G. The set of all invariant polynomials is denoted k[x1, . . . , xn]G.
The most basic example of invariants of a ﬁnite matrix group is given by the
symmetric polynomials.
Example 8. If we consider the group Sn ⊆GL(n, k) of permutation matrices, then
it is obvious that
k[x1, . . . , xn]Sn = {all symmetric polynomials in k[x1, . . . , xn]}.

§2 Finite Matrix Groups and Rings of Invariants
359
By Theorem 3 of §1, we know that symmetric polynomials are polynomials in the
elementary symmetric polynomials with coefﬁcients in k. We can write this as
k[x1, . . . , xn]Sn = k[σ1, . . . , σn].
Thus, every invariant can be written as a polynomial in ﬁnitely many invariants (the
elementary symmetric polynomials). In addition, we know that the representation in
terms of the elementary symmetric polynomials is unique. Hence, we have a very
explicit knowledge of the invariants of Sn.
One goal of invariant theory is to examine whether all invariants k[x1, . . . , xn]G
are as nice as Example 8. To begin our study of this question, we ﬁrst show that the
set of invariants k[x1, . . . , xn]G has the following algebraic structure.
Proposition 9. If G ⊆GL(n, k) is a ﬁnite matrix group, then the set k[x1, . . . , xn]G
is closed under addition and multiplication and contains the constant polynomials.
Proof. We leave the easy proof as an exercise.
□
Multiplication and addition in k[x1, . . . , xn]G automatically satisfy the distribu-
tive, associative, etc., properties since these properties are true in k[x1, . . . , xn]. In
the terminology of Chapter 5, we say that k[x1, . . . , xn]G is a commutative ring. Fur-
thermore, we say that k[x1, . . . , xn]G is a subring of k[x1, . . . , xn].
So far in this book, we have learned three ways to create new rings. In Chapter 5,
we saw how to make the quotient ring k[x1, . . . , xn]/I of an ideal I ⊆k[x1, . . . , xn]
and the coordinate ring k[V] of an afﬁne variety V ⊆kn. Now we can make the ring
of invariants k[x1, . . . , xn]G of a ﬁnite matrix group G ⊆GL(n, k). In §4, we will see
how these constructions are related.
In §1, we saw that the homogeneous components of a symmetric polynomial
were also symmetric. We next observe that this holds for the invariants of any ﬁnite
matrix group.
Proposition 10. Let G ⊆GL(n, k) be a ﬁnite matrix group. Then a polynomial
f ∈k[x1, . . . , xn] is invariant under G if and only if its homogeneous components
are invariant.
Proof. See Exercise 11.
□
Homogeneous invariants play a key role in invariant theory. In §3, we will often
use Proposition 10 to reduce to the case of homogeneous invariants.
The following lemma will prove useful in determining whether a given polyno-
mial is invariant under a ﬁnite matrix group.
Lemma 11. Let G ⊆GL(n, k) be a ﬁnite matrix group and suppose that we have
A1, . . . , Am ∈G such that every A ∈G can be written in the form
A = B1B2 · · · Bt,

360
Chapter 7 Invariant Theory of Finite Groups
where Bi ∈{A1, . . . , Am} for every i (we say that A1, . . . , Am generate G). Then
f ∈k[x1, . . . , xn] is in k[x1, . . . , xn]G if and only if
f(x) = f(A1 · x) = · · · = f(Am · x).
Proof. We ﬁrst show that if f is invariant under matrices B1, . . . , Bt, then it is also
invariant under their product B1 · · · Bt. This is clearly true for t = 1. If we assume it
is true for t −1, then
f((B1 · · · Bt) · x) = f((B1 · · · Bt−1) · (Bt · x))
= f(Bt · x)
(by our inductive assumption)
= f(x)
(by the invariance under Bt).
Now suppose that f is invariant under A1, . . . , Am. Since elements A ∈G can be
written A = B1 · · · Bt, where every Bi is one of A1, . . . , Am, it follows immediately
that f ∈k[x1, . . . , xn]G. The converse is trivial and the lemma is proved.
□
We can now compute some interesting examples of rings of invariants.
Example 12. Consider the ﬁnite matrix group
V4 =

±1
0
0
±1

⊆GL(2, k).
This is sometimes called the Klein four-group. In German, "four-group" is written
"vierergruppe," which explains the notation V4. You should check that the matrices
−1 0
0 1

,
1
0
0 −1

generate V4. Then Lemma 11 implies that a polynomial f ∈k[x, y] is invariant under
V4 if and only if
f(x, y) = f(−x, y) = f(x, −y)
Writing f = 
ij aijxiyj, we can understand the ﬁrst of these conditions as follows:
f(x, y) = f(−x, y) ⇐⇒

ij
aijxiyj =

ij
aij(−x)iyj
⇐⇒

ij
aijxiyj =

ij
(−1)iaijxiyj
⇐⇒aij = (−1)iaij
for all i, j
⇐⇒aij = 0
for i odd.
It follows that x always appears to an even power. Similarly, the condition f(x, y) =
f(x, −y) implies that y appears to even powers. Thus, we can write
f(x, y) = g(x2, y2)

§2 Finite Matrix Groups and Rings of Invariants
361
for a unique polynomial g(x, y) ∈k[x, y]. Conversely, every polynomial f of this
form is clearly invariant under V4. This proves that
k[x, y]V4 = k[x2, y2].
Hence, every invariant of V4 can be uniquely written as a polynomial in the two
homogeneous invariants x2 and y2. In particular, the invariants of the Klein four-
group behave very much like the symmetric polynomials.
Example 13. For a ﬁnite matrix group that is less well-behaved, consider the cyclic
group C2 = {±I2} ⊆GL(2, k) of order 2. In this case, the invariants consist of the
polynomials f ∈k[x, y] for which f(x, y) = f(−x, −y). We leave it as an exercise to
show that this is equivalent to the condition
f(x, y) =

ij
aijxiyj,
where aij = 0 whenever i + j is odd.
This means that f is invariant under C2 if and only if the exponents of x and y always
have the same parity (i.e., both even or both odd). Hence, we can write a monomial
xiyj appearing in f in the form
xiyj =

x2ky2l = (x2)k(y2)l
if i, j are even
x2k+1y2l+1 = (x2)k(y2)lxy
if i, j are odd.
This means that every monomial in f, and hence f itself, is a polynomial in the
homogeneous invariants x2, y2, and xy. We will write this as
k[x, y]C2 = k[x2, y2, xy].
Note also that we need all three invariants to generate k[x, y]C2.
The ring k[x2, y2, xy] is fundamentally different from the previous examples be-
cause uniqueness breaks down: a given invariant can be written in terms of x2, y2, xy
in more than one way. For example, x4y2 is clearly invariant under C2, but
x4y2 = (x2)2 · y2 = x2 · (xy)2.
In §4, we will see that the crux of the matter is the algebraic relation x2 · y2 = (xy)2
between the basic invariants. In general, a key part of the theory is determining
all algebraic relations between invariants. Given this information, one can describe
precisely how uniqueness fails.
From these examples, we see that given a ﬁnite matrix group G, invariant theory
has two basic questions to answer about the ring of invariants k[x1, . . . , xn]G:
• (Finite Generation) Can we ﬁnd ﬁnitely many homogeneous invariants f1, . . . , fm
such that every invariant is a polynomial in f1, . . . , fm?
• (Uniqueness) In how many ways can an invariant be written in terms of f1, . . . , fm?
In §4, we will see that this asks for the algebraic relations among f1, . . . , fm.

362
Chapter 7 Invariant Theory of Finite Groups
In §§3 and 4, we will give complete answers to both questions. We will also
describe algorithms for ﬁnding the invariants and the relations between them.
EXERCISES FOR §2
1. If A, B ∈GL(n, k) are invertible matrices, show that AB and A−1 are also invertible.
2. Suppose that A ∈GL(n, k) satisﬁes Am = In for some positive integer. If m is the
smallest such integer, then prove that the set Cm = {In, A, A2, . . . , Am−1} has exactly m
elements and is closed under matrix multiplication.
3. Write down the six permutation matrices in GL(3, k).
4. Let Mτ be the matrix of the linear transformation taking x1, . . . , xn to xτ(1), . . . , xτ(n).
This means that if e1, . . . , en is the standard basis of kn, then Mτ ·(
j xjej) = 
j xτ(j)ej.
a. Show that Mτ · eτ(i) = ei. Hint: Observe that 
j xjej = 
j xτ(j)eτ(j).
b. Prove that the τ(i)-th column of Mτ is the i-th column of the identity matrix.
c. Prove that Mτ · Mν = Mντ, where ντ is the permutation taking i to ν(τ(i)).
5. Consider a cube in R3 centered at the origin whose edges have length 2 and are parallel
to the coordinate axes.
a. Show that there are ﬁnitely many rotations of R3 about the origin which take the
cube to itself and show that these rotations are closed under composition. Taking the
matrices representing these rotations, we get a ﬁnite matrix group G ⊆GL(3, R).
b. Show that G has 24 elements. Hint: Every rotation is a rotation about a line through
the origin. So you ﬁrst need to identify the "lines of symmetry" of the cube.
c. Write down the matrix of the element of G corresponding to the 120◦counterclock-
wise rotation of the cube about the diagonal connecting the vertices (−1, −1, −1)
and (1, 1, 1).
d. Write down the matrix of the element of G corresponding to the 90◦counterclock-
wise rotation about the z-axis.
e. Argue geometrically that G is generated by the two matrices from parts (c) and (d).
6. In this exercise, we will use geometric methods to ﬁnd some invariants of the rotation
group G of the cube (from Exercise 5).
a. Explain why x2+y2+z2 ∈R[x, y, z]G. Hint: Think geometrically in terms of distance
to the origin.
b. Argue geometrically that the union of the three coordinate planes V(xyz) is invariant
under G.
c. Show that I(V(xyz)) = ⟨xyz⟩and conclude that if f = xyz, then for each A ∈G, we
have f (A · x) = axyz for some real number a.
d. Show that f = xyz satisﬁes f (A · x) = ±xyz for all A ∈G and conclude that
x2y2z2 ∈k[x, y, z]G. Hint: Use part (c) and the fact that Am = I3 for some positive
integer m.
e. Use similar methods to show that the polynomials

(x + y + z)(x + y −z)(x −y + z)(x −y −z)
2,

(x2 −y2)(x2 −z2)(y2 −z2)
2
are in k[x, y, z]G. Hint: The plane x+y+z = 0 is perpendicular to one of the diagonals
of the cube.
7. This exercise will continue our study of the invariants of the rotation group G of the cube
begun in Exercise 6.
a. Show that a polynomial f is in k[x, y, z]G if and only if f (x, y, z) = f (y, z, x) =
f (−y, x, z). Hint: Use parts (c), (d), and (e) of Exercise 5.

§2 Finite Matrix Groups and Rings of Invariants
363
b. Let
f = xyz,
g = (x + y + z)(x + y −z)(z −y + z)(x −y −z),
h = (x2 −y2)(x2 −z2)(y2 −z2).
In Exercise 6, we showed that f 2, g2, h2 ∈k[x, y, z]G. Show that f , h ̸∈k[x, y, z]G,
but g, fh ∈k[x, y, z]G. Combining this with the previous exercise, we have invariants
x2 + y2 + z2, g, f 2, fh, and h2 of degrees 2, 4, 6, 9, and 12, respectively, in k[x, y, z]G.
ln §3, we will see that h2 can be expressed in terms of the others.
8. In this exercise, we will consider an interesting "duality" that occurs among the regular
polyhedra.
a. Consider a cube and an octahedron in R3, both centered at the origin. Suppose the
edges of the cube are parallel to the coordinate axes and the vertices of the octahe-
dron are on the axes. Show that they have the same group of rotations. Hint: Put the
vertices of the octahedron at the centers of the faces of the cube.
b. Show that the dodecahedron and the icosahedron behave the same way. Hint: What
do you get if you link up the centers of the 12 faces of the dodecahedron?
c. Parts (a) and (b) show that in a certain sense, the "dual" of the cube is the octahedron
and the "dual" of the dodecahedron is the icosahedron. What is the "dual" of the
tetrahedron?
9. (Requires abstract algebra) In this problem, we will consider a tetrahedron centered at
the origin of R3.
a. Show that the rotations of R3 about the origin which take the tetrahedron to itself
give us a ﬁnite matrix group G of order 12 in GL(3, R).
b. Since every rotation of the tetrahedron induces a permutation of the four vertices,
show that we get a group homomorphism ρ : G →S4.
c. Show that ρ is one-to-one and that its image is the alternating group A4. This shows
that the rotation group of the tetrahedron is isomorphic to A4.
10. Prove Proposition 9.
11. Prove Proposition 10. Hint: If A = (aij) ∈GL(n, k) and xi1
1 · · · xinn is a monomial of total
degree m = i1 + · · · + in appearing in f , then show that
(a11x1 + · · · + a1nxn)i1 · · · (an1x1 + · · · + annxn)in
is homogeneous of total degree m.
12. In Example 13, we studied polynomials f ∈k[x, y] with the property that f (x, y) =
f (−x, −y). If f = 
ij aijxiy j, show that the above condition is equivalent to aij = 0
whenever i + j is odd.
13. In Example 13, we discovered the algebraic relation x2·y2 = (xy)2 between the invariants
x2, y2, and xy. We want to show that this is essentially the only relation. More precisely,
suppose that we have a polynomial g(u, v, w) ∈k[u, v, w] such that g(x2, y2, xy) = 0.
We want to prove that g(u, v, w) is a multiple in k[u, v, w]) of uv −w2 (which is the
polynomial corresponding to the above relation).
a. If we divide g by uv −w2 using lex order with u > v > w, show that the remainder
can be written in the form uA(u, w) + vB(v, w) + C(w).
b. Show that a polynomial r = uA(u, w) + vB(v, w) + C(w) satisﬁes r(x2, y2, xy) = 0
if and only if r = 0.
14. Consider the ﬁnite matrix group C4 ⊆GL(2, C) generated by
A =

i
0
0 −i

∈GL(2, C).

364
Chapter 7 Invariant Theory of Finite Groups
a. Prove that C4 is cyclic of order 4.
b. Use the method of Example 13 to determine C[x, y]C4.
c. Is there an algebraic relation between the invariants you found in part (b)? Can you
give an example to show how uniqueness fails?
d. Use the method of Exercise 13 to show that the relation found in part (c) is the only
relation between the invariants.
15. Consider
V4 =
$
±

1 0
0 1

, ±

0 1
1 0
%
⊆GL(2, k)
a. Show that V4 is a ﬁnite matrix group of order 4.
b. Determine k[x, y]V4.
c. Show that any invariant can be written uniquely in terms of the generating invariants
you found in part (b).
16. In Example 3, we introduced the ﬁnite matrix group C4 in GL(2, k) generated by
A =

0 −1
1
0

∈GL(2, k).
Try to apply the methods of Examples 12 and 13 to determine k[x, y]C4. Even if you
cannot ﬁnd all of the invariants, you should be able to ﬁnd some invariants of low total
degree. In §3, we will determine k[x, y]C4 completely.
§3
Generators for the Ring of Invariants
The goal of this section is to determine, in an algorithmic fashion, the ring of invari-
ants k[x1, . . . , xn]G of a ﬁnite matrix group G ⊆GL(n, k). As in §2, we assume that
our ﬁeld k has characteristic zero. We begin by introducing some terminology used
implicitly in §2.
Deﬁnition 1. Given f1, . . . , fm ∈k[x1, . . . , xn], we let k[ f1, . . . , fm] denote the subset
of k[x1, . . . , xn] consisting of all polynomial expressions in f1, . . . , fm with coefﬁ-
cients in k.
This means that the elements f ∈k[ f1, . . . , fm] are those polynomials which can
be written in the form
f = g( f1, . . . , fm),
where g is a polynomial in m variables with coefﬁcients in k.
Since k[ f1, . . . , fm] is closed under multiplication and addition and contains the
constants, it is a subring of k[x1, . . . , xn]. We say that k[ f1, . . . , fm] is generated by
f1, . . . , fm over k. One has to be slightly careful about the terminology: the subring
k[ f1, . . . , fm] and the ideal ⟨f1, . . . , fm⟩are both "generated" by f1, . . . , fm, but in
each case, we mean something slightly different. In the exercises, we will give some
examples to help explain the distinction.
An important tool we will use in our study of k[x1, . . . , xn]G is the Reynolds op-
erator, which is deﬁned as follows.

§3 Generators for the Ring of Invariants
365
Deﬁnition 2. Given a ﬁnite matrix group G ⊆GL(n, k), the Reynolds operator of
G is the map RG : k[x1, . . . , xn] →k[x1, . . . , xn] deﬁned by the formula
RG( f)(x) = 1
|G|

A∈G
f(A · x)
for f(x) ∈k[x1, . . . , xn].
One can think of RG( f) as "averaging" the effect of G on f. Note that division
by |G| is allowed since k has characteristic zero. The Reynolds operator has the
following crucial properties.
Proposition 3. Let RG be the Reynolds operator of the ﬁnite matrix group G.
(i) RG is k-linear in f.
(ii) If f ∈k[x1, . . . , xn], then RG( f) ∈k[x1, . . . , xn]G.
(iii) If f ∈k[x1, . . . , xn]G, then RG( f) = f.
Proof. We will leave the proof of (i) as an exercise. To prove (ii), let B ∈G. Then
(1)
RG( f)(Bx) = 1
|G|

A∈G
f(A · Bx) = 1
|G|

A∈G
f(AB · x).
Writing G = {A1, . . . , A|G|}, note that AiB ̸= AjB when i ̸= j (otherwise, we
could multiply each side by B−1 to conclude that Ai = Aj). Thus the subset
{A1B, . . . , A|G|B} ⊆G consists of |G| distinct elements of G and hence must equal
G. This shows that
G = {AB | A ∈G}.
Consequently, in the last sum of (1), the polynomials f(AB · x) are just the f(A · x),
possibly in a different order. Hence,
1
|G|

A∈G
f(AB · x) = 1
|G|

A∈G
f(A · x) = RG( f)(x),
and it follows that RG( f)(B · x) = RG( f)(x) for all B ∈G. This implies RG( f) ∈
k[x1, . . . , xn]G.
Finally, to prove (iii), note that if f ∈k[x1, . . . , xn]G, then
RG( f)(x) = 1
|G|

A∈G
f(A · x) = 1
|G|

A∈G
f(x) = f(x)
since f is invariant. This completes the proof.
□
One nice aspect of this proposition is that it gives us a way of creating invariants.
Let us look at an example.
Example 4. Consider the cyclic matrix group C4 ⊆GL(2, k) of order 4 generated
by

366
Chapter 7 Invariant Theory of Finite Groups
A =

0 −1
1
0

.
By Lemma 11 of §2, we know that
k[x, y]C4 = {f ∈k[x, y] | f(x, y) = f(−y, x)}.
One can easily check that the Reynolds operator is given by
RC4( f)(x, y) = 1
4( f(x, y) + f(−y, x) + f(−x, −y) + f(y, −x))
(see Exercise 3). Using Proposition 3, we can compute some invariants as follows:
RC4(x2) = 1
4(x2 + (−y)2 + (−x)2 + y2) = 1
2(x2 + y2),
RC4(xy) = 1
4(xy + (−y)x + (−x)(−y) + y(−x)) = 0,
RC4(x3y) = 1
4(x3y + (−y)3x + (−x)3(−y) + y3(−x)) = 1
2(x3y −xy3),
RC4(x2y2) = 1
4(x2y2 + (−y)2x2 + (−x)2(−y)2 + y2(−x)2) = x2y2.
Thus, x2 +y2, x3y−xy3, x2y2 ∈k[x, y]C4. We will soon see that these three invariants
generate k[x, y]C4.
It is easy to prove that for any monomial xα, the Reynolds operator gives us a
homogeneous invariant RG(xα) of total degree |α| whenever it is nonzero. The fol-
lowing wonderful theorem of Emmy Noether shows that we can always ﬁnd ﬁnitely
many of these invariants that generate k[x1, . . . , xn]G.
Theorem 5. Given a ﬁnite matrix group G ⊆GL(n, k), let xβ1, . . . , xβm be all mono-
mials of total degree ≤|G|. Then
k[x1, . . . , xn]G = k[RG(xβ1), . . . , RG(xβm)] = k[RG(xβ) | |β| ≤|G|].
In particular, k[x1, . . . , xn]G is generated by ﬁnitely many homogeneous invariants.
Proof. If f = 
α cαxα ∈k[x1, . . . , xn]G, then Proposition 3 implies that
f = RG( f) = RG
 
α
cαxα

=

α
cαRG(xα).
Hence every invariant is a linear combination (over k) of the RG(xα). Consequently,
it sufﬁces to prove that for all α, RG(xα) is a polynomial in the RG(xβ), |β| ≤|G|.
Noether's clever idea was to ﬁx an integer m and combine all RG(xβ) of total de-
gree m into a power sum of the type considered in §1. Using the theory of symmetric
polynomials, this can be expressed in terms of ﬁnitely many power sums, and the
theorem will follow.

§3 Generators for the Ring of Invariants
367
The ﬁrst step in implementing this strategy is to expand (x1 + · · · + xn)m into a
sum of monomials xα with |α| = m:
(2)
(x1 + · · · + xn)m =

|α|=m
aαxα.
In Exercise 4, you will prove that aα is a positive integer for all |α| = m.
To exploit this identity, we need some notation. Given A = (aij) ∈G, let Ai de-
note the i-th row of A. Thus, Ai·x = ai1x1+· · ·+ainxn. Then, if α = (α1, . . . , αn) ∈
Zn
≥0, let
(A · x)α = (A1 · x)α1 · · · (An · x)αn.
In this notation, we have
RG(xα) = 1
|G|

A∈G
(A · x)α.
Now introduce new variables u1, . . . , un and substitute uiAi · x for xi in (2). This
gives the identity
(u1A1 · x + · · · + unAn · x)m =

|α|=m
aα(A · x)αuα.
If we sum over all A ∈G, then we obtain
(3)
Sm =

A∈G
(u1A1 · x + · · · + unAn · x)m =

|α|=m
aα
 
A∈G
(A · x)α

uα
=

|α|=m
bαRG(xα)uα,
where bα = |G|aα. Note how the sum on the right encodes all RG(xα) with |α| = m.
This is why we use the variables u1, . . . , un: they prevent any cancellation from
occurring.
The left side of (3) is the m-th power sum Sm of the |G| quantities
UA = u1A1 · x + · · · + unAn · x
indexed by A ∈G. We write this as Sm = Sm(UA | A ∈G). By Theorem 8 of §1,
every symmetric polynomial in the |G| quantities UA is a polynomial in S1, . . . , S|G|.
Since Sm is symmetric in the UA, it follows that
Sm = F(S1, . . . , S|G|)
for some polynomial F with coefﬁcients in k. Substituting in (3), we obtain

|α|=m
bαRG(xα)uα = F
 
|β|=1
bβRG(xβ)uβ, . . . ,

|β|=|G|
RG(xβ)uβ

.

368
Chapter 7 Invariant Theory of Finite Groups
Expanding the right side and equating the coefﬁcients of uα, it follows that
bαRG(xα) = a polynomial in the RG(xβ), |β| ≤|G|.
Since k has characteristic zero, the coefﬁcient bα = |G|aα is nonzero in k, and hence
RG(xα) has the desired form. This completes the proof of the theorem.
□
This theorem solves the ﬁnite generation problem stated at the end of §2. In
the exercises, you will give a second proof of Theorem 5 using the Hilbert Basis
Theorem.
To see the power of what we have just proved, let us compute some invariants.
Example 6. We will return to the cyclic group C4 ⊆GL(2, k) of order 4 from
Example 4. To ﬁnd the ring of invariants, we need to compute RC4(xiyj) for all
i + j ≤4. The following table records the results:
xiyj RC4(xiyj) xiyj
RC4(xiyj)
x
0
xy2
0
y
0
y3
0
x2
1
2(x2 + y2) x4
1
2(x4 + y4)
xy
0
x3y
1
2(x3y −xy3)
y2
1
2(x2 + y2) x2y2
x2y2
x3
0
xy3 −1
2(x3y −xy3)
x2y
0
y4
1
2(x4 + y4)
By Theorem 5, it follows that k[x, y]C4 is generated by the four invariants x2+y2, x4+
y4, x3y −xy3 and x2y2. However, we do not need x4 + y4 since
x4 + y4 = (x2 + y2)2 −2x2y2.
Thus, we have proved that
k[x, y]C4 = k[x2 + y2, x3y −xy3, x2y2].
The main drawback of Theorem 5 is that when |G| is large, we need to compute
the Reynolds operator for lots of monomials. For example, consider the cyclic group
C8 ⊆GL(2, R) of order 8 generated by the 45◦rotation
A =
1
√
2

1 −1
1
1

∈GL(2, R).
In this case, Theorem 5 says that k[x, y]C8 is generated by the 44 invariants RC8(xiyj),
i + j ≤8. In reality, only 3 are needed. For larger groups, things are even worse,
especially if more variables are involved. See Exercise 10 for an example.
Fortunately, there are more efﬁcient methods for ﬁnding a generating set of in-
variants. The main tool is Molien's Theorem, which enables one to predict in ad-
vance the number of linearly independent homogeneous invariants of given total

§3 Generators for the Ring of Invariants
369
degree. This theorem can be found in Chapter 2 of STURMFELS (2008) and Chap-
ter 3 of DERKSEN and KEMPER (2002). Both books discuss efﬁcient algorithms for
ﬁnding invariants that generate k[x1, . . . , xn]G.
Once we know k[x1, . . . , xn]G = k[ f1, . . . , fm], we can ask if there is an algorithm
for writing a given invariant f ∈k[x1, . . . , xn]G in terms of f1, . . . , fm. For example,
it is easy to check that the polynomial
(4)
f(x, y) = x8 + 2x6y2 −x5y3 + 2x4y4 + x3y5 + 2x2y6 + y8
satisﬁes f(x, y) = f(−y, x), and hence is invariant under the group C4 from Exam-
ple 4. Then Example 6 implies that f ∈k[x, y]C4 = k[x2 + y2, x3y −xy3, x2y2]. But
how do we write f in terms of these three invariants? To answer this question, we
will use a method similar to what we did in Proposition 4 of §1.
We will actually prove a bit more, for we will allow f1, . . . , fm to be arbitrary
elements of k[x1, . . . , xn]. The following proposition shows how to test whether a
polynomial lies in k[ f1, . . . , fm] and, if so, to write it in terms of f1, . . . , fm.
Proposition 7. Suppose that f1, . . . , fm ∈k[x1, . . . , xn] are given. Fix a monomial
order in k[x1, . . . , xn, y1, . . . , ym] where any monomial involving one of x1, . . . , xn is
greater than all monomials in k[y1, . . . , ym]. Let G be a Gröbner basis of the ideal
⟨f1 −y1, . . . , fm −ym⟩⊆k[x1, . . . xn, y1, . . . , ym]. Given f ∈k[x1, . . . , xn], let g = f G
be the remainder of f on division by G. Then:
(i) f ∈k[ f1, . . . , fm] if and only if g ∈k[y1, . . . , ym].
(ii) If f ∈k[ f1, . . . , fm], then f = g( f1, . . . , fm) is an expression of f as a polynomial
in f1, . . . , fm.
Proof. The proof will be similar to the argument given in Proposition 4 of §1 (with
one interesting difference). When we divide f ∈k[x1, . . . , xn] by G = {g1, . . . , gt},
we get an expression of the form
f = A1g1 + · · · + Atgt + g,
with A1, . . . , At, g ∈k[x1, . . . , xn, y1, . . . , ym].
To prove (i), ﬁrst suppose that g ∈k[y1, . . . , ym]. Then for each i, substitute fi
for yi in the above formula for f. This substitution will not affect f since it involves
only x1, . . . , xn, but it sends every polynomial in ⟨f1−y1, . . . , fm−ym⟩to zero. Since
g1, . . . , gt lie in this ideal, it follows that f = g( f1, . . . , fm). Hence, f ∈k[ f1, . . . , fm].
Conversely, suppose that f = g( f1, . . . , fm) for some g ∈k[y1, . . . , ym]. Arguing
as in §1, one sees that
(5)
f = C1 · ( f1 −y1) + · · · + Cm · ( fm −ym) + g(y1, . . . , ym)
[see equation (4) of §1]. Unlike the case of symmetric polynomials, g need not be
the remainder of f on division by G—we still need to reduce some more.
Let G′ = G ∩k[y1, . . . , ym] consist of those elements of G involving only
y1, . . . , ym. Renumbering if necessary, we can assume G′ = {g1, . . . , gs}, where
s ≤t. If we divide g by G′, we get an expression of the form

370
Chapter 7 Invariant Theory of Finite Groups
(6)
g = B1g1 + · · · + Bsgs + g′,
where B1, . . . , Bs, g′ ∈k[y1, . . . , ym]. If we combine equations (5) and (6), we can
write f in the form
f = C′
1 · ( f1 −y1) + · · · + C′
m · ( fm −ym) + g′(y1, . . . ym).
This follows because, in (6), each gi lies in ⟨f1 −y1, . . . , fm −ym⟩. We claim that
g′ is the remainder of f on division by G. This will prove that the remainder lies in
k[y1, . . . , ym].
Since G a Gröbner basis, Proposition 1 of Chapter 2, §6 tells us that g′ is the
remainder of f on division by G provided that no term of g′ is divisible by an element
of LT(G). To prove that g′ has this property, suppose that there is gi ∈G where
LT(gi) divides some term of g′. Then LT(gi) involves only y1, . . . , ym since g′ ∈
k[y1, . . . , ym]. By our hypothesis on the ordering, it follows that gi ∈k[y1, . . . , ym]
and hence, gi ∈G′. Since g′ is a remainder on division by G′, LT(gi) cannot divide
any term of g′. This contradiction shows that g′ is the desired remainder.
Part (ii) of the proposition follows immediately from the above arguments, and
we are done.
□
In the exercises, you will use this proposition to write the polynomial
f(x, y) = x8 + 2x6y2 −x5y3 + 2x4y4 + x3y5 + 2x2y6 + y8
from (4) in terms of the generating invariants x2 + y2, x3y −xy3, x2y2 of k[x, y]C4.
The problem of ﬁnding generators for the ring of invariants (and the associ-
ated problem of ﬁnding the relations between them—see §4) played an important
role in the development of invariant theory. Originally, the group involved was the
group of all invertible matrices over a ﬁeld. A classic introduction can be found in
HILBERT (1993), and STURMFELS (2008) also discusses this case. For more on the
invariant theory of ﬁnite groups, we recommend DERKSEN and KEMPER (2002),
SMITH (1995) and STURMFELS (2008).
EXERCISES FOR §3
1. Given f1, . . . , fm ∈k[x1, . . . , xn], we can "generate" the following two objects:
•
The ideal ⟨f1, . . . , fm⟩⊆k[x1, . . . , xn] generated by f1, . . . , fm. This consists of all
expressions m
i=1 hi fi, where h1, . . . , hm ∈k[x1, . . . , xn].
•
The subring k[ f1, . . . , fm] ⊆k[x1, . . . , xn] generated by f1, . . . , fm over k. This con-
sists of all expressions g( f1, . . . , fm) where g is a polynomial in m variables with
coefﬁcients in k.
To illustrate the differences between these, we will consider the simple case where f1 =
x2 ∈k[x].
a. Explain why 1 ∈k[x2] but 1 ̸∈⟨x2⟩.
b. Explain why x3 ̸∈k[x2] but x3 ∈⟨x2⟩.
2. Let G be a ﬁnite matrix group in GL(n, k). Prove that the Reynolds operator RG has the
following properties:
a. If a, b ∈k and f , g ∈k[x1, . . . , xn], then RG(af + bg) = aRG( f ) + bRG(g).

§3 Generators for the Ring of Invariants
371
b. RG maps k[x1, . . . , xn] to k[x1, . . . , xn]G and is onto.
c. RG ◦RG = RG.
d. If f ∈k[x1, . . . , xn]G and g ∈k[x1, . . . , xn], then RG( fg) = f · RG(g).
3. In this exercise, we will work with the cyclic group C4 ⊆GL(2, k) from Example 4 in
the text.
a. Prove that the Reynolds operator of C4 is given by
RC4( f )(x, y) = 1
4( f (x, y) + f (−y, x) + f (−x, −y) + f (y, −x)).
b. Compute RC4(xiyj) for all i + j ≤4. Note that some of the computations are done in
Example 4. You can check your answers against the table in Example 6.
4. In this exercise, we will study the identity (2) used in the proof of Theorem 5. We will use
the multinomial coefﬁcients, which are deﬁned as follows. For α = (α1, . . . , αn) ∈Zn
≥0,
let |α| = m and deﬁne
&
m
α
'
=
m!
α1!α2! · · · αn!.
a. Prove that
m
α

is an integer. Hint: Use induction on n and note that when n = 2,
m
α

is a binomial coefﬁcient.
b. Prove that
(x1 + · · · + xn)m =

|α|=m
&
m
α
'
xα.
In particular, the coefﬁcient aα in equation (2) is the positive integer
m
α

. Hint: Use
induction on n and note that the case n = 2 is the binomial theorem.
5. Let G ⊆GL(n, k) be a ﬁnite matrix group. In this exercise, we will give Hilbert's proof
that k[x1, . . . , xn]G is generated by ﬁnitely many homogeneous invariants. To begin the
argument, let I ⊆k[x1, . . . , xn] be the ideal generated by all homogeneous invariants of
positive total degree.
a. Explain why there are ﬁnitely many homogeneous invariants f1, . . . , fm such that
I = ⟨f1, . . . , fm⟩. The strategy of Hilbert's proof is to show that k[x1, . . . , xn]G =
k[ f1, . . . , fm]. Since the inclusion k[ f1, . . . , fm] ⊆k[x1, . . . , xn]G is obvious, we must
show that k[x1, . . . , xn]G ̸⊆k[ f1, . . . , fm] leads to a contradiction.
b. Prove that k[x1, . . . , xn]G ̸⊆k[ f1, . . . , fm] implies there is a homogeneous invariant f
of positive degree which is not in k[ f1, . . . , fm].
c. For the rest of the proof, pick f as in part (b) with minimal total degree d. By deﬁni-
tion, f ∈I, so that f = m
i=1 hi fi for h1, . . . , hm ∈k[x1, . . . , xn]. Prove that for each
i, we can assume that hi fi is either 0 or homogeneous of total degree d.
d. Use the Reynolds operator to show that f = m
i=1 RG(hi)fi. Hint: Use Proposition 3
and Exercise 2. Also show that for each i, RG(hi)fi is either 0 or homogeneous of
total degree d.
e. Since fi has positive total degree, conclude that RG(hi) is a homogeneous invariant of
total degree < d. By the minimality of d, RG(hi) ∈k[ f1, . . . , fm] for all i. Prove that
this contradicts f ̸∈k[ f1, . . . , fm].
This proof is a lovely application of the Hilbert Basis Theorem. The one drawback is
that it does not tell us how to ﬁnd the generators—the proof is purely nonconstructive.
Thus, for our purposes, Noether's theorem is much more useful.
6. If we have two ﬁnite matrix groups G and H such that G ⊆H ⊆GL(n, k), prove that
k[x1, . . . , xn]H ⊆k[x1, . . . , xn]G.
7. Consider the matrix
A =

0 −1
1 −1

∈GL(2, k).

372
Chapter 7 Invariant Theory of Finite Groups
a. Show that A generates a cyclic matrix group C3 of order 3.
b. Use Theorem 5 to ﬁnd ﬁnitely many homogeneous invariants which generate k[x, y]C3.
c. Can you ﬁnd fewer invariants that generate k[x, y]C3? Hint: If you have invariants
f1, . . . , fm, you can use Proposition 7 to determine whether f1 ∈k[ f2, . . . , fm].
8. Let A be the matrix of Exercise 7.
a. Show that −A generates a cyclic matrix group C6, of order 6.
b. Show that −I2 ∈C6. Then use Exercise 6 and §2 to show that k[x, y]C6 ⊆k[x2, y2, xy].
Conclude that all nonzero homogeneous invariants of C6 have even total degree.
c. Use part (b) and Theorem 5 to ﬁnd k[x, y]C6. Hint: There are still a lot of Reynolds
operators to compute. You should use a computer algebra program to design a pro-
cedure that has i, j as input and RC6(xiyj) as output.
9. Let A be the matrix
A =
1
√
2

1 −1
1
1

∈GL(2, k).
a. Show that A generates a cyclic matrix group C8 ⊆GL(2, k) of order 8.
b. Give a geometric argument to explain why x2 + y2 ∈k[x, y]C8. Hint: A is a rotation
matrix.
c. As in Exercise 8, explain why all homogeneous invariants of C8 have even total
degree.
d. Find k[x, y]C8. Hint: Do not do this problem unless you know how to design a pro-
cedure (on some computer algebra program) that has i, j as input and RC8(xiyj) as
output.
10. Consider the ﬁnite matrix group
G =
⎧
⎨
⎩
⎛
⎝
±1
0
0
0 ±1
0
0
0 ±1
⎞
⎠
⎫
⎬
⎭⊆GL(3, k).
Note that G has order 8.
a. If we were to use Theorem 5 to determine k[x, y, z]G, for how many monomials would
we have to compute the Reynolds operator?
b. Use the method of Example 12 in §2 to determine k[x, y, z]G.
11. Let f be the polynomial (4) in the text.
a. Verify that f ∈k[x, y]C4 = k[x2 + y2, x3y −xy3, x2y2].
b. Use Proposition 7 to express f as a polynomial in x2 + y2, x2y −xy3, x2y2.
12. In Exercises 5, 6, and 7 of §2, we studied the rotation group G ⊆GL(3, R) of the cube
in R3 and we found that k[x, y, z]G contained the polynomials
f1 = x2 + y2 + z2,
f2 = (x + y + z)(x + y −z)(x −y + z)(x −y −z),
f3 = x2y2z2,
f4 = xyz(x2 −y2)(x2 −z2)(y2 −z2).
a. Give an elementary argument using degrees to show that f4 ̸∈k[ f1, f2, f3].
b. Use Proposition 7 to show that f3 ̸∈k[ f1, f2].
c. In Exercise 6 of §2, we showed that

(x2 −y2)(x2 −z2)(y2 −z2)
2 ∈k[x, y, z]G.
Prove that this polynomial lies in k[ f1, f2, f3]. Why can we ignore f4?
Using Molien's Theorem and the methods of STURMFELS (2008), one can prove that
k[x, y, z]G = k[ f1, f2, f3, f4].

§4 Relations Among Generators and the Geometry of Orbits
373
§4
Relations Among Generators and the Geometry of Orbits
Given a ﬁnite matrix group G ⊆GL(n, k), Theorem 5 of §3 guarantees that there
are ﬁnitely many homogeneous invariants f1, . . . , fm such that
k[x1, . . . , xn]G = k[ f1, . . . , fm].
In this section, we will describe the algebraic relations among the polynomials
f1, . . . , fm. We will also see that these relations have some fascinating algebraic and
geometric implications. We continue to assume that k has characteristic zero.
We begin by recalling the uniqueness problem stated at the end of §2. For a
symmetric polynomial f ∈k[x1, . . . , xn]Sn = k[σ1, . . . , σn], we proved that f could
be written uniquely as a polynomial in σ1, . . . σn. For a general ﬁnite matrix group
G ⊆GL(n, k), if we know that k[x1, . . . , xn]G = k[ f1, . . . , fm], then one could simi-
larly ask if f ∈k[x1, . . . , xn]G can be uniquely written in terms of f1, . . . , fm.
To study this question, note that if g1 and g2 are polynomials in k[y1, . . . , ym],
then
g1( f1, . . . , fm) = g2( f1, . . . , fm) ⇐⇒h( f1, . . . , fm) = 0,
where h = g1 −g2. It follows that uniqueness fails if and only if there is a nonzero
polynomial h ∈k[y1, . . . , ym] such that h( f1, . . . , fm) = 0. Such a polynomial is a
nontrivial algebraic relation among f1, . . . , fm.
If we let F = ( f1, . . . , fm), then the set
(1)
IF = {h ∈k[y1, . . . , ym] | h( f1, . . . , fm) = 0 in k[x1, . . . , xn]}
records all algebraic relations among f1, . . . , fm. This set has the following proper-
ties.
Proposition 1. If k[x1, . . . , xn]G = k[ f1, . . . , fm], let IF ⊆k[y1, . . . , ym] be as in (1).
Then:
(i) IF is a prime ideal of k[y1, . . . , ym].
(ii) Suppose that f ∈k[x1, . . . , xn]G and that f = g( f1, . . . , fm) is one representation
of f in terms of f1, . . . , fm. Then all such representations are given by
f = g( f1, . . . , fm) + h( f1, . . . , fm),
as h varies over IF.
Proof. For (i), it is an easy exercise to prove that IF is an ideal. To show that it is
prime, we need to show that fg ∈IF implies that f ∈IF or g ∈IF (see Deﬁnition 2
of Chapter 4, §5). But fg ∈IF means that f( f1, . . . , fm)g( f1, . . . , fm) = 0. This is
a product of polynomials in k[x1, . . . , xn], and hence, f( f1, . . . , fm) or g( f1, . . . , fm)
must be zero. Thus f or g is in IF.
We leave the proof of (ii) as an exercise.
□
We will call IF the ideal of relations for F = ( f1, . . . , fm). Another name for IF
used in the literature is the syzygy ideal. To see what Proposition 1 tells us about
the uniqueness problem, consider C2 = {±I2} ⊆GL(2, k). We know from §2 that

374
Chapter 7 Invariant Theory of Finite Groups
k[x, y]C2 = k[x2, y2, xy], and, in Example 4, we will see that IF = ⟨uv −w2⟩⊆
k[u, v, w]. Now consider x6 + x3y3 ∈k[x, y]C2. Then Proposition 1 implies that all
possible ways of writing x6 + x3y3 in terms of x2, y2, xy are given by
(x2)3 + (xy)3 + (x2 · y2 −(xy)2) · b(x2, y2, xy)
since elements of ⟨uv −w2⟩are of the form (uv −w2) · b(u, v, w).
As an example of what the ideal of relations IF can tell us, let us show how it can
be used to reconstruct the ring of invariants.
Proposition 2. If k[x1, . . . , xn]G = k[ f1, . . . , fm], let IF ⊆k[y1, . . . , ym] be the ideal
of relations. Then there is a ring isomorphism
k[y1, . . . , ym]/IF ∼= k[x1, . . . , xn]G
between the quotient ring of k[y1, . . . , ym] modulo IF (as deﬁned in Chapter 5, §2)
and the ring of invariants.
Proof. Recall from §2 of Chapter 5 that elements of the quotient k[y1, . . . , ym]/IF
are written [g] for g ∈k[y1, . . . , ym], where [g1] = [g2] if and only if g1 −g2 ∈IF.
Now deﬁne φ : k[y1, . . . , ym]/IF →k[x1, . . . , xn]G by
φ([g]) = g( f1, . . . , fm).
We leave it as an exercise to check that φ is well-deﬁned and is a ring homomor-
phism. We need to show that φ is one-to-one and onto.
Since k[x1, . . . , xn]G = k[ f1, . . . , fm], it follows immediately that φ is onto. To
prove that φ is one-to-one, suppose that φ([g1]) = φ([g2]). Then g1( f1, . . . , fm) =
g2( f1, . . . , fm), which implies that g1 −g2 ∈IF. Thus, [g1] = [g2], and hence, φ is
one-to-one.
It is a general fact that if a ring homomorphism is one-to-one and onto, then its
inverse function is a ring homomorphism. Hence, φ is a ring isomorphism.
□
A more succinct proof of this proposition can be given using the Isomorphism
Theorem of Exercise 16 in Chapter 5, §2.
For our purposes, another extremely important property of IF is that we can com-
pute it explicitly using elimination theory. Namely, consider the system of equations
y1 = f1(x1, . . . , xn),
...
ym = fm(x1, . . . , xn).
Then IF can be obtained by eliminating x1, . . . , xn from these equations.
Proposition 3. If k[x1, . . . , xn]G = k[ f1, . . . , fm], consider the ideal
JF = ⟨f1 −y1, . . . , fm −ym⟩⊆k[x1, . . . , xn, y1, . . . , ym].

§4 Relations Among Generators and the Geometry of Orbits
375
(i) IF is the n-th elimination ideal of JF. Thus, IF = JF ∩k[y1, . . . , ym].
(ii) Fix a monomial order in k[x1, . . . , xn, y1, . . . , ym] where any monomial involving
one of x1, . . . , xn is greater than all monomials in k[y1, . . . , ym] and let G be a
Gröbner basis of JF. Then G ∩k[y1, . . . , ym] is a Gröbner basis for IF in the
monomial order induced on k[y1, . . . , ym].
Proof. Note that the ideal JF appeared earlier in Proposition 7 of §3. To relate JF
to the ideal of relations IF, we will need the following characterization of JF: if
p ∈k[x1, . . . , xn, y1, . . . , ym], then we claim that
(2)
p ∈JF ⇐⇒p(x1, . . . , xn, f1, . . . , fm) = 0 in k[x1, . . . , xn].
One implication is obvious since the substitution yi 
→fi takes all elements of JF =
⟨f1−y1, . . . , fm−ym⟩to zero. On the other hand, given p ∈k[x1, . . . , xn, y1, . . . , ym],
if we replace each yi in p by fi −( fi −yi) and expand, we obtain
p(x1, . . . , xn, y1, . . . , ym) = p(x1, . . . , xn, f1, . . . , fm)
+B1 · ( f1 −y1) + · · · + Bm · ( fm −ym)
for some B1, . . . , Bm ∈k[x1, . . . , xn, y1, . . . , ym] (see Exercise 4 for the details). In
particular, if p(x1, . . . , xn, f1, . . . , fm) = 0, then
p(x1, . . . , xn, y1, . . . , ym) = B1 · ( f1 −y1) + · · · + Bm · ( fm −ym) ∈JF.
This completes the proof of (2).
Now intersect each side of (2) with k[y1, . . . , ym]. For p ∈k[y1, . . . , ym], this
proves
p ∈JF ∩k[y1, . . . , ym] ⇐⇒p( f1, . . . , fm) = 0 in k[x1, . . . , xn],
so that JF ∩k[y1, . . . , ym] = IF by the deﬁnition of IF. Thus, (i) is proved, and
(ii) is then an immediate consequence of the elimination theory of Chapter 3 (see
Theorem 2 and Exercise 5 of Chapter 3, §1).
□
We can use this proposition to compute the relations between generators.
Example 4. In §2 we saw that the invariants of C2 = {±I2} ⊆GL(2, k) are given
by k[x, y]C2 = k[x2, y2, xy]. Let F = (x2, y2, xy) and let the new variables be u, v, w.
Then the ideal of relations is obtained by eliminating x, y from the equations
u = x2,
v = y2,
w = xy.
If we use lex order with x > y > u > v > w, then a Gröbner basis for the ideal
JF = ⟨u −x2, v −y2, w −xy⟩consists of the polynomials
x2 −u, xy −w, xv −yw, xw −yu, y2 −v, uv −w2.

376
Chapter 7 Invariant Theory of Finite Groups
It follows from Proposition 3 that
IF = ⟨uv −w2⟩.
This says that all relations between x2, y2, and xy are generated by the obvious
relation x2 · y2 = (xy)2. Then Proposition 2 shows that the ring of invariants can
be written as
k[x, y]C2 ∼= k[u, v, w]/⟨uv −w2⟩.
Example 5. In §3, we studied the cyclic matrix group C4 ⊆GL(2, k) generated by
A =

0 −1
1
0

and we saw that
k[x, y]C4 = k[x2 + y2, x3y −xy3, x2y2].
Putting F = (x2 + y2, x3y −xy3, x2y2), we leave it as an exercise to show that
IF ⊆k[u, v, w] is given by IF = ⟨u2w −v2 −4w2⟩. So the one nontrivial relation
between the invariants is
(x2 + y2)2 · x2y2 = (x3y −xy3)2 + 4(x2y2)2.
By Proposition 2, we conclude that the ring of invariants can be written as
k[x, y]C4 ∼= k[u, v, w]/⟨u2w −v2 −4w2⟩.
By combining Propositions 1, 2, and 3 with the theory developed in §3 of Chap-
ter 5, we can solve the uniqueness problem stated at the end of §2. Suppose that
k[x1, . . . , xn]G = k[ f1, . . . , fm] and let IF ⊆k[y1, . . . , ym] be the ideal of relations. If
IF ̸= {0}, we know that a given element f ∈k[x1, . . . , xn]G can be written in more
than one way in terms of f1, . . . , fm. Is there a consistent choice for how to write f?
To solve this problem, pick a monomial order on k[y1, . . . , ym] and use Propo-
sition 3 to ﬁnd a Gröbner basis G of IF. Given g ∈k[y1, . . . , ym], let gG be the
remainder of g on division by G. In Chapter 5, we showed that the remainders gG
uniquely represent elements of the quotient ring k[y1, . . . , ym]/IF (see Proposition 1
of Chapter 5, §3). Using this together with the isomorphism
k[y1, . . . , ym]/IF ∼= k[x1, . . . , xn]G
of Proposition 2, we get a consistent method for writing elements of k[x1, . . . , xn]G
in terms of f1, . . . , fm. Thus, Gröbner basis methods help restore the uniqueness lost
when IF ̸= {0}.
So far in this section, we have explored the algebra associated with the ideal
of relations IF. It is now time to turn to the geometry. The basic geometric object
associated with an ideal is its variety. Hence, we get the following deﬁnition.

§4 Relations Among Generators and the Geometry of Orbits
377
Deﬁnition 6. If k[x1, . . . , xn]G = k[ f1, . . . , fm], let IF ⊆k[y1, . . . , ym] be the ideal of
relations for F = ( f1, . . . , fm). Then we have the afﬁne variety
VF = V(IF) ⊆km.
The variety VF has the following properties.
Proposition 7. Let IF and VF be as in Deﬁnition 6. Then:
(i) VF is the smallest variety in km containing the parametrization
y1 = f1(x1, . . . , xn),
...
ym = fm(x1, . . . , xn).
(ii) VF is an irreducible variety.
(iii) IF = I(VF), so that IF is the ideal of all polynomial functions vanishing on VF.
(iv) Let k[VF] be the coordinate ring of VF as deﬁned in §4 of Chapter 5. Then there
is a ring isomorphism
k[VF] ∼= k[x1, . . . , xn]G.
Proof. Let JF = ⟨f1 −y1, . . . , fm −ym⟩. By Proposition 3, IF is the n-th elimination
ideal of JF. Then part (i) follows immediately from the Polynomial Implicitization
Theorem of Chapter 3 (see Theorem 1 of Chapter 3, §3).
For (ii), VF is irreducible by Proposition 5 from Chapter 4, §5 (k is inﬁnite since
it has characteristic zero). Also, in the proof of that proposition, we showed that
I(VF) = {g ∈k[y1, . . . , ym] | g ◦F = 0}.
By (1), this equals IF, and (iii) follows.
Finally, in Chapter 5, we saw that the coordinate ring k[VF] could be written as
k[VF] ∼= k[y1, . . . , ym]/I(VF)
(see Theorem 7 of Chapter 5, §2). Since I(VF) = IF by part (iii), we can use the
isomorphism of Proposition 2 to obtain
(3)
k[VF] ∼= k[y1, . . . , ym]/IF ∼= k[x1, . . . , xn]G.
This completes the proof of the proposition.
□
Note how the isomorphisms in (3) link together the three methods (coordinate
rings, quotient rings, and rings of invariants) that we have learned for creating new
rings.
When we write k[x1, . . . , xn]G = k[ f1, . . . , fm], note that f1, . . . , fm are not uniquely
determined. So one might ask how changing to a different set of generators affects
the variety VF. The answer is as follows.

378
Chapter 7 Invariant Theory of Finite Groups
Corollary 8. Suppose that k[x1, . . . , xn]G = k[ f1, . . . , fm] = k[ f ′
1, . . . , f ′
m′]. If we set
F = ( f1, . . . , fm) and F′ = ( f ′
1, . . . , f ′
m′), then the varieties VF ⊆km and VF′ ⊆km′
are isomorphic (as deﬁned in Chapter 5, §4).
Proof. Applying Proposition 7 twice, we have isomorphisms k[VF] ∼= k[x1, . . . , xn]G
∼= k[VF′], and it is easy to see that these isomorphisms are the identity on constants.
But in Theorem 9 of Chapter 5, §4, we learned that two varieties are isomorphic if
and only if there is an isomorphism of their coordinate rings which is the identity
on constants. The corollary follows immediately.
□
One of the lessons we learned in Chapter 4 was that the algebra-geometry cor-
respondence works best over an algebraically closed ﬁeld k. So for the rest of this
section we will assume that k is algebraically closed.
To uncover the geometry of VF, we need to think about the matrix group G ⊆
GL(n, k) more geometrically. So far, we have used G to act on polynomials: if f(x) ∈
k[x1, . . . , xn], then a matrix A ∈G gives us the new polynomial g(x) = f(A · x).
But we can also let G act on the underlying afﬁne space kn. We will write a point
(a1, . . . , an) ∈kn as a column vector a. Thus,
a =
⎛
⎜
⎝
a1
...
an
⎞
⎟
⎠.
Then a matrix A ∈G gives us the new point A · a by matrix multiplication.
We can then use G to describe an equivalence relation on kn: given a, b ∈kn, we
say that a ∼G b if b = A · a for some A ∈G. We leave it as an exercise to verify
that ∼G is indeed an equivalence relation. It is also straightforward to check that the
equivalence class of a ∈kn is given by
{b ∈kn | b ∼G a} = {A · a | A ∈G}.
These equivalence classes have a special name.
Deﬁnition 9. Given a ﬁnite matrix group G ⊆GL(n, k) and a ∈kn, the G-orbit of
a is the set
G · a = {A · a | A ∈G}.
The set of all G-orbits in kn is denoted kn/G and is called the orbit space.
Note that an orbit G · a has at most |G| elements. In the exercises, you will show
that the number of elements in an orbit is always a divisor of |G|.
Since orbits are equivalence classes, it follows that the orbit space kn/G is the set
of equivalence classes of ∼G. Thus, we have constructed kn/G as a set. But for us,
the objects of greatest interest are afﬁne varieties. So it is natural to ask if kn/G has
the structure of a variety in some afﬁne space. The answer is as follows.
Theorem 10. Let G ⊆GL(n, k) be a ﬁnite matrix group, where k is algebraically
closed. Suppose that k[x1, . . . , xn]G = k[ f1, . . . , fm]. Then:

§4 Relations Among Generators and the Geometry of Orbits
379
(i) The polynomial mapping F : kn →VF deﬁned by F(a) = ( f1(a), . . . , fm(a))
is onto. Geometrically, this means that the parametrization yi = fi(x1, . . . , xn)
covers all of VF.
(ii) The map sending the G-orbit G · a ⊆kn to the point F(a) ∈VF induces a
one-to-one correspondence
kn/G ∼= VF.
Proof. We prove part (i) using elimination theory. Let JF = ⟨f1 −y1, . . . , fm −ym⟩
be the ideal deﬁned in Proposition 3. Since IF = JF ∩k[y1, . . . , ym] is an elimination
ideal of JF, it follows that a point (b1, . . . , bm) ∈VF = V(IF) is a partial solution of
the system of equations
y1 = f1(x1, . . . , xn),
...
ym = fm(x1, . . . , xn).
If we can prove that (b1, . . . , bm) ∈V(IF) extends to (a1, . . . , an, b1, . . . , bm) ∈
V(JF), then F(a1, . . . , an) = (b1, . . . , bm) and the surjectivity of F : kn →VF will
follow.
We claim that for each i, there is an element pi ∈JF ∩k[xi, . . . , xn, y1, . . . , ym]
such that
(4)
pi = xN
i + terms in which xi has degree < N,
where N = |G|. For now, we will assume that the claim is true.
Suppose that inductively we have extended (b1, . . . , bm) to a partial solution
(ai+1, . . . , an, b1, . . . , bm) ∈V(JF ∩k[xi+1, . . . , xn, y1, . . . , ym]).
Since k is algebraically closed, the Extension Theorem of Chapter 3, §1 asserts that
we can extend to (ai, ai+1, . . . , an, b1, . . . , bm), provided the leading coefﬁcient in
xi of one of the generators of JF ∩k[xi, . . . , xn, y1, . . . , ym] does not vanish at the
partial solution. Because of our claim, this ideal contains the above polynomial pi
and we can assume that pi is a generator (just add it to the generating set). By (4),
the leading coefﬁcient is 1, which never vanishes, so that the required ai exists (see
Corollary 4 of Chapter 3, §1).
It remains to prove the existence of pi. We will need the following lemma.
Lemma 11. Let G ⊆GL(n, k) be a ﬁnite matrix group and set N = |G|. Given any
f ∈k[x1, . . . , xn], there are invariants g1, . . . , gN ∈k[x1, . . . , xn]G such that
f N + g1f N−1 + · · · + gN = 0.
Proof. Consider the polynomial  
A∈G(X −f(A · x)). If we multiply it out, we get
!
A∈G

X −f(A · x)
	
= XN + g1(x)XN−1 + · · · + gN(x),

380
Chapter 7 Invariant Theory of Finite Groups
where the coefﬁcients g1, . . . , gN are in k[x1, . . . , xn]. We claim that g1, . . . , gN are
invariant under G. To prove this, suppose that B ∈G. In the proof of Proposition 3
of §3, we saw that the f(AB · x) are just the f(A · x), possibly in a different order.
Thus
!
A∈G

X −f(AB · x)
	
=
!
A∈G

X −f(A · x)
	
,
and then multiplying out each side implies that
XN + g1(B · x)XN−1 + · · · + gN(B · x) = XN + g1(x)XN−1 + · · · + gN(x)
for each B ∈G. This proves that g1, . . . , gN ∈k[x1, . . . , xn]G.
Since one of the factors is X −f(In · x) = X −f(x), the polynomial vanishes
when X = f, and the lemma is proved.
□
We can now prove our claim about the polynomial pi. If we substitute f = xi in
Lemma 11, then we get
(5)
xN
i + g1xN−1
i
+ · · · + gN = 0
for N = |G| and g1, . . . , gN ∈k[x1, . . . , xn]G. Since k[x1, . . . , xn]G = k[ f1, . . . , fm],
we can write gj = hj( f1, . . . , fm) for j = 1, . . . , N. Then let
pi(xi, y1, . . . , ym) = xN
i + h1(y1, . . . , ym)xN−1
i
+ · · · + hN(y1, . . . , ym)
in k[xi, y1, . . . , ym]. From (5), it follows that pi(xi, f1, . . . , fm) = 0 and, hence, by (2),
we see that pi ∈JF. Then pi ∈JF ∩k[xi, . . . , xn, y1, . . . , ym], and our claim is proved.
To prove (ii), ﬁrst note that the map
˜F : kn/G →VF
deﬁned by sending G · a to F(a) = ( f1(a), . . . , fm(a)) is well-deﬁned since each
fi is invariant and, hence, takes the same value on all points of a G-orbit G · a.
Furthermore, F is onto by part (i) and it follows that ˜F is also onto.
It remains to show that ˜F is one-to-one. Suppose that G · a and G · b are distinct
orbits. Since ∼G is an equivalence relation, it follows that the orbits are disjoint. We
will construct an invariant g ∈k[x1, . . . , xn]G such that g(a) ̸= g(b). To do this, note
that S = G · b ∪(G · a \ {a}) is a ﬁnite set of points in kn and, hence, is an afﬁne
variety. Since a ̸∈S, there must be some deﬁning equation f of S which does not
vanish at a. Thus, for A ∈G, we have
f(A · b) = 0
and
f(A · a) =

0
if A · a ̸= a
f(a) ̸= 0
if A · a = a.
Then let g = RG( f). We leave it as an exercise to check that
g(b) = 0 and g(a) = M
|G| f(a) ̸= 0,

§4 Relations Among Generators and the Geometry of Orbits
381
where M is the number of elements A ∈G such that A · a = a. We have thus found
an element g ∈k[x1, . . . , xn]G such that g(a) ̸= g(b).
Now write g as a polynomial g = h( f1, . . . , fm) in our generators. Then g(a) ̸=
g(b) implies that fi(a) ̸= fi(b) for some i, and it follows that ˜F takes different values
on G · a and G · b. The theorem is now proved.
□
Theorem 10 shows that there is a bijection between the set kn/G and the variety
VF. This is what we mean by saying that kn/G has the structure of an afﬁne variety.
Further, whereas IF depends on the generators chosen for k[x1, . . . , xn]G, we noted
in Corollary 8 that VF is unique up to isomorphism. This implies that the variety
structure on kn/G is unique up to isomorphism.
One nice consequence of Theorem 10 and Proposition 7 is that the "polynomial
functions" on the orbit space kn/G are given by
k[VF] ∼= k[x1, . . . , xn]G.
Note how natural this is: an invariant polynomial takes the same value on all points
of the G-orbit and, hence, deﬁnes a function on the orbit space. Thus, it is reasonable
to expect that k[x1, . . . , xn]G should be the "coordinate ring" of whatever variety
structure we put on kn/G.
Still, the bijection kn/G ∼= VF is rather remarkable if we look at it slightly differ-
ently. Suppose that we start with the geometric action of G on kn which sends a to
A · a for A ∈G. From this, we construct the orbit space kn/G as the set of orbits. To
give this set the structure of an afﬁne variety, look at what we had to do:
• we made the action algebraic by letting G act on polynomials;
• we considered the invariant polynomials and found ﬁnitely many generators; and
• we formed the ideal of relations among the generators.
The equations coming from this ideal deﬁne the desired variety structure VF on
kn/G.
In general, an important problem in algebraic geometry is to take a set of inter-
esting objects (G-orbits, lines tangent to a curve, etc.) and give it the structure of an
afﬁne (or projective—see Chapter 8) variety. Some simple examples will be given
in the exercises.
A ﬁnal remark is for readers who studied Noether normalization in §6 of Chap-
ter 5. In the terminology of that section, Lemma 11 proved above says that every
element of k[x1, . . . , xn] is integral over k[x1, . . . , xn]G. In the exercises, you will use
results from Chapter 5, §6 to show that k[x1, . . . , xn] is ﬁnite over k[x1, . . . , xn]G in
the sense of Deﬁnition 2 of that section.
EXERCISES FOR §4
1. Given f1, . . . , fm ∈k[x1, . . . , xn], let I = {g ∈k[y1, . . . , ym] | g( f1, . . . , fm) = 0}.
a. Prove that I is an ideal of k[y1, . . . , ym].
b. If f ∈k[ f1, . . . , fm] and f = g( f1, . . . , fm) is one representation of f in terms of
f1, . . . , fm, prove that all such representations are given by f = g( f1, . . . , fm) +
h( f1, . . . , fm) as h varies over I.

382
Chapter 7 Invariant Theory of Finite Groups
2. Let f1, . . . , fm ∈k[x1, . . . , xn] and let I ⊆k[y1, . . . , ym] be the ideal of relations deﬁned
in Exercise 1.
a. Prove that the map sending a coset [g] to g( f1, . . . , fm) deﬁnes a well-deﬁned ring
homomorphism
φ : k[y1, . . . , ym]/I −→k[ f1, . . . , fm].
b. Prove that the map φ of part (a) is one-to-one and onto. Thus φ is a ring isomorphism.
c. Use Exercise 13 in Chapter 5, §2 to give an alternate proof that k[y1, . . . , ym]/I and
k[ f1, . . . , fm] are isomorphic. Hint: Use the ring homomorphism Φ : k[y1, . . . , ym] →
k[ f1, . . . , fm] which sends yi to fi.
3. Although Propositions 1 and 2 were stated for k[x1, . . . , xn]G, we saw in Exercises 1
and 2 that these results held for any subring of k[x1, . . . , xn] of the form k[ f1, . . . , fm].
Give a similar generalization of Proposition 3. Does the proof given in the text need any
changes?
4. Given p ∈k[x1, . . . , xn, y1, . . . , ym], prove that
p(x1, . . . , xn, y1, . . . , ym) = p(x1, . . . , xn, f1, . . . , fm)
+ B1 · ( f1 −y1) + · · · + Bm · ( fm −ym)
for some B1, . . . , Bm ∈k[x1, . . . , xn, y1, . . . , ym]. Hint: In p, replace each occurrence of
yi by fi −( fi −yi). The proof is similar to the argument given to prove (4) in §1.
5. Complete Example 5 by showing that IF ⊆k[u, v, w] is given by IF = ⟨u2w −v2 −4w2⟩
when F = (x2 + y2, x3y −xy3, x2y2).
6. In Exercise 7 of §3, you were asked to compute the invariants of a certain cyclic group
C3 ⊆GL(2, k) of order 3. Take the generators you found for k[x, y]C3 and ﬁnd the
relations between them.
7. Repeat Exercise 6, this time using the cyclic group C6 ⊆GL(2, k) of order 6 from
Exercise 8 of §3.
8. In Exercise 12 of §3, we listed four invariants f1, f2, f3, f4 of the group of rotations of the
cube in R3.
a. Using ( f4/xyz)2 and part (c) of Exercise 12 of §3, ﬁnd an algebraic relation between
f1, f2, f3, f4.
b. Show that there are no nontrivial algebraic relations between f1, f2, f3.
c. Show that the relation you found in part (a) generates the ideal of all relations be-
tween f1, f2, f3, f4. Hint: If p( f1, f2, f3, f4) = 0 is a relation, use part (a) to reduce to
a relation of the form p1( f1, f2, f3) + p2( f1, f2, f3)f4 = 0. Then explain how degree
arguments imply p1( f1, f2, f3) = 0.
9. Given a ﬁnite matrix group G ⊆GL(n, k), we deﬁned the relation ∼G on kn by a ∼G b
if b = A · a for some A ∈G.
a. Verify that ∼G is an equivalence relation.
b. Prove that the equivalence class of a is the set G · a deﬁned in the text.
10. Consider the group of rotations of the cube in R3. We studied this group in Exercise 5
of §2, and we know that it has 24 elements.
a. Draw a picture of the cube which shows orbits consisting of 1, 6, 8, 12 and 24 ele-
ments.
b. Argue geometrically that there is no orbit consisting of four elements.
11. (Requires abstract algebra) Let G ⊆GL(n, k) be a ﬁnite matrix group. In this problem,
we will prove that the number of elements in an orbit G · a divides |G|.
a. Fix a ∈kn and let H = {A ∈G | A · a = a}. Prove that H is a subgroup of G. We
call H the isotropy subgroup or stabilizer of a.
b. Given A ∈G, we get the left coset AH = {AB | B ∈H} of H in G and we let
G/H denote the set of all left cosets (note that G/H will not be a group unless H is
normal). Prove that the map sending AH to A·a induces a bijective map G/H ∼= G·a.

§4 Relations Among Generators and the Geometry of Orbits
383
Hint: You will need to prove that the map is well-deﬁned. Recall that two cosets AH
and BH are equal if and only if B−1A ∈H.
c. Use part (b) to prove that the number of elements in G · a divides |G|.
12. As in the proof of Theorem 10, suppose that we have disjoint orbits G · a and G · b. Set
S = G · b ∪G · a −{a}, and pick f ∈k[x1, . . . , xn] such that f = 0 on all points of S
but f (a) ̸= 0. Let g = RG( f ), where RG is the Reynolds operator of G.
a. Explain why g(b) = 0.
b. Explain why g(a) =
M
|G| f (a) ̸= 0, where M is the number of elements A ∈G such
that A · a = a.
13. In this exercise, we will see how Theorem 10 can fail when we work over a ﬁeld that is
not algebraically closed. Consider the group of permutation matrices S2 ⊆GL(2, R).
a. We know that R[x, y]S2 = R[σ1, σ2]. Show that IF = {0} when F = (σ1, σ2), so that
VF = R2. Thus, Theorem 10 is concerned with the map ˜F : R2/S2 →R2 deﬁned by
sending S2 · (x, y) to (y1, y2) = (x + y, xy).
b. Show that the image of ˜F is the set {(y1, y2) ∈R2 | y2
1 ≥4y2} ⊆R2. This is the
region lying below the parabola y2
1 = 4y2. Hint: Interpret y1 and y2 as coefﬁcients of
the quadratic X2 −y1X + y2. When does the quadratic have real roots?
14. There are many places in mathematics where one takes a set of equivalence classes
and puts an algebraic structure on them. Show that the construction of a quotient ring
k[x1, . . . , xn]/I is an example. Hint: See §2 of Chapter 5.
15. In this exercise, we will give some examples of how something initially deﬁned as a set
can turn out to be a variety in disguise. The key observation is that the set of nonvertical
lines in the plane k2 has a natural geometric structure. Namely, such a line L has a
unique equation of the form y = mx+ b, so that L can be identiﬁed with the point (m, b)
in another 2-dimensional afﬁne space, denoted k2∨. (If we use projective space—to be
studied in the next chapter-then we can also include vertical lines.)
Now suppose that we have a curve C in the plane. Then consider all lines which are
tangent to C somewhere on the curve. This gives us a subset C∨⊆k2∨. Let us compute
this subset in some simple cases and show that it is an afﬁne variety.
a. Suppose our curve C is the parabola y = x2. Given a point (x0, y0) on the parabola,
show that the tangent line is given by y = 2x0x −x2
0 and conclude that C∨is the
parabola m2 + 4b = 0 in k2∨.
b. Show that C∨is an afﬁne variety when C is the cubic curve y = x3.
In general, more work is needed to study C∨. In particular, the method used in the above
examples breaks down when there are vertical tangents or singular points. Nevertheless,
one can develop a satisfactory theory of what is called the dual curve C∨of a curve
C ⊆k2. One can also deﬁne the dual variety V∨of a given irreducible variety V ⊆kn.
16. (Assumes §6 of Chapter 5) Let G ⊆GL(n, k) be a ﬁnite matrix group. Prove that
k[x1, . . . , xn] is ﬁnite over k[x1, . . . , xn]G as in Deﬁnition 1 of Chapter 5, §6. Hint: See
Exercise 9 of that section.

Chapter 8
Projective Algebraic Geometry
So far all of the varieties we have studied have been subsets of afﬁne space kn. In this
chapter, we will enlarge kn by adding certain "points at ∞" to create n-dimensional
projective space Pn(k). We will then deﬁne projective varieties in Pn(k) and study
the projective version of the algebra-geometry dictionary. The relation between
afﬁne and projective varieties will be considered in §4; in §5, we will study elimi-
nation theory from a projective point of view. By working in projective space, we
will get a much better understanding of the Extension Theorem in Chapter 3. The
chapter will end with a discussion of the geometry of quadric hypersurfaces and an
introduction to Bezout's Theorem.
§1
The Projective Plane
This section will study the projective plane P2(R) over the real numbers R. We will
see that, in a certain sense, the plane R2 is missing some "points at ∞," and by
adding them to R2, we will get the projective plane P2(R). Then we will introduce
homogeneous coordinates. to give a more systematic treatment of P2(R) Our start-
ing point is the observation that two lines in R2 intersect in a point, except when they
are parallel. We can take care of this exception if we view parallel lines as meeting
at some sort of point at ∞. As indicated by the picture at the top of the following
page, there should be different points at ∞, depending on the direction of the lines.
To approach this more formally, we introduce an equivalence relation on lines in the
plane by setting L1 ∼L2 if L1 and L2 are parallel. Then an equivalence class [L]
consists of all lines parallel to a given line L. The above discussion suggests that
we should introduce one point at ∞for each equivalence class [L]. We make the
following provisional deﬁnition.
Deﬁnition 1. The projective plane over R, denoted P2(R), is the set
P2(R) = R2 ∪{one point at ∞for each equivalence class of parallel lines}.
© Springer International Publishing Switzerland 2015
D.A. Cox et al., Ideals, Varieties, and Algorithms, Undergraduate Texts
in Mathematics, DOI 10.1007/978-3-319-16721-3_8
385

386
Chapter 8 Projective Algebraic Geometry
meet at a point at ∞
meet at a different point at ∞
↓
↓
↑
↑
x
y
Let [L]∞denote the common point at ∞of all lines parallel to L. Then we call
the set L = L ∪[L]∞⊆P2(R) the projective line corresponding to L. Note that two
projective lines always meet at exactly one point: if they are not parallel, they meet
at a point in R2; if they are parallel, they meet at their common point at ∞.
At ﬁrst sight, one might expect that a line in the plane should have two points
at ∞, corresponding to the two ways we can travel along the line. However, the
reason why we want only one is contained in the previous paragraph: if there were
two points at ∞, then parallel lines would have two points of intersection, not one.
So, for example, if we parametrize the line x = y via (x, y) = (t, t), then we can
approach its point at ∞using either t →∞or t →−∞.
A common way to visualize points at ∞is to make a perspective drawing. Pre-
tend that the earth is ﬂat and consider a painting that shows two roads extending
inﬁnitely far in different directions:
↓
vanishing point
↓
vanishing point
← horizon
For each road, the two sides (which are parallel, but appear to be converging) meet
at the same point on the horizon, which in the theory of perspective is called a
vanishing point. Furthermore, any line parallel to one of the roads meets at the same
vanishing point, which shows that the vanishing point represents the point at ∞
of these lines. The same reasoning applies to any point on the horizon, so that the

§1 The Projective Plane
387
horizon in the picture represents points at ∞. (Note that the horizon does not contain
all of them—it is missing the point at ∞of lines parallel to the horizon.)
The above picture reveals another interesting property of the projective plane:
the points at ∞form a special projective line, which is called the line at ∞. It
follows that P2(R) has the projective lines L = L ∪[L]∞, where L is a line in
R2, together with the line at ∞. In the exercises, you will prove that two distinct
projective lines in P2(R) determine a unique point and two distinct points in P2(R)
determine a unique projective line. Note the symmetry in these statements: when we
interchange "point" and "projective line" in one, we get the other. This is an instance
of the principle of duality, which is one of the fundamental concepts of projective
geometry.
For an example of how points at ∞can occur in other contexts, consider the
parametrization of the hyperbola x2 −y2 = 1 given by the equations
x = 1 + t2
1 −t2 ,
y =
2t
1 −t2 .
When t ̸= ±1, it is easy to check that this parametrization covers all of the hyperbola
except (−1, 0). But what happens when t = ±1? Here is a picture of the hyperbola:
-2
-1.5
-1
-.5
.5
1
1.5
2
-2
-1.5
-1
-.5
.5
1
1.5
2
If we let t →1−, then the corresponding point (x, y) travels along the ﬁrst quadrant
portion of the hyperbola, getting closer and closer to the asymptote x = y. Similarly,
if t →1+, we approach x = y along the third quadrant portion of the hyperbola.
Hence, it becomes clear that t = 1 should correspond to the point at ∞of the
asymptote x = y. Similarly, one can check that t = −1 corresponds to the point at
∞of x = −y. (In the exercises, we will give a different way to see what happens
when t = ±1.)
Thus far, our discussion of the projective plane has introduced some nice ideas,
but it is not entirely satisfactory. For example, it is not really clear why the line at
∞should be called a projective line. A more serious objection is that we have no

388
Chapter 8 Projective Algebraic Geometry
uniﬁed way of naming points in P2(R). Points in R2 are speciﬁed by coordinates,
but points at ∞are speciﬁed by lines. To avoid this asymmetry, we will introduce
homogeneous coordinates on P2(R).
To get homogeneous coordinates, we will need a new deﬁnition of projective
space. The ﬁrst step is to deﬁne an equivalence relation on nonzero points of R3 by
setting
(x1, y1, z1) ∼(x2, y2, z2)
if there is a nonzero real number λ such that (x1, y1, z1) = λ(x2, y2, z2). One can
easily check that ∼is an equivalence relation on R3 \ {0} (where as usual 0 refers
to the origin (0, 0, 0) in R3). Then we can redeﬁne projective space as follows.
Deﬁnition 2. P2(R) is the set of equivalence classes of ∼on R3 \{0}. Thus, we can
write
P2(R) = (R3 \ {0})/∼.
Given a triple (x, y, z) ∈R3 \ {0}, its equivalence class p ∈P2(R) will be denoted
p = (x :y: z), and we say that (x : y:z) are homogeneous coordinates of p. Thus
(x1 :y1 :z1) = (x2 :y2 :z2) ⇔(x1, y1, z1) = λ(x2, y2, z2) for some λ ∈R \ {0}.
At this point, it is not clear that Deﬁnitions 1 and 2 give the same object, although
we will see shortly that this is the case.
Homogeneous coordinates are different from the usual notion of coordinates in
that they are not unique. For example, the four points (1 : 1 :1), (2 : 2 : 2), (π : π : π)
and (
√
2 :
√
2 :
√
2) are in fact the same point in projective space. But the nonunique-
ness of the coordinates is not so bad since they are all multiples of one another.
As an illustration of how we can use homogeneous coordinates, let us deﬁne the
notion of a projective line.
Deﬁnition 3. Given real numbers A, B, C, not all zero, the set
{p ∈P2(R) | p = (x :y: z) with Ax + By + Cz = 0}
is called a projective line of P2(R).
An important observation is that if the equation Ax + By + Cz = 0 holds for
one set (x :y: z) of homogeneous coordinates of p ∈P2(R), then it holds for all
homogeneous coordinates of p. This is because all of the others can be written as
(λx : λy: λz), so that A · λx + B · λy + C · λz = λ(Ax + By + Cz) = 0. Later in this
chapter, we will use the same idea to deﬁne varieties in projective space.
To relate our two deﬁnitions of projective plane, we will use the map
(1)
R2 −→P2(R)
deﬁned by sending (x, y) ∈R2 to the point p ∈P2(R) whose homogeneous coordi-
nates are (x : y: 1). This map has the following properties.

§1 The Projective Plane
389
Proposition 4. The map (1) is one-to-one and the complement of its image is the
projective line H∞deﬁned by z = 0.
Proof. First, suppose that (x, y) and (x′, y′) map to the same point p in P2(R). Then
p = (x :y: 1) = (x′ :y′ :1), so that (x, y, 1) = λ(x′, y′, 1) for some λ. Looking at the
third coordinate, we see that λ = 1 and it follows that (x, y) = (x′, y′).
Next, let p = (x :y: z) be a point in P2(R). If z = 0, then p is on the projective
line H∞. On the other hand, if z ̸= 0, then we can multiply by 1/z to see that
p = (x/z: y/z:1). This shows that p is in the image of map (1). We leave it as an
exercise to show that the image of the map is disjoint from H∞, and the proposition
is proved.
□
We will call H∞the line at ∞. It is customary (though somewhat sloppy) to
identify R2 with its image in P2(R), so that we can write projective space as the
disjoint union
P2(R) = R2 ∪H∞.
This is beginning to look familiar. It remains to show that H∞consists of points at
∞in our earlier sense. Thus, we need to study how lines in R2 (which we will call
afﬁne lines) relate to projective lines. The following table tells the story:
afﬁne line
projective line
point at ∞
L : y = mx + b
→L : y = mx + bz →
(1 : m: 0)
L : x = c
→
L : x = cz
→
(0 :1 : 0)
To understand this table, ﬁrst consider a nonvertical afﬁne line L deﬁned by y =
mx+b. Under the map (1), a point (x, y) on L maps to a point (x :y: 1) satisfying the
projective equation y = mx + bz. Thus, (x :y: 1) lies on the projective line L deﬁned
by mx −y + bz = 0, so that L can be regarded as subset of L. By Proposition 4, the
remaining points of L come from where it meets z = 0. But the equations z = 0 and
y = mx + bz clearly imply y = mx, so that the solutions are (x :mx :0). We have
x ̸= 0 since homogeneous coordinates never simultaneously vanish, and dividing
by x shows that (1 : m: 0) is the unique point of L ∩H∞. The case of vertical lines
is left as an exercise.
The table shows that two lines in R2 meet at the same point at ∞if and only if
they are parallel. For nonvertical lines, the point at ∞encodes the slope, and for
vertical lines, there is a single (but different) point at ∞. Be sure you understand
this. In the exercises, you will check that the points listed in the table exhaust all
of H∞. Consequently, H∞consists of a unique point at ∞for every equivalence
class of parallel lines. Then P2(R) = R2 ∪H∞shows that the projective planes of
Deﬁnitions 1 and 2 are the same object.
We next introduce a more geometric way of thinking about points in the pro-
jective plane. Let p = (x :y: z) be a point in P2(R), so that all other homogeneous
coordinates for p are given by (λx : λy: λz) for λ ∈R\ {0}. The crucial observation
is that back in R3, the points (λx, λy, λz) = λ(x, y, z) all lie on the same line through
the origin in R3:

390
Chapter 8 Projective Algebraic Geometry
y
z
x
λ(x,y,z)
(x,y,z)
↑
line through the origin
The requirement in Deﬁnition 2 that (x, y, z) ̸= (0, 0, 0) guarantees that we get a
line in R3. Conversely, given any line L through the origin in R3, a point (x, y, z)
on L \ {0} gives homogeneous coordinates (x : y: z) for a uniquely determined point
in P2(R) [since any other point on L \ {0} is a nonzero multiple of (x, y, z)]. This
shows that we have a one-to-one correspondence.
(2)
P2(R) ∼= {lines through the origin in R3}.
Although it may seem hard to think of a point in P2(R) as a line in R3, there is
a strong intuitive basis for this identiﬁcation. We can see why by studying how to
draw a 3-dimensional object on a 2-dimensional canvas. Imagine lines or rays that
link our eye to points on the object. Then we draw the object according to where the
rays intersect the canvas:
← canvas
← eye
↓
object
↑
rays
Renaissance texts on perspective would speak of the "pyramid of rays" connecting
the artist's eye with the object being painted. For us, the crucial observation is that
each ray hits the canvas exactly once, giving a one-to-one correspondence between
rays and points on the canvas.
To make this more mathematical, we will let the "eye" be the origin and the
"canvas" be the plane z = 1 in the coordinate system pictured at the top of the next
page. Rather than work with rays (which are half-lines), we will work with lines
through the origin. Then, as the picture indicates, every point in the plane z = 1
determines a unique line through the origin. This one-to-one correspondence allows
us to think of a point in the plane as a line through the origin in R3 [which by (2) is a
point in P2(R)]. There are two interesting things to note about this correspondence:

§1 The Projective Plane
391
← the plane z = 1
z
y
x
↑
point
↑
line
• A point (x, y) in the plane gives the point (x, y, 1) on our "canvas" z = 1. The
corresponding line through the origin is a point p ∈P2(R) with homogeneous
coordinates (x :y: 1). Hence, the correspondence given above is exactly the map
R2 →P2(R) from Proposition 4.
• The correspondence is not onto since this method will never produce a line in the
(x, y)-plane. Do you see how these lines can be thought of as the points at ∞?
In many situations, it is useful to be able to think of P2(R) both algebraically
(in terms of homogeneous coordinates) and geometrically (in terms of lines through
the origin).
As the ﬁnal topic in this section, we will use homogeneous coordinates to ex-
amine the line at ∞more closely. The basic observation is that although we began
with coordinates x and y, once we have homogeneous coordinates, there is nothing
special about the extra coordinate z—it is no different from x or y. In particular, if
we want, we could regard x and z as the original coordinates and y as the extra one.
To see how this can be useful, consider the parallel lines L1 : y = x + 1/2 and
L2 : y = x −1/2 in the (x, y)-plane:
L1
L2
↓
↓
x
y
.5
.5
In the (x,y)-plane

392
Chapter 8 Projective Algebraic Geometry
We know that these lines intersect at ∞since they are parallel. But the picture does
not show their point of intersection. To view these lines at ∞, consider the projective
lines
L1 : y = x + (1/2)z,
L2 : y = x −(1/2)z
determined by L1 and L2. Now regard x and z as the original variables. Thus, we map
the (x, z)-plane R2 to P2(R) via (x, z) 
→(x :1 :z). As in Proposition 4, this map is
one-to-one, and we can recover the (x, z)-plane inside P2(R) by setting y = 1. If
we do this with the equations of the projective lines L1 and L2, we get the lines
L′
1 : z = −2x + 2 and L′
2 : z = 2x −2. This gives the following picture in the
(x, z)-plane:
L′1
L′2
↓
↓
↓
z = 0
x
z
1
In the (x,z)-plane
In this picture, the x-axis is deﬁned by z = 0, which is the line at ∞as we originally
set things up in Proposition 4. Note that L′
1 and L′
2 meet when z = 0, which cor-
responds to the fact that L1 and L2 meet at ∞. Thus, the above picture shows how
our two lines behave as they approach the line at ∞. In the exercises, we will study
what some other common curves look like at ∞.
It is interesting to compare the above picture with the perspective drawing of two
roads given earlier in the section. It is no accident that the horizon in the perspective
drawing represents the line at ∞. The exercises will explore this idea in more detail.
Another interesting observation is that the Euclidean notion of distance does not
play a prominent role in the geometry of projective space. For example, the lines
L1 and L2 in the (x, y)-plane are a constant distance apart, whereas L′
1 and L′
2 get
closer and closer in the (x, z)-plane. This explains why the geometry of P2(R) is
quite different from Euclidean geometry.

§1 The Projective Plane
393
EXERCISES FOR §1
1. Using P2(R) as given in Deﬁnition 1, we saw that the projective lines in P2(R) are
L = L ∪[L]∞, and the line at ∞.
a. Prove that any two distinct points in P2(R) determine a unique projective line. Hint:
There are three cases, depending on how many of the points are points at ∞.
b. Prove that any two distinct projective lines in P2(R) meet at a unique point. Hint: Do
this case-by-case.
2. There are many theorems that initially look like theorems in the plane, but which are
really theorems in P2(R) in disguise. One classic example is Pappus's Theorem, which
goes as follows. Suppose we have two collinear triples of points A, B, C and A′, B′, C′.
Then let
P = AB′ ∩A′B,
Q = AC′ ∩A′C,
R = BC′ ∩B′C.
Pappus's Theorem states that P, Q, R are always collinear points. In Exercise 8 of
Chapter 6, §4, we drew the following picture to illustrate the theorem:
C
B
A
A¢
B¢
C¢
P
Q
R
a. If we let the points on one of the lines go in the opposite order, then we can get the
following conﬁguration of points and lines:
C
B
A
C¢
B¢
A¢
Q
R
Note that P is now a point at ∞when AB′ and A′B are parallel. Is Pappus's Theorem
still true [in P2(R)] for this conﬁguration of points and lines?
b. By moving the point C in the picture for part (a) show that you can also make Q
a point at ∞. Is Pappus's Theorem still true? What line do P, Q, R lie on? Draw a
picture to illustrate what happens.

394
Chapter 8 Projective Algebraic Geometry
If you made a purely afﬁne version of Pappus's Theorem that took cases (a) and (b) into
account, the resulting statement would be rather cumbersome. By working in P2(R), we
cover these cases simultaneously.
3. We will continue the study of the parametrization (x, y) = ((1+t2)/(1−t2), 2t/(1−t2))
of x2 −y2 = 1 begun in the text.
a. Given t, show that (x, y) is the point where the hyperbola intersects the line of slope t
going through the point (−1, 0). Illustrate your answer with a picture. Hint: Use the
parametrization to show that t = y/(x + 1).
b. Use the answer to part (a) to explain why t = ±1 maps to the points at ∞corre-
sponding to the asymptotes of the hyperbola. Illustrate your answer with a drawing.
c. Using homogeneous coordinates, show that we can write the parametrization as
((1 + t2)/(1 −t2) : 2t/(1 −t2) : 1) = (1 + t2 : 2t : 1 −t2),
and use this to explain what happens when t = ±1. Does this give the same answer
as part (b)?
d. We can also use the technique of part (c) to understand what happens when t →∞.
Namely, in the parametrization (x : y : z) = (1 + t2 : 2t : 1 −t2), substitute t = 1/u.
Then clear denominators (this is legal since we are using homogeneous coordinates)
and let u →0. What point do you get on the hyperbola?
4. This exercise will study what the hyperbola x2 −y2 = 1 looks like at ∞.
a. Explain why the equation x2 −y2 = z2 gives a well-deﬁned curve C in P2(R). Hint:
See the discussion following Deﬁnition 3.
b. What are the points at ∞on C? How does your answer relate to Exercise 3?
c. In the (x, z) coordinate system obtained by setting y = 1, show that C is still a
hyperbola.
d. In the (y, z) coordinate system obtained by setting x = 1, show that C is a circle.
e. Use the parametrization of Exercise 3 to obtain a parametrization of the circle from
part (d).
5. Consider the parabola y = x2.
a. What equation should we use to make the parabola into a curve in P2(R)?
b. How many points at ∞does the parabola have?
c. By choosing appropriate coordinates (as in Exercise 4), explain why the parabola is
tangent to the line at ∞.
d. Show that the parabola looks like a hyperbola in the (y, z) coordinate system.
6. When we use the (x, y) coordinate system inside P2(R), we only view a piece of the
projective plane. In particular, we miss the line at ∞. As in the text, we can use (x, z)
coordinates to view the line at ∞. Show that there is exactly one point in P2(R) that is
visible in neither (x, y) nor (x, z) coordinates. How can we view what is happening at
this point?
7. In the proof of Proposition 4, show that the image of the map (2) is disjoint from H∞.
8. As in the text, the line H∞is deﬁned by z = 0. Thus, points on H∞have homogeneous
coordinates (a : b : 0), where (a, b) ̸= (0, 0).
a. A vertical afﬁne line x = c gives the projective line x = cz. Show that this meets H∞
at the point (0 : 1 : 0).
b. Show that a point on H∞different from (0 : 1 : 0) can be written uniquely in the from
(1 : m : 0) for some real number m.
9. In the text, we viewed parts of P2(R) in the (x, y) and (x, z) coordinate systems. In the
(x, z) picture, it is natural to ask what happened to y. To see this, we will study how
(x, y) coordinates look when viewed in the plane with (x, z) coordinates.
a. Show that (a, b) in the (x, y)-plane gives the point (a/b, 1/b) in the (x, z)-plane.
b. Use the formula of part (a) to study what the parabolas (x, y) = (t, t2) and (x, y) =
(t2, t) look like in the (x, z)-plane. Draw pictures of what happens in both (x, y) and
(x, z) coordinates.

§1 The Projective Plane
395
10. In this exercise, we will discuss the mathematics behind the perspective drawing given
in the text. Suppose we want to draw a picture of a landscape, which we will assume is a
horizontal plane. We will make our drawing on a canvas, which will be a vertical plane.
Our eye will be a certain distance above the landscape, and to draw, we connect a point
on the landscape to our eye with a line, and we put a dot where the line hits the canvas:
← the canvas y = 1
↑
 the landscape z = 1
the origin →
To give formulas for what happens, we will pick coordinates (x, y, z) so that our eye
is the origin, the canvas is the plane y = 1, and the landscape is the plane z = 1 (thus,
the positive z-axis points down).
a. Starting with the point (a, b, 1) on the landscape, what point do we get in the canvas
y = 1?
b. Explain how the answer to part (a) relates to Exercise 9. Write a brief paragraph
discussing the relation between perspective drawings and the projective plane.
11. As in Deﬁnition 3, a projective line in P2(R) is deﬁned by an equation of the form
Ax + By + Cz = 0, where (A, B, C) ̸= (0, 0, 0).
a. Why do we need to make the restriction (A, B, C) ̸= (0, 0, 0)?
b. Show that (A, B, C) and (A′, B′, C′) deﬁne the same projective line if and only if
(A, B, C) = λ(A′, B′, C′) for some nonzero real number λ. Hint: One direction is
easy. For the other direction, take two distinct points (a : b : c) and (a′ : b′ : c′) on the
line Ax + By + Cz = 0. Show that the vectors (a, b, c) and (a′, b′, c′) are linearly
independent and conclude that the equations Xa + Yb + Zc = Xa′ + Yb′ + Zc′ = 0
have a 1-dimensional solution space for the variables X, Y, Z.
c. Conclude that the set of projective lines in P2(R) can be identiﬁed with the set
{(A, B, C) ∈R3 | (A, B, C) ̸= (0, 0, 0)}/∼. This set is called the dual projective
plane and is denoted P2(R)∨.
d. Describe the subset of P2(R)∨corresponding to afﬁne lines.
e. Given a point p ∈P2(R), consider the set ˜p of all projective lines L containing p. We
can regard ˜p as a subset of P2(R)∨. Show that ˜p is a projective line in P2(R)∨. We
call ˜p the pencil of lines through p.
f. The Cartesian product P2(R) × P2(R)∨has the natural subset
I = {(p, L) ∈P2(R) × P2(R)∨| p ∈L}.
Show that I is described by the equation Ax + By + Cz = 0, where (x : y : z) are
homogeneous coordinates on P2(R) and (A : B : C) are homogeneous coordinates on
the dual. We will study varieties of this type in §5.
Parts (d), (e), and (f) of Exercise 11 illustrate how collections of naturally deﬁned geo-
metric objects can be given an algebraic structure.

396
Chapter 8 Projective Algebraic Geometry
§2
Projective Space and Projective Varieties
The construction of the real projective plane given in Deﬁnition 2 of §1 can be
generalized to yield projective spaces of any dimension n over any ﬁeld k. We deﬁne
an equivalence relation ∼on the nonzero points of kn+1 by setting
(x′
0, . . . , x′
n) ∼(x0, . . . , xn)
if there is a nonzero element λ ∈k such that (x′
0, . . . , x′
n) = λ(x0, . . . , xn). If we let
0 denote the origin (0, . . . , 0) in kn+1, then we deﬁne projective space as follows.
Deﬁnition 1. n-dimensional projective space over the ﬁeld k, denoted Pn(k), is the
set of equivalence classes of ∼on kn+1 \ {0}. Thus,
Pn(k) = (kn+1 \ {0})/∼.
Given an (n + 1)-tuple (x0, . . . , xn) ∈kn+1 \ {0}, its equivalence class p ∈Pn(k)
will be denoted (x0 : · · · : xn), and we will say that (x0 : · · · : xn) are homogeneous
coordinates of p. Thus
(x′
0 : · · · :x′
n) = (x0 : · · · :xn) ⇐⇒(x′
0, . . . , x′
n) = λ(x0, . . . , xn)
for some λ ∈k \ {0}.
Like P2(R), each point p ∈Pn(k) has many sets of homogeneous coordinates.
For example, (0 :
√
2 : 0 :i) and (0 : 2i: 0 :−
√
2) are the same point in P3(C) since
(0, 2i, 0, −
√
2) =
√
2i(0,
√
2, 0, i).
As in §1, we can think of Pn(k) more geometrically as the set of lines through the
origin in kn+1. More precisely, you will show in Exercise 1 that there is a one-to-one
correspondence
(1)
Pn(k) ∼= {lines through the origin in kn+1}.
Just as the real projective plane contains the afﬁne plane R2 as a subset, Pn(k)
contains the afﬁne space kn.
Proposition 2. Let
U0 = {(x0 : · · · :xn) ∈Pn(k) | x0 ̸= 0}.
Then the map φ that sends (a1, . . . , an) ∈kn to (1 : a1 : · · · :an) ∈Pn(k) is a one-
to-one correspondence between kn and U0 ⊆Pn(k).
Proof. Since the ﬁrst component of φ(a1, . . . , an) = (1 : a1 : · · · :an) is nonzero,
we get a map φ : kn →U0. We deﬁne an inverse map ψ : U0 →kn as follows.
Given p = (x0 :x1 : · · · : xn) ∈U0, since x0 ̸= 0 we can multiply the homogeneous
coordinates by the nonzero scalar λ =
1
x0 to obtain p = (1 : x1
x0 : · · · : xn
x0 ). Then set
ψ(p) = ( x1
x0 , . . . , xn
x0 ) ∈kn. We leave it as an exercise for the reader to show that ψ

§2 Projective Space and Projective Varieties
397
is well-deﬁned and that φ and ψ are inverse mappings. This establishes the desired
one-to-one correspondence.
□
By the deﬁnition of U0, we see that Pn(k) = U0 ∪H, where
(2)
H = {p ∈Pn(k) | p = (0 :x1 : · · · : xn)}.
If we identify U0 with the afﬁne space kn, then we can think of H as the hyperplane
at inﬁnity. It follows from (2) that the points in H are in one-to-one correspondence
with nonzero n-tuples (x1, . . . , xn), where two n-tuples represent the same point of
H if one is a nonzero scalar multiple of the other (just ignore the ﬁrst component of
points in H). In other words, H is a "copy" of Pn−1(k), the projective space of one
smaller dimension. Identifying U0 with kn and H with Pn−1(k), we can write
(3)
Pn(k) = kn ∪Pn−1(k).
To see what H = Pn−1(k) means geometrically, note that, by (1), a point
p ∈Pn−1(k) gives a line L ⊆kn going through the origin. Consequently, in the
decomposition (3), we should think of p as representing the asymptotic direction of
all lines in kn parallel to L. This allows us to regard p as a point at ∞in the sense of
§1, and we recover the intuitive deﬁnition of the projective space given there. In the
exercises, we will give a more algebraic way of seeing how this works.
A special case worth mentioning is the projective line P1(k). Since P0(k) consists
of a single point (this follows easily from Deﬁnition 1), letting n = 1 in (3) gives us
P1(k) = k1 ∪P0(k) = k ∪{∞},
where we let ∞represent the single point of P0(k). If we use (1) to think of points
in P1(k) as lines through the origin in k2, then the above decomposition reﬂects the
fact these lines are characterized by their slope (where the vertical line has slope
∞). When k = C, it is customary to call
P1(C) = C ∪{∞}
the Riemann sphere. The reason for this name will be explored in the exercises.
For completeness, we mention that there are many other copies of kn in Pn(k)
besides U0. Indeed the proof of Proposition 2 may be adapted to yield the following
results.
Corollary 3. For each i = 0, . . . n, let
Ui = {(x0 : · · · :xn) ∈Pn(k) | xi ̸= 0}.
(i) The points of each Ui are in one-to-one correspondence with the points of kn.
(ii) The complement Pn(k) \ Ui may be identiﬁed with Pn−1(k).
(iii) We have Pn(k) = n
i=0 Ui.
Proof. See Exercise 5.
□

398
Chapter 8 Projective Algebraic Geometry
Our next goal is to extend the deﬁnition of varieties in afﬁne space to projec-
tive space. For instance, we can ask whether it makes sense to consider V( f) for
a polynomial f ∈k[x0, . . . , xn]. A simple example shows that some care must be
taken here. For instance, in P2(R), we can try to construct V(x1 −x2
2). The point
p = (x0 :x1 :x2) = (1 :4 : 2) appears to be in this set since the components of p
satisfy the equation x1 −x2
2 = 0. However, a problem arises when we note that the
same point p can be represented by the homogeneous coordinates p = (2 : 8 : 4). If
we substitute these components into our polynomial, we obtain 8 −42 = −8 ̸= 0.
We get different results depending on which homogeneous coordinates we choose.
To avoid problems of this type, we use homogeneous polynomials when working
in Pn(k). From Deﬁnition 6 of Chapter 7, §1, recall that a polynomial is homoge-
neous of total degree d if every term appearing in f has total degree exactly d. The
polynomial f = x1 −x2
2 in the example is not homogeneous, and this is what caused
the inconsistency in the values of f on different homogeneous coordinates repre-
senting the same point. For a homogeneous polynomial, this does not happen.
Proposition 4. Let f ∈k[x0, . . . , xn] be a homogeneous polynomial. If f vanishes on
any one set of homogeneous coordinates for a point p ∈Pn(k), then f vanishes for
all homogeneous coordinates of p. In particular V( f) = {p ∈Pn(k) | f(p) = 0} is
a well-deﬁned subset of Pn(k).
Proof. Let (a0 : · · · : an) = (λa0 : · · · :λan) be homogeneous coordinates for a
point p ∈Pn(k) and assume that f(a0, . . . , an) = 0. If f is homogeneous of total
degree d, then every term in f has the form
cxα0
0 · · · xαn
n ,
where α0 + · · · + αn = d. When we substitute xi = λai, this term becomes
c(λa0)α0 · · · (λan)αn = λdcaα0
0 · · · aαn
n .
Summing over the terms in f, we ﬁnd a common factor of λd and, hence,
f(λa0, . . . , λan) = λd f(a0, . . . , an) = 0.
This proves the proposition.
□
Notice that even if f is homogeneous, the equation f = a does not make sense in
Pn(k) when 0 ̸= a ∈k. The equation f = 0 is special because it gives a well-deﬁned
subset of Pn(k). We can also consider subsets of Pn(k) deﬁned by the vanishing
of a system of homogeneous polynomials (possibly of different total degrees). The
correct generalization of the afﬁne varieties introduced in Chapter 1, §2 is as follows.
Deﬁnition 5. Let k be a ﬁeld and let f1, . . . , fs ∈k[x0, . . . , xn] be homogeneous poly-
nomials. We set
V( f1, . . . , fs) = {(a0 : · · · :an) ∈Pn(k) | fi(a0, . . . , an) = 0 for all 1 ≤i ≤s}.
We call V( f1, . . . , fs) the projective variety deﬁned by f1, . . . , fs.

§2 Projective Space and Projective Varieties
399
For example, in Pn(k), any nonzero homogeneous polynomial of degree 1,
ℓ(x0, . . . , xn) = c0x0 + · · · + cnxn,
deﬁnes a projective variety V(ℓ) called a hyperplane. One example we have seen is
the hyperplane at inﬁnity, which was deﬁned as H = V(x0). When n = 2, we call
V(ℓ) a projective line, or more simply a line in P2(k). Similarly, when n = 3, we call
a hyperplane a plane in P3(k). Varieties deﬁned by one or more linear polynomials
(homogeneous polynomials of degree 1) are called linear varieties in Pn(k). For
instance, V(x1, x2) ⊆P3(k) is a linear variety which is a projective line in P3(k).
The projective varieties V( f) deﬁned by a single nonzero homogeneous equa-
tion are known collectively as hypersurfaces. However, individual hypersurfaces
are usually classiﬁed according to the total degree of the deﬁning equation. Thus, if
f has total degree 2 in k[x0, . . . , xn], we usually call V( f) a quadric hypersurface,
or quadric for short. For instance, V(−x2
0 + x2
1 + x2
2) ⊆P3(R) is quadric. Similarly,
hypersurfaces deﬁned by equations of total degree 3, 4, and 5 are known as cubics,
quartics, and quintics, respectively.
To get a better understanding of projective varieties, we need to discover what
the corresponding algebraic objects are. This leads to the notion of homogeneous
ideal, which will be discussed in §3. We will see that the entire algebra-geometry
dictionary of Chapter 4 can be carried over to projective space.
The ﬁnal topic we will consider in this section is the relation between afﬁne and
projective varieties. As we saw in Corollary 3, the subsets Ui ⊆Pn(k) are copies of
kn. Thus, we can ask how afﬁne varieties in Ui ∼= kn relate to projective varieties in
Pn(k). First, if we take a projective variety V and intersect it with one of the Ui, it
makes sense to ask whether we obtain an afﬁne variety. The answer to this question
is always yes, and the deﬁning equations of the variety V ∩Ui may be obtained by
a process called dehomogenization.
We illustrate this by considering V ∩U0. From the proof of Proposition 2,
we know that if p ∈U0, then p has homogeneous coordinates of the form
(1 : x1 : · · · :xn). If f ∈k[x0, . . . , xn] is one of the deﬁning equations of V, then the
polynomial g(x1, . . . , xn) = f(1, x1, . . . , xn) ∈k[x1, . . . , xn] vanishes at every point
of V ∩U0. Setting x0 = 1 in f produces a "dehomogenized" polynomial g which
is usually nonhomogeneous. We claim that V ∩U0 is precisely the afﬁne variety
obtained by dehomogenizing the equations of V.
Proposition 6. Let V = V( f1, . . . , fs) be a projective variety. Then W = V ∩U0
can be identiﬁed with the afﬁne variety V(g1, . . . , gs) ⊆kn, where gi(x1, . . . , xn) =
fi(1, x1, . . . , xn) for each 1 ≤i ≤s.
Proof. The comments before the statement of the proposition show that using the
mapping ψ : U0 →kn from Proposition 2, ψ(W) ⊆V(g1, . . . , gs). On the other
hand, if (a1, . . . , an) ∈V(g1, . . . , gs), then the point with homogeneous coordinates
(1 : a1 : · · · : an) is in U0 and it satisﬁes the equations
fi(1, a1, . . . , an) = gi(a1, . . . , an) = 0.

400
Chapter 8 Projective Algebraic Geometry
Thus, φ(V(g1, . . . , gs)) ⊆W. Since the mappings φ and ψ are inverses, the points
of W are in one-to-one correspondence with the points of V(g1, . . . , gs).
□
For instance, consider the projective variety
(4)
V = V(x2
1 −x2x0, x3
1 −x3x2
0) ⊆P3(R).
To intersect V with U0, we dehomogenize the deﬁning equations, which gives us the
afﬁne variety
V(x2
1 −x2, x3
1 −x3) ⊆R3.
We recognize this as the familiar twisted cubic in R3.
We can also dehomogenize with respect to other variables. For example, the
above proof shows that, for any projective variety V ⊆P3(R), V ∩U1 can be
identiﬁed with the afﬁne variety in R3 deﬁned by the equations obtained by set-
ting gi(x0, x2, x3) = fi(x0, 1, x2, x3). When we do this with the projective variety V
deﬁned in (4), we see that V ∩U1 is the afﬁne variety V(1 −x2x0, 1 −x3x2
0). See
Exercise 9 for a general statement.
Going in the opposite direction, we can ask whether an afﬁne variety in Ui, can
be written as V ∩Ui in some projective variety V. The answer is again yes, but there
is more than one way to do it, and the results can be somewhat unexpected.
One natural idea is to reverse the dehomogenization process described earlier and
"homogenize" the deﬁning equations of the afﬁne variety. For example, consider
the afﬁne variety W = V(x2 −x3
1 + x2
1) in U0 = R2. The deﬁning equation is
not homogeneous, so we do not get a projective variety in P2(R) directly from this
equation. But we can use the extra variable x0 to make f = x2−x3
1+x2
1 homogeneous.
Since f has total degree 3, we modify f so that every term has total degree 3. This
leads to the homogeneous polynomial
f h = x2x2
0 −x3
1 + x2
1x0.
Moreover, note that dehomogenizing f h gives back the original polynomial f in
x1, x2. The general pattern is the same.
Proposition 7. Let g(x1, . . . , xn) ∈k[x1, . . . , xn] be a polynomial of total degree d.
(i) Let g = d
i=0 gi be the expansion of g as the sum of its homogeneous compo-
nents where gi has total degree i. Then
gh(x0, . . . , xn) =
d

i=0
gi(x1, . . . , xn)xd−i
0
= gd(x1, . . . , xn) + gd−1(x1, . . . xn)x0
+ · · · + g0(x1, . . . xn)xd
0
is a homogeneous polynomial of total degree d in k[x0, . . . , xn]. We will call gh
the homogenization of g.

§2 Projective Space and Projective Varieties
401
(ii) The homogenization of g can be computed using the formula
gh = xd
0 · g
x1
x0
, . . . , xn
x0

.
(iii) Dehomogenizing gh yields g, i.e., gh(1, x1, · · · , xn) = g(x1, . . . , xn).
(iv) Let F(x0, . . . , xn) be a homogeneous polynomial and let xe
0 be the highest power
of x0 dividing F. If f = F(1, x1, . . . , xn) is a dehomogenization of F, then F =
xe
0 · f h.
Proof. We leave the proof to the reader as Exercise 10.
□
As a result of Proposition 7, given any afﬁne variety W = V(g1, . . . , gs) ⊆
kn, we can homogenize the deﬁning equations of W to obtain a projective variety
V = V(gh
1, . . . , gh
s) ⊆Pn(k). Moreover, by part (iii) and Proposition 6, we see that
V∩U0 = W. Thus, our original afﬁne variety W is the afﬁne portion of the projective
variety V.
As we mentioned before, though, there are some unexpected possibilities.
Example 8. In this example, we will write the homogeneous coordinates of points
in P2(k) as (x :y: z). Numbering them as 0, 1, 2, we see that U2 is the set of points
with homogeneous coordinates (x :y: 1), and x and y are coordinates on U2 ∼= k2.
Now consider the afﬁne variety W = V(g) = V(y −x3 + x) ⊆U2. We know that W
is the afﬁne portion V ∩U2 of the projective variety V = V(gh) = V(yz2 −x3 +xz2).
The variety V consists of W together with the points at inﬁnity V ∩V(z). The
afﬁne portion W is the graph of a cubic polynomial, which is a nonsingular plane
curve. The points at inﬁnity, which form the complement of W in V, are given by
the solutions of the equations
0 = yz2 −x3 + xz2,
0 = z.
It is easy to see that the solutions are z = x = 0 and since we are working in
P2(k), we get the unique point p = (0 : 1 :0) in V ∩V(z). Thus, V = W ∪{p}. An
unexpected feature of this example is the nature of the extra point p.
To see what V looks like at p, let us dehomogenize the equation of V with respect
to y and study the intersection V ∩U1. Since gh = yz2 −x3 + xz2, we ﬁnd that
W′ = V ∩U1 = V(gh(x, 1, z)) = V(z2 −x3 + xz2).
From the discussion of singularities in §4 of Chapter 3, one can easily check that p,
which becomes the point (x, z) = (0, 0) ∈W′, is a singular point on W′:

402
Chapter 8 Projective Algebraic Geometry
x
z
Thus, even if we start from a nonsingular afﬁne variety (that is, one with no singular
points), homogenizing the equations and taking the corresponding projective vari-
ety may yield a more complicated geometric object. In effect, we are not "seeing
the whole picture" in the original afﬁne portion of the variety. In general, given a
projective variety V ⊆Pn(k), since Pn(k) = n
i=0 Ui, we may need to consider
V ∩Ui for each i = 0, . . . , n to see the whole picture of V.
Our next example shows that simply homogenizing the deﬁning equations can
lead to the "wrong" projective variety.
Example 9. Consider the afﬁne twisted cubic W = V(x2 −x2
1, x3 −x3
1) in R3. By
Proposition 7, W = V ∩U0 for the projective variety V = V(x2x0 −x2
1, x3x2
0 −x3
1) ⊆
P3(R). As in Example 8, we can ask what part of V we are "missing" in the afﬁne
portion W. The complement of W in V is V ∩H, where H = V(x0) is the plane at
inﬁnity. Thus, V ∩H = V(x2x0 −x2
1, x3x2
0 −x3
1, x0), and one easily sees that these
equations reduce to
x2
1 = 0,
x3
1 = 0,
x0 = 0.
The coordinates x2 and x3 are arbitrary here, so V ∩H is the projective line
V(x0, x1) ⊆P3(R). Thus we have V = W ∪V(x0, x1).
Since the twisted cubic W is a curve in R3, our intuition suggests that it should
only have a ﬁnite number of points at inﬁnity (in the exercises, you will see that
this is indeed the case). This indicates that V is probably too big; there should be
a smaller projective variety V′ containing W. One way to create such a V′ is to
homogenize other polynomials that vanish on W. For example, the parametrization
(t, t2, t3) of W shows that x1x3−x2
2 ∈I(W). Since x1x3−x2
2 is already homogeneous,
we can add it to the deﬁning equations of V to get
V′ = V(x2x0 −x2
1, x3x2
0 −x3
1, x1x3 −x2
2) ⊆V.
Then V′ is a projective variety with the property that V′ ∩U0 = W, and in the
exercises you will show that V′ ∩H consists of the single point p = (0 : 0 : 0 :1).

§2 Projective Space and Projective Varieties
403
Thus, V′ = W ∪{p}, so that we have a smaller projective variety that restricts to the
twisted cubic. The difference between V and V′ is that V has an extra component at
inﬁnity. In §4, we will show that V′ is the smallest projective variety containing W.
In Example 9, the process by which we obtained V was completely straightfor-
ward (we homogenized the deﬁning equations of W), yet it gave us a projective
variety that was too big. This indicates that something more subtle is going on. The
complete answer will come in §4, where we will learn an algorithm for ﬁnding the
smallest projective variety containing W ⊆kn ∼= Ui.
EXERCISES FOR §2
1. In this exercise, we will give a more geometric way to describe the construction of Pn(k).
Let L denote the set of lines through the origin in kn+1.
a. Show that every element of L can be represented as the set of scalar multiples of
some nonzero vector in kn+1.
b. Show that two nonzero vectors v′ and v in kn+1 deﬁne the same element of L if and
only if v′ ∼v as in Deﬁnition 1.
c. Show that there is a one-to-one correspondence between Pn(k) and L.
2. Complete the proof of Proposition 2 by showing that the mappings φ and ψ deﬁned in
the proof are inverses.
3. In this exercise, we will study how lines in Rn relate to points at inﬁnity in Pn(R). We
will use the decomposition Pn(R) = Rn ∪Pn−1(R) given in (3). Given a line L in Rn,
we can parametrize L by the formula a + bt, where a ∈L and b is a nonzero vector
parallel to L. In coordinates, we write this parametrization as (a1 + b1t, . . . , an + bnt).
a. We can regard L as lying in Pn(R) using the homogeneous coordinates
(1 : a1 + b1t : · · · : an + bnt).
To ﬁnd out what happens as t →±∞, divide by t to obtain
1
t : a1
t + b1 : · · · : an
t + bn

.
As t →±∞, what point of H = Pn−1(R) do you get?
b. The line L will have many parametrizations. Show that the point of Pn−1(R) given
by part (a) is the same for all parametrizations of L. Hint: Two nonzero vectors are
parallel if and only if one is a scalar multiple of the other.
c. Parts (a) and (b) show that a line L in Rn has a well-deﬁned point at inﬁnity in
H = Pn−1(R). Show that two lines in Rn are parallel if and only if they have the
same point at inﬁnity.
4. When k = R or C, the projective line P1(k) is easy to visualize.
a. In the text, we called P1(C) = C ∪{∞} the Riemann sphere. To see why this name
is justiﬁed, use the parametrization from Exercise 6 of Chapter 1, §3 to show how
the plane corresponds to the sphere minus the north pole. Then explain why we can
regard C ∪{∞} as a sphere.
b. What common geometric object can we use to represent P1(R)? Illustrate your rea-
soning with a picture.
5. Prove Corollary 3.
6. This problem studies the subsets Ui ⊆Pn(k) from Corollary 3.
a. In P4(k), identify the points that are in the subsets U2, U2 ∩U3, and U1 ∩U3 ∩U4.

404
Chapter 8 Projective Algebraic Geometry
b. Give an identiﬁcation of P4(k) \ U2, P4(k) \ (U2 ∪U3), and P4(k) \ (U1 ∪U3 ∪U4)
as a "copy" of another projective space.
c. In P4(k), which points are "4
i=0 Ui?
d. In general, describe the subset Ui1 ∩· · · ∩Uis ⊆Pn(k), where
1 ≤i1 < i2 < · · · < is ≤n.
7. In this exercise, we will study when a nonhomogeneous polynomial has a well-deﬁned
zero set in Pn(k). Let k be an inﬁnite ﬁeld. We will show that if f ∈k[x0, . . . , xn] is not
homogeneous, but f vanishes on all homogeneous coordinates of some p ∈Pn(k), then
each of the homogeneous components fi of f (see Deﬁnition 6 of Chapter 7, §1) must
vanish at p.
a. Write f as a sum of its homogeneous components f = 
i fi. If p = (a0, . . . , an),
then show that
f (λa0, . . . , λan) =

i
fi(λa0, . . . , λan)
=

i
λi fi(a0, . . . , an).
b. Deduce that if f vanishes for all λ ̸= 0 ∈k, then fi(a0, . . . , an) = 0 for all i.
8. By dehomogenizing the deﬁning equations of the projective variety V, ﬁnd equations
for the indicated afﬁne varieties.
a. Let P2(R) have homogeneous coordinates (x : y : z) and let V = V(x2 + y2 −z2) ⊆
P2(R). Find equations for V ∩U0 and V ∩U2. (Here U0 is where x ̸= 0 and U2 is
where z ̸= 0.) Sketch each of these curves and think about what this says about the
projective variety V.
b. V = V(x0x2 −x3x4, x2
0x3 −x1x2
2) ⊆P4(k) and ﬁnd equations for the afﬁne variety
V ∩U0 ⊆k4. Do the same for V ∩U3.
9. Let V = V( f1, . . . , fs) be a projective variety deﬁned by homogeneous polynomials
fi ∈k[x0, . . . , xn]. Show that the subset W = V ∩Ui, can be identiﬁed with the afﬁne
variety V(g1, . . . , gs) ⊆kn deﬁned by the dehomogenized polynomials
gj(x1, . . . , xi, xi+1, . . . , xn) = fj(x1, . . . , xi, 1, xi+1, . . . , xn),
j = 1, . . . , s.
Hint: Follow the proof of Proposition 6, using Corollary 3.
10. Prove Proposition 7.
11. Using part (iv) of Proposition 7, show that if f ∈k[x1, . . . , xn] and F ∈k[x0, . . . , xn]
is any homogeneous polynomial satisfying F(1, x1, . . . , xn) = f (x1, . . . , xn), then F =
xe
0 f h for some e ≥0.
12. What happens if we apply the homogenization process of Proposition 7 to a polynomial
g that is itself homogeneous?
13. In Example 8, we were led to consider the variety W′ = V(z2 −x3 + xz2) ⊆k2. Show
that (x, z) = (0, 0) is a singular point of W′. Hint: Use Deﬁnition 3 from Chapter 3, §4.
14. For each of the following afﬁne varieties W, apply the homogenization process given in
Proposition 7 to write W = V ∩U0, where V is a projective variety. In each case identify
V \ W = V ∩H, where H is the hyperplane at inﬁnity.
a. W = V(y2 −x3 −ax −b) ⊆R2, a, b ∈R. Is the point V ∩H singular here? Hint:
Let the homogeneous coordinates on P2(R) be (z : x : y), so that U0 is where z ̸= 0.
b. W = V(x1x3 −x2
2, x2
1 −x2) ⊆R3. Is there an extra component at inﬁnity here?
c. W = V(x2
3 −x2
1 −x2
2) ⊆R3.
15. From Example 9, consider the twisted cubic W = V(x2 −x2
1, x3 −x3
1) ⊆R3.

§2 Projective Space and Projective Varieties
405
a. If we parametrize W by (t, t2, t3) in R3, show that as t →±∞, the point (1 : t : t2 : t3)
in P3(R) approaches (0 : 0 : 0 : 1). Thus, we expect W to have one point at inﬁnity.
b. Now consider the projective variety
V′ = V(x2x0 −x2
1, x3x2
0 −x3
1, x1x3 −x2
2) ⊆P3(R).
Show that V′ ∩U0 = W and that V′ ∩H = {(0 : 0 : 0 : 1)}.
c. Let V = V(x2x0 −x2
1, x3x2
0 −x3
1) be as in Example 9. Prove that V = V′ ∪V(x0, x1).
This shows that V is a union of two proper projective varieties.
16. A homogeneous polynomial f ∈k[x0, . . . , xn] can also be used to deﬁne the afﬁne variety
C = Va( f ) in kn+1, where the subscript denotes we are working in afﬁne space. We call
C the afﬁne cone over the projective variety V = V( f ) ⊆Pn(k). We will see why this
is so in this exercise.
a. Show that if C contains the point P ̸= (0, . . . , 0), then C contains the whole line
through the origin in kn+1 spanned by P.
b. A point P ∈kn+1 \ {0} gives homogeneous coordinates for a point p ∈Pn(k).
Show that p is in the projective variety V if and only if the line through the origin
determined by P is contained in C. Hint: See (1) and Exercise 1.
c. Deduce that C is the union of the collection of lines through the origin in kn+1 corre-
sponding to the points in V via (1). This explains the reason for the "cone" terminol-
ogy since an ordinary cone is also a union of lines through the origin. Such a cone is
given by part (c) of Exercise 14.
17. Homogeneous polynomials satisfy an important relation known as Euler's Formula. Let
f ∈k[x0, . . . , xn] be homogeneous of total degree d. Then Euler's Formula states that
n

i=0
xi · ∂f
∂xi = d · f .
a. Verify Euler's Formula for the homogeneous polynomial f = x3
0 −x1x2
2 + 2x1x2
3.
b. Prove Euler's Formula (in the case k = R) by considering f (λx0, . . . , λxn) as a
function of λ and differentiating with respect to λ using the chain rule.
18. In this exercise, we will consider the set of hyperplanes in Pn(k) in greater detail.
a. Show that two homogeneous linear polynomials,
0 = a0x0 + · · · + anxn,
0 = b0x0 + · · · + bnxn,
deﬁne the same hyperplane in Pn(k) if and only if there is λ ̸= 0 in k such that
bi = λai for all i = 0, . . . , n. Hint: Generalize the argument given for Exercise 11
of §1.
b. Show that the map sending the hyperplane with equation a0x0 + · · · + anxn = 0 to
the vector (a0, . . . , an) gives a one-to-one correspondence
φ : {hyperplanes in Pn(k)} →(kn+1 \ {0})/∼,
where ∼is the equivalence relation of Deﬁnition 1. The set on the left is called the
dual projective space and is denoted Pn(k)∨. Geometrically, the points of Pn(k)∨are
hyperplanes in Pn(k).
c. Describe the subset of Pn(k)∨corresponding to the hyperplanes containing p =
(1 : 0 : · · · : 0).
19. Let k be an algebraically closed ﬁeld (C, for example). Show that every homogeneous
polynomial f (x0, x1) in two variables with coefﬁcients in k can be completely factored
into linear homogeneous polynomials in k[x0, x1]:

406
Chapter 8 Projective Algebraic Geometry
f (x0, x1) =
d

i=1
(aix0 + bix1),
where d is the total degree of f . Hint: First dehomogenize f .
20. In §4 of Chapter 5, we introduced the pencil deﬁned by two hypersurfaces V = V( f ),
W = V(g). The elements of the pencil were the hypersurfaces V( f + cg) for c ∈k.
Setting c = 0, we obtain V as an element of the pencil. However, W is not (usually) an
element of the pencil when it is deﬁned in this way. To include W in the pencil, we can
proceed as follows.
a. Let (a : b) be homogeneous coordinates in P1(k). Show that V(af + bg) is well-
deﬁned in the sense that all homogeneous coordinates (a : b) for a given point in
P1(k) yield the same variety V(af + bg). Thus, we obtain a family of varieties
parametrized by P1(k), which is also called the pencil of varieties deﬁned by V
and W.
b. Show that both V and W are contained in the pencil V(af + bg).
c. Let k = C. Show that every afﬁne curve V( f ) ⊆C2 deﬁned by a polynomial f of
total degree d is contained in a pencil of curves V(aF + bG) parametrized by P1(C),
where V(F) is a union of lines and G is a polynomial of degree strictly less than d.
Hint: Consider the homogeneous components of f . Exercise 19 will be useful.
21. When we have a curve parametrized by t ∈k, there are many situations where we want
to let t →∞. Since P1(k) = k ∪{∞}, this suggests that we should let our parameter
space be P1(k). Here are two examples of how this works.
a. Consider the parametrization (x, y) = ((1+t2)/(1−t2), 2t/(1−t2)) of the hyperbola
x2 −y2 = 1 in R2. To make this projective, we ﬁrst work in P2(R) and write the
parametrization as
((1 + t2)/(1 −t2) : 2t/(1 −t2) : 1) = (1 + t2 : 2t : 1 −t2)
(see Exercise 3 of §1). The next step is to make t projective. Given (a : b) ∈P1(R),
we can write it as (1 : t) = (1 : b/a) provided a ̸= 0. Now substitute t = b/a into the
right-hand side and clear denominators. Explain why this gives a well-deﬁned map
P1(R) →P2(R).
b. The twisted cubic in R3 is parametrized by (t, t2, t3). Apply the method of part (a) to
obtain a projective parametrization P1(R) →P3(R) and show that the image of this
map is precisely the projective variety V′ from Example 9.
§3
The Projective Algebra-Geometry Dictionary
In this section, we will study the algebra-geometry dictionary for projective vari-
eties. Our goal is to generalize the theorems from Chapter 4 concerning the V and I
correspondences to the projective case, and, in particular, we will prove a projective
version of the Nullstellensatz.
To begin, we note one difference between the afﬁne and projective cases on
the algebraic side of the dictionary. Namely, in Deﬁnition 5 of §2, we introduced
projective varieties as the common zeros of collections of homogeneous poly-
nomials. But being homogeneous is not preserved under the sum operation in
k[x0, . . . , xn]. For example, if we add two homogeneous polynomials of different
total degrees, the sum will never be homogeneous. Thus, if we form the ideal

§3 The Projective Algebra-Geometry Dictionary
407
I = ⟨f1, . . . , fs⟩⊆k[x0, . . . , xn] generated by a collection of homogeneous poly-
nomials, I will contain many non-homogeneous polynomials and these would not
be candidates for the deﬁning equations of a projective variety.
Nevertheless, each element of I vanishes on all homogeneous coordinates of ev-
ery point of V = V( f1, . . . , fs). This follows because each g ∈I has the form
(1)
g =
s

j=1
Aj fj
for some Aj ∈k[x0, . . . , xn]. Substituting any homogeneous coordinates of a point
in V into g will yield zero since each fi is zero there.
A more important observation concerns the homogeneous components of g. Sup-
pose we expand each Aj as the sum of its homogeneous components:
Aj =
d

i=1
Aji.
If we substitute these expressions into (1) and collect terms of the same total degree,
it can be shown that the homogeneous components of g also lie in the ideal I =
⟨f1, . . . , fs⟩. You will prove this claim in Exercise 2.
Thus, although I contains nonhomogeneous elements g, we see that I also con-
tains the homogeneous components of g. This observation motivates the following
deﬁnition of a special class of ideals in k[x0, . . . , xn].
Deﬁnition 1. An ideal I in k[x0, . . . , xn] is said to be homogeneous if for each f ∈I,
the homogeneous components fi of f are in I as well.
Most ideals do not have this property. For instance, let I = ⟨y −x2⟩⊆k[x, y].
The homogeneous components of f = y −x2 are f1 = y and f2 = −x2. Neither
of these polynomials is in I since neither is a multiple of y −x2. Hence, I is not
a homogeneous ideal. However, we have the following useful characterization of
when an ideal is homogeneous.
Theorem 2. Let I ⊆k[x0, . . . , xn] be an ideal. Then the following are equivalent:
(i) I is a homogeneous ideal of k[x0, . . . , xn].
(ii) I = ⟨f1, . . . , fs⟩, where f1, . . . , fs are homogeneous polynomials.
(iii) A reduced Gröbner basis of I (with respect to any monomial ordering) consists
of homogeneous polynomials.
Proof. The proof of (ii) ⇒(i) was sketched above (see also Exercise 2). To
prove (i) ⇒(ii), let I be a homogeneous ideal. By the Hilbert Basis Theorem,
we have I = ⟨F1, . . . , Ft⟩for some polynomials Fj ∈k[x0, . . . , xn] (not necessar-
ily homogeneous). If we write Fj as the sum of its homogeneous components, say
Fj = 
i Fji, then each Fji ∈I since I is homogeneous. Let I′ be the ideal generated
by the homogeneous polynomials Fji. Then I ⊆I′ since each Fj is a sum of gener-
ators of I′. On the other hand, I′ ⊆I since each of the homogeneous components

408
Chapter 8 Projective Algebraic Geometry
of Fj is in I. This proves I = I′ and it follows that I has a basis of homogeneous
polynomials. Finally, the equivalence (ii) ⇔(iii) will be covered in Exercise 3.
□
As a result of Theorem 2, for any homogeneous ideal I ⊆k[x0, . . . , xn] we may
deﬁne
V(I) = {p ∈Pn(k) | f(p) = 0 for all f ∈I},
as in the afﬁne case. We can prove that V(I) is a projective variety as follows.
Proposition 3. Let I ⊆k[x0, . . . , xn] be a homogeneous ideal and suppose that I =
⟨f1, . . . , fs⟩, where f1, . . . , fs are homogeneous. Then
V(I) = V( f1, . . . , fs),
so that V(I) is a projective variety.
Proof. We leave the easy proof as an exercise.
□
One way to create a homogeneous ideal is to consider the ideal generated by the
deﬁning equations of a projective variety. But there is another way that a projective
variety can give us a homogeneous ideal.
Proposition 4. Let V ⊆Pn(k) be a projective variety and let
I(V) = {f ∈k[x0, . . . , xn] | f(a0, . . . , an) = 0 for all (a0 : · · · : an) ∈V}.
(This means that f must be zero for all homogeneous coordinates of all points in V.)
If k is inﬁnite, then I(V) is a homogeneous ideal in k[x0, . . . , xn].
Proof. The set I(V) is closed under sums and closed under products by elements
of k[x0, . . . , xn] by an argument exactly parallel to the one for the afﬁne case. Thus,
I(V) is an ideal in k[x0, . . . , xn]. Now take f ∈I(V) and a point p ∈V. By as-
sumption, f vanishes at all homogeneous coordinates (a0 : · · · : an) of p. Since k is
inﬁnite, Exercise 7 of §2 implies that each homogeneous component fi of f vanishes
at (a0 : · · · : an). This shows that fi ∈I(V) and, hence, I(V) is homogeneous.
□
Thus, we have all the ingredients of a dictionary relating projective varieties in
Pn(k) and homogeneous ideals in k[x0, . . . , xn]. The following theorem is a direct
generalization of part (i) of Theorem 7 of Chapter 4, §2 (the afﬁne ideal-variety
correspondence).
Theorem 5. Let k be an inﬁnite ﬁeld. Then the maps
projective varieties
I
−→homogeneous ideals
and
homogeneous ideals
V
−→projective varieties
are inclusion-reversing. Furthermore, for any projective variety V, we have

§3 The Projective Algebra-Geometry Dictionary
409
V(I(V)) = V.
so that I is always one-to-one.
Proof. The proof is the same as in the afﬁne case.
□
To illustrate the use of this theorem, let us show that every projective variety can
be decomposed to irreducible components. As in the afﬁne case, a variety V ⊆Pn(k)
is irreducible if it cannot be written as a union of two strictly smaller projective
varieties.
Theorem 6. Let k be an inﬁnite ﬁeld.
(i) Given a descending chain of projective varieties in Pn(k),
V1 ⊇V2 ⊇V3 ⊇· · · ,
there is an integer N such that VN = VN+1 = · · · .
(ii) Every projective variety V ⊆Pn(k) can be written uniquely as a ﬁnite union of
irreducible projective varieties
V = V1 ∪· · · ∪Vm,
where Vi ̸⊆Vj for i ̸= j.
Proof. Since I is inclusion-reversing, we get the ascending chain of homogeneous
ideals
I(V1) ⊆I(V2) ⊆I(V3) ⊆· · ·
in k[x0, . . . , xn]. Then the ascending chain condition (Theorem 7 of Chapter 2, §5)
implies that I(VN) = I(VN+1) = · · · for some N. By Theorem 5, I is one-to-one
and (i) follows immediately.
As in the afﬁne case, (ii) is an immediate consequence of (i). See Theorems 2
and 4 of Chapter 4, §6.
□
The relation between operations such as sums, products, and intersections of
homogeneous ideals and the corresponding operations on projective varieties is also
the same as in afﬁne space. We will consider these topics in more detail in the
exercises below.
We deﬁne the radical of a homogeneous ideal as usual:
√
I = {f ∈k[x0, . . . , xn] | f m ∈I for some m ≥1}.
As we might hope, the radical of a homogeneousideal is always itself homogeneous.
Proposition 7. Let I ⊆k[x0, . . . , xn] be a homogeneous ideal. Then
√
I is also a
homogeneous ideal.
Proof. If f ∈
√
I, then f m ∈I for some m ≥1. If f ̸= 0, decompose f into its
homogeneous components

410
Chapter 8 Projective Algebraic Geometry
f =

i
fi = fmax +

i<max
fi,
where fmax is the nonzero homogeneous component of maximal total degree in f.
Expanding the power f m, it is easy to show that
( f m)max = ( fmax)m.
Since I is a homogeneous ideal, ( f m)max ∈I. Hence, ( fmax)m ∈I, which shows that
fmax ∈
√
I.
If we let g = f −fmax ∈
√
I and repeat the argument, we get gmax ∈
√
I. But gmax is
also one of the homogeneous components of f. Applying this reasoning repeatedly
shows that all homogeneous components of f are in
√
I. Since this is true for all
f ∈
√
I, Deﬁnition 1 implies that
√
I is a homogeneous ideal.
□
The ﬁnal part of the algebra-geometry dictionary concerns what happens over
an algebraically closed ﬁeld k. Here, we expect an especially close relation be-
tween projective varieties and homogeneous ideals. In the afﬁne case, the link was
provided by two theorems proved in Chapter 4, the Weak Nullstellensatz and the
Strong Nullstellensatz. Let us recall what these theorems tell us about an ideal
I ⊆k[x1, . . . , xn]:
• (The Weak Nullstellensatz) Va(I) = ∅in kn ⇐⇒I = k[x1, . . . , xn].
• (The Strong Nullstellensatz)
√
I = Ia(Va(I)) in k[x1, . . . , xn].
(To prevent confusion, we use Ia and Va for the afﬁne versions of I and V.) It is
natural to ask if these results extend to projective varieties and homogeneous ideals.
The answer, surprisingly, is no. In particular, the Weak Nullstellensatz fails for
certain homogeneous ideals. To see how this can happen, consider the ideal I =
⟨x0, . . . , xn⟩⊆C[x0, . . . , xn]. Then V(I) ⊆Pn(C) is deﬁned by the equations
x0 = · · · = xn = 0,
which have no solutions in Pn(C) since we never allow all homogeneouscoordinates
to vanish simultaneously. It follows that V(I) = ∅, yet I ̸= C[x0, . . . , xn].
Fortunately, I = ⟨x0, . . . , xn⟩is one of the few ideals for which V(I) = ∅. The
following projective version of the Weak Nullstellensatz describes all homogeneous
ideals with no projective solutions.
Theorem 8 (The Projective Weak Nullstellensatz). Let k be algebraically closed
and let I be a homogeneous ideal in k[x0, . . . , xn]. Then the following are equivalent:
(i) V(I) ⊆Pn(k) is empty.
(ii) Let G be a reduced Gröbner basis for I (with respect to some monomial
ordering). Then for each 0 ≤i ≤n, there is g ∈G such that LT(g) is a
nonnegative power of xi.
(iii) For each 0 ≤i ≤n, there is an integer mi ≥0 such that xmi
i
∈I.
(iv) There is some r ≥1 such that ⟨x0, . . . , xn⟩r ⊆I.
(v) I :⟨x0, . . . , xn⟩∞= k[x1, . . . , xn].

§3 The Projective Algebra-Geometry Dictionary
411
Proof. The ideal I gives us the projective variety V = V(I) ⊆Pn(k). In this proof,
we will also work with the afﬁne variety CV = Va(I) ⊆kn+1. Note that CV uses the
same ideal I, but now we look for solutions in the afﬁne space kn+1. We call CV the
afﬁne cone of V. If we interpret points in Pn(k) as lines through the origin in kn+1,
then CV is the union of the lines determined by the points of V (see Exercise 16 of
§2 for the details of how this works). In particular, CV contains all homogeneous
coordinates of the points in V.
To prove (ii) ⇒(i), ﬁrst suppose that we have a Gröbner basis where, for each i,
there is g ∈G with LT(g) = xmi
i for some mi ≥0. Then Theorem 6 of Chapter 5, §3
implies that CV is a ﬁnite set. But suppose there is a point p ∈V. Then all homoge-
neous coordinates of p lie in CV. If we write these in the form λ(a0, . . . , an), we see
that there are inﬁnitely many since k is algebraically closed and hence inﬁnite. This
contradiction shows that V = V(I) = ∅.
Turning to (iii) ⇒(ii), let G be a reduced Gröbner basis for I. Then xmi
i
∈I
implies that the leading term of some g ∈G divides xmi
i , so that LT(g) must be a
power of xi.
For the remainder of the proof, let J = ⟨x0, . . . , xn⟩. The proof of (iv) ⇒(iii) is
obvious since Jr ⊆I implies that xr
i ∈I for all i. For (v) ⇒(iv), we use results about
ideal quotients and saturations from Chapter 4 to obtain
I : J∞= k[x1, . . . , xn] =⇒J ⊆
√
I =⇒Jr ⊆I for some r > 0.
The ﬁrst implication uses Proposition 12 of Chapter 4, §4, and the second uses
Exercise 12 of Chapter 4, §3.
Finally, since Va(J) = {0}, (i) ⇔(v) follows from the equivalences
V(I) = ∅⇐⇒Va(I) ⊆{0} ⇐⇒Va(I) \ Va(J) = ∅
⇐⇒Va(I) \ Va(J) = ∅
⇐⇒Va(I : J∞) = ∅
⇐⇒I : J∞= k[x1, . . . , xn],
where the fourth equivalence uses Theorem 10 of Chapter 4, §4 and the ﬁfth uses
the Nullstellensatz. This completes the proof of the theorem.
□
From part (ii) of the theorem, we get an algorithm for determining if a homo-
geneous ideal has projective solutions over an algebraically closed ﬁeld. In Exer-
cise 10, we will discuss other conditions equivalent to V(I) = ∅in Pn(k).
Once we exclude the ideals described in Theorem 8, we get the following form
of the Nullstellensatz for projective varieties.
Theorem 9 (The Projective Strong Nullstellensatz). Let k be an algebraically
closed ﬁeld and let I be a homogeneous ideal in k[x0, . . . , xn]. If V = V(I) is a
nonempty projective variety in Pn(k), then we have
I(V(I)) =
√
I.

412
Chapter 8 Projective Algebraic Geometry
Proof. As in the proof of Theorem 8, we will work with the projective variety V =
V(I) ⊆Pn(k) and its afﬁne cone CV = Va(I) ⊆kn+1. We ﬁrst claim that
(2)
Ia(CV) = I(V)
when V ̸= ∅. To see this, suppose that f ∈Ia(CV). Given p ∈V, any homogeneous
coordinates of p give a point in CV. Since f vanishes on CV, it follows that f(p) = 0.
By deﬁnition, this implies f ∈I(V). Conversely, take f ∈I(V). Since any nonzero
point of CV gives homogeneous coordinates for a point in V, it follows that f van-
ishes on CV \ {0}. It remains to show that f vanishes at the origin. Since I(V) is a
homogeneous ideal, we know that the homogeneous components fi of f also vanish
on V. In particular, the constant term of f, which is the homogeneous component f0
of total degree 0, must vanish on V. Since V ̸= ∅, this forces f0 = 0, which means
that f vanishes at the origin. Hence, f ∈Ia(CV) and (2) is proved.
By the afﬁne form of the Strong Nullstellensatz, we know that
√
I = Ia(Va(I)).
Then, using (2), we obtain
√
I = Ia(Va(I)) = Ia(CV) = I(V) = I(V(I)).
which completes the proof of the theorem.
□
Now that we have the Nullstellensatz, we can complete the projective ideal-
variety correspondence begun in Theorem 5. A radical homogeneous ideal in
k[x0, . . . , xn] is a homogeneous ideal satisfying
√
I = I. As in the afﬁne case, we
have a one-to-one correspondence between projective varieties and radical homoge-
neous ideals, provided we exclude the cases
√
I = ⟨x0, . . . , xn⟩and
√
I = ⟨1⟩.
Theorem 10. Let k be an algebraically closed ﬁeld. If we restrict the correspon-
dences of Theorem 5 to nonempty projective varieties and radical homogeneous
ideals properly contained in ⟨x0, . . . , xn⟩, then
{nonempty projective varieties}
I
−→

radical homogeneous ideals
properly contained in⟨x0, . . . , xn⟩

and

radical homogeneous ideals
properly contained in⟨x0, . . . , xn⟩

V
−→{nonempty projective varieties}
are inclusion-reversing bijections which are inverses of each other.
Proof. First, it is an easy consequence of Theorem 8 that the only radical homoge-
neous ideals I with V(I) = ∅are ⟨x0, . . . , xn⟩and k[x0, . . . , xn]. See Exercise 10 for
the details. A second observation is that if I is a homogeneous ideal different from
k[x0, . . . , xn], then I ⊆⟨x0, . . . , xn⟩. This will also be covered in Exercise 9.
These observations show that the radical homogeneous ideals with V(I) ̸= ∅are
precisely those which satisfy I ⊊⟨x0, . . . , xn⟩. Then the rest of the theorem follows
as in the afﬁne case, using Theorem 9.
□

§3 The Projective Algebra-Geometry Dictionary
413
We also have a correspondence between irreducible projective varieties and ho-
mogeneous prime ideals, which will be studied in the exercises. In the exercises we
will also explore the ﬁeld of rational functions of an irreducible projective variety.
EXERCISES FOR §3
1. In this exercise, you will study the question of determining when a principal ideal I = ⟨f ⟩
is homogeneous by elementary methods.
a. Show that I = ⟨x2y −x3⟩is a homogeneous ideal in k[x, y] without appealing to
Theorem 2. Hint: Each element of I has the form g = A · (x2y −x3). Write A as
the sum of its homogeneous components and use this to determine the homogeneous
components of g.
b. Show that ⟨f ⟩⊆k[x0, . . . , xn] is a homogeneous ideal if and only if f is a homoge-
neous polynomial without using Theorem 2.
2. This exercise gives some useful properties of the homogeneous components of polyno-
mials.
a. Show that if f = 
i fi and g = 
i gi are the expansions of two polynomials as the
sums of their homogeneous components, then f = g if and only if fi = gi for all i.
b. Show that if f = 
i fi and g = 
j gj are the expansions of two polynomials as the
sums of their homogeneous components, then the homogeneous components hℓof
the product h = f · g are given by hℓ= 
i+j=ℓfi · gj.
c. Use parts (a) and (b) to carry out the proof (sketched in the text) of the implication
(ii) ⇒(i) from Theorem 2.
3. This exercise will study how the algorithms of Chapter 2 interact with homogeneous
polynomials.
a. Suppose we use the division algorithm to divide a homogeneous polynomial f by
homogeneous polynomials f1, . . . , fs. This gives an expression of the form f = a1 f1
+· · ·+as fs +r. Prove that the quotients a1, . . . , as and remainder r are homogeneous
polynomials (possibly zero). What is the total degree of r?
b. If f , g are homogeneous polynomials, prove that the S-polynomial S( f , g) is homo-
geneous.
c. By analyzing the Buchberger algorithm, show that a homogeneous ideal has a homo-
geneous Gröbner basis.
d. Prove the implication (ii) ⇔(iii) of Theorem 2.
4. Suppose that an ideal I ⊆k[x0, . . . , xn] has a basis G consisting of homogeneous poly-
nomials.
a. Prove that G is a Gröbner basis for I with respect to lex order if and only if it is a
Gröbner basis for I with respect to grlex (assuming that the variables are ordered the
same way).
b. Conclude that, for a homogeneous ideal, the reduced Gröbner basis for lex and grlex
are the same.
5. Prove Proposition 3.
6. In this exercise we study the algebraic operations on ideals introduced in Chapter 4 for
homogeneous ideals. Let I1, . . . , Il be homogeneous ideals in k[x0, . . . , xn].
a. Show that the ideal sum I1 + · · · + Il is also homogeneous. Hint: Use Theorem 2.
b. Show that the intersection I1 ∩· · · ∩Il is also a homogeneous ideal.
c. Show that the ideal product I1 · · · Il is a homogeneous ideal.
7. The interaction between the algebraic operations on ideals in Exercise 6 and the corre-
sponding operations on projective varieties is the same as in the afﬁne case. Let I1, . . . , Il
be homogeneous ideals in k[x0, . . . , xn] and let Vi = V(Ii) be the corresponding projec-
tive variety in Pn(k).
a. Show that V(I1 + · · · + Il) = "l
i=1 Vi.

414
Chapter 8 Projective Algebraic Geometry
b. Show that V(I1 ∩· · · ∩Il) = V(I1 · · · Il) = #l
i=1 Vi.
8. Let f1, . . . , fs be homogeneous polynomials of total degrees d1 < d2 ≤· · · ≤ds and let
I = ⟨f1, . . . , fs⟩⊆k[x0, . . . , xn].
a. Show that if g is another homogeneous polynomial of degree d1 in I, then g must be
a constant multiple of f1. Hint: Use parts (a) and (b) of Exercise 2.
b. More generally, show that if the total degree of g is d, then g must be an element of
the ideal Id = ⟨fi | deg( fi) ≤d⟩⊆I.
9. This exercise will study some properties of the ideal I0 = ⟨x0, . . . , xn⟩⊆k[x0, . . . , xn].
a. Show that every proper homogeneous ideal in k[x0, . . . , xn] is contained in I0.
b. Show that the r-th power Ir
0 is the ideal generated by the collection of monomials in
k[x0, . . . , xn] of total degree exactly r and deduce that every homogeneous polynomial
of degree ≥r is in Ir
0.
c. Let V = V(I0) ⊆Pn(k) and CV = Va(I0) ⊆kn+1. Show that Ia(CV) ̸= I(V), and
explain why this does not contradict equation (2) in the text.
10. Given a homogeneous ideal I ⊆k[x0, . . . , xn], where k is algebraically closed, prove that
V(I) = ∅in Pn(k) is equivalent to either of the following two conditions:
(i) There is an r ≥1 such that every homogeneous polynomial of total degree ≥r is
contained in I.
(ii) The radical of I is either ⟨x0, . . . , xn⟩or k[x0, . . . , xn].
Hint: For (i), use Exercise 9, and for (ii), note that the inclusion ⟨x0, . . . , xn⟩⊆
√
I
appears in the proof of Theorem 8.
11. A homogeneous ideal is said to be prime if it is prime as an ideal in k[x0, . . . , xn].
a. Show that a homogeneous ideal I ⊆k[x0, . . . , xn] is prime if and only if whenever
the product of two homogeneous polynomials F, G satisﬁes F · G ∈I, then F ∈I or
G ∈I.
b. Let k be algebraically closed. Let I be a homogeneous ideal. Show that the projective
variety V(I) is irreducible if I is prime. Also, when I is radical, prove that the con-
verse holds, i.e., that I is prime if V(I) is irreducible. Hint: Consider the proof of the
corresponding statement in the afﬁne case (Proposition 3 of Chapter 4, §5).
c. Let k be algebraically closed. Show that the mappings V and I induce one-to-one cor-
respondence between homogeneous prime ideals in k[x0, . . . , xn] properly contained
in ⟨x0, . . . , xn⟩and nonempty irreducible projective varieties in Pn(k).
12. Prove that a homogeneous prime ideal is a radical ideal in k[x0, . . . , xn].
13. In this exercise, we will show how to deﬁne the ﬁeld of rational functions on an ir-
reducible projective variety V ⊆Pn(k). If we take a homogeneous polynomial f ∈
k[x0, . . . , xn], then f does not give a well-deﬁned function on V. To see why, let p =
(a0 : · · · : an) ∈V. Then we also have p = (λa0 : · · · : λan) for any λ ∈k \ {0}, and
f (λa0, . . . , λan) = λd f (a0, . . . , an),
where d is the total degree of f .
a. Explain why the above equation makes it impossible for us to deﬁne f (p) as a single-
valued function on V.
b. If g ∈k[x0, . . . , xn] is also homogeneous of total degree d and g /∈I(V), then show
that φ = f /g is a well-deﬁned function on the nonempty set V \ V ∩V(g) ⊆V.
c. We say that φ = f /g and φ′ = f ′/g′ are equivalent on V, written φ ∼φ′, provided
that there is a proper variety W ⊆V such that φ and φ′ are deﬁned and equal on
V \ W. Prove that ∼is an equivalence relation. An equivalence class for ∼is called
a rational function on V, and the set of all equivalence classes is denoted k(V). Hint:
Your proof will use the fact that V is irreducible.
d. Show that addition and multiplication of equivalence classes is well-deﬁned and
makes k(V) into a ﬁeld called the ﬁeld of rational functions of the projective vari-
ety V.

§4 The Projective Closure of an Afﬁne Variety
415
e. Let Ui be the afﬁne part of Pn(k) where xi = 1, and assume that V ∩Ui ̸= ∅. Then
V ∩Ui is an irreducible afﬁne variety in Ui ∼= kn. Show that k(V) is isomorphic to
the ﬁeld k(V ∩Ui) of rational functions on the afﬁne variety V ∩Ui. Hint: You can
assume i = 0. What do you get when you set x0 = 1 in the quotient f /g considered
in part (b)?
§4
The Projective Closure of an Afﬁne Variety
In §2, we showed that any afﬁne variety could be regarded as the afﬁne portion of a
projective variety. Since this can be done in more than one way (see Example 9 of
§2), we would like to ﬁnd the smallest projective variety containing a given afﬁne
variety. As we will see, there is an algorithmic way to do this.
Given homogeneous coordinates x0, . . . , xn on Pn(k), we have the subset U0 ⊆
Pn(k) deﬁned by x0 ̸= 0. If we identify U0 with kn using Proposition 2 of §2, then
we get coordinates x1, . . . , xn on kn. As in §3, we will use Ia and Va for the afﬁne
versions of I and V.
We ﬁrst discuss how to homogenize an ideal of k[x1, . . . , xn]. If we are given an
arbitrary ideal I ⊆k[x1, . . . , xn], the standard way to produce a homogeneous ideal
Ih ⊆k[x0, . . . , xn] is as follows.
Deﬁnition 1. Let I be an ideal in k[x1, . . . , xn]. We deﬁne the homogenization of I
to be the ideal
Ih = ⟨f h | f ∈I⟩⊆k[x0, . . . , xn],
where f h is the homogenization of f as in Proposition 7 of §2.
Naturally enough, we have the following result.
Proposition 2. For any ideal I ⊆k[x1, . . . , xn], the homogenization Ih is a homoge-
neous ideal in k[x0, . . . , xn].
Proof. See Exercise 1.
□
Deﬁnition 1 is not entirely satisfying as it stands because it does not give us a
ﬁnite generating set for the ideal Ih. There is a subtle point here. Given a particular
ﬁnite generating set f1, . . . , fs for I ⊆k[x1, . . . , xn], it is always true that ⟨f h
1 , . . . , f h
s ⟩
is a homogeneous ideal contained in Ih. However, as the following example shows,
Ih can be strictly larger than ⟨f h
1 , . . . , f h
s ⟩.
Example 3. Consider I = ⟨f1, f2⟩= ⟨x2 −x2
1, x3 −x3
1⟩, the ideal of the afﬁne twisted
cubic in R3. If we homogenize f1, f2, then we get the ideal J = ⟨x2x0 −x2
1, x3x2
0 −x3
1⟩
in R[x0, x1, x2, x3]. We claim that J ̸= Ih. To prove this, consider the polynomial
f3 = f2 −x1 f1 = x3 −x3
1 −x1(x2 −x2
1) = x3 −x1x2 ∈I.
Then f h
3 = x0x3 −x1x2 is a homogeneous polynomial of degree 2 in Ih. Since the
generators of J are also homogeneous, of degrees 2 and 3, respectively, if we had

416
Chapter 8 Projective Algebraic Geometry
an equation f h
3 = A1 f h
1 + A2 f h
2 , then using the expansions of A1 and A2 into ho-
mogeneous components, we would see that f h
3 was a constant multiple of f h
1 . (See
Exercise 8 of §3 for a general statement along these lines.) Since this is clearly false,
we have f h
3 ̸∈J, and thus, J ̸= Ih.
Hence, we may ask whether there is some reasonable method for computing a
ﬁnite generating set for the ideal Ih. The answer is given in the following theorem.
A graded monomial order in k[x1, . . . , xn] is one that orders ﬁrst by total degree:
xα > xβ
whenever |α| > |β|. Note that grlex and grevlex are graded orders, whereas lex is
not.
Theorem 4. Let I be an ideal in k[x1, . . . , xn] and let G = {g1, . . . , gt} be a Gröbner
basis for I with respect to a graded monomial order in k[x1, . . . , xn]. Then Gh =
{gh
1, . . . , gh
t } is a basis for Ih ⊆k[x0, . . . , xn].
Proof. We will prove the theorem by showing the stronger statement that Gh is
actually a Gröbner basis for Ih with respect to an appropriate monomial order in
k[x0, . . . , xn].
Every monomial in k[x0, . . . , xn] can be written
xα1
1 · · · xαn
n xd
0 = xαxd
0,
where xα contains no x0 factors. Then we can extend the graded order > on mono-
mials in k[x1, . . . , xn] to a monomial order >h in k[x0, . . . , xn] as follows:
xαxd
0 >h xβxe
0 ⇐⇒xα > xβ
or
xα = xβ and d > e.
In Exercise 2, you will show that this deﬁnes a monomial order in k[x0, . . . , xn]. Note
that under this ordering, we have xi >h x0 for all i ≥1.
For us, the most important property of the order >h is given in the following
lemma.
Lemma 5. If f ∈k[x1, . . . , xn] and > is a graded order on k[x1, . . . , xn], then
LM>h( f h) = LM>( f).
Proof of Lemma. Since > is a graded order, for any f ∈k[x1, . . . , xn], LM>( f) is
one of the monomials xα appearing in the homogeneous component of f of maximal
total degree. When we homogenize, this term is unchanged. If xβxe
0 is any one of
the other monomials appearing in f h, then α > β. By the deﬁnition of >h, it follows
that xα >h xβxe
0. Hence, xα = LM>h( f h), and the lemma is proved.
□
We will now show that Gh forms a Gröbner basis for the ideal Ih with respect to
the monomial order >h. Each gh
i ∈Ih by deﬁnition. Thus, it sufﬁces to show that the
ideal of leading terms ⟨LT>h(Ih)⟩is generated by LT>h(Gh). To prove this, consider

§4 The Projective Closure of an Afﬁne Variety
417
F ∈Ih. Since Ih is a homogeneous ideal, each homogeneous component of F is in Ih
and, hence, we may assume that F is homogeneous. Because F ∈Ih, by deﬁnition
we have
(1)
F =

j
Aj f h
j ,
where Aj ∈k[x0, . . . , xn] and fj ∈I. We will let f = F(1, x1, . . . , xn) denote the
dehomogenization of F. Then setting x0 = 1 in (1) yields
f = F(1, x1, . . . , xn) =

j
Aj(1, x1, . . . , xn)f h
j (1, x1, . . . , xn)
=

j
Aj(1, x1, . . . , xn)fj
since f h
j (1, x1, . . . , xn) = fj(x1, . . . , xn) by part (iii) of Proposition 7 from §2. This
shows that f ∈I ⊆k[x1, . . . , xn]. If we homogenize f, then part (iv) of Proposition 7
in §2 implies that
F = xe
0 · f h
for some e ≥0. Thus,
(2)
LM>h(F) = xe
0 · LM>h( f h) = xe
0 · LM>( f),
where the last equality is by Lemma 5. Since G is a Gröbner basis for I, we know
that LM>( f) is divisible by some LM>(gi) = LM>h(gh
i ) (using Lemma 5 again).
Then (2) shows that LM>h(F) is divisible by LM>h(gh
i ), as desired. This completes
the proof of the theorem.
□
In Exercise 5, you will see that there is a more elegant formulation of Theorem 4
for the special case of grevlex order.
To illustrate the theorem, consider the ideal I = ⟨x2 −x2
1, x3 −x3
1⟩of the afﬁne
twisted cubic W ⊆R3 once again. Computing a Gröbner basis for I with respect to
grevlex order, we ﬁnd
G =
)
x2
1 −x2, x1x2 −x3, x1x3 −x2
2
*
.
By Theorem 4, the homogenizations of these polynomials generate Ih. Thus,
(3)
Ih =

x2
1 −x0x2, x1x2 −x0x3, x1x3 −x2
2

.
Note that this ideal gives us the projective variety V′ = V(Ih) ⊆P3(R) which we
discovered in Example 9 of §2.
For the remainder of this section, we will discuss the geometric meaning of the
homogenization of an ideal. We will begin by studying what happens when we
homogenize the ideal Ia(W) of all polynomials vanishing on an afﬁne variety W.
This leads to the following deﬁnition.

418
Chapter 8 Projective Algebraic Geometry
Deﬁnition 6. Given an afﬁne variety W ⊆kn, the projective closure of W is the
projective variety W = V(Ia(W)h) ⊆Pn(k), where Ia(W)h ⊆k[x0, . . . , xn] is the
homogenization of the ideal Ia(W) ⊆k[x1, . . . , xn] as in Deﬁnition 1.
The projective closure has the following important properties.
Proposition 7. Let W ⊆kn be an afﬁne variety and let W ⊆Pn(k) be its projective
closure. Then:
(i) W ∩U0 = W ∩kn = W.
(ii) W is the smallest projective variety in Pn(k) containing W.
(iii) If W is irreducible, then so is W.
(iv) No irreducible component of W lies in the hyperplane at inﬁnity V(x0) ⊆Pn(k).
Proof. (i) Let G be a Gröbner basis of Ia(W) with respect to a graded order on
k[x1, . . . , xn]. Then Theorem 4 implies that Ia(W)h = ⟨gh | g ∈G⟩. We know that
kn ∼= U0 is the subset of Pn(k), where x0 = 1. Thus, we have
W ∩U0 = V(gh | g ∈G) ∩U0 = Va(gh(1, x1, . . . , xn) | g ∈G).
Since gh(1, x1, . . . , xn) = g by part (iii) of Proposition 7 of §2, we get W ∩U0
= W.
(ii) We need to prove that if V is a projective variety containing W, then W ⊆V.
Let V = V(F1, . . . , Fs). Then Fi vanishes on V, so that its dehomogenization fi =
Fi(1, x1, . . . , xn) vanishes on W. Thus, fi ∈Ia(W) and, hence, f h
i ∈Ia(W)h. This
shows that f h
i vanishes on W = V(Ia(W)h). But part (iv) of Proposition 7 from §2
implies that Fi = xei
0 f h
i for some integer ei. Thus, Fi vanishes on W, and since this
is true for all i, it follows that W ⊆V.
The proof of (iii) will be left as an exercise. To prove (iv), let W = V1 ∪· · · ∪Vm
be the decomposition of W into irreducible components. Suppose that one of them,
say V1, was contained in the hyperplane at inﬁnity V(x0). Then
W = W ∩U0 = (V1 ∪· · · ∪Vm) ∩U0
= (V1 ∩U0) ∪((V2 ∪· · · ∪Vm) ∩U0)
= (V2 ∪· · · ∪Vm) ∩U0.
This shows that V2∪· · ·∪Vm is a projective variety containing W. By the minimality
of W, it follows that W = V2∪· · ·∪Vm and, hence, V1 ⊆V2∪· · ·∪Vm. We will leave
it as an exercise to show that this is impossible since V1 is an irreducible component
of W. This contradiction completes the proof.
□
For an example of how the projective closure works, consider the afﬁne twisted
cubic W ⊆R3. In §4 of Chapter 1, we proved that
Ia(W) = ⟨x2 −x2
1, x3 −x3
1⟩.
Using Theorem 4, we proved in (3) that

§4 The Projective Closure of an Afﬁne Variety
419
Ia(W)h = ⟨x2
1 −x0x2, x1x2 −x0x3, x1x3 −x2
2⟩.
It follows that the variety V′ = V(x2
1 −x0x2, x1x2 −x0x3, x1x3 −x2
2) discussed in
Example 9 of §2 is the projective closure of the afﬁne twisted cubic.
The main drawback of the deﬁnition of projective closure is that it requires that
we know Ia(W). It would be much more convenient if we could compute the projec-
tive closure directly from any deﬁning ideal of W. When the ﬁeld k is algebraically
closed, this can always be done.
Theorem 8. Let k be an algebraically closed ﬁeld, and let I ⊆k[x1, . . . , xn] be an
ideal. Then V(Ih) ⊆Pn(k) is the projective closure of Va(I) ⊆kn.
Proof. Let W = Va(I) ⊆kn and Z = V(Ih) ⊆Pn(k). The proof of part (i) of
Proposition 7 shows that Z is a projective variety containing W.
To prove that Z is the smallest such variety, we proceed as in part (ii) of Propo-
sition 7. Thus, let V = V(F1, . . . , Fs) be any projective variety containing W. As
in the earlier argument, the dehomogenization fi = Fi(1, x1, . . . , xn) is in Ia(W).
Since k is algebraically closed, the Nullstellensatz implies that Ia(W) =
√
I, so that
f m
i
∈I for some integer m. This tells us that
( f m
i )h ∈Ih
and, consequently, ( f m
i )h vanishes on Z. In the exercises, you will show that
( f m
i )h = ( f h
i )m,
and it follows that f h
i vanishes on Z. Then Fi = xei
0 f h
i shows that Fi is also zero on Z.
As in Proposition 7, we conclude that Z ⊆V.
This shows that Z is the smallest projective variety containing W. Since the pro-
jective closure W has the same property by Proposition 7, we see that Z = W.
□
If we combine Theorems 4 and 8, we get an algorithm for computing the
projective closure of an afﬁne variety over an algebraically closed ﬁeld k: given
W ⊆kn deﬁned by f1 = · · · = fs = 0, compute a Gröbner basis G of ⟨f1, . . . , fs⟩
with respect to a graded order, and then the projective closure in Pn(k) is deﬁned by
gh = 0 for g ∈G.
Unfortunately, Theorem 8 can fail over ﬁelds that are not algebraically closed.
Here is an example that shows what can go wrong.
Example 9. Consider I = ⟨x2
1 + x4
2⟩⊆R[x1, x2]. Then W = Va(I) consists of
the single point (0,0) in R2, and hence, the projective closure is the single point
W = {(1 : 0 :0)} ⊆P2(R) (since this is obviously the smallest projective variety
containing W). On the other hand, Ih = ⟨x2
1x2
0 + x4
2⟩, and it is easy to check that
V(Ih) = {(1 :0 :0), (0 : 1 :0)} ⊆P2(R).
This shows that V(Ih) is strictly larger than the projective closure of W = Va(I).

420
Chapter 8 Projective Algebraic Geometry
EXERCISES FOR §4
1. Prove Proposition 2.
2. Show that the order >h deﬁned in the proof of Theorem 4 is a monomial order on
k[x0, . . . , xn]. Hint: This can be done directly or by using the mixed orders deﬁned in
Exercise 9 of Chapter 2, §4.
3. Show by example that the conclusion of Theorem 4 is not true if we use an arbitrary
monomial order in k[x1, . . . , xn] and homogenize a Gröbner basis with respect to that
order. Hint: One example can be obtained using the ideal of the afﬁne twisted cubic and
computing a Gröbner basis with respect to a nongraded order.
4. Let > be a graded monomial order on k[x1, . . . , xn] and let >h be the order deﬁned in the
proof of Theorem 4. In the proof of the theorem, we showed that if G is a Gröbner basis
for I ⊆k[x1, . . . , xn] with respect to >, then Gh was a Gröbner basis for Ih with respect
to >h. In this exercise, we will explore other monomial orders on k[x0, . . . , xn] that have
this property.
a. Deﬁne a graded version of >h by setting
xαxd
0 >gh xβxe
0 ⇐⇒|α| + d > |β| + e
or
|α| + d = |β| + e
and xαxd
0 >h xβxe
0.
Show that Gh is a Gröbner basis with respect to >gh.
b. More generally, let >′ be any monomial order on k[x0, . . . , xn] which extends >
and which has the property that among monomials of the same total degree, any
monomial containing x0 is smaller than all monomials containing only x1, . . . , xn.
Show that Gh is a Gröbner basis for >′.
5. Let > denote grevlex in the ring S = k[x1, . . . , xn, xn+1]. Consider R = k[x1, . . . , xn] ⊆S.
For f ∈R, let f h denote the homogenization of f with respect to the variable xn+1.
a. Show that if f ∈R ⊆S (that is, f depends only on x1, . . . , xn), then LT>( f ) =
LT>( f h).
b. Use part (a) to show that if G is a Gröbner basis for an ideal I ⊆R with respect to
grevlex, then Gh is a Gröbner basis for the ideal Ih in S with respect to grevlex.
6. Prove that the homogenization has the following properties for polynomials f , g ∈
k[x1, . . . , xn]:
( fg)h = f hgh,
( f m)h = ( f h)m
for any integer m ≥0.
Hint: Use the formula for homogenization given by part (ii) of Proposition 7 from §2.
7. Show that I ⊆k[x1, . . . , xn] is a prime ideal if and only if Ih is a prime ideal in
k[x0, . . . , xn]. Hint: For the ⇒implication, use part (a) of Exercise 11 of §3; for the
converse implication, use Exercise 6.
8. Adapt the proof of part (ii) of Proposition 7 to show that I(W) = Ia(W)h for any afﬁne
variety W ⊆kn.
9. Prove that an afﬁne variety W is irreducible if and only if its projective closure W is
irreducible.
10. Let W = V1 ∪· · · ∪Vm be the decomposition of a projective variety into its irreducible
components such that Vi ̸⊆Vj for i ̸= j. Prove that V1 ̸⊆V2 ∪· · · ∪Vm.
In Exercises 11-14, we will explore some interesting varieties in projective space. For
ease of notation, we will write Pn rather than Pn(k). We will also assume that k is alge-
braically closed so that we can apply Theorem 8.

§4 The Projective Closure of an Afﬁne Variety
421
11. The twisted cubic that we have used repeatedly for examples is one member of an inﬁnite
family of curves known as the rational normal curves. The rational normal curve in kn is
the image of the polynomial parametrization φ : k →kn given by
φ(t) = (t, t2, t3, . . . , tn).
By our general results on implicitization from Chapter 3, we know the rational normal
curves are afﬁne varieties. Their projective closures in Pn are also known as rational
normal curves.
a. Find afﬁne equations for the rational normal curves in k4 and k5.
b. Homogenize your equations from part (a) and consider the projective varieties de-
ﬁned by these homogeneous polynomials. Do your equations deﬁne the projective
closure of the afﬁne curve? Are there any "extra" components at inﬁnity?
c. Using Theorems 4 and 8, ﬁnd a set of homogeneous equations deﬁning the projective
closures of these rational normal curves in P4 and P5, respectively. Do you see a
pattern?
d. Show that the rational normal curve in Pn is the variety deﬁned by the set of homo-
geneous quadrics obtained by taking all possible 2 × 2 subdeterminants of the 2 × n
matrix:

x0 x1 x2 · · · xn−1
x1 x2 x3 · · ·
xn

.
12. The afﬁne Veronese surface S ⊆k5 was introduced in Exercise 6 of Chapter 5, §1. It is
the image of the polynomial parametrization φ : k2 →k5 given by
φ(x1, x2) = (x1, x2, x2
1, x1x2, x2
2).
The projective closure of S is a projective variety known as the projective Veronese sur-
face.
a. Find a set of homogeneous equations for the projective Veronese surface in P5.
b. Show that the parametrization of the afﬁne Veronese surface above can be extended
to a mapping from Φ : P2 →P5 whose image coincides with the entire projective
Veronese surface. Hint: You must show that Φ is well-deﬁned (i.e., that it yields the
same point in P5 for any choice of homogeneous coordinates for a point in P2).
13. The Cartesian product of two afﬁne spaces is simply another afﬁne space: kn × km =
km+n. If we use the standard inclusions kn ⊆Pn, km ⊆Pm, and kn+m ⊆Pn+m given by
Proposition 2 of §2, how is Pn+m different from Pn × Pm (as a set)?
14. In this exercise, we will see that Pn×Pm can be identiﬁed with a certain projective variety
in Pn+m+nm known as a Segre variety. The idea is as follows. Let p = (x0 : · · · : xn) ∈Pn
and let q = (y0 : · · · : ym) ∈Pm. The Segre mapping σ : Pn × Pm →Pn+m+nm is
deﬁned by taking the pair (p, q) ∈Pn × Pm to the point in Pn+m+nm with homogeneous
coordinates
(x0y0 : x0y1 : · · · : x0ym : x1y0 : · · · : x1ym : · · · : xny0 : · · · : xnym).
The components are all the possible products xiyj where 0 ≤i ≤n and 0 ≤j ≤m. The
image is a projective variety called a Segre variety.
a. Show that σ is a well-deﬁned mapping (i.e., show that we obtain the same point in
Pn+m+nm no matter what homogeneous coordinates for p, q we use).
b. Show that σ is a one-to-one mapping and that the "afﬁne part" kn × km maps to
an afﬁne variety in kn+m+nm = U0 ⊆Pn+m+nm that is isomorphic to kn+m. (See
Chapter 5, §4.)
c. Taking n = m = 1 above, write out σ : P1 × P1 →P3 explicitly and ﬁnd homoge-
neous equation(s) for the image. Hint: You should obtain a single quadratic equation.
This Segre variety is a quadric surface in P3.

422
Chapter 8 Projective Algebraic Geometry
d. Now consider the case n = 2, m = 1 and ﬁnd homogeneous equations for the Segre
variety in P5.
e. What is the intersection of the Segre variety in P5 and the Veronese surface in P5?
(See Exercise 12.)
§5
Projective Elimination Theory
In Chapter 3, we encountered numerous instances of "missing points" when study-
ing the geometric interpretation of elimination theory. Since our original motivation
for projective space was to account for "missing points," it makes sense to look back
at elimination theory using what we know about Pn(k). You may want to review the
ﬁrst two sections of Chapter 3 before reading further.
We begin with the following example.
Example 1. Consider the variety V ⊆C2 deﬁned by the equation
xy2 = x −1.
To eliminate x, we use the elimination ideal I1 = ⟨xy2 −x + 1⟩∩C[y], and it is
easy to show that I1 = {0} ⊆C[y]. In Chapter 3, we observed that eliminating
x corresponds geometrically to the projection π(V) ⊆C, where π : C2 →C is
deﬁned by π(x, y) = y. We know that π(V) ⊆V(I1) = C, but as the following
picture shows, π(V) does not ﬁll up all of V(I1):
y
x
↓π
↑π
← V
↓
π(V)
We can control the missing points using the Geometric Extension Theorem (Theo-
rem 2 of Chapter 3, §2). Recall how this works: if we write the deﬁning equation of
V as (y2 −1)x + 1 = 0, then the Extension Theorem guarantees that we can solve

§5 Projective Elimination Theory
423
for x whenever the leading coefﬁcient of x does not vanish. Thus, y = ±1 are the
only missing points. You can check that these points are missing over C as well.
To reinterpret the Geometric Extension Theorem in terms of projective space,
ﬁrst observe that the standard projective plane P2(C) is not quite what we want.
We are really only interested in directions along the projection (i.e., parallel to the
x-axis) since all of our missing points lie in this direction. So we do not need all
of P2(C). A more serious problem is that in P2(C), all lines parallel to the x-axis
correspond to a single point at inﬁnity, yet we are missing two points.
To avoid this difﬁculty, we will use something besides P2(C). If we write π as
π : C × C →C, the idea is to make the ﬁrst factor projective rather than the whole
thing. This gives us P1(C) × C, and we will again use π to denote the projection
π : P1(C) × C →C onto the second factor.
We will use coordinates (t :x; y) on P1(C) × C, where the semicolon separates
homogeneous coordinates (t :x) on P1(C) from usual coordinate y on C. Thus, (in
analogy with Proposition 2 of §2) a point (1 :x; y) ∈P1(C) × C corresponds to
(x, y) ∈C × C. We will regard C × C as a subset of P1(C) × C and you should
check that the complement consists of the "points at inﬁnity" (0 :1; y).
We can extend V ⊆C × C to V ⊆P1(C) × C by making the equation of V
homogeneous with respect to t and x. Thus, V is deﬁned by
xy2 = x −t.
In Exercise 1, you will check that this equation is well-deﬁned on P1(C) × C. To
ﬁnd the solutions of this equation, we ﬁrst set t = 1 to get the afﬁne portion and
then we set t = 0 to ﬁnd the points at inﬁnity. This leads to
V = V ∪{(0 :1; ±1)}
(remember that t and x cannot simultaneously vanish since they are homogeneous
coordinates). Under the projection π : P1(C)× C →C, it follows that π(V) = C =
V(I1) because the two points at inﬁnity map to the "missing points" y = ±1. As we
will soon see, the equality π(V) = V(I1) is a special case of the projective version
of the Geometric Extension Theorem.
We will use the following general framework for generalizing the issues raised
by Example 1. Suppose we have equations
f1(x1, . . . , xn, y1, . . . , ym) = 0,
...
fs(x1, . . . , xn, y1, . . . , ym) = 0,
where f1, . . . , fs ∈k[x1, . . . , xn, y1, . . . , ym]. Working algebraically, we can eliminate
x1, . . . , xn by computing the ideal In = ⟨f1, . . . , fs⟩∩k[y1, . . . , ym] (the Elimination
Theorem from Chapter 3, §1 tells us how to do this). If we think geometrically, the
above equations deﬁne a variety V ⊆kn × km, and eliminating x1, . . . , xn corre-
sponds to considering π(V), where π : kn × km →km is projection onto the last m
coordinates. Our goal is to describe the relation between π(V) and V(In).

424
Chapter 8 Projective Algebraic Geometry
The basic idea is to make the ﬁrst factor projective. To simplify notation, we
will write Pn(k) as Pn when there is no confusion about what ﬁeld we are deal-
ing with. A point in Pn × km will have coordinates (x0 : · · · :xn; y1, . . . , ym), where
(x0 : · · · : xn) are homogeneous coordinates in Pn and (y1, . . . , ym) are usual coordi-
nates in km. Thus, (1 :1; 1, 1) and (2 :2; 1, 1) are coordinates for the same point in
P1 × k2, whereas (2 :2; 2, 2) gives a different point. As in Proposition 2 of §2, we
will use
(x1, . . . , xn, y1, . . . , ym) 
−→(1 : x1 : · · · :xn; y1, . . . , ym)
to identify kn × km with the subset of Pn × km where x0 ̸= 0.
We can deﬁne varieties in Pn × km using "partially" homogeneous polynomials
as follows.
Deﬁnition 2. Let k be a ﬁeld.
(i) A polynomial F ∈k[x0, . . . , xn, y1, . . . , ym] is (x0, . . . , xn)-homogeneous pro-
vided there is an integer l ≥0 such that
F =

|α|=l
hα(y1, . . . , ym)xα,
where xα is a monomial in x0, . . . , xn of multidegree α and hα ∈k[y1, . . . , ym].
(ii) The variety V(F1, . . . , Fs) ⊆Pn × km deﬁned by (x0, . . . , xn)-homogeneous
polynomials F1, . . . , Fs ∈k[x0, . . . , xn, y1, . . . , ym] is the set
{(a0 : · · · : an; b1, . . . , bm) ∈Pn × km | Fi(a0, . . . , an, b1, . . . , bm) = 0
for 1 ≤i ≤s}.
In the exercises, you will show that if a (x0, . . . , xn)-homogeneous polynomial
vanishes at one set of coordinates for a point in Pn × km, then it vanishes for all
coordinates of the point. This shows that the variety V(F1, . . . , Fs) is a well-deﬁned
subset of Pn × km when F1, . . . , Fs are (x0, . . . , xn)-homogeneous.
We can now discuss what elimination theory means in this context. Suppose we
have (x0, . . . , xn)-homogeneous equations
(1)
F1(x0, . . . , xn, y1, . . . , ym) = 0,
...
Fs(x0, . . . , xn, y1, . . . , ym) = 0.
These deﬁne the variety V = V(F1, . . . , Fs) ⊆Pn × km. We also have the projection
map
π : Pn × km →km
onto the last m coordinates. Then we can interpret π(V) ⊆km as the set of all
m-tuples (y1, . . . , ym) for which the equations (1) have a nontrivial solution in
x0, . . . , xn (which means that at least one xi is nonzero).
To understand what this means algebraically, let us work out an example.

§5 Projective Elimination Theory
425
Example 3. In this example, we will use (u : v; y) as coordinates on P1 × k. Then
consider the equations
(2)
F1 = u + vy = 0,
F2 = u + uy = 0.
Since (u : v) are homogeneous coordinates on P1, it is straightforward to show that
V = V(F1, F2) = {(0 :1; 0), (1 :1; −1)} ⊆P1 × k.
Under the projection π : P1 × k →k, we have π(V) = {0, −1}, so that for a given
y, the equations (2) have a nontrivial solution if and only if y = 0 or −1. Thus, (2)
implies that y(1 + y) = 0.
Ideally, there should be a purely algebraic method of "eliminating" u and v
from (2) to obtain y(1 + y) = 0. Unfortunately, the kind of elimination we did
in Chapter 3 does not work. To see why, let I = ⟨F1, F2⟩⊆k[u, v, y] be the ideal
generated by F1 and F2. Since every term of F1 and F2 contains u or v, it follows
that
I ∩k[y] = {0}.
From the afﬁne point of view, this is the correct answer since the afﬁne variety
Va(F1, F2) ⊆k2 × k
contains the trivial solutions (0 : 0; y) for all y ∈k. Thus, the afﬁne methods of
Chapter 3 will be useful only if we can ﬁnd an algebraic way of excluding the
solutions where u = v = 0.
Recall from Chapter 4 that for afﬁne varieties, ideal quotients I : J and saturations
I : J∞correspond (roughly) to the difference of varieties. Comparing Proposition 7
and Theorem 10 of Chapter 4, §4, we see that the saturation has the best relation to
the difference of varieties.
In our situation, the difference
Va(F1, F2) \ Va(u, v) ⊆k2 × k.
consists exactly of the nontrivial solutions of (2). Hence, for I = ⟨F1, F2⟩, the satu-
ration I :⟨u, v⟩∞⊆k[u, v, y] is the algebraic way to remove the trivial solutions.
Since we also want to eliminate u, v, this suggests that we should consider the
intersection
ˆI = (I :⟨u, v⟩∞) ∩k[y].
Using the techniques of Chapter 4, it can be shown that ˆI = ⟨y(1 + y)⟩in this case.
Hence we recover precisely the polynomial we wanted to obtain.
Motivated by this example, we are led to the following deﬁnition.
Deﬁnition 4. Given an ideal I ⊆k[x0, . . . , xn, y1, . . . , ym] generated by (x0, . . . , xn)-
homogeneous polynomials, the projective elimination ideal of I is the ideal

426
Chapter 8 Projective Algebraic Geometry
ˆI = (I :⟨x0, . . . , xn⟩∞) ∩k[y1, . . . , ym].
Exercise 6 will show why saturation is essential—just using the ideal quotient
can give the wrong answer. Recall from Deﬁnition 8 of Chapter 4, §4 that
I :⟨x0, . . . , xn⟩∞= {f ∈k[x0, . . . , xn, y1, . . . , ym] | for every g ∈⟨x0, . . . , xn⟩,
there is e ≥0 with fge ∈I}.
Exercise 7 will give some other ways to represent the projective elimination ideal ˆI.
Here is our ﬁrst result about ˆI.
Proposition 5. Let V = V(F1, . . . , Fs) ⊆Pn × km be deﬁned by (x0, . . . , xn)-
homogeneous polynomials and let π : Pn × km →km be the projection map. Then
in km, we have
π(V) ⊆V(ˆI),
where ˆI is the projective elimination ideal of I = ⟨F1, . . . , Fs⟩.
Proof. Suppose (a0 : · · · :an; b1, . . . , bm) ∈V and f ∈ˆI. The formula for saturation
shows that for every i, xei
i f(y1, . . . , ym) ∈I for some ei. Hence this polynomial
vanishes on V, so that
aei
i f(b1, . . . , bm) = 0
for all i. Since (a0 : · · · :an) are homogeneous coordinates, at least one ai ̸= 0 and,
thus, f(b1, . . . , bm) = 0. This proves that f vanishes on π(V) and the proposition
follows.
□
When the ﬁeld is algebraically closed, we also have the following projective
version of the Extension Theorem.
Theorem 6 (The Projective Extension Theorem). Let k be algebraically closed
and assume V = V(F1, . . . , Fs) ⊆Pn × km is deﬁned by (x0, . . . , xn)-homogeneous
polynomials in k[x0, . . . , xn, y1, . . . , ym]. Also let ˆI ⊆k[y1, . . . , ym] be the projective
elimination ideal of I = ⟨F1, . . . , Fs⟩. If
π : Pn × km →km
is projection onto the last m coordinates, then
π(V) = V(ˆI).
Proof. The inclusion π(V) ⊆V(ˆI) follows from Proposition 5. For the opposite
inclusion, let c = (c1, . . . , cm) ∈V(ˆI) and set
Fi(x0, . . . , xn, c) = Fi(x0, . . . , xn, c1, . . . , cm).
This is a homogeneous polynomial in x0, . . . , xn, say of total degree = di [equal to
the total degree of Fi(x0, . . . , xn, y1, . . . , ym) in x0, . . . , xn].

§5 Projective Elimination Theory
427
We will show that c ̸∈π(V) leads to a contradiction. To see why, ﬁrst observe
that c ̸∈π(V) implies that the equations
F1(x0, . . . , xn, c) = · · · = Fs(x0, . . . , xn, c) = 0
deﬁne the empty variety in Pn. Since the ﬁeld k is algebraically closed, the Projective
Weak Nullstellensatz (Theorem 8 of §3) implies that for some r ≥1, we have
⟨x0, . . . , xn⟩r ⊆⟨F1(x0, . . . , xn, c), . . . , Fs(x0, . . . , xn, c)⟩.
This means that each monomial xα, |α| = r, can be written as a polynomial linear
combination of the Fi(x0, . . . , xn, c), say
xα =
s

i=1
Hi(x0, . . . , xn)Fi(x0, . . . , xn, c).
By taking homogeneous components, we can assume that each Hi is homogeneous
of total degree r −di [since di is the total degree of Fi(x0, . . . , xn, c)]. Then, writing
each Hi as a linear combination of monomials xβi with |βi| = r −di, we see that the
polynomials
xβiFi(x0, . . . , xn, c),
i = 1, . . . , s, |βi| = r −di
span the vector space of all homogeneouspolynomials of total degree r in x0, . . . , xn.
If the dimension of this space is denoted Nr, then by standard results in linear alge-
bra, we can ﬁnd Nr of these polynomials which form a basis for this space. We will
denote this basis as
Gj(x0, . . . , xn, c),
j = 1, . . . , Nr.
This leads to an interesting polynomial in y1, . . . , ym as follows. The polynomial
Gj(x0, . . . , xn, c) comes from a polynomial
Gj = Gj(x0, . . . , xn, y1, . . . , ym) ∈k[x0, . . . , xn, y1, . . . , ym].
Each Gj is homogeneous in x0, . . . , xn of total degree r. Thus, we can write
(3)
Gj =

|α|=r
ajα(y1, . . . , ym)xα.
Since the xα with |α| = r form a basis of all homogeneous polynomials of total
degree r, there are Nr such monomials. Hence we get a square matrix of polynomials
ajα(y1, . . . , ym). Then let
D(y1, . . . , ym) = det(ajα(y1, . . . , ym) | 1 ≤j ≤Nr, |α| = r)

428
Chapter 8 Projective Algebraic Geometry
be the corresponding determinant. If we substitute c into (3), we obtain
Gj(x0, . . . , xn, c) =

|α|=r
ajα(c)xα,
and then
D(c) ̸= 0
since the Gj(x0, . . . , xn, c)'s and xα's are bases of the same vector space.
Our Main Claim is that D(y1, . . . , ym) ∈ˆI. This will give the contradiction we
seek, since D(y1, . . . , ym) ∈ˆI and c ∈V(ˆI) imply D(c) = 0. The strategy for
proving the Main Claim will be to show that for every monomial xα with |α| = r,
we have
(4)
xαD(y1, . . . , ym) ∈I.
Once we have (4), it follows easily that grD(y1, . . . , ym) ∈I for all g ∈⟨x0, . . . , xn⟩,
which in turn implies that D ∈I :⟨x0, . . . , xn⟩∞. Since D ∈k[y1, . . . , ym], we get
D ∈ˆI, proving the Main Claim.
It remains to prove (4). We will use linear algebra over the function ﬁeld K =
k(x0, . . . , xn, y1, . . . , ym) (see Chapter 5, §5). Take Nr variables Yα for |α| = r and
consider the system of linear equations over K given by

|α|=r
ajα(y1, . . . , ym)Yα = Gj(x0, . . . , xn, y1, . . . , ym),
j = 1, . . . , Nr.
The determinant of this system is D(y1, . . . , ym) and is nonzero since D(c) ̸= 0.
Hence the system has a unique solution, which by (3) is given by Yα = xα.
In this situation, Cramer's Rule (Proposition 2 of Appendix A, §4) gives a for-
mula for the solution in terms of the coefﬁcients of the system. More precisely, the
solution Yα = xα is given by the quotient
xα =
det(Mα)
D(y1, . . . , ym),
where Mα is the matrix obtained from (ajα) by replacing the column α by the
polynomials G1, . . . , GNr. If we multiply each side by D(y1, . . . , ym) and expand
det(Mα) along this column, we get an equation of the form
xαD(y1, . . . , ym) =
Nr

j=1
Hjα(y1, . . . , ym)Gj(x0, . . . , xn, y1, . . . , ym).
However, recall that every Gj is of the form xβiFi, and if we make this substitution
and write the sum in terms of the Fi, we obtain
xαD(y1, . . . , ym) ∈⟨F1, . . . , Fs⟩= I.

§5 Projective Elimination Theory
429
This proves (4), and we are done.
□
Theorem 6 tells us that when we project a variety V ⊆Pn × km into km, the
result is again a variety. This has the following nice interpretation: if we think of the
variables y1, . . . , ym as parameters in the system of equations
F1(x0, . . . , xn, y1, . . . , ym) = · · · = Fs(x0, . . . , xn, y1, . . . , ym) = 0,
then the equations deﬁning π(V) = V(ˆI) in km tell us what conditions the parame-
ters must satisfy in order for the above equations to have a nontrivial solution (i.e.,
a solution different from x0 = · · · = xn = 0).
From a computational perspective, we learned how to compute the saturation
I : ⟨x0, . . . , xn⟩∞in Chapter 4, §4, and then we can use the elimination theory from
Chapter 3 to compute ˆI = (I :⟨x0, . . . , xn⟩∞) ∩k[y1, . . . , ym]. Hence there is an
algorithm for computing projective elimination ideals.
We next relate ˆI to the kind of elimination we did in Chapter 3. The basic idea is
to reduce to the afﬁne case by dehomogenization. If we ﬁx 0 ≤i ≤n, then setting
xi = 1 in F ∈k[x0, . . . , xn, y1, . . . , ym] gives the polynomial
F(i) = F(x0, . . . , 1, . . . , xn, y1, . . . , ym) ∈k[x0, . . . , ˆxi, . . . , xn, y1, . . . , ym],
where ˆxi means that xi is omitted from the list of variables. Then, given an ideal
I ⊆k[x0, . . . , xn, y1, . . . , ym], we get the dehomogenization
I(i) = {F(i) | F ∈I} ⊆k[x0, . . . , ˆxi, . . . , xn, y1, . . . , ym].
It is easy to show that I(i) is an ideal in k[x0, . . . , ˆxi, . . . , xn, y1, . . . , ym]. We also
leave it as an exercise to show that if I = ⟨F1, . . . , Fs⟩, then
(5)
I(i) = ⟨F(i)
1 , . . . , F(i)
s ⟩.
Let V ⊆Pn × km be the variety deﬁned by I. One can think of I(i) as deﬁning
the afﬁne portion V ∩(Ui × km), where Ui ∼= kn is the subset of Pn where xi = 1.
Since we are now in a purely afﬁne situation, we can eliminate using the methods
of Chapter 3. In particular, we get the n-th elimination ideal
I(i)
n
= I(i) ∩k[y1, . . . , ym],
where the subscript n indicates that the n variables x0, . . . , ˆxi, . . . , xn have been
eliminated. We now compute ˆI in terms of its dehomogenizations I(i) as follows.
Proposition 7. Let I ⊆k[x0, . . . , xn, y1, . . . , ym] be an ideal that is generated by
(x0, . . . , xn)-homogeneous polynomials. Then
ˆI = I(0)
n
∩I(1)
n
∩· · · ∩I(n)
n .
Proof. First suppose that f ∈ˆI. Then xei
i f(y1, . . . , ym) ∈I for some ei ≥0, so that
when we set xi = 1, we get f(y1, . . . , ym) ∈I(i). This proves f ∈I(0) ∩· · · ∩I(n).

430
Chapter 8 Projective Algebraic Geometry
For the other inclusion, we ﬁrst study the relation between I and I(i). An element
f ∈I(i) is obtained from some F ∈I by setting xi = 1. We claim that F can be
assumed to be (x0, . . . , xn)-homogeneous. To see why, note that F can be written
as a sum F = d
j=0 Fj, where Fj is (x0, . . . , xn)-homogeneous of total degree j in
x0, . . . , xn. Since I is generated by (x0, . . . , xn)-homogeneous polynomials, the proof
of Theorem 2 of §3 can be adapted to show that Fj ∈I for all j (see Exercise 4).
This implies that
d

j=0
xd−j
i
Fj
is a (x0, . . . , xn)-homogeneous polynomial in I which dehomogenizes to f when
xi = 1. Thus, we can assume that F ∈I is (x0, . . . , xn)-homogeneous.
As in §2, we can deﬁne a homogenization operator which takes a polynomial
f ∈k[x0, . . . , ˆxi, . . . , xn, y1, . . . , ym] and uses the extra variable xi to produce a
(x0, . . . , xn)-homogeneous polynomial f h ∈k[x0, . . . , xn, y1, . . . , ym]. We leave it
as an exercise to show that if a (x0, . . . , xn)-homogeneous polynomial F dehomog-
enizes to f using xi = 1, then there is an integer ei ≥0 such that
(6)
F = xei
i f h.
Now suppose f ∈I(i). As we showed above, f comes from F ∈I which is
(x0, . . . , xn)-homogeneous. Since f does not involve x0, . . . , xn, we have f = f h,
and then (6) implies xei
i f ∈I. This holds for all 0 ≤i ≤n if we assume that
f ∈I(0) ∩· · · ∩I(n). By Exercise 7, it follows that f ∈ˆI, and we are done.
□
Proposition 7 has a nice interpretation. Namely, I(i)
n can be thought of as eliminat-
ing x0, . . . , ˆxi, . . . , xn on the afﬁne piece of Pn × km where xi = 1. Then intersecting
these afﬁne elimination ideals (which roughly corresponds to the eliminating on the
union of the afﬁne pieces) gives the projective elimination ideal.
We can also use Proposition 7 to give another algorithm for ﬁnding ˆI. If I =
⟨F1, . . . , Fs⟩, we know a basis of I(i) by (5), so that we can compute I(i)
n
using the
Elimination Theorem of Chapter 3, §1. Then the algorithm for ideal intersections
from Chapter 4, §3 tells us how to compute ˆI = I(0)
n
∩· · · ∩I(n)
n . Thus we have a
second algorithm for computing projective elimination ideals.
To see how this works in practice, consider the equations
F1 = u + vy = 0,
F2 = u + uy = 0
from Example 3. If we set I = ⟨u + vy, u + uy⟩⊆k[u, v, y] and compute suitable
Gröbner bases, then we obtain
when u = 1 : I(u)
1
= ⟨1 + vy, 1 + y⟩∩k[y] = ⟨1 + y⟩,
when v = 1 : I(v)
1
= ⟨u + y, u + uy⟩∩k[y] = ⟨y(1 + y)⟩,

§5 Projective Elimination Theory
431
and it follows that ˆI = I(u)
1
∩I(v)
1
= ⟨y(1 + y)⟩. Can you explain why I(u)
1
and I(v)
1
are different?
We next return to a question posed earlier concerning the missing points that
can occur in the afﬁne case. An ideal I ⊆k[x1, . . . , xn, y1, . . . , ym] gives a variety
V = Va(I) ⊆kn × km, and under the projection π : kn × km →km, we know that
π(V) ⊆V(In), where In is the n-th elimination ideal of I. We want to show that
points in V(In) \ π(V) come from points at inﬁnity in Pn × km.
To decide what variety in Pn × km to use, we will homogenize with respect to
x0. Recall from the proof of Proposition 7 that f ∈k[x1, . . . , xn, y1, . . . , ym] gives us
a (x0, . . . , xn)-homogeneous polynomial f h ∈k[x0, . . . , xn, y1, . . . , ym]. Exercise 9
will study homogenization in more detail. Then the (x0, . . . , xn)-homogenization of
I is deﬁned to be the ideal
Ih = ⟨f h | f ∈I⟩⊆k[x0, . . . , xn, y1, . . . , ym].
Using the Hilbert Basis Theorem, it follows easily that Ih is generated by ﬁnitely
many (x0, . . . , xn)-homogeneous polynomials.
The following proposition gives the main properties of Ih.
Proposition 8. Given I ⊆k[x1, . . . , xn, y1, . . . , ym], let Ih be its (x0, . . . , xn)-homo-
genization. Then:
(i) The projective elimination ideal of Ih equals the n-th elimination ideal of I. Thus,
+Ih = In ⊆k[y1, . . . , ym].
(ii) If k is algebraically closed, then the variety V = V(Ih) is the smallest variety
in Pn × km containing the afﬁne variety V = Va(I) ⊆kn × km. We call V the
projective closure of V in Pn × km.
Proof. (i) It is straightforward to show that dehomogenizing Ih with respect to x0
gives (Ih)(0) = I. Then the proof of Proposition 7 implies that +Ih ⊆In. Going
the other way, take f ∈In. Since f ∈k[y1, . . . , ym], it is already (x0, . . . , xn)-
homogeneous. Hence, f = f h ∈Ih and it follows that x0
i f ∈Ih for all i. This
shows that f ∈+Ih, and (i) is proved.
Part (ii) is similar to Theorem 8 of §4 and is left as an exercise.
□
Using Theorem 6 and Proposition 8 together, we get the following nice result.
Corollary 9. Assume that k is algebraically closed and let V = Va(I) ⊆kn × km,
where I ⊆k[x1, . . . , xn, y1, . . . , ym] is an ideal. Then
V(In) = π(V),
where V ⊆Pn × km is the projective closure of V and π : Pn × km →km is the
projection.
Proof. Since Proposition 8 tells us that V = V(Ih) and +Ih = In, the corollary follows
immediately from Theorem 6.
□

432
Chapter 8 Projective Algebraic Geometry
In Chapter 3, points of V(In) were called "partial solutions." Thus, V(In) \ π(V)
consists of those partial solutions which do not extend to solutions in V. The above
corollary shows that these points come from points at inﬁnity in the projective clo-
sure V of V.
To use Corollary 9, we need to be able to compute Ih. As in §4, the difﬁculty is
that I = ⟨f1, . . . , fs⟩need not imply Ih = ⟨f h
1 , . . . , f h
s ⟩. But if we use an appropriate
Gröbner basis, we get the desired equality.
Proposition 10. Let > be a monomial order on k[x1, . . . , xn, y1, . . . , ym] such that
for all monomials xαyγ, xβyδ in x1, . . . , xn, y1, . . . , ym, we have
|α| > |β| ⇒xαyγ > xβyδ.
If G = {g1, . . . , gs} is a Gröbner basis for I ⊆k[x1, . . . , xn, y1, . . . , ym] with respect
to >, then Gh = {gh
1, . . . , gh
s} is a basis for Ih ⊆k[x0, . . . , xn, y1, . . . , ym].
Proof. This is similar to Theorem 4 of §4 and is left as an exercise.
□
In Example 1, we considered I = ⟨xy2 −x + 1⟩⊆C[x, y]. This is a principal
ideal and, hence, xy2 −x + 1 is a Gröbner basis for any monomial ordering (see
Exercise 10 of Chapter 2, §5). If we homogenize with respect to the new variable
t, Proposition 10 tells us that Ih is generated by the (t, x)-homogeneous polynomial
xy2 −x+t. Now let V = V(Ih) ⊆P1 ×C. Then Corollary 9 shows π(V) = V(I1) =
C, which agrees with what we found in Example 1.
Using Corollary 9 and Proposition 10, we can point out a weakness in the Ge-
ometric Extension Theorem given in Chapter 3, §2. This theorem stated that if
I = ⟨f1, . . . , fs⟩, then
(7)
V(I1) = π(V) ∪(V(g1, . . . , gs) ∩V(I1)),
where V = Va(I) and gi ∈k[x2, . . . , xn] is the leading coefﬁcient of fi with respect
to x1. From the projective point of view, {(0 :1)} × V(g1, . . . , gs) are the points at
inﬁnity in Z = V( f h
1 , . . . , f h
s ) (this follows from the proof of Theorem 6). Since
f1, . . . , fs was an arbitrary basis of I, Z may not be the projective closure of V and,
hence, V(g1, . . . , gs) may be too large. To make V(g1, . . . , gs) ∩V(I1) as small as
possible in (7), we should use a Gröbner basis for I with respect to a monomial
ordering of the type described in Proposition 10.
We will end the section with a study of maps between projective spaces. Suppose
that f0, . . . , fm ∈k[x0, . . . , xn] are homogeneous polynomials of total degree d such
that V( f0, . . . , fm) = ∅in Pn. Then we can deﬁne a map F : Pn →Pm by the formula
F(x0 : · · · :xn) = ( f0(x0, . . . , xn): · · · : fm(x0, . . . , xn)).
Since f0, . . . , fm never vanish simultaneously on Pn, F(x0 : · · · : xn) always gives a
point in Pn. Furthermore, since the fi are homogeneous of total degree d, we have

§5 Projective Elimination Theory
433
F(λx0 : · · · :λxn) = ( f0(λx0, . . . , λxn): · · · : fm(λx0, . . . , λxn))
= (λdf0(x0, . . . , xn): · · · :λdfm(x0, . . . , xn))
= ( f0(x0, . . . , xn): · · · : fm(x0, . . . , xn)) = F(x0 : · · · : xn)
for all λ ∈k \ {0}. Thus, F is a well-deﬁned function from Pn to Pm.
We have already seen examples of such maps between projective spaces. For
instance, Exercise 21 of §2 studied the map F : P1 →P2 deﬁned by
F(a :b) = (a2 + b2 : 2ab :a2 −b2).
This is a projective parametrization of V(x2 −y2 −z2). Also, Exercise 12 of §4
discussed the Veronese map φ : P2 →P5 deﬁned by
φ(x0 : x1 : x2) = (x2
0 :x0x1 :x0x2 : x2
1 : x1x2 : x2
2).
The image of this map is called the Veronese surface in P5.
Over an algebraically closed ﬁeld, we can describe the image of F : Pn →Pm
using elimination theory as follows.
Theorem 11. Let k be algebraically closed and let F : Pn →Pm be deﬁned by
homogeneous polynomials f0, . . . , fm ∈k[x0, . . . , xn] which have the same total de-
gree > 0 and no common zeros in Pn. In k[x0, . . . , xn, y0, . . . , ym], let I be the ideal
⟨y0 −f0, . . . , ym −fm⟩and let In+1 = I ∩k[y0, . . . , ym]. Then In+1 is a homogeneous
ideal in k[y0, . . . , ym], and
F(Pn) = V(In+1).
Proof. The ﬁrst has three parts. The ﬁrst part is to show that In+1 is a homoge-
neous ideal. Suppose that the fi have total degree d. Since the generators yi −fi
of I are not homogeneous (unless d = 1), we will introduce weights on the
variables x0, . . . , xn, y0, . . . , ym. We say that each xi has weight 1 and each yj
has weight d. Then a monomial xαyβ has weight |α| + d|β|, and a polynomial
f ∈k[x0, . . . , xn, y0, . . . , ym] is weighted homogeneous provided every monomial
in f has the same weight.
The generators yi −fi of I all have weight d, so that I is a weighted homogeneous
ideal. If we compute a reduced Gröbner basis G for I with respect to any monomial
order, an argument similar to the proof of Theorem 2 of §3 shows that G consists of
weighted homogeneous polynomials. For an appropriate lex order, the Elimination
Theorem from Chapter 3 shows that G ∩k[y0, . . . , ym] is a basis of the elimination
ideal In+1 = I ∩k[y0, . . . , ym]. Thus, In+1 has a weighted homogeneous basis. Since
the yi's all have the same weight, a polynomial in k[y0, . . . , ym] is weighted homo-
geneous if and only if it is homogeneous in the usual sense. This proves that In+1 is
a homogeneous ideal.
The second part of the proof is to study the image of F. Here, we will consider
varieties in the product Pn × Pm. A polynomial h ∈k[x0, . . . , xn, y0, . . . , ym] is said
to be bihomogeneous if it can be written as

434
Chapter 8 Projective Algebraic Geometry
h =

|α|=l1,|β|=l2
aαβxα yβ.
If h1, . . . , hs are bihomogeneous, we get a well-deﬁned set
V(h1, . . . , hs) ⊆Pn × Pm
which is the variety deﬁned by h1, . . . , hs. Similarly, if J ⊆k[x0, . . . , xn, y0, . . . , ym]
is generated by bihomogeneous polynomials, then we get a variety V(J) ⊂Pn×Pm.
(See Exercise 13 for the details.)
Elimination theory applies nicely to a bihomogeneous ideal J. The projective
elimination ideal ˆJ ⊆k[y0, . . . , ym] is a homogeneous ideal (see Exercise 13). Then,
using the projection π : Pn × Pm →Pm, it is an easy corollary of Theorem 6 that
(8)
π(V(J)) = V(ˆJ)
in Pm (see Exercise 13). As in Theorem 6, this requires that k be algebraically closed.
The particular bihomogeneous ideal we will use is J = ⟨yi fj −yj fi⟩. Note that
yi fj −yj fi has degree d in x0, . . . , xn and degree 1 in y0, . . . , ym, so that J is indeed
bihomogeneous. Let us ﬁrst show that V(J) ⊆Pn ×Pm is the graph of F : Pn →Pm.
Given p ∈Pn, we have (p, F(p)) ∈V(J) since yi = fi(p) for all i. Conversely,
suppose that (p, q) ∈V(J). Then qi fj(p) = qj fi(p) for all i, j, where qi is the i-th
homogeneous coordinate of q. We can ﬁnd j with qj ̸= 0, and by our assumption on
f0, . . . , fm, there is i with fi(p) ̸= 0. Then qi fj(p) = qj fi(p) ̸= 0 shows that qi ̸= 0.
Now let λ = fi(p)/qi, which is a nonzero element of k. Then λqℓ= ( fi(p)/qi)qℓ=
fℓ(p) since qℓfi(p) = qi fℓ(p). It follows that
(p, q) = (p1 : · · · : pn, q1 : · · · : qm) = (p1 : · · · :pn, λq1 : · · · : λqm)
= (p1 : · · · :pn, f1(p): · · · : fm(p)) = (p, F(p)).
Hence (p, q) is in the graph of F in Pn × Pm.
As we saw in §3 of Chapter 3, the projection of the graph is the image of the
function. Thus, under π : Pn×Pm →Pm, we have π(V(J)) = F(Pn). If we combine
this with (8), we get F(Pn) = V(ˆJ) since k is algebraically closed.
The third and ﬁnal part of the proof is to show that V(ˆJ) = V(In+1) in Pm. It
sufﬁces to work in afﬁne space km+1 and prove that Va(ˆJ) = Va(In+1). Observe
that the variety Va(I) ⊆kn+1 × km+1 is the graph of the map kn+1 →km+1 deﬁned
by ( f0, . . . , fm). Under the projection π : kn+1 × km+1 →km+1, we claim that
π(Va(I)) = Va(ˆJ). We know that V(ˆJ) is the image of F in Pm. Once we exclude
the origin, this means that q ∈Va(ˆJ) if and only if there is a some p ∈kn+1 such
that q equals F(p) in Pm. Hence, q = λF(p) in km+1 for some λ ̸= 0. If we set
λ′ =
d√
λ, then q = F(λ′p), which is equivalent to q ∈π(Va(I)). The claim now
follows easily.
By the Closure Theorem (Theorem 3 of Chapter 3, §2), Va(In+1) is the smallest
variety containing π(Va(I)). Since this projection equals the variety Va(ˆJ), it follows
immediately that Va(In+1) = Va(ˆJ). This completes the proof of the theorem.
□

§5 Projective Elimination Theory
435
EXERCISES FOR §5
1. In Example 1, explain why xy2 −x + t = 0 determines a well-deﬁned subset of P1 × C,
where (t : x) are homogeneous coordinates on P1 and y is a coordinate on C. Hint: See
Exercise 2.
2. Suppose F ∈k[x0, . . . , xn, y1, . . . , ym] is (x0, . . . , xn)-homogeneous. Show that if F van-
ishes at one set of coordinates for a point in Pn × km, then F vanishes at all coordinates
for the point.
3. In Example 3, show that V(F1, F2) = {(0 : 1; 0), (1 : 1; −1)}.
4. This exercise will study ideals generated by (x0, . . . , xn)-homogeneous polynomials.
a. Prove that every F ∈k[x0, . . . , xn, y1, . . . , ym] can be written uniquely as a sum 
i Fi
where Fi is a (x0, . . . , xn)-homogeneous polynomial of degree i in x0, . . . , xn. We call
these the (x0, . . . , xn)-homogeneous components of F.
b. Prove that an ideal I ⊆k[x0, . . . , xn, y1, . . . , ym] is generated by (x0, . . . , xn)-homo-
geneous polynomials if and only if I contains the (x0, . . . , xn)-homogeneous compo-
nents of each of its elements.
5. In Example 3, we claimed that (I : ⟨u, v⟩∞) ∩k[y] = ⟨y(1 + y)⟩when I = ⟨u + vy, u +
uy⟩⊆k[u, v, y]. Prove this using the methods of Chapters 3 and 4.
6. As in Example 3, we will use (u : v; y) as coordinates on P1 × k. Let F1 = u −vy and
F2 = u2 −v2y in k[u, v, y].
a. Compute V(F1, F2) and explain geometrically why eliminating u and v should lead
to the equation y(1 −y) = 0.
b. Show that (I : ⟨u, v⟩) ∩k[y] = {0} and that (I : ⟨u, v⟩∞) ∩k[y] = ⟨y(1 −y)⟩. This
explains why we need saturations—ideal quotients can give an answer that is too
small.
7. Let I ⊆k[x0, . . . , xn, y1, . . . , ym] be an ideal with projective elimination ideal ˆI =
(I : ⟨x0, . . . , xn⟩∞) ∩k[y1, . . . , ym]. By Proposition 9 of Chapter 4, §4, we can write ˆI
as ˆI = (I : ⟨x0, . . . , xn⟩e) ∩k[y1, . . . , ym] for e sufﬁciently large. This exercise will ex-
plore two other ways to express ˆI.
a. Prove that if e is sufﬁciently large, then
ˆI = (I : ⟨xe
0, . . . , xe
n⟩) ∩k[y1, . . . , ym].
b. Prove that
ˆI = { f ∈k[y1, . . . , ym] | for all 0 ≤i ≤n, there is ei ≥0 with xei
i f ∈I}.
8. In this exercise, we will use the dehomogenization operator F →F(i) deﬁned in the
discussion preceding Proposition 7.
a. Prove that I(i) = {F(i) | F ∈I} is an ideal in k[x0, . . . , ˆxi, . . . , xn, y1, . . . , ym].
b. If I = ⟨F1, . . . , Fs⟩, then show that I(i) = ⟨F(i)
1 , . . . , F(i)
s ⟩.
9. In the proof of Proposition 7, we needed the homogenization operator, which makes a
polynomial f ∈k[x1, . . . , xn, y1, . . . , ym] into a (x0, . . . , xn)-homogeneous polynomial f h
using the extra variable x0.
a. Give a careful deﬁnition of f h.
b. If we dehomogenize f h by setting x0 = 1, show that we get ( f h)(0) = f .
c. Let f = F(0) be the dehomogenization of a (x0, . . . , xn)-homogeneous polynomial F.
Then prove that F = xe
0 f h for some integer e ≥0.
10. Prove part (ii) of Proposition 8.
11. Prove Proposition 10. Also give an example of a monomial order which satisﬁes the
hypothesis of the proposition. Hint: You can use an appropriate weight order from Exer-
cise 11 of Chapter 2, §4.

436
Chapter 8 Projective Algebraic Geometry
12. The proof of Theorem 11 used weighted homogeneous polynomials. The general setup
is as follows. Given variables x0, . . . , xn, we assume that each variable has a weight qi,
which we assume to be a positive integer. Then the weight of a monomial xα is n
i=0 qiαi,
where α = (α0, . . . , αn). A polynomial is weighted homogeneous if all of its monomials
have the same weight.
a. Show that every f ∈k[x0, . . . , xn] can be written uniquely as a sum of weighted
homogeneous polynomials 
i fi, where fi is weighted homogeneous of weight i.
These are called the weighted homogeneous components of f .
b. Deﬁne what it means for an ideal I ⊆k[x0, . . . , xn] to be a weighted homogeneous
ideal. Then formulate and prove a version of Theorem 2 of §3 for weighted homoge-
neous ideals.
13. This exercise will study the elimination theory of Pn × Pm. We will use the polynomial
ring k[x0, . . . , xn, y0, . . . , ym], where (x0 : · · · : xm) are homogeneous coordinates on Pn
and (y0 : · · · : ym) are homogeneous coordinates on Pm.
a. As in the text, h ∈k[x0, . . . , xn, y0, . . . , ym] is bihomogeneous if it can be written in
the form
h =

|α|=l1,|β|=l2
aαβxα yβ.
We say that h has bidegree (l1, l2). If h1, . . . , hs are bihomogeneous, show that we
get a well-deﬁned variety
V(h1, . . . , hs) ⊆Pn × Pm.
Also, if J ⊆k[x0, . . . , xn, y0, . . . , ym] is an ideal generated by bihomogeneous poly-
nomials, explain how to deﬁne V(J) ⊆Pn × Pm and prove that V(J) is a variety.
b. If J is generated by bihomogeneous polynomials, we have V = V(J) ⊆Pn × Pm.
Since J is also (x0, . . . , xn)-homogeneous, we can form its projective elimination
ideal ˆJ ⊆k[y0, . . . , ym]. Prove that ˆJ is a homogeneous ideal.
c. Now assume that k is algebraically closed. Under the projection π : Pn × Pm →Pm,
prove that
π(V) = V(ˆJ)
in Pm. This is the main result in the elimination theory of varieties in Pn ×Pm. Hint: J
also deﬁnes a variety in Pn ×km+1, so that you can apply Theorem 6 to the projection
Pn × km+1 →km+1.
14. For the two examples of maps between projective spaces given in the discussion preced-
ing Theorem 11, compute deﬁning equations for the images of the maps.
15. In Exercise 11 of §1, we considered the projective plane P2, with coordinates (x : y : z),
and the dual projective plane P2∨, where (A : B : C) ∈P2∨corresponds to the projective
line L deﬁned by Ax + By + Cz = 0 in P2. Show that the subset
V = {(p, L) ∈P2 × P2∨| p ∈L} ⊆P2 × P2∨
is the variety deﬁned by a bihomogeneous polynomial in k[x, y, z, A, B, C] of bidegree
(1, 1). We call V an incidence variety. Hint: See part (f) of Exercise 11 of §1.
§6
The Geometry of Quadric Hypersurfaces
In this section, we will study quadric hypersurfaces in Pn(k). These varieties gener-
alize conic sections in the plane and their geometry is quite interesting. To simplify
notation, we will write Pn rather than Pn(k), and we will use x0, . . . , xn as homoge-
neous coordinates. Throughout this section, we will assume that k is a ﬁeld not of

§6 The Geometry of Quadric Hypersurfaces
437
characteristic 2. This means that 2 = 1 + 1 ̸= 0 in k, so that in particular we can
divide by 2.
Before introducing quadric hypersurfaces, we need to deﬁne the notion of projec-
tive equivalence. Let GL(n + 1, k) be the set of invertible (n + 1) × (n + 1) matrices
with entries in k. We can use elements A ∈GL(n+1, k) to create transformations of
Pn as follows. Under matrix multiplication, A induces a linear map A : kn+1 →kn+1
which is an isomorphism since A is invertible. This map takes subspaces of kn+1
to subspaces of the same dimension, and restricting to l-dimensional subspaces, it
follows that A takes a line through the origin to a line through the origin. Thus A
induces a map A : Pn →Pn [see (1) from §2]. We call such a map a projective linear
transformation.
In terms of homogeneous coordinates, we can describe A : Pn →Pn as follows.
Suppose that A = (aij), where 0 ≤i, j ≤n. If p = (b0 : · · · :bn) ∈Pn, it follows by
matrix multiplication that
(1)
A(p) = A(b0 : · · · :bn) = (a00b0 + · · · + a0nbn : · · · :an0b0 + · · · + annbn)
are homogeneous coordinates for A(p). This formula makes it easy to work with
projective linear transformations. Note that A : Pn →Pn is a bijection, and its
inverse is given by the matrix A−1 ∈GL(n + 1, k). In Exercise 1, you will study the
set of all projective linear transformations in more detail.
Given a variety V ⊆Pn and an element A ∈GL(n + 1, k), we can apply A to all
points of V to get the subset A(V) = {A(p) | p ∈V} ⊆Pn.
Proposition 1. If A ∈GL(n + 1, k) and V ⊆Pn is a variety, then A(V) ⊆Pn is also
a variety. We say that V and A(V) are projectively equivalent.
Proof. Suppose that V = V( f1, . . . , fs), where each fi is a homogeneous polyno-
mial. Since A is invertible, it has an inverse matrix B = A−1. Then for each i, let
gi = fi ◦B. If B = (bij), this means
gi(x0, . . . , xn) = fi(b00x0 + · · · + b0nxn, . . . , bn0x0 + · · · + bnnxn).
It is easy to see that gi is homogeneous of the same total degree as fi, and we leave
it as an exercise to show that
(2)
A(V( f1, . . . , fs)) = V(g1, . . . , gs).
This equality proves the proposition.
□
We can regard A = (aij) as transforming x0, . . . , xn into new coordinates
X0, . . . , Xn deﬁned by
(3)
Xi =
n

j=0
aijxj.
These give homogeneous coordinates on Pn because A ∈GL(n + 1, k). It follows
from (1) that we can think of A(V) as the original V viewed using the new homo-

438
Chapter 8 Projective Algebraic Geometry
geneous coordinates X0, . . . , Xn. An example of how this works will be given in
Proposition 2.
In studying Pn, an important goal is to classify varieties up to projective equiva-
lence. In the exercises, you will show that projective equivalence is an equivalence
relation. As an example of how this works, let us classify hyperplanes H ⊆Pn up
to projective equivalence. Recall from §2 that a hyperplane is deﬁned by a linear
equation of the form
a0x0 + · · · + anxn = 0,
where a0, . . . , an are not all zero.
Proposition 2. All hyperplanes H ⊆Pn are projectively equivalent.
Proof. We will show that H is projectively equivalent to V(x0). Since projective
equivalence is an equivalence relation, this will prove the proposition.
Suppose that H is deﬁned by f = a0x0 + · · · + anxn, and assume in addition that
a0 ̸= 0. Now consider the new homogeneous coordinates
(4)
X0 = a0x0 + a1x1 + · · · + anxn,
X1 = x1
...
Xn = xn.
Then it is easy to see that V( f) = V(X0).
Thus, in the X0, . . . , Xn coordinate system, V( f) is deﬁned by the vanishing of
the ﬁrst coordinate. As explained in (3), this is the same as saying that V( f) and
V(x0) are projectively equivalent via the coefﬁcient matrix
A =
⎛
⎜
⎜
⎜
⎝
a0 a1 · · · an
0
1
· · · 0
...
...
...
...
0
0
· · · 1
⎞
⎟
⎟
⎟
⎠
from (4). This is invertible since a0 ̸= 0. You should check that A(V( f)) = V(x0),
so that we have the desired projective equivalence.
More generally, if ai ̸= 0 in f, a similar argument shows that V( f) is projectively
equivalent to V(xi). We leave it as an exercise to show that V(xi) is projectively
equivalent to V(x0) for all i, and the proposition is proved.
□
In §2, we observed that V(x0) can be regarded as a copy of the projective space
Pn−1. It follows from Proposition 2 that all hyperplanes in Pn look like Pn−1.
Now that we understand hyperplanes, we will study the next simplest case, hy-
persurfaces deﬁned by a homogeneous polynomial of total degree 2.
Deﬁnition 3. A variety V = V( f) ⊆Pn, where f is a nonzero homogeneouspolyno-
mial of total degree 2, is called a quadric hypersurface, or more simply, a quadric.

§6 The Geometry of Quadric Hypersurfaces
439
The simplest examples of quadrics come from analytic geometry. Recall that a
conic section in R2 is deﬁned by an equation of the form
ax2 + bxy + cy2 + dx + ey + f = 0.
To get the projective closure in P2(R), we homogenize with respect to z to get
ax2 + bxy + cy2 + dxz + eyz + fz2 = 0,
which is homogeneous of total degree 2. For this reason, quadrics in P2 are called
conics.
We can classify quadrics up to projective equivalence as follows.
Theorem 4 (Normal Form for Quadrics). Let f = n
i,j=0 aijxixj ∈k[x0, . . . , xn]
be a nonzero homogeneous polynomial of total degree 2, and assume that k is a ﬁeld
not of characteristic 2. Then V( f) is projectively equivalent to a quadric deﬁned by
an equation of the form
c0x2
0 + c1x2
1 + · · · + cnx2
n = 0,
where c0, . . . , cn are elements of k, not all zero.
Proof. Our strategy will be to ﬁnd a change of coordinates Xi = n
j=0 bijxj such
that f has the form
c0X2
0 + c1X2
1 + · · · + cnX2
n.
As in Proposition 2, this will give the desired projective equivalence. Our proof will
be an elementary application of completing the square.
We will use induction on the number of variables. For one variable, the theorem
is trivial since the only homogeneous polynomials of total degree 2 are of the form
a00x2
0. Now assume that the theorem is true when there are n variables.
Given f = n
i,j=0 aijxixj, we ﬁrst claim that by a change of coordinates, we can
assume a00 ̸= 0. To see this, ﬁrst suppose that a00 = 0 and ajj ̸= 0 for some
1 ≤j ≤n. In this case, we set
(5)
X0 = xj,
Xj = x0,
and
Xi = xi for i ̸= 0, j.
Then the coefﬁcient of X2
0 in the expansion of f in terms of X0, . . . , Xn is nonzero.
On the other hand, if all aii = 0, then since f ̸= 0, we must have aij ̸= −aji for some
i ̸= j. Making a change of coordinates as in (5), we may assume that a01 ̸= −a10.
Now set
(6)
X0 = x0,
X1 = x1 −x0,
and
Xi = xi for i ≥2.
We leave it as an easy exercise to show that in terms of X0, . . . , Xn, the polynomial
f has the form n
i,j=0 cijXiXj where c00 = a01 + a10 ̸= 0. This establishes the claim.
Now suppose that f = n
i,j=0 aijxixj where a00 ̸= 0. Let bi = ai0 + a0i and note
that

440
Chapter 8 Projective Algebraic Geometry
1
a00

a00x0 +
n

i=1
bi
2 xi
2
= a00x2
0 +
n

i=1
bix0xi +
n

i,j=1
bibj
4a00
xixj.
Since the characteristic of k is not 2, we know that 2 = 1+1 ̸= 0 and, thus, division
by 2 is possible in k. Now we introduce new coordinates X0, . . . , Xn, where
(7)
X0 = x0 + 1
a00
n

i=1
bi
2 xi
and
Xi = xi for i ≥1.
Writing f in terms of X0, . . . , Xn, all of the terms X0Xi cancel for 1 ≤i ≤n and,
hence, we get a sum of the form
a00X2
0 +
n

i,j=1
dijXiXj.
The sum n
i,j=1 dijXiXj involves the n variables X1, . . . , Xn, so that by our in-
ductive assumption, we can ﬁnd a change of coordinates involving only X1, . . . , Xn
which transforms n
i,j=1 dijXiXj into e1X2
1 + · · · + enX2
n. We can regard this as a co-
ordinate change for X0, X1, . . . Xn which leaves X0 ﬁxed. Then we have a coordinate
change that transforms a00X2
0 +n
i,j=1 dijXiXj into the desired form. This completes
the proof of the theorem.
□
In the normal form c0x2
0+· · ·+cnx2
n given by Theorem 4, some of the coefﬁcients
ci may be zero. By relabeling coordinates, we may assume that ci ̸= 0 if 0 ≤i ≤p
and ci = 0 for i > p. Then the quadric is projectively equivalent to one given by the
equation
(8)
c0x2
0 + · · · + cpx2
p = 0,
c0, . . . , cp nonzero.
There is a special name for the number of nonzero coefﬁcients.
Deﬁnition 5. Let V ⊆Pn be a quadric hypersurface, where the ﬁeld k is inﬁnite and
does not have characteristic 2.
(i) If V is deﬁned by an equation of the form (8), then V has rank p + 1.
(ii) More generally, if V is an arbitrary quadric, then V has rank p + 1 if V is
projectively equivalent to a quadric deﬁned by an equation of the form (8).
For some examples, suppose we use homogeneous coordinates (x :y: z) in P2(R).
Then the three conics deﬁned by
x2 + y2 −z2 = 0, x2 −z2 = 0, x2 = 0
have ranks 3, 2, and 1, respectively. The ﬁrst conic is the projective version of the
circle, whereas the second is the union of two projective lines V(x −z) ∪V(x + z),
and the third is the projective line V(x), which we regard as a degenerate conic of
multiplicity two. (In general, we can regard any rank 1 quadric as a hyperplane of
multiplicity two.)

§6 The Geometry of Quadric Hypersurfaces
441
In Deﬁnition 5, we need to show that the rank of a quadric V is well-deﬁned.
This is a somewhat subtle question since k need not be algebraically closed. The
exercises at the end of the section will give a careful proof that V has a unique rank.
One consequence is that projectively equivalent quadrics have the same rank.
Given a quadric V = V( f), we next show how to compute the rank directly from
the deﬁning polynomial f = n
i,j=0 aijxixj of V. We begin with two observations.
A ﬁrst observation is that we can assume aij = aji for all i, j. This follows by
setting bij = (aij + aji)/2 (remember that k has characteristic different from 2).
An easy computation shows that f = n
i,j=0 bijxixj, and our claim follows since
bij = bji.
A second observation is that we can use matrix multiplication to represent f. The
coefﬁcients of f form an (n + 1) × (n + 1) matrix Q = (aij), which we will assume
to be symmetric by our ﬁrst observation. Let x be the column vector with entries
x0, . . . , xn. We leave it as an exercise to show
f(x) = xtQx,
where xt is the transpose of x.
We can compute the rank of V( f) in terms of Q as follows.
Proposition 6. Let f = xtQx, where Q is an (n + 1) × (n + 1) symmetric matrix.
(i) Given an element A ∈GL(n + 1, k), let B = A−1. Then
A(V( f)) = V(g).
where g(x) = xtBtQBx.
(ii) The rank of the quadric hypersurface V( f) equals the rank of the matrix Q
Proof. To prove (i), we note from (2) that A(V( f)) = V(g), where g = f ◦B. We
compute g as follows:
g(x) = f(Bx) = (Bx)tQ(Bx) = xtBtQBx,
where we have used the fact that (UV)t = VtUt for all matrices U, V such that UV
is deﬁned. This completes the proof of (i).
To prove (ii), ﬁrst note that Q and BtQB have the same rank. This follows since
multiplying a matrix on the right or left by an invertible matrix does not change the
rank.
Now suppose we have used Theorem 4 to ﬁnd a matrix A ∈GL(n + 1, k) such
that g = c0x2
0 + · · · + cpx2
p with c0, . . . , cp nonzero. The matrix of g is a diagonal
matrix with c0, . . . , cp on the main diagonal. If we combine this with part (i), we see
that
BtQB =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
c0 ...
cp
0
...
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,

442
Chapter 8 Projective Algebraic Geometry
where B = A−1. The rank of a matrix is the maximum number of linearly indepen-
dent columns and it follows that BtQB has rank p + 1. The above observation then
implies that Q also has rank p + 1, as desired.
□
When k is an algebraically closed ﬁeld (such as k = C), Theorem 4 and Proposi-
tion 6 show that quadrics are completely classiﬁed by their rank.
Proposition 7. If k is algebraically closed and not of characteristic 2, then a quadric
hypersurface of rank p + 1 is projectively equivalent to the quadric deﬁned by the
equation
p

i=0
x2
i = 0.
In particular, two quadrics are projectively equivalent if and only if they have the
same rank.
Proof. By Theorem 4, we can assume that we have a quadric deﬁned by a poly-
nomial of the form c0x2
0 + · · · + cpx2
p = 0, where p + 1 is the rank. Since k is
algebraically closed, the equation x2 −ci = 0 has a root in k. Pick a root and call it
√ci. Note that √ci ̸= 0 since ci is nonzero. Then set
Xi = √cixi,
0 ≤i ≤p,
Xi = xi,
p < i ≤n.
This gives the desired form and implies that quadrics of the same rank are projec-
tively equivalent. In the discussion following Deﬁnition 5, we noted that projectively
equivalent quadrics have the same rank. Hence the proof is complete.
□
Over the real numbers, the rank is not the only invariant of a quadric hypersur-
face. For example, in P2(R), the conics V1 = V(x2+y2+z2) and V2 = V(x2+y2−z2)
have rank 3 but cannot be projectively equivalent since V1 is empty, yet V2 is not.
In the exercises, you will show given any quadric V( f) with coefﬁcients in R, there
are integers r ≥−1 and s ≥0 with 0 ≤r + s ≤n such that V( f) is projectively
equivalent over R to a quadric of the form
x2
0 + · · · + x2
r −x2
r+1 −· · · −x2
r+s = 0.
(The case r = −1 corresponds to when all of the signs are negative.)
We are most interested in quadrics of maximal rank in Pn.
Deﬁnition 8. A quadric hypersurface in Pn is nonsingular if it has rank n + 1.
A nonsingular quadric is deﬁned by an equation f = xtQx = 0, where Q has
rank n + 1. Since Q is an (n + 1) × (n + 1) matrix, this is equivalent to Q being
invertible. An immediate consequence of Proposition 7 is the following.
Corollary 9. Let k be an algebraically closed ﬁeld not of characteristic 2. Then all
nonsingular quadrics in Pn are projectively equivalent.

§6 The Geometry of Quadric Hypersurfaces
443
In the exercises, you will show that a quadric in Pn of rank p + 1 can be repre-
sented as the join of a nonsingular quadric in Pp with a copy of Pn−p−1. Thus, we
can understand all quadrics once we know the nonsingular ones.
For the remainder of the section, we will discuss some interesting properties of
nonsingular quadrics in P2, P3, and P5. For the case of P2, consider the mapping
F : P1 →P2 deﬁned by
F(u :v) = (u2 : uv:v2),
where (u : v) are homogeneous coordinates on P1. It is straightforward to show that
the image of F is contained in the nonsingular conic V(x0x2 −x2
1). In fact, the map
F : P1 →V(x0x2 −x2
1) is a bijection (see Exercise 11), so that this conic looks like
a copy of P1. When k is algebraically closed, it follows that all nonsingular conics
in P2 look like P1.
When we move to quadrics in P3, the situation is more interesting. Consider the
mapping
σ : P1 × P1 −→P3
which takes (x0 : x1, y0 : y1) ∈P1 × P1 to the point (x0y0 :x0y1 : x1y0 : x1y1) ∈P3.
This map is called a Segre map and its properties were studied in Exercise 14 of §4.
For us, the important fact is that the image of σ is a nonsingular quadric.
Proposition 10. The Segre map σ : P1 × P1 →P3 is one-to-one and its image is the
nonsingular quadric V(z0z3 −z1z2).
Proof. We will use (z0 : z1 :z2 : z3) as homogeneous coordinates on P3. An easy cal-
culation shows that
(9)
σ(P1 × P1) ⊆V(z0z3 −z1z2).
To prove equality, σ suppose that (w0 : w1 :w2 : w3) ∈V(z0z3 −z1z2). If w0 ̸= 0,
then (w0 :w2, w0 :w1) ∈P1 × P1 and
σ(w0 : w2, w0 : w1) = (w2
0 :w0w1 :w0w2 :w1w2).
However, since w0w3 −w1w2 = 0, we can write this as
σ(w0 : w2, w0 : w1) = (w2
0 :w0w1 : w0w2 : w0w3) = (w0 : w1 :w2 : w3).
When a different coordinate is nonzero, the proof is similar, and it follows that (9)
is an equality. The above argument can be adapted to show that σ is one-to-one (we
leave the details as an exercise), and it is also easy to see that V(z0z3 −z1z2) is
nonsingular. This proves the proposition.
□
Proposition 10 has some nice consequences concerning lines on the quadric sur-
face V(z0z3 −z1z2) ⊆P3. But before we can discuss this, we need to learn how to
describe projective lines in P3.
Two points p ̸= q in P3 give linearly independent vectors ˆp = (a0, a1, a2, a3) and
ˆq = (b0, b1, b2, b3) in k4. Now consider the map F : P1 →P3 given by

444
Chapter 8 Projective Algebraic Geometry
F(u :v) = (a0u −b0v:a1u −b1v: a2u −b2v: a3u −b3v),
which for later purposes, we will write as
(10)
F(u :v) = u(a0 :a1 : a2 : a3) −v(b0 : b1 : b2 :b3) = up −vq.
Since ˆp and ˆq are linearly independent, a0u −b0v, . . . , a3u −b3v cannot vanish
simultaneously when (u : v) ∈P1, so that F is deﬁned on all of P1. In Exercise 13,
you will show that the image of F is a variety L ⊆P3 deﬁned by linear equations.
We call L the projective line (or more simply, the line) determined by p and q. Note
that L contains both p and q. In the exercises, you will show that all lines in P3 are
projectively equivalent and can be regarded as copies of P1 sitting inside P3.
Using the Segre map σ, we can identify the quadric V = V(z0z3−z1z2) ⊆P3 with
P1 × P1. If we ﬁx b = (b0 : b1) ∈P1, the image in V of P1 × {b} under σ consists of
the points (ub0 : ub1 : vb0 :vb1) as (u : v) ranges over P1. By (10), this is the projective
line through the points (b0 :b1 : 0 :0) and (0 : 0 :b0 : b1). Hence, b ∈P1 determines a
line Lb = σ(P1 ×{b}) lying on the quadric V. If b ̸= b′, one can easily show that Lb
does not intersect Lb′ and that every point on V lies on a unique such line. Thus, V
is swept out by the family {Lb | b ∈P1} of nonintersecting lines. Such a surface is
called a ruled surface. In the exercises, you will show that {σ({a} × P1) | a ∈P1}
is a second family of lines that sweeps out V. If we look at V in the afﬁne space
where z0 = 1, then V is deﬁned by z3 = z1z2, and we get the following graph:
The two families of lines on V are clearly visible in this picture. Over an alge-
braically closed ﬁeld, Corollary 9 implies that all nonsingular quadrics in P3 look
like V(z0z3 −z1z2) up to projective equivalence. Over the real numbers, however,
there are more possibilities—see Exercise 8.
We conclude this section with the problem of describing lines in P3, which will
lead to an interesting quadric in P5. To motivate what follows, let us ﬁrst recall
the situation of lines in P2. Here, a line L ⊆P2 is deﬁned by a single equation

§6 The Geometry of Quadric Hypersurfaces
445
A0x0 + A1x1 + A2x2 = 0. In Exercise 11 of §1, we showed that (A0 :A1 :A2) can be
regarded as the "homogeneous coordinates" of L and that the set of all lines forms
the dual projective space P2∨.
It makes sense to ask the same questions for P3. In particular, can we ﬁnd "ho-
mogeneous coordinates" for lines in P3? We saw in (10) that a line L ⊆P3 can be
projectively parametrized using two points p, q ∈L. This is a good start, but there
are inﬁnitely many such pairs on L. How do we get something unique out of this?
The idea is the following. Points p ̸= q ∈L give vectors ˆp = (a0, a1, a2, a3) and
ˆq = (b0, b1, b2, b3) in k4. Then consider the 2 × 4 matrix whose rows are ˆp and ˆq:
Ω =

a0 a1 a2 a3
b0 b1 b1 b3

.
We will create coordinates for L using the determinants of 2 × 2 submatrices of Ω.
If we number the columns of Ω using 0, 1, 2, 3, then the determinant formed using
columns i and j will be denoted wij. We can assume 0 ≤i < j ≤3, and we get the
six determinants
(11)
w01 = a0b1 −a1b0,
w02 = a0b2 −a2b0,
w03 = a0b3 −a3b0,
w12 = a1b2 −a2b1,
w13 = a1b3 −a3b1,
w23 = a2b3 −a3b2.
We will encode them in the 6-tuple
(w01, w02, w03, w12, w13, w23) ∈k6.
The wij are called the Plücker coordinates of the line L. A ﬁrst observation is that
any line has at least one nonzero Plücker coordinate. To see why, note that Ω has
row rank 2 since ˆp and ˆq are linearly independent. Hence the column rank is also
2, so that there must be two linearly independent columns. These columns give a
nonzero Plücker coordinate. From (w01, w02, w03, w12, w13, w23) ∈k6 \ {0}, we get
ω(p, q) = (w01 :w02 :w03 : w12 :w13 : w23) ∈P5.
To see how the Plücker coordinates depend on the chosen points p, q ∈L, sup-
pose that we pick a different pair p′ ̸= q′ ∈L. By (10), L can be described as
L = {up −vq | (u, v) ∈P1}.
In particular, we can write
p′ = up −vq,
q′ = sp −tq

446
Chapter 8 Projective Algebraic Geometry
for distinct points (u : v), (s: t) ∈P1. We leave it as an exercise to show that
ω(p′, q′) = (w′
01 : w′
02 :w′
03 : w′
12 : w′
13 :w′
23) ∈P5,
where w′
ij = (vs −ut)wij for all 0 ≤i < j ≤3. It is easy to see that vs −ut ̸= 0
since (u : v) ̸= (s: t) in P1. Thus,
ω(p′, q′) = ω(p, q)
in P5. This shows that ω(p, q) gives us a point in P5 which depends only on L.
Hence, a line L determines a well-deﬁned point ω(L) ∈P5.
As we vary L over all lines in P3, the Plücker coordinates ω(L) will describe a
certain subset of P5. An straightforward calculation using (11) shows that
w01w23 −w02w13 + w03w12 = 0
for all sets of Plücker coordinates. If we let zij, 0 ≤i < j ≤3, be homogeneous
coordinates on P5, it follows that the points ω(L) all lie in the nonsingular quadric
V(z01z23 −z02z13 + z03z12) ⊆P5. Let us prove that this quadric is exactly the set of
lines in P3.
Theorem 11. The map
{lines in P3} −→V(z01z23 −z02z13 + z03z12)
which sends a line L ⊆P3 to its Plücker coordinates ω(L) ∈V(z01z23 −z02z13 +
z03z12) is a bijection.
Proof. The strategy of the proof is to show that a line L ⊆P3 can be recon-
structed from its Plücker coordinates. Given two points p = (a0 : a1 :a2 :a3) and
q = (b0 :b1 :b2 : b3) on L, then for the corresponding vectors ˆp, ˆq ∈k4, one can
check that (11) implies the following equations in k4:
(12)
b0ˆp −a0ˆq = (0, −w01, −w02, −w03),
b1ˆp −a1ˆq = (w01, 0, −w12, −w13),
b2ˆp −a2ˆq = (w02, w12, 0, −w23),
b3ˆp −a3ˆq = (w03, w13, w23, 0).
It may happen that some of these vectors are 0, but whenever they are nonzero, it
follows from (10) that they correspond to points of L ⊆P3.
To prove that ω is one-to-one, suppose that we have lines L and L′ such that
ω(L) = ω(L′) in P5. In terms of Plücker coordinates, this means that there is a
nonzero λ such that wij = λw′
ij for all 0 ≤i < j ≤3. We know that some Plücker
coordinate of L is nonzero, and by permuting the coordinates in P3, we can assume
w01 ̸= 0. Then (12) implies that in P3, the points

§6 The Geometry of Quadric Hypersurfaces
447
P = (0 : −w′
01 :−w′
02 :−w′
03) = (0 :−λw01 : −λw02 : −λw03)
= (0 : −w01 :−w02 :−w03),
Q = (w′
01 :0 : −w′
12 : −w′
13) = (λw01 :0 : −λw12 :−λw13)
= (w01 :0 : −w12 : −w13)
lie on both L and L′. Since there is a unique line through two points in P3 (see
Exercise 14), it follows that L = L′. This proves that our map is one-to-one.
To see that ω is onto, pick a point
(w01 : w02 :w03 : w12 : w13 :w23) ∈V(z01z23 −z02z13 + z03z12).
By changing coordinates in P3, we can assume w01 ̸= 0. Then the ﬁrst two vectors
in (12) are nonzero and, hence, determine a line L ⊆P3. Using the deﬁnition of
ω(L) and the relation w01w23 −w02w13 + w03w12 = 0, it is straightforward to show
that the wij are the Plücker coordinates of L (see Exercise 16 for the details). This
shows that ω is onto and completes the proof of the theorem.
□
A nice consequence of Theorem 11 is that the set of lines in P3 can be given
the structure of a projective variety. As we observed at the end of Chapter 7, an
important idea in algebraic geometry is that a set of geometrically interesting objects
often forms a variety in some natural way.
Theorem 11 can be generalized in many ways. One can study lines in Pn, and it
is even possible to deﬁne Plücker coordinates for linear subspaces in Pn of arbitrary
dimension. This leads to the study of what are called Grassmannians. Using Plücker
coordinates, a Grassmannian can be given the structure of a projective variety, al-
though there is usually more than one deﬁning equation. See Exercise 17 for the
case of lines in P4.
We can also think of Theorem 11 from an afﬁne point of view. We already know
that there is a natural bijection
{lines through the origin in k4} ∼= {points in P3},
and in the exercises, you will describe a bijection
{planes through the origin in k4} ∼= {lines in P3}.
Thus, Theorem 11 shows that planes through the origin in k4 have the structure of a
quadric hypersurface in P5. In the exercises, you will see that this has a surprising
connection with reduced row echelon matrices. More generally, the Grassmannians
mentioned in the previous paragraph can be described in terms of subspaces of a
certain dimension in afﬁne space kn+1.
This completes our discussion of quadric hypersurfaces, but by no means ex-
hausts the subject. The classic books by SEMPLE and ROTH (1949) and HODGE and
PEDOE (1968) contain a wealth of material on quadric hypersurfaces (and many
other interesting projective varieties as well). A more recent reference for quadrics
is HARRIS (1995).

448
Chapter 8 Projective Algebraic Geometry
EXERCISES FOR §6
1. The set GL(n + 1, k) is closed under inverses and matrix multiplication and is a group
in the terminology of Appendix A. In the text, we observed that A ∈GL(n + 1, k)
induces a projective linear transformation A : Pn →Pn. To describe the set of all such
transformations, we deﬁne a relation on GL(n + 1, k) by
A′ ∼A ⇐⇒A′ = λA
for some
λ ̸= 0.
a. Prove that ∼is an equivalence relation. The set of equivalence classes for ∼is de-
noted PGL(n + 1, k).
b. Show that if A ∼A′ and B ∼B′, then AB ∼A′B′. Hence, the matrix product
operation is well-deﬁned on the equivalence classes for ∼and, thus, PGL(n + 1, k)
has the structure of a group. We call PGL(n + 1, k) the projective linear group.
c. Show that two matrices A, A′ ∈GL(n + 1, k) deﬁne the same mapping Pn →Pn if
and only if A′ ∼A. It follows that we can regard PGL(n + 1, k) as a set of invertible
transformations on Pn.
2. Prove equation (2) in the proof of Proposition 1.
3. Prove that projective equivalence is an equivalence relation on the set of projective vari-
eties in Pn.
4. Prove that the hyperplanes V(xi) and V(x0) are projectively equivalent. Hint: See (5).
5. This exercise is concerned with the proof of Theorem 4.
a. If f = n
i,j=0 aijxixi has a01 ̸= −a10 and aii = 0 for all i, prove that the change of
coordinates (6) transforms f into n
i,j=0 cijXiXj where c00 = a01 + a10.
b. If f = n
i,j=0 aijxixj has a00 ̸= 0, verify that the change of coordinates (7) transforms
f into a00X2
0 + n
i,j=1 dijXiXj.
6. If f = n
i,j=0 aijxixj, let Q be the (n + 1) × (n + 1) matrix (aij).
a. Show that f (x) = xtQx.
b. Suppose that k has characteristic 2 (e.g., k = F2), and let f = x0x1. Show that there
is no symmetric 2 × 2 matrix Q with entries in k such that f (x) = xtQx.
7. Use the proofs of Theorem 4 and Proposition 7 to write each of the following as a sum
of squares. Assume that k = C.
a. x0x1 + x0x2 + x2
2.
b. x2
0 + 4x1x3 + 2x2x3 + x2
4.
c. x0x1 + x2x3 −x4x5.
8. Let f = n
i,j=0 aijxixj ∈R[x0, . . . , xn] be nonzero.
a. Show that there are integers r ≥−1 and s ≥0 with 0 ≤r + s ≤n such that f can be
brought to the form
x2
0 + · · · + x2
r −x2
r+1 −· · · −x2
r+s
by a suitable coordinate change with real coefﬁcients. One can prove that the integers
r and s are uniquely determined by f .
b. Assume n = 3 and f = x2
0 + · · · + x2
r −x2
r+1 −· · · −x2
3 as in part (a). Of the ﬁve
possible values r = −1, . . . , 3, show that V( f ) is empty in two cases, and in the
remaining three, V( f ) ∩U0 ⊆R3 is one of the standard quadric surfaces studied in
multivariable calculus.
9. Let f = n
i,j=0 aijxixj ∈k[x0, . . . , xn] be nonzero. In the text, we observed that V( f )
is a nonsingular quadric if and only if det(aij) ̸= 0. We say that V( f ) is singular if it
is not nonsingular. In this exercise, we will explore a nice way to characterize singular
quadrics.

§6 The Geometry of Quadric Hypersurfaces
449
a. Show that f is singular if and only if there exists a point a = (a0 : · · · : an) ∈Pn such
that
∂f
∂x0 (a) = · · · = ∂f
∂xn (a) = 0.
b. If a ∈Pn has the property described in part (a), prove that a ∈V( f ). In general,
a point a of a hypersurface V( f ) (quadric or of higher degree) is called a singular
point of V( f ) provided that all of the partial derivatives of f vanish at a. Hint: Use
Exercise 17 of §2.
10. Let V( f ) ⊆Pn be a quadric of rank p + 1, where 0 < p < n. Prove that there are X, Y ⊆
Pn such that (1) X ≃V(g) ⊆Pp for some nonsingular quadric g, (2) Y ≃Pn−p−1, (3)
X ∩Y = ∅, and (4) V( f ) is the join X ∗Y, which is deﬁned to be the set of all lines in
Pn connecting a point of X to a point of Y (and if X = ∅, we set X ∗Y = Y). Hint: Use
Theorem 4.
11. We will study the map F : P1 →P2 deﬁned by F(u : v) = (u2 : uv : v2).
a. Prove that the image of F lies in V(x0x2 −x2
1).
b. Prove that F : P1 →V(x0x2 −x2
1) is a bijection. Hint: Adapt the methods used in the
proof of Proposition 10.
12. This exercise will study the Segre map σ : P1 × P1 →P3 deﬁned in the text.
a. Prove that the image of σ lies in the quadric V(z0z3 −z1z2).
b. Use the hint given in the text to prove that σ is one-to-one.
13. In this exercise and the next, we will work out some basic facts about lines in Pn. We start
with points p ̸= q ∈Pn, which correspond to linearly independent vectors ˆp, ˆq ∈kn+1.
a. Adapting the notation used in (10), we can deﬁne a map F : P1 →Pn by F(u : v) =
up −vq. Show that this map is deﬁned on all of P1 and is one-to-one.
b. Let ℓ= a0x0 +· · ·+anxn be a linear homogeneous polynomial. Show that ℓvanishes
on the image of F if and only if p, q ∈V(ℓ).
c. Our goal is to show that the image of F is a variety deﬁned by linear equations. Let
Ω be the 2 × (n + 1) matrix whose rows are ˆp and ˆq. Note that Ω has rank 2. If
we multiply column vectors in kn+1 by Ω, we get a linear map Ω : kn+1 →k2.
Use results from linear algebra to show that the kernel (or nullspace) of this linear
map has dimension n −1. Pick a basis v1, . . . , vn−1 of the kernel, and let ℓi be the
linear polynomial whose coefﬁcients are the entries of vi. Then prove that the image
of F is V(ℓ1 . . . , ℓn−1). Hint: Study the subspace of kn+1 deﬁned by the equations
ℓ1 = · · · = ℓn−1 = 0.
14. The exercise will discuss some elementary properties of lines in Pn.
a. Given points p ̸= q in Pn, prove that there is a unique line through p and q.
b. If L is a line in Pn and Ui ∼= kn is the afﬁne space where xi = 1, then show that L∩Ui,
is either empty or a line in kn in the usual sense.
c. Show that all lines in Pn are projectively equivalent. Hint: In part (c) of Exercise 13,
you showed that a line L can be written L = V(ℓ1, . . . , ℓn−1). Show that you can ﬁnd
ℓn and ℓn+1 so that X0 = ℓ1, . . . , Xn = ℓn+1 is a change of coordinates. What does L
look like in the new coordinate system?
15. Let σ : P1 × P1 →P3 be the Segre map.
a. Show that L′
a = σ({a} × P1) is a line in P3 for all a ∈P1.
b. Show that every point of V(z0z3 −z1z2) lies on a unique line L′
a. This proves that the
family of lines {L′
a | a ∈P1} sweeps out the quadric.
16. This exercise will deal with the proof of Theorem 11.
a. Show that the Plücker coordinates w′
ij of p′ = up −vq and q′ = sp −tq are related
to the Plücker coordinates wij of p and q via w′
ij = (vs −ut)wij.
b. Use (11) to show that Plücker coordinates satisfy w01w23 −w02w13 + w03w12 = 0.
c. Complete the proof of Theorem 11 by showing that the map ω is onto.
17. In this exercise, we will study Plücker coordinates for lines in P4

450
Chapter 8 Projective Algebraic Geometry
a. Let L ⊆P4 be a line. Using the homogeneous coordinates of two points p, q ∈L,
deﬁne Plücker coordinates and show that we get a point ω(L) ∈P9 that depends only
on L.
b. Find the relations between the Plücker coordinates and use these to ﬁnd a variety
V ⊆P4 such that ω(L) ∈V for all lines L.
c. Show that the map sending a line L ⊆P4 to ω(L) ∈V is a bijection.
18. Show that there is a one-to-one correspondence between lines in P3 and planes through
the origin in k4. This explains why a line in P3 is different from a line in k3 or k4.
19. There is a nice connection between lines in P3 and 2 × 4 reduced row echelon matrices
of rank 2. Let V = V(z01z23 −z02z13 + z03z12) be the quadric of Theorem 11.
a. Show that there is a one-to-one correspondence between reduced row echelon matri-
ces of the form

1 0 a b
0 1 c
d

and points in the afﬁne portion V ∩U01, where U01 is the afﬁne space in P5 deﬁned
by z01 = 1. Hint: The rows of the above matrix determine a line in P3. What are its
Plücker coordinates?
b. The matrices given in part (a) do not exhaust all possible 2 × 4 reduced row echelon
matrices of rank 2. For example, we also have the matrices

1 a 0 b
0 0 1 c

.
Show that there is a one-to-one correspondence between these matrices and points of
V ∩V(z01) ∩U02.
c. Show that there are four remaining types of 2 × 4 reduced row echelon matrices of
rank 2 and prove that each of these is in a one-to-one correspondence with a cer-
tain portion of V. Hint: The columns containing the leading 1's will correspond to a
certain Plücker coordinate being 1.
d. Explain directly (without using V or Plücker coordinates) why 2 × 4 reduced row
echelon matrices of rank 2 should correspond uniquely to lines in P3. Hint: See Ex-
ercise 18.
20. Let k be an algebraically closed ﬁeld which does not have characteristic 2, and suppose
that f , g are nonzero homogeneous polynomials of total degree 2 satisfying V( f ) =
V(g). Use the Nullstellensatz to prove that f = cg for some nonzero c ∈k. Hint:
Proposition 9 of Chapter 4, §2 will be useful. There are three cases to consider: (1) f
is irreducible; (2) f = ℓ1ℓ2, where ℓ1, ℓ2 are linear and neither is a multiple of the other;
and (3) f = ℓ2.
21. When the ﬁeld k does not have characteristic 2, Proposition 6 shows that a nonzero
homogeneous polynomial f of total degree 2 has a well-deﬁned rank, denoted rank( f ).
In order to prove that a quadric hypersurface V has a well-deﬁned rank, we need to show
that V = V( f ) = V(g) implies rank( f ) = rank(g). If k is algebraically closed, this
follows from the previous exercise. Here is a proof that works when k is inﬁnite. The
strategy will be to ﬁrst show that if rank( f ) = p + 1, then rank(g) ≤p + 1. This is
obvious if p = n, so suppose that p < n. By a change of coordinates, we may assume
f = c0x2
0 + · · · + cpx2
p. Then write
g = h1(x0, . . . , xp) +
p

i=0
xiℓi(xp+1, . . . , xn) + h2(xp+1, . . . , xn).
a. Show that h2 = 0. Hint: If (bp+1 : · · · : bn) ∈Pn−p−1 is arbitrary, then the point
(0 : · · · : 0 : bp+1 : · · · : bn) ∈Pn

§7 Bezout's Theorem
451
lies is V( f ). Then use V( f ) = V(g) and Proposition 5 of Chapter 1, §1.
b. Show that ℓi = 0 all i. Hint: Suppose there is 1 ≤i ≤p such that ℓi = · · ·+bxj +· · ·
for some b ̸= 0 and p + 1 ≤j ≤n. For any λ ∈k, set
p = (0 : · · · : 0 : 1 : 0 : · · · : 0 : λ : 0 : · · · : 0) ∈Pn,
where the 1 is in position i and the λ is in position j. Show that f (p) = 1 and
g(p) = h1(0, . . . , 1, . . . , 0) + λb. Show that a careful choice of λ makes g(p) = 0
and derive a contradiction.
c. Show g = h1(x0, . . . , xp) implies rank(g) ≤p + 1. Hint: Adapt the proof of Theo-
rem 4.
d. Complete the proof that the rank of V is well-deﬁned.
22. This exercise will study empty quadrics.
a. Show that V(x2
0 + · · · + x2
n) ⊆Pn(R) is an empty quadric of rank n + 1.
b. Suppose we have a ﬁeld k and a quadric V = V( f ) ⊆Pn(k) which is empty, i.e.,
V = ∅. Prove that V has rank n + 1.
23. Let f = x2
0 + x2
1 and g = x2
0 + 2x2
1 in Q[x0, x1, x2].
a. Show that V( f ) = V(g) = {(0 : 0 : 1)} in P2(Q).
b. In contrast to Exercise 20, show that f and g are not multiples of each other and in
fact are not projectively equivalent.
§7
Bezout's Theorem
This section will explore what happens when two curves intersect in the plane. We
are particularly interested in the number of points of intersection. The following
examples illustrate why the answer is especially nice when we work with curves
in P2(C), the projective plane over the complex numbers. We will also see that we
need to deﬁne the multiplicity of a point of intersection. Fortunately, the resultants
we learned about in Chapter 3 will make this relatively easy to do.
Example 1. First consider the intersection of a parabola and an ellipse. Suppose that
the parabola is y = x2 and the ellipse is x2 + 4(y −λ)2 = 4, where λ is a parameter
we can vary. For example, when λ = 2 or 0, we get the pictures:
x
y
λ = 2
x
y
λ = 0

452
Chapter 8 Projective Algebraic Geometry
Over R, we get different numbers of intersections, and it is clear that there are values
of λ for which there are no points of intersection (see Exercise 1). What is more
interesting is that over C, we have four points of intersection in both of the above
cases. For example, when λ = 0, we can eliminate x from y = x2 and x2 + 4y2 = 4
to obtain y + 4y2 = 4. This leads to the solutions
(x, y) =
,
±
-
−1 +
√
65
8
, −1 +
√
65
8
.
,
,
±
-
−1 −
√
65
8
, −1 −
√
65
8
.
.
The ﬁrst two are real and the second two are complex (since −1 −
√
65 < 0). You
can also check that when λ = 2, working over C gives no new solutions beyond the
four we see in the picture for λ = 2 (see Exercise 1).
Hence, the number of intersections seems to be more predictable when we work
over the complex numbers. As conﬁrmation, you can check that in the cases where
there are no points of intersection over R, we still get four points over C (see
Exercise 1).
However, even over C, some unexpected things can happen. For example, sup-
pose we intersect the parabola with the ellipse where λ = 1:
x
y
λ = 1
Here, we see only three points of intersection, and this remains true over C. But the
origin is clearly a "special" type of intersection since the two curves are tangent at
this point. As we will see later, this intersection has multiplicity two, while the other
two intersections have multiplicity one. If we add up the multiplicities of the points
of intersection, we still get four.
Example 2. Now consider the intersection of our parabola y = x2 with a line L.
It is easy to see that in most cases, this leads to two points of intersection over
C, provided multiplicities are counted properly (see Exercise 2). However, if we
intersect with a vertical line, then we get the following picture:

§7 Bezout's Theorem
453
There is just one point of intersection, even over C, and since multiplicities seem to
involve tangency, it should be an intersection of multiplicity one. Yet we want the
answer to be two, since this is what we get in the other cases. Where is the other
point of intersection?
If we change our point of view and work in the projective plane P2(C), the above
question is easy to answer: the missing point is "at inﬁnity." To see why, let z be
the third variable. Then we homogenize y = x2 to get the homogeneous equation
yz = x2, and a vertical line x = c gives the projective line x = cz. Eliminating x,
we get yz = c2z2, which is easily solved to obtain (x :y: z) = (c: c2 : 1) or (0 :1 : 0)
(remember that these are homogeneous coordinates). The ﬁrst lies in the afﬁne part
(where z = 1) and is the point we see in the above picture, while the second is on
the line at inﬁnity (where z = 0).
Example 3. In P2(C), consider the two curves given by C = V(x2 −z2) and D =
V(x2y −xz2 −xyz + z3). It is easy to check that (1 : b :1) ∈C ∩D for any b ∈C,
so that the intersection C ∩D is inﬁnite! To see how this could have happened,
consider the factorizations
x2 −z2 = (x −z)(x + z), x2y −xz2 −xyz + z3 = (x −z)(xy −z2).
Thus, C is a union of two projective lines and D is the union of a line and a conic.
In fact, these are the irreducible components of C and D in the sense of §3 (see
Proposition 4 below). We now see where the problem occurred: C and D have a
common irreducible component V(x −z), so of course their intersection is inﬁnite.
These examples explain why we want to work in P2(C). Hence, for the rest of the
section, we will use C and write P2 instead of P2(C). In this context, a curve is a pro-
jective variety V( f) deﬁned by a nonzero homogeneous polynomial f ∈C[x, y, z].
Our examples also indicate that we should pay attention to multiplicities of inter-
sections and irreducible components of curves. We begin by studying irreducible
components.
Proposition 4. Let f ∈C[x, y, z] be a nonzero homogeneous polynomial. Then the
irreducible factors of f are also homogeneous, and if we factor f into irreducibles:

454
Chapter 8 Projective Algebraic Geometry
f = f a1
1 · · · f as
s ,
where fi is not a constant multiple of fj for i ̸= j, then
V( f) = V( f1) ∪· · · ∪V( fs)
is the minimal decomposition of V( f) into irreducible components in P2. Further-
more,
I(V( f)) =
"
⟨f⟩= ⟨f1 · · · fs⟩.
Proof. First, suppose that f factors as f = gh for some polynomials g, h ∈C[x, y, z].
We claim that g and h must be homogeneous since f is. To prove this, write g =
gm + · · · + g0, where gi is homogeneous of total degree i and gm ̸= 0. Similarly let
h = hn + · · · + h0. Then
f = gh = (gm + · · · + g0)(hn + · · · + h0)
= gmhn + terms of lower total degree.
Since f is homogeneous, we must have f = gmhn, and with a little more argument,
one can conclude that g = gm and h = hn (see Exercise 3). Thus g and h are
homogeneous. From here, it follows easily that the irreducible factors f are also
homogeneous.
Now suppose f factors as above. Then V( f) = V( f1)∪· · ·∪V( fs) follows imme-
diately, and this is the minimal decomposition into irreducible components by the
projective version of Exercise 10 from Chapter 4, §6. Since V( f) is nonempty (see
Exercise 5), the assertion about I(V( f)) follows from the Projective Nullstellensatz
and Proposition 9 of Chapter 4, §2.
□
A consequence of Proposition 4 is that every curve C ⊆P2 has a "best" deﬁning
equation. If C = V( f) for some homogeneous polynomial f, then the proposition
implies that I(C) = ⟨f1 · · · fs⟩, where f1, . . . , fs are distinct irreducible factors of f.
Thus, any other polynomial deﬁning C is a multiple of f1 · · · fs, so that f1 · · · fs = 0
is the deﬁning equation of smallest total degree. In the language of Chapter 4, §2,
f1 · · · fs is a reduced (or square-free) polynomial. Hence, we call f1 · · · fs = 0 the
reduced equation of C. This equation is unique up to multiplication by a nonzero
constant.
When we consider the intersection of two curves C and D in P2, we will assume
that C and D have no common irreducible components. This means that their deﬁn-
ing polynomials have no common factors. Our goal is to relate the number of points
in C ∩D to the degrees of their reduced equations.
The main tool we will use is the resultant of the deﬁning equations of C and D.
Resultants were discussed in §6 of Chapter 3. Readers are advised to review that
section up through Corollary 7.
The following lemma will play an important role in our study of this problem.
Lemma 5. Let f, g ∈C[x, y, z] be homogeneous of total degree m, n respectively.
Assume that f(0, 0, 1) and g(0, 0, 1) are nonzero.

§7 Bezout's Theorem
455
(i) The resultant Res( f, g, z) is a homogeneous polynomial in x and y of total degree
mn if it is nonzero.
(ii) If f, g have no common factors in C[x, y, z], then Res( f, g, z) is nonzero.
Proof. First, write f and g as polynomials in z:
f = a0zm + · · · + am,
g = b0zn + · · · + bn,
and observe that since f is homogeneous of total degree m, each ai ∈C[x, y] must be
homogeneous of degree i. Furthermore, f(0, 0, 1) ̸= 0 implies that a0 is a nonzero
constant. Similarly, bi, is homogeneous of degree i and b0 ̸= 0.
By Chapter 3, §6, the resultant is given by the (m + n) × (m + n) determinant
Res( f, g, z) = det
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
a0
...
...
am
a0
...
...
am



n columns
b0
...
...
bn
b0
...
...
bn
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠



m columns
where the empty spaces are ﬁlled by zeros. To show that Res( f, g, z) is homoge-
neous of degree mn, let cij denote the ij-th entry of the matrix. From the pattern of
the above matrix, you can check that once we exclude the entries that are obviously
zero, we have
cij =

ai−j
if j ≤n
bn+i−j
if j > n.
Thus, a nonzero cij is homogeneous of total degree i −j (if j ≤n) or n + i −j (if
j > n). By Proposition 1 of Appendix A, §4, the determinant giving Res( f, g, z) is
a sum of products
±
m+n
!
i=1
ciσ(i),
where σ is a permutation of {1, . . ., m + n}. We can assume that each ciσ(i) in the
product is nonzero. If we write the product as
±
!
σ(i)≤n
ciσ(i)
!
σ(i)>n
ciσ(i),
then, by the above paragraph, this product is a homogeneous polynomial of degree

σ(i)≤n
(i −σ(i)) +

σ(i)>n
(n + i −σ(i)).

456
Chapter 8 Projective Algebraic Geometry
Since σ is a permutation of {1, . . . , m + n}, the ﬁrst sum has n terms and the second
has m, and all i's between 1 and m + n appear exactly once. Thus, we can rearrange
the sum to obtain
mn +
m+n

i=1
i −
m+n

i=1
σ(i) = mn.
Thus, Res( f, g, z) is a sum of homogeneous polynomials of degree mn, proving (i).
For (ii), let k = C(x, y) be the ﬁeld of rational functions in x, y over C. Let
f = f1 · · · fs be the irreducible factorization of f in C[x, y, z]. Since f = b0zm + · · ·
for b0 ∈C \ {0} and m > 0, it follows that every fi has positive degree in z and
hence is irreducible in k[z] by Proposition 3 of Appendix A, §2. Thus, the irreducible
factorization of f in k[z] comes from its factorization in C[x, y, z]. The same is clearly
true for g.
Now suppose that Res( f, g, z) is the zero polynomial. If we regard f and g as
elements of k[z], then Proposition 3 of Chapter 3, §6 implies that f and g have a
common factor in k[z]. In particular, they have a common irreducible factor in k[z].
By the above paragraph, this implies that they have a common irreducible factor in
C[x, y, z], which contradicts the hypothesis of (ii).
□
This lemma shows that the resultant Res( f, g, z) is homogeneous in x and y.
Homogeneous polynomials in two variables have an especially simple structure.
Lemma 6. Let h ∈C[x, y] be a nonzero homogeneous polynomial. Then h can be
written in the form
h = c(s1x −r1y)m1 · · · (stx −rty)mt,
where c ̸= 0 in C and (r1 :s1), . . . , (rt : st) are distinct points of P1. Furthermore,
V(h) = {(r1 :s1), . . . , (rt :st)} ⊆P1.
Proof. This follows from Exercise 19 of §2.
□
As a ﬁrst application of these lemmas, we show how to bound the number of
points in the intersection of two curves using the degrees of their reduced equations.
Theorem 7. Let C and D be projective curves in P2 with no common irreducible
components. If the degrees of the reduced equations for C and D are m and n re-
spectively, then C ∩D is ﬁnite and has at most mn points.
Proof. Suppose that C∩D has more than mn points. Choose mn+1 of them, which
we label p1, . . . , pmn+1, and for 1 ≤i < j ≤mn + 1, let Lij be the line through pi
and pj. Then pick a point q ∈P2 such that
(1)
q ̸∈C ∪D ∪

i<j
Lij
(in Exercise 5 you will prove carefully that such points exist).

§7 Bezout's Theorem
457
As in §6, a matrix A ∈GL(3, C) gives a map A : P2 →P2. It is easy to ﬁnd
an A such that A(q) = (0 :0 : 1) (see Exercise 5). If we regard A as giving new
coordinates for P2 (see (3) in §6), then the point q has coordinates (0 : 0 :1) in the
new system. We can thus assume that q = (0 : 0 :1) in (1).
Now suppose that C = V( f) and D = V(g), where f and g are reduced of de-
grees m and n respectively. Then (1) implies f(0, 0, 1) ̸= 0 since (0 : 0 : 1) ̸∈C, and
g(0, 0, 1) ̸= 0 follows similarly. Since f and g have no common factors, Lemma 5
implies that the resultant Res( f, g, z) is a nonzero homogeneous polynomial of de-
gree mn in x, y.
If we let pi = (ui :vi :wi), then since the resultant is in the ideal generated by f
and g (Proposition 5 of Chapter 3, §6), we have
(2)
Res( f, g, z)(ui, vi) = 0.
Note that the line connecting q = (0 : 0 :1) to pi = (ui : vi : wi) intersects z = 0 in
the point (ui : vi : 0) (see Exercise 5). The picture is as follows:
z = 0
C
D
(0:0:1)
(ui:vi:0)
pi
The map taking a point (u : v: w) ∈P2 \ {(0 :0 :1)} to (u : v: 0) is an example of a
projection from a point to a line. Hence, (2) tells us that Res( f, g, z) vanishes at the
points obtained by projecting the pi ∈C ∩D from (0 : 0 :1) to the line z = 0.
By (1), (0 : 0 :1) lies on none of the lines connecting pi and pj, which implies that
the points (ui : vi : 0) are distinct for i = 1, . . . , mn + 1. If we regard z = 0 as a copy
of P1 with homogeneous coordinates x, y, then we get distinct points (ui : vi) ∈P1,
and the homogeneous polynomial Res( f, g, z) vanishes at all mn + 1 of them. By
Lemmas 5 and 6, this is impossible since Res( f, g, z) is nonzero of degree mn, and
the theorem follows.
□

458
Chapter 8 Projective Algebraic Geometry
Now that we have a criterion for C ∩D to be ﬁnite, the next step is to deﬁne an
intersection multiplicity for each point p ∈C ∩D. There are a variety of ways this
can be done, but the simplest involves the resultant.
Thus, we deﬁne the intersection multiplicity as follows. Let C and D be curves in
P2 with no common components and reduced equations f = 0 and g = 0. For each
pair of points p ̸= q in C ∩D, let Lpq be the projective line connecting p and q. Pick
a matrix A ∈GL(3, C) such that in the new coordinate system given by A, we have
(3)
(0 :0 : 1) ̸∈C ∪D ∪

p̸=q in C∩D
Lpq.
(Example 9 below shows how such coordinate changes are done.) As in the proof
of Theorem 7, if p = (u : v:w) ∈C ∩D, then the resultant Res( f, g, z) vanishes at
(u : v), so that by Lemma 6, vx −uy is a factor of Res( f, g, z).
Deﬁnition 8. Let C and D be curves in P2 with no common components and reduced
deﬁning equations f = 0 and g = 0. Choose coordinates for P2 so that (3) is
satisﬁed. Then, given p = (u : v: w) ∈C∩D, the intersection multiplicity Ip(C, D)
is deﬁned to be the exponent of vx −uy in the factorization of Res( f, g, z).
In order for Ip(C, D) to be well-deﬁned, we need to make sure that we get the
same answer no matter what coordinate system satisfying (3) we use in Deﬁni-
tion 8. For the moment, we will assume this is true and compute some examples of
intersection multiplicities.
Example 9. Consider the following polynomials in C[x, y, z]:
f = x3 + y3 −2xyz,
g = 2x3 −4x2y + 3xy2 + y3 −2y2z.
These polynomials [adapted from WALKER (1950)] deﬁne cubic curves C = V( f)
and D = V(g) in P2. To study their intersection, we ﬁrst compute the resultant with
respect to z:
Res( f, g, z) = −2y(x −y)3(2x + y).
Since the resultant is in the elimination ideal, points in C ∩D satisfy either y = 0,
x −y = 0 or 2x + y = 0, and from here, it is easy to show that C ∩D consists of the
three points
p = (0 : 0 : 1),
q = (1 :1 : 1),
r = (4/7 :−8/7:1)
(see Exercise 6). In particular, this shows that C and D have no common components.
However, the above resultant does not give the correct intersection multiplicities
since (0 : 0 : 1) ∈C (in fact, it is a point of intersection). Hence, we must change
coordinates. Start with a point such as
(0 : 1 : 0) ̸∈C ∪D ∪Lpq ∪Lpr ∪Lqr,

§7 Bezout's Theorem
459
and ﬁnd a coordinate change with A(0 : 1 :0) = (0 : 0 :1), say A(x :y: z) = (z: x :y).
Then
(0 : 0 :1) ̸∈A(C) ∪A(D) ∪LA(p)A(q) ∪LA(p)A(r) ∪LA(q)A(r).
To ﬁnd the deﬁning equation of A(C), note that
(u : v: w) ∈A(C) ⇐⇒A−1(u : v:w) ∈C ⇐⇒f(A−1(u, v, w)) = 0.
Thus, A(C) is deﬁned by the equation f ◦A−1(x, y, z) = f(y, z, x) = 0, and
similarly, A(D) is given by g(y, z, x) = 0. Then, by Deﬁnition 8, the resultant
Res( f(y, z, x), g(y, z, x), z) gives the multiplicities for A(p) = (1 :0 : 0), A(q) =
(1 : 1 :1) and A(r) = (1 : 4/7 :−8/7). The resultant is
Res( f(y, z, x), g(y, z, x), z) = 8y5(x −y)3(4x −7y).
so that in terms of p, q and r, the intersection multiplicities are
Ip(C, D) = 5,
Iq(C, D) = 3,
Ir(C, D) = 1.
Example 1. [continued] If we let λ = 1 in Example 1, we get the curves
x
y
In this picture, the point (0 :0 : 1) is the origin, so we again must change coordinates
before (3) can hold. In the exercises, you will use an appropriate coordinate change
to show that the intersection multiplicity at the origin is in fact equal to 2.
Still assuming that the intersection multiplicities in Deﬁnition 8 are well-deﬁned,
we can now prove Bezout's Theorem.
Theorem 10 (Bezout's Theorem). Let C and D be curves in P2 with no common
components, and let m and n be the degrees of their reduced deﬁning equations.
Then

p∈C∩D
Ip(C, D) = mn,
where Ip(C, D) is the intersection multiplicity at p, as deﬁned in Deﬁnition 8.

460
Chapter 8 Projective Algebraic Geometry
Proof. Let f = 0 and g = 0 be the reduced equations of C and D, and assume that
coordinates have been chosen so that (3) holds. Write p ∈C∩D as p = (up : vp : wp).
Then we claim that
Res( f, g, z) = c
!
p∈C∩D
(vpx −upy)Ip(C,D),
where c is a nonzero constant. For each p, it is clear that (vpx −upy)Ip(C,D) is the
exact power of vpx −upy dividing the resultant—this follows by the deﬁnition of
Ip(C, D). We still need to check that this accounts for all roots of the resultant. But
if (u :v) ∈P1 satisﬁes Res( f, g, z)(u, v) = 0, then Corollary 7 of Chapter 3, §6,
implies that there is some w ∈C such that f and g vanish at (u : v: w). This is
because if we write f and g as in the proof of Lemma 5, a0 and b0 are nonzero
constants by (3). Thus (u : v: w) ∈C ∩D, and our claim is proved.
By Lemma 5, Res( f, g, z) is a nonzero homogeneous polynomial of degree mn.
Then Bezout's Theorem follows by comparing the degree of each side in the above
equation.
□
Example 9. [continued] In Example 9, we had two cubic curves which intersected
in the points (0 :0 : 1), (1 : 1 : 1) and (4/7 :−8/7: 1) of multiplicity 5, 3 and 1 respec-
tively. These add up to 9 = 3·3, as desired. If you look back at Example 9, you'll see
why we needed to change coordinates in order to compute intersection multiplici-
ties. In the original coordinates, Res( f, g, z) = −2y(x −y)3(2x + y), which would
give multiplicities 1, 3 and 1. Even without computing the correct multiplicities, we
know these cannot be right since they don't add up to 9!
Finally, we show that the intersection multiplicities in Deﬁnition 8 are well-
deﬁned.
Lemma 11. In Deﬁnition 8, all coordinate change matrices satisfying (3) give the
same intersection multiplicities Ip(C, D) for p ∈C ∩D.
Proof. Although this result holds over any algebraically closed ﬁeld, our proof will
use continuity arguments and hence is special to C. We begin by describing carefully
the coordinate changes we will use. As in Example 9, pick a point
r ̸∈C ∪D ∪

p̸=q in C∩D
Lpq
and a matrix A ∈GL(3, C) such that A(r) = (0 : 0 :1). This means A−1(0 : 0 :1) =
r, so that the condition on A is
A−1 (0 : 0 :1) ̸∈C ∪D ∪

p̸=q in C∩D
Lpq.
Let lpq = 0 be the equation of the line Lpq, and set
h = f · g ·
!
p̸=q in C∩D
ℓpq.

§7 Bezout's Theorem
461
The condition on A is thus A−1(0 : 0 : 1) ̸∈V(h), i.e., h(A−1 (0, 0, 1)) ̸= 0.
We can formulate this problem without using matrix inverses as follows. Con-
sider matrices B ∈M3×3(C), where M3×3(C) is the set of all 3 × 3 matrices with
entries in C, and deﬁne the function H : M3×3(C) →C by
H(B) = det(B) · h(B(0, 0, 1)).
If B = (bij), note that H(B) is a polynomial in the bij. Since a matrix is invertible if
and only if its determinant is nonzero, we have
H(B) ̸= 0 ⇐⇒B is invertible and h(B(0, 0, 1)) ̸= 0.
Hence the coordinate changes we want are given by A = B−1 for matrices B in
M3×3(C) \ V(H).
Let C ∩D = {p1, . . . , ps}, and for each B ∈M3×3(C) \ V(H), let B−1(pi) =
(ui,B :vi,B :wi,B). Then, by the argument given in Theorem 10, we can write
(4)
Res( f ◦B, g ◦B, z) = cB(v1,Bx −u1,By)m1,B · · · (vs,Bx −us,By)ms,B,
where cB ̸= 0. This means Ipi(C, D) = mi,B in the coordinate change given by
A = B−1. Thus, to prove the lemma, we need to show that mi,B takes the same value
for all B ∈M3×3(C) \ V(H).
To study the exponents mi,B, we consider what happens in general when we have
a factorization
G(x, y) = (vx −uy)mH(x, y)
where G and H are homogeneous and (u, v) ̸= (0, 0). Here, one calculates that
(5)
∂i+jG
∂xi∂yj (u, v) =
 0
if 0 ≤i + j < m
m!vi(−u)jH(u, v)
if i + j = m,
(see Exercise 8). In particular, if H(u, v) ̸= 0, then (u, v) ̸= (0, 0) implies that some
m-th partial of G doesn't vanish at (u, v).
We also need a method for measuring the distance between matrices B, C ∈
M3×3(C). If B = (bij) and C = (cij), then the distance between B and C is deﬁned
by the formula
d(B, C) =
3
i, j=1|bij −cij|2,
where for a complex number z = a + ib, |z| =
√
a2 + b2. A crucial fact is that
any polynomial function F : M3×3(C) →C is continuous. This means that given
B0 ∈M3×3(C), we can get F(B) arbitrarily close to F(B0) by taking B sufﬁciently
close to B0 (as measured by the above distance function). In particular, if F(B0) ̸= 0,
it follows that F(B) ̸= 0 for B sufﬁciently close to B0.
Now consider the exponent m = mi,B0 for ﬁxed B0 and i. We claim that mi,B ≤m
if B is sufﬁciently close to B0. To see this, ﬁrst note that (4) and (5) imply that
some mth partial of Res( f ◦B0, g ◦B0, z) is nonzero at (ui,B0, vi,B0). If we write
out (ui,B :vi,B) and this partial derivative of Res( f ◦B, g ◦B, z) explicitly, we get

462
Chapter 8 Projective Algebraic Geometry
formulas which are rational functions with numerators that are polynomials in the
entries of B and denominators that are powers of det(B). Thus this m-th partial of
Res( f ◦B, g ◦B, z), when evaluated at (ui,B : vi,B), is a rational function of the same
form. Since it is nonzero at B0, the continuity argument from the previous paragraph
shows that this m-th partial of Res( f ◦B, g ◦B, z) is nonzero at (ui,B, vi,B), once B
is sufﬁciently close to B0. But then, applying (4) and (5) to Res( f ◦B, g ◦B, z), we
conclude that mi,B ≤m [since mi,B > m would imply that all m-th partials would
vanish at (ui,B : vi,B)].
However, if we sum the inequalities mi,B ≤m = mi,B0 for i = 1, . . . , s, we obtain
mn =
s

i=1
mi,B ≤
s

i=1
mi,B0 = mn.
This implies that we must have term-by-term equalities, so that mi,B = mi,B0 when
B is sufﬁciently close to B0.
This proves that the function sending B to mi,B is locally constant, i.e., its value
at a given point is the same as the values at nearby points. In order for us to conclude
that the function is actually constant on all of M3×3(C) \ V(H), we need to prove
that M3×3(C) \ V(H) is path connected. This will be done in Exercise 9, which
also gives a precise deﬁnition of path connectedness. Since the Intermediate Value
Theorem from calculus implies that a locally constant function on a path connected
set is constant (see Exercise 9), we conclude that mi,B takes the same value for all
B ∈M3×3(C) \ V(H). Thus the intersection multiplicities of Deﬁnition 8 are well-
deﬁned.
□
The intersection multiplicities Ip(C, D) have many properties which make them
easier to compute. For example, one can show that Ip(C, D) = 1 if and only if p
is a nonsingular point of C and D and the curves have distinct tangent lines at p.
A discussion of the properties of multiplicities can be found in Chapter 3 of KIR-
WAN (1992). We should also point out that using resultants to deﬁne multiplicities is
unsatisfactory in the following sense. Namely, an intersection multiplicity Ip(C, D)
is clearly a local object—it depends only on the part of the curves C and D near
p—while the resultant is a global object, since it uses the equations for all of C
and D. Local methods for computing multiplicities are available, though they re-
quire slightly more sophisticated mathematics. The local point of view is discussed
in Chapter 3 of FULTON (1969) and Chapter IV of WALKER (1950). The relation be-
tween local methods and resultants is discussed in §2 of Chapter 4 of COX, LITTLE
and O'SHEA (2005).
A different approach to the intersection multiplicities Ip(C, D), based on the Eu-
clidean algorithm, can be found in HILMAR and SMYTH (2010).
As an application of what we've done so far in this section, we will prove the
following result of Pascal. Suppose we have six distinct points p1, . . . , p6 on an
irreducible conic in P2. By Bezout's Theorem, a line meets the conic in at most 2
points (see Exercise 10). Hence, we get six distinct lines by connecting p1 to p2, p2
to p3, ..., and p6 to p1. If we label these lines L1, . . . , L6, then we get the following
picture:

§7 Bezout's Theorem
463
p1
p2
p3
p4
p5
p6
L1
L6
L4
L3
L2
L5
We say that lines L1, L4 are opposite, and similarly the pairs L2, L5 and L3, L6
are opposite. The portions of the lines lying inside the conic form a hexagon, and
opposite lines correspond to opposite sides of the hexagon.
In the above picture, the intersections of the opposite pairs of lines appear to lie
on the same dashed line. The following theorem reveals that this is no accident.
Theorem 12 (Pascal's Mystic Hexagon). Given six points on an irreducible conic,
connected by six lines as above, the points of intersection of the three pairs of op-
posite lines are collinear.
Proof. Let the conic be C. As above, we have six points p1, . . . , p6 and three pairs
of opposite lines {L1, L4}, {L2, L5}, and {L3, L6}. Now consider the two curves
C1 = L1 ∪L3 ∪L5 and C2 = L2 ∪L4 ∪L6. These curves are deﬁned by cubic equa-
tions, so that by Bezout's Theorem, the number of points in C1 ∩C2 is 9 (counting
multiplicities). However, note that C1∩C2 contains the six original points p1, . . . , p6
and the three points of intersection of opposite pairs of lines (you should check this
carefully). Thus, these are all of the points of intersection, and all of the multiplici-
ties are one.
Suppose that C = V( f), C1 = V(g1) and C2 = V(g2), where f has total degree
2 and g1 and g2 have total degree 3. Now pick a point p ∈C distinct from p1, . . . , p6.
Thus, g1(p) and g2(p) are nonzero (do you see why?), so that g = g2(p)g1−g1(p)g2
is a cubic polynomial which vanishes at p, p1, . . . , p6. Furthermore, g is nonzero
since otherwise g1 would be a multiple of g2 (or vice versa). Hence, the cubic V(g)
meets the conic C in at least seven points, so that the hypotheses for Bezout's The-
orem are not satisﬁed. Thus, either g is not reduced or V(g) and C have a common
irreducible component. The ﬁrst of these can't occur, since if g weren't reduced,
the curve V(g) would be deﬁned by an equation of degree at most 2 and V(g) ∩C
would have at most 4 points by Bezout's Theorem. Hence, V(g) and C must have a
common irreducible component. But C is irreducible, which implies that C = V( f)
is a component of V(g). By Proposition 4, it follows that f must divide g.
Hence, we get a factorization g = f · l, where l has total degree 1. Since g
vanishes where the opposite lines meet and f does not, it follows that l vanishes at
these points. Since V(l) is a projective line, the theorem is proved.
□

464
Chapter 8 Projective Algebraic Geometry
Bezout's Theorem serves as a nice introduction to the study of curves in P2. This
part of algebraic geometry is traditionally called algebraic curves and includes many
interesting topics we have omitted (inﬂection points, dual curves, elliptic curves,
etc.). Fortunately, there are several excellent texts on this subject. In addition to
FULTON (1969), KIRWAN (1992) and WALKER (1950) already mentioned, we also
warmly recommend CLEMENS (2002), BRIESKORN and KNÖRRER (1986) and FIS-
CHER (2001). For students with a background in complex analysis and topology, we
also suggest GRIFFITHS (1989).
EXERCISES FOR §7
1. This exercise is concerned with the parabola y = x2 and the ellipse x2 + 4(y −λ)2 = 4
from Example 1.
a. Show that these curves have empty intersection over R when λ < −1. Illustrate the
cases λ < −1 and λ = −1 with a picture.
b. Find the smallest positive real number λ0 such that the intersection over R is empty
when λ > λ0. Illustrate the cases λ > λ0 and λ = λ0 with a picture.
c. When −1 < λ < λ0, describe the possible types of intersections that can occur over
R and illustrate each case with a picture.
d. In the pictures for parts (a), (b), and (c) use the intuitive idea of multiplicity from
Example 1 to determine which ones represent intersections with multiplicity > 1.
e. Without using Bezout's Theorem, explain why over C, the number of intersections
(counted with multiplicity) adds up to 4 when λ is real. Hint: Example 1 gave formu-
las for the points of intersection when λ = 2. Do this for general λ.
2. In Example 2, we intersected the parabola y = x2 with a line L in afﬁne space. Assume
that L is not vertical.
a. Over R, show that the number of points of intersection can be 0, 1, or 2. Further,
show that you get one point of intersection exactly when L is tangent to y = x2 in the
sense of Chapter 3, §4.
b. Over C, show (without using Bezout's Theorem) that the number of intersections
(counted with multiplicity) is exactly 2.
3. In proving Proposition 4, we showed that if f = gh is homogeneous and g = gm+· · ·+g0,
where gi is homogeneous of total degree i and gm ̸= 0, and similarly h = hn + · · · + h0,
then f = gmhn. Complete the proof by showing that g = gm and h = hn. Hint: Let m0 be
the smallest index m0 such that gm0 ̸= 0, and deﬁne hn0 ̸= 0 similarly.
4. In this exercise, we sketch an alternate proof of Lemma 5. Given f and g as in the
statement of the lemma, let R(x, y) = Res( f , g, z). It sufﬁces to prove that R(tx, ty) =
tmnR(x, y).
a. Use ai(tx, ty) = tiai(x, y) and bi(tx, ty) = tibi(x, y) to show that R(tx, ty) is given by
a determinant whose entries are either 0 or tiai(x, y) or tibi(x, y).
b. In the determinant from part (a), multiply column 2 by t, column 3 by t2, . . ., column
n by tn−1, column n + 2 by t, column n + 3 by t2, . . ., and column n + m by tm−1.
Use this to prove that tqR(tx, ty), where q = n(n −1)/2 + m(m −1)/2 equals a
determinant where in each row, t appears to the same power.
c. By pulling out the powers of t from the rows of the determinant from part (b) prove
that tqR(tx, ty) = trR(x, y), where r = (m + n)(m + n −1)/2.
d. Use part (c) to prove that R(tx, ty) = tmnR(x, y), as desired.
5. This exercise is concerned with the proof of Theorem 7.
a. Let f ∈C[x1, . . . , xn] be a nonzero polynomial. Prove that V( f ) and Cn \ V( f ) are
nonempty. Hint: Use the Nullstellensatz and Proposition 5 of Chapter 1, §1.

§7 Bezout's Theorem
465
b. Use part (a) to prove that you can ﬁnd q ̸∈C ∪D ∪#
i<j Lij as claimed in the proof
of Theorem 7.
c. Given q ∈P2(C), ﬁnd A ∈GL(3, C) such that A(q) = (0 : 0 : 1). Hint: The points
q and (0 : 0 : 1) give nonzero column vectors in C3. Use linear algebra to ﬁnd an
invertible matrix A taking the ﬁrst to the second and conclude that A(q) = (0 : 0 : 1).
d. Prove that the projective line connecting (0 : 0 : 1) to (u : v : w) intersects the line z =
0 in the point (u : v : 0). Hint: Use equation (10) of §6.
6. In Example 9, we considered the curves C = V( f ) and D = V(g), where f and g are
given in the text.
a. Verify carefully that p = (0 : 0 : 1), q = (1 : 1 : 1) and r = (4/7 : −8/7 : 1) are the
only points of intersection of the curves C and D. Hint: Once you have Res( f , g, z),
you can do the rest by hand.
b. Show that f and g are reduced. Hint: Use a computer.
c. Show that (0 : 1 : 0) ̸∈C ∪D ∪Lpq ∪Lpr ∪Lqr.
7. For each of the following pairs of curves, ﬁnd the points of intersection and compute the
intersection multiplicities.
a. C = V(yz −x2) and D = V(x2 + 4(y −z)2 −4z2). This is the projective version of
Example 1 when λ = 1. Hint: Show that the coordinate change given by A(x : y : z) =
(x : y + z : z) has the desired properties.
b. C = V(x2y3 −2xy2z2 + yz4 + z5) and D = V(x2y2 −xz3 −z4). Hint: There are four
solutions, two real and two complex. When ﬁnding the complex solutions, computing
the gcd of two complex polynomials may help.
8. Prove (5). Hint: Use induction on m, and apply the inductive hypothesis to ∂G/∂x and
∂G/∂y.
9. (Requires multivariable calculus.) An open set U ⊆Cn is path connected if for every
two points a, b ∈U, there is a continuous function γ : [0, 1] →U such that γ(0) = a
and γ(1) = b.
a. Suppose that F : U →Z is locally constant (as in the text, this means that the value
of F at a point of U equals its value at all nearby points). Use the Intermediate Value
Theorem from calculus to show that F is constant when U is path connected. Hint: If
we regard F as a function F : U →R, explain why F is continuous. Then note that
F ◦γ : [0, 1] →R is also continuous.
b. Let f ∈C[x] be a nonzero polynomial. Prove that C \ V( f ) is path connected.
c. If f ∈C[x1, . . . , xn] is nonzero, prove that Cn \ V( f ) is path connected. Hint: Given
a, b ∈Cn \ V( f ), consider the complex line {ta + (1 −t)b | t ∈C} determined by
a and b. Explain why f (ta + (1 −t)b) is a nonzero polynomial in t and use part (b).
d. Give an example of f ∈R[x, y] such that R2 \ V( f ) is not path connected. Further,
ﬁnd a locally constant function F : R2 \ V( f ) →Z which is not constant. Thus, it is
essential that we work over C.
10. Let C be an irreducible conic in P2(C). Use Bezout's Theorem to explain why a line L
meets C in at most two points. What happens when C is reducible? What about when C
is a curve deﬁned by an irreducible polynomial of total degree n?
11. In the picture drawn in the text for Pascal's Mystic Hexagon, the six points went clock-
wise around the conic. If we change the order of the points, we can still form a "hexagon,"
though opposite lines might intersect inside the conic. For example, the picture could be
as follows:

466
Chapter 8 Projective Algebraic Geometry
p1
p5
p2
p4
p6
p3
L1
L6
L4
L3
L2
L5
Explain why the theorem remains true in this case.
12. In Pascal's Mystic Hexagon, suppose that the conic is a circle and the six lines come
from a regular hexagon inscribed inside the circle. Where do the opposite lines meet and
on what line do their intersections lie?
13. Pappus's Theorem from Exercise 8 of Chapter 6, §4, states that if p3, p1, p5 and p6, p4, p2
are two collinear triples of points and we set
p = p3p4 ∩p6p1
q = p2p3 ∩p5p6
r = p4p5 ∩p1p2.
then p, q, r are also collinear. The picture is as follows:
p6
p4
p2
p3
p1
p5
p
q
r
The union of the lines p3p1 and p6p4 is a reducible conic C′. Explain why Pappus's
Theorem can be regarded as a "degenerate" case of Pascal's Mystic Hexagon. Hint: See
Exercise 11. Note that unlike the irreducible case, we can't choose any six points on C′:
we must avoid the singular point of C′, and each component of C′ must contain three of
the points.
14. The argument used to prove Theorem 12 applies in much more general situations. Sup-
pose that we have curves C and D deﬁned by reduced equations of total degree n such
that C ∩D consists of exactly n2 points. Furthermore, suppose there is an irreducible
curve E with a reduced equation of total degree m < n which contains exactly mn of
these n2 points. Then adapt the argument of Theorem 12 to show that there is a curve F
with a reduced equation of total degree n −m which contains the remaining n(n −m)
points of C ∩D.

§7 Bezout's Theorem
467
15. Let C and D be curves in P2(C).
a. Prove that C ∩D must be nonempty.
b. Suppose that C is nonsingular in the sense of part (a) of Exercise 9 of §6 [if
C = V( f ), this means the partial derivatives ∂f /∂x, ∂f /∂y and ∂f /∂z don't
vanish simultaneously on P2(C)]. Prove that C is irreducible. Hint: Suppose that
C = C1 ∪C2, which implies f = f1 f2. How do the partials of f behave at a point of
C1 ∩C2?
16. This exercise will explore an informal proof of Bezout's Theorem. The goal is to give an
intuitive explanation of why the number of intersection points is mn.
a. In P2(C), show that a line L meets a curve C of degree n in n points, counting multi-
plicity. Hint: Choose coordinates so that all of the intersections take place in C2, and
write L parametrically as x = a + ct, y = b + dt.
b. If a curve C of degree n meets a union of m lines, use part (a) to predict how many
points of intersection there are.
c. When two curves C and D meet, give an intuitive argument (based on pictures) that
the number of intersections (counting multiplicity) doesn't change if one of the curves
moves a bit. Your pictures should include instances of tangency and the example of
the intersection of the x-axis with the cubic y = x3.
d. Use the constancy principle from part (c) to argue that if the m lines in part (b) all
coincide (giving what is called a line of multiplicity m), the number of intersections
(counted with multiplicity) is still as predicted.
e. Using the constancy principle from part (c) argue that Bezout's Theorem holds for
general curves C and D by moving D to a line of multiplicity m [as in part (d)]. Hint:
If D is deﬁned by f = 0, "move" D by letting all but one coefﬁcient of f go to zero.
In technical terms, this is a degeneration proof of Bezout's Theorem. A rigorous ver-
sion of this argument can be found in BRIESKORN and KNÖRRER (1986). Degeneration
arguments play an important role in algebraic geometry.

Chapter 9
The Dimension of a Variety
The most important invariant of a linear subspace of afﬁne space is its dimension.
For afﬁne varieties, we have seen numerous examples which have a clearly deﬁned
dimension, at least from a naive point of view. In this chapter, we will carefully de-
ﬁne the dimension of any afﬁne or projective variety and show how to compute it.
We will also show that this notion accords well with what we would expect intu-
itively. In keeping with our general philosophy, we consider the computational side
of dimension theory right from the outset.
§1
The Variety of a Monomial Ideal
We begin our study of dimension by considering monomial ideals. In particular, we
want to compute the dimension of the variety deﬁned by such an ideal. Suppose, for
example, we have the ideal I = ⟨x2y, x3⟩in k[x, y]. Letting Hx denote the line in k2
deﬁned by x = 0 (so Hx = V(x)) and Hy the line y = 0, we have
(1)
V(I) = V(x2y) ∩V(x3)
= (Hx ∪Hy) ∩Hx
= (Hx ∩Hx) ∪(Hy ∩Hx)
= Hx.
Thus, V(I) is the y-axis Hx. Since Hx has dimension 1 as a vector subspace of k2, it
is reasonable to say that it also has dimension 1 as a variety.
As a second example, consider the ideal
I = ⟨y2z3, x5z4, x2yz2⟩⊆k[x, y, z].
Let Hx be the plane deﬁned by x = 0 and deﬁne Hy and Hz similarly. Also, let Hxy
be the line x = y = 0. Then we have
© Springer International Publishing Switzerland 2015
D.A. Cox et al., Ideals, Varieties, and Algorithms, Undergraduate Texts
in Mathematics, DOI 10.1007/978-3-319-16721-3_9
469

470
Chapter 9 The Dimension of a Variety
V(I) = V(y2z3) ∩V(x5z4) ∩V(x2yz2)
= (Hy ∪Hz) ∩(Hx ∪Hz) ∩(Hx ∪Hy ∪Hz)
= Hz ∪Hxy.
To verify this, note that the plane Hz belongs to each of the three terms in the sec-
ond line and, hence, to their intersection. Thus, V(I) will consist of the plane Hz
together, perhaps, with some other subset not contained in Hz. Collecting terms not
contained in Hz, we have Hy ∩Hx ∩(Hx ∪Hy), which equals Hxy. Thus, V(I) is the
union of the (x, y)-plane Hz and the z-axis Hxy. We will say that the dimension of a
union of ﬁnitely many vector subspaces of kn is the biggest of the dimensions of the
subspaces, and so the dimension of V(I) is 2 in this example.
The variety of any monomial ideal may be assigned a dimension in much the
same fashion. But ﬁrst we need to describe what a variety of a general monomial
ideal looks like. In kn, a vector subspace deﬁned by setting some subset of the vari-
ables x1, . . . , xn equal to zero is called a coordinate subspace.
Proposition 1. The variety of a monomial ideal in k[x1, . . . , xn] is a ﬁnite union of
coordinate subspaces of kn.
Proof. First, note that if xα1
i1 · · · xαr
ir is a monomial in k[x1, . . . , xn] with αj ≥1 for
1 ≤j ≤r, then
V(xα1
i1 · · · xαr
ir ) = Hxi1 ∪· · · ∪Hxir ,
where Hxℓ= V(xℓ). Thus, the variety deﬁned by a monomial is a union of coordi-
nate hyperplanes. Note also that there are only n such hyperplanes.
Since a monomial ideal is generated by a ﬁnite collection of monomials, the vari-
ety corresponding to a monomial ideal is a ﬁnite intersection of unions of coordinate
hyperplanes. By the distributive property of intersections over unions, any ﬁnite int-
ersection of unions of coordinate hyperplanes can be rewritten as a ﬁnite union of
intersections of coordinate hyperplanes [see (1) for an example of this]. But the
intersection of any collection of coordinate hyperplanes is a coordinate subspace. □
When we write the variety of a monomial ideal I as a union of ﬁnitely many
coordinate subspaces, we can omit a subspace if it is contained in another in the
union. Thus, we can write V(I) as a union of coordinate subspaces.
V(I) = V1 ∪· · · ∪Vp,
where Vi ̸⊆Vj for i ̸= j. In fact, such a decomposition is unique, as you will show
in Exercise 8.
Let us make the following provisional deﬁnition. We will always assume that k
is inﬁnite.
Deﬁnition 2. Let V be a variety which is the union of a ﬁnite number of linear
subspaces of kn. Then the dimension of V, denoted dimV, is the largest of the dim-
ensions of the subspaces.

§1 The Variety of a Monomial Ideal
471
Thus, the dimension of the union of two planes and a line is 2, and the dimension
of a union of three lines is 1. To compute the dimension of the variety correspond-
ing to a monomial ideal, we merely ﬁnd the maximum of the dimensions of the
coordinate subspaces contained in V(I).
Although this is easy to do for any given example, it is worth systematizing the
computation. Let I = ⟨m1, . . . , mt⟩be a proper ideal generated by the monomials mj.
In trying to compute dimV(I), we need to pick out the component of
V(I) =
t#
j=1
V(mj)
of largest dimension. If we can ﬁnd a collection of variables xi1, . . . , xir such that at
least one of these variables appears in each mj, then the coordinate subspace deﬁned
by the equations xi1 = · · · = xir = 0 is contained in V(I). This means we should
look for variables which occur in as many of the different mj as possible. More
precisely, for 1 ≤j ≤t, let
Mj = {ℓ∈{1, . . . , n} | xℓdivides the monomial mj}
be the set of subscripts of variables occurring with positive exponent in mj. (Note
that Mj is nonempty by our assumption that I ̸= k[x1, . . . , xn].) Then let
M = {J ⊆{1, . . . , n} | J ∩Mj ̸= ∅for all 1 ≤j ≤t}
consist of all subsets of {1, . . . , n} which have nonempty intersection with every set
Mj. (Note that M is not empty because {1, . . . , n} ∈M.) If we let |J| denote the
number of elements in a set J, then we have the following.
Proposition 3. With the notation above,
dimV(I) = n −min(|J| | J ∈M).
Proof. Let J = {i1, . . . , ir} be an element of M such that |J| = r is minimal
in M. Since each monomial mj contains some power of some xiℓ, 1 ≤ℓ≤r, the
coordinate subspace W = V(xi1, . . . , xir) is contained in V(I). The dimension of W
is n −r = n −|J|, and hence, by Deﬁnition 2, the dimension of V(I) is at least
n −|J|.
If V(I) had dimension larger than n −r, then for some s < r there would be
a coordinate subspace W′ = V(xℓ1, . . . , xℓs) contained in V(I). Each monomial mj
would vanish on W′ and, in particular, it would vanish at the point p ∈W′ whose
ℓi-th coordinate is 0 for 1 ≤i ≤s and whose other coordinates are 1. Hence, at
least one of the xℓi must divide mj, and it would follow that J′ = {ℓ1, . . . , ℓs} ∈M.
Since |J′| = s < r, this would contradict the minimality of r. Thus, the dimension
of V(I) must be as claimed.
□
Let us check this on the second example given above. To match the notation of
the proposition, we relabel the variables x, y, z as x1, x2, x3, respectively. Then
I = ⟨x2
2x3
3, x5
1x4
3, x2
1x2x2
3⟩= ⟨m1, m2, m3⟩,

472
Chapter 9 The Dimension of a Variety
where
m1 = x2
2x3
3,
m2 = x5
1x4
3,
m3 = x2
1x2x2
3.
Using the notation of the discussion preceding Proposition 3,
M1 = {2, 3},
M2 = {1, 3},
M3 = {1, 2, 3},
so that
M = {{1, 2, 3}, {1, 2}, {1, 3}, {2, 3}, {3}}.
Then min(|J| | J ∈M) = 1, which implies that
dimV(I) = 3 −min
J∈M |J| = 3 −1 = 2.
Generalizing this example, note that if some variable, say xi, appears in every mono-
mial in a set of generators for a proper monomial ideal I, then it will be true that
dimV(I) = n −1 since J = {i} ∈M. For a converse, see Exercise 4.
It is also interesting to compare a monomial ideal I to its radical
√
I. In the
exercises, you will show that
√
I is a monomial ideal when I is. We also know from
Chapter 4 that V(I) = V(
√
I) for any ideal I. It follows from Deﬁnition 2 that V(I)
and V(
√
I) have the same dimension (since we deﬁned dimension in terms of the
underlying variety). In Exercise 10 you will check that this is consistent with the
formula given in Proposition 3.
EXERCISES FOR §1
1. For each of the following monomial ideals I, write V(I) as a union of coordinate
subspaces.
a. I = ⟨x5, x4yz, x3z⟩⊆k[x, y, z].
b. I = ⟨wx2y, xyz3, wz5⟩⊆k[w, x, y, z].
c. I = ⟨x1x2, x3 · · · xn⟩⊆k[x1, . . . , xn].
2. Find dimV(I) for each of the following monomial ideals.
a. I = ⟨xy, yz, xz⟩⊆k[x, y, z].
b. I = ⟨wx2z, w3y, wxyz, x5z6⟩⊆k[w, x, y, z].
c. I = ⟨u2vwyz, wx3 y3, uxy7z, y3z, uwx3y3z2⟩⊆k[u, v, w, x, y, z].
3. Show that W ⊆kn is a coordinate subspace if and only if W can be spanned by a subset
of the basis vectors {ei | 1 ≤i ≤n}, where ei is the vector consisting of all zeros except
for a 1 in the i-th place.
4. Suppose that I ⊆k[x1, . . . , xn] is a monomial ideal such that dimV(I) = n −1.
a. Show that the monomials in any generating set for I have a nonconstant common
factor.
b. Write V(I) = V1 ∪· · · ∪Vp, where Vi is a coordinate subspace and Vi ̸⊆Vj for
i ̸= j. Suppose, in addition, that exactly one of the Vi has dimension n −1. What is
the maximum that p (the number of components) can be? Give an example in which
this maximum is achieved.
5. Let I be a monomial ideal in k[x1, . . . , xn] such that dimV(I) = 0.
a. What is V(I) in this case?
b. Show that dimV(I) = 0 if and only if for each 1 ≤i ≤n, xℓi
i ∈I for some ℓi ≥1.
Hint: In Proposition 3, when will it be true that M contains only J = {1, . . . , n}?

§2 The Complement of a Monomial Ideal
473
6. Let ⟨m1, . . . , mr⟩⊆k[x1, . . . , xn] be a monomial ideal generated by r ≤n monomials.
Show that dimV(m1, . . . , mr) ≥n −r.
7. Show that a coordinate subspace is an irreducible variety when the ﬁeld k is inﬁnite.
8. In this exercise, we will relate the decomposition of the variety of a monomial ideal I as
a union of coordinate subspaces given in Proposition 1 with the decomposition of V(I)
into irreducible components. We will assume that the ﬁeld k is inﬁnite.
a. If V(I) = V1 ∪· · · ∪Vp, where the Vj are coordinate subspaces such that Vi ̸⊆Vj if
i ̸= j, then show that this union is the minimal decomposition of V(I) into irreducible
varieties given in Theorem 4 of Chapter 4, §6.
b. Deduce that the Vi in part (a) are unique up to the order in which they are written.
9. Let I = ⟨mi, . . . , ms⟩be a monomial ideal in k[x1, . . . , xn]. For each 1 ≤j ≤s, let
Mj = {ℓ| xℓdivides mj} as in the text, and consider the monomial
m′
j =

ℓ∈Mj
xℓ.
Note that m′
j contains exactly the same variables as mj, but all to the ﬁrst power.
a. Show that m′
j ∈
√
I for each 1 ≤j ≤s.
b. Show that
√
I = ⟨m′
1, . . . , m′
s⟩. Hint: Use Lemmas 2 and 3 of Chapter 2, §4.
10. Let I be a monomial ideal. Using Exercise 9, show that dimV(I) = dimV(
√
I) follows
from the dimension formula given in Proposition 3.
§2
The Complement of a Monomial Ideal
One of Hilbert's key insights in his famous paper Über die Theorie der algebrais-
chen Formen [see HILBERT (1890)] was that the dimension of the variety associated
to a monomial ideal could be characterized by the growth of the number of monomi-
als not in the ideal as the total degree increases. We have alluded to this phenomenon
in several places in Chapter 5 (notably in Exercise 12 of §3).
In this section, we will make a careful study of the monomials not contained
in a monomial ideal I ⊆k[x1, . . . , xn]. Since there may be inﬁnitely many such
monomials, our goal will be to ﬁnd a formula for the number of monomials xα /∈I
which have total degree less than some bound. The results proved here will play a
crucial role in §3 when we deﬁne the dimension of an arbitrary variety.
Example 1. Consider a proper monomial ideal I in k[x, y]. Since I is proper (i.e.,
I ̸= k[x, y]), V(I) is either
a. The origin {(0, 0)},
b. the x-axis,
c. the y-axis, or
d. the union of the x-axis and the y-axis.
In case (a), by Exercise 5 of §1, we must have xa ∈I and yb ∈I for some
integers a, b > 0. Here, the number of monomials not in I will be ﬁnite, equal to
some constant C0 ≤a · b. If we assume that a and b are as small as possible, we get
a picture like the following when we look at exponents:

474
Chapter 9 The Dimension of a Variety
n
m
(m,n) ←→ xm yn
b
a
The monomials in I are indicated by solid dots, while those not in I are open circles.
In case (b), since V(I) is the x-axis, no power xa of x can belong to I. On the
other hand, since the y-axis does not belong to V(I), we must have yb ∈I for some
minimal integer b > 0. The picture would be as follows:
n
m
(m,n) ←→ xm yn
b
l
a
As the picture indicates, we let l denote the minimum exponent of y that occurs
among all monomials in I. Note that l ≤b, and we also have l > 0 since no positive
power of x lies in I. Then the monomials in the complement of I are precisely the
monomials
{xiy j | i ∈Z≥0, 0 ≤j ≤l −1},
corresponding to the exponents on l copies of the horizontal axis in Z2
≥0, together
with a ﬁnite number of other monomials. These additional monomials can be char-
acterized as those monomials m /∈I with the property that xrm ∈I for some r > 0.
In the above picture, they correspond to the open circles on or above the dotted line.
Thus, the monomials in the complement of I consist of l "lines" of monomials
together with a ﬁnite set of monomials. This description allows us to "count" the
number of monomials not in I. More precisely, in Exercise 1, you will show that if
s > l, the l "lines" contain precisely l(s + 1) −(1 + 2 + · · · + l −1) monomials

§2 The Complement of a Monomial Ideal
475
of total degree ≤s. In particular, if s is large enough (more precisely, we must have
s > a + b, where a is indicated in the above picture), the number of monomials not
in I of total degree ≤s equals ls + C0, where C0 is some constant depending only
on I.
In case (c), the situation is similar to (b), except that the "lines" of monomials
are parallel to the vertical axis in the plane Z2
≥0 of exponents. In particular, we get
a similar formula for the number of monomials not in I of total degree ≤s once s is
sufﬁciently large.
In case (d), let l1 be the minimum exponent of x that occurs among all monomials
of I, and similarly let l2 be the minimum exponent of y. Note that l1 and l2 are
positive since xy must divide every monomial in I. Then we have the following
picture when we look at exponents:
n
m
(m,n) ←→ xm yn
b
l2
a
l1
The monomials in the complement of I consist of the l1 "lines" of monomials
{xiy j | 0 ≤i ≤l1 −1, j ∈Z≥0}
parallel to the vertical axis, the l2 "lines" of monomials
{xiy j | i ∈Z≥0, 0 ≤j ≤l2 −1}
parallel to the horizontal axis, together with a ﬁnite number of other monomials
(indicated by open circles inside or on the boundary of the region enclosed by the
dotted lines).
Thus, the monomials not in I consist of l1 + l2 "lines" of monomials together
with a ﬁnite set of monomials. For s large enough (in fact, for s > a + b, where a
and b are as in the above picture) the number of monomials not in I of total degree
≤s will be (l1 + l2)s + C0, where C0 is a constant. See Exercise 1 for the details of
this claim.
The pattern that appears in Example 1, namely, that the monomials in the com-
plement of a monomial ideal I ⊆k[x, y] consist of a number of inﬁnite families
parallel to the "coordinate subspaces" in Z2
≥0, together with a ﬁnite collection of

476
Chapter 9 The Dimension of a Variety
monomials, generalizes to arbitrary monomial ideals. In §3, this will be the key to
understanding how to deﬁne and compute the dimension of an arbitrary variety.
To discuss the general situation, we will introduce some new notation. For each
monomial ideal I, we let
C(I) = {α ∈Zn
≥0 | xα /∈I}
be the set of exponents of monomials not in I, i.e., the complement of the exponents
of monomials in I. This will be our principal object of study. We also set
e1 = (1, 0, . . . , 0),
e2 = (0, 1, . . . , 0),
...
en = (0, 0, . . . , 1).
Further, we deﬁne the coordinate subspace of Zn
≥0 determined by ei1, . . . , eir, where
i1 < · · · < ir, to be the set
[ei1, . . . , eir] = {a1ei1 + · · · + areir | aj ∈Z≥0 for 1 ≤j ≤r}.
We say that [ei1, . . . , eir] is an r-dimensional coordinate subspace. Finally, a subset
of Zn
≥0 is a translate of a coordinate subspace [ei1, . . . , eir] if it is of the form
α + [ei1, . . . , eir] = {α + β | β ∈[ei1, . . . , eir]},
where α = 
i/∈{i1,...,ir} aiei, ai ≥0. This restriction on α means that we are trans-
lating by a vector perpendicular to [ei1, . . . , eir]. For example, {(1, l) | l ∈Z≥0} =
e1 + [e2] is a translate of the subspace [e2] in the plane Z2
≥0 of exponents.
With these deﬁnitions in hand, our discussion of monomial ideals in k[x, y] from
Example 1 can be summarized as follows:
a. If V(I) is the origin, then C(I) consists of a ﬁnite number of points.
b. If V(I) is the x-axis, then C(I) consists of a ﬁnite number of translates of [e1]
and, possibly, a ﬁnite number of points not on these translates.
c. If V(I) is the y-axis, then C(I) consists of a ﬁnite number of translates of [e2]
and, possibly, a ﬁnite number of points not on these translates.
d. If V(I) is the union of the x-axis and the y-axis, then C(I) consists of a ﬁnite
number of translates of [e1], a ﬁnite number of translates of [e2], and, possibly, a
ﬁnite number of points on neither set of translates.
In the exercises, you will carry out a similar analysis for monomial ideals in the
polynomial ring in three variables.
Now let us turn to the general case. We ﬁrst observe that there is a direct corre-
spondence between the coordinate subspaces in V(I) and the coordinate subspaces
of Zn
≥0 contained in C(I).

§2 The Complement of a Monomial Ideal
477
Proposition 2. Let I ⊆k[x1, . . . , xn] be a proper monomial ideal.
(i) The coordinate subspace V(xi | i /∈{i1, . . . , ir}) is contained in V(I) if and only
if [ei1, . . . , eir] ⊆C(I).
(ii) The dimension of V(I) is the dimension of the largest coordinate subspace
in C(I).
Proof. (i) ⇒: First observe that W = V(xi | i /∈{i1, . . . , ir}) contains the point p
whose ij-th coordinate is 1 for 1 ≤j ≤r and whose other coordinates are 0. For
any α ∈[ei1, . . . , eir], the monomial xα can be written in the form xα = x
αi1
i1 · · · xαir
ir .
Then xα = 1 at p, so that xα /∈I since p ∈W ⊆V(I) by hypothesis. This shows
that α ∈C(I).
⇐: Suppose that [ei1, . . . eir] ⊆C(I). Then every monomial in I contains at least
one variable other than xi1, . . . , xir. This means that every monomial in I vanishes
on any point (a1, . . . , an) ∈kn for which ai = 0 when i /∈{i1, . . . , ir}. So every
monomial in I vanishes on the coordinate subspace V(xi | i /∈{i1, . . . , ir}), and,
hence, the latter is contained in V(I).
(ii) Note that the coordinate subspace V(xi | i /∈{i1, . . . , ir}) has dimension r. It
follows from part (i) that the dimensions of the coordinate subspaces of kn contained
in V(I) and the coordinate subspaces of Zn
≥0 contained in C(I) are the same. By
Deﬁnition 2 of §1, dimV(I) is the maximum of the dimensions of the coordinate
subspaces of kn contained in V(I), so the statement follows.
□
We can now characterize the complement of a monomial ideal.
Theorem 3. If I ⊆k[x1, . . . , xn] is a proper monomial ideal, then the set C(I) ⊆
Zn
≥0 of exponents of monomials not lying in I can be written as a ﬁnite (but not
necessarily disjoint) union of translates of coordinate subspaces of Zn
≥0.
Before proving the theorem, consider, for example, the ideal I = ⟨x4y3, x2y5⟩.
n
m
(m,n) ←→ xm yn
(2,5)
(4,3)

478
Chapter 9 The Dimension of a Variety
Here, it is easy to see that C(I) is the ﬁnite union
C(I) = [e1] ∪(e2 + [e1]) ∪(2e2 + [e1]) ∪[e2] ∪(e1 + [e2])
∪{(3, 4)} ∪{(3, 3)} ∪{(2, 4)} ∪{(2, 3)}.
We regard the last four sets in this union as being translates of the 0-dimensional
coordinate subspace, which is the origin in Z2
≥0.
Proof of Theorem 3. If I is the zero ideal, the theorem is trivially true, so we can
assume that I ̸= {0}. The proof is by induction on the number of variables n. If
n = 1, then I = ⟨xa⟩for some integer a > 0. The only monomials not in I are
1, x, . . . , xa−1, and hence C(I) = {0, 1, . . ., a −1} ⊆Z≥0. Thus, the complement
consists of a points, all of which are translates of the origin.
Now assume that the result holds for n −1 variables and that we have a
monomial ideal I ⊆k[x1, . . . , xn]. For each integer j ≥0, let Ij be the ideal
in k[x1, . . . , xn−1] generated by monomials m with the property that m · x j
n ∈I.
Then C(Ij) consists of exponents α ∈Zn−1
≥0 such that xαx j
n /∈I. Geometrically,
this says that C(Ij) ⊆Zn−1
≥0 corresponds to the intersection of C(I) and the hyper-
plane (0, . . . , 0, j) + [e1, . . . , en−1] in Zn
≥0.
Because I is an ideal, we have Ij ⊆Ij′ when j < j′. By the ascending chain
condition for ideals, there is an integer j0 such that Ij = Ij0 for all j ≥j0. For any
integer j, we let C(Ij) × {j} denote the set {(α, j) ∈Zn
≥0 | α ∈C(Ij) ⊆Zn−1
≥0 }.
Then we claim the monomials C(I) not lying in I can be written as
(1)
C(I) = (C(Ij0) × Z≥0) ∪
j0−1

j=0
(C(Ij) × {j}).
To prove this claim, ﬁrst note that C(Ij) × {j} ⊆C(I) by the deﬁnition of C(Ij).
To show that C(Ij0) × Z≥0 ⊆C(I), observe that Ij = Ij0 when j ≥j0, so that
C(Ij0) × {j} ⊆C(I) for these j's. When j < j0, we have xαx j
n /∈I whenever
xαx j0
n /∈I since I is an ideal, which shows that C(Ij0) × {j} ⊆C(I) for j < j0. We
conclude that C(I) contains the right-hand side of (1).
To prove the opposite inclusion, take α = (α1, . . . , αn) ∈C(I). Then we have
α ∈C(Iαn)×{αn} by deﬁnition. If αn < j0, then α obviously lies in the right-hand
side of (1). On the other hand, if αn ≥j0, then Iαn = Ij0 shows that α ∈C(Ij0)×Z≥0,
and our claim is proved.
If we apply our inductive assumption, we can write C(I0), . . . , C(Ij0) as ﬁnite
unions of translates of coordinate subspaces of Zn−1
≥0 . Substituting these ﬁnite unions
into the right-hand side of (1), we immediately see that C(I) is also a ﬁnite union of
translates of coordinate subspaces of Zn
≥0.
□
Our next goal is to ﬁnd a formula for the number of monomials of total degree
≤s in the complement of a monomial ideal I ⊆k[x1, . . . , xn]. Here is one of the key
facts we will need.
Lemma 4. The number of monomials of total degree ≤s in k[x1, . . . , xm] is the
binomial coefﬁcient
m+s
s
	
.

§2 The Complement of a Monomial Ideal
479
Proof. See Exercise 10 of Chapter 5, §3.
□
In what follows, we will refer to |α| = α1 + · · · + αn as the total degree of
α ∈Zn
≥0. This is also the total degree of the monomial xα. Using this terminol-
ogy, Lemma 4 easily implies that the number of points of total degree ≤s in an
m-dimensional coordinate subspace of Zn
≥0 is
m+s
s
	
(see Exercise 5). Observe that
when m is ﬁxed, the expression
m + s
s

=
m + s
m

= 1
m!(s + m)(s + m −1) · · · (s + 1)
is a polynomial of degree m in s. Note that the coefﬁcient of sm is 1/m!.
What about the number of monomials of total degree ≤s in a translate of an
m-dimensional coordinate subspace in Zn
≥0? Consider, for instance, the translate
am+1em+1 + · · ·+ anen + [e1, . . . , em] of the coordinate subspace [e1, . . . , em]. Then,
since am+1, . . . , an are ﬁxed, the number of points in the translate with total degree
≤s is just the number of points in [e1, . . . , em] of total degree ≤s−(am+1+· · ·+an)
provided, of course, that s > am+1+· · ·+an. More generally, we have the following.
Lemma 5. Assume that α + [ei1, . . . , eim] is a translate of the coordinate subspace
[ei1, . . . , eim] ⊆Zn
≥0, where as usual α = 
i/∈{i1,...,im} aiei.
(i) The number of points in α + [ei1, . . . , eim] of total degree ≤s is equal to
m + s −|α|
s −|α|

,
provided that s > |α|.
(ii) For s > |α|, this number of points is a polynomial function of s of degree m, and
the coefﬁcient of sm is 1/m!.
Proof. (i) If s > |α|, then each point β in α + [ei1, . . . , eim] of total degree ≤s has
the form β = α + γ, where γ ∈[ei1, . . . , eim] and |γ| ≤s −|α|. The formula given
in (i) follows using Lemma 4 to count the number of possible γ.
(ii) See Exercise 6.
□
We are now ready to prove a connection between the dimension of V(I) for a
monomial ideal and the degree of the polynomial function which counts the number
of points of total degree ≤s in C(I).
Theorem 6. If I ⊆k[x1, . . . , xn] is a monomial ideal with dimV(I) = d, then for
all s sufﬁciently large, the number of monomials not in I of total degree ≤s is a
polynomial of degree d in s. Furthermore, the coefﬁcient of sd in this polynomial is
positive.
Proof. We need to determine the number of points in C(I) of total degree ≤s. By
Theorem 3, we know that C(I) can be written as a ﬁnite union
C(I) = T1 ∪T2 ∪· · · ∪Tt,
where each Ti is a translate of a coordinate subspace in Zn
≥0. We can assume that
Ti ̸= Tj for i ̸= j.

480
Chapter 9 The Dimension of a Variety
The dimension of Ti is the dimension of the associated coordinate subspace.
Since I is an ideal, it follows easily that a coordinate subspace [ei1, . . . , eir] lies
in C(I) if and only if some translate does. By hypothesis, V(I) has dimension d,
so that by Proposition 2, each Ti has dimension ≤d, with equality occurring for at
least one Ti.
We will sketch the remaining steps in the proof, leaving the veriﬁcation of several
details to the reader as exercises. To count the number of points of total degree ≤s
in C(I), we must be careful, since C(I) is a union of coordinate subspaces of Zn
≥0
that may not be disjoint [for instance, see part (d) of Example 1]. If we use the
superscript s to denote the subset consisting of elements of total degree ≤s, then it
follows that
C(I)s = Ts
1 ∪Ts
2 ∪· · · ∪Ts
t .
The number of elements in C(I)s will be denoted |C(I)s|.
In Exercise 7, you will develop a general counting principle (called the Inclusion-
Exclusion Principle) that allows us to count the elements in a ﬁnite union of ﬁnite
sets. If the sets in the union have common elements, we cannot simply add to ﬁnd
the total number of elements because that would count some elements in the union
more than once. The Inclusion-Exclusion Principle gives "correction terms" that
eliminate this multiple counting. Those correction terms are the numbers of ele-
ments in double intersections, triple intersections, etc., of the sets in question.
If we apply the Inclusion-Exclusion Principle to the above union for C(I)s, we
easily obtain
(2)
|C(I)s| =

i
|Ts
i | −

i<j
Ts
i ∩Ts
j
 +

i<j<k
Ts
i ∩Ts
j ∩Ts
k
 −· · · .
By Lemma 5, we know that for s sufﬁciently large, the number of points in Ts
i is a
polynomial of degree mi = dim(Ti) ≤d in s, and the coefﬁcient of smi is 1/mi!.
We ﬁrst note that the ﬁrst sum in (2) is a polynomial of degree d in s when s is
sufﬁciently large. The degree is exactly d because some of the Ti have dimension
d and the coefﬁcients of the leading terms are positive and hence can't cancel. If
we can show that the remaining sums in (2) correspond to polynomials of smaller
degree, it will follow that |C(I)s| is given by a polynomial of degree d in s. This will
also show that the coefﬁcient of sd is positive.
You will prove in Exercise 8 that the intersection of two distinct translates of
coordinate subspaces of dimensions m and r in Zn
≥0 is either empty or a translate of
a coordinate subspace of dimension < max(m, r). Let us see how this applies to a
nonzero term |Ts
i ∩Ts
j | in the second sum of (2). Since Ti ̸= Tj, Exercise 8 implies
that T = Ti ∩Tj is the translate of a coordinate subspace of Zn
≥0 of dimension < d,
so that by Lemma 5, the number of points in Ts = Ts
i ∩Ts
j is a polynomial in s
of degree < d. Adding these up for all i < j, we see that the second sum in (2)
is a polynomial of degree < d in s for s sufﬁciently large. The other sums in (2)
are handled similarly, and it follows that |C(I)s| is a polynomial of the desired form
when s is sufﬁciently large.
□

§2 The Complement of a Monomial Ideal
481
Let us see how this theorem works in the example I = ⟨x4y3, x2y5⟩discussed
following Theorem 3. Here, we have already seen that C(I) = C0 ∪C1, where
C1 = [e1] ∪(e2 + [e1]) ∪(2e2 + [e1]) ∪[e2] ∪(e1 + [e2]),
C0 = {(3, 4), (3, 3), (2, 4), (2, 3)}.
To count the number of points of total degree ≤s in C1, we count the number
in each translate and subtract the number which are counted more than once. (In
this case, there are no triple intersections to worry about. Do you see why?) The
number of points of total degree ≤s in [e2] is
1+s
s
	
=
1+s
1
	
= s + 1 and the
number in e1 + [e2] is
1+s−1
s−1
	
= s. Similarly, the numbers in [e1], e2 + [e1], and
2e2 + [e1] are s + 1, s, and s −1, respectively. Of the possible intersections of pairs
of these, only six are nonempty and each consists of a single point. You can check
that (1, 2), (1, 1), (1, 0), (0, 2), (0, 1), (0, 0) are the six points belonging to more
than one translate. Thus, for large s, the number of points of total degree ≤s in C1
is given by
|Cs
1| = (s + 1) + s + (s + 1) + s + (s −1) −6 = 5s −5.
Since there are four points in C0, the number of points of total degree ≤s in C(I) is
|Cs
1| + |Cs
0| = (5s −5) + 4 = 5s −1,
provided that s is sufﬁciently large. (In Exercise 9 you will show that in this case, s
is "sufﬁciently large" as soon as s ≥7.)
Theorem 6 shows that the dimension of the afﬁne variety deﬁned by a monomial
ideal is equal to the degree of the polynomial in s which counts the number of points
in C(I) of total degree ≤s for s large. This gives a purely algebraic deﬁnition of
dimension. In §3, we will extend these ideas to general ideals.
The polynomials that occur in Theorem 6 have the property that they take integer
values when the variable s is a sufﬁciently large integer. For later purposes, it will
be useful to characterize this class of polynomials. The ﬁrst thing to note is that
polynomials with this property need not have integer coefﬁcients. For example, the
polynomial 1
2s(s −1) takes integer values whenever s is an integer, but does not
have integer coefﬁcients. The reason is that either s or s −1 must be even, hence,
divisible by 2. Similarly, the polynomial
1
3·2s(s −1)(s −2) takes integer values for
any integer s: no matter what s is, one of the three consecutive integers s−2, s−1, s
must be divisible by 3 and at least one of them divisible by 2. It is easy to generalize
this argument and show that
s
d

= s(s −1) · · · (s −(d −1))
d!
=
1
d · (d −1) · · · 2 · 1s(s −1) · · · (s −(d −1))
takes integer values for any integer s (see Exercise 10). Further, in Exercises 11
and 12, you will show that any polynomial of degree d which takes integer values for
sufﬁciently large integers s can be written uniquely as an integer linear combination
of the polynomials

482
Chapter 9 The Dimension of a Variety
s
0

= 1,
s
1

= s,
s
2

= s(s −1)
2
, . . . ,
s
d

= s(s −1) · · · (s −(d −1))
d!
.
Using this fact, we obtain the following sharpening of Theorem 6.
Proposition 7. If I ⊆k[x1, . . . , xn] is a monomial ideal with dimV(I) = d, then
for all s sufﬁciently large, the number of points in C(I) of total degree ≤s is a
polynomial of degree d in s which can be written in the form
d

i=0
ai

s
d −i

,
where ai ∈Z for 0 ≤i ≤d and a0 > 0.
In the ﬁnal part of this section, we will study the projective variety associated
with a monomial ideal. This makes sense because every monomial ideal is homo-
geneous (see Exercise 13). Thus, a monomial ideal I ⊆k[x1, . . . , xn] determines
a projective variety Vp(I) ⊆Pn−1(k), where we use the subscript p to remind us
that we are in projective space. In Exercise 14, you will show that Vp(I) is a ﬁnite
union of projective linear subspaces which have dimension one less than the dimen-
sion of their afﬁne counterparts. As in the afﬁne case, we deﬁne the dimension of
a ﬁnite union of projective linear subspaces to be the maximum of the dimensions
of the subspaces. Then Theorem 6 shows that the dimension of the projective vari-
ety Vp(I) of a monomial ideal I is one less than the degree of the polynomial in s
counting the number of monomials not in I of total degree ≤s.
In this case it turns out to be more convenient to consider the polynomial in s
counting the number of monomials whose total degree is equal to s. The reason
resides in the following proposition.
Proposition 8. Let I ⊆k[x1, . . . , xn] be a monomial ideal and let Vp(I) be the pro-
jective variety in Pn−1(k) deﬁned by I. If dimVp(I) = d−1, then for all s sufﬁciently
large, the number of monomials not in I of total degree s is given by a polynomial
of the form
d−1

i=0
bi

s
d −1 −i

of degree d −1 in s, where bi ∈Z for 0 ≤i ≤d −1 and b0 > 0.
Proof. As an afﬁne variety, V(I) ⊆kn has dimension d, so that by Theorem 6, the
number of monomials not in I of total degree ≤s is a polynomial p(s) of degree d
for s sufﬁciently large. We also know that the coefﬁcient of sd is positive. It follows
that the number of monomials of total degree equal to s is given by
p(s) −p(s −1)
for s large enough. By Exercise 15, this polynomial has degree d −1 and the coef-
ﬁcient of sd−1 is positive. Since it also takes integer values when s is a sufﬁciently
large integer, it follows from the remarks preceding Proposition 7 that p(s)−p(s−1)
has the desired form.
□

§2 The Complement of a Monomial Ideal
483
In particular, this proposition says that for the projective variety deﬁned by a
monomial ideal, the dimension and the degree of the polynomial in the statement
are equal. In §3, we will extend these results to the case of arbitrary homogeneous
ideals I ⊆k[x1, . . . , xn].
EXERCISES FOR §2
1. In this exercise, we will verify some of the claims made in Example 1. Remember that
I ⊆k[x, y] is a proper monomial ideal.
a. In case (b) of Example 1, show that if s > l, then the l "lines" of monomials contain
l(s + 1) −(1 + 2 + · · · + l −1) monomials of total degree ≤s.
b. In case (b), conclude that the number of monomials not in I of total degree ≤s is
given by ls + C0 for s sufﬁciently large. Explain how to compute C0 and show that
s > a + b guarantees that s is sufﬁciently large. Illustrate your answer with a picture
that shows what can go wrong if s is too small.
c. In case (d) of Example 1, show that the constant C0 in the polynomial function giving
the number of points in C(I) of total degree ≤s is equal to the ﬁnite number of
monomials not contained in the "lines" of monomials, minus l1 ·l2 for the monomials
belonging to both families of lines, minus 1+2+· · ·+(l1−1), minus 1+· · ·+(l2−1).
2. Let I ⊆k[x1, . . . , xn] be a monomial ideal. Suppose that in Zn
≥0, the translate α +
[ei1, . . . , eir] is contained in C(I). If α = 
i/∈{i1,...,ir} aiei, show that C(I) contains all
translates β+[ei1, . . . , eir] for all β of the form β = 
i/∈{i1,...,ir} biei, where 0 ≤bi ≤ai
for all i. In particular, [ei1, . . . , eir] ⊆C(I). Hint: I is an ideal.
3. In this exercise, you will ﬁnd monomial ideals I ⊆k[x, y, z] with a given C(I) ⊆Z3
≥0.
a. Suppose that C(I) consists of one translate of [e1, e2] and two translates of [e2, e3].
Use Exercise 2 to show that C(I) = [e1, e2] ∪[e2, e3] ∪(e1 + [e2, e3]).
b. Find a monomial ideal I so that C(I) is as described in part (a). Hint: Study all mono-
mials of small degree to see whether or not they lie in I.
c. Suppose now that C(I) consists of one translate of [e1, e2], two translates of [e2, e3],
and one additional translate (not contained in the others) of the line [e2]. Use Exer-
cise 2 to give a precise description of C(I).
d. Find a monomial ideal I so that C(I) is as in part (c).
4. Let I be a monomial ideal in k[x, y, z]. In this exercise, we will study C(I) ⊆Z3
≥0.
a. Show that V(I) must be one of the following possibilities: the origin; one, two, or
three coordinate lines; one, two, or three coordinate planes; or the union of a coordi-
nate plane and a perpendicular coordinate axis.
b. Show that if V(I) contains only the origin, then C(I) has a ﬁnite number of points.
c. Show that if V(I) is a union of one, two, or three coordinate lines, then C(I) consists
of a ﬁnite number of translates of [e1], [e2], and/or [e3], together with a ﬁnite number
of points not on these translates.
d. Show that if V(I) is a union of one, two or three coordinate planes, then C(I) consists
of a ﬁnite number of translates of [e1, e2], [e1, e3], and/or [e2, e3] plus, possibly, a ﬁnite
number of translates of [e1], [e2], and/or [e3] (where a translate of [ei] cannot occur
unless [ei, ej] ⊆C(I) for some j ̸= i) plus, possibly, a ﬁnite number of points not on
these translates.
e. Finally, show that if V(I) is the union of a coordinate plane and the perpendicular
coordinate axis, then C(I) consists of a ﬁnite nonzero number of translates of a single
coordinate plane [ei, ej], plus a ﬁnite nonzero number of translates of [eℓ], ℓ̸= i, j,
plus, possibly, a ﬁnite number of translates of [ei] and/or [ej], plus a ﬁnite number of
points not on any of these translates.
5. Show that the number of points in any m-dimensional coordinate subspace of Zn
≥0 of
total degree ≤s is given by
m+s
s

.

484
Chapter 9 The Dimension of a Variety
6. Prove part (ii) of Lemma 5.
7. In this exercise, you will develop a counting principle, called the Inclusion-Exclusion
Principle. The idea is to give a general method for counting the number of elements in a
union of ﬁnite sets. We will use the notation |A| for the number of elements in the ﬁnite
set A.
a. Show that for any two ﬁnite sets A and B,
|A ∪B| = |A| + |B| −|A ∩B|.
b. Show that for any three ﬁnite sets A, B, C,
|A ∪B ∪C| = |A| + |B| + |C| −|A ∩B| −|A ∩C| −|B ∩C| + |A ∩B ∩C|.
c. Using induction on the number of sets, show that the number of elements in a union
of n ﬁnite sets A1 ∪· · · ∪An is equal to the sum of the |Ai|, minus the sum of all
double intersections |Ai ∩Aj|, i < j, plus the sum of all the threefold intersections
|Ai ∩Aj ∩Ak|, i < j < k, minus the sum of the fourfold intersections, etc. This can
be written as the following formula:
|A1 ∪· · · ∪An| =
n

r=1
(−1)r−1


1≤i1<···<ir≤n
|Ai1 ∩· · · ∩Air|

.
8. In this exercise, you will show that the intersection of two translates of different coordi-
nate subspaces of Zn
≥0 is a translate of a lower dimensional coordinate subspace.
a. Let A = α+[ei1 . . . , eim], where α = 
i/∈{i1,...,im} aiei, and let B = β+[ej1, . . . , ejr],
where β = 
i/∈{j1,...,jr} biei If A ̸= B and A ∩B ̸= ∅, then show that
[ei1, . . . , eim] ̸= [ej1, . . . , ejr]
and that A ∩B is a translate of
[ei1, . . . , eim] ∩[ej1, . . . , ejr].
b. Deduce that dim A ∩B < max(m, r).
9. Show that if s ≥7, then the number of elements in C(I) of total degree ≤s for the
monomial ideal I in the example following Theorem 6 is given by the polynomial 5s−1.
10. Show that the polynomial
p(s) =

s
d

= s(s −1) · · · (s −(d −1))
d!
takes integer values for all integers s. Note that p is a polynomial of degree d in s.
11. In this exercise, we will show that every polynomial p(s) of degree ≤d which takes
integer values for every s ∈Z≥0 can be written as a unique linear combination with
integer coefﬁcients of the polynomials
s
0

,
s
1

,
s
2

, . . . ,
s
d

.
a. Show that the polynomials

s
0

,

s
1

,

s
2

, . . . ,

s
d

are linearly independent in the sense that
a0

s
0

+ a1

s
1

+ · · · + ad

s
d

= 0
for all s implies that a0 = a1 = · · · = ad = 0.

§2 The Complement of a Monomial Ideal
485
b. Show that any two polynomials p(s) and q(s) of degree ≤d which take the same
values at the d + 1 points s = 0, 1, . . . , d must be identical. Hint: How many roots
does the polynomial p(s) −q(s) have?
c. Suppose we want to construct a polynomial p(s) that satisﬁes
p(0) = c0,
p(1) = c1,
...
p(d) = cd,
where the ci are given values in Z. Show that if we set
Δ0 = c0,
Δ1 = c1 −c0,
Δ2 = c2 −2c1 + c0,
...
Δd =
d

n=0
(−1)n

d
n

cd−n,
then the polynomial
p(s) = Δ0

s
0

+ Δ1

s
1

+ · · · + Δd

s
d

satisﬁes p(i) = ci for i = 0, . . . , d. Hint: Argue by induction on d. [The polynomial
p(s) is called a Newton-Gregory interpolating polynomial.]
d. Explain why the Newton-Gregory polynomial takes integer values for all integer s.
Hint: Recall that the ci are integers. See also Exercise 10.
e. Deduce from parts (a)-(d) that every polynomial of degree d which takes integer
values for all integer s ≥0 can be written as a unique integer linear combination of
s
0

, . . . ,
s
d

.
12. Suppose that p(s) is a polynomial of degree d which takes integer values when s is a
sufﬁciently large integer, say s ≥a. We want to prove that p(s) is an integer linear
combination of the polynomials
s
0

, . . . ,
s
d

studied in Exercises 10 and 11. We can
assume that a is a positive integer.
a. Show that the polynomial p(s + a) can be expressed in terms of
s
0

, . . . ,
s
d

and
conclude that p(s) is an integer linear combination of
s−a
0

, . . . ,
s−a
d

.
b. Use Exercise 10 to show that p(s) takes integer values for all s ∈Z and conclude that
p(s) is an integer linear combination of
s
0

, . . . ,
s
d

.
13. Show that every monomial ideal is a homogeneous ideal.
14. Let I ⊆k[x1, . . . , xn] be a monomial ideal.
a. In kn, let V(xi1, . . . , xir) be a coordinate subspace of dimension n −r contained in
V(I). Prove that Vp(xi1, . . . , xir) ⊆Vp(I) in Pn−1(k). Also show that Vp(xi1, . . . , xir)
looks like a copy of Pn−r−1 sitting inside Pn−1. Thus, we say that Vp(xi1, . . . , xir) is a
projective linear subspace of dimension n −r −1.
b. Prove the claim made in the text that Vp(I) is a ﬁnite union of projective linear sub-
spaces of dimension one less than their afﬁne counterparts.
15. Verify the statement in the proof of Proposition 8 that if p(s) is a polynomial of degree d
in s with a positive coefﬁcient of sd, then p(s)−p(s−1) is a polynomial of degree d −1
with a positive coefﬁcient of sd−1.

486
Chapter 9 The Dimension of a Variety
§3
The Hilbert Function and the Dimension of a Variety
In this section, we will deﬁne the Hilbert function of an ideal I and use it to deﬁne
the dimension of a variety V. We will give the deﬁnitions in both the afﬁne and
projective cases. The basic idea will be to use the experience gained in the last
section and deﬁne dimension in terms of the number of monomials not contained in
the ideal I. In the afﬁne case, we will use the number of monomials not in I of total
degree ≤s, whereas in the projective case, we consider those of total degree equal
to s.
However, we need to note that the results from §2 do not apply directly because
when I is not a monomial ideal, different monomials not in I can be dependent on
one another. For instance, if I = ⟨x2 −y2⟩, neither the monomial x2 nor y2 belongs
to I, but their difference does. So we should not regard x2 and y2 as two monomials
not in I. Rather, to generalize §2, we will need to consider the number of monomials
of total degree ≤s which are "linearly independent modulo" I.
In Chapter 5, we deﬁned the quotient of a ring modulo an ideal. There is an
analogous operation on vector spaces which we will use to make the above ideas
precise. Given a vector space V and a subspace W ⊆V, it is not difﬁcult to show
that the relation on V deﬁned by v ∼v′ if v −v′ ∈W is an equivalence relation (see
Exercise 1). The set of equivalence classes of ∼is denoted V/W, so that
V/W = {[v] | v ∈V}.
In the exercises, you will check that the operations [v]+[v′] = [v+v′] and a[v] = [av],
where a ∈k and v, v′ ∈V, are well-deﬁned and make V/W into a k-vector space,
called the quotient space of V modulo W.
When V is ﬁnite-dimensional, we can compute the dimension of V/W as follows.
Proposition 1. Let W be a subspace of a ﬁnite-dimensional vector space V. Then W
and V/W are also ﬁnite-dimensional vector spaces, and
dimV = dimW + dimV/W.
Proof. If V is ﬁnite-dimensional, it is a standard fact from linear algebra that W
is also ﬁnite-dimensional. Let v1, . . . , vm be a basis of W, so that dimW = m.
In V, the vectors v1, . . . , vm are linearly independent and, hence, can be extended
to a basis v1, . . . , vm, vm+1, . . . vm+n of V. Thus, dimV = m + n. We claim that
[vm+1], . . . , [vm+n] form a basis of V/W.
To see that they span, take [v] ∈V/W. If we write v = m+n
i=1 aivi, then v ∼
am+1vm+1 + · · · + am+nvm+n since their difference is a1v1 + · · · + amvm ∈W. It
follows that in V/W, we have
[v] = [am+1vm+1 + · · · + am+nvm+n] = am+1[vm+1] + · · · + am+n[vm+n].
The proof that [vm+1], . . . , [vm+n] are linearly independent is left to the reader (see
Exercise 2). This proves the claim, and the proposition follows immediately.
□

§3 The Hilbert Function and the Dimension of a Variety
487
The Dimension of an Afﬁne Variety
Considered as a vector space over k, the polynomial ring R = k[x1, . . . , xn] has
inﬁnite dimension, and the same is true for any nonzero ideal (see Exercise 3). To
get something ﬁnite-dimensional, we will restrict ourselves to polynomials of total
degree ≤s. Hence, we let
R≤s = k[x1, . . . , xn]≤s
denote the set of polynomials of total degree ≤s in R. By Lemma 4 of §2, it follows
that R≤s is a vector space of dimension
n+s
s
	
. Then, given an ideal I ⊆R, we let
I≤s = I ∩R≤s
denote the set of polynomials in I of total degree ≤s. Note that I≤s is a vector
subspace of R≤s. We are now ready to deﬁne the afﬁne Hilbert function of I.
Deﬁnition 2. Let I be an ideal in R = k[x1, . . . , xn]. The afﬁne Hilbert function of
I is the function on the nonnegative integers s deﬁned by
aHFR/I(s) = dimR≤s/I≤s = dimR≤s −dimI≤s,
where the second equality is by Proposition 1.
Strictly speaking, aHFR/I is the afﬁne Hilbert function of R/I, but we prefer to
call it the afﬁne Hilbert function of I. With this terminology, the results of §2 for
monomial ideals can be restated as follows.
Proposition 3. Let I be a proper monomial ideal in R = k[x1, . . . , xn].
(i) For s ≥0, aHFR/I(s) is the number of monomials not in I of total degree ≤s.
(ii) For s sufﬁciently large, the afﬁne Hilbert function of I is given by a polynomial
function
aHFR/I(s) =
d

i=0
bi

s
d −i

,
where bi ∈Z and b0 is positive.
(iii) The degree of the polynomial in part (ii) is the maximum of the dimensions of
the coordinate subspaces contained in V(I).
Proof. To prove (i), ﬁrst note that {xα | |α| ≤s} is a basis of R≤s as a vector space
over k. Further, Lemma 3 of Chapter 2, §4 shows that {xα | |α| ≤s, xα ∈I} is a
basis of I≤s. Consequently, the monomials in {xα | |α| ≤s, xα /∈I} are exactly
what we add to a basis of I≤s to get a basis of R≤s. It follows from the proof of
Proposition 1 that {[xα] | |α| ≤s, xα /∈I} is a basis of the quotient space R≤s/I≤s,
which completes the proof of (i).
Parts (ii) and (iii) follow easily from (i) and Proposition 7 of §2.
□

488
Chapter 9 The Dimension of a Variety
We are now ready to link the ideals of §2 to arbitrary ideals in R = k[x1, . . . , xn].
The key ingredient is the following observation due to Macaulay. As in Chapter 8,
§4, we say that a monomial order > on k[x1, . . . , xn] is a graded order if xα > xβ
whenever |α| > |β|.
Proposition 4. Let I ⊆R = k[x1, . . . , xn] be an ideal and let > be a graded order
on R. Then the monomial ideal ⟨LT(I)⟩has the same afﬁne Hilbert function as I.
Proof. Since ⟨LT(I)⟩= {0} when I = {0}, we may assume I ̸= {0}. Fix s and
consider the leading monomials LM( f) of all elements f ∈I≤s. There are only
ﬁnitely many such monomials, so that
(1)
{LM( f) | f ∈I≤s} = {LM( f1), . . . , LM( fm)}
for some polynomials f1, . . . , fm ∈I≤s. By rearranging and deleting duplicates, we
can assume that LM( f1) > LM( f2) > · · · > LM( fm). We claim that f1, . . . , fm form
a basis of I≤s as a vector space over k.
To prove this, consider a nontrivial linear combination a1f1 + · · · + amfm and
choose the smallest i such that ai ̸= 0. Given how we ordered the leading monomi-
als, there is nothing to cancel aiLT( fi), so the linear combination is nonzero. Hence,
f1, . . . , fm are linearly independent. Next, let W = [ f1, . . . , fm] ⊆I≤s be the sub-
space spanned by f1, . . . , fm. If W ̸= I≤s, pick f ∈I≤s \ W with LM( f) minimal.
By (1), LM( f) = LM( fi) for some i, and hence, LT( f) = λLT( fi) for some λ ∈k.
Then f −λfi ∈I≤s has a smaller leading monomial, so that f −λfi ∈W by the
minimality of LM( f). This implies f ∈W, which is a contradiction. It follows that
W = [ f1, . . . , fm] = I≤s, and we conclude that f1, . . . , fm form a basis.
The monomial ideal ⟨LT(I)⟩is generated by the leading terms (or leading mono-
mials) of elements of I. Thus, LM( fi) ∈⟨LT(I)⟩≤s since fi ∈I≤s. We claim that
LM( f1), . . . , LM( fm) form a vector space basis of ⟨LT(I)⟩≤s. Arguing as above, it is
easy to see that they are linearly independent. It remains to show that they span, i.e.,
that [LM( f1), . . . , LM( fm)] = ⟨LT(I)⟩≤s. By Lemma 3 of Chapter 2, §4, it sufﬁces
to show that
(2)
{LM( f1), . . . , LM( fm)} = {LM( f) | f ∈I, LM( f) has total degree ≤s}.
To relate this to (1), note that > is a graded order, which implies that for any nonzero
polynomial f ∈k[x1, . . . , xn], LM( f) has the same total degree as f. In particular, if
LM( f) has total degree ≤s, then so does f, which means that (2) follows immedi-
ately from (1).
Thus, I≤s and ⟨LT(I)⟩≤s have the same dimension (since they both have bases
consisting of m elements), and then the dimension formula of Proposition 1 implies
aHFR/I(s) = dimR≤s/I≤s = dimR≤s/⟨LT(I)⟩≤s = aHFR/⟨LT(I)⟩(s).
This proves the proposition.
□
If we combine Propositions 3 and 4, it follows immediately that if I is any ideal in
k[x1, . . . , xn] and s is sufﬁciently large, the afﬁne Hilbert function of I can be written

§3 The Hilbert Function and the Dimension of a Variety
489
aHFR/I(s) =
d

i=0
bi

s
d −i

,
where the bi are integers and b0 is positive. This leads to the following deﬁnition.
Deﬁnition 5. The polynomial which equals aHFR/I(s) for sufﬁciently large s is
called the afﬁne Hilbert polynomial of I and is denoted aHPR/I(s).
As with the afﬁne Hilbert function, aHPR/I(s) should be called the afﬁne Hilbert
polynomial of R/I, but we will use the terminology of Deﬁnition 5.
As an example, consider the ideal I = ⟨x3y2 + 3x2y2 + y3 + 1⟩⊆R = k[x, y].
If we use grlex, then ⟨LT(I)⟩= ⟨x3y2⟩, and using the methods of §2, one can show
that the number of monomials not in ⟨LT(I)⟩of total degree ≤s equals 5s −5 when
s ≥3. From Propositions 3 and 4, we obtain
aHFR/I(s) = aHFR/⟨LT(I)⟩(s) = 5s −5
when s ≥3. It follows that the afﬁne Hilbert polynomial of I is
aHPR/I(s) = 5s −5.
By deﬁnition, the afﬁne Hilbert function of an ideal I coincides with the afﬁne
Hilbert polynomial of I when s is sufﬁciently large. The smallest integer s0 such
that aHPR/I(s) = aHFR/I(s) for all s ≥s0 is called the index of regularity of I.
Determining the index of regularity is of considerable interest and importance in
many computations with ideals, but we will not pursue this topic in detail here.
We next compare the degrees of the afﬁne Hilbert polynomials of I and
√
I.
Proposition 6. If I ⊆R = k[x1, . . . , xn] is an ideal, then the afﬁne Hilbert polyno-
mials of I and
√
I have the same degree.
Proof. For a monomial ideal I, we know that the degree of the afﬁne Hilbert poly-
nomial is the dimension of the largest coordinate subspace of kn contained in V(I).
Since
√
I is monomial by Exercise 9 of §1 and V(I) = V(
√
I), it follows immedi-
ately that aHPR/I and aHPR/
√
I have the same degree.
Now let I be an arbitrary ideal in R = k[x1, . . . , xn] and pick any graded order >
in R. We claim that
(3)
⟨LT(I)⟩⊆⟨LT(
√
I)⟩⊆
"
⟨LT(I)⟩.
The ﬁrst containment is immediate from I ⊆
√
I. To establish the second, let xα
be a monomial in LT(
√
I). This means that there is a polynomial f ∈
√
I such that
LT( f) = xα. We know that f r ∈I for some r ≥0, and it follows that (xα)r =
LT( f)r = LT( f r) ∈⟨LT(I)⟩. Thus, xα ∈
"
⟨LT(I)⟩.
If we set J = ⟨LT(I)⟩, then we can write (3) as
J ⊆⟨LT(
√
I)⟩⊆
√
J.

490
Chapter 9 The Dimension of a Variety
In Exercise 8, we will prove that if I1 ⊆I2 are any ideals of R = k[x1, . . . , xn],
then deg(aHPR/I1) ≥deg(aHPR/I2). If we apply this fact to the above inclusions, we
obtain the inequalities
deg(aHPR/J) ≥deg(aHPR/⟨LT(
√
I)⟩) ≥deg(aHPR/
√
J).
By the result for monomial ideals, the two outer terms here are equal, so that all
three degrees are equal. Since J = ⟨LT(I)⟩, we obtain
deg(aHPR/⟨LT(I)⟩) = deg(aHPR/⟨LT(
√
I)⟩).
By Proposition 4, the same is true for aHPR/I and aHPR/
√
I, and we are done.
□
This proposition is evidence of something that is not at all obvious, namely, that
the degree of the afﬁne Hilbert polynomial has geometric meaning in addition to its
algebraic signiﬁcance in indicating how far I≤s is from being all of k[x1, . . . , xn]≤s.
Recall that V(I) = V(
√
I) for all ideals. Thus, the degree of the afﬁne Hilbert poly-
nomial is the same for a large collection of ideals deﬁning the same variety. More-
over, we know from §2 that the degree of the afﬁne Hilbert polynomial is the same as
our intuitive notion of the dimension of the variety of a monomial ideal. So it should
be no surprise that in the general case, we deﬁne dimension in terms of the degree
of the afﬁne Hilbert function. We will always assume that the ﬁeld k is inﬁnite.
Deﬁnition 7. The dimension of a nonempty afﬁne variety V ⊆kn, denoted dimV,
is the degree of the afﬁne Hilbert polynomial of the corresponding ideal I = I(V) ⊆
R = k[x1, . . . , xn].
When V = ∅, we have 1 ∈I(V), which implies R≤s = I(V)≤s for all s. Hence,
aHPR/I(V) = 0. Since the zero polynomial does not have a degree, we do not assign
a dimension to the empty variety.
As an example, consider the twisted cubic V = V(y −x2, z −x3) ⊆R3. In
Chapter 1, we showed that I = I(V) = ⟨y −x2, z −x3⟩⊆R = R[x, y, z]. Using
grlex, a Gröbner basis for I is {y3 −z2, x2 −y, xy −z, xz −y2}, so that ⟨LT(I)⟩=
⟨y3, x2, xy, xz⟩. Then
dimV = deg(aHPR/I)
= deg(aHPR/⟨LT(I)⟩)
= maximum dimension of a coordinate subspace in V(⟨LT(I)⟩)
by Propositions 3 and 4. Since
V(⟨LT(I)⟩) = V(y3, x2, xy, xz) = V(x, y) ⊆R3,
we conclude that dimV = 1. This agrees with our intuition that the twisted cubic
should be 1-dimensional since it is a curve in R3.
For another example, let us compute the dimension of the variety of a monomial
ideal. In Exercise 10, you will show that I(V(I)) =
√
I when I is a monomial ideal
and k is inﬁnite. Then Proposition 6 implies that

§3 The Hilbert Function and the Dimension of a Variety
491
dimV(I) = deg(aHPR/I(V(I))) = deg(aHPR/
√
I) = deg(aHPR/I),
and it follows from part (iii) of Proposition 3 that dimV(I) is the maximum dimen-
sion of a coordinate subspace contained in V(I). This agrees with the provisional
deﬁnition of dimension given in §2. In Exercise 10, you will see that this can fail
when k is a ﬁnite ﬁeld.
One drawback of Deﬁnition 7 is that to ﬁnd the dimension of a variety V, we
need to know I(V), which, in general, is difﬁcult to compute. It would be much
nicer if dimV were the degree of aHPR/I, where I is an arbitrary ideal deﬁning V.
Unfortunately, this is not true in general. For example, if I = ⟨x2 + y2⟩⊆R =
R[x, y], it is easy to check that aHPR/I has degree 1. Yet V = V(I) = {(0, 0)} ⊆R2
is easily seen to have dimension 0. Thus, dimV(I) ̸= deg(aHPR/I) in this case (see
Exercise 11 for the details).
When the ﬁeld k is algebraically closed, these difﬁculties go away. More pre-
cisely, we have the following theorem that tells us how to compute the dimension in
terms of any deﬁning ideal.
Theorem 8 (The Dimension Theorem). Let V = V(I) be a nonempty afﬁne vari-
ety, where I ⊆R = k[x1, . . . , xn] is an ideal. If k is algebraically closed, then
dimV = deg(aHPR/I).
Furthermore, if > is a graded order on R, then
dimV = deg(aHPR/⟨LT(I)⟩)
= maximum dimension of a coordinate subspace in V(⟨LT(I)⟩).
Finally, the last two equalities hold over any ﬁeld k when I = I(V).
Proof. Since k is algebraically closed, the Nullstellensatz implies that I(V) =
I(V(I)) =
√
I. Then
dimV = deg(aHPR/I(V)) = deg(aHPR/
√
I) = deg(aHPR/I),
where the last equality is by Proposition 6. The second part of the theorem now
follows immediately using Propositions 3 and 4.
□
In other words, over an algebraically closed ﬁeld, to compute the dimension of a
variety V = V(I), one can proceed as follows:
• Compute a Gröbner basis for I using a graded order such as grlex or grevlex.
• Compute the maximal dimension d of a coordinate subspace that is contained in
V(⟨LT(I)⟩). Note that Proposition 3 of §1 gives an algorithm for doing this.
Then dimV = d follows from Theorem 8.

492
Chapter 9 The Dimension of a Variety
The Dimension of a Projective Variety
Our discussion of the dimension of a projective variety V ⊆Pn(k) will parallel what
we did in the afﬁne case and, in particular, many of the arguments are the same. We
start by deﬁning the Hilbert function and the Hilbert polynomial for an arbitrary
homogeneous ideal I ⊆S = k[x0, . . . , xn]. As above, we assume that k is inﬁnite.
As we saw in §2, the projective case uses total degree equal to s rather than ≤s.
Since polynomials of total degree s do not form a vector space (see Exercise 13),
we will work with homogeneous polynomials of total degree s. Let
Ss = k[x0, . . . , xn]s
denote the set of homogeneous polynomials of total degree s in S, together with the
zero polynomial. In Exercise 13, you will show that Ss is a vector space of dimension
n+s
s
	
. If I ⊆S is a homogeneous ideal, we let
Is = I ∩Ss
denote the set of homogeneous polynomials in I of total degree s (and the zero
polynomial). Note that Is is a vector subspace of Ss. Then the Hilbert function of I
is deﬁned by
HFS/I(s) = dimSs/Is.
Strictly speaking, we should call this the projective Hilbert function of S/I, but the
above terminology is what we will use in this book.
When I ⊆S is a monomial ideal, the argument of Proposition 3 adapts easily to
show that HFS/I(s) is the number of monomials not in I of total degree s. It follows
from Proposition 8 of §2 that for s sufﬁciently large, we can express the Hilbert
function of a monomial ideal in the form
(4)
HFS/I(s) =
d

i=0
bi

s
d −i

,
where bi ∈Z and b0 is positive. We also know that d is the largest dimension of a
projective coordinate subspace contained in V(I) ⊆Pn(k).
As in the afﬁne case, we can use a monomial order to link the Hilbert function of
a homogeneous ideal to the Hilbert function of a monomial ideal.
Proposition 9. Let I ⊆S = k[x0, . . . , xn] be a homogeneous ideal and let > be
a monomial order on S. Then the monomial ideal ⟨LT(I)⟩has the same Hilbert
function as I.
Proof. The argument is similar to the proof of Proposition 4. However, since we do
not require that > be a graded order, some changes are needed.
For a ﬁxed s, we can ﬁnd f1, . . . , fm ∈Is such that
(5)
{LM( f) | f ∈Is} = {LM( f1), . . . , LM( fm)}

§3 The Hilbert Function and the Dimension of a Variety
493
and we can assume that LM( f1) > LM( f2) > · · · > LM( fm). As in the proof of
Proposition 4, f1, . . . , fm form a basis of Is as a vector space over k.
Now consider ⟨LT(I)⟩s. We know LM( fi) ∈⟨LT(I)⟩s since fi ∈Is, and we need
to show that LM( f1), . . . , LM( fm) form a vector space basis of ⟨LT(I)⟩s. The leading
terms are distinct, so as above, they are linearly independent. It remains to prove
that they span. By Lemma 3 of Chapter 2, §4, it sufﬁces to show that
(6)
{LM( f1), . . . , LM( fm)} = {LM( f) | f ∈I, LM( f) has total degree s}.
To relate this to (5), suppose that LM( f) has total degree s for some f ∈I. If we
write f as a sum of homogeneous polynomials f = 
i hi, where hi has total degree
i, it follows that LM( f) = LM(hs) since LM( f) has total degree s. Since I is a
homogeneous ideal, we have hs ∈I. Thus, LM( f) = LM(hs) where hs ∈Is, and,
consequently, (6) follows from (5). From here, the argument is identical to what we
did in Proposition 4, and we are done.
□
If we combine Proposition 9 with the description of the Hilbert function for a
monomial ideal given by (4), we see that for any homogeneous ideal I ⊆S =
k[x0, . . . , xn], the Hilbert function can be written
HFS/I(s) =
d

i=0
bi

s
d −i

for s sufﬁciently large. The polynomial on the right of this equation is called the
Hilbert polynomial of I and is denoted HPS/I(s).
We then deﬁne the dimension of a projective variety in terms of the Hilbert poly-
nomial as follows.
Deﬁnition 10. The dimension of a nonempty projective variety V ⊂Pn(k), denoted
dimV, is the degree of the Hilbert polynomial of the corresponding homogeneous
ideal I = I(V) ⊆S = k[x0, . . . , xn]. (Note that I is homogeneous by Proposition 4
of Chapter 8, §3.)
As in the afﬁne case, the dimension of the empty variety is not deﬁned. Over an
algebraically closed ﬁeld, we can compute the dimension as follows.
Theorem 11 (The Dimension Theorem). Let V = V(I) ⊆Pn(k) be a projective
variety, where I ⊆S = k[x0, . . . , xn] is a homogeneous ideal. If V is nonempty and
k is algebraically closed, then
dimV = deg(HPS/I).
Furthermore, for any monomial order on S, we have
dimV = deg(HPS/⟨LT(I)⟩)
= maximum dimension of a projective coordinate subspace in V(⟨LT(I)⟩).
Finally, the last two equalities hold over any ﬁeld k when I = I(V).

494
Chapter 9 The Dimension of a Variety
Proof. The ﬁrst step is to show that I and
√
I have Hilbert polynomials of the same
degree. The proof is similar to what we did in Proposition 6 and is left as an exercise.
By the projective Nullstellensatz, we know that I(V) = I(V(I)) =
√
I, and, from
here, the proof is identical to what we did in the afﬁne case (see Theorem 8).
□
For our ﬁnal result, we compare the dimension of afﬁne and projective varieties.
Theorem 12. (i) Let I ⊆S = k[x0, . . . , xn] be a homogeneous ideal. Then for
s ≥1, we have
HFS/I(s) = aHFS/I(s) −aHFS/I(s −1).
There is a similar relation between Hilbert polynomials. Consequently, if
V ⊆Pn(k) is a nonempty projective variety and CV ⊆kn+1 is its afﬁne cone
(see Chapter 8, §3), then
dimCV = dimV + 1.
(ii) Let I ⊆R = k[x1, . . . , xn] be an ideal and let Ih ⊆S = k[x0, . . . , xn] be its
homogenization with respect to x0 (see Chapter 8, §4). Then for s ≥0, we have
aHFR/I(s) = HFS/Ih(s).
There is a similar relation between Hilbert polynomials. Consequently, if V ⊆kn
is a nonempty afﬁne variety and V ⊂Pn(k) is its projective closure (see Chap-
ter 8, §4), then
dimV = dimV.
Proof. We will use the subscripts a and p to indicate the afﬁne and projective cases
respectively. The ﬁrst part of (i) follows easily by reducing to the case of a monomial
ideal and using the results of §2. We leave the details as an exercise. For the second
part of (i), note that the afﬁne cone CV is simply the afﬁne variety in kn+1 deﬁned
by Ip(V). Further, it is easy to see that Ia(CV) = Ip(V) (see Exercise 19). Thus, the
dimensions of V and CV are the degrees of HPS/I p(V) and aHPS/Ip(V), respectively.
Then dimCV = dimV + 1 follows from Exercise 15 of §2 and the relation just
proved between the Hilbert polynomials.
To prove the ﬁrst part of (ii), consider the maps
φ : R≤s = k[x1, . . . , xn]≤s −→Ss = k[x0, . . . , xn]s,
ψ : Ss = k[x0, . . . , xn]s −→R≤s = k[x1, . . . , xn]≤s
deﬁned by the formulas
(7)
φ( f) = xs
0 f
x1
x0
, . . . , xn
x0

for f ∈R≤s,
ψ(F) = F(1, x1, . . . , xn)
for F ∈Ss.

§3 The Hilbert Function and the Dimension of a Variety
495
We leave it as an exercise to check that these are linear maps that are inverses of
each other, and hence, R≤s and Ss are isomorphic vector spaces. You should also
check that if f ∈R≤s has total degree d ≤s, then
φ( f) = xs−d
0
f h,
where f h is the homogenization of f as deﬁned in Proposition 7 of Chapter 8, §2.
Under these linear maps, you will check in the exercises that
(8)
φ(I≤s) ⊆Ih
s ,
ψ(Ih
s ) ⊆I≤s,
and it follows easily that the above inclusions are equalities. Thus, I≤s and Ih
s are
also isomorphic vector spaces.
This shows that R≤s and Ss have the same dimension, and the same holds for I≤s
and Ih
s . By the dimension formula of Proposition 1, we conclude that
aHFR/I(s) = dimR≤s/I≤s = dimSs/Ih
s = HFIh(s),
which then implies that aHPR/I = HPIh.
For the second part of (ii), suppose V ⊆kn. Let I = Ia(V) ⊆k[x1, . . . , xn] and let
Ih ⊆k[x0, . . . , xn] be the homogenization of I with respect to x0. Then V is deﬁned
to be Vp(Ih) ⊂Pn(k). Furthermore, we know from Exercise 8 of Chapter 8, §4 that
Ih = Ip(V). Then
dimV = deg(aHPR/I) = deg(HPS/Ih) = dimV
follows immediately from aHPR/I = HPIh, and the theorem is proved.
□
The computer algebra systems Maple and Sage can compute the Hilbert polyno-
mial of a homogeneous ideal, and the same is true for the more specialized programs
CoCoA, Macaulay2 and Singular.
EXERCISES FOR §3
1. In this exercise, you will verify that if V is a vector space and W is a subspace of V, then
V/W is a vector space.
a. Show that the relation on V deﬁned by v ∼v′ if v−v′ ∈W is an equivalence relation.
b. Show that the addition and scalar multiplication operations on the equivalence classes
deﬁned in the text are well-deﬁned. Thus, if v, v′, w, w′ ∈V are such that [v] = [v′]
and [w] = [w′], then show that [v + w] = [v′ + w′] and [av] = [av′] for all a ∈k.
c. Verify that V/W is a vector space under the operations given in part (b).
2. Let V be a ﬁnite-dimensional vector space and let W be a vector subspace of V. If
{v1, . . . , vm, vm+1, . . . , vm+n} is a basis of V such that {v1, . . . , vm} is a basis for W,
then show that [vm+1], . . . , [vm+n] are linearly independent in V/W.
3. Show that a nonzero ideal I ⊆k[x1, . . . , xn] has inﬁnite dimension as a vector space
over k. Hint: Pick f ̸= 0 in I and consider xαf .

496
Chapter 9 The Dimension of a Variety
4. The proofs of Propositions 4 and 9 involve ﬁnding vector space bases of k[x1, . . . , xn]≤s
and k[x0, . . . , xn]s where the elements in the bases have distinct leading terms. We
showed that such bases exist, but our proof was nonconstructive. In this exercise, we
will illustrate a method for actually ﬁnding such a basis. We will only discuss the homo-
geneous case, but the method applies equally well to the afﬁne case.
The basic idea is to start with any basis of I, and order the elements according to their
leading terms. If two of the basis elements have the same leading monomial, we can
replace one of them with a k-linear combination that has a smaller leading monomial.
Continuing in this way, we will get the desired basis.
To see how this works in practice, let I be a homogeneous ideal in k[x, y, z], and
suppose that {x3 −xy2, x3 + x2y −z3, x2y −y3} is a basis for I3. We will use grlex order
with x > y > z.
a. Show that if we subtract the ﬁrst polynomial from the second, leaving the third poly-
nomial unchanged, then we get a new basis for I3.
b. The second and third polynomials in this new basis now have the same leading
monomial. Show that if we change the third polynomial by subtracting the sec-
ond polynomial from it and multiplying the result by −1, we end up with a basis
{x3 −xy2, x2y + xy2 −z3, xy2 + y3 −z3} for I3 in which all three leading monomials
are distinct.
5. Let I = ⟨x3 −xyz, y4 −xyz2, xy−z2⟩. Using grlex with x > y > z, ﬁnd bases of I3 and I4
where the elements in the bases have distinct leading monomials. Hint: Use the method
of Exercise 4.
6. Use the methods of §2 to compute the afﬁne Hilbert polynomials for each of the follow-
ing ideals.
a. I = ⟨x3y, xy2⟩⊆k[x, y].
b. I = ⟨x3y2 + 3x2y2 + y3 + 1⟩⊆k[x, y].
c. I = ⟨x3yz5, xy3z2⟩⊆k[x, y, z].
d. I = ⟨x3 −yz2, y4 −x2yz⟩⊆k[x, y, z].
7. Find the index of regularity [that is, the smallest s0 such that aHFI(s) = aHPI(s) for all
s ≥s0] for each of the ideals in Exercise 6.
8. In this exercise, we will show that if I1 ⊆I2 are ideals in R = k[x1, . . . , xn], then
deg(aHPR/I1) ≥deg(aHPR/I2).
a. Show that I1 ⊆I2 implies C(⟨LT(I1)⟩) ⊇C(⟨LT(I2)⟩) in Zn
≥0.
b. Show that for s ≥0, the afﬁne Hilbert functions satisfy the inequality
aHFR/I1(s) ≥aHFR/I2(s).
c. From part (b), deduce the desired statement about the degrees of the afﬁne Hilbert
polynomials. Hint: Argue by contradiction and consider the values of the polynomials
as s →∞.
d. If I1 ⊆I2 are homogeneous ideals in S = k[x0, . . . , xn], prove an analogous inequality
for the degrees of the Hilbert polynomials of I1 and I2.
9. Use Deﬁnition 7 to show that a point p = (a1, . . . , an) ∈kn gives a variety of dimension
zero. Hint: Use Exercise 7 of Chapter 4, §5 to describe I({p}).
10. Let I ⊆k[x1, . . . , xn] be a monomial ideal, and assume that k is an inﬁnite ﬁeld. In this
exercise, we will study I(V(I)).
a. Show that I(V(xi1, . . . , xir)) = ⟨xi1, . . . , xir⟩. Hint: Use Proposition 5 of Chap-
ter 1, §1.
b. Show that an intersection of monomial ideals is a monomial ideal. Hint: Use Lemma 3
of Chapter 2, §4.
c. Show that I(V(I)) is a monomial ideal. Hint: Use parts(a) and (b) together with
Theorem 15 of Chapter 4, §3.

§3 The Hilbert Function and the Dimension of a Variety
497
d. The ﬁnal step is to show that I(V(I)) =
√
I. We know that
√
I ⊆I(V(I)), and since
I(V(I)) is a monomial ideal, you need only prove that xα ∈I(V(I)) implies that
xrα ∈I for some r > 0. Hint: If I = ⟨m1, . . . , mt⟩and xrα /∈I for r > 0, show that
for every j, there is xij such that xij divides mj but not xα. Use xi1, . . . , xit to obtain a
contradiction.
e. Let F2 be a ﬁeld with of two elements and let I = ⟨x⟩⊆F2[x, y]. Show that I(V(I)) =
⟨x, y2 −y⟩. This is bigger than
√
I and is not a monomial ideal.
11. Let I = ⟨x2 + y2⟩⊆R = R[x, y].
a. Show carefully that deg(aHPR/I) = 1.
b. Use Exercise 9 to show that dimV(I) = 0.
12. Compute the dimension of the afﬁne varieties deﬁned by the following ideals. You may
assume that k is algebraically closed.
a. I = ⟨xz, xy −1⟩⊆k[x, y, z].
b. I = ⟨zw −y2, xy −z3⟩⊆k[x, y, z, w].
13. Consider the polynomial ring S = k[x0, . . . , xn].
a. Give an example to show that the set of polynomials of total degree s is not closed
under addition and, hence, does not form a vector space.
b. Show that the set of homogeneous polynomials of total degree s (together with the
zero polynomial) is a vector space over k.
c. Use Lemma 5 of §2 to show that this vector space has dimension
n+s
s

. Hint: Con-
sider the number of polynomials of total degree ≤s and ≤s −1.
d. Give a second proof of the dimension formula of part (c) using the isomorphism of
Exercise 20 below.
14. If I ⊆S = k[x0, . . . , xn] is a homogeneous ideal, show that the Hilbert polynomials
HPS/I and HPS/
√
I have the same degree. Hint: The quickest way is to use Theorem 12.
15. We will study when the Hilbert polynomial is zero.
a. If I ⊆S = k[x0, . . . , xn] is a homogeneous ideal, prove that ⟨x0, . . . , xn⟩r ⊆I for
some r ≥0 if and only if the Hilbert polynomial of I is the zero polynomial.
b. Conclude that if V ⊆Pn(k) is a variety, then V = ∅if and only if its Hilbert poly-
nomial is the zero polynomial. Thus, the empty variety in Pn(k) does not have a
dimension.
16. Compute the dimension of projective varieties deﬁned by the following ideals. Assume
that k is algebraically closed.
a. I = ⟨x2 −y2, x3 −x2y + y3⟩⊆k[x, y, z].
b. I = ⟨y2 −xz, x2y −z2w, x3 −yzw⟩⊆k[x, y, z, w].
17. In this exercise, we will see that in general, the relation between the number of variables
n, the number r of polynomials in a basis of I, and the dimension of V = V(I) is subtle.
Let V ⊆P3(k) be the curve parametrized by x = t3u2, y = t4u, z = t5, w = u5 for
(t : u) ∈P1(k). Since this is a curve in 3-dimensional space, our intuition would lead
us to believe that V should be deﬁned by two equations. Assume that k is algebraically
closed.
a. Use Theorem 11 of Chapter 8, §5 to ﬁnd an ideal I ⊆k[x, y, z, w] such that V = V(I)
in P3(k). If you use grevlex for a certain ordering of the variables, you will get a basis
of I containing three elements.
b. Show that I2 is 1-dimensional and I3 is 6-dimensional.
c. Show that I cannot be generated by two elements. Hint: Suppose that I = ⟨A, B⟩,
where A and B are homogeneous. By considering I2, show that A or B must be a
multiple of y2 −xz, and then derive a contradiction by looking at I3.
A much more difﬁcult question would be to prove that there are no two homogeneous
polynomials A, B such that V = V(A, B).

498
Chapter 9 The Dimension of a Variety
18. This exercise is concerned with the proof of part (i) of Theorem 12.
a. Use the methods of §2 to show that HFS/I(s) = aHFS/I(s)−aHFS/I(s−1) whenever
I is a monomial ideal in S = k[x0, . . . , xn].
b. Prove that HFI(s) = aHFI(s) −aHFI(s −1) for all homogeneous ideals I ⊆S.
19. If V ⊆Pn(k) is a nonempty projective variety and CV ⊆kn+1 is its afﬁne cone, then
prove that Ip(V) = Ia(CV) in k[x0, . . . , xn].
20. This exercise is concerned with the proof of part (ii) of Theorem 12.
a. Show that the maps φ and ψ deﬁned in (7) are linear maps and verify that they are
inverses of each other.
b. Prove (8) and conclude that φ : I≤s →Ih
s is an isomorphism whose inverse is ψ.
§4
Elementary Properties of Dimension
Using the deﬁnition of the dimension of a variety from §3, we can now state several
basic properties of dimension. As in §3, we assume that the ﬁeld k is inﬁnite.
Proposition 1. Let V1 and V2 be projective or afﬁne varieties. If V1 ⊆V2, then
dimV1 ≤dimV2.
Proof. We leave the proof to the reader as Exercise 1.
□
We next will study the relation between the dimension of a variety and the num-
ber of deﬁning equations. We begin with the case where V is deﬁned by a single
equation.
Proposition 2. Let k be an algebraically closed ﬁeld and let f ∈k[x0, . . . , xn] be a
nonconstant homogeneous polynomial. Then the dimension of the projective variety
in Pn(k) deﬁned by f is
dimV( f) = n −1.
Proof. Fix a monomial order > on k[x0, . . . , xn]. Since k is algebraically closed,
Theorem 11 of §3 says the dimension of V( f) is the maximum dimension of a
projective coordinate subspace contained in V(⟨LT(I)⟩), where I = ⟨f⟩. One can
check that ⟨LT(I)⟩= ⟨LT( f)⟩, and since LT( f) is a nonconstant monomial, the
projective variety V(LT( f)) is a union of subspaces of Pn(k) of dimension n −1. It
follows that dimV(I) = n −1.
□
Thus, when k is algebraically closed, a hypersurface V( f) in Pn always has dim-
ension n −1. We leave it as an exercise for the reader to prove the analogous state-
ment for afﬁne hypersurfaces.
It is important to note that these results are not valid if k is not algebraically
closed. For instance, let I = ⟨x2 + y2⟩in R[x, y]. In §3, we saw that V( f) =
{(0, 0)} ⊆R2 has dimension 0, yet Proposition 2 would predict that the dimen-
sion was 1. In fact, over a ﬁeld that is not algebraically closed, the variety in kn or
Pn deﬁned by a single polynomial can have any dimension between 0 and n −1.
The following theorem establishes the analogue of Proposition 2 when the ambi-
ent space Pn(k) is replaced by an arbitrary variety V. Note that if I is an ideal and f
is a polynomial, then V(I + ⟨f⟩) = V(I) ∩V( f).

§4 Elementary Properties of Dimension
499
Theorem 3. Let k be an algebraically closed and let I be a homogeneous ideal in
S = k[x0, . . . , xn]. If dimV(I) > 0 and f is any nonconstant homogeneous polyno-
mial, then
dimV(I) ≥dimV(I + ⟨f⟩) ≥dimV(I) −1.
Proof. To compute the dimension of V(I+⟨f⟩), we will need to compare the Hilbert
polynomials HPS/I and HPS/(I+⟨f⟩). We ﬁrst note that since I ⊆I + ⟨f⟩, Exercise 8
of §3 implies that
deg(HPS/I) ≥deg(HPS/(I+⟨f⟩)),
from which we conclude that dimV(I) ≥dimV(I + ⟨f⟩) by Theorem 11 of §3.
To obtain the other inequality, suppose that f ∈S = k[x0, . . . , xn] has total degree
r > 0. Fix a total degree s ≥r and consider the map
π : Ss/Is −→Ss/(I + ⟨f⟩)s
which sends [g] ∈Ss/Is to π([g]) = [g] ∈Ss/(I + ⟨f⟩)s. In Exercise 4, you will
check that π is a well-deﬁned linear map. It is easy to see that π is onto, and to
investigate its kernel, we will use the map
αf : Ss−r/Is−r −→Ss/Is
deﬁned by sending [h] ∈Ss−r/Is−r to αf ([h]) = [ f h] ∈Ss/Is. In Exercise 5, you
will show that αf is also a well-deﬁned linear map.
We claim that the kernel of π is exactly the image of αf , i.e., that
(1)
αf (Ss−r/Is−r) = {[g] | π([g]) = [0] in Ss/(I + ⟨f⟩)s}.
To prove this, note that if h ∈Ss−r, then f h ∈(I + ⟨f⟩)s and, hence, π([ f h]) = [0]
in Ss/(I + ⟨f⟩)s. Conversely, if g ∈Ss and π([g]) = [0], then g ∈(I + ⟨f⟩)s. This
means g = g′ + f h for some g′ ∈I. If we write g′ = 
i g′
i and h = 
i hi as sums
of homogeneous polynomials, where g′
i and hi have total degree i, it follows that
g = g′
s + f hs−r since g and f are homogeneous. Since I is a homogeneous ideal, we
have g′
s ∈Is, and it follows that [g] = [ f hs−r] = αf ([hs−r]) in Ss/Is. This shows that
[g] is in the image of αf and completes the proof of (1).
Since π is onto and we know its kernel by (1), the dimension theorem for linear
mappings shows that
dimSs/Is = dimαf (Ss−r/Is−r) + dimSs/(I + ⟨f⟩)s.
Now certainly,
(2)
dimαf (Ss−r/Is−r) ≤dimSs−r/Is−r,
with equality if and only if αf is one-to-one. Hence,
dimSs/(I + ⟨f⟩)s ≥dimSs/Is −dimSs−r/Is−r.

500
Chapter 9 The Dimension of a Variety
In terms of Hilbert functions, this tells us that
HFS/(I+⟨f⟩)(s) ≥HFS/I(s) −HFS/I(s −r)
whenever s ≥r. Thus, if s is sufﬁciently large, we obtain the inequality
(3)
HPS/(I+⟨f⟩)(s) ≥HPS/I(s) −HPS/I(s −r)
for the Hilbert polynomials.
Suppose that HPS/I has degree d. Then it is easy to see that the polynomial on
the right-hand side of (3) has degree d −1 (the argument is the same as used in
Exercise 15 of §2). Thus, (3) shows that HPS/(I+⟨f⟩) is ≥a polynomial of degree
d −1 for s sufﬁciently large, which implies deg(HPS/(I+⟨f⟩)) ≥d −1 [see, for
example, part (c) of Exercise 8 of §3]. Since k is algebraically closed, we conclude
that dimV(I + ⟨f⟩) ≥dimV(I) −1 by Theorem 11 of §3.
□
By carefully analyzing the proof of Theorem 3, we can give a condition that
ensures that dimV(I + ⟨f⟩) = dimV(I) −1. We need some new terminology for
this. A zero divisor in a commutative ring R is a nonzero element a such that a·b = 0
for some nonzero element b in R.
Corollary 4. Let k be an algebraically closed ﬁeld and let I ⊆S = k[x0, . . . , xn] be
a homogeneous ideal. Let f be a nonconstant homogeneous polynomial whose class
in the quotient ring S/I is not a zero divisor. Then
dimV(I + ⟨f⟩) = dimV(I) −1
when dimV(I) > 0. Furthermore, V(I + ⟨f⟩) = ∅when dimV(I) = 0.
Proof. As we observed in the proof of Theorem 3, the inequality (2) is an equality if
the multiplication map αf is one-to-one. We claim that the latter is true if [ f] ∈S/I
is not a zero divisor. Namely, suppose that [h] ∈Ss−r/Is−r is nonzero. This implies
that h /∈Is−r and, hence, h /∈I since Is−r = I ∩Ss−r. Thus, [h] ∈S/I is nonzero,
so that [ f][h] = [ f h] is nonzero in S/I by our assumption on f. Hence, f h /∈I and,
hence, αf ([h]) = [ f h] is nonzero in Ss/Is. This shows that αf is one-to-one.
Since (2) is an equality, the proof of Theorem 3 shows that we also get the
equality
dimSs/(I + ⟨f⟩)s = dimSs/Is −dimSs−r/Is−r
when s ≥r. In terms of Hilbert polynomials, this says HPS/(I+⟨f⟩)(s) = HPS/I(s)−
HPS/I(s −r), and it follows immediately that dimV(I + ⟨f⟩) = dimV(I) −1.
□
We remark that Theorem 3 can fail for afﬁne varieties, even when k is alge-
braically closed. For example, consider the ideal I = ⟨xz, yz⟩⊆C[x, y, z]. One
easily sees that in C3, we have V(I) = V(z) ∪V(x, y), so that V(I) is the union
of the (x, y)-plane and the z-axis. In particular, V(I) has dimension 2 (do you see
why?). Now, let f = z −1 ∈C[x, y, z]. Then V( f) is the plane z = 1, and it fol-
lows that V(I + ⟨f⟩) = V(I) ∩V( f) consists of the single point (0, 0, 1) (you will

§4 Elementary Properties of Dimension
501
check this carefully in Exercise 7). By Exercise 9 of §3, we know that a point has
dimension 0. Yet Theorem 3 would predict that V(I +⟨f⟩) had dimension at least 1.
What goes "wrong" here is that the planes z = 0 and z = 1 are parallel and,
hence, do not meet in afﬁne space. We are missing a component of dimension 1 at
inﬁnity. This is an example of the way dimension theory works more satisfactorily
for homogeneous ideals and projective varieties. It is possible to formulate a version
of Theorem 3 that is valid for afﬁne varieties, but we will not pursue that question
here.
Our next result extends Theorem 3 to the case of several polynomials f1, . . . , fr.
Proposition 5. Let k be an algebraically closed ﬁeld and let I be a homogeneous
ideal in k[x0, . . . , xn]. Let f1, . . . , fr be nonconstant homogeneous polynomials in
k[x0, . . . , xn] such that r ≤dimV(I). Then
dimV(I + ⟨f1, . . . , fr⟩) ≥dimV(I) −r.
Proof. The result follows immediately from Theorem 3 by induction on r.
□
In the exercises, we will ask you to derive a condition on the polynomials
f1, . . . , fr which guarantees that the dimension of V( f1, . . . , fr) is exactly equal to
n −r.
Our next result concerns varieties of dimension zero.
Proposition 6. Let V be a nonempty afﬁne or projective variety. Then V consists of
ﬁnitely many points if and only if dimV = 0.
Proof. We will give the proof only in the afﬁne case. Let > be a graded order on
k[x1, . . . , xn]. If V is ﬁnite, then let aj, j = 1, . . . , mi, be the distinct elements of k
appearing as i-th coordinates of points of V. Then
f =
mi
!
j=1
(xi −aj) ∈I(V)
and we conclude that LT( f) = xmi
i
∈⟨LT(I(V))⟩. This implies that V(⟨LT(I(V))⟩) =
{0} and then Theorem 8 of §3 implies that dimV = 0.
Now suppose that dimV = 0. Then the afﬁne Hilbert polynomial of I(V) is a
constant C, so that
dimk[x1, . . . , xn]≤s/I(V)≤s = C
for s sufﬁciently large. If we also have s ≥C, then the classes [1], [xi], [x2
i ], . . . , [xs
i ] ∈
k[x1, . . . , xn]≤s/I(V)≤s are s + 1 vectors in a vector space of dimension C ≤s and,
hence, they must be linearly dependent. But a nontrivial linear relation
[0] =
s

j=0
aj
/
xj
i
0
=
%
s

j=0
ajxj
i
&

502
Chapter 9 The Dimension of a Variety
means that s
j=0 ajxj
i is a nonzero polynomial in I(V)≤s. This polynomial vanishes
on V, which implies that there are only ﬁnitely many distinct i-th coordinates among
the points of V. Since this is true for all 1 ≤i ≤n, we see that V must be ﬁnite. □
If, in addition, k is algebraically closed, then the conditions of Theorem 6 of
Chapter 5, §3 are equivalent to dimV = 0. In particular, given any deﬁning ideal I
of V, we get a simple criterion for detecting when a variety has dimension 0.
Now that we understand varieties of dimension 0, let us record some interesting
properties of positive dimensional varieties.
Proposition 7. Let k be algebraically closed.
(i) Let V ⊆Pn(k) be a projective variety of dimension > 0. Then V ∩V( f) ̸= ∅for
every nonconstant homogeneous polynomial f ∈k[x0, . . . , xn]. Thus, a positive
dimensional projective variety meets every hypersurface in Pn(k).
(ii) Let W ⊆kn be an afﬁne variety of dimension > 0. If W is the projective closure
of W in Pn(k), then W ̸= W. Thus, a positive dimensional afﬁne variety always
has points at inﬁnity.
Proof. (i) Let V = V(I). Since dimV > 0, Theorem 3 shows that dimV ∩V( f) ≥
dimV −1 ≥0. Let us check carefully that this guarantees V ∩V( f) ̸= ∅.
If V ∩V( f) = ∅, then the projective Nullstellensatz implies that ⟨x0, . . . , xn⟩r ⊆
I + ⟨f⟩for some r ≥0. By Exercise 15 of §3, it follows that HPS/(I+⟨f⟩) is the zero
polynomial, where S = k[x0, . . . , xn]. Yet if you examine the proof of Theorem 3,
the inequality given for HPS/(I+⟨f⟩) shows that this polynomial cannot be zero when
dimV > 0. We leave the details as an exercise.
(ii) The points at inﬁnity of W are W ∩V(x0), where V(x0) is the hyperplane at
inﬁnity. By Theorem 12 of §3, we have dimW = dimW > 0, and then (i) implies
that W ∩V(x0) ̸= ∅.
□
We next study the dimension of the union of two varieties.
Proposition 8. If V and W are nonempty varieties either both in kn or both in Pn(k),
then
dim(V ∪W) = max(dimV, dimW).
Proof. The proofs for the afﬁne and projective cases are nearly identical, so we will
give only the afﬁne proof. Let R = k[x1, . . . , xn].
Let I = I(V) and J = I(W), so that dimV = deg(aHPR/I) and dimW =
deg(aHPR/J). It is easy to show that I(V ∪W) = I(V) ∩I(W) = I ∩J. It is more
convenient to work with the product ideal IJ, and we note that
IJ ⊆I ∩J ⊂
√
IJ
(see Exercise 15). By Exercise 8 of §3, we conclude that
deg(aHPR/IJ) ≥deg(aHPR/I∩J) ≥deg(aHPR/
√
IJ).
by Proposition 6 of §3, the outer terms are equal. We conclude that dim(V ∪W) =
deg(aHPIJ).

§4 Elementary Properties of Dimension
503
Now ﬁx a graded order > on R = k[x1, . . . , xn]. By Propositions 3 and 4 of §3,
it follows that dimV, dimW, and dim(V ∪W) are given by the maximal dimen-
sion of a coordinate subspace contained in V(⟨LT(I)⟩), V(⟨LT(J)⟩) and V(⟨LT(IJ)⟩)
respectively. In Exercise 16, you will prove that
⟨LT(IJ)⟩⊇⟨LT(I)⟩· ⟨LT(J)⟩.
This implies
V(⟨LT(IJ)⟩) ⊆V(⟨LT(I)⟩) ∪V(⟨LT(J)⟩).
Since k is inﬁnite, every coordinate subspace is irreducible (see Exercise 7 of
§1), and as a result, a coordinate subspace contained in V(⟨LT(IJ)⟩) lies in either
V(⟨LT(I)⟩) or V(⟨LT(J)⟩). This implies dim(V ∪W) ≤max(dimV, dimW). The
opposite inequality follows from Proposition 1, and the proposition is proved.
□
This proposition has the following useful corollary.
Corollary 9. The dimension of a variety is the largest of the dimensions of its irre-
ducible components.
Proof. If V = V1 ∪· · · ∪Vr is the decomposition of V into irreducible components,
then Proposition 8 and an induction on r shows that
dimV = max(dimV1, . . . , dimVr),
as claimed.
□
This corollary allows us to reduce to the case of an irreducible variety when
computing dimensions. The following result shows that for irreducible varieties, the
notion of dimension is especially well-behaved.
Proposition 10. Let k be an algebraically closed ﬁeld and let V ⊆Pn(k) be an
irreducible variety.
(i) If f ∈k[x0, . . . , xn] is a homogeneous polynomial which does not vanish identi-
cally on V, then dim(V ∩V( f)) = dimV −1 when dimV > 0. Furthermore,
V ∩V( f) = ∅when dimV = 0.
(ii) If W ⊆V is a variety such that W ̸= V, then dimW < dimV.
Proof. (i) By Proposition 4 of Chapter 5, §1, we know that I(V) is a prime ideal
and k[V] ∼= k[x0, . . . , xn]/I(V) is an integral domain. Since f /∈I(V), the class
of f is nonzero in k[x0, . . . , xn]/I(V) and, hence, is not a zero divisor. The desired
conclusion then follows from Corollary 4.
(ii) If W is a proper subvariety of V, then we can ﬁnd f ∈I(W) \ I(V). Thus,
W ⊆V ∩V( f), and it follows from (i) and Proposition 1 that
dimW ≤dim(V ∩V( f)) = dimV −1 < dimV.
This completes the proof of the proposition.
□

504
Chapter 9 The Dimension of a Variety
Part (i) of Proposition 10 asserts that when V is irreducible and f does not vanish
on V, then some component of V ∩V( f) has dimension dimV −1. With some more
work, it can be shown that every component of V ∩V( f) has dimension dimV −1.
See, for example, Theorem 3.8 in Chapter IV of KENDIG (2015) or Theorem 5 of
Chapter 1, §6 of SHAFAREVICH (2013).
In the next section, we will see that there is a way to understand the meaning of
the dimension of an irreducible variety V in terms of the coordinate ring k[V] and
the ﬁeld of rational functions k(V) of V that we introduced in Chapter 5.
EXERCISES FOR §4
1. Prove Proposition 1. Hint: Use Exercise 8 of the previous section.
2. Let k be an algebraically closed ﬁeld. If f ∈k[x1, . . . , xn] is a nonconstant polynomial,
show that the afﬁne hypersurface V( f ) ⊆kn has dimension n −1.
3. In R4, give examples of four different afﬁne varieties, each deﬁned by a single equation,
that have dimensions 0, 1, 2, 3, respectively.
4. Let S = k[x0, . . . , xn]. In this exercise, we study the mapping
π : Ss/Is −→Ss/(I + ⟨f ⟩)s
deﬁned by π([g]) = [g] for all g ∈Ss.
a. Show that π is well-deﬁned. This means showing that the image of the class [g] does
not depend on which representative g in the class that we choose. We call π the
natural projection from Ss/Is to Ss/(I + ⟨f ⟩)s.
b. Show that π is a linear mapping of vector spaces.
c. Show that the natural projection π is onto.
5. Show that if f is a homogeneous polynomial of degree r and I ⊆S = k[x0, . . . , xn] is a
homogeneous ideal, then the map
αf : Ss−r/Is−r −→Ss/Is
deﬁned by αf([h]) = [ f · h] is a well-deﬁned linear mapping. Hence, you need to show
that αf([h]) does not depend on the representative h for the class and that α preserves
the vector space operations.
6. Let f ∈S = k[x0, . . . , xn] be a homogeneous polynomial of total degree r > 0.
a. Find a formula for the Hilbert polynomial of ⟨f ⟩. Your formula should depend only
on n and r (and, of course, s). In particular, all such polynomials f have the same
Hilbert polynomial. Hint: Examine the proofs of Theorem 3 and Corollary 4 in the
case when I = {0}.
b. More generally, suppose that V = V(I) and that the class of f is not a zero divisor in
S/I. Then show that the Hilbert polynomial of I + ⟨f ⟩depends only on I and r.
If we vary f , we can regard the varieties V( f ) ⊆Pn(k) as an algebraic family of hyper-
surfaces. Similarly, varying f gives the family of varieties V ∩V( f ). By parts (a) and (b),
the Hilbert polynomials are constant as we vary f . In general, if a technical condition
called "ﬂatness" is satisﬁed, the Hilbert polynomials are constant on an algebraic family
of varieties.
7. Let I = ⟨xz, yz⟩. Show that V(I + ⟨z −1⟩) = {(0, 0, 1)}.
8. Let S = k[x0, . . . , xn]. A sequence f1, . . . , fr of r ≤n+1 nonconstant homogeneous poly-
nomials is called an S-sequence if the class [ fj+1] is not a zero divisor in S/⟨f1, . . . , fj⟩
for each 1 ≤j < r.
a. Show for example that for r ≤n, x0, . . . , xr is an S-sequence.

§4 Elementary Properties of Dimension
505
b. Show that if k is algebraically closed and f1, . . . , fr is an S-sequence, then
dimV( f1, . . . , fr) = n −r.
Hint: Use Corollary 4 and induction on r. Work with the ideals Ij = ⟨f1, . . . , fj⟩for
1 ≤j ≤r.
9. Let S = k[x0, . . . , xn]. A homogeneous ideal I is said to be a complete intersection
if it can be generated by an S-sequence. A projective variety V is called a complete
intersection if I(V) is a complete intersection.
a. Show that every irreducible linear subspace of Pn(k) is a complete intersection.
b. Show that hypersurfaces are complete intersections when k is algebraically closed.
c. Show that projective closure of the union of the (x, y)- and (z, w)-planes in k4 is not
a complete intersection.
d. Let V be the afﬁne twisted cubic V(y −x2, z −x3) in k3. Is the projective closure of
V a complete intersection?
Hint for parts (c) and (d): Use the technique of Exercise 17 of §3.
10. Suppose that I ⊆R = k[x1, . . . , xn] is an ideal. In this exercise, we will prove that the
afﬁne Hilbert polynomial of I is a constant if and only if the quotient ring R/I is ﬁnite-
dimensional as a vector space over k. Furthermore, when this happens, we will show that
the constant is the dimension of R/I as a vector space over k.
a. Let αs : R≤s/I≤s →R/I be the map deﬁned by αs([ f ]) = [ f ]. Show that αs is
well-deﬁned and one-to-one.
b. If R/I is ﬁnite-dimensional, show that αs is an isomorphism for s sufﬁciently large
and conclude that the afﬁne Hilbert polynomial is a constant (and equals the dimen-
sion of R/I). Hint: Pick a basis [ f1], . . . , [ fm] of R/I and let s be bigger than the total
degrees of f1, . . . , fm.
c. Now suppose the afﬁne Hilbert polynomial is a constant. Show that if s ≤t, the
image of αt contains the image of αs. If s and t are large enough, conclude that the
images are equal. Use this to show that αs is an isomorphism for s sufﬁciently large
and conclude that R/I is ﬁnite-dimensional.
11. Let V ⊆kn be ﬁnite. In this exercise, we will prove that k[x1, . . . , xn]/I(V) is ﬁnite-
dimensional and that its dimension is |V|, the number of points in V. If we combine
this with the previous exercise, we see that the afﬁne Hilbert polynomial of I(V) is the
constant |V|. Suppose that V = {p1, . . . , pm}, where m = |V|.
a. Deﬁne a map φ : k[x1, . . . , xn]/I(V) →km by φ([f ]) = (f (p1), . . . , f (pm)). Show
that φ is a well-deﬁned linear map and show that it is one-to-one.
b. Fix i and let Wi = {pj | j ̸= i}. Show that 1 ∈I(Wi) + I({pi}). Hint: Show that
I({pi}) is a maximal ideal.
c. By part (b), we can ﬁnd fi ∈I(Wi) and gi ∈I({pi}) such that fi + gi = 1. Show that
φ( fi) is the vector in km which has a 1 in the i-th coordinate and 0's elsewhere.
d. Conclude that φ is an isomorphism and that dim k[x1, . . . , xn]/I(V) = |V|.
12. Let I ⊆S = k[x0, . . . , xn] be a homogeneous ideal. In this exercise we will study the
geometric signiﬁcance of the coefﬁcient b0 of the Hilbert polynomial
HPS/I(s) =
d

i=0
bi

s
d −i

.
We will call b0 the degree of I. The degree of a projective variety V is deﬁned to be
the degree of I(V) and, as we will see, the degree is in a sense a generalization of the
total degree of the deﬁning equation for a hypersurface. Note also that we can regard
Exercises 10 and 11 as studying the degrees of ideals and varieties with constant afﬁne
Hilbert polynomial.

506
Chapter 9 The Dimension of a Variety
a. Show that the degree of the ideal ⟨f ⟩is the same as the total degree of f . Also, if k is
algebraically closed, show that the degree of the hypersurface V( f ) is the same as the
total degree of fred, the reduction of f deﬁned in Chapter 4, §2. Hint: Use Exercise 6.
b. Show that if I ⊆S = k[x0, . . . , xn] is a complete intersection (Exercise 9) generated
by the elements of an S-sequence f1, . . . , fr, then the degree of I is the product
deg( f1) · deg( f2) · · · deg( fr)
of the total degrees of the fi. Hint: Look carefully at the proof of Theorem 3. The hint
for Exercise 8 may be useful.
c. Determine the degree of the projective closure of the standard twisted cubic.
13. Verify carefully the claim made in the proof of Proposition 7 that HPS/(I+⟨f⟩) cannot be
the zero polynomial when dimV > 0. Hint: Look at the inequality (3) from the proof of
Theorem 3.
14. This exercise will explore what happens if we weaken the hypotheses of Proposition 7.
a. Let V = V(x) ⊆k2. Show that V ∩V(x −1) = ∅and explain why this does not
contradict part (a) of the proposition.
b. Let W = V(x2 + y2 −1) ⊆R2. Show that W = W in P2(R) and explain why this
does not contradict part (b) of the proposition.
15. If I, J ⊆k[x1, . . . , xn] are ideals, prove that IJ ⊆I ∩J ⊆
√
IJ.
16. Show that if I and J are any ideals and > is any monomial ordering, then
⟨LT(I)⟩· ⟨LT(J)⟩⊆⟨LT(IJ)⟩.
17. Using Proposition 10, we can get an alternative deﬁnition of the dimension of an irre-
ducible variety. We will assume that the ﬁeld k is algebraically closed and that V ⊆Pn(k)
is irreducible.
a. If dimV > 0, prove that there is an irreducible variety W ⊆V such that dimW =
dimV −1. Hint: Use Proposition 10 and look at the irreducible components of V ∩
V( f ).
b. If dimV = m, prove that one can ﬁnd a chain of m + 1 irreducible varieties
V0 ⊆V1 ⊆· · · ⊆Vm = V
such that Vi ̸= Vi+1 for 0 ≤i ≤m −1.
c. Show that it is impossible to ﬁnd a similar chain of length greater than m + 1 and
conclude that the dimension of an irreducible variety is one less than the length of the
longest strictly increasing chain of irreducible varieties contained in V.
18. Prove an afﬁne version of part (ii) of Proposition 10.
§5
Dimension and Algebraic Independence
In §3, we deﬁned the dimension of an afﬁne variety as the degree of the afﬁne
Hilbert polynomial. This was useful for proving the properties of dimension in §4,
but Hilbert polynomials do not give the full story. In algebraic geometry, there are
many ways to formulate the concept of dimension and we will explore two of the
more interesting approaches in this section and the next.

§5 Dimension and Algebraic Independence
507
Algebraic Independence
If V ⊆kn is an afﬁne variety, recall from Chapter 5 that the coordinate ring k[V]
consists of all polynomial functions on V. This is related to the ideal I(V) ⊆R =
k[x1, . . . , xn] by the natural ring isomorphism k[V] ∼= R/I(V) (which is the identity
on k) discussed in Theorem 7 of Chapter 5, §2. To see what k[V] has to do with
dimension, note that for any s ≥0, there is a well-deﬁned linear map
(1)
R≤s/I(V)≤s −→R/I(V) ∼= k[V]
which is one-to-one (see Exercise 10 of §4). Thus, we can regard R≤s/I(V)≤s as a
ﬁnite-dimensional "piece" of k[V] that approximates k[V] more and more closely
as s gets larger. Since the degree of aHPR/I(V) measures how fast these ﬁnite-
dimensional approximations are growing, we see that dimV tells us something about
the "size" of k[V].
This discussion suggests that we should be able to formulate the dimension of V
directly in terms of the ring k[V]. To do this, we will use the notion of algebraically
independent elements.
Deﬁnition 1. We say that elements φ1, . . . , φr ∈k[V] are algebraically indepen-
dent over k if there is no nonzero polynomial F of r variables with coefﬁcients in k
such that F(φ1, . . . , φr) = 0 in k[V].
Note that if φ1, . . . , φr ∈k[V] are algebraically independent over k, then the φi's
are distinct and nonzero. It is also easy to see that any subset of {φ1, . . . , φr} is also
algebraically independent over k (see Exercise 1 for the details).
The simplest example of algebraically independent elements occurs when V =
kn. If k is an inﬁnite ﬁeld, we have I(V) = {0} and, hence, the coordinate ring is
k[V] = R = k[x1, . . . , xn]. Here, the elements x1, . . . , xn are algebraically indepen-
dent over k since F(x1, . . . , xn) = 0 means that F is the zero polynomial.
For another example, let V be the twisted cubic in R3 with I(V) = ⟨y−x2, z−x3⟩.
Let us show that [x] ∈R[V] is algebraically independent over R. Suppose F is a
polynomial with coefﬁcients in R such that F([x]) = [0] in R[V]. By the way we
deﬁned the ring operations in R[V], this means [F(x)] = [0], so that F(x) ∈I(V). It
is easy to show that R[x] ∩⟨y −x2, z −x3⟩= {0}, which proves that F is the zero
polynomial. On the other hand, we leave it to the reader to verify that [x], [y] ∈R[V]
are not algebraically independent over R since [y] −[x]2 = [0] in R[V].
We can relate the dimension of V to the number of algebraically independent
elements in the coordinate ring k[V] as follows.
Theorem 2. Let V ⊆kn be an afﬁne variety. Then the dimension of V equals the
maximal number of elements of k[V] which are algebraically independent over k.
Proof. We will ﬁrst show that if d = dimV, then we can ﬁnd d elements of k[V]
which are algebraically independent over k. To do this, let I = I(V) and consider the
ideal of leading terms ⟨LT(I)⟩for a graded order on R = k[x1, . . . , xn]. By Theorem 8

508
Chapter 9 The Dimension of a Variety
of §3, we know that d is the maximum dimension of a coordinate subspace contained
in V(⟨LT(I)⟩). A coordinate subspace W ⊆V(⟨LT(I)⟩) of dimension d is deﬁned by
the vanishing of n−d coordinates, so that we can write W = V(xj | j /∈{i1, . . . , id})
for some 1 ≤i1 < · · · < id ≤n. We will show that [xi1], . . . , [xid] ∈k[V] are
algebraically independent over k.
The ﬁrst step is to prove
(2)
I ∩k[xi1, . . . , xid] = {0}.
Let p ∈kn be the point whose ij-th coordinate is 1 for 1 ≤j ≤d and whose
other coordinates are 0, and note that p ∈W ⊆V(⟨LT(I)⟩). Then every mono-
mial in ⟨LT(I)⟩vanishes at p and, hence, no monomial in ⟨LT(I)⟩can involve only
xi1, . . . , xid (this is closely related to the proof of Proposition 2 of §2). Since ⟨LT(I)⟩
is a monomial ideal, this implies that ⟨LT(I)⟩∩k[xi1, . . . , xid] = {0}. Then (2) fol-
lows since a nonzero element f ∈I ∩k[xi1, . . . , xid] would give the nonzero element
LT( f) ∈⟨LT(I)⟩∩k[xi1, . . . , xid].
From (2), one easily sees that the map
k[xi1, . . . , xid] −→R/I
sending f ∈k[xi1, . . . , xid] to [ f] ∈R/I is a one-to-one ring homomorphism that is
the identity on k (see Exercise 3 of Chapter 5, §6). The xij ∈k[xi1, . . . , xid] are alge-
braically independent, so the same is true for their images [xij] ∈R/I since the map
is one-to-one. This shows that R/I contains d = dimV algebraically independent
elements.
The ﬁnal step in the proof is to show that if r elements of k[V] are algebraically
independent over k, then r ≤dimV. So assume that [ f1], . . . , [ fr] ∈k[V] are alge-
braically independent. Let N be the largest of the total degrees of f1, . . . , fr and let
y1, . . . , yr be new variables. If F ∈k[y1, . . . , yr] is a polynomial of total degree ≤s,
then the polynomial F( f1, . . . , fr) ∈R = k[x1, . . . , xn] has total degree ≤Ns (see
Exercise 2). Then consider the map
(3)
α : k[y1, . . . , yr]≤s −→R≤Ns/I≤Ns
which sends F(y1, . . . , yr) ∈k[y1, . . . , yr]≤s to [F( f1, . . . , fr)] ∈R≤Ns/I≤Ns. We
leave it as an exercise to show that α is a well-deﬁned linear map.
We claim that α is one-to-one. To see why, suppose that F ∈k[y1, . . . , yr]≤s and
[F( f1, . . . , fr)] = [0] in R≤N s/I≤N s. Using the map (1), it follows that
[F( f1, . . . , fr)] = F([ f1], . . . , [ fr]) = [0]
in R/I ∼= k[V].
Since [ f1], . . . , [ fr] are algebraically independent and F has coefﬁcients in k, it fol-
lows that F must be the zero polynomial. Hence, α is one-to-one.
Comparing dimensions in (3), we see that
(4)
aHFR/I(Ns) = dim R≤Ns/I≤Ns ≥dim k[y1, . . . , yr]≤s.

§5 Dimension and Algebraic Independence
509
Since the yi are variables, Lemma 4 of §2 shows that the dimension of k[y1, . . . , yr]≤s
is
r+s
s
	
=
r+s
r
	
, which is a polynomial of degree r in s. In terms of the afﬁne Hilbert
polynomial, this implies
aHPR/I(Ns) ≥
r + s
r

= a polynomial of degree r in s
for s sufﬁciently large. It follows that aHPR/I(Ns) and, hence, aHPR/I(s) must have
degree at least r. Thus, r ≤dimV, which completes the proof of the theorem.
□
As an application, we show that isomorphic varieties have the same dimension.
Corollary 3. Let V and V′ be afﬁne varieties which are isomorphic (as deﬁned in
Chapter 5, §4). Then dimV = dimV′.
Proof. By Theorem 9 of Chapter 5, §4, we know V and V′ are isomorphic if and
only if there is a ring isomorphism α : k[V] →k[V′] which is the identity on k.
Then elements φ1, . . . , φr ∈k[V] are algebraically independent over k if and only
if α(φ1), . . . , α(φr) ∈k[V′] are. We leave the easy proof of this assertion as an
exercise. From here, the corollary follows immediately from Theorem 2.
□
In the proof of Theorem 2, note that the d = dimV algebraically independent ele-
ments we found in k[V] came from the coordinates. We can use this to give another
formulation of dimension.
Corollary 4. Let V ⊆kn be an afﬁne variety. Then the dimension of V is equal to
the largest integer r for which there exist r variables xi1, . . . , xir such that I(V) ∩
k[xi1, . . . , xir] = {0} [i.e., such that I(V) contain no nonzero polynomials in only
these variables].
Proof. From (2), it follows that we can ﬁnd d = dimV such variables. Suppose we
could ﬁnd d+1 variables, xj1, . . . , xjd+1 such that I∩k[xj1, . . . , xjd+1] = {0}. Then the
argument following (2) would imply that [xj1], . . . , [xjd+1] ∈k[V] were algebraically
independent over k. This is impossible by Theorem 2 since d = dimV.
□
In the exercises, you will show that if k is algebraically closed, then Corollary 4
remains true if we replace I(V) with any deﬁning ideal I of V. Since we know how
to compute I ∩k[xi1, . . . , xir] by elimination theory, Corollary 4 then gives us an
alternative method (though not an efﬁcient one) for computing the dimension of a
variety.
Projections and Noether Normalization
We can also interpret Corollary 4 in terms of projections. If we choose r variables
xi1, . . . , xir, then we get the projection map π : kn →kr deﬁned by π(a1, . . . , an) =
(ai1, . . . , air). Also, let ˜I = I(V)∩k[xi1, . . . , xir] be the appropriate elimination ideal.
If k is algebraically closed, then the Closure Theorem from §2 of Chapter 3 shows
that V(˜I) ∩kr is the smallest variety containing the projection π(V). It follows that

510
Chapter 9 The Dimension of a Variety
˜I = {0} ⇐⇒V(˜I) = kr
⇐⇒the smallest variety containing π(V) is kr
⇐⇒π(V) is Zariski dense in kr.
Thus, Corollary 4 shows that the dimension of V is the largest dimension of a
coordinate subspace for which the projection of V is Zariski dense in the subspace.
We can regard the above map π as a linear map from kn to itself which leaves the
ij-th coordinate unchanged for 1 ≤j ≤r and sends the other coordinates to 0. It is
then easy to show that π ◦π = π and that the image of π is kr ⊆kn (see Exercise 8).
More generally, a linear map π : kn →kn is called a projection if π ◦π = π. If π
has rank r, then the image of π is an r-dimensional subspace H of kn, and we say
that π is a projection onto H.
Now let π be a projection onto a subspace H ⊆kn. Under π, any variety V ⊆kn
gives a subset π(V) ⊆H. Then we can interpret the dimension of V in terms of its
projections π(V) as follows.
Proposition 5. Let k be an algebraically closed ﬁeld and let V ⊆kn be an afﬁne
variety. Then the dimension of V is the largest dimension of a subspace H ⊆kn for
which a projection of V onto H is Zariski dense.
Proof. If V has dimension d, then by the above remarks, we can ﬁnd a projection
of V onto a d-dimensional coordinate subspace which has Zariski dense image.
Now let π : kn →kn be an arbitrary projection onto an r-dimensional subspace
H of kn. We need to show that r ≤dimV whenever π(V) is Zariski dense in H.
From linear algebra, we can ﬁnd a basis of kn so that in the new coordinate system,
π(a1, . . . , an) = (a1, . . . , ar). Since changing coordinates does not affect the dim-
ension (this follows from Corollary 3 since a coordinate change gives isomorphic
varieties), we are reduced to the case of a projection onto a coordinate subspace, and
then the proposition follows from the above remarks.
□
Let π be a projection of kn onto a subspace H of dimension r. By the Closure
Theorem from Chapter 3, §2 we know that if π(V) is Zariski dense in H, then we
can ﬁnd a proper variety W ⊆H such that H \ W ⊂π(V). Thus, π(V) "ﬁlls up"
most of the r-dimensional subspace H, and hence, it makes sense that this should
force V to have dimension at least r. So Proposition 5 gives a very geometric way
of thinking about the dimension of a variety.
The idea of projection is also closely related to the Noether normalization of
k[V] ∼= k[x1, . . . , xn]/I(V) studied in Chapter 5, §6. The proof of Theorem 8 of that
section implies that we can ﬁnd linear combinations u1, . . . , um of the coordinate
functions φi = [xi] ∈k[V] such that the inclusion
k[u1, . . . , um] ⊆k[V]
is a Noether normalization, meaning that u1, . . . , um are algebraically independent
over k and k[V] is ﬁnite over k[u1, . . . , um] as in Deﬁnition 2 of Chapter 5, §6. The
above inclusion corresponds to a projection map π : V →km and relates to Propo-
sition 5 as follows.

§5 Dimension and Algebraic Independence
511
Theorem 6. In the above situation, m is the dimension of V. Also, the projection
π : V →km is onto with ﬁnite ﬁbers.
Proof. The second assertion follows from Theorem 8 of Chapter 5, §6. Turning
to dimV, note that m ≤dimV by Theorem 2 since u1, . . . , um are algebraically
independent over k.
It remains to show m ≥dimV. By a suitable change of coordinates in kn, we
can assume that ui = [xi] for i = 1, . . . , m. Let I = I(V) and consider lex order on
k[x1, . . . , xn] with xm+1 > · · · > xn > x1 > · · · > xm.
Arguing as in the proof of Theorem 2, we see that k[x1, . . . , xm] ∩I = {0},
which allows us to regard k[x1, . . . , xm] as a subring of k[x1, . . . , xn]/I ∼= k[V]. Since
k[V] is ﬁnite over k[x1, . . . , xm], the Relative Finiteness Theorem (Theorem 4 from
Chapter 5, §6) implies that
(5)
xam+1
m+1 , . . . , xan
n ∈⟨LT(I)⟩.
for some am+1, . . . , an ≥0.
By the Dimension Theorem (Theorem 8 of §3), dimV is the maximum dimension
of a coordinate subspace contained in V(⟨LT(I)⟩), which by Proposition 2 of §2
equals the maximum dimension of a coordinate subspace [ei1, . . . , eir] contained in
the complement C(⟨LT(I)⟩) ⊆Zn
≥0. However,
[ei1, . . . , eir] ⊆C(⟨LT(I)⟩) =⇒xa
ij /∈⟨LT(I)⟩for all a ≥0.
Comparing this with (5), we must have {i1, . . . , ir} ∩{m + 1, . . . , n} = ∅. In other
words, {i1, . . . , ir} ⊆{1, . . . , m}, so that r ≤m. This implies dimV ≤m, and the
theorem follows.
□
Notice how Theorem 6 gives a sharper version of Proposition 5, since the projec-
tion is now onto and the ﬁbers are ﬁnite. The theorem also justiﬁes the claim made
in Chapter 5, §6 that a Noether normalization determines the dimension.
Irreducible Varieties and Transcendence Degrees
For the ﬁnal part of the section, we will assume that V is an irreducible variety. By
Proposition 4 of Chapter 5, §1, we know that I(V) is a prime ideal and that k[V] is
an integral domain. As in §5 of Chapter 5, we can then form the ﬁeld of fractions of
k[V], which is the ﬁeld of rational functions on V and is denoted k(V). For elements
of k(V), the deﬁnition of algebraic independence over k is the same as that given
for elements of k[V] in Deﬁnition 1. We can relate the dimension of V to k(V) as
follows.
Theorem 7. Let V ⊆kn be an irreducible afﬁne variety. Then the dimension of V
equals the maximal number of elements of k(V) which are algebraically independent
over k.

512
Chapter 9 The Dimension of a Variety
Proof. Let d = dimV. Since k[V] ⊆k(V), any d elements of k[V] which are alge-
braically independent over k will have the same property when regarded as elements
of k(V). So it remains to show that if φ1, . . . , φr ∈k(V) are algebraically indepen-
dent, then r ≤dimV. Each φi is a quotient of elements of k[V], and if we pick
a common denominator f, then we can write φi = [ fi]/[ f] for 1 ≤i ≤r. Note
also that [ f] ̸= [0] in k[V]. We need to modify the proof of Theorem 2 to take the
denominator f into account. As usual, we set R = k[x1, . . . , xn].
Let N be the largest of the total degrees of f, f1, . . . , fr, and let y1, . . . , yr be new
variables. If F ∈k[y1, . . . , yr] is a polynomial of total degree ≤s, then we leave it
as an exercise to show that
f sF( f1/f, . . . , fr/f)
is a polynomial in R of total degree ≤Ns (see Exercise 10). Then consider the map
(6)
β : k[y1, . . . , yr]≤s −→R≤Ns/I≤Ns
sending a polynomial F(y1, . . . , yr) ∈k[y1, . . . , yr]≤s to [ f sF( f1/f, . . . , fr/f)] ∈
R≤Ns/I≤Ns. We leave it as an exercise to show that β is a well-deﬁned linear map.
To show that β is one-to-one, suppose that we have F ∈k[y1, . . . , yr]≤s such that
[ f sF( f1/f, . . . , fr/f)] = [0] in R≤Ns/I≤Ns. Using the map (1), it follows that
[ f sF( f1/f, . . . , fr/f)] = [0]
in R/I ∼= k[V].
However, if we work in k(V) and use φi = [ fi]/[ f], then we can write this as
[ f]sF([ f1]/[ f], . . . , [ fr]/[ f]) = [ f]sF(φ1, . . . , φr) = [0] in k(V).
Since k(V) is a ﬁeld and [f] ̸= [0], it follows that F(φ1, . . . , φr) = [0]. Then F must
be the zero polynomial since φ1, . . . , φr are algebraically independent and F has
coefﬁcients in k. Thus, β is one-to-one.
Once we know that β is one-to-one in (6), we get the inequality (4), and from
here, the proof of Theorem 2 shows that dimV ≥r. This proves the theorem.
□
As a corollary of this theorem, we can prove that birationally equivalent varieties
have the same dimension.
Corollary 8. Let V and V′ be irreducible afﬁne varieties which are birationally
equivalent (as deﬁned in Chapter 5, §5). Then dimV = dimV′.
Proof. In Theorem 10 of Chapter 5, §5, we showed that two irreducible afﬁne var-
ieties V and V′ are birationally equivalent if and only if there is an isomorphism
k(V) ∼= k(V′) of their function ﬁelds which is the identity on k. The remainder of
the proof is identical to what we did in Corollary 3.
□
In ﬁeld theory, there is a concept of transcendence degree which is closely related
to what we have been studying. In general, when we have a ﬁeld K containing k, we
have the following deﬁnition.

§5 Dimension and Algebraic Independence
513
Deﬁnition 9. Let K be a ﬁeld containing k. Then we say that K has transcendence
degree d over k provided that d is the largest number of elements of K which are
algebraically independent over k.
If we combine this deﬁnition with Theorem 7, then for any irreducible afﬁne
variety V, we have
dimV = the transcendence degree of k(V) over k.
Many books on algebraic geometry use this as the deﬁnition of the dimension of an
irreducible variety. The dimension of an arbitrary variety is then deﬁned to be the
maximum of the dimensions of its irreducible components.
For an example of transcendence degree, suppose that k is inﬁnite, so that k(V) =
k(x1, . . . , xn) when V = kn. Since kn has dimension n, we conclude that the ﬁeld
k(x1, . . . , xn) has transcendence degree n over k. It is clear that the transcendence
degree is at least n, but it is less obvious that no n + 1 elements of k(x1, . . . , xn) can
be algebraically independent over k. So our study of dimension yields some insights
into the structure of ﬁelds.
To fully understand transcendence degree, one needs to study more about alg-
ebraic and transcendental ﬁeld extensions. See Section 14.9 of DUMMIT and FOOTE
(2004).
EXERCISES FOR §5
1. Let φ1, . . . , φr ∈k[V] be algebraically independent over k.
a. Prove that the φi are distinct and nonzero.
b. Prove that any nonempty subset of {φ1, . . . , φr} consists of algebraically independent
elements over k.
c. Let y1, . . . , yr be variables and consider the map α : k[y1, . . . , yr] →k[V] deﬁned by
α(F) = F(φ1, . . . , φr). Show that α is a one-to-one ring homomorphism.
2. This exercise is concerned with the proof of Theorem 2.
a. If f1, . . . , fr ∈k[x1, . . . , xn] have total degree ≤N and F ∈k[x1, . . . , xn] has total
degree ≤s, show that F( f1, . . . , fr) has total degree ≤Ns.
b. Show that the map α deﬁned in the proof of Theorem 2 is a well-deﬁned linear map.
3. Complete the proof of Corollary 3.
4. Let k be an algebraically closed ﬁeld and let I ⊆R = k[x1, . . . , xn] be an ideal. Show that
the dimension of V(I) is equal to the largest integer r for which there exist r variables
xi1, . . . , xir such that I ∩k[xi1, . . . , xir] = {0}. Hint: Use I rather than I(V) in the proof
of Theorem 2. Be sure to explain why dimV = deg(aHPR/I).
5. Let I = ⟨xy−1⟩⊆k[x, y]. What is the projection of V(I) to the x-axis and to the y-axis?
Note that V(I) projects densely to both axes, but in neither case is the projection the
whole axis.
6. Let k be inﬁnite and let I = ⟨xy, xz⟩⊆k[x, y, z].
a. Show that I ∩k[x] = 0, but that I ∩k[x, y] and I ∩k[x, z] are not equal to 0.
b. Show that I ∩k[y, z] = 0, but that I ∩k[x, y, z] ̸= 0.
c. What do you conclude about the dimension of V(I)?
7. Here is a more complicated example of the phenomenon exhibited in Exercise 6. Again,
assume that k is inﬁnite and let I = ⟨zx −x2, zy −xy⟩⊆k[x, y, z].
a. Show that I ∩k[z] = 0. Is either I ∩k[x, z] or I ∩k[y, z] equal to 0?
b. Show that I ∩k[x, y] = 0, but that I ∩k[x, y, z] ̸= 0.
c. What does part (b) say about dimV(I)?

514
Chapter 9 The Dimension of a Variety
8. Given 1 ≤i1 < · · · < ir ≤n, deﬁne a linear map π : kn →kn by letting π(a1, . . . , an)
be the vector whose ij-th coordinate is aij for 1 ≤j ≤r and whose other coordinates are
0. Show that π ◦π = π and determine the image of π.
9. In this exercise, we will show that there can be more than one projection onto a given
subspace H ⊆kn.
a. Show that the matrices

1 0
0 0

,

1 1
0 0

both deﬁne projections from R2 onto the x-axis. Draw a picture that illustrates what
happens to a typical point of R2 under each projection.
b. Show that there is a one-to-one correspondence between projections of R2 onto the
x-axis and nonhorizontal lines in R2 through the origin.
c. More generally, ﬁx an r-dimensional subspace H ⊆kn. Show that there is a one-
to-one correspondence between projections of kn onto H and (n −r)-dimensional
subspaces H′ ⊆kn which satisfy H ∩H′ = {0}. Hint: Consider the kernel of the
projection.
10. This exercise is concerned with the proof of Theorem 7.
a. If f1, . . . , fr ∈k[x1, . . . , xn] have total degree ≤N and F ∈k[x1, . . . , xn] has total
degree ≤s, show that f sF( f1/f , . . . , fr/f ) is a polynomial in k[x1, . . . , xn] .
b. Show that the polynomial of part (a) has total degree ≤Ns.
c. Show that the map β deﬁned in the proof of Theorem 7 is a well-deﬁned linear map.
11. Complete the proof of Corollary 8.
12. Suppose that φ
:
V
→
W is a polynomial map between afﬁne varieties (see
Chapter 5, §1). We proved in §4 of Chapter 5 that φ induces a ring homomorphism
φ∗: k[W] →k[V] which is the identity on k. From φ, we get the subset φ(V) ⊆W. We
say that φ is dominating if the smallest variety of W containing φ(V) is W itself. Thus,
φ is dominating if its image is Zariski dense in W.
a. Show that φ is dominating if and only if the homomorphism φ∗: k[W] →k[V] is
one-to-one. Hint: Show that W′ ⊆W is a proper subvariety if and only if there is
nonzero element [ f ] ∈k[W] such that W′ ⊆W ∩V( f ).
b. If φ is dominating, show that dimV ≥dimW. Hint: Use Theorem 2 and part (a).
13. This exercise will study the relation between parametrizations and dimension. Assume
that k is an inﬁnite ﬁeld.
a. Suppose that F : km →V is a polynomial parametrization of a variety V (as deﬁned
in Chapter 3, §3). Thus, m is the number of parameters and V is the smallest variety
containing F(km). Prove that m ≥dimV.
b. Give an example of a polynomial parametrization F : km →V where m > dimV.
c. Now suppose that F : km \ W →V is a rational parametrization of V (as deﬁned
in Chapter 3, §3). We know that V is irreducible by Proposition 6 of Chapter 4, §5.
Show that we can deﬁne a ﬁeld homomorphism F∗: k(V) →k(t1, . . . , tm) which is
one-to-one. Hint: See the proof of Theorem 10 of Chapter 5, §5.
d. If F : km \ W →V is a rational parametrization, show that m ≥dimV.
14. Suppose that V ⊆Pn(k) is an irreducible projective variety and let k(V) be its ﬁeld of
rational functions as deﬁned in Exercise 13 of Chapter 8, §3.
a. Prove that dimV is the transcendence degree of k(V) over k. Hint: Reduce to the afﬁne
case.
b. We say that two irreducible projective varieties V and V′ (lying possibly in different
projective spaces) are birationally equivalent if any of their nonempty afﬁne portions
V ∩Ui and V′ ∩Uj are birationally equivalent in the sense of Chapter 5, §5. Prove
that V and V′ are birationally equivalent if and only if there is a ﬁeld isomorphism
k(V) ∼= k(V′) which is the identity on k. Hint: Use Exercise 13 of Chapter 8, §3 and
Theorem 10 of Chapter 5, §5.
c. Prove that birationally equivalent projective varieties have the same dimension.

§6 Dimension and Nonsingularity
515
§6
Dimension and Nonsingularity
This section will explore how dimension is related to the geometric properties of
a variety V. The discussion will be rather different from §5, where the algebraic
properties of k[V] and k(V) played a dominant role. We will introduce some rather
sophisticated concepts, and some of the theorems will be proved only in special
cases. For convenience, we will assume that V is always an afﬁne variety.
When we look at a surface V ⊆R3, one intuitive reason for saying that it is 2-
dimensional is that at a point p on V, a small portion of the surface looks like a small
portion of the plane. This is reﬂected by the way the tangent plane approximates
V at p:
←
←
the surface V
the tangent plane to V at p
Of course, we have to be careful because the surface may have points where there
does not seem to be a tangent plane. For example, consider the cone V(x2 +y2 −z2).
There seems to be a nice tangent plane everywhere except at the origin:
y
x
z
In this section, we will introduce the concept of a nonsingular point p of a variety
V, and we will give a careful deﬁnition of the tangent space Tp(V) of V at p. Our
discussion will generalize what we did for curves in §4 of Chapter 3. The tangent
space gives useful information about how the variety V behaves near the point p.
This is the so-called "local viewpoint." Although we have not discussed this topic
previously, it plays an important role in algebraic geometry. In general, properties
which reﬂect the behavior of a variety near a given point are called local properties.

516
Chapter 9 The Dimension of a Variety
We begin with a discussion of the tangent space. For a curve V deﬁned by an
equation f(x, y) = 0 in R2, we saw in Chapter 3 that the line tangent to the curve at
a point (a, b) ∈V is deﬁned by the equation
∂f
∂x(a, b) · (x −a) + ∂f
∂y(a, b) · (y −b) = 0,
provided that the two partial derivatives do not both vanish (see Exercise 4 of Chap-
ter 3, §4). We can generalize this to an arbitrary variety as follows.
Deﬁnition 1. Let V ⊆kn be an afﬁne variety and let p = (p1, . . . , pn) ∈V be a
point.
(i) If f ∈k[x1, . . . , xn] is a polynomial, the linear part of f at p, denoted dp( f), is
deﬁned to be the polynomial
dp( f) = ∂f
∂x1
(p)(x1 −p1) + · · · + ∂f
∂xn
(p)(xn −pn).
Note that dp( f) has total degree ≤1.
(ii) The tangent space of V at p, denoted Tp(V), is the variety
Tp(V) = V(dp( f) | f ∈I(V)).
If we are working over R, then the partial derivative ∂f
∂xi has the usual meaning.
For other ﬁelds, we use the formal partial derivative, which is deﬁned by
∂
∂xi


α1,...,αn
cα1...αnxα1
1 · · · xαi
i · · · xαn
n

=

α1,...,αn
cα1...αnαi xα1
1 · · · xαi−1
i
· · · xαn
n .
In Exercise 1, you will show that the usual rules of differentiation apply to
∂
∂xi . We
ﬁrst prove some simple properties of Tp(V).
Proposition 2. Let p ∈V ⊆kn.
(i) If I(V) = ⟨f1, . . . , fs⟩, then Tp(V) = V(dp( f1), . . . , dp( fs)).
(ii) Tp(V) is a translate of a linear subspace of kn.
Proof. (i) By the product rule, it is easy to show that
dp(hf) = h(p) · dp( f) + dp(h) · f(p)
(see Exercise 2). This implies dp(hf) = h(p) · dp( f) when f(p) = 0, and it follows
that if g = s
i=1 hi fi ∈I(V) = ⟨f1, . . . , fs⟩, then
dp(g) =
s

i=1
dp(hi fi) =
s

i=1
hi(p) · dp( fi) ∈⟨dp( f1), . . . , dp( fs)⟩.
This shows that Tp(V) is deﬁned by the vanishing of the dp( fi).

§6 Dimension and Nonsingularity
517
(ii) Introduce new coordinates on kn by setting Xi = xi −pi for 1 ≤i ≤n. This
coordinate system is obtained by translating p to the origin. By part (i), we know
that Tp(V) is given by dp( f1) = · · · = dp( fs) = 0. Since each dp( fi) is linear in
X1, . . . , Xn, we see that Tp(V) is a linear subspace with respect to the Xi. In terms of
the original coordinates, this means that Tp(V) is a translate of a subspace of kn. □
We can get an intuitive idea of what the tangent space means by thinking about
Taylor's formula for a polynomial of several variables. For a polynomial of one
variable, one has the standard formula
f(x) = f(a) + f ′(a)(x −a) + terms involving higher powers of x −a.
For f ∈k[x1, . . . , xn], you will show in Exercise 3 that if p = (p1, . . . , pn), then
f = f(p) + ∂f
∂x1
(p)(x1 −p1) + · · · + ∂f
∂xn
(p)(xn −pn)
+ terms of total degree ≥2 in x1 −p1, . . . , xn −pn.
This is part of Taylor's formula for f at p. When p ∈V and f ∈I(V), we have
f(p) = 0, so that
f = dp( f) + terms of total degree ≥2 in x1 −p1, . . . , xn −pn.
Thus dp( f) is the best linear approximation of f near p. Now suppose that I(V) =
⟨f1, . . . , fs⟩. Then V is deﬁned by the vanishing of the fi, so that the best linear
approximation to V near p should be deﬁned by the vanishing of the dp( fi). By
Proposition 2, this is exactly the tangent space Tp(V).
We can also think about Tp(V) in terms of lines that meet V with "higher mul-
tiplicity" at p. In Chapter 3, this was how we deﬁned the tangent line for curves in
the plane. In the higher dimensional case, suppose that we have p ∈V and let L be
a line through p. We can parametrize L by F(t) = p + tv, where v ∈kn is a vector
parallel to L. If f ∈k[x1, . . . , xn], then f ◦F(t) is a polynomial in the variable t, and
note that f ◦F(0) = f(p). Thus, 0 is a root of this polynomial whenever f ∈I(V).
We can use the multiplicity of this root to decide when L is contained in Tp(V).
Proposition 3. If L is a line through p parametrized by F(t) = p + tv, then L ⊆
Tp(V) if and only if 0 is a root of multiplicity ≥2 of f ◦F(t) for all f ∈I(V).
Proof. If we write the parametrization of L as xi = pi + tvi for 1 ≤i ≤n, where
p = (p1, . . . , pn) and v = (v1, . . . , vn), then, for any f ∈I(V), we have
g(t) = f ◦F(t) = f(p1 + tv1, . . . , pn + tvn).
As we noted above, g(0) = 0 because p ∈V, so that t = 0 is a root of g(t). In
Exercise 5 of Chapter 3, §4, we showed that t = 0 is a root of multiplicity ≥2 if
and only if we also have g′(0) = 0. Using the chain rule for functions of several
variables, we obtain

518
Chapter 9 The Dimension of a Variety
dg
dt = ∂f
∂x1
dx1
dt + · · · + ∂f
∂xn
dxn
dt = ∂f
∂x1
v1 + · · · + ∂f
∂xn
vn.
If follows that g′(0) = 0 if and only if
0 =
n

i=1
∂f
∂xi
(p)vi =
n

i=1
∂f
∂xi
(p)((pi + vi) −pi).
The expression on the right in this equation is dp( f) evaluated at the point p+v ∈L,
and it follows that p + v ∈Tp(V) if and only if g′(0) = 0 for all f ∈I(V). Since
p ∈L, we know that L ⊆Tp(V) is equivalent to p + v ∈Tp(V), and the proposition
is proved.
□
It is time to look at some examples.
Example 4. Let V ⊆Cn be the hypersurface deﬁned by f
= 0, where f
∈
k[x1, . . . , xn] is a nonconstant polynomial. By Proposition 9 of Chapter 4, §2, we
have
I(V) = I(V( f)) =
"
⟨f⟩= ⟨fred⟩,
where fred = f1 · · · fr is the product of the distinct irreducible factors of f. We will
assume that f = fred. This implies that
V = V( f) = V( f1 · · · fr) = V( f1) ∪· · · ∪V( fr)
is the decomposition of V into irreducible components (see Exercise 10 of Chapter 4,
§6). In particular, every component of V has dimension n −1 by the afﬁne version
of Proposition 2 of §4.
Since I(V) = ⟨f⟩, it follows from Proposition 2 that for any p ∈V, Tp(V) is the
linear space deﬁned by the single equation
∂f
∂x1
(p)(x1 −p1) + · · · + ∂f
∂xn
(p)(xn −pn) = 0.
This implies that
(1)
dim Tp(V) =

n −1
at least one ∂f
∂xi (p) ̸= 0
n
all ∂f
∂xi (p) = 0.
You should check how this generalizes Proposition 2 of Chapter 3, §4.
For a speciﬁc example, consider V = V(x2 −y2z2 + z3). In Exercise 4, you will
show that f = x2 −y2z2 + z3 ∈C[x, y, z] is irreducible, so that I(V) = ⟨f⟩. The
partial derivatives of f are
∂f
∂x = 2x,
∂f
∂y = −2yz2,
∂f
∂z = −2y2z + 3z2.

§6 Dimension and Nonsingularity
519
We leave it as an exercise to show that on V, the partials vanish simultaneously only
on the y-axis, which lies in V. Thus, the tangent spaces Tp(V) are all 2-dimensional,
except along the y-axis, where they are all of C3. Over R, we get the following
picture of V (which appeared earlier in §2 of Chapter 1):
y
x
z
When we give the deﬁnition of nonsingular point later in this section, we will see
that the points of V on the y-axis are the singular points, whereas other points of V
are nonsingular.
Example 5. Now consider the curve C ⊆C3 obtained by intersecting the surface V
of Example 4 with the plane x + y + z = 0. Thus, C = V(x + y + z, x2 −y2z2 + z3).
Using the techniques of §3, you can verify that dim C = 1.
In the exercises, you will also show that ⟨f1, f2⟩= ⟨x + y + z, x2 −y2z2 + z3⟩is
a prime ideal, so that C is an irreducible curve. Since a prime ideal is radical, the
Nullstellensatz implies that I(C) = ⟨f1, f2⟩. Thus, for p = (a, b, c) ∈C, it follows
that Tp(C) is deﬁned by the linear equations
dp( f1) = 1 · (x −a) + 1 · (y −b) + 1 · (z −c) = 0,
dp( f2) = 2a · (x −a) + (−2bc2) · (y −b) + (−2b2c + 3c2) · (z −c) = 0.
This is a system of linear equations in x −a, y −b, z −c with coefﬁcient matrix
Jp( f1, f2) =

1
1
1
2a −2bc2 −2b2c + 3c2

.
Let rank(Jp( f1, f2)) denote the rank of this matrix. Since Tp(C) is a translate of the
kernel of Jp( f1, f2), it follows that
dim Tp(C) = 3 −rank(Jp( f1, f2)).
In the exercises, you will show that Tp(C) is 1-dimensional at all points of C except
for the origin, where T0(C) is the 2-dimensional plane x + y + z = 0.
In these examples, we were careful to always compute I(V). It would be much
nicer if we could use any set of deﬁning equations of V. Unfortunately, this does

520
Chapter 9 The Dimension of a Variety
not always work: if V = V( f1, . . . , fs), then Tp(V) need not be deﬁned by dp( f1) =
· · · = dp( fs) = 0. For example, let V be the y-axis in k2. Then V is deﬁned by
x2 = 0, but you can easily check that Tp(V) ̸= V(dp(x2)) for all p ∈V. However,
in Theorem 9, we will ﬁnd a nice condition on f1, . . . , fs which, when satisﬁed, will
allow us to compute Tp(V) using the dp( fi)'s.
Examples 4 and 5 indicate that the nicest points on V are the ones where Tp(V)
has the same dimension as V. But this principle does not apply when V has irre-
ducible components of different dimensions. For example, let V = V(xz, yz) ⊆R3.
This is the union of the (x, y)-plane and the z-axis, and it is easy to check that
dim Tp(V) =
⎧
⎨
⎩
2
p is on the (x, y)-plane minus the origin
1
p is on the z-axis minus the origin
3
p is the origin.
Excluding the origin, points on the z-axis have a 1-dimensional tangent space, which
seems intuitively correct. Yet at such a point, we have dim Tp(V) < dimV. The
problem, of course, is that we are on a component of the wrong dimension.
To avoid this difﬁculty, we need to deﬁne the dimension of a variety at a point.
Deﬁnition 6. Let V be an afﬁne variety. For p ∈V, the dimension of V at p, denoted
dimpV, is the maximum dimension of an irreducible component of V containing p.
By Corollary 9 of §4, we know that dimV is the maximum of dimpV as p varies
over all points of V. If V is a hypersurface in Cn, it is easy to compute dimpV, for in
Example 4, we showed that every irreducible component of V has dimension n −1.
It follows that dimpV = n −1 for all p ∈V. On the other hand, if V ⊆kn is an
arbitrary variety, the theory developed in §§3 and 4 enables us to compute dimV,
but unless we know how to decompose V into irreducible components, more subtle
tools are needed to compute dimpV. This will be discussed in §7 when we study the
properties of the tangent cone.
We can now deﬁne what it means for a point p ∈V to be nonsingular.
Deﬁnition 7. Let p be a point on an afﬁne variety V. Then p is nonsingular (or
smooth) provided dim Tp(V) = dimpV. Otherwise, p is a singular point of V.
If we look back at our previous examples, it is easy to identify which points are
nonsingular and which are singular. In Example 5, the curve C is irreducible, so that
dimpC = 1 for all p ∈C and, hence, the singular points are where dim Tp(C) ̸= 1
(only one in this case). For the hypersurfaces V = V( f) considered in Example 4,
we know that dimpV = n −1 for all p ∈V, and it follows from (1) so that p is
singular if and only if all of the partial derivatives of f vanish at p. This means that
the singular points of V form the variety
(2)
Σ = V

f, ∂f
∂x1
, . . . , ∂f
∂xn

.
In general, the singular points of a variety V have the following properties.

§6 Dimension and Nonsingularity
521
Theorem 8. Let V ⊆kn be an afﬁne variety and let
Σ = {p ∈V | p is a singular point of V}.
We call Σ the singular locus of V. Then:
(i) Σ is an afﬁne variety contained in V.
(ii) If p ∈Σ, then dim Tp(V) > dimpV.
(iii) Σ contains no irreducible component of V.
(iv) If Vi and Vj are distinct irreducible components of V, then Vi ∩Vj ⊆Σ.
Proof. A complete proof of this theorem is beyond the scope of the book. Instead,
we will assume that V is a hypersurface in Cn and show that the theorem holds in
this case. As we discuss each part of the theorem, we will give references for the
general case.
Let V = V( f) ⊆Cn be a hypersurface such that I(V) = ⟨f⟩. We noted earlier
that dimpV = n −1 and that Σ consists of those points of V where all of the
partial derivatives of f vanish simultaneously. Then (2) shows that Σ is an afﬁne
variety, which proves (i) for hypersurfaces. A proof in the general case is given in
the Corollary to Theorem 6 in Chapter II, §2 of SHAFAREVICH (2013).
Part (ii) of the theorem says that at a singular point of V, the tangent space is too
big. When V is a hypersurface in Cn, we know from (1) that if p is a singular point,
then dim Tp(V) = n > n −1 = dimpV. This proves (ii) for hypersurfaces, and the
general case follows from Theorem 3 in Chapter II, §1 of SHAFAREVICH (2013).
Part (iii) says that on each irreducible component of V, the singular locus consists
of a proper subvariety. Hence, most points of a variety are nonsingular. To prove this
for a hypersurface, let V = V( f) = V( f1) ∪· · · ∪V( fr) be the decomposition of
V into irreducible components, as discussed in Example 4. Suppose that Σ contains
one of the components, say V( f1). Then every ∂f
∂xi vanishes on V( f1). If we write
f = f1g, where g = f2 · · · fr, then
∂f
∂xi
= f1
∂g
∂xi
+ ∂f1
∂xi
g
by the product rule. Since f1 certainly vanishes on V( f1), it follows that ∂f1
∂xi g also
vanishes on V(f1). By assumption, f1 is irreducible, so that
∂f1
∂xi
g ∈I(V( f1)) = ⟨f1⟩.
This says that f1 divides ∂f1
∂xi g and, hence, f1 divides ∂f1
∂xi or g. The latter is impossible
since g is a product of irreducible polynomials distinct from f1 (meaning that none
of them is a constant multiple of f1). Thus, f1 must divide ∂f1
∂xi for all i. Since ∂f1
∂xi has
smaller total degree than f1, we must have ∂f1
∂xi = 0 for all i, and it follows that f1 is
constant (see Exercise 9). This contradiction proves that Σ contains no component
of V.

522
Chapter 9 The Dimension of a Variety
When V is an arbitrary irreducible variety, a proof that Σ is a proper subva-
riety can be found in the corollary to Theorems 4.1 and 4.3 in Chapter IV of
KENDIG (2015). See also the discussion preceding the deﬁnition of singular point
in Chapter II, §1 of SHAFAREVICH (2013). If V has two or more irreducible
components, the claim follows from the irreducible case and part (iv) below. See
Exercise 11 for the details.
Finally, part (iv) of the theorem says that a nonsingular point of a variety lies on a
unique irreducible component. In the hypersurface case, suppose that V = V( f) =
V( f1) ∪· · · ∪V( fr) and that p ∈V( fi) ∩V( fi) for i ̸= j. Then we can write f = gh,
where fi divides g and fj divides h. Hence, g(p) = h(p) = 0, and then an easy
argument using the product rule shows that ∂f
∂xi (p) = 0 for all i. This proves that
V( fi) ∩V( fj) ⊆Σ, so that (iv) is true for hypersurfaces. When V is an arbitrary
variety, see Theorem 6 in Chapter II, §2 of SHAFAREVICH (2013).
□
In some cases, it is also possible to show that a point of a variety V is nonsingular
without having to compute I(V). To formulate a precise result, we will need some
notation. Given f1, . . . , fr ∈k[x1, . . . , xn], as in Chapter 6 let J( f1, . . . , fr) be the
Jacobian matrix, the r × n matrix of partial derivatives:
J( f1, . . . , fr) =
⎛
⎜
⎝
∂f1
∂x1 · · · ∂f1
∂xn
...
...
∂fr
∂x1 · · · ∂fr
∂xn
⎞
⎟
⎠.
Given p ∈kn, evaluating this matrix at p gives an r × n matrix of numbers denoted
Jp( f1, . . . , fr). Then we have the following result.
Theorem 9. Let V = V( f1, . . . , fr) ⊆Cn be an arbitrary variety and suppose that
p ∈V is a point where Jp( f1, . . . , fr) has rank r. Then p is a nonsingular point of V
and lies on a unique irreducible component of V of dimension n −r.
Proof. As with Theorem 8, we will only prove this for a hypersurface V =
V( f) ⊆Cn, which is the case r = 1 of the theorem. Here, note that f is now
any deﬁning equation of V, and, in particular, it could happen that I(V) ̸= ⟨f⟩. But
we still know that f vanishes on V, and it follows from the deﬁnition of tangent
space that
(3)
Tp(V) ⊆V(dp( f)).
Since r = 1, Jp( f) is the row vector whose entries are ∂f
∂xi (p), and our assumption
that Jp( f) has rank 1 implies that at least one of the partials is nonzero at p. Thus,
dp( f) is a nonzero linear function of xi−pi, and it follows from (3) that dim Tp(V) ≤
n −1. If we compare this to (1), we see that p is a nonsingular point of V, and by
part (iv) of Theorem 8, it lies on a unique irreducible component of V. Since the
component has the predicted dimension n −r = n −1, we are done. For the general
case, see Theorem (1.16) of MUMFORD (1981).
□
Theorem 9 is important for several reasons. First of all, it is very useful for
determining the nonsingular points and dimension of a variety. For instance, it is

§6 Dimension and Nonsingularity
523
now possible to redo Examples 4 and 5 without having to compute I(V) and I(C).
Another aspect of Theorem 9 is that it relates nicely to our intuition that the dimen-
sion should drop by one for each equation deﬁning V. This is what happens in the
theorem, and in fact we can sharpen our intuition as follows. Namely, the dimen-
sion should drop by one for each deﬁning equation, provided the deﬁning equations
are sufﬁciently independent [which means that rank(Jp( f1, . . . , fr)) = r]. In Exer-
cise 16, we will give a more precise way to state this. Furthermore, note that our
intuition applies to the nonsingular part of V.
Theorem 9 is also related to some important ideas in the calculus of several vari-
ables. In particular, the Implicit Function Theorem has the same hypothesis concern-
ing Jp( f1, . . . , fr) as Theorem 9. When V = V( f1, . . . , fr) satisﬁes this hypothesis,
the complex variable version of the Implicit Function Theorem asserts that near
p, the variety V looks like the graph of a nice function of n −r variables, and
we get a vivid picture of why V has dimension n −r at p. To understand the full
meaning of Theorem 9, one needs to study the notion of a manifold. A nice discus-
sion of this topic and its relation to nonsingularity and dimension can be found in
KENDIG (2015).
EXERCISES FOR §6
1. We will discuss the properties of the formal derivative deﬁned in the text.
a. Show that
∂
∂xi is k-linear and satisﬁes the product rule.
b. Show that
∂
∂xi
 ∂
∂xj f

=
∂
∂xj
 ∂
∂xi f

for all i and j.
c. If f1, . . . , fr ∈k[x1, . . . , xn], compute
∂
∂xi ( f α1
1
· · · f αr
r ).
d. Formulate and prove a version of the chain rule for computing the partial derivatives
of a polynomial of the form F( f1, . . . , fr). Hint: Use part (c).
2. Prove that dp(hf ) = h(p) · dp(f ) + dp(h) · f (p).
3. Let p = (p1, . . . , pn) ∈kn and let f ∈k[x1, . . . , xn].
a. Show that f can be written as a polynomial in xi −pi. Hint: xm
i = ((xi −pi) + pi)m.
b. Suppose that when we write f as a polynomial in xi −pi, every term has total degree
at least 2. Show that ∂f
∂xi (p) = 0 for all i.
c. If we write f as a polynomial in xi −pi, show that the constant term is f (p) and the
linear term is dp( f ). Hint: Use part (b).
4. As in Example 4, let f = x2 −y2z2 + z3 ∈C[x, y, z] and let V = V( f ) ⊆C3.
a. Show carefully that f is irreducible in C[x, y, z].
b. Show that V contains the y-axis.
c. Let p ∈V. Show that the partial derivatives of f all vanish at p if and only if p lies on
the y-axis.
5. Let A be an m × n matrix, where n ≥m. If r ≤m, we say that a matrix B is an r × r
submatrix of A provided that B is the matrix obtained by ﬁrst selecting r columns of A,
and then selecting r rows from those columns.
a. Pick a 3 × 4 matrix of numbers and write down all of its 3 × 3 and 2 × 2 submatrices.
b. Show that A has rank < r if and only if all t × t submatrices of A have determinant
zero for all r ≤t ≤m. Hint: The rank of a matrix is the maximum number of linearly
independent columns. If A has rank s, it follows that you can ﬁnd an m × s submatrix
of rank s. Now use the fact that the rank is also the maximum number of linearly
independent rows. What is the criterion for an r × r matrix to have rank < r?
6. As in Example 5, let C = V(x + y + z, x2 −y2z2 + z3) ⊆C3 and let I be the ideal
I = ⟨x + y + z, x2 −y2z2 + z3⟩⊆C[x, y, z].

524
Chapter 9 The Dimension of a Variety
a. Show that I is a prime ideal. Hint: Introduce new coordinates X = x + y + z, Y = y,
and Z = z. Show that I = ⟨X, F(Y,Z)⟩for some polynomial in Y, Z. Prove that
C[X, Y, Z]/I ∼= C[Y, Z]/⟨F⟩and show that F ∈C[Y, Z] is irreducible.
b. Conclude that C is an irreducible variety and that I(C) = I.
c. Compute the dimension of C.
d. Determine all points (a, b, c) ∈C such that the 2 × 3 matrix
Jp( f1, f2) =
 1
1
1
2a −2bc2
−2b2c + 3c2

has rank < 2. Hint: Use Exercise 5.
7. Let f = x2 ∈k[x, y]. In k2, show that Tp(V( f )) ̸= V(dp( f )) for all p ∈V.
8. Let V = V(xy, xz) ⊆k3 and assume that k is an inﬁnite ﬁeld.
a. Compute I(V).
b. Verify the formula for dim Tp(V) given in the text.
9. Suppose that f ∈k[x1, . . . , xn] is a polynomial such that
∂
∂xi f = 0 for all i. If the ﬁeld
k has characteristic 0 (which means that k contains a ﬁeld isomorphic to Q), then show
that f must be the constant.
10. The result of Exercise 9 may be false if k does not have characteristic 0.
a. Let f = x2 + y2 ∈F2[x, y], where F2 is a ﬁeld with two elements. What are the partial
derivatives of f ?
b. To analyze the case when k does not have characteristic 0, we need to deﬁne the
characteristic of k. Given any ﬁeld k, show that there is a ring homomorphism φ :
Z →k which sends n > 0 in Z to 1 ∈k added to itself n times. If φ is one-to-one,
argue that k contains a copy of Q and hence has characteristic 0.
c. If k does not have characteristic 0, it follows that the map φ of part (b) cannot be
one-to-one. Show that the kernel must be the ideal ⟨p⟩⊆Z for some prime number
p. We say that k has characteristic p in this case. Hint: Use the Isomorphism Theorem
from Exercise 16 of Chapter 5, §2 and remember that k is an integral domain.
d. If k has characteristic p, show that (a + b)p = ap + bp for every a, b ∈k.
e. Suppose that k has characteristic p and let f ∈k[x1, . . . , xn]. Show that all partial
derivatives of f vanish identically if and only if every exponent of every monomial
appearing in f is divisible by p.
f. Suppose that k is algebraically closed and has characteristic p. If f ∈k[x1, . . . , xn] is
irreducible, then show that some partial derivative of f must be nonzero. This shows
that Theorem 8 is true for hypersurfaces over any algebraically closed ﬁeld. Hint: If
all partial derivatives vanish, use parts (d) and (e) to write f as a p-th power. Why do
you need k to be algebraically closed?
11. Let V = V1 ∪· · · ∪Vr be a decomposition of a variety into its irreducible components.
a. Suppose that p ∈V lies in a unique irreducible component Vi. Show that Tp(V) =
Tp(Vi). This reﬂects the local nature of the tangent space. Hint: One inclusion follows
easily from Vi ⊆V. For the other inclusion, pick a function f ∈I(W)\I(Vi), where W
is the union of the other irreducible components. Then g ∈I(Vi) implies fg ∈I(V).
b. With the same hypothesis as part (a), show that p is nonsingular in V if and only if it
is nonsingular in Vi.
c. Let Σ be the singular locus of V and let Σi be the singular locus of Vi. Prove that
Σ =

i̸=j
(Vi ∩Vj) ∪

i
Σi.
Hint: Use part (b) and part (iv) of Theorem 8.
d. If each Σi is a proper subset of Vi, then show that Σ contains no irreducible com-
ponents of V. This shows that part (iii) of Theorem 8 follows from the irreducible
case.

§7 The Tangent Cone
525
12. Find all singular points of the following curves in k2. Assume that k is algebraically
closed.
a. y2 = x3 −3.
b. y2 = x3 −6x2 + 9x.
c. x2y2 + x2 + y2 + 2xy(x + y + 1) = 0.
d. x2 = x4 + y4.
e. xy = x6 + y6.
f. x2y + xy2 = x4 + y4.
g. x3 = y2 + x4 + y4.
13. Find all singular points of the following surfaces in k3. Assume that k is algebraically
closed.
a. xy2 = z2.
b. x2 + y2 = z2.
c. x2y + x3 + y3 = 0.
d. x3 −zxy + y3 = 0.
14. Show that V(y −x2 + z2, 4x −y2 + w3) ⊆C4 is a nonempty smooth surface.
15. Let V ⊆kn be a hypersurface with I(V) = ⟨f ⟩. Show that if V is not a hyperplane
and p ∈V is nonsingular, then either the variety V ∩Tp(V) has a singular point at p
or the restriction of f to Tp(V) has an irreducible factor of multiplicity ≥2. Hint: Pick
coordinates so that p = 0 and Tp(V) is deﬁned by x1 = 0. Thus, we can regard Tp(V)
as a copy of kn−1, then V ∩Tp(V) is a hypersurface in kn−1. Then the restriction of f to
Tp(V) is the polynomial f (0, x2, . . . , xn). See also Example 4.
16. Let V ⊆Cn be irreducible and let p ∈V be a nonsingular point. Suppose that V has
dimension d.
a. Show that we can always ﬁnd polynomials f1, . . . , fn−d ∈I(V) with the property that
Tp(V) = V(dp( f1), . . . , dp( fn−d)).
b. If f1, . . . , fn−d are as in part (a) show that Jp( f1, . . . , fn−d) has rank n−d and conclude
that V is an irreducible component of V( f1, . . . , fn−d). This shows that although V
itself may not be deﬁned by n −d equations, it is a component of a variety that is.
Hint: Use Theorem 9.
17. Suppose that V ⊆Cn is irreducible of dimension d and suppose that I(V) = ⟨f1, . . . , fs⟩.
a. Show that p ∈V is nonsingular if and only if Jp( f1, . . . , fs) has rank n −d. Hint: Use
Proposition 2.
b. By Theorem 8, we know that V has nonsingular points. Use this and part (a) to prove
that d ≥n −s. How does this relate to Proposition 5 of §4?
c. Let D be the set of determinants of all (n −d) × (n −d) submatrices of J( f1, . . . , fs).
Prove that the singular locus of V is Σ = V ∩V(g | g ∈D). Hint: Use part (a)
and Exercise 5. Also, what does part (ii) of Theorem 8 tell you about the rank of
Jp( f1, . . . , fs)?
§7
The Tangent Cone
In this ﬁnal section of the chapter, we will study the tangent cone of a variety V at
a point p. When p is nonsingular, we know that, near p, V is nicely approximated
by its tangent space Tp(V). This clearly fails when p is singular, for as we saw in
Theorem 8 of §6, the tangent space has the wrong dimension (it is too big). To
approximate V near a singular point, we need something different.

526
Chapter 9 The Dimension of a Variety
We begin with an example.
Example 1. Consider the curve y2 = x2(x + 1), which has the following picture in
the plane R2:
x
y
We see that the origin is a singular point. Near this point, the curve is approximated
by the lines x = ±y. These lines are deﬁned by x2 −y2 = 0, and if we write the
deﬁning equation of the curve as f(x, y) = x2 −y2 + x3 = 0, we see that x2 −y2 is
the nonzero homogeneous component of f of smallest total degree.
Similarly, consider the curve y2 −x3 = 0:
x
y
The origin is again a singular point, and the nonzero homogeneous component of
y2 −x3 of smallest total degree is y2. Here, V(y2) is the x-axis and gives a nice
approximation of the curve near (0, 0).
In both of the above curves, we approximated the curve near the singular point
using the smallest nonzero homogeneous component of the deﬁning equation. To
generalize this idea, suppose that p = (p1, . . . , pn) ∈kn. If α = (α1, . . . , αn) ∈
Zn
≥0, let
(x −p)α = (x1 −p1)α1 · · · (xn −pn)αn,
and note that (x −p)α has total degree |α| = α1 + · · · + αn. Now, given any
polynomial f ∈k[x1, . . . , xn] of total degree d, we can write f as a polynomial in

§7 The Tangent Cone
527
xi −pi, so that f is a k-linear combination of (x −p)α for |α| ≤d. If we group
according to total degree, we can write
(1)
f = fp,0 + fp,1 + · · · + fp,d,
where fp,j is a k-linear combination of (x−p)α for |α| = j. Note that fp,0 = f(p) and
fp,1 = dp( f) (as deﬁned in Deﬁnition 1 of the previous section). In the exercises,
you will discuss Taylor's formula, which shows how to express fp,j in terms of the
partial derivatives of f at p. In many situations, it is convenient to translate p to the
origin so that we can use homogeneous components. We can now deﬁne the tangent
cone.
Deﬁnition 2. Let V ⊆kn be an afﬁne variety and let p = (p1, . . . , pn) ∈V.
(i) If f ∈k[x1, . . . , xn] is a nonzero polynomial, then fp,min is deﬁned to be fp,j,
where j is the smallest integer such that fp,j ̸= 0 in (1).
(ii) The tangent cone of V at p, denoted Cp(V), is the variety
Cp(V) = V( fp,min | f ∈I(V)).
The tangent cone gets its name from the following proposition.
Proposition 3. Let p ∈V ⊆kn. Then Cp(V) is the translate of the afﬁne cone of a
variety in Pn−1(k).
Proof. Introduce new coordinates on kn by letting Xi = xi −pi. Relative to this coo-
rdinate system, we can assume that p is the origin 0. Then f0,min is a homogeneous
polynomial in X1, . . . , Xn, and as f varies over I(V), the f0,min generate a homoge-
neous ideal J ⊆k[X1, . . . , Xn]. Then Cp(V) = Va(J) ⊆kn by deﬁnition. Since J
is homogeneous, we also get a projective variety W = Vp(J) ⊆Pn−1(k), and as
we saw in Chapter 8, this means that Cp(V) is an afﬁne cone CW ⊆kn of W. This
proves the proposition.
□
The tangent cone of a hypersurface V ⊆kn is especially easy to compute. In
Exercise 2 you will show that if I(V) = ⟨f⟩, then Cp(V) is deﬁned by the sin-
gle equation fp,min = 0. This is exactly what we did in Example 1. However,
when I(V) = ⟨f1, . . . , fs⟩has more generators, it need not follow that Cp(V) =
V(( f1)p,min, . . . , ( fs)p,min). For example, suppose that V is deﬁned by xy = xz +
z(y2 −z2) = 0. In Exercise 3, you will show that I(V) = ⟨xy, xz+z(y2 −z2)⟩. To see
that C0(V) ̸= V(xy, xz), note that f = yz(y2−z2) = y(xz+z(y2−z2))−z(xy) ∈I(V).
Then f0,min = yz(y2 −z2) vanishes on C0(V), yet does not vanish on all of V(xy, xz).
We can overcome this difﬁculty by using an appropriate Gröbner basis. The result
is stated most efﬁciently when the point p is the origin.
Proposition 4. Assume that the origin 0 is a point of V ⊆kn. Let x0 be a new
variable and pick a monomial order on k[x0, x1, . . . , xn] such that among monomials
of the same total degree, any monomial involving x0 is greater than any monomial
involving only x1, . . . , xn (lex and grlex with x0 > · · · > xn satisfy this condition).
Then:

528
Chapter 9 The Dimension of a Variety
(i) Let I(V)h ⊆k[x0, x1, . . . , xn] be the homogenization of I(V) and let G1, . . . , Gs
be a homogeneous Gröbner basis of I(V)h with respect to the above monomial
order. Then
C0(V) = V((g1)0,min, . . . , (gs)0,min),
where gi = Gi(1, x1, . . . , xn) is the dehomogenization of Gi.
(ii) Suppose that k is algebraically closed, and let I be any ideal such that V = V(I).
If G1, . . . , Gs are a homogeneous Gröbner basis of Ih, then
C0(V) = V((g1)0,min, . . . , (gs)0,min),
where gi = Gi(1, x1, . . . , xn) is the dehomogenization of Gi.
Proof. In this proof, we will write fj and fmin rather than f0,j and f0,min.
(i) Let I = I(V). It sufﬁces to show that fmin ∈⟨(g1)min, . . . , (gs)min⟩for all
f ∈I. If this fails to hold, then we can ﬁnd f ∈I with fmin /∈⟨(g1)min, . . . , (gs)min⟩
such that LT( fmin) is minimal [note that we can regard fmin as a polynomial in
k[x0, x1, . . . , xn], so that LT( fmin) is deﬁned]. If we write f as a sum of homogeneous
components
f = fmin + · · · + fd,
where d is the total degree of f, then
f h = fmin · xa
0 + · · · + fd ∈Ih
for some a. By the way we chose the monomial order on k[x0, x1, . . . , xn], it follows
that LT( f h) = LT( fmin)xa
0. Since G1, . . . , Gs form a Gröbner basis, we know that
some LT(Gi) divides LT( fmin)xa
0.
If gi is the dehomogenization of Gi, then gi ∈I follows easily. Since Gi is homo-
geneous, we have
LT(Gi) = LT((gi)min)xb
0
for some b (see Exercise 4). This implies that LT( fmin) = cxαLT((gi)min) for some
nonzero c ∈k and some monomial xα in x1, . . . , xn. Now let ˜f = f −cxαgi ∈I. Since
fmin /∈⟨(g1)min, . . . , (gs)min⟩, we know that fmin −cxα(gi)min ̸= 0, and it follows that
˜f min = fmin −cxα(gi)min.
Then LT( ˜f min) < LT( fmin) since the leading terms of fmin and cxα(gi)min are equal.
This contradicts the minimality of LT( fmin), and (i) is proved. In the exercises, you
will show that g1, . . . , gs are a basis of I, though not necessarily a Gröbner basis.
(ii) Let W denote the variety V( fmin | f ∈I). If we apply the argument of part (i)
to the ideal I, we see immediately that
W = V((g1)min, . . . , (gs)min).
It remains to show that W is the tangent cone at the origin. Since I ⊆I(V), the
inclusion C0(V) ⊆W is obvious by the deﬁnition of tangent cone. Going the other

§7 The Tangent Cone
529
way, suppose that g ∈I(V). We need to show that gmin vanishes on W. By the
Nullstellensatz, we know that gm ∈I for some m and, hence, (gm)min = 0 on W. In
the exercises, you will check that (gm)min = (gmin)m and it follows that gmin vanishes
on W. This completes the proof of the proposition.
□
In practice, this proposition is usually used over an algebraically closed ﬁeld, for
here, part (ii) says that we can compute the tangent cone using any set of deﬁning
equations of the variety.
For an example of how to use Proposition 4, suppose V = V(xy, xz+ z(y2 −z2)).
If we set I = ⟨xy, xz + z(y2 −z2)⟩, the ﬁrst step is to determine Ih ⊆k[w, x, y, z],
where w is the homogenizing variable. Using grlex order on k[x, y, z], a Gröbner
basis for I is {xy, xz + z(y2 −z2), x2z −xz3}. By the theory developed in §4 of
Chapter 8, {xy, xzw + z(y2 −z2), x2zw −xz3} is a basis of Ih. In fact, it is a Gröbner
basis for grlex order, with the variables ordered x > y > z > w (see Exercise 5).
However, this monomial order does not satisfy the hypothesis of Proposition 4, but
if we use grlex with w > x > y > z, then a Gröbner basis is
{xy, xzw + z(y2 −z2), yz(y2 −z2)}.
Proposition 4 shows that if we dehomogenize and take minimal homogeneous com-
ponents, then the tangent cone at the origin is given by
C0(V) = V(xy, xz, yz(y2 −z2)).
In the exercises, you will show that this tangent cone is the union of ﬁve lines
through the origin in k3.
We will next study how the tangent cone approximates the variety V near the
point p. Recall from Proposition 3 that Cp(V) is the translate of an afﬁne cone,
which means that Cp(V) is made up of lines through p. So to understand the tangent
cone, we need to describe which lines through p lie in Cp(V).
y
z
x
←the variety V
←the tangent cone
at the origin
We will do this using secant lines. More precisely, let L be a line in kn through p.
Then L is a secant line of V if it meets V in a point distinct from p. Here is the
crucial idea: if we take secant lines determined by points of V getting closer and
closer to p, then the "limit" of the secant lines should lie on the tangent cone. You
can see this in the preceding picture.

530
Chapter 9 The Dimension of a Variety
To make this idea precise, we will work over the complex numbers C. Here, it
is possible to deﬁne what it means for a sequence of points qi ∈Cn to converge to
q ∈Cn. For instance, if we think of Cn as R2n, this means that the coordinates of
qi converge to the coordinates of q. We will assume that the reader has had some
experience with sequences of this sort.
We will treat lines through their parametrizations. Suppose we have parametrized
L via p + tv, where v ∈Cn is a nonzero vector parallel to L and t ∈C. Then we
deﬁne a limit of lines as follows.
Deﬁnition 5. We say that a line L ⊆Cn through a point p ∈Cn is a limit of lines
{Li}∞
i=1 through p if given a parametrization p+tv of L, there exist parametrizations
p + tvi of Li such that limi→∞vi = v in Cn.
This notion of convergence corresponds to the following picture:
↓
↓
↑
v
L
p
vi
Li
vi+1
Li+1
Now we can state a precise version of how the tangent cone approximates a
complex variety near a point.
Theorem 6. Let V ⊆Cn be an afﬁne variety. Then a line L through p ∈V lies in
the tangent cone Cp(V) if and only if there exists a sequence {qi}∞
i=1 of points in
V \ {p} converging to p such that if Li is the secant line containing p and qi, then
the lines Li converge to the given line L.
Proof. By translating p to the origin, we may assume that p = 0. Let {qi}∞
i=1 be
a sequence of points in V \ {0} converging to the origin and suppose the lines Li
through 0 and qi converge (in the sense of Deﬁnition 5) to some line L through the
origin. We want to show that L ⊆C0(V).
By the deﬁnition of Li converging to L, we can ﬁnd parametrizations tvi of Li
(remember that p = 0) such that the vi converge to v as i →∞. Since qi ∈Li, we
can write qi = tivi for some complex number ti. Note that ti ̸= 0 since qi ̸= 0. We
claim that the ti converge to 0. This follows because as i →∞, we have vi →v ̸= 0
and tivi = qi →0. (A more detailed argument will be given in Exercise 8.)

§7 The Tangent Cone
531
Now suppose that f is any polynomial that vanishes on V. As in the proof of
Proposition 4, we write fmin and fj rather than f0,min and f0,j. If f has total degree d,
then we can write f = fℓ+ fℓ+1 + · · · + fd, where fℓ= fmin. Since qi = tivi ∈V, we
have
(2)
0 = f(tivi) = fℓ(tivi) + · · · + fd(tivi).
Each fj is homogeneous of degree j, so that fj(tivi) = t j
i fj(vi). Thus,
(3)
0 = tℓ
i fℓ(vi) + · · · + td
i fd(vi).
Since ti ̸= 0, we can divide through by tℓ
i to obtain
(4)
0 = fℓ(vi) + ti fℓ+1(vi) + · · · + td−ℓ
i
fd(vi).
Letting i →∞, the right-hand side in (4) tends to fℓ(v) since vi →v and ti →0.
We conclude that fℓ(v) = 0, and since fℓ(tv) = tℓfℓ(v) = 0 for all t, it follows that
L ⊆C0(V). This shows that C0(V) contains all limits of secant lines determined by
sequences of points in V converging to 0.
To prove the converse, we will ﬁrst study the set
(5)
V = {(v, t) ∈Cn × C | tv ∈V, t ̸= 0} ⊆Cn+1.
If (v, t) ∈V, note that the L determined by 0 and tv ∈V is a secant line. Thus,
we want to know what happens to V as t →0. For this purpose, we will study the
Zariski closure V of V, which is the smallest variety in Cn+1 containing V. We claim
that
(6)
V = V ∪(C0(V) × {0}).
From §4 of Chapter 4, we know that V = V(I(V)). So we need to calculate the
functions that vanish on V. If f ∈I(V), write f = fℓ+ · · · + fd where fℓ= fmin, and
set
˜f = fℓ+ tfℓ+1 + · · · + td−ℓfd ∈C[t, x1, . . . , xn].
We will show that
(7)
I(V) = ⟨˜f | f ∈I(V)⟩⊆C[t, x1, . . . , xn].
One direction of the proof is easy, for f ∈I(V) and (v, t) ∈V imply f(tv) = 0,
and then equations (2), (3), and (4) show that ˜f(v, t) = 0. Conversely, suppose that
g ∈C[t, x1, . . . , xn] vanishes on V. Write g = 
i giti, where gi ∈C[x1, . . . , xn],
and let gi = 
j gij be the decomposition of gi into the sum of its homogeneous
components. If (v, t) ∈V, then for every λ ∈C \ {0}, we have (λv, λ−1t) ∈V
since (λ−1t) · (λv) = tv ∈V. Thus,

532
Chapter 9 The Dimension of a Variety
0 = g(λv, λ−1t) =

i,j
gij(λv)(λ−1t)i =

i,j
λjgij(v)λ−iti =

i,j
λj−igij(v)ti
for all λ ̸= 0. Letting m = j −i, we can organize this sum according to powers of λ:
0 =

m
 
i
gi,m+i(v)ti
λm.
Since this holds for all λ ̸= 0, it follows that 
i gi,m+i(v)ti = 0 for all m and,
hence, 
i gi,m+iti ∈I(V). Let fm = 
i gi,m+i ∈C[x1, . . . , xn]. Since (v, 1) ∈V for
all v ∈V, it follows that fm ∈I(V). Let i0 be the smallest i such that gi,m+i ̸= 0.
Then
˜f m = gi0,m+i0 + gi0+1,m+i0+1t + · · · ,
so that 
i gi,m+iti = ti0 ˜f m. From this, it follows easily that g ∈⟨˜f | f ∈I(V)⟩,
and (7) is proved.
From (7), we have V = V( ˜f | f ∈I(V)). To compute this variety, let (v, t) ∈
Cn+1, and ﬁrst suppose that t ̸= 0. Using (2), (3), and (4), it is straightforward to
show that ˜f(v, t) = 0 if and only if f(tv) = 0. Thus,
V ∩{(v, t) | t ̸= 0} = V.
Now suppose t = 0. If f = fmin + · · · + fd, it follows from the deﬁnition of ˜f that
˜f(v, 0) = 0 if and only if fmin(v) = 0. Hence,
V ∩{(v, t) | t = 0} = C0(V) × {0},
and (6) is proved.
To complete the proof of Theorem 6, we will need the following fact about
Zariski closure.
Proposition 7. Let Z ⊆W ⊆Cn be afﬁne varieties and assume that W is the Zariski
closure of W \ Z. If z ∈Z is any point, then there is a sequence of points {wi}∞
i=1 in
W \ Z which converges to z.
Proof. The proof of this is beyond the scope of the book. In Theorem (2.33) of
MUMFORD (1981), this result is proved for irreducible varieties in Pn(C). Exercise 9
will show how to deduce Proposition 7 from Mumford's theorem.
□
To apply this proposition to our situation, let Z = C0(V)×{0} ⊆W = V. By (6),
we see that W \ Z = V \ (C0(V) × {0}) = V and, hence, W = V is the Zariski
closure of W \ Z. Then the proposition implies that any point in Z = C0(V) × {0}
is a limit of points in W \ Z = V.
We can now ﬁnish the proof of Theorem 6. Suppose a line L parametrized by tv is
contained in C0(V). Then v ∈C0(V), which implies that (v, 0) ∈C0(V) × {0}. By
the above paragraph, we can ﬁnd points (vi, ti) ∈V which converge to (v, 0). If we
let Li be the line parametrized by tvi, then vi →v shows that Li →L. Furthermore,
since qi = tivi ∈V and ti ̸= 0, we see that Li is the secant line determined by qi ∈V.

§7 The Tangent Cone
533
Finally, as k →∞, we have qi = ti · vi →0 · v = 0, which shows that L is a limit
of secant lines of points qi ∈V converging to 0. This completes the proof of the
theorem.
□
If we are working over an inﬁnite ﬁeld k, we may not be able to deﬁne what it
means for secant lines to converge to a line in the tangent cone. So it is not clear
what the analogue of Theorem 6 should be. But if p = 0 is in V over k, we can
still form the set V as in (5), and every secant line still gives a point (v, t) ∈V with
t ̸= 0. A purely algebraic way to discuss limits of secant lines as t →0 would be
to take the smallest variety containing V and see what happens when t = 0. This
means looking at V ∩(kn × {0}), which by (6) is exactly C0(V) × {0}. You should
check that the proof of (6) is valid over k, so that the decomposition
V = V ∪(C0(V) × {0})
can be regarded as the extension of Theorem 6 to the inﬁnite ﬁeld k. In Exercise 10,
we will explore some other interesting aspects of the variety V.
Another way in which the tangent cone approximates the variety is in terms of
dimension. Recall from §6 that dimpV is the maximum dimension of an irreducible
component of V containing p.
Theorem 8. Let p be a point on an afﬁne variety V ⊆kn. Then dimpV = dim Cp(V).
Proof. This is a standard result in advanced courses in commutative algebra [see,
for example, Theorem 13.9 in MATSUMURA (1989)]. As in §6, we will only prove
this for the case of a hypersurface in Cn. If V = V( f), we know that Cp(V) =
V( fp,min) by Exercise 2. Thus, both V and Cp(V) are hypersurfaces, and, hence,
both have dimension n −1 at all points. This shows that dimpV = dim Cp(V).
□
This is a nice result because it enables us to compute dimpV without having to
decompose V into its irreducible components.
The ﬁnal topic of this section will be the relation between the tangent cone and
the tangent space. In the exercises, you will show that for any point p of a variety V,
we have
Cp(V) ⊆Tp(V).
In terms of dimensions, this implies that
dim Cp(V) ≤dim Tp(V).
Then the following corollary of Theorem 8 tells us when these coincide.
Corollary 9. Assume that k is algebraically closed and let p be a point of a variety
V ⊆kn. Then the following are equivalent:
(i) p is a nonsingular point of V.
(ii) dim Cp(V) = dim Tp(V).
(iii) Cp(V) = Tp(V).

534
Chapter 9 The Dimension of a Variety
Proof. Since dim Cp(V) = dimpV by Theorem 8, the equivalence of (i) and (ii) is
immediate from the deﬁnition of a nonsingular point. The implication (iii) ⇒(ii) is
trivial, so that it remains to prove (ii) ⇒(iii).
Since k is algebraically closed, we know that k is inﬁnite, which implies that
the linear space Tp(V) is an irreducible variety in kn. [When Tp(V) is a coordinate
subspace, this follows from Exercise 7 of §1. See Exercise 12 below for the general
case.] Thus, if Cp(V) has the same dimension Tp(V), the equality Cp(V) = Tp(V)
follows immediately from the afﬁne version of Proposition 10 of §4 (see Exercise 18
of §4).
□
If we combine Theorem 6 and Corollary 9, it follows that at a nonsingular point
p of a variety V ⊆Cn, the tangent space at p is the union of all limits of secant
lines determined by sequences of points in V converging to p. This is a powerful
generalization of the idea from elementary calculus that the tangent line to a curve
is a limit of secant lines.
EXERCISES FOR §7
1. Suppose that k is a ﬁeld of characteristic 0. Given p ∈kn and f ∈k[x1, . . . , xn], we know
that f can be written in the form f = 
α cα(x −p)α, where cα ∈k and (x −p)α is as
in the text. Given α, deﬁne
∂α
∂xα = ∂α1
∂xα1
1
· · · ∂αn
∂xαn
n ,
where
∂αi
∂αi xi means differentiation αi times with respect to xi. Finally, set
α! = α1! · α2! · · · αn!.
a. Show that
∂α(x −p)β
∂xα
(p) =
$
α!
if α = β
0
otherwise.
Hint: There are two cases to consider: when βi < αi for some i, and when βi ≥αi
for all i.
b. If f = 
α cα(x −p)α, then show that
cα = 1
α!
∂αf
∂xα (p),
and conclude that
f =

α
1
α!
∂αf
∂xα (p)(x −p)α.
This is Taylor's formula for f at p. Hint: Be sure to explain where you use the char-
acteristic 0 assumption.
c. Write out the formula of part (b) explicitly when f ∈k[x, y] has total degree 3.
d. What formula do we get for fp,j in terms of the partial derivatives of f ?
e. Give an example to show that over a ﬁnite ﬁeld, it may be impossible to express f in
terms of its partial derivatives. Hint: See Exercise 10 of §6.
2. Let V ⊆kn be a hypersurface.
a. If I(V) = ⟨f ⟩, prove that Cp(V) = V( fp,min).

§7 The Tangent Cone
535
b. If k is algebraically closed and V = V( f ), prove that the conclusion of part (a) is still
true. Hint: See the proof of part (ii) of Proposition 4.
3. In this exercise, we will show that the ideal I = ⟨xy, xz + z(y2 −z2)⟩⊆k[x, y, z] is a
radical ideal when k has characteristic 0.
a. Show that
⟨x, z(y2 −z2)⟩= ⟨x, z⟩∩⟨x, y −z⟩∩⟨x, y + z⟩.
Furthermore, show that the three ideals on the right-hand side of the equation are
prime. Hint: Work in k[x, y, z]/⟨x⟩∼= k[y, z] and use the fact that k[y, z] has unique
factorization. Also explain why this result fails if k is the ﬁeld F2 consisting of two
elements.
b. Show that
⟨y, xz −z3⟩= ⟨y, z⟩∩⟨y, x −z2⟩,
and show that the two ideals on the right-hand side of the equation are prime.
c. Prove that I = ⟨x, z(y2 −z2)⟩∩⟨y, xz −z3⟩. Hint: One way is to use the ideal inter-
section algorithm from Chapter 4, §3. There is also an elementary argument.
d. By parts (a), (b) and (c) we see that I is an intersection of ﬁve prime ideals. Show that
I is a radical ideal. Also, use this decomposition of I to describe V = V(I) ⊆k3.
e. If k is algebraically closed, what is I(V)?
4. This exercise is concerned with the proof of Proposition 4. Fix a monomial order > on
k[x0, . . . , xn] with the properties described in the statement of the proposition.
a. If g ∈k[x1, . . . , xn] is the dehomogenization of a homogeneous polynomial G ∈
k[x0, . . . , xn], prove that LT(G) = LT(gmin)xb
0 for some b.
b. If G1, . . . , Gs is a basis of Ih, prove that the dehomogenizations g1, . . . , gs form a
basis of I. In Exercise 5, you will show that if the Gi's are a Gröbner basis for >, the
gi's may fail to be a Gröbner basis for I with respect to the induced monomial order
on k[x1, . . . , xn].
c. If f , g ∈k[x1, . . . , xn], show that ( f · g)min = fmin · gmin and ( f m)min = ( fmin)m.
5. We will continue our study of the variety V = V(xy, xz + z(y2 −z2)) begun in the text.
a. If we use grlex with w > x > y > z, show that a Gröbner basis for Ih ⊆k[w, x, y, z]
is {xy, xzw + z(y2 −z2), yz(y2 −z2)}.
b. If we dehomogenize the Gröbner basis of part (a), we get a basis of I. Show that this
basis is not a Gröbner basis of I for grlex with x > y > z.
c. Use Proposition 4 to show that the tangent cone C0(V) is a union of ﬁve lines through
the origin in k3 and compare your answer to part (e) of Exercise 3.
6. Compute the dimensions of the tangent cone and the tangent space at the origin of the
varieties deﬁned by the following ideals:
a. ⟨xz, xy⟩⊆k[x, y, z].
b. ⟨x −y2, x −z3⟩⊆k[x, y, z].
7. In §3 of Chapter 3, we used elimination theory to show that the tangent surface of the
twisted cubic V(y −x2, z −x3) ⊆R3 is deﬁned by the equation
x3z −(3/4)x2y2 −(3/2)xyz + y3 + (1/4)z2 = 0.
a. Show that the singular locus of the tangent surface S is exactly the twisted cubic. Hint:
Two different ideals may deﬁne the same variety. For an example of how to deal with
this, see equation (14) in Chapter 3, §4.
b. Compute the tangent space and tangent cone of the surface S at the origin.
8. Suppose that in Cn we have two sequences of vectors vi and tivi, where ti ∈C, such that
vi →v ̸= 0 and tivi →0. We claim that ti →0 in C. To prove this, deﬁne the length
of a complex number z = x + y
√
−1 to be |z| =
!
x2 + y2 and deﬁne the length of
v = (z1, . . . , zn) ∈Cn to be |v| =
!
|z1|2 + · · · + |zn|2. Recall that vi →v means that
for every ϵ > 0, there is N such that |vi −v| < ϵ for k ≥N.

536
Chapter 9 The Dimension of a Variety
a. If we write v = (z1, . . . , zn) and vi = (zi1, . . . , zin), then show that vi →v implies
zij →zj for all j. Hint: Observe that |zj| ≤|v|.
b. Pick a nonzero component zj of v. Show that zij →zj ̸= 0 and tizij →0. Then divide
by zj and conclude that ti →0.
9. Theorem (2.33) of MUMFORD (1981) states that if W ⊂Pn(C) is an irreducible projec-
tive variety and Z ⊆W is a projective variety not equal to W, then any point in Z is a
limit of points in W \ Z. Our goal is to apply this to prove Proposition 7.
a. Let Z ⊆W ⊆Cn be afﬁne varieties such that W is the Zariski closure of W \Z. Show
that Z contains no irreducible component of W.
b. Show that it sufﬁces to prove Proposition 7 in the case when W is irreducible. Hint: If
p lies in Z, then it lies in some component W1 of W. What does part (a) tell you about
W1 ∩Z ⊆W1?
c. Let Z ⊆W ⊆Cn, where W is irreducible and Z ̸= W, and let Z and W be their
projective closures in Pn(C). Show that the irreducible case of Proposition 7 follows
from Mumford's Theorem (2.33). Hint: Use Z ∪(W \ W) ⊆W.
d. Show that the converse of the proposition is true in the following sense. Let p ∈Cn.
If p /∈V \ W and p is a limit of points in V \ W, then show that p ∈W. Hint: Show
that p ∈V and recall that polynomials are continuous.
10. Let V ⊆kn be a variety containing the origin and let V ⊆kn+1 be the set described
in (5). Given λ ∈k, consider the "slice" (kn × {λ}) ∩V. Assume that k is inﬁnite.
a. When λ ̸= 0, show that this slice equals Vλ × {λ}, where Vλ = {v ∈kn | λv ∈V}.
Also show that Vλ is an afﬁne variety.
b. Show that V1 = V, and, more generally, for λ ̸= 0, show that Vλ is isomorphic to V.
Hint: Consider the polynomial map deﬁned by sending (x1, . . . , xn) to (λx1, . . . , λxn).
c. Suppose that k = R or C and that λ ̸= 0 is close to the origin. Explain why Vλ gives
a picture of V where we have expanded the scale by a factor of 1/λ. Conclude that as
λ →0, Vλ shows what V looks like as we "zoom in" at the origin.
d. Use (6) to show that V0 = C0(V). Explain what this means in terms of the "zooming
in" described in part (c).
11. If p ∈V ⊆kn, show that Cp(V) ⊆Tp(V).
12. If k is an inﬁnite ﬁeld and V ⊆kn is a subspace (in the sense of linear algebra), then
prove that V is irreducible. Hint: In Exercise 7 of §1, you showed that this was true when
V was a coordinate subspace. Now pick an appropriate basis of kn.
13. Let W ⊂Pn−1(C) be a nonempty projective variety and let CW ⊆Cn be its afﬁne cone.
a. Prove that the tangent cone of CW at the origin is CW.
b. Prove that the origin is a smooth point of CW if and only if W is a projective linear
subspace of Pn−1(C). Hint: Use Corollary 9.
In Exercises 14-17, we will study the "blow-up" of a variety V at a point p ∈V. The blowing-
up process gives us a map of varieties π : BlpV →V such that away from p, the two varieties
look the same, but at p, BlpV can be much larger than V, depending on what the tangent cone
Cp(V) looks like.
14. Let k be an arbitrary ﬁeld. In §5 of Chapter 8, we studied varieties in Pn−1 × kn, where
Pn−1 = Pn−1(k). Let y1, . . . , yn be homogeneous coordinates in Pn−1 and let x1, . . . , xn
be coordinates in kn. Then the (y1, . . . , yn)-homogeneous polynomials xiyj −xjyi (this is
the terminology of Chapter 8, §5) deﬁne a variety Γ ⊆Pn−1 × kn. This variety has some
interesting properties.
a. Fix (p, q) ∈Pn−1×kn. Picking homogeneous coordinates of p gives a nonzero vector
ˆp ∈kn \ {0}. Show that (p, q) ∈Γ if and only if q = tˆp for some t ∈k (which might
be zero).
b. If q ̸= 0 is in kn, show that (Pn−1 × {q}) ∩Γ consists of a single point (p, q) where q
gives homogeneous coordinates of p ∈Pn−1. On the other hand, when q = 0, show
that (Pn−1 × {0}) ∩Γ = Pn−1 × {0}.

§7 The Tangent Cone
537
c. Let π : Γ →kn be the projection map. Show that π−1(q) consists of a single point,
except when q = 0, in which case π−1(0) is a copy of Pn−1. This shows that we can
regard Γ as the variety obtained by removing the origin from kn and replacing it by a
copy of Pn−1.
d. To see what the Pn−1 × {0} ⊆Γ means, consider a line L through the origin
parametrized by tv, where v ∈kn \ {0}. Although there are many choices for v,
they all give the same point w ∈Pn−1. Show that the points (w, tv) ∈Pn−1 ×kn lie in
Γ and, hence, describe a curve L ⊆Γ. Investigate where this curve meets Pn−1 × {0}
and conclude that distinct lines through the origin in kn give distinct points in π−1(0).
Thus, the difference between Γ and kn is that Γ separates tangent directions at the
origin. We call π : Γ →kn the blow-up of kn at the origin.
15. This exercise is a continuation of Exercise 14. Let V ⊆kn be a variety containing the
origin and assume that the origin is not an irreducible component of V. Our goal here
is to deﬁne the blow-up of V at the origin. Let Γ ⊆Pn−1 × kn be as in the previous
exercise. Then blow-up of V at 0, denoted Bl0V, is deﬁned to be the Zariski closure of
(Pn−1 × (V \ {0})) ∩Γ in Pn−1 × kn.
a. Prove that Bl0V ⊆Γ and Bl0kn = Γ.
b. If π : Γ →kn is as in Exercise 14, then prove that π(Bl0V) ⊆V, so π : Bl0V →V.
Hint: First show that BL0V ⊆Pn−1 × V.
c. Use Exercise 14 to show that π−1(q) consists of a single point for q ̸= 0 in V.
In Exercise 16, you will describe π−1(0) in terms of the tangent cone of V at the origin.
16. Let V ⊆kn be a variety containing the origin and assume that the origin is not an
irreducible component of V. We know that tangent cone C0(V) is the afﬁne cone CW
over some projective variety W ⊂Pn−1. We call W the projectivized tangent cone of V
at 0. The goal of this exercise is to show that if π : Bl0V →V is the blow-up of V at 0
as deﬁned in Exercise 15, then π−1(0) = W × {0}. Assume that k is inﬁnite.
a. Show that our assumption that {0} is not an irreducible component of V implies that
V is the Zariski closure of V \ {0}.
b. Show that g ∈k[y1, . . . , yn, x1, . . . , xn] lies in I(Bl0V) if and only if g(q, tq) = 0 for
all q ∈V \ {0} and all t ∈k \ {0}. Hint: Use part (a) of Exercise 14.
c. Then show that g ∈I(Bl0V) if and only if g(q, tq) = 0 for all q ∈V and all t ∈k.
Hint: Use parts (a) and (b).
d. Explain why I(Bl0V) is generated by (y1, . . . , yn)-homogeneous polynomials.
e. Assume that g = 
α gα(y1, . . . , yn)xα ∈I(Bl0V). By part (d), we may assume that
the gα are all homogeneous of the same total degree d. Let
f (x1, . . . , xn) =

α
gα(x1, . . . , xn)xα.
Prove that f ∈I(V). Hint: Show that g(x1, . . . , xn, tx1, . . . , txn) = f (x1, . . . , xn)td,
and then use part (c).
f. Prove that W ×{0} ⊆Bl0V ∩(Pn−1 ×{0}). Hint: It sufﬁces to show that g(v, 0) = 0
for g ∈I(Bl0V) and v ∈C0(V). In the notation of part (e) note that g(v, 0) = g0(v).
If g0 ̸= 0, show that g0 = fmin, where f is the polynomial deﬁned in part (e).
g. Prove that Bl0V ∩(Pn−1 × {0}) ⊆W × {0}. Hint: Recall that W is the projective
variety deﬁned by the polynomials fmin for f ∈I(V). Write fℓ= fmin and let g be
the remainder of tℓf on division by tx1 −y1, . . . , txn −yn for a monomial order with
LT(txi −yi) = txi for all i. Show that t does not appear in g and that g0 = fℓwhen we
write g = 
α gα(y1, . . . , yn)xα. Then use part (c) to prove that g ∈I(Bl0V). Now
complete the proof using fℓ(v) = g0(v) = g(v, 0).

538
Chapter 9 The Dimension of a Variety
A line in the tangent cone can be regarded as a way of approaching the origin through
points of V. So we can think of the projectivized tangent cone W as describing all possi-
ble ways of approaching the origin within V. Then π−1(0) = W × {0} means that each
of these different ways gives a distinct point in the blow-up. Note how this generalizes
Exercise 14.
17. Assume that k is an algebraically closed ﬁeld and suppose that V = V( f1, . . . , fs) ⊆kn
contains the origin.
a. By analyzing what you did in part (g) of Exercise 16, explain how to ﬁnd deﬁning
equations for the blow-up Bl0V.
b. Compute the blow-up at the origin of V(y2 −x2 −x3) and describe how your answer
relates to the ﬁrst picture in Example 1.
c. Compute the blow-up at the origin of V(y2 −x3).
Note that in parts (b) and (c), the blow-up is a smooth curve. In general, blowing-up is
an important tool in what is called desingularizing a variety with singular points.

Chapter 10
Additional Gröbner Basis Algorithms
In §10 of Chapter 2 we discussed some criteria designed to identify situations where
it is possible to see in advance that an S-polynomial remainder will be zero in Buch-
berger's algorithm. Those unnecessary S-polynomial remainder calculations are in
fact the main computational bottleneck for the basic form of the algorithm. Finding
ways to avoid them, or alternatively to replace them with less expensive compu-
tations, is the key to improving the efﬁciency of Gröbner basis calculation. The
algorithms we discuss in this chapter apply several different approaches to achieve
greater efﬁciency. Some of them use Gröbner bases of homogeneous ideals or ideas
inspired by the special properties of Gröbner bases in that case. So we begin in
§1 by showing that the computation of a homogeneous Gröbner basis can be orga-
nized to proceed degree by degree. This gives the framework for Traverso's Hilbert
driven Buchberger algorithm, discussed in §2, which uses the Hilbert function of
a homogeneous ideal to control the computation and bypass many unnecessary
S-polynomial remainder calculations. We also show in §1 that the information gen-
erated by several S-polynomial remainder computations can be obtained simultane-
ously via row operations on a suitable matrix. This connection with linear algebra
is the basis for Faugère's F4 algorithm presented in §3. Finally, we introduce the
main ideas behind signature-based Gröbner basis algorithms, including Faugère's
F5 algorithm, in §4.
§1
Preliminaries
From now on in this chapter, when we refer to Buchberger's algorithm, we will
mean a version of the algorithm similar to the one from Theorem 9 of Chapter 2,
§10. Readers may wish to review that section before reading farther. In particular,
the algorithm maintains a list B of pairs of indices (i, j) in the current partial Gröb-
ner basis G = ( f1, . . . , ft) for which the S-polynomials S( fi, fj) remain to be con-
sidered. Moreover, the algorithm only inserts additional polynomials into the initial
list G and updates the set of pairs accordingly. Finding the corresponding reduced
© Springer International Publishing Switzerland 2015
D.A. Cox et al., Ideals, Varieties, and Algorithms, Undergraduate Texts
in Mathematics, DOI 10.1007/978-3-319-16721-3_10
539

540
Chapter 10 Additional Gröbner Basis Algorithms
Gröbner basis will be treated as a separate operation. Several criteria for bypassing
unnecessary S-polynomial remainders that are more powerful than those considered
in Chapter 2 will be introduced in later sections.
Homogeneous Gröbner Bases
There are a number of special features of the computation of Gröbner bases of ho-
mogeneous ideals that present opportunities for simpliﬁcations and shortcuts. In-
deed, to take advantage of these shortcuts some of the ﬁrst-generation computer
algebra systems for these computations, including the original Macaulay program
developed by Bayer and Stillman, accepted only homogeneous inputs.
We studied homogeneous polynomials and ideals in Chapter 8 where Theorem 2
of §3 established the equivalence of these statements:
• I ⊆k[x1, . . . , xn] is a homogeneous ideal.
• I = ⟨f1, . . . , fs⟩, where all the fi are homogeneous polynomials.
• The unique reduced Gröbner basis of I with respect to any monomial ordering
consists of homogeneous polynomials.
In this chapter, the degree of a nonzero polynomial f, denoted deg( f), will always
refer to its total degree. We write k[x1, . . . , xn]m (respectively k[x1, . . . , xn]≤m) for
the vector space of polynomials of degree m (respectively ≤m) in k[x1, . . . , xn].
By deﬁnition the zero polynomial is included in both vector spaces as the additive
identity element.
From Exercise 3 of Chapter 8, §3, we know that in applying Buchberger's algo-
rithm to compute a Gröbner basis from homogeneous input polynomials,
• all the nonzero S-polynomials generated by the algorithm are homogeneous, and
• all nonzero remainders on division of S-polynomials by the current partial Gröb-
ner basis are homogeneous.
We will next study how homogeneity can be used to organize the computations
involved in Buchberger's algorithm. First, there is a natural notion of the degree of
a pair (i, j) once we ﬁx a monomial order on k[x1, . . . , xn].
Deﬁnition 1. Let G = ( f1, . . . , ft) be an ordered list of homogeneous polynomials.
The degree of the pair (i, j) relative to the list G is deg(lcm(LM( fi), LM( fj))).
For instance, if we use grevlex order with x > y > z on Q[x, y, z], and
f1 = x2 −2y2 −2yz −z2,
f2 = −xy + 2yz + z2,
f3 = −x2 + xy + xz + z2,
then the pair (1, 2) has degree 3 since lcm(LM( f1), LM( f2)) = x2y. However, the
pair (1, 3) has degree 2 since lcm(LM( f1), LM( f3)) = x2.
Now if the fi are homogeneous, it is easy to see that the degree of (i, j) coincides
with the degree of S( fi, fj) and with the degree of the remainder S( fi, fj)
G if the

§1 Preliminaries
541
S-polynomial and the remainder are nonzero (see Exercise 1). Next, in the course
of Buchberger's algorithm, each time a nonzero S-polynomial remainder is found,
recall that new pairs are created.
Proposition 2. In Buchberger's algorithm with homogeneous input polynomials,
when a new polynomial ft is included in the partial Gröbner basis G, all the pairs
(i, t) with i < t have degree strictly greater than deg( ft).
Proof. Say deg( ft) is equal to m. Since ft is homogeneous, this is also the de-
gree of LM( ft). Consider a pair (i, t) with i < t. Then the degree of (i, t) is
deg(lcm(LM( fi), LM( ft))) which is clearly greater than or equal to m. To establish
the claim, suppose the pair (i, t) has degree equal to m. Since LM( ft) has degree m,
this implies lcm(LM( fi), LM( ft)) = LM( ft) and hence LM( fi) divides LM( ft). But
this is a contradiction since ft is a remainder on division by a list of polynomials
containing fi. Hence the degree of (i, t) must be strictly greater than m.
□
Our next proposition shows that if pairs of lower degree are always processed
before pairs of higher degree (this would hold, for instance, if a graded monomial
ordering and the normal selection strategy discussed in Chapter 2, §10 is used),
then the degrees of elements of a homogeneous Gröbner basis are very orderly and
predictable. In particular, when all pairs of degree ≤m have been processed, all ele-
ments of a Gröbner basis in degrees ≤m have been found—lower-degree elements
never appear later, as they easily can with nonhomogeneous inputs.
Proposition 3. Assume that the input F = ( f1, . . . , fs) in Buchberger's algorithm
consists of homogeneous polynomials and the algorithm is terminated at a time
when B contains no pairs of degree ≤m for some m ∈Z≥0. Let Gm be the set of
elements in G of degree ≤m at that time, i.e., Gm = G ∩k[x1, . . . , xn]≤m. Then:
(i) There is a Gröbner basis G for I = ⟨f1, . . . , fs⟩such that the set of elements of
degree ≤m in G coincides with Gm, i.e., G ∩k[x1, . . . , xn]≤m = Gm.
(ii) If f ∈I is homogeneous of degree ≤m, then LT( f) is divisible by LT(g) for
some g ∈Gm.
Proof. To prove part (i), let G be the Gröbner basis of I obtained by letting the
algorithm run to completion. Then Gm ⊆G since the algorithm only inserts new
polynomials into G. Moreover, if f ∈G \ Gm, then f is either an input polynomial
of degree > m or a remainder on division of an S-polynomial for a pair of degree
> m. So by the remark following Deﬁnition 1 above in the second case, deg( f) > m.
It follows that G ∩k[x1, . . . , xn]≤m = Gm as claimed. Part (ii) follows since LT( f)
is divisible by LT(g) for some g ∈G by the deﬁnition of a Gröbner basis. But then
g ∈Gm must hold since deg( f) ≤m.
□
A set of polynomials in I that satisﬁes the property given for Gm in part (ii)
of Proposition 3 is called a Gröbner basis of I up to degree m. You will show in
Exercise 9 that an equivalent statement is:
(1)
S( fi, fj)
G = 0 for all pairs (i, j) of degree ≤m.

542
Chapter 10 Additional Gröbner Basis Algorithms
To summarize, here is a version of Buchberger's algorithm tailored for the
homogeneous case.
Theorem 4. Let f1, . . . , fs be homogeneous and let I = ⟨f1, . . . , fs⟩.
(i) The algorithm below terminates and correctly computes a Gröbner basis for I.
(ii) The values of m in successive passes through the outer WHILE loop are strictly
increasing.
(iii) When the pass through the outer WHILE loop with a given m is complete, Gm =
G ∩k[x1, . . . , xn]≤m is a Gröbner basis for I up to degree m.
Input : F = ( f1, . . . , fs) with all fi homogeneous
Output : G, a Gröbner basis for I
B := {(i, j) | 1 ≤i < j ≤s}
G := F
l := s
WHILE B ̸= ∅DO
m := minimal degree of pairs remaining in B
B′ := {(i, j) ∈B | deg(i, j) = m}
B := B \ B′
WHILE B′ ̸= ∅DO
(i, j) := ﬁrst element in B′
B′ := B′ \ {(i, j)}
S := S( fi, fj)
G
IF S ̸= 0 THEN
l := l + 1; fl := S
G := G ∪{fl}
B := B ∪{(i, l) | 1 ≤i ≤l −1}
RETURN G
Proof. Part (i) follows from the termination and correctness proofs for the usual
Buchberger algorithm. You will complete the details in Exercise 10. When the pass
through the outer WHILE loop with a given m is complete, all pairs of degree m
in B at the start of that pass have been removed and processed. By Proposition 2
no new pairs of degree ≤m have been added. So B contains only pairs of degrees
strictly larger than m. This proves the claim in part (ii). Part (iii) then follows from
Proposition 3.
□
In what follows, we will refer to this as the degree by degree version of Buch-
berger's algorithm. We are not claiming that this, by itself, is any improvement over
the algorithms studied in Chapter 2. Indeed, this version merely singles out one

§1 Preliminaries
543
particular way of choosing the next pairs to be processed at each stage and exploits
the good consequences when the input polynomials are homogeneous. However,
it does give a framework for developing improved algorithms. Note that in some
cases all elements of the set Gm in part (iii) of the theorem might be found before
all the pairs of degree m in B′ have been processed. If that happened, no further S-
polynomial remainders for pairs in degree m would be necessary and the inner loop
could be terminated early. In the process, some S-polynomial remainder calcula-
tions could be bypassed. This is an opportunity for exactly the sort of improvement
in efﬁciency we mentioned in the introduction to this chapter. The Hilbert driven
algorithm to be presented in §2 uses the Hilbert function of the ideal I to recognize
when this happens.
Homogeneous Gröbner bases and algorithms for computing them are treated in
greater generality and in more detail in BECKER and WEISPFENNING (1993) and
KREUZER and ROBBIANO (2005).
Homogenization and Dehomogenization
Since many applications involve nonhomogeneous ideals, if we want to use facts
about homogeneous Gröbner basis algorithms, we need a way to relate the two
cases. We will use the following notation from Proposition 7 of Chapter 8, §2. If
f(x1, . . . , xn) is a polynomial of degree m and x0 is a new homogenizing variable,
then the homogenization of f is
f h(x0, x1, . . . , xn) = xm
0 f
x1
x0
, . . . , xn
x0

.
Similarly, if F = ( f1, . . . , fs) is an ordered list of polynomials, then we will write
Fh = ( f h
1 , . . . , f h
s ). Going the other way, if g = g(x0, x1, . . . , xn) is a homogeneous
polynomial, then its dehomogenization is
gd(x1, . . . , xn) = g(1, x1, . . . , xn).
Similarly, if G = (g1, . . . , gt) is an ordered list of homogeneous polynomials, then
Gd = (gd
1, . . . , gd
t ). In this chapter, the superscripts h and d always refer to ho-
mogenization and dehomogenization; they are never exponents. By Proposition 7 of
Chapter 8, §2, ( f h)d = f for all f in k[x1, . . . , xn]. It is not necessarily the case that
(gd)h = g, though, since (gd)h could differ from g by a power of the homogenizing
variable x0.
We next discuss the behavior of Gröbner bases under homogenization. Part of
the following has appeared in Chapter 8.
• Let I be a nonhomogeneousideal in k[x1, . . . , xn] and let > be a graded monomial
order. In this case, the relation between a Gröbner basis G for I and a particular
Gröbner basis for the homogeneous ideal Ih = ⟨f h | f ∈I⟩is pleasantly simple.
From Theorem 4 of Chapter 8, §4, Gh is a Gröbner basis for Ih with respect to a
product order >h. If xα and xβ are monomials in x1, . . . , xn, then

544
Chapter 10 Additional Gröbner Basis Algorithms
(2)
xαxa
0 >h xβxb
0 ⇔xα > xβ, or xα = xβ and a > b.
• Let I = ⟨f1, . . . , fs⟩, where the fi are an arbitrary ideal basis and consider J =
⟨f h
1 , . . . , f h
s ⟩. This ideal can be strictly smaller than Ih, as in Example 3 from
Chapter 8, §4. Moreover, the corresponding projective variety, V(J) ⊆Pn−1, can
be strictly larger than V(Ih). In particular, V(J) can have additional irreducible
components in the hyperplane at inﬁnity V(x0) not contained in V(Ih).
Even though the ideal J = ⟨f h
1 , . . . , f h
s ⟩can differ from Ih, it is a homogeneous
ideal. So we could apply any improved Gröbner basis algorithm tailored for the
homogeneous case to these polynomials. Moreover, the order > used to deﬁne the
product order >h can be chosen arbitrarily. But there is a remaining question: How
does a Gröbner basis for J relate to Gröbner bases for I? The following statement is
a new observation.
Theorem 5. Let G be the reduced Gröbner basis for J = ⟨f h
1 , . . . , f h
s ⟩with respect
to the monomial order >h from (2). Then Gd is a Gröbner basis for I = ⟨f1, . . . , fs⟩
with respect to the order >.
Proof. We write G = {g1, . . . , gt}. The proof is a variation on the proof of Theo-
rem 4 of Chapter 8, §4. We claim ﬁrst that each of the dehomogenized polynomials
gd
j is in I, so the ideal they generate is contained in I. In "fancy" terms, the underly-
ing reason is that setting x0 = 1 deﬁnes a ring homomorphism from k[x0, x1, . . . , xn]
to k[x1, . . . , xn], as you will see in Exercise 2. The gj are in the ideal generated by
the f h
i , so for each j we have an equation
gj(x0, x1, . . . , xn) =
s

i=1
Bi(x0, . . . , xn) f h
i (x0, x1, . . . xn)
for some polynomials Bi. Setting x0 = 1 and using the homomorphism property to
get the second line below, we have
gd
j (x1, . . . , xn) = gj(1, x1, . . . , xn)
=
s

i=1
Bi(1, x1, . . . , xn) f h
i (1, x1, . . . , xn)
=
s

i=1
Bi(1, x1, . . . , xn) fi(x1, . . . , xn),
since ( f h)d = f for all f ∈k[x1, . . . , xn]. This establishes the claim.
Next, we show that the opposite inclusion also holds. Since G is a Gröbner basis
for the ideal J, for each i, 1 ≤i ≤s, we have
f h
i (x0, x1, . . . , xn) =
t

j=1
Aj(x0, x1, . . . , xn)gj(x0, x1, . . . xn)

§1 Preliminaries
545
for some polynomials Aj. So setting x0 = 1 again we have
fi(x1, . . . , xn) = ( f h
i (x0, x1 . . . , xn))d
=
t

j=1
Aj(1, x1, . . . , xn)gj(1, x1, . . . , xn)
=
t

j=1
Aj(1, x1, . . . , xn)gd
j (x1, . . . , xn).
Together with the result from the ﬁrst paragraph, this shows Gd is a basis for I.
It remains to show that Gd is a Gröbner basis. For this we need ﬁrst to understand
where the leading monomials of the gd
j come from. The gj are homogeneous since
they are elements of the reduced Gröbner basis for the homogeneous ideal J with
respect to >h. So let LT>h(gj) = xαxa
0 and consider any other monomial xβxb
0 in gj.
By deﬁnition, xαxa
0 >h xβxb
0. However the case α = β and a > b from (2) never
occurs here since gj is homogeneous: |α| + a = |β| + b, so if α = β, then a = b as
well. In other words, we have a result parallel to (2) from Chapter 8, §4:
(3)
LM>h(gj) = xa
0 LM>(gd
j ).
Now, since G is a Gröbner basis for J, by one implication in Theorem 3 of
Chapter 2, §9,
S(gi, gj) →G 0
for all i ̸= j. Because of (3) you will show in detail in Exercise 3 that after deho-
mogenization we have
S(gd
i , gd
j ) →Gd 0.
This shows that Gd is a Gröbner basis, using the other implication in Theorem 3 of
Chapter 2, §9 and the proof is complete.
□
Hence Gröbner basis algorithms designed for the homogeneous case can be
applied to nonhomogeneous ideals in k[x1, . . . , xn] as well by homogenizing the
original polynomials, applying the algorithm to the homogenized forms and then
dehomogenizing. We should mention that even though G is a reduced Gröbner basis
for J the dehomogenized basis Gd can fail to be reduced. Here is a simple example.
Example 6. Let I = ⟨x2 + y2 −1, x + y2 −2⟩in Q[x, y], using lex order with x > y.
If we homogenize with a new variable z (rather than x0 as in the general notation),
we obtain J = ⟨x2 + y2 −z2, xz + y2 −2z2⟩. Theorem 5 is stated using the order >h
deﬁned in (2). But note that >h is the same as lex order with x > y > z in this case,
so LM( f h
2 ) = xz contains the homogenizing variable. Computing a Gröbner basis
for J we ﬁnd
G = {y4 −3y2z2 + 3z4, xz + y2 −2z2, xy2 + y2z −3z3, x2 + y2 −z2},
(where the leading term in each polynomial is listed ﬁrst). Thus

546
Chapter 10 Additional Gröbner Basis Algorithms
Gd = {y4 −3y2 + 3, x + y2 −2, xy2 + y2 −3, x2 + y2 −1}.
We can see immediately that this is not a reduced Gröbner basis since the leading
terms of the third and fourth polynomials are divisible by LT(x+y2−2) = x. Indeed,
the unique reduced lex Gröbner basis for I is {y4 −3y2 + 3, x + y2 −2}.
As in this example, further work may be necessary to produce a reduced Gröb-
ner basis for I if we proceed by homogenizing, applying an algorithm tailored for
homogeneous polynomials, and then dehomogenizing.
Gröbner Basis Algorithms and Linear Algebra
We will see next that S-polynomial remainder computations can, in a sense, be re-
placed by computations that produce the same information in a different way. The
new idea involved is to make a translation from polynomial algebra into linear al-
gebra [see LAZARD (1983) for a fuller exposition of the connections]. This is espe-
cially clear in the homogeneous case so we will again assume I is a homogeneous
ideal. As usual, we write Im = I ∩k[x1, . . . , xn]m for the vector space of homoge-
neous polynomials of degree m in I (together with the zero polynomial). We record
the following easy property of homogeneous ideals.
Lemma 7. Let I = ⟨f1, . . . , fs⟩where the fi are homogeneous. Let m ∈Z≥0. Then
every element of Im is a linear combination with coefﬁcients in k of the polynomials
xαfi where |α| + deg( fi) = m.
Proof. The proof is left to the reader as Exercise 5.
□
Let Sm be the set of all pairs (α, i) where α ∈Zn
≥0 and |α| + deg( fi) = m,
listed in any particular order. We ﬁx a particular monomial ordering and let Tm be
the set of monomials xβ with |β| = m, listed in decreasing order. Construct an
|Sm| × |Tm| matrix Mm with entries in k whose rows are the vectors of coefﬁcients
of the polynomials xαfi. If
xαfi =

β
cβxβ,
then the entries on the row of the matrix Mm for (α, i) are the cβ.
Example 8. In Q[x, y, z], use grevlex order with x > y > z and let I be generated by
the homogeneous polynomials
f1 = x2 −2y2 −2yz −z2,
f2 = −xy + 2yz + z2,
f3 = −x2 + xy + xz + z2.
For this ordering x2 > xy > y2 > xz > yz > z2 so the matrix M2 is:

§1 Preliminaries
547
(4)
M2 =
⎛
⎝
1
0 −2 0 −2 −1
0 −1
0
0
2
1
−1
1
0
1
0
1
⎞
⎠.
Similarly, the monomials of degree 3 are ordered
x3 > x2y > xy2 > y3 > x2z > xyz > y2z > xz2 > yz2 > z3.
Ordering the rows following the list x f1, y f1, z f1, x f2, y f2, z f2, x f3, y f3, z f3, we obtain
(5)
M3 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
0 −2
0
0 −2
0 −1
0
0
0
1
0 −2
0
0 −2
0 −1
0
0
0
0
0
1
0 −2
0 −2 −1
0 −1
0
0
0
2
0
1
0
0
0
0 −1
0
0
0
2
0
1
0
0
0
0
0
0 −1
0
0
2
1
−1
1
0
0
1
0
0
1
0
0
0 −1
1
0
0
1
0
0
1
0
0
0
0
0 −1
1
0
1
0
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Using row operations (including row interchanges) the Gauss-Jordan algorithm
takes the matrix Mm to the (unique) row reduced echelon form matrix Nm (see Chap-
ter 2, §1). Each nonzero row in Nm is a linear combination with coefﬁcients a(α,i) in
k of the rows of Mm. Hence each such row represents an element
g =

(α,i)∈Sm
a(α,i)xαfi ∈Im.
You will show in Exercise 6 that the nonzero rows in the matrix Nm represent a
vector space basis for Im, so each nonzero S-polynomial remainder of degree m
corresponds to a linear combination of these rows.
Example 9. Continuing from Example 8, the row reduced echelon form matrix
computed from M2 in (4) is
N2 =
⎛
⎝
1 0 0 −1 −2 −2
0 1 0
0 −2 −1
0 0 1 −1
2
0 −1
2
⎞
⎠.
The rows in N2 correspond to polynomials g1, g2, g3 that form a new basis for the
vector space I2 spanned by the original F = ( f1, f2, f3). Note that the leading term
of g3 is LT(g3) = y2, which is not in the monomial ideal ⟨LT( f1), LT( f2), LT( f3)⟩=
⟨x2, xy⟩. In other words, the computation of N2 has accomplished the same sort
of uncovering of new leading terms that S-polynomial remainders are designed to
produce. Moreover, it is easy to check that g3 = y2−1
2xz−1
2z2 is a constant multiple
of the remainder S( f1, f3)
F (from the pair of degree 2 for these polynomials). So
the row reduction has emulated a part of the computation of a Gröbner basis by

548
Chapter 10 Additional Gröbner Basis Algorithms
Buchberger's algorithm. In Exercise 7, you will compute the row reduced echelon
form matrix N3 for M3 given in (5) and see an interesting interpretation of the results.
We will also use this example in §2.
The results seen in Example 9 are consequences of the way the columns in M2
and N2 are ordered using the monomial ordering. Similar patterns hold for all collec-
tions of homogeneous polynomials. The following proposition shows how row re-
ducing the matrix Mm produces information equivalent to some of the S-polynomial
remainder computations in Buchberger's algorithm. The row reduction essentially
does all of the S-polynomial remainders in degree m at once.
Proposition 10. Let f1, . . . , fs be any collection of homogeneous polynomials, and
let I be the ideal they generate. Let g1, . . . , gt be the polynomials corresponding
to the nonzero rows in the row reduced echelon form matrix Nm. If g ∈Im is any
nonzero polynomial, then LM(g) is equal to LM(gi) for some i, 1 ≤i ≤t.
Proof. Because of the way the set Tm of monomials is ordered, the leading 1 in each
nonzero row of the row reduced echelon form matrix Nm is the leading coefﬁcient
of the corresponding polynomial gi in Im. Moreover, because of the properties of
matrices in row reduced echelon form, the leading 1's appear in distinct columns, so
the leading monomials of the gi are all distinct. Finally, as noted before, the gi are
a vector space basis for Im. So all g ∈Im, including all the nonzero S-polynomial
remainders for pairs of degree m, are equal to linear combinations g = t
i=1 cigi
with ci ∈k. Hence LM(g) is equal to LM(gi) for the smallest i, 1 ≤i ≤t, such that
ci ̸= 0.
□
The proposition implies that polynomials in I = ⟨f1, . . . , fs⟩with every possible
leading monomial could be found by computing the row reduced echelon form ma-
trices Nm for each m ∈Z≥0 in turn. Hence, stopping after degree m, the resulting
set of polynomials from the Nj, 0 ≤j ≤m, will form a Gröbner basis up to degree
m for I as in Proposition 3. You will show this in Exercise 8.
However, this idea by itself does not give an alternative to Buchberger's algo-
rithm because we have not said how to determine when the process can be stopped
with a full Gröbner basis for I. In addition, we have not said how to use the fact that
S-polynomials S( fi, fj) for pairs (i, j) of degree m give elements of Im. The F4 algo-
rithm to be presented in §3 takes some of these ideas and combines them to produce
a very efﬁcient alternative to Buchberger's algorithm that also applies when homo-
geneity is not present.
EXERCISES FOR §1
1. Let F = ( f1, . . . , fs) where the fi are homogeneous. Show that the degree of (i, j) co-
incides with the degree of S( fi, fj) and with the degree of the remainder on division of
S( fi, fj) by F if the S-polynomial and the remainder are not zero.
2. Show that the mapping
k[x0, x1, . . . , xn] −→k[x1, . . . , xn]
g(x0, . . . , xn) −→gd(x1, . . . , xn) = g(1, x1, . . . , xn)

§1 Preliminaries
549
is a ring homomorphism and is onto.
3. Consider the situation in the last part of the proof of Theorem 5, where we have
homogeneous gi, gj satisfying S(gi, gj) →G 0 and for all g, we have an equation
LM>h(g) = xa
0LM>(gd) for some a ∈Z≥0.
a. Use Exercise 2 to show that S(gi, gj)d = S(gd
i , gd
j ) for all i ̸= j.
b. Suppose that S(gi, gj) = A1g1 + · · · + Atgt is a standard representation. Show that we
obtain another standard representation if we dehomogenize both sides of this equa-
tion.
c. Deduce that S(gd
i , gd
j ) →Gd 0.
4. Let > be lex order on k[x1, . . . , xn] with x1 > · · · > xn. Show that the >h monomial
order on k[x0, x1, . . . , xn] deﬁned in (4) is the same as lex order with x1 > · · · > xn > x0
(i.e., with the homogenizing variable ordered last).
5. Prove Lemma 7. Hint: Exercise 8 of Chapter 8, §3 will be useful.
6. Show that the polynomials corresponding to the nonzero rows of the row reduced eche-
lon form matrix Nm form a vector space basis for Im for each m ∈Z≥0.
7. a. Compute the row reduced echelon form matrix N3 from the matrix M3 in (3). You
should obtain entire rows of zeroes in N3.
b. By keeping track of the polynomials xαfi corresponding to the rows in M3 as the row
operations are performed, show that the result of part (a) means there are relations
ℓ1 f1 + ℓ2 f2 + ℓ3 f3 = 0,
where the ℓi are homogeneous polynomials of degree 1 in x, y, z. Find two different
explicit relations of this form (i.e., relations where the corresponding ℓi are not just
constant multiples of each other).
c. Check that the polynomials f1, f2, f3 in this example are the determinants of the 2 × 2
submatrices of the 2 × 3 matrix

x + y
y + z
x −z
x + 2y + z
x + z
x

.
d. Show that the result of part (c) gives an explanation for the relations found in part (b).
[Hint: There are two "obvious" ways of appending an additional row to the matrix
in part (c) to obtain a 3 × 3 matrix of homogeneous polynomials of degree 1 whose
determinant is identically zero. Expanding the determinant by cofactors along the
new third row will give relations as in part (b).]
e. Determine a Gröbner basis for I = ⟨f1, f2, f3⟩with respect to grevlex order with
x > y > z.
8. a. Let f1, . . . , fs be homogeneous. Show that if all the matrices Mj for j ≤m are put in
row reduced echelon form, then the resulting polynomials form a Gröbner basis up
to degree m for I = ⟨f1, . . . , fs⟩.
b. Show by example, though, that in the procedure from part (a) it is possible to get a set
of polynomials that is strictly larger than the Gröbner basis up to degree m obtained
in the proof of Proposition 3.
9. In this exercise you will prove another characterization of Gröbner bases up to degree
m for a homogeneous ideal I. Let G = ( f1, . . . , ft) be a list of homogeneous generators
of I. By adapting the proof of Buchberger's Criterion (Theorem 6 of Chapter 2, §6) to
the homogeneous case, prove that the following are equivalent:
i. For all f ∈I≤m, LT( f ) is divisible by LT( fi) for some i, 1 ≤i ≤t.
ii. For all pairs (i, j) of degree ≤m relative to G, S( fi, fj)
G = 0.
10. a. Show that G is a Gröbner basis for a homogeneous ideal I if and only if it is a Gröbner
basis for I up to degree m for all m ∈Z≥0.
b. Use this to ﬁnish a proof of Theorem 4.

550
Chapter 10 Additional Gröbner Basis Algorithms
§2
Hilbert Driven Buchberger Algorithms
In this section we will write S = k[x1, . . . , xn]. Recall from Chapter 9, §3 that if I is
a homogeneous ideal and m ∈Z≥0, the value of the Hilbert function HFS/I at m is
deﬁned by
HFS/I(m) = dim Sm/Im = dim Sm −dim Im,
where, as in the previous section Im is the vector space of homogeneous elements
of degree m in I (together with the zero polynomial), and similarly Sm is the vector
space of all homogeneous polynomials of degree m (again together with the zero
polynomial). The notation dim here means the dimension of the indicated vector
space over k.
For example, with S = Q[x, y, z] and the ideal
I = ⟨x2 −2y2 −2yz −z2, −xy + 2yz + z2, −x2 + xy + xz + z2⟩
from Examples 8 and 9 of §1 we have dim Q[x, y, z]2 = 6 and dim I2 = 3, so
HFS/I(2) = 6 −3 = 3. Similarly, dim Q[x, y, z]3 = 10 and dim I3 = 7. So
HFS/I(3) = 10 −7 = 3.
Hilbert functions for general I are computable because of the result from Propo-
sition 9 of Chapter 9, §3:
• If I is a homogeneous ideal and > is any monomial order, then the Hilbert func-
tions HFS/⟨LT(I)⟩and HFS/I are equal.
A ﬁrst connection with Gröbner bases is an easy corollary of this statement. If G is
a set of polynomials, then as usual we will write
⟨LT(G)⟩= ⟨LT(g) | g ∈G⟩
for the monomial ideal generated by the leading terms of the elements of G.
Proposition 1. Let G ⊆I be a ﬁnite subset. Then
HFS/⟨LT(I)⟩(m) ≤HFS/⟨LT(G)⟩(m)
for all m ∈Z≥0. Equality holds for all m if and only if G is Gröbner basis for I.
Proof. The ﬁrst statement holds since ⟨LT(G)⟩m ⊆⟨LT(I)⟩m for all m. The sec-
ond statement follows then since ⟨LT(G)⟩m = ⟨LT(I)⟩m for all m is equivalent to
⟨LT(G)⟩= ⟨LT(I)⟩, which is equivalent to saying G is a Gröbner basis for I.
□
In this section we will show how information from HFS/I can be used to control
the computation of a Gröbner basis for I using the degree by degree version of
Buchberger's algorithm presented in Theorem 4 from §1. The goal, as indicated
in the comments following that theorem, is to recognize when some S-polynomial
remainder calculations are unnecessary and can be bypassed. The algorithm we will
present was ﬁrst described in TRAVERSO (1997).

§2 Hilbert Driven Buchberger Algorithms
551
Since the way we usually determine the Hilbert function HFS/I is to compute a
Gröbner basis for I and then ﬁnd the Hilbert function HFS/I = HFS/⟨LT(I)⟩using
the monomial ideal ⟨LT(I)⟩, this might seem somewhat circular at ﬁrst glance and
some explanation is required. The idea is that the algorithm we will develop in this
section is designed for certain special circumstances such as the following.
• Suppose we want to compute a Gröbner basis for a not necessarily homogeneous
ideal I with respect to a lex order. These computations can be extremely complex;
both the space required to store the intermediate polynomials generated by Buch-
berger's algorithm and the time required can be large. However, the calculation
of a Gröbner basis G for the same ideal with respect to a graded order such as
grevlex is often much easier. By the discussion in §1, if G is a Gröbner basis for
I with respect to a graded order, then Gh is a Gröbner basis for the homogeneous
ideal Ih. From this information we can compute the Hilbert function of Ih. Hence
we are led to the problem of converting the basis Gh into a Gröbner basis for Ih
with respect to a different monomial order, say a lex order with the homogeniz-
ing variable as the smallest variable. It is for this Gröbner basis conversion step
that the Hilbert function and the algorithm to be presented in this section will be
of use. We can then derive a lex Gröbner basis for I by dehomogenization. If a
reduced lex Gröbner basis is required, then further remainder computations as in
the proof of Theorem 5 of Chapter 2, §7 can be used. Experience has shown that
this seemingly roundabout approach is often more efﬁcient than the straight lex
Gröbner basis computation.
• In some cases, the computation of an initial Gröbner basis for I may even be
unnecessary because the given ideal generators are themselves automatically a
Gröbner basis with respect to some monomial order. This happens, for instance,
in the implicitization problems for polynomial parametrizations studied in §2 of
Chapter 2, where we considered ideals of the form
I = ⟨x1 −f1(t1, . . . , tr), . . . , xn −fn(t1, . . . , tr)⟩.
Let > be a monomial order with the property that for each i, xi is greater than any
monomial containing only the tj. This would be true, for instance, for lex order
with x1 > · · · > xn > t1 > · · · > tr. The leading terms of the generators of I are
the xi, which are pairwise relatively prime. Hence by Proposition 1 of Chapter 2,
§10 and Theorem 3 of Chapter 2, §9, the given generators for I are a Gröbner
basis. Some additional care is required in this case to determine an appropriate
way to homogenize and apply the information from the Hilbert function. We will
return to this situation later.
Now assume I is a homogeneous ideal and that we already know HFS/I(m) for
all m. In order to make use of Proposition 1 in the computation of a Gröbner basis for
I, we must be able to compare the known HFS/I = HFS/⟨LT(I)⟩with the HFS/⟨LT(G)⟩
for the intermediate partial Gröbner bases G generated as the algorithm proceeds.
So another necessary ingredient is an efﬁcient algorithm that computes the Hilbert
functions of monomial ideals "on the ﬂy."

552
Chapter 10 Additional Gröbner Basis Algorithms
Hilbert Functions of Monomial Ideals
Since we are discussing algorithms for computing Gröbner bases for an ideal I, in
this subsection, we use J to denote a monomial ideal. Eventually, the J here will
correspond to one of the ideals ⟨LT(G)⟩for a partial Gröbner basis G of I.
The ﬁrst thing to understand here is that algorithms for computing the Hilbert
function do not compute HFS/J(m) one m at a time, but rather produce information
that packages the values HFS/J(m) for all m simultaneously. This is done by means
of a standard trick, namely use of the generating function for the Hilbert function,
a formal power series in an auxiliary variable, say t, whose coefﬁcients encode the
Hilbert function values. This power series is known as the Hilbert-Poincaré series
(or sometimes just the Hilbert series) of S/J and is deﬁned as:
PS/J(t) =
∞

m=0
HFS/J(m)tm.
Example 2. Some easy ﬁrst cases are the following.
a. The simplest case of all is J = ⟨1⟩, for which S/J = {[0]}. Hence HFS/⟨1⟩(m) =
0 for all m ≥0. The Hilbert-Poincaré series is
PS/⟨1⟩= 0.
b. If J = ⟨x1, . . . , xn⟩, then S/J ∼= k. Hence HFS/⟨x1,...,xn⟩(0) = 1, while for all
m ≥1, HFS/⟨x1,...,xn⟩(m) = 0. The Hilbert-Poincaré series has just one nonzero
term:
PS/⟨x1,...,xn⟩= 1.
c. The next simplest case is J = ⟨0⟩, so S/J ∼= S. In this case the Hilbert
function will just count the number of monomials in each degree. From Ex-
ercise 13 of Chapter 9, §3, we know that the number of monomials of degree m
in k[x1, . . . , xn] is equal to the binomial coefﬁcient
n+m−1
m
	
. Hence
HFS/⟨0⟩(m) =
n + m −1
m

,
and the Hilbert-Poincaré series is
PS/⟨0⟩(t) =
∞

m=0
n + m −1
m

tm.
In Exercise 1, you will show that this is the same as the Taylor series (at t = 0)
of the rational function
1
(1−t)n . Hence we will also write
(1)
PS/⟨0⟩(t) =
1
(1 −t)n .

§2 Hilbert Driven Buchberger Algorithms
553
d. Generalizing parts (b) and (c), let J = ⟨x1, . . . , xp⟩for p ≤n. Then S/J ∼= S′/⟨0⟩
where S′ is a polynomial ring in n −p variables. Moreover degrees are preserved
for monomials not in J under this isomorphism. Then dim Sm/Jm will be the same
as the number of distinct monomials of degree m in the polynomial ring in n −p
variables and hence
PS/⟨x1,...,xp⟩(t) =
1
(1 −t)n−p = (1 −t)p
(1 −t)n .
Note that the rational functions in all parts of this example can be written with the
denominator (1 −t)n.
Here is a somewhat more interesting example.
Example 3. Let J = ⟨x2, xy, xz⟩⊆S = k[x, y, z]. We have HFS/J(0) = 1 and
HFS/J(1) = 3, since all the generators of J have degree 2. The monomials of degree
2 that are not in J are y2, yz, z2. Similarly for all m ≥0, the set of all monomials that
contain no factor of x will lie outside J and give linearly independent equivalence
classes in Sm/Jm that span the quotient as a vector space over k. There are exactly
m + 1 of those monomials of degree m:
ym, ym−1z, . . . , yzm−1, zm
as in part (c) of Example 2 with n = 2. Apart from these, the only other contribu-
tion to the Hilbert function comes from the monomial x of degree 1. Therefore the
Hilbert-Poincaré series of S/J will have the form:
PS/J(t) = 1 + (1 + 2)t + 3t2 + 4t3 + 5t4 + · · ·
= 1 + 3t + 3t2 + 4t3 + 5t4 + · · · .
From part (c) of Example 2, or by computing a Taylor expansion for the rational
function
1
(1−t)2 , we recognize that this can be rewritten as
t +
1
(1 −t)2 = t3 −2t2 + t + 1
(1 −t)2
= −t4 + 3t3 −3t2 + 1
(1 −t)3
.
In the last line we have multiplied the numerator and denominator by 1−t to match
the denominator for J = ⟨0⟩and n = 3 variables as in (1) above. The reason for
doing this will become clear in the theorem below.
Theorem 4. Let J be a monomial ideal in S = k[x1, . . . , xn]. The Hilbert-Poincaré
series for S/J can be written in the form
PS/J(t) = NS/J(t)
(1 −t)n ,
where NS/J(t) ∈Z[t] is a polynomial with integer coefﬁcients.

554
Chapter 10 Additional Gröbner Basis Algorithms
We include the following proof because it serves as the basis for a ﬁrst algorithm
for computing the Hilbert-Poincaré series of S/J for a monomial ideal but this will
be admittedly something of a digression from our main topic. Readers who are will-
ing to accept the theorem without proof may wish to proceed directly to Theorem 6
and the example at the end of this subsection.
Proof. Let T = {xα(1), . . . , xα(s)} be the minimal set of generators for J. We in-
clude the case s = 0, where T = ∅and J = {0}. The proof will use a slightly tricky
induction on the sum of the degrees of the generators, the integer quantity
Σ =
s

j=1
|α(j)|.
Since s ≥0 and |α(j)| ≥0 for all j when s > 0, the base cases will have either
• Σ = 0, s = 0, in which case J = {0}, or
• Σ = 0 but s ≥1, which implies J = ⟨1⟩, or
• Σ ≤n with |α(j)| = 1 for all j.
All of these cases are covered in Example 2 above. If J = {0}, then S/J ∼= S, so
PS/J(t) =
1
(1−t)n . If J = ⟨1⟩, then S/J = {[0]}, so PS/J(t) = 0 =
0
(1−t)n . If all the
generators of J have degree 1, then PS/J(t) =
1
(1−t)n−p = (1−t)p
(1−t)n for some p ≥1,
which has the required form. So the statement holds in all of these cases.
For the induction step, we will need to make use of the following lemma.
Lemma 5. Let J be a monomial ideal and let h ∈Sr be a monomial of degree r. For
each m ≥r, consider the linear mapping
αh : Sm−r/Jm−r −→Sm/Jm
deﬁned by multiplication by h. Then
(i) The kernel of αh is equal to (J :h)m−r/Jm−r, where J : h is the quotient, or colon,
ideal deﬁned in Chapter 4.
(ii) The cokernel of αh (i.e., the target space Sm/Jm modulo the image of αh), is
equal to Sm/(J + ⟨h⟩)m.
(iii) We have
HFS/J(m) −HFS/J(m −r) = dim coker(αh) −dim ker(αh).
(iv) The Hilbert-Poincaré series of S/J, S/(J + ⟨h⟩) and S/(J : h) satisfy
PS/J(t) = PS/(J+⟨h⟩)(t) + tr · PS/(J : h)(t).
Proof of Lemma 5. The main content of parts (i)-(iii) of the lemma has already
appeared in the proof of Theorem 3 from Chapter 9, §4 (although what we did there
was slightly more general in that we considered an arbitrary homogeneous ideal
I instead of a monomial ideal J and an arbitrary homogeneous polynomial f rather
than a monomial h). Part (i) is also more precise in that we did not identify the kernel

§2 Hilbert Driven Buchberger Algorithms
555
of αh in this way before. However, the statement is immediate by the deﬁnition of
the quotient ideal and you will verify this and the rest in Exercise 2.
We now turn to the proof of part (iv). Since HFS/J(m) = dim Sm/Jm, parts
(i)-(iii) imply
(2)
dim Sm/Jm = dim coker(αh) −dim ker(αh) + dim Sm−r/Jm−r
= dim Sm/(J + ⟨h⟩)m −dim (J :h)m−r/Jm−r + dim Sm−r/Jm−r
By Proposition 1 of Chapter 9, §3, the last two terms of the second line simplify to
−dim (J :h)m−r/Jm−r + dim Sm−r/Jm−r = dim Sm−r/(J :h)m−r
since the dim Jm−r terms cancel. Combining this with (2), we obtain
dim Sm/Jm = dim Sm/(J + ⟨h⟩)m + dim Sm−r/(J :h)m−r.
This implies the equality on the Hilbert-Poincaré series since the coefﬁcient of tm in
the product tr · PS/(J : h)(t) is dim Sm−r/(J : h)m−r whenever m ≥r.
□
Using the lemma, we will now complete the induction step in the proof of the
theorem. Assume that we have proved the statement for all S and J with Σ ≤ℓand
consider a monomial ideal J with Σ = ℓ+1 and minimal generating set T as before.
Since Σ > 0, it follows that s > 0. Moreover, since we have covered the cases where
all the generators have degree equal to 1 in a base case, we may assume that some
xα( j0) in T has |α(j0)| ≥2 and let xi be a variable appearing in this monomial.
By part (iv) of the lemma with h = xi and r = 1 we have an equation
(3)
PS/J(t) = PS/(J+⟨xi⟩)(t) + t · PS/(J : xi)(t).
In J+⟨xi⟩, the monomial xα( j0) in T cannot appear in a minimal generating set since
xi divides it. Hence the sum of the degrees of the minimal generators must drop. The
induction hypothesis applies and we have
PS/(J+⟨xi⟩)(t) =
N1(t)
(1 −t)n
for some N1(t) ∈Z[t].
It remains to analyze the second term on the right in (3). Let us change our
notation for the minimal generating set T so
xα( j) = xaj
i ˆx ˆα( j),
where ˆx ˆα( j) is a monomial in the variables other than xi and ˆα(j) ∈Zn−1
≥0 contains
the other exponents. By Theorem 14 of Chapter 4, §4 we get a basis for J :xi by
taking a basis {g1, . . . , gt} for J ∩⟨xi⟩and dividing each of these polynomials by
xi. In Exercise 3 you will show that this amounts to replacing xα( j) = xaj
i ˆx ˆα( j) by
xaj−1
i
ˆx ˆα( j) if aj > 0 and leaving it unchanged otherwise. Since we assumed aj0 > 0

556
Chapter 10 Additional Gröbner Basis Algorithms
in xα( j0), this means that the sum of the degrees of the minimal generators of J :xi
also decreases and by induction
PS/(J : xi)(t) =
N2(t)
(1 −t)n
where N2(t) is some polynomial in Z[t]. The proof is complete when we put the two
terms on the right of (3) over a common denominator.
□
The equality in (3) and the proof of the theorem also serve as the bases for the
following (somewhat rudimentary) recursive algorithm for computing PS/J(t).
Theorem 6. The following function HPS terminates and correctly computes the
Hilbert-Poincaré series PS/J(t) for a monomial ideal J in k[x1, . . . , xn].
Input : J ⊆S = k[x1, . . . , xn], a monomial ideal
Output : HPS(J) = the Hilbert-Poincaré series PS/J(t) of S/J
T := minimal set of generators for J
IF T = ∅THEN
HPS(J) :=
1
(1 −t)n
ELSE IF T = {1} THEN
HPS(J) := 0
ELSE IF T consists of p monomials of degree 1 THEN
HPS(J) :=
1
(1 −t)n−p
ELSE
Select xi appearing in a monomial of degree > 1 in T
HPS(J) := HPS(J + ⟨xi⟩) + t · HPS(J :xi)
RETURN HPS(J)
Proof. If we are not in one of the base cases, the same function is applied recur-
sively to the monomial ideals J + ⟨xi⟩and J :xi and the results are combined us-
ing (3). As shown in the proof of Theorem 4, both of the ideals J + ⟨xi⟩and J :xi
are closer to the base cases in the sense that the sum of the degrees of the minimal
generators decreases. Hence all chains of recursive calls will reach the base cases
eventually. In other words, the function always terminates. The correctness follows
from (3), a special case of part (iv) of Lemma 5.
□
Example 7. We reconsider Example 3 using the function HPS described in Theo-
rem 6. The variable xi is sometimes called the pivot variable in the process. If we

§2 Hilbert Driven Buchberger Algorithms
557
apply HPS to J = ⟨x2, xy, xz⟩in Q[x, y, z], we are not in a base case to start. Choos-
ing x as the pivot variable, we see that
J + ⟨x⟩= ⟨x2, xy, xz, x⟩= ⟨x⟩
and
J :x = ⟨x, y, z⟩.
(Note how the sum of the degrees of the generators has dropped in both cases.)
Applying HPS recursively to the ﬁrst ideal, we see all generators have degree 1, so
PS/(J+⟨x⟩)(t) =
1
(1 −t)2 =
1 −t
(1 −t)3 .
Moreover, J : x also has all generators of degree 1, so we are in a base case on this
recursive call too and
PS/(J : x)(t) = 1 = (1 −t)3
(1 −t)3 .
Combining these values as in (3) yields the result we saw before in Example 3 for
PS/J(t):
1
(1 −t)2 + t · 1 = −t4 + 3t3 −3t2 + 1
(1 −t)3
.
You will investigate what happens if a different pivot variable is chosen in Exercise 5.
More reﬁned recursive algorithms for computing Hilbert-Poincaré series are dis-
cussed in BIGATTI (1997). These still make use of part (iv) of Lemma 5, but include
other improvements; see Exercise 4 for the idea behind one possible additional "di-
vide and conquer" strategy. By Theorem 4, it is actually only necessary to compute
the numerator polynomial NS/J(t), so some versions of these algorithms are set up
to compute just that polynomial.
Hilbert Functions and Buchberger's Algorithm
We are now ready to discuss the improved version of Buchberger's algorithm mak-
ing use of the information from the Hilbert function. We begin by noting the follow-
ing corollary of Proposition 1 stated using the Hilbert-Poincaré series.
Proposition 8. Let G be a ﬁnite subset of a homogeneous ideal I.
(i) The series expansions of PS/I(t) and PS/⟨LT(G)⟩(t) agree up to and including the
tm terms if and only if G is a Gröbner basis of I up to degree m.
(ii) PS/I(t) = PS/⟨LT(G)⟩(t) if and only if G is a Gröbner basis for I.
Proof. This is left to the reader as Exercise 6.
□

558
Chapter 10 Additional Gröbner Basis Algorithms
The following algorithm uses information from the Hilbert function to control
when S-polynomial remainder calculations can stop in each degree and when the
algorithm can terminate.
Theorem 9. The following version of Buchberger's algorithm terminates and cor-
rectly computes a Gröbner basis for I = ⟨f1, . . . , fs⟩with fi homogeneous, using the
function HPS for monomial ideals from Theorem 6 and knowledge of PS/I(t).
Input : F = ( f1, . . . , fs) homogeneous, PS/I(t)
Output : G, a Gröbner basis for I
B := {(i, j) | 1 ≤i < j ≤s}
G := F
Δ := inﬁnity
m′ := 0
l := s
WHILE B ̸= ∅DO
B′ := {(i, j) ∈B | deg(i, j) = m′}
B := B \ B′
WHILE B′ ̸= ∅AND Δ > 0 DO
(i, j) := ﬁrst element in B′
B′ := B′ \ {(i, j)}
S := S( fi, fj)
G
IF S ̸= 0 THEN
l := l + 1; fl := S
G := G ∪{fl}
B := B ∪{(i, l) | 1 ≤i ≤l −1}
Δ := Δ −1
PS/⟨LT(G)⟩(t) := HPS(⟨LT(G)⟩)
IF PS/⟨LT(G)⟩(t) = PS/I(t) THEN
RETURN G
ELSE
m′ := min(m | HFS/⟨LT(G)⟩(m) ̸= HFS/I(m))
Δ := HFS/⟨LT(G)⟩(m′) −HFS/I(m′)
B′ := {(i, j) ∈B | deg(i, j) < m′}
B := B \ B′
In the literature this is often called the Hilbert driven Buchberger algorithm, and
we will follow this practice. See Exercise 7 below for additional information about

§2 Hilbert Driven Buchberger Algorithms
559
how the tests based on the Hilbert function might be implemented using information
from the numerator polynomial NS/J(t) in the rational function form of PS/J(t).
Proof. The algorithm terminates and is correct for the same reasons that the degree
by degree version of the standard Buchberger algorithm from Theorem 4 of §1 ter-
minates and is correct. To prove this, we will show that exactly the same pairs are
inserted into B and exactly the same polynomials are inserted into G by this algo-
rithm as in the degree by degree Buchberger algorithm (under a reasonable assump-
tion about the order in which the pairs from the sets called B′ in both algorithms are
processed). The important difference between the two algorithms is that the Hilbert
driven algorithm discards pairs without computing the S-polynomial remainders in
some cases. But in fact that only happens when the remainders would automatically
be zero; so no new polynomials would be inserted into G and no new pairs would
be generated and inserted into B by the degree by degree Buchberger algorithm.
To prove this claim, for simplicity, assume there are no pairs of degree 0 in
B at the start. (If there are, then I = k[x1, . . . , xn] and it is easy to see that the
Hilbert driven algorithm performs correctly in that case.) Then B′ = ∅on the
ﬁrst pass through the outer WHILE loop in the Hilbert driven algorithm. If the
Hilbert-Poincaré series of S/I and S/⟨LT(G)⟩agree at the start, then by part (ii)
of Proposition 8 we see that G is already a Gröbner basis and the algorithm termi-
nates immediately. Otherwise, this pass simply ﬁnds an integer m′ ≥1 such that
HFS/⟨LT(G)⟩(m) = HFS/I(m) for m < m′ but HFS/⟨LT(G)⟩(m′) < HFS/I(m′), i.e., m′
is the smallest power of t for which the coefﬁcients in the Hilbert-Poincaré series
of S/⟨LT(G)⟩and S/I differ. It follows from part (i) of Proposition 8 that G is a
Gröbner basis up to degree m′ −1 ≥0 at the end of that pass. If there were pairs in
any degrees ≤m′ −1 in B to start, this implies that the degree by degree Buchberger
algorithm would compute all those S-polynomials but all the remainders would be
zero by (1) in §1. So we have reached the end of passes through the outer loops in
the two algorithms with the same B and G in both. Moreover, both algorithms will
next consider the same set B′ of pairs of degree m′. We will now use this ﬁrst part
of the argument as the base case for a proof by induction.
Assume that we have reached the end of some passes through the outer loops in
the two algorithms with the same B and G in both, that both algorithms will next
consider the same set B′ of pairs with the same degree m′, and that elements in B′
are processed in the same order. Before this point the Hilbert driven algorithm also
computed
Δ = HFS/⟨LT(G)⟩(m′) −HFS/I(m′).
This is the number of new linearly independent basis elements needed in degree m′
to obtain a Gröbner basis up to degree m′. Both algorithms now do the same compu-
tations up to a point. Each nonzero S-polynomial remainder gives an new element
of G and additional pairs in B. Note that each of these new pairs has degree > m′ by
Proposition 2 from §1. In the Hilbert driven algorithm, each nonzero S-polynomial
remainder found decreases Δ by 1. So the correctness of the degree by degree
Buchberger algorithm tells us that the value of Δ must eventually reach 0 before
the current pass through the outer WHILE loop is completed. When that happens

560
Chapter 10 Additional Gröbner Basis Algorithms
HFS/⟨LT(G)⟩(m′) will equal HFS/I(m′). The Hilbert-Poincaré series now agree up to
and including the tm′ terms, hence G is a Gröbner basis for I up to degree m′ by
part (i) of Proposition 8. The Hilbert driven algorithm's inner WHILE loop will ter-
minate at this point. Any remaining pairs of degree m′ at this time are discarded
since the S-polynomial remainders will necessarily be zero (see (1) and Exercise 9
in §1). Hence the degree by degree Buchberger algorithm will ﬁnd exactly the same
elements in degree m′ in G and form the same pairs in B using those new elements.
Moreover, with the new value of G, the Hilbert driven algorithm recomputes
PS/⟨LT(G)⟩(t) using the function HPS and compares the result with PS/I(t). If the
series are equal, then G is a Gröbner basis for I by part (ii) of Proposition 8. Any
pairs remaining to be processed at that point may also be discarded for the same
reason as above. Otherwise a new, strictly larger, m′ is found such that PS/⟨LT(G)⟩(t)
and PS/I(t) agree up to the tm′−1 terms and G is a Gröbner basis up to degree m′ −1.
As in discussion of the base case, the degree by degree algorithm ﬁnds only zero
remainders for any pairs of degree between the previous m′ and the new m′. Thus
both algorithms will start processing the pairs of the new m′ with the same G and B
and our claim is proved by induction.
Since we know the degree by degree algorithm terminates and computes a Gröb-
ner basis for I, the same must be true for the Hilbert driven algorithm.
□
Here is a ﬁrst example to illustrate the Hilbert driven algorithm in action.
Example 10. Consider S = Q[x, y, z] and the homogeneous ideal
I = ⟨x2 −2y2 −2yz −z2, −xy + 2yz + z2, −x2 + xy + xz + z2⟩
from Examples 8 and 9 of §1. The Hilbert-Poincaré series for S/I has the form:
PS/I(t) = 2t3 −3t2 + 1
(1 −t)3
= 2t + 1
1 −t .
The Taylor expansion of this rational function starts out as follows:
(4)
PS/I(t) = 1 + 3t + 3t2 + 3t3 + · · · ,
with all coefﬁcients after the ﬁrst equal to 3. Hence HFS/I(0) = 1 and HFS/I(m) = 3
for all m ≥1. Several of the computer algebra systems discussed in Appendix C can
compute Hilbert-Poincaré series in this form; we used Maple and grevlex order with
x > y > z on Q[x, y, z] to ﬁnd this.
We note that a grevlex Gröbner basis for I was computed in the process of de-
riving the formula in (4), but we will not set up this computation as a Gröbner basis
conversion. Instead, we will simply compute a Gröbner basis for I for lex order with
x > y > z, using the Hilbert driven algorithm with the three given generators for I
and the Hilbert-Poincaré series as the input.
The initial B is {(1, 3), (1, 2), (2, 3)}, where the ﬁrst pair has degree 2 and
the remaining two pairs have degree 3. The ﬁrst pass through the outer WHILE
loop serves to determine the value m′ = 2 where the S-polynomial remainder

§2 Hilbert Driven Buchberger Algorithms
561
computations actually begin. In more detail, note that before any computation of
S-polynomials occurs, ⟨LT(G)⟩
=
⟨x2, xy⟩. The Hilbert-Poincaré series for
S/⟨LT(G)⟩is
t3 −2t2 + 1
(1 −t)3
= 1 + 3t + 4t2 + 5t3 + 6t4 + · · · .
Hence from (4), the smallest m′ for which we are "missing" leading terms for I is
m′ = 2, and Δ = 4 −3 = 1.
In the next pass through the outer WHILE loop, the algorithm makes B′ =
{(1, 3)} and B = {(1, 2), (2, 3)}. Then S( f1, f3) is
(x2 −2y2 −2yz −z2) + (−x2 + xy + xz + z2) = xy + xz −2y2 −2yz
and dividing by G yields the remainder
S( f1, f3)
G = xz −2y2 + z2.
This is not zero, so we include this as a new element f4 in G and B is updated to
B = {(1, 2), (2, 3), (1, 4), (2, 4), (3, 4)}.
All these pairs have degree 3. So the inner loop terminates with B′ = ∅and Δ = 0.
Now the algorithm again compares the Hilbert-Poincaré series PS/I(t) from (4)
and PS/⟨LT(G)⟩(t) = PS/⟨x2,xy,xz⟩(t). From Example 3 or Example 7, the Hilbert-
Poincaré series for this monomial ideal is
−t4 + 3t3 −3t2 + 1
(1 −t)3
= 1 + 3t + 3t2 + 4t3 + 5t4 + · · · .
Hence the new m′ is m′ = 3 and Δ = 4−3 = 1. We seek one additional polynomial
of degree 3.
For m′ = 3, we have B′ = {(1, 2), (2, 3), (1, 4), (2, 4), (3, 4)}, and B is updated
to ∅. The ﬁrst pair of degree 3 to be processed is (1, 2). For this pair,
S( f1, f2)
G = −2y3 + 3yz2 + z3,
where G = {f1, f2, f3, f4}. Hence this nonzero polynomial becomes f5, and this is
inserted into G. B is updated to {(2, 5), (1, 5), (3, 5), (4, 5)}, where the pair (2, 5)
has degree 4 and the other new pairs have degree 5. Δ is reduced to 0 again and
the inner WHILE loop is exited. In the process, the other pairs of degree 3 in B′
above are discarded. Since we have found the one new polynomial of degree 3 im-
mediately, those pairs are unnecessary. This illustrates one advantage of the Hilbert
driven approach.
Now another very interesting thing happens. For the monomial ideal J =
⟨x2, xy, xz, y3⟩the Hilbert-Poincaré series PS/J(t) is equal to PS/I(t) from (4). Hence

562
Chapter 10 Additional Gröbner Basis Algorithms
HFS/I(m) = HFS/⟨LT(G)⟩(m)
for all m ∈Z≥0 and the algorithm terminates. It is not necessary to compute and
reduce S-polynomials for any of the remaining pairs of degrees 4 or 5 in B. The
procedure returns the ﬁnal G containing the three original polynomials and the f4, f5
found above.
Application to Implicitization
We now return to the second proposed use for the Hilbert driven Buchberger algo-
rithm mentioned at the start of the section. Recall that we noted that in polynomial
implicitization problems, the generators of an ideal of the form
(5)
I = ⟨x1 −f1(t1, . . . , tr), . . . , xn −fn(t1, . . . , tr)⟩
in k[x1, . . . , xn, t1, . . . , tr] are already a Gröbner basis for any monomial order that
makes the xi the leading terms, since those monomials are pairwise relatively prime.
Assume for simplicity that each fi(t1, . . . , tr) is homogeneous, with deg fi = di ≥1
for each i. (If this were not true, we could introduce a homogenizing variable t0 to
attain this without changing the xi terms.)
The goal here would be to eliminate the t1, . . . , tr to obtain a basis for the elimi-
nation ideal
I ∩k[x1, . . . , xn],
for instance by converting the basis from (5) into a Gröbner basis with respect to an
elimination order such as lex order with t1 > · · · > tr > x1 > · · · > xn.
Now we run into a small problem because the Hilbert driven Buchberger algo-
rithm requires homogeneous inputs. That would be true for the given generators
only in the relatively uninteresting case that di = 1 for all i. The implicitization
operation can be done most easily in that case by standard linear algebra techniques
(see Exercise 8) so computing a Gröbner basis is overkill in a sense.
We could homogenize the generators for I by introducing a new variable as in
§1. But there is a somewhat better alternative that arrives at a homogeneous ideal
in a different, seemingly rather underhanded, way. Namely, we can introduce new
variables in order to write xi as a di-th power. Let xi = ξdi
i , and consider the ideal
˜I = ⟨ξd1
1 −f1(t1, . . . , tr), . . . , ξdn
n −fn(t1, . . . , tr)⟩
in S = k[ξ1, . . . , ξn, t1, . . . , tr]. These generators are a Gröbner basis for ˜I with re-
spect to any monomial order that makes each ξi greater than any monomial contain-
ing only the tj, for the same reason we saw before.
Since this new ideal is homogeneous we can apply the Hilbert driven algorithm
to ˜I. Moreover, applying the result of Exercise 4, the Hilbert-Poincaré series for an
ideal of this form can be written down directly—with no computations—from the
form of the generators:

§2 Hilbert Driven Buchberger Algorithms
563
(6)
PS/˜I(t) =
 n
i=1(1 −tdi)
(1 −t)n+r
.
In the computation of a Gröbner basis for ˜I, it is not difﬁcult to see that every oc-
currence of the variable ξi will be as (ξdi
i )r = ξdir for some r ∈Z≥0. As a result we
will be able to convert back to the original variables xi immediately once we have
found the generators for the elimination ideal
˜I ∩k[ξ1, . . . , ξn].
Here is a calculation illustrating this idea.
Example 11. Let us see how the Hilbert driven Buchberger algorithm could be ap-
plied to convert the given basis for
I = ⟨x −u2 −v2, y −uv, z −u2⟩
to a lex Gröbner basis with u > v > x > y > z and hence ﬁnd the elimination
ideal I ∩Q[x, y, z]. This will be generated by a single polynomial giving the implicit
equation for the variety parametrized by
x = u2 + v2,
y = uv,
z = u2.
As in the general discussion above, we introduce new variables ξ, η, ζ with ξ2 = x,
η2 = y, and ζ2 = z to make the generators of
˜I = ⟨ξ2 −u2 −v2, η2 −uv, ζ2 −u2⟩
homogeneous, and let S = Q[ξ, η, ζ, u, v]. These polynomials also constitute a
Gröbner basis already for any monomial order that makes the ξ2, η2, ζ2 the lead-
ing terms, for instance, lex with ξ > η > ζ > u > v. The Hilbert-Poincaré series
PS/˜I(t) can be written down immediately using (6):
(7)
PS/˜I(t) = (1 −t2)3
(1 −t)5
= 1 + 5t + 12t2 + 20t3 + 28t4 + · · · .
Now, with respect to lex order with u > v > ξ > η > ζ, the leading monomials
of the three ideal generators in the initial G = ( f1, f2, f3) are LM( f1) = u2, LM( f2) =
uv, LM( f3) = u2. the initial terms in the corresponding Hilbert-Poincaré series are
PS/⟨u2,uv⟩(t) = 1 + 5t + 13t2 + · · · .

564
Chapter 10 Additional Gröbner Basis Algorithms
So the initial m′ = 2, Δ = 4 −3 = 1 and we are "missing" one generator in
degree 2. There is also exactly one pair of degree 2, namely (1, 3). We ﬁnd
S( f1, f3)
G = −v2 + ξ2 −ζ2.
This becomes a new element f4 in G and we update the set of pairs to be processed
to B = {(1, 2), (2, 3), (1, 4), (2, 4), (3, 4)}.
Now, we ﬁnd
PS/⟨u2,uv,v2⟩(t) = 1 + 5t + 12t2 + 22t3 + · · · ,
and comparing with (7), m′ = 3 and Δ = 22 −20 = 2. This means we
need to ﬁnd 2 additional elements in degree 3. There are three pairs in degree 3:
(1, 2), (2, 3), (2, 4). With the ﬁrst of these, S( f1, f2)
G = −uη2 + vζ2 and we have
found one new element f5 in degree 3. With f5 included in G, you will check that
S( f2, f3)
G = 0, but S( f2, f4)
G = −uξ2 + uζ2 + vη2 ̸= 0, so this polynomial is
included in G as f6.
At this point
PS/⟨u2,uv,v2,uη2,uξ2⟩(t) = 1 + 5t + 12t2 + 20t3 + 29t4 + · · ·
so m′ = 4, Δ = 29 −28 = 1 and we must ﬁnd 1 new element in degree 4. The
S-polynomial remainder
f7 = S( f2, f5)
G = −ξ2ζ2 + η4 + ζ4
is such an element. Now the Hilbert-Poincaré series agree:
PS/⟨u2,uv,v2,uη2,uξ2,ξ2ζ2⟩(t) = PS/I(t)
and the algorithm terminates. As in the previous example, there are a number of re-
maining pairs that are unnecessary and hence discarded at termination. Substituting
back to the original variables, the polynomial
−xz + y2 + z2
(from f7 above) generates the elimination ideal I ∩Q[x, y, z].
A different (but equally underhanded) method for recovering homogeneity will
be explored in the exercises. You will see that basis conversions as in this example
can also be carried out in an alternative way.
Before leaving this topic, we should mention that there are other Gröbner basis
conversion algorithms. The FGLM algorithm [see FAUGÈRE, GIANNI, LAZARD,
and MORA (1993)] applies for ideals satisfying the conditions of Theorem 6 in
Chapter 5, §3, which are known as zero-dimensional ideals because of the result
of Proposition 6 from §4 of Chapter 9. A second conversion algorithm called the
Gröbner walk was introduced in COLLART, KALKBRENER and MALL (1998) and
applies to general ideals. Both algorithms are also discussed in COX, LITTLE and
O'SHEA (2005), in Chapters 2 and 8, respectively.

§2 Hilbert Driven Buchberger Algorithms
565
EXERCISES FOR §2
1. Show that, as claimed in Example 2,
∞

m=0

n + m −1
m

tm =
1
(1 −t)n .
Hint: The slickest proof is to use induction on n and to note that the induction step
is closely related to the result of differentiating both sides of the equality above with
respect to t.
2. Carefully verify the statements in parts (i)-(iii) of Lemma 5. Use the sketch of a proof
given in the text as an outline for the steps and as a source of hints.
3. Let J = ⟨xα( j) | j = 1, . . . , t⟩be a monomial ideal. Let xi be one of the variables
that appears in the generators and write xα( j) = x
aj
i ˆxˆα( j), where the components of
ˆα(j) ∈Zn−1
≥0 contain the other exponents.
a. Show that J ∩⟨xi⟩is generated by the xα( j) such that aj > 0 and the monomials
xixα( j) for the j such that aj = 0.
b. Using Theorem 14 of Chapter 4, §4, show that J : xi is generated by the x
aj−1
i
ˆxˆα( j) for
the j such that aj > 0 and the xα( j) for the j such that aj = 0.
4. This exercise explores another "divide and conquer" strategy for computing Hilbert-
Poincaré series. Procedures for computing these series often make use of part (a).
a. Assume that the monomial ideal J has the form J = J1 + J2, where J1, J2 are
monomial ideals such that the minimal generating set for J1 contains only the vari-
ables x1, . . . , xp and the minimal generating set for J2 contains only the variables
xp+1, . . . , xn. Let S1 = k[x1, . . . , xp] and S2 = k[xp+1, . . . , xn] and write ¯Ji = Ji ∩Si
for i = 1, 2. Show that the Hilbert-Poincaré series for S/J factors as
PS/J(t) = PS1/ ¯J1(t) · PS2/ ¯J2(t).
b. Use the result from part (a) to give another proof of the result from Exercise 1 above.
c. Use the result from part (a) to prove (6) in the text. Hint: Show ﬁrst that Pk[x]/⟨xd⟩(t) =
(1 −td)/(1 −t).
5. a. What happens in Example 7 if y or z is chosen as the initial pivot variable?
b. Apply the recursive algorithm discussed in the text to compute PS/J(t) for J =
⟨x4y, u2⟩in S = k[x, y, u, v]. Check your result using part (a) of Exercise 4.
c. Apply the recursive algorithm discussed in the text to compute PS/J(t) for J =
⟨x2yz, xy2z, xyz2⟩in S = k[x, y, z].
6. Prove Proposition 8. Hint: Use Proposition 1.
7. In this problem we will consider some additional aspects of the relation between values
of the Hilbert function HFS/J(m) and the form of the Hilbert-Poincaré series PS/J(t) =
NS/J(t)/(1 −t)n, where NS/J(t) ∈Z[t] is the polynomial from Theorem 4.
a. Let J2 ⊆J1 be two monomial ideals in S with NS/J1(t) = 1 + a1t + a2t2 + · · · and
NS/J2(t) = 1 + b1t + b2t2 + · · · . Show that
HFS/J1(s) = HFS/J2(s) for s = 1, . . . , m −1, and HFS/J1(m) < HFS/J2(m)
if and only if
as = bs for s = 1, . . . , m −1, and am < bm.
b. Deduce that if the equivalent conditions of part (a) are satisﬁed, then the following
holds: dim (J1)m −dim (J2)m = bm −am.

566
Chapter 10 Additional Gröbner Basis Algorithms
This shows that all the information needed for the Hilbert driven Buchberger algorithm
is accessible in the numerator polynomial from the rational function form of the Hilbert-
Poincaré series.
8. In the ideal I from (5) suppose fi is homogeneous of degree 1 for all i.
a. Show that the image of the corresponding parametrization mapping from Rr →Rn is
a vector subspace of Rn of dimension at most r.
b. Show how to obtain implicit equations for that subspace by row-reducing a suitable
matrix. Hint: If n > r, then the f1, . . . , fn are linearly dependent. Write (5) in matrix
form with the coefﬁcients of the tj to left of those of the xi on each row. Row-reduce
to ﬁnd rows with zeroes in the columns corresponding to the tj.
9. In this exercise, you will consider some of the calculations in Example 11.
a. Check the computations in degree 3 in the example. In particular, explain why
HFS/⟨u2,uv,v2⟩(3) = 22, so we are looking for 22 −20 = 2 generators in degree 3.
b. Check the computations in degree 4 in the example. In particular, explain why
HFS/⟨u2,uv,v2,uη2,uξ2⟩(4) = 29, and 29 −28 = 1 generator in degree 4 must be found.
10. In this exercise, we will deﬁne a Hilbert function and a Hilbert-Poincaré series for a
homogeneous ideal and relate them to the objects studied in the text. Let I be a homo-
geneous ideal and m ∈Z≥0. Then we can deﬁne HFI(m) = dim Im, where dim is the
dimension as a vector space over k. Using HFI(m), let PI(t) be the generating function
PI(t) =
∞

m=0
HFI(m)tm.
We also write HFS(m) = HFS/⟨0⟩(m) and PS(t) = PS/⟨0⟩(t).
a. Show that with these deﬁnitions HFI(m) = HFS(m) −HFS/I(m) for all m ≥0.
b. Deduce that the Hilbert-Poincaré series satisfy PI(t) = PS(t) −PS/I(t).
c. Show that PI(t) = NI(t)/(1 −t)n for some NI(t) ∈Z[t].
In other words, the Hilbert-Poincaré series PI(t) has the same form given in Theorem 4
of this section. Hilbert functions and Hilbert-Poincaré series can be deﬁned for any
ﬁnitely generated positively graded module over S and the Hilbert-Poincaré series of
any such module has the same form as in the statement of the theorem. See Chapter 5 of
COX, LITTLE and O'SHEA (2005).
We next develop another way to apply the Hilbert driven Buchberger algorithm in impliciti-
zation. Instead of introducing new variables, we can redeﬁne the way degrees are calculated.
To start, suppose S = k[x1, . . . , xn], where deg xi = di is strictly positive for each i. Let
d = (d1, . . . , dn). The d-degree of a monomial xα is the dot product α · d. A polynomial all
of whose terms have the same d-degree is weighted homogeneous.
11. In this exercise we will see that most basic properties of homogeneous polynomials also
hold with a general weight vector d = (d1, . . . , dn).
a. Show that each polynomial f ∈S can be written uniquely as a sum f = 
m fm where
the fm are weighted homogeneous polynomials of d-degree m.
b. An ideal I ⊆S is said to be weighted homogeneous if f ∈I implies fm ∈I for all m.
Show that ideal I is weighted homogeneous if and only if it has a set of generators
that are weighted homogeneous polynomials.
c. If f , g are weighted homogeneous, then using any monomial order, S( f , g) is also
weighted homogeneous, and we get a notion of a weighted homogeneous degree for
each pair in a Gröbner basis computation.
d. If G consists of weighted homogeneous polynomials and h is weighted homogeneous,
then a nonzero remainder h
G is weighted homogeneous of the same d-degree as h.
e. Show that an ideal I is weighted homogeneous if and only if a reduced Gröbner basis
consists of weighted homogeneous polynomials.

§3 The F4 Algorithm
567
12. This exercise studies how the Hilbert driven Buchberger algorithm extends to the
weighted homogeneous case. If I is a weighted homogeneous ideal, the Hilbert func-
tion HFS/I(m) and the Hilbert-Poincaré series PS/I(t) can be deﬁned in the same way as
in the text, except that now, in the formula
HFS/I(m) = dim Sm/Im = dim Sm −dim Im,
the notation Sm refers to the vector space of weighted homogeneous polynomials of
d-degree equal to m (together with the zero polynomial) and Im = I ∩Sm.
a. If I is a weighted homogeneous ideal and > is any monomial order, show that the
Hilbert functions HFS/⟨LT>(I)⟩and HFS/I are equal.
b. Show that the Hilbert-Poincaré series for J = ⟨0⟩has a form similar to that seen in
Example 2, but incorporating the weights di:
PS/⟨0⟩(t) =
1
(1 −td1) · · · (1 −tdn),
so the factors in the denominator are in correspondence with the variables xi. Hint:
Show ﬁrst that if x has degree d, then Pk[x]/⟨0⟩(t) = 1/(1 −td). Then show there is a
factorization as in Exercise 4.
c. Generalize Theorem 4 for a general weight vector d. The rational function denomi-
nator will be the same as in part (b).
d. The Hilbert-Poincaré series PS/J(t) for a monomial ideal J can be computed by a
recursive algorithm as in the case studied in the text. But the degrees of the variables
are needed in order to get the proper generalization of part (iv) of Lemma 5 and to
include the proper factors in the denominators in the base cases. Develop a recursive
algorithm for computing the Hilbert-Poincaré series PS/J(t).
e. Exactly what modiﬁcations (if any) are needed in the Hilbert driven Buchberger
algorithm for weighted homogeneous inputs?
13. Carry out the computations in Example 11 using the approach from the previous
exercises. Let S = Q[x, y, z, u, v] with weight vector d = (2, 2, 2, 1, 1), which makes
x −u2 −v2, y −uv, z −u2 into weighted homogeneous polynomials. You will see that
the results are completely equivalent to the way we presented the calculation in the text,
although the Hilbert-Poincaré series will look different because they are deﬁned using d.
§3
The F4 Algorithm
The F4 algorithm was introduced in FAUGÈRE (1999). We will use this common
terminology although it is perhaps more accurate to regard F4 as a family of algo-
rithms since there are many variants and alternatives. Nevertheless, the algorithms
in the F4 family have the following common features:
• F4 algorithms do not compute S-polynomial remainders one at a time. Instead,
they use techniques from linear algebra—row-reduction of matrices similar to
those described in §1—to accomplish what amounts to simultaneous computa-
tion of several S-polynomial remainders.
• The idea of the F4 algorithms is, in a sense, inspired by the homogeneous case
and F4 algorithms behave especially nicely then. But the algorithms are not re-
stricted to homogeneous input polynomials and, as we will see, signiﬁcantly

568
Chapter 10 Additional Gröbner Basis Algorithms
greater care is required to ﬁnd the correct matrices for the algorithm to work
with when homogeneity is not present. Moreover, F4 algorithms do not necessar-
ily proceed strictly degree by degree.
• The matrix-based Gröbner basis strategy discussed at the end of §1 had no way
to prove termination. In contrast, F4 algorithms use termination criteria derived
from Buchberger's Criterion.
In this section we will present a basic algorithm showing the common framework
of the F4 family, but not incorporating any of the variants and alternatives that pro-
duce major gains in efﬁciency. Even so, a lot of this will require explanation, so we
will present the F4 algorithm in pseudocode form and give a line-by-line overview
with special attention to the trickiest features involved before we state and prove a
theorem concerning its behavior.
Here is the pseudocode for the basic F4 algorithm we will discuss:
Input : F = ( f1, . . . , fs)
Output : G, a Gröbner basis for I = ⟨f1, . . . , fs⟩
G := F
t := s
B := {{i, j} | 1 ≤i < j ≤s}
WHILE B ̸= ∅DO
Select B′ ̸= ∅, B′ ⊆B
B := B \ B′
L :=
'lcm(LM( fi), LM( fj))
LT( fi)
· fi
 {i, j} ∈B′(
M := ComputeM(L, G)
N := row reduced echelon form of M
N+ := {n ∈rows(N) | LM(n) /∈⟨LM(rows(M))⟩}
FOR n ∈N+ DO
t := t + 1
ft := polynomial form of n
G := G ∪{ft}
B := B ∪{{i, t} | 1 ≤i < t}
RETURN G
As in the other algorithms we have considered, G starts out as the set of input poly-
nomials. Any new polynomials generated are simply included in the set G and the
value of t records the cardinality of G. The algorithm maintains a list B of pairs
for which the corresponding S-polynomials are not known to reduce to zero. But

§3 The F4 Algorithm
569
note that B is now a set of unordered pairs. Parallel to the discussion in §1 in the
homogeneous case, we can deﬁne the degree of a pair {i, j} to be the integer
deg(lcm(LM( fi), LM( fj))).
This is the same as the total degree of the leading monomial of both "halves"
(1)
lcm(LM( fi), LM( fj))
LT( fi)
· fi
and
lcm(LM( fi), LM( fj))
LT( fj)
· fj
of the S-polynomial S( fi, fj). However, this is not necessarily the same as the total
degree of the S-polynomial in the nonhomogeneous case. The total degree of the
S-polynomial can be the same, smaller, or larger depending on the other terms in fi
and fj and on the monomial order.
The body of the WHILE loop is repeated until B = ∅, i.e., until all pairs have
been processed. The selection of B′ ⊆B in the line following the WHILE statement
is a ﬁrst place that variants and alternatives come into play. If F did consist of homo-
geneous polynomials, and we wanted to proceed degree by degree as in Theorem 4
in §1, we could take B′ to be the set of all pairs of the minimal degree remaining in
B in each pass. This is also the normal selection strategy suggested by Faugère in
the nonhomogeneous case. But other strategies are possible too.
In the standard approach to computing a Gröbner basis, the next step would be
to compute the remainders S( fi, fj)
G for {i, j} ∈B′. However, as we learned in §9
of Chapter 2, we are not limited to remainders produced by the division algorithm.
We can also use any equation of the form
(2)
S( fi, fj) −c1xα(1)fℓ1 −c2xα(2)fℓ2 −· · · = S( fi, fj)
that leads to a standard representation of S( fi, fj) when the resulting S( fi, fj) is in-
cluded in G. Equation (2) uses a linear combination of xα(1)fℓ1, xα(2)fℓ2, etc.. This
suggests that linear algebra has a role to play and F4 will make crucial use of this
observation, but in a very clever way.
The F4 approach encodes the raw material for reductions as in (2) for all the
pairs in B′ simultaneously. By "raw material," we mean a set H of polynomials and
a corresponding matrix M of coefﬁcients. H is built up in stages, starting from the
polynomials in L, namely the pair of polynomials from (1) for each pair {i, j} ∈
B′. Both are included in L since {i, j} is unordered. The difference of these two
polynomials is the S-polynomial S( fi, fj) but F4 does not compute the difference—
the two "halves" are separate elements of the set L.
Then enough additional polynomials xαfℓare inserted into H so that it has the
information needed to create the new elements to be added to the Gröbner basis.
In more detail, whenever xβ is a monomial of an element of H that is divisible by
some LM( fℓ), then a unique element xαfℓwith xβ = LM(xαfℓ) is included in H. The
intuition behind this comes from (2), where such a monomial xβ cannot appear on
the right-hand side and hence must be canceled by something on the left-hand side.

570
Chapter 10 Additional Gröbner Basis Algorithms
Including xαfℓin H guarantees that we can achieve this. The ComputeM procedure
to be described below generates H and the corresponding matrix of coefﬁcients M.
The matrix M is reduced to the echelon form N in the next step and this linear
algebra calculation effectively produces the equations of the form (2). The "new"
polynomials from N+ have leading terms not divisible by the leading terms of any
of the polynomials corresponding to the rows of M, so they correspond to the S( fi, fj)
in (2). These new polynomials are included in G and the set of pairs is updated. The
claims are that the main loop terminates, and when it does G is a Gröbner basis for
I = ⟨f1, . . . , fs⟩.
The Procedure ComputeM
We next describe in detail how the F4 procedure ComputeM generates the set H dis-
cussed above and the corresponding matrix of coefﬁcients. Given the sets of poly-
nomials L and G, as indicated above, the goal is to produce a set of polynomials H
such that
i. L ⊆H, and
ii. whenever xβ is a monomial appearing in some f ∈H and there exists some fℓ∈
G with LM( fℓ) | xβ, then H contains a product xαfℓwhose leading monomial
equals xβ.
The idea is that H can be constructed with a loop that continues as long as there are
more monomials xβ to be considered.
• First, H is initialized as the set of polynomials in L, namely the
lcm(LM( fi), LM( fj)
LT( fj)
· fi
and
lcm(LM( fi), LM( fj))
LT( fj)
· fj
for {i, j} ∈B′. We see that condition (ii) essentially holds when xβ is the leading
monomial of one of these pairs of polynomials, since H contains both polyno-
mials in the pair. The leading coefﬁcient of fj is inverted too in that element of L
so the second polynomial might differ from the desired form xαfj by a constant
multiple. But that is actually irrelevant for the row reduction that comes next;
those leading monomials xβ are "done" at this point and they are not considered
again.
• Now let xβ be the largest monomial not yet considered appearing in some poly-
nomial in H. If there is some fℓin G such that the leading monomial of fℓdivides
xβ, then we include exactly one product xαfℓwith leading monomial xβ in H. If
there is no such element in G, then we do nothing. The monomial xβ is now also
"done"; so we do not consider it again.
• We claim that continuing in this way as long as there are monomials not yet con-
sidered appearing in polynomials in H eventually gives a set H with the desired
properties.
But of course we must still show that this process eventually terminates.

§3 The F4 Algorithm
571
We will need a notation for the set of monomials (not including the coefﬁcients)
contained in a polynomial f and we write Mon( f) for this. Similarly, if K is a set of
polynomials, then
Mon(K) =

f∈K
Mon( f)
is the set of all monomials contained in some f ∈K. In addition, LM(K) will denote
the set of leading monomials of the f ∈K. To understand the algorithm below,
readers should be aware that we are working under the convention that when new
polynomials are inserted in H, the set Mon(H) updates immediately (even though
there is no explicit command that does that operation). Assuming termination for
the moment, the procedure returns an |H| × |Mon(H)| matrix M containing the
coefﬁcients of all the polynomials in H. As in §1, the monomials in Mon(H) and
the columns of M should be arranged in decreasing order according the monomial
order in question.
Proposition 1. The following procedure ComputeM terminates and computes a set
of polynomials H satisfying (i) and (ii) above. It returns the corresponding matrix
of coefﬁcients M.
Input : L, G = ( f1, . . . , ft)
Output : M
H := L
done := LM(H)
WHILE done ̸= Mon(H) DO
Select largest xβ ∈(Mon(H) \ done) with respect to >
done := done ∪{xβ}
IF there exists fℓ∈G such that LM( fℓ)|xβ THEN
Select any one fℓ∈G such that LM( fℓ)|xβ
H := H ∪
'
xβ
LM( fℓ) · fℓ
(
M := matrix of coefﬁcients of H with respect to Mon(H),
columns in decreasing order according to >
RETURN M
Proof. To show that the algorithm terminates, note ﬁrst that after xβ is selected at
the start of a pass through the WHILE loop, the monomial xβ is included in the set
done so it is not considered again. Any new monomials added to Mon(H) \ done
in that pass must come from a polynomial
xβ
LM( fℓ) · fℓwhere LM( fℓ)|xβ. This has
leading monomial equal to xβ, hence any monomials added to Mon(H) \ done are
smaller than xβ in the speciﬁed monomial order >. It follows that the xβ considered
in successive passes through the loop form a strictly decreasing sequence in the

572
Chapter 10 Additional Gröbner Basis Algorithms
monomial order. In Exercise 1 you will show that this implies no new monomials
will be included in Mon(H) after some point, and ComputeM will terminate because
it is eventually true that done = Mon(H). The ﬁnal set H then has property (i)
by the initialization. Property (ii) holds since all the xβ appearing in H have been
considered at some point.
□
We note that this procedure is called SymbolicPreprocessing in FAUGÈRE (1999)
and in many discussions of F4 algorithms. An example of ComputeM will be given
later in this section.
The Main Theorem
We are now ready to state and prove the main theorem of this section.
Theorem 2. The basic F4 algorithm given above, using the procedure ComputeM
and any selection strategy for the set of pairs B′, terminates and computes a Gröbner
basis for I = ⟨f1, . . . , fs⟩.
Proof. We will prove that the algorithm terminates ﬁrst. In each pass through the
WHILE loop the pairs in the set B′ are removed from B and processed. Note that
one of two things happens—either no new polynomials are included in the set G and
B does not change after B′ is removed, or else new polynomials are included in G
(by way of a nonempty N+) and B is updated to include more pairs from those new
polynomials. We will show that in any pass for which N+ ̸= ∅, writing Gold for the
value of G at the start of the body of the loop and Gnew for the value of G at the end,
there is a strict containment
(3)
⟨LM(Gold)⟩⊊⟨LM(Gnew)⟩.
By the ACC, such a strict containment can happen only ﬁnitely many times before
the chain of monomial ideals stabilizes. From that point on, pairs are only removed
from B, so the main loop eventually terminates with B = ∅.
So we need to show that (3) is valid when N+ ̸= ∅. In fact, in case N+ ̸= ∅,
we claim that none of the leading monomials of the polynomial forms of elements
of N+ are contained in the monomial ideal ⟨LT(Gold)⟩. We show this next with an
argument by contradiction.
Suppose that n is a row of the echelon form matrix N corresponding to a poly-
nomial f such that LM( f) /∈⟨LM(rows(M))⟩, but LM( f) ∈⟨LM(Gold)⟩. The second
statement implies that there exists fℓ∈Gold satisfying LM( fℓ)| LM( f). The mono-
mial xβ = LM( f) must be one of the monomials corresponding to the columns
in M (and N). That means it must be one of the monomials appearing in the ﬁnal
set Mon(H) built up in the ComputeM procedure. However, in that procedure, note
that every time there exists an element fℓ∈G = Gold and xβ ∈Mon(H) such
that LM( fℓ)|xβ, then H is enlarged by including the polynomial
xβ
LM( fℓ) · fℓ. Hence a
row was included in the matrix M which corresponds to a polynomial with leading
monomial xβ. This contradicts what we said above. Hence the claim is established.

§3 The F4 Algorithm
573
It remains to show that the output of the algorithm is a Gröbner basis for the
ideal I generated by the input polynomials. First note that for each pair {i, j} with
1 ≤i, j ≤t for the ﬁnal value of t, the pair {i, j} will have been included in B at
some point and then removed. In the pass through the main WHILE loop in which
it was removed, the set L contained both
lcm(LM( fi), LM( fj))
LT( fi)
· fi
and
lcm(LM( fi), LM( fj))
LT( fj)
· fj
and hence the matrix M contained rows corresponding to each of these polynomials.
The S-polynomial S( fi, fj) is the difference and hence it corresponds to a linear com-
bination of the rows of M. However, the rows of the echelon form matrix N form
a basis for the vector space spanned by the rows of M. Hence S( fi, fj) is equal to a
linear combination of the polynomials corresponding to the rows of N. In Exercise 2
you will show that this yields an standard representation for S( fi, fj) using the set
Gnew at the end of that pass through the main loop. Hence the ﬁnal G is a Gröbner
basis by Theorem 3 of Chapter 2, §9.
□
Here is a small example of the algorithm in action.
Example 3. Let f1 = x2 + xy −1, f2 = x2 −z2 and f3 = xy + 1. Let us compute a
Gröbner basis for I = ⟨f1, f2, f3⟩with respect to grevlex order with x > y > z using
the algorithm given before, leaving some checks to the reader in Exercise 3.
The initial value set of pairs is B = {{1, 2}, {1, 3}, {2, 3}}, where the ﬁrst pair
has degree 2 and the remaining pairs have degree 3. If we use the normal selection
strategy suggested by Faugère, we consider the pair of degree 2 ﬁrst, take B′ =
{{1, 2}}, and remove that pair from B. We have S( f1, f2) = f1 −f2, so the set L and
the initial H in the procedure ComputeM equals {f1, f2}. We note that LM( f3) = xy
divides one of the terms in f1 ∈L and hence we update H to {f1, f2, f3}, making
Mon(H) = {x2, xy, z2, 1}. There are no other monomials in Mon(H) whose leading
terms are divisible by leading monomials from G, hence the matrix M produced by
ComputeM in the ﬁrst pass through the main loop is
M =
⎛
⎝
1 1
0 −1
1 0 −1
0
0 1
0
1
⎞
⎠.
The corresponding row reduced echelon form matrix is
N =
⎛
⎝
1 0 0 −2
0 1 0
1
0 0 1 −2
⎞
⎠
and the set N+ consists of row 3, which corresponds to the polynomial
f4 = z2 −2.

574
Chapter 10 Additional Gröbner Basis Algorithms
This is inserted into G and the set of pairs B is updated to
B = {{1, 3}, {2, 3}, {1, 4}, {2, 4}, {3, 4}}.
There are two pairs of degree 3 and three of degree 4 now. We select B′ =
{{1, 3}, {2, 3}}. Then S( f1, f3) = y f1 −x f3 and S( f2, f3) = y f2 −x f3, so L =
{y f1, y f2, x f3}. The procedure ComputeM starts with H = L and appends the new
polynomial y f3 to H since the leading term of that polynomial equals the xy2 con-
tained in y f1, Moreover, the yz2 in y f2 is divisible by LT( f4), so y f4 is also appended
to H. At this point no further terms in the polynomials in H are divisible by leading
terms from G, so H = {y f1, y f2, x f3, y f3, y f4}, Mon(H) = {x2y, xy2, yz2, x, y}, and
M =
⎛
⎜
⎜
⎜
⎜
⎝
1 1
0 0 −1
1 0 −1 0
0
1 0
0 1
0
0 1
0 0
1
0 0
1 0 −2
⎞
⎟
⎟
⎟
⎟
⎠
.
When we row-reduce M, we ﬁnd that N+ contains one new entry corresponding to
f5 = x + 2y.
Note one feature of F4 in the nonhomogeneous case—new polynomials found and
inserted into G can have lower degrees than any previously known polynomials, and
the computation does not necessarily proceed degree by degree as in the homoge-
neous case from §1.
In fact at this point three of the remaining pairs ({1, 5}, {2, 5}, and {3, 5}) have
degree 2 and you will check in Exercise 3 that the next M is a 6 × 5 matrix of rank
4 corresponding to H = {f1, f2, f3, x f5, y f5, f4} with Mon(H) = {x2, xy, y2, z2, 1}.
There is one new leading term that we ﬁnd here from the row reduced echelon matrix
N, namely the y2 in
f6 = y2 −1
2.
At this point the remaining pairs do not introduce any new leading terms, so
you will check in Exercise 3 that the algorithm terminates with G = {f1, . . . , f6}, a
nonreduced Gröbner basis for I.
Comments and Extensions
In FAUGÈRE (1999) it is suggested that the F4-type algorithms are intended primar-
ily for use with graded orders such as grevlex. If a lex Gröbner basis is desired, then
a grevlex computation, followed by a Gröbner basis conversion using one of the
algorithms mentioned in §2 might in fact be the best approach.
Our basic F4 algorithm is intended to illustrate the key features of the F4 family,
but it does not incorporate some of the improvements discussed in FAUGÈRE (1999).
The improved F4 algorithms discussed there are a deﬁnite advance over the im-
proved Buchberger algorithm from Chapter 2 for nonhomogeneous Gröbner bases.

§3 The F4 Algorithm
575
In fact F4 was the ﬁrst algorithm to succeed on several notoriously difﬁcult bench-
mark problems. However, the advantages of this approach are probably difﬁcult to
appreciate in small examples such as Example 3. Those advantages come to the
fore only in larger problems because they stem mainly from the possibility of ap-
plying well-developed and efﬁcient algorithms from linear algebra to the task of
row-reducing the matrices M. For ideals in larger numbers of variables generated by
polynomials of higher degrees, the matrices involved can be quite large (e.g., hun-
dreds or thousands of rows and columns). Moreover, those matrices are typically
quite sparse (i.e., a large fraction of their entries are zero) so the data structures
used to represent them efﬁciently in an actual implementation might involve lists
rather than large arrays. There are quite efﬁcient algorithms known for this sort of
computation and they can easily be incorporated into the F4 framework. It is also
unnecessary to compute the full row-reduced echelon form of M—any "triangu-
lar" form where the leading entries in the nonzero rows are in distinct columns will
accomplish the same thing [see part (e) of Exercise 2].
As indicated before, there are many ways to modify the basic F4 algorithm to im-
prove its efﬁciency or to take advantage of special features of a particular problem:
• Different selection strategies for the set B′ are possible.
• If a reduced Gröbner basis is desired, each time a new element is adjoined to G,
it can be used to reduce all the other elements in G.
• Additional criteria can be used to discard unnecessary pairs as we did in the
improved Buchberger algorithm from §10 of Chapter 2. (This would be possible
for several of the pairs in Example 3, for instance, since the leading monomials
are relatively prime.)
• In some cases there will be several different fℓwhose leading monomials divide
the monomial xβ ∈Mon(H) in the course of carrying out the ComputeM proce-
dure. Different strategies for choosing which xαfℓshould be inserted into H are
possible and the matrices M will depend on those choices.
We will not pursue these variations and other modiﬁcations farther here—interested
readers should consult FAUGÈRE (1999).
EXERCISES FOR §3
1. Complete the proof of Proposition 1 and show that the main loop in ComputeM always
terminates. Hint: Monomial orders are well-orderings.
2. In this exercise, you will prove the last claim in the proof of Theorem 2.
a. Let G be any ﬁnite set of polynomials and let H be a ﬁnite subset of
{xαg | g ∈G, xα a monomial}.
Let M be the matrix of coefﬁcients of H with respect to Mon(H) and let N be the
row reduced echelon form of M. Show that if f is the polynomial corresponding to any
linear combination of the rows of N, then either f = 0, or else LM( f ) is equal to LM(h)
for the polynomial h corresponding to one of the rows of N. Hint: The idea is the same
as in Proposition 10 from §1.
b. Now let N+ be the set of rows in N whose leading entries are not in the set of leading
entries from M. In the situation of part (a), show that every f in the linear span of H

576
Chapter 10 Additional Gröbner Basis Algorithms
has a standard representation with respect to the set G ∪N+, where N+ is the set of
polynomials corresponding to the rows N+.
c. Let Gold be the set G at the start of a pass through the main WHILE loop of the basic
F4 procedure. At the end of that pass, G has been enlarged to Gnew = Gold ∪N+, where
N+ is deﬁned as in part (b). Let f be the polynomial form of any linear combination of
the rows of that N. Show that f has a standard representation with respect to Gnew.
d. Deduce that if {i, j} is any pair in the set B′ on a pass through the main WHILE loop,
then the S-polynomial S( fi, fj) has a standard representation in terms of Gnew at the end
of that pass.
e. Show that parts (a), (b), (c), and (d) remain true if instead of the row reduced echelon
form N, we use any N′ obtained from M by row operations in which the leading entries
in the nonzero rows appear in distinct columns, and a submatrix (N′)+ is selected in
the same way used to produce N+ above.
3. Check the claims made in the computation in Example 3.
4. Apply the basic F4 algorithm to compute a grevlex Gröbner basis for the ideal
I = ⟨x + y + z + w, xy + yz + zw + wx, xyz + yzw + zwx + wxy, xyzw −1⟩.
This is called the cyclic 4 problem and is discussed in Example 2.6 in FAUGÈRE (1999).
The analogous cyclic 9 problem in 9 variables was one of the notoriously difﬁcult bench-
mark problems ﬁrst solved with the F4 algorithm. See FAUGÈRE (2001).
5. Suppose I = ⟨f1, . . . , fs⟩with fi homogeneous.
a. If the normal selection strategy is used to produce B′ and all pairs of some degree m
are considered, exactly how is the matrix M generated by the F4 ComputeM procedure
for that m related to the matrix Mm constructed in §1? Hint: They will not usually be
the same.
b. Which approach is more efﬁcient in this case, the F4 approach or the ideas from §1?
Why?
§4
Signature-based Algorithms and F5
In this section we will present a brief overview of another family of Gröbner ba-
sis algorithms, the signature-based algorithms stemming from FAUGÈRE (2002),
including the F5 algorithm presented there. These algorithms have been the sub-
ject of a large number of articles by many different authors proposing different
modiﬁcations and reﬁnements. This family is large and replete with complicated
interconnections. (It overlaps the F4 family as well, in a sense that we will describe
later, so the situation is even more complicated.) This tangled story is due to two
circumstances.
• F5 and related algorithms have been extremely successful in solving previously
intractable problems, especially in cryptographic applications. So they have at-
tracted a tremendous amount of interest in the computational algebra research
community.
• However, Faugère's presentation of the original F5 algorithm did not contain
a complete termination and correctness proof. So the termination of F5 in every
case was only conjectural until about 2012. Many of the articles mentioned above
presented versions of the F5 algorithm in special cases or with other additional
or modiﬁed features designed to facilitate termination proofs.

§4 Signature-based Algorithms and F5
577
There are now complete proofs of termination and correctness for the original F5,
but these are still sufﬁciently complicated that we will not present them here. We
will not trace the history of the different attempts to prove termination and correct-
ness either. For that, we direct interested readers to the excellent and exhaustive
survey of this area in EDER and FAUGÈRE (2014). Our goal is to present a simple
algorithm in the signature-based family that represents a synthesis of much of this
work, following parts of Eder and Faugère's survey. Two disclaimers are in order,
though. First, this family makes use of a number of new ideas and data structures,
some of which require the mathematical theory of modules over a ring for a full
development. Since that is beyond the scope of this text, we will need to develop
some of this in an ad hoc fashion. Second, we will omit some proofs to streamline
the presentation.
Motivating the Signature-Based Approach
One of the principal new features of the signature-based family of Gröbner basis
algorithms is the systematic use of information indicating how the polynomials gen-
erated in the course of the computation depend on the original input polynomials
f1, . . . , fs. The goal is to eliminate unnecessary S-polynomial remainder calculations
as much as possible by exploiting relations between the fi. Some of those relations
can be written down immediately—see (3) below for an example—while others are
found as consequences of steps in the calculation. We will use the following exam-
ples to motivate the approach used in these algorithms.
Example 1. Let R = Q[x, y, z] and use grevlex order with x > y > z. Consider
f1 = x2 + z and f2 = xy −z. In a Gröbner basis computation by Buchberger's
algorithm we would immediately ﬁnd
S( f1, f2) = y f1 −x f2 = xz + yz.
This is also the remainder on division by {f1, f2} so we have a new basis element f3
with LT( f3) = xz. But now
(1)
S( f1, f3) = z f1 −x f3 = z(x2 + z) −x(xz + yz) = −xyz + z2 = −z f2.
Hence S( f1, f3)
{f1,f2,f3} = 0 and we see this second remainder calculation turned out
to be unnecessary—it produced no useful information.
The criterion from Proposition 8 of Chapter 2, §10 does not capture what is
going on in this example. After computing S( f2, f3) = −y2z −z2 = f4, neither of
the pairs (1, 2) and (2, 3) is in the set B in the improved Buchberger algorithm. But
LT( f2) = xy does not divide lcm(LT( f1), LT( f3)) = x2z. So we would not be able to
rule out computing and reducing S( f1, f3) on that basis.
The S-polynomials and remainders in this computation each have the form
(a1, a2) · ( f1, f2) = a1 f1 + a2 f2

578
Chapter 10 Additional Gröbner Basis Algorithms
for some vector of polynomials (a1, a2) ∈Q[x, y, z]2. Let us keep track of the vector
(a1, a2) separately rather than immediately going to the combination a1 f1 + a2 f2.
When we do this, we see the beginnings of a systematic way to see why
S( f1, f3) = −zf2. Rewriting everything in terms of f1, f2, we have
S( f1, f3) = zf1 −xf3
= zf1 −x(yf1 −xf2)
= (xy + z, x2) · ( f1, f2).
The ﬁrst component of the vector (−xy + z, x2) is nothing other than −f2 and the
second component is f1 −z. Hence
(2)
S( f1, f3) = (−f2, f1 −z) · ( f1, f2)
and the vector (−f2, f1 −z) is one way of producing S( f1, f3) from the input polyno-
mials f1, f2. Note that we also have the relation
(3)
(−f2, f1) · ( f1, f2) = −f2f1 + f1f2 = 0.
Subtracting (3) from (2), we obtain
S( f1, f3) = S( f1, f3) −0
= (−f2, f1 −z) · ( f1, f2) −(−f2, f1) · ( f1, f2)
= (0, −z) · ( f1, f2) = −zf2.
Hence we recover the equation S( f1, f3) = −zf2 found earlier.
To say more about how signature-based algorithms would make use of such cal-
culations, we need a brief digression about vectors of polynomials. A nonzero vector
of polynomials can be written uniquely as a sum of vectors with just one nonzero
component, and where that nonzero component is a constant in k times a monomial.
Moreover, each of those special vectors can be written using the usual standard basis
vectors e1, . . . , en. We will call those the terms appearing in the vector. For instance,
the vector (a1, a2) = (x2 + 3yz, −xz) is the sum of the terms
(x2 + 3yz, −xz) = (x2, 0) + (3yz, 0) + (0, −xz) = x2e1 + 3yze1 −xze2.
In each vector of polynomials, we will see below in (5) that it is possible to order
the terms. For instance, here we might ﬁrst decree that any term containing e2 is
larger than any term containing e1, and then use grevlex order on Q[x, y, z] on the
monomials in terms containing the same ei. This will single out a largest term in
a vector of polynomials in a fashion analogous to identifying the leading term in a
single polynomial. For instance, in Example 1, we saw that S( f1, f3) uses the vector
(xy + z, x2) = (xy, 0) + (z, 0) + (0, x2) = −xye1 + ze1 + x2e2.

§4 Signature-based Algorithms and F5
579
Here the largest term is x2e2 = (0, x2).
For Example 1, a signature-based algorithm would proceed as above to compute
the vector (xy + z, x2) giving S( f1, f3) in terms of ( f1, f2). Then, it would recognize
that this has the same leading term as the vector (−f2, f1) from (3), which is known
from the start. Since the right-hand side of (3) is zero, the algorithm would know that
computing S( f1, f3) is unnecessary—Proposition 14 below guarantees that S( f1, f3)
reduces to zero in this situation.
Example 2. Now consider f1 = x2 + xy and f2 = x2 + y in Q[x, y] and use grevlex
order with x > y. Using some of the ideas from Example 1 and the following dis-
cussion, note that the S-polynomial S( f1, f2) can be written as
S( f1, f2) = (1, −1) · ( f1, f2) = xy −y.
Since that does not reduce to zero under {f1, f2}, we would include f3 as a new
Gröbner basis element. Let us write a = (1, −1) for the vector producing S( f1, f2).
The largest term in a according to the order proposed in Example 1 is the −e2.
Now consider what happens when we compute S( f1, f3). We have
S( f1, f3) = y f1 −x f3 = y f1 −x( f1 −f2) = (y −x) f1 + x f2.
Hence this S-polynomial corresponds to the vector b = (y−x, x). The S-polynomial
itself is S( f1, f3) = xy2 + xy, and
S( f1, f3)
{f1,f2,f3} = y2 + y.
This gives another Gröbner basis element. Similarly,
S( f2, f3) = y f2 −x f3 = y f2 −x( f1 −f2) = −x f1 + (x + y) f2.
Hence this S-polynomial corresponds to the vector c
=
(−x, x + y). The
S-polynomial itself is S( f2, f3) = −xy + y2, and we ﬁnd
S( f2, f3)
{f1,f2,f3} = y2 + y.
Note that these two remainder calculations have led to precisely the same result! So
if we had included f4 = y2 + y in the set of divisors, then the second remainder
would be zero. This means that one of these computations is unnecessary and it is
natural to ask whether we could have predicted this from the form of the vectors
b and c. We will see in Proposition 15 that the answer is yes, because the largest
terms in the order proposed above are the same—in both vectors, the largest term
is the xe2. We will see that once we have done one computation, the other becomes
unnecessary.
It might still be unclear exactly why we are claiming that we have identiﬁed
something of interest in these examples, but this should become more transparent
later in the section. For now, it will have to sufﬁce to say that if I = ⟨f1, . . . , fs⟩is

580
Chapter 10 Additional Gröbner Basis Algorithms
any collection of polynomials, then the S-polynomials and remainders produced in
the course of a Gröbner basis computation can all be written as
(a1, . . . , as) · ( f1, . . . , fs) = a1 f1 + · · · + as fs
for certain a = (a1, . . . , as) in k[x1, . . . , xn]s. The orderings on terms in vectors
described before generalize immediately. We will see that there are key features
of the vectors a corresponding to some S-polynomials that make computing the S-
polynomial remainder unnecessary. Moreover as in the examples above, those key
features can be recognized directly from the largest term in the vector and other
information similar to (3) known to the algorithm. In particular, it is not necessary to
compute the combination a1 f1 +· · ·+as fs to recognize that a key feature is present.
This can lead to the sort of increase in efﬁciency mentioned in the introduction to
this chapter. This approach is the principal new feature of F5 and other signature-
based Gröbner basis algorithms. The signature of a vector is another name for the
largest term. We will give a precise deﬁnition of "largest term" in (4) below.
Vectors of Polynomials and Syzygies
Now we indicate some of the theoretical background needed to make this all more
precise. Let R = k[x1, . . . , xn]. In Rs we have vector addition (using component-
wise sums in R ) and scalar multiplication by elements of R (using component-wise
products in R ). It is these operations that make Rs an example of a module over the
ring R, but we will not use that language.
Since each element f of the ideal I = ⟨f1, . . . , fs⟩has the form f = s
i=1 ai fi for
some ai ∈R, we have an onto mapping
φ : Rs −→I
a = (a1, . . . , as) 
−→
s

i=1
ai fi.
As in the examples above, we will work with the vectors a ∈Rs, and with the
polynomials φ(a) ∈I. If s > 1, the mapping φ is never one-to-one. In particular, φ
will map nonzero vectors in Rs to 0 in R. We introduce the following terminology.
Deﬁnition 3. We say a ∈Rs is a syzygy on the polynomials in ( f1, . . . , fs) if
φ(a) =
s

i=1
ai fi = 0 ∈R.
The deﬁnition of a syzygy given here generalizes the deﬁnition of a syzygy on
the leading terms of a set of polynomials from Chapter 2, §10. The difference is that
here, the sum involves the whole polynomials f1, . . . , fs, not just their leading terms.
For example, the vector (−f2, f1) = −f2e2 + f1e2 from Example 1 is a syzygy on
( f1, f2). More generally if ( f1, . . . , fs) is any list of polynomials, then for each pair

§4 Signature-based Algorithms and F5
581
(i, j) with 1 ≤i < j ≤s we have an analogous syzygy
kij = −fjei + fiej,
known as a Koszul syzygy. These are often considered to be "trivial" (i.e., not
especially interesting) as syzygies. But we will see that they play a key role in F5
and other signature-based algorithms because they are known directly from the fi.
A more interesting example comes from the relations ℓ1 f1+ℓ2 f2+ℓ3 f3 = 0 found in
Exercise 7 from §1. These equations correspond to syzygies (ℓ1, ℓ2, ℓ3) ∈Q[x, y, z]3
on the ( f1, f2, f3).
Signatures and s-Reduction
The s-tuples g = (g1, . . . , gs) in Rs can be expressed as sums of terms cxαei, where
the ei are the standard basis vectors in Rs and c ∈k. A monomial order > on R
can be extended to an analogous order on the terms cxαei in several ways. This is
discussed in detail, for instance, in Chapter 5 of COX, LITTLE and O'SHEA (2005).
In this overview, we will use just one, the POT or position-over-term order extending
the order > on R. As usual, we ignore the coefﬁcient in k and then the POT order is
deﬁned by
(4)
xαei >POT xβej ⇐⇒i > j, or i = j and xα > xβ.
For example, if R = Q[x, y, z] and > is lex order with x > y > z, then x3ye2 >POT
x4e1 since the index 2 from the e2 in the ﬁrst term is greater than the index 1 from
the e1 in the second. On the other hand, x3ze1 >POT xy5e1 since both terms include
e1 and x3z > xy5 in lex order.
See Exercise 1 for some of the general properties of the >POT order, which are
parallel to properties of monomial orders in R = k[x1, . . . , xn]. It would be possible
to write LT(g) for the largest term in the >POT order. However, to be consistent with
the literature, we will use the following terminology.
Deﬁnition 4. Let g = (g1, . . . , gs) ∈Rs. Then the signature of g, denoted s(g), is
the term appearing in g that is largest in the >POT order.
For example, if g = (x3, y, x + z2) in Q[x, y, z]3, with >POT extending the grevlex
order with x > y > z, then s(g) = z2e3.
We next consider what is known as s-reduction (or signature reduction) of vec-
tors. Given two vectors g, h in Rs, an s-reduction produces a new vector, a linear
combination of the form g −cxαh. This uses the vector sum and the component-
wise multiplication of the vector h by cxα ∈R mentioned before.
Deﬁnition 5. Let g, h ∈Rs. Let xα ∈R be a monomial and let c ∈k. We say that
g −cxαh ∈Rs is the result of an s-reduction of g by h, or that h s-reduces g to
k = g −cxαh, if

582
Chapter 10 Additional Gröbner Basis Algorithms
(i) There is a term bxβ in the polynomial φ(g) ∈R such that
LT(cxαφ(h)) = cxαLT(φ(h)) = bxβ,
and
(ii) s(g) ≥POT s(xαh).
In the case of equality in part (ii), we say the reduction is a singular s-reduction;
otherwise the reduction is a regular s-reduction.
There are two important aspects of this deﬁnition. On the level of the polynomial
φ(g −cxαh), the s-reduction performs an operation analogous to one step in the
division algorithm, in which one term of φ(g) is canceled by the leading term of
cxαφ(h) [see part (i)]. But then there is an additional condition coming from the
signatures of the corresponding vectors in Rs [see part (ii)] that puts restrictions on
which of these reduction steps are allowed. On the level of the vectors, the idea of
s-reductions is that they either leave the signature of g unchanged (the regular case),
or else cancel a term in g and yield a new signature that is smaller in the >POT order
(the singular case).
Example 6. Let R = Q[x, y, z] with grevlex order with x > y > z and let f1 =
x2y + xy, f2 = xy2 −xy and f3 = xy −xz. Let g = (y, −x, 0), so the largest term for
the >POT order is s(g) = −x e2. We have
φ(g) = y(x2y + xy) −x(xy2 −xy) = −x2y + xy2.
Consider the term −x2y in this polynomial. This is equal to LT( f1), so we can cancel
that term by adding f1. Moreover, the vector h = (1, 0, 0) = e1 corresponding to f1
satisﬁes
s(g) = x e2 >POT s(h) = e1.
Hence we have a regular s-reduction, and the result is
g + h = (y, −x, 0) + (1, 0, 0) = (y + 1, −x, 0),
for which
φ(g + h) = xy2 + xy.
Note that this does not change the signature: s(g + h) = s(g) = −xe2.
The leading term in f3 could also be used to cancel the term −x2y in φ(g). But
note that the vector corresponding to f3 is e3 and
s(g + xe3) = s(ye1 −x e2 + xe3) = xe3,
which is different from the signature of g and larger in the >POT order. Therefore,
this reduction of φ(g) does not correspond to an s-reduction of g.
Allowing several s-reductions in sequence yields the following notions.
Deﬁnition 7. Let H be a set of elements of Rs. We say g is s-reduced to k by H if
there is a ﬁnite sequence of s-reductions that takes g to k:

§4 Signature-based Algorithms and F5
583
g −c1xα(1)h1 −· · · −cℓxα(ℓ)hℓ= k
with hi ∈H and ci ∈k.
When no regular s-reductions are possible on g using H, then we have a vector
analogous to a remainder on division.
Deﬁnition 8. We say g is regular s-reduced with respect to H if g has no regular
s-reductions by any element of H.
It is possible to develop a signature-based version of the division algorithm in R to
produce regular s-reduced vectors with respect to any H. Since the idea is essentially
the same as in the usual division algorithm, we will pursue that in Exercise 5. Such
a process will be used in the signature-based algorithm presented later.
Signature Gröbner Bases
For convenience, we will assume from now on that the given generators of I and
any additional polynomials produced in the course of a Gröbner basis computation
have leading coefﬁcient equal to 1. Such polynomials are said to be monic. This will
simplify the process of forming S-polynomials and the corresponding vectors. We
begin with the notions corresponding to the S-polynomials in the usual case.
Deﬁnition 9. Let g, h ∈Rs correspond to monic φ(g) and φ(h) in R. Then the
corresponding S-vector is the element of Rs deﬁned by:
S(g, h) = lcm(LM(φ(g)), LM(φ(h)))
LM(φ(g))
· g −lcm(LM(φ(g)), LM(φ(h)))
LM(φ(h))
· h.
It is not difﬁcult to see that
(5)
φ(S(g, h)) = S(φ(g), φ(h)),
where the right-hand side is the usual S-polynomial (see Exercise 3). Hence this
deﬁnition is compatible with the usual non-signature-based deﬁnition.
The formula in Deﬁnition 9 expresses S(g, h) as a difference of two vectors.
The S-vector is singular if these vectors have the same leading term and is regular
otherwise. In the signature-based approach, the regular S-vectors play a much larger
role than the singular ones. The important cancellations of leading terms happen
when we apply φ to the S-vector, not by reason of cancellations between leading
terms in the S-vector (as happens in the singular case).
In the basic Buchberger algorithm, we check to see whether S-polynomial re-
mainders are equal to 0. There is also a notion of "reduction to zero" in signature-
based algorithms, but this term has a somewhat different meaning here. The idea
behind this is the same as the observation in Example 1.
Deﬁnition 10. We say that g ∈Rs s-reduces to zero by some set of vectors H if
there exists a syzygy k such that g can be s-reduced to k using vectors from H.

584
Chapter 10 Additional Gröbner Basis Algorithms
In particular, for a vector g that s-reduces to zero, the syzygy k will typically be a
nonzero vector of Rs, but by the deﬁnition of syzygy φ(k) = 0 in R. See Exercise 2
for an example.
Deﬁnition 11. Let I be an ideal in R and let G = {g1, . . . , gt} ⊆Rs where all φ(gi)
are monic. Then G is said to be a signature Gröbner basis for I if every element
of Rs s-reduces to zero using G. Similarly, if M = xαei is a term in Rs, we say G is
a signature Gröbner basis below M if all h ∈Rs with s(h) <POT M s-reduce to
zero using G.
There is an analog of the usual Buchberger Criterion as well.
Proposition 12. Let I be an ideal in R and G = {g1, . . . , gt} ⊆Rs and assume
φ(gi) is monic for all i.
(i) Then G is a signature Gröbner basis for I if and only if all S-vectors S(gi, gj)
with 1 ≤i < j ≤t and all ei with 1 ≤i ≤t s-reduce to zero using G.
(ii) Similarly, if M = xαei is a term in Rs, G is a signature Gröbner basis below M
for I if and only if all S-vectors S(gi, gj) and all ei with signature less than M in
the >POT order s-reduce to zero using G.
The requirement that all the ei s-reduce to zero in part (i) ensures that φ(G) is a
Gröbner basis for I and not for some ideal strictly contained in I. The connection
between signature Gröbner bases and Gröbner bases in the usual sense is given by
the following statement.
Proposition 13. If G = {g1, . . . , gt} ⊆Rs is a signature Gröbner basis for I, then
φ(G) = {φ(g1), . . . , φ(gt)}
is a Gröbner basis for I.
We omit the proofs of Propositions 12 and 13. The idea for Proposition 13 is that
Deﬁnition 11 and (5) imply that every element of I = φ(Rs), including each of the
usual S-polynomials S(φ(gi), φ(gj)), satisﬁes
S(φ(gi), φ(gj)) →φ(G) 0.
We now want to identify situations where it can be seen immediately that it is not
necessary to reduce an S-vector. Our ﬁrst result requires some terminology. If g and
h are in Rs, we say that s(g) divides s(h), or s(h) is divisible by s(g), if
s(h) = cxγs(g)
for some monomial xγ in R and some c ∈k. This implies the terms s(g) and s(h)
contain the same standard basis vector eℓ. We then have the following proposition
that covers the situation encountered in Example 1.
Proposition 14. Let G = {g1, . . . , gt} and h = S(gi, gj). If G is a signature Gröbner
basis below s(h) for I and there is a syzygy k such that s(k) divides s(h), then h
s-reduces to zero using G.

§4 Signature-based Algorithms and F5
585
Proof. If k is a syzygy as in the statement of the proposition, then you will show that
xγ k is also a syzygy in Exercise 4. This new syzygy has signature s(xγ k) = s(h).
Therefore, s(h −xγ k) <POT s(h). Since we assume G is a Gröbner basis below
s(h), the vector h −xγ k s-reduces to zero by G. But then the same is true for h by
Deﬁnition 10 and the fact that the collection of syzygies is closed under sums in Rs
(again, see Exercise 4).
□
In Example 1, for instance, we had f1 = x2 + z and f2 = xy −z. and the ﬁrst step
in a Gröbner basis computation found
f3 = S( f1, f2) = xz + yz.
The Koszul syzygy k = −f2e1 + f1e2 has s(k) = x2e2. Moreover, knowing the
vector corresponding to f3, namely ye1 −xe1, the vector h for S( f1, f3) is
S( f1, f3) = z f1 −x f3 ⇒h = ze1 −x(ye1 −xe2) = (−xy + z)e1 + x2e2.
Note that s(h) = x2e2 divides s(k). Provided we have already computed a a sig-
nature Gröbner basis in signatures below x2e2, then Proposition 14 applies and
h s-reduces to zero (in the sense of Deﬁnition 10). As we will see, our signature
Gröbner basis algorithm will process S-vectors in increasing order of their signa-
tures, which will ensure that by the time the computation corresponding to S( f1, f3)
is reached, Proposition 14 will apply and the unnecessary reduction will be detected.
The second result will cover the situation seen in Example 2.
Proposition 15. Let g, h ∈Rs with s(g) = s(h) and let G be a signature Gröbner
basis below this signature. If both g and h are regular s-reduced with respect to G,
then φ(g) = φ(h).
Proof. Aiming for a contradiction, suppose that φ(g) ̸= φ(h). Then by assumption
s(g −h) is smaller in the >POT order. But this implies that g −h s-reduces to zero
under G. Interchanging the roles of g and h if necessary, we can assume that the
>POT leading term of g −h appears in g. But this contradicts the assumption that
both g and h were regular s-reduced.
□
The main consequence of this proposition is that if S-vectors are processed in
increasing order by signature, at most one S-vector with any given signature need
be processed. For instance, in Example 2, we found that the S-vectors corresponding
to S( f1, f3) and S( f2, f3) had the same signature and the remainders were equal.
In some earlier presentations of F5-type algorithms, Proposition 14 and Propo-
sition 15 were used to develop two separate criteria for eliminating unnecessary
S-vectors (a syzygy criterion and a rewriting criterion). But in fact, Eder and Faugère
note in §7 of EDER and FAUGÈRE (2014) that those two criteria can be combined
into one, since both amount to checking whether the signature of an S-vector is divis-
ible by the signature of a known vector—either a syzygy or a previously computed
element of the intermediate signature Gröbner basis.

586
Chapter 10 Additional Gröbner Basis Algorithms
A Signature-Based Algorithm
We will present a signature-based algorithm following an outline very much like
that of Buchberger's algorithm to make connections with other approaches we have
studied more apparent. Faugère's presentation of the original F5 algorithm looked
quite different.
Input: F = ( f1, . . . , fs), fi ∈R
Output: φ(G), a Gröbner basis for I = ⟨f1, . . . , fs⟩
G := ∅
P := {e1, . . . , es}
S := {−fjei + fi ej | 1 ≤i < j ≤s}
WHILE P ̸= ∅DO
g := the element of smallest signature in P
P := P \ {g}
IF Criterion(g, G ∪S) = false THEN
h := a regular s-reduction of g by G
IF φ(h) = 0 THEN
S := S ∪{h}
ELSE
h :=
1
LC(φ(h))h
P := P ∪{S(k, h) | k ∈G and S(k, h) is regular}
G := G ∪{h}
RETURN φ(G)
In this algorithm, G represents the current intermediate signature Gröbner basis,
and S represents a set of known syzygies on the input polynomials f1, . . . , fs. The
initial value of S is the set of Koszul syzygies—syzygies of the form encountered in
Example 1—namely, the vectors
kij = −fjei + fiej,
for all pairs of indices 1 ≤i < j ≤s. Note that the choice of signs here and the
deﬁnition of the >POT order makes s(kij) = LM( fi)ej (recall that we assume all
polynomials occurring are monic). The initial value of G is ∅and the set of standard
basis vectors in Rs [with φ(ei) = fi] are placed in a set P that will also contain
S-vectors of pairs later in the computation. Each of the ei will be considered as the
algorithm proceeds and each of them will either s-reduce to zero immediately, or
else an element will be inserted in G that will imply ei is s-reduced to zero by G.
The condition on the ei from Proposition 12 will hold because of this.

§4 Signature-based Algorithms and F5
587
We may assume that the set P is always sorted in increasing order according to
the signature. The computation will proceed by increasing signatures and we will
have intermediate values of G that are signature Gröbner bases in signatures below
some term M at all times. This means that, by Proposition 15, only regular S-vectors
need to be saved and processed.
The algorithm is organized as a loop that continues as long as P is not empty. In
each pass, the g remaining in P with the smallest signature is selected and removed.
The algorithm applies a Boolean function Criterion based on Propositions 14 and 15
to discard that g immediately, if possible. This criterion tests s(g) for divisibility by
the signatures of the elements of G ∪S and returns the value true if s(g) is divisible
by one of those signatures. If the function Criterion returns false, then the algorithm
s-reduces g by G. The signature division algorithm from Exercise 5 would be used
for this. If the remainder h is a syzygy, then h is included in S. If not, then a constant
multiple of h [making φ(h) monic] is included in G and the set P is updated with
additional S-vectors. When P = ∅, the hypotheses of Proposition 12 are satisﬁed, so
G will be a signature Gröbner basis and by Proposition 13 we have a Gröbner basis
of I in the usual sense as well.
A termination and correctness proof for the Buchberger-style signature-based
algorithm can be constructed using EDER and FAUGÈRE (2014) and the references
in that survey article. The proof for this form is not as hard as that for the original
version of F5, which does things somewhat differently.
Example 16. As we have done with the other algorithms presented in this chapter,
we will trace a part of the computation of a Gröbner basis using the signature-
based algorithm described above. Let us take R = Q[x, y, z, t] with grevlex order
and x > y > z > t. Let
I = ⟨xy −yt, x2 −zt, z3 −t3⟩
and call the generators f1, f2, f3 respectively. Note that the grevlex leading terms are
the ﬁrst terms in each case and that these are listed in increasing order. At the start
of the ﬁrst pass through the main loop we have
G = ∅,
S = {−(x2 −zt)e1 + (xy −yt)e2, −(z3 −t3)e1 + (xy −yt)e3,
−(z3 −t3)e2 + (x2 −zt)e3},
P = {e1, e2, e3}.
The ﬁrst and second passes through the main loop only remove e1 and then e2
from P, insert them it into G, and then update P. On the second pass, S(e1, e2) =
x e1 −y e2 is a regular S-vector, and P is updated to
P = {x e1 −y e2, e3}.
(Note how this is a bit different from the set-up in the traditional Buchberger algo-
rithm, but accomplishes the generation of S-vectors in another way.)

588
Chapter 10 Additional Gröbner Basis Algorithms
In the third pass, we are now ready to consider the S-vector from f1, f2: g =
x e1 −y e2. We have φ(g) = yzt −xyt, where the grevlex leading term is the second
term. That term is reducible using LT( f1), and this is a valid s-reduction since
s(g) = −ye2 >POT t e1.
The signature does not change if we add te1. We compute
h = g + te1 = (x + t)e1 −ye2
for which φ(h) = yzt−yt2 and LT(yzt−yt2) = yzt. No further s-reduction is possible
so P is updated to include
S(e1, (x + t)e1 −ye2) = (−x2 −xt + zt)e1 + xy e2,
(6)
S(e2, (x + t)e1 −ye2) = −(x3 + x2t)e1 + (yzt + x2y)e2,
(7)
and h = (x+t)e1−y e2 becomes a new element in G. You will check the computation
of the S-vectors in Exercise 6.
In the next two passes through the main loop, the algorithm processes the S-
vectors from (6) and then (7). In Exercise 6, you will show that both S-vectors are
discarded without any calculation because the function Criterion returns true in
both cases. Hence after those two passes (the fourth and ﬁfth, if you are counting),
P has been reduced to the singleton set {e3}. If f3 were not present, the algorithm
would terminate at this point. Moreover, all further vectors to be considered will
contain e3 terms. It follows that φ(G) at this point gives a Gröbner basis for the
ideal ⟨f1, f2⟩⊆I.
The sixth pass through the main loop ﬁrst sets up the S-vectors for the pairs
involving e3 and the previously found elements of G. After that pass we have, with
signatures listed in increasing >POT order and only the leading (signature) term
shown:
S((x + t)e1 −ye2, e3) = −yt e3 + · · · ,
(8)
S(e1, e3) = −xye3 + · · · ,
(9)
S(e2, e3) = −x2e3 + · · · .
(10)
The next passes through the main loop process the S-vectors from (8), (9),
and (10) one at a time. In Exercise 6, you will show that the vector in (8) reduces to
zero and a new element is added to S with signature yte3. The remaining S-vectors
in (9) and (10) can be discarded since the function Criterion returns true for both.
At this point the algorithm terminates and returns the Gröbner basis
{xy −yt, x2 −zt, yzt −yt2, z3 −t3}
for I. Note how the signature criteria were able to identify several unnecessary
reductions.

§4 Signature-based Algorithms and F5
589
Comments
We have only scratched the surface concerning the family of algorithms including
F5 but we hope this is enough to give the reader an idea of what is involved.
• An interesting observation, noted in Example 16, is that with our choices (in
particular the >POT order and the ordering of the set P), the computation of the
Gröbner basis by this algorithm is incremental in the sense that Gröbner bases
for ⟨f1, f2⟩, then ⟨f1, f2, f3⟩, then ⟨f1, f2, f3, f4⟩and so forth are computed in turn.
The order in which the fi are listed can thus affect the course of the computation
rather drastically. Moreover, there are a number of other optimizations that could
be introduced based on this. For instance, only Koszul syzygies involving the
initial segment of the list of fi are needed at any time in this case, so they could
be generated along the way.
• Different strategies for selecting g from P are also possible. For instance, a
number of authors have considered Matrix F5 algorithms incorporating ideas
from the F4 family. These would select several elements of P and reduce them
simultaneously. This is the sense in which the F4 and F5 families overlap that we
mentioned at the start of this section.
• There are many different ways to set up more powerful criteria for eliminating
unnecessary S-vectors. In particular, our function Criterion looks only at the sig-
nature of the S-vector. In EDER and FAUGÈRE (2014), more general rewriting
criteria are considered that actually consider terms appearing in both "halves" of
the S-vector.
The next comments are intended for readers with more commutative algebra
background.
• Since the main goal of the F5-type algorithms is to identify and ignore unneces-
sary S-polynomial remainder computations, we may ask whether there are theo-
retical results implying that this strategy is actually successful. One of the most
impressive is stated as Corollary 7.16 in EDER and FAUGÈRE (2014). If the in-
put polynomials form a regular sequence, then the algorithm actually does no
explicit S-vector reductions to 0; any S-vector that would reduce to 0 is "caught"
and discarded by the Criterion. This is the reason for the title of FAUGÈRE (2002).
• Finally, we mention that the ﬁnal contents of S might also be of great interest in
some situations. The vectors in that set form a basis for the module of all syzygies
on the f1, . . . , fs.
Modern Gröbner Basis Software
We conclude this chapter with some general comments regarding the current state
of understanding about computing Gröbner bases. With the accumulated experience
of many people over the course of 30 years or so, it has become clear that this is
one area where "one size ﬁts all" is a poor strategy. As a result, modern Gröbner
basis software often combines several of the approaches we have described. When

590
Chapter 10 Additional Gröbner Basis Algorithms
several methods are applicable, heuristics for selecting which method will be used
in a particular computation become especially important.
The Basis command of the recent versions of Maple's Groebner package, for
instance, incorporates ﬁve different procedures for computing Gröbner bases:
• A compiled implementation of an improved F4 algorithm (from Faugère's FGb
library). This is often the fastest option, at least for grevlex orders and larger
problems. But this is not completely general in that it does not support coefﬁ-
cients in a rational function ﬁeld, provides only some monomial orders, and does
not allow for coefﬁcients mod p for large primes p.
• An interpreted Maple implementation of F4 which is completely general but usu-
ally a lot slower than the FGb version of F4.
• The traditional Buchberger algorithm, with the normal pair selection strategy and
the criteria for discarding unnecessary pairs discussed in §10 of Chapter 2 (which
can actually be superior in some special cases, e.g., for the case where a single
polynomial is adjoined to a known Gröbner basis).
• The FGLM basis conversion algorithm.
• The Gröbner walk basis conversion algorithm.
For instance, for many problems in more than 5 or 6 variables using grevlex or other
graded monomial orders, the FGb F4 algorithm (or the Maple F4) will be the default
choice. For lex Gröbner bases for zero-dimensional ideals, a grevlex Gröbner basis
computation, followed by FGLM basis conversion is the default method, and the
Gröbner walk might be employed in other cases.
Magma has a very similar suite of Gröbner basis routines.
For lex Gröbner bases, Singular makes another heuristic choice and typically
computes a grevlex Gröbner basis, homogenizes, converts to a lex basis via the
Hilbert driven Buchberger algorithm from §2, then dehomogenizes and does further
remainder calculations to produce a reduced Gröbner basis.
This is one area where further developments can be expected and the state of the
art may look quite different in the future.
EXERCISES FOR §4
1. Show that the >POT order on the terms xαei satisﬁes the following properties
a. >POT is a well-ordering on the set of terms in Rs
b. If xαei >POT xβej then for all xγ ∈R, xγ · xαei >POT xγ · xβej.
c. The >POT order on Rs is compatible with the > order on S in the sense that xα > xβ ⇔
xαei > xβei for all i = 1, . . . , s.
2. In this problem, you will reconsider the computations from Example 1 in the light of the
general language introduced following that example.
a. Show that the vector g = (−f2, f1 −z) s-reduces to zero (as in Deﬁnition 10) using h
from the set {e1, e2} of vectors corresponding to ( f1, f2).
b. If we also allow reduction by the syzygy h = (−f2, f1), show that we can s-reduce g
to (0, 0).
c. Explain why the computation in part (b) is actually unnecessary, though.
3. Show that if S(g, h) is the S-vector of g, h from Deﬁnition 10, then
φ(S(g, h)) = S(φ(g), φ(h)),
where the right side is the S-polynomial of two polynomials.

§4 Signature-based Algorithms and F5
591
4. If f1, . . . , fs are polynomials in R, show that the collection of syzygies of the fi in Rs is
closed under sums and also closed under scalar multiplication by arbitrary elements of R.
(This says that the syzygies form a submodule of the module Rs.)
5. In this problem, you will show that the algorithm below performs regular s-reduction of g
by the set H and returns a regular s-reduced "remainder" u.
Input: g ∈Rs, H = {h1, . . . , hℓ} ⊆Rs
Output: u ∈Rs
u := g
r := 0
WHILE φ(u) ̸= r DO
m := LT(φ(u) −r)
i := 1
reductionoccurred := false
WHILE i ≤ℓAND reductionoccurred = false DO
IF LT(φ(hi))|m AND s

m
LT(φ(hi))hi

< s(u) THEN
d :=
m
LT(φ(hi))hi
u := u −d
reductionoccurred := true
ELSE
i := i + 1
IF reductionoccurred = false THEN
r := r + m
RETURN u
a. Show that the algorithm always terminates.
b. Show that only regular s-reductions are performed on u in the course of the algorithm
and the output u is regular s-reduced.
c. Modify the algorithm so that it also performs singular s-reductions whenever possible.
6. In this exercise you will check a number of the steps of the computation in Example 16,
then complete the calculation.
a. Check the computations of the S-vectors in (6) and (7).
b. In the next passes through the main loop, check that the Criterion function returns true
both times. Hint: Look at the signatures of the elements of S and the signatures of the
S-vectors.
c. If we did not use the function Criterion, we would have to do s-reductions on these
S-vectors. Do that explicitly, and show that both reduce to multiples of syzygies in S.
d. Check the computations of the S-vectors in (8), (9), and (10).
e. Show that the S-vector in (8) reduces to zero, and determine the syzygy included in S
at this step.
f. Verify that the S-vectors in (9) and (10) can be discarded because the function Criterion
returns true for each of them.

Appendix A
Some Concepts from Algebra
This appendix contains precise statements of various algebraic facts and deﬁnitions
used in the text. For students who have had a course in abstract algebra, much of
this material will be familiar. For students seeing these terms for the ﬁrst time, keep
in mind that the abstract concepts deﬁned here are used in the text in very concrete
situations.
§1
Fields and Rings
We ﬁrst give a precise deﬁnition of a ﬁeld.
Deﬁnition 1. A ﬁeld consists of a set k and two binary operations "+" and "·" de-
ﬁned on k for which the following conditions are satisﬁed:
(i) (a+b)+c = a+(b+c) and (a·b)·c = a·(b·c) for all a, b, c ∈k (associativity).
(ii) a + b = b + a and a · b = b · a for all a, b ∈k (commutativity).
(iii) a · (b + c) = a · b + a · c for all a, b, c ∈k (distributivity).
(iv) There are 0, 1 ∈k such that a + 0 = a · 1 = a for all a ∈k (identities).
(v) Given a ∈k, there is b ∈k such that a + b = 0 (additive inverses).
(vi) Given a ∈k, a ̸= 0, there is c ∈k such that a · c = 1 (multiplicative inverses).
The ﬁelds most commonly used in the text are Q, R, and C. In the exercises
to §1 of Chapter 1, we mention the ﬁeld F2 which consists of the two elements 0
and 1. Some more complicated ﬁelds are discussed in the text. For example, in §3
of Chapter 1, we deﬁne the ﬁeld k(t1, . . . , tm) of rational functions in t1, . . . , tm with
coefﬁcients in k. Also, in §5 of Chapter 5, we introduce the ﬁeld k(V) of rational
functions on an irreducible variety V.
If we do not require multiplicative inverses, then we get a commutative ring.
Deﬁnition 2. A commutative ring consists of a set R and two binary operations
"+" and "·" deﬁned on R for which the following conditions are satisﬁed:
© Springer International Publishing Switzerland 2015
D.A. Cox et al., Ideals, Varieties, and Algorithms, Undergraduate Texts
in Mathematics, DOI 10.1007/978-3-319-16721-3
593

594
Appendix A Some Concepts from Algebra
(i) (a + b) + c = a + (b + c) and (a · b) · c = a · (b · c) for all a, b, c ∈R
(associativity).
(ii) a + b = b + a and a · b = b · a for all a, b ∈R (commutativity).
(iii) a · (b + c) = a · b + a · c for all a, b, c ∈R (distributivity).
(iv) There are 0, 1 ∈R such that a + 0 = a · 1 = a for all a ∈R (identities).
(v) Given a ∈R, there is b ∈R such that a + b = 0 (additive inverses).
Note that any ﬁeld is obviously a commutative ring. Other examples of commu-
tative rings are the integers Z and the polynomial ring k[x1, . . . , xn]. The latter is the
most commonly used ring in the book. In Chapter 5, we construct two other com-
mutative rings: the coordinate ring k[V] of polynomial functions on an afﬁne variety
V and the quotient ring k[x1, . . . , xn]/I, where I is an ideal of k[x1, . . . , xn].
A special case of commutative rings are the integral domains.
Deﬁnition 3. A commutative ring R is an integral domain if whenever a, b ∈R
and a · b = 0, then either a = 0 or b = 0.
A zero divisor in a commutative ring R is a nonzero element a ∈R such that
a · b = 0 for some nonzero b ∈R. Hence integral domains have no zero divisors.
Any ﬁeld is an integral domain, and the polynomial ring k[x1, . . . , xn] is an integral
domain. In Chapter 5, we prove that the coordinate ring k[V] of a variety V is an
integral domain if and only if V is irreducible.
Finally, we note that the concept of ideal can be deﬁned for any ring.
Deﬁnition 4. Let R be a commutative ring. A subset I ⊆R is an ideal if it satisﬁes:
(i) 0 ∈I.
(ii) If a, b ∈I, then a + b ∈I.
(iii) If a ∈I and b ∈R, then b · a ∈I.
Note how this generalizes the deﬁnition of ideal given in §4 of Chapter 1.
§2
Unique Factorization
Deﬁnition 1. Let k be a ﬁeld. A polynomial f ∈k[x1, . . . , xn] is irreducible over
k if f is nonconstant and is not the product of two nonconstant polynomials in
k[x1, . . . , xn].
This deﬁnition says that if a nonconstant polynomial f is irreducible over k, then
up to a constant multiple, its only nonconstant factor is f itself. Also note that the
concept of irreducibility depends on the ﬁeld. For example, x2+1 is irreducible over
Q and R, but over C we have x2 + 1 = (x −i)(x + i).
Every nonconstant polynomial is a product of irreducible polynomials as follows.
Theorem 2. Every nonconstant f ∈k[x1, . . . , xn] can be written as a product f =
f1 ·f2 · · · fr of irreducibles over k. Further, if f = g1 ·g2 · · · gs is another factorization
into irreducibles over k, then r = s and the gi's can be permuted so that each fi is a
nonzero constant multiple of gi.

§3 Groups
595
The ﬁnal assertion of the theorem says that unique factorization holds in the
polynomial ring k[x1, . . . , xn].
Proof. The proof of Theorem 2 is by induction on the number of variables. The
base case k[x1] is covered in §5 of Chapter 1. Now suppose that k[x1, . . . , xn−1] has
unique factorization. The key tool for proving unique factorization in k[x1, . . . , xn]
is Gauss's Lemma, which in our situation can be stated as follows.
Proposition 3. Let k(x1, . . . , xn−1) be the ﬁeld of rational functions in x1, . . . , xn−1.
If f ∈k[x1, . . . , xn] is irreducible and has positive degree in xn, then f is irreducible
in k(x1, . . . , xn−1)[xn].
This follows from Proposition 5 of Section 9.3 of DUMMIT and FOOTE (2004)
since k[x1, . . . , xn−1] has unique factorization.
Combining Proposition 3 with unique factorization in the rings k[x1, . . . , xn−1]
and k(x1, . . . , xn−1)[xn], it is straightforward to prove that k[x1, . . . , xn] has unique
factorization. See Theorem 7 of Section 9.3 of DUMMIT and FOOTE (2004) for the
details.
□
For polynomials in Q[x1, . . . , xn], there are algorithms for factoring into irre-
ducibles over Q. A classical algorithm due to Kronecker is discussed in Theorem
4.8 of MINES, RICHMAN, and RUITENBERG (1988), and a more efﬁcient method is
given in Section 16.6 of VON ZUR GATHEN and GERHARD (2013).
Most computer algebra systems have a command for factoring polynomials in
Q[x1, . . . , xn]. Factoring polynomials in R[x1, . . . , xn] or C[x1, . . . , xn] is much more
difﬁcult.
§3
Groups
A group can be deﬁned as follows.
Deﬁnition 1. A group consists of a set G and a binary operation "·" deﬁned on G
for which the following conditions are satisﬁed:
(i) (a · b) · c = a · (b · c) for all a, b, c ∈G (associativity).
(ii) There is 1 ∈G such that 1· a = a · 1 = a for all a ∈G (identity).
(iii) Given a ∈G, there is b ∈G such that a · b = b · a = 1 (inverses).
A simple example of a group is given by the integers Z under addition. Note Z
is not a group under multiplication. A more interesting example comes from linear
algebra. Let k be a ﬁeld and deﬁne
GL(n, k) = {A | A is an invertible n × n matrix with entries in k}.
From linear algebra, we know that the product AB of two invertible matrices A
and B is again invertible. Thus, matrix multiplication deﬁnes a binary operation on
GL(n, k), and it is easy to verify that all of the group axioms are satisﬁed.
In Chapter 7, we will need the notion of a subgroup.

596
Appendix A Some Concepts from Algebra
Deﬁnition 2. Let G be a group. A nonempty subset H ⊆G is called a subgroup if
it satisﬁes:
(i) 1 ∈H.
(ii) If a, b ∈H, then a · b ∈H.
(iii) If a ∈H, then a−1 ∈H, where a−1 is the inverse of a in G.
One important group is the symmetric group Sn. Let n be a positive integer and
consider the set
Sn = {σ : {1, . . ., n} →{1, . . . , n} | σ is one-to-one and onto}.
Then composition of functions turns Sn into a group. Since an element σ ∈Sn
permutes the numbers 1 through n, we call σ a permutations. Note that Sn has n!
elements.
A transposition is an element of Sn that interchanges two numbers in {1, . . . , n}
and leaves all other numbers unchanged. Every permutation is a product of transpo-
sitions, though not in a unique way.
The sign of a permutation is deﬁned to be
sgn(σ) =

+1
if σ is a product of an even number of transpositions,
−1
if σ is a product of an odd number of transpositions.
One can show that sgn(σ) is well-deﬁned. Proofs of these assertions about Sn can
be found in Section 3.5 of DUMMIT and FOOTE (2004).
§4
Determinants
In linear algebra, one usually encounters the determinant det(A) of an n × n matrix
A with entries in a ﬁeld such as R or C. Typical formulas are
det
a11 a12
a21 a22

= a11a22 −a12a21
and
det
⎛
⎝
a11 a12 a13
a21 a22 a23
a31 a32 a33
⎞
⎠= a11 det

a22 a23
a32 a33

−a12 det

a21 a23
a31 a33

+ a13 det

a21 a22
a31 a32

,
which simpliﬁes to
a11a22a33 −a11a23a32 −a12a21a33 + a12a23a31 + a13a21a32 + a13a22a31.
In Chapters 3 and 8, we will use determinants whose entries are polynomials.
Fortunately, the theory of determinants works for n × n matrices with entries in a

§4 Determinants
597
commutative ring, such as a polynomial ring. In this generality, the above formulas
can be extended to express the determinant of an n × n matrix as a sum of n! terms
indexed by permutations σ ∈Sn, where the sign of the term is sgn(σ) from §3. More
precisely, we have the following result.
Proposition 1. If A = (aij) is an n × n matrix with entries in a commutative ring,
then
det(A) =

σ∈Sn
sgn(σ)a1σ(1) · · · anσ(n).
Proofs of all properties of determinants stated here can be found in Section 11.4
of DUMMIT and FOOTE (2004).
A second fact about determinants we will need concerns the solution of a linear
system of n equations in n unknowns. In matrix form, the system is written
AX = b,
where A = (aij) is the n × n coefﬁcient matrix, b is an n × 1 column vector, and X
is the column vector whose entries are the unknowns x1, . . . , xn.
Assume that we are working over a ﬁeld and A is invertible. Then det(A) ̸= 0
and A−1 exists. Furthermore, the system AX = b has the unique solution given by
X = A−1b. However, rather than ﬁnding the solution by Gaussian elimination as
done in most linear algebra courses, we need a formula for the solution.
Proposition 2 (Cramer's Rule). Suppose we have a system of equations AX = b
over a ﬁeld. If A is invertible, then the unique solution is given by
xi = det(Mi)
det(A) ,
where Mi is the matrix obtained from A by replacing its i-th column with b.
We use Propositions 1 and 2 to prove properties of resultants in Chapter 3, §6 and
Chapter 8, §7. We also use Proposition 2 in the proof of the Projective Extension
Theorem given in Chapter 8, §5. When we apply Cramer's Rule in these proofs, the
entries of A and b will typically be polynomials and the ﬁeld will be the associated
ﬁeld of rational functions.
We end with a fact about cofactors. The (i, j)-cofactor of an n × n matrix A is
cij = (−1)i+jdet(Aij), where Aij is the (n −1) × (n −1) matrix obtained from A by
deleting row i and column j. Also let In be the n × n identity matrix.
Proposition 3. Let A be an n × n matrix with entries in a commutative ring, and let
B be the transpose of the matrix of cofactors (cij). Then
BA = AB = det(A)In.
Proposition 3 is used in our treatment of Noether normalization in Chapter 5, §6.

Appendix B
Pseudocode
Pseudocode is commonly used in mathematics and computer science to present
algorithms. In this appendix, we will describe the pseudocode used in the text. If
you have studied a programming language, you may see a similarity between our
pseudocode and the language you studied. This is no accident, since programming
languages are also designed to express algorithms. The syntax, or "grammatical
rules," of our pseudocode will not be as rigid as that of a programming language
since we do not require that it run on a computer. However, pseudocode serves
much the same purpose as a programming language.
As indicated in the text, an algorithm is a speciﬁc set of instructions for perform-
ing a particular calculation with numerical or symbolic information. Algorithms
have inputs (the information the algorithm will work with) and outputs (the informa-
tion that the algorithm produces). At each step of an algorithm, the next operation to
be performed must be completely determined by the current state of the algorithm.
Finally, an algorithm must always terminate after a ﬁnite number of steps.
Whereas a simple algorithm may consist of a sequence of instructions to be per-
formed one after the other, most algorithms also use the following special structures:
• Repetition structures, which allow a sequence of instructions to be repeated.
These structures are also known as loops. The decision whether to repeat a group
of instructions can be made in several ways, and our pseudocode includes differ-
ent types of repetition structures adapted to different circumstances.
• Branching structures, which allow the possibility of performing different seq-
uences of instructions under different circumstances that may arise as the
algorithm is executed.
These structures, as well as the rest of the pseudocode, will be described in more
detail in the following sections.
© Springer International Publishing Switzerland 2015
D.A. Cox et al., Ideals, Varieties, and Algorithms, Undergraduate Texts
in Mathematics, DOI 10.1007/978-3-319-16721-3
599

600
Appendix B Pseudocode
§1
Inputs, Outputs, Variables, and Constants
We always specify the inputs and outputs of our algorithms on two lines before
the start of the algorithm proper. The inputs and outputs are given by symbolic
names in usual mathematical notation. Sometimes, we do not identify what type
of information is represented by the inputs and outputs. In this case, their meaning
should be clear from the context of the discussion preceding the algorithm. Variables
(information stored for use during execution of the algorithm) are also identiﬁed by
symbolic names. We freely introduce new variables in the course of an algorithm.
Their types are determined by the context. For example, if a new variable called
a appears in an instruction, and we set a equal to a polynomial, then a should be
treated as a polynomial from that point on. Numerical constants are speciﬁed in
usual mathematical notation. The two words true and false are used to represent
the two possible truth values of an assertion.
§2
Assignment Statements
Since our algorithms are designed to describe mathematical operations, by far the
most common type of instruction is the assignment instruction. The syntax is
<variable> := <expression>.
The symbol := is the assignment operator in many computer languages. The mean-
ing of this instruction is as follows. First, we evaluate the expression of the right
of the assignment operator, using the currently stored values for any variables that
appear. Then the result is stored in the variable on the left-hand side. If there was a
previously stored value in the variable on the left-hand side, the assignment erases
it and replaces it with the computed value from the right-hand side. For example, if
a variable called i has the numerical value 3, and we execute the instruction
i := i + 1,
the value 3 + 1 = 4 is computed and stored in i. After the instruction is executed, i
will contain the value 4.
§3
Looping Structures
Three different types of repetition structures are used in the algorithms given in the
text. They are similar to the ones used in many languages. The most general and
most frequently used repetition structure in our algorithms is the WHILE structure.
The syntax is

§3 Looping Structures
601
WHILE <condition> DO <action>.
Here, <action> is a sequence of instructions. In a WHILE structure, the action is the
group of statements to be repeated. We always indent this sequence of instructions.
The end of the action is signaled by a return to the level of indentation used for the
WHILE statement itself.
The <condition> after the WHILE is an assertion about the values of variables,
etc., that is either true or false at each step of the algorithm. For instance, the condi-
tion
i ≤s AND divisionoccurred = false
appears in a WHILE loop in the division algorithm from Chapter 2, §3.
When we reach a WHILE structure in the execution of an algorithm, we deter-
mine whether the condition is true or false. If it is true, then the action is performed
once, and we go back and test the condition again. If it is still true, we repeat the
action once again. Continuing in the same way, the action will be repeated as long
as the condition remains true. When the condition becomes false (at some point
during the execution of the action), that iteration of the action will be completed,
and then the loop will terminate. To summarize, in a WHILE loop, the condition is
tested before each repetition, and that condition must be true for the repetition to
continue.
A second repetition structure that we use on occasion is the REPEAT structure.
A REPEAT loop has the syntax
REPEAT <action> UNTIL <condition>.
Reading this as an English sentence indicates its meaning. Unlike the condition in a
WHILE, the condition in a REPEAT loop tells us when to stop. In other words, the
action will be repeated as long as the condition is false. In addition, the action of
a REPEAT loop is always performed at least once since we only test the condition
after doing the sequence of instructions representing the action. As with a WHILE
structure, the instructions in the action are indented.
The ﬁnal repetition structure that we use is the FOR structure. We use the syntax
FOR each s in S DO <action>
to represent the instruction: "perform the indicated action for each element s ∈S."
Here S is a ﬁnite set of objects and the action to be performed will usually depend
on which s we are considering. The order in which the elements of S are considered
is not important. Unlike the previous repetition structures, the FOR structure will
necessarily cause the action to be performed a ﬁxed number of times (namely, the
number of elements in S).

602
Appendix B Pseudocode
§4
Branching Structures
We use only one type of branching structure, which is general enough for our pur-
poses. The syntax is
IF <condition> THEN <action1> ELSE <action2>.
The meaning is as follows. If the condition is true at the time the IF is reached,
action1 is performed (once only). Otherwise (that is, if the condition was false),
action2 is performed (again, once only). The instructions in action1 and action2 are
indented, and the ELSE separates the two sequences of instructions. The end of
action2 is signaled by a return to the level of indentation used for the IF and ELSE
statements.
In this branching structure, the truth or falsity of the condition selects which
action to perform. In some cases, we omit the ELSE and action2, i.e.,
IF <condition> THEN <action1>.
This form is equivalent to
IF <condition> THEN <action1> ELSE <do nothing>.
§5
Output Statements
As already mentioned, the ﬁrst two lines of our algorithms give its input and output.
We also always include a RETURN statements with the syntax
RETURN <output of the algorithm>
to indicate precisely where the ﬁnal output of the algorithm is returned. Most of the
time, the RETURN statement is the last line of the algorithm.

Appendix C
Computer Algebra Systems
This appendix will discuss several computer algebra systems that can be used in
conjunction with this book. Our comments here are addressed both to new users,
including students, and to instructors considering which system might be most ap-
propriate for use in a course. We will consider Maple, Mathematica, Sage, and the
specialized systems CoCoA, Macaulay2, and Singular in detail. In their different
ways, these are all amazingly powerful programs, and our brief discussions will
not do justice to their true capabilities. Readers should not expect a complete, gen-
eral introduction to any of these systems, instructions for downloading them, details
about how those systems might be installed in computer labs, or what other software
might be used in conjunction with them. Instead, we will assume that you already
know or can access local or online documentation from the web sites indicated be-
low concerning:
• How to enter and exit the program, save work and continue it in a later session.
• How to enter commands, execute them, and refer to the results generated by
previous commands in an interactive session.
• If applicable, how to insert comments or other annotations in the worksheet or
notebook interfaces provided in general-purpose systems such as Maple, Mathe-
matica, and Sage.
• How to work with lists. For example, in the Gröbner basis command, the input
contains a list of polynomials, and the output is another list which a Gröbner
basis for the ideal generated by the polynomials in the input list. You should be
able to ﬁnd the length of a list and extract polynomials from a list.
• How to save results to an external ﬁle as text. This can be important, especially
when output ﬁlls more than one computer screen. You should be able to save
output in a ﬁle and examine it or print it out for further study.
• More advanced users will probably also want to know how to create and read in
external ﬁles containing sequences of input commands or code for procedures.
For courses taught from this book with a laboratory component, we suggest that
instructors may ﬁnd that an efﬁcient way to get students up to speed is to use a ﬁrst
lab meeting to cover aspects of the particular computer algebra being used.
© Springer International Publishing Switzerland 2015
D.A. Cox et al., Ideals, Varieties, and Algorithms, Undergraduate Texts
in Mathematics, DOI 10.1007/978-3-319-16721-3
603

604
Appendix C Computer Algebra Systems
§1
General Purpose Systems: Maple, Mathematica, Sage
The systems discussed in this section—Maple, Mathematica, and Sage—include
components for computations of Gröbner bases and other operations on polyno-
mial ideals. However, this is only a very small part of their functionality. They also
include extensive facilities for numerical computation, for generating high quality
graphics, and for many other types of symbolic computation. They also incorporate
programming languages to automate multistep calculations and create new proce-
dures. In addition, they have sophisticated notebook or worksheet user interfaces
that can be used to generate interactive documents containing input commands, out-
put and text annotations.
Maple
Maple is one of the leading commercial systems of this type. The Maplesoft web site
http://www.maplesoft.com includes, among many other things, information
about the different versions available and the full electronic documentation for the
system. For us, the most important part of Maple is the Groebner package. Our
discussion applies to the versions of this package distributed starting with Maple 11.
As of summer 2014, the current version is MAPLE18 (2014).
To have access to the commands in the Groebner package, the command
with(Groebner)
must be executed before any of the following commands. (Note: In the current ver-
sion of the worksheet interface, no input prompt is generated and commands need
not be terminated with semicolons. The user can toggle back and forth between in-
ert text for comments and input commands using "buttons" in the Maple window. If
desired, a previous version of the interface can also be used. There each input line
is marked with an input prompt [> and commands generating visible output are ter-
minated with semicolons.) In any case, once the Groebner package is loaded, you
can perform the division algorithm, compute Gröbner bases, and carry out a variety
of other commands described below.
The deﬁnition of a monomial order in Maple always involves an explicit list of
variables. All of the monomial orders discussed in Chapter 2 and Chapter 3 are
provided, together with general mechanisms for specifying others. With x > y > z,
for instance,
• lex order is speciﬁed by plex(x,y,z),
• grlex order is speciﬁed by grlex(x,y,z) and
• grevlex order is speciﬁed by tdeg(x,y,z).
The lexdeg command provides a mechanism for specifying elimination orders.
For example, lexdeg([x_1,...,x_k],[x_{k+1},...,x_n]) speciﬁes an or-
der that eliminates x_1,...,x_k leaving polynomials in x_{k+1},...,x_n. The
idea is similar to (but not exactly the same as) the elimination orders discussed
in Exercise 6 of Chapter 3, §1. Weight orders as in Exercise 10 of Chapter 2, §4,

§1 General Purpose Systems: Maple, Mathematica, Sage
605
and general matrix orders are also available. The documentation for the Groebner
package gives full information about the syntax of these declarations.
The basic commands in Maple's Groebner package are NormalForm for doing
the division algorithm and Basis, for computing Gröbner bases. The syntax for the
NormalForm command is
NormalForm(f,polylist,order,options)
where f is the dividend polynomial and polylist is the list of divisors. No options
need be speciﬁed. The output is the remainder on division. If the quotients on divi-
sion are required, then an option as follows:
NormalForm(f,polylist,order,'Q')
instructs Maple to return the list of quotients as the value of the variable Q. That list
is not shown automatically, but can be seen by entering the variable name Q as a
command on a subsequent line.
The syntax of the Basis command is similar:
Basis(polylist,order,options)
The output will be a reduced Gröbner basis (in the sense of Chapter 2, §7) except
for clearing denominators. Optional inputs can be used to specify the algorithm
used to compute the Gröbner basis (see the discussion at the end of Chapter 10,
§4), to compute a transformation matrix giving the Gröbner basis polynomials as
combinations of the inputs, or to specify the characteristic of the coefﬁcient ﬁeld. If
no characteristic is speciﬁed, it is taken to be zero by default, and computations are
done over Q with no limitation on the sizes of coefﬁcients.
As an example of how this all works, consider the command
gb := Basis([xˆ2 + y,2*x*y + yˆ2],plex(x,y))
This computes a list which is a Gröbner basis for the ideal ⟨x2 + y, 2xy + y2⟩in
Q[x, y] using lex order with x > y and assigns it the symbolic name gb. With an
additional option as follows:
gb := Basis([xˆ2 + y,2*x*y + yˆ2],plex(x,y),characteristic=p)
where p is a speciﬁc prime number, the computation is done in the ring of poly-
nomials in x, y with coefﬁcients in the ﬁnite ﬁeld of integers modulo p. The same
option works in NormalForm as well.
To tell Maple that a certain variable is in the base ﬁeld (a "parameter"), simply
omit it from the variable list in the monomial order speciﬁcation. Thus
Basis([v*xˆ2 + y,u*x*y + yˆ2],plex(x,y))
will compute a Gröbner basis for ⟨vx2 + y, uxy + y2⟩in Q(u, v)[x, y] using lex order
with x > y. In each case, the answer is reduced up to clearing denominators (so the
leading coefﬁcients of the Gröbner basis are polynomials in u and v).
The symbol I is a predeﬁned constant in Maple, equal to the imaginary unit i =
√
−1. Computations of Gröbner bases over Q(i) can be done simply by including
I at the appropriate places in the coefﬁcients of the input polynomials. (This also
explains why trying to use the name I for the list of generators of an ideal will cause
an error.) Coefﬁcients in other algebraic extensions of Q (or other base ﬁelds) can
be included either by means of radical expressions or RootOf expressions. Thus,
for instance, to include a
√
2 in a coefﬁcient of a polynomial, we could simply enter

606
Appendix C Computer Algebra Systems
the polynomial using 2ˆ(1/2) or RootOf(uˆ2 - 2) in the appropriate location.
We refer the reader to Maple's documentation for the details.
Other useful commands in the Groebner package include
• LeadingTerm, LeadingMonomial, LeadingCoefficient which take as in-
put a polynomial and a monomial order and return the indicated information. The
names of these commands follow the terminology used in this text.
• SPolynomial, which computes the S-polynomial of two polynomials with re-
spect to a monomial order. Note: the results here can differ from those in this text
by constant factors since Maple does not divide by the leading coefﬁcients.
• IsProper, which uses the consistency algorithm from Chapter 4, §1 to deter-
mine if a set of polynomial equations has a solution over an algebraically closed
ﬁeld.
• IsZeroDimensional which uses the ﬁniteness algorithm from Chapter 5, §3
to determine if a system of polynomial equations has only a ﬁnite number of
solutions over an algebraically closed ﬁeld.
• UnivariatePolynomial, which given a variable and a set of generators for an
ideal computes the polynomial of lowest degree in the variable which lies in the
ideal (the generator of the corresponding elimination ideal).
• HilbertPolynomial, which computes the Hilbert polynomial of a homoge-
neous ideal as deﬁned in Chapter 9. A related command HilbertSeries com-
putes the Hilbert-Poincaré series for a homogeneous ideal deﬁned in Chapter 10,
§2. These commands are also deﬁned for nonhomogeneous ideals and compute
the corresponding asymptotic polynomial and generating function for the ﬁrst
difference of the afﬁne Hilbert function.
We should mention that Maple also includes a PolynomialIdeals package
containing a number of commands closely related to the content of this text and
with functionality overlapping that of the Groebner package to some extent. The
basic data structure for the PolynomialIdeals package is a polynomial ideal, de-
ﬁned as follows:
with(PolynomialIdeals)
J := <xˆ2 + y,2*x*y + yˆ2>
K := <xˆ3 + x*y - 1>
(The < and > from the keyboard act as ⟨and ⟩.) PolynomialIdeals contains im-
plementations of the algorithms developed in Chapters 2-4 for ideal membership,
radical membership, ideal containment, operations such as sums, products, inter-
sections, ideal quotients and saturation, and primary decomposition. Much of this
is based on the routines in the Groebner package, but the PolynomialIdeals
package is set up so that users need only understand the higher-level descriptions of
the ideal operations and not the underlying algorithms. For example,
Intersect(J,K)
computes the intersection of the two ideals deﬁned above.

§1 General Purpose Systems: Maple, Mathematica, Sage
607
Mathematica
Mathematica is the other leading commercial system in this area. The web site
http://www.wolfram.com/mathematica contains information about the dif-
ferent versions available and the full electronic documentation for the system. As of
summer 2014, the current version is MATHEMATICA10 (2014).
There is no special package to load in order to compute Gröbner bases: the
basic commands are part of the Mathematica kernel. Mathematica knows most
of the basic monomial orderings considered in Chapter 2. Lex order is called
Lexicographic, grlex is called DegreeLexicographic and grevlex is called
DegreeReverseLexicographic. The monomial order is speciﬁed by including
a MonomialOrder option within the Mathematica commands described below. If
you omit the MonomialOrder option, Mathematica will use lex as the default.
Mathematica can also use weight orders as described in the comments at the end
of the exercises to Chapter 2, §4.
Since a monomial order also depends on how the variables are ordered, Math-
ematica also needs to know a list of variables in order to specify the monomial
order you want. For example, to tell Mathematica to use lex order with variables
x > y > z, you would input {x,y,z} (Mathematica uses braces {...} for lists and
square brackets [...] to delimit the inputs to a command or function).
For our purposes, important Mathematica commands are PolynomialReduce
and GroebnerBasis. PolynomialReduce implements a variant of the division
algorithm from Chapter 2 that does not necessarily respect the ordering of the list
of divisors. This means that the quotients and the remainder may differ from the
ones computed by our algorithm, though the quotients and remainders still satisfy
the conditions of Theorem 3 of Chapter 2, §3. The syntax is as follows:
In[1]:= PolynomialReduce[f,polylist,varlist,options]
(The input prompt In[1]:= is generated automatically by Mathematica.) This
computes quotients and a remainder of the polynomial f by the polynomials in
polylist, using the monomial order speciﬁed by varlist and the optional
MonomialOrder declaration. For example, to divide x3 +3y2 by x2 +y and 2xy+y2
using grlex order with x > y, one would enter:
In[2]:= PolynomialReduce[xˆ3 + 3yˆ2,{xˆ2 + y,2xy + yˆ2},
{x,y},MonomialOrder -> DegreeLexicographic]
The output is a list with two entries: the ﬁrst is a list of the quotients and the second
is the remainder.
The command for computing Gröbner bases has the following syntax:
In[3]:= GroebnerBasis[polylist,varlist,options]
This computes a Gröbner basis for the ideal generated by the polynomials in
polylist with respect to the monomial order given by the MonomialOrder option
with the variables ordered according to varlist. The answer is a reduced Gröbner
basis (in the sense of Chapter 2, §7), except for clearing denominators. As an exam-
ple of how GroebnerBasis works, consider
In[4]:= gb = GroebnerBasis[{xˆ2 +y,2xy + yˆ2},{x,y}]
The output is a list (assigned to the symbolic name gb) which is a Gröbner basis for

608
Appendix C Computer Algebra Systems
the ideal ⟨x2 + y, 2xy + y2⟩⊆Q[x, y] using lex order with x > y. We omitted the
MonomialOrder option since lex is the default.
If you use polynomials with integer or rational coefﬁcients in GroebnerBasis
or PolynomialReduce, Mathematica will assume that you are working over the
ﬁeld Q. There is no limitation on the size of the coefﬁcients. Another possible co-
efﬁcient ﬁeld is the Gaussian rational numbers Q(i) = {a + bi | a, b ∈Q}, where
i =
√
−1 (Mathematica uses I to denote the imaginary unit). To compute a Gröbner
basis over a ﬁnite ﬁeld with p elements (where p is a prime number), you need to
include the option Modulus -> p in the GroebnerBasis command. This option
also works in PolynomialReduce.
Mathematica can also work with coefﬁcients that lie in a rational function ﬁeld.
The strategy is that the variables in the base ﬁeld (the "parameters") should be omit-
ted from the variable list in the input, and then one sets the CoefficientDomain
option to RationalFunctions. For example, the command:
In[5]:= GroebnerBasis[{v xˆ2 + y,u x y + yˆ2},{x,y},
CoefficientDomain -> RationalFunctions]
will compute a Gröbner basis for ⟨vx2 + y, uxy + y2⟩⊆Q(u, v)[x, y] using lex order
with x > y. The CoefficientDomain option is also available for remainders using
PolynomialReduce.
Here are some other useful Mathematica commands:
• MonomialList, which lists the terms of a polynomial according to the mono-
mial order. Using this, MonomialList[f,vars,monomialorder][[1]] can
be used to pick out the leading term.
• Eliminate, which uses the Elimination Theorem from Chapter 3, §1 to elimi-
nate variables from a system of equations.
• Solve, which attempts to ﬁnd all solutions of a system of equations.
Mathematica also allows some control over the algorithm used to produce a Gröb-
ner basis. For instance, it is possible to specify that the Gröbner walk basis conver-
sion algorithm mentioned in Chapter 10 will be used via the Method option in the
GroebnerBasis command. Further descriptions and examples can be found in the
electronic documentation at http://www.wolfram.com/mathematica.
Sage
Sage is a free and open-source computer algebra system under continual develop-
ment since its initial release in 2005. As of summer 2014, the latest version is STEIN
ET AL. (2014). The leader of the Sage project is William Stein of the University of
Washington; hundreds of other mathematicians have contributed code and packages.
The source code, as well as Linux and Mac OS X binary executables are available
for download from the web site http://sagemath.org; a version of Sage that
runs on Windows systems in combination with the VirtualBox operating system
virtualization software is also available there.
The design of Sage is rather different from that of other packages we discuss
in this appendix in that Sage has been built to provide a common front end for pre-
existing open-source packages, including in particular the Singular system described

§1 General Purpose Systems: Maple, Mathematica, Sage
609
in §2 below. Sage provides a browser-based notebook interface allowing users to
create interactive documents. It also incorporates command constructs based on
features of the Python programming language. It is intended to be comparable in
power and scope to the commercial packages Maple and Mathematica. But it also
has particular strengths in computational algebra and number theory because of its
development history and the other packages it subsumes.
To do Gröbner basis computations in Sage, one must ﬁrst deﬁne a polynomial
ring that contains all polynomials involved and that speciﬁes the monomial order to
be used. For instance, to compute a Gröbner basis for the ideal I = ⟨x2 + y, 2xy +
y2⟩⊆Q[x, y] with respect to the lex order with x > y, we could proceed as follows.
First deﬁne the ring with an input command like this:
R.<x,y> = PolynomialRing(QQ,order='lex')
(the QQ is Sage's built-in ﬁeld of rational numbers and the order of the variables is
determined by their ordering in the ring deﬁnition). There are many other equivalent
ways to do this too; Sage's syntax is very ﬂexible. Then the ideal I can be deﬁned
with the command
I = Ideal(xˆ2 + y,2*x*y + yˆ2)
and the Gröbner basis can be computed and displayed via the command
I.groebner_basis()
The syntax here is the same as that of the object-oriented features of Python. The last
part of this, the .groebner_basis(), indicates that we are applying a function,
or "method" deﬁned for all objects that are ideals in polynomial rings that have
been deﬁned, and requiring no other input. In the notebook interface, pressing the
TAB key on an input line with the partial command I. will generate a listing of all
methods that can be applied to an object of the type of the object I. This can be
helpful if you are unsure of the correct syntax for the command you want or what
operations are permitted on an object.
Sage's polynomial ring deﬁnition mechanism is very general. The ﬁeld of co-
efﬁcients can be any ﬁnitely generated extension of Q or a ﬁnite ﬁeld. The ﬁnite
ﬁeld with p elements for a prime p is denoted GF(p)—this would replace the QQ
in the deﬁnition of the polynomial ring if we wanted to work over that ﬁnite ﬁeld
instead. Algebraic elements, including the imaginary unit i = √−1, may be deﬁned
via the NumberField command, while ﬁelds of rational functions may be deﬁned
via the FractionField command. For example, suppose we wanted to work with
polynomials in variables x, y with coefﬁcients in the ﬁeld Q(
√
2). Here is one way
to construct the ring we want. The ﬁrst step, or something equivalent, is necessary
to deﬁne the variable in the polynomial used in the deﬁnition of the number ﬁeld;
every object used in a Sage session must be deﬁned in the context of some structure
previously deﬁned. The rt2 is the symbolic name for
√
2 in the ﬁeld F:
R.<z> = PolynomialRing(QQ)
F.<rt2> = NumberField(zˆ2 - 2)
R.<x,y> = PolynomialRing(F)
Then we could deﬁne ideals in the ring R, compute Gröbner bases, etc. Compu-
tations over Q(i) can be done in the same way by using NumberField with the
polynomial z2 + 1 satisﬁed by i = √−1.

610
Appendix C Computer Algebra Systems
To deﬁne a rational function ﬁeld Q(a) as the coefﬁcient ﬁeld, we may proceed
like this:
P.<a> = PolynomialRing(F)
K = FractionField(P)
R.<x,y> = PolynomialRing(K)
These constructions can even be combined to produce ﬁelds such as Q(
√
2)(a).
However, it is important to realize that Sage makes use of the program Singular,
whose coefﬁcient ﬁelds are not quite this general. For ideals in polynomial rings
with coefﬁcients in a ﬁeld like Q(
√
2)(a), Sage falls back to its own slower Gröbner
basis routines.
General monomial orders, including all of those discussed in the text, can be
speciﬁed in Sage. We have seen how to specify lex orders above. Graded lex orders
are obtained with order = 'deglex', while graded reverse lex orders use order
= 'degrevlex'. Various weight orders as in Exercise 10 of Chapter 2, §4, and
general matrix orders are also available.
Most of the other basic operations we have discussed are provided in Sage as
methods that are deﬁned on ideals or individual polynomials. For instance, the lead-
ing term, leading monomial, and leading coefﬁcient of a polynomial f are computed
by f.lt(), f.lm(), and f.lc(), respectively. If G is a Gröbner basis for an ideal
I and f is a polynomial in the ring containing I, then the remainder on division by G
is computed by
f.reduce(G)
If desired, the list of quotients in the division can be recovered using
(f - f.reduce(G)).lift(G)
The computation of S-polynomials can be done by using an "educational" imple-
mentation of the basic Buchberger algorithm that is made accessible through the
command
from sage.rings.polynomial.toy_buchberger import spol
Then to compute the S-polynomial of f and g, use spol(f,g).
Other useful methods deﬁned on ideals include:
• .gens(), which returns the list of generators of the ideal. One caution: As in
Python, all lists are indexed starting from 0 rather than 1.
• .elimination_ideal(varlist), where varlist is the list of variables to
be eliminated,
• .hilbert_polynomial(), deﬁned only for homogeneous ideals,
• .hilbert_series(), deﬁned only for homogeneous ideals,
• .primary_decomposition()
All of the "real work" in these computations in Sage is being done by Singular,
but comparing the form of the commands here with those in the section on Singu-
lar below shows that the Sage syntax is different. In effect, the Sage input is being
translated for presentation to Singular, and results are passed back to Sage. In par-
ticular, the output of these commands are Sage objects and can be used by parts of
the Sage system outside of Singular. It is also possible to communicate directly with
Singular if that is desired. In our experience, though, the Python-esque Sage syntax
is somewhat easier to work with for many purposes.

§2 Special Purpose Programs: CoCoA, Macaulay2, Singular
611
§2
Special Purpose Programs: CoCoA, Macaulay2, Singular
Unlike the systems discussed in §1, the programs discussed in this section were
developed primarily for use by researchers in algebraic geometry and commuta-
tive algebra. With some guidance, though, beginners can also make effective use
of them. These systems offer minimal numerical computation and graphics (at best)
and the current versions feature text-based user interfaces. They also tend to provide
direct access to less of the infrastructure of Gröbner basis computations such as S-
polynomials, remainders on division by general sets of divisors, and so on. But like
the general purpose programs, they incorporate complete programming languages
so it is possible to extend their basic functionality by creating new procedures. These
programs tend to be much more powerful within their limited application domain
since they include a number of more advanced algorithms and more sophisticated
higher-level constructs.
CoCoA
CoCoA (for "Computations in Commutative Algebra") is a free, open-source com-
puter algebra system for polynomial computations. Versions for Unix systems, Mac
OS X, and Windows are available from http://cocoa.dima.unige.it. All
documentation is also posted there. As of summer 2014, the current standard version
is ABBOTT ET AL. (2014). CoCoA-5 involved a major redesign of many features of
the system and a new C++ library of underlying functions. This means that some as-
pects of the new system and its documentation are still under development and may
change in the future. Previous versions of CoCoA-4 are also available for a num-
ber of operating systems. The development of CoCoA is currently led by Lorenzo
Robbiano, John Abbott, Anna Bigatti, and Giovanni Lagorio, of the University of
Genoa in Italy. Many other current and former members of the CoCoA team have
participated in this effort. A large number of advanced algorithms and contributed
packages are provided.
To do Gröbner basis computations in CoCoA, one must ﬁrst deﬁne a polynomial
ring that contains all polynomials involved and that speciﬁes the monomial order to
be used. For instance, to compute a Gröbner basis for I = ⟨x2+y, 2xy+y2⟩⊆Q[x, y]
with respect to the lex order with x > y, we could proceed as follows. First deﬁne
the ring with an input command like this:
use R ::= QQ[x,y], Lex;
(all CoCoA commands end with a semicolon and the double colon is correct for use
ring speciﬁcations). This sets the current ring (called R) to the polynomial ring with
coefﬁcient ﬁeld equal to the predeﬁned ﬁeld QQ (that is, the ﬁeld of rational numbers
Q). The ordering of the variables is determined by the order within the list.
Then the ideal I can be deﬁned with the command
I := ideal(xˆ2 + y,2*x*y + yˆ2);
Unlike older versions of CoCoA, the new version does require explicit asterisks for
multiplication. The Gröbner basis can be computed and displayed via the command

612
Appendix C Computer Algebra Systems
GBasis(I);
If the results are to be saved and used later under a different name, an assignment
GB := GBasis(I);
could be used. A separate command ReducedGBasis is provided for reduced Gröb-
ner bases.
CoCoA's polynomial ring deﬁnition mechanism is quite ﬂexible. Coefﬁcients in
a ﬁeld of rational functions over Q or over a ﬁnite ﬁeld may also be deﬁned as fol-
lows. For example, suppose we wanted to work with polynomials in variables x, y
with coefﬁcients in the ﬁeld Q(a). Here is one way to construct the ring we want:
K := NewFractionField(NewPolynomialRing(QQ,["a"]));
use R ::= K[x,y];
Note that this use did not specify a monomial order. The default is grevlex, which
can also be speciﬁed explicitly as DegRevLex. The grlex order is DegLex. Elim-
ination orders are speciﬁed as Elim(vars), where vars are the variables to be
eliminated (either a single variable or a range of variables in the list deﬁning the
current ring), indicated like this:
use R ::= QQ[x,y,z,w], Elim(x..y);
This would deﬁne one of the elimination orders considered in Exercise 11 in Chap-
ter 2, §4 eliminating x, y in favor of z, w. General monomial orders, including all of
the others discussed in the text, can be speciﬁed in CoCoA by means of matrices.
Many of the other basic operations we have discussed are provided in CoCoA as
functions that are deﬁned on ideals or individual polynomials. For instance, what we
call the leading term (including the coefﬁcient) is LM(f) in CoCoA, while LT(f)
or LPP(f) compute the what we call the leading monomial. LC(f) is the leading
coefﬁcient. The command NF(f,I) computes the remainder on division of a poly-
nomial f with respect to a Gröbner basis for I. A Gröbner basis for the ideal will be
computed in order to carry this out if that has not been done previously.
Other useful commands deﬁned on ideals include:
• intersect(I,J) computes the intersection of the ideals I and J.
• colon(I,J) computes the quotient ideal I :J.
• saturate(I,J) computes the saturation I :J∞of I with respect to J.
• elim(X,I) computes an elimination ideal, where X is a variable, range of vari-
ables, or list of variables to be eliminated.
• HilbertFunction(R/I) computes a representation of the Hilbert function of
R/I. This is intended for homogeneous ideals. If I is not homogeneous, then the
output is the Hilbert function of ⟨LT(I)⟩.
• HilbertPoly(R/I) computes a representation of the Hilbert polynomial of
R/I. This behaves the same way as HilbertPoly if I is not a homogeneous
ideal.
• HilbertSeries(R/I) gives a representation of the Hilbert-Poincaré series of
R/I as deﬁned in Chapter 10, §2. This is deﬁned only for homogeneous ideals.
• PrimaryDecomposition(I) is available but is implemented only for square-
free monomial ideals at the current time.

§2 Special Purpose Programs: CoCoA, Macaulay2, Singular
613
Macaulay2
Macaulay2 is a free, open-source computer algebra system for computations in al-
gebraic geometry and commutative algebra. Versions for most Unix systems, Mac
OS X, and Windows (the last running under the Cygwin operating system virtual-
ization software) may be downloaded from the Macaulay2 web site http://www.
math.uiuc.edu/Macaulay2. Complete electronic documentation is also avail-
able there. As of summer 2014, the current version is GRAYSON and STILLMAN
(2013). Macaulay2 has been developed by Daniel Grayson of the University of Illi-
nois and Michael Stillman of Cornell University; a number of packages extending
the basic functionality of the system have been contributed by other mathematicians.
Macaulay2 has a special emphasis on computations in algebraic geometry,specif-
ically computations of syzygies, free resolutions of modules over polynomial rings,
and information about varieties that can be derived from those computations. Much
of this is beyond the scope of this book, but all of it is based on the Gröbner ba-
sis computations we discuss and enough of that infrastructure is accessible to make
Macaulay2 useful for courses based on this text.
To do Gröbner basis computations in Macaulay2, one must ﬁrst deﬁne a polyno-
mial ring that contains all the polynomials involved and the monomial order to be
used. For instance, to compute a Gröbner basis for the ideal I = ⟨x2 +y, 2xy+y2⟩⊆
Q[x, y] with respect to the lex order with x > y, we could proceed as follows. First
deﬁne the ring with an input command like this:
i1 : R = QQ[x,y,MonomialOrder=>Lex]
(the i1 represents the input prompt; executing this command will generate two lines
of output labeled o1 showing the name R and its type PolynomialRing). The QQ
is Macaulay2's notation for the ﬁeld Q. The order on the ring variables is speciﬁed
by the ordering of the list in the square brackets.
The ideal I can be deﬁned with the command
i2 : I = ideal(xˆ2 + y,2*x*y + yˆ2)
Then
i3 : gens gb I
computes the required Gröbner basis and presents the result as a matrix of polyno-
mials with one row. There are many options that can be speciﬁed to control how the
computation is performed and what algorithms are used.
The remainder on division of a polynomial f by a Gröbner basis for an ideal I is
computed by
i4 : f % I
in Macaulay2. If a Gröbner basis for I has already been computed it is used, other-
wise it is computed in order to ﬁnd the unique remainder.
General monomial orders, including all of those discussed in the text, can be
speciﬁed in Macaulay2. Grevlex orders are the default. We have seen how to spec-
ify lex orders above, while grlex orders are obtained with MonomialOrder=>GLex.
The elimination orders from Exercise 11 in Chapter 2, §4 are speciﬁed like this:
MonomialOrder=>Elimination n, where n is the number of variables to elimi-
nate (from the start of the list). Various weight orders as in Exercise 11 of Chapter 2,

614
Appendix C Computer Algebra Systems
§4, and product orders are also available. These can be combined in very ﬂexible
ways giving orders equivalent to any matrix order.
Macaulay2 allows very general coefﬁcient ﬁelds in polynomial rings. For in-
stance, to deﬁne the ring of polynomials Q(u, v)[x, y] with coefﬁcients in the ﬁeld
Q(u, v) and the grlex order with x > y we could proceed as follows:
i5 : R = QQ[u,v]
i6 : K = frac(R)
(this computes the ﬁeld of fractions of the ring R, that is the ﬁeld Q(u, v) of rational
functions in u, v). Then the ring we want is
i7 : S = K[x,y,MonomialOrder=>GLex]
The coefﬁcient ﬁeld of a polynomial ring can also be a ﬁnite ﬁeld. Use ZZ/p for the
ﬁeld of integers modulo the prime p, for instance. Finite extensions of known ﬁelds
can be speciﬁed like this. For instance suppose we wanted to use polynomials with
coefﬁcients in the ﬁeld Q(
√
2). We could use:
i8 : A = QQ[rt2]/(rt2ˆ2 - 2)
(as the notation seems to indicate, this is a quotient ring of the polynomial ring in
one variable modulo the ideal generated by the polynomial rt2ˆ2 - 2). Then
i9 : L = toField(A)
"converts" this to a ﬁeld that can be used as the coefﬁcient ﬁeld for a new polynomial
ring. The ﬁeld Q(i) could be deﬁned in a similar way.
The leading term, leading monomial, and leading coefﬁcient of a polynomial
with respect to the current monomial order are computed by commands leadTerm,
leadMonomial, and leadCoefficient. The leadTerm function can also be ap-
plied to an ideal I and the output will be a set of generators for the monomial ideal
⟨LT(I)⟩.
If we have ideals I,J in the current ring, Macaulay2 allows us to compute the
sum as I + J, the ideal product as I*J, the ideal quotient as quotient(I,J), the
saturation as saturate(I,J), and the intersection as intersect(I,J). Other
useful commands include:
• radical for the radical of an ideal.
• primaryDecomposition for primary decomposition.
• hilbertFunction(m,I) for a value of the Hilbert function.
• hilbertPolynomial(I,Projective=>false) gives the Hilbert polynomial
in the form we have discussed.
• hilbertSeries computes the Hilbert-Poincaré series from Chapter 10, §2.
Singular
Singular is a free, open-source computer algebra system for polynomial computa-
tions. Versions for most Unix systems, Mac OS X, and Windows may be down-
loaded from http://www.singular.uni-kl.de. Complete documentation can
also be found there. The version of Singular that runs on Windows systems uses
the Cygwin operating system virtualization software. As of summer 2014, the cur-
rent standard version is DECKER ET AL. (2012). The development of Singular has

§2 Special Purpose Programs: CoCoA, Macaulay2, Singular
615
been directed by Wolfram Decker, Gert-Martin Greuel, Gerhard Pﬁster, and Hans
Schönemann at the University of Kaiserslautern in Germany. Many other current
and former members of the Singular team have also participated in this effort.
Singular has a special emphasis on commutative algebra, algebraic geometry, and
singularity theory. It provides usable features for certain numerical computations,
but not the same level of support for those areas or for graphics found in general-
purpose packages. Singular's major strength is that it provides highly efﬁcient im-
plementations of its central algorithms (especially Gröbner basis computations in
polynomial rings and standard basis computations in localizations, free resolutions,
resultants, and so forth). A large number of advanced algorithms and contributed
packages in the ﬁelds mentioned above, plus a procedural programming language
with syntax similar to C are also provided. Interfaces with third-party software for
convex geometry, tropical geometry and visualization, plus a comprehensive online
manual and help resource are available.
Assignment statements in Singular generally indicate the type of the result (if
that has not been previously speciﬁed), then a name for the result, an equals sign,
and the command specifying the procedure used to compute the result.
To do Gröbner basis computations in Singular, one must ﬁrst deﬁne a polynomial
ring that contains all polynomials involved and that speciﬁes the monomial order to
be used. For instance, to compute a Gröbner basis of I = ⟨x2+y, 2xy+y2⟩⊆Q[x, y]
with respect to lex order with x > y, we could proceed as follows. First deﬁne the
ring with an input command like this:
> ring r = 0, (x,y), lp;
(the > represents the input prompt; all Singular commands end with a semicolon).
This deﬁnes a ring called r with a coefﬁcient ﬁeld of characteristic zero (that is, the
ﬁeld Q). The (x,y) is the list of ring variables; the lp indicates the lexicographic
order (with variables ordered as in the list). For a polynomial ring over a ﬁnite
ﬁeld with p elements (p a prime), just change the 0 in the ring deﬁnition to the
p desired. General monomial orders, including all of those discussed in the text,
can be speciﬁed in Singular. We have seen how to specify lex orders. Graded lex
orders are obtained with Dp, while grevlex orders use dp. Various weight orders as
in Exercise 10 of Chapter 2, §4, and general matrix orders are also available.
Then the ideal I can be deﬁned with the command
> ideal i = x2 + y,2xy + y2;
No special symbol is necessary to indicate the exponents, although the long form
> ideal i = xˆ2 + y,2*x*y + yˆ2;
is also recognized.
The Gröbner basis can be computed and displayed via the command
> groebner(i);
If the results are to be saved and used later under a different name, an assignment
like
> ideal gi = groebner(i);
could be used. A command
> i = groebner(i);

616
Appendix C Computer Algebra Systems
could also be used if we wanted to overwrite the name i. No type speciﬁcation is
needed there since i has already been deﬁned as an ideal in the ring r.
Singular's polynomial ring deﬁnition mechanism is quite ﬂexible. The ﬁeld of
coefﬁcients can be any ﬁnite extension of Q or a ﬁnite ﬁeld. Coefﬁcients in a ﬁeld
of rational functions over Q or over a prime ﬁeld may also be deﬁned in the ring
command. For example, suppose we wanted to work with polynomials in variables
x, y with coefﬁcients in the ﬁeld Q(
√
2). Here is one way to construct the ring we
want:
> ring r = (0,a), (x,y), lp;
> minpoly = a2 - 2;
Then the name a represents
√
2 in the coefﬁcient ﬁeld. By changing the minpoly
declaration we could also use Q(i) as coefﬁcient ﬁeld. The minpoly declaration
should come immediately after the ring deﬁnition; also Singular does not check for
irreducibility so this should be done manually before using a ﬁeld deﬁnition of this
type. Without the minpoly declaration, we would have the rational function ﬁeld
Q(a) as the ﬁeld of coefﬁcients. Any number of such symbolic parameters can be
deﬁned by placing their names in the list with the characteristic of the coefﬁcient
ﬁeld. One limitation here is that it is not possible to deﬁne a polynomial ring whose
coefﬁcient ﬁeld is a rational function ﬁeld over a ﬁnite extension of Q.
Most of the other basic operations we have discussed are provided in Singu-
lar as functions that are deﬁned on ideals or individual polynomials. For instance,
the leading term, leading monomial, and leading coefﬁcient of a polynomial f are
computed by lead(f), leadmonom(f), and leadcoef(f) respectively. If G is a
Gröbner basis for an ideal I and f is a polynomial in the ring containing I, then the
remainder on division by G is computed by
> reduce(f,G);
If G is not a Gröbner basis, then a warning is generated since the remainder is not
uniquely determined.
Other useful commands deﬁned on ideals include:
• intersect(I,J) computes the intersection of the ideals I and J.
• sat(I,J) computes the saturation I :J∞of I with respect to J.
• eliminate(I,m) computes an elimination ideal, where m is a monomial con-
taining the variables to be eliminated.
• finduni(I) computes univariate polynomials in I for all the variables appear-
ing, provided that I is zero-dimensional.
• hilb(I) gives a representation of the Hilbert-Poincaré series deﬁned in Chap-
ter 10, §2. This is deﬁned only for homogeneous ideals.
• hilbPoly(I), computes a representation of the Hilbert polynomial. This is part
of an external package, so the command LIB "poly.lib"; must be entered
before it is accessible. This is deﬁned only for homogeneous ideals.
• Primary decompositions can be computed in several different ways using the
functions in the primdec.lib library. We refer the interested reader to the Sin-
gular documentation or to the book GREUEL and PFISTER (2008).

§3 Other Systems and Packages
617
§3
Other Systems and Packages
In addition to the computer algebra systems described in the previous sections, the
following software may also be used for some or all the computations we have
discussed.
• The REDUCE system with the Groebner and Cali packages described in previous
editions of this book is still available at http://reduce-algebra.com.
• The computer algebra system Magma is designed for computations in commuta-
tive algebra, group theory, number theory, and combinatorics. It has a very efﬁ-
cient implementation of Gröbner basis algorithms and functionality comparable
to that of CoCoA, Macaulay2, and Singular. More information, documentation,
and a web-based Magma "calculator" for small computations can be found at
http://magma.maths.usyd.edu.au/magma.
• For MATLAB users, the Symbolic Math Toolbox contains the Gröbner basis
implementation from the MuPAD system. More information can be found at
http://www.mathworks.com/products/symbolic.
• The FGb package incorporating the F4 algorithm discussed in Chapter 10, §3 is
now standard in Maple, and is available in a standalone version callable from C
programs at http://www-polsys.lip6.fr/~jcf/Software/FGb.

Appendix D
Independent Projects
Unlike the rest of the book, this appendix is addressed to the instructor. We will
discuss several ideas for research papers or projects supplementing topics intro-
duced in the text.
§1
General Comments
Independent projects in a course based on this text can be valuable in several ways:
• They can help students to develop a deeper understanding of the ideas presented
in the text by applying what they have learned.
• They can expose students to further developments in subjects beyond what is
discussed in the text.
• They can give students more experience and sophistication as users of computer
algebra systems.
• Projects can be excellent opportunities for small groups of two or three students
to work together and learn collaboratively.
• More extensive and open-ended projects can even give students a taste of doing
mathematical research.
There is much more material in our book than can be covered in a single semester.
So a project could simply be to learn a part of the text that was not covered in class.
In this appendix, though, we will concentrate on additional topics beyond what is
in this book. In most cases students would need to start by reading from additional
sources and learn the mathematics involved. In some cases, the focus might then be
on implementing algorithms and computing examples. In others, the primary goal
might be to write an expository paper and/or give an oral presentation about what
they have learned.
The descriptions we give for each project area are rather brief. Although a few
references are provided, most of the descriptions would need to be narrowed down
and ﬂeshed out before being given to students as assignments. The list is in no way
© Springer International Publishing Switzerland 2015
D.A. Cox et al., Ideals, Varieties, and Algorithms, Undergraduate Texts
in Mathematics, DOI 10.1007/978-3-319-16721-3
619

620
Appendix D Independent Projects
deﬁnitive or exhaustive, and users of the text are encouraged to contact the authors
with comments or suggestions concerning these or other projects they have used.
§2
Suggested Projects
We discuss some ideas for more theoretical projects ﬁrst, then indicate project topics
where implementing algorithms in a computer algebra system might form a part of
the project. Finally, we indicate some ideas for additional areas where the techniques
we have discussed have been applied and that might form the basis for other projects.
1. The Complexity of the Ideal Membership Problem. In §10 of Chapter 2,
we brieﬂy discussed some of the worst-case complexity results concerning
the computation of Gröbner bases and solving the ideal membership problem.
The main purposes of this project would be to have students learn about the
Mayr and Meyer examples, understand the double exponential growth of de-
gree bounds for the ideal membership problem, and appreciate the implica-
tions for computational algebra. A suggested ﬁrst reference here is BAYER and
STILLMAN (1988). For larger projects taking the ideas in different directions
toward the frontier of research, the following sources may be useful. The arti-
cle KOH (1998) shows that similar double exponential behavior can be obtained
even with ideals generated by polynomials of total degree 2. The article SWAN-
SON (2004) studies the algebraic structure of the Mayr-Meyer ideals in much
greater detail and includes a list of questions aimed at identifying the precise
features producing their behavior. This involves quite a few topics not discussed
in our text (embedded primes, etc.) but studying this in a more extensive project
might be an interesting way to motivate learning those additional topics in com-
mutative algebra. Finally, ASCHENBRENNER (2004) discusses these complex-
ity questions for ideals in polynomial rings with coefﬁcients in Z rather than a
ﬁeld.
2. Symbolic Recipes for Solving Polynomial Systems. One of the applications
of computing Gröbner bases with respect to lex and other elimination orders
discussed in the text is ﬁnding the points in V(I) for zero-dimensional ideals
I. However, for larger and more realistic problems, the polynomials in a lex
Gröbner basis can have awkwardly large coefﬁcients and these can become
problematic if standard numerical root-ﬁnding techniques are applied to gener-
ate approximations to the points in V(I). This is true especially in the higher
dimensional analogs of the situation in Exercise 14 in Chapter 2, §7—systems
that are often said to be in "shape lemma form." So other symbolic recipes
for computing the solutions of these systems have been developed, all of which
make heavy use of linear algebra in the quotient ring C[x1, . . . , xn]/I, a ﬁnite di-
mensional vector space over C. Chapter 2 of COX, LITTLE and O'SHEA (2005),
Chapter 2 of DICKENSTEIN and EMIRIS (2005), or Chapter 2 of COHEN
et. al. (1999) present the background about multiplication matrices, eigenval-
ues, trace forms etc. that form the groundwork for these methods. Several differ-
ent project topics could be generated from the material there. For example, one

§2 Suggested Projects
621
project could simply be to learn how the linear algebra leads to the statement
and proof of what is now often called Stickelberger's Theorem—the statement
that the eigenvalues of the multiplication matrix for a polynomial f give the
values of f at the points in V(I)—Theorem (4.5) and Corollary (4.6) in Chapter
2, §4 of COX, LITTLE and O'SHEA (2005). Another project could deal with the
application of these methods to real root counting and real root isolation for
systems in several variables. Another very interesting project topic would be to
investigate the idea of a rational univariate representation (RUR) for the solu-
tions introduced in ROUILLIER (1999). An RUR expresses the coordinates of
the points in V(I) as rational functions of the roots of an auxiliary polynomial
equation where the variable is a so-called separating element—usually a linear
combination of the coordinates—taking distinct values at the distinct points in
V(I). The rational functions involved are typically signiﬁcantly simpler than the
polynomials in a lex Gröbner basis for the same ideal. Moreover the methods
used to compute RUR's come from the same circle of ideas about multiplication
matrices on C[x1, . . . , xn]/I, their traces, and so forth. The Maple Groebner
package contains a RationalUnivariateRepresentation command that
can be used to compute realistic examples.
3. Gröbner Basis Conversion via FGLM. In Chapter 10, we gave a version of
Buchberger's algorithm that used Hilbert functions to convert a Gröbner basis
with respect to one monomial order into a Gröbner basis for the same ideal with
respect to another monomial order. We mentioned that there were other meth-
ods known for these Gröbner basis conversions, including the FGLM algorithm
for zero-dimensional ideals. A number of different project topics could be de-
veloped in this area. The FGLM algorithm is also based on the vector space
structure of the quotient C[x1, . . . , xn]/I discussed in topic 2 above; the original
source is FAUGÈRE, GIANNI, LAZARD, and MORA (1993). See also Chapter
2 of COX, LITTLE and O'SHEA (2005). This is now implemented in most of
the computer algebra systems discussed in Appendix C but it is also a good
programming exercise. Connections with the Buchberger-Möller algorithm for
computing the vanishing ideal of a ﬁnite collection of points were developed in
MARINARI, MÖLLER, and MORA (1993).
4. Singular Points, Dual Curves, Evolutes, and other Geometric Applications.
The geometrical material on singular points of curves and envelopes of families
of curves discussed in §4 of Chapter 3 could be extended in several different
directions to give interesting project topics. A ﬁrst topic might involve learn-
ing some of the theoretical tools needed for a more complete understanding
of curve singularities: the Newton polygon, Puiseux expansions, resolution of
singularities by quadratic transformations of the plane, etc. A good general ref-
erence for this is BRIESKORN and KNÖRRER (1986). Another beautiful and
classical topic here would be to study the construction of the dual curve of a
projective plane curve, ﬁnding the implicit equation of the dual by elimination,
and perhaps discussing the Plücker formulas for curves with only nodal and
ordinary cuspidal singularities; FISCHER (2001) and BRIESKORN and KNÖR-
RER (1986) are good sources for this. The envelope of the family of normal

622
Appendix D Independent Projects
lines to a plane curve is also known as the evolute of the curve, and these were
studied intensively in classical differential and algebraic geometry, see for in-
stance BRUCE and GIBLIN (1992). Evolutes also arise naturally in considering
the critical points of the squared Euclidean distance function from a point to
a curve. So they are closely connected to the question of ﬁnding the point on
a curve closest to a given point—a typical constrained optimization problem.
DRAISMA, HOROBE ¸T, OTTAVIANI, STURMFELS, and THOMAS (2013) con-
tains a beautiful discussion of the connection and introduces the Euclidean
distance degree of a curve as a new invariant. That article also discusses far-
reaching generalizations to analogous higher-dimensional situations and dis-
cussions of many interesting applications of these ideas to areas such as geo-
metric modeling, computer vision, and stability in control theory. Some of this
is quite advanced, but this reference is a goldmine of interesting ideas.
5. Implicitization via Resultants. As mentioned in §6 of Chapter 3, resultants
can be used for elimination of variables, and this means they are applicable to
geometric problems such as implicitization. A nice project would be to report
on the papers ANDERSON, GOLDMAN and SEDERBERG (1984a), ANDERSON,
GOLDMAN, and SEDERBERG (1984b) and MANOCHA (1994).The resultants
used in these papers differ from the resultants discussed in Chapter 3, where
we deﬁned the resultant of two polynomials. For implicitization, one needs the
resultant of three or more polynomials, often called multipolynomial resultants.
These resultants are discussed in COX, LITTLE and O'SHEA (2005). On a dif-
ferent but related note, GALLET, RAHKOOY and ZAFEIRAKOPOULOS (2013)
discusses the general problem of the relation between a univariate generator of
an elimination ideal and elements of the elimination ideal computed by means
of resultants.
6. The General Version of Wu's Method. In our discussion of Wu's method in
geometric theorem proving in Chapter 6, §4, we did not introduce the gen-
eral algebraic techniques (characteristic sets, the Wu-Ritt decomposition algo-
rithm) that are needed for a general theorem prover. This project would involve
researching and presenting these methods, and possibly considering their re-
lations with other methods for elimination of variables. Implementing them
in a computer algebra system would also be a possibility. See WANG (2001)
for a discussion of characteristic sets and CHOU (1988) and WU (2001) for
complete presentations of the relations with geometric theorem-proving. The
article WU (1983) gives a summary. Also, AUBRY, LAZARD and MORENO
MAZA (1999) compares different theories of triangular sets of polynomial
equations including characteristic sets and JIN, LI and WANG (2013) gives a
new algorithmic scheme for computing characteristic sets.
7. Molien's Theorem. An interesting project could be built around Molien's the-
orem in invariant theory, which is mentioned in §3 of Chapter 7. This theorem
gives an expression for the so-called Molien series of a ﬁnite matrix group G
over C (that is, the generating function for the dimensions of the homogeneous
components of the ring of invariants of G analogous to the Hilbert-Poincaré
series studied in Chapter 10, §2):

§2 Suggested Projects
623
∞

m=0
dim(C[x1, . . . , xn]G
m)tm = 1
|G|

g∈G
1
det(I −tg).
The algorithm given in STURMFELS (2008) can be used to ﬁnd generators for
C[x1, . . . , xn]G. This can be used to ﬁnd the invariants of some larger groups
than those discussed in the text, such as the rotation group of the cube in R3.
Molien's theorem is also discussed in Chapter 7 of BENSON and GROVE (1985)
and Chapter 3 of DERKSEN and KEMPER (2002).
8. Computer Graphics and Vision. In §1 of Chapter 8, we used certain kinds of
projections when we discussed how to draw a picture of a 3-dimensional object.
These ideas are very important in computer graphics and computer vision. Sim-
pler projects in this area could describe various projections that are commonly
used in computer graphics and explain what they have to do with projective
space. If you look at the formulas in Chapter 6 of FOLEY, VAN DAM, FEINER
and HUGHES (1990), you will see certain 4×4 matrices. This is because points
in P3 have four homogeneous coordinates. More extensive projects might also
consider the triangulation problem in computer vision, which asks for a re-
construction of a 3-dimensional object from several 2-dimensional images pro-
duced by cameras viewing the object from different viewpoints. Techniques
from algebraic geometry have been applied successfully to this question. The
basic ideas are discussed in HEYDEN and ÅSTRÖM (1997) and the beautiful ar-
ticle AHOLT, STURMFELS and THOMAS (2013) studies the resulting multiview
ideals and varieties using tools such as universal Gröbner bases, multigraded
Hilbert functions and Hilbert schemes. Needless to say, much of this is be-
yond the scope of the topics discussed in this book but large parts of the article
AHOLT, STURMFELS and THOMAS (2013) will be accessible because the pre-
sentation is very concrete and smaller special cases can be computed explicitly.
There are also connections with the article DRAISMA, HOROBE ¸T, OTTAVIANI,
STURMFELS, and THOMAS (2013) mentioned in topic 4 above in the case that
the images are "noisy" and no exact reconstruction exists. In that case, the prob-
lem is to determine a 3-dimensional structure that comes as close as possible to
matching what is seen in the 2-dimensional images.
9. Gröbner Fans, Universal Gröbner Bases, Gröbner Basis Conversion via
the Gröbner walk. How many different reduced Gröbner bases are there for
any particular ideal? Is that collection ﬁnite or inﬁnite? The so-called Gröbner
fan of an ideal is a collection of polyhedral cones in Rn that provides a way to
see that there are only ﬁnitely many different reduced Gröbner bases. Under-
standing the Gröbner fan also gives a way to produce universal Gröbner bases
for ideals—ﬁnite collections of polynomials that are simultaneously Gröbner
bases for all possible monomial orderings. All of this is discussed, for instance,
in Chapter 8 of COX, LITTLE and O'SHEA (2005) and STURMFELS (1996).
One project topic would be simply to understand how all this works and pos-
sibly to generate some examples. The software package gfan authored by A.
Jensen is the current standard for these calculations and the Sage, Macaulay2,
and Singular systems discussed in Appendix C incorporate interfaces to gfan.

624
Appendix D Independent Projects
The structure of the Gröbner fan also gives the background needed for the other
Gröbner basis conversion method that we mentioned in passing in Chapter 10,
the so-called Gröbner walk algorithm. The original source for the Gröbner walk
is COLLART, KALKBRENER and MALL (1998) and this algorithm is discussed
in Section 5 of Chapter 8 in COX, LITTLE and O'SHEA (2005); more efﬁcient
versions such as the so-called fractal walk have been developed as well. Ver-
sions of the walk have been implemented in several of the computer algebra
systems mentioned in Appendix C, including Maple, Mathematica, Singular
(hence Sage), and Magma.
10. Gröbner Covers. As we have seen in Chapter 6, many systems of polynomial
equations that arise in applications naturally contain symbolic parameters ap-
pearing in their coefﬁcients. Understanding how and whether specializing those
parameters to particular constant values changes the form of a Gröbner basis
for the corresponding ideal and affects the number and form of the solutions of
the system is often extremely important. Weispfenning's theory of comprehen-
sive Gröbner bases and his algorithm for computing them was the ﬁrst major
step here. WEISPFENNING (1992) is the original source for this; BECKER and
WEISPFENNING (1993) gives a very brief description. More recently, the the-
ory of Gröbner covers presented in MONTES and WIBMER (2010) has provided
a way to ﬁnd a simpler decomposition of the parameter space into segments on
which the Gröbner bases of specializations have a constant "shape." Projects
in this area could have a theoretical orientation, or could focus on implemen-
tation. As of July 2014 a package for the Singular computer algebra system
is under development (see http://www-ma2.upc.edu/montes/). Another
possibility for a larger project would be to include the article MONTES and RE-
CIO (2014), which uses Gröbner covers to discover "missing hypotheses" for
automatic discovery of theorems in elementary geometry, an extension of the
automatic theorem proving considered in Chapter 6.
11. Gröbner Bases for Modules and Applications. The notion of an ideal I ⊆
k[x1, . . . , xn] can be generalized to a submodule M ⊆k[x1, . . . , xn]r and there is
a natural way to deﬁne term orders (touched on brieﬂy in Chapter 10, §4) and
Gröbner bases for modules. The basic deﬁnitions can be found in ADAMS and
LOUSTAUNAU (1994), BECKER and WEISPFENNING (1993), COX, LITTLE
and O'SHEA (2005), KREUZER and ROBBIANO (2000), and EISENBUD (1999).
Indeed, even the theory of Gröbner bases for ideals naturally involves modules
such as the module of syzygies on a set of generators for an ideal or their leading
terms, so KREUZER and ROBBIANO (2000) develops the theory for ideals and
for modules simultaneously. One possible project here would be to understand
how this all works and how Buchberger's algorithm generalizes to this setting.
This is implemented, for example, in CoCoA, Macaulay2, Sage and Singular
and it can be emulated via a standard trick in Maple [the idea is discussed in Ex-
ercise 6 of Chapter 2, Section 5 in COX, LITTLE and O'SHEA (2005)]. Another
project topic building on this would be the application of modules to the con-
struction of multivariate polynomial splines—piecewise polynomial functions
of a given degree on a given polyhedral decomposition of a region in Rn with

§2 Suggested Projects
625
a given degree of smoothness. This is discussed in Chapter 8 of COX, LITTLE
and O'SHEA (2005) and the sources cited there. Other applications that might
be considered in a project include methods for multivariate Padé approximation
[see FARR and GAO (2006) for the latest work on this] and related decoding al-
gorithms for certain error control codes [see Chapter 9 in COX, LITTLE and
O'SHEA (2005)].
12. Border Bases. As we know from Chapter 5, if I is a zero-dimensional ideal,
the monomials in the complement of ⟨LT(I)⟩form a vector space basis for
k[x1, . . . , xn]/I and linear algebra in those quotient rings has appeared in several
of the project topics listed above. From work of Stetter, Möller and Mourrain, it
is known there are other ways to ﬁnd good monomial bases for k[x1, . . . , xn]/I
yielding normal forms modulo I and special sets of generators for I that are
different from the corresponding information obtained from any Gröbner basis.
Moreover, some of these alternatives yield representations of k[x1, . . . , xn]/I
that make it easier to compute good numerical approximations for the points in
V(I). There is now a well-developed algebraic theory of border bases for I that
parallels Gröbner basis theory, but with some interesting twists. Several inter-
esting project topics here might involve presenting this theory or implement-
ing border division and normal forms in a computer algebra system. Chapter 4
of DICKENSTEIN and EMIRIS (2005)—by Kehrein, Kreuzer, and Robbiano—
contains an excellent summary of this theory.
13. Algebraic Statistics. The rapidly developing ﬁeld of algebraic statistics is
based on the idea that many statistical models (i.e., families of probability dis-
tributions) for discrete data can be seen as algebraic varieties. Moreover the
geometry of those varieties determines the behavior of parameter estimation
and statistical inference procedures. A typical example is the family of bino-
mial distributions. The probability that a binomial random variable X (based on
n trials with success probability θ on each trial) takes value k ∈{0, 1, . . ., n} is
pk = P(X = k) =
n
k

θk(1 −θ)n−k.
Viewing these as components of a curve parametrized by real θ satisfying
0 ≤θ ≤1, we have a subset of the real points of a rescaled rational
normal curve of degree n lying in the hyperplane deﬁned by the equation
p0+· · ·+pn = 1. Given some number of observations we might want to estimate
θ using maximum likelihood estimation, and this leads to a constrained opti-
mization problem involving polynomial equations. A good introduction to the
basics of model construction and experimental design can be found in PISTONE,
RICCOMAGNO, and WYNN (2001). A discussion of algebraic techniques for
maximum likelihood estimation appears in Chapter 2 of DRTON,STURMFELS,
and SULLIVANT (2009). One of the main applications of these ideas so far has
been in genomics. For students with the requisite background, the Jukes-Cantor
models studied in Part I of PACHTER and STURMFELS (2005) could form the
basis of a more extensive project. A different sort of application to design of

626
Appendix D Independent Projects
experiments can be found in Chapter 4 by Kehrein, Kreuzer, and Robbiano in
DICKENSTEIN and EMIRIS (2005). This draws on the material on border bases
discussed in the previous topic description.
14. Graph Coloring Problems and Sudoku. The ﬁnal project idea we will pro-
pose involves the use of polynomials and varieties to study the solution of var-
ious graph coloring problems and related questions. The ﬁrst discussion of this
connection that we know of appears in BAYER (1982), which uses polynomial
methods to solve the three-coloring problem for graphs. Section 2.7 of ADAMS
and LOUSTAUNAU (1994) contains a discussion of this as well. More recently,
a number of authors have presented applications of these ideas to the popu-
lar Sudoku and similar puzzles. ARNOLD, LUCAS and TAALMAN (2010) dis-
cusses different polynomial translations focusing on a 4×4 version of the usual
Sudoku. Chapter 3 of DECKER and PFISTER (2013) presents one particular
polynomial translation and gives Singular procedures for generating the rele-
vant polynomial ideals and solving standard Sudoku puzzles. Several different
sorts of projects would be possible here from more theoretical discussions to
implementations of one or more approaches, comparisons between them, and
so on.
There are many other places where instructors can look for potential project top-
ics for students, including the following:
• COX, LITTLE and O'SHEA (2005) includes material on local rings, additional
topics in algebraic coding theory, and applications to combinatorial enumeration
problems and integer programming that could serve as the basis for projects.
• ADAMS and LOUSTAUNAU (1994) contains sections on minimal polynomials
of ﬁeld extensions and integer programming. These could serve as the basis for
interesting projects.
• EISENBUD (1999) has a list of seven projects in section 15.12. These are more
sophisticated and require more background in commutative algebra, but they also
introduce the student to some topics of current interest in algebraic geometry.
• KREUZER and ROBBIANO (2000) and KREUZER and ROBBIANO (2005) contain
a large number of tutorials on various topics (usually at least one at the end of
each section). These would be especially good for smaller-scale projects where
the path to be followed by the student would be laid out in detail at the start.
If you ﬁnd good student projects different from those listed above, we would be
interested in hearing about them. There are a lot of wonderful things one can do with
Gröbner bases and algebraic geometry, and the projects described in this appendix
barely scratch the surface.

References
J. Abbott, A. Bigatti, G. Lagorio, CoCoA-5: A System for Doing Computations in
Commutative Algebra (2014), available at http://cocoa.dima.unige.it
W. Adams, P. Loustaunau, An Introduction to Gröbner Bases. Graduate Studies in
Mathematics, vol. 3 (AMS, Providence, 1994)
C. Aholt, B. Sturmfels, R. Thomas, A Hilbert scheme in computer vision. Can. J.
Math. 65, 961-988 (2013)
D. Anderson, R. Goldman, T. Sederberg, Implicit representation of parametric
curves and surfaces. Comput. Vis. Graph. Image Des. 28, 72-84 (1984a)
D. Anderson, R. Goldman, T. Sederberg, Vector elimination: a technique for the im-
plicitization, inversion and intersection of planar parametric rational polynomial
curves. Comput. Aided Geom. Des. 1, 327-356 (1984b)
E. Arnold, S. Lucas, L. Taalman, Gröbner basis representations of sudoku. Coll.
Math. J. 41, 101-112 (2010)
M. Aschenbrenner, Ideal membership in polynomial rings over the integers. J. Am.
Math. Soc. 17, 407-441 (2004)
M.F. Atiyah, I.G. MacDonald, Introduction to Commutative Algebra (Addison-
Wesley, Reading, MA, 1969)
P. Aubry, D. Lazard, M. Moreno Maza, On the theories of triangular sets. J. Symb.
Comput. 28, 105-124 (1999)
J. Baillieul et al., Robotics. In: Proceedings of Symposia in Applied Mathematics,
vol. 41 (American Mathematical Society, Providence, Rhode Island, 1990)
A.A. Ball, The Parametric Representation of Curves and Surfaces Using Rational
Polynomial Functions, in The Mathematics of Surfaces, II, ed. by R.R. Martin
(Clarendon Press, Oxford, 1987), pp. 39-61
D. Bayer, The division algorithm and the Hilbert scheme, Ph.D. thesis, Harvard
University, 1982
D. Bayer, D. Mumford, What Can Be Computed in Algebraic Geometry?, in Com-
putational Algebraic Geometry and Commutative Algebra, ed. by D. Eisenbud,
L. Robbiano (Cambridge University Press, Cambridge, 1993), pp. 1-48
D. Bayer, M. Stillman, A criterion for detecting m-regularity. Invent. Math. 87, 1-11
(1987a)
© Springer International Publishing Switzerland 2015
D.A. Cox et al., Ideals, Varieties, and Algorithms, Undergraduate Texts
in Mathematics, DOI 10.1007/978-3-319-16721-3
627

628
References
D. Bayer, M. Stillman, A theorem on reﬁning division orders by the reverse lexico-
graphic order. Duke J. Math. 55, 321-328 (1987b)
D. Bayer, M. Stillman, On the Complexity of Computing Syzygies, in Computa-
tional Aspects of Commutative Algebra, ed. by L. Robbiano (Academic Press,
New York, 1988), pp. 1-13
T. Becker, V. Weispfenning, Gröbner Bases (Springer, New York-Berlin-
Heidelberg, 1993)
C.T. Benson, L.C. Grove, Finite Reﬂection Groups, 2nd edn. (Springer, New York-
Berlin-Heidelberg, 1985)
A. Bigatti, Computation of Hilbert-Poincaré series. J. Pure Appl. Algebra 119,
237-253 (1997)
E. Brieskorn, H. Knörrer, Plane Algebraic Curves (Birkhäuser, Basel-Boston-
Stuttgart, 1986)
J.W. Bruce, P.J. Giblin, Curves and Singularities, 2nd edn. (Cambridge University
Press, Cambridge, 1992)
B. Buchberger, Ein algorithmus zum aufﬁnden der basiselemente des restklassen-
rings nach einem nulldimensionalen polynomideal, Doctoral Thesis, Mathemati-
cal Institute, University of Innsbruck, 1965. English translation An algorithm for
ﬁnding the basis elements of the residue class ring of a zero dimensional polyno-
mial ideal by M.P. Abramson, J. Symb. Comput. 41, 475-511 (2006)
B. Buchberger, Groebner Bases: An Algorithmic Method in Polynomial Ideal The-
ory, in Multidimensional Systems Theory, ed. by N.K. Bose (D. Reidel Publish-
ing, Dordrecht, 1985), pp. 184-232
M. Caboara, J. Perry, Reducing the size and number of linear programs in a dynamic
Gröbner basis algorithm. Appl. Algebra Eng. Comm. Comput. 25, 99-117 (2014)
J. Canny, D. Manocha, Algorithm for implicitizing rational parametric surfaces.
Comput. Aided Geom. Des. 9, 25-50 (1992)
S.-C. Chou, Mechanical Geometry Theorem Proving (D. Reidel Publishing, Dor-
drecht, 1988)
H. Clemens, A Scrapbook of Complex Curve Theory, 2nd edn. (American Mathe-
matical Society, Providence, Rhode Island, 2002)
A. Cohen, H. Cuypers, H. Sterk (eds.), Some Tapas of Computer Algebra (Springer,
Berlin-Heidelberg-New York, 1999)
S. Collart, M. Kalkbrener, D. Mall, Converting bases with the Gröbner walk. J.
Symb. Comput. 24, 465-469 (1998)
D. Cox, J. Little, D. O'Shea, Using Algebraic Geometry, 2nd edn. (Springer, New
York, 2005)
H.S.M. Coxeter, Regular Polytopes, 3rd edn. (Dover, New York, 1973)
J.H. Davenport, Y. Siret, E. Tournier, Computer Algebra, 2nd edn. (Academic, New
York, 1993)
W. Decker, G.-M. Greuel, G. Pﬁster, H. Schönemann, Singular 3-1-6—A computer
algebra system for polynomial computations (2012), available at http://www.
singular.uni-kl.de
W. Decker, G. Pﬁster, A First Course in Computational Algebraic Geometry. AIMS
Library Series (Cambridge University Press, Cambridge, 2013)

References
629
H. Derksen, G. Kemper, Computational Invariant Theory (Springer, Berlin-
Heidelberg-New York, 2002)
A. Dickenstein, I. Emiris (eds.), Solving Polynomial Equations (Springer, Berlin-
Heidelberg-New York, 2005)
J. Draisma, E. Horobe¸t, G. Ottaviani, B. Sturmfels, R. Thomas, The Euclidean dis-
tance degree of an algebraic variety (2013). arXiv:1309.0049 [math.AG]
M. Drton, B. Sturmfels, S. Sullivant, Lectures on Algebraic Statistics. Oberwohlfach
Mathematical Seminars, vol. 39 (Birkhäuser, Basel-Boston-Berlin, 2009)
T.W. Dubé, The structure of polynomial ideals and Gröbner bases. SIAM J. Comput.
19, 750-775 (1990)
D. Dummit, R. Foote, Abstract Algebra, 3rd edn. (Wiley, New York, 2004)
C. Eder, J. Faugère, A survey on signature-based Gröbner basis algorithms (2014).
arXiv:1404.1774 [math.AC]
D. Eisenbud, Commutative Algebra with a View Toward Algebraic Geometry, 3d
corrected printing (Springer, New York-Berlin-Heidelberg, 1999)
D. Eisenbud, C. Huneke, W. Vasconcelos, Direct methods for primary decomposi-
tion. Invent. Math. 110, 207-235 (1992)
J. Farr, S. Gao, Gröbner bases and generalized Padé approximation. Math. Comp.
75, 461-473 (2006)
J. Faugère, A new efﬁcient algorithm for computing Gröbner bases (F4). J. Pure
Appl. Algebra 139, 61-88 (1999)
J. Faugère, Finding All the Solutions of Cyclic 9 Using Gröbner Basis Techniques, in
Computer Mathematics (Matsuyama, 2001), Lecture Notes Ser. Comput., vol. 9
(World Scientiﬁc, River Edge, NJ, 2001), pp. 1-12
J. Faugère, A new efﬁcient algorithm for computing Gröbner bases without reduc-
tion to zero F5. In: Proceedings of ISSAC'02, Villeneuve d'Ascq, France, July
2002, 15-82; revised version from http://www-polsys.lip6.fr/~jcf/
Publications/index.html
J. Faugère, P. Gianni, D. Lazard, T. Mora, Efﬁcient change of ordering for Gröbner
bases of zero-dimensional ideals. J. Symb. Comput. 16, 329-344 (1993)
G. Fischer, Plane Algebraic Curves (AMS, Providence, Rhode Island, 2001)
J. Foley, A. van Dam, S. Feiner, J. Hughes, Computer Graphics: Principles and
Practice, 2nd edn. (Addison-Wesley, Reading, MA, 1990)
W. Fulton, Algebraic Curves (W. A. Benjamin, New York, 1969)
M. Gallet, H. Rahkooy, Z. Zafeirakopoulos, On Computing the Elimination Ideal
Using Resultants with Applications to Gröbner Bases (2013). arXiv:1307.5330
[math.AC]
J. von zur Gathen, J. Gerhard, Modern Computer Algebra, 3rd edn. (Cambridge
University Press, Cambridge, 2013)
C.F. Gauss, Werke, vol. III (Königlichen Gesellschaft der Wissenschaften zu Göttin-
gen, Göttingen, 1876)
R. Gebauer, H.M. Möller, On an Installation of Buchberger's Algorithm, in Compu-
tational Aspects of Commutative Algebra, ed. by L. Robbiano (Academic Press,
New York, 1988), pp. 141-152

630
References
I. Gelfand, M. Kapranov, A. Zelevinsky, Discriminants, Resultants and Multidimen-
sional Determinants (Birkhäuser, Boston, 1994)
P. Gianni, B. Trager, G. Zacharias, Gröbner bases and primary decomposition of
polynomial ideals, in Computational Aspects of Commutative Algebra, ed. by L.
Robbiano (Academic Press, New York, 1988), pp. 15-33
A. Giovini, T. Mora, G. Niesi, L. Robbiano, C. Traverso, "One sugar cube, please,"
or Selection Strategies in the Buchberger Algorithm, in ISSAC 1991, Proceedings
of the 1991 International Symposium on Symbolic and Algebraic Computation,
ed. by S. Watt (ACM Press, New York, 1991), pp. 49-54
M. Giusti, J. Heintz, La détermination des points isolés et de la dimension d'une var-
iété algébrique peut se faire en temps polynomial, in Computational Algebraic
Geometry and Commutative Algebra, ed. by D. Eisenbud, L. Robbiano (Cam-
bridge University Press, Cambridge, 1993), pp. 216-256
L. Glebsky, A proof of Hilbert's Nullstellensatz Based on Groebner bases (2012).
arXiv:1204.3128 [math.AC]
R. Goldman, Pyramid Algorithms: A Dynamic Programming Approach to Curves
and Surfaces in Geometric Modeling (Morgan Kaufman, Amsterdam, Boston,
2003)
D. Grayson, M. Stillman, Macaulay2, a Software System for Research (2013), ver-
sion 1.6, available at http://www.math.uiuc.edu/Macaulay2/
G.-M. Greuel, G. Pﬁster, A Singular Introduction to Commutative Algebra, 2nd edn.
(Springer, New York, 2008)
P. Grifﬁths, Introduction to Algebraic Curves. Translations of Mathematical Mono-
graphs, vol. 76 (AMS, Providence, 1989)
P. Gritzmann, B. Sturmfels, Minkowski addition of polytopes: computational com-
plexity and applications to Gröbner bases. SIAM J. Discrete Math. 6, 246-269
(1993)
J. Harris, Algebraic Geometry, A First Course, corrected edition (Springer, New
York, 1995)
R. Hartshorne, Algebraic Geometry (Springer, New York, 1977)
G. Hermann, Die Frage der endlich vielen schritte in der theorie der polynomideale,
Math. Ann. 95, 736-788 (1926)
A. Heyden, K. Åström, Algebraic properties of multilinear constraints. Math. Meth-
ods Appl. Sci. 20, 1135-1162 (1997)
D. Hilbert, Über die Theorie der algebraischen Formen, Math. Ann. 36, 473-534
(1890). Reprinted in Gesammelte Abhandlungen, vol. II (Chelsea, New York,
1965)
D. Hilbert, Theory of Algebraic Invariants (Cambridge University Press, Cam-
bridge, 1993)
J. Hilmar, C. Smyth, Euclid meets Bézout: intersecting algebraic plane curves with
the Euclidean algorithm. Am. Math. Monthly 117, 250-260 (2010)
H. Hironaka, Resolution of singularities of an algebraic variety over a ﬁeld of char-
acteristic zero I, II. Ann. Math. 79, 109-203, 205-326 (1964)
W.V.D. Hodge, D. Pedoe, Methods of Algebraic Geometry, vol. I and II (Cambridge
University Press, Cambridge, 1968)

References
631
M. Jin, X. Li, D. Wang, A new algorithmic scheme for computing characteristic
sets. J. Symb. Comput. 50, 431-449 (2013)
J. Jouanolou, Le formalisme du résultant. Adv. Math. 90, 117-263 (1991)
M. Kalkbrener, Implicitization by Using Gröbner Bases, Technical Report RISC-
Series 90-27 (University of Linz, Austria, 1990)
K. Kendig, Elementary Algebraic Geometry, 2nd edn. (Dover, New York, 2015)
F. Kirwan, Complex Algebraic Curves. London Mathematical Society Student Texts,
vol. 23 (Cambridge University Press, Cambridge, 1992)
F. Klein, Vorlesungen über das Ikosaeder und die Auﬂösung der Gleichungen vom
Fünften Grade (Teubner, Leipzig, 1884). English Translation, Lectures on the
Ikosahedron and the Solution of Equations of the Fifth Degree (Trubner, London,
1888). Reprinted by Dover, New York (1956)
J. Koh, Ideals generated by quadrics exhibiting double exponential degrees. J. Al-
gebra 200, 225-245 (1998)
M. Kreuzer, L. Robbiano, Computational Commutative Algebra, vol. 1 (Springer,
New York, 2000)
M. Kreuzer, L. Robbiano, Computational Commutative Algebra, vol. 2 (Springer,
New York, 2005)
T. Krick, A. Logar, An Algorithm for the Computation of the Radical of an Ideal
in the Ring of Polynomials, in Applied Algebra, Algebraic Algorithms and Error-
Correcting Codes, ed. by H.F. Mattson, T. Mora, T.R.N. Rao. Lecture Notes in
Computer Science, vol. 539 (Springer, Berlin, 1991), pp. 195-205
D. Lazard, Gröbner Bases, Gaussian Elimination and Resolution of Systems of Al-
gebraic Equations, in Computer Algebra: EUROCAL 83, ed. by J.A. van Hulzen.
Lecture Notes in Computer Science, vol. 162 (Springer, Berlin, 1983), pp. 146-
156
D. Lazard, Systems of Algebraic Equations (Algorithms and Complexity), in Com-
putational Algebraic Geometry and Commutative Algebra, ed. by D. Eisenbud,
L. Robbiano (Cambridge University Press, Cambridge, 1993), pp. 84-105
M. Lejeune-Jalabert, Effectivité des calculs polynomiaux, Cours de DEA 1984-85,
Institut Fourier, Université de Grenoble I (1985)
F. Macaulay, On some formulæin elimination. Proc. Lond. Math. Soc. 3, 3-27
(1902)
D. Manocha, Solving systems of polynomial equations. IEEE Comput. Graph. Appl.
14, 46-55 (1994)
Maple 18, Maplesoft, a division of Waterloo Maple Inc., Waterloo, Ontario (2014).
http://www.maplesoft.com
M. Marinari, H. Möller, T. Mora, Gröbner bases of ideals deﬁned by functionals
with an application to ideals of projective points. Appl. Algebra Eng. Comm.
Comput. 4, 103-145 (1993)
Mathematica 10, Wolfram Research, Inc., Champaign, Illinois (2014). http://
www.wolfram.com/mathematica
H. Matsumura, Commutative Ring Theory (Cambridge University Press, Cam-
bridge, 1989)

632
References
E. Mayr, A. Meyer, The complexity of the word problem for commutative semi-
groups and polynomial ideals. Adv. Math. 46, 305-329 (1982)
R. Mines, F. Richman, W. Ruitenburg, A Course in Constructive Algebra (Springer,
New York-Berlin-Heidelberg, 1988)
B. Mishra, Algorithmic Algebra. Texts and Monographs in Computer Science
(Springer, New York-Berlin-Heidelberg, 1993)
H.M. Möller, F. Mora, Upper and Lower Bounds for the Degree of Groebner Bases,
in EUROSAM 1984, ed. by J. Fitch. Lecture Notes in Computer Science, vol. 174
(Springer, New York-Berlin-Heidelberg, 1984), pp. 172-183
A. Montes, T. Recio, Generalizing the Steiner-Lehmus theorem using the Gröbner
cover. Math. Comput. Simul. 104, 67-81 (2014)
A. Montes, M. Wibmer, Gröbner bases for polynomial systems with parameters.
J. Symb. Comput. 45, 1391-1425 (2010)
D. Mumford, Algebraic Geometry I: Complex Projective Varieties, cCorrected 2nd
printing (Springer, New York-Berlin-Heidelberg, 1981)
L. Pachter, B. Sturmfels (eds.), Algebraic Statistics for Computational Biology
(Cambridge University Press, Cambridge, 2005)
R. Paul, Robot Manipulators: Mathematics, Programming and Control (MIT Press,
Cambridge, MA, 1981)
G. Pistone, E. Riccomagno, H. Wynn, Algebraic Statistics: Computational Commu-
tative Algebra in Statistics. Monographs on Statistics and Applied Probability,
vol. 89 (Chapman and Hall, Boca Raton, FL, 2001)
L. Robbiano, On the theory of graded structures. J. Symb. Comp. 2, 139-170 (1986)
F. Rouillier, Solving zero-dimensional systems through the rational univariate rep-
resentation. Appl. Algebra Eng. Comm. Comput. 5, 433-461 (1999)
P. Schauenburg, A Gröbner-based treatment of elimination theory for afﬁne vari-
eties. J. Symb. Comput. 42, 859-870 (2007)
A. Seidenberg, Constructions in algebra. Trans. Am. Math. Soc. 197, 273-313
(1974)
A. Seidenberg, On the Lasker-Noether decomposition theorem. Am. J. Math. 106,
611-638 (1984)
J.G. Semple, L. Roth, Introduction to Algebraic Geometry (Clarendon Press,
Oxford, 1949)
I.R. Shafarevich, Basic Algebraic Geometry 1, 2, 3rd edn. (Springer, New York-
Berlin-Heidelberg, 2013)
L. Smith, Polynomial Invariants of Finite Groups (A K Peters, Wellesley, MA, 1995)
W. Stein et al., Sage Mathematics Software, version 6.3. The Sage Development
Team (2014), available at http://www.sagemath.org
B. Sturmfels, Computing ﬁnal polynomials and ﬁnal syzygies using Buchberger's
Gröbner bases method. Results Math. 15, 351-360 (1989)
B. Sturmfels, Gröbner Bases and Convex Polytopes. University Lecture Series,
vol. 8 (American Mathematical Society, Providence, RI, 1996)
B. Sturmfels, Algorithms in Invariant Theory, 2nd edn. Texts and Monographs in
Symbolic Computation (Springer, New York-Vienna, 2008)

References
633
I. Swanson, On the embedded primes of the Mayr-Meyer ideals. J. Algebra 275,
143-190 (2004)
C. Traverso, Hilbert functions and the Buchberger algorithm. J. Symb. Comput. 22,
355-376 (1997)
P. Ullrich, Closed-form formulas for projecting constructible sets in the theory of
algebraically closed ﬁelds. ACM Commun. Comput. Algebra 40, 45-48 (2006)
B. van der Waerden, Moderne Algebra, Volume II (Springer, Berlin, 1931). English
translations, Modern Algebra, Volume II (F. Ungar Publishing, New York, 1950);
Algebra, Volume 2 (F. Ungar Publishing, New York, 1970); and Algebra, Volume
II (Springer, New York-Berlin-Heidelberg, 1991). The chapter on Elimination
Theory is included in the ﬁrst three German editions and the 1950 English trans-
lation, but all later editions (German and English) omit this chapter
R. Walker, Algebraic Curves (Princeton University Press, Princeton, 1950).
Reprinted by Dover, 1962
D. Wang, Elimination Methods, Texts and Monographs in Symbolic Computation
(Springer, Vienna, 2001)
V. Weispfenning, Comprehensive Gröbner bases. J. Symb. Comput. 14, 1-29 (1992)
F. Winkler, On the complexity of the Gröbner bases algorithm over K[x, y, z], in
EUROSAM 1984, ed. by J. Fitch. Lecture Notes in Computer Science, vol. 174
(Springer, New York-Berlin-Heidelberg, 1984), pp. 184-194
W.-T. Wu, On the decision problem and the mechanization of theorem-proving in
elementary geometry, in Automated Theorem Proving: After 25 Years, ed. by W.
Bledsoe, D. Loveland. Contemporary Mathematics, vol. 29 (American Mathe-
matical Society, Providence, Rhode Island, 1983), pp. 213-234
W.-T. Wu, Mathematics Mechanization: Mechanical Geometry Theorem-Proving,
Mechanical Geometry Problem-Solving and Polynomial Equations-Solving
(Kluwer, Dordrecht, 2001)

Index
Abbott, J., 611, 627
Adams, W., 218, 624, 626, 627
admissible geometric theorem, 322
afﬁne cone over a projective variety, see
cone, afﬁne
afﬁne Dimension Theorem, see Theorem,
Afﬁne Dimension
afﬁne Hilbert function, see Hilbert function,
afﬁne
afﬁne Hilbert polynomial, see polynomial,
afﬁne Hilbert
afﬁne space, see space, afﬁne
afﬁne transformation, see transformation,
afﬁne
afﬁne variety, see variety, afﬁne
Agnesi, M., 24
Aholt, C., 623, 627
algebra over a ﬁeld, 277
ﬁnitely generated, 277
homomorphism, 278
reduced, 278
algebraic statistics, 625-626
algebraically independent, 306, 327-329,
333, 337, 507-511, 513
algorithm
algebra (subring) membership, 349, 369
associated primes, 231
Buchberger's, 80, 90-97, 109-119, 413,
539-543, 548, 557-560
Closure Theorem, 225
computation in k[x1, . . . , xn]/I, 251
ComputeM (F4 SymbolicPreprocessing),
571
consistency, 179
degree by degree Buchberger (homoge-
neous ideals), 542
dimension (afﬁne variety), 491
dimension (projective variety), 493
division in k[x], 38-40, 54, 171, 241, 284,
288, 335
division in k[x1, . . . , xn], 33, 62-70, 248,
255, 280, 315, 335, 349, 355, 413,
569, 582, 583
Euclidean, 42-44, 95, 96, 161, 170, 171,
187, 462
F4, ix, 567-576, 590, 617
F5, ix, 576, 580, 581, 585, 589
FGLM, 564, 590, 621
ﬁniteness of solutions, 251, 606
Gaussian elimination (row reduction), 9,
54, 94, 166, 548, 549, 567, 568, 570,
572, 575
greatest common divisor, 41, 44, 187, 196
Gröbner walk, 564, 590, 608, 623
Hilbert driven Buchberger, ix, 550-567,
590
HPS (Hilbert-Poincaré series), 556-558
ideal equality, 94
ideal intersection, 193-195
ideal membership, 97, 151, 184
ideal quotient, 205
improved Buchberger, 109-119
irreducibility, 218
least common multiple, 196
Matrix F5, 589
polynomial implicitization, 135
primality, 218
primary decomposition, 231
projective closure, 419
projective elimination, 429
pseudodivision, 335-336
radical generators, 184
© Springer International Publishing Switzerland 2015
D.A. Cox et al., Ideals, Varieties, and Algorithms, Undergraduate Texts
in Mathematics, DOI 10.1007/978-3-319-16721-3
635

636
Index
radical ideal, 184
radical membership, 184, 185, 325
rational implicitization, 139
regular s-reduction, 583, 591
resultant, 172
Ritt's decomposition, 337, 342, 622
saturation, 206
signature-based Gröbner basis, 576-591
tangent cone, 528
altitude, 322, 331
Anderson, D., 140, 622, 627
Arnold, E., 626, 627
ascending chain condition (ACC), 80, 82,
92, 115, 202, 212, 409, 478, 572
Aschenbrenner, M., 620, 627
Åström, K., 623, 630
Atiyah, M.F., 230, 627
Aubry, P., 622, 627
automatic geometric theorem proving,
319-343, 624
automorphism of a variety, 264
Bézier, P., 21
cubic, see cubic, Bézier
Baillieul, J., 314, 627
Ball, A.A., 27, 627
Barrow, I., 24
basis
minimal, 36, 74, 93
minimal Gröbner, see Gröbner basis,
minimal
of an ideal, see ideal, basis for
reduced Gröbner, see Gröbner basis,
reduced
standard, 78
Bayer, D., x, 76, 117, 128, 540, 620, 626,
627
Becker, T., 60, 83, 84, 116, 194, 218, 309,
543, 624, 628
Benson, C.T., 357, 623, 628
Bernoulli, J., 24
Bezout's Theorem, see Theorem, Bezout
Bigatti, A., 557, 611, 627, 628
bihomogeneous polynomial, see polyno-
mial, bihomogeneous
birationally equivalent varieties, 273-276,
302, 512, 514
blow-up, 536-538
Brieskorn, E., 464, 467, 621, 628
Bruce, J.W., 143, 148, 152, 622, 628
Buchberger's Criterion, 86-88, 91, 92,
104-113, 568, 569, 573, 576, 584
Buchberger, B., vii, 79, 116, 248, 314, 628
Caboara, M., 117, 628
Canny, J., 140, 628
centroid, 331-333, 343
chain
ascending, of ideals, 79, 80
descending, of varieties, 82, 212, 226,
409
characteristic of a ﬁeld, see ﬁeld
characteristic sets, 337, 342, 622
Chou, S.-C., 335, 338, 342, 343, 622, 628
circumcenter, 332
cissoid, see curve, cissoid of Diocles
classiﬁcation of varieties, 238, 260, 275
Classiﬁcation Theorem for Quadrics, see
Theorem, Normal Form, for Quadrics
Clemens, H., 464, 628
closure
projective, 417-422, 431, 432, 439, 502,
505, 506, 536
Zariski, 131, 199-205, 208, 209, 211,
216, 219, 226, 282, 287, 531, 532, 537
Closure Theorem, 108, see Theorem,
Closure
CoCoA, see computer algebra systems,
CoCoA
coefﬁcient, 2
cofactor, 171, 280, 549, 597
Cohen, A., 620, 628
Collart, S., 564, 624, 628
collinear, 321-323, 331, 332
colon ideal, see ideal, quotient
commutative ring, see ring, commutative
complexity, 116-119, 255, 620
comprehensive Gröbner basis, see Gröbner
basis, comprehensive
computer aided geometric design (CAGD),
20-22
computer algebra systems
CoCoA, 184, 231, 495, 611-612, 624,
627
FGb package, 617
gfan, 623
Macaulay, 540
Macaulay2, 184, 231, 495, 613-614, 623,
624, 630
Magma, 590, 617, 624
Maple, 231, 495, 560, 590, 604-606, 621,
624, 631
Mathematica, 607-608, 624, 631
MATLAB, 617
REDUCE, 617
Sage, 495, 608-610, 623, 624, 632
Singular, 184, 231, 495, 590, 614-616,
623, 624, 626, 628

Index
637
cone
afﬁne, 405, 411, 412, 494, 498, 527, 536
projectivized tangent, 537-538
tangent, 520, 527-534
conﬁguration space, see space, conﬁguration
(of a robot)
congruence (mod I), 240
conic section, 6, 27, 142, 437, 439
consistency question, 11, 46, 179, 185
constructible set, 131, 226-228, 309
control
points, 21, 22, 28
polygon, 21, 28
coordinate ring of a variety, see ring,
coordinate (k[V])
coordinates
homogeneous, 385, 388, 396
Plücker, 445-447
coset, 382, 383
Cox, D., 127, 170, 254, 462, 564, 566, 581,
620-625, 628
Coxeter, H.S.M., 357, 628
Cramer's Rule, 165, 166, 171, 428, 597
criterion
rewriting, 585, 589
sygygy, 585
cross ratio, 331
cube, 25-26, 357, 362-363, 372, 382, 623
cubic
Bézier, 21, 27
cubic, twisted, see curve, twisted cubic
curve
cissoid of Diocles, 25-26
dual, 383, 464, 621
family of, 147
folium of Descartes, 142
four-leaved rose, 12, 154
rational normal, 420-421, 625
strophoid, 24
twisted cubic, 8, 19-20, 31, 33, 36, 69,
88, 181, 207, 264, 266, 267, 400,
402-404, 406, 415-418, 420, 490,
505-507
cuspidal edge, 267
Cuypers, H., 620, 628
Davenport, J.H., 196, 628
Decker, W., 615, 626, 628
decomposition
minimal primary, of an ideal, 229-230
minimal, of a variety, 215-216, 454, 473
minimal, of an ideal, 216
primary, 229-231, 606, 610, 612, 614,
616
degenerate case of a geometric conﬁgura-
tion, 324, 326, 327, 329, 334, 340,
466
degeneration, 467
degree
of a pair, 540, 569
of a projective variety, 505-506
total, of a monomial, 2
total, of a polynomial, 2
transcendence of a ﬁeld extension, 513
transcendence, of a ﬁeld extension, 514
weighted, of a monomial, 433, 436, 566
dehomogenization
of a polynomial, 399, 528, 543, 545, 590
derivative, formal, 47, 173, 247, 516, 523
Derksen, H., 369, 370, 623, 629
descending chain condition (DCC), 82, 212,
226, 229, 409
desingularization, 538
determinant, 118, 163, 279, 312, 428, 445,
525, 596-597
trick, 280
Vandermonde, 46
Dickenstein, A., 127, 140, 625, 626, 629
Dickson's Lemma, 72, 73, 75, 77
difference of varieties, 199, 425
dimension, ix, 3-11, 101, 117, 145, 249-
252, 254-258, 260, 275, 287, 311,
318, 469-473, 477-514, 518-523,
533
at a point, 520, 533
question, 11, 486-525
Dimension Theorem, see Theorem,
Dimension
discriminant, 173, 353
division algorithm, see algorithm, division
in k[x] or k[x1, . . . , xn]
dodecahedron, 363
dominating map, see mapping, dominating
Draisma, J., 622, 623, 629
Drton, M., 625, 629
dual
curve, see curve, dual
projective plane, 395, 436
projective space, 405, 445
variety, 383
duality
of polyhedra, 363
projective principle of, 387
Dubé, T., 116, 629
Dummit, D., 171, 173, 285, 352, 513,
595-597, 629

638
Index
echelon matrix, 51-52, 79, 95-96, 447, 450,
547-549, 568-570, 572, 573
Eder, C., 577, 585, 587, 589, 629
Eisenbud, D., 60, 184, 218, 624, 626, 629
elimination ideal, see ideal, elimination
elimination order, see monomial ordering,
elimination
elimination step, 122
Elimination Theorem, see Theorem,
Elimination
elimination theory, 17
projective, 422
Emiris, I., 127, 140, 625, 626, 629
Enneper surface, see surface, Enneper
envelope, 147-152, 154-155, 621
equivalence
birational, 273-276, 302, 512
projective, 437-444, 449, 451
error control coding theory, 625
Euclidean distance degree, 622
Euler line, 332, 333, 343
Euler's formula, 405
evolute, 622
extension step, 122
Extension Theorem, 108, see Theorem,
Extension
F4 algorithm, see algorithm, F4
F5 algorithm, see algorithm, F5
factorization of polynomials, 47, 83, 186,
188, 193, 195, 236, 239, 276, 453,
456, 458, 461, 463, 594, 595
family of curves, see curve, family of
Farr, J., 625, 629
Faugère, J.-C., ix, 564, 567, 574-577, 585,
587, 589, 621, 629
Feiner, S., 623, 629
Fermat's Last Theorem, see Theorem,
Fermat's Last
FGLM algorithm, see algorithm, FGLM
ﬁber of a mapping, 238, 281-283, 286-289,
511
ﬁeld, 1, 593
algebraically closed, 5, 34, 132, 159-160,
164, 168-170, 176-184, 199, 202-
204, 207, 210-211, 405, 410-412,
414, 419, 426-427, 431-434, 442-
444, 450, 460, 491, 493, 498-503,
509-511, 524, 528
ﬁnite, 1, 5, 36
inﬁnite, 3-4, 36, 134, 138, 208, 284, 287,
408, 409, 507, 533
of characteristic zero, 188, 355, 364, 365,
368, 373, 377, 615
of ﬁnite (positive) characteristic, 188
of fractions, 268, 511, 614
of rational functions, 1, 15, 306, 336, 456,
612, 616
of rational functions on V (k(V)),
268-275, 413, 511-513
ﬁnal remainder (in Wu's Method), 340
ﬁnite generation of invariants, 361, 368
ﬁnite morphism, 287, 289
ﬁniteness question, 11, 251-255, 606
Finiteness Theorem, see Theorem,
Finiteness
Fischer, G., 464, 621, 629
Foley, J., 623, 629
folium of Descartes, see curve, folium of
Descartes
follows generically from, 328
follows strictly from, 325
Foote, R., 171, 173, 285, 352, 513, 595-597,
629
forward kinematic problem, see kinematic
problem of robotics, forward
Fulton, W., 462, 464, 629
function
algebraic, 129
coordinate, 258
deﬁned by a polynomial, 3-4, 234-238,
257, 262-264, 507
identity, xvi, 260, 270-273
polynomial, 479, 487
rational, 15, 136, 167, 268-275, 462
function ﬁeld, see ﬁeld, of rational functions
on V (k(V))
Fundamental Theorem of Algebra, see
Theorem, Fundamental of Algebra
Fundamental Theorem of Symmetric Poly-
nomials, see Theorem, Fundamental,
of Symmetric Polynomials
Gallet, M., 174, 622, 629
Gao, S., 625, 629
von zur Gathen, J., 40, 43, 46, 117, 127,
595, 629
Gauss, C.F., 348, 629
Gaussian elimination, see algorithm,
Gaussian elimination (row reduction)
Gebauer, R., 116, 629
Gelfand, I., 170, 630
genomics, 625
Geometric Extension Theorem, see
Theorem, Geometric Extension
Gerhard, J., 40, 43, 46, 117, 127, 595, 629
gfan, see computer algebra systems, gfan
Gianni, P., 184, 218, 564, 621, 629, 630

Index
639
Giblin, P.J., 143, 148, 152, 622, 628
Giovini, A., 116, 630
Giusti, M., 117, 630
GL(n, k), see group, general linear
Glebsky, L., ix, 177, 630
Goldman, R., 27, 140, 622, 627, 630
graded lexicographic order, see monomial
ordering
graded reverse lexicographic order, see
monomial ordering
gradient, 10, 145-146, 149
graph
coloring, 626
of a function, 6, 134, 260, 264, 401, 434,
444, 523
Grassmannian, 447
Grayson, D., 613, 630
greatest common divisor (gcd), 41-45,
186-187, 196
Greuel, G.-M., 76, 184, 615, 628, 630
Grifﬁths, P., 464, 630
Gritzmann, P., 117, 630
Gröbner basis, 45, 78-128, 135-136,
139-141, 149-160, 194-195, 205,
237-238, 244, 248-255, 269, 280,
305-310, 325-331, 342, 349-351,
369-370, 375, 376, 407, 410-411,
416-419, 430-433, 490, 491,
527-529, 539-591, 604-617
and linear algebra, 546-548, 567
below signature M, 584
comprehensive, 309, 624
conversion, 551, 563, 590, 621, 624
criteria for, 86-88, 111-113, 568, 584
dehomogenized, 544
homogeneous, 540-543, 590
minimal, 92
module, 624
reduced, 93-94, 116, 179, 185, 221-225,
407, 544, 551
signature, 584
specialization of, ix, 160, 220, 306-310,
315-316, 624
universal, 623
Gröbner cover, ix, 309-310, 331
Gröbner fan, 623
Gröbner, W., 79
group, 448, 595
alternating, 363
cyclic, 356
ﬁnite matrix, 356
general linear (GL(n, k)), 356
generators for, 359-360
Klein four-, 360
of rotations of cube, 362
of rotations of tetrahedron, 363
orbit of a point, 378
orbit space, 378
projective linear group (PGL(n + 1, k)),
448
symmetric, 352, 356-357
Grove, L.C., 357, 623, 628
Harris, J., 447, 630
Hartshorne, R., 211, 287, 289, 630
Heintz, J., 117, 630
Hermann, G., 184, 218, 630
Heyden, A., 623, 630
Hilbert Basis Theorem, see Theorem,
Hilbert Basis
Hilbert driven Buchberger algorithm, see
algorithm, Hilbert driven Buchberger
Hilbert function, 486-498, 552
afﬁne, 487-491, 496
Hilbert polynomial, see polynomial, Hilbert,
610, 612, 614, 616
afﬁne, see polynomial, afﬁne Hilbert
Hilbert, D., 370, 473, 630
Hilbert-Poincaré series, 552-564, 610, 612,
614, 616
Hilmar, J., 462, 630
Hironaka, H., 79, 630
Hodge, W.V.D., 447, 630
homogeneous coordinates, see coordinates,
homogeneous
homogeneous ideal, see ideal, homogeneous
homogeneous polynomial, see polynomial,
homogeneous
homogenization
of a polynomial, 181, 400, 495, 543, 562,
590
of an ideal, 415-419, 494
(x0, . . . , xn)- of an ideal, 431-432
homomorphism, see ring, homomorphism
Horobe¸t, E., 622, 623, 629
Hughes, J., 623, 629
Huneke, C., 184, 218, 629
hyperboloid, see surface, hyperboloid of one
sheet
hyperplane, 399, 405, 438
at inﬁnity, 397, 418
hypersurface, 399
cubic, 399
dimension of, 498
nonsingular quadric, 442-444, 446
quadric, 436-451
quartic, 399

640
Index
quintic, 399
tangent cone of, 527
icosahedron, 363
ideal, 29
basis for, 31, 35
binomial, 257
colon, see ideal, quotient
complete intersection, 505
determinantal, 118, 421, 549
elimination, 122-127, 425-434, 610, 612,
616
generated by a set of polynomials, 29
Gröbner basis for, see Gröbner basis
homogeneous, 407-408, 540-564
homogenization, 415-419, 494
homogenization of, 543
in a ring, 244, 594
intersection, 192, 413, 606, 612, 614, 616
irreducible, 229
maximal, 209
Maximum Principle, see Maximum
Principle for Ideals
monomial, 70-74, 469-483, 487-493,
552-557
of a variety (I(V)), 32-35
of leading terms (⟨LT(I)⟩), 76
of relations, 373
P-primary, 229
primary, 228-231
prime, 207-211, 216-218, 228-229, 373,
414
principal, 41, 42, 82, 176, 245, 432
product, 191, 413, 614
projective elimination, 425-434
proper, 209-211
quotient, 200, 425, 606, 612, 614
radical, 36, 182-184, 196, 216, 244, 253,
259
radical of (
√
I), 182, 409, 614
saturation, 202-205, 411, 425-429, 435,
606, 612, 614, 616
sum of, 189, 413
syzygy, 373
weighted homogeneous, 433, 436, 566
ideal description question, 35, 49, 73, 77
ideal membership question, 35, 45, 49, 84,
97-98, 151, 184, 620
ideal-variety correspondence, 232
afﬁne, 183, 408
on V, 259
projective, 408
Implicit Function Theorem, see Theorem,
Implicit Function
implicit representation, 16
implicitization, 17, 133-140, 551,
562-564
via resultants, 170, 622
improved Buchberger algorithm, see
algorithm, improved Buchberger
Inclusion-Exclusion Principle, 480, 484
index of regularity, 489
inﬁnite descent, 276
inﬂection point, see point, of inﬂection
integer polynomial, see polynomial, integer
integral domain, see ring, integral domain
integral element over a subring, 280, 381
invariance under a group, 360
invariant polynomial, see polynomial,
invariant
inverse kinematic problem, see kinematic
problem of robotics, inverse
irreducibility question, 218
irreducible
components of a variety, see variety,
irreducible components of
ideal, see ideal, irreducible
polynomial, see polynomial, irreducible
variety, see variety, irreducible
irredundant
intersection of ideals, 216
primary decomposition, 229
union of varieties, 215
isomorphic
rings, 243
varieties, 238, 260-265, 509
Isomorphism Theorem, see Theorem,
Isomorphism
isotropy subgroup, 382
Jacobian matrix, see matrix, Jacobian
Jensen, A., 623
Jin, M., 622, 631
joint space, see space, joint (of a robot)
joints (of robots)
ball, 296
helical ("screw"), 296
prismatic, 292
revolute, 292
"spin", 304
Jounanolou, J., 170, 631
Jukes-Cantor models, 625
Kalkbrener, M., 139, 564, 624, 628, 631
Kapranov, M., 170, 630
Kehrein, A., 625, 626
Kemper, G., 369, 370, 623, 629
Kendig, K., 504, 522, 523, 631

Index
641
kinematic problem of robotics
forward, 294, 297-302
inverse, 294, 304-314
kinematic redundancy, 318
kinematic singularity, 311-314
Kirwan, F., x, 462, 464, 631
Klein four-group, see group, Klein four-
Klein, F., 357, 631
Knörrer, H., 464, 467, 621, 628
Koh, J., 620, 631
Koszul syzygy, see syzygy, Koszul
Kreuzer, M., 60, 76, 543, 624-626, 631
Krick, T., 184, 631
Lagorio, G., 611, 627
Lagrange interpolation polynomial, see
polynomial, Lagrange interpolation
Lagrange multipliers, 9-10, 99
Lasker-Noether Theorem, see Theorem,
Lasker-Noether
Lazard, D., 117, 127, 546, 564, 621, 622,
627, 629, 631
leading coefﬁcient, 60
leading monomial, 60
leading term, 38, 60
in a vector (signature), 578, 581
leading terms, ideal of, see ideal, of leading
terms (⟨LT(I)⟩)
least common multiple (LCM), 84, 196
Lejeune-Jalabert, M., x, 631
lexicographic order, see monomial ordering
Li, X., 622, 631
line
afﬁne, 3, 389
at inﬁnity, 389, 453
limit of, 530
projective, 389, 399, 402, 436, 444, 445
secant, 529-533
tangent, 143, 145-146
Little, J., 127, 170, 254, 462, 564, 566, 581,
620-625, 628
local property, 462, 515
locally constant, 462
Logar, A., 184, 631
Loustaunau, P., 218, 624, 626, 627
Lucas, S., 626, 627
Macaulay, F., 170, 631
Macaulay2, see computer algebra systems,
Macaulay2
MacDonald, I.G., 230, 627
Magma, see computer algebra systems,
Magma
Mall, D., 564, 624, 628
manifold, 523
Manocha, D., 127, 140, 622, 628, 631
Maple, see computer algebra systems,
Maple
mapping
dominating, 514
identity, 260, 270-273
polynomial, 234
projection, 129, 234, 282, 286
pullback, 262, 274
rational, 269-274, 302
regular, 234
Segre, 421, 443, 444, 449
stereographic projection, 23, 275
Marinari, M., 621, 631
Mathematica, see computer algebra systems,
Mathematica
matrix
echelon, see echelon matrix
group, 355-357
Jacobian, 310-312, 522
permutation, 356
row reduced echelon, see echelon matrix
Sylvester, 162-163
Matsumura, H., 533, 631
Maximum Principle for Ideals, 223, 226
Mayr, E., 117, 632
Meyer, A., 117, 632
Mines, R., 184, 218, 595, 632
minimal basis, see basis, minimal
Mishra, B., 342, 632
mixed order, see monomial ordering
module, 566, 580, 589, 591, 624
Möller, H., 116, 621, 625, 629, 631,
632
Molien's Theorem, see Theorem, Molien's
monic polynomial, see polynomial, monic
monomial, 2
monomial ideal, see ideal, monomial
monomial ordering, 54-60, 73-74, 407,
604, 607, 609, 611, 613, 615
elimination, 76, 123, 128, 281, 562
graded, 416, 488, 489, 491, 541, 543,
551, 574
graded lexicographic (grlex), 58
graded reverse lexicographic (grevlex),
58, 117
inverse lexicographic (invlex), 61
lexicographic (lex), 56-58, 98-101,
122-123, 135, 139, 156-160, 177,
194, 205, 308, 347-348, 551, 574
mixed, 75
product, 75, 308, 309
weight, 75-76

642
Index
Montes, A., ix, 309, 331, 624, 632
Mora, F., 116, 632
Mora, T., 116, 564, 621, 629-631
Moreno Maza, M., 622, 627
Mourrain, B., 625
multidegree (multideg), 60
multinomial coefﬁcient, 371
multiplicity
of a root, 47, 144
of a singular point, 153
of intersection, 144-146, 458-462
Mumford, D., 117, 522, 536, 627, 632
Newton identities, 352, 354-355
Newton polygon, 621
Newton-Gregory interpolating polynomial,
see polynomial, Newton-Gregory
interpolating
Newton-Raphson root ﬁnding, 127
Niesi, G., 116, 630
nilpotent, 244, 278
Noether normalization, 284-287, 510
Noether Normalization Theorem, see
Theorem, Noether Normalization
Noether's Theorem, see Theorem, Noether's
Noether, E., 366
nonsingular
point, see point, nonsingular
quadric, see quadric hypersurface,
nonsingular
normal form, 83
Normal Form for Quadrics, see Theorem,
Normal Form, for Quadrics
normal pair selection strategy, 116
Nullstellensatz, 34, 35, 108, 131, 184, 203,
221, 252, 254, 259, 329, 419, 519, 529
Hilbert's, 5, 179, 181, 183, 184, 200, 333
in k[V], 259
Projective Strong, 411-412, 494
Projective Weak, 410-411, 427, 454, 502
Strong, 183, 325, 410, 412, 491
Weak, ix, 176-180, 210, 211, 252, 260,
410
numerical solutions, 127, 255
O'Shea, D., 127, 170, 254, 462, 564, 566,
581, 620-625, 628
octahedron, 363
operational space (of a robot), see space,
conﬁguration (of a robot)
orbit
G-, 378, 381
of a point, 378
space, 378, 381
order, see monomial ordering
order (of a group), 356
orthocenter, 331
Ottaviani, G., 622, 623, 629
Pachter, L., 625, 632
Pappus's Theorem, see Theorem, Pappus's
parametric representation, 14-17, 233
polynomial, 16, 134, 563
rational, 15, 138, 266
parametrization, 15
partial solution, 123, 125, 129
partition, 240, 309
path connected, 462, 465
Paul, R., 314, 632
Pedoe, D., 447, 630
pencil
of curves, 406
of hypersurfaces, 406
of lines, 395
of surfaces, 260
of varieties, 260
permutation, 163, 346
sign of, 163, 596, 597
Perry, J., 117, 628
perspective, 386, 390, 392, 395
Pﬁster, G., 76, 184, 615, 626, 628, 630
PGL(n + 1, k), see group, projective linear
group (PGL(n + 1, k))
Pistone, G., 625, 632
plane
afﬁne, 3
Euclidean, 319
projective, 385-392, 451
Plücker coordinates, see coordinates,
Plücker
point
critical, 102, 622
Fermat, of a triangle, 334
nonsingular, 146, 152, 462, 519, 520,
522-523, 525
of inﬂection, 153
singular, 8, 143-146, 151-153, 155, 402,
449, 519-522, 525-526, 533
vanishing, 386
polyhedron
duality, 363
regular, 357, 363
polynomial
afﬁne Hilbert, 489-491, 494-496, 505
bihomogeneous, 433-434, 436
elementary symmetric, 346-348
Hilbert, 493-495, 606
homogeneous, 351

Index
643
homogeneous component of, 351
integer, 163
invariant, 358
irreducible, 185, 594-595
Lagrange interpolation, 97
linear part, 516
monic, 583
Newton-Gregory interpolating, 485
partially homogeneous, 424
reduced, 47
S-, 85-88, 90-92, 104, 109-116, 194,
220, 257, 315, 413, 539-543,
546-548, 558-564, 567, 569, 573,
576-580, 606, 610
square-free, 47
symmetric, 346
weighted homogeneous, 433, 436, 566
polynomial map, see mapping, polynomial
PostScript, 22
power sums, 351-352, 366-368
primality question, 218
primary decomposition, see decomposition,
primary
primary decomposition question, 231
primary ideal, see ideal, primary
prime ideal, see ideal, prime
principal ideal domain (PID), 41, 176, 245
product order, see monomial ordering
projection mapping, see mapping, projection
projective closure, see closure, projective
projective elimination ideal, see ideal,
projective elimination
projective equivalence, see equivalence,
projective
projective line, see line, projective
projective plane, see plane, projective
projective space, see space, projective
projective variety, see variety, projective
projectivized tangent cone, see cone,
projectivized tangent
pseudocode, 38, 599-602
pseudodivision, see algorithm, pseudodivi-
sion
successive, 337
pseudoquotient, 336
pseudoremainder, 336
Puiseux expansions, 621
pullback mapping, see mapping, pullback
pyramid of rays, 390
Python (programming language), 609
quadric hypersurface, 399, 436-451
nonsingular, 442-444, 446
over R, 442, 448
rank of, 440
singular, 448
quotient
ﬁeld, see ﬁeld, of fractions
ideal, see ideal, quotient
vector space, 486
quotients on division, 62-66, 83, 413, 605,
607, 610
radical
generators of, 184
ideal, see ideal, radical
membership, see algorithm, radical
membership
of an ideal, see ideal, radical of
Rakhooy, H., 174, 622, 629
rank
deﬁcient, 311
maximal, 311
of a matrix, 9, 311, 441
of a quadric, 440, 441
rational
function, see function, rational
mapping, see mapping, rational
univariate representation, 621
variety, see variety, rational
real projective plane, 385, 388-390
Recio, T., 331, 624, 632
recursive, 225, 556
REDUCE, see computer algebra systems,
REDUCE
reduced Gröbner basis, see Gröbner basis,
reduced
regular sequence, 504, 589
regularity, index of, see index of regularity
Relative Finiteness Theorem, see Theorem,
Relative Finiteness
remainder on division, 38, 62-68, 70, 83-84,
86, 91-92, 94, 98, 104, 107, 109, 172,
248-251, 269, 349-350, 369-370,
376, 413, 541, 546, 558-564, 567,
569, 577-580, 605, 607, 610, 612,
613, 616
representation
lcm, 107, 221
standard, 104, 549, 569, 573, 576
resultant, 161-168, 451, 454-460, 462, 622
multipolynomial, 140, 170, 622
Reynolds operator, 365-366
Riccomagno, E., 625, 632
Richman F., 184, 218, 595, 632
Riemann sphere, 397, 403
ring
commutative, 3, 236, 242, 359, 593

644
Index
coordinate (k[V]), 257-269, 277-278,
286, 287, 359, 377-378, 381,
507-509, 594
ﬁnite over a subring, 279, 381
ﬁnitely generated over a subring, 288
homomorphism, 243, 374, 508, 544
integral domain, 236-237, 258, 268, 511,
594
isomorphism, 243, 244, 247, 258,
264-265, 270, 273-274, 285-287,
374, 376-378, 507, 509, 512
of invariants (k[x1, . . . , xn]G), 358, 359
polynomial ring (k[x1, . . . , xn]), 3
quotient (k[x1, . . . , xn]/I), 240-247, 374,
383, 505, 594, 621, 625
Robbiano, L., 60, 76, 116, 543, 611,
624-626, 630-632
robotics, 10-11, 291-314
Roth, L., 447, 632
Rouillier, F., 621, 632
row reduced echelon matrix, see echelon
matrix
Ruitenburg, W., 184, 218, 595, 632
Sage, see computer algebra systems, Sage
saturation, see ideal, saturation
Schönemann, H., 615, 628
Schauenburg, P., ix, 157, 224, 227, 632
secant line, see line, secant
Sederberg, T., 140, 622, 627
Segre map, see mapping, Segre
Segre variety, see variety, Segre
Seidenberg, A., 184, 218, 632
Semple, J.G., 447, 632
Shafarevich, I.R., 504, 521, 522, 632
sign of a permutation, see permutation, sign
of
signature Gröbner basis, see Gröbner basis,
signature
signature of a vector, 580, 581
Singular, see computer algebra systems,
Singular
singular locus, 521, 524, 525
singular point, see point, singular
singular quadric, see quadric hypersurface,
singular
Siret, Y., 196, 628
Smith, L., 370, 632
Smyth, C., 462, 630
solving polynomial equations, 49, 98, 255,
620
S-polynomial, see polynomial, S-
space
afﬁne, 3
conﬁguration (of a robot), 294
joint (of a robot), 294
orbit, 378, 381
projective, 388-392, 396
quotient vector, 486
tangent, 516-521
specialization of Gröbner basis, see Gröbner
basis, specialization of
s-reduction (signature reduction), 581-582
s-reduced vector, 583
stabilizer, 382
standard basis, see basis, standard
Stein, W., 608, 632
stereographic projection, see mapping,
stereographic projection
Sterk, H., 620, 628
Stetter, H., 625
Stillman, M., 76, 117, 128, 540, 613, 620,
627, 630
Sturmfels, B., x, 117, 330, 369, 370, 372,
622, 623, 625, 627, 629, 630, 632
subalgebra (subring), 287, 349, 359, 369
subdeterminants, 421, 549
subgroup, 596
subvariety, 258
Sudoku, 626
sugar, 116
Sullivant, S., 625, 629
surface
Enneper, 141
hyperboloid of one sheet, 270
quadric, 237, 421, 443, 448
ruled, 103, 444
tangent, 19-20, 101, 133, 135-136,
233-234, 535
Veronese, 239, 421, 433
Whitney umbrella, 141
S-vector, 583
Swanson, I., 620, 633
Sylvester matrix, see matrix, Sylvester
symmetric group, see group, symmetric
symmetric polynomial, see polynomial,
symmetric
syzygy, 580
Koszul, 581, 586
on leading terms, 110
Taalman, L., 626, 627
tangent cone, see cone, tangent
tangent line to a curve, see line, tangent
tangent space to a variety, see space, tangent
Taylor series, 552
Taylor's formula, 517, 534
term in a vector of polynomials, 578

Index
645
tetrahedron, 363
Theorem
Afﬁne Dimension, 491
Bezout's, x, 451-467
Circle, of Apollonius, 322, 330, 339-342
Closure, 131-132, 140, 142, 199-200,
219-228, 282, 289, 434, 509, 510
Dimension, 493
Elimination, 122-124, 126, 128,
135-136, 150, 152, 194, 423, 430
Extension, 125-127, 132, 136-170, 379,
422
Fermat's Last, 13
Finiteness, 251, 278, 281, 283
Fundamental, of Algebra, 4, 178
Fundamental, of Symmetric Polynomials,
347-351
Geometric Extension, 130-131, 422-423,
432
Geometric Noether Normalization,
286-287
Geometric Relative Finiteness, 282-283
Hilbert Basis, 31, 77, 80-82, 175, 176,
216, 218, 245, 368, 371, 407, 431
Implicit Function, 318, 523
Intermediate Value, 462, 465
Isomorphism, 247, 374, 524
Lasker-Noether, 229-230
Molien's, 368, 622
Noether Normalization, 284-285, 287
Noether's, 366, 371
Normal Form, for Quadrics, 439-444
Pappus's, 332, 333, 343, 393, 466
Pascal's Mystic Hexagon, 463, 465, 466
Polynomial Implicitization, 134
Projective Extension, 426-429, 597
Rational Implicitization, 138
Relative Finiteness, 280-281, 283, 511
Thomas, R., 622, 623, 627, 629
Tournier, E., 196, 628
Trager, B., 184, 218, 630
transcendence degree, see degree,
transcendence of a ﬁeld extension
transformation
afﬁne, 303
projective linear, 437
quadratic, 621
transposition, 596
Traverso, C., 116, 550, 630, 633
triangular form (system of equations),
337-339
triangulation problem (computer vision),
623
twisted cubic
curve, see curve, twisted cubic
tangent surface of, see surface, tangent
Ullrich, P., 227, 633
unique factorization of polynomials, 595
uniqueness question in invariant theory, 361,
373, 376
unirational variety, see variety, unirational
van Dam, A., 623, 629
van der Waerden, B., 170, 633
Vandermonde determinant, see determinant,
Vandermonde
variety
afﬁne, 5
irreducible, 206-218, 237, 239, 258, 267,
268, 326, 327, 337, 377, 409, 413,
418, 453, 503
irreducible components of, 215-227,
326-328, 409
linear, 9, 399
minimum principle, 228
of an ideal (V(I)), 81, 408
projective, 398
rational, 273
reducible, 236
Segre, 421-422
subvariety of, 258
unirational, 17
zero-dimensional, 252
Vasconcelos, W., 184, 218, 629
Veronese surface, see surface, Veronese
Walker, R., 458, 462, 633
Wang, D., 342, 622, 631, 633
weight order, see monomial ordering
weighted homogeneous ideal, see ideal,
weighted homogeneous
weighted homogeneous polynomial, see
polynomial, weighted homogeneous
Weispfenning, V., 60, 83, 84, 116, 194, 218,
309, 543, 624, 628, 633
well-ordering, 55-56, 66, 73, 347, 575, 590
Whitney umbrella, see surface, Whitney
umbrella
Wibmer, M., ix, 309, 624, 632
Wiles, A., 13
Winkler, F., 117, 633
Wu's Method, 335-343, 622
Wu, W.-T., 343, 622, 633
Wynn, H., 625, 632

646
Index
Zacharias, G., 184, 218, 630
Zafeirakopoulos, Z., 174, 622, 629
Zariski
closure, see closure, Zariski
dense, 510
dense set, 200, 211, 216, 221, 510
Zelevinsky, A., 170, 630
zero divisor in a ring, 500, 504, 594



Editors
John Billingsley and Peter Brett
Mechatronics and Machine Vision in
Practice 4
1st ed. 2021

Editors
John Billingsley
School of Mechanical and Electrical Engineering, University of Southern
Queensland, Toowoomba, QLD, Australia
Peter Brett
Agricultural Technologies and Robotics, University of Southern
Queensland, Toowoomba, QLD, Australia
ISBN 978-3-030-43702-2
e-ISBN 978-3-030-43703-9
https://doi.org/10.1007/978-3-030-43703-9
© Springer Nature Switzerland AG 2021
This work is subject to copyright. All rights are reserved by the Publisher,
whether the whole or part of the material is concerned, specifically the
rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microfilms or in any other physical way, and
transmission or information storage and retrieval, electronic adaptation,
computer software, or by similar or dissimilar methodology now known or
hereafter developed.
The use of general descriptive names, registered names, trademarks, service
marks, etc. in this publication does not imply, even in the absence of a
specific statement, that such names are exempt from the relevant protective
laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice
and information in this book are believed to be true and accurate at the date
of publication. Neither the publisher nor the authors or the editors give a
warranty, expressed or implied, with respect to the material contained
herein or for any errors or omissions that may have been made. The

publisher remains neutral with regard to jurisdictional claims in published
maps and institutional affiliations.
This Springer imprint is published by the registered company Springer
Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham,
Switzerland

Introduction
These are selected and revised papers from the 26th Annual Conference on
Mechatronics and Machine Vision in Practice, held in Toowoomba
December 2-5, 2019.
In the process of selecting the papers, many were rejected as lacking the
essential 'in practice' element. The survivors have been grouped into those
that deal predominantly with vision and optical sensing, those that relate to
agricultural applications, more general robotics and devices, sensing
methods and actuation, and finally industrial processes and products.
The conference was held with the hospitality of USQ's McGregor
College and its Centre for Agricultural Engineering. The local team
presented several keynotes not included here, which are likely to be
published elsewhere later.
There are strong contributions from a number of Chinese and New
Zealand Universities, together with contributions from the Philippines,
Emirates, and Germany.
For more details on the topics, there are summaries in each of the part
headings, covering a vast array of themes ranging from deep learning
applied to vision analysis to a robotic mechanism used for collecting camel
dung.

Contents
Vision and Optical Sensing
The Design of Optical Sensing Needles for Tactile Sensing and Tissue
Identification at Needle Tip
Zonglai Mo, Jun Li, Weiliang Xu and Neil G. R. Broderick
Object Detection on Train Bogies Using Structured Light Scanning
Tangwen Yang, Yantao Sun, Xiaoqing Cheng, Honghui Dong and
Yong Qin
A Method for Detecting Breaking Rate of Ganoderma Lucidum Spore
Powder Based on Machine Vision
Shanling Ji, Zhisheng Zhang, Zhijie Xia and Ying Zhu
6D Pose Estimation of Texture-Less Object in RGB-D Images
Heng Zhao, Chungang Zhuang, Lei Jia and Han Ding
Agriculture Applications
Improving Vision-Based Detection of Fruits in a Camouflaged
Environment with Deep Neural Networks
Jinky G. Marcelo, Joel P. Ilao and Macario O. Cordel II
Mechatronics for a LiDAR-Based Mobile Robotic Platform for Pasture
Biomass Measurement
M. Sharifi, S. Sevier, H. Zhang, R. Wood, B. Jessep, S. Gebbie, K. Irie,
M. Hagedorn, B. Barret and K. Ghamkhar
Vision Guidance with a Smart-Phone
John Billingsley
A High-Speed Camel Dung Collection Machine
Samuel N. Cubero, Mohammad Badi, Mohamed Al Ali and
Mohammed Alshehhi
Discussion of Soft Tissue Manipulation for the Harvesting of Ovine
Offal
Qi Zhang, Weiliang Xu, Zhisheng Zhang, Martin Stommel and
Alexander Verl
Robotics and Devices

Fabrication and Characterization​ of 3D Printed Microfluidics
Swapna A. Jaywant, Muhammad Asif Ali Rehmani, Tanmay Nayak and
Khalid Mehmood
A Modified Bresenham Algorithm for Control System of FDM Three-
Dimensional Printer
Ke Yu, Zhisheng Zhang, Zhiting Zhou and Min Dai
Design and Experimental Study on the Self-Balancing Foot Device
Rui Peng and Liang Han
Dynamic Characteristics Analysis and Optimization Design of Cross-
Beam Assembly in 3D Printer
Weijie Chu, Limiao Gu, Xiaolong Liu and Fang Jia
Design and Experimental Research of Automatic Tightening Method of
Rubber Strip on the Side of Office Screen Panel
Ruonan Wang, Liang Han, Jinghui Peng and Rui Peng
A Scene Feature Based Eye-in-Hand Calibration Method for Industrial
Robot
Guoshu Xu and Yonghua Yan
Vision-Based Trajectory Planning for a Five Degree of Freedom
Assistive Feeding Robotic Arm Using Linear Segments with Parabolic
Blend and Cycloid Functions
Priyam A. Parikh, Keyur D. Joshi and Reena Trivedi
Structure Design and Closed-Loop Control of a Modular Soft-Rigid
Pneumatic Lower Limb Exoskeleton
Jiangbei Wang and Yanqiong Fei
Sensing Methods and Actuation
Real-Time, Dynamic Simulation of Deformable Linear Objects with
Friction on a 2D Surface
Benjamin Maier, Marius Stach and Miriam Mehl
A System for Capturing of Electro-Muscular Signals to Control a
Prosthesis
Zeming Zhao, Bo Lv, Xinjun Sheng and Xiangyang Zhu

Challenges in Robotic Soft Tissue Manipulation—Problem
Identification Based on an Interdisciplinar​y Case Study of a
Teleoperated Drawing Robot in Practice
M. Wnuk, F. Jaensch, D. A. Tomzik, Z. Chen, J. Terfurth,
S. Kandasamy, J. Shahabi, A. Garrett, M. H. Mahmoudinezhad,
A. Csiszar, W. L. Xu, O. Röhrle and A. Verl
Modeling of Lens Based on Dielectric Elastomers Coupling with
Hydrogel Electrodes
Hui Zhang and Zhisheng Zhang
Industrial Processes and Products
A General Monitoring Method for the State of Spandex Based on Fuzzy
Evaluation and Its Application
Limiao Gu, Yan Wen, Yu Zhang, Weijie Chu, Yunde Shi and Fang Jia
Study on the Type Identification of Cheese Yarn Based on Low-
Resolution Pictures
Xiaolong Liu, Ran Hu, Yan Wen, Yu Zhang, Weijie Chu,
Zhisheng Zhang and Fang Jia
Research on High Feeding Speed System of L-Valve Rods Based on
Two-in-One Device
Shiwei Cheng, Liang Han, Kai Yu and Rui Peng
Conclusion
OceanofPDF.com

Vision and Optical Sensing
The first chapter is more about sensing than vision, concerned with
detecting force at a surgical needle-tip to identify the tissue layer that it has
reached. Light is the chosen communication medium, being unaffected by
the intense magnetic fields of a MRI environment.
The next chapter concerns the build-up of detritus on a train bogy.
Structured light is used to measure its volume.
At a much smaller scale, vision is used to measure the breaking of
spores that are used in Chinese medicine. These spores must be broken to
act more efficiently.
The fourth chapter in this part concerns the detection of the pose of an
object that lacks texture features, for the purpose of grasping it. Although
deep learning features in the method, there is a strong experimental basis.
OceanofPDF.com

(1)
(2)
(3)
(4)
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_1
The Design of Optical Sensing Needles for
Tactile Sensing and Tissue Identification at
Needle Tip
Zonglai Mo1, 4   , Jun Li1, Weiliang Xu2, 4 and Neil G. R. Broderick3, 4
School of Mechanical Engineering, Nanjing University of Science and
Technology, Nanjing, China
Department of Mechanical Engineering, The University of Auckland,
Auckland, New Zealand
Department of Physics, The University of Auckland, Auckland, New
Zealand
The Dodd-Walls Centre for Photonic and Quantum Technologies,
Dunedin, New Zealand
 
Zonglai Mo
Email: mozonglai2000@163.com
Keywords Needle sensing - Tissue identification - Intensity modulation -
Force sensor
1 Introduction
Needle insertion is a common surgery in a variety of procedures such as
interventional radiology [1], neurosurgery [2], brachytherapy [3], regional
anaesthesia [4], biopsies [5], drug delivery [6] and blood sampling [7]. It
also represents one of the least invasive ways to access the internal organs
of patients [8]. An example is epidural anaesthesia, a regional anaesthesia
used for pain relief that can be performed at different locations along the
spine depending on the surgery [9]. In England alone, about 280,000

epidural anaesthesia are performed yearly within the National Health
Service [10]. In China, based on the 16 million newborns in 2017, about 8
million are potential users of epidural anesthesia, according to the c-section
rate of more than 50% [11].
In the process of spinal epidural puncture anesthesia, the operator will
inject anesthetics into the spinal epidural space between the spinal nerve
and the yellow ligament through an injection needle (the width range is
only about 2-7 mm, and the distance from the subcutaneous is about 20-
90 mm). This operation requires the operator to determine the position of
the needle tip immediately when the needle tip punctures the yellow
ligament into the epidural space and stop the puncture. If the puncture
continues, the spinal nerve will be easily punctured, resulting in short-term
or lifelong postoperative low back pain, and even paralysis in severe cases
[12]. However, in the actual situation, the operator's tactile perception of
the tip is almost lost, and the judgment of the tip position mainly depends
on the operator's experience. Medical statistics show that the number of
repuncture localization and various surgical side effects caused by the
failure of puncture operation positioning can reach 13-47% [13, 14].
Therefore, how to capture tactile perception to accurately identify the
position of needle tip has been an urgent problem in clinical practice.
Force sensing is one of our main means of interacting with the
environment. A force sensor can detect contact between itself and an object,
and measures static or dynamic force magnitudes [15]. Some experimental
studies [16, 17] have shown that, without force feedback, tissue trauma and
unexpected damage to healthy organs increase during surgery. When force
feedback is incorporated into teleoperated systems can reduce robotic force
by 30-60%, peak forces by a factor of 2-6, operating time by 30% and
error rates by 60% [18]. This shows the need for force sensors to be
integrated into surgical instruments.
However, the traditional electronic sensors are easily interfered by the
electromagnetic imaging equipment used for surgery, and the electronic
sensors are too large to be integrated into the needle, so the problem of
sensor integration in the needle and tip has not been solved. In recent years,
optical fiber sensor has been developed rapidly, which overcomes the
shortcomings of the above methods. It has the characteristics of
electromagnetic immunity and small size advantages, and it has unique
advantages in the integrated technique of spinal puncture surgical needle.

According to literature statistics, Fabry-Perot interference (FPI) and Fiber
Bragg Grating (FBG) sensing technology are the main potential means of
surgical needle integration.
In the past years, only several researchers focused on tissue
identification by needle-like probe. Liu et al. [19] designed a wheeled end-
sensing probe with intensity modulated fiber optic force sensor, which was
used to locate abnormal tissues in minimally invasive surgery. The
experiment proved that the probe could effectively detect the location of
tumor. Yip et al. [20] designed a three-axis end sensing probe (5.5 mm in
diameter) based on a similar sensing structure, which was used for heart
beat detection in vivo. The test showed that the detection error rate was less
than 3%. Beekmans et al. [21] designed the terminal tactile feedback probe
needle (5 mm in diameter) based on FPI optical fiber sensing technology
combined with cantilever beam structure, and identified tissue types and
boundaries by analyzing the detected soft tissue stiffness information.
Carotenuto et al. [22] and others take the lead in the grating (FBG) sensors
are integrated into the needle inside, used to detect needlepoint into epidural
gap signal, but not its sensing structure temperature compensation
measures, installation also affect the original function of epidural needles.
Kumar et al. [23] designed the end sensing needle based on FBG to
estimate the end force of the instrument in the process of software puncture,
and the experiment proved that the sensing needle could detect the puncture
signal of tissue boundary. In addition, Liu et al. [24] designed 3d end sensor
needle for eye tissue sampling puncture surgery based on FPI principle or
FBG principle to detect weak force feedback that cannot be perceived by
human body during puncture. Beisenova et al. [25] integrated FBG
distributed sensing on the outside of the needle body for the identification
of epidural gap, which can detect the pressure signals between various parts
of the needle and soft tissues, and estimate the types of soft tissues that have
been punctured. It was verified by experiments with soft tissue composites
of different kinds of animals.
In this study, novel optical sensing solutions based on our previous
designs [26-28] are introduced and compared for needle-tip tactile sensing
and bio-tissue identification, based on interference intensity modulated
Fabry-Perot interference (FPI) principle. The design of the sensor and its
optical circuit are detailed described. The system was verified under
different working conditions, by conducting simulations, phantom tests, ex-

vivo and in-vivo animal tests. Sensor properties and environmental
influence factors on the sensor were investigated. The limitation of the
designs and further improvements are also discussed in the paper.
2 Sensing Needle Designs
2.1 FPI Force Sensing Principle
A typical FPI sensor (Fig. 1) has two cleaved single mode fibers embedded
inside of a glass capillary by epoxy. There is a cavity in micrometer level
between two fiber ends. An incident light is delivered into the fibers and
four percent of the incident light is reflected by each cleaved fiber end.
Consequently, the two beams of the reflected light are interfered with each
other, resulting in an interfered light that can be detected by a photodiode
sensor or a spectrometer.
Fig. 1 A typical FPI sensor
When an axial loading is applied, the cavity length varies, so does the
intensity of the interfered light, which is described by,
(1)
where I is the intensity of the interfered light, I1 and I2 are the light
intensities of the two reflected incident light beams, and is the intensity
phase, which is the phase difference between the two reflected lights. The
intensity phase can be calculated by,
(2)
(3)

where is the optical path-length difference between two reflected light
beams, d and ∆d are the initial cavity length and the change in cavity
length, respectively and λ is the wavelength of the incident light.
2.2 Sensing Needle Designs
To compensate for the influence of temperature variations, two identical
FPI sensors were placed at the tips of surgical needles (an injection bevel-
tip needle and an epidural needle), as shown in Fig. 2, with one sensor
serving as a reference sensor subjected only to temperature variation and
the other serving as a force sensor subjected to both temperature and axial
loading at the tip of the needle.
Fig. 2 Placement of the two FPI sensors for temperature compensation a Bevel tip injection needle.
b Epidural needle
Figure 3a shows the schematic optical circuit of the temperature
compensated FPI sensing system. A 1 mw hand-held laser with wavelength
of 1550 nm is the source of the incident light. An optical isolator prevents
reflected light returning to the source. The laser is split into two channels by
a 50/50 splitter, which are transmitted to the two FPI sensors. Then the
interfered light of the two FPI sensors are measured by two photodiode
power sensors and the measurements are further processed by a computer.

Fig. 3 Temperature-compensated FPI sensing system. a Schematic optical circuit. b The embedded
sensor and overall system. a The sensing needle. b Handheld laser source. c Optical circuit box, (1)
An FPI sensor, (2) The end faces of the sensor
In this design, for the force sensor, the FPI cavity length change is
induced by both force change and temperature change, while the reference
sensor only has the temperature influence. Consequently, the temperature
influence of the force sensor can be compensated for.
(4)

(5)
where ∆df is the effective cavity length change induced by force loadings,
∆dR is the cavity length change of the reference sensor due to temperature
variation, and k is the coefficient accounting for the difference in the
temperature influence in the two sensors, which can be calibrated
experimentally.
2.3 Tissue Samples Preparation
Five kinds of phantom human tissues (SynDaverTM Labs, USA) were
prepared for insertion experiments, i.e. phantom human skin, fat, muscle,
liver, and multiple layer abdominal tissue (Fig. 4a-e). In addition, pig tissue
and mice (Fig. 4f-g) were also used for experimental verifications.
Fig. 4 Experimental tissues a Phantom skin. b Phantom fat. c Phantom muscle. d Phantom skin-fat-
muscle multiple layer tissue. e Phantom liver. f Pig tissue. g Mice
2.4 System Performance After Temperature Compensation
To evaluate the temperature dependence after the compensation, the needle
was left in the chamber and heated to six different temperatures ranging
from 23 to 45 °C with 6 °C intervals. In each temperature condition, the
chamber temperature fluctuated up and down within 2 °C for fifteen
minutes. The temperature compensation result (in terms of the intensity
phase) is satisfactory, as shown in Fig. 5.

Fig. 5 Temperature dependence within 23-45 °C [27]
To check the effectiveness of the temperature compensation involving
axial loading, pulse forces were applied periodically at the needle tip during
the temperature change of 23-37.5 °C, as shown in Fig. 6. The intensity
phase of the force sensor is attributed to both the temperature and force,
while the intensity phase of the reference sensor only to the temperature. A
mean intensity phase error of 0.03 rad was found after temperature
compensation.
Fig. 6 Effective intensity phase after temperature compensation [26]
The intensity phase of the force sensor after compensation and the
applied dynamic forces were recorded (Fig. 7), and then a linear
relationship between them was obtained. The results show that the

temperature compensated force sensor has a force measurement range of 0-
8 N with a resolution of 0.3 N at the temperature of 23-37.5 °C.
Fig. 7 FPI signal and applied force detected by dynamic force sensor [26]
3 Experimental Verifications and Needle
Applications
To characterize the needle's force sensing capability, ex-vivo experiments
were conducted using both phantom human tissues and bio tissues.
3.1 Temperature Change Versus the Time of Death
To investigate the influence of temperature, the mouse body temperature
was measured using a digital thermometer (DS18B20) with a precision of
±0.5 °C. Temperature change after death over time was recorded in Fig. 8.
It shows that the body temperature dropped gradually within 35 min after
death, from about 36 °C to near room temperature, yielding to a polynomial
relationship. It also indicates that a mouse within one minute after death
could be regarded the same as a live mouse in terms of body temperature
and living organs.

Fig. 8 Temperature change post mortem [28]
Figure 9 is the FPI interference light intensity signals of two skin-
tumor-skin insertions and one abdomen insertion on a mouse at a time of
two hours after death. The FPI reference sensor signal (the red curve in
Fig. 9) shows that there was still a temperature influence that needs to be
compensated for two hours after death. Although there might be only little
temperature difference between mouse's body and room temperature, the
FPI signal induced by the influence is still large compared with the
penetration force.
Fig. 9 Insertions two hours after death [28]
The above results demonstrate that, for the mouse tissue penetration, the
sensing needle needs temperature compensation at least within two hours
after death of the mouse.

3.2 The Difference Between in-Vivo Insertion and Ex-Vivo
Insertion
In-vivo needle insertion is further proof of the temperature compensation
effect of the sensing needle. The purpose of this section is to compare the
difference between these two conditions, through conducting skin-tumor-
skin insertion experiments. Four mice were sampled in this experiment. As
shown in Fig. 10, the mouse was first anaesthetized in a box (Fig. 10a), and
then moved to a tube with anaesthetic gas flow, putting the mouth and nose
inside the tube for continuous anaesthesia during insertion experiments
(Fig. 10b). The in-vivo insertion experiments then began, as shown in
Fig. 10c.
Fig. 10 In-vivo mouse experiment. a Initial anesthesia. b Anesthesia setup during experiments. c
Needle insertion on an anesthetized mouse
Figure 11 shows a typical FPI interference light intensity signal during
the insertion, and its processed tip force signal after temperature
compensation is shown in Fig. 12. The signal of one ex-vivo insertion on
the skin-tumor-skin with 10 s time of death was added into Fig. 11 for
comparison, shown as the dotted line. The result indicates that in-vivo
insertions match well with the ex-vivo insertions on mice one minute from
time of death. This experiment shows a database based on freshly-killed
mice can be regarded equivalent to live mice, which can further be used for
real-time in-vivo tissue identification during needle insertions.

Fig. 11 In-vivo insertion FPI light intensity signal [28]
Fig. 12 In-vivo tip force signal after temperature compensation [28]
3.3 Needle Advancing Rate
The needle insertion force (hybrid tip/friction force) was believed relating
to the needle advancing rate, both in phantom tissue [29] and biological
tissue [30]. But studies on the influence of needle advancing rates on the
needle tip force are scarce. Therefore, the relationship between tip force and
needle advancing rate must be explicitly measured for the purpose of
characterization. Three needle advancing rates were considered during
experiments, i.e., 3.8 mm/s, 7.8 mm/s, and 14.5 mm/s, which are in line
with that in routine surgeries. The experiment was carried out on both
phantom human skin tissue and swine belly skin tissue. Based on the 10 tip

force readings of each configuration, standard deviation was calculated, the
results of which is shown in Fig. 13. The result indicates that manual
insertion may achieve similar tip force at various constant advancing rates.
Fig. 13 Insertion tip force under different needle advancing rates [27]
3.4 Automated and Manual Insertion
To figure out the influence of insertion modes on needle tip force,
automated and manual insertions were conducted on phantom human
muscle, respectively, with an overall time of around 6 s. Needle advancing
rate was set to 3.8 mm/s for automated insertion, and the operator drove the
needle as continuous and stable as possible in manual insertions. Five
automated insertions and six manual insertions were performed on phantom
muscle. The tip force signals of both situations are compared in Fig. 14.

Fig. 14 Automated and manual insertion on phantom muscle. a Four stages of one typical insertion.
b Automated and manual insertion signals [27]
From the results, different insertion stages during the procedure can be
clearly identified in both situations, indicated by a, b, c, and d in Fig. 14.
However, some differences can be found when stages were changed. In the
automated insertion, boundary displacement (from a to b in Fig. 14) took a
time similar to that for the insertion out of the tissue (from c to d). While in
the manual insertion, it highly depends on the needle advancing rate by the
operator. This experiment shows that tissue identification in manual
insertions is also possible, despite losing thickness measurement of
interacted tissue.

3.5 Friction Force and Needle Displacement
Figure 15 shows the force comparison between a reference force sensor and
tip force sensing needle, from the insertion of a 10 mm thickness phantom
human muscle tissue. The insertion procedure had four stages, denoted by a,
b, c, and d, shown in Fig. 9. It shows that the sensing needle had similar
values with ATI force sensor in the stage of boundary displacement (from a
to b), until the layer was ruptured at around 6.7 N. However, after inserting
entire tissue depth where denoted by d, friction force detected by ATI force
sensor kept steady at around 12.5 N during continuous insertion, while the
tip force from the sensing needle dropped to zero.
Fig. 15 Tip force and hybrid tip/friction force during phantom muscle insertion [27]
3.6 Phantom Tissue and Bio Tissues
Needle insertions on different kinds of phantom human tissues and swine
tissues were conducted. Four insertion rates were applied to the sensing
needle, 3.8, 7.8, 11.3, and 14.5 mm/s. Each kind of tissue was inserted
individually for 10 times under each rate. In total, 40 insertion events were
executed in every kind of tissue sample.
The results in Figs. 16 and 17 show that insertion forces of the identical
tissue vary in a specific range at different needle advancing rates, but no
general tendency is found. The results also show that some internal organs
such as liver and kidney may have very similar insertion tip force at around
4-5 N. However, that has no influence on tissue identification as they lie at
different body areas.

Fig. 16 The tip force during insertions of different phantom tissues [27]
Fig. 17 The tip force during insertions of different swine tissues [27]
To obtain the tip force database used for tissue identification in mouse,
three mice were killed and dissected for collecting tip force data of various
organs. Small internal organs, such as heart and lung, were inserted
individually three times in each mouse, except skin, muscle, and tumor,
with five insertions. In total, 9 insertions were conducted on each kind of
internal organ and 15 insertions for skin, muscle, and tumor. Figure 18
shows the tip force database for different tissues and internal organs.

Fig. 18 Tip force per various murine mouse tissue during insertion [28]
3.7 Multiple Layer Tissue Insertion
After having data field of tip forces of each types of tissue, the sensing
needle has the potential to identify the tissue type during insertions. To
assess its tissue identification function, insertion experiments on multiple
layer tissues need to be done. Two groups of multiple layer tissue were
prepared, i.e. the swine belly fat-muscle-liver tissue and the mouse tissues.
Figure 19 is the tip force sensing during the insertion of the swine fat-
muscle-liver tissue. During the insertions, the layer rupture information was
the most wanted signal, which provides very useful information, such as
tissue thickness and average tip force. It shows that layer rupture
information can be clearly captured by the tip force sensing needle.
Fig. 19 Automated swine tissue insertion [27]

For the mouse insertions, three types of multiple layer insertion
experiments were designed, the skin-leg muscle, the lateral direction of the
abdomen, and from anus to head. Skin-leg muscle experiment was
conducted after dissection, while the others were insertions first and then
dissection was undertaken for confirmation, as shown in Fig. 20.
Fig. 20 Multiple layers insertion experiments. a Skin-leg muscle insertion after dissection. b
Dissection after insertion of the lateral direction in the abdomen. c Dissection after insertion from
anus to head
Figure 21 shows the tip force results for two needle insertions of skin-
leg muscle. The needle penetrated the skin first at around 1.2 and 1.9 N,
respectively in the two insertions, as stage a shown in Fig. 16. It reached leg
muscle and broke the muscle layer at around 3.3 N (Stage b). It shows that
the sensing needle successfully captured the tissue layer breaking force
during the needle insertion. Figure 22 is the insertion result of the abdomen
in a lateral direction. It was found that the needle only penetrated the skin,
leg muscle and another skin layer on the other side.
Fig. 21 Murine skin-leg muscle insertion [28]

Fig. 22 Internal abdomen insertion in a lateral direction [28]
Another experiment was performed for internal organs identification.
The needle was inserted as stably as possible into the mouse body from the
anus to head. Figure 23 gives a force-time history of one insertion. The
result shows that there were several organs experiencing penetration.
According to needle displacement and mouse organ location, the possible
organs are colon, jejunum, liver, lung, and heart. This was also confirmed
via dissection.
Fig. 23 Insertion from anus to head, with possible tissue identification. a Colon. b Jejunum. c Liver.
d Lung. e Heart [28]

The experimental results show that the tissue layer breaking during
insertion procedure is obvious and the main organs can be successfully
identified by the tip force sensing needle. The tissue type can be confirmed
according to tip force database collected for various tissues, such as the data
in Fig. 18, and with the help of the knowledge of anatomy. Consequently, a
tissue layer sketch can be precisely drawn for clinicians during needle
insertions.
4 Discussion
This paper proposes optical sensing solutions for needle-tip applications of
needle-tip tactile sensing and bio-tissue identification. The design of the
sensor and its optical circuit are detailed described. The system is verified
via simulations, phantom tests, ex-vivo and in-vivo animal tests. During the
experiments, sensor properties and environmental influence factors on the
sensor were investigated. The results show that the designed needles are
capable of identifying tissue layers and types via analyzing force signals
during the insertion process in real time mode in a temperature-changeable
environment, with effective temperature compensation designs.
Even though the FPI sensing needle design achieved some good results,
there are still some design issues that need to be addressed. Firstly, the
cantilever beam design inside the needle increases the overall size of the
sensor, which cannot be integrated into the needle with the inner diameter
less than 1 mm. In addition, the integrated method makes the injection
needle lose its original function. Therefore, the sensor and its temperature
compensation structure need to be improved, in terms of minimizing size
and proper structure design. Secondly, the influence caused by the
operator's body movements has an important influence on the analysis of
the terminal tactile signals. For example, without the feedback design of
external factors, it is difficult to extract the effective tactile signals of the
needle tip. Therefore, it is necessary to study the mechanism of influencing
factors and design quantitative compensation methods.
References
1.
Deipolyi, A. R., et al. (2017). Needlestick injuries in interventional radiology are common and
underreported. Radiology, 285(3), 170103.

[Crossref]
2.
Petruska, A. J., Ruetz, F., et al. (2016). Magnetic needle guidance for neurosurgery: Initial design
and proof of concept. In IEEE International Conference on Robotics and Automation (pp. 4392-
4397).
3.
Singh, M. K., Parameshwarappa, V., et al. (2016). Photoacoustic-guided focused ultrasound for
accurate visualization of brachytherapy seeds with the photoacoustic needle. Journal of
Biomedical Optics, 21(12), 120501.
[Crossref]
4.
Hadjerci, O., Hafiane, A., et al. (2016). Assistive system based on nerve detection and needle
navigation in ultrasound images for regional anesthesia. Expert Systems with Applications, 61,
64-77.
[Crossref]
5.
Albanghali, M., et al. (2016). Construction of tissue microarrays from core needle biopsies-a
systematic literature review. Histopathology, 68(3), 323-332.
[Crossref]
6.
Cheung, K., & Das, D. B. (2016). Microneedles for drug delivery: trends and progress. Drug
Delivery, 23(7), 2338-2354.
[Crossref]
7.
Nagata, K., Sawada, K., et al. (2017). Effects of repeated restraint and blood sampling with
needle injection on blood cardiac troponins in rats, dogs, and cynomolgus monkeys.
Comparative Clinical Pathology, 26(6), 1347-1354.
8.
Barbé, L., et al. (2007). Needle insertions modeling: identifiability and limitations. Biomedical
Signal Processing and Control, 2(3), 191-198.
[Crossref]
9.
Roux, P. -J. A., et al. (2016). 3D haptic rendering of tissues for epidural needle insertion using an
electro-pneumatic 7 degrees of freedom device. In The 2016 IEEE International Conference on
Intelligent Robots and Systems 2016.
10.
Manoharan, V. (2011). Epidural needle insertion simulator: a device for training resident
anaesthesiologists. Master thesis, Delft University of Technology.
11.
Wang, L., & Yan, M. (2014). Research progress on labor analgesia. General nursing, 25, 2312-
2314.
12.
Tien, J. C., Lim, M. J., Leong, W. L., et al. (2016). Nine-year audit of post-dural puncture
headache in a tertiary obstetric hospital in Singapore. International Journal of Obstetric
Anesthesia, 28, 34-38.
[Crossref]
13.
Hermanides, J., Hollmann, M. W., Stevens, M. F., et al. (2012). Failed epidural: Causes and
management. British Journal of Anaesthesia, 109(2), 144-154.
[Crossref]

14.
Tran, D., Hor, K. W., Kamani, A. A., et al. (2009). Instrumentation of the loss-of-resistance
technique for epidural needle insertion. IEEE Transactions on Biomedical Engineering, 56(3),
820-827.
[Crossref]
15.
Dargahi, J., & Najarian, S. (2005). Advances in tactile sensors design/manufacturing and its
impact on robotics applications-a review. Industrial Robot: An International Journal, 32(3),
268-281.
[Crossref]
16.
Travakoli, M., Patel, R. V., & Moallem, M. (2005). Robotic suturing forces in the presence of
haptic feedback and sensory substitution. In Proceedings of 2005 IEEE Conference on CCA
2005, Control Applications.
17.
Demi, B., Ortmaier, T., & Seibold, U. (2005). The touch and feel in minimally invasive surgery.
In IEEE International Workshop on in Haptic Audio Visual Environments and their Applications.
18.
Wagner, C. R., Stylopoulos, N. & Howe, R. D. (2002) The role of force feedback in surgery:
Analysis of blunt dissection. In Symposium on Haptic Interfaces for Virtual Environment and
Teleoperator Systems. Citeseer
19.
Liu, H., Noonan, D. P., Challacombe, B. J., et al. (2010). Rolling mechanical imaging for tissue
abnormality localization during minimally invasive surgery. IEEE Transactions on Biomedical
Engineering, 57(2), 404-414.
[Crossref]
20.
Yip, M. C., Yuen, S. G., & Howe, R. D. (2010). A robust uniaxial force sensor for minimally
invasive surgery. IEEE Transactions on Biomedical Engineering, 57(5), 1008-1011.
[Crossref]
21.
Beekmans, S. V., & Iannuzzi, D. (2016). Characterizing tissue stiffness at the tip of a rigid needle
using an opto-mechanical force sensor. Biomedical Microdevices, 18(1), 15.
[Crossref]
22.
Carotenuto, B., Micco, A., Ricciardi, A., et al. (2017). Optical guidance systems for epidural
space identification. IEEE Journal of Selected Topics in Quantum Electronics, 23(2), 371-379.
23.
Kumar, S., Shrikanth, V., Amrutur, B., et al. (2016). Detecting stages of needle penetration into
tissues through force estimation at needle tip using fiber Bragg grating sensors. Journal of
Biomedical Optics, 21(12), 127009.
[Crossref]
24.
Liu, X., Iordachita, I. I., He, X., et al. (2012). Miniature fiber-optic force sensor based on low-
coherence Fabry-Pérot interferometry for vitreoretinal microsurgery. Biomedical Optics Express,
3(5), 1062-1076.
[Crossref]
25.
He, X., Handa, J., Gehlbach, P., et al. (2014). A submillimetric 3-DOF force sensing instrument
with integrated fiber Bragg grating for retinal microsurgery. IEEE Transactions on Biomedical
Engineering, 61(2), 522-534.
[Crossref]

26.
Mo, Z., & Xu, W. (2016). Temperature-compensated optical fiber force sensing at the tip of a
surgical needle. IEEE Sensors Journal, 16(24), 8936-8943.
[Crossref]
27.
Mo, Z., Xu, W., & Broderick, N. G. (2017). Capability characterization via ex-vivo experiments
of a fiber optical tip force sensing needle for tissue identification. IEEE Sensors Journal, 18(3),
1195-1202.
28.
Mo, Z., Mao, X., Hicks, K. O., & Xu, W. (2018). In-Vivo Tissue Identification on Mice Using a
Fiber Optical Tip Force Sensing Needle. IEEE Sensors Journal, 18(15), 6352-6359.
[Crossref]
29.
Crouch, J., et al. (2005). A velocity-dependent model for needle insertion in soft tissue. In
Medical Image Computing and Computer-Assisted Intervention-Miccai 2005 (pp. 624-632).
30.
Mahvash, M., & Dupont, P. E. (2010). Mechanics of dynamic needle insertion into a biological
material. IEEE Transactions on Biomedical Engineering, 57(4), 934-943.
[Crossref]
OceanofPDF.com

(1)
(2)
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_2
Object Detection on Train Bogies Using
Structured Light Scanning
Tangwen Yang1   , Yantao Sun1, Xiaoqing Cheng2, Honghui Dong2 and
Yong Qin2
School of Computer and Information Technology, Institute of
Information Sciences, Beijing Jiaotong University, Beijing, 100044,
China
The State Key Laboratory of Rail Traffic Control and Safety, Beijing
Jiaotong University, Beijing, 100044, China
 
Tangwen Yang
Email: twyang@bjtu.edu.cn
Keywords Object detection - Volume measurement - Train bogies -
Structured light scanning - 3D reconstruction
1 Introduction
By the end of 2016, China had built over 20,000 km high-speed railway, far
more than the total mileage of the rest countries of the world, and this
development attracts worldwide attention [1]. High-speed railway plays a
critical role in the progress of China development. But railway accidents
happen sometimes because of adverse weather, equipment failure, etc.
Ensuring safe and efficient operation has become a primary issue of high-
speed railway construction.
With the rapid development of machine vision, three-dimensional (3D)
measurement technology emerges in the field of object detection. Compared
with two-dimensional technology, 3D vision obtains the size, shape, volume
and other information of an object, and meanwhile overcome the problems

of lower image contrast and color interference. Structured light scanning
technology is one of the non-contact 3D measurement methods. Based on
the principle of geometrical triangulation, the depth information of a point
on an object is calculated, and the 3D model of the object can be then
obtained. Hence, the 3D technology can reconstruct and measure any three-
dimensional object. It has been widely used in many fields, such as three-
dimensional digital modeling, industrial product design, animation,
orthopedic medical, cultural relics protection, archaeological excavation,
digital city, and the rail safety inspection [2-6].
2 Related Work
It is an important safeguard measure for railway safety operation to detect
adhered objects on train bogie. Generally, this work is done by track
walkers, which is tedious, costly and inefficient. With the development of
computer and signal processing technology, new approaches are applied to
the fault detection of train bogie. Based on EEMD denoising and manifold
learning, Yu et al. [7] extracted the impulse components from the fault
signals of a running bogie and identified the fault types. Xu et al. [8] used
Hough transform to detect the tread profile in real time, which improves the
extraction accuracy and efficiency of contour geometric parameters. Xie [9]
proposed a feature extraction and fault diagnosis method based on Spark
vibration data. By analyzing the vibration data of the train body, axle box
and frame, the fault diagnosis of the train running part was effectively
completed. He et al. [10] proposed a fault diagnosis method based on rough
set (RS) and least squares support vector machine (LSSVM) for the fault
diagnosis of high-speed train running gear rolling bearings. The method has
higher accuracy and real-time performance, and provides a new method for
fault diagnosis of high-speed train rolling bearings, and the multi-source
information fusion can identify the fault of the running gear by assigning
different weight information to the data of the sensor. The continuous
improvement of data processing technology provides solutions to fault
detection in the train bogie.
Structured light technology has the characteristics of high precision,
high speed and simple structure, and is widely used in various fields. More
and more researchers have increasing interest in structural light
measurement technology, and promoted scientific research results to

industrial production and daily life. Zhang et al. [10] proposed a method
based on monocular vision, which uses sequence images acquired by a
passive sensor to reconstruct a long-distance scene, and quickly detects and
locates obstacles. Liu et al. [11] proposed a method of using multiple lines
structured light to determine whether high-speed railway fasteners are
missing by extracting the feature quantity from images and deriving the
characteristic parameters. The structured light scanning technology can be
used to calculate the volume of an object by reconstructing the three-
dimensional model of the object, which greatly improves detection
accuracy and efficiency. Riccabona et al. [12] used ultrasonic scanning
technology to reconstruct the object and the experimental accuracy could be
acceptable. Zhang et al. [14] used multi-angle images to get the point cloud
model and calculated the volume and surface area by incremental Delaunay
triangulation. Zhang [15] proposed a shading method to recover the three-
dimensional information of the measured object, and statistically sums the
heights of all the pixels to obtain the pixel volume of the irregular object.
Zhou et al. [16] established a linear model of the actual surface area and
volume of object from the pixel surface area and volume, and used the
linear model to predict the egg volume. Mao et al. [17] used binocular
stereo vision technology to measure the volume of irregular pyramids. In
general, the application of structured light technology in the measurement
of irregular object volume provides a new way of measuring the volume of
irregular objects and is of great help to improve the accuracy of three-
dimensional measurement.
3 Methods and Principles
3.1 Line-Plane Model
The linear structured light system consists of a linear laser and a camera.
The relative positional relationship between the linear laser and the camera
can convert the point coordinates in the two-dimensional image into three-
dimensional coordinates in the camera coordinate system. The relative
positional relationship between the laser and camera is determined by a
line-plane model, where the line refers to the linear equation of the line
connecting the point on the CCD plane and the optical center, and the plane
refers to the plane equation of the sector projected by the laser. The line-

plane model is established from the optical plane calibration. The line-plane
model is shown in Fig. 1.
Fig. 1 The schematic diagram of line-plane model
The laser emits a beam of light onto an object and generates a distorted
light stripe. 
 is a pixel in image, corresponding to the point 
 on the
distorted light stripe. The camera optical center 
, and 
 and 
 are on
the same line. In the camera coordinate system, 
 is the origin, and pixel
 can be obtained by image processing. The linear equation connecting
point 
 and point 
 can be expressed as
(1)
In the camera coordinate system, the equation of the structured light
plane can be expressed as
(2)

Using Eqs. (1) and (2) the coordinate of point 
 in the camera
coordinate system can be calculated. Similarly, the coordinates of any point
on the light stripe can be calculated, and the two-dimensional coordinates of
the light stripe can be further converted into three-dimensional coordinates
in the camera coordinate system.
3.2 Laser Triangulation Principle
Laser triangulation is the basis of the three-dimensional measurement using
linear structured light. To obtain information of object, the camera
coordinate system should be transformed to the world coordinate system.
The laser triangulation is used to transform the coordinate system of point
clouds. The laser triangulation principle is shown in Fig. 2.
Fig. 2 The triangulation principle
First the position of the base plane is determined, and the height of the
measured object refers to the height relative to the base plane. The laser is
projected onto the surface of the object at point 
. The actual height of the
object measured is . 
 is the mapping of the actual height  on the
CCD plane. The angles  and 
 can be calculated using the cosine theorem
and the known coordinates in the camera coordinate system. In terms of

triangle similarity principle, the relation between 
 and  is established
as
(3)
where 
 is the distance from the focal point of the CCD optical axis and
the laser optical axis to the lens center, 
 is the distance from the center
of lens to the reference point,  is the angle between the laser optical axis
and the CCD optical axis, and 
 is the angle between CCD optical axis and
CCD plane.
3.3 Light Stripe Center Extraction
In the structured light scanning system, the width of the laser beam in
image is about 10 pixels, depending on the laser's projection distance and
the surface characteristics of the object, as shown in Fig. 3.
Fig. 3 The laser light stripe width
Light stripe center extraction can be divided into stripe extraction based
on morphological characteristics and stripe extraction based on gray
features [18, 19]. The method of light stripe center extraction based on
morphological characteristics includes edge method, geometrical center
method, threshold method, etc. Light stripe center extraction based on gray
features includes gray centroid method and Hessian matrix method. The

ideal structured light stripe is a Gaussian distribution, and the actual light
stripe distribution is a Gaussian-like distribution shown in Fig. 4. The
truncated Gaussian normal distribution based on curve fitting method is
used here to fit the stripe and extract the center of the stripe.
Fig. 4 a Standard normal distribution. b Actual gray-scale distribution
Here assuming 
 is a normal distribution, as 
, the density of
probability function 
 is expressed by
(4)
When 
, the probability density function 
.
The mathematical expectation of truncated normal distribution is given
below

(5)
If  and  are symmetric about 
, then 
. The
center of light stripe is the symmetric center of truncated Gauss distribution.
3.4 ICP Algorithm
The linear structured light system reconstructs a three-dimensional point
cloud of object by collecting and analyzing the light stripe images. Because
the three-dimensional point cloud is scattered data, it is very difficult for a
single point cloud to detect adhered substances on train bogie. While
through reconstructing the point cloud twice, the difference of the two
reconstructed point clouds can be then used to measure and detect the
objects on the train bogie. To compute the difference of the two clouds, we
have to register the point clouds first. The iterative closest point (ICP)
algorithm is frequently used to achieve this goal.
The ICP is an optimal registration algorithm based on the least square
method. By calculating the corresponding points of a point cloud to be
registered and a target point cloud. The optimal rotation parameters and
translation parameters are found till the point clouds are converged. Here,
the point clouds 
 and 
 are obtained from the system. 
.
The Euclidean distance between 
 and 
 can
be expressed as
(6)
The registration of 
 and 
 can be expressed as

(7)
where 
 is the rotation matrix and the 
 is the translation matrix. The
minimized objective function is expressed as
(8)
The optimal solution of 
 and 
 is obtained by singular value
decomposition (SVD) algorithm.
4 Experimental Results
4.1 Experimental Setup
The schematic diagram of the experimental system is shown in Fig. 5. It
consists of two sets of photoelectric switches, a structured light scanning
module and a miniaturized train bogie module. The photoelectric switches
are placed on the left and right sides of the scanning module. The left
switch starts a scanning, and the right one ends the scanning. Each switch
has a laser emitter and a receiver. The signals from the switches are used to
start or end a scanning. The structured light module is located at the rail
track side, about 1 m away from the track. The images for the camera of the
scanning module are recorded by a computer. The whole working process
can be described below. As the train moves to the start point, the left switch
triggers the structured light module to start scanning the train bogie data. As
it reaches the stop point, the right switch will trigger the structured light
module to end the scanning. The images are transmitted to data processing
module for 3D reconstruction of the train bogie.

Fig. 5 The schematic diagram of an experimental setup
4.2 Three-Dimensional Reconstruction
Using the pixels on the light stripes, the calibrated camera model and the
triangulation principle, the depth of a point on the bogie can be computed
with (3). When the depth information is obtained for all the pixels of the
laser stripe, a 3D point cloud is generated. But there have a large number of
background points in the reconstructed point cloud. These points are from
the base plane, and the points behind the base plane are normalized to be
the background points. They seriously decrease the speed of the point cloud
calculation. A threshold method is used to remove the background points, as
shown in Fig. 6.

Fig. 6 The background point removal
To improve the visualization of the cloud points, a greedy projection
triangulation algorithm is used to project the three-dimensional points onto
a plane through a normal line. The point cloud obtained by the projection is
then triangulated, and the connection relationship of each point is
calculated. The 3D reconstruction result is shown in Fig. 7.
Fig. 7 The visualization result with greedy projection triangulation algorithm
4.3 Volume Measurement
The volume measurement of the adhered substances on the train bogie is
very important to eliminate the contigent risk of rail safety. Hence, a
volume measurement approach is proposed here based on the numerical

integration of a point cloud difference. The difference is calculated from
two sets of point clouds, namely a scanning point cloud and an initial point
cloud. The initial point cloud is obtained when the train bogie has no
adhered substances. The numbers of the two sets of point clouds are
different, and established at different time periods. Moreover, due to the
delay to start the acquisition and the variation of the train running speed, the
point clouds are offset. The two clouds are needed to set in the same
coordinate system. Hence, the ICP algorithm introduced previously is used
to registered them. To speed up the computation, the subsampled method is
used to reduce the number of point cloud data, and the background points
are removed as well. The registration results of two sets of point clouds are
shown in Fig. 8. The left side is the point clouds to be registered, and the
right side is the registered point clouds.
Fig. 8 The registration result of two sets of point clouds with ICP algorithm
Because of the discreteness characteristics of point cloud, there are
many redundant noise points in the point cloud difference, as shown in
Fig. 9a, and the noise points may decrease the accuracy of the volume
computation of adhered substances. To remove the noise points, a threshold
in the z axis is used to filter the redundant noise points. The point difference
after filtering is shown in Fig. 9b. Once the redundant points are filtered, we
can see that the adhered substance can be detected and figured out.
Subsequently, the volume of differential point cloud is able to calculate by
integration, too.

Fig. 9 The point cloud difference without and with filtering
5 Conclusion
An approach to detect adhered substances on train bogie is proposed using
linear structured light scanning technology, and the volume of the
substances is computed with numerical integration. The principle of
structured light scanning is first introduced, and the extraction method of
light stripe center is presented based on Gaussian distribution. To obtain the
difference of a scanning point cloud and the initial background point cloud,
the scanning point cloud is registered with the ICP algorithm to align in the
same coordinate system of the background point cloud. A train bogie setup
is used to validate the proposed algorithms, and experimental results show
that the structured light scanning method can reconstruct the 3D model of
the bogie, and the adhered substances on train bogie is able to be detected
and the volume of the substances can be computed accurately with
numerical integration.

References
1.
Gao, W., & Zhang, X. (2016). High-rail current situation of China and future prospect research.
Shanxi Architecture, 42(32), 172-173.
2.
Zhang, L-b, Wang, P-j, Zhang, X., & Li, W-t. (2016). Research on 3-D structured light rail
detection of high-speed railway and viewpoint optimization. Machinery Design & Manufacture,
4, 69-72.
3.
Wang, J., Xu, Y-j, Wang, L., & Wang, P. (2014). The application research of machine vision in
rail wear detection. Railway Standard Design, 9, 36-39.
[MathSciNet]
4.
Zhan, D., Yu, L., Xiao, J., et al. (2015). Multi-camera and structured-light vision system (MSVS)
for dynamic high-accuracy 3D measurements of railway tunnels. Sensors, 15(4), 8664-8684.
[Crossref]
5.
Liu, Z., Sun, J., Wang, H., et al. (2011). Simple and fast rail wear measurement method based on
structured light. Optics and Lasers in Engineering, 49(11), 1343-1351.
[Crossref]
6.
Ping, L. I., Wang, P. J., Chen, P., et al. (2018). Rail corrugation detection based on 3D structured
light and wavelet analysis. Railway Standard Design, 62(4), 33-38.
7.
Yu, P., Jin, W., & Qin, N. (2016). High-speed train running gear fault feature extraction based on
EEMD denoising and manifold learning. Journal of the China Railway Society, 38(4), 16-21.
8.
Xu, Z., & Chen, J. (2017). Tread profile of wheel detection method based on image processing
and Hough transform. Electronic Measurement Technology, 6, 117-121.
9.
Xie, J. (2015). Research on fault diagnosis of high-speed running gear based on deep learning
under cloud platform. Southwest Jiaotong University.
10.
He, D. Q., Chen, E. H., Li, X. M., et al. (2017). Research on fault diagnosis method of high-
speed train running gear rolling bearing based on RS and LSSVM. Journal of Guangxi
University, 42(2), 403-408.
11.
Zhang, D.-z., Wang, Y.-t., Tian, J.-w., Wang, C.-q., Guo, Q. (2008). Efficient 3D reconstruction
using monocular vision. Journal of Astronautics, 29(1), 295-300.
12.
Liu, H., Qian, G., Zhang, H., Tao, W., Zhao, H., Wang, W., et al. (2011). High-speed railway
fastener detection method based on structured-light. Measurement Technique, 5, 3-7.
13.
Riccabona, M., et al. (1995). Distance and volume measurement using three-dimensional
ultrasonography. Journal of Ultrasound in Medicine Official Journal of the American Institute of
Ultrasound in Medicine, 14(12), 881-886.
14.
Zhang, W., et al. (2016). A novel method for measuring the volume and surface area of egg.
Journal of Food Engineering, 170, 160-169.
[Crossref]

15.
Zhang, N. (2015). Study on the volume measurement method of irregular object based on
computer vision. Shaanxi University of Science and Technology.
16.
Zhou, P., Zheng, W., Zhao, C., et al. (2008). Egg volume and surface area calculation based on
machine vision. Computer and Computing Technologies in Agriculture II, 3, 1647-1653.
17.
Mao, J., Lou, X., Weixian, L. I., et al. (2016). Binocular 3D volume measurement system based
on line-structured light. Optical Technique, 42(1).
18.
Wu, J. Y., Wang, P. J., et al. (2009). Method of linear structured light sub-pixel center position
extracting based on gradient barycenter. Journal of Image & Graphics, 14(7), 1354-1360.
19.
Gao, S., & Yang, K. (2011). Research on central position extraction of laser strip based on
varied-boundary Gaussian fitting. Chinese Journal of Scientific Instrument, 32(5), 1132-1137.
OceanofPDF.com

(1)
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_3
A Method for Detecting Breaking Rate of
Ganoderma Lucidum Spore Powder Based
on Machine Vision
Shanling Ji1, Zhisheng Zhang1   , Zhijie Xia1 and Ying Zhu1
School of Mechanical Engineering, Southeast University, Nanjing,
211189, China
 
Zhisheng Zhang
Email: oldbc@seu.edu.cn
Keywords Ganoderma lucidum spore powder - Breaking rate - Computer
vision - Local gray range threshold processing
1 Introduction
Ganoderma lucidum spore powder (GLSP)is an extremely tiny spore of
Ganoderma lucidum that emerges from the pleats during the growth stage,
and has all the genetically active substances of Ganoderma lucidum. Broken
GLSP can be obtained by breaking the chitin shell of GLSP by bio-
enzymatic method, physical method or chemical method. Some
experiments have demonstrated that the medicinal effect of broken
Ganoderma lucidum spores as a traditional Chinese medicine is better than
that of unbroken Ganoderma lucidum spore [1]. Therefore, detecting the
breaking rate of GLSP is an important link in the production process.
Presently, microscopic detection and chemical fingerprinting are
common methods for detecting the breaking rate of GLSP [2]. Although the
two methods have high accuracy, they require more manual operational
experience and the operation process is complicated. Chen et al. proposed

the use of FTIR micro-spectroscopy to identify unbroken spores [3]. But
this method was not applied to the calculation of the rate of cell wall
breakage of Ganoderma lucidum spores.
With the study of deep convolutional networks, people have made great
progress in image recognition. However, directly using deep learning for
image recognition places great demands on the amount of image data. And
it is inevitable that data enhancement and other operations are required to
obtain more trained images. Therefore, microscopic images of Ganoderma
lucidum spore powder as shown in Fig. 1 are difficult to be directly
recognized using deep learning with small quantities of images.
Fig. 1 Microscopic image of broken Ganoderma spore powder
This paper proposes a method based on machine vision to detect the
breaking rate of Ganoderma lucidum spore powder, which is mainly to
detect the number of intact spores from the microscopic image like Fig. 1
and then calculate the breaking rate. The detection step is first to filter the
image bilaterally, and then extract each ROI region by using local threshold
processing and connected domain extraction. For areas that are suspected of
overlapping, segmentation is performed using distance conversion and
Gaussian mixture clustering. Each of the divided sub-pictures is square, and
then reset to the same large square. The features were extracted using

AlexNet [4]. Finally, the support vector machine was used to classify and
identify the same size square image, and the number of unbroken spores in
the original image was counted to calculate the breaking rate.
The details of the method based on machine vision for unbroken spore
identification are given in Sect. 2. Section 3 examines the effectiveness of
the local range threshold processing method and the breaking rate
calculation platform proposed in this paper through experiments, and
discusses the experimental results. Finally, the paper summarizes the full
text and elaborates on the future work.
2 Methodology
2.1 Image Acquisition and Preprocessing
The image acquisition system consists of an optical microscope, an
electronic eyepiece, and a PC. The PC side collects a 500-fold magnified
microscopic image with a resolution of 4076 × 3116. The on-site image
acquisition equipment is shown in Fig. 2.
Fig. 2 The on-site image acquisition equipment
Image bilateral filtering is a nonlinear filtering method [5]. The
grayscale image is denoised by bilateral filtering, Gaussian filtering and
mean filtering as shown in the Fig. 3. Compared to other filtering, bilateral

filtering can well preserve the details of the edges of the image and filter
out the spatial noise. Therefore, the grayscale image of the GLSP
microscopic image in this paper will be preprocessed using bilateral
filtering.
Fig. 3 a Is grayscale image; b-d are respectively filtered by Gaussian filtering, mean filtering and
bilateral filtering
2.2 Threshold Processing Based on Gray Level Difference of
Image Local
As can be seen from Fig. 3, the gray value changes of the background and
foreground are sharp, and this feature can be well represented by the local
range threshold processing. For each pixel point 
, with  as the
radius, the gray level variation 
 in the case of eight connections is
obtained. If 
 is smaller than the threshold 
, the pixel is set as the
background (the gray value is 0). Otherwise, it is the foreground (the gray
value is 1). The formula is expressed as (1) and (2).
(1)

(2)
 represents the gray value of all the pixels with the 
 point as the
core and radius , and 
 is the value of the binary image at the
 point. In order to make the image adaptive to get the appropriate L
value, assign L as follow:
(3)
 is the range of gray values of the entire image. Using the local range
threshold processing not only can better extract the foreground area, but
also avoid the influence of uneven illumination. The comparison between
the method used in this paper and other threshold processing methods is
described in Sect. 3.1.
2.3 Split the Foreground and Extract the ROI Area
In this paper, the ROI region is extracted first, and then the image
classification method is used to identify the intact spores in the image. The
image processed by the threshold in the previous step, after a simple
morphological processing, can extract the connected domain well. The
decision whether or not to further split is then determined by the area of the
connected domain. The area calculated for each connected domain is the
number of pixels. If the area is too large, it will be set to a suspected
overlapping area and needs to be further divided. As shown in Fig. 4, the
connected domain part identified by the binary map, for the non-
overlapping part, directly settles its coordinate range, and uses a square to
take a screenshot of the original image of the area. For overlapping areas,
use the minimum box of the connected area to frame it and identify the area
of the box in the original image, waiting for further segmentation. Various
screenshots are shown in Fig. 5.

Fig. 4 The left graph is a binary graph after the local extreme difference threshold processing, and
the right graph is the connected domain extracted from the binary graph and marked in the grayscale
graph
Fig. 5 The initially intercepted ROI region contains a single unbroken spore (a), a single broken
spore (b), a small region of oil (c) and a suspected overlapping region (d) and (e)
In this paper, the method of segmenting the elliptical shape model touch
unit by Winter et al. [6] is employed. Firstly, the distance conversion value
is calculated for the binary map of the overlap region, and then the pixel is
copied according to the distance conversion value of the pixel point, and
finally the Gaussian mixture model is used for fitting and clustering. As
shown in the Fig. 6, the area where the two spores overlap, the number of
overlapping spores is obtained by calculating the area of the connected
domain of the binary map, and then the cluster is fitted to the region, and
finally the spores are separated as shown in the figure. For each of these
areas, calculate the centroid, horizontal distance, and vertical distance of the
coordinates, and then use the two smaller values and the centroid as the side
length and centroid of the subsequent screenshot. As shown in Fig. 7, there
are a plurality of unbroken spore overlapping regions and suspected
overlapping regions of broken spores. Since the calculated connected
domain area exceeds the single spore area, the segmentation is performed

separately. Although the method of [6] was originally used to segment
overlapping elliptical cells, it performed well in this paper.
Fig. 6 Segmentation process of two overlapping unbroken spores

Fig. 7 Split overlapping area
2.4 Classification of Intercepted ROI Areas
After the previous steps, the spore image collected by the microscopy
device has been divided into square images containing intact spore

granules, broken spores and oil. Now we need to classify and identify these
images and calculate the number of intact spores (Fig. 8).
Fig. 8 Identifying unbroken spores
AlexNet [4] is a convolutional network proposed by Alex and Hinton in
the ILSVRC2012 competition. It improves CNN and becomes the general
structure of the CNN network. AlexNet is a deep convolutional network
model with eight layers (5 layers of convolutional layers and 3 layers of
fully connected layers) and the last layer for classification. This article uses
4096 data output from AlexNet Layer 7 as the extracted feature. Support
vector machines show many unique advantages in solving small sample,
nonlinear and high-dimensional pattern recognition, and can be applied to
other machine learning problems such as function fitting. This paper uses
AlexNet to extract the features of images and uses multiple types of support
vector machines for classification.
2.5 Calculate the Breaking Rate
For the same batch of GLSP, the same amount of spore powder was taken
before and after processing, and an evenly distributed suspension was
prepared using an equal amount of reagent. An equal amount of suspension
was taken and images were taken with a 500-fold microscope, and the
number of intact spores in the same field of view was separately counted.
The number of intact spores sampled for the unprocessed spore powder was
, and the number of intact spores after processing was 
. The
calculation formula of the breaking rate is (4).
(4)
3 Experiment and Discussion
3.1 Comparison of Various Image Threshold Processing
Methods

Figure 9 showed the processing results of grayscale images processed by
OTSU, maximum entropy, block OTSU, and local range threshold. It could
be seen from the results that the proposed method performs better than
OTSU, maximum entropy and block OTSU in the case of uneven
illumination of the image. More importantly, the location of the foreground
area extracted by the method of this paper was more accurate.


Fig. 9 Comparison of various image threshold processing methods
3.2 Test Results of the Breaking Rate Calculation Platform
As shown in Fig. 10, the built-in breaking rate calculation platform could
select functions such as segmenting the image, training the classifier or
calculating the breaking rate. Table 1 showed the number of intact spores
identified by the artificial identification and the platform of this paper, and
the breaking rate in both cases was calculated using Eq. (4) and was shown
in Table 2. According to the field conditions, a total of four batches were
tested, and the accuracy of the wall breaking rate calculated by the platform
of this paper was calculated according to the formula (5). 
 and 
represented the calculated wall breaking rate and the wall breaking rate
calculated by the platform, respectively.
Fig. 10 GLSP breaking rate calculation platform
Table 1 Number of intact spores under different conditions

 
Batch Artificial count Platform count Accuracy
Unprocessed GLSP 1
20
20
1.0000
2
42
41
0.9762
3
19
20
0.9474
4
19
18
0.9474
Processed GLSP
1
4
4
1.0000
2
2
2
1.0000
3
2
2
1.0000
4
4
4
1.0000
Table 2 Platform breaking rate calculation accuracy
Batch
Accuracy
1
0.8000 0.8000 1.0000
2
0.9524 0.9512 0.9987
3
0.8947 0.9000 0.9941
4
0.7895 0.7778 0.9582
(5)
The results of partial platform tests are given in Fig. 11. Combined with
the chart, it can be concluded that the accuracy of the method given in this
paper is at least 95%, which meets the requirements.

Fig. 11 Partial test results
4 Conclusion and Future Work
Based on machine vision, a platform was established for identifying intact
spores from microscopic images and calculating the rate of broken wall. It
can improve the detection efficiency of Ganoderma spore powder breaking
rate and provided a new method for the detection of spore powder breaking
and similar microscopic drugs.
The method of segmentation recognition by extracting connected-
domain images was established in this paper. Under the condition of
collecting micro-image data, the average recognition rate can reach 95%.
The proposed local range threshold processing method performed well in
the case of uneven illumination. The method proposed in this paper
provided a method to detect the wall breaking rate of Ganoderma lucidum
spore powder in breaking machine.

Acknowledgements
This research work is supported by the National Natural Science
Foundation of China (Grant Nos. 51775108).
References
1. Wang, W., Chen, G., Su, J., Lv, G., & Chen, S. (2017). Comparative study of tumor growth and
VEGF expression in mice with Lewis lung cancer by Ganoderma lucidum spore powder and
broken Ganoderma lucidum spore powder. Pharmacology and Clinics of Chinese Medicine,
33(02), 118-122.
2.
Zhao, J. S., Wei, L., Yan, Y., et al. (2013). Research progress on the test method of the breaking
rate of Ganoderma lucidum spore powder. Chinese Medicine Guide, 5, 431-434.
3.
Chen, X., Liu, X., Sheng, D., Huang, D., Li, W., & Wang, X. (2012). Distinction of broken
cellular wall Ganoderma lucidum spores and G. lucidum spores using FTIR microspectroscopy.
Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy, 97, 667-672.
[Crossref]
4.
Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep
convolutional neural networks. Advances in Neural Information Processing Systems, 2, 1097-
1105.
5.
Chaudhury, K. N., Sage, D., & Unser, M. (2011). Fast $O(1)$ bilateral filtering using
trigonometric range kernels. IEEE Transactions on Image Processing, 12, 3376.
[MathSciNet][Crossref]
6.
Winter, M., Mankowski, W., Wait, E., et al. (2019). Separating Touching Cells Using Pixel
Replicated Elliptical Shape Models. IEEE Transactions on Medical Imaging, 4, 883.
[Crossref]
OceanofPDF.com

(1)
 
 
 
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_4
6D Pose Estimation of Texture-Less Object
in RGB-D Images
Heng Zhao1   , Chungang Zhuang1   , Lei Jia1    and Han Ding1  
School of Mechanical Engineering, Shanghai Jiao Tong University,
Shanghai, China
 
Heng Zhao
Email: 118020910303@sjtu.edu.cn
Chungang Zhuang (Corresponding author)
Email: cgzhuang@sjtu.edu.cn
Lei Jia
Email: jierryjia@sjtu.edu.cn
Han Ding
Email: hding@sjtu.edu.cn
Keywords Pose estimation - Pose refinement - Deep learning - RGB-D
image - Point cloud
1 Introduction
Texture-less objects are common in industrial environments and estimating
their 6D poses (three in translation and three in rotation) has a wide range
of applications in robotic tasks. For example, it is important for robot bin
picking to recognize 6D pose of objects. However, 6D pose estimation still
struggles to achieve fast and reliable results in real-world scenes. The real-
world scenes are usually filled with objects of different shape and their

appearances on images are easily affected by illumination, clutter and
occlusions between objects.
Traditionally, the problem of 6D pose estimation is tackled by matching
local features like SIFT [1] and ORB [2] extracted from an RGB image to
features in a 3D model of the object [3]. However, the stability of their
features depends on the rich texture of the object. The lack of texture
implies that the 6D object pose cannot be reliably recognized with these
methods. In recent years, there are some works [4-6] which apply deep
learning for 6D object pose estimation. These methods are not end-to-end or
only estimate an approximate pose. They require further refinements to
improve the accuracy. The classic refinement method is to handle it by
using ICP algorithm [7], which linearly increases the running time.
In this paper, we propose an end-to-end deep neural network for 6D
texture-less object pose estimation from RGB-D images. The network
consists of two subnetworks. The pose estimation subnetwork takes data
combined with RGB values and depth values at pixel level as input and
predicts an approximated pose. An estimated point cloud is calculated by
rendering an object based on the approximated pose. The estimated point
clouds, together with the initial point clouds obtained from depth image, are
fed into a pose refinement subnetwork to refine the pose. By iteratively re-
rendering the object based on the improved pose, the two input point clouds
to the refinement subnetwork become more and more similar, thereby
enabling the subnetwork to generate more and more accurate pose
estimates. Some examples of 6D pose estimation of texture-less objects are
shown in Fig. 1.

Fig. 1 Examples of real-world scene images (up) overlaid with colored 3D models at the estimated
6D poses (down)
In order to reduce the cost of collecting datasets, a physically-simulated
environment is constructed to generate dataset. The LINEMOD [8] and
self-made industrial parts dataset are used to evaluate the proposed method.
The experimental results show that the proposed method is competitive and
outperforms the state-of-the-art methods on the LINEMOD dataset in some
respects. Last, we also show its utility on the self-made industrial parts
dataset.
The main contributions of this work are: (1) an end-to-end approach for
6D texture-less object pose estimation from RGB-D images. (2) a pose
estimation subnetwork taking the data combined with RGB values and
depth values at pixel level as input. (3) a pose refinement subnetwork taking
point clouds as input. In the following sections, we discuss the related work
in Sect. 2, describe our method in Sect. 3, present experimental results in
Sect. 4. Conclusions are given in Sect. 5.
2 Related Work
RGB methods. Classical methods of 6D pose estimation using RGB
images mostly rely on matching features extracted from an RGB image to
features in a 3D model of the object [3]. The performance of these methods
depends on the rich texture of the object. Recently, researchers focus on the
texture-less objects based on the deep learning method. In PoseCNN [6],

Xiang et al. proposed a network which estimates the 3D translation of an
object by localizing its center in the image and predicting its distance from
the camera. In SSD-6D [4], Kehl et al. extended the popular SSD [9]
paradigm to cover the full 6D pose space. They decompose 3D rotation
space into discrete viewpoints and in-plane rotation and treat the rotation
estimation as a classification problem. In BB8 [5], Rad et al. applied to the
detected objects a Convolutional Neural Network (CNN) trained to predict
their 3D poses in the form of 2D projections of the corners of their 3D
bounding boxes. In fact, the RGB image contains color and texture
information of an object, while the depth image contains the geometry
information. Due to the lack of geometry information, the performance of
these methods using RGB image only is still not comparable to RGB-D
based methods.
RGB-D methods. With the advent of commodity depth cameras,
researches focus on pose estimation using the RGB-D images [10-13]. The
most traditional approaches are to use template matching [8, 14, 15].
LINEMOD [8] is the most notable work belonging to this category. The
authors build the templates by rendering views of 3D models and
embedding quantized color gradient and normal features. Rios et al. [14]
proposed to learn the templates in a discriminative fashion and cascaded
detections for higher accuracy and efficiency respectively. Recently, deep
learning based RGB-D methods are used on object recognition and pose
estimation. Kehl et al. [13] employed a convolutional auto-encoder for
building regressed descriptors of locally-sampled RGB-D patches for 6D
vote casting. Li et al. [16] presented a framework for accurately inferring
6D object pose from single or multiple views by integrate three new
capabilities into a deep CNN: an inference scheme, the fusion of class
priors and the fusion of class priors. Wang et al. [17] presented
DenseFusion, a generic framework for estimating 6D object pose from
RGB-D images. The core of their approach is to embed and fuse RGB
values and point clouds at per-pixel level.
The method most relevant to the proposed method is DeepIM [18], in
which the network of only RGB image as input is able to iteratively refine
the pose by matching the rendered image against the observed image. In
order to eliminate the dependence on color and texture, our pose refinement
subnetwork refines pose by matching rendered point clouds instead of RGB

image. Finally, we show its utility on the LINEMOD and self-made
industrial parts dataset.
3 Methodology
3.1 Architecture Overview
The goal of 6D object pose estimation is to obtain the rigid transformation
from the object coordinate system to the camera coordinate system. Given
an RGB-D image, we design the network to directly output a relative
transformation that consists of a 3D rotation 
 and a 3D translation 
.
Since we estimate the 6D pose of the objects from camera images, the poses
are defined with respect to the camera coordinate system.
Figure 2 illustrates the framework of 6D texture-less object pose
estimation. We first use a segmentation network to generate object instance
masks. The masks of RGB-D image are then passed individually through
the pose estimation subnetwork, which outputs a 6D approximated pose for
each object. We assume that the 3D model of the object is available. An
estimated point cloud is calculated by rendering the 3D model of the object
based on the approximated pose. The estimated point clouds, together with
the initial point clouds obtained from depth image, are fed into a pose
refinement subnetwork to refine the pose.

Fig. 2 An overview of our 6D pose estimation framework
3.2 Instance Segmentation
Instance segmentation focuses on detecting the bounding box location and
object category as well as predicting the predefined category for each pixel
in the bounding box from an RGB image. The focus of this work is to
develop a pose estimation network. Thus we use an existing instance
segmentation architecture YOLACT [19]. The network takes in RGB
images and outputs segmentation labels, which are converted into bounding
box and binary instance masks with associated object classes and fed into
the subsequent pose estimation network.
3.3 Pose Estimation Subnetwork
The goal of pose estimation subnetwork is to predict an approximated pose.
The pose is represented by its position 
 and orientation
, which are translations and rotations relative to the
camera coordinate frame. The subnetwork takes RGB and depth images
patch cropped by the bounding box as input. The first step is to extract
correct information from the color and depth channels.
Feature Extraction. Inspired by the DenseFusion [17], separately to
generate color and geometric features from RGB image and 3D point
clouds. We extract color and geometric features from RGB and depth
images. In order to make color and depth present a similar format,
normalization processing is carried out. The subnetwork follows a simple
CNN architecture consisting of a ResNet-18 [20] feature extractor followed
by a multilayer perceptron.
Feature Fusion. After feature extraction, per-pixel feature maps from
RGB and depth images separately are generated. In order to minimize the
effects of occlusion and segmentation noise, the feature maps are sampled
at per-pixel level according to binary instance mask generated by instance
segmentation stage. Next, we need to combine the features. Figure 3
illustrates the structure of feature fusion network. The network takes two
sampled feature maps with channel dimension 32 generated by the feature
extraction stage as inputs. The multilayer perceptron is used to regress
position and quaternion values. Inspired by the PointNet [21], local and
global information aggregation is carried out. The local color and depth

features are concatenated in the layers of dimension 32, 64 and 128. After
computing the global feature, we feed it back to per-pixel local features by
concatenating the global feature with each of the local features. Then we
extract new per-pixel features based on the combined the features. Now, the
per-pixel feature is aware of both the local and global information. The per-
pixel features are fed into a final network to predict the 6D object pose.
Fig. 3 The architecture of pose estimation subnetwork
Loss Function. The loss function is defined as the distance between the
target point clouds and the point clouds transformed by the predicted pose.
Given the ground truth pose 
 and the estimated pose set
, the loss is computed as:

(1)
where N denotes the total number of per-pixel features and 
 is 3D point
on the object model at the corresponding pixel location.
Pose Clustering. We obtain the 6D object pose for each per-pixel
features from above section. Now, we need to choose a best pose as the
estimated final pose and feed it into the subsequent pose refinement
network. Inspired by the PPF [22]. First, a new cluster is created with the
highest vote pose hypothesis. Then, similar poses are grouped together. If a
pose is obviously different from the existing clusters, a new cluster will be
created. The score of a cluster is the sum of the number of all contained
poses. Finally, the pose with highest score will be return.
3.4 Pose Refinement Subnetwork
So far we have obtained an approximated pose 
 using pose estimation
subnetwork. The goal of pose refinement network is to predict more
accurate 6D pose based on 
.
Rendering Point Clouds. Inspired by the DeepIM [18], in which the
authors render 3D model of the object to obtain RGB image. However, this
method requires that the color and texture of 3D model is as same as
possible to real-world scenes. We assume that the 3D model of the object
and camera-intrinsic parameters are available. In order to eliminate
dependence on color and texture, an estimated 3D point cloud is obtained
by rendering the 3D model of the object under 6D pose 
.
Network Structure. Figure 4 illustrates the structure of the subnetwork.
The subnetwork takes the estimated point clouds and the initial point clouds
obtained from depth image as input and predicts 
 between the current
approximated pose and the target pose. Inspired by the PointNet [21].The
 has to be invariant if the pose of the two point clouds is changed
together. Therefore an affine transformation matrix is predicted by a mini-
network and directly apply this transformation to the coordinates of the two
input point clouds. The mini-network is composed by basic modules of

point independent feature extraction, average pooling and fully connected
layers. Unlike pixel arrays in images or voxel arrays in volumetric grids,
point cloud is a set of points without specific order. Therefore the average
pooling layer as a symmetric function to aggregate information from the
two point clouds. Finally, several linear layers is used to predict 
 and
 between the current approximated pose and the target pose. Deserved
to be mentioned, the pose refinement subnetwork predicts a pose 
 for all
point cloud instead of poses for every points. This procedure can be applied
iteratively and generate potentially finer pose estimation each iteration.
Fig. 4 The architecture of pose refinement subnetwork
Loss Function. The loss function is define as the distance the target
point clouds and the current estimated point clouds. The estimated pose
 is obtained from the concatenation of the per-iteration
estimations:
(2)

where M is iterations, 
 is the result of pose clustering in the part of
pose estimation subnetwork. Given the ground truth pose 
, the
loss is computed as:
(3)
where N denotes the total number of per-pixel features and 
 is 3D point
on the object model at the corresponding pixel location.
4 Experiments
In this section, the experiments are discussed as below. We test our
approach on two datasets, the LINEMOD dataset and self-made industrial
parts dataset. The LINEMOD dataset is a widely-used dataset that allows to
compare with a broader range of the existing methods. In order to prove the
effectiveness of our method and reduce the cost of collecting datasets in
real-world scenes, a physically-simulated environment is constructed to
generate industrial parts dataset. The networks are trained on this
physically-simulated dataset and are tested on the images of real-world
scenes.
The proposed networks are implemented by PyTorch deep learning
library. The test images for industrial parts are captured from Intel
RealSense SR300 with size 480 × 640. The networks are trained on a
NVIDIA GeForce GTX 1080Ti. Each mini-batch has one image. The
learning rate is set to 0.001 for first 5 k iterations. After 5 k iterations, the
learning rate is set to 0.0001. The self-made industrial parts dataset for
training and validation is set to 9:1.
4.1 Datasets
LINEMOD [8] has become a de facto standard benchmark for 6D pose
estimation of texture-less objects in cluttered scenes. It contains 13 texture-
less objects with discriminative color, shape and size. Each object is
associated with a train and test image set showing one annotated object

instance with significant clutter but only mild occlusion. A full 3D model
representing the object is also provided. We compare our method with
existing methods on this dataset.
Self-made Industrial Parts Dataset is generated using physically-
simulated engine in order to reduce the cost of constructing dataset in real
application. The two industrial parts are shown in Fig. 5. The physically-
simulated environment is constructed on the basis of the Blender [23]
python API. The objects in the workspace are randomly pushed into the
simulation environment. By using mesh based collision detection technique,
the simulated pose distribution of objects is similar to the real-world scene.
The various rendered images are obtained when the camera and light are
variably configured in the physically-simulated environment. The
physically-simulated dataset is used for training networks only. We also
showcase its utility on the images of real-world scenes using the trained
networks.
Fig. 5 a The 3D models of two industrial parts. b The real-world image of two industrial parts. c
Generated RGB image using the physically-simulated environment. d Generated depth image
correspond with the RGB image
4.2 Evaluation Metrics
The average 3D distance of model points (referred to as ADD metric) [8] is
used for performance evaluation. The metric computes the average distance
between the 3D model points transformed using the estimated pose and the
ground truth pose. For symmetric objects, we use the closest point distance
in computing the average distance. An estimated pose is correct if the
average distance is within 10% of the 3D model diameter. For most objects,
this is approximately a 20 mm threshold but for smaller objects, the
threshold drops to about 10 mm.
4.3 Evaluation on LINEMOD Dataset

The comparison between our method and other methods is shown in
Table 1. The bold font means the highest recognition rate for each sequence.
It is worth mentioning that these methods is based on deep learning. As for
time efficiency, Wadim et al. [4] and Mahdi et al. [5] require a further pose
refinement step for improved accuracy. The classic refinement method is to
handle it by using ICP algorithm [7], which linearly increases the running
time. Our method needs around 100 ms to complete the whole pipeline of
6D pose estimation for objects on this dataset. It suggests that the proposed
method is characterized by low computation cost. We render the original
image with 3D model at the estimated 6D pose. The visualization results are
shown in Fig. 6.
Table 1 Comparison with other methods on the LINEMOD dataset
 
Wadim et al. [4] (%) Mahdi et al. [5] (%) Wang et al. [17] (%) Ours (%)
Ape
76.3
96.6
92.3
89.4
B.Vise
97.1
90.1
93.2
96.1
Camera
92.2
86.0
94.4
97.1
Can
93.1
91.2
93.1
90.0
Cat
89.3
98.8
96.5
94.0
Driller
97.8
80.9
87.0
93.0
Duck
80.0
92.2
92.3
96.2
Egg.B
93.6
91.0
99.8
100.0
Glue
76.3
92.3
100.0
100.0
Hole.P
71.6
95.3
92.1
90.5
Iron
98.2
84.8
97.0
97.9
Lamp
93.0
75.8
95.3
95.2
Phone
92.4
85.3
92.8
96.2
Average 88.5
89.3
94.3
95.1

Fig. 6 Proposed pose estimation method on LINEMOD dataset. The first and third column: original
images. The second and fourth column: images overlaid with colored 3D models at the estimated 6D
poses
4.4 Evaluation on Self-made Industrial Parts Dataset
In order to show the utility of the proposed approach, the experimental
results on industrial object are also presented. The industrial parts dataset is
generated using physically-simulated engine. There are about 1 k images
for the two objects. Each object features in about 3 k instances. After the
dataset generation and network training, the networks on the dataset and
real-world scene images are simultaneously evaluated.
In Table 2, we show the performance of our network after training and
evaluating on the self-made industrial parts dataset. There are about 700
images for each object. The metric computes the average distance between
the 3D model points transformed using the estimated pose and the ground
truth pose. The threshold is set by 10% of the 3D model diameter. The

visualization evaluation results on the self-made industrial parts dataset are
shown in Fig. 7.
Table 2 Evaluation results of industrial object dataset
 
Diameter (m) Threshold (m) Evaluation (%)
obj_01 0.1121
0.0112
98.1
obj_02 0.1373
0.0137
98.5
Fig. 7 a Histogram showing errors of our network when the network is trained with and without
pose refinement subnetwork. b and c The visualization evaluation results of obj_01 and obj_02 for
each images
The evaluation results on real-world scene images are shown in Fig. 8.
We overlie the original images with colored 3D models at the estimated 6D
poses. For better visibility, the background is kept in gray. The first column
shows several original test images. The second column shows the results of
pose estimation subnetwork. The third column shows the results of pose
refinement subnetwork. The last column shows the comparison results. The
red and green lines represent the silhouettes of the pose without and with
pose refinement, respectively. The evaluation results on industrial parts
dataset show that the trained network is effective.

Fig. 8 Proposed pose estimation method for industrial application. Original real-world scene images,
pose estimation subnetwork results, pose refinement subnetwork results and comparison results are
presented in each column
5 Conclusion
In this paper, an efficient method is proposed for 6D pose estimation of
texture-less objects with RGB-D images. In order to eliminate dependence
on texture, RGB-D images are fed into a pose refinement subnetwork to
predict an approximated pose. The pose refinement subnetwork of 3D point
clouds as input is used to iteratively refine the approximated pose by
matching the rendered point cloud against the observed point cloud. Two
experiments using the LINEMOD dataset and self-made industrial parts
dataset are conducted to verify the effectiveness of the proposed method for
pose estimation. The experimental results show the effectiveness of our
method on the LINEMOD dataset and its utility on self-made industrial
parts dataset. The proposed method performs best for five out of thirteen

objects, and has highest score for 95.1% average recognition rate on the
LINEMOD dataset. In addition, the method achieves 98.3% average
recognition rate on self-made industrial parts dataset. This mean that the
proposed method of estimating 6D pose of texture-less object can be used
in industry, such as bin picking and warehouse automation.
Acknowledgements
The insightful comments of the reviewers are cordially appreciated. This
research work is supported in part by National Natural Science Foundation
of China under grant No. 51775344.
References
1.
Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. International
Journal of Computer Vision, 60(2), 91-110.
[Crossref]
2.
Rublee, E., Rabaud, V., Konolige, K., et al. (2011). ORB: An efficient alternative to SIFT or
SURF. In IEEE International Conference on Computer Vision (pp. 2564-2571).
3.
Rothganger, F., Lazebnik, S., Schmid, C., et al. (2006). 3d object modeling and recognition using
local affine-invariant image descriptors and multi-view spatial constraints. International Journal
of Computer Vision, 66(3), 231-259.
[Crossref]
4.
Kehl, W., Manhardt, F., Tombari, F., et al. (2017). SSD-6D: Making RGB-based 3D detection
and 6D pose estimation great again. In IEEE International Conference on Computer Vision (pp.
1521-1529).
5.
Rad, M., & Lepetit, V. (2017). BB8: A scalable, accurate, robust to partial occlusion method for
predicting the 3D poses of challenging objects without using depth. In IEEE International
Conference on Computer Vision (pp. 3828-3836).
6.
Xiang, Y., Schmidt, T., & Narayanan, V., et al. (2017). Posecnn: A convolutional neural network
for 6d object pose estimation in cluttered scenes. arXiv preprint arXiv:​1711.​00199.
7.
Besl, P. J., & McKay, N. D. (1992). Method for registration of 3-D shapes. Sensor Fusion IV:
Control Paradigms and Data Structures., 1611, 586-606.
8.
Hinterstoisser, S., Lepetit, V., Ilic, S., et al. (2012). Model based training, detection and pose
estimation of texture-less 3d objects in heavily cluttered scenes. In Asian Conference on
Computer Vision (pp. 548-562).
9.
Liu, W., Anguelov, D., Erhan, D., et al. (2016). Ssd: Single shot multibox detector. In European
Conference on Computer Vision (pp. 21-37).

10.
Brachmann, E., Krull, A., Michel, F., et al. (2014). Learning 6d object pose estimation using 3d
object coordinates. In European Conference on Computer Vision (pp. 536-551).
11.
Choi, C., & Christensen, H. I. (2012). 3D textureless object detection and tracking: An edge-
based approach. In International Conference on Intelligent Robots and Systems (pp. 3877-3884).
12.
Choi, C., & Christensen, H. I. (2016). RGB-D object pose estimation in unstructured
environments. Robotics and Autonomous Systems, 75, 595-613.
[Crossref]
13.
Kehl, W., Milletari, F., Tombari, F., et al. (2016). Deep learning of local RGB-D patches for 3D
object detection and 6D pose estimation. In European Conference on Computer Vision (pp. 205-
220).
14.
Rios-Cabrera, R., & Tuytelaars, T. (2013). Discriminatively trained templates for 3d object
detection: A real time scalable approach. In IEEE International Conference on Computer Vision
(pp. 2048-2055).
15.
Tejani, A., Tang, D., Kouskouridas, R., et al. (2014). Latent-class hough forests for 3D object
detection and pose estimation. In European Conference on Computer Vision (pp. 462-477).
16.
Li, C., Bai, J., & Hager, G. D. (2018). A unified framework for multi-view multi-class object
pose estimation. In European Conference on Computer Vision (pp. 254-269).
17.
Wang, C., Xu, D., Zhu, Y., et al. (2019). Densefusion: 6d object pose estimation by iterative
dense fusion. In IEEE Conference on Computer Vision and Pattern Recognition (pp. 3343-
3352).
18.
Li, Y., Wang, G., Ji, X., et al. (2018). Deepim: Deep iterative matching for 6d pose estimation. In
European Conference on Computer Vision (pp. 683-698).
19.
Daniel, B., Zhou, C., Xiao, F., et al. (2019). Yolact: Real-time instance segmentation. arXiv
preprint arXiv:​1904.​02689.
20.
He, K., Zhang, X., Ren, S., et al. (2016). Deep residual learning for image recognition. In IEEE
Conference on Computer Vision and Pattern Recognition (pp. 770-778).
21.
Qi, C. R., Su, H., Mo, K., et al. (2017). Pointnet: Deep learning on point sets for 3d classification
and segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (pp. 652-
660).
22.
Drost, B., Ulrich, M., Navab, N., et al. (2010). Model globally, match locally: Efficient and
robust 3D object recognition. In IEEE Computer Society Conference on Computer Vision and
Pattern Recognition (pp. 998-1005).
23.
Blender. [online]. Available: https://​www.​blender.​org/​.
OceanofPDF.com

Agriculture Applications
When using machine vision to count fruit in situ, a major problem is the
occlusion of the fruit by other fruit and foliage. The first chapter in this part
attacks the problem with convolutional neural nets.
In great detail, the second chapter gives the design and construction of a
robot for assessing pasture biomass. LIDAR data is combined with location
by satellite in order to map the field.
The third chapter shows the practicality of implementing a visual
steering within a simple Android smartphone. An HTML page is linked
from which the reader can download all the code that is needed to
demonstrate the effectiveness.
The fourth chapter describes a robot for the collection in an arid
environment of a valuable resource—camel dung. Far from being a source
of amusement, the machine shows itself to be of great practical value.
An important component of the harvesting of lamb meat is the offal.
Special manipulators are needed for the soft tissue of the ovine intestines.
OceanofPDF.com

(1)
(2)
 
 
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_5
Improving Vision-Based Detection of
Fruits in a Camouflaged Environment
with Deep Neural Networks
Jinky G. Marcelo1, 2   , Joel P. Ilao1    and Macario O. Cordel II1  
De La Salle University, Manila, Philippines
Central Mindanao University, Maramag, Bukidnon, Philippines
 
Jinky G. Marcelo (Corresponding author)
Email: jinkymarcelo@cmu.edu.ph
Joel P. Ilao
Email: joel.ilao@dlsu.edu.ph
Macario O. Cordel II
Email: macario.cordel@dlsu.edu.ph
Keywords Fruit detection and counting - Camouflaged object detection -
Deep convolutional neural networks
1 Introduction
Detecting objects from a similarly colored environment has been an open
problem in computer vision. Camouflage or color similarity is one of the
major issues where the object gets occluded or merged when they are of
similar color with the environment resulting in a difficult object detection
[1]. One of the applications in agriculture where camouflage problem exists
is at detecting fruits or vegetables in a camouflaged environment for yield

estimation, e.g. the foreground contains green fruits or vegetables and the
background contains green foliage.
Harvest time is critical particularly for vegetables and fruits; thus, there
is a need to automatically detect and localize fruits in images for yield
estimation for proper planning in labor, market and transport arrangement
[2, 3]. Recent advances (e.g. [2, 3]) in computer vision has led to obtaining
fruit detection and counting from images; however, this area still faces
challenges because of illumination changes, scale variation, occlusion and
situations of camouflage. Camouflage situations refer to the blending of the
fruits to its foliage, stems and other objects of the environment which in
turn makes the fruit detection a difficult task.
Initial efforts on fruit detection and counting using deep neural networks
were presented in [2-6, 9]. Keresztes et al. [4] achieved an R2 correlation of
0.96 for 45 samples of grapes and 0.85 for 150 samples of apples between
the manual and automatic counting. Chen et al. [2] proposed fruit counting
based on a combination of two convolutional neural networks which
achieved an accuracy of 0.96 on 71 images with 7,200 oranges and 0.91 on
21 images with 1,749 apples. Rahnemoonfar and Sheppard [3] attained a
0.91 accuracy on 100 real images for automatic yield estimation using
synthetic training data. Fourie et al. [5] implemented a fruit detection and
localizer algorithm on 21 apple images with 442 objects resulting in 0.98
accuracy. Stein et al. [6] used a pre-trained detector and Lidar to efficiently
detect, track, count and localize every piece of fruit with an error of 0.014
in a total of 522 trees with 71,609 mangoes. Sa et al. [9] developed a real-
time fruit detector that can perform up to a 0.83 F1 score with a field farm
dataset comprising of at most 170 samples. All the mentioned prior works
have promising results; however, these systems were trained to detect
objects (e.g. oranges and tomatoes) with high color difference from the
leaves. This allows them to train their network with few samples. By
increasing the number of samples for the training dataset, deep learning
models can achieve better performance and generalization.
Though general object detection frameworks have brought remarkable
breakthroughs in detecting different types of objects [7, 8], the current
object detection algorithms fail in specific application scenarios like fruit
detection due to occlusion and color similarity between the objects and the
environment as illustrated in Fig. 1, last column. As shown in the last
column of Fig. 1, the model pre-trained on MS COCO dataset [10] fails to

detect any bell pepper or chili pepper. On the contrary, as illustrated in
Fig. 1, third column, our work performs object detection and counting on
fruits with heavy occlusion and high color similarity with its environment
by increasing the number of images and re-purposing a region-based
convolutional neural network. The contributions of this paper are:
Fig. 1 a Input image: input images with high occlusion and color similarity of the objects with the
environment (bell pepper or chili pepper). b Ground truth: each image contains groundtruth
rectangular bounding box around each object which identifies the xmin, ymin, xmax, ymax for the
object. c Ours: by re-purposing the region-based convolutional network to bell pepper and chili
pepper images, our proposed system performs well in detecting heavily-occluded and camouflaged
objects in a similarly-colored foreground and background. d Faster R-CNN: the result predicted by a
generic object detection system
(1) we built two datasets (Bell Pepper and Chili Pepper) for sweet pepper
detection. The sample images were taken from the sweet pepper field,
capturing the natural settings of the object. The datasets are composed
of 7700 images with 29,915 chili peppers and 3312 images with
14,548 bell peppers;
 
(2) we exhaustively tested various region-based convolutional neural
networks (Faster R-CNN Inception, Faster R-CNN Resnet50, Faster
RCNN Resnet101, R-FCN Resnet101) for a very challenging task of
detecting heavily occluded objects and highly-similar color of objects
with its environment. To our knowledge, this is also the first attempt
that a region-based convolutional neural network is used in
 

recognizing and localizing camouflaged objects for agricultural
applications.
2 Proposed System
Figure 2 shows the overall block diagram of the proposed system. The
network takes an image and outputs a set of objects with rectangular
bounding boxes and the probabilities associated with it.
Fig. 2 Block diagram of our proposed system. The proposed system leverages the strength of
region-based convolutional neural network for fruit detection in a camouflaged environment
2.1 Image Datasets
Sweet pepper was chosen as the sample dataset since it is considered as a
high-value crop in the Philippines. Two varieties of peppers (green bell
pepper and green chili pepper) were used, that were high in occlusion and
have high color similarity with the environment. The samples for the green
bell pepper dataset were collected during the day with no artificial lighting
in a greenhouse in Impasugong, Bukidnon, Philippines. A total of 552
images were taken for green bell pepper dataset. The samples for the green
chili pepper dataset were collected under an uncontrolled variability in
illumination since the climatic condition of the farm location was relatively
cold and humid in a cultivated farm in Lantapan, Bukidnon, Philippines. A
total of 2200 images were taken for green chili pepper dataset. The images
of both datasets were acquired of size 3008 × 2000 in jpeg format using a
Nikon D3200 24.2 MP digital SLR camera.

2.2 Data Augmentation and Annotation
To improve the performance and generalization of deep neural networks,
data augmentation was applied to the existing datasets. Data augmentation
was implemented by applying horizontal flipping, rotations, shears,
cropping and translation to existing datasets. These techniques ensure that
the model can work under multiple angles and different orientations. The
augmented images were added as additional samples to the training and test
sets. After data augmentation, there was a total of 3312 images from 552
naturally captured images of bell pepper and 7700 images from 2200
naturally captured images of chili pepper.
The regions of interest for ground truth annotations were drawn and
extracted using a labeler software [11]. As illustrated in the second column
of Fig. 1, a rectangular box was drawn around each object in each image
and these generated bounding boxes were exported into xml files in Pascal
Voc format which stores the coordinates of the bounding box of regions of
interest.
2.3 Implementation Details
The deep learning models were implemented using Tensorflow [14] by
leveraging on transfer learning of deep vision systems. To fully explore the
capability of CNNs in detecting and localizing fruit objects, four pre-trained
models from MS COCO dataset [10] were fine-tuned and evaluated for bell
pepper and chili pepper datasets. These pre-trained models include Faster
R-CNN Inception-v2 [7, 12], Faster R-CNN Resnet50 [7, 13], Faster R-
CNN Resnet101 [7, 13] and R-FCN Resnet101 [8, 13]. The pre-trained
model was downloaded from Tensorflow Object Detection API [14], which
is an open-source framework built on top of Tensorflow and trained on the
Microsoft COCO [10] dataset.
Table 1 shows a summary of the training and testing datasets. We fine-
tuned the modified object detector networks [7, 8] using our respective
datasets for green chili and green bell pepper, with momentum equal to 0.9
and an initial learning rate of 0.0003. The learning rate decreases by a factor
of 3 × 10−5 every 9 × 105 iterations. The learning rate was further reduced to
3 × 10−6 at 1.2 × 106 iterations. All the models were trained with a
momentum optimizer. We trained our system using NVIDIA GeForce
RTX2070. A network was trained separately for each dataset with a batch

size of 1. The network was trained for 6000 epochs and 20,000 epochs for
bell pepper and chili pepper dataset, respectively.
Table 1 Summary of training and testing datasets. The annotated bell pepper dataset and chili pepper
dataset were randomly split into two datasets: 70% for training and 30% for validation
Dataset
Bell peppers images objects Chili peppers images objects
Training (70%)
2318
10,141
5390
20,361
Validation (30%)
994
4407
2310
9554
Total images and objects 3312
14,548
7700
29,915
2.4 Evaluation Metrics
Object classification per category was evaluated using average precision
(AP). As shown in Eq. (1), the AP score is defined as the mean precision at
the set of 11 equally spaced recall values.
(1)
In order to evaluate the model on the task of object localization, we
determined how well the model predicted the location of the object. As
shown in Eq. (2), the localization task was evaluated based on the
thresholds of Intersection over Union (IoU). A threshold of 0.5 was set
which means that if the IoU exceeds the threshold, then the detection is
marked as correct detection. The model with the highest average precision
at 0.5 IoU was selected as the model for detecting and localizing
camouflaged fruits for inference to validation data.
(2)
3 Results
Table 2 and 3 present the result of the detection performance of four
different models and their corresponding training time to bell pepper dataset
and chili pepper dataset, respectively. Out of the four fine-tuned models,
Faster R-CNN Resnet101 exhibited the best performance for the bell pepper
dataset, yielding an AP of 0.966. For the chili pepper dataset, Faster R-CNN

Resnet101 also outperformed other models with an average precision of
0.922.
Table 2 Comparison of the results obtained by our proposed system employing different region-
based CNNs on the bell pepper dataset. Faster R-CNN Resnet101 (Bold text) outperforms other
models with AP@0.5 = 0.966
Method
Basenet
Training time Average precision
R-FCN
Resnet101 34 min
0.963
Faster R-CNN Resnet101 33 min
0.966
Faster R-CNN
Resnet50
23 min
0.964
Faster R-CNN
Inception
21 min
0.960
Table 3 Comparison of the results obtained by our proposed system employing different region-
based CNNs on the chili pepper dataset. Faster R-CNN Resnet101 (Bold text) outperforms other
models with AP@0.5 = 0.922
Method
Basenet
Training time Average precision
R-FCN
Resnet101 1 h 42 min
0.904
Faster R-CNN Resnet101 1 h 35 min
0.922
Faster R-CNN
Resnet50
1 h 5 min
0.917
Faster R-CNN
Inception
47 min
0.903
The features of Faster R-CNN Resnet101, a very deep network, were
sufficient in the transfer learning for the detection of bell peppers and chili
peppers. It is evident that the region proposal network contributed to higher
accuracy and efficiency. From this result, it can be concluded that the model
performs well in predicting the occurrence and position of the fruits in an
image amidst high levels of occlusion and even those highly-similar in
color between the fruits and the background.
After finishing the training, the model trained with the highest average
precision was selected as the best model and exported to a single file for
inference. The inference system's performance was measured using the
validation datasets for bell peppers and chili peppers. Figure 3 presents the
sample result of inference of our proposed system and a pre-trained Faster
R-CNN object detection system on images with high color similarity and
heavy occlusion. Despite the high degree of color similarity between the
fruits and the foliage, our proposed method can detect the fruits efficiently
and correctly. Also, the system correctly recognized and localized the fruits
even those fruits which are almost hidden due to heavy occlusion.

Fig. 3 Inference system on the highly-similarly colored environment. Our proposed system (3rd
column) substantially performed better than the generic region-based object detector (4th column) on
detecting bell peppers and chili peppers in a camouflaged environment
Figure 4 shows the visualization of feature maps after applying the
filters at the first and last convolutional layer in the Resnet101 model for
bell pepper and chili pepper input, respectively. It can be observed that the
result of applying filters in the first convolutional layer retains most of the
input image features. This means that there are many activations on the
edges and textures within the image. But as the network goes deeper into
the model, the feature maps become more sparse and visually less
interpretable. This implies that the filters abstract the features from the
image into more general concepts and convert it to the required output
classification domain.

Fig. 4 Visualization of feature maps to bell pepper and chili pepper input. a Input images (bell
pepper and chili pepper); b visualization image of feature maps of the first convolutional layer; c
visualization image of feature maps of the last convolutional layer
4 Conclusion and Future Work
We presented a system that automatically detects and localizes fruits from
images captured from the natural settings of the fruits. By increasing the
number of images and leveraging on the four pre-trained networks, the
evaluation results show that the fine-tuned model on Faster R-CNN
Resnet101 performed the best among all the models in detecting heavily-
occluded and camouflaged fruits. It yielded an average precision of 0.92 for
chili pepper and 0.96 for bell pepper. The inference shows that the fine-
tuned model on Faster R-CNN detected very well to heavy-occluded and
similarly-colored foreground and background bell pepper and chili pepper
images. This indicates that the trained fruit detection and counting model
can be integrated into applications for precision agriculture such as
automated fruit harvesting, yield estimation, and plant phenotyping.
One direction of future work is to integrate the trained fruit detector to
an unmanned ground vehicle. Moreover, it can also be extended to detect
other parts of a plant such as leaves, flowers, and stems which may be used
for plant phenotyping and plant pathology. The proposed system can still be
improved by extending its functions to more camouflaged images in
agriculture and other domains.
Acknowledgements
We would like to thank the farm owners and farmers for helping us in data
collection. The first author acknowledges the Commission on Higher
Education, in collaboration with De La Salle University and Central
Mindanao University for funding the scholarship grant.
References
1.
Ratthi, K., Iyshwarya. V. S., Yogameena, N. B, Menaka, K. (2017) Foreground segmentation
using motion vector for camouflaged surveillance scenario. In International Conference on
Wireless Communications, Signal Processing and Networking (WiSPNET) (pp. 172-176).
2.
Chen, S. W., Shivakumar, S. S., Dcunha, S., Das, J., Okon, E., Qu, C., et al. (2017). Counting
apples and oranges with deep learning: A data-driven approach. IEEE Robotics and Automation

Letters 2(2).
3.
Rahnemoonfar, M., & Sheppard, C. (2017). Deep count: Fruit counting based on deep simulated
learning. Sensors, 17(4), 905.
[Crossref]
4.
Keresztes, B., Abdelghafour, F., & Randriamanga, D. (2018) Real-time fruit detection using deep
neural networks. In Proceedings of the 14th International Conference on Precision Agriculture.
5.
Fourie, J., Hsiao, J., & Werner, A. (2017). Crop yield estimation using deep learning. In 7th
Asian-Australasian Conference on Precision Agriculture.
6.
Stein, M., Bargoti, S., & Underwood, J. (2016). Image based mango fruit detection. Sensors:
Localisation and Yield Estimation Using Multiple View Geometry.
7.
Ren, S., He, K., Girshick, R., & Sun, J. (2015). Faster R-CNN: Towards real-time object
detection with region proposal networks. In Advances in Neural Information Processing Systems
(pp. 91-99).
8.
Dai, J., Li, Y., He, K., & Sun, J. (2016). R-FCN: Object detection via region-based fully
convolutional networks. In Advances in Neural Information Processing Systems (pp. 379-387).
9.
Sa, I., Ge, Z., Dayoub, F., Upcroft, B., Perez, T., & McCool, C. (2016). Deepfruits: A fruit
detection system using deep neural networks. Sensors, 16(8), 1222.
[Crossref]
10.
Lin T. et al. (2014) Microsoft COCO: Common objects in context. In D. Fleet, T. Pajdla, B.
Schiele & T. Tuytelaars (Eds.), Computer Vision ECCV 2014. Lecture Notes in Computer
Science, vol: 8693. Cham: Springer.
11.
Tzutalin (2015). LabelImg. Git code. https://​github.​com/​tzutalin/​labelImg.
12.
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., & Wojna, Z. (2016). Rethinking the inception
architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (pp. 2818-2826).
13.
He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 770-
778).
14.
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., ... & Ghemawat, S. (2016).
Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint
arXiv:​1603.​04467.
OceanofPDF.com

(1)
(2)
(3)
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_6
Mechatronics for a LiDAR-Based Mobile
Robotic Platform for Pasture Biomass
Measurement
M. Sharifi1   , S. Sevier1, H. Zhang1, R. Wood1, B. Jessep1, S. Gebbie1,
K. Irie3, M. Hagedorn3, B. Barret2 and K. Ghamkhar2
Development Engineering, Lincoln Research Centre, AgResearch,
Lincoln, New Zealand
Forage Science, Grasslands Research Centre, AgResearch, Palmerston
North, New Zealand
Red Fern Solutions Ltd, Christchurch, New Zealand
 
M. Sharifi
Email: mostafa.sharifi@agresearch.co.nz
Keywords Biomass - Pasture - LiDAR - CAD - CAM - RTK-GNSS -
Mechatronics design
1 Introduction
With the world's growing population and the significance of sustainable
agriculture, agricultural productivity, and food supply, digital and precision
agriculture is at the top of the priority list for speeding up the efficiency of
agricultural productivity. In New Zealand, pastoral sector is the largest
export contributor, where dairy exports generate $14b and meat export
generates $5b per annum [1]. To evaluate agricultural productivity, it is
important to accurately and consistently measure yield in pasture plants.
This trait can vary depending on different factors such as soil, environment,
fertilization, cultivation time, etc. Pasture yield can be more specifically

measured using fresh weight (FW) and dry weight (DW), which increases
the efficiency of methods such as genomic selection, thus predicting forage
productivity with minimal cost [2].
The conventional method used to estimate dry matter yield (DMY) in
plots of forage grasses involves destructive mechanical harvesting of a
known area, drying the harvested material, weighing the dry material and
then using the data to calculate the dry weight per area harvested. Fresh
weight can be estimated using visual scoring by an experienced plant
breeder. These conventional methods limit plant breeders to effectively (in
terms of both cost and accuracy) assess DMY and FW in large scale
capacities, as well as precluded grazing which is critical for selecting and
evaluating the pasture plants. Furthermore, the ability to measure pasture
growth rate at high spatio-temporal resolution is indeed constrained by
current technologies in pasture plant breeding, despite its economic value
and impact in the selection process [3, 4].
Plant phenomics or the use of sensors and digital technologies for
measuring traits in plants has enabled efficient and reliable phenotyping of
many plant traits. Mobile, in-field robotic platforms equipped with sensors
and computational power can assist plant breeders and other researchers to
conduct the required high-throughput plant phenotyping. [5-7].
Light detection and ranging (LiDAR) sensing technology, measures
distance to a target by emitting the target with frequent impulsive laser
signals and simultaneously capturing those reflected laser signals. Unlike
traditional cameras, LiDAR scanners directly capture distance and
distribution data [8]. LiDAR is a preferred tool for characterisation of plant
biophysical [9, 10] and detailed physical characteristics in the field [11, 12].
Ground-based LiDAR systems have shown promising results in the
estimation of biomass in many types of crops as well as pasture [13]. With
the successful application of ground-based mobile robotics in agriculture to
enhance precision and productivity, our objective in this paper is that an
improved LiDAR-based mobile robotic platform based on an early
prototype could offer increased efficiency in forage yield measurement.
Here we present the mechatronic design and development of a LiDAR-
based mobile robotic platform for rapid and accurate measurement of
pasture biomass. The mechatronic process including system design
specification, mechanical design and 3D CAD modelling, electrical and
control system architecture, alongside control software architecture is

presented. This is followed by a brief review of field experimental results
and the overall performance of the system.
2 Methodology
In this section, the integrated mechatronic design and development process
of the LiDAR-based robotic platform has been presented.
2.1 Design Overview and System Specification
Following a consultation meeting with the end-users, the specifications of
the robotic platform were identified as below:
Be a lightweight and robust platform, all electrically powered, for the
purpose of carrying LiDAR to measure pasture biomass of grass plots.
Be equipped with a high accuracy real-time kinematic (RTK) Global
Navigation Satellite System (GNSS) to map measured pasture biomass of
grass plots.
Be able to be operated and controlled remotely through a wireless
controller, monitored through a graphical user interface which is
handheld by a nearby operator.
Be transportable on a trailer from its base (workshop) to the field, with
maximum incline of 1 in 3 for loading/unloading and maximum incline
of 1 in 20 during pasture biomass scanning operation.
Be suitable for environmental conditions including weatherproofing and
all types of agricultural terrain.
Be easy to clean using gentle water pressure followed by drying and
spraying with disinfectant.
The main chassis was designed to have no suspension (rigid frame), and
600 mm minimum ground clearance to minimise disturbance of pasture
plots. The wheel centres width across the platform were fixed at 1400 mm,
and wheel centres from front to back wheels were 1250 mm. To accurately
measure pasture biomass, accurate ground height measurement (from
ground to LiDAR) was needed, thus an additional measurement mechanism
was included. This mechanism included two 'floating' wheels to detect, and
record LiDAR to ground variations, these were placed on each side of the
chassis. The LiDAR unit was positioned in between the front and rear

wheels at a height of 1500 mm and was facing down to the grass plots
vertically. In this position it is able to scan grass plots up to 1400 mm wide.
To provide a simple, reliable, and manoeuvrable drive for precision
operation in the pasture plots the wheeled robotic platform was designed to
encompass a differential steering drive system. This was provided by the
two rear wheels while the two front wheels are castering. Each of the
driving wheels was equipped with a geared high torque brushless DC
(BLDC) motor, electrical braking system for greater safety, and encoder
feedbacks for closed loop speed control. Inflated tyres were utilized for all
wheels to provide a softer ride while minimising height variations of
LiDAR relative to the ground.
All the electrical and electronic components were graded with a
minimum of IP54 protection class (protected against dust that could
interfere with operation of the equipment and protected from splashing
water from all directions), but ideally IP65 (protection against dust and
protection from jets of water from all directions). The platform included a
stand-alone industrial embedded control system to control the operation
modes and drive, this was interfaced with a central high-level computing
system to carry out the LiDAR measurement and had a graphical user
interface. Three operation modes were considered including: high torque
low speed (up to1 km/h) for loading/unloading from the trailer, fast speed
mode (up to 6 km/h) for any other non-scanning movements, and scan-
mode with a constant speed of 3 km/h. The ground height measurement
system was designed to be engaged during scanning mode, and disengaged
during the other two modes.
2.2 3D CAD Mechanical Design and Manufacturing
The robotic platform was designed using Autodesk Inventor Professional
2019 running on Windows 10. Initially, a concept design was modelled in
CAD, which enabled further refinement of the specification and design.
Once final design was complete, CAD data was leveraged to enable
efficient manufacture using modern methods, including CNC cutting,
folding and machining. 3D printing was also used for rapid prototyping of
small components.
This design included all the parts that needed to be manufactured and
parts supplied off the shelf. 3D CAD modelling enabled agile and improved
design and also expedited rapid prototyping and manufacture of mechanical

parts. It also enabled further computer aided engineering (CAE) and
analysis of the mechanical parts and driving joints in terms of structural and
motion analysis. Figure 1 shows an overview of the 3D CAD model of the
robotic platform including the height measurement mechanism.
Fig. 1 3D CAD model of the robotic platform with all componentry: (1) chassis, (2) rear driving
wheels, (3) front castering wheels, (4) ground height measurement system, (5) control system and
electronics, (6) RTK-GNSS receiver, (7) LiDAR, (8) Battery, (9) Height detection reflector plate
As mentioned earlier, the 3D CAD model enabled rapid prototyping and
manufacturing of the robotic platform through computer-aided
manufacturing (CAM). In the design process, it was also aimed to design

the robotic platform by considering factors such as modularity and
manufacturability. The robot chassis was manufactured through laser
cutting and CNC machining. Figure 2 depicts the manufactured robot
chassisand as an example.
Fig. 2 Manufactured chassis of the robotic platform
2.3 Electrical, Control, and Sensing Systems Architecture
The electrical and control system architecture on this robotic platform was
comprised of a low-level control system interfaced with a high-level
computing processor. The low-level control system encompassed an
embedded brushless DC (BLDC) motor speed controller and is responsible
for controlling platform operation modes, receiving speed commands and
speed feedbacks while controlling and providing Pulse-Width-Modulation

(PWM) power to both BLDC drive motors in a closed-loop PID control
system. The high-level computing process system encompassed a touch
screen industrial computer, that is interfaced to the embedded speed
controller, RTK-GNSS receiver, and LiDAR. This high-level processing
system runs software with a graphical user interface. This software is
responsible for collecting and processing the LiDAR, RTK-GNSS geo-
location information, and speed feedback. The algorithms embedded in this
software generate both pasture biomass estimation and a scanning map.
Figure 3 presents the electrical and control system architecture including
both low level and high-level systems and attached sensors.
Fig. 3 Electrical, control, and sensing system architecture of the robotic platform
2.4 Software System Architecture
The software system is composed of both low and high-level processing
systems. The low-level processing system, the embedded speed controller,
accommodates the required software for the robotic platform motion and
operation modes control. A kinematic based model responsible for
differential driving system, receives the input linear and angular velocity
commands, calculates the required power for each of the driving motors,

and controls the platform with the desired speed through inverse
kinematics. The controller also provides different operation modes (slow,
fast, and scan) with set speeds through user input. In the scan mode, the
height measurement system is engaged with the ground (through an active
linear actuator), while it is disengaged in both slow and fast operation
mode.
The high-level software comprises real-time data streaming and
recording from the LiDAR, RTK-GNSS receiver, and motor-drive software,
along with in-field data analysis to calculate biomass (post-capture).
3 Experimental Results and Discussions
Several in-lab and in-field experiments were conducted to evaluate the
performance of the platform in different operation modes and conditions.
When in trailer loading and unloading operation mode (slow mode) the
platform could reliably drive a ramp with the expected incline angle of 1 in
3 (~18°). This was evaluated through several ramp tests inside Engineering
Development Laboratory (EDL), AgResearch Ltd. The experimental results
for this operation mode showed that the drawn current by each of the
driving BLDC motors were within the expected and safe limits. Figure 4
shows the ramp test setup and the plotted electrical currents drawn by the
driving motors and from the two batteries during the experiment.

Fig. 4 The ramp experiment using the slow mode control: a the platform and the test ramp setup, b
the drawn electrical current from driving motors and batteries
In-field experiments were carried out for both fast and scanning
operation modes to test the low-level control system and driving
performance of the platform. In the scanning mode, the platform is expected
to scan the pasture plots on a direct path with minimal steering requirement.
Thus, the steering rate was limited to a maximum of 10 degrees to protect
the ground height measurement system from unwanted damage due to sharp
turns (or spot turns). Figure 5 shows the completed platform during the in-
field experiments.

Fig. 5 Fully developed LiDAR based robotic platform during the in-field experiments
In-field experiments were also carried out in scan-mode to evaluate and
validate the performance of the pasture biomass measurement system
through LiDAR data and the high-level software processing system. The
developed algorithms within the high-level software system, automatically
segment the scanned LiDAR data from a sequence of raw measures into
volumetric estimates for multiple rows of pasture plots. Ground height can
vary substantially between the scans. Thus, the ground height is also
calculated at each sequence of the LiDAR scan through measuring the
vertical variations of the reflector plate of the ground height measurement
mechanism. Through further processing, pasture biomass is estimated using
volumetric data from the segments. Figure 6 shows example LiDAR 3D
data of a single row of perennial ryegrass achieved from the high-level
processing system. From the obtained LiDAR volumetric data, pasture fresh
weight (FW) and dry weight (DW) were estimated. Finally, the percentage
dry matter yield (%DMY) was calculated from the two indicators as the
main pasture yield indictor.

Fig. 6 Example of LiDAR volumetric data (Top-down view) of a single row of perennial ryegrass
pasture [13]
4 Conclusion
A LiDAR-based mobile robotic platform for non-destructive and rapid
measurement of pasture biomass was developed. An integrated mechatronic
design and development process was incorporated to develop the mobile
robotic platform including mechanical, electrical, electronic, sensors, and
software system. A low-level software system was responsible for motion
control and driving the robotic platform, while a high level software system
was responsible for carrying out the integration of LiDAR measurement,
GNSS-RTK receiver data, and the odometry from the low level control
system. The integrated data from different sources were processed in real-
time to generate LiDAR 3D volumetric data. The LiDAR volumetric data
was then further processed to estimate pasture FW, DW, and DMY. Early
results show that this integrated approach provides a precise, non-
destructive, and cost-effective way for real-time in-filed measurement of
pasture yield with highly anticipated scientific and commercial benefits.
Acknowledgements
We acknowledge technical support from staff at PGG Wrightson's Seeds
and New Zealand Agriseeds. Technical staff at AgResearch: Craig
Anderson, Angus Heslop, Anthony Hilditch, Peter Moran, and Jana Schmidt

are highly appreciated for their input in designing the machine. The
research to develop the LiDAR, electronics, and mechanics of the system
was funded by Pastoral Genomics, a joint venture co-funded by DairyNZ,
Beef+Lamb New Zealand, Dairy Australia, AgResearch Ltd, New Zealand
Agriseeds Ltd, Grasslands Innovation Ltd, and the Ministry of Business,
Innovation and Employment (New Zealand).
References
1.
Ministry for Primary Industry, New Zealand. (2019). https://​www.​mpi.​govt.​nz/​exporting.
2.
Pollock, C. J., Abberton, M. T., & Humphreys, M. O. (2005). Grass and forage improvement:
Temperate forages. Grassland: a Global Resource 57-68.
3.
Cayley, J. W. D., & Hannah, M. C. (1995). Response to phosphorus fertilizer compared under
grazing and mowing. Australian Journal of Agricultural Research, 46(8), 1601-1619.
[Crossref]
4.
McNaughton, S. J., Milchunas, D. G., & Frank, D. A. (1996). How can net primary productivity
be measured in grazing ecosystems? Ecology, 77(3), 974-977.
[Crossref]
5.
Lingfeng, D., et al. (2011). A novel machine-vision-based facility for the automatic evaluation of
yield-related traits in rice. Plant Methods, 7(1), 44.
6.
Wang, L., et al. (2014). Estimation of leaf biochemical content using a novel hyperspectral full-
waveform LiDAR system. Remote Sensing Letters, 5(8), 693-702.
7.
Fernando, S., et al. (2008). Active sensor reflectance measurements of corn nitrogen status and
yield potential. Agronomy Journal, 100(3), 571-579.
8.
Molebny, V., Kamerman, G., & Ove, S. (2010). Laser radar: from early history to new trends. In
Electro-Optical Remote Sensing, Photonic Technologies, and Applications IV. (vol. 7835).
International Society for Optics and Photonics.
9.
Holmgren, J., Nilsson, M., & Olsson, H. (2003). Estimation of tree height and stem volume on
plots using airborne laser scanning. Forest Science, 49(3), 419-428.
10.
Næsset, E. (2002). Predicting forest stand characteristics with airborne scanning laser using a
practical two-stage procedure and field data. Remote Sensing of Environment, 80(1), 88-99.
[Crossref]
11.
Harding, D. J., et al. (2001). Laser altimeter canopy height profiles: Methods and validation for
closed-canopy, broadleaf forests. Remote Sensing of Environment, 76(3), 283-297.
[Crossref]
12.
Lovell, J. L., et al. (2003). Using airborne and ground-based ranging LiDAR to measure canopy
structure in Australian forests. Canadian Journal of Remote Sensing, 29(5), 607-622.

[Crossref]
13.
Ghamkhar, K., et al. (2018). Using LIDAR for forage yield measurement of perennial ryegrass
(Lolium perenne L.) field plots. In Breeding grasses and protein crops in the era of genomics.
Cham: Springer, pp. 203-208.
OceanofPDF.com

(1)
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_7
Vision Guidance with a Smart-Phone
John Billingsley1  
University of Southern Queensland, Toowoomba, QLD4350, Australia
 
John Billingsley
Email: john.billingsley@usq.edu.au
Keywords Vision guidance - Agriculture - Tractor - JavaScript
1 Introduction
A paper describing the development of a vision-guidance strategy for farm
vehicles has been cited over a hundred times, including six citations within
the last two years. This simple strategy has now been encapsulated in the
form of JavaScript code suitable for a smart-phone or tablet and is
demonstrated in action. A URL is given from which the source can be
downloaded for further research and development.
The vision processing that exploits the camera of a smartphone can be
complemented by the inbuilt sensors of GPS and three-axes of acceleration.
These give all that is required for low-cost guidance in a variety of
situations. With the addition of simple Bluetooth communication to a
steering module, this means that precision automatic guidance along a row
crop can now be applied to the smallest of mobile devices at a budget cost.
A goal of agricultural robotics has been the development of autonomy
for farming vehicles. With autonomy comes a relaxation of the requirement
that the machine must be large to optimize the effort of a human driver. It is
suggested that on safety grounds small autonomous vehicles would be more
acceptable than large ones.
The software outlined here is still in an early stage of development, but
the implementation is already such that it can be demonstrated in the form

of a simple HTML web page, supported by JavaScript code. This was
transplanted almost untouched from the C++ of the original application.
The working software can be found at the web location http://​www.​
essdyn.​com/​rowfit.​htm [1] from which the source can be saved for further
research and development.
2 Background and History
The original 1990s project used a camera that was part of a camcorder,
webcams were not yet common. The computer used was a PC in a tower
case, since there were no tablets or laptops. The display was a bulky
monochrome CRT. After an industry sponsored launch that was a
commercial failure, the project was relaunched some years later
Recent developments in HTML5 mean that video data can be accessed
from the camera with ease, allowing software written in JavaScript to be
run by common browsers in the form of a web page.
It was not difficult to take the original C++ code and knit it into a
JavaScript environment. This runs at an adequate speed to analyse the video
frames at the full frame rate.
3 Evolution of the Central Algorithm
The first vision guidance project really commenced before the founding of
USQ's National Centre for Engineering in Agriculture, where the later work
was based. A Masters student, Murray Schoenfisch, gave a presentation that
concerned the use of buried cable for sensing a steering error. It was clear
from his slides that rows of crop could readily be discerned, so the
emphasis turned to machine vision [2].
The first step was to record some video footage from a car that was
driven somewhat erratically along the rows. The video footage was
captured to a PC using a primitive home-grown frame-grabber. This was
only capable of capturing a binary image at relatively low resolution.
The later paper describing the strategy [3] receives citations to this day.
Recent examples such as Zhang et al. [4] and García-Santillánet al. [5]
involve substantial computation such as clustering algorithms to achieve
their objective. In contrast the early strategy bypassed many of the

conventional 'tools' of image filtering. In consequence it has led to
algorithms that can readily be migrated to other technologies.
A feature common to many image analysis techniques is the attribution
of pixels to membership of a set. In this case, the sets are "plant" or "soil"
and when accessed, the pixels were tagged in this conventional way. In the
early days, discrimination was by means of a brightness threshold and then,
when colour capture became possible, by inspection of the chrominance
component of a YUV stream.
An innovation was in automatic threshold control. A 'farmer's guess'
was entered to signify the proportion of the image that should be "plant",
based on the state of the crop. When lighting conditions changed, the
discrimination threshold was automatically varied until the proportion of
"plant" pixels in the portholes of interest reached that level.
Having divided the pixels into two sets, a 'toolbox' technique would
have been to apply a filter that identified pixels on the boundary between
the regions, seeking to define the shape and location of a crop row. In
emerging or weedy conditions, no such connected or compact regions might
exist.
Instead, the strategy treated "plant" pixels as simple data points, through
which regression lines could be drawn to identify any formation into rows.
The parameters defining these lines then determine the lateral displacement
and heading of the vehicle. The heading relative to the rows is given by
movement of the vanishing point, while lateral displacement is given by the
deviation of the mean slope from the vertical. The calculation also included
a quality figure to indicate the degree of discernment and reliability.
To fit such lines, it was necessary to ensure that the processed pixels
were members of a single row at a time. 'Keyholes' were selected that
should each contain only one row. When the parameters of the fitted line
were extracted, they defined corrections to the keyhole position and slope
for use in the following image frame, Thus the keyholes could be moved
rapidly from frame to frame to track the rows, while the hardware could
then take its time to steer the vehicle and bring the set of keyholes back to
their datum position (Fig. 1).

Fig. 1 An early example of 'keyholes' fitted to the rows
With values for displacement and heading, a control strategy was able to
demand a heading change proportional to the lateral error. This was limited
in magnitude. The difference between observed and demanded heading then
generated a steering demand, also limited in magnitude.
4 Methodology: Bringing the Implementation up
to Date
With the advent of HTML5 and Canvas, the programming task has become
much easier. The original rowfit.cpp code can be simplified and dropped
straight into a JavaScript file rowfit.js. The same code calculates the
parameters and quality of the regression fit.
The video stream is now accessed by invoking [6].
navigator.mediaDevices.getUserMedia(constraints)
This first asks the user for permission to access the camera, then
presents the frames as an array data that is a property of a variable that has
been equated to readFrame().
This data differs from the format of DirectX data, in that there are four
bytes per pixel, not three. The fourth byte controls opacity, allowing an
underlying image to show through if the opacity is not set at 255. Also, the
colour bytes are in the reverse order.

The picbit(x, y) function is still used, returning 0 or 1. It is now
calculated as the difference between the green and red components of the
pixel when compared against a threshold. This threshold is controlled just
as before to maintain the proportion of marked pixels to be the ratio entered
during setup.
As before, the quality is assessed in terms of the 'moment of inertia' of
the plant pixels about the fitted line. If the keyhole bridges a neighbouring
row, that moment will be large and the quality will be low. If the quality for
both rows is persistently of an unacceptable standard, the vanishing point
and slope both decay to their datum values.
The Canvas environment gives a further advantage. A mouse or finger
drag can be used to set up the datum parameters.
The vanishing point and horizon can be dragged to match the image on
the screen by dragging above the top of the stripes.
By dragging horizontally below the bottom of the stripes, the datum tilt
can be set to compensate for any lateral offset of the camera.
By dragging in the lower half of the stripes, the angular separation of the
keyholes can be set to match the rows in the image.
In the upper half of the stripes, the width of the keyholes can be adjusted.
To gain access to the 'promise' of the 'mediaStream', the web page
must either be stored on the user's own machine or be at an address that is
reached by a secure 'https' call. A demonstration page can be found at
http://​www.​essdyn.​com/​rowfit.​htm. Since this page is not secure, you must
first save it as a 'web page complete' and then open that file with your
browser. In the process, you will be able to see all the finer points and
shortcomings of the software by examining the source.
5 Further Work
Although this code demonstrates the ability to generate steering signals, the
Bluetooth communication protocol must be devised, including all necessary
safety features.
The present GPS signals available to low cost devices are not yet of a
quality that will allow precision steering, though they should suffice for
headland turns. With the escalation of constellations of satellites, this will

be remedied in a very short time, allowing vision and GPS to work in
mutually-supporting partnership.
Tracking the intersection of the fitted lines would allow the horizon to
be tracked in undulating ground. By tracking their angular split, precise
altitude could be measured of a drone being flown for crop inspection or
selective spraying.
This is a technology that is easily accessible and should stimulate a host
of further research projects.
6 Conclusions
The release of the algorithm in this form has many interesting implications.
It makes vision guidance accessible to anybody who wishes to exploit it,
though it is sincerely hoped that they will attribute its origin.
Some tractor manufacturers are known for the difficulty of interfacing a
new sensor with their system. They use encryption to preserve exclusivity.
Could the attraction of simple 'apps' such as this open up the way for
farmers to use their own ingenuity to upgrade their machinery?
In the consumer field, the concept of computer as entertainer has spun
off a dazzling array of products and services. In this flood of products and
technologies there are many that can be exploited for solving 'real'
engineering problems, such as those that have so long confronted the
farmer. As these new opportunities are seen to emerge, new problems are
brought to light that will require new strategies for their solution.
References
1. Billingsley, J. (2019). Rowfit demonstration of row tracking, seen at. http://​www.​essdyn.​com/​
rowfit.​htm.
2.
Billingsley, J., & Schoenfisch, M. (1995). Vision and Mechatronics Applications at the NCEA. In
Fourth IARP workshop on Robotics in Agriculture and the Food Industry, Toulouse.
3.
Billingsley, J., & Schoenfisch, M. (1997) The successful development of a vision guidance system
for agriculture. In Computers and Electronics in Agriculture (journal). Amsterdam Netherlands:
Elsevier, pp. 147-163.
4.
Zhang, X., et al. (2018). Automated robust crop-row detection in maize fields based on position
clustering algorithm and shortest path method. In Computers and Electronics in Agriculture vol.
154, pp. 165-175.

5.
García-Santillán, I., Guerrero, J. M., Montalvo, M., et al. (2018). Curved and straight crop row
detection by accumulation of green pixels from images in maize fields. Precision Agriculture,
19(1), 18-41.
[Crossref]
6.
MDN (many contributors). (2019). MediaDevices.getUserMedia() In Mozilla Develpers Network,
viewed at. https://​developer.​mozilla.​org/​en-US/​docs/​Web/​API/​MediaDevices/​getUserMedia.
OceanofPDF.com

(1)
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_8
A High-Speed Camel Dung Collection
Machine
Samuel N. Cubero1   , Mohammad Badi1, Mohamed Al Ali1 and
Mohammed Alshehhi1
Department of Mechanical Engineering, Khalifa University, PO Box
2533, Abu Dhabi, UAE
 
Samuel N. Cubero
Email: samuel.cubero@ku.ac.ae
Email: scubero@live.com
URL: https://www.samcubero.com
Keywords Dung - Manure - Litter - Collecting - Animal - Camel - Horse
- Mechatronic - Machine - Conveyor - Rake
1 Introduction
Animal dung (manure or droppings) from horses, bovines (cattle) and
camels are rich in bacteria and nutrients. They are manually collected and
used to enrich soils and fertilize farm crops, plants and gardens. Large
beaches are often strewn with small bottles, empty cans, and other forms of
litter that need to be collected regularly and disposed of to maintain public
safety and cleanliness. In the UAE (United Arab Emirates, a small oil-rich
country beside Saudi Arabia that boasts the world's tallest skyscraper in
Dubai), temperatures often reach as high as 50 °C during the summer. At
present, collecting camel dung, horse manure and litter on a large scale is
considered to be exhausting and tedious work, and is usually done
manually, using shovels, wheelbarrows and small hand tools. This paper
briefly describes the 'prior art', or existing machines designed for collecting

animal dung and small litter, including their advantages and disadvantages.
A practical 'mechatronic' design project is described, leading to the creation
of a working prototype for a machine that shows good potential for use as a
litter and dung collector on sandy ground and beaches. It was developed at
Khalifa University, Abu Dhabi, during mid-2018 to mid-2019, at the
Department of Mechanical Engineering. It is a manually-steered self-
propelled vehicle that—with a few modifications and improvements—could
become a highly reliable dung collector which can be successfully mass-
produced and marketed worldwide in the very near future. Modeling and
engineering analysis for this machine's front-wheel-drive motor and the
rake conveyor motor are also described in detail. Several performance
problems were experienced and their potential solutions and remedies are
briefly discussed.
1.1 Different Types of Animal Dung
The manure (or dung) usually produced by horses and camels is quite
similar in size, shape and texture, and is typically shaped in the form of
round balls. In contrast, the manure produced by cows contains more
moisture and typically appears formless, similar to a slurry or a sticky wet
mud. The dung shown in Fig. 1a, b are typical for camels and horses,
respectively, and they appear to be much dryer and possess lower density
(or mass per unit volume) compared to the cow dung shown in Fig. 1c.
Fig. 1 Typical a camel dung, b horse dung, and c cow or bull dung
It is important to note that teeth on a rake would not be very effective at
collecting wet cow dung, but a rake can collect horse and camel dung balls
that are larger than the gaps between the rake's teeth. A moving scraper, a
fan blade or a solid paddle (with no holes or gaps) would be able to collect
all types of dung. Unfortunately, such a collection mechanism would also
collect unwanted sand which may take up a lot of room in the collection bin

or storage area for the dung. Cow dung collection requires a mechanism
that can handle the sticky slurry-like dung while avoiding the collection of
too much sand and dirt. In this project, our priority was to focus mainly on
collecting camel and horse dung at high speed (or at least, faster than an
average human worker).
1.2 Prior Art in High-Speed Litter Collection Machines
This section briefly examines the features, pros and cons of some patented
designs and commercially available litter cleaners—machines that could
potentially collect horse and camel dung effectively, especially off sandy or
desert ground. The different kinds of litter collection machine designs
investigated fall into the following 4 categories:
1. Rotating blade or screw collection machines 
2. Rake conveyor machines
 
3. Vibrating filter machines
 
4. Vacuum-type litter collection machines.
 
1.2.1 Rotating Blade or Screw Collection Machines
Hansson [1] patented a 'Manure removing machine' in 1978 that uses a
rotating feeder screw to scrape and transfer manure off the ground. The
mass flowrate of manure that can be removed by this design would only be
very small due to the small contact area between the screw blades and the
ground. The circular or cylindrical shape of the feeder screw means that it
can only touch the ground at one outside edge, and the axis of rotation
points in the forwards direction of driving for the vehicle, therefore, only a
small area of manure can be removed off the ground in one pass of the
vehicle. This design could be improved if one or two feeder screws have
their axis (or axes) of rotation(s) oriented orthogonal to the forwards
direction of driving for the vehicle to increase contact area of the feed-
screw with the ground. Unfortunately, this design does not seem like it
would have a high manure collection rate. The feed-screws would also be
very expensive to manufacture due to their complex geometries, and the
entire machine requires a large truck or vehicle to tow it.

Vinyard [2] patented a 'Rear-mounted manure gathering machine' in
1994. This device uses rotating blades (resembling several paddle wheels,
stacked one on top of each other) to scrape and lift (or fling) manure off the
ground to higher levels of rotating fan blades, which transports the manure
to an elevated tank at the rear of the machine. Unfortunately, this design
would also pick up a lot of sand and dirt, along with the manure. Sand and
fine particulates can be sifted out and prevented from entering a collection
bin or container using appropriate filters. The entire machine is towed
behind a tractor.
The video shown in [3] shows how dry and wet cow manure (or cow
patties) can be collected effectively using a conventional 'ride-on' mower
for cutting grass. However, the video shows that some grass is also
collected. If this 'rotary mower' type dung collection method is used on a
beach or in a sandy desert, the rotating blades will collect a large amount of
sand, along with any animal dung or litter, thus wasting much space in the
collection bin. Although very simple and useful for collecting cow dung on
grassy pastures, this method needs some way to filter out or separate the
sand from the dung.
A video demonstrating a hand-powered 'cow dung collecting machine'
is shown at [4]. This hand-powered cow manure collecting machine seems
to experience mechanism seizure or jamming if there is too much dung at
the bottom of the ramp, because one of the rotating blades on the chain
conveyor must compress the dung beneath it at the front of the ramp,
requiring very high torque. This problem could be avoided if the conveyor
wheel (sprocket) radius was much larger, or the radial length of the scraper
blades was made much longer (to avoid compressing too much manure).
The solid blade scraper design seems to be effective at collecting moist cow
dung which cannot be easily collected by teeth on a rake (because wet dung
can easily pass through the gaps between rake teeth). Just like other
scraping blade designs, some sand and soil would also be collected along
with the manure, and these need to be filtered out.
1.2.2 Rake Conveyor Machines
The 'Barber Surf Rake' is perhaps the most popular type of design for
collecting small pieces of litter on sandy beaches because rakes tend to act
like filters or sieves and do not collect much sand. Each rake only collects
large pieces of litter that cannot fit between the gaps of the rake teeth. A

demonstration video [5] shows this machine being towed behind a large
tractor, collecting litter very reliably at a fairly high mass flowrate. The
'Surf rake' uses a 'chain conveyor' to move rakes up a steep incline, to
transport litter from the ground up to the top of the collection bin behind the
conveyor. This machine can also automatically raise and rotate its large
collection bin, so it can empty all its contents into a larger collection bucket
or garbage bin (or dumpster), and finally return its collection bin to its
original position behind the conveyor. This action is similar to how
contemporary garbage trucks use hydraulic-powered manipulators to empty
large dumpsters and return them to their starting positions (Fig. 2).
Fig. 2 The Barber 'Surf Rake' litter collecting machine.
Source www.​hbarber.​com
A similar version of the 'Surf Rake' machine, called the 'Litter picker'
[6], is widely used for collecting small litter in grassy parks. The Barber
'Turf Rake' machine is also very similar to the 'Surf Rake' but is used for
picking up stones. Demonstration videos of these commercially available
machines can be found at www.​hbarber.​com.
1.2.3 Vibrating Filter Machines or Sieves
The Barber 'SAND MAN' sand sifter machine [7] operates on a very
different concept, unlike the previous rake conveyor-type machines. It first
scoops up any litter and sand from the ground using rotating buckets. These
buckets elevate the sand and the litter (or all debris) and places them on a
vibrating sifting screen, positioned higher above the ground, to sieve the
debris and retain objects that cannot pass through the screen (or mesh,
which functions as a filter). The debris gradually moves towards a

collection bin, due to the cyclic backward movements of the screen, while
any sand and small pieces of dirt falls through the screen to the ground.
Theoretically, this vibrating filter design should work well for fairly solid,
dry, ball-shaped manure, and warrants future investigation and testing.
However, it is very likely that wet or very moist dung will cause problems
by blocking the screen (mesh) holes. Also, some of the holes in the filtering
screen may occasionally become blocked by tight fitting stones or objects.
1.2.4 Vacuum-Type Litter Collection Machines
Large volumes of litter can be removed from the ground using powerful
vacuum suction cleaners mounted on large trucks and steerable vehicles,
like the machines shown in [8] and [9]. A handheld, hand-steered, self-
propelled and human-guided vacuum cleaning machine is shown in [10],
and is effective for small to medium litter collection jobs. While very
effective for collecting litter on hard ground, like sealed street surfaces and
grassy areas, it would easily collect a great deal of sand and even water,
which needs to be removed or prevented from entering the collection bin.
The overall effectiveness of this type of collection method for collecting
animal dung may be examined in a future project.
1.3 Selection of the Animal Dung Collection Method
Several companies manufacture and sell high-speed litter-collection
machines, such as BarberTM, HermanesTM and WidontecTM. These
machines usually require a large tractor or truck to move them around, and
they are quite expensive. For example, the Barber 'Beach Rake' model litter
collection machine costs approximately $55,000 USD (not including the
towing tractor), and is used by many different cleaning companies
worldwide to perform large scale litter collection on public beaches. This
project, however, is targeted more towards meeting the needs and budgets
of local camel and horse ranchers who cannot justify the purchase of very
expensive machines and tractors, or who do not wish to hire costly laborers
to manually collect animal manure and keep their stables and ranches clean
on a regular basis. Such a machine needs to be easy to transport, reasonably
affordable (below $10,000) and easy to operate by one person. Therefore,
after considering the previous collection methods, this team decided to
focus on developing a self-propelled camel and horse dung collection
machine based on the BarberTM 'Surf Rake' or 'Litter Picker' rake conveyor

design (but without the collection bin lifting and tipping feature), since it
appears to perform reliably on sandy ground.
2 Drive Motor Modeling and Selection
The main goal of this section is to calculate the required power needed from
a driving motor to rotate the two front wheels, assuming 'worst-case
loading' conditions. The total mass of the entire machine (approximately) is
set to mtotal = 100 kg (including a full payload). The maximum mass of the
vehicle is 60 kg (approx.) and the maximum payload is 40 kg. Treat the
polar mass moments of inertia for all 4 wheels as negligible (Fig. 3).
Fig. 3 Modelling the vehicle as a single mass on a constant slope hill
Assume maximum slope of hill or sand dune to climb is: θ = 30° (worst
case driving).
Force to overcome static gravity on a slope of θ = 30°:
(1)
Static gravity force is Fg = 100 × 9.81 × sin (30°) = 490.5 N
Approximate drag force of rake teeth pushing through sand under
vehicle: Fdrag
 = 250 N.
Drive force needed to (barely) climb a 30° slope:

(2)
On flat level ground, maximum linear acceleration:
(3)
Choose maximum linear (forward) velocity: v = 0.2 m/s.
Wheel radius: r = 0.2 m (front wheel diameter: d = 0.4 m)
Wheel speed:
(4)
(= 1 rad/s × 1 rev/(2π rad) × 60 s/min = 9.55 RPM, maximum front
wheel
speed)
Theoretical Power needed for drive motor:
(5)
Assume 85% motor and gear efficiency, required motor power:
Torque needed:
(6)
This is the minimum output torque needed for the worm-drive gearbox
driving the front two wheels, at a speed of 9.55 RPM. Therefore, we need to
select a motor with a continuous power rating >174 W. TRAMECTM (Italy)
manufactures and supplies a suitable motor and gearbox unit that has a
rated continuous power rating Pmotor = 180 W or 0.18 kW. The front wheels
and axle rotate at the same speed as the output shaft of the worm gearbox of
this motor (1:1 ratio). A 3 phase AC motor was selected to avoid the wear
and maintenance issues associated with brushed DC motors. Its speed is set
by a Panasonic VF200 speed controller. (Alternatively, a BLDC motor can
be used.)
3 Conveyor Design, Motor Modelling and
Selection

The manure collecting mechanism must perform several critical functions
in a predictable and reliable manner, namely:
Collect as much manure as possible off the ground (faster than a human
worker)
Remove or exclude sand or dirt (e.g. some kind of filtration process)
Transport or elevate the manure to the top of a collection bin or container
Deal with shock, or adapt to immovable obstacles (e.g. strong stones,
roots)
Prevent human injury or accidents
Allow for easy maintenance work (replacement of worn parts) and repair
work.
McGuire [11] describes the designs of many different kinds of conveyor
systems, including 'Tabletop chain conveyors'. For this dung collection
application, the table is inclined at a steep 58° (degrees) to the horizontal, to
keep the base of the vehicle (or distance between the front and back wheels)
fairly short and compact so the vehicle can make 'tight turns', or have a
small turning circle.
Fayed and Skocir [12] describe how plain chain conveyors are suitable
for constant speed operations and only require lubrication and minor
maintenance. This may be true for a clean factory environment, however, if
a great deal of dirt or sand builds up on the chains and sprocket wheels—as
would be expected on a farm, beach or a sandy desert—the chain conveyor
could eventually encounter much more friction, require more motor torque,
and eventually fail to rotate. Therefore, to keep the conveyor chains and
sprocket wheels as clean as possible, and to prevent human injury, clear
plastic covers (made of transparent acrylic sheet) were designed and built to
cover the entire conveyor area, including all power transmission elements
of the machine. This is a very important requirement to satisfy the 'safety
regulations' or 'safety standards' for most countries.
The following discussion describes how to calculate the approximate
required power needed for the driving motor to rotate a 'chain conveyor'
that moves animal dung from the ground to the collection bucket, while also
rotating all elements of the conveyor system (i.e. both conveyor chains, all
rakes and all rotating conveyor shafts and sprockets/wheels). We will also
calculate the 'effective' or 'reflected inertia' of the entire conveyor system

and (maximum possible) payload of the dung at the 'drive shaft' (at position
A, as shown in Fig. 4).
The existing design of the conveyor system for the first prototype is
shown in Fig. 4 above. The 'straight' rake teeth are aligned in a row, spaced
approximately 20 mm apart. This spacing distance between each tooth can
be reduced to collect smaller sized dung pellets if necessary, however, this
will result in greater drag force in the sand and a heavier rake, hence, more
power would be required from the conveyor motor. The rake teeth are made
of round steel (similar to thick nails, but approximately 2 mm in diameter),
each welded onto an UA (Unequal Angle) or L-shaped rake bar which is
bolted to a conveyor chain at each of its ends. The 'rake bar' is
approximately 980 mm wide. The 2 conveyor chains are similar to bicycle
chains but larger, and wrap around the sprocket wheels at positions A, B, C
and D. The left-side and right-side chains both travel at the same speed,
driven by 2 sprocket wheels located on opposite ends of a long shaft,
located at position A.
Fig. 4 Conveyor design for collecting animal dung (Each sprocket wheel radius is r = 60 mm)
Animal dung is quite lightweight (and density varies based on moisture
content), so for a fully-loaded conveyor, assume that the 'worst case' total
mass of the animal dung being carried up the straight 58° slope is mpayload 
= 5 kg (assuming all ascending rakes are full). The masses or inertias of the
21 'L-shaped' UA sections ('Unequal Angle' solid steel bars) that are used

for the rake bars that hold all the teeth, is significant, because each rake bar,
2 connection bolts and 4 nuts, and all straight teeth weighs approximately
1.4 kg. Two different types of teeth are shown in Fig. 5. For the first
prototype, all the rake bars were made with 'straight teeth'. In fact, the
largest load on the chain conveyor system will be caused by the 'drag force'
of the teeth being pulled through the sand, beneath the machine. This drag
force was assumed to be 250 N (Fdrag). Drag force through the sand can be
experimentally determined by measuring the force on a load cell or a
'spring balance' (that measures tension force) in series or inline with a
pulling rope connected to the front of the prototype vehicle (shown in
Fig. 9), as the vehicle is being pulled across level sandy ground. Fdrag is
proportional to the number of teeth (or number of rakes) dragging through
the sand underneath the vehicle. Dragging more than one row of teeth
through the sand may not even be necessary, although it increases the
chances of collecting manure that happens to pass through the first rake.
Fig. 5 'Straight teeth' and 'Bent teeth' rake designs for the conveyor
The total torque needed for the driving motor to rotate the shaft at
position A, and to rotate the entire conveyor system can be calculated from
this dynamics equation:
(7)

where Tdynamic is the torque needed to accelerate the entire system from rest
up to the top speed, in a given time t, and Tconstant is the sum of all
persistent or constant torques that need to be overcome, such as torque due
to gravity loading of the payload (under worst case loading conditions)
Tgravity, drag torque Tdrag created by the total force of all the rake teeth
between B and C being pulled through the sand, and the sum of all other
continuous friction torques, Tfriction. The torque due to gravity loading of the
rake bars will be negligible because the conveyor system is almost perfectly
'balanced', having the same number of rakes on the 'ascending' side (going
up the 58° ramp, between positions B and A) as on the 'descending' side
(between positions A and C). For simplicity, let us assume Tfriction is
negligible and focus on calculating the Tgravity and Tdrag torques as 'seen' at
the drive shaft at position A (which will be rotating at the same speed as the
output shaft of the gear box connected to the driving motor that drives the
entire chain conveyor system). Each conveyor wheel (or sprocket) shown in
Fig. 4 has a radius r = 60 mm.
(8)
(9)
(10)
Assuming 21 rakes, each weighing 1.4 kg, and spaced approximately
144 mm apart,
(11)
(12)
where Jref is the 'Reflected inertia' or 'equivalent inertia' of the entire
conveyor system (including all rotating components) as 'seen' by the shaft
at A, where the output of the motor gearbox equals the speed of the shaft at

A. Note that 'i' is the index number for all solid rotating bodies rotating at
each position in Fig. 4 (namely, positions A, B, C and D), mi is the mass of
the cylindrical shaped body (which could be a sprocket wheel, or a 1 m long
shaft), and ri is the radius of that round solid. The mass of each rake is
mrake = 1.4 kg, and its radial distance to the axis of rotation is rw = 0.06 m.
This equation above is only approximate since it does not consider the
'centers of mass' for each tooth on every rake, because mrake is treated like
a point mass in-line with the chain. Also, the inertias of the two long chains
(on the left and right-hand sides) are considered negligible in this
calculation. Also note that all rotating masses on the conveyor system are
rotating at the same angular velocity and angular acceleration because there
are no gear reductions (no changes in torque nor rotational speed) from one
shaft to another. All rotating sprockets and shafts are rotating at the same
speed as the output shaft of the gearbox unit connected to the drive motor at
A. For more details about the derivation of the general equation for
'reflected inertia', also known as 'equivalent inertia', refer to Klafter et al.
[13], 'Robotics Engineering—An Integrated Approach (Table 1).'
Table 1 Mass moments of inertia for all rotating sprockets and shafts of the conveyor (approx)
I. No.
1
2
3
4
5
6
7
8
9
10
11
12
13
mi mass
0.9 kg
0.9
0.9
2.6
0.9
0.9
2.5
0.9
0.9
2.5
0.9
0.9
2.5
ri radius
0.06 m
0.06 0.06 0.01 0.06 0.06 0.01
0.06 0.06 0.01
0.06 0.06 0.01
mir2
i
/2 
×10−3
1.62 kg.m2 1.62 1.62 0.13 1.62 1.62 0.125 1.62 1.62 0.125 1.62 1.62 0.125
Jref = 15.1 kg.m2 (approximately) for the entire conveyor, as 'seen' at
the shaft at A. α is the constant angular acceleration of the conveyor system
to accelerate from rest up to maximum speed within a given time t. Assume
that the maximum speed ωmax = 0.5 rev/s = π rad/s = 3.14 rad/s must be
reached within a maximum time t = 3 s, then the maximum constant angular
acceleration of the conveyor α = ωmax/t = 1.05 rad/s2.
It is important to check that the speed of the rake teeth relative to the
ground is faster than the maximum forward velocity of the vehicle, or the
rake rotation may be too slow to pick up the animal dung, causing the dung
to 'pile up' or build up in front of the vehicle. The length of the 'straight'

tooth is 60 mm, so when this is added to the sprocket wheel radius of
60 mm, the distance from the axis of rotation to the tooth tip is
approximately 120 mm, or rtooth = 0.12 m. Tangential velocity of the tooth
tip is v = r ω, so the tooth will be traveling at v = 0.12 × 3.14 = 0.377 m/s,
which is still higher than the vehicle's top speed of 0.2 m/s (almost double).
Therefore, the operating speed of 0.5 rev/sec is suitable as the top operating
speed for the conveyor system. Assuming the entire front face of the
conveyor is fully loaded, with 5 kg of wet manure (i.e. all front rakes are
fully loaded), we can find the fastest possible manure collection rate.
Distance travelled along front face of conveyor from B to A is d = 1.15 m
Maximum linear conveyor speed v = r ωmax = 0.06 π rad/s = 0.19 m/s
Speed v = Distance/Time = d/t, so t = d/v = 1.15/0.19 = 6.05 s
Mass flow = Mass/Time = 5 kg/6.05 s = 0.826 kg/sec or 49.6 kg/min.
The total dynamic torque needed to accelerate the entire conveyor from
rest to top speed can now be calculated as:
Total required motor torque is
Theoretical motor power required to drive the conveyor is
(13)
This is the minimum torque needed from the motor and gearbox driving
the shaft at A.
Assume 85% motor and gear efficiency, required motor power: Preq = 
(Pth/0.85) = 123.4 W, therefore, the motor for driving the conveyor system
can be a lot smaller than the vehicle driving motor. In order to avoid piling
up manure, which would increase drag force on the vehicle, the conveyor
operating speed should always be higher than 1.67 rad/s when the vehicle is
going at the maximum forward speed of 0.2 m/s.
4 Steering Mechanism
Kershaw and Van Gelder [14] describe different kinds of steering systems
used for steered vehicles and contemporary automobiles. To keep the design

as simple as possible, the entire vehicle is manually steered in a manner that
is similar to how a boat is steered with a rudder. Figure 6 shows a 'Top
view' diagram of the steering shaft (at the center) which moves connecting
rods that rotate the rear wheels. To turn the vehicle right, the operator
rotates the steering shaft clockwise using a handle, so the two (unpowered)
rear wheels point left, and vice versa for turning the vehicle left. This is
known as a 'parallelogram steering mechanism' and is only recommended
for low-speed applications. High vehicle speeds would cause significant
tyre (or wheel) scrubbing, or lateral wheel dragging, because the rear
wheels need to point in a direction that is tangent to its appropriate 'turning
circle' which is centered on the point of rotation (as seen in a 'top view').
To avoid wheel scrub (which can cause significant tyre wear and high
ground friction), it is better to use an 'Ackermann' steering mechanism [15,
16] which points both wheels tangent to their own turning circles, which
happen to be concentric, centered on the same point of rotation. However,
because this vehicle is designed for use mainly on sandy ground, like
beaches or in the desert, the 'parallelogram' steering mechanism was
considered adequate for this environment (Fig. 7).
Fig. 6 Manual steering mechanism at rear of the vehicle

Fig. 7 Photo of steering mechanism from under the vehicle
5 Field Test Results
5.1 Real-World Test Runs
The first prototype (based on the design in Fig. 4) had a dismal success rate
of about 10% at collecting animal dung off sandy ground. i.e. For every 10
pieces of dung that the vehicle drove over, usually one or no pieces were
transferred to the conveyor. Horse and camel dung tended to move forward
and roll off the 'straight teeth' on each rake.
One of the big problems encountered was the problem of new or moist
dung sticking to the rake teeth (which had to be manually removed by hand,
with a cloth or paper towels). Pieces of dung that stick to a rake can easily
fall off or return to the ground when they are dragged under the vehicle.
This problem can be partially solved by adding a 'cleaning rake' to act as a
filter or a 'comb' for the conveyor rakes, to push off any sticky dung,
allowing it to roll into the collection bin, as shown in Fig. 8.

Fig. 8 Cleaning rake can force animal dung into the collection bin
For example, the 'cleaning rake' may be oriented perpendicular to line
AB at Position A, with its teeth almost tangent to the wheel at A, at the very
end of the conveyor, as shown in Fig. 4, so that its teeth can slide through
the gaps of the moving teeth on the conveyor, and can direct any dung to
roll down into the collection bin.
As shown in Fig. 4, the 'Stationary curved ramp' was added to help
catch pieces of dung and stop them from falling off the straight teeth of the
rake. Without this curved ramp, the dung pieces kept falling or rolling
forward off the teeth. However, because the lowest part of the curved ramp
had to be as low as possible to the ground to prevent pieces of dung falling
forward off the teeth, the curved ramp ended up blocking and piling up
dung pieces in front of the vehicle, in a similar manner to a snow plough
pushing snow forward, except sand is piled up.
The curved ramp was an impediment or obstacle for the pieces of dung,
and was not a good solution to the problem of dung pieces falling off each
rake. Numerous field tests proved that the 'straight tooth' rake and 'curved
ramp' idea was unsuccessful and unreliable for collecting dung pieces and
transferring them to the conveyor in a reliable manner. A feasible solution to
this problem will be discussed in the next section.
5.2 Possible Improvements to the Rake Design
A feasible solution to stop dung pieces from falling off the teeth was
proposed by the first author. Instead of using forward-moving straight teeth
that tend to flick any dung forwards, bent teeth can be used on all rakes to
scoop up dung and stop any pieces from falling off the rake near position B
(See Figs. 5 and 10) so that a 'curved ramp' is no longer necessary. As seen
in Fig. 10, the bent teeth on each rake can act like a filtering 'scoop' and can
hold the dung pieces in place before they begin ascending up the steep ramp

to the collection bin. A more reliable, simpler, lower-cost design for the
animal dung collection machine is shown in Fig. 10c. The 'rotating rake'
shown in this diagram consists of 4 straight-tooth rakes attached to a
rotating shaft. It serves to push out any dung that may cling to the bent teeth
on the moving conveyor rakes, in a similar manner to the 'cleaning rake'
shown in Fig. 8. This design offers many advantages over the design of the
existing conveyor shown in Fig. 4, namely:
It requires far fewer shafts and sprockets (i.e. simpler design, less
material needed)
It uses far fewer rakes because the conveyor chains are much shorter
(hence, much less cost and setup time needed during manufacture)
Much fewer components means far less weight and less manufacturing
cost
It does not create as much friction or drag force with the ground (because
not many teeth are in contact with the ground—i.e. Only one rake is
dragging through the sand, unlike the many rakes seen in Fig. 4,
therefore, Tdrag or Fdrag would be much lower, so a much smaller
conveyor motor may be used.
There is no need for any curved ramp (which doesn't help with collection
anyway!)
Less space is needed for the conveyor, so the collection bin can be much
larger.
Unfortunately, this potentially better solution - using 'bent teeth' as
shown in Figs. 10a, b—was too late to implement by the time the project
was completed, so this is left as future work.
At the back of the machine (see Fig. 9) is the steering handle which
rotates the steering shaft for turning the back wheels. In the side view, the
lower motor and worm-drive gearbox is shown connected to the front axle
using a chain, for driving the front two wheels. The upper motor also has a
worm-drive gearbox for rotating the conveyor system, driven by the top
shaft. In the back view, Fig. 9b, near the left wheel, is a 12 V DC car battery
which is connected to a 12 V DC to 240 V AC power converter. This
powers the 2 AC motor speed controllers shown in the bottom right corner.
There are 2 'On-Off' switches for activating or de-activating each AC motor
in the upper right-hand corner of Fig. 9b. A video showing the KU
prototype in action is available for viewing at [17]. The total cost of all

parts and materials to build the prototype in Fig. 9 comes to approximately
$3000 USD or about 11,000 AED.
Fig. 9 a Side view and b back view of the animal dung collector prototype built at KU
Using flexible rubber or plastic material for the 'bent teeth' could
improve each rake's effectiveness at picking up small pieces of litter on
hard, rocky or rough ground surfaces. Alternatively, if each bent tooth, or
each rake was 'spring-loaded' to allow backward flexing, the conveyor
would perform much better on hard ground or on rough surfaces, because
the tips of the rake teeth would be able to follow the ground's contours or
rub against the ground, thus preventing high resistance collision forces and
large gaps under the rake (which could miss some pieces of dung). These
kinds of improvements will help to significantly reduce or prevent shock
loads and very high drag forces caused by rigid rake teeth getting caught on
hard immovable objects, or grass roots, while improving the effectiveness
of manure collection.
Therefore, if the improvements mentioned earlier are implemented, and
the parts costs can be reduced by about half, this design could become a
commercially feasible product if it is mass produced on a large scale.
6 Conclusions
This paper described the design, engineering analysis, performance results,
and observed problems and their potential solutions for a camel and horse

dung collection machine. This paper described the current 'state of the art'
in litter and dung collecting machines, motor modelling and selection for
vehicle locomotion, steering mechanics and motor modelling and selection
for the chain conveyor mechanism. Animal dung collection was examined
and analyzed briefly. At present, the current prototype is unable to reliably
collect round clumps of camel dung off the ground, mainly because of
inherent problems with the 'straight teeth' rake design. With a few
improvements to the rake design, and some modifications—i.e.
implementing flexible or spring-loaded 'bent teeth'—this machine could
become very reliable at collecting camel and horse dung. A future version
of this machine that uses the conveyor design shown in Fig. 10c could be
built to keep manufacturing costs as low as possible.
Fig. 10 a and b A 'Bent tooth' design; c A simplified lower-cost conveyor design
References
1.
Hansson, B. O. (1978, Oct 25). Vehicle carried manure removing machine. U.S. Patent
4,289,439.
2.
Owen Vinyard (1994, Mar 29). Rear-mounted manure gathering machine and method of
handling manure. U.S. Patent 5,297,745.
3.
OldManStino. Best Way to Collect Cow Manure. (July 1, 2018). Accessed Sep 11 2018. (Online
Video). Available: https://​youtu.​be/​598HG4M0kuQ.

4.
Vibhute, R. (2017 Sep 2). Cow Dung collecting machine. Accessed Sep 11 2018. (Online Video).
Available: https://​youtu.​be/​9SkNVRLZoMI.
5.
Barber, H. & Sons. (2012, Dec 21). USA. Hurricane Sandy Beach Cleanup with Beach Cleaning
Machine. Accessed Sep 11 2018. (Online Video). Available: https://​youtu.​be/​NyyRmrdI_​-M.
6.
Barber, H. & Sons (2016, July 1). USA. Litter Picker Machine for Event Cleanup. Accessed Sep
11 2018. (Online Video). Available: https://​youtu.​be/​REY7eP8ewyY.
7.
Barber, H., & Sons. (2011, Oct 27). USA. SAND MAN: Barber's Walk-Behind Beach Cleaner.
Accessed Sep 11 2018. (Online Video). Available: https://​youtu.​be/​6J96qxqIQn0.
8.
Awesome inventions. (2018, Mar 12). Meet 'Hermanes'—The giant street cleaning vacuum.
Accessed Sep 11 2018. (Online Video). Available: https://​youtu.​be/​RABRJgNuuUw.
9.
Widontec. (Oct 20 2014). Widontec MC3 self-propelled trike vacuum cleaner. Accessed Sep 11
2018. (Online Video). Available: https://​youtu.​be/​vczNjC5atpU.
10.
Chau, H. (May 14 2017). E-Vacuum Machine/ Litter picker. Accessed 11 Sep 2018. (Online
Video). Available: https://​youtu.​be/​MfmW8PRlqvY.
11.
McGuire, P. M. (2009). Tabletop Chain Conveyor. In Conveyors: Application, Selection, and
Integration (Systems Innovation Book Series) 1st ed., USA: CRC Press, pp. 17-23.
12.
Fayed, M. E., & Skocir, T. S. (1996). Chain-Type Conveyors. In Mechanical Conveyors:
Selection and Operation, USA: CRC Press, pp. 303-304.
13.
Klafter, R. D., Chmielewski, T. A., Negin, M. (1989). Mechanical systems: Components,
dynamics, and modeling. In Robotic Engineering: an integrated approach. Englewood Cliffs,
USA: Prentice-Hall, pp. 119-124.
14.
Kershaw, J., & VanGelder, K. (2017). Steering systems. In Automotive Steering and Suspension
(Master Automotive Technician) Kindle Edition, USA: Jones & Bartlett Learning, pp. 365-366.
15.
Wilson, C. E., & Sadler, J. P. (2003). Mechanisms for specific applications. In Kinematics and
Dynamics of Machinery 3rd ed., India: Pearson India Education Services, pp. 75-77.
16.
Norris, W. (1906). Steering. In Modern Steam Road Wagons (Ed.), London (pp. 63-67). UK:
Longmans Green & Co.
17.
Alali, M. (April 30, 2019). Animal Dung Collecting Machine. Accessed 23 Oct 2018. (Online
Video). Available: https://​youtu.​be/​vrpnm_​3tm8g.
OceanofPDF.com

(1)
(2)
(3)
(4)
 
 
 
 
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_9
Discussion of Soft Tissue Manipulation for
the Harvesting of Ovine Offal
Qi Zhang1   , Weiliang Xu1   , Zhisheng Zhang2   , Martin Stommel3  
and Alexander Verl4  
Department of Mechanical Engineering, The University of Auckland,
Auckland, New Zealand
School of Mechanical Engineering, Southeast University, Nanjing,
China
Department of Electrical and Electronic Engineering, Auckland
University of Technology, Auckland, New Zealand
Institute of Control Engineering (ISW), Stuttgart University, Stuttgart,
Germany
 
Qi Zhang
Email: qzha862@aucklanduni.ac.nz
Weiliang Xu (Corresponding author)
Email: p.xu@auckland.ac.nz
Zhisheng Zhang
Email: oldbc@seu.edu.cn
Martin Stommel
Email: mstommel@aut.ac.nz
Alexander Verl
Email: alexander.verl@isw.uni-stuttgart.de

Keywords Soft tissue manipulation - Ovine offal sorting - Robotic
manipulation - Modelling
1 Introduction
Soft tissues exist in a wide range of fields, including industrial, domestic,
medical and food applications, and their robotic manipulations have
attracted a lot of attention in recent years. There are many challenges in soft
tissue manipulation, which includes: (a) 3D soft tissue modelling requires
very high computational cost; (b) the shape of soft tissue changes
significantly during manipulation; (c) the interaction between soft tissue
and manipulator is very complicated; (d) strategies and techniques of rigid
objects manipulation cannot be applied directly to soft tissue. The
applications of soft tissue manipulation mainly include medical assistance,
surgical suturing, meat cutting and food harvesting.
Ovine offal harvesting is one of the significant studies in soft tissue
manipulation because ovine offal is a major export co-products of meat
processing. In New Zealand, ovine offal made up a third of the total edible
offal, with 22,184 tones and worth 63 million dollars [1]. At present, ovine
offal is harvested manually. The market volume is limited due to the low
efficiency and high cost of manual labour. Plants generally halted offal
collection, especially the lower-value products, when the labour is scarce.
This phenomenon also happens in Australia, with a yield of only 63-91%
even for the higher-value organs such as heart, liver and kidney [2].
Existing robotic systems for organ sorting mainly are used for organ
harvesting of small animals such as poultry [3-5]; they cannot be used for
large animals such as sheep due to their size and mechanical properties. The
automatic processing of large animal carcass is after the removal of the
organ package as a whole [6]. There is currently no automatic system for
harvesting lamb organs.
Therefore, it is necessary to develop a robotic system that can
automatically and efficiently sort internal sheep organs and study its
manipulation. In this paper, the requirements for robotic sorting of ovine
offal are analyzed in Sect. 2. Section 3 introduces the current practices in
the animal slaughtering and processing industry. Section 4 proposes a
robotic system for ovine offal sorting and summarizes the problems to be
solved. The potential solutions for the above problems are discussed in

Sect. 5. Section 6 gives the conclusion and future work of ovine offal
harvesting.
2 Requirements Analysis
The aim of this research is to use a robotic system to manipulate and
separate the internal organs of sheep (Fig. 1). We use "organ package" to
represent the internal sheep organs that need to be manipulated. It contains
mainly heart, lungs, liver, stomach (i.e. rumen, abomasum, omasum and
reticulum), spleen, gallbladder and intestines with the folded structure,
connected by blood vessels and connection tissues. We assume that the
organ package has been put on the table manually or automatically. The
robotic system first needs a machine vision to capture the configuration of
the organ package. The removal device is then needed to cut and remove
individual organs. The sorting device is also required to order the
configuration of the organ package. This is because the organ packages are
generally unordered and have a spatial configuration (i.e. the organs may
overlap/occlusion), making it impossible for machine vision to recognize
organs directly.
Fig. 1 Schematic diagram of the research objective
The manipulation devices (including sorting device and removal device)
should satisfy the following requirements: (a) The diameter of the table is
about 600-1000 mm, depending on the size of the organ package; (b) The
payload of the devices is about 5-20 kg, depending on the weight of the
organ package; (c) The requirement of position accuracy is generally about
±3 mm; (d) The devices can be washable and withstand high temperature

and high-pressure cleaning; (e) The components of the device such as
grease, lubricant, tools and fixture materials should be food grade. At the
same time, the sorting of ovine offal should satisfy some special needs of
the food industry. Soft tissue manipulation requires high hygiene
requirements in food industries because of their susceptibility to bacterial
contamination. In addition, quality control is another important requirement
for this industry. This because soft tissues are relatively fragile and are
easily damaged when they come into contact with the manipulator. The
unique environmental conditions of soft tissue manipulation should not be
neglected. A manipulation environment with the appropriate temperature,
humidity and pressure are required.
The manipulation tasks in ovine offal sorting include adjusting the
configuration of the organ package, grasping the single organ, cutting off
the connection tissue and removing the single organ. The purpose of
adjusting the configuration of the organ package is to expose the
overlapping organs for identification and to adjust the recognizable organs
to a suitable position for grasping and removal.
3 Current Practices
In the animal slaughtering and processing industry, most of the processing,
such as carcass-splitting, deboning and packaging, can be performed by
robotic systems, although the system has some shortcomings. Applications
of robotic systems in the animal slaughter and processing industry are
shown in Fig. 2. Figure 2a shows carcass-splitting automation using a 6-
DoF robot arm, the manipulation task cutting off the connection tissue in
ovine offal sorting can be achieved using the similar robotic system.
Figure 2b shows a robotic chicken deboning system. Figure 2c shows a
robotic system for auto-shacking of poultry, the manipulation task grasping
single organs and removing single organs in ovine offal sorting can be
achieved using the similar robotic system.

Fig. 2 Current practices in the animal slaughtering and processing industry [6]
4 Proposed System and Its Operation
The proposed system consists of three parts, (A) machine vision, (B) sorting
device and (C) dual-arm robot (Fig. 3). The machine vision is used to
identify the types and positions of organs and guide the action of the robotic
system. The sorting device is used to order and manipulate the organ
package, adjusting its pose to expose single organs. It can be a rotation
table, a peristaltic table [7-10], a shaking platform [7, 8], a robotic arm, or a
soft machine table [11]. The dual-arm robot is used to cut and remove
single organs. One end effector of the dual-arm robot is (C1) a vacuum
gripper with many small suckers, and the other is (C2) a specific cutting
device for soft tissue. The removal process involves lifting the identified
single organ first with the vacuum gripper and then cutting the connection
tissue of this single organ with cutting device, and finally placing this
separated organ to the designated position with the vacuum gripper again.

Fig. 3 The concept of a robotic system for ovine offal harvesting. (A) machine vision, (B) sorting
device, (C) dual-arm robot: (C1) a vacuum gripper and (C2) a specific cutting device
In this paper, we propose a partial approach to separating the organ
package. We mainly focus on four organs, the heart, lungs, liver and spleen.
Figure 4 shows the process of the robotic system harvesting sheep internal
organs. The operation begins with the placement of the unordered package
on the table. At the same time, the camera begins to capture images of the
organ package and then judges the configuration of the organ package. If
there is a separable organ (i.e. can be recognized and separated), the organ
will be removed by the dual-arm robot. If there is a recognizable organ (i.e.
can be recognized but not separated), the organ package will be
manipulated by the sorting devices until this organ can be separated. If there
is no organ can be recognized, select an organ with distinct features (i.e. the
most recognizable feature) and manipulate the organ package with the
sorting devices until this organ becomes separable. The next step is to
repeat the above judging process until all the organs are separated. The final
step is to clean up the table and start a new process. This step can also be
executed if the organs cannot be detected after a long unsuccessful
manipulation.

Fig. 4 The operating process of a robotic system for ovine offal harvesting
There are many problems that need to be solved in ovine offal sorting.
1. Mechanics modelling of the organ package. It is difficult to build the
organ package model because of the complexity of its components. The
organ packages are generally unordered, with the organ having an
irregular surface shape, special microstructure (e.g. the heart has a
unique laminar structure, and the lungs contain alveoli) and physical
loading properties (e.g. stress and strain). The connection tissues are
located in different parts of different organs and have different physical
loading properties.
 
2. The contact between the organ and the manipulator. The softness and
stickiness of the organs complicate the contact between the organ and
the manipulator. The surface of the organ is distributed or covered with
fatty tissue, mucus, and membranes, which affect the manipulation and
grasping of the organ.
 
3. The estimation of the organ package deformation. Different organs
have different behaviors under the external force. They interact with
each other during manipulation by connecting tissues. Some organs like
the intestines and stomach are filled with intestinal or digestive fluids
and partially digested food debris. It is difficult to estimate the organ
deformation required for manipulation control with high accuracy.
 
4. The control of soft tissue manipulation. There is an indirect
simultaneous localization problem [12] in the control of soft tissue
manipulation, which is multiple points called control points on an
object should be manipulated to their desired points simultaneously
 

(Fig. 5). The control points are difficult to manipulate directly.
Therefore, the other points called manipulation points on the object
surface are needed to realize the manipulation. The manipulation points
are the contact points between the manipulator and the object, and the
control points are the points on the object that needs to be controlled.
Fig. 5 Single organ manipulation
5 Discussion
5.1 Mechanics Modelling of the Organ Package
There are three modelling methods available for soft tissue modelling: finite
element method (FEM), mass-spring-damper (MSD) method, and
reproducing kernel particle method (RKPM). FEM is a numerical method to
solve engineering and mathematical physics problems. It is prone to error
when modelling complex objects. Moreover, its solution accuracy is low
when analyzing large deformed objects. It is difficult to meet the
requirements of high precision and high efficiency at the same time.
MSD method describes the object by a set of points with mass,
connected by springs or dampers or their combination. The method can well
describe the viscoelastic behavior of the object and has higher
computational efficiency than FEM. However, the MSD method has several
disadvantages. The parameters of these models are difficult to estimate. The
MSD model has systematic errors and localization of the deformation for a

complex network. It is difficult to handle large deformations of objects
using the MSD method.
RKPM is a mesh-free method using a correction function in an integral
transformation to impose reproducing conditions. This method can
accurately model extremely large deformation without the mesh distortion
problem because its computation does not require an explicit mesh.
Adaptive modelling can be easily accomplished by changing particle
definitions for the desired refinement without re-meshing. Compared with
FEM, the non-uniformity of the RKPM node spacing does not cause the
irregular shape of the mesh. Consequently, it has higher solution accuracy
under large deformation. RKPM has a higher efficiency than FEM for
handling large material deformation due to its smooth shape function. At
present, RKPM is mainly used to build the model of 2D objects, and its
application in 3D objects can be studied in the future.
In this proposed system, FEM will be used to build the mechanics
model of a single organ, MSD method will be used to establish the
mechanic model of connection tissue, and the combination of FEM and
MSD method will be used to establish the mechanics model of organ
package.
5.2 The Contact Between the Organ and the Manipulator
Different handling devices used to manipulate soft tissues has different
contact types and contact forces, which can result in different deformation
mechanics of soft tissues. For example, the finger has a point contact and
creates a point force when it contacts with the soft tissue. The table has a
surface contact and creates surface forces. Multi-finger manipulator is the
most commonly used manipulator in soft tissue manipulation. We can
extend the fingertip contact theory [13] to soft tissue manipulation. Soft-
finger contact refers to any external forces and pure torques that can be
exerted at the contact point as long as its direction inside or on the friction
cone. The soft-finger contact can be used as a contact between the rigid
finger and the soft tissue.
The contact part changes during the manipulation. For instance, the
contact between the finger and the object changes from a point to a set of
points (Fig. 6), some of which stick to the tip and some of which slide on
the tip [14]. Therefore, it is necessary to build the contact model between
the organ and the manipulator. The purpose of this model is to compute the

manipulative force and velocity applied in soft tissue manipulation. In the
proposed system, MSD method will be used to build the contact model of
fabricated organs. The concept of the model is shown in Fig. 7, which the
unknown MSD model is fixed at one end and connected to the force applied
by the manipulator at the other end.
Fig. 6 Object a before and b after two-finger grasp [14]
Fig. 7 The concept of the contact model
5.3 The Estimation of the Organ Package Deformation

Soft tissue satisfies the energy balance during the manipulation process
[15], expressed by the equation
(5)
which is the potential energy 
 consumed during organ manipulation is
equal to the kinetic energy 
 produced. The potential energy contains the
work generated by the external force 
 and the stored energy in its
deformation 
 when it has no boundary constraint. The work generated
by the boundary constraint 
 is added to the potential energy when the
organ is constrained. The estimation of organ deformation can be achieved
by minimizing its potential energy, which is computed from the static
equilibrium [16] equation
(6)
where  is the displacement of the point on the organ.
In the proposed system, we extend this theory to organ package
manipulation to obtain the transfer of energy between organs and
connection tissues. When the same external force is applied to the organ
package and an individual organ respectively, the transfer energy between
organs and connection tissues can be obtained by the kinetic energy
 and stored energy 
 in the deformation of an individual
organ minus the kinetic energy 
 and stored energy 
 in the
deformation of organ package. This is expressed by
(7)

5.4 The Control of Soft Tissue Manipulation
There are generally two kinds of control methods for soft tissue
manipulation: model-based control method and model-free control method.
For the model-based control method, the controller design is based on the
error between the desired deformation and the feedback and the model error
between the theoretical deformation calculated by the soft tissue
mathematical model and the feedback. For the model-free control method,
the controller design is based on the error between the expected
deformation and the feedback deformation and the estimated deformation
Jacobian matrix obtained by the estimator. The most common feedback in
soft tissue manipulation is the deformation of control points, called the
deformation feature. The description of the deformation feature is very
important for the model-free control method because the control is realized
based on these deformation features. The types of deformation feature
include points, distances, angles, curvatures, contours and surfaces. In the
proposed system, we prefer to use the model-based control method, because
it is difficult to recognize the deformation feature of organ package during
manipulation.
The sensing system includes machine vision for detecting shapes or
selecting manipulation points to guide the action of the robot and
force/tactile sensors for detecting shapes or contact conditions to
supplement contact information between the robot and the object. The
sensing system is used to verify the models of soft tissue manipulation in
model-based control method and to detect the deformation features in
model-free control method. The visual sensor is the major detection
equipment in this paper, and the tasks of it are model verification,
parameters identification, shape estimation and tissue/organs classification.
Existing image processing methods can apply in the proposed system.
6 Conclusion
There is currently no automatic system available for ovine offal sorting.
This paper analyzed the requirements of robotic sorting system of ovine
offal from the aspects of system composition, equipment requirements and
food industry requirements. Current practices of the animal slaughtering
and processing industry were described. We then introduced a robotic
system including machine vision, sorting device and dual-arm robot and

introduced its manipulation process. The problems to be solved in ovine
offal sorting were summarized, which includes mechanics modelling of the
organ package, the contact between the organ and the manipulator, the
estimation of the organ package deformation and the control of soft tissue
manipulation. The potential solutions to these problems were discussed by
reviewing the existing methods and theories of soft tissue manipulation. We
plan to use the combination of FEM and MSD method to build the
mechanics model of the organ package. MSD method will be used to build
the contact model between the organ and the manipulator. The energy
balance theory and its extension will be used to estimate the deformation of
the organ package. Model-based control method will be used to implement
the manipulation of organ package efficiently. Our future work is to
complete the first step of the ovine offal sorting, which is manipulating the
connecting organs from the initial configuration to the desired
configuration.
Acknowledgements
The research reported in this paper is supported by the Royal Society of
New Zealand. The first author acknowledges the provision of a doctoral
scholarship from the China Scholarship Council (CSC).
References
1.
MIA. Annual Report 2018. (2018). Meat Industry Association MIA (Trade Association
representing New Zealand meat processors, exporters and marketers).
2.
Meat Technology Update. (2008). CSIRO Food and Nutritional Sciences: Meat Industry
Services.
3.
Jansen, T. C., & Spijker, R. (2011). Method and apparatus for mechanically processing an organ
or organs taken out from slaughtered poultry. U.S. Patent No. 20110244773 A1.
4.
Jansen, T. C., & Spijker, R. (2012). Method and apparatus for mechanically processing an organ
or organs taken out from slaughtered poultry. U.S. Patent No. 8303383B2.
5.
Jansen, T. C., & Spijker, R. (2015). Method and apparatus for mechanically processing an organ
or organs taken out from slaughtered poultry. U.S. Patent No. 9004987B2.
6.
Choi, S., Zhang, G., Fuhlbrigge, T., Watson, T., & Tallian, R. (2013). Applications and
requirements of industrial robots in meat processing. In International Conference on Automation
Science and Engineering (CASE) (Vol. 2, pp. 1107-1112).

7.
Stommel, M., Xu, W. L., Lim, P. P. K., & Kadmiry, B. (2014). Robotic sorting of ovine offal:
Discussion of a soft peristaltic approach. Soft Robot, 1(4), 246-254.
[Crossref]
8.
Stommel, M., Xu, W. L., Lim, P. P. K., & Kadmiry, B. (2015). Soft peristaltic actuation for the
harvesting of ovine offal. In Robot Intelligence Technology and Applications 3 (Advances in
Intelligent Systems and Computing) (Vol. 345, pp. 605-615).
9.
Stommel, M., & Xu, W. L. (2016). Optimal, efficient sequential control of a soft-bodied,
peristaltic sorting table. IEEE Transactions on Automation Science and Engineering, 13(2), 858-
867.
[Crossref]
10.
Stommel, M., & Xu, W. L. (2016). Learnability of the moving surface profiles of a soft robotic
sorting table. IEEE Transactions on Automation Science and Engineering, 13(4), 1581-1587.
[Crossref]
11.
Deng, Z., Stommel, M., & Xu, W. L. (2016). A novel soft machine table for manipulation of
delicate objects inspired by caterpillar locomotion. IEEE/ASME Transactions on Mechatronics,
21(3), 1702-1710.
[Crossref]
12.
Henrich, D., & Wörn, H. (2000). Robot manipulation of deformable objects. London: Springer.
[Crossref]
13.
Nguyen, V. D. (1988). Constructing force-closure grasps. The International Journal of Robotics
Research, 7, 3-16.
[MathSciNet][Crossref]
14.
Guo, F., Lin, H., & Jia, Y. B. (2013). Squeeze grasping of deformable planar objects with
segment contacts and stick/slip transitions. In International Conference on Robotics and
Automation (ICRA) (pp. 3736-3741).
15.
Chen, J. S., Pan, C., Wu, C. T., & Liu, W. K. (1996). Reproducing kernel particle methods for
large deformation analysis of nonlinear structures. Computer Methods in Applied Mechanics and
Engineering, 139(1-4), 195-227.
[MathSciNet][Crossref]
16.
Navarro-Alarcon, D., & Liu, Y. H. (2018). Fourier-based shape servoing: a new feedback method
to actively deform soft objects into desired 2-D image contours. IEEE Transactions on Robotics,
34(1), 272-279.
[Crossref]
OceanofPDF.com

Robotics and Devices
The part starts with the first of several chapters that concern 3D printing.
Here, the focus is on the fabrication of structures to control very small
flows of liquid.
The second chapter in this part concerns the control of the 3D printer
itself. Its movement must be decoded form the files that describe the object
being fabricated.
A more prosaic problem is addressed in the third chapter, that of
removing the wobble from a four-legged table by automating one or more
of the legs.
The fourth chapter returns to the topic of the 3D printer. In this case, the
stability of the cross-beam is of particular importance because the printer is
very large.
The fifth chapter in the part is more practical than theoretical, dealing
with the automatic tightening of the rubber strips linking sections of an
office partition.
Next is the chapter that deals with the calibration of a robot. By
mounting a camera in the end effector, vision of a set of fixed targets can be
used to determine the pose of the hand.
A feeding robot can be used in a domestic or patient care situation,
where vision locates the mouth to be fed and the dynamics of the movement
must be carefully controlled to avoid spilling.
The final chapter in this part concerns the design, control and
experimental results of a lower-limb exoskeleton that combines soft and
rigid links.
OceanofPDF.com

(1)
 
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_10
Fabrication and Characterization of 3D
Printed Microfluidics
Swapna A. Jaywant1   , Muhammad Asif Ali Rehmani1, Tanmay Nayak1
and Khalid Mehmood1  
Department of Mechanical and Electrical Engineering, SF&AT,
Massey University, Auckland, 0632, New Zealand
 
Swapna A. Jaywant (Corresponding author)
Email: S.Jaywant@massey.ac.nz
Khalid Mehmood
Email: k.Arif@massey.ac.nz
Keywords Microfluidics - 3D printing - Characterization
1 Introduction
Microfluidics is an integral part of lab-on-chip (LOC) and a micro total
analysis system (µTAS) and sometimes also referred by these names. The
field of microfluidics has proven high potential in many applications
ranging from environmental assays to clinical analyses. This includes
various point-of-care diagnostic tools, therapeutic devices, and water
quality monitoring techniques and so on [5, 6, 8, 10, 12, 14, 15]. Several
techniques are available today for manufacturing of microfluidic channels
such as injection moulding, softlithography and paper microfluidics [1].
Among many methods, softlithography technique using
polydimethylsiloxane (PDMS) micro-moulding is a highly popular method
[2, 7]. Microfluidics fabrication using PDMS can be easily prototyped with
simple procedures [4]. However, this multi-step process requires special

equipment, and in many cases access to a cleanroom. Furthermore, it
generally manufactures the final product at the second step (casting). The
process is manual and cannot be fully automated [2].
Due to advancements in the modern additive manufacturing methods,
3DP has been shown as a promising platform for the fabrication of
microfluidic devices. 3D printers convert a computer-aided design (CAD)
into a physical 3D object by depositing the desired material in a layer by
layer fashion [11]. The main advantages of 3DP are automated fabrication
process, cost-effectiveness, higher printing resolution, etc. Additionally,
these machines make the process simpler and lower the size of the required
infrastructure and can be used as desktop printers [3].
In microfluidics, the major benefit of using 3DP is the elimination of the
need for a mould to cast the final shape/product. This allows a significant
reduction in the material cost, creates the possibility of mass manufacturing,
and saves significant development time. Many common 3DP techniques
like FDM, stereolithography (SLA), and polyjet printing have been
compared using open channel micro-mixers by several researchers [9, 13].
Mixing in the micro-mixers is primarily dependent upon diffusion of two
different flows. Hence, effective mixing is only possible with a slower flow
rate and longer channel length [13]. However, investigation of internal
features of microfluidic devices, the effect of flow rate and the channel size
are also important parameters to explore the possibilities of leakage through
the microfluidic devices.
In this paper, we explore the possibility of using FDM and SLS
technology for internal features of the microfluidic devices. These
technologies have been compared in terms of their ability to fabricate
microchannels. The comparison is based on the minimum possible channel
size, fluid flowrate, and leakage in the microchannel body.
2 Materials and Method
2.1 Instrumentation
The FDM-based 3D-printer used was a Tiertime UP 02, equipped with a
0.2 mm nozzle and the printer was controlled through UPStudio software.
Stereolithography file (stl format) of 3D parts was constructed using
parametric 3D modeling software—Autodesk Inventor Professional 2020.
An orange spool of polylactic acid (PLA) filament having a dimeter of

1.70 mm used to deposit the layers of modeled microfluidic channels. The
SLS-based 3D-printer used was DTM Sinterstation 2500 Plus. The 3D-
objects were printed with a nylon powder known as Precimid 1170. Pico
Plus Syringe Pump from HARVARD APPARATUS was used for injecting
the water at different flow rates. Pressure measurements were performed
with the help of PX3 Series heavy duty Honeywell pressure transducer
having pressure measurement range of 667 psi with total error band (TEB)
of ±1% full scale span (FSS).
2.2 Fabrication of Microchannel
The microchannels were fabricated with different diameter sizes. Each
microchannel consisted of an inlet port and the main channel as shown in
Fig. 1. The input connector was connected at the inlet port. The inner
diameter of the inlet port was kept constant at 0.5 mm. However, the main
channel was fabricated with various diameter sizes (0.25, 0.3, 0.35, 0.4,
0.45, and 0.5 mm).
Fig. 1 a CAD image of Microchannel. b Cross-sectional view of microchannel. c Pictorial view
showing the internal measurements of a microchannel
Two different processes were explored during the fabrication of these
microfluidic channels: FDM and SLS. Pressure measurements were

performed on these channels at different flow rates. In the SLS printer, the
inside power of the laser was set at 18 watts and outline power was kept at
9 watts. The slicer fill spacing was kept at 0.15 mm. The FDM printer was
optimised with nozzle temperature at 207 °C and platform temperature at
68 °C to achieve the best results during the printing process. Other process
parameters used during manufacturing are summarized in Table 1.
Table 1 3D printers and working parameters
3D printer Layer thickness (mm) Nozzle diameter (mm) Material used
FDM
0.1
0.2
PLA
SLS
0.1
NA
Precimid 1170
3D printed channels were cleaned, and the syringe was connected to
each input port of the microchannels to provide the inlet connection.
2.3 Experimental Setup
The experimental set up is explained in Fig. 2a. Initially, all the printed
channels were cleaned with compressed air jets. Water at different flow
rates (varying from 0 to 40 µL/Min. in steps of 5) was injected at the input
port with the help of the syringe pump. A disposable, 10 mL syringe was
actuated on the syringe pump. A pressure sensor was placed in-between the
syringe pump and the input port of the channel for corresponding pressure
measurement. The pressure was obtained as voltage value which, in turn,
was converted into psi value using the data-sheet. The change in flow-rate
at the input port resulted in a change in pressure at the input port and
leakage was observed in the microchannel. 

Fig. 2 a Experimental set up. b Microchannel printed with FDM method
3 Results and Discussion
The SLS technology uses nylon powder for laser printing, due to which the
microchannels with a diameter of less than 1.5 mm created using this
method were filled up with the nylon powder resulting in blocked
microchannels. However, microchannels with a diameter of 1.5 mm and
above created using this method did not have any blockages. Whereas the
microchannels created using the FDM method having diameters ranging
from 0.25 to 0.5 mm did not have any blockages (Fig. 2b). As a result of
these observations, the SLS method was dropped and the FDM method was
continued for further experiments. The water was passed through the FDM
method based microchannels to study the effect of various flow rates on the
leakage.
The graph (Fig. 3) explains the relationships among the applied flow
rate, the pressure generated at the input port, and the diameter size. As
depicted in the graph there is a proportional relationship between the flow
rate and the pressure. Additionally, at a certain flow rate, there is a linear

increase in the pressure for a decrease in the diameter size. In the case of
microchannels with a diameter of 0.4, 0.45, and 0.5 mm, no leakage is
observed.
Fig. 3 Change in flow rate versus developed pressure at the input port
However, in the microchannel with a diameter of 0.35 mm, no leakage
has been observed until pressure 0.2 psi. Above this pressure, the body of
the microchannel started to leak from the top and bottom side near the input
port. It has been observed that the microchannels with a diameter of 0.3 mm
and less started to leak even at a minimum flow rate.

4 Conclusion
In this paper, we demonstrated the fabrication of internal features of the
microfluidic device below 0.5 mm by optimizing the printing parameters of
PLA material. Moreover, the experimental results showed the correlation of
internal feature (microchannel diameter) with the pressure the microfluidic
device can handle without any leakages. Further treatment of microchannel
with void filling material or chemical can also enhance the overall fluidic
pressure in the device. Comparison between the SLS and FDM cannot be
presented due to the inability of SLS for printing internal features of
microchannel since the cleaning of the internal features at this scale was not
successful after numerous attempts. The printed SLS internal features were
clear with channel sizes exceeding more than 1.25 mm which does not fall
in the category of a microfluidic device.
Acknowledgements
This work was supported by Massey University Research Funds (MURF)
provided by the College of Sciences.
References
1.
Amin, R., Knowlton, S., Hart, A., Yenilmez, B., Ghaderinezhad, F., Katebi-far, S., et al. (2016).
3d-printed microfluidic devices. Biofabrication, 8(2), 022001.
[Crossref]
2.
Bhattacharjee, N., Urrios, A., Kang, S., & Folch, A. (2016). The upcoming 3d-printing revolution
in microfluidics. Lab on a Chip, 16(10), 1720-1742.
[Crossref]
3.
Bressan, L. P., Robles-Najar, J., Adamo, C. B., Quero, R. F., Costa, B. M., de Jesus, D. P., et al.
(2019). 3d-printed microfluidic device for the synthesis of silver and gold nanoparticles.
Microchemical Journal, 146, 1083-1089.
[Crossref]
4.
Chen, C., Mehl, B. T., Munshi, A. S., Townsend, A. D., Spence, D. M., & Martin, R. S. (2016).
3d-printed microfluidic devices: fabrication, advantages and limitations: A mini review.
Analytical Methods, 8(31), 6005-6012.
[Crossref]
5.
Kou, S., Cheng, D., Sun, F., & Hsing, I. M. (2016). Microfluidics and microbial engineering. Lab
on a Chip, 16(3), 432-446.
[Crossref]

6.
Lafleur, J. P., Joensson, A., Senkbeil, S., & Kutter, J. P. (2016). Recent advances in lab-on-a-chip
for biosensing applications. Biosensors & Bioelectronics, 76, 213-233.
[Crossref]
7.
Lee, K. G., Park, K. J., Seok, S., Shin, S., Park, J. Y., Heo, Y. S., et al. (2014). 3d printed
modules for integrated microfluidic devices. RSC Advances, 4(62), 32876-32880.
[Crossref]
8.
Liao, Z., Wang, J., Zhang, P., Zhang, Y., Miao, Y., Gao, S., et al. (2018). Recent advances in
microfluidic chip integrated electronic biosensors for multiplexed detection. Biosensors and
Bioelectronics.
9.
Macdonald, N. P., Cabot, J. M., Smejkal, P., Guijt, R. M., Paull, B., & Breadmore, M. C. (2017).
Comparing microfluidic performance of three-dimensional (3d) printing platforms. Analytical
Chemistry, 89(7), 3858-3866.
[Crossref]
10.
Samiei, E., Tabrizian, M., & Hoorfar, M. (2016). A review of digital microfluidics as portable
platforms for lab-on a-chip applications. Lab on a Chip, 16(13), 2376-2396.
[Crossref]
11.
Waheed, S., Cabot, J. M., Macdonald, N. P., Lewis, T., Guijt, R. M., Paull, B., et al. (2016). 3d
printed microfluidic devices: enablers and barriers. Lab on a Chip, 16(11), 1993-2013.
[Crossref]
12.
Whitesides, G. M. (2006). The origins and the future of microfluidics. Nature, 442(7101), 368.
[Crossref]
13.
Yi-Qiang, F., Hong-Liang, W., Ke-Xin, G., Jing-Ji, L., Dong-Ping, C., & Zhang, Y. J. (2018).
Applications of modular microfluidics technology. Chinese Journal of Analytical Chemistry,
46(12), 1863-1871.
[Crossref]
14.
Zeraatkar, M., Filippini, D., & Percoco, G. (2019). On the impact of the fabrication method on
the performance of 3d printed mixers. Micromachines, 10(5), 298.
[Crossref]
15.
Zhang, J., Yan, S., Yuan, D., Alici, G., Nguyen, N. T., Warkiani, M. E., et al. (2016).
Fundamentals and applications of inertial microfluidics: A review. Lab on a Chip, 16(1), 10-34.
[Crossref]
OceanofPDF.com

(1)
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_11
A Modified Bresenham Algorithm for
Control System of FDM Three-
Dimensional Printer
Ke Yu1, Zhisheng Zhang1   , Zhiting Zhou1 and Min Dai1
School of Mechanical Engineering, Southeast University, Nanjing,
211189, China
 
Zhisheng Zhang
Email: oldbc@seu.edu.cn
Keyword Modified Bresenham algorithm - FDM Three-dimensional
printer - MCU
1 Introduction
The stepper motor is a device that is controlled in an open-loop method
with many advantages, such as fast response speed, accurate movement and
strong anti-interference ability. With the aid of a stepper driver, a stepper
motor could complete a small angle move in a single step precisely. In the
case of the 3D printing, stepper motors are usually used to move the
extrusion nozzle of the printer to a particular location. After a 3D model is
sliced on the host computer, a series of G-codes are generated. According to
the feeding speed and target coordinates specified by the G-codes, a group
of stepper motors cooperate to complete the point-to-point motion. The
linear motion of the nozzle is made up from motors on three axes by
interpolation algorithm, through mechanical structures such as transmission
belts and pulleys. At present, three-axis interpolation algorithms which are
commonly used in 3D printing are DDA algorithm [1] and Bresenham

algorithm. The Bresenham algorithm was firstly proposed by Bresenham
[2] in 1965 and Angel [3] gave a method to speed it up by breaking the line
into segments for computer graphic systems, in 1991. Sun [4] also proposed
a parallel Bresenham algorithm to interpolate a line symmetrically. Dai [5]
applied the Bresenham algorithm on a control system for stepper motors.
This paper introduces a modified Bresenham algorithm. The improved
algorithm is more efficient than the conventional one and suitable to be
applied on the control system of the FDM 3D printer.
2 Bresenham Line Generation Algorithm
As a common algorithm to draw straight lines in computer graphics,
Bresenham algorithm is usually utilized in computer numerical systems.
Linear rasterization refers to a methodology that simulates a line with a
series of pixel points. The algorithm constructs a set of vertical grid lines
through the center of each row of pixels. According to the generation order
of a line from the starting point to the ending point, the algorithm calculates
and generates the intersection point of the target line and vertical grid lines.
Then the linear interpolation is performed.
(x0, y0) and (x1, y1) are used to represent the starting and ending
coordinates of the target line to be interpolated. The slope of the trajectory
line should be
(1)
If the slope k < 1, for every pixel increased in the x direction, whether a
pixel is increased in the y direction is determined according to the error
term epsilon, as shown in Fig. 1. Suppose the line starts at the origin [i.e. (0,
0)], with ε0 = 0, and x is increased by one step further then, i.e., x++. In this
case, εi+1 = εi + k. If εi+1>0.5, then the value of y should be increased by one
step further, i.e., y++, and εi+1 is substituted by εi+1 − 1 for the next
calculation. If the εi+1 ≤ 0.5, there is no step advanced in the y direction,
with the εi+1 is kept as the original value for the next calculation. This
operation is repeated in a loop until the trajectory reaches the final
coordinate in the x direction.

Fig. 1 Diagram of the decision process of Bresenham algorithm
(2)
(3)
(4)
There are a number of floating-point calculations in this loop, which
affects the efficiency of the process. In order to avoid this kind of floating-
point arithmetic, following transformation is applied on the formulas.
Let dy = y1 − y0, dx = x1 − x0, Ei = 2εi · dx, and the formulas then
become:
(5)
(6)

Through this transformation, integer data are used only on the
calculation to make the decision whether a step should be advanced in the y
direction, avoiding the floating-point arithmetic and division calculations
(Fig. 2).

Fig. 2 The decision process of conventional Bresenham algorithm (ei = Ei + 2dy − dx)
3 Modification on Bresenham Algorithm

There is still room for improvement on the computational efficiency of the
Bresenham algorithm. In the existing decision process, it is necessary to
determine for each step to feed. When the slope of the interpolated line is in
a small amount, there are several steps fed in the x direction with no feeding
steps in the y direction. In this case, if it is possible to calculate the number
of feeding steps on the same row of x axis, there is no need to make a
decision on whether or not to feed in the y direction after each step is
advanced in the x direction, thereby saving time and improving efficiency.
In other words, it is possible that a set of interpolation points on the same
row could be predicted at a time.
Before the introduction of the modified method, a slight transformation
takes place on the Formulas (5), (6), which were introduced in Sect. 2.
Let ei = Ei + 2dy − dx, and the formulas become:
(7)
(8)
This can make the formulas more simple for the subsequent work.
On this basis, straight interpolated lines with the slope in the interval [0,
1] are discussed here. Any straight lines whose slopes are not within this
interval can be adjusted by simply exchanging x and y.
For ease of understanding, a custom definition "feeding stair" is
introduced. When the slope of the straight line is in the interval [0, 1]. Since
the projection of the straight line in the x direction is longer than which in
the y direction, according to the conventional Bresenham method, one step
is certainly fed in the x direction after each decision formula is calculated,
and whether a step should be fed in the y direction will be based on the
result of decision. Under this circumstance, there will be a case where no
steps are fed on the y direction and a certain number of steps are fed on the
x direction. In this case, the steps fed in the x direction are defined as a
"feeding stair", and the width of this stair is the number of interpolated
points in the x direction (i.e. the number of steps fed on the x direction
increased by 1). Each time a step in the y direction is advanced, the whole
trajectory is defined to have risen by one stair.

Here a hypothesis is introduced:
Among the several feeding stairs obtained by the Bresenham method in
a straight line, except the first stair and the last stair, the width m of all other
stairs satisfies
(9)
where F stands for the downward rounding value of dx/dy.
Proof
Suppose there are m interpolated points on a certain stair, the
decision variables corresponding to these points are named by e1, e2, ...,
em, and the decision variable corresponding to the last interpolated point on
the previous stair is named by e0. Furthermore, the decision variable
corresponding to the last but one interpolation point on the previous stair is
named by e−1. (See Fig. 3.)
Fig. 3 A feeding stair with the width m
If there is more than one interpolated points on the previous feeding stair,
the following formulas could be derived directly:
(10)
(11)

(12)
where: e−1 ≤ 0; e0 > 0; em−1 ≤ 0; em > 0.
Substitute (10), (11) into (12):
(13)
Because em > 0, 
; and because e-1 ≤ 0,
, and here it could be derived that, 
.
According to the previous definition, 
.
Because em-1 ≤ 0, 
; and because e0 > 0,
, and here it could be derived that, 
.
According to the previous definition, 
.
Therefore, 
.
From this conclusion, the width of every other stairs must be F or F + 1,
except the first and the last stairs. If the decision variable of the last
interpolated point of a certain stair is e0, the number of feeding steps of the
next stair can be predicted by determining the sign of the decision variable
(set to ef) of the feeding step F (i.e. the Fth step) on this feeding stair.
As we calculate that ef = e0 + 2Fdy − 2dx, if ef < 0, the width of stair is
F + 1, and ef + 2dy is the value of the decision variable of the last point of
the next stair. Otherwise, the width of next stair is F, and the decision
variable of the last point of the stair is ef. Through this conclusion, the
number of steps to be fed on the same stair can be calculated by one time of
decision, and the efficiency of the algorithm is improved (Fig. 4).


Fig. 4 The decision process of the modified Bresenham algorithm
4 The Software Implementation of the Modified
Algorithm on the FDM 3D Printer
The work flow of the FDM 3D printing is briefly introduced in Fig. 5. After
the 3D model is drawn in the modeling software on the computer, it needs
to be converted into a STL format model and sliced in the slicing software.
The model is finally transformed into a series of G-codes which could be
easily realized by the control system of the printer.


Fig. 5 The process to complete the 3D printing on a FDM printer
The subsequent work is similar to the processing of G-code in GRBL
firmware. Each G-code is parsed into a structure in C language, named by
"block". In this structure, parameters which are required to run a G-code are
calculated, and the implementation process of the nozzle path is planned.
Then, using the parameters inside the structure, the pins of the MCU can be
controlled to emit a series of pulses in timer interrupts to control the
movement of stepper motors.
In a timer interrupt, Bresenham method could be applied to combine the
multi-axis motion planning, with the acceleration and deceleration process
of stepper motors. According to the previous introduction, the algorithm
needs to be applied with the consideration of the specific environment of a
3D printer. If the stepper motor starts directly at the feeding speed which is
preset by the G-code, out-of-step phenomenon may occur. The acceleration
and deceleration process is generally introduced during the movement to
avoid this problem. Taking trapezoidal acceleration and deceleration
process [6] as an example, since the total number of the stepper motor's
feeding steps in the x direction (Suppose dx > dy, otherwise the role of x
and y should be exchanged) in each block is predetermined according to the
G-code, the number of steps required for acceleration and deceleration
process is later calculated from the given constant acceleration value.
On the basis of the conventional Bresenham algorithm, the process for a
feeding step decision should be completed in a single interrupt of the
relevant timer. In other words, a certain step should be advanced in the x
direction and another step in the y direction should be advanced according
to the result of the decision formula in each timer interrupt. When it comes
to the modified Bresenham algorithm, it is still a single decision in a single
interrupt, but multiple steps in the x direction could be fed in this interrupt.
In each Interrupt Service Routine of the timer of the MCU, after each
step is given according to the Bresenham algorithm (as the MCU gives a
flip on the I/O port), the time that the next interrupt will last is calculated
and preset then [7]. In other words, the value of the timer's auto-reload
register is modified (For example, in the STM32 series, this value is
TIMx → ARR). Then the frequency of the stepper motor is changed by
adjusting the time period required to generate each pulse (Fig. 6).

Fig. 6 Trapezoidal curve that the stepper motor's feeding speed follows
Specifically, the number of the steps fed in the x direction is defined as
step_completed, and the value of step_completed is incremented by one for
each step of the x axis until the specified total number of steps by G-code
are completed. Based on this definition, the number of total steps fed in the
x direction at the end of the acceleration phase can be defined as
accelerate_until, and the number of total steps fed at the beginning of the
deceleration phase could be defined as decelerate_after. Then, when the
value of step_completed is smaller than accelerate_until, the stepper motor
is in the acceleration phase and the value of the auto-reload register of
MCU should be adjusted decreasingly according to the trapezoidal
acceleration law, in order to make raise the frequency of the stepper motor.
Similarly, when the value of step_completed is greater than
decelerate_after, the stepper motor is in the deceleration phase. Otherwise,
the motor is in a state of constant speed and the value of the auto-reload
register remains unchanged.
The specific steps to apply the modified Bresenham algorithm on the
3D printing control system are as follows:
(1) Calculate the value of dx, dy according to the coordinates of the target
point (x1, y1) from the G-code combined with the coordinates of the
nozzle (x0, y0) currently. Then, calculate the coordinates (x2, y2), (x3,
y3) where the stepper motor enters and exits the uniform motion.
 
(2) Through the decision variable e of each point, the straight trajectory of
the acceleration section is interpolated using the conventional
Bresenham algorithm, which is, the straight line from the starting
point (x0, y0) to the ending point (x2, y2) where the acceleration
 

section ends. The value of the auto-reload register is reset after every
step in the x direction is advanced during this process.
(3) In the linear interpolation of constant speed section [the straight line
from the starting point (x2, y2) to the ending point (x3,y3)], the
modified Bresenham algorithm proposed in this paper is used. The
specific method should refer to what was proposed in the previous
Sect. 3.
 
Firstly, the first row of interpolation (the 1st stair) is completed by the
conventional Bresenham method.
Secondly, when it is in the corresponding feeding step's interrupt
service function where the interpolation jumps from the first stair into the
second stair, some significant parameters of the second and third row could
be calculated and predicted.
The decision variable ef of the last interpolated point of the second stair
could be predicted by the decision variable e of the last interpolated point of
the first stair, and this process also determines the stair width m of the
second stair to obtain the number of interpolated steps on this stair. On this
basis, in each single interrupt of timer, m − 1 steps are fed directly on the x
axis and then, 1 step on the y axis is fed. This process should be looped
until the last but one stair of the line is fed.
Finally, the conventional Bresenham method is used again to complete
the last stair of interpolation.
(4) The interpolation process enters the trajectory of deceleration section,
which is the straight line from the starting point (x3, y3) of the
deceleration section to the ending point (x1, y1) of the whole process
planned by the G-code. This line of the deceleration section is
interpolated using the conventional Bresenham algorithm.
 
Through the application of the conventional Bresenham algorithm in the
3D printing control system, when it is in the process of acceleration and
deceleration, it is could be tricky to utilize the modified method introduced
in this paper because the value of the auto-reload register needs to be
changed after each pulse is sent, during the acceleration and deceleration
section. However, this limitation does not affect the efficiency of the
modified algorithm to a large extent. Generally speaking, the number of

feeding steps in the constant speed section will occupy a very large
proportion in a common G-code, where the modified algorithm can be
applied to improve efficiency.
5 The Hardware Design of the Control System
The mechanism about how the model is formed by a FDM 3D printer has
been elaborated in the last section. A common type of FDM three-
dimensional printer—XYZ type three-axis printer, which is shown in Fig. 7,
is chosen as the experimental platform, and the hardware environment for
application of the algorithm is going to be introduced as follows.
Fig. 7 The experimental FDM 3D printer
The 32-bit microcontroller STM32F103ZET6 is selected as the
controller of the 3D printer. With the ARM 32-bit Cortex-M3 CPU and
72 MHz maximum frequency, this controller is suitable for the occasion of

the 3D printer, where high calculation speed and adequate I/O ports are
required. As is illustrated in Fig. 8, a group of hardware devices are
designed to be interacted with the MCU.
Fig. 8 The hardware layout of the control system of a 3D printer
The position control of the nozzle from the MCU is realized by the
movement of the stepper motor. In this process, the MCU controls the
output pulse of the I/O port to the stepper motor driver through the program
command, and the Allegro A4988 chip is used as the stepping driver in this
3D printer. As a complete microstepping motor driver with a built-in
translator for easy operation, it could regulate the currents in each of the
two output full-bridges with fixed off-time PWM(pulse width modulated)
control circuitry [8]. Generally speaking, the built-in translator in the driver
is sequenced when there is a transition from low level to high level on the
STEP pin's input. Then the stepper motor is advanced by one step.
The A4988 driver is designed to operate bipolar stepper motors in full-
step, half-step, quarter-step, eighth-step, or sixteenth-step modes. As these
five modes of microstepping could be switched by changing the voltage on
logic inputs of pin MS1, MS2 and MS3, the sixteenth-step mode is chosen
to be the mode for the experiment while the logic inputs of these three pins
are all set to be high (which means these pins are set to be connected to the
VCC3.3 power port).
Some peripheral circuit is designed to meet the requirements for the
typical application of the A4988 driver. As an example, the application
diagram on the x-axis stepper motor is demonstrated in Fig. 9. The STEP
pin, DIR pin and ENABLE pin are connected to the I/O ports of the MCU
and respectively control the steps advanced, the direction and the activation
of the motor. And the four pins of OUT1A, OUT1B, OUT2A, OUT2B are

linked with the corresponding ports by jump wires on the four-phase
stepper motor. On this basis, another two duplicates of the driver modules
for stepper motors are placed on the PCB layout of the control system in
order to generate the motion on the y axis and z axis.
Fig. 9 The application on the x axis motor of the A4988 driver
The point-to-point motion of the nozzle in a single slice in a 3D model
is finally realized by sets of belts and pulleys. As these parts drive the
nozzle to a specific position, the transition from a pulse signal in MCU to
the movement of the nozzle is completed, as it is shown in Fig. 10.
Fig. 10 The transition of signals from the MCU

6 Experiment
6.1 Software Simulation
Before the test on the practical control system of a 3D printer, a C program
is firstly established and run on the computer with the CPU of Intel Core i7-
7700HQ to check the efficiency of the modified algorithm according to the
flowcharts of Figs. 2 and 4. This program is made to interpolate lines inside
the pixel grids on the computer without the interaction with hardware on the
3D printer, in order to do the most intuitive comparison.
By the comparison of the efficiency of the conventional and modified
Bresenham algorithm in Table 1, when the slope of the interpolated line is
1, the modified algorithm consumes almost the same time on the decision
process compared to the conventional one. However, the modified
algorithm works more effectively on the lines with small slopes. And the it
could save a greater proportion of time when the slope gets smaller.
Table 1 The comparison on the time required to interpolate a line on computer, from (0, 0) to a
particular point (μs)
 
(100, 100) (1000, 100) (10,000, 100)
Conventional algorithm 0.4
2.2
20.4
Modified algorithm
0.4
0.8
2.1
6.2 Hardware Implementation
As the STM32F103Z is chosen to be the MCU of the control system, the
experiment uses Keil MDK v5.14 as the development environment. Keil
MDK includes Arm C/C++ Compiler with assembler, linker, and highly
optimized run-time libraries. And it is possible to measure crucial
parameters in this environment, such as the running time of a program on
the corresponding MCU.
In order to verify the improvement on the efficiency of the modified
algorithm, a Keil project is established in C language. The codes include the
process to send stepping pulses to the stepper motors.
In this test, the Keil project is downloaded into the PCB of the control
system, and then the project starts running when Keil MDK supervises this
process. By setting a group of breakpoints in the program, results of how
long it takes for the decision process of the interpolation in both the

conventional and modified Bresenham algorithms could be achieved. The
results include the time for the whole decision process and the pulses'
generation, but without the delay period in the interrupt functions.
The values of the time consumed by the conventional and modified
Bresenham algorithms on the hardware environment are observed and
compared in Table 2.
Table 2 The comparison on the time required to interpolate a line on the control system of the 3D
printer, from (0, 0) to a particular point (μs)
 
(100, 100) (1000, 100) (10,000, 100)
Conventional algorithm 121.03
716.92
6216.92
Modified algorithm
120.69
605.81
4902.68
Time-saving ratio (%)
0.28
17.1
21.1
By the comparison of the time consumed in Table 2, the trend that the
modified method becomes more effectively with the decrease of the slopes
of lines still complies the discipline found in results achieved in the
simulation test. However, it doesn't seem to be so "effective" compared to
the software simulation, where it saves more than half of the time. The
reason is that the actions to output the pulse signals take a certain
percentage of the time that is not going to be influenced by the interpolation
algorithm. In conclusion, the modified algorithm could still work better
than the conventional one in practice.
7 Conclusion
A modified Bresenham algorithm is introduced in this paper. This improved
algorithm focuses on the simplification of the decision formulas and the
frequency reduction of the decision process, compared with the
conventional Bresenham algorithm. Taking into account the principle of
linear motion control in the control system of 3D printer, the paper gives
preconditions and specific procedures to apply the new algorithm on a FDM
3D printer to make it more efficient.
Also, the hardware design of the control system is introduced and the
environment for tests in practice is established. Finally, tests on software
simulation and hardware implementation are carried out to verify the
modified Bresenham algorithm's efficiency. As a result, for the

interpolation of a straight line with the slope of 10 or more in the
experimental control system, the modified algorithm could save about 17-
20% of the time consumed, which are hundreds of microseconds, on the
decision process.
Acknowledgements
The research work is supported by the National Natural Science Foundation
of China (Grant Nos. 51775108).
References
1. You, D. Z., et al. (2008). The realization of DDA interpolation algorithm with 2-axis motion.
Equipment Manufacturing Technology, 1, 41-43.
2.
Bresenham, J. E. (1965). Algorithm for computer control of a digital plotter. IBM Systems
Journal, 4(1), 25-30.
[Crossref]
3.
Angel, E., & Morrison, D. (1991). Speeding up Bresenham's algorithm. IEEE Computer Graphics
and Applications, 11, 16-17.
[Crossref]
4.
Sun, Y., & Kang, D. (2001). The parallel algorithm of Bresenham. Computer Engineering and
Applications, 37, 136-137.
5.
Dai, M., et al. (2017). Design of multi-step stepper motor coordinated control system based on
Bresenham algorithm. Presented at 24th International Conference on Mechatronics and Machine
Vision in Practice, Auckland, New Zealand.
6.
Zheng, J. X., & Zhang, M. J. (2007). Study of real-time interpolation algorithm with varying
interpolation period based on trapezoidal velocity profile. Machine Tool & Hydraulics, 35, 77-80.
7.
Divic, J., Duric, J., & Vrancic, K. (2014). Microcontroller implementation of dynamically
adaptable control of stepper motor with continuous second derivative of speed curve. Presented at
37th International Convention on Information and Communication Technology, Electronics and
Microelectronics (MIPRO), Opatija, Croatia.
8.
Lan, J., & Zhang, H. R. (2015). Design of micro stepping motor drive controller based on STM32.
Microcomputer & Its Applications, 34, 43-46.
OceanofPDF.com

(1)
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_12
Design and Experimental Study on the
Self-Balancing Foot Device
Rui Peng1 and Liang Han1  
School of Mechanical Engineering, Southeast University, Nanjing,
211189, China
 
Liang Han
Email: melhan@seu.edu.cn
Keywords Foot device - Balanced height - Brake element - Mechanical
modeling
1 Introduction
Table is a kind of common furniture with a flat top and a lower pillar, and
its development has a long history. Nowadays, different needs of people for
the table make it more variety [1]. Taking the square table as an example, it
can be aligned together to obtain a larger use area. It is also more space-
saving and easier to use, which causes it to be favored by offices, small and
medium-sized restaurants. Of course, the furniture of the table brings us
convenience together with a series of problems at the same time.
First of all, one of the problems that bother us is that the table is
constantly shaking during use. Due to the wear of the table legs or the
unevenness of the ground, the legs cannot be grounded at the same time,
causing the table to shake, which has a great impact on use.
In addition, we often need to align two or more tables together to
accommodate more guests in restaurants, hotels, cafes, conference rooms,
etc. However, because of many factors such as the flatness of the floor, the
difference in precision during the manufacture of the table and the wear of

the table legs, even the two tables that look exactly the same will usually
have a certain height difference, which causes it difficult to perfectly align.
And it has a certain influence on beauty or use.
Nowadays, there are many devices that can adjust the height of table
feet, but their adjustment methods are generally tedious [2]. At present, the
FLAT Tech has invented a hydraulic structure to adjust the height of the
foot device in allusion to the above situation [3]. However, the height of the
table is susceptible to the influence of ambient temperature due to the
characteristics of liquid expansion and contraction. Moreover, the adoption
of hydraulic structure for adjustment also puts forward high requirements
on the sealing of the footing device. The liquid in the device may leak and
lack stability under long working time. Therefore, in view of the above
problems, a footing device with mechanical structure that can automatically
balance the horizontal height is designed in this paper. Its advantages are as
follows: pure mechanical structure will not produce common problems of
hydraulic structure, which improves the stability of the footing device under
long working time and provides some references for the follow-up research.
2 Structure Design
2.1 General Planning
Take a four-foot square table as an example, as shown in Figs. 1 and 2. The
device is installed in the legs, including four groups of the foot and a link
block in the middle of them. Each group of the foot includes a support
element, a pulley and a set of elastic rope components. Four support
elements are uniformly distributed in a ring with the link block as the
center. The link block is connected with four groups of the elastic rope
components and each component includes a tension spring and two wire
ropes. Each group of elastic rope components is turned by a pulley and
connected with a support element. The link block will be pulled by elastic
rope when the height of a support element changes. And it will drive the
other three support elements and make their height change accordingly,
which changes the height of the table.

Fig. 1 Four-foot square table
Fig. 2 Foot device
After the table reaches the adjustable height, the brake element will
expand and push the piston wall. Then they will together squeeze the inner
wall of cylinder and produce a brake friction, which brings table self-
locking ability and improves the stability of the foot device.
2.2 Support Element Design
The main body of the support element is in the form of a piston cylinder, as
shown in Fig. 3. The cylinder is located at the uppermost end of the support
element and the opening end of the cylinder is arranged downward. The

piston is hollow and has four small grooves in its wall. One end of the brake
element is nested in the cylinder and can move up and down the axial
direction of the cylinder. The elastic rope component is connected with the
piston and is turned by a pulley arranged on the top of the cylinder. There is
no screw type structure in the device, and the height adjustment of the
support element is operated without the aid of tools. A spring is located
inside the piston and arranged in a cavity formed by the combination of the
cylinder and the brake element nested to generate an elastic support force
between the cylinder and the brake element which arranged along the axis
of the cylinder.
Fig. 3 Profile of support element
Small grooves in the piston wall allow the wall to expand when
squeezed by the brake element. The outer wall is provided with bulges,
which works together with the track set on the inner wall of the cylinder to
restrict the movement path of the piston. The spring is placed in a pipe
inside the cylinder, and the piston has a circular slot at the bottom that
provides room for the pipe to move up and down. The wire rope is
connected to the piston through the thread at the lower left of the piston.
2.3 Brake Element Design
The brake element enables the foot device to have the self-locking
performance, which means the ability to maintain the target height after the

height adjustment is completed and not be affected by the load applied to
the table, as shown in Fig. 4. The brake element comprises two rings and a
brake strip fixed on the inner side of each ring. The brake element is
installed on the outside of the spring pipe, the ring can slide along the pipe,
and the friction between the inside of the ring and the pipe can also provide
a certain braking effect. When the supporting element is under pressure, the
piston moves upward relative to the table base. Under the action of the
piston, the lower ring of the brake element will also move upward, making
the brake strip expand outward and contact with the inner wall of the piston.
The piston wall and cylinder wall are squeezed to create an axial friction
force to play a braking effect.
Fig. 4 Brake element
3 Mechanical Modeling
3.1 Piston Force Modeling
The brake friction provided by the brake element is the key to ensure the
performance of the device. Sufficient braking friction is required during the
design process to ensure that the load applied to the table during normal use
does not cause the height of the footing to change. Now take a single foot as
an example and assume that the cylinder is fixed, it is analyzed that when a
certain additional load P is applied to the foot, the force on the piston is
shown in Fig. 5.

Fig. 5 Force on piston
In Fig. 5, brake friction Ff is generated when the piston wall squeezes
against the cylinder wall. In order to make the table in use to maintain
stability, it is necessary to satisfy the following conditions:
(1)
(2)
In the formula, fmax is the maximum friction between piston and
cylinder, P0 is the maximum load generated by the table on a single footing
during normal use, Fg is the ground support force, Fp is the wire rope
tension, Fs is the spring force, Fb is the brake element pressure. Substitute
the relations of the above forces into Formula (2) to get:
(3)
(4)
In the formula, Fc is the pressure of cylinder inner wall on piston wall,
and it can be ignored due to the small deformation of piston wall.

Therefore, the pressure of piston wall on the brake strip N can be
approximately equal to Fc.
3.2 Brake Strip Force Modeling
The pressure of the piston on the brake element causes the brake strip
bending deflection. Now the force analysis on the brake strip is carried out
[4], as shown in Fig. 6.
Fig. 6 Force on brake strip
Both ends of the brake strip are fixed bearing, and its curve is shown in
Fig. 6. The bending moment at two points A and B, which are
approximately L/4 apart from each end, is zero. Therefore, these two points
can be regarded as hinge and the middle part can be regarded as the
compression bar supported by hinge at both ends. According to material
mechanics [5, 6], the differential formula of the flexural line is:
(5)

Since the deflection change of brake strip in this device is much smaller
than its length, (dω/dx)2 in Formula (5) can be omitted compared with 1,
and then
(6)
Since the brake strip is subjected to both force N and force Fb/4, the
deflections generated by them on brake strip can be calculated respectively
and the deflection curve of the strip can be obtained by superposition. The
deflection of the brake strip under the action of force N is
(7)
The deflection of the brake strip under the action of force Fb/4 is
(8)
In Formula (8),
(9)
So the deflection of the brake strip is
(10)
Substitute the following boundary conditions in the formulas above
(11)
The pressure N of piston wall on brake strip can be obtained. Therefore,
it is necessary to satisfy the following conditions to stabilize the table:

(12)
As a result, in order to stabilize the table, the load range applied to the
table during normal use should meet the following conditions:
(13)
4 Experiment
4.1 Processing of Experimental Sample
On account of the small size of the device, this device requires high
precision. Furthermore, considering that the piston outer wall and cylinder
inner wall contact condition, it is not suitable for hard materials. Therefore,
the traditional processing method cannot guarantee the processing accuracy
[7].
Moreover, the accuracy of 3d printing processing has been greatly
improved, which can satisfy the requirement. Meanwhile, the cost of a
single product is far lower than that of machine tool processing, and the
materials used can meet the purpose of the experiment, so 3d printing is
selected for processing experimental samples [8]. The sample completed is
shown in Fig. 7.

Fig. 7 Experimental sample
4.2 Experiment Design
1. Experimental purpose: 
Explore the influence of different combination of elastic elements,
including brake strips and springs, on the stability of the footing device and
find the best combination of elastic elements.
2. Experiment procedure: 
Choose different combination of elastic components to assemble into
multiple groups of footing devices for experiment. As shown in Fig. 8.

Fig. 8 Experimental foot device
Apply different loads to the table and measure the variation in the height
of the table under the corresponding loads and establish variation-load
curve.
The measuring point is 50 mm away from the edge of the table top, and
is located on the middle perpendicular line of each edge. The loading point
is located at the corner of the table.
The height variation of each foot shall be measured and the mean value
shall be taken for each set of footing devices.
4.3 Test System Design
In order to explore different elastic element for the influence of foot self-
locking performance, a large number of experiments are necessary.
Meanwhile, because the adjusting range is small (about 10 mm), and higher
adjustment resolution is required, it is necessary to design a special test
system. The test system includes displacement sensor [9], pressure sensor,
pressure loading device and controller [10]. The composition block diagram
is shown in Fig. 9 and the test device is shown in Fig. 10.

Fig. 9 Test system
Fig. 10 Test device
4.4 Result
Select several groups of different types of brake strip and spring
combination, including materials and size of strip, spring size (only 5
groups of results are listed in the following). Apply the pressure loading
device to the corner of table, and measure the current load according to the
pressure sensor, and use the laser displacement sensor to measure the
current height variation of the table. Figure 11 shows the height variation -
load curve of five different types of foot devices.

Fig. 11 Variation—load curve
According to the experimental result, it is obvious that the parameters of
the elastic element will affect the stability of the foot device, and the
stability in group 5 is the best. When the load is less than 6.5 kg, the
variation is less than 1 mm. When the load is less than 8.5 kg, the variation
is less than 2 mm. The brake strip of this group adopts silicon rubber, and
its size is 3*7.5*30 mm. The spring material is stainless steel, and its size is
1*10*50 mm.
5 Conclusion
In this paper, two common problems in the use of table are introduced, and
a foot device which can balance the horizontal height automatically is
designed. By analyzing the internal stress of a single foot subjected to
external load, the conditions that the table load of the foot should meet
when the height of the foot remains stable are given. At the same time, the
feasibility of the scheme is proved by experiments, and the influence of
elastic elements inside on the stability of the foot is studied. The
experimental result shows that the parameters of elastic components
(including brake strip and spring) have a great influence on the stability of
the footing when the size of piston cylinder is unchanged, and a set of
parameters of elastic components that can meet the requirements of use are

given, which provides a reference for the optimization of the stability of the
device in the future.
References
1.
Li, J. L. (2011). On the impact of lifestyle evolution upon the functions and forms. Central South
University of Forestry &Technoloy.
2.
Tao, L. H, Duan, Y. G, & He, L. R. (2016). A multi-function desk design. Science & Technology
Vision, 2016(08):242 + 228.
3.
Pike, A. L., Gilmore, D. B., & Hope, R. L. (2018). Support for supporting a structure on a
surface. US9909709, March 6 2018.
4.
Liu, H. B., Liu, Y. L., & Shi, X. F. (2018). Precise derivation of beam deflection equation.
Journal of Huaqiao University (Natural Science), 39(06), 840-843.
5.
Liu, H. W. (2004). Mechanics of materials (I) (pp. 177-180). Beijing: Higher Education Press.
6.
Yu, F. (2017). The analysis on the force and deformation of tubing string in three-dimensional
curved well bore based on elastic rod theory. China University of Petroleum.
7.
Sen, S. (2013). Research on machining process of piston cylinder for vacuum casting line of
epoxy resin. High Voltage Apparatus, 49(06), 109-112.
8.
Park, C., Kee, W., Lim, H.-P., & Park, S.-W. (2019). Combining 3D-printed metal and resin for
digitally fabricated dentures: A dental technique. The Journal of Prosthetic Dentistry.
9.
Wang, Z. G., & Zhong, H. M. (2019). Mechanical wear detection technology based on laser
sensor. Laser Journal, 40(08), 18-21.
10.
Zhang, J. F, Wu, Z. J., Feng, P. F., Yu, D. W., & Zhao, Y. M. (2017). Force loading device.
Beijing. CN102156041A, August 17, 2011.
OceanofPDF.com

(1)
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_13
Dynamic Characteristics Analysis and
Optimization Design of Cross-Beam
Assembly in 3D Printer
Weijie Chu1   , Limiao Gu1, Xiaolong Liu1 and Fang Jia1
Department of Mechanical Engineering, Southeast University,
Nanjing, 211189, China
 
Weijie Chu
Email: chuweijie138@163.com
Keywords FDM 3D printer - Cross-beam assembly - Dynamic
characteristics - Optimization design
1 Introduction
As a subversive manufacturing technology, additive manufacturing solves
the forming problem of many complicated structural parts. As the carrier of
sprinkler movement of 3D printer, the dynamic performance of the beam is
closely related to the processing performance, which directly affects the
dimensional accuracy and surface quality of the processed parts. Therefore,
it is of significance to study the dynamic characteristics of cross beam
assembly.
Many scholars at home and abroad have studied the accuracy of FDM
3D printers from different angles. Molong et al. [1]. proposed a finite
preview filtered B-spline method for minimizing the error of tracking the
desired trajectory. Dixit et al. [2]. proposed a method based on Taguchi
parameter design to find the optimal factor of each machining parameter of
the machine. Marwah [3] and others studied the influence of the thickness

of the table 3D printer, the printing speed, the nozzle temperature and the
bed temperature on the dimensional accuracy of the parts. Zhu [4] and Zhou
et al. designed a simple and efficient nozzle cooling device to improve the
printing accuracy caused by the softening of the printing material in
advance. Gao and Zhou [5] and others made a flow field analysis for the
printer nozzle, and then optimized the nozzle structure to improve the
printing accuracy.
In order to improve the printing accuracy of 3D printers, many scholars
at home and abroad have done a lot of research, but few scholars have
researched from the perspective of the dynamic characteristics of the cross-
beam. This paper analyzes the dynamic characteristics of a huge size FDM
3D printer cross-beam assembly, and proposes a new cross-beam
component structure based on topology optimization, and optimizes the size
of the new cross-beam structure to improve the dynamics while achieving
lightweight features, which in turn improve the printing accuracy.
2 Modal Analysis
Modal analysis is the basis of dynamic response analysis. Its essence is to
describe the structural dynamics according to the inherent characteristics of
the structure, including frequency, damping and mode shape [6-8].
Static simulation analysis shows that the maximum stress of the cross
beam is 3.20 MPa. Then the calculated stress results were imported into the
modal analysis, and the prestressed modal analysis was performed on the
cross-beam assembly. The first 12 natural frequencies and vibration modes
of the crossbeam were obtained through simulation analysis. The specific
results are shown in the following (Table 1).
Table 1 The first 12-order natural frequencies of the cross-beam assembly
Order Frequency
(Hz)
Mode of vibration
1
93.991
Slide spray head twists around Y axis and bends along Z direction in the
middle of cross beam
2
122.58
The middle part of the beam bends upward in Z direction
3
135.13
Torsion of both ends of the beam in X direction
4
143.50
The beam bends downward in Z direction and twists at both ends in X
direction

Order Frequency
(Hz)
Mode of vibration
5
168.38
Torsion of cross beam around Y axis
6
174.59
Cross beam bends backward along the X direction and twists around the X
axis at both ends
7
215.24
The middle of the beam is bent in the X direction, and the sides are bent in
Y the Z direction
8
238.37
Torsion of slide around Z-axis
9
287.33
Torsion of slide around X-axis
10
320.47
Torsion of slide around Y-axis
11
339.31
The beam sways up and down around the Z-axis
12
372.83
The beam swings up and down around the Z-axis and twists around the Y
axis
3 Modal Test
In order to verify the correctness of the established finite element model, a
modal test was performed on the cross-beam assembly. The test
environment at the site is shown in Fig. 1.
Fig. 1 Equipment field test environment

The 5th-order experimental modes identified by the modal test
correspond to the 2nd, 4th, 5th, 7th, and 9th modalities of the computational
modes, respectively. It can be seen from the Table 2 that the error between
the experimental mode and the calculated modal frequencies is within 4%,
and the vibration mode is consistent, so the finite element simulation model
of cross-beam assembly has high accuracy.
Table 2 Comparison of experimental modal and calculated modal frequencies
Order Test mode (Hz) Computational mode (Hz) Errors (%)
2
123.16
122.58
0.4
4
145.89
143.50
1.6
5
161.53
168.38
4
7
219.41
215.24
1.9
9
277.16
287.33
3.5
4 Harmonic Response Analysis
4.1 Harmonic Response Load Setting
From the harmonic response analysis, the true response value of the cross-
beam to the actual load at its natural frequency can be obtained [9, 10].
When the printer accelerates or decelerates at maximum acceleration of
1 g, the corresponding X-direction contact force of the gear and the rack are
1185 N and 1067 N, respectively, and the Y-direction contact forces are
432 N and 416 N, respectively. Therefore, the driving forces of 1185 N and
432 N were applied in the X and Y directions, respectively, at the gear
meshing positions on both sides of the X-axis. According to the modal
analysis result, the load excitation frequency band was set to 0-600 Hz. The
harmonic response curve of the point P1 (B) in the middle of the cross-
beam and the point P2 (A) in the nozzle of the printing nozzle was selected
as the evaluation index. The location of the two key points is shown in
Fig. 2.

Fig. 2 Response point location
4.2 Harmonic Response Results Analysis
According to the design requirements, the positioning accuracy of the X
and Y axes is 0.05 mm. The maximum displacement frequency response in
X and Y directions are shown in Table 3, and it is within the design
requirements. So the designed 3D printer meets the design requirements.
Table 3 Maximum displacement of measuring point of sprinkler
Direction
Frequency (Hz) Maximum displacement value (mm)
Point P1 X direction 144
0.0253
Point P1 Y direction 123
0.0263
Point P2 X direction 144
0.0191
Point P2 Y direction 123
0.0263
5 Cross-Beam Component Optimization Design
5.1 Overall Topology Optimization of Cross-Beam Component
In order to obtain the optimal size of the component, it is necessary to
optimize the size of the component structure [11].
Topology optimization [12-14] can be implemented with the Shape
Optimization module in Workbench. The middle of the cross-beam

component was used as the topology optimization area, all of which were
divided by hexahedral mesh, and the sides of the cross-beam component
and the nozzle slide were used as non-optimized, such as Fig. 3 shows.
Fig. 3 Schematic diagram of the cross-beam component topology optimization area
The load setting is consistent with the harmonic response analysis. The
optimization target was set to remove 40% of the material, and the topology
optimization result in the middle of the cross-beam component is shown in
Fig. 4.

Fig. 4 Cross-beam component topology optimization results
In Fig. 4, red is the part of the material that can be removed, and gray is
the part of the material that needs to be retained.
New cross-beam component model was created as shown in Fig. 5.
There are 9 ribs inside the cross-beam component, the thickness of the ribs
is 20 mm, the spacing of the ribs is 160 mm, and the bottom of the cross-
beam component is provided with a weight reducing hole with a diameter
of 95 mm.

Fig. 5 New beam model
Static analysis of the new model shows that the maximum deformation
of the cross-beam component is 0.024 mm, which is located in the middle
of the cross-beam component (Fig. 6).
Fig. 6 Cross-beam component deformation cloud

The modal analysis was carried out on the optimized cross-beam
component. The first six natural frequencies of the cross-beam component
are shown in Table 4.
Table 4 The first six natural frequencies of the cross-beam component
First
Second
Third
Fourth
Fifth
Sixth
166.7 Hz 203.8 Hz 408.5 Hz 534.9 Hz 612.8 Hz 620.1 Hz
According to the simulation results, after the topology optimization, the
total mass of the cross-beam component is reduced from 134.19 to
76.48 kg, and the maximum static deformation of the cross-beam
component increases from 0.02 to 0.024 mm. The increase of deformation
is small, and the first three natural frequencies increase to a certain extent.
The frequency has been increased to some extent. It can be seen that the
effect of cross-beam component topology optimization is obvious, and the
preliminary lightweight design of the cross-beam component is realized.
5.2 Cross-Beam Component Size Optimization Design
To determine the optimal size of the cross-beam, it is necessary to optimize
the size. Multiple sets of cross-beam rib thickness, rib spacing and cross-
beam bottom hole diameter are used as design variables (input parameters),
and the maximum static deformation value of the cross-beam component
and the first natural frequency are used as constraints, and the minimum
mass of cross-beam component is taken as the optimization objective to
optimize the size of cross-beam. The mathematical model of the optimized
design is
(1)
Considering the objective factors of computer performance and
optimization of design space, the size optimization design variables of the
cross-beam are set to six, including three rib thicknesses, two rib spacings
and cross-beam bottom hole diameters. Considering the symmetry of the

two sides of the cross-beam, it is only necessary to mark one side size, as
shown in Fig. 7.
Fig. 7 Schematic diagram of size optimization parameters
The relevant parameters of the cross-beam after topology optimization
are shown in Table 5.
Table 5 Related parameters of the cross-beam after topology optimization
Parameters
t1
(mm)
t2
(mm)
t3
(mm)
d1
(mm)
d2
(mm)
s1
(mm)
u
(mm)
u (Hz) m
(kg)
Numerical
values
20
20
20
160
160
95
0.024
166.67 76.48
According to the constraints, the optimal point is obtained, and the
values are shown in Table 6.
Table 6 Related parameters of optimal point
t1 (mm) t2 (mm) t3 (mm) d1 (mm) d2 (mm) s1 (mm) u (mm)
u (Hz) m (kg)
16.626
16.893
16.858
157.5
158.15
92.079
0.023474 168.83 75.735
Compare the above-mentioned optimal design point related parameters
with the relevant parameters without cross-beam optimization design,
which are listed in Table 7. It can be seen that the deformation of the whole
cross-beam is slightly increased, but it is much smaller than the limit value,
which can be ignored. The frequency has been improved by nearly 10%
before optimization, and the dynamic performance of the cross-beam has
been improved to some extent. At the same time, the cross-beam mass has
decreased by 43.6%, which greatly reduces the cross-beam mass.

Table 7 Comparison of performance before and after cross-beam optimization
Beam
structure
u
(mm)
Deformation
improvement
ratio (%)
u (Hz) First order natural
frequency
improvement ratio
(%)
m
(kg)
Total weight
improvement
ratio (%)
Unoptimized
0.020
0
154.30 0
134.19 0
Optimized
0.023
−15
168.83 +9.4
75.735 −43.6
Taking into account the actual processing and other factors, the size
parameters of the candidate design point 2 is approximately rounded, and
the optimized structure size of the cross-beam is finally obtained, as shown
in Table 8.
Table 8 Dimensions after the cross-beam is rounded
t1 (mm) t2 (mm) t3 (mm) d1 (mm) d2 (mm) s1 (mm)
16.5
17.0
17.0
157.5
158.0
92.0
6 Conclusion
In this paper, the dynamic characteristics of the cross-beam assembly of a
large FDM 3D printer are studied. The prestressed modal analysis of the
cross-beam was executed. Then the modal test was implemented to verify
the correctness of the established finite element model. The harmonic
response analysis of the cross-beam assembly was executed, and it is found
that the displacement response results are higher than the printer design
accuracy, that is, the printer cross-beam assembly meets the design accuracy
requirements.
Finally, Workbench was used to optimize the design of beam
components with larger mass, and the structure and optimal size of the
cross-beam were obtained. The first-order natural frequency is increased by
10% while the weight loss of the cross-beam is 43.6%, which improves the
dynamic characteristics of the cross-beam and helps to improve the printing
accuracy of the printer. It provides a valuable reference for the design of the
3D printer.
Acknowledgements
The authors greatly acknowledge the grant of 1001 Group which supported
this research and professors who provided suggestions.

References
1.
Duan, M., Yoon, D., & Okwudire, C. E. (2018). A limited-preview filtered B-spline approach to
tracking control—with application to vibration-induced error compensation of a 3D printer. In
Mechatronics (pp. 287-296).
2.
Dixit, N. K., Srivastava, R., & Narain, R. (2016). Dimensional accuracy improvement of part
fabricated by low cost 3D open source printer for industrial application. In 2016 10th
International Conference on Intelligent Systems and Control (ISCO) (pp. 1-6), Coimbatore.
3.
Marwah, O. M. F., Yahaya, N. F., Darsani, A., et al. (2019). Investigation for shrinkage
deformation in the desktop 3D printer process by using D-OE approach of the ABS materials.
Journal of Physics: Conference Series, 1150, 012038.
4.
Zhu, L., Zhou, M., Gao, Q., et al. (2018). Temperature field analysis and structure optimization
of FDM 3D printer sprinklers. Combined Machine Tool & Automatic Processing Technology,
534(08):23-28.
5.
Qiang, Gao, Min, Zhou, Lili, Zhu, et al. (2018). Flow field analysis and structural optimization
of nozzles in FDM 3D printers. Combined Machine Tools and Automated Machining Technology,
537(11), 34-38.
6.
Sun, S. (2008). Research on metal structure dynamics of gantry crane. Wuhan University of
Technology.
7.
Xie, Z., Shi, K., Liu, B., et al. (2013). Dynamics simulation of a cantilever cross-beam of a laser
cutting machine. Combined Machine Tool & Automatic Processing Technology, 07, 19-21.
8.
Wang, Z., Zhang, Q., & Wang, C. (2014). Finite element analysis and experimental study of laser
cutting machine body. Forging Equipment and Manufacturing Technology, 03, 44-47.
9.
Gu, Y. (2018). Structure design and research of large-size dual-thread parallel FDM printer.
10.
Zhang, Y., Zhang, R., Zhu, L., Zhao, C., et al. (2018). Analysis of dynamic characteristics and
influencing factors of shearer rocker. Vibration and Shock, 37(317(09)), 122-127.
11.
Liu, C., Tan, F., Wang, L., et al. (2016). Research on optimization design of column structure for
dynamic performance of machine tool. Journal of Mechanical Engineering, 52(3), 161-168.
[Crossref]
12.
Luo, Z., Chen, L., Huang, Y., et al. (2004). Topological optimization design of continuum
structures. Advances in Mechanics, 04, 463-476.
13.
Yang, W. (2007). Research on several problems of topology optimization of complex mechanical
structures. Dalian University of Technology.
14.
A new solution for topology optimization problems with multiple loads: The guide-weight
method. Science China (Technological Sciences), (06), 1505-1514 (2011).
OceanofPDF.com

(1)
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_14
Design and Experimental Research of
Automatic Tightening Method of Rubber
Strip on the Side of Office Screen Panel
Ruonan Wang1, Liang Han1   , Jinghui Peng1 and Rui Peng1
School of Mechanical Engineering, Southeast University, Nanjing,
211189, China
 
Liang Han
Email: melhan@seu.edu.cn
Keywords Tightening of rubber strip - Mechanism design - Neuron PID -
Tightening experiment
1 Introduction
In modern office space, screen desks are used worldwide for its user-
friendly spatial segmentation. Screen panel is an important component of
screen desk. With the continuous updating of technology, the continuous
increase of varieties, the gradual formation of specialized production and
the continuous improvement of management level, the office furniture
industry is developing rapidly. After years of rapid development, the
number of enterprises, the number of employees and the benefits of the
office furniture industry have increased by several times [1]. The reason is
that the entry threshold of the office furniture industry is low, and it is a
labor-intensive industry. However, in recent years, the economic benefits of
the office furniture industry have dropped significantly. The reason is that
the labor cost is getting higher and higher. At present, the screen surface
fabric decoration of the wooden screen desk in the office furniture industry

mainly relies on manual operation and stirrups, resulting in lower
production efficiency. In this paper, for the problem of manual tightening of
the rubber strip on the side of the office screen panel at home and abroad,
an efficient method for automatic tightening of rubber strips on the side of
office screen panel is proposed. In order to verify the rationality of the
method, an automatic tightening experimental device was built and the
relevant tightening experiment was carried out. The preliminary
experimental results show that the design of the automatic tightening
method for the rubber strip on the side of the office screen panel is
reasonable and feasible. The proposed method provides a design basis for
practical production device of automatic tightening of the rubber strip on
the side of the office screen panel.
2 Structure Design
The mechanical part of the system mainly includes a flexible fixing system
for the screen panel, a rubber strip feeding system and a rubber strip
pressing system. The three-dimensional diagram of the rubber strip
automatic tightening system is shown in Fig. 1. Since the screen has six
sizes, it is necessary to design a screen flexible tightening system for
flexible fixing of different sizes of strips. The diameter of the rubber strip is
3 mm, and the rubber strip needs to be fed in an orderly manner through the
strip feeding system. The rubber strip pressing system is vertically hoisted
on the two-dimensional gantry screw module. Under the action of the
gantry screw module, the rubber strip pressing system can smoothly move
around the screen panel and embed the strip on the side of the screen panel.

Fig. 1 The three-dimensional diagram of the rubber strip automatic tightening system. (1) Flexible
fixing system for the screen panel, (2) rubber strip feeding system, (3) rubber strip pressing system
2.1 Screen Panel Flexible Fastening System Design
The screen flexible fastening system is used for positioning clamping of 6
different size screen panels. Since the screen is only changed in length, the
designed flexible fixing system remains unchanged in the width and
thickness directions. The system consists of a positioning centering
clamping mechanism and a pneumatic sliding table. The positioning
centering clamping mechanism includes a left chuck, a right chuck, a left-
hand thread, a right-hand thread, a positioning support plate, and a motor.
The top of the pneumatic slide is fixed to the limit groove. When the rubber
strip pressing system works, the four pneumatic sliding tables take off and
fall orderly, which not only realizes the fixing of the screen panel, but also
ensures the smooth operation of the rubber strip pressing system around the
screen panel. The three-dimensional design of flexible fixed screen panel is
shown in Fig. 2.

Fig. 2 The three-dimensional design of flexible fixed screen panel. (1) Left chuck, (2) right chuck,
(3) left-hand thread, (4) right-hand thread, (5) positioning support plate, (6) motor, (7) pneumatic
sliding tables, (8) limit groove
2.2 Rubber Strip Feeding System Design
The rubber strip is soft and has a diameter of 3 mm, so the feeding is carried
out by means of winding release and friction feeding. The rubber strip
release device is mounted above the rubber strip press-in system, and the
feed of the rubber strip is fed through two sets of friction wheel feed
systems. The smooth feeding of the rubber strip requires the strip release
device and the synchronous drive of the two sets of friction wheels. The
rubber strip is soft and the surface friction coefficient is not large, the
friction wheel feed system must have a sufficient driving force if it is
desired to ensure smooth feeding of the rubber strip. In order to increase the
driving force, we have focused on the design of the drive wheel. The force
analysis of the rubber strip is shown in Fig. 3.
Fig. 3 The force analysis of the rubber strip

Regarding the design of the drive wheel, we have analyzed the force of
the rubber strip based on the existing research [2]:
(1)
(2)
(3)
, so the drive wheel is designed as a V-shaped groove to
increase the driving friction. The design of the driven wheel uses a stainless
steel toothed wheel as the driven wheel. Because the toothed drive wheel
has the characteristic of increasing friction, it can grab the rubber strip well.
The three-dimensional diagram of the friction wheel feeding feed
mechanism is shown in Fig. 4.

Fig. 4 The three-dimensional diagram of the friction wheel feeding feed mechanism. (1)
Synchronous motor, (2) primary reducer, (3) drive wheel, (4) driven wheel
2.3 Rubber Strip Pressing System Design
The rubber strip pressing system is hoisted on the two-dimensional screw
module, and moves around the screen panel under the driving of the screw
module. The screen pressing system is the core part of the whole system.
The three-dimensional design of the strip pressing system is shown in
Fig. 5.

Fig. 5 The three-dimensional design of the strip rubber pressing system. (1) Rubber strip release
mechanism, (2) rubber strip buffer mechanism, (3) friction wheel feed mechanism, (4) cutting
mechanism, (5) pressure roller mechanism, (6) dragging mechanism
3 Multi-motor Synchronous Control System
In most industrial production, the traditional PID controller is mainly used
to improve the response rate of the controller. Traditional PID controller is
simple to operate and easy to realize the required motion, but it is difficult
to tune the PID parameters. If the speed of the motor can not be well
controlled, the soft and elastic rubber strip may be broken during
movement. The smooth embedding of rubber strips requires synchronous
and cooperative motion of eight motors. For such a non-linear, strong
coupling control object, the traditional PID controller is not enough to

achieve the purpose of motion. [3, 4] Because of the strong non-linear
approximation ability and self-learning ability of the neural network, people
in recent years have formed the neural network PID control by combining
the BP neural network algorithm and the traditional PID. The control can
automatically adjust the parameters of the controller and solve the difficult
problem of setting the parameters of the PID. In the process of in-depth
study, it is found that the traditional BP neural network algorithm has many
advantages, but it has the disadvantages of slow convergence speed and
easy to fall into local minimum. In this study, by introducing inertia term,
momentum term and improving learning rate strategy, the PID controller
based on improved BP neural network is redesigned to improve the
performance.
3.1 Modeling of Permanent Magnet Synchronous Motor
Permanent magnet synchronous motor (PMSM) has the characteristics of
high energy density, light weight, small volume, easy control and high
precision. So PMSM is used in this study [5]. The PMSM is shown in
Fig. 6.

Fig. 6 The model of PMSM
Stator voltage formula of permanent magnet synchronous motor in
three-phase static coordinate system:
(4)
The flux formula is shown as follows:

(5)
The torque formula is shown as follows:
(6)
In the formula, 
—phase voltage of three-phase winding A, B
and C; 
—line current of three-phase winding A, B and C;
—full flux linkage of three-phase winding A, B and C;
—A, B, C three-phase stator winding self-inductance;
—mutual inductance between phase A and phase B stator
winding; 
—mutual inductance between A phase and C phase
stator winding; 
—mutual inductance between B-phase and C-
phase stator winding; 
—stator resistance; 
—rotor permanent magnet
flux linkage; 
—the angle between axis A and axis D of DQ shafting.
From the above voltage formula and flux formula, it can be seen that the
voltage and flux linkage in ABC three-phase static coordinate system are
relatively complex and vary with the relative position between stator and
rotor. In order to facilitate the analysis and research, the model is further
simplified from three-phase stationary coordinate system to two-phase
rotating coordinate system. The simplified model is shown in the Fig. 7.

Fig. 7 The simplified model of PMSM
According to the principle of magneto motive force equivalence, the
static ABC coordinate system is transformed into the rotating dq coordinate
system, and the transformation relationship is obtained as follows:
(7)
The conversion from static ABC coordinate system to rotating dq
coordinate system is equivalent to the equivalent of permanent magnet
synchronous motor to DC motor.
The voltage formula of permanent magnet synchronous motor under DP
shafting at this time:
(8)

(9)
The electromagnetic torque formula is shown as follows:
(10)
In the formula, 
—stator voltage dq axis component; 
—stator
current dq axis component; 
—stator inductance component on dq
axis; 
—stator flux component on dq axis;
—electromagnetic
Torque of Motor; 
—rotor speed; 
—stator current; β—angle between
d-axis and stator magneto motive potential.
3.2 Optimized BP Neural Network Algorithms
As shown in Fig. 8, BP neural network is a kind of multi-layer feedforward
neural network with hidden layer in its structure [3, 6]. Gradient descent is
used to adjust the connection weights of each neuron in the network, which
can minimize the error of the network.

Fig. 8 Structure of BP neural network
In the learning stage of training network, forward calculation formula is
shown as follows:
(11)
In the formula, 
—input of the i-th neuron in the hidden Layer; 
—output of the i-th neuron; 
—input of the k-th neuron in the lower
layer of 
; 
—output of the k-th neuron.

After calculating the output value of the system, the calculated output
value is compared with the expected output value. If there is a deviation
between the calculated output value and the expected output value, the
deviation needs to be fed back to the system to correct the connection
weights between layers so that the calculated output of the neural network
is consistent with the expected output. The weight correction formula is as
follows:
(12)
Formula (13) can be obtained from Formula (12):
(13)
The traditional BP neural network uses the stochastic gradient descent
algorithm to make the weights better. The convergence speed of the
traditional BP is slow, and it is easy to fall into the local minimum. In this

study, the gradient momentum term optimization algorithm is introduced to
improve the convergence speed.
The weight correction formula after introducing the gradient momentum
term is as follows:
(14)
In the formula, 
—gradient momentum accumulated in iteration of
loss function; 
—gradient momentum accumulated during iteration of
loss function; 
.
3.3 Design of PID Controller Based on BP Neural Network
Digital PID controller can be divided into position PID control and
incremental PID control. The position PID control output sampling time
data is related to the output data of the previous time. It needs to accumulate
the deviation, and the calculation is huge. So the position PID control is not
easy to apply to this system. BP neural network algorithm is easy to
implement, and it can also approximate arbitrary non-linear model. Using
the self-learning ability of the neural network, we can find the best
proportion, integral and differential parameters in the operation of the
system. The PID controller of BP neural network consists of two parts:
traditional PID controller and neural network algorithm. The output of the
neuron in the output layer of the neural network corresponds to the PID
parameters respectively, so that the PID parameters can be adjusted by the

self-learning of the network. The PID structure of BP neural network is
shown in the Fig. 9.
Fig. 9 The PID structure of BP neural network
The expression of incremental PID is as follows [7]:
(15)
In the formula, k—sampling number; u(k)—k-time system output value;
e(k), e(k − 1)—deviation between input and output of system at K and (k-1)
time; 
—proportional constant; 
—integral constant; 
—differential
constant.
In this study, a three-layer BP neural network is used, and its structure is
shown in Fig. 10.

Fig. 10 Three-layer BP neural network
The calculation formula is as follows:
(16)
In the formula, 
; g(.) = 1/2(1 + tanh(x)).
The mean square error function is as follows:

(17)
3.4 Simulation Experiment Analysis
Although the system requires eight synchronous motors to work together,
the control algorithm for each motor is the same. Therefore, this simulation
experiment built a traditional PID control system and a BP neural network
PID control system in the Matlab/Simulink environment. The simulation of
BP neural network PID is realized by writing S function [8]. The simulation
results are shown in the Fig. 11.
Fig. 11 The simulation results
It can be seen from the figure that the control effect of the optimized BP
neural network PID control algorithm is obviously better than the
traditional PID control algorithm.

4 Tightening Experiment
In order to verify the rationality of the design, a complete prototype was
built and preliminary experiments were carried out. The experimental
prototype is shown in the Fig. 12.
Fig. 12 The experimental prototype. (1) Lead screw module, (2) screen board, (3) flexible fixed
platform for screen panel, (4) frame, (5) supporting device, (6) indenter mechanism
Through the cooperation of the speed parameters of the rubber strip
release motor and the rubber strip friction feed motor, the rotary motor of
the indenter device and the running motor of the indenter device, the best
effect of the rubber strip embedded in the screen plate was debugged. The
experimental sample obtained is shown in Fig. 13. Table 1 is the
experimental data of total rubber strip feed under multi-motor synchronous
drive. The experimental data of the length of the rubber strip winding is
shown in the Fig. 14.

Fig. 13 The experimental sample
Table 1 Tightening experimental data
Length of tape
winding
(mm/2 min)
Speed of
tape
release
motor
(mm/s)
Speed of
front
friction
wheel
motor
(mm/s)
Speed of
rear
friction
wheel
motor
(mm/s)
Speed of
drag
motor
(mm/s)
Speed of
rotating
motor
(mm/s)
Speed of XY
directional
motor (mm/s)
726
6
8
8
8
6
10
945
8
10
9.6
9.4
7
12
1338
12
14
13.8
12
9
16
1675
14
16
15.8
14
11
18
1917
16
18
17.6
16
13
21
2275
19
22
21.6
19
17
24
2773
23
26
25.4
23
21
28
3365
28
31
30.6
27
25
33
3847
32
35
34.2
32
30
37
4214
35
38
37.6
35
33
40

Fig. 14 The experimental data
The width of the groove on the side of the bare leaky screen was 3 mm.
When the clamping experiment was carried out, the screen was wrapped
around tablecloth with thickness of 0.5 mm, and the width of the groove
became 2 mm. Rubber strip diameter is 3 mm, So the deformation of rubber
strip in the process of embedding was related to the quality of rubber strip
embedding. One of the important factors affecting the deformation of
rubber strip was whether the motors were synchronized and coordinated. By
analyzing the experimental data, it can be concluded that the total winding
length of the tape in two minutes was nearly linear with the speed, which
provided a guarantee for the smooth embedding of the tape. The
preliminary experimental results showed that the rubber strip can be
embedded automatically by the method proposed in this study. The width of
the groove on the side of the covered screen plate was smaller than the
diameter of the rubber strip, which caused the hoop force was insufficient
and the rubber strip was easy to fall off. It is believed that the effect of
stirrups will be better in the future by optimizing the experimental

conditions. Better clamping effect can be obtained by optimizing
experimental conditions in the future.
5 Conclusion
In this paper, a method of automatic tightening of rubber strip on the side of
office screen panel is described. Through mathematical modelling static
analysis, the rationality of the design was verified. In addition, the task of
multi-motor synchronous control used in the automatic tightening method
of rubber strip on the side of office screen panel was analyzed in this paper.
By comparing the PID and neuron PID control algorithms, the simulation
results of MATLAB showed that neuron PID control had strong robustness.
In order to verify the rationality of the method, an automatic tightening
experiment device was set up and relevant experiments were carried out.
The preliminary experimental results showed that the design of automatic
tightening method for rubber strip on the side of office screen panel was
reasonable and feasible. This method provided a design basis for the
practical production device of automatic tightening of rubber strip on the
side of office screen panel.
References
1. Yong, J. (2015). A preliminary study on the relationship between modern corporate culture and
modern office environment. Interior Design, 25-27.
2.
Wang, Y. (2015). Research and design of FDM rapid prototyping feed system. Huazhong
University of Science and Technology.
3.
Cui, B., Li, Y., & Duan, Y. (2011). Spanning slab system based on neural network PID control.
Journal of Shenyang University of Technology, 33(20), 188-192.
4.
Hu, J., & Wang, W. (1999). Research on neural network training methods with addition al items.
Computing Technology and Automation, 18(2), 16-19.
5.
Wang, C., Xia, J., & Sun, Y. (2013). Modern motor control technology. Beijing: Mechanical
Industry Press.
6.
Xie, W. (2017). Research on multi-motor synchronous control based on BP neural network PID
algorithm. Shenyang University of Technology.
7.
Tao, Y., Yin, Y., & Ge, L. (1999). New PID control and its application. Beijing: Mechanical
Industry Publishing.

8.
Lin, R., & Qiu, G. (2001). Shenmu PID control simulink imitation based on S function true
model. China Instrumentation, 6, 4-5.
OceanofPDF.com

(1)
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_15
A Scene Feature Based Eye-in-Hand
Calibration Method for Industrial Robot
Guoshu Xu1 and Yonghua Yan1  
School of Mechanical Engineering, Shanghai Jiao Tong University,
Shanghai, China
 
Yonghua Yan
Email: yhyan@sjtu.edu.cn
Keywords Industrial robot - RGB-D camera - Eye-in-hand calibration -
Bundle adjustment (BA)
1 Introduction
Eye-in-hand calibration is a basic task before the robot works. When the
RGB-D camera is mounted on the end-effector of the robot, the
transformation matrix from the robot end-effector coordinate system to the
camera coordinate system is obtained by Eye-in-hand calibration. After
calibration, visual grabbing work, three-dimensional scene reconstruction
work and so on can be carried out.
Eye-in-hand calibration is usually divided into two steps: solving for
camera intrinsic parameters and solving for hand-eye transformation matrix.
Firstly, intrinsic parameters of camera like focal length and principal points
need to be obtained. At present, the commonly used method is Zhang's
calibration method [1], which uses the camera to take multiple
checkerboard pictures at different positions to solve the camera intrinsic
parameters. In this method, checkerboard is a man-made marker. The
advantage is that it can obtain high calibration accuracy, however this
method is an artificially involved and time-consuming work. Besides, high

quality checkerboard plays a key role in the accuracy of calibration result.
For the industrial occasion where optical parameters (focal length,
principle points) are often changed, it is necessary to re-calibrate at each
time, thus Zhang's method can not be easily implemented. There are some
other calibration methods based on the constraints of essential matrix [2].
The principle is constructing cost function according to the essential matrix
having two equal non-zero singular values. Stochastic optimization method
like dynamic mountain climbing method [3] is used to find parameters that
can minimize the cost function and the optimal parameters are the camera
intrinsic parameters. However, stochastic optimization algorithm can not
strictly prove its convergence, so the obtained optimal parameters are often
local optimal values rather than global optimal values.
Once getting the camera intrinsic parameters, we can further get the
hand-eye transformation matrix. Tsai's method [4] is usually used to solve
the hand-eye calibration model for robots (Fig. 1).
Fig. 1 The model of hand-eye calibration for robots
The core of eye-in-hand calibration is to solve the matrix equation like
AX = XB. A represents the relative transformation between two camera
poses, B represents the relative transformation between two end effector
poses, X is the hand-eye transformation matrix which need to be solved. N
groups of camera relative pose transformation 
 and end effector
relative pose transformation 
 (i = 0...N − 1) are obtained by N times

pose transformation of robot. Assuming that matrix 
 represents the end
effector coordinate system relative to the robot base coordinate system in
the 
 transformation, so 
. Since 
 is directly obtained
from the robot, 
 can be obtained easily. As for 
, we can use
chessboard as a reference object and calculate the camera pose
transformation by obtaining the camera pose matrix relative to the
chessboard.
It is seen that man-made markers like chessboard play an important role
in traditional hand-eye calibration methods, and these marker-based
methods are manual and time-costing. However, labor-intensive and time-
consuming work are intolerable in industrial application. What is more, auto
parameters adjustment and online camera calibration is a trend in the future.
In order to overcome the shortcomings of traditional hand-eye calibration
methods, an automatic eye-in-hand calibration method based on scene
feature is proposed. Firstly, the camera intrinsic parameters are solved by
using ORB feature extraction and Bundle Adjustment to deal with small
motion image sequence. Then, ORB feature and PnP are used to solve the
pose transformation between multi-view images, BA is also used to
optimize the result. Finally, the eye-in-hand transformation matrix is
obtained by solving the corresponding relations between the pose
transformations of multi-view images and the pose transformations of the
robot. Experiments show that our method is simple and feasible with high
accuracy. The robot can re-calibrate quickly for many times with no rely on
markers. It is believed that our method has good prospects in industrial
applications.
2 Methodology
2.1 Camera Imaging Model
Camera imaging process is a mapping between the 3D world and a 2D
image. Pinhole camera model is the simplest camera model. The model is
shown in Fig. 2.

Fig. 2 Camera pinhole model
The imaging process can be described by formula 1.
(1)
In formula 1, assuming a point P in the world space, 
 is
the coordinate of point P in the camera coordinate system and 
 is
the coordinate of point P in the pixel coordinate system. The matrix K is
called the camera intrinsic calibration matrix. From formula 1, we can get
the transformation relationship from camera coordinate system to pixel
coordinate system (formula 2).
(2)
2.2 Camera Intrinsic Parameters
The first step of hand-eye calibration is to solve the camera intrinsic matrix
K. Bundle Adjustment (BA) [5] is a non-linear least squares optimization
model. BA can be used to optimize camera parameters and pose estimation

by minimizing the re-projection error between images. Ha [6] uses BA to
estimate the camera parameters from a small motion image sequence. In his
method, Harris corner detector is used in the reference image to extract the
local features and KLT algorithm is used to find the corresponding features
in other images. Experiments show that Ha's method is useful for outdoor
scenes but less effective in indoor scenes.
Considering that most industrial robots work in indoor environment, we
present a method for solving camera intrinsic matrix by using ORB feature
and Bundle Adjustment. The principle is shown in Fig. 3.
Fig. 3 Small motion image sequence used in BA
In Fig. 3, 
 is the camera coordinate system of 0-th frame, 
is the camera coordinate system of i-th frame. 
 is the homogeneous
transformation matrix from 
 to 
. 
 is a feature point, 
 is
 coordinate in 
, 
 is 
 coordinate in 
. 
 (in red) is
the ORB feature point corresponding to 
 in 0-th frame, 
 (in red) is the
ORB feature point corresponding to 
 in i-th frame. 
 (in blue) is the
reprojection point corresponding to 
 in i-th frame.

The 0-th frame is thought as reference frame, the j-th ORB feature point
in the reference frame is 
 and its coordinate is 
. The depth
value of 
 is 
, which can be directly gotten from corresponding depth
image. According to the camera imaging model (formula 1), we can get the
coordinate of 
 in the camera coordinate system 
 (formula 3).
(3)
Function T maps the point coordinate from 
 to 
, so the
coordinate of 
 in 
 can be described by formula 4.
(4)
In formula 4, 
 represents the rotation vector,
 represents the translation vector. The
transformation from rotation vector to rotation matrix can be achieved by
Rodrigues formula (formula 5).
(5)
In formula 5, 
, 
 is the conversion symbols from vector to
antisymmetric matrix. According to the formula derived by Yu and Gallup
[7], when the rotation angle is very small, Rodrigues formula can be
approximated to formula 6.

(6)
According to formula 2, the projection from 
 to
 can be described by formula 7.
(7)
The reprojection from 
 to 
 is a combination of formulas 3, 4, 5, 6,
and 7. For a small motion image sequence, the error function W is the sum
of reprojection errors for all the feature points 
 (j = 1...n) in the
reference frame. 
 is an abstract symbol for
reprojection, so the error function W can be described by formula 8.
(8)
In formula 8, n is the image numbers in the small motion image
sequence, m is the numbers of extracted ORB features point. If feature
point 
 has projection in i-th frame, then 
; else 
. K is the
camera intrinsic calibration matrix. 
 are the rotation vector and
translation vector from the coordinate system of reference frame to the
coordinate system of i-th frame. 
 is the depth value of the ORB feature
point 
.

ORB feature [8] consists of key point and descriptor. The key point is
Oriented FAST corner point which is scale invariant and rotation invariant.
The descriptor is a binary descriptor named BRIEF. ORB feature points are
extracted in every frame and use bidirectional Brute-Force [9] to match the
corresponding feature points. Figure 4 shows the matching of ORB feature
points between two similar images. At the same time, threshold is set to
preserve matching point pairs with high matching accuracy.
Fig. 4 Matching of ORB feature points in two images
BA is used to solve the variables 
 by minimizing the cost
function W. Since it is a small motion image sequence, the initial value of
 can be set as zero vector. Camera intrinsic calibration matrix contains
four unknown parameters 
. The initial value of 
 is half
of sum of image width and height. The initial value of 
 is half of the
image width, the initial value of 
 is half of the image height. The initial
value of 
 can get from corresponding depth image. After setting all initial

values, we utilize Ceres [10] to do the BA optimization. The detailed
working flow is shown in Table 1.
Table 1 Flow of solving camera intrinsic parameters
2.3 Hand-Eye Transformation Matrix
After getting the camera intrinsic calibration matrix, the hand-eye
transformation matrix can be calculated by Tsai's method. The key problem
is how to get the pose transformations between multi-images? We have
discussed traditional methods which rely on marker like chessboard in
Chap. 1. Here, we take the idea from camera location in ORB-SLAM [11,
12] and put forward a method to solve pose transformations between multi-
images with ORB and PnP, followed by BA optimization. Although ORB
feature and BA are still used like the way we use in calculating camera
intrinsic calibration matrix, the clear difference is that the image sequence
used here is from obvious camera moving rather than small motion. As a
result, PnP is used to solve the pose transformations between two adjacent
frames.
PnP [13] is a method for estimating camera pose by N sets of 3D space
points and their corresponding 2D projection points. The pose
transformations between multi-view images is shown in Fig. 5.

Fig. 5 Pose transformations between multi-view images. 
 represent the pose
transformation from 1-th frame to 2-th frame
Firstly, ORB feature points are extracted in two adjacent color frames
and bidirectional Brute-Force is used to find 2D-2D matching pairs. We
further find the 3D-2D matching pairs with the depth image and camera
intrinsic calibration matrix K. Considering the measurement error of camera
sensor, the depth images are preprocessed before been used to find the 3D-
2D matching pairs. Preprocessing contains the following three aspects:
1. Fill the hole in the depth image and replace it with the mean of
neighborhood pixels.
 
2. Apply Gaussian filter to the depth image to reduce noise.
 
3. Considering that error of measured depth value is large when it is too
close or too far away, so we set an interval from 0.5 to 5 m. And it is
thought that these points between 0.5 and 5 meters as "good points",
only "good points" are used in 3D-2D matching pairs.
 
We utilize PnP to solve pose transformation 
 between two adjacent
images, and set the 
 as initial value, then BA is used to optimize
. The detailed process flow is shown in Fig. 6.

Fig. 6 Solve pose transformation between adjacent images
Finally, N groups of camera pose transformation matrix 
 (i = 0...
N − 1) can be obtained. End effector pose transformation matrix 
 (i = 
0...N − 1) can be gotten directly from robot. Substitute the 
 and
 into AX = XB, and we can get the hand-eye transformation matrix X
by Tsai's method.
3 Experiments and Results
3.1 Experimental Setup

The experiment system includes a PC, a JAKA Zu7 six-axis industrial robot
and an Intel RealSense D435 RGB-D camera. Experiment platform is
shown in Fig. 7. The software environment is Ubuntu16.04 LTS with ROS,
OpenCV and PCL. The parameters of camera are shown in Table 2.
Fig. 7 Experiment platform
Table 2 Camera parameters
Intel RealSense D435 camera
Sensor
Stereo IR + RGB
Color image resolution (max) 1920 * 1080
Depth image resolution (max) 1280 * 720
Maximum frame radio fps
90
Work distance (m)
0.105-10
Interface
Type-C, USB3.0
3.2 Results and Analysis
1. Camera Intrinsic Parameters 
We get color image sequence of small motion with 30 frames and take the
first frame as reference frame. The depth image of reference frame is also
captured to initialize depth value of each feature point. Since it is a small

motion, there is little difference between frames. The resolution of each
frame is 640 * 480. The image sequence is shown in Fig. 8. The experiment
results are shown in Table 3.
Fig. 8 The small motion image sequence. a Color image of reference frame. b Depth image of
reference frame. c Color image of last frame. d Depth image of last frame
Table 3 Camera intrinsic parameters
 
Zhang's method
614.18 614.17 322.25 245.24
Our method
617.24 617.24 324.14 247.52
Relative error (%) 0.5
0.5
0.6
0.9
Results of our method are the average result of 10 independent repeats.
At the same time, Zhang's method results are thought as standard values.
From the Table 3, the relative error of focal length 
 is 0.5%, the

relative error of principle point 
 is less than 1%. In other calibration
method like Jiang's method [14], the relative error of focal length is 0.5%
and the relative error of principle point is 1.5%. So, it can be concluded that
our calibration method has a good precision. Meanwhile, our method needs
no man-made marker, thus it is a better choice in application.
2. Hand-Eye Transformation Matrix 
We get color image sequence of obvious motion with 50 frames and its
corresponding depth image sequence. The resolution of each frame is 640 *
480. The image sequence is shown in Fig. 9. The result is shown in Table 4.
Fig. 9 The obvious motion image sequence. a The 1-th color frame. b The 2-th color frame. c The 3-
th color frame. d The 4-th color frame
Table 4 Hand-eye transformation matrix

Matrix
Trans vector
Euler angles
3. Object Grasping Experiment 
Since the accuracy of hand-eye transformation matrix is not easy to be
evaluated quantitatively, an object grasping experiment is designed to
evaluate indirectly. If the grasping error is within the acceptable range, it
can be declared that the accuracy of hand-eye transformation matrix is
acceptable because the grasping error includes the hand-eye matrix error,
vision detection error, robot motion error and so on. In our experiment, the
grasping object is a box as shown in Fig. 10.
Fig. 10 The grasping object. a Color image. b Depth image
The object pose in camera coordinate system can be calculated from the
color image, depth image and camera intrinsic matrix K, then the pose
transformation from camera coordinate system to end-effector coordinate
system can be solved by the hand-eye transforming matrix. The
transformation from end-effector coordinate system to robot base

coordinate system is known, the object pose in robot base coordinate
system can be finally solved. Before grasping, it is vital to set the tool
coordinate system. The grasping scene is shown in Fig. 11.
Fig. 11 The grasping scene
It is found that the sucker is about in the center of the box and the
grasping result is satisfied. According to the foregoing analysis, we think
the hand-eye transformation matrix is of good accuracy and can be used in
practice.
4 Conclusion
In the paper, we put forward a scene-feature based eye-in-hand calibration
method. In first step, ORB feature extraction and Bundle Adjustment are
used to deal with small motion image sequence to get camera intrinsic
parameters. In second step, ORB feature extraction and PnP are used to deal
with obvious motion image sequence, followed by BA optimization.
Finally, we get hand-eye transformation matrix.
We compare our experiments results with existing methods results, and
find that our method is applicable and effective. At the same time, since our

method requires no man-made marker, it is time-saving and can be
conveniently used for multi-calibration in practice.
References
1.
Zhang, Z. (2000). A flexible new technique for camera calibration. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 22(11), 1330-1334.
[Crossref]
2.
Mendonca, Paulo, R. S., & Cipolla, R. (1999). A simple technique for self-calibration. In IEEE
Computer Society Conference on Computer Vision & Pattern Recognition. IEEE (pp. 500-505).
3.
Anthony, W., & Gerhard, R. (2004). Estimating intrinsic camera parameters from the
fundamental matrix using an evolutionary approach. Eurasip Journal on Advances in Signal
Processing, 2004(8), 1-12.
4.
Tsai, R. Y., & Lenz, R. K. (2002). A new technique for fully autonomous and efficient 3D
robotics hand/eye calibration. IEEE Transactions on Robotics and Automation, 5(3), 345-358.
[Crossref]
5.
Triggs B. (1999). Bundle adjustment—A modern synthesis. In Proceedings of the International
Workshop on Vision Algorithms: Theory and Practice. Springer (pp. 298-372).
6.
Ha, H., Im, S., Park, J., et al. (2016). High-quality depth from uncalibrated small motion clip. In
IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE (pp. 5413-5421).
7.
Yu, F., & Gallup, D. (2014). 3D reconstruction from accidental motion. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR). IEEE (pp. 3986-3993).
8.
Rublee, E., Rabaud, V., Konolige, K., et al. (2012). ORB: An efficient alternative to SIFT or
SURF. In 2011 International Conference on Computer Vision. IEEE (pp. 2564-2571).
9.
Jakubović, A., & Velagić, J. (2018). Image feature matching and object detection using Brute-
force matchers. In 2018 International Symposium ELMAR (pp. 83-86).
10.
Agarwal, S., Mierle, K., et al. (2014). Ceres solver. http://​ceres-solver.​org.
11.
Mur-Artal, R., Montiel, J. M. M., & Tardos, J. D. (2015). ORB-SLAM: A versatile and accurate
monocular SLAM system. IEEE Transactions on Robotics, 31(5), 1147-1163.
[Crossref]
12.
Mur-Artal, R., Tardos, J. D. (2017). ORB-SLAM2: An open-source SLAM system for
monocular, stereo, and RGB-D cameras. IEEE Transactions on Robotics, 1-8.
13.
Fischler, M. A., & Bolles, R. C. (1981). Random sample consensus: A paradigm for model fitting
with applications to image analysis and automated cartography. Communications of the ACM,
6(6), 381-395.
[MathSciNet][Crossref]

14.
Jiang, Z., & Wu, W. (2010). An essential matrix-based camera self-calibration method. Journal
of Image and Graphics, 15(4), 565-569.
OceanofPDF.com

(1)
(2)
 
 
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_16
Vision-Based Trajectory Planning for a
Five Degree of Freedom Assistive Feeding
Robotic Arm Using Linear Segments with
Parabolic Blend and Cycloid Functions
Priyam A. Parikh1   , Keyur D. Joshi2    and Reena Trivedi1  
Nirma University, Ahmedabad, India
Ahmedabad University, Ahmedabad, India
 
Priyam A. Parikh (Corresponding author)
Email: myrobo56@gmail.com
Keyur D. Joshi
Email: joshikeyurd@gmail.com
Reena Trivedi
Email: reena.trivedi@nirmauni.ac.in
1 Introduction and Background
This paper mainly focuses on the trajectory planning of a serial manipulator
using joint space scheme and face reorganization algorithm. Trajectory
planning of a multi-degree of freedom robot can be done using Cartesian
scheme as well as joint space scheme [1]. Since Cartesian scheme mainly
deals with end-effectors' position, orientation and their time derivatives, it
is not recommended to design trajectory in Cartesian plane. Researchers do
not use Cartesian space for trajectory planning, because the inverse of the
motion transfer matrix or Jacobian matrix does not exist [2]. The robotic
arm is designed to feed the physically challenged people, who are suffering

from Parkinson, neurological disorder, paralyses. End-effector of the robot
is a spoon, having single degree of freedom. Therefore, it is important to
control joint acceleration and angular velocity as the arm carries food.
Many trajectory planning methods are available in joint space scheme; e.g.
cubic polynomial, fifth order polynomial, sixth order polynomial and many
more. It is quite obvious that the higher the order of polynomial, smoother
will be the trajectory. However trajectory planning using lower order
polynomial or linear polynomial is also possible, but it gives discontinuities
in joint rates, which in turn causes more vibrations. Moreover, actuator
must provide almost an infinite acceleration to achieve instantaneous
velocity in the case of straight line or first order polynomial [3]. In order to
counter this issue, straight line trajectory is divided in three parts, where
first part and last part are generated using second order polynomial and the
middle part is a straight line. The meaning of the parabolic blend is
blending of parabola with a straight line. Researchers have concluded that
parabolic blend gives continuity in velocity but there is no continuity in the
acceleration [4]. A discontinuity in acceleration and velocity is not desired
for such a robotic arm which carries food.
On the other hand, higher order polynomials give smoother trajectory,
but provide acceleration peaks in the middle of the trajectory. To resolve
that problem one can also achieve zero velocity and acceleration at the start
point and end point of the trajectory without resorting to a higher order
polynomial [5]. This can be achieved by designing trajectory using cycloid
functions. However sometimes full cycloid function also provides peaks of
acceleration in middle of the trajectory, but the magnitude of the peaks are
lesser compared to higher order polynomials [6]. To overcome above
mentioned problems, a trajectory is designed using semi cycloid function,
which gave smoother trajectory, cut down the peaks of acceleration and
provided zero acceleration at the ends. However it provided nonzero value
of velocity at the end of the trajectory.
Figure 1a shows the 3D model of the robot, whereas hardware setup of
the robot with nomenclature is shown in Fig. 1b, c respectively. All the
hardware details of the robot are shown in Table 1. This paper is divided in
three parts; first part discusses about the hardware setup, problem
description and literature survey. In the second part of the paper, the
kinematics and inverse kinematics are discussed. The trajectory planning is
shown in the last part of the paper. In addition with that comparison of

trajectory generated by LSPB (Linear segment with parabolic blend),
cycloid and semi cycloid function are shown along with angular velocity
and angular acceleration.
Fig. 1 Details of feeding robotic arm a 3D CAD design, b actual model, c nomenclature, and d D-H
matrix frame assignment
Table 1 Hardware details
# Details of feeder robotic arm used
Parameter
Description
1 Degree of freedom
Five
2 Type of robotic arm
T-R-R-R-R
3 Joint actuators (total five) Servo motor: metal gears, stall torque 15 kg/cm
4 Working speed
0.13 s/60° at 7.2 V (no load)
5 Working voltage
4.8-7.2 V
6 Controller board used
Arduino Mega
7 Battery
LI-Po 7.2 V
1.1 Literature Review
Researchers have worked on trajectory planning to make it smoother and to
get continuity in acceleration as well as velocity. Reham [7] worked on
trajectory tracking control for robot manipulator using fractional order

fuzzy PID controller, using which they achieved lesser steady state error.
However they applied this algorithm to 5th order polynomial. Zhang et al.
[8] designed trajectory for 6DOF robot manipulator using genetic algorithm
to reduce robot operation time. Many researchers tried to redevelop inverse
kinematics algorithms, some of them tried minimize the number of
solutions, but the issue of singularity was always there. Wang et al. [9]
analyzed singularity for 7R 6DOF painting robot to prevent robot joint from
folded back situation. Valente et al. [10] developed jerk-bounded trajectory
for industrial robots which was based on sine jerk motion profile.
Furthermore Zhao et al. [11] gave a comparison between cubic polynomial
and quintic polynomial for a 6DOF robotic arm. Gasparetto and Zanotto
[12] deigned trajectory using cubic B spline and quintic B-spline method
for 6DOF industrial robots. Li et al. [13] gave an approach for smoother
trajectory planning for high speed pick and place parallel robots using
quintic B-splines. Gallant and Gosselin [14] extended the capability of a
robotic manipulator using optimizing robot trajectory with the help of SQP
algorithm. Kucuk [15] designed trajectory using optimal trajectory
generation (OTGA) for serial and parallel manipulators.
1.2 Problem Description and Methodology
The robot trajectory is divided in three parts; the first part of the trajectory
deals with deceleration, in which the end-effector of the robot grabs the
food. In the second part robot carries the food and tries to reach at the
destination, which is an acceleration mode. In the last part of the trajectory,
end-effector decelerates near the destination point. It becomes important to
control over the acceleration and velocity.
Initially the trajectory was designed using sixth order polynomial. But
this method was not able to perform in the middle part of the trajectory as it
was giving positive and negative peaks of acceleration in the middle stage.
In the next iteration, trajectory was designed using combination of parabolic
blend and first order polynomial. This method provided continuity in the
velocity, but failed to provide continuity in acceleration. It also provides
zero acceleration at the middle of the trajectory, which is also not suitable.
Furthermore, trajectory was designed using fully cycloid function. It
satisfied zero velocity and acceleration conditions at the end-points of the
trajectory, but provided multiple peaks of acceleration and deceleration,
which is not desired. At last, the trajectory was designed semi cycloid

function, which made trajectory smoother, protected middle part of the
trajectory from zero acceleration, cut down the acceleration peaks at the
middle and protected actuator from providing instantaneous velocity and
infinite acceleration.
The webcam attached to the robot recognizes the face of the user and
locates mouth. Based on mouth position, it finds the position vector in XYZ
plane. As shown in Fig. 1(d, the robot works in XZ plane (as per DH
matrix), hence the position vector in X direction remains same for all
patients. Camera finds only the position vector in Z direction (height). The
values of position or target vector are fed in inverse kinematics algorithms,
which calculate joint angles. Based on these joint angles, trajectories are
planned in joint space.
2 Trajectory Planning
2.1 Kinematic Analysis
As discussed earlier, it becomes essential to perform inverse kinematics
before getting into the joint space trajectory planning. Inverse kinematic is
done using Peter-Corke toolbox in MATLAB. Robotic arm's home position
before applying any kinematics assignments is shown in Fig. 2. For the
particular home position, initial angles along with DH parameters are
shown in Table 2. For the initial and final position of the robot,
corresponding transformation matrices of end-effector with respect to base
are shown in Eqs. 4 and 5 in Chap. 15 respectively. It should be noted that
final position varies with the height of patient. The initial and final angles
found from inverse kinematics are shown in Table 2. Equations for Joint 1
and Joint 4 are not taken in consideration as the difference between their
boundary conditions is approximately zero.

Fig. 2 Simulated initial (Home) and final position of the feeding robotic arm
Table 2 Joint parameters forward and inverse kinematics where JA stands for joint angle, JD stands
for joint distance, TA stands Twsting angle and LL stands for link length
Joint # Joint parameters forward kinematics
Inverse Kinematics
JA initial (°) JD (mm) TA (°) LL (mm) JA initial (°) JA final (°)
1
180
50
−90
0
−21
0
2
90
0
0
140
119
61
3
0
0
0
120
0
−43
4
0
0
0
100
0
−7
5
−90
0
0
80
−87
−10
(1)
(2)
2.2 Locating the Target

The webcam from INTEX is mounted on the upper base of the robotic arm.
Camera captures an image only for one time per user to get information of
user seating in the chair. This image is processed to locate the height of the
user's mouth. The face was recognized by using Viola Johns algorithm [16].
The height of mouth from top of a face on average was found to be around
80% of the total height of the face from the top. This height of the mouth
was set as target for the feeding robotic arm. This is required only one time
for one user. Small variation in the mouth location (e.g. around ±0.5 cm) is
not a concern, as mouth position can be adjusted by the user.
2.3 Parabolic Blend with Linear Path
Trajectory planning using parabolic blend and linear path is very common
in industries. As shown in Fig. 3, this trajectory is divided in three parts; (i)
constant acceleration or ramp velocity (ii) zero acceleration and (iii)
constant deceleration. The total trajectory time is T = 6 s. A generalized plot
of joint angles versus time is shown in Fig. 4 to identify the conventions
used in this paper. Time taken by robot to reach from θi to θA is tf1 = 1 s.
Similarly, it takes tf3 = 1 s to reach from θB to θf.. Robot takes tf2 = 4 s to
reach from θA to θB. All the joint angles with their intermediate points are
shown in Table 3 with angular acceleration and velocity.

Fig. 3 Robotic feeding arm forward pass end position according to position of the user's mouth
Fig. 4 Trajectory segmentation with convention used in this paper

Table 3 Joint angles with Intermediate points
Joint Joint angles, angular velocity and angular acceleration
θi
θA
θB
θf
 (°/s2) Angular velocity in the linear path (°/s)
 (°/s)
1
0
0
0
0
0
0
0
2
−85 −75 33
45
20
27
20
3
−75 −65 −52 −42 20
3.25
20
4
37
0
0
37
0
0
0
5
124 114 −28 −38 20
−35.5
20
For the first part of the trajectory, a second order polynomial with its
two time derivatives is shown in Eqs. 4 to 5 in Chap. 15 respectively. All
the initial and final conditions are shown in Eqs. 6, 7 in Chap. 15, and Eq. 1
respectively. Angular displacement and velocity at intermediate points A
and B, can be found using Eqs. 3-5. Angular displacement for joint 2, 3 and
5 are shown jointly in Eq. 6. Angular velocity in the linear path can be
found using Eq. 6 (Table 4).
Table 4 Values of cooficients for the parabolic blend and linear parth
 
Parabolic blend first half Middle Part Parabolic blend last half
a
b
c
NA
(3)
(4)
(5)

(6)
(7)
(8)
(9)
(10)
(11)
(12)
(13)
(14)
Similarly, angular displacement, angular velocity and angular
acceleration for the last part are given in Eqs. 7 to 9 respectively. Initial and
final conditions for the last part of the trajectory are given in Eqs. 10 to 13
respectively. Angular displacement for joint 2, 3 and 5 are shown jointly in
Eq. 13.

(15)
(16)
(17)
(18)
(19)
(20)
(21)
(22)
Middle path of the trajectory, which is a linear displacement, is shown
along with its first and second time derivatives in Eqs. 15-17. Its initial and
final conditions are shown in Eqs. 18 and 19 respectively. Angular
displacement for joint 2, 3 and 5 are shown jointly in Eq. 22. All the
coefficients are calculated using Eqs. 2, 13 and 21.
(23)
(24)

(25)
(26)
(27)
(28)
(29)
(30)
2.4 Trajectory Design Using Cycloid Function
This sub-section discusses about trajectory design using cycloid functions.
A cycloid is defined as the curve traced by a point on a circle as it rolls on a
straight line without slipping. It is a geometric entity used by gears and
cams [17]. Angular displacements along with its first and second time
derivatives are shown in Eqs. 23 to 25 respectively. Angular displacements
for all the joints are shown in Eq. 27.
(31)
(32)

(33)
(34)
2.5 Trajectory Design Using Semi Cycloid Function
Angular displacements along with its first and second time derivatives are
shown in Eqs. 28 to 30 respectively. Angular displacements for all the joints
are shown in Eq. 31.
(35)
(36)
(37)
(38)
3 Results
A trajectory (angular displacement v/s time) generated and compared for
joint 5 using LSPB, cycloid and semi cycloid is shown in Fig. 5.
Comparison of angular velocity and angular acceleration is shown in Figs. 6
and 7 respectively.

Fig. 5 Angular displacement versus time using all three trajectory methods for Joint 5
Fig. 6 Angular velocity versus time using all three trajectory methods for Joint 5

Fig. 7 Angular acceleration versus time using all three trajectory methods Joint 5
As shown in Fig. 5, Semi cycloid trajectory is smoother than cycloid
and LSPB as is does not contain any linear part as well as it does not
produce any peaks of displacement. As shown in Fig. 6, LSPB generates
trapezoidal velocity response to protect actuator from providing infinite
acceleration at the beginning. The cycloid function provides zero velocity at
the beginning and at the end, whereas semi cycloid trajectory is not able to
provide zero velocity at the end point. As shown in Fig. 7, LSPB is not able
to provide zero acceleration at the end of the trajectory, which is not
desired. Cycloid and semi function are able to achieve zero acceleration at
the ends.
4 Conclusion and Future Scope
The trajectory generated by LSPB gives continuity in velocity, but there is
no continuity of acceleration. LSPB fails to protect servo actuator from
providing instantaneous velocity in the middle of the trajectory. It also fails
to protect actuator from zero acceleration in the middle part. Therefore,
constant velocity and zero acceleration forces actuator to draw more current
instantaneously, which can cause winding damage along with unwanted
vibrations. Due to above mentioned reasons, trajectory generated by LSPB

may not be able to help robotic arm in order to protect the food in the
middle of the trajectory.
The trajectory generated with cycloid function satisfies zero
acceleration and velocity at the beginning and end of a trajectory. However
it provides peaks of acceleration and deceleration, which is a concern for a
robot. Furthermore, a trajectory generated with cycloid function is not
smoother due to its sinusoidal behavior.
The trajectory generated using semi cycloid function removes zero
acceleration and constant velocity in the middle of the trajectory, which
helps servo actuator from drawing lesser current from the battery. Moreover
semi cycloid function removes acceleration peaks form the trajectory and
gives smoother trajectory as compared to LSPB and full cycloid function.
In future, we are planning to achieve continuity in acceleration and
velocity by designing trajectory using ANN, Genetic algorithm and fuzzy
PID controller. We plan to use an advanced open source cameras with
inbuilt processors such as PIXY cam, ESP32 CAM, NOIR 16MP with
raspberry pi to add more features and options for the user.
References
1.
Chiaverini, S., Siciliano, B., & Egeland, O. (1994). Review of the damped least-squares inverse
kinematics with experiments on an industrial robot manipulator. IEEE Transactions on Control
Systems Technology, 2(2), 123-134. https://​doi.​org/​10.​1109/​87.​294335.
[Crossref]
2.
Cheah, C. C., Kawamura, S., & Arimoto, S. (1999). Feedback control for robotic manipulator
with an uncertain Jacobian matrix. Journal of Robotic Systems, 16(2), 119-134. https://​doi.​org/​
10.​1002/​(sici)1097-4563(199902)16:​2%3c119:​aid-rob5%3e3.​0.​co;2-j.
[Crossref][zbMATH]
3.
Guan, Y., Yokoi, K., Stasse, O., & Kheddar, A. (2005). On robotic trajectory planning using
polynomial interpolations. In 2005 IEEE International Conference on Robotics and Biomimetics-
ROBIO. https://​doi.​org/​10.​1109/​robio.​2005.​246411.
4.
Rossi, C., & Savino, S. (2013). Robot trajectory planning by assigning positions and tangential
velocities. Robotics and Computer Integrated Manufacturing, 29(1), 139-156. https://​doi.​org/​10.​
1016/​j.​rcim.​2012.​04.​003.
[Crossref]
5.
Macfarlane, S., & Croft, E. A. (2003). Jerk-bounded manipulator trajectory planning: Design for
real-time applications. IEEE Transactions on Robotics and Automation, 19(1), 42-52. https://​doi.​
org/​10.​1109/​tra.​2002.​807548.
[Crossref]

6.
Seraji, H., Long, M. K., & Lee, T. S. (1993). Motion control of 7-DOF arms: The configuration
control approach. IEEE Transactions on Robotics and Automation, 9(2), 125-139. https://​doi.​
org/​10.​1109/​70.​238277.
[Crossref]
7.
Mohammed, R. H., Bendary, F, & Elserafi, K. (2016). Trajectory tracking control for robot
manipulator using fractional order-fuzzy-pid controller. International Journal of Computer
Applications, 134(15), 22-29. https://​doi.​org/​10.​5120/​ijca2016908155.
8.
Zhang, J., Meng, Q., Feng, X., & Shen, H. (2018). A 6-DOF robot-time optimal trajectory
planning based on an improved genetic algorithm. Robotics and Biomimetics, 5(1). https://​doi.​
org/​10.​1186/​s40638-018-0085-7.
9.
Wang, X., Zhang, D., Zhao, C., Zhang, H., & Yan, H. (2018). Singularity analysis and treatment
for a 7R 6-DOF painting robot with non-spherical wrist. Mechanism and Machine Theory, 126,
92-107. https://​doi.​org/​10.​1016/​j.​mechmachtheory.​2018.​03.​0.
[Crossref]
10.
Valente, A., Baraldo, S., & Carpanzano, E. (2017). Smooth trajectory generation for industrial
robots performing high precision assembly processes. CIRP Annals, 66(1), 17-20. https://​doi.​
org/​10.​1016/​j.​cirp.​2017.​04.​105.
[Crossref]
11.
Zhao, X., Wang, M., Liu, N., & Tang, Y. (2017). Trajectory planning for 6-DOF robotic arm
based on Quintic polynomial. In Proceedings of the 2017 2nd International Conference on
Control, Automation and Artificial Intelligence (CAAI 2017). https://​doi.​org/​10.​2991/​caai-17.​
2017.​23.
12.
Gasparetto, A., & Zanotto, V. (2010). Optimal trajectory planning for industrial robots. Advances
in Engineering Software, 41(4), 548-556. https://​doi.​org/​10.​1016/​j.​advengsoft.​2009.​11.​001.
[Crossref][zbMATH]
13.
Li, Y., Huang, T., & Chetwynd, D. G. (2018). An approach for smooth trajectory planning of
high-speed pick-and-place parallel robots using quintic B-splines. Mechanism and Machine
Theory, 126, 479-490. https://​doi.​org/​10.​1016/​j.​mechmachtheory.​2018.​04.​0.
[Crossref]
14.
Gallant, A., & Gosselin, C. (2018). Extending the capabilities of robotic manipulators using
trajectory optimization. Mechanism and Machine Theory, 121, 502-514. https://​doi.​org/​10.​1016/​
j.​mechmachtheory.​2017.​09.
[Crossref]
15.
Kucuk, S. (2017). Optimal trajectory generation algorithm for serial and parallel manipulators.
Robotics and Computer-Integrated Manufacturing, 48, 219-232. https://​doi.​org/​10.​1016/​j.​rcim.​
2017.​04.​006.
[Crossref]
16.
Viola, P., & Jones, M. J. (2001). Rapid object detection using a boosted cascade of simple
features. In Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision
and Pattern Recognition, Vol. 1 (pp. 511-518).

17.
Saha, S. K. (2014). Introduction to robotics 2e. Mcgrawhill education. ISBN 93-329-02801.
OceanofPDF.com

(1)
(2)
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_17
Structure Design and Closed-Loop
Control of a Modular Soft-Rigid
Pneumatic Lower Limb Exoskeleton
Jiangbei Wang1, 2 and Yanqiong Fei1, 2  
Research Institute of Robotics, Shanghai Jiao Tong University,
Shanghai, 200240, China
Institute of Medical Robotics, Shanghai Jiao Tong University,
Shanghai, 200240, China
 
Yanqiong Fei
Email: fyq_sjtu@163.com
Keywords Soft robot - Module - Soft-rigid exoskeleton - Closed-loop
control
Reasearch supported by the National Natural Science Foundation of China
under Grant No. 51475300 and 51875335, Joint fund of the Ministry of
Education under Grant No. 18GFA-ZZ07-171, Institute of Medical
Robotics of Shanghai Jiao Tong University under Grant No.
IMR2019QY01.
1 Introduction
The exoskeletons have been developed from academic to commercial
applications [1, 2] in recent decades. The conventional rigid exoskeletons
are constituted by rigid links and joints actuated by electric or hydraulic
motors [3]. They have the advantages of high force output, good
controllability and greater accesses to mature fabrication and integration

technologies. However, the rigid exoskeletons are confronted with
challenges such as poor compliance with the human body, difficult
alignment with the biological joints [4] and large weight/inertia applied on
the biological extremities, degrading the wearing comfort, safety and
metabolic efficiency [5]. The soft exoskeletons that adopt soft actuators,
soft sensors and soft structural materials such as cables [6] and fabrics [7]
provide new approaches. The soft exoskeletons have no explicit mechanical
joints or take the soft actuators as the flexible joints [8]. Two mainstream
configurations of the soft exoskeletons are the Tendon-Driven Exoskeletons
(TDEs) [6] and the Bending-Driven Exoskeletons (BDEs) [9], differentiated
by their soft actuators.
The TDEs have no explicit mechanical joints and can be made
lightweight, low-profile and highly flexible, indicating good wearability
and portability. However, the high tension forces (up to 80 N [6]) required
to actuate the biological joints, induce large compression forces to the joints
and shear forces to the skin of extremities, which may reduce the wearing
comfort.
The BDEs utilize the soft bending actuators as their active soft hinges to
provide torques for rotation of the biological joints [9]. The torque is
exerted as several couples of transverse forces on the wearer's extremities,
producing no net pulling or pushing forces on the biological joint and only
normal instead of shearing pressures on the biological limbs, which
overcomes the aforementioned disadvantage of the TDEs.
From the ergonomics, the exoskeletons with a variable-stiffness or
heterogenous structure [10] exhibit better conformity with the human body
than those with a homogeneous structure [11]. Therefore, it is necessary to
design the BDEs into a soft-rigid structure, i.e. with soft hinges and rigid
links. The soft hinges can supply compliant actuation for the wearer's
joints, and the rigid links can transfer the actuation forces from the
exoskeleton to the human body effectively. The rigid links also leave spaces
for integrating sensors and circuits for data acquisition and wireless
communication, enabling the feedback control without the tethering of
electric wires.
In the paper, we propose the novel modular soft-rigid BDEs actuated by
the CPAMs, incorporate the sensing circuits into it, and further achieve the
closed-loop control.

2 Structure Design
In anatomy, the lower limb of human body mainly includes three joints
(hip, knee and ankle) and three skeletal regions (thigh, crus and foot), in
which the hip joint links the whole lower limb to the waist [12]. The
proposed soft-rigid exoskeleton is designed to assist the flexion and
extension of the hip, knee and ankle joints through three customized
bidirectional CPAMs which are connected by four 3D-printed rigid parts
corresponding to the waist, thigh, crus and foot respectively (Fig. 1a).
Sensors and transmission circuits are embedded into each rigid part to
monitor the wearer's motion and publish the sensed signals wirelessly
(Fig. 1b). According to the locations on the human body, the exoskeleton is
divided into four modules, i.e. the waist-hip, thigh-knee, crus-ankle and foot
modules (Fig. 1c). Each module consists of a CPAM (sealed by the
proximal and distal rigid caps) and a rigid case (with the upper and lower
parts 3D-printed individually and then assembled by the screws and nuts for
containing the sensing unit) except the foot module which only has the rigid
case. Two different modules can be joined by the complementary convex
and concave surfaces at their two ends.

Fig. 1 Structure design of the modular soft-rigid pneumatic lower limb exoskeleton. a The external
electro-pneumatic control system. b The structure of single module. c Four modules of the
exoskeleton ➀ waist-hip module, ➁ thigh-knee module, ➂ crus-ankle module, ➃ foot module
The main body of the bidirectional CPAM is in shape of elliptical
cylinder (for the hip and knee joints) or torus (for the ankle joint), which is

made of the longitudinally elastic fabric (polyester & latex) and the
reinforcement inextensible fabric (cotton & linen) on the neutral plane
(Fig. 1b). To ensure the air tightness, two elastomeric inner bladders
(ELASTOSIL® M 4601 A/B) are inserted into the two cavities respectively,
resulting in two closed air chambers. The proximal and distal rigid caps are
plugged into the two ends of the cylinder or torus and then tied with the
fabrics by the hose clamps, resulting in the bidirectional CPAM. For each
CPAM, pressurization of the chamber on one side leads to the bending
deformation towards another side, and thus the bidirectional bending can be
achieved. The left side chamber's pressure denotes positive pressure while
the right one denotes negative pressure.
The sensing unit of each module (Fig. 1b) mainly includes two pressure
sensors for monitoring the inflating pressure of the two bladders
respectively, an Inertia Measurement Unit (IMU) for detecting the
inclination angle of each skeletal segment, and two flex sensors stacked
face-to-face and sandwiched between the two inextensible fabric layers for
measuring the bidirectional bending angle of the bidirectional CPAM. The
sensed signals are collected by the Arduino Pro Mini and transmitted by the
Bluetooth module wirelessly.
For reducing the exoskeleton weight, the electro-pneumatic control
system is not integrated with the exoskeleton (Fig. 1a). The only tethering
between the exoskeleton and the control system is the detachable pneumatic
pipes. On the control board (Arduino Mega 2560), four Bluetooth packages
are incorporated to receive the signals transmitted from the four sensing
units of the exoskeleton individually. According to the received signals and
the programmed control algorithm, the control board can automatically
adjust the inflating pressures of the soft hinges of the exoskeletons via the
Electro-Pneumatic Regulators (EPRs). The whole electronic system is
powered by a linear DC source and the pressure is supplied by an air
compressor.
3 Closed-Loop Control and Experiment
Able to detect the inflating pressures and bending angles of the exoskeleton
hinges, we can further implement the closed-loop control for the
exoskeleton. The overall control scheme (Fig. 2a) consists of the position
controllers (i.e. the external closed loop), the pressure servos (i.e. the

internal closed loop), the CPAMs and the soft-rigid exoskeleton
mechanism. This closed-loop control scheme makes the two processes (i.e.
the pressure regulation and the actuator's bending) decoupled and thus
allows their parameters to be tuned individually.
Fig. 2 Control scheme of the exoskeleton. a The overall control scheme. b The pressure servo. c The
position controller. i = 1, 2, 3 represents the hip, knee and ankle joints respectively
The pressure servo (Fig. 2b) is to control the Electro-Pneumatic
Regulators (EPRs) to supply a target inflating pressure (Pri) for the air
chambers of the CPAM. First, the target pressures are decomposed into the
reference pressures for the left and right chambers of the CPAM, i.e. PLri
and PRri. Then the errors (∆PLi and ∆PRi) between the reference and actual
pressures are calculated and amplified by the proportion component (KPPi),

resulting in the impulse frequencies (
 and 
). Signs of the
frequencies represent the rotation directions of the stepper motors of the
EPRs, i.e. dLi and dRi. To ensure the stepper motors work normally, the
frequencies are saturated by the maximum and minimum frequencies (fmin 
= 32 Hz and fmax = 8000 Hz), resulting in fLi and fRi. For safety, a pressure
limit switch with the algorithm Eq. (1) is used to limit the inflating
pressures of the chambers.
(1)
The proportional gain (KPPi) is tuned to make the pressure servo
response fast as well as avoid the oscillation. The final tuned gains for the
three exoskeleton hinges are KPP1 = 55 Hz kPa−1 (hip), KPP2 = 90 Hz kPa−1
(knee) and KPP3 = 110 Hz kPa−1 (ankle).
The position controller (Fig. 2c) estimates the error between the target
and actual bending angles (i.e. ∆θi) of the exoskeleton hinges and then
generates the reference pressure (Pri) through a Proportion-Integral (PI)
component. The reference pressure is then applied to the CPAM through the
pressure servo and thus actuate the exoskeleton hinge to bend according to
the target signals.
The proportional gain (KPθi) is to improve the response speed while the
integral gain (KIθi) is to eliminate the static error of the bending angles. For
the position control, the gains are tuned by the Good Gain method [13],
resulting in KPθ1 = 43 kPa rad−1, KIθ1 = 80 kPa rad−1 s−1 (hip), KPθ2 = 
70 kPa rad−1, KIθ2 = 130 kPa rad−1 s−1 (knee) and KPθ1 = 86 kPa rad−1, KIθ1 
= 158 kPa rad−1 s−1 (ankle).
The pressure servos and position controllers are applied to control the
exoskeleton to track a standard gait cycle [14]. Figure 3a shows that the
proposed exoskeleton can achieve the motion of all phases in the gait cycle.
It should be noted that gait cycle time is elongated into 40 s in the
experiment due to limit of the exoskeleton's response speed.

Fig. 3 The exoskeleton tracking the joint angles of a gait cycle. a The final states of different phases.
b-d The joint angles. ➀ Initial contact. ➀~➁ Loading response. ➁~➂ Mid stance. ➂~➃ Terminal
stance. ➃~➄ Pre swing. ➄~➅ Initial swing. ➅~➆ Mid swing. ➆~➇ Terminal swing
According to Fig. 3b-d, the bending angles of the exoskeleton hinges
can be controlled effectively. In the stable stage (10-120 s), the maximum
tracking errors of the hip, knee and ankle joints are 5.9°, 7.4° and 1.7°
respectively. The tracking errors majorly occur when the reference
pressures (Pri) cross zero. The inflating pressures of the left (right)
chambers of the exoskeleton hinges should be zero when the reference
pressures are negative (positive) in theory, i.e. PLi = 0 if Pri < 0, and PRi = 0
if Pri > 0. But in practice, due to the offset of the pressure sensors and the
residual of the regulators, it is impossible to measure or control the zero
pressure. Therefore, to ensure the controllability of the inflating pressure,
we set a lower boundary for it, i.e. Pmin = 0.01 MPa. It is why the inflating
pressures (PLi and PRi) cannot track the reference pressures (Pri) in the
range from −0.01 to 0.01 MPa, which is the main cause of the position
tracking errors.
Figure 3b-d also indicates that the static response of the exoskeleton
exhibits an overall delay of 0.64 s (hip), 0.58 s (knee) and 0.44 s (ankle)

relative to the reference signals. In addition, the transition response (5-10 s)
at start of the tracking shows a natural oscillation frequency of 2.5 Hz (or
period of 0.4 s), which constricts the respond speed of the exoskeleton and
causes the aforementioned time delays. We can reduce the effect of the time
delays on the tracking accuracy by elongating the gait cycle (to 40 s for
instance), as shown in Fig. 4, but at cost of low operation speed.
Fig. 4 Gait tracking of the exoskeleton with cycle time of a 2 s, b 5 s, c 20 s, d 40 s
In general, the proposed exoskeleton shows good controllability despite
the slow response speed. The proposed exoskeleton is able to assist the
basic movement of the lower limb at low speed.
4 Conclusion

In this work, a novel modular soft-rigid pneumatic exoskeleton for lower
limb is presented. It consists of three soft-rigid modules, one foot module
and an external electro-pneumatic control system. In each soft-rigid
module, onboard sensing units are integrated with the exoskeleton for
detecting the inflating pressures and bending angles of the pneumatically
actuated soft hinges, and also transmitting the signals to external
subscribers wirelessly. The exoskeleton is controlled by an external electro-
pneumatic control system which receives the signals from the sensing units
and then supplies the appropriate inflating pressures for the exoskeleton
accordingly. The gait tracking experiments show that the soft-rigid structure
allows the exoskeleton to conform well with human body and its
controllability is verified.
References
1.
Eschweiler, J. M. P., Gerlach-Hahn, K., Jansen-Troy, A., & Leonhardt, S. (2014). A survey on
robotic devices for upper limb rehabilitation. Journal of NeuroEngineering and Rehabilitation.
2.
Rupal, B., Rafique, S., Singla, A., Singla, E., Isaksson, M., & Virk, G. (2017). Lower-limb
exoskeletons: Research trends and regulatory guidelines in medical and non-medical
applications. International Journal of Advanced Robotic Systems, 14, 1-27.
[Crossref]
3.
Redlarski, G., Blecharz, K., Dąbkowski, M., Pałkowski, A., Tojza, P. M. (2012). Comparative
analysis of exoskeletal actuators. Pomiary Automatyka Robotyka.
4.
Schiele, A. (2009). Ergonomics of exoskeletons: Objective performance metrics. In World
Haptics 2009 Third Joint EuroHaptics Conference and Symposium on Haptic Interfaces for
Virtual Environment and Teleoperator Systems (pp. 103-108).
5.
Browning, R. C., Modica, J. R., Kram, R., & Goswami, A. (2007). The effects of adding mass to
the legs on the energetics and biomechanics of walking. Medicine and Science in Sports and
Exercise, 39(3), 515-525.
[Crossref]
6.
Asbeck, A. T., De Rossi, S. M. M., Holt, K. G., & Walsh, C. J. (2015). A biologically inspired
soft exosuit for walking assistance. The International Journal of Robotics Research (IJRR),
34(6), 744-762.
[Crossref]
7.
Yap, H. K., et al. (2017). A fully fabric-based bidirectional soft robotic glove for assistance and
rehabilitation of hand impaired patients. IEEE Robotics and Automation Letters, 2(3), 1383-
1390.
[MathSciNet][Crossref]

8.
Koh, T. H., Cheng, N., Yap, H. K., & Yeow, C. H. (2017). Design of a soft robotic elbow sleeve
with passive and intent-controlled actuation. Frontiers in Neuroscience.
9.
Hassanin A. F., Steve, D., & Samia N. M. (2017). A novel, soft, bending actuator for use in
power assist and rehabilitation exoskeletons. In IEEE International Conference on Intelligent
Robots and Systems.
10.
Yap, H. K., Lim, J. H., Nasrallah, F., Goh, J. C. H., & Yeow, R. C. H. (2015) A soft exoskeleton
for hand assistive and rehabilitation application using pneumatic actuators with variable
stiffness. In 2015 IEEE International Conference on Robotics and Automation (ICRA) (pp.
4967-4972).
11.
Polygerinos P., et al. (2013). Towards a soft pneumatic glove for hand rehabilitation. In 2013
IEEE/RSJ International Conference on Intelligent Robots and Systems (pp. 1512-1517).
12.
OpenStax. (2013). Anatomy and Physiology, 1st ed. Houston: OpenStax.
13.
Haugen, F. (2012). The good gain method for simple experimental tuning of PI controllers.
Modeling, Identification and Control.
14.
Charalambous, C. P. (2014). Repeatability of kinematic, kinetic, and electromyographic data in
normal adult gait. In Classic Papers in Orthopaedics.
OceanofPDF.com

Sensing Methods and Actuation
In this part, the first chapter concerns prediction of the way that flexible
objects will move when manipulated. The effects of friction are simulated,
and then, experiments are performed to validate the results.
For the advanced control of an actuated prosthesis, it is necessary to
derive signals from the remaining muscles. This chapter describes the
development, construction and evaluation of a highly portable sensor.
Next is another chapter dealing practically with the interaction between
a robot and soft tissue.
The human eye alters its focal length by muscular deformation of the
tissue of the lens. In this chapter, a hydrogel is used to obtain the same
objective by the application of an electrical potential.
OceanofPDF.com

(1)
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_18
Real-Time, Dynamic Simulation of
Deformable Linear Objects with Friction
on a 2D Surface
Benjamin Maier1, Marius Stach1    and Miriam Mehl1
Institute for Parallel and Distributed Systems, University of Stuttgart,
Stuttgart, Germany
 
Marius Stach
Email: marius.andre.stach@gmail.com
Keywords Handling of deformable linear objects - Dynamics - Friction -
Real-time simulation
1 Introduction
Stationary robotic arms can be used to help in the assembly process of
various devices, e.g., for populating electronic circuit boards, assembling
pumps or casings of appliances. Many of those fields of application involve
handling flexible, linear objects such as electrical and optical wires, strings
and cords, hoses and flexible tubes. Often, these objects are manipulated on
a workspace with a flat surface, as this simplifies sensing and motion
planning. In order to control their handling, control algorithms can benefit
from models of the linear object, e.g., as is done in model predictive
control. A key requirement is that the computation of the model is real-time,
i.e., ideally having a lower runtime than the duration of the simulated
process.
Much research has dealt with modeling and simulation of deformable
linear objects, which usually are well approximated by one-dimensional

objects. A method of fitting a model to observation data by minimizing an
energy function is presented in [1]. The authors of [2] establish a
framework based on differential geometry for modeling deformable linear
objects. They describe flexure, torsion and extension and extend their static
model to account for dynamics and contact in [3] and [4]. However, one end
of the object is always fixed. The authors in [5] use a similar approach but
represents the linear object using straight segments to reduce computational
costs. Another model based on differential geometry is presented in [6],
discrete cosine transform is applied to reduce the number of parameters and
computational costs, however, it only considers a static scenario. Nonlinear
Finite Element Methods have also been used to model the deformation of
one-dimensional objects [7, 8], but they have the disadvantage of increasing
computational complexity for higher numbers of degrees of freedom.
We base our work on [3] and extend their dynamic model of an
inextensible, deformable linear object in a two-dimensional space by the
possibility to prescribe an arbitrary motion and angle of one end of the
object. Furthermore we account for friction between the object and the
surface. Depending on the scenario and the rigidity of the flexible object,
often only some portion of the object has contact with the surface, whereas
the rest hovers slightly above and experiences no friction, as in Fig. 1. Thus,
our model allows to specify friction forces for portions of the object.
Fig. 1 The left end of a cable is moved and rotated, friction forces occur only at the parts of the
cable, where it touches the surface of the table, as can be seen from the shadow
In order to run our simulation in real-time, we develop an efficient,
parallel simulation program. We give details on our considerations and
optimizations, demonstrate the real-time capabilities and make the source

code freely available. By comparison with experiments we validate the
modeling approach.
The remainder of this paper is structured as follows: In Sect. 2 the
model is derived, in Sect. 3 we address the efficiency of the program and in
Sect. 4 the modelling approach is validated by comparing the simulation to
experiments.
2 Modeling
In this section, we model the dynamic behavior of an inextensible, elastic,
one-dimensional object in a 2D geometric setting. In Sects. 2.1 and 2.2, we
introduce our notation which follows the work of [2]. We extend this
formulation to incorporate the prescribed position and angle at the end of
the object in Sect. 2.3 and friction between parts of the object and the 2D
ground surface in Sect. 2.4.
2.1 Representation of the Linear Object
The linear object of length 
 is defined as the set of points 
 where the
linear coordinate, 
, is the distance along the object of point 
from the start point, 
 This start point is assumed to have the
prescribed position, 
, as depicted in Fig. 2. Let 
 be the angle at
time  between the -axis and the object at coordinate . The Cartesian
coordinates of 
 at time , 
 can now be
formulated as

Fig. 2 Coordinates of the start point 
 and a point 
 on the object
(1)
Derivation with respect to time yields the velocity,
(2)
To spatially discretize the formulation, we subdivide the interval of the
coordinate  into  equal-sized intervals 
 of size
. We use linear hat functions
(3)
as nodal basis with 
 to discretize the angle 
:

Here, the coefficients, 
, are the degrees of freedom. The vector
 fully specifies the discretized state of the object, the 
values therefore serve as generalized coordinates.
This ansatz approximates the shape of the object as a sequence of
circular arcs. Note that, while the presented formulation leads to the same
mathematical objects as in [2], we use a node based approach instead of the
authors' element based indexing, as this simplifies the equations.
2.2 Equations of Motion
To formulate the dynamics, we use the Lagrangian form of the equations of
motion,
(4)
where the Lagrange function 
 is the difference between kinetic
energy 
 and potential energy 
.
The kinetic energy 
 depends on the density  given as mass per cross
section area. For now, we assume 
 and, thus, 
.
Then, 
 is given by
(5)
Inserting (2) and (5) into (4) yields the kinetic energy contribution to the
Lagrange equation of motion,
The entries 
 and 
 of the matrices 
 and 
 are defined
as

(6)
where the abbreviations
(7)
are used.
The potential energy 
 is assumed to result from flexural bending of
the object. It is given by
with the tridiagonal stiffness matrix,
and flexural rigidity 
. Combining the contribution of 
 with the term
for 
, we get the equations of motion,
(8)

2.3 Prescribed Position and Angle of the Start Point
In this section, we extend the formulation to account for the prescribed,
time varying displacement 
 of the start point 
 as
formulated in Eq. (1). Furthermore, the start angle of the object at 
 is
also set to a prescribed transient value.
Consequently, the new kinetic energy 
 has contributions from the
prescribed movement that is superimposed on the internal movement of the
object. Following definitions (1) and (5), we get
with the additional contribution 
. When calculating the
derivatives of 
 with respect to 
 and , as needed for the Lagrange
equations of motion (4), one gets values
(9)
These terms have to be added to the left hand side of the governing
equation, Eq. (8).
To prescribe the angle at the start point 
, the respective degree of
freedom 
 is simply fixed to the prescribed value, 
, and removed
from the vector of unknowns in the solution process.
2.4 Friction on the Underlying Surface
Next, we add a formulation for the friction to our model. Friction forces
occur between the object and the surface on which it slides when being
manipulated. To be able to specify friction for any portion of the object, we
define 
 discrete friction forces 
 acting at equidistant positions,

 for 
. Every force is proportional to the gravitational
force 
 at its point of action and to the respective sliding friction
coefficient 
. In order to specify no friction at some intervals of the object,
the corresponding values of 
 can be set to zero. The direction of the
friction forces is always opposite to the current direction of motion of the
points of action. Thus, the discrete friction forces can be formulated as
with the velocities 
. In general, dissipative
forces can be considered by adding generalized forces 
 to the Lagrange
equations of motion:
For the friction, 
 is calculated as
(10)
with the symbols
(11)
and
(12)

Finally, the governing equation that follows from Eqs. (8)-(12) is given
by
(13)
where the entries 
, 
 and 
 of matrix 
 and vectors  and 
 are
given by Eqs. (11), (12) and (9), respectively. Note that this is a nonlinear
equation in the vector of coordinates 
 because all vectors and matrices
also depend on 
.
By defining 
 we transform Eq. (13) into a system of first-
order, nonlinear differential equations:
(14)
An explicit time stepping scheme can be used to solve this system of
differential equations in the variables 
 in time. Consequently, a
linear system with system matrix 
 has to be solved in every time step.
3 Efficient Implementation
In the following, we illustrate how the presented model can be solved
efficiently to enable real-time computation. In Sects. 3.1-3.3, we present
algorithmic, numerical and methodological considerations that reduce the
runtime of a simulation program compared to naïve approaches. We give
details on our reference implementation, which we make available as an
open-source project.1 For every efficiency consideration, we quantify the
effect in our implementation.
3.1 Algorithmic Considerations
A simulation program has to compute the vectors  and 
 and the matrices
 and 
 of Eq. (14) in every timestep. We consider the following

term, which appears twice in the computation of 
 and once in the
computation of 
:
(15)
To save computation time, in every timestep we first compute the values
of
(16)
for all required combinations of the indices 
 and 
 each of them
occurring twice in Eq. (15). Thus we can get every value of 
 just
by adding four of these terms. In our implementation, we measured a
reduction in the total simulation runtime by 77%.
A second opportunity to precompute values comes with the symmetry
properties of the involved matrices. It can be easily verified that the
matrices 
 and 
 as well as the expressions in Eq. (16) are
symmetric whereas matrix 
 is skew-symmetric. Computing the off-
diagonal entries once and reusing them leads to an additional drop in
computation time by 32% in our implementation.
3.2 Numerical Considerations
Several of the computed quantities are defined as integrals over a part or the
whole length of the linear object, e.g., 
 and 
. For their
computation, a numerical quadrature scheme has to be chosen. Such
schemes typically approximate the integral value by a weighted sum of the
integrand evaluated at specified sampling points. For example, the
composite Simpson's rule is such a scheme. It uses equally spaced sampling
points and is known to integrate polynomials of up to order 3 exactly.
Also adaptive schemes exist which subdivide the integration interval
recursively until a required error tolerance is met. The adaptive Archimedes
quadrature is such a scheme. It approximates the integrals by the surface
areas of trapezoids.

We determined by numerical experiment that when using the same
composite Simpson's rule for every quadrature that occurs during
computation, at least 14 sampling points per rule are required, for the whole
simulation to remain numerically stable. When using the adaptive
Archimedes quadrature instead, in our case we have to set the error
tolerance parameter to 
. It turns out that as a result we get only 2-3
function evaluations per quadrature on average, which reduces the total
computation time by 44%. An explanation for this effect is that the
occurring integrals are diverse, some have small integration intervals where
the integrand is smooth while others involve more complex integrands.
There exist nested integrals, e.g., to compute 
 in Eq. (6), we integrate
over an expression containing 
, which again consists of an integral, as
can be seen in Eq. (7).
To implement the quadrature efficiently, it is crucial to only integrate
over the support of the integrands. E.g., in the interval 
 only the
ansatz functions 
 and 
 have nonzero values.
For the solution of the differential equations, Eq. (14), we use a forth
order Runge-Kutta scheme. Using such a high order scheme allows to
achieve stable results with larger time steps. Compared to using the first
order forward Euler integration scheme, we could increase time steps such
that the total computation time was reduced by 75%.
3.3 Technical Considerations
If a real-time implementation is favored, a suitable low-level programming
language should be selected. By transferring our initial MATLAB
implementation to C++ code, we reduce the runtime by 96%.
The computations of the entries of the matrices in Eq. (14) are
independent of each other and thus can be done concurrently. We parallelize
our code to run on multiple cores of shared-memory computers by
employing OpenMP.2
To assess the efficiency of our code, we use the Gprof tool to profile
simulation runs. This can reveal bottlenecks resulting from a non-optimal
implementation, which can be fixed in consequence. Thus, among other
optimizations, we removed avoidable 'if' branches in the implementation of

the ansatz function given by Eq. (3) and reduced the number of arithmetic
operations in the adaptive quadrature implementation. The two mentioned
functions account for 23 and 52% of the total runtime. In total,
improvements lead to runtime reductions of factors of approximately 2-3.
4 Experiments
In this section, four simulation experiments are carried out using the
developed model. In the first two, we demonstrate the effects of friction and
bending, in the last two, we compare the results to real experiments. Finally,
we present runtime and scalability results.
4.1 Experiments #1 and #2—Friction and Bending
In experiment #1, the parameters 
 and 
 are set to 1. Initially, the
one-dimensional object is positioned horizontally with its start point on the
right side. The start point is then constantly accelerated in y-direction with
 for a time span of two seconds. The angle at the start point is not
prescribed. Two runs are performed where the friction coefficient, 
, is set
for the whole object to 0 or 1, respectively, corresponding to no friction and
constant friction. The time step width is set to 
 and 
elements are used.
The results of these two runs are depicted in the left and right image in
Fig. 3. The trajectories of the prescribed start point and of the opposite end
of the object are shown by the solid blue and dashed grey lines. The
colorful lines depict the states of the linear object in intervals of 0.2 s. It can
be seen that the friction prevents the object from swinging to the right side.
Instead, the object gets dragged along a path that closely follows the
prescribed trajectory.

Fig. 3 Experiment #1—accelerated movement in 
-direction from bottom to top left: no friction,
right: with friction
In the two runs of the next experiment, #2, the flexural rigidity, 
, is
set to the two different values 1 and 5, while the remaining parameters are
set to 
 and 
. The initial position of the object is
horizontal with the start point on the left side. The start point is moved to
the right side for one second, following the prescribed trajectory
 Additionally, the angle of the object at the start point
describes a prescribed counterclockwise rotation of 
. Like in
experiment #1, 
 elements are used with a time step width of
.
Figure 4 shows the results of both runs, the depicted states of the object
are in the interval of 0.1 s. It can be seen that the object bends to the right at
first, then to left as it is pushed over the surface. In the case with lower
flexural rigidity, inertia causes the object to strongly bend such that the free

end finally points downwards whereas in the case of a more rigid object the
direction stays approximately the same.
Fig. 4 Experiment #2—accelerated movement in 
-direction and counterclockwise rotation of the
start angle with different flexural rigidities. top: Rflex = 1, bottom: Rflex = 5
These two examples have shown that bending and friction play an
important role to the dynamics of a one-dimensional object in the described
setting. The presented modelling approach is capable of describing these
effects and their influence can be studied in simulations by varying material
parameters.
4.2 Experiments #3 and #4—Validation
Next, we compare our model to experiments carried out with the help of a
robotic arm. In experiment #3, a cable is placed horizontally on a wooden
table with the right end being gripped by a Franka Emika Panda robot. The
gripper is turned counterclockwise by 180° for ten seconds with a constant
angular velocity, as shown in Fig. 5. The length from the gripper and the
density of the object are 
 and 
 Flexural rigidity
and friction coefficient are estimated as

Fig. 5 Experiment #3—manipulation of a cable
 and 
. The same action is simulated with
 elements and a time step width of 
.
Experiment #4 is conducted using a flexible tube. Again, the gripper
holds one end of the object and performs a defined movement. The
movement in this scenario is a translation by the vector 
 and
a rotation by 135°. The movement lasts four seconds and has a smooth
velocity profile of zero velocity, acceleration and jerk at both 
 and
. The experiment is visualized in Fig. 6. Again, a simulation of the
scenario is run with 
 elements, for this case with time step width
. The used parameters are
 and 
.

Fig. 6 Experiment #4—manipulation of a flexible tube
The simulation results of experiments #3 and #4 are depicted in Fig. 7.
In the left image, the state of the simulated object in experiment #3 is
shown for intervals of 0.5 s. By visually comparing them to the photos in
Fig. 5, we find a good match. In both experiment and simulation, the object

initially starts to bend with the free end remaining at a similar location until
approximately 
, before the whole object rotates and maintains a
rigid shape. Similarly, the simulation results of experiment #4 compare well
to the photos in Fig. 6. It can be seen that the final position matches well.
Fig. 7 Simulation results for experiment #3 (left) and #4 (right). The colorful lines depict states of
the object in intervals of 0.5 s
Additionally, the simulation results show that, in the beginning, the left
end of the tube moves slightly upwards. This behavior was also observed
during the experiment.
Further comparisons between experiments and simulations are given in
Fig. 8. The movements of experiments #3 and #4 are captured from above
and overlaid with respective simulation results. For the left image, the same
velocity profile as for experiment #4 has been used for the rotation. Again, a
good match is found.
Fig. 8 Overlay of simulation results (blue dotted lines) over pictures of experiment #3 with
accelerated start and end of the movement (left) and experiment #4 (right)

The comparisons between experiments and simulations show that the
model can accurately compute real behavior of deformable flexible objects,
such as cables and flexible tubes. The choice of using three elements in the
discretization means that the simulated object is comprised of three circular
arcs. The visual comparison with the experiment justifies this low number
for the presented scenarios.
4.3 Evaluation of Runtime
Table 1 compares the durations 
 of the presented experiments and the
runtimes of the simulations. We executed each simulation on a computer
with Intel® Xeon® E3-1585 v5 quad-core processor with base frequency of
3.5 GHz and 8 MB cache. We measured runtimes 
 and 
 using 1, 2
and 4 threads. With runtime we mean the total duration of the simulation
program, including input and output and without any visualization step. The
parallel efficiency for the run with 4 threads, 
 is listed, as
well as the real-time factor 
. A value higher than 1 means that the
simulation took less computational time than the process in the real
experiment.
Table 1 Runtimes of the simulation
Experiment Duration in
reality (s)
Runtime
Parallel efficiency
(%)
Real-time
factor
4
threads
2
threads
1
thread
#1
2
661 ms
785 ms
1.26 s
48
3.0
#2
1
356 ms
394 ms
610 ms
43
2.8
#3
5
4.10 s
5.36 s
8.61 s
53
1.2
#4
4
1.44 s
1.66 s
2.56 s
44
2.8
It can be seen that for each experiment the runtime decreases, if more
threads are used, as the computational work is distributed to more cores.
Because only the computation of the matrix and vector entries in Eq. (14) is
parallelized, the parallel efficiency for 4 threads is at a low value of around
50%.
The absolute value of the runtime for a simulation depends mainly on
the used time step width, which, in turn, has to be small enough for the

simulation to remain stable. This depends on the scenario, number of
elements and material parameters. In the simulations carried out in the
previous sections, the time step widths could be chosen such that a
computation time below real-time was possible. This is shown by the real-
time factors in the last column of Table 1.
5 Conclusion
This paper developed a dynamic model of an inextensible, one-dimensional
object in a 2D geometric setting, based on differential geometry and using
the Lagrangian formulation of the equations of motion. The model accounts
for a prescribed position and rotation of one end point and models linear
friction between parts of the object and the underlying table. The model was
implemented as open source code and shown to run in real-time.
Algorithmic, numerical and technical considerations of an efficient
implementation were presented. Simulations were carried out to show the
behavior of the model. Experiments with a robot arm were conducted and
reproduced by the simulation. Finally, the simulation runtime was evaluated
and shown to be up to a factor of 3 below the duration of the process in the
experiment.
Future work can use this model for control or combine it with a
simulation in a 3D space over the 2D workspace, where also gravity plays a
role.
Acknowledgements
The authors acknowledge the International Research Training Group on
Soft Tissue Robotics "Simulation-Driven Concepts and Design for Control
and Automation for Robotic Devices Interacting with Soft Tissues" funded
by Deutsche Forschungsgemeinschaft as GRK 2198/1, as well as the
support from the Institute for Control Engineering of Machine Tools and
Manufacturing Units at the University of Stuttgart.
References
1. Javdani, S., Tandon, S., Tang, J., O'Brien, J. F., & Abbeel, P. (2011). Modeling and perception of
deformable one-dimensional objects. In 2011 IEEE International Conference on Robotics and
Automation (pp. 1607-1614). Shanghai.

1
2
2.
Wakamatsu, H., & Hirai, S. (2004). Static Modeling of linear object deformation based on
differential geometry. The International Journal of Robotics Research, 23(3), 293-311.
3.
Wakamatsu, H., Takahashi, K., & Hirai, S. (2005). Dynamic modeling of linear object
deformation based on differential geometry coordinates. In Proceedings of the 2005 IEEE
International Conference on Robotics and Automation (pp. 1028-1033). Barcelona, Spain.
4.
Wakamatsu, H., Yamasaki, T., Tsumaya, A., Arai, E., & Hirai, S. (2006). Dynamic modeling of
linear object deformation considering contact with obstacles. In 2006 9th International
Conference on Control, Automation, Robotics and Vision (pp. 1-6). Singapore.
5.
Huang, J., Di, P., Fukuda, T., & Matsuno, T. (2008). Dynamic modeling and simulation of
manipulating deformable linear objects. In 2008 IEEE International Conference on Mechatronics
and Automation (pp. 858-863) Takamatsu.
6.
Luo, C., Mo, Z., Zhou, Y., & Kuang, M. (2017) Static modeling and simulation of linear object
based on differential geometry and discrete cosine transform. In 2017 IEEE International
Conference on Mechatronics and Automation (ICMA) (pp. 1342-1347). Takamatsu.
7.
Simo, J. C., & Vu-Quoc, L. (1988). On the dynamics in space of rods undergoing large motion: a
geometrically exact approach. Computer Methods in Applied Mechanics and Engineering, 66(2),
125-161.
8.
Boyer, F., & Primault, D. (2005). Finite Element of nonlinear cable: applications to robotics. Far
East Journal of Applied Mathematics, 19(1), 1-34.
[MathSciNet][zbMATH]
Footnotes
https://​github.​com/​maierbn/​dynamic_​linear_​object.
 
www.​openmp.​org/​.
 
OceanofPDF.com

(1)
 
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_19
A System for Capturing of Electro-
Muscular Signals to Control a Prosthesis
Zeming Zhao1   , Bo Lv1, Xinjun Sheng1    and Xiangyang Zhu1
State Key Laboratory of Mechanical System and Vibration, Shanghai
Jiao Tong University, 800 Dongchuan Road, Minhang, Shanghai,
China
 
Zeming Zhao
Email: zemzhao@163.com
Xinjun Sheng (Corresponding author)
Email: xjsheng@sjtu.edu.cn
URL: http://bbl.sjtu.edu.cn
Keywords EMG acquisition device - HD-sEMG
1 Introduction
In recent years, more and more attention has been paid to the amputated
patients. However, it is not good enough to focus on treating their illness
only. It is also crucial to improve their quality of life in the future and help
them return to their normal lives [1, 2]. Installing a prosthesis is an ideal
solution to the problems mentioned above. The prosthesis can be traced to
Ancient Rome, which was used as decorations merely for many years in
that era. In recent years, with the development of driver and electronic
technology, controllable prosthetic hands have emerged. With the increasing
functions and degree of freedoms (DOFs) of the prosthetic hands, it is
getting harder and harder to control it properly. Electromyography (EMG)
signal whose essence is the superposition of the action potential of the

muscle tissue cells occurring during voluntary contraction is considered to
be an excellent carrier for controlling the multi-DOF prosthetic hands. It is
intuitive for people to use it to control the prosthesis to a desired goal.
Because the amputees need little  onerous and suffering anti-intuitive
training. EMG signal can be divided into two parts, surface EMG (sEMG)
signal and invasive EMG (iEMG) signal. Compared with iEMG, sEMG is
more popular because of its non-invasive acquisition, safety, and
convenience [3]. In this paper, all the work focuses on sEMG only.
In 1967, During [4] first used sEMG collected from a pair of
antagonistic muscles to control a prosthesis successfully. Since then, a lot of
work has been done on the inverse decomposition and application of the
sEMG signal. Before 2000, much work has focused on exacting relevant
features from the sEMG signal to reduce the data dimension and then
classify them by using pattern recognition algorithms such as LDA or SVM
[5, 6]. In the past 20 years, people began to use sEMG signal to complete
much more complicate tasks such as synchronous proportional control [6]
for some specific joint angles and decomposition of MUAPt [7]. Previous
work like During did, though brilliant, requires only a few channels of
sEMG data because it can be done with relatively little information of
muscles. However, as much information as possible has to be collected
from the muscles we interested to complete the complicate tasks mentioned
above. Under this circumstance, the high-density sEMG (HD-sEMG)
acquisition equipment which can easily meet the extremely high
requirement is a substitute for the traditional equipment. It supports
deploying tens or even hundreds of electrodes on the upper arm at the same
time, which means we can collect even hundreds of channels of sEMG data
simultaneously from the target area.
Because it has a promising prospect, many companies have introduced
their HD-sEMG acquisition system. DELSYS INC. launched NeuroMap
System, Bagnoli System and Tiber [8]. TMSi launched Refa and SAGA [9].
OT Bioelectronica developed a series of products such as Quattrocento,
EMG-USB2+, Sessantaquattro, Quattro, and Due [10]. Among all these
systems, Tiber, Quattro, Due, and SAGA are portable devices. Referring to
the technical indicators like number of acquisition channels, sampling
frequency, A/D Resolution, these portable devices are not comparable with
the desktop devices. Most of the commercial portable HD-sEMG
acquisition devices have only a sampling frequency of 1 kHz at most. Since

the energy of sEMG signal is mainly distributed in the frequency range of
20-450 Hz, the device only with a sampling frequency of 1000 Hz can
only meet the minimum requirements of the sEMG acquisition equipment
[11, 12], which is not suitable for high-quality sEMG acquisition. But on
the other hand, desktop devices are not portable. This limits the location and
the condition of experiment that makes harder to recording sEMG signal.
Herein, we develop a new portable sEMG recording system with high
performance. In this paper, the acquisition system will be introduced first.
After that, the results of the test and related experiment will be presented
briefly.
2 System Description
As we mentioned above, it is essential to develop a portable HD-sEMG
acquisition system of high performance. The system we developed will be
introduced briefly in this chapter. And the block diagram of the system is
shown in Fig. 1. The specific parameters of the system are listed in Table 1.
Fig. 1 Block diagram of system
Table 1 Comparsions of specification parameters
 
Tiber
Quattrocento
Proposed System
Channels
64, 128
96, 192, 288, 284
64, 128, 192, 256
A/D resolution
16 bits
16 bits
12, 16, 20 bits

 
Tiber
Quattrocento
Proposed System
Dynamic range
11 mVpp
33 mVpp
21 mVpp
Noise baseline
<1.5 μVrms <4 μVrms
<6 μVrms (16 bits)
Sampling rate
2000
512, 2048, 5120, 10240
500, 1000, 2000, 4000
CMRR (dB)
<−80
<−95
<−85
Bandwidth (Hz) 10-450
High pass filter: 0.7, 10, 100, 200
Low pass filter: 130, 500, 900, 4400
User-defined
The whole system consists of two parts: the acquisition device and the
adaptable upper computer software. The acquisition device is expandable so
that the number of acquisition channels can be adjusted from 64 to 256. It
can satisfy the demand of the experimental needs while ensuring the
lightweight and low power consumption of the equipment as much as
possible. On this basis, to use it more convenient, we develop the adaptable
upper computer software to form a complete acquisition system. The
software supports essential functions such as data storage, real-time data
display, making triggers, setting configuration, etc. Ethernet is used for
communication between upper and lower computers to ensure the integrity
of data transmission. Figure 2 shows the proper method to collect sEMG
signals from forearm by using this system.

Fig. 2 Usage of the device: a Power supply (2S Li battery); b bias-driven electrode; c reference
electrode; d device (main control module & signal acquisition module); e extension cable; f 64-
channel array electrode; g preamplifier module; h arm of subject; i ethernet cable
sEMG electrodes can be measured in a referential montage [13-15] in
which each of the electrodes(f) is measured with respect to a single
reference electrode(c). Array-electrode is connected to amplifier module
which is linked to signal acquisition module with an extension cable. And
the main control module is linked with upper computer through ethernet
cable(i). And we can use PC software for a variety of functions.
2.1 HD-SEMG Acquisition Device
The device consists of three main modules: preamplifier module, signal
acquisition module, and main control module [16]. The basic block diagram
of this device is shown in Fig. 3. And the photos of the assembled PCB are
shown in Fig. 4.

Fig. 3 Block diagram of device

Main Control Module
The main control module shown in Fig. 4a has
two main functions: controlling of the overall functionality of the whole
hardware, and data pre-processing. The microcontroller unit (MCU) of the
main control module is ARM Cortex M4 microcontroller (STM32F407),
whose dominant frequency is up to 168 MHz. Furthermore, it is equipped
with a Floating-Point Unit (FPU) and a DSP instrument set, which makes it
control the peripherals easier and data pre-processing faster [17].
Fig. 4 Photo of main assembled PCB: a photo of MCU board (main control). The dimension of
circuit is 100 × 100 mm; b photo of AD board (signal acquisition). The dimension of the circuit is
100 × 100 mm; c, d photo of AMP board (preamplifier). The dimension of the circuit is 50 × 70 mm
MCU communicates with the upper computer (Human-Machine Interface)
using Fast Ethernet (100 Mbps). LAN8720A which supports

communication with an Ethernet MAC via a standard RMII interface is
selected as the physical layer (PHY) transceiver. TCP/IP Protocol is used to
ensure the integrity of the data transferring. MCU communicates with the
signal acquisition module using Serial-peripheral Interface. And the SPI
clock is set to run at 20 MHz. Direct Memory Access (DMA) channel is
used here to save the computing power of MCU. Data pre-processing
including channel selection, digital filtering, and data packaging is
controlled by the upper computer.
Signal Acquisition Module
Each signal acquisition module which is
shown in Fig. 4b, offers 64 unipolar inputs with matching analog and
components—filters, amplifiers, A/D converter, and serial-peripheral
interface (SPI)—fully integrated in a quite small footprint. The ADS1299
provides a low-noise programmable gain amplifier (PGA) for each channel
to achieve different demands of biopotential measurement. For instance,
gain of PGA is set to 24 in this system [14]. With an instrument amplifier
AD8227 placed on the Preamplifier Module, this system's specifications for
low input-referred noise level (6 μV) and dynamic range (±20 mV;
resolution of 0.610 μV/bit) are well-suited for measuring sEMG signal.
ADS1299 includes an on-chip sinc digital filter to cutoff the noise with the
frequency more than sampling frequency [18].
Preamplifier Module
The whole device is not suitable for wear due to its
size. Therefore, the sEMG signal needs to be amplified before connecting to
the signal acquisition module through the extension line in order to ensure
the signal quality, so that the preamplifier module between the electrodes
and the acquisition module is needed. PCB of this module is shown in
Fig. 4c, d.
Each preamplifier module, consisting of 64 sets, can amplify 64 channels of
sEMG signal simultaneously. Instrument amplifier AD8227 is selected to
reduce the power consumption of the whole system. Amplification
multiples of these amplifiers are designed as the minimum amplification
factor, which is five times. This is aimed to reduce the noise leaded-in from
the amplifier to the greatest extent. Furthermore, the board is designed on a
4-layer PCB to make sure the analog signal lines through holes at most
once. This design is aimed to reduce the circuit noise as far as possible [19].

2.2 PC Software
The PC software is a GUI coded by using PyQt5, which is a comprehensive
set of Python bindings for Qt v5 that has excellent cross-platform
performance. Furthermore, matrix computation in the program is using
NumPy, and another numerical computation is integrating C++ code. This
can relieve the problem that Python programs run much slower than other
programming languages like C++, Java, C#. The software is responsible for
these functions: setting configuration for the overall system, real-time
display, data processing and so on.
Data processing is a set of digital filters actually, including a bandpass
filter (User defined, 10-450 Hz by default) and a notch filter (50 or 60 Hz)
to reduce the unwanted noise. There is no more tautology about the PC
software here, because of the limitation of length. Figure 5 shows the
Graphical User Interface (GUI) of the system.

Fig. 5 GUI of the system: a start interface, b option interface (control tab), c option interface
(information input tab), d real-time display interface, e experiment interface
3 Device Testing and Results
3.1 Testing
Device testing includes two parts. One is the performance testing for the
system to test the relative performance parameters. The other is a quite easy
experiment about the gesture recognition to test the overall system.
The performance testing is to measure the system signal quality of the
system. One is to measure the input-referred noise by recording the data

when two differential inputs of the amplifier and the reference electrode.
The other is to compute the signal to noise ratio (SNR) as follows:
(1)
The experiment of gesture recognition is to extract four traditional time-
domain features (WL, MAV, SSC, ZC) from the sEMG signal every 100 ms
to make a high-dimensional vector. After that, PCA and LDA are used to
classify the sEMG signal efficiently [20, 21].
3.2 Results
The input-inferred noise of ADS1299 itself is less than 1 μVRMS. And the
input-inferred noise of the system is measured to be 5.7 μVRMS in this
system when the collecting device is very close to the interference source
(0.5 m from the mains-powered computer). In practice, total baseline level
of ≤10 μV is readily achievable.
The result of the SNR of system is shown as Fig. 6. And SNR of system
is 29.614 dB which is very close to 30 dB. Note that a SNR = 9.54 dB
means the amplitude of sEMG signal is 3 times above the baseline noise
level. As s rule of thumb, signals with SNR ≥10 dB can be considered high
quality, and even signals ≥3 dB are sufficient for detections of on and off.

Fig. 6 The result of the RMS calculation (sEMG signal acquired by system (black); smoothed RMS
signal overlaid (orange))
The result of the 7-class gesture classification which is shown in Fig. 7
is more than 95% accuracy that shows the device can satisfy the most
experimental requirements.

Fig. 7 The results of the 7-class gesture recognition
4 Conclusion
In this paper, a high-performance HD-sEMG acquisition system is
introduced. The size of the device is 105 mm × 110 mm × 90 mm. And it
weighs less than 230 g (64 channels). The results of the test mentioned in
the previous chapter shows that it can satisfy the experimental requirements
of various sEMG experiments.
Acknowledgements
This work is supported in part by the National Natural Science Foundation
of China (Grant No. 91748119, 51905339), and by the Science and
Technology Commission of Shanghai Municipality (Grant No.
18JC1410400).

References
1.
Farina, D., Jiang, N., Rehbaum, H., et al. (2014). The extraction of neural information from the
surface EMG for the control of upper-limb prostheses: emerging avenues and challenges. IEEE
Transactions on Neural Systems and Rehabilitation Engineering, 22(4), 797-809.
[Crossref]
2.
Ahsan, MDR. (2010) Advances in electromyogram signal classification to improve the quality of
life for the disabled and aged people. Journal of Computer Science, 6(7), 706-715.
3.
Vasquez Tieck, J. C., Weber, S., Stewart, T. C., Roennau, A., & Dillmann. R. (2018). Triggering
robot hand reflexes with human EMG data using spiking neurons. In M. Strand, R. Dillmann, E.
Menegatti, & S. Ghidoni (Eds.), Intelligent Autonomous Systems 15. IAS, Advances in Intelligent
Systems and Computing (vol. 867). Springer, Cham.
4.
During, J., & Miltenburg, T. C. M. V. (1967). An EMG-operated control system for a prosthesis.
Medical and Biological Engineering, 5(6), 597-601.
[Crossref]
5.
Ishii, C., Harada, A., Nakakuki, T., et al. (2011) Control of myoelectric prosthetic hand based on
surface EMG. In IEEE International Conference Mechatronics & Automation.
6.
Moura, K. O. A., Favieiro, G. W., & Balbinot, A. (2016) Support vectors machine classification
of surface electromyography for non-invasive naturally controlled hand prostheses. In 2016 38th
Annual International Conference of the IEEE Engineering in Medicine and Biology Society
(EMBC). IEEE.
7.
Li, C., Zhou, Y., & Chen, Y. (2015). The research of fractional order control system based on
surface EMG signal. In 2015 3rd International Conference on Mechatronics and Industrial
Informatics.
8.
Delsys family of products. https://​www.​delsys.​com/​products/​.
9.
TMSi-high density EMG (HD EMG). https://​www.​tmsi.​com/​applications/​hd-emg/​.
10.
OT-hardware. https://​www.​otbioelettronica​.​it/​en/​products/​hardware.
11.
Benatti, S., Casamassima, F., Milosevic, B., et al. (2015). A versatile embedded platform for
EMG acquisition and gesture recognition. IEEE Transactions on Biomedical Circuits and
Systems, 9(5), 620-630.
[Crossref]
12.
Yazicioglu, RF., Merken, P., Puers, R., et al. (2008). A 200 $\mu$ W eight-channel EEG
acquisition ASIC for ambulatory EEG systems. In IEEE International Solid-state Circuits
Conference-digest of Technical Papers. IEEE.
13.
Bohorquez, J. L., Yip, M., Chandrakasan, A. P., et al. (2011) A biomedical sensor interface with a
sinc filter and interference cancellation. IEEE Journal of Solid-State Circuits, 46(4), 0-756.

14.
Wang, C. (2012) Design of a 32-channel EEG system for brain control interface applications.
Biomed Research International, 2012(1), Article ID 274939.
15.
Brunelli, D., Farella, E., Giovanelli, D., et al. (2016). Design considerations for wireless
acquisition of multichannel sEMG signals in prosthetic hand control. IEEE Sensors Journal,
16(23), 8338-8347.
16.
Lopez, C. M., Prodanov, D., Braeken, D., et al. (2012). A multichannel integrated circuit for
electrical recording of neural activity, with independent channel programmability. IEEE
Transactions on Biomedical Circuits and Systems, 6(2), 101-110.
[Crossref]
17.
STM32F407ZGT6 High-performance foundation line, ARM Cortex-M4 core with DSP and
FPU, 1 Mbyte Flash, 168 MHz CPU, ART Accelerator, Ethernet, FSMC. https://​www.​st.​com/​
resource/​en/​datasheet/​stm32f407zg.​pdf.
18.
ADS1299-x low-noise, 4-, 6-, 8-channel, 24-bit, analog-to-digital converter for EEG and
biopotential measurements datasheet (Rev. C). http://​www.​ti.​com/​product/​ADS1299.
19.
Maereg, A. T., & Castellini, C. (2015) Low-cost wearable multichannel surface EMG acquisition
for prosthetic hand control. In IWASI—International Workshop on Advances in Sensors and
Interfaces. IEEE.
20.
Zhang, X., Huang, H., & Yang, Q. (2013) Real-time implementation of a self-recovery EMG
pattern recognition interface for artificial arms. In Engineering in Medicine & Biology Society.
IEEE.
21.
Tavakoli, M., Benussi, C., & Lourenco, J. L. (2017). Single channel surface EMG control of
advanced prosthetic hands: a simple, low cost and efficient approach. Expert Systems with
Applications, 79, 322-332.
[Crossref]
OceanofPDF.com

(1)
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_20
Challenges in Robotic Soft Tissue
Manipulation—Problem Identification
Based on an Interdisciplinary Case Study
of a Teleoperated Drawing Robot in
Practice
M. Wnuk1   , F. Jaensch1, D. A. Tomzik1, Z. Chen1, J. Terfurth1,
S. Kandasamy1, J. Shahabi1, A. Garrett1, M. H. Mahmoudinezhad1,
A. Csiszar1, W. L. Xu1, O. Röhrle1 and A. Verl1
University of Auckland, Auckland, New Zealand
 
M. Wnuk
Email: markus.wnuk@isw.uni-stuttgart.de
Keywords Soft material - Problem formulation - Manipulation - Robot -
Teleoperation - Case study
1 Introduction
A large part of the current research in robotics is motivated by industrial
needs, i.e. through shortcomings in applications associated with production
and automation systems, in which rigid industry robots interact with rigid
materials. The rigid robots in combination with rigid materials are therefore
a well-investigated field and these robot applications have become an
indispensable part of the production industry. Methods for the problem
formulation and solving methods have been developed and have found their
way into the state of the art in research and industry.

If rigid robots need to interact with highly-deformable soft materials,
the level of knowledge is lower. The interaction between rigid robots and
soft materials presents challenging and unresolved questions. Existing
methodologies developed for the interaction of rigid robots with hard
materials cannot be transferred to the interaction with soft materials. New
methodologies and principles need to be developed and derived for this
highly interdisciplinary topic involving simulation technology,
computational modelling, sensing, robotics and control methods.
The major obstacles in developing suitable solutions from robotic
devices that safely and adequately interact with soft tissues are the lack of
information and knowledge on how soft tissues/materials deform and how
signals can be effectively recorded and accurately interpreted to provide
real-time feedback to controllers and actuators. The soft tissue manipulation
can only be addressed if this lack of information and knowledge can be
filled through suitable approaches. For facing the challenge to generate that
information close and interdisciplinary collaborations between different
fields of research are required. Those fields cover simulation technology,
cyber-physical engineering, sensors, robotic device technology and
expertise in biomedical engineering, as summarized in Fig.1. The lack of
significant use cases that cover all of these fields makes it difficult to
investigate how to solve problems with this complexity. Even more crucial
is the fact, that there is no comprehensive and cross-thematic problem
formulation, that incorporates all these different fields.

Fig. 1 Research fields involved in the problem of rigid robot soft material interaction
2 State of the Art
The field of research in robotics is vast. To provide an overview on the state
of the art and relevant research questions with respect to this paper, we first
present a very brief overview of key research questions in the areas in
which robotic devices interact with hard materials. This is followed by a
more detailed overview on research of the interaction with soft materials
and on advances in simulation technology that enable their use in the design
and development of new control strategies.
There are many applications using robots, e.g. industrial, service, swarm
and bio-inspired robotics. Independent of the field, a physical robot
generally consists of a kinematic structure with sensors and actuators that
determine its behavior through algorithms in the robot's controller in order
to handle or manipulate objects. Hereby, the robot and the interacting
objects consist of soft or hard materials. Depending on the type of materials
interacting with each other and depending on the fragility of the material,
which could be damaged, different concepts of robots are considered.
The interaction between a rigid robot structure and objects with high
stiffness is typical for industrial robots for handling or assembly [1].

Industrial robots usually interact with workpieces made of wood, metal or
plastic. For industrial robots, research focuses on their dynamic behavior,
accuracy and maximum load by optimizing single components or control
strategies [2]. The gripping process of rigid objects and its related
technologies are addressed in [3, 4]. The challenge is to automatically find
possible gripping positions. Algorithms solving such problems often include
learning strategies [5]. Other applications are the integration of milling and
drilling processes into the field of robotics [6]. For such robots, the
interaction is well known and a high level of automation does already exist.
However, some industrial applications do exist where rigid kinematic
structures handle soft or less rigid objects, e.g. robots handling and forming
objects like toasted bread and pasta in the food industry [7]. Some of these
industrial requirements are addressed within the EU-funded research project
"PicknPack—Flexible robotic systems for automated adaptive packaging of
fresh and processed food products". Similarly, there exist initial concepts
for automated harvesting, for example handling delicate tomatoes [8],
requiring a good force limitation during gripping. This can be achieved by
simulating the maximum allowed force on the tomato [9]. All of these
interactions essentially focus on gripping, in particular on reducing the
gripping force in order to avoid damage of the handled object. The specially
developed algorithms, however, do not use any information beyond force
limits or force controllers. A better understanding of the interaction with
soft tissues would increase the benefit to automation. An improved
understanding of the interaction can only come from sophisticated
simulation techniques and models, since this is the only way to enable an
easy-to-use framework with freely tunable parameters in order to
investigate different interaction scenarios in a cost and time-effective
manner. However, such a simulation-driven approach has not been
considered so far.
Furthermore, human-robot-interaction is considered to increase the
productivity of robots and humans sharing the same workspace [10, 11].
The goal hereby is to combine good comprehension, intelligence and
sensing of humans with the power and endurance of robots. Safety, i.e. the
robot will not hurt the human under any circumstances, is of utmost
priority. To avoid injury, research focuses on sensor-based solutions and
safe controllers [12] by reducing force and speed limits in order to lower the
kinetic energy of the robot and the resulting injury potential. If there is an

interaction between humans and robots, often soft robots are designed to
reduce hazard by damping collisions due to their soft kinematic structure
[13, 14]. Independent of the robot kinematics and its stiffness, human-
robot-interaction also includes communication mechanisms between robots
and humans. Sharing information and giving instructions is possible via
various interfaces and interaction methods, e.g. speech, gesture and haptic
[15, 16] instead of keyboard, mouse or graphical user interface (GUI).
Another design concept for robotic devices interacting with soft
materials leads to soft robots, e.g. as investigated within the EU-projects
"RoboSoft" and "STIFF-FLOP". In these projects, robots can change their
stiffness when needed. Many of the investigated soft robots in [12] and [17]
are based on bio-inspired structures. In general, research on soft robots
gained significant momentum in many areas of research in the last few
years, cf. [18]. The main difference between the research area of soft robots
and the interaction between robots with soft materials as discussed within
this paper is that in the area of soft robot research, parts of robots are
deformable. While rigid robots are commonly used in industrial
applications, key drivers within the field of soft robots are biomedical
applications, such as surgical tools (snakes) [19]. Nevertheless, soft robots
were investigated in the context of some industrial related problems, e.g. for
designing reliable soft grippers [20], or using them for exploration, rescuing
and inspection [21-23].
The diversity of all of these approaches shows the complexity and
interdependence of the underlying topics. However, a comprehensive
problem formulation summarizing the individual challenges associated with
each field and joining them to an overall description of underlying
difficulties especially in between these fields of research has not been given
yet.
3 Case Study Based on a Teleoperated Robot
Painting on Soft Material
3.1 Description of the Setup
The setup consists of a 6-axis robot, which has a flexible pen with
integrated flex sensors as its end effector. A balloon is fixed to a fixture in
front of the robot with a load cell for z-axis force. For the human-machine

interface, a haptic device is used. The control for all components of the
setup is implemented on several PCs or as microcontrollers respectively.
Fig. 2 shows the setup for the case study schematically.
Fig. 2 Schematic depiction of the setup of the practical case study
The robot used here is a Universal Robots UR5 with control
implementation as described in [24]. The pen contacting the balloon is
flexible and manufactured using silicone material around the pen lead. For
measurement of the x- and y-forces on the balloon and on the pen, four flex
sensors are fixed to the pen and connected and evaluated using an ESP32
microcontroller. A load cell is used for the normal force measurement. It is
included via another ESP32. The human-machine interface is realized using
a haptic device (Force Dimension 'omega.6') with 6D input and 3D force
feedback for x, y and z. The main controller and lower level robot controller
are implemented on separate PCs. Communication between these
controllers and with the wirelessly connected ESP32 sensor modules is
realized using the MQTT protocol. Fig. 3 shows two pictures from the setup
of the described case study. Further supplemental material showing the
setup is available for reference [25].

Fig. 3 Pictures of the described case study. a Overview of the setup during teleoperated drawing; b
close-up view of the end effector of the case study's setup during a drawing process of the letters
"IRTG"
3.2 Central Challenges of the Case Study
To point out the difficulties of the case study with regards to robotic
manipulation the central challenges of the use case are summarized in the
following list. Hereby it is highlighted which key aspects of soft object
manipulation are regarded in this case study and which key aspects could
not be investigated with the described setup.
Highly nonlinear and sensitive system behavior: The central challenge of
the task is introduced by the balloon that is contacted with a rigid tip of
the pen. The deformation behavior of the balloon around the contact
point is highly nonlinear and further yields a critical force limit where the
balloon may burst under too high contact forces in the contact point.
Passive compliant manipulator: Another challenge is introduced by the
flexible pen that is included as a compliant element into the system. This
compliant element allows to cope with the hard constraints that the
balloon's sensitive surface imposes on the contact forces even when
lacking a real-time feedback loop (human operator and time delay for the
remote control). But it also introduces challenges for sensing and
controlling due to the deformation the manipulator undergoes during the
drawing process.
Contact force-sensing: Measuring the three-dimensional contact force
between the pen and the balloon during drawing is challenging because

of the compliant pen that bends under the occurring shearing forces.
While the normal force can be observed easily with a load cell mounted
behind the balloon, the lateral forces cannot be obtained right away. This
is especially challenging from a sensor design point of view, because the
lateral contact forces can only be measured indirectly through the
deflection of the pen. Besides the sensor design, this involves the
problem of calibrating the sensor with respect to the stiffness
characteristic of the compliant manipulator.
Distributed architecture for remote control: The key requirement of
remote controlling the painting task increases the complexity of the setup
significantly and imposes challenges to the system's control architecture.
It presupposes a control setup that allows the individual components of
the system to communicate and exchange data reliably and within
appropriate cycle times. A violation of critical time constraints can lead
to a failure of the system resulting in the robotic manipulator bursting the
balloon.
Parametrization of the human-machine interface: The remote control of
the setup requires to interface a human operator with the system. Hereby,
the provided force feedback and force limits of the robotic manipulator
have to be adapted in order to allow the operator reliable and safe control
of the robot during the drawing process. Providing appropriate interfaces
that enable human operators to interact with soft materials via robotic
devices is hereby a challenging task that requires careful
parameterization.
Servo control: To control the robotic manipulator remotely with a human
operator involves the challenge of servo controlling the robotic
manipulator from an input device. Servo controlling robots is a
challenging topic in itself, which is not further regarded within this
publication. This is because our setup of the painting robot builds on the
framework of [24], such that challenges related to servo control are
encapsulated in their controller and are not apparent in the investigated
case study.
Drawing trajectory: Deriving a trajectory in order to draw a defined
image or pattern on the balloon is a highly challenging topic as well. This
topic is not investigated within the case study, since a human operator
controls the robot remotely and therefore takes care of the path planning
because the superior cognitive abilities of a human facilitate the practical

implementation of the setup. However, this yields the drawback that
problems associated to path planning and trajectory generation cannot be
addressed within this case study.
Based on the described setup, the problems associated to the highlighted
challenges are identified and analyzed in the following section.
4 Problem Analysis and Identification
4.1 Unresolved Problems of Soft Sensor Development
For soft tissue robotics sensing is important for accurate closed-loop control
but it is also especially challenging. To point out the challenge we refer to
the conducted case study where the contact forces between the pen's tip and
the balloon need to be provided to close the feedback loop in order to
enable stable control of the robot's movement. We observed, that the force-
sensing is much more complex when manipulating deformable objects than
when manipulating rigid objects due to the soft object's physical properties
that couple forces with position changes. In our setup, a flexible pen is used
as manipulator in order to decouple the rigid robot from the sensible surface
of the balloon. This relaxes the constraints on the force control by
minimizing the forces exhibited on the balloon and thereby ensures safe
operation even under delay in the robot control. But minimizing the forces
yields a challenge for force-sensing.
When drawing on the balloon, the contact force highly depends on the
soft surface's elastic deformation, which makes the contact force a three-
dimensional force including the normal force and the lateral forces on the
tangent plane of the contact point. Therefore, in this case study, the force-
sensing can be separated into two problems: normal and tangent forces
detection. While measuring the normal force is straightforward, sensing the
tangent forces is not as easy because the shear forces in the contact point
are so small that they cannot be resolved by available force sensors
commonly used in the field of robotics. Instead, we rely on an indirect
measurement of the forces, which is given by the bending of the soft pen
that allows to translate the low force signals into larger deflection signals
which can then be measured much easier with available sensor technology.
This allows to derive a general challenge in the field of sensor
technology in the field of soft object manipulation. Designing and
fabricating task-specific sensors that work under the given circumstances

such as high deformation and low forces and can be interfaced with the
given hard- and software tools used in soft objects manipulation
applications. Even though the design and fabrication processes and
materials changed a lot with respect to the traditional sensors, the basic
working principles are more or less the same. The most commonly working
principle adopted for the soft sensors in recent years is still a conversion of
the deformations or the force information out of quantities such as
resistance, capacity or voltage, which are easily measurable electronic
signals [14]. Moreover, the advent of soft robotics and wearable electronics
has driven the development of compact, integrated and efficient soft
sensors, i.e. stretchable sensors, which require an integration of soft
materials and conventional rigid electronics. However, because of the
difference in the mechanical properties of soft objects and rigid electronics,
the integration of electrical parts into highly stretchable soft materials is a
challenging research task [26]. Novel fabrication methods, such as
embedded 3D printing, lithographic or planar printing, provide an approach
towards this challenge. Yet, all of these approaches are still in an early
research stage, so that there are no commercial solutions available up to
now, which makes the sensor integration for soft object manipulation tasks
a challenge in itself.
One particular problem in this field is the calibration of the sensors.
This is not straightforwardly possible because of the highly compliant and
nonlinear behavior of the involved soft materials [27]. The lack of universal
principles in this field implies the need to calibrate the sensor without any
theoretical guidance before it can be used in an application.
4.2 Unresolved Problems of Soft Material Modelling
Models of soft materials relate physical quantities, such as deformation and
force in order to provide information that is not directly measurable or
accessible. In general, a model of a deformable object can be expressed as a
system of second order nonlinear partial differential equations (PDE)
(1)
Here,  is a three dimensional vector denoting the position of each
individual point in the volume of the deformable object at any time. 
 is

the density distribution of the body, 
 quantifies the damping properties of
the object and 
 is a functional describing the potential energy stored in
the object caused by the external forces 
 acting on the object [28].
Modelling the balloon as a deformable object within the case study
yields several challenges that can be generalized to problems for soft object
modelling in the context of robotic manipulation. First, the balloon has a
very thin but highly stretchable surface, such that a description of the
object's geometry requires nonlinear functions  mapping from the
Euclidean space to the current position of the object. Second, the mass
distribution 
 depends on the current configuration of the object and is
therefore dependent on the nonlinear mapping , while the damping
properties 
 and the potential energy  are given by the constitutive
properties of the material of the balloon, which is rubber and therefore a
hyperplastic material that also yields a highly nonlinear behavior. Third, the
parameters of the model and the model of the underlying constitutive
relations are a priori unknown and cannot be identified because the
identification process is elaborate and time-consuming or requires
equipment which is not available in a manipulation scenario. The
significance of the third problem even increases when the manipulated
object is not only elastically deformable, as it is the case with the balloon in
our case study, but also undergoes plastic deformation.
As a consequence of the mentioned problems, the challenge that
imposes itself in the context of modelling deformable objects for robotic
manipulation lies in the highly nonlinear partial differential equations
determining the object's behavior, which cannot be solved straight forward,
because the required parameters are generally unknown. Even when leaving
the problem of parameter identification aside and assuming a perfectly
parameterized model, solving the resulting highly nonlinear PDE system is
computationally expensive and therefore cannot be done during
manipulation.
A possible solution to this problem is to use reduced order models that
approximate the deformation behavior of the manipulated objects by a finite
set of degrees of freedom. Since such a surrogate model significantly
reduces the required computational resources, the model can be solved

online during manipulation, which allows to incorporate parameter
estimation techniques to identify the initially unknown parameters of the
model during runtime [29].
4.3 Unresolved Problems of Control Architecture
The development of successful systems for manipulation of soft objects
often requires novel sensors and actuators paired with extensive simulations
[30]. As such, the individual components will be developed by experts in
their respective field and changed frequently. A classical control system is
not flexible enough to accommodate this workflow. Our aim is to decouple
individual components as much as possible, such that (small) changes of
one component do not require rework of the system as a whole. In this case
study, we incorporate aspects of previously outlined cloud-based control
architectures [31, 32] to achieve this feat. Components of the system
communicate via the MQTT protocol, which is based on TCP/IP. The
decoupling is present in different aspects.
Spatial: teleoperation implies that the operator and the robot are located
apart from each other. In addition, we allow for every component of the
system to be located anywhere where the network is available.
Temporal: the usage of default TCP/IP protocols and hardware prevents
the realization of hard real-time capability across the whole system. While
individual components can be real-time capable, the TCP/IP communication
between them is not. The components need an interface to communicate
with other real-time and non-real-time components across different cycle
times.
Structural: MQTT allows a basic hierarchical data structure through
message tags, so-called topics. A message topic has a structure similar to a
file path. The messages can be encoded as simple ASCII characters or
employ binary encoding. For ease of use, we chose ASCII encoding
together with the JSON format. The components of the system provide
information about the type of measurement or command value, the SI unit
and the value itself. This minimal data standardization allows for easier
modification of individual sensors or actuators.
As communication is based on TCP/IP, theoretically any network
capable device or system can connect to the system, including cloud-based
systems. This allows to leverage the advantages of cloud-computing for
robotic soft object manipulation: high-level programming frameworks can

be used to model, analyze and control complex systems and sufficient
computational resources are readily available. This can alleviate the
previously identified problem of limited computational time for model
solving. Regarding our case study, we were able to quickly combine
different devices and programming frameworks, namely ESP32
microcontrollers, and portable computers executing the different parts of the
control system in form of high-level programming languages.
The realization of (near) real-time capability, temporal decoupling and
fail-safe methods for communication loss poses the biggest challenges
when trying to implement a control system based on TCP/IP. We directly
experienced the impact of the connection quality on the performance of the
system. Connecting the control system via the university's wireless network
impacted the system's performance drastically, introducing a delay, which
was noticeable by the operator. An approach to solve this problem is to
encapsulate the real-time requiring components of the control into a local
robot control node while deploying the non-real-time components into the
cloud [33, 34].
Please note that the failure of individual components or their
communication was not examined in this case study, but also poses an
important challenge to be addressed in further research.
4.4 Unresolved Problems of Process Control
The control system and the process control are important parts because they
connect all the incoming model and process data to calculate suitable set
points and actions. Therefore, different problems caused by the
interdisciplinarity of all the incoming data were expected. The execution as
well as the programming of the control system have to handle these
interdisciplinary problems. It could not be expected to solve these problems
and to achieve an operable use case for the further problem identification.
The case study therefore integrated a human into the control loop through
the teleoperated concept of the system. Humans can solve such problems
intuitively through experience and learning. The integration of this learned
process control, however, is very difficult. Through the teleoperated
concept, the human substitutes the process control and the control
execution. Through this approach it was possible to set up the case study for
the further problem investigation.

To keep the use case as close to an industrial control loop as possible,
the human only received visual feedback about the robot's position and
haptic feedback from the force sensors via the haptic device. This
corresponds to the same sensor information, that an industrial control
requires to solve a similar use case. The human controlled use case was
particularly helpful for the problem investigation, because of the direct
feedback the user got within the experiments. The following problems were
identified for a control loop controlling the interaction between a rigid
object and a deformable object.
A general problem resulting from the soft material is the identification
and quantification of relevant control variables. The process of drawing on
the soft surface is highly dependent on the object's behavior that is
dominated by its elastic deformation. Different control variables (normal
force, tangential force, velocity, acceleration) are related to effects that lead
to failure of the drawing process. These effects are correlated to the
corresponding control values by the behavior of the elastic material. If the
dependencies cannot be identified, the control development becomes a
high-dimension optimization which is very dependent on the specific use
case. Solving such a problem would need domain knowledge about the
process and a high-resolution behavioral model of the material.
Even for the employed teleoperated setup it was hard to scale the
different feedback values in order to achieve a robust drawing process. For
industrial applications with limited accessibility, like a programming
interface, the quantification of control values is even more complex.
Methods which can automatically optimize the process based on their own
experiences are therefore desired but are generally unknown.
Another problem identified within the use case is the delay of feedback
values, which can turn out critically and can subsequently lead to system
failure. The integration of model-based sensors or a high-resolution model
within the control system can also lead to delays of feedback for the control
system if the model's computation times violate the time constraints given
by the process control. If the delay is caused by the sensors or the control
architecture, an internal reduced order model that provides sufficiently fast
computation to predict the next timesteps can solve these issues. If a
sufficiently accurate model cannot be obtained within the given
computation time due to the high complexity of the underlying equations,

other methods for the state prediction should be investigated. But it remains
difficult to derive a general problem formulation for different use cases.
An approach to the mentioned problems is given by reinforcement
learning (RL). RL is an exploratory and interaction-based learning method
that can find and optimize parameter dependencies or complete process
control solutions [35]. The control system optimizes itself thought own
experiences. This allows to address the delayed feedback and can also help
to find a model to predict future states, because RL is able to handle
delayed reward signals in contrast to common optimization approaches.
4.5 Unresolved Problems of Actuator Design
Typical conventional robots are stiff by design to fulfill their purpose of
accurate positioning, but this limits the functionality in performing tasks on
flexible or soft material. Depending on the task, the robot might be required
to interact compliant with the soft material to ensure safe operation without
damaging or extensively deforming the manipulated object. In the
conducted case study the compliance was ensured by the flexible pen
separating the rigid robot from the deformable balloon. A significant
observation here was that for the drawing process an adjustable stiffness of
the pen would have been highly beneficial.
There are two approaches to achieve such an adaptive compliance with
a robotic system. One way is to implement an appropriate force control. The
other way is to directly integrate soft materials into the actuators of the
robot. For both approaches the bandwidth is an important characteristic to
measure the capability of force transmission and positioning accuracy. The
bandwidth largely depends on the quality and frequency of the sensor
signals as well as the actual torque produced by the rigid motor. Compared
to a soft actuator, rigid ones often have the advantage in torque or force
density and are therefore capable of achieving a higher bandwidth because
of the possibility to amplify the motor torque easily, using a highly reducing
gearbox. The gearbox itself however introduces some problems with
regards to the manipulation of soft objects. One of these problems is that
any torque ripple introduced by the motor is being multiplied by the
gearbox as well. This means that either a torque sensor behind the gear has
to be implemented or the torque has to be estimated with a detailed model
of the drive system, including losses, backlash and effects of hysteresis

introduced by the gearbox, to be able to produce an accurately enough
torque output.
Within the conducted case study, the actuators of the used UR5 robot
were not altered in any way. A speed control, implemented in [24], was
used. However, for a potential actuator redesign, the mentioned aspects
should be considered. Depending on the actuator topology, the actuators in
the robot joints could serve as sensors, making additional sensors, e.g. the
force sensor to measure contact forces, redundant. This yields the advantage
that fewer sensors lead to an increased flexibility of the system's operation
conditions and lower the risk of system failure due to a reduced number of
components. This could be achieved by multi-motor actuator designs that
allow to combine actuation and sensing such as the concept presented in
[36]. Also, a more precise output torque from such a multi-motor actuator
might allow different superimposed control strategies, which compensate
the system's non-linearities. Finally, a different actuator concept might
allow to adapt the robot dynamics towards the different types of soft
materials that the robot interacts with during the manipulation, such as
altering the dynamics of the robotic manipulator.
Specific applications might be best suited with an actuator concept
neither fully rigid nor fully soft. This can be achieved by connecting rigid
and soft elements in series but also by the integration of elastic parts within
a rigid actuator.
5 Linking Problem Formulations from Different
Disciplines
The problem formulations of the subtopics described above are mostly
formulated within the specific subtopic itself. Additionally, it is also
important to consider the correlations of the subtopics which can lead to
further problems. The correlations of soft tissue applications experienced on
the specific case study are shown in Table 1 as Harvey balls.
Table 1 Harvey ball diagram of the identified cross-correlations between the different fields
involved in the case study
Cross-
correlation
Sensor Material
model
Control
architecture
Control
strategy
Actuator
design
Sensor
 

Cross-
correlation
Sensor Material
model
Control
architecture
Control
strategy
Actuator
design
Material model
-
Control
architecture
-
Control strategy
-
Actuator design
-
 
Weak
correlation
Strong
correlation
An important correlation was identified between the sensor and the
material model. The sensor must provide data as input for the model to
identify the constitutional relations. A good soft material model always
needs sensor input in order to remain as close to reality as possible,
especially beyond the prediction horizon. Within the use-case some process
variables were hard to measure directly with sensors. Models can help to
transfer a measured indirect value into the needed process variable. Control
architecture and the process control itself have a very strong correlation,
especially in this case study where an unconventional approach has been
taken. Solutions to soft tissue control problems are often model-based
control approaches. The quality of the solution therefore depends strongly
on the quality of the models. Model accuracy, prediction horizon and
computation time of the model are crucial for the control system.
The computational limits can also be addressed through a specific
architecture. Therefore, the control architecture also yields a stronger
correlation with the process control and material model. Depending on the
type of actuator being used, there is a strong correlation of the actuator
design and the process control as well as the architecture, since the
actuators define the necessary number of control layers and the complexity
of the actuator control loop. Also, the motor control greatly depends on
precise and fast sensor input.
6 Conclusion
In this paper we investigated challenges in robotic soft tissue manipulation.
The contribution aims at a generalized problem formulation in the field of
soft object manipulation. We reviewed several works that have been

presented in recent years addressing various problems in the field and also
providing specific solutions to these individual problems.
From the multitude of the different approaches we infer the following
shortcoming: a general problem formulation, describing the underlying
challenges in soft tissue manipulation, is lacking. To address this gap, we
investigated the problems occurring during manipulation of a soft object
with a sandbox scenario of a remotely controlled painting robot. Based on
this experimental case study we identified and analyzed underlying
problems of the interaction between rigid robotic devices and soft
deformable objects and provided approaches of how to solve the
investigated challenges. Hereby, the involved disciplines such as sensing,
modelling, controlling and actuator design are discussed with respect to the
problems introduced by the manipulated soft objects and the
interdisciplinary connections between the different areas are correlated to
each other.
In the end, the investigated use case shows that robotic interaction with
soft objects is subject to highly interdisciplinary problems that can only be
solved if the applied approaches take into account all requirements,
assumptions and limitations from all involved fields. Therefore, we
encourage an interdisciplinary understanding of the problems in the field of
soft object manipulation, because considering all involved fields will enable
the development of generally applicable solutions. Further case studies will
be key to develop such a problem formulation providing an
interdisciplinary understanding.
Acknowledgements
The authors would like to thank the Faculty of Engineering and the
Auckland Bioengineering Institute of the University of Auckland for
providing facilities and tools that enabled our interdisciplinary approach.
We would like to thank Gal Gorjup and Minas Liarokapis for providing
their teleoperation interface for our implementation of a remote control.
Funding
The research leading to this publication has received funding
from the German Research Foundation (DFG) as part of the International
Research Training Group "Soft Tissue Robotics" (GRK 2198/1), the Vice-
Chancellor's Strategic Development Fund of the University of Auckland

and the Faculty Research Development Fund of the Auckland
Bioengineering Institute of the University of Auckland.
References
1.
Siciliano, B., & Khatib, O. (Eds.). (2016). Springer handbook of robotics (2nd ed.). Berlin:
Springer.
[zbMATH]
2.
Groover, M. P. (2007). Automation, production systems, and computer-integrated manufacturing
(3rd ed.). Upper Saddle River, NJ: Prentice Hall Press.
3.
Buchholz, D., Futterlieb, M., Winkelbach, S., et al. (2013). Efficient bin-picking and grasp
planning based on depth data. In Proceedings of the IEEE International Conference on Robotics
and Automation (pp. 3245-3250). https://​doi.​org/​10.​1109/​icra.​2013.​6631029.
4.
Vahrenkamp, N., Asfour, T., & Dillmann, R. (2012). Simultaneous grasp and motion planning:
humanoid robot ARMAR-III. IEEE Robotics Automation Magazine, 19(2), 43-57. https://​doi.​
org/​10.​1109/​MRA.​2012.​2192171.
[Crossref]
5.
Dang, H., Allen, P. K. (2012). Learning grasp stability. In 2012 IEEE International Conference
on Robotics and Automation (pp. 2392-2397).
6.
Abele, E., Schützer, K., Bauer, J., et al. (2012). Tool path adaption based on optical measurement
data for milling with industrial robots. Production Engineering, 6, 459-465. https://​doi.​org/​10.​
1007/​s11740-012-0383-9.
[Crossref]
7.
Carloni, R., Visser, L. C., & Stramigioli, S. (2012). Variable stiffness actuators: a port-based
power-flow analysis. IEEE Transactions on Robotics, 28, 1-11. https://​doi.​org/​10.​1109/​TRO.​
2011.​2168709.
[Crossref]
8.
Davis, S., Casson, J. W., Moreno Masey, R. J., et al. (2007). Robot prototyping in the design of
food processing machinery. Industrial Robot: the International Journal of Robotics Research
and Application, 34(2), 135-141. https://​doi.​org/​10.​1108/​0143991071072748​7.
[Crossref]
9.
Li, Z., Li, P., Yang, H., et al. (2013). Stability tests of two-finger tomato grasping for harvesting
robots. Biosystems Engineering, 116, 163-170. https://​doi.​org/​10.​1016/​j.​biosystemseng.​2013.​07.​
017.
[Crossref]
10.
Albu-Schäffer, A., Eiberger, O., Grebenstein, M., et al. (2008). Soft robotics. IEEE Robotics
Automation Magazine, 15(3), 20-30.
[Crossref]

11.
Iida, F., & Laschi, C. (2011). Soft robotics: challenges and perspectives. Procedia Computer
Science, 7, 99-102. https://​doi.​org/​10.​1016/​j.​procs.​2011.​12.​030.
[Crossref]
12.
Krüger, J., Katschinski, V., Surdilovic, D., et al. (2010). Flexible assembly systems through
workplace-sharing and time-sharing human-machine cooperation (PISA). In ISR 2010 (41st
International Symposium on Robotics) and ROBOTIK 2010 (6th German Conference on
Robotics) (pp. 1-5).
13.
Deepak, T., Rahn, C. D., Kier, W. M., et al. (2008). Soft robotics: Biological inspiration, state of
the art, and future research. Applied Bionics and Biomechanics, 5(3), 99-117. https://​doi.​org/​10.​
1080/​1176232080255786​5.
[Crossref]
14.
Lee, C., Kim, M., Kim, Y. J., et al. (2017). Soft robot review. International Journal of Control,
Automation and Systems, 15(1), 3-15.
[Crossref]
15.
Buss, M., Carton, D., Gonsior, B., et al. (2011) Towards proactive human-robot interaction in
human environments. In 2011 2nd International Conference on Cognitive Infocommunications
(CogInfoCom) (pp. 1-6).
16.
Dillmann, R., Vernon, D., Nakamura, Y., et al. (2013). Human centered robot systems: cognition,
interaction, technology. Cognitive Systems Monographs (vol. 6). Berlin, Germany: Springer.
17.
Kim, S., Laschi, C., & Trimmer, B. (2013). Soft robotics: a bioinspired evolution in robotics.
Trends in Biotechnology, 31(5), 287-294. https://​doi.​org/​10.​1016/​j.​tibtech.​2013.​03.​002.
[Crossref]
18.
Albu-Schäffer, A., Brock, O., Raatz, A., et al. (Eds.). (2015). Soft robotics: Transferring theory
to application. Berlin, Germany: Springer.
19.
Webster, R. J., Okamura, A. M., & Cowan, N. J. (2006). Toward active cannulas: miniature
snake-like surgical robots. In 2006 IEEE/RSJ International Conference on Intelligent Robots and
Systems (pp. 2857-2863).
20.
Deimel, R., & Brock, O. (2016). A novel type of compliant and underactuated robotic hand for
dexterous grasping. The International Journal of Robotics Research, 35(1-3), 161-185. https://​
doi.​org/​10.​1177/​0278364915592961​.
[Crossref]
21.
Murphy, R. R., Tadokoro, S., Nardi, D., et al. (2008). Search and Rescue Robotics. In B.
Siciliano & O. Khatib (Eds.), Springer handbook of robotics (pp. 1151-1173). Berlin: Springer.
[Crossref]
22.
Nishi, A. (1996). Development of wall-climbing robots. Computers & Electrical Engineering,
22(2), 123-149. https://​doi.​org/​10.​1016/​0045-7906(95)00034-8.
[MathSciNet][Crossref]
23.
Stokes, A. A., Shepherd, R. F., Morin, S. A., et al. (2014). A hybrid combining hard and soft
robots. Soft Robotics, 1(1), 70-74. https://​doi.​org/​10.​1089/​soro.​2013.​0002.

[Crossref]
24.
Gorjup, G., Dwivedi, A., Elangovan, N., et al. (2019). An intuitive, affordances oriented
telemanipulation framework for a dual robot arm hand system: on the execution of bimanual
tasks. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).
25.
Wnuk, M., Jaensch, F., Tomzik, D., et al. (2019). Supplemental material for case study about
challenges in soft tissue manipulation. https://​doi.​org/​10.​18419/​darus-482. Accessed 06 Nov
2019.
26.
Muth, J. T., Vogt, D. M., Truby, R. L., et al. (2014). Embedded 3D printing of strain sensors
within highly stretchable elastomers. Advanced Materials, 26(36), 6307-6312.
[Crossref]
27.
Rus, D., & Tolley, M. T. (2015). Design, fabrication and control of soft robots. Nature,
521(7553), 467.
[Crossref]
28.
Terzopoulos, D., Platt, J., & Barr, A. et al. (1987). Elastically deformable models. In
SIGGRAPH'87 Proceedings of the 14th annual conference on Computer graphics and
interactive techniques (pp. 205-214). https://​doi.​org/​10.​1145/​37401.​37427.
29.
Wnuk, M., Pott, A., Xu, W., et al. (2017). Concept for a simulation-based approach towards
automated handling of deformable objects—a bin picking scenario. In 2017 24th International
Conference on Mechatronics and Machine Vision in Practice (M2VIP) (pp. 1-6).
30.
Tomzik, D. A., & Xu, X. W. (2017). Requirements for a cloud-based control system interacting
with soft bodies. In 2017 24th International Conference on Mechatronics and Machine Vision in
Practice (M2VIP) (pp. 1-5).
31.
Hinze, C., Tomzik, D. A., Lechler, A., et al. (2019). Control architecture for industrial robotics
based on container virtualization. Tagungsband des 4. Kongresses Montage Handhabung
Industrieroboter (pp. 64-73). Berlin, Germany: Springer.
32.
Tomzik, D. A., & Xu, X. W. (2018). Architecture of a cloud-based control system decentralised
at field level. In 2018 IEEE 14th International Conference on Automation Science and
Engineering (CASE) (pp. 353-358).
33.
Hinze, C., Xu, W., Lechler, A., et al. (2017). A cloud-based control architecture design for the
interaction of industrial robots with soft objects. In 2017 24th International Conference on
Mechatronics and Machine Vision in Practice (M2VIP) (pp. 1-6).
34.
Hinze, C., Tasci. T., Lechler, A., et al. (2018). Towards real-time capable simulations with a
containerized simulation environment. In 2018 25th International Conference on Mechatronics
and Machine Vision in Practice (M2VIP) (pp. 1-6).
35.
Jaensch, F., Csiszar, A., Kienzlen, A., et al. (2018). Reinforcement learning of material flow
control logic using hardware-in-the-loop simulation. In 2018 First International Conference on
Artificial Intelligence for Industries (AI4I) (pp.77-80).
36.
Terfurth, J., & Parspour, N. (2019). Integrated planetary gear joint actuator concept for wearable
and industrial robotic applications. In 2019 Wearable Robotics Association Conference

(WearRAcon) (pp. 28-33).
OceanofPDF.com

(1)
 
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_21
Modeling of Lens Based on Dielectric
Elastomers Coupling with Hydrogel
Electrodes
Hui Zhang1    and Zhisheng Zhang1  
School of Mechanical Engineering, Southeast University, Nanjing,
China
 
Hui Zhang
Email: 103200003@seu.edu.cn
Zhisheng Zhang (Corresponding author)
Email: oldbc@seu.edu.cn
Keywords Transparent electrode - Stretchable hydrogel - Dielectric
elastomer - Lens
1 Introduction
Adjusting the focal length using a single tunable lens has been intensively
developed. The main tuning mechanism for a single lens is realized through
shape changing. Hard lenses have a fixed focal length and need motorized
translation for auto-focus. In contrast, a crystalline lens in a human eye has
a variable focal length under stimulation by ciliary muscles [1], and this
inspired the development of deformable lens which consists of membrane
enclosure of liquids [2].
DEAs have been integrated with the liquid lens, and they can tune the
lens just like ciliary muscles doing to the eye's lens. Wide tuning range is
attributed to large actuation strain with highly pre-stretch and high Maxwell

stress in DEAs. Lens systems can be used in portable or biomedical devices
such as mobile phones, endoscopes and digital cameras [3].
This paper is novel in that computing models are employed for soft-
actuated lenses. In Sect. 2, the fabrication and analysis are detailed, and
Sect. 3 represents the viscoelastic behavior in lens under cyclic voltage
signals. Finally, the conclusion is drawn in Sect. 4.
2 Fabrication and Theoretical Analysis
Here we synthesize a kind of hydrogel containing lithium chloride which
has high solubility in water and hygroscopic property [4]. Then we use the
hydrogel as soft conductor combining with the dielectric elastomer of VHB
4910 to fabricate lens (Fig. 1). To limit the evaporation of water from the
hydrogels, we shall cover them with multi-layer VHB films.
Fig. 1 Tunable liquid lens based on DEA. a The front view at reference state. b The side view at
actuated state
Figure 1 demonstrates our proposed tunable liquid lenses with a DEA
actuator. The actuator separates two hydraulic chambers which are filled
completely with ionic water or oil. Activation causes the DEA to bulge
away from or towards the hydraulic chamber that is connected to the lens
enclosure. In this way, the internal pressure inside is changed to cause the
shape change of the lens enclosure. As the hydraulic chamber has a bigger

volume compared to the lens enclosure, small deformation in the electro-
active membrane will cause a larger deformation in the lens membrane.
This hydraulic amplification could enable greater curvature change.
The focal length f of a thick lens is described as the follow equation. For
converging lens, f is positive, R1 > 0, R2 < 0,
, where d is the thickness of a lens [5]. The
refractive index n = 1.476.
u is the distances from the object to the lens, and v is from the lens to
the image. When u > f, v > f, the real image is upside-down. There are three
conditions: ① f < u < 2f, v > 2f, the images are magnified. ② u > 2f, f < v < 
2f, the images shrink. ③ u = 2f and v = 2f, the images are as big as objects.
R1 and R2 are the radius of the both buckling surfaces separately, R1 is
closer to the light source, and R2 is farther. In Table 1, R1 equals R2 with
different values. And d is 1.5 cm as a constant. The focal length increases as
the curvature of the soft-actuated lens becomes larger. In Table 2, R1 and R2
equal 2 cm. When d increases, and the focal length has a little change.
Table 1 Same thickness d resulting in different focal lengths f
Varies
Value (cm)
R1
0.5
0.8
2.1
4
6.4
R2
0.5
0.8
2.1
4
6.4
d
1.5
1.5
1.5
1.5
1.5
f
1
1.2
2.5
4.5
7
u > f, v > f, the real image is upside-down
Table 2 Different thicknesses d corresponding to focal lengths f
Varies
Value (cm)
R1
2
2
2
2
2
2
R2
2
2
2
2
2
2
d
0.5
1
2
3
4
5
f
2.2
2.3
2.5
2.8
3.1
3.5
u > f, v > f, the real image is upside-down

High transparency allows DEAs to transmit electrical signals without
impeding optical signals. Figure 2 shows the setup and tunable focus on the
objects. The focus distance is from 5 to 60 cm by theoretically. A greater
lens deformation could reach to larger scale focus.
Fig. 2 Demonstration of optical tuning using the DEA
We adopt the model of ideal dielectric elastomers, and the stretchable
materials are taken as incompressible. The energy of the active region is
attributed to the stretching of the dielectrics [6]. The areal strain is defined
as σareal = (A − Ap)/Ap × 100%, where A is the area of the active region
covered by the hydrogel for various voltages, and Ap is the area of this
region after the pre-stretch. In Fig. 3, the equations exist:
, and 
.

Fig. 3 Areal strain as a function of voltage for soft-actuated lens using hydrogel electrodes
Figure 3 represents the predicted voltage-strain curve with the hydrogel
of thickness t = 0.5 mm. The lens will be stretched radially to two times its
original radius, λp = 2. The hydrogel also contributes to the total volume of
the active region. The parameters used in our simulation are μ = 10 kPa and
Jlim = 125. The relative permittivity of DE is set to be ɛ = 4.159 × 
10−11 F/m. The model accounts for homogeneous deformation of the active
region. The strain-voltage curve of the actuator using the hydrogel
electrodes is similar to the one with carbon grease [7].
3 Discussion
Large actuated strain is attainable under the condition of large prestretched
dielectrics and thin hydrogels. The rate of the hydrogel evaporation can be
reduced by encapsulation in the condition of cyclic loading.
In Fig. 4, the hydrogel and the VHB are subject to tensile load and
unload. The displacement is described as 
. The
parameters are set as Ref. [8]. The shear modulus μα = 16 kPa and μβ = 
60 kPa, Jlim = 150 and η = 0.2. The curvature of the lens system is R = 2 cm.
The thickness of the lens membrane is 1 mm. The pre-stretch λp of the
dielectrics is imposed to be 1.5.

Fig. 4 a The simulation results of the soft lens subjected to three cycles of voltage pattern. b
Displacement as a function of voltage for the three cycles
According to the Gent model, the Helmholtz free energy density of the
lens is given by the sum of stretching energy density with viscoelasticity as
[9]
(1)
There is the relation given by 
, then,
(2)
(3)

A significant hysteresis is observed during cyclic loading and it
becomes repeatable in Fig. 4. The peak of the displacement is shifted from
the peak of the voltage signal and occurs after it. To account for these
viscoelastic phenomena, a constitutive model is developed by employing
several dissipative non-equilibrium mechanisms as Eq. (4).
(4)
4 Conclusion
In conclusion, we demonstrated the use of DEA with hydrogel electrodes to
make optical lens system. The transparent electrode features are for both
considerable stretch and high conductivity. The internal pressure inside the
lens enclosure is changed to cause the shape change. Small deformation of
the electro-active membrane will cause large deformation in the liquid lens
enclosure. When the thickness d of a lens keeps a constant, the focal length
increases as the curvature of the soft-actuated lens becomes larger. When d
increases, the focal length has a little change.
The strain-voltage curve of the actuator using the hydrogel electrodes is
similar to the one with carbon grease. Large actuating strain is attainable
under the condition of large prestretched dielectric and thin hydrogel. The
rate of evaporation of the hydrogel can be reduced by encapsulation in the
condition of cyclic loading. Future work will do more experiments to verify
the theoretical analysis and further reduce the actuating voltage for the lens
system.
References
1. Keong, G. K., La, T. G., Shiau, L. L., et al. (2014). Challenges of using dielectric elastomer
actuators to tune liquid lens. In: Electroactive Polymer Actuators and Devices (EAPAD) 2014
International Society for Optics and Photonics.
2.
Shian, S., Diebold, R. M., & Clarke, D. R. (2013). Tunable lenses using transparent dielectric
elastomer actuators. Optics Express, 21, 8669-8676.
[Crossref]

3.
Choi, D. S., Jeong, J., Shin, E. J., et al. (2017). Focus-tunable double convex lens based on non-
ionic electroactive gel. Optics Express, 25, 20133.
[Crossref]
4.
Chen, B., Bai, Y., Xiang, F., et al. (2014). Stretchable and transparent hydrogels as soft conductors
for dielectric elastomer actuators. Journal of Polymer Science Part B: Polymer Physics, 52, 1055-
1060.
[Crossref]
5.
Zhang, H., Dai, M., & Zhang, Z. S. (2019). The analysis of transparent dielectric elastomer
actuators for lens. Optik, 178, 841-845.
[Crossref]
6.
Sun, J. Y., Zhao, X., Illeperuma, W. R., et al. (2012). Highly stretchable and tough hydrogels.
Nature, 489, 133-136.
[Crossref]
7.
Brochu, P., & Qibing, P. (2010). Advances in dielectric elastomers for actuators and artificial
muscles. Macromolecular Rapid Communications, 31, 10-36.
[Crossref]
8.
Zhang, H., Dai, M., & Zhang, Z. S. (2019). Application of viscoelasticity to nonlinear analyses of
circular and spherical dielectric elastomers. AIP Advances, 9, 045010-1—045010-5.
9.
Gu, G. Y., Gupta, U., Zhu, J., et al. (2017). Modeling of viscoelastic electromechanical behavior in
a soft dielectric elastomer actuator. IEEE Transactions on Robotics, 1-8.
OceanofPDF.com

Industrial Processes and Products
The part starts with a practical process for detecting breakage of the yarn in
the spinning of Spandex.
The next chapter has nothing to do with the dairy product, but concerns
identification by machine vision of the spool on which the yarn is wound.
The end of this cylinder is coded with the information that describes the
colour of yarn.
The final chapter relates to the assembly-line feeding of parts for the
construction of spray equipment. The components in question are of an
awkward shape and call for an ingenious design to attain the required speed.
OceanofPDF.com

(1)
 
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_22
A General Monitoring Method for the
State of Spandex Based on Fuzzy
Evaluation and Its Application
Limiao Gu1   , Yan Wen1, Yu Zhang1, Weijie Chu1, Yunde Shi1 and
Fang Jia1  
Department of Mechanical Engineering, Southeast University,
211189 Nanjing, China
 
Limiao Gu
Email: glmfzu@foxmail.com
Fang Jia (Corresponding author)
Email: jfang@seu.edu.cn
Keywords Spandex - Core spun yarn - Fuzzy evaluation - Monitoring
algorithm
1 Introduction
The demand of core spun yarn is rising in the modern textile industry. As
the important raw material of core spun yarn, the spandex is prone to break
due to the improper pre-draft times and the wear of the godet rolls resulting
that the defects of core spun yarn increase. Timely detection and treatment
of broken spandex are the important procedures to ensure the quality of
core spun yarn. At present, the manual inspection adopted by most spinning
enterprises not only fails to find the problem in time, but also increases the
labor intensity.

A myriad of scholars and enterprises have studied different monitoring
ways of spandex and core spun yarn in the spinning process. Gorbunov et
al. [1] proposed an automated control system based on machine vision to
identify fabrics parameters to find defects. Kim et al. [2] proposed a method
to monitor yarn wrapping quality to identify hollow yarns. The
QUANTUM3 system of Uster company was able to monitor the defects in
the production of core spun yarn, and the defective segments are separated
manually. Ling-yun et al. [3] developed a sensor to identify the core spun
yarn breakage based on the tunnel magnetoresistance effect. Wang Senxiao
et al. [4] proposed a dual-coil differential sensor and detection device based
on the principle of electromagnetic induction to judge the state of yarn.
Catarino et al. [5] developed a sensor to measure the input tension of yarn,
and the state of the core yarn was judged by analyzing the tension.
Most researches pay attention to the direct detection of the state of core
spun yarn at present, but the research schemes on the monitoring of spandex
have been rarely reported. Meanwhile, the existing detection algorithms are
complex and require high hardware cost [6, 7]. Aiming at the above
problems, this paper proposes a general monitoring method for the state of
spandex based on fuzzy evaluation to real-time monitor and timely solve
the problems of spandex.
2 A Low-Cost Monitoring System for the State of
Spandex
The core spun yarn is produced by twisting the roving in a certain direction
with spandex and wrapping on the textile bobbin. Figure 1 shows the
spinning process of core spun yarn, including roving 1, spandex 2, the guide
hook 3, the godet rolls 4, twisting end of spinning frame 5, core spun yarn 6
and the textile bobbin 7. As shown in Fig. 2, the godet roll is composed of
N40 NdFeB magnet 1, white painted wheel 2 and threaded shaft 3. The
spandex is located in the upper part of the spinning frame about 50 cm
away from the godet roll [8, 9]. Nevertheless, the spandex is liable to shake
in a certain range because of its small diameter and long drawing distance.
Therefore, to directly monitor the state of the spandex is very challenging
for hardware [10].

Fig. 1 The spinning process of core spun yarn
Fig. 2 The composition of the godet roll
The godet roll is driven to rotate because of the spandex under normal
conditions. If the spandex breaks, the godet roll will stop running due to
friction. Hence, this system judges whether the spandex breaks by detecting
the rotation of the godet roll, so as to lessen the hardware requirement and
complexity of the algorithms.
The monitoring system consists of sensors, the signal preprocessing
module, the threshold comparison module, the pulse detection module, the
MCU and actuators as shown in Fig. 3. The upper waveforms represent the
output signal of modules and the action of the system under normal rotation
of the godet rolls. When the godet rolls are abnormal, the output signal and
the behavior of the system are shown below. The communication and power
supply of the system are realized with chain structure which the circuit
boards are connected one by one to reduce the length of the wires and cost.

Fig. 3 The system composition and behavior
3 The Design of Monitoring and Communication
Algorithms
3.1 An Intermittent Monitoring Algorithm Based on Fuzzy
Evaluation
The monitoring algorithm for spandex needs to meet the high real-time
requirements and the computation efficiency. This algorithm is based on
fuzzy evaluation and intermittent inquiry to balance efficiency and time
with strong robustness. As shown in Fig. 4, the hierarchical structure model
in view of the spinning technology is established, which includes rational
speed, historical detection data and other positions' status. Since the output
signal generated by the pulse detection module lasts for 0.4 s after receiving
the single pulse, the low rotational speed caused by the wear of the godet
roll will generate pulse signals at the pulse detection module. Therefore, the
rotational speed layer includes the previous speed and the current speed.
The historical monitoring data layer includes spandex breakage, the wear of
the godet roll and the damage of the godet roll.

Fig. 4 The hierarchical structure model for the state evaluation of the godet roll
The evaluation set of the state of the godet roll is V, which includes
spandex breakage V1, the wear of the godet rolls V2 and the damage of the
godet rolls V3. According to the hardware design of the monitoring system
and the opinions of the technological engineers, the membership degree of
the evaluation object to the evaluation set is determined from every single
factor. The evaluation matrix for the ration speed is S = [S1 S2]T, and the
evaluation matrix for historical detection data is H = [H1 H2 H3]T. The
elements of each matrix are the membership degree of a certain evaluation
object in a certain evaluation factor. Finally, the initial fuzzy relation matrix
B = [S H P]T is introduced. The monitoring algorithm will query 8 godet
rolls at an interval of 1 s and revise the fuzzy relation matrix. The weight
matrix A is constructed by the analytic hierarchy process based on the scale
theory and processed according to the formula 1. The factor weight of the
solution layer is determined as W. Similarly, the factor weights of the
criteria layers can be obtained as W1, W2 and W3

(1)
The 
 means the elements of matrix A. On this basis, the criterion
layer is judged:
(2)
The final comprehensive evaluation results are obtained, and the
membership degree Re of matrix E is calculated according to the principle
of maximum membership degree:
(3)
The actual state of the godet roll is distinguished to judge the state of
spandex. In addition, the permissible detection marks are introduced into
the algorithm and the marks of unproductive positions are closed. If it is
judged as spandex breakage, the actuator will be triggered and the historical
monitoring data will be updated. Before an actuator executes, the detection
marks of other circuit boards are temporarily turned off by the close
command to avoid multiple actuators working at the same time. Another
command will be sent to enable other boards to continue detecting after the
actuator has done. If it is judged as the wear of the godet roll, the system
will provide early warning but not cut roving. If the godet roll is identified
as damaged, the actuator will run and give an alarm. The flow of the
intermittent monitoring algorithm is shown in Fig. 5.

Fig. 5 Flow chart of intermittent general monitoring algorithm
The godet rolls may be accidentally touched by the operators during the
process of reconnecting the spandex, which leads to the misjudgment that
the spandex breaks again. By analyzing the joint process, it is known that
the rotation duration due to the unintentional touching does not exceed 2 s.
Hence, the electromagnet working mark is added to the monitoring
algorithm. As shown in Fig. 6, after the electromagnet executed, the
permissible working mark is temporarily closed. In this situation, the
actuator will not be triggered even if the operators mistakenly touch the
godet roll. After the joint is completed, the godet roll begins to rotate again.

If the rotation lasts longer than 6 s, the working mark will be turned on. At
this moment, the actuator will be triggered if the spandex breaks.
Fig. 6 Flow chart of mistake recognition algorithm
3.2 A Process Identification Algorithm
The speed of the godet rolls will have an obvious change with the process
alternation. The process identification algorithm is based on the change rate
of speed to identify the current process by variance evaluation. According
to the technology and monitoring system, the following conclusions can be
drawn:
The time of all godet rolls will not exceed 3 s from rotating to stopping in
the doffing process.
The speed of the godet rolls is low in the initial spinning stage and the
pulse detection module will generate low-frequency pulses.

The speed of the godet rolls begins to rise such that the pulse frequency
of the pulse detection module will increase in middle spinning stage.
During the high-speed spinning stage, the pulse detection module keeps
high level.
Based on these points, if all 8 godet rolls are in the stall identified by the
monitoring algorithm, the current process is judged as doffing and the
actuator will not work. If it does not belong to the doffing, the average
speed of all position will be calculated. The change rate of speed is obtained
with the average speed of the previous moment. The process array A is
constituted of the rational speed change rate in a few seconds. It can be
concluded that the current process has changed when the array data change
dramatically. The reference arrays of each process are generated according
to the slope of pre-measured speed of the initial spinning, middle spinning,
high-speed spinning and doffing processes. For example, the rational speed
of doffing shows a downward trend and the absolute value of slope will
increase gradually. Formula 4 is used to calculate the square error between
the array A and reference arrays.
(4)
The 
 is the element of array A and the 
 is the element of reference
arrays of each process. The square error is the similarity between arrays so
that the current process is corresponding to the process reference array with
the minimum square error.
3.3 An Adaptive Communication Algorithm
The monitoring system installed on both sides of the spinning frame have
opposite communication directions. This communication algorithm not only
suits the direction of data transmission, but also improves the efficiency and
reliability through appropriate communication protocol and FSM (Field
Signature Method).
The data will be stored in certain array corresponding to the
communication direction, and then are processed based on FSM function
and communication protocol. The improved FSM function includes sending
direction, array name and length. The sending directions are left and right
instead of usart1 and usart2. According to the direction information in the

type frame, the meaning of left and right corresponding to usart1 and usart2
can be changed in real time, and the parameter does not need to be
modified. Hence, the direction requirement can be easily satisfied. The
framework of the communication protocol is shown as Table 1, including
synchronization header, type frame, length frame, content frame and BCC
check frame.
Table 1 Communication protocol framework
No. Type
Number of
bytes
Instructions
1
Synchronization
header
1
0×7E
2
Type frame
1
Upper 5 bits represent the command type
Third bit is the direction of communication
Lower 2 bits mean the number of length type
3
Length frame
0, 1, 2
Setting delay time and electromagnet command have
no length frame
Setting working state and cutting command have 1
byte
IAP command is 2 bytes
4
Content frame
N
N is the number of content frame
5
BCC check frame
1
The XOR value is calculated by first four content
4 Experimental Verification
4.1 Experimental Platform and Conditions
The experiment of given monitoring method for spandex includes 264
monitoring circuit boards based on the chain structure and four spinning
frames. The experimental environment is shown in Fig. 7 including roving
1 and sensors 3. The spinning frame 2 and the right frame are equipped with
monitoring system and monitored by them. The spinning frame 4 and the
left frame are monitored with manual inspection by 4 operators. When the
number of work positions is large enough, the probability of the spandex
breakage in this experiment tends to be about 5-7%. Hence, it can be
assumed that the number of the spandex breakage of four spinning frames is
the same [11, 12].

Fig. 7 Experimental environment
The experimental conditions are as follows:
Spandex specifications include 15D, 20D, 30D, 40D and 70D, and the
pre-draw time is set as 4.
The count of core spun yarn is set as 60, the front roller speed is set as
120 r/min and the diameter of the front roller is 25 mm.
Every spinning frame spins at 512 positions and each specification
spandex is spun 2 round.
4.2 System Running Test
The output signal can be obtained from the threshold comparison module of
the monitoring system as shown in Fig. 8. The output signals were smooth
and stable and conformed to the actual speed. It can be seen that the strong
interference of the spinning workshop can be resisted by system, and a
reliable basis is provided for the monitoring algorithm.

Fig. 8 The output waveform of comparison module
The comparison of the spandex breakage and normal spinning is shown
in Fig. 9. The spandex of middle positions was broken. The roving 1 is
stopped feeding by the white actuators 3 and the LED 2 in the senor end
gave an alarm. Meanwhile, the monitoring data were transmitted to the PC
according to the communication algorithm and the abnormalities were
displayed as shown in Fig. 10. The actuators 3 were not triggered when the
godet rolls were touched in reconnecting process. The spandex of right
positions was in the normal state so that the actuators did not work and
LED 4 was in the extinguished state. When the doffing process was carried,
all the godet rolls stopped running within 2 s. The doffing process was
distinguished by the process identification algorithm and the actuators were
not triggered.

Fig. 9 Comparison of the spandex breakage and normal positions
Fig. 10 The experiment of PC communication on field
4.3 Analysis of Experimental Results
The experiment shows that the proposed monitoring method has higher
accuracy and is suitable for various specifications of spandex. As shown in
Table 2, the monitoring accuracy of most of the larger diameter spandex is
over 98%. The monitoring accuracy is equal to the correct number divided
by the positions' number of spandex breakage and misjudgments. In the
case of low speed of the front roller, the godet rolls are not driven by the

smaller spandex, which is prone to have misjudgments in initial stage. With
the accumulation of monitoring data, the misjudgments gradually decrease
and the accuracy will be over 98%. However, the accuracy of the manual
inspection in 1 round is far less than 80%.
Table 2 Experimental results
No. The specifications of
spandex
Monitoring
accuracy (%)
Average discovery
time (min)
Average roving
length saved (m)
1
15D
97.7
1.8
16.65
2
20D
98.2
1.5
13.82
2
30D
99.2
1.2
10.99
4
40D
99.7
1
9.11
5
70D
99.7
1
9.11
Compared with manual inspection and processing, the monitoring
method stops the roving feeding in time to reduce the waste of raw
materials, and the roving length saved is shown in Table 2. The roving
length used is determined by formula 5:
(5)
The L is the used roving length, and the s is the speed of roller. The d
represents the diameter of roller and t means the discovery time. If the
spandex breaks, it can be recognized by the monitoring system in 2 s so that
the roving length used is about 0.314 m. However, the average discovery
time of operators is shown in Table 2. The spandex of smaller diameter is
prone to break, and the treatment of spandex will affect the inspection of
other positions. Based on this, the average discovery time will increase
gradually. The operators are responsible for a large number of spinning
frames instead of two in reality. Therefore, the average discovery time and
the number of roving saved will be more.
5 Conclusion
In view of the shortcomings of existing detection methods for spandex, this
paper proposes a low-cost monitoring system, which makes the design of
hardware and algorithms more easily. To realize real-time monitoring of
spandex and process identification, an intermittent monitoring algorithm

based on fuzzy evaluation and a process identification algorithm are
proposed with high robustness. Furthermore, the adaptive communication
algorithm suits the change of direction of data transmission and realizes
real-time display of the situation on host computer. Finally, the experiments
show that the accuracy of the monitoring method for spandex is more than
98% and the raw materials are saved more than 9.11 m per station, which is
better than existing methods and applicable to various spandex.
Acknowledgements
The authors greatly acknowledge the grant of Daiyin Group (www.​daiyin.​
com) which supported this research and professors who provided
suggestions.
References
1.
Gorbunov, V., Bobkov, V., Htet, N. W., & Ionov, E. (2018). Automated control system of fabrics
parameters that uses computer vision. In 2018 IEEE Conference of Russian Young Researchers
in Electrical and Electronic Engineering (EIConRus), Moscow (pp. 1728-1730).
2.
Kim, H. J., Kim, J. S., Lim, J. H., & Huh, Y. (2009, November). Detection of wrapping defects
by a machine vision and its application to evaluate the wrapping quality of the ring core spun
yarn. Textile Research Journal, 79(17), 1616-1624.
3.
Ling-yun, X., Dong-fang, Z., & Qing-guang, C. (2017). Design of yarn break detection based on
tunnel magnetoresistance effect. In 2017 16th International Conference on Optical
Communications and Networks (ICOCN), Wuzhen (pp. 1-3).
4.
Wang, S., Wang, J., Shang, L., & Wang, T. (2016). Double coil electromagnetic induction
differential ring ingot break detection system. In 2016 National Metallurgical Automation
Information Network Conference (pp.180-184).
5.
Catarino, A., Rocha, A., & Monteiro, J. (2003). Low cost sensor for the measurement of yarn
input tension on knitting machines. In 2003 IEEE International Symposium on Industrial
Electronics (Cat. No. 03TH8692), Rio de Janeiro, Brazil (Vol. 2, pp. 891-896).
6.
Pinto, J. G., Monteiro, J., Vasconcelos, R., & Soares, F. O. (2002). A new system for direct
measurement of yarn mass with 1 mm accuracy. In 2002 IEEE International Conference on
Industrial Technology, 2002. IEEE ICIT '02, Bankok, Thailand (Vol. 2, pp. 1158-1163).
7.
Shuai, W., Chongqi, M., & Hanming, L. (2010). Yarn quality tracking system based-on RFID. In
2010 International Conference on Computer and Information Application, Tianjin (pp. 103-
105).
8.
Wang, W., & Liu, J. (2018). Spinning breakage detection based on optimized hough transform.
Journal of Textile, 39(04), 36-41.

9.
Carvalho, V., Monteiro, J., Vasconcelos, R. M., & Soares, F. O. (2004). Yarn mass analysis with
1 mm capacitive sensors. In 2004 IEEE International Symposium on Industrial Electronics,
Ajaccio, France (pp. 633-638).
10.
Jun, L. D. (2007). The research of broken filaments detection device on viscose filament yarn. In
2007 International Conference on Computational Intelligence and Security Workshops (CISW
2007), Heilongjiang (pp. 910-913).
11.
Roy, S., Sengupta, A., Maity, R., Sengupta, S. (2013). Yarn parameterization based on image
processing. In 2013 IEEE International Conference on Signal Processing, Computing and
Control (ISPCC), Solan (pp. 1-6).
12.
Shanghai textile holding company. (2006). Cotton textile manual (3rd Edn). China Textile Press.
OceanofPDF.com

(1)
 
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_23
Study on the Type Identification of Cheese
Yarn Based on Low-Resolution Pictures
Xiaolong Liu1   , Ran Hu1, Yan Wen1, Yu Zhang1, Weijie Chu1,
Zhisheng Zhang1 and Fang Jia1  
Department of Mechanical Engineering, Southeast University,
211189 Nanjing, China
 
Xiaolong Liu
Email: liuxiaolong97@foxmail.com
Fang Jia (Corresponding author)
Email: jfang@seu.edu.cn
Keywords Color feature extraction - Color-histograms - Cheese yarn -
Neural network
1 Introduction
With the development of Industry 4.0, the traditional manufacturing
factory's direction has gradually transformed into intelligent factory. As an
important pillar industry, the spinning industry urgently needs to upgrade
production methods. With the support of the artificial intelligence
technology, the use of machine vision and neural networks for automated
operation is the future direction of the spinning industry.
In view of the machine vision-based sorting method, many scholars
have proposed different algorithms for identification. Wang [1] proposed an
improved self-organizing feature mapping network algorithm for image
color feature extraction. P. Sundara Vadivel proposed an efficient image
retrieval system based on color histograms [2], edges and texture features.

After identifying images with color similarity, only texture and edge-based
search of the identified images is required. This process greatly reduces the
time required for the retrieval process by avoiding the fusion process.
Surina Borjigin proposed a color image segmentation algorithm based on
multi-level Tsallis-Havrda-Charvát entropy [3]. Chen Qian proposed a main
color extraction algorithm for color feature extraction of clothing images to
judge the main color of the clothing color according to the H, S channels of
the color histogram [4]. This method can identify the main color of the
clothes, but needs further optimization in the identification of flower colors.
Chen Huiyuan and Liu Zeyu designed a cascaded convolutional neural
network detection framework for the rapid detection of large-scale remote
sensing image ship targets, which improved the speed of ship detection in
remote sensing images, but the accuracy was low [5]. Li Wenyu proposed
an intelligent detection algorithm for color fabric defects based on energy
local binary mode. By analyzing the energy feature images of the color
fabrics [6], they found that the defects are usually irregular, non-uniform
local bright areas, while the background pattern of the color fabric presents
a regular, uniform brighter area in the energy feature image, this method
can detect color defects and tissue structure defects, and the detection
accuracy is 94.9%, which is higher than the accuracy of 89.4% using BP
neural network.
Machine vision is an indispensable part of the process of intelligent and
automated development of manufacturing. The application of machine
vision is of great significance for the classification and identification of
cheese yarn. It is of great significance to realize the classification and
identification of the package yarn by machine vision. The traditional
machine vision-based processing method has a large amount of
computation, a slow processing speed, and high-performance requirements
for hardware devices. This paper combines manual data processing and
machine vision technology, locate and extract the digital information from
the label, then Processing data by BP neural network. This paper proposes a
new method for identifying the type of package yarn [7].
2 Location of Labels Based on Hough Algorithm
2.1 Imagine Preprocessing

Analysis of the image in the gray space can avoid the interference of the
color of the image on the shape feature. Therefore, after acquiring the
image, the grayscale processing is first performed to convert it into a
grayscale image. The formula for grayscale conversion is [8]:
(1)
Among these: G(x, y) is the gray value obtained by transforming each
feature point on the image. r(x, y), g(x, y), b(x, y) represent the color of the
pixel with coordinate (x, y) in the original image.
After grading the picture, use the mean filtering to remove the noise in
the picture, then use the histogram to equalize the image. The gray
information of the comparative set is stretched to the entire gray range. This
method increases the contrast of the image and makes the image look
sharper (Fig. 1).
Fig. 1 a Original image; b grayscale processing; c mean filtering; d histogram equalization; e
binarization
In this experiment, the gray value of the yarn and the label is very
different to the gray value of the hole in the middle of the picture. By
binarizing the image, the hole in the middle of the image can be converted
to black and the other parts to white.

2.2 Hough Algorithm Based on Circular Symmetry
The label to be searched for in this paper is circular. Hough transform is a
commonly used circular search method, but it has a large amount of
calculation and takes up a large storage space, which is not suitable for the
real-time operation in this paper. Therefore, this paper proposes a random
Hough transform based on far symmetry in combination with the actual
situation [9-11].
The distance and angle of the bobbin relative to the camera vary over a
small range, so the size of the label and the angle of view are almost the
same in the captured image. On this basis, the position of the label can be
determined on the contour image according to the symmetry of the circle, as
follows:
First, use a line L1 with a line width of two pixels to pass through the
image. The line will intersect the circle at two points P1 (x1, y1) and P2 (x2,
y2). The center of the circle must be at the vertical line "M" of the
connection of these two points. Since the image itself may have interference
information, this paper uses the line set L = {L1, L2, L3, ..., L10} to
intersect with the picture, and the distance between each line is the same, as
shown in Fig. 2.
Fig. 2 Lines cut circular diagram

In the process of performing the above steps, use a box containing 2 * 2
pixels to slide along the line Ln. When all the four pixels contained in the
box are black, record the coordinates of the point Pn(xn, yn) as shown in
Fig. 3.
Fig. 3 Locating the coordinates of the intersection
Continue to slide the box forward. When white appears in the square,
record the coordinates 
 of the point. By the coordinates of Pn and
, according to Formula 2, the set Cx = {C1, C2, ..., C10} of abscissas of
the center of the circle can be obtained:
(2)
Then get the average of the horizontal axis of the circle:
(3)

If the coordinates of the center of the set Cx = {C1, C2, ..., C10} differ
from the average by more than 10% of the radius of the label, then
recalculate the average of the abscissa of the center without that point.
Then, calculate the ordinate Cy of the center of the circle in the same
way and get the coordinates C(x, y) of the center of the circle. The method
has small calculation amount and small storage space, and is suitable for
applications with high real-time requirements.
2.3 Label Locating Algorithm Flow
The height of the camera remains the same, so the size and shape of the
label in the picture is fixed, so the radius r of the label can be obtained by
statistics. After the label is located by the manner described above, the pixel
points within the range are extracted in the original image and saved in a
new image to obtain a label image of the bobbin.
The overall algorithm flow is as follows:
Step 1: Acquire image information and convert the image to gray space;
Step 2: Use mean filtering to remove image noise and use histogram
equalization to improve picture contrast;
Step 3: Binarize the image;
Step 4: Locating the center of the label using a Hough transform
algorithm based on the symmetry of the circle;
Step 5: Obtain an image within the range of the label in the original
image.
3 Extraction of Neural Network Input
Information
In order to improve production efficiency and ensure the correct rate of
industrial production, the color of the labels in the factory is printed
according to the discrimination habits of the human eye. Currently, in all
color spaces, the HSV color space is expressed in the closest way to the
color-resolving habits of the human eye, and can well express the color
information perceived by the human eye, and is less affected by the
illumination. The H channel represents the hue and the S channel represents
the saturation. Therefore, the color channel selected in the label feature
extraction is the H channel and the S channel of the HSV color space [12].

(r, g, b) represents a pixel in the RGB color space, the value of "r g b"
is a number between 0 and 1, and max is the maximum of the three [13],
and min is the minimum of the three, then the conversion of RGB space to
HSV space The formula is:
(3)
(4)
(5)
Extract the color histogram of the H channel and the S channel, and
quantize the value of the H channel from 0 to 360 to 0 to 10. The
quantization interval used is 36, and the value of the S channel ranges from
0 to 50, and the quantization value is 0 to 5. Obtain the quantized color
histogram, quantize and normalize it, and finally calculate the color ratio.
The obtained label histogram information is shown in Fig. 4.
Fig. 4 Histogram information
Algorithm flow:

Step 1: When the sensor is triggered, the camera takes a picture of the
package yarn;
Step 2: Use the method in Part II to get the image of the label;
Step 3: Preprocessing the image by using medium filtering and bilateral
filtering;
Step 4: Convert the image from the RGB color space to the HSV color
space;
Step 5: Quantize the global color histogram of the H channel and the S
channel to obtain a quantized color histogram.
Step 6: Normalize the quantized color histogram, and the ordinate value
in the figure can represent the proportion of each color.
4 Processing Method Based on Deep Neural
Networks
4.1 Process of Algorithm
Deep neural network (DNN) is a neural network with multiple hidden
layers (two or more layers). The theory has proved that single-layer neural
networks cannot solve the linear indivisible problems, considering that the
data of this project is complicated, DNN was selected to demand the
requirement.
After the feature information of the picture is extracted by the color
histogram, the feature information will processed by the neural networks. In
this paper, the sample information, the detected cheese yarn information
and the flag information are input into the input layer of the neural
networks, training the neural networks to obtain the weights and then
import the weights to the industrial equipment. Compared with the
convolutional neural networks, this method can reduce calculation and
shorten the operation time [14-16].
The process of the cheese yarn classification neural networks mainly
includes data preparation, network establishment, network training and
classification effects evaluation. The training data was obtained by the
simulation experiment platform, the neural network needs to determine the
depth, the number of nodes in each layer and the activation function, to
optimize network performance, the neural network adjusts the weights

constantly during the training process. Finally, the optimal algorithm was
obtained after the classification effect evaluation.
4.2 Preparation of Training Data
The training samples are the data that includes the classification flag, and
the weights of the neural network can be adjusted by training to improve the
classification accuracy. In this experiment, samples are obtained under the
simulation experiment platform. Each group of samples consists of three
parts, which are the characteristic information of the to-be-detected cheese
yarn, the characteristic information of the reference cheese yarn, and the
classification flag. The characteristic information of the to-be-detected
cheese yarn and the characteristic information of the reference cheese yarn
are determined by the H and S values of the image obtained under the
simulation experiment platform in the HSV color space. In the HSV color
space, the values of H, S, V are 0-180, 0-255, 0-255 respectively. The
range of H was divided into 10 intervals to obtain the number of pixels in
each interval, the number of pixels constitutes a 1 × 10 matrix. Similarly,
the value range of V is divided into five intervals, and a 1 × 5 matrix is
obtained. According to the above, a 1 × 30 matrix composed of the
characteristic information of the to-be-detected cheese yarn and the
characteristic information of the reference cheese yarn is obtained. If the to-
be-detected cheese yarn is the same type as the reference cheese yarn, the
value of classification flag is 1, otherwise, the value of classification flag is
0. In conclusion, each training sample is a 1 × 31 matrix.
4.3 Establishment of Neural Networks
Considering that the data of this project is relatively complicated and the
calculation performance of the industrial field is limited, this experiment
chooses the double hidden layer neural network, to reduce the amount of
calculation, the input layer and the hidden layer 1 are not set to be fully
connected. The structure of neural networks is shown in Fig. 5.

Fig. 5 The structure of the neural networks
 are training samples, hidden layer 1 was set 6 nodes,
where 
 are obtained by 
, 
 are obtained by
, weights 
 are 15 × 3 matrix:
(6)
(7)
Hidden layer 2 was set 2 nodes, connected with hidden layer 1 fully, the
weight 
 is a 6 × 2 matrix, 
 is a 2 × 1 matrix:

(8)
According to the 
, y outputs Y between 0 and 1. Y represents
the probability that the to-be-tested cheese yarn and the sample cheese yarn
are the same type, 
 represents the probability that cheese yarns are the
different type.
The combination of loss function and activation function has great
influence on training success rate and classification accuracy of neural
network. The activation function is the nonlinear distortion force in the
neural network structure, the loss function is the comparison between the
neural network output value and the real value, reflect the degree of data fit.
The 2-classification neural network model usually selects the Sigmoid
function as activation function. To eliminate the gradient disappearance, the
cross entropy was selected as loss function. The Sigmoid function, the
Sigmoid derivative function, and the cross entropy function are as follows:
(9)
(10)
(11)
Finally, the neural network was built by the Tensorflow framework.
5 Algorithm Verification
The data acquisition and extraction methods proposed in this paper were
tested, photographing the label position of the cheese yarn in the laboratory
environment, then the information of the pictures was extracted and training
the neural network.

5.1 Label Location and Data Collection
The shapes of label for the data collection include solid color, star shape,
square shape, triangle shape, and diagonal shape, the colors include black,
green, purple, red, and blue. The pictures of different labels are as shown in
Fig. 6.
Fig. 6 The labels of cheese yarn
A total of 1000 labeled images were processed in the experiment, the
labels that center deviation distance less than 5% of the radius (rough
measurement) was accurate. According to the results, the accuracy of label
location was 95.4%. The results are shown in Fig. 7.

Fig. 7 Accuracy of labels location
After locating the coordinates of the label, removing the background of
the images according to the coordinates of the label, the H and S channels
of the remaining part of the image in the HSV color space are extracted to
obtain the quantized color histogram.
5.2 Verification of Neural Network
The experimental platform is a black box structure with OpenMV as the
core, the camera is OV7725 and the processor is STM32H743VI, which is
controlled by Python. The training data was collected on the experimental
platform. A total of 36,524 samples of 30 types of cheese yarn were
collected. After the post-processing and histograms, 26,350 available
samples were obtained. The available samples were divided to 2 groups,
with 20,000 training samples and 6350 test samples. The neural network

was built based on Python 3.6 and Tensorflow 1.12, training on the PC. The
correct rate was detection by the test samples during the training process.
The training was completed while the correct rate above 99.8% stably.
Obtain weight matrixes 
 after the training, import
the weight matrixes into OpenMV, a large number of experiments were
performed on the experimental platform to detect the correct rate. In
summary, the overall correct rate is 99.76%, which beyond the requirements
of the 99.5% in industrial field.
6 Conclusion
In this papers, a method of label location based on symmetry of circle is
proposed, which reduces the computational complexity and can locate the
label in a shorter time. On the basis of locating labels, color information of
labels is picked up, and color histograms of H and S channels are extracted
for quantification and normalization.
References
1.
Wang, Y. (2016). An improved image compression algorithm based on self-organizing feature
mapping. Radio Engineering, 36(12), 18-20.
2.
Sundara Vadivel, P., Yuvaraj, D., & Navaneetha Krishnan, S. (2018). An efficient CBIR system
based on color histogram, edge, and texture features. Concurrency and Computation Practice
and Experience, 57(6), 738-748.
3.
Borjigin, S., & Sahoo, P. K. (2019). Color image segmentation based on multi-level Tsallis-
Havrda-Charvát entropy and 2D histogram using PSO algorithms. Pattern Recognition, 92, 107-
118.
[Crossref]
4.
Lv, M., Gao, T., & Zhang, N. (2019). Main color extraction algorithm and its application to
clothing image retrieval. Journal of South China Normal University, 51, 111-119.
5.
Chen, H., Liu, Z., & Guo, W. (2019). Fast detection of ship targets for large scale remote sensing
image based on a cascade convolutional neural network. Journal of Radars, 8(3), 413-424.
6.
Li, W. (2014). Research on automatic detection for yarn-dyed fabric defect based on machine
vision and image processing. Doctor, Donghua University.
7.
Cheng, Y. Y., Li, H. Y., & Zhang, Y. F. (2011). A new method of denoising mixed noise using
Limited Grayscale Pulsed Couple Neural Network. In Cross Strait Quad-Regional Radio Science

and Wireless Technology Conference (pp. 1410-1413).
8.
Boerner, H. (1989). Feature extraction by grayscale morphological operations—A comparison to
DOG filters. In International Workshop on Industrial Applications of Machine Intelligence and
Vision.
9.
Shi, D. C., Zhang, B., & Wang, N. (2014). Fast circle detection based on improved randomized
Hough transform. In 7th International Symposium on Advanced Optical Manufacturing and
Testing Technologies.
10.
Djekoune, O. (2013). A new modified Hough transform method for circle detection. In 5th
International Joint Conference on Computational Intelligence.
11.
Jiang, L., Tang, P., Zhu, Y., Jiang, J., & Zhang, Y. (2013). Multi-circle detection algorithm based
on symmetry property. International Journal of Applied Mathematics and Statistics, 47(17),
337-345.
12.
Marcu, G., & Abe, S. (1996). New HSL and HSV color spaces and applications. In Imaging
sciences and display technologies.
13.
Morshidi, M. A., Marhaban, M. H., & Jantan, A. (2008). Color segmentation using multi layer
neural network and the HSV color space. In International Conference on Computer and
Communication Engineering.
14.
Zhang, M., Ding, X. Q., & Li, X. (2013). Neural network based color recognition for bobbin
sorting machine. Telkomnika-Indonesian Journal of Electrical Engineering, 11(7), 3728-3735.
15.
Liu, Y. X. (2010). BP neural network classification method under Linex loss function and its
application to face recognition. Journal of Jilin University (Science Edition), 48(3), 411-413.
16.
Uchimura, S., Hamamoto, Y., & Tomita, S. (1995). On the effect of the nonlinearity of the
sigmoid function in artificial neural network classifiers. In International Conference on Neural
Networks Proceedings.
OceanofPDF.com

(1)
 
© Springer Nature Switzerland AG 2021
J. Billingsley, P. Brett (eds.), Mechatronics and Machine Vision in Practice 4
https://doi.org/10.1007/978-3-030-43703-9_24
Research on High Feeding Speed System
of L-Valve Rods Based on Two-in-One
Device
Shiwei Cheng1, Liang Han1   , Kai Yu1 and Rui Peng1
School of Mechanical Engineering, Southeast University, Nanjing,
211189, China
 
Liang Han
Email: melhan@seu.edu.cn
Keywords L-valve rods - Feeding machine - High speed - Two-in-one
device
1 Introduction
With the introduction of Industry 4.0 and China Manufacturing 2025,
industrial automation has become more and more prominent in the
manufacturing industry. At present, many enterprises in China are shifting
from labor-intensive enterprises to high-tech enterprises with high degree of
automation [1]. The key part of the production line is to arrange the
workpieces according to a specific attitude to complete the subsequent
processing of the workpiece. The speed of loading and unloading directly
affects the production efficiency of the entire assembly line.
In this paper, an automatic feeding system is designed for the LVR,
which is a key component of spray equipment. The system is required to
order the disorderly LVR in the correct direction and accurately place it in
the specified position, and improve the efficiency to meet the feed rate

requirements of the line. The process flow chart of the LVR automatic
feeding system device designed in this paper is shown in Fig. 1.
Fig. 1 The process flow chart of the LVR automatic feeding system device
2 LVR Feeding Device Overall Design
2.1 Structural Design Requirements
After analyzing LVR orientation requirements and the installation space of
the system device, the following technical objectives are formulated: (a)
Orient the L valve, move the big endian down and the little endian forward
to the next station. The physical drawing of LVR and the positioning
attitude are shown in Fig. 2. (b) The feeding speed of LVR automatic
feeding device should be greater than 180 pieces/min to meet the speed
requirement. (c) Ensure that the loading system can send the workpiece to
the next station in a stable, correct and uniform speed. At the same time, it

should be ensured that the workpiece will not be damaged or even broken
during the loading process.
Fig. 2 The physical drawing of LVR and the positioning attitude
2.2 Overall Design Scheme
According to the shape of LVR and the requirement of feeding speed, the
electromagnetic bowl vibratory feeder is used to arrange and orient the
workpiece [2]. At present, for the LVR workpieces, the output speed of a
single bowl vibratory feeder is 100 pieces/min, which does not meet the
loading speed requirement of the device. Therefore, the two bowl vibratory
feeders are used to simultaneously sort and orient the workpieces, and the
sorted and oriented workpieces are respectively output from the two curved
fixed sections and the vertical moving sections, and finally the workpieces
are output in turn by the common vertical fixed section, so as to realize the
feeding mode of the two channels in one. Since the number of workpieces
needs to be identified and the two vertical moving sections are moved,
sensor detection devices and pneumatic devices are also required in the
design. Figure 3 shows the position layout and schematic diagram of LVR
automatic feeding mechanism in the whole workpiece processing device.

Fig. 3 The position layout of LVR automatic feeding system device
2.3 The Design of Workpiece Channel
According to the overall design of the system, the whole feeding device is
divided into vibration feeding device, workpiece channel, positioning
device, sensor detection and pneumatic device. The workpiece channel is
composed of two left and right curved fixed sections, two vertical moving
sections and one vertical fixed section. First, the two vibratory feeders sort
the disorderly LVR, and then transport the directional workpieces to the left
and right curved fixed sections and the vertical moving sections. When the
sensor detects that the workpiece in channel is full, the left cylinder pushes
the left moving section to the right, so that the discharge port of the moving
section is aligned with the feeding port of the vertical fixed section. After

the workpieces in channel reach the designated position through the vertical
fixed section, the left cylinder returns, and the right cylinder performs the
corresponding action. Figure 4 shows a partial enlarged view of the
automatic feeding mechanism of the LVR.
Fig. 4 The partial enlarged view of LVR automatic feeding system device
The LVR workpiece channel consists of two left and right curved fixed
sections, two vertical moving sections and one vertical fixed section. The
two curved fixed sections are fixed on the fixed vertical plate by bolts, and
the curved fixed section inlet is fixedly connected with the outlet of the
bowl vibratory feeder for transporting the workpieces oriented by it, and the
outlet of the curved fixed section is aligned with the inlet of the vertical
moving section. The two vertical moving sections are fixed to the vertical
moving plate by bolts, the outlet of which is aligned with the vertical fixed
section.
The channel is designed by the method of self-weight feeding of
workpiece. The method does not require a power device and has a simple
structure. In order to ensure that the workpiece passes smoothly in the

channel without losing the orientation state, the cross-sectional dimension
of the channel must be correctly determined. According to the geometric
relationship of Fig. 5:
Fig. 5 Channel width calculation diagram
(1)
where, 
—channel width (mm), 
—piece length (mm), —the clearance
between the end face of the workpiece and the inner wall of the channel
(mm).
Due to the presence of clearance , it is inevitable for the workpiece to
tilt and rotate in channel, as shown in the dotted line below. A torque
composed of reaction force 
 and moment arm 
 is generated, so that the
workpiece has a tendency to continue to rotate. The larger the value of e,
the larger the angle of rotation. When the workpiece diagonal 
 is close to
or less than the channel width 
, the workpiece is in danger of getting
stuck or losing orientation. Therefore, the size of the clearance should be
able to ensure that the contact with the side wall, its diagonal 
 and the
horizontal angle  is greater than the friction angle . In this case, the
torque composed of the reaction force 
 and the moment arm 
 can

prevent the rotation of the workpiece, so that it can be transported smoothly
in the correct directional state [3].
From Fig. 5:
(2)
that is:
(3)
So the maximum clearance allowed 
 should be calculated
according to limit case 
.
Due to:
(4)
 can be calculated as:
(5)
that is:
(6)
where, 
—sliding friction coefficient between workpiece and side wall,
 = 0.1-0.5 D—workpiece diameter(mm) [4].
Combined with the structural particularity of LVR, the rectangular block
composed of the largest size of LVR workpiece is used for calculation in
design. 
, 
 can be calculated as:

(7)
According to this, the channel can be designed. However, due to the
center of gravity of LVR is at the big endian. In addition, since the length of
the small endian is greater than the length of the large endian, it may occur
that the workpiece rotates clockwise in the channel as shown in Fig. 6.
Fig. 6 The workpiece rotates in the channel

Therefore, it is necessary to design the channel according to the
characteristics of the workpiece, that is, the channel hole is consistent with
the shape of workpiece, which can effectively solve the problem of rotation.
To make the L valve workpiece smoothly from one section into another
section, each outlet and inlet of sections are designed into a horn shape.
Figure 7 shows the design model of vertical fixed section, and 3D printing
is used for machining.
Fig. 7 The design model of vertical fixed section
3 Analysis of System Vibration and Stress
The cylinder of the device pushes the moving vertical plate to stop at the
positioning block, and oil pressure buffer is installed to make hydraulic
buffer to absorb part of energy before contacting the positioning block.
According to the relevant theories of mechanical vibration, the model can
be regarded as a single degree of freedom system with viscous damping, the
force analysis is shown in Fig. 8 [5].

Fig. 8 stress analysis diagram of the system
The viscous damping force 
 is proportional to the velocity 
:
(8)
 represents the displacement of the moving vertical plate, and the
differential equation can be obtained by applying Newton's law:
(9)
where F is cylinder force. To solve (9), assume that the form of the solution
is:
(10)
where 
 and  are undetermined constants, the following characteristic
equation can be obtained:
(11)
The root of the characteristic equation can be solved as follows:
(12)
The two solutions of homogeneous differential equation are:
(13)
The particular solution of inhomogeneous differential equation that we
can solve is:

(14)
So the general solution of the equation of motion is:
(15)
where 
 and 
 are obtained from the initial conditions 
and 
, and the motion will decay exponentially over time.
Under the condition of variable stress, the main failure form of
mechanical parts is fatigue fracture. S-N curve represents the relationship
between stress value and cyclic fatigue life, and represents the relationship
between fatigue strength and fatigue life of materials under certain cyclic
characteristics. Because the designed positioning block is hit periodically
by the moving vertical plate, fatigue analysis is carried out on the
positioning block below. Based on the analysis and comparison of the
references, the following power function formula is used to calculate the
general materials [6]:
(16)
where  and 
 are material constants; take the logarithm of both sides of
the above equation and sort out:
(17)
In the formula,  and  are material constants, 
 is cycle number, 
and  are constants, and 
 is equivalent stress. When 
 is equal to 106,
. According to the fatigue characteristic estimation method of
domestic materials [7], Positioning block material is aluminum alloy, Take
, According to the material property sheet [8], the
tensile strength of aluminum alloy 
. By substituting the
above values into the formula, the following equation can be obtained:

(18)
So 
. Substitute into the above equation to get the
S-N curve of 6061-T6 (Fig. 9).
Fig. 9 S-N curve

4 Establishment and Experimental of Core Part
in the System
At present, some prototypes of LVR automatic feeding device system
designed in this paper are built, that is, the right half of the two-in-one
system is built to carry out the sensor experiment and the part transfer
experiment. The prototype is built as shown in Fig. 10.
Fig. 10 Establishment of the core part of the system
4.1 Sensor Experimental Analysis
The design adopts the feeding method of the workpiece by its own weight.
It is necessary to install a sensor on the vertical curved section to detect the
number of workpieces passing through. When the quantity reaches a certain
value, the cylinder moves with the moving vertical plate. Since the

workpiece is in free falling, and each workpiece is arranged next to each
other, the gap distance that the sensor can detect is the vertical distance
difference between the small ends of two LVR 
, as shown in
Fig. 11.
Fig. 11 Calculation of workpiece gap
According to the free-falling body formula, the response time t of the
sensor can be calculated as follows:
(19)
The design adopts fiber optic sensor for detection. The model is BF-5R-
D1-N digital fiber sensor and FD-320-05 fiber head made by AUTONICS.
This sensor can detect up to 20,000 times per second and has a resolution of
1/10,000. The light source adopts red LED modulated light to be
transmitted to the fiber head through the optical fiber, and the modulated
light interacts with the measured workpiece to change the intensity of the
light. After signal processing, the amount of light received is calculated, and

compared with the set threshold, the workpiece reaches the sensor fiber [9].
Within a certain distance of the head, the value of the amount of reflection
will be greater than the set threshold, indicating that the workpiece reaches
the specified position feedback signal to the controller [10]. Figure 12
shows the difference in the amount of light received by the sensor through
the sensor in different modes.
Fig. 12 Sensor light absorption curve
4.2 Loading Rate Test Experiment
After completing the sensor test experiment, this section tests the loading
rate of LVR, counts the number of detection signals of the sensor, that is,
each time the detected part passes, the count is incremented by one, and the
cylinder actions when the count value reaches the set value. The feeding
experiment sets the target value to 5, 12, 15, and 18 respectively, and each
target value is carried out 8 times. The feeding time (ms) is counted as
shown in Fig. 13.

Fig. 13 Results of feeding rate test
5 Conclusion
This paper designs the overall structure of LVR high feeding speed system,
and analyzes the key components of the system. This includes the
workpiece channel, positioning device and sensor device, and realizes the
feeding of LVR in turn, to achieve the feeding mode of the two channels in
one. This paper introduces the structural design of the system, and finally
carries out the debugging and experiments of the prototype. The
experimental data shows that the system can output the disordered LVR in
the correct direction and accurately position it to the specified position. At
the same time, it can greatly improve the feeding efficiency to meet the
feeding speed requirements of the assembly line.
References
1.
Li, X. (2016). Automatic feeding system design and experimental study on automobile fuel
injection pump plunger (pp. 9-10). Southeast University.

2.
Sun, C. (2017). A study on key technologies of automatic feeding system of magnetic tile (p. 7).
Southeast University.
3.
Han, L. (2011). Electronic precision mechanical design (4th ed., pp. 115-117). Southeast
University Press.
4.
Xu, X. H. (1986). Electronic precision mechanical design (pp. 83-84). National Defense
Industry Press.
5.
Rao, S. S. (2009). Mechanical vibrations (4th ed., pp. 94-99). Tsinghua University Press.
6.
Li, K. (2013). Research on the mechanical components fatigue design method and its application
under impact load (p. 23). Hefei University of Technology.
7.
Wu, K. J., Yu, X. H., & Qian, R. M. (2006). Mechanical design (pp. 67-74). Higher Education
Press.
8.
Zhao, S. B. (1994). Anti-fatigue design (pp. 339-346). China Machine Press.
9.
High performance single/double digital display fiber amplifier. Retrieved from https://​www.​
autonicschina.​cn/​series/​3000437.
10.
Huang, J. R. (2018). A study on the key technology of automatic inlay equipment for popular
ornaments (pp. 35-37). Southeast University.
OceanofPDF.com

Conclusion
Although their topics cover a diversity of fields, these papers show the
application of practical testing to the task of theoretical development.
Mechatronics and its sub-field of robotics will never cease to provide a
wealth of interesting research problems.
OceanofPDF.com


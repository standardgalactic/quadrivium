Matrix 
Analy sis 
and 
ch 
= 
Bals 
S503 
eB S 
< 
<5 
S 
: 
. Meyer 
'Carl D 


ay 


Matrix 
Analysis 
and 
Applied 
Linear 
Algebra 
Second Edition 

; 
a 
Von 
; 
ee 
are 
"el oe! a 
The only way to learn mathematics is to do mathematics. 
— Paul Halmos 
| 

7 nite 
jtitiltd 
Society for Industrial and Applied Mathematics 
Philadelphia 

Copyright © 2023 by the Society for Industrial and Applied Mathematics 
10987654321 
All rights reserved. Printed in the United States of America. No part of this book may be reproduced, stored, or transmitted in any 
manner without the written permission of the publisher. For information, write to the Society for Industrial and Applied Mathematics, 
3600 Market Street, 6th Floor, Philadelphia, PA 19104-2688 USA. 
No warranties, express or implied, are made by the publisher, authors, and their employers that the programs contained in this volume 
are free of error. They should not be relied on as the sole basis to solve a problem whose incorrect solution could result in injury to 
person or property. If the programs are employed in such a manner, it is at the user's own risk and the publisher, authors, and their 
employers disclaim all liability for such misuse. 
Trademarked names may be used in this book without the inclusion of a trademark symbol. These names are used in an editorial 
context only; no infringement of trademark is intended. 
Publications Director 
Kivmars H. Bowling 
Executive Editor 
Elizabeth Greenspan 
Acquisitions Editor 
Elizabeth Greenspan 
Developmental Editor 
Rose Kolassiba 
Managing Editor 
Kelly Thomas 
Production Editor 
David Riegelhaupt 
Copy Editor 
Susan Fleshman 
Production Manager 
Donna Witzleben 
Production Coordinator 
Cally A. Shrader 
Compositor 
Carl D. Meyer 
Graphic Designer 
Doug Smock 
Library of Congress Cataloging-in-Publication Data 
Names: Meyer, C. D. (Carl Dean), author. 
Title: Matrix analysis and applied linear algebra / Carl D. Meyer, North 
Carolina State University, Raleigh, NC. 
Description: Second edition. | 
Philadelphia : Society for Industrial and 
Applied Mathematics, [2023] | Series: Other titles in applied 
mathematics ; 188 | Includes bibliographical references and index. | 
Summary: "Matrix Analysis and Applied Linear Algebra, Second Edition 
circumvents the traditional definition-theorem-proof format, and 
includes topics not normally found in undergraduate textbooks. Taking 
readers from elementary to advanced aspects of the subject, the author 
covers both theory and applications. 
The theoretical development is 
rigorous and linear, obviating the need for circular or non-sequential 
references.An abundance of examples and a rich variety of applications 
will help students gain further insight into the subject.A study and 
solutions guide is also available"-- Provided by publisher. 
Identifiers: LCCN 2022048558 (print) | LCCN 2022048559 (ebook) | ISBN 
9781611977431 (hardback) | ISBN 9781611977455 (paperback) | ISBN 
9781611977448 (ebook) | ISBN 9781611977462 (ebook other) 
Subjects: LCSH: Algebras, Linear. | Matrices. | Algebras, Linear. | 
Matrices. | AMS: Linear and multilinear algebra; matrix theory -- 
Instructional exposition (textbooks, tutorial papers, etc.).| Linear 
and multilinear algebra; matrix theory -- Basic linear algebra -- Norms 
of matrices, numerical range, applications of functional analysis to 
matrix theory. 
Classification: LCC QA188 .M495 2023 (print) | LCC QA188 (ebook) | DDC 
512/.5--dc23/eng202301 
12 
LC record available at https://Iccn.loc.gov/2022048558 
LC ebook record available at https://Iccn.loc.gov/2022048559 
SleUi.. 
is a registered trademark. 

Contents 
Preface . 
Photo Credits. 
The Language of Linear ai 
ae 
Pe 
1.1 
Introduction 
1.2 
The Language 
' 
1.3 
Elementary Geometry 
. 
Pear 
1.4 
Euclidean Norm and Standard Inner Product 
. 
1.5 
Non-Euclidean Norms and Convergence 
1.6 
$Orthogonality 
A 
hae 
1.7 
Linearity and Matrix Aaiepueetion 
: 
1.8 
Elementary Properties of Matrix Multiplication 
. 
1.9 
Matrix Inner Product and Norms 
. 
1.10 
Unitary and Orthogonal Matrices 
. 
1.11 
Short History of Matrix Theory 
. 
92 
115 
Systems, Elimination, and Echelon Forms 
. 
. 
. 
119 
2.1 
Gaussian Elimination 
2.2 
Elementary Matrices and Echelon Forms 
. 
2.3. 
Nonsingular Matrices and Inverses 
2.4 
Rank of a Matrix 
2.5 
General Linear Systems 
2.6 
Two Typical Applications 
2.7 
The Kronecker Product 
sae 
2.8 
Making Gaussian Elimination Work 
. 
2.9 
Ill-Conditioned Systems 
2.10 
Triangular Factorizations 
BR 
Biel 
gs 
2.11 
Short History of Triangular Factorizations 
Eigensystem Basics 
.........4.-. 
3.1 
Introduction to Eigensystems 
. 
3.2 
Similarity and Diagonalization 
3.3. 
Functions of Diagonalizable Matrices 
3.4. 
Normal Matrices 
{19 
136 
159 

vi 
Contents 
3.5 
Singular Value Decomposition (or SVD) 
. 
. 
. 
. 
+ 
358 
3.6 
Positive Definite Matrices and Quadratic Forms 
. 
382 
VECEOL SPACES 
bs 
vee Serer 
niin, saa 
IOP 
4.1 
Spaces and Subspaces 
... 
1 
0+ 
+ 
+ 
+ 
«+ 
« 
« 
409 
4.2 
The Fundamental Subspaces 
...... 
++ 
++ 
428 
4.3 
Orthogonal Complements and Projections 
. 
. 
. 
. 
456 
4.40 es beastsquares= 
PEPE 
nee 
. 
Fok 
ws 
ee 
+ 
ASL 
AG 
Coordinates: 
<;eermes 
SI. 
Rw 
ce 
et 
ew 
OUD 
4:6> = Chane ofpases' 
(PN 
aaee 
. 
PR. 
Re 
es 
os 
BLS 
4.7" 
Complementary Subspaces..:........ 
544 
4.8 
Jordan Form for Nilpotent Matrices... 
.. 
.. 
571 
4.9 
Jordan Form for General Matrices 
...... 
. 
584 
4.10 
Functions of Nondiagonalizable Matrices 
. 
. 
. 
. 
. 
595 
4.11 
Difference Equations, Limits, and Summability 
. 
. 
616 
Inner Product Spaces and Fourier Expansions 
. 
645 
Bd ee LReT Ee rOOUCLISDACEEme 
eck) 
a 
gis. 
.6 
.« 
corkcls 
oe 
naan 
5.2 
Gram=Schmidt Process 
. 
<0. 
% 
«06 
«(5 
= 
«nw 
we0oe 
Boe 
QR KACCOLIZACION 
ees 
ete 
aes 
<P 
se 
eee 
eOUG 
5.4" 
Fourier: Expansions 
"os 
9 
«= 
0% 
6 
2 
« 
fo 
oe 
5. 
Discrete rourier Lransioriiey 
« 
29 
« 
« 
© 
s 
«0s 
ae) 
eho 
Eigenvalue Continuity and Computations 
. 
. 
. 
757 
6.¢ 
 Bigenvalne Gontinuity 
(ou) 
. 
aw 
+ 
© 
ois) 
eee 
ron 
6.2 
.Eigenvalue;Localizations 
|. 
te 
. 
"nu. 
"an! «ee 
eroo 
6.3" 
~ PowersMiethodsvian.a 
ett 
cae) 
© 
Skee 
(so) 
oe 
eleel 
a 
te eee 
6.4 
QR terations tes 
sock 
ve 
ak 
«- 
alee 
a 
Pas 
a 
Perron—Frobenius and Nonnegative Matrices 
. 
797 
cea 
Introduction 
(ps. 
5) 
steers 
eae 
oe 
3 
t.a° 
-Nonnegative Matrices. 
. 
«. 
.« 
2 
« 
« 
:0ss. 
sce 
Meee 
eee 
1.3 . Positive Matricésat® 
aungwamaca 
se 
oe. 
ees 
7.4 
Perron—Frobenius Theorem 
.......... 
808 
7.5 
«~<.Primitive, Matrices 
ao 
u-tiare 
«ee eco 
ee] 
7.6, > «Periodic Matrices». 
u.-anaeatac: 
ee 
ee 
th 
Differentiation of the Perron Root and Vector 
. 
. 
840 

Contents 
7.8 
Perron Complementation 
7.9 
Markov Chains 
. 
7.10 
Stochastic Complementation 
7.11 
Simon—Ando Theory for Weakly Coupled one 
vil 
851 
859 
882 
890 
Matrix Analysis via Resolvent Calculus 
. 
. 
. 
. 
899 
8.1 
<A Short Review 
, 
8.2 
Eigenvalue Continuity 
. 
8.3 
Eigenvalue Differentiation 
Appendix: An Overview of Determinants 
9.1 
Introduction 
9.2 
Fundamentals 
hy 
satin 
OR 
2 
9.3. 
Additional Properties of Determinants 
. 
Index 
899 
O71. 

von Neumann Essay 
As a mathematical discipline travels far from its empirical source, or still 
more, if it is a second and third generation only indirectly inspired by ideas 
coming from "reality" it is beset with very grave dangers. It becomes more and 
more purely aestheticizing, more and more purely l'art pour l'art. This need 
not be bad, if the field is surrounded by correlated subjects, which still have 
closer empirical connections, or if the discipline is under the influence of men 
with an exceptionally well-developed taste. But there is a grave danger that 
the subject will develop along the line of least resistance, that the stream, so 
far from its source, will separate into a multitude of insignificant branches, and 
that the discipline will become a disorganized mass of details and complexities. 
In other words, at a great distance from its empirical source, or after much 
"abstract" inbreeding, a mathematical subject is in danger of degeneration. At 
the inception the style is usually classical; when it shows signs of becoming 
baroque, then the danger signal is up. It would be easy to give examples, to 
trace specific evolutions into the baroque and the very high baroque, but this, 
again, would be too technical. In any event, whenever this stage is reached, the 
only remedy seems to me to be the rejuvenating return to the source: the re- 
injection of more or less directly empirical ideas. I am convinced that this was a 
necessary condition to conserve the freshness and the vitality of the subject and 
that this will remain equally true in the future. 
— John von Neumann 
From his essay The Mathematician 
Works of the Mind, University of Chicago Press, 1947. 

Preface: Second Edition 
This second edition of Matrix Analysis and Applied Linear Algebra differs 
substantially from the first edition in that this edition has been completely rewrit- 
ten to include reformulations, extensions, and pedagogical enhancements. The 
goal in preparing this edition was to create an easily readable and flexible text- 
book that is adaptable for a single semester course or a more complete two- 
semester course. The following features are some of the characteristics of this 
edition. 
Scaffolding. Reacting to criticism concerning the lack of motivation in his 
writings, Gauss remarked that architects of great cathedrals do not obscure 
the beauty of their work by leaving the scaffolding in place after the con- 
struction has been completed. His philosophy epitomized the formal presen- 
tation and teaching of mathematics throughout the nineteenth and twentieth 
centuries, and it is still commonly found in mid-to-upper-level mathematics 
textbooks. The inherent efficiency and natural beauty of mathematics are 
compromised by straying too far from Gauss's viewpoint. But, as with most 
things in life, appreciation is generally preceded by some understanding sea- 
soned with a bit of maturity, and in mathematics this comes from seeing 
some of the scaffolding. This book's front cover portrays this philosophy. 
Purpose. The purpose of this text is to present the contemporary theory and 
applications of linear algebra to university students studying mathematics, 
engineering, or applied science at the postcalculus level. Because linear al- 
gebra is usually encountered between basic problem solving courses such as 
calculus or differential equations and more advanced courses that require 
students to cope with mathematical rigors, the challenge in teaching applied 
linear algebra is to expose some of the scaffolding while conditioning students 
to appreciate the utility and beauty of the subject. Effectively meeting this 
challenge and bridging the inherent gaps between basic and more advanced 
mathematics are primary goals of this book. 
Linear Development. An effort was made to ensure that the development of 
the theory is completely linear. With the exception of a few references to 
determinants in the appendix, material at any point is not dependent on 
subsequent developments. This allows instructors to move linearly through 
their course without the need to substantially refer to later developments. 

Preface: Second Edition 
e 
Graduated Sophistication. In addition to being linear, the development is 
also graduated by level of sophistication. The text starts from traditional 
first principles in the earlier chapters and progresses through a deeper un- 
derstanding of both theory and applications in the later chapters. This allows 
for a traditional single-term course based on roughly half of the text without 
having to refer to more advanced topics, and it provides for a complete two- 
term course covering the range of theory and applications generally reserved 
for discussions beyond the basics. 
e 
Carefully Constructed Exercises. There are ample exercises at all levels of 
difficulty throughout the text. Each section is complemented by problems 
that range from straightforwardly easy to moderately challenging to being 
difficult for all but the best students. Many exercises are carefully constructed 
so as to condition students for topics that soon follow. 
e 
Supplemental Solutions and Study Guide. An accompanying Study and So- 
lutions Guide containing complete solutions and discussions of each exercise 
is available as a standalone volume. The supplement is designed to help stu- 
dents understand the point of each exercise and to guide their development 
as they move through the subject. It is possible to use the text in conjunction 
with the Study and Solutions Guide for self-directed study. 
e 
Flexible Rigor. The development is rigorous at all levels, but accompanying 
narratives allow formality to be tempered or bypassed. Theorems and proofs 
are clearly highlighted, but the text is designed so that the degree of rigor 
that an instructor chooses to employ is flexible enough to accommodate a 
strict treatment or a casual overview. Along with each major theorem there 
are associated discussions and illustrative examples designed to convince stu- 
dents of the validity of the theorem without a deep dive into the proof. 
e 
Applications. As in the first edition, there is an emphasis on applications, but 
in this second edition both the depth and breadth of the applications have 
been expanded. The material on numerical aspects as well as the development 
of the theory and applications of the discrete Fourier transform are particular 
examples. 
e 
History. This second edition also contains enhancements of the historical 
remarks over those in the first edition. There is a sharper focus on the per- 
sonalities of the individuals who created and contributed to the subject's 
development. 
e 
Acknowledgments. This edition evolved from the first edition due to the help 
of countless people from around the world who suggested improvements to 
nearly all facets of the book, and I am deeply indebted to them. Special 

Preface: Second Edition 
xi 
thanks go to the hundreds of students at NC State University who took both 
undergraduate and graduate courses in the last twenty-two years using the 
first edition of this text. Experiences derived from teaching these students 
were invaluable in preparing this second edition, and my interactions with 
them are responsible for many improvements. Finally, I wish to acknowledge 
Elizabeth Greenspan, Executive Editor at SIAM, for her encouragement and 
patience over the years since the first edition appeared. This edition would 
not have been possible if it were not for her belief in me and my book. 
e 
Dedication. I dedicate this book to Bethany, my friend, partner, and wife, and 
to our children Martin D. Meyer and Holly Meyer Kenney, and to our grand- 
children, Margaret Elizabeth Meyer Harding, Allison Catherine Kenney, and 
Ryan Patrick Kenney. 
Carl D. Meyer 
March 27, 2023 

es seine 
rerenenek 

Photo Credits: Second Edition 
Page 167 (Schur): Archives of the Mathematisches Forschungsinstitut Oberwolfach (CC BY-SA 2.0 
DE). https: //creativecommons.org/licenses/by-sa/2.0/de/deed.en 
Page 167 (Haynsworth): Courtesy of the Department of Mathematics and Statistics, College of 
Sciences and Mathematics, Auburn University. 
Page 284 (Doolittle): Courtesy of Antiochiana, Antioch College. 
Page 285 (Crout): Courtesy of the MIT Museum. 
Page 286 (Goldstine): Courtesy of American Philosophical Society. 
Page 424 (Courant): Used with permission of The National Academies Press, from Biographical 
Memoirs: Volume 82, Page 78, 2003; permission conveyed through Copyright Clearance Center, Inc. 
Page 658 (Schmidt): Archives of the Mathematisches Forschungsinstitut Oberwolfach (CC BY-SA 
2.0 DE). https: //creativecommons.org/licenses/by-sa/2.0/de/deed.en 
Page 765 (Galois duel): Bowen, Jonathan & Giannini, Tula. (2015). Galois Connections: Mathematics, 
Art, and Archives. DOI: 10.14236/ewic/eva2015.18. 
Page 766 (Gerschgorin): Courtesy of the University of St Andrews. 
Page 766 (Taussky): Archives of the Mathematisches Forschungsinstitut Oberwolfach (CC BY-SA 
2.0 DE). https: //creativecommons.org/licenses/by-sa/2.0/de/deed.en 
Page 766 (Brauer): 
Reprinted with permission from North Carolina Academy of Science. "Alfred 
T. Brauer, Mathematician, and Developer of Libraries," by Richard D. Charmichael, Journal of the 
Elisha Mitchell Scientific Society, Vol. 102, No.3, 1986, pp. 88-106. 
Page 772 (von Mises): Courtesy of the University of St Andrews. 
Page 774 (Wielandt): Archives of the Mathematisches Forschungsinstitut Oberwolfach (CC BY-SA 
2.0 DE). https: //creativecommons .org/licenses/by-sa/2.0/de/deed.en 
Page 779 (Rutishauer): Courtesy of the University of St Andrews. 
Page 779 (Francis): Reprinted from Gene Golub, Frank Uhlig, "The QR algorithm: 50 years later its 
genesis by John Francis and Vera Kublanovskaya and subsequent developments," IMA Journal of 
Numerical Analysis, 2009, Volume 29, Issue 3, Page 468, by permission of Oxford University Press. 

xiv 
Photo Credits: Second Edition 
Page 779 (Kublanovskaya): Reprinted from Gene Golub, Frank Uhlig, "The QR algorithm: 50 years 
later its genesis by John Francis and Vera Kublanovskaya and subsequent developments," IMA Jour- 
nal of Numerical Analysis, 2009, Volume 29, Issue 3, Page 474, by permission of Oxford University 
Press. 
Page 791 (Golub): Courtesy of Stanford News Service. 
Page 791 (Golub license plate): Gene Golub's license plate, photographed by Professor P. M. Kroo- 
nenberg of Leiden University. 
Page 791 (Kahan): Photographed by George M. Bergman, Berkeley. Archives of the Mathematisches 
Forschungsinstitut Oberwolfach. 
Page 798 (Perron): Photographed by Karl-August Keil, Augsburg. Archives of the Mathematisches 
Forschungsinstitut Oberwolfach. 
Page 798 (Frobenius): Courtesy of the University of St Andrews. 
Page 834 (Leslie): Reprinted by permission from Springer Nature: Springer The Leslie matrix (1945) 
by Nicolas Bacar Copyright 2011. 
Page 849 (Neumann): Courtesy of Maxine Lerman. 
Page 867 (Brin and Page): Photographed by Joi Ito (CC BY 2.0). 
https: //creativecommons.org/licenses/by/2.0/legalcode 
Page 890 (Ando): Photograph by Stefano Siviero. 
Page 899 (Kato): Photographed by George M. Bergman, Berkeley. Archives of the Mathematisches 
Forschungsinstitut Oberwolfach. 

CHAPTER 
1 
The Language of 
Linear Algebra 
1.1 INTRODUCTION 
Friends or family may soon ask you what you are currently studying, and you 
might say "linear algebra." But since you are reading this book, I hope that you 
will say "applied linear algebra." Regardless of your answer, expect the follow-up 
response "Oh, that's matrices, isn't it?" or perhaps "Aha, that's solving systems 
of linear equations—like we learned about in high school, right?" While these 
topics are relevant, and while they are among the first things discussed, they do 
not define the subject. You can dodge these questions by pleading ignorance and 
say, "I'm not sure because I'm just getting started." This gets you off the hook 
for the time being, but eventually you will need to have a better reply. And even 
if no one ever asks, it's still important for you to know the correct answer. 
So, what is it? 
Linear Algebra 
Linear algebra is the study of linear functions defined on vector spaces. 
Of course, the conciseness of this definition hides things that require further 
explanation, but once an appreciation of linearity and vector spaces is acquired, 
the meaning of these words becomes clear. On the other hand, formulating a 
description of applied linear algebra is where things get wobbly. It's tempting 
to say something like "applied linear algebra is using linear algebraic concepts 
to solve problems from practical applications." This isn't wrong, but it's just 
too vague. So let's take another stab at it. How about saying something like, 

Chapter 1 
The Language of Linear Algebra 
"the applied part is the emphasis on matrix computations?" Computation is 
important, but by itself it's too narrow to be the defining feature. Perhaps it 
is wise to sidestep the meaning of applied linear algebra, but since the term 
appears in the title of this book, I feel I cannot completely evade the issue, and 
I am obliged to say something about it. 
Applied 
Linear Algebra 
- The art of 'applying linear algebra lies in the ability 
to discover and use 
_ 
the most appropriate coordinate systems by which to model, analyze, and 
solve meaningful problems in the context of vector spaces. A primary 
mode of expression of this art is by means of matrix decompositions. 
Comprehending and appreciating all that these words embody requires an 
understanding of most of the material in this book. So, for now, just remem- 
ber that the defining words appeared here, and, as you move through the text, 
periodically return to reflect on these words as your depth of understanding 
increases. 
1.2 
THE LANGUAGE 
Linear algebra has its own terminologies and notations. Before any mathematics 
Scalars 
can be appreciated you must first become fluent in the language' of linear algebra. 
The term scalar is just another word for "number," and the set of numbers F 
from which entries for a particular theory or development are drawn is called 
the underlying scalar field. In this book F is either the field R of real numbers 
or the field C of complex numbers. In completely abstract settings F may be a 
more general algebraic field, but such issues need not concern you here. 
There are times when it is necessary to distinguish between the field R of 
real numbers and the complex field C, while at other times the distinction is 
not relevant. When it matters, use the explicit notation R or C. When it does 
not matter, the notation F is used with the understanding that F can be either 
R or C, In other words, whenever you see the symbol F you are free to think 
in terms of either real or complex numbers for the context under consideration. 
Individual scalars in F are generally denoted by lowercase, plain-face letters 
a,b,c,... along with their Greek counterparts a, 6,7¥,.... 
The French mathematician Pierre-Simon Laplace (1749-1827) said that, "Such is the advantage 
ofa well-constructed language that its simplified notation often becomes the source of profound 
theories." Linear algebra is a testament to the validity of Laplace's statement. 

1.2 The Language 
3 
Vectors and Euclidean n-Space 
At the outset, a vector is an n-tuple shaped either as a row v = (v1, V2,...,Un) 
V1 
or as a column v = { : | whose entries (or coordinates) v; come from the 
Un 
underlying scalar field F, which is either R or C. The set of all such vectors 
is denoted by F" and is called the Euclidean n-space over F. Use the respec- 
tive notations R" and C" when it is necessary to explicitly distinguish real 
Euclidean n-space from complex Euclidean n-space. 
It often does not matter whether a vector is written as a row or as a column. 
For example, R? is the set of ordered pairs of real numbers (the cartesian plane) 
and it is usually irrelevant if the ordered pairs are written as (x,y) or as (;) 
When it matters, explicit designations such as R'** or R?*! or, more generally, 
b>" or ke may be used to make it clear that the discussion is respectively 
limited to row vectors or column vectors. Individual vectors are always denoted 
by lowercase, boldface letters such as a,b,c.... 
Vector Spaces 
Euclidean n-spaces are special cases of abstract vector spaces that are defined and 
developed in Chapter 4 on page 409. However, this abstraction' is not required at 
the outset. Here is what you initially need to know about vector spaces. Euclidean 
n-space F" is the prototypical vector space, and all other vector spaces are 
either disguised versions of F" or generalizations of it. Consequently, it is best 
to completely understand the properties and geometry of F" before examining 
abstractions, so, until it is time to consider more general vector spaces, the term 
vector space will be synonymous with Fuclidean n-space, and the term vector 
will be synonymous with n-tuple (usually shaped as a column unless otherwise 
indicated or implied by the context). 
Transpose and Conjugate Transpose 
As mentioned earlier, it often does not matter if vectors in F" are considered 
to be columns or rows, but sometimes it does. When it matters it is handy to 
have mechanisms that convert columns into rows and vice versa. There are two 
of them. The transpose operation applied to a vector v € F" is denoted by v/, 
and the effect is simply to change the orientation of v. In other words, 
While explaining on page 770 of the second volume of Mathematical Thought from Ancient 
to Modern Times how Arthur Cayley's addiction to abstraction prevented his concept of an 
algebraic group from attracting any attention at the time, Morris Cline observes that, "Pre- 
mature abstraction falls on deaf ears whether they belong to mathematicians or to students." 
Mathematics teachers, writers, and researchers all realize Cline's obvious truth, but it is in 
their nature to crave abstraction—it's their opiate, and rehab is extremely difficult. (More 
about Cayley is on page 116). 

Chapter 1 
The Language of Linear Algebra 
V1 
T 
V1 
. 
v2 
v2 
= (Vy, Va,+-+, Un) 
and" 
(v4, V2, +--+, riley a 
on 
Un 
The complex conjugate V of v € F" is the vector of conjugates. For example, 
if v = (v1, v2,.--) Un), in which vg = a, +if, (i = J/—1 is the complex 
unit), then V = (U7, U2, ..., MH), where T; = ax —iG,. It is often necessary to 
combine conjugation with transposition, so the conjugate transpose operation is 
defined to be v* = Vv! = v!. For example, 
2431) 
2+ 3i 
4 
\=(2-31,4,71)) 
and 
(2-314, 7)"=| 
4 
Ti 
ae 
Conjugation has no effect on real numbers, so v* =v' whenever v € R". 
By convention, vectors in F" are generally considered as columns unless 
otherwise indicated or called for by the context, and, as mentioned earlier, they 
are denoted by lowercase boldface letters such as a,b,c,..., so the transpose 
and conjugate transpose notations such as a' ,b',c*,... generally denote row 
vectors. Regardless of whether the discussion involves row or column vectors, 
plain-face subscripted letters such as aj, b3,c,,... denote individual entries in 
corresponding vectors. While it is legitimate to write v € R® as well as v" € R®, 
more clarity (if needed) is available by writing something like v € R°®*! and 
vl fe R?*3. 
Unit Vectors and the e-vector 
The i*" unit vector e; € F" is defined to be the vector whose i' entry is 1, 
and all other entries are zero. 
0 
0 
e; = | 1 | « i" entry 
and 
e? =(0---010--- 0). 
0 
+ 
: 
i" entry 
0 
For example, the unit vectors in R? are e; = eS and eg = a while the 
unit vectors in R® are 
occ (0) ei 
yore (0) 
Pema 
The shape of a unit vector e; is allowed to vary with the context in which it is 
used (i.e., it can be either a row or a column), but when clarity is needed, the 
conventions introduced earlier are used. 

1.2 The Language 
5 
1 
The vector 
e = ));e; = ( 
: 
1 
warrant reserving the boldface' letter "e" specifically for this vector. The size and 
shape of e are derived from the context, but when it is necessary to emphasize 
that a vector of ones should be shaped as a row, we may write e7 = (1,1,...,1). 
of all ones occurs frequently enough to 
Vector Addition and Scalar Multiplication 
Vector addition and scalar multiplication in F" are defined to be entrywise 
addition and entrywise multiplication. In other words, if 
U1 
V1 
u1+vi1 
Quy 
U2 
V2 
u2 + v2 
aug 
Le 
: 
and Vise 
. 
|, then u+v= 
: 
and au = 
Un 
Un 
Un + Un 
aUn 
Addition and scalar multiplication are often used in conjunction with the trans- 
pose and conjugate transpose operations. When they are, the following self- 
evident rules apply. 
and 
(222) 
Vector addition and scalar multiplication have the following useful geometrical 
interpretations in R? and R'. 
Parallelogram Law 
A vector v in R? or R® can be perceived as a directed arrow from the origin 
to the point defined by the coordinates of v. This allows vector addition in R? 
and R® to be visualized with the parallelogram law, which states that the sum 
u+v 
is the vector defined by the diagonal of the parallelogram as shown in 
Figure 1.2.1 below. 
The plain-face "e" is Euler's number e = limn—+.9(1+1/n)" = 2.71828---, which is the base 
of the natural logarithm. No conflicts will occur by paying attention to whether boldface e or 
plain-face e is used. 

Chapter 1 
The Language of Linear Algebra 
ut+V = (uy+V}, Uj+V2) 
FIGURE 1.2.1: PARALLELOGRAM LAW FOR VECTOR ADDITION IN R? 
Matrices 
A matriz is a rectangular array of numbers such as 
ho 
OF 
82 
47 
I 
iy 
(tl Ome) 
Z3 a= (3 0) X3 = (=35), 
3X3 
(0 
0 ), 
x 
Vi 6 
' 
x 
16-3 
(1.2.3) 
A2x3 = 'ie ee a 
Ciya = (2—iV3,6"-). 
The shape of a matrix is the respective number of its (horizontal) rows and 
(vertical) columns. Shape is indicated by a subscript of the form m x n and is 
pronounced "m by n" (not "m times n"). For example, writing A2,3 indicates 
that matrix A has two rows and three columns. When the shape is evident from 
the context, the shape subscript is omitted. 
A matrix is said to be square whenever it has the same number of rows as 
columns—e.g., matrix I in (1.2.3) is a square matrix. When the number of rows 
is not equal to the number of columns, the matrix is said to be rectangular—e.g., 
A in (1.2.3) is a rectangular matrix. 
Matrix Notations 
Matrices are usually denoted with uppercase boldface letters A,B,C,..., and 
individual entries from a matrix are represented with corresponding subscripted 
lowercase plain-face letters. For example, the entry in row i and column j of 
matrix A is denoted by a,; (the row number is always listed first). Alternately, 
the (i,j) entry of A is sometimes denoted as [A];;. For the matrices A, Z, x, 
and c in (1.2.3), a2g = —1/9, 
[Z]31 
= V7, xo = —3.54, and co = e73 (only 
one subscript is needed to designate an entry from a vector, which is a matrix 
consisting of a single row or single column). An m by n matrix of generic or 
unspecified entries a;; may be denoted by writing A = [aij]lmxn- No distinction 
is drawn between 1 x 1 matrices and scalars—e.g., the matrix [3],,1 is regarded 
to be the same as the scalar 3. 

1.2 The Language 
T 
The symbol F"*" denotes the set of all m x n matrices whose entries come 
from F. Use the respective notations R™*" and C™*"" when it is necessary 
to explicitly distinguish the set of all real m xn matrices from the set of all 
complex m xn matrices. Writing A € F™*" means that A belongs to F"*" 
(ie., A is some m xn matrix of numbers that can be real or complex). 
Row and Column Notation with the Wild Card Symbol " * " 
To designate a single row—say the i*" row—in a matrix A,,xn, write A;,. In 
this context the asterisk 
x acts as a wild-card symbol. In other words, writing 
Aj; indicates that the row subscript is fixed at i while the column subscripts 
vary through j = 1,2,...,n. Similarly, A,; denotes the j* column of A. For 
example, if A is the matrix in (1.2.3), then 
Mra 1/9) 
and 
Aco 3) 
Submatrices 
A submatriz S of a given matrix A is simply another matrix contained within 
A. Submatrices always lie on the intersection of some subset of rows with some 
se 
sik 
subset of columns in A. For example, if A = (2 
4 1), then the matrix 
Sho 
tek 
al 
ace é i that lies on the intersection of rows 1 and 2 with columns 1 and 
3 is a submatrix of A. A convenient notation to express this fact is to write 
S = A[1,2|1,3]. In general, writing 
Alligisenm's tp | Iugdancees fa 
(1.2.4) 
denotes the p x q submatrix of A that lies on the intersection of rows 7,71, .-.,%p 
with columns j1,j2,..-,jq m A. 
Note: By convention, a matrix is to be considered as a submatrix of itself. 
Matrix Equality 
Matrices A = [a;;]| and B = [b,;] are said to be equal (and we write A = B) 
whenever A and B have the same shape and corresponding entries are equal. 
In other words, a;; = bj; for each entry. 
Addition and Scalar Multiplication 
Addition and scalar multiplication for matrices are the same entrywise operations 
defined on page 2 for vectors. In particular, for A,B € F""", the entries in the 
sum A+B 
are obtained by adding corresponding entries 
[A + Bhi; = [Alay + [Bhis, 
and the difference of A and B is the mxn 
matrix A —B obtained by 
subtracting corresponding entries. 
For a € F and A = {a;;] € F"*", the product aA is defined to be 
the matrix of entrywise products—i.e., 
[aA],; = aa;;. Scalar multiplication is 
commutative in the sense that aA = Aa. Other basic properties of addition 
and scalar multiplication are summarized in Exercise 1.2.1 on page 12. 

Chapter 1 
The Language of Linear Algebra 
Diagonal Entries and Diagonal Matrices 
As illustrated below, the main diagonal in a square matrix 
Qn1i 
OGn2 
is defined to be the set of entries {a11,@22,.--,@nn} in A lying on the line from 
the upper-left-hand corner to the lower-right-hand corner, and each entry of the 
form a,j is called a diagonal entry. There are clearly other diagonal lines' in A, 
but since diagonal lines in A other than the main diagonal are rarely required, 
the term "diagonal" is henceforth interpreted to mean "main diagonal" unless 
otherwise specified. A diagonal matriz is a square matrix 
QO 
ee 
Tees 
0 
Die 
ei. 
seme 
0 
O 
«++ 
x 
in which each off-diagonal entry is zero—the stars « may or may not be zero. 
Triangular Matrices 
An upper triangular matric U is a square matrix such that all entries below 
the main diagonal are zero (i.e., uijj = 0 for ¢ > 7). Similarly, L is lower 
triangular when ¢;; = 0 for i < j. For example, 4 x 4 upper-triangular and 
lower-triangular matrices respectively look like 
kK 
k 
ke 
* 
ee 
ea 
tO) 
msl 
PLO es 
wor 
— 
ok 
ee OO 
Us 
OL Oak 
and 
L= 
x 
xk 
* 
Of? 
O}.0 
Ome 
k 
kk 
x 
where the stars 
* may or may not be zero. 
Transpose and Conjugate Transpose for Matrices 
The transpose and conjugate transpose of a matrix A € F"*" are extensions 
of the same concepts defined for vectors on page 3. In other words, the transpose 
of Amn 
is defined to be the nx m matrix A? obtained by interchanging 
rows and columns in A. More precisely, if A = [a;;], then [A?]i; = aj;;. For 
example, 
ae 
ier 
r 3 6 
Serie 
mien 
oNay 
etd 
5 
6 
ii 
: 
' 
Diagonals in A are generally defined as {@ip,,@2p2,---,4np,}, 
where 
{pi,po,... »Pn} is 
some permutation of N = {1,2,...,n}. The main diagonal is the diagonal that corresponds 
to the natural order NV. 

1.2 The Language 
9 
It is evident that (AT)" = A for all matrices. 
The conjugate transpose of A is defined to be AT = A'. Just as for 
vectors, the conjugate transpose A' is denoted by A*, so [A*];; = @;;. For 
example, 
isn 
sue 
tia 
144i ne 
ha 
hill 
ae 
el 
a 
ane 
and it should be clear that (A*)* = A for all A ¢ F"*". However, if ACR™™", 
then A* = A'. In other words, since complex conjugation has no effect on real 
numbers, A* can be replaced by A? when dealing with real matrices, but no 
harm is done by sticking with the * notation. 
The following theorem presents two fundamental properties of transposition 
and conjugate transposition. 
1 
ia Theorem, For all A Be 
é Le and ae F, the 
following state- z 
_ments are true. 
ae 
(A+B) aA Bt 
(A+B): =A" +B a ae 
(aA)! = a and (aA)* =A". 
(1.2.6) 
Proof Formal arguments for the statements involving the transpose parts of 
(1.2.5) and (1.2.6) are given. The proofs of the statements involving conjugate 
transposes are similar and are left as exercises. For each i and J, it is true that 
[((A+B)*]i; =(A +B], = [Aly + [Blje = [A7]y + [B7 
ij = [A7 + B7]ay. 
This proves that corresponding entries in (A +B)' and A? +BP are equal, 
and therefore (A + B)" = A'? +B'. Similarly, for each i and j, 
(aA) ]iz = [eA] 
je 
= [A]js = [A]; 
—> 
(@A)" =cA7. 
OF 
Note: The implication symbol => is short for "implies that." The black box Hf 
at the end of the proof is the ged symbol that is short for the latin phrase quod 
erat demonstrandum, which translates to "that which was to be demonstrated." 
Caution! When the asterisk symbol * is used as a superscript asin A*, it designates conjugate 
transpose, but when used as a subscript as in A,,;, it is the wild-card symbol introduced on 
page 7. In advanced theory the conjugate transpose of A is also called the adjoint of A. 
Since this is the first formal theorem and proof in this text, an introductory statement is in 
order. Computers can perform arithmetic much faster and more accurately than we can, and 
they are now rather adept at symbolic computation and mechanical manipulation of formulas. 
But computers cannot do mathematics very well—humans still have a monopoly on the ability 
to reason abstractly and create logical and innovative arguments. Regardless of whether your 
orientation is applied or theoretical, the degree to which you understand mathematics is gov- 
erned by your capacity to understand and create mathematical arguments and proofs. Linear 
algebra is a good subject for developing and honing these skills. 

For example, consider 
1 
244i 
1-31 
1 
244i 
1-3: 
A= (2-4 
3 
6) and 
B= (244 
3 
e6). 
(htt) 
1-31 
8 — 61 
5 
TST 
fobs EAS 
5 
Can you see that A is hermitian but not symmetric, while B is symmetric but 
not hermitian? 
Nature abounds with symmetry, and very often physical symmetry mani- 
fests itself as a symmetric matrix in mathematical formulations. The following 
example involving two springs in a symmetric configuration illustrates this fact. 
Consider two springs with respective stiffness constants k; and ky that are 
connected as shown below. 
Node 1 
ky 
Node 2 
ky 
Node 3 
f 
' 
' 
\ 
i 
a 
' 
' 
\ 
\ 
it 
"i 
x2 
H 
2s) ' 
oo See tet 
+1 
++! 
! 
' 
——— 
F, 
-F, ——————— 
-F3 
F3 = 
SYMMETRIC SPRINGS 

1.2 The Language 
pith 
The configuration in the top portion of this diagram is the neutral (no-tension) 
position in which no force is being exerted on any of the nodes. In the lower 
configuration the springs are displaced as indicated so that there are forces on 
each of the three nodes. Let F; be the force on node i. Consider displacement 
to the right to be in the positive direction, while displacement to the left is 
negative—e.g., displacing a node 2 inches to the left means that it is moved 
—2 inches. If node 1 is displaced x; units, node 2 is displaced x2 units, and 
node 3 is displaced by x3 units, then the length of the left-hand spring changes 
by AL; = x2 — 2 units, and the right-hand spring changes by AL = x3 — x2 
units. For example if 2; = —3in, ro = —2in, and x3 = 1 in, then the length 
of the left-hand spring changes by AL = rp — 21 = (—2) — (—3) = +1 in, and 
the right-hand spring changes by ALy = 23 — rq = (+1) — (—2) = +3in. 
Draw some pictures with numerical units to convince yourself that the ex- 
pressions for the AJZ;'s remain correct when the displacements are different 
from those illustrated in the diagram—e.g., draw the picture for x; = +1 in, 
Lq = —2in, and x3 = —3in so that AL, = x2 — 21 = (—2) — (+1) = —3 in, and 
AL 
= £3 — £2 = ( 3) 
( 2h = 
{Lea 
Hooke's law! says that when the length of a spring is changed by an amount 
AL from its neutral length, the spring exerts a force equal to 
F = kAL, where 
k; is the positive stiffness constant inherent to the spring. The force exerted at 
one end of a spring is naturally equal to the force exerted at the other end, but 
in an opposing direction. By Hooke's law, the force on node 1 is 
F, = ki AL, = k(x aim £1), 
and the force on node 3 is 
F3 = —kyALo = —ko(x3 ie Ge). 
The minus sign is present to indicate that F3 is in the opposite direction of F}. 
The force exerted by the left-hand spring on node 2 is the same as the force on 
node 1, but in the opposite direction, while the force exerted by the right-hand 
spring on node 2 is equal to the force on node 3, but in an opposing direction. 
Therefore, the total force on node 2 is 
Fo — —F; = F3 a —k (x2 = £1) 5 ko(x3 = £2) = ky21 = (ky cia k2)x2 + kox3. 
Hooke's law is named for Robert Hooke (1635-1703), an English physicist, but it was generally 
known to several people (including Newton) before Hooke's 1678 claim to it was made. Hooke 
was a creative person who is credited with several inventions, including the wheel barometer, 
but he was reputed to be a man of "terrible character." This characteristic virtually destroyed 
his scientific career as well as his personal life. It is said that he lacked mathematical sophis- 
tication and that he left much of his work in incomplete form, but he bitterly resented people 
who built on his ideas by expressing them in terms of elegant mathematical formulations. 

12 
Chapter 1 
The Language of Linear Algebra 
Organize the three equations for Fi, F2, and F3 by writing 
Fi =—kizi1+ 
kix2 
+ 023, 
Fo = 
kyx1- (ky ss k2)x2 + kox3, 
(1.2.8) 
F3= 
Oxi+ 
kore 
— koz3, 
and observe that the matrix of coefficients 
—ky 
ky 
0) 
Kee ( 
ky 
—(kit+k2) 
ke 
0 
ko 
—ko 
(known as the stiffness matrix) is a symmetric matrix. As a preview of things 
to come, notice that by writing the respective force and displacement vectors as 
F; 
x 
Fs (=) Bi Xa (=: 
), 
the system of equations in (1.2.8) can be interpreted 
F3 
z3 
in matrix form (as described on page 61) by writing 
Fy 
—ky 
ky 
0 
v1 
(=) -= ( 
ky 
—(ki +k) 
i) (= 
), or equivalently, 
F = Kx, 
F3 
0 
ko 
—kog 
23 
which is the matrix analog of Hooke's law. The point of this example is that 
symmetry in the physical problem translates into symmetry in the mathematics 
by way of the symmetric matrix K. When the two springs are identical (i.e., 
when k, = kg =k), even more symmetry is present, and in this case the stiffness 
matrix is 
—l 
1 
0 
K=#( 1 -2 
1), 
(1.2.9) 
0 
1 
-1l 
which is an example of yet another (but rarer) kind of symmetry called cen- 
trosymmetry. See Exercise 3.2.15 on page 318 for some properties of centrosym- 
metric matrices. 
Exercises for section 1.2 
1.2.1. Many features of addition and multiplication of real and complex num- 
bers are inherited by vector addition and scalar multiplication. Verify 
that the following ten properties ' hold for all vectors x, y, z € F" and 
for all scalars a, B € F. 
Vector Addition 
(Al) x+y €F" for all x,y € F" (closure property for addition) 
Other properties could have been listed, but these properties are singled out because they are 
the ones used as the axioms in the definition of a vector space on page 410. 

1.2 The Language 
1222s 
1.2.3. 
1.2.4. 
1.2.5. 
152.6; 
13 
(A2) x+(y+z) =(x+y) +z (associative property) 
(A3) x+y =y+x (commutative property) 
(A4) There is an element 0 € F" such that x +0 = x for every 
x € F" (additive identity property) 
(A5) For each x € F", there is an element (—x) € F" such that 
x +(—x) =0 (additive inverse property) 
Scalar Multiplication 
(M1) ax €F" forall 
a€C and x € F" (closure property for scalar 
multiplication) 
(M2) (a8)x = a(Bx) (associative property) 
(M3) a(x+y) = ax+ay (scalar multiplication distributes over vector 
addition) 
(M4) (a+6)x = ax+x (scalar multiplication distributes over scalar 
addition) 
(M5) 1x =x 
for every x € F" (1 is an identity element under scalar 
multiplication) 
Do the ten properties in Exercise 1.2.1 hold if F" is replaced by F"*"? 
Identify all submatrices of A = ( : a) 
Determine the unknown quantities in the following expressions. 
@) ax= (55) 0) 2(797 5°) =(5 8) 
Identify each of the following as symmetric, skew symmetric, or neither. 
Onl) somata 
Construct an example of a 3 x 3 matrix A that satisfies the following 
conditions. 
(a) A is both symmetric and skew symmetric. 
(b) 
<A is both hermitian and symmetric. 
(c) A is skew hermitian. 

14 
Chapter 1 
The Language of Linear Algebra 
1.2.7. 
1.2.8. 
Lea-oe 
1.2.10. 
PZ. 
Te2at 
ae 
Explain why the set of all n x n symmetric matrices is closed under 
matrix addition. That is, explain why the sum of two n x n symmetric 
matrices is again an n xX n symmetric matrix. Is the set of all n x n 
skew-symmetric matrices closed under matrix addition? 
Prove that each of the following statements is true. 
(a) If A= [a;;] is skew symmetric, then aj; =0 for each ). 
(b) If A = [a;;] is skew hermitian, then each aj; is a pure imagi- 
nary number—i.e., a multiple of the imaginary unit i. 
(c) If A is real and symmetric, then B= iA is skew hermitian. 
Let A be any square matrix. 
(a) Show that A+A7 issymmetric and A—A7 is skew symmetric. 
(b) Prove that there is one and only one way to write A as the 
sum of a symmetric matrix and a skew-symmetric matrix. 
If A and B are two matrices of the same shape, prove that each of the 
following statements is true. 
(a) (A+B)* = A*+B*. 
(b) 
(@A)* =@A*. 
Using the conventions given in the example concerning springs on page 
10, determine the stiffness matrix for a system of n identical springs, 
with stiffness constant k, connected in a line similar to that shown in 
the diagram on page 10. 
Consider the n x n Hilbert matrix defined by 
1 
1/2 
L/3 
vee 
1/n 
1/2 
1/3 
1/4 
sss 
If/n41 
= 
[is 
1/4 
1/5 
=~ 
Ine? 
1/n 
1f(n+1) 
1/(n+2) 
--- 
1/(2n—1) 
Express the individual entries h;; in terms of i and j. 

1.3 Elementary Geometry 
15 
1.3 ELEMENTARY GEOMETRY 
The geometry of R? and R?® is usually introduced with equations involving 
cartesian coordinates, but using cartesian coordinates in R" or C" becomes 
increasingly cumbersome as n grows beyond 3. A vector approach is preferred. 
For example, rather than describing the line £ that passes through the origin 
and a nonzero point v = (v1, V2, v3) € R® by the equations By) Vy = Y/ Vp = 2/03, 
L is better described as the set L = {av|a € R} of all scalar multiples of v. 
L = f{av|aeR} 
THE LINE IN R? GENERATED (OR "SPANNED") BY VECTOR 
Vv 
Similarly, as depicted below, the plane P through the origin and two non- 
colinear vectors u,v € R® is generated from the parallelogram law by forming all 
possible linear combinations ' (sums of scalar multiples) of u and v to produce 
P = {au+ Bvl|a,8 € R}. 
P ={ou+fv|a,8 
eR} 
THE PLANE IN R° GENERATED (OR "SPANNED" ) BY VECTORS 
u AND v 
Formally, a linear combination of vectors 
{vi,v2,...,Vr} 
is defined to be a sum of scalar 
multiples ajv1 +-::+Qrvr. We rarely require nonlinear combinations, so the word "linear" 
is often omitted—simply refer to a sum of scalar multiples as being a combination of vectors. 

Chapter 1 
The Language of Linear Algebra 
The same mechanisms are used in higher dimensions—i.e., the line through 
the origin and a nonzero vector v € F" is defined to be L = {av|a € F}, and 
the plane through the origin that contains two non-colinear vectors u,v € F" 
is P = {au + Bv|a,8 € F}. Although we cannot visually see these objects 
in higher dimensions, we can nevertheless deal with them mathematically (and 
intuitively) in the same manner as is done in R? and R®. 
Subspaces of F" and Spanning Sets 
Lines and planes through the origin in R® are flat surfaces that are generated 
by combinations of one or more vectors, and even R? itself can be described in 
such a manner. For example, the three unit vectors in (1.2.1) on page 4 generate 
R® because 
ay 
{aye1 + age2 + a3e3 | a1, A2,a3 
€ R} = {(% ) 
Higher dimensional spaces have more wiggle room to generate "flat surfaces" 
that are generalizations of lines and planes through the origin. For example, 
the extra dimensions in moving from R® up to R° allow room not only for 
lines generated by a single vector, planes generated by two vectors, 3-D surfaces 
generated by three vectors, but there is also room for "4-D surfaces" generated 
by combinations of four vectors in addition to all of R°, which is generated 
by the five unit vectors {e1,e2,e3,e4,e5, }. As n grows, there are increasingly 
more of these "flat surfaces" through the origin in F", so we need a way to keep 
track of them. The following definition provides the mechanism for doing so. 
1.3.1. Definition. 
For a set of vectors 
V = {v1,V2,...,v,} C F", the 
span of VY is defined to be the set 4 of all possible linear combinations 
X = {ayvi + aQv2+-::+ 
a-v,| a; € FH. 
Alternately, V is called the subspace of F" that is spanned (or 
generated) by V, and we say that "V is a spanning set for X." 
1 
0 
2 
For example, the span of VY = \v: 
= ( 
0), Vo= (-1}, V3= (-2)} 
—2 
1 
—1 
is, by definition, the set of all possible combinations ¥ = {a;v,+a2Vv2 +agv3}, 
but by observing that v3 
is in fact a combination of v, and vo (because 
v3 = 2v; + 3v3), it follows that the set of all combinations of {V1,V2,Vv3} 
is 
the same as the set of all combinations of {V1, V2}. In other words, 
X = span {v1, V2, V3} = span {v1, V2}, 

1.3 Elementary Geometry 
17 
and consequently V spans a subspace that is actually the plane ¥ through the 
origin in R® that is generated by {vj1, vo}. 
The following two special cases are worthy of note. 
e 
The subspace of F" that is spanned by V = {0} is just Y% =0 (the origin), 
and this is called the trivial subspace. 
e 
The subspace of F" spanned by the set of unit vectors V = {e1,e2,...,en} 
is all of F". Consequently, F" is considered as a subspace of itself. 
The algebraic and geometric properties of F" and its various subspaces are 
developed in more detail in Chapter 4 on page 409. 
Affine Spaces 
If we could visualize subspaces in F" with our eyes, then they would look like 
flat surfaces passing through the origin. But there are also flat surfaces that do 
not pass through the origin. Strictly speaking, such objects are not subspaces, 
but they are not significantly different because, as illustrated in the diagram 
below, they are simply subspaces that have been translated away from the origin 
by some vector p. 
A= p+ = {p+av|aeR} 
LPs 
jhe a 
oie p dhe ye: 
a= {av|aeR} 
| | 
FIGURE 1.3.1; AFFINE SPACES IN R® 
1.3.2. Definition. If 1 CF" is a subspace of F", and if p € F" is any 
nonzero vector, then 
A=pt 
t+ ={p+xixe dt} 
is called an affine space. In other words, affine spaces are simply sub- 
spaces that have been translated away from the origin, so they may be 
thought of as flat surfaces not passing through the origin in F". 

Chapter 1 
The Language of Linear Algebra 
For example, if 4% = span {e1,e2}, where e; and e2 are the first two 
unit vectors in R°, then % is just the xry-plane, so for the third unit vector 
e3 € R®, the affine space defined by A =e3+4 
is simply the plane that is one 
unit above the zy-plane and parallel to the ry-plane. 
Linear Independence and Dependence 
For a given set of vectors V = {v1,v2,-.-,Vvr} C F", it may be possible to 
express one of the vectors as a linear combination of the others, but on the 
other hand, it may not. The concepts of linear dependence and independence 
are based on which of these alternatives holds. The set V is linearly dependent 
when some v; € V can be expressed as a linear combination of the others—..e., 
Vea, fj UVi for scalars a; € F. Otherwise, V is linearly independent. 
An alternate way to distinguish dependence from independence is to think 
in terms of solving an equation 
Q1V1 
+ Q@eve+:::+arv, =0 
(1.3.1) 
in which the a;'s are unknowns. One solution for the a;'s is the trivial (or 
zero) solution a, = a2 =::: =a, = 0, but this is not relevant. The question 
concerning dependence vs. independence is whether or not (1.3.1) has a nontrivial 
solution for the a;'s. If {a1,a2,...,a,} is a solution to (1.3.1) for which some 
a; # 0, then v; can be expressed in terms of the other vectors by writing 
v; = —(1/a;) Na, Pe a;Vv;, so in this case V is linearly dependent. On the other 
hand if (1.3.1) has only the trivial (zero) solution, then no vector v; can be 
isolated on the left-hand side so as to express it as a combination of the others, 
and this means that V is linearly independent. In light of these observations, 
the following formal definition is adopted. 
1.3.3. Definition. Let V = {vi,v2,...,vr}C F". 
e Y is linearly dependent when there are scalars a;, not all 
a3 2) 
of which are zero, such that a,V1 
+ QoVo+°>::+a,vrp=0. 
© 
e 
YV is linearly independent when the only solution for the 
qj sin a1v1+Q2Vv2+:-:+a,Vv, = 0 is the trivial (or zero) 
(1.3.3) 
solution 
Q; =) =---= 
a, = 0. 
e 
By convention, the empty set @ is considered to be linearly inde- 
pendent. 
For example, consider 
ve (vi 
er 
eee) 
and 

1.3 Elementary Geometry 
19 
s-fu-(m-(Da-()} 
ass 
Notice that V is linearly dependent because v3 = —2v; + vo, or equivalently, 
2vi —V2+v3 = 0 so that (1.3.2) holds. On the other hand, 4 is linearly inde- 
pendent because no one of the x;'s is a combination of the other two (Exercise 
1.3.4), or equivalently, the only set of a;'s for which a,v1 + aoVv2 + a3Vv3 = 0 
is Q1 = @2 = a3 = 0, so (1.3.3) holds. 
Geometrically, a set of two vectors VY = {v1,v2} in R?® is linearly indepen- 
dent if they are not directed along the same line—i.e., vj # ave, and a set of 
three vectors V = {v1,v2,v3} in R° is linearly independent if they do not all 
lie in the same plane—look at the diagram illustrating the parallelogram law on 
page 15. More generally, 
e 
V = {v1,V2,...,vr} C F" is linearly independent when no 
(1.3.5) 
one of the vectors is in the space spanned by the others. 
Note: It is important to realize that individual vectors are neither dependent nor 
independent-——only sets are. For example consider e, = Wak eo = oe and 
e= en Because e simultaneously belongs to the set S; = {e,, e}, which is 
linearly independent, as well as Sp = {e1, e, e2}, which is a dependent set, it 
does not make sense to say that e is a "dependent vector" or an "independent 
vector." In other words, dependence and independence are relative to the sets 
under question, and hence they are strictly set-related properties. 
Exercises for section 1.3 
1.3.1. Sketch a picture in R® of the subspace spanned by each of the following. 
OO ONOUe OO 
» .0-0} 
of) 
OO} —(.O.O.0) 
ODO (DO) 
Explain why (i), (ii), and (iv) do not span R®, and then explain why 
(iii) and (v) are spanning sets for R°. Hint: Consider the unit vectors 
€1,€2,e3 given in (1.2.1). 

20 
Chapter 1 
The Language of Linear Algebra 
1.3.3. 
1.3.4. 
1.3.5. 
1.3.6. 
Sat. 
1.3.8. 
13:9; 
Determine which of the following sets are linearly independent and which 
are linearly dependent. 
amie)» Bevan) )} 
= ayeta) tay Pata) 
(oy 
E = {e1,€0,...,€n,} CF" 
(ie., the n unit vectors in F") 
Provide the details to verify that the set ¥V in (1.3.4) is linearly inde- 
pendent. 
Determine the total number of linearly independent subsets that can be 
PPD 
Ie 
Se 
AD 
constructed using the columns of A = € 
pene 
tie: ). 
Ou ome 
Show that {x1,x2,...,x,}C F" is linearly independent if and only if 
{X1, X2, ... Xx} is linearly independent. 
Prove that if the rows in A,,yx, are linearly independent, then for ev- 
ery Bmx p the rows in the augmented matrix (A|B) are also linearly 
independent. Is the converse true? 
Prove that every subset of a linearly independent set is also linearly 
independent. 
Prove that if D is a linearly dependent subset of V, then V must also 
be a dependent set. 
(a) 
Explain why the set V = {0} containing only the zero vector is 
linearly dependent. 
(b) 
Explain why any set containing a zero vector must be linearly 
dependent. 
(ie 2 
P23 
133.11. Let A= (: 5 °). 
(a) Are the columns of A linearly independent? 
(b) Are the rows of A linearly independent? 
(c) The space spanned by rows of A is appropriately called the row 
space of A, and similarly, the space spanned by the columns of 
A is called the column space. Give a geometric description of 
the row space and the column space of A. 

6 
: 
re 7 
bw 
| 

22 
Chapter 1 
The Language of Linear Algebra 
1.4 EUCLIDEAN NORM AND STANDARD INNER PRODUCT 
Much of linear algebra is geometric in nature because in large part it grew out of 
the need to generalize the basic geometry of R? and R?® to higher-dimensional 
spaces. A standard approach is to express geometric concepts in R? and R® in 
terms of coordinates (ordered pairs or triples), and then extrapolate to ordered 
n-tuples in R" and C". The concept of norm (or length) of a vector is a typical 
example. As shown in the following diagram, the respective norms (denoted by 
||u|| and ||v||) of u = (2,y) € R? and v = (z,y,z) € R® are obtained from 
the Pythagorean theorem by computing the length of each hypotenuse in the 
indicated triangles to produce 
ull 
= 22+ y? and —||v|| = Va? + y? + 2. 
as 
v|| 
= V2? + 
y? + 2? 
EUCLIDEAN NORM IN R? AND R® 
This standard measure of length (the square root of the sum of squares) is the 
euclidean norm in R* and R®. The extension to complex spaces requires a small 
wrinkle to ensure that norms of vectors with complex elements are real numbers. 
1.4.1. Definition. The euclidean norm of x € F" is defined to be 
[xl] = Vlaal? + |rol? +--+ + [rn|?. 
If F" = R", then ||x|| = /27+a3+---+ 22. 
Recall that the complex conjugate of a complex number z =a+ib is 
Z7=a— ib, 
and the magnitude (or absolute value) of z is |z| = /Zz = Va2 +62. The fact 
that |z|? = Zz =a*+0b? isa real number ensures that ||x|| is real when F = C. 
For example, if 
x== (0, +1; 22; 4); 
oands 
yet, 2 
Ostet i) 
(1.4.1) 
then ||x|| = VO+1+4+4+16=5 and |ly|| =/1+412+0+2 
=3. 

oo 
Proof. Proving (1.4.2) and (1.4.3) is Exercise 1.4.5, and (1.4.4) (the triangle 
inequality) is discussed and proven on page 28. 
Wl 
A 
'Normalization 
; 
Given a vector 
0 # x € F", it is frequently convenient to identify another vector 
u € F" that points in the same direction as x (i.e., is a positive multiple of 
x) but has unit length. Such a vector is constructed by normalizing x, which 
simply means that x is divided by its norm to produce u = x/ ||x||. Setting 
a =1/||x|| in (1.4.3) yields 
jell = |/ lel |] 
= 1/ Heel) lll = 2. 
(1.4.5) 
' 
For example, the normalization of x in (1.4.1) is 
u = x/ |x| =x/5 = (0, -1/5, 2/5, -2/5, 4/5). 
—_ 
____ Euclidean Distance 
The norm concept naturally leads to the notion of distance in F". As illustrated 
in the following figure, the parallelogram law from page 5 makes it visually 
evident that the vector u — v € R?® is one side of the shaded parallelogram, so 
the distance between vectors u and v in R? is |ju—v\|. 
DISTANCE BETWEEN TWO VECTORS 
; P <= Q is short-hand notation for saying "P if and only if Q." This means that statements 
P and Q are equivalent in the sense that 
P => Q (P implies Q) and Q => P (Q implies 
P) are both true statements. 

2 
1 
teayy 
For example, if x = ( 
:), y= (2). and z= ( 
1 i) 
then 
= 
=i 
(x|y) = (2)(1) + (4)(2) + (—2)(3) = 4 = (y|x), 
(y|z) = (1)(1 + 2i) + (2)(1) + (3)(-i) = 3 -i, 
(ly) = (1 — 28)(1) + (1)(2) + (i)(3) = 3 +i = (y[2). 
The value of (x|y) is independent of whether x and y are rows, or 
columns, or one of each. But to aid algebraic manipulation it is convenient to 
identify F" with F"*', and think in terms of column vectors because if 
ry 
Y1 
r2 
y2 
2 = 
Q 
and 
y = 
' 
In 
Un 
then (x|y) can be associated with the composition of row x? (or x*) with 
column y by means of sequentially moving across the row and down the column 
The standard inner product is the same as the dot product that you may have previously 
encountered, and in some older texts the standard inner product is called the scalar product. 
Since the standard inner product is the only inner product used until more general ones are 
introduced later in the development, the term "standard" is suppressed until some distinction 
is required. 

We now have multiple notations for the inner product—the bracket notation 
(x|y) and the row-by-column product notations x7y when F =R, and x*y 
when F = C. Each notation has advantages. For example, writing (x|y) is 
more efficient than using x' y or x*y because there is no need to worry about 
whether or not vectors are columns, and we need not worry about whether F is R 
or C. On the other hand, you will soon come to appreciate how row-by-column 
products are handy for performing algebraic manipulations. Get comfortable 
with both notations because both are used—the choice is governed by context. 
Inner Product Properties 
To help get used to both of the inner product notations, digest the following list 
of basic properties that are written with both notations. It is always assumed 
that vectors are columns when row-by-column products are indicated. 
a 
a 
ee 
(x|x) = x"x > 0, and (x|x)=x"x=0 => x=0 
(1.4.8) 
(ely +2) =x"(y +2) =x"y +x%2= (ely) + (xlz) 
(1.4.9) 
(x|ay) = x*(ay) = a(x*y) = a (xly) 
(1.4.10) 
(ax|y) = (ax)*y = a(x"y) = a (x|y) 
(1.4.11) 
tly =sey—y x= (yl) 
(1.4.12) 
If 
F=R, then (x)* becomes (x)7, and the conjugation in (1.4.11) and (1.4.12) 
disappears. Properties (1.4.8)—(1.4.12) are immediate consequences of the defi- 

Chapter 1 
The Language of Linear Algebra 
nition of inner product. For example, property (1.4.9) follows by writing 
(x|y +2) =x"(y +2) = S"35(yj +23) = > Biys + 754) 
j 
yy 
= So a5yj + Laz = xy 
+ 
x" = (xly) + (x|z). 
ui) 
j 
This property is expressed by saying that every inner product is linear-in the 
right-hand argument—see pages 52 and 54 for a more detailed discussion of 
linearity. It is also true that the standard inner product is linear in the left-hand 
argument—see Exercise 1.4.8. 
Inner Product and Norm 
; 
A particularly useful aspect of the standard inner product is that it generates 
the euclidean norm by means of writing 
Vix) = 
> lesl? = (bx 
(1.4.13) 
By using row-by-column inner product notation this can also be written as 
|x|| = 
Vx?x 
when 
F=R, 
and 
||x||=Vx*x 
when 
F=C. 
(1.4.14) 
This simple connection between norm and inner product belies its importance 
because, as will be seen in later developments, the inner product essentially 
defines the nature of geometry in higher dimensional spaces. This is important 
down the road because it will mean that changing the definition of the inner 
product is tantamount to changing the geometry of the underlying space. 
Cauchy—Schwarz Inequality 
A particularly important relationship between the euclidean norm and the stan- 
dard inner product is the Cauchy—Schwarz inequality—also called the Cauchy— 
Bunyakovskii-Schwarz (or CBS) inequality! It is one of the most important in- 
equalities in mathematics. 
This is named in honor of the three men who played a role in its development. The basic 
inequality for real numbers is attributed to Cauchy in 1821, whereas Schwarz and Bunyakovskii 
contributed by later formulating useful generalizations of the inequality involving integrals of 
functions. 
Augustin-Louis Cauchy (1789-1857) was a French mathematician who is generally regarded 
as being the founder of mathematical analysis—including the theory of complex functions. 
Although deeply embroiled in political turmoil for much of his life (he was a partisan of the 
Bourbons), Cauchy emerged as one of the most prolific mathematicians of all time. He authored 
at least 789 mathematical papers, and his collected works fill 27 volumes—this is on a par with 
Cayley and second only to Euler. It is said that more theorems, concepts, and methods bear 
Cauchy's name than any other mathematician. 

1.4 Euclidean Norm and Standard Inner Product 
Proof. 
Set a= x*y/x*x = x*y/'||x||? (assume x ¥ 0 because there is nothing 
to prove if x = 0) and observe that x*(ax — y) =0, so 
0 < |lax — y||? = (ax — y)*(ox — y) = Gx*(ax — y) —y*(ax —y) 
2 
4/2 
1.4.16 
fy 
ys ae WC) 
xP 
Since y*x = x*y, it follows that (x*y) (y*x) = |x*y|', so 
2 
2 
2 
< HylE lit —Ix*y! 
= 
2 
|x| 
Now, 0 < ||x||? implies 0 < |[y|| \|x||" — |x*y|?, and thus the CBS inequality 
is obtained. Establishing the conditions for equality is Exercise 1.4.9. 
lf 
Example (Classic Cauchy—Schwarz Inequality) 
It is important to also write the Cauchy—Schwarz inequality in terms of summa- 
tion notation because that is the way it was originally conceived. If x,y € R"™?, 
2 
A 
2 
2 
then (ys = 3; yj) , while ||x||° = 0,27, and |ly|l" = 0, yj. Conse- 
quently, the inequality (1.4.15) for real numbers can be stated as 
(Sam) < (x4) (D4) 
(1.4.17) 
This is the classic Cauchy—Schwarz inequality. 
Victor Bunyakovskii (1804-1889) was a Russian professor of mathematics at St. Petersburg, and 
in 1859 he extended Cauchy's inequality for discrete sums to integrals of continuous functions. 
His contribution was overlooked by western mathematicians for many years, and his name is 
often omitted in classical texts that simply refer to the Cauchy-Schwarz inequality. 
Hermann Amandus Schwarz (1843-1921) was a student and successor of the famous German 
mathematician Karl Weierstrass at the University of Berlin. Schwarz independently generalized 
Cauchy's inequality just as Bunyakovskii had done earlier. 

Chapter 1 
The Language of Linear Algebra 
Triangle Inequality 
One reason that the CBS inequality is important is because it helps to establish 
that the geometry in higher-dimensional spaces is consistent with the geometry 
in the visual spaces R? and R°. In particular, consider the situation depicted 
in the following diagram. 
TRIANGLE INEQUALITY IN R? 
Imagine traveling from the origin to the point x and then moving from x 
to the point x + y. Clearly, you have traveled a distance that is at least as 
great as the direct distance from the origin to 
x + y along the diagonal of the 
parallelogram. In other words, it's visually evident that ||x + y|| < ||x|| + |ly||. 
This is the triangle inequality in R?. (The triangle inequality in R or C says 
that |a+ 6| < |a|+ |6|.) Higher-dimensional spaces do not afford the luxury of 
visualizing geometric concepts with our eyes, so the question of whether or not 
the triangle inequality remains valid has no visual answer. The CBS inequality 
is precisely what is required to prove that with respect to the triangle inequality 
the geometry of higher dimensions is no different than that of the visual spaces. 
1.4.6. Theorem. (The Triangle inequality) 
IIx+yll < Ilxll+llyll 
for every x, y € F". 
Equality holds if and only if y = ax, where a is real and positive. 
Proof. 
Consider x and y to be column vectors, and write 
Ix+yll =(x+y)"(x+y) =x*x4x"y+y*x+y*y 
pee 
lies 
: 
(TALS) 
= ||x||? 
+ x*y + y*x + 
lly'. 
Recall that if 
z=a-+ ib, then 
z+ %=2a=2Re(z) 
and |z|? = a? + 0? > a?, 
so that |z| > Va? = |a| = |Re(z)| > Re(z). Using the fact that yx ax 
y 

1.4 Euclidean Norm and Standard Inner Product 
29 
together with the CBS inequality in (1.4.15) yields 
x"y ty"x = 2Re(x"y) < 2|x"y| < 2/|x|| ly]. 
Consequently, it follows from (1.4.18) that 
Ix + 
yl? < [lxl? +2 [xl Ilyll + lly? = (eel + Ilyll)?. 
The fact that equality holds if and only if y = ax, where a is real and positive, 
is Exercise 1.4.10. 
I 
It is not difficult to see that the triangle inequality can be extended to any 
number of vectors in the sense that || 57; x;|| <)>; ||x:||. Furthermore, it follows 
that for real or complex numbers, | 
>; 0i| < do; |ai| (the triangle inequality for 
scalars). 
Backward Triangle Inequality 
While the triangle inequality produces an upper bound for the norm of a sum, 
one of its corollaries yields a useful lower bound for the norm of a difference. 
1.4.7. Corollary. For all x, y < F", 
| xl — Ilyll | < lx—yl 
(1.4.19) 
This is colloquially referred to as the "backward triangle inequality." 
Proof. 
This is a consequence of the triangle inequality because 
llxI| = lk -y+yll < |lx-yll+lyll = 
Iixll-llyl < |xk—-yll 
and 
llyll = llx-—y — xl S$ x—yll + [xl =» 
—(lxil—llyl) <llx—yll. 
@ 
Exercises for section 1.4 
2 
1 
1.4.1. Consider the euclidean norm with u = @) ang v= (-:) 
: 
=o 
= 
(a) Determine the distance between u and v. 
(b) Verify that the triangle inequality holds for u and v. 
(c) Verify that the CBS inequality holds for u and v. 
1.4.2. Explain why ||x — y|| = |ly — x||. 

Chapter 1 
The Language of Linear Algebra 
1.4.3. 
1.4.4. 
1.4.5. 
1.4.6. 
1.4.7. 
1.4.8. 
1.4.9. 
Show that (a1 +a2 +++: +Qn)? <n(a?+az+---+a7) for a ER. 
In terms of the euclidean norm, use mathematical notation to describe 
each of the following. 
(a) The solid ball in R" centered at the origin with unit radius. 
(b) A solid ball centered at the point 
c=(€; 
€ 
--- 
&) with 
radius p. 
For x € F" and a € F, prove that properties (1.4.2) and (1.4.3) on 
page 23 are true. 
If x,y 
€ R" such that 
||x—y|| = ||x+y||, what is x7 y? 
yi 
The Parallelogram Identity. An elementary fact from plane geometry is 
that the sum of the squares of the diagonals in a parallelogram is twice 
the sum of the squares of the sides. 
Prove that the parallelogram identity remains valid in F" by showing 
that ||x + yll? + |lx —y||? = 2(||xl|? + lly|l?) for all x,y € F". Along 
with the triangle inequality, this reinforces the notion that euclidean 
geometry in F" is essentially the same as that in R? and R°. 
Verify that the standard inner product is linear in the left-hand argument 
by showing that (x + y|z) = (x|z) + (y|z) for x,y,z € F". 
Equality in CBS. For x,y ¢ F", x # 0, explain why equality holds in 
the CBS inequality if and only if y = ax, where a = x*y/x*x. 
Hint: Use (1.4.16) on page 27. 


32 
Chapter 1 
The Language of Linear Algebra 
1.5 NON-EUCLIDEAN NORMS AND CONVERGENCE 
There are notions of length other than the euclidean measure. For example, 
urban dwellers navigate on a grid of city blocks with one-way streets, so they are 
prone to measure distances in the city not as the crow flies but rather in terms 
of lengths on a directed grid. For example, instead of than saying that "it's a 
one-half-mile straight-line (euclidean) trip from here to there," they are more 
apt to describe the length of the trip by saying, "it's two blocks north on Dan 
Allen Drive, four blocks west on Hillsborough Street, and five blocks south on 
Gorman Street." In other words, the length of the trip is 2+ |—4|+ |—5|=11 
blocks—absolute value is used to insure that southerly and westerly movements 
do not cancel northerly and easterly movements. This "grid norm" as well as the 
euclidean norm are special cases of a more general class of norms defined below. 
ASL Definition. For p> 1, the p-norm of x € F" is defined to be 
ie 
n 
1/p 
l>Ilp = (>: 
a) 
(||x||, is the euclidean norm). 
i 
It can be shown that all p-norms satisfy the same three properties of the 
euclidean norm that are listed in Theorem 1.4.2 on page 23—.e., 
|x|, 20 
and 
|x||,,=0<=—>x=0, 
l|ox'|,, = |@| ||x||, 
for all scalars a, 
(1.5.1) 
IIx+yllp < lxll, +Ilyllp 
The triangle inequality ||x+y||,, < ||x||, + lly||, for a general p-norm is 
also known as Minkowski's inequality, and its proof is developed in Exercise 
1.5.7. The generalized version of the CBS inequality in Theorem 1.4.5 on page 
27 for p-norms is Hodlder's inequality, which states that if 
p> 1 and q>1 
are 
real numbers such that 1/p+1/q=1, then 
Ix"y| < |Ixll, Ilyllg: 
(1.5.2) 
Its proof is developed in Exercise 1.5.5. Other than the euclidean 2-norm, the 
most commonly used p-norms are for 
p= 1 and p= oo. 
1.5.2. Definition. 
For x € F" with coordinates 
x;, the vector one- 
norm is defined to be 
x, 
Dot ae » |z;| 
(the grid norm), 
(1.5.3) 
i=1 
and the vector infinity-norm is defined to be 
i = jm. IIxI, = max |z;| 
(see (1.5.5) below). 
(1.5.4) 

1.5 Non-Euclidean Norms and Convergence 
33 
To prove that limp_,o9 ||x||p = max, |x;| = ||x||,,, relabel the entries of x 
by setting Z; = max; |x;|, and if there are other entries with this same maximal 
magnitude, then label them %,...,%,. Label any remaining coordinates as 
Xp41°'*Xn. Consequently, |£;/%1|< 1 for i=k+1,...,n, so, as p— ov, 
n 
1/p 
IIx||, = be 
ar) = |x, (i 
+ 
i=1 
n 
Zy1 
Pp 
x Tas et 
p\ 
1/p 
) — |Zi|. (1.5.5) 
Example 
3 
If x= (4-0), then ||x||,=9, 
||xll, = V35, and ||x||,, =5. 
1 
Unit Spheres 
To get a feel for the 1-, 2-, and oo-vector norms, it helps to consider the shapes 
and relative sizes of the unit spheres (hollow unit balls) for each norm in R®. 
These are the set of points Sp = {x| ||x||, = 1, p = 1, 2, co} that are a distance 
1 from the origin measured in the 1-, 2-, and oo-norms. As illustrated in Figure 
1.5.1, the respective unit 1-, 2-, and oo-spheres in R® are the surfaces on an 
octahedron, a ball, and a cube. 
It is visually evident that S, fits inside S3, which in turn fits inside Sy. 
This means that ||x||; > ||x||2 > ||x\|o0 for all x € R®. You are asked in Exercise 
1.5.4 to show that these inequalities hold for all vectors x € F". 
S; = UNIT 1-SPHERE 
S2 = UNIT 2-SPHERE 
Soo = UNIT 00-SPHERE 
FIGURE 1.5.1 
General Vector Norms 
Because there are many other legitimate concepts of "length" in F" other 
than the p-norms, a more general definition of "norm" is required to cover all 
cases. Furthermore p-norms are defined in terms of vector coordinates, so when 
more general kinds of vector spaces are encountered in Chapter 4, coordinate- 
dependent norms are of limited value in gauging "length." For these reasons it is 
necessary to have a completely general concept of "norm" that covers all cases, 
including p-norms. It is therefore natural to use the three properties in (1.5.4) 
on page 32 as the basis for a general definition of "norm." 

Explicit reference to specific norms is always made by employing a dis- 
tinguishing subscript * in the notation ||x||, by saying something like ||x||, , 
\|x\|,, or ||x||,,. When the norm notation 
||x|| appears without a subscript, 
its definition should either be inferred from the context or else it should be 
interpreted as being any norm function satisfying the conditions in Definition 
bo. 
General Backward Triangular Inequality 
The same logic that produces the backward triangle ene 
| lll — lly] | < le — yl] 
(1.5.6) 
in (1.4.19) on page 29 can be applied to show that (1.5.6) holds for all vector 
norms. 
Convergence of Sequences 
The concept of convergence of vector sequences is analogous to convergence of 
scalar sequences. 
Two issues in this definition require attention. The first is that no partic- 
ular norm is specified, so it raises the question of whether or not convergence 
depends on the choice of the nome The second question concerns what might 
be called "norm-wise convergence" (as prescribed by Definition 1.5.4) compared 

1.5 Non-Euclidean Norms and Convergence 
35 
to "coordinate-wise convergence" that involves convergence of the individual co- 
ordinates [xz]; of x,. In particular, if x, — x in the sense of Definition 1.5.4, 
must each of n scalar sequences {[x,];} converge, and if so, will they converge 
to x; (the i' coordinate of x)? And conversely, does convergence of each of 
the n scalar sequences [x,]; > v; guarantee that x, — x in the sense of Def- 
inition 1.5.4, regardless of which norm is used? The following theorem provides 
answers to all of these questions. 
155. Rheoceas for al norms on F, 
"the following stotemente are true. 
° lead varies continuously mith ee = i 
Oe 
e For every pair of norms ||x||, and. lal 
: ee necessarily p-norms), 
there are positive constants @ and 2 ( depending only on a norms) 
'such that 
< |x 
= els 
This means that ||x, — x||, > 0 if and only if ice xi, 
0. in 
other words, convergence is not norm dependent. 
ee) 
for all 1 
nonzero vectors in F". 
(1.5.8) 
e 
Let {x,} be a sequence in F", and let the individual coordinates 
of x, be [x;]|;. The sequence converges in norm (as defined in Def- 
Ly 
2 
inition 1.5.4) tox= | . 
if and only if it converges coordinate 
wise. In other words, 
lim xX; = x 
jim 
a [xe]i = =, 
tor every t= 1,2,..-.7n.> 
(1.5.9) 
k- 00 
Proof of (1.5.7). The fact that ||x|] varies continuously with x is a consequence 
of the backward triangle inequality (1.5.6) because if x, > x, then 
< ||xz—x|| 
+0 => 
|[xell > Ill. 
Caution! The converse is not true—i.e., 
||xx|| > ||x|| = 
Xk 
x. 
Proof of (1.5.8). For S, = {y| lly||, = 1}, let # = minyes, 
|ly||, > 0, and 
An important mathematical fact is that if f is a continuous function mapping a set KC F"" 
that is closed (one containing all of its limit points—think of a closed interval as opposed to an 
open interval on the real line) and bounded, then f attains 
a minimum and maximum value 
at points in K. Unit spheres in F" are closed (for much the same reason that intervals 
[a, b] 
on the real line are closed), and certainly unit spheres are bounded. Since every norm on F" 
is continuous, this minimum is guaranteed to exist. 

36 
Chapter 1 
The Language of Linear Algebra 
observe that 
ES, => 
IxIla = [Xl 
> lll, min lly lla = lle 4. 
a 
IP, 
x 
lle 
The same argument shows that there is a v > 0 such that ||x||, > v||x||,, so 
(1.5.8) is produced with a= 
and 6 =1/v. (Specific values for a and 6B for 
some common norms are given in Exercise 1.5.4.) 
Proof of (1.5.9). Assume first that {x,} converges norm-wise as prescribed in 
Definition 1.5.4. Since convergence is not norm dependent, it must be the case 
that ||x;, —x||, + 0. Applying the Cauchy-Schwarz inequality (page 27) yields 
|[xx]i — vs] = Jez (x4 — )| < |leslla l]X* — Xllo = |X — Xllp > 0, 
and hence [xz]; — x; for each i. Conversely, if [x,]; + 2; for each 7, then 
| 
[xx] — x; — 0 so that 
* 
2 
2 
xe —xl|2 = > |fxels — 211? 9 0. 
i=1 
And since (1.5.8) means that convergence in any norm ensures convergence in 
all norms, it follows that ||x, —x||" > 0 for all norms, and thus x, — x by 
Definition 1.5.4. 
I 
Exercises for section 1.5 
2 
1+i 
1.5.1. Find the 1-, 2-, and oo-norms of x = 
2 
andes = 
1 rs 
—2 
4i 
1.5.2. Explain why ||x — y|| = ||y — x|| is true for all norms. 
1.5.3. For x € F", demonstrate that the function defined by ||x||, = p filo 
where p > 0 (i.e., a positive multiple of the euclidean norm) is a vector 
norm that is not a p-norm. 
Note: Non-intuitive things can hold for non-euclidean norms. For ex- 
ample, if e; is the i" unit vector as defined on page 4, then intuition 
suggests that 
|je,;|| = 1 for any legitimate norm. But this is not true 
because |le;||, = 
for the *-norm of this exercise. 

1.5 Non-Euclidean Norms and Convergence 
37 
1.5.4. 
(a) For xe F"™', prove that ||x||, > ||xllp > |Ixll,- 
(b) For x € F"*?, explain why ||x||, < a||x||;, where a is the 
hee 
ay. 
[e-e) 
1 
oe 
Ws 
70 
(i,j)-entry inthe matrix 
2 
[1 
* 
wn]. 
i 
1.5.5. Hoélder's Inequality. ' The classical form of Hélder's inequality states that 
if 
p>1 and q>1 
are real numbers such that 1/p+1/q=1, then 
n 
n 
1/p 
n 
1/q 
SAAS (>: 
a) (>: 
un) 
1 
god: 
east 
Derive this inequality by executing the following steps: 
(a) Use f(t) = (1—A)+At—+# 
and its derivative for 
0 < X < 1 to 
establish the inequality 
Og <a 
= Ale 
for nonnegative real numbers a and £. 
(b) Let 
X=x/||x||, and y =y/|ly||,, and apply the inequality in part (a) 
to obtain 
n 
1 
nm 
1 
nm 
Seals =) lee 
lol? = 1: 
j=1 
PS] 
ja 
(c) Deduce the classical form of Hélder's inequality, and then explain why 
this means that 
Ix"y| < |Ixll, Ilylla: 
1.5.6. Paz's Inequality. Prove that if the components of x € R"™! sum to zero 
(We.wx 
em 0 for et (11,..4,1)), then 
Ix?y] < ||xl|1 (weston) 
for all y € R"™?, 
Hint: Apply Holder's inequality in (1.5.2) (or part c in Exercise 1.5.5). 
Note: Paz's inequality is always as sharp as (and usually sharper than) 
Hélder's inequality because (ymax — Ymin)/2 < max; |y;| = ||y|loo- 
: Ludwig Otto Hélder (1859-1937) was a German mathematician who studied at Gottingen and 
lived in Leipzig. Although he made several contributions to analysis as well as algebra, he is 
primarily known for the development of the inequality that now bears his name. 
Azaria Paz was an Israeli computer scientist at the Technion—Israel Institute of Technology 
who became interested in programming computers to play chess. He introduced the use of this 
inequality in his 1971 book Introduction to Probabilistic Automata, and the inequality has 
sense become a staple in the development of the theory and applications of Markov chains. 

38 
Chapter 1 
The Language of Linear Algebra 
1.5.7. Minkowski's Inequality.' The triangle inequality ||x + y||,, < |x| Pe lly > 
for a p-norm is historically known as the classical Minkowski inequality, 
which states that for p> 1, 
1/p 
1/p 
1/p 
nm 
n 
nm 
Doleit+ul} 
<( doled] 
+ | 
dole? 
i=1 
i=1 
i=1 
Derive this inequality by executing the following steps. 
(a) Prove the inequality for p = 1 by using the triangle inequality for 
scalars. 
(b) For p> 1, let q be the number such that 1/g=1-—1/p. Verify that 
for scalars a and £, 
ja + BP = lat Alla + BIP/4 < [alla 
+ AIP! + |B la + BP', 
and make use of Holder's inequality in Exercise 1.5.5. 
Hermann Minkowski (1864-1909) was the son of a merchant in the Russian village of Alekso- 
tas (now in Kaunas, Lithuania), but he spent most of his life as a mathematician and profes- 
sor in Konigsberg, Ziirich and Géttingen, where he was a close colleague and life-long friend 
of David Hilbert, first at K6énigsberg and later at Gottingen. Albert Einstein studied under 
Minkowski, and Minkowski was famous for providing a mathematical description in terms of 
four-dimensional space-time geometry (since called "Minkowski spacetime" ) for Einstein's spe- 
cial theory of relativity. In addition to the "Minkowski inequality," his name is attached many 
other mathematical concepts and results (e.g., see pages 161 and 622), and there is also "as- 
teroid 12493 Minkowski." Minkowski tragically died suddenly from a ruptured appendix at the 
relatively young age of 44. 

1.6 Orthogonality 
39 
1.6 ORTHOGONALITY 
The notion of two nonzero vectors being orthogonal (i.e., perpendicular) to each 
other is a visual concept in R? and R®, but in higher dimensions our eyes 
cannot be used to "see" orthogonality. The remedy for this is to express the 
venerable Pythagorean theorem (depicted below in Figure 1.6.1 in its visual form) 
in terms of inner products to generate an abstract definition of orthogonality 
that is consistent with the visual interpretation. All discussions in this section 
are specific to the euclidean 2-norm, so interpret ||x|| to mean ||x||,. 
u+v 
2 
2 
2 
ul v <= lulls + Ilvil2 = llu + vile 
FIGURE 1.6.1; PYTHAGOREAN THEOREM IN R? 
If inner products are used to express norm (or length) via ||x||, = \/(x|x) then 
the Pythagorean theorem in R? as depicted in Figure 1.6.1 can be expressed by 
using the inner product properties on page 25 to write 
ul v <> |lulld + ivi =llu+ vil3 
<=> (ulu) + (v|v) = (a+ vu 
v) = (ulu) + (uly) + (vju) + (v|v) 
eee VO Oe 
In other words, u 1 v if and only if 0 = (u|v). This suggests how orthogonality 
(or perpendicularity) should be defined in more general spaces. 
1.6.1. Definition. 
Two nonzero vectors x,y € F" are defined to be 
orthogonal (or perpendicular) to each other whenever (x|y) = 0, and 
this is denoted by writing x | y. In particular, 
e 
xly —=x''y =0 when F=R, 
e 
x by ==> xy =0 when 
= C. 

Chapter 1 
The Language of Linear Algebra 
1 
4 
For example, vectors x = fe 
and y = e 
in R* are orthogonal because 
= 
—4 
1 
x! y = 0. However, in spite of the fact that uv = 0, the vectors u = (2) 
i 
and v= (0) in C® are not orthogonal because u*v F 0. 
ih 
Pythagorean Theorem in R" 
There is a direct generalization of the Pythagorean theorem in R? as depicted 
in Figure 1.6.1 to R" for n > 3. Proceed in the same manner as described in 
R? by using ||x||, = \/(«|*) together with the inner-product properties on page 
25 to write 
llx+yllp =(x+ylx+y) = (x+ylx) + (x+yly) 
= (x|x) + (y|x) + (xly) + (yly) 
= |[xllp +2 (xly) + lly. 
This proves the Pythagorean theorem in R" as stated below. 
1.6.2. Theorem. 
||x + y||2 = ||x\|3 + |ly||2 if and only if (x/y) =0 (ie., 
if and only if x Ly). 
While the Pythagorean theorem in R" is exactly the same as it is in R', 
things are a bit more complicated in C—see Exercise 5.1.18 on page 656. 
Orthonormal Sets 
The unit vectors {e;,e2} C R? and {e1,€2,e3} C R® are sets that contain 
mutually orthogonal vectors of unit length. There are many such sets, so it is 
convenient to have a name for them. 
1.6.3. Definition. A set B= {uj,U2,...,u,} 
CF", k <n, is called an 
orthonormal set whenever ||u;||, = 1 for each i, and u; 1 uj; for all 
i # j. In other words, 
(u,lu,) = 
1. 
when i=, 
a 
0 wheni #47. 
B is said to be a complete orthonormal set when k = n. All orthonormal 
sets are necessarily linearly independent (Exercise 1.6.4). 

1.6 Orthogonality 
41 
Example (Orthogonal vs Orthonormal Sets) 
i 
1 
—1 
The set B' = fu 
= (-1) 
, Up = (:) 
F la = (=1)} is an orthogonal set 
1 
2 
because the vectors in B' are mutually orthogonal—i.e., (uu; = 0 for every 
tJ), but B' is not an orthonormal set because each vector does not have unit 
length. However, it's easy to convert an orthogonal set not containing a zero 
vector into an orthonormal set by simply normalizing each vector as described 
on page 23. Since |/ui||, = V2, ||uzl|, = V3, and |lus||, = V6, it follows that 
B= {u1/V2, u2/V3, u3/ V6} is a complete orthonormal set in R®. 
Unitary and See Matrices 
ae 64. Definition, © ain Ue _poxn hoe clue. or oe are 
complete orthonormal sets is called a unitary matrix. A real unitary 
matrix P € R""" is said to be an 
1 
orthogonal matriz. ? Below are 
some simple examples 
i 
@) 
0 
. 
0 
1 
re 
0 
. 
. 
* 
e 
The nxn 
matrix 
I, = 
ee 
ae 
tc 
is called an identity matrix of 
Ohieneegsl 
order n, and it is perhaps the simplest example of an orthogonal matrix. 
The terminology "identity matrix" is used because I, is the matrix analog of 
the scalar identity 1. This along with properties of I,, are further explained 
on page 71. 
e 
Any matrix P obtained by permuting the columns (or rows) of I is an 
orthogonal matrix because a permutation cannot destroy orthonormality. 
Such matrices P are called permutation matrices because the respective 
products AP and PA affect permutations of the columns and rows of A 
according to the permutation of I that generates P. Permutation matrices 
are discussed in more detail on page 141. 
e 
The matrix 
1//2 
1/V3 
—-1/V6 
P={| 
-1/V2 
1/V3 -1/V6 
(1.6.1) 
0 
Ls oe VG 
t It is shown in Theorem 1.10.1 on page 92 that U has orthonormal columns if and only if it 
has orthonormal rows, so this definition can be phrased by using only columns or only rows. 
: This terminology is an unfortunate anomaly of history, and care must be taken to distinguish 
between a pair of vectors x and y being "orthogonal" and a matrix P that is "orthogonal." 
Orthogonal matrices are not perpendicular to anything—the terminology is meant to suggest 
that P is real and the columns and rows of P are each complete orthonormal sets. 

| 
| 
; 
j 
; 
~ wee Tac® 
dap 
Y 
_ a 
Angles in R" | 
: 
ey 
elf lfievateliet? be ct Gns Ae nae 
_—, 
. 
Now that right angles 
in higher dimensions 
make sense, consider 
how other angles 
__ 
can be defined. Proceed just as before, but use the law of cosines rather than the 
Pythagorean theorem. In reference to the triangle in Figure 1.6.2 below, the law 
of cosines in R? and R° says that ju —v||3 = |ju|3 + ||v|3—2 lulls |Ivllp cos. 
If u and v are orthogonal, then this is the Pythagorean theorem. 
FIGuRE 1.6.2: LAw oF cosines in R? 
Consequently, 
2 
2 
cos) = (tulle + Ilvllz = lu-vilp _ u?u+v7v —(u-v)"(u—v) 
2|lulls |Ivilo 
2 |lulls IIvl2 
oul vy 
uv 
~ 2lfulle Ivile 
lull Ive' 
This also makes sense for x, y € R" because the Cauchy—Schwarz inequality 
from Theorem 1.4.5 ensures that |x7y|/ |x|, llyllz <1 so that there is a unique 
value @ in [0, 7] such that cos@ = x7 y/||x|\, |ly||,. 
a 
[ —

—4 
1 
For example, to determine the angle between x = 
a and y = 
: : 
2 
2 
compute cos@ = 2/(5)(3) = 2/15, and use the inverse cosine function to con- 
clude that @ = 1.437 radians (rounded). Just as in R? and R?®, the cosine is 
fundamental to applications that are naturally set in R". This is illustrated in 
the following example involving linear correlation. 
Example (Linear Correlation) 
The cosine of the angle between two vectors x,y € R" is important in determin- 
ing the degree to which data in x is linearly related to data in y. For example, 
consider a business that desires to know to what degree its sales X (a random 
variable) are linearly related to the taxes Y (another random variable) that it 
pays. There may not be an exact relationship because sales do not necessarily 
translate into profits on which taxes are paid, and taxes may depend on other 
factors such as depreciation of equipment, inventory, etc. Suppose that 
£1 
Y1 
x2 
y2 
x 
Nite 
Ao dh 
Lm 
Ym 
contain sample observations (sometimes just called samples or realizations) of the 
random variables X and Y. In other words, x; and y; might be the respective 
sales and taxes that the business had paid 7 years ago. Determining the extent 
to which the y;'s are linearly related to the 2; 's is equivalent to measuring how 
close each y; is to Box;+1 for some constants 89 and /; that do not depend 
on i. In other words, the problem can be stated as follows. 
Problem: How close is y toa linear combination §9x+ 
e, where e= | - 
|? 
1 
Solution: The cosine as defined in (1.6.3) does the job. To see why, let V be 
a random variable whose mean (or expected value) is wy = E[V] and whose 

44 
F 
Chapter 1 
The Language of Linear Algebra 
standard deviation and variance are 
ov =VE[(V—py)?] 
and 
of =E[(V - pv)'| 
(assumed to be nonzero). 
Similarly, for a realization v € R™*! of m samples of V, the respective sample 
mean, standard deviation, and variance of the sample data in v are 
= ya Ui ae ev 
git 
il 7 [ly )? toe I|v re Hella 
ee 
ee ie 
mos 
athe 
(1.6.4) 
2 
eats ACE = ies = lv — Hvella 
em —t 
m—-1 
Raw data from different sources may be difficult to compare when the units 
of measure or scale are different—e.g., one researcher may use the metric sys- 
tem while another uses American units. To compensate, data is frequently first 
standardized into unitless quantities. The standardization of a random variable 
V and a realization v of V are respectively defined to be 
= Aba 
and: 
2y = 
Ov 
Sy 
Entries in zy are often referred to as standard scores or z-scores. It is straight- 
forward (Exercise 1.6.9) to show that wz, =0, oz, =1, and 
|Zvllo=Vm—1, 
be, =0, 
and 
s,, =1. 
(1.6.6) 
Vv — five 
Ze 
(1.6.5) 
1.6.6. Theorem. Let z, and zy be the respective standardizations of 
samples (or realizations) x,y € R™ of two random variables X and Y 
as defined in (1.6.5), where sx and sy are each nonzero. 
e 
Z, = Zy if and only if there exist constants Bo and 6, such that 
y = Box + Bie, where fo > 0. 
(1.6.7) 
¢ 
Z, = —Zy if and only if there exist constants 69 and §; such that 
y = Box + Sie, where fo < 0. 
(1.6.8) 
In other words, 
y = $ox + B,e for some fo, 
8; € R if and only if 
Zx = +Zy, in which case y is said to be perfectly linearly correlated 
with x. 
For a discrete random variable V whose range (all possible values) is {v1,035..;0n} 
and 
whose mean is p, the population standard deviation is o = VDC? —p)*/n, while the 
sample standard deviation (itself a random variable) for m samples {21,22,... tm} of V 
is defined to be 
s = 
oe ee — Ux)?/(m—1). The reason for using m—1 
instead of m 
in s is because if m is used, then the result is not an unbiased estimator for o 
(it tends 
to underestimate 
a). However, these subtleties do not matter in the context of correlation 
because regardless of whether m or m—1 
is used, the square-root term in (1.6.4) divides out 
when forming the sample correlation coefficient in (1.6.10). 

1.6 Orthogonality 
45 
Proof of (1.6.7). 
First suppose that zx = zy so that 
s 
8 
8 
y ==> (x—pye) + ye = 
x + (15 
= ~ 9) 
e = fox 
+ fre, 
where {o = Sy/sx >0 and 6; = py — Boux. Conversely, if y = Box + Bre for 
some (op > 0, then direct calculation shows that Uy = Polx+61 and sy = Bos 
so that 
ee pe 
er gee PDH 1) Or Or eae 
ea 
Sy 
Bosx 
Sx 
Proof of (1.6.8). 
The proof is similar to that of (1.6.7). If z, = —zy, then 
y = Pox + Bie, where Bo = —(sy/sx) <0 and /; = pty — Boux. Conversely, if 
y = ox 
+ Bie for some fo <0, then Hy = Poltx + 61, and 
X — [xell, = 
Bo(X — 
Uxe 
[ot 
=sxe)ly Il 
= 
m1 
vim —1 
because |Go| = —8po (recall (1.4.3) on page 23). In a manner similar to (1.6.9), 
it now follows that zy = —zy. 
& 
Sy 
Since zy varies continuously with v, the existence of a "near" linear re- 
lationship between x and y amounts to saying that zx is "close" to +z, in 
some sense. The fact that 
||zx||. = ||4zy||, = V~m—1 means z and +z, 
differ only in orientation, so a natural measure of how close zyx is to +zy is 
cos@, where @ is the angle between zy and Zy. 
The geometric intuition is easily visualized. "Centering" x and y by sub- 
tracting away their respective means forces them to be orthogonal to e. For 
example, (e|x — xe) = e7x — pxe7e = Mx — Mx = 0. In other words, as 
depicted in the diagram below, 
STANDARDIZATION IN R® 

Below is a summary of the important facts concerning correlation. 
¢ 
Txy =0 if and only if zx and zy are orthogonal, in which case x and y 
are said to be completely uncorrelated. 
e 
|rxy| =1 if and only if there exists a linear relationship y = Sox + 6,e, in 
which case x and y are said to be perfectly correlated. 
> When 6; >0, y is said to be positively correlated with x. 
> 
When 6; <0, y is said to be negatively correlated with x. 
e 
In general, |rxy| measures the degree to which y is linearly related to x. 
More specifically, the degree to which |ry,y| 
1 is a measure of the degree 
to which y © 89x + Bie for some 69 and 6). 
> 
Positive correlation is measured by the degree to which ryy * 1. 
> 
Negative correlation is measured by the degree to which ryy + —1. 
> 
Ifthe data in x and y are plotted in R? as points (a;,y;), then, as 
depicted in the following figure, rxy + 1 indicates that the points lie 
near a straight line with positive slope, while rxy + —1 means that the 
points lie near a line with negative slope, and ryy * 0 means that the 
points do not lie near a straight line. 
niga is also sometimes called the sample Pearson correlation coefficient or the Pearson product- 
moment correlation coefficient. 

1.6 Orthogonality 
Correlati 
47 
Txy ~ 1 
xy © —1 
Txy ~0 
POSITIVE CORRELATION 
NEGATIVE CORRELATION 
NO CORRELATION 
> 
If |rxy| 
1, then the method of least squares (page 481) can be used 
to determine a "best-fitting" straight line. 
on vs. Covariance 
The covariance between two random variables X and Y is defined to be 
Cov|X, Y] = E[(X ry Ux )(Y a py)], 
and it is a measure of how much X and Y change together. The sample co- 
variance of respective realizations x,y € R™*! of X and Y is defined to be 
1 
ct 
ee Clie 
al 
Vaie 
xy 
pny, > 
(ai — Ux) 
(Yi — By) = ieee 
ep) 
(1.6.11) 
m—1¢ 
m-1 
t1 
Using this together with the definitions in (1.6.4) and (1.6.10) yields the conclu- 
sion that sample correlation and sample covariance are related by the equation 
rh riot 
(1.6.12) 
Analogously, the population correlation coefficient of two random variables 
X and Y is defined to be 
Cov[X, Y] 
OX Vee 
(6213) 
Oxoy 
So, regardless of whether the consideration is that of random variables or ob- 
servations of random variables, correlation is simply a normalized version of 
covariance, and both can be used to draw inferences concerning how X and Y 
(or the observations in x and y) are related. But correlation and covariance 
are not equivalent statistical measures because correlation is dimensionless—it 
is simply a number without a unit of measure, while covariance always has a 
unit of measure that is the product of the units for X and Y thus making the 
magnitude of covariance not as easy to interpret as that of correlation. 

48 
Chapter 1 
The Language of Linear Algebra 
However, if x and y (or X and Y) have been standardized to respectively 
become zy and zy (or Zx and Zy) then 1 = 82, = 83, 
=9Zx =7Zy, SO 
Taxty = Saxzy 
Md 
PZxZy = Cov[Zx, Zy]. 
(1.6.14) 
In other words, correlation agrees with covariance when everything has been stan- 
dardized. 
If {x1,X2,...,Xn} C R™ is a set of respective observations realized from a 
set of random variables {X1, X2,...,Xn}, then it is common to extract insight 
by constructing the sample correlation matric R = [r;;|, where rij is the sample 
correlation coefficient between x; and x;. Sometimes the sample covariance 
matriz (also referred to as a variance-covariance matriz) S = [s;;), where sj; 
is the sample covariance between x; and x; is similarly used. By virtue of 
(1.6.12), it follows that the entries of R and S are related by the equation 
Sij 
{~= 
IeGet 
Tig 
818; 
( 6 5) 
where s; = Sx,. However, if all observations x; have been first standardized to 
become zx, , then (1.6.14) ensures that 
S=R. 
An Application Involving Stock Prices 
The actual closing prices for the stocks of three companies, Ci, C2, and C3, 
for a one-month period are respectively given by 
47.25 
68.87 
28.01 
46.32 
68.17 
27.69 
46.81 
67.75 
28.04 
47.09 
67.55 
28.27 
46.86 
66.84 
28.15 
46.20 
65.71 
27.77 
46.05 
66.35 
27.58 
47.29 
67.19 
Pug 
47.10 
68.02 
27.30 
47.01 
67.11 
27.26 
46.79 
67.68 
27.76 
X1 = | 46.99 
|, 
xp= 1] 6749 
1, 
and 
xg = | 27.79 
46.67 
67.32 
27.61 
47.00 
67.38 
27.69 
47.67 
68.87 
27.88 
46.95 
68.75 
27.86 
47.27 
68.51 
28.14 
47.98 
68.98 
28.20 
47.78 
69.06 
27.75 
48.51 
69.46 
27.83 
48.72 
69.64 
27.94 
48.57 
69.42 
27.10 
48.30 
68.09 
27.40 
The sample correlation matrix (to five significant digits) is 
= 
0.79734 
1 
0.1328 
—0.068747 
0.1328 
it 
1 
0.79734 
—0.068747 
( 
) 
(1.6.16) 

1.6 Orthogonality 
49 
The diagonal entries in R are necessarily equal to one (everything is per- 
fectly correlated with itself), so they tell us nothing. However, examining the 
off-diagonal entries indicates that x, and x2 exhibit a rather strong positive 
correlation, whereas x; and x3 are poorly correlated. Similarly, the correlation 
between x2 and x3 is weak. Consequently, the conclusion is that investors seem 
to view companies C; and C2 roughly in the same light in the sense that their 
stock prices move together in a somewhat linear fashion, while the variation in 
the price of C3's stock is not well correlated to that of either C; or Co. 
In case you are interested, C; is a major wireless telecommunication com- 
pany, and Cy is a large supplier of computer chips used in smartphones and 
other devices. On the other hand, company C3 is a utility company that sup- 
plies electricity and natural gas to locations in the middle part of the US. In light 
of this information the correlation results in (1.6.16) make good sense. This ex- 
ample makes the point that just "eyeballing" data is often not sufficient to reveal 
relevant connections. For the sake of comparison, here is the sample covariance 
matrix (to five significant digits) for the observed stock prices. 
61415 
1.0695 
.045266 
.55475 
61415 
 —.016876 
s=( 
ii 
—.016876 
.045266 
.10863 
Exercises for section 1.6 
1.6.1. Using the standard inner product, determine which of the following pairs 
are orthogonal vectors in the indicated space. 
(a) x=(-2) 
and vat) in R°, 
i 
0 
(by 
Stes 
ie 
and 
y= 
ee 
in C?, 
1-i 
1—i 
i 
on Se 
= 
and eV = 
2; 
in R', 
4 
1 
il et 
1-i 
(d) rel i and 
y=(=) 
in C', 
i 
—i 
0 
Y1 
0 
Y2 
(Ct 
ee}. 
eit ey 
in R" 
0 
Un 
1.6.2. Find two vectors of unit norm that are orthogonal to u = tear 

50 
Chapter 1 
The Language of Linear Algebra 
1.6.3. Consider the following set 
L 
1 
—1 
—1 
ul 
zy 
—l 
B= 
a1 > 
0 
|? DS Te 
i 
ape x3 = 
D) 
2 
0 
0 
(a) Verify that B is an orthogonal set. 
(b) Convert B into an orthonormal set. 
1.6.4. Explain why every orthonormal set B= {uj,u2,...,ug} 
CF", k<n, 
must be a linearly independent set. 
1.6.5. Prove that if L is a triangular matrix that is unitary, then L must be 
a diagonal matrix of the form 
elf1 
O 
eee 
0 
) 
Setea 
ee 
L= 
; 
—" 
0 
QO 
«.-- 
@On 
In particular, if L is real, then J,, = +1 for each k. 
1.6.6. Let x,y € R" be nonzero vectors. 
(a) Prove that if ||x||, = |ly||,, then (x+y) 1 (x—y). 
(b) For the standard inner product in R?, draw a picture of this. 
That is, sketch the location of 
x+y and x—y 
for two vectors 
with equal norms. 
1.6.7. Why is the definition for cos @ given in (1.6.3) not good for C"? Explain 
how to define cos@ so that it makes sense in C". 
1.6.8. Pythagorean Theorem in C". The difference between the Pythagorean 
theorem in R" as stated on page 40 and the Pythagorean theorem in 
C" is that while the implication 
x 
Ly = > ||x+ y\l3 = lIx|l3 Sg lly I3 
is valid in C" as well as in R", the converse fails in C" —i.e., 
2 
2 
2 
IIx+yYllo= x+y, =6 
xty 
when x,yeC". 
(a) Provide an example of a pair of vectors x,y € C? that demon- 
strates this. 
(b) Now prove that if |lax + By||3 = |lax||5 + ||@yl|2 for x,y © C" 
and for all scalars a,G € C, then x Ly. 

1.6 Orthogonality 
1.6.9. 
1.6.10. 
1.6.41, 
51 
Show that all standardizations z of a vector x € R™ have the properties 
that ||z||,=/m—1, 
pz =0, and s, =1. 
Let {x1,X2,...,Xn} C R" be a set of completely uncorrelated ob- 
servations realized from a set of random variables {X,, X2,...,Xn}-. 
Describe the sample correlation matrix R and the sample covariance 
matrix S. 
A small company has been in business for four years and has recorded 
annual sales (in tens of thousands of dollars) as follows. 
Sales 
23 
27 
30 
34 
Determine the degree to which the company's sales are correlated with 
its time in business, and then plot the data (year on the horizontal axis 
and sales on the vertical axis) to visualize the degree of a linear trend. 

52 
Chapter 1 
The Language of Linear Algebra 
1.7 LINEARITY AND MATRIX MULTIPLICATION 
Linear algebra was defined on page 1 to be the study of linear functions defined 
on vector spaces, so it is imperative to know exactly what a linear function is. 
In elementary mathematics the term "linear function" refers to functions whose 
graphs are straight lines, but in higher mathematics the meaning of linearity is 
somewhat different. Recall that a function f is a rule for associating each point 
inaset D, the domain of f, to a well-defined point in another set R, the range 
of f. In linear algebra the domain D is always a vector space, and the range R 
is always a subset of a vector space, so expressions such as f : F" > F" mean 
that 
D=F" but R CF". We will eventually want to consider linear functions 
defined on abstract vector spaces, but in the beginning the discussion is limited 
to linear functions defined on F", F"*"", or their subspaces. 
Add. Definition. A function f : F" — F" is said to be linear when 
f(x t+y) = f(x) + fly) 
(1.7.1) 
f (ax) = af (x) 
(1.7.2) 
and 
for every x,y € F" and for all scalars a € F. These two conditions 
may be combined to say that f is a linear function whenever 
f(ax+y) =af(x)+f(y) 
forall x,y eF",aeF. 
{1-73} 
The function f : 
R — R defined by f(x) = az 
is one of the simplest 
linear functions. You should be able to convince yourself that it satisfies the two 
defining conditions (1.7.1) and (1.7.2). The graph of this function is of course a 
straight line through the origin in R?. In fact, the graph of every linear function 
must pass through the origin because f(0) = f(x — x) = f(x) — f(x) = 0. 
However, just because the graph of a function g(x) is a straight line does not 
necessarily mean that g(x) is linear. For example, the function g defined by 
g(x) =ax+ 6 for 6 #0 
is not linear because it fails to satisfy (1.7.1). 
Affine Functions 
While the function g(x) 
= ar+8 with 6 £0 
is not linear, it is almost linear in 
the sense that g(x) is just a linear function that has been translated by a nonzero 
constant 6, and its graph is that of a straight line that has been translated away 
from the origin. Translations of linear functions are called affine functions, and 
their graphs turn out to be affine spaces as defined on page 17. 
For example, the function f : RR? > R defined by f(x1, 22) = 0121 + aon 
is linear, and its graph in R® is a plane through the origin. For 8 4 0, the 
function f (21,22) = a1271+aQ9%24+ 
6 is not linear—it is an affine function—and 

1.7 Linearity and Matrix Multiplication 
53 
its graph in R® is a plane not passing through the origin—it is an affine space 
as depicted in Figure 1.3.1 on page 17. 
By analogy, the graph of the linear function f : R" + R defined by 
F (21, 22,...,@n) = O21 + 22 +++ + AnSn 
is a "flat" surface called a hyperplane in R"*! passing through the origin—it 
is a subspace of R"*!. The graph of the affine function 
f(@1,%2,...,2n) = 0101 + OQT2 +++: +OnIn+B, 
BHO, 
is a flat surface that has been translated away from the origin—it is an affine 
space in R"*t!. While we cannot use our eyes to visualize such surfaces in higher 
dimensions, we can nevertheless deal with them abstractly by means of the defin- 
ing properties (1.7.1) and (1.7.2). 
Examples of Linear Functions 
Below are just a few examples of linear functions that involve familiar concepts. 
e 
Transposition. The function f : F"*' > F!*" defined by f(x) =x? that 
maps a vector to its transpose is linear because it follows from (1.2.2) on 
page 5 that 
T 
f(xty)=(x+y) =x' +y" = f(x) +fly), 
and 
(1.7.4) 
f(ax) = (ax)' = ax™ = af (x). 
Similarly, the function f : F"*" — F"*~™ defined by f(A) = A? that maps 
a matrix to its transpose is linear due to Theorem 1.2.1 on page 9. The same 
cannot be said for the conjugate transpose because if g : C"*" > C"*™ is 
the function defined by g(A) = A*, then g(aA) = (aA)* = a@A* 4 ag(A). 
One might say that g is conjugate linear. 
e 
Differentiation and Integration. Differentiation and integration are re- 
garded as "operators" on the respective spaces of differentiable and integrable 
functions. But more precisely, they are linear operators. In particular, if D 
is the operator D(f) = f' that maps a function f to its derivative f', then 
the familiar properties of differentiation yield 
Dif+g9)=(f+9) =f +9 =D(f)+ Dg), 
and 
D(af) = (af) =af' = aD(f). 
Similarly, the operator I(f) = f fdx that maps a function to its integral is 
linear because 
Hes = fo + g)dx = [te Ms [oa aie 
and 
Maf)= f 
afde=a f 
fax = ots), 

Chapter 1 
The Language of Linear Algebra 
e 
Inner Products. Linearity is hard wired into the definition of the inner 
product given on page 24. However, the issue is clouded by the fact that 
(x|y) = x*y involves two arguments. The precise statement is that (x|y) 
is linear in the right-hand argument, y. In other words, for a fixed vector 
x, the function f, : F" + F defined by fx(y) = (xly) is linear because 
properties (1.4.9) and (1.4.10) on page 25 respectively translate to say that 
fxly +2) = (xly +2) = (xly) + (x|z) = fly) + fe(), 
- 
and 
fx(ay) = (xlay) = a (xly) = afx(y). 
The inner product is not linear in the left-hand argument when F = C but 
rather it is conjugate linear because for a fixed y, (1.4.11) on page 25 says 
that if fy(x) = (x|y), then 
fy (ax) = (ax|y) = @ (x|y) = afy(x) # ofy(x). 
However, if F = R, then (x|y) = (y|x) by (1.4.12), in which case the 
standard inner product on R" is linear in both arguments. 
e 
Systems of Linear Equations. A system of linear equations 
41121 + A272 + 1373 = Yi 
Q21X1 + A22%2 + A23%3 = Yo 
a31XL1 + A32%2 + A33%3 = Y3 
£ 
yields a linear function f : R? > R? by letting x = @ and y = (:: 
), 
x3 
Y3 
and defining 
Ly 
Q112%1 + 41222 + 01323 
Y1 
XH 
ee 
= (Gain + az2%2 #0332 ) 
= (% ) 
=y. 
(1.7.5) 
a312%1 + a32%2 + A33%3 
Y3 
To see that f is linear, let 
Qa11 
a12 
a13 
aj = | Gat), 
ag — | a22)), ag — |) a23 
|, 
a31 
a33 
a33 
and write f as f(x) =a a; + Vga. +273a3 = y, or more compactly, 
3 
if (see 5 Oa, 
j=l 

1.7 Linearity and Matrix Multiplication 
55 
Linearity is established by verifying that (1.7.3) holds by writing 
3 
3 
3 
3 
f(ax+z) id (an; + 2;)a hye (arja; + 2;a;) = y axjaz + y 25a; 
j=l 
jal 
a 
3 
3 
= aS > 
aja; + S48; = af (x) + f(z). 
j=l 
ae 
Are you starting to get the feeling that linearity lurks around nearly ey- 
ery corner? It indeed does, and as developments unfold many more important 
examples of linearity will surface. 
Matrix Multiplication 
_ 
Until a person 
is in the right frame of mind, the issue of deciding how to best 
define a multiplication operation between matrices A and B is not at all trans- 
parent, especially if the definition has to be both natural and useful. Composing 
A and B by multiplying corresponding entries (similar to matrix addition) 
seems natural enough, but it is not particularly useful! It was not until Arthur 
Cayley (featured on page 116) came to a proper frame of mind in 1857 that the 
"correct" way to multiply matrices was discovered. 
Let f : R? > R? and g: R? > R? be two linear functions that are 
respectively defined as 
= 
L1 
e 
a0, = bx 
= 
Ly 
_ | Ary + Bxo 
eles ee tee 
ene 
es Pals lal 
ai 
where the coefficients a,b,c,d,a,3,y,0 are real constants. Functions f and g 
are linear functions of the type discussed in the example on page 54. Consider, 
as Cayley did, composing f with g to create another linear function 
noo) = #(000)) = 4 (Set )= (tana, tb t alas) 
Cayley proposed using the associated arrays of coefficients to represent these 
linear functions. That is, he respectfully represented f, g, and h with 
choke 
© 
menos 
ye} 
, 
faa 
by af 
+ bd 
nai jest 5 ) 
> and Bale ene 
Entry-wise multiplication between two matrices of the same shape is often called Hadamard 
multiplication in honor of the French mathematician Jacques S. Hadamard (page 951). The 
Hadamard product of A and B is usually denoted by AoB, or, alternately, as A x B. 

56 
Chapter 1 
The Language of Linear Algebra 
After making this association it was only natural for Cayley to call H the 
composition (or product) of F and G, and to write 
b 
a B\_ 
faat+by 
aB+bd\ 
_ 
Fa=(° u fe ipa noes et 
Siebel 
Thus matrix multiplication was born. In other words, the product of two matrices 
represents the composition of the two associated linear functions. By means of 
this observation, Cayley brought to life the subjects of matrix theory and linear 
algebra. 
Extending Cayley's "matrix multiplication" in (1.7.6) to larger matrices is 
facilitated simply by observing that the (7, 7)-entry in H = FG is the row-by- 
column product (from Definition 1.4.4 on page 25) of the i" row of F with the 
j* column in G. That is, 
hit = F14Gui = (a eal 
hy2 = Fi.Gu2 = (a (4), 
har = FoxGui = (€ a)(*), 
hoo = FoxGu2 = (€ a) (5). 
Extending this observation produces the following definition of multiplication 
between more general matrices. 
1.7.2. Definition. 
Matrices A and B are said to be conformable 
for multiplication in the order AB whenever A has exactly as many 
columns as B has rows—i.e., 
A is mx 
p and B is px n. 
e 
The matriz product Am »Bpxn is defined to be the m xn ma- 
trix whose (i, 7)-entry is the row-by-column product of the i*" row 
of A with the j*" column in B. That is, 
P 
[AB] ,, ae Aj, B,,; = aj1b1; am ai2b9; Sea Dippy; = s Gindp;- 
k=1 
When An 
xp, and Byxn are not conformable (ie., when p #4 q), no 
product AB is defined. 
For example, if 
b 
b 
b 
Aa({ 
M1 
%2 
ais 
ise 
11 
b12 
013° 
bag 
~ 
Lao1 
a2 
93 
au 
=| 
621 
baa 
a3. 
bag 
23 J 9 ai 
b31 
632 
633 
34) 
5.4 
feet saat inside ones match ce 
shape of the product 

1.7 Linearity and Matrix Multiplication 
57 
then the product AB exists and has shape 2 x 4. Consider a typical entry of 
this product—say the (2,3)-entry. Entry [AB]23 is the product of the second 
row of A with the third column of B 
Go, 
029 
0o3 
boi 
baa | bog} 
boa 
|, 
( 
G11 
Q12 
443 
bi1 
by: | 
big} 
bia 
b31 
032 | b33 | b34 
sO 
3 
[AB], = Ao«Bu3 = @21b13 + @22b23 + a23b33 = Ne 2K br. 
k=1 
Ltt 
Si 3 
ao 
If A=( a el and B= ( 
yn 
aes | =), 
then AB = ( Sees rh 
2300 s6 
neshes tte 6 
8, Pie omar 
In this case the product AB exists, but the product BA is not defined because 
B is 3x4 and A is 2 x 3 (the inside dimensions don't match in this order). 
Matrix Multiplication Is Not Commutative 
Even when the products AB and BA each exist and have the same shape, 
they need not be equal. For example, 
Neel oodles Wat bes 
crear) ty 
Wik der enatey 
ve d( a 
eke 
The lack of commutivity is a primary difference between scalar algebra and 
matrix algebra. While it is generally true that AB # BA, there are special cases 
when two matrices commute—e.g., consider A = € 
and B= ea ba ih 
Nontrivial Divisors of Zero = > No Cancellation Law 
Another big difference between matrix multiplication and scalar multiplication 
is that while the property 
as=09=— 
o=0 
sor 
£=0 
holds for scalars, it fails for matrices—e.g., for A and B in (1.7.7), AB =0 
but A #0 and B #0. Consequently, the scalar cancellation property 
Op = ay with a A 0 
P= + 
fails for matrices. For example, if A = C ee B= e ab C2 @ 'eb 
then AB = bi oO = AC, but 
B#C 
in spite of the fact that A #0. 

58 
Chapter 1 
The Language of Linear Algebra 
Columns and Rows of a Product 
There are various ways to express the individual rows and columns of a matrix 
product. For example, to extract the jt column of AmxpBpxn, notice that 
AB = [ABwi | 
AB,» | 
pi | ABsn|, sO 
bi; 
b2; 
ABA 
-& A 
Bars 
tAeit 
Ache 
ttobAe 
[AB]; 
j= 
[Ant | Ava] --- | Aan] 
(1.7.8) 
bpj 
= Axibi; a Ax2b25 a OSE Axpdp;- 
In other words, columns of AB are combinations of columns of A. Similarly, 
the rows of AB are combinations of the rows of B. 
A consequence of these observations is that if Xm, and Ym 
xs are ma- 
trices for which each column in Y is a linear combination of the columns of X 
such that 
Youg = Xo1045 + Kygag + °° Kap? 
for 9 = 142,22 72, 
then Y = XQ where Q,xs5 = [a;;]. Similarly if the rows of E,,., are combi- 
nations of the rows of F,y,, such that 
Ese = bak ia.+ Bioko + 
> + 8,830? 
for 4 =1,2,...57, 
then E = PF for Px, = [8;;|. These observations are formally summarized 
in the following theorem. 
1.7.3. Theorem. The columns of a matrix product AB are combinations 
of columns of A, and the rows of AB are combinations of rows of B. 
In particular, if Amp = [aij] and Bpxn = [bj], then 
P. 
[AB]; = ABs = ¥ Ascdey, 
(1.7.9) 
and 
ee 
[AB]. =AupB=) a. By. 
(1.7.10) 
k=1 
Moreover, each of the following is true. 
e 
The columns of Y,,.; are combinations of the columns of X 
if and only if Y = XQ for some Q,x.. 
MmxXT 
e 
The rows of E,.,, are combinations of the rows of Fem if and 
only if E = PF for some P,,y-. 

1.7 Linearity and Matrix Multiplication 
59 
es 
| 
For example, if A = (3 <4 oe and B= (: -7 
2}, then the second 
(a 
column and second row of AB can be respectively computed without computing 
the entire product by writing 
—5 
z= 
Sk 280 
aid 
and 
3 
—-5 
1 
[AB]o. 
= Ao,.B=(3 
-4 
5) (2 7 2) 
= (6 <3 —=5 
): 
U2) 
0 
Linearly Independent Columns 
For a single column x € F™', it follows from (1.7.8) that the matrix-vector 
product Ax is a concise way to represent a linear combination of the columns 
in A because 
ty 
z 
Pp 
x=] 
. 
= AX SS Ann 2 = Ayit1 + Axyote +->>+Axpty. (1.7.11) 
k=1 
Lp 
This in turn provides a convenient way to characterize the columns in A as 
being a linearly independent or linearly dependent set as defined on page 18. 
1.7.4. Theorem. The columns in A € F"*? are linearly independent if 
and only if x = 0 is the only vector in F?*! for which Ax =0. 
e 
Equivalently, the columns in A are linearly dependent if and only 
if there is a column x #0 such that Ax = 0. 
Proof. 
By Definition 1.3.3 (page 18), the columns of A are a linearly indepen- 
dent set if and only if A,12; + A,oto +-+-+ Axp%p = 0 has only the trivial 
(i.e., zero) solution for the x; 's, which by (1.7.11) is equivalent to saying that 
the only column vector x for which Ax = 0 is x = O. The columns in A 
are linearly dependent, by definition, when there is a nontrivial (i.e., nonzero) 
solution for the x; 's, which by (1.7.11) is equivalent to saying that Ax = 0 for 
some 
x #0. 
& 
For example, a quick way to see that the columns in 
1 
i 
= 2 
A= ( 
3. 
—2 = 
= 
52 
are a linearly independent set is to observe that each row sum in A is zero, so if 
e is the vector of all ones, then Ae = 0, and thus Theorem 1.7.4 ensures that the 
columns in A must be linearly independent. The following discussion concerning 
Vandermonde matrices is a deeper example that illustrates how Theorem 1.7.4 
can be put to good use in more theoretical situations. 

60 
Chapter 1 
The Language of Linear Algebra 
Example (Vandermonde Matrices) 
Matrices of the form 
. 
1's, 
st 
oan 
1 
wo! 
a2 
a 
Vmxn =< 
5 
. 
. 
(1.7.12) 
1 
a@m 
«22 
eo? 
in which x; # 2; for all i # j are called Vandermonde' matrices, and the 
columns of V are linearly independent whenever n < m. To see this, invoke 
Theorem 1.7.4 by means of an indirect approach. That is, suppose that 
LS 
ea 
af 
ae 
ao 
0 
peg 
gf 
2. 
ae 
ay 
0 
sla 
4 
=|. 
(1.7.13) 
1 pts 
a2, 
coe 
ant 
An-1 
0 
so that 
Qo + 2j;Q0, + rag Sa 
ee Oi =O 
ior each te 1.2... in. 
This implies that the polynomial 
p(x) = ag + aye + age? + +--+ An—12"1 
has m distinct roots—namely, the 2;'s. However, deg p(x) < n-1 and the 
fundamental theorem of algebra guarantees that if p(x) is not the zero polyno- 
mial, then p(x) can have at most n—1 
distinct roots. Therefore, (1.7.13) holds 
if and only if a; = 0 for all i, and thus Theorem 1.7.4 ensures that the columns 
of V constitute a linearly independent set. 
Note: The value of the determinant of a square Vandermonde matrix is given 
in Exercise 9.3.25 on page 969. 
Alexandre-Theophile Vandermonde (1735-1796) was a French mathematician who made a va- 
riety of contributions to mathematics, but he is perhaps best known for being the first European 
to give a logically complete exposition of the theory of determinants. He is regarded by many as 
being the founder of that theory. The matrix V (and its associated determinant) was named 
after Vandermonde by Henri Lebesgue (1875-1941) (another famous French mathematician), 
but V does not appear in Vandermonde's published work. Vandermonde's first love was mu- 
sic, and he took up mathematics only after he was 35 years old. He advocated the theory that 
all art and music rested upon a general principle that could be expressed mathematically, and 
he claimed that almost anyone could become a composer with the aid of mathematics. 

1.7 Linearity and Matrix Multiplication 
61 
Selecting Individual Columns, Rows, or Entries 
It is frequently convenient to be able to select a particular column, row, or entry 
from a matrix A by means of matrix multiplication. This is accomplished by 
forming the product of A with appropriate unit vectors e;. The i" unit column 
(or row) was defined on page 4 to be the column (or row) whose i'" entry is 
one, and all other entries are zero. That is, 
0 
0 
atl 
ale i* entry 
and 
ef =(0---010---0). 
(1.7.14) 
ti 
' 
i" entry 
0 
It follows from (1.7.9) and (1.7.10) that the respective j*" column, i*" row, and 
the (i,7)-entry of A are given by 
Ae;=A.;, 
ef A=A;,, 
and 
ef Ae;— az. 
(1.7.15) 
Systems of Linear Equations 
Every system of m linear equations in n unknowns 
G42 + A402 + 2** + Ojn Fp = 0, 
G2121 + A22%2 ++-++ Aantn = be 
Ol LY 
Qn2l2 =F 
On Bae Om 
can be written as a single matrix equation Ax = b in which 
aii 
Giz) 
"ain 
Ly 
by 
@21 
422 
*:: 
Qn 
L2 
b2 
A= 
; 
ied 
a 
ae | 
Vg 
lepesieve ll eS 
ers 
(1.7.16) 
QAm1l 
Am2 
**' 
Amn 
rn 
bm 
Naturally, the matrix A containing the coefficients is called the coefficient ma- 
triz for the system. Columns x and b are respectively referred to as the un- 
knowns and the right-hand side for the system. 
Linear systems for which m = n are called square systems, and when 
m#n, asystem is said to be rectangular. For example, the 3 x 4 system 
27, + 329 + 403 + 8x4 = 7, 
32, + 5xq + 6x3 + 2x4 = 6, 
4x71, + 209 + 443 + 924 = 4, 

62 
Chapter 1 
The Language of Linear Algebra 
is rectangular, and it becomes the single matrix equation Ax =b, where 
Cie 
ey ee 
x 
7 
Aoa=(2 Dp 
0) 
2) 
le 
ced sel 
gals 
and 
Dgx1 = {| 6 
}. 
a°o° 40 
"sh 
4 
Writing a linear system as a matrix equation does not provide a computa- 
tional advantage,' but it is an advantage in developing theoretical issues involving 
linear systems. For example, characterizing when a linear system possesses a so- 
lution is facilitated by writing the system in matrix form Ax = b and using 
Theorem 1.7.5 as stated below. 
Consistent Systems of Linear Equations 
An mx n linear system Ax = b is said to be consistent when there exists at 
least one solution for x. Various tests for consistency are presented when the 
issue is addressed on page 201, but the following fact is apparent at this point. 
1.7.5. Theorem. Ax = b represents a consistent system of linear equa- 
tions if and only if b is a linear combination of the columns in A. 
Proof. 
If A is mxn, and if xn x1 satisfies Ax = b, then 
ot 
r2 
b= Axe [Axi Axe +++ 
Asn] 
: 
= Ayj21 + A,ore +---+ Ayntn. 
oF 
Conversely, if b can be expressed as b = et xj;A,;, then Ax=b. 
Bf 
Products of Triangular Matrices 
A nice thing about triangular matrices as defined on page 8 is that their tri- 
angular structure is not destroyed by matrix multiplication. In particular the 
product of any number of upper triangular matrices is again upper triangular, 
and similarly, the product of any number of lower triangular matrices is also 
lower triangular. This observation is incorporated in the following theorem. 
1.7.6. Theorem. 
If A and B are nxn upper (lower) triangular ma- 
trices, then the product AB is also upper (lower) triangular. 
e 
Moreover, [AB];; = a;;bi;. In other words, the i*" diagonal entry 
of a product of two upper (lower) triangular matrices is the product 
of the respective i*" diagonal entries. 
Techniques for solving linear systems are discussed in detail in Chapter 2 on page 119. 

1.7 Linearity and Matrix Multiplication 
63 
Proof. 
If A and B are nxn upper-triangular matrices, then AB is also 
upper triangular because when i > j, 
[AB]ij = AinBuj = (0--- 
O04 --- ain ) oy | =O, 
(L272) 
A similar argument provides the result for lower-triangular matrices. Taking 
Exercises for section 1.7 
1.7.1. Consider the following three geometric transformations in R?. 
f(p) 
' fp) 
ROTATION 
REFLECTION 
PROJECTION 
(a) Explain why each transformation represents a linear function 
f : R* > R', and determine the matrix associated with each of 
these functions. That is, determine the matrix A = (= a) 
such that 
f(p)=Ap, 
where p= ley 
Hint. Recall from analytic geometry that the formulas for ro- 
tating a point (p1,p2) counterclockwise through an angle @ to 
produce (q1,q2) are 
qi = (cos@)p; —(sin@)p2 
and = qe = (sin@)p; + (cos @)po. 
(b) By using matrix multiplication, determine the linear function 
obtained by performing a rotation followed by a reflection. 
(c) By using matrix multiplication, determine the linear function 
obtained by first performing a reflection, then a rotation, and 
finally a projection. 

64 
Chapter 1 
The Language of Linear Algebra 
1.7.2. Each of the following is a function from R? into R?. Determine which 
are linear functions. 
1.7.3. For x € F" and for constants €; € F, verify that 
f(x) = 12 o Egmy rhe 
2 
EnIn 
is a linear function. 
1.7.4. Give examples of at least two different physical principles or laws that 
can be characterized as being linear phenomena. 
Lf 
==2' 
93 
th 
2: 
i 
1.7.5. For: Av= (« —5 1), Be (0 s), and C= (2), compute the 
4 
-3 
8 
Saal 
3 
following products when possible. 
(a) AB, 
(b) BA, 
(c) CB, 
(a) CTB, 
(¢) A2, 
(f) B2, 
(g) 
CPC, 
th) CG 
"f) BB', 
"Gi B'B. 
tec ACI 
f 2a 
32) 
il 
1:7-62) Leta A = ly ice 
eile Eh ae 
| 
. 
aor 2,0 
(a) Express the second column of AB as a linear combination of 
the columns of A. 
(b) Express the second row of AB as a linear combination of the 
rows in B. 
1 
2th 
2 
Deel ete (: ; 0) 
and b= (: 
), 
By using simple inspection, decide 
1 
4 
whether or not the system of linear equations defined by Ax = b is a 
consistent system by using Theorem 1.7.5 on page 62. 

1.7 Linearity and Matrix Multiplication 
65 
1:7.8; 
7.9. 
1.7.10. 
bey de 
eee. 
Consider the following system of equations. 
201 + 
fo + 
3.= 
.38 
474 
aE Din 
MKD) 
221 =i 222 
=D 
(a) Write the system as a matrix equation of the form Ax = b. 
(b) Write the solution of the system as a column s and verify by 
matrix multiplication that s satisfies the equation Ax = b. 
(c) Write b as a linear combination of the columns in A. 
ik 
19)" 
Let E = (0 1 0) 
and let A. be an arbitrary 3 x 3 matrix. 
& 
il 
(a) Describe the rows of EA in terms of the rows of A. 
(b) Describe the columns of AE in terms of the columns of A. 
Suppose that A and B are m xn matrices. If Ax = Bx holds for 
all n x 1 columns x, prove that A = B. Hint: What happens when 
xX =e; is a unit column? 
If A = [a;;(t)] is a matrix whose entries are functions of a variable t, 
the derivative 
of A with respect to ¢ is defined to be the matrix of 
derivatives. That is, dA/dt = [da;;/dt]. Derive the product rule for 
differentiation d(AB)/dt = (dA/dt)B + A(dB/dt). 
Three tanks each containing V gallons of brine are connected as shown 
in Figure 1.7.1. All spigots are opened at once. As fresh water at the rate 
of r gal/sec is pumped into the top of the first tank, r gal/sec leaves 
from the bottom and flows into the next tank, and so on down the line. 
FIGURE 1.7.1 

r 
2 xi a 
M 7 
- 
' 
"widen € O20 toda ieee 
OS tiga! 
6, | 
' 
% 
wif 
al oi AY 
; 
wary 
bi 
agi 
; 
j 
j 
' 
, 

1.8 Elementary Properties of Matrix Multiplication 
67 
1.8 ELEMENTARY PROPERTIES OF MATRIX MULTIPLICATION 
The previous section illustrated some significant differences between scalar and 
matrix algebra—e.g., it was shown on page 57 that matrix multiplication is not 
commutative, and there is no cancellation law. Matrix theory can tolerate the 
lack of these two features, but the situation would be dire if the distributive and 
associative laws were not available. Fortunately, these two properties hold. 
Distributive Properties 
For conformable matrices there is a left-hand and right-hand law for distributing 
matrix multiplication over matrix addition. 
A 8. 
1, Theorem: For conformable 'matrices, 'the following distributive 
_ 
properties hold. 
eo 
° 
A(B+C)= ABE AC (left-hand oe — 
e 
(D+E)F=DF+EF 
(right-hand distributive law). 
Proof. 
The left-hand distributive law is established by showing that correspond- 
ing entries in A(B+C) and AB+AC are equal by writing 
[A(B + C)jij = Aie(B + C)j = S-[A]ix(B + Clay = So[Alix ([Blag + (Clay) 
k 
k 
= = dl 
AJix[Blag + [Alix[Cles) = 5 [Alin [Blay + 5 [Alia[Cle 
k 
k 
= pea + AixCuz = [AB]ij + [AC]; 
Since this is true for each 7 and J, it follows that A(B+C) = AB+AC. The 
proof of the right-hand distributive property is similar. 
& 
Associative Property 
Associativity of multiplication in scalar algbra means that a(8y) = (a8)y. That 
is, the order in which multiplication is performed does not matter. While this 
looks innocent, it is actually a powerful property. In most algebras, associativity 
is more crutial than the commutative property, so if we have to do without one of 
them, it is better to be able to keep associativity. Luckily, matrix multiplication 
is associative. 
1.8.2. Theorem. For all matrices 
Am xp, Bpxq, and Cgxn, 
A(BC) =(AB)C 
(the associative law). 

Chapter 1 
The Language of Linear Algebra 
Proof, 
Recall from (1.7.9) on page 58 that the j* column of BC is a linear 
combination of the columns in B. That-is, 
q 
[BC]. = Buiciy "+ B,2C2; ss icelalle 
o Buglaj => S| Bakery: 
k=1 
Use this along with the left-hand distributive property to write 
q 
q 
[A(BO)]i; = Aiw[BC].j = Aux D> Buncng = >) AiweBarCes 
k=1 
k=1 
qd 
= SUIAB)ixce; = [AB]ixCxj = [(AB)C]ij. 
0 
k=1 
Linearity of Matrix Multiplication 
For a fixed A € F"™*", let f : F"*%? > F"*? be the function f(Xnxp) = AX 
that is defined by matrix multiplication. The left-hand distributive property 
guarantees that f is a linear function because for all scalars a, and for all 
nxXp matrices X and Y, 
f(aX+ Y) = A(aX 4+ Y) = A(aX) + AY = a(AX) + AY 
= af(X) + f(Y). 
The linearity of matrix multiplication is not surprising because it was the con- 
sideration of linear functions that motivated the definition of the matrix product 
at the outset. 
Powers of Square Matrices 
If Anxn #0, then, analogous to scalar algebra, we define A® = jie Positive 
integer powers of A are also defined in the natural way. That is, 
A* = AA:::A. 
—— 
k times 
Note that powers of non-square matrices are never defined due to the lack of 
conformity for multiplication. 
The associative law guarantees that it makes no difference how matrices are 
grouped for powering, so, for example, AA? = A?A, and 
A> AAA 
AA =A AL 
The usual laws of exponents hold—i.e., for nonnegative integers r and s, 
ATA? 2 AT® 
"and a( A) =A 
A discussion of fractional powers is deferred to §4.10 on page 595. 
Just as in the case of scalars, 0° is indeterminate. 

1.8 Elementary Properties of Matrix Multiplication 
69 
Powering Sums 
For nxn matrices A and B, what is (A +B)*?? Be careful! Matrix multi- 
plication is not commutative, so familiar formulas from scalar algebra often do 
not hold for matrices. The distributive properties must be used to write 
(A+B)? =(A+B)(A+B) =(A+B)A+(A+B)B 
———" 
—— 
——=S" 
= A?+BA+AB+B?, 
and this is as far as you can go. The familiar form A? +2AB +B? is obtained 
only in rare cases when AB = BA. To evaluate (A +B)*, the distributive 
rules must be applied repeatedly, and the results are more complicated—try it 
fork = 3. 
Example 
A small airline serves five cities, A, B, C, D, and H, in which H is the "hub 
city." The routes between the cities are depicted in the diagram below, which is 
an example of a directed graph . The five cities are called the nodes of the graph, 
and the directed paths (the routes) between the nodes are called the edges of the 
graph. Matrix powers are natural tools for analyzing such a network. 
A SMALL AIRLINE'S NETWORK 
Traveling from city A to city B requires at least two connecting flights. Flights 
(A > H) and (H — B) 
provide the minimal number of connections. However, 
if space on either of these two flights is not available, then a passenger has to 
take at least three flights. Several questions arise. How many routes from city A 
to city B require exactly three flights? How many routes require no more than 
four flights—and so forth? Since this network is small, these questions can be 
answered by "eyeballing" the diagram, but the eyeball method is not effective in 
larger networks that occur in practice. To see how matrix algebra can be applied, 
create a connectivity matrix C = [c;;| (also known as an adjacency matriz) in 
which 
one { 
at there is a flight from city 7 to city J, 
(1.8.1) 
0 otherwise. 

70 
Chapter 1 
The Language of Linear Algebra 
For our small network, 
C= 
(1.8.2) 
TSQune 
FOOrFO 
BL FrRoOOOL 
rFPoOoOCOrFR),A 
FPOrROCOYD 
Ore 
ee 
& 
Matrix C together with its powers C?, C, C',... provide all of the information 
needed to analyze the network. Entry c;, is the number of direct routes from 
city i to city k, and cj; is the number of direct routes from city k to city 
j, SO CikCej is the number of 2-flight routes from city 7 to city j that have a 
connection through city k. Consequently, the definition of matrix multiplication 
given on page 56 says that the (i,j) -entry in C? = CC is 
5 
[C7]; => CikCkj = the total number of 2-flight routes from city i to city j. 
k=1 
Similarly, the (i,7)-entry in the product C? = CCC is 
5 
[C5 ae 
CikyCk,koCkoj = number of 3-flight routes from city 7 to city J, 
ki,ko=1 
and, in general, 
5 
[Cig = 
ye 
Ciky Chiko *** Ckn—akn—1Ckn—15 
(1.8.3) 
k1,k2,°*+,kn-1=1 
is the total number of n-flight routes from city i to city j. Therefore, the total 
number of routes from city 7 to city j7 that require no more than n flights is 
given by [C]i; a [C7], aie 
OP ae [C" |. = [C ail (i Sp oe 
(eid Pe 
Problem: For our small network, how many routes from A to B require exactly 
four flights, and how many routes from A to B require no more than four 
flights? 
Solution: Use C in (1.8.2), and compute 
1 
2 
Na oy ee 
2S +2 aoe & 
Me fous 
ig ke 
Te 
oe ai 
22. oO 8 ose 
Se Guts 
CO? | 
ee, diet 2 
10) te Ce) pod eC |r en 
Pak eae A 
2 2s 
2 ie 
'Getic 6 
a 
Be ge 
5 
5 5 5 4 
999 
9 2 
and 
41))19. 
<2 
Pa 
16 
(ie 
Pests em thre 
Cu CACO 
Ct St 
err 
tin 
i 
Ores 
1f 
skierid 
beth 
wea 
16 
16 
16 
16 
28 
The number of routes from A to B requiring exactly four flights is given by 
[C*]12 = 7 (try to identify them), and there are [C+ C7 + CFE C412 = 11 
routes from A to B that require no more than four flights. 

1.8 Elementary Properties of Matrix Multiplication 
71 
Reverse Order Law for Matrix Transposition 
The operation of transposition has an interesting effect on a matrix product—a 
reversal of order occurs. 
1.8.3. Theorem. For conformable 
matrices A and B, 
(AB)' =B7AT and (AB)* =B*A* (the reverse order laws). 
Proof. 
By definition, (AB);, = [AB],; = Aj.«B.;. Consider the (i, 7)-entry of 
the matrix B' A? and write 
LEEIN 
= (B'),. (A*) aj os 
[Boe (Aa 
=> [BleilAlje = > [Alje[Blet = Aja Bui. 
k 
k 
Thus (AB);, = [B7A™],, for all ¢ and j, so (AB)' =B7A7. The proof for 
the conjugate transpose case is similar. 
& 
Example 
For A € R™*", the two products A? A and AA? (along with their complex 
counterparts A*A and AA* when A € C™%*") are especially important be- 
cause of their frequent appearance throughout both the theory and application 
of linear algebra. A prominent feature of these matrices is that they are always 
square and symmetric (or hermitian). Note that A7A is n x n while AA" is 
m xm, and they are symmetric because the reverse order law in Theorem 1.8.3 
implies that 
(ATA) =ATAT' =ATA 
and 
(AAT) =AT AT=AAT. 
Similarly, A*A and AA*® are each hermitian. 
Identity Matrix 
For scalars, the number 1 is the identity element for multiplication because it has 
the property that it reproduces whatever it is multiplied by. For matrices, there 
is a multiplicative identity matrix with similar properties. The n x n matrix 
| 
ae 
@ 
WP 
oes 
al 
having ones on the main diagonal and zeros elsewhere, is called the identity 
matriz of order n. Identity matrices have the property that for all A ¢ F'*" ' 
AI,=A 
and 
ImA=A. 

od 
For example, finding the inverse of A = ( 
1 + by brute force requires you 
to find the entries in the matrix A~! = ie 4 such that A~1A =I = AA7!, 
Expanding the right-hand equation 
I= AA™! by direct multiplication yields: 
Li 
O\ 
(ate 
b-pd 
a 
l=a+c 
O=6+d 
a=2. 
6=>—1 
(i HS a+2c 
b+2d 
O=a+2c 
1=b+2d 
c= 
It is straightforward to verify that A7~t = ( 
he a also satisfies the left-hand 
equation A~'A =I. Such a brute force method for calculating an inverse matrix 
is not practical and is not recommended. A better technique for computing A~! 
is given in Theorem 2.3.6 on page 163. 
Example 
irA = = 1) for which 6 = ad— be 
£0, then A is nonsingular and 
a 
1f/d 
—b 
1 — 
AG 
te j (es 2) 
: 
(1.8.4) 
This is verified using direct multiplication to show that AA~! = A-!A =I. 
Expression (1.8.4) is a handy formula for inverting 2 x 2 nonsingular matrices, 
and it is worth remembering. 

1.8 Elementary Properties of Matrix Multiplication 
73 
Note: Perhaps you recall from prior experience that 6 is the determinant of A. 
Formula (1.8.4) is just a hint of more general relationships that are developed in 
the discussion of determinants on page 939. 
Existence and Uniqueness of AW! 
As mentioned above, not all matrices are invertible. For example, the matrix 
ie (; #) 
(1.8.5) 
is a singular matrix because there is no matrix X for which 
I = AX. To see 
why, let X = é Z i and observe that I= AX says 
€ 4 = ree a) = 1= (Yu = (AX =a+7= [AX]o = [Toi = 0, 
which is impossible. Not all matrices are invertible, but when an inverse ezists, 
it is unique because if X,; and Xg are both inverses for a nonsingular matrix 
A, then the defining properties of an inverse given in Definition 1.8.4 together 
with associative property yields 
X, = X,1 = X\ (AX) = (KX, A)X2 = 1X2 = Xo. 
(1.8.6) 
Inversion and Transposition 
In addition to the existence and uniqueness properties mentioned above, there are 
a few other elementary properties of matrix inverses that are direct consequences 
of the definition. The first is the rather obvious fact that 'Cao he = A (make 
sure you can see why), and the second is the fact that inversion and transposition 
are commutative in the following sense. 
1.8.5. Theorem. A, is nonsingular if and only if A? (and A*) is 
nonsingular, in which case 
(A) -(A7y) 
and (AeA) 
Proof. 
This is a consequence of the reverse order law for transposition (Theorem 
1.8.3 on page 71) because AX 
Are eA! Xx 
The proof of the conjugate transpose case is analogous. 
& 
Reverse Order Law for Inversion 
Similar to the reverse order law for transposition (Theorem 1.8.3), there is an 
analogous reverse law for inverting the product of nonsingular matrices. 

Proof. 
Let X =B™'A™" and verify that (AB)X = I= X(AB). For example, 
(AB)X = (AB)B"'A~' = A(BB™')A7* = A(DA™* = AA =L 
A similar argument shows that X(AB) =I, and thus both (1.8.7) and (1.8.8) 
are established. Applying these two properties inductively produces (1.8.9). For 
example, when k = 3, write 
(Ai1A2A3) — 
= (Ai{AzA3}) | 
= {AoA3} 
1 Aj! = AZ Ast AT. 
Continuing in such a fashion produces (1.8.9). 
1 
Matrix Equations 
If Anxn is nonsingular, then the associative property allows the solution for 
X in the matrix equations AnxnXnxp = Bnxp and XpxnAnxn = Bpxn to 
be represented in terms of A~! in the same way as solutions are expressed for 
analogous scalar equations. In particular, if A is nonsingular, then 
X= A7'B 
is the unique solution for X in AX=B 
(1.8.10) 
because A(A~'B) = (AA~')B=IB=B, and X = A7'!B is the only so- 
lution because if AY = B, then multiplication on the left by A7! yields 
Wes Apt exe 
When A is singular or not square, the equations AX =B and XA=B 
may or may not be solvable for X. When solutions exist, they cannot be repre- 
sented in terms of A~! as in (1.8.10), and they need not be unique. 
i
'
> 
'
,
q
(

1.8 Elementary Properties of Matrix Multiplication 
75 
Nonsingular Systems of Linear Equations 
It was shown on page 61 how every system of m linear equations in n unknowns 
can be written as a single matrix equation AmxnXnx1 = Dbmx1. When a system 
is square (i.e., m=n) and A, 
x» is nonsingular, then the associated system of 
equations is called a nonsingular system. The following realization is a special 
case of (1.8.10). 
e 
Every nonsingular system of linear equations has a unique solution. When 
the system is written in matrix form as Ax = b, the unique solution can be 
described as x = A~'b. 
(1.8.11) 
The expression x = A~'b is a convenient theoretical way to represent the solu- 
tion for a nonsingular system, but computing A~! and then forming the product 
with b is a numerically inefficient way to compute the system's solution. It turns 
out that this approach requires roughly three times more computational effort 
than solving the system with straightforward elimination methods. More infor- 
mation about the computational effort required to solve a nonsingular system is 
discussed on page 125. 
Block Multiplication 
Executing multiplication between two matrices by partitioning one or both of 
them into submatrices (a matrix contained within another matrix) is a common 
and useful technique. Submatrices are sometimes called blocks. Suppose that A 
and B are partitioned into submatrices as indicated below. 
Ai: 
Ai2 
--: 
At, 
Bii 
Bio 
-:: 
Bui 
Aoi 
Ago 
-:: 
Aor 
Boi 
Boo 
-:: 
Bat 
= 
; 
é 
' 
: 
ah 
, 
: 
: 
5 
Asi 
As2 
eae 
Asr 
Bri 
Br2 
ERAS 
Brt 
If the block-pairs (A;x,B,;) 
are conformable for multiplication, then A and 
B are said to be conformably partitioned. For such matrices, the product AB 
is formed by combining the blocks exactly the same way as scalars are combined 
in ordinary matrix multiplication. That is, the (2,7)—block in AB is 
AiiBi; + Ai2Boj + +++ + AirB,;. 
For example, consider the partitioned matrices 
Por 
aes 
1 
Sida Ve) abl 
Cl 
0 
ale 
ko 
= 
lana 
aie a saa 
ea | 
Fa 
=(¢ G)) 
fay 
et 
3 4 
where I = (4 '9 and C= (5 ray The product AB is computed with block 
multiplication by writing 
w-($ 3)(8 8)-(¥ &)- 

1 
For example, if c = (0) and d= 
= , then the outer product is 
4 
ie 
1 
Sot Qua 
cd" = (0) 
(2 -1 
2 are 0 
00 0) 
. 
4 
12 -4 8 28 
"a 
In this case, the inner product d?c does not exist because 
¢ and d do not 
' 
have the same number of components. An important feature of outer products 
is that every matrix product can be expressed as a sum of outer products. 
Proof. 
This is just a special case of block multiplication because if B is parti- 
tioned into columns b; and C partitioned into rows c?', then applying block 
multiplication yields 
A =BC = |bi |be, | --- |bp| 
=bicl + beck +---+ bye. 

1.8 Elementary Properties of Matrix Multiplication 
at 
For example, here is a typical outer product decomposition. 
w= BCS 5 He -1) 
Block Triangular Matrices 
Trace 
A square matrix T that can be partitioned as 
A 
B 
4b 
SE 
IIB Be 
ects ( 0 
Cor—r)x(n—r) 
eee 
is said to be block triangular. If the block triangular matrix T in (1.8.12) is the 
coefficient matrix for a system of linear equations Tx = b, and if x and b are 
similarly partitioned as x = ee and b= at then block multiplication 
x2 
shows that Tx = b reduces to the two smaller systems 
Ax; + Bxo = bj 
{ 
ei anit 
(1.8.13) 
If these two systems are consistent, then the solution for x = Gy can be 
obtained by back substitution, which means that Cx2 = bg is first solved for 
X2, which is then substituted "back" into Ax, = b; — Bx, and solved for xj. 
This is an advantage in terms of computational effort. For example, if T is 
100 x 100 while A and C are each 50 x 50, and if standard elimination meth- 
ods are used, then, without taking advantage of the block-triangular structure, 
about 10°/3 flops' (short for "floating-point operations") are required to solve 
Tx = b. But only about (250 x 10%)/3 flops are needed to solve both 50 x 50 
subsystems in (1.8.13). 
1.8.9. Definition. 
The trace of A = [a;;| € F"~" is the sum of its 
diagonal entries. In other words, trace (A) = a1; + a22 +++: + nn. 
A flop is a multiplication together with an addition (ie, a8 +). It is used because many 
matrix computations are dominated by inner product operations. Some authors define a flop 
to be just any single arithmetic operation. You will generally be safe by simply doubling the 
flop estimates in this text to gauge the order of individual operation counts for most matrix 
computations. Flop counts are discussed in more detail on page 233. 

78 
Chapter 1 
The Language of Linear Algebra 
Trace is another example of a linear function because if f(A) = trace (A), 
then for all A,B € F""™"", and for all a € F, 
nm 
nm 
f(aA +B) = trace (aA + B) = S lad + Blu = oS (aaj, + bi) 
i=1 
i=1 
0) 
nm 
n 
n 
= Dadi 
+ Dib = aD an + Dba 
= atrace (A) + trace (B) = af (A) + f(B). 
(1.8.14) 
Among the many interesting properties of the trace is the remarkable fact 
that while matrix multiplication is not commutative, the trace function is one of 
the few cases where the order of the matrices can be changed. 
1.8.10. Theorem. For matrices Amxn and Bnxm, 
| 
| 
- trace(AB) =trace(BA). 
(1.8.15) 
Furthermore, this result can be extended to say that any product of 
conformable matrices can be permuted cyclically without altering the 
trace of the product. For example, 
trace (ABC) = trace (BCA) = trace (CAB). 
(1.8.16) 
But non-cyclical permutations fail—e.g., trace (ABC) ¢ trace (BAC). 
Proof. 
To prove (1.8.15), write 
trace(AB) =) [AB] =) AieBsi= )_ )- indies = SSS beiai 
i 
} 
a 
I 
k 
a 
= Sa Ss. bKidik = SSB ear = S [BA] ke = trace (BA). 
ar 
k 
k 
To establish (1.8.16), use what has just been proven to observe that 
trace ([AB]C) = trace (C[AB}) = trace ([CA]B) = trace (B[CA]). 
Example (Trace of A*A ) 
It is common to encounter expressions of the form trace(A*A) for A € | eesti 
so the following observation will prove to be useful. 
e 
 trace(A*A) = 5), , 
lai? =, ||Aiellg = 0, llAayl2- 
(1.8.17) 

1.8 Elementary Properties of Matrix Multiplication 
79 
The fact that trace(A*A) = >>, ; |ai;|? follows by noting that 
[A*A] jj 
= [A*]jeAny = [Aaj]"Aey = lAeillg = 5— lau? 
1) 
=> trace(A*A) 
=) [A*A];3 = >> Aull = > lag? = dbase. 
al 
A 
a,j 
Exercises for section 1.8 
1.8.1. 
1.8.2. 
1.8.3. 
1.8.4. 
1.8.5. 
1.8.6. 
L827. 
If possible, find the inverse of A = Gi 3 and B= 'e ar and then 
check your answer by direct multiplication. 
Les 
Let C= ( 
2 0) and D = ie aE Write CD as a sum of outer 
—-1 
1 
products, and then verify that your result is correct comparing it with 
what is obtained by direct multiplication. 
Explain why every nonzero row (column) in an outer product cd? isa 
multiple of every other nonzero row (column). 
A square root of A € F"*" is defined to be a matrix B € F"*" such 
that B= A. 
(a) Show that not all square matrices have a square root by consid- 
ering the matrix A = bs at): 
(b) How many different square roots does the 3 x 3 identity matrix 
have? 
(c) How many different square roots does the 2 x 2 zero matrix 
have? 
Explain why the rows of A € F"%*" area 
linearly independent set if and 
only if the only vector x7 € F'*™ for which x7A =O? is x? =07. 
Prove that if S = {uz,U2,.-.,Un} C F™*! 
is linearly independent 
and Pm xm 
is a nonsingular matrix, then {Pu;,Pupg,...,Pu,} is also 
linearly independent. Is this result still true if P is singular? 
If A is nonsingular and symmetric, prove that A~! is symmetric. 

80 
Chapter 1 
The Language of Linear Algebra 
1.8.8. 
1.8.9. 
1.8.10. 
LBL. 
1.8.12. 
Vi Ges 
1.8.14. 
gH Ua 
18:16. 
If A is a square matrix such that 
I— A is nonsingular, prove that 
A(I- A)" = (1-A)*A. 
Prove that if A is mxn and B is 
nxm such that AB =I,, and 
BA =IJ,{jthen m= xk. 
If A, B, and A+B 
are each nonsingular, prove that 
A(A =I B)'B = B(A dl B)-'A pe An ob BY hae 
Is it true that A(I+ BA) = (I+ AB)A forall Anyxn and Brym? 
For Amxn and Byxm, prove that I,, + AB is nonsingular if and only 
if I, + BA is nonsingular, in which case 
(I+ AB)! =I- A(I+BA)"B. 
Hint: What is (I+ AB)(I— A(I+ BA)~'B)? 
Let A,Be Ron and let 
a € F be such that A+ aB is nonsingular. 
Explain why A = (A +aB)~!A and B = (A +aB)~'B commute 
under multiplication even though A and B may not commute. 
For every matrix A, 
x, explain why there is no solution for X,.,, in 
the matrix equation AX — XA =I. 
Prove that if A € R"*" is skew symmetric, then trace(A) = 0. Is the 
same true if A € C"*" is skew hermitian? 
For matrices A;y;, Bsx5,and C,,, such that A and B are nonsin- 
gular, verify that each of the following is true. 
=1 
Note: A generalization of these formulas to include = =) 
is given 
on page 167. 
a) 
(A 
0 
oe 
ET ot 
er 
0 
B 
= 
Coane. 
© (3 8)" A") 

ate - 
Ce. (b) 

82 
Chapter 1 
The Language of Linear Algebra 
1.9 MATRIX INNER PRODUCT AND NORMS 
The standard inner product (x|y) = x*y. for vectors has a natural generalization 
to include matrices, and just as the inner product for vectors is married to the 
euclidean vector norm via ||x||, = 
\/(x|x) = Vx*x, the standard inner product 
for matrices generates an analogous matrix norm. 
1.9.1. Definition. For A,B < F'*", the expression 
(A|B) = trace (A*B) 
is defined to be the standard matrix inner product, and 
|All» = V(ATA) = Virace (A*A) = 
/3_ ais? 
(1.9.1) 
a,j 
is called the Frobenius! matrix norm. Alternately, from (1.8.17), 
Alle = Do lau? = DU Aiella = Do lAsilla = trace (A*A). 
(1.9.2) 
Note: It is straightforward to see that (A|B) satisfies the four inner product 
properties (1.4.8)—(1.4.12) on page 25 (see Exercise 1.9.2). 
The Frobenius matrix norm is just a reinterpretation of the vector 2-norm 
because the entries in any matrix can be strung out to form a long row (or 
column) vector, and computing its euclidean norm is equivalent to computing 
the Frobenius matrix norm. For example, by stringing out A = 'A me into 
a four-component vector, the euclidean norm on R* can be applied to write 
1/2 
Alle = [2? + (—1)? + (—4)? + (-2)?]'"" 
=5. 
CBS Inequality for Matrices 
The Cauchy-Schwarz (or CBS) inequality for vectors says | (x|y)| < ||x||, |ly|l. 
(page 27). This generalizes to matrices with the Frobenius norm as follows. 
1.9.2. Theorem. 
(CBS Inequality for Matrices) For all A,B ¢ F™*", 
and for the standard inner product for matrices in Definition 1.9.1, 
|(A|B)| < Ally IIBilp 
t 
sels 
: 
: 
: 
This is named in honor of Ferdinand Georg Frobenius (1849-1917). See page 798 for his photo 
and small biography. 

1.9 Matrix Inner Product and Norms 
83 
Proof. 
If C = A*B, then cj; = [A*]j.Bsj = (Asj)*B.j, and the CBS in- 
equality for vectors implies that |c;;| < |/A.j|l, ||B.,||,. This together with the 
triangle inequality and the classic Cauchy—Schwarz inequality (1.4.17) on page 
27 yield 
({A(B} |? = [trace (A*B)|' = de eS (Siesl) 
< (Slots Bolk) <(C4vl2)(TB.18) 
=|Al7 (BIZ. a 
More General Matrix Norms 
Because it is an extension of the euclidean vector norm, the Frobenius matrix 
norm is well suited for many applications, but not for all, so alternatives need to 
be explored. Before developing recipes for other matrix norms, it makes sense to 
formulate a definition of a general matrix norm by starting with the properties on 
page 34 for a general vector norm, and then ask what might be added to account 
for matrix multiplication because vector norm properties alone have nothing to 
do with a multiplicative operation. In other words, an additional property that 
relates ||AB|| to ||A|| and ||B|| is needed. The Frobenius norm indicates what 
such a property might be. The vector version of the CBS inequality insures that 
2 
2 
2 
Axa = >- |Asexl? < S> Aselld alla = AMF Ilxlle 
a 
4 
=> 
||Axll2 < ||Allr Ixllo- 
(1.9.3) 
The inequality in (1.9.3) is expressed by saying that the Frobenius matrix norm 
||*||,» and the euclidean vector norm 
||x||, are compatible. This compatibility 
condition along with the characterizations in (1.9.2) produce the conclusion that 
for all conformable matrices A and B, 
ABI. = So MAB) eal = D0 IABeslla SD IAlle Beall 
= |All [Bealls =lAlle [Ble > 
lIABlly < Alle IiBlle- 
This suggests that the submultiplicative property ||AB|| < ||A|| ||B|| is an addi- 
tional property that should be added to the definition of a general vector norm 
to make the idea applicable to matrices. 

Because the first three properties that define a general matrix norm are 
taken from the three properties on page 34 that define a general vector norm, 
matrix norms generally inherit the properties of a vector norm—an outstanding 
example is the backward triangle inequality 
[|All — Bll] < |A-BI. 
(1.9.5) 
Matrix—Vector Norm Compatibility 
<< 
It was observed in (1.9.3) that the Frobenius matrix norm is compatible with the 
euclidean vector norm in the sense that ||Ax||, < ||A||, ||x||,. This concept of 
norm compatibility is formalized and expanded upon below. 
Example (Lack of Norm Compatibility) 
Norm compatibility cannot be taken for granted. For example, the Frobenius 
matrix norm is not compatible with the vector oo-norm because if 
ee er mn a ar then 
||Al|p=5 
and 
||xi|,=1 
so that 
Axll0 = |](8) || =6 £ Alle ltl], =5 x 1=5. 
This example shows that there is a need for a way to generate (or "induce" ) a 
matrix norm from a given vector norm in such a way that the induced matrix 
norm is automatically compatible with the underlying vector norm. 
Induced Matrix Norms 
The concept of an induced matrix norm as described below is fundamental to 
applications where the Frobenius norm may either not be applicable or may not 
be the best choice for gauging the "magnitude" of a matrix. 

Proof. 
The fact that the expression in (1.9.6) satisfies the first three norm condi- 
tions in Definition 1.9.3 is a straightforward consequence of the fact that the un- 
derlying vector norm possesses these properties. To prove the compatibility con- 
— 
dition ||Ax|| < ||Alj ||x|], assume x 4 0 (otherwise there is nothing to prove), 
and let y = x/||x|| so that |ly|] = 1. Use ||Ay|| = ||A (x/ |[x||) || = | Axl / llx| 
to conclude that 
eae 
| Axl 
|Ayl| 
< 
max Axi =|Al => 
7 
To see that || 
AB|| < ||Alj ||B]|, let xo be a vector such that ||xo|| = 1 and 
|ABxo|| = max |ABx|| = |ABI) 
<All) = 
[Axl] < AT It. 
Applying the matrix-vector norm compatibility feature twice yields 
|| AB|| = || ABxo]| < || Al] |/Bxo]l < ||A_] [BI [lxol] = Al BI. 
To establish (1.9.7), write 
i! 
1 
1 
minjjx\j=1 | Axl] 
ix 
|| Ax| 
~ ¥#0 la 
(Ay) 
|A~*yl| 
Powe eee 
| ACY ee |. (%)| 
y#0 ||A(A~ty)|| 
yz0 
ly] 
y#0 
yl 
= max ||ATx||=||A7||. 
: The footnote on page 35 explains why maximum value in this expression must exist. 
: Some authors alternately refer to induced norms as being operator norms. 

86 
Chapter 1 
The Language of Linear Algebra 
In words, an induced norm ||A|| represents the maximum extent to which 
vectors on the unit sphere are stretched under transformation by A, whereas 
1/||A7!|| measures the extent to which a nonsingular matrix A shrinks vectors 
on the unit sphere. Figure 1.9.1 depicts this situation in R? for the induced 
matrix 2-norm. 
'prey 
|All 
1 
min 
||Ax]|| 
= 
I[x||=1 Ax 
| A-? |} 
FIGURE 1.9.1. TRANSFORMATION OF THE UNIT 2-SPHERE IN R?. 
Since the 1- 2-, and oo-norms are the most used vector norms for F", it 
is of particular interest to determine the matrix norm induced by each of these 
vector norms. 
Induced Matrix 1- and oo-Norms 
Because the euclidean vector norm (the 2-norm) and the Frobenius matrix norm 
in Definition 1.9.1 are both the square root of a sum of squares, intuition might 
suggest that the vector 2-norm should induce the Frobenius matrix norm. But 
this is not the case—something unexpected happens. The description of the 
matrix 2-norm requires tools from Chapter 3, so its development is deferred to 
Theorem 3.5.5 on page 363, and the focus here is on the induced 1- and oo-norms. 
1.9.6. Theorem. 
The matrix norms induced by the vector 
1- and 
oo-norms are as follows. 
e 
|All, 
max || x|| 
max > 
lal 
x= 
(1.9.8) 
= the largest absolute column sum. 
© 
[Allo =, max ||Ax||. = max > lai 
ae 
j 
(1.9.9) 
= the largest absolute row sum. 

1.9 Matrix Inner Product and Norms 
87 
Proof of (1.9.8). 
For all x with ||x||, =1, the scalar triangle inequality yields 
[Axl = 30 tex] = 30] Sasa] <> 
Do 
laul esl = Do (esl 2 laa) 
t 
a 
i) 
4 
4) 
J 
a 
os oo x51) (max > lau!) = max he [aig] - 
Equality is attained when A,,; is the column with largest absolute sum and 
x =e; because |lex||, =1 and ||Aeg||, = || Axal|, = max; >>; lac]. 
Proof of (1.9.9)." 
For allex swith [|x| = 1, 
| Axloo = max| J) 
aija3| <max Y" Jay [aj] < max) lass. 
j 
j 
j 
For real matrices, equality is attained when Ax, is the row with largest absolute 
sum and x is the vector such that 
: 
{ 
1 if agj 2 0, 
, 
|Aksx| = 27; |@nj| = max; )), |acj|, 
7 
: 
ecause 
; 
: 
—1 
if ak & 0, 
|AixX| = | 
'Se Gij;2; < 
|ax;| for 2 = ie 
Thus |/Ax||,, = max;|Aj.x| = max; >7, |aij| = |[Ag«x||,,, with ||x||, = 1. 
For complex matrices in which aj = T4304 and Ax, is the row with largest 
absolute sum, set 2; =e 17 so that ||x||,,=1 and ayj2; =Thj =|ax;|. 
Example 
For A= Fi (a Be the induced matrix norms 
||A||; and |/A||,. are 
|All; =1/V3 + V8/V3 = 2.21 
(the largest absolute column sum) 
and 
PAN = 4/\/3 = 2.31 
(the largest row column sum). 
The Frobenius norm is 
|| 
Alle = 
4/trace (ATA) = V6 = 2.45. 
While 
||Alji, 
||Al|.o, and |/Al|# are not equal, they are nevertheless in the 
same ballpark. The same is true for the induced 2-norm as well because it 
will follow from the discussion on page 363 that 
||/A||z2 = 2. This observation 
holds in general because it can be demonstrated that for all n x n matrices, 
|All; < aij |All;, where ai; is the (,7)-entry in the following matrix 
1 
D 
So 
off 
1 i 
| 
(1.9.10) 
(ee) 
n 
; 
PN 
EN Te) 
eT 
Oe 
A similar relation between the common vector norms appeared in Exercise 1.5.4 
on page 37, and Exercise 3.5.22 on page 380 asks you to use the relationships 
between norms to derive the a;;'s in (1.9.10). 

Proof. 
The proofs of (1.9.11) and (1.9.12) are reformulations of those given for 
Theorem 1.5.5 on page 35. To see that (1.9.13) is true, note that the fourth 
property in the definition of a general matrix norm implies that ||A*|| < ||A||*. 
Consequently, 
||A|| < 1 implies that 
||A*|/ — 0, so (1.9.12) ensures that 
a;;(k) + 0, or equivalently, A' +0. 
& 
Exercises for section 1.9 
1.9.1. Evaluate the Frobenius norm for each of the following matrices. 
i 
Rea 
4-2 
4 
A={ 
3 7?) "Before. 
=o PP 
thy 9 
(1 2) 
(: 
0 :) 
( 
45 
2) 
1.9.2. For A,B ¢F™*", verify that the inner product (A|B) = trace (A*B) 
satisfies the inner product properties (1.4.8)—(1.4.12) on page 25. 
1.9.3. Is ||AB||,, = ||BAl|, for all A,B ¢ F"*"? 
1.9.4. Theorem 1.9.5 ensures that for each vector norm for F", there is always 
a compatible matrix norm for F"*"". Establish the converse—1.e., prove 
that for any given matrix norm for F"*", there exists a compatible 
vector norm for F". 

— 
(aA) +6.) (oA Deroy ace eas Gk 
eine Ix ae Acne 
ps F'*", then 
_— 
ss 
ake 
II(@, A)l] = lol + Aflac 
is a vector norm for V by verifying that the defining three properties in 
Definition 1.5.3 on page 34 hold. 
1.9.7. 
(a) Explain why ||I|| =1 for every induced matrix norm, regardless 
of the underlying vector norm. 
(b) What is ([ncnll? 
1.9.8. Prove that if A €¢ F"*" is nonsingular, then |A-*|| > VnllAllz ; 
1.9.9. Given a nonsingular matrix P € F""", verify that the function defined 
by |All, =||P~1AP||,, is a matrix norm for F""". 
1.9.10. 
(a) Let E; bean nxn diagonal matrix in which the i" diagonal 
entry is one and all other entries are zero. It is obvious to see that 
F<], = ||Eill,, = |Eillp = 1. Use the definition of an induced 
norm in (1.9.6) to show that it is also true that ||E;||, = 1. 
(b) 
Create an example to show that the same is not true for all 
matrix norms on F"*". 
1.9.11. For the 1,2,00, and Frobenius matrix norms, prove that |a;;| < ||A) 
for each entry a;; in A. 

90 
Chapter 1 
The Language of Linear Algebra 
1.9.12. 
1.9.13. 
1.9.14. 
1.9.15. 
1.9.16. 
9.1%, 
1.9.18. 
| eA 
Prove that if B is asubmatrix of A ¢ F"*", then ||B|| < |/A|| for the 
Frobenius norm as well as the 1-matrix norm and the oo-matrix norm. 
Note: The corresponding statement for the matrix 2-norm is established 
in (3.5.18) on page 364. 
Explain why ||Allp = |lA*|le- 
For A,B € F"*", establish each of the following inequalities. 
(a) |trace (B)|? < n [trace (B*B)}. 
(b) |trace (B?)| < trace (B*B). 
(c) |trace ([AB]?)| < trace (A*B?) for hermitian matrices. 
Prove that if A,B ¢ F"*", then 
A*A)+t 
B*B 
trace (A*B) < Se 
Let ||/A||, be the matrix norm induced by a vector norm 
||x||, on F". 
Explain why ||A||, = MAX |x|] <1 \|Ax||, . 
Let ||/Aj|, be the matrix norm induced by a vector norm ||x||, on F". 
(a) Prove that if a € R is the smallest number such that 
| Ax||, <a||x||, 
for all x € F", 
then ||A||, =a. 
(b) Prove that if A is nonsingular, then ||A~! be is the largest 
value of 6 such that ||Ax||, > 6||x||, for all x € F". 
For 
Ac F"*" and € €F, the expression R(€) = (€I— A)~! is called 
the resolvent matrix. For each vector norm 
||x|| on F" and its induced 
matrix norm ||A||, prove that if |€| > ||Aj|, then 
IR ae 
€] — |All 
Give an example of a sequence of square matrices to show that having 
A* -+ 0 is not sufficient to guarantee that |A|| — 0 for all matrix 
norms. 

a 
* 
B 
— 
Ms 

Proof of (1.10.1) <=> (1.10.2). 
Assume that Unxn = (u1 |ug| 
+++ jun) is uni- 
tary. To prove U~! = U*, observe that 
~ 
Q 
1 wheni=j, 
- 
= 
[U Uy = wey = 5 alae cae: UST SSuU%Su-} 
(E66) 
Conversely, if U* = U~!, then the above implications are reversible, and thus 
U is unitary. 
Bf 
Proof of (1.10.4) <=> (1.10.5) <= (1.10.1). 
The columns of a matrix U are 
orthonormal if and only if U*U =I 
(ie., U* = U~!), so UU* =I, which is 
equivalent to saying that rows of U are orthonormal. 
Proof of (1.10. z 
<=> (1.10.3). 
If U is unitary, then U*U =I 
by (1.10.2), so, 
for all x € F"*? 
]ux||2 = (Ux| U3 = (Ux)"Ux = x*U*Ux = x*x = |[x||?_ => 
|luxl| = ||xll2- 
Conversely, suppose that (1.10.3) is true, and let U = (uy |u2|-- - |un). To 
see why U must be unitary, use the fact that if ||ux||, = ||x||, holds for all 
x €F"*! | then it holds for x =e;, so 
|eilly 
= 
lleilly = 
|luillg=1 
for each i. 
Now use the fact that (1.10.3) implies that (Ux|Ux) = (x|x) for all x € F", 
and respectively let 
x =e; +e, and 
x =e;+ie, for 7 
4k to show that 
Re(u;|uxg)=0 
and 
Im (u;|u,x) =0. 
Consequently, (u;|u,) =0 for all 7 
4k, sothe columns in U are orthonormal, 
and thus U is unitary by (1.10.4). 
& 
—

1.10 Unitary and Orthogonal Matrices 
93 
Example 
It is straightforward to verify that the matrices 
1/V2 
1/V¥3 -1/V6 
P= (=u 1/V3 —-1/V6 
and eG) ee ee 
0 
1/V3 
_-2/V6 
from (1.6.1) and (1.6.2) on page 41 respectively satisfy P7P = PP? =I and 
U*U = UU* =I. This is an easy way to verify that P is orthogonal and U is 
unitary. However, notice that U7U 
414 UU', so while P and U are both 
unitary, P is orthogonal but U is not. 
Ae 10. 2. 
Corollary, A product a unitary 
v (0 
orthogonal matrices is 
s also 
oes ie orthogonal). 
Proof. 
If U = U,U2:--U,x, where each U; is unitary, then (1.10.2) ensures 
that UF = tee for each 7, so applying the reverse order laws (Theorems 1.8.3 
and 1.8.6) yields 
UU" = (UUs spo Ue UU = UU, Uy 
= (U,U2:--U,) + =U-!. 
Frobenius Norm is Unitarily Invariant 
1.10.3. Theorem. If U,,.., and Vnxn are unitary, then 
UAV||, = |All, 
forall 
AcF™*". 
(1.10.7) 
More generally, if the columns of Xs. and the rows of Yn 
xz are each 
orthonormal sets, then 
|XAY||,=||All, 
forall 
Ac E™*". 
(1.10.8) 
Property (1.10.7) is summarized by saying that the Frobenius matrix 
norm is a unttarily invariant norm. 
Proof. 
Since (1.10.7) is just a special case of (1.10.8), only the proof of (1.10.8) 
is required. If the columns of Xx, and rows of Ynx~ are each orthonormal 
sets, then the same argument in (1.10.6) shows that X*X =I,, and YY* =I,. 
Use this together with the properties of the trace given in Theorem 1.8.10 (page 
78) and (1.9.2) to write 
\|XAY||?, = trace ((XAY)*(XAY)) = trace (Y*A*X*XAY) 
= trace (Y* 
A* AY) = trace (A* AYY™) = trace (A*A) = Alle - 

Chapter 1 
The Language of Linear Algebra 
Isometries 
The fact that unitary matrices U have the property that |/Ux||. = ||x||, for all 
x € F"™! can be expressed by saying that U is an isometry (a norm preserving 
mapping) on F". The n x n orthogonal matrices represent the linear isometries 
on R", and the n x n complex unitary matrices are the linear isometries on 
C". The term "isometry" has an advantage in that it can be used to treat the 
real and complex cases simultaneously, but for clarity we usually revert back to 
the more cumbersome "orthogonal" and "unitary" terminology when we want to 
emphasize that we are either in a real or perhaps a more general complex space. 
An elementary (but important) class of isometries are rotation operators (or 
rotators). Below is an introduction to the concept of rotation in x. R?, and 
generalizations to R". 
Rotations in R? 
Since orthogonal matrices are isometries, their action on a vector can only be 
to change its orientation—i.e., to somehow rotate the vector. To understand 
rotations in R*, consider a nonzero vector u = (ui, U2) that is rotated coun- 
terclockwise through an angle @ to produce v = (v1, v2), and ask, how are the 
coordinates of v related to the coordinates of u? To answer this question, refer 
to the figure below, and use the fact that 
||u|| = v = ||v||_ together with some 
elementary trigonometry to obtain 
v1 = vcos(¢+ 0) = v(cos@ cos ¢ — sin @sin @), 
v2 = vsin(¢ + 6) = v(sin@ cos ¢ + cos 
@ 
sin ). 
(1.10.9) 
v=(V, V5) 
6 
u=(u,,u,) 
\e 
ROTATION COUNTERCLOCKWISE IN R2, 
Substituting cos@ = u/v and sing = u2/v into (1.10.9) yields 
es 
= (cos @)ui — (sin #)uz 
V1 
cos@ 
—sin®@ 
Ut 
eben 
so. Pe eae 
eet (ink 
(1.10.10) 
In other words, 
v = Pu, where P is the rotation (or rotator) matrix 
cos@ 
—sin@ 
Pate 
ae 
(1.10.11) 
sin 0 
cos @ 

1.10 Unitary and Orthogonal Matrices 
95 
Notice that P is an orthogonal matrix because P'P =I (something would be 
wrong otherwise because P must be an isometry). Furthermore, P? is also a 
rotator, but through an angle —6. In other words, if v = Pu, then u = P'v, 
so the action of P? on v is to rotate v clockwise into u. 
Rotations in R® 
Rotating vectors in R® around any one of the coordinate axes is similar to 
rotations in R?. For example, consider rotation around the z-axis. Suppose 
that v = (v1, v2, v3) is obtained by rotating u = (uj, U2, uz) counterclock- 
wise around the z-axis from the perspective of looking down the z-axis toward 
the origin through an angle 6. Just as before, the goal is to determine the re- 
lationship between the coordinates of u and v. Since the rotation is around 
the z-axis, it is evident from the figure below that the third coordinates are 
unaffected—i.e., v3 = u3. To see how the zxy-coordinates of u and v are re- 
lated, consider the respective projections up = (u1, U2, 0) and vp = (v1, ve, 0) 
of u and v onto the zry-plane. 
u=(U,,U5,U3) 
<7 
ae V=(Vj, 
Vo, V3) 
ae 
+S 
u, = (Uj, Up, 0) 
et 
oC) 
a= 
» Vo, 9) 
ROTATION AROUND THE Z-AXIS 
It is apparent that the problem reduces to rotation in the ry-plane, and this was 
solved in (1.10.10). Combining (1.10.10) with the fact that vg = ug produces 
the equation 
V1 
cosé 
—sin@ 
0 
U4 
Vo | = 
esi 8 
cos@ 
0 
us 
U3 
0 
@ 
U3 
In other words, 
cos@ 
—sin@ 
0 
P=) | sin' 
cos@ 
0 
0 
0 
1 
is the matrix that rotates vectors in R® counterclockwise around the z-axis 
through an angle 0. It is easy to verify that P, is an orthogonal matrix because 
PEEPS =]. Furthermore, ee = Re rotates vectors in the opposite direction 

96 
Chapter 1 
The Language of Linear Algebra 
(i.e., clockwise) around the z-axis. Similar techniques can be used to derive 
orthogonal matrices that rotate vectors around the z-axis or around the y-axis. 
Below is a summary of the three rotators that affect the rotation of a vector 
u € R® counterclockwise around each coordinate axis from the perspective of 
looking down the axis toward the origin through an angle @ by means of a 
multiplication P,u in which P, is as described below. 
Z 
1 
0 
0 
P,={0 
cos? 
—sin@ 
y 
0 sin@ 
cos 8 
\y, - 
x 
ROTATION AROUND THE X-AXIS 
Zz 
cos@ 
0 
sin@é 
9 
Rae 
ewe. 
U 
gv 
' 
—sin@ 
0 cosé 
Ww 
x 
ROTATION AROUND THE Y-AXIS 
Z 
cosé 
—sin@ 
0 
"gio 
Pesan 
cos@ 
0 
y 
0 
0 
1 
X 
ROTATION AROUND THE Z-AXIS 
Note: The minus sign appears above the diagonal in P, and P., but below 
the diagonal in P,. This is not a mistake! See Exercise 1.10.22 on page 111. 
Performing rotations in R® by means of the rotators P,, P,, and P, is funda- 
mental in applications involving robotics and computer graphics. For example, 
suppose that a solid as shown in Figure 1.10.1 is to be rotated from the po- 
sition in View (a) to that in View (d). This can be accomplished with three 
counterclockwise rotations. First rotate the solid in View (a) 90° around the 
x-axis to the position in View (b). Then rotate the solid in View (b) 45° around 
the y-axis to the position in View (c) and, finally, rotate the solid in View (c) 
60° around the z-axis to end up positioning the solid as shown in View (d). 
—

1.10 Unitary and Orthogonal Matrices 
97 
The process can be followed by watching how the notch in the corner as well as 
the designated vertex v and the lighter shaded face moves through the various 
rotations. 
View (c) 
View (d) 
FiGurRe 1.10.1 
Problem: If the coordinates of each vertex in View (a) are specified, what are 
the coordinates of each vertex in View (d)? 
Solution: If P, is the rotator that maps points in View (a) to corresponding 
points in View (b), and if P, and P, are the respective rotators carrying View 
(b) to View (c) and View (c) to View (d), then 
imeonen0) 
1 a/2 
08 1/2 
1/2 —V3/2 
0 
Pes: 0 "), y= ( 
0 
ie 
ae ) 
p.= (Wr 
1/2 ), 
Om 
0 
—1/V2 
0 1/V2 
0 
oe 
1 
SO 
1 
te 
eri 
%4/6 
P= PoP Pe] ——_ | 3 V8 
= v2 
(1.10.12) 
er age 
=o 
0 
is the orthogonal matrix (recall Corollary 1.10.2) that maps points in View (a) to 
their corresponding images in View (d). For example, focus on the vertex labeled 

Chapter 1 
The Language of Linear Algebra 
v in View (a), and let Va, 
Vo, 
Ve, and vq denote its respective coordinates 
1 
1 
in Views (a), (b), (c), and (d). If va =i peat then Vn =)2Va = (0). 
v3 
v2/2 
Ve= Pyvo = PyP2zVa = 
i 
i 
and 
vg¢@=P.zv.= P,P,P2Va = 
yGla 
. 
0 
More generally, if the coordinates of each of the ten vertices in View (a) are 
placed as columns in a vertex matriz, 
> 
vi 
v2 
vi0 
vi 
«V2 
10 
all 
i 
de 
ge 
+ 
Lye 
ee 
LO 
Z1 
2 
£10 
Va= 
y1 
yo 
-- 
yo 
|» then 
Va =P2PyP2Va = 
tn 
$e 
t10 
Zi 
22 
°** 
210 
21 
22 
210 
is the vertex matrix for the orientation shown in View (d). The wire-frame rep- 
resentation of the solid in View (d) is obtained by identifying pairs of vertices 
(vi,v;) in Vq that have an edge between them and by creating an edge between 
the corresponding vertices (V;,V;) in Vg. Displaying a 3-D polytope such as the 
solid shown in View (a) on a two-dimensional computer monitor is accomplished 
by projecting the vertices onto the yz—plane—see Exercise 1.10.23. 
Rotations in R" 
Rotations in higher dimensions are straightforward generalizations of rotations 
in R°. To build the rotator that rotates vectors in the (i,7)-plane in R", just 
embed the 2 x 2 rotator ( sci an into the (i,7)-position of the n x n 
—sin@ 
cos@ 
identity matrix analogous to what is done on page 96. In other words, define a 
plane rotation matriz to be an orthogonal matrix of the form 
col 7% 
col j 
+ 
+ 
1 
€ 
s 
+— row 1 
1 
Pij = 
om 
(1.10.13) 
cas! 
Cc 
<— row j 
1 
in which c? + s*? = 
1. The entries 
c and 
s are respectively meant to sug- 
a cosine and sine, but designating a rotation angle @ as is done in R? and 
R° 
is not meaningful in higher dimensions—this is also why the placement of 

1.10 Unitary and Orthogonal Matrices 
99 
the minus sign is arbitrarily assigned to be below the diagonal. In fact, the mi- 
nus sign can alternately be attached to one of the c's on the diagonal—e.g., 
c 
see 
s 
. Plane rotation matrices are also called Givens rotations in honor 
Ss 
Cor 
—C 
of Wallace Givens' who pioneered their use in the early days of automatic matrix 
computations (e.g., see the example concerning Givens reduction on page 676). 
Applying P,; to x € R" accomplishes a rotation in the (i, j)-plane in the 
sense that 
CL; + 8X5 
ot 
Cn 
If x; and z; are not both zero, and if c and s are respectively defined by 
x; 
xy 
a 
ps wie Ee 
7 
2 
2 
2 
4/ U5 +25 
qf 25+ £5 
ands 
then 
Ly 
Pi jx = 
: 
. 
(1.10.14) 
Ln 
In other words, any component (the j*" in this case) can be selectively annihi- 
lated by a rotation in the (i,7)-plane without affecting any entry except x; and 
x;. Consequently, plane rotations can be applied to annihilate all components 
below any particular entry in x. For example, to annihilate all entries below the 
J. Wallace Givens, Jr. (1910-1993) graduated from Lynchburg College in 1928, and he com- 
pleted his Ph.D. at Princeton University in 1936. After spending three years at the Institute 
for Advanced Study in Princeton as an assistant of O. Veblen, Givens accepted an appointment 
at Cornell University but later moved to Northwestern University. In addition to his academic 
career, Givens was the Director of the Applied Mathematics Division at Argonne National 
Laboratory and, like his counterpart A. S. Householder (page 104) at Oak Ridge National 
Laboratory, Givens served as an early president of SIAM. 

100 
Chapter 1 
The Language of Linear Algebra 
first position in x, apply a sequence of plane rotations as follows: 
= 
2 
2 
2 
J 2i +23 
Vryt +2, +23 
0 
0 
0 
Pisx = 
sp 
P13P 12x = 
r4 
In 
tn 
(1.10.15) 
III 
0 
0 
Pins: :PisPi2x= |] 0 | =||xIl2e1- 
0 
The product of plane rotations is generally not another plane rotation, but 
such a product is always an orthogonal matrix (Corollary 1.10.2), and hence it 
is an isometry. So, if we are willing to interpret "rotation in R"" as a sequence 
of plane rotations, then we can say that it is always possible to "rotate" each 
nonzero vector onto the first coordinate axis. More generally, the same logic that 
led to (1.10.15) shows that every nonzero vector x € R" can be rotated onto 
the i*" coordinate axis by a similar sequence of n—1 
plane rotations. In other 
words, there is an orthogonal matrix P = Pj, --- Pi i41Pi;-1-::Pii such that 
Px= {xe 
(1.10.16) 
The following example illustrates one of the many ways this feature can be useful. 
Constructing Orthonormal Sets via Rotation 
Orthonormal sets play an important role in the sequel, and it is often necessary 
to construct them by starting with a given vector x; € R" with ||x;||, =1 and 
somehow augment x, with vectors x2,X3,...,Xn 
so that {x1,X2,...,Xn} 
is 
a complete' orthonormal set. One way to do this is to construct an orthogonal 
matrix Q whose first column is Q,; = x;. Then, by virtue of Theorem 1.10.1, 
the set of all columns x; = Q,; for 7 = 1,2,...,n does the job. So, how can 
such a Q be constructed? 
Plane rotations are one clever way to accomplish this. Since ||x;||, = 1, it 
follows from (1.10.15) that an orthogonal matrix P = P),---Pj3P4. 
can be 
built from a sequence of plane rotations so that Px, = e;. This means that 
x; = PTe; = P4, so setting 
Q = P7 yields an orthogonal matrix that serves 
the purpose (the transpose of an orthogonal matrix is again orthogonal). 
For example, to extend 
6 
(1.10.17) 
A complete orthonormal set in R" is taken to mean an orthonormal set containing n vectors. 

1.10 Unitary and Orthogonal Matrices 
101 
to a complete orthonormal set, sequentially annihilate the second and fourth 
components in x; by using (1.10.14) to produce the following results. 
-1/V5 
 2//5 
0 0 
—1 
V5 
Poe 
eens uve oo 
le 
2 
Pehle 
0 
sa 
0 
Om 
100) 
Sle 
S\ 
ip 
0 
me 
Lhe 
-2 
a) 
(5/308 
0) 8/3 
1 V5 
1 
= 
eer 
ieee, 
rie 
ae 
Ore 
Pis(Puxx) = 
igaitdiewneatip ad KE 
peal) 
2a 
a0 
2/3. 
0 0 5/3 
—2 
0 
Therefore, the columns of 
—1/3 
-2/V5 
0 —2/3/5 
Q= (PsP 12)" = PPI, = 
23 cals 
ales 
(1.10.18) 
—2/3 
0 
0 
5/3 
constitute a complete orthonormal set containing the specified vector x}. 
Note: The discussion on page 106 describes another technique that is both 
simple and that works in R" as well as in C". Also see Exercise 1.10.17. 
Projection 
For u € R" with |jul|, = 1, let £ = span{u} be the line determined by u. 
Consider projecting a nonzero x € R" orthogonally onto £. If Pc isthe nxn 
matrix defined as 
P;=uu', 
(1.10.19) 
then as depicted in the diagram below the resulting orthogonal (or perpendicular) 
projection of x onto £ is given by Pex = uu? x = (u|x) u. 
FIGURE 1.10.2: PROJECTION ONTO A LINE 
To see that this must be correct, observe that x = Pcex + (x — Px), and 
this is the resolution of x into orthogonal components—use P4 = P¢ = P70 
verify that Pex | (x—Pcx) in general. Thus Px is the orthogonal projection 
of x onto £, and the length of this projection is 
||Pcxllo = ||(au™)x||, = |]u(u7x)||, = |u7x| [fully = lu? x| = | (ulx) |. (2.10.20) 

102 
Chapter 1 
The Language of Linear Algebra 
Loosely speaking, this means that: 
|u?x| = | (u|x) | provides an intuitive sense 
(1.10.21) 
of how much of x lies in the direction of u. 
a 
The restriction that |/u||, = 1 is not significant because every nonzero vector 
can be normalized. In other words, if you start with a nonzero vector u that is 
not of unit length, simply use u/||ul|, in place of u to yield 
uur 
suu? 
ux 
The concept of a projection onto the line defined by a vector u of unit 
length extends to the notion of a projection onto the hyperplane ut that is 
called the orthogonal complement' of u, which is defined to be the set 
ut = {v € R"| (u|v) = 0} 
consisting of all vectors v that are orthogonal to u. In this case, define the 
orthogonal projector (the projection operator) onto u+ as follows. 
1.10.4. Definition. For u ¢ R" with |u|, = 1, the orthogonal pro- 
jector* onto the orthogonal complement u+ of w is defined to be 
Q=1- nul; 
(1.10.23) 
As shown below, Qx is the orthogonal projection of x € R" onto ut. 
ut 
PROJECTION ONTO ut 
tT 
More general orthogonal complements are defined on page 456 
t 
; 
: 
The matrix Q =I— uu? 
is also called an elementary projector to distinguish it from more 
general orthogonal projectors introduced on page 457. 

1.10 Unitary and Orthogonal Matrices 
103 
To understand this diagram, notice that x = (I— Q)x + Qx is a resolution 
of x into orthogonal components because (I—Q)x = u(u? 
x) = Px is a vector 
on the line £L = span{u}, and Qx = (I— uu")x is in ut because 
(u|Qx) = u? (I — uu?) = u? — jlulgu? =0. 
(1.10.24) 
Due to the inherent orthogonality, the parallelogram law makes it visually evident 
that Qx must be the result of projecting x orthogonally onto ut, and, just 
as in the case of projection onto a line, this is corroborated in R" by showing 
that the Pythagorean theorem (page 40) holds—i.e., 
Qxll3 + I — Q)x|3 = Ixll3- 
Note that (I-— Q)x = uu?x = Px is the orthogonal projection of x onto 
L = span{u} so that the geometry depicted in this diagram is consistent with 
that of the previous diagram illustrating projection onto a line. 
Example 
2 
2 
Determine the orthogonal projection of x = (0) onto L = span {u 
= (= ) 
} 
i 
3 
as well as the orthogonal projection of x onto u+. Because 
|lul|, 4 1, it 
is necessary to replace u by its normalization u/||ul|,. Doing so yields the 
orthogonal projection of x onto £ as 
u'x 
1/4 
Pex 
= ——u= 
—( 
-1.], 
hE 
OD) ( ) 
zl 
so the orthogonal projection of x onto u~ 
is 
uu! 
1 
2 
— 
a 
. 
— 
= 
=x 
— 
—— 
dP 
Qx 
(x | x = (I-—P,;)x=x- 
Px 
5 (3) 
Reflection 
If u 4 O, then the respective projectors Pc and Q = I— Pz in (1.10.19) 
and (1.10.23) are not isometries (Exercise 1.10.14), and hence they cannot be 
orthogonal matrices. Nevertheless, they provide the stepping stones leading to 
the concept of reflections about u+. The associated reflectors (the matrices 
that affect reflections) are isometries that generate another important class of 
orthogonal matrices. 

i 
oie 
|| Ox 
- Rx 
| 
Rx 
FIGuRE 1.10.3: REFLECTION ABOUT ut 
To understand the geometry in this diagram, assume that 
||u|| = 1 so 
that Qx = (I— uu")x is the orthogonal projection of x onto u+. To locate 
Rx = (I— 2uu')x, notice that Q(Rx) = Qx. In other words, Qx is simul- 
taneously the orthogonal projection of x onto u+ as well as the orthogonal 
projection of Rx onto ut. This together with 
lx — Qx||. = |[u(u?x)|| = |u*x| = ||Qx — Rx||, 
The word "elementary" is used to be consistent with the definition of an "elementary matrix" 
given on page 139. Elementary reflectors are also called Householder transformations in honor 
of Alston Householder (1904-1993) who pioneered the use of elementary reflectors for matrix 
computations. 
Although his 1937 Ph.D. dissertation at the University of Chicago concerned the calculus of 
variations, Householder's passion was mathematical biology, and this was the thrust of his 
career until it was derailed by the war effort in 1944. Householder joined the Mathematics 
Division of Oak Ridge National Laboratory in 1946 and became its director in 1948. He stayed 
at Oak Ridge for the remainder of his career where he became a leading figure in numerical 
analysis. Like his counterpart, J. Wallace Givens (page 99) at the Argonne National Laboratory, 
Householder was one of the early presidents of SIAM. 

1.10 Unitary and Orthogonal Matrices 
105 
means that Rx must be the reflection of x about ut. 
It is visually evident from the above diagram that the reflector R = I—2uu7 
is an isometry on R®. The same is true in R" because R is an orthogonal matrix. 
In fact, R is a symmetric orthogonal matrix, so 
R?R=R?=I 
(see Exercise 1.10.12). 
(1.10.25) 
Just as plane rotations can be used to rotate any nonzero x € R" onto 
the i*" coordinate axis as described in (1.10.16) on page 100, reflectors can also 
accomplish the same task. For a given 0 4 x © R", the trick is to find the 
correct u that has the appropriate orthogonal complement ut so that when 
x # e; is reflected about u+ the result is ||x||,e;. To find such a u, reverse 
engineer the situation by observing that 
2 
x — ||x|le; 
Rx = (1-257, ) 
X= |e 
ash 
xe aui=t||xi'e;,) == eS 
ulu 
a 
where a = 2u?x/u?u. This means that u € span (x — ||x||e;). In fact, any 
nonzero vector in span (x — ||x|| e;) will suffice because if u = 6(x—||x|| e;) for 
any 6 #0, then 
uTx = B(\[x||? — ||xl| 22) and u7u = 26?(|[x|° — ||xl|e) — a =5, 
de 
ak 
uu 
ux 
u 
Since any nonzero § will do the job, pick 8 = 1, and set 
u = x—||x||e; to 
build R =I—2uu?/u'u so that Rx = ||x||, e;. It turns out that the opposite 
sign in u can also be used.' That is, if 
u = x = |x| 
e:, 
(1.10.26) 
then ul u = 2(||x||°||x|| 2) and u?x = ||x||7+||x|| 2; sothat u?x/u?u = 1/2. 
Setting R =I—2uu?/uu yields 
ix 
Rx =x —2 (Fa) u=x+u=+'||x||e; 
(note the sign reversal). (1.10.27) 
u?u 
The choice of the sign is irrelevant for theoretical purposes, but it matters for practical numer- 
ical computations. To avoid what is known as "catastrophic cancellation" (see page 235) when 
using floating-point arithmetic, set u = x + sign(2;) ||x||. e:. 

106 
Chapter 1 
The Language of Linear Algebra 
Complex Reflection 
When x has complex entries, simply replacing transpose by conjugate trans- 
pose and using (1.10.26) to build a complex reflector R = I — 2uu* /u*u that 
has property (1.10.27) does not always work without a slight modification. For 
x€C"! and agiven 1<i<n, set 
u= { ee 
ae tes 
and 
u=xFp||x\e; 
(1.10.28) 
z;/|x;| 
if x; is not real 
to build the complex reflector R. = I—2uu*/u*u. It is straightforward to verify 
that u*x/u*u = 1/2, so the complex analog of (1.10.27) is 
Fis cE pF || xhre:. 
(1.10.29) 
Notice that R* 
=R =R7™!, so R is unitary, hermitian, and R? =I. 
Constructing Orthonormal Sets via Reflection 
The discussion on page 100 shows how to use plane rotations to extend a given 
vector x € R" with ||x||, = 1 to a complete orthonormal set in R" by building 
an orthogonal matrix Q whose first column is Q,; = x. A reflector can be 
used in a similar fashion. Setting 7 = 1 and using the minus sign in (1.10.26) 
produces a reflector R such that Rx = e1, and since R~! = R by (1.10.25), 
it follows that x = Re; = R,,. Thus x is the first column in R, and since R 
is an orthogonal matrix, its columns provide a complete orthonormal set in R" 
that contains x. 
—1 
' 
1 
For example, if x = 3 
: , then ||x||, =1. To extend x to a complete 
—2 
—A4 
: 
i 
orthonormal set in R+, set 
u=x—e, = 3 
and compute 
7) 
2 
ao) = 
7 
¥ 
uu 
L | eke 0Gs 
—2 
1 
0 
2 
The columns of R are an orthonormal set in R* that contains x. 
Comparing the columns of R_ in (1.10.30) with those of the orthogonal 
matrix Q in (1.10.18) on page 101 shows that while the first columns agree 
(they must), the orthonormal set produced by the columns in R_ is different 
from the orthonormal set generated by the columns of Q. This underscores the 
point that there is not just one complete orthonormal set in R". 
If the first entry of a given vector x € C" of unit norm is complex, then 
extending x to a complete orthonormal set in C" by constructing a complex 
reflector requires altering the above procedure by a small amount. Using (1.10.28) 

1.10 Unitary and Orthogonal Matrices 
107 
with the minus sign produces a complex reflector R such that Rx = pe;, so 
multiplying this on the left by R and using the fact that R? =I yields 
x = (uR)e; = Ue; = Ux1, 
where 
U = (uR). 
(1.10.31) 
The matrix U is unitary because Zy=1 and 
U*U = 7R*yR = R? =1 
(recall (1.10.2) on page 92). 
Therefore, the columns of U = wR are a complete orthonormal set in C" 
that contains the given vector x. This comes in handy when developing Schur's 
triangularization theorem on page 306. 
Exercises for section 1.10 
1.10.1. Determine which of the following matrices are isometries. 
WV 
ai 
9-0/5) 
0 
tac) 
et 
(a) 
i/Vo 
V6 
—2/4/6 
(b) (: 0 -1) 
1/V3 
1/V3 
1/V3 
Oe: 
0 
elf 1 
0 
ove 
0 
Ord 
ee 
Se 
oe 
Le 
(c) 
10s 
0.0 
(d) 
Om 
Om Cnet 
a. 
aes 
Ome 
i 
Meee 
OS 
1.10.2. Let U and V be two nxn unitary (orthogonal) matrices. 
(a) Explain why the product UV must be unitary (orthogonal). 
(b) Explain why the sum U+V 
need not be unitary (orthogonal). 
(c) Explain why ( 
aie Vv - ) 
must be unitary (orthogonal). 
1.10.3. 
(a) 
Explain why the standard inner product is invariant under a 
unitary transformation. That is, if U is any unitary matrix, 
and f= Ux andy = Uy, then 
UV 
xy 
(b) 
Given any two vectors x,y € R", explain why the angle between 
them is invariant under an orthogonal transformation. That is, 
if u = Px and v = Py, where P is an orthogonal matrix, 
then 
COS Ue = COSUx,. 

108 
Chapter 1 
| 
The Language of Linear Algebra 
1.10.4. Explain why U € C""" is unitary if and only if U* is unitary. Does 
your same argument show that P € R"*" is orthogonal if and only if 
P? is orthogonal? 
1+i 
1+i 
1.10.5. Is 
. Ls 
a unitary matrix? 
v3 
V6 
1.10.6. 
(a) How many 3x 3 matrices are both diagonal and orthogonal? 
(b) How many nxn matrices are both diagonal and orthogonal? 
(c) How many n xn matrices are both diagonal and unitary? 
1.10.7. 
(a) Under what conditions on the real numbers a and £ will 
Sheri 
paa 
as OY: Pies 
be an orthogonal matrix? 
(b) Under what conditions on the real numbers a and £6 will 
0 
@ 
O 
46 
a2 
an 
OF 
16. 
0 
Lies 
0 
16 
OF 
a 
im 
0, 
+a. 
© 
be a unitary matrix? 
1.10.8. The Cayley Transformation. Let A be skew hermitian (or real skew 
symmetric), and assume that (I+A)~! exists (a fact that is established 
in Exercise 2.5.26 on page 218). Prove that 
U=(I-A)(I+ 
A) + =(1+ A)"1(I- A) 
is unitary (or orthogonal). Hint. Recall Exercise 1.8.8 (page 80). There 
is a more direct approach, but it requires the diagonalization theorem 
for normal matrices. More is said about the Cayley transformation in 
the example on page 346. 
1.10.9. Prove that elementary projectors Q = I— uu! with 
|u|] = 1 "are 
singular. 

1.10 Unitary and Orthogonal Matrices 
109 
1.10.10. 
1.10.11. 
1.10.12. 
1.10.13. 
1.10.14. 
1.10.15. 
1.10.16. 
Let B = {uj,ug,...,uz} be an orthonormal set and let Q; be the 
elementary projector Q; =I—u,u/ for i = 1,2,...,k. Show that if 
(pi, P2,---,Pe) is any permutation of (1,2,...,k), then 
Qp, Qn. ++ Qn, = QiQ2- ++ Qe. 
Idempotent Matrices. Matrices that have the property A? = A are said 
to be idempotent. Show that all orthogonal projectors 
Q =I-— uu! as 
defined in (1.10.23) are idempotent as well as being symmetric. 
Note: This is a special case of Theorem 4.3.7 on page 464 that says the 
properties of being symmetric and idempotent completely characterize 
orthogonal projectors in general. 
Involutory Matrices. Square matrices that have the property A? = I 
(i.c., A~1 = A) are said to be involutory. Show that every elementary 
reflector is unitary, hermitian, and involutory. 
1 
Let x= a/3)(-2). 
(a) Determine an elementary reflector R such that Rx lies on the 
x-axis. 
(b) Verify by direct computation that your reflector R is symmet- 
ric, orthogonal, and involutory. 
(c) Extend x to a complete orthonormal set in R® by using an 
elementary reflector. 
It is visually evident in R? and R® that the respective projectors Pr 
and Q =I-—Pg defined in (1.10.19) and (1.10.23) are not isometries. 
Establish this fact more generally for R" by showing that there are 
vectors x € R" such that ||pcx|l, < ||x||, and ||qx||, < ||x||.. 
Let x, y € R"*' be vectors such that. ||x|| = |ly|| but x 4 y. Explain 
how to construct an elementary reflector R such that Rx =y. 
Hint: The vector u that defines R can be determined visually in R° 
by considering the diagram on page 104. 
Let R = I-2uu* € C"™" (n> 1), where 
|lul| = 1. Prove that if 
Rx =x, then x € C"™? is orthogonal to u. 

110 
Chapter 1 
The Language of Linear Algebra 
1.10.17. Alternate Reflector Construction. For a vector Xn 1 with 
||x|| = 1, 
partition x as x= ts : where x is n—1x 1. 
(a) Show that if x € R" with x; 
#1, andif a= (1—2)~!, then 
xl 
Rnaxn = € 
~ r) 
x 
l-—axx 
is an elementary reflector. 
(b) Show that if x 
eC" with |z1| #1, and if 
q 
be 
1 
if x 
is real, 
ee 
Be 
x1/\x1| 
if x, is not real, 
2—% 
ed 
| 
ox 
Raxh ae ( 
x 
p(I Se oe 
is a complex elementary reflector in the sense that 
R=n(1-2 
a 
1— |z4| 
then 
* 
uu ) for some u € C™!?, 
u*u 
The results of this exercise provides another easy way to extend 
a vector x to a complete orthonormal set in either R" or C". 
1.10.18. Use the result of Exercise 1.10.17 to extend the vector x, in (1.10.17) 
to a complete orthonormal set, and verify that your answer is correct. 
Is your result the same as that in (1.10.18)? Should it be? 
1.10.19. Suppose that R and S are elementary reflectors. 
(a) Is & Ht) an elementary reflector? 
(b) Is ; a an elementary reflector? 
(c) Must RS be an elementary reflector if Rnxn 4 Snxn? 
1.10.20. Perform the following sequence of rotations in R? beginning with 
(1) 
1. 
Rotate vo counterclockwise 45° around the z-axis to produce vj. 
2. 
Rotate v; clockwise 90° around the y-axis to produce vo. 
3. 
Rotate v2 counterclockwise 30° around the z-axis to produce v3. 
Determine the coordinates of v3 as well as an orthogonal matrix Q 
such that Qvo = v3. 

1.10 Unitary and Orthogonal Matrices 
Ett 
1.10.21. 
1.10.22. 
1.10.23. 
Suppose that a vector v € R® is first rotated counterclockwise around 
the x-axis through an angle @ and then that vector is rotated counter- 
clockwise around the y-axis through an angle ¢. Is the result the same 
as first rotating v counterclockwise around the y-axis through an angle 
@ followed by a rotation counterclockwise around the z-axis through an 
angle 0? 
Explain why the rotation matrix that rotates a vector counterclockwise 
around the y-axis (from the perspective of looking down the y-axis 
toward the origin) through an angle @ is given by 
cos@ 
0 
sin@ 
Piet 
lego 
oi 
0 
—sin@ 
0 
cosé 
Hint: View the problem from the perspective of the standard coordinate 
system in R® oriented to look like this. 
y 
3-D Computer Graphics. To display and manipulate images of a 
three-dimensional solid on a two-dimensional computer display monitor, 
use a wire-frame representation of the solid consisting of vertices on the 
solid's surface connected by straight line edges obtained in a manner as 
described in the example associated with Figure 1.10.1 on page 96. Let 
the screen represent the yz-plane, and let the x-axis be orthogonal to 
the screen so that it points toward the viewer's eye as shown below in 
Figure 1.10.4. 
FIGURE 1.10.4 

112 
Chapter 1 
The Language of Linear Algebra 
A polytope in the «yz-coordinate system appears to the viewer as the 
orthogonal projection of the polytope onto the yz-plane. This projection 
is easily drawn by setting the x-coordinate of each vertex to 0 (i.e., 
ignore the «-coordinates). Plot the (y, z)-coordinates on the yz-plane 
(the screen), and draw edges between appropriate vertices. For example, 
suppose that the vertices of the polytope in View (a) in Figure 1.10.1 
are numbered as indicated below in Figure 1.10.5. 
x 
3 
FIGURE 1.10.5 
The associated vertex matrix is 
Vi 
V2, 
Vs. 
V4- 0 
V5 
VG 
eV7 5 Ve 
Vous 
vi6 
r 
0 
1 
1 
Oo" 
6 
1 
1 
1 
V=y 
GH 
© 
1 
1 
'oh 
Mae 
eect 
a 
a 
0 
0 
8 
€1 
€2 
©€3 
€4 
€5 
@€6 
C7 
C8 
€9 
€10 
S11 
©€12 
©€13 
©€14 
©€15 
{> de 23 via hora 
aubees — F—7. - og" Se hig 
E=(2 
3 41568106 
7 
8 
9 
9 
w 
5) 
in which the k*" column represents an edge between the indicated pair 
of vertices. The image on the computer's screen (the yz-plane) is dis- 
played by dropping the first row from V and plotting the remaining 
yz-coordinates. Edges are drawn between appropriate vertices as dic- 
tated by the information in the edge matrix E. 
(a) Compute the rotated vertex matrix PV for the rotated poly- 
tope shown in View (d) in Figure 1.10.1. Plot the points in the 
second and third row of PV as yz-coordinates on the monitor 
(or on your paper). Draw edges between appropriate vertices 
as indicated by the edge matrix E to create a two-dimensional 
image. 

1.10 Unitary and Orthogonal Matrices 
113 
(b) Explain how to translate the polytope in Figure 1.10.5 to a 
different point in space such that vertex 1 is at p = (Zo, yo, Zo) 
instead of at the origin. 
(c) How can a polytope (or its image on a computer monitor) be 
scaled? 
1.10.24. Hidden Faces. When using a computer to generate and display three- 
dimensional convex polytopes such as those in Figure 1.10.1 on page 96 
it is desirable to not draw those faces that should be hidden from the 
perspective of a viewer positioned in front of the monitor as shown in 
Figure 1.10.4. The cross product in R® (usually introduced in elemen- 
tary calculus courses) can be used to decide which faces are visible and 
which are not. Recall that if 
Ul 
V1 
U2U3 — UZv2 
u={| 
ue} 
and 
v={| 
v2 
}, then 
ux v= { ugv1 —w103 
|, 
U3 
U3 
U1V2 — U2U1 
and u X v is a vector orthogonal to both u and v. The direction of 
u X v is determined from the right-hand rule illustrated below. 
uxXv 
RIGHT-HAND RULE 
Assume that the origin is interior to the polytope, and consider a partic- 
ular face and three vertices vo, 
vi, and v2 on the face that are posi- 
tioned as shown below in Figure 1.10.6. Vector n = (vi — vo) X (v2—Vo) 
is orthogonal to the face, and it points in the outward direction. 
n= (v1 — Vo) x (V2 — Vo) 
A 
vo 
FIGURE 1.10.6 

a 
(z 
7 
16% 
9 
a> 
'9 
: 
: 
— 
ae ee 
4 
up muti 
i e 
| 
@ 
y tae 
1 é 
ere: 
° 
: 
va? 
f 
4 

1.11 Short History of Matrix Theory 
115 
1.11 
SHORT HISTORY OF MATRIX THEORY 
The earliest recorded analysis of simultaneous equations is found in the ancient 
Chinese book Chiu-chang Suan-shu (Nine Chapters on Arithmetic), estimated 
to have been written some time around 200 B.C. In Chapter VIII of this book 
there appears a problem of the following form. 
Three sheafs of a good crop, two sheafs of a mediocre crop, and 
one sheaf of a bad crop are sold for 39 dou. Two sheafs of 
good, three mediocre, and one bad are sold for 34 dou; and one 
good, two mediocre, and three bad are sold for 26 dou. What is 
the price received for each sheaf of a good crop, each sheaf of a 
mediocre crop, and each sheaf of a bad crop? 
Today this problem would be formulated as three equations in three unknowns 
Oe + 2Y az = 30, 
2x + 3y + z= 34, 
L 
2y + 32 = 26, 
where x, y,and z represent the price for one sheaf of a good, mediocre, and 
bad crop, respectively. The ancient Chinese saw right to the heart of the matter. 
They placed the coefficients (represented by colored bamboo rods) of this system 
in a square array on a "counting board" 
and then manipulated the lines of the 
array according to prescribed rules of thumb. Their counting board techniques 
and rules of thumb found their way to 
Japan and eventually appeared in Europe 
with the colored rods having been replaced by numerals and the counting board 
replaced by pen and paper. 
The ancient Chinese appreciated the advantages of array manipulation in 
dealing with systems of linear equations, and they possessed the seed that might 
have germinated into a genuine theory of matrices. Unfortunately, in the year 213 
B.C., emperor Shih Hoang-ti ordered that "all books be burned and all scholars 
be buried." It is presumed that the emperor wanted all knowledge and written 
records to begin with him and his regime. The edict was carried out, and it will 
never be known how much knowledge was lost. The book Chiu-chang Suan-shu 
(Nine Chapters on Arithmetic) mentioned above was compiled on the basis of 
remnants that survived. 
More than a millennium passed before further progress was documented. 
The Chinese counting board with its colored rods and its applications involving 
array manipulation to solve linear systems eventually found its way to Japan. 
Takakazu Seki Kowa (1642-1708), whom many Japanese consider to be one of 
their greatest mathematicians, carried forward the Chinese principles involving 
"rule of thumb" elimination methods on arrays of numbers similar to Gaussian 
elimination as discussed in §2.1 on page 119. His understanding of the elemen- 
tary operations used in the Chinese elimination process led him to formulate 
the concept of what we now call the determinant. While formulating his ideas 

116 
Chapter 1 
The Language of Linear Algebra 
concerning the solution of linear systems, Seki Kowa anticipated the fundamen- 
tal concepts of array operations that today form the basis for matrix algebra. 
However, there is no evidence that he developed his array operations to actually 
construct an algebra for matrices. 
From the middle 1600s to the middle 1800s, while Europe was flowering 
in mathematical development, the study of array manipulation was exclusively 
dedicated to the theory of determinants. Curiously, matrix algebra did not evolve 
along with the study of determinants. It was not until the work of the British 
mathematician Arthur Cayley that the matrix was singled out as a separate en- 
tity, distinct from the notion of a determinant, and algebraic operations between 
matrices were defined. 
Arthur Cayley (1821-1895) 
Arthur Cayley began his career by studying literature 
at Trinity College, Cambridge, but he developed a side 
interest in mathematics, which he studied in his spare 
time. This "hobby" resulted in his first mathematical 
paper in 1841 when he was only 20 years old. To make a 
living, he entered the legal profession and practiced law 
for 14 years. However, his main interest was still math- 
ematics. During the legal years alone, Cayley published 
almost 300 papers in mathematics. In 1863, Cayley was 
given a chair in mathematics at Cambridge University, 
and thereafter his output was enormous. Only Cauchy 
and Euler were as prolific. 
ARTHUR CAYLEY 
In 1855 Cayley was composing linear functions, and he introduced the basic 
ideas described in the discussion of matrix multiplication on page 55 mainly to 
simplify notation. However, this led to Cayley's 1857 publication A Memoir on 
the Theory of Matrices that laid the foundations for the modern theory. This 
is generally credited for being the birth of linear algebra. Because the idea of 
the determinant preceded concepts of matrix algebra by at least two centuries, 
it has been said that matrix theory was well developed before it was created. 
This must have been the case because matrix theory and linear algebra exploded 
after the publication of Cayley's memoir, and the subject quickly evolved into 
the discipline that now occupies a central position in both applied and theoretical 
mathematics. 
In 1850 Cayley crossed paths with James J. Sylvester (1814-1897), and 
between the two of them matrix theory was nurtured and brought to maturity. 
Due to their theory of invariants, the two became known as the "invariant twins." 
Although Cayley and Sylvester shared many mathematical interests, they were 
quite different people, especially in their approach to mathematics. Cayley had 
an insatiable hunger for the subject, and he read everything that he could lay 
his hands on. Sylvester, on the other hand, could not stand the sight of papers 
written by others. Cayley never forgot anything he had read or seen—he was a 

1.11 Short History of Matrix Theory 
137: 
living encyclopedia. Sylvester, so it is said, would frequently fail to remember 
even his own theorems. Cayley often said, "I really love my subject," and all 
indications substantiate that this was indeed the way he felt. He remained a 
working mathematician until his death at age 74. 
James J. Sylvester (1814-1897) 
Sylvester entered the University of London at age 14 
and later attended St. John's College, Cambridge, but 
he was denied a degree because he could not subscribe 
to the Thirty-Nine Articles of the Church of England. 
However, he was successful in earning a bachelor's and 
master's degrees from Trinity College, Dublin. After 
graduation he came to the United States in 1841 to 
accept the chair of mathematics at the University of 
Virginia, but he was appalled by what he found. Com- 
pared to the dignified air at British universities, the 
atmosphere at UVA was completely unrefined. The stu- 
JAMES J SYLVESTER 
dents were a drunken and lawless lot, and Sylvester 
quickly learned that US students did not regard their instructors with the same 
reverence as did their European counterparts. Just prior to Sylvester's arrival 
at UVA a student murdered a faculty member, and shortly thereafter Sylvester 
himself was embroiled in an argument with a knife-wielding student concerning 
academic fees. When only a mild punishment was meted out, Sylvester resigned 
in protest and quickly returned to England to work as an actuary, study law, 
and later became a professor at the Royal Military Academy in Woolwich. After 
being forced into retirement, Sylvester returned to the US in 1876 to chair the 
mathematics department at Johns Hopkins University. This time Sylvester asked 
to be paid in gold. After seven productive years at Hopkins Sylvester returned 
to London, where he remained for the rest of his life. 
Sylvester is believed to be the first to use the term "matrix" in the context 
of mathematics. In 1850 he defined a matrix to be an "oblong arrangement 
of terms," and he viewed matrices as entities which could be used to evaluate 
and describe various determinants. Sylvester and Cayley were quite a pair—two 
lawyers whose passion was mathematics. 

atan shatter 'det wate sslidioade 0 
1 60 
al Pore) 
Veen churt 
wr 
ele 
i 
' 
é 
? 
4 
4 
vie 
i 
" 
'el ae el § 
wus 
My books are water; those of the great geniuses 
are wine. Everybody drinks water. 
— Mark Twain (1835-1910) 
Advice to all professors: Teach what you know, 
and know what you teach. 
— Nicholas J. Rose (1924-) 

CHAPTER 
2 
Systems, 
Elimination, 
and Echelon Forms 
2.1 
GAUSSIAN ELIMINATION 
The purpose of this chapter is to introduce basic principles for solving and ana- 
lyzing a system of m linear equations in n unknowns as shown below. 
Gj 21 + 81222 +--+ + Aindn = 01 
A212 + A2Q2%2 +++: + AenIn = be 
(2.1.1) 
OmiL1 + Om202 + 
8? + Omni tm = Om 
The coefficients 
a;; 
as well as the right-hand entries 
b; are assumed to be 
known, and the first goal is to determine the unknowns 
2;. As discussed on 
page 61, it is often convenient to represent such a system as a single matrix 
equation Ax = b in which 
An xn = [aij] is the coefficient matriz, 
Xn x1 
is 
the column vector of unknowns, and bm» 
x1 is the column vector containing the 
entries from the right-hand side. For each such system there are exactly three 
possibilities for the solutions for x. 
> 
Unique Solution: There is one and only one set of values for the x; 's that 
satisfies all equations in (2.1.1) simultaneously. Equivalently, there is one and 
only one column vector x for which Ax = b. 
> 
No Solution: The system is inconsistent in the sense that there is no set 
of values for the 2;'s that simultaneously satisfies all equations in (2.1.1). 
Equivalently, the set of column vectors x satisfying Ax = b is the empty 

120 
Chapter 2 
Systems, Elimination, and Echelon Forms 
set (denoted by 0). Consistency issues for linear systems were previously 
introduced and discussed in Theorem 1.7.5 on page 62. 
> 
Infinitely Many Solutions: There are infinitely many different sets of val- 
ues for the 2,'s that simultaneously satisfy (2.1.1). The set {x|Ax = b} 
(the set of all column vectors x such that Ax = b) is called the general 
solution. If a system has more than one solution, then it must possess in- 
finitely many solutions. In other words, it is impossible for a system to have 
exactly two different solutions—if x and y are two distinct solutions, then 
{z|z=a(x—y)+y, a€ F} is a set of infinitely many different solutions. 
Square and Nonsingular Systems 
Part of the job in dealing with a linear system is to decide which one of the above 
three possibilities is true. The other part of the task is to compute the solution if 
it is unique or to describe the general solution when there are infinitely many so- 
lutions. Gaussian elimination is a tool that can be used to accomplish all of these 
goals, but before considering completely general linear systems, it is better to 
first understand how Gaussian elimination is used to solve square and nonsingu- 
lar linear systems. One reason for this is because each nonsingular system possess 
a unique solution (see (1.8.11) on page 75), and hence existence and uniqueness 
considerations are not an issue for nonsingular systems. So, throughout this sec- 
tion the focus is on square and nonsingular systems before transitioning to more 
general systems. 
Equivalent Systems and Elementary Operations 
Two linear systems are said to be equivalent if they possess equal solution sets. 
The strategy behind Gaussian elimination is to systematically transform one 
linear system into a simpler but equivalent system by successively eliminating 
unknowns by means of a combination of the following three elementary opera- 
tions that preserve equivalence. 
(i) Interchange the i*" and j*" equations. 
(ii) Replace the i" equation by a nonzero multiple of itself. 
(aed) 
(iii) Replace the j*" equation by a combination of itself plus a 
multiple of the i" equation. 
(2.1.4) 
These operations will eventually be applied to solve completely general linear sys- 
tems, but first focus on using them to solve a nonsingular system. The following 
simple (but typical) example describes basic Gaussian elimination in detail. 
Example 
Apply Gaussian elimination to solve the following nonsingular system. 
22 
OS 
fh, 
Meee 
| 
Gr Py 
7 ae 
(20) 
—2¢ 
+ 
Qy + 
2 
= 
7 

2.1 Gaussian Elimination 
121 
At each step focus on one position called the pivot position, and eliminate all 
terms below this position using the three elementary operations (2.1.2)—(2.1.4). 
The coefficient in the pivot position is called a pivot element (or simply a pivot), 
and the equation in which the pivot lies is referred to as the pivot equation. For 
elimination to work, a zero cannot occupy a pivot position. If a coefficient in a 
pivot position is ever 0, then the pivot equation is interchanged with an equation 
below the pivot equation to produce a nonzero element in the pivot position (this 
is always possible for nonsingular systems). Unless it is zero, the first coefficient 
of the first equation is taken as the first pivot! For example, the circled () in 
the system below is the pivot for the first step. Pivot positions will be circled 
throughout this introduction, and E, will denote the k*" equation. 
Oe ee ee 
62 + 2y + 2 =-1 
—2xr + 2+2z2= 
7 
STEP 1. Eliminate all terms below the first pivot by first subtracting three times 
the first equation from the second 
ye 
sean 
a Ss, 
Tol 
Se 
Ula 
2z 
= 
—-A4 
(E2 — 3E;), 
On --o2y 
2 eT 
and then add the first equation to the third equation to produce the 
following equivalent system. 
Qc 
yee es = 
el 
ee 
oy 
38y + 22 = 
8 
(F34+ Fy) 
STEP 2. Select a new pivot by moving down and to the right—i.e., by moving 
down the diagonal? If this coefficient is not zero, then it is the next 
pivot. Otherwise, interchange with an equation below this position so 
as to bring a nonzero number into the second pivot position. In our 
example, —1 is the second pivot as identified below. 
Ai 
Yorn 
es = 
ol 
Gy — 
22 
= —-4 
OUP 2 eg —— aa) 
i Except for getting rid of a zero in the pivot position, do not interchange equations simply for 
computational convenience—e.g., don't do an interchange to avoid fractions. Practical inter- 
change strategies are discussed in §2.8 on page 233. 
The strategy of selecting pivots in practical computation is more complicated than simply using 
the next coefficient on the diagonal. However, use the down-and-right strategy for now—more 
practical strategies will be introduced later. 

122 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Srp 3. Eliminate all terms below the second pivot by adding three times the 
second equation to the third equation to produce the following equiva- 
lent system. 
Qik 
othe 
o2ineerel 
Cy 
24 
Dee 
A 
(2.1.6) 
= 
4a 
—4 
(E3 + 3E2) 
In general, the strategy at each step is to move down and to the right to select 
the next pivot, and then eliminate all terms below the pivot until you can no 
longer proceed. In this example, the third pivot is —4, but since there is nothing 
below the third pivot to eliminate, the elimination process is complete. 
The final result of elimination is a triangular system, such as (2.1.6), that 
is equivalent to the original system. The original system is said to have been 
triangularized. Triangular systems are easily solved by back substitution, which 
means the last equation is solved for the value of the last unknown and then 
substituted back up into the penultimate equation, which is in turn solved for 
the penultimate unknown, etc., until each unknown has been determined. For 
example, back substitution on (2.1.6) proceeds as follows. First solve the last 
equation to obtain 
Biss. 
Then substitute z= 1 back into the second equation in (2.1.6) to determine 
y= 4— 27 =4—-—2(1) 
= 2. 
Finally, substituting z= 1, y= 2 back into the first equation in (2.1.6) yields 
1 
1 
= 
1 _ 
— 3 
_ 
—_ 
= — 
which completes the solution process. 
Augmented Matrices 
It is clear from the preceding example that there is no reason to explicitly write 
out or carry along the symbols such as "2," "y," "z," and "=" at each elim- 
ination step. They are excess baggage because the arithmetic involves only the 
coefficients and right-hand side. The system is streamlined by discarding the 
extraneous symbols to produce a rectangular array in which each row repre- 
sents one equation. This allows for more efficient manipulation. For example, 
the system in (2.1.5) reduces to 
1 
75 
Me | 
se 
ea ial | 
(the vertical line emphasizes where "=" appeared). 
2 
25 
if 

2.1 Gaussian Elimination 
123 
When an m xX n linear system is written as a matrix equation Ax = b, the 
m x (n+ 1) matrix defined by [A|b] is called the associated augmented matrix 
(the coefficient matrix has been "augmented" with the column representing the 
right-hand side). Instead of applying Gaussian elimination to a set of equations, 
it is streamlined by replacing the three elementary equation operations (2.1.2)- 
(2.1.4) on page 120 with three equivalent row operations applied to the rows of 
[A|b]. In other words, (2.1.2)—(2.1.4) respectively become the three elementary 
row operations defined below. 
Elementary Row Operations 
TYPE I: 
Interchange rows i and j. 
ipaare 
TYPE II: 
Replace row 7 by a nonzero multiple of itself. 
(2.1.8) 
TYPE III: 
Replace row 7 by a combination of itself plus a 
: 
; 
(2.1.9) 
multiple of row 7. 
Applying any combination of these three types of row operations to an aug- 
mented matrix for a linear system produces an augmented matrix for an equiv- 
alent linear system. For example, to solve system (2.1.5) on page 120 by using 
elementary row operations, start with its augmented matrix [A|b] and triangu- 
larize the coefficient matrix A by performing exactly the same sequence of row 
operations corresponding to the elementary operations executed on the equations 
themselves. 
i 
—4 
0 ae a i toy a) 
—2 
7/ 
Rat+Ri 
0 
2 
2 
1 
1 
1 
—(0 —1 
-2 
4). 
0 
QO 
—4 
—4 
The final array represents the triangular system 
26 
+. 
y 
+ 
Zs 
= 
" 
Soe 
2g, 
= 
a 
— 
47 
= 
-4 
that is solved by back substitution as described earlier to produce x = —1, 
y =2, and z=1. Below is the formal algorithm for back substitution. 
Back Substitution 
When the augmented matrix of an n x n nonsingular system has been triangu- 
larized to the form 
ah 
ibe 
TOS 
a 
C1 
0 
t22 
::: 
tan 
c2 
in which each ¢t,; 4 0, 
(2.1.10) 
0 
0 
-++ 
 tnn 
Cn 

124 
Chapter 2 
Systems, Elimination, and Echelon Forms 
the formal algorithm for executing back substitution on (2.1.10) is as follows. 
set. gai Geftin 
for t=n—1,-n— 2). 2.52; 1, 
wi = (Cj — tipi Tig1 — tie+2%it2 — +++ — tindn) [tii 
(2.1.11) 
end for 
Interchanges 
Back substitution requires that all pivots (the diagonal elements in the case of 
square and nonsingular systems) are nonzero. If a zero is encountered in a pivot 
position, then interchange the corresponding equation (or row) with one below 
it. Theorem 2.3.1 on page 159 guarantees that this is always possible when A 
is square and nonsingular. 
Example 
Solve the following system using Gaussian elimination with back substitution. 
U—= 
w=) 
3 
0 
1 
—-il 
eS 
—2u+4v- w= 
1 
sh 
—2 
4 
—-1 
1 
—2u + 5u —4w = —2 
—2 
5 
—4 
—2 
Since zero is not allowed in a pivot position, first interchange rows one and two 
before eliminating below the first pivot: 
' 
Interchange R; and R2 ( 
@) 4 
-] 
——_——_> 
a5 
Wy 
—+{o0@ 
=-1 
0 
i 
Back substitution yields the solution as shown below. 
w=-6/—2=3 
v=3+w=3+3=6 
u=—(1—4v+w) /2=-—(1-—24+3) /2=10 
Summary of Basic Gaussian Elimination 
Here is a summary of how standard (or basic) Gaussian elimination is applied 
to solve a square and nonsingular system AyynX = b. 
1. First convert the system to an augmented matrix [A|b], and then execute 
the following steps for 
k = 1,2,...,n —1. 
2. Determine whether or not the entry in the (k,k) position (the k*" pivot 
position) in the augmented array is zero. If it is nonzero, then it is designated 

2.1 Gaussian Elimination 
125 
to be the k*" pivot. Otherwise bring a nonzero entry into the pivot position 
by interchanging the k*" row with a row below the k*" row. 
3. Use Type IIT row operations (2.1.9) to eliminate all terms below the pivot. 
4. After k—1 steps the system is triangularized—i.e., the augmented array has 
the form [T|c], where T is upper triangular. 
5. Use back substitution (2.1.11) to extract the solution for x. 
Flop Count for Gaussian Elimination 
One way to gauge the efficiency' of an algorithm is to count the required number 
of arithmetical operations in terms of flops (short for "floating-point operations" ) 
which is an operation of the form af+v7 (see page 233 for a detailed discussion). 
No distinction is made between additions and subtractions (both are considered 
to be additive operations), and no distinction is made between multiplications 
and divisions (both are considered to be multiplicative operations). The tedious 
counting details are omitted, but it is nevertheless important to be aware of the 
approximate flop count for Gaussian elimination with back substitution to draw 
comparisons when other algorithms are encountered. 
Gaussian Elimination Flop Count. Gaussian elimination with back sub- 
stitution applied to an n xn nonsingular system requires O(n°/3) ? 
flops. The exact flop count involves some additional second-order and 
first-order terms (Exercise 2.1.27) but for large values of n the lower- 
order terms are negligible compared to the cubic term n?/3. 
Gauss—Jordan Elimination 
t 
A variation of Gaussian elimination is the Gauss—Jordan method, which has two 
features that distinguish it from standard Gaussian elimination. 
Counting arithmetic operations to gauge the efficiency of an algorithm is not quite as impor- 
tant as it once was. Arithmetic in older computers was slow compared to other aspects such 
as statement execution and memory management, but newer computers execute arithmetic 
relatively fast compared to other hardware functions. Furthermore, contemporary machines 
execute instructions in parallel so that different numerical tasks can be performed simultane- 
ously. An algorithm that lends itself to parallelism or minimal memory manipulation may have 
a higher flop count but might nevertheless run faster than one with a lesser flop count. 
Flop counts for matrix computations are generally polynomial expressions in n (or mn), the 
size of the matrices involved. In these cases saying that a matrix algorithm requires O(an?) 
flops for a given constant alpha means that the leading (highest order) term in the flop-count 
polynomial is an?. The intuition is that as n grows, terms involving lower powers of n in the 
flop-count polynomial become increasingly insignificant compared to the highest order term, 
so, for large n, an O(an?P) algorithm requires roughly an? flops. More rigorous discussions 
of the "big-oh" and "little-oh" notations are given on page 932. 

126 
Chapter 2 
Systems, Elimination, and Echelon Forms 
(1) Each pivot is forced to be 1. 
(2) All terms above and below the pivot (which is 1) are eliminated. 
In other words, applying the Gauss-Jordan method to solve a nonsingular system 
Ax = b produces 
@i1 
@12 
°° 
Gin 
by 
io 
0 
x1 
a21 
@22 
°*: 
@2n 
b2 
O42 
0 
r2 
[A|b] = 
crise 
Ver 
—> [I |x] = 
pth 
Qni 
Qn2 
'** 
Gnn 
bn 
OS 
OF 
vaet= 
td 
Ln 
Notice that the solution appears in the last column, so Gauss—Jordan circumvents 
the need to perform back substitution. The following typical example makes the 
Gauss—Jordan process clear. 
Example 
Apply the Gauss-Jordan method to solve the following system. 
22, + 272 + 673 = 
4 
244+ 
 to-+ Try = 
6 
221° 
= 
622 
°— 
Tz, ==] 
The row operations are indicated, and the pivots are circled. 
(Ona: 
4\ 
R,/2 
rr 
ss 
2 
a 
6 
emi 
> ie te ae 
6 | Re-2Ri 
=) 
aa6. 7 
tei 
=—2 
-6 
-7 | =1/ 
Me+2hi 
Gye1 
1k 
o\ 
Res Re 
— | 6 
Sh 
flog | Ra) 
eee 
2) 
i 
ee 
ee 
oe 
O) 
ted 
ied 
3) 
R3+4Ro 
(| 
0 
4 
4 
i 
4 
4 
Beis 
—> 
Oo ye =n 
|e 
ae 
[Ode 
Ry 
oe Ree 
0. 
Owe 
hm) Meugy/o 
Re 
/S 
pw O(a) 
1 
Flop Count for Gauss—Jordan 
At first glance it may seem that there is little difference between the Gauss— 
Jordan method and Gaussian elimination with back substitution because elimi- 
nating terms above the pivot with Gauss—Jordan seems equivalent to performing 
back substitution. But this is not correct. Gauss—Jordan requires more arithmetic 
than Gaussian elimination with back substitution. 
=

en 
a 
2.1 Gaussian Elimination 
| 
. 
127 
Recall that Gaussian elimination with back substitution requires O(n?/3) 
flops (see page 125). Compare this with the O(n3/2) factor required by the 
Gauss—Jordan method, and it is apparent that Gauss—Jordan requires about 
50% more flops than Gaussian elimination with back substitution. For small 
systems of the textbook variety, these comparisons are not significant. However, 
the systems encountered in practical work can be quite large (e.g., on the order of 
millions or even billions), and the difference between Gauss—Jordan and Gaussian 
elimination with back substitution can be substantial. For example, even for 
n = 10,000, the difference between n3/2 and n?/3 is about 1.67 x 10!!, which 
is a significant number of flops. 
Although the Gauss—Jordan method is not recommended for solving linear 
systems that arise in practical applications, it does have some theoretical ad- 
vantages. Furthermore, as described on page 163, Gauss—Jordan can be a useful 
technique for other computational tasks such as matrix inversion. 
Echelon Form 
Beyond nonsingular linear systems, Gaussian elimination and Gauss—Jordan re- 
duction are used to solve and analyze singular and rectangular systems, and 
they are employed to reveal characteristic features of matrices. But to do so it 
is necessary to modify the two procedures. 
Zero pivots are problematic, so they are avoided whenever possible. When 
A is square and nonsingular, nonzero pivots can be made to fall on the main 
diagonal (a rigorous proof comes later), and row reduction yields a triangular 
matrix such as 
(x) 
* 
* 
* 
i. Rie 
However, when rectangular or singular matrices are row reduced, the pivot posi- 
tions need not be on the main diagonal, and the final result may not be triangular. 
For example, consider the 4 x 5 matrix 
A= 
(12) 
NoOrN 
re mo 
® 
bo owor 
ee 
Ov 
CO "IOI 
Ww 

Fs Feet ead peices, Wc pst aveg tacked 
wa eee 
-row. If this is not possible, then continue moving the pivot position to the right 
and repeat the process until all possible pivot positions have been exhausted. 
The following example should make this procedure clear. 
G) aaatyva 
Gutris aes\. 
a-(Fisiy— 
oo 
@- - 
12355 
iG) 
se MeMbeal 
24047 
oo 
-2 -2 1 
1.1 
G2 1 
Dro abana 
a 
o 
o @ -2--2 
a 
0 
2 -2|_p_ 
"1 ooo 
o @}| 
|ooooe@® 
000 
a0 3 
0 
0 
0 
0 
0 
Rather than being triangular, the form of E is a jagged or stair-step 
type of 
structure that is hereafter referred to as an echelon form (or, alternately, as a row- 
echelon form to emphasize that it is produced by row operations). In general, 
a row-echelon form has its nonzero entries on or above a stair-step line that 
emanates 
from the upper-left-hand corner and decends down and to the right. 
The pivots are always the first nonzero entries in each row, and zero rows are 
always at the bottom. A typical 6 x 8 row-echelon 
form with the pivot positions 
circled 
is illustrated below. 
® 
Se 
SE 
SS 
Se 
CS 
0 
@ 
=* & 
= 
* 
= 
00 
0:@ 
«== 
= 
« 
E=/0 0 0 0 00:@ - 
(2.1.14) 
000 
000 
0 
0 
000 
00000 
Below is a formal summary of the process used to reduce a matrix A to echelon 
form by means of Gaussian elimination. 

2.1 Gaussian Elimination 
129 
Reduction to Echelon Form 
Let U be the matrix obtained after k —1 elimination steps have been applied 
to A. To execute the k*" step, proceed as follows. 
1. Moving from left to right in U, locate the first column U,,; that contains a 
nonzero entry at or below its k*" entry. Position (k,7) is the pivot position 
for step k. 
3. If necessary, interchange row k with a lower row to bring a nonzero number 
into the (k,7)-position, and then annihilate all entries below this pivot. 
4. If row Ux, and all rows below it are entirely zero, then the elimination 
process is completed. 
Reduced Echelon Form 
When the Gauss—Jordan process is used in place of Gaussian elimination to 
reduce a matrix A to echelon form, the result is similar except that each pivot 
is forced to be 1, and all terms above and below the pivot are eliminated. The 
echelon form produced by Gauss—Jordan reduction is called the reduced row- 
echelon form because it is a reduced version of the form produced by Gaussian 
elimination. The notation Ea, is used to distinguish the reduced echelon form 
derived from A by Gauss—Jordan reduction as opposed to an echelon form E 
obtained by standard Gaussian elimination. 
A typical 6 x 8 reduced row-echelon form is shown below in (2.1.15), where 
the positions marked as * can contain either zero or nonzero numbers. 
ie 
Ou Den O90 
to 
ee 
40. 
bet 
| 
ate 
Oa eee 0 
he 
Baad) 
Pete ano 01® * 
(2.1.15) 
Om 
PO 
Rome 
eh 
0 
OO 
LOMO 
MOR 
an 
we 
Compare this reduced form (2.1.15) with the unreduced form in (2.1.14) on page 
128. The formal definition of the reduced row-echelon form is as follows. 
2.1.1. Definition. A matrix E,,.,, is said to be in reduced row-echelon 
form provided that the following three conditions hold. 
1. E is in row-echelon form. 
2. The first nonzero entry in each row (i.e., each pivot) is 1. 
3. All entries above as well as below each pivot are 0. 

130 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Example 
; 
; 
To determine the reduced echelon form Ea, derived from the matrix A in 
(2.1.12) on page 127, apply Gauss—Jordan reduction as shown below. 
@2133 
Ciper of eget s 
Gy 2° 3-33 
Lott Berar 44 
0 
o "Gy -922 
0° 
(iy 
sila] eae Arie tnd Ever eamaae es we he 
s e me me eee 
aac aie 
'ee 
hes oe 
a 
Ge Re ae 
(CBs we eget salen 
yee 
ae 
Bole OleOle oat wpe ee OL) IF 
ou 
oO 0 (0) 
0. aloo 
&) 
laa i Sal aaa 
Oo om 
ord 
aero 
Tar 
Ci) aaron? 6 
Cord Meier 
oon enor wae 
21.16 
hoc Todt oy Cou fe ac OM CMON OHS 
* 
( 
) 
Ge CNON AO eS 
OF LOO ROiewe 
Caution! The pivot columns in E, are unit columns e;, but these unit columns 
need not appear in their natural positions. For example, the pivot columns in 
Ea in (2.1.16) are {e1, e2, e3}, but they respectively occupy positions 1,3, 5. 
Moreover, a particular unit column e; can appear more than once in a reduced 
row-echelon form, but not every appearance of e; corresponds to the position 
of a pivot column—consider E = (5 : on 
Uniqueness 
Comparing (2.1.16) and (2.1.13) shows that they have the same "form" (i.e., 
the stair-step structure and the pivot positions are the same). In other words, 
Gaussian elimination and Gauss—Jordan can produce different entries, but they 
produce the same "form." The formal proof of this fact is Theorem 2.2.10 on 
page 151. However, both the form and the entries in the reduced echelon form 
Ea are uniquely determined by A. This is Theorem 2.2.11 on page 152. This 
uniqueness is an advantage for theoretical developments, and it is the primary 
reason for introducing Ea. 
Exercises for section 2.1 
2.1.1. Use Gaussian elimination with back substitution to solve the following 
system: 
qe 
Sy Gib Akay 
Se Mit 
%4 + a9 
2e3 = 1, 
t+ 2259 -- 3a3 = I. 

2.1 Gaussian Elimination 
131 
2.1.2. Apply Gaussian elimination with back substitution to the following sys- 
tem: 
221 Sar bh) 
= 0, 
—21 + 249 — 23 = 0,7 
—%2+%3= 1. 
2.1.3. Use Gaussian elimination with back substitution to solve the following 
system: 
4r5 ag 323 = 3, 
—21 + 7x2 — 523 = 4, 
—%1 + 8x2 — 623 = 5. 
2.1.4. Solve the following system: 
fi te ie 
4 = 1, 
21+ t+ 3234+ 
324 = 3, 
Zit #2 + 2x3 + 34 = 3, 
21+ 300 + 343 + 324 = 4. 
2.1.5. Consider the following three systems where the coefficients are the same 
for each system, but the right-hand sides are different (this situation 
occurs frequently): 
4g — 8y +52 
=1/0)0, 
Ay — fy +4z=0/1)0, 
3a — 4y+ 
2z 
=0|0/{1. 
Solve all three systems at one time by performing Gaussian elimination 
on an augmented matrix of the form 
[A | bi | be | bs]. 
2.1.6. Suppose that matrix B is obtained by performing a sequence of row 
operations on matrix A. Explain why A can be obtained by performing 
row operations on B. 
2.1.7. Find angles a, 8, and y such that 
2sina— 
cos@+3tany 
= 3, 
A4Asina +2cos $8 —2tany 
= 2, 
6sina—38cos8+ 
tany=9, 
wheren0) orm 
27, 0 = 6 <= 27, and Oy <7. 

; 
we; 
21 +3ag-223=4, 
~ 
—21 + 4x — 3243 = 5, 
—21 + 5x2 — 423 = 6, 
using Gaussian elimination and explain why this system must have in- 
finitely many solutions. 
2.1.10. By solving a 3 x 3 system, find the coefficients 
in the equation of the 
parabola y =a+6xr+-zx? that passes through the points (1,1), (2,2), 
and (3,0). 
2.1.11. Use the Gauss—Jordan method to solve the following system: 
deg = 323 = 3, 
—21 + 122 — 523 = 4, 
= Me 8x2 = 6x3 =e 
2.1.12. Apply the Gauss-Jordan method to the following system: 
te Code aa eA 1, 
£1 + 2rq + 2x3 + 224 = 0, 
x1 + 2%q + 323 + 324 = 0, 
21 + 220 + 343 + 42,4 = 0. 
2.1.13. Use the Gauss—Jordan method to solve the following three systems at 
the same time. 
224 Sani by 
SS 
0, 
—£1 + 27g —r%3 = 
0/1) 0, 
=p 
ear 
ONN0. | 1. 

2.1 Gaussian Elimination 
2.1.14. 
Zeke 
2.1.16. 
Aah le 
2.1.18. 
2.1.19. 
2.1.20. 
133 
Reduce each of the following matrices to row-echelon form. 
Dt itedan tie 
Coy 
pe, 
aaa 
(aes) 
(tat ei gs 
@ (2469) @[2 60 
(ll6 
3 
gage 8 
Ba owe 
Od0r 
3 310.058 
pig eat RY aber 
ee 
Determine the reduced row-echelon form for each of the matrices in 
Exercise 2.1.14. 
Determine which of the following matrices are in row-echelon form: 
Wee ee 
0000 
(a) (0 0 4). (b) (0199), 
OF 0 
cue ot 
i eect 
Po 
ass G24 
Once Ot, 00 
() (0 97 -8) 
(4) |o 00001 
6.0 
6 CFO 6 
Give an example of two different echelon forms that can be derived from 
A=(5 45): 
2 
-4 
-8 
6 
3 
Let Ea _ be the reduced row-echelon form for A = (0 1, 
aed Pee ), 
3 
—2 
0 
0 
8 
(a) Write each non-pivot column in Ea as a combination of the 
pivot columns to the left-hand side of the non-pivot column. 
(b) Write each non-pivot column in A as a combination of the pivot 
columns to the left-hand side of the non-pivot column. What do 
these observations suggest? 
Let Ea, be the reduced row-echelon form for A #0. 
(a) Explain why the set of pivot columns in Eq must be linearly 
independent. 
(b) Based on the observation made in Exercise 2.1.18, what does 
this suggest about the set of pivot columns in A? 
By looking at the typical reduced row-echelon form Ea 
in (2.1.15) on 
page 129, give an intuitive explanation of why the non-zero rows in every 
nonzero reduced echelon form should be a linearly independent set. 

134 
Chapter 2 
Systems, Elimination, and Echelon Forms 
2.1.21. How many different "forms" are possible for a 3 x 4 matrix that is in 
row-echelon form? 
2.1.22. Suppose that [A|b] is reduced to a matrix [E|c]. 
(a) Is [B|c] in row-echelon form if E is? 
(b) If [Ble] is in row-echelon form, must E be in row echelon form? 
2.1.23. Suppose that 100 insects are distributed in an enclosure consisting of 
four chambers with passageways between them as shown below. 
At the end of one minute, the insects have redistributed themselves. Assume that 
a minute is not enough time for an insect to visit more than one chamber and 
that at the end of a minute 40% of the insects in each chamber have not left the 
chamber they occupied at the beginning of the minute. The insects that leave 
a chamber disperse uniformly among the chambers that are directly accessible 
from the one they initially occupied—e.g., from #3, half move to #2 and half 
move to #4. 
(a) If at the end of one minute there are 12, 25, 26, and 37 insects 
in chambers #1, #2, #3, and #4, respectively, determine what 
the initial distribution had to be. 
(b) Ifthe initial distribution is 20, 20, 20, 40, what is the distribution 
at the end of one minute? 
2.1.24. Show that the three types of elementary row operations discussed on 
page 123 are not independent by showing that the interchange operation 
(2.1.7) can be accomplished by a sequence of the other two types of row 
operations given in (2.1.8) and (2.1.9). 

2.1 Gaussian Elimination 
135 
2.1.25. For a linear system Ax = b, performing row operations on [A|b] does 
not change the solution of the system. However, no mention of column 
operations was ever made because column operations can alter the solu- 
tion. 
(a) Describe the effect on the solution of a linear system when 
columns A,; and A,, are interchanged. 
(b) Describe the effect when column A,; is replaced by aA,; for 
a#0. 
(c) Describe the effect when A,; is replaced by Ax; + Axx. 
Hint: Experiment with a 2 x 2 or 3 x 3 system. 
2.1.26. Explain why a linear system can never have exactly two different solu- 
tions. Extend your argument to explain the fact that if a system has more 
than one solution, then it must have infinitely many different solutions. 
2.1.27. Solving a general n x n system with Gaussian elimination and back 
substitution requires 
n> 
n 
2 
qs 
: 
. 
se Seay 
3 multiplicative operations 
3 
and 
n' ae n® _ 
5n 
additive operations 
Ape, 
AEG 
z 
, 
Verify this for n = 3. If you are up to the challenge, try to verify these 
counts for a general n. 
2.1.28. For a general 3 x 3 system, verify that the Gauss—Jordan method uses 
3 
2 
nen 
sad 
ag 
51 + 7 
multiplicative operations 
and 
noon 
ve 
oy rag 
additive operations. 
If you want a challenge, try verifying these counts for a general n x n 
system. 

136 
Chapter 2 
Systems, Elimination, and Echelon Forms 
2.2 ELEMENTARY MATRICES AND ECHELON FORMS 
The three elementary row (or column) operations 
'TYPE I: 
Interchange rows (columns) 7 and j. 
Typeé II: 
Multiply row (column) i by a #0. 
TYPE III: Add a multiple of row (column) i to row (column) j. 
are the basic components of matrix reduction, and the arrow notation A —+ B 
is a way to indicate that matrix B is the result of applying some sequence of 
elementary operations! to matrix A. While convenient, this arrow notation is 
limited because algebraic manipulations cannot be performed with arrows. For 
example, if A,B,C,D 
are the same size with 
A = B and C = D, then 
A+C=B+4D, but it cannot be inferred that 
A — B and C + D implies 
A+C-—B+D. This means that in order to use matrix algebra when elemen- 
tary operations are involved, a mechanism to turn arrows 
(—+) into equalities 
(=) is needed. This is accomplished with the elementary matrices defined below. 
2.2.1. Definition. An elementary matriaz of Type I, II, or III is created 
by performing the respective elementary operation to an identity matrix. 
For example, the matrices 
i 
Ts) 
0.6 
to). 
A 
B= (1 0 0), B= (10 
a 0) 
with 
a #0, B,=(0 1 0) (2.2.4) 
OF Ome 
(Oh Oe 
(gelD) 
ci 
are 3 x 3 elementary matrices of Types I, I, and III, respectively, because E, 
arises by interchanging the first and second rows (or columns) in 13,3, whereas 
E2 is generated by multiplying the second row (or column) of I by a, and Es 
is constructed by multiplying the first row (or third column) of I by a@ and 
adding the result to the third row (or first column). 
Elementary Operations via Multiplication by Elementary Matrices 
Elementary matrices allow us to execute row or column operations by means of 
matrix multiplication. 
e 
When used as a left-hand multiplier, an elementary matrix of Type I, II, or 
II executes the corresponding row operation. 
Row operations are more prevalent than column operations, so the arrow notation A > B 
will generally denote row reduction unless otherwise stated or implied by the context. When 
it 18 necessary to explicitly distinguish row reduction from column reduction we respectively 
write 
A —+> B and A 
col 
— + B. When it is not clear from the context that both row and 
column operations are used, this can be made explicit with the notation A <*> B, 

2.2 Elementary Matrices and Echelon Forms 
137 
e 
When used as a right-hand multiplier, an elementary matrix of Type I, II, 
or III executes the corresponding column operation. 
rg 
oe 
es 
For example, respectively multiplying 
A= 
{2 4 
8 ) 
on the left-hand side by 
6 
13 
the elementary matrices in (2.2.1) yields 
at 
4nt 
1 
a 
| 
1 
2 
4 
Ey AS 
(1 124 
EA = (02 a4 as), BaA = ( 
2 
4 
8 
), 
Sheat3 
Se 
CLS 
a+3 
2a+6 
4a+13 
which is the same result obtained by applying the associated row operations to 
A. Similarly, the respective column operations are performed by the right-hand 
multiplications 
Fo 
Naat] 
ip 
step 
ah 
1+4a 
2 
4 
AE; 
=[{4 
2 :). AB» = (2 
4a 
')), AB, = (2:80 ve) ) 
6258248 
3 6a 
13 
3+13a 
6 
13 
These observations allow any elimination or reduction process to be represented 
by a sequence of matrix multiplications. 
Example 
4: 
etmAs— (2 
4 
8 ) 
. The sequence of row operations used in the Gauss—Jordan 
3h 
{e118} 
reduction A — Ea 
is indicated below, where R; denotes the i*" row. 
ae 
top 
2g 4 
A=(2 4 
= ) 
pam —+(0 0 0 
3 6 
13/ 
Rg —-3R, 
Om. Ome 
Interchange R2 and Rg 
1 
2 
4\ 
Ri -—4Re 
20 
OS 
OQ 
@ 
1 
— 
(e050) 
18 
k= Hae 
(oy a) 
Ot) 
(2.2.2) 
The same four reduction steps are accomplished by the following sequence of 
left-hand multiplications with the corresponding elementary matrices. 
140 
i O00 
ey 
ie 
6 20 
(0 1 0)(0 0 1\( 01 
0)(-2 1 0)A=Ba. 
Ou Out 
Ob ON oe Oca. 
OF (ow fi 
Since the product of these four elementary matrices is 
130 
—4 
P=(-3 0 
1), 
Ces 
=, 
0 
the entire Gauss—Jordan reduction of A boils down to the equation PA = Ea, 
thereby eliminating the need for arrows. 

138 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Accumulating Products of Elementary Matrices 
Row reducing the augmented matrix [A|I] > [B|P] reduces A to B while 
simultaneously accumulating the product P of the elementary matrices such 
that PA = B. To see why this works, suppose that the reduction requires k 
elementary operations, and let P; be the elementary matrix corresponding to the 
ith reduction step so that P = P,---P2P; and B = (Px: -P2P,)A = PA. 
Since X[Y |Z] = [XY | XZ] (see page 58), the effect of row reducing [A|I] is 
to generate the following sequence of matrices. 
STEP 1: 
P,/A|]| = [P,A|P3] 
STEP 2: 
P2[PA | Pi] = [P2P1A | P2P3| 
STEP k: 
P, |: -PoP\A | 
wie 
@ P2P3| = [Px eee P2P;A | 
P; ssie P2P3| = [B | 
P| 
Thus the reduction [A |I} + [B|P] circumvents the need to explicitly form the 
individual elementary matrices P; to generate P = P;--- PoP. 
Example 
Ll 
2 
4 
For the matrix A = (: 4 
8 ) 
from the example on page 137, use Gauss— 
3° 
6 
13 
Jordan reduction to determine the matrix P such that PA = Ea. 
Lo 24h TeOnG 
po a ee 0 
Te 204 bt Gee 
Al=[2 4 
8 
010) [oo 0 
21 0)+|o 0. 210] 
326 13-7 000d 
o*.6 1s 
0 Mo by 
001 
|-3 01 
[7a 
ie 6 
122) 01/4304 
[oo 1/9 0 1] +[0 03 
"20 1| 
= [Ba /P 
00 0/-2 10 
00 0]-2 10 
Note that this P agrees with the P obtained in (2.2.3) on page 137. 
Inverses of Elementary Matrices 
2.2.2. Theorem. All elementary matrices of Types I, II, and III are non- 
singular, and the inverse of an elementary matrix is again an elementary 
matrix of the same type. Moreover, if P = P,---P:P, 
in which each 
P; is an elementary matrix, then P is nonsingular, and 
Pol = (P,-.- Papi) = Ppt PP 

2.2 Elementary Matrices and Echelon Forms 
139 
Proof. 
If Ey isa Type! matrix obtained by interchanging the i*" and j*" rows 
(columns) in the identity matrix I, then E? =I 
(i.e., repeating the interchange 
restores natural order), so that ES = E,. Let E2 bea Type II matrix obtained 
from I by multiplying the i*" row (column) by a 4 0. To reverse the operation, 
the i*" row (column) needs to be multiplied by 1/a, so Ez! looks exactly like 
E2 except that a in Ep is replaced by 1/a in BZ ! For example, 
Or 
0) 
1 
0 
0 
B= (0 a 0) 
witha#0 
=> By'=(0 ve 0). 
OORT 
0 
0 
a 
It should be clear that E.E, 1of= EK, 1B.. Consider a Type III matrix 
obtained by adding a times row 7 in I to row j. Undoing the operation (to 
get back to I) requires that —a times row i be added to row j, so Ez 1 looks 
just like E3 except that a in Eg is replaced by —a in Bt: For example, 
IORO 
LOT 
0 
B= (0 1 0) = Bs =( 0.1 0). 
(2.2.4) 
eg 
@ 
i 
8) 
0) 
ol 
It is easy to verify that E3E3 ' = APS E; 'Es. The fact that products of ele- 
mentary matrices are nonsingular is a direct consequence of Theorem 1.8.6 on 
page 74. 
Hf 
More General Elementary Matrices 
While the three types of elementary matrices given in Definition 2.2.1 are the 
most common, there are other kinds of elementary matrices. A general elemen- 
tary matrix is any square matrix of the form I— uv*, where u,v € |e seed bi 
v*u #1, then I—uv* 
is nonsingular and direct multiplication shows that 
uv* 
=i 
ivi 
sl eae eee 
u oe 
v*tu-l 
(2.2.5) 
In other words, the inverse of a general nonsingular elementary matrix is again 
a general elementary matrix. 
Elementary matrices of Types I, I, and III given in Definition 2.2.1 are each 
special cases of (2.2.5) because appropriate choices of u and v_ will generate 
them. For example, if E,,E2,E3 are the respective Types I, I, and III elemen- 
tary matrices in (2.2.1) on page 136, and if e; denotes the i" unit column, then 
E, =I—uu', where 
u=e,—-e2, 
Ee: = I—(1—a)eze3, 
and 
E3 = I+ae3e/. 
Verification of these formulas is Exercise 2.2.7. 

140 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Example (Elementary Projectors and Reflectors) 
Examples of general elementary matrices that are not Type I, IJ, or III include 
the orthogonal projectors Q = I— uu', 
||ul|, = 1, from Definition 1.10.4 
(page 102) and the reflectors 
R =I-2uu', 
|l\u||,=1, from Definition 1.10.5 
(page 104). The elementary projector Q is necessarily singular (Exercise 1.10.9, 
page 108), so, unlike the elementary matrices of Types I, I, and IU, general 
elementary matrices need not be invertible. 
Formula (2.2.5) applied to R yields R~' = R because setting v* = 4 be 
in (2.2.5) implies that 
vtu—1=2u7u-1=2-1=1 
=> R '=I-2uu'? =R, 
or equivalently, R? =I. In other words R is an involutory matrix (see Exercise 
1.10.12 on page 109). Moreover, R = R', so R is a symmetric, involutory, 
and an orthogonal elementary matrix, which corroborates (1.10.25) on page 105. 
Elementary Triangular Matrices 
Standard Gaussian elimination is based on Type III elementary matrices, so it 
is important to realize the generalization of such matrices. Elementary lower- 
triangular matrices are triangular matrices of the form T;, = I— c,ep in which 
c, is a column with zeros in the first k positions. For example, if 
0 
1 
Oeeee 
0 
'My 
one 
0 
Gaaibes 
a4: 
0 
O 
«a. 
@ 
Ch 
. 
thens 
1, =) | 
0 
1 
0 
0 
PRE 
be 
0 
0 
pent 
0 
[Pte 
(Vim 
ica 
= pbs 
0 Rew: 
1 
Fal 
ie 
ee 
Oa 
Ue lew 
oO, 
. ao 
TU1=I 
Ts | Oe 
eet 
, =1+c.e, = 
oe 
a ak EL 
TE 
(2.2.6) 
OF Oe 
eae 
at 
- 
0 
0 
Gvestes 
pyper 
eee 
oe 
eT 
which is also an elementary lower-triangular matrix. The utility of elementary 
lower-triangular matrices lies in the fact that all of the Type III row operations 
used by Gaussian elimination to annihilate the entries below the kt" pivot when 
triangularizing A,» can be accomplished with one multiplication by T,. If 
* 
* 
vee 
a1 
* 
Pon 
* 
0 
* 
rene 
a2 
* 
Ssihe 
* 
Ax-1 = 
0 
0 
De site 
Ak 
* 
* 
OO 
Ok+1 
* 
* 

2.2 Elementary Matrices and Echelon Forms 
141 
is the partially triangularized result after k —1 elimination steps, then 
TyAg—1 = (I— peg) Ap—1 = An—1 — cpep Ax—1 
BRE Gila OM 
hy ok 
0 
QU 
ae 
ware 
a2 
* 
* 
OT =) 
Sl 
Od 
a, 
* 
«++ 
* 
|, 
where 
cy = 
0 
(2.2.7) 
OO 
Ora 
antes 
xe 
An41/ 
Ab 
(Nowe 
cat 
(tie 
28 
ou 
ealin 
is the column containing the multipliers used to annihilate all entries below 
ee 
ie! 
a,. For example, A = (2 
3 ') 
can be triangularized by two multiplications 
3 
4 
6 
T2T,A with elementary triangular matrices Ty 
=I—c,e? and Tz; =I— coed, 
where c; and cz are sequentially determined from (2.2.7) as follows. The first 
pivot is a; = I, 50 
0 
a 
ik.) 
(8) 
1 
2 
3 
Ci = 
(024,071) = 1 — ce = 
(0-201) 
0), 
cand) 
TyA= (: —1 -2). 
3 
3h 
Cael 
0 21-8 
0) 
it 
0 
O 
The second pivot is ag = —1, so cg = (2) and Tz =I—c,ef = (0 if 0). 
2 
0 
-—2 
1 
1 
2 
3 
Therefore, T2T,A = (0 —1 -2 
== UJ. 
0 
0 
1 
Interchange and Permutation Matrices 
Type I elementary matrices are called elementary interchange matrices (or sim- 
ply interchange matrices) because they perform an interchange of two rows 
(columns) when used as a left-hand (right-hand) multiplier. Since the elementary 
interchange matrix Q associated with an interchange of rows (columns) i and 
j is obtained from the identity matrix I by interchanging its i*" and j*" rows 
(columns), Q is necessarily a symmetric matrix because 
qi; = 1 = qji, and 
everything else retains the structure of I. For example, if the first and third 
rows (or columns) in the 4 x 4 identity matrix are interchanged, then the result 
1S 
mo 
at 0 
Ge 
OO WP ee 
Oe oro 
ro | 
60 "Ona 
In addition to being symmetric, interchange matrices are also involutory—i.e., 
Q? = I. This is evident because applying Q twice simply restores rows (or 
columns) to natural order. Therefore, if Q is an interchange matrix, then 
Q=Q? and) Q? =I, so that 
Q=Q? =Q". 
(2.2.8) 

142 
Chapter 2 
Systems, Elimination, and Echelon Forms 
In other words, interchange matrices are symmetric, involutory, and orthogonal. 
In general, a product of elementary matrices is not necessarily another el- 
ementary matrix, so in particular, 
a product P = QiQ2:-- Q, of interchange 
matrices P; need not be another interchange matrix. However, when applied 
to a conformable matrix A, the result PA = Q;Q2---Q,A is to sequentially 
interchange pairs of rows so that in the end the rows of PA are a permuta- 
tion of the rows in A. A similar statement hold for columns with right-hand 
multiplication AP. This motivates the following definition. 
2.2.3. Definition. A product 
P = Q,Q2::-Q, of elementary interchange 
matrices Q; is called a permutation matria. 
e 
It follows from (2.2.8) and the reverse order laws that every permu- 
tation matrix is an orthogonal matrix because P?' =P~!. 
(2.2.9) 
Example (Order Matters) 
The respective 4 x 4 interchange matrices associated with an interchange of the 
first and second rows (or columns) and an interchange of the second and fourth 
rows (or columns) are 
On Lie Uae 0 
tooth. 0 
Lalet 
0 
6 
LO O40 4 
Q2={5 
010 
and 
Qa=|9 
9 
1 0 
G. 0-0 3 
0100 
When these are multiplied, the following two permutation matrices are created. 
Oe 
Olan 
Oe 
OC 
Ole 
ab 
OC 
Oru 
at 
tO 
U 
50 
QuQi2=( 
5 9 
1 0 | 912Q%=]5 5010 
(2.2.10) 
LO. 
O50 
OF LOO 
The order of the interchanges matters. For example, let 
1 
Pie 
Daye 
PLE 
eee 
res 
A= 
5 io 
11 
12 
| 
(2.2.11) 
Loy 
14" 
£5) 
16 
and observe the following effect of applying Qo1Qj2 and Qi2Q2u4. 
He ri 8 
13 
14 
15 
16 
QuQir2A = 
foie! 
Sibir 
ae ft 
Po 
304 
9 
AOS Tat? 
QuQuA=1 
9 
19 
1 
12 
2 
eS 
ee 
hk 
ve 
eS 

2.2 Elementary Matrices and Echelon Forms 
143 
Symmetric Permutations 
There is often a need to permute the rows as well as the columns in a square 
matrix A such that exactly the same permutation is applied to both. For ex- 
ample, consider applying the permutation P = Qo4Qj» in (2.2.10) to both the 
rows and columns of the 4 x 4 matrix A in (2.2.11). 
Caution! At first glance it might appear that this is equivalent to forming the 
product PAP, but this is wrong. 
The correct way to proceed is to pay attention to the order of the interchanges. 
The result of interchanging the first two rows as well as interchanging the first 
two columns is obtained by Q;2AQj9. This is then followed by the second set 
of interchanges 
Qo4(Qi2AQ12)Qa4 = (Q24Qi12) A(Qi2Qzaa). 
Since P = Qo4Qi2, and since interchange matrices are symmetric, by (2.2.8), 
it follows that P? = (Q24Qi2)" = Qi2' Qo4? = Qi2Qo4, and thus reordering 
both the rows and columns according to the permutation defined by P = Qo4Qj2 
is accomplished by the product 
Ou 
Ome 
(Q24Qi2)A(Qi2Qo4) 
= PAP? = 
+ i = e 
2p 
Leek) 
Val 
Since these kinds of permutations are frequent occurrences, the following termi- 
nology is used to describe them. 
2.2.4. Definition. 
If A is a square matrix and if P is a permutation 
matrix, then B = PAP" is called a symmetric permutation of A 
because B is the result of re-ordering both the rows and columns of A 
according to the permutation defined by the order of the rows in P. 
e 
P'AP is also asymmetric permutation of A, but the permutation 
is defined by the order of the rows in P?, or equivalently, by the 
order of the columns in P. In general, 
P' AP 4 PAP'. 
Example (Undirected Graphs) 
Planar graphs are 
a common example of where symmetric permutations are 
found. An undirected graph is a set of nodes (points) connected by edges (undi- 
rected paths) as illustrated below. 

144 
Chapter 2 
Systems, Elimination, and Echelon Forms 
GRAPH #1 
The adjacency matrix A for a graph containing n nodes is the n x n matrix 
in which a;; = 1 if there is an edge between nodes i and j, and aj; = 0 if 
not. Vice versa, if A,» has nonnegative entries, then the graph for A is the 
set of n nodes in which there is an edge between nodes i and j if and only if 
ai; #0. For example, the adjacency matrix for Graph #1 shown above is 
OF 10k 6-50 
a 
al A 
a 
A lal A Oe 
Ae Fo 
om) whacky 
Piss i 
and vice versa, the graph for A; is Graph #1. 
Consider renumbering the nodes in Graph #1 according to the permutation 
(1, 2,3,4,5) — (5,1,4,3,2) so that the result is Graph #2 that is shown below 
along with its adjacency matrix Ao. 
kee II 
et 
el 
ol 
oe) ooror 
ood 
rf poo 
oO 
OoOrocor- 
GRAPH #2 AND ITS ADJACENCY MATRIX Ap 
Notice that Ag is a symmetric permutation of A; because A> = P7 AP, 
where 
1 
0 
Piss les |e5 | e4 | e3 | e1| = 
) 
(2.2.12) 
0 
0 
is the permutation matrix obtained by permuting the columns of I according 
to the permutation (1,2,3,4,5) > (5,1,4,3,2). This observation is typical of 
what happens in general. 
ooocdr.S 
et 
i 
=) 
OH 
S86 
oor 
oo 
e 
Asymmetric permutation 
P? AP of an adjacency matrix A for a graph G 
is the adjacency matrix for the graph that is identical to G except that the 

2.2 Elementary Matrices and Echelon Forms 
145 
nodes are relabeled according to the permutation defined by the columns in 
P, or equivalently by the rows in P?. 
Reversing Elementary Operations 
An important consequence of Theorem 2.2.2 on page 138 is that all row and 
column reductions can be reversed. To be precise, if A is reduced to B_ by 
a sequence of row and column operations then there are nonsingular matrices 
P =P, -::-P2P; and Q = Q;Q2---Q; composed of elementary matrices P; 
and Q; corresponding to the respective row and column operations such that 
PAQ = (P;,---Pi)A(Qi -+-Q:) =B. 
Theorem 2.2.2 guarantees that P and Q are nonsingular, so 
AP Ra BOile = (PP BOs, 
In other words, the reduction A —> B by the row and column operations asso- 
ciated with the P;'s and Q;'s can be reversed to reduce 
B —+ A by applying 
the elementary operations corresponding to the P; 1s and O- 's. In particu- 
lar, reduction by row operations is reversible—i.e., 
A—>3>B = 
B— 
SA. 
Furthermore, it is clear that 
A —> B and B —>C 
guarantees 
A —> C. 
This is called transitivity. Combining reversibility with transitivity yields 
A-—>B 
and 
A—SC = B-—SA 
and 
A-—SC 
row 
(22513) 
=> B—>C. 
Similar facts hold for column reduction as well as for as mixed reduction. These 
observations justify using the word equivalent | in the following discussion. 
Equivalence 
e 
In general, when A is reduced to B by any sequence of row operations, the 
process is equivalent to a multiplication PA = B, where P is the product 
of the respective elementary matrices. This is often expressed by saying that 
A is row equivalent to B and by writing A '~ B. 
e 
Similarly, when B is obtained from A by column operations, the process is 
equivalent to AQ = B, where Q is the product of the elementary matrices 
corresponding to the respective column operations. In this case we say that 
: 
: 
l 
A is column equivalent to B, and we write A ~ B. 
e 
If both row and column operations are used to reduce A to B, then the 
process can be represented as a product PAQ = B. In this case we say that 
A is equivalent to B, and we write 
A~ B. 
Formally, an equivalence relation on a set S is a binary operation ~ between the elements of 
S such that the following three properties hold for all A,B,C € S. (1) 
A~A 
(reflexivity), 
(2) 
A~B => B~A, (symmetry), (3) [A~B and 
B~C] = 
A~C 
(transitivity). 

146 
Chapter 2 
Systems, Elimination, and Echelon Forms 
For example, transitivity of row reduction can be restated by writing 
A''B 
and 
BAC = AC, 
(2.2.14) 
and (2.2.13) can be rewritten as 
A' B' and A' CO => B*'C. 
(2.2.15) 
Row Operations Preserve Column Relationships 
Look back at the example in (2.1.16) on page 130, and notice that the column 
relationships in Ea are exactly the same as those in A. That is, Ex2 = 2E,1, 
Ey3 = 2Ey1 + Ex3, and Ayo = 2Ay1, 
Asz3 = 2Au1 + Aug. This agreement is 
not an accident nor does it have anything to do with E, being an echelon form. 
It is simply a consequence of the somewhat unintuitive fact that row operations 
preserve column relationships. 
2.2.5. Theorem. If Ayn ~ Brmxn, then the column relationships 
in B are exactly the same as those in A. More precisely, for scalars 
{B1, B2,..-,Bn} it is true that 
nm 
mr 
Ag =) SA > Bay = 3} OB 
(2.2.16) 
k=1 
k=1 
Similarly, if A - B, then A and B have the same row relationships. 
Proof. 
If A'*" B, then there is a nonsingular matrix P such that PA = B. 
Use the fact that B,., = PA. for each k (by Theorem 1.7.3 on page 58) to 
conclude that if Ay; = S>7_, BeAr, then 
Byj = PAs =P) BrAck = >~ BrP Ask =~ Bi Bur. 
k=1 
Ka 
k—t 
Similarly, B,; = See oR => Axj = AS, GeAxp because P-!'B = A. 
The fact that A and B have the same row relationships when A 2B follows 
by considering transposes. 
Theorem 2.2.5 is important because it is often the case that when A is 
row-reduced to an echelon form E, the simplified structure of E readily reveals 
its column relationships so that E can be used as a guide to discover hidden 
relationships in A. Moreover, column relationships are even more apparent in 
a reduced echelon form E,. The following example makes this clear, 

2.2 Elementary Matrices and Echelon Forms 
147 
Column Dependencies 
To discover the column dependancies in A = 
, apply Gauss— 
rFPwonN 
re [oon 
a) BO 
bo Coro 
MD 
Ww Wane 
Jordan reduction to produce the reduced echelon form Ea shown below. 
oe 2 eta 
Sink De as 
ye 
eee gare he 
Ae 
eae etl lor sO) 0 Op seet ae 0: @) 2-2 
S270, Ge ome 
es Cali eee 
Ry Sabena 
12 hd 
3 
ait, ee ap Poly, 
Dep -OSO eG 
Gime 
2 
ens 
0 
0 
liga 
0 
0 
rie shai 
ilps @ ile 
eee 
are 
(2.2.17) 
ON OPO 
0 
Grt0ue 
On woo 
Cae are is = 
(Owe. 
OP Io 
es dee as 1" | 
Paea| Oe) eicawe 
ne 
Ger OqwO nO me) 
O GelielhOmmder() 
0 
Ome 00m 0 
Oye 
yO: ORO 
Since the three pivot columns in Ey 
are the three unit columns, the relationships 
E,o = 2Ey4 
and 
E,4 = Ext sF E.3 
in Ea are obvious by observing that the multiplier 2 in the first equation 
appears explicitly as the first entry in E,2, and the two multipliers in the second 
equation (namely 1 and 1) are the first two entries in E,4. Theorem 2.2.5 says 
that the column relationships in A and Ea must agree, so 
Axo = 2Axi1 
and 
Ama = Axi == A,3. 
Thus Ea 
isa perfect guide to the column relationships in A. These observations 
follow as a corollary to Theorem 2.2.5, and a general statement can be formulated 
as follows. 
Basic Columns 
As illustrated in (2.2.17), Theorem 2.2.5 ensures that non-pivot columns are nec- 
essarily combinations of the pivot columns. In other words, the pivot columns in 
A are more "basic" than the non-pivot columns in the sense that the non-pivot 
columns can be considered to be redundant because they contain no additional 
information beyond what is contained in the pivot columns. This motivates the 
following definition. 

Caution! A and Ea 
each has its own set of basic columns, and they occupy the 
same positions. But, as is evident in (2.2.17), their entries do not agree, so care 
must be exercised to specify which set of basic columns is under consideration. 
As the story unfolds it will be seen that the basic columns in A are of more 
interest than those in Ea, so unless otherwise made explicit when considering 
A and Eg (or any other echelon form derived from A), the phrase "basic 
columns" will generally refer to the basic columns in A. 
Generalizing the observation concerning column dependencies that is illus- 
trated in (2.2.17) is a direct corollary of Theorem 2.2.5 on page 146. The formal 
statement of this corollary in terms of basic columns is given below. 
ee 
: ] oe es Eun = Ex, + (2B. St 5 se + UpE xy, ee 
Gado 
2s Ae Hi Ax, + pa Ab, + 9 
Hp Abs: 
Echelon Forms 
Theorem 2.2.5 and Corollary 2.2.7 are the basis for rigorously establishing prop- 
erties of row-echelon forms that were observed earlier from only an intuitive point 
of view. The following four theorems 'concerning echelon forms are the founda- 
tion for subsequent theoretical developments. The first of these concerns linear 
independence (and dependence) among the rows and columns in an echelon form. 
Recall from Definition 1.3.3 (page 18) that a set V = {Vi,V2,-.-,Vr} is linearly 
independent when the only solution for the a;'s in Q1V,+Q2V2+:-:-+a;,v, =0 
Rigorous proofs for these theorems are included for the sake of completeness, but their formality 
may be omitted from a first reading by simply examining the issues on some generic examples. 

2.2 Elementary Matrices and Echelon Forms 
149 
is the zero solution aj = a2 =-::=a,=0. This is equivalent to saying that 
no vj; can be expressed as a linear combination of the other vectors in V. 
By looking at a typical echelon form E such as the one in (2.1.14), it 
seems apparent that the stair-step structure of E prohibits a nonzero row from 
being a combination of the others. A similar observation can be made about 
the pivot columns in E. In other words, the set of nonzero rows and the set of 
pivot columns in an echelon form each appear to be linearly independent. The 
following theorem makes this rigorous. 
2. 
2. 8. Thesrcol tt Bot 
is an echclen. fo m that has ae T nonzero 
d. 
hence ae pivo columns), the ach of the following is true. 
arly independent. @. 2 18) 
oe "The set of r pivot columns in E is 
lin 
¢ 
The set of r nonzero rows in E is linearly independent. 
(2.2.19) 
® 
Each non-pivot column in E isa combination of the pivot columns 
to its left-hand side. 
(2.2.20) 
Proof of (2.2.18). 
Given an echelon form E whose pivot columns are in posi- 
tions {p1,p2,-..-, Pr}, let Eg be the reduced echelon form obtained from E by 
using row operations to make each pivot a 1 and by annihilating entries above 
these pivots so that the set of pivot columns in Eg is the set of unit vectors 
{e1,e2,...,@-}, which is linearly independent. Since E '~' Em, Theorem 2.2.5 
guarantees that the set of pivot columns in E must also be linearly independent. 
Proof of (2.2.19). 
Let {Ey.,Eo.,...,E,-.} be the nonzero rows in E. The goal 
is to show that the only solution fol the a;'s in Pg phes 
, %E;, = 0 is the zero 
solution a, = 
= a, = 0. Let the first nonzero entry in each Ej, be ejp, (ie., 
Cn 
Os the th pivot). Since e€1p, is the only nonzero entry in column Exp, , 
it follows by focusing on the p, entry in each row that 
r 
r 
DES Gis 
=> 
T= We oy Sion 
= > (0'= 04. 
Since the only possible nonzero entries in Ey», are €1p, and €2p,,, focusing on 
. 
re 
. 
. 
entry po in 
O= )),_, aiE;, and using a; = 0 yields 
0= icin, = A1€1p. = QUE 
SP 
= CYNE a, re 
QE aor 
i=l 
Successively applying this same logic to columns Ex»,,Exp,,...,Exp, 
yields 
O— 
az 
a2 
--- = a, (Exercise 2.2.19), and thus {E,,,Eo.,...,E,.} 
is 
linearly independent by Definition 1.3.3. 

150 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Proof of (2.2.20). 
This is a direct consequence of Corollary 2.2.7 because each 
non-pivot column in the reduced echelon form Eg is a combination of the pivot 
columns (unit vectors) to its left-hand side, and since E'\" Ep, Theorem 2.2.5 
ensures that the same must be true in E. 
While Theorem 2.2.8 guarantees that an echelon form E has at least r 
independent columns (the pivot columns), it leaves open the question of whether 
or not E can have more than r independent columns (perhaps involving non- 
pivot columns). The next theorem says "no," this is not possible. 
pos f 
there 
ae are ceo ge poneee rows in an oso form Le 
1ere are aes 
i a eapuee columns i 
in E 
| mans is s not 
t 
necessarily unique—see be ue 2: 2. 18. 
Proof. 
If r=n, then there is nothing to prove because Theorem 2.2.8 ensures 
that the set of all columns is linearly independent (since each column is a basic 
column). If 
r<n, let F = {f1, fo,...,f-41} be any set of 
r+1 columns from 
E. If 
it 
UR 
i} # 
uj; 
hee 
f 
B=( 0 jh then 
i= (3) 3%, 
_, for each he 
Since the zeros at the bottom have no effect on dependence or independence, it 
suffices to consider the set {uj, U2,...,U,+41}. In other words, 
Cex rt 
= uy Woes Up+1| 
and = Fmxr4i = [fi fo --: f,+1| 
have identical column relationships. Since C'S" Ec, the column relationships 
in C are the same as those in Ec by Theorem 2.2.5. However, Ec has at 
most r nonzero rows, and hence it has at most r pivot columns, so at least one 
column—say ny—in Ec is a non-pivot column, and Theorem 2.2.8 ensures 
that ng must be a combination of the pivot columns in Eg. Consequently, 
because the column relationships in C and Ec are the same, it follows that 
ux is a combination of the other columns in C, so f, must be a combination 
of the other columns in F. In other words, F must be a linearly dependent set. 
Therefore, any set S of more than r columns from E must contain a linearly 
dependent set, and thus S must also be linearly dependent (see Exercise 1.3.8 
on page 20). 
By row-reducing a few matrices A to echelon form E, it seems rather 
intuitive that while the entries in E need not be uniquely determined by A 
(Exercise 2.1.17), the "form" (i.e., the stair-step structure of E as illustrated in 
(2.1.14) on page 128) is uniquely detanhinedl Below is a formal proof. 

Proof of (2.2.21). 
If 
A'®" E and A'S" F, where E and F are in echelon 
form, then E '© F by (2.2.15) on page 146. Therefore, if E and F respectively 
have exactly r and s nonzero rows, then E and F respectively have exactly 
r and s linearly independent columns by Theorem 2.2.9. Since the column 
relationships in E and F are the same by Theorem 2.2.5, we must have r= s. 
Proof of (2.2.22). 
Assume A #4 0 (otherwise there is nothing to prove), and 
let A'X" E and A' F, where E and F are in echelon form. By (2.2.21), 
E and F have the same number of basic columns—say there are r of them. 
There is no loss of generality by assuming that E and F are each in reduced 
echelon form so that the set of basic columns in E and F is the set of unit 
vectors {e1,€2,...,e,}. Since E'*" F (see (2.2.15) on page 146), the position 
of the first pivot column in E and F must agree for otherwise it would be 
impossible to produce E or F from the other by row operations. Proceed by 
induction. Suppose that the positions of first j pivot columns in E and F 
agree, say they are in positions p1,p2,...,p;, but the (j + 1)-st pivot columns 
in E and F are in respective positions pj+; and qj41 with qj41 > pj41. 
Hence Exp,,, =@j+1, and Fy »,,, 
is a non-pivot column in F so that Fxp,,, 
is a combination of the pivot columns {F yp, = €1, Fp, = €2, ---, Fap; =e;} to 
the left of Fxp,,, (see Corollary 2.2.7). Theorem 2.2.5 ensures that the column 
relationships in E and F are the same, so Ex»,,, = ej+1 must be a combination 
of vectors in {Exp, = €1, Exp, = €2, ..., Exp, = ej}, which is impossible. Thus 
Fypj41 = Expjy. = €j+1, and it follows by induction that all pivot positions, and 
hence all basic columns in A, are uniquely determined. 
Proof of (2.2.23). 
If A'*" E, where E is in row echelon form, then the set of 
pivot (basic) columns in E is linearly independent (Theorem 2.2.8, page 149), 
and the column relationships in A are the same as those in E (Theorem 2.2.5, 
page 146). This means that the set of basic columns in A must also be linearly 
independent. 
Mf 

152 
Chapter 2 
Systems, Elimination, and Echelon Forms 
The final theorem in this sequence of results concerning echelon forms es- 
tablishes the fact that both the structure-and the entries in the reduced echelon 
form Ea, are unique. 
2.2.1, Theorem. Eq is uniquely determined by A. 
Proof. 
Assume A # 0 (otherwise there is nothing to prove), and let A '~ E 
and A '\" F, where E and F are in reduced echelon form. Each has the 
same pivot columns {e;,@2,...,e,} in the same positions {pi,p2,...,pr} by 
Theorem 2.2.10, so the pivot columns Ey,, = e; = Fy; 
(1 <j <r) are 
uniquely determined. If E,, and Fy, are respective non-pivot columns in E 
and F having q pivot columns to the left of each of them, then 
QI 
Br 
: 
qd 
. 
qd 
Ex - = 
aA 
= Se 
AjE xp; 
and 
Kir = 
" 
a Sn By F sp; ce 
j=l 
j=l 
0 
0 
Since E '\' F (by (2.2.15) on page 146), Theorem 2.2.5 ensures that the column 
relationships in E and F are the same, so a; = 8; for each j. Therefore, 
Ex. = F., for all non-pivot columns, and thus 
E=F. 
& 
Testing Row Equivalence 
The result of Theorem 2.2.11 removes ambiguity from the following simple way 
to test if two matrices of the same size are row equivalent. 
2.2.12. Theorem. For A,B <¢F*", A' B if and only if Ea = Ep. 
Proof. 
Since Ea, '~" A and B'~' Ep, the transitivity of row equivalence as 
TOW 
expressed in (2.2.14) and (2.2.15) on page 146 ensures that if A '<" B, then 
Ex '~ AY'B''ER 
=> Ea' Ep. 
Conversely, if Ea pei Ep, then 
TOW 
TOW 
TOW 
A~E, ~EgprB 
=> A.B. 
SE 

2.2 Elementary Matrices and Echelon Forms 
153 
Example (Extending Linearly Independent Sets) 
A frequent issue that arises in both theory and practice concerns the problem 
of extending a given set of r <n vectors {x1,X2,...,x-} CF" to a maximal 
linearly independent set {x,,x2,...,X,} C F" containing n vectors. There 
are several ways to extend linearly independent sets, but a particularly simple 
way is to place the r given vectors as columns in Xnyx- = [k1|Xe2| --- |X,], 
and then adjoin the n x n identity to create the n x (r +1) augmented matrix 
A = [X|I], which necessarily has rank(A) = n (the last n columns are 
linearly independent). Since the column relationships in A are the same as 
those in Ea (by Theorem 2.2.5, page 146), the first r columns in E, are an 
independent set, and hence they are pivot columns, so E, must have the form 
Ea = 
[e; | e2 | 
a 
|e, | Caxnl- 
The n basic columns in A are therefore those in X together with the remaining 
pivot columns Cp,,Cp,,-.-,C€p,_,) 
mm C. Thus 
1X7, Koper 
eam} Wen, Cp 
ee epee y 
provides an extension of the columns in X to a maximal set of n independent 
columns in F". 
1 
=F 
For example, to extend the independent set @ 
: ( 
to a maxi- 
-1 
1 
mal independent set of four vectors in R*, execute the reduction 
i 1 at) 08010 
moo 
bo <1 
\peeieor to. 0 
Sree 
Gin ae 
Saletan 
1s oleate 
sODOMIR GO Ot. |s 
rT 
een 
(e000 
eat 
and note that the pivot positions are 1, 2, 3, and 5 so that the basic columns 
in A are {Ay1, Axo, Ax3,Ax5}. Therefore, a linearly independent set of four 
vectors that contains the two given vectors is 
1 
alll 
1 
0 
0 
1 
0 
0 
ak 
J} 
aL 
Be 
(@) 
PY 
1 
= 
1 
0 
0 
Exercises for section 2.2 
1 
1 
1 
2.2.1. Write A = (-2 —1 -1) as a product of elementary matrices. 
3 
—2 
3 

154 
Chapter 2 
Systems, Elimination, and Echelon Forms 
2.2.2. Determine which of the following sets are linearly independent. For those 
sets that are linearly dependent, write one of the vectors as a linear 
combination of the others. 
 {(2) @) @)} © 1G) G) GY 
(Gilg 
erie 
Ua 
BERG 
rg eu 
ee 
Kiyo 
2 ey, (2 2 C2) Lee ae 
(e) 
OWORONFH 
OWrFRBRONO]O 
OWORFRNO 
rFwWwOoOrRONO 
2 
1 
1 
2.2.00, Let A = (-: —4 
1). 
2 
ao 
LL 
(a) Apply only elementary operations of Type III to reduce A to 
an upper triangular matrix U, and record the individual ele- 
mentary matrices that are used. 
(b) By considering the inverses of the elementary matrices used in 
part (a), find a lower triangular matrix L such that A = LU, 
where L has 1's on its diagonal. 
2.2.4. Consider a particular species of wildflower in which each plant has several 
stems, leaves, and flowers, and for each plant let the following hold. 
S = 
the average stem length (in inches). 
L = 
the average leaf width (in inches). 
F = 
the number of flowers. 
Four particular plants are examined, and the information is tabulated 
in the matrix 
thal 
ial 
#1 
/1 
1 
10 
#9. 
{2° 
1 
12 
A= 43] 
2 
2 
15 
Ha \ Soe? 
For these four plants, determine whether or not there exists a linear rela- 
tionship between S, L, and F. In other words, do there exist constants 
Qo,Q1,Q2, and ag such that ag +a,S +aghL+a3F =0? 
—

2.2 Elementary Matrices and Echelon Forms 
155 
2.2.5. 
2.2.6. 
Demet « 
2.2.8. 
2.2.9. 
Let A,B e F™*". 
(a) Explain why A °'B if and only if Ear = Epr. 
aed. 
0 ai 
2 ah 
So 
(b) Are A = (: 
—1 
4 | and B = (; [4 -i) row 
pms 
regs 
2 faom2e 
Gs 
equivalent? 
(c) Are the matrices in part (b) column equivalent? 
Verify equation (2.2.5)—i.e., show that if v'u #1, then 
uv! 
(I—uv') 
aI ak 
Verify that if E,, E2,E3 are the respective elementary matrices of Types 
I, II, and III, given in (2.2.1) on page 136, and if e; denotes the i" unit 
column, then E; 
= I—uu', where 
u=e;—e€2, Eo = I—(1—a)eged, 
and E3 =I+ ae3e} . Now formulate generalizations of these formulas 
for n x n elementary matrices of Types I, I, and III. 
Identify the basic columns for each of the following matrices, and then 
express each non-basic column in terms of the basic columns. 
Path 
al. Oe ey 
ot kal 
i A 
vee 
26s 
aE eater 
A=(2459) B=|2 60 
Ce 
ne eer 
27 EnaT 6 
h 
2 & 
ee 
TO 
8 
= 
@ 
@ 
8 
pee 
aoe Rea Ms Baa 
i 
0) 
al 
Let EK = (0 1 
2). You already know that E,3 can be expressed in 
0 
O 
0 
terms of E,; and E,2. However, this is not the only way to represent 
the column dependencies in E . Show how to write E,; in terms of E,2 
and E,3 and then express E,2 as a combination of E,; and E,3. 
Note: This exercise illustrates that the set of pivotal columns is not 
the only set that can play the role of "basic columns." Taking the basic 
columns to be the ones containing the pivots is a matter of convenience 
because everything becomes automatic that way. 

156 
Chapter 2 
Systems, Elimination, and Echelon Forms 
2.2.10. Construct a matrix A whose reduced row-echelon form is shown below. 
Is A unique? 
ae a 
pat BA 
Gr 6.) =&.0 
7°6 
pie 
jc 9 9 Syeog 
ee 
Othe OL 
hes 
Gc0-0 
\Oohs0-e 
oo Oo 
68 0.0 8 
1 
0 
2.2.11. Extend the set 
( 
3} a to a maximal linearly independent 
1 
set in R*. 
2.2.12. For each of the following matrices A find elementary lower-triangular 
matrices T; and T> such that T2T,;A = U, where U is upper 
triangular, and use T; and T2 to factor 
A as 
A=LU, where L is 
lower triangular having 1's on its diagonal, and U is upper triangular 
with the pivots on its diagonal. 
Pe 
ay 
2 
1 
1 
(a) A=(4 7 "| 
(b) A=(-4 —4 
1) 
6 
187 
22 
2 
08 Ut 
2.2.13. Determine the elementary lower-triangular matrix T,; that T,A anni- 
hilates all entries below the first pivot in 
ie 
PwN 
oe 
© 
bo Oop 
Ww NOR 
Now find the elementary lower-triangular matrix T2 such that T.2T,A 
executes the first two steps of Gaussian elimination. 
2.2.14. 
(a) 
Are all permutation matrices involutory? Why? 
(b) 
Are all permutations symmetric? Why? 
2.2.15. Prove that if B is 
asymmetric permutation of A, x», then the diagonal 
entries in B are the diagonal entries of A except that they have been 
permuted. 
2.2.16. If A is the adjacency matrix for an undirected graph, must A be 
symmetric? Why? 

2.2 Elementary Matrices 
2.2.17. 
2.2.18. 
2.2.19. 
2.2.20. 
2.2.21. 
2.2.22. 
2.2.23. 
and Echelon Forms 
157 
Let A be the adjacency matrix for Graph #1 on page 144 and let P 
be the permutation matrix in (2.2.12). Show that P7AP 4 PAP', 
and then draw the graph of PAP? to note how the nodes become 
renumbered. Explain this renumbering in terms of the structure of P. 
Construct an echelon form E3,4 with exactly two nonzero rows and 
exactly five different linearly independent subsets of two columns. 
Supply the third step in the proof of (2.2.19) to show that a3 =0. 
TOW 
. 
. 
. 
If A ~ E, where E is an echelon form having r nonzero rows, explain 
why A must have a set of r linearly independent columns. Is such a 
set unique? 
(a) 
If A '©' E, where E is in echelon form, what conditions on 
E will guarantee that the set of all columns in A is linearly 
independent? 
(b) 
Ifthe set of all columns in A, 
x» is linearly independent, what 
must Ea, be? 
Express the non-pivot columns in A = 
as linear com- 
NEE 
EN 
Oot 
Nae) Oowor 
He OD 
0 NI 
OU 
binations of the pivot columns. 
Let 
A bea 3X6 matrix in which Ay3 = Axi +2Axu2, Axa = —Axa, 
and Axs = Axi aa SAGO 
(a) Find a reduced row-echelon form that can be produced from A 
by row operations. 
(b) Is the reduced row-echelon form you found in part (a) unique, 
or is there another? Explain why. 

158 
Chapter 2 
Systems, Elimination, and Echelon Forms 
2.2.24. Hermite Form. The reduced row-echelon form E, for a square matrix 
Anxn_ is closely related to its Hermite form} which is derived by row 
reducing A to an upper triangular matrix Ha such that each diagonal 
entry hy of Ha is either a one or a zero, and 
hii sah 
H,; = &, 
In other words, PA = Ha, but Ha is a permuted version of Ea, in 
which the pivot rows have been interchanged so that each pivot (which 
is a one) is on the main diagonal of 
Ha. For example, if 
Leen 
GG 
i 
e2ar0 
Le OZ 
A=(2 
4 '), 
then Ea, = (0 
0 1) 
and "Hate 
1.0 OO. 
3) 
Orend 
OOO 
OF 
OPEL 
Prove that every Hermite form is idempotent—i.e., Hi = Ha. 
2.2.25. Rank Normal Form. For A € F"%*", let r be the number of nonzero 
rows in Ea. Explain why there are nonsingular matrices P ¢ F'*™ 
and Q € F"*" such that 
rsa (B5 8) =m 
Matrix Na is called the rank normal form for A. It is the simplest 
form obtainable by using both row and column operations to reduce A. 
Since there is only one reduced row-echelon form that can be produced 
from A by row operations, it follows that the rank normal form Na 
is 
uniquely determined by A. 
2.2.26. 
(a) 
Find a nonsingular matrix P such that PA = Ea, where 
Teo MIg 4 
A=(246 7), 
1223 
4G 
(b) Find nonsingular matrices P and Q such that PAQ =N 
is 
in rank normal form as described in Exercise 2.2.25. 
In some older books this is called the Hermite normal form in honor of the French mathemati- 
cian Charles Hermite (1822-1901), who, around 1851, investigated reducing matrices by row 
operations. 

2.3 Nonsingular Matrices and Inverses 
159 
2.3 NONSINGULAR MATRICES AND INVERSES 
This section presents basic facts and properties of nonsingular matrices along 
with some aspects surrounding the computation of matrix inverses. Given a 
square matrix A, a fundamental issue is determining whether A is singular or 
nonsingular. The following theorem is a first step in this direction. 
- 
2.3.1. Theorem. A square ma ix A is 
nonsingular if 
and only if Ba at : 
Proof. 
The proof rests on the observation that nonsingular matrices cannot 
contain a zero row because if N is nonsingular with N;, = 0, then formula 
(1.7.10) on page 58 for the i*" row of a product can be applied to conclude that 
Loa NNG a le NING NN 
OU 
which is impossible. Let P be a nonsingular matrix such that PA = Ea. If 
A is nonsingular, then PA is also nonsingular (Theorem 1.8.6, page 74), and 
hence Ea 
is nonsingular. Since nonsingular matrices cannot contain a zero row, 
Ea 
has no zero rows, so every row (and column) in Ea, contains a pivot, which 
must be a 1. Thus Eq, =I. Conversely, if Eg =I, then PA = Ey =I. Mul- 
tiplying on the left by P~! and on the right by P yields AP =I. Therefore, 
PA =I=AP, and hence P = A™! because inverses are unique (page 73). 
Thus A is nonsingular. 
By definition, matrices A and X are inverses of each other if and only 
if AX =I,x, = XA (page 72). Both equations are necessary to rule out the 
possibility of non-square inverses. However, the following corollary shows that if 
A and X are square, then only one of the defining equations is needed. 
2.3.2. Corollary. If A and X are square matrices, then 
AX=1<=]}XA =I. 
(2.3.1) 
In other words, A,» is nonsingular if and only if there exists a matrix 
Xnxn such that AX =I or XA =I, and ineither case, 
X = A7!. 
Proof. 
Let P bea nonsingular matrix such that PA = Ka. If AX =I, then 
P = PAX = EaP. Nonsingular matrices cannot have a zero row, so KaP 
does not have a zero row, and hence Ea cannot have a zero row—otherwise 
E,P = P would have to have a zero row. Consequently, Ea = I, so Aa 
exists by Theorem 2.3.1, and thus multiplying AX = I on the left by At 
yields X = A~', which implies that XA = I. Conversely, if XA =I, then 
ATX? = I" =T 
(by the reverse order law), so the result from the first part 

160 
Chapter 2 
Systems, Elimination, and Echelon Forms 
eT 
eS 
wi 
nat 
of the proof yields X7 = (A") "Since (AT) 
=(A~*) 
(Theorem 1.8.5, 
page 73), it follows that 
Xx? = (AT) + =(A7)' => X=A7) =» XA=I 
Bf 
Caution! The argument above is not valid for nonsquare matrices. If m # n, 
then it is possible that AmxnXnxm =Im, but XA #I,. For example, consider 
1/32 
1/2 
he 
Lb 
t=] 
1) 
ye (- 3 
0 ). 
A ( ; af and 
Temes 
A particularly important corollary of Theorem 2.3.1 provides the following 
useful characterizations of nonsingularity. 
2.3.3. Corollary. Each of the following statements is equivalent to saying 
that A, 
x, is nonsingular. 
e 
The set of columns in A is linearly independent. 
e 
The set of rows in A is linearly independent. 
e 
The only vector x, 1 for which Ax =0 is 
x=0 
(2.3.2) 
e 
det(A) #0 ("det" is determinant—see the appendix on page 939). 
In particular, it follows that if A is nonsingular, then every subset of 
columns and every subset of rows is linearly independent. 
Proof. 
Use the fact that row operations preserve column relationships (Theorem 
2.2.5, page 146) to write 
A-! exists —> A'S" 
I= E, <> columns of A are independent. 
Use this to construct the argument for rows by using transposes together with 
the fact that A is nonsingular if and only if A? is nonsingular (Theorem 1.8.5, 
page 73). Details are called for in Exercise 2.3.11. The fact that A is nonsingular 
if and only if the only vector x satisfying Ax = 0 is x =0 
is a consequence 
of Theorem 1.7.4 on page 59. The fact that A is nonsingular if and only if 
det (A) #0 is Theorem 9.2.6 on page 945 in the appendix. Subsets of columns 
and rows from a nonsingular matrix are linearly independent because subsets of 
independent sets are also independent—see Exercise 1.3.8 on page 20. 
Diagonal Dominance 
The characterization of nonsingularity given in (2.3.2) is especially helpful in 
theoretical situations where nonsingularity needs to be established. The following 
discussion concerning diagonal dominance illustrates how (2.3.2) is used. 

raeGt Secig 2 
- One of the most important facts concerning diagonally Does teet matrices 
is the 
following theorem presented by Hermann Minkowski (page 38) in 1900. 
Proof. 
Use (2.3.2) with an indirect argument. Suppose that there exists a vector 
x #0 such that Ax = 0. Let x, be the entry of maximum magnitude in x, 
and focus on the k'" component of Ax to observe that 
Ax 
20. 
f==> (Ax)x Ay X03 
=> 
Opeth = — Ss 
Anj@;. 
j=l 
JAR 
Taking absolute values of both sides and using the triangle ubadinlig? together 
with the fact that |x;| <|a;,| for each j Rasa 
n 
n 
Dawe Daa ee ane 
(> 
laxs| 
level: 
j=l 
j 
j#k 
js! 
in 
tek 
Jj#k 
j#k 
laxk| |tx| = 
But this implies that |az%| <5 }=1 |ax;|, which violates the hypothesis that A 
jzR 
j 
is diagonally dominant. Therefore, the assumption that there exists a vector x 
such that Ax = 0 must be false, and thus A is nonsingular by (2.3.2). 
For example, to appreciate the value of diagonal dominance, consider 
A=(0s 1). 
(2.3.3) 
0) 
8) 
It is apparent just by inspection that A is diagonally dominant, and hence no 
computation is required to conclude that A is nonsingular. 

observation that the coefficients a; must satisfy the equations _ 
, 
ao +0121 +0922 +--+ +Om—i2y) =e(m1)= 41, 
ao + a1r2 + A223 +--+ + Om—123°* = (22) = ye, 
a9 + a1tm + 0227, +:+++Qm-12m | 
= (tm) = ym, 
or equivalently, 
1 ay 
betg ee 
ao 
yn 
m—1 
1. 
zh 
a 
_ | # 
(2.3.5) 
i 
Tes, 
x2, 
ante 
zn 
Qm-1 
Ym 
mxXm 
The coefficient matrix is a square Vandermonde matrix V as defined in (1.7.12) 
on page 60, where it was argued that the only vector x for which Vx = 0 is 
x = 0. In other words V is nonsingular by (2.3.2), so the Vandermonde system 
(2.3.5) has a unique solution for the a; 's—i.e., there is only one possible set of 
coefficients for the polynomial @(t) in (2.3.4). It turns out that &(t) is given by 
"=> eel cas 
(2.3.6) 
i=1 
[jaz — 23) 
which can be verified by showing that this is indeed a polynomial of degree 
m —1 that passes through each point in S. This polynomial ¢(t) is called the 
Lagrange' interpolation polynomial of degree m— 1. 
Joseph Louis Lagrange (1736-1813) was born in Turin, Italy and is considered by many to be 
one of the two greatest mathematicians of the eighteenth century—Buler is the other. Lagrange 
occupied Euler's vacated position in 1766 in Berlin at the court of Frederick the Great who 
wrote that "the greatest king in Europe" wishes to have at his court "the greatest mathemati- 
cian of Europe." Lagrange left Berlin after twenty years and ended up in France. Lagrange's 
mathematical contributions are wide and deep, but he had a particularly strong influence on 
the way mathematical research evolved. He was the first of the top-class mathematicians to 
recognize the weaknesses in the foundations of calculus, and he was among the first to propose 
a rigorous development. 

2.3 Nonsingular Matrices and Inverses 
163 
Computing an Inverse 
Gauss—Jordan reduction is a straightforward way to determine if a square matrix 
A is nonsingular, in which case it provides 
a method for computing A~!. As 
discussed on page 138, Gauss—Jordan reduction of [A | I] produces [I| P], where 
P is a nonsingular matrix such that PA = Ea. Theorem 2.3.1 ensures that A 
is nonsingular if and only if 
E,4 =I, in which case A~! = P. Therefore, 
Gauss—Jordan 
[A | I] ————> [I|A~'] 
whenever A is nonsingular. 
(2.3.7) 
If A is singular, then Ea #1, so Ea must have at least one zero row. This 
situation is exposed during the Gauss—Jordan reduction of [A | 
I], and thus the 
process reveals whether or not A is singular as well as providing A~! via (2.3.7) 
when A is nonsingular. These observations are summarized below. 
2 
5 < Theorem, Cnale applving Gauss-J ae ee iG ia 
| 1. 
Gauss-J onder 
e If A is nonsingular, then [AL ] Ce! [A> a 
-e 
If A is singular, then a zero row must emerge on 'he left- 
hand cide. 
of ue augmented array. 
Example 
Tage 
aL, 
Te find "A ** for A i= (: 
2 a), reduce {A |I] as follows. 
eS 
aa 
1 
ds 
wah rs Oye) 
leet 
L020 
Ain=(2 Dg 
Felts 0) + (s 
Naas, 
Ret 
Paves es 0) 
1 
Mad 
Like Lyon 
Get tee 
yet 
100 
2 -1 
0 
1 
0 
2° 
219 
0 
(0 Weil ele salateved oj (0 ER 
Bare 
Sy, -1) = a7 
Ce 0nd 
0 -1 
1 
0 
1 
Oo -1 
f 
Note that the reduction shows that A is nonsingular because Eq, = I. To 
Oy a 
verify that A7! = (= 2 +), it suffices to check that AA~' =I because 
Os! 
Corollary 2.3.2 insures that A~1A =I will then automatically be true. 
Example (Inverses of Triangular and Diagonal Matrices) 
As an example of the utility of some of the previous results, consider triangular 
matrices T = [t;;] (either lower or upper). Theorem 2.3.1 guarantees that T' is 
nonsingular if and only if Ey =I. This leads to the conclusion that 
e 
T is nonsingular if and only if each t,, 4 0. 
(2.3.8) 

164 
Chapter 2 
Systems, Elimination, and Echelon Forms 
In such cases applying Gauss—Jordan reduction to [T|I] + Ejrjy = (I | X} 
yields 
X = T~! by Theorem 2.3.6. Executing this reduction for a 3 x 3 upper 
tir 
ti2 
tis 
(or lower) triangular matrix such as T = 
: ta tas 
shows that 
33 
e 
T-! is also upper (or lower triangular) with [T~'],; = 1/tii. 
(2.3.9) 
In particular this means that a diagonal matrix D = diag (dit, dz2,.--;4nn) is 
dia 
d22 
; 
nonsingular if and only ifeach d;; 4 0, in which case D-*= 
dan 
Flop Count for Inversion 
It is instructive to know roughly how much computational effort in terms of 
flops is required to invert a matrix compared to that required to solve a system 
of linear equations by row reduction methods. 
"Matrix Inversion Flop Count. Computing A~! via Theorem 2.3.6 re- 
quires O(n) flops. The exact flop count involves additional second- 
order and first-order terms (Exercise 2.3.29), but for large values of n 
the lower-order terms are negligible compared to n°. 
Nonsingular Systems and Matrix Inversion 
Up to this point three methods for solving an n x n nonsingular system Ax = b 
have been given—Gaussian elimination with back substitution, Gauss—Jordan, 
and now by means of computing A~! and forming the product x = A~!b. The 
flop count required by Gaussian elimination with back substitution is O(n*/3) 
(page 125), while Gauss—Jordan requires O(n3/2) flops (page 126). Since ex- 
plicitly computing AW! to obtain the solution via x 
= A~'b requires O(n') 
flops! you can see why matrix inversion is almost never employed to numerically 
solve a large linear system—it requires about three times the effort as does Gaus- 
sian elimination with back substitution. In fact, it is relatively rare to encounter 
large problems in applied work that require explicit knowledge of the entries in 
A~!, so explicit matrix inversion is not often needed. 
From another perspective, consider standard matrix multiplication AB be- 
tween two n x n matrices. A little thought reveals that n° flops are required 
(Exercise 2.3.28). In other words it takes just about the same computational 
effort to perform one matrix multiplication as performing one large matrix in- 
version. This fact is counterintuitive to many people because it "feels" like ma- 
trix inversion should be the more expensive task—but not true! So, as far as 
The product A~'b only requires n? flops, so it is not significant compared to the O(n?) 
required to compute A}. 

2.3 Nonsingular Matrices and Inverses 
165 
large-scale numerical work is concerned, matrix multiplications as well as matrix 
inversions are both rather costly, so practitioners try to avoid them whenever 
possible. 
Finally, since Gaussian elimination with back substitution is more efficient 
than Gauss—Jordan for solving a single linear system, it is tempting to conjec- 
ture that Gaussian elimination also offers an advantage over Gauss—Jordan for 
inverting a matrix. In other words, A~! can be obtained by solving the matrix 
equation AX =I by applying Gaussian elimination with back substitution to 
solve the n linear systems [AX]..; = AX,; =1,; =e; for the columns Agi, 
which are the columns of A~!. However, the back substitution flop counts for 
each of these n systems accumulate to the point that the total flop count is 
the same as that when Gauss—Jordan is used. So, in terms of the computational 
effort required to invert a matrix, Gaussian elimination with back substitution 
and Gauss—Jordan are equivalent. 
Factoring Nonsingular Matrices 
A common theme throughout mathematics is to analyze objects by factoring 
or somehow decomposing them into simpler components to gain some kind of 
advantage—e.g., we try to factor polynomials into products of polynomials of 
lower degree, and we like to factor large integers into products of prime numbers. 
The same goes for matrices. Much theory and many applications revolve around 
factoring matrices into products of simpler components. The following theorem 
shows that nonsingular matrices are those that have "elementary" factorizations. 
2.3.7. Theorem. A square matrix A is nonsingular if and only 
if A is 
the product of elementary matrices. 
Proof. 
If A is nonsingular, then Gauss—Jordan reduces A to I by row opera- 
tions. If P;,Po,...,Px is the sequence of elementary matrices that corresponds 
to the elementary row operations used, then 
P,---PoPiA=I = 
A=P)'P;'---P;', 
which is a product of elementary matrices by Theorem 2.2.2. Conversely, if A is a 
product of elementary matrices A = P;P2:--P,;, then A must be nonsingular 
because the P;'s are nonsingular (Theorem 2.2.2), and a product of nonsingular 
matrices is nonsingular (Theorem 1.8.6, page 74). 
& 
A simple corollary of Theorem 2.3.7 nicely ties up the connection between 
equivalence and matrix multiplication. In particular, it was shown on page 145 
that if A '~" B, then PA = B for some nonsingular matrix P, but no mention 
was made concerning the converse. The following corollary provides the converse 
for this statement as well as for statements concerning column equivalence and 
two-sided equivalence. 

Proof. 
If PA = B for some nonsingular matrix P, then A '~" B because 
P =P,P2---P, is a product of elementary matrices, each 
of which executes 
an elementary row operation under left-hand multiplication. The other aspects 
of the corollary follow similarly. 
Block Inversion and Schur Complements 
A, 
ag 
Cc, s 
. 
. 
Occasionally, all or part of the inverse of a block matrix fs : = : ) 
in which 
A or B is nonsingular is needed. Formulas for such inverses are derived by 
executing block versions of Gaussian elimination. If A is nonsingular, then R 
can be annihilated by multiplying the first block-row by —RA7! and adding 
the result to the second block-row to produce 
Gee 1) & B)> (0 Bo 
ery 
or equivalently, by analogy with elementary matrices, 
I 
ONG 
(Ag 
GC. 
ee 
A: 
Cc 
—-RA-! 
I 
RB 
\0,B—RAa 4," 
Now, a block-column operation can be used to annihilate C. The result in terms 
of "block-elementary matrices" is 
Sree Ga 
aan 
SP Pie omierss 
where 
S=B-—RA™!C. If S is nonsingular, then using the facts that 
(idk 3)" (alert) aaa sfeadeta)ae (eee) 
(Exercise 2.3.8) together with the reverse order law for inversion leads to the 
block inversion formula 
(28) 
= (0 "$'°)(3 8) (aks 4) 
(0 )(*0 sm )(-ea-s 1) 
ie +A-~iCS-IRA-}. 
—A-ics-1 ) 
-S-'RA-1 
s-} 

2.3 Nonsingular Matrices and Inverses 
167 
Similarly, if B and T = A —CB™!R are each nonsingular, then analogous 
reasoning (Exercise 2.3.9) leads to 
Ly gehen 
T-1 
—-T-1CB-! 
RB 
VB 
RT) 
BL 
BART 
CBA): 
These observations are summarized in the following theorem. 
2.3.9. Theorem. The following inversion formulas hold when the indi- 
cated Inverses exist. 
; 
Pee 
is 
iL et 
A G\") 
(A*VAcS 
TRA a ics 
BBY 
7) 
9 
osrtnAtt 
gt 
AO, 
of 
rt 
tea 
R B 
Be RI 
Bo a) 
The terms 
S =B—RA7'C and T=A-— CB™!R are called Schur 
complements.' For a single column ¢ and row r* and ascalar 8, 
: 
: 
ae 
Avert AT 
: 
Avie 
A 
1 
( 
) . 
ie 
\ 
where 
s=6-—r*A'c. 
r 
s§ 
mle 
i Schur complements are named in honor of the German mathematician Issai 
Schur (1875-1941). Schur was a student and collaborator of Ferdinand Georg 
Frobenius (page 798). Schur and Frobenius were among the first to study ma- 
trix theory as a discipline unto itself, and each made significant contributions 
to the subject. Schur could never understand the Nazi preoccupation with his 
Jewish heritage. He was a German in his mind and not a Jew. Nevertheless, 
he was removed from his respected academic position in Germany and ended 
A 
up in Palestine, where he died in poverty on his 66th birthday. 
ose te 
It was Emilie V. Haynsworth (1916-1985), a student of Alfred Brauer (page 
766) and a mathematical granddaughter of Schur, who in 1968 introduced 
the phrase "Schur complement." She is famous for having developed many 
of the important aspects of the concept and popularizing it during her 
career at Auburn University. Haynsworth was only one of two women who 
were recognized by the linear algebra community to be outstanding female 
researchers of their generation—the other was her friend Olga Taussky 
(page 766). Each followed in the footsteps of their mentor Alfred Brauer. 
In 1965, Auburn University honored Emilie Haynsworth by elevating her 
to be one of the first recipients of the position of "Research Professor." 
E. HAYNSWORTH 

168 
Chapter 2 
Systems, Elimination, and Echelon Forms 
The Sherman—Morrison Formula 
There is no useful general formula for (A + B)~!, but there are some special 
sums for which something can be said. For example, it follows from (2.2.5) on 
page 139 that the inverse of 
I+ cd* in which c,d € F'*! with 
1+d*c 40 
is 
cd* 
Oe eT often 
(I+ cd") 
=I 
Tide 
(2.3.11) 
If I is replaced by a nonsingular matrix A satisfying 
1+ d*A~'c #0, then 
the reverse order law for inversion in conjunction with (2.3.11) yields 
(A+ed*)" = (A[.+ A~ted*}) "= [I+ A"Med"] 1A} 
at 
es 
A~'ed* 
TOR Coe Avtcd*ATT 
~ 
1+d*A-!c 
1+d*tA-lc 
This is called the Sherman—Morrison formula! It has a generalization known as 
the Sherman—Morrison—Woodbury formula that is given below. 
2.3.10. Theorem. (Sherman—Morrison formula) If A,.,, is nonsin- 
gular, and if c,d € F"™! are such that 
1+d*A~'c #0, then A+cd* 
is nonsingular, and 
A cd*A7* 
bod pet 
eed) 
Wee? ai 
(A+cd*) 
=A 
cai years 
(2.3.12) 
The Sherman—Morrison—Woodbury formula is a generalization 
saying that if C,D ¢ F"** are such that (I+ D*A~!C)~! exists, then 
(ACD) (=A) -ACO+D'A 
'C) D*A™. 
(03.13) 
Proof. 
The Sherman—Morrison—Woodbury formula (2.3.13) can be verified with 
direct multiplication, or it can be derived as indicated in Exercise 2.3.14. 
It is so named because it appeared in the 1949-1950 work of American statisticians J. Sherman 
and W. J. Morrison, but they were not the first to discover it. The formula was independently 
presented by the English mathematician W. J. Duncan in 1944 and by American statisticians 
L. Guttman (1946), Max Woodbury (1950), and M. S. Bartlett (1951). Since its derivation is 
so natural, it almost certainly was discovered by many others along the way. Recognition and 
fame are often not afforded simply for introducing an idea, but rather for applying the idea to 
a useful end. 

2.3 Nonsingular Matrices and Inverses 
169 
Updating an Inverse 
To appreciate the utility of the Sherman—Morrison formula, suppose that A~1 
is known from a previous calculation, but now one entry in A needs to be 
changed or updated—say we need to add a to aj;. It is not necessary to start 
from scratch to compute the new inverse because Sherman—Morrison shows how 
the previously computed information in A~! can be updated to produce the 
new inverse. Let 
c =e; and d=ae,, where e; and e; are the respective i" 
and j** unit columns. The matrix cd" has a in the (i, j)-position and zeros 
elsewhere so that 
B=A+cd* = A +aee; 
is the updated matrix. According to the Sherman—Morrison formula, 
- 
A-eerA* 
Bric (A 
aL nenO A ee 
PAueceie;) 
ii ae? Ale; 
(2.3.14) 
[AW «LA 
]x 
= Arb q 
(recall (1.7.15) on page 61). 
This shows how A~! changes when a;; is perturbed, and it provides a useful 
algorithm for updating A7?. 
Example 
Start with A = é a) and eA ee ee rar Update A by adding 1 to 
a21, and then use the Sherman—Morrison formula to update A~'. The updated 
matrix is 
0-(3)-( Do H( eC) or-asnat 
Applying the Sherman—Morrison formula yields the updated inverse 
B7! = Av} eater EEL a 
1+e7 A-leg 
Pai AS li 
i 
Sateen \ 
od ( 
1 
_{=3 
2 
a 
Ayes 2 od 
(=e. 
ay 
De 
a) 
Neumann Series 
Another special sum that often requires inversion is 
I-A, but such a sum is not 
always nonsingular. However, if the entries in A are small enough to guarantee 
that ||A|| <1 for any matrix norm (see Definition 1.9.3 on page 84), then, as 
the following theorem shows, (I—A)~! exists, and while there is no closed-form 
expression for it in general, (I — A)~! has a simple series expansion. 
Ateset At 
,-1 
[A *]«2[A7*]1 

5 ho i 
'iV IA ea 
forte 
i 
ope 
aeet ewroule 
SoeiinOl 
ail! aphnwy «) fo 
' 
1. VOT) GPE 
4 
Proof. 
If ||A|| <1, then Theorem 1.9.7 on page 88 ensures that A" — 0, so 
analogous to the development of the geometric series in scalar algebra, 
(I=A)(I+A+A72+.--4A"')=I1-A" 
>I as 
n- 00, 
and thus (I— A)" =I+A+A?+---= 
70, A" B 
First Order Approximations of (A + E)~! 
There is no general formula for (A + E)~! that is useful, but the Neumann 
expansion (2.3.15) allows something to be said when the magnitude of the entries 
in E is small relative to those in A. In particular, if A~! exists, and if the 
entries in E are small enough in magnitude to insure that || A-1E|| <1, then 
utilizing the Neumann expansion produces 
(A +E)" = (A(1- [-A"B)) )~ = (I- [-A"1B] hoe 
0° 
F 
(2.3.16) 
2 (>: 
[-A-1B] Jan. 
Consequently, neglecting higher order terms in the Neumann expansion yields 
the first-order approximation 
(AC Boe As a AT IGA ge 
(2.3.17) 
Continuity of the Inverse 
+ 
A simple determinant proof of the fact that the entries in the inverse A~! of a 
nonsingular matrix vary continuously with the entries in A is given in Theorem 
9.3.8 on page 963, but the result is also a consequence of the Neumann series 
expansion in (2.3.15). Here is the formal statement. 
Recall that if a is a scalar, then 
1+a+a?2+a3+4+.--. 
is called a geometric series, and it 
converges if and only if |a| <1, in which case we, oes] 
(cer) 

nsingular Matrices and Inverses 
171 
—-— 
: 
2.3 No 
& 
Proof. 
Since A~! is fixed and ||E|| — 0 (for any matrix norm), the discussion 
can be limited to E's such that ||A-7E|| <1 sothat (A+E)~! exists and is 
given by the Neumann expansion (2.3.16), which can be rewritten as- 
Co 
(A+E)'—A+=-A'EA) (-EA71)*, 
F 
k=0 
Taking norms and using the fact that the norm of a product is less than or 
equal to the product of the norms along with the triangle inequality and the 
convergence of the scalar geometric series yields 
(A+B)? —A™"*| = ABA" 5 
-(-BA-Yé 
k=0 
«_ JASE! 
<ATBA™| So ||ea-)|* 
= 
k=0 
+0 
asE-O0. 
IE 
Derivatives and Sensitivity of Inverses 
The degree to which A! is sensitive to small changes in A is a fundamental 
concern, and there are various ways to explore the problem. One approach is to 
apply elementary calculus to examine the derivative of A~!, and there are two 
ways of doing this. The first approach involving differentiation is to consider the 
entries in A = A(t) to be differentiable functions of a real variable t. Let 
' 
denote differentiation with respect to t, and assume that A~! = A(t)~! exists 
and is differentiable so that the product rule for differentiation (recall Exercise 
1.7.11) applied to AA~! =I 
yields 
AA' + A/A7=0 
=> AM =—AA'AT, 
(2.3.18) 
The absolute magnitude ||A~ 
|| (in any norm) provides some feeling for the 
degree of the sensitivity of A~! to small changes, but more insight is almost 
always obtained by examining relative magnitudes. For the case at hand, the 
relative magnitude of the derivative of AW! is ||A~!'|/||A~+||. Taking norms 
in (2.3.18) and using the sub-multiplicative property of a matrix norm produces 
JAM = AC AAM |] SAT ATA 
essing 
=f 
ells 
A 
A'|| 
= 
||A 
A 
where « = ||A|| ||A~1||. Thus the following theorem is established. 

The other approach to differentiating A~! is to consider each entry [A~"]nx 
in A7! to be a function of the n? entries in Ann, and consider the partial 
derivatives 0[A~1]n, /Oa;;. The determinant formula for A~! that is given in 
(9.3.9) on page 961 shows that [A~'],, is the quotient of two determinants, each 
of which is a sum of products of entries from A. This ensures that as long as 
A varies in such a way that A~! exists, the partial derivatives 0[A~+]),,/0a;; 
each exist. The formula for these derivatives is rather simple. 
Proof. 
Use the fact that 0A/0a,; = Bj; = ee, (a matrix whose (i, /)-entry is 
1 and has zeros elsewhere) along with the standard product rule to differentiate 

2.3 Nonsingular Matrices and Inverses 
173 
AA~! =I 
with respect to aj;. The result is 
OA 
OA7} 
OA} 
ALA 
ana 
be ei 
= 
Bas; 
e 
Bag 
ee; A 
+A Ts 
0 
oA7} 
dai; 
= —A7teyes A" => —[Am"),i [A+] 54 
The derivative of the (h,k)-entry in A7! is 
OA 
Tag 
T = 
ig 
lvoe 
MALE ee ep, 
dai; | 
ex = —e, [A ]ai[Aq"]jxex =—[A"]ns[A "]jx- 
ij 
Oa; i] 
Exercises for section 2.3 
2.3.1. When possible, find the inverse of each of the following matrices. Check 
your answer by using matrix multiplication. 
4 
-8 
5 
Pe 
A=(4 —7 ') B=(: 5 °) C= 
3-4 
2 
Th soe 
2.3.2. Find the matrix X such that X = AX+B, where 
QO 
-l 
0 
ly 
2 
A=(0 
0 -1) and 
Ba(3 1). 
@) 
0) 
0 
3 
3 
2.3.3. Use Corollary 2.3.3 to decide if dep ay ie a G ak & mip 
is a linearly independent set: 
BPR 
ee wOnwnd 
wWwwhnh 
re me 
WHF 
2.3.4. Explain why it is apparent simply by inspection that the following matrix 
is nonsingular. 
(te 
IL 
ie 
oes 
al 
Ie 
Te 
ee 
AN 
1 
PQ 
ro 
HE 
Anxn = 
: 
; 

174 
Chapter 2 
Systems, Elimination, and Echelon Forms 
2.3.5. 
2.3.6. 
2.3.7. 
2.3.8. 
2.3.9. 
2.3.10. 
2.3.11. 
Prove that if the entries of F,,, satisfy San \fizj| 
<1 for each i (ie., 
each absolute row sum < 1), then I+ F is nonsingular. 
Note: This means that sufficiently small perturbations of the identity 
matrix are necessarily nonsingular. 
If Anxn is a nonsingular matrix, which (if any) of the following state- 
ments are true? 
fay AwA™.- 
(bicA SAT Ee "(ey Acer Age 
(4) AWL 
(e) ATRL. 
(f) ARTI. 
Which (if any) of the following statements are true? 
(a) A~B 
=> 
A?~B'. 
(b) AO' B => 
AT*N'BT, 
() ASB = ATRBT 
(2) ARB 
=> AB. 
(:) ARB = 
AnvB. 
(ff) ANB = A''B. 
Explain why block triangular matrices of the form 
- 
0 
[wey 
Ele 2 md a=(4 a 
are nonsingular. Give an explicit expression for P~' and Q-!. 
Assuming that B and T = 
A—CB™!R are each nonsingular, derive 
the formula 
Ae Ne ( T-1 
Te Gha 
HR. 
Bye 
eRe Re 
Ben hte oR a) 
Suppose that A,B,C,D <€ R"*" are such that AB? and CD?" are 
each symmetric and AD? — BC? =I. Prove that A7D — CTB =I. 
Prove the statement regarding rows in Corollary 2.3.3. That is, prove 
that a square matrix A is nonsingular if and only if the set of its rows 
is linearly independent. 

2.3 Nonsingular Matrices and Inverses 
175 
2.3.12. 
2.3.13. 
2.3.14. 
2.3.15. 
2.3.16. 
Z.3-Li. 
2.3.18. 
Prove that for N ¢ F"*", the matrix 
A =I+N*N is nonsingular. 
Let A € F"*", 
(a) Explain why A must be singular if it contains a zero row. 
(b) Explain why A must be singular if it contains a zero column. 
(c) Explain why A must be singular if it contains two identical 
rows or two identical columns. 
(d) Explain why A must be singular if one row (or column) is a 
multiple of another row (or column). 
Derive the Sherman—Morrison—Woodbury formula. Hint: Use the Schur 
complement formulas on page 167, and consider the product 
(o 5) (ar 5) (or 1): 
2s Ome 
iO 
1 
You are given that A = (= 1 
| 
onde A =. (0 
1 1). 
—1 
0 
1 
iO 
2 
(a) Use the Sherman—Morrison formula to determine the inverse of 
the matrix B that is obtained by changing the (3,2)-entry in 
A from 0 to 2. 
(b) Let C be the matrix that agrees with A except that c32 = 2 
and c33 = 2. Use the Sherman—Morrison formula to find C7!. 
Show that if A,B € F"*" are nonsingular, and if B is obtained from 
A by replacing column A,; with a column b, then 
(A7'b — e;)[A™']j« 
Eee 
te 
a 
[A7"]j«b 
Suppose the coefficient matrix of a nonsingular system Ax = b is up- 
dated to produce another nonsingular system (A + cd')z = b, where 
b,c,d € R"*', and let y be the solution of Ay = c. Show that 
z=x-—yd'x/(1+d7y). 
Let A,B € F"*", where A is nonsingular. What value of a € R will 
ensure that A+B is nonsingular whenever 
|e| < a? 

176 
Chapter 2 
Systems, Elimination, and Echelon Forms 
2.3.19. 
2.3.20. 
2.3.21. 
2.3.22. 
2.3.23. 
The approximation (2.3.17) says that (A + E)~! » A7* — A"EA™ 
when the entries in E are sufficiently small. Illustrate this by examining 
how good this approximation is for A = « t) and E = (3 gb 
Consider E, A € F"™" along with any matrix norm. Prove that if 
|EA|| <1, then 
Pe Faken! 
~ 1— [BI Al 
S-(EA)! 
b= 
Sensitivity of Inverses. Suppose that a nonsingular matrix A is per- 
turbed by a matrix E whose entries are small enough in magnitude to 
ensure that ||A7*E|| <1 (for any norm), and interpret a <8 to mean 
that a is bounded above by something not far from 8. By using a first 
order approximation of (A +E)~!, derive the statement that says 
A-'-(A+E)*? 
E 
eae s SEA where 
«= ||Alj||A~*||. 
This is a discrete analog of the continuous result in Theorem 2.3.13, 
and it helps to underscore the fact that the condition number k is the 
primary feature in bounding the relative change in the inverse. 
Find the condition number (using the Frobenius norm) for the matrix 
1-1 
0 
OF°0 
0 
~1 
-1 
0 
0 
OF 
Ope! 
0 
0 
A 
|e 
Ep 
iad 
Oyo, 
One 
6 
1 -1 
0 
TEO. 
0 
J. 
Give an example to show that a matrix can have a very small determi- 
nant without having a large condition number. 

2.3 Nonsingular Matrices and Inverses 
177 
2.3.24. Differentiating a Solution. Consider a nonsingular system Ax = b, 
where A € R"*" and be R". 
Deo OnE Awe ( 
2.3.26. 
(a) First assume that A is a constant nonsingular matrix, but con- 
sider the components in b to be n independent real variables so 
that each component x; = x;(b1, b2,...,bn) in the solution x is 
Ox, [Obi 
i 
x2 /Obi 
a function of the n variables b;, and let 0x/0b; = 
dan [Obi 
Assuming that the indicated derivatives exist, show that 
Ox 
Bae [Aq*] #4" 
Now let b € R" be a constant vector and consider the entries 
in A € R"*" to be independent variables so that if Ax = b, 
then each 2; = 2;(a11,012-..,;@nn) 
is a function of the n? 
variables a;;. Assuming that A is nonsingular throughout an 
appropriate region, and assuming that the indicated derivatives 
exist, show that 
Ox 
0a; = —2; [A~] #0 
Note. The results in both parts of this problem indicate that 
the sensitivity of the solution x to small changes in A or b is 
governed by the magnitude of entries in A~!. This is corrob- 
orated by the alternate approach of differentiating the solution 
that is developed on page 253, where it is assumed that entries 
in A, b, and x are functions of a single variable t. 
aii a and A = det(A) = 11022 — @12421, apply 
a21 
422 
Lae 
: 
= 
E 
a 
—a 
direct differentiation to each entry in A7~! = A7! ( 22 
a to 
—a21 
aii 
compute 0A~!/daj2, and then compute —[A~'],;[A~"]o, to illustrate 
the validity of the formula in Theorem 2.3.14 on page 172. 
As indicated in the figure below, a missile is fired from enemy territory, 
and its position in flight is observed by radar at five different locations. 

2.3.27. 
x = Range 
Height (y miles) 
a 
see 
an 
Find the fourth-degree Lagrange interpolation polynomial é() that fits 
these observations, and make a conclusion about using &(x) to estimate 
how far down range the missile will land. 
Let A = & oy € R""", where a # 0, c and d are columns, 
and B is (n—1) x (n—1). Consider the result of pivoting on a to 
annihilate the entries of d? with column operations. Expressed as a 
block-column operation, this amounts to multiplying the first column 
of A by —d?/a and adding the result to the remaining part of A to 
produce 
a 
d' 
col 
(a4 
0 
T 
c 
B 
cen 
= coe 
e 
Prove that if A is diagonally dominant, then so is B — ca 
Note: This guarantees that each complete step of column-wise Gaussian 
elimination on a diagonally dominant matrix produces another diago- 
nally dominant matrix. This is important when numerically applying 
Gaussian elimination to solve a diagonally dominant system of linear 
equations—see Exercise 2.8.10 on page 246. 

' 
a 
—- Pan 
; 
: 
5 
a 
ae 
} 
} 
. 

For example, 
lpi teclac ahd 
sae ro 
{2464.4 
0 
0 
=o kee ot beat 
age Ceres. tee | 
Sian eee eres 
BE 
2.4 04 
7 
Be got aaa 
so the basic columns in A are {Ay 1, Ax43,Ax5}, and rank (A) = 3. 
The next theorem uses rank to characterize nonsingular matrices. 
ws 
pastas 
Bae 
asc 
se 
st 
ae 
s 
A 
TNS 
SANT: 
ise 
Proof. 
rank (A) =n 
if and only if there are exactly n pivots in Ea, which is 
equivalent to saying that Ea =I, and Theorem 2.3.1 (page 159) says that this 
happens if and only if A is nonsingular. 
Row Operations Preserve Rank 
Tt 
Since row operations are used to determine the rank of a matrix, it is intuitive 
that row operations should not be able to change the rank—something would be 
amiss otherwise. Below is a formal proof of this fact. 
The word "rank" was introduced in 1879 by the German mathematician Ferdinand Georg 
Frobenius (page 798), who thought of it as the size of the largest nonsingular submatrix in A 
(see Theorem 2.4.13 on page 187). While he did not use the term "rank," James J. Sylvester 
(page 117) had earlier used the same concept in 1851. 

2.4 Rank of a Matrix 
181 
Proof. 
Use the uniqueness of Ea (page 152) to conclude that 
Eq, ~ A'' B'" Ep 
=> 
E, ~ Ep 
=> 
E, = Eg. 
It follows from the definition of rank that rank(A) =rank(B). 
@ 
Caution! The converse of Theorem 2.4.3 is not true. Consider A = G af and 
B= (3 a for which rank (A) = rank(B), but A re B. Similarly, (2.4.2) 
is not true when P is singular—consider P = ( a) and A = Gi "at 
Rank as the Number of Independent Columns 
Rather than thinking about rank (A) in terms of echelon forms it is generally 
more useful to consider rank (A) as the maximal number of linearly independent 
columns (and rows) in A. 
+ 
he maximal number! 
In 
particular the basic columns 
in A cons itute a maximal linearly — 
edi 
Proof. 
If rank(A) = r, then Theorem 2.2.9 (page 150) says that there are 
exactly r linearly independent columns in Ea, and one such set is the set 
of pivot columns. Since row operations preserve column relationships (Theorem 
2.2.5, page 146), it follows that there are r linearly independent columns in A, 
and one such set consists of the columns in A that correspond to the pivot 
positions—i.e., the basic columns. Conversely, if a maximal linearly independent 
set of columns from A contains r columns, then a maximal linearly independent 
set of columns in Eq must contain r columns. Consequently, 
Ea must have 
exactly r nonzero rows (by Theorem 2.2.9), and thus rank(A)=r. 
@ 
For the sake of convenience, all subsequent references to "the number of independent columns 
in A" should always be interpreted to mean the size of a maximal linearly independent subset 
of columns from A. Similarly, "the number of independent rows in A" will be the size of a 
maximal linearly independent subset of rows from A. 

182 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Proof. 
If 
n+k vectors, 
k > 1, from F" are placed as columns in a matrix 
Anx(ntk)» then rank (A) <n, so Theorem 2.4.4 ensures that the columns must 
be linearly independent. 
According to Theorem 1.7.4 on page 59, the entire set of columns in Amxn 
is linearly independent if and only if the only vector Xp x1 for which Ax = 0 
is x = 0. Combining this observation with Theorem 2.4.4 yields the following 
important extension of Corollary 2.3.3 on page 160. 
. square mat 
Example (The Wronski Matrix and Independent Functions) 
This example illustrates the utility of the statements presented in Corollary 2.4.6 
above. Let V be the vector space of real-valued functions of a real variable, and 
let S = {fi (x), fo(xz),...,fn(x)} be a set of functions that are n — 1 times 
differentiable. The Wronski' matrix is defined to be 
fi(z) 
fa(z) 
++ 
fn(z) 
fi (2) 
faz) 
sss 
f(a) 
W(z) = 
: 
7 
(2.4.3) 
(r=) (9) 
iamea 3) 
+a 
at) (x) 
This matrix is named in honor of the Polish mathematician Jozef Maria Héené Wronski 
(1778-1853) who studied four special forms of determinants, one of which was the determinant 
of the matrix that bears his name. Wronski was born to a poor family near Poznan, Poland, 
but he studied in Germany and spent most of his life in France. He is reported to have been an 
egotistical person who wrote in an exhaustively wearisome style. Consequently, almost no one 
read his work. Had it not been for his lone follower, Ferdinand Schweins (1780-1856) of Heidel- 
berg, Wronski would probably be unknown today. Schweins preserved and extended Wronski's 
results in his own writings, which in turn received attention from others. Wronski also wrote 
on philosophy. While trying to reconcile Kant's metaphysics with Leibniz's calculus, Wronski 
developed a social philosophy called "Messianism" that was based on the belief that absolute 
truth could be achieved through mathematics. 

2.4 Rank of a Matrix 
; 
183 
The importance of the Wronski matrix W/(z) is given below. 
Proof. 
Suppose that 
0.= ay fi (x) SF Q2 fo(x) Seis Ont n{2) 
(2.4.4) 
for all values of x. Evaluating derivatives at x = xo yields 
Ce a1 f<*) (x9) + a2 fs" (ao) +:-'toanf" (9) 
for 
k=0,1,2,...,.n—1. 
Q1 
a2 
This means that 
v= | . | issuch that W(zo)v = 0. But W(zo) is nonsin- 
| 
On 
gular, so v = 0. Therefore, the only solution for the a; 's in (2.4.4) is the trivial 
solution a, =-::=@Q, =0, which ensures that S is linearly independent. 
IH 
For example, to verify that the set of polynomials P = {1,2,2?,...,a"} is 
linearly independent, observe that the associated Wronski matrix 
Gp 
fe 
S08 
a" 
(Oy 
Tl 
Dee 
sone 
nar! 
er 
és 
n—2 
OMORe 
Oe 
n! 
is triangular with nonzero diagonal entries. Consequently, W() is nonsingular 
for every value of x, and hence P must be an independent set. 
Rank as the Number of Independent Rows 
Now that it has been established that rank (A) is the number of independent 
columns in A, the next step is to show that rank (A) is also the number of 
independent rows in A. 
- : 
2.4.8. Theorem. yank Ages) = r if and only if the maximal number 
of linearly independent rows in A is equal to r. 

184 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Proof. 
Begin by observing that ifthe r rows in X;xm are linearly independent, 
then rank (X) =r because if rank (X)=s <r, then 
2 
bg 
Prk = Ex=| "6" 
sae ee ie 
rxm 
Since P,, 4 0 (no row of a nonsingular matrix can be zero), P,»X is a non- 
trivial combination of the rows in X. But, by definition, it is impossible for a 
nontrivial combination of independent vectors to sum to zero, so s = r. Now 
assume that rank(Am yn) =7 and prove the maximal number of independent 
rows in A is r. There are nonsingular matrices P and Q such that PA = Ea 
and E,aQ = Gee 0) .= N (see Exercise 2.2.25, page 158), so 
0 
0 
mn Bp est as gba abe lle ae 
yeaa 
Z(m-r) xm 
PAQ=N = QTATPT =N? = he er 
yet 
(2.4.5) 
where X,%m consists of the first r rows in Pa which must be linearly inde- 
pendent by Corollary 2.3.3 (page 160) and Exercise 1.3.8 (page 20). Therefore, 
rank (X) =r, so Theorem 2.4.3 ensures that 
rank (A*) = rank ekrtion = Tak (os) = 7 == rank (A). 
(2.4.6) 
Consequently, there are r independent columns in A? (by Theorem 2.4.4), and 
hence there are r independent rows in A. Conversely, if the maximal number 
of linearly independent rows in A is equal to r, then A? has r independent 
columns, so Theorem 2.4.4 guarantees that r = rank (A*), which is the same 
as rank (A) by (2.4.6). 
@ 
Transposition Preserves Rank 
The fact that transposition preserves rank emerged in (2.4.6), but because this 
fact has far-reaching consequences, and because rank is also preserved by conju- 
gate transposition, a statement of these facts as a separate theorem is warranted. 
2.4.9. Theorem. For all A € F"*", 
rank (A) =rank(A") 
and 
 rank(A) = rank (A*). 

2.4 Rank of a Matrix 
185 
Proof. 
The fact that rank(A) = rank(A™) 
is from (2.4.6). To prove that 
rank (A) = rank (A*), the steps leading to (2.4.6) can be duplicated but with 
transpose (x)* re 
replaced 
by conjugate transpose (x)*. Alternately, if PA = Ea, 
then P A = PA = Eg, which is also a row-echelon form that has the same 
number of nonzero rows as Ea, so rank(Ea) = rank (Ea). Since the con- 
jugate of a nonsingular matrix is again nonsingular (because P~! = P-'), it 
follows that rank (A) = rank (A), and using rank (A) = rank (A') yields 
rank (A*) = rank (A') =rank(A)=rank(A). 
@ 
Column Operations Preserve Rank 
Theorem 2.4.3 says that if A "<" B, then rank(A) = rank(B) 
(i.e., row 
operations preserve rank). An immediate consequence of Theorem 2.4.9 is the 
fact that column operations also preserve rank. 
2.4.10. Theorem. If Aw 
Ss B, then rank Ale 
= rank (B), or equiva- 
lently, 
rank (AQ) =rank(B) when Q is nonsingular. 
(2.4.7) 
Proof. 
If 
A®'B, then AQ =B, or equivalently, Q7 
A? = BY for a nonsin- 
gular Q. Therefore, with the aid of Theorems 2.4.3 and 2.4.9, it follows that 
A' '~ B => 
vank(A') =rank(B") 
== 
rank(A)=rank(B). 
© 
Example (Independence of Complex Conjugate Vectors) 
To illustrate the utility of Theorem 2.4.10 consider the following natural question. 
Given a complex vector 
0 
4x € C"!, is the set {x, x} linearly independent? 
Well, this is not much of a question because the answer is clearly "no." For 
example if x 
=a+ia for 
0#a¢€R"*', then 
X=a-—ia, so ix=x. 
A more substantial question is, what necessary and sufficient conditions on 
x are required to guarantee that {x, X} is linearly independent? And more 
generally, what necessary and sufficient conditions on {x,,x2,...,x,} ensure 
that {x1,X1,X2,X1...Xx,Xx} 
is linearly independent? Developing a useful an- 
swer based only on first principles involving the definition of independence is 
quite a challenge, but Theorem 2.4.10 makes things easy. 
For example, in the simple case concerning the independence of {x, x}, the 
answer is provided by the observation that if x = u+iv € C"*! is a complex 
vector, where u,v € R'~*, and if 
Oreste oF then [x |Jnx2 = [ut iv|u—iv] = [ul vi(} 4) =[ulvlQ 

186 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Since Q is nonsingular (it has a nonzero determinant), Theorem 2.4.10 ensures 
that rank [x|xX] = rank [u|v], and therefore {x,X} is linearly independent if 
and only if {u,v} is linearly independent by Corollary 2.4.6 (page 182). 
The logic for the more general question involving {X1,X1,X2,%1-. sXhy Met 
is almost identical to that for the simpler question, but this time let P be the 
Q 
Q 
. 
block-diagonal matrix Pozo. = 
2 
containing k copies of the 
Q 
2x2 matrix Q defined above so that 
[x1 |X | 
x2 | 
X2 | 
ame | Xz bie => [uy tiv, | 
uy, —iv, | 
Up +iv2 | 
Up —iv2 
nee 
= [ui | 
vi | ue | 
vo | 
Cat. | uz | ve] P- 
Since P is nonsingular (because Q is—see Exercise 2.4.23 on page 198), The- 
orem 2.4.10 guarantees that 
rank [x1 |X | 
x2 | Xe | vee | xx | Xx = rank [uy |vi | 
w2 | 
v2 | 
see | uz | vel, 
and thus Corollary 2.4.6 again yields the conclusion that 
e 
{xX1,X1,X2,X1...Xx, Xx} ts linearly independent if and only if 
(2.4.8) 
{U1, V1, U2, V2,..-,UK, Ve} és linearly independent. 
Rank and Matrix Equivalence 
TOW 
It has now been established that if A ~ B, then rank(A) = rank(B) (but 
not conversely), and similarly, if A 2 B, then rank (A) = rank (B) (but not 
conversely). It follows that if A ~ B (i.e., if PAQ =B 
for nonsingular P and 
Q) then rank (A) = rank (B). However, unlike row or column equivalence, the 
converse of the statement for general matrix equivalence is true—i.e., equal rank 
characterizes matrix equivalence. 
2.4.11. Theorem. A ~ B if and only if rank (A) = rank (B). That is, 
PAQ =B 
for nonsingular P,Q <=> rank (A) = rank (B). 
In particular rank (PA) = rank (A) = rank (AQ). 
Proof. 
The fact that A ~ B implies rank (A) = rank (B) is a consequence 
of Theorems 2.4.3 and 2.4.10. Conversely, if rank(A) = rank(B), then, as 
observed in (2.4.5), 
A w~ Ora y ~B,soA~B. 
UB 

2.4 Rank of a Matrix 
187 
Rank of a Product Cannot Exceed Rank of Any Factor 
Theorem 2.4.11 says that multiplication by nonsingular matrices preserves rank, 
but the same cannot always be said for products involving singular or rectangular 
matrices—e.g., it need not be the case that rank(AB) = rank (A)rank (B). 
While there is a general formula for rank(AB) (see Theorem 4.2.11 on page 
436), it is a bit complicated, so it is often more useful to just have a simple 
upper bound for rank (AB). The next theorem provides one by asserting that 
the rank of a product never exceeds the rank of any factor. 
24, 12, 
Theorem. For 
al A 
¢ - "and E 
B 
€ Ee, : 
rank a 
"rank 
nA) 
au { 
rank (B).. 
_ Equivalently, os (AB) s min{rank (A), rank :(B)} 
Proof. 
If rank(A)=r and if PA=E, = (oes then 
rank (AB) = rank (PAB) = rank (EaB) 
= rank bear )B = TOK (ee! <r=rank (A). 
In other words, the rank of a product never exceeds the rank of the left-hand 
factor. This together with Theorem 2.4.9 yields 
rank (AB) = rank (AB)* = rank (BTA) < rank (B') =rank(B). 
Rank as the Largest Nonsingular Submatrix 
Another way to think about rank is the way that Georg Frobenius originally 
defined the concept in 1879 (see the footnote on page 180). 
2.4.13. Theorem. If A ¢ F"*" is nonzero, then rank (A) is the size of 
a maximal nonsingular submatrix of A. 
e 
In particular, if rank(A) = r, then a maximal nonsingular sub- 
matrix is any r xr submatrix that lies on the intersection of r 
independent rows and r independent columns in A. 

188 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Proof. 
If rank(A)=r> 0, then A has r independent rows and r indepen- 
dent columns. Let such a collection be in respective row positions {i1,i2,..-, ir} 
and column positions {j1,j2,..-,jr} The first objective is to show that if Wexp 
is the submatrix lying on the intersection of these rows and columns, then W 
is nonsingular. By means of row and column permutations, 
A~(Y¥ D aed 
and 
rank(B) =r. 
For convenience, set 
WwW 
x 
M=[W|Xbans N=(v1z,. u=(>), v= e) 
mxXr 
Permutations do not affect independence, so rank(M) = r = rank(U). In 
particular, this means that the rows in N are combinations of the rows in M, 
and hence there is a matrix P(m_,;)x, such that N= PM (by Theorem 1.7.3, 
page 58). Use an indirect argument to show that W is nonsingular. Suppose that 
W is singular. Then there is a vector 
v 
£0 such that Wv = 0 (see Theorem 
Wv 
0 
1.7.4, page 59 or Corollary 2.3.3, page 160) so that Uv = & C ie) 
Vv 
Vv 
Since the columns of U are independent, Theorem 1.7.4 ensures that Yv 4 0. 
Vv 
However, if x = =) then 
Mx =0, 
== 
-Nx =0. 
== 
-Yv = 0, which is a contradiction. 
Therefore, W must be nonsingular, and thus A has at least one nonsingu- 
lar r x r submatrix. There is no larger nonsingular submatrix because row and 
column permutations will bring any nonsingular submatrix E to the upper left- 
hand corner, and then block Gaussian elimination can be performed as in (2.3.10) 
on page 166 to yield A ~ @ = ~ bh yi where 
S = 
H— GE™!F. Con- 
sequently, rank (A) = rank (E) + rank (S) > rank (E) (see Exercise 2.4.23). 
A=(:2 r 1), 
(2.4.9) 
SOM. 
then it can be verified that rank (A) = 2. This means that a maximal nonsin- 
gular submatrix is of size 2 x 2, and one of them lies on the intersection of two 
independent rows and two independent columns. It is evident that {Aix, Aas} 
and {A,xi,A.3} are respective independent sets of rows and columns, so! 
BeAr aye e :) 
is a maximal nonsingular submatrix of A. 
For example, if 
The following corollary illustrates the utility of Theorem 2.4.13. 
The submatrix notation Alii, i1,... tp |J1,J92,--+,Jq] was introduced in (1.2.4) on page 7. 

2.4 Rank of a Matrix 
189 
Proof. 
If Rryn and C,,x, are the respective submatrices containing rows 
{Ai,»,---, Asx} and columns {A,,,,...,A.;,}, then B is a submatrix of R 
and C. Since rank(R) <r and rank(C) <r, it follows that B is in fact 
a maximal nonsingular submatrix of R and C, so rank (C) = rank(B) =r 
and rank (R) = rank(B) =r (by Theorem 2.4.13). Thus the columns of C 
and the rows of R are linearly independent by Theorems 2.4.4 and 2.4.8. 
If 
For example, if A is the matrix in (2.4.9), then A[/2,3|1,3] = G a is 
nonsingular, so 
{Ao« = (2,4,1), 
Age = (3,6,1)} 
and 
{An=(2), 4e=(2)} 
should each be linearly independent sets, which indeed is easy to verify. To see 
that the converse of Corollary 2.4.14 need not be true, consider 
iy Relea 
A=(0 1 0) 
(2.4.10) 
10 
=0 
in which the first two rows and columns are each linearly independent sets, but 
the submatrix A[1,2|1,2] = ts 0) is singular. Moreover, the singleton sets 
{Aix} and {A,1} are each linearly independent, but AT Tae Gite 
208 is 
singular. 
Full-Rank Factorization 
In a preponderance of applications, decomposing (or factoring) a matrix into 
a product of simpler matrices is fundamental for revealing aspects of the data 
contained in the matrix or for gaining a theoretical or computational advantage. 
A decomposition that is sometimes useful, especially for theoretical purposes, is 
the full-rank factorization described in the following theorem. 

Example 
Proof. If Ea = ee and if Bmxr = [Axo,A«, °°: Axs,] contains the 
basic columns from A, then rank(C) = r = rank(B) (by Theorem 2.4.4), 
Furthermore, 
A = BC. To see this, consider the k*" column in BC. If the 
corresponding column in A is basic (say Axg = Axo;), then C,% = €7, 80 
(BC)., = BCy, = Be; = Baj = Aus, = Asn. 
If A, isnonbasic with p basic columns {Axp,,Axb.,---, Axo, } to its left-hand 
side, then C,; is nonbasic in C with p basic columns {e),€2,...,ep} to its 
left-hand side so that Cy, = ys by CjRej = prey cjrCxp,- It now follows from 
Corollary 2.2.7 (page 148) that 
P 
P 
Au, = So yn Asn, = ye B,jcjk = (BC). 
(by (1.7.8) on page 58), 
j= 
j=l 
(2.4.11) 
(see (2.1.16) on page 130) by observing 
and thus A = BC is a full-rank factorization. 
One full-rank factorization of the matrix A = 
NrNr 
em 
Oo 
bo Om 
O= oe 
w Noe 
Ww 
is obtained from Ea = 
that rank (A) =3 and that the basic columns are {A,1, Ax3, Ax5}. Full-rank 
factors of A are 
roi 
ants} 
20 
2 
0 
B=[An|Aww|Awl=[7 9 5] 
and c=(3 011 0). (2.4.12) 
207 
O 
©. Om 
1 
C 
It is easily verified that A = BC 
and rank (Bay3) = 3 = rank (C3x5). 

2.4 Rank of a Matrix 
191 
Outer Products and Rank—One Matrices 
Matrices for which rank (Am xn) = 1 have simple full-rank factorizations be- 
cause Ea has exactly one nonzero row, call it d*, and A has exactly one basic 
column, call it c, so, by Theorem 2.4.15, A = cd* is a full-rank factorization. 
Recall from Definition 1.8.7 (page 76) that cd* is called the outer product of ¢ 
and d, so the observation here can be rephrased by saying that if rank (A) = 1, 
then A is necessarily the outer product of two column vectors. 
Conversely, if A = cd* for nonzero c and d, then A 4 0 means that 
rank (A) >1, and 
c #0 means that rank (c) =1, so Theorem 2.4.12 yields 
= ronk(e) 
=rank(A)> 1 =~ 
rank(A)=1. 
Consequently, the following result has been produced. 
2.4.16. Theorem. 
rank(A) = 1 if and only if A = cd* is an outer 
_ product of two nonzero column vectors. 
: 
Outer products occur frequently throughout matrix algebra because every 
matrix product BC can be expanded as a sum of outer products involving the 
columns of B and the rows of C by writing BC = }°, By«C., (see Theorem 
1.8.8 on page 76). Consequently, if rank (Amxn) =r and A= ByxrCrxn 
is 
a full-rank factorization, then A = Soe BrxC.%, which means that 
e 
Every rank-r matriz is a sum of r rank-one matrices—t.e., r outer products. 
Nonsingular Matrices B*B and CC* 
Full rank matrices for which rank (Bmx,r) =r = rank (C,xn) are useful in part 
because, as the next theorem shows, they generate nonsingular matrices when 
multiplied by their conjugate transposes. 
2.4.17. Theorem. If rank (Bmx,;) =7 = rank (Crxn), then (B*B)-x, 
and (CC*),., are both nonsingular. 
Proof. 
It follows from (2.3.2) on page 160 that B*B is nonsingular when the 
only vector x satisfying B*Bx = 0 is x = 0. To demonstrate that this is 
indeed the case, use the fact that B has independent columns together with the 
observation that 
B*Bx=0 = 
x*B*Bx=0 
=> 
|Bx|?=0 — 
Bx=0. 
Since B_ has independent columns and Bx = 0, Theorem 1.7.4 (page 59) en- 
sures that x = 0. Thus B*B is nonsingular. Because 
rank( Gs), 7.=rank (C) =r, 

192 
Chapter 2 
Systems, Elimination, and Echelon Forms 
replacing B by C* in the product B*B produces the conclusion that CC* is 
nonsingular. 
Mf 
Example (Rank of a Product) 
To illustrate the utility of Theorem 2.4.17, consider the rank of a product. It 
has been established that multiplication by nonsingular matrices does not alter 
rank—e.g., if P is nonsingular, then rank (PA) = rank (A). But this need not 
be true when P is singular or rectangular. Nevertheless, there are cases when 
multiplication by a rectangular matrix preserves rank. Let A € F"**, Be pe. 
and Ce F***, 
e 
Jf B has full column rank (i.e., if rank (B) =r), then 
rank (BA) = rank (A). 
(2.4.13) 
e 
If C has full row rank (i.e., if rank (C) = s), then 
rank (AC) = rank (A). 
(2.4.14) 
The observation in (2.4.13) is a consequence of Theorem 2.4.12 (page 187) and 
Theorem 2.4.17 because 
rank (BA) < rank (A) = rank ((B*B]~'B*BA) < rank (BA). 
Similarly, (2.4.14) is true because 
rank (AC) < rank (A) = rank (ACC*[CC*]~') < rank (AC). 
Pseudoinverse 
If A is singular or rectangular, then A has no inverse, but by means of a full- 
rank factorization it is possible to construct a pseudoinverse for A that has 
"inverse-like" properties. 
2.4.18. Definition. Let rank (Amyn) =r, and let 
A= BryyrCryxn bea 
full-rank factorization as described in Theorem 2.4.15 so that B*B and 
CC* are r xr nonsingular matrices. The n x m matrix 
A' = CCC 
(B* By B* 
(2.4.15) 
is called the pseudoinverse 
(or alternately, the Moore—Penrose 
inverse') of A. While B and C need not be unique, A' is uniquely 
determined by A—this is proven below in Theorem 2.4.19. 
The pseudoinverse At is also called the Moore—Penrose inverse in honor of Eliakim H. Moore 

2.4 Rank of a Matrix 
Example 
193 
To find the pseudoinverse of the matrix A given in (2.4.11), use the full-rank 
factors B and C found in (2.4.12) to compute 
Be 
Fin 
eke 
tea 
i 
dathe 
Was 
2238 
(CO") 
= 
4(-2 
9 
0 
and 
(B'B)~ = 
(24 
15 10). 
i 
0 
O14 
—38 
-10 
14 
By definition, 
i 
aa 
A 
ade 
40 
Bi) 
S707 Ae 
Ale, OC CG2\a.( Bi Bath: ato oee 
6 
pa0 ( g.) 
R 
19 =2 
ors 
0 
6) 00 
2222 
0 
oO 14 
B47 
104 
ho 
Ad 
48 
248 
-104 
-88 
=gbq| 
39 
-68 
185 
—110 
87 
180 
81 
—198 
—84 
—280 
28 
308 
ES, of Aj 
2.4. 19. Theorem. For A E enn, the See At E mmm is 'the 
ae solution for x in the oe Penrose equations 
AXA =A, XAX=X, 
(AX)'=AX, 
(KA) =XA. 
(2.4.16) 
Proof. 
Suppose that X, and X» both satisfy (2.4.16). Use properties of the 
Frobenius norm (page 82) and the trace function along with those in (1.9.4) on 
page 84 to observe that 
AX, — AXo9||? = trace(AX, — AX2)*(AX, — AX») = trace(AX, — AX»)? 
= trace (AX,) — trace (AX2) — trace (AX,) + trace (AX) = 0, 
and hence AX, = AX». A similar argument shows that X,;A = X2A, so 
XK, = 
X, AX, = X,AXy = X2AXo= Xo. 
It is now a simple matter (Exercise 2.4.31) to verify that for any full-rank fac- 
torization A = BC, the expression for A' in (2.4.15) satisfies (2.4.16). 
I 
(1862-1932), considered by many to be America's first great mathematician, and Roger Penrose 
(1931-), a well-known English mathematical physicist. Each formulated concepts of generalized 
matrix inversion. Moore's work appeared in 1922, but it failed to attract much attention. 
Penrose defined his generalized inverse of A € R'™*" 
in 1955 to be the unique solution X 
to the four equations given in (2.4.16), the simplicity of which contributed to the popularity 
of the concept. Around the same time, Arne Bjerhammar (1917-2011), a Swedish geodesist, 
published his version of a generalized matrix inverse, but because his work was written in 
Swedish it received little attention until an English version appeared in 1958. The formula for 
A? in terms of a full-rank factorization was given in the 1964 text The Theory of Matrices in 
Numerical Analysis by Alston Householder (page 104). 

194 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Basic Properties of A' 
Several properties of the pseudoinverse are developed in subsequent sections and 
exercises, but some of the more elementary ones can be observed here. 
At = A>! when A is square and nonsingular because we can take B = I 
and C = A to write 
At=A*(AA*)=A*(A*)7AT=A™. 
Caution! The reverse order law cannot be applied to C*(CC*)~! and 
(B*B)~!B* when B and C are not square. 
AAt ZI, and A'A ¥I, in general, but rather 
AtA=C*(CC")-'C, 
and 
AA'=B(B*B)"*B" 
(2.4.17) 
(see the cautionary note given above). 
At = C'B? because B = BI,,, and C =I, ,C are respective full-rank 
factorizations of B and C, so 
Bt = (B*B)"'B* 
and 
Ci=c*(CC*)". 
(2.4.18) 
Caution! While "reverse-order" rule At = CB? holds for full-rank factor- 
izations, it is not true for general factorizations—see Exercise 2.4.36. 
BIB=I=CC! while BB'= AA' and CIC= ATA. 
(2.4.19) 
rere { 
(A*A)-*A* 
when rank (Ap un) = 1, 
a 
and in general, 
A*(AA*) 
when rank (Amxn) =™, 
At = (A*A)'A* = A*(AA*)! (developed in Exercise 3.5.14 on page 379). 
rank (At) =rank(A) because 
rank (A') = rank (A'AA') < rank (A) = rank (AATA) < rank (At). 
ATAAS =A 
AMAA! 
(2.4.20) 
This follows because (2.4.18) and (2.4.19) imply that B*BBt = B* and 
CIiCC* = C*. so 
A'AA* = C'CC*B* = C*B* = A* = C*B* = C*B*BB! = A*AAt. 
p= A'b is one solution to Ax = b when this equation is consistent because 
if b = Ax for some x, then using (2.4.19) along with At = CiB? yields 
b = Ax = BCx = B(B'B)Cx = BB! (Ax) 

Proof of (2.4.22). 
Combining Theorem 2.4.12 (page 187) and (2.4.20) produces 
rank (A*A) < rank (A*) = rank (A'AA*) <rank(A*A), 
so rank (A* 
A) = rank (A*) = rank (A) by Theorem 2.4.9 (page 184). Replac- 
ing A by A® yields the second equality in (2.4.22). 
lf 
Proof of (2.4.23). 
If A*x=0, then AA*x = AO = 0. Conversely, 
— 
O=AA*x = 0=ATAA*x=A'*x, 
by (2.4.20). 
Replacing A by A* produces the second part of (2.4.23). 
'Exercises for section 2.4 
2.4.1. Determine the rank of each of the following matrices. 
Ay 
at 
il 
By 
(0) 
bal 
Lea 
3 
1ahoe Stes 
eee 
Aybar 
@ (2469) @) 
Ber 
OC 
ISog aei 
ah e eg op 
AG 
1 
ik By 
3 
8 
6 
OO 
see—o 
Om 
Om 
BA PE 
i 
iN). 
gs} 
pe 
mle 
al 
0} 
2.4.2. Consider the matrix A = (3 ert 2). 
Gs) 
Dr 
(a) Determine a maximal linearly independent subset of columns 
from A. 
(b) Determine the total number of linearly independent subsets that 
can be constructed using the columns of A. 

196 
Chapter 2 
Systems, Elimination, and Echelon Forms 
2.4.3. 
2.4.4. 
2.4.5. 
2.4.6. 
2.4.7. 
2.4.8. 
Suppose that in a population of a million children the height of each one 
is measured at ages 1 year, 2 years, and 3 years, and accumulate this 
data in a matrix 
eyr 
iain 
Syn 
#1 
fhir 
Miz 
his 
#2 | hor 
hao 
has 
#i 
hit 
hi2 
his 
Explain why there are at most three "independent children" in the sense 
that the heights of all the other children must be a combination of these 
"independent" ones. 
Prove that S = {aj,a2,...,an} C F™*! is a linearly independent set 
if and only if S' = {ar, 5 a, Bs, ie rae is linearly 
we: 
1 
0 
1 
1 
independent. Hint. Consider Q = 
2 
2 
0 
-1l 
2 
—6 
8 
2 
iATe AS) 
oe 
led 
| 
and B= (: tA 
-1) equivalent ma- 
0 
-8 
8 
3 
3 
-9 
12 
3 
trices? 
W 
X 
Lr Ac 
Ay where rank (A) = r = rank(W,x,), show that 
there are matrices B and C such that 
I 
W 
WC 
A = 
= 
(3) w(t¢) 
ee ete 
Hint: Consider a Schur complement. 
Show that the set of polynomials p,(x) = yg 
Gh se, 
erent 
must be linearly independent. Hint. Think Wronski. 

2.4 Rank of a Matrix 
2.4.9. 
2.4.10. 
2.4.11. 
2.4.12. 
2.4.13. 
2.4.14. 
2.4.15. 
2.4.16. 
2.4.17. 
2.4.18. 
2.4.19. 
197 
Which of the following sets of functions are linearly independent? 
(a) 
{sinw, cosx, xsinz}. 
(bse 
ter, tere" 
|. 
(c) 
{sin? x, cos? z, cos 2}. 
Prove that the converse of the Wronski theorem (Theorem 2.4.7 on page 
183) is false by showing that S = {x?, |a|*} is a linearly independent 
set, but the associated Wronski matrix W/() is singular for all values 
Of go: 
2°11 
For 
A= € —2 1), 
find rank (A) and all nonsingular submatrices 
i 
=ZE 
Al 
of maximal order in A. 
Is it possible that rank (AB) < rank(A) and rank (AB) < rank (B) 
for the same pair of matrices? 
Is rank (AB) = rank (BA) when both products are defined? Why? 
For A € F™*".- explain why A*A =0 
if and only if 
A=0. 
for 
A €F™*", prove that rank (AA*A) = rank (A). 
Consider square matrices Any, for which n > 2. 
(a) Construct an example to show that rank (A) 4 rank (A®). 
(b) What is the strongest possible condition that you can think of 
to ensure that rank (A) = rank (A?)? 
(c) Is it ever possible for rank (A?) > rank (A)? 
Explain why Amn has full row rank (i.e., rank (A) = m) if and only 
if the only row vector y? for which y'A=0 
is y? =O. 
For square matrices with a full-rank factorization A = BC, explain 
why rank (A?) = rank (CB). 
If A,;A2:::Ax 
is a product of square matrices such that some A, is 
singular, explain why the entire product must be singular. 

198 
Chapter 2 
Systems, Elimination, and Echelon Forms 
2.4.20. 
2.4.21. 
2.4.22. 
2.4.23. 
2.4.24, 
2.4.25. 
2.4.26. 
2.4.27. 
2.4.28. 
2.4.29. 
Suppose that A is an m x n matrix. Give a short explanation of why 
each of the following statements is true. 
(a) rank(A) < min{m, n}. 
(b) rank(A) <m 
if one row in A is entirely zero. 
(c) rank(A) <m 
if one row in A is a multiple of another row. 
(d) rank(A) <m 
if one row in A is a combination of other rows. 
(e) rank(A) <n 
if one column in A is entirely zero. 
Prove that rank (AA*A) = rank (A) for all 
A ¢ F"*". 
For Amxp; Bmxq; Cnxp; Dnxq, explain why rank Ge = > rank (A). 
Prove that rank is "4 = rank (A) + rank (B). 
Prove that if 
A¢ F"*~" and 
Be F""? are such that A*B = 0, then 
rank (A|B) = rank (A) + rank (B). 
For A,B € F"*", prove that if A*B = 0, then 
rank (A + B) = rank (A) + rank (B). 
Prove that if A is nonsingular, then 
& Nae 
rank | p e) = rank (A) + rank (S), 
where 
S=B-—RA7'C is the Schur complement from page 167. 
Hint: Consider block Gaussian elimination. 
Let Anxn = (ie Md in which B is (n—1) x (n—1). 
(a) Prove that if rank (A) =n—1=rank(B), then a = d*B-'c. 
. 
. 
B 
(b) Explain why the rows in [B|c] and the columns in 2) must 
d* 
be linearly independent sets. 
Prove that if rank (Anyn) =1, then A? =7A, where 7 = trace (A). 
As a function f:F"*" + F, is f(A) =rank(A) a linear function? 

2.4 Rank of a Matrix 
2.4.30. 
2.4.31. 
2.4.32. 
2.4.33. 
2.4.34. 
2.4.35. 
2.4.36. 
2.4.37. 
199 
Suppose that A is an m xn matrix. Give a short explanation of why 
rank (A) <n whenever one column in A is a combination of other 
columns in A. 
Verify that for any full-rank factorization A = BC, the resulting pseu- 
doinverse Ai = C*(CC*)~!(B*B)~!B* satisfies the four Penrose equa- 
tios AXA=A, XAX =X, (AX)*=AX, and (KA)* =XA. 
For D = diag (Aq, A2,..., An), show that the Moore—Penrose pseudoin- 
verse of D is given by Di = diag (al, Va ri) , where 
er 
if A; = 0: 
Explain why each of the following statements is true for every A € F"*". 
(Gyan =A 
(b) At* = a*t 
(c) (A*A)' = ATA*t and (AA*)' = A*tat 
Normal Equations. For A ¢ F"*" with rank(A) =r and b€ F™™', 
A* Ax = A*b is called the system of normal equations, which are fun- 
damental to the theory of least squares as developed on page 481. Prove 
that at least one solution of the normal equations is given by x = A'b. 
Note: In the process you will be proving that the normal equations are 
always consistent, regardless of whether or not Ax = b is consistent. 
Minimum Norm Solution. It was established in (2.4.21) on page 194 that 
if Ax = b is consistent, then p = A''b is one particular solution, but 
an even stronger statement can be made. Prove that p = A'b is the 
minimum norm solution in the sense that ||A'b|| < ||x|| for all x such 
that Ax = b. Hint. Consider the Pythagorean theorem from Exercise 
5.1.18 on page 656. 
Construct examples of matrices X and Y such that (XY)! 4 Y'X?. 
Show. toot it «dyes OU toreg == hc 7 Chen 
(onl 
Oo, 
0 
0 

200 
Chapter 2 
Systems, Elimination, and Echelon Forms 
2.4.38. 
2.4.39. 
2.4.40. 
2.4.41. 
2.4.42. 
2.4.43. 
2.4.44, 
Show that A? = C* (B*AC*)"* B* when A = BC is a full-rank 
factorization. 
Show that a full-rank factorization need not be unique by demonstrating 
1 
g 
0 
0 
two different full-rank factorizations for A = ( 
0 
: : 2). 
SO 
eee 
Compute the pseudoinverse At of the matrix A in Exercise 2.4.39. 
For rank(Am xn) = 7, prove that if 
A = BC; = BC are both full- 
rank factorizations for A, then C;, = C2. Similarly, demonstrate that 
if 
A= B,C = B.C are both full-rank factorizations, then B; = Bo. 
Suppose that rank (Am xn) = 7, and let P be a nonsingular matrix 
such that PA = E,. Use Exercise 2.4.41 to prove that if P~! 
is 
partitioned as P~! = [X,,x,|Y], then X is the matrix B of basic 
columns from A. 
} 
Explain why Pa By ogi Aap 
Suppose that A € F"*" with rank(A) = r, and let Umym and 
Vnxn be unitary matrices (see Definition 1.6.4, page 41). Prove that 
(UAV)! = V*ATU*. Hint. Recall Theorem 1.10.1 on page 92. 

2.5 General Linear Systems 
201 
2.5 GENERAL LINEAR SYSTEMS 
An earlier focus was on solving square and nonsingular systems of linear equa- 
tions, but now it is time to analyze more general systems AmxnXnx1 = Dmx 
of m linear equations in n unknowns. Unlike nonsingular systems, a general 
rectangular or singular system may have infinitely many solutions, or it may have 
no solution at all. So, before analyzing solutions, it must first be established that 
there are indeed solutions to analyze. This is the issue of consistency. 
Consistency 
A linear system Ax = b is consistent when it possesses at least one solution, 
and it is inconsistent if no solution exists. A preliminary statement concerning 
consistency was developed in Theorem 1.7.5 (page 62) where it was established 
that Ax = b is consistent if and only if b is some combination of the columns 
in A, but there is more to be said. 
The consistency of a real 2x 2 or 3x3 system is easy to comprehend 
because the respective graphs are just two lines in R? or three planes in R°, and 
the question of consistency boils down to whether the pair of lines or the three 
planes intersect at a common point. It naturally follows that a linear system of m 
equations in two or three unknowns is consistent if and only if the corresponding 
m lines or m planes have at least one common point of intersection. While easy 
to state, these conditions are not easy to validate visually. Rather than using 
geometry to determine the consistency of Ax = b, Gaussian elimination can be 
a better alternative because row-reducing the augmented matrix [A|b] to an 
echelon form [E]c] reveals consistency—or lack of it. 
Suppose that somewhere in the process of reducing [A|b] to [E|c] a row 
appears in which the only nonzero entry is on the right-hand side such as in the 
example shown below. 
Pe 
63 
¢3 
gz 
ee 
* 
@ 
@) 
0) 
ee 
a 
* 
Oy 
) 
(0) 
6) 
ek: 
* 
Rowi 
— 
]0 
0 
0 
0 
0 
0 
a | «<—a0 
e 
e 
o 
e 
e 
e 
e 
e 
e 
e 
e 
e 
e 
e 
If this occurs in the i*" row, then the i*" equation of the associated system is 
Ox; + Org +---+0%, =a. 
For a #0, this equation has no solution, and hence the original system must 
also be inconsistent (because row operations do not alter the solution set). The 
converse also holds. That is, if a system is inconsistent, then somewhere in the 
elimination process a row of the form 
(00---O|a) 
with 
a#0 
(2:51) 

202 
Example 
Chapter 2 
Systems, Elimination, and Echelon Forms 
must appear—otherwise, the back substitution process can be completed and a 
solution is produced. 
Caution! No inconsistency is indicated when a row of the form (0 0 --- 0|0) 
is encountered. This simply says that 0 = 0, and although this is no help in 
determining the value of any unknown, it is nevertheless a true statement, so it 
doesn't indicate an inconsistent system. 
Another way to characterize the consistency (or inconsistency) of a system 
Ax = b isto observe that if b is a non-basic column in [A|b], then no pivot can 
exist in the last column, and hence the system is consistent because the situation 
(2.5.1) cannot occur. Conversely, if the system is consistent, then the situation 
(2.5.1) never occurs during row reduction, and consequently b cannot be basic 
in [A|b]. In other words, Ax = b is consistent if and only if b is a non-basic 
column in [A|b], or equivalently, all basic columns in [A|b] are columns in A. 
And since the rank of a matrix is the number of its basic columns, the preceding 
observation means that consistency can also be characterized by saying that 
Ax = b is consistent if and only if rank|A|b] = rank (A). 
These results together with that in Theorem 1.7.5 are equivalent ways of 
saying that a system is consistent. They are summarized below. 
2.5.1. Theorem. Each of the following statements is equivalent to saying 
that Ax = b is a consistent system of linear equations. 
e 
During reduction of [A|b] to echelon form, a row (00 --- 0ja) 
with a #0 never appears. 
eb is anon-basic column in [A|b]. 
rank|A|b] = rank (A). 
eb is a combination of the columns in A. 
eb isa combination of the basic columns in A. 
To determine if the system 
%1+ %o+ 2434+ 274+ 25 
= 1 
221 + 249 + 4934+ 494+ 325 =1 
221 + 279 + 443 + 404 + 275 = 2 
32; + 5% + 8434+ 624+525 
=3 

2.5 General Linear Systems 
203 
is consistent, apply Gaussian elimination to the augmented matrix 
OS hae ad 
Omnia Sez f 
1 
AD 
(eee Sp 
EL 
0 
O: OUR 
d) 2p 
[Ab] 
at se 
reg ke dn a aS ee 
0 
She say teh Ge sje Gih 
@ 
8 
8 
@ 
@ 
0 
(Ce See 
1 
mee oe OC) 0a? 
0| —[BIc] 
Came Odean 
ar 
ie) 
Orne) 
0 
Because a row of the form (00 --- 0|a@) with 
a #0 never emerges, the system 
is consistent. It can also be observed that b is a non-basic column in [A|b] or 
that rank|A|b] = rank (A). By completely reducing A to Ea, it is possible 
to verify that b is indeed a combination of the basic columns {A,1, Ax2, Axs}. 
Homogeneous Equations and Nullspace 
2.5.2. Definition. A system of linear equations in which the right-hand 
side consists entirely of zeros is called 
a homogeneous system. The 
associated matrix equation Ax = 0 is a homogeneous equation. 
e 
The set of all possible solutions for Ax = 0 is called the nullspace' 
of A and is denoted by N (A). 
e 
Consistency is never an issue for homogeneous equations because 
x = 0 (called the trivial solution) always satisfies Ax = 0. In 
other words, 0 € N (A) for all matrices A. 
The major question concerning homogeneous equations is, are there solu- 
tions other than the trivial solution, and if so, how can they be best described? 
Gaussian elimination again provides the answer, but before executing an elimi- 
nation process note that when the augmented matrix [A |0] is row-reduced to 
an echelon form, the zero column on the right-hand side is not altered by ele- 
mentary row operations—i.e., every echelon form derived from {A | 0] must have 
the form {E|0], where E is in echelon form. The last column of zeros is excess 
baggage, so there is no need to carry it along from step to step. Just reduce the 
coefficient matrix A to a row-echelon form—say Ea —and remember that the 
right-hand side is entirely zero so that back substitution is applied to the system 
defined by 
Eax = 0. The details of the solution process are best understood by 
considering a typical example such as the one below. 
i The nullspace N(A) of A is sometimes called the kernel of A in older or foreign literature. 

204 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Example 
To find all solutions to the homogeneous system 
21 + 2rq + 223 + 324 = 0,7 
27, +42%0+ 
243+ 324 = 0, 
(2:5:2}, 
32, +6272+ 
243+ 424 =0, 
rh BARN 
or equivalently, to determine the nullspace NV (A) of A = (2 : 2), 
reduce 
A to an echelon form, either reduced or unreduced. We will generally use Ea. 
ha 
as 
bie Ags 
ein, of 
A= 
241 3)—4(0 0 -3 3) 
(0011 = 
ae 
(2533) 
ee 
de 4 
00 
-5 
-5 
GD O 
Hence the original system (2.5.2) is equivalent to the reduced system 
21 + 272 + 74 = 0, 
(2.5.4) 
—73 —%4 = 0. 
Since there are four unknowns (or variables) but only two equations, it is impos- 
sible to extract a unique solution for each variable. The best that can be done is 
to solve for two of the variables in terms of the others. Rather than making an 
arbitrary choice of which variables to solve for, adopt the following convention. 
Convention for Solving Reduced Systems 
When A is m x n with rank (A) =7T, solve the reduced homogeneous 
system E,x = 0 for the r basic (or pivot) variables in terms of the 
nm—r non-basic (non-pivot) variables. The n—r non-basic variables 
are hereafter called the free variables because their values are not 
determinable—they are allowed to vary "freely" over F. 
Example 
Since rank (A) = 2 in (2.5.3), and since the pivots (and basic columns) lie in 
the first and third positions, the reduced system (2.5.4) is solved for the basic 
variables x; and 23 in terms of the free variables x2 and x4 to produce 
£1 = —2%2 — Za, 
2 
is "free," 
(2,5.5) 
3 = —%4, 
G4 
is "free." 

2.5 General Linear Systems 
205 
This form of the solution of the homogeneous system is called the general so- 
lution. As the free variables v2 and 24 range over all possible values, the ex- 
pressions in (2.5.5) define all possible solutions of Ax = 0 —i.e., they determine 
the nullspace N(A) of A. For example, when x2 and x4 assume the values 
X2=1 and x4 = —2, then a particular solution 
Dagar Us 
Te, 
Dae 2, 
Lace 
2 
is produced. When x2 = 7 and x4 = V2, then another particular solution 
£1 = —2n — V2, 
Ly =e 
r3 = —V2, 
r4= V2 
is generated. 
Spanning the Nullspace 
Rather than describing N(A) in the form (2.5.5), subsequent developments 
make it more convenient to express N (A) as a linear combination obtained by 
separating the coefficients of the free variables into distinct columns as shown 
below. 
v1 
—2%2—%4 
—2 
—1 
— 
v2 
aa 
v2 
oe 
1 
0 
aicca| 
Real 
140 Magi 
Alte | epee 
ney 
(2.5.6) 
@4 
v4 
0 
1 
The reason for expressing N (A) in this form is to emphasize that every vector in 
N (A) (i.e., every homogeneous solution) is a combination of the two particular 
homogeneous solutions 
9) 
=I 
Ligeia: 
|rrendaeDaces 
|g, 
0 
i 
In the language of Definition 1.3.1 on page 16, the set H = {hi,ho} spans 
N (A). This can alternately be phrased by saying that H = {hj,ho} 
is a 
spanning set for N(A). The fact that h; and hg are particular solutions is 
clear because h; is produced when the free variables assume the values x2 = 1 
and 24 = 0, whereas hg is generated when z2 = 0 and x4 = 1. The formal 
way to represent all possible solutions to the homogeneous system Ax = 0 in 
(2.5.2) is to use set notation to write 
N (A) = {x|x = 22h; + 24h2, where 2,24 
€ F} = span{hi,ho}. 
(2.5.7) 
This example is typical in the sense that extracting N (A) (i.e., the solution set 
for a completely general system of m homogeneous equations in n unknowns) 
exactly parallels the process illustrated above. Below is a formal summary. 

206 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Determining N (A) 
For Amxn With rank (A) =r, the nullspace N (A) can be obtained as follows. 
1 
a: 
Row-reduce A to an echelon form E—we generally use E = Ea. 
Use back substitution (if necessary) on the system defined by Ex = 0 to 
solve for the r_ basic variables in terms of the n —r free variables. 
Separate coefficients of the free variables x ,,vf,,...,f,_, 
into respective 
columns hy;,ho,...,hp,—,, and, as illustrated in (2.5.6) and (2.5.7), express 
the nullspace as 
N (A) = {x|x =2y,hi+as,ho+---+27,_,hn—,, where rs, € F} 
= span{hj,ho,...,hn—r} 
(2.5.8) 
The columns h; represent particular solutions of Ax = 0. As the free vari- 
ables z+, range over all possible values in F, the expression for x in (2.5.8) 
generates N (A), or equivalently, all possible solutions to Ax = 0. In other 
words, the vectors B = {hy, ha,...,hp_,-} are a spanning set for N (A). 
Linearly Independent Spanning Set for N (A) 
As the following theorem shows, the n —r vectors in (2.5.8) are not just a 
spanning set for N (A), they are a linearly independent spanning set for N (A). 
This has important consequences in the sequel. 
2.5.3. Theorem. 
For Am xn 
with rank(A) = 
r, the spanning set 
B= {hj,ho,...,hn_,-} for N(A) that is described in (2.5.8) is lin- 
early independent. 
Proof. 
Ifthe n—r 
free variables are x+,,r7,,...,07,_,, 
then, as depicted 
below, the nature of how the vectors in B are constructed ensures that [hi], = 1 
(the f; entry in h; is 1), and [ho]y, = [h3]s, =--- = [hn_r]¢, = 0. 
N (A) = {x f, hy a Lf, hg poo 
Ss of Alar Fe | 
Lf, € F} 
* 
* 
* 
1 
S= fi 
0 
- fi 
0 = fi 
= If, 
' + foe 
+ £4, 
+ fe 
fteeet 
te: 
0 
+ fo 
a fn—r 
vee teva 
ee 
+ 
| 
= 
oa 
i 
oi 
J 
=n 

2.5 General Linear Systems 
207 
Consequently, h; cannot be a linear combination of {hg,...,h,—,}. Similarly, 
[ho]f, = 1, and [hi]y, = [ha]p, =-++ = [hn—r]y, =0, so he is not a combina- 
tion of {h;,hg,...,h,_,}. The same logic applies for every other vector—i.e., 
the complementary zero-one entries in positions f1, fo,...,fn—r guarantee that 
no hj; can be a combination of the other h's, and thus B must be linearly 
independent. 
# 
A direct corollary of Theorem 2.5.3 is the fact that there can be no more 
than n—r independent solutions of the homogeneous equation Ax = 0 be- 
cause B being an independent spanning set for N (A) precludes the existence 
of additional independent solutions since all solutions are combinations of vectors 
in B, This observation is formally stated below. 
25.4, Cota. For 
athe sane eS 
ar ea ae 
_ pendent subset of N (A) can contain at most n—r_ vectors, and — 
es ae ho,.. sTtn-r} is one 
> maximal 
al 
independent subset of N 
V (A) 
Proof. 
To make the argument rigorous, put the vectors from the linearly inde- 
pendent spanning set B = {hj, ho,...,h,_-} C N(A) described in (2.5.8) and 
Theorem 2.5,3into Hy,.;--°= [hi |hg|--- hase so that rank (H) = n—r. 
If {x1,X2,...,x-} C N(A) is linearly independent, and if these vectors are 
columns in X,x%, then for each j there are scalars 6;; such that 
n—-r 
a8 S_ hi Bij => X=HB, 
where 
Bi,_,)xz = (Bij). 
i=1 
Applying Theorem 2.4.12 on page 187 yields 
k = rank (X) = rank (HB) < rank(H)=n-r. 
8 
Orthonormal Spanning Set 
Theorem 2.5.3 guarantees the existence of a linearly independent spanning set 
for N (A), but an even stronger statement can be made—namely that there is 
always an orthonormal spanning set. (Recall Definition 1.6.3 on page 40, and 
remember that orthonormal sets are always linearly independent—this was Ex- 
ercise 1.6.4 on page 50) 
2.5.5. Theorem. If A ¢ F'*" with rank(A) =r, then N(A) has an 
orthonormal spanning set B = {u),uUo,...,Up_;}. 

208 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Proof. 
To create an orthonormal spanning set for N (A), first construct an 
orthogonal set. {x1,X2,..-;Xn-r} 
C N(A) with each x; # 0, and then nor- 
malize the results at the end. The process is sequential. Begin with any nonzero 
Cc 
. 
x; € N(A). To construct x2, use Eq = 
=, where C,.,, contains the r 
nonzero rows in Ea, and remember that N(A) = N(C). Solve the system 
cS 
of (r+1) x n of homogeneous equations {| — 
)x = 0 for a nonzero solution 
x 1 
X2. Thus xp € N(A), and xg | x; because 
(x2|x:) = 0. The next vec- 
tor in the sequence is similarly constructed by solving the (r+ 2) x n system 
x = 0 for a nonzero x3 so that 
5] 
414 
xg 
© N(C)=N(A), 
xg3ixi, 
and 
x3 1 xo=0. 
Repeat the process until an orthogonal set {x1,x2,...,Xn—r} C N(A) is ob- 
tained, and then normalize each vector to produce B = {uj,Ug,...,Un—;}, 
where u; = x;/ ||x;||. 6 is linearly independent (Exercise 1.6.4), and it spans 
N (A) because otherwise B could be augmented with additional vectors from 
N(A) to extend B to a linearly independent spanning for N (A), which is 
impossible by virtue of Corollary 2.5.4. 
UY 
AZ CO 
If A= (: Ae :) 
is from (2.5.3) (page 204), then rank(A) = 2. To con- 
SO 
lee 
struct an orthonormal spanning set for N (A) containing n—r = 2 vectors, start 
ie 
10 Ok 
C 
with x; = 
9 | from (2.5.6) (page 205) and Ea = (0 Ord | 
= Gi 
0 
OF OO 
a0 
0 
(=) 
Thus {x1,x2} C N(A) is an orthogonal spanning set for N(A). Finish the 
process by normalizing each vector to produce an orthonormal spanning set 
—2 
—1/5 
B={u1,us}, where uj = 
— 
"Al and_u, 
=—= 
V5. ae 
0 
and solve 
I| 
me 
or 
Fon 
Oro 
rR 
Ne 
dd \| on) 
oe R 
Pa bw I| 
| to Ss 
on 
all" V5 
ell Vat\ 7 1 
Note: The technique in the proof of Theorem 2.5.5 is not the only way to produce 
an orthonormal spanning set. The Gram—Schmidt process discussed on page 658 
is another algorithm that can be applied. 

2.5 General Linear Systems 
209 
Uniqueness, Nullspace, and Rank 
All homogeneous systems are consistent—the trivial solution x = 0 always 
makes Ax = 0 true. The real question is, when is the trivial solution the only 
solution, or equivalently, when is N (A) = 0? The answer is, when there are 
no free variables because the existence of at least one free variable allows for an 
infinite number of homogeneous solutions. If A is m xn with rank(A) =r, 
then there are n —r free variables, so saying that there are no free variables is 
equivalent to saying that r =n. 
In other words, N(Am xn) = 0 if and only if rank(A) = n. This is not 
new—it was established earlier in Corollary 2.4.6 on page 182. And the special 
case in which A is square was addressed in Corollary 2.3.3 on page 160, but it 
was not phrased in terms of "nullspaces." The importance of these ideas warrants 
bringing them together into concise statements that are summarized as follows. 
2.5.6. Theorem. (Summary) For every A ¢ ee the es 
state- 2 
ments are equivalent. 
e 
N(A)=0. 
e 
rank(A) =n. 
e 
Thecolumns in A are linearly independent. 
e 
The homogeneous system A,..,X = 0 has a unique solution (which 
is x= 0). 
For square matrices, N (A) = 0 <=> A is nonsingular, or equivalently, 
N (A) £0 
if and only if A is singular. 
e 
Furthermore, (2.4.23) on page 195 can be rephrased by saying that 
N(A)=N(A*A) 
and 
N(A*) = (AA). 
(2.5.9) 
Nonhomogeneous Equations 
Now consider a nonhomogeneous equation Ax = b, where b 4 0. Unlike homo- 
geneous systems, a nonhomogeneous system may be inconsistent, so tests such 
as those in Theorem 2.5.1 (page 202) must be applied to determine if solutions 
do indeed exist. Unless otherwise stated, it is assumed that the systems in this 
section are consistent. The procedure for determining the general solution (the 
set of all possible solutions) for a nonhomogeneous equation 
A,» »x = b for 
which rank (A) = r is essentially the same as that for solving a homogeneous 
equation—it is summarized below. 
1. Row-reduce 
[A|b] to an echelon form 
[E|c]| (reduced or unreduced) and 
verify consistency using any of the tests in Theorem 2.5.1. 

210 
Chapter 2 
Systems, Elimination, and Echelon Forms 
2. If the system is consistent, then use back substitution (if necessary) on the 
system Ex = c to solve for the r_basic (pivot) variables in terms of the 
n—r free (non-pivot) variables. 
3. Separate the constants and the coefficients of free variables rf,,@f,,---,Lf,_, 
into respective columns p, hi,h2,...,hn—, (see the example below), and 
express the solution set as 
S=p+N(A) ={x|x=p+a2y,hit---+25,_,hn-r, 24, € F}. (2.5.10) 
As the free variables xs, vary over F, expression (2.5.10) generates all pos- 
sible solutions of Ax = b. The column p is one particular solution (ob- 
tained when each xs, = 0), and N (A) = {xp,hi+ays,ho+---+27,_,hn—r} 
is the general solution for Ax = 0 (see Theorem 2.5.7 below). Note that 
S =p+N (A) is an affine space as defined in Definition 1.3.2 on page 17. 
Example 
Determine the general solution for the nonhomogeneous system Ax = b, where 
Le 
oa 
39 
4 
A=(32 4 
1 2) and 
b= (5). 
(2.5511) 
5 eas a FE 
7 
Note that the coefficient matrix A is the same as that in the example of the 
homogeneous system on page 204. The Gauss—Jordan procedure produces, 
pee ae ta Pew: 
Ve20 
Vie 
(Alb) =e t2 4 siaeg 
s)>(o 011 
1) 
= Ejay), 
Ret ei bees & Nite, 
O76 
fos CLO 
so the system is consistent by Theorem 2.5.1. Solve the reduced system 
%1+2%2 +274 =2 
Ge 4 
for the basic variables x,,x3 in terms of the free variables x2, x4. 
Zr, = 2— 222 peed 
Z2q is "free" 
03 = 1— r4 
x4 is "free" 
Separate the constants and the coefficients of the free variables into respective 
columns to create the general solution. 
vy 
2— 2%9— 274 
2 
=i) 
aT 
v2 
es 
x2 
pas 
0 
1 
0 
lice 
bas 
1-24 
ii) 
22 
log | hal 
LA 
La 
0 
0 
1 
(2.5.12) 
I p+ 22h; + r4ho = p+ N (A). 

2.5 General Linear Systems 
211 
As the free variables x2 and x4 range over R, this expression generates the 
ow 
solution set for Ax = b. The column p = { , ]} in (2.5.12) is a particular 
0 
solution of Ax = b that is produced when the free variables assume the values 
Z2 = 4 = 0, and it was shown in (2.5.6) and (2.5.7) on page 205 that the 
general solution of the associated homogeneous system Ax = 0 is given by 
=) 
Eat 
N (A)= 
<a 
* 2d 
af 
= span {hy, ho}. 
0 
1 
In other words, the general solution of Ax = b is the affine space 
S = p+ N (A) 
that is composed of a particular solution p plus the general solution N (A) of 
the homogeneous system' Ax =0. 
The following theorem rigorously establishes the generality of the observa- 
tions made in the preceding example. 
2.5.7. Theorem. 
If Ajnx%nX = b is a consistent system of linear equa- : 
tions, then the general solution (the solution set) is the affine space 
SapenN a) 
| 
(2.5.13) 
where p isa particular solution (i.e., Ap = b) and the nullspace N (A) 
is the general solution of the homogeneous equation Ax = 0. (Recall 
Definition 1.3.2 on page 17 together with the accompanying discussion 
of affine spaces.) 
Proof. 
Let 
S=p+N(A) and S' = {x| Ax =b}. A standard way to prove 
that two sets are equal is to argue that each is contained in the other—i.e., prove 
that S = S' by showing that 
S C S' and S' CS. If 
xE S, then 
x=p+n 
for some n € N(A) so that Ax 
= Ap+ An=b+0=b, which implies that 
x € S', and thus S C S'. To show inclusion in the other direction, let x € S' 
so that 
Ax=b=Ap = 
A(x-p)=0 = 
(x-p)EN(A). 
Consequently, 
x = 
p+ (x—p) € S, which implies that S' C S, and therefore, 
S = 
This statement should have a familiar ring to students who studied differential equations. 
Exactly the same situation holds for the general solution to a linear differential equation. This 
is no accident—it is due to the inherent linearity in both problems. 

{ 
- os 
oy 
no 
Me 
G3. 
Acti 
LEeosy 
Proof. 
These statements follow directly from those in Theorem 2.5.6. 
Example 
: 
27, +4272+62r3 = 2 
v1 + 202 -+323= 
1 
Consider the nonhomogeneous system 
. Gauss—Jordan 
+ 23 =—3 
227 + 422 
= 
8 
reduction applied to the associated augmented matrix yields 
2426 
2 
Tid 
0! 
—2 
ee 
ae 
1 
Gt 
1 
2 
[AID 
(So: 
Hoang 
oe [ee Eta, 
2240) 
8 
0 
0 0 
0 
The system is consistent (because the last column is non-basic), and there is a 
unique solution because rank (A) = 3 =the number of unknowns. Alternately, 
it can be observed that there are no free variables or that N (A) = 0. Looking 
=a 
at Evajp) shows that the unique solution is x = ( 
:). 
=1 
Summary of General Linear Systems 
Here is a summary of facts concerning homogeneous and nonhomogeneous equa- 
tions whose coefficient matrix is A ¢ F"*" with rank (A) =r. 
e 
Homogeneous Equations Ax = 0 
> 
They are always consistent (the trivial solution x = 0 is a solution). 
> x= 0 is the only solution (ie., N (A) = 0) if and only if r =n. 
> 
For r<n, the general solution is 
N{A) = {x|x =2,,hi+az,,ho+--- +27,_,Hn—r, where xs, € F} 
= span {hy, ho,...,hp_,} 
(2.5.14) 

2.5 General Linear Systems 
213 
It is obtained by solving for the r basic variables in terms of the 
n—r 
free variables and organizing the coefficients of each free 
variable x», into a corresponding column h,, each of which is a 
particular homogeneous solution. As the free variables 
xy, vary 
over FF, (2.5.14) generates all possible homogeneous solutions—i.e., 
B= {hj,ho,...,h,_,} spans N (A), and, moreover, B isa linearly 
independent spanning set for N (A). 
An orthonormal spanning set B= {uj,u2,...,Un—r} 
for N(A) 
can always be constructed by using the procedure in the proof of 
Theorem 2.5.5. 
e 
Nonhomogeneous Equations Ax =b with 
b40 
Db 
They are consistent if and only if any of the following equivalent 
conditions hold. 
— A row of the form (00 --- 0| a) with a #0 never appears 
during row reduction of [A|b]. 
— b is anon-basic column in [A|b]. 
— rank[A|b] = rank (A). 
— b is a combination of the basic columns in A. 
— b is acombination of all columns in A. 
When consistent, the general solution is 
S=p+N (A) = Dor {z;7,hi aig rf, hg acter Lf,—-Hn—r; Lf, € F}. 
It is obtained by solving for the basic variables in terms of the free 
variables and accumulating the constant terms into p. 
p is a particular solution (i.e., Ap = b) obtained by setting each 
free variable to zero. 
p as well as the columns h; are uniquely determined—i.e., they 
do not depend on the echelon form that [A|b] is reduced to (see 
Exercise 2.5.31). 
If consistent, Ax = b has a unique solution if and only if any of the 
following equivalent statements hold. 
— r=n 
(the number of unknowns). 
— N(A)=0. 
— Ax =O 
possesses only the trivial solution. 

feet wwiywrilal 
s, 
>" 
(Qw +t 3yt52=1, 
; Qu +04 3y+5z=7, aes 
~ 
4wt 
4y+8z=0, 
.. 
4wt+ 
4y + 8z = 8, 
(Cis 
CO! eked a EAB 
'cues 
w+a+2y+3z=0, 
w+ot+2y+3z=5, 
c+ y+ z=0. 
EE 
e te Seo 
2.5.2. Consider the following system: 
aa 
22+ 2y+ 3z=0, 
4¢ + 8y + 12z = —4, 
62+ 2y+ az=—4. 
(a) Determine all values of a for which the system is consistent. 
(b) Determine all values of a for which there is a unique solution, 
and compute the solution for these cases. 
(c) Determine all values of a for which there are infinitely many 
different solutions, and give the general solution for these cases. 
2.5.3. Determine the general solution for each of the following homogeneous 
systems. 
2 
= () 
gy leg + a3 22450 
late 
0 
xc 
Zs 
(a) 
22,+4a%9+ 23+324=0 
 (b) 
321 +6%2+ 
2@3+ 4724 =0 
Ce aa 
: 
. 
date 
8+ 4y+2z2=0 
t+ t+ 273 
=) 
22+ y+z=0 
(c) 
321 
+ 323 + 324 =0 
(a) 
47+2y+z=0 
22, + %9+3%3+ 24=0 
eee 
@1+2%.+3273- 24=0 
8 + 5y+2z2=0 

2.5 General Linear Systems 
215 
2.5.4, Determine the general solution for each of the following nonhomogeneous 
2.5.5. 
2.5.6. 
2.5.7. 
2.5.8. 
systems. 
2 
=f 
Rs eae 
Metis 6, 
(gy Was Ras iene 
ong a aI B eS 
ie ee 
Of 
oY 2 =o, 
3%, 
+ 629+ w3+4r4=5. 
a a 
ak Ue 
8x + dy +2 =10. 
G1 + +273 
we 
20+ yt+z=2, 
(c) 301 
os = O04 =408 
(a) 
4g + 2y +2z=5, 
2%, + %9+3¢%3+ 
24 =3, 
6x + 3y + z = 8, 
%1+2%0+3273- 
24 =0. 
8x + 5y+z= 
8. 
Among all solutions that satisfy the homogeneous system 
xr+2y+z=0 
24 + 4y+z=0 
x+2y—z=0 
determine those that also satisfy the nonlinear constraint y — ry = 22. 
Among the solutions that satisfy the set of linear equations 
Gy+ %2+2%34+ 2074+ 45 
= 1 
22%, + 2%9 + 423 + 444+ 325 = 1, 
2x1 + 249 + 423 + 4a4 + 225 = 2, 
321 + 5X9 + 823 + 644+ O25 = 3, 
find all of those that also satisfy the two constraints 
(1 — x2)? — 4x2 =0, 
. 
De = t= 
0. 
TitA, en iso Watrix such that ae Gi; = Onion each t= 152.5, 
(i.e., each row sum is 0), explain why the columns of A are a linearly 
dependent set, and hence rank (A) <n. Hint. What is N (A)? 
Explain why a homogeneous system of m equations in n unknowns 
where m <n must always possess an infinite number of solutions. 

216 
Chapter 2 
Systems, Elimination, and Echelon Forms 
2.5.9. 
2.5.10. 
aL. 
2.5.12. 
2.5.13. 
2.5.14. 
2.5.15. 
2.5.16. 
adel 
ts 
If A isan mxXn matrix with rank (A) =m, explain why the system 
Ax =b must be consistent for every right-hand side b. 
Construct 
a 3x4 matrix A and 3x1 columns b and c such that 
Ax = b is consistent, but Ax = c is inconsistent. 
Consider two consistent systems Ax = b and Ax = c that differ 
only on the right-hand side. Is the system Ax = b+c 
also consistent? 
Explain why. 
If hy and hg are solutions of the same homogeneous system Ax = 0, 
must the sum h; + ho also be a solution? Explain why. 
If s; and sg are both solutions for the same nonhomogeneous system 
Ax=b 
with b#0, is the sum s; +S: also a solution? 
Is it possible for a parabola whose equation has the form y = a+8z2+7yx? 
to pass through the four points (0,1), (1,3), (2,15), and (3,37)? Why? 
Suppose that [A|b] is the augmented matrix for a consistent system of 
m equations in n unknowns where m > n. What must Ea _ look like 
when the system possesses a unique solution? 
Suppose that an augmented matrix [A|b] is reduced by means of Gaus- 
sian elimination to a row-echelon form [E|c]. If a row of the form 
CO 
>On 
ces 
OC On) ee 
does not appear in [E]c], is it possible that rows of this form could have 
appeared at earlier stages in the reduction process? Why? 
Suppose that A is the coefficient matrix for a homogeneous system of 
four equations in six unknowns and suppose that A has at least one 
nonzero row. 
(a) Determine the fewest number of free variables that are possible. 
(b) Determine the maximum number of free variables that are pos- 
sible. 

2.5 General Linear Systems 
217 
2.5.18. 
2.5.19. 
2.5.20. 
2.5.21. 
2.5.22. 
2.5.23. 
2.5.24. 
2.5.25. 
Construct a homogeneous system of three equations in four unknowns 
oO 
ss 
that has ro ( +24 
0 
a nonhomogeneous system of three equations in four unknowns that has 
n 
il 
—2 
= 3 
(:) 
+ a ) 
sie ai( ) 
as its general solution. 
0 
0 
1 
Abe ae 
4 
Let A=(2 5 "| 
and b= (9). 
By Wp 
12 
) Show that Ax =b 
is a consistent system. 
) Write the general solution in the form p+ N (A). 
) Sketch a picture of N(A) in R?. 
) Sketch a picture of the affine space 
p+ N(A) in R®. 
as its general solution, and then construct 
PPNOW 
Determine the nullspace of the following matrices. 
ior oo 
4 
it 
at 
jg YS 4s 
12 
29 9 
(ao A= | 6) 484. 9 
(D) Bie emis 
3 5: 8605 
ls Oued 
=3" 
1, 3 
For A= (2 3 
10), find N(A —3I) and N(A 
+ 2I). 
Se se Ned 
Determine 
N (A —. I) and N(A-— oI) for 
Beauly ahs ea eand er ae 
Use the procedure from the proof of Theorem 2.5.5 to create an or- 
1 
=2° 
=1 
3 
thonormal spanning set for 
N (A), where A = (- 4-9 +). 
Sy SG) 8 
9 
For A € F™*", prove that N(A) = NM = {(I— AtA)yly € F"™'}, 
where A! is the pseudoinverse of A that is defined on page 192. 
If A is the coefficient matrix for a homogeneous system consisting of 
four equations in eight unknowns and if there are five free variables, 
what is rank (A)? 

218 
Chapter 2 
Systems, Elimination, and Echelon Forms 
2.5.26. 
2.5.27. 
2.5.28. 
2.5.29. 
2.5.30. 
Let 
S€ R"*" be skew-symmetric—i.e., 
S = —S'. 
(a) Prove that 
I—S 
is nonsingular. 
(b) Prove that 
I+S is nonsingular. 
(c) Prove that A = (I+$)(I—S)~! is an orthogonal matrix. 
Hint. Consider Theorem 1.10.1 on page 92 and Exercise 1.8.8 
on page 80. 
If BAx = Bb is a consistent system for some B # 0, is Ax = b 
consistent? Explain why. 
Recall from Exercise 2.4.34 on page 199 that for 
A € F™*"", be F™™?, 
A* Ax = A*b is called the system of normal equations (also see the 
theory of least squares on page 481). This system is always consistent 
regardless of whether or not Ax = b is consistent. In the case when 
Ax = b is consistent, prove that its solution set is exactly the same as 
the solution set for the normal equations. 
Caution! As pointed out in Exercise 2.5.27, BAp = Bb does not 
guarantee that Ap =b 
for all B. 
1 
2 
4 
Consider the tall, skinny matrix A = 
999 1000/ 
so9x2 
(a) Explain why Ax = b has a unique solution for each b that 
makes the system consistent 
(b) Do the normal equations A*Ax = A'b also have a unique 
solution? 
In order to grow a certain crop, it is recommended that each square foot 
of ground be treated with 10 units of phosphorous, 9 units of potassium, 
and 19 units of nitrogen. Suppose that there are three brands of fertilizer 
on the market— say brand ¥, brand ), and brand Z. One pound of 
brand % contains 2 units of phosphorous, 3 units of potassium, and 5 
units of nitrogen. One pound of brand Y contains 1 unit of phosphorous, 
3 units of potassium, and 4 units of nitrogen. One pound of brand Z 
contains only 1 unit of phosphorous and 1 unit of nitrogen. 
(a) Determine whether or not it is possible to meet exactly the rec- 
ommendation by applying some combination of the three brands 
of fertilizer. 
(b) Take into account the obvious fact that a negative number of 
pounds of any brand can never be applied, and suppose that 

2.5 General Linear Systems 
219 
2.5.31. 
2.5.32. 
because of the way fertilizer is sold only an integral number of 
pounds of each brand will be applied. Under these constraints, 
determine all possible combinations of the three brands that can 
be applied to satisfy the recommendations exactly. 
(c) Suppose that brand ¥ costs $1 per pound, brand Y costs $6 
per pound, and brand Z costs $3 per pound. Determine the 
least expensive solution that will satisfy the recommendations 
exactly as well as the constraints of part (b). 
For A € F™*" with rank(A) =r, 
lett 
E'"' A and F'\" A be 
two row-echelon forms, and consider using each of them to determine 
the general solution for Ax = 0 by solving for the r basic (or pivot) 
variables in terms of the n—r free (or non-pivot) variables xf, and by 
writing the result in the form 
R= 
eT. 
ee SS xp,h; (using E) 
and 
x= De "F,Qi 
(using F). 
i=1 
eal 
Explain why h; = g; for each 7. This implies that when the general 
solution p+ N(A) for a consistent nonhomogeneous system Ax = b 
is obtained as illustrated in (2.5.12), the particular solution p is also 
uniquely determined. 
ie 
2 
jh. 
8} 
il 
2 
4 
-1 
3 
8 
Use 
A= 
]|]1 
2 
3 5 
71] 
along with an unreduced row-echelon 
2 
4 
2 
(6) 
2 
3 
6 
I 18} 
form and then with Ea, to determine N (A) to corroborate the state- 
ment in Exercise 2.5.31. 

220 
Chapter 2 
Systems, Elimination, and Echelon Forms 
2.6 TWO TYPICAL APPLICATIONS 
Systems of linear equations arise in practical applications in a myriad of different 
ways, so it is impossible to adequately illustrate all of them. Instead, two typical 
examples are singled out below to illustrate why linear systems are frequently 
large in size and to show why there is often a special structure to the linear 
systems that arise in practice. 
Two-Point Boundary Value Problems 
Given an interval 
[a,b] and two numbers a and £, consider the problem of 
trying to find a function y(t) that satisfies the differential equation 
u(t)y"(t)+v(t)y/(t)+w(t)y(t) = f(), 
where 
y(a) =a and y(b) = 8. (2.6.1) 
All values of the functions 
u, 
v, 
w, and f are assumed to be known on 
[a,b]. Because the unknown function y(t) is specified at the boundary points 
a and b, problem (2.6.1) is called a two-point boundary value problem. Such 
problems abound in nature, and they can be difficult to handle because it is 
often not possible to express y(t) in terms of elementary functions. Numerical 
methods are usually employed to approximate y(t) at discrete points inside 
[a,b]. Approximations are produced by subdividing the interval [a,b] into n+1 
equal subintervals, each of length h = (b—a)/(n +1) as shown below. 
h 
h 
h 
SS Eee 
eos 
a 
to =a 
tj =a+h 
tg =a+2h 
a 
th =a+nh 
tna1 =b 
Derivative approximations at the interior nodes (or grid points) t; 
=a+ih are 
made by using a Taylor series expansion y(t) = >> y*)(ti)(t — ti)*/k! to 
write 
y(t) h? 
y(t) h3 
y (te + hy = yt) ey bee 
ee 
; 
: 
(2.6.2) 
; 
= 
a 
ele 
y(t; )h? 
y!"(t;)h > 
-0. 
and then subtracting and adding these expressions to produce 
y'(ti) = 2 
=z 
+ O(n) 
and 
72 
+ O(h4), 

2.6 Two Typical Applications 
221 
where O(h") represents all terms! containing p'" and higher powers of h. The 
resulting approximations 
y'(t) wv Yetebe) = ute =A) 
y(tirh) 
— 2y(ti) + y(t +h) 
' 
2h 
h? 
are called centered difference approximations, and they are preferred over less 
accurate one-sided approximations such as 
y' (ti) ae y(t; + . y(ti) 
ae 
y' (ti) Ss y(t) e h) 
The value h = (b—a)/(n+1) is called the step size. Smaller step sizes produce 
better derivative approximations, so obtaining an accurate solution usually re- 
quires a small step size and a large number of grid points. Evaluating the centered 
difference approximations at each grid point and substituting the result into the 
original differential equation (2.6.1) produces a system of n linear equations in 
nm unknowns, where the unknowns are the values y(t;). A simple example can 
serve to illustrate this point. 
and y"(t;) © 
6-5) 
Example 
Suppose that f(t) is a known function and consider the two-point boundary 
value problem 
y(t) = f(t) on [0,1] with (0) = y(1) =0. 
The goal is to approximate the values of y at n equally spaced grid points 
t; interior to [0,1]. The step size is therefore h = 1/(n+1). For the sake of 
convenience, let y; = y(t;) and f; = f(t;). Using the approximation in (2.6.3) 
along with yo =0 and yn+i =0 produces 
i-1 — 241 + Yi41 
See 
which generates the system of linear equations 
—Yyjs-1 + 2y; — Yi & —=p2f, 
«ford = 1, 
200..,7 
(The signs are chosen to make the 2's positive to be consistent with later devel- 
opments.) The matrix formulation of this system is Ay ~ b, where 
2 
-l1 
Y1 
fi 
sl 
@ 
=1l 
Y2 
fa 
A= 
ees 
(oe 
ie 
Sle 
(2.6.4). 
=—1 
al 
Yn-1 
Sat 
peu 
27 
nxn 
Yn 
fn 
Saying that a function g(h) 
is O(h?) 
(read as 
'gy is big-oh of h?") means that g(h)/hP 
remains bounded as h + 0 but g(h)/h% becomes unbounded when q > p. Intuitively, this 
says that as h — 0, the function g(h) goes to zero as fast as h? goes to zero. See page 932 
for more discussion of the "big-oh" and "little-oh" notations. 

222 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Approximate values of the unknown function y(t) at the grid points ft; are 
obtained by solving Ay = b (which turns out to be a nonsingular system). 
Larger values of n produce smaller values of h and hence better approximations 
to the exact values y(t;). Consequently, the solution to a large system of linear 
equations is required to gain an accurate sense of what y(t) looks like. 
Notice the pattern of the entries in A. The nonzero elements occur only on 
the main diagonal, the subdiagonal, and the superdiagonal lines. Such a matrix 
is said to be tridiagonal. This is characteristic in the sense that when finite 
difference approximations are applied to the general two-point boundary value 
problem, a tridiagonal system is the result. The coefficient matrix in this example 
is also symmetric, which is an added bonus for numerical computation—more 
about this comes later in §2.10. 
Tridiagonal systems are particularly nice because they are relatively inex- 
pensive to solve. When Gaussian elimination is applied, only two flops are needed 
at each step of the triangularization process. Furthermore, Gaussian elimination 
preserves all of the zero entries that were present in the original tridiagonal sys- 
tem, so back substitution is also cheap to execute because there are at most only 
two flops required at each substitution step (see Exercise 2.10.20 on page 282). 
Electrical Circuits 
The theory of electrical circuits is an interesting application that naturally gives 
rise to rectangular systems of linear equations. In a direct current circuit con- 
taining resistances and sources of electromotive force (abbreviated EMF) such as 
batteries, a point at which three or more conductors are joined is called a node 
or branch point of the circuit, and a closed conduction path is called a loop. Any 
part of a circuit between two adjoining nodes is called a branch of the circuit. 
The circuit shown below in Figure 2.6.1 is a typical example that contains four 
nodes, seven loops, and six branches. 
Ey 
E4 
FIGURE 2.6.1: CIRCUIT WITH FOUR NODES, SEVEN LOOPS, AND SIX BRANCHES 
The problem is to relate the currents J, in each branch to the resistances 
Ry and the EMFs Ex. This is accomplished by using Ohm's law in conjunction 

Kirchhoff's rules may be used without knowing the directions of the currents 
and EMFs in advance. You may arbitrarily assign directions. If negative values 
emerge in the final solution, then the actual direction is opposite to that assumed. 
To apply the node rule, consider a current to be positive if its direction is toward 
the node—otherwise, consider the current to be negative. The node rule always 
generates a homogeneous system. For example, applying the node rule to the 
circuit in Figure 2.6.1 yields the following four homogeneous equations in the six 
unknowns Jj, Io,..., 16. 
For node 1: 
I, —Ip —I5 =0 
For node 2: 
—I; —Ig3 +14 =0 
For node 3: 
Ig+I5+Ig =0 
For node 4: 
Ig — Ig —Ig =0 
To apply the loop rule a direction (clockwise or counterclockwise) must be 
chosen as the positive direction, and all EMFs and currents in that direction 
are considered positive, and those in the opposite direction are negative. It is 
possible for a current to be considered positive for the node rule but considered 
negative when it is used in the loop rule. If the positive direction is considered 
to be clockwise in each case, then applying the loop rule to the three indicated 
loops A, B, and C in the circuit shown in Figure 2.6.1 produces the following 
three nonhomogeneous equations in six unknowns—the I;,'s are treated as the 
unknowns, while the R;'s and E;,'s are assumed to be known. 
For loop A: 
1,R; —13R3+15R5 = E; — £3 
For loop B: 
IgRo—1sR5+1IeRe = E2 
For loop C: 
13R3+14R4—IgRe = E3 + Es 

224 
Chapter 2 
Systems, Elimination, and Echelon Forms 
There are 4 additional loops that also produce loop equations thereby mak- 
ing a total of 11 equations (4 nodal equations and 7 loop equations) in 6 un- 
knowns. If the circuit is in a state of equilibrium, then the physics of the situation 
dictates that for each set of EMFs Ex, the corresponding currents J, must be 
uniquely determined. In other words, physics guarantees that the 11 x 6 system 
produced by applying the two Kirchhoff rules must be consistent and possess a 
unique solution. 
Suppose that Ajixgx = b is the 116 system generated by Kirch- 
hoff's rules. It follows from Theorem 2.5.8 (page 212) that this system has a 
unique solution if and only if rank (A) = 6, the number of unknowns. Further- 
more, Theorem 2.5.1 (page 202) says that the system is consistent if and only if 
rank|A|b] = rank (A), so combining these two facts yields the conclusion that 
rank|A\|b] = 6. 
Consequently, when [A|b] is reduced to Eyajp} there will be exactly six nonzero 
rows and five zero rows, and therefore, five of the original eleven equations are 
redundant in the sense that they can be "zeroed out" by forming combinations 
of some set of six "independent" equations. It is desirable to know beforehand 
which of the eleven equations will be the redundant ones. 
Notice that in using the node rule, the equation corresponding to node 4 
is simply the negative sum of the equations for nodes 1, 2, and 3, and that the 
first three equations are independent in the sense that no one of the three can 
be written as a combination of any other two. This situation is typical. For a 
general circuit with 
nm nodes, it can be demonstrated that the equations for 
the first m —1 nodes are independent, and the equation for the last node is 
redundant—see the discussion on page 449. 
The loop rule also can generate redundant equations. Only simple loops 
(loops not containing smaller loops) give rise to independent equations. For ex- 
ample, consider the loop consisting of the three exterior branches in the circuit 
shown in Figure 2.6.1. Applying the loop rule to this large loop will produce 
no new information because the large loop can be constructed by "adding" the 
three simple loops A, B, and C contained within. The equation associated 
with the large outside loop is 
T,R, + 12Re4+ 144R4 = FE, + Ep + Es, 
which is precisely the sum of the equations that correspond to the three compo- 
nent loops A, B, and C. This phenomenon will hold in general so that only 
the simple loops need to be considered when using the loop rule. 
In other words, the more general 11 x 6 rectangular system can be replaced 
by an equivalent 6 x 6 square system that has a unique solution by dropping 
the last nodal equation and using only the simple loop equations—see Exercise 
2.6.4. This is characteristic of practical work in general. The physics of a problem 
together with natural constraints can often be employed to replace a general 
rectangular system with one that is square and possesses a unique solution. 

2.6 Two Typical Applications 
225 
Exercises for section 2.6 
2.6.1. 
2.6.2. 
2.6.3. 
2.6.4. 
2.6.5. 
Divide the interval [0, 1] into five equal subintervals, and apply the finite 
difference method in order to approximate the solution of the two-point 
boundary value problem 
y(t) =125t, 
(0) = y(1) =0 
at the four interior grid points. Compare your approximate values at the 
grid points with the exact solution at the grid points. 
Note: You should not expect very accurate approximations with only 
four interior grid points. 
Divide [0,1] into n+1 equal subintervals, and apply the finite difference 
approximation method to derive the linear system associated with the 
two-point boundary value problem 
y(t) — y(t) = F(t), 
yO) = y(1) = 0. 
Divide [0,1] into five equal subintervals, and approximate the solution 
to 
y(t) — y(t) = 125t, 
~—-y(0) = (1) = 0 
at the four interior grid points. Compare the approximations with the 
exact values at the grid points. 
Suppose that R; = 7 ohms and EF; = 7 volts in the circuit shown in 
Figure 2.6.1. 
(a) Determine the six indicated currents. 
(b) Select node number 1 to use as a reference point and fix its 
potential to be 0 volts. With respect to this reference, calculate 
the potentials at the other three nodes. Check your answer by 
verifying the loop rule for each loop in the circuit. 
Determine the three currents indicated in the following circuit. 
1Q 
1Q 
9 volts 

wow). «ijl 
em 
Sa 
1% 
to 
aA8 eiyigo fer plein' 
(a) 
(b) 
(c) 
(d) 
(e) 
(f) 
(g) 
How many nodes does the circuit contain? 
How many branches does the circuit contain? 
Determine the total number of loops and then determine the 
number of simple loops. 
Demonstrate that the simple loop equations form an "indepen- 
dent" system of equations in the sense that there are no redun- 
dant equations. 
Verify that any three of the nodal equations constitute an "in- 
dependent" system of equations. 
Verify that the loop equation associated with the loop containing 
Ri, Re, Rs, and Ry can be expressed as the sum of the two 
equations associated with the two simple loops contained in the 
larger loop. 
Determine the indicated current J if Rj = Ro = R3 = Ry =1 
ohm, Rs = Rg = 5 ohms, and E = 5 volts. 

2.7 The Kronecker Product 
22h 
2.7 
THE KRONECKER PRODUCT 
Standard matrix multiplication as defined in Definition 1.7.2 on page 56 is by 
far the most useful way to compose (or multiply) matrices, but it is not the only 
way. The following definition presents another kind of product that occasionally 
arises in specialized applications. 
2 7A. Definition. The oe vlog of t 
two matrices Amxn as 
: 
Bpxq is defined to be the mp x a4 ia dae 
/[ aB apB + dipB\ 
one 
ee 
am B ana aoe OmnB 
This is also called the 
tensor product or the direct product. 
A@B= 
Lene 
ar eg 
ee 
4 
1 
4 
4 
ANG 
be \ sO 
4 212 
a(t oa 
(Pea) ee Pi eae) 
Leopold Kronecker (1823-1891) was born in Liegnitz, Prussia (now Legnica, Poland), to a 
wealthy business family that hired private tutors to educate him until he enrolled at Gymna- 
sium at Liegnitz where his mathematical talents were recognized by Eduard Kummer (1810— 
1893), who became his mentor and lifelong colleague. Kronecker went to Berlin University 
in 1841 to earn his doctorate, writing on algebraic number theory, under the supervision of 
Dirichlet (see page 388). Rather than pursuing a standard academic career, Kronecker returned 
to Liegnitz to marry his cousin and become involved in his uncle's banking business. But he 
never lost his enjoyment of mathematics. After estate and business interests were left to oth- 
ers in 1855, Kronecker joined Kummer in Berlin who had just arrived to occupy the position 
vacated by Dirichlet's move to Gottingen. Kronecker didn't need a salary, so he didn't teach 
or hold a university appointment, but his research activities led to his election to the Berlin 
Academy in 1860. He declined the offer of the mathematics chair in Gottingen in 1868, but he 
eventually accepted the chair in Berlin that was vacated upon Kummer's retirement in 1883. 
Kronecker held the unconventional view that mathematics should be reduced to arguments 
that involve only integers and a finite number of steps, and he questioned the validity of non- 
constructive existence proofs, so he didn't like the use of irrational or transcendental numbers. 
Kronecker became famous for saying that "God created the integers, all else is the work of 
man." Kronecker's significant influence led to animosity with people of differing philosophies 
such as Georg Cantor (1845-1918), whose publications Kronecker tried to block. Kronecker's 
small physical size was another sensitive issue. After Hermann Schwarz (see page 27), who 
was Kummer's son-in-law and a student of Weierstrass (see page 585), tried to make a joke 
involving Weierstrass's large physique by stating that "he who does not honor the Smaller, is 
not worthy of the Greater," Kronecker had no further dealings with Schwarz. 

228 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Le yaeet 
ee 
le Aisle S10 
fife 1759 hae, 
ey Mai ei: eam. 
(I@A)= 
yey hae 
il ae 
Pegg OO yo 3" 4 
Prnsretes | i aiid) 
poe 
O 1 <8 
and 
(3 4 2(2 °) ave ¢) 
edi: ae 
aie 
01 
01 
rau ip a bol Ai fo Se a 
(A@I)= 
1 0 
1 <0 
caeeel 
= \ dtoet 
OG =3-° 0 
0(4 ) is '4 -3(4 a 
Soteo 1 0 = 
Properties of the Kronecker Product 
The Kronecker product has a variety of interesting properties, but those in the 
following theorem are some of the basic ones. 
2.7.2. Theorem. For conformable matrices, and scalars a, each of the 
following statements is true when the indicated sums and products exist. 
e A®(B@C)=(ASB) 
eC. 
(2.7.1) 
e A®(B+C)=(A@B)+(A@C). 
(2.7.2) 
e (A+B) ®C=(A®@C)+(BSC). 
(2.7.3) 
e (A®B)(C@D) = (AC) @ (BD). 
(2.7.4) 
e a(A @B) = (aA) 
®B=A® 
(aB). 
(2.7.5) 
e (A@B)* =A*@B*. 
(2.7.6) 
e rank (A ® B) = [rank (A)|[rank (B)|. 
(240) 
Assume A is mx m and B is n x n for the following. 
¢ (A®I,)(Imn ® B)=A®B=(I,, ®B)(A @l,). 
(2.7.8) 
e trace(A @B) = [trace (A)|[trace (B)}. 
(2.7.9) 
e det (A @B) = [det (A)|"[det (B)]™. 
(2.7.10) 
e (A®B)!=A~!@B™! for nonsingular matrices. 
(2.7.11) 
e (A@B)! =A! @Bt in general. 
(2.7.12) 
e If A and B are unitary, then 
A ®B is unitary. 
(2.7.13) 
Eigenvalue properties are given in Exercise 3.2.29 on page 321. 
The proof of each statement follows more or less directly from the definition 
together with elementary properties of linear algebra. To illustrate this, the proof 
of (2.7.4) along with (2.7.11) and (2.7.7) is given. The details of the remaining 
properties are left to the reader. 

2.7 The Kronecker Product 
229 
Proof of (2.7.4). 
To see that (A ®B)(C ®@ D) = (AC) @ (BD), suppose that 
A ismxn, B is pxq, C is nx 2, and D is qx y. Notice that 
Aijbst = [A ® BlpG—1)+8,¢(j-1)4 
and 
Cjudty = [C @ D]q(j—1)-+2, y(u—1)4+0 
so 
[(A ® B)(C ® LEY euler y(u-1)+v — SS Ss 
Aig DstCjudiy 
4} 
18 
=) aiytju > Darden = [AC]iu{BD]ov = [(AC) @ (BD)] 4-140, yu-)40- 
j 
t 
Thus (A ®B)(C@D)=(AC)@(BD). 
Proof of (2.7.11). 
Use (2.7.4) to prove if A and B are nonsingular, then 
(A@B)'=A-!@B-! by writing 
(A @B)(A~! @B~) = (AA~) @(BB"!) =I@I=I1. 
Proof of (2.7.7). 
The fact that rank (A ® B) = [rank (A)][rank (B)]| now fol- 
lows because if rank (A) = rq and rank(B) = 1», then there are nonsingular 
matrices Pz, Qa, Pp», and Q, such that 
EAC 
att 
\omandem 
by 
BOs ara 
(see Exercise 2.2.25 on page 158). Repeated use of (2.7.4) yields 
I,, 
0 
ie 
(0) 
(P. @ Ps)(A ®B)(Qz ® Qp) = (PoAQa) 8 (P»BQs) = ("GF 9) @ (7B o)- 
The matrices (P, © Py) and (Q, © Q,) are each nonsingular by (2.7.11), so 
rank (A ® B) = rank Me Ay @ Ce "4 — rank 
2%: 
Ser 
because this matrix has rg copies of I,,. 
& 
The Vec Operation 
An operation that is often used in conjunction with the Kronecker product is 
the vec operation defined below. 

One of the most important functions of the vec operator is to deal with 
the lack of commutivity in expressions such as AX + XB by using the fact 
that vec(AX + XB) = [(I, ® A) +(B? @In)| vec(X). This along with two 
supporting properties are established in the next theorem. 
Proof of (2.7.14). 
Use the fact that (AX),; = AX,; to conclude that 
A 
Xe 
AXx1 
(AX) «1 
A 
X42 
AX.2 
(AX) «2 
(In ® A)vec(X) = 
e 
. 
|= 
. 
= 
=vec(AX). 
I Eo os 
Axia 
(Axis 
Proof of (2.7.15). 
Use the fact that (XB),. = XB, = do; X«jbje to obtain 
mle 
pal 
oo 
(ea 
eel 
a bj X75 
T 
bi2T 
beet 
-:: 
bngl 
ed 
Do; 0j2X«5 
(BY @In)vec(X)=| 
| 
tae 
mes 
| oe 
binl 
bonI 
Sil 
bani 
Xan 
pa bes 
(XB) a1 
(XB).«2 
= 
=vec(XB). 
& 
(XB).2 
Some texts define vec(X) to be the vector obtained by moving row wise through X to stack 
its entries into a column. In this case the properties in Theorem 2.7.4 need to be reformulated 
to reflect the different ordering. 

2.7 The Kronecker Product 
231 
Proof of (2.7.16). 
It follows from the definition of vec that for two matrices U 
and V of the same size, vec(U + V) = vec(U) + vec(V), so combining (2.7.14) 
and (2.7.15) yields 
vec(AX + XB) = vec(AX) + vec(XB) = (I, @ A)vec(X) + (B? @ I) vec(X) 
= [(In ® A) + (B7 @1,)]| vec(X). 
& 
Solving Matrix Equations 
For matrices Amxm, Brxn, Cmxn, and Xmxn, property (2.7.16) in Theorem 
2.7.4 allows the matrix equation AX + XB = C to be transformed into an 
ordinary linear system Mx = c by setting 
M = [(, @ A) +(B' @I,)], 
x=vec(X), 
and 
c=vec(C). 
For example, to solve AX 
+ XB = C for A = i a B= 
a and 
Or ¢ cl convert the matrix equation into a 4 x 4 linear system Mx = c, 
OR 
where 
he 
Get 
ae 
1 
ik 
a) 
(0) 
0 
M = [(In @A)+ (BY @Im)J={7 
9 1 1}>*= 
[a] ad c= 
[0 
abies 
> 
229 
1 
3 
to obtain 
the solution as x = 1/5 
ee 
, or equivalently X = 1/5 (_} a 
9 
Exercises for section 2.7 
2.7.1. Solve the matrix equation AX +XB=C 
for X, where A = 6 ar 
(Qik 
Pil 
B= 
a and C= 'é ae 
2.7.2. Apply Theorem 2.7.4 (page 230) to find all matrices that commute with 
Py 
wezipsnerg 
2.7.3. Explain why (A @B)* = A* @ BF for every k =1,2,3,.... 
2.7.4. Prove that (A@I,)Imn ®B)=A®@B=(I, ®B)(A@I,) for Anxm 
and Bnxn—this is property (2.7.8). 

232 
Chapter 2 
Systems, Elimination, and Echelon Forms 
2.7.5. 
2.7.6. 
Use 2x2 matrices to demonstrate the validity of Cle aie naa 
— 
(1 
bi2 
that A@(B@C) = (A@B)9C for A=(%2 77), 
B= (ju pa), 
and C= Ne nl 
C21 
C22 
2 
—1 
—1 
2 
—-1 
The Discrete Laplacian Matrix. Let A = 
AS 
TR Bece 
be 
—l 
2 
—1 
—1 
2 
the n*"-order finite difference matrix from (2.6.4) on page 221, and let 
4 
-1 
=1 
4 -1 
T= 
So 
Bay 
ott 
. Show that 
—1 
4 -1 
aa 
4/ 
nxn 
T 
-I 
-I 
T 
-I 
(I, ®A)+(A @I,) = 
ane 
is 
== L292 
-I 
T 
-I 
-I 
T 
This matrix L is called the discrete Laplacian matrix because it repre- 
sents a finite-difference discretization of the two-dimensional Laplacian 
operator 
8x? © By?" 
It is illustrated in the example on page 388 how L is used to numerically 
solve Laplace's equation Vu = 0 to determine the heat flow in a square 
plate. There is a natural extension in the sense that a discretization of 
O7u bs O7u ae O7u 
b 
ea 
a 
we 
LoVe 
Ox? 
Oy? 
Oz? 
: 
: 
the three-dimensional Laplacian Vu = 
L,3xn3 = (In 
@1n 
@A)+ (In 
@AOI,)+ (AOI, @I,), 
thus providing an elegant connection between finite-difference discretiza- 
tions of the one-, two-, and three-dimensional Laplacians—and beyond. 

2.8 Making Gaussian Elimination Work 
233 
2.8 MAKING GAUSSIAN ELIMINATION WORK 
After basic Gaussian elimination is understood, it must be turned into a practi- 
cal algorithm that can be used for realistic applications because in its pure form 
Gaussian elimination can fail miserably. For pencil and paper computations in- 
volving exact arithmetic, the strategy is to keep things as simple as possible (like 
avoiding messy fractions) in order to minimize those "stupid arithmetic errors" 
that we are all prone to make. But very few problems in the real world are of 
the textbook variety, and practical applications involving linear systems usually 
demand the use of a digital computer. Computers don't care about messy frac- 
tions, and they don't introduce errors of the "stupid" variety. Computers that 
are used to solve large practical problems produce a more predictable kind of 
error, called roundoff error, and it's important! to spend a little time up front 
to understand this kind of error and its effects on solving linear systems and on 
matrix computations in general. 
Floating Point Numbers 
t 
Numerical computation in a computer is performed by approximating the real 
numbers R with a finite set F of floating-point numbers described below. 
2.8.1. Definition. A t-digit, base-b floating-point number is 
7 eae +.d4d2 se 
: dy x bP 
with 
dy a 0. 
The base 6 is a positive integer. It is standard to use 6 = 2 (binary 
representation) for internal machine manipulations, but it is convenient 
to use b = 10 for text-book examples. The digits d; are nonnegative 
integers in the range 0<d; <b—1. The value of t (the number of 
digits) is called the precision, and it is generally set by software or 
hardware engineers. The exponent p is a bounded integer that also 
depends on software or hardware design. * 
Floating-point numbers are just adaptations of the familiar concept of sci- 
entific notation with b = 10, which is the value used in examples that follow. 
For any fixed set of values for t, 6b, and p, the corresponding set F of floating- 
point numbers is necessarily a finite set, so many real numbers are excluded from 
F. Those exclusions are approximated by standard rounding methods. 
The electronic computer was one of the most significant scientific and technological develop- 
ments of the twentieth century, and it altered the course of science for all future time. A student 
who passes through a contemporary linear algebra course without gaining an appreciation for 
at least the elementary issues of what is involved when a computer is used to solve a practical 
linear system is missing a fundamental aspect of applied mathematics. 
The definitions given here suffice for the purposes at hand. Readers interested in the complete 
description of the subtleties involved in defining floating-point numbers can find it in the text 
Matrix Computations, 4th dition, by G. H. Golub and C. F. Van Loan. 

234 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Rounding Convention 
. 
There is more than one way to approximate real numbers with floating-point 
numbers by means of rounding. This text adopts the most common rounding 
convention that approximates a real number x with a floating-point number 
fl(z) that is defined to be the nearest element in F to z. In case of a tie, 
round away from 0. For example, if the exact base-10 decimal expansion of a 
real number x is « = +.didq-+-ddt41°++ X 10? with d; 
#0, then the t-digit 
floating-point approximation of x is defined to be 
i 
[+.did2 ao dt} x 10? 
if di+t <= 5, 
ae { 
[+.dido-+++de-1(d_ +1)] x 10? 
if dj41 > 5. 
In other words, to determine a t-digit floating-point approximation, the (t+1)st 
digit must be computed and examined to decide whether or not the t*" digit 
should be bumped up. This is just the simple rounding rule that is taught in 
elementary school. For example, in 2-digit, base-10 floating-point arithmetic, 
fl (3/80) = fi(.0375) = fl(.375 x 10-*) = .38 x 10~* = .038. 
Standard Rules of Arithmetic May Not Hold 
Strange things can occur in floating-point arithmetic. For example, notice that 
if 7 = 21/2 and €=11/2, and if 2-digit base-10 arithmetic is used, then 
Fi(n +) F Fl(n) + FUE) 
and — fl(n€) 
# fl(n) Fl(§). 
Furthermore, several familiar rules of real arithmetic do not hold for floating- 
point arithmetic—associativity is one outstanding example. In other words, 
fU( [fU(a) + FUB)] + FUCy)) # FIC F(a) + fU[fU(B) + FU()]). 
For example, in 8-digit floating-point arithmetic, fu( [#1 aS 101°)] de 101°) =; 
but f( + fi{101° — 10?°]) =1. Order matters! The same is true for floating- 
point multiplication. These, among other reasons, make the analysis of floating- 
point computation both tedious and difficult. It also means that care must be 
exercised when working the examples and exercises that follow. While calculators 
and computers can be instructed to display varying numbers of digits, most have 
a fixed internal precision with which all calculations are made before numbers are 
displayed, and this internal precision cannot be altered. The internal precision of 
your device is almost certainly greater than the precision called for by the exam- 
ples and exercises that follow, so each time you make a t-digit calculation with 
a calculator or computer you should manually round the result to t significant 
digits, and then manually re-enter the rounded number before proceeding to the 
next calculation. The goal is to mimic the internal floating-point processor in a 
machine, so do all calculations and rounding manually, and don't take shortcuts 
by chaining arithmetic operations together. 

2.8 Making Gaussian Elimination Work 
235 
Catastrophic Cancellation 
Finite-precision floating-point arithmetic introduces inherent errors due to the 
rounding process, but some of these errors occur in a surprising way. While you 
may not be shocked by the fact that dividing a moderate sized number by a 
relatively tiny number can cause problems, you might be surprised to learn that 
subtraction of two nearly equal floating-point numbers can also be problematic. 
To understand why, consider two numbers whose exact values are a = .1235 
and 6 = .1234, and compare the exact difference with the difference obtained 
using 3-digit floating-point arithmetic. 
a— 6 =.0001 (exact) 
and 
fil(a)— fl(8) = .124 — .123 = .001 (3-digit fl) 
The relative error produced by floating-point subtraction is 
| (exact difference) — (3-digit fl difference)| 
| (exact difference) | 
ie 
relative error = 
In other words, floating-point subtraction produces a 900% relative error! If you 
think that low precision is the reason for this, make the same calculations with 
as high a precision as you wish using 
of 
till) 
1935; 
8 
= AL-++1284, 
f(a) = .11---124, 
fl B= 511. --123, 
to see that the results are exactly the same. In general, when floating-point ap- 
proximations of nearly equal exact quantities are subtracted, there will be a large 
relative error between the exact difference and the floating-point difference be- 
cause the leading significant digits are lost when subtracting the approximations, 
so the difference between the floating-point approximations is forced to depend 
on the uncertainties in the trailing digits. This phenomenon is called catastrophic 
cancellation. Another illuminating example is given in Exercise 2.8.5. 
Exact Gaussian Elimination vs. Floating-Point Gaussian Elimination 
Compare the results of executing Gaussian elimination using floating-point arith- 
metic for solving a linear system with the results obtained by using Gaussian 
elimination with exact arithmetic on the following simple system. 
47x + 28y = 19 
(2.8.1) 
89x + 53y = 36 
To apply Gaussian elimination using exact arithmetic, multiply the first equation 
by the multiplier m = 89/47 and subtract the result from the second equation 
to produce the triangular system 
hig 
Oe 
| 19 
Ceeesiy 
47 
Lares 

236 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Back substitution yields the exact solution 
g=Y° 
and 
"y= =1. 
Now compute the floating-point solution using Gaussian elimination with 3-digit 
arithmetic. The 3-digit multiplier is 
fl(m) = fl (3) = .189 x 10! = 1.89. 
Working from inside the parentheses to the outside produces 
fi( 
fl(m) si(a7)) = fl(1.89 x 47) = .888 x 10? = 88.8, 
fl ( 
fl(m) f1(28)) = fl(1.89 x 28) = .529 x 10? = 52.9, 
fi ( 
fl(m) f1(19)) = fl(1.89 x 19) = .359 x 10? = 35.9, 
so the result of the first step of 3-digit Gaussian elimination is 
AT 
28 
19 
) 
( 
f1(89 — 88.8) 
f1(53 — 52.9) 
f1(36 — 35.9) 
_ 
[AT 
28 
19 
"(3 2 | a): 
The goal is to triangularize the system—i.e., to produce a 0 in the (2, 1)-position. 
However, this cannot be accomplished with 3-digit arithmetic, so we are faced 
with a dilemma. Back substitution cannot be executed until the value 
.2 is 
annihilated, but Gaussian elimination is incapable of doing so. As crude as it 
may sound, the best strategy is to just cheat! In other words, we will henceforth 
agree simply to enter 0 in the position that we are trying to annihilate, regardless 
of the value of the floating-point number that might actually appear. The value 
of the position being annihilated is not even computed. In other words, don't 
even bother computing 
fl [so 
~ fu( fim) fa) = fl(89 — 88.8) = .2. 
Just enter 0 in the (2,1)-position and blindly keep going. Thus the result of 
3-digit Gaussian elimination for this example is 
AT 
28 
19 
Or 
od 
a i 
be 
Apply 3-digit back substitution to obtain the 3-digit floating-point solution 

2.8 Making Gaussian Elimination Work 
237 
False Expectations 
It is only natural to expect that if three significant digits are carried throughout 
the process, then the floating-point solution should agree with the exact solution 
in three significant places. But the vast discrepancy between the exact solution 
(1,—1) and the 3-digit solution (—.191,1) in the preceding example quickly 
dashes any such hope. In fact, the floating-point computation failed to even 
produce the correct signs! This example illustrates some of the problems that can 
be encountered while trying to solve linear systems with Gaussian elimination 
and floating-point arithmetic. 
It is tempting to write off the disaster in the preceding example to the fact 
that .2 was replaced by 0 in the (2,1)-position. While it had an effect in this 
example, subsequent examples in (2.8.2) and (2.8.7) below show that even when 
no cheating is required to produce a triangular system, the round-off error alone 
can still produce horrible results. 
Sometimes using a higher precision may help, but this is not always possible 
because on all machines there are natural limits that make extended precision 
arithmetic impractical past a certain point. Even if it is possible to increase 
the precision, it may not compensate because there are many cases for which an 
increase in precision does not produce a comparable decrease in the accumulated 
roundoff error. Given any finite precision t, it is not difficult to provide examples 
of linear systems for which the computed t-digit solution is just as bad as the 
one in the 3-digit example above. 
Order Matters in Practice 
Although the effects of rounding in Gaussian elimination cannot completely be 
eliminated, there are some simple techniques that can help to minimize them. 
When using exact arithmetic, the order in which the equations are digested 
is irrelevant, but in the floating-point implementation of Gaussian elimination, 
rearranging the equations can make a huge difference. Below is a simple example 
that illustrates this point. 
It is easy to verify that the exact solution to the system 
—10-*x+y=1, 
2.8.2) 
gr+y=2, 
_ 
is given by 
; 
aged 
q 
wagll 
Pie EISOOO 
Tuan 
BP HR PLI000TF 
Consider Gaussian elimination with floating-point arithmetic. The ultimate goal 
when using t-digit arithmetic is not to produce the exact solution but rather to 
end up with solution components that agree with the exact ones to ¢ significant 
places. When Gaussian elimination with 3-digit arithmetic is used to process the 
equations (2.8.2) in their original order, the result is 
-10~* 
1 
1 
a 
es 
| 1 
1 
1 
2} 
Ro+10*R, 
0 
10 
104 

238 
Chapter 2 
Systems, Elimination, and Echelon Forms 
because in 3-digit arithmetic 
fl(1 + 104) = fl(.10001 x 10°) = .100 x 10° = 10%, 
(2.8.3) 
and 
fl(2 + 10*) = fl(.10002 x 10°) = .100 x 10° = 10%. 
(2.8.4) 
Back substitution produces 
2s0"ands 
'y=. 
Although the computed solution for y is close to the exact solution for y, the 
computed solution for x is not at all close to the exact solution for «—the 
computed solution for x is certainly not accurate to three significant figures as 
might have been hoped for. 
Now interchange the equations in (2.8.2) before applying 3-digit Gaussian 
elimination. The result is 
<= {(\7* 
J} 
1 
1 
1 
7 
Mehta 
Ott \ ies, 
1 
tee Pile 
* 
Rh, 
because 
fl(1 +10~*) = fl(.10001 x 101) = .100 x 10! = 1, 
(2.8.5) 
and 
fl(1+2 x 10~*) = fl(.10002 x 10!) = .100 x 10! = 1. 
(2.8.6) 
This time, back substitution produces the 3-digit computed solution 
pal 
and. 
y= i, 
which is as close to the exact solution as can reasonably be expected because the 
computed components agree with the exact ones to three significant digits. 
Why Did Reordering Make a Difference? 
The answer lies in comparing (2.8.3) and (2.8.4) with (2.8.5) and (2.8.6). When 
the equations are digested in original order, the multiplier is 104, which is so 
large that it completely swamps the arithmetic involving the relatively smaller 
numbers 1 and 2 and prevents them from being taken into account. In other 
words, the smaller numbers 
1 and 2 are "blown away" as though they were 
never present so that our 3-digit computer produces the exact solution to another 
—19-4 
system, namely, ( 4 
ah which is quite different from the original 
system. Answering the question, "What system have I really solved (i.e., obtained 

2.8 Making Gaussian Elimination Work 
239 
the exact solution of), and how close is this system to the original system?" is 
called backward error analysis, as opposed to forward error analysis in which 
one tries to answer the question, "How close will a computed solution be to the 
exact solution?" Backward error analysis has proven to be an effective way to 
analyze the numerical stability of algorithms. 
When the equations are interchanged before Gaussian elimination is applied, 
the multiplier is 1074, which is small enough so that the arithmetic does not 
swamp out the significance of the numbers 1 and 2. In this case, the 3-digit 
computer produces the exact solution to the system 'ei ' | ay which is 
(as well as the "forward 
y) 
close to the original system—i.e., the "backward error' 
error") is relatively small. 
In summary, the villain is a large multiplier that prevents some smaller 
numbers from being fully accounted for, which may result in the exact solution of 
another system that is significantly different from the original system. The lesson 
to be learned is that by maximizing the magnitude of the pivot by reordering the 
equations at each step, the magnitude of the associated multiplier is minimized. 
This helps to control the growth of numbers that emerge during the elimination 
process, which in turn helps mitigate the effects of roundoff error. The problem 
of growth in the elimination procedure is illustrated by the Wilkinson matrix 
W,, in Exercise 2.8.13 on page 248. 
Partial Pivoting 
2.8.2. Definition. 
The process of reordering the rows at each step of 
Gaussian elimination to maximize the magnitude of the pivot is called 
partial pivoting. At each elimination step the positions on and below 
the pivot position are searched to find the coefficient of maximum mag- 
nitude, which then is brought to the pivot position by a row interchange. 
For example, the third step of Gaussian elimination with partial pivoting in 
a typical case looks like this. 
* 
x 
* 
* 
Ox 
* 
Os 
* 
* 
* 
* 
0 0G 
*« 
«| 
* 
Oo 
@ 
Gm 
* 
Oo? 
& 
8 
* 
The positions in the third column marked "S$" 
are searched to find the coefficient 
of maximal magnitude and, if necessary, a row interchange is performed to bring 
this coefficient into the circled pivotal position. 
Formally, an algorithm is said to be stable if the algorithm returns the exact solution to a nearby 
problem for all possible cases. Because Gaussian elimination following an interchange acted in 
a "stable way" for one particular example does not mean that it is a stable algorithm—it has 
to act "stably" for all linear systems, which it does not—see (2.8.7) on page 240. 

240 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Brief Analysis of Partial Pivoting 
It was observed in the analysis of (2.8.2) on page 237 that controlling the growth 
of the numbers in the coefficient array during elimination is important to keep 
roundoff error from having an undue effect. To understand how partial pivoting 
helps control growth, notice that no multiplier ever exceeds 1 in magnitude. 
This is seen by considering the following two typical steps in an elimination 
procedure: 
ere, wee 
ee 
* 
ee 
eee te 
el: 
* 
Ty Ses 
Serie 
* 
Ok: 
Foe 
ere 
* 
APE 
C9 ion 
alee: 
+ 
Be, 
Lo, 
ce tin iene 
* 
Gal) < Onbk 
ak 
*« | R4—(q/p)Rs 
Oe 
Lh 
= 
eta 
* 
0-0 
+ 
* 
« | */ 
Rg —(r/p)Rs 
05 
ine Oh 
aeicth 
na 
* 
The pivot is p, and the two multipliers are q/p and r/p. If partial pivoting 
has been employed, then |p| > |gq| and |p| > |r| so that for each multiplier, 
if <1 
and 
=| 
Silt. 
Pp 
Pp 
By guaranteeing that no multiplier exceeds 1 in magnitude, the possibility of 
encountering relatively large numbers that can swamp the significance of smaller 
numbers during an elimination step is reduced—but not completely eliminated. 
To see that there is still more to be done, consider the following example. 
Partial Pivoting Can Fail 
The exact solution to the system 
—10x + 10°y 
Hn 
Se 
ea 
107 
2, 
(eer 
is given by 
oe 
a 
_ 1.0002 
1.0001 
2 ab COOL 
Suppose that 3-digit Gaussian elimination with partial pivoting is used. Since 
| —10| > 1, partial pivoting does not call for an interchange, so 3-digit elimina- 
tion yields 
beets 10° | 
40° 
—10 
10° 
10° 
lugs 
Que] 5 
Re lO he 
Neen 104 | io!) 
because 
fl(1 + 10*) = fl(.10001 x 10°) = .100 x 10° = 102, 
and 
fl(2 + 104) = fl(.10002 x 10°) = .100 x 10° = 104. 

2.8 Making Gaussian Elimination Work 
241 
Scaling 
t 
Back substitution yields 
c=0 
and 
y=1. The computed 3-digit solution 
for y is okay, but the computed 3-digit solution for « is terrible, so, when 
considered in its entirety, the computed solution must be considered to be a bad 
approximation of the exact solution. 
What is the source of difficulty here? This time, the multiplier cannot be 
blamed. The trouble stems from the fact that the first equation in (2.8.7) contains 
coefficients that are much larger than the coefficients in the second equation. 
That is, there is a problem of scale due to the fact that the coefficients are of 
different orders of magnitude. This suggests that the system should be somehow 
rescaled before attempting to solve it. If the first equation in the above example 
is rescaled to insure that the coefficient of maximum magnitude is a 1, which 
is accomplished by multiplying the first equation by 10~°, then the system 
(2.8.2) on page 237 is obtained, and it was seen that partial pivoting produces 
an excellent approximation to the exact solution. 
The previous observations point to the fact that the success of partial pivoting 
can hinge on maintaining the proper scale among the coefficients. Therefore, the 
second refinement needed to make Gaussian elimination practical is a reasonable 
scaling strategy. Unfortunately, there is no known scaling procedure that will 
produce optimum results for every possible system, but experience has shown 
that the following strategy usually works acceptably well. 
e 
Column Scaling. Choose (or adjust) units that are natural to the 
problem and do not distort the relationships between the size of 
things. These natural units are usually self-evident, and further col- 
umn scaling past this point is not ordinarily attempted. 
e 
Row Scaling. For each 1 <k <n, divide equation k by the entry 
in Aj, that has maximum magnitude (bg is not considered)!. This 
ensures that the entry of maximal magnitude in each row of the 
resulting coefficient array (but not the right-hand side) is equal to 1. 
Row scaling doesn't alter the exact solution but column scaling does. For 
example, if the original units of the j*" unknown in Ax = b are millimeters 
but are changed to meters, then A is changed (or scaled) to become A in which 
Any = A,;/1000. Exercise 2.1.25(b) on page 135 addresses the change in x. 
The reason for not taking the right-hand side into account when row-scaling a system or when 
complete pivoting is employed is because applications frequently require the solution to many 
systems Ax = b; that have the same coefficient matrix A but different right-hand sides 
b;. Equilibrating A just once is an advantage in such situations, especially when the various 
systems are solved by using an LU factorization of A as described in §2.10. 

242 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Partial pivoting together with the scaling strategy described above makes 
Gaussian elimination with back substitution an effective and practical tool. Over 
the course of time, this technique has proven to be reliable for solving a pre- 
ponderance of the unstructured linear systems encountered in practical work. 
However, for those who want more of an iron-clad guarantee, there is a more 
conservative option. 
Complete Pivoting 
Although it is not extensively used, there is an extension of partial pivoting 
known as complete pivoting, which in some special cases can be more effective 
than partial pivoting in helping to control the effects of roundoff error. 
2.8.3. Definition. 
Complete pivoting at each step of Gaussian elimi- 
nation is the process of searching all positions in the coefficient matrix 
A that are below or to the right of the pivot position for the coefficient 
of maximum magnitude and then bringing it to the pivot position by 
appropriate row and column interchanges. Just as in the case of row 
scaling, the right-hand side b is not taken into account. 
For example, here is the third step in a typical situation. 
COCOX 
ooo 
x 
* tn 
(aye 
* 
DMWWNWN* 
* 
* 
&*& 
& 
& 
*¥ 
DWNN* 
* 
Search the positions marked "S$" for the coefficient of maximal magnitude. 
If necessary, interchange rows and columns to bring this maximal coefficient 
into the circled pivotal position. Recall from Exercise 2.1.25 that for a system 
Ax = b, the effect of a column interchange in A is equivalent to permuting (or 
renaming) the associated unknowns. 
It should be clear that complete pivoting is at least as effective as partial 
pivoting. Moreover, it is possible to construct specialized examples where com- 
plete pivoting is superior to partial pivoting—a famous example is presented in 
Exercise 2.8.13. However, one rarely encounters systems of this nature in prac- 
tice. A more insightful comparison between no pivoting, partial pivoting, and 
complete pivoting is given in Exercise 2.8.13 on page 248. 
Use 3-digit arithmetic together with complete pivoting to solve the following 
system. 
ier 
y = -2, 
=Oa dl Oat ae 1: 
ESS) 

2.8 Making Gaussian Elimination Work 
243 
Since 10 is the coefficient of maximal magnitude that lies in the search pattern, 
interchange the first and second rows and then interchange the first and second 
columns. 
1 
-1l 
—2 
ze —9 
10 
12 
ag awe k) 
12 
1 -1 
—2 
10 
—-—9 
12 
10 
—9 
12 
(4 
1 | (5 
ai | ay 
The effect of the column interchange is to rename the unknowns to @ and 9, 
where 
£=y and §j =z. Back substitution yields 7 = —8 and <= —6 so that 
x=y=-8 
and 
y=2Z=  -6. In this case, the 3-digit solution and the 
exact solution agree. If only partial pivoting is used, the 3-digit solution will not 
be as accurate. However, if scaled partial pivoting is used, the result is the same 
as when complete pivoting is used. 
Summary 
Pivoting and scaling schemes are used in Gaussian elimination to control the 
growth of the numbers so as to help prevent large quantities from swamping out 
the significance of smaller ones. Complete pivoting in conjunction with proper 
scaling works well—see Exercises 2.8.13 and 2.8.14. If the cost of using complete 
pivoting was nearly the same as the cost of using partial pivoting, then complete 
pivoting would always be used. However, searching is computationally expen- 
sive, and it can be argued that complete pivoting approximately doubles the 
computational cost over straight Gaussian elimination, whereas partial pivoting 
adds only a negligible amount. Couple this with the fact that it is extremely rare 
to encounter a practical system where scaled partial pivoting is not adequate 
while complete pivoting is, and it is easy to understand why complete pivoting 
is seldom used in practice. Gaussian elimination with scaled partial pivoting is 
the preferred method for dense systems (i.e., not a lot of zeros) of moderate size. 
Exercises for section 2.8 
2.8.1. Consider the following system: 
10-*2 
—y = 1, 
Ga 
y = 0. 
(a) Use 3-digit arithmetic with no pivoting to solve this system. 
(b) Find a system that is exactly satisfied by your solution from 
part (a), and note how close this system is to the original system. 
(c) Now use partial pivoting and 3-digit arithmetic to solve the 
original system. 

244 
Chapter 2 
Systems, Elimination, and Echelon Forms 
(d) Find a system that is exactly satisfied by your solution from 
part (c), and note how close this system is to the original system. 
(e) Use exact arithmetic to obtain the solution to the original sys- 
tem, and compare the exact solution with the results of parts (a) 
and (c). 
(f) Round the exact solution to three significant digits, and compare 
the result with those of parts (a) and (c). 
2.8.2. Consider the following system: 
z+ 
y=3, 
—10x + 10°y = 10°. 
(a) Use 4-digit arithmetic with partial pivoting and no scaling to 
compute a solution. 
(b) Use 4-digit arithmetic with complete pivoting and no scaling to 
compute a solution of the original system. 
(c) This time, row scale the original system first, and then apply 
partial pivoting with 4-digit arithmetic to compute a solution. 
(d) Now determine the exact solution, and compare it with the re- 
sults of parts (a), (b), and (c). 
2.8.3. With no scaling, compute the 3-digit solution of 
—32+ y= -—2, 
10z — 3y = 7, 
without partial pivoting and with partial pivoting. Compare your results 
with the exact solution. 
2.8.4. Consider using floating-point arithmetic (without scaling) to solve the 
following system: 
8352 + .667y = .168, 
3332 + .266y = .067. 
(a) Is the system consistent when 5-digit arithmetic is used? 
(b) What happens when 6-digit arithmetic is used? 

2.8 Making Gaussian Elimination Work 
245 
2.8.5. 
2.8.6. 
2.8.7. 
The standard quadratic formula says that one root of 2? + br +1=0 
is given by 
A-—b 
—2 
—<—_ = —— 
= 
Jee 
5 
irene 
where 
A= 
/b? 
—4. 
Use 3-digit arithmetic with b = 123 to first compute 2; = (A — b)/2 
and then compute x2 = —2/(A +b). Compare the results with the 
exact solution by computing the relative error in each case, and draw a 
conclusion as to why one expression provides a better computed solution 
than does the other. 
Consider using floating-point arithmetic (without partial pivoting or 
scaling) to solve Ax = b, where 
835 
667 
.5 
168 
A =| .333 
.266 
.1994 
and 
b= (087). 
iL@y 
Isso 
til 
.436 
(a) Determine the 4-digit general solution. 
(b) Determine the 5-digit general solution. 
(c) Determine the 6-digit general solution. 
In theory, determining whether or not a given set is linearly independent 
or determining the rank of a matrix is a well-defined problem with a 
straightforward solution. In practice however these problems are often 
not as well defined because they become clouded by the use of floating- 
point arithmetic. Contradictory conclusions can be produced depending 
on the precision of the arithmetic and the choice of the algorithm. For 
example, consider the matrix 
1 
Jeo 
A= ( 
ae 
Saat 
6 ), 
of 
del 
AUTO! 
In theory (i.e., in exact arithmetic), the columns are linearly independent 
and rank (A) = 3, but using 3-digit arithmetic (without pivoting or 
scaling) the columns are a linearly dependent set, and rank (A) = 2. 
(a) Verify this by first using exact arithmetic to determine Ea. 
(b) Now use 3-digit floating-point arithmetic (without partial piv- 
oting or scaling) to compute Ea, and deduce the "3-digit nu- 
merical rank." 
(c) What happens to the "3-digit numerical rank" if partial pivoting 
is incorporated? 

246 
Chapter 2 
Systems, Elimination, and Echelon Forms 
2.8.8. Sensitivity of the Normal Equations. Perform the indicated calculations 
using the following matrices. 
da 
4 
1 
A=(:2 4 ) and 
b= (2 " 
cli bia sO 
1.01 
(a) Find rank(A), and solve Ax = b using exact arithmetic. 
(b) Find rank (A7A), and solve ATAx=A7b exactly. 
(c) Use 3-digit arithmetic (without partial pivoting or scaling) to 
find rank(A) and to solve Ax = b. 
(d) Use 3-digit arithmetic (without partial pivoting or scaling) to 
compute A? 
A, A'b, and the solution of the system of normal 
equations 
A? Ax = A'b (see page 481 along with Exercise 
2.4.34 on page 199 and Exercise 2.5.28 on page 218). 
(e) What conclusion do you draw from these results? 
2.8.9. Consider the following system in which the coefficient matrix is the 
Hilbert matric. 
Hilts 
L+a 
-z=- 
pUaeae 
tong 
Le 
Loe ial 
ghee 
Ray 
svg eral a 
pel 
=£+-y+sz=-= 
gene 
Bw a 
ale 
(a) First convert the coefficients to 3-digit floating-point numbers, 
and then use 3-digit arithmetic with partial pivoting but with 
no scaling to compute the solution. 
(b) Again use 3-digit arithmetic, but row scale the coefficients (after 
converting them to floating-point numbers), and then use partial 
pivoting to compute the solution. 
(c) Proceed as in part (b), but this time row scale the coefficients 
before each elimination step. 
(d) Now use exact arithmetic on the original system to determine 
the exact solution, and compare the result with those of parts 
(a), (b), and (c). 
2.8.10. Recall from page 161 that a matrix An.n = [aij] is said to be diagonally 
dominant whenever |a;;| > dj 
4: \@ij| for each i = 1,2,...,n. Explain 
why partial pivoting is not needed to solve Ax = b by Gaussian elimi- 
nation when A? is diagonally dominant. Hint. Recall Exercise 2oeee 
on page 178. 

2.8 Making Gaussian Elimination Work 
247 
2.8.11. To see that changing units can affect a floating-point solution, consider 
a mining operation that extracts silica, iron, and gold from the earth. 
Capital (measured in dollars), operating time (in hours), and labor (in 
man-hours) are needed to operate the mine. To extract a pound of silica 
requires $.0055, .0011 hours of operating time, and .0093 man-hours of 
labor. For each pound of iron extracted, $.095, .01 operating hours, and 
.025 man-hours are required. For each pound of gold extracted, $960, 
112 operating hours, and 560 man-hours are required. 
(a) Suppose that during 600 hours of operation, exactly $5000 and 
3000 man-hours are used. Let xz, y, and z denote the number 
of pounds of silica, iron, and gold, respectively, that are recov- 
ered during this period. Set up the linear system whose solution 
will yield the values for z, y, and z. 
With no scaling, use 3-digit arithmetic and partial pivoting to 
compute a solution 
(Z,y,2Z) of the system of part (a). Then 
approximate the exact solution (x,y,z) by using your machine's 
(or calculator's) full precision with partial pivoting to solve the 
system in part (a), and compare this with your 3-digit solution 
by computing the relative error defined by 
@= i" +5 +E 
/p?2 + y?2 + 22 
er = 
Using 3-digit arithmetic, column scale the coefficients by chang- 
ing units: convert pounds of silica to tons of silica, pounds of 
iron to half-tons of iron, and pounds of gold to troy ounces of 
gold (1 lb. = 12 troy oz.). 
Use 3-digit arithmetic with partial pivoting to solve the column 
scaled system of part (c). Then approximate the exact solution 
by using your machine's (or calculator's) full precision with par- 
tial pivoting to solve the system in part (c), and compare this 
with your 3-digit solution by computing the relative error e, as 
defined in part (b). 
2.8.12. Consider the system given in (2.8.8) on page 242. 
(a) 
(b) 
Use 3-digit arithmetic with partial pivoting but with no scaling 
to solve the system. 
Now use partial pivoting with scaling. Does complete pivoting 
provide an advantage over scaled partial pivoting in this case? 

248 
Chapter 2 
Systems, Elimination, and Echelon Forms 
2.8.13. Consider the following well-scaled matrix: 
yee 
gun 
") 
C4 
Spivsqenr'piatiqge 
oo 
9° 
i 
ae 
ee 
te 
Wi 
e 
. 
° 
Ss 
os 
os 
Ie mae ak 
wig Phe 
Sree 
ae 
res 
(a) Reduce W,, to an upper-triangular form using Gaussian elimi- 
nation with partial pivoting, and determine the element of max- 
imal magnitude that emerges during the elimination procedure. 
(b) Now use complete pivoting and repeat part (a). 
(c) Formulate a statement comparing the results of partial pivoting 
with those of complete pivoting for W,,, and describe the effect 
this would have in determining the t-digit solution for a system 
Wik = by 
2.8.14. Suppose that A isan n x n matrix of real numbers that has been scaled 
so that each entry satisfies 
|a;;| <1, and consider reducing A to tri- 
angular form using Gaussian elimination with partial pivoting. Demon- 
strate that after k steps of the process, no entry can have a magnitude 
that exceeds 2*. 
Note: Exercise 2.8.13 shows that there are cases where it is possible for 
some elements to actually attain the maximum magnitude of 2* after 
k steps. 

2.9 Ill-Conditioned Systems 
249 
2.9 ILL-CONDITIONED SYSTEMS 
Gaussian elimination with partial pivoting on a properly scaled system is a fun- 
damental algorithm in the practical use of linear algebra. However, it is not a 
universal algorithm (there is no such thing) nor can it be used blindly. The pur- 
pose of this section is to make the point that when solving a linear system some 
discretion must always be exercised because there are some systems that are so 
inordinately sensitive to small perturbations that no numerical technique can be 
used with confidence. Such systems are said to be ill conditioned. The notion 
of "conditioning" for matrix inversion was briefly introduced in Theorem 2.3.13 
(page 172) and in Exercise 2.3.21 (page 176) but it was not fully developed, so 
the purpose here is to reflect on this issue a bit more to appreciate just how much 
trouble a badly conditioned system can cause. Much is revealed in the following 
small but illustrative examples. 
Bob's Dance with the Devil 
Bob was a fellow whose job was to design an expensive and critical device, code 
named X-42, on which people's safety will depend, and when the dust settled, 
the design hinged on the solution of a linear system. The systems encountered 
in practice are generally too large to be solved by hand, and finding the exact 
solution is not possible, so Bob had to rely on a computed solution. For the sake 
of this example let's use the small system 
8352 + .667y = .168 
3332 + .266y = .067 
(2.9.1) 
for the X-42 but pretend that it is like a real-world problem in the sense that 
the exact solution is unattainable. In order for the X-42 to operate safely, the 
design required that all components of a computed solution must be accurate 
within a tolerance of +.001. Bob proceeded, as every good engineer or scientist 
does, to feed the system into his best available software package called Six-Pack. 
Like most good software, Six-Pack was developed over a period of many years 
by teams of experts, but it was a black box to Bob. He simply input his system, 
and Six-Pack regurgitated an answer. Six-Pack returned a computed solution of 
(2.9.1) to be 
&j = —-666 
and 
£5 = 834. 
Before spending mega-millions to build a prototype of X-42 for testing, Bob 
wanted to be confident about the computed solution, so he naturally checked it 
to make sure that it was accurate to within the prescribed tolerance. Substituting 
the computed solution (£1,€2) into (2.9.1) reveals that the reszduals are 
ry = .835£; + .667£ — .168 = 0, 
333€, + .266£5 — .067 = —.001. 
I 
2 
In other words, Siz-Pack's computed solution (—666, 834) exactly satisfies the 
first equation and comes very close to satisfying the second. Being confident 

250 
Chapter 2 
Systems, Elimination, and Echelon Forms 
that the computed solution met the required tolerance, Bob gave the go-ahead 
to invest the company's resources to build a prototype. After a couple of years 
of fabrication Bob was given the honor to push X-42's "start button" for the 
first test. It exploded! Of course, Bob was fired; no other company would have 
him; his spouse took the children and left; and he ended up in dead-end job 
teaching remedial math to people who could not spell "mathematics." For the 
rest of his life Bob kept thinking "I checked everything—what could have gone 
so wrong?" Well, at the end of it all, when he was finally greeted by Satan, the 
Devil informed Bob that the exact solution to his system was 
y=1° 
and 
y=: 
Not only did Siaz-Pack fail to give Bob a solution with the necessary accuracy, 
it didn't even get the signs correct! "Let me at him" Bob cried. "Who?" asked 
Satan. "You know, Mr. M, the guy responsible for Siz-Pack." "Oh, he's not 
down here—he's up there in Margaritaville" replied Satan. And he went on to 
say "There was nothing wrong with Mr. M's software. His algorithm did just 
what any good (i.e., stable) algorithm is required to do—it returned the exact 
solution to a system that is very close (certainly within your tolerance) to the 
one you gave it." The Devil was correct because Sixz-Pack's solution (—666, 834) 
is the exact solution to 
8352 + .667y = .168, 
3332 + .266y = .066. 
The system in (2.9.1) is ill conditioned because its solution is extremely sensitive 
to small changes. For example, when .067 in (2.9.1) changes to .066, the exact 
solution changes from (1,—1) to (—666, 834). As he was laughing and leading 
Bob to a pit of fiery brimstone, Satan informs Bob "it was your mistake thinking 
that you could compute a reliable solution for an ill-conditioned system, and you 
compounded your ignorance by thinking that you could check the accuracy of it 
by evaluating the residuals." 
What the Prince of Darkness was trying to say is that problems associated 
with ill conditioning can go far beyond the inability to gauge the accuracy of a 
computed solution. He was saying that even if Bob had used God's computer— 
the one capable of doing exact arithmetic—Bob was still inviting disaster. 
In practical problems it is frequently the case that the right-hand side b in 
a linear system Ax = b is input data obtained from observations. For example 
suppose that the numbers b; and by in the design system 
8352 + .667y = b, 
33320 + .266y = be 
for the X-42 are measurements made by a test instrument that provides readouts 
accurate to +.001. Ifthe readout shows (b1, b2) = (.168, .067), as in (2.9.1), then 
in fact all solutions with 
167 <b; <.169 
and 
.066 < be < .068 
(2.9.2) 

2.9 Ill-Conditioned Systems 
251 
have to be considered equally valid. Shown below are the exact solutions for 
three of these possibilities. 
(b1, 
be) = (.169,.066) 
=> 
(x,y) = (—932, 1167) 
(2.9.3) 
(b1, be) = (.168,.067) 
=> 
(2,y) =(1,—1) 
(2.9.4) 
(bi, bo) = (.167,.068) 
=> 
(x,y) = (934, —1169) 
(2.9.5) 
In other words, even if God had let Bob use his software in place of Six-Pack, Bob 
still would have been in trouble. Since no one of the solutions (2.9.3), (2.9.4), or 
(2.9.5) can be preferred over any of the others, totally different designs would be 
implemented depending on how the last significant digit appears in the readout. 
The success of Bob's X-42 was reduced to relying on blind luck rather than on 
scientific principles. 
Moral: You are dancing with the Devil (even if allowed to use God's computer) 
when you try to squeeze information out of an ill-conditioned system. 
Path to Redemption 
Ill conditioning is intrinsic to the system itself and is not a result of any nu- 
merical procedure, so you cannot expect some numerical trick to remove it. If 
the exact solution is sensitive to small perturbations, then any computed solu- 
tion cannot be less so, regardless of the algorithm used. Rather than trying to 
extract accurate solutions from ill-conditioned systems, engineers and scientists 
are usually better off investing their time and resources in trying to redesign the 
associated experiments or their data collection methods so as to avoid producing 
ill-conditioned systems. 
Determination of Ill Conditioning 
It is easy to visualize what causes a 2 x 2 system to be ill-conditioned. Two 
equations in two unknowns represent two straight lines, and the point of inter- 
section is the solution for the system. The system is ill-conditioned when the 
lines are almost parallel because, as illustrated below, if line L is tilted only 
slightly to become L', then the point of intersection (the solution) is drastically 
altered. 
a 
L! 
Perturbed 
Solution 
SS 
Oi ginal 
Solution 
ILL-CONDITIONED 
2 X 2 SYSTEMS REPRESENT ALMOST PARALLEL LINES 

252 
Chapter 2 
Systems, Elimination, and Echelon Forms 
This was exactly the situation for X-42's system (2.9.1) on page 249. In general, 
ill-conditioned systems are those involving almost parallel lines, almost parallel 
planes, and generalizations of these notions. The problem of course is that these 
situations cannot be easily visualized beyond R?, so a different approach is 
needed. 
Another technique might be to experiment by slightly perturbing selected 
components and observing how the solution changes. A radical change in the 
solution caused by a small perturbation indicates ill conditioning, but nothing 
can be concluded when a small perturbation does not produce large change in 
the solution, so many experiments may be needed. Performing experiments can 
be computationally expensive for large systems, and they still may not provide 
the desired insight, so this is not the answer. A better way to gauge the condition 
of a linear system is by means of a measure described below. 
The Condition Number 
For a given norm, the associated condition number « = ||A\j || 
A~'|| was in- 
troduced in Theorem 2.3.13 (page 172) to provide an indication of possible sen- 
sitivity of A~! to perturbations in A. This number 
« also helps to gauge 
the sensitivity (or condition) of a nonsingular linear system Ax = b. To see 
why, use a bit of calculus to analyze the situation by considering the entries 
of A = A(t) and b = b(t) to be differentiable functions of a variable ¢ in 
an interval [a,b] on which A is nonsingular, and compute the relative size of 
the derivative of the solution x = x(t) by differentiating b = Ax to obtain 
b! = (Ax)' = A'x + Ax' or equivalently, x' 
= A7'b' — A~1A'x (' denotes 
differentiation with respect to t). Use any vector norm and a compatible matrix 
norm along with the triangle inequality to observe that 
Ix! = ||Atb! — A~*a'x|| < [API] + |]A"2a'| 
< ||A~*|| [[b'] + [A |] AI Mex 
This is the absolute magnitude of x'. The relative magnitude of x' is 
el ATT 
We = 
Ix] 
* ~ 
eal 
— 
A 
b'| 
1) [AT 
WAU A ll arg HAMA Ta 
Wb te tA , { 
[bill , WAM 
(pI a. [Al > 
(+ + aT) (because ||b]] < || Al] ||x/]). 
Thus the following theorem is established. 

2.9 Ill-Conditioned Systems 
eas 
" 
(253 
In other words, the relative change in the solution is bounded by the sum 
of the relative changes in A and b magnified by the condition number 
tk. A 
small « indicates that a small relative change in A or b cannot produce a 
large relative change in x, but for larger values of « small relative changes in 
A or b can possibly (but not necessarily) result in a large relative change in x. 
Since the credibility of the solution to Ax = b in the face of uncertainties is 
gauged in relation to the condition of A, just how large must « be to consider a 
system to be "ill conditioned?" Without context this is a somewhat meaningless 
question because some applications may be less tolerant while others might be 
more forgiving for the same value of «. Nevertheless, the following rule of thumb 
can help to put this issue into perspective with regard to applying Gaussian 
elimination. 
A Rule of Thumb 
If Gaussian elimination with partial pivoting is used to solve a well-scaled nonsin- 
gular system Ax = b using t-digit floating-point arithmetic, then, assuming no 
other source of error exists, it can be argued that when « is on the order of 10?, 
the computed solution is expected to be accurate to at least t—p significant 
digits, more or less. In other words, one expects to lose roughly p significant 
figures. This doesn't preclude the possibility of getting lucky and attaining a 
higher degree of accuracy—it just says that you shouldn't bet the farm on it. 
Consider again the 2 x 2 system Ax =b 
used in the X-/2 example (2.9.1) on 
page 249 in which 
835.667 
_ (168 
Sid 
RA 
Se gbe tale and x=(_). 
u Discrete analogs of this theorem along with more complete statements concerning conditioning 
and the effects of uncertainties in linear systems are developed on page 372. 

254 
Chapter 2 
Systems, Elimination, and Echelon Forms 
—266000 
667000 
Direct computation shows that At = ( 
333000 vse ' 
k= kp = |Allr [AT 
|p = 1, 323, 759 © 1.3 x 10°. 
The exact value of « is not as important as its order of magnitude. Because 
« #108, Theorem 2.9.1 holds the possibility that the relative change (or error) 
in the solution can be roughly up to a million times larger than the relative 
change (or error) in A or b. For example, if the right-hand side is perturbed 
ee 
-6 
3 
: 
to become 
b=b+f, where f = eos ji then the relative change in b is 
so (2.9.6) (and Exercise 2.9.5) suggests that the relative change in the solution 
can be as much as 
Ilfll 
|b], 
In other words, this roughly allows for as much as a 1000% change in the 
solution, which is huge. But remember, this is just an upper bound, so treat 
it as a worst-case scenario. Upper bounds like (2.9.6) often provide pessimistic 
estimates, but practical problems in real-world applications do not afford the 
luxury of knowing just how pessimistic they might be, and since the exact values 
of errors and solutions are rarely obtainable, the worst-case scenarios must always 
be taken seriously and guarded against. However, for the simple example at hand 
all values are known exactly. That is, 
j 
ainsllé 
(067 
nN 
ai 
hiainaet a lt 
eo 
Ki 
~ 10. 
(2.9.7) 
so the actual relative change in the solution (to 15 significant digits) is 
Kill 
i Seg a 1.05705085024326 = 1, 
which is about a 100% change. While this is only a tenth of the worst-case 
possibility in (2.9.7), it is still pretty bad, and system should be considered 
to be ill conditioned. In passing, observe that according to the rule of thumb 
given earlier, Gaussian elimination with 8-digit arithmetic on the X-42 system is 
expected to return only about t —p = 8—6=2 
correct significant digits—less 
than the X-42 specs called for. 
The complete story of conditioning has yet to be told. As pointed out on 
page 164, it is about three times more costly to compute A~! than to solve 
Ax =b, so it may not make sense to compute A~! to estimate the condition 

2.9 Ill-Conditioned Systems 
255 
of A. Moreover, if « is large, then accurate computation of A~! may not 
be possible. Fortunately, having an estimate on the order of magnitude of & is 
often sufficient, and there are a variety of techniques (beyond the current scope) 
for making such estimates without explicitly computing A7~!. More about 
appears in subsequent sections, and some additional insight by the way of discrete 
analogs to (2.9.6) is provided in Exercises 2.9.5 and 2.9.6. The full development 
of condition numbers is on page 372. 
Exercises for section 2.9 
2.9.1. Consider the following ill-conditioned system that appeared in (2.9.1). 
8352 + .667y = .168 
.333z + .266y = .067 
(a) Describe the outcome when you attempt to solve the system 
using 5-digit arithmetic with no scaling. 
(b) Again using 5-digit arithmetic, first row scale the system before 
attempting to solve it. Describe to what extent this helps. 
(c) Now use 6-digit arithmetic with no scaling. Compare the results 
with the exact solution. 
(d) Using 6-digit arithmetic, compute the residuals for your solution 
of part (c), and interpret the results. 
(e) For the same solution obtained in part (c), again compute the 
residuals, but use 7-digit arithmetic this time, and interpret the 
results. 
(f) Formulate a concluding statement that summarizes the points 
made in parts (a)-—(e). 
2.9.2. Perturb the ill-conditioned system given in Exercise 2.9.1 above so as to 
form the following system: 
8352 + .667y = .1669995, 
333z + .266y = .066601. 
(a) Determine the exact solution, and compare it with the exact 
solution of the system in Exercise 2.9.1. 
(b) On the basis of the results of part (a), formulate a statement 
concerning the necessity for the solution of an ill-conditioned 
system to undergo a radical change for every perturbation of 
the original system. 

256 
Chapter 2 
Systems, Elimination, and Echelon Forms 
2.9.3. 
2.9.4. 
2.9.5. 
2.9.6. 
Consider the two straight lines determined by the graphs of the following 
two equations: 
: 
8352 + .667y = .168, 
3332 + .266y = .067. 
(a) Use 5-digit arithmetic to compute the slopes of each of the lines, 
and then use 6-digit arithmetic to do the same. In each case, 
sketch the graphs on a coordinate system. 
(b) Show by diagram why a small perturbation in either of these 
lines can result in a large change in the solution. 
(c) Describe in geometrical terms the situation that must exist in 
order for a system to be optimally well conditioned. 
Using geometric considerations, rank the following three systems ac- 
cording to their condition, and then compare the results obtained by 
computing the condition number « for each coefficient matrix. 
L.O0la:—y-= 235; 
1.0012 — y = .235, 
(Le 
toe Doh 
oF Oe 
(b) 
+ .9999y = .765. 
(c) 
1.001z"+ y = .235, 
z+ .9999y = .765. 
Perturbing the Right-Hand Side. Let Ax = b be a nonsingular system in 
which perturbations (or errors) occur only in b so that the perturbed 
system is Ax = b+ ff. For any vector and compatible matrix norm, 
prove that the relative change in the solution is 
Ix— 3] lif 
Ie 
> bl 
where « = ||Alj ||A~?|| is the condition number for A. In other words, 
the relative change in x cannot be worse than the relative change in b 
magnified by «. This corroborates the result in Theorem 2.9.1. 
Perturbing the Coefficient Matrix. Let Ax = b be a nonsingular system 
in which perturbations (or errors) occur only in A so that the perturbed 
system is (A + E)x = b. Interpret a< 8 to mean that a is bounded 
above by something not far from 8. Prove that for any vector and 
compatible matrix norm the relative change in the solution is 
Ix— xl] 
|lEl| 
OLS aren 
ied| 
||| 
where «& = ||A|| ||A~?|| is the condition number for A. Thus the relative 
change in x is bounded above by something close to the relative change 
in A magnified by «. This adds support to the conclusion drawn from 
Theorem 2.9.1. Hint: Recall (2.3.17) on page 170. 

2.9 Ill-Conditioned Systems 
257 
2.9.7. Consider the following system. 
Sf 
OY-+ 22=15 
217 + 19y + 162 = 56 
39x + 48y + 53z = 140 
(a) Use exact arithmetic to find the solution, and then change 15 
to 14 to again solve the system with exact arithmetic. 
(b) Use the euclidean vector 2-norm and the Frobenius matrix norm 
in the following problems. Compute the relative change in the 
right-hand side, and compare this to the relative change in the 
solution. 
(c) Use exact arithmetic to compute A~!, and use it to compute 
the condition number «Ff for the coefficient matrix A. 
(d) Compute the upper bound given in Exercise 2.9.5 for the relative 
change in the solution, and compare the result with the actual 
relative change from part (b). Make some conclusion based on 
your observations. 
2.9.8. Show that the condition number « for the following upper triangular 
matrix T,., grows rapidly as n grows, and explain why lim #& = oo. 
m—- Co 
[ay 
eee ed 
Cee 
ge ele 
Le 
it 
nxn 
Note: Observe that det (T) = 1 for all n. In other words, the deter- 
minant of a matrix need not be an indication of its condition. 
2.9.9. Let f(x) =sin7a on [0,1]. The object of this problem is to determine 
the coefficients a; of the cubic polynomial 
that is as close to f(x) as possible in the sense that 
r= | [f(@)-ple)Pae 
0 
= [ucrér—2y>a fi 
stseyte+ f 
[Soou') a 

por a ae 
; 
e° 
Shiginioy of 
er! 
_— 
Sade 
sak 
| 
| 
it 
mi 
Giaciiar form. eet that the case ei neon =4 is 
typical, explain why a general system H,x = b will be ill 
conditioned. Notice that even complete pivoting is of no help. 

2.10 Triangular Factorizations 
259 
2.10 
TRIANGULAR FACTORIZATIONS 
Now that the fundamentals of solving a nonsingular system of linear equations 
using Gaussian elimination are well in place, the goal here is to describe and 
analyze the process in the context of matrix factorizations. Throughout this 
section all linear systems AnynXnx1 = bnx 1 are assumed to be square and 
nonsingular unless otherwise stated. When no zero pivots are encountered during 
the execution of "basic" Gaussian elimination (i.e., without pivoting or scaling 
for numerical stability), row interchanges are not necessary, so reduction of A 
to an upper triangular form U, 
x, can be accomplished by using only Type III 
Pe 
PR 
27 
row operations. For example, if A = € 4 "| 
, then applying basic Gaussian 
3) 
ite) 
AP 
elimination yields 
a 
et 
ame 4 
23 
PS 
Py 
€ 7 ') 
R2-2Ri > (0 3 ») 
+ (0 
3 a) 
=U 
(2.EQeAy 
6 
18 
22/ 
R3—-—3R, 
0 
12 
16/ 
R3 —4Re 
GQ 
@ 
4 
The numbers 2, 3, 4 in the operations Rg —2R,, R3—3R,, and R3 —4R2 
are examples of multipliers, the formal definition of which is as follows. 
2.10.1. Definition. The number m,; used to annihilate the (7, j)-position 
during Gaussian elimination by subtracting m,; times row R; from 
row R; (i.e., Rj —mi;R;) is called the (7, j)-multiplier. For example, 
Mig = 2, M13 = 3, and mo3 =4 in (2.10.1). 
Each of the Type III operations in (2.10.1) can be executed by means of a 
left-hand multiplication with the corresponding elementary matrix G;, and the 
product of all of these G;'s is 
Lye 
FO c0 
1 = Oem) 
he She 
be 
0.70 
GaGaGi = (1 1 0)( Omi 0) 
(-2 1 0) 
=(-2 
1 0). 
0 =27 17 N=30, 4 
OO 
1 
5 -4 1 
In other words, G3G2G,A = U, so that 
A = G;'G,'G,'U = LU, where 
L is the lower-triangular matrix 
ih 
oO" 
L=G,'G;'G;'= (: 1 0). 
oy 
a 
al 
Thus A = LU 
is a product of a lower-triangular matrix L and an upper- 
triangular matrix U. Naturally, this is called the LU factorization of A. 
Observe that U is the end product of Gaussian elimination using only 
Type III operations, and U has the pivots on its diagonal while L has 1's on 
its diagonal. Moreover, L_ has the remarkable property that each entry ¢;; below 
the diagonal in L is precisely the multiplier used to annihilate the (i, 7)-posttion. 
This is characteristic of what happens in general. However the simplicity of the 
example in (2.10.1) masks the details of the following rigorous proof. 

© 2 
Bile 
Se apisorta 
Oe 
eas 
7 
bs 
AA & gotewten o 
lifts? iio "ere 
La ee, 
Proof. 
Recall from (2.2.7) on page 141 that annihilating all entries below the 
kth 
result after k —1 elimination steps as shown below with 
* 
* 
tae 
ay 
* 
see 
* 
Oa 
es 
a2 
K 
ee 
Ok 
~A,-1 = 
Oy 
Wh 
ors 
"ay. 
See 
Fy 
and 
Ci 
(Qe 
0) 
Oke gt 
sis 
0 0 
alans lets 
then 
ae 
Ox 
, 
; 
TyAge1 = (L- cpe;,) Apt = Apei 
6, Ap_y = 
OF 
0 
0 
(Os 
Notice that T, does not alter the first k — 1 columns of Ax—1 because 
0 
Ak+41/ Ak 
An/ 
Ak 
0 
nee 
* 
pivot can be accomplished with one multiplication by an elementary lower- 
triangular matrix of the form T, =I—c,e7. In particular, if c, contains the 
multipliers used to annihilate all entries below a, in the partially triangularized 
* 
* 
+ *& 
* 
ep [Ax—1],; = 0 whenever j < k—1. Therefore, if no row interchanges are 

2.10 Triangular Factorizations 
261 
required, then reducing A to an upper-triangular matrix U by Gaussian elim- 
ination is equivalent to executing a sequence of n—1 
left-hand multiplications 
T,-1:::T2T,A =U with elementary lower-triangular matrices so that 
7 Nod 
ed 
Wohl Wala 
Oh 
(2.10.4) 
Making use of the fact that eF Cp = 0 whenever j < k and applying (2.2.6) 
from page 140 reveals that 
Ty Tz ---T,2) = (I+ cre?) (1+ cze2) --- (I+ en—1e7_,) 
2.10.5) 
m 
(2.10. 
=I+cyef +cge4 +---+e,_1e2_,. 
Om0m 
oe 
0 
Qo 
crete) 
ONG. 
08 
Oe 
0 
By observing that c.ep = | 0 Ole 
"0 
OS 
0" 
| wheresthe? ¢j,-"s are 
0 0 
fpetn 
© 
oo 
0 
Site 
Te. 
ae 
ye: 
the multipliers used at the k*" stage to annihilate the entries below the k*" 
pivot, it now follows from (2.10.4) and (2.10.5) that A = LU, where 
ie. 
"10 
Paid 
hese 
bate 
Lp 
pga 
0 
és, 
fs 
F 
+++ 
10 
L=14 ce! + coe} +--+ +en_1€)_, = 
ak ae 
iba 
42-1026) 
Ln 
laa ens eel 
Thus the existence of an LU factorization is established when no zero pivots 
emerge using Type III operations. 
Proof of Uniqueness. 
'The LU factors must be nonsingular because they have 
nonzero diagonals (see page 163), so if LyU; = A = L2U2 are two LU factor- 
izations for A, then 
Le Ly 
U5 Uy = 
(2.10.7) 
Notice that Ly 1, is lower triangular, while U2U;' is upper triangular be- 
cause the inverse of an upper (lower) triangular matrix is again upper (lower) 
triangular (page 163), and because the product of two upper (lower) triangu- 
lar matrices is also upper (lower) triangular (Theorem 1.7.6, page 62). Conse- 
quently, (2.10.7) implies that L>'L; =D = U2U;' must be a diagonal ma- 
trix. However, 
[Le];, = 1 = Leal (page 163), so it must be the case that 
L5'L; =1= U2U;", and thus L; =L2 and U1=U>. 
& 
LU as a Road Map 
The factorization A = LU is the matrix formulation of the entire Gaussian 
elimination process from beginning to end. In loose terms, the LU factorization 
is a road map in the sense that it provides all of the relevant information about 
how to get from A to U by Gaussian elimination—it tells you exactly what 
the reduction sequence looks like along the way by giving you the multipliers 
used at each step, and it tells you the end result, U. 

262 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Efficient Memory Management 
: 
Once L and U are known, there is usually no need to manipulate with A. This 
together with the fact that the multipliers used in Gaussian elimination occur in 
just the right places in L means that A can be successively overwritten with 
the information in L and U as Gaussian elimination evolves. The rule is to 
store the multiplier /;; in the position that it annihilates—namely, the (i, 7)- 
position of the array. For a 3 x 3 matrix, the beginning and end of the process 
looks like this. 
Q11 
G12 
413 \ Type III operations 
(/U11l 
U12 
13 
@21 
@22 
423 
_—_————> 
£91 
U22 
23 
@31 
432 
433 
£31 
€32 
33 
Example 
2 
ae 
ee 
Generating the LU factorization of the matrix A = (3 Toms ) 
in (2.10.1) by 
6 1822 
successively overwriting a single 3 x 3 array would evolve as shown below. 
Le 
ae 
7 hn 
ee 
2 
mee 
€ 7 ") 
Ro—2R, + { @) 
3 
3 
—=|@ 
3 
3 
6 
18 
22/ 
R3—3Ri 
(3) 12 
16/ 
Rs—4Re 
B) @ 4 
ie Cpa 
2, 
2 
2 
Thus l= (2 1 0) 
and' U's (0 
3 2). 
This is an important feature in 
3 
4 e 1 
0 
0 
4 
practical computation because it guarantees that an LU factorization requires 
no more computer storage than that required to store the original matrix A. 
Solving Ax = b with LU 
To solve a nonsingular system Ax = b by using the LU factors of Ayyn, 
rewrite Ax = b as L(Ux) =b, and set y = Ux. This shows that Ax = b is 
equivalent to the two triangular systems 
Livia De 
andes 
Usice 5. 
(2.10.8) 
To extract the solution for x, first solve the lower-triangular system Ly = b 
for y by forward substitution. That is, if Ly = b is 
1 
0 
0 
0 
Y1 
bi 
bo1 
1 
0 
0 
y2 
bo 
(Nie 
eo 
0 
y3 
¥5 
b3 
. 
. 
. 
a 
' 
en 1 
Ln2 
Ln3 
mvs 
ah 
ie 
bn 
then set 
Yi=b1, 
yo=bo—Lloiyi, 
ys =b3—L31y1 — Layo, 
ete. 

2.10 Triangular Factorizations 
263 
Example 
The forward substitution algorithm can be written concisely as 
i-1 
Yi=6b; 
and 
y=); —- So bikya 
fos 
hae 203, 
2, it 
(2.10.9) 
k=1 
After y is known, the upper-triangular system Ux = y is solved using the 
standard back substitution procedure by starting with %, = Yn/Unn, and setting 
1 
n 
t= 
(1 
= Sy vt) for 
44-7. = 146 m—2, ...41: 
(2.10.10) 
Use 
k=i+1 
It can be argued that only O(n") flops are required when (2.10.9) and (2.10.10) 
are used to solve both triangular systems Ly = b and Ux = y, so it is relatively 
cheap (from a flop point of view) to solve Ax = b once both L and U are 
known—recall from page 125 that solving Ax = b from scratch with Gaussian 
elimination requires O(n°/3) flops. 
If only one system Ax = b is to be solved, then there is no significant 
difference in computational cost between reducing the augmented matrix [A|b] 
to a row-echelon form and the LU_method. However, if it becomes necessary to 
later solve other systems Ax = b with the same coefficient matrix but with 
different right-hand sides, then the LU approach offers a big advantage because 
if the LU factors of A were saved when the original system was solved, then 
solutions to all subsequent systems Ax = b can be obtained by just a forward 
solve followed by a back solve. In other words, the computational cost for each 
subsequent system is O(n") flops whereas it would take O(n?/3) flops to start 
from scratch each time. The need to solve Ax = b for multiple b's often occurs 
in applications where the b's are time-dependent inputs to a physical system in 
which the coefficient matrix A is fixed by the physics of the problem. 
Use the LU factorization to solve Ax = b and Ax=b, where 
Fat 
he) 
12 
. 
6 
A=(4 if a) b= (24), and 
b= (2). 
Gn 1822 
12 
70 
The LU factors of A were determined on page 262 to be 
i, 
Or 
@ 
Be 
Pk 
ON 
L=(2 i} 0) and 
u=(0 3 3). 
Suey 
eel 
0 
0 
4 
Set Ux = y and write Ax = L(Ux)=b 
= > Ly = b. Solve the two 
triangular systems 
Ly=b 
and 
Ux=y 

264 
Chapter 2 
Systems, Elimination, and Echelon Forms 
by first solving Ly = b with forward substitution to produce 
oO 
op 
10 
0\ 
/y 
12 
Yi 
' 
(2 
1 0) 
(m) = (24) —— 
yo = 24—2y, 
= 0, 
3 4 
1/ 
\ys 
ta 
y3 = 12 — 3y, — 4yo = —24. 
Now apply back substitution to solve Ux = y to obtain 
a9) 
9 
£1 
12 
x, = (12 — 2x2 — 2x3) /2 = 6, 
(0 3 3) 
(22) =( 0) => 
zo = (0—323)/3 
=6, 
OD 
seks 
EA 
ae 
r3 = —24/4 
= -6. 
To solve Ax = b, simply repeat the forward and backward substitution steps 
with b replaced by b. Solving Ly = b with forward substitution yields 
10 0\/m 
6 
yi 
= 6, 
(2 
7 0) 
(in) = (2) => 
iy = 24 — 241 = 12, 
she 
BE 
+ 
ys = 70 — 3y1 — 4y2 = 4, 
and back substitution applied to Ux = y produces 
eee 
Ly 
6 
x1 = (6 — 2x2 — 2x3)/2 = —1, 
(0 
3 2) 
(# | 
= (12) => 
9 = (12.— 3ta) fo= 3, 
a 
ie 
VOSA 
. 
ta 4/4 td 
Computing A~! with LU 
Although matrix inversion is not used for solving Ax = b, there may be a rare 
application where explicit knowledge of A~! is desirable, in which case the LU 
factors of Anxn can be used to compute A~!. The strategy is to solve the 
matrix equation AX =I one column at a time. Since AX, j = Lj =e), set 
xj; = X,,;, and solve each of the n systems Ax; =e; by the standard two-step 
LU process. 
(1) 
Set y; = Ux;, and solve Ly; =e; for y; by forward substitution. 
(2) Solve Ux; = y; for x; = [A71],; by back substitution. 
This method has at least two advantages: (1) it's efficient, and (2) any LU pro- 
gram written to solve Ax = b can also be used to compute A7!. 
Caution! It may be tempting to use A~! = (LU)-! = U—!L7—!. However, 
computing U~! and L~! separately and multiplying the results is not as com- 
putationally efficient as the method just described. 

2.10 Triangular Factorizations 
265 
Existence of the LU Factorization 
Unfortunately, not all square matrices, and not even all nonsingular matrices, 
possess an LU factorization. For example, to see that 
USS 
ON ce / 
fk Es ty 
U11 
 U12 
Perley 
ia 
fo, 
1 
0 
U22 
is impossible, compare the (1,1) and (2,1) entries on each side. 
Comparing (1,1)-entries 
=> 
0= uy. 
Comparing (2,1)-entries 
=> 
1=0. 
Clearly, 
A = i A has no LU factorization. The problem here is the zero pivot 
in the (1,1)-position. Removing the zero pivot requires a row interchange, which 
is not a Type III operation, so Theorem 2.10.2 does not apply to guarantee the 
existence of the LU factorization. In fact, it turns out that the LU factorization 
will exist af and only if a zero pivot does not emerge during row reduction to 
upper-triangular form with Type III operations. This, along with an equivalent 
characterization of existence, is proven in Theorem 2.10.4 below, but first a 
definition is required. 
2.10.3. Definition. A principal submatriz in A, ., is any submatrix 
that lies on the intersection of the same set of rows and columns in A. 
Equivalently, a principal submatrix is one that is obtained by deleting 
exactly the same set of rows as columns from A. Every square matrix 
is allowed to be called a principal submatrix of itself. 
e 
The kxk leading principal submatrix A; in A isthe 
kxk 
principal submatrix that lies on the intersection of the first k rows 
and columns. Equivalently, A; is the principal submatrix obtained 
by deleting the last n —k rows and columns from A. 
For example, the leading principal submatrices in A = [a;;| are 
Oi 
Ga} 
OR 
aN 
P 
Q21 
@22 
°°: 
aa 
aii 
412 
Ai = (an) Aa=( 
) 
vs 
Ap = 
. 
: 
: 
: 
Qa21 
422 
: 
: 
Ds, 
: 
Qk1 
Qk2 
*"' 
Gkk 
until we get to A, = A. 

od 
a 
, aie on ft : pert: 
Proof of (2.10.11). 
Assume A has an LU factorization, and partition A as 
ae 
La./.0 
Un Ui 
LiUn 
* 
aatun 
(Ee 2) CG ge) = 
(2): 
where Lj; and Uy, are k x k. The leading principal submatrix A, = L,;U1, 
is nonsingular because Ly; and Uj; are each nonsingular (they are triangular 
with nonzero diagonal entries). Conversely, suppose that each leading principal 
submatrix A, in A is nonsingular. Use induction to prove that each A, pos- 
sesses an LU factorization. This is true for 
k = 1 because if A, = (aj) is 
nonsingular, then A; = (1)(a,1) is its LU factorization. Assume that A, has 
an LU factorization and show that this together with its nonsingularity implies 
that A,+1 must also possess — LU factorization. If A, = L,U, is the LU 
factorization for A,, then Aid EW 8 foe § Bes) 
, and 
x 
Apa 
Le? 
0 
LU, 
Leb 
©1018) 
MEDS 
C7 
Ak+1 
- 
cou. 
1 
0 
On+1 —c 
A;z'b 
— 
This is the LU factorization for A;,41 because 
L 
pene 
ae 
aU 
Uk 
lap 
2.10.14 
= 
an 
=> 
. 
. 
k+1 
PUL 
4 
k+1 
ec 
oped 
( 
) 
are lower and upper-triangular matrices, respectively, and L has 1's on its diag- 
onal while the diagonal entries of U are nonzero. The fact that Ak+1— clAy Ib 
is nonzero follows from the fact that Ax, and L,41 being nonsingular Sie: 
Unsi = Ly PARE nonsingular. Thus it has been proven that each lending 
principal submatrix has an LU factorization, and hence A, = A has an LU 
factorization. 
Proof of (2.10.12). 
It is already known from Theorem 2.10.2 that if a zero pivot 
does not emerge during row reduction to upper-triangular form with Type III 

2.10 Triangular Factorizations 
267 
operations, then the LU factorization exists, so only the converse requires proof. 
If the LU factorization of each leading principal submatrix exists, then (2.10.13) 
exhibits the LU factorization for A,4,. By nature of the LU factorization, pivot 
k+1 is the (k+1,k+1)-entry in U,41, and this pivot is nonzero because 
Ux+1 is nonsingular. 
& 
Incorporating Row Interchanges 
Up to this point row interchanges in LU factorizations have not been considered 
because if a row interchange is required to remove a zero pivot, then the LU fac- 
torization does not exist. However, the discussion in §2.8 (page 233) concerning 
floating-point computation emphasizes that practical computation necessitates 
row interchanges in the form of partial pivoting (page 239). So even if no zero 
pivots emerge, it is still the case that row interchanges must generally be used. 
If A is nonsingular, then all zeros in pivot positions can be removed by 
means of row interchanges. This makes it intuitively clear that the rows of every 
nonsingular matrix can be permuted so that zero pivots are never encountered. In 
other words, for each nonsingular matrix A, there exists a permutation matrix 
P (a product of elementary interchange matrices—see Definition 2.2.3, page 142) 
such that PA has an LU factorization. The following simple example illustrates 
how P is determined, and then Theorem 2.10.5 formalizes these observations 
along with additional information concerning the effect of row interchanges. 
Example 
Pe) 
Notice that A = (: 
4 "| 
does not have an LU factorization because a zero 
2-6 
8 
eae 
a 
1 23 
pivot emerges after the first Type III reduction step (: : ") 
—> (0 : i ), 
or equivalently, the leading 2 x 2 principal submatrix in A is singular. Since 
row interchanges are necessary, let's go all the way and use partial pivoting as 
discussed on page 239 to reduce A to upper-triangular form. 
iL 
2a Saye 
Dh 
ti 
Te 
A "(2 4 | 
__interchange Ry and R2 = (: 
9 2) 
orearry 
2 Ons 
2 
6 
8/ 
Rg—-—(1)Ri 
2 
4 
a 
€ 
4 
tf 
— 
00-12) « 
—> {0 2 
1) =U. 
(0 
9 
1 
__interchange Rz and R3 
bh 
Our S2 
The permutation matrix corresponding to the two elementary interchanges is 
Om 
Od) 
Oma 
O 
P=E:E: = (0 
0 1) 
(3 0 o)=(0 0 1). 
Qi 
Oe (a 
i 
@ 
@ 
The matrix obtained by permuting the rows of A in the order (2,3,1) now has 
an LU factorization. That is, 
Oy ly MU 
1 
@) @ 
De ah 
pa=(268)=(1 
1 0)(0 2 
iL D8} 
45 
0) al 
OO 
jl De 
AN i Sa 
I| IS Ge 

Proof. 
Let Ty, =I-—c,e} be an elementary lower-triangular matrix as defined 
on page 140, and let 
E=I-— uu? with u = e,4; — ex4; be the interchange 
matrix associated with an interchange of rows k+i and k+j (see Exercise 
2.2.7, page 155). Notice that ef E =e} because e? has 0's in positions k +7 
and k+j. This together with the fact that E? =I guarantees that 
ET,E = E? —Ec,egE=I1-—G,ef, 
where 
¢, = Ecg. 
In other words, the matrix 
T; = ET,E =I1—-é, 
7 
(2.10.15) 
is also an elementary lower-triangular matrix, and T, agrees with Ty, in all 
positions except that the multipliers y,,; and z+; have traded places. Suppose 
that an interchange of rows k+i and k+ 
is necessary immediately after the 
kt stage so that the sequence of left-hand multiplications ET;,T;,_;---T, is 
applied to A. Since E? =I, we may insert E? to the right of each T to obtain 
ET, T;,-1---T, = ET, E'T,_iE* --- 
EE' TE" 
= (ET,E) (ET;_1E)--- (ET, E)E 
= er 
eae 
In such a manner the necessary interchange matrices E can be "factored" to 
the far-right-hand side, and the matrices T retain the desirable feature of be- 
ing elementary lower-triangular matrices. Furthermore, (2.10.15) implies that 

2.10 Triangular Factorizations 
269 
Te. sy -..T, differs from T,T,-1:::Ti only in the sense that the multipli- 
ers in rows k +i and k +) have traded places. Therefore, row interchanges in 
Gaussian elimination can be accounted for by writing 
Tyee 
ToT PA = U, 
where P is the product of all elementary interchange matrices used during the 
reduction and where the T;,'s are elementary lower-triangular matrices in which 
the multipliers have been permuted according to the row interchanges that were 
implemented. Since all of the T;, 's are elementary lower-triangular matrices, the 
same reasoning used in (2.10.4)—(2.10.6) yields 
PA 
LU, 
where 
(Lied. Leeu hn, 
(2.10.16) 
is lower triangular with 1's on its diagonal. Thus PA has an LU factorization 
in which the multipliers in L are permuted according to the row interchanges 
that define P. 
UB 
Theorem 2.10.5 means that reduction to triangular form with interchanges 
can proceed just as in the case when no interchanges are used by successively 
overwriting the array originally containing A with each multiplier replacing the 
position it annihilates. Whenever a row interchange occurs, the corresponding 
multipliers will be correctly interchanged as well. The permutation matrix P is 
simply the cumulative record of the various interchanges used. The information 
in P is easily accounted for by a simple technique that is illustrated in the 
following example. 
Example 
Use partial pivoting on the matrix 
il 
2 
—3 
4 
z 
AMET 
MAE 
8 
A 
2 
3 
2» 
1 
—3 
—1 
1 
-4 
to determine the LU decomposition PA = LU, where P is the associated per- 
mutation matrix. Proceed by successively overwriting the array A with compo- 
nents from L and U. Keep track of row interchanges with a permutation counter 
column p that is initially set to natural order—i.e., 1, 2, 3, 4. Permuting com- 
ponents of p as the various row interchanges are executed will accumulate the 
desired permutation. The matrix P is obtained by executing the final permu- 
tation residing in p to the rows of an appropriate size identity matrix. For the 
sake of clarity, the multipliers @;; in the following process are shown in boldface 
type. 
ee 
a 
tn 
Ge 
Oe 
|| 2) 
cs) 
2 
ee 
oes 
la 
[Alp] = 
7. 
a 
ets || -3 
? 
De 
89) 
Dales 
sks 
ee 1 
| 
ea 
Scam 
eet 
=4 | 4 

270 
Chapter 2 
Systems, Elimination, and Echelon Forms 
rig 
els 
ieee ee Ba 
4. 
Bath 
-~e'] 
2 
t/a) © yecel es & 
PY 
Saja 
540 
si0" 
4 
7 
yr Se 
abiad 
Wino 
hah 
eee 
asa 
hia 
81S 
ET 
ORT 
MG 
14. 
Sieh 
8 
tod 
4 
eyein en 
aes) 
2 
4 
4 
(gt 
6b 
Ps 
Bii4e 
Se 
210 S10 
}4 
a8)a 
Bb 
doa yr ya 
= ta 
RNISERE LIB 
Gels 
Sa 
bode] 
Soe 
a 
ee 
8 
ey 
ae 
eee 
Oe 
UT es Vi a 
aes 8 
4 
Pe 
Ol 
ee: 
egy 
Pye 
yg 
Es 
7) 
Pade 
peice 
tote 
pial 
(2.10.17) 
{fab /B TiS 
AT 
Therefore, 
1 
or 
0 
6 
a8 
12" 
"3 
0100 
cil 
res/a 
ie 
MOLE 
i sf 16! 10 
aap 
hear ee 
Ll 
eye 
ge 
eos 
0b ee 
es pe OD 
Lj) ee aes 
ot) 
Mor 
Via 
Ov0eT 0 
Formal LU Algorithm with Pivoting 
The process illustrated in the preceding example can be formalized by the fol- 
lowing algorithm in which the array containing A, x, 
is overwritten with the 
LU factors. 
for i= 1 (tom =A 
p = argmax;<p<n |@ip| (the value at which the max is attained) 
Af 
\au| < |p| 
interchange 
Aj, and A,, 
record the permutation---see the preceding example 
end if 
for j=i+1. ton 
A;5 = A;i/Qii 
end for 
for 7 =14 l° to n 
for k=i1+1 ton 
Ajk = Ajk — AziQik 
end for 
end for 
end for 
The computational cost of this algorithm is O(n3/3) flops. 

2.10 Triangular Factorizations 
271 
Solving Ax = b with LU and Partial Pivoting 
It is easy to combine the advantages of partial pivoting with the LU decompo- 
sition in order to solve a nonsingular system Ax = b. Because permutation 
matrices are nonsingular, the system Ax = b is equivalent to 
PAx = Pb, 
so the LU solution techniques in (2.10.8)—(2.10.10) can be applied to solve this 
permuted system. In other words, once the factorization PA = LU has been 
performed, then Ly = Pb is solved for y by forward substitution, and Ux = y 
is solved for x by back substitution. 
It should be evident that the permutation matrix P is not actually used. All 
that is necessary is knowledge of the LU factors along with the final permutation 
contained in the permutation counter column p. The column b = Pb is simply 
a rearrangement of the components of b according to the final permutation 
shown in p. In other words, the strategy is to first permute b into b according 
~ 
to the permutation p, and then solve Ly = b followed by Ux=y. 
Example 
Use the LU decomposition with partial pivoting that was obtained in (2.10.17) 
to solve the system Ax =b, where 
1 
2 
-3 
4 
3 
- 
e018. 
12 8 
med 
ies 
Ag 
sta 
ee ae 
Ewavel > ea) == 
1 
—3 
-l 
1 
-4 
5S 
Simply permute the entries in b by the permutation 
p=(2 
4 
1 
3), and 
call the result b. Solve Ly = b by applying forward substitution 
il 
0 
OPO 
Y1 
60 
Y1 
60 
SUE 
iG 
PaO 
4s 
oe 
eet 
as 
sod es 
enn 
iealh 
ale 
Sr 
aaa | St 
oe 
latory 
V2e=5 
13 
1 
Yd 
1 
YA 
—15 
and then solve Ux = y by applying back substitution 
ask 
8 
r1 
60 
12 
Cah 
ertoee10 
Goa) 
50 
6 
Ope 
s3 
6 
23 
=i) 
Soe 
13 
Ono 
0 
1 
A 
~15 
=15 
The LDU and LDL' Factorizations 
The LU factorization is asymmetric in the sense that the lower factor has 1's 
on its diagonal while the upper factor has a non-unit diagonal—it contains the 
pivots. This is easily remedied by factoring the diagonal entries out of the upper 
factor as shown below. 
Ui1 
UWi2 
°:: 
Win 
Tey 
(ono 
(e 
1 ui2/ui1 
-+: Uin/U11 
QO 
wu22 
+: 
Uan 
O 
wue22 
°::: 
0 
0 
1 
"++ Un /U22 
td 
ba 
3 
ll 
eens: 
(2.10.18) 

272 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Setting D = diag (u11, u22,---,Unn), the diagonal matrix of pivots, and redefin- 
ing U to be the rightmost upper-triangular matrix in (2.10.18) allows any LU 
factorization to be written as A = LDU, where L and U are respective lower 
and upper-triangular matrices with 1's on both of their diagonals. This is called 
the LDU factorization of A. Since it is just a reformulation of the LU factor- 
ization, the LDU factorization does not always exist, but when it does exist, it 
is unique. 
When A € R"*" is symmetric, the LDU factorization assumes the even 
more symmetric form A = LDL', where L is lower triangular with 1's on 
the diagonal. To see that this factorization is just the symmetric version of the 
LDU factorization, observe that 
A=j=A' 
== 
LDU=U DL] U DL. 
These are each LDU factorizations for A, so the uniqueness of the LDU fac- 
torization means that U = L?. Naturally, 
A = LDL' is called the LDL? 
factorization of A, and it has the same existence and uniqueness properties of 
an LU factorization. The analog for complex and hermitian matrices similarly 
holds—i.e., if A = A*, then A = LDL". 
22 Sie 2 
The LU factors for A = (3 7 
"| 
were determined on page 262 to be 
CLS 
22 
Oi. 
O 
ie 
We 
L=(2 1 0) and 
u=(0 3 2), 
3.4 
1 
0-0 
4 
. 
. 
. 
. 
2 
0 
0 
so factoring the pivots out of U into a diagonal matrix D = (: 
3 | and 
redefining U to be what remains yields the LDU factorization 
ik OG 
D (ye 
it 
A=LDU = (: 1 a) 
(0 
3 a) 
(0 
1 1). 
By le ah 
OF (O44 
®) ole a 
va 
2 
4 
6 
Similarly, the LU factors for the symmetric matrix A = (: 11 2) are 
6 
24 
70 
Le 
Zee 
6 
Lb (2 1 0) anid' 
ul pexj 
te 
E37 
a2" 
F- 
ole 
Ziyi 
O04 
so factoring the pivots out of U into a diagonal matrix produces the symmetric 
LDL? factorization 
r 
Tom 
ZF 
OF" 
0 
ee 
ees 
A=LDL =(2 1 0)(i 3 0)( 1 '). 
(2.10.19) 
ae 
al 
0 
0 
4 
De {Or 
al 

2.10 Triangular Factorizations 
273 
Positive Definite Matrices 
When modeled with mathematics, symmetries in natural phenomena generally 
manifest themselves as symmetric matrices (e.g., see the coupled springs example 
on page 10), so a great deal of attention is afforded to properties of symmetric 
matrices. Moreover, nature frequently endows the inherent symmetric matrices 
with the property of possessing an LDL? factorization in which all pivots p; are 
positive (sample applications are developed in §3.6 on page 382). Consequently, 
special characterizations and terminology are created for such matrices. 
2. 
10. « Detinition. A poste ee matric i 
isa real and symmetric 
= OF complex hermitiant matrix Ae ce such that AX = 0 for all 
"nonzero x € re. . 
ree 
The formal connection between triangular factorizations and positive defi- 
niteness is articulated in the following theorem. Additional information concern- 
ing positive definiteness is in Theorem 3.6.3 on page 383. 
2.10.7, Theorem. For a real and symmetric or complex hermitian matrix 
Ac F?*" whose leading principal submatrices are Ax, the following 
_ 
statements are equivalent. 
e A is positive definite. 
(2.10.20) 
e 
Each A, is positive definite. 
(2.10.21) 
e 
A hasan LDL? factorization LDL* with each d;; > 0. (2.10.22) 
e A can be uniquely factored as 
A = R*R, where R 
is upper triangular with positive diagonal entries. This 
is the Cholesky! factorization. If 
A = LDL* 
is 
(2.10.23) 
the LDL? factorization, then the Cholesky factor 
ip 
R=D 
VL 
Proof of (2.10.20) <=> (2.10.21). 
Suppose that A is positive definite and Ax is 
a leading principal submatrix so that A = €:: me For all nonzero y € pes 
Vi Any = cy" o)( x pols) Rt An > 0, where age 
: A complex hermitian matrix A € C"*" such that x*Ax > 0 for all nonzero 
x € Cet 
Gs 
formally called hermitian positive definite, but the word "hermitian" is often suppressed. 
; This is named in honor of the French military officer Major André-Louis Cholesky (1875-1918). 
A brief history of him and his factorization is given on page 284. 

274 
Chapter 2 
Systems, Elimination, and Echelon Forms 
and thus A, is positive definite. Conversely, if each A, is positive definite, 
then A,, = A is positive definite. 
Proof of (2.10.20) <> (2.10.22). 
Suppose that A is positive definite, and use 
induction to prove that each leading principal submatrix A, has an LDL? fac- 
torization with positive pivots in D. The result is trivial for k = 1 because if 
x*Ax > 0 forall x 4 0, then taking x = e; yields a,; >0, so Ay = [1] [@11}[1] 
is the desired factorization. Now assume that A, = L;,D;,L; is an LDL? fac- 
torization, where D; has positive diagonal entries, and establish the result for 
Ap 
c¢ 
kx1 
ArH = be y where 
c¢F™ 
and = 
& = 4(k+1),(k-+1)- 
The induction hypothesis ensures that A, is nonsingular because each factor in 
A; = L,D,L{ is nonsingular. Set 
L 
Ly 
0 
ae 
D, 
; 
k+1 = eyncine .) 
and 
Dy+1 = ( 
0 sere 
(2.10.24) 
and use (cs =(L,')" along with direct multiplication to verify that 
Lx4iDeyi bps = Agii- 
(2.10.25) 
Now show that 
a—c*A;'c>0. Let x= (Uk yo ene # 0, and use the fact 
from (2.10.21) that Aj,4+1 is positive definite to conclude that 
a— c*A,'c = e744 ;Dx41e%41 = x* Axx > 0. 
Thus (2.10.25) is the LDL? factorization of Ay41, and hence every Ax (includ- 
ing A, = A) hasan LDL? factorization with positive pivots in D. Conversely, 
if 
A =LDL* 
is an LDL' factorization for which each p; = dy > 0, define 
aa 
ee 
ta 
Dee 
5 ee u : 
and 
R=D!/2y* 
(2.10.26) 
ge 
so that 
A = LDL* = LD'/?D!/*L* = R*R. 
(2.10.27) 
Note that R (and A) are nonsingular, so N(R) = 0. Therefore, Rx 40 for 
all nonzero x € F"*', and hence x*Ax = x*R*Rx = ||Rx||? >0. Thus A is 
positive definite. 

2.10 Triangular Factorizations 
275 
Proof of (2.10.22) <=> (2.10.23). 
The fact that (2.10.22) => (2.10.23) is a con- 
sequence of (2.10.26) and (2.10.27) together with the uniqueness of the Cholesky 
factor (established in Exercise 2.10.19). Conversely, if 
A = R*R, where R is 
upper triangular with positive diagonal entries rj, let D= diag (rity. ean 
and factor D out of R to produce R = DU, where U is an upper- pee 
matrix with a unit diagonal so that 
A=R?R=U!DDU = U'D*U 
is an LDL? factorization of A in which the pivots are positive. 
There are some corollaries that emerge from the detail in the preceding 
proof. The first is the fact that all positive definite matrices are nonsingular be- 
cause (2.10.22) guarantees that there exists an LDL? factorization A = LDL* 
in which each d;; > 0, which means L and D, and hence A, are nonsingular. 
And since each leading principal submatrix A, of a positive definite matrix is 
also positive definite by (2.10.21), it follows that each A, is also nonsingular. 
These observations along with some additional consequences are formally stated 
in the following corollary. 
2.10.8. Corollary. If A is a positive definite matrix, and if A, is the 
k* leading principal submatrix, then each of the following is true. 
e 
Each A, (and A) is nonsingular. 
(2.10.28) 
e 
aj; >0 for each diagonal entry of A. 
(2.10.29) 
e 
Theentry of maximum magnitude is on the diagonal of A. (2.10.30) 
Caution! These do not ensure positive definiteness (Exercise 2.10.14), 
but having det (A;) >0 for each k does (Exercise 2.10.15). 
Proof. 
The proof (2.10.28) preceded the statement of the corollary. Statement 
(2.10.29) follows from the fact that if A is positive definite, then x*Ax > 0 for 
all nonzero x, so setting x =e; yields 0 < ef Ae; = a;;. To prove (2.10.30), 
suppose that the entry of largest magnitude is a;;, 
14 J, and let o = sign(a;;) 
so that ca;; = |a;;|. Use the fact that A is positive definite together with 
e; Ae; =e; Ae; (because A = A*) to write 
0 < (e; — ce;)*A(e; — ae;) = ej Ae; + eF Ae; — 2ce; Ae; = ay + a3 — 2\a;5| 
=> 
2lais| < aii + 455 < |aig| + laig] => 
aig| < laisl, 
which is impossible. Hence a;; with i # j cannot be the entry of maximum 
magnitude, and thus the largest entry in A must be a diagonal entry. 
Mf 

276 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Example 
24 
6 
; 
The matrix A = (: a 2) 
is positive definite because, as shown in (2.10.19), 
6 
24 
70 
its LDL' factorization 
YOON 
fs OUT 
ro 23 
A=upi?=(210)(0 2 o)(o 1 4) 
a: 
4 si 
KO Ree 
Awa Ss 
has positive pivots in D. The Cholesky factor R in the Cholesky factorization 
A = R'R is obtained by setting 
Formal LDL' Algorithm (for Real and Symmetric Matrices) 
A formal algorithm for computing the LDL' factorization is obtained by ob- 
serving that if Anxn = LDL? in which dj; 40, 1;; =1, and lj, =0 when 
k > j, then equating a;; and [LDL7];; for i> j yields 
aig So Lia! [DL'] )kg = yin 
]ik[D «(LJ = = likdgl 
jz 
k=1 
e 
k So 
lindalje = (x = 
+ bigdgglig = (s 
baat] + bijdy5. 
= 
k=1 
k=1 
This implies that 
I ae 
ima 
j-l 
i 
= 
2 
eee 
em 
& 
: 
24 
dj;=a;; — 2 
Tieden when i=, and at 
aa (0 
_ Yh when i>). 
Therefore, the LDL' algorithm can be formalized as follows. 
ft Olea leet 
O72 
dig = O53 — DI Pdix 
for t=j+1 
to-n 
(a:; Sees Linlsnd ) 
/dii 
(2.10.31) 
end for 
end for 
Because only the lower triangular parts of A and L are considered, this LDLT 
algorithm requires roughy half the computational effort as that required to com- 
pute the LU factors. In other words, the computational cost of this algorithm is 
O(n?/6) flops. 

2.10 Triangular Factorizations 
277 
Formal Cholesky Algorithm (for Real and Symmetric Matrices) 
If Anxn is positive definite, then the LDL™ algorithm (2.10.31) will also pro- 
duce the Cholesky factor R for which A = R'R by setting R = D1/?L7. 
However, the process leading to (2.10.31) can be somewhat streamlined for pos- 
itive definite matrices by defining 
L=LD/?=R7 
so that 
A= LL? 
and directly computing the entries Ix; = [L}.; in the lower-triangular factor. 
Since Ix; =0 when k > j, it follows that for i>, 
n 
j 
i=1 
kj = S- lindjk se do lindane a (s 
tli) + biglig, 
k=1 
k==1 
k=1 
ayy = [LL]; = 
n 
k 
and hence 
Gal 
55 = 43 — ) _ when i = j, 
(2.10.32) 
k=1 
and 
1 
JSS 
lig = 7, (o 
= > tals) when i > 7. 
These observations give rise to the following Cholesky algorithm. 
fore 7.— 1" toarn 
pia 
1/2 
lj = (aj - Dre ie 
DOr 
== 77 1 LON 
lig = (a; pai linljx ) 
/1i5 
a) 
end for 
end for 
Just as in the case of the LDL' 
algorithm, the computational cost of this 
Cholesky algorithm is O(n?/6) flops. When A is positive definite, the Cholesky 
algorithm cannot fail due to a division by zero or a negative number appearing 
under the radical because each 1;; > 0 (by Theorem 2.10.7), and (2.10.32) 
ensures that 
= 
a i; 
Finally, note that both of the eee (2.10.31) and (2.10.33) are efficient 
ways to numerically check whether or not a matrix A is rr definite because 
if either some d;; <0 in (2.10.31), or if some a;; — 
Be, <0 in (2.10.33), 
then A is not positive definite. 

278 
Chapter 2 
Systems, Elimination, and Echelon Forms 
S 
etric Pivotin 
a 
For plerrecdene Rey matrix A there is a permutation matrix P such that PA 
has an LU factorization, but when A is symmetric, the permutation destroys 
the symmetry—i.e., PA is no longer symmetric. Consequently, incorporating 
row interchanges to affect partial pivoting does not directly produce an LDL* 
factorization, so a modification is needed in which a symmetric permutation 
PAP? (page 143) is applied at each step. Such a permutation can move any 
diagonal entry in A to any other diagonal position (Exercise 2.2.15 page 156). 
For example, a symmetric permutation can bring the diagonal element p; 4 0 
of maximum magnitude into the (1,1)-position to produce 
19 
Bf 
aes 
Oh, 
c 
P| AP a(z ede 
(2.10.34) 
1 
0 
. 
—c1/p1 rt can be applied to each 
and then symmetric elimination with G; = ( 
side to yield 
TAT 
1 
O\(pi 
ct 
1 
-—cl/p,\ 
_ 
(pi 
0 
) 
CP ALIGT 
= atm) 
Cate) (0 Took 
| 0. Becta 
a a ne where 
A, = B,— cic} /pi, 
(2.10.35) 
or equivalently, PiAPT = Gi'(4 4, 
)GT"' =1i(G po, LP, im which 
L; = cae 
is lower triangular with 1's on its diagonal. Repeating the 
same process on A; produces P2A;PJ = L2(% A, )u, where Pp» is an- 
other permutation matrix and Ly is another lower triangular matrix with 17s 
on its diagonal, so after two steps 
Sia: 
oe 
Ds 
ue 
2 
= 
PAP? = i( po ie where 
P=(4 §,)Pi andL=(p,0 eae 
Provided that a nonzero pivot p; 40 
is available at each step, the process can 
be continued until eventually a factorization PAP? = LDL? emerges in which 
P is a permutation matrix, L is lower triangular with 1's on its diagonal, and 
D = diag (pi, p2,...,Pn) is a diagonal matrix of pivots. 
In contrast to using partial pivoting in the LU process, symmetric pivoting 
on a nonsingular symmetric matrix can fail because zero pivots cannot always be 
s 
: 
Om 
ee 
; 
ae 
resolved—e.g., consider 
A = 
oF, i However, symmetric pivoting on a positive 
definite matrix cannot break down because all pivots p; will be positive, and 
if they are maximized at each step, then pi > po >-:- 
> p, > 0 (Exercise 
2.10.13). 

2.10 Triangular Factorizations 
279 
Iterative Refinement 
An approximate (or computed) solution x; for a nonsingular system Ax = b 
can be refined to produce an updated solution x2 by computing the residual 
r; = b— Ax, and solving the system Ay; =r to create 
X2=X1+yY1. 
This is the exact solution because 
Ax y = Ax; + Ay; 
= 
Ax; +r; 
= Ax; + b— Ax; =b. 
But this is for exact arithmetic. When floating-point arithmetic is used in con- 
junction with an LU technique, the errors inherent in x; (see the rule of thumb 
on page 253) are of the same kind when computing yj, so it is not clear that x» 
will be more accurate than x,. To remedy the situation it can be argued that 
if t-digit arithmetic is used to compute x,, then employing double precision 
(i.e., 2t-digit arithmetic) to form r; and rounding to single precision can result 
in an improvement. Thus the refinement procedure can be iterated to produce 
increasingly accurate results for reasonably well-conditioned systems. 
Exercises for section 2.10 
ee 
2.10.1. 
(a) Determine the LU factors of A = € 18 
2 ). 
3 
16 
30 
(b) 
Use the LU factors to solve Ax; = bj; as well as Ax2 = bg, 
6 
6 
where by, = 0) 
and bz = ( 
°). 
—6 
12 
(c) Use the LU factors to determine A7!. 
2.10.2. By trying to construct one, show that it is impossible for A = G 4 
to have an LU factorization. Now reach the same conclusion by applying 
the results of Theorem 2.10.4 on page 266. 
2 
E 
0 
2.10.3. Determine all values of € for which A = ( 
1 € | 
fails to have an 
Gaei-e 
LU factorization. 
147 
2.10.4. 
(a) 
Determine the LDU factorization for A = (: 18 2). 
Sh 
ANG} 
G0) 
(b) 
Prove that if a matrix has an LDU factorization, then the LDU 
factors are uniquely determined. 

280 
Chapter 2 
Systems, Elimination, and Echelon Forms 
2.10.5. 
2.10.6. 
2.10.7. 
2.10.8. 
2.10.9. 
2.10.10. 
1 
2 
4 
17 
17 
oe 
oes 
ee 
Se 
hoa 
Let A=|[5 
3 
3 
9 
and b=| 
3 
02 
-2 
6 
4 
(a) Explain why A does not have an LU factorization. 
(b) Use partial pivoting and find the permutation matrix P as well 
as the LU factors such that PA = LU. 
(c) Use the information in P, L, and U to solve Ax =b. 
Random Integer Inverses. If A is a matrix that contains only integer 
entries and if it has an LU factorization in which all of its pivots are 
1, explain why A~! must also be an integer matrix. This fact is used 
to construct random integer matrices that possess integer inverses by 
randomly generating integer matrices L and U with unit diagonals 
and then constructing the product A = LU. 
Find the LDL" factorization for A = (= 3 2). Is A positive 
definite? 
Paseo 
1 
2S 
Show that A = (2 8 
12 | 
is positive definite, and find its Cholesky 
factorization. 
urs 
1 
-1l 
-l 
—l 
Let A=(-1 
10 -* and b= ( 
1). 
—l1 
-8 
26 
ivf 
(a) Find the LDL" factorization of A, and explain why A is 
positive definite. 
(b) Use the result of part (a) to determine the Cholesky factor R 
for A. 
(c) Corroborate the results in part (b) by executing the individual 
steps in the formal Cholesky algorithm (2.10.33) on page 277 to 
compute R?. 
(d) Use the Cholesky factorization to solve Ax = b. 
Formulas for Pivots. Assume that A has an LU factorization, and let 
ApS (es a) be its k*" leading principal submatrix for k > 1. 
(a) Explain why the k* pivot pe = ugp is given by 
Dk = Ak — e AG yb: 
(b) Prove that the k*" pivot is also given by 
re te 
(Ai) =a 
fork = 
det (Ax)/det(A,_1) 
for k = 2,3,...,n. 

2.10 Triangular Factorizations 
281 
2.10.11. 
2.10.12. 
2.10.13. 
2.10.14. 
2.10.15. 
2.10.16. 
2.10.17. 
2.10.18. 
2.10.19. 
Prove that A € F""*" is positive definite if and only if C* AC is positive 
definite for every nonsingular matrix C. 
Note: The term C*AC is called a congruence transformation of A. 
More about congruence can be found on page 395. 
Prove that A is positive definite if and only if every principal submatrix 
in A is positive definite. 
Note: Additional characterizations of positive definiteness are in Theo- 
rem 3.6.3 on page 383. 
Let A € R"*" be symmetric and positive definite. 
(a) Explain why symmetric pivoting on A cannot break down due 
to the emergence of a zero pivot that cannot be symmetrically 
permuted out of the way. 
(b) Show that if at each step of symmetric pivoting the size of the 
pivot is maximized, then the pivots are pj > pg >--- > pn > 0. 
Construct examples of symmetric matrices to show that none of the 
statements in (2.10.28)—(2.10.30) implies positive definiteness. 
Prove that a real and symmetric (or complex hermitian) matrix A is 
positive definite if and only if det(A;,) > 0 for each of its leading 
principal submatrices Ax. 
Prove that a real and symmetric (or complex hermitian) matrix A is 
positive definite if and only if det(S) > 0 for all of its principal sub- 
matrices S. 
Use the result of Exercise 2.10.16 to provide an alternate proof in terms 
of determinants for the fact in (2.10.30) that states that the entry of 
maximum magnitude in a positive definite matrix is on its diagonal. 
Prove that if rank (Amxn) =n, then A*A is positive definite. 
Prove that the Cholesky factorization for a positive definite matrix is 
unique in the sense that there is a unique upper triangular matrix R 
with positive diagonal entries such that A = R*R. 

282 
Chapter 2 
Systems, Elimination, and Echelon Forms 
2.10.20. 
2.10.21. 
fest be 0 
4 
. 
. 
s 
a 
Consider the tridiagonal matric T = 
a re Be 73 
o 
0 
as 
fe 
(a) Assuming that T possesses an LU factorization, verify that it 
is given by 
cf 
0 
0 
0 
az 
71. 
0 
4.0 
pee 
ay, Vi 
TT 
1 
0 
0 
us 
0 
TS = ND 
0 
Lis 
0 
a2/T2 
iy 
0}? U 
0 
0. 
"3 
"ys 
12 
0 
0 
a3 / 73 
«dW 
0 
0 
0-14 
where the 7;'s are generated by the recursion formula 
: 
Yi 
™m =P, 
and 
m1 = igi - a 
tu 
(b) Apply this recursion formula to obtain the LDL? factorization 
of 
and conclude that A (along with the n xn version of A) is 
positive definite. 
Note: The same holds for tridiagonal matrices of arbitrary size, so the 
LU factors of every tridiagonal matrix are easy to compute, and hence 
tridiagonal linear systems are easy to solve. Tridiagonal matrices occur 
frequently in practice—e.g., the application on page 220 that concerns 
two-point boundary value problems is a common situation that naturally 
gives rise to a large tridiagonal linear system. 
Anxn is called a band matriz if aj; =0 whenever |i— | > w for some 
positive integer w, called the bandwidth. In other words, the nonzero 
entries of A are constrained to be in a band of w diagonal lines above 
and below the main diagonal. For example, tridiagonal matrices have 
bandwidth one, and diagonal matrices have bandwidth zero. If A is a 
nonsingular matrix with bandwidth w, and if A has an LU factoriza- 
tion A = LU, then L inherits the lower band structure of A, and 
U inherits the upper band structure in the sense that L has "lower 
bandwidth" w, and U has "upper bandwidth" 
w. Illustrate why this 
is true by using a generic 5 x 5 matrix with a bandwidth of w = 2. 

2.11 Short History of Triangular Factorizations 
283 
2.11 
SHORT HISTORY OF TRIANGULAR FACTORIZATIONS 
The Chinese counting board, its colored rods, and Seki 
Kowa's "rule of thumb" elimination methods (mentioned 
on page 115) found their way to Europe, where they even- 
tually became known as Gaussian elimination in honor of 
the German mathematician Carl Friedrich Gauss (1777- 
1855) whose extensive use of it popularized the method. 
Gauss is regarded as one of the greatest mathematician 
of all time, and his astounding career requires several vol- 
umes to document. He was referred to by his peers as the 
"prince of mathematicians." Upon Gauss's death one of 
them wrote that "His mind penetrated into the deepest 
secrets of numbers, space, and nature; He measured the course of the stars, 
the form and forces of the Earth; He carried within himself the evolution of 
mathematical sciences of a coming century." History proved this to be true. 
The Gauss—Jordan variation of elimination evolved around the same time, 
but there has been some confusion as to which Jordan should receive credit for 
the technique. Most people now agree that the method was in fact introduced by 
a geodesist named Wilhelm Jordan (1842-1899) and not by the more well known 
mathematician Marie Ennemond Camille Jordan (1838-1922), whose name is 
often mistakenly associated with the algorithm, but who is otherwise correctly 
credited with other important topics in matrix theory, the Jordan canonical form 
(page 585) being the most notable. Wilhelm Jordan was born in southern Ger- 
many, was educated in Stuttgart, and was a professor of geodesy at the technical 
college in Karlsruhe. He was a prolific writer, and he introduced his elimination 
scheme in the 1888 publication Handbuch der Vermessungskunde. Interestingly, 
a method similar to Jordan's variation of Gaussian elimination seems to have 
been discovered and described independently by an obscure Frenchman named 
Clasen who appears to have published only one scientific article—it appeared in 
1888, the same year as Jordan's Handbuch appeared. 
Herman Goldstine (profiled on page 286) was one of the pioneers of digital 
computation and the use of electronic computers to solve systems of linear equa- 
tions. He once noted that the art of solving linear equations remained pretty 
much the same as Gauss left it until World War II. During this span of time the 
systems of primary interest were those arising from geodetic calculations that 
utilized continental survey data to accurately determine boundaries by means 
of Gauss's theory of least squares (page 481). In matrix notation these systems 
have the form A? Ax = A'b and are called the normal equations (see Exercises 
2.4.34 and 2.5.28 on pages 199 and 218). However, Gauss did not use matrices 
to either describe or solve these systems. He simply employed a straightforward 
elimination technique that remained in use for over 100 years until the time of 
Cholesky. 
CARL F. GAuss 

284 
Chapter 2 
Systems, Elimination, and Echelon Forms 
Andre-Louis Cholesky (1875-1918) 
The arithmetic for the Cholesky factorization on page 277 
was formulated by Major André-Louis Cholesky who was 
a French military officer. Although originally assigned to 
an artillery branch, Cholesky later became attached to 
| 
the Geodesic Section of the Geographic Service in France 
| 
where he became noticed for his extraordinary intelli- 
gence and his facility for mathematics. From 1905 to 1909 
Cholesky was involved with the problem of adjusting the 
triangulation grid for France. This was a huge computa- 
tional task, and there were arguments as to what com- 
es 
putational techniques should be employed. It was during 
A. CHOLESKY 
this period that Cholesky invented the ingenious method 
for solving a positive definite system of equations that is the basis for the matrix 
factorization that now bears his name. Unfortunately, Cholesky's mathematical 
talents were never allowed to flower. In 1914 war broke out, and Cholesky was 
again placed in an artillery group—but this time as the commander. On August 
31, 1918, Major Cholesky was killed in battle. Cholesky never had time to publish 
his clever computational methods, but they were carried forward by word-of- 
mouth. Issues surrounding the Cholesky factorization have been independently 
rediscovered several times by people who were unaware of Cholesky, and, in 
some circles, the Cholesky factorization is known as the square root method. 
While Cholesky's method contained the essence of the matrix factorization that 
now bears his name, he did not formulate his algorithm as a matrix factorization. 
Myrick Hascall Doolittle (1830-1911) 
The American Myrick Doolittle dedicated much of his time 
to solving the normal equations to adjust triangulations 
for the United States Coast and Geodetic Survey. He was 
reported to be a human computer, and he was proud of his 
ability to solve the normal equations in less time than that 
required by his contemporaries. In 1878 he published his 
technique in a U. S. Coast and Geodetic Survey report. 
Doolittle's algorithm was a variant of Gaussian elimina- 
tion, and while it was not in terms of a matrix factoriza- 
tion, the arithmetic was equivalent to that of using the LU 
factorization. In later years his method became known as 
the Gauss—Doolittle method. 
M. H. DOOLITTLE 

2.11 Short History of Triangular Factorizations 
285 
Tadeusz Banachiewicz (1882-1954) 
It was not until the 1930s that Gaussian elimination became 
formulated and analyzed in terms of matrices. The computa- 
tional statistician Paul Sumner Dwyer (who also contributed 
to the numerical implementation of triangular factorizations) 
suggests in his 1951 text Linear Computations that the Polish 
astronomer Tadeusz Banachiewicz may have been the first to 
realize that the problem of extracting a numerical solution 
to a linear system by means of Gaussian elimination is really 
one of matrix factorization. Banachiewicz's matrix factoriza- 
tion ideas appeared in 1938 in Bull. Int. L' Acad. Polonaise, 
T. BANACHIEWICz 
but because he formulated things in terms of "cracovians" (his invention), his 
numerical methods received little attention in the west. Banachiewicz seemed 
to realized that the diagonal matrix D in the LDU factorization can be sub- 
sumed either into the left-hand factor as (LD)U or into the right-hand factor 
as L(DU), and the resulting differences can affect tabular manipulations and 
storage requirements. 
Banachiewicz was born to the owners of an estate near Warsaw, and he 
studied at Warsaw University in 1903. After the Russians closed the University 
in 1905, Banachiewicz moved to Gottingen, Germany, and in 1906 to the Pulkowa 
Observatory. After passing the habilitation exam in 1910, Banachiewicz worked 
in Kazan, Russia, where he conducted heliometrical studies of the moon. He 
became an astronomy professor in Dorpat, Russia (presently Tartu, Estonia) 
in 1915, and after Poland regained its independence, Banachiewicz moved to 
Cracow, Poland in 1919 to become a professor at the Jagiellonian University 
and the director of Cracow Observatory. Banachiewicz was widely known in his 
time for his development of the theory of cracovians, a special kind of matrix 
algebra that was used to solve several astronomical, geodesic, and mechanical 
problems. He is known for determining the orbit of Pluto. 
Prescott Durand Crout (1907-1984) 
In the era of the slide rule and the mechanical desk calcula- 
tor the advantages of the compact LDU factorizations were 
quickly appreciated. In 1941 Prescott Durand Crout inde- 
pendently discovered and described the (LD)U formulation 
in his paper "A short method for evaluating determinants 
and solving systems of linear equations with real or com- 
plex coefficients," 
Transactions of the American Institute 
of Electrical Engineers, 1941, (60)1235-1241. The technique 
became popular with people using mechanical calculators, 
and the Marchant Calculating Machine Company even is- 
sued detailed instructions on using Crout's method in some 
of their technical manuals. 
P. D. CroutT 

286 
Chapter 2 
Systems, Elimination, and Echelon Forms 
John von Neumann (1903-1957) and Herman Heine Goldstine (1913-2004) 
Using triangular factorizations to organize the arithmetic 
was important, but perhaps an even more significant devel- 
opment was given by John von Neumann (see page 649) and 
Herman Heine Goldstine (mentioned earlier on page 283), 
who was one of the original developers of ENIAC, the first 
of the modern electronic digital computers. In their seminal 
1947 paper "Numerical inverting of matrices of high order" 
published in the Bulletin of the American Mathematical So- 
ciety, they developed a round-off error analysis for a trian- 
gular factorization that put to rest several misgivings con- 
cerning the accumulated round-off errors inherent in matrix 
inversion and the solution of linear systems. In the same arti- 
cle they also put forward the notion of the condition number 
used in Theorems 2.3.13 (page 172) and 2.9.1 (page 172) for 
a positive definite matrix as well as the concept of backward 
error analysis (mentioned on page 239) that was later devel- 
oped and widely applied in the 1960s by James H. Wilkinson 
(1919-1986), who is generally regarded as the modern father 
of matrix computations. Goldstine wrote in his 1972 book The Computer from 
Pascal to von Neumann that: 
H. GOLDSTINE 
"By the late fall of 1946 or very early in 1947 von Neumann and I finally 
understood how matters really stood. Von Neumann remarked one day 
that even though errors may build up during one part of the computa- 
tion, it was only relevant to ask how effective is the numerically obtained 
solution, not how close were some of the auxiliary numbers, calculated on 
the way to their correct counterparts. We sensed that at least for posi- 
tive definite matrices the Gaussian procedure could be shown to be quite 
stable. With this as our goal, we rapidly satisfied ourselves in a heuristic 
fashion that for such matrices this was true. It remained then to produce 
a completely rigorous analysis and discussion of the procedure. It seemed 
to me very important at this time to start modern numerical analysis off 
on the right foot, so I drafted a paper which went in depth into errors, 
numerical stability, and pseudo-operations; and then I embarked on the 
main problem. In mid 1947 I was informing von Neumann, 'I am writ- 
ing the final sections of the matrix inversion paper and rewriting others 
I wrote last week.' Von Neumann agreed with what I had written, and 
after several iterations a paper eventuated in September 1947." 
James Wilkinson often acknowledged that the work of Goldstine and von Neu- 
mann laid the foundations and was the beginning of modern numerical analysis. 

CHAPTER 
3 
Eigensystem Basics 
3.1 
INTRODUCTION TO EIGENSYSTEMS 
Almost everything up to this point has been either motivated by or evolved 
from the consideration of systems of linear algebraic equations, but there is a 
substantial amount of linear algebra that lies beyond linear systems. The study of 
eigensystems composed of eigenvalues and eigenvectors' is the primary example. 
The purpose of this section is to introduce the elementary concepts and properties 
of eigensystems—deeper and more complete developments appear in subsequent 
sections. 
Motivation stems from systems of linear differential equations rather than 
systems of linear algebraic equation. For example, consider the problem of solving 
the system of two linear differential equations, 
wu, = du,/dt = Tu; — 4u2 and 
ul = dug/dt = 5u; — 2ue for the two unknown functions 
u; = ui(t) and 
U2 = U2(t). In matrix notation, this system is 
Ut 
a 
7 
—4 
U4 
" 
. 
Rea 
' 
Ga = (i =) a) 
, 
or equivalently, 
ui = Au, 
(anton) 
/ 
ul, 
ae 
if 
= 
U1 
: 
where 
u' 
= 
}), 
A=(, 
_,}, and u= 
nae Because solutions of a 
U ?4 
single equation u' = Au have the form u = ae, it is natural to seek solutions 
of (3.1.1) that also have the form 
ui=aje* 
and 
up =are. 
(S52) 
The words eigenvalue and eigenvector are derived from the German word eigen, which means 
"owned by" or "peculiar to." Eigenvalues and eigenvectors are also called characteristic values 
and characteristic vectors, proper values and proper vectors, or latent values and latent vectors. 

288 
Chapter 3 
Eigensystem Basics 
Differentiating these expressions and substituting the results into (3.1.1) yields 
a,Ae*t = Taye** — dage** = ay A = Tay — 4a 
(; 2) & a (2) 
a2Ae*t = 5a e** —2are** 
= ag = 5a — 2a2 
5 
—2/] 
\ae 
a2 
In other words, solutions of (3.1.1) having the form (3.1.2) can be constructed 
provided solutions for A and x = (2 
be found. Clearly, x = 0 trivially satisfies Ax = Ax, but x = 0 provides no 
useful information concerning the solution of (3.1.1). What is really needed are 
scalars \ and nonzero vectors x that satisfy Ax = Ax. Writing Ax = Ax 
as (A —AI)x = 0 shows that the vectors of interest are the nonzero vectors 
in N(A—AI). Theorem 2.5.6 on page 209 ensures that N (A — AI) contains 
nonzero vectors if and only if A — AI is singular, so the scalars of interest are 
precisely the values of A that make A—AI singular or, equivalently, the A's for 
which* det (A — AI) = 0. These observations motivate the following definitions. 
in the matrix equation Ax = Ax can 
3.1.1. Definition. For A € F""", the scalars 4 €¢ C" and the vectors 
Xnx1 #0 satisfying Ax = Ax are respectively called eigenvalues and 
eigenvectors 
of A, anda pair (A,x) such that Ax = \x is called an 
eigenpair for A. The set of distinct eigenvalues, denoted by o (A), 
is 
called the spectrum of A. 
e 
A€o(A) = A-—ALis singular <=> det (A — AI) = 0. 
(3.1.3) 
e 
{x40 | x € N(A — XI)} is the set of all eigenvectors associated 
with A, and N(A —AI) is called an eigenspace for A. 
e 
N(A—AI) contains at most n—r 
linearly independent eigenvec- 
tors, where r = rank (A — AI) (see Corollary 2.5.4 on page 207). 
e 
Nonzero vectors y € C"*! such that y7(A — AI) = 0 are called 
left-hand eigenvectors for A. 
Eigenvectors have a simple geometric interpretation. The defining equation 
Ax = \x says that under transformation by A an eigenvector x experiences 
only changes in magnitude or sign—the orientation of Ax is the same as that 
of x. The eigenvalue \ is the amount of "stretch" or "shrink" to which the 
eigenvector 
x is subjected when transformed by A. The following diagram 
illustrates this feature in R?. 
Basic knowledge of determinants is required for this section. If needed, refer to the appendix 
on page 939 for an overview of the relevant facts concerning determinants. 

3.1 Introduction to Eigensystems 
289 
Ax 
= Ax 
AN EIGENVECTOR IN R? 
Example 
Consider the problem of finding the eigenvalues and eigenvectors of the matrix 
A= : ye) in (3.1.1). As noted in (3.1.3), the eigenvalues are the scalars » 
for which det (A — AI) = 0. Expansion of det (A — AI) produces the second- 
degree polynomial 
t—- 
—4 
p(X) = det (A = AT) =| Wares. =? -5A+6 = (1-2)(-3), 
which is called the characteristic polynomial for A. The eigenvalues are the 
solutions of the characteristic equation p(A) = 0, which are 
A= 2 and A =3. 
The eigenvectors associated with A = 2 and \ =3 
are the nonzero vectors in 
the respective eigenspaces N (A — 21) and N(A-—3I), which are determined 
by solving the two homogeneous systems (A — 2I)x =0 and (A —3I)x=0O. 
Fore =o 
Aol 
= 
Ces 
eer 
Lia 4)5 
a 
x1 = (4/5)x2 
0 
0 
Xo is free 
ee 
a N (A= 2) = {x | ewes 
1 
—-l 
1 =— 
ae 
0 
0 
an 
2 is free 
= N(A=31) = {x | x=6(7)} 
In other words, the eigenvectors of A associated with A = 2 are all nonzero 
Aad 
For A\=3: A-3I at rd 
multiples of x = Bk and the eigenvectors associated with A = 3 are all 
nonzero multiples of y = ( 
: r 

290 
Chapter 3 
Eigensystem Basics 
To complete the discussion concerning the system of differential equations 
uw' = Au in (3.1.1), use (3.1.2) with the eigenpairs (A1,x) and (A2,y) of A 
that were found above to produce the two solutions 
hy = ex = 
oP) 
and 
ug = ey = e* cy 
: 
It turns out that all other solutions are linear combinations of these two particular 
solutions—more about eigenvectors and differential equations is discussed on 
page 330. 
Characteristic Polynomial and Equation 
The terminology from the preceding example along with some basic facts are 
formalized and summarized below. 
3.1.2. Definition. The characteristic polynomial of A € F"*" is 
defined to be p(A) = det (A — AI), and the equation p(A) = 0 is called 
the characteristic equation. 
e 
The definition of determinant in (9.2.1) ensures that deg p(A\) = n, 
~ 
and the leading term in p(A) is (—1)"A" (see Exercise 3.1.8). 
e 
The eigenvalues of A are the solutions of the characteristic equa- 
tion (ie., the roots of p(A)). The fundamental theorem of alge- 
bra' ensures that there are exactly n eigenvalues, but some may 
be repeated and some may be complex numbers, even if all entries 
in A are real. 
— When A is real, \ € a (A) if and only if \ € o (A). 
In other words, complex eigenvalues occur in conjugate 
(3.1.4) 
pairs because the roots of a polynomial with real coef- 
ficients occur in conjugate pairs (see Exercise 3.1.4). 
Example (A Real Matrix with Complex Eigencomponents) 
To illustrate the fact that even simple matrices of real numbers can have com- 
plex eigenvalues and eigenvectors, consider A = (; aay The characteristic 
polynomial is 
det(A— at) =|" 7 
= 
, 
(|= Cay 
atom be 
t 
: 
The fundamental theorem of algebra is a substantial result (it was Gauss's doctoral dissertation) 
that insures every polynomial of degree n with real or complex coefficients has exactly n roots, 
but some roots may be complex numbers (even if all the coefficients are real), and some roots 
may be repeated. 

3.1 Introduction to Eigensystems 
291 
so the characteristic equation is \7 — 2\+2=0. The quadratic formula yields 
yo PEO _ 2t2VRi 
5 
9 
== 1 4, 
so a (A) = {1+i, 
1—i}. Notice that the eigenvalues are complex conjugates of 
each other, as they must be by (3.1.4). Now find the eigenspaces. For \ = 1 +i, 
A-M=(4 a) G 7 aA N(A~Af) = span {(+)\. 
For A\=1-—i, 
A-at=(} YG al zim N(A—A) = span] (7). 
In other words, the eigenvectors associated with A; = 1+i are all nonzero 
multiples of x; = Cae and the eigenvectors associated with Aj = 1—i are 
all nonzero multiples of x» = ce ). This example makes it clear that avoiding 
complex numbers, even when matrices are real, may not be possible. 
Normalized Eigenvectors 
Every nonzero multiple of an eigenvector x for A is again an eigenvector for 
A because if a #0, then 
Ax 
Ax = 
Alax) = X(ex): 
This means that every eigenvector can always be normalized whenever it is con- 
venient to do so by setting a = 1/||x||. In other words, x = x/||x|| 
is a 
normalized eigenvector for A in the sense that if (A,x) is an eigenpair for A, 
then (A,X) is also an eigenpair for A, but with 
||x|| = 1. For example when 
the eigenvectors 
ey = a and oy = hace for the matrix A = 
: a in 
the preceding example are normalized, the result is the two eigenvectors 
Real Eigenvalues and Orthogonal Eigenvectors 
As pointed out above, real matrices can have complex eigenvalues. However, if 
A is real and symmetric or complex and hermitian, then all eigenvalues must 
be real numbers. 

as i 'ee 7% wit 
- "BEaPy 
"Supp Sariree fay at nad tee WEA x) bea gate Mores for A re 
=" 
il = : 1. The result follows by using Ax = Ax => Ax* =x*A*=x"*A to 
_ observe that: 
N= Ax*x =x"(Ax)=x*Ax 
and 
\=)x*x = (x*A*)x =x*Ax. 
Thus A must be real because 
\=. 
E 
In addition to having real eigenvalues, the eigenvectors of hermitian (and 
real-symmetric) matrices that correspond to distinct eigenvalues must be orthog- 
onal to each other. 
Proof. 
Use A = A* together with the fact that jy is real (by Theorem 3.1.3) 
to observe that Ay = py => y*A*=fy* 
=> y*A=ypy%. It now follows 
that 
(y|x) (u — A) = y*x(u — A) = (uy")x — y* (Ax) = y"Ax — y*Ax = 0, 
and thus (y|x) =0 because 
\Ayw. 
J 
Eigenvalues of Triangular, Diagonal, and Nonsingular Matrices 
Eigenvalues of general square matrices are often difficult to compute because 
solving higher degree polynomial equations can be formidable. However, the 
eigenvalues of triangular and diagonal matrices are available by inspection—they 
are simply the diagonal entries because 
tin 
tg 
ee 
fig 
t22 
++: 
tan 
es 
= 
=> 
det (T — AI) = 
(t11 — A)(ta2 — A) +++ (trn — A) 
tnn 
= > {ti,te2,-..,tnn} 
are the eigenvalues of T. 
(3.1.5). 
Similarly, the eigenvalues of a diagonal matrix are its diagonal entries. 
On the other hand, eigenvalues of general nonsingular matrices are not so 
easy to discern except for the fact that they can never be zero. 

3.1 Introduction to Eigensystems 
293 
Proof. 
The result follows from the observation that 
0€a0(A) 
=> Ax=0 
forsomex 
#0 
<=> N (A) #40 <> A is singular (by Theorem 2.5.6, page 209). 
Mf 
Coefficients in the Characteristic Equation 
Computing eigenvalues by directly solving the characteristic equation can be a 
daunting task. It was proven in the nineteenth century that it is impossible to 
express the roots of a general polynomial of degree five or higher using radicals of 
the coefficients. This means that there does not exist a generalized version of the 
quadratic formula for polynomials of degree greater than four, and general poly- 
nomial equations cannot be solved by a finite number of arithmetic operations 
involving +, —,X,+, o/ (see page 765). Unlike solving Ax = b, the eigenvalue 
problem generally requires infinite (i.e., iterative) algorithms that do not involve 
explicit knowledge of the characteristic polynomial. Nevertheless, for theoretical 
work and textbook-type problems it can be helpful to have explicit formulas for 
the coefficients in the characteristic equation, and such formulas are possible in 
terms of principal minor determinants. 
Recall that an r x r principal submatriz of An 
xn is a submatrix that lies 
on the same set of r rows and columns, and an r xr principal minor is the 
determinant of an r x r principal submatrix. In other words, r x r_ principal 
minors are obtained by deleting the same set of n — r rows and columns, and 
there are (") = n!/r!(n —r)! such minors. For example, the 1 x 1 principal 
minors of 
—3 
1 
-3 
Age ( 
20 
8 10 
(3.1.6) 
2 
-2 
4 
are the diagonal entries —3, 3, and 4; the 2 x 2 principal minors are 
—3 
1 
—3 
-3 
cee) 
Bp 4 ge | 
Q 
tl . Ss A me 
and the only 3 x 3 principal minor is det (A) = —18. 
Related to the principal minors are the symmetric functions. The kt? sym- 
metric function of Ai, A2,.--,An is defined to be the sum of the product of the 
\;'s taken k at a time. That is, 
Spe Ag 
A 
Li. <<a <n 

Proof. 
At least two proofs of (3.1.7) are possible, and although they are con- 
ceptually straightforward, each is somewhat tedious. One approach (not used 
here) is to successively use the result of Exercise 9.2.15 on page 954 to expand 
det (A — AI). The other approach is the proof given below that rests on the 
observation that if 
p(r) = deb(AtseMl) Saf T)"\" Bag N24 Gp ie eae a 
is the characteristic polynomial for A, then the characteristic equation is 
AA 
QA ocean 
yA op eee 0,, 
where 
oc =({—1)"a;. 
Taking the r'" derivative of p(A) yields p")(0) =rla,_,, and hence 
Cn—r = (0). 
| 
(3.1.11) 
It is now a matter of repeatedly applying the differentiation formula (9.2.10) in 
Theorem 9.2.15 on page 952 to evaluate derivatives of p(X) = det (A — AI). The 
result after r applications of this formula is 
p\") (A) = S2 Diz..-i, (A), 
tj Atk 

3.1 Introduction to Eigensystems 
295 
where Dj,...;,(A) is the determinant of the matrix identical to A — AI except 
ae 
: 
% 
Sy 
ee 
that rows 71, i2,...,%, have been respectively replaced by Sig Op. 5 
1s eT 
It follows that Dj,...;,(0) = (—1)"det (Aj,...i,), where Aj,;,...3, 
is a matrix 
identical to A except that rows 71,i2,...,7, have been respectively replaced 
by ef ,e2,... ,e7, and det (Aj,...;,) is the (n —r) x (n—r) principal minor 
obtained by deleting rows and columns 11, i2,...,i, from A. Consequently, 
pO) =) Daa, O)=(—1)" ¥° det (Ai,..z,) 
1; Atk 
iG FR 
srt xe (—1)" S (all n—rXn-—r 
principal minors). 
The factor r! appears because each of the r! permutations of the subscripts on 
Aj,...i, describes the same matrix. Therefore, (3.1.11) says that 
(ails 
Car = aN (p= S "(all nm—frxXn-—r 
principal minors). 
To prove (3.1.8), write the characteristic equation for A as 
(A — A1)(A — Aa) +++ (A= An) = 0, 
(Salek2) 
and expand the left-hand side to produce 
Ne gL 
ee 
1 eae 
ay = 0, 
(3.1.13) 
(Using n = 3 or n = 4 in (3.1.12) makes this clear.) Comparing (3.1.13) 
with (3.1.7) produces the desired conclusion. Statements (3.1.9) and (3.1.10) are 
obtained from (3.1.7) and (3.1.8) by setting 
k=1 andk=n. 
UB 
Example (The Companion Matrix) 
For every matrix A ¢€ F"*" there is an associated characteristic equation, but 
can the tables be turned? In other words, is every polynomial equation 
p(x) =" +a,-70" 
| +.-- bar +a9 =0 
the characteristic equation of some n x n matrix? Yes! And one such matrix is 
@ 
@ 
ccm 
16) 
—ao 
ik 
(Q) 
e55 
10) 
—Q1 
el 
ee 
le 
(3.1.14) 
0) 
0) 
=an=3 
OQ 
@ 
cos 
i 
Seren 
poe 
which is called the companion matriz of p(x). This is an immediate consequence 
of Theorem 3.1.6 because direct determinant evaluation shows that 
a, = (—1)* S (all k xk principal minors). 
The 4 x 4 case makes it clear. 

296 
Chapter 3 
Eigensystem Basics 
Multiplicities 
—3 
1 -3\~ 
Ta? 
. 
Using the matrix 
A= { 20 
3 i) 
in (3.1.6) along with the principal minors 
2-2 
4 
calculated there, the characteristic equation is determined from the formulas in 
(3.1.7) to be 8 — 4\? — 34 18 =0. To find integer eigenvalues of A apply a 
standard result from elementary algebra stating that if the coefficients a; in 
A" + Opa Ae) + +? FaiA+a9 
=0 
are integers, then every integer solution is a factor of ag. This means that 
all integer eigenvalues for the problem at hand must be contained in the set 
S = {+1, +2, +3, +6, +9, +18}. Evaluating p(A) for each 
4 € S reveals that 
p(3) = 0 and p(—2) = 0, so 
AX = 3 and A = —2 are eigenvalues for A. To 
determine the third eigenvalue, deflate the problem by dividing 
3 
Pile 
Sst 
ETN ey 
Thus the characteristic equation (in factored form) is (A — 3)?(A + 2) = 0, 
so a(A) = {3, —2} in which \ = 3 is repeated. This repetition is expressed 
by saying that the algebraic multiplicity of 4 = 3 is two. The eigenspaces are 
obtained as follows. For A = 3, 
1 wOwea/2 
a3 
A-at— (0 1 
0 
=> N (A ~31) = span { oy}. 
Or) 
0. 
2 
For \ = —2, 
ib 
0 
1 
=i 
A+ 
(0 1 -2) => N (A +21) 
= span { 
( 
2). 
OO 
0 
1 
Notice that although the algebraic multiplicity of \ = 3 is two, there is only one 
independent eigenvector associated with \ = 3. As will later be seen, eigenvalues 
that are deficient in the sense that their algebraic multiplicity exceeds the number 
of associated independent eigenvectors pose difficulties, so there is a need for the 
following terminology to distinguish such cases. 

Spectral Radius 
; 
= 
==} 
iy 
y=3 
z 
For example, 
the matrix A = ( 
20 
3 0] used above has o (A) = {3, —2} 
: 
2 -2 
4 
in which 
{ 
alg mult, (3) = 2 } 
. { 
alg mult, (—2) =1 } 
' 
an 
: 
geo mult, (3) =1 
geo mult, (—2) =1 
so A = —2 is simple, but \ = 3 is neither simple nor semisimple. 
To see that a semisimple eigenvalue need not be simple, let A = ( 
' 2) 
and note that a (A) = {2} with alg mult, (2) = 2. There is an associated set of 
two linearly independent eigenvectors, namely {e1,e2}, so geo mult, (2) = 2, 
and hence A = 2 is semisimple but not simple. 
aegiienmacies 

298 
Chapter 3 
Eigensystem Basics 
There are times in applied work when precise knowledge of each eigenvalue 
is not required, but rather just an upper bound on p(A) is all that is needed. A 
rather crude (but cheap) upper bound on p(A) is obtained by observing that 
ANE€a(A) 
=> 
|A\ <||Al] 
for every matrix norm. 
(3.1.16) 
This is true because if (A,x) is any eigenpair for A, and if ||A||, is any matrix 
norm, then there is some vector norm 
||x||, that is compatible with the given 
matrix norm (see Exercise 1.9.4 on page 88). Consequently, if (A,x) is any 
eigenpair for A, then x #0, so taking norms yields 
Ax =Ax => 
|A\([xll, = Axl, = IIAxll, < IATL xl, => 
Dl SIA. 
In other words, (3.1.16) can be restated as follows. 
| 
3.1.9. Corollary. p(A) < |All for all A ¢ F"*" and all matrix norms. 
=. 
710 
For example, consider A = (: 1 0) 
with the Frobenius, 1-, and co- 
0 
Of 
norms from pages 82 and 86. Since ||A||, = V5, and ||A||, =2=||Al|,,, the 
inequality in (3.1.16) guarantees that all eigenvalues are in (or on) a circle of 
radius 2 centered at the origin. This is indeed the case because computing the 
eigenvalues of A reveals that o (A) = {1+i, 1 —i, 1}. 
Exercises for section 3.1 
3.1.1. Determine the eigenvalues and eigenvectors for the following matrices. 
Agee? 
Dee 
ANS 
8 
ay 
ay) 
As 
Ate ( 14 
He) 1B = 
4 
14 
8 
G0 
1 
4 
=) 
280 
aie 
Q 
af 
& 
OR 
OeRS 
30 
BO 
Dis |e —155 ) B=(3 3 0) 
al 
Oe Om 
-8 
-6 
-3 
-1 
: 
c 
: 
Sel ceetoreA a= 
20 
15g 
5 
|: determine which of the following are 
SZ ee 
(S12 
eigenvectors for A without doing an eigenvalue-eigenvector computa- 
tion. For those that are eigenvectors, identify the associated eigenvalue. 
i 
1 
—1 
0 
(a) 
a 
lh i 
5 dei Ved 
be 
1 
0 
2 
0 

3.1 Introduction to Eigensystems 
299 
3.1.3. By computing only norms find a circle around the origin that contains 
3.1.4. 
3.1.5. 
3.1.6. 
Seles 
3.1.8. 
3.1.9. 
3.1.10. 
3.1.11. 
2 
1 
-l 
all of the eigenvalues of A = (0 
4 =). Is this the smallest circle 
Ose 
=f 
around the origin that contains the eigenvalues of A? 
Prove that if p(a) is a polynomial having real coefficients, then com- 
plex roots must occur in conjugate pairs—i.e., p(A) = 0 if and only if 
p(A) = 0. See Exercise 3.2.3 for an alternate approach. 
Show that the eigenvalues of A =cyyid7,,, are 
\=0 and 
\=d?7c. 
What is the algebraic multiplicity of each of these? 
Determine the characteristic equation as well as the eigenvalues for 
Anxn = 
Show that if (A, x) and (jy, y) are respective right-hand and left-hand 
eigenpairs for 
A € F""", then y'x =0 whenever \F pL. 
For A € F"~", use the definition of determinant (page 941) to show that 
the characteristic polynomial det (A — AI) is an n*" degree polynomial 
whose term is (—1)"A"". Hint. [A — AI];; = aiz — 6:3, where 
5. 
fl t=3, 
O~)0 
ift#;. 
For T = Gs ar prove that det (T — XI) = det (A — Al)det (C — AI) 
to conclude that AG a =a(A)Ua(C) for square A and C. In 
particular, for a block-diagonal matrix, all a, =O (A)Wia(C)- 
Determine the eigenvectors of D = diag (Aj, A2,.-.,An)- In particular, 
what is the eigenspace associated with A;? 
For A € F™*", prove that A*A and AA* have the same nonzero 
eigenvalues. 

300 
Chapter 3 
Eigensystem Basics 
3.1.12. Explain why the eigenvalues of A*A and AA* are real sand nonneg- 
ative for every A € F™*". Hint: Consider |Ax||3 / IIx|I3 - When are 
the eigenvalues of A*A and AA* strictly positive? 
3.1.13. 
(a) 
If A is nonsingular, and if (A, x) is an eigenpair for A, show 
that (A~!, x) is an eigenpair for Aé* 
(b) For all a ¢ o(A), prove that x is an soli of A if and 
only if x is an eigenvector of (A —aI)7!. 
3.1.14. A square matrix A € F"™" is called idempotent when A = A?. Prove 
that if A is idempotent, then o (A) = {1,0}. 
3.1.15. 
(a) Show that if (A, x) is an eigenpair for A, then (A*, x) is an 
eigenpair for A* for each positive integer k. 
(b) 
If p(x) =ap+a,z+agzr?+---+a,2* is any polynomial, then 
we define p(A) to be the matrix 
p(A) = aol + ayA + ag A? +++- + a,A*. 
Show that if (A, x) is an eigenpair for A, then (p(A), x) is an 
eigenpair for p(A). 
3.1.16. For A € F"*", prove that there exists 
a 5 > 0 such that A — lI is 
nonsingular for all e 
#0 such that |e| < 6. 
3.1.17. For every A € F"*", is it true that 1A € 0 (A*A) if X 
€o(Anxn)? 
Explain why. 
3.1.18. 
(a) Prove that all eigenvalues of a skew hermitian (or real and skew 
symmetric) matrix A ¢ F"*" must be pure imaginary numbers 
(multiples of i). 
(b) 
Let Ae¢R""*" be real and skew symmetric. Explain why A is 
singular when n is odd. 
3.1.19. A square matrix A is said to be nilpotent whenever A* = 0 for some 
positive integer k. Prove that if A is nilpotent, then trace (Ay =20- 
Hint: What is o (A)? 
3.1.20. Prove that if A ¢ F"*" is such that (I-A)? =0, then trace (A) =n. 

3.1 Introduction to Eigensystems 
301 
3.1.21. 
3.1.22. 
3.1.23. 
3.1.24. 
3.1.25. 
3.1.26. 
3.1.27. 
If x1, X2,...,X» are eigenvectors of A associated with the same eigen- 
value A, explain why every nonzero linear combination 
V = 
1X1 + A2X_g +°°: + AnXn 
is also an eigenvector for A associated with the eigenvalue X. 
Explain why an eigenvector for a square matrix A cannot be associated 
with two distinct eigenvalues for A. 
Suppose o(Anxn) = ¢(Bnxn). Does this guarantee that A and B 
have the same characteristic polynomial? 
Construct 2 x 2 examples to prove the following statements. 
(a) AE€o(A) and weo(B) & A+pweo(A++B). 
(b) AE€o(A) and pEo(B) 
= 
AwEo(AB). 
Show that if (Ax, c) is an eigenpair for A but A ¢ o(A), then 
(A—AD='c =c/O, —4). 
Let 
Ac F"™". 
(a) Explain why A and A" have the same eigenvalues. 
Hint: What is det (A7)? 
(b) Explain why \ € 0 (A) == \€a(A*). 
Hint: Recall Exercise 9.2.9. 
(c) Do these results imply that 
\ € o(A) == 
€0(A) when A 
is a square matrix of real numbers? 
(d) Let (u,y) be a left-hand eigenpair for A in the sense that 
y?(A — pI) = 0. Explain why 
must also be an eigenvalue 
for A associated with some right-hand eigenvector. 
Consider matrices Amn and Bnxm.- 
(a) Explain why AB and BA have the same characteristic poly- 
nomial if m =n. Hint: Recall Exercise 9.3.17 (page 968). 
(b) Explain why the characteristic polynomials for AB and BA 
can't be the same when m#n, and then explain why a (AB) 
and a (BA) agree, with the possible exception of a zero eigen- 
value. Note: For a different approach to this problem see Exer- 
cise 3.2.32 on page 321. 

302 
Chapter 3 
Eigensystem Basics 
3.1.28. 
3.1.29. 
Prove that if A,B € F"*" commute (i.e., AB = BA), then A and B 
have a common eigenvector. 
Hint: For \ € 0 (A), 
let the columns of X be a linearly independent 
spanning set for N(A—AI) so that (A — \I)BX = 0. Explain why 
there exists a matrix P such that BX = XP, and then consider any 
eigenpair for P. 
Newton's Identities! Let {\1, A2,---,An} be the roots of the polynomial 
p(A) = A®+.e,A"-1 4 cgA"-2 +--+ +en, and let Te = AF +AH+--- +R. 
Newton's identities say that for each k = 1,2,...,n, 
Ch = —(T1Ck—1 + T2Ch—-2 + +++ +7h-101 + Th)/k 
where 
co =1. 
Derive these identities by executing the following steps. 
(a) Show p'(A) = p(A) 37, 
(A —Ai)7! (logarithmic differentiation). 
(b) Use the geometric series expansion for (A — Aj) 
(c) 
3.1.30. 
3.1.31. 
3.1.32. 
1 to show that for 
|A| = max;|A;|, 
n 
n 
T1 
Iara 
a 38 
tL, 
Combine these two results, and equate like powers of X. 
For A,B € F"*", prove that if trace(A*) = trace(B*) for each 
k=1,2,...,n, then A and B have the same characteristic equation, 
so A and B have the same eigenvalues with the same multiplicities. 
Extend Exercise 3.1.30 by proving that if A ¢ F™*™ and B € F"*", 
and if trace(A*) = trace(B*) for each k, then A and B have the 
same nonzero eigenvalues with the same multiplicities. 
For A, 
Be F"*", prove that AB and BA have the same character- 
istic equation, and hence AB and BA have the same eigenvalues with 
the same multiplicities. Hint. Recall Exercise 9.3.17 (page 968). 
These identities are reported to have been discovered by Isaac Newton around 1666. Newton 
was apparently unaware of earlier work by Albert Girard in 1629 who is also credited with their 
derivation. In some places these identities are called the Newton—Girard formulas. They are 
useful in matrix theory because, as Exercises 3.1.29-3.1.36 illustrate, they reveal the connection 
between coefficients of characteristic polynomials, traces, and sums of powers of eigenvalues. 

3.1 Introduction to Eigensystems 
303 
3.1.33. For Aj, Ao,...,A, € F""", prove that all cyclic permutations of the 
product A;A»---A, have the same characteristic equation, and hence 
all such products have the same eigenvalues with the same multiplici- 
ties. For example, the cyclic permutations when k = 3 are A;A»oAz, 
A oAsA;, AgA;Ao. 
3.1.34. Let 
Ac F"*" and 
Be F"*™, where 
m#n (say m > 7n). Prove that 
if trace ([AB]*) = trace([BA]*) for each k = 1,2,...,n, then AB 
and BA have the same nonzero eigenvalues with the same multiplicities, 
but AB has m—n more zero eigenvalues than BA has. 
3.1.35. Let A ¢ F"*" and Be F"*™, where m 4 n (say m > n). Prove 
that AB and BA have the same nonzero eigenvalues with the same 
multiplicities, but AB has m—n more zero eigenvalues than BA. 
Hint: Recall Exercise 9.3.17 on page 968. 
3.1.36. Le Verrier—Souriau-Frame Algorithm! Let the characteristic equation 
for A be given by A" + @A"—-! + @A"-?2 +--- +e, =0, and define a 
sequence by setting Bp =I and B, = —|trace (AB,_1)/k|I + ABx_-1 
(Otel 
a ees ite PE TOVe, COAL 
_ trace (ABxg-1) 
; 
fOr, Whisk 
eM. 
Ch 
= 
Hint: Use Newton's identities in Exercise 3.1.29 along with part (a) of 
Exercise 3.1.15. 
This algorithm has been rediscovered and modified several times. In 1840, the Frenchman Ur- 
bain Jean Joseph Le Verrier (1811-1877) (sometimes written Leverrier) provided the connection 
with Newton's identities. J. M. Souriau, also from France, and J. S. Frame, from Michigan State 
University, independently modified the algorithm to its present form—Souriau's formulation 
was published in France in 1948, and Frame's method appeared in the United States in 1949. 
Paul Horst (USA, 1935) along with Faddeev and Sominskii (USSR, 1949) are also credited 
with rediscovering the technique. Although the algorithm is intriguingly beautiful, it is not 
practical for floating-point computations. 
Le Verrier is famous for the discovery of Neptune in 1846. Motivated by the need to explain 
discrepancies between the theoretical and observed orbit of Uranus, Le Verrier predicted the 
existence and position of Neptune by using only mathematics based on the laws of Newton 
and Kepler. Le Verrier sent his predictions to J. G. Galle, an astronomer in Berlin, asking 
him to provide verification. Galle was able to locate Neptune the same night he received Le 
Verrier's letter to within 
1° of Le Verrier's prediction. The discovery of Neptune using only 
mathematics came to be regarded as one of the most remarkable scientific achievements of the 
19th century. But unknown to Le Verrier, similar calculations were simultaneously being made 
by an English mathematician named John Couch Adams, and this evolved into a long-standing 
controversy about who should be credited with the discovery of Neptune. 

304 
Chapter 3 
Eigensystem Basics 
3.2 
SIMILARITY AND DIAGONALIZATION 
A fundamental objective in matrix theory is to perform some kind of transforma- 
tion that will reduce a matrix A to a simpler form while preserving important 
information or features embedded in A. For example row reduction A —+ Ea 
creates a simpler form while preserving column relationships (recall Theorem 
9.2.5 and Corollary 2.2.7 on pages 146 and 148). However, row reduction gener- 
ally destroys eigenvalues, so when eigensystems are being investigated a different 
type of transformation is required. 
3.2.1. Definition. Two n x n matrices A and B are said to be similar 
whenever there exists a nonsingular matrix P such that P~'AP = B. 
The product P~!AP is called a similarity transformation of A. 
Many standard functions of a square matrix are invariant under a similarity 
transformation—e.g., trace and determinant are easily shown to be similarity 
invariants. Eigenvalues are particularly important similarity invariants. 
3.2.2. Theorem. 
Similar matrices have the same eigenvalues with the 
same multiplicities because similar matrices have the same characteristic 
polynomial. 
Proof. 
Suppose that A,B € F"*" are similar so that 
B = P~!AP for some 
nonsingular P. Apply the product rule for determinants (Theorem 9.2.10 on 
page 947) to obtain the characteristic polynomial for B as 
det (B — AI) = det (P~'AP — XI) = det (P~')det (A — Al)det (P) 
= det (P~')det (P)det (A — AI) = det (I)det (A — AI) = det(A—AXI). 
Of 
Similar matrices need not have the same eigenvectors, but there is neverthe- 
less a close relationship between eigenvectors of A and those of 
B = P~!AP 
that can be derived by writing 
Ax =Ax @ P7'A(PP™')x=)P7lx © B(P7lx)=A(P7!x). 
(3.2.1) 
In other words, x is an eigenvector for A corresponding to an eigenvalue ) if 
and only if P~!x is an eigenvector for P~!1AP corresponding to 4. 
Diagonalization by Similarity Transformations 
A significant amount of theory and many applications revolve around the prob- 
lem of finding a similarity transformation that reduces a square matrix A to 
the simplest possible form. The optimal situation is to be able to reduce A to 
a diagonal matrix by a similarity transformation. 

3.2 Similarity and Diagonalization 
305 
"3, 2:3. Definition. A € 
psn is ae to be ie ne ee it is simi- : 
lar to 
a ccone matrix OE 
C5 se 
Seni: _D. for some Vile 
tas | 
be 
Nilpotent Matrices 
Not all square matrices are diagonalizable, and one outstanding class of nondi- 
agonalizable matrices are the nilpotent matrices as defined below. 
3.2.4. Definition. A : square matrix N is said to be nilpotent of index 
k when N*¥ =0 but N*-! 40 for some nonnegative integer k. 
Example 
The zero matrix is clearly nilpotent, but some other common kinds of nilpotent 
matrices are those that are strictly upper (or lower) triangular such as 
ORC 
iomeraEs 
thir 
Qe 
Pn 
hos 
No 
* 
(all diagonal entries are zero). 
(Seo) 
0 
Straightforward multiplication will confirm that N" = 0. To see that no nilpo- 
tent matrix except the zero matrix is diagonalizable, suppose that N #4 0 is such 
that N* = 0, and consider the consequence of having a nonsingular matrix P 
such that P~'NP = D is diagonal. It then would follow that 
D* =(P INP)*'=P'N*P=0 = D=0 = 
N=0, 
(3.2.3) 
which is not true. Thus nonzero nilpotent matrices are not diagonalizable. There 
are also other kinds of matrices that are not diagonalizable, but nilpotent ma- 
trices are at the core of the theory of non-diagonalizability. 
Characterizing Diagonalizability 
If not all square matrices are diagonalizable, what are characteristics of those 
that are? The following fundamental theorem provides the answer. 
3.2.5. Theorem. 
A € F"*" is diagonalizable if and only if A has a 
linearly independent set of n eigenvectors, in which case we say that A 
has a complete linearly independent set of eigenvectors. 

306 
Chapter 3 
Eigensystem Basics 
Proof. 
If A is diagonalizable, then P~'AP = D = diag (Ai, 2,--- ae for 
some nonsingular P, or equivalently, AP = PD. Equating columns across this 
equality yields AP,; = P.j;A; for each j, so {P41, Px2,--. »Pun} is a set of n 
eigenvectors. The nonsingularity of P ensures that this set is linearly indepen- 
dent (by Corollary 2.3.3 on page 160). Conversely, if {x1,X2,..-,Xn} is a set of 
linearly independent eigenvectors with Ax; = A;x,, then P = [x; |x2| +++ | Xn] 
is a nonsingular matrix such that AP = PD, where D = diag (AdsAg, <soandS 
or equivalently, P~'AP =D, and thus A is diagonalizable. 
Mf 
The following corollary emerged from the preceding proof of Theorem 3.2.5. 
yrollary. P-!AP = diag (A1, Ag,---;An) if and only if 
— {(Aj, 
Pay) |Z = 1,2,--.,n} are eigenpairs corresponding to a complete 
independent 
set of eigenvectors P.; for A. 
eae 
Since A = ie a is nilpotent, it follows from (3.2.3) on page 305 that A 
cannot be diagonalized by a similarity transformation. This non-diagonalizability 
can also be substantiated in the context of Theorem 3.2.5 by noting that A = 0 
is the only eigenvalue for A (because A is triangular), and by observing that 
there is only one independent eigenvector because 
N(A~A) = 
(A) = span{ (4). 
In other words, A does not possess a complete set of independent eigenvectors, 
so Theorem 3.2.5 ensures that A is not diagonalizable. 
Schur's Triangularization Theorem 
Since not all square matrices are diagonalizable, it is natural to wonder about 
the next best possibility—triangularization by similarity. Fortunately, this is al- 
ways possible. In fact, as Issai Schur (see page 167) realized in 1909, a similarity 
transformation U that triangularizes a given square matrix always can be con- 
structed so that U is a unitary matrix—i.e., U*U =I 
(recall Definition 1.6.4 
on page 41 and Theorem 1.10.1 on page 92). 
3.2.7, Theorem. (Schur's Triangularization Theorem) Every A ¢ F"*" 
is unitarily similar to an upper-triangular matrix. In other words, for 
each A there exists a unitary matrix U such that U*AU = T is upper 
triangular. The matrices U and T are not unique, but the diagonal 
entries of T must be the eigenvalues of A by virtue of (3.1.5) and 
Theorem 3.2.2 on page 304. 
—

3.2 Similarity and Diagonalization 
307 
Proof. 
The proof is by induction on the size of the matrix. For 1 x 1 matrices 
there is nothing to prove. Assume that all (k —1) x (k—1) matrices are uni- 
tarily similar to an upper-triangular matrix, and consider a k x k matrix A. 
Suppose that (\,x) is an eigenpair for A, and normalize x as discussed on 
page 291 so that ||x|| = 1. It was shown on page 106 how to construct a unitary 
matrix W with W,, =x. If W = (x|V), then V*x =0, so 
waw=(=)atlv)=(=)oxiav)=(3 3) =(3 5). 
Since 
A = V*AV is k— 1x k—1, the induction hypothesis insures the exis- 
tence of a unitary matrix U such ie U*AU = T is upper triangular. The 
kxk matrix G a is unitary, and since 
W 
is unitary, so is the product 
0 U 
similar to an upper-triangular matrix because 
vrau=(2 &)weaw(s 8)=<(8 2)(3 202 8) 
EONS 
Bey Nee be 
ene Mr vA Li) Sm Ome 
Therefore, by induction, all square matrices are unitarily similar to an upper- 
triangular matrix. 
U= W(4 2 ) (Exercise 1.10.2, page 107). It now follows that A is unitarily 
Note: The preceding proof is constructive, so it can be regarded as a sequential 
algorithm for triangularizing 
A. It therefore follows that the eigenvalues in 
o(A) can be made to appear in any given order on the diagonal of T by 
correspondingly ordering the eigenvectors used in the construction of T. 
Geometric Multiplicity Never Exceeds Algebraic Multiplicity 
Schur's triangularization theorem provides the theoretical basis for several results 
used in the sequel, but an immediate consequence of Schur's theorem is the fact 
that the geometric multiplicity of an eigenvalue can never exceed its algebraic 
multiplicity. 
3.2.8. Theorem. For every A € F"*" and for each \ € a (A), 
geo mult, (A) < alg mult, (A). 
(3.2.4) 

308 
Chapter 3 
Eigensystem Basics 
Proof. 
For 
4 €o(A), let g = geo mult, (A) and a= alg mult, (A). Invoke 
Schur's triangularization theorem in such a way that U*AU = 
a a | 
where T,; isan a xX a upper triangular matrix whose diagonal elements are A; 
and To. isan (n — a) x (n —a) upper triangular matrix such that A ¢ 0 (T22). 
Consequently, 
I OO 
aia eae Dad 
CSD 
in which C = T29—AlI is nonsingular—otherwise A € 0 (T22). Since 
g=n-—r, 
where r = rank (A — AI) (see (3.1.15) on page 297), it follows that 
r= rank (U*(A—AI)U) >rank(C)=n-a 
=> a>n-r=g 
Ob 
Diagonalizability in Terms of Multiplicities 
Diagonalizability, or lack of it, can be characterized in terms of multiplicities. In 
light of Theorem 3.2.8, it is somewhat intuitive that if An.» has an eigenvalue 
A such that alg mult, (A) < geo mult, (A), then there are not enough indepen- 
dent eigenvectors to fill out a complete set, and hence A cannot be similar to a 
diagonal matrix by Theorem 3.2.5. While the intuition seems clear, the formal 
details require a bit more thought. 
3.2.9. Theorem. A € F""" is diagonalizable if and only if 
alg mult, (A) = geo mult, (A) 
for each AX € o (A). 
Equivalently, A is diagonalizable if and only if each of its eigenvalues 
is semisimple. 
Proof. 
Let o (A) = {A1,X2,...,As} be the distinct eigenvalues of A, and let 
a; = alg mult, (Ai) and g; = geo mult, (Ai) for each A; € g (A). Suppose that 
a, = 9; for each i, and let ¥; = {xj1, Xj0,... ,Xig,} be a linearly independent 
set of g; eigenvectors associated with \;. Define 
k 
B, =|) 4, 
for each 
k=1,2,...,8, 
(3.2.5) 
i=1 
and observe that B, contains 
n eigenvectors for A because 
a; = gi and 
ye, a; =n. The goal is to show that B, is linearly independent by using induc- 
tion on k. For 
k = 1 there is nothing to prove. Argue that By+1 is linearly 
independent when 6, is. Consider a linear combination of vectors from Br+i 
Gk+1 
G1 
Gk 
jeu 
j=1 
7=1 

3.2 Similarity and Diagonalization 
309 
Multiplying on the left by A—A, 411 and using (A — AgyiT)xij = (Ai — Angi) 
iz 
annihilates the last sum in (3.2.6) and produces 
g1 
Ik: 
Se aij(ar - Ar+1)X1j Se Ne Ong (Ak — Agei) Xk} = 0; 
The independence of By implies that aij(A; — An41) = 0 for each 
1 <i <k 
and 1<¥7<4g,;. But the eigenvalues are distinct, so a,j =0 foreach 
1<i<k 
and 1 < j < g;, and hence (3.2.6) reduces to pee Q(k-+1)7X(e-+1)j = 0. The 
independence of +1 now implies that a(,41); =0 for each 1 <j < ge41, so 
all a;;'s in (3.2.6) are zero, and thus 6,1, is linearly independent. Therefore, 
by induction, B, is a linearly independent set of n eigenvectors for A, and 
thus A is diagonalizable by Theorem 3.2.5 on page 305. Conversely, suppose 
that P~!AP = D = diag (Aq, 2,...,An). As observed in Corollary 3.2.6 on 
page 306, {(A;,P.;)|j7 = 1,2,...,n} must be a complete set of eigenpairs for 
A, and the columns P,; can be arranged by common eigenvalue to produce 
xilae 
Aol 
P-'AP = 
ao 
BV Be 
Consequently, the first a; columns in P are independent eigenvectors for j, 
the following set of a2 columns in P are eigenvectors for 2, and so on. Thus 
there are at least a; independent eigenvectors for each 1, so g; > a; for each 
i. But Theorem 3.2.8 on page 307 says that g; <a; for each i, and therefore 
alg mult, (A) = geo mult, (A) for every 
AG a(A). 
BE 
Theorem 3.2.9 and its proof yield at least two useful corollaries. The first 
is the observation that the sets B, in (3.2.5) are linearly independent, so eigen- 
vectors corresponding to distinct eigenvalues are independent. 
3.2.10. Corollary. If {\;, A2,...,As} is a set of distinct eigenvalues for 
Anxn, and if 41,%2,...,¥, 
are corresponding linearly independent 
sets of eigenvectors, then 1; U ¥)U-::U 4 is linearly independent. 
e 
In particular, if \;,4; € a (A) are eigenvalues such that A; 4 Aj, 
and if 4; and 4; are respective linearly independent sets of eigen- 
vectors for A; and A,;, then 41; U%; is a linearly independent set. 
The second corollary stems from the fact that if all eigenvalues of A are 
distinct, then A is diagonalizable because alg mult, (A) = 1 = geo mult, (A) 
for each 
A € a (A). This is formally stated below. 

310 
Chapter 3 
nit das 
stem Basics — 
Example (A Diagonalizable Matrix) 
To determine whether or not 
' 
1 
-4 
-4 
; 
A= ( 
8 —11 -s) 
(3.2.7) 
—8 
8 
5 
is diagonalizable, check the validity of either Theorem 3.2.5 on page 305 or The- 
orem 3.2.9 on page 308. The characteristic equation is found by using Theorem 
3.1.6 on page 294 to be 
AP 6A + BN 2 Oe (X 1) (AE 37 0, 
so Aj = 1 isasimple eigenvalue, and Az = —3 is repeated twice. In other words, 
so alg mult, (Ai) = 1 and alg mult, (Az) = 2. Eigenvectors for each of these 
eigenvalues are determined by solving the homogeneous systems (A — 1I)x = 0 
and (A + 3I)x =0 
to obtain the eigenspaces 
tants spin{( BJ} and wats 
= 
aon{(2), (8). 
Each of these spanning sets is a linearly independent set of eigenvectors for A, 
so geo mult, (Ai) =1 and geo mult, (Az) = 2. Consequently, A must be diag- 
onalizable. To explicitly exhibit the similarity transformation that diagonalizes 
A, set 
Lee 
1 
0 
0 
Pe ( 
24 0). and verify 
P~'AP = (0 —3 
0) = 
bP 
=P) 
TN 
al 
0 
0 
-3 
Example (A Nondiagonalizable Matrix) 
ue 
ea 
Pg 
On the other hand, the triangular matrix T = (0 1 1) 
is not diagonalizable 
Om On 
because o (T) = {1} with alg multy (1) =3 (see (3.1.5) on page 292), and 
1 
N(T-1I)= span} (0) = > 1= geo multp (1) < alg mult, (1) = 3. 
In other words, T fails to have three independent eigenvectors, so it is impossible 
to build a nonsingular matrix P33 whose columns are eigenvectors for T, and 
thus T cannot be diagonalized by a similarity transformation. 
Block Diagonalization with a Real Similarity Transformation 
If a real matrix A = QDQ™! is diagonalizable, then some of the entries in Q, 
Q-!, and D may have to be complex when A has some complex eigenvalues. 
However, all of the matrices involved can be taken to be real by settling for a 
block-diagonal result with either 2 x 2 blocks or scalar entries on the diagonal. 

Proof. 
Let {pr,,Pr.---Pr,} be the set of distinct real eigenvalues and let the 
distinct complex eigenvalues be {X¢,, Ac, Aco; Acas +++» Acys Ac, $- Diagonalizabil- 
ity ensures that for each eigenvalue its algebraic and geometric multiplicity 
agree, so if R,, is a linearly independent spanning set for N(A — p,,1), then 
R = Ui_,Rr; 
is aset of r linearly independent real eigenvectors {r1,r2,...,17} 
corresponding to real eigenvalues (see Corollary 3.2.10 on page 309). Similarly, if 
C., is a linearly independent spanning set for N(A —.,1), then C = U*_,Co, 
is a set of t linearly independent eigenvectors {x,,x2,...,xz} for the 2's. It 
is straightforward to see that this implies C = {X1,X2,...,Xz} is also linearly 
independent (Exercise 1.3.6, page 20) and that these vectors are eigenvectors 
corresponding to the A's. Consequently, Corollary 3.2.10 again yields that 
R uC uC = {ri,Ye, tee slipg Kd) Ry & 272, soe eke 
is a complete linearly independent set of n eigenvectors for A. Write each 
complex eigenvalue and eigenvector as 
Apo 05 +P; Ap Sy —i8;, 
and 
x; = Uy +ivz, X= uz—iv;, (3.2.9) 
where a,,@; 
€R and uj,v; € R"'. It follows from (2.4.8) on page 186 that 
Arie roe 
Pye iy Vig Ua V2, 0° 
0 Uk, Vet 
is linearly independent, and hence the matrix 
P = [R|ui|viluel|ve|---|ue|ve], 
where 
R= [ri|r2|---|r-] 
(3.2.10) 

a 
Lan 
7 
7 
4, 
where B; = Cs. 8;), and thus (3.2.8) is 
established. The characteristic 
equation for B; is p;(x) = x? —2a;x+ (a? +83), so the eigenvalues of B; are 
o (Bj) =a; £18; = {A;, Aj}. & 
The Cayley—Hamilton Theorem 
: 
For a given polynomial g(r) = ag +}a;2+a2z?+---+a,r* and a square matrix 
A, the corresponding matrix polynomial is defined to be 
q(A) = aol +a, A + agA? +---+a,A*. 
A remarkable fact that was discovered by William Hamilton' and Arthur Cayley 
states that every square matrix satisfies its own characteristic equation. 
The nonsingularity of P can also be established by repeatedly invoking the formula for the 
determinant of a sum that is given in Exercise 9.2.15 on page 954 to show that det (P) 40. 
William Rowan Hamilton (1805-1865), an Irish mathematical astronomer, established the re 
sult in 1853 for his quaternions, matrices of the form 
be a) that resulted from 
his attempt to generalize complex numbers. In 1858 Arthur Cayley (see page 116) enunciated 
the general result, but his argument was simply to make direct computations for 2x2 and 
3x3 matrices. Cayley apparently didn't appreciate the subtleties of the result because he 
stated that a formal proof "was not necessary." Hamilton's quaternions took shape in his mind 
while walking with his wife along the Royal Canal in Dublin, and he was so inspired that he 
stopped to carve his idea in the stone of the Brougham Bridge. He believed quaternions would 
revolutionize mathematical physics, and he spent the rest of his life working on them. But the 
world did not agree. Hamilton became an unhappy man addicted to alcohol who is reported 
to have died from complications associated with a severe attack of gout. 

3.2 Similarity and Diagonalization 
313 
Proof. 
Schur's triangularization theorem on page 306 insures the existence of a 
unitary matrix U such that U*AU = T is upper triangular, and, as pointed 
out in the note following the proof of Schur's theorem, the eigenvalues A can be 
made to appear in any order on the diagonal of T. So, if ¢ (A) = {\y,)2,..., As} 
with A; repeated a; times, then there is a unitary U such that 
Ty 
* 
ata 
* 
ri 
* 
nh 
* 
ERGY = oo" 
* 
ri 
mera 
* 
DAU == 
. 
|, 
where T; = 
: 
T. 
Xi 
ai Xa; 
For each i, the matrix T; — ,I is an a; x a; strictly upper-triangular matrix 
having zeros everywhere on the diagonal, so, as discussed on page 305, it is 
nilpotent in the sense that (T; —.,1I)** = 0. Consequently, (T —),I)** has the 
form 
Sa 
Bis te 
"Sea 
atexey 
fe 
(T-A,D% = 
0 + 
* | — ¢* row of blocks. 
k 
This means that (T — \,I)"(T — A2gI)® ---(T—A,1)% = 0. The characteristic 
equation (in factored form) for A is 0 = (x—A1)% (a@—A2)" +++ (a@—As)* = p(z), 
sO 
U*p(A)U = U*(A — A111 (A — Ag)? «(A — A, DU 
= (T — 1D" (T — dAeI)™ --- (T—A,D™ = 0, 
and thus p(A)=0. 
& 
Example (A~! is a Polynomial in A ) 
Using only first principles, without the Cayley—Hamilton theorem, it is difficult 
to establish the somewhat remarkable fact that for every nonsingular A € F"*" 
there is a polynomial g(x) such that 
' 
A = @(A). 
(3.2.12) 
But with the aid of Cayley-Hamilton theorem this is straightforward because if 
p(x) = 2 +2") +++++en-10 +p =0 is the characteristic equation for A, 
then p(A) = A" + GA) 
iene Ae. T= 0, so multiplication by Ane 
yields 
—1 
Ave 
Ata. + +¢,_jl+c,A7'=0 => Asie (Are: +... eat) 
Thus g(x) = —c71(2""-1+cy2""-?2 ++-+++c,_1) is such that g(A) = A7?. 

314 
Chapter 3 
Eigensystem Basics 
Example (Toeplitz Matrices) 
Matrices that have constant entries on each diagonal parallel to the main diagonal 
are called Toeplitz matrices' For example, a 4 x 4 Toeplitz matrix T along with 
a tridiagonal Toeplitz matrix A are shown below: 
to 
ti 
te 
t3 
to 
ty 
0 
0 
oe 
eee 
ey 
ty 
to 
res 
424 
ot 
ti 
0 
T= 
(Erie 
Tesi 
ais, 
ti]? 
ae 
0 
ta} 
"tO 
ty 
bao 
eee 
es) 
EEO 
0 
0 
t_1 
to 
Toeplitz structures occur naturally in a variety of applications, and Toeplitz ma- 
trices are often the result of discretizing differential equation problems—e.g., see 
the discussion of two-point boundary value problems on page 220. The Toeplitz 
structure is rich in special properties, but tridiagonal Toeplitz matrices are par- 
ticularly nice because they are among the few nontrivial structures that admit 
formulas for their eigenvalues and eigenvectors. For example, the eigenvalues and 
eigenvectors of 
A= 
ee 
x 
with 
a#0c 
are given by 
(c/a)1/? sin (1j/(n + 1)) 
(c/a)?/? sin (2jm/(n + 1)) 
) and x; = | (€/a)/?sin(8jx/(n+1)) | (3.2.13) 
Aj = 6+ 2av/c/a cos ( 
: 1 
n+1 
(c/a)"/? sin (nj/(n + 1)) 
for 7 =1,2,...,n. In particular, this says that the eigenvalues are each distinct 
(i.e., simple), so A is diagonalizable by Corollary 3.2.11 (page 310). Deriving 
the facts in (3.2.13) is nontrivial and requires a bit of heavy lifting. For the 
adventurous reader, here is the derivation. 
Derivation of (3.2.13). 
Let (A,x) be aneigenpair for A. For each k =1,...,n 
the k*' component in (A —AI)x =0 
is crp_1 + (b—A)ty + arK41 =0 with 
Lo = Ln41 = 0 or equivalently, 
c 
mat (2) rn+i+(<) z,=0 
for k=0,...,n—1 with % =2p11 =0. 
Otto Toeplitz (1881-1940) was a professor in Bonn, Germany, but because of his Jewish back- 
ground he was dismissed from his chair by the Nazis in 1933. In addition to the matrix that 
bears his name, Toeplitz is known for his general theory of infinite-dimensional spaces devel- 
oped in the 1930s. 

3.2 Similarity and Diagonalization 
315 
These are second-order homogeneous difference equations, and solving them is 
similar to solving analogous differential equations. The technique is to seek solu- 
tions of the form x, =r" for constants € and r. This produces the quadratic 
equation r? +(b—\)r/a+c/a=0 
with roots r; and rg, and it can be argued 
that the general solution of x%42 + ((b — A)/a)arn41 + (c/a)az = 0 is 
k 
ice 
4 
Lh = ee it i 
- : 
is Bah rs where q@ and £ are arbitrary constants. 
For the eigenvalue problem at hand, r; and ry must be distinct—otherwise 
rp = ap*+ Bkp*, and xo = XLn+1 = 0 implies each x, = 0, which is impossible 
because x is an eigenvector. Hence x; = ark + 'Cre, And "io = Ly41 = 0 yields 
0=a+f 
6 | Oa. 
we! 
i2nj/(n+1) 
fac ed a (aS a ar 
so ry = rge!?74/(n+1) for some 1 <j <n. Couple this with 
(eA) 
he 
= = (r—ri)(r 
12) ei Hare ce 
re 
to conclude that ry = /c/ae™I/("+)) 
rg = \/cfaei79/("4)) | and 
A= b+ar/c/a (cinileet) rv enie-siie+d)) = 2a,/c/a ae (4) ; 
Therefore, the eigenvalues of A must be given by 
Aj = 6+ 2a1/c/a cos (=) 
for each 7 = 152,...,7.- 
Since these A; 's are all distinct (cos@ is a strictly decreasing function of 6 on 
(0,7), and 
a#0#c), A must be diagonalizable by Corollary 3.2.11 (page 
310). Finally, the k*? component of any eigenvector associated with Aj satisfies 
rz, =ark 
+ Brk with a+ 
8 =0, so 
ae Brice? me eae erly) = 
(£) "sin 
gk 
a 
a 
n+1 
Setting a = 1/2i yields a particular eigenvector associated with A; as 
(c/a)!/2 sin (1jx/(n + 1)) 
(c/a)?/? sin (2j7/(n + 1)) 
xj =| 
(€/a)® sin (BFn/(n +1) 
(c/a)"/? sin ene + 1)) 
Because the 2; 's are distinct, {x1,X2,...,Xn} 
is a complete linearly indepen- 
dent set, so P = (x; |x2|--+|xXn) diagonalizes A. 

316 
Chapter 3 
Eigensystem Basics 
Example (Deflation) 
It's often the case that a right-hand and left-hand eigenvector for some eigenvalue 
is known. Rather than starting from scratch to find additional eigenpairs, the 
known information can be used to reduce or "deflate" the problem into one of 
smaller size. Suppose that respective right-hand and left-hand eigenvectors x 
and y for an eigenvalue \ of A € F"*" are already known. Furthermore, 
suppose that y'x 4 0 (such eigenvectors are guaranteed to exist when A is 
diagonalizable—see Exercise 3.2.26 on page 320). Normalize y so that |ly||, = 1, 
and then rescale x by setting x = x/y?x so that now y'x = 1. Construct any 
unitary matrix U = [y|X] that has y as its first column. This ensures that 
0 =y*X = y'X. One way to do this is by building a reflector as described on 
page 106. Set P = [x|X]. Straightforward multiplication together with y7X = 
0 and X*X =I 
shows that 
Ba ies 
he 
pndeR 
WA Pes titles |e peihere 1 = XT AN 
mae 
ey) 
wd 
Wi Ral = 
A 
is (n—1) x (n—1). The eigenvalues of B constitute the remaining eigenvalues 
of A (Exercise 3.1.9), and thus an nxn eigenvalue problem is deflated to 
become one of size (n — 1) x (n—1). 
Note: When A is real and symmetric, all eigenvalues are real, and a right-hand 
eigenvector is also a left-hand eigenvector. In this case take x = y to be an 
eigenvector with ||x||, = 1. The unitary matrix U can be taken to be the real 
reflector U = R = I- 2uu?/u7u with u = x — e; (see Definition 1.10.5 
along with (1.10.27) on page 105). Then 
P 
= 
U=R, sco P-!' = R-!=R, 
and hence P~'AP = RAR = & 's in which B = B?. Thus an nxn 
symmetric problem is deflated into an (n —1) x (n—1) problem that is also 
symmetric. 
Exercises for section 3.2 
3.2.1, Determine which of the following matrices is diagonalizable by a simi- 
larity transformation. Hint. Recall Exercise 3.1.1. 
tf 
2 
36 
8 
3-2 
3h 
Ae 
An 
ae 
B= ( 
A 
«Sa 
:) c=(0 
ti 
( iA 
11) 
Sf Re ae 28 

3.2 Similarity and Diagonalization 
317 
3.2.2. 
3.2.3. 
3.2.4. 
3.2.5. 
3.2.6. 
3.2.7. 
3.2.8. 
3.2.9. 
3.2.10. 
Seoelds 
Suppose that A,B € F"*" have exactly the same characteristic equa- 
tion. Can one be diagonalizable while the other is nondiagonalizable? 
Explain why. 
Use Schur's triangularization theorem (page 306) to show that complex 
eigenvalues of real matrices occur in conjugate pairs. Hint. First show 
that if A is unitarily similar to T, then T is unitarily similar to T. 
Diagonalize A = ee ng with a similarity transformation, or else 
explain why A can't be diagonalized. 
Show that similar matrices need not have the same eigenvectors by 
giving an example of two matrices that are similar but have different 
eigenspaces. 
(a) 
Verify that alg mult, (A) = geo mult, (A) for each eigenvalue of 
A 
=3" 
=—3 
A= ( 
0 -1 
0). 
6 
6 
5 
(b) 
Find a nonsingular P such that P~!AP is a diagonal matrix. 
Let A € F"*". Prove that if x*Ax =0 
forall 
xe C"*!, then 
A=O. 
Show that having x? Ax = 0 for all x € R™' is not enough to ensure 
that A =O, even if A is real. 
1 
—4 
-4 
Verify the Cayley-Hamilton theorem for A = ( 
8 
-—ll 
-8 iy 
Hint: This is the matrix in (3.2.7) on page 310. 
Explain why the following "proof" of the Cayley-Hamilton theorem is 
not valid. p(A) = det (A — AI) = > p(A) = det (A — AI) = det (0) = 0. 
3 
A 
ik 
\ = 2 is an eigenvalue for A = ( 
Coe 0). Find alg mult, (A) as 
—-2 
-3 
0 
well as geo mult, (A), and conclude something about the diagonaliz- 
ability of A. 
For A € F"*", let r = rank(A) and t be the number of nonzero 
eigenvalues. Explain why t <r. 

ute ct 
wale 
tulied Gsciia a finde att. 
| 
fhe (0 
natheeBprarionds 
FI 
ine 
— 
C= 
: «£2 
: 
Bo 
0 
£0 «en 
= 20] 
oo 
OO 
1 —Qn-1/ 
nxn 
3.2.14. If 
B= P~-!AP, explain why B* = P-!A*P. 
3.2.15. Centrosymmetric Matrices. 
A matrix A € R"™*" is called centrosym- 
metric (alternately, cross-symmetric) when it is symmetric about its 
midpoint. The stiffness matrix for two identical springs given in (1.2.9) 
on page 12 is an example. Also, the grid in a properly formed cross- 
word puzzle is centrosymmetric. Centrosymmetry is characterized by 
saying that aj; = @(n41-i),(n+1—j), OF More conveniently by saying that 
1 
A=JAJ, where J is the reversal matric J = ( 
Z 
) 
1 
(a) 
(b) 
(c) 
Explain why A = JAJ is equivalent to aj; = @(n+1-1),(n+1—3)> 
and then show that if A, B € R"*" are centrosymmetric, then 
so are A', A~! (when A is nonsingular), and AB. 
Let A be skew-centrosymmetric. Explain why A € ao (A) if 
and only if -\€ a (A). 
Ti 
In 
T2 
n-1 
Let 
x= | . | and 
x= 
. 
|. Show that if (A,x) is an 
Be 
"vs 
eigenpair for a centrosymmetric matrix A, then (\,X) is also 
an eigenpair for A. 
Let (A,x) be an eigenpair for a centrosymmetric matrix A, 
where geo mult, (A) = 1. Explain why x is either symmetric 
or skew-symmetric about its midpoint. 
3.2.16. Compute lim,_,., A" for A= eal aaa 
Leaya 

3.2 Similarity and Diagonalization 
319 
3.2.17. 
3.2.18. 
3.2.19. 
3.2.20. 
3.2.21. 
3.2.22. 
Cry 
go 3 
The eigenvalues of A = 
ia : = ri 
are {1,2,1+%,1—i}. Find 
—2 09 
-6 
a nonsingular matrix P that is real and P~!'AP = G ai where 
D = diag(1,2) and B is areal 2 x 2 matrix with o(B) =1+i. Hint: 
With the given information everything can be computed using only real 
arithmetic. 
Apply the technique used to prove Schur's triangularization theorem on 
page 306 to construct an orthogonal matrix P such that 
P7 AP 
is 
13-9 
16 
—11 
upper triangular for A = ( 
Prove that every matrix in F"*" is arbitrarily close (using the Frobenius 
matrix norm) to a diagonalizable matrix. 
Note: This is sometimes expressed by saying that the set of diagonal- 
izable matrices is a dense subset of F"*". In loose terms it means that 
if you throw a random dart at F"*", then you will hit a diagonalizable 
matrix with probability one. 
The spectral radius of Anxn is p(A) = max e,:a) |A| (see page 297). 
Prove that if A is diagonalizable, then 
p(A) = jim JAP 
for every matrix norm. 
Note: This result can be extended to nondiagonalizable matrices—see 
Exercise 3.2.21 and Theorem 4.11.5 on page 621. 
Use Schur's triangularization theorem along with the fact that matrix 
norms are continuous (Theorem 1.9.7, page 88) to extend the result of 
Exercise 3.2.20 to nondiagonalizable matrices. That is, prove 
p(A) = lim AP 
for every matrix norm and every A € F"*". 
k— oo 
Hint: Consider how you solved Exercise 3.2.19. 
The finite difference matrix Any» = 
eae 
rs 
was in- 
a =e? 
troduced in the example on page 221. Explain why the eigenvalues of A 
are given by 
Aj = Asin? 
FOL gpl, 
2 hon Ms 
jn 
2(n + 1) 

320 
Chapter 3 
Eigensystem Basics 
6 
4 
3.2.23. Let N be the nilpotent matrix N= 
: 
0 
nmxn 
(a) Show that \ € o¢(N+N7) if and only if iA co (N— N'). 
(b) Explain why N + N7 is singular if and only if n is odd. 
det (N — N7) 
A 
= 
ves oat n/2- 
(c) Prove that if n is even, then det(N4NT) 
(—1) 
a 
ee ge | 
3.2.24. Each row sum and column sum in A = 
: : : 
is 4, so e (the 
et gl 
ee 
vector of all 1's) is both a right-hand and left-hand eigenvector associ- 
ated with 
\ = 4 € a (A). Use the deflation technique described in the 
example on page 316 to determine the remaining eigenvalues of A. 
3.2.25. Let {xi,X2,...,Xn} be a set of linearly independent eigenvectors for 
Anxn associated with respective eigenvalues {A1,A2,..-,An}, and let 
Prxn = (x1|-+:|Xn). Prove that if 
yt 
P-'=| 
: 
|, 
where each y? is a row vector, 
fi 
Yn 
then {y1,y2,---,¥n} is aset of linearly independent left-hand eigenvec- 
tors respectively associated with {\1,A2,...,An} (ie, y7 A = Ay? ). 
3.2.26. 
(a) Show that if A € F""*" is diagonalizable, and if \ € o(A), 
then there are respective right-hand and left-hand eigenvectors 
x and y associated with A such that y'x 40. (Theorem 4.7.8 
on page 550 contains a related result.) 
(b) 
Show that not every right-hand and left-hand eigenvector x and 
y associated with A € 0(A) must satisfy y'x 4 0. 
(c) Show that (a) need not be true when A is not diagonalizable. 
3.2.27. Prove that A =c,xid{,,, #0 is diagonalizable if and only if d?c 
0. 
3.2.28. Prove that A = iy a is diagonalizable if and only if W,,, and 
Zix+ are each diagonalizable. 

3.2 Similarity and Diagonalization 
321 
3.2.29. 
3.2.30. 
3.2.31. 
3.2.32. 
3.2.33. 
3.2.34. 
Eigenvalues of Kronecker Products. Let the respective eigenvalues of 
Amxm and Brxn be denoted by \; and f4j, and prove each of the 
following statements. 
(a) The eigenvalues of A@B are the mn numbers {\j;}/%, jet 
(b) The eigenvalues of (In 
@A)+(B@In) are {Ai + wy} 
21 j21- 
Hint: Consider Schur's triangularization theorem. 
Simultaneous Triangularization. Prove that if A and B are square ma- 
trices such that AB = BA, then A and B can be triangularized by 
the same unitary similarity transformation. Hint: Use Exercise 3.1.28 
on page 302 along with the development of Schur's triangularization 
theorem. 
For A,B ¢€ F"*" such that AB = BA, let 
C = A+B, and let 
{¥1;2;+++;Y%n} be the eigenvalues of C. Explain why there is some 
ordering of the eigenvalues a; of A and the eigenvalues 8; of B such 
that Vi = OG + Bi. 
For A,B € F"*", prove that AB and BA have the same eigenvalues 
by showing that ary a is similar to ae Art Note: The fact 
that o (AB) = 0 (BA) was also the point of Exercise 3.1.27 on page 
301 but by means of a different technique. 
Simultaneous Diagonalization. For diagonalizable matrices A,B € F"*", 
prove that A and B can be diagonalized by the same similarity trans- 
formation if and only if AB = BA. 
Brauer's Rank-One Update Formula. In 1952 Alfred Brauer (page 766) 
discovered how to alter a single eigenvalue by means of a rank-one modifi- 
cation. Suppose that {\j,A2,...,An} are the eigenvalues for A € F"*", 
and let (Ai, c) be a particular eigenpair. 
(a) Prove that if d, 1 is arbitrary, then the eigenvalues of A+cd* 
are given by 
{(Aq aE dsc), NOMAGs 
ced pepe 
In other words, the eigenvalues of A + cd* agree with those 
of A except that A; is replaced by A; + d*c. (A somewhat 
stronger result is given in Theorem 3.4.15 on page 352.) 
Hint: Consider Schur's Triangularization Theorem. 
(b) What is an eigenvector of A + cd* associated with the eigen- 
value A; + d*c? 

322 
Chapter 3 
Eigensystem Basics 
3.2.35. 
3.2.36. 
(c) How can d be selected to guarantee that the eigenvalues of 
A-+cd* and A agree except that A; is replaced by a specified 
number 1? 
Note: A more general rank-one update formula for eigenvalues and 
eigenvectors is developed in Theorem 3.4.15 on page 352. 
Higher Rank Updates. Suppose that {A1, A2,..-,An} are the eigenvalues 
for Anxn, and let the columns of C,,, be r independent eigenvec- 
tors of A corresponding to the first r eigenvalues {A1,A2,--.,Ar}. 
M1 
Let Lyx = ( Sl 
, and let D7,,, be any matrix (not nec- 
Ar 
essarily diagonal) such that L+D7C = M is diagonalizable. Prove 
that the eigenvalues of 
A+ CD? are {j11,... pr, Ar41,---;An}, where 
{111, H2,.--,r} are the eigenvalues of M. In other words, the update 
CD? affects only the first r eigenvalues. 
Eigenvectors in Terms of Determinants. Let ¢ be a simple eigenvalue for 
A € F"*", and let C = adj(A—(CI), where adj(x) is the adjugate 
matrix (see page 961). Explain why each nonzero column in C is an 
eigenvector for A associated with ¢. Similarly, each nonzero row in 
C is a left-hand eigenvector for A. In particular, if c;, is the (k,7)- 
cofactor (see page 958) of A — CI, then any nonzero columns and rows 
Ci 
C2 
T 
x = 
: 
and 
y 
(1615, 054, 45.< 9g) 
Cin 
are respective right and left-hand eigenvector for A associated with ¢. 
Hint: Consider Exercises 9.3.9 and 9.3.12 on page 967. 

3.3 Functions of Diagonalizable Matrices 
323 
3.3 FUNCTIONS OF DIAGONALIZABLE MATRICES 
For A € F"*", how are terms such as sinA, e", or InA defined? A naive 
first reaction might be to apply the given function to each entry of A such as 
in ae ol 
us tea ae 
) 
(3.3.1) 
a2, 
a92 
sin a2, 
Sin a9 
But doing so results in matrix functions that fail to have the same properties as 
their scalar counterparts. For example, since sin? «+cos? x = 1 for all scalars z, 
it is desirable to have definitions of sin A and cosA to result in the analogous 
matrix identity sin? A+cos? A =I 
for all matrices A. The entrywise approach 
in (3.3.1) clearly fails in this regard. 
One way to define matrix functions possessing properties consistent with 
their scalar counterparts is to use infinite series expansions. For example, consider 
the exponential function 
yee 
PoP 
ee 
e Oa 
easy ane 
(3.3.2) 
k=0 
Formally replacing the scalar argument 
z by a square matrix A (2° = 1 is 
replaced with A° =T) results in the infinite series of matrices 
A? 
Ae 
oa 
AM ss ea ep 
(3333) 
called the matrix exponential. While this produces a matrix that has properties 
analogous to its scalar counterpart, it suffers from the fact that convergence must 
be dealt with, and then there is the problem of describing the entries in the limit. 
Diagonalization solves these problems. 
If A is diagonalizable, then A = PDP~! = Pdiag(A1,...,An) P-!, and 
A= PD? Pate? diag( dy). 2,5). P= ei a0 
In other words, the infinite series (3.3.3) is not needed to define e*. Instead, 
define eP = diag (e*!, e*?, ..., e*"), and set 
e* = PePP-! = Pdiag (ce, e%?,...,e°") P™*. 
(3.3.4) 
As illustrated in (3.3.8) below, this idea generalizes to any function f(z) that 
is defined on the eigenvalues of A by setting 
f(A) = Pf(D)P! = Pdiag (f(A1), f(A2), ---, f(An)) P- 
(3.3.5) 

BV dams 
wit 0 
emeee mom zt Caro 4. 7, 
Zon page 550), mad adopt tae 
ca 
folloningty 
4c aA 
1B 
cc 
nition. 
j 
td 
Power Series Expansions 
Exactly same reasoning that connected the series expansion (3.3.2) for e* and 
the matrix series (3.3.3) to the similarity representation for e" in (3.3.4) can be 
applied to any function f(x) that is defined on o (A) and has a power series 
expansion. In particular, if 
co 
en 
Ss Cn(z— zo)" 
converges when |z — zo| <r, 
(3.3.7) 
770) 
.
and if |A; — zo| <r for each eigenvalue A; of a diagonalizable matrix A, then 
the same logic leading to (3.3.4) can be used to show that 
FALE) 
SOP 
Sees 
O 
a 
Omid 
Ste ES teemistO 
S> en(A — 201)" = f(A) =P 
en 
RAN 
sar 
EG 
82) 
n=0 
; 
3 
: 
0 
OOP 
FARE 

3.3 Functions of Diagonalizable Matrices 
325 
Subsequent developments show that the matrix series on the left-hand side of 
(3.3.8) converges if and only if |\; — zo| <r for each \;, regardless of whether 
or not A is diagonalizable, so if f(x) in (3.3.7) is defined on ao (A), then 
co 
f(A) = >> en(A — I)" 
(3.3.9) 
n=0 
serves to define f(A) when A is not diagonalizable—see Theorem 4.10.4 on 
page 601. 
Example (A Matrix Exponential) 
Let 
Ne ie Be with 
a+6<0, 
(3.3.10) 
and let t € F. To determine the matrix exponential e4*, compute eigenpairs 
(A1,X1) and (A2,xX2), and then set P = [x1 | x2] to compute 
saree ide sal 
Dee 
cea at 
The characteristic equation for A is A? + (a+8)\=0, so the eigenvalues are 
A, = 0 and Az = —(a+). Computing eigenvectors by solving (A —A,I)x; = 0 
and (A — A2I)x2 = 0 yields x; = Malia and x2 = * ). Consequently, 
Peal # (ues) andi ach CAs fex)int 
(ie woe 
ghee that 
iO abe) Potoergg (C ea)  eore) (el) 
CO 
At)* 
1 
5 ( ) 
converges to 
eAt — =al(E Drees 4] 
Example (Neumann Series Revisited) 
The function f(z) =(1—z)~1 has the geometric series expansion 
1 
co 
= ae 
if and only if |z| < 1. 

326 
Chapter 3 
Eigensystem Basics 
This means that the matrix function f(A) = (I—A)~! can be expressed as 
co 
(=A) = — A* 
if and only if |A| <1 for all 
AG a(A). 
(3.3.11) 
k=0 
This is the Newmann series from page 170, where it was proven in Theorem 
2.3.11 that for any matrix norm, 
|All<1 = ay = Soak 
(3.3.12) 
(but not conversely). Notice that (3.3.12) is a simple consequence of (3.3.11) 
because |A| < |/A|| for all 4 € o(A) and for all matrix norms (see (3.1.16) on 
page 765), so if ||A|| <1, then |A| <1 for all A € o(A), and thus (3.3.11) 
guarantees convergence of the Neumann series to (I— A)7!. 
Caution! (I— A)~! can exist without the Neumann series expansion being 
valid because all that's needed for 
I— A to be nonsingular is 1 ¢ 7 (A), while 
convergence of the Neumann series requires each |A| < 1. 
Limits of Powers of Diagonalizable Matrices 
Computing limz.., A* is a standard problem that arises in a variety of ap- 
plications. For nondiagonalizable matrices the issues are a bit involved, so they 
are deferred to page 632, but for diagonalizable matrices the situation is rather 
straightforward. 
Suppose that A € F"*" 
is diagonalizable and that the eigenvalues are 
ordered so that A = Pdiag (Aj, A2,.--,An)P7~! with |Ai| > |A2| >--- > \An|- 
Using f(x) =2* produces f(A) = A* = Pdiag (AF, A§,...,*) P-1, and this 
makes the following theorem easy to prove. 
3.3.2. Theorem. When A € F"*" js diagonalizable, lim,z_,., A" exists 
if and only if 
p(A) <1, 
in which case limp... A* = 0, 
or else 
(3.3:13) 
p(A)=1, 
and 
= 1 is the only eigenvalue on the unit cir- 
cle, in which case lim,_,,, A* = Pie eo 
where alg mult(A = 1) =r. 

3.3 Functions of Diagonalizable Matrices 
327 
Proof. 
If A = Pdiag (Ai, A2,..-,An) 
P7* with |Ai| > |Ae| > ++: > |An|, then 
limy+oo A* exists if and only if limy 5.0 A¥ exists for each i, and this happens 
only if p(A) = max; |A;| < 1. So, there are two possibilities to consider—the 
first is when p(A) <1, and the second is for 
p(A) = 1. The observation that 
p(A) <1 <= >|A,| 
<1 for all ¢ <> \¥ 5 0 for all i <= lim A®=0 
—0o 
produces the first statement in (3.3.13). When p(A) = 1 there are eigenvalues 
such that |A| = 1. But unless 
4 =1, the powers of 2 oscillate around the unit 
circle, which prevents limz_,.. A" from existing. This means that lim,z_,., A* 
exists when p(A) = 1 if and only if \ = 1 is the only eigenvalue on the unit 
circle, in which case A¥ + 0 for all \; #1, so limp soo A* = vale Be 
where alg mult(1)=r. 
Wf 
Example 
Bd 
bag G a then direct computation yields o(T) = {1, 1/4}, so 
p(T) = 1, and A = 1 is the only eigenvalue on the unit circle. Thus the 
second condition in (3.3.13) is satisfied, so limg_,.,T* must exist. Evaluate 
the limit by computing respective eigenvectors for A; = 1 and Az, = 1/4 as 
xKp= (;) and x29 = CAE and set P = |[x;|x2] = ( Ea This ensures 
that yd ene and T* SBI y Ae 
Therefore, 
; 
a 
a 
1k 
0) 
= (ee 
i 
—1 
By Teel Pepe) RP gai he 
k-co 
k—-co 
b 
Tt) 
(10 
L/(3D AS) 
ea 
sess 
1 
INO Pes ule 
se 278 
0" 
The next example presents a typical application in which limg_,., T® provides 
crucial information. 
(3.3.14) 
Example (Population Migration) 
Suppose that the population migration between two geographical regions—say, 
the North and the South—is as follows. Each year, 50% of the population in 
the North migrates to the South, while only 25% of the population in the South 
moves to the North. This situation is depicted in the transition diagram shown 
below in Figure 3.3.1. 
FIGURE 3.3.1: TRANSITION DIAGRAM 

Sar = Ma (.5) + sa (75) 
a 
that can be written in matrix form as 
(asi Se+2) = (re se) ( . mm) = Por =P T- 
@a22@) 
The row vector pf = (no so) is the initial population Gistributiom 
and the 
vectors py = (mas) and pe,, = (marr Sasr) are the respective population 
¥ 
6S 
distributions 
at the end of years & and &+1. The matrix T=: (s &) & 
called the transition matriz because it reflects the transitions depicted im Figure 
3.3.1. Starting with pg and iteratively applying (3.3.16) produces the infinite 
sequence 
Pi=poT, pr=PiT=pT. 
pf =PeT=RNT, ---. 
Nw =e. -~-. 
This shows that the powers 
of T determine how the two populations 
evel: 
The long-run (or limiting) population distribution is p< = Bmax Pe. and ik 
is computed by using (3.3.14) along with mg + so = 1 te write 
/ ee 
' son 
Ti 
a 
Pall 
YW 2 
Poo = bm, Pe 
= im pe T = pe lm T* = (rose) 'fy ae @3m) 
=(a0 = SS ae 
3 
3 
3 
3 
Therefore, if the migration patterm continues te held, them the pepulation Gs 
tribution will approach a steady state im whieh 1/3 of the pepulation & im the 
North and 2/3 of the population is im the South. 4nd tks & mniependent of fhe 
initial distribution! 

3.3 Functions of Diagonalizable Matrices 
329 
Observations. 
It's clear from (3.3.14) that the rate at which the population distribution 
stabilizes is governed by how fast (1/4)* + 0. In other words, the magnitude 
of the largest subdominant eigenvalue of T determines the rate of evolution. 
You can get a feel for this rate by looking at the first few powers of T to 
appreciate how fast T* converges. Computation to three significant places 
reveals that 
72 — 
0.375 
0.625 
73 — 
0.344 
0.656 
T= 
0.336 
0.664 
0.312 
0.687 
}' 
A028" 
GOT) 
© 
* \ 0.332. 
0.668)" 
To — 
0.334 
0.666 
To — 
0.333 
0.667 
POSEY 
OWA: 
7 \ 0.333 * 0,667 
> 
This shows that the population distribution will be essentially stable (i.e., to 
three decimal places) after six years. 
It was shown in the example on page 327 that an eigenvector corresponding 
to A; = 1 (the dominant eigenvalue of T) is x; = Gok $05 El ==4(x0) Xo) 
Te 
where x2 is an eigenvector associated with \2g = 1/4. If P~! = (%% ) 
, then 
2 
y1 is a left-hand eigenvector of T corresponding to A; = 1 (see Exercise 
3.2.25, page 320), and (3.3.14) on page 327 means that 
= Do xiv = Vie because 
pax =o, + So = I: 
This not only shows why the initial distribution is washed out in the limit, 
but it also shows that the steady-state (or limiting) distribution must be the 
left-hand eigenvector whose components are nonnegative and sum to one. 
Note. This application is a special case of the power method presented in 
Theorem 6.3.1 on page 772. 
Bauer-—Fike Perturbation Bound 
It is often important to understand how the eigenvalues of a matrix are affected 
by perturbations. This is a complicated issue in general, but for diagonalizable 
matrices the problem is more tractable. 

Proof. 
Assume that 8 ¢ o(A) so that BI — A is nonsingular (otherwise 
(3.3.18) is trivial), and observe that 
(61 — A)-1(61- B) = (61- A)-1(61- A—E) =I- (61—A)"E. 
It must be the case that ||(GI1 — A)~!E|| > 1 because if ||(@I — A)~'E|| < 1, 
then I — (G1 — A)~'!E is nonsingular by (3.3.12), which is impossible because 
(81 —B) (and hence (81 — A)~1(8I — B) is singular). Consequently, 
1 < ||(61— A)"*B|| = |P(G1- D)1P 
BI < ||P ||(61— D)- || 
|P-*|| [EI 
1 
= «(P) ||E|| max | — A;|~* = «(P) Ell ain, Bul? 
and this produces (3.3.18). 
& 
Similar to the situations for matrix inversion and solutions of linear systems 
(discussed on pages 172 and 252), the expression «(P) in (3.3.18) is a condition 
number in the sense that if K(P) is relatively small, then the \;'s are relatively 
insensitive to small perturbations, but as «(P) grows larger, then there is greater 
room for the eigenvalues to be sensitive. 
Systems of Differential Equations 
A system of differential equations was used on page 287 to motivate the intro- 
duction of eigenvalues and eigenvectors. The development has come full circle 
to the point where such systems can be more completely understood. The goal 
here is to solve a system of first-order homogeneous linear differential equations 
with constant coefficients a;; for unknown functions u; = u;(t) as shown below. 
(Throughout, (x)' denotes differentiation with respect to t.) 
uy = 01141 + aygug +--+ + QinUn, 
ui(0) = ¢1, 
Us = 0210 + Ao9tlg ++ °> + dontn, 
u2(0) =e 
with 
: 
(3.3.19) 
/ 
Un = An1U1 + Angle + +++ AnnUn, 
in 
ce. 

3.3 Functions of Diagonalizable Matrices 
331 
Since the unique solution to a single differential equation u/(t) = au(t) with 
u(0) =e 
is u(t) 
= ec, it is only natural to try to use the matrix exponential 
in an analogous way to solve a system of differential equations. Begin by writing 
(3.3.19) in matrix form as u' = Au, u(0) =c, where 
ui (t) 
Chae 
Chipy 
nC 
hips 
C1 
u2(t) 
@21 
G22 
'*: 
Gan 
c2 
Ms 
: 
} 
GAtes 
: 
A- 
he 
leh 
hand? 
c= 
Un (t) 
Qn1 
An2 
'** 
Ann 
Cn 
If A is diagonalizable with eigenvalues {\1,A2,...,An}, then 
M1 
rit 
A=P 
ae 
pi 
=> e4t=P 
a 
P 4 
43-420) 
Ne 
erAnt 
and differentiation with respect to t yields 
Ait 
deAt 
Aie 
1 
a 
P-) = Ac*! ac 
A. 
(3.3.21) 
Anerrt 
This shows that u = e"'c is one solution to u' = Au with u(0) =c. The 
following theorem establishes the fact that u = e"'c is the only solution. 
3.3.4. Theorem. 
If A ¢ EF"*" 
is diagonalizable with eigenvalues 
{X1,A2,---,An}, then the system of differential equations u' = Au 
with u(0) =c has a unique solution, and it is given by 
usec. 
(3.3.22) 
Moreover, this solution can be expressed as a linear combination 
u =e = £,e%!'x, + foe??'xn +++ + Ene 
xn 
(3.3.23) 
of a complete independent set of eigenvectors {x1,X2,...,Xn} for A, 
where €; = [P7'c]; and P = [xi |x9|---|xn]. 
Note: Nondiagonalizable systems are discussed on page 609. 
Proof of (3.3.22). 
To see that the solution u = eAte is unique, suppose that 
v(t) = v is another solution so that v' = Av with v(0) =c. Differentiating 
e Aty by using the product rule produces 
—At 
: S ¥ =e 'y'-e "'Avy=0 
=> -e "'v is constant for all 7. 

332 
Chapter 3 
Eigensystem Basics 
When t = 0, we have e~Aty| <0 = e°v(0) = Ic = c (see Exercise 3.3.3), 
and since e~4"tv is constant for all t, it follows that e Aty = c for all t. 
Multiplying on the left by e4' and using e"te~A* 
= I (see Exercise 3.3.5) 
produces v = e4tc = u. Thus the solution in (3.3.22) is unique. 
Mf 
Proof of (3.3.23). If A = Pdiag (A1, \2,---,An) P~*, then the columns in 
P = [x,|x2|---|x,] 
are a complete independent set of eigenvectors for A 
(Corollary 3.2.6, page 306), and 
erit 
eit 
&1 
u=e*c=P 
oe 
P-'c = [x1 |x2|---|Xn] 
- 
: 
ernt 
ernt 
En 
& 
: 
=I 
= fe%'x, + ne?!x9 ferret Ene"'xn, 
where 
=P 
"cc 
En 
Example (Diffusion Between Cells) 
Important issues in medicine and biology involve the question of how drugs or 
chemical compounds move from one cell to another by means of diffusion through 
cell walls. Consider two cells as depicted below in Figure 3.3.2 that are both 
devoid of a particular compound. A unit amount of the compound is injected 
into the first cell at time t = 0, and as time proceeds the compound diffuses 
according to the following assumption. 
Cell 1 
Cell 2 
FIGURE 3.3.2: DIFFUSION BETWEEN TWO CELLS 
At each moment of time the rate (amount per second) of diffusion from one cell 
to the other is proportional to the concentration (amount per unit volume) of 
the compound in the cell giving up the compound—say the rate of diffusion from 
cell 1 to cell 2 is a times the concentration in cell 1, and the rate of diffusion 
from cell 2 to cell 1 is 8 times the concentration in cell 2. Assume a, 6 =: 
Question. What is the concentration of the compound in each cell at any given 
time t, and, in the long run, what are the steady-state concentrations? 
Answer. If u;, = u,(t) denotes the concentration of the compound in cell k at 

3.3 Functions of Diagonalizable Matrices 
333 
time t, then the statements in the above assumption are translated as follows. 
du 
Sa = rate in — 
rate out = Bug—au;, 
where 
wu 4(0) = 1. 
duz 
ee rate in — 
rate out = au; — Buz, 
where 
w2(0) =0. 
In matrix notation this system is u' = Au with u(0) =c, where 
a=(73 AN w= (i), and 
Sey) 
Since A is the matrix (3.3.10) that was used in the example on page 325, the 
results from that example can be used to write the solution as 
wproensal[(s s)-reem(s HC). 
Bs 
ahs, 
QA 
-(a+B)t 
ay 
& 
—(a+B)t 
eases 
rary 
and 
walt) = 
5 (1 e 
i 
so that 
In the long run, the concentrations in each cell stabilize in the sense that 
B 
ee 
oe 
and 
jim u2(t) = ap. 
io te) = 
Example (Stability in Systems of Differential Equations) 
A large variety of physical situations can be modeled by u/ = Au, and the form 
of the solution in (3.3.23) on page 331 makes it clear that the eigenvalues and 
eigenvectors of A are intrinsic to the underlying physical phenomenon being 
investigated. One might say that the eigenvalues and eigenvectors of A act as 
its genes and chromosomes because they are the basic components that either 
dictate or govern most characteristics of A along with the physics of associated 
phenomena. 
For example, consider the long-run behavior of a physical system that is 
modeled by u'/ = Au. A standard question is, will the system eventually blow 
up, or will it settle down to some sort of stable state? Might it neither blow up 
nor settle down but rather oscillate indefinitely? These are questions concerning 
the nature of the limit 
lim u(¢) = lim eAtce = lim (1e**xy 4 oe%2*xy ++. + fe esc, |, 
t7.0o) 
t— co 
t—0o 

334 
Chapter 3 
Eigensystem Basics 
and the answers depend only on the eigenvalues. To see how, recall that for a 
complex number \ = x +iy and for a real parameter t > 0, 
At _ g(ttiv)t — erteivt — e™ (cos yt + isin yt). 
(3.3.24) 
The term e'¥ = (cosyt+isinyt) 
is a point on the unit circle that oscillates 
around the unit circle as a function of t, so |e!| = |cosyt +isinyt| = 1 and 
= le elt*| = |e**| = ett. 
le 
Consequently, if A is diagonalizable, then the stability of u' = Au with 
u(0) =c 
can be described in terms of the eigenvalues A; € 7 (A) as follows. 
e 
If Re(A;) <0 for each 7, then jim e*! = 0} and Jim u(t) = 0 for every 
initial vector c. In this case u/ = SG is said to be Sate system, and A 
is called a stable matria. 
e 
If Re(\;) >0 forsome i, then components of u(t) can become unbounded 
as t —+ oo, in which case the system u' = Au as well as the underlying 
matrix A are said to be unstable. 
e 
If Re(\;) <0 for each i, then the components of u(t) remain finite for 
all t, but some components can oscillate indefinitely if some A; = 0. This 
is called a semi-stable situation. 
The next example presents an application in which stability is important. 
Example (Simple Predator—Prey Model) 
Consider two species of which one is the predator and the other is the prey, and 
assume there are initially 100 in each population. Let uj = ui(t) and ug = uo(t) 
denote the respective population of the predator and prey species at time t, and 
suppose that their growth rates are given by u4 = ui + ue and us = —uy + U2. 
Questions. What is the size of each population at any future time? Will either 
population eventually become extinct? If so, how long will it take? 
Answers. Write the system as u' = Au, u(0) =c, where 
a=(1 2), w= (tt), ma o= (32). 
The characteristic equation for A is p(\) = \? —2\+2=0, so the eigenvalues 
for A are \; =1+i and Ag = 1~—i, and straightforward computation produces 
associated eigenvectors x, = ce ) and x2 = Gru Consequently 
aqport=(4 f(g" (2,)2(4 9, 

3.3 Functions of Diagonalizable Matrices 
335 
It follows from Theorem 3.3.4 on page 331 that the unique solution of u/ = Au 
with u(0) =c 
is 
u = ee = €,:e%"x, + f:e*?*x9, 
where Aa =P le= 50( : i) me 50( 32). 
With the aid of (3.3.24) the solution reduces to 
uz(t) 
; 
{cost +sint 
t) = 
= 
: 
me) ea Ae 
cost — sint 
The system is unstable because Re(A;) > 0 for each eigenvalue. Indeed, w(t) 
and u(t) both become unbounded as t + oo. However, a population cannot 
become negative. Once its numbers are zero, it is extinct. Figure 3.3.3 below 
shows that the graph of u2(t) (the prey) will cross the horizontal axis before 
that of ui(t) (the predators), and hence the prey becomes extinct when 
TH (A Ne 
100e* (cost — sint) = 0 => 
cost=sint 
=> 
t=. 
FIGURE 3.3.3: EVOLUTION OF PREDATOR AND PREY 
Of course, the model is only valid up to t = 7/4 because if there is no prey for 
predators to feed on, then predators numbers must eventually fall. 
Exercises for section 3.3 
3.3.1. Determine cos A for A = eee el ogi 
3.3.2. Explain why sin? A + cos? A =I for a diagonalizable matrix A. 
3.3.3. Explain e®? =I for every square zero matrix. 
3.3.4. Explain why every square matrix A commutes with its exponential em 

336 
Chapter 3 
Eigensystem Basics 
3.3.5. 
3.3.6. 
3.3.7. 
3.3.8. 
3.3.9. 
3.3.10. 
3.3.11. 
3.3.12. 
3.3.13. 
a 
we 
Show that e4 is nonsingular for all A € F""", and [e"| 
=e Ax 
Suppose that A ¢ F"*" is diagonalizable with the distinct eigenval- 
ues that are ordered as |Ai| > |A2| > |As| > --: = |Ax|. Prove that 
rank (limp_4o0 A*/Af) = alg mult(A1) . 
The spectral mapping property says that if f(A) exists on the eigen- 
values {\1,A2,-.-,An} of AE F"*", then {f(A1),...,f(An)} are the 
eigenvalues of f(A). 
(a) Establish this for diagonalizable matrices. 
(b) Establish this when an infinite series f(z) = 07°.) én(z — 20)" 
defines f(A) = 0°25 ¢n(A — 201)" as discussed in (3.3.8). 
Assuming that A is diagonalizable, explain why det (e") == arracetAy 
It is shown in (3.3.21) on page 331 that de"*/dt = AeAt = eAtA 
when A is diagonalizable. Explain why this also holds when A is not 
diagonalizable. 
Suppose that for nondiagonalizable matrices A € F"*" an infinite series 
f(z) = WR en(z — 20)* is used to define f(A) = 
P25 ck(A — zo1)* 
as discussed in (3.3.8). Neglecting convergence issues, explain why there 
is a polynomial p(z) of at most degree n —1 such that f(A) = p(A). 
This issue is revisited in the example on page 602. 
If f(A) exists for a diagonalizable A, explain why Af(A) = f(A)A. 
What can you say when A is not diagonalizable? 
Explain why e4+B = e4eB whenever AB = BA. Give an example 
to show that e4*+B, e4eB, and eBe* all can differ when AB 4 BA. 
Hint: The results of Exercise 3.2.33 (page 321) can be used for the 
diagonalizable case. Consider F(t) = e(A+B)t — eAteBt and F'(t) for 
the more general case. 
For 
AE F"*™ and 
Be F"*", prove that 
e(A@In)+(Im@B) _ oA g B 
where ® is the Kronecker product defined on page 227. 
Hint: What is e4®! and elm®B? 

3.3 Functions of Diagonalizable Matrices 
337 
3.3.14. Prove that if A € F"*" is skew hermitian, then e" is unitary. 
3.3.15. Matrix Equations. Consider the matrix equation AX+XB =C, where 
Ate RAS Be Brand Oe RN 
(a) 
(b) 
Prove that there is a unique solution for X if and only if 
ao (A)No(—B) =90. Hint: Recall Theorem 2.7.4 on page 230 
and Exercise 3.2.29 on page 321. 
Suppose that Re(A) <0 and Re(yu) <0 
for each 
\ € 0 (A) 
and 4 € 0(B). Prove that 
co 
X= -| eAtCeB! dt 
0 
is the unique solution Hint: Consider the differential equation 
dX/dt = AX + XB, 
X(0) = C, and use the fact that if 
Re(A) <0 for each 
\ € a (A), then lim;,.,.e4* = 0. (This 
was established for diagonalizable matrices on page 334, and it 
is generalized to nondiagonalizable matrices in Exercise 4.10.19 
on page 614.) 
3.3.16. Stability in an Electronic Circuit. A particular electronic device consists 
of a collection of switching circuits that can be either in an ON state 
or an OFF state. These electronic switches are allowed to change state 
at regular time intervals called clock cycles. Suppose that at the end of 
each clock cycle 30% of the switches currently in the OFF state change 
to ON, and 90% of those in the ON state revert to the OFF 
state. 
3.3.17. 
(a) 
(b) 
Show that the device approaches an equilibrium in the sense 
that the proportion of switches in each state eventually becomes 
constant, and determine these equilibrium proportions. 
Independent of the initial proportions, about how many clock 
cycles does it take for the device to become essentially stable? 
Determine the stability of u' = Au, u(0)=c 
for each of the following 
coefficient matrices. 
(a) a=(4 3) @ a=(5 3) © 4=(1 2) 

338 
Chapter 3 
Eigensystem Basics 
3.3.18. 
3.3.19. 
Competing Species. Consider two species that coexist in the same en- 
vironment but compete for the same resources. Suppose that the pop- 
ulation of each species increases proportionally to the number of its 
own kind but decreases proportionally to the number in the competing 
species. More precisely, assume that the population of each species in- 
creases at a rate equal to twice its existing number but decreases at a 
rate equal to the number in the other population. Suppose that there 
are initially 100 of species I and 200 of species II. 
(a) Determine the number of each species at all future times. 
(b) Determine which species is destined to become extinct, and com- 
pute the time to extinction. 
Cooperating Species. Consider two species that survive in a symbiotic 
relationship in the sense that the population of each species decreases 
at a rate equal to its existing number but increases at a rate equal to 
the existing number in the other population. 
(a) If there are initially 200 of species I and 400 of species II, deter- 
mine the number of each species at all future times. 
(b) Discuss the long-run behavior of each species. 

3.4 Normal Matrices 
339 
3.4 NORMAL MATRICES 
Example 
+ 
Theorem 3.2.5 (page 305) says that diagonalizable matrices A € F""*" are those 
that have a complete linearly independent set of eigenvectors. However, there are 
special matrices that possess a complete orthonormal set of eigenvectors. Such 
matrices are diagonalizable because orthonormal sets are necessarily linearly 
independent (Exercise 1.6.4, page 50), but in addition to being diagonalizable 
they have other important properties which merits them a special name. 
1 
-1 
1 
i 
because, as illustrated there, o (A) = {1+i,1—i}, and the respective eigenspaces 
N(A—(1+ilI) = span {({)} 
and 
N(A~[1~ilI) = span {(7)}. 
In particular, {x = (os Xoie ei is a complete orthogonal set of eigen- 
The matrix A = 
) 
used in the example on page 290 is a normal matrix 
vectors for A because 
(xi1|X2) = xjX2 = (-i 1)(7) = 0. Normalization 
produces a complete orthonormal set of eigenvectors 
(= pen vals) para) 
Caution! Not all complete sets of eigenvectors for a normal matrix are orthonor- 
mal sets—they need not even be orthogonal sets. For example, an identity matrix 
is normal, but every nonzero vector is an eigenvector, so there are infinitely many 
complete sets of independent eigenvectors that are not orthonormal. 
3.4.2. Theorem. The following two statements are equivalent to saying 
that A € F"*" is normal. 
e 
U*AU=D, where U is unitary and D is diagonal. In 
(3.4.1) 
other words, A is unitarily similar to a diagonal matrix. 
© 
A'A= AA' 
(3.4.2) 
The proof depends in large part on the following useful lemma. 
The word "normal" is not meant to suggest that a matrix is someway "conventual" or "or- 
dinary." In mathematical contexts "normal" refers to situations involving perpendicularity or 
orthogonality. 

arn 
ee ean Wa sdenticgtil ne There is 
taihted tis 
prove for Aart, 
so for n = 2 look at ey (1, 1)-entry on each side of T*T = TT* to see that 
TT = naan 0 jigs ales es aul os - pies 
\ti2 
tre 
to2 
toa} 
\i 
=> [tral = [tal + [e2l? => 
tie =0. 
Now assume that the statement holds for all k x k triangular matrices, and 
consider a (k +1) x (k +1) upper triangular matrix T = ey a in which 
ro 4 
T, is an upper k x k triangular matrix and c isa k x 1 column. Since 
(TED Tie 
ere, 
"a+ 
) 
_—p 
pT 
— pg 
ac ) 
ac* 
is | 
al? 
? 
the (1,1)-blocks ensure that T{T;, = T;,T; + cc*. Taking the trace on each 
side while remembering that trace (X*X) = trace (XX*) (page 78) yields the 
conclusion that 0 = trace (cc*) = trace (c*c) = lle) . Consequently, 
c = 0, so 
Ty is triangular and T;T; = T,T;. The induction hypothesis guarantees that 
T;, is diagonal, and thus T is diagonal. 
Proof of Theorem 3.4.2. If A is normal, then by definition it has a complete 
orthonormal set of eigenvectors B = {uj,ug,...,Un}. If {A1,A2,...,An} are 
the respective associated eigenvalues, then U = [u,|u2|---|u,] is a unitary 
matrix such that U*AU = diag (Ai, A2,.--,An) = D (by Corollary 3.2.6 on 
page 306). 
Proof that (3.4.1) > (3.4.2): If U*AU = diag (A1, A2,.-.,An) = D for a 
unitary U, then 
A= UDU* and U*U =I = UU* (page 92), so 
|Ai|? 
Ps 
AAS UD tUDU" = UD? DU" = u( ral 7 
U" 
= UDD*US =UDU*UD*U =. AA. 
Proof that (3.4.2) =» Normal: By Schur's triangularization theorem (page 306), 
there is a unitary matrix U such that U*AU = T, where T is upper trian- 
gular. If A*A = AA*, then 
T'T = U*ACUU*AU = U*A*AU = UTAA Ue 
AULT AU lb 

3.4 Normal Matrices 
341 
so Lemma 3.4.3 ensures that T is diagonal, and hence A is unitarily similar to 
a diagonal matrix. Consequently, the orthonormal set of columns in U must be 
an orthonormal set of eigenvectors for A (by Corollary 3.2.6 on page 306). 
Not only do normal matrices possess a complete orthonormal set of eigen- 
vectors, their eigenspaces that correspond to distinct eigenvalues are orthogonal 
to each other. 
(3.4.4. Theoban if Ve € pen j 
is pel oh if be 'and ee 
are eigenpairs for A ak ee ai 7 Ase then oe = a In other words 
MAR Al) L(A - aD es ey a3) | 
Proof. 
If A is normal, then (3.4.2) says that A*A = AA". For an eigenpair 
(A;,x;) for A, let 
B= A—.,I so that B*B = BB*. It follows from (2.5.9) 
in Theorem 2.5.6 (page 209) that N(B) = N (B*), so x; 
© N(B) = N(B*) 
implies that 
0 = xB = xjA — \;x}. Let (A;,x;) be a different eigenpair for 
A with A; #4;. Multiply the preceding equality on the right by x; to obtain 
O = x7 AX; — AVX; 
Xj = AUX, 
Xp AX (Ay Ae eX, == 
= 
x; 1 x; 
a 
N(A — AjI) L N(A —A,I). 
| 
Spectral Theorems for Normal Matrices 
The following corollary of Theorem 3.4.2 along with its condensed version in 
Theorem 3.4.6 explicitly exhibit spectral decompositions that are indispensable 
tools for dealing with normal matrices. 
3.4.5. Corollary. A ¢ F"*" is normal if and only if 
oe 
a 
ASUDU Ul 
JU aa 
(3.4.4) 
Fo 
where {\1,A2,.--,An} are eigenvalues of A and U = [u; | ug| --- | un] 
is a unitary matrix whose columns are a set of orthonormal eigenvectors 
with Au; = A;u; for each 2. 
i 
reine 
e 
The matrices uj;u; € 
are the respective orthogonal projectors 
onto the span of uj. 
The next theorem is essentially the same result given in (3.4.4) except that 
if multiple eigenvalues exist (i.e., alg mult(A;) = ai), then they are lumped 
together to compress the expression in (3.4.4). 

Proof. 
Suppose first that A is normal. The matrix U = [U,|U2| 
--- | Uglaxn 
is unitary (by Theorem 3.4.4), and 
Arla, 
Mla, 
S\ 
Aue 
A=U 
U* = [Uy | ++ [Ud 
Arla, 
Arla, 
UX 
k 
k 
Sy UU) Ge 
i=1 
i=1 
The matrix G; = U;U}, is clearly hermitian, and the other properties in (3.4.5) 
follow from the fact that the columns in each U; as well as those in U are 
orthonormal. In particular, 
UU = Le = G7 = U,070; 
07 = UU = G;. 
k 
k 
UU* =I = 
I=) U,Uf=)>G,. 
f=) 
il 
Conversely, if A is any matrix for which there are matrices G, satisfying (3.4.5) 
and (3.4.6), then using these properties with straightforward multiplication pro- 
duces 
: 
A' A= (xe) bs Gi 
vit 
oe. 
: 
oe Se [As|G; = (>: 
6) (3-57) = AAS 
i=1 
i=1 
i=1 

3.4 Normal Matrices 
343 
which is equivalent to saying that A is normal by Theorem 3.4.2. 
Ml 
An immediate corollary of the spectral theorem for normal matrices is the 
specialization of Definition 3.3.1 for f(A) that is given on page 324. 
34 1. 
Cary tt . is oar with Fy es Boe On and - 
Jd i isa oo valued function defined on a 
7(A), ihe 
f(A) = FO) 4 f02)@2 +0 | f0n)G 
ae 
the racneee CG are 
» those defined is Thee 3.4.6. 
Proof. 
This directly follows by replacing the matrix P in (3.3.6) on page 324 
by the unitary matrix U described in the proof of Theorem 3.4.6. 
Simultaneous Diagonalization 
Given a normal matrix A, Theorem 3.4.2 ensures that it is unitarily similar 
to a diagonal matrix. Consider the possibility of extending this fact to a pair 
of normal matrices A and B by posing the question, "can A and B be 
simultaneously diagonalized by the same unitary matrix?" The answer is "no," 
unless they commute, in which case it is then possible. 
3.4.8. Theorem.' Normal matrices A,B ¢ F"*" can be simultaneously 
diagonalized by the same unitary matrix W if and only if AB = BA. 
Proof. 
Let o(A) = {Aj,A2,--.,Ax} in which alg mult(A;) = a;, and let U 
be a unitary matrix such that U*AU = D'= diag (il,,, Asla.,..., 
Ak 1a, 
) - 
Apply U as a similarity transformation to B to obtain the block matrix 
Bir 
Bio 
-:- 
Bir 
Bai 
Bozo 
-:- 
Baz 
} 
UAB 
; 
, 
where 
each B,; is a; X aj. 
Bri 
Bro 
-:: 
Brk 
It is immediate that if AB = BA, then U*AU and U*BU commute. Com- 
paring the (i, 7) -blocks on each side of 
Biz 
A1la, 
Aila, 
Bii 
Bio 
-:- 
Biz 
Bi 
B 
B 
Box 
Agla, 
A2Tas 
Boi 
Boo 
share 
Box 
Bat 
Mela; 
Bri 
Bro 
-+:: 
Bre 
Bri 
Bro 
-*: 
Br 
Arla, / 
u This theorem is the normal version of the result in Exercise 3.2.33 on page 321. 

Building 
Chapter 3 
, 
Eigensystem Basics 
shows that \;B;; = A;B;; when ij (this will be clear if you do it for k = 3), 
or equivalently (A; — \;)Bi; = 0. Since Aj # Xj; for i ¥ j, it follows that 
B,; = 0 for each i # j, and hence U*BU = diag (By1, Boo, ..., Bex)- Since 
B is normal, so is U*BU and this in turn ensures that each block B,; is 
normal, so there is a unitary matrix V; such that VB 
Vi = D,; is diagonal 
for each i. If V = diag (Vi, V2,.--, Vk), then W = UV is unitary and 
W*BW = diag (D:, Do,..., Dx) is diagonal. Furthermore, 
mata? 
Agi. 
A2la2 
Azla2 
WAW =V UAUV=V- 
; 
V= 
» 
<4 Bh 
Arla; 
Arla, 
and thus W 
is a unitary matrix that simultaneously diagonalizes both A and 
B. Conversely, if W is a unitary matrix that simultaneously diagonalizes both 
A and B, say W*AW = D,; and W*BW = Dz, then D;D2 = D2D; 
implies that AB=BA. 
UB 
3.4.9. Corollary. Hermitian matrices A,B ¢ F"*" can be simultane- 
ously diagonalized by the same unitary matrix if and only if the product 
AB is also hermitian. 
Proof. 
If AB is hermitian, then BA = B*A* = (AB)* = AB, so Theorem 
3.4.8 guarantees simultaneous diagonalization. Conversely, simultaneous diago- 
nalization implies that AB = BA, so (AB)* = B*A*=BA=AB. 
BE 
Complete Orthonormal Sets of Eigenvectors 
Let A € F"*" be normal with o (A) = {Aj, A2,...,As} and g; = geo mult (Aj). 
A complete orthonormal set B of eigenvectors for A can be obtained by building 
orthonormal spanning sets B; = {u,,ug,..., Ug, } foreach N(A—A,I) (perhaps 
by the technique used on page 207 to establish Theorem 2.5.5 or by the Gram-— 
Schmidt process discussed on page 658), and then taking 
B= B,UB2U---UBs. 
Theorem 3.4.4 guarantees that each eigenvector in B; is orthogonal to each 
eigenvector in B;, and hence B is an orthonormal set. Being normal implies 
diagonalizability, so geo mult(A;) = alg mult(A;) = a; for each i by Theorem 
3.2.9 (page 308), and thus B is a complete orthonormal set because it contains 
oie 9 = D1 4 = N Vectors. 

3.4 Normal Matrices 
345 
Frobenius Norm of a Normal Matrix 
The Frobenius matrix norm of A € F"*" is defined on page 82 to be 
Alle = [0 laal?, 
ij 
and Theorem 1.10.3 (page 93) ensures that the Frobenius norm is unitarily in- 
variant, i.e., || 
UAV||, = ||A||, when U and V are unitary. This together with 
(3.4.1) means that the norm of a normal matrix is somewhat special because if 
{A1, A2,-.-,An} is the full set of eigenvalues for A, then 
A = UDU*, where U 
is unitary and D = diag (Aj, A2,...,An), so that 
All = [UDU* 
|p = [DI- = >> Al'. 
(3.4.7) 
i=1 
Surprisingly, as the following theorem shows, the converse is also true. 
3.4.10. Theorem. A matrix A ¢ F"*" with eigenvalues oe Age Any 
is normal if and only if {|All = 0%, |Ail?- 
Proof. 
The proof that normality implies 
|| All, OE 
eA: 1s 3-4- (indo 
establish the converse, use Schur's triangularization theorem (page 306) to write 
A = UTU*, where U is unitary and T is upper triangular, and let 
T = D+N, 
where D = diag (Aj, A2,.--,An) 
is the diagonal of T and N is the strictly 
upper-triangular part of T. Use the unitary invariance of the Frobenius norm 
and the fact that D and N have no non-zeros in common positions to write 
* 
2 
2 
|All = UTU* ||} = Tp =||D+Nllz = ||DIlz + IINIz- 
The hypothesis is that || Allz = DIZ; and hence IN|, = 0, or equivalently, 
N = 0. This in turn implies that A = UDU%, and thus A must be normal by 
Theorem 3.4.2 (page 339). 
& 
Symmetric and Hermitian Matrices 
Several common types of matrices are normal. For example, it follows from 
(3.4.2) that real-symmetric and hermitian matrices, real skew-symmetric and 
skew-hermitian matrices, and orthogonal and unitary matrices are all normal. 
These classes of matrices inherit all of the "normal" properties, but it is worth 
looking a bit closer at the real-symmetric and hermitian matrices because they 
have some special eigenvalue properties of their own. 

Proof. 
The result in (3.4.8) is just a restatement of Theorem 3.1.3 on page 
292. To prove (3.4.9), suppose that A is real and symmetric. Then all of its 
eigenvalues are real, so for each \; € a (A), its eigenspace N(A — ,I) can be 
spanned by a linearly independent set of g; = geo mult(A;) = a; = alg mult (A;) 
real eigenvectors, which, by the discussion on page 344, can be converted into an 
orthonormal set B; of real vectors. Since }); 9; = }0,a; =n, the set 
B = U,;B; 
is a compete orthonormal set of real eigenvectors for A. When these are placed 
as columns in a matrix P, the result is a (real) orthogonal matrix such that 
T 
0 
2 
eee 
0 
P'AP=D=|. 
. 
. _ . 
J, where each ., is real. 
Om Oe ee 
Conversely, if P' AP =D, where P is orthogonal and D is real and diagonal, 
then A = PDP? is real and A = A'. The proof of (3.4.10) is similar to that 
of Theorem 3.1.3 on page 292, and it is Exercise 3.1.18. 
& 
Example (Cayley Transformation) 
When trying to decide what's true about matrices and what's not, it helps to 
think in terms of the following associations. 
Hermitian matrices 
«—> 
Real numbers (z = 2). 
Skew-hermitian matrices 
«—> 
Imaginary numbers (z = —2). 
Unitary matrices 
«—+ 
Points on the unit circle (z = e'®). 

3.4 Normal Matrices 
347 
For example, the complex function f(z) = (1—z)(1+z)~! maps the imaginary 
axis in the complex plane to points on the unit circle because |f(z)|? = 1 when- 
ever Z = —z, It is therefore reasonable to conjecture as Cayley did in 1846 that 
if A € F"*" is skew hermitian (or real oe symmetric), then 
f(A) =(1— A) + A)! = 
(14+ A)"1(1— A) 
(3.4.11) 
should be unitary (or orthogonal). Cayley's intuition was correct, and the func- 
tion in (3.4.11) is now called the Cayley transformation. 
To prove Cayley's conjecture, use the fact from (3.4.10) that says if A is 
skew hermitian, then A is normal and a (A) = {A\i,A2,...,A%} contains only 
pure imaginary numbers. Consequently, f(z) is defined on 
(A), and (I+A)7! 
exists. Since f(z) maps a (A) to points on the unit circle, there is some 6; for 
each A; such that f(A;) =e! =cos6; +isin@;. For the spectral components 
G,; defined in the spectral theorem (page ads Corollary te (page 343) yields 
(I— A)(I+ A)? = f(A) = S70, 
)G; = en, 
Properties of the G,'s in (3.4.6) and 050 = i %) = 1 ensure that 
k 
k 
f(A)*f(A) = (Soe G;) (lea, rs yeteG, "Ye, =I 
j=1 
g=1 
and thus f(A) 
= (I— A)(I+ A)~! is inthe on Exercise ee) on page 
336 guarantees that (I— A)(I+ A)~1 =(1+A)"1(I— A). 
Rayleigh Quotients and Eigenvalues 
Since the eigenvalues of a hermitian (or real-symmetric) matrix A € 
are 
real, they can be ordered as A, > Ag >-:: > An, and there is a convenient way 
to characterize them as explained below. 
Rex 
3.4.12. Theorem. 
If A ¢€ F"*" is hermitian (or real-symmetric) with 
eigenvalues A; > Ag >°::: 2 An, then 
'Mi = max 
x Ax 
and 
A, = 
min x Ax: 
(3.4.12) 
\[xl,=1 
\[xI|p=1 
Moreover, if {u,,U2,...,U,} is a complete orthonormal set of eigenvec- 
tors such that Au; = ,u; for each i, and if S = span {uj,...,ug—1} 
and S' = span {uz41,..-.,Un}, then (in addition to the extreme eigen- 
values) each interior eigenvalue can be expressed as 
Ap = max x [Ax = min x" Ax, 
(3.4.13) | 
yrla=t 
I] Ig=1 
where the max and min are each attained at x = ug. 
i Exercise 3.4.9 on page 356 gives alternate expressions for Az, while the two formulations of the 
Courant—Fischer theorem on pages 425 and 468 present stronger and more general variants. 

348 
Chapter 3 
Eigensystem Basics 
Proof. 
Let U = [u|ue2|-::|un] and D = diag (Ai, A2,---)An) be the re- 
spective unitary and diagonal matrices such that A = UDU", For, vectors x 
such that ||x||, =1, let y = U*x so that Yj |yil? = llyli2 = xl|g =1. Use 
this to write 
- 
x*Ax = x*UDU*x = y*Dy = ys lyil?, 
i=1 
from which it follows that for all x with ||x||, = 1, 
= Xn os 
yal? = ie 
=x"Ax= Yl <r e 
ysl? = Aj 
~ 
(3.4.14) 
=> 
}\, < min x*Ax 
and 
max x* Ax < 4. 
\|x||,=1 
I|x||p=1 
Equality holds in the above two inequalities for respective eigenvectors xX = Uy 
and x = uy, and thus (3.4.12) is proven. The expressions (3.4.13) for an interior 
eigenvalue are modifications of (3.4.14) by means of the observations 
x 
Ss anid xl = 1 US (08 Onset 
a) ey We 
gee 
x 
Sand {xfg 
1 
x" U = ie. 
Ye 0, oO) SY Wie ba yl" =Seks 
because if x 1 S and ||x||, =1, then 
nm 
n 
Nees Nk S lys|? > ye dlyi|? =x*Ax 
with equality when x = ug, 
i=k 
i=k 
and hence Ay = max jx\.=1 
x* Ax. Similarly, if x 1 S' and ||x||, =1, then 
xls 
k 
k 
Nig Ad Ne lysl? < SS Nilyi]? =x*Ax 
with equality when x = up, 
i=1 
i=1 
so Xk = min |x |]g=1 x Axe 
xLst 
The characterizations in (3.4.12) are sometimes called variational descrip- 
tions for the eigenvalues of a hermitian matrix, and the associated expressions 
often appear in the equivalent forms 
x AX 
peal 
A, = max 
— 
and 
A, = min 
=f 
(3.4.15) 
x0 
9X xX 
xO 
9X xX 
and those in (3.4.13) are equivalent to saying that 
x* Ax 
nl 
we WA 
Ay = Olax 
= min 
: 
(3.4.16) 
oo OME Nuns 
x#0 
 X*X 
xls 
aS 
Because these kinds of quotients appear throughout applied mathematics, they 
have been given a special name. 

3.4 Normal Matrices 
; 
349 
The expressions (3.4.13) and (3.4.16) for interior eigenvalues require knowl- 
edge of the sets S or S' of orthonormal eigenvectors, and this limits their 
applicability. The Courant—Fischer "min-max" theorem on page 425 presents al- 
ternative ways to use Rayleigh quotients to express the interior eigenvalues that 
can sometimes be more useful. 
Ky Fan's Maximum and Minimum Principles 
+ 
The variational descriptions of the extreme eigenvalues in (3.4.12) are obtained 
by allowing a single vector x to vary over the unit 2-sphere, so it is natural 
to wonder if these results can somehow be extended by allowing more than one 
vector to simultaneously vary over the unit sphere. For example, if two orthogonal 
vectors x; and x2 are allowed to simultaneously vary over the unit 2-sphere, 
can information concerning the first two (or last two) eigenvalues be extracted? 
Ky Fan! showed in 1949 that the answer is "ves" in the following sense. 
This terminology is in honor of the English physicist John William Strutt (1842-1919) who was 
the son of the second Baron Rayleigh of Terling Place. Strutt became the third Baron Rayleigh 
in 1873 and thereafter was known simply as Lord Rayleigh. He succeeded the famous James C. 
Maxwell as the Cavendish professor of experimental physics at Cambridge from 1879 to 1884 
and later gained international fame when he was awarded the 1904 Nobel prize in physics "for 
his investigations of the densities of the most important gases and for his discovery of argon 
in connection with these studies." 
Rayleigh was said to have been both modest and generous. He donated his Nobel prize proceeds 
to the University of Cambridge, and upon receiving the Order of Merit in 1902 he said "The 
only merit of which I personally am conscious was that of having pleased myself by my studies, 
and any results that may be due to my researches were owing to the fact that it has been a 
pleasure for me to become a physicist." 
Ky Fan (1914-2010) was born in Hangzhou, China, and moved to Beijing in 1932 to study at 
Peking University. In 1939 he went to France to become a student of Maurice Fréchet (page 
649) at the Université de Paris, where he received a D.Sc. degree in 1941. After his formal 
training Fan came to America, where he spent the bulk of his career—most: notably at the 
University of Notre Dame and the University of California at Santa Barbara. He originally 
specialized in topology and functional analysis, but his interests grew to include linear alge- 
bra, where he made significant contributions that helped to establish the subject as a modern 
mathematical discipline. Under the influence of John von Neumann and Hermann Weyl, Fan 
made notable contributions to operator and matrix theory, convex analysis and inequalities, 
and linear and nonlinear programming. Aspects of Fan's work found wide application in math- 
ematical economics and game theory, potential theory, calculus of variations, and differential 
equations. Several important inequalities now bear Ky Fan's name. 

Proof. 
Let U = [uj|ug|---|u,] be a unitary matrix such that 
U* AU =D 
= diag (1, A2,.--;An), 
or equivalently, A = UDU*. 
For Xnx-« = [x1 |xX2|--- |x] such that X*X =I,, set 
Zxxn = X*U = [21 |Z2|-+- |Zn], 
where 
2; = X*uy. 
Since X*X and XX* have the same nonzero eigenvalues (Exercise 3.1.11, page 
299), the largest eigenvalue of XX* is A; = 1. This together with (3.4.12) in 
Theorem 3.4.12 on page 347 means that 
llzs ll = ZZ; = uj XX"u; < pe x*XX*x = Ai =1 
for each yy 
= 
It now follows that 
n 
trace (X"AX) — kdy = trace (Z(D — AxI,)Z") = trace ee — Aq) 252; 
j=l 
(Aj — At) lz lI3 (because trace(z;z5) = trace(z¥z;) = 2*2; = LA) 
k 
n 
k 
(Aj 
— 
Aw) = DOG — Aw) + SO (AG — An) S SOG — Ax) 
j=l 
j=k+1 
j=1 
(S,) 
— kA, 
j=l 
I 
M> 
" Mn 

3.4 Normal Matrices 
351 
and hence trace (X*AX) < ee A; for all X,~, such that X*X =I,. Equal- 
ity is attained when X = [u; |ue|--- | ux], and thus (3.4.17) is proven. The 
proof of (3.4.18) follows from (3.4.17) and the observation that for sets of real 
numbers, min; {a1,Q2,...,@,} = — max; {—a1,—a2,...,—an_}. In particular, 
min trace(X*AX) = — max 
trace (X*(—A)X) 
X*X=I, 
X*X=I k 
=-S°(-\j)= S03. 0 
j=n—k+1 
j=n—k+1 
Eigenvalues and Eigenvectors for a Rank-One Update 
It is not uncommon to encounter a problem in which the eigen-components of 
a hermitian or real-symmetric matrix A,» are already known, but new infor- 
mation (or a perturbation) in the form of a rank-one update matrix acc* 4 0 
later becomes available. The task is to determine the eigen-components of the 
updated (or perturbed) matrix A+acc* by using the known eigen-components 
of A to avoid having to start from scratch. 
This problem reduces to one of updating a diagonal matrix because her- 
mitian matrices are normal, and they have real eigenvalues, so 
A = UDU*, 
where U is unitary and D = diag (Aj, A2,...,An) 
is real. This means that 
A+acc* = U(D + avv*)U* 
in which v = U*c. Similarity does not alter 
eigenvalues, so a (A + acc*) 
=o (D+ avv*). To develop a useful relationship 
between the eigenvalues of D (i.e., those of A) and the eigenvalues of D+avv* 
(i.e., those of A + acc*), it must be assumed that A; < Ag < --- < Ay and 
v; #0 for each 7. Under these conditions, 
AX; € o(A+ acc") for each i. 
(3.4.19) 
Otherwise, A; € ¢(D+avv*) 
for some i, so (D+ avv*)z = \;z for some 
z #0. Multiplying this on the left by the i"" unit row vector e*, coupled with 
the fact that e7Dz = A,e;*z, yields 
\we;*z = eX (D + avv")z = ef Dz + aejvv"z = \We;°2+ aviv'z 
hen. V2 = 0 av 
2 = 
0S 
Dz SA 
= 
2=— Be; 
for some 8 # 0. But 0 = v*z = Bv*e; = 80; 4 0, which is impossible, and 
thus (3.4.19) must hold. This observation leads to the following theorem. 

t 
FIGURE 3.4.1: THE SECULAR FUNCTION s(x) AND ITS ROOTS &; 
Proof. 
Let q(x) = det(A+acc*—alI) = det([A—al]+acc*) be the character- 
istic polynomial for A+ acc*, and let p(x) = det(A—zI) be the characteristic 
polynomial for A. If €; € 
¢(A+acc*), then (3.4.19) ensures that p(&) £0, 
The New Oxford American Dictionary lists one of the alternate definitions of "secular" to be 
"slow changes in the motion of the sun or planets," and in mathematics the term "secular 
equation" seems to have originated in work concerning celestial mechanics in which "secular 
variation" was related to the problem (in modern notation) of choosing x to maximize x*Ax 
subject to ||x||. = 1. Today the term "secular equation" is generally regarded as a synonym 
for "characteristic equation." 

3.4 Normal Matrices 
353 
so A — €jI is nonsingular. Therefore, (9.3.3) in Theorem 9.3.2 on page 956 can 
be used to conclude that 
0 = q(&) =det([A — &I] + acc*) = det(A — &I)(1 + ac*(A — &I)~'c) 
= p(&i)s(&i) 
—> 
O0=s(&), 
where 
s(x) =(1+ac*(A—aI)~'c). 
Conversely, if s(€;) = 0, then € ¢ 0(A) (otherwise s(&;) is not defined), so 
A — &I is nonsingular and q(€;) = p(&)s(&) = 0. Thus the eigenvalues of 
A-+acc* 
are the roots of s(x), and the first part of (3.4.20) is established. 
To derive the second equality, use (A — zI)~' = U(D — zI)~!U* along with 
c = Uv to write 
s(x) = 1+ac*(A —aI)~'c = 1+ av*U*U(D — 21) U*Uv 
aye 
=1+av*(D—-2zI)'v=1 +a5> 
.o 
Seti 
eer 
A 
The eigenvectors of A+acc* are now easily deduced from the developments 
surrounding Theorem 3.4.15. 
3.4.16. Corollary. Under the conditions of Theorem 3.4.15, 
y=(A-&l)'¢ 
is an eigenvector for A+acc* associated with the eigenvalue &;. 
Proof. 
If & 
€o0(A-+acc*), then s(€;) =0, or equivalently 
—1 = ac*(A — &1) 
1c Sac'y. 
Use this together with the identity A(A — €,I)~! -I= €;(A —&)~! (which is 
derived from (A — €;I)(A — €;I1)~! =I) to conclude that 
(A + acc*)y = Ay + acc*y = Ay —c= A(A-&])*c—c 
= [A(A -& I)? -Ijc =&(A-&I) 'c= y. 
Ol 
Example 
iD te ) is updated to become 
D + vv? = G 2) in which v = cay 
then the secular equation is 
1 
1 
(1—2)(2—2)+(2-—2)+ 
(1-2) 
of) 
Se gre NOB 
(1—z)(2—<2) 
5+ 
75 
= 
2g -5r45=0 
=> o(D+w")={ al 

apa than 
are the in 
Pisin 
i " / 
Proof. 
Let x be the conver hull x of the eigenvalues. The convex hull x 
is defined to be the set of all possible conver combinations >); a;\j;, where 
aj; > 0 is real and $);a; =1. The convex combination of two different points 
Ai, A2 € C is a point "between A; and Ag on the line segment connecting A, 
and Ag. The convex combination of three non-colinear points A,, A2, Az € C 
is a point in or on the triangle defined by the three points. The convex hull 
x is a convex polygon in C, and it is the smallest convex set that contains 
the eigenvalues of A (see Exercise 3.4.13). A conceptional interpretation is to 
think of each distinct A; as a peg on a board. When a stretched rubber band 
is placed around the pegs and released, it contracts to the convex hull x of the 
eigenvalues. This is depicted below for a set of ten distinct eigenvalues. 
CONVEX HULL x OF TEN EIGENVALUES OF A. 
Inequality (3.4.21) hinges on the fact that a;; € x for each i. To see this, start 
with 
A = UDU*, where U is unitary and D = diag (Aj, Ag,... »An). Let uj; 

3.4 Normal Matrices 
355 
be the j column of U, and use an outer product expansion as described in 
Theorem 1.8.8 on page 76 to write 
Cente [(UDU"|;; — 
So Ajujus 
= So Ag {uyusis S53 So Aj luigl?. 
J 
5 
i) 
j 
uw 
This is a convex combination because each |i 5 |? is real and nonnegative, and 
Ms \uij|? = [UU*];; =1. Therefore, aj; € y for each i. To see why this means 
that (3.4.21) must hold, let 6 be the distance between a pair of eigenvalues that 
are farthest apart from each other—i.e., 6 = max,,; |; — ;|. Convexity ensures 
that the distance between any pair of points in or on y cannot exceed 6 (look at 
the picture above). Since every a,; isin x, it follows that max;,; |a,;—a,;;| < 4, 
which is (3.4.21). 
1 
Exercises for section 3.4 
Biehl, 
lice AN = (ete ety, a normal matrix? 
3.4.2. Construct an orthogonal matrix P such that 
P' AP 
is diagonal, where 
41 
0O 
—-12 
Aas ( 
0 
25 
0) 
2) \ eta, 
08134 
3.4.3. Construct an example of a 3 x 3 matrix that is normal but not symmet- 
ric, hermitian, skew symmetric, skew hermitian, orthogonal, or unitary. 
3.4.4. Give an example of a complex square matrix A that is symmetric but 
does not have real eigenvalues. 
3.4.5. Let A e¢ F""*"" bea normal matrix. 
(a) If all eigenvalues of A are real, then must A necessarily be 
hermitian? Why? 
(b) If all eigenvalues of A are real, then must A necessarily be 
symmetric? Why? 
3.4.6. Show that A € R"*" is normal and has real eigenvalues if and only if 
A is symmetric. 
3.4.7. Let A € R"*" be symmetric with eigenvalues A; > A2 > ++: > An- 
Prove thatt Aq =)a,;, forall a= 1,2)... 2; n- 

356 
Chapter 3 
: 
Eigensystem Basics 
3.4.8. 
3.4.9. 
3.4.10. 
3.4.11. 
3.4.12. 
3.4.13. 
3.4.14. 
3.4.15. 
3.4.16. 
Prove that A € F"*" is normal if and only if trace (A*A) = )j-1 |Ail?, 
where {\i,\2,---;An} are the eigenvalues of A. 
Let A € F"*" be hermitian with eigenvalues A; > A2 >-+-: > An, and 
let {uj,Ug,...,Un} be a complete orthonormal set of eigenvectors such 
that Au; =,u; for each i. Prove the adjunct to Theorem 3.4.12 that 
says if S = span{uy,...,ug} and S' = span{ug,...,Un}, then 
Ax = min x*Ax = max x*Ax 
foreach k=1,2,...,n. 
xe 
xEes! 
ll<I]Q=1 
ll>ellg=1 
Note: The space S here is not the same as in Theorem 3.4.12. 
1 
0 
3 
Explain why A = (0 —2 0) 
is normal, and then compute the spec- 
3 
ONeL. 
tral components (\;,G;) in the spectral decomposition theorem on page 
342. Verify that your spectral components satisfy (3.4.5) and (3.4.6). 
1 
Oaxs 
Compute 
B= V/A for A= (0 —2 
0 y 
and then verify your answer 
3 
Ord 
by showing that B? = A. 
Explain why AG; = G;A = \;,G; for each spectral projector G; of a 
normal matrix A as defined in the spectral theorem on page 342. 
Explain why the convex hull x of ¢(An xn) is the smallest convex set 
that contains a (A). 
For A € F"*" 
(not necessarily hermitian) and a given v € F"*! 
(not necessarily an eigenvector for A), prove that the value of a € F 
that minimizes ||Av — av]||, is the Rayleigh quotient r = (v*Av)/v*v. 
Hint. Subtract and add 7r. 
Let A € F"*" be a normal matrix and let f(z) = ee a # 
ie: 
Show that f(A) = Al is the Moore-Penrose pseudoinverse of A given 
in Definition 2.4.18 on page 192. Is A? also normal? 
Prove that if A € F"*" is normal, then the limit 
lim A(eI + A)~? = At 
e+ 0 
exists and is the Moore—Penrose pseudoinverse of A. 

3.4 Normal Matrices 
3.4.17. 
3.4.18. 
3.4.19. 
3.4.20. 
3.4.21. 
3.4.22. 
357 
Prove that if all eigenvalues of Any» are real and exactly t of them 
are nonzero, then 
[trace (A)]? < t [trace (A?)] < rank (A) trace (A?). 
In particular, if A #0 is hermitian, then show that 
[trace (A)| Z 
SSG 
trace (A?) a 
Hint. Consider the Cauchy—Schwartz inequality. 
Construct an example to show that the converse of Theorem 3.4.4 is false. 
In other words, show that it is possible for N (A — A,I) L N (A — 4,1) 
when A; #4 Aj without A being normal. 
For a normal matrix A, explain why (A,x) is an eigenpair for A if 
and only if (A,x) is an eigenpair for A*. 
Let 
Ue F"*" bea unitary matrix. 
(a) Show that all eigenvalues of U are of the form \ = e'9, and 
then explain why every unitary matrix is unitarily similar to a 
diagonal matrix of the form D = diag(e'!, ei, ..., 
(b) Does this mean that if P € R"*" is real and orthogonal, then 
the eigenvalues of P are either 1 or —1? If you answer "yes," 
then explain why. If your answer is "no," then give an example 
of an orthogonal matrix with complex eigenvalues. 
Prove that if U € F"*" is a unitary matrix that is also upper (or lower) 
triangular, then U = D = diag (el, ele, ein) 
Prove that if P € R"*" is real and orthogonal then P is orthogonally 
similar to a real block-diagonal matrix of the form 
+1 
ae 
b= 
cos 6; sin 0, 
—sin 6; cos 6; 
cos 64 sin 6; 
—sin 6; cos 04 

358 
Chapter 3 
° 
Eigensystem Basics 
3.5 SINGULAR VALUE DECOMPOSITION (OR SVD) 
Eigenvalues and eigenvectors are defined only for square matrices, so it is nat- 
ural to ask if extensions to rectangular matrices exist. The key to dealing with 
a rectangular Am 
xn is to revert to the analysis for square matrices by think- 
ing in terms of (A*A)nxn and (AA*)mxm. Both are hermitian (or real and 
symmetric), and hence they are normal. And they have the following properties. 
e rank (A*A) =rank(AA*) =rank(A)=r 
(Theorem 2.4.20, page 195). 
e A*A and AA* have nonnegative eigenvalues (Exercise 3.1.12, page 300). 
e A*A and AA* have the same positive eigenvalues A; > Ap > -:: > A, > 0. 
— The same holds for zero eigenvalues when m = n. 
—Ifm>n, then AA* has m —n more zero eigenvalues than A*A. 
—Ifm <n, then A*A has n —m™ more zero eigenvalues than AA*. 
(See Exercises 3.1.27 and 9.3.17, pages 301 and 968.) 
These properties set the stage for the following definition. 
3.5.1. Definition. For A ¢ F"™*" with rank (A) =r, let the positive 
eigenvalues of A*A (and AA*) be A; > A >-:: > A, > 0. The 
r nonzero singular values of A are defined to be o; = VA;. When 
r<p=min{m,n}, A is said to have p—r zero singular values. 
The goal is to use the singular values of a rectangular matrix A ¢€ F™*" in 
place of the eigenvalues of a square matrix to develop a decomposition similar 
(but not identical) to that in (3.4.1) on page 339. To do so, let L be the r xr 
diagonal matrix L = diag (Ai, A2,...,Ar) containing the positive eigenvalues 
of A*A (or AA*). Theorem 3.4.2 (page 339) ensures that there are unitary 
matrices Vnxn =| Vi | V2 |] and Xmxm= [Xi | Xe ] such that 
et 
erat 
Sat 
NXT 
nx(n—r) 
mxr 
mx(m—r) 
Vi 
V*A*AV, 
V*A*AV 
Co pHa MEABAVice 
AAT Vel valle 
elie 
Vis 
V5A*AVi 
V5 A*AV2 
and 
(3.5.1) 
xj 
», Ge 
* 
* 
* 
eos ee 
AAS PRY ES 
(eee aie 
xs 
X5AA*X, 
XZAA*X2 
Looking at the lower right-hand blocks and using the fact that M*M = 0 if 
and only if M=0 (Exercise 2.4.14, page 197) yields 
V5A*AV> =) 
= 
AV> = 0, 
(3.5.2) 
X3AA*X,=0 
=> A*X,=0. 
(3.5.3) 
These observations lead to the following singular value decomposition theorem. 

Proof. 
Let Vaxn = [Vi] V2] and Xmxm = [Xs | X2,] be unitary 
oe tay 
ae uae 
: 
matrices that respectively diagonalize A*A and AA®* as in (3.5.1). If 
U = [AV,D7* | Xo], 
(3.5.5) 
then U(9 9)" = [aviD-! Ela. 
(=)- AVIV; = A. The 
} 
last equality here follows from (3.5.2) because 
A= AT= AVV* =A(ViVi+ V2V2) = AV\V}. 
To see that U is unitary, simply verify that U*U =I by using 
L = Vj A* AV, 
from (3.5.1) along with (3.5.3) and the fact that Ky has orthonormal columns 
to write 
UUE 
sae 
Cae D-!ViA*X2 
= 
D-""LD-* 
0 eee 
7 
X3AViD~1 
X2X2 
) 
i 
Ve 
Note: The simplicity of the proof of the SVD belies the importance of its role 
in both the theory and applications of linear algebra. While there are several 
important matrix decompositions, none finds its way into as many different facets 
of applied mathematics and science as does the SVD. This will become evident 
as more of the story unfolds with multiple revisitations to the SVD. 
Example 
13/5 9 2/15 
Perform a singular value decomposition of A = (=4/s —31 ; 
i) by following 
—2/5 
—58/15 
the logic in the proof of Theorem 3.5.2. The first step is to diagonalize 
1 
/369 
192 
s 
iL 
fdlh 
Gey 
le 
ATA=—( 
) and 
AA =z(« 109 
82 
ee ae 
9\14 
82 
136 
: The SVD has been independently discovered and rediscovered several times. Those credited 
with the early developments include Eugenio Beltrami (1835-1899) in 1873; M. EB. Camille 
Jordan (1838-1922) in 1875; James J. Sylvester (1814-1897) in 1889; L. Autonne in 1913; and 
C. Eckart and G. Young in 1936. 

360 
Chapter 3 
Eigensystem Basics 
with respective orthogonal matrices V and X. Computing eigenvalues reveals 
that o (ATA) = {ri = 25, A2 = 9}, o (AA') = {Ai => 25, A3 = 9,A3 = O}, sO 
r = rank (A) = 2, and the nonzero singular values for A are 01 = VM = 5 
and og) = V/A = 3. Thus the matrix D in (3.5.4) is 
D = ¢ i Compute 
eigenvectors of A7 A and AA' to be 
N (ATA 
— 
251) = span{(7)}, N(ATA-91) = span {(~3)}, 
N (AA? — 251) = spon { 
(2), N (AA* — 91) = smn {(_1)}. 
N (AA? — 01) = N (AA™) = span {(-2)\. 
Normalizing the eigenvectors produces the orthogonal matrices 
. 
yee 
tee 
yaw 
v=—(5 
3) x=2(2 
1 -2) = 
Vi=V, 
X= 
(-2 : 
5 
3 
3 
—1 
2 
2 
The unitary matrix in (3.5.5) is 
U = [AV,D~!| X2] = 4 (=2 1 2), so 
the resulting SVD is 
D 
te 
o/s Y* 3/3 
5 0 
3/5 
—4/5 
A=U|—}V"* = 
[2/3 . 1/3 —2/3 
Ons 
; 
e) 
(=2 iby 13 )(0 2) 
(22 3/5) 
Note: The computations in this example are meant to illustrate the proof of The- 
orem 3.5.2, but this is not the way that singular values are computed in practice. 
People's careers have revolved around creating practical SVD algorithms, and 
a significant degree of numerical sophistication is required before being able to 
understand and appreciate the nature of some of their methods—see page 791. 
Singular Vectors 
In a loose sense the singular values for a rectangular matrix play a role similar 
to eigenvalues for a square matrix. So, are there vectors for rectangular matrices 
that in some way resemble eigenvectors for square matrices? Well, kind of—if 
you are willing to stretch things a bit. If A = u(e oe is an SVD, and if 
v; and u;, 
1<j <r, are respective columns in V and U, then 
AV=U(9 9)—=Avs=oju; 
and 
ATU=V(2 8) => atu, =o5vj. 
For this reason the following terminology is adopted. 

7 ~ 
"ay P Pag we _ mn a 
et 
BS 
Notice that the singular vectors are also legitimate eigenvectors—not for A 
but rather for A*A and AA* because 
we 
2 
2 
Ag ACN, (igeaeegae 
1) Vi 
mvandy 
PAA steal 
pray en |Ug 
is equivalent to saying that 
A*Av;= ov; and -AA*u;=o7u; 
forl <j <r, 
3.5.7 
A*Av; =0 
and 
AA*u; =0 
100-9 >t 
( 
) 
Caution! While the right-hand and left-hand singular vectors for a rectangu- 
lar matrix A are respective orthonormal eigenvectors for A*A and AA*, the 
converse is not true! That is, respective orthonormal eigenvectors for A*A and 
AA* 
are not necessarily singular vectors for A. It is a misconception that 
simply finding any set of orthonormal eigenvectors for A*A and AA* is suffi- 
cient to construct a U and V that will produce an SVD of A, but life is not 
this easy. Taking A = I is a particularly simple example that illustrates this. 
The columns of any unitary matrix U will provide orthonormal eigenvectors for 
AA* =I, and likewise, any other unitary matrix V_ will contain orthonormal 
eigenvectors for A*A =I. All singular values are 1, so D =I, and the only 
way that UDV* can be an SVD for I (ie., UIV* =I) isif 
U=V. Thus U 
cannot be taken to be just any orthonormal set of eigenvectors of AA*. 
However, there is a duality in the following sense. If V_ is an arbitrary 
orthonormal set of eigenvectors for A*A, then U is fixed by (3.5.5), but if U 
is taken as an arbitrary orthonormal set of eigenvectors for AA*, then V is 
fixed by V = [A*U,D~'| Y2], where Y = [Yi | Yo] is a unitary matrix that 
diagonalizes A*A (see Exercise 3.5.9). 

362 
Chapter 3 
Eigensystem Basics 
Degree of Uniqueness 
The nonzero singular values of A € F™*" are uniquely defined by A because 
they are the positive square roots of the nonzero eigenvalues of A*A (and 
AA*), but what about the singular vectors—to what degree are they uniquely 
defined by A? The preceding discussion in which A =I shows that singular 
vectors are not in any way unique because if U is any unitary matrix, then 
UIU* =1 
isa 
legitimate SVD for I. In other words an SVD for a given matrix 
need not be unique. 
However, something a little more specific can be said in the special case of 
singular vectors that correspond to distinct singular values. 
3.5.4. Theorem. 
If v and v are right-hand singular vectors corre- 
sponding to the same non-repeated singular value for A ¢ F"*", then 
v = ey for some 6, and similarly for left-hand singular vectors. In 
other words, singular vectors corresponding to non-repeated singular 
values are unique up to a scalar multiple e!®. 
Proof. 
If (o,v) 
is a right-hand singular-value pair for A, then it follows 
from (3.5.6) that (o?,v) is an eigenpair for A*A. If o is not repeated, then 
alg mult,» (0?) = 1 = geo multy-, (07) (Theorem 3.2.9, page 308), so if 
(o, Vv) is any other right-hand singular-value pair, then v = av for some a € F. 
Since ||¥||, = 1, it follows that ja] = 1, and hence V = e!'v for some 6. The 
logic is similar for left-hand singular-value pairs. 
Extremal Singular Values 
If A é F""*" is hermitian with eigenvalues A; > Ag >--: > Ay, then Theorem 
3.4.12 (page 347) characterizes the largest and smallest eigenvalues of A as 
Ai When X-AX 
afi) 
Agi enh ee A 
(3.5.8) 
I[<I]2=1 
\|x||,=1 
This in turn provides expressions for extremal singular values because singular 
values of A are simply square roots of eigenvalues of A*A. In particular, if o; 
is the largest singular value for A, then the first part of (3.5.8) ensures that 
Go MAX|x\|,=1 X"A* AX = maX}jx\|,=1 || Ax||5 , or equivalently, 
01 = max ||Ax||, = ||Avill. 
(3.5.9) 
\|x||,=1 
(the maximum is attained at first right-hand singular vector Vieja TA, 
ees 
nonsingular, then its smallest singular value is 0, > 0, so the second part of 
(3.5.8) implies that o? = MiN)}x\,=1 X*A*AX = Minjx|,=1 |Axl|5, or equiva- 
lently, 
Om = 
min 
||Ax||, = ||Av,|l. 
(3.5.10) 
I[x|]2=1 
(the minimum is attained at the last right-hand singular vector Ve) 

3.5 Singular Value Decomposition (or SVD) 
363 
Matrix 2-Norm 
The characterizations (3.5.9) and (3.5.10) reveal important relationships between 
the euclidean vector 2-norm and singular values that unlock the mystery sur- 
rounding the nature of the matrix 2-norm that is induced by the standard vector 
2-norm. For A € F™*" and x € F"*!, Theorem 1.9.5 on page 85 establishes 
the fact that every vector norm induces a compatible matrix norm via 
1 
ses asa 
psc |S eseareraeres 
Bs 
when A~! exists. (3.5.11) 
The induced matrix 1-norm and oo-norm were developed on page 86, but the 
formulation of the induced matrix 2-norm had to wait until (3.5.9) and (3.5.10) 
could be developed to complete the picture. Respectively combining these ex- 
pressions with those given in (3.5.11) produces the following theorem. 
lar Theorem. 'The Le Denon eee called it the eo norm) 
that is. induced by the vector euclidean 2-norm i is [ 
lal = a= os LAxl, for A E eRrnn Co 
2 
Oe 
where oj is the largest singular value of A. 'Compatibility with the 
euclidean vector norm (i.e., ||Ax||, < ||A||, ||x||,) is guaranteed by 
Theorem 1.9.5. Moreover, if A,.» is nonsingular with singular values 
O12 092+ 2 0, 
> 0, then 
1 
Av} 
a 
a 
EE 
| 
I 
Om 
 minyjx},—1 ||Axll, 
(3.5.13) 
The figure below updates the illustration on page 86 to reflect the geometric 
interpretation of the matrix 2-norm in R® in terms of singular values. 
Cmax |JAxl,= AN = 
71 
elle 
any 
oe 
mes MA 
Tay, 
%3 
FIGURE 3.5.1: INDUCED MATRIX 2-NORM IN R?. 

364 
Example 
Chapter 3 
Eigensystem Basics 
—13/5 
—2/15 
~ 
: 
Let 
A= (- 
a "31/18 ) 
be the matrix from the example on page 359 where it 
—2/5 
—58/15 
was determined there that the largest eigenvalue of A7A is Amax(A7 A) = 25. 
Consequently, 
|Alls ot = 
Amax(AT A) = V25 =i. 
Alternately, Amax(AA?) = 25 produces the same result. 
Properties of the Matrix 2-Norm 
Needless to say, computing 
|/A||, is generally more involved than computing 
|A||,, (the largest absolute row sum), or ||A||, (the largest absolute column 
sum), or even ||A||,, (the square root of the sum of squares), but the numerous 
theoretical properties of the matrix 2-norm beyond those of a general matrix 
norm compensate for the difficulty in computing it. 
3.5.6. Theorem. For every A €EF™*", the following statements hold. 
e 
||All, =A". 
(3.5.14) 
¢ 
||A*Allo = [|All = |AA* lo. 
(3.5.15) 
© (3 8), = mex lAls IBla}- 
(3.5.16) 
© 
[PrxmAQnxqllz = l|All2 when P*P =Im, Q°Q=Iy. 
(3.5.17) 
e 
If B is any submatrix of A, then ||B||, < ||Al|,. 
(3.5.18) 
e 
|All, = es Nice 
ax, ly" 
Ax]. 
(3.5.19) 
Proof of (3.5.14). This is true because A*A and AA* have the same nonzero 
eigenvalues, so A and A* have the same singular values. 
Proof of (3.5.15). The largest eigenvalue of (A*A)*(A*A) = (A*A)? is the 
square of the largest eigenvalue of A*A 
(Exercise 3.1.15, page 300), so the 
largest singular value of A*A is the square of the largest singular value of A. 
Proof of (3.5.16). Let 
C = i ny so that' 
Ce Crs (orae te It follows 
from Exercise 3.1.9 on page 299 that o (C*C) = a (A*A) Ua (B*B), 
and thus 
||Cllp = Amax(C*C) = max{rmax(A*A), Amax(B*B)} = max{||A|3, |[B|3}. 
Proof 0f-(3:5.17). If? P* Pi 1, then 
|| PAQx||3 = x*Q* A*P*PAQx = x*Q*A* AQx = y*A*Ay, 

3.5 Singular Value Decomposition (or SVD) 
365 
where y = Qx. Now, Q*Q =I, ensures that ||x||, = 1 <=> |ly||, =1, so 
na, [PAQx|3 
= max, |Ayl3 = 
/A(3. 
Proof of (3.5.18). Let P and Q be permutation matrices (which are necessarily 
unitary) such that PAQ = ie ak Consequently, ||A||, = \(2 Hil Use 
* 
this along with 
4 
0 4 | 
= 1 to write 
2 
(0 of =M(5 o)(2 2) sll 
(5 o)MLMCE ILC 9) Ih, 
= "ale. 
Proof of (3.5.19). Applying the Cauchy—Schwarz (CBS) inequality (Theorem 
1.4.5, page 27) yields 
ly" Ax| < lvls Axl, => 
max, ly*Ax| < max, ||Ax|, = 
|All. 
Iv llg=1 
Equality is attained when x and y are respective right-hand and left-hand 
singular vectors associated with the largest singular value o,. 
& 
Distance to Lower-Rank Matrices 
There are a variety of ways to interpret the meaning of a singular value, but 
one of the most important is that of the next theorem that shows o,4+, 
is the 
2-norm distance between A,,x» with rank (A) =r and aclosest matrix 
Bm xn 
having rank (B) =k <r. 
3.5.7. Theorem. 
Let a; > 02 >::: =a, > 0 be the nonzero singular 
values of A € F™*". For each k <r, the 2-norm distance from A to 
a closest m x n matrix of rank k is 
Pee! BT 
em 
355.20 
ae I 
lz = Op41. 
( 
) 
The distance to a nearest rank-k matrix in the Frobenius norm is 
min 
||A—B\lr = [oii + Crag test o| = 
(3.5.21) 
rank(B)=k 
Truncating an SVD A= uy ee to become B = uy a 
produces a closest matrix rank-k matrix in both norms. 

366 
Chapter 3 
: 
Eigensystem Basics 
Proof 
of (3.5.20). Let A= uy Ale be an SVD, and let rank (Bmxn) =k 
Tf Unseen =| Vs |U2] and Vaxn = [ gy | V2], then BV, is mx (k+ ys: 
mx (k+1) 
nx (k+1) 
; 
Since rank (BV) < rank (B) =k, (BVi)x = 0 for some x # 0, which can be 
assumed to be normalized so that ||x||, = 1. For Dy41=diag (01, 02,.--, Ok+1)5 
Oh+1 = ee |De+i2lly < |De+1Xllp < |/UT(A — B)Vixl|, < |A — 
Bllo- 
' 
3 
3 
0 
* 
Equality in the last expression is attained for B = Ue 0)v . 
i 
Proof of (3.5.21). For any rank k matrix B, set 
C = U*BV so that 
2 
* 
~ 
OC 
JA - Bp = |U(A-B)VIz = ||(% ) 
-¢} 
= >> 
ea Cuil? a ye leée|? rf Se leag|? = ys 
lo; — cul? wy Ss \cis|?. 
i>r 
Ij 
i>r 
(3.5.22) 
Consequently, arank k matrix B that minimizes |/A — Bll must be such that 
cj =0 for 
i# J. Among rank k matrices C with zero off-diagonal entries, the 
oj 
fori <k, Thus the mini- 
one that yields a minimum in (3.5.22) has ¢; = ta for i>r. 
: 
Dy 
[0 
4re 
mum is attained at B =U( shy o)V ; and oe ,|A-B lz = se oF 
i=k+1 
Distance to Singularity 
A special case of Theorem 3.5.7 reveals how close each nonsingular matrix is to 
being singular. It follows from (3.5.20) that min;ank(B)=k<n ||A — Bll2 = on41, 
and this is smallest when k + 1=n. Thus the following corollary is produced. 
3.5.8. Corollary. If A, is nonsingular, then the 2-norm distance to a 
closest singular matrix is the smallest singular value 0, = 1/||A7+Ilo. 
Effects of Small Perturbations on Rank 
Another important consequence of Theorem 3.5.7 concerns how small pertur- 
bations to A ¢ F"*" can affect its rank. If rank(A) = r < min{m, n}, 
then intuition might suggest that for relatively small perturbations E, having 
rank (A +E) >r 
is just as plausible as rank(A +E) <r. But not true! 

cvs Se 
' 
a 
7 
- 
' 
oi con 
> 
7 
. 
3.5 Singular Value Decomposition (or SVD) 
wrcgnit 
(367 
Proof. 
Suppose to the contrary that rank(A +E) = k <r. Theorem 3.5.7 
then implies that 
El], =||A-—(A+E)|, = ee |A — Bll, = on+1 2 or, 
which contradicts ||E||,<o,. Thus rank(A+E)>r=rank(A). 
Likelihood of a Drop in Rank 
If rank (Amxn) =T < min{m, n}, then Theorem 3.5. 9 guarantees that pertur- 
bations of even the slightest magnitude can increase the rank. But what is the 
likelihood that an increase in rank will actually occur? To answer this question, 
let A=U(0 NE be an SVD of A in which D = diag (04, o2,...,0,) are 
the nonzero singular values, and suppose that ||E||, < o,. Apply U* and V 
to E to produce 
* 
= 
Ei 
Ej\2 
: 
a 
Ei 
Ej2 
* 
U*EV = es mal 
or equivalently, 
E= Ole ea ; 
where Ey, is 
rxr sothat 
A+E= a a Since ||El|. < o;,, 
it follows from (3.5.18) on page 364 that ||/Ei1||, <o,, and hence 
|[D-"En 
||, < IDO Bailly < oF" = 1. 
Consequently, 
I + D~''E,, 
is nonsingular (by Theorem 2.3.11 on page 170), 
which in turn forces D + Ej; = D(I+ D~'E,,) to be nonsingular. It now 
follows that 
rank (A +E) = ronkU( pes ae )v" = TONE ee ao 
= rank (D + Ey1) + rank (S) = rank (A) + rank (S) 
in which S is the Schur complement 
S = By. — Ey, (D+ E11) En. 
(3.5.23) 

¢ The rank of 
a randomly perturbed matrix will almost surely increase (3 5 94) 
if it is not already of maximal rank. 
Consequences for Linear Systems 
Theorem 3.5.9 and the realization in (3.5.24) have important implications for 
computing solutions of linear systems of equations Ax = b. When floating- 
point arithmetic is used (which it almost always is for real-world problems), the 
computed solution is the exact solution of a different system whose coefficient 
matrix is 
A+ E. 
~ 
Suppose first that A is nonsingular, and suppose that a stable algorithm 
is used to solve Ax = b. Recall from page 239 that being "stable" means that 
the algorithm returns the exact solution of a nearby system—i.e., the computed 
solution is the exact solution of (A + E)x = b, where the entries in E have 
relatively small magnitudes. In particular, if ||E||, <o, (the smallest singular 
value), then Theorem 3.5.9 guarantees that A +E will also be nonsingular, 
which means that a stable algorithm applied to a nonsingular system will return 
a computed solution that is the exact solution to a nearby system that is again 
nonsingular. 
On the other hand, if A is singular (or rank deficient), then perturbations 
of even the slightest magnitude can increase the rank thereby producing a system 
with fewer free variables than the original system theoretically demands, so even 
a stable algorithm can result in a loss of information or degrees of freedom. Al- 
though rounding errors are not truly random, they are random enough to make 
it highly unlikely that the Schur complement S in (3.5.23) will be zero. Con- 
sequently, if A is rank deficient, then a small perturbation E due to roundoff 
will almost certainly cause rank(A +E) > rank(A). The moral is to try to 
avoid floating-point solutions of rank-deficient systems. Such problems can often 
be distilled down to a nonsingular (or full-rank) core or to full-rank pieces, and 
these are the components that should be dealt with. 
Singular Values of a Perturbed Matrix 
Another useful consequence of Theorem 3.5.7 on page 365 is the following result 
that helps to estimate the singular values of a perturbed matrix. 
—

Proof. 
Let 
A= u( me oe be an SVD, where D, = diag (01, 02,...,0p)- 
Note that not all zero blocks are present depending on the value of p. Set 
- 
Kee Cie 0) WV", where Dy_1 = diag (01, o9,..., 04-1). The largest 
: 
singular value of 
A—Ajx_1 is o% so adding and subtracting E and then using 
the backward triangle inequality (page 84) produces 
Ok = ||A— Ag-illp = ||A+E— Ax-1— Ell 
> (A+B) — Analy 
Bll 
— 
> By—||El|, 
(by Theorem 3.5.7). 
Couple this with the observation that 
ce 
Silas alder ee 
a 
2 
< 
mi 
A+E-B 
E||2 
= 
E 
Sr 
el = 
ll2 + 
[Ello = Be + 
||Elle 
to conclude that |ox — Be| < ||El|2. 
_ Numerical Rank 
The rank of a matrix is an easy concept to grasp, and in theory its value can 
be determined in a variety of ways, one of which is by counting the number of 
nonzero singular values. But regardless of whether one counts nonzero singu- 
lar values or uses any other method, computing rank (A) using floating-point 
arithmetic is problematic (see Exercise 2.8.7 on page 245) because rank (A) is 
integer valued. In other words, rank (A) is a discontinuous function of A, so, as 
observed in Theorem 3.5.9 and statement (3.5.24), the slightest perturbation can 
(and most likely will) cause the rank to jump if it is not already maximal. This 
realization motivates the concept of numerical rank based on Theorem 3.5.10. 

370 
Chapter 3 
Eigensystem Basics 
When a stable algorithm such as that on page 791 is used with t-digit 
floating-point arithmetic to numerically compute singular values of A, it will 
return the exact singular values 6, of a nearby matrix A +E for which 
\|Blj2 +5 x 10-*||Aljo. If rank (A) =r, then p—r of the singular values of A 
are theoretically zero, where p = min{m,n}. Since Theorem 3.5.10 guarantees 
that p—r of the computed 6, 's cannot be larger than ||E]j2, it is reasonable 
to make the following definition. 
3.5.11. Definition. For A € F"*" with p= min{m,n} anda 
given SVD 
algorithm that returns the exact singular values 6; > 82 >--: 2 Bp of 
A+E, the numerical (or computed) rank of A is defined to be the 
number 7 such that 
fi 
2 ire IE > hae 
> 
This is the value that most commercial software packages return when 
rank (A) is called for. 
Distortion of the Unit Sphere 
Important aspects of a matrix A are revealed by the shape of 
A(S2) = {Ax| |x|], = 1}, 
the image of the unit 2-sphere Sj under transformation by A. The degree to 
which A distorts Sz measures the ability of A to lengthen shorter vectors or 
to shorten longer ones. The expressions in (3.5.9) and (3.5.10) show that the 
largest and smallest singular values, 0, and op, of Any» are the respective 
lengths of the longest and shortest vectors in A(S2), and the illustration in 
Figure 3.5.1 on page 363 suggests that A(S2) is an ellipsoid whose respective 
longest and shortest semi-axes have lengths 0; and o,. It can now be rigorously 
established that in general, A(S2) is in fact an ellipsoid in R", and moreover, 
the intermediate singular values of A provide the lengths of the intermediate 
semi-axes of A(S2). 
To see this suppose that A € R"*" is nonsingular (singular and rectangular 
matrices are treated in Exercise 3.5.26), and let 
A = UDV? bean SVD in which 
D contains the singular values 0, > 02 >-:: > on, > 0 of A. Rather than 
examining the shape of A(S2), first consider the shape of 
UT (A(S2)) = {w|w=U?Ax=DV"x 
for es Pee 
ee 
Since U? is an isometry (a length preserving transformation), the shape of 
A(S2) under transformation by U* is not affected—the effect is only to rotate 

3.5 Singular Value Decomposition (or SVD) 
371 
A(S2) around the origin in R" (see page 94). Observing that 
we 
we 
w? 
Seek 
2 
2 
MDW |v%elb=b=1 
5.25 
shows that U™ (A(S2)) is an ellipsoid in R" whose k*' semi-axis is o,e%, OF 
equivalently, A(S2) is the ellipsoid whose k*" semi-axis is 
U(oxnex) = onUz, 
where ug is k*" left-hand singular vector. 
Furthermore, because Av; = o,U,x for the right-hand singular vector vz = Vx, 
it follows that v,z is a point on S_ that is mapped to the k*" semi-axis vector 
on A(S2). Figure 3.5.2 below depicts this situation in R®. 
J 5U2> Avo 
aa 
0 3U3— Avs 
FIGURE 3.5.2: SEMI-AXES AS SINGULAR VECTORS 
The degree to which A distorts the unit 2-sphere is measured by the ratio of 
the largest stretch to the smallest one, and Theorem 3.5.5 (page 363) guarantees 
that this "distortion ratio" is given by 
max, Axl 
x|/o= 
1 
=a 
eee ee 
min 
||Ax||, 
73 
I|xI]2=1 
r 
Such an expression was called a condition number for A on pages 172 and 
252, but it was without regard to a specific norm. The point here is that with 
respect to the 2-norm, there is an explicit and elegant description of the "2-norm 
condition number." These observations are formally summarized below. 

- 
: a 
a. 
i 
— 
"ar 
; 
= 
> 
a 
= 
; 
= 
, 9 
; 
af 
lie Px ell etn tthe Mie Cl 
NED A 
ea TIF 
Proof. 
The facts in (3.5.26)—(3.5.28) are from the preceding discussions, and 
(3.5.29) follows from the observation that 0, = 0, if and only if D =I 
in an 
SVD A=UDV! sothat 
A= UV', which is unitary. 
Uncertainties, Errors, and Perturbations in Linear Systems 
Uncertainties in a linear system Ax = b arise in various ways. For example, 
they may emanate from modeling errors (simplifying assumptions are frequently 
required); they might be due to data collection errors (infinitely precise mea- 
surement devices do not exist); there may be data-entry errors (numbers like 
V2, 7, and 2/3 cannot be entered exactly); and finally, errors arising from 
floating-point computation are a prevalent source of uncertainty. Uncertainties 
may reside exclusively in b, or exclusively in A, or they may influence both 
A and b. A fundamental issue for practical users of linear algebra is to gauge 
the degree to which uncertainties, errors, or perturbations can affect the solution 
x. Related problems were treated earlier (pages 172, 176, and 252) with the aid 
of calculus to provide intuitive insight, but now a more complete and detailed 
picture can be developed to show how all of these issues boil down to analyzing 
SVD components. 
The simplest case is when only the right-hand side b is affected. This is 
also one of the most prevalent situations in practical work because it is often the 
case that b contains input data derived from empirical observations whereas A 
is known exactly because it is based on underlying physics defining the problem. 
And sometimes it is possible to aggregate uncertainties and shift all of them to 
the right-hand side. So, start by assuming that Ax = b is a nonsingular system 
in which only b is subject to an uncertainty e, and consider AX = b—e =b. 

3.5 Singular Value Decomposition (or SVD) 
373 
The relative uncertainty' in b is llell> / [bl]. = ||b—bll2/ ||bl],, and the relative 
uncertainty in x is ||x —x||,/||x||,. The following theorem shows how the 
relative uncertainty in x is bounded by the relative uncertainty in b. 
Notation: Normally e designates a vector of all ones, but throughout the re- 
mainder of this section the notation is changed to allow e to denote a vector 
whose components represent uncertainties or errors. 
Ix— 
xl] _ Atel, — HAMA 
Tl lela _ 
 llelle 
Ix| 
xis 
IIb, 
*TbIo 
Now combine ||x||y< ||A7*||, [Ibllz and |lel|.= || AG - Xl < Alls I —¥)ll. 
to produce 
|x = Xl]2 
llell. 
llells 
1 lel. 
lp 
~ WAT Illa ~ TAT. AT 
Me [blz 2 [blla' 
Equality Is Possible 
A matrix A is considered to be well conditioned when kz is small' relative to 1 
because in such a case Theorem 3.5.13 means that small relative uncertainties in 
b cannot greatly affect the solution, but as A becomes more ill conditioned (ie., 
as kg becomes larger), small relative uncertainties in b might produce larger 
relative uncertainties in x. To clarify this, it must be determined if equality in 
(3.5.30) can be realized for every nonsingular A. The next theorem shows that 
equality in each side of (3.5.30) is indeed possible, and the relationship between 
b and its uncertainties e that will produce equality can be made explicit. 
Knowing or estimating absolute uncertainties such as 
|le||, and 
||/x —x||) are generally not 
meaningful or helpful. For example, an absolute uncertainty of a half of an inch might be 
excellent when measuring the distance between the earth and the nearest star, but it is not 
good in the practice of eye surgery. 
See the rule of thumb on page 253 to get a feeling of what "small" and "large" might mean in 
the context of numerical analysis. 

Proof. 
Suppose first that b = Bu; and e = eu, for scalars 6 and e. Since 
Av, =o,Ux implies that vz;/o, = A~'u, for each k, it follows that 
x= A-'b=A7!(6u) = — and 
x—x=A'e=A7'(eun) = — 
1 
and thus 
Ix Xlly _ =) lel _,, lel, 
Ixln 
Non) 
1B) 
IIb 
On the other hand, if b = Su, and e = €uj, then the same argument yields 
x = Bv,/o, and x —X=€vj/01, so 
|x = Xllp (=) lel _ -illelle 
——~2 
=(-)— = 
re 
IIx. 
a1) 
|B| 
~? |Ibll, 
In other words, Theorem 3.5.14 guarantees that the worst case as well as 
the best case scenarios can each be realized for every nonsingular A, so while a 
small «2 ensures that relatively small changes in b cannot produce relatively 
large changes in x, it is a certainty that a large Ko will perpetuate a huge effect 
when b and e lie in unfortunate directions. But on the other hand, it is also 
a certainty that if b and e lie in more fortunate directions, then changes in b 
will have almost no effect on x—in fact, a large K2 can actually mitigate the 
effects of e resulting in a minimal change in x. Nevertheless, in light of the fact 
that the direction of e can rarely be known, the worst case must be guarded 
against, and increasing caution is required as Ko increases. 
In hind sight, the results in Theorems 3.5.13 and 3.5.14 could have been 
anticipated from the geometry illustrated in Figure 3.5.2 on page 371 because 

3.5 Singular Value Decomposition (or SVD) 
375 
A~!' maps the ellipsoid defined by the singular vectors back to the unit sphere. 
This means that when a vector y is in (or near) the span of uj, its length is 
reduced by the action of A~! relative to that when y is in (or near) the span 
of un. More precisely, if y; = au; and yp, = aun, then |lyi||. = lal = |lynll., 
but Avty; = ao; v1 and Aly, = ao; Vn, so 
Ap! 
| 
yill Sess 
per 
|A-ly, 
Ayal 
lp = #2 ||AW'Yn||, < ||A*ynlla- 
Continuity dictates that the same holds when y; and y,, are sufficiently close 
to the relative spans of u, and u,. 
Uncertainty in Both Sides 
Suppose that there are uncertainties in both sides of a nonsingular system 
Ax=b, and consider (A — E)x = b—e. The aim now is to establish an 
upper bound on ||x — x||, /||x/|, (the relative change in x) in terms of both 
|E||. / ||A||, (the relative change in A) and |lel|,/||bl|, (the relative change 
in b) along with the condition number k2 = 01/o,. Theorem 2.9.1 on page 
253 does this in a continuous sense by using calculus to show that if A = A(t) 
and b = b(t) are differentiable functions of a variable t in an interval [a,b] on 
which A(t) is nonsingular, and if x = x(t) is the solution to A(t)x(t) = b(#), 
then the relative size of the derivative x' = x'(t) is 
I'l, 
(bl, 
AY 
el = las s rar 
While this is mathematically elegant, it lack specificity about the acceptable 
size of uncertainties that might be tolerable. The discrete bound below is not as 
pretty as the derivative bound, but it more clearly illuminates things. 
3.5.15. Theorem. 
Let A € F"*" be nonsingular with singular values 
01 > 02 >°::dn > 0, and consider Ax 
= b and (A—E)x=b~—e. If 
|E||, <n, then A —E 
is also nonsingular, and 
IIx — Xl 
K2 
llelle . HEI 
IIXIlo 
> 1 — ke ||Ell, 
/ |All 
7 ae 
where kg = 01/0n = |All A. 

376 
Chapter 3 
' 
Eigensystem Basics 
Proof. 
The fact that A —E 
is nonsingular when \|E\|, <n is a consequence 
of Theorem 3.5.9 on page 367. For convenience, let B = A"'E, and obeere 
that 
||Bl]p < ||A7'||, |IEllp < o/on =1 so that (I- B)-? = > B" (by 
Theorem 2.3.11 on page 170). Starting with 
x—-=x-—(I-B)-1A-1(b-e) = (I- (I- B)') x+ (I-B) "Ae, 
take norms and apply the inequalities 
|All, _, ||Avell, - All A" Tle 
llelle _ 
lela 
eee 
ia. 
xis 7 edb 
"2 bl, 
to write ee 
By! 
1B)", 
xo 
/ele, 
3.5.31) 
Fee 
aa meee elt Na Sy aeons 
The identity I— (I— B)-!= —B(I— B)"! derived from I = (I— B)(I- Br 
together with the triangle inequality yields 
be 
[e-e) 
im 
1 
A. 
A 
| 
\|Bllo 
[eile 
2 IB Saapaygann es (er a oaa = Teas 
Use these in (3.5.31) together with ||B||, < |A7* ||, |E||, = K2 ||E||, / ||All, to 
conclude that 
|X = Xllo 
Ke 
llelle , Ello 
el, 
Taleb Tia 
(Ter lag) 
Note: Theorems 3.5.13 and 3.5.15 remain valid for any matrix norm for which 
\|I|| = 1 and where « is defined to be k = ||A|| Aw || (Exercise 3.5.23). 
Checking an Answer 
Suppose that x is a computed (or approximate) solution for a nonsingular sys- 
tem Ax=b, and suppose that the accuracy of x is "checked" by computing 
the residual r = b— Ax. If every component of r is exactly zero, then x must 
be the exact solution. However, if r #0 but ||r||, is zero to t significant digits, 
can we be confident that x is accurate to roughly t significant figures? No! As 
illustrated in Bob's dance with the Devil on page 249, the signs may not even be 
correct. Theorems 3.5.13 and 3.5.14 remove the mystery behind using residuals 
to check an answer. 
Rewrite r= b—Ax as Ax = b~—r, and apply Theorem 3.5.13 on page 
373 with the error term e replaced by r to conclude that 
mille, 
x= 2 © | Ill 
2 Ibi, ~ [lle ~ [bly 
(3.5.32) 

3.5 Singular Value Decomposition (or SVD) 
377 
In other words, this says that for relatively small values of K2, a computed (or 
approximate) solution X is relatively accurate if and only if ||r||, is relatively 
small. However, Theorem 3.5.14 on page 374 says that equality on the right- 
hand side of (3.5.32) is possible, so when this occurs and kK is large enough to 
overwhelm the size of a small 
||r||,, the computed (or approximate) solution 
x will necessarily be inaccurate. Thus the bottom line concerning "checking an 
answer" 
Residuals are reliable indicators a u 
is as follows. 
rae, only hen - is fied - 
ably well conditioned. As A becomes more ill conditioned, the residuals 
become inereasingly unreliable indicators of ea 
Exercises for section 3.5 
WeOeke 
3.5.2. 
3.5.3. 
3.5.4. 
3.5.5. 
3.5.6. 
Evaluate the matrix 2-norm for each matrix below. 
mies 
he 
Bh 
ay 
4-2, 
4 
AS 
ub B= [oOo ony 
"Ge 
22 
Se 
zeie 
a) 
4-2 
4 
1 
3 
—1 
EV bs Bal aah kD De 
Determine a singular value decompositions for each of the following ma- 
trices. 
") 
i (3) 
—4 
(a) A= ( : 
Explain why both of the following factorizations represent singular value 
decompositions of the same matrix. 
Determine ||A|j2 as well as ||A~*||2 for 
A = 
1/v2. 
1f/V3 
1/V6\ 
(V3 
Lue 0 v2 
V2 
0 1/V3 —2/V6 
oe 
Reb 
1 
9 (1) 
ioe d/8 
1/6 
0 
as 0 V3 
If A is a normal matrix, how are its eigenvalues related to its singular 
values? How is this different from the case in which A is hermitian? 
If \ is an eigenvalue for A € F"*", then 1+ X is an eigenvalue for 
I+ A (by Exercise 3.1.15 on page 300). Is the same true for singular 
values? That is, if o is a singular value for A, must 1+o be asingular 
value for 
I+ A? 

378 
Chapter 3 
- 
Eigensystem Basics 
3.5.7. 
3.5.8. 
3.5.9. 
3.5.10. 
3.5.11. 
3.5.12. 
3.5.13. 
Proving that rank (A*AA*) = rank (A) was Exercise 2.4.21 on page 
198. Use an SVD to establish the same result. 
Let 0; > 02 > -+:: > o, > O be the nonzero singular values of 
A ¢ F"*". Explain why the distinct eigenvalues of B = ( 
be 3) 
are o(B) = {201, +09, «..+ 
of, 0}: 
For A © F"*" with rank(A) = 7, let Umxm 
=-[ Ui | Us] 
we 
mxr 
mx(m-—r) 
be a unitary matrix whose columns are a complete orthonormal set 
of eigenvectors for AA*, and let Ynxn = [ Yi | Y2 ] be a uni- 
wr 
wow 
nmxr 
nx(n—r) 
tary matrix that diagonalizes A*A. If V = [A*U,D" | Yo], where 
D = diag (01, 02,...,0,) contains the singular values of A, show that 
A=U & , V* is a singular value decomposition of A. 
For a convergent sequence {A;}?°, of matrices, let A = limg_.. Ax. 
(a) Prove that ifeach A, is singular, then A is singular. 
(b) Ifeach A; is nonsingular, must A be nonsingular? Why? 
Let u,v € F™™*! be nonzero. Prove that the nonzero singular value of 
the rank-one matrix A = uv* is o = ||u|\, ||v||,. Note: This means 
that |/Al], = |juv" 
||, = |lulle |Ivlo- 
Explain why every A € F"*" with rank(A) =r can be expanded as 
e 
= s . 
* 
A 
O7,U;V;, 
=, 
where the o;'s are the nonzero singular values of A, and the u;'s and 
v; 's are the respective left-hand and right-hand singular vectors. 
Give an example to show that there need not be a unique closest matrix 
of rank k to a given matrix A. In other words, if o; > 02 >°::>0;, 
are the nonzero singular values of A, then for k <r there can be two 
matrices B; # Bo such that ||A — By||2 = ||A — Bille = o441. 

3.5 Singular Value Decomposition (or SVD) 
379 
3.5.14. 
3.5.15. 
3.5.16. 
3.5.17. 
3.5.18. 
SVD and the Pseudoinverse. Let A = uly nee be an SVD for 
AcF™*", where rank(A) =r and D = diag (01, 02,...,0r)- 
(a) Explain why the Moore—Penrose pseudoinverse of A € F"*" is 
At ee "Abi = Poet 
Hint: Recall Exercise 2.4.44, page 200. 
(b) Use this SVD formulation to verify that A' satisfies the four 
Penrose equations given in (2.4.16) on page 193—i.e., show that 
AA'TA=A, A'AAt= At, (AAT)* = AAT, (ATA)* = ATA. 
(c) Establish the formulas At = (A*A)'A* = A*(AA*)/, 
Pseudoinverses Lack Continuity. Theorem 2.3.12 on page 171 guarantees 
that the inverse of a nonsingular matrix varies continuously with the 
entries in A. Show that the same is not true for the pseudoinverse by 
considering A(x) = ( ah 
Let o, be the smallest nonzero singular value of Aj x. Prove that if 
le] <o2, then (A7A+¢eI)~! exists, and lim,,9(A7A+eI)~!A7 = Al. 
Prove that if rank(Amxn) =7r and oj > 02 >:+::>0, > 0 are the 
nonzero singular values, then (3.5.27) on page 372 generalizes to say 
Oo; = ymin, Axis =1/ AT]: 
x1lN(A) 
Generalized Condition Number. Let rank (Am xn) = 7 with nonzero 
singular values 01 > 02 >-+:: > 0, > 0. Extend the bounds in Theo- 
rem 3.5.13 on page 373 to include singular and rectangular matrices by 
showing that if x and x are the respective minimum 2-norm solutions 
of consistent systems Ax = b and Ax = b = b—e 
(recall Exercise 
2.4.35 on page 199), then 
_illelle - Ix-%l2 2 | lell 
m1 
Sam. ie 
(ee 
, 
Where 
ko = — 
=||All. 
Atl. 
> [bla 
Uxlla 
Ce 
al 
2||A'll, 
Does the same reasoning in the proof of Theorem 3.5.14 on page 374 
prove that these upper and lower bounds are attainable for every A? 

380 
Chapter 3 
: 
Eigensystem Basics 
3.5.19. 
3.5.20. 
3.5.21. 
3.5.22. 
Let 
A ¢ F"™*" have rank(A) =r. Prove that if A = u(e oyu 
is an SVD in which D = diag (01, o2,...,0,-), then the solution of 
minimum norm for a consistent linear system Ax = b is 
x= Se in =Atb 
rs 
cs 
hae 
Hint: Recall Exercise 2.4.35 on page 199. 
Normal Equations and SVD. For A € F'*" with rank(A) =r and 
for b € F™™!, the linear system A*Ax = A*b is called the system of 
normal equations, and such a system is always consistent—see Exercise 
2.4.34 on page 199. Prove that if 
A=U(' 9) V* isan SVD in which 
D = diag (04, 02,...,0,), then a solution of the normal equations is 
0. 
i=l 
M 
Show that if « is the two-norm condition number for A € F"*", then 
the two-norm condition number for A*A is k?. 
Note: In particular, this means that if A is ill conditioned, then the 
system of normal equations A*A = A*b is even more so, and hence 
solving the normal equations may pose numerical difficulties when using 
floating-point arithmetic. 
Norms and Singular Values. Let A ¢ F™*" with rank (A) =r. 
(a) Show that ||Al|, = (0j_, 02)? where Of 204 2°" 2G, 
0 
are the nonzero singular values of A. 
(b) Conclude that 
Alle <|lAllp S$ VnllAll., 
and note that this explains the (2,F) and the (F,2) entry in 
the matrix 
1 
Y 
ol 
beeee 
Ja |: 
(3.5.33) 
F 
from (1.9.10) on page 87 that provides the multipliers Qi, for 
the relations 
||/A||, < aj; |All; - 

3.5 Singular Value Decomposition (or SVD) 
381 
3.5.23. 
3.5.24. 
3.5.25. 
3.5.26. 
(c) Now derive the remaining entries a;; in (3.5.33) for A ¢ F"*". 
Hint: Recall the relations between the common vector norms 
from Exercise 1.5.4 on page 37, and revisit the proof of Theorem 
1.9.6 on page 86. 
Note: Other families of matrix norms are defined in terms of the singu- 
lar values. For example, using just the first k singular values produces 
bid 
the Hilbert-Schmidt norms 
||A|| = ou: 0?) . The matrix 2-norm 
(1 
4 
||A||, = 01 is the most important special case. Another variation called 
the Schatten p-norm is defined by taking ||A|| = ()°;_, oP)? for inte- 
gers p> 0. 
Verify that the results in Theorem 3.5.13 on page 373 and Theorem 
3.5.15 on page 375 hold for any matrix norm for which ||I||, = 1, where 
Kz is replaced by kK, = ||All, || 
A7*]|, . 
Ky Fan's Extension. Let 0, > 02 >-:: >o, be the singular values of 
AeéF™*"", where s = min{m,n}. Use Ky Fan's theorem on page 350 
to prove that for matrices X € F"** such that X7X =I, with k < s, 
m1 
2 
2 
: 
C—O 
xa 
d 
— 
AXA 
lean 
Dy j= 
max ||AX||p 
and 
SOF 
iain, AX lp 
Gs 
j=n—k+1 
Pivots and Conditioning. An ill-conditioned matrix might be suspected 
when a small pivot u,;; emerges during the LU factorization of A be- 
cause by Ay = 1/u,; is then large, and this opens the possibility of 
A-!=U™"!L7! having large entries. Unfortunately, this is not an ab- 
solute test, and no guarantees about conditioning can be made from the 
pivots alone. 
(a) Construct an example of a matrix that is well conditioned but 
has a small pivot. 
(b) Construct an example of a matrix that is ill conditioned but has 
no small pivots. 
Extend the discussion on page 370 concerning the distortion of the unit 
2-sphere Sj C R" to include singular and rectangular matrices by show- 
ing that if rank(Amxn) = 7 and o1 > 02 >::: 2 0, > 0 are the 
nonzero singular values of A, then the image A(S2) C R" is an ellip- 
soid (possibly degenerate) in which the k'" semi-axis is o,up = Ave, 
where uz and vx are respective left-hand and right-hand singular vec- 
tors for A. 

382 
Chapter 3 
. 
Eigensystem Basics 
3.6 
POSITIVE DEFINITE MATRICES AND QUADRATIC FORMS 
As defined on page 273, a positive definite matrix A € F"*" is real and ayer 
metric (or complex and hermitian)' such that x*Ax > 0 forall 
OAx EF'. 
Analogously, A is said to be positive semidefinite when x*Ax > 0 for all 
x € F"*!. There is a direct relationship between definiteness and eigenvalues. 
3.6.1. Theorem. Let A € F"*" be real and symmetric or complex and 
- hermitian with eigenvalues A; and singular values oj. 
_e A is positive definite if and only if A; > 0 for each 7. 
° Ais positive semidefinite if and only if A; > 0 for each 2. 
e 
In both cases, A; = 0; for each 7. 
Proof. 
Assume that A is positive definite (the semidefinite proof is the same 
but with >0 replaced by > 0) so that x*Ax > 0 for all nonzero x € F". In 
particular, if (A,x) is any eigenpair of A with ||x||, = 1, then 
AA llx<|l3 = Ax xe Axe 
0: 
Conversely, suppose that each A; > 0. Since A is normal, there is a unitary 
matrix U such that 
A = UDU%, where D = diag (Aj, A2,...,An). This is 
also an SVD for A, so \; =0;. Define D!/? = diag (Aq/", ee ioe hal y. and 
use the fact that D!/?U*x 4 0 if and only if x £0 to conclude that 
x*Ax = x*UD!/?D!/2U*x = ||D*/2U*x||; = 0 
forall x 
Oo 
@ 
Matrix Factorizations 
Definiteness can be characterized in terms of the following special factorizations. 
3.6.2. Theorem. Let A € F"*" be real and symmetric or complex and 
hermitian. 
e A is positive definite if and only if A = B*B for some nonsingular 
matrix B. 
> The Cholesky factorization R*R (on page 273) in which R 
is upper-triangular with positive diagonals is a special case. 
e A is positive semidefinite if and only if A = B*B 
for some matrix with rank (B) = rank (A). 
(3.6.1) 
A complex hermitian matrix satisfying x*Ax > 0 for all x 4 0 is often called hermitian 
positive definite, but throughout this section the word "hermitian" is suppressed and we simply 
say "positive definite." In all discussions concerning definiteness it is understood that the 
relevant matrices are either real and symmetric or complex and hermitian. 

3.6 Positive Definite Matrices and Quadratic Forms 
383 
Proof. 
If A is positive definite, then all eigenvalues \; are positive, and there 
is a unitary matrix U such that 
A = UDU* = UD!/?D!/2U* 
in which 
D!/? = diag (petal ...,\/?). Thus B = D'/2U* is a nonsingular matrix 
such that A = B*B. Conversely, if A = B*B for some nonsingular B, then 
A is positive definite because Bx 4 0 for every x 
0 so that 
x, Axes x B*Bx = ||Bx||; > Ueaior all x 2.0: 
The semidefinite case is similar. If A is positive semidefinite of rank r, then 
exactly r eigenvalues \; are positive and A = UDU* = U; a 2p)/ ; 
7 in 
which Unxn = [U1 | Us ] is unitary, D1/? = diag (\l/",...,d2/7,0,...,0), 
wr 
MXT 
nx(n—r) 
and D}/? = diag: zy! "st S047), Thus B = Dy'? j is arank r matrix such 
that A = B*B. Conversely, if A = B*B, then A is positive semidefinite 
because 
x* Ax = x*B*Bx = ||Bx!||> >0 forall x. 
Positive Definite Summary 
The characterizations of positive definite matrices given above as well as those 
on page 273 along with a few other useful ones are summarized below. 
3.6.3. Theorem. Let A ¢ F"*" be real and symmetric or complex and 
hermitian. Each of the following statements is equivalent to saying that 
A is positive definite. 
e 
x*Ax>0 forevery 
OAxEF™!. 
e 
All eigenvalues of A are positive. 
e A has an LU (and LDU) factorization where all pivots are positive. 
e 
A=B'B for some nonsingular B. 
> A = R*'R is the unique Cholesky factorization in which R 
is upper-triangular with positive diagonals. If A = LDL*, then 
the Cholesky factor is R = D!/?L* (see page 273). 
e 
Every principal submatrix of A is positive definite. 
e 
All principal minors of A are positive. 
e 
The leading principal minors of A are positive. 
Proof. 
The first four characterizations have already been proven. The last three 
are straightforward consequences—see Exercises 3.6.8-3.6.10 on page 401. 

384 
Chapter 3 
Eigensystem Basics 
Negative Definite 
A € F™" igs called negative definite (or semidefinite) matrix when A is real 
and symmetric (or complex and hermitian) and —A_ is positive definite (or 
semidefinite). This means that A is negative definite if and only if x" Ax < 0 
for all nonzero x € F™*', or equivalently, A < 0 for all 
\ € o(A). Replace 
<0 by <0 when A is negative semidefinite. If A is not definite in either the 
positive or negative sense, then A is said to be an indefinite matria. 
Semidefinite Analogs 
Theorems 3.6.1 and 3.6.2 along with Exercises 3.6.7 and 3.6.8 tend to suggest 
that statements about positive definite matrices such as those in Theorem 3.6.3 
can automatically be extended to positive semidefinite matrices simply by re- 
placing the word "positive" with "nonnegative" and >0 with > 0. While this 
is often the case, it is not universally true. To begin with, semidefinite matrices 
that are singular do not have an LU factorization, so straightforward extensions 
of the LU results for positive definite matrices do not exist. But perhaps the 
most significant case in which the analogy fails is the statement regarding lead- 
ing principal minors. Unlike the positive definite case, having only nonnegative 
leading principal minors does not ensure that A is positive semidefinite—e.g., 
consider A = ( ait}: However, having all principal minors nonnegative is a 
necessary and sufficient condition for a matrix to be positive semidefinite. 
3.6.4. Theorem. A real and symmetric or complex hermitian matrix 
A ¢F"*" is positive semidefinite if and only if all principal minors in 
A are nonnegative. 
Proof. 
Suppose that all leading principal minors in A are nonnegative, and 
let A, be the leading k x k principal submatrix in A so that det (A,) > 0. 
For all scalars «, «I+ A, is the leading k x k principal submatrix in «I+ A, 
and if the eigenvalues of A, are {/1,/42,--.,} (including repetitions), then 
{e+ 11,€+ p2,...,€+ fx} are the eigenvalues of «I+ Ay. Now let 
€ > 0, and 
use the fact that the determinant is the product of the eigenvalues to write 
det (eI + Ax) = (€ + wi)(€ + M2) ++ (e+ we) =e + sye8) + ++ 
4+ e541 + Sp, 
where s; is the j'" symmetric function of the 1; 's (page 293). It follows from 
Theorem 3.1.6 (page 294) that s; is the sum of the j xj principal minors 
in Ax, which are also principal minors in A, and hence 
8s; > 0 for each j. 
Therefore det («I+ A,) >0 
for all € > 0. In other words, each leading principal 
minor in e€I+A 
is positive, so Theorem 3.6.3 guarantees that «I+ A is positive 
definite. Consequently, x*(eI+ A)x > 0 for all 
OA x € FF"! and every «> 0. 
Let 
€ > OT (ie., through positive values) to conclude that x*Ax > 0 for 
all x. Conversely if A is positive semidefinite, then Exercise 3.6.8 establishes 

3.6 Positive Definite Matrices and Quadratic Forms 
385 
that every principal submatrix is positive semidefinite, and the determinant of 
a positive semidefinite matrix is nonnegative—see Exercise 3.6.5. 
Example (Vibrating Beads on a String) 
To get a sense of how physical phenomena give rise to positive definite matrices, 
consider n small beads, each having mass m, spaced at equal intervals of length 
L on a very tightly stretched string or wire under a tension T as depicted in 
Figure 3.6.1. Each bead is initially displaced from its equilibrium position by a 
small vertical distance—say bead k is displaced by an amount 
cy, at t = 0. 
The beads are then released so that they can vibrate freely. 
Equilibrium Position 
A Typical Initial Position 
FIGURE 3.6.1 
Problem: For small vibrations, determine the position of each bead at time 
t >0 for any given initial configuration. 
Solution: The small vibration hypothesis validates the following assumptions. 
e 
The tension T remains constant for all time. 
e 
There is only vertical motion (the horizontal-forces cancel each other). 
e 
Only small angles are involved, so the approximation sin@ © tan @ is valid. 
Let yz(t) = yr be the vertical distance of the k'" bead from equilibrium 
at time t, and set yo = 0 = yn41. 
te 
> | — 
ey 
ete 
> + a 
FIGURE 3.6.2 
If 6; is the angle depicted in Figure 3.6.2, then the upward force on the ee 
bead at time ¢ is F,, = Tsin6,, while the downward force is Fa = T sin @y-1, 

386 
Chapter 3 
; 
Eigensystem Basics 
so the total force on the kth bead at time $18 
F=F,-—Fy=T(sin@, — sin 0,1) © T (tan ™;, =— tan 6,_1) 
_ am [Yetta Ye Ye Yen) _ 7 
Syge 
gp 
= (HH ee ) 7 (Ye-1 — 2k + 
Ye+4) 
Newton's second law says force = mass x acceleration, so to model the motion of 
the kt" bead, set 
my, = + 
(oh —2yn + Yeti) => Yet (vet + 2y% — Ye+1) = 0 
together with y,(0) = cz and y},(0) = 0. These equations constitute a system of 
n second-order linear differential equations, each of which is coupled to its neigh- 
bors so that no single equation can be solved in isolation. To extract solutions, 
the equations must somehow be uncoupled, and this is where the importance of 
matrix diagonalization is realized. Write the equations in matrix form as 
yy 
2 -1 
YI 
Vy 
=f" 
V9) 
2a 
y2 
Y3 
Be he 
eo 
ys | 
—]| 
0 
3.6.2 
= 
'cs 
; 
aE 
(3.6.2) 
=I 
ue 
—1 
2 
Yn 
0 
or equivalently, y" + Ay =0 with y(0) 
=c =(c1,¢2,...,¢n)? and y'(0) =0. 
Since A is a symmetric matrix, P'AP = D = diag (A1,A2,;--.,;An), Where 
P is an orthogonal matrix whose columns are eigenvectors of A and where the 
A;'s are the eigenvalues of A. Making the substitution 
y = Pz in y'+Ay =0 
produces 
Mt 
zl! + Dz = 0, 
zh 
. ~ te: 
; 
: 
MO =P 
c= cy, 
of 
de) 
Fae | ee 
ol 
be 
Z 
10) 
0 
; 
Sven 
(0) 
Pa 
Oost 
die 
Ae 
zn 
0 
In other words, by changing to a coordinate system defined by orthonormal 
eigenvectors of A, the original system (3.6.2) is completely uncoupled so that 
each equation z; + Anz = 0 with z,(0) = é and 2,(0) = 0 can be solved 
independently. Recall from elementary differential equations that 
apet¥>* 4+ Bpe-tV—>x 
when Ax < 0, 
7a aes 
Zn (t) = 
a, cos (tV Ax) + By sin (tV Ax) when A; > 0. 
Vibrations are expected to be sinusoidal, which suggests that each a 
0. Tn 
other words, the mathematical model would be inconsistent with reality if the 

3.6 Positive Definite Matrices and Quadratic Forms 
387 
symmetric matrix A in (3.6.2) were not positive definite. In fact, A is indeed 
positive definite because there is a Cholesky factorization 
A = R?R with 
ry 
—1/ri 
T2 
—1 / 
T2 
bets 
k-1 
R 
=4)—— 
a 
ie 
with 
rz = 4/2 ———. (3.6.3) 
mL 
k 
Tana 
—1/fa-1 
lm 
It is thus mathematically assured that each A, > 0. Since A is a tridiagonal 
Toeplitz matrix, the formula in (3.2.13) on page 314 shows that 
ya 
kr 
AT 
kr 
Nee ee, 
eee 
eo eo 
. 
Say J ( cos se :) a sin 
Din +1) 
(see Exercise 3.2.22). 
Therefore, 
Zk = QQ, COS (tV rx) + 6, sin (tVrx) 
2p (Os ee 
rey = OpCOs (tVrx), 
(3.6.4) 
24,(0) = 0 
and for P = [x1|x2|---|Xn], 
y= Pz = 2X1 + ZaXg t+-- + 2ZyXp = si (é 
cos (tV/Ax) ) 
x3. 
(3.6.5) 
j=1 
This means that every possible pattern of vibration is a combination of modes 
determined by the eigenvectors x;. To understand this more clearly, suppose 
that the beads are initially positioned according to the components of x; —i.e., 
c=y(0)=x,;. Then 
¢= Pc = P?x; =e;, so (3.6.4) and (3.6.5) reduce to 
et es (tV Ax) : : 
vis ay (cos (tVAx) ) 
x5. 
(3.6.6) 
In other words, when y(0) =x,, the j*" eigenpair (A;,x,;) completely deter- 
mines the mode of vibration because the amplitudes are determined by x;, and 
each bead vibrates with a common frequency f = JrAj /2m. The motion defined 
by (3.6.6) is called a normal mode of vibration. Equation (3.6.5) translates to 
say that every possible mode of vibration is a combination of the normal modes. 
2 
-1 
0) 
For example, if n = 3, then A = 
,/(T/mL) (= 2 
-1 ) 
has eigen- 
0 
-1 
2 
values \1 = (T/mL)(2 — V2), 2 = (T/mL)(2), 
Az = (T/mL)(2 + V2), with 
respective orthonormal eigenvectors 

388 
Chapter 3 
. 
Eigensystem Basics 
so the three normal modes are as shown below in Figure 3.6.3. 
Mode for Gn x) 
Mode for Ce: X5) 
Mode for (Nak at 
FIGURE 3.6.3: NORMAL MODES OF VIBRATION FOR n= 3 
Example (Heat Flow in a Plate) 
Another natural situation that produces positive definite matrices is the numeri- 
cal solution of the partial differential equation that describes heat flow. According 
to the laws of physics, the temperature at time ¢ at a point (x,y,z) in a solid 
body is a function u(z,y,z,t) satisfying the diffusion equation 
sale 
ee 
oan Ay 
= Ot Ou, Ou 
Be BV 
where MEE AEE 
tang ae a 
(3.6.7) 
is the Laplacian of u and K is a constant of thermal diffusivity. At steady 
state the temperature at each point does not vary with time, so 0u/0t =0 and 
u=u(z,y,2) satisfy Laplace's equation V*u = 0. Solutions of this equation are 
often called harmonic functions. The nonhomogeneous equation V2u = f (Pois- 
son's equation) is addressed in Exercise 3.6.34. To keep things simple, consider 
the following two-dimensional problem. 
Problem: For a square plate as shown below in Figure 3.6.4(a), explain how to 
numerically determine the steady-state temperature at interior grid points when 
the temperature around the boundary is prescribed to be u(x, y) = g(x,y) for 
a given function g. In other words, explain how to extract a numerical solution 
to V?u =0 
in the interior of the square when u(z,y) = g(x,y) on the square's 
boundary. This is called a Dirichlet problem! 
Solution: Discretize the problem by overlaying the plate with a square mesh 
containing n? interior points at equally spaced intervals of length h. As il- 
lustrated in Figure 3.6.4(b) for n = 4, label the grid points using a row-wise 
ordering scheme—i.e., label them as you would label matrix entries. 
Johann Peter Gustav Lejeune Dirichlet (1805-1859) held the chair at Gottingen previously 
occupied by Gauss. Because of his work on the convergence of trigonometric series, Dirichlet 
is generally considered to be the founder of the theory of Fourier series, but much of the 
groundwork was laid by S. D. Poisson (page 405) who was Dirichlet's Ph.D. advisor. 

_u(x,y) = g(«,y) on the boundary 
: 
sa 
h 
(a) 
FIGURE 3.6.5 
(b) 
Approximate 0?u/0x? and 6?u/dy? at the interior grid points (a;,y;) by 
using the second-order centered difference formula (2.6.3) developed on page 221 
to write 
e 
Ou 
u(x; a h, Yj) = 2u(z;, ya) ie u(x; +h, y;) 
4y 
Ox? (eis) 
i. 
he 
eat 
: 
(3.6.8) 
Pu a 
amt May Yj — h) = 2u( ai, Y;) a u(x, Yj oe h) bss O(h*) 
Oy? (@i,ys) 
h? 
Adopt the notation ig = u(x;,y;), and sum the expressions in (3.6.8) using 
Vela, y;) = 9 for interior points (x;,y;) to produce 
dug; = (Ui—1,j + Uig1,g + Ui,g-1 + Uaje1) + O(A®) 
for 
i,j =1,2,...,n. 
In other words, the steady-state temperature at an interior grid point is approxi- 
mately the average of the steady-state temperatures at the four neighboring grid 
points as illustrated below. 
: 
ee 
Ui—1,5 
F Ui41,g F Ui j—1 + Ui 
jt1 
'ae 
ue 
ugg = 
+ O(n) 
Neglecting the O(h®) terms results in the five-point difference equations 
4uig — (ty ag 
aegis Ug each ia isn 
etotiny 
479 = 1, 2,..2, 7; 
(3.6.9) 

390 
Chapter 3 
Eigensystem Basics 
that constitute an n2 xn? linear system Lu = g in which the unknowns are the 
uij's, and the right-hand side contains boundary values. For example, a mesh 
with nine interior points produces the following 9 x 9 system. 
4 
0 
u11 
Joi + 910 
1 
ul 
U2 
GJo2 
0 
4 
U13 
go3 + 914 
1 
0 
Ua 
920 
0 
0 
u22 | = 
0 
0 
1 
U23 
G24 
0 
0 
U31 
930 + 941 
0 
0 
U32 
942 
0 
0 
U33 
943 + 934 
FIGURE 3.6.6: THE DISCRETE LAPLACIAN FOR n= 3 
The coefficient matrix of this system is the discrete Laplacian, and in general it 
has the symmetric block-tridiagonal form ' 
A eee | 
oad 
-I 
T 
-I 
—1 
4 -1 
| 
ME 
he 8 
with 
T= 
fod me 
. 
(3.6.10) 
-I 
T 
-I 
-1 
4 -1 
-I 
T 
n2xn? 
A primary attribute of L is that it is positive definite. 
3.6.5. Theorem. The discrete Laplacian matrix L in (3.6.10) is positive 
definite. 
Proof. 
The matrix T in (3.6.10) is a symmetric tridiagonal Toeplitz matrix, 
so the formula in (3.2.13) on page 314 insures that the n eigenvalues of T are 
Ay = 4— 20s ( 
n+1 in (= 1,000. 
(3.6.11) 
If U is an orthogonal matrix such that U?TU = D = diag (Aj, Ao,... jan)s 
and if B is the n? x n? block-diagonal orthogonal matrix 
D 
-I 
ee 
Se, 
: 
iae 
et 
Bi | See 
9 ea) 
then 
B'LB=L= 
Pip, 
00 
U 
tpg 
—I 
D 
U 
0 
Note that L is the two-dimensional version of the one-dimensional finite-difference matrix in 
the two-point boundary value example on page 221. 

3.6 Positive Definite Matrices and Quadratic Forms 
391 
Consider the permutation obtained by placing the numbers 1,2,...,n? row-wise 
in a square matrix, and then reordering them by listing the entries column-wise. 
For example, when n= 3 this permutation is generated as follows: 
ky 
Pls 
as} 
v = (1,2,3,4,5;6, 
7,8,9) 4 A= (4 5 °) 
— (1,4, 7, 2,5, 8, 3,6, 9) =v. 
1 
ash 
8) 
Equivalently, this can be described in terms of wrapping and unwrapping rows by 
AXA 
wrap 
Unwrap os 
_ 
Z 
writing v—> A —> A? 59. If P. is the associated n? x n? permutation 
matrix, then 
png 
ba 
e 
i 5 
1 
re 
, 
"aiy 
ons 
0 
a 
Ch 
ae 
Petes] 
ee 
eee | oe 
Aepiatey 
: 
: 
: 
3 
—-1 
x 
-1 
0 
OFF 
eae 
a 
Ri 
-1 
% 
nxn 
Try it on the 9 x 9 case to see why it works. Now, T; is another symmetric 
tridiagonal Toeplitz matrix, so formula (3.2.13) on page 314 again applies to yield 
o(T;) = {\; —2cos(jr/n+1), 7 =1,2,...,n}. This together with (3.6.11) 
produces the n? eigenvalues of L as 
in 
jt 
. 
Ay = 4-2 os (7) +- 
os (27), 
bf Sly 
en, 
or, by using the identity 1 —cos@ = 2sin?(0/2), 
Ney 
sin (55) +5in 
n+) ) 
|? 
tap 
12 
eae 
AoDekah 
Thus each ;; is positive, and thus L must be positive definite. 
Mf 
e 
The fact that L is positive definite has a couple of important implications. 
First, being positive definite means that the discrete Laplacian L is non- 
singular, and hence Lu = g yields a unique solution for the steady-state 
temperatures on the square plate—something would be amiss if this were 
not true. 
e 
More importantly, knowing that L is positive definite means that special 
purpose algorithms designed specifically to efficiently solve positive definite 
systems can be applied. Several such algorithms exist—two of the classi- 
cal ones are the Cholesky algorithm described on page 277 that creates the 
Cholesky factorization 
L = R?R and a popular technique called the pre- 
conditioned conjugate gradient iterative method. 

392 
Chapter 3 
: 
Eigensystem Basics 
Square Roots 
A matrix B € F"%" is called a square-root of A € F"™" whenever B? = A. 
Not all matrices have square roots—e.g. consider A = (3 a): And, just as 
in the case of scalars, when a matrix square root exists, it need not be unique. 
However, things are different in the case of positive definite (or semidefinite) 
matrices. 
"ae 6. 
'Theorem. ip A is Fpoative Ashok (semidefinite), then A has a 
Bos, 
unique is definite (semidefinite) square Toot. 
Proof. 
If A is positive definite with eigenvalues A; > A2 > ++: > An > 0, then 
A = UDU*%, where U is unitary and D = diag (Aj, A2,.-.,An). The matrix 
A!/2 = UD!/2U* with D!/? = diag (vr, Ue 
VXn) is a square root of 
A that is also positive definite. To prove uniqueness, assume that B is a positive 
definite matrix with eigenvalues 8; > 82 >-:: > Bn > 0 such that B? = A. 
This means that 8? = \;, and hence 8; = /); for each i. The matrix B+/\;1 
is nonsingular for each \, (its eigenvalues {8; + /Xi, Bo + VX; ---, Bn + Vit 
are all positive), and hence 
A-—A JI = (B+ VAI) (B- VX), so (A—Ad)x 
= 0 <> (B- /AiI)x =0 
In other words, N(A — \;I) = N(B — VAjI) for every i so that A and B 
have the same set of eigenvectors. Therefore, by virtue of being normal, there is 
a complete orthonormal set of eigenvectors {v1,V2,...,Vn} that is common to 
both A and B, so V=[v,|---|v,] 
is a unitary matrix such that 
V*AV=D 
and 
V*BV=D!". 
(3.6.13) 
This is true for all positive definite square roots of A, so if C? = A = B?, 
where C and B are both positive definite, then 
V*CV =D'? =v*BV = C=B. 
(3.6.14) 
The proof for positive semidefinite matrices is identical with the realization that 
the eigenspaces of A and B for zero eigenvalues agree because 
N(A)=N Wesy = N(B*B)=N(B) 
(by Theorem 2.4.20, page 195). 
Polar Decomposition 
Every complex number 
z can be written as z = re'®, where r > 0. This is 
called the polar form of z. The matrix version of this is as follows. 

4 
i 
7 
3.6 Positive Definite Matrices and Quadratic Forms 
393 
Proof. 
Let A=U(9 9)V* beanSVDof A sothat AA*=U(T) 9) U*. 
If R = (AA*)!/2 — su 
hee then 
A = R(UV*) is a polar decomposi- 
tion. Uniqueness in the nonsingular case follows because R must be positive 
definite, so if RyU; = A = R2U2 in which each R; and U; is respectively 
positive definite and unitary, then RZ Ri = U2U} is unitary, and thus 
I=(R,'Ri)(R, Ri)' =]R, 
Ri RR,' => Ri=Rs == 
Ri =R, 
because R; is the unique positive definite square root of R?. If 
A = RU 
is normal, then A*A = AA* implies that U*R?U = R?. Since R is the 
unique positive semidefinite square root of R? and U*RU is the unique positive 
semidefinite square root of U*R?U, it follows that R = U*RU, and thus 
UR = RU. Conversely, if RU = UR, then U*R* = R*U* so that 
A*A = U*R*RU = R*U*UR = R'R = R? = RR* = RUU*R* = AA*, 
and thus A is normal. 
& 
Quadratic Forms 
Terms such as x? Ax (for real matrices) and x*Ax (for complex matrices) 
are frequent occurrences in both theory and applications, and because they are 
quadratic in nature the following terminology is adopted. 
Ag 3. Definition. For Keg 'and x e Re the expression 
ie led a queda form, a it is ad . be a positive donnie (or 
__ semidefinite) form when A is positive definite (or semidefinite). When 
ok and x are 
> complex, -x* Ax is called a complex quadratic form. 

394 
Chapter 3 
' 
Eigensystem Basics 
Note that while the previous definition does not require A to be symmetric 
in the real case or hermitian in the complex case, these conditions can always be 
forced to hold because 
A+A? 
a 
————_ 
x, 
2 
xTax=x" | 
| and 
Ax =x'| > 
and (A+ A7)/2 and (A +A*)/2 are respectively symmetric and hermitian. 
Consequently, it is tacitly assumed that the matrix A in every quadratic form is 
either real and symmetric or complex and hermitian. Furthermore, since the de- 
velopment of complex quadratic forms parallels that of real quadratic forms, ex- 
plicit reference to complex forms is omitted throughout the rest of this section—it 
is understood that unless otherwise stated, transposition (x)' can be replaced 
by conjugate transposition (x)* with only minor adjustments. 
Taylor's Theorem in R" 
A common place where quadratic forms are encountered is in optimization prob- 
lems that arise from the application of Taylor's theorem in R". If f is asmooth 
real-valued function defined on R", and if x9 € R"*!, then Taylor's theorem 
says that the value of f at x € R"*' is given by 
(x — x9)? H(xo)(x — xo) 
: 
+ O(||x — 
xoll2); 
f(x) = (Xo) + (x — x0)" 
g 
(x0) + 
where g(xo) = Vf(xo) (the gradient of f evaluated at xo) has components 
gi = Of /Ox;| 
, and where H(xo) is the Hessian matrix whose entries are given 
xO 
by hi; = 0? f/0x,0z;| 
. Just as in the case of a single variable, the vector xo 
0 
is called a critical point when g(xo) = 0. If xo is a critical point, then Taylor's 
theorem shows that the quadratic form (x — xo9)'H(x9)(x —xo) governs the 
behavior of f at points x near to x9. This observation yields the following 
conclusions regarding local maxima or minima. 
e 
If xo is a critical point such that H(xo) is positive definite, then f has a 
local minimum at xg. 
e 
If xo is acritical point such that H(xo) is negative definite (i.e., z'Hz <0 
for all 
z 
40), then f has a local maximum at xp. 
Diagonalization of Quadratic Forms 
A diagonal quadratic form is a quadratic form x? Dx in which D isa diagonal 
matrix, in which case x" Dx = Oy, dux?. Every quadratic form x7 Ax can 
be diagonalized by making a change of variables. As explained above, A € R"*" 
can be assumed to be symmetric so that there is an orthogonal matrix Q such 

3.6 Positive Definite Matrices and Quadratic Forms 
395 
that Q7AQ = D = diag (Ai, A2,.--,An), where 
3 € o(A) (by Theorem 
3.4.11, page 346). Setting y = Q?x (or, equivalently, 
x = Qy) gives 
nm 
Ke Aka) GAQv = y Dye eau. 
(3.6.15) 
i=1 
This shows that the nature of a quadratic form is determined by the eigenvalues 
of A (which are necessarily real). The effect of diagonalizing a quadratic form 
in this way is to rotate the standard coordinate system so that the graph of 
x' Ax =a 
for agiven constant a appears in "standard form" in y coordinates. 
If A is positive definite, then ; > 0 for all A; € a (A), so (3.6.15) makes it 
clear that the graph of x' Ax =a for a > 0 is an ellipsoid centered at the 
origin. 
For example, consider the quadratic equation 
25 = Ale? — 2dr + 3422 = (2 22) ( 
ie iy ee) =xTAx. 
(3.6.16) 
The eigenvalues and associated eigenvectors of A are 
3 
—4 
Ai = 25, 
x 
= (1/5) (4), and 
Ag = 50, 
x2 
= (1/5) ( 
ab 
so A is positive definite, and Q = (1/ 5)(3 ay is an orthogonal matrix such 
that 
pean 
ap 
a 
= 
Q COS, a =): 
The positive definite quadratic form x? Ax is diagonalized by setting x = Qy, 
and the quadratic equation (3.6.16) expressed in y coordinates becomes 
95 =x? Ax = y'Q?AQy = y! Dy = 25y? + 50y2 => 
y?+2y3 =1, 
which is an ellipse whose semi-major and semi-minor axes have respective lengths 
1 and 1/v2. 
Congruence 
As illustrated above, diagonalizing a symmetric matrix A by an orthogonal 
similarity transformation creates a change of coordinates that diagonalizes the 
associated quadratic form x' Ax. However, something less than an orthogonal 
similarity can also do the job. In particular if C is any nonsingular matrix 
(not necessarily orthogonal) such that C' AC = D is diagonal, then changing 
coordinates by means of x = Cy yields 
x7 Ax=y'C  ACy=y' Dy=) 
diy; 
(but dy ¢o(A)). 
a 
This realization motivates the following definition. 

There is a canonical (or standard) form that is achievable by means of 
congruence. For a symmetric matrix A € R"*" with rank(A) =r, let Q be 
an orthogonal matrix such that 
Q7 AQ = D, where the eigenvalues in D are 
ordered by sign—i.e, 
Pi 
D=| 
i 
, 
where each p; >0 and each n; < 0. 
+) 
1//Pr 
7 
1/J—ny 
P 
( x 
i ¥ ( 
= 
) 
ee 
' r: ( 
. 
1//Bs 
1/V=m 
I 
then C = QS is a nonsingular matrix such that 
iff P= 
I, 
C' AC = S7Q7AQS = ( =I; 
i where 
s+t=r. 
(3.6.17) 
0 
nmkn 
This is the canonical form of A obtainable by a congruence transformation. 
Making a change of coordinates defined by x = Cy produces the canonical 
representation of the associated quadratic form as 
8 
t 
x?Ax =y'C'y = a ye = SE. y; where 
s+t=r. 
(3.6.18) 
i=1 
i=1 
Diagonalization by Congruence Using Symmetric Elimination 
Determining the congruence transformation C = QS used in (3.6.17) hinges on 
computing Q and D, so a complete orthonormal eigen system for A must be 
found. It is usually easier to build a congruence transformation that produces 
the canonical form by using symmetric elimination involving only elementary row 
and column operations similar to using symmetric pivoting to build an LDL? 

3.6 Positive Definite Matrices and Quadratic Forms 
397 
factorization as described on page 278. Starting with a symmetric matrix A 4 0, 
proceed as follows. 
e Begin with a nonzero in the (1,1)-position. 
> If not all diagonal entries are zero, select the diagonal a;; of maximal 
magnitude (for numerical stability) and symmetrically permute a;; 
to the (1,1)-position with a permutation matrix P, as shown in 
(2.10.34) on page 278— PT AP, is a congruence transformation. 
> If ax, = 0 for all k, then identify some a;; = a;; 4 0 for i#j 
(maximize the magnitude of a,; for numerical stability). Add row 
2 to row jg, and add column 7 to column j with the congruence 
transformation 
Gf AG; =B in which G, is the elementary matrix 
G, =I+ e,eF . This produces bj; = 2a;; #4 0, which can then be 
symmetrically permuted to the (1,1)-position with a permutation 
matrix to produce the congruence 
fl 
PTG?AG,P; = PTBP, = & fs where 
p, #0. 
e Use symmetric elimination with 
wey? se) ee 
0 
I 
Pl 
; 
C= 
1//—p1 
—e? /pi 
ea: 
0 
I 
Pi 
, 
to obtain 
T 
pf 
Pi 
cy 
a 
ae 
0 
= 
+1 
O 
Ee es sical oe 
ied mel Neg 
sell 
(0) 
Thus C; = G,;P,L;, 
is nonsingular and CTAC, = ( 
avER ). 
1 
e 
If A; 40, then repeat the process. Successively executing the procedure 
eventually reduces A by congruence to a diagonal matrix in which each 
diagonal entry is +1 or 0. 
e A final symmetric permutation can move all +1's to the top and keep all 
I; 
zeros at the bottom. Thus the canonical form 
—I; 
) 
is obtained 
0 
by congruence transformations involving only elementary row and column 
operations. 
Sylvester's Law of Inertia 
) 
In 1852 J. J. Sylvester (page 117) characterized congruent matrices with a "law" 
(Theorem 3.6.11 below) that generated the following specious terminology. 

WietAny We tae 
ee whieh oy 
? 
¥ 
One of Sylvester's lasting contributions was his discovery and proof of his 
"law" asserting that the canonical form in (3.6.17) is invariant under a congru- 
ence transformation. 
: 
Proof. 
If A & B, then rank(A) = rank(B) = r =the number of nonzero 
eigenvalues. Let inertia(A) = (s,j,n—r) and inertia (B) = (t,k,n—r), and 
suppose that s >t. As shown in (3.6.17), 
I, 
I 
" 
A= Di = ( —I; 
) and B2Dz= ( —I, ) (s+j =r =t-+hk). 
i) 
Since congruence is transitive (ie., 
E = F and 
F =G implies E & G), it 
follows that D, ~ D2 so that C'D,C = Dy for some nonsingular C. Partition 
Xixn 
Xtxn 
Cree 
i 
, and define B = 
. 
, where Z = Gide: 
e i 
ny 
Z 
Since rank (B) < n, there is a nonzero x = 
a 
such that Bx = 0, or 
equivalently, Xx = 0 and Zx = 0, the latter of which implies that x2 = 0. 
Consequently, x = te ni where x} = (21, 22, ..., 24) 
#0, and hence 
x7 Dax = 09 Penne oe 
0. 
(3.6.19) 
The word "inertia" in this context is misleading, but it has survived too long to rid the literature 
of it. The following sentence extracted from Sylvester's original 1852 paper explains in his own 
words his motivation. It also illustrates the flowery over-the-top language that was the style 
of mathematical exposition at the time. "My knowledge of the fact of this equivalence is, as 
I have stated, deduced from that remarkable but simple law to which I have averted, which 
affirms that the invariability of the number of the positive and negative signs between all 
linearly equivalent functions of the form hs, cra" (subject, of course, to the condition that 
the equivalence is expressible by means of equations into which only real quantities enter); a 
law to which my view of the physical meaning of quantity of matter inclines me, upon the 
ground of analogy, to give the name of the 'Law of Inertia for Quadratic Forms,' as expressing 
the fact of the existence of an invariable number inseparably attached to such forms." 

3.6 Positive Definite Matrices and Quadratic Forms 
399 
ae 
2 
sc 
, 
But it is also true that if y = (2%) ce Oa boa Coc) os CAL where 
y3 = (Yer1, Ueto. +s Yn) 
#0, then 
ar 
4 
x°Dix = x7 
C7! DyO7!x = y" Day = — 941 — Yen 
4 <0, 
which contradicts (3.6.19). Assuming that s <t yields a similar contradiction, 
so 
s =t, and thus inertia(A) = inertia(B). Conversely, if A and B have 
inertia, then 
AZ D,=D.2=B 
= 
ASB. 
FE 
An Amazing Consequence 
A consequence of Sylvester's law is that the signs of the eigenvalues of any sym- 
metric matrix can be determined without any eigen computations whatsoever. 
Symmetric elimination using only elementary row and column operations as de- 
scribed on page 397 will reduce any symmetric matrix to its canonical form by 
congruence thereby revealing the signs of its eigenvalues. This is one of the most 
amazing (and most useful) facts in all of linear algebra. For example, this is par- 
ticularly helpful in optimization problems such as those described on page 394 
involving Taylor's theorem where the Hessian matrix H must be evaluated for 
definiteness in order to determine if a critical point yields a local minimum or 
local maximum. 
Simultaneous Diagonalization by Congruence 
There are times when two quadratic forms 
x? Ax = SS Ajj LiL 5 
and 
x' Bx Se big L5X5 
tJ 
i] 
must be dealt with simultaneously, in which case it is desirable to simultaneously 
diagonalize each of them by using the same change of coordinates y = Cx so 
that 
yTAy=) 
ay? 
and 
y'By=)_ Biy?. 
4 
a 
This amounts to finding a nonsingular matrix C such that 
en 
© 
or 
Gi 
@ 
ooo 
0 
Q) 
ey 
ooo. 
© 
7 
O° 
Boo 
=O 
CO 
| 
ende CRC 
| 
| 
ele a tro 
OF Ales as, 
However, it is not always possible to simultaneously diagonalize two symmetric 
: 
: 
; 
io) 
matrices with the same congruence transformation—e.g., consider A = { 4 e 
and B= Oe a (Exercise 3.6.27 asks for the details). Notice that neither of 
these two matrices is positive definite. When one of the two matrices in question 
is positive definite, the following theorem shows that the simultaneous diagonal- 
ization by congruence is always possible. 

Proof. 
y aia 
a 
re 
~ 
If A is positive definite, then A! is also positive definite (Exercise 
3.6.4), so A~! has a unique positive definite square root A~'/2 (by Theorem 
3.6.6, page 392). Since B is symmetric, so is A~!/7BA~1/?, and hence there is 
an orthogonal matrix P such that P?A~!/?BA~!/?P = D isa diagonal matrix 
(by Theorem 3.4.11, page 346). Therefore, 
C = A~!/?P is a nonsingular matrix 
such that CTBC =D and CTAC=P7TA-7AA-1/?2P=I. 
Exercises for section 3.6 
3.6.1. 
Oe 
Ps 
3.6.3. 
3.6.4. 
3.6.5. 
3.6.6. 
3.6.7. 
Which of the following matrices are positive definite? 
husky 
=1 
20-68 
Mra 
ue. 
A=(21 
5 
:). B=( 6 3 a) c=(0 6 2), 
—1 
1 
5 
fey 
(OP 
ts) 
2, 2A 
How do the eigenvalues of positive definite (or semidefinite) matrix relate 
to its singular values? 
Let A € R"*" be symmetric. Explain why each a;;j > 0 when A is 
positive semidefinite, and why each a;; > 0 when A is positive definite. 
Let A € F"*" be either real and symmetric or complex and hermitian. 
Explain why A~! is positive definite if and only if A is positive definite. 
Explain why det(A) >0 when A is positive semidefinite, and then 
explain why det(A) >0 when A is positive definite. 
Explain why trace(A) > 0 when A is positive semidefinite and why 
trace(A) >0 when A is positive definite. 
Let A € F"*" be either real and symmetric or complex and hermitian. 
(a) Prove that if A is positive definite, then C*AC is positive 
definite for every nonsingular C. 
(b) Similarly, prove that if A is positive semidefinite then C*AC 
is positive semidefinite for all C € F"**, 

3.6 Positive Definite Matrices and Quadratic Forms 
401 
3.6.8. 
3.6.9. 
3.6.10. 
3.6.11. 
3.6.12. 
Let A € F"*" be either real and symmetric or complex and hermitian. 
(a) Prove that A is positive definite if and only if every principal 
submatrix in A is positive definite. 
(b) Similarly, prove that A is positive semidefinite if and only if 
every principal submatrix in A is positive semidefinite. 
Let A € F"*" be either real and symmetric or complex and hermitian. 
(a) Explain why det(A) >0 when A is positive definite. 
(b) Prove that A is positive definite if and only if all principal 
minor determinants of A are positive. 
Let A € F"™"" be either real and symmetric or complex and hermitian. 
Prove that A is positive definite if and only if all leading principal 
minor determinants of A are positive. Hint: Recall Exercise 2.10.10, 
page 280. 
Prove that if A,U ¢ F""*" are such that A is positive semidefinite, U 
is unitary, and UA is normal, then UA = AU. 
Spring—Mass Oscillations. 
Two weights having respective masses 
mj, 
and mz are suspended between three tightly stretched identical springs 
that have spring constant k and length / when not stretched or com- 
pressed. The equilibrium (at rest) position is on the top. Each weight 
is initially displaced horizontally from rest and then allowed to oscillate 
freely. The configuration on the bottom represents the system in motion, 
where x; = 2;(t) is the displacement at time ¢ of weight i from its 
equilibrium. Assume that there is no vertical displacement or damping. 
y 
Cie 
ya) 
or 
2 
—l 
m=(") oy) ee) 
= 
om 
oe Ze 
and explain why K is positive definite. 

402 
Chapter 3 
' 
Eigensystem Basics 
(b) 
(c) 
3.6.13. 
3.6.14. 
3.6.15. 
Look for a solution of the form x = e'®tv for a constant vector v, and 
show that this reduces the problem to solving an algebraic equation of 
the form (K — AM)v = 0 for A = 6?. This is called a generalized 
eigenvalue problem because it becomes an ordinary eigenvalue problem 
when M =I. The generalized eigenvalues \, and Az are the roots of 
the equation det (K — \M) = 0. Find them when 
k= 1, m, =1, and 
m2 = 2, and describe the two modes of vibration determined by the two 
solutions v, and vo. 
¥ 
Set m,; = m2 = m and k = 1, and apply the technique used in the 
example concerning vibrating beads on a string on page 385 to determine 
the normal modes. Compare the results with those of part (b). 
Free Hanging Spring—Mass Oscillations. 
Three weights of mass m 
are suspended from three identical springs that have 
spring constant k and length / when not stretched 
or compressed. The equilibrium (at rest) position is 
on the left. Each weight is initially displaced verti- 
cally from rest and then allowed to oscillate freely. 
The configuration on the right represents the sys- 
tem in motion, where y; = y;(t) is the displacement 
| 
Yi 
at time t of weight i from its equilibrium. Assume 
that there is no damping and that the springs are 
weightless. 
(a) Show that the equation of motion is 
Y2 
My"+Ky=0, 
where 
M=mlI, 
| 
2-1 
0 
Yl 
K =K(-1 
2 -1), and y= (2). 
Gy 
—t 
1 
y3 
Note: k33 = 1 is not a mistakel. 
Y3 
(b) 
Explain why K is positive definite. 
(c) Find the normal modes when m=1=k. 
Prove that A ¢ F"*" 
is positive semidefinite if and only if the block 
; 
A 
: 
as 
s 
. 
matrix Z = ( 
A e is positive semidefinite. 
Find the smallest value of x that will make zI—A positive semidefinite 
lu) 
WE 
eetoe 
ifn 
V2 
V4 
--. 
Van 
tor A 
: 
"ae 
. 
|. Hint: See Exercise 3.1.5 on page 299. 

3.6 Positive Definite Matrices and Quadratic Forms 
403 
3.6.16. 
3.6.17. 
3.6.18. 
3.6.19. 
3.6.20. 
3.6.21. 
3.6.22. 
3.6.23. 
3.6.24. 
Prove that trace (A*) < [trace (A)]? when A is positive semidefinite. 
Explain why the inequality is strict when A is positive definite. 
Let A,B € F"*" be positive semidefinite. 
(a) Prove that trace (AB) < trace (A)trace(B). Hint: Make use 
of the CBS inequality for matrices in Theorem 1.9.2 on page 82. 
(b) Give an example to show that this need not be true for indefinite 
matrices. 
(c) Is trace(A) a legitimate matrix norm for the set of all positive 
semidefinite matrices? 
Let A and B be hermitian. Prove that if B is positive definite, then 
[trace (A)| ; 
t 
DB 
race 
)2 
trace (B) 
Hint: Make use of the CBS inequality for matrices from Theorem 1.9.2 
on page 82. 
Prove that if 0 4 A € F"*" is positive semidefinite, then the spectral 
radius of A as defined on page 297 is p(A) = 
lim [trace (A?)}/? 
poo 
Give a simple 2 x 2 example to show that this need not be true when 
A is indefinite. 
Prove that if A € R"*" is symmetric and positive semidefinite, then 
|aiz| < (Gis + 53)/2. 
Prove that if A € R"*" is symmetric and positive semidefinite, then its 
largest entry lies on its diagonal in the sense that max;,; |a;;| < max; aij. 
Prove that if A € R"*" is symmetric and positive semidefinite, then 
laij| < ./auiaj; for all 1 A j. Note: This fact means that if aj = 0, 
then "A; = 0 and* Ay, =0: 
Prove that if A € R"*" is a nonsingular matrix in which each row has 
been normalized so that ||Aj«||, =1, then p(I—(2/n)A7A) <1. 
By diagonalizing the quadratic form 13x? + 10xy+13y?, show that the 
rotated graph of 132?+10ry+13y? = 72 is an ellipse in standard form. 

404 
Chapter 3 
. 
Eigensystem Basics 
3.6.25. 
3.6.26. 
3.6.27. 
3.6.28. 
3.6.29. 
(a) Using only elementary row and column operations as described on 
page 397, find a nonsingular matrix C such that C7AC 
is in the 
0 
3, 
0 
canonical form for congruence, where A = € : t), 
(b) Now reduce A to this canonical form by a congruence transfor- 
mation that is built from eigen-components of A. 
Consider the quadratic form 
(—227 + 7x3 oa Ax? + 42129 + 162173 + 202273). 
Ole 
f(x) = 
(a) Find asymmetric matrix A so that f(x) =x' Ax. 
(b) Diagonalize the quadratic form using the LDL? factorization 
and determine the inertia of A. 
(c) Is this a positive definite form? 
(d) Verify the inertia obtained in (b) is correct by computing the 
eigenvalues of A. 
(e) Verify Sylvester's law of inertia by making up a nontrivial con- 
gruence transformation C and then computing the inertia of 
CA AC, 
Explain why it is not possible to simultaneously diagonalize A = € oT 
and B= é a with the same congruence transformation. 
Positive Definite Schur Complements. The Schur complement of a nonsin- 
gular matrix Aj; in a block matrix an A) was defined on page 
167 to be S; = Ago — Ao Aj Ai. Similarly, if Ago is nonsingular, 
then its Schur complement is Sg = Aq — Ai2A55 Aoi. Prove that if 
A= Oe uae} is positive definite, then both Schur complements are 
12 
22 
also positive definite. 
Block Cholesky Factorization. Show that if A = 
ee wee 
is pos- 
. 
. 
. 
. 
. 
. 
22 
itive definite in which Aj, 
is square, then A has a block-Cholesky 
factorization. In other words, exhibit an upper block-triangular matrix 
R 
R 
R= ( oa ey such that A = R*R, where Rj; and Ro» are each 
positive definite (but not necessarily triangular). 

3.6 Positive Definite Matrices and Quadratic Forms 
405 
3.6.30. Let A = eae mi) be positive definite in which Aj, is square, and 
let S = Ago — Af, A7,'Ai2 be the Schur complement of Aj. Prove 
that ||Sll. < |All. - 
3.6.31. Let A = (ae ey be positive definite in which Aj, is square, and 
let o (A) = {A1,A02,.--, Ax}. Prove that 
oe 
IAT Aula ST 
min; ri ; 
Hint: Use the result that you had to derive to solve Exercise 3.6.29. 
3.6.32. Let A= ee ea be positive definite in which Aj, is square, and 
al 
let o (A) = {Aj, A2,...,Ax}. Prove that if the spectrum of the Schur 
complement S = Ago — Ani AT Ate is o (S) = {f1, f2,-..,H;}, then 
max; Hi — Max; Vi 
min; Li ae min; ri 
Note: This means that S is better conditioned than A. 
3.6.33. Explain why trying to produce better approximations to the solution of 
the Dirichlet problem for heat flow in a plate as described on page 388 
by using finer meshes with more grid points results in an increasingly 
ill-conditioned linear system Lu = g. 
3.6.34. For a given function f the equation V?u = f is called Poisson's | 
equation. Consider Poisson's equation on a square in two dimensions 
with Dirichlet boundary conditions. That is, 
Pu, Ou 
a2 
ta 
f(x,y) 
with 
u(z,y)=g9(z,y) 
on the boundary. 
zr 
Y 
Siméon Denis Poisson (1781-1840) was a prolific French scientist who was originally encouraged 
to study medicine but was seduced by mathematics. While he was still a teenager, his work 
attracted the attention of the reigning scientific elite of France such as Legendre, Laplace, and 
Lagrange. The latter two were originally his teachers (Lagrange was his thesis director) at the 
Ecole Polytechnique, but they eventually became his friends and collaborators. It is estimated 
that Poisson published about 400 scientific articles, and his 1811 book Traité de mécanique was 
the standard reference for mechanics for many years. Poisson began his career as an astronomer, 
but he is primarily remembered for his impact on applied areas such as mechanics, probability, 
electricity and magnetism, and Fourier series. This seems ironic because he held the chair of 
"pure mathematics" in the Faculté des Sciences. The next time you find yourself on the streets 
of Paris, take a stroll on the Rue Denis Poisson, or you can check out Poisson's plaque, along 
with those of Lagrange, Laplace, and Legendre on the first stage of the Hiffel Tower. 

406 
Chapter 3 
f 
Eigensystem Basics 
3.6.35. 
3.6.36. 
Discretize the problem by overlaying the square with a regular mesh 
containing n? interior points at equally spaced intervals of length h as 
described in the heat flow example on page 388. Let fi; = f(xi,y;), and 
define f to be the vector 
f = (fir, fi2,---, 
fin| fei, 
fa2--»»fan|> +> fat, fra, --- nF AS fe 
Show that the discretization of Poisson's equation produces a system 
of linear equations of the form Lu = g — hf, where L is the discrete 
Laplacian and where u and g areas described in the heat flow example. 
Show that the n? x n? linear system Lu = g that results from dis- 
cretizing Laplace's equation as described on page 390 can be written as 
a matrix equation AU+UA=G, where A, 
x, is the finite-difference 
matrix 
Pei 
Eg 
2s) 
=1 
Aken = 
Sens 
PRT! 
Bes 
; 
<j 
9s 
=i 4s 9 
and where G contains the boundary value information 
uio + Uo1 
Uo2 
Sis 
Reg 
U0n + U1,n+1 
U20 
0 
tee 
0 
U2,n+41 
Gnxn = 
: 
Un—1,0 
0 
aor 
0 
Un—-1,n41 
Uno +Un+1,1 
Un+1,2 
°** 
Un,n—1 
Un+i,n + Un,n+1 
The matrix Unxn = [uij| is the matrix of unknown values of u(zj, Yj) 
at the interior grid points. Now explain why the solution for U can be 
obtained by using the Kronecker product (page 227) to yield 
[In ® A) + (A @I,)] vec(U) = vec(G). 
Use part (b) of Exercise 3.2.29 on page 321 to construct an alternate 
derivation of (3.6.12) on page 391. That is, show that the n? eigenvalues 
of the discrete Laplacian L,,2,,2 are given by 
rij =4 sin? (Se) + si' on ABR toa 
Pte 
: 
| 2(n + 1) 
a 
2(n+1)/}' 
1,9 
=1,2,...,n. 
Hint: Recall Exercises 2.7.6 (page 232) and 3.2.22 (page 319). 

— 
= 
a 
; 
: 
—1 
2 -1 
7 
7 
> 
; 
= 
a 
_ 
Se 
—_— 
i 
S 
ee 
_ 
a 
- 
aa 
ae 
i 
= 
ae 
— 
as 

PaaS 
- 
' 
: 
7 
» Ph 
aa 
7 
; 
ieee 
4. 0" 
Gage 
hein) 
Gegely (pala 
\ 
7 
It is not the critic who counts; not the man who points out how the strong man stumbles, 
where the doer of deeds could have done them better. The credit belongs to the man 
who is actually in the arena, who errs, who comes short again and again because 
there is not significant achievement without error and shortcoming. 
— Theodore Roosevelt (1858-1919), 26th president of the United States 
I have the result, but I do not yet know how to get it. 
— Carl Gauss (1777-1855) 

CHAPTER 
4 
Vector Spaces 
4.1 
SPACES AND SUBSPACES 
After matrix theory became established toward the end of the nineteenth cen- 
tury it was realized that many mathematical entities that seemed on the surface 
to be different from n-tuples were in fact quite similar. For example, in addition 
to points in R? and R°, objects such as polynomials, continuous functions, 
and differentiable functions were recognized to satisfy the same additive and 
multiplicative properties. Rather than studying each topic separately, it was rea- 
soned that it is more efficient and productive to study many topics at one time 
by studying the common properties that they satisfy. This led to the following 
axiomatic' definition of a vector space that encompasses more than F". Never- 
theless, the axioms in Definition 4.1.1 below are motivated by F", so F" can 
be used as an intuitive guide while digesting these axioms. 
The idea of defining a vector space by using a set of abstract axioms was contained in a general 
theory published in 1844 by Hermann Grassmann (1808-1887), a theologian and philosopher 
from Stettin, Poland, who was a self-taught mathematician. But Grassmann's work was origi- 
nally ignored because he tried to construct a highly abstract self-contained theory, independent 
of the rest of mathematics, that contained nonstandard terminology and notation, and he had 
a tendency to mix mathematics with obscure philosophy. Grassmann published a complete 
revision of his work in 1862 but with no more success. Only later was it realized that he 
had formulated the concepts we now refer to as linear dependence, bases, and dimension. The 
Italian mathematician Giuseppe Peano (1858-1932) was one of the few people who noticed 
Grassmann's work, and in 1888 Peano published a condensed interpretation of it. In a small 
chapter at the end, Peano gave an axiomatic definition of a vector space similar to the one now 
being used, but this drew little attention outside of a small group in Italy. The current defini- 
tion is derived from the 1918 work of the German mathematician Hermann Weyl] (1885-1955). 
Even though Weyl's definition is closer to Peano's than to Grassmann's, Wey] did not mention 
his Italian predecessor, but he did acknowledge Grassmann's "epoch making work." Weyl's 
success with the idea was due in part to the fact that he thought of vector spaces in terms 
of geometry, whereas Grassmann and Peano treated them as abstract algebraic structures. As 
will be seen, it is the geometry that is important. 

410 
Chapter 4 
Vector Spaces 
Formal Definition of a Vector Space 
A general vector space involves four things—two sets Y and F, and two alge- 
braic operations called vector addition and scalar multiplication. 
(1) V isa nonempty set of objects called vectors. 
(2) F is a scalar field—for us F is either the field R of real numbers or the 
field C of complex numbers. 
(3) 
Vector addition (denoted by x+y ) is an operation between elements of V. 
(4) Scalar multiplication (denoted by ax) is an operation between elements of 
F and VY. 
The formal definition stipulates how these four things relate to each other. 
4.1.1. Definition. A set V is called a vector space over F when vector 
addition and scalar multiplication satisfy the following properties. 
(Al) 
x+y e€/Y 
forall x,y € V. This is called the closure property 
for vector addition. 
(A2) 
(x+y)+z=x+(y+z) for every x,y,z 
€ Y. 
(A3) x+y=y-+x 
for every x,y € V. 
(A4) 
There is a vector z € V (called the additive identity') such 
that x+z=x for every x € Y. The additive identity element 
is also called the zero vector, and it is denoted by writing 
ZO: 
(A5) 
For each vector x € Y there is a vector y € V (called the ad- 
ditive inverse' of x) such that 
x + y =z=O0. The additive 
inverse of x is denoted by writing y = —x. 
(M1) 
ax € VY for all a € F and x € V. This is the closure 
property for scalar multiplication. 
(M2) 
(a8)x =a(6x) for all a,8 € F and every x€ V. 
(M3) 
a(x+y)=ax-+ay for every a€F andall x,y € V. 
(M4) 
(a+ 8)x=ax+ 6x for all a,8 €F and every x€ V. 
(M5) 
1x=x for 1¢€F and every x€ V. 
A theoretical treatment of linear algebra would concentrate on the logical 
consequences of these defining properties, but the objectives in this text are 
different, so we will not dwell on the axiomatic development. Nevertheless, it is 
The additive identity z and the additive inverse y of x are unique—see Exercise 4.1.1. 

4.1 Spaces and Subspaces 
A411 
important to recognize some of the more significant examples and to understand 
why they are indeed vector spaces. 
Example (Coordinate Spaces) 
a2 
zx 
Since the coordinate spaces F!*" = {(x1, 22,..., %n)} and F™™! = 
' 
zL 
with x; € F are the models for the formal definition, they naturally have to be 
listed as the primary examples of a vector space, and they will continue to be 
the object of most of our attention. In the context of vector spaces it usually 
makes little difference whether a coordinate vector is depicted as a row or as a 
column. When the row or column distinction is irrelevant, or when it is clear 
from the context, the common symbol F" is used to designate a coordinate 
space. In cases where it is important to distinguish between rows and columns, 
the explicit notations F!*" or F"*! are used. 
Example (Matrix Spaces) 
The additive axioms A(1)—A(5) are properties possessed by matrix addition, 
and the multiplicative axioms M(1)—M(5) are properties possessed by scalar 
multiplication between scalars and matrices, so it is clear that: 
e 
Theset R"*" of mx n real matrices is a vector space over R. 
e 
Theset C"*" of mxn complex matrices is a vector space over C. 
As in previous sections, the common notation F"*" is used to include both the 
real and complex matrix spaces. 
Example (Function Spaces) 
With function addition and scalar multiplication defined by 
(f+9)(@) =f(a)+g(a) 
and 
(af)(x) =of(z), 
the following sets are vector spaces over R. 
e 
The set of functions mapping an interval [a,}] into R. 
e 
The set of all real-valued continuous functions defined on [a,b]. Note that the 
closure properties A(1) and M(1) hold because the sum of two continuous 
functions is again continuous, and a scalar multiple of a continuous function 
is also continuous. 
e 
The set of real-valued functions that are differentiable on [a,b]. The closure 
properties A(1) and M(1) hold because the sum of two differentiable func- 
tions is differentiable (the derivative of a sum is the sum of the derivatives), 
and a multiple of a differentiable function is differentiable (the derivative 
of a constant times a function is the constant times the derivative of the 
function). 
e 
The set of all polynomials with real coefficients. 

Proof. 
If S is a subset of VY, then S automatically inherits all of the vector 
space properties of V except (A1), (A4), (A5), and (M1). However, if S 
is closed with respect to addition and scalar multiplication (i-e., if (A1) and 
(M1) hold for S), then these together with the inherited properties ensure the 
existence of an additive identity and additive inverses in S —i.e., (A4) and (A5) 
must also hold for S. To prove this, let z be the additive identity in V, and 
observe that for all x € V and for 0 € F, we have 0x = z because if y € V is 
the additive inverse of x, then x + y =z so that 
z=x+y+z=(14+0)x+y+z=1x+0x+y+z 
= (0x+z)+(1x+y)=0x+z=0x 
forallxeV. 
Consequently, 
z = Ow for any w € S, so if S is closed with respect to scalar 
multiplication, then z = Ow € S. In other words, the additive identity for V 
must be in S, so (A4) holds for S. To validate (A5) for S, use the fact that if 
w <5, then lw and (—1)w are both in S by M(1) so that lw+(—l)weS 
by A(1), and hence 
w + (—1)w = lw + (—1)w = [1 + (—1)]w = Ow =z. 
In other words, (—1)w € S is the additive inverse of w for each w € S. Notice 
that this means it is legitimate to define —w to be (—1)w so that subtraction 
w—Ww=zZ=0 and 
v—-w=v+4 (—1)w is justified. 
I 

4.1 Spaces and Subspaces 
413 
Example 
Flatness 
e 
The nullspace N(A) of Amxn is a subspace of F". 
e 
The set of all solutions x for a consistent m x n system of linear equations 
Ax =b 
is not a subspace of F" when b 40. 
e 
The set of all n x n matrices such that trace(A) = 0 is a subspace of the 
space F"*", 
e 
The set of all nxn matrices such that det(A) = 0 is not a subspace of 
the space F"*". 
e 
The set of all n x n hermitian matrices is a subspace of F"*". 
e 
The set of all n x n normal matrices is not a subspace of F"*". 
e 
The set of all real-valued continuous functions defined on [a,b] is a subspace 
of the space of real-valued functions defined on [a, }]. 
e 
Similarly, the set of real-valued functions that are differentiable on [a,}] is 
a subspace of the space of all real-valued functions defined on [a, }]. 
Consider identifying all of the subspaces of IR". In a general vector space V 
the trivial space {0} C V is always a subspace of VY, and the whole space 
Y is regarded to be a subspace of itself, so these two subspaces are present in 
every vector space. Consider R*. As illustrated in Figure 1.2.1 on page 5 and 
Figure 4.1.2 below, vector addition in R? and R?® is visualized by using the 
parallelogram law, which states that for two vectors u and v, thesum u+v is 
the vector defined by the diagonal of the associated parallelogram. In particular 
this means that in addition to the trivial space and the whole space, lines through 
the origin in R? are subspaces of R? because A(1) and M(1) hold—the sum of 
two points on a line £ through the origin is again on £, and a scalar multiple 
of a point on L is also on L. But what about straight lines not through the 
origin? No, they cannot be subspaces because subspaces must contain the zero 
vector (i.e., they must pass through the origin). 
How about curved lines through the origin—can some of them be subspaces 
of R?? Again the answer is no. As depicted below in Figure 4.1.1, the paral- 
lelogram law indicates why the closure property (Al) cannot be satisfied for 
lines with a curvature because there are points u and v on the curve for which 
u+v, the diagonal of the corresponding parallelogram, is not on the curve. And 
any piece of the plane (e.g., quadrants, polygons, etc.) other than lines through 
the origin will not be closed with respect to addition or scalar multiplication. 
Consequently, the only subspaces of R? are the trivial subspace, lines through 
the origin, and the whole space. 

; 
nm) 
be Purpose @ a 
t 
- 
FIGURE 4.1.1 
FIGURE 4.1.2 
In R?® the trivial subspace, lines through the origin, and the whole space 
are again subspaces, but there is also another kind—planes through the origin. 
If P is a plane through the origin in R®, then, as shown in Figure 4.1.2 above, 
the parallelogram law guarantees that the closure property for addition (A1) 
holds—the parallelogram defined by any two vectors in P is also in P so that 
if usv € P, then u+v € P. The closure property for scalar multiplication 
(M1) holds because multiplying any vector by a scalar merely alters its length 
or direction, but its angular orientation does not change so that if u € P, then 
au € P for all scalars a. Lines and surfaces in R® that have curvature cannot be 
subspaces for essentially the same reason depicted in Figure 4.1.1. And any piece 
of R? (e.g., octants, polytopes, etc.) other than lines and planes through the 
origin will not be closed with respect to addition or scalar multiplication. So, the 
only subspaces of R® are the trivial subspace, lines and planes through the origin, 
and the whole space itself. What has emerged is that the concept of a subspace 
in the visual spaces R? and R® has an obvious geometrical interpretation—they 
are precisely the flat surfaces that contain the origin. 
This means that although you cannot use your eyes to see "flatness" in 
higher dimensions, your mind can conceive it and deal with it through the no- 
tion of a subspace. Whenever you need an intuitive guide while contemplating 
subspaces of an abstract vector space, think in terms of flat surfaces passing 
through the origin. 

4.1 Spaces and Subspaces 
415 
Spanning Sets 
Example 
In higher dimensional and abstract vector spaces there is more "space" than in 
R? and R? , so there is more room to hold "flat" surfaces other than lines and 
planes. Consequently a mechanism is needed to describe the generalizations of 
lines and planes. The idea of a spanning set was earlier introduced in Definition 
1.3.1 on page 16 for subspaces of F", and the same notion applies to abstract 
vector spaces. For the sake of completeness, the concept is restated below. 
Sal. Theovent: Tek y be a 8 space over FE, and let. x é y. 'The 
; bet, © Pot all vectors that can be written as a linear combination 
Lor vectors from xX isa subspace of V, in which case S is called the 
subspace that is spanned (or generated ) by X, and & is called a 
er 'spanning set for 6. This 1 
is denoted by writing a 
= span AMY: 
; 
oe TEX is fede. then S- is cad to be finitely generated. Every s 
sub- . 
space of F" is finitely generated. 
Proof. 
The reason that S = span(#) is a subspace of VY is because the two 
closure properties (A1) and (M1) are satisfied. That is, if x = }>,éx; € S 
and y = >0),x; € S are two linear combinations of vectors from 4, then 
the sum x+y = )),(& + )x; is also a linear combination in span(¥), 
and for any scalar 8, Bx = 5°,(6&)x; € S is also a linear combination in 
span (4). To prove that every subspace S C F" has a finite spanning set, first 
note that if S = {0} is the trivial subspace, then it is spanned by z = 0. 
If S # {0}, let O # x, € S, and consider span(xi). If span{xi} # S, 
then there is another vector 
xg € S such that xg # a ,x,;, 
so when used 
as columns in a matrix, rank [x;] = 1 and rank [x;|x2] = 2 (because both 
columns are basic). If span {x1,x2} # S, then there is another vector x3 € S 
such that x3 # a,x; + a2X2, 
so rank [x;|x2|x3] = 3 (all columns are again 
basic). But this process cannot continue indefinitely because eventually it would 
force rank [x1 |X2| +++ |Xn+ilnx(n41) = +1, which is impossible because there 
are only n rows. Thus there must be a finite set V = {x1,Xo,...,x,} that 
spans S. 
& 
e 
The set of polynomials P,, = {p(t) = o¢_, ant® | ax € F} with deg p(t) <n 
is a finitely generated vector space over F that is spanned by the polynomials 
Cit tee ene: 
e 
The set of all polynomials over F is not finitely generated—it is spanned by 
the infinite set of polynomials fie Cit: + 
: 
e 
The set of real-valued functions that are continuous and periodic with period 
2x on (—7,7) is a vector space over R that is not finitely generated—it is 

416 
Bases 
Chapter 4 
Vector Spaces 
spanned by the infinite set of functions 
B 
1 
cost 
cos2t 
sint 
sin2t 
sin3t 
\ 
={ Dar' fT ' 
JT ne 
JT' 
JT ' 
JT ' 
Given a spanning set ¥ for a vector space Y, there may be more than one way 
to express a given vector x € V as a linear combination of vectors from 1. For 
example, let P C R® be the space spanned by 
x= {m= (1),0=(-t).x=(1)}, 
1 
and consider e; = (2) 
€ P. Since 
0 
e1 = (- 
=") x, + (- ") +ax3 
forall 
acR, 
there are infinitely many different ways to express e; as a linear combination 
of vectors from . The reason for this is because there are redundancies in %. 
In other words 4 
is a linearly dependent set—e.g., x3 = 2x; + xX. If xg is 
removed from 4, then 4' = {x1, x2} is still a spanning set for P, but now 
Q\X1 + @2xX2 = e; has a unique solution for a, and ag. This can be seen by 
writing a,x; + a2xo =e; asthe 3 x 2 linear system 
1 
1 
x 
1 
1 
i 
(: -1) (a) =(0}, where 
rank (1 -1) =2, 
1 -1 
: 
0 
1 
-1 
so there is a unique solution given by a; = 1/2 = ag. These considerations 
motivate the following definition. 
4.1.4, Definition. A linearly independent spanning set for a vector space 
Y is called a basis for V. 
e 
Every finitely generated vector space V # {0} has a basis. 
(4.1.1) 
e 
The trivial subspace Z = {0} is a special case because it has no 
linearly independent spanning set, so its basis is the empty set 0). 
Statement (4.1.1) is immediate because if ¥ is a finite spanning set for 
V # {0}, and if redundant vectors (vectors that are combinations of the others) 
are eliminated one-by-one from ¥ until the resulting set is linearly independent, 
then it must still be a nonempty spanning set, and hence a basis, for V. The 
next theorem presents alternate characterizations of bases for subspaces of F". 

4.1 Spaces and Subspaces 
417 
Proof. 
First prove that (4.1.2) <=> (4.1.3) and then show (4.1.4) <=> (4.1.2). 
(4.1.2) ==> (4.1.3): Use an indirect argument. Let B be a basis for S, and 
suppose that B is not a minimal spanning set. Then there is a spanning set 
X = {X1,X2,...,X«} for S in which k <n. Since each bj; € S, there are 
scalars a,; such that 
k 
be= Sagi 
tors = 150. ean. 
(4.1.5) 
i=1 
If Axxn = [aij], and if the b's and x's are placed as columns in matrices 
Brmxn = [bi|b2|-+-| 
bn] and = 
Xmxxe = [x1 |x2|+-+| 
xx], 
then (4.1.5) can be written as 
B = XA. Since rank(A) < k <n, it follows 
that 
N (A) #40 (by Theorem 2.5.6, page 209). If z 
#0 is such that Az = 0, 
then Bz = 0. But this is impossible because the columns of B are linearly 
independent, and hence N (B) = O (again by Theorem 2.5.6). Therefore, the 
supposition that there exists a spanning set for S containing fewer than n 
vectors must be false, and thus 6 is a minimal spanning set. 
(4.1.3) == (4.1.2): Again use an indirect argument. If B is a minimal spanning 
set for S but not a basis for S, then B is linearly dependent, so some b; is a 
linear combination of the other b's. This means that the set 
BSS Dijpnee, 
D1 Dat <2, Dn} 
still spans S, but B' contains fewer vectors than B, which contradicts the fact 
that B is a minimal spanning set, and thus B is a linearly independent spanning 
set for S. 
(4.1.4) ==> (4.1.2): If B is a maximal linearly independent subset of S but not 
a basis for S, then there exists a vector 
v € S such that v ¢ span(B). This 
means that the extension set 
Bea 
v ves 
bi bea, 
by v} 

418 
Chapter 4 
: 
Vector Spaces 
is linearly independent (recall (1.3.5) on page 19). But this is impossible because 
B is a mazimal linearly independent subset of S, and thus B is a basis for S. 
(4.1.2) => (4.1.4): Suppose that B is a basis for S but not a maximal linearly 
independent subset of S. Corollary 2.4.5 on page 182 ensures the existence of 
a maximal independent subset, so let 
 C S be a maximal independent subset 
that contains more vectors than B. The previous argument in (4.1.4) => (4.1.2) 
shows that J is a basis for S. But this is impossible because it was proven in 
(4.1.2) => (4.1.3) that a basis is a minimal spanning set, and 6 is a spanning 
set containing fewer vectors than Y. Thus 6 must be a maximal linearly inde- 
pendent subset of S. 
& 
Vector Coordinates 
Given a spanning set S = {s;,So,...,S,} for a vector space Y, there may be 
more than one way to represent a vector v € VY as a linear combination of the 
1 
0 
1 
spanning vectors in S. For example S = {(?) = 84, (:) = 85, (2) = sa} 
1 
1 
2 
0 
spans a plane P through the origin in R®, and v = (2) € P can be written 
2 
as 
v=-—s,;+8S2+8s3 
aswellas 
v= Q25p. 
This happens because S is linearly dependent. However if B = {b,, bo,..., bp} 
is a linearly independent spanning set for 
VY —i.e., a basis for Y—then the 
representation of v in terms of the basis vectors is unique. 
4.1.6. Theorem. 
If 6 = {bj,bo,...,b,} is a basis for a vector space 
Y, then there is a unique way to express each v € Y as a combination 
v =a,b; + a9b2+-:-+ a,b, of the basis vectors. 
Proof. 
If 
v=ayb; +aqb2+-+:-+a,b, and v = Bib; + Bob +---+ Brbn, 
then subtraction produces 
0 = (a1 — 61)bi + (a2 — B2)be +--+ + (An — Bn)bn. 
Hence (a, — 6;)=0 for each i= 1,2,...,n (by (1.3.3), page 18), and thus the 
representation in terms of basis vectors is unique. 
& 
The realization in Theorem 4.1.6 motivates the following definition. 

4.1 Spaces and Subspaces 
419 
41 
1. 
7. 
Definition, The coetielonte ay in ae representation ve 
a ae ab; - 
of ve V in terms of a basis B = '{b1,by,.. 
bn} for V are called the 
coordinates of 
Vv with oe to ir and they are denoted by Sane 
; a1 : 
Og 
: 
ING 
While there is only one way to represent a vector in terms of a given basis, 
there can be many different bases for a space, so it is natural to wonder how 
the coordinates 
[v]gx of a vector v with respect to one basis B are related to 
the coordinates [v]z of v with respect to a different basis B'. It turns out 
to be just a nonsingular transformation in the sense that [v]p: = C[v]g for a 
nonsingular matrix C, the exact nature of which is developed on page 514. 
Dimension 
Example 
A vector space can have infinitely many different bases—e.g, any two non-colinear 
vectors in a plane P through the origin in R® will be a basis for P. However, 
while two bases B, and By for 
asubspace S C F" need not be equal, Theorem 
4.1.5 guarantees that 6, and 6B. must contain the same number of vectors 
because each is a minimal spanning set, so they cannot be different in size. This 
leads to the following definition. 
4.1.8. Definition. The dimension of a finitely generated vector space V 
is defined to be 
dimY = number of vectors in any basis for V. 
= number of vectors in any minimal spanning set. 
= number of vectors in any maximal independent subset of V. 
If VY is not finitely generated, then VY is said to be infinite dimen- 
sional. 
e 
If Z = {0} is the trivial subspace, then dim Z = 0 because the basis for 
this space is the empty set. 
e 
If £ isa 
line through the origin in R?, then dim £=1 
because a basis for 
L consists of any nonzero vector lying along CL. 
e 
If P isa plane through the origin in R?, then dim P = 2 because a minimal 
spanning set for P must contain two vectors from P. 

420 
Stuff 
Chapter 4 
Vector Spaces 
i 
0 
0 
e 
dimF* =3 
because the three unit vectors {( 
0) 
' (2) 
; (°) \ 
constitute 
a basis for R°. 
e 
dimF" =n because the unit vectors 
S = {e1,€2,...,€n} in F" form a 
basis. This basis S is called the standard basis for F". 
e 
dimF"*" = mn because the set of matrices B = {Ej;j} in 
which E;; has a one in the (i, j)-position and zeros elsewhere 
(4.1.6) 
is a basis for F™*". This basis B is called the standard basis 
fone ss 
e 
Thespace 73 of 3 x3 of matrices for which trace(A) =0 has dim 
73 = 3 
tO 
Gt 
0 0 
: 
: 
because ae: y fe wo le air is a basis for 7. 
e 
Thespace P,, of polynomials having coefficients in F and deg p(x) < n has 
dim P,, =n because {1, x, 27,..., 2} is a basis for Pp. 
e 
The space P of all polynomials having coefficients in F is infinite dimen- 
sional because P is not finitely generated. 
The previous examples show that in a loose sense the dimension of a space 
is a measure of the amount of "stuff" in the space—e.g., a plane P in R? 
has more "stuff" in it than a line L, but P contains less "stuff" than the 
entire space R°. Since subspaces are generalizations of flat surfaces through the 
origin. The concept of dimension provides a way to distinguish between "flat" 
objects according to how much "stuff" they contain. Another way to think about 
dimension is in terms of "degrees of freedom." In the trivial space Z = {0} there 
are no degrees of freedom—you can move nowhere—whereas on a line there is 
one degree of freedom, length; in a plane there are two degrees of freedom, length 
and width; in R® there are three degrees of freedom, length, width, and height. 
The higher the dimension, the more wiggle room there is. 
4.1.9. Theorem. For vector spaces M and N such that M CN, the 
following statements are true. 
e 
dimM <dimWN. 
e 
If dimM =dimN, then M=N. 
Proof of (4.1.7): Let dim M =m and dimN =n. If m>n, then there would 
exist a linearly independent subset of NV (namely, a basis for M ) containing 
more than n vectors, which is impossible because dim 
WN is the size of a maximal 
independent subset of VV. Thus m <n. 
Proof of (4.1.8): If 
m=n but M#N, then there exists a vector x € N but 
x ¢ M. Consequently, if By, is a basis for M, then x ¢ span(By), and the 

Addition 
Example 
4.1 Spaces 
and Subspaces 
421 
extension set € = By, U {x} is a linearly independent subset of N containing 
m+1=n-+1 
vectors. But this is a contradiction because dim = n is the 
size of a maximal independent subset of NV. Thus M=WN. 
& 
of Subspaces 
Proof of (4.1.9): The two closure properties (A1) and (M1) in Definition 4.1.1 
on page 410 hold for 
S = 
¥ +. To see that (A1) is valid, let u,v € S so that 
u=x,+y1 and v = x2+ yo, where x1,x2 € ¥ and yi,yo € J. Because 
X and Y are closed with respect to addition, it follows that x; +xg € 4 and 
yity2 € ¥, and therefore u+ v = (x1 + x2) + (yi +y2) € S. (M1) holds for 
S because closure of scalar multiplication for Y and Y ensures that if x 
Ev 
and yé€ y, then ax € # and ay € ¥ for all scalars a, so 
uEex+yY = 
u=x+y, 
xEeErXx,yeyY 
= 
au=ax+ayers+y. 
Proojoy; 41. 10)s lf Sx 
4 X1,X9,.. 
4 Xp yp aud Oye AV Vo,--3, 
Ver are 
respective spanning sets for Y and ), then z¢€ ¥+ J) means that 
iP 
t 
zZ=x+y, 
where 
xe€v, yey 
=> 
<a 
oy 
and 
y=) _Biyi 
i=1 
i=1 
r 
t 
== 
4= Dear: +> Biyi € span(Syx USy). 
Hf 
i=1 
i=1 
6.lt L4eG R? and lk © R? are subspaces defined by two different lines 
through the origin, then £; +L. = R?. This follows from the parallelogram 
law—sketch a picture for yourself. 
e 
If 2; CR® and £2 C R? are subspaces defined by two different lines through 
the origin, then £; + £2 =P 
is a plane through the origin. 
e 
If 
PC R® and CCR 
are respective subspaces defined by a plane through 
the origin and a line £ not contained in P then 
P+L= R?. 
e 
If 
PCR® and LCR? are respective subspaces defined by a plane through 
the origin and a line £ contained in P then 
P+L=P. 

Proof. 
Assume that 49 
Y 4 4 or Y. Otherwise there is nothing to prove 
because if YNY=4, then 
YX CY, sco X+V=Y. Similarly if TN V =), 
then 
¥+ 
Y=. For 
YNYV FA 
or Y, the strategy is to construct a basis for 
X%+¥Y and count the number of vectors it contains. Suppose that dim YN Y =t, 
and let S = {z1,Z2,...,z¢} be a basis for VN 
Y. Since 
SC X and SCY, 
there must exist vectors {x1,X2,...,Xm} and {y1,y2,..-,¥n} such that 
[sp 
CP ee 
ee 
basis for 
and 
By 
ay er 
aay ies yy, 
v= S 
basistory 
(see the example on page 153). It follows from (4.1.10) that B = Bx 
UBy spans 
* + Y. The aim is to show that B is also linearly independent so that it is a 
basis for 4 + Y. To establish independence, observe that if 
t 
m 
n 
Se (OuvIS == Ds, fax, air 'e VeVk = 0, 
(4.1.11) 
oh 
j=1 
k=1 
then 
n 
t 
m 
So Ye = 
Yo az + > Bix; 
EX, 
k=1 
il 
wat 
It is also true that 7, Yk¥k € VY, so DO, WeYR € XY NY, and hence there must 
exist scalars 6; such that 
n 
G 
n 
t 
Ss VY k = yy 6;Z; 
or, equivalently, 
A WY k — De a 
Oo 
ae 
k=1 
i=l 
i=1 
Since By is an independent set, it follows that all of the Yr'S (as well as all 
6; 's) are zero, and (4.1.11) reduces to S~S_, aiz; + eps Oe But Gyn 
also an independent set, so the only way this can hold is for all of the a;'s as 

4.1 Spaces and Subspaces 
423 
well as all of the §;'s to be zero. Therefore, the only possible solution for the 
a's, B's, and y's in the homogeneous equation (4.1.11) is the trivial solution, 
and thus B is a linearly independent spanning set for 4% +, and hence a basis 
for 
+ Y. Thus 
dim (4+ Y) =t+m+n=(t+m)4+(t+n)-t 
=dim¥+dimy—dim(#ny). 
If By and By are respective bases for subspaces Y and Y, then (4.1.10) 
on page 421 ensures that By 
UBy spans 4+, but in general By UBy need 
not be a basis for 4 + Y. In other words, it is not always possible to paste 
two bases together to create a basis for a sum. However, a corollary to Theorem 
4.1.11 says that pasting bases together is legitimate when 1M Y = 0. 
4.1.12. Corollary. If By and By are respective bases for subspaces 1 
and ¥ ofa vector space V, and if x Y= 0, oo ae Bx Z 
is 
a basis for ¥ - 
y. 
Proof. 
If dim# =r and dimy = s, andif (NY =0, then dim(V¥+Y) =r+s 
by Theorem 4.1.11. Since B = By UBy contains r+s vectors and is a spanning 
set for ¥ +, all that is required is to observe that 6 is linearly independent. 
If GB is linearly dependent, then redundant vectors could be removed to create 
an independent set B' containing less than r+s vectors with 6' still spanning 
X +, which is impossible because this would make dim(¥ + Y)<r-+s. 
& 
Example (Eigenvalue Bounds) 
Theorem 4.1.11 is a handy item for your toolbox, and the following lemma il- 
lustrates why by highlighting a result concerning bounds for the eigenvalues of 
a hermitian matrix. 
4.1.13. Lemma. Let A € F"*" be a hermitian matrix with eigenvalues 
dt > Ap > +++ > An. For each k-dimensional subspace M C F", there 
are vectors x,y € M with unit 2-norm such that 
LAK Sm Ap 
and  y AY = Agiie). 
(4.1.12) 
Proof. 
To prove the first inequality, let {u;,U2,...,U,} be an orthonormal set 
of eigenvectors such that Au; = A;u,; foreach j, and let S = span fq 
Ups caer Ly te 
Since M+S C F", Theorem 4.1.9 ensures that dim(M +S) < dimF" = n. 
Applying Theorem 4.1.11 together with the fact that dimS =n —k+1 
yields 

424 
Chapter 4 
f 
Vector Spaces 
n > dim(M + S) = dim M + dim S 
— dim MNS 
=k+(n—k+1)—-dimMnS=n+1—dimMNS 
= 
dimMnS=21. 
Consequently, there is a nonzero vector x € MNS C S. It can be normalized and 
expressed as a linear combination x = )0/_, ajuj with x"x = Bie log? = 
so that Ax = }0_, ajAjuy and 
nm 
nm 
x°Ax = So |ay)?Aj S Awd legl? = Ar, 
j=k 
j=k 
which is the first inequality in (4.1.12). The second inequality is obtained by 
applying the first one to —A, or the above argument can be repeated with 
S = span tuj,...,.U,—2.4)-. 
0 
Courant-—Fischer! Minimax Theorem 
Lemma 4.1.13 paves the way to variational formulas for the eigenvalues of a her- 
mitian matrix that are improvements over those in Theorem 3.4.12 on page 347. 
Ernst Sigismund Fischer (1875-1954) discovered and proved this result for matrices in 1905, 
and Richard Courant (1888-1972) extended it in 1920 to infinite-dimensional operators. Fis- 
cher received his doctorate at the University of Vienna in 1899, 
4 
where we wrote on the theory of determinants, and went on to 
study with Minkowski (page 38) at Gottingen in Germany. Fis- 
cher was primarily an algebraist, but like so many of his genera- 
tion he contributed to multiple areas of mathematics. He perhaps 
became best known for the Riesz—Fischer theorem, an important 
result in the theory of Lebésgue integration that says the space 
of all square-integrable functions is complete in the sense that all 
Cauchy sequences in the space converge. Furthermore, Fischer's 
work on orthogonal functions helped lay the foundation for what 
later became known as Hilbert spaces. 
ERNST S. FISCHER 
Richard Courant (1888-1972) was a student of David Hilbert at Géttingen in Germany, 
and in 1924 they jointly published the text Methoden der mathematischen Physik. But, as is 
often the case, the junior partner did most of the work—the contribution from Hilbert was 
said to be in the form of lecture notes. With the help of his own 
student, Kurt Friedrichs (1901-1982), Courant initiated the second 
volume of "Courant—Hilbert" in 1925. In spite of fighting for Ger- 
many (and being wounded) in World War I, the Nazis removed 
Courant from his position as the director of the Mathematics In- 
stitute at Gottingen in 1933, and after a short time in England 
he accepted a relatively low paying visiting position at New York 
University in 1934, which at that time did not have a particularly 
strong mathematics program. Courant's personality and reputation 
together with his Gottingen connections helped him attract high 
quality lecturers and visitors to NYU, and in 1935 he was given a 
professorship along with the task of building up the mathematics 
department. Gottingen was the model, and the program flourished 
on the mathematical talent that was leaving Germany. Friedrichs 
ie aes LN 
and James J. Stoker (1905-1992) were among the first to join Courant in 1937, and then many 
other notables followed. In 1946 Courant's program was titled The Institute for Mathematics 
and Mechanics. Upon Courant's retirement in 1958 Stoker succeeded Courant as the pro- 
gram's director, and in 1965 the institute was renamed The Courant Institute of Mathematical 
Sciences. 

Proof. 
It follows from Lemma 4.1.13 that for any k-dimensional subspace M, 
min x"Ax < Ax, 
(4.1.14) 
eM 
= 
Illg=1 
so if M is allowed to vary over all k-dimensional subspaces, then 
_ 
max — 
min x Axts dj: 
dim M=k in i 
"as 
ae 
There is at least one k-dimensional subspace M' for which equality is attained. 
Let {u1, U2,...,Un} be an orthonormal set of eigenvectors for A such that 
Au; =,u; for each jy 
cand let M' = apae {uj,Uo,...,uz}. If x EM' with 
Keel baeuex 
ne 6;u; with ie (6;|? = 1 so that 
k 
k 
x*Ax = D716)? 2 AK > 16il = Av, => 
min x*Ax > rz. 
i=1 
i=l 
- 
fixlig=2 
This together with (4.1.14) means that min xea" x*Ax = Ax, and thus the 
l>llo=1 
first expression in (4.1.13) is proven. The second expression follows from the 
first by replacing A with —A and realizing that the eigenvalues of —A are 
fli > Me 2 +++ > Mn, where pe = —An—K+1, Or equivalently Ay = —Un—K+1, 
along with the fact that for a set S of real numbers, maxaes{a}=— minges{—a} 
and minges{a} = —maxges{—a}. 
fH 
Exercises for section 4.1 
4.1.1. Let V bea 
vector space over F. 
(a) Prove that the additive identity (the zero vector z = 0) in V 
is unique. 
(b) For each x € VY, prove that the additive inverse y of x is 
unique. 

426 
Chapter 4 
, 
Vector Spaces 
4.1.2. With the usual addition and multiplication, determine whether or not 
4.1.3. 
4.1.4. 
4.1.5. 
4.1.6. 
4.1.7. 
4.1.8. 
the following sets V are vector spaces over the indicated scalar field F. 
(a) V=R over F=R. 
(b) V=C over F=C. 
(c) The set V of rational numbers over R. 
Why must a real or complex nonzero vector space contain an infinite 
number of vectors? 
Determine which of the following subsets of R" are in fact subspaces of 
R" when n> 
2. 
(EE) goes st gray Beall 1 
(py ix | a = or 
(c)' {x | 23903 = OF, 
(d) '| 
La-ol, 
(e) '| Sa-1). 
Determine which of the following subsets of R"*" are in fact subspaces 
or RR' * 
(a) The symmetric matrices. 
) The diagonal matrices. 
) The nonsingular matrices. 
(d) The triangular matrices. 
) The upper-triangular matrices. 
) All matrices that commute with a given matrix A. 
) All matrices such that A? = A. 
Explain why the following sets S are not subspaces. 
(a) The set S of all solutions x € F" for a consistent nonhomoge- 
neous m X n system of linear equations Ax = b. 
(b) The set S of all n x n normal matrices in F"*". 
For subspaces 4', Y of a vector space Y, explain why 7+) = 0 implies 
thaie A= 0S), 
Let 4 be a subspace of F™*', and let S = (81, 
Sa) aeshn ree 
(a) For Amxn = [Si|S2|--- |Sp], explain why span(S) = ¥ 
if 
and only if the linear system Ax = b is consistent for every 
bed, 
(b) Use this to determine if S = {(1,1,1), (1,—1,-1), (3, 1,1)} 
spans R°. 
—

4.1 Spaces and Subspaces 
427 
4.1.9. 
4.1.10. 
Ai. 11, 
4.1.12. 
4.1.13. 
4.1.14. 
4.1.15. 
4.1.16. 
For 
A € R™*" and ¥ C R"', the set A(4) = {Ax|x € Y} that 
contains all possible products of A with vectors from 24 is called the 
set of images of © under mapping (or transformation) by A. 
(a) Prove that if XY is a subspace of F", then A(%) is a subspace 
Olena 
(b) Prove that if {s],so,...,S,} spans 7, then {As;, Aso,..., As,} 
spans A(%). 
Determine a basis and the dimension of the space spanned by the set 
1 
1 
2 
1 
3 
" 
2 
0 
8 
1 
3 
o == 
Bs 
We 
et 
ok 
ee re oe 
Bd 
a 
3 
2 
8 
1 
6 
For a vector space V, and for M,N C VY, explain why 
span (MUN) = span(M) + span(N), and then conclude that 
dim (span (M UN) ) = dim (span (M) ) + dim (span (N) ) 
— dim (span (M) NM span (N)). 
Let XY and Y be two subspaces of a vector space V. 
(a) Prove that the intersection 4M YJ is also a subspace of V. 
(b) Explain why the union 4% UY need not be a subspace of VY. 
For vectors S = {s1,S2,...,S,} from a vector space V, prove that 
span (S) is the intersection of all subspaces ¥ C V that contain S. 
Hint: For M = iy X, prove span(S) C M and M C span(S). 
Explain why every linearly independent subset of n vectors from an 
n-dimensional vector space VY must be a basis for V. 
Determine the dimension of the subspace of n x n symmetric matrices 
O 
nxn 
inv 
os: 
Let 7, = {A € F"*" |trace(A) = 0} be the subspace of trace-zero 
matrices. 
(a) Determine dim 73. 
(b) Determine dim7,, for a general value of n. 

rn. * Pelvis y 
a He = 2 (sin Retain eS 
The defining properties for a linear function have much the same "feel" 
as the closure properties (A1) and (M1) from Theorem 4.1.2 on page 412 that 
characterize the concept of a subspace, but there is more to it than just a "similar 
feel." Linear functions and subspaces are intimately related as explained below. 
Proof. 
To argue that R(f) is a subspace of M, demonstrate that the closure 
properties (A1) and (M1) on page 412 hold. To establish (A1), suppose that 
y1,y2 € R(f) so that there are vectors x1,x2 € NV such that y; = f(x;) and 
y2 = f(x2). The linearity of f ensures that 
yi t+ yo = f(x) + f(xe) = f(x1+xe)€ R(f) => 
(Al). 
To show (M1), suppose that a € F and y € R(f). Consequently, there is some 
|
x €N such that f(x) =y, so the linearity of f produces 
ay = af(x) = flax) € R(f) => 
(M1). 
Proving that N(f) is a subspace of N is similar. If x1,x2 € N(f), then 
f (x1) = 0 = f(x2), so the linearity of f produces 
f(x1 + X2) = f(xi) + f(xz)=0+0=0 
=> 
xi +x2EN(f) — (Al). 
If 
ae F and xe N(f), then f(x) =0 
and linearity yields 
f(ax) =af(x)=a0=0 = > axe N(f) = 
(M1). 
Of 
* Some texts call N(f) the kernel of f. 

_ 4,2 The Fundamental Subspaces 
429 
Rank Plus Nullity Theorem 
For a linear function f : V + M_ between two vector spaces VV and M over 
F, dim R(f) and dim N(f) are respectively called the rank' and nullity of f, 
and the next theorem reveals a fundamental connection between them. 
Proof. 
Suppose that dim N(f) = s, dimR(f) =r, and dimN = n. Let 
B, = {x1,X2,...,Xs} be a basis for N(f), and extend B, to a basis for N 
with extension vectors By = {Z1,Z2,...,Zn—s} so that B, U Bo is a basis for 
N. The set B = {f(z1), f(z2),..-, f(Zn—s)} must be a basis for R(f) because 
B is a linearly independent spanning set. To see that B spans R(f), observe 
that if y € R(f), then y = f(x) for some x € N, so the linearity of f yields 
x=) Ox+) 
bia —>y =f(x) => ofl) + >) a fl%) = Ss 
ai f (zi). 
i=l 
i=l 
i=l 
i=l 
= 
To argue that B is linearly independent, equate a linear combination to zero 
and again use the linearity of f to write 
O= >) Fis) F | 
Dee.2| 
9D) Bye NG) — Span (Bi). 
i=1 
i=l 
i=1 
Consequently, there are scalars y; such that 
s 
n—s 
8 
n—s 
Sox. =>) 
Bi => Dx) Biz = 0 => cach %=0 and 6; =0 
i=1 
i=1 
i=1 
i=1 
because B,UBz is linearly independent. In particular, each §; being zero means 
that B is linearly independent. 
In loose terms this "rank-plus-nullity" theorem is analogous to a conserva- 
tion law in the sense that it says there is only a limited amount of "stuff" to 
go around, and a balance must always be maintained. The more "stuff" packed 
into R(f), the less there can be in N(f), and vice versa. 
Composition of Linear Functions 
It follows from the definition of linearity that if 
g: MW ~ P and f:P—> M 
are 
linear, then the composition fg: N — M defined by (fg)(x) = f(g(x)) is also 
linear. A consequence of the proof of Theorem 4.2.3 is that rank of a composition 
cannot exceed the rank of either function. 
The term "rank" is used in this context to be consistent with the result in Theorem 4.2.8 on 
page 433. 

Predh First note that 
t 
RUfo) C R(f) because if y € R(fg), then there must 
exist an x such that 
= (fg)(x) = f(g(x)) => y = f(z) for some z (namely z = 9(x)). 
Therefore, dim R(fg) < dim R(f) by Theorem 4.1.9 on page 420. Now suppose 
that dim =n, and construct a basis B for R(g) as described in the proof 
of Theorem 4.2.3. That is, let By = {x1,X2,...,Xs} bea basis for N(g), and 
let Bo = {21,Z2,...,Z%n—s} be vectors that extend B, to a basis for N so that 
B = {9(z1), 9(Z2),---;g9(@n—s)} is a basis for R(g). Thus dimR(g) =n—s. 
Since {f(9(z1)), f(g(z2)),-.-,f(g(@n—s)} must span R(fg), it follows that 
dim R(fg) 
<n-—s (because a basis is a minimal spanning—see Theorem 4.1.5, 
page 417), and thus dim R(fg) <dimR(g). 
Range and Nullspace of a Matrix 
Not only is the range of every linear function f : VM — M a subspace of 
M, the reverse is also true in the sense that if S is a finite dimensional sub- 
space of M then S is the range of some linear function. In particular, if 
span {S1,S2,...,Sn}=<S, then the function f: 
:F" > M defined by 
LE OWES on ln) = Somes 
is linear and R(f) = S. For this reason ae spaces and their subspaces are 
often referred to as linear spaces. 
Among the most fundamental linear functions f : F" > F™ are those 
defined by a given matrix A € F"*" via the mapping f(x) = Ax. This 
therefore means that A generates a subspace of F" by means of its range 
R(f) = (f(x) 
|x € F"} = {Ax|x 
€ F"} CF". 
(4.2.1) 
v1 
. 
v2 
Since Ax = [Axi | Axo| +--+ | Aan] 
= a1 Anj@y isa linear combination 
in 
of the columns of A, it is apparent that the set of all images Ax is the same 
as the set of all linear combinations of the columns of A. In other words, R(f) 
in (4.2.1) is the space spanned by the columns of A. 
E

4.2 The Fundamental Subspaces 
431 
The nullspace of the linear function f(x) = Amynx is the subspace of F" 
defined by N(f) = {x € F"| Ax = 0} (i-e., all homogeneous solutions). This set 
was called the nullspace of A in Definition 2.5.2 on page 203 and was denoted by 
N (A). Theorem 2.5.3 on page 206 shows that a linearly independent spanning 
set (and hence a basis) for N(A) is B = {h,hg,...,h,_,}, where the h,'s 
are the vectors in the general solution of Ax = 0 obtained by row-reducing A 
to echelon form and solving for the r basic variables in terms of the n—r 
free 
variables as described on page 205. 
The spaces R(A) and N(A) along with their counterparts R(A*) and 
N (A*) are fundamental components in the structure of finite dimensional vector 
spaces, and consequently these spaces are frequently referred to as being the "four 
fundamental subspaces." Below is a summary. 
4.2.5. Definition. The range and the nullspace of A € F™*" are 
defined to be the respective subspaces of F™ and F" 
given by 
R(A)={Ax|x¢F"} 
and 
N(A) = {x€ F"| Ax = 0}. 
R(A) is also called the column space! of A because it is the space 
spanned by the columns of A. 
e 
R(A*) = {ATy|y € F"} is the row space? of A because it is 
the space spanned by the rows of A (but stacked upright). 
« 
N(A') = fy © FUAy — 0} is the left-hand nullspace be- 
cause Aly = 0 => y'A = 0 (left-hand homogeneous solutions). 
Conjugate Spaces 
; 
t 
Since R(A*) and N(A*) are the respective row space and nullspace of A, 
it makes sense to call these spaces the conjugate row space and the conjugate 
nullspace, respectively. For real matrices there is of course no distinction between 
R(A') and R(A*), but for complex matrices it is worthwhile to note that 
matrices A,B € F"*" have the same column space if and only if A and B 
have the same column space, so 
RAM) 
Be ie RAS) 
RB?) 
and similarly, 
(4:22) 
NAD een (Ba N (A) = NB"). 
Because R(A) is the set of all "images" Ax of vectors x € F" under transformation by A, 
the range or column space is also called the image space of A in some places. 
Strictly speaking, the range of A" is a set of columns, while the row space of A is a set of 
rows. However, no logical difficulties are encountered by considering them to be the same when 
properly interpreted. 

Proof. 
If R(A) = R(B), then AQ =B 
for some nonsingular matrix Q (see 
Exercise 4.2.36), or equivalently, B* = Q*A*. Since Q* is also nonsingular, 
B*x = 0 if and only if A*x = 0, and thus N(A*) = N(B*). Conversely, 
assume that N(A*) = N(B*), and use the pseudoinverse properties (2.4.16) 
and (2.4.20) on pages 193 and 194 to conclude that 
A*(I- AAT)=0 = B*(I—-AAl)=0 = (I-AAT)B=0 
=> A(A'B)=B = R(B)C R(A). 
The same argument with A and B interchanged yields R(A) C R(B), so 
R(A) = R(B). By considering transposes (or conjugate transposes), all of the 
other implications in the theorem are consequences of the fact that A '\" B if 
and only if 
Ea = Eg (Theorem 2.2.12, page 152) together with the observations 
in (4.2.2) along with A '\" B < + R(A7) = R(B?) (see Exercise 4.2.31). 
Since zero rows are irrelevant in a spanning set, the following result is an 
immediate corollary of Theorem 4.2.6. 

4.2 The Fundamental Subspaces 
433 
For example, to determine if 
2 
1 
3 
.. 
2 
6 
= 
A= 
al 
b 
4 
and 
p= 
3 
4 
KrF 
OO 
PwWNH 
4 
1 
9 
2 
span the same subspace of R', place the vectors as rows in matrices A and B 
and compute 
ee 
es 
3} 
Tie 
22) 
OE 
a 
A Sulocesd 1S 
+ (001 1)=B, 
Bye 
ital 
lh 
ap! 
(Oh 
(9) 
0) 
6) 
One 
L 
ek 
(Ee 
al 
Be 2 3 1) > (6 O 4 |) =Ep. 
Since the nonzero rows in Ea and Eg agree, Corollary 4.2.7 ensures that 
span {A} = span {B} . 
Dimension of R(A) and N (A) 
By definition, dimension is the number of vectors in a basis—i.e., the number 
of vectors in a linearly independent spanning set. The set of all columns in 
A € F"*" spans R(A), but this set will not be a basis unless the entire set 
of columns is linearly independent, which it may or may not be. But restricting 
attention to just the basic columns in A does the job. Recall that Theorem 2.2.10 
on page 151 says that the set B of basic columns in A is linearly independent. 
And since every column in A is a combination of the basic columns (Corollary 
2.2.7, page 148), B is also a spanning set, and hence a basis for R(A). This 
observation produces the following theorem. 
and 
4.2.8. Theorem. dim R(A) = rank (A) = dim R (A?) = dim R(A*). 
Proof. 
The first equality stems from the remarks in the previous paragraph, 
and the others follow from the fact that rank (A) = rank (A") = rank (A*) 
(Theorem 2.4.9, page 184). 
The dimension of N (A) is now immediate because the rank-plus-nullity 
theorem (page 429) applied to f(x) = Ax yields the following result. 
4.2.9. Theorem. (Rank-plus-nullity for Matrices) For A ¢ F"'*", 
dim R(A) + dim N (A) = n, 
(4.2.3) 
or equivalently, dim N (A) = n — rank (A). 

434 
Chapter 4 
Vector Spaces 
Summary: Dimensions and Bases for the Fundamental Subspaces 
There are many different bases for the fundamental subspaces of a matrix, but 
a single row reduction of A to echelon form Ea, can reveal a basis for each 
fundamental subspace. Below is a summary for A € F"*" with r= rank (A). 
dim R(A) =r: The set of the r basic (or pivot) columns in A is a basis for 
R(A) (see Theorem 2.2.10, page 151). 
dim N(A)=n-—r: The set B = {hi,ho,...,hp--} 
in which the h;'s are 
the vectors in the general solution of Ax = 0 obtained by 
row-reducing A to echelon form and solving for the r_ basic 
variables in terms of the n —r free variables as described on 
page 205 is a basis for N(A) (see Theorem 2.5.3, page 206). 
Moreover, an orthonormal basis for N(A) can be produced 
from Ea by following the procedure described in the develop- 
ment of Theorem 2.5.5 on page 207. 
dim R (A7) =r: The set of the r nonzero rows in Ea (stacked as columns) 
is a basis for R(A') (see Corollary 4.2.7 and Theorem 2.2.8, 
page 149). 
dim N (A?) =m-—r: 
Row reducing the augmented matrix [A |I,,] > [Ea | P] 
produces a nonsingular matrix P such that PA = E,. The 
set of the last m—r columns in P? is a basis for N (A?) 
(see Exercise 4.2.32, page 454). 
Note: Conjugating bases for R(A') and N(A7) produces respective bases 
for R(A*) and N(A*). 
Example 
1 
To find bases for each of the four fundamental subspaces of A = (2 
3 Dm 
— 
dO Feb 
ew 
w ae 
perform the row reduction 
er ae a 
We 
I 220) 
{= 1/ae eee 
Aln=(2 Ab 
Ueesd 
we 
3 0) + (« 0 bol 
2/3 
-1/3 
0) 
=[EalP| 
3:6:cu aed tee une te 
Ome: Oy 0.5) 2/3. 2-5/3) ot 
to conclude that 
1 
Ps 
© 
BRA) 
(2), (:)} is a basis for R(A). 
1 
: 
is a basis for N (A). 
if 

4.2 The Fundamental Subspaces 
435 
e 
Brat) = 
is a basis for R (A). 
Oo 
i 
Fre 
O° 
1/3 
e 
Byvar) = {(-/) } 
is a basis for N (A?). 
The only reason for row reducing the augmented matrix [A |I] rather than just 
A by itself is to obtain a nonsingular matrix P such that PA = Ea so that 
a basis for N (A™) can be obtained. If By(qr) is not needed, then reducing 
A —> Ea 
suffices to produce bases for the first three fundamental subspaces. 
Ranges and Nullspaces of Products 
For all matrices Ay,x7 and Byx,, an elementary but useful observation is that 
R(AB)C R(A) 
and 
N(AB)2DN(B). 
(4.2.4) 
Each of these statements is evident because 
y€R(AB) = 
4xsuch that y =(AB)x = A(Bx) = 
ye R(A), 
and 
xeN(B) = 
Bx=0) 
=> 
ABx=0 
== 
xe 
(AB). 
Sufficient conditions for equality are given in Corollary 4.2.12 on page 437. 
Products of the form A*A and AA* 
are special not only because they 
occur throughout both the theory and applications of linear algebra but also 
because of the following realization 
4.2.10. Theorem. The following statements are true for all 
A € F*". 
R(A*A)=R(A*) 
and 
R(AA*)=R(A), 
(4.2.5) 
N(A*A)=N(A) 
and 
N(AA*)=N(A*). 
(4.2.6) 
Proof. 
The first statement in (4.2.4) ensures that 
R(AA*) C R(A), and since 
dim R(AA*) = rank (AA*) = rank (A) = dim R (A) 
(see Theorem 2.4.20, page 195), it follows from Theorem 4.1.9 on page 420 that 
R(AA*) = R(A). Replacing A by A* yields R(A*A) = R(A*). The state- 
ments N(A*A) = N(A) and N(AA*) = N(A*) are proven in the second 
part of Theorem 2.4.20 on page 195, but they also follow from the part of (4.2.4) 
that guarantees N (A) C N(A*A) and N(A*) C N (AA*) because 
dim N (A) = n—rank (A) =n -—rank (A*A) = dim N (A*A) 
and 
dim N (A*) = m—rank (A) =m-—rank(AA*) =dimN(AA*). 
& 

436 
Example 
Chapter 4 
: 
_ Vector Spaces 
As an example of the utility of Theorem 4.2.10, consider a singular value de- 
composition (or SVD) Amxn = vere 0) Vv" as described on page 359, 
where r = rank(A). If the unitary matrices U and V are partitioned as 
U = [(Ui)mxr|U2] and V = [(Vi)nxr| V2], then (3.5.7) on page 361 guar- 
antees that R(U;) = R(AA*), R(Vi)=R(A*A), 
N (U2) = N(AA*), and 
N (V2) = N (A*A), so, by Theorem 4.2.10, 
R(A)=R(Ui), 
N(A)=R(V2), 
(4.2.7) 
R(A*)=R(Vi), 
N(A*) = R(U2). 
In other words, since U and V are unitary, this means that: 
e 
A singular value decomposition of A naturally provides orthonormal bases 
for each of the four fundamental subspaces of A. 
Caution! The reverse is not true—i.e., not just any set of orthonormal bases for 
the fundamental subspaces can be used to construct an SVD for A. 
Rank of a Product 
Except for special cases such as rank(A*A) = rank(A) = rank(AA*) 
or 
rank (PAQ) = rank (A) for nonsingular P and Q, the exact rank of a general 
product AB can be elusive because multiplication by rectangular or singular 
matrices can drastically alter rank, so for this reason the course bound 
rank (A) 
rank (AB) < ae (B) 
(from Theorem 2.4.12 on page 2.4.12) 
is often the best alternative. However, if the exact rank of AB is absolutely 
needed, then there is a formula to produce it. 
4.2.11. Theorem. For A €¢ F"*"" and Be F""?, 
rank (AB) = rank (B) — dim N (A)M R(B). 
(4.2.8) 
Proof. 
The strategy is to first prove the proposition for the special case in which 
B has full column-rank, so assume that rank(B) = p. The rank-plus-nullity 
theorem (Theorem 4.2.9) ensures that 
dim N (AB) = p — rank (AB) = rank (B) — rank (AB), 
(4.2.9) 
so it suffices to prove that dim N (AB) = dim N (A)NR(B). Let Vpxs be ama- 
trix whose columns constitute a basis for N (AB) so that ABV = 0, or equiva- 
lently, 
R(V) = N (AB). Observe that R(BV) = N(A)n R(B) because first, 

4.2 The Fundamental Subspaces 
437 
using (4.2.4), R(BV) C R(B) and A(BV)=0 
= > R(BV)CN(A) so 
that 
R(BV) C N(A)MR(B), and second, 
x€N(A)NR(B) 
= Ax=0andx=By forsomey = ABy=0 
=> 
y=Vziforsomez = x=By =BVz, 
which means that N (A) R(B) C R(BV), and thus R(BV) = N(A)NR(B). 
Since rank (BV) = rank(V) (because B has full column rank—see (2.4.13) 
on page 192), it follows that 
dim N (A) 
R(B) = dim R(BV) = rank (BV) = rank (V) = dim N (AB), 
and therefore (4.2.9) yields rank(AB) = rank(B) — dimN (A) 
R(B). In 
the more general case where rank(B) = r < p, let B = (Bi)nxr(Ba)rxp be 
a full-rank factorization as described in Theorem 2.4.15 on page 190, and note 
that rank (AB) = rank (AB) by (2.4.14) on page 192. Furthermore (4.2.4) 
together with the fact that rank (B) = rank (B,) means that R(B) = R(Bj), 
so it follows from the first part of the proof that 
rank (AB) = rank (AB)) = rank (B,) — dim R(B,) NN (A) 
= rank(B)-—dimR(B)N N(A). 
& 
It was shown in (4.2.4) on page 435 that the containments R(AB) C R(A) 
and N(AB) > N (B) are valid for all Amxn and By xp. Sufficient conditions 
for equality to hold are established below. 
4,2.12. Corollary. Let A ¢F"*" and 
Be F"*?. 
e 
lf rank(Bnxp) =n, then R(AB) = R(A). 
e 
If rank(Amxn) =n, then N (AB) = N(B). 
For related results recall (2.4.13) and (2.4.14) on page 192. 
Proof. 
If rank(Bnxp) =n, then R(B) = F", so R(B)NN(A) = N(A), 
and (4.2.8) reduces to 
dim R(AB) = rank (AB) = n— dim N (A) = dim R(A) => R(AB) = R(A). 
If rank (Amxn) = 7, then N (A) =0, and (4.2.8) says that 
dim R(AB) = rank (AB) = rank (B)=dimR(B) 
= 
R(AB)=R(B). 
The upper bound rank (AB) < min{rank (A), rank (B)} for the rank of a 
product was established in Theorem 2.4.12 on page 187. A complementary lower 
bound is produced as a corollary of Theorem 4.2.11. 

ome 
VES: 
digas nou 
« 
(4.1.7) on page 420 implies 
<2 er 
Proof. 
Since N (A). R(B) CN ( 
that 
dim 
N (A) R(B) < dim 
N (A) = n—rank (A), 
5 
so Theorem 4.2.11 yields 
: 
rank (AB) = rank (B) — dim N(A)N R(B) > rank(B)+rank(A)—n. 
@ 
Nullity of a Product 
; 
The rank-plus-nullity theorem (page 433) expresses the nullity of AB in terms 
of rank(AB). The next theorem presents another formula for the nullity of 
AB along with bounds on the nullity of a product. 
Proof. 
The statement in (4.2.10) is a direct consequence of the formula for the 
rank of a product given in Theorem 4.2.11 coupled with the rank-plus-nullity 
theorem. To establish the left-hand inequality in (4.2.11), start with the fact 
that (4.2.10) ensures that v(AB) > v(B) and then use this along with v(A) = 
v(A™) and v(AB) =v(AB)" (because A and B are both nxn) to write 
|
v(AB) = (AB)? = (BTA?) > v(A7) = v(A). 
|
To obtain the right-hand inequality in (4.2.11), use the rank plus nullity theorem 
together with the inequality in Corollary 4.2.13 to conclude that 
v(AB) =n —rank (AB) < 2n — (rank (A) + rank (B)) = v(A) + v(B). 

4.2 The Fundamental Subspaces 
439 
Example (Bases for Intersections) 
Another consequence of Theorem 4.2.11 is that. its proof shows how to construct a 
basis for an intersection of two subspaces. For example, a basis for N (A). R (B) 
can be built from the following procedure in which it is assumed that AB 4 0 
(otherwise N (A) R(B) = R(B)) and r =rank(Bnxp). 
(1) Find a basis {b;,bo,...,b,} for R(B) (e.g., basic columns in B). 
(2) 
Put these basis vectors into an n x r matrix B, = [b;|be| 
--- |b,]. 
(3) 
Find a basis {vj,vo,...,vs} for N(ABj). 
(4) The set B = {Biv,, Bivo,...,Biv,} is a basis for 
N (A) 
R(B). 
More generally, this procedure extends to create a basis for any intersection 
MON in which M and WN are subspaces of F" such that dim M =k and 
dim 
NV =r. The trick is to create matrices A and B such that M = N (A) 
and N = R(B). 
(1) 
Let Bnxr = [mi|ne| +++ |n,-] be a matrix whose columns are a basis 
for N. This means that R(B) =WN. 
(2) 
Let Mnxx = [mi |mp2|--- |m,] contain a basis for M. 
(3) 
Let (A) nx (n—) be a matrix whose columns are a basis for N (M7) 
so that M7 A? = 0, or equivalently, AM=0. 
This means that 
N(A)=M. 
(4) 
Find a basis {vi,v2,...,vs} for 
N (AB). Note that 
s = dim 
N (AB) =r — rank (AB) =r — (r —dim 
N (A)N R(B)) 
= dim 
N (A)N R(B) =dimMnWN. 
(5) B= {Bv1,Bvo,...,Bvs} is a basis for 
N (A) R(B) =MNWN. 
Note: Alternate ways to obtain the intersection MN of two subspaces are 
given in Theorems 4.3.12 and 4.3.16 on pages 469 and 472, respectively. 
Range and Rank of a Sum 
Since there is a useful formula for the exact rank of a product, intuition might 
suggest that there should be a useful formula for the rank of a sum as well. But 
alas, there is not. The statements in the next theorem are about the best that 
can be said for general matrices. 

Proof of (4.2.12). 
This is true because if x € 
R(A +B), then there is a vector 
y such that 
x=(A+B)y=Ay+By = > xe{R(A)+R(B)} 
(see Definition 4.1.10). 
Proof of (4.2.13). 
Use R(A+B) C R(A) + R(B) together with Theorem 
4.1.9 on page 420 along with the formula for the dimension of a sum (Theorem 
4.1.11, page 422) to write 
rank (A + B) = dim R(A +B) < dim(R(A) + R(B)) 
= dim R(A) + dim R(B) — dim (R(A)N R(B)) 
< dim R(A) + dim R(B) = rank (A) + rank (B). 
Proof of (4.2.14). 
This follows from (4.2.13) because 
rank (A) = rank (A —B+B) < rank(A —B)+ rank (B) 
= > rank (A) — rank (B) < rank (A —B), 
and 
rank (B) = rank (B— A+ A) < rank (B-— A) +rank(A) 
= rank (A — B) + rank (A) 
= > —(rank (A) —rank(B)) < rank (A —B). 
Proof of (4.2.15). 
Simply combining (4.2.13) and (4.2.14) yields 
r—k =rank (A) — rank (—B) < rank (A — (—B)) = rank (A +B) 
<rank(A)+rank(B)=r+k. 
@ 

4.2 The Fundamental Subspaces 
441 
Rank-One Updates 
A direct consequence of Theorem 4.2.15 is that if rank(Amyn) = 7, and if 
ce F™*! and 
de F"™?, then 
r—1<rank(A+cd*)<r-+1. 
While sometimes helpful, this is not enough when the exact rank of A+cd"* is re- 
quired. Because rank can often be tied to the number of degrees of freedom or the 
level of independence in underlying models or data, it can be imperative to know 
exactly when (or how) an update will decrease, maintain, or increase rank (A). 
As the proof of the next theorem shows, the singular value decomposition (SVD) 
on page 359 is the key to revealing the exact value of rank (A +cd*). 
4.2.16. Theorem. For rank (Amxn) | r and vectors Cmx1 and da 
: r4les eg R(A) andd ¢R(A*) 
| 
: 
oes 'R(A) xor! dé R(A*) 
FC 
© Or 
cé R(A) ondde Ray eos 
r—l1<=>ceR(A) 
andde R(A*) anda=0, 
rank (A +cd*) = ¢ 
where 
a =1+d*A'e in which A! is the pseudoinverse (see Definition 
2.4.18 on page 192 and Exercise 3.5.14 on page 379). 
Proof, 
Let A = UDV* bean SVD for A in which D= 'Sere of and let 
ee Bite ( and y= V "d= ay where x1,yi € F'™' so that 
rank (A + cd*) = rank (UD xy*)V"*) = rank (D + xy*). 
Since 
- 
D 
[7,0 
rank (D + xy*) +1=rank (P 
poate ye rake alike a 
= rank( oe i, 
Wool 
it follows that 
a 
D 
O 
x, 
rank (A + cd*) = rank( D .) —l=rank| 
0 
0 
x2})—1. 
(4.2.16) 
Yap 
ol 
yi y3 
-1 
The term "xor" is short hand for "exclusive or." Saying that p xor q is true means that either 
p or q is true, but not both. 

442 
Chapter 4 
F 
Vector Spaces 
Furthermore, d*Atc = d*VD'U*c = y{D71x1, so 
I 
0 
O 
D 
oO 
x 
i 
O0-= 0s 
One 
Do 
( 0 
I 0) (0 0 
x2}(0O 
I 
" 
ee 
ees 
=(0 z) 
-y*D"1 
(a 
¥iuys 
 =s 
0 
0 
1 
0 
yn) 
-—2 
where Z has the form 
OOF 
oe 
O 
* 
z= ( 
y 2) - 
ers 
sy 
(4.2.17) 
x 
ae 
: 
e 
eeante 
Hence (4.2.16) becomes 
rank (A + cd*) = rank( 
—1=r+rank(Z) —1. 
(4.2.18) 
By considering the eight logical possibilities for x2, 
y2, and a being zero (or 
nonzero), it is clear from the structure of Z in (4.2.17) that 
2) => 
xX) £0 and yo + 0, 
Xo 
— 
OXOL ey 51 — 0; 
Waele Zi) 24 
i) 
c 
(4.2.19) 
Xo 
= 0 and yo =O anda 0, 
0. = 
x9 = O andays=0 
and a= 0. 
Let' U ="|(Ui);,x-| 
U2] 'and V = [(Vi)nur 
| Vol If xo = 0 and y2 = 0, 
then (4.2.7) on page 436 allows the conclusion that 
e= Ux =U1x; = ce€ R(U;) = R(A) 
and 
d= Vy =NVayi = de R(V}j) sek (A"), 
Conversely, if 
c€ 
R(A) and de R(A*), then 
c= U,w, and d = Viwoe for 
some w, and we, and since U and V are unitary, 
x2 = Usc = UZUiw, = () 
and 
y2 > V5d = V5 Viw2 ==1(); 
Thus x2 =0 <= >ce R(A) and yo =0<—>de€ 
R(A*), so (4.2.18) together 
with (4.2.19) yields the desired result. 

4.2 The Fundamental Subspaces 
443 
Example (Graphs and Matrices) 
As introduced on pages 69 and 143, a graph is a set {N1,No,...,Nm} of points 
called nodes together with a set {F,, H2,...,E,} of paths between the nodes 
called edges. A connected graph is one in which there is a sequence of edges 
linking any pair of nodes, and a directed graph is one in which each edge has an 
assigned direction. For example, left-hand graph shown below in Figure 4.2.1 is 
connected and undirected, while the right-hand graph is connected and directed. 
Ey 
Ey 
E, 
| 
E2 
B3 
CONNECTED UNDIRECTED GRAPH 
CONNECTED DIRECTED GRAPH 
FIGURE 4.2.1 
The connectivity of a directed graph is independent of the directions as- 
signed to the edges—i.e., changing the direction of an edge doesn't change the 
connectivity. (Exercise 4.2.39 presents another type of connectivity in which di- 
rection matters.) On the surface, the concepts of graph connectivity and matrix 
subspaces seem to have little to do with each other, but in fact there is a close 
relationship. 
Recall from page 144 that the adjacency matrix for a graph containing m 
nodes is the m X m symmetric matrix A in which a;; = 1 =a,;; when there 
is an edge between nodes 7 and j, and a;; = 0 otherwise. In this discussion 
loops on nodes (edges from a node to itself) are not allowed, so by convention, 
ai; = 0. In some contexts the edges may have weights assigned to them, but 
weighted edges do not appreciably affect the theory, so for the sake of simplicity 
only unweighted graphs are considered here. 
The degree of a node N; is the number of edges that touch N;, and the 
degree matrix of the graph is the diagonal matrix D in which d;; is the degree 
of node N;. The respective adjacency and degree matrices for the graphs in 
Figure 4.2.1 are 
Om Fi 
0s 0 
27 0F OO 
70 
LoOei te 
O40 00 
Aga ei iia OmOw0s | za andin d=) 1207-0, 52 0 0 
Ope tanOy Oat 
G0 
0:.2)/0 
Opin Ome 
Oe Oe O02 
The graph Laplacian matriz is defined to be 
L = D — A. In this example, 
2-1 
-1 
0 
0 
—1 
4 -1 
-1 
-1 
Pap 
A 
tear 
= 
2 
0 
0 
|. 
(4.2.20) 
0-1 
0 
2 -1 
Mme 
(OME 
1 
wre 

444 
Chapter 4 
: 
Vector Spaces 
When directions are involved, an additional matrix called the incidence matrix 
is defined to be the m x n matrix E whose (i,k) -entry is 
1 if edge Ey is directed away from node Nj. 
(4.2.21) 
1 
if edge Ey is directed toward node Nj. 
Cik =) 
= 
0 
if edge E; neither begins nor ends at node Nj. 
For example, the incidence matrix for the directed graph in Figure 4.2.1 is 
Ej 
E2 
E3 
E4 
Es 
Es 
Nevel 
"Gael 
<0 
G76 
No{ 
1-1 
0-1 
0 
1 
Banal) 
Of 
1 
tae 
Oe 
(4.2.22) 
Nel 
o 
"0 
"OC 
Sa—t 
0 
Nel 
0. 
(OO 
ey 
3 
To see how all the above matrices are related, notice that e%, =1 if E, touches 
N,, otherwise e?, = 0, so 
[EE'] ,, =Ssyene 
= # edges touching N; = deg 
N; = di. 
(4.2.23) 
k=1 
Furthermore, for each pair of nodes N; and Nj, 
_ f —1 
when N; and N; are directly connected by Ex, 
i di 
0 when N; and N; are not directly connected. 
This means that for i 4 j, 
—1 
when JN; and N; are directly 
connected 
[BE'],, 
Cie 
{ 
4 
J 
y 
: 
>> a 
a 
0 
otherwise. 
In other words, [EE'],, = —aj;, so this together with (4.2.23) shows that 
EE? =D-A=L. 
(4.2.24) 
Each edge in a directed graph is associated with two nodes, the nose and the 
tail of the edge, so each column in E contains exactly two nonzero entries—a 
(+1) and a (—1). Consequently, all column sums are zero, so e' E = 0, where 
= (1,1,...,1), and hence e € N(E"). The rank plus nullity theorem to- 
gether with the fact that 
rank (EE*) = rank (E) = rank (B7) 
(4.2.25) 
produces the observation that 
rank (L) = rank (E) =m — dim N (E') <m-1. 
(4.2.26) 
This holds regardless of the connectivity of the associated graph, but, as the next 
theorem shows, equality is attained if and only if the graph is connected. 

4.2 The Fundamental Subspaces 
445 
Proof. 
If G is connected, then for every pair of nodes (N;,N;) there is an 
edge between them. This means that if x € N(E7), then x7E = 0 so that 
xX; = 2; for every i and j (looking at E in (4.2.22) will make this clear). Hence 
x = ae for some a # 0. Consequently, dim N (E7) = 1. This together with 
the observations in (4.2.24)—(4.2.26) produces the conclusion that 
rank (L) = rank (E) = m— dim N (E*) = m~—1. 
Conversely, suppose rank (E) = m—1, and prove that G is connected by an 
indirect argument. If G is not connected, then G is decomposable into two 
nonempty subgraphs G; and Go in which there are no edges between nodes in 
G, and nodes in Gy. This means that the nodes in G can be ordered so as to 
make E have the form 
E|| 
0 
Emxn = ( Bthmxn 
) 
5 
4.2.27 
man = 
(Mlm 
(4.2.97) 
where 
E,; and Ep are the respective incidence matrices for G,; and Go. It 
follows from (4.2.26) that 
rank (E) = rank (1 i ) 
= rank (Ei) +rank (E1) < (mi —1)+(m2—-1)=m-2, 
which contradicts the hypothesis that rank (E) 
= m—1. Thus the supposition 
that G is not connected must be false. 
& 
The maximally connected subgraphs in an unconnected graph G are called 
the connected components of G. If G is connected, then G is considered to be 
a subgraph of itself and thus it is the only connected component. A corollary 
that emerges from (4.2.27) in the preceding proof indicates exactly how many 
connected components there are in an unconnected graph. 
4.2.18. Corollary. A graph has exactly k connected components if and 
only if rank (E) = rank (L) =m— k. 
e 
This is equivalent to saying that L has exactly k zero eigenvalues. 

446 
Chapter 4 
Vector Spaces 
Proof. 
If G has exactly k connected components, then the nodes of G can be 
reordered according to the connected components so as to make the incidence 
matrix have the form 
Ei 
E2 
Emxn = 
oo 
' 
Ex, 
(4.2.28) 
where (Ei)m;xn, 
is the incidence matrix of the i*? connected component and 
my +m2+-:+-+m, =m. Consequently, 
k 
k 
rank (L) = rank (E) = So rank (Ej) = So(m —1l)=m-Kk. 
i=1 
i=1 
Conversely, assume that rank(E) = m—k, 
but suppose there are h # k 
connected components. By the same logic used above, reordering nodes via con- 
nected components produces an incidence matrix of the form (4.2.28) but with 
h diagonal blocks E;. This then means that 
h 
h 
rank (BE) = So rank (E;) = Simi —1)=m-A, 
i=1 
i=1 
contrary to the hypothesis that rank (E) 
=m-—k. Thus G must have exactly 
k connected components. Since L is symmetric (and hence diagonalizable), and 
since rank (L) = rank (E), 
alg mult, (0) = geo mult, (0) = dim N (L) 
= 
m—rank(L)=k. 
Given a connected graph G, it is natural to wonder, "just how connected is 
G?" This question motivated Miroslav Fiedler' to introduce his concept of the 
algebraic connectivity of G that he defined to be the smallest nonzero eigenvalue 
of L. (This is now called the Fiedler eigenvalue in some places.) Since 
L = EE™ 
is symmetric and positive semidefinite (recall Theorems 3.6.1 and 3.6.2 on page 
382), the eigenvalues {X1,A2,.-.,Am} of L are real and nonnegative, so if G 
is connected, then the eigenvalues of L can be ordered 
Apa Ag 2 tS 
oy > Aer 08 
(4.2.29) 
Miroslav Fiedler (1926-2015) was a Czech mathematician known for his contributions to linear 
algebra and graph theory. His two papers Algebraic Connectivity of Graphs in 1973 and A 
Property of Eigenvectors of Nonnegative Symmetric Matrices and Its Application to Graph 
Theory in 1974 are considered to be ground breaking achievements that laid the foundation 
for the algebraic analysis of graphs and have since motivated a torrent of results in both pure 
and applied mathematics. 

4.2 The Fundamental Subspaces 
447 
where A;—1 is the algebraic connectivity (or Fiedler value). 
To understand why A—-1 is a reasonable measure of connectivity, consider 
how far E is from a closest incidence matrix for a disconnected graph obtained by 
removing edges from G. The singular values 01; > 09 > ++: > Om_-1 > Om =0 
of E are the square root of the eigenvalues \; of EE? = L, so in particular 
Am-1 = 07,_,. Theorem 3.5.7 on page 365 says that om 1 is the distance 
from E to a closest matrix of rank m— 2, and there is at least one incidence 
matrix E of rank m— 2 that is obtainable by removing edges from G by 
means of zeroing out columns in E. Removing "nonbasic edges" by zeroing out 
the nonbasic columns in E plus one additional "basic edge" by zeroing out 
the corresponding basic column in E guarantees that rank (E) = m— 2, so 
the associated graph G is unconnected and |E — E||2 Op FSS) es ar 
example, the nonbasic columns in (4.2.22) are E3 and Eg, while E4 is basic, so 
removing the corresponding edges from G by zeroing out these columns produces 
0 
oC 
'1O1o'S 
So Oo 
FPrOoOO 
0 
0 
' 
0 
0 
and the resulting graph G has two connected components consisting of nodes 
{N1, No, N3} and {N4,N5}. Clearly, 
E and G are not unique. 
Another way to see why ,,—1 is a plausible measure of graph connectivity 
is to consider how the eigenvalues of L change when any edge in a connected 
graph G is removed. Suppose that there are n edges in G. By relabeling edges 
there is no loss of generality 
by focusing on edge E,, so that the incidence matrix 
of G has the form E = [E|c], where the last column c corresponds to Ep. 
When edge E,, is removed from G to produce G. 
, the respective Laplacians of 
G and G are 
my 
Dg 
Obes 
L=EE?+cc?' 
and 
L=EE'. 
If the respective eigenvalues of L and L are Aj > Ag >::: > Am-1 > Am = 9 
and We, = do 2S Bs eee, > seas = 0, then Theorem 3.4.15 on page 352 
and the accompanying illustration in Figure 3.4.1 show that the eigenvalues 
of L interlace those of L, so in particular Am—1 < Am-—1. In other words, 
when an edge is removed, the graph becomes closer to being disconnected and 
the penultimate eigenvalue moves closer to zero. Or you could look at this by 
observing that when an edge is added, the penultimate eigenvalue increases, 
so, as the connectivity of G changes by either deleting or adding edges, the 
penultimate eigenvalue respectively grows either smaller or larger. 
In addition to the prominent role of the Fiedler eigenvalue A,,-1 of L, 
an associated eigenvector X,—1 
(generally called a Fiedler vector) partitions 
a connected graph G. The algebraic signs of the entries in x,,—; 
indicate a 
natural bisection of G by grouping nodes corresponding to negative values in 

448 
Chapter 4 
Vector Spaces 
Xm—1 into one cluster C~ and nodes associated with positive or zero values of 
Xm-—1 
into another cluster C+. Fiedler proved in 1975 that the portion of G 
corresponding to C~ is connected, and if x;,—1 has no zero entries, then so is 
C+. This technique is the basis for what is often called spectral graph partitioning 
or spectral clustering. For example, the eigenvalues of the Laplacian (4.2.20) for 
the graph G in Figure 4.2.1 are {5,3,3,1,0} and a Fiedler vector associated 
with the Fiedler value A = 1 is 
-1 
0 
x= 
|-1], 
so C7 ={N,,N3} and Ct = {No, 
Na, Ns} 
1 
1 
is the Fiedler (or spectral) partition of the nodes in G. There are other graph, 
partitioning algorithms, but the Fiedler method has proven to be widely appli- 
cable. 
The intuition for the Fiedler partition as well as the motivation for the 
name "graph Laplacian matrix" can be gleaned from the situation for physical 
vibrations as described in the example on page 385 concerning vibrating beads 
on a string. Recall from that example that the normal modes of vibration for 
three equally spaced beads on a tightly stretched string are as shown below. 
Smallest eigenpair 
Second smallest eigenpair 
Largest eigenpair 
These modes, from left to right, respectively correspond to the smallest, second 
smallest, and largest eigenvalues and their respective eigenvectors for the matrix 
a 
2-1 
0 
L= (- 2 
—1 iF 
(4.2.30) 
0-1 
2 
which, as shown on page 221, is the discretization of the second derivative Op- 
erator f". The normal mode in the middle is determined by the eigenvector 
1 
X2=a| 
0] 
for the second smallest eigenvalue of L, and this mode naturally 
bisects the beads. This vector x2 is also a Fiedler vector for the graph Laplacian 
matrix 
L 
( 
1 
-1 
0 
-(12.), 
@—@—~e 
cs ) 
(4.2.31) 

4.2 The Fundamental Subspaces 
449 
for the graph of the three beads at rest. In other words, physics produces the 
Fiedler partition. The Laplace operator Au = V?u (page 388) applied to a scalar 
valued function wu is the higher dimensional analog of the second derivative, and 
the form of its discretization, the discrete Laplacian, is similar to that of 
D—A, 
the graph Laplacian, and hence the coincidence of the terminology "Laplacian 
matrix." 
In this example the discrete Laplacian L in (4.2.30) is not exactly equal to 
the graph Laplacian L in (4.2.31), but a slightly different physical configuration 
of vibrating beads can be constructed to make L = L. 
Example (Electrical Circuits) 
Recall from page 222 that applying Kirchhoff's node rule to an electrical circuit 
containing m nodes and n branches produces m homogeneous linear equa- 
tions in n unknowns (the branch currents), and Kirchhoff's loop rule provides a 
nonhomogeneous equation for each simple loop in the circuit. For example, con- 
sider the circuit below in which R;, J;, and E; are the respective resistances, 
currents, and voltages. The node and loop equations from page 222 are: 
Node 1: 
I, — Ig -—I5 
=0 
Node 2: — J; —13 +14 =0 
Node 3: 
I3+I5+1Ig6 
=0 
Node 4: 
Ig—I4—Ig =0 
(4.2.32) 
Loop A: 
Ri —13R3+15Rs5 = Ei — E3 
Loop B: 
J2R2—I5Rs +16Re = Eo 
Loop C: 
I3R3 + I4R4 —I¢6Re = E3 + Es 
1 
—-1l 
QO 
-l 
0 
—1 
0 
-l 
1 
0 
0) 
Beet 
Meneee 
ne er ce 
cost. 
| 
| 
(4.2.33) 
0 
il 
QO 
—-l 
0 
-l 
Notice that the 4 x 3 homogeneous system of node equations in (4.2.32) is the 
system Ex = 0. This observation holds for general circuits. The goal is to 
compute the six currents 
I), [2,...,J¢ by selecting six independent equations 
from the entire set of node and loop equations in (4.2.32). In general, if a circuit 
containing m nodes is connected in the graph sense, then Theorem 4.2.17 on 

450 
Chapter 4 
Vector Spaces 
page 445 ensures that rank (E) = m-—1, so there are m—1 
independent node 
equations. Furthermore, for the vector e of all ones, 
eE=0 = E,,+Exu+:::+Em 
=0, 
which means that any row in E can be written as a combination of the others. 
Therefore, every subset of m—1 rows in E must be linearly independent, so 
when any node equation is discarded, the remaining ones are guaranteed to be 
independent. 
To determine an n X n nonsingular system that has the n branch currents 
I, as its unique solution, it is therefore necessary to find n—m-+1 
additional in- 
dependent equations, and, as discussed on page 222, these are the loop equations 
like those in (4.2.32). A simple loop in a circuit is now seen to be a connected 
subgraph that does not properly contain other connected subgraphs. Physics 
dictates that the currents must be uniquely determined, so there must always be 
n—m-+1 
simple loops, and the combination of these loop equations together 
with any subset of m—1 node equations will be a nonsingular n x n system 
that yields the branch currents as its unique solution. For example, any three of 
the nodal equations in (4.2.32) can be coupled with the three loop equations to 
produce a 6 x 6 nonsingular system whose solution is the six branch currents. 
If unit resistors 
(R; = 1) are used with FE; = i, and if the first three node 
equations are used together with the loop equations, then the resulting system 
Ax = b and its solution are given by 
<7) 
One 
iy 
0 
i 
17/12 
=1. 
0. 
-=1.. 
Pee Ome 
In 
0 
In 
7/3 
0 
Oo 
ro. 
eee 
P| 
baa 
FS 
[easy 
1s 
GOL 
1uO) 
eed 
Tea 
=| 
tee 
a: 
Tay 
ot ase 
Oo 
t. 
6.0 — 
meet 
Is 
2 
Is 
=11/%2 
oO 
Por 
rr? 
"Cees 
Ig 
7 
Ie 
—5/4 
Exercises for section 4.2 
4.2.1. Determine bases for each of the four fundamental subspaces associated 
1 
Pa 
Ge 
ol: 
5 
with 
A = (-2 —4 
0 
4 2), 
i 
Pe 
DA 
Yah 
9 
4.2.2. Construct a 4 x 4 homogeneous system of equations that has no zero 
coefficients and three linearly independent solutions. 
4.2.3. Suppose that AmnX = b is a consistent system of linear equations 
with 
b #0 and rank (A) =r. 
(a) Prove that the system has n—r+1 
linearly independent solu- 
tions. 
(b) Explain why no set of linearly independent solutions for Ax — b 
can contain more than n—r-+1 vectors. 

4.2 The Fundamental Subspaces 
451 
4.2.4. 
4.2.5. 
4.2.6. 
4.2.7. 
4.2.8. 
4.2.9. 
4.2.10. 
4.2.11. 
4.2.12. 
Explain why a system of linear equations AmynXnx1 = bmx1 is con- 
sistent if and only if b € R(A). 
Suppose that A isa 3x3 matrix such that 
wo {()) (Dena om (Dm 
1 
and consider a linear system Ax =b, where b = (-7). 
(a) Is Ax=b 
consistent? 
(b) Does Ax = b have a unique solution? 
Explain why R(Am xn) =O implies A = 0. Does the implication also 
hold when R(A) is replaced by N (A)? 
Se 
she 
ae 
=2 
=1 
0 3 -4 
9% 
—5 
If A=] -1 
0 3 -5 
3] 
and b=| 
-6 |, is be R(A)? 
—1 
0 3 -6 
4 
—7 
-1 
0 3-6 
4 
~7 
og 
1 
-4 
4 
Consider the matrices A = (2 0 | 
and B= (4 —8 °). 
iP 
fe 
0 
-4 
5 
(a) Do A and B have the same row space? 
) Do A and B have the same column space? 
(c) Do A and B have the same nullspace? 
) Do A and B have the same left-hand nullspace? 
For Anxn and asubspace M CF", prove that dim A(M) = dimM 
if and only if 
N(A) 
NM =0. 
For matrices P,,, and C,,,, prove that if rank (C) = rank (P) =r, 
then rank (PCP?) =r and R(PCP*) = R(P). 
Let Anxn be nonsingular and let rank (Bnxp») =r < p. Prove that if 
{x1,X2,.--,X,} 
is a basis for R(B), then {Ax;,Ax2,...,Ax,} isa 
basis for R(AB). 

452 
Chapter 4 
Vector Spaces 
4.2.13. 
4.2.14. 
4.2.15. 
4.2.16. 
4.2.17. 
4.2.18. 
4.2.19. 
4.2.20. 
Determine whether or not hoe 
set B= 
mate 
), (_ 
i)} is a basis for 
)() 
Dp 
WnNnr 
the space spanned by the set A= {( 
(Oey 
ey eg 
eet 
For A = (2 as) 
a :) 
and v = 
, verify that v € N (A), 
8 
1 
3 
seen 
2 
3 
0 
) 
and then extend {v} to a basis for N (A). 
For a set of vectors S = {x),X2,...,Xz~} C R", prove that 
rank (x1X1 + X2x3 +++++XRXE) =T7, 
where r is the size of a maximal linearly independent subset of S. 
Explain why rank (AB) = rank (A) — dim N (B7)) R(A?). 
= 
ii 
ah 
t 
3 
beeen 
Let A= (-4 2 2| 
and B=(-1 Septal 
0), 
Ome 
OmaO 
Dew 
6 
oD 
als 
(a) Determine dim N (A) R(B). 
(b) Find a basis for N (A) 
R(B). 
Prove that the following statements are equivalent for A € F"*". 
(a) N(A)=N(A?) 
(b) R(A) = R(A?) 
(c) R(A)NN(A)=0 
Prove that if A,B € F"*" are hermitian and positive semidefinite, then 
R(A+B)=R(A)+4+ R(B). 
Idempotent Matrices. 
A square matrix A € F"*" is called idempotent 
when A = A?. Prove that every nonzero nxn idempotent matrix A 
has the following properties. 
(a) o (A) = {1,0}. 
(b) A is diagonalizable. Hint: Recall Theorem 3.2.5 on page 305. 
(c) rank (A) = trace (A). 
(d) If Byxn is also idempotent such that AB = BA = 0, then 
rank (A + B) = rank (A) + rank (B). 
(e) rank (A) +rank(I— A) =n. 

4.2 The Fundamental Subspaces 
453 
4.2.21. 
4.2.22. 
4.2.23. 
4.2.24. 
4.2.25. 
4.2.26. 
Let A,B € F"*" Show that if A+B = I and rank (A)+rank(B) <n, 
then AB =0=BA and A?=A and 
B=B?—ie., A and B are 
idempotent. 
Hint: First show that in fact rank (A) + rank(B) = n, and use this 
to conclude that R(A)O R(B) =0. 
Let A,B € F"*" be idempotent matrices. Prove that A+B 
is idem- 
potent if and only if AB = BA = 0. 
Let A,B € F"*" such that A*B =0. 
(a) Prove that 
R(A) 1 R(B) =0. 
(b) Explain why this means that 
R(A +B) = R(A)+R(B). 
Hint: Recall Exercise 2.4.25 on page 198. 
Frobenius Inequality. Establish the validity of Frobenius's 1911 in- 
equality that says for all matrices for which ABC is defined, 
rank (AB) + rank (BC) < rank (B) + rank (ABC). 
Hint: If M = R(BC)NN (A) and NV = R(B)NN (A), then 
MCN. 
For matrices Amyxn and Bm xx, 
consider the augmented (or parti- 
tioned) matrix [A |B]. 
(a) Prove that R[A|B] = R(A)+ R(B). Hint: Recall Definition 
4.1.10 on page 421. 
(b) Explain why each of the following formulas hold. 
rank [A |B] = rank (A) + rank (B) — dim (R(A) 
R(B)), 
and 
dim N [A |B] = dim N (A) + dim N (B) + dim (R(A)N R(B)). 
(c) Compute dim(R(C)MN(C)) and dim(R(C)+N(C)) for 
-1 
—1 
C=] 
-1 
-1 
—1 
ee 
—4 
2 
=) 
© 
—-6 
4 
—6 
4 
(eee) 
(=) wWwwwr 
Show that part of Sylvester's law of nullity in Theorem 4.2.14 on page 
438 is not valid for rectangular matrices because v(A) > v(AB) 
is 
possible. Is v(B) > v(AB) also possible? 

454 
Chapter 4 
Vector Spaces 
4.2.27. 
4.2.28. 
4.2.29. 
4.2.30. 
4.2.31. 
4.2.32. 
4.2.33. 
4.2.34. 
4.2.35. 
Let S C F"™! be a subspace, and let A € F"*"". The image of S 
is A(S) = {Ax|x € S}, and this is a subspace of F™*1 (see Exercise 
4.1.9, page 427). Prove that if N(A)NS = 0, then dim A(S) = dim(S). 
Hint: Let the columns in S = [s;|s2| --- |S] be a basis for S. 
Prove that if A = Ge ) 
is a square matrix in which N(A;) = R(AQ), 
then A must be nonsingular. 
For A € F™*" explain why x € R(A7) <> X € R(A*) and why 
x € N(AT) <>» x € N(A*). Does this mean that if {x1,x2,...,Xr} 
and {y1,Y2,-..,¥m-_r} are respective bases for 
R(A™) and N(A7*), 
then {X,,X2,...,X,} and cee eek 
ee are respective bases for 
R(A*) and N (A*)? 
Suppose that A is a matrix with m rows such that the system Ax = b 
has a unique solution for every b € R™. Explain why this means that 
A must be square and nonsingular. 
For A ¢ F"*", explain why R(A7) = R(B7) if and only if A '~' B. 
For A é F™*" with rank(A) =r, let 
P= pee. be a nonsingular 
matrix such that PA = Ea, where P2 is (m—r) x m. 
(a) Prove that N(A*) = R(P35) by demonstrating that the set of 
columns in P% is a basis for N (A*). 
(b) Prove that R(A) = N (Po). 
Prove that if y*b = 0 for every y € N(A*), then the linear system 
Ax =b must be consistent. 
Suppose that Ax = b is a consistent system of linear equations, and 
let a ¢ R(A*). Prove that the inner product a*x is constant for all 
solutions to Ax = b. 
For a subspace M C F", and a vector x € F", writing x | M means 
that x is orthogonal to every vector in M. Prove that each of the 
following statements holds for every A € F™*", 
(a). 
sx:@N (A) 2A 4). 
(b) 
xe R(A) 
= >x1N(A*) 
(Hint: Use Exercise 4.2.32). 

4.2 The Fundamental Subspaces 
455 
4.2.36. 
4.2.37. 
4.2.38. 
4.2.39. 
For A,B é€ F™"", explain why R(A) = R(B) if and only if AQ=B 
for some nonsingular matrix Q. 
Explain why the Fiedler value Am—1 
(or algebraic connectivity—see 
page 446) for a graph Laplacian matrix L can be expressed in terms of 
a constrained Rayleigh quotient by showing that 
Am-1 = ymin x"Lx, 
(e is the vector of all ones.) 
x 2 
xle 
Suppose that a connected (unweighted) graph G containing m nodes 
N = {Ni,No,.-.,Nm} and-n edges € = {Fi, Eo,...,H;,} is parti- 
tioned (or bisected) into two disjoint connected subgraphs by removing 
a subset of edges from €. This naturally partitions the nodes into two 
disjoint sets—say NM = 
SUS, with 
SNS =. Define a vector 
xm x1 
whose entries are 
_ 
{+1 
when N, €S, 
a ee when N, € S, 
and let L be the associated graph Laplacian matrix (page 443) for G. 
Prove that 
x'Lx _ f{ the minimum number of edges that had to 
4 
| 
have been removed to create the partition (S,S). 
Hint: Arbitrarily assign directions to the edges in G to create an inci- 
dence matrix E, and then examine x' E,, for each column in E. 
Strongly Connected Graphs. The example on page 443 starts with a 
graph to construct a matrix, but it's also possible to reverse the situation 
by starting with a matrix to build an associated graph. The graph of 
Anxn (denoted by G(A)) is defined to be the directed graph on n 
nodes {N,, No2,...,N,} in which there is a directed edge leading from 
N; to N; if and only if aj; 
40. The directed graph G(A) is said to be 
strongly connected provided that for each pair of nodes (N;,N,) there 
is a sequence of directed edges leading from N; to N;,. The matrix 
A is said to be reducible if there exists a permutation matrix P. such 
that PTAP = Gs Bak where X and Z are both square matrices. 
Otherwise, A is said to be irreducible. Prove that G(A) is strongly 
connected if and only if A is irreducible. 
Hint: Prove the contrapositive: G(A) is not strongly connected if and 
only if A is reducible. 

For example, if M = {x} is a single nonzero vector in R?, then, as illus- 
trated below in Figure 4.3.1, M+ is the line through the origin that is perpen- 
dicular to x. If M is a plane through the origin in R*, then M+ is the line 
through the origin that is perpendicular to the plane. 
kA 
M = {x} 
FIGURE 4.3.1: ORTHOGONAL COMPLEMENTS 
Even if M is just a subset (i.e., not a subspace) of F", then the orthogonal 
complement M+ is always a subspace of F" because the closure properties 
(A1) and (M1) on page 412 are satisfied—see Exercise 4.3.1. However, more 
can be said when M happens to be a subspace of F". 
T 
3 
ae 
4 
Orthogonality here is in terms of the standard inner product (x|y) =x*y for F". Extensions 
involving more general spaces with different inner products come later. 

4.3 Orthogonal Complements and Projections 
457 
Proof. 
The fact that MO M+ =0 
is immediate because 
xEMNM+* 
=> 
(x|x)=0 = x=0 
(by (1.4.8) on page 25). 
To see that F" = M+ .MC+, suppose that dimM = r, let the columns of 
M,,x; be an orthonormal basis for M, and set 
P = MM". For each v € F", 
let m= Pv and n=(I-P)v so that 
v=m-+n=Pv+(I-P)v. 
(4.3.1) 
It follows that m = MM*v € R(M) = ™M and ne€ M+ because n*m = 0 
due to the fact that M*M=I. 
& 
While the matrix M in (4.3.1) is not unique, the next theorem shows that 
the matrix P = MM* 
as well as the components m € M and ne M+ in 
(4.3.1) are unique. 
4.3.3. Theorem. If A/ is an r-dimensional subspace of F", and if the 
columns of M,,,., are an orthonormal basis for M, then Py, = MM* 
is unique. Moreover, each v € F" has a unique resolution into orthogo- 
nal components 
v 
=m-+n, where me M and 
ne M+. 
a 
e 
m = Pyyv is called the orthogonal projection of v onto M, 
and n = (I — P)v is the orthogonal projection of v onto M+. 
(Extensions involving more general inner products are considered in 
Exercise 4.3.26 on page 476.) 
e P is called the orthogonal projector onto M, while the orthog- 
onal projector onto M+ is 
I—Pyy = Pay... (See Figure 4.3.2 on 
page 458.) 
(4.3.2) 
e 
Notice that R(I— Py.) =N(Py)=M-. 
Proof. 
To see that Py, = MM" 
is unique, suppose that Mj, and Mg are 
both n xr matrices whose columns constitute an orthonormal basis for M. 
Since M]M, = I, = M3Mbp, and since the columns of Mz are combinations 
of the columns of My, and vice versa, there are matrices X and Y such that 
Rear ah 
Gehan EP tee 
M,; = M2Y 
Y =M35M, 
M, = M2M3M; J © 
Using these observations leads to the conclusion that 
MM? = (M2M3M,)Mj = M2(M3M1M?>) = M2(M;M7{M2)* = M2M5. 
Uniqueness of the resolution 
v= m-+n, where me M and ne M+ follows 
because if 
v =m , +n, =m2+npo, where mj,m2 € M and ny,n2 € Mt, 
then subtraction yields 0 = (m,;—mg2)+(nj—n2) = > (m;—mg2) = —(ni—n2), 
which means that (m;—m2) € 
MOM+ =0, and similarly for (nj —n2). 
JW 

458 
Chapter 4 
Vector Spaces 
Example (Orthogonal Projection in R®) 
Let M bea 
plane through the origin in R? so that the orthogonal complement 
M+ is the line through the origin that is perpendicular to M. The respective 
orthogonal projections m and n of v onto M and M+ are shown below. 
FIGURE 4.3.2: ORTHOGONAL PROJECTIONS AND PROJECTORS IN R® 
Example (Projection onto an Affine Space) 
If v £0 is a vector in a vector space VY, and if M is a nonzero subspace of VY, 
then 
A=v+M is an affine space in VY (recall Definition 1.3.2 on page 17). As 
depicted in Figure 4.3.3, A is a copy of M that has been translated away from 
the origin through v. Consequently, the notion of an orthogonal projection onto 
A is analogous to the corresponding concept for subspaces. 
b 
, 
' 
, 
A=v4+mMmM 
4 
FIGURE 4.3.3: PROJECTION ONTO AN AFFINE SPACE 

4.3 Orthogonal Complements and Projections 
459 
To determine the orthogonal projection p of b € V onto A, put things back 
into the context of subspaces, where the answer is already known, by subtracting 
v from b as well as from everything in A. As illustrated in Figure 4.3.3, this 
translates A back down to M and it respectively translates v to 0, b to 
(b—v), and p to (p—v). This means that p—v 
is the orthogonal projection 
of b—v onto M, so 
p-v=Py(b-v) 
= p=vtPy(b-v). 
(4.3.3) 
Applications to the problem of solving linear systems are developed in Exercises 
4.3.28-4.3.31 on pages 477-479. 
Properties of Orthogonal Complements 
Some of the more important properties of orthogonal complements are summa- 
rized in the next theorem. 
: 4. 
Ze 4. 
'Theorem The ale 
properties hold 
for 
subspaces Mé e 
Bt, 
e 
dim Mt = n— ee = 
fe 
ee 
(4.3.4) 
a se Ad 
ee 
SS . 
- 
(4.3.5) 
e MeN == Ne C Me 
(4.3.6) 
ee 
(M+N)4 =MtnNt 
: 
(4.3.7) 
e 
(M AN)t=Mt+NE 
| 
(4.3.8) 
Proof of (4.3.4): 
This follows from Theorem 4.1.11 on page 422 and the fact 
that 
MN M+ =0. 
Proof of (4.3.5): 
First show that M+" CM. For any v € M+", write 
v=m-+n where 
me M and 
ne M* (by Theorem 4.3.3), and observe that 
0 = (n|v) = (n[m+n) = (n|m)+ (n|n) = (n[n) = 
n=0 = 
vem, 
so M+" C M. 
It follows from (4.3.4) that dim M+ = n—dimM 
and 
aE 
dim M+ = n—dimM+, 
so dim Mt" = dim. M, and thus M+ 
= M 
(by Theorem 4.1.9, on page 420). 
Proof of (4.3.6): 
This is a consequence of the observation that 
xe€Nt=>xlLNIM=xlM=xeMm-. 
Proof of (4.3.7): 
This is the realization that 
xe(MiN) 
=> xl (M4N) e3x1M 
and xiv 
a x eMaiN 
}. 
Proof of (4.3.8): 
Using (4.3.7) together with (4.3.5) yields 
(Mt4N1)" = Mt ant =MON. 
Taking the perp of both sides and invoking (4.3.5) produces (4.3.8). 
lf 

460 
Chapter 4 
Vector Spaces 
Orthogonal Decomposition Involving the Fundamental Subspaces 
The rank-plus-nullity theorem for matrices on page 433 provides a connection 
between R(A) and N(A), but a more important relationship between the 
fundamental subspaces of A € F'™" is revealed in the next theorem, which 
some authors call the fundamental theorem of linear algebra because it shows how 
F™ and F" decompose into orthogonal components defined by the fundamental 
subspaces of A. This theorem also helps explain why the four fundamental 
subspaces of A are, well, fundamental. The simplicity of the theorem's proof 
belies its importance. 
43.5, Theoran: (Orthogonal Decomposition) For every A € F"*", 
| RAY EN CAD 
ind BO SA, 
(4.3.9) 
A : 
By virtue of Theorem 4.3.2 this means that 
| 
a = R(A) @N(A*) 
and 
F®=R(A*)@N(A). 
(4.3.10) 
Proof. 
The first equality in (4.3.9) is the observation that 
xe R(A)* <> x*A=0 <> A*x=0 <> xe N(A'). 
The second equality in (4.3.9) is obtained by replacing A by A*. 
& 
Orthogonal Decomposition and URV Factorizations 
An important realization in linear algebra is that vector space decompositions 
via direct sums of subspaces are generally tied to matrix factorizations. This is 
no better illustrated than by observing how the orthogonal decompositions in 
Theorem 4.3.5 produce a factorization of A ¢ F"*". If rank(A) =r, and if 
Bria) = {u1, Ug,... Uae ' 
Bn as) = {Up+1, U;+1,--- Um} 
Bras) = {Vi, V2,---,Vr}, 
By 
a) = {Vr41, Vr+1)+--Vn} 
are respective orthonormal bases for R(A), N(A*), R(A*), and N(A), then 
4 
By = Bria) U Byvas) = {Uy Fug, 
oe sn | 
(4.3.15) 
an 
By = Bras) U Bnya) — {v1, Vo, mere Rie 
Rees 
are respective orthonormal bases for F"" and F" (see Exercise 4.3.2). If the 
vectors in By and By are placed as columns in matrices 
Ure uy 
[us| +++ | tn] and 
Van = [valve| + 
[vn], 
(4.3.13) 

4.3 Orthogonal Complements and Projections 
461 
then U and V are unitary with u'A =0 for i>r and Av; =0 for j>r. 
This means that 
ujAv; 
«+; 
ujAv,; 
0 
--. 
0 
TAY = Woe AVE 
we 
Wl Ay, Oo ot R, 
0 
ee 
0 
One. 
oO 
0 
ete 
0 
6 
uae 
6 
thus producing the factorization 
A = URV* =U Stik 4 
V*, 
(4.3.14) 
where c;; = u;Av,;. Note that C is nonsingular because it is r x r and 
"2 a 
= rank (U* AV) = rank (A) =r. 
rank (C) = rank ( 
Re 
A factorization in the form of (4.3.14) is called 
a URV factorization of A. 
To complete the marriage between URV factorizations of A and the vector 
space decompositions F™ = R(A) ® N (A*) and F" = R(A*)@ N(A), it is 
necessary to show that the observations above are reversible in the sense that 
every URV decomposition of A defines respective sets of orthonormal bases for 
each of its fundamental subspaces. More specifically, given unitary matrices U 
and V and a nonsingular matrix C, 
,, such that (4.3.14) holds, the first r 
columns in U must be an orthonormal basis for R(A); the last m—r columns 
in U are an orthonormal basis for N(A*); the first r columns in V are 
an orthonormal basis for R(A*); and the last n —r columns of V are an 
orthonormal basis for N (A). This is proven below. 
4.3.6. Theorem. (URV Factorization) When rank (A, x.) = 17, there are 
unitary matrices Umxm and Vn x, and a nonsingular matrix C,., 
such that 
A] URV 2 U oy 2 Vv" 
(4.3.15) 
MXN 
if and only if each of the following statements hold. 
The first 7 columns in U are an orthonormal basis for R(A). 
The last m—r columns of U are an orthonormal basis for N (A*). 
The first r columns in V are an orthonormal basis for R(A*). 
The last n —r columns of V are an orthonormal basis for N (A). 

462 
Chapter 4 
j 
Vector Spaces 
Proof. 
The discussion leading to (4.3.14) shows that a URV factorization is 
generated by orthonormal bases for the four fundamental subspaces of A. Con- 
versely, suppose that Um xm and Vnxn are unitary and C is nonsingular such 
that 
A = URV* = U min) 
V*. To show that the columns in U and 
V provide orthonormal bases for the fundamental subspaces of A, partition 
U = (Ui(mxr) | U2) and V = (Viqnxr) |V2), and use the fact that right-hand 
multiplication by a nonsingular matrix does not alter the range (Corollary 4.2.12, 
page 437) to observe that 
R(A) = R(URV) = R(UR) = R(U;C|0) = R(UiC) = R(U)). 
Thus the first r columns in U must be an orthonormal basis for R(A). The 
orthogonal decomposition theorem on page 460 guarantees that 
N(A*) = R(A)~ = R(U1)~ = R(U2), 
so the last m—r columns in U are an orthonormal basis for N (A*). Similarly, 
left-hand multiplication by a nonsingular matrix does not change the nullspace 
(Corollary 4.2.12, page 437), so again with the aid of Theorem 4.3.5, 
N(A) = N(URV*) = N(RV*) =N oe ) 
aN (CVG) = NV) = AV) RLV). 
Thus the last n —r columns of V must be an orthonormal basis for N (A). 
Finally, R(A*) = N(A)* = R(V2)~ = R(V;) shows that the first r columns 
in V are an orthonormal basis for R(A*). 
Hf 
SVD Is a Special URV 
Every collection of orthonormal bases for the fundamental subspaces of A pro- 
duces a different URV factorization. The singular value decomposition (page 359) 
De xr 
* 
A=u( i 0) 
¥ 
where 
r=rank(A) 
is a special URV factorization in which C = D is the diagonal matrix of 
nonzero singular values, and U and V are unitary matrices whose columns 
are singular vectors. Recall from page 361 that a set of left-hand singular vectors 
{U1,U2,...,U,} is an orthonormal basis for R(A) while PU, pt. U-test 
is an orthonormal basis for N (A*). Similarly a set of right-hand singular vectors 
{V1,V2,-..,V,} 
is an orthonormal basis for R(A*), and AVy 300i eva 
is an orthonormal basis for N (A). Thus an SVD is a URV factorization. 

4.3 Orthogonal Complements and Projections 
463 
However, the singular-vector bases forming U and V are not just any 
orthonormal bases for the respective fundamental subspaces. The development 
of the SVD on page 358 shows that they are "special" in the following sense. 
e 
Bras) = {V1,V2,--.,Vr} 
is an orthonormal set of eigenvectors for A*A 
that are respectively associated with the r nonzero eigenvalues of A* A. 
© 
Byya) = {Vr+1, Vr+2,+--Vn} can be any orthonormal basis for N (A). 
e 
Bra) = {U1, U2,-..,u,} is an orthonormal basis for R(A) in which 
u,; = Av;/||Avi||, = Avi/o; 
(see (3.5.5) on page 359). 
© 
By ar) = {Ur41, Up+2,+--Um} can be any orthonormal basis for N (A*). 
Alternately, Bra) = {u1,U2,...,u,} can be taken to be an orthonormal set 
of eigenvectors for AA*, while Bya+) = {Up+41, Ur+2,-..Um} can be any or- 
thonormal basis for N(A*), in which case Bria+) = {V1,V2,---,Vr} 
is the 
orthonormal basis for R(A*) in which v; = A*u,/||A*u;||, = A*u;/o;, and 
Bnva) = {Vr+1, Vr+2,--»Vn} is any orthonormal basis for N (A). 
Orthogonal Projectors, URV, and the Pseudoinverse 
Every URV factorization (including an SVD) of A € F'*" provides concise 
descriptions of the orthogonal projectors onto the fundamental subspaces of 
A. Furthermore, there is a close relationship between these projectors and the 
Moore-Penrose pseudoinverse A? of A defined on page 192. 
Suppose that rank(A) = 7, and let A = UG; Ns be any URV fac- 
torization with U = [(U1) mxr | U2] and Viz [(Vi)nxr | Vo]. Theorem 4.3.6 
ensures that 
R(A)=R(U1), 
N(A)=R(V2), 
R(A")=R(Vi), N(A*) 
= R(U2), 
so the columns in U, and Ug are respective orthonormal bases for R(A) and 
NGA) 
(A)-, while the columns in V; and V2 are respective orthonor- 
mal bases for R(A*) and N (A) = R(A*). According to Theorem 4.3.3 on 
page 457, this means that the respective orthogonal projectors onto each of the 
fundamental subspaces are as follows. 
Pray UiUj=UL p)U" Pxay—U2U3=U(5 ,°_ )Ut 
Bee vm 
Von 
VaVe= Vy 
)V* 
' 
-1 
Moreover, for every URV factorization of A, the matrix Al = Vie uF 
is the Moore—Penrose pseudoinverse of A because it satisfies the four Penrose 
conditions in Theorem 2.4.19 on page 193. (This generalizes the result in Exercise 

464 
Chapter 4 
Vector Spaces 
3.5.14 on page 379 that draws the same conclusion for an SVD.) Combining this 
observation with (4.3.16) produces the-elegant fact that 
PRA) = AAT, 
Pyva*) =I-AAl, 
PRA*) = ATA, 
Pyva) =]-—AiTA, 
R (At) = A 
In particular, if rank (Amxn) =, then At = (A*A)~1'A* (by (2.4.15) on 
page 192), in which case the orthogonal projector onto R(A) becomes 
Pra) = A(A*A) 
1 
A*. 
(4.3.18) 
(4.3.17) 
Moreover, if rank (Amn) =7, and if Bm», is the matrix containing the basic 
columns of A, then Pra) = Pais) = BB! = B(B*B)~'B*. 
Orthogonal Projectors as Idempotents 
Recall that a square matrix P is said to be idempotent whenever P? = P. It is 
evident from Theorem 4.3.3 that all orthogonal projectors must be idempotent, 
3) is 
idempotent but not an orthogonal projector because R(P) / N (P). So, exactly 
when is an idempotent matrix an orthogonal projector? Answers are contained 
in the next theorem. 
but not all idempotent matrices are orthogonal projectors—e.g., P = ( 
4.3.7. Theorem. Each of the following statements is equivalent to saying 
that a nonzero matrix P is an orthogonal projector. 
« 
PP? =P and P*= Pp. 
(4.3.19) 
© P2=P and R(P) 1 N(P). 
(4.3.20) 
¢ 
P?=P 
and |PI|, =1. 
(4.3.21) 
Proof. 
If P is an orthogonal projector onto M, then P = MM*, where the 
columns of M are an orthonormal basis for M = R(P) (recall (4.3.2) on page 
457). This guarantees that P is hermitian and idempotent, R(P) | N (P) 
(by the orthogonal decomposition theorem), and 
||P||, = 1 (by (3.5.17) on 
page 364). If P is idempotent and hermitian, then there is a unitary matrix 
U = [U,| U2] such that 
P = UDU*, where D isa diagonal matrix containing 
the eigenvalues of P, which are either 1 or 0 (Exercise 3.1.14, page 300), and 
they can be arranged so that P = Si kts = U, Uj, which is an orthogonal 
projector. Now suppose that P is idempotent with R(P) 1 N(P), and let 
U= [U; | Us] be a unitary matrix in which the columns of U,; and Uys are 
respective orthonormal bases for R(P) and N(P). Being idempotent forces 
R(P) = {x|Px = x} because for some y, 
x € R(P) <> x= Py = P'y = P(Py) = Px 
(4.3.22) 

4.3 Orthogonal Complements and Projections 
465 
so that PU; = U; and PU2 =0. Consequently, 
U*PU = U*[PU; | PU2] = U*[Ui|0] =(§ 9) => P=UiU}, 
which is an orthogonal projector. Finally, assume that P is idempotent with 
||P ||, = 1. Schur's triangularization theorem (page 306) ensures the existence 
of a unitary matrix U such that P = UTU*, where T is upper triangular 
with the eigenvalues on P on its diagonal. Since P is idempotent, so is T, 
and, as observed earlier, o (P) = 0 (T) = {1,0}. If r = alg mult(1), then the 
development of Schur's theorem shows that T can be made to have the form 
Teo on yaaowhich 
i 
rene 
* 
O 
x 
x 
OD 
Lib: 
are 
OPO 
ROccre 
Lips 
& 
and 
To92=]{ 
. 
ee 
with k=n—r. 
0 
0 
9 
17 
xe 
Oy OO 
aac 
The 2-norm is unitarily invariant, so ||T||, = ||P||. = 1. This forces every non- 
diagonal entry in the first r rows of T to be zero (otherwise |/T||, > 1 by 
(3.5.18) on page 364), and hence T = (i A ). Having zeros on its diagonal 
means that TS, = 0, so T being idempotent implies that T = T* = i Hy, 
Thus P = UTU* = U, Uj is an orthogonal projector. 
Mf 
Closest Point Theorem 
The notion of orthogonal projection in higher-dimensional spaces is consistent 
with the visual geometry in R? and R®. For example, it is visually evident from 
Figure 4.3.4 below that if M is a subspace of R?, and if b is a vector outside 
of M, then the point in M that is closest to b is 
p= Pyb, the orthogonal 
projection of b onto M. 
b 
peri 
te 
'< 
nin |b — 
ml, 
; 
M 
FIGURE 4.3.4 
The situation is exactly the same in higher dimensions. But rather than using 
your eyes to understand why, use mathematics—it's surprising just how easy it 
is to "see" such things in abstract spaces. 

aes 
edt er 
nated yor 
4 
Proof. 
If p=Pmb, then p—meM 
for all me WM, and 
b—p=(I-Py)be 
M+, 
so (p—m) | (b—p). The Pythagorean theorem (page 40) now ensures that 
|b — 
ml} = |b~p+p— ml} = [lb — pl + lip — 
ml} > Ib — pI. 
In other words, minmem ||/b — m||, = ||b — p||,. To see that there is not another 
point in M that is as close to b as p is, suppose that there is a vector m € M 
such that ||b — mi|, = ||b — p||,. The Pythagorean theorem again yields 
a 
pS 
3) 
~ 
12 
~~ 
|b — mail| = ||b-p+p— ml} = ||b—pl,+|p—mlz; — 
|lp—mi, =0, 
andthuy m=p. 
& 
Note: The closest point theorem forms the foundation for the vector space theory 
of least squares that is presented on page 486. 
URV vs Similarity 
Similarity transformations (page 304) and orthogonal decompositions are two 
important ways of decomposing a matrix. Similarity provides theoretical advan- 
tages via similarity invariants (properties invariant under a similarity transfor- 
mation), but orthogonal factorizations can afford computational advantages. So, 
when is the best of both worlds realized—i.e., when is a URV decomposition also 
a Similarity transformation? First, A must be square, and second, (4.3.15) must 
be a similarity transformation so that U = V. Surprisingly, this happens for a 
rather large class of matrices. 

4.3 Orthogonal Complements and Projections 
467 
i 
2 
: 
=r, the following a 
statements are equivalent. 
Sees 
Heed 
e A= uy 
: ae 
swith, U 
"unitary and 
rank (C)= 
ii (4 
3. 24) 
. Hoe ee 
«8 R(A)= RA'). 
oe - - 
_ eee 
e N(A)= wa" Ce 
e AA'=ATA (Ah is defined ¢ 
on page 192). 
7 ae 
Proof. 
The fact that (4.3.25) <= (4.3.26) <=> (4.3.27) is a direct consequence 
of the orthogonal decomposition theorem (Theorem 4.3.5 on page 460). It suffices 
to prove (4.3.24) <=> (4.3.26) and then (4.3.26) <=> (4.3.28). If (4.3.24) holds, 
then it is a URV factorization with V = U = (U;|Uz), so 
R(A) = R(U)) = R(V1) = R(A*) 
(by Theorem 4.3.6). 
Conversely, if 
R(A) = R(A*), then "perping" both sides and applying the or- 
thogonal decomposition theorem yields N (A) = N (A*), so constructing 
a URV 
factorization as described in (4.3.13) on page 460 yields one in which U = V. 
To see that (4.3.28) <=> (4.3.26), use (4.3.17) to observe that 
AAT = A'A <> Pray = Pras) — R(A)=R(A*). 
OD 
4.3.10. Definition. A square matrix A satisfying any of the conditions 
in Theorem 4.3.9 is called an EP, matrix! (or EP matrix for short). 
Alternately, A can be said to be an RPN matriz, which is short for 
"range perpendicular to nullspace." 
i 
4 
Ge! 
For example, A = ( 
3 
nt ) 
is an EP matrix because rank (A) = 2 
13 
14 
15 
16 
and N(A) = span {{e1 — 2e2 +e3}, {2e1 — 3e2 +e4}} = N(A*) (see Exer- 
cise 4.3.12 on page 474). 
The unintuitive terminology "EP; matrix" comes from the 1950 text Introduction to Linear 
Algebra and the Theory of Matrices by Hans Schwerdtfeger (1902-1990). Schwerdtfeger defined 
"P., matrices" to be square matrices of rank r that possess a principal submatrix of rank 
r. 
When these matrices have the additional property that the linear relations between the rows 
are the same as those between the columns, Schwerdtfeger called them "EP, 
matrices." In 
other words, R(A)=R (A7) (see Exercise 4.3.11). For this reason some authors have also 
used the terminology "range-symmetric" (or range-hermitian) matrices. Schwerdtfeger was a 
student of Otto Toeplitz (see page 314) in Bonn, but after fleeing Germany in 1936 he spent the 
rest of his career in Australia and later in Canada at McGill University, where he concentrated 
on geometrical aspects of group theory. 

468 
Chapter 4 
Vector Spaces 
Example (Relation to Normal Matrices) 
Recall that normal matrices are those that have a complete set of orthonormal 
eigenvectors (page 339). The figure below illustrates how normal matrices fill the 
niche between hermitian and RPN matrices in the sense that 
real-symmetric 
=> 
hermitian 
=> 
normal 
=> 
BPS, 
with no implication being reversible—details are called for in Exercise 4.3.19. 
Hermitian 
Example (Courant-Fischer via Orthogonal Complements) 
Orthogonal complements can be used to reformulate the Courant—Fischer theo- 
rem on page 425 that provides variational descriptions for the eigenvalues of a 
hermitian matrix. 
4.3.11. Theorem. If the eigenvalues of a hermitian matrix A € F"*" are 
Ay > Ag > ++: > Ay, then for each k, 
Ay = max min x*Ax = min max x*Ax, 
(4.3.29) 
VY 
xEV 
hi 
eeyit 
ielig=1 
ixel|g=1 
where V = {V¥i,V2,-.-,Vn--e} and Vo = {vi, Vo,..-;Ve_1} 'vary over 
linear independent subsets of F". 
Proof. 
'To prove the first equality (the max-min part), first observe that 
dim V+ = dim{span(V)}+ =n—(n—k)=k 
(see Exercise 4.3.4). 
Given any V, Lemma 4.1.13 on page 423 ensures 
x* Ax < \, for some x € Yt 
with 
"|||, ==" Bo 
min x"Ax < Ax. Consequently, as V varies over all 
\|x\|9=1 
independent sets of n —k vectors, 
Max ai 
ka AE 
yY 
xept 
IIxllg=1 

4.3 Orthogonal Complements and Projections 
469 
To see that this inequality is reversible, consider a complete orthonormal set 
of eigenvectors {uj,U2,...,U,} such that Au; = Aju; for each i, and let 
S' = span {uz41,...,Un}. Note that dim S'+ = k. Theorem 3.4.12 on page 
347 ensures that 
min x*Ax = Xx, soas V varies over all independent sets of 
xEs! 
llIlg=1 
n—k vectors, 
max winx Ax > -min x Axe A;, 
xepvl 
xes/t 
llIlo=1 
ll lo=1 
and thus the first equality in (4.3.29) is established. The second equality follows 
from the first by replacing A with —A and realizing that the eigenvalues of —A 
are [lj 2 lg 2 +++ > Mn, Where py = —An—K41, Or equivalently A_, = —Un—r41, 
along with the fact that for a set S of real numbers, maxges{a}=— minges{—a} 
and minges{a} = —Maxges{—a}. 
Note: The difference between the results in (4.3.29) and those in (3.4.13) in 
Theorem 3.4.12 on page 347 is that the formulas in (3.4.13) are constrained 
to linearly independent sets of eigenvectors whereas the expressions in (4.3.29) 
involve general independent sets that are not necessarily eigenvectors. 
Example (Alternating Projections) 
It is not uncommon to encounter situations where it is necessary to determine 
the nature of the intersection MON of two subspaces M,N CF", and there 
are various ways of approaching the problem—the example on page 439 illus- 
trates one such technique. But more elegant methods involving projections are 
available, and a wonderful example that illustrates ideas from this section is 
the following theorem credited to John von Neumann' that shows why succes- 
sively alternating projections onto one space and then the other produces the 
projection on the intersection in the limit. 
4.3.12. Theorem. 
If MM, NV C F" are subspaces, and if P = Py, and 
Q = Pw are the respective orthogonal projectors onto M and WN, 
then the orthogonal projector onto MAAN is given by 
Pann = jim (PQ)*. 
(4.3.30) 
Note: This result generalizes to include any number of subspaces. 
The formula in (4.3.30) has undoubtedly been discovered and rediscovered over the years, but 
credit is generally given to John von Neumann (page 649) because of his introduction of it 
in his 1950 monograph Functional Operators. Vol II: The Geometry of Orthogonal Subspaces, 
Annals of Math. Studies, Vol 39, Princeton University Press. Alternating projection methods 
subsequently became popular in a wide range of applications—the Kaczmarz projection method 
in Exercise 4.3.29 is one such instance. 

470 
Chapter 4 
Vector Spaces 
This theorem is proved by combining the following two lemmas, each of 
which is of interest in its own right. The hypotheses are those in the theorem's 
statement. 
a 3.13. ene PQ is eanilee: to ad ete: 
matrix, and PQ has ia 
_ Sleonvalue 0O<NS _ 
Soret 
se 52. 
e 
fon eS 
Proof. 
Let 
r = rank(PQ). 
To see that PQ is diagonalizable, note that 
(QP)*(QP) = PQP is hermitian and positive semidefinite, so it has real eigen- 
values Ay > Ag > ++: 
> Ay > O and n—v,T 
zero eigenvalues, and there is a 
unitary matrix Unyxn = (UL | cn such that U*PQPU = o ae where 
MXT 
nx(n—r) 
D = diag (Ai, A2,...,Ar). This is a specialized URV factorization, so Theo- 
rem 4.3.6 on page 461 ensures that the columns of Uj, are an orthonormal 
basis for R([QP]*[QP]) = R([QP]*) = R(PQ), and the columns of U2 are 
an orthonormal basis for N ({QP]*|QP]) = N(QP) 
(recall Theorem 4.2.10 
on page 435). Consequently, QPU2 = 0, or equivalently, USPQ = 0, and 
R(U;) = R(PQ) means that U; = PQX for some Xn x, sO 
PU; = P?QX =PQX=U; 
= 
D=U{PQPU;, = UjPQU,. 
These facts lead to the conclusion that 
U*PQU = gees ace = te B 
U3PQU; 
UsPQU2 
0 i for 
B = UTPQU2. 
ip He e ay then H-! = le ey, and 
(UH)-!PQ(UH) = G | 
, 
Consequently, 4; € ¢(PQ), and 0 < A; < p(PQ) < ||PQ]|, < ||P, ||Q\l, <1, 
where p(x) is spectral radius (recall Corollary 3.1.9 on page 298) and (4.3. 21) 
on page 464). 
& 
4.3.14.Lemma. N (I— PQ) = R(P) 1 R(Q) = N(1— QP). 

4.3 Orthogonal Complements and Projections 
471 
Proof. 
If 
x¢ R(P)MR(Q), then Px =x and Qx =x 
by (4.3.22) on page 
464. Multiplication of the second equation by P yields 
PQx 
= Px=x = xeN(I-PQ), 
which shows that R(P) 1 R(Q) C N (I— PQ). To establish the reverse inclu- 
sion, first observe that 
x€N(I—-PQ) 
= 
x=PQx=Py, 
where y=Qx 
== 
xeR(P) 
Jand= 
ye R(Q) 
(4.3:31) 
=k 
xX 
end) s Ove Vy. 
Using these with the fact that P and Q are both hermitian yields 
x"x = x"(Py) = (x"P)y = (x*P")y = x"y = x"(Qy) = (x"Q"*)y = y*y 
so that 
||x||, = 
|ly||,. In general, the Cauchy—Schwarz inequality says that 
Ixy] < |lxlly llyllp, but as observed above, x*y = x"x = |[xllp = ||xlle llylla, 
which means that equality holds in Cauchy—Schwarz, and hence x = ay for some 
scalar a by Theorem 1.4.5 on page 27. This together with the last implication 
in (4.3.31) proves that 
x € 
R(P)1 R(Q) sothat N (I— PQ) C R(P)NR(Q), 
and therefore N (I— PQ) = R(P)N R(Q). Reversing the roles of P and Q 
produces the conclusion that 
N (I— QP) = R(Q)NR(P) 
= R(P)NR(Q) =N(I-PQ). 
@ 
e 
Now establish that lim,5..(PQ)* = Puan. 
Proof of Theorem 4.3.12. 
Since P and Q are hermitian, the orthogonal de- 
composition theorem (page 460) along with Lemma 4.3.14 ensures that 
R(I— PQ) = R(I- QP)* =[N(1- QP)|" = [N(I- PQ), 
which means that 
I— PQ is an RPN (or EP) matrix. Hence there is a unitary 
matrix V=[ Vi | Ve | such that 
wr 
nxk 
nx(n—k) 
" 
Crxk 
9) 
x7 
1-PQ=v( 
0 
A bir 

472 
Chapter 4 
Vector Spaces 
where C is nonsingular, and where the columns of V2 are an orthonormal basis 
for N (I— PQ) (by Theorem 4.3.9 on page 467). Consequently, 
BQ ca ery aN = Pajavt (Cay Ae 
Notice that 1 ¢ o (I—C), 
for otherwise 0 € a (C), which is impossible because 
C is nonsingular. Consequently, Lemma 4.3.13 ensures that p(I—C) <1, and 
hence (I— C)* + 0 (by Theorem 3.3.2 on page 326). It therefore follows from 
(4.3.16) on page 463 and Lemma 4.3.14 that 
: 
OO 
- 
Jim (PQ)* =V ¢ 7) 
V = V2V3 = Pya-pq) = PRpynr(Q) = Puno: & 
4.3.15. Corollary. The rate at which (PQ)* > Pyyaw is dictated by 
the second largest eigenvalue of PQ —i.e., if 1 > Ag > Ag > -::Ay are 
the distinct eigenvalues of PQ, then (PQ)* goes to Pyiny as fast as 
Ate Os 
Proof. 
'This follows because PQ is similar to a diagonal matrix. 
I 
Example (Anderson—Duffin Formula) 
Von Neumann's alternating projection formula in Theorem 4.3.12 leads to an 
interesting closed-form expression for the projector Pyjay. 
4.3.16. Theorem. (Anderson—Duffin') For subspaces M,N C F", the 
orthogonal projector onto MONA 
is given by 
Puan = 2P(P + Q)'Q, 
(4.3.32) 
where 
P= Py and Q = Py 
are the respective orthogonal projectors 
onto M and N, and where (x)! is the Moore—Penrose pseudoinverse. 
t 
one 
William N. Anderson, Jr. (1940-) and Richard J. Duffin (1909-1996) introduced formula 
(4.3.32) in their 1969 article Series and Parallel Addition of Matrices, J. Math. Anal. Appl., 
Volume 26, pp. 576-594. The result was part of Anderson's Ph.D. thesis at Carnegie Mellon Uni- 
versity, where Duffin was the University Professor of Mathematical Sciences. Duftin was famous 
for his work in electrical networks, graphs, mathematical programming, differential equations, 
and mathematical modeling, and in 1982 he was honored with the prestigious John von Neu- 

4.3 Orthogonal Complements and Projections 
473 
Proof. 
Since R(P+Q) = R(P)+R(Q) =M+4+WN 
(see Exercise 4.3.14 on 
sak 475), and since P + Q is hermitian, it follows from (4.3.17) on page 464 
that 
(P+ Q)(P+Q)' = (P+Q)'(P + Q) = Pawigy = Pauw. 
(4.3.33) 
Furthermore, PyyiyP =P =P*=PPysiy because 
MC M+N, so 
(P+Q)(P+Q)'P = P(P+ Q)'(P+Q) 
= Q(P+Q)'P=P(P+Q)iQ. 
(4.3.34) 
Repeatedly using (4.3.34) yields 
P(P + Q)'(QP)* = Q(P + Q)'P, 
Q(P + Q)'(PQ)* = P(P + Q)'Q. 
(4.3.35) 
In light of Theorem 4.3.12, let L = limp0(PQ)* = Pmaw = limg-+c0(QP)*, 
and use 
MNN C(M+WN) with (4.3.33) to conclude that 
L= Paw = (P+ Q)(P+Q)'L = P(P + Q)IL+ Q(P + Q)'L 
= Jim {PP + Q)'(QP)* + Q(P + Q)'(PQ)*} 
= Q(P+Q)iP+P(P+Q)'Q 
by (4.3.35) 
=2P(P+Q)'Q. 
& 
Exercises for section 4.3 
4.3.1. For MCF", prove that M+ is always a subspace of F", even when 
M is not a subspace. 
4.3.2. For 
Ac F™*", let Bray, Byia+), Briax), and By 
a) be respective 
orthonormal bases for R(A), N(A*), R(A*), and 
N (A). Explain 
why Bria) 
UBya+) and Bria) U Byya) are respective orthonormal 
bases for F" and F". 
mann Theory Prize by the Institute for Operations Research and the Management Sciences 
(INFORMS) for his lasting contributions. The award is regarded as the "Nobel Prize" of the 
field. Anderson and Duffin refer to the expression P(P+Q)'Q asthe parallel sumof P and Q 
because it is the matrix generalization of the scalar function rir2/(r1 +r2) = r1(r1 +72)~ +72, 
which is the resistance of a circuit composed of two resistors 7; and r2 connected in parallel. 
They along with others went on to exploit this analogy in a number of interesting ways, but 
their projection formula stands out as being particularly elegant. 

474 
Chapter 4 
; 
Vector Spaces 
4.3.3. 
4.3.4. 
4.3.5. 
4.3.6. 
4.3.7. 
4.3.8. 
4.3.9. 
4.3.10. 
4.3.11. 
4.3.12. 
4.3.13. 
What is F"+? What is 0+? 
For a linearly independent set V = {vi,V2,-.., Vz} CF", explain why 
dim V+ =n—k 
in spite of the fact that V is not a subspace of F". 
Find the orthogonal projection of b onto M = span{u} as well as 
onto M+, where b = (5) and u= tea 
Find a basis for the orthogonal complement of M = span 
wWwoOonr Or 
PN 
eZ, 
Verify the orthogonal decomposition theorem for A= (2 4 
3 
6 a me 
Ww 
CO Nee 
—4 
-2 
-4 
-2 
For A= ( 
2-2 
2 
1), 
find the orthogonal projector onto R(A). 
—4 
1 
-—-4 
—2 
3 
Use the matrix A in Exercise 4.3.8 along with v = (2) to find vectors 
3 
x € R(A) and 
y€ N(A') such that 
v=x+y. Is there more than 
one choice for x and y? 
Explain why the rank plus nullity theorem (Theorem 4.2.9, page 433) is 
a corollary of the orthogonal decomposition theorem. 
Prove that if the linear relationships between the rows of A € R"*" 
are the same as those between the columns, then A is an EP (or RPN) 
matrix. 
1 
2 
ola) « 
n 
n+1 
n+ 2 
ses 
Qn 
Qn+1 
nck? 
can 
98 
Explain why A = 
j 
"| 
is an EP matrix. 
Go Dat line tn $2 
oe 
ee 
Let P and Q be the respective orthogonal projectors onto subspaces 
M,N CF. 
(a) Prove that PQ =0 
if and only if M 1 WN. 
(b) Is it true that PQ =0 
if and only if QP = 0? Why? 
—

4.3 Orthogonal Complements and Projections 
A75 
4.3.14. 
4.3.15. 
4.3.16. 
4.3.17. 
4.3.18. 
4.3.19. 
4.3.20. 
4.3.21. 
4.3.22. 
4.3.23. 
4.3.24. 
Let P and Q be the respective orthogonal projectors onto subspaces 
M,N CF". 
(a) Prove that 
R(P +Q) = R(P)+R(Q). 
Hint: Recall Exercise 4.2.25 on page 453. 
(b) Prove that P + Q is an orthogonal projector if and only if 
PQO=0=OQP: 
Hint: Recall Exercise 4.2.22 on page 453. 
Describe all 2 x 2 orthogonal projectors in F?*?. 
Prove that ||PQ||, = ||QP||, when P,Q € F"*" are both orthogonal 
projectors. 
Let P,Q € F"*" be orthogonal projectors. Prove that PQ is an or- 
thogonal projector if and only if PQ = QP. 
Let A be anormal matrix. Explain why R(A — AI) | N (A —AI) for 
every scalar 1. 
Explain why the following chain of implications must hold, and then 
construct examples to show that none of the implications is reversible. 
real-symmetric = > hermitian 
=> 
normal 
=> 
RPN. 
Prove that if A € F"*" is hermitian and if M is a subspace of F" 
such that M C R(A), then dim A(M) = dim M. 
Prove that if P ¢ F"*" is an orthogonal projector, then ||Px||, < ||x||. 
for all x € F". 
For an orthogonal projector P, prove that ||Px||, = ||x||, if and only 
if xé€ R(P). 
Explain why A*Pra) = A* for all A € F™*", where Pray is the 
orthogonal projector onto R(A). 
Prove that if A,B € F"*" are hermitian and positive semidefinite, then 
A(A + B)'B = B(A+B)!A. Note: This generalizes the analogous 
statement for orthogonal projectors given in (4.3.34) on page 473. 
Hint: Recall Exercise 4.2.19 on page 452. 

476 
Chapter 4 
Vector Spaces 
4.3.25. 
4.3.26. 
4.3.27. 
Let {uj,u2,...,u,} be an orthonormal basis for a subspace U C iat 
and let b be any vector in F"*', 
(a) Explain why p = >0j_,(ujb)u; is the orthogonal projection of 
b onto U. 
5 
(b) Use this to compute the orthogonal projection of b = 
: 
3 
—3/5 
0 
4/5 
onto w= anon {( 
as ) 
Gl Gale 
0 
1 
0 
A general inner-product space is a vector space V over a field F that 
is equipped with an inner product function 
(x|y) that satisfies the 
properties that are given in (1.4.8)—(1.4.12) on page 25. In such spaces 
we define ||x|| = \/(x|x), and we take x | y to mean that (x|y) = 0. 
Replace F" by V in Definition 4.3.1 to define ¥+, and assume that 
Theorem 4.3.2 remains valid when F" is replaced by V (which it does) 
so that each v € V can be uniquely written as 
v = x+y, where 
xc Vv 
and y € ¥+. Define the orthogonal projection of v onto 4X to be x, 
and define the orthogonal projector onto X to be the function Px that 
maps v to x—i.e., Py(v) =x: 
(a) Prove that Px is a linear function. 
(b) Extend the results of Exercise 4.3.25 to n-dimensional inner- 
product spaces by proving that if {x1,x2,...,x,-} 
is an or- 
thonormal basis (with respect to the given inner product) for 
a subspace 4, then for every 
b € V, p= )>j_, (xi|b) x; is 
the orthogonal projection of b onto 4%. 
For two distinct nonzero vectors u,v € R" with v 4 au, the line L 
through u and v is 
£=u+span{u—v}, which is an affine space. 
fs 
FIGURE 4.3.5 ORTHOGONAL PROJECTION OF b ONTO THE AFFINE SPACE 
CL. 

4.3 Orthogonal Complements and Projections 
A77 
Refer to Figure 4.3.5 to explain why the orthogonal projection p of a vector 
b € R" onto CL is 
(ay) aba) 
pe [a= vyFa=v) 
(u —v). 
4.3.28. An affine space v+.M C R" with dimM = n-—1 is called a hyperplane. 
Hyperplanes in R? and R° are lines and planes (not necessarily through 
the origin), respectively. The i*" equation A;,x =; ina 
linear system 
AmxnX = b isa hyperplane in R", and the solutions of Ax = b occur 
at the intersection of the m hyperplanes defined by the rows of A. 
(a) Prove that for a given scalar 8 and a nonzero vector u € R", 
the set H = {x|u'x = 6} is a hyperplane in R". 
(b) Explain why the orthogonal projection of b € R" onto H is 
u'b— 
8 
given by 
p=b-— ree u. 
4.3.29. Kaczmarz's' Projection Method. The solution of a nonsingular system 
ee ry CG ) 
= G ) 
is the intersection of the two hyperplanes 
a21 
422 
v2 
b2 
Hy ={(1, 
£2) | 1121+ G12%2 = b1}, Ho={(21, 
Lo) | @2171 
+ a2272 = bg}. 
It is visually evident that by starting with an arbitrary point po and 
alternately projecting orthogonally onto H; and Hy as depicted in 
Figure 4.3.6, the resulting sequence of projections {p1, P2, P3, P4,--- } 
converges to H, 
He, the solution of Ax = b. 
/ Stefan Kaczmarz (1895-????) was a Polish mathematician in the faculty of mechanical engi- 
neering of Jan Kazimierz University of Lwéw from 1919 to 1939, where he collaborated with 
the famous Stefan Banach. The two of them where among a school of bright young Polish 
mathematicians who were beginning to flower in the first part of the twentieth century. Kacz- 
marz published his alternating projection idea in 1937, but soon after he was called up for 
military service by the Russian-controlled part of partitioned Poland, and he was never heard 
from again. Theories of his death include the possibility that he died in combat against the 
Germans, or that he was killed in a bombing raid on a train in which he was traveling. Another 
theory has him murdered by the NKVD (a group run by the Soviet secret police) in 1940 as 
part of the Katyn massacre. 

a yr dae: 
iv 
geese 
Me 
wri a 
Cray 
as i 
ie 
oe 
x nals, 
east hae 
* 
' 
FIGURE 4.3.6: KACZMARZ ALTERNATING PROJECTION METHOD 
4 
i 
/ 
i 
108 
* 
i 
' 
f 
4 
» 
' 
= 
. 
~ 
o 
4 
* 
ui 
a 
: 
* 
This idea illustrated in Figure 4.3.6 is generalized by using Exercise 
4.3.28. For a consistent system A,y-x = b with rank(A) 
=r, scale 
the rows so that ||Aj.||) =1 for each i, and let H; = {x| Ai«x = bi} 
be the hyperplane « 
defined by the i" equation. Begin with an arbitrary 
vector Po € R'*', and successively perform orthogonal projections 
onto each hyperplane to generate the following sequence: 
Pi = Po — (AisPo — b1) (Aix)? 
P2 = pi — (Aa«pi — b2) (Ass) 
(project po onto H; ), 
(project p; onto Hz), 
Pn = Pn-1 — (AnsPn—1— bn) (Ans) 
(project pr—1 onto Hp). 
When all n hyperplanes have been used, continue by repeating the 
process. For example, on the second pass project pn onto H,; then 
project Pp4i onto He, etc. For an arbitrary po, the entire Kaczmarz 
sequence is generated by executing the following double loop: 
for k = Opa djeut 
rf ath ne Py EI 
Pkn+i = Pen+i-1 — (AixPkn+i-1 — 0: ) (Ais)? 
Prove that the Kaczmarz sequence converges to the solution of Ax = Ae 
by showing that ||Pin+i — ees 
= |Pin+i— es x||3— (AiePenti-1 = bi)? 
: 
= 

4.3 Orthogonal Complements and Projections 
479 
4.3.30. 
4.3.31. 
For a subspace M C R" and a vector x € R", the orthogonal distance 
between x and M is defined to be dist (x,M) = ||x—pl|,, where 
p is the projection of x onto M. Prove that the orthogonal distance 
between x and Mt is the same as the orthogonal distance between Rx 
and M+, where 
R = I—2P,y. This proves that R reflects everything 
in R" about M+, so R is naturally called the reflector about Mt. 
The elementary reflectors I—2uu?/u?u_ in Definition 1.10.5 on page 
104 are special cases—go back and look at Figure 1.10.3 on page 104. 
Cimmino's Reflection Method. In 1938 the Italian mathematician Gian- 
franco Cimmino' used the following elementary observation to construct 
an iterative algorithm for solving linear systems. For a 2 x 2 system 
Ax =b, let H; and Hp2 be the two lines (hyperplanes) defined by the 
two equations. For an arbitrary guess ro, let r; be the reflection of ro 
about the line H,, and let ro be the reflection of rp about the line 
Hy. As illustrated below in Figure 4.3.7, the three points rg, 
r;, and 
rg lie on a circle whose center is H; MHz (the solution of the system). 
FIGURE 4.3.7: CIMMINO'S OBSERVATION 
The mean value m = (r; + 1r2)/2 is strictly inside the circle, so m 
is 
a better approximation to the solution than ro. It is visually evident that 
iterating this observation produces a sequence that converges to the solution 
of Ax = b. Prove that this holds in general by using the following blueprint. 
Gianfranco Cimmino (1908-1989) graduated at Naples under the direction of Mauro Picone, 
who in 1932 designed and taught one of the first courses on numerical methods ever offered 
at an Italian university. After studying with Carathéodory in Germany, Cimmino returned to 
Italy and eventually became the Chair of Mathematical Analysis at Bologna, where he spent 
his career. Like Kaczmarz's method in Exercise 4.3.29, Cimmino's method did not attract much 
attention until many years after its development. Kaczmarz's and Cimmino's methods were 
also similar in that they both seem to have been reinvented several times. The Cimmino method 
never became popular for solving linear systems, but it did become the basis for algorithms that 
are used to solve systems of inequalities that found applications in computerized tomography, 
radiation treatment planning, and medical imaging in general. 

480 
Chapter 4 
Vector Spaces 
(a) For a scalar 8 and a vector-u € R" such that |/u||, = 1, consider 
the hyperplane H = {x|u'x = (} (see Exercise 4.3.28), and use an 
elementary reflector (see Definition 1.10.5 on page 104) to show that 
the reflection of b € R" about H is 
r= b—2(u'b — B)u. 
For a system Ax = b in which the rows of A € R"*" have been 
scaled so that ||Aj.||, =1 for each i, let H; = {x|Aisx =};} be the 
hyperplane defined by the i' equation. If ro € R"*' is an arbitrary 
vector, and if r; is the reflection of ro about H;, explain why the 
mean value of the reflections {ri,r2,...,%n} is 
m=ro —(2/n)A7'e, 
where 
¢€=Aro—b. 
Iterating this produces 
m, = Mz-_-1 — (2/n)ATex-1, 
where 
€,—~1 = Amz_1 — b. 
Show that if A is nonsingular, and if x 
= A~'b, then 
x —m, = (I- (2/n)ATA)* (x — mo). 
Prove m, — x for all mo. Hint: Recall Exercise 3.6.23 on page 403. 
Note: It turns out that m, converges even when A is rank deficient, 
in which case the limit is a solution to Ax = b provided that the 
system is consistent. If the system is inconsistent, then the limit is 
a least squares solution. Cimmino's method also works with weighted 
means. If W = diag (w1,we,...,Wn), where w; > 0 and >> w; = 1, 
then mz = mg; — wA?We,;_; 
isa convergent sequence in which 
0 <w < 2 isa "relaxation parameter" that can be adjusted to alter 
the rate of convergence. 

4.4 Least Squares 
481 
4.4 LEAST SQUARES 
The following problem arises in almost all areas where mathematics is applied. 
At discrete points 2; (e.g., points in time), observations 
y; of an event are 
made, and the results are recorded as a set of ordered pairs 
D= {(t1,y1), (2, Y2)) «++5 (CmsYm)} 
(mM > 2). 
(4.4.1) 
On the basis of these observations, the goal is to make estimations or predictions 
at points that are between or beyond the observation made at x;. The problem 
boils down to finding the equation of a curve y = f(x) that closely fits the 
points in D so that the phenomenon can be estimated at any non-observation 
point x with the value y= f(Z). 
Traditional (or Ordinary) Least Squares 
When the data in D suggests a linear trend, the traditional theory revolves 
around the fundamental problem of fitting a straight line to the points in D. 
(Lm; Ym) 
Em 
FIGURE 4.4.1: LEAST SQUARES LINE 
The strategy is to determine the coefficients a and 
in the equation of the line 
f(x) 
=a+ $x that best fits the points (x;, yi) in the sense that the sum of the 
squares of the vertical' errors €1,€2,.--,€m indicated in Figure 4.4.1 is minimal. 
Only vertical errors are considered because there is a tacit assumption that only the obser- 
vations 
y; 
are subject to error or variation. The 2;'s are assumed to be errorless—think 
of them as being exact points in time, which they often are. If the 2; 's are also subject to 
variation, then horizontal as well as vertical errors in Figure 4.4.1 need to be considered, and 
a more general theory known as total least squares would emerge. The least squares line iB 
obtained by minimizing only vertical deviations will not be the closest line to points in D 

482 
Chapter 4 
Vector Spaces 
The vertical distance from an observation (xj, yi) toa line f(x) =a-+ 8x is 
& = yi — f(ai) = yi — (a+ Bai). 
(4.4.2) 
Some of the €;'s will be positive while others are negative, so instead of mini- 
mizing >; «€;, the aim is to find values for a and B such that 
m 
m™m 
vad ep = yr 5 tae? Soe 8x;)" 
is minimal. 
(4.4.3) 
t=1 
i=1 
This is the traditional (ordinary) least squares problem. The difference be- 
tween this and statistical linear regression is that linear regression considers the 
yi's and e;'s to be random variables such that E[{e;| = 0 for each 1—i.e., linear 
regression includes the hypothesis that the errors "average out to zero." "You 
need not be concerned with the distinction at this point—linear regression is 
taken up on page 489. 
Minimization techniques from calculus say that the minimum value in (4.4.3) 
must occur at a solution to the system of the two equations 
0 ga ie — Bai)" ) 
m 
= ——_________" 
= -2 
; 
-a— Bz;), 
0 
Ba 
> 
(Yi 
i) 
0 oe Ga Baxi)" ) 
_ 
enh 
at Se eae. SF 2S 25° (yi —a— Bx;) x. 
op 
i=1 
Rearranging terms produces two equations in the two unknowns a and 8 
m 
™m 
m 
Soi a+ (Som) a=Soy 
11 
i=1 
I=] 
(4.4.4) 
m 
mm 
m 
ss oe) 
Ca 
eh 
= Nes Lai 
ll 
cI 
oh 
eT 
Y1 
Late 
Yy2 
mn 
iY 
he 
By setting A = 
: 
= 
|e 
. 
|, and x= (3 , it is seen that the 
ig 
i 
two equations (4.4.4) take the matrix form A?AX = Ay, which is called the 
in terms of perpendicular distance, but L is nevertheless optimal in a certain sense—see the 
Gauss—Markov theorem on page 494. 
The terminology and statistical development of regression analysis was popularized by the En- 
glish statistician Sir Francis Galton (1822-1911) in his 1886 publication of Regression Towards 
Mediocrity in Hereditary Stature in which he observed that extreme characteristics such as 
heights of taller and shorter parents are not completely passed on to their children, but rather 
the characteristics of their children tend to revert or "regress" toward a mediocre point (the 
mean of-all children). Galton was a cousin of Charles Darwin, whose book On the Origin of 
Species stimulated Galton's interest in exploring variation in human populations. 

4.4 Least Squares 
483 
system of normal equations. The normal equations are always consistent (even 
when AX =y 
is inconsistent) because ATy € 
R(A7) = R(A7A). 
The solution X of the normal equations 
A? AX = A'y is called a least 
squares solution for the associated system AX = y (generally inconsistent) 
because X = (ei contains the coefficients in f(x) = a+ 6x (the least squares 
line) that provides the least squares fit. The vector 
¥ = AX is the predicted or 
estimated vector because its entries y; = f(x;) are the least squares estimates 
of y; that are predicted by the least squares line. The €;,'s in (4.4.2) are the 
entries in the residual (or error) vector 
€ = y — y = y — AX, so 
det = Divi — HK)? = lly - Fz = Te = (y — AR)" (y— AR). 
(4.4.5) 
i=1 
This number is referred to as the error sum of squares, and it is denoted by SSE. 
In the perfect (or ideal) situation when all data points (x;, y;) exactly lie on 
L, then 
€ = 0, and AX =y 
is a consistent system. But if not all data points 
are on a straight line, then |le||, > 0, which in turn means that y — Ax # 0, 
so that AX = y represents an inconsistent system. This observation can be 
helpful in identifying the matrix A involved in setting up more general least 
squares problems by asking yourself, "what system is required to model an ideal 
situation?" If Ax = y models the "ideal" situation that is not actually realized, 
then the solution of the associated normal equations 
A? AX = Ay provides 
the least squares solution. 
Partitioning Sums of Squares 
In addition to the error sum of squares (SSE) in (4.4.5), there are two other 
relevant sums of squares. Let 
= ly = (0; yi)/m and e be a column of 1's, 
and define 
SST: 
The total sum of squares =~) (yi— pb)? = lly — yell ; 
SSR: 
The regression sum of squares = )>\,(% — bw)? = |l¥ — yell , 
The relation between SST, SSE, and SSR is revealed in the following theorem. 
4.4.1. Theorem. For a set {(x;,y;)} of m > 2 non-colinear data points 
DSi: 
YA 
sod for A= 
book 
and yb 
= 
1, let y= Ax, where xis the 
deere 
ens 
Um 
least squares solution obtained from AT AX = Ay. It is always true 
that py = 
= Hy, and 
SST = SSE + SSR. 
(4.4.6) 

484 
Chapter 4 
Vector Spaces 
Proof. 
To see that py = Hy, 
it suffices to show e'y = ely. The non- 
colinearity assumption forces rank (A) = 2 so that A'A is nonsingular, and 
hence the solution of ATAX = ATy is 
%=(ATA)!ATy 
=> J=A(A7A)'ATy =Py, 
(4.4.7) 
where 
P = A(ATA)~!A? = P™ is the orthogonal projector onto R(A) (see 
(4.3.18) on page 464). Furthermore, e € 
R(A) means that Pe =e so that 
e'F =e'Py =e'Ply=ely = 
py =by. 
To prove that SST = SSE+SSR, simply verify that (y—y) 1 (y—pe) (Exercise 
4.4.2) and invoke the Pythagorean theorem (page 40) to conclude that 
oat 
one 
2 
Aw) 
2 
~ 
2 
lly — wells = I(y —¥) + (¥ — He) 
lls = lly —Fllo+ll¥ — ella. 
& 
Coefficient of Determination 
The significance of being able to partition the total sum of squares as indicated 
in (4.4.6) can now be understood. The sample correlation coefficient between the 
observed vector y and the predicted vector y = AX is 
(Helle _ 89 
a 
lly — pe|l, 
Sy 
While r may be of some interest, it is not as important as r?, which is given a 
special name. 
(see (1.6.10) on page 46). 
4.4.2. Definition. The term 
a Wee 
Ne 
OR Es 
ly spells 
& 
Vary 
S6t 
SST 
i 
i =r 
is called the coefficient of determination. 
The utility of r? stems from consideration of variation. The total variation 
SST = |ly — pel = (yi — wy)? is the variation in y from its mean without 
regard to variations in x. But if, for example, y tends to vary linearly with x 
in a positive manner, then the y;'s generally increase as the «,'s increase, so a 
more important issue is, "how much (or what percentage) of the total variation 
in y is explained by the variation in x as determined by the least squares line 
L?" The term SSE = |ly — yII5 = (vi — 9)" is the variation in y that is 
not explained by C£ (i.e., by the variation in x). Since SST 
= SSE+ SSR, the 
proportion (or percentage) of the total variation in y that is explained by CL is 
_SSE _SSR_ 
SST sot" 
° 

4.4 Least Squares 
485 
Goodness of Fit 
A primary use of the coefficient of determination 0 < r? < 1 is to assess how well 
the least squares line £ fits the data (x;,y;). Each data point is exactly on L if 
and only if SSE = ||y — vale = 0—ice, if and only if r? = 1. This means that all 
of the variation in y is completely explained by £. At the other extreme, r? = 0 
if and only if SSR = ||¥ — yell} =0, which means that none of the variation in 
y is explained by L. This is equivalent to saying that L is perfectly horizontal 
and that there is not a linear relationship between x and y. For example if 
r? = .85, then 85% of the variation of y is explained by £. Whether or not 
this translates to saying that the C is a good fit for the data can be subjective 
and application dependent, but it nevertheless provides more insight than looking 
only at the raw residual SSE = )>,_, 6? = \(y — 9)? = lly — y\I3 
Example (Sales Estimation) 
Suppose that a company has been in business for four years, and the sales 4; 
for year x; (in tens of thousands of dollars) are shown in the table in Figure 
4.4.2. Plotting the data points (x;,y;) for 21 = 1,22 = 2,23 = 3, and z4 = 4 
indicates that they do not exactly lie on a straight line, but nevertheless there 
is a linear trend in sales. Consequently, to predict the sales for a future year it 
is reasonable to fit the linear trend with a straight line f(x) = a+ za that best 
fits the data in the sense of least squares. 
SALES 
w 
jam) 
e 
YEAR 2; 
SALES y; 
0 
1 
2 
3 
4 
YEAR 
FIGURE 4.4.2: LINEAR SALES TREND 
If sales were exactly linear, then there would exist an 
a and ( such that 
23 
ee 
y; =a+ Ba; for each 1 = 1,2,3,4 so that (3 
= ( ; 
Cu or equiva- 
34 
1 
4 
lently, 
y = AX. But sales are not exactly linear, so €; = yi — (a + Ba;) = Oxtor 
at least one i, or equivalently, 
«e 
= y— Ax # 0. Least squares theory guarantees 
that the solution X of the associated system of normal equations 
wrag= Aly = (8 B)(8)= (HH) = = (8)= (88) 

486 
Chapter 4 
Vector Spaces 
yields the least squares line £L as 
G(x) = @+ Br = 19.5 + 3.62, 
which in turn provides a sales estimate for any year z—e.g., Y(5) = $375,000 
is the estimated sales for year five. To get a feel for how well the line L ex- 
plains the observed sales over time, set Y = AX, and compute the coefficient of 
determination from (4.4.8) to be 
~ 
2 
2 _ I¥ — Helle _ 648 gg ¢q03. 
_ 
lly—pell, 
65 
Thus about 99.7% of the variation in sales over time is explained by CL, or 
equivalently, only about .3% of the variation in sales over time is not explained 
by L. This suggests that the least squares model can be a good predictor of 
future sales, assuming of course that the trend continues to hold. 
Vector Space Theory of Least Squares 
Viewing concepts from more than one perspective generally produces a deeper 
understanding, and this is particularly true for the theory of least squares. While 
the classical calculus-based theory of least squares as discussed earlier can be 
extended to cover more general situations, it is generally replaced by a more 
intuitive development based on vector space geometry. This approach not only 
produces a cleaner theory, but it also brings the entire least squares picture into 
sharper focus. Rather than fitting a straight line to a data set of ordered pairs, 
more general least squares concerns the following problem. 
e 
Given A ¢ F™*"" and y € F", find a vector X € F" such that AX is as 
close to y as possible in the sense that ||y — AX||> = minxeF |/y — Ax(||> A 
or equivalently, ||y — AX||, = minxer ||y — Ax|l,. 
Since AX is always a vector in R(A), the problem boils down to finding the 
vector p € R(A) that is closest to y. The closest point theorem on page 466 
solves this problem because it guarantees that p = P R(A)Y, Where Prray is 
the orthogonal projector onto R(A). Figure 4.4.3 below illustrates the situation 
in R°. 
We 
. 
min |ly — Ax||, 
= |ly — 
Bee 
lb = 
lly - 
Pll 
a 
R(A) 
P=Pacay¥ 
FIGURE 4.4.3: PROJECTION ONTO R(A) 

4.4 Least Squares 
487 
Therefore, ¥ is a vector such that ||y — AX||5 is minimal if and only if 
Ax =p=Prvapy. 
However this is just the system of normal equations A*AX = A*y in disguised 
form' because 
AX = Priayy => Pray 
AX = Prrayy —> Pra) (AX — y) = 0 
<=> (AX —y) € N (Pray) = R(A)" = N(A*) 
<=> A*(AR—y) =0 <> A*AR=A'y. 
In summary, this means that the definition of a general least squares solution 
can be stated in any one of three equivalent ways. 
_ 44.3. Definition. A least squares solution for a system 
of linear equa- 
tions Amynx = y (possibly inconsistent) is defined 
to be a vector 
_ 
x € F" that satisfies any one of the following three equivalent state- 
- ments 
in which Ppa) is the orthogonal projector onto R(A). 
— 
ely — AX|lp = minxer lly — Ax|lp 
(4.4.9) 
e 
AX=p=Paayy (the projection equation) 
(4.4.10) 
e 
A*Ax=A'*y (the normal equations) 
(4.4.11) 
Note that the 2 x 2 system of normal equations in (4.4.4) on page 482 is 
just a special case of the more general system of normal equations (4.4.11) that 
results from the vector space theory. 
Caution! The statements in (4.4.9)—(4.4.11) are the theoretical foundations for 
least squares theory, but they are generally not used for practical floating-point 
computation. Explicitly forming the product A*A and then solving the nor- 
mal equations is ill-advised because if & is the two-norm condition number for 
A, then «? is the two-norm condition number for A*A (see Exercise 3.5.21 
on page 380), so any sensitivities to small perturbations (e.g., rounding error) 
that are present in the underlying problem are magnified by computing A*A 
(see Exercise 2.8.8 on page 246). Stable algorithms generally involve orthogonal 
reduction techniques that are discussed on pages 671 and 675. 
Note that this discussion allows for complex matrices whereas earlier discussions were restricted 
to real matrices. This is because traditional linear least squares analysis is almost always in 
the context of real numbers, but more general least squares applications can involve complex 
matrices. 

488 
Chapter 4 
Vector Spaces 
The Set of All Least Squares Solutions 
If rank(Amxn) =7, then (A*A)nxn is nonsingular, so the system of normal 
equations (4.4.11) yields a unique least squares solution given by 
% = (A*A)' A*y. 
But unlike the traditional problem on page 481, A need not have full column 
rank, in which case there are infinitely many least squares solutions. The set of 
all least squares solutions is the complete solution set for the projection equation 
AX = p in (4.4.10), and Theorem 2.5.7 on page 211 ensures that this set is 
S = Xpart + N (A), 
where Xpart is a particular solution of Ax = p. A convenient particular solution 
is the pseudoinverse solution Xpart = Aly because 
A(Aty) = (AA')y = Prrayy =p 
(recall (4.3.17) on page 464). 
Therefore, the set of all least squares solutions for a general system Ax =y 
is 
S=Aty+N(A). 
(4.4.12) 
Note that S is also the solution set for Ax = y when this system is consistent 
because if y € R(A), then AATy = Prrayy =y. 
Not only is ATy a particular least squares solution, it is the unique mini- 
mal 2-norm solution among all least squares solutions. This follows from (4.4.12) 
because if z is any other least squares solution, then z = A'ty +h, where 
he N(A)=R(A*)-=R (At) (recall (4.3.17), page 464), and hence the 
Pythagorean theorem (page 40) yields 
2 
2 
2 
2 
l2lla = || ATy 
+ 
hl], = ||Aty||, + llblls > |A*yll., 
with equality holding if and only if h = 0 —i.e., if and only if z = Aly. These 
observations are summarized in the following theorem. 
4.4.4. Theorem. Let A,,,..,x = y bea general system of linear equations. 
e 
The set of all least squares solutions is S = Aly + N (A). 
— X= Aly is the minimal 2-norm least squares solution. 
e 
Ifthe system is consistent, then its solution set is S = Aly + N (A). 
— X= Aly is the minimal 2-norm solution. 
e 
In either case, there is a unique solution (or least squares solution) 
if and only if rank (A) =n, and it is given by 
t= aly =(ataytaty=>° UW), 
ere 
where uj,v;,0; are the SVD components of A (Exercise 3.5.20). 

4.4 Least Squares 
489 
Linear Regression 
The traditional least squares problem of fitting data points («;,y;) to a straight 
line as discussed on page 481 morphs into the statistical theory of linear regres- 
sion (also called multiple regression) when the goal is to relate a random variable 
Y that cannot be observed exactly to a linear combination of two or more vari- 
ables (often called independent variables) X1,X2,...,Xp that are not subject 
to error or variation and can be exactly measured or observed (e.g., X; =the 
precise month of the year, X2 =the exact time of day, X3 = your current age, 
etc) together with another random variable € such that 
Y = Bo + BiX1 + BoXo++--+BnXn+e, 
(4.4.13) 
in which the parameters {o, 51,...,(, are unknown constants. The role of ¢€ 
is to account for the fact that Y cannot be observed or measured exactly, or 
that other factors (e.g., simplifying assumptions or modeling errors) are not 
considered, but the effects of all of these errors' "average out" to zero in the 
sense that Ele] = 0, where E|x] denotes expected value (or mean). In other 
words, the regression assumption is that the mean value of Y at each point 
where (Xj, X2,...,Xn) can be observed is given by 
E(Y) = Bo + 1X1 + BoX2+-+:+ BaXn. 
(4.4.14) 
Estimating the unknown parameters 
{§; involves making a series of m 
re- 
spective measurements or observations 
y; and 
(2j1,2%j2,.-.,2in) 
of Y and 
(X1,Xo,...,Xn) and hypothesizing that 
Yi = Bo + iti + Boia t+-+:+ Brtin+€;, 
*=1,2,...,m, 
(4.4.15) 
where €; is a random error for which it is assumed that Ele;| = 0. This results 
in vectors and matrices' 
Y1 
Sprig, 
Baily 
30 
He 
Bo 
€1 
Y2 
tot 
22 
a 
io, 
Br 
€2 
se 
ee ee 
ae 
ae) 
ee 
ee 
Le 
(4.4.16) 
Ym 
1 mi 
Lm2 
°**: 
Lmn 
Bn 
Em 
such that 
y = X@+e, or equivalently 
y 
XG =e. An estimate B of B is 
provided by the general theory of least squares by taking @ to be a vector such 
The difference between a measurement (or observation) error and a modeling error may be 
significant to an experimentalist, but mathematicians and statisticians generally do not make 
a distinction because they are equivalent from a mathematical standpoint. 
Note the difference in notation between the least squares and linear regression discussions. This 
is because it is a long-standing convention in traditional statistics to express a linear regression 
model as y = X+€ 
rather than using the ordinary least squares notation Ax=y. 

490 
Chapter 4 
Vector Spaces 
=> 
2 
a= 
. 
. 
that ly 
- xA| is minimal, or equivalently, 
@ is a solution to the system of 
2 
normal equations xTxB = XTy. Consequently, the estimate of the mean value 
of Y for any given set of values for (X1, X2,...,Xn) is 
E[Y] = Bo + B1X1 + BoX2t+-+-+BnXn, 
(4.4.17) 
where Bi is the least squares estimate for §;. For the least squares estimate 
y¥ = XQ, exactly the same analysis that led to the coefficient of determination 
on page 484 holds for the more general case of multiple regression, so 
a 
2 
po = ly = 
Hella, where 
[= Wy, 
(4.4.18) 
lly — Hell> 
can again be used to gauge the proportion of the observed vector y that is 
explained by the regression model, and thus it measures the goodness of fit. 
The Gauss-Markov theorem on page 494 says that under reasonable as- 
sumptions about the random error 
e€, the least squares estimates @ for the 
@B and the estimate (4.4.17) for (4.4.14) are optimal in a particular sense. But 
before getting into this, it may be helpful to look at a simple example. 
Example (Stale Pop) 
Everyone knows that when the unused half of an open can of soda (or "pop" as 
it is called in Greeley, Colorado) is put back into the refrigerator, it increasingly 
loses its palatability the longer it is left. For a particular brand (say Coca-Cola) 
there can be several factors that influence this—e.g., the number of days an 
opened can remains in the refrigerator, the refrigerator's temperature, the origi- 
nal level of carbonation, amounts of ingredients such as high fructose corn syrup, 
artificial sweeteners, phosphoric acid, flavorings, etc. It is reasonable to conjec- 
ture that storage time and temperature are the primary factors, and other factors 
"average out." To predict the loss of palatability make a linear hypothesis of the 
form 
Y = Bo + 81X1+ BoX2+6¢€, 
where 
Efe] = 0 
. 
. 
' 
in which 
Y = the percent of palatability lost compared to that 
of a fresh can as subjectively judged by a panel of tasters, 
X, = the number of days stored after opening, 
X2 =the temperature (°C) of the refrigerator during storage time. 
In other words, the hypothesis is E(Y) = Bo + 8,X1 + BX», and producing an 
estimate E[Y] = 8p + 6,X1 + BoXo is accomplished by conducting experiments 
to determine least squares estimates for each 6;. The following table records the 

4.4 Least Squares 
491 
storage times and temperatures for nine experiments along with judgements of 
loss of palatability for each of these values. 
2 = Storage Time (days) 
x2 = Storage Temp (°C) 
y = Palatability Loss (%) 
iS 
IMG) 
ALO) 
I 
HS) 
PPT) 
Yas) 
PAs) 
The model is y = XG+ €, where 
Lee 
al 
€1 
16 
liar 
22! 
€2 
20 
it 
ate 
€2 
li 
th 
2 
Al 
Bo 
€4 
y= 
oes 
x = 
IgE 
PA 
it 
p=(#), 
and 
Eé— 
€5 
22 
eS. 
Bo 
€6 
20 
ik 
apie 
Gl 
€7 
23 
il 
eutlce 
2 
€8 
25 
Se 
3 
€9 
with the assumption that Efe;] = 0 for each i. Least squares estimates B; for 
the £;'s are obtained by solving the normal equations 
9 16 
18\ 
(Bo 
177 
_ 
9 
XG 
= Xx 
yo 
(:: 42 2) CHA (221) => B= (27/8). 
18 
36 42/\% 
369 
5/2 
In this case the coefficient of determination (to three significant places) is 
a 
2 
»_ (ie —uell 2 — 97.3, 
lly — Hell, 
so more than 97% of the variation in y is explained by the regression model, 
while less than 3% of the variation is not, and thus the least squares fit is pretty 
good. For example, the regression model predicts that a half can of opened Coke 
stored for three days in a refrigerator set at 
4°C' is expected to lose about 
Bo + Bi(3) + Bo(4) = 9 + (7/16)(3) + (5/2)(4) = 27.5% 
of the palatability of an unopened can. 
Least Squares Estimates are Optimal 
Drawing inferences about natural phenomena based upon physical observations 
and estimating characteristics of large populations by examining small samples 
are fundamental concerns of applied science. Numerical characteristics of a phe- 
nomenon or population are often called parameters, and the goal is to design 
functions or rules called estimators that use observations or samples to estimate 

492 
Chapter 4 
Vector Spaces 
parameters of interest. For example, the mean height h of all people is a pa- 
rameter of the world's population, and one way of estimating h is to observe 
the mean height of a sample of k people. In other words, if hj is the height of 
the i' person in a sample, then the function h defined by 
k 
= 
i\ 
h(hy, ha, vee hr) — ,. (>: 
ms) 
i=1 
is an estimator for h. Moreover, h is a linear estimator because h is a linear 
function of the observations h,. 
Good estimators should possess at least two properties—they should be un- 
biased and they should have minimal variance. For example, consider estimating 
the center of a circle drawn on a wall by asking Larry, Moe, and Curly to each 
throw one dart at the circle. To decide which estimator is best, knowledge about 
each thrower's style is required. While being able to throw a tight pattern, it is 
known that Larry tends to have a left-hand bias in his style. Moe doesn't suffer 
from a bias, but he tends to throw a rather large pattern. However, Curly can 
throw a tight pattern without a bias. Typical patterns are shown below. 
LARRY 
MOE 
CURLY 
Although Larry has a small variance, he is a poor estimator because he is 
biased in the sense that his average is significantly different than the center. 
Moe and Curly are each unbiased estimators because they have an average that 
is the center, but Curly is clearly the preferred estimator because his variance is 
smaller than Moe's. In other words, Curly is the unbiased estimator of minimal 
variance. 
4.4.5. Definition. 
An estimator @ (considered as a random variable) 
for a parameter @ is said to be unbiased when E[6) = 
= 0, and @ is 
called a minimum variance unbiased estimator for @ whenever 
Var|6] < Var|¢] for all unbiased estimators @ of 0. 
These ideas make it possible to precisely articulate the sense in which least 
squares is optimal. Consider a linear hypothesis 
Yo= foo BTA pares, 
Aye 

4.4 Least Squares 
493 
in which Y is a random variable that cannot be exactly observed (perhaps due 
to measurement error), the X;'s are independent variables whose values are 
not subject to error or variation (they can be exactly observed or measured), 
and where € is a random variable accounting for the error. As explained in the 
previous section, least squares estimates (; for 8; are obtained by observing 
values y; of Y at m different points (21, @j2,...,in) € R", where aj; is the 
value of X; at the i" observation. This produces the linear model 
Yi = Bo + Pita + Botig +-+++Bntint+e@, 
t=1,2,...,m, 
(4.4.19) 
in which 
¢€; is a random variable accounting for the i'" observation or mea- 
surement error. Thus y; is also a random variable. A standard assumption is 
that observation errors are not correlated with each other, but they have a com- 
mon variance o? (not necessarily known) and a zero mean. In other words, it is 
assumed that' 
2 
Ele;] =O foreach i 
and 
Covle,¢;] = { 
o~ 
wheni=j, 
O 
wheni#j. 
As shown in (4.4.16) on page 489, the matrix version of the model in (4.4.19) 
is 
y = X@+e. In practice, the m > n points (%j1,2i2,--..,2in) 
at which 
observations y; are made can almost always be selected to insure that X has 
full column rank (see Exercise 4.4.11 for the rank-deficient case), so the complete 
statement of a standard linear model is 
rank CXeccet)) =n+1, 
y=XG+e 
such that 4 Ele] 
=O 
(so Ely] = XP), 
(4.4.20) 
Cove] = 071 = Cov[y] 
(so Var[e;] = 0? = Var[y:]), 
in which 
E[e,| 
Covle1,€1] 
Covie1,€2] 
++: 
Cover, én] 
Ele] 
Covleg,€1] 
Covle2,€2] 
++: 
Coviea, ém| 
Ele] = 
: 
and Cov[e] = 
4 
; 
Elen] 
Covlém,€1] 
Covlém,€2] 
-°++ 
Covlém, €m!| 
A primary goal is to determine the best (i.e., minimum variance) linear (lin- 
ear function of the y;'s) unbiased estimators for the components of 3. Gauss 
realized that this is precisely what the theory of least squares provides. 
: Recall from elementary probability that for random variables A,B and a constants 
a, }, 
e 
EfaA+ B] =ak[A]+E[B] 
(ie., expectation is linear), 
e 
Var[A] = E[(A - ja)? = E[A?] — p,, 
® Cov[AB] = E[(A — #a)(B — )] = EAB] - wane, 
e 
Var[aA + bB] = a?Var[A] + 6?Var[B] 
when Cov[AB] = 0. 

+ 
Proof. 
It is clear that B = X'y isa 
linear estimator because each component 
B; = Yl X ix yx is a linear function of the observations yz. To see that @ is 
unbiased, use E[y] = X@ and Xt = CCE) ae (see page 194) to write 
E[A] =E[Xty] = XtEly] = X'xe = (XTX) 'X?xe=8. 
To argue that B = X'y has minimal variance among all linear unbiased estima- 
tors for B, let @* be an arbitrary linear unbiased estimator for G. Linearity 
of 8* implies the existence of a (constant) (n+1) xm matrix L such that 
3* =Ly so that 
m 
m 
Var[B7] = Var[Lisy] = Var | S— liye | 
= 0? S12, = 0? ||Lielld 
k=1 
k=1 
(see the variance formula in the footnote on page 493). Unbiasedness ensures that 
@ = E[S*] = E[Ly] = LEly] = LX@ for all 
@¢ R"*!, and hence LX =I,41 
(Exercise 1.7.10, page 65). Therefore, Var[G*] is minimal if and only if Lj, is 
the minimum norm solution for z" in the left-handed system z7X = e?. In 
general, the unique minimum norm solution is given by z' = e? Xt = x! 
(Theorem 4.4.4, page 488), and thus Var[3*] is minimal for each i if and only if 
Lae xe or equivalently, 
L = X'. Therefore, the (unique) minimal variance 
linear unbiased estimator for B is Xty = B. The proof of (4.4.21) follows along 
the same lines, so details are left to the reader. 
Gauss is generally credited with realizing this theorem in 1821, but historians are ambivalent 
about Markov's contribution. Andrei Andreevich (or Andrey Andreyevich) Markov (1856-1922) 
was a slow starter in school at Petrograd (now St. Petersburg), Russia, but he eventually found 
his stride in studying mathematics and became a student of Pafnuty Lvovich Chebyshev who 
stimulated his interest in probability. This led to Markov's development of Markov chains (page 
859), that subsequently launched the theory of stochastic processes. Markov preferred the rig- 
orous side of probability theory, and he is said originally to have had a negative attitude toward 
statistics—he judged it strictly from a mathematical point of view. But when his views later 
softened, his interests merged to produce what eventually became known as "mathematical 
statistics." However, in the 2001st edition of the historical anthology Statisticians of the Cen- 
turies, Hugene Seneta suggests that while Markov had an interest in statistical linear models, 
it may be inappropriate for his name to be attached to this theorem. More about Markov is 
on page 859. 

4.4 Least Squares 
495 
Least Squares Curve Fitting 
When a set of observations 
D = {(21,y1), 
(2,y2), ..., (2m;Ym)} does not 
follow a linear trend, attempting to fit the data to a straight line as described 
on page 481 is not productive, so a more general approach is to fit the data to a 
curve defined by a polynomial 
p(x) 
=an + az + agz* +---+an2" 
with a specified degree n < m that comes as close as possible in the sense of 
ordinary least squares. 
(Zm,Ym) ? 
p(x) 
| 
! 
Em ! 
! 
| 
° (fm; P(Lm)) 
(x2, yo) ? 
(x2, p(z2)) 
FIGURE 4.4.4: LEAST SQUARES POLYNOMIAL 
The same assumptions described for ordinary least squares problems as discussed 
on page 481 remain in effect, and the analysis is identical to that described on 
page 486. For the e; 's indicated in Figure 4.4.4, the objective is to minimize the 
sum of squares 
m 
m 
2 
be 
pS oe € 
a= De (yi — p(zi))" 
= (y — Ax) (y — Ax), 
(4.4.22) 
fat 
t=1 
where 
Ih 
a 
a 
ny 
ao 
Y1 
€1 
lao 
«tp 
os 
ae 
orl 
Y2 
€2 
A= 
: 
: 
; 
: 
Je. 
: 
a) ae 
: 
Ca 
: 
a) ae Ax. 
1am 
th, + 
am, 
ae 
Be 
a 
In other words, the least squares polynomial of degree n is obtained from the 
least squares solution associated with the system Ax = b because it was demon- 
strated on page 487 that a vector x such that |ly — AX||> is minimal must 

496 
Chapter 4 
: 
Vector Spaces 
satisfy the normal equations A7AX = A'y. The least squares polynomial 
do 
ay 
' 
p(x) 
= Qo + @:24 +--+ +Gn2" defined by 
X= | . 
is unique provided that 
din 
x; #2; for all 
i#j because A is a Vandermonde matrix, and it is shown on 
page 60 that all such matrices have full column rank, which in turn makes ATA 
nonsingular so that 
K = (A7A)"1ATy 
is the unique least squares solution. 
Example (Missile Tracking) 
A missile is fired from enemy territory, and its position in flight is observed by 
radar tracking devices at the following positions. 
x =Position down range (miles) 
0 
250 
500 
750 
1000 
y =Height (miles) 
0 
8 
5a 
[Oe 
20 
Suppose that intelligence sources indicate that enemy missiles are programmed 
to follow a parabolic flight path—a fact that seems to be consistent with the 
trend suggested by plotting the observations as shown below. 
y 
= 
Height 
——————>> 
0 
— 
a 
eee 
== 
0 
250 
500 
750 
1000 
x = Range ———_—_»> 
FIGURE 4.4.5: MISSILE OBSERVATIONS 
Problem: 
Where is the missile expected to land? 
Solution: 
Determine the parabola p(x) = ap + ayx + aor? that best fits the 
observed data in the ordinary least squares sense, and then estimate where the 
missile will land by finding the roots of p to determine where the parabola 
crosses the horizontal axis. In its raw form the problem will involve numbers 

4.4 Least Squares 
497 
having relatively large magnitudes in conjunction with relatively small ones, so 
it is better to first scale the data by considering one unit to be 1000 miles. For 
0 
0 
0 
1 
ee 
2ome 
625 
ao 
.008 
A= 
i © 
5) 
E25 
, 
RS (a: 
), and 
Vis 
015. 
|, 
iP 
idoy 
Bakes) 
ag 
.019 
ee 
Bl 
1 
.02 
~ 
the aim, as described on page 495, is to find a value * such that 
a2 
: 
2 
lly — AX||) = min, |ly — Ax||), 
xeR 
which is equivalent to solving the normal equations 
vie 
5 
2.5 
1.875 
Qo 
.062 
AT AX = Aly, => 
(2s 1.875 
1.5625 
) a 
|= ( 
o1sr ) 
1.875 
1.5625 
1.3828125/ 
\ ao 
.0349375 
ao 
—2.285714 x 10-4 
= 
X=) 
ai.) 
= ( 
3.982857 x 107? | 
(to seven places). 
a2 
—1,942857 x 10-2 
Thus the least squares parabola is 
p(x) = —.0002285714 + .039828572 — .0194285727, 
and the quadratic formula yields x = 0.005755037 and x = 2.044245 (to seven 
places). These are where p(x) crosses the x-axis, so the estimated point of 
impact (after rescaling) is 2044.245 miles down range! To get a sense of how 
good the fit is, compute 
—2.285714 x 10-4 
8.514286 x 1073 
y= AX 
| 
UAg2857 x 10-- 
ity == 0.0124 = fry, 
1.871426 x 10-2 
NO UCAS 910 
to evaluate the coefficient of determination from (4.4.18) on page 490 to be 
2 l¥—pellz _ 2.807428 x 10-4 
= 
ee 
9983 743. 
Therefore, about 99.84% of the variation in y is explained by the least squares 
parabola, so the fit is pretty good, and people who are somewhere around 2044 
miles down range should seek immediate shelter. 
Remember that the observations are not expected to lie exactly on the least squares curve, so 
x = 0.005755037 
is just the least squares estimate of the launch point (the origin). 

498 
Chapter 4 
Vector Spaces 
Least Squares vs. Lagrange Interpolation 
For a given set of m points D = {(x1,y1), 
(@2, 42), +++ (tm; thm)} in which 
the 2,'s are distinct, it is established on page 551 that the Lagrange interpolation 
polynomial 
iu 
ITjzi(e — 25) 
(x) = ya Yi 
i=1 
[jy (z — 23) 
exactly passes through each point in D. So why would one want to settle for a 
least squares fit when an exact fit is possible? 
One answer is that in practical work the observations 
y; are rarely ex- 
act due to small errors arising from imprecise measurements or from simplified 
assumptions, so the goal is to fit the trend of the observations and not the un- 
certain observations themselves. Furthermore, to exactly hit all data points, the 
interpolation polynomial f(x) is usually forced to oscillate between or beyond 
the data points, and as m becomes larger the oscillations can become more 
pronounced. Consequently, ¢(xz) is generally not useful in making predictions 
beyond the observations. The missile tracking example on page 496 drives this 
point home. 
The fourth-degree Lagrange interpolation polynomial for the five observa- 
tions listed on page 496 is 
11 
Leberg 
er 
1 
3 
eve ets Rake 
eens ee ee 
gee 
(2) = 375" + 
Zs0000" ~ ia7s0000" + 468750000007 
It can be verified that ¢(x;) = y; for each observation. As the graph in Figure 
4.4.6 indicates, (a) has only one real nonnegative root, so it is worthless for 
predicting where the missile will land. This is characteristic of Lagrange inter- 
polation. 
60 
40 
20 
500 
1000 
1500 
FIGURE 4.4.6: INTERPOLATION POLYNOMIAL FOR MISSILE TRACKING DATA 

4.4 Least Squares 
499 
Epilogue 
It was mentioned on page 482 that Sir Francis Galton introduced the concept 
of linear regression, but he did not invented the theory of least squares. That 
honor belongs to Carl Gauss. While viewing a region in the Taurus constellation 
on January 1, 1801, Giuseppe Piazzi, an astronomer and director of the Palermo 
observatory, observed a small "star" that he had never seen before. As Piazzi 
and others continued to watch this new "star" (which was really an asteroid) 
they noticed that it was in fact moving, and they concluded that a new "planet" 
had been discovered—a really big deal back then. However, their new "planet" 
completely disappeared in the autumn of 1801. Well-known astronomers of the 
time joined the search to relocate the lost "planet," but all efforts were in vain. 
In September of 1801 Gauss decided to take up the challenge of finding this 
lost "planet." Gauss allowed for the possibility of an elliptical orbit rather than 
constraining it to be circular—which was an assumption of the others—and he 
proceeded to develop the method of least squares. By December the task was 
completed, and Gauss informed the scientific community not only where the lost 
"planet" was located, but he also predicted its position at future times. They 
looked, and it was exactly where Gauss had predicted it would be! The asteroid 
was named Ceres, and Gauss's contribution was recognized by naming another 
minor asteroid Gaussia. 
This extraordinary feat of locating a tiny and distant heavenly body from 
apparently insufficient data astounded the scientific community. Furthermore, 
Gauss refused to reveal his methods, and there were those who even accused 
him of sorcery. These events led directly to Gauss's fame throughout the entire 
European community, and they helped to establish his reputation as a mathe- 
matical and scientific genius of the highest order. 
Gauss waited until 1809, when he published his Theoria Motus Corporum 
Coelestium In Sectionibus Conicis Solem Ambientium, to systematically develop 
the theory of least squares and his methods of orbit calculation. This was in 
keeping with Gauss's philosophy to publish nothing but well-polished work of 
lasting significance. When criticized for not revealing more motivational aspects 
in his writings, Gauss remarked that architects of great cathedrals do not obscure 
the beauty of their work by leaving the scaffolds in place after the construction 
has been completed. Gauss's theory of least squares has indeed proven to be a 
great mathematical cathedral of lasting beauty and significance. 
Exercises for section 4.4 
4.4.1. Consider the ordinary least squares problem of fitting a straight line 
y =a+ zx to m non-colinear data points (2;,y;). Let x be the least 

500 
Chapter 4 
Vector Spaces 
1 
24 
. 
. 
1 
22 
squares solution such that ATAX = ATy inwhich 
A= 
|. 
.- 
|, 
ee 
Y1 
y=|. |¢R(A), and rank (A) =2. 
Ym 
a = Female Retort eal 
aes m So iyi—(So%) (Su) 
J' 
Bis 
; 
; 
where A = m)> 
2? — (SD 2;)* = m||x — pxel|> in which e is 
the vector of 1's and pu, denotes the mean. 
(a) Show that x= ( 
(b) Prove that (Ux, /y) always lies on the least squares line. 
(Xs Lxe)" (y - bye) 
Hint: Show ~ = 
and a@ = py — Bix. 
2 
Ix — Uxe||5 
acpi Cov yN belies 
(c) Show that B = 
52 = 
Var |x| 
= 
xy 82' 
where sx, Sxy, and rxy are the respective sample standard de- 
viation, covariance, and correlation as defined in (1.6.4), (1.6.10), 
and (1.6.11) on pages 44-46. 
4.4.2. For the least squares problem described in Exercise 4.4.1, prove that 
(y —y) L(y —pye), where y = AX in which x is the associated least 
squares solution, and ps = ply = Ly. 
4.4.3. For the ordinary least squares problem, let 
¢€; be the i*" error (the 
vertical deviation from the least squares line) as depicted in Figure 4.4.1 
on page 481. Prove that }°,€; =0. 
4.4.4, Hooke's law says that the displacement y of an ideal spring is propor- 
tional to the force x that is applied—i.e., y = kx for some constant k. 
Consider a spring in which & is unknown. Various masses are attached, 
and the resulting displacements shown in Figure 4.4.7 are observed. Us- 
ing these observations, determine the least squares estimate for k. 
z (Ib) 
y (in) 
Z 
5 
lyon 
ff 
15.4 
y 
8 
eo 
10 
22.0 
it 
Ove 
x 
FIGURE 4.4.7: HANGING SPRING 

4.4 Least Squares 
4.4.5. 
4.4.6. 
4.4.7. 
4.4.8. 
501 
Show that the slope of the line that passes through the origin in R? and 
comes closest in the least squares sense to passing through the points 
{(a1, Y1)s (xo, Yy2), ately ( 
Dds Ym) } is given by B aes py Paya} 2h De 
Caution! The result of Exercise 4.4.1 does not apply to this case. 
A small company has been in business for three years and has recorded 
annual profits (in thousands of dollars) as follows. 
Assuming that there is a linear trend in the declining profits, predict the 
year and the month in which the company begins to lose money. 
An economist hypothesizes that the change (in dollars) in the price of a 
loaf of bread is primarily a linear combination of the change in the price 
of a bushel of wheat and the change in the minimum wage. That is, if B 
is the change in bread prices, W_ is the change in wheat prices, and M 
is the change in the minimum wage, then 
B = aW+ 6M. Suppose that 
for three consecutive years the change in bread prices, wheat prices, and 
the minimum wage are as shown below. 
Year 1 
Year2 
Year 3 
Use the theory of least squares to estimate the change in the price of 
bread in Year 4 if wheat prices and the minimum wage each fall by $1. 
Consider the problem of predicting the amount of weight that a pint 
of ice cream loses when it is stored at low temperatures. There are 
many factors that may contribute to weight loss—e.g., storage temper- 
ature, storage time, humidity, atmospheric pressure, butterfat content, 
the amount of corn syrup, the amounts of guar gum, carob bean gum, 
locust bean gum, cellulose gum, and the lengthly list of other additives 
and preservatives sometimes used. Conjecture that storage time and 
temperature are the primary factors, so to predict weight loss make a 
linear hypothesis of the form 
Y =o + 61X1 + BoX2 +¢, 

502 
Chapter 4 
Vector Spaces 
4.4.9. 
4.4.10. 
where Y = the weight loss (grams), X; = the storage time (weeks), 
X» = the storage temperature (F°), and € is a random variable to 
account for all other factors. Assume that E[e] = 0, so the expected 
weight loss at each point (1,22) is E[Y] = Bo + Bi1X1 + B2X2. An 
experiment in which values for weight loss are measured for various 
values of storage time and temperature is shown below. 
Using this data, estimate the expected weight loss of a pint of ice cream 
that is stored for nine weeks at a temperature of —35°F. Then use the 
coefficient of determination to gauge the goodness of fit. 
After studying a certain type of cancer, a researcher hypothesizes that 
in the short run the number (Y) of malignant cells in a particular 
tissue grows exponentially with time (T). That is, 
Y = Boe®?. De- 
termine least squares estimates for the parameters $9 and 6; from the 
researcher's observed data given below. 
Y (cells) 
16 
27 
45 
74 
122 
Hint: What common transformation converts an exponential function 
into a linear function? 
For 
Ac F"*"" and y € F", prove that x2 is a least squares solution 
for Ax = y if and only if x2 is part of a solution to the larger system 
(Pomme 
Note: It is not uncommon to encounter least squares problems in which 
A is large and sparse (mostly zero entries). For these situations the 
system above may contain significantly fewer nonzero entries than the 
system of normal equations thereby helping to mitigate memory require- 
ments, and it circumvents the need to explicitly form the product A*A 
that has inherent numerical sensitivities as explained on page 487. 

4.4 Least Squares 
4.4.11. 
4.4.12. 
4.4.13. 
503 
Rank Deficient Models. In multiple regression models 
y = XG +e as 
described on page 489 it can happen that the matrix Xmx(n+1) is rank 
deficient (i.e., rank (X) <n-+1). Consequently, the normal equations 
X?X@B=X7y do not havea unique solution so that at any given point 
(%1,%2,...,%p), there are infinitely many different estimates 
E[Y] = BortiBity 
> ob Baty. 
To remedy the situation, points at where estimates are made must be 
1 
B 
| 
. 
h 
Rl 
restricted. Prove that if 
t= | . | ER (X*), ane We 
= 
E 
is 
Bn 
any least squares solution, then the estimate defined by 
EY] = Bo + Bits +--+ Batn = t™ 6 
—_ 
is unique in the sense that E[Y] is independent of which least squares 
solution £ is used. 
Using least squares, first fit the following data 
y 
2 
eo ee eS ee A 
AS el Oe Sa 
with a line y = 89+6,2 and then fit the data with a quadratic function 
y = Bo + Bix + Box. Determine which of these two curves best fits 
the data by computing the error sum of squares and the coefficient of 
determination in each case. (See the note in Exercise 4.4.13.) 
Let x be the unique least squares problem associated with an inconsis- 
tent system Ax =y in which rank (Am xn) =n. 
(a) Explain why the error sum of squares given in (4.4.5) on page 
483 can be expressed as 
SSEa = >) 7 = |y— ARI3 = | - Pa)yll.=y' y 
—y? Pay 
7 
where P,4 is the orthogonal projector onto R(A). 
(b) Prove that if A is augmented with a column 
c to produce 
B = [A|c] with rank(B) = 
+1, and if SSEg is the error 
sum of squares for the least squares problem associated with 
Bx = y, then SSEg < SSE,4, with equality holding if and 

. a 
fia 3 . 
'ove 
] ; 7 
( 
pee Bins. CLP 
a Ne 
ary 
Lat 
TOy 
a 
" 
pz 
PN 
y Qy 
J 
; 
de 
= i — 
5 ai 
_ 
by, 
ihe 
vos 
ane Sierra 
where 
Q=I1-. se dee —o 
Hint: Recall that the trace of an idempotent matrix is its rank (see 
Exercise 4.2.20, page 452), and use the fact that for a matrix Z = [Z;,] 
of random variables, E[{Z] is defined to be the matrix whose (i, j)-entry 
is E[Z;;]. 
shies 
—— 
; 

4.5 Coordinates 
505 
4.5 COORDINATES 
Example 
The connection between linear functions and matrices is at the heart of linear 
algebra. As discussed on page 55, matrix algebra grew out of Cayley's discovery 
that the composition of two linear functions is represented by the multiplication 
of two matrices, but there is more to say. The purpose here is to develop the 
formal theory of linear functions defined on vector spaces, so in many respects 
this is the point at which the theoretical study of linear algebra begins in earnest. 
Developments over time are prone to produced different terminologies for 
essentially the same or similar mathematical concepts, and so it is in linear 
algebra. For vector spaces U,V over F, the following terminologies are often 
used in lieu of the term "linear function" as defined on page 428 
— A linear function f : U > Y is alternately called a linear transformation 
from U into V, so the two terminologies can be used interchangeably. 
— A linear function f:U—>U that maps U back into itself is called a linear 
operator on U. 
e 
The function' 0(x) = 0 that maps all vectors in a space U to the zero vector 
in another space Y is a linear function (or transformation) from U into V, 
and, not surprisingly, it is called the zero transformation. 
e 
The linear function J(x) =x that maps every vector from a space U back 
to itself is a linear operator on U/ —it is called the identity operator on U. 
e 
For 
Ac F™*" and x ¢ F™!, the linear function f(x) = Ax is a linear 
operator on F" when m=n. 
e 
Rotators P that respectively rotate vectors u in R?,R°®, and R" through 
an angle @ as discussed on pages 94-100 are linear operators on their re- 
spective spaces because in each case the "action" of P on uw is described by 
multiplication with a square matrix. And for the same reason, reflectors and 
projectors as discussed on pages 104 and 457 are linear operators. 
e 
If W is the vector space of all functions from R to R, and if V is the 
subspace of differentiable functions, then the mapping D(f) = df/dzx defines 
a linear transformation from Y into W because differentiation is linear. If V 
is the subspace of all continuous functions from R into R, then the mapping 
S(f)= (fs f(t)dt is a linear operator on VY —recall the examples on page 53. 
Note: It would be wrong to infer from this example that the "action" of all linear 
operators can be described by multiplication with a matrix—general differential 
and integral operators cannot because they are defined on infinite-dimensional 
spaces. However the action of all linear functions on finite-dimensional spaces 
can, and that is one of the main points to be established. 
Functions (such as this zero function) are generally denoted by plain-face roman characters 
while bold-face lower- and upper-case letters are respectively reserved for vectors and matrices. 

506 
Chapter 4 
Vector Spaces 
Vector Coordinates 
Recall from the discussion on page 418 that if B = {uj,U2,...,Un} is a basis 
for an n-dimensional space U, then each v € U can be uniquely written as 
V = a1; + QoU2 +-++:+anUn. Definition 4.1.7 on page 419 defines the a; 's in 
this expansion to be the coordinates of v with respect to B, and these coordinates 
a1 
a2 
are generally listed in a column and denoted by writing [v|g = 
.}. Note 
Aan 
that the order of the basis vectors is important. If B' is a permutation of B, 
then [v]g' is the corresponding permutation of [v]g. 
In subsequent discussions S,, = {e),€2,...,@n} denotes the standard basis 
of unit vectors (in natural order) for F". The subscript n on S,, is suppressed 
when it is understood that the dimension of the underlying space is n. If no other 
basis is explicitly mentioned, then the standard basis is assumed. For example, 
if no basis is mentioned when writing a vector such as 
. 
(-2), 
(4.5.1) 
then it is understood that this is the representation with respect to S3 in the 
sense that v = [v|s = 16e; — 2e2 — 5e3. The standard coordinates of a vector 
are defined to be its coordinates with respect to S, so, 16, —2, and —5 are 
the standard coordinates of the vector v in (4.5.1). 
If the standard basis S is replaced by a different basis 
°-fo-())-»-@)-»-()} 
then determining [v]g for the vector in (4.5.1) requires finding the scalars a1, a2, 
and ag such that a,x; + a2x2 + a3x3 = v, and this amounts to solving the 
3 x 3 system of linear equations 
lee 
Ba | 
Q4 
16 
Q1 
9 
(: 2 2) 
(a) =(-2) ==> le = (@ | 
= ( 
2). 
(4.5.2) 
ee 
ss 
a3 
—5 
a3 
—=3 
The general relationship between the coordinates of a vector with respect to two 
different bases is developed in detail on page 514. 
The Space of All Linear Functions 
It is straightforward to see that if / and V are each vector spaces over F, then 
the set £(U, V) of all linear functions from U to V is another vector space over 
F because the defining properties on page 410 are satisfied—convince yourself. 
Consequently, £(U,V) has a basis Bc, and each f € L(U,V) has a set of 
coordinates with respect to Br. A convenient basis for L(U, Y) is as follows. 

Proof. The linearity of f;; is evident from its definition. Bc is a basis because 
it is a linearly independent spanning set for L(U, VY). Independence follows be- 
cause if Sr njifji =90 for scalars 7;;, use [ug]g = ex to write 
y 
me) 
aN 
if j aw 
kere ' igh 
454) 
m 
on 
Oath} (uz) 
>E nt CUk) — 
te ve 
arte 
be 
ODEs 
~ 
DX 
To see that Bc spans L(U,V), let f € L(U,V), and examine the action of f 
onany ue U. Expand u= )%_, €;u,; and f(uj) = 77", avi to obtain 
f(a) = (Som) = Ses) Bi Ae, 
j=l 
j=l 
tel 
(4.5.5) 
SS Ra = Sata 
4,9 
This holds for all 
uc U, so f = D0, ; aaj fyi, and hence Bre spans L(U,V). 
Thus Be isa 
basis for L(U, sit | 
Coordinate Matrix Representations 
It now makes sense to talk about the coordinates of f € L(U,V) with respect 
to the basis Be given in Theorem 4.5.1. The coordinates emerged in the Pret 
above where it was established that f = >/, ; 
aijfji. By definition, the aj;;'s are 
the coordinates of f with respect to Be. Furthermore, it "alter from (4.5.4) 
that 
f(ux) = De oe Saal (ux) = Year = 
[f(ux) |e = 
i 
: 
(4.5.6) 
tJ 
OnE 
Rather than listing all coordinates a;; of f as one long column containing mn 
entries as was done for the coordinates of a vector, (4.5.6) suggests arranging 
the a;;'s as columns in an m x n matrix in which the k*" column contains the 
coordinates of f(u,) with respect to B'. This leads to the following definition. 

Notation: Whenever f :U4—U 
(i.e., f isa linear operator) and B is the only 
basis under consideration, write [{f]g in place of [f|gg. The coordinate matrix 
[f]e is square in all of these cases. 
| 
:
Example 
Let P be the linear operator that maps each point v = (z,y,z) € R® to its 
orthogonal projection P(v) = (z,y,0) in the zy-plane, and consider the basis 
for R° given by 
tfa-(). (sae 
Q 
os 
To determine the coordinate matrix of P with respect to B, use the fact from 
Definition 4.5.2 that says the k' column in [P]g is [P(u,)]s. In particular, 
P(uj)= (1) =1luj+lug—-lug 
=> 
[P(u))5 = (1), 
P(ug) = (2) = 0u; + 3ug—2ug = > [P(u2)]p = (3), 
1 
0 
P(us) = (2) 
=0u,; + 3ug—-2u3; 
=> 
[P(u3)|z = ( 
2), 
2 
Thus the coordinate matrix of P with respect to B is 
Now consider the same projector P, but this time determine the coordinate 
matrix [P]gg of P with respect to the two different bases 
e={n-() -() w=()} 
e-fu=()) o-(0) m=(9} 

4.5 Coordinates 
509 
In this case Definition 4.5.2 that says the k*'' column in [P]gg: is [P(uz)|s'. 
In particular, 
P(uy) II (0) =—1lv1+0v2+0vs 
=> 
[P(u,)|e = @ 
PCr ie (1) = Sivice ivy 20ve =. (PCs = ea 
ik 
0 
AL 
—1 
Thus the coordinate matrix of P with respect to B and B' is 
Plew 
= ("2 2) 11). 
0 
0 
Example (The Identity Operator) 
If 
I: U —-U 
is the identity operator, and if B = {uj, u2,...,u,} is any basis 
for U, the coordinate matrix of J with respect to B is 
ile = ([£(us)]e| [Z(u2)) |---| 1 (un)}s) = (Curls | 
fuale | 
--- | 
funds) 
£20 
eee 
= (e; |e2| --: |en) = 
, Ei 
SL ae 
Ou Dineen 
st 
In other words, the coordinate matrix of the identity operator is the identity 
matrix. This seems natural enough, but don't be deceived. This need not be the 
case when two different bases are used for U/. For example consider the standard 
basis S for R® along with the basis B given in (4.5.7) on page 508, and consider 
the identity operator on R? with the two bases in the orders shown here. 
i R = PR 
i Re Se 
| 
| 
and 
| 
| 
B 
S 
S 
B 
Now J has two different coordinate matrices--namely [I|gs and [I]sg. Defi- 
nition 4.5.2 says that the k*" column in [I]gs is [I(uxg)]s = [ugls = ug, so 

510 
Chapter 4 
; 
Vector Spaces 
On the other hand, the k*" column in [J]sg is [J(ex)|B = [e,|s, which means 
that for each k = 1,2,3 it is necessary to solve the equation 
ex = AipUy + A2Q~U2 + A3~U3 
for the coordinates ajr. 
This can be written as Be, =e, in which 
mor 
a 
Hoes 
G=B= 
B = (u|uy|us) = 
: : : » 
Ch = | 
Oak 
=> 
=I 
— 
C=B%. 
3k 
/ 
In other words, 
1 
ne-(43-4)-(14 2) =m 
=! 
1 
1 
The fact that [Z|sg and [J]gs are inverses of each other is not an accident. 
Theorem 4.5.5 on page 513 shows that this holds in general for any pair of bases. 
Action of a Linear Function as Matrix Multiplication 
At the heart of linear algebra is the realization that the theory of linear functions 
defined on finite-dimensional vector spaces is essentially the same as the theory 
of matrices. This is due to the fundamental fact that the "action" of a linear 
function f € L(U,V) on a vector 
u € YU is the same as the matrix-vector 
product of the matrix coordinates of f with the vector coordinates of u. 
4.5.3. Theorem. If f €¢ £(U,V), where U and Y are finite-dimensional 
vector spaces over F with respective bases B and B', then for each 
u € U, the vector of coordinates of the image f(u) is given by the 
matrix product [f(u)|g = [f]ee[uls. 
Proof. 
Let B= {ui,uz,...,u,} and B' = {vi,vo,..., vm}. If 
u= pass €;U;, 
and if f(u;) = )7j2, jVvi, then 
: 
ain 
O43 
==" 
Gig 
2 
21 
Q29° 
=" 
Aan 
[ule = 
. 
and 
[f] ep =s 
; 
En 
Am1 
Am2 
es 
Amn 
so, according to (4.5.5) on page 507, 
f(u) = S aig€ivi = 'ye (Sous) 
ig 
i=1 
\j=1 

4.5 Coordinates 
511 
In other words, the coordinates of f(u) with respect to B' are the terms 
yoya1 Oj€; for 
i=1,2,...,m, and therefore 
2 
ies 
O11 
O12 
<*: 
Qin 
3 
O25 65 
Q21 
22 
-*+ 
Qan 
2 
[f(u)]e: = 
é : 
= 
i 
== [flee (ule. 
| 
Dee Cages 
Qm1 
Am2 
°*** 
Amn 
bn 
Example 
To illustrate Theorem 4.5.3, consider the action of the operator D(p(t)) = dp/dt 
on the space P3 of polynomials of degree three or less. To see that the action of 
D on P € P3 is realized by matrix-vector multiplication, compute the coordi- 
nate matrix of D with respect to the basis B = {1,t,t?,t?} to be 
0 
iDigeltn 
(4.5.8) 
0) 
owoo 
If 
p= p(t) 
=ao + ait + agt? +a3t®, then D(p) = a; + 2agt + 3a3t? so that 
ao 
Q1 
2 
(Rig ile a) emand a) (DGp\ eee | ae! 
ag 
0 
The action of D on p is realized by means of matrix multiplication because 
Be 
OL 
0 0 \e/ ao 
2 
OO 
2 
a 
[D(p)]s a 
ae Pa 
@ 
G@ 
2 
as 
a [Dl [Pe 
0 
Oo 
© 
© 
© 
a3 
Composition of Linear Functions 
Let U,V,W be three finite-dimensional vector spaces over F with respective 
bases B, B', and B". For two linear functions f :U— V and g: ¥ > W, the 
composition of g with f is the function c:U— W such that c(u) = g(f(u)) 
for all u € U. This composition is denoted by c = gf, and it is also a linear 
function because for all x,y €U, and for all scalars a € F, 
c(ax +y) =9(f(ax+y)) =g(af(x) + f(y)) 
= ag(f(x)) + 9(f(y)) = ae(x) + cy). 
Since c € L(U,W), it has a coordinate matrix representation 
[c]gg", and, as 
the following theorem shows, the coordinates of c are related to the coordinates 
of g and f just as Cayley designed them to be—namely [clas = |g] 8'8"|f)se- 
Recall the discussion on page 55 that explains how Cayley discovered and devel- 
oped matrix algebra in the context of composing linear functions. 
(4.5.9) 

Proof of (4.5.10): For all 
ue U, use Theorem 4.5.3 twice to obtain 
[eles [ule = [c(u)] 5, = [9(f(4))] 5. = [glare [f(¥)] g, = [glee [flee (ule. 
This holds for all 
u€U, 
so [e]ge" = [g|BB"|f] BB (Exercise 1.7.10, page 65). 
Proof of (4.5.11) and (4.5.12): Make use of linearity in conjunction with Theorem 
4.5.3 in a manner similar to that above—the details are Exercise 4.5.16. 
Example 
For the linear functions f : R® > R? and g: R? > R? defined by 
F(a, Y, z) a (x tan 
z) 
and 
g(u, v) = (2u meus 
u), 
the composition c: R® — R? is the linear function 
cx, y, z) aa g(f (2, y, z)) = g(x Tie 
z) re (2x Ye 
2S 
y). 
The respective coordinate matrices of c, g, and f with respect to the standard 
bases for R® and R? are 
dss. =(7 1 0)> Ile =(7 79), and 
[las =(3 1 _9). 
This corroborates (4.5.10) because [gf]s,s, = [cls3s. = (9]s2[f]ssso- 
Coordinates of the Identity Operator 
The example on page 509 shows that the coordinate matrix of the identity oper- 
ator is not necessarily the identity matrix when more than one basis is involved. 
Also, the coordinate matrices of J in that example became inverted when the 
order of the bases was reversed. The next theorem establishes that this is true 
in general. 
j
q 
;

Proof. 
If only one basis is involved, say B, then the k'" column of [J]g is 
[I(ux)]e = [ue]e = ex, so [Z]e = Inxn. When two different bases B and B' 
are involved, consider the composition J(u) = I(I(u)) of I with itself. Put 
subscripts on I to distinguish the order of composition I(u) = (J2(Ii(u)), and 
use the two bases for U as indicated in the diagram below. 
Ih:u—-u 
In:U—-U 
(loli) :u —Uu 
| 
| 
| 
| 
so that 
| 
| 
B 
B' 
jee 
Nes 
B 
B 
Applying (4.5.10) in Theorem 4.5.4 yields 
Inxn = Ue = [ej = [eles (lee => [hee = [hlpvp, 
and thus [I]ge = [I]gp because 
[=I)=]. 
Ol 
Change of Basis Operator 
Given two bases B = {x1,x2,...,X,} and B' = {yi,yeo,...,yn} for a vector 
space V, the change of basis operator is the linear function defined by f(x;) = yi 
for each i. The coordinate matrices of f are as follows. 

514 
Chapter 4 
Vector Spaces 
Proof. 
Start with [f]s = [Fx] | 
vee Lf n)Ie| = [ivils | 
he | 
Iynla]_ and 
expand y; = >>, ix; so that 
Flys) = do af (%:) = ays = (file =lale 
a 
= 
[fle = [Lowle |---| LFn)le'] = [Ivile| -- | 
bynls] = Us. 
The part of (4.5.15) that involves the identity operator is produced by using 
(4.5.10) on page 512 along with (4.5.14) on page 513 to write 
Tnen = [Ivile| -~ | 
[ynler] = [Ladle | 
«+ | 
Gn)le"] 
a [flee = If] ee = Jee' (fl = 
(fle = ges = [I] 5B. 
B 
Changing Vector Coordinates 
Changing bases from B = {x1,X2,...,Xn} to B' = {y1,y2,---,¥n} for a space 
Y naturally changes the coordinates for each v € Y. The relationship between 
[v]e and [v]g is now clear because [v]g = [I(v)]e = [Z]se'|v]p = Clv]z. 
This is formally stated below. 
4.5.7. Corollary. If B = {x1,x2,...,Xn} and B' = {y1, y2,...,yn} are 
bases for a vector space VY, and if f(x;) = y; is the change of basis 
operator in Theorem 4.5.6, then 
[v| py = Clvls, 
where 
C= (Z| Bp = (fl = [f\p°- 
(4.5.16) 
Example 
Consider the following two bases for R°. 
safue()-me()-==() 
S= {¥: = (0). yas (1), oe (0) ester Aart teen) 
By Theorem 4.5.6, the coordinate matrix of change of basis operator f(x;) = y; 
from B to S is 
ise = [las = (1 2 2) = (-1 2-1) =fle= 
lls. 
1S 2iee3 
a 
rr 
and the coordinate change matrix from S to B is [I|ps = (: 
vie=( 2), then 
(vs =[Tlssivle= 
(—1 2 -1)( 2)=(-2), 
Notice that this agrees with the results in (4.5.1)-(4.5.2) on page 506. 

aaa 
~ 
4 
Linearity is not implied in the above definitions, but when f € L(U,V) for finite 
dimensional vector spaces U/,V, the following properties are realized. 
Proo; 
07 (4.0.11) NG) =O=—— f(xy) —OV xy —— xy = 0 — x=y,. 
Proof of (4.5.18): Suppose that dim =n and dimV =m. Since R(f) C Y, 
the rank-plus-nullity theorem on page 429 along with (4.5.17) ensures that 
m= dim VY > dim R(f) =n -— dim N(f) =n. 
Proof of (4.5.19): If f is a surjection, then R(f) = V, and the rank plus nullity 
theorem yields dim Y = dim R(f) =n —dim N(f) 
<n=dimUu. 
Proof of (4.5.20): Suppose that dim =n = dim V. If f is an injection, then 
N(f) = 0 by (4.5.17), so that dim R(f) = n — dim N(f) = n = dimU. This 
together with R(f) C V ensures that R(f) = V (by Theorem 4.1.9, page 420), 
and thus f is a surjection. If f is a surjection, then R(f) = V, and hence 
dim N(f) =n—dim R(f) =0, so f is an injection by (4.5.17). 
1 
Invertible Functions 
Invertibility for functions defined on vector spaces is similar (but not identical) 
to invertibility for matrices. 

Proof. 
Uniqueness of f~1 follows from the same argument given for matrix 
inverses in (1.8.6) on page 73. To prove (4.5.22), suppose that f is both one- 
to-one and onto. Being onto means that for each v € VY there is a u € U 
such that f(u) = v, and being one-to-one implies that u is unique because 
fa) =v=f(u) 
= 
wt=u. If 
9: Y > U is the function defined by 
g(v) = u, then gf = Iy and fg = ky, so, by uniqueness of the inverse, 
g = f-!. Conversely, if f~! exists, then f is one-to-one because 
I= for =F (Rat 
UO) Se xy, 
and f is onto because for each 
v € V, u= f—!(v) €U 
is a vector such that 
flu)=v. 
Of 
The definition of invertibility as well as (4.5.22) does not involve linearity, 
so it is of interest to consider what happens when linearity is added to the 
picture. The first realization is that the inverse of a linear invertible function is 
again linear, and the second is that the statement in (4.5.22) becomes relaxed 
in the sense that injectivity and surjectivity are not both needed to characterize 
invertibility—either one by itself does the job. These facts are established below. 
, is linear. 2a 
ent o saying that 

4.5 Coordinates 
Example 
517 
Proof of (4.5.23): If f € LU, V) is invertible, then it is a bijection by (4.5.22), so 
for each v1, v2 € V there exists uj,ug € U such that f(u;) = v;. Consequently 
avi +v2 =af(ui) + f(u2) = f(aui + u2) 
= f- (avi + v2) = f-'(f(au + ue) 
= au + Ug = af~*(v1) + fl (va). 
If f ¢ L,Y) is invertible, then statements (4.5.24)—(4.5.26) follow because 
f is a bijection by (4.5.22), and it was established that (4.5.24) <=> (4.5.25) in 
Theorem 4.5.9 on page 515. 
Proof of (4.5.25) <=> (4.5.26): f is surjective if and only if R(f) = V, so, by the 
rank plus nullity theorem on page 429, this is equivalent to N(f) = 0, which in 
turn is equivalent to saying that f is injective by Theorem 4.5.9. Consequently, 
any one of the conditions (4.5.24)—(4.5.26) ensures that f is bijective and 
therefore invertible by (4.5.22). 
Ml 
Linear operators on finite dimensional spaces occur with regularity, and 
when they do the following corollary can be important. 
4.5.12. Corollary. For a linear operator f on a finite dimensional space 
U with a basis B, f is invertible if and only if [f]g is nonsingular, in 
which case [f],° = [f~ 'Jn. 
Proof. 
If f is invertible, then 
Inxn = Ue =(ff'le =[flelf 
'ls = 
([fls' =[f7'ls. 
Conversely, if [f]g is nonsingular, then f is invertible, for otherwise N(f) = 0 
so that f(x) =0 for some x #0. Consequently, [x]g #0, and 
0=(f)|e=(flekxls 
— 
N([f]s) #0 => 
(fl is singular, 
which is a contradiction. Thus [f]g must be nonsingular. 
M 
To determine the inverse of the linear operator on R® defined by 
f(z,y,z) =(az@t+yt+z, c+ 2y4 
2z, c+ 2y+4 3z), 
use Corollary 4.5.12 to observe that 
fi, = 
(22 2) = Ns =lflsh=(-1 2-1). 
i 
2 
@ 

518 
Chapter 4 
Vector Spaces 
Theorem 4.5.3 on page 510 ensures that for each u = (z,y, z) 
ER', 
f(s, = UE lealulss = («1 z -1) (7) . (-«# a2). 
0 
-l 
2 
Zz 
—y+z 
This defines the action of f~! on u, so it follows that 
a eey 2) ae (22 — y, i 
yl ey —y +2). 
Isomorphic Spaces 
Theorems 4.5.3 and 4.5.4 on pages 510 and 512 produce the realization that 
except for notation, studying linear functions on finite-dimensional spaces is in 
essence the same as studying matrix algebra. This observation is formalized via 
the concept of isomorphic spaces as defined below. 
4.5.13. Definition. Two vector spaces U,V over F are said to be tso- 
_morphic whenever there exists a linear bijection (see page 515), here- 
after called an isomorphism from one onto the other. The correspon- 
dence between u € U and v € VY produced by an isomorphism is often 
denoted by writing u 7 v. 
Isomorphic spaces are structurally identical. Even though the appearance 
of the vectors in U and VY may differ and the respective addition and scalar 
multiplication in U and VY may not look alike, the spaces are essentially the 
same—only the notation is different. 
Example 
R™*"" and R"™" are isomorphic because the correspondence 
A= [ai5] E 
(411 +++ G1nQ21 +++ Aan + ea 
= tee Gril 
Omen) = 7. 
that associates the entries in A = [a,;] with a single long row vector a is an 
isomorphism. The mapping A ++ a is clearly one-to-one, and it is linear because 
(aA +B)H aa+b. 
e 
Thespace P, of real polynomials of degree two or less and R® are isomorphic 
because the correspondence p(x) <= [p]s between p(x) = ap 
+ayrz +a9z? 
ao 
and its coordinates [p]s = (®: with respect to S = {1,z,x7} (the stan- 
a2 
dard basis for Pz) is an isomorphism. It should be clear that the map- 
ping p ++ [p|s is one-to-one and is linear because if p = >, a2" and 
q =>", 8:2'*, then 
(Pp +4) =) (vai + Bai + yp + als = pls + [als = 7[Pls + [als. 
1 

4.5 Coordinates 
519 
The fact that addition and scalar multiplication in Pz and R?® are notation- 
ally different is of little evenness is a superficial distinction. Except 
for symbolism, Pz and R® can be regarded as being the same vector space. 
The same holds for P,, and R"*!, 
All n-dimensional Spaces are Isomorphic 
The observation in the preceding example is the model for the realization that 
all n-dimensional vector spaces over F are essentially the same as F". 
45.14, 'Theorem, Let u co 
an ) be finit e dime 
onal sp: 
spaces over FF 
olf dim(/) = 
= Dy 
'then Ue and. Fr" are EOmornue 
ae 5. 21) 
: 
U and ¥ are isomorphic if 
and only 
'if ee dim). (4 5. 
28) 
Proof of (4.5.27): For a basis 
B = {uj,...,Un} of U, the correspondence 
uz [u]g is an isomorphism because the mapping u +> [ul], is one-to-one 
(if [uljg = c = [ulg, then u = cyu, + coug + ---+c¢,U, = U), and linearity 
follows from (au+v) +> [au + v]g = [aulg + [v]e = [ule + [v]z. 
Proof of (4.5.28): If U and VY are isomorphic, then it is a direct consequence 
of Theorem 4.5.9 on page 515 that dim(U/) = dim(V). Conversely, suppose that 
dim(U) = dim(V) = n. Then Y& and VY are each isomorphic with F" (by 
(4.5.27)), and hence U and V are isomorphic because if f; : UY > F" and 
fo: F" > Y are each isomorphisms, then the composition fof; :U— V is also 
an isomorphism (see Exercise 4.5.20). 
& 
The results of Theorem 4.5.14 coupled with dim L(Y, V) = (dimU) (dim V) 
(see (4.5.3) on page 507) produces the following corollary. 
4.5.15. Corollary. 
The space of linear functions £(U,V) 
for which 
dim(U/) =m and dim(V) =n 
is isomorphic to F'"*". 
e 
Inparticular, the linear operators £(U) and F"*" are isomorphic. 
Dual Spaces 
Linear functions that map vectors to scalars are common occurrences, so they 
are given a special name. 
4.5.16. Definition. A function f € £(V,F) is called a linear functional 
on Y, and Y* = L(V,F) is called the dual (or dual space) of V. In 
other words, a linear functional is a linear function that maps vectors to 
scalars, and Y* is the space of all linear functionals defined on Y. 

520 
Chapter 4 
Vector Spaces 
yak: 
For example, if c = ( 
, 
is a constant vector in F" then the function 
Cc 
defined by the standard inner product 
f(v) =c'v= > av 
i 
is a linear functional on F". In fact, it turns out that all linear functionals 
on a finite dimensional space Y can be expressed as an inner product in a 
similar manner—see (4.5.31) and Theorem 5.1.8 below. Other examples of linear 
functionals include the trace function f(An xn) = trace(A), which is a linear 
functional on F"*", and f(g) = if g(t) dt, which is a linear functional on the 
space of integrable functions. 
The next theorem shows that a space V and its dual V* are isomorphic. 
4.5.17. Theorem. If * is the dual of an n-dimensional space V, then 
_ each of the following is true. 
e 
dimY* =dimyY. 
(4.5.29) 
e 
Y* is isomorphic to V. 
e 
IfB= {uj,uy,...,u,} isa basis for V, then the dual basis for V* 
is given by B* = {f1, fo,..-, fn} in which f; is the linear functional 
that maps v € V toits i" coordinate relative to B. In other words, 
v= >. Gu; <= flv) = hi. 
(4.5.30) 
Proof. 
To see that dimV* = n = dimY, use Theorem 4.5.1 on page 507 to 
conclude that 
dim V* = dim L(V, 
F) = dim(V) dim(F) = (n)(1) = dim. 
Since their dimensions are equal, Theorem 4.5.14 on page 519 ensures that V 
and Y* are isomorphic. To observe that B* is a basis for Y*, note that B* is 
a linearly independent set containing n vectors (i.e., B* is a maximal indepen- 
dent subset of Y* —recall (4.1.4) on page 417). Independence follows because if 
>=; Fifi =0 (the zero functional), then 
0 =0(v) = (= 
at) 
(v) = De Bifi(v) => Bie 
for all 
ve Vv. 
(al 
henteee 
For 
v= uj, fi(v) = fi(uj) = ie ee ee and thus 0 = >°; Bifi(uj) = B; 
for each 7. 
W 

4.5 Coordinates 
521 
Linear Functionals and Inner Products 
We are now in a position to see that linear functionals and standard inner prod- 
ucts are virtually indistinguishable in the sense that every linear functional on a 
finite-dimensional space boils down to the standard inner product on F". 
al on: an eens space 
YY, then there are vectors ab oe such that f(v) = a™b, where 
a=(fla. and b= [v]z. Moreover, a 
a= [fle 
_is the unique vector for 
— Ly is true for all ve ve 
ee 
s 45. 18. Theorem, If 
iy i ane ne ic al 
Proof, 
alse" Bz pela ..,Un} and B* = {fi, fo,..-,; fn} be the respective 
bases for V and Y* as describes in Theorem 4.5.17. Expand f in terms of B* as 
ay 
f =o, %f; and expand v = >), &u; in terms of B so that [f]a.= | : | =a 
an 
1 
and [v]js= | : | =b. These along with (4.5.30) imply that for every v € V, 
En 
v)= (Saif) (v) = rai fi(v) = dias =alb. 
(4.5.31) 
Moreover, a = [f]g+ is the unique vector that makes this true for all v because 
if a? [v]g =a! [v]g for all v, then taking 
v =u, for 
i=1,2,... yields 
0=(a—aje, 
V7 => 
a—a=0 
== 
a—a 
a 
This theorem only involves the standard inner product for F" and does not 
depend on 
VY having its own inner product. But if there is an inner product 
defined on Y (i.e., if V is an inner-product space), then it will later be shown 
in Theorem 5.1.8 on page 654 how the statement above extends to general inner 
products. 
Double Dual Space 
It was observed in Theorem 4.5.17 that if Y is an n-dimensional vector space 
over F, then so is the dual space Y*. This means that V* has its own dual 
v**, called the double dual, and it follows from (4.5.29) that 
dim) =dimy 
= dim) =n: 
Consequently, Theorem 4.5.14 on page 519 again applies to ensure that V and 
y** are isomorphic. This time there is a "natural" isomorphism between YV and 
y** that is characterized by the correspondence v <= f% in which fy € V*™* is 
the linear functional on V* defined by f%(f) = f(v) foreach f € V*. Linearity 
of f% is clear because 
filaf +9) = (af 
+ 9)(v) =of(v) + 9(v) = af (f) + AQ). 
The next theorem shows that the mapping v+> f% is indeed an isomorphism. 

522 
Chapter 4 
: 
Vector Spaces 
Proof, 
Linearity of the mapping v > f% follows because for v1,v2 € V and 
@éF, (avitve2) 
flovitv,)» and 
flovr4va) 
(Ff) = f(av1 + V2) = af (vi) + f(v2) = oe 
OB no 
cas Wd Wh a Di eo 
=, hie kay a afi, + i Se 
Thus (av; + v2) > aff, + f%,. Furthermore, the nullspace of this mapping is 
zero. Otherwise there exists 
0 # v € V such that v +> 2%, where zy is the 
zero vector in Y**, which means that 2*(f) =0 
for all f € Y*, or equivalently, 
f(v) =0 
for all f € V*. But this is a contradiction because if the i*" coordinate 
€; of v is nonzero, then f;(v) 
#40 for the functional f; in (4.5.30). Thus the 
mapping v+>+ f%* is an isomorphism by Theorem 4.5.11. 
The fact that VY and Y* are isomorphic stems from the fact that their 
dimensions are equal, but the bijection between the two of them is not so "nat- 
ural." However, there is a more natural correspondence between VY and )** 
given by 
v= f%. This natural mapping further makes V and Y** virtually 
indistinguishable, so the theory and language are relaxed a bit by considering 
y** = Y. This property is generally referred to as reflexivity. 
Historical (and Editorial) Comments 
The results of Theorem 4.5.14 and Corollary 4.5.15 on page 519 are remark- 
able because they mean that all finite dimensional vector spaces can be studied, 
developed, and completely understood by means of n-tuples and matrices. For 
example, except for notation, every n-dimensional space over F is the same as 
F", and L(U,V) is no different than F"*". In retrospect this makes sense in 
light of the discussion on page 55 that explains how Cayley "hard wired" these 
features into his original matrix theory. As powerful as these observation are, they 
were historically discounted by academicians on the grounds that (1) abstracting 
linear algebra to become independent of n-tuples, matrices, and coordinates bet- 
ter prepares students for studying linear functions on infinite dimensional vector 
spaces, and (2) the coordinate-dependent aspects of the subject are secondary 
to the fundamental concepts of linear algebra. 
These views peaked in the 1960's. Cayley's matrix theory had become aca- 
demically passé, and linear algebra in university curricula was relegated to the 
position of a steppingstone for crossing into infinite dimensional functional anal- 
ysis. And since these aims were consistent with the tenet that the important 
properties of linear functions are those that are basis independent, it seemed 

4.5 Coordinates 
523 
natural to the mindset of the 1960's academic to sterilize the presentation of 
linear algebra by removing as many references to matrices, coordinates, and de- 
terminants as possible. 
But during pasteurization, a colossal technological transformation occurred. 
The digital computer emerged to change everything—science, engineering, biol- 
ogy, medicine, business, finance, ..., and on and on. New scientific constructs 
arose from components that hinged on the understanding and development of 
sophisticated computer modeling techniques and digital algorithms. No serious 
scientist or engineer could afford to be ignorant about these new structures. And 
what do you suppose was at the base of these structures? Linear algebra!! But not 
the sterilized 1960's version of the subject. The cornerstones were matrix factor- 
izations, matrix algorithms, matrix computations, and, more generally, matrix 
analysis of all flavors that were the antithesis of the purist's paradigm. 
Regardless of the formulation, continuous or otherwise, solutions to real- 
world problems depend on digital analytics, algorithms, and discretizations that 
revolve around the ability to analyze and solve large systems of algebraic equa- 
tions, to compute and manipulate eigenvalues and eigenvectors of large matri- 
ces, and to apply discrete transform procedures for compressing, filtering, and 
recognizing patterns in large sets of discrete or digitized data. These along with 
many other computational and analytical objectives have one thing in common— 
the need to factor and manipulate matrices in useful ways, which in turn often 
amounts to expressing data and concepts in clever coordinate systems. These are 
the topics that are now at the core of modern instruction. 
Teaching and learning linear algebra from a viewpoint that minimizes ma- 
trices, coordinates, and determinants is today as silly as advocating nude bicycle 
riding on the premise that the rider's practical clothing obscures the machine's 
natural beauty. The objective should be to appreciate the inherent elegance of 
linear algebra while at the same time developing a command of the immense 
utility of the subject. 
Exercises for section 4.5 
4.5.1. Which of the following functions are linear operators on R?? 
(a) 
f(x,y) a (ay ley) 
(b) 
f(x,y) = (y, 2), 
(c) 
f(x,y) = (0, ry), 
(d) 
f(z, y) a (en Ue) 
(e) 
f(x,y) = (z,siny), 
(f) 
f(a,y) = (2+ ¥, 2 =). 

524 
Chapter 4 
- 
Vector Spaces 
4.5.2. 
4.5.3. 
4.5.4. 
4.5.5. 
4.5.6. 
4.5.7. 
4.5.8. 
4.5.9. 
Let B bea basis for an n-dimensional space U over F. 
(a) Prove that [ax + y]s = [x] + [ys for all x,y €U and for 
all 
ac F. 
(b) Is it true that 
x =O in U if and only [x]g =0 in F"? 
(c) Prove that {y1,y2,---,¥r} is a linearly independent subset of 
U if and only if {[yi]s, [y2]s,---,[¥r]e} is a linearly indepen- 
dent subset of F" . 
For a linear operator f on an n-dimensional space U, explain why 
dim R(f) = dim R([f]g) and dim N(f) = dim N((f]g) for every basis 
B for U. 
For the operator f : R? + R® defined by f(x,y) =(x+y, —2¢ + 4y), 
determine [f]g, where B is the basis B = fur em ; sis 
; \. 
Let B = {x,Xe,..-,Xn} and B' = {y1,yo,.--,¥n} be bases for an 
n-dimensional subspace VY C F™™?. Prove that if Kmxn and Ymxn 
are the respective matrices whose columns are the vectors in B and B', 
then the matrix P such that P[v]g =[v]g is 
P = Y'X. 
Let f :R?— R?® be the linear transformation defined by 
f(x,y) = (z+ 3y, 0, 2x — 4y). 
(a) Determine |[f|ss, where S and S' are the standard bases for 
R? and R°, respectively. 
(b) Determine [f]ss, where S" is the basis for R®? obtained by 
permuting the standard basis according to S" = {e3, €2, e; }. 
Let f : R® > R?® be defined by f(z,y,z) =(x@-y, y—2, x—z) and 
1 
1 ( 
0 
1 
consider 
v = (:) and the basis B-{(0), (*). (2) }. 
2 
1 
1 
0 
(a) Determine [f]g and [v]p,. 
(b) Compute [f(v)]s, and then verify that [f]s[v]e = [f(v)]|z. 
For a given A € R"*", let f be the linear operator on R"*! defined 
by f(x) = Ax. With respect to the standard basis S, prove that 
Lf 
hse 
For a linear operator f onaspace U witha basis B, define f* to be the 
k-fold composition f*(x) = f(f(---(f(x)))). Explain why hid eal 
for all nonnegative integers k. 

4.5 Coordinates 
525 
4.5.10. Let ao be the projector (the projection operator) that maps each point 
4.5.11. 
4.5.12. 
4.5.13. 
v € R' to its orthogonal projection p(v) on the line y = x as indicated 
below. 
Determine [p]s,, and then use this to compute p(v) for v = (e). 
The standard basis for R?*? is 
S={Ui= (9 0) Ue=(o 6), Us=(7 6), Ua=(9 t)}- 
Compute [f]s for each of the following linear operators on R?*?, and 
then verify that [f(U)]; = 
[fls[U]s for U= ( a 
X+xX? 
(8) f(Kax2)==t* 
Pe Xe tA 
KX A tor A fed a: 
For a composition c = gf in which f € L(U,V) and g € L(V,W), 
extend (4.5.10) in Theorem 4.5.4 by using two different bases Bi and 
By for Y as indicated in the following diagram 
f:u—Vv 
g:¥V—W 
(gf):u—-W 
| 
| 
| 
| = 
| 
| 
B, 
Bi 
By 
Bi 
By 
Bs 
to show that [c]p, 
5, = [ofle.e, = [gls.8% [fle.B:- 
For P2 and P3 (the spaces of polynomials of degrees less than or 
equal to two and three, oe let s : Po — P3 be the linear 
transformation defined by s(p ={- ol x)dx. Determine 
[{s]gg, where 
B= {1,t)t?} and B' ={1,t, " oh 

526 
Chapter 4 
; 
Vector Spaces 
4.5.14. 
4.5.15. 
4.5.16. 
4.5.17. 
4.5.18. 
4.5.19. 
Let U be an n-dimensional-space with a basis B = {ui tii 
(a) Prove that a set of vectors {x1,X2,..-,Xr} © U is linearly 
independent if and only if the set of coordinate vectors 
{pals, bral, ---» bers} ©R™ 
is a linearly independent set. 
(b) Let f be a linear operator on U, and suppose that the basic 
columns in [f], occur in positions 61, b2,...,6,. Explain why 
{ f(us,), f(ue,), +» 
f(us,)} is a basis for R(f). 
Let q be the linear operator on R? that rotates each point counter- 
clockwise through an angle 6, and let r be the linear operator on R? 
that reflects each point about the z -axis. 
(a) Determine the coordinate matrix of the composition [rq|s rela- 
tive to the standard basis S = Sy for R®. Hint: Recall (1.10.9) 
on page 94. 
(b) Determine the coordinate matrix with respect to S of the lin- 
ear operator that rotates each point in R? counterclockwise 
through an angle 20. 
Let U,V be finite-dimensional spaces over F with respective bases B 
and B', and let f,g € L(U,V). Provide the details for (4.5.11) and 
(4.5.12) in Theorem 4.5.4 on page 512 by proving that [af]ge = alf]se' 
for all 
a€ F and [f+ g9]se = [f|ee + [glee'. 
Let A € F"*" be a given nonsingular matrix, and let I be the identity 
operator on F". Explain how to construct bases B and B' for F" such 
that [I]pn = A. 
Explain why the linear operator f : R? > R® defined by 
F(Z 92) = (2a, 
—%-+ 2y — 2, Z=—y) 
is invertible, and then describe f~!. 
Prove that if f :U— V is an isomorphism between two finite dimen- 
sional spaces U and VY over F, and if B= {uj,...,Up} is a basis for 
U, then f(B) = {f(u1),...,f(up)} is a basis for V. 

4.5 Coordinates 
4.5.20. 
4.5.21. 
4.5.22. 
4.5.23. 
527 
Explain why the composition of two isomorphisms is another isomor- 
phism. That is, show that if /,V, and W are finite dimensional spaces 
over the same field, and if f; 
:U—> V and fg: V— W are each 
isomorphisms, then the composition fof; :U4— W is an isomorphism. 
For a given constant vector c € R", determine the coordinate matrix 
[fls,s, With respect to the standard bases for the linear functional on 
R" defined by f(x) =c?x. 
2 
=i 
0 
; 
For the basis B = {x= (-1). x= ( 
2) 
x = 
-1)} of R', 
0 
= 
1 
3 
let f be a linear functional such that f(x1) = 1, f(xe) = 2, and 
f(x3) =3. Determine f(v) for v = (=). 
U3 
2 
1 
0 
3 
For the basis B = {x= (-1), 22 = ( 
2), = (-1)} of R°, 
determine the dual basis b*. 

528 
Chapter 4 
; 
Vector Spaces 
4.6 CHANGE OF BASES 
It was seen in the previous section that a linear function f and its coordinate 
matrix [f] can have properties in common (e.g., consider invertibility) so this 
suggests that it might be feasible to examine properties of [f] in order to infer 
properties of f. But this is fraught with danger because properties of [f]5,' 
might vary with the choice of B and B'. 
To be able to use [f] to reveal properties intrinsic to f it is necessary to 
identify properties of coordinate matrices that are invariant among all possible 
bases, and this in turn requires a clear understanding of the effect of a basis 
change on coordinate matrices. Changing matrix coordinates is somewhat more 
involved than changing vector coordinates as described on pages 506 and 514, 
so the purpose of this section is to explore these issues. Then this knowledge is 
used to sort out basis-independent properties. 
The Fundamental Result 
Throughout this discussion, U/ and Y are finite dimensional spaces over F, and 
f € L(U,V). For a pair of bases B, and By for U and a pair of bases Bi and 
Bi for V, the following theorem relates [f]s,5, to [f]s,8,- 
4.6.1. Theorem. If f € L(U,V), where (B,,B2) and (Bj, B64) are re- 
spective pairs of bases for U4 and VY, then 
[f le = ee: [flee Zu), Bo» 
where Jy, and fy are the respective identity operators on U and Y. 
Proof. 
Consider the composition (IyfIy) :U — V with bases for U and V 
as indicated below. 
Iy:V¥V—YV 
f:u—-yvV 
Iy:u—u 
| 
| 
| 
| 
| 
| 
Bi 
Bi 
Bo 
Bs 
B, 
Bo 
Successively apply (4.5.10) in Theorem 4.5.4 on page 512 to write 
flee, = Lv flule.e, = Uv)e,8:[fle.8;Uuls,5,. 
w 
SVD and URV via Change of Bases 
The power of Theorem 4.6.1 is realized by considering the singular value decom- 
position and the URV factorization respectively developed in Theorems 3.5.2 
(page 359) and 4.3.6 (page 461). Each derivation was via matrix theory, but in 
fact each decomposition is just a straightforward consequence of Theorem 4.6.1. 

4.6 Change of Bases 
529 
To see that an SVD of A € F"*" is simply the coordinate matrix of A with 
respect to a special pair of bases, consider A as a linear function A: F" > F" 
using ordinary matrix-vector multiplication along with respective pairs of bases 
(B,,B2) and (Bi, BS) for F" and F" where 
By =Sn, 
Bo AVIV 
9; 25-5 ¥n} 4 
Bi = Sm, 
By = {u1,2,...,Um}s 
in which S, and S,, are the standard bases for F" and F™ and By and Bj 
are the singular vector bases defined in (3.5.6) on page 361. Theorems 4.6.1 and 
4.5.5 (page 513) combine to yield 
A =[A]s,5, = [m]5% 
5m (Ale. Unls.82 = lm]s;s,,[Ale.6;Unlgs, 
= U[A]z,6, 
V+ = U[A]s,8,V*, 
in which U = [Lm] 8% Sin = lp \--* || and Ve = li, \s.s. = (Vi lem Valea 
unitary matrices containing the singular vectors. The SVD is produced by using 
the relations in (3.5.6) on page 361 to evaluate 
[A] 8.6%, = [AWle, 
; | 
[A(v,)] 5% | 
[A(vr+1)] 2; 
vos ACn)le| 
= [ore | 
re | 
ope, | 0| see | 
0] = 'eG 0) where. 
Te Gis 
A URV factorization for A € F"*" is produced in exactly the same way, 
except that By = {v1,v2,..-,Vn} and By = {u1, U2,...,Um} are the respective 
orthonormal bases defined in (4.3.12) and (4.3.11) on page 460. 
Linear Operators and Matrices 
Specializing Theorem 4.6.1 to linear operators yields the next two corollaries. 
4.6.2. Corollary. 
Let f :U—U 
bea 
linear operator on a finite di- 
mensional space U, and let J be the identity operator on U. Setting 
B, = Bi = B and By = B, = B' in Theorem 4.6.1 and using (4.5.14) 
on page 513 produces 
[fle = Ueelfle Wee = Lee [fle Ul ee'- 
(4.6.1) 
Mother-in-Law Rule: 
It is generally easy to remember that the change-of- 
basis formula [f]g = [J]'e[f|e(Z]ee requires left-hand and right-hand multi- 
plication by coordinate matrices of J, but the correct order of B and B' is 
important and not always easy to remember. My own mnemonic device is the 

is 
oa 
1b 
y 
' 
. 
Ff a 
nln 
Gost joe Sheree ts "- 
et 
y 
—— 
en eo ee 
For example, if S is the standard basis for R?, then the coordinate matrix 
Oma 
: 
* 
1 
1 
> 
OL A= led: a 
with respect to B-{(1), (3)} is 
[A]s = (Z]se[A]s[Z]es = gsAllles 
=(4 T) (2 3) 2)=G 2): 
Notice that Ay = 1 and Az = 2 are the two eigenvalues of A, and the two 
vectors in B are the corresponding eigenvectors. As shown below, this happens 
in general. 
Diagonalization by Similarity 
Theorem 3.2.5 on page 305 states that a square matrix is diagonalizable by a 
similarity transformation if and only if A has a complete set of eigenvectors. 
Corollary 4.6.3 provides a sharper interpretation of this diagonalization result 
by exposing what it really means. 
: 

~— Proof. 
Since APr = Apr for each k, it follows that 
[Als 7 [[A@Ds | 
A(p2))s | 
Oth A(pn)la| = [A1e1 | 
A2xe2| +++ | 
Anen] 
See 
i 
a 
eG 
Ot 
2 ae 
Sy 
hue 
Soy 
peat hy 
; 
. 
ce oe NG 
Equation (4.6.3) in Corollary 4.6.3 guarantees that P-'AP =[A]z=D. 
§& 
Given A € F"*", it is understood that A = [A]s, but Theorem 4.6.4 
makes the point that S may not be the best basis by which to represent A 
with—an eigenbasis B (provided one exists) is preferable. But in general an 
eigenbasis need not exist, so this presents a new problem—namely finding a 
basis B for F" such that even if [A]g is not diagonal, it nevertheless has a 
more desirable structure than [A]s = A exhibits. Solving this problem is at 
the heart of linear algebra, and much of the subsequent material is devoted to 
addressing this problem—a partial solution is given at the end of this section on 
page 539. 
Meaning of Similarity 
In general, two square matrices A,B € F"*" are similar whenever there exists 
a nonsingular matrix Q such that B= Q-!CQ, and Corollary 4.6.3 makes it 
clear exactly what this means. 

532 
Chapter 4 
: 
- Vector Spaces 
Proof. 
If B and C are coordinate matrices for the same linear operator f 
but with respect to bases B and B' so that [f]g = B and [f|s =C, then, by 
(4.6.1), Be (]e-8Cl] bp = Q-!CQ for Q= [I] ep°, and thus B and C are 
similar. Conversely, suppose that B and C are similar so that 
B = Q-'CQ 
for some nonsingular Q. Let f : F" — F" be the linear operator defined by 
f(x) = Cxn 1 with ordinary matrix-vector multiplication, and consider bases 
B = {q1,q2,---;dn} (the columns of Q) and B' = S (the standard basis) so 
that [f]e =[C]s =C and [I] = [I]es = Q. It follows from (4.6.1) that 
(fle = Was fle les = Q*CQ =B. 
Thus B and C are both coordinate matrices for f. 
& 
In other words, Theorem 4.6.5 means that anytime you see similar matrices, 
you should realize that the matrices are in essence the same linear operator, but 
just with respect to different bases. 
Similarity Invariants 
To be able to use the properties of a coordinate matrix [f]g to infer proper- 
ties of the underlying operator f, and vice versa, it is necessary to distinguish 
properties of [f],z that do not depend on the choice of the basis B. Theorem 
4.6.5 guarantees that any matrix similar to |[f]g is also a representation of f, 
so the basis-independent properties of f will be reflected by all matrices similar 
to [f]g. This leads to the following definition. 
4.6.6. Definition. A similarity invariant is a function s defined on 
F"*" such that s(C) = s(Q~'CQ) for all © and all nonsingular Q. 
This means that if f is a linear operator on F", then s([f]g) has the 
same value for all B, so that similarity invariants for matrices can be 
extended to linear operators by defining 
s(f) = s([f]s) by using any convenient basis B for F". 
(4.6.4) 

4.6 Change of Bases 
533 
Example (Common Similarity Invariants) 
Many of the common matrix functions that have so far been encountered are 
similar invariants. Here are a few of them. 
e 
Rank. Since multiplication by nonsingular matrices does not change rank 
(Theorem 2.4.11, page 186), rank (Q~'CQ) = rank (C), so rank is a simi- 
larity invariant. This means that dim R(x) and dim 
N (x) are also similarity 
invariants. 
e 
Trace. Apply the property trace(XYZ) = trace(YZX) = trace(ZXY) 
from Theorem 1.8.10 on page 78 to see that trace is a similarity invariant 
because 
trace (Q~'CQ) = trace (CQQ"') = trace (C). 
e 
Determinant. The product rule det (XY) = det (X)det(Y) from Theo- 
rem 9.2.10 on page 947 shows that the determinant is a similarity invariant 
because 
det (Q~'CQ) = det (Q~')det (C)det (Q) = det (Q~") 
det (Q)det (C) 
= det (Q~'Q)det (C) = det (I)det (C) = det (C). 
e 
Characteristic Polynomial. The function char(A) that maps A ¢€ F"*" 
to its characteristic polynomial p(x) = det(A — cI) is a similarity invariant 
because the determinant is—i.e., 
char(Q~!CQ) = det([Q~'CQ] — zI) = det(Q-'CQ — tQ7'Q) 
= det(Q7*[C — zI]Q) = det[C — zI] = char(C). 
e 
Eigenvalues. Eigenvalues along with their multiplicities are similarity in- 
variants because, as shown above, the characteristic polynomial for C is the 
same as the characteristic polynomial for Q~'CQ, and 
geo multg (X) = dim N(C — Al) = dim N(Q"YIC a \IjQ) 
= dim N([Q7~*CQ] — Al) = geo multig-1cqj (A): 
Example (Extension to Linear Operators) 
Similarity invariants for matrices are extended to linear operators as described in 
Definition 4.6.6 on page 532. For example consider the linear operator f on Rex 
defined by f(A) = A". Similarity invariants are basis independent, so the stan- 
dard basis B = {Eu =(4 vba IU a Ea 
ae yt 
suffices to compute a coordinate matrix for the transpose operator as 
iS 
SS 
1 
= [[B11}s | [Bails | (Ex2]s | Bale |] = | ¢ 
0 

arn of 7} 
of degree t 
Re 
ustee s™ (ine 8) was 
computed in 
(38) on age 
S11 
to be 
0 
[Dis =| 5 
0 ooor 
he 
—) 
This reveals that the derivative operator D is singular (i.e., non-invertible) 
with 
a zero trace and only one distinct eigenvalue (A = 0) with alg mult(\) = 4 and 
geo mult(\) = dim N(D) 
= 
4—rank(D) 
= 1. Also notice that the derivative 
operator 
is nilpotent of index 4 because [D]g is nilpotent, and nilpotentence 
is 
another similarity invariant since 
C* = 0 => (Q-'CQ)* 
= 0. 
Invariant Subspaces 
For a linear operator f on a vector space V, and for ¥ CY, 
f(*) = {f(x) | xe #} 
is the set of all possible images of vectors from 4 under mapping by f. Note 
that f(V) 
= R(f) is the extreme 
case. When 24 is a subspace 
of V, it is easily 
verified that f(4') is also a subspace of V, but f(4') is generally not related 
to 4. However, in some situations it can happen that f(4') C 4', and these 
kinds of subspaces are the focus of the rest of this section. 
Sie 
64 
2 
a i 
For A = (= ps 5), yt he (-1), and x2 = ( 
2), the subspace ¥ 
5 
0 
spanned by B= {xi, x2} is invariant under mapping by A because 
Ax; =2x, <2 
and 
Axy =x, +2x2 E24, 

4.6 Change of Bases 
535 
and the image A(x) of any x = ax; + 6x2 E24 
is 
A(x) = A(ax + 8x2) = aAx; + BAx2 = (20 + 8)x, + 28x, € &. 
This equation describes the action of A restricted to 4', so it can alternately 
be expressed in the notation of a restricted operator 
A) (x) = (2a+ B)x1+26xg 
foreach 
x =a0x,+$x2E%. 
This restriction operator A ye has a coordinate matrix [Ay] g that is obtained 
by using A; (x1) = 2x, and Ay (x2) = X1 + 2x2, to produce 
ele = (eo ]s | [ya@)].)=(9 3): 
Simplified Coordinate Matrices 
Invariant subspaces are important because they lead to coordinate matrices hay- 
ing simplified structures as described in the next two theorems. 
4.6.8. Theorem. 
Let 4 be an r-dimensional subspace of an 
n- 
_ dimensional space VY, and let By = {x1,X2,...,x,-} be a basis for 
x that is part of a basis B = {x1,X2,-.-,%r,¥1,Y2,---;¥n—r} for V. 
For a linear operator f on Y, subspace 4' is invariant under f if and 
only if [f]g has the block-triangular form 
[fle = Ce o in which 
A= fel. 
(4.6.5) 
Proof. 
Suppose first that 7 is invariant under f. By definition, the coordinate 
matrix of f with respect to B is 
fle = ((fGa)le|---|UF@re | @wls|-- [if Gn—ris). 
(4.6.6 
Since each f(x;) is contained in , only the first r vectors from B are needed 
to represent each f(x;), but if VY 
= span {y1,yo,-.., Yn—r} is not an invariant 
subspace for f, then all the basis vectors in B may be needed to represent the 
iM 
r 
Q 
tT 
= 
- 
a 
f(y;)'s, so f (xj) = jay age and f(y;) = Dj) Bigi + Vina 
VisYi- Hence 
15 
81; \ 
[f(x;)]B = | °? 
and = {[f(y;)|a = és 
4.6.7 
0 

536 
Chapter 4 
Vector Spaces 
Using these in (4.6.6) produces the block-triangular matrix 
Oi 
ie 
Oi 
ite 
oe 
Org 
a, 
s o Cer 
Bra 
a Br 
= 
Arxr 
B 
4.6.8 
cdraad 
hae 
eee 
epee = a 
(4.6.8) 
0 
fe hE 
The equations f(x;) = )-j_; jx: for 7 =1,2,...,7 mean that 
a 
a 
a4 
Qir 
f 
oa 
f 
ie 5 
PRE 
EZ ys 
A 
fa] 4. me 
» 
80 
fee = 
ae 
page 
= 
Arxr: 
Orj 
Ari 
Ar2 
cee) 
Arr 
Conversely, if [f]g = za mat then the coordinates of each f(x,;) are given 
by (4.6.7), which means that f(x;) = )7j_, aijx: for each j, soif x € ¥, then 
x= 7x; and f(x) =)0, , 
&iaijxj 
© ¥. Thus 7 is invariant under f. 
& 
The more invariant subspaces that can be found, the simpler [f|g becomes. 
For example, if the subspace Y spanned by By = {y1,yo2,.--,¥n—r} in The- 
orem 4.6.8 is also invariant under f, then only the vectors in By are used to 
express f(y;) = >); Vijyi in the proof of Theorem 4.6.8 so that the §'s in 
(4.6.7) are all zero, which means that B = 0 in (4.6.8), and 
= 
Arxr 
0 
"4 
[nel ey 
U. 
[fle = ( 
0 
ated = ( 
f 
sl, ) 
(4.6.9) 
This observation easily generalizes to any number of invariant subspaces to 
produce the following generalization of (4.6.9). 
4.6.9. Theorem. Let f be a linear operator on an n-dimensional space 
Y, and let ¥,Y,...,Z be subspaces of V with respective dimensions 
T1,72,---,re 
With >°,r; = n and bases 
By, By,...,Bz 
such that 
B=BxUByU---UBz 
is a basis for V. Subspaces ¥,),...,Z are 
each invariant under f if and only if [f], has the block-diagonal form 
Ar, xT 1 
0 
Arne 
0 
0 
B 
Hong 
0 
[fle = 
oe 
ae 
(4.6.10) 
0 
0 
igi 
Co. 
in which case A = Be e. 
B= fA a C= fe os 

2 
= 
The subspace ¥ = span {a 
= (a) 
cea (2) C R* is an invariant 
0 
subspace for 
tp 
te 
ee 
= 
0 57 16 
27, 
AN 
0 
3 
10 
14 
4° 
88) 
10e 
id 
because Tq; = qi + 3q2 and Tq2 = 2qi + 4q2 insure that for all a and 8, 
the images 
. 
T(aqi + Baz) = (a + 28)aqi + (3a + 48) 
a2 
lie in &. To construct a nonsingular matrix Q such that Q~!'TQ has a block- 
triangular form 
apie = 
extend B = {qi, q2} to a basis for R* by using the technique described on 
page 153 to obtain 
1 
0 
2 
-1 
10 
0) 
0 
ill 
4% 
@O 
© 
a3 =| 9 |, G4= | 9 | 8° 
that Q = (a1 | a | as | as) = 
ea 
0 
il 
0 
0) 
@ 
il 

538 
Chapter 4 
Vector Spaces 
Since the first two columns of Q span an invariant subspace for T, Corollary 
4.6.10 guarantees that Q~'TQ must be block triangular. This is corroborated 
by computing 
0 
-1 
-2 
0 
- 
-1 
Q*=|(7 230] 
and QTQ= 
(0) 
0 
(8) 1 
Note that the upper-left-hand block is ( 
4 4 = IT) 2 
{a1 G2} 
Continuing with this example, suppose that q3,q4 are replaced by the alternate 
0 
0 
extension vectors qs = 
es . 
= 
a , which also augment {qi,q2} to 
=i 
1 
generate a basis for R*. This is preferable because the spaces ¥ = span {qi, G2} 
and Y = span {q3,q4} are both invariant under T, so Corollary 4.6.10 guar- 
antees that if Q = (a: | 
q2 | qs | ai), then Q-!TQ is block diagonal. This is 
verified by computing 
Q"'TQ= ( 
RRR 
NnNNr 
wWwnr 
ase (3 1) Geb aug (7 8) S ae ee 
Eigenspaces Are Invariant 
As more invariant subspaces are found, the resulting coordinate matrices be- 
come more refined, so this opens the question, "what is a good way to find a 
lot of invariant subspaces?" The key is eigenspaces. For example if (A,x) is 
an eigenpair for T ¢ F"*", then 4 = span {x} is an invariant subspace for 
T because Tx = \x € 4, so Theorem 4.6.8 and Corollary 4.6.10 say that 
Qr1TOe (3 a where Q = (x| x |---|) and ITy..| 
= [A]ix1. More 
{x} 
generally, the entire eigenspace 1, = N(T — Al) is invariant under T, so if 
By = {x1,X2,...,Xg} 
is a basis for 4%, then Q-!TQ = fa cn where 
Q = (xi|-:-|x,| * | --- |x) and foes = [XI,]. The extension of this 
observation is formally stated below as a theorem. 

_ The diagonalization theorems on pages 305 and 308 now follow as corollaries. 
J 
3 
ING (Generalized Eigenspaces) 
| 
There are other important ways beyond eigenspaces by which to generate in- 
—— 
variant subspaces of A € F"*"". For example, if \ € o(A), and if x and 
k > 1 are a respective vector and integer such that 
x «¢ N(A— M)* but 
x@¢@ N(A—X1)* 
*, then 
T= {(A —A1)F-1x, (A - AD) *x, ..., (A-Al)x, x} 
is called a Jordan chain, and os ) (the span of 7) is an invariant subspace for 
A because if y € (7), then y = SP rayee (J), sousing (A—Al)y; = yi-1 
for 
i>1 and (A—AlT)yi = 0 yields 
Ay = (A-ADy + Ay 
= y 
OuYi—1 + = 
Aas | 
€ (7). 
i—2 
Gail 
Since Ayi =Ay1 and Ay; =Ayi+yi-1 for 
1>1, 
Ae, 
for 7 = 1, 
[Ayilz = { 
Alyélz + [yi-1]z = Aeit+ei-1 
fori > 1, 

540 
Chapter 4 
Vector Spaces 
so that 
Anal, = [Avis | 
i | 
Ayals] = 
seas 
=B. 
(46.12) 
~rR 
Matrices such as B are called Jordan blocks. If enough Jordan chains J; can be 
constructed so that U;J; is a basis for F", and if Q is the nonsingular matrix 
containing these chains, then Theorem 4.6.9 and Corollary 4.6.10 on pages 536 
and 537 guarantee that 
ia) 
Bi 
Q-*AQ = 
M54) 
= 
Bo 
| 
=F 
(46.13) 
is a block-diagonal matrix in which each Jordan block B; = EY ( oN has 
the form (4.6.12). The space (7) is often called a generalized eigenspace for 
A, and the vectors y; = (A — \I)*-Ix for j <k are referred to as generalize 
eigenvectors because y; 
€ 
N(A—AI)/. Matrices having the form of J in (4.6.13) 
are called Jordan forms—their complete theory is developed in §4.9 on page 584. 
Exercises for section 4.6 
4.6.1. Give an example of a function defined on F"*" that is not a similarity 
invariant. 
4.6.2. Are eigenvectors of similarity invariants—i.e., if A,B € F"*" are simi- 
lar, do they have the same eigenvectors? 
4.6.3. For f(z,y,z) =(a+2y—z, —y, r+7z), use the standard basis S and 
1 
1 
1 
the basis S' = (2), (2), (:)} for R° to determine [fls and 
the nonsingular matrix Q such that [f]s = Q-'[f]sQ. 
Zee 
O 
iL 
i 
1 
4.6.4. Let 
A = (: 1 ') 
and, B = {(:). (2), (2)}. Consider A as 
OS: 
1 
2 
3 
a linear operator on R"*? using standard matrix-vector multiplication 
A(x) = Ax, and determine [A],. 

4.6 Change of Bases 
4.6.5. 
4.6.6. 
4.6.7. 
4.6.8. 
4.6.9. 
4.6.10. 
4.6.11. 
541 
Show that C = & 4 and B= (FR 18) are similar matrices, by 
finding a nonsingular matrix Q such that 
C = Q-'BQ. 
Hint: Consider [B]s and [B]s,, where S is the standard basis and 
eed Ye 
Let f be the linear operator f(x,y) = (—7x — 15y, 6x +12y). Find a 
basis B for R? such that (fle = & a) and determine a matrix Q 
such that [f]s = Q7'[f]sQ, where S is the standard basis. 
Let P(x,y) = (xcos@—ysin§@, xsin6+ycos@) be the rotator discussed 
on page 94. Find a basis B so that [P]z = ee ie it 
Suppose that B is similar to D = diag (Aj, Ao,..., An). Explain why 
this means that B — \,I is singular for each 2. 
Explain why similarity is transitive in the sense that if A and B are 
similar, and if B and C are similar, then A and C are similar. 
A linear operator f on a space VY is said to be nilpotent of index k 
when) f* = 0 buts f*>! 40. 
(a) Prove that if f is a nilpotent linear operator of index n on an 
n-dimensional space VY, and if y € V issuch that f"~!(y) £0, 
then Bi-- 4" *(y), J Ey), mot 
(¥ ey) 18a pasis tor @/ 
(Oy 
al 
Oe 
and [f]g =N= 
S 
al 
0 
(b) Prove that any two nilpotent matrices A,B € 
of index 
nm are similar and that all such matrices are of rank n—1 and 
have a zero trace. 
Tee 
For \ € a(A), let x and k > 1 bea 
respective vector and integer 
such that 
x €¢ N(A—AlI)* but x ¢ N(A—AI)*; Prove that the 
associated Jordan chain 
Ge= {(A — \I)*-1x, (A — AI)P-?x, ..., (A—-ADx, x} 
is a linearly independent set. 

542 
4.6.12. 
4.6.13. 
4.6.14. 
4.6.15. 
4.6.16. 
Chapter 4 
Vector Spaces 
A linear operator f is idempotent whenever frasf. 
(a) Prove that if f is an idempotent linear operator on an n- 
dimensional space V, and if ¥ = {x;}J_, and Y = {yi} 
are respective bases for R(f) and N(f), then 
B= VUY 
isa 
basis for V. 
Hint: 
Show f(x;) = x; and use this to deduce that B is 
linearly independent. 
Show that [f]5 = (§ ay 
(c) Explain why all n x n idempotent matrices of rank r must be 
similar, and then conclude that rank (E) = trace (E) for every 
idempotent matrix. 
Note: This is an alternate approach to that used to solve Ex- 
ercise 4.2.20 on page 452. 
Let f be an arbitrary linear operator on a vector space V. 
(a) Is the trivial subspace {0} invariant under T? 
(b) Is the entire space V invariant under T? 
Describe all of the subspaces that are invariant under the identity oper- 
ator I on aspace VY. 
Let f be the linear operator on R* defined by 
F (21, 22,23, 04) = (41 +22+ 223-24, to+24, 
223-24, 23+ "2), 
and let Y = span {e1,e2} be the subspace that is spanned by the first 
two unit vectors in R*. 
(a) Explain why 4 is invariant under f. 
(b) Determine Hetero 
(c) Describe the structure of [f]g, where B is any basis obtained 
from an extension of {e;,e2}. 
Let f and Q be the matrices 
ge 
Og 
eS 
Te 
he 
Oe 
= 
(beg 
"TO? 
6 
ep 
7 
At 
oye 
y= 
9-9 
59) 
bitte 
Ae 
and 
ye | be 
OUT 
3 
—-5 
-13 
—-7 
3 
3 
(a) Explain why the columns of Q are a basis for R'*. 
(b) Verify that ¥ = span {Qs1,Q.x2} and Y = span 455, eae 
are each invariant subspaces under f. 

4.6 Change of Bases 
4.6.17. 
4.6.18. 
4.6.19. 
4.6.20. 
543 
(c) Describe the structure of Q~!'TQ without doing any compu- 
tation. 
(d) Now compute the product Q-!'TQ to determine 
7) {Qu1,Qu2} 
1h 
T/ {Qu3,Qua} 
Let f be a linear operator on a space V, and suppose that 
B= Aud ay lls; W1,---,Wg} 
is a basis for V such that [f]g has the block-diagonal form 
Explain why U = span {uj,...,u,} and W = span {wi,...,wq} must 
each be invariant subspaces under f. 
If frnxn and Pry» are matrices such that P~'TP = ee 
C i} 
explain why (f= span {P.i..-, ber) ands) = spon {Py 
. 
4 ban} 
are each invariant subspaces under f. 
AL 
fs 
4 
Find the Jordan form for A = (= 0 2). 
Determine all possible different (up to a permutation of the blocks) 
Jordan forms of a 4 x 4 nilpotent matrix N. 

544 
Chapter 4 
Vector Spaces 
4.7 COMPLEMENTARY SUBSPACES 
The sum of two subspaces ¥ and JY of a vector space V as defined on page 
491 is the set 
¥+)={x+y|x€4 and y € Y}, and it was established there 
that Y + 
is another subspace of Y. For example, if V is a plane through 
the origin in R® and Y isa 
line through the origin as depicted in Figure 4.7.1, 
then the parallelogram rule makes it visually evident that 4 + y =R'. 
FIGURE 4.7.1: COMPLEMENTARY SUBSPACES AND OBLIQUE PROJECTION 
In addition to adding up to the entire space, 
¥ and J are special in the sense 
that 
¥ MY =0—they are said to be disjoint spaces. This means that R? (and 
every vector in R®) can be "resolved" (or decomposed) into a pair of oblique 
components. These observations motivate the following definitions. 
4.7.1. Definition. Subspaces 1, Y of a vector space V are called com- 
plementary subspaces whenever 
Vert 
and 
Any =90, 
(4.7.1) 
in which case Y is said to be the direct sum of X and JY. This is 
denoted by writing V = Y @ y. For each v € V, there are unique? 
vectors 
x € 4, y€ Y such that v=x-+y. 
e 
x is the oblique projection of v onto * along (or parallel to) ¥. 
e y is the oblique projection of v onto Y along (or parallel to) ¥. 
These concepts are generalizations of those in Definition 4.3.1 on page 456 
in which ¥ and Y are orthogonal to each other. It should be clear that when 
X 1 Y, the notion of oblique projection includes the concept of orthogonal 
+ 
Uniqueness follows because if 
v = x1 +y1 =x2+ yo, then x; —x29 = yi —y2, and (4.7.1) 
ensures that x1 — x2 
= 0 = yi — yo. 

4.7 Complementary Subspaces 
545 
projection as discussed on page 457—compare Figure 4.3.2 on page 458 with 
Figure 4.7.1. 
Convention: It is often necessary to distinguish between oblique and orthogonal 
projections, so henceforth adopt the convention of taking the word "projection" 
to mean oblique projection. In the special case when 4 | Y, explicitly write or 
say "orthogonal projection." 
h respective bee ce aa 
the vector space V if and my ie 
Bet 
0 
"y= 
a 
thes 
empty set), 
and ce J By is a basis for V. 
Proof. 
Let By = {x1,X0,...,x,} and By =1y1; yo,-..4, 
75}. First: assume 
that ¥Y and JY are complementary. This forces By 1 By = . For otherwise, 
z € Bx 1 By implies that z € 
4M Y = 0, which is impossible because By 
and By are linearly independent sets. To see that By U By is a basis for B, 
recall that By 
UBy spans V (by (4.1.10) on page 421), and use 
YN Y =O 
to 
conclude that By U By is linearly independent because 
Tr 
8 
r 
8 
0=) axit+ 
> By; => Yoaixi=—>- Biy; 
par 
y= 
= 
= 
=> Sax, EX NY 
and 
> Biyg EX NY 
i=1 
i 
ifs 
Ss 
—— Sux: = 0 > Bis = 
ao; =0=6, 
foreach 2 and j. 
a 
j=l 
Conversely, suppose now that By 
By =@ and Bx UBy isa 
basis for V. This 
means that dimV =r+s. Furthermore, By 
UBy spans 
¥ +), and hence 
Bx UBy isa basis for ¥+Y, so 
V= V+) with dimV=r+s=dim(V+yY). 
This together with the formula for the dimension of a sum (Theorem 4.1.9, page 
420) implies that dim(¥ MY) = 0 or equivalently 1M Y= 0 because 
dim(4¥ 
+ Y) =dim ¥ + dimyY — dim(4# NY) =r+s—dim(any). 
Thus V=%¥+4+) with 
YN Y=0O0. 
Projections in F" 
Given a pair of non-trivial complementary subspaces 4', .Y C F" along with an 
arbitrary vector v € F", consider the problem of determining the projection x 
of v onto *X along Y. It is straight forward to verify that the function defined 
by f(v) = x is a linear operator on F" (Exercise 4.7.1, page 562), so one 

5 
. 
— 
r 
and 
—— 
= 
a 
| 
a] 
% 
= 
4 
. 
~ 
a 
» og" 
Sealant 
~ og eer 
ter qrom™ Pgh site EEN, ¥5 
Proof. 
If f :F" — F" is the linear operator such that f(v) is the projection 
of v onto #¥ along J, then 
f(x)=x 
and 
f(y)=0 
foreach xe and ye), 
(4.7.3) 
so in particular, f(x;) =x; and f(y;) = 0 for each of the respective vectors in 
Bx and By. Theorem 4.7.2 ensures that B = By U By is a basis for F", and 
the coordinate matrix of f with respect to B is 
(fle = (LAGa)Is |---| FCs | 
[Fie | 
--- | 
LF(ve)ls) 
= (xls vs | 
ere | 
ls ae 
(ols) 
(4.7.4) 
I, 0 
= (6:1 
lerl0|---|0) 
= (4 ae 
The coordinate matrix of f with respect to the standard basis S for F" is 
P = [fls = ZJes{f|s[]se = Ues[flalTlgs = Q & m1 
Q7' (see page 529). 
Since [f(v)]s =[f]s[v]s (Theorem 4.5.3, page 510), it follows that in standard 
coordinates the projection of v € F" onto ¥ along Y is given by 
x=Pv. 
8 
In the language of restricted operators given in Corollary 4.6.10 on page 
537, the above proof shows that the restriction P ss is the identity operator on 
X while Phy is the zero operator. Other facts about projectors that this proof 
reveals are summarized in the following corollary. 

Proof of (4.7.5): If Pi, Po € F"*" are both projectors onto ¥ along Y, and 
if 
v=x+y with xe 
and ye Jy, then 
Pjv 
= x = Pov 
for all v € F". 
- 
Therefore, 
(P; —P2)v = 0 for all v € F", so Py —P2 = 0, and thus P; = Po. 
| 
Proof uy (4.1.6). PvP (Pv) = Px=x=Py = 
Fo ae 
] 
Proof of (4.7.7): If 
v= x+y for x € X and y € J, then by definition y 
is the projection of v onto Y, and I—P 
is the projector onto Y along ¥ 
because (I— P)(v) =v—Pv=x+y-x=y. 
Proof of (4.7.8) and (4.7.9): Each is a direct consequence of Definition 4.7.1. 
Proof of (4.7.10): This follows because P is similar to the diagonal matrix 
( 
i a (recall Corollary 3.2.6 on page 306). 
Proof of (4.7.11): This follows from (4.7.2) because trace is invariant under a 
similarity transformation. 
& 
Idempotents and Projectors 
Statement (4.7.6) says that if P ¢ F"*" is a projector, then P is idempotent 
(P? =P). But what about the converse—is every idempotent matrix necessarily 
a projector? The next theorem says "yes." 

548 
Example 
Chapter 4 
; 
Vector Spaces 
Proof. 
Every projector is idempotent by (4.7.6). Proving that every idempotent 
is a projector relies on the realization that if P = P?, then F" = R(P)@N (P) 
(ie, R(P) and N(P) are complementary subspaces). To see this, first observe 
that F" = R(P)+N(P) because for each v € F", 
v=Pv+(I—P)v, 
where 
Pye R(P) and (I—-P)veN(P), 
(4.7.12) 
and then note that R(P)N(P) =0 because 
xe R(P) 
=>» 
x= Pz for 
some z, and 
xE€N(P) = 0=Px=P'z=Pz=x. 
Thus F" = R(P)@ N(P), and hence P is the projector onto R(P) along 
N (P)—indeed, if v = x+y is the unique resolution of v € F" with x ¢ R(P) 
and y € N(P), then, by definition, x is the projection of v onto R(P) along 
N (P), so, due to the uniqueness of x, (4.7.12) shows that 
x= Pv. 
8 
Notice that there is a one-to-one correspondence between the set of idem- 
potents defined on F" and the set of all pairs of complementary subspaces of 
F" in the following sense. 
e 
Each idempotent P defines a pair of complementary spaces—namely, R (P) 
and N (P). 
e 
Every pair of complementary subspaces 4 and JY defines an idempotent— 
namely, the projector onto Y along Y. 
Let X and JY be the subspaces of R® that are respectively spanned by 
6 -{((2.()} 
5 ={()} 
Theorem 4.7.2 on page 545 ensures that these are complementary subspaces 
1 
0 
1 
because rank (1 1 
1) = 3 means that By U By is a basis for R°. In 
—] 
—2 
0) 
the notation of Theorem 4.7.3 on page 546, the projector onto ¥ along ) is 
determined from (4.7.2) to be 
owas 
ot 
Teor 
Pant 
22 
Pee 
p=a(% Genel at 1 -1)( 1 0) ( 
mead 
0) 
=} 
=) 
uO 
Cs 
0 
o.oo 
al 
Form Vv == (2 
Cae the projections of v onto XY along Y and onto 
along 4 are respectively given by 
Py = @ and 
(I—P)v = (3) 

Proof. If A is diagonalizable, then simply expand A = PDP™!, where the 
eigenvalues in D are grouped so that D = diag (AiI, AoI,...,As1). Let P 
and P~! be partitioned conformably so that 
YT 
MI 
ae, 
al 
4 
A= PDR =( Xi] X2{->-| Xs) 
eye 
es 
ys 
= AyX1Y] aP A2X%2Y5 Sp 20D ae AEN 
PP-! =I 
translates to 0}_, X;Y* =I, while P-'P =I means that 
wee 
ofl ewhen 
d= 4, 
(XpV 
sj 
Xeviy 
Gaia when 
ix j, 
Vem 
2k when i 4 j. 
Thus X;Y} 
is a projector. Since the columns in X; constitute a basis for 
N(A—A,I) (Corollary 3.2.6, page 306), and the columns in Y; form a basis 
for N (A — AjI)*, it follows that R(X;Y}) = R(Xi) = N(A— iI) (Corollary 
4.2.12, page 437), and 
N (Xi¥?) = N(Y;)* = R(¥i) =[N(A-AD)*}¢ = R(A - Add) 

550 
Chapter 4 
Vector Spaces 
(recall (4.3.9) on page 460). Therefore, X;¥} = Gj is the projector onto 
N(A—Aj,I) along R(A—AjI), and thus (4.7.13) is proven. You are asked 
to prove the converse of the spectral theorem in Exercise 4.7.19, and statement 
(4.7.16) is a consequence of Exercise 4.7.24 on page 565. 
ed 1. 
7. 
7 
Corollahy (Functions of Diagonalizable Matrlees) Let A be 
diag- 
- 
onalizable with o (A) = {A1,Az2,-.-,As}, and let A = }>;_, AiG; be 
: its spectral decomposition. For functions f:C—C 
defined on a (A), 
f(A )= fQx)Gi + fO2)Ge + 1G. 
: 
(4.7.17) 
In essence, this is just a reformulation of (3.3.6) on page 324. The theory 
for nondiagonalizable matrices is in §4.10 on page 595. 
Proof. 
This is a direct consequence of (3.3.6) on page 324. 
I 
Note: While the representation for f(A) in (4.7.17) emanates from that in 
(3.3.6) on page 324, the expression in (4.7.17) has the distinct theoretical advan- 
tage in that all G,'s are uniquely defined whereas the similarity transformation 
P in (3.3.6) is not unique. It is interesting that while the X;'s and Y*'s in the 
proof of Theorem 4.7.6 are not unique, their product X;Y* = G; is unique. 
In case \ is a simple eigenvalue of A € F"*", the associated spectral 
projector is particularly "simple." 
4.7.8. Theorem. 
If x and y* are respective right-hand and left-hand 
eigenvectors associated with a simple eigenvalue \ € o(Ayyx»), then 
y*x £0, and 
Ge xy yx 
(4.7.18) 
is the spectral projector onto N (A — AI) along R(A — AI), regardless 
of whether or not A is diagonalizable. 
Proof. 
Since X is a simple eigenvalue, the respective right-hand and left-hand 
eigenspaces N (A — AI) and N(A— XI)" are each one dimensional (Theorem 
3.2.8, page 307), so any right-hand and left-hand eigenvectors x and y* asso- 
ciated with \ respectively span these spaces. If y*x = 0 was possible, then 
N(A-—AI) LN(A-AD* 
=> N(A-AD)= [N(A-AD)*]* = R(A-QD) 
(by Theorem 4.3.5 on page 460). But this is a contradiction because 0 is a simple 
eigenvalue of (A — XI) so that index (A — AI) = 1 (Theorem 4.7.13, page 555), 

4.7 Complementary Subspaces 
551 
and Corollary 4.7.12 on page 555 forces R(A— AI) N(A-—AI) = 0, which 
cannot happen. Now, G is a projector because G? = x(y*x)y*/(y*x)? =G, 
and Corollary 4.2.12 on page 437 shows that R(G) = range(x) = N (A — Al) 
and N(G)= N(y*)=R(A-AI). 
@ 
Interpolation Formula for Spectral Projectors 
Given a set of m points D = {(21,y1), 
(x2,y2), ---, (2m,Ym)} in which the 
x;'s are distinct, it was established on page 162 and illustrated on page 498 that 
the Lagrange interpolation polynomial given by 
us 
[Tji(@ — 25) 
Z 
j 
Lo ele 
i=1 
[Tj ee(xs — £5) 
exactly passes through each point in D. This means that if A is diagonalizable 
with o (A) = {Aj,A2,-..,As}, andif f:C—C 
is defined on o (A), then the 
interpolation polynomial 
Menem oe) 
# 
j 
pa) => | fA) => 
(4.7.19) 
i=1 
TTjeeQs . Aj) 
is such that p(A) = f(A) for each A € o(A). This leads to the following 
Lagrange interpolation formula for the spectral projectors. 
4.7.9. Theorem. If A is diagonalizable with o(A) = {A1,\2,.--, As}, 
then the spectral projectors G; in the spectral decomposition theorem 
on page 549 are given by 
G,= Tae et 
| 
(4.7.20) 
Thj2iQx — 3) 
Proof. 
Given a function f : C + C defined on o(A), 
let p(x) be the 
Lagrange interpolation polynomial (4.7.19) such that p(A) = f(A) for each 
\ € o(A). The spectral decomposition of f(A) in (4.7.17) can be written 
as 
A — X,1) 
Tat 
FO)Gi = S00) -Sly 522 ee 
=r 
Te Wee) 
5 
1 
ite 
Ag 
. 
' 
- 
Taking j;(z)= 
On reek 
produces f;(A) 
= G; so that 
[Tj gs(A — AD) 
Gi = fi(A) = —2 
Fac re Xj) 
: 

-" 
Proof. 
Use Theorem 4.7.9 in (4.7.17) on page 550. 
If A € F"*" is diagonalizable with three distinct eigenvalues {A;,A2,A3}, then 
F(A) = FA1)G1 + FA2)G2 + fAs)Gs_ with 
_ (A=)oI)(A—AsI) 
(A—\i1)(A—AsI) 
Example 
(A—AyI)(A—al) 
= Se 
= = SE 
 , 
Gg = 
Gr 
Di Aa)Oi— 
rs)? 2? 
Qa= Ares)? 
Os An)As—Aa) 
Example 
' 
1 
Fy 
—. 
. 
. 
. 
Determine the spectral projectors for A = ( 
8 ire -s) 
. This is the matrix 
—8 
in (3.2.7) on page 310 where it was shown that A is diagonalizable with two 
|
distinct eigenvalues, 43 = 1 and Ag = —3, so there are two spectral projectors, 
|
G, = the projector onto N (A — 11) along R(A — 11), 
G2 = the projector onto N (A + 3I) along 
R(A+ 3). 
There are several ways to find these projectors. 
Solution 1. Compute bases for the necessary nullspaces and ranges, and use 
(4.7.2) as illustrated in the example on page 548. 
Solution 2. Compute G; = X;Y/ as described in the proof of Theorem 4.7.6. 
The required computations are essentially the same as those needed above. Since 
much of the work has already been done in the example on page 310, just com- 
plete the arithmetic to obtain 
eal 
1 
-1 
-1 
>'e 
1 0) 
= (X:|X2) 
sti) 
Pr eal | 
aererpemy: 
hey 
Peale 
Orel: 
Ya 
2 
-2 
-1 
sO 
es 
1 
-1 
-1 
0 
1 
1 
Gi= 
Xi Ya = ( 
2 
—2 =), Go=X2Yi= (-2 3 
2). 
=; 
DY 
2 
-2 
-1 
Solution 3. Since \; = 1 is a simple eigenvalue, (4.7.18) can be used to compute 
G from any pair of associated right-hand and left-hand eigenvectors x and 

4.7 Complementary Subspaces 
553 
y?, which can be obtained by direct computation or else extracted from P and 
1 
P-! tobe x= ( | 
and y? = (1, —1, —1). To emphasize that the choice of 
eigenvectors is irrelevant, note that for any other pair 
X =ax and y! = By', 
1 
Bex 
Qa 
2 
1 
-1 
-1 
seaman, ae, 2) = 
y?x 
ap 
a 
a 
Invoking (4.7.15) yields the other spectral projector as Gg 
= I— Gy). 
Solution 4. An easier but somewhat ad hoc solution is obtained from Corollary 
4.7.7 considering f|(z) 
=x —A, =x-—1 
and fo(z) =x—rA2=2+3 
to write 
FRA) 
AST = (1G, © 3Gs\ = (Geer ee 
fo(A) =A+31l= (1G, = 3G2) +3 (G, =P Go) = 4G), 
so that 
c= (A * 
31) 
ee a T) 
Solution 5. Use the interpolation formula (4.7.20) to compute the G,'s. This 
is not significantly different than what Solution 4 boils down to. 
Range-Nullspace Decomposition 
There are infinitely many different ways to generate complementary subspaces 
in F", but when starting with a matrix A € F"*", there are two particu- 
larly important ways of doing so. The first is by means of the orthogonal de- 
composition theorem (page 460) that produces F" = R(A) ®N(A*), where 
N (A*) = R(A)~ (i.e, R(A) and N(A*) are orthogonal complements). This 
holds for all matrices regardless of whether they are square or rectangular. But 
when A is square, a power of A will produce a different decomposition of F" 
(but not necessarily an orthogonal decomposition). 
The rank plus nullity theorem says that dim R(A) + dim N (A) = n, so 
this leads one to wonder if 
R(A) and N(A) might always be complementary 
subspaces when A is square. While this is trivially true for nonsingular matri- 
° 
n 
. 
. 
. 
(0) 
1 
ces, it need not hold for singular matrices—e.g., consider A = & we where 
R(A)NN (A) #0. But as the next theorem shows, there is always some power 
A* for which R(A*) and N (A*) will be complementary. 

- 
See eR I 
As 
ie 
Re 
Proof. 
As A is powered the nullspaces grow and the ranges shrink. 
NASON (A) CN (A?) C-:- CN (AP) CN (At) Cee 
R(A°) DR(A) DR(A') D---D R(A*) DR(A*?) D--- 
Equality must exist at some point in each of these chains because if not, then 
dim N (A*) < dimN (A't!) and dimR(A') > dimR(A'+!) for every i, 
which is impossible in a finite dimensional space. The rank plus nullity theo- 
rem guarantees that the first place where equality is attained is the same in each 
chain. Moreover, once equality is attained it is maintained throughout in both 
chains because if k > 0 is the smallest integer such that R(A*) = R(A*+?), 
then for every integer i > 1, 
R(A**) = R(A'A*) = A'(R(A*)) = A! (RA) — RASA, 
The rank plus nullity theorem ensures that the same is true in the nullspace 
chain so that the following holds. 
N (A°) CN(A)Cc::: CN (A*) SN (Are) aN (Ab) ee 
R(A') 2 RA) OS AS RAG 
oR AS) ae 
Furthermore, 
R (A*) 9 N (A*) =0 (and hence R(A?)M N(A?) = 0 for all 
p =k) because if x € 
R(A*).N(A*), then x = A*y for some y € F", and 
A*x =0, so 
Aéy = A*x=0>y €N (A) = N(A*) 3 x=0> R(A*)NN(A*) =0. 
The formula for the dimension of a sum (page 422) together with the rank plus 
oullity theorem yields R(A*) +N (A*) =F" because 
dim [R (A*) +.N (A*)] = dim R(A*) +dim N (A*) — dimR(A*) 
AN (A*) 
= dim R(A*)+dimN(A*)=n =» R(A*)+N(A*)=F". 
w 
(4.7.22) 
(4.7.23) 

, 
4.7 Complementary Subspaces 
555 
Proof. 
The first statement is self evident. If R(A?)M N(A") = 0, then 
R(A?) @ N (A?) =F" (by the preceding dimension argument), so 
R(A®) = AP(E") = A(R (A?) + N(A?)) = A(R(A?)) = 
(AP), 
which implies that 
p>k. 
A singular matrix A has at least one zero eigenvalue, so a natural question 
is, how is index(A) related to the number of zero eigenvalues? The next theorem 
provides the answer. 
Proof. 
By considering the m zero eigenvalues first, Schur's triangularization 
theorem (page 306) ensures the existence of a unitary matrix U such that 
Ages U(r Se ee where Ty; is an m xX m triangular matrix of the form 
O 
To22 
Ore 
Taek 
Fe 
(ieee 
0 
a 
= 
: 
, so Tij = 0. Consequently, A™ = U(o alte so 
es 
* 
22 
0 
that rank (A™) = rank (Att) because T22 is nonsingular. Thus m<k. 
& 
Or 
To determine the index of 
A= 
{0 
1 
'), 
first note that A is singular 
Oat 
md 
(rank (A) = 2), so index(A) > 0. Powering A yields 
AS OO 
(a) 
© 
A (0 0 0) and 
Ao € 0 0). 
® 
@ 
@ 
O 
@ 
© 
Thus index(A) =2 because rank (A) > rank (A?) = rank (A®). Alternately, 
nay) 
(2)} 40) -em 3). 148 
SOLA out Ae ieaR (A*), which also guarantees that index(A) = 2. 
An important consequence of Theorem 4.7.11 is that it leads to the re- 
alization that every singular matrix essentially decomposes into two pieces, a 
nonsingular core part C and a nilpotent part N with N® = 0. 

Proof. 
The subspaces R(A*) and N (A*) are each invariant subspaces for 
A because A(R(A*)) = R(A*+!) = R(A*), and if x € A(N (A*)), then 
x = Ay for some y € N(A*), which implies A*x = A*tly = 0 so that 
x € N(A*) = N (A**!), so A(N(A*)) 
CN (A*). Therefore, if 
B= ¥ UY, 
where X and y are respective bases for R (A*) and N rae). then Theorem 
4.6.9 and Corollary 4.6.10 on pages 536-537 ensure that 
CaO 
Yea 
es i a 
ee Le 
scenes yaar y jal 
de bivperes be 
To see that C,,, is nonsingular, note that 
. 
. 
. 
. 
k 
es 
dim R(C) = dimR (A), .,,)) = dim R(A**) = dim R(A*) =r. 
The fact that N* = 0 follows from the observation that 
R—lA 
lA 
0 
(by 
(4.5.10 
512 
Be | neha 2 | the 
pe 
Cai hSor10) encnsee 
ete); 
The similarity transformation is the matrix Q = [K|Y], where the respective 
columns in X and Y are the basis vectors from 4 and Y—see Corollary 
4.6.10 on page 537. 
# 
' oA ee 1s nonsingular, then k = 0 and r=n, in which case 
C= A, N is not present, 
and Q =I. In other words, the nonsingular core is everything, and there is no nilpotent part, 
so (4.7.24) says absolutely nothing about nonsingular matrices. 

4.7 Complementary Subspaces 
557 
ao a ee 
(A) = 
s r ca 8 
ey in 
ie "4 in which 
=)» then: 
= Bu 
and iq? 
- YVe ale els 
the ie 
0 0)Q*= 
= the projector onto R (ay) along N (A*), 
onto x 
v (Ab) 
slong R(A* '): 
Example (Drazin pseudoinverse) 
The core-nilpotent decomposition (4.7.24) of a singular matrix A € F"*" leads 
a generalized inverse concept that is distinctly different than the Moore—Penrose 
pseudoinverse defined on pages 192-193. The Drazin pseudoinverse A? of A 
is defined by inverting the core C and simply neglecting the nilpotent part N 
in (4.7.24) to produce 
AD = Olen wer: 
(4.7.25) 
It should be evident that A? = A~! when A is nonsingular. Analogous to 
the four Penrose equations in Theorem 2.4.19 on page 193 that uniquely define 
the Moore—Penrose pseudoinverse A' of A, the Drazin inverse can also be 
characterized by a set of algebraic equation as follows. If k = index(A), then it 
can be proven that A? is the unique solution to the three equations 
KAX =X) 
AX = XAs APS X Ae 
(4.7.26) 
Possessing a unique solution for X here means that while Q in (4.7.25) is not 
unique, the product defining A? in (4.7.25) is unique. Three other notable 
properties of A? are as follows. 
e 
If Ax=b 
isa 
consistent system in which 
be R (A*), then x = APb is 
the unique solution that belongs to R(A*) (see Exercise 4.7.33). 
e 
AA? is the projector onto R (A*) along N (A*), and I— AA® is the 
complementary projector onto N (A*) along R (A*) (see Exercise 4.7.35). 
The equations (4.7.26) were formulated by Michael P. Drazin (1929-) in 1968, and they served 
as the original definition of his pseudoinverse. Drazin's concept initially attracted little atten- 
tion due to his abstract algebraic development. But eventually Drazin's generalized inverse 
was recognized to be a useful tool for analyzing non-orthogonal types of problems involving 
singular matrices. In this respect Drazin's inverse is complementary to the Moore-Penrose 
pseudoinverse whose applications tend to involve problems in which orthogonality is somehow 
wired in (e.g., least squares). 

Proof. 
If AP = At, then 
AAt—=AAD—~-APA=AtA => A isEP,. 
Conversely, if A is EP,, then A SUG ue aioe where U is unitary and 
C is nonsingular (Theorem 4.3.9, page 467) so that At = A 
hs (see 
Exercises 2.4.43-2.4.44 on page 200), which is easily shown to satisfy the Drazin 
equations (4.7.26). 
Example (Matrix Groups) 
An algebraic group is a set G together with an associative operation © between 
its elements such that G is closed with respect to ©; G possesses an identity 
element E (which can be proven to be unique); and every member A € G has 
an inverse A* € G (which can be proven to be unique). These are the axioms 
(Al), (A2), (A4), and (A5) in the definition of a vector space given on page 
410 except that + is replaced by ©. A matriz group is a set of square matrices 
that constitute an algebraic group under standard matrix multiplication. For 
example, the general linear group GL,(F) is the matrix group consisting of all 
nm X n matrices over F whose determinants are nonzero. The special linear group 
SL,,(F) made up of n x n matrices whose determinants equal 1 is a subgroup 
of GL,,(F), as is the set of n x n unitary matrices. The identity element in all 
of these groups is the standard identity matrix E = I,, and the group inverse 
of A €¢GL,,(F) is the usual inverse—i.e., A* = A-!. 
But there are also singular matrix groups (i.e., groups of matrices whose 
determinants equal zero). For example, the set 
e-{(¢ 2)lne9 

4.7 Complementary Subspaces 
559 
is a singular matrix group with respect to ordinary matrix multiplication. The 
interesting twist is that the notions of an identity and inverse in G are different 
than those in GL,(F). The identity element in G is E = ae oe because 
AE = A=EA for each A €G, and 
are (S a) (ids) s}) 
ran 
because AA# = E = A#A. The connection between matrix groups and previ- 
ous developments is revealed in the following theorem. 
4.7.17. Theorem. A matrix A € F"*" with rank(A) =r <n be 
longs to a singular matrix group G with respect to standard matrix 
| 
es if and only if index(A) = 1. 
- 
e 
If index(A) = 1 and A= oF Dee is a core-nilpotent 
decom postin | in which Q = [X | Y], where the columns of K and 
__ 
Y are respective bases for R(A) and N (A), then the matrix group 
- 
containing A is the set 
G= {Q( se ee | Z is nonsingular} 
Moreover, the group inverse A* €G of A as well as the identity 
E in G are respectively given by 
Aol, 3G ad Ol a 
ares) 
e 
If 
r>0 and A=BC isa 
full-rank factorization (page 190), then 
A* = B(CB) °C, 
(4.7.29) 
Note that this means that A* can be obtained by row operations via the 
reduced echelon form Ea (see the example on page 190). In particular, 
if 
r= 1, then 
A# = A/|trace(A)]?. 
(4.7.30) 
Proof. 
Suppose first that A belongs to a singular matrix group G in which 
E € G is the identity element and A* € G is the group inverse of A. Since 
A =EA = (A*#A)A = A*A?, it follows that 
rank (A) = rank (A*A?) < rank (A?) < rank (A) 
(4.7.31) 
=> 
rank(A)=rank(A*) 
=> 
indec(A)=1 

560 
Chapter 4 
Vector Spaces 
Conversely, if index(A) 2 1, and if Q = [X|Y] in which the columns of X 
and Y are respective bases for R(A) and N (A), then the set 
G= {sl Gar ee | Z is nonsingular } 
is a matrix group in which E = meee SEES: is the group identity because 
for each G € G it is evident that GE = G = EG. The group inverse of 
=a 
GeGis GF=G= a ae because GG* = G#G = E. The 
E 
: 
shed 
ON 
a 
theorem is completed by observing that if A = aye 
0 )Q 1 is a core- 
: 
Cz 
a.0 
7 
nilpotent decomposition, then A €G with A*# =A? = Q( ; 
0 )Q 1 and 
E = AA# = A#A. The derivation of (4.7.29) and (4.7.30) is developed in 
Exercises 4.7.18 and 4.7.17 on page 564. 
4.7.18. Corollary. 
If A € F"*" with rank (A) =r <n, then A 
belongs to a singular matrix group GQ if and only if the three equations 
AXA=A, 
XAX=X, AX=XA 
(4.7.32) 
possess a unique solution for X, in which case X = A® is the group 
inverse of A in G. (Theorem 7.7.2 on page 843 presents a novel appli- 
cation of A.) 
e 
A* =A? if and only if index(A) =1. 
Proof. 
If A belongs to a singular matrix group, then it follows from Theorem 
4.7.17 that the matrix A* given in (4.7.28) is one solution for X in (4.7.32). 
It is the only solution because if Z also satisfies (4.7.32), then 
A=AZA 
=> A*A = AA* = AZAA*® = ZAA*A =ZA = AZ 
=> Z=ZAZ= AA*Z = ATAZ = ATAA* — A*®. 
Conversely, if (4.7.32) has a solution for KX = A*, then the implications in 
(4.7.31) show that index(A) = 1, so A must belong to a singular matrix group 
by Theorem 4.7.17. 
Angle between Complementary Subspaces 
The angle between nonzero vectors 
u,v € R" 
as given in Definition 1.6.5 
on page 43 is the number 0 < @ < 7/2 such that cos@ = v'u/||v|l, |lull,. 
It is natural to try to extend this idea to somehow make sense of angles be- 
tween subspaces of R". A discussion of angles between completely general sub- 
spaces requires concepts developed later in the text, so the general develop- 
ment is deferred. However, the angle between a pair of complementary subspaces 
can be understood at this point. 

While this is a perfectly fine definition, it is not easy to use—especially when 
the numerical value of cos@ is required. The trick in making @ more accessible 
is to think in terms of projections and to use sin@ = (1 —cos?6)!/?. Let P be 
the projector such that R(P) = R and N(P)=WN, and recall that the matrix 
2-norm (pages 85 and 363) of P is 
(Pl, = apes ||Px||, =o1 
(the largest singular value of P). 
(4.7.34) 
Dia: 
f 
In other words, 
||P||, is the length of a longest vector in the image of the 
unit sphere under transformation by P. To understand how sin@ is related 
to ||P||,, consider the situation in R®. The image of the unit 2-sphere under 
transformation by P is obtained by projecting the sphere onto R along lines 
parallel to NV'. As depicted in Figure 4.7.2, the result is an ellipse in R. 
RES 
I|v|| = max ||Px|| = ||P| 
||><l]=1 
FIGURE 4.7.2: OBLIQUE PROJECTION OF THE 2-SPHERE ONTO R ALONG N 
The norm of a longest vector v on this ellipse equals the norm of P. That is, 
I|v||. = maxjxj|,=1 ||Px|l. = ||P||2, and it is apparent from the right triangle in 
Figure 4.7.2 that 
1 
if 
sot) 
a 
Ivo 
Ive 
TPT 
Examination of the geometry in Figure 4.7.2 shows that in R?, a number 0 
satisfies (4.7.33) if and only if @ satisfies (4.7.35). (A rigorous proof for R" is 
(4.7.35) 

562 
Chapter 4 
Vector Spaces 
deferred.) In other words, either the expression for cos@ in (4.7.33) or that for 
sin@ in (4.7.35) serves to define @. 
Exercises for section 4.7 
4.7.1. 
4.7.2. 
4.7.3. 
4.7.4. 
4.7.5. 
4.7.6. 
For 
V=4 0), let 
f: 
VV be the function defined by f(v) = x, 
where x is the projection of v onto VY along y. Prove that f is 
linear. Because f is a linear projection operator, it is referred to as a 
projector, which is short for "projection operator." 
Let V¥,VC R? be subspaces whose respective bases are 
o-{.Q} — {Oh 
(a) Explain why 4 and ) are complementary subspaces. 
(b) Determine the projector P onto *¥ along Y as well as the 
complementary projector Q onto J along 4%. 
2 
(c) Determine the projection of v = (-1) onto Y along 4. 
1 
(d) Verify that P and Q are both idempotent. 
(e) Verify that R(P) = 4 =N(Q) and N(P)=V= R(Q). 
Construct an example of a pair of nontrivial complementary subspaces 
of R°, and explain why your example is valid. 
Construct an example to show that if 
V= 4+) but 
YNY FO, then 
a vector v € Y can have two different representations as 
v = x, + y1 
and 
v = xX2+yo2, where x1,x2€ 4% and yj,y2 € Y, but x; # xe and 
yi FY2. 
Prove that direct summation is associative. That is, if Y,Y,Z 
are 
subspaces of a vector space V such that [4 6 Y] 6 Z = V, then 
X @(YV @ Z] = V. 
Let By = {X1,X2,...,Xp} and By = {y1,yo,...,y;,} be respective 
bases for subspaces 1', Y of some vector space such that By N By = 0. 
(a) Prove that By U By is a linearly independent set if and only if 
ALY = 0; 
(b) Give an example to show that if By U By is linear independent 
but By 
By #0, then it is possible to have 
*NY 40. 

4.7 Complementary Subspaces 
563 
4.7.7. 
4.7.8. 
4.7.9. 
4.7.10. 
4.7.11. 
4.7.12. 
4.7.13. 
Prove that if V,Y are subspaces of a finite dimensional space VY such 
that each v € V has a unique decomposition 
v =x+y with xe x 
and ye 
Y, then V=XOQY. 
Let {¥}*_, be a collection of subspaces from a vector space Y, and 
let B; denote a basis for ¥;. Prove that the following statements are 
equivalent. 
(i) VHXR4+4%4+-::+%, 
and ep il (Ag a 
gy) eet O MOE 
Catll, Oss 2,0... 
2, ke 
(ii) For each vector v € Y, there is one and only one way to write 
vV=xX,+xXe+---+x,, where x; € 4. 
(iii) 
B= B,UB,)U---UB, with B08; = 
for iF jis a basis 
for VY. 
Note: When any of the above statements holds, V is said to be the direct 
sum of the %;'s, and this is denoted by writing 
V = 1, 6 4%) @-:-@® AX. 
Suppose that {%;}*_, is a collection of subspaces from an n-dimensional 
vector space V such that pret dim %; =n and 4,94; =0 for each 
i # j. Construct an example to show that it need not be the case that 
VHX, 8X2 8::: PX pz. 
Let S and K be the respective spaces of n x n symmetric and skew- 
symmetric matrices. Explain why R"*" = S @K, and then find the 
ee 
ae 
projection of A = (3 
5 °| 
onto S along K. Hint: See Exercise 
1.2.9, page 14. 
Prove that if P 4 0 is projector, then ||P|| > 1 for every matrix norm. 
Note: In passing, recall that ||P||, = 1 if and only if P is an orthogonal 
projector (Theorem 4.3.7, page 464). 
Explain why ||I— P|], = ||P||, for all projectors that are not zero and 
not equal to the identity. 
Prove that if u,v € R"*' are vectors such that v*u = 1, then 
I — uv" 
||, = luv" 
||, = [ull [Ivllz = lluv"lle 
where ||x||,- is the Frobenius matrix norm. 

564 
Chapter 4 
Vector Spaces 
4.7.14. 
4.7.15. 
4.7.16. 
AT LG. 
4.7.18. 
4.7.19. 
4.7.20. 
Describe all 2 x 2 projectors in F?*?, 
Explain why AG; = G;A = \;G; for the spectral projector G; asso- 
ciated with the eigenvalue A; of a diagonalizable matrix A. 
Power Method. Let (\i,x) be an eigenpair for a diagonalizable matrix 
A whose distinct eigenvalues are 
|Ai| > |A2| = [As] 2 +++ 2 |Asl- 
(This implies that A is real—otherwise 1 is another eigenvalue with 
the same magnitude as ;.) Explain why 
k 
lim (+) =G, 
(the first spectral projector). 
1 
Note: If y = G,x, where x ¢ N (Gj), then (Ai,y) is an eigenpair 
for A, and A*x/\* — y. Since the denominator \¥ acts only as a 
scaling factor to keep the length of A*x under control, the direction of 
A*x tends toward the direction of y. Consequently, any scaling factor 
vz in place of A* can be used to produce an eigenvector in the limit— 
one example is /, = || A&x|| . The power method is considered in more 
detail in Theorem 6.3.1 on page 772. 
For c,d € F" with d*c 40, explain why the group inverse of the rank- 
one matrix A = cd* is given by A* = A/[d*c]? = A/[trace(A)]?. 
Since all rank-one matrices can be expressed this way, it follows that 
A# = A/|trace(A)|? 
for all rank-one matrices. Note that this is a 
special case of the following exercise. 
Let A € F"*" be a group matrix having rank(A) = r > 0. Prove 
that if A = BC is a full-rank factorization (page 190), then CB is 
nonsingular and A* = B(CB)~?C is the group inverse of A. 
Spectral Theorem Converse. For A ¢€ F"*", prove that if the G,'s 
are the projectors defined in Theorem 4.7.6, and if (4.7.13)—(4.7.15) are 
satisfied, then A is diagonalizable. 
Uniqueness of Spectral Projectors. Explain why there is no wiggle room 
in spectral decomposition theorem in the sense that if the G,'s are 
projectors satisfying (4.7.13)—(4.7.15), then they are unique—i.e., G 
must be the projector onto N(A — ;I) along R(A — ),I). 
Hint: Recall that the trace of a projector equals its rank. 
a 

4.7 Complementary Subspaces 
565 
4.7.21. 
4.7.22. 
4.7.23. 
4.7.24. 
Suppose that 4 and Y are complementary subspaces of F", and let 
Q = [X|Y] be a nonsingular matrix in which the columns of X and 
Y constitute respective bases for 4 and Y. For an arbitrary vector 
Vigan explain why the projection of v onto Y along Y can be 
computed by the following two-step process. 
(1) Solve the system Qz =v 
for z. 
(2) Partition z as z= (=), and set 
p = Xz}. 
Z2 
For complementary subspaces ¥ and Y of F", let P be the projector 
onto & along J, and let Q = [X|Y] in which the columns of X 
and Y are respective bases By and By for ¥ and Y. Prove that if 
Q7'AnxnQ is partitioned as Q~!AQ = are reeds then 
Olen: OS ea 
Q(o #3 QU = PAU), 
Q(, 0/0 =U—P)AP, Q(5 4), )Q?=C—P)AM—P). 
Note: This means that when A is considered as a linear operator on 
F"", the coordinate matrix of A with respect to 
B = Bx U By is 
[A]g = we feu in which the blocks are the coordinate matrices 
of the following restricted operators. 
Agee [PAP,,] 
Ate [PAI Zs P),,| 
Bx 
By Bx 
Abr (I & P)AP/,.| 
IS ld EPA P| 
Bx By 
By 
Suppose that F" = V@Y, where dim X =r, and let P be the projector 
onto 4 along Y. Explain why there exist matrices B, 
x, and Cyn 
such that P = BC, where rank (B) = rank(C) =r and CB =I,. 
This is a full-rank factorization for P as defined on page 190. 
Let E,F € F"*" be projectors. 
(a) Prove that 
E+ F is a projector if and only if EF = FE = 0; 
in which case 
E+F 
is the projector onto 
R(E)@R(F) along 
N(E)ON(F). 
(b) Let Gy,Go2,...,G, be the spectral projectors associated with 
{1,A2,---;Ar} 
C o(Anxn) as defined in the spectral decom- 
position theorem (page 549). Explain why )7>;_, G; is the pro- 
jector onto @f_,N (A —A,J) along M7_,R(A — A\I). 

566 
Chapter 4 
Vector Spaces 
4.7.25. 
4.7.26. 
4.7.27. 
4.7.28. 
Let B,F ¢ F"*" be projectors. Prove that E—F is a projector if 
and only if EF = F = FE; in which case E — F is the projector onto 
R(E)ON(F) along N (E)& R(F). 
Hint: Consider the properties in Corollary 4.7.4 on page 547. 
Let E,F € F"™" be projectors. Prove that if EF = FE, then P = EF 
is the projector onto 
R(E)N R(F) along N(E)+N(F). 
— 
Let P and Q be projectors. 
(a) Prove R(P) = R(Q) if and only if PQ=Q and QP =P. 
(b) Prove N (P) = N (Q) if and only if PQ=P and QP=Q. 
c) Prove that if E,,E2,...,E, are projectors with the same range, 
and if a1,Q9,...,Q@_ 
are scalars such that py: a; = 1, then 
>; 2j;H; is a projector. 
Inner, Outer, and Reflexive Pseudoinverses. An inner pseudoinverse for 
AcéF™*" isa matrix X € F"*™ such that AXA = A, while an outer 
pseudoinverse for A is a matrix X satisfying XAX = X. (These 
are the first two Penrose equations in Theorem 2.4.19 on page 193.) 
When X is both an inner and outer pseudoinverse it is called a reflexive 
pseudoinverse. 
(a) If Ax = b is a consistent system of m equations in n un- 
knowns, and if AW is any inner inverse for A, explain why the 
solution set for Ax = b can be expressed as 
A-b+R(I-A7A) = {A-b+ (I— A7A)h| he F*}. 
(b) Let M and CL be respective complements of R(A) and N (A) 
so that F" = R(A)@M and F" = LEN (A), and let P and 
Q be the respective projectors onto R(A) along M and onto 
£ along N(A). Prove that for any inner pseudoinverse A~ 
for A, the matrix X = QA'P is the unique reflexive inverse 
such that R(X) = £ and N(X) = M. Hint: Use the result 
from Exercise 4.7.27. 
4.7.29. 
(a) 
Prove that if A ¢ F""*" is diagonalizable, then inder(A) < 1. 
4.7.30. 
(b) 
Show that the converse is not true for singular matrices by giving 
an example of a singular matrix A such that inder(A) =1 but 
A is not diagonalizable. 
Prove that if A ¢ F"*" has index k > 0, then index(A*) = 1. 

4.7 Complementary Subspaces 
567 
4.7.31. 
4.7.32. 
4.7.33. 
4.7.34. 
4.7.35. 
4.7.36. 
4.7.37. 
4.7.38. 
4.7.39. 
Let P be a projector different from the identity. 
(a) Explain why index(P) = 1. 
(b) What is the core-nilpotent decomposition of P? 
Find a core-nilpotent decomposition and the Drazin inverse of 
=2 
0 
-4 
A= ( 
ee. 
1). 
3 
2 
2 
Let A be a square matrix of index k > 0, and let 
be R (A®); 
(a) Explain why the linear system Ax=b must be consistent. 
(b) Explain why x = A?b 
is the unique solution in R(A*). 
(c) Explain why the general solution is given by A?b+ N(A). 
Let A € F"*"" have index(A) = k, and suppose that A*x = b is 
a consistent system of linear equations. Explain why there is a unique 
solution in R(A*). 
For A € F"*" with index k and Drazin inverse A?, explain why 
AA" is the projector onto R(A*) along N (A*). What does I-AA? 
project onto and along? 
For singular matrices A € F"*", explain why R(A) | N (A) implies 
index(A) = 1, but not conversely. 
When A € F"*" is nonsingular, A~! can be represented as a polyno- 
mial in A—recall (3.2.12) on page 313. Explain why the Drazin pseu- 
doinverse A? has the same property when A is singular—i.e., A? 
can also be represented as a polynomial in A. 
For singular matrices, prove that A is a group matrix if and only if 
R(A)NN (A) =0. 
Alternate Core-Nilpotent Decomposition. Prove that if A € F"*" has 
index(A) =k > 0, then there exist unique matrices C, N € F"*" such 
that 
A=C+N for which CN = NC = 0, where N is nilpotent of 
index k and index(C) <1. 
Hint. Consider A?A? and A(I— AA"), where A" is the Drazin 
pseudoinverse of A defined in (4.7.25) on page 557. 

568 
Chapter 4 
Vector Spaces 
4.7.40. 
4.7.41. 
4.7.42. 
4.7.43. 
Let A €F"*" be ina 
singular matrix group G. 
(a) Explain why the identity E in G is the (oblique) projector onto 
R(A) along N (A). 
(b) Let 
Be F"*" bea matrix such that AX = B has a solution 
for X. Explain why X = A*#B isa 
particular solution for X, 
and why the set of all solutions can be written as 
{X = A*B + (I- E)H|H € F"*"}. 
(c) Suppose that YA = B has a solution for Y. Explain why 
Y = BA?® 
isa 
particular solution for Y, and why the set of 
all solutions is 
{Y = BA* + K(I—E)|KeF"*"}. 
Hint: Use the fact that A? is the unique matrix satisfying 
A = AA#A, A# = A*#AA#, and AA* = A*A to first 
establish that (A*)# = (A#)*. 
Prove that if A € F""" is a positive semidefinite matrix, and if M is 
a subspace of F" such that M C R(A), then 
R(A) 
=M@A(M?+). 
Hint: Show that dim A(M+) = dimR(AP yj.) by using Theorem 
4.2.11 on page 436. 
Splitting Semidefinite Matrices. Let A ¢ F"*" be positive semidefinite, 
and let M be a subspace of F" such that M C R(A). Prove that 
there exist unique positive semidefinite matrices B and C such that 
A=B+C, 
where 
R(B)=M and R(C)=A(M?+). 
Hint: Use Exercise 4.7.41 along with the fact that direct sums are 
associative (Exercise 4.7.5 on page 562), and write A = IAI* with 
I = 
P+(I-—P), where P is the oblique projector onto M along 
N = A(M+)@N(A). 
Projection onto a Hyperplane. 
For u,w € R" with u'w 40, let M 
be the hyperplane (defined on page 53 and in Exercise 4.3.28 on page 
477) given by 
M=ut", and let W= span{w}. 
(a) Explain why R" = 
M@wW. 
(b) For b€R""*', explain why the projection of b onto M along 
W is given by 
(c) For a given scalar 8, let H be the hyperplane in R" defined 
by H = {x|u'x = 6}. Explain why the projection of b onto 
hg 
H along W must be given by p = b— (a) w. 
ul w 

4.7 Complementary Subspaces 
569 
4.7.44, Oblique Projection Method. 
Assume that A,.,x =b is a nonsingular 
system that has been row-scaled so that ||Ajx||,. = 1 for each i, and 
let H; = {x| Ajxx = b;} be the hyperplane defined by the i" equation 
(see Exercise 4.3.28 on page 477). In theory, the system can be solved 
by making n-— 1 oblique projections of the type described in Exercise 
4.7.43 because if an arbitrary point p; in Hy, is projected obliquely 
onto Hz along H; to produce pe, then pz isin H,M Hg. If pe is 
projected onto H3 along H; M He to produce p3, then ps3 € H,M 
H2NHz3, and so forth until p, € N%_,H;. This is similar to Kaczmarz's 
method given in Exercise 4.3.29 on page 477, but here the projections 
are obsane instead of orthogonal. pee 
projecting pz, onto Hyz+1 
along M*_,H; is difficult because M*_,H; is generally unknown. This 
aeons is overcome by modifying the procedure as follows—use Figure 
4.7.3 below with n= 3 as a guide. 
FIGURE 4.7.3 
Step 0. 
Begin with any set {pi »P5 ete pe 2 C Hy, such that 
{(p; 
(1) = Ds), (p\" — ps"), ats (p () = ps")} is linearly independent 
and Aox(P} i pi") £0 for Iie Oe Bier aen i 
Step 1. 
In cae project pi) onto H» through ps? poe to 
produce {pi ; pe 
) rae py} CHiN Hp (see Figure 4.7.3). 
Step 2. 
Project p im 
) onto H3 through py, p", ne p?? to produce 
{pe p?, a4 Or CH, NH2NH3. And so the process continues. 

"eony Ents 25 
pry a 
AP k Oe 
G 
aver 
= 
>» 
S 

4.8 Jordan Form for Nilpotent Matrices 
571 
4.8 JORDAN FORM FOR NILPOTENT MATRICES 
A matrix A € F"*" can be diagonalized by a similarity transformation if and 
only if A possesses a complete set of linearly independent eigenvectors. These 
eigenvectors provide a basis B for F" such that the coordinate matrix [A]g is a 
diagonal matrix. But for matrices that are deficient in independent eigenvectors 
(ie., for matrices such that geo mult(X) < alg mult(X) for some 
 € a (A)), 
the next best alternative to diagonalization is triangularization such as in Schur's 
theorem on page 306. While Schur's theorem is useful in many situations, it 
nevertheless hides some intrinsic characteristics of the underlying matrix. The 
purpose of this section is to remedy this situation by developing a different theory 
for triangularizing nondiagonalizable matrices by similarity. 
Nilpotent Matrices 
The backbone of the development is the theory of nilpotent matrices. Such ma- 
trices were formally introduced in Definition 3.2.4 on page 305, and some of 
their properties have appeared in previous discussions. But for completeness the 
definition is repeated below and put into the current context. 
4.8.1. Definition. A matrix L € F'*" is nilpotent whenever L* = 0 
for some positive integer k. 
e 
The smallest positive integer k such that L* = 0 is called index 
_ of nilpotency, 
and this agrees with the concept of index as defined 
in Theorem 4.7.11—1.e., index(L) =k (as explained below). 
To see that k = index(L) is the smallest positive integer such that L* = 0, 
suppose that p is a positive integer such that L? = 0, but L?-! 4 0. It follows 
fromn(47,20 hate A (L°)oa->-te 
(LE yet ais 
(LS?) 
3 -, and 
this makes it clear that it's impossible to have 
p< k or 
p>k, so 
p=k 
is the 
only choice. 
While nilpotent matrices can assume many different forms, there are some 
special nilpotent matrices that are defined as follows. 
4.8.2. Definition. A nonzero nilpotent matrix N ¢ F"*" is in standard 
form (or standard nilpotent form) if N is upper triangular with zeros 
on the diagonal and 1's on the super diagonal as shown below. 
Naxn a 
peace 
(4.8.1) 

572 
Chapter 4 
Vector Spaces 
Note: Throughout the discussion of nilpotent matrices an uppercase N will 
always denote a nilpotent matrix in standard form, whereas an uppercase L 
will denote a general nilpotent matrix that may or may not be in standard form. 
Being upper triangular is arbitrary—the lower triangular counterpart N? of 
(4.8.1) may be referred to as a lower standard nilpotent form. 
Straightforward computation shows that each time the standard form in 
(4.8.1) is powered, the line of 1's moves up to the next super-diagonal position 
until N" =0 so that index(N) =n. For example, when n = 4, 
OF 
Om 
2 
OF0 
' 
N* = 
0 
oo 
Fo 
and each is nilpotent with index(N) = 4, index(N?) =3, index(N*) = 2, 
and index(N*) = 1. 
Two important characteristics of nilpotent matrices L are that o(L) =0 
(Exercise 3.1.19, page 300) and, with the exception of the zero matrix, nilpo- 
tent matrices are not diagonalizable (page 305). This means that L,., cannot 
have n linearly independent eigenvectors, or equivalently, dim N (L) < n. The 
purpose of the following developments is to extend a specialized eigenbasis B 
(i.e., a special basis for N(L)) to a basis 7 for F" such that the coordinate 
matrix [L],7 is block diagonal in which each diagonal block is a nilpotent matrix 
in standard form to produce the following theorem. 
4.8.3. Theorem. If L ¢ F""" is a nonzero nilpotent matrix of index k 
with dim N (L) = ¢t, then there is a similarity transformation P such 
that 
Ni 
No 
PLPeN]= 
(4.8.2) 
N;: 
in which each block N; is a nilpotent matrix in standard form. Ma- 
trix N is called the Jordan form for L, and N is unique up to the 
arrangement of the blocks N; in N. (see Theorem 4.8.4 on page 578). 
Proof. 
The approach is a constructive process that is organized into five steps. 
(1) Construct a specialized basis B = {b1, be,...,b,} for N (L). 
(2) Extend each b € B to a "Jordan chain" (to be defined in (4.8.5)). 
(3) Prove that the set J of all Jordan chains is linearly independent. 
(4) Prove that J contains exactly n vectors so that it is a basis for F". 
(5) Conclude that [L]z is the matrix in (4.8.2). 

4.8 Jordan Form for Nilpotent Matrices 
573 
Step 1: Construct a specialized basis for N(L) by considering the nested se- 
quence of subspaces 
O= Mz C Mg-1 © Mp2 G++ 
C Mi © My = N (L) 
in which 
MM, — AL) NG) 
tor tony (k —1),.<, 1,0. 
(4.8.3) 
Start with any basis S,_, for Mxz_1, and augment S;,_; with a set 
Sp—2 of extension vectors so that S,_1 U Sp_2 is a basis for M,_9. 
Repeat the process—i.e., augment S,_1; U Sp_2 with a set S,_3 so 
that Sp_1 U Sp_2 U Sp_3 is a basis for Mz_3. Sequentially continue 
this extension process until a basis 
Bong 
Sees Oo) Sorby, boy bat 
(4.8.4) 
for N(L) is produced. Figure 4.8.1 below depicts a typical extension 
process when k = 5. 
M4 
Ms 
Mp2 
Mi 
Mo = N (L) 
Step 2: 
FIGURE 4.8.1: 
A TYPICAL EXTENSION PROCESS FOR k=5 
Extend each b € B to form a Jordan chain Jp as follows. If b € Sj, 
then solve the system of equations L'x = b for x (any solution suf 
fices). This system is consistent and has only nonzero solutions because 
Sic Mi C R(L'). Build the associated Jordan chain Jp by setting 
pe xX,' -X;...,lix, x}. 
(4.8.5) 
The first vector L'x = b in % is an eigenvector for L because 
Lb = 0. (The other vectors in J are often called generalized ergen- 
vectors.) There are t Jordan chains, and chains built from b € S; have 
length i+ 1. The heuristic diagram in Figure 4.8.2 below illustrates 
Jordan chains built from the basis vectors in 6B for the typical case 
depicted above in Figure 4.8.1. 

a 
sadn Mrilegees Yew 
od «imal 
wy Wo 
ve 
FIGURE 4.8.2: TYPICAL JORDAN CHAINS Jp, BUILT FROM BASIS VECTORS by € N (L) © 
Step 8: Prove that the set 
J = Jp, 
UA, U--- Up, Of all vectors from all 
Jordan chains is linearly independent. The trick is to accumulate the 
vectors in J by level. Let X; be the matrix whose columns are the 
vectors from the top level of chains in S; —e.g., the columns in X,_ 
are the vectors from the top level of chains in S,_1; 
 X,_2 contains 
the top-level vectors from the chains in S,_2; and so on until Xo has 
vectors in So. This means that LX; contain vectors at the second 
highest level in chains from S;, while the columns in L?X; are the 
vectors in the third highest level of chains in S;, and so on. A typical 
case when k = 5 that corresponds to the previous illustration in Figure 
4.8.2 is shown below in Figure 4.8.3. 
ony 
oust 
N(L) 
EAS eee 
FIGURE 4.8.3: TYPICAL ACCUMULATION OF VECTORS IN J BY LEVEL WHEN ky 

4.8 Jordan Form for Nilpotent Matrices 
575 
Step 4: 
Collect vectors by level in matrices B; for i = 0,2,...,k—1, with level 
0 being the vectors on the bottom, level 1 has vectors one level above 
the bottom, and so on with level k —1 being the highest level (refer to 
Figure 4.8.3 above). 
Level 0: 
Bo = [Xo | LX, |L?X> | rol each, erat | Lo Xe 
Level 1: 
B, = [X1 | LX» | vee Le? Xo | L-2X;- 1) 
Level k —2: 
Br_2 = [Xp—2 | 
LX,-_1| 
Levelk —1: 
Br_-1 = [X,—1] 
The columns in Bo are linearly independent (they are the basis vectors 
for N(L)), and LBo = 0. This implies that [Bo | Bj] also has inde- 
pendent columns. Otherwise B,; = BoQ for some matrix Q so that 
LB, = LBoQ = 0, which is impossible because LB, is a submatrix 
of Bo. Repeat this argument for successively higher levels. For exam- 
ple, the columns in [Bo |B,] being independent forces the columns in 
[Bo | Bi | Bz] to be independent—otherwise [Bo | B; | Bz] = [Bo | BiJQ 
for some Q, but L?By = 0 and L?B,; = L(LB;) =0 (because LB; 
is a submatrix of Bo), so 
L*(Bo1.8: 
| Be] =L7(Bo 
|B Q=0 = > L?7B2=0, 
which is impossible because L?By is 
asubmatrix of Bo. Repeating this 
argument for successively higher levels eventually produces the conclu- 
sion that [Bo|Bi|--- |B,-1] has linearly independent columns, and 
thus 
J = Jb, 
UA, U:::U Ip, is an independent set. 
Verify that 7% contains exactly n vectors to conclude that J isa 
basis for F". As observed earlier, each chain % derived from b € S; 
has exactly i+ 1 vectors, so if S; contains 
n,; vectors, then the total 
number of vectors in 7 is 
k—1 
LOLGL 
SE + 1)n;. 
(4.8.6) 
i=0 
The number of vectors in S; is n; = dimM,; — dim M,4, and these 
dimensions are determined from the formula for the rank of a prod- 
uct that says rank (L*t!) = rank (L') — dim R(L') 9 N(L) (recall 
Theorem 4.2.11 on page 436). In other words, if rank (L') =r; then 
ria = 7; —dimM,;, and hence 
nea=dim Mi —dimMiy = 1; —2rig1 + ize: 
(4.8.7) 

576 
Chapter 4 
Vector Spaces 
Use this in (4.8.6) with r, = 0 and ro = n along with a shift of the 
_ index of summation to obtain 
k-1 
k-3 
total = S-(i+1)r Sy (6+ Driga + S064 rite 
i=0 
i=0 
i=0 
k-1 
= 
|ro + 2r1 + SG +1)r;| — |2r1 + » 
airit 
et > —1)r; 
i=2 
1=2 
k-1 
=rot > [(i+1) - 21+ (i-1)]ri = 70 = 1. 
t=2 
Thus the collection J = Jp, U Jb, U-::U Jp, Of all Jordan chains 
constitutes a basis for F". 
Step 5: The proof is completed by observing that each chain J is the basis 
for an invariant subspace of L because it follows from (4.8.5) that if 
be S;, then L*t'x = L(L'x) = Lb=0 
so that if (A) = span(A), 
then 
Ue 
ke Lok 
ee 
key 
564 0.8 Litas Lk Le ee 
Consequently, F" = (Jb,) ® (Jb.) 
B+: 
+ 
@(Ap,), 80 it follows from 
Theorem 4.6.9 on page 536 that 
Ni 
No 
[L]z 
: 
where 
I 5e,) 5 
(4.8.8) 
Nt 
The fact that each N; in (4.8.8) is a nilpotent matrix in standard form 
is immediate because if b; € S;, then 
(Spee Ux x57 
2 Lox) me 
foreomense 
(4.8.9) 
so by Definition 4.5.2 on page 508, 
Mon) 
) = (BOs, [EE | 
-- [ECs ) 
Oe 
= (lls, |x, |---| Za, ) 
Se |i 
: 0 (+1) x (J+1) 
Finally, by virtue of Corollary 4.6.10 on page 537, the nonsingular matrix 
P = [Jo, | Jb. | ++: | Ap,] containing all vectors from all of the Jordan 

4.8 Jordan Form for Nilpotent Matrices 
577 
chains is a similarity transformation such that P~'LP = N is the 
nilpotent matrix in (4.8.2). 
i 
While there are more efficient theoretical ways to develop Theorem 4.8.3, 
the five-step process in the proof has the advantage of providing an algorithm to 
actually construct all components of the theory while simultaneously revealing 
the nuanced structure of L. Below is a summary. 
Summary of Nilpotent Theory 
The important features of the Jordan form for a nilpotent matrix Lyyx7, of index 
k are as follows. 
Ni 
N 
L is similar to a block-diagonal matrix P~!'LP = N = 
: 
Ni 
in which each N; is a nilpotent matrix in standard form as in (4.8.10). 
The number of blocks in N is t = dim N(L) because each block is derived 
from a Jordan chain, and there is one chain for each vector b in the basis 
B in (4.8.4) for N (L). 
The size of a largest block in N is k x k because the longest chains are 
build from b's € S,_1, and each such chain has length k (recall (4.8.5) or 
look at Figure 4.8.2). 
The number of 1x17 blocks in N is ny = ry-1 — 27; + ri41 
in which 
r; =rank (L') because (4.8.7) guarantees that there are exactly n; vec- 
tors in S;, and each 7 x7 block corresponds to a chain built from a basis 
vector in S;. 
The set JZ containing all vectors from all Jordan chains built from the basis 
B in (4.8.4) for N(L) is a basis for F", and [L]7 =N. 
Prxn = [Ji|Jo|---| Je] is the nonsingular matrix containing the Jordan 
chains in the order in which they appear in 7. 
The "structure" of N in (4.8.2) on page 572 is unique up to the arrangement 
of blocks in N in the sense that if L is similar to any block-diagonal matrix 
B = diag (Bj, Bo,...,B;) in which each B; has the form 
OnRes 
0 
0 
0 
0 
~E; 
0 
B; = 
; 
eb 
Hog 
for 
€ a 0, 
(4.8.11) 
0 
0 
0 
Ej 
OO 
0 
OF 
m:xm; 
then s = t = dim N(L), and the number of blocks in B having size 7 x 2 
must be nj = Ti-1 — 27; +7i41, Where r; = rank (L'). This is proven below. 

Proof. 
Suppose that L is similar to both B and N, where B is a block 
matrix having s blocks B,; of the form (4.8.11). This implies that B and 
N are similar, and hence rank (B') = rank (L') = 1; for every nonnegative 
integer i. In particular, index(B) = inder(L) = k. Each time a block B, of 
size m; X m,; is powered, the line of €; 's moves to the next higher diagonal level 
so that 
m—p 
ifp<m; 
rank (B?) = { - 
if p> ae 
Since rp = rank (B?) = )>;_, rank (B?), it follows that if w; is the number of 
i 
xi blocks in B, then 
Tk-1 = Wk, 
Th-2 = We-1 + QW, 
Th-3 = We-2 + 2Wp-1 + Sw, 
and in general, r; = wi41 + 2uwi42 +---+ (kK —i)we. It is straightforward to see 
that ry; — 2r; + ri341 = w;. Consequently, the total number of blocks in B is 
k 
k 
k 
s=) wi = > (iva — 2ri + rig) = Di = dim N (L) =. 
iH 
t=1 
i=1 
i=1 
Note on Uniqueness: The manner in which the 
Jordan theory was developed 
produced Jordan blocks N, in standard nilpotent form. However, having 1's on 
the super diagonals of the Jordan blocks is simply a matter of aesthetics because 
any nonzero value can be forced onto the super diagonal of any N; —see Exercise 
4.8.10. In other words, the fact that 1's appear on the super diagonals of the N;'s 
is artificial and is not relevant to the "structure" of the Jordan form—all that is 
important is the number and sizes of the Jordan blocks (or chains) and not the 
values appearing on the super diagonals of these blocks. 
For 3x3 nilpotent matrices L there are only three possible Jordan forms N 
(up to arrangement of Jordan blocks in N) because the index k of L is either 

4.8 Jordan Form for Nilpotent Matrices 
579 
1, 2, or 3, and the size of the largest block must be k x k. Consequently, for 
each respective index the Jordan form is 
evel 
ge bagi 
C40 
k=1: N=(0 0 0), k=2:N=(0 0 0), k=3:N=(0 0 1). 
00. 6 
0 0 0 
00.0 
Example (Not All Bases for NV (L) Can Generate the Necessary Jordan Chains) 
The development of the Jordan form for a nilpotent matrix relies on the con- 
struction of the Jordan chains in (4.8.5), which in turn depend on the specialized 
basis B for N (L) in (4.8.4) that is built from the nested sequence of subspaces 
M; in (4.8.3). It is a common misconception that any basis for N(L) can do 
the job. To see that this is incorrect, consider the nilpotent matrix 
eas 
1 
Omi 
0 
i= & 0 -2) and its Jordan form 
N= te 
0 
a) 
—-4 
0 
-2 
00 
0 
If P-'LP = N, where P = [x,|x2|x3], then LP = PN implies that 
Lx; = 0, Lx2 = x, and Lx3 = 0. In other words, B = {x,,x3} must be a 
basis for N(L) and x2 must satisfy the equation Lx2 = x;. But not all bases 
for N(L) will allow this. For example, 
={=(2).-()] 
is a basis for N (L) obtained by solving Lx = 0 with straightforward Gaussian 
elimination, but B' cannot work because xi ¢ R(L)—i.e., no vector x4 exists 
such that Lx, = x{. So even though 8' is an otherwise perfectly good basis 
for N(L), it cannot be used to build P or the associated Jordan chains. 
While a proper basis for N (L) is needed to produce the full complement 
of Jordan chains and the similarity matrix P, such a basis is not required if 
only Jordan form N by itself is required because, as the theory shows, N is 
completely determined simply by ranks of powers of L. 
Finding the P Matrix 
While practical work rarely requires the similarity transformation P such that 
P-!LP = N, it nevertheless is worthwhile to have a structured process for 
finding P when it is needed. Assuming that the index k of a nilpotent matrix L 
has been determined (either by finding the first integer k such that rank (L*) = 
0 or powering L until L* = 0), the following procedure returns the complete 
set of Jordan chains and the similarity transformation P. 
1. 
Start with the fact that M,-1 = R (Toca) (Exercise 4.8.5), and deter- 
mine a basis {y1,y2,---,¥q} for R (ice4) (perhaps by using an echelon 
form to find the basic columns in L*~'). 

580 
Chapter 4 
; 
Vector Spaces 
2. Extend {y1,y2,.--,yq} to a basis for My-2 = R(L'-?) 9 N(L) as 
follows. 
5 
> 
Finda 
basis {v1,v2,--.,Vs} for N (LB), where B is a matrix con- 
taining a basis for R(L*~?). The set {Bv1, Bv2,...,Bvs} is a basis 
for M,—2 (see page 439). 
> 
Find the basic columns in [y1|y2|--:|¥q|Bvi|Bvz2|---|Bvs]. Say 
they are {y1,...,¥q,Bva,,---,Bvg,} 
(all of the y;'s are basic be- 
cause they are a leading linearly independent subset). This is a basis 
for M,—2 that contains a basis for M ,—1. In other words, 
Spani = {¥17Y2;+--5 
Yo} 
and 
Spay ={Bvg.; Bvz,,--- , Bvg, 
}: 
3. Repeat the above procedure k —1 times to construct a basis for N (L) 
that is of the form B = Sp_1 U Sp_2 U-++ U So = {b1, bo,..., bz}, where 
Sp_1 U Sp_-g U-+- US; is a basis for M,; for each 1=k—1,k—2,...,0. 
4. 
Build a Jordan chain for each b; € B as follows. If b; € S;, then solve 
the system L'x; = b; for x;, and set J; =[L*x;|L*~'x; |---| Lx; |x;]. 
The desired similarity transformation is Ppyxn», = [Ji|J2|---| Jz]. 
Example 
To find P and N such that P~'LP =N 
is in Jordan form, where 
I< 
dak 
OD 
ees 
Nace ORE gaat 
Nace! a: 
Po 
Po 
14 
oe ea 
L= 
218 
YOu 
gi 
Olt? 
ae 
ee 
ee 
esiaeteg 
121 
poles 
first determine N by computing r; = rank (L") to reveal that r) = 3, re =1, 
and r3 =0. Hence index(L) = 3, and 
the number of 3 x 3 blocks = rg — 2r3 + r4 = 1, 
the number of 2 x 2 blocks = r; — 2r2 + r3 = 1, 
the number of 1 x 1 blocks = rp — 27; + ro = 1. 
Consequently, the Jordan form of L is 
Notice that three Jordan blocks were found, and this agrees with the fact that 
dim N (L) = 6 — rank (L) = 3. Now determine P by following the procedure 
described in the previous example. 

4.8 Jordan Form for Nilpotent Matrices 
581 
1. Since rank (L?) = 1, any nonzero column from L? will be a basis for 
Mz = R(L*), soset S.=¢bi =([L7),,=]| 
° 
2. To extend S2 to a basis for M, = R(L)N N(L), use 
1 
L 
=2 
6 
3 
3 
3 
iN 
5 
—6 
=3 
—3 
4g 
—2 
-1 
0 
0 
0 
0 
B = 
[Lyi] Lao | L.3] = 
2 
1 
0 
=>), 
ub = 
0 
0 
0 
|> 
—-5 
-3 
—1 
—6F 
=—35—3 
-3 
-—2 
-1 
—6 
-—3 
--3 
at 
=i 
and determine a basis for 
N (LB) to be {vi 
= ( 
2| 
(Vo = ( 
0) 
\ 
Reducing [b;|Bv;|Bv2] to echelon form shows that its basic columns 
are in the first and third positions, so {b,,Bv2}. Thus 
5 
7 
ik 
BVo == Doc 
ee 
and 
S2US; = {bi, bz} is a basis for My. 
3 
1 
3. Now extend S:US; 
toa 
basis for Mp = N(L). This time, 
B=I, soa 
basis for N (LB) = N (L) can be computed to be 
2 
—4 
=! 
Vile 
B'S 
V2 
0 
0 
and {Bv1, Bv2, Bv3} = {vi, 
V2, v3}. Reducing [b;|b2|vi|v2| v3] to 
echelon form reveals that its basic columns are in positions one, two, and 
three, so v; is the needed extension vector. Therefore, the complete nested 
basis for N (L) is 
6 
—5 
2 
—6 
7 
—4 
9 
—1 
b, = 
Z ESo, 
bo= 
Sua 
lS Si, 
and 
b3= 
wl So. 
—6 
3 
0 
—6 
1 
0 
4. 
Complete the process by building a Jordan chain for each b; € S; by 
solving L'x; =b; for x; and by setting J; = [L'x;|---|Lx,;|x;]. Since 
X, = e; satisfies L?x, = b;, we have J; = [L'e,|Le;|e,]. 
Solving 

582 
Chapter 4 
Vector Spaces 
Lx = be for x2 yields x2 = (—1,0,2,0,0,0)7, so Jz = [Lx | x2]. 
Finally, Jz = [b3]. Putting these chains together produces 
6 
—6 
P=[Ji|J2|Js]=| 
9 
~6 
—6 
al 
3 
—2 
2 
=5 
-3 
ooocor 
FPwNNA 
ooOonNnNoF 
It can be verified by direct multiplication that P~'LP =N. 
Exercises for section 4.8 
4.8.1. Can the index of an n x n nilpotent matrix ever exceed n? 
4.8.2. Determine all possible Jordan forms N for a 4 x 4 nilpotent matrix. 
4.8.3. Explain why the number of blocks at least of size i x i in the Jordan 
form for a nilpotent matrix is given by rank (L'~') — rank (L'). 
4.8.4. For a nilpotent matrix L of index k, let M; = R(L')MN(L). Prove 
that M; c M;-1 for each i = 0, i oe 
se! k. 
4.8.5. Prove that R(L'~') 4 N(L) = R(L*~') for all nilpotent matrices L 
of index k > 1. In other words, prove M,_; = R (Le *), 
4.8.6. Find the Jordan form N for L = 
such that P~'LP =N. 
(+ 
2 
—1 
0 
=—3 
1 3 
along with P 
—2 
4.8.7. Determine the Jordan form for the 8 x 8 nilpotent matrix 
41 
30 
15 
—54 
-39 
—-19 
9 
6 
2 
=6 
—5 
=o 
L= 
—32 
-—-24 
-13 
—10 
—7 
—2 
—4 
—3 
—2 
17 
12 
6 
NrFwWrOONF 
| 
3 
4 
1 
0 
2 
2 
0 
1 

4.8 Jordan Form for Nilpotent Matrices 
583 
4.8.8. 
4.8.9. 
4.8.10. 
Prove that if 0 4 L € F"*" is such that L? = 0 with rank(L) =r 
then there exists a unitary matrix W such that 
b) 
Bi 
Bo 
W*LW = 
re 
, 
where Bre (4 e 
B, 
0 
in which {o1,02,...,0,} are the nonzero singular values of L. (The 
zero block in W*LW is not present when 2r = n). 
Note: In other words, if one is willing to give up the 1's in the Jordan 
blocks for index-two nilpotent matrices and replace them with singular 
values, then the reduction of L to a first cousin of the Jordan form 
can be accomplished by a unitary similarity, which is naturally stronger 
than generic similarity. 
Suppose that L ¢ F"*" is such that L? = 0. 
(a) Prove that rank (L + L*) = rank (L) + rank (L*). 
(b) Prove that 
R(L+L*) = R(L)+ R(L*). 
Hint: Recall Exercises 2.4.25 and 4.2.23 on pages 198 and 453. 
Prove that if N is the Jordan form for a nilpotent matrix L as de- 
scribed in Theorem 4.8.3 on page 572, then for any set of nonzero scalars 
{€1,€2,...,€}, 
L is also similar to a matrix N of the form 
ey Ni 
0 
tee 
0 
2 
0 
egNo 
::: 
0 
N= 
0 
0 
CE 
exN¢ 
In other words, forcing 1's to be on the super diagonal of the N;'s 
in (4.8.2) to force them to be in standard nilpotent form is artificial 
because any nonzero value can be made to appear on the super diagonal 
of any N;. What's important in the "Jordan structure" of L is the 
number and sizes of the nilpotent Jordan blocks (or chains) and not the 
values appearing on the super diagonals of these blocks. Hint: Consider 
diagonal matrices D; = diag (1, €;,...,€"'~'). 

584 
Chapter 4 
Vector Spaces 
4.9 JORDAN FORM FOR GENERAL MATRICES 
Moving from the Jordan form for a nilpotent matrix to the Jordan form for a 
general square matrix requires extending the notion of "index" as well as that 
of a "Jordan block." 
4.9.1. Definition. The index of an eigenvalue \ for A € F"*" is 
defined to be the index of the matrix (A — AI) as defined on page 554. 
In other words, index() is the smallest positive integer k such that 
any one of the following statements is true. 
rank |(A — AI)*] = rank |(A — AI)*t*). 
e R{(A—AI)*] = R[(A — AD'). 
e 
N{[(A—AI)*] = N[(A—AD**). 
e 
Ri(A-AD A] ON[(A-AD*] = 
e 
FX =R{(A—AD*| 
@N[(A— ADS]. 
When A is singular, 
\=0¢€o0(A), so inder(A) = index(A = 0). It 
is understood that index(w) =0 if and only if wp ¢oa(A). 
For nilpotent matrices, a "Jordan block" is a nilpotent matrix N in stan- 
dard form as in (4.8.10) on page 576, but for non-nilpotent matrices, the defini- 
tion of a Jordan block must be extended to allow nonzero eigenvalues to be on 
the diagonal. 
4.9.2. Definition. A Jordan block for 
\ € o(A) is a square matrix of 
the form 
Wok 
BOA) =A+N=| 
© 
: 
A 
A Jordan segment for \ € 0 (A) is a block-diagonal matrix 
Bi()) 
Ty 
Ba(A) 
BAA) 
containing t Jordan blocks, where t = dim N (A — AI). 

y Some texts refer to J as the Jordan canonical form or the Jordan normal form. Marie En- 
nemond Camille Jordan (1838-1922) discussed the idea of such a form over a finite field in 
1870 in Traité des substitutions et des équations algebraique, and it earned him the Poncelet 
Prize of the Académie des Science. But Jordan may not have been the first to realize the con- 
cept. It has been reported that the German mathematician Karl Theodor Wilhelm Weierstrass 
(1815-1897) had previously formulated results along similar lines. However, Weierstrass did 
not publish his ideas because he was fanatical about rigor, and he would not release his work 
until he was sure it was on a firm mathematical foundation. Weierstrass once said that "a 
mathematician who is not also something of a poet will never be a perfect mathematician." 

586 
Chapter 4 
; 
f 
- 
Vector Spaces 
Proof. 
Digest the distinct eigenvalues o (A) = {A1,A2,--.,As} one at a time 
with the core-nilpotent decomposition from Theorem 4.7.14 on page 556. Suppose 
that index(A,) =k, and ty = dim N(A — iI), and let X, be a nonsingular 
matrix such that 
a 
L; 
0 
xXy(A-—ADX = ( 
A an) 
(4.9.3) 
in which L is nilpotent of index k; and Cy is nonsingular (it doesn't matter 
whether C, or Ly is listed first, so, for the sake of convenience, the nilpotent 
block is listed first). The results for nilpotent matrices in Theorem 4.8.3 on page 
572 ensure that there is a nonsingular matrix Y, such that 
Ni(A1) 
VecuryY? = N(A1) — ( er 
Nz, (A1) 
is a block-diagonal matrix possessing the following features. 
01 
> 
Every block N,(A,) = 
iy. e : 
is in standard nilpotent form. 
0 
> 
There are tj = dim N (L,) = dim N (A — AjI) such blocks. 
> 
The number of i xi blocks N,(A;1) is 
nj(A1) = rank (Li-*) — 2rank (Li) + rank (L{*?). 
Since C, is nonsingular, rank (L7) = rank ((A — A11)") — rank (C,) for each 
p=1,2,..., and thus the number of 7 xi blocks N,(Ai) in N(Aj) is 
ni(A1) = Ti-1(A4) — 2ri(A1) + ri+1(A1), 
where 
ri(A1) = rank [(A = X11)' : 
Now, Qi=xX: (4 a is nonsingular, and Qy7*(A — A,1)Q; = (ga C ) we 
equivalently, 
. 
ea 
N(A 
Qz1AQ; =( eae 
a 
jie ion 
), 
(4.9.4) 
0 
Ci +AiI1 
0 
Aj 
Fy 
= 
Bi (1) 
in which J(A1) = N(\i)+A,1 = ( e 
is a Jordan segment contain- 
B:, (A1) 
here 
ing t; Jordan blocks B,(A1) = N,(Ai) + AVI = 
i se 
. Because the 
- 
1 
. 
. 
. 
a1 
Jordan structure of J(A;) is inherited from that of L,, the Jordan structures 

.4.9 Jordan Form for General Matrices 
587 
of the segment J(A,) and Ly are identical. Now consider the second eigenvalue 
Ag in a (A) = {Ai,2,...,As}. It follows that 
oO (A a AiI) = {0, (A2 os A1), (A3 = ADs sey (As = A1)}, 
and since o (Li) = 0, the core-nilpotent decomposition in (4.9.3) shows that 
a (Cy) = {(A2 — Ai), (Ag — An), -- «5 (As — A}. 
Consequently, o (Aq = Cy + AyD) = {Xo,A3,..., As} in (4.9.4). This means that 
the core-nilpotent reduction leading to (4.9.4) can be repeated on A, — AI to 
produce a nonsingular matrix Q» such that 
J(A2) 
0 
0 
Ag 
Qz1A1Q2 = ( 
) 
where 
o (Az) = {X3, Na, S¢ Aa ty 
(4.9.5) 
and where J(A2) = diag (Bi(A2), Bo(A2),..., 
Bz, (A2)) is a Jordan segment 
det 
composed of t2 = dim N(A — A2I) Jordan blocks B,(A2) = 
1 
A2 
> 
The number of 7 xi Jordan blocks in segment J(A2) is 
ni (Az) = ri-1(A2) a 2ri(A2) =F ri+i(A2), where ri(A2) = rank ((A = d2I)*). 
If Po =Q, (3 ah ih then P» is a nonsingular matrix such that 
TOL 
ONO 
PAP» = ( 6 a) 0), where 
o (Ag) = {Asz, A4,.--,As}- 
Repeating this process until all eigenvalues in g (A) have been used results in a 
nonsingular matrix P, such that P;|AP, = J = diag (J(A1), J(A2),..-, J(As)) 
in which each J(\;) is a Jordan segment contains t; = dim N (A — A;I) Jordan 
blocks. Proving the uniqueness of the structure for the Jordan form is virtually 
identical to that given for Theorem 4.8.4 on page 578. 
& 
Recall from Theorem 3.2.9 on page 308 that A € F"*" 
is diagonaliz- 
able if and only if every eigenvalue of A is semisimple—i.e., if and only if 
alg mult, (A) = geo mult, (A) for each 
4 € o (A). The Jordan theory presents 
yet another way to characterize diagonalizability by realizing that the Jordan 
form J in (4.9.1) is a diagonal matrix if and only if the size of each Jordan 
block in J is 1 x 1, thus providing the following corollary. 

"Tas eres PLE. 
A 
- 
wes 
te 
Bigs. 
0-0: 
"a8 
Mika 
el a Mie pues 
"di 
Vo 
ROP 
eee 
2 
To find the Jordan form for A = 
ae Peder 
ae Ad 
| first compute 
6 6 
6O~ 
Oe oe 
Let Spe hots 
Sas 
the eigenvalues (which may be the hardest part) to reveal two distinct eigen- 
values \; = 2 and Ag = —1. Hence there are two Jordan segments in the 
Jordan form J = begs an Compute ranks rj(2) = rank ((A —2I)') and 
ri(—1) = rank ((A +1)*) until rg(x) =rr4i(*) to obtain 
r1(2)=rank(A—21) 
=4, 
1(-1)=rank(A+I) 
=4, 
ro(2) = rank ((A —21)?) =3, — ro(—1) = rank((A +1)?) =4, 
r3(2) = rank ((A — 2I)) = 2, 
r4(2) = rank ((A — 21)4) =2. 
Thus k; = inder(A1) = 3 and kg = inder(Az2) = 1. This means that the largest 
Jordan block in J(2) is 3 x 3, while the largest Jordan block in J(—1) is 
1x 1 
so that J(—1) is a diagonal matrix (the associated eigenvalue is semisimple 
whenever this happens). Furthermore, 
n3(2) 
= 1re(2) — 2r3(2) + r4(2) = 1 
=> 
one 3 x 3 block in J(2), 
n2(2) 
=11(2) — 2re(2) + r3(2) = 0 
= > no 2x 2 blocks in J(2), 
mi(2) 
= 70(2) — 2ri(2) + re(2) = 1 
= > one 1 x 1 block in J(2), 
m(—1) = ro(—1) = 2ri(—1) +ro(-1) = 2 = > two1x 
1 blocks in J(—1), 
0 2 
= 
0 
Therefore, J(2) = 
0 
and J(—1) = ( 
F 
) 
so that 
-1 
0 
Ju Ge 

4.9 Jordan Form for General Matrices 
589 
Example (Computational Difficulties) 
The previous example suggests that determining the Jordan form for Anyn is 
straightforward, and perhaps relatively easy. In theory, it is—just find o (A), 
and calculate some ranks. But in practice both jobs are numerically difficult. 
To begin with, the rank of a matrix is a discontinuous function of its entries, 
and rank computed with floating-point arithmetic can vary with the algorithm 
used and is often different than rank computed with exact arithmetic (recall 
Exercise 2.8.7 on page 245). Furthermore, computing higher-index eigenvalues 
with floating-point arithmetic is fraught with peril. To see why, consider 
Ome 
L(e) = 
t 
me: F 
whose characteristic equation is 
A" —e=0. 
c 
97 
nxn 
For € = 0, zero is the only eigenvalue (and it has index n), but for all € > 0, 
there are n distinct eigenvalues given by €!/"e?*™'/" for k = 0,1,...,n—1, 
(and each has index 1). For example, if n = 32, and if € changes from 0 
to 10~'®, then the magnitude of the eigenvalues of L(e) changes from 0 to 
10—!/2 = .316, which is substantial for such a small perturbation. Sensitivities 
of this kind present significant problems for floating-point algorithms. In addition 
to showing that high-index eigenvalues are sensitive to small perturbations, this 
example also shows that the Jordan structure is highly discontinuous. L(0) is 
in Jordan form, and there is just one Jordan block of size n, but for all e 40, 
the Jordan form of L(e) is a diagonal matrix—i.e., there are n Jordan blocks of 
size 1 x 1. Lest you think that this example somehow is an isolated case, recall 
from Exercise 3.2.19 on page 319 that every matrix in F"*" is arbitrarily close 
to a diagonalizable matrix. 
e 
All of these observations make it clear that it is hard to have faith in a 
Jordan form that has been computed with floating-point arithmetic. Conse- 
quently, numerical computation of Jordan forms using floating-point arith- 
metic is generally avoided. 
Example 
In a metaphorical sense the Jordan form of A € 
contains the complete 
DNA for A —just about everything there is to know about A is contained in 
the Jordan components. For example, if the Jordan form for A is 
ere 

590 
Chapter 4 
. 
Vector Spaces 
then it can be concluded that 
> 
Aoyg has three distinct eigenvalues, namely o (A) = {4, 3, 2}; 
alg mult(4) =5, alg mult(3) = 2, and alg mult (2) = 2; 
geo mult(4) =2, geo mult(3)=1, and geo mult (2) = 2; 
index(4) = 3, index(3) = 2, and index(2) = 1; 
\ =2 is a semisimple eigenvalue, so, while A is not diagonalizable, part of 
it is; i.e., the restriction Ay N(A~21) 
is a diagonalizable linear operator. 
VOOM: 
i 
ae 
Of course, if both P and J are known, then A can be completely reconstructed 
from A = PJP~!, 
but the point of this example is that only J is needed to 
reveal the complete eigen structure along with other similarity invariants such 
as the determinant and trace. 
Jordan Chains and the P Matrix 
Constructing the set of Jordan chains that makes up the columns in P such 
that A = PJP! is nearly identical to the process for building Jordan chains 
for nilpotent matrices as described in the proof of Theorem 4.8.3 on page 572. 
But for nilpotent matrices there is only one eigenvalue (A = 0) whereas in the 
general case the entire spectrum o (A) = {A1,A2,.-.,As} must be considered, 
and a complete set of Jordan chains for each eigenvalue is required. For each 
A €a(A), the subspaces M; defined in (4.8.3) on page 573 are replaced by 
M, = R((A—AI)') 
AN(A—AI) 
for i=0,1,...,h, 
(4.9.6) 
where k = index(A). Analogous to the nilpotent theory, it easily follows that 
. 
= Mk C Mg-1 © +++ 
C My = N(A—AI). And since (A — AD) 
A—a1)*) 
is a nilpotent linear operator of index k, the same process used to build Jordan 
chains for nilpotent matrices can be used to build Jordan chains for a general 
eigenvalue A. The process adapted to the general case is as follows. 
e 
Foreach 
A€a(Anxn), set M; = R((A —Al)') 
A N(A—AI) for 
i=k—1,k—2,...,0, where k = index(4). 
e 
Construct a basis B for N (A -— Al). 
> Starting with any basis S,_; for Mx_ , sequentially extend S,_1 with 
sets Sp2, Sp—3,..., So such that 
Sp-1 
is a basis for 
May_1, 
Sp—1 U Sp_2 
is a basis for 
My_9, 
Sp-1 
USk-2 
US,_3 
is a basis for 
Mg,_3, 
(4.9.7) 
Continue until a basis B = Sy_1 USp_2 U+++U Sp = {b, bg,... , bz} for 
Mo = N (A — Al) is obtained. 
e 
Build a Jordan chain from each eigenvector b, € B. 

4.9 Jordan Form for General Matrices 
591 
> Solve (A — AI)'x, = b, (anecessarily consistent system) for x,, and con- 
struct a Jordan chain from b, by setting 
P, = | 
(A-Al)'x,| (AAD 
1x, |---| (AAD) x, |] Genre 
> Each such P, corresponds to one Jordan block B,(\) in the Jordan seg- 
ment J(A) associated with X. 
> The first column in P, is an eigenvector associated with \, and subsequent 
columns are sometimes called generalized eigenvectors. 
e 
If all such P,'s for a given \; € o(A) are put in a matrix Pj, and if 
P =|P,|P2|---|P;], then P is a nonsingular matrix such that 
P-'AP = J = diag (J(A1), J(A2),...,J(As)) is in Jordan form as described 
in Theorem 4.9.3 on page 585. 
Example (Caution!) 
As the example on page 588 shows, determining a Jordan block in a Jordan 
form is theoretically straightforward—just find an eigenvalue and calculate some 
ranks. Determining the corresponding Jordan chain in the P matrix seems more 
complicated. But need it be so if the Jordan block is already known? For ex- 
oe 
ample, consider a typical px p Jordan block B = 
o ka file and let 
yx 
P, =[x,xX2-:: x,]| be the part of 
P =[---|P,|---] that corresponds to the 
position of B in the Jordan form J 
= PAP™! so that AP, = P,B. Equating 
columns on both sides of 
A 1 
Aq exoncee Xe jeer Xi Nene Xs | 
shows that there is not much wiggle room for determining the x,;'s because 
Ax, = Axi 
=> 
x, is aneigenvector 
=> 
(A-—AI)x, 
=0, 
Axo =X1+AxX. 
==> 
(A-ADx=x1 
=> 
(A—AI)'x.=0, 
— 
(A—AI)*x3=0, 
Ax3 = Xo + Ax3 
=> 
(A-AI)x3 =x2 
AXp =Xp-1tAXp 
=> 
(A-ADXp=%-1 
= 
(A — AI)' x, = 0. 
Because 
{X1,X2,.-- see, must be linearly independent, 
it is necessary that 
x; € N[(A—Al)'] 
but x: ¢ N[(A — AI)*~*]. Such vectors are often called 
generalized eigenvectors of grade i. It is tempting to conclude from these ob- 
servations that finding {x1,xX2,...,X } should be relatively easy—just find any 

592 
Chapter 4 
; 
Vector Spaces 
eigenvector x;, and then successively solve the systems (A — AI) xi41 = x; for 
i =1,2,...,p—1 
to obtain the other x;'s. This would completely circumvent 
the need to use the nested subspaces M, in (4.9.6) on page 590—in fact, there 
are sources that suggest this as a viable method. 
However, there is no free lunch here—the procedure fails if the eigenvector 
x, is not a "special" one. In other words, not every basis for N(A — AI) can 
be used to build Jordan chains associated with an eigenvalue \ € a (A). To see 
this, consider 
-) with o(A) 
= {1}, index(1) 
= 2, 
(4.9.8) 
—1 
> I 
a 
Amo 
=) 
i) 
teh 
0 
and Jordan form J = é 
: a 
If P = [x,|x2|x3] is a nonsingular ma- 
at 
trix such that P~'AP = J, then {x;,x2} must be a Jordan chain such that 
(A —I)x; =0 and (A —I)x2 = x1, while x3 is another eigenvector not de- 
pendent on x,. If you try to build the Jordan chain {x;,x2} by using standard 
elimination to solve (A —I)x = 0 and generate a basis 
g=fo=( 4), m=(%)) me Nana, 
then you must either take x; = b; or x; = bg, and then you must solve 
(A —I)x2 = b; or (A —I)x2 = beg to determine x2. But in each case this is 
impossible because these two systems are both inconsistent—i.e., by ¢ R(A —I) 
and b3 ¢ R(A—I). So, even though B is a legitimate basis for N(A — 11), 
B is not suitable for building Jordan chains. You are asked in Exercise 4.9.2 to 
find the "special" basis for N(A — 11) that will yield a proper Jordan chain. 
Example (Direct Sum Decomposition into Generalized Eigenspaces) 
The Jordan decomposition of A € F"*" provides a natural direct-sum decompo- 
sition of F" into invariant subspaces. If P~'AP = J = diag (Bi, Bo,..., Bm) 
is in Jordan form, where the B's are the complete set of Jordan blocks, and 
ie Seat [A | 
Jo|--+| el is a matrix of associated Jordan chains, then, as ex- 
plained on page 539, each generalized eigenspace (Ses (the span of 7;) is an 
invariant subspace of A with 
— Nahr 
Ii 
Al(5,) 
The nonsingularity of P ensures that F°= 7,0 .2O@::-O/7, m-* 

4.9 Jordan Form for General Matrices 
593 
Exercises for section 4.9 
eI 
are O28 eee as eee Be ey 
ER Fe 
RRMA 
4.9.1. 
4.9.2. 
4.9.3. 
4.9.4. 
4.9.5. 
4.9.6. 
4.9.7. 
Find the Jordan form of the following matrix whose distinct eigenvalues 
are o(A) = {0,—1,1}. Don't be frightened by the size of A. 
—4 
-5 
-3 
1 
-—2 
0 
rt 
—2 
4 
i 
Ol 
3 
(Wy 
ail 
2 
OF 
0 
0 
0 
0 
0 
0 
Ap 
Sail 
1 
2 
—4 
2 
0 
-3 
1 
—8 
-14 
—-5 
1 
56 
0 
1 
—4 
4 
7 
4 
-3 
By 
Soll 
Sa) 
4 
2 
—-2 
—2 
5 
=3 
0 
4 
-1 
6 
7 
3 
0) 
2 
0 
0 
3 
3.0 
i 
Let' A. = & 1 -2) be the matrix in (4.9.8) on page 592. Use the 
—4 
0 
-1 
process described on page 590 to construct a nonsingular matrix P such 
that P~'AP = J is in Jordan form. 
Use the Jordan form to explain why index(A) < alg mult(A) for each 
A € a {Anxn)- 
For 4 € a (A), let M; = R((A—AI)')N N(A—AI) for 
0<i<k, 
where index(X) = k. 
(a) Prove that 
0 = M, C Mg_1 C---C Mp = N(A-—AI). 
(b) Explain why (A — AI)'x, = by, is a consistent system, where 
b, € S; in which b, and S; are defined in (4.9.7) on page 590. 
Does the result of Exercise 4.8.5 on page 582 extend to non-nilpotent 
matrices? That is, if A € o0(A) with index(A) =k > 1, is it true that 
Mine i= R((A - Wey 
Use the Jordan form to explain why index(X) = 1 if and only if A isa 
semisimple eigenvalue. 
Prove that A € F"*" is similar to A? , and then give an example of a 
complex matrix C that is not similar to C*. 
1 
1 
Hint: Consider the reversal matric R = 
: 
obtained by re- 
4: 
versing the order of the rows (or columns) of the identity matrix I. 

594 
Chapter 4 
. 
Vector Spaces 
4.9.8. 
4.9.9. 
4.9.10. 
4.9.11. 
4.9.12. 
4.9.13. 
4.9.14. 
For every positive integer n, explain why there does not exist a real or 
complex matrix A of size 2n x 2n such that I— A is singular with 
rank (I-A) =n+1 
and rank((I— AJ?) =1. 
Cayley-Hamilton Theorem. The Cayley-Hamilton theorem states that 
every A €F"*" satisfies its own characteristic equation (see Theorem 
3.2.13 on page 312). Use the Jordan form to prove this. 
Prove that there exists left-hand and right-hand eigenvectors y* and x 
for \ € o(Anxn) such that y*x = 0 if and only if alg mult, (A) > 1. 
In particular, this shows that y*x # 0 for all left and right-hand eigen- 
vectors whenever A is a simple eigenvalue. 
Prove that there exists left-hand and right-hand eigenvectors y* and x 
for 1 € 0(Anxn) such that y*x #0 
if and only if the Jordan form for 
A contains at least one 1 x 1 Jordan block J, = [Ajixi. 
Explain why we cannot simply negate the statement in Exercise 4.9.10 
to produce the result in Exercise 4.9.11. 
If \ is a simple eigenvalue for A € C"*", then Exercise 4.9.10 shows 
that no pair of left-hand and right-hand eigenvectors for 
can be or- 
thogonal. However, they can be arbitrarily close to being orthogonal. 
Demonstrate this by constructing a matrix with a simple eigenvalue 
such that the magnitude of the cosine of the angle between any associ- 
ated left-hand and right-hand eigenvectors is arbitrarily small. 
Prove that if \ is an eigenvalue of A € F"™" such that index(\) =k 
and alg mult, (A) = m, then dim N[(A — AI)*] = m. Is it also true 
that dim N[(A — AI)™] = m? 

4.10 Functions of Nondiagonalizable Matrices 
595 
4.10 
FUNCTIONS OF NONDIAGONALIZABLE MATRICES 
The development of functions of nondiagonalizable matrices parallels the devel- 
opment for functions of diagonal matrices that was presented in §3.3 on page 323 
except that the Jordan form is used in place of the diagonal matrix of eigenval- 
ues. Recall from Definition 3.3.1 on page 324 that if A € F"*" is diagonalizable, 
say 
A= PDP™!, where D = diag (AjI, AoI,...,A51), and if f(A;) exists for 
each A; € 0 (A), then f(A) is defined to be 
ik pS 
f(A)=Pf(D)P-!=P 
Po!, 
f(As)I 
The Jordan form decomposition A = PJP~! 
from Theorem 4.9.3 on page 
585 provides a straightforward generalization to nondiagonalizable matrices by 
defining f(A) = Pf(J)P~!, where J is the Jordan form for A. However, there 
are a couple of wrinkles that need to be ironed out. First, f(J) must be defined 
(this is not as obvious as f(D) is for diagonal matrices). And after this is taken 
care of assurance must be given that Pf(J)P~! is a uniquely defined matrix. 
This also is not clear because, as pointed out in Theorem 4.9.3, the similarity 
transformation P is not unique. 
To make sense of f(J), assume throughout that 
A = PJP~!ecF"*" with 
ao (A) = {\j,2,.--,As} and where J = diag (J(A1),J(A2),...,J(As)) 
is the 
Jordan form (4.9.1) on page 585 in which each segment J(A;) is a block-diagonal 
matrix containing one or more Jordan blocks. That is, 
Bi(j) 
ae 
Bo(A;) 
: 
J(A;) = 
- 
, 
where 
B,(A;) = 
| 
: 
7 
Be, (Aj) 
% 
f(3Q.) 
7 
Defining f(J) = 
i 
in which f(J(\;)) = 
f(B.(s)) 
f(JOs)) 
requires defining F(B,(A;)). To keep the notation from getting out of hand, 
APL 
let B, = 
ees 
denote a generic k x k Jordan block, and suppose that 
d 
f:F—F 
has a Taylor series expansion about \. That is, for some r > 0, 
FEO) 
PLGA) 
fle) = fA)+PO)2-At 
GR 
rye Ot 
for eA <r. 
The series representation (3.3.9) on page 325 for a matrix function suggests that 
f(B,) should be defined as 
ae (A) 
f(B,) = FAT+ £0). —) + 5 @, — 
ay? + Ge. - 8+ 

/ 
. 
. 
10 
|i 
:% de 
a 
ee 
a? 
hs 
* 
i 
' 
Sh 
ce 
ine 
vt 
nT 
et ag E winitee LE CALL gale 
' a 
| 
Every Jordan form is a block-diagonal matrix J = 
r B. 
composed 
of Jordan blocks B,, so (4.10.2) suggests defining f(J) = 
ie 
as long 
as a sufficient number of derivatives of f exist on o(A). More precisely, if 
the size of the largest Jordan block associated with \ € o(A) is k (ie., if 
index (A) =k), then f(A), f(A), -.., f*-)(A) must exist in order for f(J) to 
make sense. This leads to the following formal definition. 
: 

The above definition of f(A) is inadequate unless it can be proven that 
_ the expression in (4.10.3) produces a uniquely defined matrix. Theorem 4.10.3 
on page 598 will not only establish this, but it will also provide an alternate ex- 
pression for f(A) that involves neither the Jordan form J nor the transforming 
matrix P. This necessitates generalizing the notion of a spectral projector pro- 
duced by extending the developments in Theorem 4.7.6 on page 549. Partition 
J into its s Jordan segments as described on page 584, and partition P and 
P-! conformably as 
J(A1) 
Qi 
P=(Pi|---|P.), J= 
oh 
bond 
P=) =| ae 
J(As) 
Q, 
Set G; = P,;Q,;, and observe that if k; = index(A;), then G; is the pro- 
jector onto N{(A — \;1)*] along R{(A — A;1)*]. To see this, notice that 
L; = J(A;) — AiI is nilpotent of index k;, but J(A;) —A;I is nonsingular when 
i# j, so 
J(A1) — Agl 
(A= Xd) =P(J—-XDP =P 
Poe: 
P-! 
(4.10.4) 
TOs 

598 
Chapter 4 
: 
Vector Spaces 
is a core-nilpotent decomposition as described on page 556 (reordering the eigen- 
values can put the nilpotent block L; on the bottom to realize the form in 
(4.7.24)). Corollary 4.7.15 on page 557 ensures that P;Q; = G; is the projector 
onto N[(A —Aj,I)*] along R[(A —A;I)*], and this is true for all similarity 
transformations that reduce A to J. If A happens to be diagonalizable, then 
k; =1 for each i, and the matrices G; = P;Q; are the spectral projectors in 
Theorem 4.7.6 on page 549. For this reason there is no ambiguity in continuing to 
use the G,; notation, and to continue to refer to the G;'s as spectral projectors. 
e 
The thing to remember is that in the diagonalizable case, G; projects onto 
the eigenspace associated with \;, and in the nondiagonalizable case G; 
projects onto the generalized eigenspace associated with j. 
4.10.3. Theorem. (Spectral Resolution Theorem) Let A € F"*" with 
o (A) = {A1,A2;---, As} such that kj = index(A;), and let G; = P;Q; 
be the spectral projector onto N[(A—jI)*] along R[(A—A,I)*]. If 
f:F-—F 
is a function such that f(A;), f/(Ai), ---. FY (Aj) exist 
for each A; € 0 (A), then 
{A= < y POD 4 — AI) Gj. 
(4.10.5) 
See 
Moreover, the spectral projectors G; have the following properties. 
e 
G@,+Go+-:--+G,=1 
(4.10.6) 
e 
G,G;=0 when i Fj. 
(4.10.7) 
e 
N; =(A—A,DG; = G;(A—jI) is nilpotent of index k;. (4.10.8) 
e 
dim N{(A — A,I)*] = alg mult, (A;) = rank (G;). 
(4.10.9) 
¢ 
7-1 Gi is the projector onto @f_,N{(A — A,I)*] 
along N_,R[(A — \1)*}. 
(4.10.10) 
All results reduce to those in Theorem 4.7.6 when A is diagonalizable. 
Proof. 
Start with 
f (JQ) 
. 
f(A) =Pf(J)P-1 =P 
'el 
Pt =) 
> Pif(I(A)) 
Qi, 
f(J(s)) 
i=l 
(4.10.11) 

4.10 Functions of Nondiagonalizable Matrices 
599 
where f(I(Ai)) — 
f(B. (As) 
, in which the B,(A;)'s are the Jordan blocks 
associated with A;. Let L; = J(A;) — \;I, and express (4.10.2) as 
Ff!) 
ah 
Be 
"\\T.. 
2 
Ame OS iin 
f (JO) = f(A)I+ f(a) 
Li + ji Bae 
ote ee i 
so that (4.10.11) becomes 
: 
eatin. 
F(A) = SOP. F(T) Q: = 
Ppa Gi. 
(4.10.12) 
i=1 
i=1 j=0 
The terms P,L! Q; can be simplified by noticing that 
Qi 
0 
P?°P=I => Q,P;= 'a ig ry = PSG 
1s | PQ, | ona 
Q, 
0 
so this together with (4.10.4) yields 
(J(A1) — Ail)? 
(A —X,1)'G; =P 
Li 
P-'G, = P\L/Q,. 
(J(As) — AaT)? 
Thus (4.10.12) becomes 
s 
ki,-l 
@) 
f(A)= 
u a (A — d,1)7 Gy. 
(4.10.13) 
J: 
This expression is independent of which similarity P is used to reduce A to J, 
so this proves that f(A) is uniquely defined. Property (4.10.6) follows by using 
(4.10.13) with the function f(z) = 1. Property (4.10.7) is a consequence of 
Tat 
a9, 
0 ifi4j. 
To prove (4.10.8), establish that (A — A;I)G; = G;(A — A;I) by noting that 
(4.10.14) implies P-1G,; = (0---Q;---0)' and G,P = (0---P;---0). This 
together with (4.10.4) provides (A — A,I)G; = P;L;Q; = G;(A — d;I). Now, 
N? = (P,L,Q,)) =P;L/Q; 
for 7 =1,2,3,..., 
and thus N; is nilpotent of index k; because, as previously observed on page 
597, L,; is nilpotent of index k;. Property (4.10.9) was established in Exercise 
4.9.14 on page 594, and statement (4.10.10) is a consequence of Exercise 4.7.24 
on page 565. 
abe 
PC) Pre { 
(4.10.14) 

600 
Example 
Example 
Chapter 4 
' 
Vector Spaces 
A coordinate-free version of the representation for f(A) in (4.10.3) on page 
597 results by separating the first-order terms in (4.10.5) on page 598 from the 
higher-order terms to write 
6 
pe 
oa 'Sages 
CpN 
tal 
f(A) = SI f pout DEE Nie 
(4.10.15) 
t=1 
where N; = (A — A,I)G; = Gi(A — A;I). Using (4.10.15) together with the 
identity function f(z) = z produces a coordinate-free version of the Jordan 
decomposition of A itself as A = beer [AiGi + Ni]. This is the extension of 
the spectral theorem for diagonalizable matrices on page 549 to nondiagonaliz- 
able matrices. Another version of (4.10.5) results from lumping all matrices in 
(4.10.15) into a single matrix to write 
Ce 
et! 
j 
_ 
il TCs 
ye We FMA Liz, 
where 
Lij = a 
(4.10.16) 
i=1 j=0 
The Z;;'s are often called the component matrices or the constituent matrices. 
6 
2 
8 
Problem: Describe f(A) for functions f defined at A = (-2 2 2), 
0 
O 
2 
Solution: A is block triangular, so it is easy to see that A; = 2 and Ay = 4 
are the two distinct eigenvalues with index(\;) = 1 and index(A2) = 2. Thus 
f(A) exists for all functions such that f(2), f(4), and f'(4) exist, in which case 
F(A) = f(2)Gi + f(4)Ge + f"(4)(A — 41)Go. 
The spectral projectors could be computed directly, but things are easier if some 
judicious choices of f are made. For example, 
feyel 
=x 
T = (Ata Cane 
Gy = (A —41)?/4, 
ei, Cee eae ey 
ame 4 Pi Ges 
Knowing the spectral projectors means that any function defined at A can be 
evaluated. For example, if f(z) = z!/?, then 
H(A) = VB 
= Vis 
+ VIG, 
+ U/aVH(A —a0)G 
= 3 (<1 : si | 
0 0 
This technique illustrated above is rather ad hoc, but it always works if a suf- 
ficient number of appropriate functions are used. For example, using f(z) = z? 

4.10 Functions of Nondiagonalizable Matrices 
601 
for p= 0,1,2,... generates a system of equations that produces the component 
matrices Z;; given in (4.10.16) because 
for fle Lie La 
Se Zan, 
for f(z) =z: 
A=DYAZo+>d 
Za, 
for f(z) = a 
A? = >> A? Zio ae So 2GZir ae + 2Z 52, 
This can be considered as a generalized Vandermonde linear system (page 60) 
Zio 
Bec, 
Sail 
: 
: 
I 
Al 
eee 
Ne 
a 
Sn 
it 
Zs30 
A 
Z 
2 
Ae 
Rey 
Acur 2aae 
ed 
Dhell 
tees) 
a= 
KR 
ire 
Z., 
: 
Z21 
that can be solved for the Z;;'s. Other sets of polynomials such as 
41, ea)! 
An 
4 (ee) 
(Ay) Pe (A) 
will generate other linear systems that yield solutions for the Z;;'s. 
. Series Representations for Matrix Functions 
It was suggested in the discussion of matrix functions for diagonalizable matrices 
on page 325 that an infinite series representation for a function f can be used 
to define f(A), even when A is not diagonalizable. The next theorem makes 
this rigorous. 
4.10.4. Theorem. If pe, c;(z — 20)? converges to f(z) at each point 
inside a circle |z— z| =r, and if that A ¢ F""" is a matrix such that 
|\\; — 20| < 7 for each eigenvalue \; € o (A), then ee cj(A — zl) 
converges to f(A). 
Proof. 
If P~!AP =J 
is in Jordan form, then it is straightforward to see that 
ae cj(A — 201)! converges if and only if 
aOR = zol)!)P = ae — 21))P = Lal — zI) 
io 
Ss C5 (By = zol)? 
j=0 

602 
Chapter 4 
Vector Spaces 
ok 
converges, where B, = ( 
8 at )- 
\MI+N isa kxk generic Jordan block 
d 
apd 
with N = 
'.. 
|. It therefore suffices to prove that p gt c;(B, — 21)' 
0 
. 
converges to f(B,). A theorem from analysis states that if pret €;(z — 2)" 
converges to f(z) when |z—zo| <1, then the series can be differentiated term 
by term to yield a series that converges to derivative of f at points inside the 
circle of convergence. This means that, for each 1 = 0,1,2,..., 
Ve = Sis, (1) 
(z—z)2* 
when 
|z—-2| <r. 
(4.10.17) 
i 
| 
a: 
j=0 
Using f(z) = 2 in (4.10.1) on page 596 yields 
(B,— 21)? = a-aytt (Z)aayong+(, 7 JQ zy C-UNe 
This together with (4.10.17) produces 
CO 
[ee 
Co 
S| ¢j(Bx — 201) 
> ej(A 
— 20)9 | 
1+ Ya(Pa--) 
sol 
bal 
70 
9=0 
4=0 
= 
j 
a 
a (,2 )O-2) ates 
j=0 
fia-)) 
= f(A)I+ f(A)N4+-+:+ Te 
{(A)N®* = f(B.). B 
Example (Hermite Interpolation Polynomial and f(A) ) 
Corollary 4.7.10 on page 552 says that if A € F"*" 
is diagonalizable, and if 
f(A) exists, then f(A) is expressible as some polynomial in A, and Exercise 
3.3.10 on page 336 suggests how to use the Cayley-Hamilton theorem to extend 
this result to nondiagonalizable matrices. However, better tools are now available 
to make this fact more evident by explicitly exhibiting a polynomial p(x) such 
that f(A) = p(A). 
Suppose that o(A) = {Aj,A2,...,As} with index(\;) = kj. The trick is 
to find a polynomial p(z) such that for each i= 1,2,...,8, 
PA) =f), 
POA)=f'Os), 
.-, pV) = Ff") 
(4.10.18) 
because if such a polynomial exists, then (4.10.5) on page 598 guarantees that 
p alts 
ee 
pA) aes 
(A-ATPG,=S> So NaN = Ni)? 
G; = f(A). 
i=1 j=0 
fuel, HO 
—

4.10 Functions of Nondiagonalizable Matrices 
603 
Since there are k = )>;_, k; equations in (4.10.18) to be satisfied, look for a 
polynomial of the form 
P(z) = ao + a1z + A227 +++» + aR—12* 1 
(4.10.19) 
by writing the equations in (4.10.18) as the following k x k linear system Hx = f. 
pry 
fq) 
"ee 
LAr 
CAP 
OR 
aan 
Ae 
ao 
f(1) 
ae 
eS 
a 
; 
p(As)=fQs-— 
=> 
TA 
AL 
AT 
Pe 
pe 
. 
f (As) 
Ss a 
ee 
a2 
BS 
Oal= fiakorarot 
ll 
0est 
QAR Btohes 
( — 1)ak-? 
ys 
F/O) 
Pi =f"O) = 
(020 
2 64 
= 
(hath 
2r.e" 
: 
fi) 
Ak—-1 
The coefficient matrix H can be proven to be nonsingular because the rows 
in each segment of H are linearly independent. The rows in the top segment 
of H are a subset of rows from a Vandermonde matrix (page 60), while the 
nonzero portion of each succeeding segment has the form VD, where the rows 
of V are a subset of rows from a Vandermonde matrix and D is a nonsingular 
diagonal matrix. Consequently, Hx =f has a unique solution, and thus there is 
a unique polynomial p(z) = ap 
+a1z+Q227+-+:+a,x_12"~! that satisfies the 
conditions in (4.10.18). This polynomial p(z) is called the Hermite interpolation 
polynomial, and it has the property that f(A) = p(A). 
Note: It follows as a consequence of this that Af(A) = f(A)A for all functions 
such that f(A) exists. 
Example (Functional Identities) 
Scalar functional identities generally extend to the matrix case. For example, the 
scalar identity sin? z + cos? z = 1 extends to matrices as sin" Z + cos? Z = I, 
and this is valid for all Z € F"*". While it is possible to prove such identities on 
a case-by-case basis by using (4.10.3) or (4.10.5), there is a more robust approach 
as described below. 
For two functions f), fo: F > F anda polynomial p(z, y) in two variables, 
let h be the composition defined by h(z) = p(fi(z), fo(z)). If 
Ae F"*" with 
o (A) = {Ai,A2,.--,As} for which inder(\;) = k;, and if h is defined at A, 
then it is legitimate to say that h(A) = p(fi(A), fo(A)) because the result from 

604 
Chapter 4 
; 
~ 
Vector Spaces 
the previous example ensures that there are polynomials g(z) and q(z) such 
that h(A) =g(A) and p(fi(A), fo(A)) =4(A), where for each A; € 0 (A), 
dj 
z), 
fao(z 
} 
iy 
Ja! 
g(a) = A) = LOH LON) = GQ) for 
§= 01,54 
-1, 
Z=r 
so g(A) = q(A), and thus h(A) = p(fi(A), f2(A)). To build matrix functional 
identities for A, choose f,; and fg in h(z) = p(fi(z), fo(z)) that will make 
hAy= RO) =h" 
4) =~" = n*-)(d;) =0 
foreach 
A; €a(A), 
thereby guaranteeing that 0 = h(A) = p(fi(A), fo(A)). This technique pro- 
duces a plethora of functional identities. For example, using 
filz) = sin? z 
fo(z) = cos? z 
produces h(z) = p(f1(z), fo(z)) = sin? z + cos 
p(t, y)= a? +y?—1 
a 
a 
Since h(z) = 0 for all z € F, it follows that h(Z) =O for all Ze F"*", and 
thus sin? Z + cos? Z =I for all Z ¢ F"*". It should be evident how this tech- 
nique can be extended to include any number of functions fi, fo,..., fm witha 
polynomial p(x1,22,...,2%m) to produce even more complicated relationships. 
Integral Representations 
Two elegant and important results in complex analysis are the Cauchy—Goursat 
theorem and the Cauchy integral formula. If f :C—C 
is analytic' at all points 
in and on a simple closed contour T C C with positive orientation; then the 
Cauchy—Goursat theorem says that 
| 
f(€)dé = 0, 
(4.10.20) 
and the Cauchy integral formula says that if €) is interior to I, then 
cet ed bet AG, 
Gree 
ean 
f(€) 
f(€o) = sai | oat 
and 
f)(£) = ma | Eceyne 
(4.10.21) 
These formulas are extended to include matrix functions with the aid of the 
resolvent matrix as defined below. 
A function f of a complex variable € is analytic (or holomorphic) at &o if f is differentiable 
at €o as well as at each point in some neighborhood of €o. If f is analytic at a point, then 
its derivatives of all orders exist and are analytic at that point. 
For convenience, a simple closed contour with positive orientation is hereafter referred to as 
just a contour. Technically, a contour is an arc consisting of a finite number of smooth arcs 
joined end to end. Being closed and simple means the contour is a closed loop that doesn't 
cross itself, and positive orientation refers to moving around the contour counterclockwise. 

4.10 Functions of Nondiagonalizable Matrices 
605 
_ 4.10.5. Definition. For fixed A € F"*" with €¢0(A), the resolvent 
is defined to be R(€) = (€1— A)7!. When A and € are both allowed 
_ to vary, use the notation R(é, A). 
oe 
Note: It is not uncommon to see the resolvent be defined as (A — €1)7! 
_ the negative of this definition, but then a pesky minus ge must ee 
be dealt with in later developments. 
: 
The following theorem shows how the resolvent matrix is used to extend the 
Cauchy integral formula (4.10.21) to matrix functions. In all that follows, it is 
understood that the integral of a matrix is defined by entrywise integration. 
4.10.6. Theorem. (Integral Representations) If f : C — C is analytic in 
and on a contour I' that contains o (A) in its interior, then 
SAS mak F(E)R(E) 
dé 
(4.10.22) 
Furthermore, if [; is a contour that contains a nonempty subset of t 
eigenvalues {Aj,A2,..-,Az} C a (A) in its interior, and all other eigen- 
values of A are exterior to T;, then 
== 
= | 
RK 
Rid —-C.4G,. -+G 
(4.10.23) 
is the projector onto 
N((A — Ail) @ N((A — ol)") @--- ON ((A — ALD)™), 
and 
rank (G oD 
rank (G ">. 
alg mult, 
(A 
(4.10.24) 
In particular, if \ € 0 (A) is interior to I') and all other eigenvalues of 
A are exterior to I'), then the spectral projector for A is 
Ge = Rede. 
(4.10.25) 
M1 
1g 

606 
Chapter 4 
Vector Spaces 
Proof of (4.10.22): 
If f(z) = (€—z)~!, where € ¢ o(A), then the resol- 
vent matrix is R(€) = (€1— A)~! = f(A), so applying the spectral resolution 
theorem on page 598 to r(z) produces 
ee FON 
RO) =(-ay =f) = Sa ania, 
eee: 
(4.10.26) 
s 
kj-1l 
1 
é 
EE hme, 
i=1 j=0 
If o (A) is in the interior of T, and if f is analytic in and on I, then it follows 
from (4.10.26) and the Cauchy integral formulas in (4.10.21) that 
1 
271 [sera = [ 
sere 
aytae 
r 
TW 
JP 
8 
ky-1 esata — iI} Gidé 
~EE lo Leap a's 
s 
kj,-1 
(j) 
= 
Sa aie, = (4) 
Proof of (4.10.23): 
If eigenvalues {A;,A2,..., 
A} are interior to T, and all 
other eigenvalues of A are exterior to T;, then the Cauchy—Goursat theorem 
(4.10.20) guarantees that for i > t, 
1 
dé 
= esa 
for ine Ge Boae 
and for i <t the Cauchy integral formulas (4.10.21) with f(€) =1 yield 
1 
dé 
1 
dé 
Ini 
= 
d 
Gyan 
(Ne 
Sle 
= 
8 
6"S) (6 
Qmi Ip, €— 
Be 
Ori 
hee (2) 
oe? 
lees 
Use these with (4.10.26) to write 
1 
1 
s 
k,-1 
1 
aa [ Ie See (Emap (A — 
Wil) Gag 
Sees 
d 
) 
: 
i 
t 
i=1 

4.10 Functions of Nondiagonalizable Matrices 
607 
Statement (4.10.10) in the spectral resolution theorem on page 598 ensures 
R(G) = N((A _ 11)*) cP) N((A — doI)*?) O:::@ N((A — zI)**). 
Proof of (4.10.24): 
Since rank (P) = trace (P) for every projector (page 547), 
it follows that 
t 
t 
rank (G) = trace (G) = trace SS G, = SL trace (G;) 
i=1 
i=1 
The last equality was established in Exercise 4.9.14 on page 594. 
Wf 
Example (Inverses by Contour Integration) 
e 
First realize that if f(z) = z~', then f(A) = A7! for every nonsingular 
A € F""*". This is a straightforward consequence of the spectral resolution 
eet 
theorem (page 598) because for a Jordan block B, = ( 
ee 
; 
AY 
ROK 
(k—1) 
Ae 
Oe tS) 
2! 
(k— 1)! 
ae 
Se 
ee 
al) yok 
{et 
FOY 
FA) 
= ijl en 
ain 
B, 
a 
bf 
a 
oy 
a. 
f (Oy) 
eae 
FAVES 
aE A) 
fA) 
lett 
Gee 
is the Jordan form for 
A = PJP™!, then (4.10.3) on 
page 597 says that 
Ae 
Die Dae [ 
By' 
=Pf(J)P~ = f(A). 
e 
Using f(€) =€7! in (4.10.22) on page 605 yields 
fi I 
oa wy 
—1 
fee 
—1 
A= f(A) = oa fé (€I — 
A)~!dé, 
(4.10.27) 

608 
Chapter 4 
; 
~ 
Vector Spaces 
where [ is a simple closed contour that surrounds all of the eigenvalues of 
A but excludes the origin. 
e 
A variety of pseudoinverses have been proposed to generalize the notion of 
invertibility when A is singular—e.g., the Moore—Penrose inverse Al, the 
group and Drazin inverse A* and A", or those suggested in Exercise 4.7.28 
on page 566. So, which of these is the most "natural" extension of A-!? The 
answer is application dependent, but from a purely mathematical perspective 
an argument in favor of the Drazin inverse can be made because if 
a) 
a 
2)=4 § 
if z= 0, 
and I is a simple closed contour that surrounds all nonzero eigenvalues of 
A but excludes A; =0 so that it is neither in nor on I, then 
Des 
zt 
1 
—1 
—1 
AP = 9(A) = 5 f EMeI— Ay tae, 
To see this, suppose that index(0) =k, and let A = Olas ey be 
a core-nilpotent decomposition (page 556) in which C is nonsingular and N 
is nilpotent of index k. To evaluate 
1 
[era-onta 
0 
= 
i 
271 [ere-ayag = =e 
r 
" 
0 
[ereany ta 
rT 
first note that (€I— N)~' = €-'14+ €-?N +.---+€-*N*-1 when € £0 
(verified by direct multiplication). Since €~/ is analytic in and on T for 
j = 2,3,..., the Cauchy—Goursat theorem in (4.10.20) on page 604 ensures 
that 
[eins =o, <=> [eranyec=o. 
r 
r 
Therefore, employing (4.10.27) produces 
HA) = 
95 fe Met- ay tae=Q 
(S$) Qt =a? 
i 
271 
Note: It follows from Corollary 4.7.16 on page 558 that g(A) = At = AP 
if and only if A is an EP, matrix as defined on page 467. Different integral 
representations for the Moore-Penrose inverse are given in Exercise 4.10.21 
on page 615. 

4.10 Functions of Nondiagonalizable Matrices 
609 
Systems of Differential Equations 
The purpose here is to extend the discussion on page 330 of systems of differential 
equations from the diagonalizable case to the nondiagonalizable case. Write the 
system of differential equations shown in (3.3.19) on page 330 in matrix form as 
u(t) = Anxnu(t) 
with 
u(0)=c, 
(4.10.28) 
but now the assumption that A,,x, is diagonalizable is dropped. Suppose in- 
stead that o (A) = {Aj1,A2,...,As} with index(A;) = k;. The development par- 
allels that for the diagonalizable case, but e4* is now a little more complicated 
than that shown in (3.3.20) on page 331. Using f(z) = e* in (4.10.3) and 
(4.10.2) yields 
" 
a 
t2ert 
ge-1ert 
ae 
rn ere 
oh 
et 
wert 
7+, 
On IP lane to  Pamawithne 
i= 
uote ol 0.29) 
. 
hz! 
ert 
tert 
ert 
while setting f(z) =e" in (4.10.5) produces 
At Ro piedit 
Sr), 
7 (A — Ail)'Gi. 
(4.10.30) 
i=1 j=0 
Kither of these can be used to show that de4t/dt = Ae"! = eA*A, so, just 
as in the diagonalizable case, u(t) = e"*c is the unique solution of (4.10.28) 
(the uniqueness argument given in Theorem 3.3.4 on page 331 remains valid). 
In the diagonalizable case, the solution of (4.10.28) involves only the eigenvalues 
and eigenvectors of A as described in (3.3.23) on page 331, but generalized 
eigenvectors are needed for the nondiagonalizable case. Using (4.10.30) yields 
the solution to (4.10.28) as 
v;(Ai), where v;(\;) = (A—AiT)/Giec. (4.10.31) 
Each vz,—1(Ai) is an eigenvector associated with \; because (A — dl)" 
G, = 0, 
and {vz,—2(Ai), ---, Vi(As), Vo(As)} is an associated chain of generalized eigen- 
vectors. The behavior of the solution (4.10.31) as t — oo 
is similar but not 
identical to that discussed on page 333 because for 
A= 2+ iy and t > 0, 
0 
lea 
O, 
unbounded 
if] 0 and 7 > 0; 
tie' = tie (cos yt + isin yt) > ¢ oscillates indefinitely 
if 
c= 7 =0 and y £0, 
1 
ie =p 
= 0: 

610 
Chapter 4 
Vector Spaces 
In particular, if Re(\;) <0 for every 4; € (A), then u(t) > O for every 
initial vector c, in which case the system is said to be stable. 
e 
Nonhomogeneous Systems. 
It can be verified by direct differentiation 
that the solution of u/(t) = Au(t)+f(t) with u(to) =c is given by 
t 
u(t) = eA(¢-to)¢ +f eAlt-7) f(r) dr. 
to 
e 
Part (e) in Exercise 4.10.21 on page 615 addresses singular systems. 
Example (Nondiagonalizable Mixing Problem) 
There is a belief in some circles that nondiagonalizable systems do not occur 
naturally, but this is a myth. To make the point that even simple problems 
in nature can be nondiagonalizable, consider three V gallon tanks as shown 
in Figure 4.10.1 that are initially full of polluted water in which the i'" tank 
contains c; lbs of a pollutant. In an attempt to flush the pollutant out, all spigots 
are opened at once allowing fresh water at the rate of r gal/sec to flow into the 
top of tank #3, while r gal/sec flow from its bottom into the top of tank #2, 
and so on. 
she Fresh 
—_—_—> 
r gal/sec 
|r gal/sec 
FIGURE 4.10.1: FLUSHING POLLUTANTS FROM CONNECTED TANKS 
Problem: How many pounds of the pollutant are in each tank at any finite time 
t > 0 when instantaneous and continuous mixing occurs? 
Solution: If u;(t) denotes the number of pounds of pollutant in tank i at time 
t > 0, then the concentration of pollutant in tank i at time t is u,(t)/V lbs/gal, 
so the model wj(t) = (Ibs/sec) coming in—(lbs/sec) going out produces the non- 
diagonalizable system: 
tld) 
= 
oe) ae 0 
ui(t) 
ul (eye |= 
Oe 
eee 
u2(t) 
|, or u'=Au with u(0)=c= (=). 
ui,(t) 
0 
0 -1/ 
\us(t) 
5 

4.10 Functions of Nondiagonalizable Matrices 
611 
7 
= 
il 
0 
A is simply a scalar multiple of a single Jordan block B, = ( 
0 -1 
1) 
, SO 
0 
0 
-1 
e'" 
is easily determined by replacing t by rt/V and \ by —1 in the second 
equation of (4.10.29) to produce 
At 
1 rt/V 
(rt/V)? /2 
el 
bie 
re 
he 
(4.10.32) 
(0) 
0 
i 
At e(rt/V)B 
e 
a 
Therefore, 
c1 + co(rt/V) +3 (rt/V)? /2 
co + ¢3(rt/V) 
; 
C3 
and, just as common sense dictates, the pollutant is never completely flushed 
from the tanks in finite time. Only in the limit does each u; > 0, and it is clear 
that the rate at which u; — 0 is slower than the rate at which u2 — 0, which 
in turn is slower than the rate at which u3 — 0. 
Concluding Remark 
The discussion of functions of matrices given in this section represents just the 
tip on an iceberg. There is enough known about the ideas, properties, and ap- 
plications of matrix functions to fill an entire book! 
Exercises for section 4.10 
4.10.1. Lake #7 in a closed system of three lakes of equal volume V_ initially 
contains 
c; lbs of a pollutant. The water in the system is circulated 
at rates (gal/sec) as indicated in Figure 4.10.2. Assuming continuous 
mixing, find the amount of pollutant in each lake at time t > 0, and 
then determine the pollution in each lake in the long run. 
2r 
Lem poe 
—- 
ee ae 
FIGURE 4.10.2 
j For example, see Functions of Matrices: Theory and Computation by Nicholas J. Higham, 
Society for Industrial and Applied Mathematics, 2008. 

612 
Chapter 4 
: 
Vector Spaces 
4.10.2. 
4.10.3. 
Explain why Af(A) = f(A)A whenever f(A) exists. 
ee 
et 
Show that if f is a function defined at A = (° ; 2) 
then 
a 
0 
8 0 
uy 
Oa 
8 
f(A) = Fa)l+ Bg"(@)N + frs'(a) 
+ =O] n? for N= (0.0 1). 
4.10.4. 
4.10.5. 
4.10.6. 
4.10.7. 
Composition of Matrix Functions. If h(z) = f(g(z)), where f and g are 
functions such that g(A) and f(g(A)) exist, then h(A) = f(g(A)). 
However, it's not legal to prove this simply by saying "replace z by 
A." One way to prove that h(A) = f(g(A)) is to demonstrate that 
h(B,) = f(9(B.)) for a generic Jordan block and then invoke (4.10.3) 
on 597. Do this for a 3 x 3 Jordan block—the generalization to k x k 
blocks is similar. That is, let h(z) = f(g(z)), and use Exercise 4.10.3 to 
prove that if g(B,) and f (9(B.)) each exist, then 
: 
Ae 
0 
h(B,) 
= f(g(B.)) 
for 
B.=(0 : 1), 
Let A € o(Anxn) 
and let My C F be a neighborhood of X that 
contains no other eigenvalues of A. Prove that the spectral projector 
for A is given by 
G,=f(A), 
where 
f(z) = a be : a 
For each \ € o(An xn), explain why the associated spectral projector 
G) can be expressed as a polynomial in A. 
With the convention that o) =0 for 7 >k, explain why 
yk 
Ge) \k-1 
(o Ni? ee (aie \k-m+1 
Ns 
Gyn 
; 

4.10 Functions of Nondiagonalizable Matrices 
613 
4.10.8. 
4.10.9. 
4.10.10. 
4.10.11. 
4.10.12. 
4.10.13. 
4.10.14. 
Spectral Mapping Property. 
Prove that if (A,x) is an eigenpair for A, 
then (f(A),x) is an eigenpair for f(A) whenever f(A) exists. Does it 
also follow that alg mult, (A) = alg mult pay (f(A))? 
Let f be defined at A, and let \ € 0 (A). Give an example or an 
explanation of why the following statements are not necessarily true. 
(a) f(A) is similar to A. 
(b) geo mult, (A) = geo multsay (f(A)). 
(c) index 
a 
(A) = index say(f(A)). 
692 
8 
Determine e" for A = (2 2 2). 
0 
0 
2 
23, 
= 
8=9 
For f(z) =4/z-—1, determine f(A) when A= ( 
5. 
iy 
a). 
—1 
-2 
1 
Explain why every nonsingular A € F"*" has a square root, and then 
formulate necessary and sufficient conditions for the existence of V/A 
when A is singular. 
re! 
Perturbed Jordan Block. Perturb Jpx7 = 
ee 
by adding a small 
0 
number 
¢€ > 0 to the lower left-hand entry so that 
Otel 
: 
c 
J+E= 
Shea 
, 
where 
E=€e,e;. 
€ 
0 
nmxn 
Show that o(S + BE) = fe'/", eh... ei * where w= e'™" 
(then* roots of unity are discussed on page 718). 
Note: This exercise shows that a small change to the entries in a ma- 
trix can greatly affect its eigenvalues. Consequently, floating-point com- 
putation of high-index eigenvalues as well as the Jordan form itself is 
problematic—even the best software packages cannot overcome this dif 
ficulty for high-index situations. 
Explain why a function f is defined at A ¢ F"*" 
if and only if f 
is defined at A', and then prove that f(A?) = [F(A)] 
Why can't 
(x)* be used in place of (x)"? 
Hint: Recall Exercise 4.9.7 on page 593. 

614 
Chapter 4 
~ 
Vector Spaces 
4.10.15. 
4.10.16. 
4.10.17. 
4.10.18. 
4.10.19. 
Use the technique discussed in the example concerning functional iden- 
tities on page 603 to establish the following identities. 
(a) eA4e-4 =I 
forall 
AE C™". 
(b) e%4 = (e4)® for all 
ae C and 
Ac C™™". 
(c) e4 =cosA+isinA for all 
Ae C"*". 
(a) Show that if AB = BA, then e4+B = e4eB. Hint: Consider 
infinite series. 
(b) 
Give an example to show that e4*+B # e"eB in general. 
Find the Hermite interpolation polynomial p(z) described in (4.10.19) 
3 
2 
1 
on page 603 such that p(A) =e" for A= (= —2 
-1 ). 
8) 
I 
II 
Cayley-Hamilton vs. Hermite. 
The Cayley-Hamilton theorem (page 
312) says that every A € F"*" satisfies its own characteristic equa- 
tion, and this guarantees that A"*! 
(j =0,1,2,...) can be expressed 
as a polynomial in A of at most degree n—1. Since f(A) = p(A) 
for some polynomial, the Cayley—Hamilton theorem ensures that f(A) 
can be expressed as a polynomial in A of at most degree n — 1. Such 
a polynomial can be determined whenever f();), 7 =0,1,...,a;—1 
exists for each A; € o (A), where a; = alg mult(A;). The strategy is 
the same as that on page 602 using the Hermite interpolation polynomial 
except that a; is used in place of k; . If p(z) = agt+ayz+-+-+an_12"7! 
is a polynomial such that for each A; € o (A), 
pO) 
= FO), 
p'A)=f'OAs), 
-- 
ph PO) = fo - VQ), 
then p(A) = f(A). Why? These equations are an n x n linear system 
with the a;'s as the unknowns, and, for the same reason used for the 
Hermite polynomial on page 602, a solution is always possible. 
(a) What advantages and disadvantages does this approach have 
with respect to the Hermite interpolation approach? 
(b) Use this method to find a polynomial p(z) such that p(A) =e" 
3 
Ps 
1 
fOr eA = (-2 i a . Compare with Exercise 4.10.17. 
Show that if Re (A) <0 foreach \€ 0(A), 
(ie., A isa stable matriz ) 
then lim:_,.. e4* = 0. 

4.10 Functions of Nondiagonalizable Matrices 
615 
4.10.20. Prove that if Re(A) <0 and Re(u) <0 for each 
\€a(Am ym) and 
A €oa(Bnxn), then the unique solution Xmy, of the matrix equation 
AX +XB=Cyxn is 
4.10.21. 
Hint: C 
Matrix I 
co 
x= -| en Gem' at. 
0 
onsider the differential equation dX/dt 
= AX+XB, X(0)=C. 
nverses and Integration of Exponentials. The standard nonsingu- 
lar inverse as well as other pseudoinverses can all be expressed in terms 
of integration of matrix exponentials. 
(a) 
(b) 
Show that if A € F"*" is nonsingular, then 
[tae =A-teA'+C, 
where Ce F"*" is arbitrary. 
In the case when Re(A) > 0 for each 
4 € a (A) (ie., —A is 
a stable matria), conclude that 
A-l= ih e At dt. 
0 
Show that if A' is the Moore-Penrose pseudoinverse (page 192) 
of Avek''*"" then lim,..e7° "' =I-ATA= Pya), and 
then prove that 
co 
lo e) 
Ve ii Se INGE ii Ate 
AA tat. 
0 
0 
Prove that if —A is a stable matrix, then the Drazin pseudoin- 
verse of An 
xn is given by 
ere) 
te 
AY =f eA takdt, 
where 
k= index (A). 
0 
Demonstrate that if A ¢€ F"*" is singular with inder(A) =k, 
and if A? is the Drazin pseudoinverse (page 557) of A, then 
k=1 
k! 
A 
A 
Atd¢ = APeAt 4 (I- AA" 
)t ci 
eee 
al oC 
where C € F"*" is arbitrary. Conclude that if b € F", then 
r 
A 
INE 
x= APM FAAP SEIT Fa 
7 tb 
is a particular solution for the singular system of differential 
equations x' + Ax = b. 

616 
Chapter 4 
: 
Vector Spaces 
4.11 
DIFFERENCE EQUATIONS, LIMITS, AND SUMMABILITY 
ea emerge ie ere ee nes Fete 
A linear difference equation of order m with constant coefficients has the form 
y(k +1) =my(k) + Am—ry(k— 1) + Aylk—-m+1)+r% 
(4.11.1) 
in which Xo,\1,---,Am along with initial conditions y(0),y(1),--.,y(m — 1) 
are known constants, and y(m),y(m-+1),y(m+ 2)... are unknown. Difference 
equations are the discrete analogs of differential equations, and, among other 
ways, they arise by discretizing differential equations. For example, discretizing 
a second-order linear differential equation results in a system of second-order 
difference equations as illustrated in the two-point boundary value problem dis- 
cussed on page 220. 
The theory of linear difference equations parallels the theory for linear dif- 
ferential equations, and a technique similar to the one used to solve linear dif- 
ferential equations with constant coefficients produces the solution of (4.11.1) 
as 
Xo 
= 
k 
eens 
Je 
Soe 
ee 
ee 
4.11.2 
y(k) =a 
of 
( 
) 
in which the \;'s are the roots of A —A,,A™ 1 —---— Ag =0, and the f;'s 
are constants determined by the initial conditions y(0), y(1),...,y(m—1). The 
first term on the right-hand side of (4.11.2) is a particular solution of (4.11.1), 
and the summation term in (4.11.2) is the general solution of the associated 
homogeneous equation defined by setting Ao = 0. 
Systems of first-order linear difference equations with constant coefficients 
written in matrix form are 
x(k +1) = Ax(k) 
(a homogeneous system), 
or 
(4.11.3) 
x(k +1) = Ax(k)+b(k) 
(a nonhomogeneous system), 
in which matrix Ay», the initial vector x(0), and vectors b(k), 
k =0,1,..., 
are known. The standard problem is to determine the unknown vectors x(k) for 
each k = 1,2,... along with an expression for the limiting vector limz_.o, x(k). 
These systems are most frequently used to model linear discrete-time evolution- 
ary processes, and the goal is usually to predict how (or to where) the process 
eventually evolves given the initial state of the process. For example, the pop- 
ulation migration problem in the example on page 327 produces the system of 
homogeneous linear difference equations given in (3.3.15) and (3.3.16), and the 
long-run (or steady-state) population distribution is obtained by finding the lim- 
iting solution derived in (3.3.17). 
Solving the equations in (4.11.3) is conceptually easy. Direct substitution 
will verify that 

4.11 Difference Equations, Limits, and Summability 
617 
x(k) =A*x(0) 
for 
k=1,2,3,... 
d 
an 
- 
(4.11.4) 
x(k) = A*x(0)+ 5) A*-F-'b(j) 
for 
&=1,2,3,... 
iW) 
are respective solutions to (4.11.3). Aside from finding x(k) for any finite k, 
the more important (and substantial) problem is to understand the nature of the 
limiting solution limgz_,.. x(k). This boils down to analyzing limp_,., A*, and 
the first step in this analysis is to articulate necessary and sufficient conditions 
under which A' — 0. 
Convergence to Zero 
If A € F, then it is elementary that \* — 0 if and only if || < 1, so it is 
natural to ask if there is an analogous statement for matrices. The first inclina- 
tion is to replace absolute value |x| by a matrix norm || x ||, but this doesn't 
work for any of the standard norms such as the 1-, 2-, oo-, and Frobenius matrix 
norms. For example, if A = it a then A* + 0, but |/A|| = 2 for these 
standard norms. Although it is possible to construct a rather goofy-looking ma- 
trix norm ||* ||, such that |/Al|g <1 when limy.A* = 0, the underlying 
mechanisms governing convergence to zero are better understood and analyzed 
by using eigenvalues and the Jordan form rather than norms. In particular, the 
spectral radius p(A) = max ye, a) |A| that was introduced in Definition 3.1.8 on 
page 297 plays a central role. 
4.11.1. Theorem. lim;_,.. A*,,, =0 if and only if p(A) <1. 
Proof. 
If P~!AP =J 
is in Jordan form, then 
so 
eels 
Ak =py*p-1=P 
BE 
pie 
dh = ( 
=e) 
ts) 
' 
DN 
is a generic Jordan block in J. Clearly, A®* — 0 if and only if B® — 0 for 
each Jordan block, so it suffices to prove that B* > 0 if and only if |A| < 1. 
Using the function f(z) = 2" in formula (4.10.2) on page 596 along with the 
convention that (5) =0 for 7 >k produces 

618 
Chapter 4 
Vector Spaces 
BS = 
re 
(Bak? 
(4.11.6) 
mxm 
(Exercise 4.10.7, page 612). It is clear from the diagonal entries that if Bt + 0, 
then \* > 0, so |A| < 1. Conversely, if |A| <1 then limz co (Ara = 0 for 
each fixed value of j because 
(') 
cals 
SD org (i)e 
j 
< Baik — 0. 
5 
j! 
j! 
j! 
It can be seen that the last term on the right-hand side goes to zero as k — oo 
by either applying L'Hépital's rule or by realizing that k? goes to infinity with 
polynomial speed while |A|*~/ is going to zero with exponential speed. There- 
fore, if |A| 
<1, then B* — 0, and thus the theorem is proven. 
& 
e 
The expression in (4.11.6) makes it clear that the rate at which 
A* + 0 is governed by the magnitude as well as the index of 
(4.11.7) 
the largest eigenvalue of A. 
Neumann Series, the Complete Statement 
Intimately related to the question of convergence to zero is the convergence of 
the Neumann series )\7- A" that was introduced in Theorem 2.3.11 on page 
170. As discussed on pages 325-326, the Neumann series converges if and only 
if limg+o00 A* = 0 when A is diagonalizable. It can now be shown that the 
diagonalizability assumption can be dropped. Below is the complete statement. 
4.11.2. Theorem. The following statements are equivalent for A € F"*", 
e 
The Neumann series 
I+ A +A?+--- converges. 
(4.11.8) 
e 
p(A) <1. 
(4.11.9) 
. 
k Bibel 
° 
jim A* =0. 
(4.11.10) 
For each case, (I— A) * exists and })°,, A* = (1— A)". 
(4.11.11) 

4.11 Difference Equations, Limits, and Summability 
619 
Proof. 
The fact that (4.11.9) and (4.11.10) are equivalent is Theorem 4.11.1, 
and it was argued in the proof of Theorem 2.3.11 on page 170 that (4.11.10) 
implies (4.11.8), so all that is required is to prove that (4.11.8) implies (4.11.9). 
If So A" converges, it follows that yp. BE must converge for each Jordan 
block B, in the Jordan form for A. This together with (4.11.6) implies that 
[eo BE] ,, = Ceo A" converges for each \ € o (A), and this geometric series 
converges if and only if |\| <1. Thus the convergence of 5\7°, A* implies 
p(A) < 1. If O°, A* converges, then it converges to (I— A)~! because 
(I—A)(I+A+A*-1)=I-A*>4Iaskoo. 
UB 
Sensitivity of Inverses 
Exercise 2.3.21 on page 176 asks you to use a first-order approximation of the 
Neumann series to show that the relative change in the norm of a nonsingular 
matrix inverse subject to a small perturbation is roughly bounded by the relative 
size of the perturbation magnified by a condition number. This statement can 
now be made more concrete and rigorous. 
4.11.3. Theorem. Let A € F"*" be nonsingular, and let ||x|| be any 
induced norm, and let « = ||A||||A~1|| be the associated condition 
number (see pages 172 and 252), If E ¢ F"*" is a small perturbation 
in es sense that Le 
1B|| <1, then (A — o is nonsingular, and 
a ETAT, ACE 
RIEI/ IAI 
[Aq T 
={-[A"E]  1—s/EI/1Al 
Proof. 
First notice that p(A7 1K) < |A- 1 || <1 (page 298) forces x A-1E 
to be nonsingular (by Theorem 4. i 
2), and hence A— E = A(I— 
A7!B) is 
also nonsingular. 
The Neumann expansion for (I— A~'E)~! ee 
| co 
ore) 
|a— ate = [a] 
< Ata 
ey 
gil 
(Ady 12) 
1 
i 
Sle 
hie 
Se 
1—||A~*E]| ~ 1—||A~*]||[E I 
Direct multiplication and a little matrix algebra show that 
(A= BE) 
A 
=AtE(A=E) =A 'E(I-—A"E) A". 
Therefore, the relative sige in the norm of the inverse of A is 
(OnEE seed eee eee 
lA IE 
jaa] 
<1 [Ay ST aT 
SEI /IAl 
~ 1—« El] 
/ Al 

620 
Chapter 4 
Vector Spaces 
Resolvent Perturbations 
For A € F"*" with &) ¢o(A), the resolvent matric R(f, 
A) = (E01 — A)7* 
was used in Theorem 4.10.6 on page 605 to extend the Cauchy integral formula 
to matrix functions. For a fixed 
 ¢ o(A), an important problem concerns 
the extent to which R(&),A) changes when A is slightly perturbed to become 
A+E. There are two facets to this problem. The first concerns just how small 
||E|| must be to ensure existence of R(€, A+E), and the second is to bound the 
norm of the difference between R(é), A+E) and R(£, A). These problems can 
be viewed as a special cases of the previous discussion, but because the details 
are important in the later development of the analyticity of spectral projectors 
(page 923), they are included in the following theorem. 
4.11.4, Theorem. 
If ||ER(€, 
A)|| < 1 for some induced norm and a 
fixed 
> 
€a (A), then R(€,A+E) exists and 
Ro, 
A +E) =R(é,A)[I-ER(6,A)] 
(4.11.13) 
Furthermore, 
2 
|R(éo, 
A + E) — R(€, A)|| < SET 
In particular this means that R(é),A + E) > R(&,A) as 
E> 0. 
(4.11.14) 
Proof. 
p(ER(&,A)) < ||ER(&,A)|| <1 guarantees that 
I— ER(f, 
A) is 
nonsingular, so 
R(éo, A) [I- ER(, A)]"* = (601 — A)7! [I- E(éoI — A)-2] 
= [(I-E(@I-A))(@1-A)] = (G01- (A+B)? (4.11.15) 
Using the Neumann expansion of [I— ER(&,A)]~* in (4.11.15) yields 
co 
R(fo, A +E) = R(é, A) + R(&, A) 5 [ER(f, A)]*. 
(4.11.16) 
k=1 
This is sometimes called the second Neumann series. Using an inequality similar 
to that in (4.11.12), the absolute change in the norm of the resolvent is 
|R(G, A+B) —RG,A)| = |R@, AYER, A) 
(ERG, A)}*| 
k=0 
2) 
< [R(G,A) BR(G,A)] > 
/ER(,a)f* < RGA IE 
: 
: 
oI oe 
TR. AE] 

4.11 Difference Equations, Limits, and Summability 
621 
Spectral Radius as a Limit 
The elementary fact that p(A) < ||A|| for all A € F"*" and for all matrix 
norms was established in Corollary 3.1.9 (page 298), but this was just a precursor 
to the following elegant relationship between spectral radius and norm. 
1/k 
Proof. 
First note that p(A)* = p(A*) < JA*|| 
=> 
p(A) < |/A*]| 
Now observe that p(A/(p(A)+.€)) <1 for every € > 0, so, by Theorem 4.11.1 
on page 617, yc nega Tas 
k00 \ p(A) +e 
k-00 (p(A)+e)* 
Consequently, there is a positive integer K, such that ||A*||/(p(A) +6)* <1 
for all k > K,, or equivalently Ae < p(A)+e 
for all k > K,. Therefore, 
p(A)< TPN 
<p 
A) qe. 
for ik 2 ke. 
1/k 
Because this holds for each € > 0, it follows that limp 
co | A* || 
p(A). 
The utility of 
Theorem 4.11.5 is illustrated in the next two corollaries, the 
results of which are important to the subsequent developments of the Perron— 
Frobenius theory. 
Bounding a Spectral Radius 
For X,Y € R™"*", define X < Y to mean 
2;; < yj; for each 7 and j 
(entrywise inequality), and for B <¢ F""*", let |B| denote the matrix of absolute 
values |b;;| (or complex modulus) of the entries in B. 
4.11.6. Corollary. If |B] < A for 
AG R"*" and 
Be F"™"", then 
p(B) < p(|B\) < e(A). 
(4.11.18) 

622 
Chapter 4 
Vector Spaces 
Proof. 
The triangle inequality produces |B*| < |B|* for every positive integer 
k. Furthermore, |B] < A implies that |B|* < A*. This with (4.11.17) yields 
IB ll, = IIIB Ilo <I IBI Ilo < A*ll. 
= |B 
< | BIE < |All 
: 
A 
1/k 
: 
= Jim |[B*||0* < jim <|||BIF||2" < Jim <a' 
1/k 
co 
1/k 
k-00 
oP 
= p(B)<p(\|B\)<p(A). 
4.11.7. Conilary For B E R'™" with B> O, it is true that rI—B 
is 
nonsingular with (rI—B)~! > 0 if and only if p(B) <r. 
(Proofs 
it pi.) <2", then p(B/r) = 1, so 
rI—-B= r(1 — =) is nonsingular and (rI — B)~! = 2s ek >0 
(by Theorem 4.11.2, page 618). To prove the converse, first note that the triangle 
inequality ensures that |PQ| < |P||Q| for all conformable P and Q. To see 
why rI—B 
being nonsingular with (rI—B)~! > 0 implies that p(B) <r, let 
(A,x) be any eigenpair for B, and use B > 0 together with (rI—B)~! > 0 
to write 
Ax =| BX) 
Set 7 [Al x) ex) == |B xk Bi x)= Bix 
= > —B|x| < —|Bx|=—|A||x| = 
(rI—-B)|x| < (r—|A)) |x| 
=> 
|x| < (r—|A|) (rI—B)7'|x| 
(4.11.19) 
=> 
r-—|dr|>0 
(because (rI— B)~! > 0 and |x| > 0). 
Now, |A| 
#47; otherwise (4.11.19) would imply that |x| (and hence x) is zero, 
which is impossible. Hence |\| <r for all 
A € o(B), and thus p(B)<r. 
& 
M-Matrices 
It is not uncommon in practical applications to encounter real and nonsingular 
matrices that have nonpositive off-diagonal entries and a nonnegative inverse. 
These matrices have a special designation. 
4.11.8. Definition. An M-matriz' is a nonsingular matrix A € R"*" 
such that a;; < 0 for all i #j and A~!>0 
(each entry of A~! is 
nonnegative). 
Some of the more important properties of M-matrices are consequences of 
Corollary 4.11.7—they are summarized in the following theorem. 
The term "M-matrix" is due to Alexander Ostrowski (a well-known twentieth-century mathe- 
matician) who introduced it in 1937 to honor Hermann Minkowski (see page 38). 

Proof of (4.11.20). 
If A is an M-matrix, then, by definition, a,; <0 for i 4 j, 
and A is nonsingular with A~' > 0. If r = max;|aj| and B = rI — A, 
then B > 0. By virtue of being an M-matrix, 
A = rI —B 
is nonsingular with 
A~!=(rI—B)~!>0, so Corollary 4.11.7 ensures that p(B) <r. Conversely, 
if A = rl—B, where B > 0 and r > p(B), then a; < 0 for each 7 F j. 
Corollary 4.11.7 guarantees that (rI—B)~! exists and (rI—B)~! > 0. In other 
words, A is nonsingular with A~! >0, and thus A is an M-matrix. 
I 
Proof of (4.11.21). 
If A is an M-matrix, then A = rI—B 
for some B > 0 
and r > p(B) by (4.11.20). Consequently, if A € o(A), then 
AX 
=r-—§8 
for 
some 
8 =a+ib € 0(B), so r > p(B) > || = Va*2 +6? > |a| > a implies 
that Re(A) =r—a>0. Conversely, if A is any matrix such that aj; <0 for 
i#j and Re(A) >0 
for all \ © a (A), 
then there is a circle of radius y and 
centered at y that contains o (A)—see Figure 4.11.1. Let r be any real number 
such that r > max{2y, max; |a;;|}, and set B = rI— A. It is apparent that 
B > 0, and, as can be seen from Figure 4.11.1, the distance |r — A| between r 
and every \ € 0 (A) is less than r (remember that Re (A) > 0). 
Ficure 4.11.1 

624 
Chapter 4 
Vector Spaces 
All eigenvalues of B look like 8 =r—A, and |6|=|r—A| <r, so p(B) <r. 
Now, A =rI—B is nonsingular (since 0 ¢ 7(A)) with B20 and r> p(B), 
so Corollary 4.11.7 ensures that A~' > 0. Thus A is an M-matrix. 
@ 
Proof of (4.11.22). 
If Axx is the principal submatrix lying on the intersection 
of rows and columns 71,...,7?4_in an M-matrix A =rI—B, where B>O 
and 
r > p(B), then 
A = rI—B, where B > 0 is the corresponding principal 
submatrix of B. If P is a permutation matrix such that P7BP = ies aE 
then B 1 eae and if Case pas then 0 < C <B, so, by 
(4.11.18) on page 621, p(B) = p(C) < p(B) <r. Therefore, Corollary 4.11.7 
guarantees that A is an M-matrix. 
Mf 
Proof of (4.11.23). 
If A isan M-matrix, then det (A) > 0 because the eigenval- 
ues of a real matrix appear in complex conjugate pairs, so det (A) = []/_, \; > 0 
(page 294). Hence each principal minor is positive because each submatrix of an 
M-matrix is again an M-matrix by (4.11.22). Now prove that if A,» is a matrix 
such that a;; 
<0 for 
1#Jj and each principal minor is positive, then A must 
be an M-matrix. Proceed by induction on 
n. For n = 1, the assumption of 
positive principal minors implies that A = [a] with a >0, so A~!= [1/a] > 0. 
Suppose the result is true for 
n =k, and consider the LU factorization 
Ax kee 
I 
0 
A 
c 
A (k-+1)x(k+1) = ( 
ap 4 
= ere kts Vee) = LU. 
We know that A is nonsingular (det (A) is a principal minor) and A > 0 (it is 
a 1x1 principal minor), and the induction hypothesis ensures that A~! > 0. 
Combining these facts with 
c <0 and d? <0 produces 
me 
ee 
AW ne 
I 
0 
—ee yp — 
lel 
A-—dtA-le 
Age 
— Ue la 
a 
= 0: 
0 
1 
click 
Salt 
-dTA-le 
and thus the induction argument is completed. 
Proof 
of (4.11.24). 
If A =M-—N isan M-matrix, where M~! > 0 and N > 0, 
then H = M™N is clearly nonnegative and 
I— H = M~!A is nonsingular. 
Since (I— H)~!(I— H) =I, it follows that 
(I=Hy 1H)' 
HSA SNS 
0 
(I=H) 10) 
so Corollary 4.11.7 (page 622) guarantees that p(H) <1. 
& 
Note: Comparing properties of M-matrices with those of positive definite ma- 
trices reveals several parallels, and, in a rough sense, an M-matrix often plays 

4.11 Difference Equations, Limits, and Summability 
625 
the role of "a poor man's positive definite matrix." Theorem 4.11.9 represents 
only a small sample of M-matrix theory—there is in fact enough known about 
M-matrices to fill a monograph on the subject. For example, there are at least 
50 known equivalent conditions that can be imposed on a real matrix with non- 
positive off-diagonal entries (often called a Z-matriz) to guarantee that it is an 
M-matrix! 
Linear Stationary Iterations 
The reason splittings of a nonsingular A € R"*" as described in (4.11.24) are of 
interest is because they produce iterative algorithms for solving systems of linear 
equations. Advantages of iterative techniques over direct elimination methods are 
realized when A is large and sparse (i.e., many zeros, preferably in a pattern). 
To see how splittings can be exploited, consider a nonsingular linear system 
Ax = b in which 
A = M—N, where M7! exists, and lett 
H = M~!N 
(H_ is called the iteration matriz), and let d = M~'b. For an arbitrary vector 
x(0)nx1, a linear stationary iteration is defined by setting 
x(k) =Hx(k—1)+d 
for k=1,2,3,.... 
(4.11.25) 
The salient feature is that if p(H) <1 (which is guaranteed by (4.11.24) to be 
true when A is an M-matrix), then for every initial vector x(0), 
lim x(k) =x = A7?b. 
(4.11.26) 
k—oo 
To see this, recall from Theorem 4.11.2 on page 618 that if p(H) < 1, then I-H 
is nonsingular and (I—H)~! = )°°°,, H*. Furthermore, (I-H)~'M~! = A"?. 
Successive substitution applied to (4.11.25) yields 
x(k) = H*x(0) a (I SE 
alae H2 ees Hd, 
so whenever p(H) < 1, 
lim x(k) = (I—H)-'d = (I— H)"'M"!b=A "'*b=x. 
(4.11.27) 
k-co 
While it is important to know that (4.11.25) always converges, a more important 
question for numerical work is, just how fast is the convergence? It is clear from 
the note (4.11.7) on page 618 that the convergence rate of (4.11.25) is governed 
by the size of p(H) along with the index of its associated eigenvalue. But what 
really is needed is an indication of how many digits of accuracy can be expected 
An extensive list of such conditions appears on page 134 in the text Nonnegative Matrices in 
the Mathematical Sciences by Abraham Berman and Robert J. Plemmons, Academic Press, 
1979. A sample of some of these conditions is given in Exercise 4.11.14 on page 642. 

626 
Chapter 4 
; 
Vector Spaces 
to be gained per iteration. So as not to obscure the simple underlying idea, 
assume that H, 
x» is diagonalizable with 
o (H) = Lapras 
oe PAGS 
where 
1> |A1| > |A2| Pa |A3| rte fit |As| 
(which is not uncommon in applications), and let €(k) = x(k) —x denote the er- 
ror after the kt" iteration. Subtracting x 
= Hx+d (a consequence of (4.11.27)) 
from x(k) = Hx(k —1)+d_ produces (for large k) 
e(k) = He(k — 1) = H*e(0) = (AFG, + ARGo +--+ + A$Gy)e(0) © ATGr€(0), 
where the G,;'s are the spectral projectors occurring in the spectral decomposi- 
tion of H (page 549). Similarly, e(k — 1) = d*-'G,€(0), so comparing the as 
components of e(k —1) and e(k) reveals that after several iterations, 
1 
~ — =—— 
foreach 
1=1,2,...,n. 
€;(k) 
|Ai| 
(1) 
To appreciate the significance of this, suppose for example that 
les(kK —1)| = 10° 
~and' 
 je;(k)| = 10°? 
with 
 p>q>0 
so that the error in each entry is reduced by p—q 
digits per iteration. Since 
p—q=logig 
= —logi9 e(H), 
(4.11.28) 
€;(k) 
it is seen that —log,g ¢(H) provides an indication of the number of digits of 
accuracy that can be expected to be eventually gained on each iteration. For this 
reason, the number R = — log;9 p(H) (or, alternately, 
R = — In 
p(H)) is called 
the asymptotic rate of convergence, and this is the primary tool for comparing 
different linear stationary iterative algorithms. 
The trick is to find splittings that guarantee rapid convergence while in- 
suring that H = M~!N and d = M~'b can be computed easily. The three 
classical splittings that meet this criterion are the Jacobi, Gauss-Seidel, and 
SOR (successive over relaxation) methods that are briefly described below. 
Jacobi's Method 
Jacobi's method is produced by splitting 
A = D—N, where D is the diagonal 
part of A (assume that each a; #0), and —N consists of the off-diagonal 
entries. Clearly, both 
H = D~!N and d = D~'b can be formed with little 
effort. The i*" component in the Jacobi iteration x(k) = D~'Nx(k—1)+D>~!b 
is given by 

4.11 Difference Equations, Limits, and Summability 
627 
This shows that the order in which the equations are considered is irrelevant 
and that the algorithm can process equations independently (or "in parallel"). 
For this reason Jacobi's sys was referred to in the 1940s as the method 
of simultaneous displacements! Jacobi's method is guaranteed to converge for 
all initial vectors x(0) and for all right-hand sides b when A is diagonally 
dominant (page 161) because |a,;| > >; |@ij| for each ¢ and p(H) < ||H||,, 
implies that 
p(H) < ||HI|,. = mex ie 
il mS a] 
(4.11.30) 
|axs| 
Gauss-Seidel Method 
The Gauss-Seidel' method is the result of splitting 
A= M-—N=(D-L)-U, 
where D is the diagonal part of A (assume that each a,; 
#0), and where —L 
and —U contain the respective entries occurring below and above the diagonal. 
The iteration matrix is 
H = M~'N = (D—L)7=!U, and d = (D—L)~'b, 
and the iteration is x(k) = (D — L)~1Ux(k — 1) + (D — L)~'b. Rewriting this 
as Dx(k) = b+ Lx(k) — Ux(k—1) shows that the i" entry in the iteration is 
aji(k) = (bj — Di 
jeg Vig Ly(k) — ojos Gig 
Zi (k — 1)) /aui 
(4.11.31) 
Jacobi considered this method in 1845, but it seems to have been independently discovered by 
others. In addition to being called the method of simultaneous displacements in 1945, Jacobi's 
method was referred to as the Richardson iterative method in 1958. 
Carl Gustav Jacob Jacobi (1804-1851) was born in Pots- 
dam, Germany, was educated at the University of Berlin, 
and was a professor at the University of Konigsberg. Dur- 
ing his prolific career he made contributions that are still 
important facets of contemporary mathematics. His accom- 
plishments include the development of elliptic functions; a 
systematic development and presentation of the theory of 
determinants; contributions to the theory of rotating liq- 
uids; and theorems in the areas of differential equations, 
calculus of variations, and number theory. In contrast to 
his great contemporary Gauss, who disliked teaching and 
was anything but inspiring, 
Jacobi was regarded as a great 
teacher (the introduction of the student seminar method 
is credited to him), and he advocated the view that "the 
sole end of science is the honor of the human mind, and 
that under this title a question about numbers is worth as 
much as a question about the system of the world." Jacobi 
once defended his excessive devotion to work by saying that "Only cabbages have no nerves, 
no worries. And what do they get out of their perfect wellbeing?" Jacobi suffered a breakdown 
from overwork in 1843, and he died at the relatively young age of 46. 
CARL G. JACOBI 
J Ludwig Philipp von Seidel (1821-1896) studied with Dirichlet in Berlin in 1840 and with 
Jacobi (and others) in Kénigsberg. Seidel's involvement in transforming Jacobi's method into 
the Gauss-Seidel scheme is natural, but the reason for attaching Gauss's name is unclear. 
Seidel went on to earn his doctorate (1846) in Munich, where he stayed as a professor for the 
rest of his life. In addition to mathematics, Seidel made notable contributions in the areas of 
optics and astronomy, and in 1970 a lunar crater was named for Seidel. 

628 
Chapter 4 
: 
Vector Spaces 
In other words, Gauss-Seidel determines 2x;(k) by using the newest possible 
information—namely, 21(k),22(k),...,vi-1(k) 
in the current iterate in con- 
junction with 241(k — 1), 2i+2(k —1),-.-,@n(k —1) from the previous iterate. 
This differs from Jacobi's method because Jacobi relies strictly on the old data 
in x(k —1). The Gauss-Seidel algorithm was known in the 1940s as the method 
of successive displacements (as opposed to the method of simultaneous displace- 
ments, which is Jacobi). Because Gauss-Seidel computes x;(k) with newer data 
than that used by Jacobi, it appears at first glance that Gauss-Seidel should be 
the superior algorithm. While this is often the case, it is not universally true—see 
Exercise 4.11.9 on page 641. 
Another major difference between Gauss-Seidel and Jacobi is that the order 
in which the equations are processed is irrelevant for Jacobi, but the value (not 
just the position) of the components 2x;(k) in the Gauss-Seidel iterate can change 
when the order of the equations is changed. Since this ordering feature can affect 
the performance of the algorithm, it was the object of much study at one time. 
Gauss-Seidel also enjoys 
a memory advantage over Jacobi because as soon as 
anew component 2;(k) is computed, it can immediately replace the old value 
xi(k—1), whereas Jacobi requires all old values in x(k —1) to be retained until 
all new values in x(k) have been determined. 
Something that both algorithms have in common is that diagonal dominance 
in A guarantees global convergence for each of them. To see this for Gauss— 
Seidel, let (A,z) be any eigenpair for H, and suppose that the component of 
maximal magnitude in z occurs in position m. Write (D —L)~!Uz = Xz as 
(D —L)z = Uz, and write the m*" row of this latter equation as \(d—1) = u 
where 
d == Onan eint 
dnesdcan 
Qmj2j, 
and 
u=—) mj; 
gsm 
j>m 
Diagonal dominance |dmm| > >) j2m |@mj| and |z;| < |2m| for all j yields 
ful + 1 =| > aamgzs| +] 30 mses] < lem ( Do lamal + Yo lem ) 
j<m 
j>m 
j>m 
< [2m|l@mm| = |d| => 
[ul < |d| — i. 
This together with A(d—1) =u and the backward triangle inequality (page 29) 
produces the conclusion that 
jul 
tu 
ALS erp 
ialelt 
<1, 
andthus 
p(H) <1. 
Note: While diagonal dominance in A guarantees convergence for both Jacobi 
and Gauss-Seidel, it is a rather severe condition that is not present in many 
applications. For example the discrete Laplacian matrix in (3.6.10) on page 390 

4.11 Difference Equations, Limits, and Summability 
629 
that results from discretizing Laplace's equation on a square is not diagonally 
dominant (e.g., look at the fifth row in the 9 x 9 matrix there). But such systems 
are always positive definite (Theorem 3.6.5), and there is a classical theorem 
stating that af A is positive definite, then the Gauss-Seidel iteration converges 
to the solution of Ax = b for every initial vector x(0). The same cannot 
be said for Jacobi's method, but, as mentioned in the note on page 624, M- 
matrices possess properties resembling positive definiteness, and Jacobi's method 
is convergent for M-Matrices (see Theorem 4.11.9 on page 623). 
Successive Over Relaxation (or SOR) Method 
The successive over relaxation (or SOR) method improves on Gauss-Seidel by 
introducing a real number w ¥ 0 (called a relaxation parameter) to form the 
splitting 
A = M-—N, where 
M=w-!D—L and N=(w7!—1)D+U. As 
before, D is the diagonal part of A (a;; 
#0 is assumed) and —L and —U 
contain the entries occurring below and above the diagonal of A, respectively. 
Since M~? = w(D —wL)-! =w(I—wD='!L)-!, the SOR iteration matrix is 
H,, =M7*N=(D-wL)"*{[(1-w)D+wU] =(I-wD7'L)"1[(1-w)I+wD7!U], 
and the k*" SOR iterate emanating from (4.11.25) is 
x(k) 
= H,,x(k — 1) +w(I—wD7!L)"'D~*b. 
(ar leae) 
This is Gauss-Seidel when w = 
1. Using w > 1 He called over relaxation, 
while taking w < 1 is referred to as under relacation! Writing (4.11.32) in the 
form (I —wD7!L)x(k) = [(1 —w)I+wD7!U]x(k —1)+wD~'b and consid- 
ering the 7*" component on both sides of this equality produces 
mikv= (1 -o)a(k = 1) — — (8; — So asjery(k) — > aya (k — 1) )). (4.11.33) 
uw 
J<t 
j>u 
The matrix splitting approach is elegant and unifying, but it obscures the simple 
idea behind SOR. To understand the original motivation, write the Gauss-Seidel 
iterate in (4.11.31) as %,(k) 
=2;(kK—1)+cx, where cz is the "correction term" 
—(b, — 5 a45%;(k) )— Laut ). 
j<t 
This suggests that the performance of the iteration can be affected by adjust- 
ing (or "relaxing") the correction term—i.e., by replacing cy with weg. The 
resulting algorithm, 7;(k) = %;(kK—1)+weg, is in fact (4.11.33), which produces 
Over relaxation is generally more effective than under relaxation, and hence the terminology 
"SOR" is the one that stuck. 

630 
Chapter 4 
; 
Vector Spaces 
(4.11.32). Moreover, it was observed in early developments that Gauss-Seidel 
applied to finite difference approximations for elliptic partial differential equa- 
tions such as Laplace's equation (page 390) often produces successive corrections 
c, that have the same sign, so it was reasoned that convergence might be ac- 
celerated for these applications by increasing the magnitude of the correction 
factor at each step (i.e., by setting w > 1). Thus the technique became known 
as "successive over relaxation" rather than simply "successive relaxation." It is 
not hard to see that p(H,,) <1 only if 0 <w <2 (Exercise 4.11.11), and it can 
be proven that positive definiteness of A is sufficient to guarantee p(H.,) < 1 
whenever 0 < w < 2. But determining w to minimize p(H,,) is generally a 
difficult task. 
Nevertheless, there is one famous special case' for which the optimal value 
of w can be explicitly given. If det (AD — L — U) = det (AD — BL — 8~!U) for 
all real A and 6 40, and if the iteration matrix Hy for Jacobi's method has 
real eigenvalues with p(H;) <1, then the eigenvalues A; for Hy are related 
to the eigenvalues A,, of H,, by 
(Aw +w—1)? =w?A2X,,. 
(4.11.34) 
From this it can be proven that the optimum value of w for SOR is 
and 
"<p (Huo) = Wot 
L- 
(4.11.35) 
2 
Wopt 
= —-—=— 
== 
" 
1+/1- (AD) 
Furthermore, setting w = 1 in (4.11.34) yields p(Hgs) = p?(H,z), where Hes 
is the Gauss-Seidel iteration matrix. For example, the discrete Laplacian L,,2 2 
on page 390 satisfies the special case conditions, and the spectral radii of the 
iteration matrices associated with L are 
Jacobi: 
p(H;) 
= costh 
= 1—(m*h?/2) 
(see Exercise 4.11.12), 
Gauss-Seidel: 
p(Hgs) 
= cos? th 
1 —77h?, 
1—sin zh 
P 
(Hop) 
1+ sin 7th 
eee 
in which h = 1/(n+1). Examining asymptotic rates of convergence reveals 
that Gauss-Seidel is twice as fast as Jacobi on the discrete Laplacian because 
This special case was developed by the American numerical analyst David M. Young, Jr. 
(1923-2008), who produced much of the SOR theory in his 1950 Ph.D. dissertation under the 
direction of Garrett Birkhoff at Harvard University. The development of SOR is considered 
to be one of the major computational achievements of the first half of the twentieth century, 
and it motivated at least two decades of intense effort in matrix computations. After some 
years in the aerospace industry in California, Young joined the faculty of the University of 
Texas, where he held the Ashbel Smith Chair for Mathematics and Computer Sciences. Young 
was instrumental in founding the University Computation Center, the Research Center for 
Numerical Analysis, and the Institute for Computational Engineering and Sciences at UT. 

4.11 Difference Equations, Limits, and Summability 
631 
Res = —logy)cos* th = —2log,)costh = 2Ry. However, optimal SOR is 
even better because 1 — 27h is significantly smaller than 1 — 72h? for even 
moderately small h. The point is driven home by looking at the asymptotic 
rates of convergence for h = .02 (n = 49) as shown below: 
Jacobi: 
Ry = .000858, 
Gauss-Seidel: 
Rog =2R;, ~ .001716, 
SOR: Ropt © .054611 © 32Res = 64Ry. 
In other words, after things settle down, a single SOR step on L (for h = .02) 
is equivalent to about 32 Gauss-Seidel steps and 64 Jacobi steps! 
Note: In spite of the preceding remarks, SOR has limitations. Special cases 
for which the optimum w can be explicitly determined are rare, so adaptive 
computational procedures are generally necessary to approximate a good w, 
and the results are often not satisfying. While SOR was a big step forward 
over the algorithms of the nineteenth century, the second half of the twentieth 
century saw the development of more robust methods (e.g., the preconditioned 
conjugate gradient method, the generalized minimal residual method (GMRES), 
and multigrid techniques) that relegated SOR to a secondary role. 
Limits of Powers 
Now broaden the focus to issues concerning when limz_,., A" exists but may be 
nonzero. Start from the fact that limp. A" exists if and only if limp_.. B* 
exists for each Jordan block in (4.11.5) on page 617. It's clear from (4.11.6) 
on page 618 that lim,-,.. BX cannot exist when 
|A| > 1, and the story for 
|A| <1 is already known (page 617), so only the case when || = 1 needs to be 
considered. If |\| = 1 with 
\ #1 (ie, 
\ =e with 0 < 6 < 27), then the 
diagonal terms \* oscillate indefinitely, and this prevents B* (and A* ) from 
having a limit. When \ = 1, 
: 
(7) i 
eer 
w > | 
(4.11.36) 
(*) 
1 
mxm 
has a limiting value if and only if m = 1, which is equivalent to saying that 
\ = 1 is a semisimple eigenvalue. But 
\ = 1 may be repeated p times so that 
there are p Jordan blocks of the form B, = [1]1.1. Consequently, limz—oo A* 
exists if and only if the Jordan form for A has the structure 
g=ptap= (i? ps 
0 
3) 
, where p= alg mult(1) and p(K) <1. 
(4.11.37) 
Now describe what limp. A* looks like. When p = 0, limg4..A* = 0 
(because p(A) < 1), but limg5.0 A® £0 when p # 0. The limit in this 

632 
Chapter 4 
Vector Spaces 
case can be evaluated in a couple of different ways. One way is to partition 
Qi 
, 
P= (Pi |P2) and P-! = (=), and write 
Qo 
A 
I 
0 
41 
I xp 
(8) 
ei 
' 
k 
ne 
xp 
= 
p 
fk Anxn = Le ( 
'0 oy P 
P( 0 
limp-so0 xe )P 
Qi 
=P('exe 
0 
\p-1— 
Ipxp 
0.) ( — 
4.11.38 
=i, ae 3 
= (Pi|P2)( Hs $)(=) 
( 
) 
= P,Q; =Gi, 
where Gj is the spectral projector onto N(A —I) along R(A —I) (see Theo- 
rem 4.10.3, page 598). Another way to evaluate limz_,o0 A® is to use f(z) = 2" 
in the spectral resolution theorem (page 598). If o (A) = {A1,A2,.-.,As} with 
1 = ry > |Ae| > -:- = |As|, and if inder(A;) = ki, where kj = 1, then 
linigeses (") Ne) =0 for 
i>2 (page 618), and 
s 
kj-1l 
A' = es ys (5) NIA NIV 
G; 
s 
kj-1 
= G,+ ys SS (Gra \I)G;3>G, 
as 
k—- oo. 
ieavyeoane 
Note that if p(A) < 1, then 
I— A is nonsingular, and N(I— A) = {0}, so 
regardless of whether the limit is zero or nonzero, 
limz_5., A*® is always the 
projector onto 
N (I— A) along R(I— A). The following theorem summarizes 
these observations. 
4.11.10. Theorem. For A ¢ F""", lim zp... A" exists if and only if 
p(A) <1 
or else 
(4.11.39) 
p(A) =1, 
where \ = 1 is the only eigenvalue on the 
unit circle and \ = 1 is semisimple. 
When it exists, 
jim, A* = G, [the projector onto N (I— A) along R (I— A)]. (4.11.40) 
Cesaro Summability 
With each scalar sequence {A}, 2, \3,...} there is an corresponding sequence 
of averages {j1, /12,U3,...} in which 
Ai + A2 
ONE 
Ag ae cen 
Hi=A1, 
p2= 
eee 
2 
n 

4.11 Difference Equations, Limits, and Summability 
633 
This sequence of averages is called the associated Cesaro' sequence, and when 
limn—co Mn = A, the sequence {\,} is said to be Cesdro summable to 
(or 
merely "summable") to 2. It can be proven (Exercise 4.11.13) that if {\,,} 
converges to A, then {in} converges to A, but not conversely. In other words, 
convergence implies summability, but summability does not ensure convergence. 
To see that a sequence can be summable without being convergent, notice that 
the oscillatory sequence {0,1,0,1,...} fails to converge, but it is summable to 
1/2, which is the mean value of {0,1}. This is typical because averaging has 
a smoothing effect so that oscillations that prohibit convergence of the original 
sequence tend to be smoothed away (or averaged out) in the Cesaro sequence. 
Similar statements hold for general sequences of vectors (Exercise 4.11.13), 
but Cesaro summability is particularly interesting when it is applied to a se- 
quence P = CAP IN, of powers of a square matrix A. Convergence criteria 
and the nature of limits were established in (4.11.39) and (4.11.40), so from 
now on, suppose that P fails to converge. The goal is to examine when P is 
summable and what P is summable to. 
It is common to say that A € F"*%" isa convergent matrix when limp_,o) A* 
exists and A is a summable matriz when limy...(I+ 
A+ A?+---+A*~!)/k 
exists. As in the scalar case, if A is convergent to G, then A issummable to G, 
but not conversely (Exercise 4.11.13). To analyze the summability of A in the 
absence of convergence, begin with the observation that A is summable if and 
only if the Jordan form J = P~'!AP for A is summable, which is equivalent to 
saying that each Jordan block B, in J is summable. Consequently, A cannot 
il 
be summable whenever p(A) > 1 because if B,= 
'.*. | is a Jordan block 
N 
in which |\| > 1, then each diagonal entry of (I+ B, +---+B%~) /k is 
1+A+---+A* 1 1 
(1--¥ 
od 
ee 8 
ee 
Stee 
4.11.41 
ose) 
k 
k\1-~A 
ae 
as 
( 
) 
and this becomes unbounded as k — oo. In other words, it is necessary that 
p(A) <1 for A to be summable. Since it has already been established that 
A is convergent (and hence summable) to 0 when p(A) < 1, only the case 
requiring analysis is that in which A has eigenvalues on the unit circle. 
We co (A) such thate | Ah ly 
eA7 491 ands hinder (A) > Al, then 
al 
1-—A 
there is an associated Jordan block B,= 
- ~ | «that is larger than 1x 1. 
a 
Ernesto Cesaro (1859-1906) was an Italian mathematician who worked mainly in differential 
geometry but also contributed to number theory, divergent series, and mathematical physics. 
After studying in Naples, Liége, and Paris, Cesaro received his doctorate from the University 
of Rome in 1887, and he went on to occupy the chair of mathematics at Palermo. Cesaro's 
most important contribution is considered to be his 1890 book Lezione di geometria intrinseca, 
but, in large part, his name has been perpetuated because of its attachment to this concept of 
"Cesaro summability." 

634 
Chapter 4 
; 
- 
Vector Spaces 
Each entry on the first superdiagonal of (I+ B, +--+ + Bk-!) /k is 05/0A 
(recall (4.10.2) on page 596). However, the term 05/0A oscillates indefinitely as 
k —» oo. In other words, A cannot be summable if there are eigenvalues \ # 1 
on the unit circle such that index(A) > 1. 
Similarly, if \ = 1 is an eigenvalue of index greater than one, then, again, 
A cannot be summable because each entry on the first superdiagonal of 
I+B, +--+ Be! 
| 
14-24+---4+(k-1) 
k(R—1) . k-4 
k 
ny 
k 
we 
Therefore, if A is summable and has eigenvalues \ such that |A| = 1, then it is 
necessary that inder(\) = 1. This condition also is sufficient—i-e., if p(A) = 1 
and each eigenvalue on the unit circle is semisimple, then A is summable. This 
follows because each Jordan block associated with an eigenvalue jz such that 
|u| <1 is convergent (and hence summable) to 0, and for semisimple eigenvalues 
such that |A| = 1, the associated Jordan blocks are 1 x 1 and hence summable 
because (4.11.41) implies that as k + oo, 
(em feementey emt || A 
bob beaviohiono die Wa 
Tee 
aa pe eS 
Lim\ 
aKa 
ok 
i] 
fore == "1: 
In addition to establishing a necessary and sufficient condition for A to be 
Cesaro summable, the preceding analysis also reveals the nature of the Cesaro 
A 
1 
limit because if A is summable, then each Jordan block B, = 
cn 
in 
a 
the Jordan form for A is summable, and the preceding analysis shows that 
(1)... #A= land mder(A)=1, 
Poe 
n ———_——————_ = 
(Ole 
if |A{| = 1, A #1, and index(A) =1; 
0 
Let Ay 
ek 
Consequently, if A is summable, then the Jordan form for A must look like 
iF 
bes 
k 
= 
I 
J=P'AP = ( 
ie 4) 
, 
Where 
p=alg mult,(A=1), 
(4.11.42) 
and the eigenvalues Ac of C are such that |Ac| < 1, or else |Ac| = 1 with 
Ac #1 and inder(Ac) = 1. Consequently, C is summable to 0, and hence J 
is summable to ea a Thus the Cesaro limit is 
I 
me 
k-1 
ie 
k-1 
+ A+ : + A 
-p ("+ : + J 
)Po4P ihe pte 
Comparing this expression with that in (4.11.38) on page 632 shows that the 
Cesaro limit is exactly the same as the ordinary limit, had it existed. In other 
words, if A is summable, then regardless of whether or not A is convergent, A 
is summable to the projector onto N(I— A) along R (I— A). The following 
theorem summarizes the established facts concerning Cesaro summability. 
—

q 
; 
'3 ye 
i — 
Sati hin 
| 
ate 
"IT 
& 
Proof. 
The equivalence of (4.11.43) and (4.11.44) and the properties of G were 
established in the previous section on Cesdro summability on page 632. To prove 
that (4.11.43) <= (4.11.45), suppose first that (4.11.43) holds so that either 
A* -—5 0 or else the unit eigenvalues are semisimple. This forces A* to be 
bounded because, as shown in (4.11.42) on page 634, the Jordan J form for A 
must then be given by 
Virgod 
& 
and the eigenvalues Ac of C are such that |Ac| < 1, or else |Ac| = 1 with 
Ac #1 and index(Ac) = 1. This implies that J* (and hence A*) is bounded. 
Conversely, if A* is bounded, then it follows from J* that p(A) <1; otherwise 
some block in J* looks like that in (4.11.6) on page 618 that is unbounded when 
|\| > 1, which is impossible because this would force A* to be unbounded. If 
p(A) <1, then there are no eigenvalues on the unit circle to worry about. When 
p(A) =1, the unit eigenvalues have to be semisimple because if one is not, then 
there will be a Jordan block B, that is 2 x 2 or larger such that 
Jie PotAP = é ) 
, 
where 
p=alg mult, (A=1), 
Cae 
Be oe) = 
||BE| +00 => ||A*|| +0, 
which is impossible. 
@ 

636 
Chapter 4 
Vector Spaces 
Stochastic Matrices 
One of the most common types of matrices exhibiting the properties in Theo- 
rem 4.11.11 are the stochastic matrices. 
A matrix P € R""" is stochastic (al- 
ternately, row stochastic) whenever P > 0 (entrywise) and all row sums equal 
1—i.e., Pe =e. This means that 1 € o (P) with eigenvector e and ||P||,, = 1. 
Therelare, p(P) =1 because 
1<p(P)<||P||,,=1 
(recall Corollary 3.1.9 on page 298). 
Since Pe =e, it also follows that P*e = P(Pe) = Pe =e, and iterating this 
observation k times produces P*e = e for integers k > 0, so P* is stochastic 
for each k > 0, and hence IPA = = 1 for k > 0. In other words, P is 
bounded. Thus the following corollary to Theorem 4.11.11 is obtained. 
4 
11. Dv. 
Corollary. When P is Leelee the following 
statements hold. 
ne _p(P)= 
= 1 
€ o(P). 
:  Allof the unit eee of P are semisimple. 
ve in. particular, p(P) =1 
is always a semisimple eigenvalue of P so 
that A = 0 is a semisimple eigenvalue of I — P,, or equivalently, 
index(I 
—P) = 1. 
e P is Cesdro summable to the spectral projector onto N (I—P) 
along R(I—P). 
Other valuable properties and applications of stochastic matrices are devel- 
oped in the discussion of Markov chains in section §7.9 on page 859. 
Example (A Shell Game) 
To illustrate the utility of Cesaro summability, consider a game in which a pea 
is placed under one of four shells, and an agile manipulator quickly rearranges 
them by a sequence of discrete moves. On each move the position of the shell 
containing the pea is interchanged with that of a neighboring shell by means 
of either a left or right shift according to the probabilities indicated in Figure 
Gin sak cen 
er Fa Nn celia 
FIGURE 4.11.2: MOVING A PEA UNDER A WALNUT SHELL 

4.11 Difference Equations, Limits, and Summability 
637 
When the pea is under shell #1, it is moved to position #2, and if the pea is 
under shell #4, it is moved to position #3. When the pea is under shell #2 or 
#3, it is equally likely to be moved one position to the left or to the right. 
Problem 1: Given information about where the pea might start, what is the 
probability of finding the pea in any given position after k moves? 
Problem 2: In the long run, what proportion of time does the pea occupy each 
of the four positions? 
Solution to Problem 1: Let p;(k) denote the probability that the pea is in 
position j after the k'" move, and translate the given information into four 
difference equations by writing 
A 
pilk)\ 
0 1/2 
0 
Oy 
/pi(k—1) 
po(k)=pi(k—1) + AAD 
pak) | | 
1 
0 
1/2 0 Wt po(k-1) 
pa(k-1) 
or 
= 
. (4.11.46) 
pa(k)=—5 — + pa(k—1) 
p3(k) 
0 
1/2. 
0 
1 § pa(k—1) 
pa(n) =P) 
pak)? 
\o 
0 
1/2 0/\pa(k-1) 
The matrix equation on the right-hand side is a homogeneous difference equation 
p(k) = Ap(k — 1) whose solution from (4.11.4) on page 617 is p(k) = A*p(0), 
and thus Problem 1 is solved. Note that A? is stochastic (alternately, A is 
column stochastic). If, for example, you are shown that the pea is initially under 
shell #2, then p(0) = e2, and after six moves the probability that the pea is in 
the fourth position is p4(6) = (A®e.) 4 = 21/64. If you are not shown where the 
pea starts, then the best you can do is to assume that it is equally likely to start 
under any one of the four shells. In this case p(0) = (1/4, 1/4, 1/4, 1/4)7, and 
the probabilities for occupying the respective four positions after six moves are 
the components in p(6) = A®p(0), or 
pi(6) 
11/32". 
0. 
21/64_-> 
0 
1/4 
43 
p2(6) \ _ 
0 
43/644 
O- 
21/32 
1/4) 
24 
85 
p3(6) | 
| 
21/32 
O 
43/64 
0 
1/4] 
~— 956 | 85 
pa(6) 
0 
21/64 
O- 
11/32 
1/4 
43 
Solution to Problem 2: There is a straightforward solution when A is a con- 
vergent matrix because if A* + G,; as k + o, then p(k) + Gip(0) =p, and 
the components in this limiting (or steady-state) vector p provide the answer. 
Intuitively, if p(k) 
+ p, then after awhile p(k) is practically constant, so the 
probability that the pea occupies a particular position remains essentially the 
same move after move. Consequently, the components in lim,_,.. p(k) reveal 
the proportion of time spent in each position over the long run. For example, if 
it turns out that limz_,.. p(k) = (1/6, 1/3, 1/3, 1/6)7, then, as the game runs 
on indefinitely, the pea is expected to be under shell #1 for about 16.7% of the 

638 
Chapter 4 
Vector Spaces 
time, under shell #2 for about 33.3% of the time, etc. But this analysis cannot 
be applied to our particular game. 
The Fly in the Ointment: The preceding analysis rests on the assumption 
that A is convergent. But A is not convergent for this particular shell game 
because a bit of computation reveals that o (A) = {+1, +(1/2)}. That is, there 
is an eigenvalue other than 1 on the unit circle, so Theorem 4.11.10 on page 632 
guarantees that limg—oo A® does not exist. Consequently, there is no limiting 
solution p to the difference equation p(k) = Ap(k—1), so the simple intuitive 
analysis suggested in the previous paragraph cannot be applied. 
Cesaro to the Rescue: However, A is summable because p(A) = 1, and 
every eigenvalue on the unit circle is semisimple, which are the conditions in 
Theorem 4.11.11. So, as k > oo, 
I+A+---+A*1 
k 
The job now is to interpret the meaning of this Cesaro limit in the context of our 
game. Focus on a particular position—say the j*" one—and set up "counting 
functions" (random variables) defined as 
) 
POE? Cipioreee 
_ J 1 ifthe pea starts under shell 7, 
2 tj otherwise, 
and 
ie { 
1 
if the pea is under shell j after the i*" move, 
0. otherwise, 
1=1,2,3,.... 
Notice that X(0) + X(1) +---+X(k—1) counts the number of times the pea 
occupies position j before the k'" move, so (X(0) + X(1) +-+-+X(k—1))/k 
represents the fraction of times that the pea is under shell j before the k*" 
move. Since the expected (or mean) value of X(i) is, by definition, 
E[X(i)] 
=1x P(X(i) =1) + 0x P(X(i) =0) =p, (3), 
and since expectation is linear (ie., E[aX 
(i) + X(h)] = aE[X(i)] + E[X(h)] ), 
the expected fraction of times that the pea occupies position j7 before move k 
is 
R eee _ E|X(0)] + E[X(1)] + --- 
+ E[X(k — 
1)| 
_ b;(0) ate 
<+ ph) 
ee 2 per = p(k — =| 
a fe + Ap(0) a 1 (Eeemee 
| 
J 
= Gip(0)| 

4.11 Difference Equations, Limits, and Summability 
639 
In other words, as the game progresses indefinitely, the components of the Cesaro 
limit p = G,p(0) provide the expected proportion of times that the pea is under 
each shell, and this is exactly what we wanted to know. 
Computing the Limiting Vector. Of course, p can be determined by first 
computing G,, but there is some special structure in this problem that can be 
exploited to make the task easier. Recall from Theorem 4.7.8 on page 550 that 
if \ is a simple eigenvalue for A, and if x and y* are respective right-hand 
and left-hand eigenvectors associated with \, then xy*/y*x is the projector 
onto N (A — AI) along R(A — AI). This can be used for the shell game because 
A = 1 is asimple eigenvalue for A. Furthermore, an associated left-hand eigen- 
vector is available for free, namely e? = (1, 1, 1, 1) because each column sum of 
A isone so that e7 A =e'. If x is any right-hand eigenvector of A associated 
with A = 1, then by noting that e?p(0) = p1(0) + p2(0) + p3(0) + pa(0) = 1, 
the limiting vector is determined to be 
xe! p(0) 
x 
i 
In other words, the limiting vector is obtained by normalizing any nonzero solu- 
tion of (A—I)x = 0 to make the components sum to one. Not only does (4.11.47) 
show how to compute the limiting proportions, it also shows that the limiting pro- 
portions are independent of the initial values in p(0). In other words, you do not 
have to know where the pea starts to solve the problem. Absolutely amazing! A 
simple calculation reveals that x = (1, 2, 2, 1)? is one solution of (A—I)x = 0, 
so the vector of limiting proportions is p = (1/6, 1/3, 1/3, 1/6)7. Therefore, 
if many moves are made, then, regardless of where the pea starts, the pea is 
expected to end up under shell #1 in about 16.7% of the moves, under #2 for 
about 33.3% of the moves, under #3 for about 33.3% of the moves, and under 
shell #4 for about 16.7% of the moves. 
Note: The shell game (and its analysis) is a typical example of a random walk 
with reflecting barriers, and these problems belong to a broader class of stochas- 
tic processes known as irreducible, periodic Markov chains. The shell game is 
irreducible in the sense of Exercise 4.2.39 on page 455, and it is periodic because 
the pea can return to a given position only at definite periods as reflected in the 
periodicity of the powers of A. More details are given in §7.9 on page 859. 
Exercises for section 4.11 
4.11.1. Verify that the expressions in (4.11.4) on page 617 are indeed the solu- 
tions to the difference equations in (4.11.3) on page 616. 

640 
Chapter 4 
Vector Spaces 
4.11.2. Which of the following are convergent, and which are summable? 
—1/2 
3/2 
-—3/2 
Oak 
0 
—1 
-2 
-3/2 
a= 
f b 1/2) vai(' 0 1) cal i 
@ 
i.) 
1 
—1 
1/2 
dO 
0 
1 
1 
3/2 
4.11.3. Projectors from Full-Rank Factorizations and SVD. Recall that a full-rank 
factorization of 
Ac F™*" with rank(A) =r is 
A= BmxrCrxn in 
which rank (B) =r = rank (C) (see page 190). 
(a) Prove that if A is 
nxn with r<n, andif R(A) and N (A) 
are complementary subspaces of F", then CB is nonsingular 
and P = B(CB)7~!C is the projector onto R(A) along N (A). 
Hint: Recall Corollary 4.7.12 on page 555 along with (2.4.13) 
and (2.4.14) on page 192. 
(b) Prove that if 
R(I— A) and N (I— A) are complementary sub- 
spaces of F", and if 
I— A = BC is a full-rank factorization, 
then Ay 
xn is summable to 
G = I—B(CB)7'!C. 
(c) Using the assumptions in part (a), lett 
A =U Ca 'i 
V* be 
a singular value decomposition as described in Theorem 3.5.2 on 
page 359, and let U and V be partitioned as U = [Uj | Up] 
and V =[V,|Vo] in which U, and Vj, are each n xr. Ex- 
plain why P = Ui(V{U,)~!V}% 
is the projector onto R(A) 
along N (A). 
Note: This can be a useful observation because all good ma- 
trix computation software contains numerically stable SVD im- 
plementations that return U; and Vj. Moreover, the factors 
from any URV factorization (page 461) can be used in the same 
way. In fact, even ordinary Gaussian elimination will produce 
full-rank factorizations—see Theorem 2.4.15 on page 190. 
4.11.4. Index and Full-Rank Factorizations. For A ¢ F"*" with \ € a (A), 
let 
M, = A —ATI, and consider the following procedure. 
Factor: 
My, = B,C, 
(perform a full-rank factorization) 
Define: 
Mz =Cj,B, 
(reverse the factors) 
Factor: 
Ms = Be2C2 
(perform a full-rank factorization) 
Define: 
M3 =C2Bo 
(reverse the factors) 
etc. 
In general, M; = C,_1B;_-1, where M;_; = B,_1C;_; 
is a full-rank 
factorization. 

4.11 Difference Equations, Limits, and Summability 
641 
4.11.5. 
4.11.6. 
4.11.7. 
4.11.8. 
4.11.9. 
(a) Explain why after a finite number of steps this procedure must 
produce a matrix My, that is either nonsingular or zero. 
(b) Prove that if k is the smallest positive integer such that My, 
is nonsingular or M; = 0, then 
index(d) = { 
k—1 
if Mg, is nonsingular, 
kK 6 at M;) = 0. 
—-3 
-8 
-—9 
(c)seForA re ( 
he 
1p 
°), 
use this procedure to find the index 
—1 
-2 
it 
of each eigenvalue in 0 (A) = {4,1}. Note: A is the same 
matrix used in Exercise 4.10.11 on page 613. 
=) 
tel 
cat') 
Let A= ( 
"cee 
yl 
°) 
(the matrix from part (c) in Exercise 4.11.4). 
—l 
—=2 
1 
(a) Find the Jordan form for A. 
Hint: Use the results of part (c) in Exercise 4.11.4. 
(b) For any function f defined at A, find the Hermite interpolation 
polynomial defined in (4.10.19) on page 603, and then express 
f(A) asa polynomial in A. 
For each matrix in Exercise 4.11.2, use the results of Exercise 4.11.3 to 
evaluate the limit of each convergent matrix, and evaluate the Cesaro 
limit for each summable matrix. Hint: Use Theorem 2.4.15 on page 190. 
Verify the results obtained for the shell game in the example on page 636 
by first computing the Cesaro limit G; using Exercise 4.11.3 along with 
Theorem 2.4.15 on page 190, and then determine the limiting vector 
TeerAgieeet 
Akt 
p= lim (A p(0) 
for any initial vector p(0) > 0 with e?p(0) = 1. 
Must it be true that if there is some matrix norm such that ||A|| < 1, 
then 'limz),6, A* =07? 
By examining the iteration matrix, compare the convergence of Jacobi's 
method and the Gauss-Seidel method for each of the following coefficient 
matrices with an arbitrary right-hand side. Explain why this shows that 
neither method can be universally favored over the other. 
ite 
py 
gp) 
Doe 
eel 
A=(i 1 
) Aa=( ie 
2) 
ome 
Ti 
te 
=t 
2 

642 
Chapter 4 
_ Vector Spaces 
2 -1 
0 
4.11.10. Consider the 3 x 3 finite-difference matrix A = (-1 : -1) from 
the two-point boundary value problem in (2.6.4) on page 221. 
(a) Verify that A satisfies the special-case determinant conditions 
given in the discussion of the SOR method on page 630 that 
produces the optimum SOR paramater in (4.11.35). 
(b) Compute the value of this optimum SOR relaxation parameter. 
(c) Find the asymptotic rates of convergence for Jacobi, Gauss— 
Seidel, and optimum SOR for A. 
(d) Use x(0) = (1,1, 1)? and b = (2, 4, 6)" to run through sev- 
eral steps of Jacobi, Gauss-Seidel, and optimum SOR to solve 
Ax =b 
until you can see a convergence pattern. 
4.11.11. Prove that if p(H,,) <1, where H,, is the iteration matrix for the SOR 
method, then 0 < w < 2. Hint: Use det (H,,) to show |Ax| > |1 —w| 
for some Ay € a (H,). 
4.11.12. Let H be the Jacobi iteration matrix for the discrete Laplacian L,,2,,2 
in (3.6.10) on page 390. Show that p(H) =cosz/(n+1). Hint: Con- 
sider the proof of Theorem 3.6.5 on page 390. 
4.11.13. Consider a scalar sequence {Aj, A2,A3,...} and the associated Cesaro 
sequence of 
averages {j/1, U2, 
U3,---.}, where Wp = (Ay +Aq4+---+An)/n. 
Prove that if {An} converges to A, then {,} also converges to X. 
Note: Like scalars, a vector sequence {v,} C F" converges to v if and 
only if for each € > 0 there is a natural number N = N(e) such that 
|Vn — v|| < € for all n > N, and by virtue of the fact that all norms 
for F" are equivalent (Theorem 1.5.5, page 35), it doesn't matter which 
norm is used. Therefore, your proof should also be valid for vectors and 
matrices. 
4.11.14. M-Matrices Revisited. Matrices A € R"*" with nonpositive off-diagonal 
entries are called Z-matrices. For the class of Z-matrices prove that the 
following statements are equivalent. 
(a) A is an M-matrix. 
b) All leading principal minors of A are positive. 
) A hasan LU factorization, and both L and U are M-matrices. 
) There exists a vector x > 0 such that Ax > 0. 
) Each a;; >0 and AD is diagonally dominant for some diago- 
nal matrix D with positive diagonal entries. 
(f) Ax>0O implies x > 0. 

4.11 Difference Equations, Limits, and Summability 
643 
4.11.15. 
4.11.16. 
4.11.17. 
For A € F"*", prove that if lim,.. A* exists, and if 
B=I-A, 
then 
lim A* =1-—BB?*, 
k—-co 
where B* is the group inverse of B as defined on page 558. In other 
words, this says that the limiting matrix can be characterized as the 
difference of two identity elements—namely I, the identity in the multi- 
plicative group of nonsingular matrices, and BB*, the identity element 
in the multiplicative group containing B. 
Let A = BC bea 
full-rank factorization of a group matrix A € F"*". 
(a) Explain why the group inverse of A is A* = B(CB)~2C, In 
particular, this means that if A = (U,)(DV7) or, alternately, 
A = (U,D)(V}) 
is the full-rank factorization derived from a 
singular value decomposition A = U 3 a 
V* in which 
U =[U;|U2] 
and V = [Vi|V2], where U, and V; are 
each n xr, then A* =U ,(V%AU}))~!V%. 
(b) Show that A* is also given by A# = A(A3)'A, where At is 
the Moore—Penrose inverse of A. 
Prove that if A € F"*" is a group matrix with rank(A) 
=n—1, 
and 
if A = UDV* is a singular value decomposition of A, then the group 
inverse of A is given by 
Un 
A#=PAtP, 
where 
P=I- "7% 
Ur Vn 
in which u, and v, are the respective last columns in U and V, and 
At is the Moore—Penrose inverse of A. 

——- 
F 
ro 
ee "4 
a 
' 
: 
'3 : 
a 
2 
we 
ay 
+ 
[dink at tw wiskp® fal 
ee 
1 itentinin 
oh COMERS 
as AY dell 
eee 
ot meds 
PL 
tat ie 
inst 
7 7 
— 
: 
Z 
: 
' ah z. LE mat dete 
ee 
2 
' 
' 
e 
as 
at 
£ 
| 
'Ss 
Seear 
Ue 
+t) 0d ghee Xe 
ee 
'es 
tite a 
A exe themes: 
oe Neti 
pit 
® 
oy. 
' 
- 
; 
oe 
ene) 
Vv 
we 
: 
ae 
2 A= 
We share a philosophy about linear algebra: we think basis-free, 
but when the chips are down we close the office door 
and compute with matrices like fury. 
— Irving Kaplansky (1917-2006) speaking 
about Paul Halmos (1916-2006) 
Predictions are hard, especially about the future. 
— Niels Bohr (1885-1962) 

CHAPTER 
5 
Inner Product Spaces 
and 
Fourier Expansions 
5.1 
INNER PRODUCT SPACES 
The standard inner product of vectors x,y € F"*! as given in Definition 1.4.4 
on page 25 is conveniently described (and computed) by summing the products 
of the vector coordinates, or more simply by row-by-column multiplication x*y. 
However not all vector spaces are coordinate spaces—page 411 contains common 
examples of non-coordinate spaces. 
The standard inner product does not apply to non-coordinate spaces, so it 
is desirable to somehow extend the standard inner product concept to define a 
"veneral inner product" that will work in all cases. The properties of the standard 
inner product (1.4.8)—(1.4.12) on page 25 are the basis for doing so. 
5.1.1. Definition. An inner product on a vector space Y over F isa 
function that maps each ordered pair of vectors x and y to a scalar 
(x|y) € F such that the following four properties hold. 
(x|x) € R with (x|x) > 0, and (x|x) 
=0 <> x =0 
(5.1.1) 
(xlay) = a(xly) for allae F 
(5.12) 
(x|y +z) = (xly) + (x|z) 
(5.1.3) 
(xly)=@IX) 
 GFF=R, then (xly)=(ylx)) 
(6.1.4) 
A vector space V over F that is equipped with an inner product (x|y) 
is called an inner-product space. 

646 
Example 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
For a fixed x € V the inner product is linear in right-hand argument, 
and for a fixed y € V the inner product is conjugate linear in the left-hand 
argument—i.e., (x|jay 
+ z)= a (x|y)+(x|z) and (ax 
+ y|z) = @ (x|z) + (y|z) . 
For = F* with the standard inner product these correspond to saying that 
x*(ay +z) = a(x*y) 
+ (x*z) and (ax 
+ y)*z = @(x"z) + y"z. 
e 
Clearly, F" with the standard inner product (x|y) = x*y is an inner prod- 
uct space. 
e 
However, F" has many different inner products. For example, if A € (ees 
is any fixed nonsingular matrix, then (x|y) = x*A*Ay is an inner product 
for F". This inner product is sometimes called an A-inner product or an 
elliptical inner product. 
e 
For A,BéF"*", the function defined by 
(A|B) = trace (A*B) 
(5.1.5) 
is an inner product for F"*". In the real case, (A|B) = trace (A7B) is an 
inner product on R™*"". These are referred to as the standard inner products 
for matrices. They reduce to the standard inner products for vectors when 
(ian 
e 
An inner product for the vector space V of real-valued integrable functions 
defined on an interval (a,b) is given by (f\g) = f f(t)g(t)dt. Sometimes a 
weighting Peete w(t) is incorporated to define the inner product on V to 
be (flg) =f. f(t)g(t)w(t)at 
Norms, Inner Products, and the CBS inequality 
Just as the standard inner product for F"*! produces the euclidean norm on 
F"*" by ||x||2 = Vx*x, every general inner product on an inner-product space 
Y defines a more general norm on Y by setting 
III = Vl). 
(5.1.6) 
By the definition on page 34, a general vector norm must obey three properties. 
<f, 
0 \ and" 
|x|; a 
= O 
(5.1.7) 
llox|| = Ja] |x| 
(5.1.8) 
IIx +y|| < |lx|| + lly|| 
(triangle inequality) 
(5.1.9) 
It is straightforward to verify that (5.1.6) satisfies (5.1.7) and (5.1.8), but, just as 
in the case of euclidean norms, verifying that (5.1.6) satisfies the triangle inequal- 
ity requires a generalized version of the Cauchy—Schwarz (or CBS) inequality on 
page 27. 

5.1 Inner Product Spaces 
647 
5 
1. 
2. Theorie: 'General CBS 
$ inequality? tee vy e an innerproduct 7 
space. If fell is defined to be Isl = V a then 
ye vi forall xuyeV. 
(6.1.10) 
Equality holds ce and 
only 
i. ye 
= oe for a= 
= (xly) / x2. 6410 
: 
Proof. 
The proof is essentially the same as that for Theorem 1.4.5 on page 27 
except that x*y is replaced by (x|y), but for the sake of completeness, the 
details are repeated in general inner-product notation. Set a = (x|y) / ||x||7 
(assume x # O, for otherwise there is nothing to prove), and observe that 
(x|ax — y) =0. Linearity and conjugate linearity of the inner product yield 
2 
a 
0 < |lax —y||" = (ax — ylax — y) = @ (x|ax — y) — (ylax — y) 
ly 
I" Ill? — (xly) (yx) 
lx" 
i 
Since (y|x) = (x]y), it follows that (x|y) (y|x) =|(xly)|?, so 
—(ylax —y) 
= (y|y) — a(y|x) = 
liv 
I? Wall? = [xly)? 
0< 
>|" 
| (xy) |< [lll llyll- 
The equality condition (5.1.11) was established in Exercise 1.4.9 on page 30. 
The job of showing that ||x|| = \/(«|x) is a legitimate vector norm can now 
be completed by using the CBS inequality to prove the validity of the triangle 
inequality in (5.1.9). 
5.1.3. Theorem. If 
is an inner-product space, then ||x|| = \/ (|x) is 
a norm on Y in the sense of Definition 1.5.3 on page 34. 
Proof. 
The fact that 
||*|| = 
./(*|*) satisfies the first two norm properties 
(5.1.7) and (5.1.8) follows directly from the definition of an inner product—see 
Exercise 5.1.10. To establish the triangle inequality in (5.1.9), use 
(x|y) + (y|x) = (xly) + (ly) = 2Re ((xly)) <2] (xly) | 
together with the CBS inequality to write 
Ix tyl|? = (x +y|x+y) = (|x) + (xly) + (ylx) + (ly) 
< |lx||? +2] (xly)| + llyll? < (xi +llyi)?. 

648 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
Example (Matrix and Function Norms Generated by Inner Products) 
e 
The standard inner-product (A|B) = trace(A*B) on F™*" generates the 
Frobenius matrix norm because 
(A|A) = trace (A*A) = Wee 
(5.1.12) 
e 
The standard inner-product (f bs 
g =f f(t)g(t)dt on the space V of real- 
valued integrable functions on (a, b) Aes anorm on Y by setting 
fll = V(FIP) 
t)2dt. 
(5.1.13) 
Orthogonality, enc Gi 
and Projection 
Replacing x*y by (x|y) and setting ||x|| = \/(*|x) produces definitions and 
properties of orthogonality, angles, and projections in general inner-product 
spaces that echo the same concepts as those in F" equipped with the standard 
inner product. Below are some of the more notable ones. 
~—«$.1.4. Definition. Let x,y,u € VY be nonzero vectors in an inner-product 
space with inner product (x|x), where ||ul| = 1. 
oe 
x and y are defined to be orthogonal (to each other) whenever 
(x|y) = 0. 
: 
e When Y is real, the cosine of the angle 0 € 0, 7] between x and 
y is defined to be cos@ = (x|y) /||x|| |ly||, where ||*|| = 
/(«|x). 
e 
p= (u|x)u 
is the orthogonal projection of x onto 
the subspace (or line) spanned by u, and | (u|x) | pro- 
(5.1.14) 
vides an intuitive sense of how much of x lies in the 
direction of u. (see pages 101-102). 
e 
If {x,,x2,...,X,} is a set of mutually orthogonal vec- 
tors in V, then the generalized Pythagorean theorem 
(5.1.15) 
holds in the sense that ||>°, x; ||? = ia. 
For example, the angle 6 between A = € iy and B= 6 att with 
respect to the standard inner product in (5.1.5) is 7/2 because 
(A|B) 
_ trace (A7B) 
NAllrliBllp  ||All- liBlle 
Caution! Be careful to distinguish between "orthogonal matrices" 
(i.e., those 
for which P? = P~') and matrices that are "orthogonal to each other" (such 
as A and B given above). 
cos @ = 
=, 
oLe., A LB: 
Continuity of Inner Products 
The CBS inequality guarantees that inner products are continuous functions of 
their arguments. 

5.1 Inner Product Spaces 
649 
Proof. 
Use the properties (x|y + z) = (x|y) + (x|z) and 
(x + y|z) = (2|x + y) = (zx) + (zly) = (z|x) + (zly) = (xlz) + (yz) 
along with the CBS inequality to obtain 
(xk lye) — (x|y) = (Xe — Xlyx) + (xlyn — y) 
< ||k« — xll llyell + [xl] lyk -yl| > 0. 
What Other Norms Are Generated by Inner Products? 
Since each inner product generates a norm by the rule 
||x|| = \/(«|x), it is 
natural to ask if the reverse is also true. That is, for each vector norm 
||x|| 
on a space VY, does there exist a corresponding inner product on V such that 
/ (x|x) = ||*||? If not, under what conditions will a given norm be generated by 
an inner product? This is a substantial question with no obvious answer. It took 
the combined efforts of Maurice R. Fréchet' and John von Neumann' to unravel 
the mystery and to provide an elegant solution. 
5.1.6. Theorem. 
For a given norm 
||*|| on a vector space V, there 
exists an inner product on V such that (x|*) = ||«||' if and only if the 
parallelogram identity 
llx + yll? + lx — yl? = 2( xl]? + lly?) 
(5.1.16) 
holds for all x,y € V. 
Maurice René Fréchet (1878-1973) began his illustrious career by writing an outstanding 
Ph.D. dissertation in 1906 under the direction of the famous French mathematician Jacques 
Hadamard (page 55) in which the concepts of a metric space and compactness were first formu- 
lated. Fréchet developed into a versatile mathematical scientist, and he served as professor of 
mechanics at the University of Poitiers (1910-1919), professor of higher calculus at the Univer- 
sity of Strasbourg (1920-1927), and professor of differential and integral calculus and professor 
of the calculus of probabilities at the University of Paris (1928-1948). 
Born in Budapest, Hungary, John von Neumann (1903-1957) was a child prodigy who could 
divide eight-digit numbers in his head when he was only six years old. Due to the political unrest 
in Europe, he came to America, where, in 1933, he became one of the six original professors 
of mathematics at the Institute for Advanced Study at Princeton University, a position he 
retained for the rest of his life. During his career, von Neumann's genius touched mathematics 
(pure and applied), chemistry, physics, economics, and computer science, and he is generally 
considered to be among the best scientists and mathematicians of the twentieth century. 

650 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
Proof. 
Consider real spaces—complex spaces are handled in Exercise 5.1.19. If 
there exists an inner product such that (x|*) = \|*1|? , then the parallelogram 
identity is immediate because (x + y|x + y)+(x — y|x — y) = 2(x|x)+2 (yly) . 
The difficult part is establishing the converse. Suppose that \|x|| is a given norm 
that satisfies the parallelogram identity, and prove that the function defined by 
((xly)) = 5 
(Ie +yll? - xy) 
(5.1.17) 
is an inner product for VY such that ((x|x)) = [><]? for all x by showing 
the four defining conditions in Definition 5.1.1 on page 645 hold. The first and 
fourth conditions (5.1.1) and (5.1.4) are immediate. To establish the third con- 
dition (5.1.3), use the parallelogram identity with x replaced by (x+y) and 
y replaced by (x +z) and then with x replaced by (x —y) and y replaced 
by (x —z) to write 
2 
2 
2 
lx 
+ 
yll? + x +2l|? = 5 (x+y +x+2l)" + lly 
—2l), 
1 
2 
2 
ok 
2 
2 
IIx yl" + |lx— 
zl" =5Clk-y 
+x— all + [lz —yIl ). 
Subtraction produces 
2 _ |l2x+(y+z)|/° —|l2x—- (y+2)Il" 
2 
Ix 
+ 
y|/?—llx — yl? +l]x + 2l|°—[lx — 2 
5 
Now use (5.1.17) to obtain 
((xly)) + ((x|z)) = a( 
Ix + yl? — [Ix — yl? + |x + 
zl)? — |x - zl') 
= a( lax + (y +. 
z)||? — 2x — (y +2)||7) 
(5.1.18) 
"3 (ff p27) AGE) 
Setting z = O produces the statement that 
((x|y)) = 2((x|y/2)) 
for all 
y € V. Replacing y by y+z yields ((x|y+z)) = 2((x|(y+z)/2)), and 
thus (5.1.18) ensures that ((x|y)) + ((x|z)) = ((x|y +z)). Now prove that 
((x|ay)) =a ((x|y)) for all real a. This is valid for integer values of a by the 
result just established, and it holds when a is rational because if 8 and y are 
integers, then 
fy) )=2 
(ly). 
(Ge 
7 
ss 
cy 
=¥)) = (Crxl6y)) = 81 (Gely)) 
((x 
Because 
||x+ay|| 
and ||x—ay|| 
are continuous functions of a 
(Exercise 
5.1.14), (5.1.17) guarantees 
((x|ay)) 
is a continuous function of a. There- 
fore, if @ is irrational, and if {a,} is a sequence of rational numbers such that 
On — a, then ((x|any)) > ((x|ay)) and ((x|any)) = an ((x|y)) + a ((x|y)) , 
so ((xlay)) =a((x|y)). 
Wl 

5.1 Inner Product Spaces 
651 
Example (p-norms and Inner Products) 
Since the euclidean 2-vector norm on F" is generated by the standard inner 
product via ||x||, = /x*x, Theorem 5.1.6 guarantees that the parallelogram 
identity (5.1.16) must hold. This is corroborated by observing that 
llx+yl3 + x—yllp =(«+y)*(x+y) 
+ (x-y)*(x-y) 
= 2(x"x +y*y) = 2(||x|I3 + llyll3). 
The parallelogram identity is so named because it expresses the fact that the 
sum of the squares of the diagonals in a parallelogram is twice the sum of the 
squares of the sides as illustrated below in Figure 5.1.1. 
FIGURE 5.1.1; THE PARALLELOGRAM IDENTITY IN R?. 
What about the other p-norms—are any of them generated by an inner 
product? Without Theorem 5.1.6 it would be difficult to provide an answer, 
but Theorem 5.1.6 makes it easy to see that the answer is "no" because the 
parallelogram identity (5.1.16) only holds when p= 2. To see that 
Ix + yll> + lx —yllp A 2(Ikxl3 + llyll2) for all x,y €F" when p #2, 
let 
x =e, 
y =e€2 so that |ley + ea ||° = 2?/P = lle, — e2||.,. Consequently, for 
all integers p > 0, 
llex + e2l|> + llex — eal), =2°*?/? 
and 2(|lex||> 
+ 
lleall>) = 4. 
Except when p = 2, 2(?+?)/P 4 4 (even as p — 00), so Theorem 5.1.6 guar- 
antees that the euclidean norm is the only norm on F" that is generated by an 
inner product. 
Conclusion: Since inner products and p-norms for p # 2 do not play well 
together, the analysis of problems involving orthogonality (e.g., least squares 
problems or SVD related applications) must necessarily revolve around 2-norm 
considerations. 

652 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
Adjoint Operators 
; 
For matrices A € F"*", the conjugate transpose A* plays a central role in 
both theory and applications, but for a linear operator T on a more general 
vector space Y where matrices are not present, there is a way to define a linear 
operator T* that is the natural extension of the conjugate transpose—provided 
that V has an inner product. 
mK 
5.1.7. Definition. Let T be a linear operator on an inner-product space 
— 
~ Y. A linear operator T* such that (x|T(y)) = (T*(x)|y) for all x,y 
in V is called 
an adjoint operator of T. In many common situa- 
— 
tions (e.g., when V is finite dimensional or more generally when T is 
- continuous), T* exists and is unique. 
ty 
Consider A € F"*" as a linear operator on F", where A(y) = Ay for 
each y € F"™? (ie., ordinary matrix-vector multiplication). Under the standard 
inner product for F" the adjoint of A reduces to A* (the conjugate transpose 
of A) because 
(xlAy) =x" Ay =x"(A*}*y. = (A"x)*y = (Afxly) 
(for all x 
yer; 
so, by definition, A* is the adjoint of A. For this reason, instead of calling A* 
the conjugate transpose, it is common to refer to A* as the adjoint matriz! 
Example 
Let V be the real inner-product space of real-valued functions with continuous 
second derivatives that have h(0) = h(1) = 0 for all h € V. Let the inner 
product of f,g € V be (f\g) = B f(t)g(t)e'dt. This is a weighted inner product 
as mentioned in the example on page 646 with w(t) =e'. 
Problem: Determine the adjoint of the second derivative operator T(h) = h". 
Solution: For all f,g € V, use integration by parts twice to compute 
1 
I 
(f|T(9)) 
i, 
1 
/ f(t)g!(etdt 
= 
F(t)g'(t)e| 
i} 
a (t)(f"(t) + f(t))etat 
0 
0 
0 
II 
i 
1 
{H+ eOlave'|,— [Ur +270 + FOlacetae) 
=(f" + 27"+ fla). 
This holds for all f,g € V, so, by definition, the operator T* 
defined by 
T*(h) = h(t) + 2h'(t) + A(t) is the adjoint of T. 
Some older texts used the term "adjoint matrix" to refer to the adjugate matrix (the transpose 
of the matrix of cofactors) as defined on page 960, but in modern literature the terms "adjoint" 
and "adjoint matrix" are as defined here. 

5.1 Inner Product Spaces 
653 
Example 
Let V be the same vector space used in the previous example, and let T be the 
same second-derivative operator used in the previous example. 
Problem: Instead of the con 
inner product used above, use the unweighted 
inner product (f|g) = ee ( 
t)dt to determine the adjoint of T. 
Solution: Again use integration by parts twice to compute 
"in = fi soa" eee 
= ey 
- f soso 
=-{r@s|- 
festa) =(r"1a) = rN). 
This time it turns out that the adjoint of T is T* = T. This is an example 
of what is called a self-adjoint (or hermitian) operator—i.e., operators that are 
their own adjoints. 
Adjoint Properties 
Properties of adjoint operators on an inner-product space VY over F mimic those 
of conjugate transposes of matrices. For example, if L,T,S are linear operators 
on Y, and if the indicated operations exist, then the following properties are 
immediate consequences of Definition 5.1.7. 
e 
(T+S)* =T*+S* 
e 
(aT*)=aT™ forall 
acF 
o(T*)* = 0 
oe 
[* =] 
e 
(TL)*=L'T* 
e 
IT' exists <=> (T*)—! exists, in which case (T—*)* =(T*)~' 
e 
Analogous to matrices, a normal operator is defined to be 
one for which T*T = TT*, and a unitary operator is one 
(Gack 
for which T* = T~! (see Exercise 5.1.21 on page 657). 
Linear Functionals and Inner Products Revisited 
Recall from Definition 4.5.16 on page 519 that a linear functional f on a vector 
space VY over F isa 
linear function that maps vectors in Y to scalars in F, and 
the dual space V* of VY is the set of all linear functionals on V. It was shown in 
Theorem 4.5.18 on page 521 how linear functionals on an n-dimensional space 
VY can be viewed as standard inner products on F". The next result extends 
this statement to more general inner products on VY. 

bee! vera eA 
. 
t+ 
4 
Hee, 
Proof. 
Let B= {xi,X2,...,Xn} be an orthonormal basis' for ee: respect 
a 
1 
a =Jj > 
to the underlying inner product so that (x;|x;) = dj; = 
Nueite 
gt Set 
u = >>, f(xi)x;, and for each v € V expand v = }),&;x;. Observe that 
f(x;) = (u|x;), and use the inner product properties (5.1.1)-(5.1.4) to obtain 
f(v) = #06) aye Ef 
ou) =e eplahg) = ( 
Dew) = (ulv). 
j 
j 
i 
j 
Uniqueness is immediate because if f(v) = (u|v) = (u|v) for all v € V, then 
(u—ulv)=O0Vv = > u=uU 
(see Exercise 5.1.7, page 655). 
Mf 
Exercises for section 5.1 
a 
Yl 
Walle tor x= (= ) 
ave (:: 
), 
determine which of the following are inner 
£3 
y3 
products for R°*?, 
(a) 
(xly) = z1y1 + rays, 
(b) 
(xly) = 21y1 — Zaye + ray, 
(c) 
(xly) = 2r1y1 + tayo + 4xrgy3, 
(d) (xly) = viyy + 23y3 + 23y3. 
2 
1 
5.1.2. Determine the angle between x = (-1) and y = (:). 
1 
2 
5.1.3. Using the standard inner product for R"*", determine the angle be- 
tween the following pairs of matrices. 
(a) To fe and 
Baa ma 
(b) As a and 
Bea 'ful 
; The Gram—Schmidt procedure on page 658 shows how orthonormal bases can be constructed. 

5.1 Inner Product Spaces 
655 
5.1.4. 
5.1.5. 
5.1.6. 
alee s 
5.1.8. 
5.1.9. 
5.1.10. 
5.1.11. 
5.1.12. 
Why is the definition for cos@ given in Definition 5.1.4 on page 648 not 
good for C"? Explain how to define cos@ so that it makes sense in C". 
Consider a real inner-product space with ||*||? = (*|+). Prove that if 
IIxl| = lly], then x+y) 1 (xy). 
Construct an example using the standard inner product in R" to show 
that two vectors x and y can have an angle between them that is close 
to 7/2 without xy being close to 0. Hint: Consider n to be large, 
and use the vector e of all 1's for one of the vectors. 
For a general inner-product space V, explain why each of the following 
statements must be true. 
(a) If (x|y) =0 forall 
xe VY, then y =0. 
(b) 
(ax|y) =@(xly) for all x,y € V and for all scalars a. 
(c) 
(x+yl|z) = (x|z) 
+ (y|z) for all x,y,z€ 
V. 
Let T and L be linear operators on a general inner-product space V. 
(a) Prove that if (u|T(v)) = (u|L(v)) for all u,v € Y, then T= 
L. (This is the generalization of the fact that for A,B ¢ F"*" 
and for the standard inner product on F", x*Ay = x*By for 
all x,y € F" guarantees that A = B.) 
(b) Explain why (v|v) = (v|L(v)) 
for all v € V implies that 
ibe 
8 & 
Alternate Characterization of Unitary Operators. Let T be a linear op- 
erator on an inner-product space V with 
||x|| = 
\/(«|x). Prove that 
|Tv || = ||v|| for all v € V if and only if (T(x)|T(y)) = (xly) for all 
x,y € V. Explain why this means that T is a unitary operator if and 
only if Tvl) = |v. 
Let V be an inner-product space with an inner product (x|y). Explain 
why the function defined by ||x|| = 
./(«|x) satisfies the first two norm 
properties in (1.9.4) on page 84. 
Does the Frobenius matrix norm on F""*" satisfy the parallelogram 
identity in (5.1.16)? 
For A €F"*" with n> 2, consider the induced matrix norms 
||A||, , 
|A||,, and ||A]|,,. Are any of these generated by some inner product 
one i 

656 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
5.1.13. 
5.1.14. 
5.1.15. 
5.1.16. 
aed 
bea Ege 
Dekelos 
Let @ be the angle between two nonzero vectors x and y from a real 
inner-product space. 
(a) Prove that cos@=1 if and only if y=ax for a>0. 
Hint: Look at the proof of Theorem 5.1.2 on page 647. 
(b) Prove that cos@ = —1 if and only if y=ax for a<0. 
Verify that the proof given on page 29 of the backward triangle inequality 
| xl — Ilyll] < lx —yIl 
is valid for every norm defined by (5.1.7)-(5.1.9) on a general vector 
space VY, and then explain why ||x|| is a continuous function of x. 
For a real inner-product space with Il*||? = (x|x), derive the inequality 
2 
2 
(x|y) < os 
Hint: Consider x — y. 
With respect to the standard inner product for matrices given by (5.1.5) 
on page 646, verify that the set 
ont 
eo; 
Serre 
2h 
A 
fia 
1 
ee 
p={({ mpegs 
Wo ten fe (ya) ges ai 
is an orthonormal basis for R?*? 
For A,B € F""*", establish each of the following inequalities. 
) |trace (B)|* < n [trace (B*B)). 
) |trace (B?)| < trace (B*B). 
trace (A? A) + trace (B7™B 
(c) trace(A'B) < 
RECA 
Bala for real matrices. 
) |trace ([ AB)?)| < trace (A?B?) for hermitian matrices. 
General Pythagorean Theorem. 
Let V bea 
general inner-product 
space with |x|? = (x|x) . 
(a) When y is a Ge ante prove that x 1 y if and only if 
|x+y||? = |x|]? + llyll?. (Something would be wrong if this 
were not true because this is where the definition of orthogonal- 
ity originated.) 
(b) Construct an example to show that one of the implications in 
part (a) does not hold when Y is a complex space. 
(c) When Y G a comple apa prove that x 1 y if and only if 
lax + By||? = |jax||? + \|Gy||" for all scalars a and 8. 

5.1 Inner Product Spaces 
657 
5.1.19. 
5.1.20. 
5.1.21. 
Extend the proof of Theorem 5.1.6 on page 649 from real spaces to 
complex spaces. Hint: Suppose that V is a complex space with a norm 
||x|| that satisfies the parallelogram identity (5.1.16), and set 
2 
2 
Bear 
Nae IPG = AY 
(ly), = 
bey —Ie—yl 
Prove that the function defined by ((x|y)) = (xly), +i(ix|y), is an 
inner product on Y. Note: This expression for ((x|y)) is often called 
the polarization identity. 
Let T be a linear operator on an n-dimensional complex inner-product 
space V whose basis is B = {x1,X2,...,Xn}, and let G be the matrix 
of inner products—i.e., gi; = (x;|x;). A matrix such as G is frequently 
called a Gram matriz or, alternately, 
a Gramian—see page 658. 
(a) Show that for each u,v € V, (u|v) =[u]%G[v]g, where [ul], 
and [v]g are the respective coordinate vectors of u and v with 
respect to B. 
(b) Explain why G is positive definite (and hence nonsingular and 
hermitian). 
(c) Prove that if T* is the adjoint of T with respect to the un- 
derlying inner product, then the coordinate matrix of T* with 
respect to B is given by [T*]g = G"! [T], G—ie., [T*]g is 
similar to [T]%. 
(d) Explain why [T*]s = [T]% when B is an orthonormal basis, in 
which case T being a self adjoint operator means then [T], is 
a "self-adjoint matrix" —i.e., a hermitian matrix. 
Let T be a linear operator on an n-dimensional complex inner-product 
space V whose basis is B = {x1,x2,...,Xn}, and let T* be the ad- 
joint of T with respect to the underlying inner product. As defined in 
Exercise 5.1.20, let G be the Gram matrix of inner products. 
(a) Prove that if T is normal in the sense of (5.1.19) on page 
653, then [T]g and G~'[T]%G commute. Conclude that if 
B is an orthonormal basis, then [T]g is a normal matrix—i.e., 
[T]a[T]z = [T]p[T ls. 
(b) If T is unitary in the sense of (5.1.19) on page 653, then explain 
why G~![T*]gG = [T],', and if B is an orthonormal basis, 
then [T]g is a unitary matrix—ie., [T]~ = [T],°. 

658 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
5.2 GRAM-SCHMIDT PROCESS 
+ 
Concepts concerning orthogonality in a general inner-product space V nearly 
always mimic those in F" with the standard inner product x*y replaced by 
the inner product (x|y) in VY. Orthonormal sets are examples. But because of 
their central role in more general theory, they are worth explicit development. 
5.2.1. Definition. An orthonormal set in an inner-product space V is 
aset {uj,U2,...,ug} C V such that (u;|uj) = 0 for all « # j and 
ee 
ees 
: 
: 
1 wheni=j, 
| 
|u| = 1 for each i. More concisely, (uj;|uj) = 
6 wine 
The following are direct analogs of facts from F" (see Exercise 5.2.1). 
e 
Every orthonormal set is linearly independent. 
(5.2.1) 
e 
Every orthonormal set of n vectors in an n-dimensional inner-product space 
Y is an orthonormal basis for VY. 
F" abounds with orthonormal bases—the standard basis {e1,€2,...,€n} 
of unit vectors is the most common. But in more general inner product spaces 
analogs of unit vectors e; € F" may not be defined, and it is not clear where 
orthonormal bases might come from. This raises the questions, does every n- 
dimensional inner-product space possess an orthonormal basis with respect to 
its inner product, and, if so, how can such a basis be obtained? The following 
theorem answers both questions. 
5.2.2. Theorem. 
If {x,,x2,...,Xn} is a linearly independent set in an 
inner-product space, then the Gram—Schmidt' sequence defined by 
k-1 
mo 
ond 
sai SY TS sada 
|| || 
l= 
ule So (uj |x) u; 
is an orthonormal set for which span {uj,---,u,}=span {x,,--+,xp}. 
for: Ks 2 
Jorgen P. Gram (1850-1916) was a Danish actuary who presented the essence 
of this orthogonalization procedure in 1883. Gram was apparently unaware that 
Pierre-Simon Laplace (1749-1827) had earlier used the method. Gram is remem- 
bered primarily for this process, but his name is also associated with the Gramian ~ 
matrix G in which 9;; = (x;|x;), and, more specifically, the matrix A*A that 
historically was referred to as the Gram matriz of A. 
J. GRAM 
Erhard Schmidt (1876-1959) was a student of Hermann Schwarz (of CBS in- 
equality fame) and the great German mathematician David Hilbert (1862-1943). 
Schmidt explicitly employed the orthogonalization process in 1907 in his study of 
integral equations, which in turn led to the development of what are now called 
Hilbert spaces. Schmidt made significant use of the orthogonalization process to 
ae 
develop the geometry of Hilbert Spaces, which explains the reason for including 
B. Scumipr 
Schmidt's name. 
a 

5.2 Gram-—Schmidt Process 
659 
Proof. 
Let vz = xz — Sale! (u;|x,) u; and vz = ||v;||. Apply induction to 
show that By = {uj,u2,...,ux} is an orthonormal set for all k. When k = 1 
there is nothing to prove. Assume that B,—1 is orthonormal, and prove that 
this implies 6; must be orthonormal. For each j < k —1, 
SS 
Sue Aa 
ET 
u,|x 
~ 
(uj; |Ux) = (3 Heer se = a = = > (wile) (uj|u,) 
Thus By, is an orthogonal set. It is also an orthonormal set because each vec- 
tor in B, has unit norm. Let A, and U, be the respective spaces spanned 
by {x1,X,. +2 XK} and {tj,U2,...,u,}. To see that % = Us, observe that 
xj = Yui + 
+ (uj |x) u; for each 
1<i<k so that 
k 
k 
i-1 
xeEX, 
= 
x= aK Sawn + 
a" (u;|x;) u 
— 
geal 
Gai! 
1 
Se ET 
ee Be Ok 
Pe 
By assumption, 
{x,,X2,...,x,} 
is linearly independent, and by (5.2.1) so is 
{uj,Ug,...,uz}, so dima, =k=dimls, and thus 4 =U, foreach k. 
Of 
5.2.3. Classical Gram—Schmidt Algorithm 
Summarized below is the straightforward or "classical" implementation of the 
Gram-Schmidt algorithm in which a ~ b means "a is overwritten by (or defined 
to be) 6." 
initialize: 
uy, <- = 
bal 
xX] 
oie 
(¢ =D 
oO 
Hoe 
Gell 
wo y= I 
Vik 
(u;|Xx) 
end for 
end for 

660 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
Example (Classical Gram-Schmidt in F" ) 
zu 
While the Gram-Schmidt procedure applies to any inner-product space, the most 
common applications involve F" with the standard inner product (x|y) = x"y, 
so it is instructive to look at an example. 
Problem: Find an orthonormal basis for the space spanned by the linearly 
independent set 
1 
1 
3} 
0 
2 
1 
> 
Sime 
0]? 
Pie 
0 
|? 
x2 i 
1 
-1 
—1 
-—1 
Solution: 
; 
x1 
1 
0 
k=1: 
we —=— 
pal] 
V2) 9 
0 
0 
2 
ug 
1 
k=2; 
Usp <= Xo — (uj x2)uy on 
We Ba <= ju T = 
16 
0 
2 
0 
uf 
1 
1 
u 
k=3: 
ug 
x3 —(u)x3)u; — (up x3)u2 = 
; £ug et 
eee 
: 
[us| 
V3 
\2 
Thus 
1 
0 
1 
Te 
1 
0 
pee rad 
Pap 
G 
cia 
Onl 
= 
Ae 
3 
—_ 
1 
v2 = 
0 
v3 1 
is an orthonormal basis for span {x1,x2,x3} = V3 = U3. 
5.2.4. Matrix Description 
The Gram-Schmidt process in F" can be described in terms of matrices. Let 
Ue, On sc4 
and 
Ux = (ui |ua|---|ur-i) 
a 
for kl; 
and notice that 
uj Xk 
* 
—— 
ULX;= 
; 
and 
U,U;x, = Se u,; (u; xz) = 2 (u;x,) U;. 
oe ees 
i=1 
i=1 
Since U,U; = P,, is the orthogonal projector onto U,p_1 = X,—1, (by Theorem 
4.3.3 on page 457), and since 
k-1 
>, 
GS SS (u; xz) u; = XE — U,U; xr = (I = U,U;) Xk = (I = Phy 
i=1 
the Gram-Schmidt vectors in Theorem 5.2.2 can be concisely expressed as 
(TO, Ux 
(I— Px) x, 
= oe SF 
or 
bk = 
le a 
[dQ — U,U*) xx] 
Ore 
ics ba D ne ann 
(5.2.2) 
Vk 
in which vz = ||(I — Px) xz . 

5.2 Gram-—Schmidt Process 
5.2.5. Modified Gram-Schmidt Algorithm 
A disadvantage of the Gram—Schmidt algorithm is that floating-point implemen- 
tations in F" often result in a degradation or complete loss of orthogonality as 
the process evolves. However, rearranging the order of arithmetic can go a long 
way to mitigate this unfortunate feature. To understand this, use the alternate 
representation in (5.2.2), and let E; 
=I and E; = I—u;_,u*_, for 
i>1. The 
orthogonality of the u;'s ensures that 
Ex Ca E2E, =I- uu; == U2QU5 as 
Up—1Ujz_1 =I[- U,U;, 
so the Gram—Schmidt sequence can alternately be expressed as 
Ex «+: 
Eg E1 x, 
un = ————— 
fork 
= 
1,2,...,n. 
[Ex --- 
EgE1 xq] 
ercees 
This produces the modified Gram-—Schmidt algorithm. 
Normalize 1-st 
(33a) 
(tm 38,30) 
Apply E2 
fu, Eox2, Eox3, ..., Eox, } 
Normalize 2-nd 
{u, U2, Eoxg, ..., Eoxn} 
ee aun Ea haxy ee hee) 
Normalize 3-rd 
{u1, U2, U3, E3E2x4,..., E3E2xp} 
Implement this as follows. 
eee: 
initialize: 
uy, <- es and Ute 
TOR 
ea 
[| 
forse == 2 (COP 
TCOlrejeeraeto 
1 
T(k-1)j ? U(R—1)Y5 
uy 
Uy — T(R-1)jUk—-1 (= Exuy) 
end for 
uz + Ug/ ||UR| 
end for 
Theoretically there is no difference between this modified algorithm and the 
classical algorithm on page 659—they return the same results in exact arithmetic. 
But when floating-point arithmetic is used the modified algorithm tends to be 
more stable in the sense that it does a better job of minimizing the loss of 
orthogonality as the process evolves. This is primarily due to the fact that the 
k* step of the classical algorithm alters only the k* vector, whereas the 
step of the modified algorithm updates all vectors from the kt? through the last. 
As the following example illustrates, conditioning the un-orthogonalized tail in 
this way can make a difference. 

662 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
Example (Classical vs Modified Gram-—Schmidt) _ 
Compare the results of using 3-digit floating-point arithmetic with classical 
Gram-Schmidt against those using modified Gram-Schmidt on the set 
1 
1 
1 
fxr 
= (19-2), x2 = (20°), 2 
= 
0 
: 
10-3 
0 
i= 
The classical algorithm: 
pees Sill 
Urs) = |, SOe Upstate 
kee 2: 
fi. (uj 
xq) = T, 90 
0 
U2 © X2 — (uy X2) UW = ( o.), 
ment) (4) 
k=8: 
fl(utx3) =1 and fl(ujx3) =—10-%, so 
0 
uz ¢ x3 — (ut 
x3) U1 — (ug x3) U2 = (<0: ), 
—10-4 
ug 
0 
u3 
<~ fl | —— } = [ -.709 
}. 
|| us| 
—.709 
Thus classical Gram—Schmidt with 3-digit arithmetic returns 
1 
0 
0 
{un 
= (20-2), U2 = ( 
0), ie = ( 
--70 
\. 
(5.2.3) 
10-3 
—1 
—.709 
This is highly unsatisfactory because ug and ug are far from being orthogonal. 
The modified algorithm: 
ihe = he 
fl ixy||= 1, sO {uj, U2, ug} <~ {x1, x2, x3}. 
k=2: 
fl(ufug) =1 and fl(ufus3) =1, so 
0 
Ug © Up — (uj us) u; = ( 
0 i' 
—10-3 
0 
uz + ug — (uz ug) U1 = (a0 ), 
0 
ut 
(2) 
lip 2 Sera! 
Se 
I|w2|| 
zi 

5.2 Gram-—Schmidt Process 
663 
* I w 
uz u3 = 0, so 
0 
ug + ug — (uz 
U3) Ug = (-20-*) 
0 
aes 
Ug tee = | ii 
|| us| 
0 
Modified Gram-—Schmidt therefore produces 
fo-(2).=-(Hes-(} 
2s 
This time the degree of orthogonality is as good as can be expected using 3-digit 
arithmetic. Comparing (5.2.4) against (5.2.3) illuminates the advantage afforded 
by modified Gram—Schmidt over the classical Gram—Schmidt. 
Exercises for section 5.2 
5.2.1. Let V be an n-dimensional inner-product space with inner product 
(x|y) and norm 
||x|| = \/(x|x). 
(a) Show that every orthonormal set in V is linearly independent. 
(b) Why is every orthonormal set of n vectors an orthonormal basis 
for V? 
1 
2 
1 
a4 
1 
oe 
ie 
re 
2 
5.2.2. Let o = span< 
xX; = 
i}> 
X2=|_] 
|. 
-x3s= 
2 
—1 
Ht 
1 
(a) Use the classical Gram-Schmidt algorithm (with exact arith- 
metic) to determine an orthonormal basis for S. 
(b) Verify directly that the Gram—Schmidt sequence produced in 
part (a) is indeed an orthonormal basis for S. Hint: Recall 
Corollary 4.2.7 on page 432. 
(c) Repeat part (a) using the modified Gram—Schmidt algorithm, 
and compare the results. 
5.2.3. Use the Gram—Schmidt procedure to find an orthonormal basis for the 
SS 
ee 
al 
four fundamental subspaces of A = (: 4 
6 
-2 ) 
3 
—6) (9) —3 
5.2.4. Apply the Gram—Schmidt procedure with the standard inner product 
om {().() @} 

664 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
iz 
0 
EX. 
5.2.5. Let A=(:2 4 1) 
and b= (1), 
132 
1 
(a) Compute the orthogonal projectors onto each of the four funda- 
mental subspaces associated with A. 
(b) Find the point in NV (A)~ that is closest to b. 
5.2.6. Explain what happens when the Gram—Schmidt process is applied to an 
orthonormal set of vectors. 
5.2.7. Explain what happens when the Gram—Schmidt process is applied to a 
linearly dependent set of vectors. 
5.2.8. 
(a) Apply classical Gram-Schmidt with 3-digit floating-point arith- 
1 
1 
1 
metic to {x= ( 
0 ).x2=(0), x= (a0) b 
You may 
10-3 
0 
0 
assume that fl (V2) = 1.41. 
(b) 
Again using 3-digit floating-point arithmetic, apply the modified 
Gram-Schmidt algorithm to {x,, x2, x3}, and compare the re- 
sult with that of part (a). 
5.2.9. Depending on how the inner products r;; are defined, verify that the fol- 
lowing code implements both the classical and modified Gram—Schmidt 
algorithms applied to a set of vectors {x),x2,...,Xn}. 
Louisa 
to 
bn 
uj <— Xj 
for.1=1 
to. .7— 1] 
(uj|x;) 
(classical Gram—Schmidt) 
(5G) ae 
5 
- 
(u;|u;) 
(modified Gram—Schmidt) 
Sa Uj — TiZU; 
end for 
733 <— |luyl| 
alae V99 —al() 
quit 
(because x; € span {x,,Xo,... erate) 
else 
Uy —— Uj/T7, 
end for 
If exact arithmetic is used, will the inner products rij be the same for 
both implementations? 
a - 
gi 

5.2 Gram-—Schmidt Process 
665 
5.2.10. Let V be the inner-product space of real-valued continuous functions 
defined on the interval [—1,1], where the inner product is defined by 
i 
(fl9) = ii 
fa@)gle)aa, 
and let S be the subspace of V that is spanned by the three linearly 
independent polynomials g9=1, 
qi =2, 
q2=2'. 
(a) Use the Gram—Schmidt process to determine an orthonormal set 
of polynomials {po,p1,p2} that spans S. These polynomials 
are the first three normalized Legendre! polynomials. 
(b) Verify that p, satisfies Legendre's differential equation 
(1 —2*)y" — Qary' + n(n+1)y =0 
for n = 0,1,2. This equation and its solutions are of consider- 
able importance in applied mathematics. 
Adrien—Marie Legendre (1752-1833) was one of the most eminent French mathematicians of 
the eighteenth century. His primary work in higher mathematics concerned number theory and 
the study of elliptic functions. But he was also instrumental in the development of the theory 
of least squares, and some people believe that Legendre should receive the credit that is often 
afforded to Gauss for its introduction. Like Gauss and many other successful mathematicians, 
Legendre spent substantial time engaged in diligent and painstaking computation. It seems 
that he was also somewhat of an anti-government political activist because it was reported 
that in 1824 Legendre refused to vote for the government's candidate for Institut National. 
This resulted in his government pension being stopped, and he subsequently died in poverty. 

666 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
5.3 QR FACTORIZATION 
es 
eee 
+ 
The Gram—Schmidt process in F" frequently appears disguised as a matrix 
factorization. If Amxn = [ay |ag|---| an | has linearly independent columns 
a,, and if the Gram-Schmidt process in Theorem 5.2.2 on page 658 is applied 
with x, — a, to create an orthonormal basis {qi,q2,---,dn} for R(A) in 
which q, is identified with u,, then 
kesDy 
qi = Su 
nat # a 
ak — 
Dini (di|@x) a for k = 2,3,...,n, 
(5.3.1) 
Vy 
Vk, 
where 1 = |\a;|| and vz = |lax — #2) (alae) 
ail| for & > 1. Bringing each 
a; to the left-hand side in equations (5.3.1) and moving all other terms to the 
right-hand side produces 
@f=rj0q,° 
and" 
a, = (qilax) Qt eee (qk—1| Ax) Qe-1 
+qx 
for k>1. 
These equations can be condensed into a single matrix equation by writing 
v, 
(qila2) 
(qilas) 
--- 
(q1|an) 
0 
v2 
(q2|as) 
--- 
(q2|an) 
[ar |a2|---|an] =[ailaz|---|an]] °° 
a 
ee 
0 
0 
ee 
eels 
« 
More concisely, Amxn = QmxnRnxn 
in which the columns of Q are an or- 
thonormal basis for R(A) and R is an upper-triangular matrix with positive di- 
agonal elements. This observation is important enough to recast it as a theorem. 
5.3.1. Theorem. Every A ¢€ F'"'*" having linearly independent columns 
can be uniquely factored as 
A = QR in which the columns of Q,,xn 
are an orthonormal basis for 
R(A) and R, 
x, is an upper-triangular 
matrix with positive diagonal elements, and as indicated in (5.3.2), this 
is an equivalent way to express the Gram—Schmidt orthogonalization 
process. Lacking a better name, this factorization is generally just called 
the QR factorization' of A. 
Proof. 
Only the uniqueness of Q and R needs to be argued. If A = QR, then 
A*A=R*R. Since A has independent columns, A*A is positive definite, and 
hence R*R is the unique Cholesky factorization of A* A (recall Theorem 2.10.7 
on page 273 and Exercise 2.10.19 on page 281). The uniqueness of R. ensures 
that Q = AR7™! must also be unique. Exercise 5.3.1 on page 685 provides an 
alternate argument. 
When A and Q are not square, some authors emphasize this by calling A = QR aa rectan- 
gular QR factorization as opposed to a square one. 

5.3 QR Factorization 
667 
Example 
. 
: 
0 
-—20 
—-14 
The QR factorization of A = (3 oy 
-4) 
is obtained by using the standard 
AY 
wey es g 
inner product for R" and applying the Gram—Schmidt procedure to the columns 
of A as shown below. 
0 
oe 
ts 
ont 
* 
initialize: 
rj, < l|ai||, =5 and qi © Res Sng (3/5) 
Pil 
4/5 
ki) 
'i 
fat 
12 <° Qj ao = 25 
—20 
Q2 <~- a2 —7T12q1 = ( 
12) 
—9 
ef 
20 
ey See Ilq2llo =25 
and qo Qe — =( 12 
122 
20) \ =) 
ee 3} 
ee 
oie. 
yf 
ues 
C3 
aa) a3 Oe T2342 = - (=) 
PON 
Gest? 
q3 
epee 
733 oa: I|q3|l. = 10 and 
q3 - — = 5(- 
33° 
20 \ 12 
0 
-—20 
—15 
5 
25 
—4 
Therefore, 
Q = (15 12 "16 and R= (: 25 
0). 
20 
-9 
12 
i). 
0) 
iN) 
QR and Volume 
An n-dimensional parallelepiped is a solid in R™ with parallel opposing faces 
whose adjacent sides are defined by vectors from a linearly independent set 
{X1,X2,.--;Xn}C R™. As shown in the shaded portions of Figure 5.3.1 below, 
a two-dimensional parallelepiped is a parallelogram, and a three-dimensional 
parallelepiped is a skewed rectangular box. 
III 
\|(I— P2)xal| 
| 
FIGURE 5.3.1: PARALLELPIPEDS IN 2-D AND 3-D 

668 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
Problem: Determine the volumes of these two parallelepipeds, and then make 
the natural extension to define the volume of an n-dimensional parallelepiped. 
Solution: Volume is area in two dimensions, so by looking at the triangles in 
the left-hand side of Figure 5.3.1 it is visually evident that the area V2 of the 
shaded parallelogram is the same as the area of the dotted rectangle. The width 
of the dotted rectangle is v1 = ||xi|,, and its height is v2 = ||(I—P2)xell2, 
where Py is the orthogonal projector onto the space (line) spanned by x1, and 
I— P» is the orthogonal projector onto span {x1 }> . In other words, V2 is the 
length of its base times its projected height, v2, so 
V2 = ||x1ll (= P2)xa\|5 = V2. 
Similarly, the volume V3 of a three-dimensional parallelepiped is the area of 
its base times its projected height. The area of the base as just determined is 
V2 = ||x1||. (I — P2)xel|, = vive, and it is evident from Figure 5.3.1 that the 
projected height is v3 = ||(I— P3)xs||,, where P3 is the orthogonal projector 
onto span {x1,xX2}. Therefore, the volume of the parallelepiped generated by 
{X1,X2,X3} is 
I[x1ll2 | — P2)xall, || — P3)xsll. 
a See 
ET 
Per 
emer me a eer 
area of base 
projected height 
This makes it clear how to inductively define V4, V5, etc. In general, the volume 
of the parallelepiped generated by a linearly independent set {x,,xX2,...,Xn} is 
Vi = ||X1 
lo [I - P2)xall2--- || — Pn)Xnllp = 1¥2+++ Yn, 
(5.3.3) 
where P, is the orthogonal projector onto span {x1,X2,...,Xx—1}, and where 
%4=||xil|, 
and 
v,=(|\((I—P.)xx||, 
for 
k&>1. 
(5.3.4) 
These are exactly the terms in (5.2.2) on page 660 that are generated by the 
Gram-Schmidt process. In particular, the 1's are the diagonal entries of R. 
Thus the following definition of volume in higher dimensions is formulated. 
5.3.2. Definition. The volume V,, of the n-dimensional parallelepiped 
generated by the columns 
{x;,x2,...,X,} 
in a nonsingular matrix 
Ae R"*"=QR is V, = 4¥2---¥, = det (R), which is the prod- 
uct of the diagonal elements of R in the (unique) QR factorization 
of 
A. Since det (Q) = +1, it follows that V,, = |det(A)| (see page 950 
for more about this). 

5.3 QR Factorization 
669 
Note 1. 
If {x1,X2,...,Xn} 
is an orthogonal set, then the parallelepiped it 
generates is a rectangular box, and (5.3.3) says that V;, = ||x1||5 ||xally--+ ||Xnllo, 
which is exactly what it is expected to be. 
Note 2. 
The Gram-Schmidt process generates QR factors, but since these fac- 
tors are unique, it does not matter how the QR factors are obtained. Procedures 
other than Gram-Schmidt can generate the QR factorization—e.g., Householder 
or Givens reduction can be employed as described on pages 671 and 676. This 
means that using the R factor to define volume makes volume a uniquely defined 
concept that is independent of Gram—Schmidt or any other algorithm. 
QR vs LU 
The QR and LU factorizations of A € F"*" are two of the most important 
decompositions of a nonsingular matrix. While they are not the same, there are 
nevertheless some striking analogies between them. 
e 
Each factorization represents a reduction to upper-triangular form—LU by 
Gaussian elimination, and QR by Gram-Schmidt. 
e 
As discussed on page 261, the LU factorization A = LU is the complete 
"road map" or history of Gaussian elimination in the sense that the entry 
£;; below the main diagonal in L is the multiplier used to annihilate the 
(i, 7) -position during Gaussian elimination, and the entries in U represent 
the final result of Gaussian elimination. Given the factors L and U the 
entire Gaussian elimination process can be reconstructed without explicit 
knowledge of A. 
e 
Analogously, the QR factorization A = QR is the complete "road map" or 
history of the Gram—Schmidt orthogonalization process in the sense that the 
entries rj; above the main diagonal in R are the inner products used at the 
it" step to create the j*" column q; in Q, and the diagonal entries rj; 
are positive numbers representing the norms of the un-normalized orthogonal 
Gram-Schmidt vectors. The set of columns in Q (the final result of the 
process) is an orthonormal basis for R(A), and, analogous to LU, given 
Q and R, the entire Gram—Schmidt process can be reconstructed without 
explicit knowledge of A. 
e 
When they exist, both factorizations 
A = LU and A = QR are uniquely 
determined by A. 
e 
Once the LU factors (assuming they exist) of a nonsingular matrix A are 
known, the solution of Ax = b is easily computed—solve Ly = b by 
forward substitution, and then solve Ux = y by back substitution (see page 
262). The QR factors can be used in a similar manner. If A € F"*" 
is 
nonsingular, then Q* = Q~! (because Q has orthonormal columns), so 
Ax=b —} QRx=b <— Rx=(Q'b, 
(5.3.5) 

670 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
which is an upper-triangular system that can be solved by back substitution. 
QR and Least Squares 
While the LU and QR factors can be used in similar ways to solve nonsingular 
systems, things are different for singular and rectangular systems, and this is 
where the analogy between LU and QR breaks down. The LU factorization 
is not defined for rectangular matrices, but the QR factors of Am 
xn always 
exist as long as A has linearly independent columns. Furthermore singular or 
rectangular systems Ax = b can be inconsistent, in which case a least squares 
solution as described in §4.4 on page 481 may be called for. This means that 
a solution for x in the system of normal equations A*Ax = A*b is needed 
(recall (4.4.11) on page 487). But even in the best of circumstances—e.g., when 
rank (Amxn) =n so that A*A is nonsingular—an LU factorization of A*A 
cannot be obtained by applying Gaussian elimination to A itself. In other words, 
row-reducing A is of little or no use in finding the LU factorization of A*A, 
so row-reducing A is wasted effort when seeking a least squares solution. 
Here is where the beauty of QR shines through. Just as QR provides the 
solution to a consistent system as shown in (5.3.5), the following theorem shows 
that this same exact formula yields the solution of the normal equations. In other 
words, it is not necessary to know beforehand whether Ax = b is consistent or 
not because the same procedure producing A = QR provides either a solution 
or a least squares solution. 
5.3.3. Theorem. If rank (Amxn) =n, and if A = QR is the QR fac- 
torization, then the solution for x in the nonsingular triangular system 
Rx = Q*b 
(5.3.6) 
is either the solution or the least squares solution for x in Ax = b, 
depending on whether or not Ax = b is consistent. 
Proof. 
If Ax =b 
is consistent, and if A = QR is the QR factorization then 
QRx = b. Since Q7! = Q*, this system is equivalent to Rx = Q*b. Now 
suppose that Ax = b is inconsistent and that a least squares solution is desired 
so that a solution x to the system of normal equations 
A* Ax = A*b is needed. 
Making the substitution A = QR and using Q*Q =I 
produces 
A* A = (QR)"(QR) = R*Q*QR = R'R, 
(5.3.7) 
which means that the normal equations can be written as R*Rx = 1 O7b. 
Since R* 
is nonsingular (it is triangular with positive diagonal entries), this 
simplifies to become the equivalent system Rx=Q*b. 
US 

5.3 QR Factorization 
671 
Computational Notes 
Suppose that Am 
xn is real with rank(A) = n, and suppose that Ax = b 
is inconsistent. It might be tempting to compute a least squares solution by 
explicitly computing 
A? A, and then factoring A? A = LU to solve the normal 
equations 
A? Ax = A'b. But this is inadvisable for some important reasons. 
e A lot of unnecessary arithmetic is required to form A' A when m and n 
are large 
e 
More importantly, forming the matrix product A?' 
A using floating-point 
arithmetic can result in a loss of significant information—revisit Exercise 
2.8.8 on page 246 to see how sensitive the formation of a product A7A can 
be to rounding errors. 
e 
Finally, even if one knows A?'A accurately, solving ATAx = A'b by 
means of factoring A? A 
= LU with floating-point arithmetic may be prob- 
lematic because if the two-norm condition number of A is K2 (defined in 
Theorem 3.5.12 on page 372), then the condition number for A7A is «3 
(recall Exercise 3.5.21 on page 380), and this increases the likelihood of a 
significant loss of numerical stability for larger values of ko. 
e 
However, if a QR factorization is used to obtain a least squares solution 
via (5.3.6) then the problem of increasing the condition number is nonexis- 
tent because ||A||, = ||QR||, = ||R||, (the 2-norm is unitarily invariant by 
(3.5.17) on page 364), so K2(R) = Ko(A). 
e 
It is worthwhile noting that when A is real and has full column rank, the 
QR factorization leading to the expression A? A = R'R as in (5.3.7) is 
the Cholesky factorization of A? A (see page 273). In other words QR via 
Gram-Schmidt is an alternative to the Cholesky algorithm on page 277. 
As seen in (5.3.2), QR 7s Gram-Schmidt, but this does not mean that Gram— 
Schmidt is the only way to compute a QR factorization. There are at least two 
somewhat superior techniques for computing this factorization, and each relies 
on orthogonal triangularization methods—one is based on elementary reflectors 
(otherwise known as Householder transformations, introduced on page 103) and 
the other on plane rotations (otherwise known as Givens rotations, introduced 
on page 98). 
Householder Reduction 
While Householder reduction applies equally well to rectangular matrices as 
it does to square matrices, it is convenient to describe the process for square 
matrices—the application to rectangular matrices is then clear. Gaussian elimi- 
nation seeks to reduce A, 
yn» to upper-triangular form by employing elementary 
lower-triangular matrices T, associated with elementary row operations to suc- 
cessively annihilate entries below the kt? pivot one entry at a time (recall page 
140). Householder reduction instead uses elementary reflectors R, in place of 

672 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
the T;'s to annihilate all entries below the kt' pivot at once with just a sin- 
gle elimination step. With A = [Asi |As2|--: | A«n], Householder proceeds as 
follows. 
i 
e 
Start with the first column and use the procedure on page 106 with x = Axy 
uu 
to construct the elementary reflector R; = I — aoe? where 
1 
if x; is real, 
zi/|x;| 
if x; is not real. 
(5.3.8) 
u=A,,+p||A.i||e1, 
with 
u={ 
As shown in (1.10.29) on page 106, R; reflects A; onto the first standard 
basis vector to yield 
ti 
0 
R, Ay = Fu || Ax2|| Ee; = 
: 
" 
(5.3.9) 
Applying R, across all of A produces 
Ri A=[Ri 
An |RiAw| ++: |RiAen|= 
in which Ay is (n—1) x (n—1). Thus all entries below the (1, 1)-position 
are annihilated by a single reflection. 
e 
Now repeat the same procedure on Ag to construct an elementary reflector 
R2 that annihilates all entries below the (1,1)-position in Ag. If we define 
Roe Mo Bp ), then R2Rj, is an orthogonal matrix (by Exercise 1.10.2 on 
page 107) such that 
41. 
3" 
tin 
e 
After n—1 steps the final result is R,_;---R2R,A = 
ae 
="; 
tnn 
in which P = R,_;---R2R, 
is an orthogonal (or unitary matrix) 
e 
In the case of a square and real matrix the process produces an orthogonal 
matrix P and an upper-triangular matrix T such that PA = T. Setting 

5.3 QR Factorization 
673 
Q =P" and R=T 
yields the factorization 
A = QR, which suggests that 
this should be the QR factorization for A. Caution! This need not be the 
case. By definition, the upper-triangular matrix R in the QR factorization 
must have positive diagonal elements but if the signs in (5.3.8) and (5.3.9) are 
not correctly chosen, then the resulting R can have some negative diagonal 
entries. To generate the QR factorization from Householder, the minus sign 
(—) must be used in (5.3.8). However, to aid in numerical stability when 
using floating point arithmetic, the plus sign (+) should be used—see the 
footnote on page 105. 
e 
When A is not square, either all of the rows or all of the columns will be 
exhausted before the other, so the final result is one of the two following 
upper-trapezoidal forms: 
* 
* 
* 
O 
x 
* 
Roa tReRiAjen =) |e 
when 
m>n, 
OO 
0) 
10) 
0 
* 
ox 
* 
* 
* 
Rm-1°+*R2RiAmxn = | - 
ok 
SPIES 
.| 
when 
m<n. 
Oto eae 
ins 
* 
Se 
mxm 
If m =n, 
then the final form is an upper-triangular matrix. The previous 
observations are summarized in the following theorem. 
5.3.4. Theorem. For every A ¢ F'™", there exists a unitary matrix P 
such that 
PAST 
(5.3.10) 
has an upper-trapezoidal form. When P. is constructed as a product of 
elementary reflectors as described above, the process is called House- 
holder reduction. If A is square, then T is upper triangular, and if 
A is real, then P can be taken to be an orthogonal matrix. 
Example (Householder Reduction) 
Apply Householder reduction to find an orthogonal matrix P such that PA = T 
0 
-—20 
-14 
is upper triangular with positive diagonal entries, where A = (: 27 
1). 
ila 
- 

674 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
Solution: 
To annihilate the entries below the (1,1)-position and to guarantee 
that t11 is positive, equations (5.3.8) and (5.3.9) dictate setting 
—5 
uju; 
uy = Axi — 
|| Axil] e1 = Avr — 5e1 = ( 
2) and 
R, =I-2 
ul uy 
To compute the matrix-by-matrix product RA = [RAww|RA.2| +--+ | RAs], 
it is wasted effort to actually determine the entries in R = I— 2uu!/ulu. Just 
compute u' A,; and then 
ul Ax; 
uu 
RA,; = Axj — 2 ( 
) 
u. 
for each 7 =1,2,...,7. 
(5.3.11) 
Using this observation yields 
R,A = [Ri Aw |Ri Axe | R,Ax3| == 
To annihilate the entry below the (2,2)-position, set 
gd. +10 
et 
A» es (a: and 
and 
uz = [Ao].1 ~ | 
[Aa].4 le = 25 ) 
° 
If Ro 
=1—2upuz /ulu, and R2 = ( 
' fe) (as explained above, these are 
not explicitly formed), then 
- 
5 
25 
—4 
RoAo = (75 at 
and 
RRA =T= (0 7H 10). 
If Ry =1—2007/G7G is an elementary reflector, then so is 
ie 
1) 
uu! 
0 
Ree 
A 
= Ih 9 = 
i 
= 
re ({ 4.) 
Tu 
with 
u 
ie 
: 
and consequently the product of any sequence of these Rx's can be formed by 
using the observation (5.3.11). In this example, 
1 
O- 
15.20 
P = R2R; = —| 
-20 
+12 3), 
—15 
-16 
12 
You may wish to verify that P is indeed an orthogonal matrix and PA = T. 

5.3 QR Factorization 
675 
Givens Reduction 
Another alternative to Gram—Schmidt is orthogonal reduction by means of plane 
rotations (also called Givens rotations) that are discussed on page 98. The process 
for a matrix A having independent columns proceeds as follows. 
e 
Set x =A, and let Pj2 be the plane rotation shown in (1.10.15) on page 
100 that zeros out the (2,1) entry. Apply Py2 across all of A. 
e 
Follow this by applying the rotation 
P 3 to P j2.A. As shown below, this 
produces a zero in the (3,1) position without destroying the zero created in 
the previous step. 
e 
After n—1 
rotations all entries below the first one are annihilated. 
4/0? + x2 
4/2? 
+ «32 + «2 
0 
0 
©3 
0 
Piox = 
ie 
P13P 12x = 
oh 
Ln 
In 
I|x|| 
0 
0 
Pin: -PigPi2x = |* © 
|= ||x\|/2.e1, 
0 
e 
Applying Pin-::-PigPi2 across all of A produces 
Note that applying rotations P2,---P24P23 does not alter the zeros already 
created in the first column from the previous step. 
e 
If A is nonsingular, then eventually the process generates an orthogonal 
matrix P (that is the product of plane rotations) and an upper-triangular 
matrix T such that PA = T. If 
Q = P? and R=T, then A = QR, 
which suggests that this might be the QR factorization. But not quite—the 
last diagonal entry rnn 
is not necessarily positive. However, this is easily 
remedied. If ran <0, then multiplication by ( 
- zis (another orthogonal 
matrix) will turn the factorization into a legitimate QR factorization. 

676 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
Example (Givens Reduction) 
Use Givens reduction (i.e., plane rotations) to reduce the matrix 
0 —20 
—14 
A= (2 27 = —4 
4 
11 
-2 
to an upper-triangular matrix T, and then determine an orthogonal matrix P 
such that PA = T. 
Solution: The plane rotation that uses the (1,1)-entry to annihilate the (2,1)- 
entry is determined from (1.10.13) on page 98 to be 
oo 
3 27 
-4 
Pio=(-1 0 0) so that 
Pw 
= (1 20 
i'). 
Om0s 
i 
4 
11 
-2 
Now use the (1,1)-entry in Pj2A to annihilate the (3,1)-entry in Pj2A. The 
plane rotation that does the job is again obtained from (1.10.13) to be 
1/30 
4 
5 
25 
—4 
Pia =3( 0 
5 0) so that 
PuPd=(0 
20 
i'). 
iNeed, 
eg 
@ = 
=15 
Finally, using the (2,2)-entry in P13Pj2A to annihilate the (3,2)-entry produces 
1/5 
9. 
0 
5 25 
—4 
P93 == (: 
4 -) so that 
Po3P13Pij2A = T = (0 
25 
0). 
FLOCSe 
"4 
or*0" 
16 
Since plane rotation matrices are orthogonal, and since the product of orthogonal 
matrices is again orthogonal, it must be the case that 
1 
0 
15 
20 
P = Po3P13P12 = — ( 
—20 
12 2) 
25\-15 
-16 
12 
is an orthogonal matrix such that PA = T. 
Computational Effort 
It is of interest to compare the computational effort required to reduce an n Xx n 
matrix to upper-triangular form by each of the four methods discussed up to this 
point—Gaussian elimination, Gram—Schmidt, Householder, and Givens. Because 
the number of multiplicative operations is about the same as the number of ad- 
ditive operations for each of these algorithms, the computational effort is gauged 
by counting only multiplicative operations. Furthermore, lower-order terms are 
not significant in making comparisons, so they are neglected in following approx- 
imations. 

5.3 QR Factorization 
677 
ze a umm 
ary of 
Computati nal E 
Effort 
é The approximate number of multiplications 2 
divisions fequived to reduce 
an nxn matrix to an upper-triangular form is as follows. 
Be Gaussian elimination (sealed part i | pivoting) sn? / 
2 
Gram-Schmidt procedure (classical and modified) an 
'Householder reduction ~ an' Is 
3. 
Mp 
Givens reduction ~ 
¥ 
4n8/3, 
This makes it clear that in terms of the amount of arithmetic required, 
Givens is the least efficient in reducing a dense matrix to triangular form. The 
key word here is "dense," (i.e., nearly all entries are nonzero). But if A is a 
sparse matrix, (i.e., many entries are zero and occur in some pattern), then the 
ability to selectively annihilate specific entries without disturbing others is the 
big advantage that Givens reduction brings to the table, so applications involving 
sparse matrices is where Givens reduction is generally found (see the example 
on page 682). 
Numerical Stability 
Computational effort is not the only consideration to be taken into account 
when comparing triangularization techniques. An efficient algorithm that returns 
inaccurate results is generally not preferred to a less efficient one that consistently 
produces accurate answers. 
The numerical analysis of triangularization algorithms is rather detailed and 
pedantic, and much has been written on the subject, but you can nevertheless 
cut through the fog to gain an intuitive feel for the situation by considering 
the effect of applying a sequence of "elementary reduction" matrices to a small 
perturbation of A. Let E be a matrix such that 
||E]|,, is small relative to 
|| A||7, and consider 
P, ---PoPi(A +E) = (Px:--P2P1A) + (Px: P2PiE) = PA+PE. 
Ifeach P; is an orthogonal matrix, then the product P = P;,-:-P2P; is also an 
orthogonal matrix (Exercise 1.10.2), and consequently ||PE]||, = ||E||, (Theo- 
rem 1.10.3, page 93). In other words, a sequence of orthogonal transformations 
cannot magnify the magnitude of E, and you might think of E as representing 
the effects of roundoff error. This suggests that Householder and Givens reduc- 
tions should be numerically stable algorithms. On the other hand, if the P; 's are 
elementary matrices of Type I, II, or III, then the product 
P = P,--:P2Pi can 
be any nonsingular matrix—see Theorem 2.3.7 on page 165. Nonsingular matri- 
ces are not generally norm preserving (i.e., it is possible that ||PE]|,, > ||E||, ), 

678 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
so the possibility of E being magnified is generally present in Gaussian elimi- 
nation methods, and this suggests the possibility of numerical instability. 
Strictly speaking, an algorithm is considered to be numerically stable if, 
under floating-point arithmetic, it always returns an answer that is the exact 
solution of a nearby problem. To give an intuitive argument that the Householder 
and Givens reductions are stable algorithms for producing the QR factorization 
of Anxn, suppose that Q and R are the exact QR factors, and suppose that 
floating-point arithmetic produces an orthogonal matrix Q + E and an upper- 
triangular matrix R+F that are the exact QR factors of a different matrix 
A =(Q+E)(R+F) =QR+QF+ER+EF=A+QF+ER+ 
EF. 
If E and F account for the roundoff errors, and if their entries are small relative 
to those in A, then the entries in EF are negligible, and 
Ax A+QF+ER. 
But since Q is orthogonal, 
||QF||,, = ||F ||, and |/Al|, = ||QRI||, = ||RI|-. 
This means that neither QF nor ER can contain entries that are large relative 
to those in A. Hence A = A, and this is what is required to conclude that the 
algorithm is stable. 
Gaussian elimination is not a stable algorithm because, as alluded to in the 
summary on page 243, problems arise due to the growth of the magnitude of the 
numbers that can occur during the process. To see this from a heuristic point of 
view, consider the LU factorization of A = LU, and suppose that floating-point 
Gaussian elimination with no pivoting returns matrices 
L+E and U+F 
that 
are the exact LU factors of a somewhat different matrix 
A=(L+E)(U+F)=LU+LF+EU+BEF=A+LF+EU+EF. 
If E and F account for the roundoff errors, and if their entries are small relative 
to those in A, then the entries in EF are negligible, and 
Aw ALULP-EU 
(using no pivoting). 
However, if L or U contains entries that are large relative to those in A 
(and this is certainly possible), then LF or EU can contain entries that are 
significant. In other words, Gaussian elimination with no pivoting can return the 
LU factorization of a matrix A that is not very close to the original matrix A, 
and this is what it means to say that an algorithm is unstable. It was shown on 
page 240 that if partial pivoting is employed, then no multiplier can exceed 1 in 
magnitude, and hence no entry of L can be greater than 1 in magnitude (recall 
that the subdiagonal entries of L are in fact the multipliers). Consequently, 

5.3 QR Factorization 
679 
L cannot greatly magnify the entries of F, so, if the rows of A have been 
reordered according to the partial pivoting strategy, then 
AxA+EU 
(using partial pivoting). 
Numerical stability requires that A ~ A, so the issue boils down to the degree 
to which U magnifies the entries in E —i.e., the issue rests on the magnitude of 
the entries in U. Unfortunately, partial pivoting may not be enough to control 
the growth of all entries in U. For example, when Gaussian elimination with 
partial pivoting is applied to the Wilkinson matriz 
rte 
V6 
Om 
0) 
1" 
Sheet 
oe 
athe 
Cipee 
i 
la 
ee, 
1o0 
ee 
Wie 
. 
. 
. 
. 
; 
EM ee 
Taree 
ste 2 ke 
pe: 
We 
El. 
2] 
sel 
pee 
ey 8 eet 
al 
Re 
ee 
ee 
eel 
at 
the largest entry in U is unn = 2"~!. However, if complete pivoting is used on 
W,,, then no entry in the process exceeds 2 in magnitude (see Exercises 2.8.13 
and 2.8.14 on page 248). In general, it can be proven that if complete pivoting 
is used on a well-scaled matrix An 
xn for which max |a;;| = 1, then no entry of 
U can exceed y = n/2 (2131/2413. pny? in magnitude. Since y is a 
slow growing function of n, the entries in U won't greatly magnify the entries 
of E, so 
: 
AFA 
(using complete pivoting). 
In other words, Gaussian elimination with complete pivoting is stable, but Gaus- 
sian elimination with partial pivoting is not. Fortunately, in practical work it is 
rare to encounter problems such as the matrix W,, in which partial pivoting 
fails to control the growth in the U factor, so scaled partial pivoting is generally 
considered to be a "practically stable" algorithm. 
Algorithms based on the Gram—Schmidt procedure are more complicated. 
First, the Gram—Schmidt algorithms differ from Householder and Givens reduc- 
tions in that the Gram—Schmidt procedures are not a sequential application of 
elementary orthogonal transformations. Second, as an algorithm to produce the 
QR factorization even the modified Gram—Schmidt technique can return a Q 
factor that is far from being orthogonal, and the intuitive stability argument 
used earlier is not valid. As an algorithm to return the QR factorization of A, 
the modified Gram-Schmidt procedure has been proven to be unstable, but as 
an algorithm used to solve the least squares problem (see Theorem 5.3.3 on page 
670), it is stable. In other words, stability of modified Gram-Schmidt is problem 
dependent. 

ae 
bs eb oles 
Sieve 
eerre 
"ae 
= 
Pies' § 
frase etd 
Pb ipest Co 
Ay 
Reduction to Hessenberg Form 
~ 
It is often desirable to triangularize a square matrix A by means of a simi- 
larity transformation—i.e., P~'AP = T. But this can be a computationally 
challenging task, so applications often revert to the next best thing, which is 
to find a similarity transformation that will reduce A to a matrix in which all 
entries below the first subdiagonal are zero. Such a matrix is said to be in upper 
Hessenberg form—illustrated below is a generic 5 x 5 Hessenberg form. 
#7 
HOR 
ae 
OK 
Le 
¥ 
Ses 
H=]0<*«*« 
«* 
* 
(5.3.12) 
(OG) 
Es 
eS 
OM OOS 
xk 
H is said to be unreduced when all subdiagonal entries are nonzero. If a zero 
occurs on the subdiagonal, then H can be "reduced" (i.e., partitioned) into two 
or more smaller unreduced Hessenberg forms. 
Problem: Use Householder reduction to construct an orthogonal matrix P 
such that 
P? AP = H is upper Hessenberg. Note: This is the first step in QR 
iteration algorithms for computing the eigenvalues of A—see pages 779 and 780. 
Solution: Apply Householder at each step to annihilate entries below the main 
diagonal. Begin by letting A,, denote the entries of the first column that are 
below the (1,1)-position—this is illustrated below for n = 5: 

5.3 QR Factorization 
681 
If R; isan elementary reflector determined according to (5.3.8) on page 672 for 
* 
i 
ee 
0 
; 
which R,A,; = 
9 
|> then Ry = (4 &,) is an orthogonal matrix such that 
0 
1 
SO 
A 
0 R, 
Axi 
Ay 
A. 
(5.3.13) 
aii 
AiRi \ _ 
RiAw. 
R,AiR; 
II 
nal matrix Ry such that R2A2R» = 
. Matrix Ro = te o ) 
0 
Ro 
is an orthogonal matrix such that 
R2R, AR, R2 = 
After n —2 of these steps, the product P = R,R2---R,~_2 
is an orthogonal 
matrix such that 
P' AP = H is in upper-Hessenberg form. 
Note: 
If A is a symmetric matrix, then H? = (P? AP)? = P? ATP = H, 
so H is symmetric. As illustrated below for n = 5, a symmetric Hessenberg 
form is a tridiagonal matrix, 
H =P' AP = 
OOS 
4) OO 
eo 
6 
ae ox 
* 
* 
© 
* 
* 
* OC ¥ 
*¥ 
OCO 
so the following useful corollary is obtained. 
5.3.5. Corollary. 
Every real-symmetric matrix A can be transformed 
by Householder reduction to a symmetric tridiagonal matrix H_ that is 
orthogonally similar to A. 

682 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
Example (QR Factorization of Hessenberg) 
When the QR factorization of an upper-Hessenberg matrix is required, Givens 
reduction is the perfect tool for doing so because, as mentioned earlier, while 
Householder reduction is usually preferred for dense matrices, Givens reduction 
can exploit the zero pattern in a Hessenberg form. The advantage of using plane 
rotations comes from the fact that the action of Px,,+41 affects only the kt? and 
(k +1) rows. This is illustrated below for a 5 x 5 Hessenberg form. 
Op 
ae eh 
Pe ee ee 
OF 
ee 
de 
oe ek 
O 
* 
* 
* 
*) p 
(Ue Ee 
ee oes 
pret eey ty leeeten |My) x 
pital 
eeceat oe awe ee 
ORO 
kia kek 
ORO 
Sa eee 
ORO) 
ue ae 
OM OMe Ounecs ex 
Ome Oi Ohare oe 
0 O=30 
#7 x 
(5.3.14) 
ek 
eae 
eC 
eee 
O 
*« *« * 
*j} p 
Oe Ee 
Fue" 
esas fg aot 
Pee 
a 
ei 
CO) 
es 
es 
Out OOS Os xa 
(Oe (Oe 9) eet 
On One) 
Ome 
In general, 
Prin: Po3Pi2H = R 
is upper triangular in which all diagonal entries, except possibly the last, are 
positive—the last diagonal can be made positive by the technique mentioned on 
page 675. Thus an orthogonal matrix P is obtained such that PH = R, or 
equivalently, 
H = QR in which Q = P'. 
Jacobi Reduction 
Theorem 3.4.11 on page 346 guarantees that if A is real and symmetric, then 
there exists an orthogonal matrix P such that 
P?' AP = D is a diagonal ma- 
trix. Furthermore, the theory shows that P must be a matrix whose columns 
are a complete orthonormal set of eigenvectors for A. Computing eigenvalues 
and eigenvectors can be an involved process, so it is desirable to have alternate 
ways to diagonalize a real-symmetric matrix. One of the earliest and most clever 
techniques for doing this without explicit knowledge of the eigenvalues or eigen- 
vectors was devised by Carl Jacobi (see page 627). In fact Jacobi's reduction can, 
in some cases, be a viable algorithm for computing eigenvalues and eigenvectors. 
Jacobi's Idea 
For A= ( 7 with € # 0, set o = (a—7)/2€, and let 6 be the angle 
such that cot2@ = o with 
|6| < 7/4. If P = 6 ft) is the rotator (the 
8 
plane rotation) such that 
c = cos@ and 
s = sin@, then using the identity 
cot 20 = (c? — s*)/2cs yields the diagonalization 
PTAP = lee A) a9) 
(5.3.15) 

5.3 QR Factorization 
683 
Angle @ need not be explicitly computed because if t = s/c = tan@, then 
Coasts ol ent? 
2es7—a 
wt 
=> #74%¢t-1=0 = 
t=-c+tvVl4o?. 
Choosing t = —o + sgn(c)V1+ 0? ensures that sgn(t) = sgn(c) so that 
pes 
OS one 
2 
at sl = 
|tenéla 
1g 
16) = 3/4, 
Thus the terms c and s in P are obtained as follows. 
(i) 
Set 
o = (a —y)/2€. 
(ii) 
Set 
t= —o+sgn(c)V1 +0?. 
(iii) 
Set 
e=1/V1+#. 
(iv) 
Set s =te. 
(5.3.16) 
Jacobi's Diagonalization Procedure 
Jacobi's idea applied to an n x n symmetric matrix A yields an n x n plane 
rotation P;; that annihilates the off-diagonal entry € = a;; # 0 of largest 
magnitude by means of the transformation Pi,APi; = B, where c and s are 
computed from (5.3.16) with a = aj, 
y = a;;, and o = (ay — aj;)/2a;;. 
Matrix B agrees with A except in the i" and j*" rows and columns, where 
bj; = b;; = 0. However, if the process is repeated on B to annihilate its off- 
diagonal entry of largest magnitude, then the zeros in positions (i,7) and (j,i) 
that were created in the prior step are generally destroyed. The good news is 
that, when taken together, the affected off-diagonal entries in B have smaller 
magnitude than their combined counterparts in A, so iterating Jacobi's idea 
produces a sequence of matrices that will converge to a diagonal matrix. The 
precise statement and proof of this fact is in the following theorem. 
5.3.6. Theorem. (Jacobi's Diagonalization Theorem) Let A ¢ R"*" with 
n > 2 be symmetric. If {A;,}729 is the sequence with Ap = A and 
As PP Ag Pa: where P; is the plane rotation constructed to an- 
nihilate the off-diagonal entry € #0 of largest magnitude in A,_; by 
using (5.3.16), then 
lim A; =D 
isa 
diagonal matrix. 
(5.3.17) 
k-+0o 
Furthermore, P = limg.. PiP2:::P, 
is an orthogonal matrix such 
that 
P7 AP = D. 

684 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
Proof. 
Begin by proving that each Jacobi transformation decreases the com- 
bined magnitude of the off-diagonal entries. Suppose that aj; = € # 0 is the 
off-diagonal entry of maximum magnitude in A,—1, and, for convenience, re- 
move the subscripts for a moment by setting 
A= Agey= [tpg] 
and. 
B =A, = PUAPG = |[bp1. 
For any X € R"*", let Xog = X — diag(X) (i.e., zero out the diagonal entries 
of X). It follows from Jacobi's idea that the 2 x 2 submatrix lying on the 
intersection of the i*" and j*" rows with the i" and j** columns in B is 
Cans 
Aix 
Ag 
co 
=a \ 
f 
bg, 
0 
=S" 
Co) 
\.Giae 
G3y-)_.\ 
6. a Cia \ OR 
O37 
Je 
Since the Frobenius norm is unitarily invariant, 
at 
= 
ay 
Ai 
Ay 
«a4 
ICs 
aC Nei ai) Alea ert 9) 
This together with byx = axx for all k 
Ai,7 and ||B||,, = ||Al|, yields 
[Boal = Blip — DH = 
= Blip — Do bbe — (6% + 83;) 
kHi,j 
= Az ? > 
ak, — (aj, - 2a, od a;,) 
kHi,j 
(5.3.18) 
= |All — >- a2, — 2a? 
k 
be Aorl|? mi 2ai,. 
Thus each application of a Jacobi transformation decreases the combined mag- 
nitude of the off-diagonal entries. Moreover, since a2, < a?, for all p # q, 
it sfolloge that ||Aonll = peg Mpq < (n? —nja a;,, and (5.3.18) ensures that 
= (||Aorll} — ||Borll?) /2, 10) 
|| Aor lis — ||Bosll 
2 
=> ||Borll} 
< 5 
||Aogll?;, 
where s=(1- ce Ne 
n2—n 
|| Aotll < (n? — n) 
Consequently, after k Jacobi transformations (with the subscripts restored), 
| 
Since 
0<6 <1 
for all n> 2, it follows that [Ag]o¢ > 0, and thus A; > D 
a diagonal matrix. 
MJ 
[Avlonl|? < 
5*|[Aclol|2 

5.3 QR Factorization 
685 
Exercises for section 5.3 
2 OSE ee iit Mile - 
ae a bas ED es Ls ee 
eo 
5.3.1. 
5.3.2. 
5.3.3. 
5.3.4. 
5.3.5. 
5.3.6. 
The uniqueness of the QR factorization of a matrix A € F™*" hav- 
ing independent columns was proven in Theorem 5.3.1 on page 666 by 
appealing to the uniqueness of the Cholesky factorization of A*A. Con- 
struct an alternate uniqueness proof that does not depend on a Cholesky 
or LDL" factorization. Hint: Consider Exercise 1.6.5 on page 50. 
Le Oss 
1 
oe 
Fe 
oy 
en 
4 
tl 
Let A= 
|) 
7 
_3] 
and b=], 
ge 
oda 
1 
(a) Determine the QR factorization of A. 
(b) Use the QR factors from part (a) to determine the least squares 
solution to Ax = b. 
(a) Using Householder reduction, compute the QR factors of 
1 
19m 
34 
Ks (-2 —5 
20). 
i 
 BYe 
(b) Repeat part (a) using Givens reduction. 
Suppose that r = rank(A) € F"*" and P xm 
is a unitary matrix 
such that PA = T = (73). 
(a) Explain how to use these matrices to find orthonormal bases for 
R(A) and N (A*). Hint: Recall Corollary 4.2.12 on page 437. 
(b) If, in addition to the matrices in part (a), Qnxn is a unitary 
matrix such that QB* = 
Sg , then explain how these ma- 
trices yield orthonormal basis for 
R(A*) and N (A). 
i te 
= a= 7) 
Let A = ( 
232-72 
i). Use Householder reduction to find or- 
—4 
1 
-4 
-2 
thonormal bases for the four fundamental subspaces of A, and then 
construct a URV factorization of A. What do you notice about this 
particular URV factorization that makes it somewhat special? 
Ae 
Se et 
5 
Let A= fe a =) 
and b= Ca) 
eye 
(me 
30 
(a) Find the QR factorization of A by using Householder reduction. 
(b) Use the results to find the least squares solution for Ax = b. 

= 
« 
is 
'—< 
tal 
5 
¥ 
. 
¢ fad 
acy) i} 
| 
ow 
we, 
oe 
rae ify. °'5 

5.4 Fourier Expansions 
687 
5.4 FOURIER EXPANSIONS 
+ 
Given a basis B = {uj,us,...} for a vector space V, the expansion of x € V 
in terms of basis vectors as x = )~ ~SkUx is a fundamental cornerstone of linear 
algebra. The scalars €, are uniquely defined for each x, and, as defined on page 
419, they are called the coordinates of x with respect to B. However, if V is 
an inner-product space and B is an orthonormal basis, then each coordinate €; 
specializes to become an inner product because 
(uj|x) = (uj Dd, Sete) = DOG (aslo) = & lll!" = &. 
k 
This means that each x € V can be uniquely expressed as x = )>, (ux|X) Ux 
(recall Theorem 4.1.6 on page 418). Special terminology is associated with these 
kinds of expansions. 
C : 5.4.1. Definition. Let B= {u,, i : 4 be an orthonormal basis for an 
inner-product space Y. For x € V, the representation 
_ 
x= > (uy|x) u 
(5.4.1) 
is called the Fourier 
' expansion (or orthogonal expansion) of x with 
respect to B. The coordinates €, = (u,|x) are called the Fourier 
coefficients of x. 
The Fourier expansion of x resolves it into mutually orthogonal vectors 
€,Upz, each of which is the orthogonal projection of x onto the space (or line) 
spanned by ux. As alluded to in the discussions on pages 101 and 648, the 
magnitude of each Fourier coefficient €, provides an intuitive sense of how much 
of x is directed along ux. The next theorem makes this notion precise. 
Jean Baptiste Joseph Fourier (1768-1830) was a French mathematician and physicist who, while 
studying heat flow, developed expansions similar to (5.4.1). Fourier's work dealt with special 
infinite-dimensional inner-product spaces involving trigonometric functions as discussed in the 
example on page 689. Although they were apparently used earlier by Daniel Bernoulli (1700— 
1782) to solve problems concerned with vibrating strings, these orthogonal expansions became 
known as "Fourier series," and they are now a fundamental aspect in almost all areas of applied 
mathematics. 
Born the son of a tailor, Fourier was orphaned at the age of eight. Although he showed a great 
aptitude for mathematics at an early age, he was denied his dream of entering the French 
artillery because of his "low birth." Instead, he trained for the priesthood, but he never took 
his vows. However, his talents did not go unrecognized, and he later became a favorite of 
Napoleon. Fourier's work is now considered as marking an epoch in the history of both pure 
and applied mathematics. The next time you are in Paris, check out Fourier's plaque on the 
first level of the Hiffel Tower. 

Example 
t 
Proof. 
Equation (5.4.2) follows from the observation that 
cos Aq = (ux|%) / || ell l]>|] = (uel) / [ll] = &e/ [ll] - 
Statement (5.4.3) follows because 
x = y +z and y | z, where y = pene: gu; 
and" Zoe ye ey 416i. Parseval's identity is a consequence of the Pythagorean 
theorem (5.1.15) on page 648 because 
IIx? = ||€rur + €oua +++ + Eaunl|? = ||évuill? + [l€ouall? +--+ [lEnual? 
=|6|? 
+ |€l?+---+|é,/?. 
-1 
To determine the Fourier expansion of x = 
3 with respect to the standard 
inner product and the orthonormal basis 
$ 
1 
1 
1 
(1 
ena 
Pai 
= 
<1 
Uso 
= — 
[1 
, ug 
= — 
—] 
(m (4): a (:) > 
a(-)}. 
compute the Fourier coefficients to be 
€1 = (ui|x) 
= —3/V2,  & = (ug|x) 
= 
2/V3,  &s = (us|x) 
= 
1/V6. 
Marc-Antoine Parseval des Chénes (1755-1836) was a French royalist who had to flee from 
France after Napoleon ordered his arrest for publishing poetry against the regime. Parseval's 
identity appeared in the second of his five mathematical publications. 

5.4 Fourier Expansions 
689 
The Fourier expansion of x with respect to F is therefore 
-3 
2 
1 
ser Ul at paneer Up te 11g, 
v2 
v3 
v6 
and the respective orthogonal projections onto the basis vectors in F are 
. 
ae 
= (-1 
2 
2 
: 
1 
1 
sy 
— =< 
= 
_— 
; 
= — U = — 
) 
= — 
ue = 
— 
_ 
Ee 
Ws 
MARS, /) er 
gees 
ey? 
Pee 
EN 
e 
Notice what is going on under the surface. The Fourier expansion represents 
a change of coordinates (or change of basis) from the standard basis S for 
R® to what might be termed a "Fourier basis" defined by F. The stan- 
xX = uy + €oue + €3u3 = 
dard coordinates [x]s5 = ( | 
are transformed into "Fourier coordinates" 
1 
—3//2 
ixpg = ( 
2/V3 
, and the transforming matrix is the change of coordinate 
1/V6 
1/V2 
1/V3 
—1/V6 
(or change of basis) matrix F = [I|¢s = | -1/v2 
1/v3 -1/V6 | as de- 
0 
1/V3—-2/V6 
scribed in Corollary 4.5.7 on page 514. In other words, [x]s5 = F[x|¢. The 
situation is also reversible in the sense that [x]¢ = F7[x]s because F is an 
orthogonal matrix. 
Example (Fourier Series) 
Let V be the inner-product space of real-valued functions that are integrable on 
the interval (—7,7) with the inner product and norm given by 
T 
T 
1/2 
(ila) =f" 
Feoatoae 
ana 
gh=(f" Peat). 
It is straightforward to verify that the trigonometric functions 
Bie 4100s t. Cos2t ce neil isnt OF Sil sees.) 
constitute a set of mutually orthogonal vectors, so normalizing each vector pro- 
duces the orthonormal set 
B=4 1 
cost 
cos2t 
sint 
sin2t 
sin 3t 
} 
(5.4.4) 
Salen 
Oe N/T ie Ber? OS) 
Tae fy ae ke 
The Fourier expansion of f € VY with respect to B is 
F(t) 
1 ay 
cos kt navy: sin kt 
(5.4.5) 
')=ag—— 
a,—— 
. 
aR 
er 
kat 
' vu 
k=1 
; vr 

690 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
in which the Fourier coefficients are given by 
— (|1)-z 
=e | toa 
an = (SE |p) 
= sy 
' 
f(t)cosktdt 
fork =1,2,3,..., 
a= (Se f= [ f(@)sinktdt for k=1,2,3,... 
Substituting these coefficients in (5.4.5) produces the infinite series 
F(t) = = + » (an cos nt + bp sin nt) , 
(5.4.6) 
where 
ie 
Leis 
7 
An = — 
f(t)cosntdt 
and 
b,= = 
f(t) sin nt dt. 
(5.4.7) 
T 
Je 
= 
The series F(t) in (5.4.6) is called a Fourier series for f(t), but, unlike the 
situation in finite-dimensional spaces, F(t) need not agree with the original 
function f(t). After all, F' is periodic, so there is no hope of agreement when 
f is not periodic. However, the following statement is true. 
e 
If f(t) is a periodic function with period 27 that is sectionally continu- 
ous' on the interval (—7,7), then the Fourier series F(t) converges to f(t) 
at each t € (—7,7), where f is continuous. If f is discontinuous at to but 
possesses left-hand and right-hand derivatives at to, then F'(to) converges 
to the average value 
- 
+ 
where f(to) and f(tg) denote the one-sided limits f(tj) = lim,_,,- f(t) 
0 
and f(s )= limy_, 4+ f(t). 
For example, the square wave function defined by 
oj 
al" 
when "=<. t-<, d, 
fe) = { 
1 when 
0<t<z, 
(illustrated below in Figure 5.4.1) satisfies the conditions above. The value of f 
at t = 0 is irrelevant—it is not even necessary that f(0) be defined. 
A function f is sectionally continuous on (a,b) when f has only a finite number of discon- 
tinuities in (a,b) and the one-sided limits exist at each point of discontinuity as well as at the 
end points a and b. 

5.4 Fourier Expansions 
691 
FIGURE 5.4.1 
To find the Fourier series of f with respect to the trigonometric basis B in 
(5.4.4), compute the coefficients in (5.4.7) to be 
ig 
ie 
sey 
(ho 
ps 
an = — 
F(t)cosneat = = | —cosntdt += | cos 
nt dt 
(al 
RO 
iN 
hier 
T 
Jo 
=0, 
ie 
(ofS 
re 
bn = — 
f(t)sinneat == f —sinntat += | sin 
nt dt 
US 
ee 
Ud 
T 
Jo 
/ te Late 
ce 
0 
when n is even, 
0 
~ | 4/(n7) 
when n is odd, 
so that 
Ae 
a 
Age 
— 
4 
. 
ia tine godless, spe RD Ma MOI OE 
For each t € (—7,7), except for t = 0, it must be the case that F(t) = f(t), 
and 
T(Ce ern) 
2 
Not only does F(t) agree with f(t) everywhere f is defined, but F also 
provides a periodic extension of f in the sense that the graph of F(t) is the 
entire square wave depicted above in Figure 5.4.1—the values at the points of 
discontinuity (i.e., the jumps) are F(+n7) = 0. 
F(0) = 
=0. 
Example (SVD Is a Fourier Expansion) 
The singular value decomposition (or SVD) that is discussed on page 358 is 
a Fourier expansion of a matrix A € F""*", and the Fourier coefficients (or 
Fourier coordinates) are in fact the singular values of A. To understand why, 
suppose that rank (A) =r, and let 
A= ils oY = 01U, 
Vj + 02QU2gV5 +++: +0,U,-V, 
(5.4.8) 
mxn 

692 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
be an SVD for A in which D = diag (01, 02,...,0,) contains the nonzero 
singular values and Umym = (ui|U2|-°*|Um) and Vaxn = [vil vel -:: | Vn] 
are unitary matrices containing respective left- and right-hand singular vectors. If 
matrices Z, €¢ F™*" are defined to be Z, = ugvz, then F = {Z1, Zo,.. - tig t 
is an orthonormal set with respect to the standard matrix inner product defined 
in (5.1.5) on page 646 because 
(Z;|Z;) = trace (Z*Z,;) = trace ((ujvj)*ujv}) = trace (viujujv;) 
trace (0) 
if i # j, } 
a . 
ifs Fd, } (see page 78) 
oy eas (Vivi) 
afi =), 
eh fae 
vem 
=i ifi # J, 
Tent 
i=. 
Writing the expansion in (5.4.8) as 
A=012; +09Z2+:-::+o,Z, 
with 01 >02>::->0,>0 
(5.4.9) 
shows that this is indeed a Fourier expansion with respect to the orthonormal set 
F = {Z,,Zo,...,Z,}, so, by definition, the singular values o, are the Fourier 
coefficients. This can be independently verified by writing 
(Z;,,|A) = trace (Zi, 
A) = trace (AZ;) = trace (Av;,u;,) = tracelo,u,z| 
uz 
= of trace (U,U,) = Of UZUn =O. 
(recall (3.5.6) on page 361). 
All of this provides an alternate interpretation of the singular value components. 
It means that the data in A can be resolved into orthonormal components 
Zp =UgV;, and og provides a sense of how much of A lies in the direc- 
tion of Z,. In particular, o,Z, 
is the orthogonal projection of A onto the 
1-dimensional space in F"*" that is spanned by Z,. More finely granulated, 
each column A,; can be expressed as the Fourier expansion 
Aaj = >) (up| Any) ue = > (up Ass)ux = >_(on0G7 
UR 
(5.4.10) 
k 
k 
k 
in terms of the orthonormal basis B = {uj,U2,...,Um} defined by left-hand 
singular vectors of A (page 361). The coordinates of A,j; with respect to B 
are the Fourier coefficients 
(ui|Ax;) 
o10ig 
(u1|A.;) 
o2025 
[Axj]g = | (urlAes) | = | ot 
|. 
(5.4.11) 
0 
0 
0 
mx 
0 

ory 
5.4 Fourier Expansions 
693 
Example (Spectral Decomposition Is a Fourier Expansion) 
Similar to the preceding example, the spectral decomposition (page 341) of a nor- 
mal matrix A ¢ F"*" is another example of a Fourier expansion. If A is normal 
with eigenvalues {1,A2,...,An}, and if U = [u,|ug| +++ |u,] is a unitary ma- 
trix whose columns are a set of orthonormal eigenvectors with Au; = \;u,; for 
each 7, then 
A=)S°AU; 
(5.4.12) 
in which U; = ujus € F"*" 
is the orthogonal projector onto the span of 
u;. With respect to the standard matrix inner product (page 646), the set 
B={Uj,U2,...,U,} is an orthonormal basis for span (B) because 
(U;|U;) = (ujuj|uju;) = trace (u;[ujuj|uz) = {) fit; 
Thus (5.4.12) is a Fourier expansion of A whose Fourier coefficients are the 
eigenvalues of A. This is corroborated by observing that 
(U;|A) = trace (U; 
A) = trace (AU;) = trace (Au;u;) 
= i; trace (ujzuj) = A; trace (uju,;) = ;. 
Consequently, 2 indicates how much of A lies in the direction of U;—i.e., 
A, U; is the orthogonal projection of A onto the 1-dimensional subspace spanned 
Filtering, Compression, and Dimension Reduction 
A main application of Fourier expansions concerns the need to filter unwanted 
noise from a vector x (or matrix A) of observed data by means of truncating its 
Fourier expansion and retaining only the most significant Fourier coefficients. In 
terms of coordinates, this means transforming data from the standard coordinate 
system S into Fourier coordinates F (as illustrated in the example on page 688). 
Discarding relatively insignificant Fourier coordinates allows the most significant 
portion of the data to be viewed in a space of reduced dimension. Moreover, doing 
so can often reduce the ratio of low-level noise (or uncertainty) to significant 
information. Detailed examples of this are given in the LSI example on page 
694, the PCA development on page 697, and the DFT section on page 716. The 
underlying idea in all such applications is easily understood from the following 
simple scenario. 
The Fundamental Fourier Analysis Principle 
Consider sampling some sort of signal (e.g., audio, video, radar, etc.) or col- 
lecting observations from an experiment in which the readings are recorded in 
a data vector x € F". To keep things simple suppose that only four samples 

694 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
v1 
rs 
. 
s)he 
eA 
(or observations), 21,22,23, and 24, are recorded in x = ( 
"a 
= [x]s. (In 
~ 
v4 
real applications the number of samples is often huge.) Let F = {uj, U2, Ug, U4} 
be an orthonormal basis (a "Fourier basis") that is appropriate to the situation 
under consideration, and consider the Fourier expansion 
4 
pees (uy |x) Ui (u2|x) Ug + (ug |x) ug + (u4|x) Ww = ye Chili 
k=1 
Suppose that the data is contaminated with some low-level random noise (or 
uncertainty) that is unknown other than it is assumed to be uniformly distributed 
across all directions in F —i.e., 25% of the noise is in the direction of each uj. 
For the sake of illustration, suppose that the Fourier coordinates are 
mobo) et fae 
x]e = 
i Ix) = | 0.02 
(u4|x) 
0.01 
Discarding the two smaller Fourier coordinates amounts to giving up only 3% 
of the total information in the data, but 50% of all noise is removed. More- 
over, instead of representing the data in a four-dimensional space, only a two- 
dimensional space is required to represent the most significant portion of the 
data—i.e., x = .49u,; + .48u.. This dimension reduction is often at the heart of 
data compression. 
Example (Latent Semantic Indexing or LSI) 
An interesting application of the SVD example on page 691 concerns the science 
of information retrieval in which it is necessary to search a moderately sized static 
collection of technical text documents (e.g., a set of medical or legal documents, 
or large sets of maintenance manuals) to retrieve documents or pages that are 
best suited to match a few target terms of interest. 
Exact term matching via direct search or look-up is generally not optimal 
because documents that do not explicitly contain a given query term will not 
be found even though they may contain information relevant to the query. For 
example, suppose that document D, explicitly mentions the terms 
y 
Dy) {gas 
Gives? 
"autor 
while terms in document Dp» are 
(5.4.13) 
Do ~ { "fuel," "tire," "car"}. 
A direct search for documents containing { "gas"} or even { "gas," "auto"} fails 
to find D in spite of the fact that D2 may be important. The remedy is 

5.4 Fourier Expansions 
695 
to employ the dimension reduction and compression advantages of a Fourier 
expansion via the SVD as explained earlier. 
Latent semantic indexing (or LSI) starts with an appropriate "dictionary" 
of terms 7;,7>,...,Tm that are either single words or short phrases such as 
"landing gear." It is up to the developer to decide how extensive the LSI dictio- 
nary should be, but even if the entire English language is used, there probably 
will not be more than a few hundred-thousand terms, and this is well within the 
capacity of existing computer technology. Each document D, is scanned for key 
terms—this is called indexing the document. Computer programs are available 
for automatic indexing, but in special cases human readers might be employed. 
After the entire collection has been indexed, each document D; is repre- 
sented by a document vector 
aij 
a24 
ay 
eel, 
(5.4.14) 
Am; 
in which a;; = the number of times term 
TJ; occurs in document D;. All 
terms in the dictionary may not be equally important, so practical applications 
often compensate by replacing raw frequency counts in (5.4.14) with weighted 
frequencies.' The document vectors a; are the columns in an m xX n term-by- 
document matrix 
Di 
Dae 
Ds 
Ty f @1 
@12 
~-* 
Gin 
Ta | Gey 
@22 
-** 
Goan 
A = [a; |a2 --: |an| = 
; 
Im 
\@m1i 
"Qm2 
°°: 
G@mn 
Naturally, most entries in each document vector a; will be zero, so A is a 
sparse matrix—this is good because it means that sparse matrix technology can 
be employed when computations are made. 
Information retrieval systems "mine" a document collection by submitting 
queries to it in the form of a query vector q! = (q1,92,---,4m) that represents 
a few terms of interest in which 
{ 
1 
if term 7; appears in the query, 
i 
Rie. 
0 
otherwise. 
(The q;'s might also be weighted.) To measure how well a query q matches 
document D,;, check how close q is to a; in an angular sense by computing 
A good discussion of term weighting is contained in the monograph Understanding Search 
Engines: Mathematical Modeling and Text Retrieval, 2nd Edition by Michael W. Berry and 
Murray Browne, SIAM, 2005. 

696 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
the magnitude of the cosine of the angle between q and a; —i.e., compute 
q'a; 
q' Ae; 
a er 
ggg 'nares 
(5.4.15) 
llallo asl — all, Asi. 
cos 6; = 
If |cos@;| > 7 for some threshold tolerance 7, then document Dj; is considered 
relevant and is returned to the user—otherwise it is ignored. Selecting 7 is part 
art and part science that is based on experimentation and desired performance. 
The columns of A along with q are often initially normalized to have unit 
length because then |q' 
A| = (|cos61|, |cos62|, ..., |cos@,|) provides the infor- 
mation that allows the retrieval system to rank the relevance of each document 
relative to the query. However, due to things like variation and ambiguity in the 
use of vocabulary, presentation style, and even the indexing process, there is al- 
most always uncertainty (or "noise") in A, so the results in |q? 
A| are generally 
sub-optimal measures of how well query q matches the various documents. 
To filter out some of this noise, the techniques suggested in the fundamental 
filtering principle discussed on page 693 are employed. If rank (A) =r, then an 
SVD 
USP 
Hy 
Ie > ive = > Zi 
with 01 2022>°:::> Or > 0 and Zi = ujvi 
i=1 
i=1 
is judiciously truncated at an appropriate gap in the singular values. If only the 
k <r largest singular values are used, then 
k 
k 
Ana \iemive =) ciZin 
(kicir) 
(5.4.16) 
t=1 
Deal 
can be used in place of A in (5.4.15). In other words, instead of using cos 6;, 
query q is compared with document D; by using the magnitude of 
q' Axe; 
cos ¢; = ———__>—_, 
' 
{alls |Axesll, 
(5.4.17) 
One generally can be generous in the number of SVD components that are 
dropped. There is usually significant "noise" or "uncertainties" in A due to vari- 
ations and ambiguities in language. This together with the fact that numerical 
accuracy is not an important issue (knowing a cosine to two or three significant 
digits is fine) means that it generally suffices to replace a full SVD of A by a 
low-rank truncation as in (5.4.16) where k is significantly less than r. 
e 
The LSI Advantage. 
The primary value of LSI stems from the fact that 
compressing the data in A by using a truncated SVD forces latent semantic 

= 
5.4 Fourier Expansions 
697 
connections to be revealed that would otherwise go undetected. For instance, 
consider the prior example in which document Dj, is indexed by the key 
terms 
D, ~ {gas, tire, auto} 
while document Dy is indexed by 
Dz ~ {fuel, tire, car}. 
If a user's query is just qg ~ {gas}, or even if gq ~ {gas, auto}, then direct 
query matching by brute-force search or by using (5.4.15) will find only D, 
even though Dp» is just as relevant. However, by using A, in place of A 
there is a better chance of finding Dp. 
> Here is the reason. The o;'s are the Fourier coordinates of A with respect 
to F = {Zi, Zo,...,Z,}, so dropping o,41::-o, amounts to projecting A 
onto the smaller dimensional space spanned by {Zj, Zo,...,Z%}. Replac- 
ing A by its projection A, alters (and constrains) the directions in which 
vectors point. Consequently, the effect of this projection is to realign the 
data into fewer directions and thus "move" (or in a rough sense "rotate" ) 
some of the data closer together in an angular sense! This is why, for ex- 
ample, document D2 ~ {fuel, tire, car} has a chance of being matched to 
the query q ~ {gas} when A, is used but not when A itself is used. Be- 
cause of the common term tire, the projection mapping A — A, tends to 
force D, ~ {gas, tire, auto} and D2 ~ {fuel, tire, car} to map to vectors 
along more closely aligned directions, thus revealing their latent semantic 
connection. 
Conclusion. A balancing act is needed when employing LSI. Choosing k too 
small aligns everything into just a few directions, and the ability to discriminate 
between different documents is lost. Keeping k too large will not force relevant 
latent connections to be revealed. This is where the math stops and the engi- 
neering begins. In passing, it should be noted that LSI is not web search. The 
World Wide Web is too large and diverse for LSI to be applied—although in the 
very beginning of the Internet, it was a first attempt. 
Principal Component Analysis (or PCA) 
Depending on the objective, PCA can eee subtly different things to different 
people. Originally, PCA was developed? to transform a set of correlated random 
This is why the cosine measure of proximity is used in place of euclidean distance or some 
other norm-based measure. 
The English statistician Karl Pearson (1857— 1936), one of the early founders of mathematical 
statistics, is said to have had the basic idea in 1901, but the mathematical development of 
PCA is generally credited to Harold Hotelling (1895-1973) in 1933—he was the first to use the 
term "principal component analysis." Hotelling was an American mathematical statistician and 
economic theorist who was a faculty member at Stanford University (1927-1931), Columbia 
University (1931-1946), and the University of North Carolina at Chapel Hill (1946-1973). A 
street (Hotelling Court) in Chapel Hill NC now bears his name. 

698 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
variables {X1,X2,...,Xn} into a set of linearly uncorrelated random variables 
called the principal components, where the first principal component is a linear 
combination of the X;'s that accounts for as much variability as possible (i.e., it 
has maximal variance), and succeeding components are linear combinations that 
account for as much of the remaining variation as possible while being uncorre- 
lated with all previous components. This will become clear as the development 
unfolds. 
First, it is helpful to review some basic definitions and terminology. Recall 
from elementary probability that for random variables X,Y and constants a, 3, 
e 
ElaX + Y] =aE[X]+E[Y] 
(i.e., expectation is linear), 
e 
Var[X] =E|(X - E[X])?] = E[X?] — E[X]?, 
e 
Cov[X,Y] =E[(X —E[X])(Y — E[Y])] = ELXY] — E[X]E[Y], 
e 
X and ¥Y are uncorrelated when Cov|X, Y] = 0 (see (1.6.13), page 47), 
e 
VarlaX + BY] = a?Var[X] + 8?Var[Y] + 2a8 Cov[X, Y]. 
e For arandom vector X = (X1, 43) v2 3) An) © RR? (i.e., a row vector of 
random variables), E[X] = (ELX,], E[X»], ..., ELX,]) (component-wise 
expectations, and similarly for random matrices). 
e The variance-covariance matric' is the positive semidefinite matrix 
¥ = Var[X] = E[(X — E[X])7(X — E[X])] = E[X7X] — E[X]7E[X] 
(recall (3.6.1) on page 382). 
di = 
A 
a 
(5.4.18) 
If 
Var[X;] = 
o?, th 
® 
ar| 
| 
o;, 
then ee 
ee when i # 7. 
e For aconstant A € R"™*, the variance-covariance matrix of XA is 
Var[XA] = E[A7X? XA] — E[A7X7] E[XA] 
= ATE[XTX]A — ATE[X7] E[X]A 
(5.4.19) 
= AT (E[X?X] — E[R7] E[X]) 
A= ATSA. 
a2 
a 
2 
e Ifa 
. 
|, then Xa=)?,a;X; and Var[Xa] = a? Va. 
(5.4.20) 
An 
e For a,b € R"! and random variables 
Y = Xa and Z = Xb, 
Cov[Y, Z] = Cov[Xa, Xb] = E[(Xa — E[Xa]) (Xb — E[Xb]) | 
In some sources the variance-covariance matrix 37 = Var[X] is simply called the covariance 
matrix of X and is alternately denoted by Cov[X] or Cov[X, 
X] 

5.4 Fourier Expansions 
699 
= E[(X — E[X])a(X — E[X])b] 
= E[a™ (X — E[X])" 
(X — E[X]})b] 
=a? Var[X]b = a! Db. 
(5.4.21) 
The Principal Components. Given a vector 7 ean of correlated random variables 
X;, the first principal component is defined to be the random variable that is a 
linear combination Z, = Xv of the X;'s with v; €¢ 
R"*! and vil], = 1 that 
has maximal variance among all possible linear combinations Xa with a ¢ R™*! 
and |la||, =1. In view of (5.4.20), the requirement is that 
Var[Xvi] = max a! Ya. 
llallp=1 
If Ay > Ap > +++ 
> An > O are the eigenvalues of Y = Var[X], then Theorem 
3.4.12 on page 347 guarantees that 
max a' Ya = 1, 
ljal|>=1 
where the maximum is attained at an eigenvector 
a = v; for » corresponding 
to A; with ||v;||, = 1. In other words: 
e 
The first principal component is Z, = Xvi, and Var[Z3] = A1. 
The second principal component Z. = Xv2 with |val|, = 1 must account for 
as much of the remaining variation as posible while having Cov[Z1, Z2] = 0. By 
Ce a means that Var[Xv2] = 
vz Nv2 with ||vo||, = 
1 must be maximal 
over vi. ' Theorem 3.4.12 again applies because (3.4.13) on page 347 yields 
Var[Xvo] = max a' 
Sa = Ao, 
llallg=1 
where the maximum value is attained at a = vo, a unit-length eigenvector of 
» corresponding to A. Therefore: 
e 
The second principal component is given by Zo = Xvo, and Var[Z2] = Az. 
The process for determining the remaining principal components is similar. For 
example, (3.4.13) on page 347 ensures that the variance of 23 = Xv3 with 
val =1 is 
; 
Var[Xv3] = 
max a' a=)sz, 
alspan{v1,v9} 
Jallg=1 
Having a 1 vi 
is equivalent to saying Xa and Xv; are uncorrelated because using (5.4.21) 
yields a'v; =0 
=> 
Cov[Xa, Xvi] = al Sv, =a" (\,v1) =0, and conversely, assuming 
i S(O), 

700 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
where the maximum is attained at a unit-length eigenvector a = v3 for XY 
corresponding to A3 so that: 
e 
The third principal component is given by Z3 = Xvz, and Var[Z3] = As3. 
Continuing in this manner all principal components Z; are found to be 
Z = (Z1, 22, ears ZA) = (Xiv1, X2V2, ene, Ve) = XV, 
where V = [vi|ve2|-::|Vn] 
is an orthogonal matrix containing a complete 
set of orthonormal eigenvectors of 37 corresponding to its respective eigenvalues 
Ay > Ag >+++ > An > O (assuming SY is nonsingular). The principal components 
were created to be uncorrelated, and this is corroborated by (5.4.19) because the 
variance-covariance matrix for Z is (by Theorem 3.4.11 on page 346), 
Aq 
70 
0 
ee 
es 
0 
Aur 
ew 
Var[Z] = Var[kV]=VTZV=|. 
.. 
. 
|, 
(5.4.22) 
6 0 Sanne 
which is the spectral decomposition of © = V Var{Z| V7 = yer 
VE AS 
pointed out in (5.4.12) on page 693, this is a Fourier expansion of 
in niet 
the ,'s are the Fourier coefficients, but now the Fourier coefficients have the 
interpretation as variance. 
Total Variation and Dimension Reduction. Since Var|X;| = A; = i;, the total 
variation in X is defined to be 
» 
Var[X >> 
Age = tT ace (2), 
(5.4.23) 
Thus the term 
di 
At +Ag+:::+An 
explains the proportion of the total variation due to the i'" principal component, 
and, more generally, the ratio 
Ay trAgters 
+g 
Mere ToT 
(5.4.24) 
explains the proportion of the total variation due to the first k principal com- 
ponents. Relative to the dimension reduction discussion on page 735, this means 
that as the ratio (5.4.24) becomes larger, less information is lost by discarding the 
trailing principal components, so the first k components may suffice to account 
for most of the variation in X. 

5.4 Fourier Expansions 
701 
Estimated Principal Components and Sample Variance-Covariance Matrix. The 
development so far has been in terms of the entire population variance-covariance 
matrix 3, which is not often known. In these cases & is generally estimated 
by a sample variance-covariance matric 3 realized from a set of m indepen- 
dently drawn observations (or samples) of X= (X1, X2,..., Xn), where the 
observed data is recorded in a data matriz Xmxn = (eee a riled Xi; is the 
i" sample of X;. For example, if X; = H, Xz = W, and X3 = S area 
per- 
son's respective height: weight, and shoe size, and if m people are sampled, then 
Li = hj, Vig = w;, and x3 = s; are the respective height, weight and shoe size 
of the i" person who is measured, and the resulting data matrix is 
Recall from (1.6.11) on page 47 that the sample covariance of respective realiza- 
T1 
Y1 
; 
x2 
y2 
tions (or samples) x = 
: 
and y = 
of two random variables X 
and Y is given by 
are 
1 
(x — etx)" (y — ey) 
oo = Sea D(a = bx) 
(Yi — By) = 
eal 
: 
Similarly, if wt = (11, p2,-.., Un) = e7 X/m, where pj is the j*" sample 
mean, then the sample variance-covariance matriz is 
a 
oe ory 
le 
Ceara 
Xeon 
y= ( 
en") ( 
"ee =A™A, 
where 
Amxn= ait lee (5.4.25) 
m—1 
Widi ee 
It can be proven that X is an unbiased estimate for &, so the eigenvalues 
Ne and associated (normalized) eigenvectors V; of >y; provide estimates of the 
principal components (and associated variances) of X by means of Z, = XV;. 
In Terms of an SVD. The SVD can be brought into the picture. Rather than 
explicitly forming the sample variance-covariance matrix >» = ATA to extract 
its eigenvalues Bw an SVD of A will do the job. Recall from discussions on pages 
358-361 that if an SVD is A = cal No with D = diag (G1, G2,...,Gr), 
then 
a 
= \ 
anand 
ATAV = VD?2, 
where 
V = [Vil Vol +--+ | Vn] 

702 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
is an orthogonal matrix. In other words, 
AT AV; = A,V; with ||V;i||, =1, so an 
SVD of A produces estimates of the principal components as 
Ai = Xj, 
where ¥; is a right-hand singular vector corresponding to the 7 singular value 
G; of A. By (5.4.23), >>, @? is an estimate for the total variation in X. 
v 
Conventions. Here is where the terminology and notation become somewhat mud- 
dled. Strictly speaking, a principal component is a random variable, but when 
a random vector Xnx1 is sampled m times to produce a data matrix Xm xn, 
it is common to identify X with x drop the hat notation * for designating 
estimates, and adopt the following procedure and convention. 
(1) First, standardize the data in each column of X as described on page 44 to 
make the sample variance-covariance matrix agree with the sample correla- 
tion matrix R.= X7X described on page 48. 
(2) Consider the i'" principal component of X to be z; = Xv; = 0;u;, where 
v; and u, are respective right-hand and left-hand singular vectors corre- 
sponding to the i*" singular value o; of X = UDV'. 
Principal Component Regression (PCR). PCA is often used in conjunction with 
linear regression as discussed on page 489. Recall that the objective in regression 
is to estimate unknown parameters (6; in a linear model 
Y = Bo + 1X1 + BoXo+---+ 
BnXn +e, 
by taking m respective measurements or samples y; and (2j1,2j2,...,2in) of 
Y and (Xj, X2,...,Xn) and hypothesizing that 
Yi = Bo + Biti + Boti2+--++ Bntin +e, 
with E[e;| "3 ) fort) 02. 
As shown on page 489, the result is a matrix equation y = X@+.e, where y 
and Xmxn 
are data matrices that contain the respective observations for Y 
and (X1, X2,...,X,). Under the assumptions detailed on page 493, the Gauss— 
Markov theorem (page 494) guarantees that the best linear unbiased estimator 
for @ is the least squares solution 3B = (XTX) "xTy = Xty. 
As more variables X; (possibly correlated) are included in a regression 
model the possibility of "overfitting" increases, and this renders the results less 
meaningful than otherwise might be obtained by using a smaller number of 
uncorrelated variables. The remedy is to regress on just the most significant 
principal components of the original variables. It is assumed throughout that 
m>n=rank(X). 

5.4 Fourier Expansions 
703 
(1) First standardize X as described on page 702 so that X7X = R is the 
sample correlation matrix. 
(2) Perform an SVD X = Ups )v?, or, equivalently, XV = u( rae 
and set Z= XV in which 2; = Xv; = 0;u; is the i" principal component. 
Using 
X = ZV" in y = XG+e yields the regression equation y = Z¢+e, 
where ¢€ = V7Q, or, equivalently, G = V¢. 
(3) Retain the k most significant o;'s and regress y on Zp = [21 |Z2| --- | zg] 
by using ordinary least squares to obtain estimated regression coefficients 
ul 
y/o1 
uz 
y/o2 
C= (Zit) Ly Zeya 
(see Exercise 5.4.9). (5.4.26) 
ul y/on 
(4) The PCR estimator of @ based on k principal components is B, = Vilg: 
Data Analysis. Analyzing an array of data X = [z;;| € R™*" to reveal things 
such as principal trends, hidden patterns, clusters of commonality, or just to 
graphically visualize the data are common data analysis problems. But inter- 
preting data that is unlabeled or unclassified, or not realized from well-defined 
random variables requires a different approach. In such applications each of the 
m rows x? in X or, equivalently, each of the m columns x; in 
Xe = ixy | xa 
lon 
(5.4.27) 
is viewed as a point (vector) in R", and the entire data set is regarded as a 
"data cloud" in R". For notational convenience, always consider data points to 
be the column vectors x; in X'—e.g, when n = 3, 
Z1 
Y1 
Z1 
xy 
x 
04 
susie 
a 
22 
Yo 
22 
a 
T 
cay 
Mg 
X= 
5 
; 
fe 
= 
, 
a 
S: => 
Yi 
Y2 
ek: 
Um 
= 
Xi | Xeon 
| Xan 
21 
22 
eae 
Zm 
Lm 
Um 
Zm 
x 
and the entire data set is a "cloud" in R®*! as depicted below in Figure 5.4.2 (a). 
The data mean (or centroid of the cloud) is w= Sv", x;/m = XTe € R"™". 
Suppose that the objective is to characterize the principal trend of the data 
with a principal trend line. This is not well-suited to linear regression as de- 
scribed on page 489 because regression requires some variable to be dependent 
or explainable as a linear combination of other variables, which is not an assump- 
tion being made here. As illustrated in Figure 5.4.2(b), the main trend of the 
data is characterized by a line through the cloud's centroid jz in the direction 

704 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
: 
; 
1 
of maximum spread—i.e., by a line £1 = {aw + w|a € R}, where w € R"* 
is a unit-length vector in the direction of maximum variance. 
(Principal trend line) 
w (Direction of principal trend line) 
FIGURE 5.4.2(a): DATA CLOUD IN R? 
FIGURE 5.4.2(b): PRINCIPAL TREND LINE 
The directional variance along £L, = aw+ yp is defined to be the variance of 
the set of orthogonal projections of the data onto £,. The orthogonal projector 
onto span {w} is (ww! )nxn (page 457), so the orthogonal projection of each 
x; onto L£, is given by 
T(x; —) + 
= ww' 
x, + (I—- ww") 
x; = ww 
pt 
(page 458). 
This is illustrated below in Figure 5.4.3 for a data point x; € R°. 
a 
i (:) (Data point in the cloud) 
w (Direction of £1) 
FIGURE 5.4.3: PROJECTION ONTO THE PRINCIPAL TREND LINE 
The mean value of the projected data is 
lS 
jes 
— 
<%= — ) ww"? 
x; (I—ww 
4=1 
i=1 
*) = ww" w+ (I— ww!) p= p. 

5.4 Fourier Expansions 
705 
In other words, the mean of the projected data is in fact the centroid of the 
cloud, independent of w. Let 
(5.4.28) 
and use ||w||, =1 to calculate the variance of the projected data as 
Lr 
ee 
- 
2 
1 
in 
2 
— es 
[Xi — pl|p = oe ww" (xi — H) + 
ul, = =p ||ww"' (xi — 1)||, 
21 
i 
Dail 
heat 
ae 
fg Oe |w? 
(x; — i= papas \(x; — pw)? wl? = || Aw||3 =w! 
A' Aw. 
t=1 
i=1 
Since MAaX||w||.=1 w! 
AT Aw = of occurs at w = vj, aright-hand singular vec- 
tor corresponding to the largest singular value o, for A, the following theorem 
is realized. 
5.4.3. Theorem. The variance a? of projected data onto £, is maxi- 
mized when £; isin the direction of v,;, a dominant right-hand singular 
vector of A. The projection of a data point x; in the cloud onto Ly is 
X, = vivi(xi—p) tp 
(5.4.29) 
in which yp is the centroid of the cloud. 
Analogous to the earlier development of the principal components, sec- 
ondary trend lines through the data cloud are defined by the remaining right- 
hand singular vectors v2,...,V,. For example the second principal trend line 
Lo = {avo + pla e€ R} 
is the line perpendicular to the first principal trend line £;, and the variance 
o% of the data projected onto £2 is maximal among the variance along all lines 
perpendicular to Lj. 
It can be shown that the axes of the ellipsoid in R" that best fits the 
data in the sense of least squares are in fact defined by the principal direction 
vectors {Vv1,V2,.--,Vn}- For this reason these vectors are often referred to as 
the principal axes of the cloud. Below is a summary. 

it 
y 
: 
_——— 
Aree 
° 
ander ee a ees 
Visualization of High-Dimensional Data. Visualization of a data cloud in R" for 
n > 3 is not possible. Any visual insight or analysis must therefore be gleaned 
from the projection of the cloud onto a visual subspace of R*. Subspaces that are 
the most useful for this type of dimension reduction are the principal subspaces 
spanned by a subset of the first three principal direction vectors {vj, 
V2, v3}. 
For example, a cloud such as the one on page 703 can be visualized in the two- 
dimensional principal subspace V2 = span {vi, v2} by using the two largest 
singular value components of 
(X—ep?)? 
n 
AT = 2 So (ujvl)7 
a 
ei 
i 
(5.4.30) 
= So oiviut 
(assuming rank (A) =n <™m). 
i=1 
The transpose is used so that the data (the rows in X) can be considered as 
column vectors. The orthogonal projector onto V2 is 
P» = vive Se Vovs, 
so 
P,A? = o1viu, as o2VvQus 
is the projection of the cloud onto V2. This is illustrated below in Figure 5.4.4 
for a centered cloud in R°. 
Because of the analogy and parallel development between the principal components (which 
are random variables) and the principal directions (which are singular vectors), people often 
conflate the two concepts and use the terminology "principal components" for one, or the other, 
or both, so be careful when reading or hearing someone write or speak about these concepts. 

5.4 Fourier Expansions 
707 
FIGURE 5.4.4: PROJECTION OF A CENTERED DATA CLOUD ONTO A PRINCIPAL SUBSPACE 
The benefit of such a projection is that it provides an indication of how the data 
is dispersed throughout the cloud. If the cloud is in R" for n > 4, then the cloud 
cannot be seen or otherwise visualized in its entirety. Nevertheless, a projection 
of the cloud onto a visible subspace of R?® is still possible. For example, to create 
a two-dimensional image of the projection of a cloud in R" onto V2, use the 
fact from (5.4.11) on page 692 saying that the k'" Fourier coefficient of (A7),; 
with respect to B = {v1,V2,..-,Vn} is 
\ 
nfor hee 
gy = (ula) 
= 
(G4 BEES 
This means that 
P2(A'),; = €1jV1 =I £2;V2, 
so plotting the set of points (€1;,€2;) for 7 =1,2,...,m in R? yields the picture 
of the two-dimensional projection onto 2 similar to that shown in Figure 5.4.4. 
Likewise, plotting the points (€1;,€2;,63;) for 7 =1,2,...,m in R® produces 
the three-dimensional visualization of the projection of the cloud onto V3. In 
such a manner principal directions and dispersions in a high-dimensional data 
cloud can be visually perceived. Insight can also be derived from the projection 
of a cloud onto just a one-dimensional space (a line), and this can be used for 
clustering and partitioning the data as described below. 
Cluster Analysis. Unsupervised learning generally refers to machine learning al- 
gorithms that are used to analyze unlabeled (or unclassified) data. The most 
common unsupervised learning technique is cluster analysis, which is the branch 
of exploratory data analysis designed to expose hidden patterns or clusters of 
commonality in such data. The Fundamental Theorem of Clustering is a well- 
accepted principle that asserts "nothing works always." Consequently, good data 
analysts must have a large bag of clustering tools, and SVD algorithms evolving 
from the ideas on the prior pages are standard implements in this bag. 

708 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
For data that can be plotted in R},R', or R?, then, as depicted below 
in Figure 5.4.5, clusters in the data usually can be visually identified by proper 
choices and orientation of the axes. 
FIGURE 5.4.5: THREE DATA CLUSTERS IN R? 
But for higher-dimensional data, projections onto visual subspaces are required. 
A first step in analyzing a data cloud often involves partitioning the cloud into 
two or more disjoint subsets that are as distinctive as possible. For example, 
use the illustration in Figure 5.4.6 below as a model, and pretend that the three 
data clusters are not visually apparent. As before, let the coordinates of the data 
points be columns of X7 as in (5.4.27), and consider the SVD of A? as shown 
in (5.4.30) except that the 1/,/m term is suppressed because it does not affect 
the singular vectors. When clusters exist, their projections onto the principal 
trend line often (but not always) produce well-delineated gaps. 
Li, =av,+p 
Li=avy+p 
FIGURE 5.4.6: PROJECTION OF CLUSTERS ONTO THE PRINCIPAL TREND LINE 
The projections of cloud points onto £, are, by (5.4.29), the columns in 
X = vivj (X7 — pe?) + pe" =viv 
AT + pe? = v,[o,uT] + pe', 

5.4 Fourier Expansions 
709 
so the gap between the respective i" and j'" projection X; and X; onto L; is 
[Xs — Xsllp = orl — wal. 
(5.4.31) 
When the components of u, are sorted in increasing ordered and relabeled as 
81 = Up(1) S $2 = Up(2) S +++ S $n = Up(n); 
in which p is the permutation created by the sort, then the largest difference 
CRS max{ Si+1 —s;} is called the principal gap in u,. (Use the first gap if two 
are the same.) If the principal gap in uy occurs between positions p(k) and 
P(k +1) so that Gu, = S41 — Sk = Up(k+1) — Up(a), then (5.4.31) ensures that 
the distance between contiguous projections X,(,) and Xp(~41) onto Ly is 
|oce+1) — Xp(k) iP art (Up(e+1) it Up(k)) 
(5.4.32) 
The gap G,; on L; between X,(,) and X,(%41) is an indicator of the distance 
between the two most separated clusters in the cloud, and the size |G,| of G 
is given by (5.4.32). G, is called the principal projected gap. Consequently, a 
natural partition of the cloud is obtained by slicing it with an affine hyperplane 
H that is perpendicular to the principal trend line £; and positioned in the 
principal projected gap as illustrated below. 
hi 
Ly=avi+p 
ra 
Li=oav+ph 
EP 
\ 
| r" 
vf 
\ 
| 
\ 
\ 
NS 
| 
\ 
\ 
UL 
\ vies. 
ce 
| 
A 
PACINO pon 2 
\ 
\ 
SS. 
col 
@ 
"? 
\ 
rrr 
0 
aaah 
\ 
\ 
FIGURE 5.4.7: PARTITIONING A CLOUD AT THE PRINCIPAL PROJECTED GAP 
The separating hyperplane H is the subspace vi that is translated away from 
the origin by a vector ovi +p for some ) € R, so H= (fov1 +p) +v;. The 
position of 
in the principal projected gap is irrelevant, but it can be noted 
that the midpoint of G is [Xp(x) 
+Xpce+1)|/2 +H, so H could be placed there. 
The goal is to separate the cloud into two distinct clusters, and this is 
accomplished by partitioning the data (the columns of X7) in such a way that 
points in the cloud that fall below (or behind) H are put into one set and 
points that are above (or beyond) # go into another set. In other words, the 

710 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
partition decision reduces to whether or not a projected column x; is below or 
above the principal projected gap G1, and it is clear that if G, occurs between 
positions p(k) and p(k+1), then x; projects below (behind) the G; whenever 
p(j) < p(k) and above (beyond) G, when p(j) = p(k +1). Thus the gap- 
partitioning strategy can be summarized as follows. 
e 
The Principal Gap Partition. Sort the components of u; in increasing order, 
and let p be the permutation created by this sort—e., 
Up(1) S Up(2) S*** S Up(n)- 
If the largest gap in this sequence occurs between positions p(k) and p(k+1), 
then the principal gap partition of the cloud (the columns in XT) is 
Pp = {Xp(1))Xp(2)r-+-sXp(ayt and Pa = {Xp(e+1)) Xp(k+2)> +++» Xp(n)} 
where Pg contains those columns that are below G, (the principal gap) 
and P,4 contains columns above GQ}. 
This principal gap partitioning technique can be employed in conjunction 
with a divide-and-conquer strategy. In other words, once a dominant cluster 
in the cloud is identified, it is removed and the process is restarted with the 
remaining data until the analyst is comfortable that no additional viable clusters 
can be found. For a variation of this idea, see Exercise 5.4.10. 
Secondary Gaps. Principal gap partitioning can fail for unusual cloud structures, 
one of which is illustrated below in Figure 5.4.8(a), where elongated clusters are 
closely separated across the principal trend line £L; = {€v; + 
| € € R}. In spite 
of the fact that there are two distinct clusters, there will be no discernible gap 
when the cloud is projected onto £;, but when projected onto the secondary 
trend line Lo = {Ev2+p|€ € R}, a perceptible gap Gz is present, and the two 
distinct clusters are revealed. 
Ly =avo+p 
: 
_ £2 =avo+p 
FIGURE 5.4.8(a): NO PROJECTED GAP IN Lj 
FIGURE 5.4.8(b): PROJECTED GAP IN Lo 

5.4 Fourier Expansions 
711 
Extensions of gap partitioning to account for secondary principal directions can 
be summarized as follows, where G; is the largest gap when the cloud is projected 
onto the trend line defined by the i'" principal direction (i.e., by the i*" right- 
hand singular vector v;. 
If: |G,| > 7 for some user defined tolerance 7, then use the first principal 
direction v; and £, to perform the principal gap partition. 
Else: Successively compare 
|G;| to a tolerance 
7; (variable with 7) for 
i = 2,3,...,k, where k& is a user-input parameter. For the smallest 7 such 
that |G;| >7;, perform a secondary gap partition using L,. 
Repeat: Divide-and-conquer by extracting the resulting cluster from the 
cloud, and then adjust the centroid of the remaining data to repeat the 
process until a desired degree of clustering is produced. 
Note: A rather obvious alternative is to compare gaps |G;| in several trend lines 
£;, before making a partition, and then use the one containing the largest gap 
to extract a cluster. However, a small gap in a primary direction can be more 
significant than a somewhat larger gap in a subordinate secondary direction. 
This is why the tolerances 7; in the second step should be allowed to vary with 
i. Another approach is presented in Exercise 5.4.12. 
Exercises for section 5.4 
5.4.1. Using the standard inner product, determine the Fourier expansion of 
x with respect to B, where 
1 
1 
1 
nya 
1 
(71 
al BEC rey) 
zal) al aly 
_2 
2 
0 
3 \ 1 
6 
> 
5.4.2. With respect to the inner product for matrices (page 646), verify that 
p= {+(2 3), 4G 4) $04) Fb Df 
' 
2) 
Oh 
is an orthonormal basis for R?*?, and then compute the Fourier expan- 
sion of A= ES ' with respect to B. Hint: See Exercise 5.1.16 on 
page 656. 

712 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
5.4.3. 
5.4.4. 
5.4.5. 
5.4.6. 
5.4.7. 
Let M _ be an r-dimensional subspace of F". If B = {ui, u2,..., Ur} 
is an orthonormal basis for M, and if x € M, then, by (5.4.1) on 
page 687, x is equal to its Fourier expansion with respect to B—i.e., 
x = )7\_,(u;'x)u;. However, if x ¢ M, then equality is not possible 
(why?), so the question is, what does the Fourier expansion on the right- 
hand side of this expression represent? Answer this question by showing 
that the Fourier expansion is in fact the point in M that is closest to 
x in the euclidean norm. 
Given an orthonormal basis B for a space V, explain why the Fourier 
expansion for x € V is uniquely determined by B. 
If {uj,,U2,...,u,} is an orthonormal basis for an inner-product space 
Y, explain why (x|y) = >0, (x|u;) (u;/y) holds for every x,y € V. 
With respect to the orthonormal set 
. { 
1 
cost 
cos2t 
sint 
sin2t 
sin 3t 
} 
ONTO 
ENT eh 
determine the Fourier series expansion of the saw-toothed function de- 
fined by f(t) = t for —a < t < am. The periodic extension of this 
function is depicted below in Figure 5.4.9. 
mt 
FIGURE 5.4.9 
Bessel's Inequality The B= {Uj,U2,...,ux,} 
be an orthonormal set 
in an n-dimensional inner-product space V. Derive Bessel's inequality 
saying that if x ¢ V and €; = (u;|x), then se |E:|? < |x|]? . 
Explain 
why equality holds if and only if x € span {uj, U2,..., uz}. 
Hint: Consider ||x — 30*_, €;u;||?. 
Friedrich Wilhelm Bessel (1784-1846) was a German astronomer and mathematician who de- 
voted his life to understanding the motions of the stars. In the process he introduced several 
useful mathematical ideas, one of which is this inequality. 

5.4 Fourier Expansions 
5.4.8. 
5.4.9. 
5.4.10. 
713 
Alternate Query Matching Strategy. An alternate way to measure how 
close a given query q is to a document vector aj; is to replace the query 
vector q in (5.4.15) by the projected query q = Prva), where PRA) 
_is the orthogonal projector onto R(A) along R (A)-, to produce 
q' Ae; 
Al 
a 
es ay 
Ildliz Aes. 
(5.4.33) 
It's proven on page 466 that q = Prca)q is the vector in R(A) (the 
document space) that is closest to q, so using q in place of q has the 
effect of using the best approximation to q that is a linear combination 
of the document vectors a;. Discuss the advantages of this approach. 
Verify that the expression for fe given in (5.4.26) on page 703 is indeed 
correct. 
The Principal Partition' Rather than using gaps in the projections onto 
£, as described on page 710 to partition a data cloud, slice it with the 
principal hyperplane H, = vi + 
passing through the cloud's centroid 
as illustrated below for the cloud in Figure 5.4.2 on page 703. 
Ey 
Li =av, +p 
FIGURE 5.4.10: PRINCIPAL PARTITION 
Using the terminology on pages 703-710, let the coordinates of the 
data points x; in the cloud be the columns in X? as in (5.4.27), and 
Once this partition has been made, it can be refined as suggested by D. L. Boley in Principal 
Direction Divisive Partitioning, Data Mining and Knowledge Discovery (1998), 2(4), pp. 325- 
344, where the cluster with the largest variance is repartitioned. The thinking is that the greater 
the spread in a cluster, the more amenable it should be to further partitioning. Successive 
repetitions produce any desired number of disjoint clusters. However, this (as with all clustering 
algorithms) can fail to capture natural trends—e.g., consider the example depicted in Figure 
5.4.8(a) on page 710. 

714 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
consider the SVD of A? as shown in (5.4.30) on page 706 with the 
1/,/m term omitted (because it does not affect the singular vectors). 
Partition the cloud by putting the points that are on one side of H; (say 
the points "above" H, as depicted in Figure 5.4.10) into one set and 
putting points on the other side (the points "below" 1; ) into another 
set, where the distinction between "above" and "below" is made by 
determining whether the projection x; of a data point x; onto the 
principal trend line L, lies to one side of y or the other. Explain why 
this principal partition is completely determined simply by the signs of 
the components of a dominant left-hand singular vector u, for A— 
e.g., data points corresponding to positive signs in u, can be placed in 
the "above" cluster while columns corresponding to negative signs go 
into the "below" cluster. A data point associated with a zero entry in 
u, is arbitrarily assigned to either cluster. 
5.4.11. Using Secondary Partitions. If in addition to partitioning a data cloud 
with the principal hyperplane H; = vj + w as described in Exercise 
5.4.10, the cloud is sliced with the secondary hyperplane Hz = vt + p, 
then the result is four disjoint sets Cop, Co1, Cio Cii (some of which 
might be empty) in which 
Coo = the set that is to the "right" of H,; and "above" Ho. 
Coi = the set that is to the "right" of H; and "below" Ho. 
Cio = the set that is to the "left" of H; and "above" Ho. 
C11 = the set that is to the "left" of H,; and "below" Ho. 
The cloud is thus partitioned by the four quadrants formed by the 
orthogonal coordinate system {v1, v2}. Similarly, using {v1, v2, v3} 
produces eight disjoint clusters (some possibly empty) determined by the 
eight corresponding octants, etc. The logic from Exercise 5.4.10 shows 
that the partition formed by Hx is determined by the signs of the entries 
in a corresponding left-hand singular vector uz. Use this to determine 
the clusters formed by partitioning a cloud of 10 data points with the 
principal hyperplanes determined by {vi, v2, v3}, where the signs in 
the associated left-hand vectors {u,, U2, ug} are as shown below. 
i 
= iS) 
= i) 
lst 
+ 
- 
5d oe 
3/7 
+ 
+ 
+ 
4 
ne 
ee 
5 
See 
eet 5 
2. 
6/+ 
+ 
- 
ae | ae 
ee 
8)— 
+ 
4 
9/+ 
- 
+ 
1\+ 
+ 
+ 

5.4 Fourier Expansions 
715 
5.4.12. Using Gaps in Secondary Directions. Suppose that the gap partitioning 
technique described on page 710 is extended as follows. When k direc- 
tions {v1, V2,..., Vx} are used to partition a data cloud, label each entry 
Ung. ing UU, = [ui |ug| ++: herpes with either an A or B, depend- 
ing on whether the data point x; projects above or below the principal 
projected gap in £,. In other words if p; is the permutation created 
by the sort up,(1) S Up,(2) S +++ S Up;(m) of components in u;, and 
if the largest projected gap in £; occurs between positions p;(k) and 
p;(k +1) then label the entries in u; as 
nw 
LA. if Dili) 
> 
D5(h), 
Pil) ~ ) B 
if p;(i) 
< p;(k). 
The Partitioning Rule: Data points x; and x; belong to the same 
cluster if and only ifrows i and j in U, have thesame "AB" pattern. 
Use this rule to determine the clusters formed by partitioning a cloud 
containing 10 data points using the three principal directions determined 
by {v1, V2, v3} in which the "AB" patterns in U3 = [uj |u2| us] are 
SOMAANTHRWNHE 
BBLBBLBSBLRLR 
| BHBLBLBLBE 
| BHBBBSraaans 

716 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
5.5 DISCRETE FOURIER TRANSFORM 
A microphone is placed near an acoustic guitar in a recording studio while a 
guitarist strums a standard E chord—.e., E (on string 6), B (on string 5), E (on 
string 4), G# (on string 3), B (on string 2), E (on string 1). The sound signal that 
is recorded during one second of time is shown in Figure 5.5.1 below. 
y (AMPLITUDE) 
0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1 
t (TIME) —> 
FIGURE 5.5.1: SOUND SIGNAL FROM A NOISY E CHORD 
The sound of a chord should consist of harmonic' components in the sense that 
the sound signal should be a trigonometric sum form 
y(t) = se a, cos 2kt + By sin 27kt. 
k 
However, the harmonic components are not at all apparent in Figure 5.5.1. Other 
than the fact that the different harmonics in the chord blend together, the pure 
sound of the vibrating strings is contaminated by some extraneous noise. For 
example, vibrating guitar strings emit a buzzing sound when not properly fretted 
or when the guitar neck and bridge heights are not properly adjusted. In other 
words, Figure 5.5.1 is in fact a signal of the form 
y(t) = Os aj, cos 2rkt + By sin ankt. + noise. 
(5.5.1) 
k 
A primary objective is to reveal the individual harmonic components in the 
chord from the recorded signal while at the same time filtering out a substantial 
portion of the noise. This is going to be accomplished by analyzing snapshots (or 
samples) of the analog signal at n discrete points in time tp = Ofny ty = In. 
Strictly speaking, a harmonic is a function of the form hz (t) = e?7'*t = cos 2rkt + isin 2rkt, 
where & is an integer, but for the time being identify "harmonic" with "sinusoidal" (a combi- 
nation of cosines and sines). 

5.5 Discrete Fourier Transform 
717 
tg =2/n, 
..., 
tn-1 =(n—1)/n. The samples are the amplitudes of the signal 
at these discrete times, and they are collected in a vector 
y(to) 
yo 
y(t) 
yl 
y= 
u(t) 
= 
= 
, 
where 
Pee y(t) = y(k/n). 
(5.5.2) 
y(tn—1) 
nee 
To be a faithful representation of the continuous signal y(t), the discrete ver- 
sion y must necessarily involve many samples, so n must be large. Furthermore, 
many sampling windows [0,1), 
[{1,2), 
[2,3),... may be required to process a 
signal over longer intervals of time. Consequently, an immense amount of infor- 
mation must be stored, and this presents another challenge—data compression. 
But as the time required to analyze, filter, and compress a signal increases, 
the results become less valuable because signal processing must often happen in 
real time to be practical for many applications. Consequently, a major objective 
is to devise practical techniques that facilitate rapid signal analysis. The key is 
to apply the fundamental Fourier analysis principle described on page 693. 
The Most Appropriate Coordinate System 
It is stated on page 2 that applied linear algebra is the art of determining the 
most appropriate coordinate system in which to formulate and analyze a given 
problem and then to realize an advantage by means of a matrix decomposition. 
Successfully addressing the challenges articulated in this application is a perfect 
illustration of this art. 
The standard basis' T = {€9,€1,...,€n—1} is a time-related basis because 
the k*" standard coordinate y, in 
Yn-1 
is the amplitude of the signal at time t = k/n, and writing y = po, Ykek 
represents the resolution of y into time components. However, a representation 
Throughout this section it is convenient to index all sequences and components by starting 
with 0 rather than 
1. In other words, the first position in a vector is indexed with 0, the 
second with 1, etc., until the nth position is indexed with n— 1. In particular, this means 
that the standard basis for F" is 
eo = 
' 
, 
e1 = 
: 
5 Gers) (Cyt == 
0 
0 
1 

718 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
in terms of time coordinates is not the best way to analyze the signal. It is 
better to express y in terms of a frequency-related basis F, and then analyze 
the signal in terms of its frequency coordinates [y|y. Understanding why this is 
true requires some knowledge about the roots of unity, so here is a quick review. 
Roots of Unity 
For a given positive integer n, and for k =0,1,2,..., let 
Qnrk 
: 
OTK 
k — 92nki/n — cog —— + isin 
—. 
n 
n 
; 
Dire. 
Pe 
PAE 
w = e2ti/n — eos — +isin— 
and 
w 
n 
n 
The complex numbers 1, w, w?,...,w"—! are called the n'" roots of unity be- 
cause they represent all solutions to 2" = 1. Geometrically, the n*? roots of 
unity are the vertices of a regular n-sided polygon inscribed in the unit circle in 
the complex plane. This is depicted in the diagram below for n = 3 and n= 6. 
1 
WA 
JW 
(ORIENTED COUNTERCLOCKWISE) 
a 
f= 3 
n=6 
The roots of unity are cyclic in the sense that if k > n, then w* = wk(modn), 
where k(mod n) denotes the remainder when k is divided by n. For example, 
if 
n=6, then wWS=1, w'=w, we =w?, w9 =w?, etc. If € is the conjugate 
aT 
2 
€=B = cos — —isin = = e~271/", 
(5.5.3) 
n 
n 
then 1, €, €?,...,€"~! are also n roots of unity, but, as illustrated below for 
n=3 and n=6, they are oriented clockwise around the unit circle rather than 
the counterclockwise ordering of the w*'s. 
e4 
A 
3 A 
NA ey 
LM 
(ORIENTED CLOCKWISE) 
g 
€2 
é 
5 ii Ww 
=) i a 
Properties of w and € 
The following properties of the roots of unity are important to the development. 

: 
rw 
: 
; 
F 
Proof. 
The first equality in (5.5.4) is true because 
- 
. 
wk = e2rik/n = cos(2nk/n) — isin(2rk/n) = e~27#/" — wk 
and the second equality in (5.5.4) follows from 
B 
Cee 
eee 
Using the Ydentity (1+ w* + wk +--+. + wl") (1 — wk) = 0 yields (5.5.5), 
and equation (5.5.6) is self-evident. 
From Continuous to Discrete 
Moving from y(t) to y in (5.5.2) amounts to converting a continuous-time 
= 
signal into a discrete-time signal, so it is convenient to replace the continuous- 
time variable t € {0,1) with a discrete-time variable t that is given by the 
nm xX 1 vector 
to 
0/n 
ti 
1/n 
i) 
|e 
ee | eee 
(5.5.7) 
as 
(n—1)/n 
In other words, t contains the times t € [0,1) at which samples are being taken. 
For any function y(t) defined on (0,1), writing y = y(t) denotes the vector 
whose components are the sampled values of y(t) at the times in t, and any of 
the following notations may be used. 
y(to) 
y(0/n) 
Yo 
y(t1) 
y(1/n) 
Yi 
y = y(t) = ly]r = 
y(t2) | _ | y(2/n) 
She 
y(tr—1) 
y(n—1/n) 
a 

720 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
For example, if the continuous cosine and sine functions cos(27 ft), sin(27ft), 
are sampled at t, then their discretizations are 
cos(27 fto) 
cos (2rf 2) 
cos(27 ft1) 
cos (nf: +) 
cos(27 ft) = 
cos(2rftz) | — 
cos (rf: 2) 
d 
cos(2m ftn1) 
cos (2x f-2=1) 
an 
5.5.8 
sin(27fto) 
sin (nf: 2) 
( 
) 
sin(27ft1) 
sin (2-2) 
sin(2r ft) = | sim@rft2) | 
=] 
sin (2nf-2) 
sin(2m ftr1) 
sin (2nf-2=1) 
The discrete versions of the continuous exponentials e?"/* and e~?"f¢ become 
wf 
cof 
wif 
elf 
oe | ee 
ands. 
eft a) 
ee, 
(5.5.9) 
yr-DF 
e(n-1)f 
Familiar identities involving trigonometric and exponential functions of a 
continuous variable t remain valid for the discrete variable t. In particular, 
e2nift f ea 27ift 
2a 
te 
isi 
e 
= cos(27ft) +isin(27ft), 
cos(27ft) 
2 
a 
(5.5.10) 
eatift ty ee 2nift 
ar 
2i 
; 
e 2mift — cos(Qrft) —isin(Qxft), 
sin(2Q7ft) 
and 
e2ri(n—f)t _ ,—2mift_ 
(5:0,2 5) 
Integer Frequencies 
The continuous functions cos(27ft), 
sin(27ft), and e?"f¢ are periodic func- 
tions, and the value of f in each of these is the frequency—i.e., the number of 
cycles per unit of time. Geometrically, f is the number of oscillations the graphs 
of cos(27ft) and sin(27ft) exhibit over [0,1), and, for e?/*, f is the number 
of revolutions traced around the unit circle in one unit of time. It is necessary to 
understand integer frequencies before non-integer frequencies can be analyzed, 
so, for the time being, all frequencies are assumed to be nonnegative integers! 
To emphasize this, f is replaced by letters 7 or k. 
The assumption that frequencies are integers is not overly harsh because the Fourier series for 
a periodic function requires only integer frequencies—see the example on page 689. 

5.5 Discrete Fourier Transform 
PAD 
The advantage of integer frequencies is that the set of discrete exponentials 
{e2mi0t Q2milt Q2mi2t 
 Q2mi(n-1)ty 
(5.5.12) 
is an orthogonal set because for nonnegative integers 7 and k (<n) , properties 
(5.5.4) and (5.5.5) yield 
(om eee = (e7eueya e2mikt _ (emit) e 
Qrikt -¥ 
WPI yyPk 
(5.5.13) 
n—1 
: 
in 
= Sura =a Rice 4, 
cn 
On 
aikk 35. 
Consequently, the set in (5.5.12) becomes an orthonormal basis for C" by 
normalizing each exponential—i.e., multiply each e?7** by 1/,/n to produce 
uz = e?"!Kt /./n. The resulting orthonormal set is the frequency basis. 
The Frequency Basis 
5.5.2. Definition. 
The orthonormal basis F = {u,m,...,un—i}, where 
a 
\ 
Wyse 
Qrikt 
a 
= 
e 
1 
2k 
uy, = ii oh 
" 
for k=O 
one 
n= 
Dk 
is called the frequency (or Fourier) basis for C". It is important to 
note that (5.5.11) translates to say 
eo 2Tikt 
yk 
Vn 
for each k==-1,2,.. 357: 
(5.5.14) 
The orthonormality of F means that the sample vector y has a Fourier 
expansion 
y = (ugly) uo + (uz |y) ur +++: + (Up-1|y) Un-1 
(5.5.15) 
with respect to *, and the coordinates 
(uoly) 
(uily) 
lyl- = 
(Un—-1|y) 
of y with respect to F are the Fourier coefficients (ux|y) (page 687). These 
coordinates are called the frequency coordinates of y because |(ux|y)| 
is the 
magnitude of the projection of y onto the span of ug (page 102). In other 
words, | (uz|y)| indicates how much of the signal has frequency k > 0. The 
DC component of the signal is at k = 0. 

Inner Product Spaces and Fourier Expansions 
Advantage of Frequency Coordinates 
To understand why it is a huge advantage to express the data in y in terms of 
frequency coordinates rather than time coordinates, remember that the assump- 
tion in (5.5.1) is that the discrete data in y comes from a continuous signal 
y(t) that is a combination of harmonic (sinusoidal) components possibly con- 
taminated by noise. Revealing the individual harmonics in y(t) is the key to the 
problem of compressing the signal as well as cleaning it, and this is exactly what 
the frequency coordinates do. 
To see how, consider continuous exponential functions e 
and eso" 
for t € [0,1) that have frequency 0 <k <n/2, where n is an even number! 
When these exponential functions are sampled at the vector t in (5.5.7), the 
time coordinates eas and cmc 
are given by (5.5.9) on page 720 
with f =k, but the frequency coordinates Coat x and aa 
simpler because (5.5.14) says that 
2rikt 
xy are much 
e2tikt x= Jn ug 
and 
e2tikt = JN Un—k- 
Consequently, it follows that 
[on lea vie, 
Bnd) 6 | =v 
iene 
(5.5.16) 
In other words, the discrete exponentials have just a single nonzero frequency 
coordinate—namely ,/n in position k for the positive exponential and in posi- 
tion n—k 
for the negative exponential. Therefore, when cosine and sine signals 
c(t) = acos(27jt) and s(t) = 8sin(2rkt) on [0,1) with respective frequencies 
0<j<n/2 and 
0<k<n/2 and respective amplitudes a and 8 are sampled 
at t to produce 
c=c(t)=acos(2rjt) 
and 
s=s(t) = @sin(2zrkt), 
the frequency coordinates 
[c|¢ and [s]¢ each have only two nonzero compo- 
nents. This follows because the coordinates of a linear combination are the linear 
It will later become clear that it is an advantage to have n = 2" for positive integers r and 
why it is desirable to have n > 2k. For now, note that 
n>2k — > n-—k wks 
; 
1
.

5.5 Discrete Fourier Transform 
723 
combination of the coordinates (Exercise 4.5.2, page 524), so (5.5.16) together 
with the identities (5.5.10) on page 720 produce 
[c]z = [a cos(2mjt)]| , = $( 
le] fh [o>], a ie, + €n—j); 
and 
[s]- = [Gsin(2rkt)] , 
= 5 (le) x (pases we 
n 
ae 
(ex = Ona: 
In other words, as illustrated below in (5.5.17), the nonzero positions in the 
scaled coordinates (2/,/n)|c]|z = a and (2/,/n)|s]~ = ib correspond to the 
respective frequencies 7 and k of c and s, and the values in these positions 
correspond to the respective amplitudes a and 8. 
0 
+0 
0 
+0 
0 
+1 
0 
+1 
9 
Paes, 
9 
—B 
Hk 
ye =a= 
and 
ale = ib =1 
(5.5.17) 
oy |) <= (e=3) 
B | + (n—k) 
07, 
at) 
ee (nel) 
Example 
Below are respective plots of the time coordinates [c]7 and [s]7 and the scaled 
frequency coordinates a = (2/,/n)|[c]¢ and ib = (2/,/n)[s|- for n = 32 and 
Geet 
P 
B 
[3 sin(8rt) | 
- 
ike 
4 
0 ee, 
eicttatetast tate 
8 
16 
24 
28 
32 
- Bi 
a = (2/32) [a cos(87t) % 
b = (2/32) [8 sin(8rt) | 
ie 
FIGURE 5.5.2: DISCRETE TIME AND FREQUENCY DOMAINS FOR COSINE AND SINE 

724 
Example 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
Replacing time coordinates by frequency coordinates reduces the informa- 
tion in the respective cosine and sine coordinate vectors from n nonzero entries 
down to only two. This is significant, but the situation is even better. The orig- 
inal signals c(t) and s(t) are each completely characterized by just the single 
coordinate in the upper half of a = (2/,/n)[c]z and ib = (2/,/n)[s]y. The 
positions j <n/2 and k<n/2 in (5.5.17) are the frequencies, and the val- 
ues in these positions are the corresponding amplitudes. In other words, when 
a sinusoidal signal (a combination of cosines and sines) in which all component 
frequencies are between 0 and n/2 is sampled in time and then converted into 
frequency coordinates, half of the frequency coordinates can be discarded! Below 
is a summary of the facts established to this point. 
5.5.4. Theorem. 
If c(t) = acos(27jt) and s(t) = @sin(2rkt) 
for 
té (0,1), and if c = c(t) = acos(2mjt) and s = s(t) = Gsin(27kt), 
where t is given by (5.5.7) with n even and n > max{2j,2k}, then 
2 
: 
oe 
: 
es =O, te, 
)) = a 
ahd 
abl = Bi(—e, + €n_x) = ib. 
The nonzero entries in aj/2 and bj/) (the upper half of a and b) 
completely define c(t) and s(t). The respective positions 7 and k are 
— 
the frequencies, and the respective values a and { are the amplitudes. 
Consider h(t) = cos(27t) + 4sin(47t) + 2cos(6mt) +2sin(6mt) for t € [—2,2] as 
shown below. 
7 
[, 
FIGURE 5.5.3: A(t) = cos(2mt) + 4sin(4nt) + 2cos(6zt) + 2sin(6zxt) 
If A(t) is sampled at eight points t = 0, 1/8, 2/8, ..., 7/8 in {0,1) to obtain 
time coordinates hg,; = h(t), then the result is shown below. 

5.5 Discrete Fourier Transform 
725 
3 
4.7071 
—2 
= lLfeH( tel 
h = (hl; = | "1% 
3.2929 
2 
—6.1213 
NoauakhdwtHornwnaan 
FIGURE 5.5.4: A(t) SAMPLED AT EIGHT POINTS 
The coordinates of a linear combination are linear combinations of the co- 
ordinates (see Exercise 4.5.2 on page 524), so (5.5.17) means that the scaled 
frequency coordinates (2/\/8)[h]- of h must be 
2 
p 
—lh 
= — 
([eos(2nt 
+ 4\sin(47t)|¢ 
+ 2/cos(67t)| 
+ + 2/sin(6rt 
) 
lle = 
(Icos(2mt)]x + 
dlsin(Art)]> + 
2[eos(6nt)] 
x + 
2[sin(Brt)] = 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
1 
0 
0 
4 
0 
0 
0 
=i 
Sale 
eri 
eg leralns. 
[oll de 
acl wa 
thec tel 
eet aeioe 
0 
0 
2 
2 
2 
2 
0 
4 
0 
0 
0 
4 
1 
0 
0 
0 
1 
0 
The nonzero entries must be as indicated because cos(27t) 
>a=1, 
k=1, 
and 4sin(4mt) > 6 = 4, k = 2, etc. The real and imaginary parts, a and b, 
are plotted below. The cosine components of h(t) are reflected in a, while b 
indicates the sine components. 
fe ae RE((2/v8) [h] =) 
(COSINE INFO) 
b= Im ((2/V8)[h] x) 
(SINE INFO) 
FIGURE 5.5.5: DISCRETE FREQUENCY DOMAIN 

726 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
Notice that only half of the entries in (2/\/8)[h|z are needed to describe h(t). 
In other words, h(t) is completely characterized by 
0 
' 
1 
: 
a1/2 + iby/2 = 
0 
+1 
a 
2 
Changing Time Coordinates into Frequency Coordinates 
The frequency coordinates (2/./8)[h]z in the previous example were able to be 
determined without ever using the time coordinates [h]7 because the individ- 
ual harmonic components of h(t) were explicitly given. However, in practical 
applications knowledge of the individual harmonics in a continuous signal y(t) 
is rare, and the problem is almost always that of determining the harmonics by 
examining time samples. In other words, the problem is, given |[y|7, find ly|r. 
This is applied linear algebra at its best. Recall from Theorem 4.5.3 on page 
510 that if %¢ and VY are vector spaces with respective bases B and B', and 
if T : U > Y is a linear transformation, then for y € U, the coordinates of 
T(y) € V with respect to B' are 
[T(y)le = [T]ee'ly]e. 
In particular, if YU = 
V = C" and T =I, then setting 
B= 7 and B' = F 
yields [y]¥ = [I(y)]¥ = [I]r+ly]7, or equivalently, 
le = We7Iyl7 
(5.5.18) 
because [I] = [I], by Theorem 4.5.5 on page 513. Since T is the standard 
basis for C", 
Lee 
1 
1 
1 
1 
oa 
ww 
wr-l 
nyt 
1 
jn— 
er = [uo 
|ur| ++ | una a 
1 
w 
= 
es 
1 wn-l 
yn-2 
5 
nxn 
This is a unitary matrix because its columns are an orthonormal set, so (1.10.2) 
on page 92 ensures that 
fa 
7 
1 
ae 4 
g 
ipa: 
ay 
1 
2 
4 
n—2 
1 
1 (a, 
fred 
€ 
ie 
Thus the following fundamental fact has been established. 

For example, the Fourier matrices of orders 2 and 4 are given by 
- 
1 
1 
1 
al 
Syke 
Zl 
=e 
i 
Ae ee EG 
EA 
Noe ara 
(5.5.21) 
il 
i=l 
=i 
3 
Kory = 
r , the discrete Fourier transform (or DFT) of y is the product 
25 
1 
1 
1 
i 
3 
0 
ih 
sa 
Sl 
i 
5 
6 — 101 
eo 
0h ew 
eae 
dM becca 
booed 
bea Ca 
(5.5.22) 
1 
do 
Sth 
al 
8) 
6 + 10i 
Properties of F,, and F7,! 
tT 
The Fourier matrix F, has some interesting properties. To begin with, it is a 
Vandermonde matrix as defined on page 60. Furthermore, F,, is a rare example 
of a naturally occurring matrix that is both complex and symmetric but not 
hermitian—i.e., F2 =F, but F* #F,,. Perhaps the most valuable property 
This definition of the DFT excludes the scaling factor 1/,/n present in (5.5.20). This allows 
for a cleaner decomposition of the Fourier matrix in (5.5.38) on page 743, which in turn yields 
the fast Fourier transform algorithm on page 746. Furthermore, the definition more closely 
resembles that for the continuous Fourier transform F{2x(t)}= [° e~?7fta(t)dt = F(f). 
co 
Some computer codes might include scaling factors when computing 
a DFT—e.g., the DFT of 
y is computed as (1/n)Fny. Be sure to check the code's documentation. 

728 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
is the fact that F,,/./n is a unitary matrix. Among other things this means that 
F,, is particularly easy to invert because 
{F,/Vn} 
= {Fn/Va}" =Fa/vn => Fy =F, /n, 
(5.5.23) 
so inversion can be accomplished by conjugation. But inversion is even simpler 
than conjugation because invoking (5.5.11) on page 720 produces 
ode [e@riot Jermilt |... jePatint)e) 
n= 
os bees Jen 2ri(n-lt | Cea ier ap 
The columns in this matrix are the same as those in F,,, but, with the exception 
of the first column, they are in reverse order. In other words, 
1 
0 
Omg 
= 
ONG 
Ure 
Ko = BJ 
awe 
(O00 
£ 
4a: 
he 
(5.5.24) 
OL 
cea) 
GO 
so (5.5.23) simplifies to become 
1 
| 
ae es 
F,,J 
(inversion by scaled column reversal). 
(5.5.25) 
For example, consider the Fourier matrix 
F,4 given in (5.5.21). Using either 
conjugation from (5.5.23) or scaled column reversal from (5.5.25) produces 
1 
1 
1 
1 
1 
—1_ 
1 
1 
=—1 
= 
Bape 
aaa 
yes 
(5.5.26) 
Le 
1 
oh 
i 
Direct multiplication verifies that F,F;' = ake 
5.5.6. Definition. For any x € C"*', the product F>'x is called the 
inverse discrete Fourier transform (or inverse DFT) of x. 
It follows from (5.5.25) that the inverse DFT of x € C"™! is 
y =F,,'x =F,,(Jx/n). 
(5.5.27) 
This means that the inverse DFT can be executed on x by applying the ordinary 
DFT to a scaled reversal of x. Consequently, if one has a favorite algorithm or 

5.5 Discrete Fourier Transform 
729 
program (call it FFT to suggest the fast Fourier transform algorithm discussed 
on page 746) for executing an ordinary DFT, then exactly the same procedure 
can be used to return the inverse DFT by performing three simple steps. 
(1) 
y+—REV(x) 
(Reverse the last n — 1 components—this forms Jx ) 
(2) 
y+¢—FFT(y) 
(This forms F,Jx) 
(3) yt—n-ty = (This forms n-!F,,Jx = Fz!x) 
For example, if x= | _, 
|, and if FFT(y) returns the value of Fay, then 
3 
i 
4 
1 
FFT(REV(x)) 
FET ( ; 
-1 
3 
0 
0 
Fp 
ee (etal era ee | 
4 
4a\en! 1 
4i 
A slightly different inversion technique comes from (5.5.23), which says 
Exe 
= 
(5.5.28) 
so an algorithm or program (such as the FFT on page 746) that performs the 
ordinary DFT can be adapted to return the inverse DFT as follows. 
(1) y+-X 
(Conjugate entries in x) 
(2) 
y<+<— FFT(y) 
(Form F,,x) 
(3) 
yon ly 
(Form n—/F,,x = F,'x) 
i 
For example, if x = 
- , and if FFT(y) returns the value of Fay, then 
3 
FFT(X) 
G 
i 
ew 
; 
ef) 
A) 
1 
- 
(Oy Wy 
0 
FP, x= 
rer 
elas 
FEE 
lees 
yi | 
Oecd 
see 
ON 
3 
—4i 
i 
Changing Frequency Coordinates into Time Coordinates 
Theorem 5.5.5 on page 727 says that changing time coordinates y = 
[y]7 
into frequency coordinates 
[y]¢ is accomplished with 
a DFT by computing 
[ylz = Fn ly]r/Vn. So, to go in the opposite direction to transform frequency 
coordinates back into time coordinates, use (5.5.25) to conclude that 
lyl7r = VnF; lyl-z =Fn (B=) , 
where J is given in (5.5.24). 

730 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
In other words, time coordinates are recovered from frequency coordinates 
by simply executing 
a DFT to a scaled reversal' 
of the frequency coordinates. 
Alternately, conjugation can be used by applying (5.5.23) to obtain 
fylr = Falvle 
DFT Signal Analysis 
Combining the DFT's ability to transform time coordinates into frequency coor- 
dinates together with the knowledge of what the frequency coordinates of cosines 
and sines must look like provides a powerful tool for analyzing harmonic signals 
that have been sampled in time. These ideas are brought together below. 
5.5.7. Theorem. If y is a vector of n time samples from a harmonic 
signal y(t), and if mn is even and exceeds twice the highest frequency 
in y(t), then the cosine and sine components of y(t) are respectively 
revealed (and completely determined) by the real and imaginary entries 
in the upper half of the vector 
2 
= F,y =a-+ib. 
In particular: 
e 
The j'" entry in aj/2 (the upper half of a) is a; 40 if and only 
if y(t) has a cosine component of the form a; cos 27Jjt. 
e 
The k entry in bi/2 (the upper half of b) is —S, 4 0 if and 
only if y(t) has a sine component of the form ({, sin 27kt. 
Proof. 
For m = 
(n/2) —1 
(the maximum detectable frequency), and for 
y(t) = doer) ay cos 2mjt+ pg Be sin Wkt, let y = y(t) = yet Cen ee, 
where c; =a; cos27jt and sz, = 8, sin27kt. Remembering the conventions for 
indexing the standard basis vectors (see the footnote on page 717), use 
Filylr = Vnlyle 
from (5.5.20) together with Theorem 5.5.4 and the fact that coordinates of a 
sum are the sum of the coordinates to conclude that 
2 
2 
2 
Sy ie va Wie = <3 Peron Y sglelz +O lel 
j 
k 
j 
k 
The term "reversal" is in the context of the J matrix from (5.5.24) in which all components 
except the first are reversed. 

5.5 Discrete Fourier Transform 
731 
Example 
= So aj(ej + en—j) +1 >> Be(—ex + en—r) 
(5.5.29) 
j 
k 
a \ 
0 
—Bo\ 
+0 
oil 
soi 
-Bi \ +1 
a; | 
oJ 
a 
ony | 
Dies 
+ m=(n/2)-1 
: 
Be 
+—m 
SEA baie at 
ee litem) 
sass) 
te 
a; | 
+ (n-3) 
B, | < (n—k) 
a 
fe (n—2) 
1 
+ (n—2) 
ao / < (n—1) 
Bo 
+ (n—1) 
Find the harmonic components of the signal y(t) shown below in Figure 5.5.6. 
6 — 
-6 
FIGURE 5.5.6: SIGNAL WITH UNKNOWN HARMONIC COMPONENTS 
It appears that no component has frequency greater than m = 1. If this is 
correct, then analyzing the signal requires at least n samples, where n is even 
and n > 2m = 2. In other words, n = 4 is the smallest number of samples that 
will do the job. Sample the signal at t = 0, 1/4, 1/2, 3/4 as indicated in Figure 
5.5.6 to obtain 
naa} _( 8 
y 
=lyl7 = y(t) 
ytl/2) 
Som 
Beh 
y(3/4) 
—5 
1 
I 
0 
0 
0 
From (5.5.22) on page 727, aFay S95 abs 
= 
3 +1 * 
=a-+ib. 
6 + 101 
3 
5 

732 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
' 
: 
: 
0 
The individual harmonic components in y(t) are identified from aj/2 = ey 
and by/2 = 
mee Theorem 5.5.7 ensures that there is one cosine component 
with frequency j =1 and amplitude a= 3, and one sine component having 
frequency k =1 and amplitude 6 =5. Therefore, the signal is 
y(t) = 3cos 2nt + 5 sin 2zt. 
(5.5.30) 
Had the signal contained frequencies greater than m = 1 (which it did not), 
they could not have been discovered using only four samples because the high- 
est frequency m that can be revealed with an even number of n samples is 
m = (n/2) — 1. Consequently, conservatively large values of n are required in 
the face of uncertainty. Exercise 5.5.3 examines what happens when n = 8 rather 
than n = 4 samples are used for the signal in Figure 5.5.6. 
Non-Integer Frequencies 
When a harmonic signal y(t) is sampled at n points in [0,1) to produce y, 
then Theorem 5.5.7 says that the integer frequencies are revealed by the positions 
of nonzero components in (2/n)F,y. However, column vectors do not have 
fractional positions, so non-integer frequencies cannot be identified by examining 
positions of nonzero entries. For example, when y(t) = cos(2710t) is sampled 
at_ 32 points in [0,1), then, as illustrated in Figure 5.5.7 below, the frequency 
f =10 along with everything else is clearly revealed. 
0 
a = Re(F32y/16) 
0 
5 
10 
15 
20 
25 
30 
0 
ib = Im(F32y/16) 
=0 
5 
10 
15 
20 
5 
5 
, 
25 
30 
FIGURE 5.5.7: FREQUENCY DOMAIN FOR y(t) = cos(2710t) 
But if the frequency is changed from 10 to 10.25 and y(t) = cos(27 10.25 t) 
is sampled at 32 points in [0,1) to generate y, then plotting the real and 
imaginary components of F3,y/16 = a-+ib produces the following results. 

5.5 Discrete Fourier Transform 
733 
' 
a = Re (F32y/16) 
0 
5 
10 
15 
20 
25 
30 
FIGURE 5.5.8(a): SPECTRAL LEAKAGE INTO THE REAL FREQUENCIES 
1 
Sc 
=) a 
oo 
Leek: 
f=) 
=) = 
SE 
0 
ib = Im (Fa2y/16) 
0 
5 
10 
15 
20 
25 
30 
FIGURE 5.5.8(b): SPECTRAL LEAKAGE INTO THE IMAGINARY FREQUENCIES 
The phenomenon displayed in Figures 5.5.8(a) and 5.5.8(b) is referred to as 
spectral leakage because when a signal is not periodic over the sampling inter- 
val, the effect is that non-integer frequencies fracture and "leak out" across the 
entire frequency domain. When this occurs, a standard practice is to estimate 
the frequencies at which the signal's power is most concentrated. This is called 
power spectrum estimation or power spectral density estimation. Physics says that 
power is proportional to amplitude squared. The values in (2/n)Fn,y = a+ ib 
correspond to amplitudes, so the power a frequency en be gauged by (or 
defined to be) the value of | 
[(2/n)Fnyls| = | 
[a+ ib], = as of 62. 
For example, if 9(t) = cos(27 10.25¢) is sampled at 32 points in {0,1) to 
generate y, and if F32y/16 =a+ ib, then He power spectral density estimate 
obtained by plotting the points | 
(Fs2¥/16];| = as + 67 is shown below. 

734 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
| 
(Fs2y/16], |' 
5 
10 
15 
20 
25 
30 
POWER SPECTRUM FOR 4(t) = cos(27 10.25 t) 
While this estimate of the power spectrum does not reveal the exact frequency 
f = 10.25 (or the fact that the signal is a pure cosine), it nevertheless makes 
it clear that the bulk of the power in the signal is concentrated near f = 10. 
(A higher-resolution graph will show that the true frequency is in fact a bit 
to the right of f = 10.) The problem caused by a non-integer frequency can 
be completely alleviated by sampling over an interval whose length provides 
for integer periods. For example, if y(t) = cos(2710.25t) 
is sampled at 128 
points over the interval [0,4) to generate y, then as shown below, the frequency 
domain determined by Fj2sy/64 indicates a cosine with amplitude a =1 and 
frequency f = 41, which translates into 41 cycles in 4 seconds, or equivalently, 
10.25 cycles in 1 second. 
a = Re(Fizsy/64) 
0 
20 
10 
60 
80 
100 
120 
ib = Im(F28y/64) 
FREQUENCY DOMAIN FOR jj(t) = cos(27 10.25) SAMPLED AT 128 POINTS ON [0, 4) 

5.5 Discrete Fourier Transform 
735 
Filtering 
A primary use of the DFT is to filter a signal by detecting and removing noise or 
to band-limit a signal by filtering out low or high frequency components of the 
signal. For example, consider the signal from the noisy E chord that is displayed 
in Figure 5.5.1 on page 716. The harmonic components are masked by the noise 
to the extent that it is difficult to perceive any harmonic oscillations at all. To 
see how the DFT and its inverse can filter out noise and reveal a cleaner signal, 
let y = [y]7 be a vector containing n = 1024 samples! of the noisy E chord 
that are taken at equally spaced points in [0,1), and apply Theorem 5.5.7 by 
computing 
(2/n)Fny = Fiozsy /512 = a+ 
ib. 
(5.5.31) 
Plotting the results yields the following charts. 
a = Re (Fiog4y/512) 
0 
100 
200 
300 
400 
500 
600 
700 
800 
900 
1000 
FIGURE 5.5.9(a): DFT oF NoIsy 
E CHORD—REAL PART 
b = Im (Fio24y/512) 
165 
L 
4 
0 
100 
200 
300 
400 
500 
600 
700 
800 
900 
1000 
ll 
= 
i 
Mate 
n 
=e 
ie 
FIGURE 5.5.9(b): DFT OF NoISy E CHORD—IMAGINARY PART 
Dominant frequencies in the noisy E chord are unmasked by the DFT to 
some extent, but they are nevertheless contaminated by the inherent spectral 
leakage and low-level noise. The power spectrum estimate (shown below in Figure 
2 
5.5.10) that is obtained from the values | 
[Fiozsy/512);|" 
= a? + 6° helps to 
further clarify things. 
The reason for using n = 1024 is because the highest frequency in a pure E chord is slightly 
less than 330 Hz, and Theorem 5.5.7 says that at least twice this many samples per second are 
needed. But in addition to this consideration the decomposition of the Fourier matrix and the 
resulting fast Fourier transform algorithm described on pages 743 and 746 requires n to bea 
power of 2. So, since 1024 
is the smallest power of 2 that exceeds 660, setting n = 1024 
is the smallest number that will satisfy these constraints. 

736 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
1 ) 
100 
=©200 
»=6 300— 
«400 
500 
600 
700 
800 
900 
1000 
7 
T 
T 
T 
—I 
i 
mL 
a 
ee | 
: 
| 
[Fio2ay/512); 
FIGURE 5.5.10: POWER SPECTRUM OF NOISY E CHORD 
An absolutely pure E chord played on a guitar (with standard tuning) is 
theoretically given by 
fi = 82.407 
(E—sixth string, open), 
6 
fa =123.471 
(B—fifth string, second fret), 
SD 
aaa 
eewnicteenl 
I dig pee Bains eae fe) 
k=1 
fs = 246.942 
(B—second string, open), 
fe = 329.628 
(E—first string, open), 
so the frequencies revealed in Figure 5.5.10 are as good as can be reasonably 
expected by taking 1024 samples in [0,1). In addition to revealing dominant 
frequencies, Figure 5.5.10 helps to see the leakage and to somewhat distinguish 
it from frequencies contributing to low-level noise. 
To filter out frequencies that contribute to low-level noise, select a drop 
tolerance 7, and replace all components in both the real and imaginary parts 
of (2/n)F,y = a+ ib whose magnitude falls below 7 with zeros to produce 
@ + ib. Applying the inverse DFT to a+ ib provides a filtered or "cleaned" 
signal in time coordinates. 
For example, looking at small amplitudes in Figures 5.5.9 and 5.5.10 shows 
that low-level noise components never exceed 
tT = 0.2 in magnitude, so use this 
as the drop tolerance to clean up the noisy E chord. In other words all respective 
components a; and §; in a and b whose magnitude is less than 7 are set 
to zero to produce a "clean" DFT whose components are a+ ib. Applying the 
inverse DFT to a+ ib, produces the time coordinates Y for the filtered or 
"cleaned" version of the noisy E chord. In particular, it follows from Theorem 
5.0.7 on page 730 that 
» 
7 ny =atib ie 5Fni(a + ib), 
so the filtered or "cleaned" time coordinates of the noisy E chord are obtained 
from the inverse DFT by using (5.5.27) or (5.5.28) on pages 728-729 to compute 
Vib 
2x Baa (acai 

5.5 Discrete Fourier Transform 
Tah 
The resulting filtered signal is shown below in Figure 5.5.11. 
y (AMPLITUDE) 
0.1 
0.2 
0.3 
0.4 
0.5 
t (TIME) — 
0.6 
0.7 
0.8 
0.9 
1 
FIGURE 5.5.11: SOUND SIGNAL FOR THE FILTERED E CHORD 
Now go back and compare the original signal in Figure 5.5.1 on page 716 with 
the filtered signal in Figure 5.5.11 to see just how dramatic the cleaning effect 
of the DFT is. Analogously, noise components can be isolated and examined by 
looking at the power spectrum to select drop tolerances around the dominant 
frequencies to "zero them out" along with significant leakage. 
Convolution 
5.5.8. Definition. The convolution of two functions f(x) and g(x) is 
defined to be the function (f * 
g)(t) that is produced by the integral 
CO 
(fea()= {Flog =a) de. 
oo 
In other words, f *g is the integral of the product of f and g after g 
is reversed and shifted. This is illustrated in the next example. Exercise 
5.5.18 establishes that convolution is commutative—..e., 
CO 
(fen =f ie ae [slay fe 2ae = (9 
0. 
—-0O 
Example 
To appreciate the reversing, shifting, and sliding aspects of a convolution, con- 
sider the following simple example in which f(x) and g(x) are as shown below. 
y 
8 Noes | 
es) >~ rey = 
=s Be 
R — 
sh 
3 
SS | = = 
8 
. 
. 
eta 
ss 
x 
, 
. BO 
es 
ten 
esas 
-2 
+ 
-1 
0 
1 
2 
3 
t-3 
FiGuRE 5.5.12(a): Reversal of g 
Figure 5.5.12(b): Shifted reversal 

738 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
As depicted in Figure 5.5.12(a), the function r(x) = 9(-a) is the reflection (or 
reversal) of g(x) about the vertical axis. So, as illustrated in Figure 5.5.12(b), 
r(a —t) = g(t—2) isa shifted and reflected (reversed) version of g(x) for each 
value of t. As t moves from —oo to +00, the triangle defined by g(t—a) in 
Figure 5.5.12(b) "slides" from right-to-left. At each t the convolution (f *g)(t) 
is the area under the product f(«)g(t — x), which is nonzero only when f(z) 
and g(t—«) intersect. Since f(z) =1 forO<a@<1 and zero everywhere else, 
(f *g)(t) yields the area in the intersections shown below in Figure 5.5.13. 
y 
y 
f(z) 
1 
r 
x 
x 
0 
1 
g(3 
- 2) 
y 
f(2) 
1 
,é 
1 
i 
o 
I 
i, 
ie 
ee 
ee 
t-3 
t-2 
t-3 
t-2 
g(t-2) 
for 3<t< 
4 
g(t- 
ax) for t>4 
FIGURE 5.5.13: SLIDING THE REVERSAL OF g AGAINST f 
It follows from straightforward integration (or by geometric considerations) that 
a 
—4 
eae for 2<t<3, 
* 
g)(t) = 
fA) 
(f *9)(t) 
( a 
for 3<t<4, 
0 
elsewhere. 
The convolution is maximized at t = 3, in which case (f *g)(3) = 1/2. In other 
words, t = 3 is the point at which g(t— 2) most closely matches f(z). 
Discrete Convolution 
The discrete version of convolution is obtained by replacing functions with se- 
quences (or vectors), replacing t with a discrete variable k, and using summa- 
tion in place of integration. The precise definition is as follows. 

ad 7 
; 6 
coo 
ee 
ere 
This definition simplifies when a,b € R" are vectors whose respective en- 
tries are a; and 6; for 7 = 0,1,2,...,.—1 because in this case all other 
entries in (5.5.32) can be taken to be zero so that a *b can be regarded as a 
2n x 1 vector whose k*" entry is 
k 
lan bie e cyceey 
fore (0) 
U2 ana ene, 
(5.5.33) 
j=0 
Note that the last entry in a*b is [a* b](gn-1) = 0. This superficial zero is 
included for the sake of convenience to make the size of the convolution twice 
the size of the original vectors—this provides some balance in later formulas. 
Qo 
Bo 
; 
ar 
Bx 
The convolution of two nx 1 vectors a = 
and b= 
3 
nee 
Good 
can be expressed by using (5.5.33) with the understanding that a; = 8; =0 
for 
j =n to form the 2n x 1 vector 
ao 
Bo 
afi + a1 80 
ao 
B2 + 0181 + a28o 
ax b= 
(5.5.34) 
An—2Bn—1 + An-1Bn—2 
Qn—1Bn-1 
0 
2nx1 
Analogous to reversing g(x) and sliding it across f(x) to produce a contin- 
uous convolution as illustrated in the example on page 737, a "reverse-and-slide" 
procedure also produces the discrete convolution a* b when a,b € R". This is 
illustrated below in Figure 5.5.14, where b is first reversed and then sequentially 
slid across a. Summing the products in the intersection sequentially produces 

740 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
the entries in the vector axb given in (5.5.34)—the last zero is artificially added. 
a 
Bn—1 
ao: 
Bn-1 
as 
0 
Br 
ao xB 
ROMs 
On—2 
X 
Bn-1 
prea 
is 
= 
a0 X Bo 
cs 
a1 x By 
Qn—1 X Bn—2 
On—1 * Bn—1 
Qy 
0 
a2 X Bo 
nm—2 
FIGURE 5.5.14: REVERSE AND SLIDE 
Example (Polynomial Multiplication) 
rE 
1 
Convolution is a natural occurrence, and polynomial multiplication is an exam- 
ple. For two polynomials 
a0 
Bo 
n—1 
n—1 
. 
a1 
Bi 
Dt) ye a,x" and q(x) = sD Bee, Jet fies 
and b= 
k=0 
k=0 
ride 1 
Bee 1 
The product p(x)q(x) = yo + 12 + Yor? +:+++en-20?"-? is a polynomial of 
degree 2n—2 in which 7, is exactly the k*' component of the convolution' a*b 
because 
2n-—2 
1? 
2n—2 
p(x)q(x) = Se So, es a [a * blxz*. 
(5.5.35) 
k=0 | j7=0 
k=0 
In other words, polynomial multiplication and discrete convolution are equivalent 
operations, so any efficient or fast way to perform a convolution is also an efficient 
or fast way to multiply two polynomials, and conversely. 
Convolution and the DFT 
There are two facets involved in efficiently performing a convolution. The first 
is the realization developed below that the discrete Fourier transform has the 
ability to convert a convolution into an ordinary product, and vice versa. The 
second is the realization that it is possible to devise a fast algorithm to compute 
a discrete Fourier transform. 
The realization that the DFT transforms a convolution into a product, and 
vice versa, is the following convolution theorem in which a x b denotes the 
Because of its occurrence in polynomial (and infinite series) multiplication, discrete convolution 
as defined in (5.5.34) was originally called the Cauchy product, a term that can still be found 
in many sources. 

Proof. 
Observe that for each t, the t®" entry in Fy; x Fux is 
[F.j x Fee], = gtigtk — gt(i+k) 
[Patil > 
and hence the columns of F have the property that 
Boe Fi, 
= Eee 
«foreach 7° 7, k= 0). 
ee) (2n — 1). 
This means that if Fa, Fb, and F(ax*b) are respectively expressed as combi- 
nations of columns of F by writing 
m—1 
n—-1 
2n—2 
Fa= So oxFex, 
Fb= 0 6:Fsx, 
and 
F(a*b)= S- [ax bleFu, 
k=0 
k=0 
k=0 
then the computation of (Fa) x (Fb) is exactly the same as forming the product 

742 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
of two polynomials in the sense that 
n—-1 
n—-1 
2n—2 
k 
(Fa) x (Fb) = (s 
oP.s) (S 
at] = >, =, Fux 
k=0 
k=0 
k=0 | j=0 
2n—2 
ee ye [ax b],.Fux = F(a*b) 
by (5.5.35). 
& 
k=0 
The convolution theorem ensures that the convolution of two n x 1 vectors 
can be computed by executing the following three discrete Fourier transforms of 
order 2n 
Aanx1 * Dax1 = F3,[(Fena) x (Fanb)). 
(5.5.37) 
The fact that one of them is an inverse DFT is not a problem—recall the discus- 
sion on pages 728 and 729. However, on the surface it is not apparent that this 
provides an advantage. 
It is a straightforward exercise to verify that executing a convolution by do- 
ing the arithmetic indicated in (5.5.34) requires n? scalar multiplications. Per- 
forming a DFT of order 2n by standard matrix-vector multiplication requires 
An? scalar multiplications, so using matrix-vector multiplication to perform the 
computations on the right-hand side of (5.5.37) requires at least 12 times the 
number of scalar multiplications demanded by the definition of convolution. So, 
if there is an advantage to be gained by using the convolution theorem, then it 
is necessary to be able to perform a DFT in far fewer scalar multiplications than 
that required by standard matrix—vector multiplication. 
This hurdle was overcome in 1965 when the two Americans J. W. Cooley 
and J. W. Tukey (see the footnote on page 748) introduced a fast Fourier trans- 
form (or 
FFT) algorithm that requires only on the order of (n/2)logsn scalar 
multiplications to compute F,,x. Using this FFT together with the convolution 
theorem requires only about 3nlog,n multiplications to perform a convolution 
of two mn x 1 vectors, and when n is large, this is significantly less than the n? 
factor demanded by the definition of convolution. 
The FFT 
is considered to be one of the most significant twentieth century 
advances in numerical computation, and it has since been developed into many 
different flavors and been applied to a countless number of applications. The 
discussion below presents the fundamental principles of the FFT that explain 
why it is indeed so incredibly fast. 
Fast Fourier Transform (or FFT) 
The speed of the FFT emanates from the fact that if n is a power of 2, then as 
the following fundamental decomposition theorem indicates, 
a DFT of order n 
can be executed by performing two transforms of order n fae 

_ Proof. Observe that if n = 2", then (é)" = (e27)"/ * so 
Peet eee 
ee 
st == them rootsof unity 
{3 
if and only if 
gl 
re gk ye ees } 
== thei(n/ 2)" roots of unity. 
This means that the (j,k)-entries in the Fourier matrices F, and F,, /2 are 
[Pale 
eeand mE, ale =e 
(5.5.39) 
If the columns of F,, are permuted so that those with even subscripts are listed 
before those with odd subscripts, and if P2 is the corresponding permutation 
matrix, then F,,P/ can be partitioned as 
A(n/2)x(n/2) 
Byn2)x(n/2) 
a 
Pepe 
yy Banoo 
Fei 
Bai" Foe ( 
) 
C(n/2)x(n/2) 
G(n/2)x(n/2) 
By using (5.5.39) together with the facts that 
2; 
eet 
and 
ee = a = be 
— 1, 
it is seen that the entries in A, B, C, and G are 
A jr = Fy arn = ere = [Bais 
Byx = Fy2x41 = CICS Nees tel = CEs 
E((n/2) +9) 2k = Erk e2ak ane e2ak gus [F 
Ck = F(n/2)+j, 2k = 
n/a) jks 
El) 
ORI) = Erken/2¢9 ¢25k ee — Eg grt — —¢ [F 
Gye = F(n/2)+7, 2k+1 = 
Rilahe 

744 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
a 
€ y2 
In other words, if D,,/2 is the diagonal matrix D,,,. = 
§ " 
— 
, 
€(n/2)-1 
then 
A(n/2)x(n/2) 
Bon/2)x(n/2) 
Frj2 
 Dny2Fnj2 
FiPa = 
— 
. 
C(n/2)x(n/2) 
G(n/2)x(n/2) 
Frj2 
—Dnj2Fn2 
The fact that the decomposition of a Fourier matrix of order n = 2" can 
be expressed in terms of two Fourier matrices of order n/2 = 2"! leads to 
the FFT algorithm. To get a feel for how this happens, consider the case when 
n = 8, and proceed to "divide and conquer." If 
Zo 
v0 
os 
24 
(0) 
x 
uA 
e 
6 
x4 
mea SS 
then 
Pgxg = | — | = | — 
v4 
|? 
21 
(1) 
25 
x3 
X4 
x6 
as 
L7 
27 
then 
F4 
D4F4 
x{°) 
Fx" ar D4F4x\" 
Fgxg = 
= 
: 
(5.5.40) 
Bae eg 
sXe 
Fyx — DyFax 
eae) 
(0) 
Gi 
(2) 
x 
x 
v4 
2 
v5 
2 
But Pyx\" =| 
— | = | — |} and P4x 
== | my 
joc} 
Ss 
es 
v2 
a 
r3 
(3) 
v6 
X95 
x7 
x 
Bx? 
( 
F2 
DoF2 ( 
x ) ( 
Fox)" + DoFox\)) 
4X4 
= 
= 
Fo 
—De2F2 
xi) 
Fox) " DoFox\) 
and 
(2) 
(2) 
3) 
(5.5.41) 
(1) 
te ra) X5 
foe + DoF 2x 
F'4x, 
7 
— 
" 
F2 
—De2F2 
x2) 
Fox?) Py DoF ox?) 
. 
1 
ee 
1% 
Since Fp = ( 
), it is a trivial matter to compute the terms 
Foxy", Fox, 
Fox), 
Fox). 
Therefore, computing Fgxg is accomplished by working backward through the 
preceding sequence of steps. That is, start with 
vO 
xg = 
ae 
= 
a 
' 
(5.5.42) 

5.5 Discrete Fourier Transform 
745 
and use (5.5.41) followed by (5.5.40) to work downward in the following tree. 
Fx) 
F>x$) 
F x) 
Fox?) 
'\ 
L 
\ 
Va 
Fyx©) 
Fyx\) 
\ 
iL 
\ 
L 
Fxg 
However, there is a wrinkle that must be dealt with. Working downward 
through this tree requires starting with the permutation Xg in (5.5.42). So how 
is this initial permutation determined? Look back to observe that the entries in 
Xg were obtained by first sorting the z;'s into two groups—the entries in the 
even positions were separated from those in the odd positions. Then each group 
was broken into two more groups by again separating the entries in the even 
positions from those in the odd positions. These permutations are shown below. 
(0 {eons eG 
7) 
vo 
(0 2 
4 6) (2 A 
7) 
(5.5.43) 
va 
ZN 
(o 4) (2 6) (2 5) (3 7) 
In general, these even—odd sorts (sometimes called perfect shuffles) produce the 
permutation necessary to initiate the algorithm. A clever way to obtain the final 
permutation is to use binary representations and observe that the first level of 
sorting in (5.5.43) is determined according to whether the least significant bit is 
0 or 1, the second level of sorting is determined by the second least significant 
bit, and so on—this is illustrated in the following table for n = 8. 
NATURAL ORDER 
FIRST LEVEL 
SECOND LEVEL 
0 + 000 
0 + 000 
0 + 000 
1< 001 
2< 010 
4< 100 
2< 010 
4 «+ 100 
2< 010 
34 011 
64 110 
6 <4 110 
4 <4 100 
1 
001 
14 001 
54 
101 
34 011 
54+ 101 
6+ 110 
5 
101 
34 011 
74111 
Telli 
To lll 
However, all intermediate levels in this sorting process can be eliminated because 
something nice happens. The binary bits in the final ordering turn out to be 

746 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
exactly the reversal of the binary bits in the natural ordering. For example: 
NATURAL ORDER 
BIT REVERSAL 
0 + 000 
000 
+ 0 
1+ 001 
100 
4+ 4 
2 «+ 010 
010 
4 2 
34 011 
1104+ 6 
(5.5.44) 
4 + 100 
0014 1 
5 «+ 101 
101465 
64 110 
01163 
74111 
1ll3o7 
In general, the fast Fourier transform (or FFT) can be implemented by 
executing the following array manipulations! 
Fast Fourier Transform (or FFT). For a given input vector x containing 
n = 2" components, the DFT F,,x is the result of successively creating 
the following arrays. 
ee <— rev(x) 
(bit reverse the subscripts) 
Por p= 
OU too oe ced 
eo mi/2? 
D 
ee 
(Half of the (27+!) roots of 1, 
ue 
. 
; 
perhaps from a lookup table) 
e- (2) -1)mi/27 Oe 
x0) oar (Xo Neg 
aa 
Xezr-i2 J 
: 
ae 
QIK 2T 
SS 
ROG (Xa Xe Ke 
Ke 
2I.X 2" 
F-1 
XO) 4 [D x XM] 
Be x to mean ) 
X <— 
Qi+1 
x 
Qr—j-1 
There are a variety of different ways to implement the FFT, and choosing a practical imple- 
mentation frequently depends on the hardware being used as well as the application under 
consideration. Because the FFT ranks high on the list of useful algorithms (it is consistently 
listed in the top 10 algorithms that had the greatest influence on science and engineering in 
the 20th century), many more facets of the FFT have been investigated and developed than 
those presented here (e.g., FFT when n is not a power of 2). In fact, there are entire texts 
devoted to FFT issues, so the interested reader need only go as far as the nearest library or 
search engine to learn more. 

5.5 Discrete Fourier Transform 
TAT 
Example 
ro 
To execute the FFT on x = 
a , (i.e., to compute F4x,) initiate the process 
x3 
by setting X <— rev(x) = (xo, 22,21,23), and then proceed as follows. 
For 402 
D +— (1) 
(Half of the square roots of 1) 
KO C= (ap 21) 
XM ¢ ~~ (a. 
23) 
and 
Dx xX) 
(xq 
23) 
é. ene Coat *, (eee oer 
x0) — [D x x] 
CE Game i 
GNI md be} 
For j=1: 
D— ( 4 
(Half of the 4% roots of 1) 
to — £2 
KOR 
(fw e 
eusaend 
OD exes | ele 
Lie 
3 
—it;, +173 
fo +22 — 21 — #3 
20 — 
2 +12, —ixg 
5 
Lor ea+ 1+ x 
we 
XO 
+ [Dx XM] 
ty ee Broadens + ing 
= Box 
KOE D = xX) 
j 
Notice that this agrees with the result obtained by using direct matrix—vector 
multiplication with F4 given in (5.5.21) on page 727. 
How Fast Is It? 
Just count the number of multiplications that the FFT uses. The 7%" iteration 
requires 2/ multiplications for each column in X"), and there are 27~—! 
columns, so 2'~! multiplications are used for each iteration! Since r iterations 
are required, the total number of multiplications used by the FFT does not 
exceed 2'~!r = (n/2) logan. This is important enough to state as a theorem. 
5.5.12. Theorem. 
If n is a power of 2, then applying the FFT to a 
vector of n components requires at most (n/2)log,n multiplications. 
J Actually, a slightly smaller multiplication count can be obtained by taking advantage of the 
fact that the first entry in D is always 1 and by observing that no multiplications are needed 
when j = 0. But when n is large these savings are relatively insignificant, so they are ignored 
in the multiplication count. 

748 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
The (n/2)log)n count represents a tremendous advantage over the n? factor 
demanded by direct matrix-vector multiplication. To appreciate the magnitude 
of the difference between n? and (n/2) logy n, look at Figure 5.5.15 below. 
300000 
f(n) = (n/2) logon 
0 
128 
256 
Bills 
eS 
FIGURE 5.5.15: n? veRsuS (n/2)loggn 
The small red portion at the bottom of the graph in Figure 5.5.15 is the area 
under the curve 
f(n) = (n/2)log,n—this area is minuscule relative to the 
area under f(n)=n?. For example, if n = 512, then n? = 262,144, 
but 
(n/2) log, n = 2304. In other words, for n = 512, the FFT is on the order 
of 100 times faster than straightforward matrix—vector multiplication, and, as 
Figure 5.5.15 illustrates, the gap grows dramatically wider as n increases. 
Since Cooley and Tukey ' introduced the FFT in 1965 it has risen to a 
position of fundamental importance and is now a principal ingredient in the 
technology that touches our lives countless times each day. The FFT is generally 
regarded as being among the top-ten most important computational develop- 
ments of the twentieth century. 
The FFT made its debut in the five page 1965 paper An Algorithm for the Machine Calculation 
of Complex Fourier Coefficients by James W. Cooley at IBM and John W. Tukey of Princeton 
University. As the story is told, a physicist named Richard Garvin was involved in questions 
concerning weapons and defense for the US government when he learned of Tukey's work 
on computing Fourier transforms. Garvin took Tukey's ideas to the IBM research center at 
Yorktown Heights, NY, to have them adapted and programmed for digital computation, and 
John Cooley was assigned to the task. Word of Cooley's program spread, and demand for it 
became so great that Cooley and Tukey decided to publish their now-famous paper. As is so 
often the case in science and mathematics, the germ of the idea had apparently occurred to 
others in earlier times—and some say even to Gauss himself. 

5.5 Discrete Fourier Transform 
749 
Example (Fast Integer Multiplication) 
Consider two positive integers whose base-b representations are 
C= (9n-1%n-2°''11%0)o 
and 
d= (bn-1dn—2-++4100)p. 
The convolution theorem together with the FFT affords a fast way to compute 
the product cd. To see how, let 
n—-1 
el 
si 
e 
DCE) = ys ype, 
Q(z) = SS Opt®, 
c= 
: 
and 
ods 
i 
, 
k=0 
k=0 
: 
; 
Yn-—1 
On—=1 
so that 
C= pO 
yl b 2 Ps 
yd + 408° = p(o), 
and 
C= 5,309 Fe bnoD" 
OD 
Ob = Gib). 
It follows from (5.5.35) on page 740 that the product cd is given by 
cd = p(b)q(b) = [c * dlon_2b?"? 4 [e * d]an—3b°"-3 + ---+ [ce * d]1b! + [c * d]ob®. 
It looks as though the convolution c *d provides the base-b representation for 
cd, but this is not quite the case because it is possible to have some [c*d], > b. 
For example, if c = 20119 and d = 42510, then 
ed = (8x10*) + (4x10°) + (14x10) + (2101) + (5x10°), 
(5.5.45) 
but 
5 
1 
5 
2 
Cede | 900] 
es} 
20] 
a= 
2 
2 
4 
8 
0 
However, when numbers like 14 (i.e., greater than or equal to the base) appear in 
cxd, it is relatively easy to decompose them by writing 14 = (1x10!) + (4x10°), 
so 
14x10? = [(1x101) + (4x10°)] x10? = (1x 10°) + (4x107). 
Substituting this in (5.5.45) and combining coefficients of like powers produces 
the base-10 representation of the product 
cd = (8x 10+) + (5x 10%) + (4x10?) + (2x10') + (5x10°) = 8542510. 
Computing c*d directly demands n? multiplications, but using the FFT in 
conjunction with the convolution theorem requires only about 3nlogyn multi- 
plications, which, for large values of n, is considerably fewer than the n? mul- 
tiplications needed for direct computation. Thus it is possible to multiply very 
long base-b integers faster than by using direct methods. Most digital comput- 
ers have binary integer multiplication (usually 64-bit multiplication not requiring 
the FFT) built into their hardware, but for ultra-high-precision multiplication 
or for more general base-b multiplication, the FFT can be a viable tool. 

750 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
Exercises for section 5.5 
5.5.1. 
5.5.2. 
5.5.3. 
5.5.4. 
5.5.5. 
Explicitly write out Fg, the Fourier matrix of order eight. 
zo 
cy 
Apply the FFT algorithm to the vector xg = 
, and verify that 
x7 
your answer agrees with the result obtained by computing Fgxg directly. 
If eight samples of the signal y(t) shown in Figure 5.5.6 are taken at 
t 
=0,1/8,2/8,...,7/8, the result is y = 
, where r = 1/72. 
—2r 
Use these samples along with Fg (found in Exercise 5.5.1) to reveal the 
harmonic components in y(t), and confirm that they agree with those 
found in (5.5.30). 
A waveform given by a finite sum 
pls ee +e (ax cos 27 f,7 + By sin 27 f_T) 
k 
in which the f,'s are integers and max{f,} < 3 is sampled at eight 
equally spaced points between 
tr = 0 and t=1. Let 
(0/8) 
0 
(1/8) 
—5i 
a 
eet 
H 
0 
aa 
bee rir il 
and suppose that 
y= qFsx = 
= 
(5/8) 
4 
(6/8) 
143i 
a(7/8) 
Bi 
What is the equation of the waveform—i.e., what is ax, 
Bx, and fp? 
Suppose that the function y(t) = sin87t is sampled at eight equally 
spaced points t = 0,1/8,...,7/8 in [0,1). What is the result of using 
F's together with Theorem 5.5.7 on page 730 to reveal the amplitude 
and frequency of y(t)? Is this the correct amplitude and frequency? If 
not, explain what went wrong. 

5.5 Discrete Fourier Transform 
751 
5.5.6. Consider a waveform that is given by a finite sum 
25) = Di (az, cos 2a fT + By sin 27 f;,7) 
k 
in which the f;,'s are distinct integers, and let 
ies Se (a, cos 2m fet + Bx sin 27 fxt) 
k 
be the vector containing samples of z(r) at n > 2max{f,} equally 
spaced points between tT =0 and +t =1. Use the DFT to prove that 
Ixlg = 5 >. (0% +62). 
k 
5.5.7. 
(a) Evaluate the discrete Fourier transform of 
| 
ede 
(b) Evaluate the inverse DFT of 
oF 
=) 
Fo 
D2F2 
5.5.8. Verify directly that F4 = ( 
Ps where the Fy, P4, and 
Fo 
—D2F2 
Dz» are as defined in (5.5.38) on page 743. 
5.5.9. Evaluate the following convolutions. 
1 
4 
—1 
1 
1 
Qo 
(a) (2)+(5). 
0) ( 0) 
x ( 
0). 
(@ (1) «(#). 
3 
6 
it 
—1 
1 
a2 
5.5.10. Use the following vectors to perform the indicated computations: 
ai) ear ane a 
be 
A 
ay 
a 
6 
0 
(a) Compute a*b, F4(a*b), and (F4a) x (F,b). 
(b) By using F;' as given in (5.5.26) on page 728, compute 
F71[ (Psa) x (Fab)]. 
Compare this with the results guaranteed by the convolution 
theorem. 

752 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
5.5.11. 
5.5.12. 
5.5.13. 
5.5.14. 
5.5.15. 
5.5.16. 
For p(z) = 2a —3 and q(x) = 3x —4, compute the product p(x)q(x) 
by using the convolution theorem. 
Use convolutions to form the following products. 
(a) 
4310 x 2110. 
(b) 
1238 x 601s. 
(c) 
10102 x 11019. 
Let a and b be nx 1 vectors, where n is a power of 2. 
(a) Show that the number of multiplications required to form a*b 
by using the definition of convolution is n?. 
Hint: 1+2+---+k=k(k+1)/2. 
(b) Show that the number of multiplications required to form a*b 
by using the 
FFT in conjunction with the convolution theorem 
is 3nlog.n+7n. Sketch a graph of 3nlogyn (the 7n factor is 
dropped because it is not significant), and compare it with the 
graph of n? to illustrate why the FFT in conjunction with the 
convolution theorem provides a big advantage. 
1 
ao 
ay: 
q 
oe 
Prove that c7(a*a)=(c7a) 
for 
c= 
and a= 
5 
nome 
Ont 
For p(x) = Siar a,v* and the n*" roots of unity €,, let 
A=(A9 01 02 **An-1)' 
and p=(p(1) p(é) p(€2) ---p(er-1))*. 
Explain why F,a =p and 
a= F>'!p. This says that the DFT allows 
one to go from the representation of a polynomial p(x) in terms of its 
coefficients a, to the representation of p(x) in terms of its values p(é*), 
and the inverse transform takes us in the other direction. 
For p(x) 
=ag 
+ayx + aga? + +--+ Qn—12"—1, prove that 
wees 
2 
~ >. [P(E |? 
= lao)? + laa? +--+ Jon—a?, 
k=0 
where {1, &, 7, ..., €"-1} are the n"" roots of unity. 

5.5 Discrete Fourier Transform 
(Gos) 
5.5.17. 
5.5.18. 
5.5.19. 
5.5.20. 
For two polynomials p(x) = S779 aga* and q(x) = rt Bea®, let 
p(1) 
q(1) 
p(§) 
q(&) 
p= 
: 
and 
q= 
; 
p(e2n-) 
g(2"-1) 
where {1, €, €7,...,€?"-1} are now the 2n*" roots of unity. Explain 
why the coefficients in the product 
p(x)q(z) = 
+912 + you ee 
Yon a0 
'Yo 
p(1)q(1) 
: 
v1 
q(&) 
must be given by | y2 | = F;, p(€*)q(€?) 
|. This says that the prod- 
uct p(x)g(x) is completely determined by the values of p(x) and q(x) 
at the 2n*" roots of unity. 
Prove that convolution is commutative in the sense that 
(oe) 
CO 
(Fea)= f fe)gt-a)de= [ g(x) st 2) dr = (9x Nl. 
—= OO) 
=O) 
The example in (5.5.34) suggests that axb = b*a for a,b € R". Give 
a formal proof of this fact. Hint: Consider the convolution theorem. 
Prove that ((f *g) *h)(t) = (f *(g*h))(t) when the indicated convo- 
lutions exist—i.e., prove that convolution is associative. 
Strictly speaking, the convolution of two vectors in R"" produces a vec- 
tor in R?", so associativity (ax b)*c = a*(b*c) does not make sense 
unless modifications to the definition are made by padding vectors with 
an appropriate number of zeros to make things have compatible sizes. For 
x € R", let 
XE R"" and % € R*" be the padded vectors that respec- 
tively double and quadruple the size of x. Prove that if a,b,c € R", 
then discrete convolution is associative in the sense that 
(ax b)*c=ax(bxc). 
Hint: Consider the convolution theorem. 

754 
5.5.21. 
5.5.22. 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
Prove that the following properties hold for the convolution of two func- 
tions f(x) and g(z). 
: 
(a) fx(gt+h)=f*g+f*h (distributive property) 
(b) a(f*g) =(af)*g = f*(ag) for a € R (scalar multiplication) 
(c) 
(f*g)' = f*(9') =(f')*9, where x' = dx /dt (differentiation) 
Circulant Matrices. A circulant matrix is a square matrix that has the 
form 
co 
det 
eng 
C1 
C1 
co 
Cn—-1 
c2 
C 
c2 
C1 
co 
C3 
Cn—-1 
Cn-2 
Cn-3 
Co' 
nxn 
In other words, the entries in each column are the same as the previous 
column, but they are shifted one position downward and wrapped around 
at the top—the (j,k)-entry can be described as cjx = C(j_%) (mod n): 
(Some authors use C? rather than C as the definition—it doesn't 
matter.) 
(a) If Q is the circulant matrix defined by 
OF Oc ae Ome 
L 
dig 
Dawhscxctc 
all O 
yal, 
ual 
tie rs 
i 
lis ef ee 
and if p(x) =co+cir2+++++en_12"7!, verify that 
C = p(Q) = col +e1Q +--+ + en-1Q™?. 
Explain why the Fourier matrix of order n diagonalizes Q in 
the sense that 
is 1g 
0) 
FQF !'=D= 
sith 
: 
0 0 
gn-l 
where the €*'s are the n*" roots of unity. 
Prove that the Fourier matrix of order n diagonalizes every 
nxn circulant in the sense that 
KOA 
SF 
go 
0 
ete 
p+ 
0 
0 
0 
p(é"-1) 
where p(x) =co+c12 +++: +e,_-12"7!. 
If C, and C2 are any pair of 
nxn circulants, explain why 
C, C2 = CoC)—.e., all circulants commute with each other. 

5.5 Discrete Fourier Transform 
755 
5.5.23. Verify by direct computation the diagonalization result in part (c) of 
iE 
Key 
Miles} 
Exercise 5.5.22 for the circulant C = 
; 
4 3 
(0) 
i 
{ay 
at 
5.5.24. For a nonsingular circulant C,,,, explain how to use the FFT algo- 
rithm to efficiently perform the following operations. 
(a) Solve a system Cx = b. 
(b) Compute C7!. 
(c) Multiply two circulants C,Co. 
5.5.25. For the vectors 
ao 
Bo 
ao 
Bo 
a1 
Bi 
¢ 
x, 
- 
; 
eS 
Kd 
c 
Aa: We 
> 
|, and b= 
ae 
; 
An—1 
Bn-1 
0 
/onx1 
0 
/oanx1 
let C be the 2n x 2n circulant matrix (see Exercise 5.5.22) whose first 
column is a. 
(a) Show that the discrete convolution can be described as a matrix— 
vector product by demonstrating that 
axb=—Cb. 
(b) Use this relationship to give an alternate proof of the convolu- 
tion theorem. Hint: Use the diagonalization result of Exercise 
5.5.22 together with the result of Exercise 5.5.15. 
5.5.26. Bit Reversing Permutation Matrix. Let n = 2", and let P,, be the even— 
odd permutation matrix described in Theorem 5.5.11 on page 743, and 
let @ denote the Kronecker product as defined on page 227. Explain 
why 
a (Lo¢r-1) 54) P2) (Igcr—2) & P4) se (I2 & Po¢r—1) )(Ig0 ® P2,) 
is the permutation matrix associated with the bit reversing permutation 
described in (5.5.43) and (5.5.44) on page 745. 
ro 
Lo 
v1 
@4 
Hint: Work it out for n = 8 by showing that Rg| 
*' | = | *? | to 
7 
ee 
see why it holds in general. 

756 
Chapter 5 
Inner Product Spaces and Fourier Expansions 
In/2 
Dn;2 
5.5.27. FFT in Factored Form. Let B, = ( 
a 
) 
with 
= 2". First 
n/2 
—~44n/2 
verify that the decomposition-of the Fourier matrix given in Theorem 
5.5.11 on page 743 can be expressed in terms of a Kronecker product by 
writing F, = By(Iz @F,,/.)Pn. Next, expand on this idea by proving 
that F,, can be factored as F, = L,R, in which 
Ly = (Ip0 ® Bor) (Ie (03) Bor-1) is (Iocr—2) ® By) (Igo-» ® Bo), 
and R,, is the bit reversing permutation matrix in Exercise 5.5.26. 
Hint: Define two sequences by the rules 
Lox = (Ipr—x ®&) Box ) Lok-1 
and 
Rox = Ror-1 (Ior-% ® Po), 
where L; = 1, R, =I,, 
Bo = Fo, Po = lo, and use properties 
(2.7.1) and (2.7.4) from Theorem 2.7.2 on page 228 and induction on k 
to prove that Ipr-% @ For = LoxRo for k =1,2,...,r. 
Note: Since F, = L,R,, the DFT F,,x = L,R,x is obtained by first 
performing the bit reversing permutation to x followed by r applica- 
tions of the terms (Ip--. ® Box) from L,. This is the FFT algorithm 
in factored form. 
If you think your simple mathematical model explains everything, 
you don't understand how complicated life is. 
— John von Neumann (1903-1957) 
Technical skill is mastery of complexity, while 
creativity is mastery of simplicity. 
— Erik Christopher Zeeman (1925-2016) 

CHAPTER 
6 
Eigenvalue Continuity 
and 
Computations 
6.1 
EIGENVALUE CONTINUITY 
An indispensable fact in both the theory and practice of linear algebra is that 
the eigenvalues of a matrix A € F"*" vary continuously with the entries of 
A. Because this is fundamentally important, two different approaches are pre- 
sented in this text. An elementary approach relying on Schur's triangularization 
theorem (page 306) is given below, but the issue is later revisited in Theorem 
8.2.5 on page 915 from a deeper perspective involving resolvent integrals, where 
continuity as well as differentiability of eigenvalues and other eigencomponents 
are discussed! Articulating and understanding eigenvalue continuity requires the 
rather specialized formulation that is given in the following theorem. 
es 
6.1.1. Theorem. (Eigenvalue Continuity) Consider A € 
with any 
matrix norm. For each 4 € 0 (A) let ay = alg mult, (A), and let € > 0 
be a number such that the open e-disk D, = {z||z—A| <} around A 
contains no other eigenvalue of A (see Figure 6.1.1 on page 759). For 
each such ¢€ there exists a 6 > 0 such that 
|E||<6d 
=> A+E 
has exactly a) eigenvalues 
(6.1.1) 
(counting multiplicities) in each D,. 
There are several ways to approach eigenvalue continuity. When authors do not want to com- 
pletely ignore the topic, they often dodge the issue by passing the details off to the argument 
principle in complex analysis, a consequence of which is Rouché's theorem, which yields the 
conclusion that the roots of a polynomial equation are continuous functions of the coefficients. 
Eigenvalue continuity is then a corollary because the coefficients of every characteristic equation 
are continuous functions of the entries of the underlying matrix—they are just sums of deter- 
minants (see page 294). While being correct, this argument leaves an empty feeling because it 
provides no insight whatsoever from the point of view of linear algebra. 

758 
Chapter 6 
Eigenvalue Continuity and Computations 
Proof. 
Assume that (6.1.1) does not hold so that there is some A € a (A) and 
some e€-disk Dy such that for every 5 > 0 there is a matrix Es with ||E5|| <6 
but A+ Es does not have exactly a, eigenvalues in D,. Taking 6 = 1/k 
for k = 1,2,3,... yields a sequence of matrices E, such that 
||Ex|| < 1/k 
but A, = A+ E, fails to have exactly a, eigenvalues in D). By Schur's 
triangularization theorem (page 306), there exists a unitary matrix U; and 
an upper-triangular matrix T, such that UjA,U, = Tx for each k. The 
eigenvalues of A, are the diagonal entries of T;. It is known from elementary 
analysis that every bounded sequence contains a convergent subsequence. The 
sequence of unitary matrices U;, is bounded (||U,||, = 1), so there is a conver- 
gent subsequence U;, + U, which is necessarily unitary (the limit of a unitary 
sequence is unitary). Furthermore, A, — A (because Ex, — 0), so Ax, > A, 
and 
Ty, = UL An, Uns UAV ST, 
(6.1.2) 
which is necessarily upper triangular (the limit of an upper-triangular sequence 
is upper triangular). Thus the diagonal entries of T are the eigenvalues of A. 
It follows that the subsequence of matrices E,, is such that ||Ex,|| 
<< 1/k;, and 
for each € > 0, 
there is a \ € a (A) such that for every k;, the 
(6.1.3 
matrix Ax, fails to have exactly a, eigenvalues in D). 
me 
The diagonal entries {Aj (ki) } Py of T;, are the eigenvalues of A,,, and the 
diagonal entries {\;}"7_, of T are the eigenvalues of A, so (6.1.2) means that 
lim; +00 Aj(ki) = Aj for each 7. This in turn implies that for each € > 0 there 
exists 
a K; > 0 such that k; > Kj ensures 
|A,;(k;) —A;| < ¢. If K = v;K;, 
then 
kj > K. = > |Aj(hi) —Ajz| << € 
for every 3. 
This forces each \;(k;) to eventually be inside the e-disk D),;, and since A; is 
repeated a, times, there must be exactly a), eigenvalues of Ax, in each De 
for all k; > K. But this contradicts (6.1.3), so the supposition that (6.1.1) does 
not hold is false, and thus the theorem is proven. 
Example (Continuity for a Generic 9 x 9 Matrix) 
The diagram below depicts the continuity statement of Theorem 6.1.1 for a 
generic complex matrix Agx9 having 3 distinct eigenvalues o (A) = {\1, A2, A3} 
with a), =3, 
a), = 2, and a), = 4. (Remember that eigenvalues of complex 
matrices need not be in conjugate pairs.) 

6.1 Eigenvalue Continuity 
759 
FIGURE 6.1.1. EIGENVALUE CONTINUITY 
Around each 4; there is an e-disk D), that surrounds no other eigenvalue of 
A. Theorem 6.1.1 says that for each such € > 0 there is a 6 > 0 such that 
for all E with ||E|| <6 (the choice of norm is irrelevant) there are 3, 2, and 
4 respective eigenvalues of A +E inside of D),, 
D),, and D),. In other 
words, as E - 0, all of the eigenvalues (counting multiplicities) of 
A+E are 
trapped in these disks. Since 
€ > 0 can be arbitrarily close to 0, it follows 
that as E —> 0, the eigenvalues of A +E (along with their multiplicities) can 
be squeezed arbitrarily close to those of A. This means that as 
« > 0 and 
E — 0, the nine eigenvalues of A+ E become the eigenvalues of A with their 
appropriate multiplicities. 
Perturbations of Simple Eigenvalues 
The effect on simple eigenvalues to small perturbations in A € F""" is of inter- 
est in many applications, not the least of which is numerical analysis involving 
eigenvalue computations. The following corollary of Theorem 6.1.1 provides an 
important realization concerning simple eigenvalues that is pivotal when the dif- 
ferentiability of eigenvalues is considered (see the example on page 763). 
6.1.2. Corollary. 
Simple eigenvalues remain simple under sufficiently 
small perturbations. 
Proof. 
If X is a simple eigenvalue for A € F"~", then for each € > 0 such 
that D, is an e-disk around 2 that excludes all other eigenvalues of A, The- 
orem 6.1.1 ensures the existence of a 6 > 0 such that 
A+ E has exactly one 

760 
Chapter 6 
Eigenvalue Continuity and Computations 
eigenvalue jy (counting multiplicities) inside of D, whenever 
|E|| < 6, and 
thus alg multa,p("¥)=1. 
& 
Perturbations of Real and Simple Eigenvalues of Real Matrices 
When a real matrix A € R"*~" having a real and simple eigenvalue is perturbed 
by a real perturbation E € R"*", an even stronger statement than that in 
Corollary 6.1.2 emerges from the fact that the eigenvalues of A+E must appear 
in conjugate pairs. 
6.1.3. Corollary. Real and simple eigenvalues of real matrices remain real 
and simple under sufficiently small real perturbations. 
Proof. 
Let A,E €R"*". If \ €o(A) is real and simple, then for each € > 0 
such that D, is an e-disk around X that excludes all other eigenvalues of A, 
Theorem 6.1.1 ensures the existence of a 6 > 0 such that A+ E has exactly 
one eigenvalue y inside of D, whenever ||E|| < 6. This eigenvalue 
must be 
real because otherwise 7i would be another eigenvalue of A +E that is also 
inside of D, (draw the picture), and this is impossible. 
IH 
Alternate Interpretation of Eigenvalue Continuity 
Another way to think about eigenvalue continuity is in terms of the distance be- 
tween an unordered set A = {Aj, A2,...,An} of eigenvalues for A € F"*" (mul- 
tiplicities included) and an unordered set of eigenvalues V = {w1,w2,...,Un} 
for A+E. In general, a metric on a class of objects H. is defined to be a function 
d:H#xH—R 
such that the following four properties hold for each ¥,V € H. 
(i) 
d(¥,Y) >0 
(ii) d(¥%,V)=0e5x4=y 
(ity 
al vy Y) = dy, 
4) 
(iv) 
d(¥,Z) <d(¥,yY)+d(y, Z) 
Consider two sets 4 = {1,22,...,%n} and Y = {y1, y2,...,Yn} in which order 
is irrelevant. Define the distance d(4',Y) between Y and ) by placing their 
elements into respective vectors x and v, and consider all permutations of the 
entries in one of the vectors—say v. If v, denotes one of the n! permutations 
of v, then for any vector norm 
||x|| the function 
aX; 
¥) = min ||x — v-|| 
(6.1.4) 
satisfies the metric properties (i)—(iv) (this is Exercise 6.1.2). 
For example, if V = {2,1,—1} and Y = {2,5,2}, and if the co-norm is 
2 
used in (6.1.4), then x = 
: , and the 6 permutations of v are 
Dy, 
» 
5 
Sr 
aie coos 
5], 
Mae = "a = (2), Vi3 = Vir 
2 
' 
B, 
5 
2 

6.1 Eigenvalue Continuity 
761 
80 doo(¥,Y) = ming ||x — vr||,, = min{4,6,3} = 3. If A = {15 A2,+++;An} 
and W = {y1,¢2,..., Wn} are the respective eigenvalues of Any, and 
A+E, 
then Theorem 6.1.1 implies that 
d(v,A)+0 
as EO. 
This provides a convenient way to conceptualize eigenvalue continuity. 
Example (Weakness of Eigenvalue Continuity and Eigenvalue Sensitivity) 
While the eigenvalues of A € F"*" vary continuously with the entries in A, 
the continuity can be weak in the sense that 
||E|| can be "reasonably" small 
but the eigenvalues UV = {w1,¢2,...,Wn} of A+E can nevertheless be quite 
far from the eigenvalues A = {Aj,A2,...,An} of A. Sometimes ||E|| has to be 
incredibly small to ensure that d(W,A) is just reasonably small. For example, 
consider 
Ann = 
Laat 
ithe 
Dea 
ya 
, 
and 
A+E= 
ahs 
01 
0 
01 
0 
5 
0 
6 
0 
The eigenvalues of A are all zero, so A = {0,0,...,0}. It can be shown that 
the eigenvalues of 
A+ E are 
Y= (oe Series, SH ope kN 
where 
w= e27i/n 
(see Exercise 4.10.13 on page 613). The oo—distance between WV and A as de- 
fined on page 760 is doo(W, 
A) = 64/". While d.o(W, A) > 0 as |/El|,, =6 > 0, 
|E||,,.. has to be extremely tiny to ensure that d.o(W,A) <e 
for even a reason- 
ably small 
¢. For example, when n = 20, one must have 
||E||,, = 10~*° to 
guarantee that do.(W,A) < 107". 
Moral: One lesson here is that eigenvalue continuity does not preclude eigenvalue 
sensitivity. In other words, in spite of the eigenvalues of A depending continu- 
ously on the entries in A, the eigenvalues can be extremely sensitive to small 
perturbations in the sense that a small relative change in A (i.e., ||E|| / ||A|| is 
small) can produce a large relative change in the eigenvalues in the sense that 
d(W, A) 
||| 
can be immense. In the above example the relative size of the perturbation is 
WE||5 / Alles = 10-*°, but it produces a relative change in the eigenvalues of 
size doo(U, A)/ ||El|,, = 10°8. The sensitivity of the eigenvalues of A to small 
changes in A can be revealed by knowledge of the magnitude of the derivatives 
(if they exist) of the eigenvalues (see the example on page 763 and the discussion 
on page 922). 

762 
Chapter 6 
Eigenvalue Continuity and Computations 
Continuity Relative to a Paramater 
Higenvalue continuity as articulated in Theorem 6.1.1 was based on the assump- 
tion that the eigenvalues of A € F"*" are functions of n? independent variables 
(i.e., the entries of A). But in many applications the entries in A are dependent 
on a single parameter t. Examples are matrices such as 
et 
1 
t 
cost 
cos?t 
A(t) = ( 1») or 
..A(t).= ( 
cost 
cos? t cost). 
cos?t 
cos?t 
cos*t 
In such cases the eigenvalues A(t) of A(t) are functions of the single variable 
t, so the continuity question is a bit different from the prior case. The issue now 
is, for a given (fixed) value to, what does it mean to say that the eigenvalues of 
A(t) are continuous at to? While the question is slightly different, the approach 
and the answer are essentially the same as in the prior case provided that each 
entry a;;(t) is continuous at to. It suffices to analyze the situation for to = 0, 
so the assumption is that A(t) > A(0) (entrywise) as t > 0. 
6.1.4. Theorem. (Continuity With Respect to a Parameter) Suppose that 
each entry a,;(t) in A(t) ¢ F"*" is continuous at t = 0 so that 
lim A(t) = A(0) =A 
(i.e., ai; (t) ea ai; (0) = aj; as o> 0). 
Let a, = alg mult, (A), and let € > 0 be a number such that the 
e-disk D, = {z||z—A| <} around each \ € o(A) contains no other 
eigenvalue of A. For each such € there exists a 56 > 0 such that 
it} 
<d = 
A(t) 
has exactly a) eigenvalues (counting 
ae 
multiplicities) in each Dy. 
oe 
Consequently, if A(t) € o(A(t)), then lim; 
+0 A(t) = A(0). 
Proof. 
The proof is a straightforward adaptation of the proof of Theorem 6.1.1 
on page 757. Details are called for in Exercise 6.1.5. 
Example (Lack of Continuity in Eigenvectors) 
While the eigenvalues of A ¢ F"*" must always vary continuously with the 
entries in A, components of an eigenvector for A need not depend continuously 
on the entries of A. For example, consider the matrix 
Git] 
d 
: 
A(t) = () ') 
, which has eigenvalues Aj(t) =0 and Ao(t) =t. 

6.1 Eigenvalue Continuity 
763 
The vector x(t) = aS) is an eigenvector associated with A2(t) = t. While 
A2(t) is continuous at t = 0, as Theorem 6.1.4 says it must be, the first com- 
ponent of its eigenvector x(t) fails to be continuous at t = 0. In spite of the 
fact that individual components of a single eigenvector can fail to be continu- 
ous, there is a sense of continuity when eigenspaces are consider in total. This is 
articulated in Theorem 8.2.4 on page 914. 
Example (Lack of Differentiability in Eigenvalues) 
Although they are continuous functions of the entries ai; of A, the eigenvalues 
are not necessarily differentiable functions of the a;;'s. For example, consider 
the 2 x2 case in which the eigenvalues \; and A 2 are each functions of the 
Qi1 
12 
do1 
a9, 
)* Lhe characteristic equation 
four independent variables a;; in A = ( 
for A is \2—7T\+6=0, where 
T = trace(A)=a11 +422 
and 
6 =det(A) = a11492 — a12021, 
so the two eigenvalues are 
T+ 
Vr? —46 
At (a1, 412,421, a22) = ren 
See 
mt 
(6.1.6) 
7 —Vre— 45 
A2(@11, 412, 421, 422) = a 
ee 
The partial derivatives 0A;/0a;; and OA2/Oa;; with respect to each entry aj; 
exist and are nicely behaved as long as 7? 4 46, but all of these partial deriva- 
tives fail to exist when 7? — 46 = 0. For example, 
a. 
= 
5 | aod 
0ay1 Po) 
T* — 46 | 
Consequently, the eigenvalues cannot be differentiable whenever 
tT? — 46 = 0. 
Notice that this happens precisely when the eigenvalues have coalesced to become 
a single eigenvalue of multiplicity two. In other words, a general analysis of 
eigenvalue differentiability must necessarily be limited to considering eigenvalues 
that remain simple throughout some neighborhood. This is addressed in §8.3 on 
page 922. 
An example of the non-differentiability of eigenvalues of a matrix dependent 
on a single parameter is provided by 
A(t) = é a 
, which has eigenvalues A;(t) = t!/? and A9(t) = —t!/?. 
As Theorem 6.1.4 guarantees, each eigenvalue is continuous at t = 0, but each 
fails to have a derivative at t = 0. As in the prior example, non-differentiability 
occurs exactly at the point were the two eigenvalues coalesce into a single eigen- 
value with a double multiplicity. 

764 
Chapter 6 
Eigenvalue Continuity and Computations 
Exercises for section 6.1 
Gydeks 
6.1.2. 
6.1.3. 
6.1.4. 
6.1.5. 
For A € F"®%" with o (A) = {Xi,A2,---,As}, let M = {A1,A2,--- Az} 
and N = {Xp4i,---,As}, with k < s. Suppose that Sayed aj =™, 
where a; = alg mult, (Aj). Prove that if I is a closed contour (see 
page 604) not passing through any Aj; € o(A) such that [ surrounds 
M but excludes NV, then there exists a 6 > 0 such that the total 
multiplicity of all eigenvalues of A +E lying inside of [is equal to m 
whenever ||E]| < 6. 
Place the elements of X = {21,22,...,2n} and VY = {y1,y2,---,Ynt 
into respective vectors x,v € F", where order is irrelevant, and let vz 
denote a permutation of v. Show that for any vector norm 
||x|| the 
distance between XY and J defined by 
d(*, 
Y) = min ||x — v;|| 
satisfies the metric properties (7)—(iv) given on page 760. 
Let Y and y be the respective sets of eigenvalues for 
(ap a! 
OL 
OFZ 
0 
2 
A= 
-aeete 
and 
A+E= 
ge 
hee 
: 
0 
20 
0 
20 
0/ 
21x21 
6 
0 
where E = den ;e/ (e; is 918t unit vector—not the (2,1)-entry of 
E). Use the metric (6.1.4) with the oo-norm to determine how small 6 
must be in order to guarantee that d..(¥,Y) < 107 when ||E||,, < 6. 
0 0 
t3(t + 1) 
Let A(t)={1 
0 -t8—<¢(t+1)? 
}, and let 
A = A(O). 
Vea 
(t+1)? 
(a) Verify that Theorem 6.1.4 holds for A(t). That is, for each 
€ > 0 such that the e-disk D, around each \ € 0 (A) contains 
no other eigenvalue of A, find a corresponding value of 6 > 0 
such that 
|t| < 6 implies A(t) has exactly a) eigenvalues 
(counting multiplicities) in each D,. Hint. One eigenvalue of 
A(t) is Ay(t) =t. 
(b) If A(t) is the set of eigenvalues for A(t), find the oo—distance 
as described on page 760 between A(t) and A(0) for real values 
of ¢ such that 0<¢<1, and verify that d.(A(t),A(0)) 
+ 0 
as t= 0. 
Provide the details of the proof of Theorem 6.1.4. 

6.2 Eigenvalue Localization 
765 
6.2 EIGENVALUE LOCALIZATION 
The objective is to eventually focus on some classical methods for computing 
eigenvalues and eigenvectors that have not been covered in previous chapters. 
However, computing eigenvalues for large matrices lacking special structure can 
be challenging. As mentioned on page 293, there is no closed formula for eigen- 
values in terms of the entries for matrices of order n > 4 that use only a finite 
number of arithmetic operations and radicals—i.e., there is no higher-degree 
analog of the quadratic formula that can be seiployed: This means that general 
eigenvalue calculations for n > 4 must necessarily rely on iterative or otherwise 
infinite algorithms—even popular symbolic software for doing exact arithmetic 
is not useful. Consequently, substantial effort in the early twentieth century was 
focused on developing localizations, approximations, and bounds for eigenvalues. 
For example the crudest (but simplest) localization technique is the spectral ra- 
dius bound given in Corollary 3.1.9 on page 298 that says for all A € F"™*", 
p(A) < |/Al| for all matrix norms = > A<||Al| for all 
\€o(A). 
(6.2.1) 
This estimate is extremely cheap to compute, especially if the 1-norm or oo-norm 
is used. But one often gets what they pay for, which in this case is one big circle 
whose radius is usually much larger than the spectral radius of A. However, 
localizing eigenvalues inside of circles (or disks as some prefer to say) can never- 
theless be quite productive. 
This stems from the fact that polynomial equations of degree 5 or higher are generally not 
solvable in terms of their coefficients by using a finite number of arithmetic operations and radi- 
cals. The respective Italian and Norwegian mathematicians Paolo Ruffini (1765-1822) and Niels 
Henrik Abel (1802-1829) made progress establishing the non-solvability of general quintic equa- 
tions, but it was a young Frenchman, Evariste Galois (1811-1832), who pro- 
vided a group theoretic framework, now called "Galois Theory," that provided 
the complete understanding of why general polynomial equations of degree five 
and higher cannot be solved in terms of their coefficients using a finite number 
of arithmetic operations and root extractions. Galois, while famous today, was 
a tragic figure during his own life. Affected by his father's suicide, he became 
a volatile young man. He was twice denied admittance to the Ecole Polytech- 
A 
nique, where it is said that during an entrance exam in 1829, he became so 
BE. GALoIs 
angry by what he perceived to be stupid questions that he threw an eraser 
at the examiner. Charged with making a threatening toast about the King, his radical nature 
landed him in prison in 1831, where he attempted suicide by stabbing himself before being 
stopped by other prisoners. Galois's genius went unrecognized during his lifetime, in part due 
to bad luck. He presented his work without making a backup copy to Cauchy, who esas 
to lose it. Galois submitted a paper to Fourier, who took 
it home but promptly died, so that paper also was lost. 
At age 20 Galois was shot and killed in a duel, purport- 
edly over a woman. Legend has it that the night before 
Galois was killed, he stayed up all evening to write out 
all that he knew connecting group theory and polyno- 
mial equations, which he then mailed to a friend. 'These 
results eventually found their way to Joseph Liouville 
(1809-1882) in 1843. He published them in the Journal 
de Mathématiques Pures et Appliquées in 1846. 
May 30, 1832 

766 
Chapter 6 
Eigenvalue Continuity and Computations 
6. 2, 
1. Theorem. feehcugorin's Theorem' ) 
The eigenvalues of A € xn 
are contained in the union G, of n Gerschgorin ae (or disks) cen- 
tered at aj; with radii given by the deleted absolute it" row sum. More 
prey all eigenvalues are pele inside the union of circles" 
We 
—ay|<1;, where rj = s: 
lag dor WS Aye ey tt 
(6.2.2) 
jFt 
e lIfaunion U of k circles does not touch any of the other 
n—k circles, then there are exactly k eigenvalues (counting 
(6.2.3) 
multiplicities) in the circles in U. 
e 
Since g(A?) =a (A), the deleted absolute row sums in (6.2.2) can 
be replaced by deleted absolute column sums, so the eigenvalues of 
A are also contained in the union G, of the circles defined by 
lz 
nr 
73, Sj, where cj = > laj;| for f= 12,2. 347: 
(6.2.4) 
| 
ij 
. 
e 
Combining (6.2.2) and (6.2.4) means that the eigenvalues 
(6.2.5) 
of A are contained in the intersection G, 
Ge. 
Proof. 
Let (A,x) be an eigenpair for A, and assume x has been normalized 
: Semyon A. Gerschgorin (alternately Semen Gerggorin or Gershgorin, 1901-1933) 
published his circle idea for localizing eigenvalues in 1931, but the concept 
appears earlier in work by L. Lévy in 1881, by H. Minkowski (page 38) in 
1900, and by J. Hadamard (pages 55 and 951) in 1903. However, each time 
the idea surfaced, it gained little attention and was quickly forgotten until 
Olga Taussky (1906-1995), the premier woman of linear algebra, and her fellow 
German emigré Alfred Theodor Brauer (1894-1985) became captivated by Ger- 
schgorin's paper and his idea. Taussky (who became Olga Taussky-Todd after 
marrying the 
fort to strengthening, promoting, and popularizing Gerschgorin-type eigenvalue 
bounds. Their work during the 1940s and 1950s ended the periodic rediscov- 
eries, and they made Gerschgorin (who might otherwise have been forgotten) 
GERSCHGORIN 
numerical analyst John Todd) and Brauer devoted significant ef- 
famous. During his short life Gerschgorin began to make his mark in the Soviet 
O. TAUSSKY 
Union in the 
A. BRAUER 
areas of mechanics and applied mathematics, but he may have 
literally worked himself to death. An obituary suggested that work stress weak- 
ened his health until he was taken by an "accidental illness." The advent of ever 
faster and more powerful computers relegated eigenvalue localization results to 
be primarily of theoretical interest, but Gerschgorin's circles remained popular. 
For a complete survey of eigenvalue localization and a copy of Gerschgorin's orig- 
inal paper, see the monograph Gerggorin and His Circles by Richard S. Varga, 
Springer Series in Computational Mathematics, Springer, Berlin, 2004. 

6.2 Eigenvalue Localization 
767 
so that ||x||,o =1. If 2; is a component of x such that |x,;|=1, then 
n 
nm 
Ax, = [Ax], = [Ax], = YS aijay 
=> 
(A-ayi)xi = > ai52;, 
'1 
' 
j#t 
EOD ee oie o | 
oak 
At 
jFt 
and hence 
|A = Gis =p. cee ai |x; | = 
YS aig, 
j#t 
Thus X is in one of the Gerschgorin circles, and therefore the union of all such 
circles contains o(A). To prove (6.2.3), let D = diag (a11, a22,...,@nn) and 
B=A—D, and set C(t) =D+¢#B for t € [0,1]. The first part of the theo- 
rem shows that the eigenvalues of A;(t) of C(t) are contained in the union of 
the Gerschgorin circles C;(t) defined by |z — a;;| <tr;. The circles C;(t) grow 
continuously with t from individual points a;; when t = 0 to the Gerschgorin 
circles of A when t=1. This implies that if U/ is the union of k Gerschgorin 
circles of A centered at 4j,i,,@izi.,---)@i,i, that are isolated from the remain- 
ing n—k circles, then for every t € [0,1] the union U/(t) = eG. (t) is disjoint 
from the union U(t) of the other n—k Gerschgorin circles of C(t). Theorem 
6.1.4 on page 762 ensures that each eigenvalue ;(t) of C(t) varies continuously 
with t. This means that each A;(t) is on a continuous path [; in the complex 
plane having one end at ,(0) = a;; and the other end at A,;(1) € 
a(A). But 
since U(t) MU(t) = @ for all ¢ € [0,1], the paths T,,,T,,,...,T';, 
must be 
entirely contained in U, and hence the end points j,(1),A:.(1),.-., A, (1) are 
in U. Similarly, the other n —k eigenvalues of A must be in the union of the 
complementary set of circles. 
Hf 
Example (Gerschgorin Circles) 
e 
Consider estimating the eigenvalues of 
ay 
wl 
At aaa! 
206 
el 
tT: 
(6.2.6) 
Te 
The bound (6.2.1) on page 765 with the oo-norm puts o (A) in (or on) one 
relatively large circle of radius 7 centered at the origin. 
e 
Gerschgorin's theorem does better. Figures 6.2.1 (a) and (b) respectively 
show the union G, and G, of the Gerschgorin circles derived from row sums 
and column sums, and Figure 6.2.1 (c) shows the intersection G,G_ of the 
row and column circles. 

768 
Chapter 6 
Eigenvalue Continuity and Computations 
FIGURE 6.2.1(c): THE INTERSECTION G, 
Ge. 
Part (6.2.5) in Gerschgorin's theorem therefore guarantees that one eigen- 
value is in (or on) the circle centered at —5, while the other two eigenvalues 
are in the union of the other two circles in Figure 6.2.1(c). This is corrobo- 
rated by computing o (A)={5, (1 +5V5)/2} ~ {5, 6.0902, —5.0902}. 
Example (Diagonally Dominant Matrices Revisited) 
Recall from page 161 that An 
xn is said to be diagonally dominant (some authors 
say strictly diagonally dominant) whenever 
n 
lags | > 'SS lagz|. 
for each 
1= 1,2;...,7, 
(6.2.70) 
and it was proven in Theorem 2.3.5 that all diagonally dominant matrices are 
nonsingular. This is now a corollary of Gerschgorin's theorem because (6.2.7) 
ensures that the origin cannot be in any Gerschgorin circle. ' 
(Recall 0 ¢ a (A) 
if and only if A is nonsingular—see Theorem 3.1.5 on page 293). For example, 
the 3 x 3 matrix A in (6.2.6) is diagonally dominant, and thus A is nonsingu- 
lar. Even when a matrix is not diagonally dominant, Gerschgorin estimates still 
may be useful in determining whether or not the matrix is nonsingular simply 
by observing if zero is excluded from a (A) based on the configuration of the 
Gerschgorin circles given in (6.2.5). 
Irreducible Diagonal Dominance 
As pointed out in the previous footnote, diagonal dominance and Gerschgorin's 
theorem are intimately related, and the connections between them provided the 
motivation for Olga Taussky-Todd and Alfred Brauer to further develop and 
extend the theory of eigenvalue localization. One of the most useful extensions 
is the following theorem discovered by Olga Taussky-Todd in 1949. 
In fact, this result was the motivation behind the original development of Gerschgorin's circles. 

t 
= 
Proof. 
Assume A, 
x,» is irreducibly diagonally dominant but singular. Then 
there is an x # 0 such that Ax = 0. Assume that x is normalized so that 
IXllo =1, and let Z= {1,2,...,n} and I= {i € Z||z,;|=1}. Using Ax =0 
and the triangle inequality yields 
: 
n 
n 
n 
OP ie aus; 
Yee ZZ. 
==> 
|axi| < se |g, la, | < = Jax; 
Viel. 
aa 
Z 
ia 
This is the reverse of the hypothesis in the first inequality in (6.2.8), so 
n 
n 
|axi| = NE |as;| |x; | = wy lax, 
Viel, 
(6.2.9) 
oA 
ee 
For any j € Z such that a;; 40 with j #7, it follows from (6.2.9) that |x;|=1 
so that 7 € I. Irreducibility ensures that every row in A has a nonzero off- 
diagonal entry, and for each pair of nodes (N;,.N;) in G(A) there is a sequence 
of directed edges leading from N; to Nz, or equivalently, for each 1 € I, there 
is a sequence of off-diagonal entries such that @j;,@i,i. 
*++i,_1,i, # 0, where 
in =k. Hence |x,|=1, so 
k € 1. But Ny can be any node whatsoever, which 
means that k € Z. In other words, 
I = Z. But this is a contradiction because 
the second inequality in the hypothesis (6.2.8) ensures that (6.2.9) cannot hold 
for all 1 € Z so that 
14 Z, and thus the theorem is proven. 
@ 
Recall that A is irreducible if and only if the graph G(A) of A is strongly connected—i.e., 
for each pair of nodes (N;,N,) in G(A) there is a sequence of directed edges leading from 
N; to Nz. See Exercise 4.2.39 on page 455 or the discussion on page 808. 

770 
ME is | 
Chapter 6 
Eigenvalue Continuity and Computations 
Proof. 
If (6.2.7) holds and a;; > 0 for all i, then all Gerschgorin circles are 
strictly contained in the right-half plane, so Re(A) > 0 for each A € o (A). 
Hermitian matrices have only real eigenvalues (page 292). Consequently, A > 0 
for all \ € 0 (A), which ensures that A is positive definite (page 382). In the 
case of an irreducibly diagonally dominant matrix that is hermitian with positive 
diagonal entries, some Gerschgorin circle can touch the origin, but, by virtue of 
being nonsingular, no eigenvalue can be zero, so again, 
A > 0 for all 
Ac a (A), 
and thus A is positive definite. 
Example (Small Vibrations Revisited) 
To illustrate the utility of the preceding results, recall the problem concerning 
vibrating beads on a tightly stretched string that is analyzed on page 385. Under 
the assumption that only small vibrations are involved, it was shown that the 
equations of (vertical) motion are a coupled system of second order differential 
equations v'' + Av = 0 in which 
2 
-1 
= 
O21 
Ae 
= eet 
bas 
(6.2.10) 
mL 
; 
, 
The solution shows that the j*" normal mode of vibration is determined by 
the j'" eigenpair (A;,x;) of A, and the frequency of each bead in this mode 
is f = JrAj /2x. The physics of the situation implies that each eigenvalue Aj 
must be positive so that A must be positive definite. This was corroborated 
on page 387 by exhibiting a Cholesky factorization A = R'R in which R is 
nonsingular. But now the fact that A is positive definite is a direct consequence 
of Corollary 6.2.3 because A is clearly symmetric, irreducible, and diagonally 
dominant in the sense of (6.2.8). Note. The same analysis used above also applies 
to spring-mass vibration problems as described in Exercise 3.6.13 on page 402. 
Example (Heat Flow Revisited) 
To reinforce the power of the Corollary 6.2.3, reconsider the problem of heat 
flow in a plate as discussed on page 388. Recall that at steady state the tempera- 
ture u(z,y) at each interior point does not vary with time, and u(x, y) satisfies 

6.2 Eigenvalue Localization 
771 
Laplace's equation V?u = 0. When the temperature around the boundary is 
prescribed to be u(x,y) = g(x,y) for a given function g, this is the Dirichlet 
problem. As explained on pages 389-390, a numerical solution is achieved by 
discretization that involves overlaying the plate with a square mesh containing 
n* interior points at equally spaced intervals and approximating 0?u/0zx? and 
0?u/Oy? at the interior grid points with second-order centered difference for- 
mulas. This produces a system of linear algebraic equations Lu = g given in 
(3.6.10) on page 390. The salient feature of this system that allows it be solved 
numerically by any number of efficient special-purpose algorithms is that L is 
positive definite. This is Theorem 3.6.5 on page 390, and it was initially proven 
by resorting to the tricky gymnastics on page 314 involving tridiagonal Toeplitz 
matrices to explicitly exhibit the eigenvalues of L. 
However, no gymnastics (tricky or otherwise) are needed at this point be- 
cause the fact that L is positive definite is just a direct consequence of Corollary 
6.2.3. By looking at the structure of L in (3.6.10) on page 390 it is evident that 
L is symmetric, irreducible (any node in the graph G(L) is accessible from any 
other node), and diagonally dominant in the sense of (6.2.8) (looking at the case 
when n=83 
can help make this clear). 
Exercises for section 6.2 
Orme 
Nhe 
dl 
1 
ik 
4 
Al 
i 
6.2.1. Determine whether or not Anyn =|! 
! ' 
DY 
lis nonsingular 
dee 
OB 
sooty 
suet, 
without doing any computations, and explain your answer. 
6.2.2. Explain why Gerschgorin's theorem guarantees that 
(Mg ORS le 
Li Kimsbiowi = 
Lica V* wahd Niers bap {0 
Que 5 eon 
must have at least two real eigenvalues. Confirm this fact by computing 
the eigenvalues of A. 
6.2.3. Let A € R"*" be irreducible in which a;,>0 
Vi and aj; <0 Vj #71. 
Prove that if A is singular and a, > Da —a;; 
Wi, then Ae = 0, 
where e is a column of all ones. Hint: Consider the proof of 
Theorem 
6.2.2 on page 769. (Note that the converse is trivially true since Ae = 0 
implies that N (A) # 0.) 

772 
Chapter 6 
Eigenvalue Continuity and Computations 
6.3 POWER METHODS 
t 
t 
Almost all iterative algorithms for eigenvalue-eigenvector calculations implicitly 
rely on the fundamental fact that for most matrices 
Am xm and vectors Xm~1, 
successive multiplications produce terms {x, Ax, A?'x,...} that tend toward 
the direction of an eigenvector associated with the domitiahe eigenvalue of A. 
i 
3. L Theorem. (The Power tuihoay) Ssoce that AéE es 
is 
diagonalizable t whose distinct eigenvalues are 
: 
Iai] > Ag] > [As] > --: = ide: 
This forces A, to be real—otherwise A; # A, is an eigenvalue with 
[Ay] = |Ai|. For 
v € F™, let v(v) be the first component in v that has 
maximal magnitude—.e., if v(v) = v;, then |v;| = max; |v,| = ||v ||, 
If xo ¢ R(A — A,I) with v(xo) = 1, and if 
Ax, -1 
A"Xo 
pie SEAR 
1OF 1 
4-2; 
V(AXn-1)  V(A"Xo) 
2a 
then lim, 4..%, =x and lim,.,., x(Ax,) = Ar, where, Ax = Aix. 
Proof. 
Consider f(z) = (z/A1)", and use the spectral decomposition theorem 
(page 549) and Corollary 4.7.7 on page 550 along with |A;/Ai| <1 for i >2 to 
observe that 
(+) = f(A) = fOx)G@1 + f(A2)G@a +--+ + fOn)Ge 
i 
in 
(6.3.1) 
AQ 
Ak 
=e a 
ei 
rg ee 
si G,—7 G, 
asn—-oo, 
1 
The development of the power method (originally called von Mises iteration) was considered to 
be a significant achievement when Richard von Mises introduced it in 1929, but later sousdidibiat 
relegated its computational role to that of a special purpose technique. 
Nevertheless, 
it remains as an important idea because, as alluded 
to in the first paragraph, many practical algorithms for eigenvalue- 
eigenvector computation in some way or another implicitly rely on its 
mathematical essence. Richard Edler von Mises (1883-1953) (the term 
"Edler" signifies a noble landless family) was an Austro-Hungarian 
born scientist and mathematician whose early career was interrupted 
in 1915 by World War I in which he served as a test pilot, flight 
x 
. 
— 
instructor, and aircraft designer. He later became the Gordon McKay 
RICHARD VON MISES 
Professor of Aerodynamics and Applied Mathematics at Harvard University. His brother was 
the prominent economist Ludwig Edler von Mises. 
Diagonalizability is not necessary. In general, all that is required is for indexz(Ai) = 1 (see 
Exercise 6.3.5) 

6.3 Power Methods 
773 
where G, is the spectral projector onto N (A —,I) along R(A — d,I). Hence 
A"x0/\7 > Gixo 
€ N(A—Aj,I) for all xo. This is an eigenvector associated 
with A, provided that G)xo #0, or equivalently, x9 ¢ R(A — 2,1). Further- 
more, the convergence rate is governed by how fast |A2/A|" — 0. The direction 
of A"xo tends toward the direction of an eigenvector while \? acts as a scaling 
factor to keep the length of Ax under control. This suggests that AT can be 
replaced by a more convenient scaling factor. Two popular choices are ||A"xo||, 
and v(A"Xx9), but the latter has the advantageous property that v(av) = av(v) 
for all 
a € F and v € F™. This helps because if x, = A"xo/v(A"xo) and 
V(A""x9/AT) 
> 7, then 
A""xo 
ates 
(A"/A?)xo 
lim Xn = 
li 
~ noo U(A"Xy/AP) 
nate" ~ 
node V(AP2xe) ~ 
nso 1(AP 
xo 
/XP) 
(6.3.2 
aa 
02) 
= 
=~ 
€ R(G1) = N(A- dD). 
Thus x, — x = G,xo/7 is an eigenvector associated with \,. As a bonus, 
v(Ax,) — A; because 
® 
° 
Xe 
jim AXn = dim V(AXn—1)" 
(633.0) 
so if v(Ax,) > v*, then v(Axn_1) > v* as n — o. Note. You can- 
not naively conclude that v(Ax,) — v(Ax)—see Exercise 6.3.3. The limit 
on the left-hand side of (6.3.3) is Ax = Ax, while the limit on the right- 
hand side of (6.3.3) is A?x/v* = \?x/v*. These two limits must agree, so 
Aix = (A?/v*)x, and thus v* = \, (because \; 
#0 and x has at least one 
nonzero component). 
Mf 
Summary. The power method as presented above can be summarized by ob- 
serving that starting with xp ¢ R(A — iI), where v(xo) = 1, the iteration 
CPA Se 
UV 
1) ead eae iee bi LOLA 
all 
2h 
sug 
(6.3.4) 
Uy 
converges to an eigenpair (Ai, x) for A. It follows from (6.3.1) that the rate at 
which (6.3.4) converges depends on how fast (A2/A1)" — 0 (Exercise 6.3.7 shows 
that ||Xn —x||,, = O(|A2/A1|")), so convergence becomes agonizingly slow as 
|A;| nears 
|A2|. A redeeming feature is that each iteration requires only one 
matrix—vector product that can be exploited to reduce the computational effort 
when A is large and sparse. When the development is altered by replacing the 
scaling factor v(A"xo) with ||A"xo||,, and (6.3.4) is modified to start with 
\|Xo||. =1 and iterate 
Vike Xe 
il Vill oe Xd cS =, 
and 
A) =xTAx,, 
(6.3.5) 
m 
then modifying the proof of Theorem 6.3.1 shows that x, — +G,xo/ ||G1xo||, 
and \\™) — d, (see Exercise 6.3.8). Finally, the hypothesis in Theorem 6.3.1 
that A is diagonalizable can be relaxed because the power method still converges 
when A is not diagonalizable provided that index(A;) = 1 (see Exercise 6.3.5). 

774 
Chapter 6 
Eigenvalue Continuity and Computations 
xample (Population Migration Revisited) 
| eae ae the debut migration example on page 327 that addresses the problem 
of determining the long-run population distribution between the North and South 
when 50% of the population in the North migrates to the South each year, while 
25% of the population in the South each year moves to the North. If no and so 
are the respective initial proportions of population in the North and South, and 
nz and s, are the respective proportions of the total population in the North 
and South at the end of year k, then, assuming that n, +s, = 1 foreach k, 
Ay 
gh 
(3.3.16) on page 328 says that for pg = (ng 8x) and 
T= et nae 
Dp = p,_,T, 
so when iterated 
Aes = Dale: 
(6.3.6) 
This is the power method. Observe that T is diagonalizable because T has 
distinct eigenvalues. Indeed, Te = e implies \; = 1 € o(T), and |Ag| < 1 
for the other eigenvalue because Gerschgorin's theorem shows that |A2| <1 or 
A2 = 1, but Az 4 1—otherwise I—T would be nilpotent, which it is not. Except 
for using left-hand multiplication rather than right-hand multiplication, (6.3.6) 
is the power method without the scaling function v(pz_,T). Scaling factors are 
not needed because pg_;T > 0 and pj_,Te=1 
(ie., ||pg_,T||, =1 for each 
k), so there is no need to scale the iterates in order to control their magnitude. 
Moreover the v's are not required to determine the dominant eigenvalue A; = 1. 
As Theorem 6.3.1 predicts, the power method produces a left-hand eigenvector 
associated with A; = 1 to be p4, = limg40 py = (1/3, 2/3), which agrees 
with (3.3.17). 
Inverse Power Method (Also Called Inverse Iteration) 
Given a real approximation a ¢ o(A) to any real \ € o(A), this algorithm 
determines an eigenpair (\,x) for a diagonalizable matrix A € F"™*™ by ap- 
plying the power method to 
B = (A—al)~!. Helmut Wielandt' is credited with 
The relationship between the power method and inverse iteration now 
seems rather obvious, but it originally took 15 years to make the connec- 
tion. Inverse iteration was not introduced until 1944 by the German mathe- 
matician Helmut Wielandt (1910-2001), who called it functional iteration. 
Wielandt was a student of Issai Schur (page 167) and Erhard Schmidt 
(page 658) at the University of Berlin. Originally an algebraist working on 
finite group theory, World War II took Wielandt to the Kaiser Wilhelm 
Institute and the Aerodynamics Research Institute at G6éttingen, where, 
through his work with vibrations and aircraft wings, he came to appreci- 
ate the need for estimating and computing eigenvalues of matrices. After 
Sai 
: 
the war years he moved to the University of Mainz. In 1951 he was 
_H®LMUT WIELANDT 
given the position of "Ordinary Professor" at the University of Tiibingen, where he remained 
except for periodic visits to the University of Wisconsin in the 1960's. While his group theory 
work is well known, his complete development of Perron—Frobenius theory (page 797) made 
him famous in the linear algebra community. Wielandt was widely regarded to have been the 
natural successor of Frobenius and Schur and who was at least their equal. 

6.3 Power Methods 
775 
the introduction of the idea. Recall from Exercise 3.1.13 that 
x is an eigenvector for A <=> x is an eigenvector for B, 
6.3.7 
A € 0(A) = (A—a)7! € o(B). 
( 
) 
If |A—a| <|A; — | for all other 4; € o(A), then (A—a)~! is the dominant 
eigenvalue of B because |A—a|~! > |\; —a|~!. Therefore, applying the power 
method to B_ produces an eigenpair 
((A —a)~',x) 
for B from which the 
eigenpair (A,x) for A is determined. That is, if xo ¢ R(B-— AI), and if 
ne bke =H (A=— Ol 
ka 
a SU al 
Xn = ae LOE 
"Up bees 
n 
then (vp,Xn) + ((A—a)~!,x), an eigenpair for B, so (6.3.7) guarantees that 
(v;++a,Xn) > (A,x), an eigenpair for A. But rather than using matrix 
inversion to compute y, = (A —alI)~!x,, it can be more efficient to solve the 
linear system (A —al)y, = x, for yn. Because this is a system in which the 
coefficient matrix remains the same from step to step, the efficiency is further 
enhanced by computing an LU factorization of (A—al) at the outset so that at 
each step only one forward solve and one back solve is needed to determine yy 
(see page 262). Since the convergence rate of inverse iteration is inherited from 
that of the power method, convergence depends on how fast (u2/H1)" + 0 in 
which py = |A—a|~! and p2 = |A—a|~!, with 
being the second closest 
eigenvalue to ». As mentioned in the summary on page 773, the limit x of the 
standard power method satisfies ||x, — x||, = O|A2/A1|" so that 
Consequently, if the inverse power method converges to x (not necessarily the 
same x in the equation above), then its iterates satisfy 
A 
[nea 
= lle _ 
AT el, —xll, 
(6.3.8) 
—a 
[Xn —xllo 
2 
fi 
|Xn+1 — X|> =O 
Rayleigh Quotient Iteration 
In the standard version of inverse iteration a constant value of a is used at each 
step to approximate an eigenvalue 
A. An enhanced variation called Raylezgh 
quotient iteration uses the current iterate x, to improve the value of a on the 
next step by setting 
Recall that for A € R™*™ 
the function R(x) =x? Ax/x!x 
is called the 
Rayleigh quotient (page 349), and if Ax = Ax (x #0), then R(x) = d. If 

776 
Chapter 6 
Eigenvalue Continuity and Computations 
v is an approximation to x, then R(v) is a good approximation of A, partic- 
ularly when A = A'. To see this, use the Taylor expansion of R(v) about x 
as described on page 394 to obtain 
(v = 
x)FH)(v 
— x) 
9 
where V.R(x) is the gradient of R(v) evaluated at x. To determine VR(v), 
differentiate R(v) =v' Av/v'v with respect to each v; by using the quotient 
rule for differentiation along with the symmetry of A, and then evaluate the 
result at x to produce 
R(v) = R(x) +(v —x)? VR(x) + 
+0O(|\v — x||3), (6.3.9) 
O(vF Av) 
(v7 
Vv) 
CEN 
a) 
see 
TA 
Ov; 
_ 
(v?v)? 
Vv=x 
_ (vT v)(eP 
Av + v" Ae;) — (e7 v 
+ v7e:)(v" Av) 
a 
(viv)? 
ay 
7 
—— 
T 
ACSC 
20074 eel 2 CD BA WP REE I, 
(x?x)? 
Thus VR(x) = 0, so it follows from (6.3.9) that 
= 
sae 
= 
R(v) — R(x) = 
2) IAS) + flv — x9) 
and therefore 
: 
|R(v) — A] = |R(v) — R(x)| = O(|lv — x9). 
(6.3.11) 
If the Rayleigh quotient update a, = R(x,) is made when moving from x, to 
Xn+1 im the inverse power method, then R(x,) + A as x, > x, so the term 
ee R(x,,)| becomes essentially constant. When this constant is absorbed into 
the big O constant obtained by combining (6.3.8) with (6.3.11), the conclusion 
is that if ||x, —x||, <¢, then, eventually, 
A — R(x,,) 
|Xn+1 — Xllp = O 
5 lan = lla = OLA — Rn) en — 2 
~ 
TL 
zi Oem: 
(6.3.12) 
e 
This means that for symmetric matrices Rayleigh quotient iteration eventu- 
ally exhibits cubic convergence —i.e., the number of accurate digits eventually 
triples on each iteration. 
> It is rare in numerical analysis to encounter iterative algorithms that 
exhibit this remarkable behavior. Striking results can often be obtained 
with only one or two iterations, even when xo is nearly in R(B-— AI). 

6.3 Power Methods 
> 
777 
For a close to 4, computing an accurate floating-point solution of 
(A —al)yn = Xp is difficult because A —al is nearly singular, which 
almost surely guarantees that (A —al)y, = x, is an ill-conditioned sys- 
tem. But only the direction of the solution is important, and the direction 
of a computed solution is usually reasonable in spite of conditioning prob- 
lems. Finally, the algorithm can be adapted to compute approximations 
of eigenvectors associated with complex eigenvalues. 
e 
For Non-symmetric Matrices. If ) is the closest eigenvalue to A, then a 
good estimate a to A must be known in advance such that |A—a| < |A—al. 
The analysis for Rayleigh quotient iteration does not apply, and the rate of 
convergence depends on how fast 
[(A - @)/X-a)}" +0, 
so convergence can be slow when A and X are close. 
> If A is too close to '¥ roundoff error can divert the iterates toward an 
eigenvector associated with X instead of 4 in spite of a theoretically cor- 
rect a. Even though the Rayleigh quotient update for a cannot be used 
for non-symmetric matrices, there is nevertheless an updating strategy 
available. Take advantage of the fact that vz ++a-— 
to update a at 
each step by setting Qn41 = An +1/p. 
Exercises for section 6.3 
6.3.1. 
6.3.2. 
6.3.3. 
6.3.4. 
ee) 
ae 
Find a dominant eigenpair for A= ( 
ih 
2 | 
by the power method. 
6) 
Apply the inverse power method (inverse iteration) to find an eigenvector 
for each of the eigenvalues of the matrix A in Exercise 6.3.1. 
Explain why the function v(v) used in the development of Theorem 
6.3.1 is not a continuous function, so statements like v(xpn) > v(x) 
when x, — xX are not valid. Nevertheless, if limn..»Xn, #4 0, then 
lity, 00 U(Xn) 7 U- 
Let (\,x) be an eigenpair for A € R"*"™ with 
||x||, = 1, and let 
v be an approximation to x with ||v||, =1 and |/v—x||, =e. The 
analysis leading to (6.3.11) showing that |R(v) — A| = O(c?) depends 
on the assumption that A is symmetric. Show that when symmetry is 
not assumed, |R(v) — A| < 2||Al], ||v — xl], = O(€). 

778 
Chapter 6 
Eigenvalue Continuity and Computations 
6.3.5. 
6.3.6. 
6.3.7. 
6.3.8. 
6.3.9. 
Let o (A) = {Ai,A2,---,As} be the distinct eigenvalues of Am xm with 
|Ai| > |A2| > |As]| > +++ > |As| and index(A1) = 1. Explain why the 
power method converges even when A is not diagonalizable. 
Suppose that v(A"x9/A?) > y. Explain why || = ||Gixo||,,, where 
v(x), Xo, and Gj, are as defined in Theorem 6.3.1. Use this to conclude 
that if x, — x in the power method, then ||x||,, = 1. 
For the power method in Theorem 6.3.1, give an argument to explain 
why ||Xn —x||,, = O(|A2/A1|"). Note. This means that eventually, 
|Xn+1 — X||,, = O(|A2/A1)) ||xn — X||,,, Which is (slow) linear conver- 
gence (recall the discussion on page 626 concerning asymptotic rates of 
convergence and estimating the number of digits of accuracy that can 
be expected to be eventually gained on each iteration.) 
Show that the modification in (6.3.5) yields x, — +G 1x0/ ||G1xo||, 
and \\") —» \y, where the + indicates alternating signs as x, settles 
down to an eigenvector. 
Hint. Consider f(z) = op (z/|Ai|)" , where on = { 
+1 
when n is even. 
—1_ 
when n is odd. 
Revisit Exercise 3.3.16 on page 337 and apply the power method to 
corroborate the solution found there. 

6.4 QR Iterations 
779 
6.4 QR ITERATIONS 
The QR iteration algorithm for computing the eigenvalues of a general matrix 
came from an elegantly simple idea that was proposed by Heinz Rutishauser' in 
1958 and refined by J. F. G. Francis' in 1961-1962 and independently published 
in the Soviet Union around the same time by Vera N. Kublanovskaya: The un- 
derlying concept is to alternate between computing QR factors (Rutishauser 
used the LU factors—see Exercise 6.4.5 on page 796) and reversing their order 
as shown below. Starting with A, = A € R"*", 
Factor: 
A, = Q;R,, 
Set: 
Ao = R,Q,, 
Factor: 
Ag = QoRo, 
Set: 
As = R2Qz, 
In general, Axii1 = R~Qzx, where Q, and Rx are the QR factors of Ax. 
Heinz Rutishauser (1918-1970) was a Swiss mathematician and early computational and com- 
puter scientist who was instrumental in helping to develop the first Swiss 
computer along with computer languages and compilers. His major mathe- 
matical achievement was his article "Solution of Eigenvalue Problems with 
the LR Transformation," Nat. Bur. Stand. App. Math, Ser. 49, 1958, pp. 
47-81, in which he laid out his discovery and proof of the fact that repeat- 
edly factoring and reversing the LU factors (he called them LR factors) of 
a matrix would, under proper conditions, converge to an upper triangular 
matrix revealing the eigenvalues of the matrix. This became known as the 
LR iterative method. 
H. RUTISHAUSER 
John G. F. Francis (1934-) is an English computer scientist who extended Rutishauser's ideas 
to QR factorizations. Born in London in 1934, he attended Cambridge University, but failed to 
complete a degree. After leaving Cambridge, Francis went to work at the Na- 
tional Research Development Corporation in England where he concentrated 
on computer programming languages and algorithms. It was there in 1961 
that he formulated his ideas on QR iterations. But immediately thereafter 
he abandoned numerical analysis and never returned. Francis apparently had 
no idea of the importance of his QR algorithm until he was informed of it in 
his retirement in 2007. The irony is that his iterative QR technique is widely 
regarded to be one of the ten most important algorithms of the twentieth 
a 
century. As recognition, he was awarded an honorary doctorate degree by — Joun Francis 
the University of Sussex in 2015. 
Vera Nikolaevna Kublanovskaya (1920-2012) was a Russian mathematician and numerical ana- 
lyst. She is remembered for three 1960-1962 articles (in Russian) on the QR al- 
games 
gorithm. After working at the Russian Academy of Sciences ina group headed 
= 
by the well-known mathematician L. V. Kantorovich, Vera Kublanovskaya be- 
; 
came a senior researcher at the Steklov Mathematical Institute, a premier part 
of the Russian Academy in Moscow. In 1968 she moved to the St. Petersburg 
Marine Technical University, where she later became Professor and Chair of 
Applied and Computational Mathematics. In 1985 she was recognized for 
her contribution with an honorary doctorate by the University of Umea in 
KypianovsKAYA 
Sweden. 

780 
Chapter 6 
Eigenvalue Continuity and Computations 
Notice that the product P, = QiQ2-+-Qx is an orthogonal matrix such that 
PT AP, = Q?QiRiQi = Ao, 
PP AP2 = QFQTAQi 
Qs = QZ A2Q2 = As, 
(6.4.1) 
PT AP, = Agi. 
In other words, 
Ao, A3,A4,... 
are each orthogonally similar to A; = A, so 
each A, has the same eigenvalues as A;. But the process does more than just 
create a matrix that is similar to A at each step. The magic lies in the fact 
that if the process converges, then the limit of the sequence of the Aj,'s is an 
upper-triangular matrix whose diagonal entries are the eigenvalues of A. To see 
this, observe that if P, > P, then Pz; — P and 
lim Ay = lim Pp? ,AP,-1=PTAP=U 
( 
—0o 
means "defined to be"). 
k-00 
Ill 
Also, Pg-1 = QiQ2+--Qx_-1 = Px_2Qx_-1 means that P{_, = Qf_,P{_2, so 
Q? 4 =P?_,P.-2- le follows'from Ay = Hyp-19;- 7 that 
lim Rz—-1 = lim A,Q?_, = lim A;,(P?_,P,_-2) = U(P7P) =U. 
k—o0o 
k-oo 
k-0o0o 
Therefore, limp,.9 Ax = U = limg_4. Re-1, and thus limp... Ax is upper 
triangular having diagonal entries equal to the eigenvalues of A. 
However, as is often the case, there is a big gap between theory and practice, 
and turning this amazing fact into a practical algorithm requires significant effort. 
For example, one obvious hurdle that needs to be overcome is that the R factor 
in a QR factorization has positive diagonal entries, so, unless modifications are 
made, the naive version of the QR iteration described above cannot converge for 
matrices with complex or nonpositive eigenvalues. Remedies for these problems 
are developed below. 
Hessenberg Matrices 
A big step toward turning the QR iteration into a practical method is to realize 
that everything can be done with upper-Hessenberg matrices (pages 680-681). 
As illustrated there, Householder reduction can be used to produce an orthogonal 
matrix P such that 
P' AP = H, is upper Hessenberg. Since A and Hy, have 
the same eigenvalues, A can be replaced by H, in the QR iteration. The exam- 
ple on page 682 shows how Givens reduction generates the QR factors of an upper 
Hessenberg matrix. Applying Givens reduction to H, produces the Q-factor of 
Hj as the transposed product of plane rotations Q; = P7,P%,--- Pa and 
Q: is conveniently upper Hessenberg again (constructing a 4 x 4 example will 
convince you). Since multiplication by an upper-triangular matrix can't alter 
the upper-Hessenberg structure, the matrix R,Q, = H» at the second step of 

6.4 QR Iterations 
781 
the QR iteration is again upper Hessenberg. And so on for each successive step. 
Being able to iterate with Hessenberg matrices results in a significant reduction 
of arithmetic. Moreover if A = A', then H;, =H? for each k, which means 
that each Hy is in fact symmetric and tridiagonal, which is an even greater 
advantage as detailed on page 786. 
The prior statements are for unreduced Hessenberg forms (no zero subdiag- 
onal entries). If a zero subdiagonal (to some appropriate tolerance) is detected, 
then the problem can be deflated into two or more unreduced Hessenberg forms. 
All Hessenberg matrices encountered in the sequel are assumed to be unreduced. 
Revealing Complex Eigenvalues 
When the H;,'s converge, the entries at the bottom of the first subdiagonal tend 
to die first—i.e., a typical pattern might be 
H, = 
(6.4.2) 
On 
eeaar Oe 
3 
oe 
+ 
& 
& 
When e is satisfactorily small, define * (the (n,n)-entry) to be a computed 
eigenvalue, and deflate the problem. An even nicer state of affairs is to have a 
zero (or a satisfactorily small) entry in row n—1 and column 2 as illustrated 
below for n = 4. 
kok 
Ok 
Ox 
x 
Ok 
* 
Ox 
Hytss 3 [ edit 
pit 
(6.4.3) 
O70 
es 
Applying the quadratic formula to the trailing 2 x 2 block 
f.=(+ 1) 
in H, produces two computed (or approximate) eigenvalues a;,, and 6; . In 
the case of complex eigenvalues, 6, = @,, and this is how complex eigenvalues 
can be revealed. But in practice complex eigenvalues are usually computed using 
the double shift iterations described on pages 782 and 788. 
Explicit Single Shift QR 
Instead of factoring H;, at the k*" step, use an approximate real eigenvalue 
such as ay = * = [Hg|nn in (6.4.2), and factor the shifted matrix 
H; — a,1 =Q,Rz, 
which produces 
Hy41 = ReQy + axl. 
(6.4.4) 
Using Ry = Q? (Hx — axI) in (6.4.4) produces 
Hii = QP (Hg — o¢1)Qx + onl = QEH:Qz, 
(6.4.5) 

782 
Chapter 6 
Eigenvalue Continuity and Computations 
so o (Hy41) =o (Hx). The inverse power method (inverse iteration) from page 
774 is now at work. To see how, temporarily drop the subscripts and write 
H—al=QR 
as 
Q?=R(H-al)™. 
If 
a \€o0(H) =a 
(A) (say, |\—a| =€ 
for real a and 4), then the analysis 
surrounding inverse iteration ensures that the rows in Q' are close to being 
left-hand eigenvectors for H associated with . In particular, if ql is the last 
row in Q7, then 
ranel = eR = q2 QR = qi (H — al) = qi H 
— aq, ~ (A—a)a;, 
so Tnn = |rnn| © ||(A— @)az||, =|A— al =€ and q7z © te; The significance 
of this is revealed by looking at a 4 x 4 pattern for 
Hy41 =RQ+al 
KEE 
ok 
x 
x 
« 
© 
a 
es 
Oe 
we 
eae 
oh 
een 
eae 
- 
a 
Selh0r 
0. 
Beeee 
Ox 
*e 
0 
a 
@ 0 7 
«€ 
00 
+1 
a 
(6.4.6) 
* 
* 
* 
* 
* 
* 
* 
* 
— 
* 
* 
* 
* 
a 
* 
* 
* 
* 
Sw 
leO 
ee 
* 
ela 
Olok 
ok 
* 
0 
0 & 
ate 
OD 
Oe. 
Om ote 
The strength of the last approximation rests not only on the size of ¢€, but it is 
also reinforced by the fact that « ~ 0 because the 2-norm of the last row of Q 
must be 1. This indicates why this explicit single shift QR iteration can provide 
rapid convergence to a real eigenvalue. 
After sufficient convergence is observed, a computed eigenvalue A, is de- 
fined to be the (n,n) entry, and the problem can be deflated to compute An—1 
by working on the leading principal submatrix. However, the subdiagonal entries 
are trying to more slowly converge to zero, so if one of these has become suffi- 
ciently small, then the problem can be further deflated to provide even smaller 
matrices to work on. 
Explicit Double Shift QR 
The double shift iteration uses two shifts a, and (, that are the eigenvalues 
of the 2 x 2 matrix Hy in the lower right-hand corner of Hy, as described in 
(6.4.3). Remember that 8, = @, when a, is complex. This is the preferred 
version to reveal complex eigenvalues. 
Factor: 
H, — axl = Q;,Ry 
Set: 
Hyii = RzQz + axl 
(so Hy41 = QP H;,Q,) 
(6.4.7) 
Factor: 
Hy1i — 6,1 = Q:41Rei 
Set: 
Hye = Rei1Qey1 + Bel (so Heys = QF, ,Q7H.QeQesi) 
(6.4.8) 

6.4 QR Iterations 
783 
Implicit Q-Theorem 
Each step of an explicit QR iteration requires the QR factorization of a shifted 
Hessenberg matrix H —alI. The following remarkable theorem circumvents the 
need to explicitly make the shift. First the theorem is proven, and then its 
application is explained on pages 784 and 788. 
Hors ieee 
eee oe : 
fs 
a | 
Mee Pie if 2S a - 
2 are orthogonal 
| E 
suc 
: 
-unreduced real eae forms, and if 
Qu a es G8 
ae aP,, ee 
7 oS each de 
= 2, 3,. 
ae 
RS 
ee 
ee 
aol 
ee 
Proof. 
For X = P7Q the strategy is to show that X = ( : " 
so 
+1 
that Q = PX, or equivalently, Q,; = +P.; for each j. First observe that by 
virtue of being the product of real and orthogonal matrices, X is also real and 
orthogonal. The hypothesis that Q,,; = P.; implies that X,, = e,. Further- 
more, 
KX = KP7Q = (P7 AP)P7Q = P?(AQ) = P"(QH) = XH. 
Examining the j*" column on each side of this equation while taking the Hes- 
senberg structure into account yields 
j+1 
j 
KX(e = KX (XH feeXHs— Xe = S) Xeshag + Kage hyn, 
4=1 
4 
j 
Kx, 
= wy xs FOR Jeol, 
2 eet 
etees 0:40) 
Exerc 
= 
ad 
lied 
Heese 
Since hj;+41); #0 for each j, an induction argument proves that (6.4.9) implies 
that X is upper triangular. For example, when j = 0 there is nothing to prove 
because X,,; =e,. When j =1 the Hessenberg structure of K shows that the 
zero pattern in X,¢ is 
oO 
* 
1 
1 
; 
Keg = —([KXui — Xe hii] = — [Ka -—erhulJ=|] 
|=] 
. | eR". 
21 
hoi 

784 
Chapter 6 
Eigenvalue Continuity and Computations 
The zero pattern in X,3 is obtained by using this with 7 = 2 in (6.4.9) to get 
* 
13 
* 
£23 
1 
* 
£33 
oe 
Xag = — [KXu2 — Keahi2 — Xe2hv] =] 
0] 
=] 
9 | ER". 
hg2 
0 
0 
Formal induction along these lines proves that X is real and upper triangular. 
Since X is also orthogonal, Lemma 3.4.3 on page 340 guarantees that X must 
be diagonal. In general, an orthogonal matrix need not have real eigenvalues 
(see Exercises 3.4.20 and 3.4.21 on page 357), but since X is real, diagonal, and 
orthogonal, its eigenvalues (i.e., its diagonal entries) are all real, and therefore 
X = diag(+1, +1, ..., +1). Thus Q.; = =+P,; because 
Q=PX. 
Implicit Single Shift QR 
In the explicit single shift QR iteration a shift matrix a,I is subtracted from 
H;,, as indicated in (6.4.4), and then a QR factorization of Hy, — axl = 
Q, Rx 
is used to arrive at the next iterate Hy; = 
QP H,Q, in (6.4.5) on page 781. 
By invoking the implicit Q-Theorem, H;+; can be obtained without explicitly 
subtracting a, from every diagonal entry of Hx. If an orthogonal matrix P, 
can be found such that BH; 
is an unreduced upper Hessenberg matrix, 
where the first column of P;, agrees with the first column of the Q-factor Qrz, 
then the implicit Q-Theorem guarantees that P;, = Q;, (up to signs). 
To understand how P, is constructed, consider the 5 x 5 example 
hit 
* 
* 
* 
* 
Rigi 
0 
Gee 
ho 
at 
H, = 
O 
* 
* 
* 
* 
0 
Ob 
partart« 
0 
0 
0 
* 
x 
Setting 
K= H; —al= Q:Re, 
(6.4.10) 
and comparing the first column on each side of this shows that 
hii =a 
h 
ae 
hat 
Hos 
0 
: 
: 
Ky, = 
; 
= [QzJai711 => 
[Qz]1 is a multiple of K,, = 
A 
0 
0 
Since [Qx]«1 must have unit norm, it follows that 
(hit a a)/ 
(hit = a)? + ne 
Ky, 
hoi/y/(hi1 
— a)? +h, 
51 
[(Q.]1 = +——— =+ 
: 
= 

6.4 QR Iterations 
785 
C1 
—81 
~ 
8 
Cc 
~ 
Let H®) = H,. If P; = 
: 
me ; 
, then [Py]x1 = [Qx]a1 and 
1 
PTH P, = 
® eae 
Re H) 
(= means "defined to be"). 
O 
RON 
TOE 
ee 
in which @ can be nonzero. The strategy is to invoke the implicit Q-Theorem by 
restoring H) to upper Hessenberg with an orthogonal similarity transformation 
PTH 
P, in which 
[Pg]a1 = [Qg]a1. This is accomplished by applying an 
additional series of elementary rotations 
1 
1 
1 
~ 
(Pp 
9) 
~ 
1 
~ 
i 
P2= 
82 
c2 
P3 = 
Ce 
ans 
ea —= 
1 
1 
83 
c3 
C4 
—S84 
L 
1 
84 
C4 
in which the c,'s and s,'s are the rotation parameters from (1.10.14) on page 
99 such that when successively applied on the left-hand side, P3, P7, and PY 
will respectively zero out the (3,1), (4,2) and (5,3) positions. When the P,;'s 
are also applied on the right-hand side, the results are as shown below. 
* 
ok 
* 
ok 
Ok 
oR 
ba 
ee 
a 
KS 
* 
pig 
i ge cet 3 
= 
as 
* 
* 
ie 
ee 
eg 
H?) = PTH P, = 
(yee 
sh 
ar 
H®) = PPH®P, = 
Opt 
kek 
0 
Olek 
ok 
0 
&) 
* 
* 
Ox 
OFRO Os 
OOO 
OM 
eee 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
~ 
~ 
* 
* 
* 
* 
ES 
— 
oe 
* 
* 
* 
* 
* 
H®) — PLHP, = 
OMe 
OTe 
so 
pk 
H®) = PTH P, = 
OV 
AG. 
oS 
cee 
OG 
See 
eee 
OF Ose. 
00 
@ 
* 
* 
000 
* 
* 
Thus the unwanted nonzero @ is "chased out" of the Hessenberg form. Now, 
C1 
* 
* 
eee 
PS 
ES 
ae 
oe 
Sy 
* 
* 
* 
* 
P; = P,P2P3P4 — 
0 
82 
* 
* 
* 
(6.4.12) 
CO 
RU 
he 
es 
0 
0 
OR 
sac 
is an orthogonal matrix such that P?/H,P, = H©) 
is an upper Hessenberg 
matrix (assumed to be unreduced—otherwise the problem can be uncoupled) 
such that [Px]x1 = [Qzx]x1, so the implicit Q-Theorem guarantees that Px = Qx 
(up to signs). 
e 
In general, if the iterates PLHP, = H,41 converge (adjusting for signs as 
described below), then they converge to an upper triangular matrix whose di- 
agonal entries are the eigenvalues of H (and hence of A). But as mentioned 
on page 782, the last subdiagonal entry in PTH,P, will go to zero before 
the entire matrix converges, so the (n,n) entry can be taken to be a com- 
puted eigenvalue, and the problem can be deflated to the leading principal 
(n—1) x (n—1) submatrix. 

786 
Chapter 6 
Eigenvalue Continuity and Computations 
Dealing with the Signs 
; 
The signs of the columns in Q;, are not an issue because changing them amounts 
to changing the QR factorization in (6.4.10) on page 784 by observing that 
a 
+1 
Hy, — a = QuRe = (QeSk)(ShRe) =QeRe, 
where 
S, = 
x 
+1 
The matrix Qk = Q;Sx is still an orthogonal matrix, and Ry = S;Rx is still 
upper triangular, so choosing the signs in S$; to make all diagonal entries in Rx 
positive produces the unique QR factorization of H; —a,I. When this is done, 
the next iterate in (6.4.5) on page 781 becomes 
Hyat = (QeSx)7 
He (QeSx) = Se(QE 
He Qz)Sx- 
(6.4.13) 
Implicit Symmetric QR 
When A € R"*" is symmetric, then, as noted on page 780, the Hessenberg 
matrices H, are all symmetric tridiagonal matrices of the form 
H,= Ll. = 
'i < 
(assumed to be unreduced). 
(6.4.14) 
Tn—-1 
dn 
The implicit single shift method on page 784 is specialized a bit. Rather than 
choosing the shift to be a, = dy, as in (6.4.4) on page 781, better results usually 
can be obtained by setting a, to be the eigenvalue of Gone He that is 
closest to dn. This is called the Wilkinson' shift, and for symmetric matrices it 
can produce superior convergence. 
Recall from (6.4.5) on page 781 that if Ty, — a,I = Q;,R,x 
is the QR 
factorization, then the next iterate is Ty4, = (a Ea WP The goal in implicit 
James Hardy Wilkinson (1919-1986) was a preeminent twentieth-century English computa- 
tional scientist who is widely regarded as being the father of matrix computations. Wilkinson 
was born in Strood, England, and studied at Trinity College, Cambridge, where he graduated 
as "Senior Wrangler," the top mathematics undergraduate at Cam- 
: 
on 
bridge. He began doing ballistics work in 1940, but transferred in 
1946 to the National Physical Laboratory (the national measure- 
ment standards laboratory for the UK) at Teddington, England, 
where he worked with Alan Turing developing the ACE computer, 
one of the first stored-program machines. This took him into numer- 
ical work, where he became internationally known for his analysis 
and development of matrix algorithms. In 1970 he received the Tur- 
ing Award (one of highest honors in computer science) for this work 
along with his development of backward error analysis, and in 1974 
he was elected as a Distinguished Fellow of the British Computer 
Society. Wilkinson's text The Algebraic Eigenvalue Problem, Oxford 
JAMES H. WILKINSON 
University Press, 1965, ISBN:978-0198534181, became a primary reference for computational 
scientists, and, along with its revisions, it set the standard for which all other works in the 
area were measured against. To this day it is still regarded as being one the best treatises on 
matrix computations ever written. 

6.4 QR Iterations 
787 
QR is to find an orthogonal matrix P;, such that [Px]x1 = {[Qx]«1 (up to signs) 
and P{'T;,P, is also tridiagonal. Specializing (6.4.11) on page 784 yields 
(dy — a)/v 
C1 
x1/Vv 
$1 
Ley etgese 
X 
= 
: , 
Where 
v= ,/(di—a)?+2?. 
(6.4.15) 
0 
0 
Everything now parallels the steps described on page 784. For a 5 x 5 example, 
Ci 
or. 
 b 
eS 
s 
c 
& 
let TO) = T, and P; = 
' 
fe, 
: 
so that [PiJs1 = [Qxlx1 and 
1 
Ci 
—— Osh 
~ 
81 
Ci 
~ 
Py 
1 
' 
so that [Pi].1 = [Qxla1 and 
1 
es 
Ke) 
0) 
(0) 
~ 
ae 
cs 
ees 
Re 
Og) 
PPTOP, = 
&) 
* 
* 
* 
0 
a T?) 
One 
OM 
+e 
are 
Oe 
Oi 
Ome 
vanes 
in which @ can be nonzero. The strategy is the same as before—restore T?) to 
tridiagonal form by chasing out the unwanted nonzeros with the same elementary 
rotations used in non-symmetric implicit QR iterations 
1 
il 
1 
12s) = 
82 
C2 
Po = 
csae—83 
P= 
1 
5 
1 
83 
C3 
Ca 
—84 
1 
1 
84 
C4 
but now the zero chasing occurs in positions above as well as below the secondary 
diagonals as illustrated below. 
Pe 
ae 
1) 
(Ye 
40 
a 
ee 
Oy 
9) 
0) 
T® =PTT@P,=(0 
+ +» 
0) 
TY =PTTOP,=[0 ++ 
+ @ 
ONG) 
ere 
0 
0 
* 
»* 
* 
OR 
OR 
Oa 
eee 
AO) 
ke 
ks 
mp) — PTTMP, a4 
(ayteeyte) 
oe 
a5 oo 
x* 
* 
* 
Oe 
4S 
ie 
et 
eo) 
Sh) Ee Sd 
LS) 
) 
e 
As shown in (6.4.12) on page 785, the first column in the orthogonal matrix 
pee P,P2P3P.4 agrees with that in Q, (up to sign), so the implicit 
Q-Theorem implies that Ty41 = To) = P?T,P, (possibly adjusting for 

788 
Chapter 6 
Eigenvalue Continuity and Computations 
signs). In general these iterates converge to a diagonal matrix containing the 
eigenvalues of A. The eigenvectors are the columns in the orthogonal matrix 
formed by accumulating products of the various orthogonal matrices used in 
the process. However, one might not want to wait for all off-diagonal entries 
a; in (6.4.14) to converge to zero because when the Wilkinson shift is used, 
the bottom entry «,_1 rapidly goes to zero, usually cubically, leaving d, to 
be an eigenvalue for A before the other entries have converged. Thereafter, 
the problem can be deflated to work on smaller matrices. 
Implicit Double Shift QR 
A big advantage of the double shift strategy on page 782 is that when complex 
eigenvalues exist, they can be determined using only real arithmetic. This will 
follow from the fact that Q,Q,41 
(and hence Hz+2) in (6.4.8) on page 782 
must be real. To see why, let Q = Q,Q;41, which is an orthogonal matrix, and 
R = R,z4iR¢, which is upper triangular with positive diagonal entries, and set 
K = (Hy — a¢ I)(Hy — 6x1) = H? — (ax + Be) 
He + (on Be)I1, 
(6.4.16) 
where a, and {, are the eigenvalues of the 2 x 2 matrix H, in the lower 
right-hand corner of H, in (6.4.3) on page 781. 
Note that K is real because (ay +x) = trace (Hx) and az, = det (H;.). 
Now, K = QR is the QR factorization of K because (6.4.7) on page 782 implies 
A,Q, = QeHe41 = 
AeQe — Qe 
Be = QeHet1 — Qe 
Fx 
=> 
(Ag — 8 DQ = Qx(Hey1 — Be 1). 
Coupled with the fact that H, — a, I commutes with H;, — §,I1 yields 
K = (Hy — 8¢1)(Hy — axl) = (Ay — Be 1)Q,Ry 
(6.4.17) 
= Qk (Hay — Ge DRe = Qe Qe 41Re4i1 
Ri, = QR. 
Since K is real and nonsingular (assuming a, ¢ o (Hz)), its QR factorization 
is uniquely defined and real, and thus Q = Q;,Q,;41 is real. 
The implicit Q-Theorem can again be brought to bear to determine Hy+2 in 
(6.4.8) by finding an orthogonal matrix P, such that P{H,P, is an unreduced 
upper Hessenberg matrix, where [Px].1 = Qui. Use the same logic employed 
for the implicit single shift method. Let Hy = [hij] and R = [ri;], and equate 
the first columns on each side of K = QR to obtain 
ha, + hishea — (ag 
Br)hi1 
+ arBr 
haihii + heghei — (ap + Be)haor 
h32he21 
K 
Kyi = 
0 
=Quri > Q.1.=4——*-= 
Kill 
ono 
oa 
2 

6.4 QR Iterations 
789 
Consider a 6 x 6 example in which we set 
* 
Ok 
Ok 
Ok 
OK 
Ok 
* 
ok 
Ok 
OK 
OK 
OK 
(Ghat 
= 
Oh 
ke 
Re 
kT 
H 
= Hk = 
OMMO 
Vetch? 
FP 
Onn 
OF 
ree 
kee 
OR 
Om 
OM 
Oke 
ak 
Reduce H") with orthogonal similarity transformations built from elemen- 
tary reflectors (Householder transformations) as illustrated below. Let R; be 
a 3x3 reflector whose first column is +(4) (see pages 105-106) and set 
Cc 
PT = CE "a =P; so that [Pi].1 = Qu: and 
* 
* 
* 
* 
* 
* 
* 
* 
o* 
a 
* 
* 
H®? =P,HOp,=|©@ 
* 
, . . . |» 
Where @ is possibly nonzero. 
Fah oats 
* 
* 
* 
0 
Oe 
OR 
OE 
cman 
This has introduced a "bulge" rather than just one nonzero as was the case in the 
first step of implicit QR. But the game is the same except that instead of chasing 
just one nonzero out of the array, the "bulge" needs to be chased out. Next let 
Rz bea 3x 3 reflector operating on rows 2, 3, and 4 that will annihilate the 
(3,1) and (4,1) entries as described in the section on Householder reduction on 
ee 
'he 
Me) 
& 
page 671, and set P? = (< 
R2 
) 
=P» so that 
ie) 
2 
* 
* 
oe 
* 
OX 
* 
Ok 
k 
ok 
OK 
Ox 
H®) = P,H?P, = 
4 ® * = * = 1, 
where @ is possibly nonzero. 
0) 
(Cy MOS 
OR 
Oe 
Ome 
Oke 
c 
eee 
Thus the bulge has been moved down and to the right. The process similarly 
continues by using another 3 x 3 reflector R3 operating on rows 3, 4, and 5 
i 
Tgekouno 
iS 
to annihilate the (4,2) and (5,2) entries. If P47' = (3 Re | 
= P3, then 
H@ = P,H©P,; = 
i 5. 
ee a | (bulge again moves down and right). 
00 
@® 
* 
* 
* 
o 
0) 
©) © 
2 
& 
fp. = es a = P, in which R4 is the 3 x 3 reflector that annihilates 
the (5,3) and (6,3) entries when acting on rows 4, 5, and 6, then 
H® = P,P, = 
366 
ae 
ee 
oe Ey 
eS 
Rt 
@)* 
* 
* 
* 
* 
Scorn 
ee Oo 
Ol 
es 
xa oO 
x 
* 
*¥ 
¥ 

790 
Chapter 6 
Eigenvalue Continuity and Computations 
Now the bulge consists of a single entry. It is removed with one plane rotation 
oe 
iy 
6 
ap 
PF={ 
0 
ec s } to produce 
07 
—s" 
"Cc 
oe 
Oe 
me | he 
onc 
kate 
aee 
ook 
6 
TT 
5p. 
_ 
Ono 
tee 
ewe 
Hie Prt Pie 
ee 
ee 
ek 
ye 
Och 
O. 
Oe eet 
0 02°00 
wears 
Fo 
Thus P, = P;P2P3P4P; is an orthogonal matrix such that 
P7H,P;, = H® 
is an upper-Hessenberg matrix (assumed unreduced—otherwise the problem is 
deflated) and [PxJ.1 = Qui. 
In general this process yields an orthogonal matrix P; such that P}H;P, 
is in Hessenberg form, where the first column in P;, agrees with the first column 
in Q=Q;Qz41 (up to sign), so the implicit Q-Theorem guarantees that 
Py =Q=Q;.Qe41 
is the matrix (up to the signs) required at step (6.4.8) on page 782. The comments 
surrounding (6.4.13) on page 786 regarding the handling of column signs apply 
here as well. 
e 
In summary, the matrix K in (6.4.16) on page 788 and its factorization 
is never explicitly computed, and, generally speaking, only a few iterations 
Hey = PoH.es (exclusively with real arithmetic) are needed to produce 
a small entry in the (n — 1,n — 2)-position as illustrated in (6.4.3) on page 
781. Unfortunately, there are some pathological cases for which convergence 
cannot be obtained—see Exercise 6.4.3 on page 795. But when they converge, 
the iterates exhibit asymptotic quadratic convergence. 
Computing the Eigenvectors 
If A € R"*" is not symmetric, then the QR iteration yields a sequence of or- 
thogonal matrices, the product of which is an orthogonal matrix P such that 
PAP = R, where R is upper triangular with the eigenvalues of A as di- 
agonal entries so that A is similar to R. While similar matrices need not 
have the same eigenvectors (page 304), it is true that Rx = Ax if and only 
if A(Px) = A(Px). Consequently, the problem boils down to accumulating P, 
computing the eigenvectors of R (by inverse iteration (page 774) or by solvy- 
ing homogeneous triangular systems (R — AI)x = 0), and then multiplying the 
results by P. 

6.4 QR Iterations 
791 
Computing A Singular Value Decomposition 
Now consider applying QR iterations to compute a singular value decomposition 
of A €R"*". The narrative is in terms of square matrices, but this is only for 
convenience of exposition—all statements apply equally as well to rectangular 
matrices with only minor (and rather obvious) modifications. 
Since the nonzero singular values of A are the positive square roots of 
the eigenvalues of A? A (or AA7), and since these are symmetric matrices, 
it is tempting to simply apply the implicit symmetric QR iteration (page 786) 
to either of these matrices. But this is not usually the practice because explic- 
itly forming either of these products squares the two-norm condition number 
of A (Exercise 3.5.21, page 380), which can result in a loss of significant in- 
formation (Exercise 2.8.8, page 246). This problem is circumvented by applying 
ingenious ideas introduced by G. H. Golub' and W. M. Kahan' that hinge on us- 
ing elementary reflectors (Householder transformations) to produce orthogonal 
Gene Howard Golub (1932-2007) was the Fletcher Jones Professor of Computer Science at 
Stanford University and one of the best known numerical analysts of his era. He became fa- 
mous for creating, analyzing, and programming SVD algorithms, and he was the catalyst for 
the development of a vast array of applications of the SVD. The Golub—Kahan algorithm first 
appeared in 1965, and it was widely recognized as being the most effective way of computing 
a SVD. In 1970 Golub developed an implementation of the algorithm in conjunction with the 
German mathematician Christian Reinsch (1934-2022) that became the standard in commer- 
cial software. Golub was affectionately known to his students as "Prof 
SVD," which was the designation on his personalized California license 
plate. 
A Chicago native, he received his PhD from the University of IIli- 
nois at Urbana-Champaign in 1959 under Abraham Taub, writing on the 
use of Chebyshev polynomials in iterative solutions of linear equations 
and their relation to the method of successive over relaxation (page 629), 
a hot topic at the time. In 1979 he collaborated with Charles F. Van Loan 
from Cornell University, which led to four editions of Matric Computa- 
tions, Johns Hopkins Studies in the Mathematical Sciences, 1983, 1989, 
1996, 2013, ISBN: 978-1421407944, that became the "go-to" reference for 
the subject. Golub's awards and achievements included the B. Bolzano 
Gold Medal for Merits in the Field of Mathematical Sciences, elections to the National Academy 
of Sciences, the National Academy of Engineering, the American Academy of Arts and Sci- 
ences, and the Royal Swedish Academy of Engineering Sciences, and he held 11 honorary 
doctorates. He was president of SIAM from 1985 to 1987, and he was a founding editor of 
the SIAM Journal on Scientific Computing (SISC) and the SIAM Journal on Algebraic and 
Discrete Methods (SIAD), which became the SIAM Journal on Matrix Analysis and Appli- 
cations (SIMAX). With his large physical stature, booming voice, and personality to match, 
Gene Golub dominated any room or social gathering. He was a gregarious individual who cul- 
tivated collaborators, producing more than thirty PhD students, many of whom went on to 
have distinguished careers achieving fame in their own right, and he had somewhere on the 
order of 250 coauthors. 
GENE H. GOLUB 
William Morton Kahan (1933-), aka "Velvel," was trained in mathemat- 
ics at the University of Toronto. He later became associated with the 
departments of mathematics, computer science, and electrical engineer- 
ing at the University of California, Berkeley. In 1989 he was given the 
Turing Award, one of highest prizes in the area of computer science, for 
his contributions to numerical analysis. In addition to his mathematical 
work, he was instrumental in setting the IEEE standards for floating- 
point computations used in all modern computers. 
W. M. KAHAN 

792 
Chapter 6 
Eigenvalue Continuity and Computations 
matrices P? and Q such that P7? AQ is in upper bidiagonal form 
bit 
bie 
b 
b 
P7AQ = 
edugable 
tatodubtcne: 
(6.4.18) 
It can be assumed that the diagonal and super-diagonal entries in B are nonzero 
(see page 794). Bidigonalization involves alternately applying reflectors PT to 
the left-hand side of A followed by Q; on the right-hand side such that PY? 
zeros out everything below the (1,1) position and then Q; zeros out everything 
to the right of the (1,2) position; P4 annihilates everything below the (2,2) 
position and Q» then annihilates everything to the right of the (2,3) position; 
and so on as illustrated below for a generic 4 x 4 array. 
Pe 
RR 
ee 
OO 
A he 
ae 
() eee 
ee 
er 
— 
O 
« 
* 
* 
ee 
O 
*« 
* 
* 
Ol 
Feo 
ae 
xx 
OO) 
* 
* 
0 
0 
'Ee 
-¥ 
Oe 
ae 
= 
Oe 
eed 
0 
P23 A2Q2=|[5 
9 
« 
» 
|Q2= (| 
O 
« «|= AB 
(6.4.19) 
OORT 
ee 
x 
(I 
besa), 
Sie" 
kone 
Oe 
(0 
24 
00 
GH 
s 
ae 
aera 
= 
(eT 
8) 
Se 
= 
P3 A3Q3 = 
0 
0 
* 
x 
a= (i 
O 
« 
x 
=A,=B 
OF 
OS Ox 
OO" 
O"# 
Thus 
PTAQ=B, 
where P?=PZPTPT 
and 
Q=Q:Q.Q3. 
After reducing A to B, the strategy is to find orthogonal matrices U? and 
V such that U7BV = & (or equivalently, 
B = UEV") is a singular value 
decomposition for B. Then, 
(U7P7)A(QV)=Z 
sothat 
A= (PU)E(V7Q7) 
(6.4.20) 
is a singular value decomposition for A in which the columns of (PU) and 
(QV) are the respective left-hand and right-hand singular vectors. 
The nonzero singular values of B (and A) in ¥ are the positive square 
roots of the eigenvalues of the symmetric tridiagonal matrix T = B'B, so 
consider one step of implicit symmetric QR with a Wilkinson shift a, (page 
786) to move from 
b7, 
biibi2 
A 
biibi2 
Bae + b?, 
b22be3 
Ty = B, By = 
boabeg 
©6034 O25 
dagbaa 
(6.4.21) 

6.4 QR Iterations 
793 
to the next iterate T,41. This requires that T, is in unreduced Hessenberg 
form. In other words, biibi(i41) AO for each i = 1,2,...,n2—1, which means 
bi AO and byi41) 
#0 for each i = 1,2,...,n—1. 
It is assumed that this 
holds. Moreover, it is assumed that each b;; remains bounded away from zero 
as the iteration proceeds. If the problem is defective in these regards, then it 
is decomposed as described on page 794. Implicit symmetric QR requires an 
orthogonal matrix P, such that 
[Px]x1 = [Qx]x1 
(up to sign), 
where Q, is the Q-factor in the QR factorization T;, — a,I = Q;,R, so that 
Tri1 = Q7T,Q; = P?T,P; = P?B7B,P,. 
(6.4.22) 
It follows from (6.4.15) on page 787 that 
(b14 = a)/v 
Ci, 
bi1b22/v 
31 
[Qk] «1 = ae 
: 
= 
: 
where 
v= 
(611 — a)? + (611622)? : 
0 
0 
The matrix P, can be determined by another bidiagonalization reduction. To 
see how, consider a 4 x 4 example. Let T, = BiBx, where 
* 
ie 
Cy) 
= 33 
B= f 
) 
and set 
V,= € ars 
0 
1 
so that Vi is an orthogonal matrix such that 
oOo 
x* 
* fey 
hs 
(eS) ee 
Se 
* 
[Valet = Qs1 
and 
BiVi = (e 
0 
% OS 
) 
where @ may be nonzero. 
oo 
* 
*¥ Ox 
*% 
1) 
Now restore ByVi to bidiagonal form by chasing out unwanted nonzeros with 
alternate pre- and post-multiplications by respective elementary rotations Ur 
(yest, 203)" and V; (i = 2,3), where Ur annihilates the (2, 1)-entry; V2 
annihilates the (1,3)-entry; UZ annihilates the (3,2)-entry; V3 annihilates 
the (2,4)-entry; and finally, Ur annihilates the (4,3)-entry. For example, Uf 
is a rotation in the (1,2)-plane, V2 is a rotation in the (2,3)-plane, etc. This 
process is illustrated below. 
2 
ee) 
A 
7 
Vet 
¥) 
ek 
OO 
Web .vie | 
4 
urpviv. 
= (3 
tae 3 
wy 
0) 
0 
* 
0 
0 
(0) 
23 
+e 
xe 
0 
0 
Pek 
OO) 
CPEPBN Ve = ({ : 2) UUPBAV.VaVs = (5 

794 
Chapter 6 
Eigenvalue Continuity and Computations 
«040 
UPOTOTBLV, VV. = ( = 7 une 
GOs 
ooo 
* 
Consequently, 
aie 
sa 
hes 
UP = UF U2 UT 
and "Vi = Vi¥2V3 
(6.4.23) 
are orthogonal matrices such that ULB Vi = Bx+1 is bidiagonal, and 
VIT.Vi = ViBLBRV« = (VE BrUx)(UE 
Be Ve) = Bey Brett 
is tridiagonal with 
[V].1 = [Viaje = Q.; (because no rotation Vo, V3,... 
affects the first column of Vj, just as on page 787). Therefore V, is the matrix 
P;, in (6.4.22) needed to make one step of the implicit symmetric QR iteration 
Tree V, Pav 
Qf Te Qe: 
Consequently, these iterates converge to a diagonal matrix containing the eigen- 
values of T = B'B. This means that bij41)bi 
20 (6 =1,2,...,2—1) in 
(6.4.21). But the b,;'s are nonzero, so bi;41) + 0 (provided that the 6;;'s stay 
bounded away from zero). In other words, 
o1 
By oo 
'y 
= di, 
sae 
and the respective limits U7 and V of Uz and Vj in (6.4.23) provide the 
decomposition U7BV = SS. Thus a singular value decomposition for A is 
produced as indicated in (6.4.20). 
In practice the last off-diagonal entry b(n~1)n(n—1),(n—1) 
in Ty usually 
converges cubically to zero so that the computation can typically be deflated 
after just a few steps. 
Decomposing Bidiagonal Matrices 
As mentioned on page 792, the SVD algorithm requires that the diagonal and 
super-diagonal entries in the bidiagonal matrix B in (6.4.18) must be nonzero. 
If this is not the case, then B must be decomposed into smaller bidiagonal 
matrices for which this holds. When 6:41) = 0 in B, the problem naturally 
decomposes into 
— 
{fen 
1 
ee 
B=(4 stg where B, is 2 xX 7. 
If b;; 
=0 for 
<n, then the i* row can be zeroed out with left-hand plane 
rotations P;;. For example, in a 5 x 5 bidiagonal matrix with b33 = 0, P34 
followed by P35 zeros out the third row as shown below. 
* 
* 
0 
0 
0 
*« 
* 
0 
0 
0 
3 
0 
= 
* 
0.0 
(Nie 
ale 
en 
0 
Pz5P34B=|[0 
0 0 
« 
0 
|=Po,/ 
0 0 
0 0 
« 
|=] 
° 
(6.4.24) 
ODROST 
Oe 
Or 
On 
On 
se 
ace 
0 
OOO 
Os 
Ole 
(Oy 
Xow 
(0). 
@) 
oe 
A 

6.4 QR Iterations 
795 
When a bottom diagonal entry 6, is zero such as in the top half of (6.4.24), 
applying plane rotations P(n—1),n1 P(n—2),n)«++)Pin on the right zeros out the 
last column. For example, in the top half of (6.4.24), 
x 
# 
90 
* 
* 
* 
*« 
ox 
2} 
O 
* 
x 
Po3P 13 == 
0 
«x 
O 
P43 = 
Okemo 
le 
0 
0 
0 
ORO 
0 
ORO 
0 
Thus the 5 x 5 matrix in this example decomposes as shown below. 
Exercises for section 6.4 
6.4.1. Prove that if \ € 
o(H), where H € F""" is in unreduced Hessenberg 
form, then geo multgy (A) = 1. 
il 
0 
0 
6.4.2. Let 
H= (-1 —2 1). 
0 
By 
1 
(a) Apply the "vanilla" QR iteration to H. 
(b) Apply the single shift QR iteration on H. 
6.4.3. Show that the QR iteration can fail to converge using 
oO 
i 
il 
* 
© 
Hee 0 0) or 
H= (1 1 ). 
@ 
k 
© 
@ 
a 
a 
(a) First use the "vanilla" QR iteration on H to see what happens. 
(b) Now try the single shift QR iteration on H. 
(c) Finally, execute the double shift QR iteration on H. 
6.4.4. State and prove a version of the implicit Q-Theorem on page 783 that 
holds for complex matrices. 

796 
Chapter 6 
Eigenvalue Continuity and Computations 
6.4.5. Rutishauser's LR Iteration. Let 
A = A; = L,R,; € R"*" be the LU fac- 
6.4.6. 
torization (R is used here in place of U because that is what Rutishauser 
(page 779) used), and consider the sequence A, obtained by alternately 
performing LU factorizations (assuming they exist) and then reversing 
the order as shown below. 
Ay = L,R; 
and 
A» = RL, 
A» = LoRe 
and 
A3 = Rolo 
Az = L3R3 
and 
Ay = R3L3 
Axii = RL, 
ete. 
(a) Show that each A, is similar to A to conclude that each iterate 
has the same eigenvalues as A. 
(b) Suppose that T;, = L,L2---L, converges (say lim, T;, = T), 
and suppose that A, converges (say lim, A, = U). Prove 
that lim, R, = U is an upper triangular matrix having the 
eigenvalues for A = A, as diagonal entries. 
Let A = (eee ee "t Use pencil-and-paper computation with 4-digit 
floating-point arithmetic (or use a hand-held calculator) to perform a 
few steps of Rutishauser's basic LR iteration as described in Exercise 
6.4.5 to observe how the eigenvalues of A are revealed. Corroborate 
your observations by factoring the characteristic equation for A. 
It is easier to write ten volumes on theoretical 
principles than to put one into practice. 
— Leo Tolstoy (1828-1910) 

CHAPTER 
i 
Perron—Frobenius 
and 
Nonnegative Matrices 
7.1 
INTRODUCTION 
A € R™*" is said to be a nonnegative matrix whenever each aij = 0, and this 
is denoted by writing A > 0. In general, 
A > B means that each a,; > };;. 
Similarly, A is a positive matrix when each a;; > 0, and this is denoted by 
writing A > 0. More generally, 
A > B means that each a;; > b;;. Applications 
abound with nonnegative and positive matrices. In fact, many of the applications 
considered earlier in this text involve nonnegative matrices. For example, the 
connectivity matrix C for airline routes in (1.8.2) on page 70 is nonnegative. If 
L is discrete Laplacian in (3.6.10) on page 390, then (4I—L) > 0. The matrix 
eA? in (4.10.32) on page 611 that defines the solution of the system of differential 
equations in the mixing application on page 610 is nonnegative for all t > 0. 
And the system of difference equations in (4.11.46) on page 637 that describes 
the shell game on page 636 has a nonnegative coefficient matrix. 
Since nonnegative matrices are pervasive, it's natural to investigate their 
properties, and that is the purpose of this chapter. A primary issue concerns 
the extent to which the properties A > 0 or A > O translate to spectral 
properties—e.g., to what extent does A have positive (or nonnegative) eigenval- 
ues or eigenvectors? The study of these questions is called the "Perron—Frobenius 
theory" because it evolved from the contributions of the German mathematicians 

798 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
Oskar (or Oscar) Perron' and Ferdinand Georg Frobenius.' Perron published his 
treatment of positive matrices in 1907, and in 1912 Frobenius contributed sub- 
stantial extensions of Perron's results to cover the case of nonnegative matrices. 
In addition to saying something useful, the Perron—Frobenius theory is ele- 
gant. It is a testament to the fact that "beautiful mathematics eventually tends 
to be useful, and useful mathematics eventually tends to be beautiful." 
Oskar Perron (1880-1975) originally set out to fulfill his father's wishes to be in the fam- 
ily banking business, so he only studied mathematics in his spare time. But he was eventu- 
ally captured by the subject, and, after studying at Berlin, Tiibingen, 
and Gottingen, he completed his doctorate, writing on geometry, at 
the University of Munich under the direction of Carl von Lindemann 
(1852-1939) (who first proved that 7 was transcendental). Upon grad- 
uation in 1906, Perron held positions at Munich, Tubingen, and Hei- 
delberg. Perron's career was interrupted in 1915 by World War I in 
which he earned the Iron Cross. After the war he resumed work at 
Heidelberg, but in 1922 he returned to Munich to accept a chair in 
mathematics, a position he occupied for the rest of his career. In ad- 
dition to his contributions to matrix theory, Perron's work covered a 
wide range of other topics in algebra, analysis, differential equations, 
number theory, geometry, and continued fractions. He was a man of 
extraordinary mental and physical energy. In addition to being able to 
climb mountains until he was in his mid seventies, Perron continued 
to teach at Munich until he was 80 (although he formally retired at 
age 71), and he maintained a remarkably energetic research program 
into his nineties. He published 18 of his 218 papers after he was 84. 
OsKAR PERRON 
Ferdinand Georg Frobenius (1849-1917) earned his doctorate under the supervision of Karl 
Weierstrass (page 585) at the University of Berlin in 1870. As mentioned earlier, Frobenius was 
a mentor to and a collaborator with Issai Schur (page 167), and, in ad- 
dition to their joint work in group theory, they were among the first to 
study matrix theory as a discipline unto itself. Frobenius in particular 
must be considered along with Cayley and Sylvester when thinking of 
core developers of matrix theory. However, in the beginning, Frobe- 
nius's motivation came from Kronecker (page 227) and Weierstrass, 
and he seemed oblivious to Cayley's work (page 116). It was not until 
1896 that Frobenius became aware of Cayley's 1857 work, 
A Memoir 
on the Theory of Matrices, and only then did the terminology "ma- 
trix" appear in Frobenius's work. Even though Frobenius was the first 
to give a rigorous proof of the Cayley-Hamilton theorem (page 312), 
he generously attributed it to Cayley in spite of the fact that Cayley 
had only discussed the result for 2 x 2 and 3 x 3 matrices. But credit 
in this regard is not overly missed because Frobenius's extension of 
Perron's results are more substantial, and they alone may keep Frobenius's name alive forever. 
F. G. FROBENIUS 

7.2 Nonnegative Matrices 
799 
7.2 NONNEGATIVE MATRICES 
As mentioned in the introduction, Perron established his results for positive ma- 
trices in 1907, and Frobenius later extended and augmented them to include 
nonnegative matrices in 1912. However, the presentation in subsequent sections 
does not follow this chronology of events nor does it utilize most of the original 
arguments of Perron and Frobenius. As a consequence of being polished and 
streamlined by many contributors over many years, a smoother and more con- 
tinuous development is realized by first focusing on matrices Ay,yxn > 0 with 
nonnegative entries and examining the extent to which nonnegativity is inherited 
by eigenvalues and eigenvectors of A. Before proceeding, it is necessary to make 
the following simple observations concerning positive and nonnegative matrices. 
Preliminary Observations 
ee Ue ten 
7 
Ogee Ee 
Weezah 
N>0, 
u>v>0 
= Nu>Nv 
Ci2 
2) 
N? 052 > 0 Nz=0"=>" N=0 
(7.2.3) 
Notations, Conventions, and Facts 
For Anxn > 0, the following notations and conventions are used throughout 
the development of the Perron—Frobenius theory. 
e 
r=p(A) _ (the spectral radius—see page 297). 
eo 
N ={x eR? ix > 0 with x + 0}. 
e 
P={xeR"|x> 
0}. 
e 
R(é)=(€1—A)7! for 
€#r 
(the resolvent matrix—see page 605). 
e 
If 
€>r, then R(é)= z 
a (=) >0O 
(see page 622). 
(7.2.4) 
k=0 
e 
|A| denotes the matrix of absolute values—i.e., |A|ij = |aij|- 
e 
When r # 0, there is no loss of generality in assuming that 
r =1 because A can be replaced with its normalization A/r, 
and p(A/r) = 1< > p(A) =r. This assumption is generally 
used throughout the presentation in this chapter. 
(7.2.5) 
e 
If 
AGCR"*" and 
Be C"*" are such that |B| < A, then 
p(B) <p(A) 
(this is Corollary 4.11.6 on page 621). 
(72:6) 

800 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
A Nonnegative Eigenpair 
Our development begins by proving that every nonnegative matrix Anxn has a 
nonnegative eigenpair in which the nonnegative eigenvalue is in fact the spectral 
radius of A. This is the most general (and difficult *) facet of Perron—Frobenius 
theory. Once this is established all other parts fall neatly into place. 
7.2.1. Theorem. (Frobenius 1912) For Ann >0 with r=p(A), _ 
e réo(A), 
(r is called the Perron root of A). 
goes, 
-e Ax=rx for some x EW. 
: 
oe 
Se 
ears) 
When such an eigenvector x is normalized so that ||x||, = 1, it is called 
a Perron vector for A. 
Proof. 
Assume A # 0—otherwise there is nothing to prove. If r = 0, then 
A is nilpotent (consider the Jordan form), so 
¢(A) = {0}, and A* = 0 but 
A*-! 40 
for some k. Hence A(A*~!) = 0 says that each nonzero column of 
A*-! is a nonnegative eigenvector for A associated with r =0. For the proof 
of the two statements when r #0, it can be assumed that r = 1—see (7.2.5). 
Proof of (7.2.7). To prove that 
1 € o(A), suppose, to the contrary, that 
1¢o0(A) sothat (I-A)! exists. The nonnegativity of A means that the se- 
quence of partial sums S,, = Bes A' is monotone increasing—i.e., S, < Sp41 
for all integers n > 0. Furthermore, this sequence S,, is bounded above. To see 
this, observe from (7.2.4) that 1 ¢ oa (A) ensures the existence of limg_,1+ €R(€), 
whose value is 
Jim, €R(é) = (I-A) 
and 
E1S > (A/é)' <E1S>(A/E)' =RO 
for all 
€>1. 
4=0 
2=0 
Consequently, 
n 
SY (4) SéR(E) 
=> 
Sa = lim >» 
(4) Se A)~ 
+ 
i=0 
1 
for all integers n > 0. Since bounded monotonic sequences converge, 5 ae AS 
is a convergent series. But this is a contradiction because the Neumann series 
Functional analysts may dispute this by saying the results are a consequence of Brouwer's 
fixed-point theorem saying that a continuous function on R" mapping a compact convex set 
to itself must have a fixed point. However establishing this requires at least as much effort as 
ae applying first principles from matrix theory that avoid hiding the linear algebra under 
the covers. 

' 
7.2 Nonnegative Matrices 
801 
converges if and only if r < 1 (page 618). Therefore, the supposition that 
1 ¢o(A) is false, andthus 
1€o(A). 
& 
Proof of (7.2.8). Let o(A) = {Ay = 1, A2,...,As} in which k; = index(A;), 
and apply the spectral resolution theorem (page 598) to the function 
fzZete(1+e= 2)" 
with e> 0 
to obtain 
a FOG) 
f(A) = 
7 
(A —\,1)'G;, where G, is the i'" spectral projector. 
Since f(z) = jle™(1+e¢—z)~9+, it follows that 
lim f(\;)=0 
fori>2 
(0<j<k—1), 
e>0+ 
lim f(1)=0 forj<kj—1, 
and 
f-(1) =(k—1)!. 
e>0+ 
Consequently, 
lim, f(A) =(A—TD*-'G, = L 
(= means "defined to be"). 
(7.2.9) 
€ 
Observe that, L # O—otherwise R(Gi) 
C N(A-I)"7' c N(A- is 
which contradicts the fact that R(G,) = N(A—TI)". From another point 
of view, 
a Neumann expansion yields 
if 
hy 
CS 
iN 
m 
f(A)= [+e 
A] 
= eRe.) as 
so it can be concluded that 
L40 
and 
L>0O. 
(i210) 
In other words, L has at least one positive entry—say 1,; > 0. Since 
(A—-DL=(A-1D"™G, =0=G,(A-1I)" =L(A -D), 
(7.241) 
it follows that L,; and Lj, are respective nonnegative right-hand and left-hand 
eigenvectors of A associated with A; =l=r. 
& 
Exercises for section 7.2 
7.2.1. Convince yourself that (7.2.1)-(7.2.3) on page 799 are indeed true. 

802 
Chapter 7 
| 
Perron—Frobenius and Nonnegative Matrices 
1.22. 
Tdode 
(err ® 
7.2.0. 
7.2.6. 
sachs 
7.2.8. 
dazed, 
faaeLOa 
Imo= Ayxz,,vond x,y € R"*! are such that 
0 <x <y, explain why 
Ax < Ay. 
Suppose that A > 0 is nonsingular. Explain why it is impossible for 
A-~! to be nonnegative. 
Let the V = [v1 |v2|--- | ve] € R"** have rank (V) =k > 0,-and let 
A=V'V e«R***. Prove that if (v;|vj) = v/v; <0 for i#j, then 
Ao* > 0, 
Monotone matrices. A nonsingular matrix A € R"*" is said to be mono- 
tone if for all x,y 
€ R"*!, Ax > Ay 
=> 
x>y. Prove thata 
nonsingular matrix is monotone if and only if A~! > 0. 
Essentially nonnegative matrices. 
A € R""" is said to be essentially 
nonnegative if each off-diagonal entry is nonnegative. Prove that if A 
is essentially nonnegative, then A has a real eigenvalue A, such that 
Ax > Re(A) 
for all 
4 € 0(A). Give an example to show that », need 
not equal p(A) when A has negative entries on its diagonal. 
I 
Oe 
ets: 
Verify (7.2.7) and (7.2.8) for the matrix A = (0 2 0). 
Cr 
Ine2 
Is it possible for 
0 A Anyn > 0 to have alg mult, (r) > 1? Either give 
a proof or a counterexample. 
Is it possible for 
0 A Anyxn > O to have index(r) > 1? Either give a 
proof or a counterexample. 
(a) 
Isit possible for 
0 4 An 
xn > 0 to have two linearly independent 
eigenvectors that are both nonnegative? Either give a proof or a 
counterexample. 
(b) 
Is it possible for 
0 # Any» > 0 to have n linearly independent 
eigenvectors that are all nonnegative? Either give a proof or a 
counterexample. 
7.2.11. Is it possible for 
0 4 Any» > 0 to have more than one eigenvalue on 
its spectral circle? Either give a proof or a counterexample. 

a 

Proof of (7.3.1). 
This is a corollary of (7.2.7) on page 800. 
& 
Proof of (7.3.2). 
r>0, for otherwise A is nilpotent, which is impossible. 
— In all subsequent proofs it is assumed without loss of generality that 
r = 1 
(see statement (7.2.5) on page 799). 
Proof of (7.3.3). 
If A > 0, then Az > 0 for all 
z € N (by (7.2.1) on 
page 799). The second part of Frobenius's theorem on page 800 guarantees that 
Ax =x 
for some x € NV, so x must be a positive eigenvector. 
Ml 
Proof of (7.3.4). 
Suppose that (A,z) is any eigenpair with z> 0. If (1, y) is 
the pe Oe: for A" guaranteed by (7.3.3) (with A replaced by AT), 
then y'z>0 and y'z=y?Az=)Ay'z = 
\=1. 
OB 
Proof of (7.3.5). 
First observe that k; = inder(1) = 1 because if k, > 2, then 
the matrix L = (A—TI)*1~1G, in (7.2.9) on page 801 would be nilpotent (recall 
(4.10.8) on page 598). This is impossible because (7.2.10) on page 801 says that 
0#L>0. Thus k; = 1, so \; = 1 is a semisimple eigenvalue for ee which 
in turn means that 
alg mult, (1) = geo mult, (1) = rank (Gi), with Gy =L>0, G; £0. (7.3.8) 
Moreover, AG; 
= Gi 
= > G, > 0 (by (7.2.1)). Consequently, G; con- 
tains only one independent column—otherwise there would be two independent 

7.3 Positive Matrices 
805 
positive eigenvectors for 
4 = r = 1, which violates (7.3.4). Therefore, (7.3.8) 
ensures that alg mult, (1)=1. 
J 
Proof of (7.3.6). 
Let x > 0 bea 
positive eigenvector guaranteed by (7.3.3), and 
let p = x/||x||1. Suppose that q > 0 is another eigenvector for \; = 1 such that 
5% =e"q =1 
(E is the vector of all ones). The fact that alg mult, (1) =1 
means dim N(A —I) = 1, so 
q=ap 
forsomea 
=> 
e'q=ae'p = 
a=1. 
O 
Proof of (7.3.7). 
If p is the Perron vector for A, and if 0 < € < min,a,, 
then B = A—eI > O and Bp = (1—e)p, so (7.3.4) ensures that p is the 
Perron vector for B, and hence p(B) =1-—e. Let 
w= (a+i@) €o(A) with 
|u| =1 but uw #1. First observe that 4 # —1. Otherwise, 
6 = —1—e€ €a(B), 
which is impossible because |6| = 1+e€> p(B). If 
w= (a+i8)€a(A) with 
|u| =1 but 
wA+1, then a <1 (draw a circle to see this). Using a <1 with 
1=a7+ 6? yields 
(p(B) |? = (le) 
le 90 el 
ae ey ee 
=(a-€)° +6? 
=|n—¢? => p(B) <|n—€, 
which is impossible because pp — € € 0 (B) (ie., there cannot be a point in the 
spectrum having greater magnitude than the spectral radius). Therefore, r = 1 
is the only eigenvalue on the spectral circle of A. 
& 
Collatz—Wielandt Formulas for Positive Matrices 
The Courant—Fischer theorem on page 425 exhibits variational (or "min-max" 
and "max-min") representations for the eigenvalues of a hermitian matrix. The 
following theorem shows that the Perron root also has such descriptions. 
7.3.2. Theorem. The Perron root for Any.» > 0 is given by 
Ax|; 
AX 
r= min max ae =max min ea where 
x €P. 
(7.3.9) 
xEP1l<i<n 
2; 
XEP teen 
0, 
These are the Collatz—Wielandt formulas' for positive matrices. Ex- 
tensions to nonnegative matrices are on page 817 and in Exercise 7.3.8. 
i Lothar Collatz (1910-1990) introduced these formulas in 1942, and in 
1950 Helmut Wielandt (page 774) used them as the centerpiece for a 
complete development of the Perron—Frobenius theory for both positive 
and nonnegative matrices. Our approach is the reverse of Wielandt's in 
that we first develop the theory without reference to (7.3.9) and then pro- 
duce the Collatz—-Wielandt formulas as corollaries. After his time at TU 
Darmstadt, Collatz moved to the University of Hamburg and founded the 
Institute of Applied Mathematics. He was an internationally recognized 
scholar who had many honors bestowed upon him. He remained active 
. 
up to the end when he unexpectedly died from a heart attack in Varna, 
pee Coes 
Bulgaria, while attending a mathematics conference. 

806 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
Proof. 
For x €P, let 
Ge = mane-[Ax]s/s04 
so that [Ax];/xz; < & for all j, and hence &x > Ax > 0. If q' is the left- 
hand Perron vector for A, then q?x > 0, so, by (7.2.2) on page 799, 
&x>Ax>0 
=> 
é,q) x > q? Ax = rq' x a fF VRE 
Since €, =r, where p is the right-hand Perron vector for A, it follows that 
min{€,} =r, and thus the min-max part in (7.3.9) is proven. Establishing the 
xEP 
max-min part is similar. For x € P, let vx = mini<j<n [Ax];/z; so that 
[inx 
< Ax => pyq?x<q'Ax=rq'x = <r 
VxeEP. 
Since Up =7, it follows that max {Lx } =r. 
Ht 
x 
Exercises for section 7.3 
7.3.1. Verify Perron's theorem by computing eigenvalues and eigenvectors for 
Tae 
A=(1 83). 
i) ioe 
Find the right-hand Perron vector p as well as the left-hand Perron 
vector q!. 
7.3.2. Provide the details that explain why the Perron vector is uniquely de- 
fined. 
7.3.3. Find the Perron root and the Perron vector for A = ig eat 
where a+8=1 with a,8>0. 
7.3.4. Prove that if An, > O and if (A,x) 
is any eigenpair for A such 
that 
|\| = r, where r is the Perron root for A, then A|x| = r|x| 
and |x| > 0. In other words, when normalized, |x| is the Perron vector 
for A. Hint. First show that r|x| < A|x|, and then use an indi- 
rect argument to show that equality holds by considering Ay, where 
y = A|x|—r|x]. 

7.3 Positive Matrices 
7.3.5. 
7.3.6. 
7.3.7. 
7.3.8. 
807 
Suppose that Anxn > 0 has p(A) =r. 
(a) Explain why limp.(A/r)* exists. 
(b) Explain why limz...(A/r)* = G > 0 is the projector onto 
N(A—rI) along R(A —rI). 
(c) Explain why rank (G) = 1. 
Prove that if every row (or column) sum of A,yp, > 0 is equal to p, 
then p(A) = p. 
To show the extent to which the hypothesis of positivity cannot be re- 
laxed in Perron's theorem, construct examples of square matrices A 
such that A > 0, but A #0 
(ie., A has at least one zero entry), 
with r = p(A) € a(A) that demonstrate the validity of the following 
statements. Different examples may be used for the different statements. 
(a) r can be 0. 
) alg mult, (r) can be greater than 1. 
) index(r) can be greater than 1. 
(d) N(A-—rI) need not contain a positive eigenvector. 
) r need not be the only eigenvalue on the spectral circle. 
The Collatz—Wielandt formulas in Theorem 7.3.2 on page 805 use min 
x 
and max. Show that P can be replaced by NV in the max-min formu- 
xe 
lation, but not in the min-max formula. In other words, 
[Ax]; 
; 
[Ax]; 
max min 
=—=j = DUG» 
Mil max 
xEN 2140 
2; 
xEN xi f0 
Xj 

808 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
7.4 PERRON-FROBENIUS THEOREM 
When the results of Theorem 7.2.1 (page 800) for nonnegative matrices are com- 
pared with those of Theorem 7.3.1 (page 804) for positive matrices, a natural 
question arises—at least it was natural to Frobenius. Rather than accepting that 
the major issues concerning spectral properties of nonnegative matrices had been 
settled, Frobenius asked, are there conditions short of strict positivity but more 
specific than general nonnegativity that will provide most of the conclusions in 
the list (7.3.1)—(7.3.7) on page 804? 
Frobenius had the remarkable insight to see that the important properties 
hinged not so much on the distinction between positive entries and zeros, but 
rather on the positions of the zero entries. For example, (7.3.3) and (7.3.5) are 
false for A = @ ag but they are true for A= ¢ "i 
. Frobenius's genius 
was to see the difference between A and A in terms of reducibility and to relate 
reducibility (or the lack of it) to spectral properties of nonnegative matrices. 
Reducibility and graphs are introduced in Exercise 4.2.39 on page 455, but the 
main points are revisited in the following overview. 
Reducible And Irreducible Matrices 
e 
Anxn is said to be a reducible matrix when there exists a permutation ma- 
trix P such that P7AP = ie a where X and Z are both square. 
Otherwise A is called an irreducible matriz. 
e 
PAP iscalled a symmetric permutation of A. The effect is to interchange 
rows in the same way as columns are interchanged—Exercise 7.4.2, page 818. 
e 
The graph G(A) of A is defined to be the directed graph on n nodes 
{N1, No,..-,Nn} in which there is a directed edge (or path) leading from 
N; to N, if and only if aj 40. 
e 
G(P'AP) =G(A) whenever P is a permutation matrix—the effect is sim- 
ply a relabeling of nodes. 
e 
G(A) is called strongly connected if for each pair of nodes (N;, Nz) there is 
a sequence of paths (directed edges) leading from N; to Ng. 
e A is an irreducible matrix if and only if G(A) 
is strongly 
connected—see Exercise 4.2.39 on page 455. 
(7.4.1) 
Example (Strongly Connected Graphs And Irreducible Matrices) 
A= ( 3 is reducible because 
P?' AP = (i :) where P = « OE and, 
as seen from Figure 7.4.1 below, G(A) is not strongly connected because there 
is no sequence of paths (edges) leading from node 1 to node 2. On the other 
hand, A = ( 
' 
is irreducible, and G (A) is strongly connected because each 

7.4 Perron—Frobenius Theorem 
node is accessible from the other. 
G(A) 
G(A) 
FIGURE 7.4.1 
Perron—Frobenius Theorem 
When the zeros in a nonnegative matrix Any are in just the right positions 
to insure irreducibility, Frobenius showed how to salvage all but one of Per- 
ron's properties in Theorem 7.3.1 on page 804. The major parts of the following 
theorem are due to Frobenius, but because his motivation and arguments were 
extensions of Perron's theorem on page 804, this theorem has come to be known 
as the Perron—Frobenius theorem. 
809 
e 
Note. In previous discussions it was convenient to assume that p(A) = 1, 
but, unless otherwise stated, this assumption is no longer used. 
7.4.1. Theorem. 
(Perron—Frobenius Theorem) If A,,..,, > 0 is irre- 
ducible, then each of the following is true. 
r=p(A)e€o(A) and r>0. 
(7.4.2) 
alg mult, (r) =1. 
(7.4.3) 
There exists an eigenvector x > 0 such that Ax = rx. 
(7.4.4) 
There is a unique vector p, called the Perron vector, such that 
Ap =rp, 
p>, 
and 
p=) 
ak 
(7.4.5) 
Except for positive multiples of p, there are no other non- 
(7.4.6) 
negative eigenvectors for A, regardless of the eigenvalue.. 
p(A+E) > p(A) for all 
E > 0 with E # 0. In other 
(7.4.7) 
words, any increase in A forces p(A) to strictly increase. 
The proof of the Perron—Frobenius theorem is given on page 810, but first 
it is helpful to have the following lemma that shows how to use nonnegativity in 
conjunction with irreducibility to produce a positive matrix. 
7.4.2. Lemma. If A,,x.,, > 0 is irreducible, then 
m1 
So anA® >0O 
whenever each a,x > 0. 
(7.4.8) 
k=0 

810 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
Proof. 
Let ayy denote the (i,j)-entry in A*, and observe that 
ait) = 
+ Gihy Ohya? Shyiig > 0 <=> ain, > 0, Gnynz > 0; --+s Ghyiag > 0 
hi, 
hk-1 
for some set of indices hy,h2,...,hx—1. This is equivalent to saying that there 
is a sequence of k edges N; + Np, > Nn, 7 ++: 
+ Nj that connects N; to 
N; in G(A). In other words, 
Gee > 0 <> there is a path from N; to Nj; of length k in G(A). 
(7.4.9) 
Irreducibility insures that G(A) is strongly connected, so for each pair of nodes 
(N;,N;) there is some sequence of k < n paths from N; to Nj, and this 
implies that an > 0 for some 
0<k<n-—1. Thus 
a 
n—-1 
m— 1 
= oa => axal >0 
Vv (i,3), 
k=0 
k=0 
2) 
and (7.4.8) is proven. 
Proof Of The Perron—Frobenius Theorem 
With the aid of Lemma 7.4.2, the Perron—Frobenius properties in (7.4.2)—(7.4.6) 
are proven by appealing to Perron's theorem on page 804. However the order 
in which (7.4.2)—(7.4.6) are established is necessarily different than the order in 
which they are listed on page 809. 
Proof of (7.4.4). 
It is already known from Theorem 7.2.1 on page 800 that 
r€o(A) and Ax=rx for some x EN. If R(é) = (€1— A)7! with €>r7, 
then (7.4.8) together with the Neumann series expansion (7.2.4) on page 799 
yields R(€) > 0. Consequently, 
(€I-A)x = (€-r)x => x=(€-—r)R(€)x>0 
(by (7.2.1), page 799). 
a 
Proof of (7.4.2). 
It is known from (7.2.8) on page 800 that r € o (A) so that 
only the fact that r > 0 requires proof. The irreducibility of A ensures that no 
row of A is entirely zero, so if x > 0 is a positive eigenvector vector guaranteed 
by (7.4.4) on page 809, then rx = Ax > 0, and this implies that 
r>0. 
& 
Proof of (7.4.3). 
If 
A€¢o(A), then, (€—A)~! € a (R(€)) (see Exercise 4.10.8 
on page 613). Moreover, A; # Aj = > (€ — Ai)71 A (€—A;)~! means that 
alg mult, (A) = alg multp(e, ((€—A)~") 
for each \ € 0 (A). 
(7.4.10) 

7.4 Perron—Frobenius Theorem 
811 
Choosing € so that 0 <€—r < |€—2| for 
r #4 \ € o(A) guarantees that 
p(R(é)) = (€—r)-1, and, as observed above, R(€) > 0, so (7.3.5) on page 804 
together with (7.4.10) yields 
1 = alg multpe) ((€—r)~*) = alg mult, (r). & 
Proof of (7.4.6). 
The proof is the same as that for (7.3.4) on page 804. 
Proof of (7.4.7). 
If A is irreducible, then so is A+E for all E > 0. Let 
r=p(A) and 
s=p(A+E). 
If x>0 
is the Perron vector for 
A+E, and if 
q > 0 is the Perron vector for A', then 
(s —r)q'?x = q" (sx) — (rq')x = q? 
(A+ E)x — q? Ax = q' Ex > 0. 
Since q'x > 0, it follows that s —r = (q?Ex)/q?x>0, andthus 
s>r. 
UW 
The following two corollaries of the Perron—Frobenius theorem are useful in 
subsequent developments. 
7.4.3. Corollary. If Any, > 0 is irreducible with r = p(A), and if 
zEWN issuch that Az >rz, then Az=rz, and z>0. 
Proof. 
Suppose that rz 4 Az. If q > 0 is the Perron vector for A', then 
(A —rI)z>0Oand (A-rl)z40 = 
q'(A-rI)z>0. 
But this is impossible because q?(A —rI) = 0. Thus rz = Az. Consequently, 
z must be a multiple of the Perron vector for A by (7.4.6) on page 809, so it 
follows that 
z>0. 
& 
7.4.4. Corollary. 
If A, > 0 is irreducible with r = p(A), and if 
G is the spectral projector associated with r (i.e., the projector onto 
N(A—rI) along 
R(A—rT)), then G > 0. In fact, 
de 
Gees 
2 0, 
q'p 
where p and q are the respective Perron vectors for A and A 
Proof. 
If A > 0 is irreducible, then so is A'. Consequently, part (7.4.5) 
of the Perron—Frobenius theorem on page 809 says that the respective Perron 
vectors p and q for A and A' are both positive. Furthermore, part (7.4.3) 
on page 809 says that r is a simple eigenvalue of A (and A'), so formula 
(4.7.18) on page 550 for the spectral projector of a simple eigenvalue produces 
G=pq'/q'p>o. 
= 

812 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
Example (Leontief's input-output Economic Model) 
In the 1930's Wassily Leontief 's' interest in how changes in one economic sector 
influence other sectors led him to formulate this input-output economic model. 
Suppose that n major industries in a closed economic system each make one 
commodity, and let a J-unit be what industry J produces that sells for $1. For 
example, the Boeing Company makes airplanes, and the Champion Company 
makes rivets, so 
a BOEING-unit is only a tiny fraction of an airplane, but a 
CHAMPION-unit might only be a few rivets. If 
0 < s; = # J-units produced by industry J each year, and if 
0 < aj; = # F-units needed to produce one J-unit, 
then 
ai;8; = # I-units consumed by industry J each year, and 
nr 
ye ai38; = # I-units consumed by all industries each year, 
7=0 
so 
n 
dj; = 3 — ye ai;8; = # I-units available to the public (nonindustry) each year. 
j=1 
Consider d = (dj, do, ..., dn)' to be the public demand vector, and think of 
S = (81, 82,---, 8)? as the industrial supply vector. 
Problem: Determine the supply 
s > 0 that is required to satisfy a given 
demand d > 0. 
Solution: At first glance the problem seems to be trivial because the equations 
d; = 8 — pee ajj;S; translate to (I—-A)s =d, so if 
I—A 
is nonsingular, then 
Wassily Leontief (1906-1999) was the 1973 Nobel Laureate in Economics. He was born in St. 
Petersburg (now Leningrad), where his father was a professor of economics. After receiving 
his undergraduate degree in economics at the University of Leningrad in 1925, Leontief went 
to the University of Berlin to earn a Ph.D. degree. He migrated to New York in 1931 and 
moved to Harvard University in 1932, where he became Professor of 
Economics in 1946. Leontief spent a significant portion of his career 
developing and applying input-output analysis that eventually rev- 
olutionized business planning and became a permanent tool used by 
governments and private industries around the world. One of Leon- 
tief's secret weapons was the computer. He made use of large-scale 
computing techniques (relative to the technology of the 1940s and 
1950s), and he was among the first to put the Mark I (one of the 
first electronic computers) to work in 1943 on nonmilitary projects. 
He is quoted as having said, "From my studies, I came to the con- 
clusion that you have to use logic and mathematics and quantitative 
science together. You have to develop the interplay between theory 
and empirical analysis. One cannot only postulate and hypothesize. 
One must roll up ones sleeves and do the dirty work too." 
WASSILY LEONTIEF 

7.4 Perron—Frobenius Theorem 
813 
s = (I—A)~!d. The catch is that this solution may have negative components in 
spite of the fact that A > 0. So something must be added. It's not unreasonable 
to assume that major industries are strongly connected in the sense that the 
commodity of each industry is either directly or indirectly needed to produce 
all commodities in the system. In other words, it's reasonable to assume that 
G(A) is a strongly connected graph so that in addition to being nonnegative, 
A is an trreducible matrix. Furthermore, it's not unreasonable to assume that 
p(A) <1. To understand why, notice that the j'" column sum of A is 
= total number of all units required to make one J-unit 
a? ] 
g Ss | 
= total number of dollars spent by J to create $1 of revenue. 
In a healthy economy all major industries should have c; <1, and there should 
be at least one major industry such that c; <1. This means that there exists a 
matrix E>0, but 
E+0, such that each column sum of 
A+E 
is 1, so 
e'(A+E)=e!, 
where 
e? is the row of all 1's. 
This forces p(A) < 1; otherwise the Perron vector p > 0 for A can be used 
to write 
1=e'p=e"(A+E)p=1+e7Ep> 1 
because 
E>0, E40, p>0 
=> 
Ep>oO. 
(Conditions weaker than the column-sum condition can also force p(A) < 1—see 
Corollary 4.11.7 on page 622.) The assumption that A is a nonnegative irre- 
ducible matrix whose spectral radius is 
p(A) < 1 combined with the Neumann 
series provides the conclusion that 
co 
(2A) =) 
APS 0. 
k=0 
Positivity is guaranteed by Lemma 7.4.2 on page 809 because A is irreducible. 
Therefore, for each demand vector d > 0, there exists a unique supply vector 
given by 
s=(I—A)~'d >0. The fact that (I— A)-'>0 and s>0 
leads 
to the interesting conclusion that: 
e 
An increase in public demand by just one unit from a single industry will 
force an increase in the output of all industries. 
Note: The matrix I — A is an M-matrix as defined and discussed on page 
622. The realization that M-matrices are naturally present in economic models 
provided some of the motivation for studying M-matrices during the first half of 
the twentieth century. Some of the M-matrix properties given in Theorem AALO 
on page 623 and Exercise 4.11.14 on page 642 were independently discovered and 
formulated in economic terms. 

814 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
Perron—Frobenius Converse 
The Perron-Frobenius theorem asserts that if A > 0 is irreducible, then A 
has a simple and positive eigenvalue associated with a positive right-hand and 
left-hand eigenvector. It turn out that the converse of this statement is also true. 
; 14S. Site 'as 0 is ifsiacinie ifa 
aad tee 
Af xe hee a eee 
positive eigenvalue \ > 0 that is associated with a positive tight-hand- 
oe aes a 0 and a 
positive left-hand eigenvector q' > 0. 
Sane 
Proof. 
Assume first that A has a simple positive eigenvalue A > 0 associ- 
ated with respective positive right-hand and left-hand eigenvectors p > 0 and 
q! > 0, and prove that A must be irreducible. If D = diag (p1, po,.--, Pn) 
contains the components of p as diagonal entries, then 
Do *AD 
= 6s 
TAAL 
P=—; 
(7.4.11) 
is a stochastic matrix! so that p(P) =1€o(P) (see Corollary 4.11.12 on page 
636). Since P and A are similar matrices, 1 is a simple eigenvalue of P having 
respective right-hand and left-hand eigenvectors D~'p=e>0 and q7D>0. 
Clearly, P is irreducible if and only if A is irreducible, so it suffices to prove 
that P is irreducible. Theorem 4.11.11 on page 635 and its corollary on page 636 
ensure that P is Cesaro summable to the spectral projector G onto N (I— P) 
along R(I— P)—i.e., 
PE Peet ee Peas 
lim ——————————_- = G > 0. 
k-oo 
k 
The fact that G > 0 follows because 1 € a (P) is a simple eigenvalue so that 
D>'pq?D 
Ge anes >0 
(by (4.7.18) on page 550). 
(7.4.12). 
To see that P must be irreducible, suppose that the contrary holds—i.e., suppose 
that P is reducible so that P = Q(t oe where X and Z are square, 
and Q is a permutation matrix. This implies that there is a position (i,7) with 
1A 
J such that [PEs =(O"forrall b==152,242 
~ Therefore: 
I+P+.--.-4+P*} 
iis ses Ce 
=O for k=1,2,... 
=> 
G, =0, 
a 
which contradicts (7.4.12). Thus P, and hence A, must be irreducible. Con- 
versely, if A is irreducible, then the Perron—Frobenius theorem establishes that 
A. has a simple positive eigenvalue, namely 
\ = p(A), and associated positive 
right-hand and left-hand eigenvectors, namely the Perron vectors. 
Constructing the stochastic matrix P to develop and study properties of nonnegative matrices 
is a common technique that is often referred to as stochastization. 

7.4 Perron—Frobenius Theorem 
815 
Continuity of the Perron Root 
The fact that the Perron root of Ay 
xn > 0 varies continuously with the entries 
in A is a corollary of Theorem 6.1.1 on page 757 and Theorem 8.2.5 on page 
915. These theorems establish that the eigenvalues of any square matrix are con- 
tinuous functions of its entries, but both theorems necessarily require arguments 
involving complex numbers. However the continuity of the Perron root of a non- 
negative matrix is a question that is entirely in the realm of real numbers, so 
it seems reasonable that there should exist a proof involving only real analysis. 
Indeed there is, and it is of independent interest, so it is presented below. 
es 4. 
6. Theorem. If {Aye 88 sequence co 
nxn ee matrices 
such that Tins A, 
=A: and i: Th and r are the ee Perron 
roots of Ak ane A, then limp—+o0 P k a 
Proof. 
Since A > 0 (the limit of a nonnegative sequence is nonnegative), the 
argument can be divided into two cases: (1) A > 0 and irreducible; (2) A > 0 
and reducible. 
(1) The Irreducible Case. Let Ex = Ay —A, and let px and q' be respective 
right- and left-hand Perron vectors for A, and A with ||px||, =1= |l/q]|,- If 
qx = ming;, then q>q,e, and q' p, > Ge 
pr =% > 0 for all k. Using this 
with the Cauchy-Schwarz inequality and ||x||. < ||x||, for all x € R" yields 
(re — ra? 
pel = la" (rePx) — (ra" Pel = |a? (AxPe) — (a7 A)P a 
= |q" (Ag — A)px| = |a7 Expel < ||Ealle 
Bells — [Bell 
q' pr 
dx 
(2) The Reducible Case. If r =0, then A is nilpotent, say A? = 0, so 
rel' = [e(Ax)|" = o( Ag) S AGI > AP 
08 == 
re > OS 3. 
Now assume that r > 0, and establish the following fact. 
a 
Ir, —T| < 
=) 
le 
re Po 
e 
Every subsequence {rp,} of {rr} has a sub-subsequence {rei, 
such that 
Vie te 
(7.4.13) 
To prove this, adopt the notation X ~ Y to mean that Y =P' XP 
for some 
permutation matrix P so that A ~ 63 oa where U and W are square. 
If either U or W 
is reducible, then they in turn can be reduced in the same 
fashion. Reduction of diagonal blocks can continue until at some point 
NS] 
Pee tere 
ee 
(7.4.14) 
Ol 
tums 
Om 
acs 
ae 

816 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
is block triangular with square diagonal blocks, one of which—call it B—is nec- 
essarily irreducible and has p(B) = p{A) =r > 0. Apply the same symmetric 
permutation that produced (7.4.14) to each A, so that 
e 
eee 
@ 
one 
e 
Ax ~ 
e 
nae 
B, 
tae 
e 
F 
eae. 
ha 
fear 
a 
where B, and B have the same size and occupy the same positions. It follows 
from (7.2.6) on page 799 that if by = p(Bx), then bk <r, for each k. And 
since A; — A implies B; — B, case (1) (the irreducible case) ensures that 
p(Br) > p(B) (ie, bg 4 7). In particular, if {rz,} is any subsequence of 
{rz}, then 
bp, <r; for each kj, and by, 
Tr. 
(7.4.15) 
Every subsequence {r,,} is bounded because p(x) < ||*|| for any matrix norm, 
and this implies that 0 < rz, < ||Az,|| = ||A+Exz,|| 
— ||A||. Hence every 
subsequence {rz,} has a convergent sub-subsequence Re? io. This together 
with (7.4.15) yields 
bk, Sk, 
Sothat 
r< ae 
(7.4.16) 
ag 
Now show that r* = r. Any sequence of Perron vectors 
{vx,} for Ax, is 
bounded (because IVa, 1 = 1), and hence {v,z,} has a convergent subsequence 
VE;. 
2 v* #0. Since fe, 
r*, this means that 
mg 
Av" = lim Ag, lim vg,, = lim[Ag, Ve,,] = lim[rs,.Ve,,] = lim T,,lim VE, = 7" Vv" 
i] 
j 
J 
J 
=> 
r* is aneigenvalue for 
A => 
r* <r. 
Therefore, (7.4.16) ensures that r* = r, and thus (7.4.13) is established. To prove 
that r, — r, suppose to the contrary that rz "4 r so that there is a subsequence 
{rz} and a number 
¢€ > 0 such that 
|rz, —r| > € for all 
s = 1,2,3,.... 
However, (7.4.13) guarantees that {rz,} has a subsequence {rx, } such that 
Pee which is a contradiction, and thus r, > Tr. 
; 
A False Proof 
The characterization of spectral radius in (4.11.17) on page 621 says that 
: 
myjl/m __ 
lim en 
= 0 (X), 
(7.4.17) 
so it is tempting to use this to establish the continuity of the Perron root (or the 
spectral radius in general) by writing 
lim re = lim 
lim AR" = lim 
lim JAR" = 
lim JA™/™ = r, 
k- oo 
k-co M—0o 
mM—- co k— oo 
m—->oo 

7.4 Perron—Frobenius Theorem 
817 
A standard result from real analysis says that it is acceptable to interchange the 
limits on k and m if the convergence in (7.4.17) is uniform on 
Rea fee Re XO}, 
but alas, it is not. To see this, observe that if 
Fa(X) = XY" 
and 
f(K) = p(X), 
then fm(aX) = afm(X) and f(aX) = af(X) for all a > 0 and for all 
X € Ri*". The convergence of fm to f cannot be uniform on R{*" because 
otherwise, for each € > 0, there would exist an integer M such that m > M 
implies |fmn(X)— f(X)| < € for all X € R{*". In particular then, m > M 
implies that | fm(aX)— f(aX)| < € for all a > 0 and for all X € R{*", or 
equivalently, 
a\fm(X) — f(X)|<e 
for all 
a>0 and for all X € R"*", 
which is impossible. 
Collatz—Wielandt Formula for Nonnegative Matrices 
The following theorem extends the results from Theorem 7.3.2 on page 805 con- 
cerning the Collatz—-Wielandt formulas for positive matrices to a more general 
max-min formula that holds for nonnegative matrices. 
7.4.7. Theorem. The Perron root r for Any, = 0 is given by 
Axe 
oe a x where 
MN ={xe€R"|x>0,x 
40}. 
xEN 2; 40 
2; 
Note. Unlike the Collatz—Wielandt results for positive matrices in which 
there was a min-max as well as a max-min formulation, only the max-min 
version holds for nonnegative matrices (see Exercise 7.3.8, page 807). 
Proof. 
Let E be the matrix of all 1's, and consider Ay = A + (1/k)E >0 
for k = 1,2,3,.... 
If x = ming,¢o [Ax];/z; for x € iN. then jx = Ax, 
Since 
0 < A < Ax, it follows that Ax < Axx, and hence xx < Axx for all 
x € N. The Perron root rz and the left-hand Perron vector q; for A, are 
both positive, so apo > 0, and 
fedex 
d, AcK= ed; Se 
i Se YR GUN 
Since rz, 
> 1 (by Theorem 7.4.6), it follows that ux <7 VxeEN. If z>0 
is 
an eigenvector such that Az=rz then pz =7T, and thus maxxeN lx =T. 
i 

818 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
Exercises for section 7.4 
7.4.1. 
7.4.2. 
7.4.3. 
7.4.4. 
7.4.5. 
7.4.6. 
7.4.7. 
7.4.8. 
7.4.9. 
Determine whether A = 
is reducible or irreducible. 
oonoo 
ouocor 
OoOnNnNOoOOW 
eFoono 
ofnooo°o 
For a given matrix A, .», suppose that the purpose is to interchange 
rows of A in exactly the same way as columns are interchanged. If P is 
the permutation matrix corresponding to the desired permutation, then 
explain why forming the product 
P?7 AP 
is the correct way to affect 
the permutation rather than using PAP. 
Can an irreducible matrix contain a zero row or zero column? Why? 
Suppose that A is a nonnegative matrix that possesses a positive spec- 
tral radius and a corresponding positive eigenvector. Does this force A 
to be irreducible? 
If Anxn is irreducible, then explain why (I+ A)""!>0. 
Hither prove or disprove each of the following statements for nonnegative 
matrices A € R"*" and for every positive integer k. 
(a) If A is irreducible, then A? A and AA" are irreducible. 
(b) If A is irreducible, then A* is irreducible. 
(c) If A is reducible, then A* is reducible. 
It follows from (4.11.18) or (7.2.6) on page 799 that p(A +E) > p(A) 
for all Anxn > O and E > 0, and (7.4.7) in the Perron—Frobenius 
theorem says that there is strict inequality 
p(A +E) > p(A) when A 
is irreducible and E ¥ 0. Give an example to show that it is possible 
for 
p(A +E) = p(A) when A is reducible. 
Let A,B € R"*". 
(a) Prove that if 
0< A <B, then ||A||, < ||B||,. 
(b) Prove that if 
O0< 
A<B and 
BZA, then |All, < ||B]|,. 
Prove that if A > 0 is irreducible, then A is similar to a matrix having 
all of its row sums equal to r, the Perron root. 

7.4 Perron—Frobenius Theorem 
819 
7.4.10. Prove that if An 
xn» is nonnegative and irreducible, then 
7.4410 Let LER? with n>? 
(a) Prove that if A? <0, then A? is reducible. 
(b) Explain why this means that A? <0 is impossible. 
7.4.12. As discussed on pages 69 and 143, the adjacency matrix for a directed 
graph G on n nodes is the n x n matrix A = [a;;| such that 
fal 
1, 
if there is an edge from node 7 to node J, 
) 
0, otherwise. 
The in-degree dj(in) of node j is the number of directed edges coming 
into node j, and the out-degree dj;(out) of node j is the number of 
edges that leave node j. Let d(in) and d(out) be the column vectors 
whose respective j*" entries are d;(in) and dj(out), and let Din) and 
D out) be the diagonal matrices whose respective diagonal elements are 
dj (in) and d; (out). 
(a) Prove that if the adjacency matrix A for G is irreducible, then 
p(AD Gury) = 1, and the right-hand Perron vector for AD Gut) 
is given by 
e 
d(out) 
p= e! d(out) ; 
(b) Prove that p(Diiny A) = 1, and the left-hand Perron vector for 
Din is 
Tee din) 
(c) Do the results in (a) and (b) also hold for weighted graphs, where 
the respective in-degree and out-degree of node 7 is interpreted 
to be the respective sum of the weights of the edges coming in 
and going out of node j? 

Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
7.5 
PRIMITIVE MATRICES 
The only property in the list on page 804 that irreducibility is not able to salvage 
is (7.3.7), which states that there is only one eigenvalue on the spectral circle. 
Indeed, A = e iy is nonnegative and irreducible, but the eigenvalues +1 are 
both on the spectral (unit) circle. The property of having (or not having) only 
one eigenvalue on the spectral circle divides the set of nonnegative irreducible 
matrices into two important classes. 
7.5.1. Definition. Let A € R"*" be nonnegative and 
irreducible. 
e 
The number h of eigenvalues on the spectral circle of A is defined 
to be the period! of A. 
e 
If 
h=1, then A is said to be primitive or aperiodic. In other 
words, r = p(A) is the only eigenvalue on the spectral circle of a 
primitive matrix. 
e 
When h>1, A is called imprimitive or periodic. 
Primitivity and Limits 
The reason that primitivity is an important concept is because primitivity is ex- 
actly what is needed to guarantee that linear evolutionary processes have limiting 
values. The precise statement is as follows. 
7.5.2. Theorem. A nonnegative irreducible matrix A with p(A) =r 
is 
primitive if and only if limg..,(A/r)* exists, in which case 
a 
fave 
pq' 
dim (=) = Geer 20 
(7.5.1) 
where G is the spectral projector associated with r, and p and q? 
are the respective right-hand and left-hand Perron vectors for A. 
Proof. 
It is clear from Definition 7.5.1 that 
A is primitive <> A/r is primitive, 
<= p(A/r) = 1 is the only eigenvalue on the unit circle, 
<=> limp-+o0(A/r)* exists 
(by (4.11.39) on page 632). 
7 
P 
: 
. 
: 
: 
The period h of an irreducible matrix A is often called the index of imprimitivity of A. 

7.5 Primitive Matrices 
821 
The fact that limg..(A/r)* = G = pq" /q"p is a consequence of (4.11.40) 
on page 632 together with Corollary 7.4.4. 
I 
Frobenius's Test for Primitivity 
So how does one decide if a given A is primitive or not? The answer, due to 
Frobenius, is pretty simple—just check to see if some power of A is a positive 
matrix. 
7.8.3. Theorem. (Frobenius Primitivity Test) A > 0 is primitive if and — 
only if A" >0 for some positive integer 
m>0. 
oe 
Proof. 
First assume that A™ > 0 for some m. This implies that A is irre- 
ducible; otherwise there exists a permutation matrix such that 
xX 
™m 
A=P/( 
ike —— ee, 
s a has zero entries 
VY m. 
0 
Z 
Oe 
ZEt 
Suppose that A has h eigenvalues {A1,A2,.-.,An} on its spectral circle—e., 
r= p(A) = [Ai] 
=-++ = [An] > Angil > ++ 
> [Anl. 
It follows that \" (1 < k <h) is on the spectral circle of A™ (Exercise 4.10.8, 
page 613). Since A™ > 0, Perron's theorem on page 804 ensures that A™ 
has only one eigenvalue (which must be r™) on its spectral circle. Consequently, 
rm = \™ — 
NY —...= \N™, and hence alg multam (r™) = h. However, Perron's 
theorem also guarantees that the spectral radius of A™ is a simple eigenvalue, 
and therefore h = 1. Conversely, if A is primitive, then lim,,.(A/r)* > 0 
by Theorem 7.5.2. Hence (A/r)™ > 0 for some m, andthus A" >0. 
& 
Wielandt's Exponent 
Suppose that the goal is to decide whether or not a nonnegative matrix An 
xn 
is primitive by computing the sequence of powers A, A*,A°,...- This can be 
a laborious task (n3 flops are required to perform one matrix-by-matrix mul- 
tiplication), so it would be nice to know when enough powers of A have been 
computed to render a judgement. Unfortunately there is nothing in the state- 
ment or proof of Frobenius's test to help us with this decision. However, Wielandt 
(page 774) provided an answer by proving that a nonnegative matrix Anxn 1s 
primitive if and only if Av' —2n+2 5 9, Furthermore, n?—2n+2 
is the smallest 
such exponent that works for the class of all n x 
primitive matrices having 
all zeros on the diagonal—see Exercise 7.5.10. 

822 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
Example (Boolean Matrix Multiplication) 
ae 
Problem: Determine whether or not A = € : 2) 
is primitive. 
Solution: If B = b(A) is the Boolean matrix that results from setting 
ee 
bbe 
aij > 0, 
— 
0 
if aj= 0, 
then [B*],; > 0 if and only if [A*],; > 0 for every k > 0. This means that 
instead of using A, A?, A%,... to decide on primitivity, one need only compute 
B,=)(A), 
Bo=0(B,B,), 
B3;=6(BiB2), 
Bs =0)(B,Bs),..., 
going no further than B,2~2n42, and these computations require only Boolean 
operations AND and OR. The matrix A in this example is primitive because 
0 
1 0 
ia 
a 
ito 
ris 
Rane 
| 
p= (0 0 1), B= (1 1 0), 
py = (0 1 1), B= et 1) Bee) 
eee ae 
Lato 
Garis. 
triad 
eas 
iin 
Exercises for section 7.5 
7.5.1. Determine whether A = 
is primitive or imprimitive. 
CONC 
O owooor 
Ono 
OCOW -KH 
Oo 
O10 
or 
OOO 
7.5.2. Suppose that A, 
x, > 0 is primitive. 
(a) Prove that p(A) = jim [trace (A*)] aa 
—0o 
(b) Show that the result in part (a) fails to hold when the hypothesis 
of primitivity is relaxed to assume only that A is nonnegative 
and irreducible. 
7.5.3. Let A >O be an irreducible matrix, and let as*) denote entries in A". 
Prove that A is primitive if and only if 
7.5.4. Essentially positive matrices. 
A matrix A € R"*" is said to be essen- 
tially positive if A is irreducible and a;; > 0 for every i 4 j. Prove 
that each of the following statements is equivalent to saying that A is 
essentially positive. 
(a) There exists some a € R such that A+ al is primitive. 
Hint. Recall Exercise 7.4.5 on page 818. 
(b) eS 0. for all 
70. 

7.5 Primitive Matrices 
823 
7.5.5. Let A be an essentially positive matrix as defined in Exercise 7.5.4. 
Prove that each of the following statements is true. 
(a) A has an eigenpair (€,x), where € is real and x > 0. 
(b) If A is any eigenvalue for A other than €, then Re(A) < €. 
(c) € increases when any entry in A is increased. 
7.5.6. Let Anxn > 0 be irreducible having spectral radius p(A) = r and 
Perron vector p, and let zx; > 0 be any nonzero nonnegative vector. 
z? Akt, 
(a) Prove that if A is primitive, then jim Si Cra ee 
(b) The adjacency matriz for a directed graph G on n nodes was 
defined in (1.8.1) on page 69 and on page 143 to be the n x n 
matrix A = [a;;| such that 
eas { 
1 
if there is an edge from node 7 to node j, 
sy 
0 
otherwise. 
Prove that if the adjacency matrix A for G is primitive and if 
P, is the total number of distinct paths of length k in G, then 
eee 
ots: 
lim 
k—>co 
Py =. 
(c) Give an example to show that the result in part (b) need not be 
true when A is periodic. 
7.5.7. Let A be the adjacency matrix for a graph G for which A is primitive, 
and let P, be the total number of distinct paths of length k in G. 
(a) If Px(ix) denotes the number of paths of length k in G that 
originate at node i, prove that 
where p is the right-hand Perron vector for A. 
(b) If P,(*j) denotes the number of paths of length k in G that 
terminate at node j, prove that 
Pi(-*7 
at ee 
lim 
k-00 
k 
where q' is the left-hand Perron vector for A. 

824 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
7.5.8. Let A be the adjacency matrix for a simple (no loops or multiple edges 
between nodes) connected undirected graph G containing n > 1 nodes. 
(a) 
(b) 
Prove that if A is periodic and has p(A) = 1, then n = 2 
and A = fc ay Hint. Consider A?. 
Give an example to show that the result in part (a) is not valid 
if p(A) > 1. 
7.5.9. Random Walk on a Graph. A random walk on a connected, undirected, 
weighted graph G of n > 1 nodes whose adjacency matrix is A is an 
infinite path traced by a walker who travels from node to node in G by 
selecting an edge at random each time a move is made. More specifically, 
if the walker is at node i, and if there are k edges {Ej;,, Fij.,-++ Fiz, } 
having respective weights {a;;,,@:j;,,°**@ij,} that touch node i, then 
the walker travels to the next node along edge £j;, with probability 
ai) Dae ai;,. The transition matria for the random walk is P = [pj,;], 
where p;; is the probability of moving from node i to node j in one 
step. 
(a) If the degree d; of node j is defined to be the sum of the 
weights of the edges that touch node j, and if d is the column 
and D is the diagonal matrix whose respective j*" entry and 
j*" diagonal element is dj, then explain why 
P = D~!A and 
p(P) =1. Hint. What are the right-hand and left-hand Perron 
vectors for P? 
(b) Prove that if A is primitive, then 
jim Labor = are for every 4 = 1,2,...}n: 
Give an interpretation of this result in terms of the amount of 
time that the random walker spends at each node. 
Outs 
6 
0 
a 
ea 
ec 
a 
7.5.10. Wielandt constructed the matrix W,=]: 
: 
°°. 
°. 
: 
to show 
0 6 i 
0 
4 
1 
wig 
S202 
0 
that 
W" ~?"+2 > 0, but [W"'-2"+1],, = 0. Verify that this is true 
fort =A, 

7.6 Periodic Matrices 
825 
7.6 
PERIODIC MATRICES 
As defined on page 820, a periodic (or imprimitive) matrix Ayy, > 0 is irre- 
ducible and has more than one eigenvalue on its spectral circle. However these 
eigenvalues cannot occur just ers on this circle. The developments in this 
section are primarily due to Wielandt! (see page 774), and they establish the re- 
markable fact that the h eigenvalues on the spectral circle of a periodic matrix 
are in fact uniformly distributed—more precisely, they are the distinct h*" roots 
of the spectral radius. Furthermore, the eigenvalues inside of the spectral circle 
are rotationally invariant through an angle 27/h. 
Uniform Distribution of Eigenvalues on the Spectral Circle 
It was reiterated in (7.2.6) on page 799 that if A ¢ R"*" and Be C"*" are 
such that |B| < A, then p(B) < p(A). To prove that the eigenvalues on the 
spectral circle of a periodic matrix A are uniformly distributed around that 
circle, Wielandt first provided the following technical (but powerful) lemma that 
establishes conditions for equality to hold in p(B) < p(A). 
7.6.1, Lemma. (Wielandt's Lemma) Let A € R"~" and Be C"*" be 
such that A is irreducible and |B| < A. It is true that p(B) = p (A) 
if and only if there exists a 0 < @ < 2m such that 
el Ol 
ela2 
"B=ec*®DAD- 
forsome 
D= 
- 
(7.6.1) 
cian 
Proof. 
Suppose first that p(B) =r = p(A), and let (u,x) be an eigenpair 
for B such that |y| =r so that 
u=re®? 
for some 0 <¢< 2r. 
(76,2) 
It follows that 
r|x| = |u| [x| = |zx| = |Bx| < |B] |x| < Alx]. 
(7.6.3) 
Consequently, r|x| = A|x| (by Corollary 7.4.3 on page 811), and 
bref S10), 
(7.6.4) 
so this along with (7.6.3) yields r|x| = |B||x|. Since (A — |B])|x| = 0 and 
(A —|B]) > 0 with |x| > 0, it follows that A = |B| by (7.2.3). For each 
Unzerlegbare nicht-negative Matrizen, Math. Zeits., Vol. 52, 1950, pages 642-648. 

826 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
Oey 
pete! 
k=1,2,...,n, the terms z,/|a| are on the unit circle, so rx/|r_| = e'°* for 
some Q,z. Set 
; 
ei@1 
ela2 
D= 
- 
i 
(7.6.5) 
elon 
and notice that x = D|x|. This means that 
BD|x| = Bx = px = re'®x = re'?D|x| 
=> 
e !?D-!BD|x| = r|x| = Axl. 
(7.6.6) 
For convenience, let C = e'?D~!BD, and note that |C| = |B| = A to write 
(7.6.6) as 0 = (|C| — C)|x|. Considering only the real part of this equation 
yields 0 = (|C| — Re(C) )|x|. But |C| > Re(C), and |x| > 0, so it follows 
from (7.2.3) that Re(C) = |C|, and hence 
Ree) = |6;| = 1) Re (qj) * +Im (ci) 7 == Inte, 
=> 
In (Cy at: 
Therefore, 
C = Re(C) =|C| = A, which implies 
B = e'?DAD~!. Conversely, 
if 
B=e'®DAD7~|, then similarity insures that p(B) =p(el?A)=p(A). 
Bf 
With the aid of Wielandt's Lemma 7.6.1, the proof of his fundamental the- 
orem concerning the uniform distribution of eigenvalues on the spectral circle of 
a periodic matrix can be given. 
7.6.2. Theorem. (Wielandt's Theorem) Let A > 0 be irreducible and let 
p(A) =r. If A has exactly h distinct eigenvalues {Xo, \1,-.-,; An—1} 
on its spectral circle, then each of the following statements is true. 
e 
alg mili, (Ag)1 for k= 0, 1,252.08 — 1, 
(7.6.7) 
© 
Ay = re?™Fi/h for k = 0,1,2,...,2 —1. In other words, the eigen- 
values on the spectral circle are the h*" roots of r given by 
{r, rw, rw", ..., rw}, 
where 
w= 67/4. 
(7.6.8) 
Proof of (7.6.7). 
Let A, = re 
for k = 0,1,2,...,h —1 be the distinct 
eigenvalues of A on the spectral circle. Set B = A in Wielandt's lemma, and 
take yp = re', or equivalently, set ¢ = 6, in Qi:602 afor Or hk <h— 1 The 
lemma guarantees the existence of a diagonal matrix D; such that 
A =e'*D, AD," = e*D, AD; ! = D;(eA)Dz1=D,A,D;!. 
(7.6.9) 

7.6 Periodic Matrices 
827 
In other words, A is similar to A, = eA for each 0 < k < h—1. Since 
r is a simple eigenvalue of A (by the Perron—Frobenius theorem on page 809), 
Ap, = re is a simple eigenvalue of e% A = A; for each k. Eigenvalues along 
with their algebraic multiplicities are similarity invariants (consider the Jordan 
form), so the eigenvalues of A, and A along with their algebraic multiplicities 
agree for each k. Thus Ax is a simple eigenvalue of A for each 
O<k <h—-1. 
Proof of (7.6.8). 
For each eigenvalue Ag = re, (7.6.9) ensures that there is a 
diagonal matrix D, such that A = e% D,AD,". Consider two distinct values 
of k—say 
k= k, and k = kp —so that 
A=e™D,,AD,' 
and 
A=e%2D,,AD;,, 
and write 
Ameo) Dz AD, a= e2Dre 
2DpAD OD, 
(7.6.10) 
= oF 82)(Dy, Dy, )A(Dj, Di.) 
Consequently, rei(9x1+%%2) 
ig also an eigenvalue for A that is on its spectral 
circle. This means that the set of eigenvalues S = {r,re'1,..., re!@»-1} is closed 
under multiplication, which in turn implies that the set G = {1,e',...,el@»-1} 
is also closed under multiplication. It follows that G is a finite commutative 
group of order h. A standard result from algebra states that the ht power of 
every element in a finite group of order h must be the identity element in the 
eroup.' Therefore, (ce!) =1 for each 
O<k <h-—1, so G must be the set of 
the ht" roots of unity e27*i/h for 
0<k<h-—1, and thus S must be the h'" 
roots of r. 
# 
For stochastic matrices (page 636) in particular, the following corollary is a 
direct consequence of Wielandt's theorem together with the results in Corollary 
4.11.12 on page 636. 
7.6.3. Corollary. If a stochastic matrix P has h unit eigenvalues, then 
they are uniformly distributed around the unit circle. In other words 
they are the h*" root of unity 
2 
= 
Qni 
fue 1) 
where yee 
i This is a corollary of Lagrange's theorem for finite groups. For example, see Corollary 7.37 on 
page 174 in Neal H. McCoy's text Introduction to Modern Algebra (Revised Edition) or any 
other undergraduate book on modern algebra. 

828 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
Rotational Invariance of the Entire Spectrum 
Theorem 7.6.2 can be rephrased by saying that the set of eigenvalues on the 
spectral circle of periodic matrix A is invariant under a rotation about the 
origin through an angle 27/h. Remarkably, the entire spectrum of A is in fact 
invariant under the same rotation. 
r Cae if A is es ae _ ees 
on Aes ee 
 eirele, 
ther a (A) is invariant under rotation about ihe 
'origin a 
& an angle 2n/ h. In other words, ¢ (A) = ale' 
ani/h A), 
s. 
e Moreover, no rotation 
1 less tae aie A can preeer ye oO (A). 
Proof. 
As usual, let 
r = p(A). Since \ € 0 (A) <> Ae?™i/2 € a(e?/P A), it 
follows that o(e?7/"A) is obtained by rotating o(A) through an angle 27/h. 
As in the proof of Theorem 7.6.2, set B = A in Wielandt's lemma, and take 
pw = re'i/h in (7.6.2) so that ¢ = 27/h in (7.6.1). Consequently, A and 
e2ti/hA are similar, so o(A) = o(e2™'/"A). No rotation less than 27/h can 
keep o (A) invariant because (7.6.8) makes it clear that the eigenvalues on the 
spectral circle won't go back into themselves for rotations less than 27/h. 
Sufficient Condition For Primitivity 
Trying to determine if an irreducible matrix A > 0 is primitive or periodic 
by finding the eigenvalues is generally a difficult task, and even the Frobenius 
test on page 821 (or the Boolean implementation in the example on page 822) 
can be challenging for large matrices. So it is natural to ask if there is another 
and easier way to determine primitivity. It turns out that there is, and, as the 
following theorem shows, the test is often trivial. 
7.6.5. Theorem. Let A,,,,, > 0 be an irreducible matrix. 
e 
If A is periodic, then trace(A) = 0, but not conversely. 
(7.6.11) 
e 
If A has a positive diagonal element, then A is primitive, 
but not conversely. 
(7.6.12) 
Proof of (7.6.11). 
Suppose that A is periodic with h > 1 distinct eigenvalues 
on its spectral circle. It follows from Theorem 7.6.4 concerning the rotational 
invariance of the spectrum that if Ag € 0 (A), then Ap = Age27#*/h Cg (A) for 
hie UL 
a 
wl, oa 
3 An = Ao > e*mik/h — 
(roots of unity sum to 0O—by (5.5.5) on page 719). 

7.6 Periodic Matrices 
829 
This implies that the sum of all of the eigenvalues is zero. In other words, 
A periodic 
= > trace(A)=0. 
(Recall (3.1.9) on page 294.) 
The fact that the converse is not true is in Exercise 7.6.7 on page 839. 
Proof of (7.6.12). 
If A has a positive diagonal entry, then trace(A) #0 so 
that A cannot be periodic, and thus it must be primitive. The fact that the 
converse is not true is also in Exercise 7.6.7. 
I 
Determining the Period from the Characteristic Equation 
The powers of an irreducible matrix A > 0 determine whether or not A is 
periodic, but in case A is periodic, then the powers of A say nothing about 
the period itself—i.e., the number of eigenvalues on the spectral circle. The 
next theorem shows how the period h can be determined without explicitly 
calculating the eigenvalues. 
| 166. Theorem. Suppose that Anxn is periodic, and write the charac- 
_ 
teristic equation for A as 
CL) aon poahe toa ao eg ae ates "2 au ' ey 6) 
in which only the terms with nonzero coefficients are listed—i.e., each 
Ch; #0, and n> (n—ki) >-:- > (n—kg). The period (or index of im- 
primitivity) h of A is the greatest common divisor of {ky, ko, ..., Ks}. 
Proof. 
If {\1,A2,.--;An} are the eigenvalues of A (including multiplicities), 
then Theorem 7.6.4 concerning rotational invariance on page 828 ensures that 
{wA1,WAg,...,WAn} are also the eigenvalues of A, where w = e2ti/h The coef- 
ficients in the characteristic equation are symmetric functions of the eigenvalues 
(Theorem 3.1.6, page 294). In particular, 
Ck, = (1) Se a Nig, = (—1)* yy oe Wig, = wi cg, SE he ak 
1<t1<+<tp, Sn 
1St1<s<ik,; <n 
Therefore, h must divide each k;. To show that h is the greatest common 
divisor of {k1, ko,..., ks}, suppose that it is not. If d divides each k; with 
d>h, and if y =e2"/% then y—*) =1 
so that c(yA) =0 foreach 
A €o (A), 
which implies that y\ € o(A). But this means that o (A) is invariant under 
a rotation through an angle (27/d) < (27/h), which is impossible because it 
contradicts Theorem 7.6.4. 
@ 

830 
Example 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
Ob 
70) 
0 
ver 
Problem: Find the period of A = 
' : 0 
2 
Qrare 
L 
U, 
Solution: Using the principal minors to compute the characteristic equation as 
illustrated on page 296, produces the characteristic equation 
ce(z) = «4 — 52? +4=0, 
so that kj = 2 and ky = 4. Since gcd{2,4} = 2, it follows that h = 2. The 
characteristic equation is relatively simple in this example, so the eigenvalues 
can be explicitly determined to be {+2,+1}. This corroborates the fact that 
h = 2. Notice also that this also illustrates the property that o (A) is invariant 
under rotation through an angle 27/h = 7. 
The Frobenius Form 
Much more is known about nonnegative matrices than what has been presented 
here—in fact, there are entire books on the subject! But before moving on to 
applications, there is one more important theorem that needs to be included 
because it completely reveals the structure of a periodic matrix. However, as 
seen below, the proof, due to Helmut Wielandt (page 774), is somewhat long 
and involved. 
7.6.7. Theorem. For each periodic matrix A,» having period h > 1, 
there exists a permutation matrix P such that 
0 
Ai2 
0 
tae 
0 
0 
0 
Ao3 
::: 
0 
PAP 
| 
(7.6.13) 
0 
0 
tes 
QO. 
Anaian 
Ani 
0 
tee 
0 
0 
where the zero blocks on the main diagonal are square and the indicated 
Aj; blocks are nonzero. This is often called the Frobenius form. 
e 
Note: Since the diagonal blocks are square, the sizes of off-diagonal 
blocks Aj; are determined by the sizes of the zero diagonal blocks. 
For example, look at the pattern in (7.6.23) on page 836. 
ii 
: 
A good example is the book Nonnegative Matrices by Henryk Minc, John Wiley & Sons, 
(Wiley—Interscience), 1988. 

7.6 Periodic Matrices 
831 
Proof. 
Let p(A) =r, and let Ay = re?7*/h, 
O<k <h, be the h eigenvalues 
of A on its spectral circle. Consider any eigenpair other than Aj = r. There is 
no loss of generality in considering \ = ; = re?™'/" along with an associated 
eigenvector v. By using B = A in the proof of Wielandt's lemma on page 825, it 
follows from (7.6.4) on page 825 that |v| > 0. Part (7.6.7) of Wielandt's theorem 
on page 826 ensures that A is simple so that geo mult(A) = 1. Consequently, 
the normalized eigenvector 
Ae 
is the unique eigenvector associated with 
whose first entry is equal to one. 
Furthermore, the diagonal matrix described in (7.6.5) on page 826 becomes the 
uniquely defined matrix 
ik 
x /\x 
D= 
Sea 
: 
(7.6.14) 
Ln/|xn| 
The first goal is to show that D' =I. By using k = 1 in (7.6.9) on page 826 it 
follows that A = e2"'/"DAD7—!. Iterate this h times to obtain 
A = e2t/an (e?"/"DAD"") D-! = efi/*p2aAD-2 
— efti/hp? ("DAD") D~? = e&i/hpD3 AD-3 
- e2hmi/hDpk ap? Sr D'?AD~". 
This means that if p > 0 is the Perron vector for A, then y = D"p is an 
eigenvector for A corresponding to r because 
Ay = AD"p = (D?AD 
") D?p = D'Ap = rD"p = ry. 
Consequently, y = ap for some a # 0 because the Perron—Frobenius theo- 
rem (page 809) ensures that dim N(A —rI) = 1. It follows from (7.6.14) that 
[D") i=l so examining the first entry in ap = y = D'p yields ap; = pi, or 
equivalently, 
a= 1, and thus 
p=y = D'p. Therefore, for each 
1 <j <n, 
pj = (D"),,P5 = 
[D"| te 
D1. 
jg 

832 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
This means that the diagonal entries of D are h*' roots of unity, but not 
necessarily distinct nor in order. If there are s <h distinct diagonal entries in 
D, then order them as {1 = e2thii/h e2mkei/h | @2tksi/h} where 
Oe ky 
ke <n 
hy < hj 
(7.6.15) 
and let P be a permutation matrix such that 
e2tiki/hy, 
e2tika/hy,,, 
PTDP = 
=D, 
(7.6.16) 
e2tiks /by,, 
where n; is the number of times that e?"!*//" appears in D. Let A =PTAP, 
and partition A conformably with D. That is, 
Aili 
"Arg 
+> 
Ads 
~ 
Ag, 
Ase" 
+> 
Ags 
' 
A= 
- 
|, 
'where 
-A,ovis ny X Ng. 
(7.6.17) 
Asi 
As2 
a 
Algae 
Use A = e?7/h-DAD~! along with the fact that PP? =I to write 
Aer Penal P =e "(P DPE APP DP) =e Ale 
Examining the block in the (p,q)-position on each side of A = &2"i/hDAD-! 
yields 
Cad lag 
= Ch gh 
titer dl dW 
(7.6.18) 
In particular, when p= q, this implies that Ap, = 0 for each p. When p#4q, 
then (7.6.18) says that either 
Ang = 0 
or 
e2ti/h — e2Ti(kq—kp)/h 
(7.6.19) 
Matrix A in (7.6.17) is irreducible (because A is), so each row of blocks in A 
must contain at least one nonzero block. Consider the rows of A one at a time. 
p=1: 
Suppose Aj, #0. Since kj = 0, (7.6.19) reduces to e271/P = @27ika/h, 
This forces kg = 1 (because 0 < ky < h). Therefore, by virtue of 
(7.6.15), q = 2 is the only possibility. Thus kg = 1, and Ajo is the 
only nonzero block in the first row of blocks in A. 
p=2: 
The logic is similar. Suppose that Ao, # 0. Since kp = 1, (7.6.19) 
reduces to e271/h — e2ti(ka—1)/h which implies that k; —1= 1, or 

7.6 Periodic Matrices 
833 
equivalently, kg = 2. Again, by virtue of (7.6.15), q = 3 is the only 
possibility. Thus k3 = 2, and Ag3 is the only nonzero block in the 
second row of blocks in A. 
p<s: 
Similar successive arguments yield the conclusion that kpi1 = p and 
that Ap (p41) is the only nonzero block in the p*" row of blocks in A. 
This leaves only the last row of blocks in A to consider. 
p=s: 
Suppose that A,, # 0 with q < s < h. Since kg = q—1 and 
k, =s—1, (7.6.19) reduces to 
enh 
ete 
2) PS Tor equivalently, 
eet eg) 
ht. 
This can only happen under the existing constraints if 1+s—q = h, and 
the only integer solution for this equation that satisfies 
1<q<s<h 
is 
g=1 and s=h. Thus the last row of blocks in A is in fact the 
ht' row, and the only nonzero block in this last row is Aji. In other 
words, the only possible form for A is that given in (7.6.13) 
Example (Leslie Population Age Distribution Model) 
Divide a population of females into age groups Gi, Go, ..., Gn, where each 
group covers the same number of years. For example, 
G, = all females under age 10, 
G» = all females from age 10 up to 20, 
G3 = all females from age 20 up to 30, 
Consider discrete points in time, say t = 0,1,2,... years, and let b, and sx 
denote the birth rate and survival rate for females in G;,. That is, let 
b, = Expected number of daughters produced by a female in Gx, 
s, = Proportion of females in G;, at time ¢ that are in G,4, at time t+ 1. 
If 
fx(t) = Number of females in G; at time tf, 
then it follows that 
filt an 1) = fi(tjor "i= fo(t)be ap 
ko ap alt) Ons 
and 
(7.6.20) 
iva ap 1) = fro) Spt 
HOR [8 SS Be By coo 9 te 

834 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
Furthermore, 
re 
f(t) 
fit) 
+ falt) +--+ + fn(t) 
The vector F(t) = (Fi(t), Fo(t),-.., Fn(t))" represents the population age dis- 
tribution at time t, and, provided that it exists, F* = limy. F(t) is the 
long-run (or steady-state) age distribution. 
F,(t) 
= % of population in G, at time t¢. 
Problem: Assuming that s1,...,8, and b2,...,b, are positive and stationary 
in time, explain why the population age distribution approaches a steady state, 
and then describe it. In other words, show that F* = limy,.. F(t) exists, and 
determine its value. 
Solution: The equations in (7.6.20) constitute a system of homogeneous differ- 
ence equations that can be written in matrix form as 
by 
bo 
oa 
bn-1 
Dn 
8s, 
0 
ee 
aco 
th 
f(t+1)=Lf(t), 
where L=|9 
82 
0 
OP 
(7.6.21) 
0, 
a-Oiptagrt 
epee 
TO 
nxn 
in which f(t) = (fi(t), fo(t), ..., fn(t). The matrix L is called the Leslie ma- 
tric! Notice that in addition to being nonnegative, L is also irreducible when 
S1,-..,Sn and b9,...,b, are positive because the graph G(L) is strongly con- 
nected. Moreover, L is primitive. This is obvious if in addition to sj,...,5, 
and bo,...,bn being positive one has b; > 0 (recall Theorem 7.6.5 on page 
828). But even if b} = 0, L is still primitive by Frobenius's test for primitivity 
on page 821 because L"*? > 0. The Boolean technique from the example on 
page 822 can be used to easily verify this. (See Exercise 7.6.8 for another way to 
verify primitivity.) Consequently, Theorem 7.5.2 on page 820 guarantees that 
lim (=) =G=P4 50, 
t=oo 
\T 
where r = p(L), and p> 0 and q! > 0 are the respective right-hand and 
left-hand Perron vectors for L. Combine this with the fact that the solution to 
This is in honor of Patrick H. Leslie (1900-1972) who was a British ecologist and who formulated 
this model in 1945 to study an age-structured population of rodents. His 
original article, On the use of matrices in certain population mathematics, 
Biometrika, 33, pp. 212-245, 1945, was motivated by rodent reproduction, 
but in subsequent years after the appearance of his article, Leslie's model 
found acceptance in the study and analysis of a wide variety of different 
species, including human populations. 
. H. LESLIE 

7.6 Periodic Matrices 
835 
the system of difference equations in (7.6.21) is f(t) = L'£(0) (by (4.11.4) on 
page 617), and assume that f£(0) 40 to produce the conclusion that 
f(t) 
rt 
4 
ip 
q' £(0) 
=e 
7 6) 
hae 
( 
) 
(recall that ||x||, is a continuous function by Theorem 1.5.5 on page 35). Now, 
ae 
Fel) = Teo 
is the quantity of interest, and (7.6.22) allows the conclusion that 
= % of population that is in G;, at time t 
'=m 
= 
lim 
Be 
cs: eee 
ee ED Be era 
mree EI e 
Hie Sos f(t)/r? 
= ime tele =p 
(the Perron vector!). 
00 
1 
In other words, while the numbers in the various age groups may increase or 
decrease, depending on the value of r (see Exercise 7.6.9), the proportion of 
individuals in each age group becomes stable as time increases. And because the 
steady-state age distribution is given by the Perron vector of L, each age group 
must eventually contain a positive fraction of the population. 
Periodicity In The Graph Of A Periodic Matrix 
Recall that a symmetric permutation 
P? AP does not affect the structure of the 
directed G(A) graph of A—it only relabels the nodes. So, if A is a periodic 
matrix, then the structure of its graph is the same as that of its Frobenius form 
(7.6.13) on page 830, and this says something special about G(A). 
7.6.8. Definition. A directed cycle of length k in a directed graph G is 
a directed path (a connected sequence of nodes') that begins and ends 
at the same node. All graphs in the following discussion are directed, so 
the adjective "directed" will be omitted but nevertheless implied. 
e 
The greatest common divisor of the lengths of all cycles in G is 
called the index of G. 
i Repetitions of nodes are allowed in a directed path. A directed path containing no repeated 
nodes is called a simple path, and a directed cycle containing no repeated edges or nodes 
(except for the identical initial and terminal nodes) is referred to as a simple cycle or a circutt. 

836 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
For example, the graph of the 4 x 4 matrix A shown below in Figure 7.6.1 
contains cycles of lengths k = 2,3,4,.... 
ou 
Oro 
Ma fe 
ae 
ASS 
7 toh ooo 
G(A) 
Cale 0e0 
FIGURE 7.6.1: INDEX OF THE GRAPH OF A 
Notice that index G(A) =1, and note that A is primitive (because A® > 0). 
This is no accident. In general, it follows from the structure of the Frobenius 
form (7.6.13) on page 830 that the period of A is equal to index G(A). 
To see this, consider the following typical 9 x 9 Frobenius form of a periodic 
matrix A having period h = 3, 
9 
if 
0 
2 
0 
3 
0 
1H 
O,60 
0 
4 
ms 
P'AP=|.0 
.0,Y 
|-=, 
5 
x 
(7.6.23) 
ALAN 
IA 
6 
0 
(6 
0 
8 
0 
9 
0 
and examine all possible cycles beginning and ending at any node. For example 
the graph in Figure 7.6.2 below depicts all possible cycles beginning and ending 
at No. It is clear that each of these cycles has length 3n, for n = 1,2,3,.... 
FIGURE 7.6.2: CYCLES IN THE GRAPH OF (7.6.23) 
The same is clearly true for all other cycles beginning and ending at any other 
node, so index G(A) = 3. This is a specific case, but it is nevertheless represen- 
tative of what happens for a general Frobenius form of period h. 

7.6 Periodic Matrices 
837 
Proof. 
A rigorous proof amounts to formalizing the details observed in the 
preceding typical example. A completely formal argument is given in the book 
by Minc that was cited on page 830. 
H 
If A > 0 irreducible with period h, and if P is a permutation matrix such 
that 
P? AP is in Frobenius form as described on page 830, then Exercise 7.6.6 
on page 839 shows that 
Ci 
Cc 
PTAP = 
ii 
=D 
(7.6.24) 
Cp, 
is block diagonal, where each block C, is square and is the i*" cyclic permuta- 
tion of Aj2A3:--Api. That is, 
Cy = AyoAo3°-+ An—ijpAni, 
Co = Ao3A34::: Ani Ars, 
C3 = Aga: 
Ani AizAos, 
(7.6.25) 
Cpr = AniAi2:+:An-t1a- 
The next theorem shows that all of these C's are primitive and have the same 
eigenvalues. 
7.6.10. Theorem. If A,,.,, > 0 irreducible with period h, and if P isa 
permutation matrix such that P? AP is in Frobenius form, then each 
diagonal block C; in (7.6.24) and (7.6.25) is primitive, and all of these 
blocks have the same nonzero eigenvalues with the same multiplicities. 
Proof. 
First show that each C; is irreducible. Let FAG = PTA*P, and notice 
that for each 
k = 1,2,3,..., all diagonal blocks in A* are zero except when k 
is a multiple of h, in which case the it? diagonal block in A™" is 
Pas lee Ker for (i= Oy lig Preadte 
10 

838 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
Consequently, 
n—-1 
" 
el 
peg =1+([A) + [A'%*] +--+ Amr tt Cy + C7 +--+ CP, 
a 
a 
bE 
k=0 
ti 
where m is the greatest integer such that mh <n— 1. If some C; is reducible, 
i 
i 
i 
= 
x Y)Q7, 
where X 
then there is a permutation matrix Q such that C = Q( 0.2 JQ , whe 
and Z are square. Without loss of generality we can absorb this permutation 
Q into P and assume that C; = (3 ey Then every power C2" = © i 
is block triangular, and hence 
beg =(% *) 
(7.6.26) 
at 
is also block triangular. However, A is irreducible because A is, so it follows 
from (7.4.8) in Lemma 7.4.2 (page 809) with a, =1 that 
n—1 f 
SCA 
0, 
k=0 
which contradicts (7.6.26). Thus each C; must be irreducible. Now prove that 
all of the C;'s have the same nonzero eigenvalues with the same multiplicities. 
Notice that if we set 
X=Ajis1 
and 
Y = Ajiiiyo-+:AniAi2:-> Adi, 
{ C;=XY ! 
{ CPS XY 
ie 
\ 
—> 
- 
OFFS == NL, € 
Che = iY OSH aa 
Since trace (AB) = trace(BA) for every pair of conformable matrices A and 
B (by Theorem 1.8.10 on page 78), it follows that trace (Cf) = trace (C¥,,) for 
every k. This guarantees that C; and C,+,; have the same nonzero eigenvalues 
with the same multiplicities (see Exercise 3.1.31 on page 302), and this holds for 
each 
7 = 1,2,...,h—1, so all of the blocks C; have the same set of nonzero 
eigenvalues. To prove that each C, is primitive, let p(A) = r, and observe 
that p(A") =r" = p(C;) for each i because each C; has the same nonzero 
spectrum. By Theorem 7.6.2 on page 826, the eigenvalues of A are 
then 
{re rer pe ta 
pet Abts 
o< 
tants, 
With 
PAG a 
Hence the eigenvalues of A" are {r,...,r,AP,,,...,A2} with r being repeated 
h times. Ifeach C; has p eigenvalues 
{r,re?"!(1/P) 
re?ti(2/P) | pe2ti(P-1/p)} 
on its spectral circle, then A' must have ph eigenvalues on its spectral circle. 
This means that p= 1, and thus each C;, is primitive. 
I 

7.6 Periodic Matrices 
839 
Exercises for section 7.6 
Oo 
7.6.1. Let A= (: 
0 2). 
7.6.2. 
7.6.3. 
7.6.4. 
7.6.5. 
7.6.6. 
7.6.7. 
7.6.8. 
7.6.9. 
OF 
2) 
0 
(a) Show that A is irreducible. 
(b) Find the Perron root and Perron vector for A. 
(c) Find the number of eigenvalues on the spectral circle of A. 
Suppose that the period of a 5 x 5 nonnegative irreducible matrix A 
is h = 3. Explain why A must be singular with alg mult, (0) = 2. 
Construct a 5 x 5 periodic matrix having period h = 3. 
1) 
0 
0 
1 
Verify that A = (: 
; 3 
is irreducible, and then decide if A is 
1 
0 
0 
primitive or periodic. If you say "periodic," then determine the period 
of A. 
Without computing the eigenvalues or the characteristic polynomial, 
explain why o (P,) = {1, w, w?,...,w" 1}, where w = e?7/" for 
O 
91 
on 
eee 
0 
We 
Ur 
le 
aes 
P,= 
: e 
Ous05 
ccna 
«Onn? 
tO 
m0 
0 
Prove that if A > 0 irreducible and has period h, then A" is a 
symmetric permutation of a block diagonal matrix with square diagonal 
blocks in which the size of the i*" diagonal block is n; x nj. 
Let Anxn > 0 be an irreducible matrix. Give an example to show that 
trace(A) =0 =A 
is periodic. 
Use the characteristic equation as described in Theorem 7.6.6 on page 
829 to show that the Leslie matrix in (7.6.21) is primitive even if b; = 0 
(assuming all other b;,'s and s,s are positive). 
In the Leslie population age distribution model on page 833 explain what 
happens to the vector f(t) as t + oo depending on whether r < 1, 
(ps ily ole 
fe ee Ie 

840 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
7.7 DIFFERENTIATION OF THE PERRON ROOT AND VECTOR 
If Ap € R"*" is nonnegative and irreducible, then the Perron—Frobenius the- 
orem ensures that the Perron root ro = p(Ao) is a real and simple eigenvalue 
for Ao. In addition to varying continuously (Theorem 7.4.6, page 815), it is also 
true that under small and real perturbations to Ag the Perron root remains 
real and simple (by Corollary 6.1.3 on page 760). 
Let Ns be a éneighborhood of Ap in R"*" defined by 
Ns = {A|A 
is nonnegative and irreducible and ||A — Ao|| < 6} 
(the choice of norm is irrelevant). As A varies throughout N5, its spectral 
radius r(A) varies as a real valued function of the n? independent variables 
ai; in A such that r= r(A) is differentiable at Ao and all partial derivatives 
Or/Oa;; exist for A € Ns. The proof of these facts is deferred to Theorem 8.3.2 
on page 924. Moreover, if p= p(A) and q? =q7(A) are the respective right- 
hand and left-hand Perron vectors for A € Ns, then Op/Oa;; and 0q! /dai; 
exist throughout M5. This follows because if C = adj(A—rI) 
(adjugate is 
defined on page 961), then rank (C) =1 (because r is a simple eigenvalue of 
A), and every nonzero column and row in C is a respective right-hand and 
left-hand eigenvector for A associated with 
r (see Exercise 3.2.36 on page 
322 and Exercise 9.3.12 on page 967). When normalized by the 1-norm, these 
respective eigenvectors are the Perron vectors 
p = p(A) and q? = q'(A), 
each of whose components are in terms of cofactors of A —rI. Every cofactor is 
a sum of products of entries from A—rI, each of which varies continuously with 
A and has well-defined partial derivatives. Furthermore, the spectral projector 
associated with r is G = G(A) = pq' /q'p (by Theorem 4.7.8 on page 550), 
so 0G/Oa;; also exists for A € N5. 
The present concern is not so much the differentiability of the Perron com- 
ponents, but rather the goal is to establish formulas for their partial derivatives. 
(7.7.1) 
7.7.1. Theorem. Let Ag € R"*" be nonnegative and irreducible, and 
let A vary in Ns as described in (7.7.1). If r = r(A) is the Perron 
root of A and G = G(A) is the associated spectral projector, then 
Or 
UP; 
—_—_— = Gii SS —S—_——— 
Dai 
ey 
gene 
When evaluated at Ao, the matrix of partial derivatives is 
(7.7.2) 
He 
|= (Ao)] = GT Ao) = SRE, 
qo Po 
where po and qé are the respective right-hand and left-hand Perron 
vectors for Ag. 

7.7 Differentiation of the Perron Root and Vector 
841 
Example 
Proof. 
For any matrix function f(A), adopt the short-hand notation Of in 
place of Of /Oa,;. Start with the fact that trace(G) = rank(G) =1 (because 
G is idempotent and r is simple (recall (4.7.11) on page 547), and then use 
AG =rG = GA along with trace (XY) = trace (YX) to conclude that 
trace (A [OG]) = trace (A [OG?]) = trace (a 
[G(OG) + (0@)G]) 
= trace(AG [0G]) + trace([9G] GA) 
=r trace(G[OG] + [OG]G) = r trace (O[G?]) = r trace (OG) 
=r 0 [trace (G)| = 0. 
This together with OA = e,e} yields 
dr = O[r trace (G)] = O[trace (rG)| = d|trace (AG)| = trace (O[AG}) 
= trace([OA] G + A [0G]) = trace ((9A] G) = trace (e;e; 
G) 
gentle 
Ser 
my 
SP Sr yg iP; 
q'p 
ik UkPk 
OP 
A 
u® 
0) 
@ 
@ 
il 
0 
For « > 0, lei Ang = 
|: : «. -. 
: 
|. 
This is a nonnegative irreducible 
ORO) 
ie 
Cet 
(mee 
ite 
OU 
fan 
matrix whose eigenvalues are 
{ele ene to eee 
hiere 
mci era 
(see Exercise 3.1.6, page 299). In particular, 
ro = p(Ao) = e/" and it is 
straightforward to verify that 
Po = 
" 
and) 
dy = (dn mtnrov ses ests) 
ne 1 
"9 
are respective right and left-hand positive eigenvectors for Ag corresponding to 
rp. There is a neighborhood Ns of Ao such that each A € Ns has a real and 
simple eigenvalue r = r(A), and according to Theorem 7.7.1, 
1 
TO 
ra 
Wee 
a 
=p) 
te 
1 
ro 
lies 
Or 
qoPo 
ra 
ra! 
1 
pas 
—(Ao)| 
=—=— 
= 
0 
0 
0 
aij 
Qo Po 
n 

842 
Differentiation of the Perron Vector 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
In particular, 
—(n-1) 
1 
>oo 
ase 
0. 
Ani 
nm 
nemiy/n 
The conclusion is that as € decreases, relatively small perturbations E (i.e., 
\|E|| / || Ao|| is small) that affect the (n,1) entry of Ao can have a relatively 
large effect on ro. On the other extreme, 
Or 
rir} 
e(n-1)/n 
Avy) 
= 
= ——-—0 ase 
0. 
hae 0) 
n 
n 
In other words, ro is relatively insensitive to perturbations in the (1,n) entry of 
Ao. For example, if n =4 and «= 1074, then ro = 10~'. If Ao is perturbed 
to become 
Aj +E= 
then r(Ao + E) = 10-1. The relative change in Ao is 
(Ao 
+E) = Aollg _ IIEllo _ 107*(1 — 107%) 
|| Aolloo 
I|Aollec 
but the relative change in ro is 
Ir(Ao 
+E) —r(Ao)l _ 4 _ 19-9 ~ 100%. 
~ 1074 = .01%, 
r(Ao) 
On the other hand, if Ag is perturbed in the (1,n) entry to become 
O 
ete 
t 
Ore 
LPG 
AotE=| 
9 
0 01}: 
1 
SO 
io 
0) 
then, r(Ao + E) = .10025 (to five places). This time the relative change in Ao 
is |[E||,, / ||Aoll,, =1= 100%, but the relative change in ro is only 
|r(Ao + E) — r(Ao)| 
r(Ao) 
Let the entries a;; of 
A € R"*" be n? independent real variables such that A 
is nonnegative and irreducible throughout some region, and let r, p, q', and 
G be the respective Perron root, the right and left-hand Perron vectors, and the 
associated spectral projector for A. To differentiate these terms with respect to 
aij, first note that r being simple ensures that inder(r) = 1, which in turn 
means that A—rI is a group matrix and hence has a group inverse (A —rI)*. 
It follows from the group inverse properties on pages 558-560 that 
(A —rI)(A—rI)® =(A—rl)*(A —rl) =Paa_pp,n(a-rp)> 
= .0025 = .25%. 
G = Py(a-ry),R(A-rt) 
= 1 - (A —rI)(A—rl)*, 
CA)) 
(A —rI)G =0=G(A —rI), and (A —rI)*G = 0 = G(A — rl). (7.7.4) 

a 
'oe 
7 
PW as 
er 
_ on 
ae 
a 
' 
» 
a 
: 
ae 
2 
i? 
: 
m2 — -_ ay Mes = oso i's" Fe 
ee 
a 
Proof of (7.7.5). 
As before, let Of = Of /Oaj;. Using Or = gj; from (7.7.2) on 
page 840 and 0(A) = E,; to differentiate 
0 = (A —rI)G and 0 = G(A — rT) 
yields 
0 = (Ei; —g;i1)G+(A—rlI)(0G) 
and 
0=(0G)(A-rI) + G(E,; — gjil). 
Pre-multiplying this first equation and post-multiplying the second equation by 
(A —rI)*, respectfully, and using (7.7.4) along with (7.7.3) yields 
0 = —WG + (A —rl)*#(A —rI) (OG) = —WG + (I- G) (0G) 
and 
0 = (9G) (A —rl)(A —rl)* — GW = (0G) (I- G) 
so that WG = (I—G) (0G) and GW = (0G) (I—G). Add these two equations 
and ue G = G2 
= 
0G = (0G)G+G(0G) to produce the desired 
conclusion that WG + GW=-0G. 
& 
Proof of (7.7.6). Setting 
8 = [q?(dp)/q?p] 
and using (7.7.5) along with 
Wp = 0 to differentiate p = Gp yields 
ap = (0G) p+G (dp) = (8G) pi Pt (op) = (8G) p+ 4p = Wp+8p. (7.7.7) 
Since 1 =e? p => 0=e7 (dp) and e?p =1, it follows by multiplying (7.7.7) on 
the left-hand side by e7 that 6 = —e7Wp = —a, and thus 0p = (W —al)p. 
The formula for 0q? is similarly produced (see Exercise 7.7.2 on page 849). 
The group inverse (A — rI)* provides a particularly elegant way to express 
the formulas in Theorem 7.7.2, but group inversion is not as familiar as standard 
'matrix inversion. So, for those more comfortable with traditional inversion, the 
following corollary is given to assuage their sensitivities. 

Proof. 
The nonsingularity of Z can be argued in two different ways. As noted 
earlier, inder(r) =1, so 
R(A —rI)M N(A —rI) = 0, and hence 
ee weer 
Theorem 4.2.16 on page 441 can be employed to conclude that 
rank (Z) = rank ({A — rl] + G) = rank ([A — rl] + pq™/q*p) = (n—1) +1. 
Alternately, by using the properties in (7.7.3) and (7.7.4) it follows that 
(A - rl) + G] (A -rt)# +G| =(A-—rI(A-rl#+G=I 
so that Z~1 = (A —rI)# +G, or equivalently, (A — rI)# = Z~! — G. Conse- 
quently 
W=Y+GE; 
and W=Y+E,G, 
(7.7.10) 
so, by Theorem 7.7.2, OG = YG+ GY.4 2GE;;G. To obtain the formula in 
(7.7.8), note that 
[GE,;G];; = e} Ge,et Ge; = Judi 
and 
[97¢G]i; = QyiGij- 
Since rank (G) = 1, every 2 x 2 principal minor determinant must be zero. In 
particular, det pe a ) 
=0 implies gig; = 9ij9ji, and thus GH;;G = g;,G 
so that GE,;p = g;;p. The formulas in (7.7.9) result from this by using (7.7.10) 
in (7.7.6) along with Gp =p and e'p=1 = 
e!(dp) =0 
to obtain 
dp = Wp —- ap = Yp+ GE,;p — ap = Yp + (gj; — a)p. 
Multiplication of Op = Yp + (g;; — a)p on the left-hand side by e" yields 
0=7+ (93-0) 
=> 
93;-a=~—y, and thus Op = (Y — yI)p. The second 
formula in (7.7.9) is similarly derived (see Exercise 7.7.3, page 849). 

7.7 Differentiation of the Perron Root and Vector 
845 
Example 
Let the entries of A = 
3 a 
be independent real variables such that A is 
nonnegative and irreducible throughout some region, and let p be the Perron 
vector for A. 
Problem: By direct differentiation, verify that Op/0b agrees with the formula 
given in the first equation in (7.7.6) of Theorem 7.7.2 on page 843. 
Solution: First apply quadratic formula to \? — (a + d)A + (ad — be) = 0 to 
obtain 
an 
SE 
A= = 
where 
A= ./(a+d)? —4(ad — bc). 
The Perron root r is the larger of these two. Hence the eigenvalues of A are 
d 
A 
d)—A 
SS = 
so 
2r—(a+d)=A 
and 
r—-p=A. 
Furthermore, A — rI = G . if d . 7) being singular means that 
det(A—rI)=0 = 
(a—r)(d—r)=be. 
Differentiating r? — (a + d)r + (ad — bc) =0 with respect to b yields 
Or 
Or 
Or 
Cc 
ara, — (at dja, —c=0 
a 
iP) 
Ob 
2r=(geed) A" 
Note that x = G . 3) satisfies (A — rI)x = 0. Since x is a multiple of the 
Perron vector, all components in x must be of the same sign. Because c > 0, it 
follows that r—d > 0. Normalizing x by it 1-norm produces the Perron vector 
(eae 
where 
Y=r—d-+c¢. 
p2 
Vv 
Cong, 
Differentiate the second component of p to get Op2/0b = —c?/Av?. There is 
no need to differentiate the first component because e'p=1 = > e70p=0, 
and thus 0p;/0b = c?/Av? so that 
e) 
2 
1 
a % a a? 
eae) 
Now corroborate this by evaluating terms in the first expression in (7.7.6) on 
page 843. Since rank(A —rI) = 1, (4.7.30) on page 559 says that 
# 
(A —rI) 
SSE? Pe a—T 
b 
ad; ae =F 
b 
(A —rI) 
~ [trace(A — rI)|? at 
c 
foal 
alk c 
ey 

846 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
and since A is diagonalizable (because it has distinct eigenvalues), the spectral 
projector for r is obtained from Theorem 4.7.9 on page 551 as 
q- AH ATH 
(ot 
b ) 
Por nee 
We 
oa 
c 
d-p)- 
Notice that go, = c/A = Or/db, which corroborates the result in (7.7.2) on 
page 840. Now, 
W = -(A~rI)*Ey2 = - [(A-rI)*],, of = 
€ aan) 
Ww 
(a—r) 
ak 
—c 
9 iy J 
p=—[(a-#= ae (n= ( ) 
and 
a =e! Wp = —-la—¢0) 
=> 
0p = 
A2py 
—c(a—r +c) (Raa 
A'y 
c/v 
so, with the aid of some algebra and recalling that A = 2r — (a +d), the first 
component in Wp — ap is seen to be 
Cc 
a—r+c)(r—d 
c 
[cA 
Cc 
ltrs 
ea a 
a 
Vv 
~ A2y | yp | Ap?' 
which agrees with the first component in (7.7.11). Similarly, the second compo- 
nent in Wp — ap is 
patie' 
nee ens See) oe) 
io 
wes 
ais _ -¢ 
sald 40s 
Vv 
- Aa 
|e |e Aree 
which is the same as the second component in Op/Ob, and thus these results 
corroborate the fact that Op/0a;; = (W —al)p given in (7.7.6). 
Differentiation With Respect to a Parameter 
It is not uncommon to encounter real matrices whose entries all vary with a single 
= 
2 
parameter t € R such as in A(t) = (ae +e) for t € (0,1). In general, if 
the entries a,;(t) of a variable matrix A(t) are each differentiable functions of 
t over some interval (a,b), and if A(t) is nonnegative and irreducible in (a,b), 
then, by the same reasoning alluded to on page 840, it can be shown that the 
Perron root r(t) and the Perron vector p(t) for A(t) are both differentiable 
functions of t. Expressions for the derivatives r' = dr/dt and p' = dp/dt 
can be derived from scratch by the same logic used to produce the results in 
Theorems 7.7.1 and 7.7.2, but they also follow as corollaries of these theorems. 

Proof. 
Differentiate the composite function r = r(ai1(t), @12(t),...,@nn(t)) by 
applying the chain rule with the aid of Theorem 7.7.1 on page 840 to produce 
Or 
Gia; ;P; 
q' A'p 
ee 
r= 5 —a, 
9450; 
eas = 
; 
2a 
4 ze i ee 
q'p 
q'p 
4,9 
Employing this with e7p = 1 nee 
Cd 
pir 
arp | 
= es] A'p=GA'p. 
=> 7'=e'GA'p. 
(7.7.15) 
To obtain (7.7.13), apply the first Suen in (7.7.6) on page 843 along with 
A'= ae q;,;, Bi; and set B i 
ae Qi, 50% to write 
O 
p= 
aly = -(W - al) pai = Bp— D(A -rI)* Epa 
ie! 
iJ 
i, 
= Bp —(A-—rI) #) aie u|P= Bp —(A—rl)*A'p. 
inj 
Since e*'p=1 => e7p/=0, it follows that 8 =e7(A—rI)*#A'p. Equation 
(7.7.14) is derived from (7.7.13) by using Z| = (A—rI)*# +G 
along with 
GA'p=pr' (from (7.7.15)) to obtain 
p' = Bp — [Z-'-G] A'p= (8 +r')p—Z'A'p=fp-Z AP, 
where 
£=8+r'. Applying e'p' =0 
to this yields € =e"Z~*A'p, and thus 
peep -Z Ap. 
i 

848 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
Example 
To illustrate the results in Corollary 7:7.4, consider the nonnegative and irre- 
3 
2 
ducible matrix A = A(t) = ts 
9 p fae 
for t € (0,1). The Perron root is 
r=r(t)=t?—t+1 
(from the quadratic formula), and it easily follows that the 
respective right-hand and left-hand Perron vectors are nt) =p = (1 /2)(4 ) 
and q(t)? =q! =p'. Furthermore, 
Pe feoke 2 
ee het 
Wee 
a 
Ae Bale e=2(3 al fe Teal 1 yk 
appetite 
ee a oe 
(A —rlI) eel 1 =) (see page 559), and Z = ( 4/2 2 51/3): 
Consequently, 
q'A'p _ a2 (ar ") (a) 
= (1, 1) a 
= 9 —1, 
Ge 
pa 
fz 
(2¢ —1)/2 
ena ea) giz in ies a i) 
(1/2, 1/2) 
-0.n(gt 2) (A= 
each of which agrees with the fact that r' = 2t —1. To illustrate (7.7.13) and 
(7.7.14), note that A'p = 'es meni 
= (t-1/2) (F) = (t—1/2)e, so 
ee 
A? 
a, oe ify 
(St 
1 
TP 
( 
el) Ap 
is 
iy 
1 
=—a() 
=> 
fp=e'(A-rI)*A'p=0 
— 
Bp-—(A—rI)*A'p 
at 
ph, Oe 
ee UE eee ANS 
eS 
wo 
(ales 28 
(1) = €-1/2¢ 
=> £=e'Z'A'p=2t-1 
=> &p—Z'A'p 
= (2t—1)p—(t-1/2)e= 
and 
each of which agrees with the fact that p' = 0. 

7.7 Differentiation of the Perron Root and Vector 
849 
Exercises for section 7.7 
+ 
7.7.1. Let 
Z= A—rI+G in which A > 0 is irreducible, r is its Perron root, 
and G is the associated spectral projector. Prove that if (A —rI)* is 
the group inverse of 
A—rI, then Z~' =(A—rI)*#+G. 
7.7.2. Under the conditions in Theorem 7.7.2, derive the second equation in 
(7.7.6)—i.e., show that dq? = q?(W — aI), where &@ = q? We. 
7.7.3. Provide the details to derive the second formula in (7.7.9)—i.e., show 
that Oq™ = q'(Y — 41), where 
¥ = q' Ye. 
7.7.4. Deutsch-Neumann Second Derivative Formula! Let the entries ay; of 
A €R"*" be n? independent real variables such that A is nonnegative 
and irreducible throughout some region, and let r, p, gq', and G be 
the respective Perron root, the right and left-hand Perron vectors, and 
the associated spectral projector for A. 
(a) Show that 
O?r 
ea (A 
-rD 
fi, 
ou + oie(A — DF] - 
(b) For matrix functions f(A), let 0°f/07, denote the second partial 
derivative of f with respect to a;;. Show that the second partial 
derivative of r with respect to aj; is 
Or 
yn —295:(A — aie, 
a 
In the mid-1980's Emeric Deutsch and Michael "Miki" Neumann (1946-2011) focused on de- 
veloping the use of the group inverse in Perron—Frobenius theory, and they derived this second 
derivative formula. Emeric Deutsch was a mathematician at the Polytechnic Institute (now Pro- 
fessor Emeritus at the NYU Tandon School of Engineering in Brooklyn, New York), and Miki 
Neumann was the Stuart and Joan Sidney Professor of Mathematics and Board of Trustees 
Distinguished Professor of Mathematics as well as head of 
. 
Ry 
the mathematics department at the University of Connecti- 
cut. Born in Jerusalem, Miki earned his B.Sc. from Tel Aviv 
University in 1970 and his Ph.D. in computer science from 
London University in 1972. After teaching for a year at the 
University of Reading, England, he took a position at the 
Technion-Israel Institute of Technology in Haifa, Israel, after 
which he spent five years at the University of Nottingham 
in England. In 1980 he brought his family to the University 
of South Carolina in the United States, where he spent five 
years before settling down to his productive career at the 
§& 
a 
University of Connecticut. 
Mik NEUMANN 

ae radi 
: 
a 
ne 
; habeas 
wo 'nclbaiatedat a: omeg y's 
"at » hin F, 
4) 
i3 fagl iia hee ene 
Mir Lasar "0 alc' hypt Daeg t? hy 
a ie ard, 
"ay 
rary 
Wy ay heey 
aft? 
ry 
Mie 
Lk ee ae 
ee 
haverl. 
dled 
i? 
A 
wy 
Tuy 
ret 
,elCi 
waa 
Ont 
: 
ai 
7 
< 
A 
a 
a 
i 
: 
' 
wey 
* 
(6) 
oy 

7.8 Perron Complementation 
851 
7.8 > PERRON COMPLEMENTATION 
The purpose of this section is to establish the remarkable fact that the Perron 
vector of an irreducible matrix A > 0 can be obtained by gluing together Perron 
vectors of smaller matrices called Perron complements. 
7.8. 1. 
'Definition. Let . e pon 
nonnegative and irreducible with 
spectral radius. i =F) and oe as Ea 
a ao ae ay 
ce ee oe 
ae 
A 
(7.8.1) 
\Am Aes Age / 
7 
Le 
in which all diagonal blocks A;; are square. Define R; C,, and Z; 
to be the respective complements of A, in the i'" block-row, the . 
block-column, and in A—i.e., R; and C; respectively denote the i" 
row and 7" column of blocks with Aj; removed, and Z; is the principal 
submatrix of A obtained by deleting the i'" row and i" column of 
— 
blocks. The Perron complement of Aj; is defined to be 
P, = Ay + R,(rI — Z,) *C,. 
(7.8.2) 
The nonsingularity of rl — Z,; is established below in Theorem 7.8.2. 
Example 
=. 
: 
: 
TANG 
Pore 
— 
(An 
Are 
If A > 0 is irreducible with p(A) =r 
is partitioned as A = ee esol 
then 
R; =Ai2, 
Ci =Aa1, Zi = Avo, 
Ro = Aoi, 
Co=Ai2, 
Zo= Ai. 
The two Perron complements are 
P, = Ay + Aio(rI—Age)'Ao 
and 
P2 = Ag+ Aai(rI— Ait) 
An. 
Ayi 
Aiz 
Aus 
If A is partitioned as A = (a: Ao An). then there are three Perron 
Asi 
YAsz 
"Ass 
complements, and the second one is 
% 
TAS 
eA 
Ay 
P»5 = Ado = Ro(rl = Z2) eC = (Aoi A2s)(" NY, 
rl — ies ) 
OR: i 
The other complements, P; and P3, are similarly formed. Although Perron 
complements are not the same as the Schur complements defined on page 167, 

852 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
. 
2 
c 
A 
A 
there is a connection. For example, if A = ( 
ims xe \: then 
Pi= 
ri - [Schur complement of (rI — Ag2) in rI — Al, 
and similarly for Po. 
e 
Notice that each Perron complement P; can be neatly described by saying 
"shift, complement, and shift," which mean the following: 
(1) Shift A;; by rI. 
(2) Form the i" Schur complement of the shifted matrix. 
(3) Then shift the result by rl. 
Why Is It Called the "Perron Complement?" 
The reason for the terminology is because all Perron complements inherit "Per- 
ron properties" from their parent matrix. In particular, if A is nonnegative and 
irreducible, then so is each Perron complement P; derived from A. Further- 
more, p(A) = p(P;) =r € a (P;) foreach 7. And, most importantly, the Perron 
vectors of the P,;'s combine to form the Perron vector of the parent matrix A. 
Before these facts can be established a few preliminary results are needed, some 
of which are of interest in their own right. 
In order for each Perron complement P; to be well defined, the existence 
of (rI—Z,)~! must be ensured. This, along with the fact that (rI—Z,;)~! > 0, 
is the point of the following theorem. 
7.8.2. Theorem. 
If A,,, > 0 is an irreducible matrix with p(A) =r 
that is partitioned as in (7.8.1), then 
s fZoer. 
(7.8.3) 
e 
rI—Z,; is nonsingular, and (rI — Z;)~! > 0. 
(7.8.4) 
In other words, rI — Z; is an M-matrix (see page 622). 
Proof. 
To prove that p(Z;) <r, suppose to the contrary that p(Z;) >r. If Q 
is the permutation matrix such that 
Z; 
C; 
rg 
' 
a 
Z; 
8) 
Q7AQ = & co =A; 
-andit B= ( 
). 
(7.8.5) 
then A is also irreducible, and p (A) i 
iS (B) 
. Furthermore 
A >B =.0 
implies r > p(B) (by Corollary 4.11.6, page 621), and hence r = p (B) = —( Za). 
But this impossible. To see why, recall that (7.2.8) in the Frobenius theorem 

7.8 Perron Complementation 
853 
on page 800 guarantees the existence of a vector 
v > 0, v # 0, such that 
[=e 
Vv 
. 
. 
> 
Ziv = 7 V, "80°27 = & 1s a nonnegative nonzero vector such that Bz = rz. 
However, A >B = 
Az > Bz =rz. Corollary 7.4.3 on page 811 now forces 
Az=rz 
and 
2z>0, 
which contradicts the fact that z= (ak 
Thus p(Z;) <r. Corollary 4.11.7 on page 622 now guarantees that rI — Z;, is 
nonsingular and (rI—Z;)"'>0. 
J 
Inherited Perron Properties 
Some of the properties that Perron complements inherit from their parent matrix 
can now be established. 
7.8.3. Theorem. If Aa > 0 is an irreducible matrix with p(A) = ro 
_ that 
is partitioned as in (28-1), and if P; is the i*" Perron complement, 
hen 
| 
— 
: 
i 
© P,>0 forevery i, 
(7.8.6) 
e 
Pp, is irreducible for every 1, 
(7.8.7) 
- p(P;) =réo (P;) for every 1. 
(7.8.8) 
Proof. 
P; > 0 because 
(rI — Z;)~' > 0 (by (7.8.4)), and all of the other 
terms in P; are nonnegative. Before proving that P,; is irreducible, show that 
réoa(P;). Let p; = ce) be the partitioned right-hand Perron vector for the 
nonnegative irreducible matrix A given in (7.8.5) so that (rI— A)p,; = 0. In 
other words, 
Consequently, 
(rita? TCR aaa )() = (0) 
TSAy 
(Ci 
i 
\(0 
i 
- 
'ic 0 
ge WEA = 
(rI-P,)yi=0, 
(7.8.9) 
so (r,y;) is a right-hand eigenpair for P; with y; > 0. A similar argument 
shows that there is also a left-hand eigenpair (r,w/) for P; with w/ > 0, and 
hence r € 0 (P;). Next establish that r is a simple eigenvalue of P;. Begin with 
the fact that the Perron—Frobenius theorem insures 
r is a simple (and hence 

854 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
semisimple) eigenvalue for Ann as well as for Neer in (7.8.5). Let n=stt, 
where rl—Z; is 
sx s and rlI—P; is txt. Since rI— Z; is nonsingular and 
I 
0\/rlI-Z 
—-C; 
I healed in we 
0 
) 
ars 
po 
H 
UJ 
R,(rI 
— Z,;)-! I 
-R, 
rl—- Ax 
0 
I 
0 
i Cl 
it follows that rank (rI— Z;) =s and 
1 = alg multx (r) = geo multx (r) = dim N(rI — A) =n-—rank(rI — A) 
=n— [rank(rl — Z;) + rank(rI — P)| 
= is 
— rank(rI — Zi)| + E 
— rank(rI — P;)| 
(7.8.10) 
= E 
— rank(rI — P,)| = dim N(rI — P;) = geo multp, (r). 
Now show that alg multp, (r) = geo multp, (r) by means of stochastization. In 
other words, transform P; into a row-stochastic matrix (page 636) T by using 
y2 
the eigenvector y; = ( > 0 of P; from (7.8.9) to define 
Ut 
Yt 
Since p(T) =1¢€o0(T), and since T and P;/r are similar, 
p(Pi) = p(r'T) =rp(T) =r. 
Moreover, all multiplicities of eigenvalues A € 0 (T) agree with their counter- 
parts A/r € a (P;). All unit eigenvalues of T are semisimple (Corollary 4.11.12, 
page 636), so all eigenvalues on the spectral circle of P; must also be semisimple. 
Combining this with (7.8.10) above proves that 
alg multp, (7) = geo multp, (r) =1 = + r isa simple eigenvalue of Pj. 
The irreducibility of P; is now a consequence of Theorem 7.4.5 on page 814. 
Coupling Perron Vectors 
The developments in the proof above are more important than they might seem 
because they reveal the fundamental relationship between the Perron vectors 
of A and the Perron vectors for the various Perron complements P;. If the 
Pi 
for A is partitioned conformably with 
right-hand Perron vector p = ( 
Pk 

7.8 Perron Complementation 
855 
the partition of A in (7.8.1) on page 851, then the nature of the permutation 
in (7.8.5) on page 852 makes it clear that p; = y;, where y; > 0 is the vector 
in (7.8.9) on page 853. Normalizing y; produces the Perron vector for P; as 
ys 
yi 
Di 
mT 
2 
= 
SS ee 
Oo 
= 
es, 
Where 
€;=—e°p;. 
(7.8.11 
i 
llyill, 
ery, 
el p, 
Pi = i 2; 
&i 
pi. 
( 
) 
In other words, the right-hand Perron vectors z; of smaller Perron complements 
can be "glued" together to build the right-hand Perron vector of A by forming 
€1Z1 
£222 
p= 
ak: 
(7.8.12) 
fnzk 
A similar analysis shows that this works on the left-hand side as well—ie., if 
q' =(q/,...,q}) isthe left-hand Perron vector for A, and if w/(rI — P;) =0 
so that x = wi i wile is the left-hand Perron vector for P;, then a = 14%; 
with n; =qie so that 
Qeeskgxt ox, 
-"- 
TeXE 
(7.8.13) 
The bits of "glue" are the scalars £; =e? 
p; and 7; =qje, but they depend on 
p; and qi. However, there is a clever way out of this dilemma because these 
"slue bits" can be manufactured as shown in the theorem below. 
7.8.4. Theorem. (Coupling Theorem) Let A,,,,, > 0 be irreducible with 
p(A) =r that is partitioned as shown in (7.8.1) on page 851, and let 
D, a; 4, ae be the respective right- and left-hand Perron vectors 
for A and P,;. The 
kx k matrices Cp and Cz, whose (t,7) entries 
are 
[Crlij = e' Aj 52; 
and 
[Cx]; = x? Ajje 
(7.8.14) 
are called the right- and left-hand coupling matrices, respectively, 
and they inherit the following properties from A. 
e 
Each is nonnegative and irreducible with spectral radius r = p(A). 
£1 
-e 
The right-hand Perron vector € = 
for Cpr and the left- 
bk 
hand Perron vector 7' =(m 
-:: 
mm) for Cy are called coupling 
vectors because the respective right- and left-hand Perron vectors 
121 
for A are given by p= 
and qi =(mx!l 
-- mx, 
). 
EkZk 

856 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
Proof. 
The proof is for the right-hand statements—proving the left-hand state- 
ments is similar. Clearly, 
Cr > 0 because each term cj =e T AigZ; is nonneg- 
ative. Since cj; = 0 <= > Ajj; = 0, it follows that Cr must be irreducible (if 
Cr could be permuted to a block triangular form, then so could A). To prove 
the rest of the theorem, let 
e! 
0O 
0 
z 
O 
0 
(8) 
el 
~e 
9) 
0 
Z2 
Pact 
Oo 
IR 
—— 
. 
Chavek 
Abe 
deans 
Sen 
fz 
(7.8.15) 
OO: 
£Ge 
tty 
a) 
en 
O° 
(0: 
= 
3p / ye 
These are examples of special purpose transformations known respectively as 
restriction and prolongation operators because when n>k, R "restricts" 
n- 
tuples down to k-tuples while L "prolongates" 
k-tuples back up to n-tuples 
in an inverse-like manner since RL = Ix. It follows from (7.8.11) that L€ = p 
and Rp = 6, and since Cg = RAL, 
Cr& = RALE = RAp = Rirp) = r€. 
Now, € > 0 (because p; > 0 for each i), and e'€=e7Rp=e'p=1, 
so 
= p(Cpr) and hence € is the Perron vector for Cr. Thus 
£121 
p= 
eae 
(by (7.8.12)). 
EkZk 
7.8.5. Corollary. If A = Os 
is just a two-level partition with 
a1 
Ado 
Perron complements P; and P» whose respective Perron vectors are 
Z, and Z2, then the right-hand Perron vector for A is p = (2 
222 
in which 
Ci2 
C= Ce 
iy = —————__ 
and 
f= 
= 
Se 
where ¢;; = e* AjjZ;. 
Proof. 
In this case the coupling matrix Cp = (a i) is only 2 x 2, and 
C21 
€22 
it is straightforward to verify that the two components in its Perron vector 
g= ay are given by (7.8.16). 

7.8 Perron Complementation 
857 
Example 
The matrix 
is irreducible with p(A) =7, and the two Perron complements are 
EE 
Tf 
YW 
} 
P, = Ai == Aji2(71 == Ao?) Aoi = 7 ae at 
; 
with p(P1) = e 
and 
eee 
aN 
eta 
a 
2 
22 + Aoi (7I 
Aji) 
Ajo 
= (30 99)? 
with p(P2) 
16: 
The respective right-hand Perron vectors for P; and Po are 
»=(f)»-(%8) 
The right-hand coupling matrix is 
e?Aiiz1 
ef 
Ayoz 
ay 
cot ts 
Peale 
i} with p(C) =7, 
e Aoi Z1 
ef! Anz 
BA 
and the coupling vector (the Perron vector of Cr) is € = Go Thus the 
right-hand Perron vector of A is 
ue) II 
bol 
rR CR. 
N 
N wo 
= Se 
II 
(ory 
es 
rb 
bw 
Fr 
What Is the Optimal Value of k? 
The granularity of the partition of A in (7.8.1) is an issue worthy of consid- 
eration. As the number of diagonal blocks increases and the partition becomes 
finer, the sizes of the Perron complements P; become smaller, thereby making 
it easier to determine the Perron vectors of the P;'s. But then the order of the 
matrix inversion embedded in each complement becomes larger, and the size of 
the coupling matrix [C Rilkxk becomes larger. In the extreme cases when k=n 
or k = 1, there is no uncoupling of the Perron vector whatsoever—i.e., if k =n, 
then Cr =A, andif k =1 
(no partition at all), then P; = A. The optimal 
partition is that which best suits the needs of the underlying application. If the 
value of & is not driven by an application, then setting k = 2 makes sense, 
especially in light of the fact that a divide-and-conquer strategy can then be 
efficiently implemented as illustrated in the example for stochastic matrices on 
page 886. 

858 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
Exercises for section 7.8 
7.8.1. 
Toe. 
7.8.3. 
7.8.4. 
7.8.5. 
7.8.6. 
Construct an example to show that a nonnegative irreducible matrix A 
can be primitive without all of its Perron complements being primitive. 
While primitivity of an irreducible matrix A > 0 is not sufficient to 
guarantee all of its Perron complements are primitive, show that if an 
irreducible diagonal block A;; in A is primitive, then the corresponding 
Perron complement P; must also be primitive. Give an example to show 
that the converse is not true, even if Aj; is irreducible. 
While Perron complements need not be primitive, most of them are. 
Show that if A > O is irreducible and A,;; has at least one nonzero 
diagonal entry, then the corresponding Perron complement P; must be 
primitive. 
Consider a nonnegative irreducible partitioned matrix and its associated 
coupling matrix 
Ati 
- 
Arg 
eTAi1Z1 
++: 
eT AigZp 
Asst | OE OO 
a 
and 
C= 
Arr 
++: 
Akk 
eTAgizi 
«+: 
eT AgRZE 
Prove that A is primitive if and only if C is primitive. 
Consider a general A € F"*" partitioned as 
A = 
(4 
412) 
with 
Azo1 
A22 
square diagonal blocks, and define A-complements to be 
Pi 
(A) = An + Aio(AI — Age)7'Ao1 
when A ¢ 0 (Ago), 
and 
P2(A) = Ao2 + Aoi (Al —— Ai1)7* Age 
when A ¢ Oo (Aq1) : 
Extend Theorem 7.8.3 on page 853 by proving that if (x, ) is an eigen- 
pair for A in which x = Cul then (A,x;) is an eigenpair for P;()) 
whenever P;(A) exists. 
Bin-1)x(n-1) 
© 
d* 
a (AI 
— B)~1!c 
1 
LetwAys a= ( 
) 
, and suppose that A € a0 (A) but 
A ¢o(B). Show that x = ( 
) 
is an eigenvector for A 
associated with X. 

7.9 Markov Chains 
859 
7.9 MARKOV CHAINS 
One of the most elegant applications of Perron-Frobenius theory is the alge- 
braic development of the theory of finite Markov chains by means of stochastic 
matrices. Recall from the discussion on page 636 that a matrix P € R"*" is 
stochastic (alternately, row stochastic) whenever P > 0 (entrywise) and all row 
sums equal 1—i.e., Pe = e, where, as usual, e is a column of 1's. If all column 
sums equal 1 (i.e., e7P = e"), then P is said to be column stochastic, and when 
the row sums as well as the column sums are both equal to 1, (i.e., e"Pe = 7 
P is called doubly stochastic. Unless otherwise stated, all stochastic matrices in 
subsequent discussions are row stochastic, so the prefix "row" is omitted from 
the terminology. 
Recall from page 636 that if P is stochastic, then \ = 1 is a semisimple 
eigenvalue with an eigenvector e and ||P||,, = 1 = p(P). Furthermore, P 
being stochastic forces P* to be stochastic for each k > 0. As will be seen, 
every stochastic matrix defines a Markov chain (defined below), and conversely. 
ae 9.1. Definition. A Markov' chain is a stochastic process (a set of ran-- 
dom variables {X;}?29, where X; has the same range {51,59,..., Sn}, 
called the state space) that satisfies the Markov property 
oe 
PX S15 
oS 
8) 
= PX = 5, X, = 5) 
for each 
t= 0,1,2,.... 
Interpret this as a sequence of random events occurring 
at discrete times 
t = 0,1,2,..., where the state of the process X;4; at time 
t+ 1 is 
independent of past behavior—it only depends on the state at time t. 
: Andrei Andreyevich Markoy (1856-1922) was born in Ryazan, Russia, and he graduated from 
Saint Petersburg University in 1878 where he later became a professor. His early study was num- 
ber theory because that was the interest of his well-known teacher 
Pafnuty Lvovich Chebyshev (1821-1894). But when Markov dis- 
covered that he could apply his knowledge of continued fractions 
to probability theory, he embarked on a new course that would 
make him famous—enough so that a lunar crater was named in his 
honor in 1964. In addition to being involved with liberal political 
movements (he once refused to be decorated by the Russian Czar), 
Markov enjoyed poetry, and in his spare time he studied poetic 
style. It was therefore no accident that led him to analyze the distri- 
bution of vowels and consonants in Pushkin's work, Hugene Onegin, 
by constructing a simple model based on the assumption that the 
probability of a consonant occurring at a given position in any word 
should depend only on whether the preceding letter is a vowel or a 
consonant and not on any prior history. This was the birth of the 
Ron aroy 
"Markov chain." Markov was wrong in one regard—he apparently 
believed that the only real examples of his chains were to be found in literary texts. But 
Markov's work in 1907 has grown to become an indispensable tool of enormous power. It 
launched the theory of stochastic processes that is now the foundation for understanding, 
explaining, and predicting phenomena in diverse areas such as atomic physics, quantum theory, 
biology, genetics, social behavior, economics, and finance. Markov's chains serve to underscore 
the point that the long-term applicability of mathematical research is impossible to predict. 
TT ne een gr et eR fre 
St 
ero yas 74 0 Ya Iie 

860 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
e 
To emphasize that time is considered discretely rather than continuously the 
phrase "discrete-time Markov chain" is often used, and the phrase "finite- 
state Markov chain" is used to emphasize that the state space is finite rather 
than infinite. 
An easy way to conceptualize a Markov chain is to consider a mouse that 
moves randomly through a maze of chambers $1, 52,...,Sn- These are the states 
of the process. The random variable X; is the chamber occupied by the mouse at 
time t. The Markov property asserts that the process is memoryless in the sense 
that the chamber (or state) occupied at the next time period depends only on 
the current state (or chamber) and not on the mouse's past movements. In other 
words, the mouse obeys the Markov property when it is not using its memory, 
provided it has one—in fact, experimenting with a real mouse and comparing 
the observations to those from a Markov chain model can indicate whether or 
not the mouse is actually exhibiting memory. 
Every Markov chain defines a stochastic matrix, and conversely. Let's see 
how this happens. The value p;;(t) = P(X; = S; | Xt-1 = S;) is the probability 
of being in state S; at time ¢ given that the chain is in state S; at time t—1. 
This is called the transition probability of moving from 5S; to S; at time t. The 
matrix of transition probabilities P,.,(t) = [p,;(t)] is a nonnegative matrix, 
and a little thought should convince you that each row sum must be 1. In other 
words, P(t) is a stochastic matrix. 
When the transition probabilities do not vary with time (ie., pi; 
(t) = pi; 
for all t), the chain is said to be stationary or time-homogeneous, (or just ho- 
mogeneous), in which case the transition matrix is a constant stochastic matrix 
P = [p,;]. Our focus is primarily on these kinds of chains. Conversely, every 
stochastic matrix P,y, defines an n-state Markov chain because the entries 
pi; define a set of transition probabilities that can be interpreted as a stationary 
Markov chain on n states. 
A probability distribution vector is defined to be a nonnegative row vector 
p' = (pi, p2,.--,Pn) such that 2, Pk = 1. (Every row in a stochastic matrix 
is such a vector.) The k*" step probability distribution vector (hs 172 02 
tea 
an n-state Markov chain is defined to be 
p'(k) = (pi(k),p2(k),...,Pn(k)) 
in which 
p;(k) = P(X, = 94). 
In other words, p;(k) is the probability of being in the j*" state after the kth 
step, but before the (k+1)% step. The initial distribution vector is 
p' (0) = (p1(0), p2(0),-..,Pn(0)) 
in which 
p;(0) = P(Xp = Sj) 
is the probability that the chain starts in Mei 

7.9 Markov Chains 
861 
Primary Objectives 
The major goals in analyzing a Markov chain are: (1) Understanding the tran- 
sient behavior of the chain. (2) Describing the limiting behavior for the process. 
Specific objectives can be articulated as follows. 
e 
Given an initial distribution vector p'(0), describe the k*" 
(7.9.1) 
step distribution p7(k) for each finite value of k = 1,2,... 
<a 
e 
Determine whether or not limp... P* and limg4., p'(k) ex- 
(7.9.2) 
ist, and when they do, determine the value of each. 
i 
e 
Corollary 4.11.12 on page 636 ensures that all stochastic matri- 
ces are Cesaro summable, so if limz+4, P* and limyz-+. p' (k) 
do not exist, then the Cesaro limits 
r 
a 
a 
oa 
bat 
tim | 
PLO) +P7) +--+ PT (k 3] 
== 
ibion 
k—-co 
(7.9.3) 
k-00 
k 
I+P+---4+P*-1 
p' (0) fioPapcPe 
always exist. The goal here is to interpret the meaning of this limit. 
Let's start with the statement in (7.9.1). 
792. Theorem. For a Markov chain with transition matrix P,,., and = 
initial distribution vector p'(0), the k*" step distribution vector is 
oe (pi(k), p2(k), . ple) pT OP 
| 
(7.9.4) 
e 
Moreover, the (i,7)-entry in P* is the probability of moving from 
5; to S; in exactly k steps. For this reason, P* is called the 
k* step transition matriz. 
Proof. 
Elementary probability ensures that P(E V F) = P(E) + P(F) when 
E and F are mutually exclusive events, and the conditional probability of E 
occurring given that F occursis P(E| F) = P(EAF)/P(F), where the symbols 
A and V denote AND and OR, respectively. To describe the j*" component 
p;(1) in the first-step probability vector p' (1), write 
p;(1) = P(X1=S;) =P |xi=5; NO GaN Gaara Xo=Sn)| 
=P (mi=S; REG R(X 
= 5) pass) Vow V (X= Sy A Xo=Sn)| 
n 
n 
Z. Se eck IN Xo=Si| >> P|Xo = si PIX, Sn si| 
4=1 
L i= 

862 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
This reveals what to expect after one step when starting with p7 (0). Next, 
moving from p'(1) to p?(2) the "no memory" Markov property means that 
the chain essentially starts over but now with p'(1) as the initial distribu- 
tion. In other words, p?(2) = p?(1)P. Similarly, p'(3) = p'(2)P, and so on. 
Successive substitution yields 
p'(k) = p"(k—1)P = p"(k—2)P? =--- = p7(0)P", 
which proves (7.9.4). Setting p7(0) =e? yields p'(k) = e/P* = [P*],,, so 
the probability of moving from 5; to S$; in exactly k steps is p; (k) = Lag 
In other words, the (i,j)-entry in P* is the probability of moving from S; to 
S; in exactly k steps. 
W 
Limiting Behavior 
Analyzing the limiting behavior of a Markov chain is facilitated by dividing the 
class of stochastic matrices P associated with each chain into three mutually 
exclusive categories. 
Case 1: P is Irreducible and Primitive 
Case 2: 
P is Irreducible and Imprimitive 
Case 3: 
P is Reducible 
Case 1 — Irreducible and Primitive 
Suppose that P is irreducible and primitive. The Perron root is r = p(P) = 1, 
and Pe = e implies that e/n is the right-hand Perron vector for P. Let 
mw? =(1,72,.-.,%mn)? >O be the left-hand Perron vector so that m7e = 1. 
Theorem 7.5.2 on page 820 guarantees that limz_,., P* exists if and only if P 
is primitive, in which case 
T1 
72 
Aga 
cash: Cutie OLN tat ed 
ae 
a 
jim P a Gees FPN er 
= 
a 
dol 
peel 
(7.9.5) 
mclpsees uw 
where G is the spectral projector associated with r = 1. This means that a 
limiting probability distribution exists because 
lim p'(k) = lim p'(0)P* = p7(0) lex" | 
k— oo 
k—-00 
(7.9.6) 
[p7 (0) ela 
ar 
I 
Notice that because >?, px(0) = 1, the term p'(0)e drops away, so one major 
conclusion is: 
e 
The value of limg+op'(k) = 7? is independent of the initial distribution 
p'(0). This is to be expected in light of the Markov property. 

7.9 Markov Chains 
863 
Example (Mouse in a Maze) 
Consider the Markov chain defined by placing a mouse in the three-chamber box 
with connecting doors as shown in Figure 7.9.1, and suppose that the mouse 
moves from the chamber it occupies to another chamber by picking a door at 
random—say that the doors open each minute, and when they do, the mouse is 
forced to move by electrifying the floor of the occupied chamber. 
#2 
" HEH 
=r) 
EA ae 
FIGURE 7.9.1: MOousE MAZE 
#3 
If the mouse is initially placed in chamber #2, then the initial distribution vector 
is p'(0) = (0, 1, 0) = e3. But if the process is started by tossing the mouse 
into the air so that it randomly lands in one of the chambers, then a reasonable 
initial distribution is p'(0) = (.5, .25, .25) because the area of chamber #1 is 
50% of the box, while chambers #2 and #3 each constitute 25% of the box. The 
transition matrix for this Markov chain is the stochastic matrix 
oO 
12 
172 
Bre (11 0 
2/2). 
(7.9.7) 
WES OE 
(0 
It is apparent that P is irreducible, and a standard eigenvalue calculation reveals 
that o(P) = {1, —1/3, /,—2/3}, 
so P is nonnegative and irreducible with 
r = p(P) =1, and e/3 
is the right-hand Perron vector. Consider for example 
the third-step transition matrix 
Diy 
gas 
ge 
Pp? = (z/21 2/9 
usar). 
(7.9.8) 
i) gen 14/27 
2/9 
If the mouse starts in chamber #2, then the probability of being in chamber #1 
after three steps is [eels = 7/27. Alternately, starting in ##2 means that the 
initial distribution vector is p' (0) = (0, 1, 0) = e4, in which case the third-step 
distribution vector is 
Deol p (0)Pi— e4 P2 217/27, 2/9, 14/27): 
On the other hand, if, as suggested earlier, the mouse is tossed into the air so 
that it randomly lands somewhere in the maze, then the initial distribution is 
p? 
(0) = (1/2, 1/4, 1/4), and the probability of finding the mouse in each of the 
respective chambers after three moves is 
p" 
(3) = p? (0)P? = (13/54, 41/108, 41/108). 

864 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
In particular, the probability of the mouse being in #1 after three moves is now 
given by 
: 
p? 
(3) = 13/54. 
The transition matrix P in (7.9.7) clearly passes the Frobenius test (page 821) 
for primitivity because, as seen in (7.9.8), P® > 0 and hence there exists a 
limiting distribution. In fact, (7.9.6) shows that 
lim p?(k) = 727 = (m1, 72, 73) = (1/4, 3/8, 3/8) 
k—-00 
= the left-hand Perron vector for P, 
1/4 3/8 3/8 
and (7.9.5) shows that limp... P* = @ 3/8 x/8). 
1/4 3/8 3/8 
The limiting probabilities 7; can be heuristically interpreted as the ex- 
pected fraction of time the mouse spends in each respective chamber in the long 
run because after a finite number of moves the probability of being in chamber 
j settles down to be essentially the constant 7;. For example, in the long run 
the mouse is expected to spend 37.5% of the time in chamber #2. A rigorous 
justification of this interpretation is given on page 865. 
Case 2 — Irreducible and Imprimitive 
If P is irreducible and has h > 1 eigenvalues on the unit (spectral) circle, 
then 
limgz_.. P* cannot exist (Theorem 7.5.2, page 820). This means that 
limp-+co p? (k) cannot exist because taking p'(0) = e? for each i would in- 
sure that P* has a limit. However, each eigenvalue on the unit circle is simple 
(Theorem 7.6.2, page 826), so Theorem 4.11.11 on page 635 guarantees that P 
is Cesaro summable and 
1 
72 
Tn 
_ 
I+P+---+P 
(e/n)n? 
~enT™ 
if 
WFQ 
8-9 
hy 
lim) ——WW______- = G = -~—~ __ = —__ =e' 
= 
; 
: 
Sohal 
ke 
k—+00 
k 
ml(e/n) 
7Te 
oe 
Ty 
12 
eee 
Tn 
where e/n is the right-hand Perron vector for P and m7 = (7,79,...,n) is 
the left-hand Perron vector. Notice that this is exactly the same as the expres- 
sion in (7.9.5) on page 862 that represents limz_,., P* when P is primitive. 
Analogous to (7.9.6), the k*" step distributions in the imprimitive case have a 
Cesaro limit given by 
pet | Pair De) ae 
Dr ia) 
k- 00 
k 
( 
7.9.9 
ee 
wad Sah 
= 
T 
ee 
Ce ale ae eee 
7 ia ena 
iy 
jim Pp | 
i 
|=» (Olen 
=m, 
and, just as in the primitive case, this Cesaro limit is independent of the initial 
distribution. 

7.9 Markov Chains 
865 
Interpreting the Cesaro Limits 
The analysis for interpreting the limits in (7.9.6) and (7.9.9) is essentially the 
same as the analysis of the shell game given in the example on page 636, but 
for the sake of completeness the logic is reiterated below. The trick is to focus 
on one state, say S;, and define a sequence of random variables {Z;,}?29 that 
count the number of visits to $;. Let 
7 2 
1 
if the chain starts in S;, 
°™~ ) 
0. otherwise, 
and for 7 > 1, 
(7.9.10) 
Z, = 
41 ifthe chain is in 9; after the 1" move, 
0 otherwise. 
Since Zp + Z, ++-::+ Z,_1 counts the number of visits to S; before the kth 
move, (Zp + Z; +-::+ Zp_1)/k represents the fraction of times that S; is hit 
before the k*" move. The expected (or mean) value of each Z; is 
E[Z;] = {1 x P(Z;=1)} + {0 x P(Zj=0)} = P(Zi=1) = pj 
(i), 
and, since expectation is linear, the expected fraction of times that S$; is hit 
before move k is 
E om | _ ElZo] + E[Z) +++ 
+ E[Z,—1] 
k 
k 
- [peepee 
+7; 
by (7.9.9) 
- 
pa 
9.9). 
e 
In other words, the j*' component 7; in the left-hand Perron vector ne 
represents the the long-run fraction of time that the chain spends in Sj. 
It was established in Exercise 4.11.13 on page 642 that when limyz-+.0 p" 
(k) 
exists, 
lim 
k—co pee ee 
k 
| 
= 
lim p7(k), 
k—oo 
so the interpretation of the limiting distribution limg 4.0 p!(k) for the primitive 
case is exactly the same as the interpretation of the Cesaro limit in the imprim- 
itive case. This makes the heuristic statement on page 864 that 7; represents 
the long-run fraction of time that the mouse spends in each chamber a rigorous 
conclusion. The results for irreducible chains are summarized in the following 
theorem. 

Example (Google's PageRank) 
Google's search engine and the PageRank ranking system that underlies it were 
conceived by Sergey Brin and Larry Page while they were computer science 
graduate students at Stanford University. 
They took a leave of absence in 1998 
to focus on their growing business, and in a public presentation at the Seventh 
International World Wide Web conference (WWW98) in Brisbane, Australia, 
their paper titled The PageRank citation ranking: Bringing order to the Web 

7.9 Markov Chains 
867 
made small ripples in the information science community that quickly turned 
into waves. Today's Google technology that evolved from their original PageRank 
idea is now proprietary and is no doubt more sophisticated, but Brin and Page's 
basic concept for ranking web pages remains of interest. 
The idea is that a web page is "important" and deserves to be highly ranked 
if it is pointed (linked) to by other "important" pages. This means that the 
importance of your page (your "PageRank") might be determined by summing 
the PageRanks of all pages that point to yours. But Brin and Page also reasoned 
that when an important page points to several places, its weight (PageRank) 
should be distributed proportionately. In other words, if Wikipedia points to 
your web page, that's good, but you do not deserve the full weight of Wikipedia's 
PageRank because Wikipedia points to many other places—e.g., if Wikipedia 
points to 99 pages in addition to yours, then you should only get credit for 
1/100 of Wikipedia's PageRank. This is the basis for the following recursive 
definition of raw PageRank. 
7.9.4. Definition. (Raw PageRank) Suppose that there are n web pages, 
: and initially consider all web pages to be equally important by defining 
_ 
r;(0) =1/n to be the initial rank of the 7" page. Successively refine 
_allranks 
by setting 
= 
Og 
R= 123.5, 
(7.9.11) 
ae 
where r;(k) is the rank of page 7 at iteration k, I; is the set of pages 
pointing (linking) to page i, and O, is the number of out-links from 
page j. If r; = limp 4.7i(k) exists, then r,; is defined to be the raw 
PageRank of the i'" web page. 
Exploring convergence of r;(k) is facilitated by translating (7.9.11) into 
) Larry Page's and Sergey Brin's PageRank idea and the 
Google search engine that they built from it along with 
the Alphabet conglomerate that subsequently emerged has 
paid off handsomely. At the beginning of 2023, Forbes mag- 
azine reported that Larry Page's and Sergey Brins's re- 
spective net worth was $93 Billion and $89 Billion putting 
them in the respective 6*" and 7*" places among Amer- 
ica's most wealthy people at that time. Math and computer 
science can indeed be lucrative pursuits. 
SERGEY BRIN & LARRY PAGE 

868 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
matrix terms. Let Hnxn be the hyperlink matrix defined by 
h,, — £ 
1/0: 
if there is a link from page i to page J; 
(7.9.12) 
Lip 
0 
otherwise, 
and let 
rT (k) = (r1(), r2(h), «++ ta(k)) 
be the vector of rankings after the k*" iteration. It follows from (7.9.11) that 
r'(k +1) =r" (k)H =r' (0)H*. 
(7.9.13) 
In other words, the raw version of the PageRank vector is obtained by the familiar 
power method iteration (page 772) that may or may not converge, depending on 
whether or not limz_,.. H® exists. In addition to convergence problems, other 
issues arise from the fact that the web is littered with dangling nodes, web pages 
that contain no out-links, so H has many zero rows. Consequently, there is 
nothing to ensure that lim,_,., H* exists—in fact, it almost certainly will not. 
The dangling nodes problem is fixed by replacing all zero rows in H with the 
uniform distribution e'/n, the components of which are pretty small because 
n is huge in reality. This replacement is accomplished by a rank-one update. If 
a is the column in which 
pe 
ake 
e 7 is a dangling 
node, 
dp { datas 
| 
a 
then 
S=H-+ae™/n 
(7.9.14) 
0 
otherwise, 
is identical to H except that zero rows in H are replaced by e? /n. The matrix 
S is a row stochastic matrix (because Se = e) that is the transition matrix for 
a Markov chain that represents a random walk on the graph defined by the web's 
link structure with the rule that the probability of moving from a dangling node 
to any page in the web is equal to 1/n. 
Consequently, issues of PageRank convergence boil down to the limiting 
properties of discrete Markov chains. In particular, if S is irreducible and prim- 
itive, then for every initial probability distribution vector p'(0), the chain de- 
fined by p'(k +1) = p7(k)S will converge, and the limit would be unique in 
the sense that it will be independent of p7(0). 
Unfortunately, the adjustment to compensate for dangling nodes that pro- 
duces § in (7.9.14) is not enough to guarantee the primitivity of S. For a matrix 
P > 0 to be primitive, it is necessary and sufficient that P be irreducible with 
P"™ > 0 for some positive integer m (page 821). The link structure of the World 
Wide Web is not a strongly connected graph, so S is a reducible matrix. Since 
irreducibility on top of stochasticity is required, another adjustment is needed. 
The second adjustment is to add another rank-one term E = ee! /n > 0 
to S in (7.9.14). More generally, the uniform vector e7 /n can be replaced by 
any probability vector v? > 0 to yield E = ev". Such a vector v" is called 

7.9 Markov Chains 
869 
a personalization vector because it allows the flexibility of weighting PageRanks 
so as to personalize them. The result is the Google matrix 
G=aS8+(l-a)E 
with 
0<a<1. 
(7.9.15) 
Experimentation originally led Google to use a = .85. The matrix E = ee" /n 
is called the teleportation matriz because it models a web surfer who with prob- 
ability (1 — a) becomes bored following links and enters a random URL to 
"teleport" to a random page in the web. The effect of E = ev' is similar 
but the random destination is weighted by the entries of v' rather than being 
uniform. 
Because G is a convex combination of the two stochastic matrices S and 
E > 0, it follows that G > 0 is an irreducible stochastic matrix that is primitive. 
Consequently, the power method 
p'(k+1) = pT(h)G 
must converge (independent of the initial vector) to a unique stationary distri- 
bution vector 77. 
7.9.5. Definition. The PageRank of the i*" web page is defined to be 
the 7" component 7; in 7m? = limg_,.,p?(k), which is the left-hand 
Perron vector for G. When a user query is made, links to relevant pages 
are returned to the user in the order of the PageRanks. 
Example (A Tiny Web) 
Determine the PageRank of each of the six pages in the Tiny Web with links as 
depicted below in Figure 7.9.2. 
Y 
FIGURE 7.9.2: TINY WEB 

870 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
The hyperlink matrix H as defined in (7.9.12) for the Tiny Web is 
Py 
(Pee 
Pe 
she 
Ps 
53 
Pip 
delj2tiy2 
0 
0 
0 
Bae 
On 
ee 
: 0 
P13 
is 
0 
0 
18 
0 
H =p, 
: , 
oe) 
SOME 
faernya 
Fai | \Giv) 
Oe 
vein 
AEDs 
8s 
kD 
Pattie 
i. 
oO 
ant. 
ue 
Entries in the second row of H are all zero because the second page has no 
out-links—i.e., it is a dangling node. This is remedied by adding the rank-one 
term ae' /6 to H in which a= ep to produce the stochastic matrix 
Of 
fags 2/2 
OP 
OODERG 
1/6 1/6 1/6 
1/6 
1/6 
1/6 
S sll ees 
(Ge 
bee 
1/3 
0 
Cn 
ro 
OPO 
ie ee 
i 
hoy 
(> 
1 
Cee 
0 
240 
Oacy 
Ae ab Ooene 
as defined in (7.9.14). However, S is not irreducible (it is block-triangular), so 
the second adjustment is to augment S by forming the convex combination of 
S and E = ee? 
/6 as defined in (7.9.15) with a =.9. The result is the Google 
matrix 
1/60 
7/15 
7/15 
1/60 
1/60 
1/60 
1/6) 
fe? 
1/6 
Afem 
jer 
1/6 
- 
_ | 19/60 
19/60 
1/60 
1/60 
19/60 
1/60 
G=98+1E=1| 
yi6) 
1/60 
1/60 
1/60 
7/15 
7/15 
1/60 
1/60 
1/60 
7/15 
1/60 
7/15 
1/60 
1/60 
1/60 
11/12 
1/60 
1/60 
This is a primitive stochastic matrix whose stationary probability vector (its 
left-hand Perron vector) +? rounded to four significant figures is 
1 
2 
3 
10 
DG 
Came 
nm' = (.03721 
.05396 
.04151 
.3751 
.206 
.2862). 
The heuristic interpretation is that 7; represents the fraction of time that a 
random web surfer is expected to visit page P,;. PageRank orders the pages in 
the Tiny Web as (4 6 
5 
2 3 
1)—e.g., page Py is the most important 
page and page P, is the least important page. This is the order in which a 
Google-type search engine for the Tiny Web would prioritize relevant results to 
a query from a user. 
This example exposes only the tip of Google's iceberg. The interested reader wanting more 
details is referred to the texts Google's PageRank and Beyond: The Science of Search Engine 
Rankings, Princeton University Press, 2006, ISBN-13:978-0691122021, and Who's #1?: The 
Science of Rating and Ranking, Princeton University Press, 2013, ISBN-13:978-0691162317, 
both by Amy N. Langville and Carl D. Meyer. 

7.9 Markov Chains 
871 
Periodic Markov Chains 
Some irreducible Markov chains exhibit the special property of periodicity in 
the sense that each state can only be entered at regular periods. The shell game 
on page 636 is one such example. A simpler example is provided by considering 
an electronic switch that can be in one of three states (positions) {.$1,92, 53} 
that change at uniform clock cycles. Suppose that if the switch is in either Sj 
or $3, then it must change to 92 on the next clock cycle, but if the switch 
is in Sg, then there is an equal probability that it changes to S$, or $3 on 
OO 
the next clock cycle. The transition matrix is P = (s ) 5). and it is clear 
@- 
ih 
that P is irreducible and imprimitive because G(P) is strongly connected and 
o(P) = {+1, 0}. Since the left-hand Perron vector is m7 = (.25, .5, .25), the 
long-run expectation is that the switch should be in S$, 25% of the time, in S» 
50% of the time, and in S3 25% of the time, and this agrees with what common 
sense suggests. But, unlike the mouse in a maze on page 863, the switch cannot 
be in just any position after any given clock cycle because if the chain starts in 
either S; or $3, then it must be in S2 on every odd-numbered cycle, and it 
can occupy 5; or $3 only on even-numbered cycles. The situation is similar 
but with reversed parity when the chain starts in Sj. This is an example of a 
periodic chain whose period is 2, which agrees with the index of imprimitivity 
(page 820). The fact that the period of the chain is the same as the index of 
imprimitivity is no accident. It is true in general because the transition matrix 
can be put into Frobenius form (7.6.13) on page 830 by reordering (or renaming) 
the states according to the symmetric permutation used to prove Theorem 7.6.7. 
In accordance with Definition 7.5.1 on page 820, the following terminology is 
adopted. 
7.9.6. Definition. A periodic Markov chain is one whose transition 
matrix P is periodic—i.e., irreducible and imprimitive. The period of 
the chain is the index of imprimitivity (page 820) for P. 
e 
An aperiodic Markov chain is one whose transition matrix P is 
aperiodic—i.e., irreducible and primitive. 
Case 3 — Reducible 
Suppose P is a reducible stochastic matrix. Analyzing its limiting behavior is 
accomplished by reducing the issues as much as possible to those involving irre- 
ducible chains. If P is reducible, then, by definition, there exists a permutation 
matrix Q and square matrices X and Z such that 
Q™PQ = & ob For convenience, denote this by writing P ~ @ mae 

872 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
If X or Z is reducible, then another symmetric permutation can be performed 
to produce 
~ 
Hy 
se 
oe 
+ ce ~ € aay ), where R, U, and W 
are square. 
Dak 
0 
0 W 
Repeating this process eventually yields 
Kip 
X12 
a 
Xa 
Oo 
X99 | 1s 
ge 
ce 
, 
Pw 
; 
. 
|, 
where each X,; is irreducible or Xj; = [O]1x1. 
0 
Onneed 
Xae 
Finally, if there exist rows having nonzero entries only in diagonal blocks, then 
symmetrically permute all such rows to the bottom to produce 
Pi 
Pia 
Pe 
ie 
Pi,r+1 
Ppt 
Pim 
OR 
Sp 
pipe 
mm aye 
Po,r+1 
Poo 
gt 
a, 
Py rta 
Pyrite 
Eee 
; 
(7.9.16) 
er 
ea 
Pris 
0 
a 
Ss 
Cine 
Cate 
CO 
0 
Paes 
aobesd 
Ove 
0 
0 
i) 
i) 
Pram 
where each P1,...,Prr is either irreducible or [0|11, and P,+1,-41,-.--;Pmm 
are irreducible (these cannot be zero matrices because each has row sums equal 
to 1). As mentioned on page 808, the effect of a symmetric permutation is simply 
to relabel nodes in G(P) or, equivalently, to reorder the states in the chain. 
7.9.7. Definition. 
When the states of a reducible chain have been re- 
ordered so that its transition matrix assumes the form in (7.9.16), P is 
said to be in a canonical form for reducible stochastic matrices. 
e 
When P is in the canonical form (7.9.16), the subset of states cor- 
responding to P;, for 
1<k<vr 
iscalled the k'" transient class 
(because once left, a transient class cannot be reentered). 
e 
The subset of states corresponding to P5747 for. 7 > 1 is called 
the j*" ergodic class. Each ergodic class is a Markov chain unto 
itself that is imbedded in the larger reducible chain, and it may or 
may not be primitive. 
e 
Unless otherwise stated, transition matrices for reducible chains are 
assumed to be in canonical form (7.9.16). 

7.9 Markov Chains 
873 
O 
T22 
Pu) 
Pip 
Py ri 
+ 
Pim 
Pri,r+1 
Ties 
ilddepiee 
a se 
© 
ee 
m 
) 
Pry 
Pp rti 
+++ 
Prm 
Pim 
It is convenient to make the identification 
P ~ T = ( 
par" 
Tx ), where 
Proof. 
First observe that 
OiP in) 
oreacn b= 1,2 ser 
(7.9.17) 
because either Pxy = 
[0]1x1 
(in which case p(Px,) = 0) or Py, # O is 
irreducible, which means that there are blocks Px; (j # k) that have nonzero 
entries—otherwise P;,, would appear in the bottom of (7.9.16). Since spectral 
radius cannot exceed any matrix norm, 
Pye <e 
and 
Pye # e 
=> 
Perle eee 
p (Pre) —. 
This in turn implies that p(Ti1) <1 (because Tj, is block triangular). Thus 
limp_5o0 T?, =0 (by Theorem 4.11.1 on page 617). 
I 
Now consider T22, the ergodic part of T. Since P,41-41,...,Pmm 
are 
each irreducible stochastic matrices, each Pj; 
(r < j < m) has a limiting 
value if and only if P;,; is primitive (Theorem 7.5.2, page 820), in which case 
limp+oo Pj; = em7, where 7} is the stationary distribution (the left-hand 
Perron vector) for P;;—see (7.9.5) on page 862. Hence 
ems, 
Ea 
lim TS. = 
Hs 
22 i 
k—-oo 
em! 
k—0o 
ip 
m 
(7.9.18) 
(because when both exist, the standard limit agrees with the Cesaro limit). 
Therefore, if P is a reducible stochastic matrix in canonical form, then the 
structure of the Cesaro limit is 
_ 
I+P+---+P*? 
OPE Zaye 
lim ~=—————_—_— 
=i ape 
k-co 
k 
4 See Exercise 7.9.6 on page 880 for related developments. 

874 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
where G is the projector onto N(I—P) along 
R(I—P). This means that 
(-P)G=0 => (TG we) (0 g) =o = O-Tu)Z= Tuk. 
Since 
I—'T,1 is nonsingular (because p(T11) <1), it follows that 
Z = (I— Ti) 
* TE, 
and thus the following results concerning limits of reducible chains are produced. 
; 7.9.9. Theorem. (Summary of Reducible Markoy Chains) If the states 
of a reducible Markov chain are ordered to make the transition matrix 
have the canonical form 
described on page 872, then 
I—'T,; is nonsingular. For the stationary 
distribution (the left-hand Perron vector) for P;; (r+1<j<m), and 
eT 1 
for 
E= 
i 
, the Cesaro limit is 
i 
eT me 
lin ——_——_——_—— = 
0 
E 
iP. 
pk 
0 
(I—T 1) 7!Ty2E 
ne 
k--00 
k 
See 
where G is the spectral projector onto N (I—P) along R(I—P). 
e 
limp... P* exists if and only if P,41-41,---;Pmm in (7.9.19) are 
each primitive, in which case 
=G. 
(7.9.20) 
k—oo 
ia (Ce 
0 
K 
These facts show that every reducible chain eventually gets absorbed (or 
trapped) into one of the ergodic classes—i.e., into a subchain defined by Pra jr4j 
for some j > 1. When P,4;,-4; 
is primitive, the chain settles down to a 
steady-state defined by the stationary distribution (the left-hand Perron vector) 
OD Pty aly 
Duta, P,-45,r+j 18 imprimitive (i.e., periodic), then the process will 
oscillate inside of the j*" ergodic class forever. 

7.9 Markov Chains 
875 
Absorption Into an Ergodic Class 
Notwithstanding issues related to the existence of limits in a reducible chain, 
there are other important questions concerning which ergodic class the chain 
will end up in and how long it takes to get there. This time the answer depends 
on where the chain starts—i.e., things depend on the initial distribution. In what 
follows, adopt the following notation with the following assumption. 
e 
Let 7; denote the i" transient class. 
e 
Let €; be the j ergodic class. 
e 
Suppose that the chain starts in the p" state of 7;. 
To determine which ergodic class the chain eventually lands in (but not what 
happens thereafter), convert every state in each ergodic class into an absorbing 
state (i.e., a "trap" or a "sink") by setting 
Prejre7 =] 
foreach 4 =v 
in (7.9.19). The transition matrix for this modified chain is 
ete 
lit. 
bys 
Code) 
and 
lim P* 
k- co 
(t (ie eee 
0 
0 
0 
Lin 
Lip 
Lis 
0 
O 
0 
Loi 
Loe 
Lo, 
0 
O 
0 
L,4 
L,.2 
[bee 
0 
O 
0 
I 
0 
0 
0 
O 
(0) 
8) 
I 
8) 
Tn 
Soe 
0 
ipa 
<I 
Consequently, the (p,q)-entry in block L;; represents the probability of even- 
tually hitting the q" state in €; when initially in the p' state in Jj. 

Proof of (7.9.21). 
Since the (p,q)-entry in block L,; is the probability of first 
hitting the q'" state in €; when initially in the p" state in 7;, the probability 
of being absorbed somewhere into €; is 7, [Lij],, = [Lije],. 
Proof of (7.9.22). 
This follows directly from the previous observations. 
Proof of (7.9.23). 
To determine the expected number of steps required to first 
hit an ergodic state, count the number of times the chain is in transient state 
S; given that it starts in transient state S$; by reapplying the argument given 
in (7.9.10) on page 865. That is, given that the chain starts in S;, let 
PLE Seto. 
_ 
f1 ifthe chain is in 5; after step k, 
Lor { 
0 otherwise, 
a 
0 otherwise. 
Since 
E[Zx] = {1 x P(Ze=1)} + {0x P(Z_=0)} = P(Ze=1) = [Th], | 

7.9 Markov Chains 
877 
and since }77°) Z, is the total number of times the chain is in S;, we have 
Co 
Co 
oe) 
E[# times in $;| start in S;] = E SOL = Sit [Zx] = ost [Th]; 
k=0 
k=0 
k=0 
= [(I- A 
hele 
(because p(T11) <1). 
& 
Proof of (7.9.24). 
Summing the expression above over all transient states S; 
produces the expected number of times the chain is in some transient state when 
starting in transient state S;. This is the same as the expected number of times 
before first hitting an ergodic state. In other words, 
E[# steps until absorption | start in i" transient state] = ((I—Tj)~'e],. 
mJ 
Example (Absorbing Markov Chains) 
In many applications there is only one transient class, and the ergodic classes 
are each single absorbing states (i.e., once entered they cannot be left). If the 
single transient class contains r transient states, and if there are s absorbing 
states, then the canonical form for the transition matrix is 
Pi1 
°°" 
Dir 
Dien 
20° 
Wohl 
Pri 
5° 
prr 
Pr,r+1 
°"* 
Prs 
©; Gs Pio. ) 
OC 
eee 
0 
1 
aids 
ai 
OL 
cto 
0 
ey 
A chain of this nature is called an absorbing Markov chain provided that some ab- 
sorbing state is accessible from every transient state. This means that Pj2e > 0, 
which in turn guarantees that p(Pi11) <1 because P;;e+ Pi2e =e, so 
Pype>O 
=> 
Pye<xe 
=> 
heey peace — 
p(Piz) <1. 
Theorem 7.9.10 shows that the probability of being absorbed into the j¢" ab- 
sorbing state (which is state S,+,;) given that the chain starts in the i" transient 
state (which is 5;) is 
P(absorption into S,4,|start in $; for 1 <i <r) = [(I- Pai) Pals 
(7.9.25) 
while the amount of time spent in S; is 
E[# times in 9;| start in S;] = [(I— PH ; 
and the expected time until absorption is 
E[# steps until absorption | start in $;] = [(I- Pir) e), S30). 
(7.9.26) 

878 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
Example (Fail-Safe System) 
Consider a system that has two independent controls, A and B, that can pre- 
vent the system from being destroyed. The system is activated at discrete points 
in time t,,t2,t3,..., and the system is considered to be "under control" if either 
control A or B holds at the time of activation. The system is destroyed if A 
and B fail simultaneously. 
> 
For example, many automobiles have two independent braking systems—one 
is operated by a foot pedal, whereas the emergency brake is operated by a 
hand lever. The automobile is "under control" if at least one braking system 
is operative when a driver tries to stop, but a crash occurs if both braking 
systems fail simultaneously. 
If one of the controls fails at some activation point but the other control holds, 
then the defective control is repaired before the next activation. If a control holds 
at time t = ty, then it is considered to be 90% reliable at t = t,41, but ifa 
control fails at time t = tz, then its untested replacement is considered to be 
only 60% reliable at t = tr41. 
Problem: Just how safe is such a system? In other words, how long can such a 
system be expected to operate safely before a catastrophic failure occurs? 
Solution: This is a four-state absorbing Markov chain with the states being the 
controls that hold at any particular time of activation. In other words the state 
space is the set of pairs (a,b) in which 
_f1 
if A holds, 
_f1. 
if B holds, 
at if A fails, 
' Sank: if B fails. 
State (0,0) is absorbing, and the transition matrix (in canonical form) is 
CLL) 
(LO 
(0, 
Dye) 
eta 
81 
09 
09 
01 
Pp (1,0) 
54 
6 
.06 
04 
(0,1) 
54 
.06 
36 
.04 
(0,0) 
0 
0 
0 
1 
with 
81 
.09 
.09 
01 
Piz =) 
3549 
SOE 
OG 
arid" 
Pya ="17 
104 
04 
welG 
saad6 
04 
It follows from (7.9.20) on page 874 that lim,z_,. P* exists and is given by 
lim PMH 
@ Ca Ravig Fae 
k-oo 
: 
0 
1 

7.9 Markov Chains 
879 
This makes it clear that the absorbing state must eventually be reached. In other 
words, this proves the validity of the popular belief that "if something can go 
wrong, then it eventually will." Rounding to three significant figures produces 
44.6 
6.92 
6.92 
os 
(I—Piu)"'= | 41.5 
8.02 
659] 
and 
(-Pu)te= (561), 
41.5 
6.59 
8.02 
56.1 
so the mean time to failure starting with two proven controls is slightly more 
than 58 steps, while the mean time to failure starting with one untested con- 
trol and one proven control is just over 56 steps. The difference here doesn't 
seem significant, but consider what happens when only one control (say the foot 
brake) is used in the system. In this case, there are only two states in the chain, 
1 (meaning that the foot brake works) and 0 (meaning that it doesn't). The 
transition matrix is 
i 
@ 
so now the mean time to failure is only (I— P,)~'e = (1 —.9)~! = 10 steps. 
It is interesting to consider what happens when three independent controls are 
used. How much more security does your intuition tell you that you should have? 
See Exercise 7.9.10 for the answer. 
Exercises for section 7.9 
1/450 
30. 
3/4 
7.9.1. Find the stationary distribution for P = 
Uh ue ue ae 
. Does 
ie 
SPP 
this stationary distribution represent a limiting distribution in the reg- 
ular sense or only in the Cesaro sense? 
7.9.2. Recall from page 859 that a doubly-stochastic matrix is a nonnegative 
matrix P,,,, having all row sums as well as all column sums equal to 
1. For an irreducible n-state Markov chain whose transition matrix is 
doubly stochastic, what is the long-run proportion of time spent in each 
state? Assuming they exist, what is the form of the limits 
lim (I+P+---+P*")/k 
and 
lim P*? 
k—-0o 
k—00 
Note: The purpose of this exercise is to show that doubly-stochastic 
matrices are not very interesting from a Markov-chain point of view. 
However, there is an interesting theoretical result (due to G. Birkhoff 
in 1946) that says the set of n x n doubly-stochastic matrices forms 
a convex polyhedron in R"*" with the permutation matrices as the 
vertices. 

880 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
7.9.3. 
7.9.4. 
7.9.5. 
7.9.6. 
oaks 
(9:8. 
7.9.9. 
7.9.10. 
Prove that if P is a stochastic matrix, and if (A,x") is a left-hand 
eigenpair for P such that 
A #41, then x Le. 
Explain why rank (I—P) =n—1 
for every irreducible stochastic ma- 
trix P,,. Give an example to show that this need not be the case for 
reducible stochastic matrices. 
Prove that the left-hand Perron vector for an irreducible stochastic ma- 
trix Pyxn (n> 1) is given by 
1 
m= sap (Pts Pas oo) 
Fa), 
where P; isthe i*" principal minor determinant of order n—1 in I—P. 
Hint: See Exercises 9.3.9 and 9.3.12 on page 967. 
A matrix S,.x%» > 0 having row sums less than or equal to 1 with at 
least one row sum less than 1 is called a substochastic matriz. 
Note. The transient part of the matrix T in Lemma 7.9.8 on page 873 
is substochastic. 
(a) Explain why p(S) <1 for every substochastic matrix. 
(b) Prove that p(S) <1 for every irreducible substochastic matrix. 
Let Ppxn be an irreducible stochastic matrix, and let Qzy,% be a prin- 
cipal submatrix of P, where 
1<k <n. Prove that p(Q) <1. 
Let Py» be an irreducible stochastic matrix, and let Q;.,% be a prin- 
cipal submatrix of 
I— P, where 
1 < k < n. Explain why Q is an 
M-matrix as defined on page 622. 
Let Pnxn (n> 1) be an irreducible stochastic matrix. Explain why all 
principal minors of order 
1 <k <n in I—P are positive. Hint. Recall 
Theorem 4.11.9 on page 623. 
Use the same assumptions given for the fail-safe system described in the 
example on page 878, but now use three controls, A, B, and C, instead 
of the two used there. Determine the mean time to failure starting when: 
(a) there are three proven controls; (b) there are two proven and one 
untested control; (c) there are three untested controls. 

7.9 Markov Chains 
881 
7.9.11. A mouse is placed in one chamber of the maze shown in Figure 7.9.1 
on page 863, and a cat is placed in another chamber. Each minute the 
doors to the chambers are opened just long enough to allow movement 
from one chamber to an adjacent chamber. Half of the time when the 
doors are opened, the cat doesn't leave the chamber it occupies. The 
same is true for the mouse. When either the cat or mouse moves, a door 
is chosen at random to pass through. 
(a) Explain why the cat and mouse must eventually end up in the 
same chamber, and determine the expected number of steps for 
this to occur. 
(b) Determine the probability that the cat will catch the mouse in 
chamber #j for each j = 1, 2,3. 

The fact that each stochastic complement S; is nonnegative and irreducible 
with p(S;) =1 is a consequence of Theorem 7.8.3 on page 853. The fact that 
each §; is also a stochastic matrix is established below. 
Piz 
Pio 
Proof. 
It suffices to prove this for k = 2 and S; in 
P= eS a because 
a symmetric permutation can always be performed to bring any principal sub- 
matrix to the upper left-hand corner. In this case Ry = Py2, C, = Poi, and 
Z; = Poe so that S; = Py, + Pio(I — Poe) !Po1. Let e = 'eS in which e; 
and eg are appropriately sized columns of ones and observe that 
Pe=e 
=> 
Psie,4 (I - P22)e2 iS P22) 
Poe; =e 
=> 
Pio(I— P22) 'Poiei = Pizeg =e) — Prey 
=> 
e, = Pye, + Pi2(I = P22)! P21e; =S,;e,. 
i 

7.10 Stochastic Complementation 
883 
Interpreting the Entries in a Stochastic Complement 
Every stochastic complement S$; is the transition probability matrix of a Markov 
chain that is smaller than its parent chain defined by P. To understand the rela- 
tionship between these two chains, consider the simpler (but equivalent) situation 
where the set of states {1,2,...,n} is partitioned into two clusters, 
= {ter 
ANG 
Opis (hor LP 
Oy. 1} 
so that 
; 
ler 
rfl 
2 
Pi 
Pi2 
S, = Py + Pio(I 
— Poe)! Pai, 
ra De 
, 
with 
(7.10.2) 
i 
Po 
P22 
So = Poo + Pai(I— Pii)7!Pi. 
Focus on one of these complements—say S2—and interpret the 
(i, 7)-entry 
[Saliy = [P22]; = [Pai(I = Pi1)71Pyalij- Notice first that [P22]; is the proba- 
bility of moving from state 
r+i€ Sg to state 
r+ 7 € Sg in one step, while 
NE 
[Pai(I os Pi) 'Pia| _= Dy [Poi| \(r 
= Pi) P| 
a] 
Was 
ik 
kj 
The term [Poi], is the probability of moving from r+i € Sp to k € S; in one 
step, while [(I - P11)! P| kj is the probability of hitting state 
r+j € S2 the 
first time the chain enters S2 when the process starts from k € S;. This can 
be seen by considering the states in S2 to be absorbing so as to artificially force 
the process to stop as soon as the chain enters Sj. Statement (7.9.25) on page 
877 says that [(I —P,,)-1Pi2| i is the probability of entering Sj at state r+j 
when the chain starts in k € S;. Consequently, 
[Pai] i, [(I _ Bip 
hae is 
the probability of moving directly from r+i€ Sz to k € S; and then, perhaps 
after several steps inside of S,, reentering S2 at state 
r+j without regard to 
what happened while the process was in S;. In other words, 
[s2| 
ij 2 [P22] 
ij + SE 
[P21] 
ik t is Pu) 'Pi| 
is the probability of moving from r+i € Sg to r+j € S2 in a single step or else 
by moving directly from r+i € Sz to somewhere inside of S; (perhaps staying 
there for awhile) and then hitting state 
r+ 
upon first reentry into So. 
kj 
Censored Markov Chains 
The above analysis explains why Sz is the transition probability matrix for a 
chain that records the location of the process only when the process is visiting 
states in So, and visits to states in S, are simply ignored or censored out. This 
motivates to the following terminology. 

To understand the meaning of a censored distribution suppose that the 
state space S for an n-state aperiodic Markov chain defined by P in (7.10.1) 
is partitioned into disjoint clusters as 
S=S,US_2U::-US; 
(7.10.3) 
in which §;; is the j*" state in cluster S;. Partition the t*" step distribution 
and the stationary distribution for P as 
P(t) = 
(pT ()|p3(t)| --- [pE(t)) and a" = (wP |wF |---| 
E). (7.10.4) 
Let X; be the state of the chain after the t*" step, and let Y; be the cluster 
that contains X, after the t' step. The probability of being in state Si; (the 
i state of the it cluster) after t steps is 
P(X; = Si) = [p (¢)], 
(the j*" component of p7(t)), 
and the limiting probability of being in Sj; is 
Jim P(X = Sij) = Jim, [Pi], = [mi |, (the j*" component of m7), 
Similarly, the probability of being inside cluster S; after t steps is 
P(Y,=i)=pre, 
(7.10.5) 

7.10 Stochastic Complementation 
885 
and the limiting probability of being somewhere in S; is 
jim P% = 1) = Jim p; 
(tle =e. 
(7.10.6) 
7 
Since 7" is the left-hand Perron vector for the transition matrix P, it follows 
from (7.8.11) on page 855 that the j*" component of i*" censored distribution 
s} with respect to the partition (7.10.3) is 
In other words, 
[si], is the limiting conditional probability of being in Sj; 
given that the process is somewhere in S;. These facts are summarized below. 
7.10.4. Theorem. 
(Censored Probability Distributions) The censored 
probability distributions defined on page 884 are the stationary distri- 
butions s? of the censored Markov chains defined oy the stochastic 
complements S; . In addition to satisfying s/S; = s7, where s} > 0 
and s/e =1, censored distributions have the following properties. 
Fos 
(re a |---|mf) 
is the stationary distribution of the 
parent chain defined by P in (7.10.1), then 
sn) wie 
foreach (51,2... 6 
(7.10.7) 
e 
If P is primitive, then the j*" component of s/ is the limiting 
conditional probability of being in the j*" state of cluster S; given 
that the process is somewhere in S;. In other words, 
[si], = lim P(Xe = Sy |% = 2), 
where X; and Y; are the respective state and cluster number of the 
chain after the t'" step. 
Let Pyxn be the partitioned transition matrix in (7.10.1) on page 882 for 
an n-state irreducible Markov chain, and let s/ be the stationary (or censored) 
distribution determined by the i*" stochastic complement 
S;. The left-hand 
version of the coupling theorem for Perron complements on page 855 specializes 
to yield the k Se 
nan of the stationary distribution (the left-hand Perron 
vector) m? = (77 | me |-+-|mz) of P in terms of the censored distributions 
s? by means of ie left-hand coupling matrix as defined in (7.8.14) on page 855. 

nr vat bev 
tine 
Proof. 
Irreducibility is guaranteed by the coupling theorem (page 855), and Cy, 
is (row) stochastic because 
Ps 
sv 
ee 
0 
Pi 
Onn 
Piz 
er 
occ: 
re) 
: 
Cr= 
a 
be. 
he 
Ee 
: 
Tee 
ae: 
= LgxnPnrxnRnxk, 
0 
Ad 
ae 
Pea 
ees 
LP pg 
0 
cn 
<1 
where egy 1 is acolumn of 1's and the e;'s in R are columns of 1's of appropriate 
e1 
size, so Rezy , = ( 
: 
=@nx1, and 
er 
Cregxi = LPRegy1 = LPeny1 = Lenx1 = exx:1- 
The coupling theorem fa page 855 eu ore that the components m7 in 77 
are given by 7? =7;s!, where the 7's stad the stationary probabilities for the 
chain defined Ry C,. In other words, if n° = (7 
-:: 
m) is the stationary 
distribution for Cz, then the stationary cee of P is 
ae 
fi 
Ee 
ik 
nm" 
= (mS{,71282,---, 
kS,)- I 
Example (Coupling Censored Distributions) 
Consider the irreducible chain whose transition matrix is partitioned as 
(7.10.10) 
ornpo|lonor 
ol. 
sal 
0 
2 
ea) 
0 
cil 
eal 
8x8 

7.10 Stochastic Complementation 
887 
The two stochastic complements and their respective stationary (or censored) 
distribution vectors are! 
08993 
5566 
.2329 
1205 
.2832 
.2409 
.1110 
.3649 
S, = | 2327 
.1311 
5885 
.04764 
g, — | -2576 
.2395 
.2819 
.2210 
16651 
"21949 
".09861 
5155)? 
~*~ | .2511 
.2823, 
(1329 
3337 
|? 
04427 
.5156 
.4102 
.02988 
.3633 
 .1683 
 .2006 
.2678 
sf =(.1495 
.3054 
.3341 
.2111), 
sf = (.2955 
.2264 
.1803 
.2978), 
The 2 x 2 coupling matrix is 
C 
és ae 
( 
.6974 a 
HE = 
ae 
) 
s$Prie 
sf Pe 
6587 
.3413 
which is again irreducible and stochastic (up to rounding errors). The coupling 
vector is the stationary distribution vector for Cz, which is 
7 =(m, 2) = (.6852, .3148), 
so the stationary distribution vector for P is 
nm" = (m81 | 7289 ) 
([.6852] x [.1495, 3054, 3341, 2111] | 
[.3148] x [.2955, 2264, 1803, .2978]) 
— (.1024 2092 .2289 .1446 .09302 .07129 .05676 09375). 
Divide-and-Conquer. Because each Stochastic complement §; is also nonnega- 
tive and irreducible with the common spectral radius p(S;) = 1, a divide-and- 
conquer strategy can easily be employed. For example, determining the station- 
ary distribution wm? for the 8 x 8 matrix P in (7.10.10) can be accomplished 
by a two-step process as follows. First partition the stochastic complements S$, 
and So as 
.08993 
.5566 
.2329 
.1205 
.2832 
.2409 
1110 
.3649 
PRIA 
mle 
5885 
.04764 
S 
20760 
22395 
.2819 
.2210 
Ss; = 
= 
: 
.16651 
.2194 
09861 
.5155 
PIL 
ASS} 
AGP) 
kaishi/ 
104427 
75156 
.4102 
.02988 
MOOOMELOSS 
.2006 
.2678 
Now compute the two stochastic complements in each of these along with their 
respective censored vectors and the associated coupling vectors by using the 
: After all computations are made in 16-digit arithmetic, the results reported in this example 
are rounded to four significant figures. 

888 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
left-hand version of (7.8.16) on page 856. 
1744 
.8256 
ns 
From $1: 
(Si), = een ora 
(S1), 
(1) = (.3286 6714) 
(31). = (.6128 .3872) 
4278 
.5722 
.9056 
.0944 
nt =(mi 
ma) = (.4548 5454) 
5776 
.4224 
_ 
(3468 
.6532 
From 82: 
(82), (ate Ton) 
(Sz) mire 6045 
(sz) = (.5561 .4339 ) 
(2) = 
(.3771 
.6229) 
mz =(M21 
N22) = (-5219 .4781) 
Coupling the (s abe 's produces the censored vectors for S; and Sg as 
st = (mi( 81) | mo( (si)") = (.1495 
.3054 
.3341 
.2111), 
and 
T 
Ts 
8 = (N21( S2) T | 
ne2( S2), ) = (.2955 
.2264 
.1803 
 .2978). 
Finally, again using the left-hand version of Corollary 7.8.5 (page 856) yields the 
coupling coefficients 7? = (m1, 72) = (.6852 .3148) needed to glue sf? and sf 
together to produce the stationary distribution vector for P as 
a" = (msi |nost) = (.1024 .2092 .2289 .1446 .09302 .07129 .05676 .09375). 
Exercises for section 7.10 
7.10.1. Consider an n-state irreducible Markov chain whose transition matrix 
P has been partitioned as indicated in (7.10.1) on page 882 and whose 
state space S has been accordingly partitioned into k disjoint clusters 
S = 8, USQU:::US,. 
(a) Prove that if 7 = (af |73'|---|a2) is the stationary distri- 
bution for P and if Y; is the number of the cluster occupied 
after the t*" step, and if A denotes logical AND, then 
P(Y; =i AYini = J) oa p; 
(t)Pije 
P(¥; = 1%) 
pj (te 
| 
(b) When P is primitive, explain why the entries in the left-hand 
coupling matrix C = C; on page 886 are 
cy = 8; Pye = lim P41 =5|Y: =i). 
P(Yis1 =5|%; = 4) = 
In other words, c;; is the transition probability of moving from 
S; to S; after the chain has achieved equilibrium. 

7.10 Stochastic Complementation 
889 
7.10.2. Let P be an irreducible stochastic matrix that is primitive. 
(a) Give an example to show that the stochastic complements for 
P need not be primitive. 
(b) Prove that if a diagonal block P,; in P is primitive, then the 
corresponding stochastic complement S; must also be primi- 
tive. 
(c) Explain why S; must be primitive whenever P;; has at least 
one nonzero diagonal entry. 
(d) Give an example to show that the converse of parts (b) and (c) 
is not true. 
7.10.3. Let P be an irreducible stochastic matrix partitioned as in (7.10.1) on 
page 882, and let S; be the associated i*" stochastic complement. 
(a) Prove that. ||S; — P,i||,, = ||[Rill,, so that limr,+o S; = Pa. 
Ce 
as 
(b) Use this to prove thatifS=|[ 
. 
. 
. 
 . 
|, then 
Oc 
Or ees: 
§ = |P -Sl,. =2max |Rilleo- 
This is a measure of the degree to which P differs from a strictly 
block-diagonal matrix, so it is called the deviation from complete 
reducibility. 

890 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
7.11 
SIMON-ANDO THEORY FOR WEAKLY COUPLED SYSTEMS 
CL SRS et? SSE SOA ig ee ices etmemend ED ee ae 
ec nerregr 
This is a theory that was proposed by Herbert Simon and Albert Ando' to help 
understand the relation between the short-term and long-term behavior of a large 
economy that could be divided into two or more almost independent economies. 
One example of such a system would be a collection of robust national economies 
that have little interaction with each other. Another example would be a set of 
industrial sectors where trade is prevalent between companies in the same sector 
but not across sectors. Figure 7.11.1 illustrates such a system in which there are 
three closely coupled economies, but only weak connections between them. 
Bs 
a 
[Pésll >> U4 lPasll 
23 
FIGURE 7.11.1: WEAKLY COUPLED SYSTEM 
Herbert A. Simon (1916-2001) was an American economist and political 
scientist who won the Nobel Prize in Economic Sciences in 1978 for his con- 
tributions to modern business economics and administrative research based 
on his concept of bounded rationality that states individuals do not make 
perfectly rational decisions due to the inability to obtain and process all the 
information needed to do so. Rather than believing that economic behavior 
was rational and based upon all available information to produce optimal 
outcomes, Simon thought that economic decisions were generated by "sat- 
isficing," a term he coined by combining "satisfy" and "suffice." Because 
humans could not possibly obtain or process all of the information required 
for fully rational economic decisions, they use information at hand to pro- 
duce results that are "good enough." This evolved into his theory of "bounded rationality." 
After earning his Ph.D. from the University of Chicago in 1943, Simon eventually joined the 
faculty at the Carnegie Institute of Technology (now Carnegie Mellon University) in 1949, 
where he remained for more than 50 years as a professor of administration, psychology and 
computer science. In addition to his Nobel Prize, Simon received the Turing Award in 1975 
for his work in computer science and artificial intelligence, and in 1986 he was awarded the 
United States National Medal of Science. 
HERBERT A. SIMON 
Albert K. Ando (1929-2002) was born in Tokyo to the family running the 
Ando Corporation, 
a major construction company. After World War II he 
moved to the United States, where he received a B.S. degree in economics 
from Seattle University in 1951, an M.A. in economics from St. Louis Uni- 
versity in 1953, and a Ph.D. in mathematical economics from what is now 
Carnegie Mellon University in 1959. It was at Carnegie Mellon that he met 
and collaborated with Herbert Simon on research concerning the aggregation 
of variables that evolved into the Simon—Ando theory. In 1967 Ando took a 
position at the University of Pennsylvania where he remained as Professor 
of economics and finance until his death. 
PGR: 
ALBERT K. ANDO 

7.11 Simon—Ando Theory for Weakly Coupled Systems 
891 
These weakly coupled systems can be modeled with a Markov chain in 
which the (7, j)-entry of the transition matrix P represents the flow of goods or 
capital from company i to company j. Being weakly coupled means that this 
matrix has relatively large values in the diagonal blocks P;; and relatively small 
ones elsewhere. For example, when the circles on the left-hand side in Figure 
7.11.1 represent three countries with strong internal trade (the solid lines), but 
little international trade (the dashed lines), and when industries are ordered by 
country in a partitioned matrix as shown in the middle in Figure 7.11.1, most 
of the mass in this matrix is in the (colored) diagonal blocks. In addition to 
being called weakly coupled, such matrices have alternately been called nearly 
uncoupled or nearly completely decomposable. 
If there are no outside influences, and if flow rates remain constant, then 
a system of n economic entities will eventually reach a state of equilibrium 
provided that the transition matrix P, 
x, is primitive, which is a reasonable 
assumption for applications involving economics. In other words, if the entries in 
the state vector w(t) = (m1(t),72(t),...,7n(t)) represent individual propor- 
tions of the total goods or capital in the system at times t = 0,1,2,3,..., then 
m"(t) is a probability vector that evolves according to #7 (t+1) = #7 (t)P, and 
it has an equilibrium (or steady state) given by 
earn 
pee 
Ti 
aytT 
ality: 
Ue aay aH 
jim a (Cr (0) lim P = (0)G —1m (0 em t-te 
which is the left-hand Perron vector for P (recall Theorem 7.9.3 on page 866). 
In practical terms this means that barring outside influences, the flow of 
goods and capital between any two industries throughout the entire economy 
tends to be essentially constant in the long run. But actually computing m7 for 
real-world economies can be problematic not only due to scale but because suf- 
ficiently complete data is generally impossible to know or gauge. Consequently, 
inferring characteristics of the long-term behavior in each economic sector re- 
duces to empirically observing the evolution of the global economy over a long 
time horizon. 
The brilliance of Simon and Ando was their realization followed by their 
explanation of the fact that the short-run behavior in weakly coupled systems 
determines the system's long-run characteristics. This means that by making 
only a few empirical observations in the short term it is possible to approximate 
information about the long-term behavior. This was completely in line with 
Simon's philosophy of "bounded rationality" mentioned in the footnote above. 
Simon-Ando Principles for Weakly Coupled Economic Systems 
Simon and Ando formulated their principles based on the following intuition 
along with their observations of weakly coupled systems. Consider an economic 
system composed of k weakly coupled clusters of industries {C1,C2,...,Cx} in 
which each individual cluster C; is composed of strongly connected entities. 
e 
Short-run Dynamics: Due to the weak coupling between the C;'s, the k 
components in the global state vector m7(t) = (w7(t), 73 (t), ..., 
74 (d)) 

892 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
should each begin to evolve in such a way that individual industries in every 
C; independently tend toward their own approximate local equilibrium with- 
out regard to what is happening in other clusters. In the short run, the evolu- 
tion within each C; behaves approximately like a censored Markov chain as 
defined in Definition 7.10.3 on page 884. Consequently, in the short term the 
industries in each C; should rapidly approach approximate local equilibria 
that are roughly proportional to the values in the stationary (censored) dis- 
tributions s/ of the associated stochastic complements S;. In other words, 
after a short period of time there should be a short-run stabilization interval 
T = (t1,t2) during which the global chain is in approximate equilibrium in 
the sense that 
nm? (t) & (ais{, ort ae QRS; ) for t € T, 
mes hy 
where the a;'s are constants. Furthermore, for states S;,,S,, in the same 
cluster C;, there is an approximate relative equilibrium in the short run in 
the sense that 
Tas(t) _ (Side, 
i) GA 
(7.11.2) 
e 
Middle-run Dynamics: As t moves beyond Z, the a;'s in (7.11.1) begin 
to evolve with ¢t but in such a way that 
nT (t) 
(Ar(t)s?, Bo(t)sz, «++, Bx 
(t)sp ) for t > te. 
(7.11.3) 
In other words, each 6;(t) becomes more time dependent when t > to, but 
there is nevertheless a relative equilibrium maintained in the sense that for 
states S;,,,S,, in the same cluster C;, and for t > to, 
B;(t)s? 
ar 
Tae 
oT 
; ) ei 
Me ( 
ya = (S; Jax 
when ¢t > tg, just as in (7.11.2). 
Ty; (t) 
(8 
(t)s? ) 
, 
(Ge 
e 
Long-run Dynamics: In the long run, as t + oo, each £;(t) in (7.11.3) 
will settle down in route to its limiting value 7,;, the i** component in the 
left-hand coupling vector 7 (or the stationary probability vector for the left- 
hand coupling matrix Cz; in Theorem 7.10.5 on page 886). So, after a long 
period of time beyond tz, the global state vector is approximately constant 
again—i.e., for t >> te, 
mn" (t) = (181, 7283, °°°, MSE) = Jim. (a(t)sf, POR Bx(t)se )) 
and all along the way the relative equilibrium that was achieved in the short- 
term is maintained throughout the long-term evolution because 
z 
Te:(t) 
(81) 
lim a7 
(t) = 
et) 
is. ae 
z 
i 
a 
jim" 
(t) = (mi, 282, ---. 86) => jim Gun 
Cale. (7.11.4) 
a 

7.11 Simon—Ando Theory for Weakly Coupled Systems 
893 
Sketch of the Proof for the Simon—Ando Principles 
t 
The proof in Simon and Ando's original article' is a bit involved, but the under- 
lying ideas in terms of stochastic complements are relatively straightforward: It 
suffices to consider a system of three weakly coupled clusters whose primitive 
he 
Pit 
Pi2 
Fis 
Si 
transition matrix is P = (en P22 
Po it 
and S = 
S2 
) 
is the 
Par 7 
P37" 
Pas 
S3 
block-diagonal matrix of primitive stochastic complements. Recall from Exercise 
7.10.3 on page 889 that if 
S;= Pi +R,(I-Z,)*Ci, 
then the deviation from complete reducibility is defined to be 
§ = ||P —S|],, = 2max |[Ri ||... 
Since each stochastic complement is a primitive stochastic matrix, the Perron— 
Frobenius theorem guarantees that each unit eigenvalue of S; is simple, so, 
for sufficiently small 
6, continuity of the eigenvalues dictates that the non- 
unit eigenvalues of each S; must necessarily be clearly separated from its unit 
eigenvalue—otherwise the o(P) would contain a cluster of at least 3 non-unit 
eigenvalues positioned near \ = 1. In other words, 
P > S as 6 — 0, and the 
cluster consisting of the 2 non-unit eigenvalues together with 
4 = 1 in o(P) 
must split and map to the 3 unit eigenvalues in o(S)—one unit eigenvalue in 
each o(S;). This splitting effect is pictorially illustrated below when m = 9. 
o(P) 
(aay ; 
¥ 
J 
1 
1 
1 
a (Si) 
a (S2) 
a (Ss) 
FIGURE 7.11.2: SPECTRUM SPLIT IN A 9 X 9 WEAKLY COUPLED SYSTEM 
H. Simon and A. Ando, Aggregation of variables in dynamic systems, Econometrica, 29(1961), 
pp. 111-138. 
: Details related to proof sketched here are in the author's article Stochastic complementation, 
uncoupling Markov chains, and the theory of nearly reducible systems, SIAM Rev., Vol 31, No. 
2, 1989, pp. 240-272. 

894 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
This kind of spectrum splitting almost always occurs in weakly coupled systems, 
but there are rare pathological cases that can occur when the eigenvalues of P 
are incredibly ill conditioned! The spectrum splitting depicted in Figure 7.11.2 
means that each power sequence 
a? 
(€4+ 1) 
a] 
(1)S;. 
for F002, 
rapidly converges to s/ (the Perron vector for $;), and hence Si — es} rel- 
atively fast. While not necessary, assume for convenience that S; is diagonaliz- 
able, and order the eigenvalues of Smym as 
Ar = Ag =Azg =1 >> [ral = |As| [+--+ > [Am 
lpi 
so that s=2( et )a in which D = diag (Aq, s,---;Am).- If 
fist 
S° = lim S* = 
fos} 
too 
fst 
in which each f; is a column of ones of appropriate size, then it can be shown 
that 
|P* 
— S|], < t6 + #]Aal*, 
where 6 is the deviation from complete reducibility, and « = ||Z|| \-Aga = 
Consequently, in the short run, 
P"''2 8S" 
"for 
eT. 
(7.11.5) 
Moreover, if 
T 
: 
s' 
= lim m7 (0)S* = (a187, a283, a385), 
then for each t = 1,2,3,---, it will follow that 
Il? 
(t) — 
87 ||, <t5 
+ Kral'. 
(7.11.6) 
Since « is a constant and Aq << 1, the term «|A4|' has a negligible effect in 
the short run so that when ¢ € TZ, 
ni (t) ws? = (a1s1, 285 , 01385 ), where 
a; = 7 (O)e, 
(ALT) 
which is what (7.11.1) says. To see that the ratios of components within each 
cluster that are realized during the short run in (7.11.2) are approximately main- 
tained throughout the evolution beyond the short-term stabilization interval 
Examples are given in the author's article cited in the previous footnote. 

7.11 Simon—Ando Theory for Weakly Coupled Systems 
895 
Lo (fi, ts), consider t > to, and let t = t+ 7, where f € Z. As observed 
in (7.11.5), P' = S@© so that 
w(t) =n" (+7) = 27 (7)P! w 27 (r)S™ = (B87, Bost, 383 ) 
in which 6; = m7 (r)f;, where f; is a column of ones of appropriate size. Thus 
for states S,,,S,, in the same cluster C;, 
m(t) 
(C8) 
(eP),, 
HN 
ul) 
(aoet) Ew 
Applying Simon—Ando 
There are countless applications of the Simon—Ando principles, but only two 
are mentioned here. First, as alluded to earlier, the utility of Simon—Ando can 
be realized for large-scale systems for which the transition matrix P is not 
available, perhaps due to lack of sufficient data. But if the system's behavior can 
be empirically observed for a short period of time, then the system's long-run 
relative equilibria can be easily inferred. Here is how. Suppose that after a short 
period of time the state of the system is empirically observed at t =¢ € Z when 
the system is in a short-term approximate equilibrium so that 
nm" (é) = (ai (t), 73 (t), --+, 7p (E)) © (181, 0283, -++, ORS, 
)- 
While the constants a; and the censored distributions s? are not directly ob- 
servable, they nevertheless can be approximated from the information in 77 (t) 
Became 
te 
(1, 
1) et) hens: 
ft; 
se 
4 
wit 
at 
a, = a,(1) = a,(s? 
f,) = (ays? 
)f, wai Of 
and 
si x 
4 \ on a : 
This means that a; is approximated by the probability of being in cluster C; at 
time t = ¢ and the censored distribution s} is approximately the corresponding 
conditional distribution at time ¢. It follows from (7.11.4) that in the long run, 
the ratio between the equilibrium (limiting) values between two states S;, and 
Sy, in the same cluster C; will be 
Tx, (oo) 
Ness (t) 
Te, (t) 
Ty, (OO 
too Ty, (t) 
Ty, (é) 
d 
In other words, empirically watching a weakly coupled system evolve for a short 
period of time will predict the relative equilibria within each cluster that would 
be observed if the system were allowed to evolve indefinitely. 

896 
Chapter 7 
Perron—Frobenius and Nonnegative Matrices 
Reverse Simon—Ando for Finding Clusters 
ay 
A novel application involves determining clusters in a data set containing n 
data points in R'™, where each data point is a column in an m x n matrix A. 
The fundamental theorem of clustering is a generally recognized principle that 
asserts "nothing works always." Consequently, there is nearly an endless supply 
of clustering techniques available, and a researcher's job is to choose one that is 
well-suited for their task at hand. Reverse Simon—Ando is one possible choice. 
A similarity measure (or metric) d(Ax;,Axk) 
is first chosen that relates 
how close one data point is to another. For example, an ordinary vector norm 
|| 
Ax; —-Axs|| could be used, or an angular measure such as the cosine of the 
angle between Ax; and Ax, could be employed. Another choice often used in 
clustering is the metric known as the Gaussian kernel that is given by 
d(A.;, Axx) = eal Anj Ase ll? /207 
in which o is a free parameter. And there are countless other techniques for 
creating a similarity matriz Cyyn in which cp; = d(A,;, Axx), which is sym- 
metric when d(A,x;, Axx) = d(Axn, Axj). Often times a clustering algorithm is 
run several times to create an ensemble similarity matric whose (i,j) entry is 
the number of times that A,,; clusters with A,,;. 
Once a similarity matrix C is created it is scaled by setting P = DCD, 
where D is a diagonal matrix such that P is doubly stochastic (i.e., row sums 
and column sums both equal 1) by a symmetric variant of the Sinkhorn—Knopp 
algorithm! Assuming that P is primitive, the limiting (or stationary distribu- 
tion) for P is uniform—i.e., lim;,.. 7? (t) = a? =e/n for all m7(0) (see the 
solution to Exercise 7.9.2 on page 879). Consequently, if the data in A can be 
clustered, then it can be shown that some symmetric permutation of P = DCD 
will be nearly block diagonal? 
The catch is that the permutation is not known in advance, and it is rarely 
apparent. This is where reverse Simon—Ando enters. The term "reverse Simon— 
Ando" is used because instead of trying to infer something about the long-term 
characteristics by observing short-term behavior, the strategy is reversed in the 
The scaling matrix 
D_ is obtained by first scaling rows to make all row sums equal to 
1 
and then rescaling the resulting matrix to force each column sum equal to 
1. This process 
is iterated, and under reasonable conditions diagonal matrices D; and Do (unique up to a 
scalar multiple of each other) are eventually produced such that D,CDz2 is doubly stochastic 
with D2 =€Dy. This is generally known as the Sinkhorn—Knopp algorithm because it was 
popularized by Richard Sinkhorn and Paul Knopp in their article Concerning nonnegative 
matrices and doubly stochastic matrices, Pacific Journal of Mathematics, Vol 21, No 2, pp. 
343-348, 1967. Taking D = JED, yields a diagonal matrix such that P = DCD 
is doubly 
stochastic and symmetric. 
The proof of this along with the symmetric variant of Sinkhorn—Knopp and associated facts 
can be found in the article by the author and Charles D. Wessell, Stochastic data clustering, 
SIAM J. Matrix Anal. Appl, Vol 33, No 4, pp. 1214-1236, 2012. 

7.11 Simon—Ando Theory for Weakly Coupled Systems 
897 
sense that the goal is to use the known long-run stationary distribution m7 =e /n 
in conjunction with the short-run Simon—Ando principles to reveal clusters in 
the data. 
If Prxn is partitioned as in (7.10.1) on page 882 such that P;, is nj x nj, 
then (7.10.7) on page 885 together with the fact that +' =e/n ensures that 
so when P is nearly block diagonal, the approximate short-run equilibrium 
guaranteed by (7.11.1) on page 892 is attained—i.e., for t € Z, 
nm" (t) & (a18], 0283,..., @eS,) 
for constants aj. 
It follows that, when P is nearly block diagonal and t € Z, the short-run 
approximate equilibrium will be 
a 
a 
a 
(ay 
m(tya (2, SEL 
tage 
ae 
i, ea ae 
(7.11.8) 
Ny 
Ny 
ng 
ng 
? 
} 
Uk 
Nk 
Therefore, 
e 
Components in the same cluster produce nearly equal values in 17 
(t). 
When P is just a symmetric permutation of a nearly block-diagonal matrix, the 
nearly equal components shown in (7.11.8) that determine clusters will not nec- 
essarily appear contiguously, but nearly equal values will generally be apparent 
by inspection, so while knowledge of the symmetric permutation is not required 
to reveal clusters—-it can be deduced if needed. 
The only remaining issue is the proper choice of k, the number of clusters 
to look for. As indicated in Figure 7.11.2 on page 893, k is generally given by 
the number of eigenvalues of P that are clustered near A = 1. The doubly- 
stochastic matrix P = DCD is symmetric when C is, so o (P) is real, which 
means that gaps in these eigenvalues are readily identifiable. The procedure for 
using these gaps is described in the following summary. 


CHAPTER 
8 
Matrix Analysis 
via 
Resolvent Calculus 
8.1 
A SHORT REVIEW 
The continuity and differentiability properties of eigenvalues and eigenvectors 
are fundamental to the theory and practice of linear algebra. Anyone (students 
or practitioners) wanting to understand and use linear algebra at a level beyond 
algebraic manipulations needs to have some appreciation of the mathematical 
foundations of these topics, and this requires a transition from matrix algebra 
to matrix analysis. The purpose of this chapter is to move in this direction 
by applying features of the resolvent calculus that was popularized by Tosio 
Kato. Consequently, some familiarity with real and complex analysis is required. 
Tosio Kato (1917-1999), born in Japan, earned a Bachelor of Science degree in physics in 
1941 from the Imperial University of Tokyo. But the war interrupted his education. When he 
returned to his studies, he earned a doctorate in physics in 1951 for his thesis entitled On the 
Convergence of the Perturbation Method. In 1954-1955 Kato was invited to visit the United 
States, spending time at the University of California, Berkeley and New York University, which 
brought him notoriety in both the physics and mathematical communities in the US. In 1958 
Kato was appointed a professor of physics at the University of Tokyo, where he led a research 
group in the physics department, and in 1962 he became a professor at the Ue, of Califor- 
nia, Berkeley, a position he held until 1988. His awards include the 
prestigious 1960 Asahi Prize in Japan and the 1980 Norbert Wiener 
Prize in Applied Mathematics given jointly by the American Math- 
ematical Society (AMS) and (SIAM). He is perhaps most famous 
for his two influential books Perturbation Theory for Linear Oper- 
ators, Springer-Verlag, 1966, 1976, 1980, and A Short Introduction 
to Perturbation Theory for Linear Operators (often fondly referred 
to as "baby Kato"), Springer-Verlag, 1982 and 1995. 
Tosio KATO (CIRCA 1968) 

900 
Chapter 8 
Matrix Analysis via Resolvent Calculus 
While it is assumed that the reader has some background in these areas, a quick 
review may be helpful.' Here are some of the basic aspects of continuity and 
differentiability that will be used. 
Notation and Terminology 
The following notation and terminology are adopted throughout this chapter. 
> 
FF is ascalar field that is either R or C. 
> 
Sn = {e1,€2,..-,€n} is the standard basis for F". 
> 
 ||x|| denotes a norm on F" or F"*". Choosing a particular norm is generally 
unnecessary in this chapter because all norms are equivalent in the sense 
explained in Theorem 1.5.5 on page 35. However, for matrix norms it is 
frequently necessary to have 
||I|| = 1 (which holds for any induced norm), 
so this assumption will be made when needed. 
> N(x) is an €-neighborhood of x € F"—i.e., given 
0<e€¢€R, N(x) is the 
set of points z € F" such that |/z—x|| <e. 
> 
OCF" isan open set—i.e., for each x € O, there exists an e-neighborhood 
of x such that N(x) C O. 
> 
KCF"" 
isa 
closed set—i.e., its complement F"\K is open. 
> 
A set K C F" is compact if it is closed and bounded—i.e., in addition to 
being closed, there exists M € R such that ||z—x|| < M for all x,zEK. 
> 
f:OCF" >F" 
isa 
bounded function on © if there exists 
M € R such 
that || f(x)|| <M for all x € O. 
> 
The cartesian product of sets A and B is 
Ax B= {(a,b)|ae A, be B} 
(the set of all ordered pairs). 
Since the purpose of this section is review, most theorems are presented without proof. The 
technical details can be found in most good texts on real and complex analysis. Helpful real 
analysis references are Walter Rudin's second edition of Principles of Mathematical Analysis, 
Robert Bartle's The Elements of Real Analysis, and Hans Sagan's Advanced Calculus. For 
complex analysis see Complex Variables and Applications, 7th ed. by James W. Brown and Ruel 
V. Churchill, Introduction to Complex Analysis in Several Variables by Volker Scheidemann, 
Introduction to Complex Analysis, Part 11, Functions of Several Variables by Boris V. Shabat, 
or Analytic Functions of Several Compler Variables by Robert Gunning and Hugo Rossi. 

8.1 A Short Review 
901 
Continuity 
A function f :O CF" + F" is continuous at a € © if for every real ¢ > 0 
there exists a real 6(€,a) > 0 such that for z € O, 
z—all<d(e,a) 
=> |[f() 
- fla)ll <e, 
or equivalently, limz_,, f(z) = f(a). 
e 
f is uniformly continuous on O when 6 = 6(e) depends only on e—i.e., 5 
is independent of a. 
e 
Ifthe standard coordinates of f(z) are 
fi (2) 
RICE aes ( 
: ) 
fm(2) 
then f is continuous at a if and only if each coordinate f; : F" > F is 
continuous at a. 
e 
If f is continuous everywhere on acompact set K C F", then f is uniformly 
continuous and bounded on K. 
e 
If f:F" >R 
is continuous on a compact set K CF", then f attains its 
maximum and minimum values on K. 
Example (A Discontinuous Function) 
Saying lim,-., f(z) = f(a) means, among other things, that the result is inde- 
pendent of the direction along which z > a. With this in mind it is clear that 
for z= (x,y), the function f : R? + R given by 
rY 
—— _ fe 
0 
j= | 
PP 
or Z = 
' 
0 
for z=0 
is not continuous at a = 0 = (0,0) because f(z) ~ 1/2 as z — 0 along the 
line y = x while on the other hand 
f(z) + —1/2 as 
z— 0 along the y = —z. 
Differentiability 
A function f : O C F" > F" is said to be differentiable at a € O if there 
exists a linear transformation f'(a):F" — F™ such that for nonzero é € F", 
Ifat+e)-fa-f@El _, 
bet 
\|E| +0 
lle|| 
e 
When it exists, f'(a) is unique and is called the derivative of f at a. The 
value of f'(a) at c € F" is denoted by f'(a)(c) € F". Some texts call f'(a) 
the total derivative and some refer to it as the Fréchet derivative. 

902 
Chapter 8 
Matrix Analysis via Resolvent Calculus 
e 
If f'(a) exists, then f is continuous at a. 
e 
For f:F—F 
the derivative f(a) reduces to the ordinary formulation of 
differentiation—.e., 
(see Exercise 8.1.3). 
f(a +e) — f(a) 
€ 
/ 
en 
BE 
f'(@) = lim 
Example (Sample Derivative in R?) 
To show directly that the function f : R? — R? defined by 
f(x) = f(a1,22) = (2} + 23, 2122) 
is differentiable at each a = (a1, a2) € R', let © = (€1, €2), and observe that 
f(at+e) —f(a)= (2(ae1 + a2€2), a1€2 + a2é1) + (<i +3, e162). 
If La : R? > R? is the function Lat (2(ai21 + A2%2), a\£2 + ax), then 
it is straightforward to see that La is a linear operator on R?, and 
f(a+e) — f(a) —La(e) = (e? + 6, e1€2) = (llell. e162) 
Since 
|| f(a + €) — f(a) — Lale) 
(€1€2)? 
2 
> 
9 
llella 
llella 
it will follow from (8.1.1) that f is differentiable at a and f'(a) = La pro- 
vided that it can be demonstrated that limyey,+0 (€1€2)"/ llell3 =O" 
I hiss 
accomplished by arguing that for each é > 0, there exists a 6 =6 (€) such that 
fe onl 
= lel ii 
(8.1.2) 
(€1€2)? 
a 
IIElla < 6 
Taper 
llello 
(8.1.3) 
The argument is easy. 0 < (€,—€2)? yields 2eye2 < llell; which in turn implies 
Lo 
) 
(€1€2)" — llelle 
ely ~ 
4 
' 
so setting 6 = 2Vé produces (8.1.3). Therefore, lim|je}),—+0 (€1€2)?/ |lel|5 = 0, 
and thus (8.1.2) ensures that f is differentiable at a and f'(a) = La. 
Some familiar properties of differentiation are summarized below—see any 
of the texts mentioned in the footnote on page 900 for details. 

8.1 A Short Review 
903 
Properties of Derivatives 
If f,g: 
OCF" + F" and ¢: O > F are differentiable at a € O, and if 
a €F, then the indicated derivatives exist and the following properties hold. 
° 
(af +9)'(a) = of'(a) + g(a) € LEE", F") 
° 
 (¢9)'(a)(e) = [9(a)(©)]9(a) + o(a)[9'(a)(c)] € F" 
(8.1.4) 
© 
(fg) (ac) = (f/(a)(c) | 9(a)) + (F(@) |9/(a)(©)) € F 
e 
Chain Rule: If h = go f is the composition of g and h, and if g is 
differentiable at b= f(a), then h'(a) = g'(b)o 
f(a). 
Directional Derivatives 
Let 
ac O, 
OAuUECF", and 
0#e€ EF. When it exists, the limit 
Du f(a) = lim fla +eu) — 
f(a) 
e>0 
€ 
an 
is called the directional derivative at a in the direction of u. Suppose that the 
derivative f'(a) as given in (8.1.1) on page 901 exists. To derive the relationship 
between the directional derivative D, f(a) and f'(a), notice that the linearity 
of f'(a) implies that f'(a)(eu) = ef'(a)(u), and observe that 
1 |) 
fat+eu) — fla) 
mile 
€ 
2 F"(a)(u)| = a [E{re ey (are cf'(ay(u)} | 
_ fla+ eu) ~ fla) ~ef'a)(u)ll _ fla 
+ 
eu) — fla) ~ fi(a)(eu)| 
le! [Jul 
leu 
The existence of f'(a) means that as 
« — 0 (from any direction), the limit of 
the right-hand side of this expression is zero, and hence the left-hand side goes 
to zero, which in turn implies that 
f(a+eu) 
— f(a) 
S 
+ fi(a)(u). 
In other words, the directional derivative of f at a in the direction of u is 
Du f(a) = lim 24) —F9) _ pay (uy, 
(8.1.5) 
¢ 
which is the derivative of f at a, evaluated at u. Consequently: 
e 
If f is differentiable at a, then all directional derivatives exist at a, and 
Duf(a) = f'(a)(u). 

904 
Chapter 8 
Matrix Analysis via Resolvent Calculus 
Partial Derivatives 
Let f(z) = f(z1,22,.--,2n) € F, and let 
Sn, = {€1,€2,...,€n} 
and 
S = {Il} 
(8.1.6) 
be the respective standard bases for F" and F. It follows directly from the 
definition of partial differentiation that the directional derivatives De, f(a) € F 
in the direction of standard-basis vectors e; are the usual partial derivatives 
evaluated at a. That is, 
De, f(a) = lim 
: 
5°) 
(8.1.7) 
e 
Therefore, if f is differentiable at a, then all partial deriva- 
(8.1.8) 
tives Of /Oz; exist at a. 
The Jacobian Matrix 
Now suppose that f: OCF" > F", and let 
ze O. If S, and S,, are the 
respective standard bases for F" and F" so that 
"1 
fi(z) 
la. 
= (7 | and els, = (_ ) 
Zn 
fm (2) 
then it is easy to see that 
fi(a+t ee;) — fila) 
fered fer 
; 
Sin 
€ 
: 
fm(at €€;) > fm(a) 
€ 
It follows from (8.1.5) and (8.1.7) that the standard coordinates for the direc- 
tional derivatives De, f(a) € F" are 
lim fi(a+ ee;) — fi(a) 
e—>0 
€ 
[De, f(a)] 5 = | lim ——__4_-+* 
= 
Sm 
lim fm (a SP €e;) — fim(a) 
€>0 
€ 
eft) 
De, fi (a) 
az; 
De; fim(a) 
Ofm (a) 

8.1 A Short Review 
905 
This together with (8.1.5) produces the coordinate matrix of f'(a) as 
P@]s.s, = (Penis, | @(errlsn | | '@enlsn) 
=((Daflalls, | Def@lsn 
| | Def@isn) 
soe) aw 
he 
_|2@ 4o . #e |_5 
This is the Jacobian matria, and it is the primary tool used to manipulate deriva- 
tives in higher dimensions. This is summarized below for future reference. 
8.1.1. Theorem. (The Jacobian Matrix) If f :O CF" — F" is differen- 
tiable at a € O, then the standard-coordinate matrix of f'(a) is 
Ofi 
Ofy 
oft 
a 
bn 
ue 
aa 
Ofs 
Of 
Of 
O12 
ae eC 
Bin 
Oh 
Of 
OZ (a) a, 
oe 
OZn (@) mxn 
which is called the Jacobian matriz of f at a. It is used to determine 
the standard coordinates of f'(a) evaluated at c € F" by applying 
Theorem 4.5.3 on page 510 to write 
[f(a))ls,, = [f'(@a)]s.sn[els, = Ilels,. 
(8.1.9) 
When it is understood that all coordinate representations are with re- 
spect to the standard bases, then the coordinate notation may be sup- 
pressed to write f'(a)(c) = Jc. For f:R" > R, the Jacobian of f at 
Of 
ie 
of 
a (a) 
a is the transpose of the familiar gradient vector V f(a) = 
of 
O2n, (a) 

906 
Chapter 8 
Matrix Analysis via Resolvent Calculus 
Example (Sample Derivative in R? Continued) 
: 
; 
The sample derivative example on page 902 shows that the function f : R" 
~ R 
defined by 
f(x) = f (x1, 22) iz (xi i x5, ©1 2) 
is differentiable at each a = (a1, a2) € R?. If Sy is the standard basis for an 
then the Jacobian matrix for f is 
Ofi 
ofi 
corre 
ial be ale eA bel re 
=['@s.=| 
of) are} 
har 
a)" 
Ox, 
0x2 
Consequently, the value of f'(a) at c = (ci,c2) (with respect to the standard 
coordinates) is 
_ 
(2a, 
2a 
ci \ _ [ 2(a1¢1 + a2c2) 
[f'(a)(©)] 5, a [f(a] s, [ec] 
S2 a J [ec] 
Scan ( 
ee 
ay ) 
6a Zz ( 
aoc, + a1c2 ), 
and this corroborates the result in the example on page 902 that says 
f'(a)(c) = La(c) = (2(a1c1 + a2c2), ice + a2¢1). 
Real vs Complex Differentiability 
No distinction has been made between F = R and F = C so far, but there 
are differences between real and complex differentiability. In the real case the 
following conditions are sufficient for differentiability. 
8.1.2. Theorem. If all of the first-order partial derivatives of a real func- 
tion f:OCR"—+R"™ 
exist at each point in some neighborhood of 
a € O, and if all first-order partial derivatives are continuous at a, 
then f is differentiable at a. 
However, things are more "complex" in the complex case. If a function 
f :C-—+C 
ofa 
single complex variable 
z = x + iy is decomposed into real 
and imaginary components f(z) = u(z,y) +iv(z,y), then f can be considered 
as a real mapping f : R* + R? by writing f(z,y) = (u(z,y),v(z,y)). But 
as the following example shows, the differentiability of f :R? > R? as a real 
function at a point (x9, yo) does not ensure the differentiability of f:C3C 
as a complex function at zp = 0 + iyo. 

8.1 A Short Review 
Example (Real Differentiability =4 Complex Differentiability) 
Consider the function f(z) = |z|? =2?+y?. In this case u(x,y) =2?+y? and 
v(@,y) = 0, and Theorem 8.1.2 ensures that the real function f:R*? > R', 
defined by 
f(x,y) = (2* +y", 0), 
907 
is differentiable at all a € R?. However, the complex function f : C > C defined 
by f(z) =|z|? is not differentiable except at z =0 because for all 29, 
eee 
a eee aaa = 
+H +4, 
and as € —-> 0 along the real axis this expression goes to % +z, but as 
«> 0 
along the imaginary axis the expression tends toward 2% — zp. Thus the limit 
(and hence the derivative f'(zo)) is not defined except at zo = 0. Thus real 
differentiability of f does not imply that it is complex differentiable. 
Cauchy—Riemann Equations 
For two real functions u,v : R? > R, consider the differentiability of the complex 
function f(z) = u(z,y) +iv(z,y) at 29 = x9 + iyo. As shown on page 904, the 
differentiability of f : R? + R? at (xo,yo) guarantees the existence of Ou/Oz, 
Ou/Oy, Ov/Ox, and Ov/Oy at (xo, yo), but the differentiability of f :C > C 
at 2 = Zo + iyo says more—it forces the following Cauchy—Riemann equations 
to hold. 
8.1.3. Theorem. (Cauchy—Riemann Equations) For f :O C C > C, let 
f(z) = ulz,y) +iv(z,y). If f is differentiable at zp = zo + iyo € O, 
then Ou/Oz, Ou/Oy, Ov/Ox, and Ov/Oy exist at (xo, yo), and these 
partial derivatives satisfy the Cauchy—Riemann equations 
du 
_ du 
Ox 
Oy 
Ou 
Ov 
and: 
=— ==. 
at 
(2 
: 
8.1.10 
x 
By 
Ax 
(Zo, Yo) 
( 
) 
In other words, the Cauchy—Riemann equations are necessary for the 
existence of f'(zo). However the Cauchy-Riemann equations are not 
sufficient to ensure differentiability—more is needed. 
e 
If the first-order partial derivatives of u and v exist throughout 
O and satisfy the Cauchy-Riemann conditions (8.1.10), and if these 
partial derivatives are continuous at (zo, yo), then f'(z0) exists. 
Analytic Functions 
The differentiability of a function at a single point is seldom as important as 
the differentiability of the function throughout some region. For this reason the 
concept of analyticity plays a central role in complex analysis. 

Chapter 8 
Matrix Analysis via Resolvent Calculus 
Just as in the case of real functions, issues involving f :C" — C"™" can be 
reduced to those involving coordinates with respect to the respective standard 
bases S, and S,, for C" and C™. For z€C" and f(z) eC", write 
21 
Br 
Yi 
fi (z) 
bla = (2 
)=(7) #i(2) + 
and 
Ves, = ( ) 
ae 
oe 
Yn 
fm(z) 
This allows f(z) to be analyzed in terms of its individual coordinate functions 
f;(z) : C" — C. For each such function there are n associated univariate 
complex functions f;,(z~) : C — C that are obtained by holding all variables 
except 2% = %_ +iyp constant. It follows from a theorem by Hartogs' stating 
that f; is "totally" analytic (at a point) in the sense of Definition 8.1.4 if 
and only if f; is "partially" analytic in the sense that each univariate function 
fi, (Ze) is an analytic function of the single variable z,. This is another place in 
which complex analysis differs from real analysis because in the "real world" a 
function f :R" > R can be partially differentiable in each variable separately 
but can fail to be totally differentiable—see Exercise 8.1.8 on page 910. Below 
is a summary (without proofs) of some relevant facts concerning analyticity and 
the Cauchy—Riemann equations. 
Summary of Analyticity 
Let 
f: 
OCC" 4C", and let the coordinate functions f; :O CC" > C be 
decomposed into real and imaginary components f;(z) = u;(x,y) + iv;(x,y) 
Assuming that each first order partial derivative Ou;/Ox,, Ov;/Oyx, Ouj/Oyr, 
and 0v;/Oxr, exists and is continuous in O, the following statements are true. 
e 
fj is analytic in © if and only if for each k = 1,2,...,n the Cauchy-— 
Riemann equations 
Ou; ad Ov; 
Ou; nai 
Ov; 
é 
Oz, => Our 
a 
Oyp = "BER 
hold in Or 
(8.1.11) 
Friedrich Hartogs (1874-1943) published his theorem in 1906. The text by Gunning and Rossi 
mentioned in the footnote on page 900 refers to a similar result as "Osgood's lemma" in honor 
of William F. Osgood (1864-1943). 

8.1 A Short Review 
909 
e 
f is analytic in O if and only if each coordinate function f; of f is analytic 
im, ©: 
e 
Analytic functions are precisely the functions that can be locally represented 
by a convergent power series. 
p-Norms on C" Are Not Analytic 
Norms are another example where differentiability in R" does not extend to dif- 
ferentiability in C". While f(x) = ||x||, is certainly differentiability for x € R", 
the example on page 907 shows that f fails to be analytic as a function on C". 
In fact, none of the standard p-norms on C" are analytic. This becomes im- 
portant in subsequent sections when issues concerning the differentiability of 
eigenvectors are examined because it is necessary to somehow isolate a uniquely 
defined eigenvector, and in the case of one-dimensional eigenspaces this is ac- 
complished by normalization. However, normalization by one of the standard 
norms usually destroys analyticity in C", so other means of normalization must 
be sought. For this reason it is better to attack questions of differentiability of 
eigenvalues and eigenvectors by examining the issues in terms of the spectral 
projectors—they are uniquely defined, and no normalization is required. This 
strategy is developed in subsequent sections. 
Exercises for section 8.1 
8.1.1. Let f:F" > F" bea 
linear function. 
(a) Prove that f is continuous at each a € F". 
(b) Does your argument hold for affine functions g(x) = f(x) +c, 
where c € F" is a constant vector? 
(c) Is your argument valid for linear functions f : F"*"" > F?*?? 
8.1.2. Let f :C" > C"™ be an affine function—i.e., f(z) = L(z) +b, where 
L € L(C",C™) 
is linear and b € C™ is constant. Prove that f is 
differentiable at each a € C" and f'(a) = L so that f'(a)(z) = L(z) 
for all z € C". (This is the generalization of the familiar one-dimensional 
result that says if f(z) = Jz+, then f'(a) = | for all 
a€C.) 
8.1.3. Let f : C > C be differentiable at a € C in the sense of (8.1.1) on 
page 901. Explain why this reduces to the ordinary formulation of the 
derivative in C. That is, explain why (8.1.1) implies 
Faye 
f+) £0) 
e—0 
E 

910 
Chapter 8 
Matrix Analysis via Resolvent Calculus 
8.1.4. 
8.1.5. 
8.1.6. 
So1.7. 
8.1.8. 
Consider the derivative (or total derivative) f'(a) of f : R" > R at 
a € R". For c € R", explain why evaluating f/(a)(c) is accomplished 
by using the gradient V f(a) to compute 
n 
of 
f'(a)(o) = (VF (a)le) = V7Fla)e = D5 (a). 
1=1 
Let T: C"*" >C 
be the function that maps each matrix Zpxp, to 
its trace—i.e., T(Z) = trace(Z). Explain why T'(A) exists at each 
AéC"*"" and why 
T'(A)(C) =trace(C) 
for all 
Ce C?*". 
Since C"*" is isomorphic with Cc the trace of a matrix can be con- 
sidered as a function 
T:C" > C. That is, instead of thinking of the 
trace as an operator on a matrix Z,.,, unravel Z row-by-row to turn 
it into a column vector by making the identification 
ae 
il 
Dixie? (tas, Basse 
dy 
21k tatpeaneas onl 
ieaeerass sind Ey 
and define T(Z) = )7'_, ii. With respect to the standard bases S,2 
(ordered as indicated in the above unraveling) and S; = {1} for ce 
and C, respectively, explain why the Jacobian matrix for T'(A) 
is 
given by 
J = [T'(A)] 
5 os (1,0,.++010,1,..+ 
0] +++ 0,0,.0-51) 142 
~ Inxn 
(the unraveled identity matrix) for all A. Use this to corroborate Ex- 
ercise 8.1.5 by unraveling Cy, to show that T'(A)(C) = )°y_, ci. 
Prove that the set of nonsingular matrices in C"*" is an open set, and 
then explain why the singular matrices in C"*" constitute a closed set. 
Hint: Recall Theorem 3.5.9 on page 367. 
Hartogs' theorem ensures that for a function f : 
O C C" > C of 
several complex variables, "partial analyticity" and "total analyticity" 
are equivalent—i.e., being an analytic in each variable z; while all others 
are held constant is equivalent to f being an analytic function in the 
sense of Definition 8.1.4 on page 908. However, things are different in 
the "real world." In particular, if f : R" > R is differentiable in each 
variable separately, there is no guarantee that f is differentiable (or 
even continuous). Illustrate this fact with the function 
f(a,y) = 
aor for (x,y) # (0,0) 
0 
for (x, y) = (0,0) 

8.2 Eigenvalue Continuity 
911 
8.2 EIGENVALUE CONTINUITY 
A fundamental fact of linear algebra is that eigenvalues vary continuously with 
the entries in the underlying matrix. In spite of its importance in both theory and 
practice, the discussion and proof of this theorem are frequently omitted from 
linear algebra textbooks and courses for reasons discussed in the footnote on page 
757. A matrix theory approach relying on Schur's triangularization theorem was 
developed in Theorem 6.1.1 on page 757. But a more revealing approach that can 
be carried to higher levels of analysis stems from the use of resolvent integrals 
popularized by Tosio Kato (page 899). 
The strategy is to first establish the continuity of the spectral projectors, 
and then to use this to deduce eigenvalue continuity. The spectral projector 
approach to continuity has two significant advantages. First, it naturally leads 
to an easy analysis of eigenvalue differentiability, and second, it is extremely 
elegant—it epitomizes the simultaneous beauty and utility of matrix analysis. 
Resolvents 
For a given fixed A € C"*" and any z ¢a(A), 
a resolvent matrix was defined 
in Definition 4.10.5 on page 605 to be R(z) = (zI— A)~!. As a function of z 
alone, R(z) is analytic at each € ¢ a(A). This follows because the entries in 
R(z) are rational functions involving determinants (by (9.3.8) on page 961), and 
determinants are just sums of products. In fact, it is easy to see (Exercise 8.2.1, 
page 920) that 
dR(z) 
dz 
However, when z and Z vary simultaneously, R(z,Z) = (zI—Z)~! isa function 
on the cartesian product space C x C"*" = {(z,Z)|z¢€C, 
ZeEC"*"}. If z 
hits an eigenvalue of Z while both are varying independently, then R(z, Z) fails 
to exist at that point. This kills the possibility for continuity and differentiability 
of the double-varying resolvent. The continuity of the eigenvalues of Z cannot 
be employed to rectify this because the goal here is to use resolvents to establish 
eigenvalue continuity. The next lemma resolves this issue. 
= —{R(z)'}. 
(8.2.1) 
8.2.1. Lemma. For a fixed pair (€9,A0) with 9 ¢ o(Ao), there is a 
neighborhood Nq(f,A0) 
C C x C"*" of (€, Ao) such that 
(€,A) €Na(So,A0) 
=> €¢a(A). 
In other words, R(€,A) exists everywhere in some neighborhood of 
(9, Ao). The norm on C x C"*" is understood to be 
(8, B)|| = [8] + ||Bllaz, 
where || x ||,¢ is an induced norm on C"*" (see Exercise 1.9.6, page 
89). The subscript "JZ" is suppressed when context makes things clear. 

912 
Chapter 8 
Matrix Analysis via Resolvent Calculus 
Proof. 
Let a be a real constant such that 0 <a <1/||R(&, Ao)||, and let 
(€,A) € Na(&o, Ao) so that 
I\(é, A) — (0, Ao)|| <a, 
or equivalently, — 
|€ — 0] + ||A — Aol| <a. 
Set 
e=€-—) and 
E=A-— Ao so that |e|+ ||/E|| <a, and write 
él — A = (I — Ao) + dE = [I - (el— E)R(E0, Ao)| (oI — Ao). 
(8.2.2) 
Since 
ll — E)R(Eo, Ao)|| < |lel — El] ||R(€0, Ao) || < (lel + El] ) R(E0, Ao)|] < 1, 
it follows that I — (eI — E)R(&, Ao) is nonsingular (see pages 618-621), and 
hence (8.2.2) implies that €I— A must also be nonsingular. 
& 
Not only does the resolvent function R(é,A) exist at every point in some 
neighborhood of (€), Ao) with 
 ¢ (Ao), but in fact it is analytic at (€0, Ao). 
8.2.2. Theorem. The resolvent function R: C x C'%" > C'*" is ana- 
lytic at each point (€,A) for which € ¢a(A), and 
R'(é, A)(z,Z) = —R(é, A)(2I — Z)R(é, A). 
(8.2.3) 
Proof. 
Given a fixed pair (€,A) 
€ C x C"*" with € ¢ o0(A), 
the proof of 
Lemma 8.2.1 ensures that if 
0< a<1/||R(€,A)||, then R(€é+e,A+E) exists 
for all (c,E) € 
Cx C"*" such that ||(e, E)|| = Je] + ||E|| < a. For convenience, 
let 
F = eI —E 
so that ||F|| < |e] + ||E|| = ||(€,E)|| <a and |/FR(€, A)|| <1. 
Respectively replace € and A in (8.2.2) with 
€+¢ and A+E, and drop the 
"0" subscripts on €) and Ap. Employing a Neumann expansion yields 
R((é,A) + (e,E)) 
=R(E+e,A +E) =R(€,A)[I+FR(E,A)] 
= R(é, A)(I— FR(é, A) + {FR(é, A)}? +--+] 
= R(é, A) — R(é, A)FR(é, A) + R(é, A){FR(é, A)}? ie 
FR(é, A)}*. 
As seen in Exercise 2.3.20 on page 176, ||FR(¢,A)|| <1 implies 
1 
1 
-TFIIRG Ay] = To FY 

8.2 Eigenvalue Continuity 
913 
Using ||F|| < ||(e,E)|| with the above observations produces 
IR((é, A) + (c,E)) — R(é, A) + R(é, A)FR(é, A)]| 
eB) 
Bs 
aW$ ||FI' 
a* ||Fl| 
~ {1—ac? | FIl} Ie, E)|| ~ 
1-74 [FI 
Consequently, ||(€,E)|| > 0 forces ||F|| > 0, which in turn drives the right-hand 
side to zero. The function L: C x C"*" + C"*" that is defined by 
is clearly a linear function of (z,Z), and thus the resolvent function R_ is differ- 
entiable at each point (€,A) where R exists. And since Lemma 8.2.1 on page 
911 guarantees that R exists at each point in some neighborhood of (€, A), it 
follows that R is analytic at (€,A). The definition of the derivative on page 
901 says that 
R'(é, A)(z,Z) = L(z,Z) = -R(€,A)(eI-Z)R(é, A). 0 
An immediate corollary of resolvent differentiability is resolvent continuity. 
8.2.3. Corollary. The resolvent function R.: C x C"~" > C"*" is con- 
tinuous at each point (€,A) for which € ¢a(A). 
Spectral Projector Continuity 
Resolvent continuity implies that the spectral projectors are continuous functions 
of the entries of the underlying matrix. To see this, consider a fixed matrix A 
with } € 0 (A), 
and let [) be a counterclockwise oriented circle 
WE alice 
ape SA 
(8.2.4) 
around 2 that neither touches nor surrounds any other eigenvalue of A. Since 
I, is a compact set (because it is closed and bounded), and since R(€, A) exists 
and is a continuous function of € on [), the continuity properties on page 901 
ensure the existence of a real M >0 such that ||[R(€, A)|| <M for all € €T). 
For all matrices E such that ||E|| 
<1/M it follows that 
p(ER(E,A)) < ERE, A)|| < El |R(E, A)|| < EM <1, 
(8.2.5) 
and by Theorem 4.11.4 on page 620, this implies the existence of R(€, A + E) 
for all € 
€I'y. Theorem 4.10.6 on page 605 says that 
1 
G(A)=5— | RE, A)dé 
= Drie, 

914 
Chapter 8 
Matrix Analysis via Resolvent Calculus 
is the spectral projector for A, and 
G(A +B) sal R(é, A + E)dé 
(8.2.6) 
is the projector that is equal to the sum of the spectral projectors of 
A+E 
corresponding to the set of eigenvalues of A +E that are either in or on hy 
These observations pave the way for the following continuity theorem. 
8. 2.4. Thepre. for a fixed A, the function GIA + E) i 
in (8.2. 6) is 
continuous at 0. That is, 
dim, G(A +E) = G(A). 
(8.2.7) 
Proof. 
It is not always true that the limit of an integral is equal to the integral 
of the limit, so the proof of (8.2.7) is not as trivial as it might appear to be. 
Given € > 0, the aim is to demonstrate the existence of a 6 such that 
|E|| <6 
=> 
||G(A+E) — G(A)|| <e. 
(8.2.8) 
Begin with the observation that 
a 
Raise 
IRIE, A+E)-R¢€, A) a 
ul om 
R(€, A)|| |dg]. 
The inequality (8.2.5) in Ban ai with (4.11.14) on page 620 and the fact 
that ||R(€,A)|| < M for all € € Ty imply that 
\< 
IR(g, ADI El 
_M? [El 
~ 1—||RE, A)|| |B] ~ 1-4 ||B| 
Since fh, |dé| = 2zry, it follows that 
_ M*\El 
_ M* 
Elna 
| Secale 
Setting 6 = €/(r,M?+eM) produces (8.2.8), and thus the theorem is proven. 
R(Eé, 
A + E) — R(é, A) 
for all € ET). 
||G(A + E) — G(A) 
Eigenvalue Continuity 
Continuity of the spectral projectors produces eigenvalue continuity, but because 
multiplicities are involved, care must be exercised in articulating the precise 
nature of this continuity. 

om~n~ . 
a 
ae 
8.2 Eigenvalue Continuity 
915 
Proof. 
Let 
E=Z—A, and let G(Z) 
= G(A+E) be given by (8.2.6) where 
I) is the circle in (8.2.4) of radius r, around one eigenvalue \ € o(A) but 
excludes all other eigenvalues. Since rank (P) = trace (P) for all projectors P 
(by (4.7.11) on page 547 or Exercise 4.2.20 on page 452), and since spectral 
projectors as well as trace are continuous functions of their matrix arguments 
(trace is continuous because it is linear—see Exercise 8.1.1 on page 909), it follows 
that as 
E > 0, 
rank (G(A + E)) = trace (G(A + E)) — trace (G(A)) = rank (G(A)). 
In other words, for every €, > 0 there is a 5, >0 such that ||E|| <6) implies 
lrank (G(A + E)) — rank (G(A))| <). 
In particular, when €, = 1 there isa 6, >0 such that 
|E|| <6, = > |rank(G(A + E)) — rank (G(A))| <1. 
But rank is always a nonnegative integer, so this means that 
rank (G(A +E)) = rank (G(A)) for all E € Ns, (0). 
By Theorem 4.10.6 on page 605, rank (G(A + E)) is the total multiplicity of all 
eigenvalues of A+E that are inside I), and rank (G(A)) = alg mult, (A) =a. 
Consequently, for all E € N5,(0) there are exactly a eigenvalues of A +E 
(counting multiplicities) inside of [. This is true for each 4; € a (A), 
so if 
) =e 
Oe, 
ek As 
then for each r > 0 such that the circles Ty, = {z||z—j| =r} around all of 
the ,'s neither touch nor surround other eigenvalues of A, 
||/E|| <6 implies 
that there are exactly a; eigenvalues of A +E (counting multiplicities) inside 
of [), foreach 7. 
@ 

916 
Chapter 8 
Matrix Analysis via Resolvent Calculus 
Example (Eigenvalue Continuity for Generic 9 x 9 Matrix Revisited) 
Figure 8.2.1 below illustrates Theorem 8.2.5 for a generic 9 x 9 complex matrix 
A that has 3 distinct eigenvalues' o (A) = {A1,A2,A3} with respective alge- 
braic multiplicities a), =3, a), = 2, and a), = 4. The same picture appears 
in Figure 6.1.1 on page 759—it is duplicated here to help corroborate and clarify 
the results from the different approaches to eigenvalue continuity. 
FIGURE 8.2.1. EIGENVALUE CONTINUITY 
Around each 4; there is a circle [,, of common radius r > 0 that surrounds 
no other eigenvalue of A. The previous Theorem 8.2.5 says that there isa 6 > 0 
such that for all E € N5(0), there are 3, 2, and 4 respective eigenvalues of 
A+E 
inside of T),, 
Iy),, and T),. In other words, as E — 0, all of the 
eigenvalues (and multiplicities) of A +E are trapped in these circles. And this 
remains true as the common radius r shrinks. So, as E > 0, also let r > 0 
so as to squeeze the eigenvalues of A+ E (along with their multiplicities) ever 
closer to those of A. In the limit the nine eigenvalues of A +E go to the 
eigenvalues of A with the appropriate multiplicities. 
e 
Note. An alternate interpretation of eigenvalue continuity is given in the 
discussion on page 760. 
Simple Eigenvalues 
It is a direct corollary of Theorem 8.2.5 that if Ao is a simple eigenvalue of A, 
and if [9 is a circle around Xo that excludes all other eigenvalues of A, then 
Complex eigenvalues need not be in conjugate pairs for complex matrices. Corollary 8.2.7 on 
page 917 presents an interesting fact concerning continuity of real eigenvalues of real matrices. 

8.2 Eigenvalue Continuity 
917 
there exists 
a 6 > 0 such that A+ E has exactly one simple eigenvalue inside 
of To for all E € N5(0). This corollary is formalized below. 
Real and Simple Eigenvalues 
When E is sufficiently small in norm, the previous corollary ensures that all of 
the circles described in Theorem 8.2.5 that surround a simple eigenvalue of A 
will contain exactly one eigenvalue of A +E. In the special case when A and 
E are both real, the eigenvalues of A +E appear in conjugate pairs, and this 
observation produces another important fact. 
8.2.7. Corollary. The real and simple eigenvalues of A € R"*" remain 
real and simple under small real perturbations. 
Proof. 
If, under the conditions of Theorem 8.2.5, A, € @ (A) is real in addition 
to being simple, and if E is real and has sufficiently small norm, then the 
eigenvalue p, of A+ E inside of the associated circle I, must also be real 
because otherwise 7, would be another eigenvalue of A +E that is also inside 
of I, which is impossible—any circle centered on the real line that surrounds 
a complex number z must also surround Zz. 
& 
i 
Zz insipe |, = 
2. insipn I', 
Polynomial Root Continuity 
The footnote on page 757 mentions that the shortcut approach to eigenvalue con- 
tinuity depends on the fact that the roots of polynomial equations are continuous 
functions of the polynomial coefficients, but proving this fact is the sticky part 
of such a development, so it is often omitted. However, this fact is now a simple 
corollary of Theorem 8.2.5. 

ee 
eS Raa 
ay Pee 
utes rans 
re 
" 
eee '~a 
bcd 'Proof. Ft 
eee Tee a polynomial equation 
' 
P(t) = 2" + Ona"? +--+ +aiz+a9 = PSS 
ns oO 
. 
companion matrix 
0" 
0 
"a. 
0 
—ao 
+ 
|
1 
0 
++ 
0 
-a1 
:
Cm 
he ase 
(defined on page 295). 
Genet 
Noone 
ee 
ee 
Example (Laurent Expansion of the Resolvent) 
An important result from complex analysis says that if a complex function f(z) 
is analytic in the annular domain D between two concentric circles Co and 
C, centered at z with respective radii r; > ro > 0, then at each z € D 
the function f(z) has a mca Laurent ales a about zo given by 
Fle) = 
Jo 
an( ~ 0)" Der 
an 
n=0 
in which 
(8.2.9) 
f(€) dé 
"1 fp f@e 
An Sie ieee twe0) 
Sand" 
62 = 
[Sn 
(n > 1), 
— Z)rt+ 
ori 
r (€ = Zo) Toe 
where I is any positively oriented contour in D. 
Problem: For A ¢€ C"*"™ with rank r < _m, develop the Laurent expansion 
of the resolvent R(z) = (zI— A)~! about z=0. 
Solution: A is singular, so 0 = Ag € a (A) with index(\9) 
=k > 0. Use a 
core-nilpotent decomposition 
Jie Qlers Say 
(8.2.10) 
as defined in (4.7.24) on page 556, where C is nonsingular and N is nilpo- 
tent of index k. The similarity transformation Q is not important in the de 
velopment, so suppress it by writing A ~ eG 3) in place of (8.2.10). Re- 
call that the Drazin pseudoinverse' AP of A defined in (4.7.25) on page 557 
Recall that A? = A* is the group inverse of A when inder(Ag) < 1—see Corollary 4.7.18 
on page 560. 

8.2 Eigenvalue Continuity 
919 
Ld 
—1 
. 
. 
. 
*. 
is AP wn (6; oN and the spectral projector associated with A g = 0 is 
G=I-AAPwx~ to a The Laurent expansion of 
pe 
I-C)"} 
0 
Rie) tel 2A) 
a A ee) 
(8.2.11) 
about zero is obtained by separately determining the Laurent expansions of 
(zI—C)~! and (zI—N)-! about zero. It is easily verified by direct multiplication 
with (zI—N) that 
(21 -N) S712 °N- sa PNET 
for 2 40, 
(8.2.12) 
so, by uniqueness, (8.2.12) must be the Laurent expansion for (zI—N)~!. To 
determine the Laurent expansion of (zI—C)~! about zero, consider an annular 
domain as depicted below in Figure 8.2.2 that is defined by the area between two 
positively oriented concentric circles Co and C; around Ag = 0 such that C; 
surrounds Co, but Cj; neither surrounds nor touches any nonzero eigenvalue 
A; for A. 
FIGURE 8.2.2. ANNULAR DOMAIN 
The matrix coefficients A, and B, corresponding to the scalar coefficients 
Gy and b, in (8.2.9) are respectively obtained by applying the Cauchy integral 
formulas in (4.10.21) on page 604 and the Cauchy—Goursat theorem in (4.10.20) 
on page 604. Since (£1 — C)~! is analytic in and on Cp, the Cauchy integral 
formula produces 
ae 
aol 
T= 
=i 
271 
Jp 
E 
271 
JC, 
E 
Similarly, for n = 1,2,3..., the Cauchy integral formulas in (4.10.21) for higher 
order derivatives along with repeated differentiation of R(z) (see (8.2.1) on page 
911 or Exercise 8.2.1 on page 920) produce 
1 / 
(Gb aC)n 
a6 aget = of a (€L = C)~* 
dé 
: 
I 
ay 
En+1 
Oni 
enti 
= anal 
PGs \a 
forn 
= 1,2,3...- 
ea 
A, = 
n! 

920 
Chapter 8 
Matrix Analysis via Resolvent Calculus 
Furthermore, £"~!(€I — C)~! is analytic in and on Co for n 2 1, so the 
Cauchy—Goursat theorem in (4.10.20) on page 604 ensures that 
Ber, 
cue it 
een cen a, 
een 
Ba= 55 | 
¢ 1(€l—C) =a fe (€1—C)"'dé=0 
for n> 
Consequently, the Laurent expansion of (€I — C)-! about zero is 
co 
Bla) b= (-C ya (O 2 (Coie int ian 
(a Ce ieeat, 
n=0 
Now incorporate these results along with (8.2.12) into (8.2.11) and use 
C7! 
0 
D 
Cau avfobo\. 
(0°06 
AP ~( 0 
ar AG=A(I-AA \~ (9 he "bye ak 
and A*G = 0 to obtain the Laurent expansion of the resolvent about z = 0 as 
n+1 
R(z) = (2 — A)~ =u 
A™-!G@2-7 — S [A?| 2" 
(8.2.13) 
n=0 
Of course, once this formula has been derived, it is straightforward to verify it 
without using any complex analysis just by multiplying the right-hand side of 
(8.2.13) by (zI — A)—this is Exercise 8.2.2. 
Exercises for section 8.2 
8.2.1. For a fixed A € C"*", consider the resolvent R(z) = (zI— A)! asa 
function of z € C, and show that 
dR(z) | 
wr So == {R(z)*} . 
8.2.2. Verify that the formula on the right-hand side of (8.2.13) for the Laurent 
expansion of R(z) is correct by direct multiplication with (zI — A). 
8.2.3. Prove that the set 
DC C"*" of matrices having n distinct eigenvalues 
is an open set. 
8.2.4. 
Let M = {A1, A2, sey Ax } Ss oO (Anxn) = {A1, d2, Sizals As} > where the 
multiplicities of the eigenvalues in M sum to m > 0. Prove that if T 
is a closed contour (see the footnote on page 604) that surrounds M, 
then there exists a neighborhood NV5(0) such that for all E € N;(0), 
the total multiplicity of all eigenvalues of A +E lying inside of T. is 
also equal to m. 

8.2 Eigenvalue Continuity 
921 
8.2.5. Let VY and )Y be the respective sets of eigenvalues for 
8.2.6. 
0 
1 
oO 1 
ON e2 
2 
and 
A+E= 
ae 
, 
0 
20 
0 
20 
0 
6 
0 
where E = deg;e7. Consider the metric in (6.1.4) on page 760 with 
the oo-norm—i.e., doo(¥V,Y) = ming ||% —YVx||,, where Y, denotes 
a permutation of the eigenvalues in Y. Determine how small 5 must 
be in order to guarantee that d..(¥,) < 107!. Compare this to the 
discussion in the example on page 761. 
Define the total resolvent to be the resolvent R(z,Z) in which both z 
and Z are allowed to vary, and call R(z,A) the partial resoluent—i.e., 
the second argument is held constant at A. 
(a) Show that the Jacobian matrix at (€,A) for the total resolvent 
is the n? x (n? +1) matrix 
Ori 
Ori 
Ori 
mA) SEGA) 
SEA) 
Ori2 
Ori2 
Ori2 
J= 
Oz (€, A) 
Oz11 (6) A) 
Oza (6) A) 
OW ; 
Ornn 
Ornn 
rete) Se, A) 
Sek 
en a 
Hint: C"*" is isomorphic with Cc". If you can't complete the 
exercise with this hint, then look ahead on page 925. 
Explain how the derivative of the partial resolvent is obtained 
from the Jacobian matrix that represents the derivative of the 
total resolvent. 

922 
Chapter 8 
Matrix Analysis via Resolvent Calculus 
8.3 EIGENVALUE DIFFERENTIATION 
Now turn to the issues surrounding the differentiability of eigenvalues as a func- 
tion of the entries. To appreciate the problem, consider the 2 x 2 case in which 
the eigenvalues \; and A» are each functions of the four independent vari- 
ables 2; in Z = ee a. The characteristic equation is A? — TA + 6 = 0, 
where T = 241 + Z99 = trace(Z) and 6 = 211222 — 212221 = det (Z), so the two 
eigenvalues of Z are 
7+ Vr? —46 
tT —V7T? —46 
VA 
5 
and 
9(Z) = 
; 
(8.3.1) 
The partial derivatives 0\1/0z;; and OA2/0z;; with respect to each entry 2; 
exist and are nicely behaved for values of Z such that rT? 4 46. However, all 
of these partial derivatives fail to exist when tT? — 46 = 0, so the eigenvalues 
of Z cannot be differentiable when 7? — 46 = 0. Notice that this happens 
precisely at those matrices Z for which the eigenvalues have coalesced to become 
a single multiple eigenvalue. In other words, a general analysis of eigenvalue 
differentiability must necessarily be limited to considering eigenvalues that remain 
simple throughout some neighborhood. 
In subsequent developments matrix A € C"*" 
is understood to be fixed 
(its entries do not vary), Ao is a simple eigenvalue of A, and 
To = {€ €C||E 
— Ao| =r} 
(8.3.2) 
is a circle of radius r around Apo such that all other eigenvalues of A are exterior 
to ['9. Corollary 8.2.6 on page 917 guarantees that for every such circle there 
exists a real number 6 > 0 such that each Z € N5(A) has exactly one simple 
eigenvalue \(Z) inside of 9. Consider Z to be a variable matrix (a matrix of 
n" variables) that varies throughout N5(A), or equivalently, 
Z = A+E where 
E is a variable matrix with ||E|| <6 (any norm with ||I|| = 1 will suffice). The 
simple eigenvalue 
MZ): Ng(A) C Cx = € 
is a complex valued function of Z, where (A) = Ao. The ultimate goal is to 
prove that A(Z) is differentiable at A and to derive an explicit expression for 
the derivative \'(A)(C) at all 
C € C"*". 
Differentiability of Spectral Projectors 
Establishing the differentiability of \(Z) at A by direct means can be challeng- 
ing. However, resolvents easily do the job once it is realized that the associated 
spectral projector is analytic at A. 

8.3 Eigenvalue Differentiation 
923 
8.3. L Thora Gales of ee Poojectors) 1 Z ae a ae . 
eigenvalue A(Z) for each Z = Ns(A), then the function CG that maps 
Z to the ae oe ao ae 
ee! is oe at = and - 
633) 
| 
Proof. 
Let 
Z= A+E with ||E|| < 6. The strategy is to use integral repre- 
sentations for G(A + E) and G(A) given in Theorem 4.10.6 on page 605 to 
approximate 
G(Z) — G(A) = G(A + E) — G(A) 
with a linear operator L. As observed in Corollary 8.2.3 on page 913, R(€, A) 
is continuous at each € € Io, and IT is compact, so there is a constant M € R 
such that ||R(€,A)|| <M for all € € To. If 6 is constrained so that 6 <1/M, 
then 
JER(E,A)|| < BI |R(E,A)|| < JE] < 6M <1 
forall €€To. 
(8.3.4) 
The second Neumann series expansion (4.11.16) on page 620 yields 
R(gé,A +E) =R(f,A) + R(g, A)ER(E, A) 
+R(é, A){ER(é, A)}? ) {ER(E, A)}* 
=R(E,A)+R(E, AJER(E, A) 
+R(é, A){BR(é, A)}? [I- ER(é, A) 
Consequently, the spectral projector G(A +E) for \(A +E) is 
G(A+E)= a |, RGA + Bae 
1 
a 
RGA ae R(é, AJER(é, A)dé 
271 
Jro 
+55 [ REAERGAPT-ERE Ad 
271i 
Jr, 
= G(A)+ L(E) + F(B), 
where L and F are the operators 
L(2) = 5 | RUG AYZR(G, Ade 
1 5 | RIE AMZR(E, A)} T- ZG, A) 
ak. 
OL 
Hints 
F(Z) = 

924 
Chapter 8 
Matrix Analysis via Resolvent Calculus 
Consequently, 
|G(A +E) — G(A) = L(B)|| _ FCB) 
|E|| 
|E|| 
Operator L is linear, so if it can be established that ||F(E)||/||E|| + 0 as 
\|E|| 
+ 0, then G is differentiable at A by (8.1.1) on page 901, and the 
derivative of G at A is G'(A)(Z) = L(Z). To prove this, use the fact that 
Sr, || = 2ar together with 
| 
[I -ER(é, A)}7* | 
= 
(by (8.3.4) and (4.11.12) on page 619) 
1 
poe 
ald 
oy eZ 
to conclude that 
FEI <5. / Re ARE. 
AP BRE, A) *as| 
M3 \|E|\? 
~ On(1 — JEM) 
Jr 
ae] = SO 
0 
1 — ||B|| M 
Hence ||F(E)|| / ||E|| + 0 as ||E|| > 0, and thus G is differentiable at A with 
G'(A)(Z) = L(Z). Since R(€, A) exists at each point in some neighborhood of 
A, the same argument with A replaced by A+e shows that G is differentiable 
at each point in some neighborhood of A, so G(Z) is analytic at A. 
I 
The analyticity of spectral projectors has important consequences. The first 
is that the right-hand and left-hand eigenvectors for \(Z) respectively defined 
by the columns and rows of G(Z) are differentiable at A (Exercise 8.3.5). The 
most important implication concerns the differentiability of the eigenvalue \(Z). 
Differentiability of Simple Eigenvalues 
Under the assumptions leading up to Theorem 8.3.1, namely that (Z) is a 
simple eigenvalue of Z for each Z € N5(A), the differentiability of \(Z) is 
now easily established. 
8.3.2. Theorem. 
(Differentiability of Simple Eigenvalues) Let \(Z) be 
the continuous function on N5(A) C C"*" that maps Z € Ns(A) to 
the eigenvalue \(Z). If \(Z) is simple for each Ze N5(A), then A(Z) 
is differentiable at A. 
e 
In particular, all partial derivatives O0/0z;; exist at A. 
(8.3.5) 

8.3 Eigenvalue Differentiation 
925 
Proof. 
Since G(Z) is the spectral projector for a simple eigenvalue, it follows 
that rank(G(Z)) = 1 for all Z € Ng(A) (by (4.7.18) on page 550). Further- 
more, G(Z) is idempotent, so 
trace(G(Z)) = rank(G(Z)) =1 
forall 
ZeN(A) 
(8.3.6) 
(by (4.7.11) on page 547 or Exercise 4.2.20 on page 452). These facts together 
with the identity ZG(Z) = \(Z)G(Z) produce 
\(Z) = trace(ZG(Z)) 
forall 
ZeNg(A). 
(8.3.7) 
Since G(Z) is differentiable at A, and since trace is differentiable everywhere 
because it is linear (see Exercise 8.1.5 on page 910), equation (8.3.7) shows that 
A(Z) is a composition of differentiable functions, and thus \(Z) is differentiable 
at A by (8.1.4) on page 903. The existence of the derivative \/(A) guarantees 
that all partial derivatives 0\/0z;; at A (by (8.1.8) on page 904). 
Hf 
Although it has now been established that \'(A) exists, there is not yet a 
good way to evaluate X'(A)(C) for an arbitrary C € C"*". Using (8.3.7) to 
evaluate \'(A)(C) is less than straightforward because, in spite of its elegance, 
the integral representation of G'(A) given in (8.3.3) on page 923 is not partic- 
ularly helpful for this purpose. The coordinate matrix representation of G'(A) 
is required—i.e., the Jacobian matrix for G(Z) at A is needed. 
Coordinate Representation of G'(A) 
The coordinate spaces C", C"*", and cr' along with their standard bases 
will appear throughout subsequent discussions, so the following notations are 
adopted to avoid confusion. 
e 
S, = {e;|t=1...,n} is the standard basis for C". 
oe, =, = eeF [6,7 ='1...,7) is the standard basis for C°"" that 
is orderded as 
Snxn - {Eu, HK) Ein, Ea, CoaCas) Fon, say 
Eni; ves ,Enn}. 
(8.3.8) 
e 6 
S,2 = {ej |7,7 = 1,...,n} 
is the standard basis for Cc", where e;; is 
the n? x 1 column obtained by unraveling E,; row-by-row. For example, if 
0 
Re 
athe Noe Cs 4 unravels to become e2; = ('). 
The basis is 
0 
ordered as 
Sz = {e1ty+++58in Coie 
Congres erhiteatas Cre 
(8.3.9) 

926 
Chapter 8 
Matrix Analysis via Resolvent Calculus 
To derive the coordinate representation (the Jacobian) of G'(A) as de- 
scribed in Theorem 8.1.1 on page 905, recall that C"*" and c" are isomorphic 
so that nothing is lost by considering n x n matrices to be n? x 1 column vec- 
tors. Consequently, instead of considering the coordinates of a linear operator on 
C"*" with respect to Sp%n, one may consider the operator as a function on c" 
and examine its coordinates with respect to S,2. In particular, rather than con- 
sidering G asa function G:C"*" > C"*" and writing G(Z \= hk GhkEnk 
in terms of its coordinate functions gp,, consider 
G : cv > cv and write 
G(Z) = don.p Gnkenk 
$o that the coordinates of G with respect to S,2 are 
listed in a single column 
[G(Z)] 
5. = (9115 
912) +++) Gin | G21, 922+++s92n| °° PO 
Gna 
<5 Oa) 
The differentiability of G at A ensures the existence of all partial derivatives 
of the coordinate functions gng = Gnk(Z115-++5Zijs+++32nn) at A (by (8.1.8) on 
page 904), and the existence of the partial derivatives at A means that 
O9nk 
Oz; 
CRISK ate (4 is tees Ceasers: asa 
Theorem 8.1.1 on page 905 guarantees that the Jacobian matrix for G at 
A isthe n? x n? matrix 
0911 
0911 
0911 
0211 ( ) 
Oz12 ( 
OZnn ( ) 
Ogi2 
Ogi2 
0912 
2912 0 
a 
ANS 
wee 
A 
=(GA)]s =| 
den 
Bag" 
Bide tit 
(8.3.10) 
8gnn 
L 
"a 
ELLYN 
acca 
f 
qwene 
toss 
g 
0z11 
0212 
Ozan 
n2xn2 
The columns in J are ordered just as the basis vectors e;; in S,,2 are ordered in 
(8.3.9). In this ordering the entries in the (i, 7)-column of J can be conveniently 
arranged row-wise back into an n xX n matrix 
Ogi1 
0912 
O9in 
A 
rend 
OZ; ( 
OZ; ) 
O24; ee, 
Og21 
0g22 
092 
(A) 
A)e 
te 
ee oeCA. 
OG 
Jig =| 
ij 
zi; ei 
O24 Sr 
be 
(A). 
(8.3.11) 
| 
) 
: 
O24; 
nxi 
Agni 
dgn2 
dgnn 
ee (A) 
aoe (A) 
+: 
Bu; (A) 
a 
In other words, J can also be conveniently described as the n? x n x n three- 
dimensional array J illustrated below in Figure 8.3.1. 

8.3 Eigenvalue Differentiation 
927 
FIGURE 8.3.1. THREE DIMENSIONAL JACOBIAN MATRIX 7 
There are n? planes in 7 in which the (i, 7)-plane is the matrix Ji; in (8.3.11) 
that represents the (i,7)-column of J. This observation simplifies the "book 
keeping" required to compute the Jacobian matrix of G at A. 
To this end, make use of the fact that 
4 = A(Z) being a simple eigenvalue 
for each Z € N5(A) implies that index(Z—XI) = 1. This ensures that (Z—AI) 
is a group matrix—.e., it belongs to a multiplicative group (Theorem 4.7.17 on 
page 559). Consequently (Z— AI) has a group inverse (Z—X1)* in this group, 
and this group inverse is a special case of the Drazin inverse defined on page 557. 
In particular, 
N ((Z—Al)*) = N(Z—Al) = R(G), 
and 
(8:3.12) 
R((Z—Al)*) = R(Z— Al) = N(G). 
Also, if C € C"*" isa group matrix and Bny,p is any matrix such that CX = B 
and YC = B have solutions for X and Y, respectively, and if P is the 
projector onto N(C) along R(C), then 
X=C*B+PH and Y=BC* +KP forsome H,KeC"*". = (8.3.13) 
(See Exercise 4.7.40 on page 568.) These observations are used to produce the 
following theorem concerning the partial derivatives of G. 

Proof. 
Entry-wise differentiation of (Z—AI)G = 0 by the standard product 
rule (Exercise 8.3.2 on page 937) produces 
0= (Z a AI) [OG /dz;5] + ((OZ/0z;;] = [Or/Oz;|1) G 
= 
(Z = AI) [OG /0z;;| = [0r/Ozi3|G = E,yjG. 
Since (8.3.12) ensures that 
(Z—Al*G =0 = G(Z— Al), 
(8.3.15) 
the first part of (8.3.13) can be applied to produce 
[OG /dz3] = [0A/Ozj](Z — AN*G — (Z — AN*E,,;G + GH 
=—(Z—)*E,jG+GH 
for some 
He C"*", 
(8.3.16) 
Similarly, entrywise differentiation of G(Z— AI) = 0 yields 
[OG /dz43] (Z— AL) = [0/023] G + GE,;, 

8.3 Eigenvalue Differentiation 
929 
so the second part of (8.3.13) together with (8.3.12) guarantees that 
[OG /dzij] = [0/0z,;)G(Z — A1)* — GE,;(Z — \1I)*# + KG 
= -GE;;(Z—AI)#+KG 
for some 
K€ C"*". 
(8.3.17) 
Furthermore, differentiating both sides of 
G = G? = GG yields 
Substituting (8.3.16) and (8.3.17) into this equation and making use of (8.3.15) 
produces 
By adding (8.3.16) and (8.3.17) it is seen that 
2[0G/dz;] = —(Z — A1)*E,;G — GE,;(Z — \)* + GH+ KG 
= —(Z — M)*E,;G — GE,;(Z — Al)* + [8G /0z;,), 
and thus 
[OG /0z;5| = —(Z == ANFE,;G = GE, ;(Z = rI)*. 
a 
Coordinate Representation of '(A) 
Now that there is an explicit expression for OG/0z,;, it is a relatively simple 
matter to apply it to determine the partial derivatives O0\/0z;;, which in turn 
generate the Jacobian matrix for A at A. 
8.3.4. Theorem. (Partial Derivatives of Simple Eigenvalues) If \ = \(Z) 
is a simple eigenvalue for Z € N5(A), and if G = G(Z) = [9;;(Z)|nxn 
is the spectral projector for A, then 
Or 
oe 
Gat 
8.3.18 
On. 
Is 
( 
) 
Ly 
In particular, if x» = | : | and yo = (W%,---,Yn) are respective right- 
Din 
hand and left-hand eigenvectors for A9 = A(A) such that ypxo = 1, 
then 
Dy 
Oz; (A) = 9):(A) = 25. 

930 
Chapter 8 
Matrix Analysis via Resolvent Calculus 
Proof. 
As observed in (8.3.6) on page 925, the fact that X is simple and G is 
idempotent for all Z €N5(A) implies that trace (G) = rank (G) = 1 so that 
\ = A trace (G) = trace(AG) = trace(ZG). Differentiating this with respect 
to 2; yields 
Oakes O[trace (ZG)] _ ree ae] ee iz 
0G f. OZ <| 
a) 
O24; 
02: 
Oz 
O23 
Oz; 
Z 
= trace iz 
| + trace | 
2) c| 
; 
Oey 
Oz; 
However, (8.3.15) on page 928 says that (Z — AI)#G = 0 = G(Z— Al)*, so 
Theorem 8.3.3 together with ZG = AG = GZ ensure that 
trace Z Ea = —trace [(Z— AI)*E;;GZ] — trace [ZGE;;(Z — AI)*] 
ij 
=—Atrace [(Z—A)*E,;G] —Atrace [GE,;(Z— Al)*] 
=—Ntrace [G(Z— AI)*E,;| —Atrace [Ejj;(Z—AI)*G] 
= (9) 
Consequently, 
Z 
was = trace 
a 
G| = trace |E,;G] = trace [eie; G| = e; Ge; = ¢g, 
Wl 
OZig 
O25 
By considering A= A(Z) = A(211,..+52ij;---»2nn) a8 a function from cr' 
into C and by using the standard basis S,2 for cr together with the standard 
basis S; = {1} for C, it is now straightforward to determine the Jacobian 
matrix for A at A. 
8.3.5. Theorem. (Jacobian Matrix for \ at A) Let \ = \(Z) be asimple 
eigenvalue for Z€ N;(A), and let G(Z) be the spectral projector. If 
. 
. 
. 
2 
. 
A = A(211,+++2ij,)+++,2nn) 
is considered as a function from C" 
into 
C, then the Jacobian matrix can be represented as 
=| Fr A] =las(Adlnan = (GCA = GF. 
Moreover, the value of X'/(A) at any 
Ce C"*" is 
\'(A)(C) = trace (GoC) = ygCxo, 
(8.3.19) 
where xo and yo are respective right-hand and left-hand eigenvectors 
associated with Ag = A(A) such that y§xo = 1. 

8.3 Eigenvalue Differentiation 
931 
Proof. 
When X is considered as a function from C"' into C, the coordinate 
matrix representation of /(A) with respect to the standard bases S,,2 and 
S; = {1} is obtained from Theorem 8.1.1 on page 905 and Theorem 8.3.4 on 
page 929 as 
J =a, = (i(Aen)Is, [blends 
+++ |[X(A)(Cnn)]s;) 1xn? 
OX 
Or 
Or 
OX 
Ee 
ar 
eee CA 
eee 
ADe 
es 
(= 
et ) 
OZi3 
Oona = ile 
- (gu(A) Gel 
Ree 
gi 
Aue 
Gnn(A)) 
= (om May 
os 
Tye 
Se 
Ta) 
: 
To evaluate \'(A) at a matrix C,x,, string out the rows of C into one long 
2 
row vector 
C € C" 
so that its coordinates with respect to the standard basis 
Sn2 in (8.3.9) are 
T 
CePA Cite Cioy ces, Cin, C21, C29 014 Conn sey Chl, Coes Cn) 
ae 
Apply Theorem 4.5.3 on page 510 and use G(A) = Go = xoy$ to write 
nm 
m 
= WAls vs, Ilsa =)>5 > gn Aes 
a oe S95(A)eis = Sy [GoC],, = trace (GoC) 
j=l i=l 
j=l 
= trace (xoygC) = yoCxo. 
Hf 
Theorem 8.3.5 has important consequences. In particular it provides a rela- 
tively clear understanding of how a simple eigenvalue reacts when the underlying 
matrix is slighty perturbed. 
Perturbations of Simple Eigenvalues 
Since \/(A)(E) = yjExo for right-hand and left-hand eigenvectors x9 and yo 
for \9 = A(A) with y>xo =1, it follows from the definition of differentiability 
on page 901 that 
|f(E)| 
E) — \(A) = 
yoExo 
+ 
f(E 
here 
lim ~~~->0. 
(8.3.20) 

932 
Chapter 8 
Matrix Analysis via Resolvent Calculus 
This is sometimes expressed in "little-oh" order notation! by writing 
MA +E) — (A) = y$Exo + 0((|Ell). 
Applying the triangle and CBS inequalities to (8.3.20) yields 
|\(A +E) — X(A)| < ly$Exo| + |f(E)| < llyolla IIXoll2 Ello + [f()|. (8-3-21) 
The term 
= ||xoll5 |lyol|) is the norm of the spectral projector Go = G(A). 
To be precise, % = ||G(A)llp = IIG(A) 
llr = llxollz lyolla, 80 (8.3.21) can be 
expressed by writing 
MA +E)—A(A)| <4 ||Bl|, 
for small ||E|),. 
(8.3.22) 
In other words, the absolute change in (A) under a small perturbation to A 
is never much more than the magnitude of the perturbation 
||E]||, magnified by 
« = ||G(A)||, = |lyoll, |/Xo||,. Consequently, « represents a condition number 
for \(A)—the smaller « is, the less sensitive A(A) is to perturbations to A. 
In the real case 
« has a revealing geometric interpretation. Recall from 
(4.7.35) on page 561 that 
« = ||G(A)||, = 1/sin¢d, where ¢ is the angle 
between R(G(A)) = span(x9) and N(G(A)) = yg. Alternately, 
K = |l¥oll2 llXolle = llyolle IlXollz /¥o X0 = 1/ cos8, 
where @ is the angle between xg and yop. Therefore, 
& grows as the angle 
between N(A—ApoI) and R(A —AolI) decreases, or, equivalently, as x9 and yo 
become closer to being orthogonal. The minimum value & = 1 is attained when 
N(A—Aol) L R(A—Aodl), or, equivalently, when x9 and yo are directed along 
the same line—i.e., x9 is a scalar multiple of yo. In other words, if A — Aol is 
RPN (or EP) as described in Definition 4.3.10 on page 467, then Xo is perfectly 
conditioned. 
A simple eigenvalue can never have orthogonal left-hand and right-hand 
eigenvectors (see Exercise 4.9.10 on page 594), but the inner product can be ar- 
bitrarily close to zero (see Exercise 4.9.13 on page 594). While the angle between 
two complex eigenvectors is technically undefined, the geometric interpretation 
of « nevertheless remains valid—i.e., « measures the degree of deviation from 
co-linearity (see Exercise 8.3.7 on page 937). 
If A is normal, then Xo is perfectly conditioned because G(A) is then an 
orthogonal projector (recall Theorem 3.4.6 on page 342), so & = ||G(A)||, =1. 
The above observations are summarized below in the following theorem. 
Given two vector space functions f, 
g:U— V, saying that f is o(g) ("f is little-oh of g") 
as x — ¢ means that limx—e || f(x)||/||g(«)|| = 0. There is also a "big oh" concept. Saying 
that f is O(g) ('f is big-oh of g") as 
x +c means that there exists K, 6 €R such that 
Il f(x)|| < K||g(x)|| whenever ||x 
— || < 6. Stating that f is O(g) as 
||x|| + co means 
that || 
f(x)|| < K||g(x)|| whenever 
||x|| > 6. A useful abuse of notation is often employed by 
writing 
f=g+o(h) or f 
=g+O(h) to signify that f—g is o(h) or O(h), respectively. 

933 
Derivatives of Simple Eigenvectors (Eigenvectors for Simple Eigenvalues) 
Given an eigenvector for a simple eigenvalue of A € C"*", an important prob- 
lem is to determine the extent to which the eigenvector is affected when A 
undergoes a slight perturbation. A natural way to analyze this problem is to 
cast it in terms of the derivative of an eigenvector that varies as a function of 
the entries of the underlying matrix. 
To develop the theory let xo and yg be respective right-hand and left- 
hand eigenvectors with yjxo = 1 for a simple eigenvalue A) € a (A) so that 
Go = xoy6 is the spectral projector for Ap. Let N5(A) be a d-neighborhood 
around A such that A(Z) is asimple eigenvalue as Z varies throughout N5(A), 
and let G(Z) be the spectral projector for A(Z) so that 
A(A)=Ao 
and 
G(A)=Go. 
For Z€N;(A), the function x:.N5(A) 
CC" > C"™? defined by 
is an eigenvector for Z corresponding to the simple eigenvalue (Z). Continuity 
of G ensures the continuity of x because as Z— A, 
x(Z) > G(A)xo = Goxo = Xo = x(A). 

934 
Chapter 8 
Matrix Analysis via Resolvent Calculus 
Moreover, the differentiability of G at A guarantees the differentiability of x 
at A. In particular, 
x/(A)=G!(A)xp 
and 
x'(A)(C) = [G'(A)(C)] xo for 
Ce C"*™. 
Consequently, the sensitivity of x9 = x(A) is governed by the sensitivity of 
the spectral projector at A, and this sensitivity is reflected in the results of 
Theorem 8.3.3 on page 928. These realizations lead to the following statements 
concerning perturbations of eigenvectors of simple eigenvalues. 
Perturbation of Simple Eigenvectors 
As before, let xg and yo be respective right-hand and left-hand eigenvectors 
for a simple eigenvalue 9 of A such that y$x9 =1 so that Go = xoyf is 
the associated spectral projector. Let Ms5(A) be a d-neighborhood around A 
such that \(Z) is a simple eigenvalue for Z as Z varies throughout NV5(A), 
let G(Z) be the spectral projector for \(Z), and let x(Z) = G(Z)xo so that 
MA) =X, 
G(A)=Gpo, 
and 
x(A)=Xp. 
The next theorem describes the degree to which the eigenvector xg is sensitive 
to small perturbations in A. 
8.3.7. Theorem. (Perturbation of Simple Eigenvectors) If, under the above 
conditions, A is perturbed by a small amount E, then the eigenvector 
xo = x(A) of A becomes the eigenvector x(A +E) for 
A+E and 
the relative change in x9 = x(A) is 
Ix(A + E) — x(A)|| 
CAPS NA = Aol) I 
HEI + oC HEI) 
for any norm on C". This can alternately be expressed as 
I[x(A + E) — x(A)|| 
$< |/(A — Aol) || ||| 
for small 
||El) . 
8.3. 
ce 
(A = 
rot)* | EI 
El. 
(8.3.23) 
In other words, the relative change in x9 is never much more than the 
magnitude of the perturbation 
||E|| magnified by y = (A — ol)* || 
: 
Thus 7 is a condition number for x9 —the smaller y is, the less sensi- 
tive Xo is to perturbations to A. 

8.3 Eigenvalue Differentiation 
935 
Proof. 
Apply [G(A + E)—G(A)] to xo and use 
G(A + E) — G(A) = G(A)(E) + o(|E])), 
to get 
x(A +E) —x(A) = G(A)(E)xo + o(||E|)), 
and then interpret this in the context of standard coordinates. That is, consider 
ix(A + E)]s, — ix(A)ls, = [G/(A)(B)xols, + o(||El)). 
(8.3.24) 
To compute [G'(A)(E)xo]s,, unravel E = [e;j]nxn row-by-row so that its co- 
ordinates with respect to S,2 are 
= 
T. 
[E]s,. =— (611, €12," <8) elms ©2127 
es) €Ony 5 Enlr€nd«--5 
Ena) 
Use the Jacobian matrix J for G at A that is given in (8.3.10) on page 926 
to write 
[G'(A)(B)]s,. = [G"(A)]s,2[Els,2 =J[Els,. 
= €115 (11) + 125 «(12) + +++ + €1nd «(1n) 
+ €2154(21) + €225 (22) + +++ + €2nJx(2n) 
= En1d «(n1) aF En25 «(n2) Se 
Ennd x(n) 
= Sy Eig 3 (43) 
4,9 
where J,(;;) isthe (7, 7)-column of J. Now create an n x n matrix by arranging 
the entries J,(;;) into rows in the matrix Jj; that is given by (8.3.11) on page 
926. In other words, the coordinate matrix of G'(A)(E) with respect to the 
standard basis Snxn for C"*" is 
[G(A)(E)] si 5n = ay 
Eng Jag. 
If S, is the standard basis for C", then 
Ln 
and 
[G'(A)(E)xols, = [G'(A)(E) 
sax lXols, = )_ ig SizX0- 
a,j 

936 
Chapter 8 
Matrix Analysis via Resolvent Calculus 
Use (A—AoI)#xo = 0 (recall (8.3.12) on page 927) in the formula for J;; given 
in Theorem 8.3.3 on page 928 to produce 
Jijxo = —2;((A —- Aol)*]«i- 
Consequently, the standard coordinates for G'(A)(E)xo are 
[G'(A)(E)xols, = — >. €25[(A — Aol)* 
]si 
=— os (sia 
ae sane Lj 
=—)) [(A-Aol)*E], , 23 
= —(A — AoI)* Exo, 
so (8.3.24) becomes 
x(A + E)]s, — [x(A)]s, = —(A — Aol)*E [x(A)]s, + o((|Ell). 
Thus 
|[x(A + E) — x(A)| 
I|x(A)|| 
for any norm on C" (the [x|s, notation is now suppressed). This can alternately 
be expressed as 
||x(A + E) — x(A)|| 
I|x(A)|| 
which means that the relative change in Xp = x(A) under a small perturbation 
to A is never much more than the magnitude of the perturbation ||E]| magnified 
by y = ||(A- Nol)*||, so y represents a condition number for x(A)—the 
smaller + is, the less sensitive xg is to perturbations to A. 
I 
S ||(A — Aol)? || 
IIEll + o(l|Ell) 
<< (A _ Aol)* | ||E|| 
for small ||E]| , 
Exercises for section 8.3 
= il 
8.3.1. It is clear that the eigenvalues of Z = (a iy ) 
are Ay = |x —-1| 
0 
Vax 
and A, = Vz, and there exist respective neighborhoods Nj and No 
around « = 1 and x =0 
such that A, remains simple as x varies 
throughout Nj; while Az is simple for all x € No. However, d\,/dx 
fails to exist at c=1 and dAg/dx does not exist at x = 0. Explain 
why this is not a contradiction to the statements in Theorem 8.3.2 or its 
corollary on page 924 that ensures the differentiability of simple eigen- 
values. 

8.3 Eigenvalue Differentiation 
937 
8.3.2. 
8.3.3. 
8.3.4. 
8.3.5. 
8.3.6. 
8.3.7. 
If each entry in X = [xp] € C™*? and Y = [yx] € C?*" is a function 
of n? variables 2ij, then prove the standard product rule 
Under the hypothesis of Theorem 8.3.3, prove that trace | =4()) 
i) 
Prove that rank & 
Oz, (a)| =, 
Under the hypothesis of Theorem 8.3.1, let 
Geos Aye ==. Co 
be the function that maps Z € Ns(A) to the j*" column of the spec- 
tral projector G(Z). Explain why G,, is differentiable at A. In other 
words, the right-hand eigenvectors for \(Z) 
that are defined by the 
columns of G(Z) 
are differentiable at A. The same is true for the 
left-hand eigenvectors that are defined by the rows of G(Z). 
The spectral projector for a simple eigenvalue Ao is unique and is given 
by Go = xy*, where x and y* are respective right-hand and left- 
hand eigenvectors for Ag that have been normalized so that y*x = 1 
(by Theorem 4.7.8 on page 550). 
(a) Construct an example to illustrate that these facts do not ensure 
that x and y* are unique. 
(b) Suppose that (x1,y1) and (x2,y2) are two such pairs of nor- 
malized eigenvectors. Describe the relationship that must exist 
between the x;'s and between the y;'s. 
Let « = ||G(A)]||, = ||xIl. |ly||, be the condition number for a simple 
eigenvalue as described in Theorem 8.3.6 on page 933. 
(a) Explain why « > 1, and then prove that « = 1 if and only if 
x =ay for some scalar a. 
(b) Construct an example to show that « can be arbitrarily large. 

You know that I write slowly. This is chiefly because I am never satisfied 
until I have said as much as possible in a few words, and writing 
briefly takes far more time than writing at length. 
— Carl Friedrich Gauss (1777-1855) 

APPENDIX 
An Overview of 
Determinants 
9.1 
INTRODUCTION 
Reference was made on page 115 to the ancient Chinese counting board on 
which colored bamboo rods were manipulated according to prescribed "rules of 
thumb" in order to solve a system of linear equations. The Chinese counting 
board is believed to date back to at least 200 B.C., and it was used more or less 
in the same way for a millennium. The counting board and the "rules of thumb" 
eventually found their way to 
Japan where Takakazu Seki Kowa (1642-1708), a 
great Japanese mathematician, synthesized the ancient Chinese ideas of array 
manipulation. Kowa formulated the concept of what we now call the determinant 
to facilitate solving linear systems—his definition is thought to have been made 
some time before 1683. 
About the same time—somewhere between 1678 and 1693—Gottfried W. 
Leibniz (1646-1716), 
a German mathematician, was independently developing 
his own concept of the determinant together with applications of array manipu- 
lation to solve systems of linear equations. It appears that Leibniz's early work 
dealt with only three equations in three unknowns, whereas Seki Kowa gave a 
general treatment for n equations in n unknowns. It seems that Kowa and Leib- 
niz both developed what later became known as Cramer's rule (see page 957), 
but not in the same form or notation. These men had something else in common. 
Their ideas concerning the solution of linear systems were never adopted by the 
mathematical community of their time, and their discoveries quickly faded into 
oblivion. 
Eventually the determinant was rediscovered, and much was written on 
the subject between 1750 and 1900. During this era, determinants became the 
major tool used to analyze and solve systems of linear equations, while the theory 
of matrices remained relatively undeveloped. But mathematics, like a river, is 

940 
Appendix 
An Overview of Determinants 
ever changing in its course, and major branches can dry up to become minor 
tributaries while small trickling brooks can develop into raging torrents. This is 
precisely what occurred with determinants and matrices. The study and use of 
determinants eventually gave way to Cayley's matrix algebra, and today matrices 
and linear algebra are in the main stream of applied mathematics, and the role 
of determinants has been relegated to a minor backwater position. Nevertheless, 
it is still important to understand what a determinant is and to know a few of 
its fundamental properties. 
It is assumed that most readers of this book have prior knowledge of the 
elementary aspects of determinants. Consequently, determinants are relegated 
to this appendix, where the aim is not to study them for their own sake but 
rather to provide an overview and highlight properties of determinants that are 
useful in the context of linear algebra and its applications. Accordingly, many 
secondary properties are omitted or confined to the exercises, and the details in 
proofs will be kept to a minimum. 
9.2 FUNDAMENTALS 
Over the years there have evolved various "slick" ways to define the determinant, 
but each of these slick approaches seems to require at least one sticky theorem 
in order to make the theory sound. The approach here is to opt for expedience 
over elegance and stay with the classical treatment. 
A permutation p = (pi1,p2,---;Pn) of the numbers (1,2,...,n) 
is simply 
any rearrangement. For example, the set 
{(1, 2,3) 
(1, 3, 2) 
(2, 1,3) 
(2, 3, 1) 
(3, 1, 2) 
(3, 2,1)} 
contains the six distinct permutations of (1,2,3). 
In general, the sequence 
(1,2,...,n) has n! = n(n —1)(n —2)---1 different permutations. Given a per- 
mutation, consider the problem of restoring it to natural order by a sequence of 
pairwise interchanges. For example, (1,4,3,2) can be restored to natural order 
with a single interchange of 2 and 4 or, as indicated in the following diagram, 
three adjacent interchanges can be used. 
hy 473 32)) 
(C143. 2) 
Cle 
4 dene) 
Gls 22 oanet ) 
(1, 
2, 4,3) 
Cle 23535.4) 
The important thing here is that regardless of how it is done, the number of 
interchanges required to restore the permutation to natural order is an odd 

A.2 Fundamentals 
941 
number. Try to restore (1,4,3,2) to natural order by using an even number 
of interchanges, and you will discover that it is impossible. This is due to the 
following general rule that is stated without proof. The parity of a permutation 
ts unique—i.e., if a permutation p can be restored to natural order by an even 
(odd) number of interchanges, then every other sequence of interchanges that 
restores p to natural order must also be even (odd). Accordingly, the sign of a 
permutation p is defined to be the number 
+1 
if pcan be restored to natural order by an 
even number of interchanges, 
—1 
if pcan be restored to natural order by an 
odd number of interchanges. 
For example, if p = (1,4,3,2), then o(p) = —1, and if p = (4,3,2,1), then 
o(p) = +1. The sign of the natural order p = (1,2,3,4) is naturally o(p) = +1. 
The general definition of the determinant can now be given. 
9.2.1. Definition. For an n Xn matrix A = lanl, the determinant of 
A is defined to be the scalar! 
det (A) = » O(P)A1p, 42, 21 Gnpe> 
(9.2.1) 
p 
where the sum is taken over the n! permutations p = (p1,p2,.--,Dn) 
of (1,2,...,n). Observe that each term @1p,d2p,°*:@np,, 
in (9.2.1) is 
one of the "diagonals" in A (see the footnote on page 8) that contains 
exactly one entry from each row and each column of A. Determinants 
of nonsquare matrices are not defined. 
Example 
For a general 2 x 2 matrix A = = oe), the 2! =2 permutations of (1,2) 
are (1,2) and (2,1), so det(A) contains the two terms 
o(1,2)aj1a22 
and 
o(2,1)aj2@21. 
Since o(1,2) =+1 and o(2,1) = —1, we obtain the familiar formula 
wd 
= 011422 — 412491. 
(9.2.2) 
a21 
22 
: The determinant of A is usually denoted by det (A), but sometimes the alternate notation 
|A| may be used. However, |A| can also denote the matrix [ass] of absolute values. Context 
will generally dictate which concept is being used. 

942 
Appendix 
An Overview of Determinants 
Ma 
Pees 
Similarly, to use the definition to evaluate det(A), where A = (4 : °), 
use the 3! = 6 permutations of (1,2,3) to form the terms in the expansion of 
det (A) that are shown in the following table. 
3x5x7=105 
Therefore, 
det (A) = 5 a(p)aip; d2p.43p, = 45 — 48 — 72 + 84 + 96 — 105 = 0. 
Pp 
Triangular Determinants 
Perhaps you have seen rules for computing 3 x 3 determinants that involve run- 
ning up, down, and around various diagonal lines. Such schemes are mnemonic 
devices derived from (9.2.1) to avoid having to deal with the mathematical defi- 
nition of a determinant. While mnemonic diagonal rules facilitate working with 
3 x 3 determinants, they do not easily generalize to matrices of order greater 
than three. So, in case you have forgotten (or never knew) how to run around 
diagonals to evaluate a determinant, don't worry about it—just remember the 
2 x 2 rule given in (9.2.2) as well as the following theorem concerning triangular 
matrices in conjunction with Theorem 9.2.11 on page 948. 
9.2.2. Theorem. The determinant of a triangular matrix is the product 
of its diagonal entries. In other words, 
by tie 
big 
OG 
tog: 
ne 
ton 
' 
: 
3 
: 
See ty1to2 is tan; 
) 
0 
bids 
Cnt 
and similarly for a lower triangular matrix. 

A.2 Fundamentals 
943 
Proof. 
Recall from (9.2.1) that each term tip, tap, +++tnp, contains exactly one 
entry from each row and each column. This means that there is only one term 
in the expansion of the determinant that does not contain an entry below the 
diagonal, and this term is ti,to2:-+tnn. 
Transposition Doesn't Alter Determinants 
It is important to know how various matrix operations affect the values of as- 
sociated determinants. Fortunately, transposition is one of the operations that 
does not change the value of a determinant. 
9.2.3. Theorem. det (AT) =det(A) for all nxn matrices. 
Proof. 
As p= (p1,p2,.--,Pn) varies over all permutations of (1,2,...,n), the 
set of all products {o(p)a1p,@2p, °**@np,, } is the same as the set of all products 
{0(P)@p,1Gp.2°**Gp,n}- Explicitly construct both of these sets for n = 3 to 
convince yourself. 
Hf 
Effects of Elementary Row and Column Operations 
Theorem 9.2.3 insures that it is not necessary to distinguish between rows and 
columns when discussing properties of determinants, so theorems concerning 
determinants that involve row manipulations will remain true when the word 
"row" is replaced by "column." For example, knowing how elementary row and 
column operations alter the determinant of a matrix is crucial, but, by virtue of 
Theorem 9.2.3, it suffices to limit the discussion to elementary row operations. 
9.2.4. Theorem. 
Let B be the matrix obtained from A, , by one of 
the three elementary row operations: 
Type I: 
Interchange rows 7 and 7. 
Type Il: 
Multiply row i by a 40. 
Type Ill: 
Add a@ times row 7 to row j. 
det (B) = —det (A) for a Type I operation. 
(9.2.3) 
det (B) = adet (A) for a Type II operation. 
(9.2.4) 
det (B) = det (A) for a Type III operation. 
(9.2.5) 
Proof of (9.2.3). 
If B agrees with A except that Bj, = Aj. and Bj. = Ace 
then for each permutation p = (p1,p2,.--,Pn) of (1,2,...,n), 
Dip; wierte Din, eee Din; cee Orion — Ain, aie Ajn; oad Qin, aeons Anpn 
== Q1p, eee Qin; eee Qin; ehielta Anpn 

944 
Appendix 
An Overview of Determinants 
Furthermore, o(p1,.--)Piy+++)Pjy+++)Pn) = =O (pt, . SVD; sh <6 Piss 
Pn) be- 
cause the two permutations differ only by one interchange. Consequently, defini- 
tion (9.2.1) of the determinant guarantees that det (B) = —det (A). 
Proof of (9.2.4). 
If B agrees with A except that B,;. = aAj;,, then for each 
permutation p= (P1, P2, wells Pn), 
bip, °** Dip, ++ Onp, = Bip *** WAip; *** np, = A(A1p, *** Qip; °° 'Onpn )s 
and therefore the expansion (9.2.1) yields det (B) = adet (A). 
Proof of (9.2.5). 
If B agrees with A except that Bj. = Aj« + aAix, then 
for each permutation p = (p1,p2,.--;Pn); 
bip, cay Dip; vi bjp; "++ Onp, = G1p, *** Qip;*** (jp; a QAp; ) "* 
Anpn 
= A1p, oe 
* Ain; faite jp; a0 'Onpn a a(1p, aus 
* Bip; ale 
* Qip; o* 
' Grips 
so that 
det (B) = 3 o(p)aip, deci: ip; Srey 
Ajp; ee Anpn 
Pp 
(9.2.6) 
+a 3 O(P)Q1p, Pac Qip; Bhs Qip; Shes Anpn« 
Pp 
The first sum on the right-hand side of (9.2.6) is det (A), while the second sum is 
the expansion of the determinant of a matrix A in which the i*" and j*" rows 
are identical. For such a matrix, det(A) = 0 because (9.2.3) says that the sign 
of the determinant is reversed whenever the i*? and j*" rows are interchanged, 
so det(A) = —det(A). Consequently, the second sum on the right-hand side of 
(9.2.6) is zero, and thus det (B) =det(A). 
& 
Determinants of Elementary Matrices 
A direct consequence of Theorem 9.2.4 is that the determinant of each of the 
three types of elementary matrices is immediately available. 
9.2.5. Theorem. 
If E, F, and G are respective elementary matrices 
of Types I, II, and HI corresponding to the elementary operations in 
Theorem 9.2.4, then 
det (E)=—1, 
det(F)=a, 
and 
det(G)=1, 

A.2 Fundamentals 
945 
Proof. 
Recall from the definition of an elementary matrix on page 136 that 
each of these elementary matrices is obtained by performing the associated row 
(or column) operation to an identity matrix of appropriate size. Theorem 9.2.2 
guarantees that det (I) =1 regardless of the size of I, so if E is obtained 
by interchanging any two rows (or columns) in I, then (9.2.3) implies that 
det (EZ) = —det (I) = —1. Similarly, if F is obtained by multiplying any row (or 
column) in I by a #0, then (9.2.4) yields det (F) = adet (I) =a, and if G 
is the result of adding a multiple of one row (or column) in I to another row 
(or column) in I, then (9.2.5) guarantees that det(G)=det(I)=1. 
& 
Invertibility and Determinants 
A useful characterization of the nonsingularity of a square matrix A can be 
formulated in terms of determinants. As shown below, the issue of nonsingularity 
(or singularity) simply boils down to whether or not det (A) = 0. 
9. 
2.6. Ahectem. 
square uae A is  ahealer if aa only if 
det (A) a 
0. ee A is ee if and oy if det (A) 
= 0. 
Proof. 
If Pnxn isan elementary matrix of Type I, II, or III, andif A,y, is any 
other matrix, then the product PA is the matrix obtained by performing the 
elementary operation associated with P to the rows of A. Applying Theorem 
9.2.5 yields the conclusion that 
det (EA) 
= 
—det(A) 
= 
det (E)det (A), 
det (FA) 
a det (A) 
det (F)det (A), 
det(GA) 
= 
det(A) 
= 
det (G)det (A). 
In other words, det (PA) = det (P)det (A) whenever P is an elementary ma- 
trix. This observation easily extends to any number of elementary matrices 
P;,...,P; by writing 
det (P;P2---P,A) 
= det (P;)det (P2---P,A) 
= det (P,)det (P2)det (P3---P,A) 
(O20) 
+ = det (P;)det (P2)--- det (P;,)det (A). 
Consequently, if P;,P2,...,P% 
is a sequence of elementary matrices such that 
P,P.-:-P,A = Ea, then 
det (P;)det (P2)---det(P,)det(A) 
= 
det (Ea), 
and hence det (A) # 0 if and only if det(E,a) #4 0 (because elementary ma- 
trices have nonzero determinants). Theorem 2.3.1, page 159, says that A is 
nonsingular if and only if 
Ea =I, and since det (I) = 1, it follows that 
A nonsingular <> det (Ea) = 1 <= > det(A) #0. 
Bf 
Corollary 2.3.3 on page 160 guarantees that a square matrix A is singular 
if and only if one row (or column) is a combination of other rows (or columns), 
so the following corollary is immediate. 

946 
Appendix 
An Overview of Determinants 
fone ay 
(A) 
if 
and 
only 
if 
one rc 
= 
Ee 
a ear com 
ation of other Tows a in ai In particular, a 
det (A) = 0 when one row er column) is oely. Zero. 
Small Determinants <> Near Singularity 
It may be tempting to extrapolate from Theorem 9.2.6 that det (A) is somehow 
a measure of how close A is to being singular, but don't do it—it is not nec- 
essarily the case! Nearly singular matrices need not have determinants of small 
magnitude. For example, A = ( 
re + 
becomes increasingly close to the sin- 
gular matrix fe at as n grows, but det (A) = 1 for all n. Vice versa, small 
determinants need not indicate near singularity. For example, 
Ye 
tal 
6 
iY 
oh 
ar 
An = 
Pe 
aks 
Pe. Qe. 
is not close to any singular matrix (see Corollary 3.5.8, on page 366), but 
det (A,) = (.1)" is extremely small for large n. 
Minor Determinants 
Example 
Determinants of submatrices in a given matrix A arise often, and the various 
kinds of such determinants are defined below. 
9.2.8. Definition. A k x k minor determinant (or simply a minor) 
of Amxn is the determinant of any 
kx k submatrix of A. 
A kxk 
principal minor is a minor lying on the intersection of the same set 
of k rows as columns. The k x k leading principal minor is the 
principal minor lying on the intersection of the first k rows and columns. 
a Oe 
An example of a 2 x 2 minor in A = (: 5 °) 
is é 4 
= —6, but this is 
if 
ts 
not a principal minor. An individual entry of A can be regarded as a 1 x 1 
minor, and det (A) itself is considered to be a 3 x 3 minor of A. An example 
of a 2 x 2 principal minor is E 
7 le 
= —12, but this is not a leading principal 
minor. The 2 x 2 leading principal minor is i a 
= 

A.2 Fundamentals 
947 
Determinants and Rank 
Theorem 2.4.13 on page 187 says that the rank of A € C™*" is the size of 
the largest nonsingular submatrix in A, but Theorem 9.2.6 guarantees that 
the nonsingular submatrices of A are simply those submatrices with nonzero 
determinants, so the following theorem is immediate. 
_ 9.2.9. Theorem. rank (A) = size of 
the largest nonzero minor in A. 
Example 
2 
Ie 
os 
all 
To determine the rank of A = (4 
be 6 1) 
by using determinants, notice that 
(ete 
at 
there are clearly 1 x 1 and 2 x 2 minors that are nonzero, so rank(A) > 2. In 
order to decide if the rank is at least three, the 3 x 3 minors must be examined. 
There are exactly four 3 x 3 minors, and they are 
egies' 
oe i 
heey ih 
eo 
AG 
0, 24086 
Tl 08 oh BG 
ee ONS 6 al es 0 
Fans 29 
Rate al 
eck <l 
aol 
Since all 3x3 minors are 0, it must be the case that rank(A) = 2. This 
example illustrates why using determinants is generally not an efficient way to 
compute the rank of a matrix. 
Product Rule for Determinants 
It was shown in (9.2.7) that the determinant of a product of elementary matrices 
is the product of their respective determinants. The next theorem extends that 
observation to products of all square matrices. 
9.2.10. Theorem. det (AB) = det (A)det (B) for all n x n matrices. 
Proof. 
If A is singular, then AB 
is also singular because Theorem 2.4.12 
on page 187 says that rank(AB) < rank(A). Consequently, Theorem 9.2.6 
implies that 
det (AB) = 0 = det (A)det (B), 
so the theorem is trivially true when A is singular. If A is nonsingular, then 
A can be written as a product of elementary matrices A = P;P2-:-P, that 
are of Type I, IJ, or III by Theorem 2.3.7 on page 165. Therefore, (9.2.7) can be 
applied to produce 
det (AB) = det (Pi P2 tee P,B) = det (P, )det (P2) --+- det (P;,)det (B) 
= det (P| P2:--P,) det (B) = det (A)det(B). 
@ 

948 
Appendix 
An Overview of Determinants 
Computing Determinants 
Example 
The product rule in Theorem 9.2.10 provides a practical way to compute deter- 
minants. Recall from the discussion involving the LU factorization in §2.10 on 
page 259 that for every nonsingular matrix A, there is a permutation matrix P 
(a product of elementary interchange matrices) such that PA = LU in which L 
is lower triangular with 1's on its diagonal, and U is upper triangular with the 
pivots on its diagonal. The product rule yields det (P)det (A) = det (L)det (U). 
Since every elementary interchange matrix E has det(E) = —1 (Theorem 
9.2.5), and since P is a product of elementary interchange matrices, it follows 
that 
det (P 
+1 
if P is the product of an even number of interchanges, 
lh) 
—1 
if P is the product of an odd number of interchanges. 
The determinant of a triangular matrix is the product of the diagonal entries 
(Theorem 9.2.2), so det (L) =1 and det (U) = uj1u22---Unn, where the uj; 's 
are the pivots. Combining these observations yields det (A) = +uj1U22°+:Unn; 
where the sign depends on the number of row interchanges used. This is a use- 
ful technique for computing a determinant—the details are summarized in the 
following theorem. 
9.2.11. Theorem. If PA,,.,;, = LU is an LU factorization obtained with 
row interchanges (use partial pivoting for numerical stability), then 
det (A) = 011 t22++' Unn- 
The u,;'s are the pivots, and ¢@ is the sign of the permutation. That is, 
_ f +1. 
if an even number of row interchanges are used, 
—1 
if an odd number of row interchanges are used. 
If a zero pivot emerges that cannot be removed (because all entries below 
the pivot are zero), then A is singular and det (A) = 0. See Exercise 
9.3.19 for the use of orthogonal reduction to compute det (A). 
Use partial pivoting and the LU factorization to evaluate the determinant of 

A.2 Fundamentals 
949 
The LU factors were computed in the example on page 269 to be 
: 
ovenog a 
Ai 4G Ses 
Cel om 
bes .stee te Sune 
ce 
hye = 
ao 
i 
on roe 
L= 
(Wilmot 
ay 
ved pnt ard bi ii aed ag dees Die en ihe 
L/S 
(ac 
eT 
C0 ria 
The only modification that is required is to keep track of how many row in- 
terchanges are used. Examining the order of the calculations on page 269 (or 
looking at P) shows that the pivoting process required three interchanges, so 
o =—l, and thus det (A) = (—1)(4)(5)(—6)(1) = 120. 
Block Triangular Determinants 
The next theorem is a block-matrix analog of Theorem 9.2.2 for determinants of 
triangular matrices. 
9. 
2, 
12. 
'Theorem. ae A and Ce are square, 'then 
el BY — aa(A Jace (©). 
Proof. 
Let PyA = L,U; and PoC = L2U,2 be respective LU factorizations. 
Using P;' =P? (see (2.2.9) on page 142) together with the fact that transpo- 
sition does not affect determinants along with the product rule yields 
det (A) =— det (P7L,U;) = det (P,)det (L, )det (U,) = det (P,U,), 
det (C) = det (P]L2U2) = det (P2)det (L2)det (Uz) = det (P2U2). 
Write 
eB 
ee ee 
L1U, 
) 
ORC) 
aa 
0 
PZ bs 
= (Ger) (outa) Coeau, 
and note that 
aet(" pr) =aet( 7G) aS 2, ) 
= det (Pi)det (P2) 
(9.2.8) 
because 
ye ee) is also a permutation matrix and the determinant of a per- 
mutation matrix is +1 depending on its parity. The determinant of a triangular 
matrix is the product of its diagonal entries, so 
—1 
det (7G 
0 - ) 
1) 
aya 
det (\ sigs = det (U;)det (U2), 
It therefore follows from (9.2.8) and the product rule that 
det($ = = det (Pi)det (P2)det (U1 )det (U2) 
= det(PU;)det(P 
U2) = det (A)det(C). 

950 
Appendix 
An Overview of Determinants 
Volume and Determinants 
The definition of a determinant is purely algebraic, but there is a concrete geo- 
metrical interpretation. A solid in R™ with parallel opposing faces whose adja- 
cent sides are defined by vectors from a linearly independent set Axyy kp pe 
08 y Ky} 
is called an n-dimensional parallelepiped. As depicted in Figure 9.2.1, a two- 
dimensional parallelepiped is a parallelogram, and a three-dimensional paral- 
lelepiped is a skewed rectangular box. 
x1 
FIGURE 9.2.1: PARALLELEPIPEDS 
9.2.13. Theorem. If A € R"*" with rank(A) =n, then the volume 
of the n-dimensional parallelepiped generated by the columns of A is 
Vn = [det (aATA)]'". If A is square, then V,, = |det (A). 
Proof. 
As discussed on page 668, if Anxn = QnxnRnxn 
is the QR factoriza- 
tion of A, then the volume of the n-dimensional parallelepiped generated by 
the columns of A is V, = \V/2-+-Up, = det(R), where the 1%'s are the diag- 
onal elements of the upper-triangular matrix R. When A,,.x, is rectangular, 
use Q?7Q =I 
together with the product rule and the fact that transposition 
doesn't affect determinants to write 
det (A7 
A) = det (R7Q7QR) = det (R7R) = det (R7)det (R) 
x 
(9.2.9) 
= (det (Re (vibes Vs 
In particular, if A is square, then det (A? 
A) = det (A7)det (A) = (det (A))?, 
so V, =|det(A)|. 
& 
Hadamard's Inequality 
It is geometrically evident from Figure 9.2.1 that the volume of the parallelepiped 
P generated by the columns of A = [x,|x2|---|xX,]| cannot exceed the volume 
of a rectangular box whose perpendicular sides have length ||x;||,. This is the 
essence of Hadamard's inequality, which is formally stated below. 

Proof. 
It was established in (5.3.4) that if 
Piet 
AS alxilcidas) 
andh, Ajealeuinale 
oa) 
2 
nxj' 
and if P, is the orthogonal oe onto R(Ax_;) for k >1, then 
¥44=|\x1||, 
and 
v, = ||(I—Px)xz||, 
(the projected height of x;). 
However, 
vp = |(1— Px)xell2 < || —Pr)[I3 lixell3 = [lull 
(recall 4.3.7 on page 464), 
so, by (9.2.9), det (A7A) < IIx1|I5 |lxoll$---(|xnll5 or, equivalently, 
1/2 
det (A)] < TJ xello =[[ (do leis!" | 
. 
with equality holding if and only if the x;,'s are mutually orthogonal. 
& 
Derivative of a Determinant 
It's sometimes necessary to compute the derivative of a determinant whose en- 
tries are differentiable functions. The following formula shows how this is done. 
i Jacques Hadamard (1865-1963), a leading French mathematician of the first half of the twenti- 
eth century, discovered this inequality in 1893. Influenced in part by the tragic death of his sons 
in World War I, Hadamard became a peace activist whose politics drifted far left to the extent 
that the United States was reluctant to allow him to enter the country to attend the Interna- 
tional Congress of Mathematicians held in Cambridge, Massachusetts, in 1950. Due to support 
from influential mathematicians, Hadamard was made honorary president of the congress, and 
the resulting visibility together with pressure from important U.S. scientists forced officials to 
allow him to attend. 

Proof. 
This follows directly from the definition of a determinant by writing 
d(det (A 
d 
d(ay Q2p.°** Gee) 
SE Se oem tan, nm = 
Lew ee oern 
P 
Ss 
! 
/ 
aca 
= 
o(p) (ab. @2p: eure anpn _ Q1p; 2 2p, fcreika Anpn + cee + Q1p, 42p. 
ae) 
Pp 
~, 
- D2 (P)44p, 2292 7 Onp, + ye O(P)Q1p, 4p, ***Onp, 
P 
Pp 
decode DE O(P)A1p, A2p. ei Bnpa 
P 
= det (D;) + det (D2) +---- + det (D,). 
Example 
To evaluate the derivative d(det(A))/dt for A = ( 
se 
; begat apply formula 
(9.2.10) to obtain 
d (det (A)) a 
dt 
eu 
as" 
cost 
sint 
in 
ef 
e* 
—sint 
cost = (e' +e *) (cost +sint). 
Check this by first expanding det (A) and then computing the derivative. 
Exercises for section 9.2 
9.2.1. Use the definition to evaluate det (A) for each of the following matrices. 
Se 
2a 
Diels 
aL 
(a 
pene (-s 4 0). 
(b) A=( Oin2 1). 
2 
a6 
il 
OR OMe 
@11 
412 
413 
(ae 
A= € i a), 
(dee 
MAres (aa a22 
os 
0 
Q31 
432 
433 

A.2 Fundamentals 
9.2.8. 
9.2.2. 
9.2.3. 
9.2.4. 
9.2.5. 
9.2.6. 
9.2.1. 
953 
What is the volume of the parallelepiped generated by the three vectors 
X1 = (3,0,—4,0)7, x2 =(0,2,0,—2)7, and xg = (0,1,0,1)7? 
Using Gaussian elimination to reduce A to an upper-triangular matrix, 
evaluate det (A) for each of the following matrices. 
i 
Ae 
ea} 
il 
3 
5 
(a) A= 
(252 1). 
(bipAeiWei 
a4 2 
|. 
ih 
ah 
al 
3 
—2 
4 
i 
2 
-3 
4 
0 
0 
—2 
3 
. 
fre 
197 8 
is 
ee 
Oat 
nee} 
ish A= | 5 
3 
8 
i 
(dy A 
a te 
1 
|: 
—3 
-1 
1 
-4 
0 
2 
-3 
0 
2 a pee ON 
Oe 
0 
a 
i 
ei 
Dil 
0 
0 
he 
2 
wal 
it 
(e) A= 
Pe 
Ok PO Aaa et 
ma 
0) 
Ome 
2 
—1 
: 
: 
0 
0 
0 
—1 
i 
111 
1 
3 
—2 
Use determinants to compute the rank of A = 
a = ; 
2, 
5 
—6 
Use determinants to find the values of @ for which the following system 
possesses a unique solution. 
0 
=3 
1 
rf 
If A is nonsingular, explain why det (A~') = 1/det (A). 
Let {ai, S1Do oo rae) and {bi, bo, ore 
vectors. Prove that 
.,b,} be two sets of n x 1 column 
det es ajby ) 
— det (A)det (B), 
j=l 
where Anyn = (a1a2:::a,] and Bryn = [bibe::- bn]. 
Explain why determinants are invariant under similarity transforma- 
tions. That is, show det (P~'AP) = det (A) for all nonsingular P. 

954 
Appendix 
_ 
An Overview of Determinants 
9.2.9. Explain why det (A*) = det (A). 
9.2.10. 
(a) Explain why |det(Q)| = 1 when Q is unitary. In particular, 
9.2.11. 
9.2.12. 
9.2.13. 
9.2.14. 
9.2.15. 
9.2.16. 
det (Q) = +1 if Q is an orthogonal matrix. 
(b) How are the singular values of A € C"*" related to det (A)? 
Prove that if A is mxn, then det(A*A) > 0, and explain why 
det (A*A) > 0 if and only if rank (A) =n. 
If A is nxn, explain why det (aA) = a"det (A) for all scalars a. 
If A is an nxn skew-symmetric matrix, prove that A is singular 
whenever n is odd. Hint: Use Exercise 9.2.12. 
How can you build random integer matrices with det (A) = 1? 
If the k*" row of An 
xn is written asasum A,, = x! +y? +.---4+27, 
where x?,y7,...,z7 are row vectors, explain why 
Aix 
Aix 
Aix 
det (A) =det} 
x? | +det 
yT | 
4---+det] 
2? 
As 
ins 
oe 
Considering transposes yields a similar statement for columns. 
Determinant of a General Sum. Use Exercise 9.2.15 to first show 
that if A and B are 2 x 2, then 
det (A + B) = det (A) + det (B) + det[Ax1 | Bx2] + det[Ba1 | Axa]. 
Now generalize this to show that for n x n matrices, det(A +B) is 
the sum of the 2" determinants 
det (A + B) = 
yb tdetiCer [Cugaha as Cy 
Cy 5 E{Ax5 Baz} 
j=1,n 
Considering transposes yields a similar statement for rows. 

A.2 Fundamentals 
9.2.17. 
9.2.18. 
9.2.19. 
9.2.20. 
9.2.21. 
955 
The CBS inequality says that |x*y| < lIxll3 lly ll3 for_X,y 6C°~*, Use 
Exercise 9.2.11 to give an alternate proof of the CBS inequality along 
with an alternate explanation of why equality holds if and only if y is 
a scalar multiple of x. 
2a 
3 
4 
Let ace ( 
0 
4-a2 
-5 ) 
1 
= 
8a 
(a) First evaluate det (A), and then compute d(det (A)) /dz. 
(b) Use formula (9.2.10) to evaluate d(det (A)) /dz. 
Define d A/dx = |da;;/dx] (the matrix of derivatives) when the entries 
of A = [a,;(x)| are differentiable functions of z. Is it always the case 
that d(det (A))/dx = det (dA/dz) when A is square? 
For a set of functions S = {f1(x), fo(x),..., fr(x)} that are n—1 times 
differentiable, the determinant 
fi(z) 
foe) 
oe 
Tinta) 
f(x) 
Lay 
a 
Tate) 
Ga 
ie 
FOV 
ey f= 
(al 
fn 
a) 
is called the Wronskian of S. If S is a linearly dependent set, explain 
why w(z) =0 
for every value of x. Hint: See (2.4.3) on page 182. 
Consider evaluating an n x n determinant from the definition (9.2.1). 
(a) How many multiplications are required? 
(b) Assuming a computer will do 1,000,000 multiplications per sec- 
ond, and neglecting all other operations, what is the largest 
order determinant that can be evaluated in one hour? 
(c) Under the same conditions of part (b), how long will it take to 
evaluate the determinant of a 100 x 100 matrix? 
Hint: 100! ~ 9.33 x 10'°". 
(d) If all other operations are neglected, how many multiplications 
per second must a computer perform if the task of evaluating 
the determinant of a 100 x 100 matrix is to be completed in 
100 years? 

956 
Appendix 
An Overview of Determinants 
9.3 ADDITIONAL PROPERTIES OF DETERMINANTS 
Block matrices are a common occurrence, but evaluating the determinant of a 
two-by-two block matrix is different than evaluating a 2 x 2 scalar determinant. 
: 
9.3.1. Theorem. If A and D are square matrices, then 
7K 
BY. 
(det (A)det (D -CA'B) when A~' exists, 
— 651) 
det 
ea 
: 
: 
sok} 
. (¢ 
, D) 
det (D)det (A - BD~!C) when D~' exists. 
The matrices 
D— CA~!B and A—BD7!C are the Schur comple- 
ments—see page 166. 
Proof. 
If A~+ exists, then ie aa = boar a es CAME and 
Theorems 9.2.10 and 9.2.12 produce the first formula in (9.3.1). The second 
formula follows by using a similar argument. 
& 
Rank-One Updates 
It is easy to provide examples to show that the function f(A) = det(A) 
is 
not linear. In particular, det (A + B) 4 det (A) + det (B) except in rare special 
cases. Nevertheless, there are some statements that can be made regarding the 
determinant of certain types of sums. The results in Exercises 9.2.15 and 9.2.16 
are general statements concerning determinants of sums, but the following result 
concerning rank-one updates is particularly useful. 
9.3.2. Theorem. 
If A,,,, is nonsingular, and if c and d are nx 1 
columns, then 
e 
det (I+cd7) =14d"7c, 
(9.3.2) 
e 
det (A +cd7) = det (A) (1+d7A-'c). 
(9.3.3) 
Exercise 9.3.7 generalizes these expressions, and Exercise 9.3.8 gives the 
formula for det (A +ced?) when A is singular. 
Proof. 
The proof of (9.3.2) follows by applying the product rule along with 
Theorem 9.2.12 (page 949) to the product 
I 
0O 
I+cd? 
c 
I 
Ona 
fed 
Cc 
dsl 
0 
1} 
\=d? 1) \0 
1+d%c/}* 
To prove (9.3.3), write 
A+cd? = A (I + Ast cd.) , and apply the product 
rule along with (9.3.2). 

A.3 Additional Properties of Determinants 
957 
Example 
Lick AG 
1 
to 
1 
1 
l+A2q 
::: 
1 
To evaluate the determinant of A = 
; 
; 
: 
: 
J 
, Nee 0; 
1 
Loe 
ener 
(3 3 
express A as a rank-one updated matrix A = D+ ee', where D is the 
diagonal matrix D = diag (Ai, A2,...,An) and e7 =(1 
1 
++. 
1). Apply 
(9.3.3) to produce 
nm 
n 
1 
det (D + ee") = det (D) 
(1 
+e7D~'e) 
= 
ri 
— 
(D + ee") = 
det 
(D) ( 
= 
(Ts) (45 
Cramer's Rule 
+ 
This classical result is a corollary of the rank-one update formula (9.3.3). 
9. 3. 3. Theoren. Came st Rule) The solution for 
the 3 
ie unknown in 
a nonsingular ee Anxnk = 
= bis 
Coe 
det (Ay) 
oo 
Ly = det 
(A) ' 
where A; = eb A A 
oe is, A; is 
identical to A except that column A,; has been replaced by b. 
Proof. 
Since A; = A+(b— A,;)e7, where e; isthe i" unit vector, formula 
(9.3.3) may be applied to yield 
det (A;) = det (A)(1 +eF 
A (b— A.i) ) 
= det (A) (1 tel (x ei) } 
= det (A) (1+ 2; — 1) = det (A) a. 
Thus «; = det (A,)/det(A) because A being nonsingular insures det (A) 4 0 
by Theorem 9.2.6. 
Gabriel Cramer (1704-1752) was a mathematician from Geneva, Switzerland. As mentioned in 
the introduction on page 939, Cramer's rule was apparently known to others long before Cramer 
rediscovered and published it in 1750. Nevertheless, Cramer's recognition is not undeserved 
because his work was responsible for a revived interest in determinants and systems of linear 
equations. After Cramer's publication, Cramer's rule met with instant success, and it quickly 
found its way into the textbooks and classrooms of Europe. It is reported that there was a time 
when students passed or failed the exams in the schools of public service in France according 
to their understanding of Cramer's rule. 

958 
Appendix 
An Overview of Determinants 
Example 
To determine the value of t for which «3(t) is minimized in 
#00 
14¢ 
x1(t) 
1 
(0 t #) (2a) = (415 ), 
1 
#3 / \aa(t) 
1/t? 
notice that only one component of the solution is required, so it's wasted effort to 
solve the entire system just to get an expression for x3(t). Cramer's rule yields 
ed 
| aa 
0 t 1/t 
eG 
ti 
+ 
#7 
x3(t Cr 
ee 
eet toe L 
fo 
Aft 
= 
6 
1 
ose 
so set dax3(t)/dt =0 to conclude that x3(t) is minimized at t = —1/2. 
Cofactors 
While A,» has minor determinants of various sizes, the (n—1) x (n—1) 
minors are especially significant. 
_ 
9.3.4. Definition. For each position (i,j) in Anxn, the (i,j)-cofactor 
— 
is defined to be 
Ap eat-1 
Mi, 
where M;; is the (n —1) x (n—1) minor obtained by deleting the i'" 
row and j*" column of A. The matrix of cofactors is denoted by A. 
20 
—e 
Oi 
PE 
2 
For example, the cofactors Aj; and Aj3 in A= ( 
) 
are 
Agi=(—1)?*"Mo1 
= (—1)(-19)=19 
and 
Aj3=(—1)'+3My3=(+1)(18) 
= 18. 
—54 
-—20 
18 
The entire matrix of cofactors is A = ( 
19 
¢ 6). 
(9.3.4) 
—6) 
=2 
2 
Cofactor Expansion 
The cofactors of a square matrix A appear naturally in the definition of det (A). 
For example, the determinant of a general 3 x 3 matrix is, by definition, 
Q11 
G12 
413 
G21 
422 
493 | = A11022033 + 212023431 + 13021032 
@31 
432, 
433 
— €11423032 — 412021433 — 13422031 
Lh 
9.3. 
= 411 (@22433 — 423432) + A12 (a23431 — 421433) 
EES 
+ 413 (421432 — a22431) 
= ayAi1 + ai2Ago + a3 Aj3. 

A.3 Additional Properties of Determinants 
959 
Because this expression is in terms of the entries of the first row along with the 
corresponding cofactors, (9.3.5) is called the cofactor expansion of det(A) in 
terms of the first row. Notice that this expansion simply amounts to the inner 
product of the first row of A with the cofactors for the first row. It should be 
clear that there is nothing special about the first row of A. That is, it's just 
as easy to write an expression similar to (9.3.5) in which entries from any other 
row or column appear. For example, the terms in (9.3.5) can be rearranged to 
produce 
det (A) = (oh) (a93031 = 421033) + ag2 (411433 = 13031) + a32 (a13@21 = 11423) 
= a42Aq2 + ag9Ao9 + a32A39. 
This is the cofactor expansion for det (A) in terms of the second column, which 
is the inner product of the second column of A with the cofactors of the second 
column. The 3 x 3 case is typical, and exactly the same reasoning can be applied 
to a completely general mn x n matrix in order to produce the following results. 
9.3.5. Theorem. The following expansions hold for all A € F"*". 
: 
: 
S yj 
det(A) 
ik=7, 
and 
~ yy 
. 
fdet(A) 
fh =, 
In other words, the inner product of any row (column) with the cofactors 
from the same row (column) yields det (A), but the inner product of any 
row (column) with the cofactors from a different row (column) is zero. 
Proof. 
The argument for showing that det (A) is given by the inner product 
of any row (column) with the cofactors of the same row (column) is exactly 
the same as that for the 3 x 3 cases illustrated above—details are left to the 
reader. To prove that the inner product of a row (column) with the cofactors from 
a different row (column) is zero, consider the case for columns—in particular, 
consider the inner product of the k*" column of A with the cofactors from the 
7° = column tor eka jet A be the result of replacing the j'" column in 
A by the k*" column of A. Since A has two identical columns, det (A) = 0. 
Furthermore, the cofactor associated with the (i,7)-position in A is AG j, Which 
is the cofactor associated with the (i, 7)-position in A, so expansion of det (A) 
in terms of the j*" column yields 

960 
Appendix 
An Overview of Determinants 
jth 
pth 
+ 
L 
ail 
vee 
Aik 
spe 
Aik 
wee 
ain 
: 
f 
se 
O= det (A) =| Gm 
++" 
Cth 
tt 
Gin 
oes 
Cin | = y ain Aij- 
: 
3 
; 
j=1 
An1 
eee 
ank 
race 
Onk 
ee 
ann 
Thus the inner product of the k*" column of Anxn with the cofactors of the 
j*' column of A is zero. A similar result holds for rows. 
& 
Example 
hee, 
To evaluate det (A) by a cofactor expansion, the effort is minimized by expand- 
ing det (A) in terms of the row or column that contains a maximal number of 
0 
O 
One? 
: 
fe 
al 
Gy 
.5 
; 
f 
fth 
zeros. For example, if 
A= | 5 7 
9 g 
|, then the expansion in terms of the 
0 
3 
-1 
4 
first row is most efficient because 
i 4 
<6 
det (A) = a1 Ais + aigAi2 + a13A13 + aiaAra = a14Ara = (2)(-1) : g 4 
: 
Now expand this remaining 3 x 3 determinant either in terms of the first column 
or the third row. Using the first column produces 
il 
6 
ii 
Z 
3 
-l Ss (7)(+1)| § i ae (3)(-1)| 3 ag eo ae pe 
so det(A) = (2)(—1)(—34) = 68. Try an expansion using different rows or 
columns, and verify that the final result is the same. 
This example illustrates how cofactor expansions can take advantage of zeros 
in convenient positions. However, for a matrix Ay 
yn with no zero entries, it can 
be verified that evaluating det (A) with successive cofactor expansions requires 
n!\ ip 
rath en Pisin 
I 
3 
(n—1)! 
multiplications. Even for moderate values of n, this number is too large for the 
expansion by cofactors to be a practical computational method. 
The Adjugate Matrix and A~! 
Since there is one cofactor for each position in An xn, it follows that the array 
of cofactors A = [Ais] is also an n x n matrix. However, it turns out that AS 
the transpose of the matrix of cofactors, is more useful than A itself, so A' is 
given a special name. 

—_ 
t 
The observations made above immediately lead to the following determinant 
formula for A~!. 
For example, if A = 
is nonsingular, then 
d 
7 
ag 
d —c 
d —b 
SATA) tht 
cai 
Ach) 
2 
em 
aoe 
and det (A) = ad — be, so 
adj (A) 
1 
d —b 
i) ae 
= 
ei det(A) 
ad—be\-c 
a 
This handy formula was also obtained in (1.8.4) on page 72. 
Older texts sometimes refer to adj (A) = A" as the adjoint of A, but this terminology is 
ambiguous because, as pointed out on page 652, A* 
is frequently called the adjoint of A. 
Context usually makes it clear which concept is being referred to. 

962 
Appendix 
An Overview of Determinants 
Example 
: 
HA 
Suppose that an application only requires [A- Py and [A ee where 
SPes 
A=(2 
0 6). 
ae 
ae 
It would be wasted effort to compute AW! just to get the two desired entries. The 
cofactors A»; and Ais were determined in (9.3.4) on page 958 to be Aoi = '9 
and Aj3 = 18, and it's straightforward to compute det (A) = 2, so 
A 
19 
y 
A 
18 
rele aN =z 
and 
[A]. 
= —F = — = 9. 
31 
det (A) 
2 
However if the entire inverse A~! is desired, then using the matrix of cofactors 
A in (9.3.4) produces 
oes oe 
=i 
ee a 
adj(Ay) 
© 
Ae we typ ee =2) 
det(A) 
det (A) 
( 
Example 
Formula (9.3.8) for A! is also a direct consequence of Cramer's rule because 
AS is the i*" component in the solution to Ax = e,;, where e; is the 7%" 
unit vector. By Cramer's rule, this is 
det (A; 
bere 
! 
det (A) 
where A, is identical to A except that the i'* column has been replaced by 
e;. The cofactor expansion in terms of the i" column yields 
th 
t 
Qa11 
0) 
Qin 
det (Aj) = 
[aii 
< 
F 
= 
ain] 
= Aji 
an 
0 
ann 
Continuity of det (A) and A7! 
The issue of continuity is fundamental throughout mathematics, so it is only 
natural to ask about the continuity of det (A) as well as A~!. While the de- 
terminant formula for A~! given in (9.3.8) is not practical for numerical work, 
it is the perfect theoretical tool for establishing the continuity ' otAe® 
The continuity of A~! was also proven earlier in Theorem 2.3.12 on page 171. Note that the 
continuity of A? 
is in direct contrast with the lack of continuity exhibited by the pseudoin- 
verse At (see Exercise 3.5.15 on page 379). 

A.3 Additional Properties of Determinants: 
963 
Proof. 
The sum, the product, and the quotient of continuous functions are each 
continuous. In particular, the sum and the product of any set of numbers vary 
continuously as the numbers vary, so, since det(A) 
is a sum of products of 
entries from A, it follows that det(A) is a continuous function of the aj;'s. 
By (9.3.9), each entry [A~'],; = 
A,;/det(A) 
in A7! is a quotient of two 
determinants, so, as long as A remains nonsingular, A~! varies continuously 
with the a;;'s. 
i 
The Moral: There is a lesson to be learned from the preceding proof. The 
formula A~! = adj(A) /det (A) is nearly worthless for actually computing the 
value of A~!, but, as the above proof demonstrates, the formula is nevertheless 
a useful mathematical tool. It's not uncommon for applied oriented students to 
fall into the trap of believing that the worth of a formula or an idea is tied to its 
utility for computing something. This example makes the point that things can 
have significant mathematical value without being computationally important. 
In fact, most of the theory of determinants is in this category. 
Differential Equations and the Wronskian Determinant 
A system of n homogeneous first-order linear differential equations 
ax; (t) 
dt 
= aj (t)x1(t) + ajo (t)xzo(t) +--+ + ain(t)tn(t), 
i=1,2,...n, 
can be expressed in matrix notation by writing 
4 (t) 
ayi(t) 
ar2(t) 
in(t) 
r3(t) 
@_(t) | | 
aai(t) 
aza(t) 
A2n(t) 
r(t) 
iy aN erga 
(f)/ Van () 
(t), wo(t),...,Wn(t)} be a set 
t)x(t), and put these solutions 
in a matrix W(t)nxn = [wi(t) | wo(t) |---| wn(t)] so that W'(t) = A(t)W(t). 
Matrix W(t) is the Wronski matric, and w(t) = det (W(t)) is the Wronskian 
determinant (see Exercise 9.2.20 on page 955). 
or, equivalently, x'(t) = A(t)x(t). Let S = {w 
of n x 1 vectors that are solutions to x/(t)= A 

ry 
4us Petia 
Ley 
ers 
ee: 
Proof. 
Let W = W(t) and A = A(t). The rule for differentiating determi- 
nants given in Theorem 9.2.15 (page 952) says that dw(t)/dt = >>", det (Dj), 
where 
Wi1 
W112 
Win 
, 
: 
Txx7/ 
, 
Te 
} 
Di=| 
Wi 
We 
-': 
Win | =W+eie; W' —ee; W. 
|
' 
{
Wnt 
Wn2 
*** 
Wnn 
(Note that (—e;e/W) subtracts Wj, from the i" row while (+e;e/ W') 
adds W', to the i'" row.) Use the fact that W' = AW to write 
D; = W + ee! 
W' — ee? W 
= W +eje; 
AW — exe; W = (I +e; (e7 A—e;) )W, 
and apply the formula from Theorem 9.3.2 (page 956) for the determinant of a 
rank-one updated matrix together with the product rule (Theorem 9.2.10, page 
947) to produce 
det (D;) = (1 + e7 Ae; — e7 e;) det (W) = aji(t) w(t). 
Therefore, 
a = » 
det (D;) = (> 
ii 0) 
w(t) = trace A(t) w(t). 
In other words, if 7(t) = trace A(t), then w(t) satisfies the differential equation 
t 
w'(t) = T(t) w(t), the solution of which is w(t) = w(€) et r(é) Cos 
In addition to its mathematical elegance, Theorem 9.3.9 is a useful result 
because it provides the following important corollary. 

— 
Proof. 
If S is linearly independent at €), then W/(&) is nonsingular, so 
nt 
Tae) 0. Tf i T(€)d€ is finite when ¢ € (a,b), then ibs Bos is finite 
and nonzero on (a,b), so, by Theorem 9.3.9, w(t) 4 0 on (a,b). Therefore, 
W(t) is nonsingular for t € (a,b), and thus S is linearly independent at each 
= 
- 
 ¢teé(a,d). 
Exercises for section 9.3 
9.3.1. Use a cofactor expansion to evaluate each of the following determinants. 
ert 
6) 0, 22) 3 
Ou tie: 
1; 0. 2 
1 080 1 
@ | 
624) 
Cy 
Saal 
(yl, yo 
1 
—2 
Ono 3. © 
fob) 
i 6 
=il 
* 
ie ee 
Doren 
oO 
ela) 
@ (§ 23] 
(b) 
E iriw 
tev 
= 
692 —320 

9.3.4. Is the following equation a valid derivation of Cramer's rule for solving 
a nonsingular system Ax =b, where A, is as described on page 957? 
det (A; 
as 
wei 
a: 
4 
~ ra det (A *Ai) = det [er KS Ste, eEj+1 ove e,,| ae 
, 
9.3.5. 
(a) By example, show that det (A +B) 4 det (A) + det (B). 
(b) Using square matrices, construct an example that shows that 
nee \e BF # det (A)det (D) — det (B)det (C). 
9.3.6. Suppose rank (Bmxn) =n, and let Q be the orthogonal projector onto 
N (B*). For A = [B|emx1], prove c7Qc = det (A7A)/det (B7B). 
9.3.7. If Anxn isa nonsingular matrix, and if D and C are n x k matrices, 
explain how to use (9.3.1) to derive the formula 
det (A + CD*) = det (A)det (I, +D7A7'C). 
Note: This is a generalization of (9.3.3) because if c; and d; are the 
i' columns of C and D, respectively, then 
A+CD? = A+cidf +e2d3 +--+ e,d7. 

A.3 Additional Properties of Determinants 
9.3.8. 
9.3.9. 
9.3.10. 
9.3.11. 
9.3.12. 
9.3.13. 
9.3.14. 
9.3.15. 
967 
Prove that for all matrices A € F"*" and columns c,d € F™!, 
det (A + cd") = det (A) + d" [adj (A) 
Je. 
Explain why A is singular if and only if A[adj(A)] = 0. 
For a nonsingular linear system Ax =b, explain why each component 
of the solution must vary continuously with the entries of A. 
For scalars a, explain why adj(aA) = a"~ladj(A). Hint: Recall 
Exercise 9.2.12. 
For an n Xn matrix A, prove that the following statements are true. 
(a) If rank(A) <n—1, then adj(A) =0. 
(b) If rank(A) =n-—1, then rank (adj(A)) =1. 
(c) If rank (A) =n, then rank (adj(A)) = 
n. 
In 1812, Cauchy discovered the formula that says that if A is nxn, 
then det (adj(A)) = [det (A)]"~* Establish Cauchy's formula. 
For the following tridiagonal matrix A,,, let D, = det(A,), and derive 
the formula D, = 2Dn—i1 — Dn—2 to deduce that D, =n+1. 
Deal VEO ye 
a0 
Kf 
62 Slee 
A,= 
ae eh 
Geel 
eee 
(oe 
Wa 
© 
By considering rank-one updated matrices, derive the following formulas. 
are 
1 
Dae 
1 
(a) 
ey 
oe 
SOS 
: 
oa 
ee 
1 
1 
Lon 
ey 
foe fe 
i 
p fo 
py. 
28) 
i 
(oa 
eas 
={ (1+ 2s) 
ifoAs, 
ahd 
alee 
0 
i @— 
2s 
6 of 
alaxn 
1+ai4 
a2 
An 
ay 
l+aa 
An 
(c) 
Sail ce OY te Oy + «+ 4 in. 
Ay 
op, 
lobar 

968 
Appendix 
An Overview of Determinants 
9.3.16. 
9.3.17. 
9.3.18. 
9.3.19. 
9.3.20. 
9.3.21. 
9.3.22. 
A bordered matriz has the form B = ( 
yr z) in which Anyn is 
nonsingular, x is a column, y' is a row, and a is a scalar. Explain 
why the following statements must be true. 
A 
x 
ak apie —det (A + xy"). 
(b) 
(a) 
x 
Pan TD 
F 
fo |= nvTadi (A) 
If B is 
mxn and C is n xm, explain why (9.3.1) guarantees that 
A™det (AI, — CB) = \"det (AI, — BC) is true for all scalars A. 
For a square matrix A and column vectors c and d, derive the fol- 
lowing two extensions of formula (9.3.3). 
(a) If Ax=c, then det (A +cd7) = det (A) (1+d7x). 
(b) If y'A=d?, then det (A + cd") = det(A)(1+y7c). 
Describe the determinant of an elementary reflector (page 104) and a 
plane rotation (page 98), and then explain how to find det(A) using 
Householder reduction (page 671) and Givens reduction (page 676). 
Suppose that A is a nonsingular matrix whose entries are integers. 
Prove that the entries in A~! are integers if and only if det (A) = +1. 
Let 
A =I-—2uv" bea 
matrix in which u and v are column vectors 
with integer entries. 
(a) Prove that A~! has integer entries if and only if v'u = 0 or 1. 
(b) A matrix is said to be involutory whenever A~! = A. Explain 
why A =I- 2uv" is involutory when vu = 1. 
Use induction to argue that a cofactor expansion of det (An x») requires 
cnn! eee Ea 
+ 
: 
pnd 
per 
ey 
(n—1)! 
multiplications for n > 2. Assume a computer will do 1,000,000 multi- 
plications per second, and neglect all other operations to estimate how 
long it will take to evaluate the determinant of a 100 x 100 matrix using 
cofactor expansions. Hint: Recall the series expansion for e", and use 
LOOM 
Gispe10 @ 

A.3 Additional Properties of Determinants 
9.3.23. 
9.3.24. 
9.3.25. 
9.3.26. 
969 
Determine all values of \ for which the matrix A—AI is singular, where 
0 
-3 
—-—2 
At ( 
Daa 
5 
2). 
—2 
-3 
0 
Hint: If p(A) = A" + an—1A"-1 +++» +a1A+ a9 is a monic polynomial 
with integer coefficients, then the integer roots of p(A) are a subset of 
the factors of ap. 
Suppose that fi(t), fo(t),..., intt) are solutions of n*'-order linear 
differential equation y™ + p;(t)y"—) + ---+ pp_1(t)y' + pp (ty = 0, 
and let w(t) be the Wronskian determinant 
fi(t) 
f(t) 
fn(t) 
1 (t) 
3(t) 
fro) 
w(t) = 
: 
fe, ee 
F-Y) 
By converting the n*'-order equation into a system of n first-order 
equations with the substitutions 7; = y, 22 = y',...,2n = y'—, 
t 
2 
d 
show that w(t) = w(fo)e Jec Boren arbitrary constant & . 
Evaluate the Vandermonde determinant by showing 
1 
2 
oe 
pee 
1 
ea) 
are 
oe 
: 
; 
= [[@ = Ne 
: 
2 
q>i 
can 
Ves 
eae 
When is this nonzero? 
: 
—_ 
Tee 
ON es 
a 
er 
a 
Hint: For the polynomial p(A) = 
, use induction 
+ 
. 
. 
CA 
k-1 
i 
aie, 
iy 
CED 
Gy 
ae 
to find the degree of p(X), the roots of p(X), and the coefficient of \*—! 
in p(A). 
Jacobi's Formula for the Derivative of a Determinant. Suppose 
that each entry in An 
xn = [a;;(x)] is a differentiable function of a real 
variable x. Use formula (9.2.10) to derive the formula 
j=l i=1 
d(det (A)) 
dx 
dai; 
° 
a 
GAR 
ee Ai ad trace | 
ee 
\. 

970 
Appendix 
An Overview of Determinants 
9.3.27. Consider the entries of A to be independent variables, and use formula 
(9.2.10) to derive the formula 
Odet(A) 
3 
0a; 
a 
RES 
9.3.28. Laplace's Expansion. In 1772, the French mathematician Pierre-Simon 
Laplace (1749-1827) presented the following generalized version of the 
cofactor expansion in Theorem 9.3.5. For an n x n matrix A, let 
A(ijio+++ix|jijo+++ je) = the k x k submatrix of A that lies on 
the intersection of rows 71, 22,...,%% 
with columns 71, j2,.-.-,Jk; 
and let 
M(iyi2+++ 
tn |Jijo+++ Je) = the 
n —k x n—k minor determinant 
obtained by deleting rows 71, 722,...,2% 
and columns jj, J2,...,j~ from A. 
The cofactor of A(i,---t%|j1+::Jjg) is defined to be the signed minor 
A(ig+++tg|ja-++ jg) = (1) tte tat +i M (iy +++ ig | j++ 5k) 
This is consistent with the definition of a cofactor given earlier because 
if A(i|j) = ai, then A(i|j) = (-1)*9M(i|j) = (-1)4IMy = Aj. 
For each fixed set of row indices 1 <i, <-+:<ip <n, 
det(A)= 
So det A(ir-++t¢|jr-+-de)A(ir 
= 
++ te | 
Sa ++ Se) 
I<ji<o-<jeSn 
Similarly, for each fixed set of column indices 1 < j, <--- < jx <n, 
det (A) = 
SE det A(ir-++ig | j1-*-ju)ACir 
+++ te |51-*- Se). 
1<ip<-<ip<n 
Each of these sums contains 
(7) terms. Use Laplace's expansion to 
evaluate the determinant of 
A= 
or 
oo 
e 
Ornw 
0 
1 
=i 
0 
in terms of the first and third rows. 

Index 
A 
asterisk, 
7 
asymptotic rate of convergence, 
626 
Abel. Niels. 
765 
augmented matrix, 
123 
absolute magnitude, 
171 
HAS SADS) Dae 
B08 
absolute uncertainty or error, 
372 
B 
absolute value of complex number, 
22 
absorbing Markov chains, 
876, 877 
absorbing state, 
875, 877 
absorption, into ergodic classes, 
875 
accumulation products of elementary matrices, 
138 
back substitution, 
77, 122, 124 
backward error analysis,, 
239 
backward triangle inequality, 
29, 34, 656 
for matrices, 
84 
action of linear function, 
510 
balls, hollow, 
33 
Adams, J.C., 
303 
Banach, Stefan, 
477 
addition 
Banachiewicz, Tadeusz, 
285 
of subspaces, 
421 
band matrix, 
282 
of vectors, 
5 
Bartle, Robert, 
900 
properties of, 12 
Bartlett, M. S., 
168 
additive identity, 
12, 410 
base-b representations, 
749 
additive inverse, 
12, 410 
bases 
adjacency matrix, 
144, 443, 69 819 823 
changing of, 
528 
number of paths, 
823 
for the fundamental subspaces, 
434 
periodic, 
824 
basic columns, 
148, 200 
adjoint 
and consistency, 
202 
matrix and operators, 
9, 652, 961 
combinations of, 
202 
properties of, 
653 
independence, 
151 
adjugate matrix, 
322, 961 
basic variables, 
204 
affine 
basis, 
416 
functions 
52 
change of, 
529 
derivative and continuity, 
909 
characterizations of, 
ALT 
projections, 
458 
for assum, 
423 
space, 
17, 458, 476 
for direct sum, 
544 
aggregation in Markov chains, 
888 
for dual space, 
520 
airline network example, 
69 
for intersection of spaces, 
439 
algebraic connectivity, 
446, 455 
for space of linear transformations, 
507 
algebraic group, 
3, 558 
independence 
532 
algebraic multiplicity, 
296, 297, 307 
Fourier, 
721 
Alphabet conglomerate, 
867 
frequency, 
721 
alternating projection formula, 
469 
orthonormal, 
685 
analog, 
716 
standard, 
420 
analytic function, 
604, 908 
time 
(CLs 
analytic, resolvent, 
912 
Bauer—Fike bound, 
329 
analyticity of spectral projectors, 
923 
beads on a string, 
385, 770 
Anderson—Duffin formula, 
472 
belongs to, 
7 
Anderson, W.N., Jr., 
472 
Beltrami, Eugenio, 
359 
Ando, Albert K., 
890 
Berman, Abraham, 
625 
angle, 
42, 43 
Bernoulli, Daniel, 
687 
between complementary spaces, 
560 
Bessel's inequality, 
712 
cosine of, 
648 
best rank k approximation, 
378 
minimal, 
561 
biased estimator, 
492 
aperiodic Markov chain, 
871 
bidiagonal form, 
792 
aperiodic matrix, 
820 
bidiagonal, decomposing, 
794 
applied linear algebra, definition, 
2 
bijection, 
515, 516 
arg, 
270 
binary representation, 
745 
argument principle, 
757 
Birkhoff, Garrett, 
630 
Ashbel Smith Chair, 
630 
Birkhoff's theorem, 
879 
associative property, 
12, 67 
bisection, of a graph, 
455 
of convolution, 
753 
bit reversal, 
745 
of direct sums, 
562 
bit reversing permutation matrix, 
755 

972 
Bjerhammar, Arne, 
193 
block Cholesky factorization, 
404 
block matrices, 
75 
and linear operators, 
565 
determinant of, 
956 
inverse of, 
167 
multiplication of, 
75 
block-diagonal matrix 
coordinate matrix, 
536 
eigenvalues of, 
299 
form, 
536 
form for real arithmetic, 
310 
inverse of, 
166 
pseudoinverse of, 
200 
rank of, 
198 
block-triangular matrix, 
77 
coordinate matrix, 
535 
determinant of, 
949 
eigenvalues of, 
299 
inverse of, 
174 
Bohr, Niels, 
644 
Boley, Daniel L., 
713 
Boolean matrix, 
822 
Boolean multiplication, 
822 
bordered matrix, 
167, 968 
boundary value problems, 
220 
bounded function, 
900 
bounded powers, 
635 
bounds, for eigenvalues, 
423 
bounds, for spectral radius, 
621 
branch point in electrical circuits, 
222 
branch, 
222,449 
Brauer, Alfred, 
167, 321, 766, 768 
Brin, Sergey, 
866 
Brouwer's fixed-point theorem, 
800 
Brown, James W., 
900 
Bunyakovskii, Victor, 
27 
C 
cancellation law, 
57 
cancellation, catastrophic, 
235 
canonical form, reducible matrix, 
872 
canonical, quadratic form, 
396 
Cantor, Georg, 
227 
cartesian product, 
900 
catastrophic cancellation, 
235 
Cauchy integral formula, 
604, 919 
Cauchy product, 
740 
Cauchy—Bunyakovskii-Schwarz inequality, 
Cauchy—Goursat theorem, 
919, 920 
Cauchy—Riemann equations, 
907 
Cauchy—Riemann, 
908 
Cauchy—Schwarz inequality, 27, 30, 647, 655, 955 
classic version, 
27 
matrix version, 
82 
Cauchy, Augustin-Louis, 
26 
Cauchy's determinant formula, 
967 
- Cayley transformation, 
108, 346 
Cayley—Hamilton theorem, 
312, 317, 594, 602, 614 
Cayley, Arthur, 
55, 116, 940 
CBS inequality, 
27, 30, 647, 655 
censored distribution, 
885 
censored Markov chain, 
883, 892 
censored probability distribution, 
884 
centered difference approximations, 
221 
centroid, 
703, 706 
centrosymmetric, 
12 
centrosymmetric, 
318 
Cesaro sequence, 
633 
Cesaro summability, 
632, 635, 633 
Cesaro, Ernesto, 
633 
change of basis, 
528 
by similarity, 
529, 530 
operator for, 
513 
change of coordinates, 
419, 514, 688 
SVD and URV, 
528 
characteristic equation, 
289, 290 
coefficients of, 
294 
and trace, 
302, 303 
characteristic polynomial, 
289, 290, 299, 301-303 
characteristic values and vectors, 
287 
Chebyshev, Pafnuty Lvovich, 
494, 859 
checking an answer, 
376 
Chinese counting board, 
115, 939 
Cholesky algorithm, 
277 
Cholesky factor, uniqueness, 
281 
Cholesky factorization, 
382, 383, 404, 671, 
Cholesky, Andre-Louis, 
273, 284 
Churchill, Ruel V., 
900 
Cimmino, Gianfranco, 
479 
Cimmino's reflection method, 
479 
circuit, 
449, 835 
circulant matrix, 
754, 755 
circulants and convolution, 
755 
classical Gram—Schmidt algorithm, 
659 
classical vs modified Gram—Schmidt, 
662 
classification, data, 
707 
clock cycles, 
871 
closed contour, 
604 
closed set, 
900 
closest point and Fourier expansion, 
712 
closest point theorem, 
466 
closest rank k matrices, 
378 
closest rank k matrix, 
365 
closure property, 
12, 410 
cloud partitioning, 
708 
cluster analysis, 
707 
clustering with Simon—Ando, 
896 
clustering, spectral, 
448 
Coca-Cola, 
490 
coefficient matrix, 
61, 119 
coefficient of determination, 
484, 490 
coefficients, Fourier, 
687 
cofactor expansions, 
959 
cofactor, 
958, 970 

Index 
cofactors in eigenvectors, 
322 
Collatz-Wielandt formula, 
805, 807, 817 
Collatz, Lothar, 
805 
column, 
3 
basic, 
147 
dependencies, 
147 
equivalence, 
145,166 
independence, and nonsingularity, 
160 
operations, 
135, 136 
and rank, 
185 
scaling, 
241 
space, 
20, 431, 432 
stochastic, 
859 
combination, linear, 
15 
commutative property, 
7, 12, 57 
for convolution, 
753 
commuting matrices, 
80, 231, 321 
eigenvectors of, 
302 
compact set, 
900 
companion matrix, 
295, 318, 917, 918 
compatible norms, 
83, 84 
competing species model, 
338 
complementary subspaces, 
544 
angle between, 
560 
projector onto, 
547 
complete 
orthonormal set, 
40, 100 
pivoting, 
242 
and numerical stability, 
679 
reducibility, deviation from, 
889 
set of eigenvectors, 
305 
complete space, 
424 
complex 
numbers, 
2 
conjugate, 
2, 4, 22 
vectors, independence of, 
185 
differentiability, 
907 
eigenvalues using QR, 
781 
exponential, 
334 
magnitude, 
22 
reflector, 
106 
component matrices, 
600 
composition 
of functions, 
429 
of linear functions, 
56, 511 
of matrix functions, 
603, 612 
computation, SVD, 
791 
computer graphics, 
96, 111 
condition and determinant, 
257 
condition, linear system, 
252 
condition number, 
172, 176, 252, 256, 372, 373 619, 671 
2-norm, 
372 
bounds for, 
373, 375 
for eigenvalues, 
329, 932, 
for eigenvectors, 
934, 936 
generalized, 
379 
for positive definite, 
405 
for Schur complement, 
405 
conditional probability, 
861 
conditioning and pivots, 
381 
conformable, 
56 
conformably partitioned, 
75 
congruence transformation, 
281 
congruent matrices, 
395, 396 
conjugate 
linear, 
53, 646 
pairs of eigenvalues, 
317 
row space, 
431 
transpose, 
4, 9 
reverse order law, 
71 
vectors, independence of, 
185 
connected components, 
445 
connected graph, 
443 
connectivity 
algebraic, 
445, 446, 455 
matrix, 
69 
strong, 
455 
connectivity, strong, 
455 
consistent linear system, 
62, 201, 202 
constituent matrices, 
600 
continuity 
uniform, 
901 
weak eigenvalue, 
761 
continuity of 
eigenvalues, 
757, 758, 915 
simple eigenvalues, 
917, 759 
with respect to a parameter, 
762 
alternate interpretation of, 
760 
eigenvectors, 
762 
inner products, 
649 
inverses, 
170, 962 
linear and affine functions, 
909 
norms, 
35, 656 
for matrices, 
319 
Perron root, 
815, 816 
Perron vector, 
840 
polynomial roots, 
917, 918 
pseudoinverses, lack of, 
379 
resolvents, 
913 
spectral projectors, 
914 
continuous Fourier transform, 
727 
continuous functions, max and min, 
contour, 
604 
contour integral for matrix inverses, 
convergence, 
34 
coordinate-wise, 
35 
matrix sequence, 
88 
to zero, 
326 
convergent matrix, 
633 
convex combination, 
354, 869 
convex hull, 
354, 356 
convex set, 
356 
convolution, 
737, 738, 741, 752-755 
Cooley, James W., 
742, 748 
cooperating species model, 
338 
35, 901 
607 
973 

974 
coordinates, 
3, 418, 687 
change of, 
514, 688 
SVD and URV, 
528 
independent property, 
532 
of 
acomposition, 
511 
of identity operator, 
509, 512 
ofasum, 
524 
of a vector, 
418, 419, 506 
via frequency, 
724 
coordinate matrix, 
508 
block structure, 535, 536 
of a derivative, 
905 
coordinate spaces, 
411 
core, 
555 
core-nilpotent decomposition, 
556, 567 
correlation, 
43 
correlation coefficient, 
46, 47,484 
correlation matrix, 
48 
cosine, 
43, 648, 720 
and frequency coordinates, 
724 
counting board, 
115, 939 
coupling theorem, 
855 
coupling vectors and matrices, 
855 
Courant—Fischer theorem, 
347, 349,425 
alternate version, 
468 
Courant, Richard, 
424 
covariance, 
47, 493, 698 
covariance matrix, 
48, 698 
Cramer, Gabriel, 
957 
Cramer's rule, 
939, 957 
critical point, 
394 
cross product, 
113 
cross-symmetric, 
318 
Crout, Prescott Durand, 
285 
curve fitting, 
162, 495 
cycle, 
835 
cyclic products, eigenvalues of, 
303 
D 
dangling nodes, 
868 
Darwin, Charles, 
482 
data clusters, 
707 
data compression, 
694 
data matrix, 
701 
data visualization, 
706 
DC component, 
721 
decomposing Perron vectors, 
854 
decomposition into generalized eigenspaces, 
decomposition, spectral, 
341 
decomposition, URV, 
460 
deficient, 
296 
defined to be, 
785 
definite matrices, 
383, 884 
deflation, eigenvalue problems, 
316 
degree of 
anode, 
443 
degree, in and out in graphs, 
819 
degrees of freedom, 
420 
dense matrix, 
677 
Index 
dense subset, 
319 
dependencies among columns, 
147 
derivative 
coordinate matrix of, 
905 
definition, 
901 
directional, 
903 
Fréchet, 
901 
Jacobian matrix, 
905 
partial, 
904 
properties of, 
903 
operator, 
511 
total, 
901, 910 
derivatives of 
convolutions, 
754 
determinants, 
951, 955,969 
matrices, 
65 
affine functions, 
909 
eigenvalues and eigenvectors, 
929, 933 
Perron root and vector, 
843, 844, 847, 849 
spectral projectors, 
843, 844, 928 
inverses, 
172 
traces, 
910 
solutions, 
177 
determinants 
73, 304, 941 
sums, 
954, 956, 966, 968 
minors, 
946 
block triangular, 
949 
computing, 
948 
product rule, 
947 
similarity invariant, 
533 
volume, 
950 
determinants and 
condition, 
257 
eigenvalues, 
294 
eigenvectors, 
322 
rank, 
947 
Deutsch, Emeric, 
849 
deviation from complete reducibility, 
889, 893 
DET 
2%, 728 
diagonal 
entries, 
8 
dominance, 
160, 642 
in Gaussian elimination, 
178 
irreducible, 
768 
matrices, 
8 
inverses of, 
163 
eigenvalues of, 
292 
diagonalizability, 
304, 305 
diagonalizable matrix, function of, 
550 
diagonalization 
circulants, 
754 
congruence, 
395 
Jacobi's method, 
682 
quadratic form, 
394 
similarity, 
530 
simultaneous, 
321, 399 
diagonally dominant matrices, 
160, 246, 642, 647, 768 
dictionary, LSI, 
695 

Index 
difference equations, 
315, 328, 616 
difference, of ranks, 
439 
differentiability, 
901 
of eigenvalues, 
763, 924 
of spectral projectors, 
923 
real and complex versions, 
differential equations, 
287 
independent solutions, 
963 
nonhomogeneous, 
610 
singular systems, 
615 
solutions, 
331 
stability, 
333, 609, 610 
systems, 
330, 609 
uncoupling, 
385 
906, 907 
differentiation, linearity, 
53 
differentiation, Perron root and vector, 
diffusion, 
332 
diffusion equation, 
388 
dimension, 
419 
reduction of, 
dimension of 
direct sums, 
544 
images of subspace, 
451, 475 
intersection of spaces, 
453 
nullspace, 
433 
range and nullspace, 
433 
space of transformations, 
507 
subspaces, 
420 
sum of subspaces, 
422 
direct product, 
227 
direct sum, 
456, 544 
associativity, 
562 
of generalized eigenspaces, 
592 
of several subspaces, 
563 
symmetric and skew-symmetric matrices, 
563 
directed cycle, 
835 
69, 443 
directional derivative, 
903 
840, 842 
694, 706 
directed graph, 
directional variance, 
704 
Dirichlet problem, 
388, 771 
Dirichlet, Johann P., 
227, 388 
discrete 
convolution, 
738 
cosine, 
720 
exponential, 
720 
Fourier transform, 
716, 727 
Laplacian, 
232, 388, 628 
condition of, 
405 
eigenvalues of, 
406 
three-dimensional, 
407 
sine, 
720 
time, finite-state Markov chain, 
859 
time vector, 
719 
disjoint subspaces, 
544 
distance 
between sets, 
760 
euclidean, 
23 
function, 
764 
to lower rank matrices, 
365 
to singularity, 
366 
orthogonal, 
466, 479 
distinct eigenvalues, 
310 
distortion ratio, 
371 
distribution 
censored, 
885 
limiting, 
328 
population, 
328 
vector, kt" step, 
860 
distributive property, 
12, 67 
of convolution, 
754 
DNA of a matrix, 
589 
document vector, 
695 
domain, 
52 
Doolittle, Myrick Hascall, 
284 
dot product, 
24 
double dual, 
521 
double shift QR, 
782, 788 
doubly stochastic, 
859, 879 
Drazin pseudoinverse, 
557, 558, 918, 927 
as a polynomial, 
567 
as a contour intergral, 
608 
as a standard integral, 
615 
equality with Moore—Penrose pseudoinverse, 
Drazin, M- P., 
557 
519, 520 
dual space, 
519, 653 
Duffin, Richard J., 
472 
Duncan, W. J., 
168 
Dwyer, Paul Sumner, 
285 
dual basis, 
E 
echelon form, 
128, 149, 150, 180 
reduced, 
129 
uniqueness, 
151, 152 
Eckart, C., 
359 
economic, input-output model, 
812 
edge matrix, 
112 
69, 143 
editorial comment, 
522 
edges, 
eigenpair, 
288 
eigenspace, 
288 
for normal matrices, 
341 
invariant, 
538 
eigensystems, 
287 
558 
975 

976 
eigenvalue, 
287, 288, 297 
bounds for, 
423, 766 
condition number, 
932 
conjugate pairs, 
317 
continuity, 
719, 757, 758, 760, 762, 917 
weak continuity, 
761 
Courant—Fischer minimax theorem, 
425 
differentiability, 
924 
lack of, 
763 
distinct, 
310 
generalized, 
402 
index of, 
584, 593 
Jacobian matrix, 
930 
largest and smallest, 
347 
multiplicities, 
297 
partial derivative of, 
929 
perturbations, 
329, 917, 931, 933 
Rayleigh quotient representation, 
356 
real, 
291 
semisimple, 
sensitivity, 
297, 593, 
761, 921 
similarity invariant, 
533 
spread, 
354 
variational description, 
468 
trace and determinant, 
294 
trace of products, 
303 
variational description of, 
348, 350 
eigenvalues of 
block-triangular matrices, 
299 
cyclic products, 
303 
discrete Laplacian, 
406 
projectors, 
547 
products, 
301 
idempotents, 
452 
rank-one matrices, 
299 
rank-one updates, 
sums, 
321, 322 
321 
Kronecker products, 
321 
Laplacian, 
391 
Matrix product, 
302 
stochastic matrices, 
636 
triangular matrices, 
292 
tridiagonal Toeplitz matrices, 
314 
eigenvector, 
287, 288 
computation of, 
790 
condition number for, 
934, 936 
continuity, lack of, 
762 
derivatives of, 
933 
determinants, 
322 
independence, 
305 
normalization, 
291 
perturbation of, 
934 
simple, 
933 
orthogonality, 
292 
using cofactors, 
322 
eigenvectors of 
commuting matrices, 
302 
projectors, 547 
similar matrices, 
304 
tridiagonal Toeplitz matrices, 
314 
Einstein, Albert, 
38 
electrical circuits, 
222, 449 
electronic circuit, stability, 
337 
elementary matrices, 
determinant of, 
944 
interchange matrix, 
141 
inverse of, 
138 
lower-triangular matrices, 
140, 
nonsingular matrices, 
165 
products of, 
138 
properties of, 
136 
triangular, 
156 
136, 139, 145 
260 
elementary row (column) operations, 
and determinants, 
943 
reversion of, 
145 
elementary projector, 
140 
elementary reflector, 
construction of, 
110 
determinant of, 
968 
ellipsoid, degenerate, 
381 
elliptical inner product, 
646 
empty set, 
18, 120 
end of proof symbol, 
9 
ENIAC, 
286 
ensemble matrix, 
896 
EP matrix, 
467 
EP, matrix, 
558 
equal matrices, 
7 
equivalence and rank, 
186 
equivalence relation, 
145 
equivalence, summary, 
166 
equivalent, 
23, 145 
norms, 
35, 37 
systems, 
120 
ergodic class, 
872, 876 
error sum of squares, 
483 
error vector, least squares, 
483 
error, absolute and relative, 
372 
error, relative, 
619 
essentially nonnegative matrices, 
essentially positive matrix, 
estimated vector, 
483 
estimators, 
491 
euclidean norm, 
22 
properties of, 
23, 30 
euclidean space, 
3 
Euler's number, 
5 
evolutionary processes, 
616 
existence of the resolvent, 
911 
explicit double shift QR, 
782 
explicit single shift QR, 
781, 784 
802 
822, 823 
123, 136 
104, 140, 479, 671 
Index 

Index 
exponential 
integrals, 
615 
complex, 
334 
discrete, 
720 
inverse of, 
614 
Kronecker product of, 
336 
limits, 
614 
matrix products, 
336 
sums, 
614 
extension set, 
417, 153 
F 
Faddeev and Sominskii, 
303 
fail-safe system, 
878 
fast Fourier transform, 
742, 746, 756 
integer multiplication, 
749 
factored Kronecker product, 
756 
operation count, 
752 
Fiedler eigenvalue, 
446 
Fiedler value, 
455 
Fiedler vector, 
447 
filtering signals, 
735 
finite difference matrix, 
319 
finite group, 
827 
finite-difference matrix and SOR, 
642 
finitely generated, 
415 
first principal component, 
699 
Fischer, Ernst, 
424 
five-point difference equations, 
389 
fixed point of a reflector, 
109 
flatness, 
413 
floating point numbers, 
233 
floating-point operations, 
77 
flop, 
77 
flop count for matrix inversion, 
164 
Forbes magazine, 
867 
forward error analysis, 
239 
forward substitution, 
262 
four fundamental subspaces, 
431 
Fourier 
basis, 
721 
coefficients, 
687 
coordinates, 
689 
expansion, 
687 
and projection, 
712 
and spectral theorem, 
693 
mavens, 
(CA 
series, 
687, 689 
transform 
continuous, 
727 
discrete, 
716, 227 
inverse, 
728 
Fourier, Jean Baptiste Joseph, 
687 
Fréchet derivative, 
901 
Fréchet, Maurice, R., 
649 
Frame, J. 5., 
303 
Francis, J. F.G., 
779 
free variables, 
204 
freedom, degrees of, 
420 
frequency, 
720 
basis, 
721 
coordinates, 
718, 721, 724 
domain, 
722 
Friedrichs, Kurt Otto, 
424 
Frobenius 
form, 
830, 837 
inequality, 
453 
matrix norm, 
82, 648, 655 
for normal matrices, 
345 
test for primitivity, 
821 
theorem for nonnegative matrices, 
Frobenius, Ferdinand Georg, 
82, 167, 180, 798 
full-rank factorization, 
189, 640 
non-uniqueness of, 
200 
of a projector, 
565 
with SVD, 
640 
functions 
affine, 
52 
composition of, 
56, 603 612, 
invertible, 
516 
domain and range of, 
52 
linear, 
52 
norm of, 
648 
spaces, 
411 
Jordan block, 
596 
functions of 
diagonalizable matrices, 
324, 550 
general matrices, 
597 
and Cayley-Hamilton, 
614 
integral representation, 
605 
using Cauchy formula, 
604 
Jordan blocks, 
596 
nondiagonalizable matrices, 
598 
normal matrices, 
343 
functional identities, 
603 
functional iteration, 
774 
functional, linear, 
519 
fundamental subspaces, 
431 
bases for, 
434 
orthonormal bases, 
436 
fundamental theorem of algebra, 
60, 290 
fundamental theorem of clustering, 
fundamental theorem of linear algebra, 
G 
Galle, J.G., 
303 
Galois, Evariste, 
765 
Galton, Sir Francis, 
482, 499 
gap, principal, 
709 
Garvin, Richard, 
748 
Gauss—Jordan method, 
125 
reduction, 
137 
for matrix inversion, 
163 
operation counts, 
126 

978 
Gauss—Markov theorem, 
490, 494 
Gauss-Seidel iteration, 
626, 627 
Gauss, Carl F., 
283, 408, 499, 938 
as ateacher, 
627 
doctoral dissertation, 
290 
gaussian elimination, 
120, 124, 283 
and diagonal dominance, 
178 
and LU factorization, 
259 
effects of roundoff, 
253 
growth in, 
239 
modified, 
129 
numerical stability, 
678 
operation counts, 
125 
gaussian kernel, 
896 
general diagonal, 
8 
general linear group, 
558 
general solution, 
120, 205 
convention, 
204 
for nonhomogeneous systems, 
210, 211 
generalized 
eigenvectors, 
540 
condition number, 
379 
eigenspace, 
540, 592 
eigenvalue problem, 
402 
eigenvectors, 
573, 591 
inverse 
Drazin, 
557, 558, 567, 608, 615, 918, 927 
group, 
558, 559, 643, 918, 927 
inner, outer, and reflexive, 
566 
Moore-Penrose, 
192, 200, 356, 557, 558, 608, 615 
genes and chromosomes, 
333 
geometric multiplicity, 
297, 307 
geometric series, 
170, 325, 618 
Gerschgorin circles, 
766 
Gerschgorin, S. A., 
766 
Girard, Albert, 
302 
Givens reduction, 
675 
and determinants, 
968 
numerical stability, 
679 
Givens rotations, 
98, 671 
Givens, Wallace, 
99 
Goldstine, Herman Heine, 
283, 286 
Golub, G. H., 
233, 791 
goodness of fit, 
485 
Google matrix, 
869 
Google, 
866 
gradient, 
394, 905, 910 
Gram matrix, 
657, 658 
Gram-—Schmidt process, 
208, 344, 658, 662, 666 
algorithms, implementations, 
664 
alternate expression, 
660 
and volume, 
667, 668 
modified, 
661 
numerical stability, 
679 
Gram, Jorgen P., 
658 
Gramian, 
657, 658 
graph, 
443 
cycle, 
835 
directed, 
69 
index, 
835 
Laplacian, 
443, 445 
of a matrix, 
455, 808 
partition, 
455, 447 
random walk on, 
824 
simple, 
824 
undirected, 
143, 157 
weighted, 
819 
graphics, three-dimensional rotations, 
Grassmann, Hermann G., 
409 
Greeley, Colorado, 
490 
Greenspan, Elizabeth, 
xi 
grid norm, 
32 
grid points, 
220 
group inverse, 
558, 559, 918, 927 
and derivatives, 
928 
and limits, 
643 
in terms of Moore—Penrose inverse, 
from full-rank factorization, 
643 
group matrices, 
558 
group, finite, 
827 
Gunning, Robert C., 
900, 908 
Guttman, L., 
168 
H 
Holder, Ludwig O., 
37 
Holder's inequality, 
32, 37 
Hadamard product, 
55 
Hadamard, Jacques, 
55, 766, 951 
Hadamard's inequality, 
950 
Halmos, Paul, 
ii, 644 
Hamilton, William R., 
312 
Harding, Margaret E. M., 
xi 
harmonic, 
716 
components, 
716 
functions, 
388 
Hartogs, Friedrich, 
908 
Hartogs' theorem, 
910 
Haynsworth, Emilie V., 
167 
heat equation, 
388, 406 
heat flow, 
770 
hermite form, 
158 
hermite interpolation polynomial, 
602, 641 
Hermite, Charles, 
158 
hermitian matrix, 
10, 468, 475 
eigenvalues of, 
291 
eigenvectors of, 
292 
eigen components of, 
346 
extremal eigenvalues of, 
347 
positive definite, 
273, 282 
positive semidefinite, 
382 
skew, 
346 
Hessenberg form, reduction to, 
680, 781 
Index 

Index 
Hessenberg matrices, 
780 
QR factorization of, 
682 
unreduced, 
781, 783, 795 
Hessian matrix, 
394, 399 
hidden faces, 
113 
Higham, Nicholas J., 
611 
Hilbert matrix, 
14, 246, 258 
Hilbert-Schmidt norms, 
381 
Hilbert, David, 
38, 424, 658 
historical comment on rise of matrix theory, 
522 
holomorphic function, 
604, 908 
homogeneous equations, 
203, 206, 209, 212 
homogeneous Markov chain, 
860 
Hooke, Robert, 
11 
Horst, Paul, 
303 
Hotelling, Harold, 
697 
Householder 
transformations, 
104, 671, 791 
Householder reduction, 
671, 673 
and determinants, 
968 
numerical stability of, 
679 
Householder, Alston S., 
104, 193 
hyperlink matrix, 
868 
hyperplane, 
53, 477 
oblique projection onto, 
568 
principal, 
713 
I 
idempotent matrices, 
109, 453, 464, 547 
and eigenvalues, rank, trace,sum, 
452 
and projectors, 
547 
orthogonal projectors, 
464 
eigenvalues of, 
300 
hermite form, 
158 
sum of, 
453 
idempotent operator, 
542 
identities, functional, 
603 
identity matrix, 
41, 71 
identity operator, 
505 
coordinates of, 
509, 512 
ill conditioned matrix, 
172 
ill-conditioned systems 
249, 373, 777 
image, and range, 
427, 428, 431 
image of a subspace, 
451, 475 
dimension of, 
454 
imaginary, pure, 
300 
implication symbol, 
9 
implicit double shift QR iteration, 
788 
implicit Q-Theorem, 
783 
implicit single shift QR, 
784 
implicit symmetric QR, 
786 
imprimitive matrices, 
820, 825 
maximal roots of, 
826 
spectrum of, 
828 
imprimitivity, 
820 
index of, 
829 830 
test for, 
821 
in-degree, 
819 
incidence matrix, 
444 
inconsistent system, 
119, 201 
indefinite, 
384 
independence, 
18 
and orthonormality, 
40, 50 
and rank 
complex conjugate vectors, 
185 
independent 
columns and rows and rank, 
181, 183 
columns and rows and echelon form, 
149, 150 
columns and nullspace, 
209 
eigenvectors, 
305 
independent solutions for 
differential equations, 
963 
linear algebraic equations, 
450 
spanning set, nullspace, 
206 
subset, maximal, 
417 
variables, 
489 
index 
of a square matrix, 
554 
by full-rank factorization, 
640 
of imprimitivity, 
820, 829,830 
of nilpotency, 
571 
of an eigenvalue, 
584 593 
ofagraph, 
835 
indexing for LSI, 
695 
induced norms, 
85, 90 
2-norm, 
363 
inertia, 
398 
infinite dimensional, 
419 
infinite series of matrices, 
601 
information retrieval, 
694 
initial distribution vector, 
860 
injections, 
515 
inner product, 
24, 25, 645, 646 
continuity of, 
649 
elliptical, 
646 
for matrices, 
82, 88 
geometric interpretation of, 
101 
linearity, 
26, 30, 54 
space, 
476, 645, 566 
inner pseudoinverse, 
566 
input-output economic model, 
812 
integer matrices, 
280, 954, 968 
integer multiplication, 
749 
integral formula for matrix functions, 
604 
integral of a matrix, 
605 
integral representations of inverses, 
615 
integral solution of AX+XB=C, 
337 
integration, linearity of, 
53 
interchange matrix, 
141 
interpolation formula for spectral projectors, 
551 
interpolation, Hermite, 
603 
interpolation, Lagrange, 
162 
intersection of subspaces 
dimension of, 
438, 453 
basis for, 
439 
projection onto, 
469, 472 
979 

980 
invariant subspaces, 
534, 536, 537, 592 
and block-diagonal matrices, 
536 
and block-triangular matrices, 
535 
and eigenspaces, 
538 
from Jordan chains, 
576 
invariants, similarity, 
533 
inverse, 
72 
and determinants, 
961 
as a polynomial, 
313 
by contour integration, 
607 
computation of, 
163 
continuity of, 
170, 962 
derivative of, 
172 
integral representation of, 
615 
iteration (inverse power method), 
of elementary matrices, 
138 
of discrete Fourier transform, 
728 
of a block matrix, 
166, 167 
perturbation of, 
176, 619 
power method, 
774 
one sided, 
159 
reverse order law, 
73 
generalized 
Drazin, 
557, 608, 558, 918, 927 
Moore-Penrose, 
192, 200, 356, 557, 558, 608, 615 
group, 
558, 559, 643, 918, 927 
inner, outer, reflexive, 
566 
uniqueness of, 
73 
inversion and transposition, 
73 
inversion, flop count, 
164 
invertible functions and operators, 
516, 517 
involutory matrix, 
109, 140, 968 
irreducible diagonal dominance, 
768 
irreducible Markov chain, summary, 
irreducible matrix, 
455, 808 
isometry, 
94 
isomorphic spaces, 
518 
iterative methods, 
625 
iterative refinement, 
279 
J 
Jacobi, Carl Gustav, 
627 682 
Jacobi's 
derivative of determinant, 
969 
diagonalization method, 
682 
iteration, 
626 
reduction, 
682 
Jacobi theorem, 
683 
Jacobian matrix, 
905 
and eigenvalues, 
930 
and trace, 
910 
Jordan blocks, 
540, 584 
functions of, 
596 
perturbed, 
613 
Jordan chains, 
539, 572, 573 
and invariant subspaces, 
576 
construction of, 
590 
independence in, 
541 
774 
866 
Jordan form, 
540 
. for general matrices, 
584, 585 
for nilpotent matrices, 571-577 
general theorem, 
585 
numerical considerations, 
589 
segment of, 
584 
similarity transformation, 
579 
structure, 
578, 583 
uniqueness of, 
578 
Jordan, Marie Ennemond Camille, 
283, 359 585 
Jordan, Wilhelm, 
283 
K 
kt? step distribution vector, 
860 
kt? step transition matrix, 
861 
Kaczmarz's projection method, 
477, 569 
Kahan, W.M, 
791 
Kantorovich, L. V., 
779 
Kaplansky, Irving, 
644 
Kato, Tosio, 
899 
Kenney, Allison C., Holly M, and Ryan P., 
xi 
Kepler, J., 
303 
kernel, 
203, 408 
Kirchhoff's rules, 
223, 449 
Knopp, Paul, 
896 
Kowa, Takakazu Seki, 
115, 283,939 
Kronecker product, 
227 
and FFT, 
755, 556 
and Laplace's equation, 
406 
and Laplacian, 
232 
eigenvalues of, 
321 
of exponentials, 
336 
properties of, 
228 
Kronecker, Leopold 
,_ 
xii, 227 
Krylov sequence, 
318 
Kublanovskaya, Vera N., 
779 
Kummer, Ernst Eduard, 
227 
Ky Fan, 
349, 350 
L 
Lagrange interpolation, 
162, 178, 498, 551 
Lagrange, Joseph-Louis, 
162, 405 
Lagrange's theorem on finite groups, 
827 
Langville, Amy, 
870 
Laplace, Pierre-Simon, 
2, 405, 658, 970 
Laplace's equation, 
629, 770 
and Kronecker product, 
406 
Laplace determinant expansion, 
970 
Laplacian, 
388 
and Kronecker product, 
232 
condition of, 
405 
eigenvalues of three-dimensional, 
407 
matrix for graphs, 
443 
largest eigenvalue, 
347 
latent semantic indexing, 
694 
latent values and vectors, 
287 
Index 

Index 
Laurent expansion, resolvent, 
920 
Laurent series, resolvent, 
918 
law of cosines, 
42 
law of nullity, 
438, 453 
LDL algorithm, symmetric, 
276 
LDL factorization, symmetric, 
271 
LDU factorization, 
271 
Le Verrier—Souriau-Frame algorithm, 
303 
Le Verrier, U. J. J., 
303 
leading principal submatrix, 
265 
leakage, spectral, 
733 
learning unsupervised machine, 
707 
least squares, 
283 
and Householder, 
685 
and orthogonal projection, 
486 
and QR, 
670 
polynomial, 
495 
solution, 
483, 487, 488 
total, 
481 
vector space theory, 
486 
why?, 
491 
Lebesgue, Henri, 
60 
left-hand 
coupling matrix, 
885, 886 
eigenpairs, 
299 
eigenvectors, 
288, 2289 301, 316, 594 
and projectors, 
550 
in inverses, 
320 
nullspace, 
431 
Legendre polynomials, 
665 
Legendre, Adrien-Marie, 
405, 695 
Legendre's differential equation, 
665 
Leibniz, Gottfried W., 
939 
length, 
22 
Leontief, Wassily, 
812 
Leontief's input-output model, 
812 
Leslie population model, 
833 
Leslie, P. H., 
834 
Lévy, L., 
766 
limits 
and group inversion, 
643 
and primitivity, 
820 
and spectral radius, 
617 
in irreducible Markov chains, summary, 
in reducible Markov chains, 
874 
of powers, 
631, 632 
of vector sequences, 
642 
limiting distribution, 
328, 637 
Lindemann, Carl Louis Ferdinand von, 
798 
linear 
algebra, 
definition of, 
1 
formal study of, 
505 
combination, 
15 
correlation, 
43 
equations, 
61 
estimation, 
491 
functions, 
52 
action of, 
510 
and subspaces, 
428 
composition of, 
511 
continuity of, 
909 
coordinates of, 
507 
defined by matrix multiplication, 
68 
derivative of, 
909 
space of, 
506 
functional, 
519, 653 
mapping, 
428 
model, 
493 
operators, 
505 
and and block matrices, 
565 
regression, 
489, 491 
spaces, 
430 
stationary iterations, 
625 
systems, 
61 
and LU factorization, 
262 
and linearity, 
54 
diagonally dominant, 
246 
independent solutions of, 
450 
perturbed, 
252 
sensitivity of, 
177 
updating, 
175 
linear transformation, 
505 
linearity, of inner products, 
26, 30 
linearity, of projectors, 
476 
linearly dependent, 
18 
linearly independent, 
18 
and rank, 
182 
and extensions, 
153 
and basic columns, 
151 
and complex conjugate vectors, 
185 
and maximal subsets of, 
417 
lines in R" not through the origin, 
476 
lines, projection onto, 
476 
Liouville, Joseph, 
765 
little-oh notation, 
932 
long-run dynamics, 
892 
loops in electrical circuits, 
222-224 
lower-triangular matrices, 
8 
products of, 
62 
LR iteration, 
796 
LSI, 
694 
981 

982 
LU factorization, 
154, 156, 259, 260, 669 
algorithm for, 
270 
existence of, 
266 
operation counts, 
263 
solving linear systems, 
262 
vs QR, 
669 
with row interchanges, 
268 
M 
M-matrix, 
622, 642, 813, 880 
machine learning, 
707 
magnitude of complex number, 
22 
main diagonal, 
8 
mapping, linear, 
428 
Markov chains, 
636, 639, 859 
absorbing, 
877 
aggregation, 
888 
censored, 
883 
periodic, 
871 
stochastic complement, 
882 
Markov property, 
859 
Markov, Andrei Andreevich, 
494, 859 
matrix, 
6 
addition, 
7 
coordinates, 
507 
diagonal, 
8 
equality, 
7 
equations, 
615 
AX+XB=C, 
337 
exponential, 
323, 325 
and differential equations, 
330, 609 
integration of, 
615 
inverse of, 
614 
Kronecker product of, 
336 
limits of, 
614 
products, 
336 
sums, 
614 
functions, 
324, 597, 
as polynomials, 
552, 602 
integral representation of, 
605 
interpolation formula for, 
551 
graph of, 
143 
group, 
558 
inverse, 
72 
flop count, 
164 
multiplication, 
56 
as a linear function, 
68 
by blocks, 
75 
properties of, 
67 
norms, 
84-86, 
l-norm and co-norm, 
86 
2-norm , 
363, 364 
continuity of, 
319 
Frobenius, 
82 
polynomials, 
300, 312 
sequences, convergence of, 
34, 38 
spaces, 
411 
triangular, 
127 
max, min of continuous functions, 
35 
maximal independent subset, 
417 
maximal nonsingular submatrix, 
187 
maximum, minimum principles, Ky Fan, 
mean, 
44 
measure, similarity, 
896 
metric, 
760 764 
Meyer, Bethany B. and Martin D., 
xi 
middle-run dynamics, 
892 
min-max theorem, 
349 
Minc, Henryk, 
830 
minimal angle, 
561 
minimal spanning set, 
417, 419 
349, 450 
minimax expressions for eigenvalues, 
425 468 
minimum norm solution, 
199 
and SVD, 
380 
sensitivity, 
379 
minimum variance estimator, 
492 
Minkowski's inequality, 
32, 38 
Minkowski inequality, 
38 
Minkowski, Hermann, 
38, 622,766 
minor determinant, 
946 
principal, 
383 
missile tracking, 
496 
modified gaussian elimination, 
129 
modified Gram—Schmidt, 
661 
modified matrix, eigenvalues of, 
322 
monotone matrix, 
802 
Index 
Moore—Penrose pseudoinverse, 
192, 200, 557, 558, 608 
and group inverse, 
643 
and normal matrices, 
356 
integral representation, 
615 
Moore, E. H., 
193 
Morrison, W. J., 
168 
mother-in-law rule, 
529 
multiple regression, 
489 
multiplication count in FFT, 
747 
multiplication 
of integers, 
749 
of matrices, 
56 
of polynomials and convolution, 
740 
scalar, 
5 
multiplicities, algebraic and geometric, 
multipliers in gaussian elimination, 
partial pivoting, 
240 
N 
near singularity, 
946 
nearest rank k matrix, 
365 
nearly completely decomposable, 
891 
nearly uncoupled, 
890 
negative definite, 
384, 394 
neighborhood, 
900 
Neptune, 
303 
Neumann series, 
170, 325, 618 
second Neumann series expansion, 
Neumann, Michael "Miki", 
849 
297, 307 
620, 923 
235, 238, 259 

Index 
Newton-Girard formulas, 
302 
Newton, Isaac, 
11, 302, 303 
Newton's Identities, 
302 
Newton's second law, 
386 
nilpotent matrices, 
136, 300, 305, 571 
Jordan form of, 
577, 578 
rank of and range of sum of, 
583 
standard form, 
571 
unitary similarity to, 
583 
nilpotent part, 
555 
nodes 
in electrical circuits, 
222, 223, 449 
in graphs, 
69, 143, 443 
inagrid, 
220 
noisy E chord, 
716 
DET ot. 
135 
nondiagonalizable matrices, 
305 
spectral radius of, 
319 
spectral resolution of, 
598 
nonhomogeneous algebraic systems, 
209-211, 213 
nonhomogeneous differential equations, 
610 
nonnegative matrices, 
797, 799 
spectral radius of, 
622 
nonnegative, essentially, 
802 
nonpositive matrix, 
819 
nonsingular matrices, 
72, 73 
and column and row independence, 
160 
and row independence, 
174 
and open sets, 
910 
as a product of elementary, 
165 
determinants of, 
945 
rank of, 
180 
reduction of, 
159 
sequences of, 
378 
nonsingular systems, 
75 
norm, 
22, 23, 34, 37, 84 
ik Fehavell eeky 
S35 tele) 
compatibility, 
83, 84 
continuity, 
35 
equivalent, 
35 
euclidean, 
23 
Frobenius, 
82 
Hilbert-Schmidt, 
381 
induced, 
85, 90 
l-norm and co-norm, 
86 
2-norm, 
363, 364 
and singular values, 
380 
matrix vs vector, 
380 
of a function, 
648 
of rank-one matrices, 
378 
of projectors, 
563 
of operators, 
85 
of a waveform, 
751 
Schatten p-norm, 
381 
normal equations, 
199, 218, 283, 483, 670 
and SVD, 
380 
sensitivity of, 
246 
normal matrices, 
339, 468, 475 
eigenspaces of, 
341 
functions of, 
343 
norm of, 
345 
spectral theorem for, 
341 
triangular matrix, 
normal modes of vibration, 
340 
normal operator, 
653 
normalization, 
23 
of eigenvectors, 
2 
nullity, 
429 
of a product, 
438 
nullspace, 
203, 430, 
91 
431 
387, 401, 402, 770 
and rank and independent columns, 
209 
basis for, 
203, 430, 434 
dimension of, 
433 
independent spanning set for, 
of AA* and A*A, 
209, 435 
of functions, 
428 
of products, 
437, 438, 435 
orthonormal spanning set for, 
numerical computation, Jordan form, 
589 
numerical rank, 
369, 370 
numerical stability, 
O 
oblique projection, 
678 
544, 546, 
method for linear systems, 
569 
oh notation, 
125, 221, 932 
Ohm's law, 
223 
one-to-one, 
515, 516 
onto, 
51/5; 516 
open set, 
900, 910 
operation counts, 
135 
for convolution, 
752 
for Gauss—Jordan elimination, 
for gaussian elimination, 
125 
for matrix inversion, 
164 
for LU factorizatio n, 
263 
operator, linear, 
505 
norm, 
8) 
idempotent, 
542 
invertible, 
517 
restricted, 
534 
order notation, 
125 
order notation, 
932 
, 932 
originating paths, 
823 
orthogonal complement, 
206 
207 
126 
102, 454, 456, 459, 553 
orthogonal decomposition theorem, 
553 
orthogona! diagonalization, 
346 
orthogonal distance, 
466, 479 
orthogonal eigenvectors, 
292 
orthogonal expansion, 
687 
orthogonal matrix, 
41, 92, 954 
983 

984 
orthogonal projections, 
102, 457, 476, 525, 545, 687 
and least squares, 
486 
and three-dimensional graphics, 
111 
and pseudoinverse, 
464 
and URV factors, 
463 
coordinates of, 
508 
elementary, 
101, 140 
onto an intersection, 
472 
pairs of, 
469 
product of, 
475 
properties of, 
464 
sum of, 
475 
orthogonal reduction by plane rotations, 
675 
orthogonal set, 
41 
orthogonal similarity, 
339 
orthogonal triangularization, 
671, 673 
orthogonal vectors, 
39 
orthogonality, left and right eigenvectors, 
594 
orthogonality, 
648 
orthonormal basis 
and Fourier expansions, 
687 
and orthogonal reduction, 
685 
extending to, 
100, 106 
for fundamental subspaces, 
436 
spanning set for nullspace, 
207 
orthonormal set, 
40, 100, 106, 658 
rows or columns, 
41 
orthonormality and independence, 
40, 50 
oscillations, spring-mass, 
401, 402 
Osgood, William F., 
908 
out-degree, 
819 
outer product, 
76, 191 
outer pseudoinverse, 
566 
over relaxation, 
629 
P 
p-norm, 
32, 651 
Page, Larry, 
866 
PageRank, 
866, 869 
parallel sum, 
473 
parallelepiped, 
667, 950 
parallelogram identity, 
30, 91, 649, 651, 655, 657 
parallelogram law, 
5, 413 
parity of a permutation, 
941 
Parseval, Marc-Antoine, 
688 
Parseval's identity, 
688 
partial derivative, 
904, 906 
of a simple eigenvalue, 
929 
of spectral projectors, 
928 
partial differential equations, 
388 
partial pivoting, 
237 
and diagonal dominance, 
246 
numerical stability, 
679 
partial resolvent, 
921 
particular solution, 
205, 210 211 
partition, of a graph, 
447, 455 
partition, sum of squares, 
483 
partitioned matrix, 
75 
- 
and linear operators, 
565 
partitioning a data cloud, 
708 
paths in graphs, 
823, 835 
Paz, Azaria, 
37 
Paz's inequality, 
37 
PCA, 
697 
Peano, Giuseppe, 
409 
Pearson correlation coefficient, 
46 
Pearson, Karl, 
697 
Penrose conditions, 
463 
Penrose equations, 
193, 199, 379 
Penrose, Roger, 
193 
perfect shuffles, 
745 
period of a matrix, 
820 
periodic matrices, 
820, 825, 828 
periodic functions, 
690, 691 
periodic graphs, adjacency matrix, 
824 
periodic Markov chains, 
871 
permutation counter, 
269 
permutation matrices, 
41, 142, 267 
permutations, 
and bit reversal, 
755 
in determinants, 
940 
symmetric, 
143, 808 
perpendicular vectors, 
39 
Perron complement, 
851 
Perron properties, inherited, 
853 
Perron root, 
800, 804 
continuity of, 
815, 816 
derivatives of, 
840, 846, 847 
increasing, 
818 
number of paths ina graph, 
823 
Perron vector, 
800, 804, 809 
derivatives of, , 
842, 843, 844, 849 
number of paths in a graph, 
823 
Perron—Frobenius theorem, 
809 
Perron—Frobenius theory, 
797 
and Wielandt, 
774 
Perron, Oskar, 
798 
Perron's theorem, 
804 
personalization vector, 
869 
perturbations 
and rank, 
366 
by rank-one matrices, 
441 
of eigenvalues, 
329, 759, 931, 933, 934 
of eigenvectors, 
934 
of inverses, 
176, 619 
of linear systems, 
249, 252, 256 
of Jordan blocks, 
613 
of resolvents, 
620 
of singular values, 
368 
Piazzi, Giuseppe, 
499 
Picone, Mauro Picone, 
479 
pivot element, 
121 
in partial pivoting, 
237 
columns, 
147 
equation, 
121 
variables, 
204 
Index 

Index 
pivoting 
complete, 
242 
partial, 
237 
symmetric, 
278 
pivots 
and conditioning, 
381 
determinant formula for, 
280 
in echelon form, 
128 
unique number, 
151 
plane rotations, 
98, 668,671 
Plemmons, Robert J., 
625 
Poisson, Siméon D., 
405 
Poisson's equation, 
405, 388 
polar decomposition, 
392 
polarization identity, 
657 
polynomials 
multiplication and convolution of, 
740 
of matrices, 
300, 312 
of matrix functions, 
602 
representation of A~!, 
313 
representation of Drazin inverse, 
567 
roots and continuity of, 
918 
population correlation coefficient, 
47 
population distribution, 
328 
population migration, 
327, 774 
population model, Leslie, 
833 
population variance-covariance matrix, 
701 
positive definite matrices, 
273, 281, 382, 383, 770 
condition number for, 
405 
maximum entry, 
281 
Schur complement of, 
404 
positive definite form, 
393 
positive matrices, 
797, 804 
positive orientation, 
604 
positive semidefinite matrices, 
382, 384 
square roots of, 
392 
power method, 
772 
power spectral density and estimation, 
733 
powers 
of a matrix, 
68, 69 
limits of, 
326, 631, 632 
ofasum, 
69 
precision, 
233 
preconditioned conjugate gradient, 
391 
predator-prey model, 
334 
predicted vector, 
483 
primitive matrices, 
820, 821 
primitivity, sufficient conditions for, 
828 
principal axes, 
705, 706 
principal component analysis, 
697 
and regression, 
702 
and principal directions, 
706 
principal 
gap and partition, 
709, 710, 714 
hyperplane, 
713 
minors, 
293, 647 
in M-matrices, 
642 
nonnegative and positive, 
383, 384 
positive, 
383 
projected gap, 
709 
submatrices, 
265, 281, 293 
leading, 
265 
in stochastic matrices, 
880 
subspaces, 
706 
trend line, 
703, 706 
probability distribution vector, 
860 
censored, 
885 
products 
nullity of, 
438, 453 
of matrices, 
56 
characteristic equation of, 
303 
eigenvalues of, 
302 
elementary, 
138 
nonsingular, 
73 
of orthogonal projectors, 
474 
rule for determinants, 
947 
range and nullspace of, 
435 
rank of, 
192, 436 
square root of, 
79 
trace of, 
78 
projection, 
63, 648 
affine, 
458 
and Fourier expansion, 
712 
method for solving linear systems, 
oblique, 
544 
onto a hyperplane, 
568 
orthogonal, 
457 
onto a hyperplane, 
477 
onto an intersection, 
469 
onto aline, 
101, 476 
projectors, 
505, 546, 547, 562, 640 
and full-rank factorization, 
640 
and pseudoinverse, 
464 
and URV factors, 
463 
complementary, 
547 
coordinates of, 
508 
difference of, 
566 
elementary, 
140 
norm of, 
5615 563 
oblique, 
546 
orthogonal, 
457, 464 
product of, 
475, 566 
rank and trace of, 
547 
spectral, 
449, 598 
sum of, 
453, 565 
477, 569 
985 

986 
prolongation operator, 
856 
proof, end of, 
9 
proper values and vectors, 
287 
pseudoinverses, 
Drazin, 
557, 558, 918, 927 
asa polynomial, 
567 
as a contour intergral, 
608 
as a standard integral, 
615 
equality with Moore—Penrose pseudoinverse, 
group, 
558, 559, 918, 927 
and derivatives, 
928 
and limits, 
643 
in terms of Moore—Penrose inverse, 
from full-rank factorization, 
643 
inner, outer, reflexive, 
566 
Moore—Penrose, 
192, 199, 656 
and least squares, 
488 
and normal matrices, 
and SVD, 
379 
and unitary, 
200 
asa limit, 
356 
basic properties, 
continuity, 
379 
basic properties, 
integral representation, 
uniqueness of, 
193 
pure imaginary, 
300 
Pythagorean theorem, 
and closest point, 
generalized, 
648 
Q 
QR factorization, 
666 
and Hessenberg matrices, 
and least squares, 
670 
and volume, 
667, 668 
uniqueness of, 
685 
vs LU factorization, 
QR iteration, 
779 
symmetric, 
786 
and complex eigenvalues, 
393-396 
643 
356 
194, 199 
194, 199 
615 
50, 656 
466 
682 
669 
781 
quadratic forms, 
Hl 
695 
quod erat demonstrandum, 
9 
quaternions, 
query vector, 
R 
280 
random variable, standardized, 
random walk, 
639 
onagraph, 
824 
range decomposition, semidefinite matrices, 
random integer matrices, 
43, 44 
568 
558 
Index 
range, 
428, 431, 434 
basis for, 
430, 432 
of A*A and AA*, 
435 
of a product, 
435, 437 
of asum, nilpotents, 
439, 453, 583 
dimension of, 
433 
partitioned matrix, 
453 
range-hermitian matrices, 
467 
range-nullspace decomposition, 
554 
rank, 
180-183 
and column operations, 
and consistency, 
202 
and determinants, 
947 
and equivalence, 
186 
and graph connectivity, 
and nullspace, 
209 
and perturbations, 
366 
and row operations, 
180 
and Schur complement, 
and similarity invariants, 
and submatrices, 
187, 198 
and transposition, 
184 
numerical, 
368-370 
of A*A and AA*, 
of block diagonal, 
198 
of differences, 
439 
of idempotents, 
452 
of nonsingular matrices, 
of partitioned matrices, 
of product, lower bound, 
of projector, 
547 
of rank-one perturbation, 
185 
445 
198 
533 
195 
180 
453 
187, 192, 436, 438, 452 
441 
ofsum, 
198, 439, 452, 583 
of triple product, 
197 
rank normal form, 
158 
rank plus nullity, 
429, 433, 474 
theorem, 
524 
rank-one matrices, 
191 
diagonalizability, 
320 
eigenvalues, 
299 
singular value, 
378 
sum of, 
452 
rank-one update 
determinant of, 
956 
eigenvalues of, 
321, 322, 351 
eigenvectors of, 
353 
rank of, 
441 
Sherman—Morrison formula for, 
rank-trace relationship, 
357 
Rayleigh quotient, 
347, 349, 356, 775 
iteration, 
775 
168 
Fiedler value, 
455 
Rayleigh, Lord, 
349 
real eigenvalues, 
291 
simple and perturbations of, 
real numbers, 
2 
real orthogonal matrices, eigenvalues of, 
realizations, 
43 
rectangular matrix, 
6 
760, 917 
357 

Index 
QR factorization, 
666 
system, 
61 
reduced row-echelon form, 
129, 152 
reducible Markov chain, limits, 
871, 874 
reducible matrix, 
455, 808 
canonical form, 
872 
refinement, iterative, 
279 
reflection, 
63, 104 
about a hyperplane, 
479 
elementary, 
671 
method for solving linear systems, 
479 
reflectors, 
104 479, 505 
complex, 
106 
constructing, 
110 
elementary, 
103, 140 
determinant of, 
968 
reflexive pseudoinverse, 
566 
reflexivity, 
145, 522 
regression, linear, 
491,482, 489 
regression, principal components, 
702 
Reinsch, Christian, 
791 
relative equilibrium, 
892 
relative error, of inverses, 
619 
relative magnitude, 
171 
relative uncertainty or error, 
372, 473 
relaxation parameter, 
480, 629 
representation in terms of a basis, 
418 
residual sum of squares, 
483 
residuals, 
249, 376 
resolvent matrix, 
90, 605, 606, 620, 799, 911 
analyticity of, 
912 
continuity of, 
913 
existence of, 
911 
Laurent expansion of, 
918, 920 
perturbation of, 
620 
total and partial, 
921 
restricted operator, 
534, 565, 856 
reversal matrix, 
318, 593 
reverse order law 
for inversion, 
73, 74 
for transposition, 
71 
reverse Simon—Ando, 
896 
reversing binary bits, 
745 
reversing elementary operations, 
145 
Richardson iterative method, 
627 
Riesz representation theorem, 
654 
Riesz—Fischer theorem, 
424 
right-hand rule, 
113 
robotics, 
96 
Roosevelt, Theodore, 
408 
roots of unity, 
718 
in periodic and imprimitive matrices, 
826 
roots, continuity of, 
917 
Rose, Nicholas J, 
118 
Rossi, Hugo, 
900, 908 
rotation, 
63 
determinant of, 
968 
in R® and R", 
94, 95, 98 
Givens, 
671 
plane, 
98, 671 
rotators, 
94, 541, 505 
Rouché's theorem, 
757 
round-off errors, 
233-235, 677 
in using gaussian elimination, 
253 
row equivalence, 
145, 152, 166 
row independence and nonsingularity, 
174 
row interchanges in LU, 
268 
row operations and rank, 
180 
row operations, 
123, 136 
row scaling, 
241 
row space, 
20, 431, 432 
row stochastic, 
636 
row vectors, 
3 
row-by-column multiplication, 
25 
row-echelon form, 
128, 180 
reduced, 
129 
RPN matrix, 
467, 468,475 
Rudin, Walter, 
900 
Ruffini, Paolo, 
765 
Rutishauser, Heinz, 
779, 796 
Ss 
Sagan, Hans, 
900 
sample correlation coefficient, 
46, 48 
sample covariance, 
47, 701 
sample of a signal, 
717 
samples, 
43 
saw-toothed function, 
712 
scalar multiplication, 
5, 7,12 
scalar product, 
24 
scale, 
241 
scaling a linear system, 
241 
scaling, in 3-dimensional graphics, 
113 
scaling, row and column, 
241 
Schatten p-norm, 
381 
Scheidemann, V., 
900 
Schmidt, Erhard, 
658, 774 
Schur complement, 
166, 167, 367, 956 
and rank, 
198 
condition number, 
405 
positive definite, 
404, 405 
Schur, Issai, 
167, 306, 789 
Schur's triangularization theorem, 
306 
Schwarz, Hermann A., 
27, 658 
Schwerdtfeger, Hans, 
467 
second Neumann series, 
620, 923 
secondary trend lines, 
706 
sectionally continuous, 
690 
secular equation, 
352 
Seidel, Ludwig, 
627 
self-adjoint operator, 
653 
semi-stable, 
334 
987 

988 
semidefinite, range decomposition, 
568 
semidefinite matrix, 
382, 384 
semisimple eigenvalue, 
297, 588, 590, 593 
Seneta, Eugene, 
494 
sensitivity of 
eigenvalues, 
761, 921, 932 
eigenvectors, 
934, 936 
inverses, 
176, 619 
linear systems, 
177, 252 
minimum norm solution, 
379 
sequences, 
34, 88, 378 
Cesaro, 
633 
limits of, 
642 
series for f(A), 
601 
set metric, 
760 
Shabat, Boris V., 
900 
shape, 
6 
shell game, 
636, 871 
Sherman—Morrison formula, 
168, 175 
Sherman, J., 
168 
shifts in QR iteration, 
781, 782, 788 
short-run dynamics, 
891 
short-run stabilization interval, 
892 
SIAM, 
99, 104 
sign of a permutation, 
941 
signal analysis, 
730 
signal filtering, 
735 
signal, sample, 
717 
similarity, 
304, 530, 531 
and change of basis, 
529, 530 
and determinants, 
953 
and transpose, 
593 
invariants, 
304, 532, 533 
orthogonal, 
346 
transitivity, 
541 
unitary, 
339 
similarity measure and matrix in clustering, 
Simon—Ando theory, 
890 
applied to clustering, 
895, 896 
Simon, Herbert A., 
890 
simple eigenvalues, 
297 
continuity of, 
759, 917 
differentiability of, 
924 
perturbations of, 
760, 917, 933 
simple eigenvectors, 
933 
simple graphs, 
824 
loops, 
224 
path, 
835 
cycle, 
835 
simultaneous diagonalization, 
321, 399 
simultaneous displacements, 
627 
simultaneous triangularization, 
321 
sine, discrete, 
720 
sine, frequency coordinates, 
724 
single shift QR, 
781, 784 
singular differential equations, 
615 
896 
Index 
singular matrices, 
72 
groups of, 
558 
sequences of, 
378 
singular value decomposition, 
358, 378, 462, 
as a Fourier expansion, 
691 
computation of, 
791 
non-uniqueness, 
377 
singular values, 
358 
and determinants, 
954 
and full rank-factorization, 
640 
smallest, 
379 
variational description of, 
381 
singular vectors, 
360-362, 463 
singularity, distance to, 
366 
Sinkhorn—Knopp algorithm, 
896 
Sinkhorn, Richard, 
896 
sinusoidal, 
716 
skew hermitian matrices, 
14, 346 
skew symmetric matrices, 
10, 14, 218, 954 
eigenvalues of, 
300, 346 
skew-centrosymmetric, 
318 
skew-hermitian exponential, 
337 
smallest eigenvalue, 
347 
smallest singular value, 
379 
solution, linear systems, derivative of, 
177 
solutions, difference equations, 
616 
solutions, differential equations, 
331, 610 
SOR iteration, 
626 
finite difference matrix, 
642 
Souriau, J. M., 
303 
spanning set, 
16, 205, 415 
for nullspace, 
206 
for range, 
430 
minimal, 
417, 419 
orthonormal, 
207 
testing, 
432 
sparse least squares, 
502 
sparse matrix, 
677 
special linear group, 
558 
spectral circle, imprimitive and periodic matrices, 
826 
spectral clustering, 
448 
spectral decomposition of variance, 
700 
spectral decomposition, Fourier expansion, 
693 
spectral graph partitioning, 
448 
spectral leakage, 
733 
spectral mapping property, 
336, 613 
spectral norm, 
363 
spectral projectors, 
341, 549, 598, 640 
analyticity and differentiability of, 
923 
and eigenvectors, 
550 
commuting property, 
356, 564 
continuity of, 
914 
derivative of, 
843, 928 
generalized, 
557 
integral representation, 
605 
interpolation formula for, 
551 
sum of, 
565, 604 

Index 
spectral radius, 
297, 319, 617, 765, 
as alimit, 
617, 621, 822 
bounds for, 
621 
Collatz—-Wielandt formula for, 
805, 817 
for nondiagonalizable matrices, 
319 
for nonnegative matrices, 
622 
spectral representation of matrix functions, 
324 
spectral resolution of f(A), 
598 
spectral theorem for diagonalizable matrices, 
549 
spectral theorem for normal matrices, 
341 
spectrum, 
288 
of a projector, 
547 
of imprimitive and periodic matrices, 
828 
sphere, distortion of, 
370 
spheres, 
33 
splitting semidefinite matrices, 
568 
splitting, 
625 
spread of eigenvalues, 
354 
spring-mass oscillations, 
401, 402 
square matrix, 
6 
square root, unique positive semidefinite, 
392 
square root, 
79 
Square system, 
61 
square wave function, 
690 
stability, differential equations, 
333 
stable algorithm, 
239, 368, 370, 678 
stable differential equations, 
610 
stable electronic circuit, 
337 
stable matrix, 
334, 614, 615 
stable system, 
334, 610 
standard basis, 
420, 506, 900 
standard coordinates, 
506 
standard deviation, 
44 
standard inner product, 
24, 82, 88 
standard nilpotent form, 
571 
standard scores, 
44 
standardization of data, 
44 
stationary distribution vector, 
866 
stationary Markov chain, 
860 
steady-state distribution, 
637 
steady-state, 
329 
step size, 
221 
stiffness constant, 
11 
stiffness matrix, 
12 
stochastic complement, 
882 
stochastic matrices, 
636 814, 859, 637 
doubly, 
879 
stochastization, 
814 
Stoker, James J., 
424 
strictly upper triangular, 
305 
strongly connected graph, 
455, 808 
Strutt, John W., 
349 
stuff, 
420 
submatrices, 
7, 75 
and rank, 
187 
subset, 
412 
989 
subspaces, 
16, 412 
addition of, 
421 
created by linear functions, 
428 
dimension of, 
420 
invariant, 
534, 536 
image of, 
451, 475 
projection on intersections of, 
469 
spanning sets for, 
415 
substochastic matrix, 
880 
subtraction, floating-point, 
235 
successive displacements, 
628 
successive over relaxation iteration, 
626, 629 
sum decomposition of semidefinite matrices, 
568 
sum of 
matrices, 
7 
projectors, 
453, 475 
spectral projectors, 
604 
squares, partitioned, 
483 
subspaces, 
421, 422, 544 
dimension of , 
422 
ranks, 
439 
rank-one matrices, 
452 
summability, Cesaro, 
632 
summability, 
635 
summable matrix, 
633 
surjections, 
515 
SVD, 
358 
and full-rank factorization, 
640 
and pseudoinverse, 
379 
and URV, 
462 
and fundamental subspaces, 
436 
as Fourier expansion, 
691 
computation of, 
791 
coordinate change, 
528 
Sylvester, James J.. 
117, 180, 359 
Sylvester's law of inertia, 
397 
Sylvester's law of nullity, 
438, 353 
symmetric function, 
293 
symmetric matrices, 
10, 468, 475, 
diagonalization of, 
346 
eigen-components of, 
346 
eigenvalues of, 
291 
eigenvectors of, 
292 
extremal eigenvalues, 
347 
reduction to tridiagonal form, 
681 
symmetric permutation, 
143, 808 
symmetric pivoting, 
278 
symmetric QR iteration, 
786 
symmetric, skew, 
346 
symmetries, 
10 
symmetry, in equivalence, 
145 
system of linear equations, 
61 
systems, differential equations, 
330 
T 
¢digit arithmetic, 
233 
Taub, Abraham, 
791 
Taussky-Todd, Olga, 
167, 766, 768 

990 
Taylor expansion, 
776 
Taylor series, 
220, 595 
Taylor's theorem, 
394 
teleportation matrix, 
869 
tensor product, 
227 
in FFT, 
755 
Laplacian, 
232 
term-by-document matrix, 
695 
terminating paths, 
823 
text mining, 
694 
three-dimensional rotations, 
96, 111 
time basis, 
717, 718 
time domain, 
722 
Todd, John, 
766 
Toeplitz matrices, 
314, 771, 708 
Toeplitz, Otto, 
314 
Tolstoy, Leo, 
796 
total derivative, 
901, 910 
total least squares, 
481 
total resolvent, 
921 
total sum of squares, 
483 
total variation, 
700 
trace, 
77, 78, 304 
and characteristic equation, 
303 
and spectral radius, 
822 
and sums of eigenvalues, 
294 
as a similarity invariant, 
533 
derivative of, 
910 
inequalities, 
90, 656 
of a product, 
78 
and eigenvalues, 
303 
of a projector, 
547 
of idempotents, 
452 
of imprimitive and periodic matrices, 
of powers, and characteristic equation, 
trace-rank relationship, 
357 
transform, discrete Fourier, 
727 
transformation, linear, 
505 
transient class, 
872, 876 
transition diagram, 
327 
transition matrix, 
328, 824,861 
transition probability, 
860 
transitivity, 
145 
for similarity, 
541 
transposition, 
3, 8, 9 
and determinants, 
943 
and inversion, 
73 
and linearity of, 
53 
and rank, 
184 
and similarity, 
593 
reverse order law, 
71 
trapezoidal form, 
673 
trend of observations, 
498 
triangle inequality, 
28, 31 
backward, 
29, 34 
828 
302 
Index 
triangular matrices, 
8, 62 
- 
block, 
77 
determinants of, 
942 
eigenvalues of, 
292 
elementary, 
260 
inverses of, 
163 
normal, 
340 
unitary, 
50, 357 
triangular system, 
122 
triangularization 
orthogonal, 
671-673 
short history of, 
283 
simultaneous, 
321 
Schur's theorem, 
306 
using gaussian elimination, 
122-125, 140, 259 
and floating-point arithmetic, 
233-243 
tridiagonal matrices, 
222, 282, 681, 781, 786 
toeplitz matrices, 
314 
trivial solution, 
18, 203,212 
trivial subspace, 
17, 412, 419 
Tukey, John. W., 
742, 748 
Twain, Mark, 
118 
two-point boundary value problem, 
220 
U 
unbiased estimator, variance, 
504,492 
uncertainties in linear systems, 
372 
uncorrelated, 
698 
under relaxation, 
629 
undirected graph, 
143 
uniform continuity, 
901 
unique representations, 
418 
unique solution 
for differential equations, 
331 
for homogeneous systems, 
209 
uniqueness of 
inverses, 
73 
nonhomogeneous solutions, 
212 
solutions in nonsingular systems, 
75 
singular vectors, 
362 
unit ball, 
33 
unit columns in echelon forms, 
130 
unit eigenvalues for stochastic matrix, 
636 
unit sphere, 
33, 370, 381 
unit vectors, 
4, 61 
unitarily invariant norm, 
93 
unitarily similarity, nilpotent matrices, 
583 
unitary matrices, 
41, 92 
Cayley transformation, 
346 
determinant of, 
954 
eigenvalues of, 
357 
pseudoinverse of, 
200 
skew-symmetric and matrix exponential, 
337 
triangular, 
50 
unitary operator, 
653, 655 
unitary similarity, 
339 
unity, roots of, 
718 
University of Virginia, 
117 

Index 
unreduced Hessenberg form, 
680, 783,795 
unstable matrix, 
334 
unsupervised learning, 
707 
update, rank-one, 
441 
updated matrix, eigenvalues of, 
322 
updating, linear systems, 
175 
upper triangular matrix, 
8, 62 
upper-trapezoidal form, 
673 
URV factorization, 
460, 461, 685 
and coordinate change, 
528 
Vi 
Van Loan, Charles F., 
233,791 
Vandermonde matrices, 
60, 496, 727 
Vandermonde determinant, 
969 
Vandermonde, Alexandre-Theophile, 
60 
Varga, Richard S., 
766 
variables, free and basic 
204 
variance-covariance matrix, 
48, , 698, 701, 
variance, 
44, 493, 698 
directional, 
704 
unbiased estimate, 
504 
variation, total, 
700 
variational descriptions 
for eigenvalues, 
348, 350, 425, 468 
for singular values, 
381 
vec operation, 
229 
vector 
3, 410 
vector addition, 
5 
vector coordinates, 
418, 419, 506 
change of, 
419, 514 
vector norms, 
23, 32, 34 
vector sequence, convergence of, 
34 
vector space, 
410 
vertex matrix, 
98 
vibrations 
small, 
385, 770 
spring-mass, 
401, 402 
visualization of data, 
706 
volume 
and determinants, 
668, 950 
and Gram—Schmidt and QR factorization, 
von Mises, Richard, 
772 
von Neumann, John, 
viii, 286, 649, 756 
von Neumann's alternating projection formula, 
W, X, Y, Z 
weak eigenvalue continuity, 
761, 921 
Weierstrass, Karl, 
585, 798 
weighted graphs, 
819 
weighting function, 
646 
well conditioned, 
373 
Wessell, Charles D, 
896 
Weyl, Hermann, 
409 
Who's #17, 
870 
why least squares?, 
491 
Wielandt, Helmut, 
774, 805, 821, 825 
Wielandt's theorem, 
824-826 
wild-card symbol, 
7 
Wilkinson matrix, 
239, 679 
Wilkinson shift, 
786 
Wilkinson, James H., 
286, 786 
wire frame figure, 
111 
wire-frame representation, 
98 
Woodbury, M., 
168 
Wronski matrix, 
182, 183, 197 
Wronski, Jozef M., 
182 
Wronskian determinant, 
955, 969 
Wronskians and differential equations, 
963 
Young, David M., 
630 
Young, G., 
359 
Z-matrix, 
625, 642 
z-scores, 
44 
Zeeman, Hrik C., 
756 
zero transformation, 
505 
zero vector, 
410 
991 

3) 
° 2 jes 
= 
. 
1%. 
wiley Guin Ted 
_ 
eile 
oles el 
oe 
= 
wiht) ey 
® 
0 
mw 
a 
* 
f 
"™ 
>= 
+' 
s 



Zz 
ee 
OT 



This second edition has been almost completely rewritten to create a textbook that is flexible 
enough for a one- or two-semester course. The author achieves this by increasing the level 
of sophistication as the text proceeds from traditional first principles in the early chapters to 
deeper theory and applications in the later ones, and by ensuring that material at any point is 
not dependent on subsequent developments. While theorems and proofs are highlighted, the 
emphasis is on applications and the text is designed so instructors can determine the degree of 
rigor. 
Matrix Analysis and Applied Linear Algebra, Second Edition contains 
* carefully constructed exercises ranging from easy to moderately challenging to difficult, 
many of which condition students for topics that follow; 
and 
historical remarks that focus on the personalities of the individuals who created and 
contributed to the subject's development. 
This textbook i is designed for use in either a one- or two-term course. It can also serve as a 
reference to anyone who needs to use or apply linear algebra. 
~ .
Carl D. Meyer is Emeritus Professor of Mathematics at North Carolina State 
J 
~ 
University, and he is a SIAM Fellow. 
For more information about SIAM books, journals, 
conferences, memberships, or activities, contact: 
W 
Sle 
; 
ait 
® 
Society for Industrial and Applied Mathematics 
3600 Market Street, 6th Floor 
Philadelphia, PA 19104-2688 USA 
+1-215-382-9800 
siam@siam.org * www.siam.org 
Hl 
ISBN: 978-1 
1-61 197-743-1 
| 
| 
ili 
| 
9 seaGrs 977431 
21S-94
CO
Ml 


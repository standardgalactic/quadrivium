
Table of Contents
Cover
Table of Contents
Title Page
Foreword
Reader Support for This Book
Part I: Introduction
CHAPTER 1: Why Another Book About Testing?
Many Cultures of Testing
Why Us?
Why Testing?
Why Testers?
Why Not "Traditional Testing?"
Notes
Part II: Rapid Software Testing Methodology
CHAPTER 2: Foundation
The Meaning of Testing
Our Vision for RST
Design Features
Foundational Ideas
Notes
CHAPTER 3: How to Do a Test
The Process of Testing
The Anatomy of a Check
The Anatomy of a Test
The Art of Bug Reporting
The Bug Pipeline

Notes
CHAPTER 4: How to Do a Test Strategy
The Structure of Test Strategy
First of All Things: Center Yourself
Developing a Strategy Is an Ongoing Exploration
Twelve Test Strategy Entry Points
Seasons of Testing: Strategy Throughout the Project Cycle
The Challenge of Alignment: Strategy Is Different for
Testers and Builders
Practices Worth Practicing
Notes
CHAPTER 5: How to Account for Your Work
Telling a Compelling Story
Stories Mediate Software Projects
Artifact-based vs. Activity-based Management
Please Don't Fake Your Testing
Notes
CHAPTER 6: How to Apply Automation in Testing
Robots! Help!
The Trouble with "Automation"
Classic Traps of Automation in Testing
Considerations for GUI-level Automation
Considerations for Adopting a Tool
The Golden Rule of Tool Adoption
Notes
CHAPTER 7: How to Approach AI and Testing
What If We Could Test by Magic?
Notes
Part III: Application and Customization

CHAPTER 8: Prospective Testing
Notice Something Before We Say Anything More ...
But ... Why Bother Doing This at All?
How to Do Prospective Testing
A Cheat Sheet for Prospective Testing
Notes
CHAPTER 9: Test Reporting (Without Pretentious Metrics)
Notes
CHAPTER 10: Working with Quality Characteristics
Developing the Model
Using the Model
Epilogue
Notes
CHAPTER 11: Adventures in Testability
CHAPTER 12: Mindopsy: Dissecting My Thinking as a Tester
Notes
CHAPTER 13: Rapid Usability Testing
Notes
CHAPTER 14: RST Meets Signals-Based Testing
Signals for Testing
How This Looked at Microsoft
Analyzing the Signals and Looking for Failure
Getting Started
How Does Signals-Based Testing Fit with RST?
Notes
CHAPTER 15: Deliberate Practice of Testing Skills
Note
Part IV: Context and Culture
CHAPTER 16: Lessons of the Post Office Horizon Scandal

The Post Office Horizon Scandal
How Bad Was Horizon?
The Significance for Testers of the Horizon Issues
Why RST Appeals to Me
Notes
CHAPTER 17: Sociology and Software Testing
Notes
CHAPTER 18: How Testing Looks to Management
Introduction
Interview
Notes
CHAPTER 19: Technical Work Is All About Learning
Learning through Play
Learning the Importance of the Business Side
Learning to Be Helpful in More Than One Way as a Tester
Learning That a "Quality" Process Can Interfere with
Software Testing
Learning to Speak and Persuade
Learning Forever
CHAPTER 20: From RST to AST
The Start of My Testing Career
How Experiencing RST Led to My Passion for Testing
Becoming Involved in the CDT Community
Building a Close Working Relationship with the AST
CHAPTER 21: Notes on a Seeker's Journey into Testing
"I Trust I Make Myself Obscure"
Notes
CHAPTER 22: Becoming an RST Instructor
From Development to Management to Training

Years of Trying to Be Normal
I Found Context-Driven Testing
Instructor in the Making
The Key to Mastery Is Deliberate Practice
Why I Prefer the Context-Driven Approach to Testing
Notes
Part V: Appendices
APPENDIX A: Roadmap of Testing
APPENDIX B: The Heuristic Test Strategy Model
General Test Techniques
Project Environment
Product Elements
Quality Criteria Categories
Note
APPENDIX C: Heuristics of Testability
Interesting Testability Dynamics
Guidewords for Analyzing Testability
Notes
APPENDIX D: Elements of Excellent Testing
Evolving Work Products
Testing Skills and Tactics
About the Contributors
Index
Copyright
Dedication
About the Authors
Acknowledgments
End User License Agreement

List of Illustrations
Chapter 2
Figure 2-1: Depth vs. Breadth in Testing
Figure 2-2: Parnism: methodological imperialism
Figure 2-3: The formality continuum
Figure 2-4: Formality with deliberation
Figure 2-5: Levels of agency in organizations
Figure 2-6: Not all metrics constitute good evidence.
Chapter 3
Figure 3-1: The iceberg of testing work
Figure 3-2: Basic logic of one step of a check
Figure 3-3: More accurate logic of a check
Figure 3-4: Chain of causality
Figure 3-5: Obscured chain of causality
Figure 3-6: Multi-threaded chain of causality
Figure 3-7: The product as an island
Figure 3-8: Seeing what happens at the end of the world
Figure 3-9: Oracles take different forms.
Figure 3-10: A single step of a test
Figure 3-11: Tacit steps of a procedure
Figure 3-12: The leaky pipeline of testing
Chapter 4
Figure 4-1: Visual Strategy Notes for a Bibliography Generator
Ap...
Figure 4-2: The basic ART grid
Figure 4-3: Characteristic risks for each quadrant

Figure 4-4: Grid with clockwise process and activity list
Figure 4-5: The structure of the HTSM
Figure 4-6: Flow of risk-based testing
Figure 4-7: Key elements of Testing Skills and Tactics
Figure 4-8: The Rapid Testing Framework "Vortex"
Chapter 5
Figure 5-1: A low-tech testing dashboard
Figure 5-2: How your clients see your testing
Figure 5-3: Examples of threads
Chapter 6
Figure 6-1: Tools are more than just a way to simulate users
Chapter 7
Figure 7-1: Considerations for evaluating a magic testing box
Chapter 9
Figure 9-1: Metrics-Based Reporting of Two Projects
Figure 9-2: A Sample PCO
Figure 9-3: Zoomed-In PCO
Figure 9-4: Wide view of color-coded RCO
Figure 9-5: RCO Legend
Figure 9-6: Release Coverage Outline
Chapter 12
Figure 12-1: Structured notes made on a whiteboard
Figure 12-2: Following the energy
Figure 12-3: Tree-type notes, full view
Figure 12-4: Tree-type notes, detail view
Figure 12-5: Real-time tree notes

Chapter 15
Figure 15-1: Group notes on the test strategy
Figure 15-2: Help file used as a found coverage outline
Chapter 21
Figure 21-1: Epistemic Paths
OceanofPDF.com

Taking Testing Seriously
The Rapid Software Testing Approach
 
 
James Bach
Michael Bolton
 
 
 
 
 
OceanofPDF.com

Foreword
I have spent a lot of time reflecting on testing, and if you are reading this
book, chances are you have, too. James asked me to write this foreword
because, over the years, all this reflection has brought me to an important
conviction—one that James and I both share: the way software testing is
currently taught is fundamentally flawed.
As a university professor with over 25 years of experience researching and
teaching programming and software testing, I have witnessed firsthand how
misconceptions and flawed practices ripple through education and into the
industry. These deficiencies affect not only the way we teach but also the
competence of the testing practitioners we prepare for the field. The
consequences are far-reaching, shaping how testing is perceived, practiced,
and valued.
This experience, combined with my profound passion for software testing,
makes me feel very honored to write the foreword for this book.
Throughout my career, I've been surprised—again and again—by how
widespread and deeply rooted the misunderstandings about testing are. One
of the most confounding misconceptions about testing is the belief that it is
easy and that anyone can do it. Nothing could be further from the truth.
Effective testing requires a unique combination of skills.
Another pervasive problem is the wrong definitions of testing that you will
find online or even in textbooks. Many of these definitions focus on proving
correctness or conforming to specifications. As the legendary Edsger W.
Dijkstra pointed out: "Testing can only demonstrate the presence of bugs—
but never their absence." And although he was right, sadly, his statement
has been misunderstood and misused.
Instead of encouraging more research into the foundations of testing, these
misconceptions of testing have inadvertently contributed to the sidelining of
testing as a discipline, with university curricula prioritizing formal proofs
over testing.

This neglect has had a significant impact. Students often find testing boring,
perceiving it as less creative than writing code. Teachers hesitate to
integrate testing into their curricula, worried it might mean sacrificing
"more important" topics. As a result, testing remains undervalued, despite
its vital role in evaluating software quality.
This realization has been both a professional challenge and a personal
mission for me. Over the years, I have made it my goal to speak about how
we should teach testing, advocating for a scientific approach—one that
prioritizes empiricism and adaptability over rigid, procedural, rational
validation.
That is why I am so thrilled to see this book finally come to life. It
represents a long-overdue effort to redefine how we view testing—not just
as a technical activity, but as a cultural, philosophical, and human-centered
science. Taking Testing Seriously: The Rapid Software Testing Approach is
a call to rethink testing as a scientific discipline grounded in direct
observation, thoughtful learning, and a commitment to truth and
responsibility. The emphasis on context and the rejection of "best practices"
are liberating, urging readers to adapt their approaches to the unique
challenges of each project.
One of the things that makes this book stand out is the authors' commitment
to precise language. Language and vocabulary hold the power to shape how
we learn, understand, think, and communicate. While some might dismiss
this level of precision as nitpicking, it is far from that. The care taken with
language in this book reflects an honest understanding of the role of
language in building shared meaning and ensuring clarity, which is needed
when taking testing seriously. In a discipline like software testing, where
miscommunication has already been shown to lead to serious consequences,
the authors' meticulous attention to terminology is essential.
This is the very first book on software testing that equips readers with the
tools and mindset they need to transform their testing practices. The book
challenges outdated paradigms, inspires critical thinking, and offers
practical guidance for anyone committed to excellence in testing. As you
turn these pages, I encourage you to reflect on your own testing practices
and consider how the ideas presented here can reshape your approach.
Testing, after all, is a craft that requires care, curiosity, and courage. James

and Michael have given us a book that is both groundbreaking and highly
practical. This book marks an essential step forward for our field, filling a
critical gap by redefining how we approach and think about testing. This
step is much needed, as testing today is more complex than ever. With the
rise of AI, we face the challenge of testing nondeterministic systems that
learn, adapt, and make decisions in ways that are often opaque,
unpredictable, and difficult to trace. These complexities can only be tackled
with the mindset presented in this book. I am confident this book will
inspire a new generation of testers to embrace their role as critical thinkers
and innovators, shaping the future of testing.
—Prof. Dr. Tanja E. J. Vos Professor of Software Engineering at the
Technical University of Valencia, Spain, and the Open Universiteit, the
Netherlands, Vice-dean of research, Chair of the Dutch Association of
Software Engineering, Editorial Board Member of the Software Testing,
Verification and Reliability journal. Valencia, Spain, July 2025
Improving software quality, one student at a time.
OceanofPDF.com

Reader Support for This Book
Companion Download Files
Taking Testing Seriously mentions some additional files, such as checklists
or spreadsheets. Also, some pages are designed as forms or handouts. These
items are available for digital download from
https://www.wiley.com/go/takingtestingseriously.
How to Contact the Authors
James Bach can be reached at james@satifice.com. His website is
https://www.satisfice.com.
Michael Bolton can be reached at michael@developsense.com. His website
is https://www.developsense.com.
They can both be contacted via LinkedIn as well.
OceanofPDF.com

Part I
Introduction
In This Part
Chapter 1: Why Another Book About Testing?
OceanofPDF.com

CHAPTER 1
Why Another Book About Testing?
—By James Bach and Michael Bolton
Many Cultures of Testing
We have a cultural problem in testing: too many of them. Too many testing
cultures, that is. (National or ethnic cultures are not a problem for testing
but rather a benefit.)
We could start this book like everyone else does—writing as if all the
testing experts in the world agree on what good testing is. They don't agree,
so let's not do that.
This is a book written by two guys, James and Michael, plus some close
colleagues who have some different takes, yet still share a testing culture.
We are full of opinions about the best ways to think about testing. But we
are not authorities on software testing, because no one is an authority on
software testing. It's just a bunch of people and their beliefs.
By "culture," we mean "the distinctive ideas, customs, social behavior,
products, or way of life of a particular nation, society, people, or period,"
after our favorite dictionary, the Oxford English. There are different
cultures of software testing in the English-speaking world, all using mostly
the same words to describe the testing craft—but not meaning the same
things by those words!
It can be a bit maddening because the various cultures rarely label
themselves—or even admit that others exist. Instead, we talk past each
other. In 1999, Cem Kaner, James Bach, and a few other friends declared
ourselves a distinct "school" of testing thought. We named it Context-
Driven. By establishing a school, we sought to differentiate ourselves from
the other schools, as we saw them. There was never widespread consensus
about how the schools should be identified and named, but we (Michael and
James) currently break them out as follows:

Factory School: Believes that testing should be centered on artifacts
and algorithms rather than testers—like a big factory where the
workers could be replaced with robots, at least in principle. This
school dominates the industry, especially the government sector and
regulated contexts.
Quality School: Believes that creating quality should be the central
concern, and that testing is a minor and somewhat distasteful activity
compared to quality advocacy. This school tends to advocate against
having a dedicated tester role because the "whole team owns quality."
Agile/DevOps School: Like the Quality School but dominated by
imperatives and mechanisms of product delivery. This school believes
that testing should be done by developers and automated as part of the
continuous delivery pipeline. It holds that full-time testers should not
exist, except as a temporary expedient.
Analytical School: Found almost exclusively in academia. This school
believes testing is a part of computer science. It sees testing as an
algorithmic process and seeks theories and methods to mathematically
model and optimize testing under controlled conditions.
Context-Driven School: Believes there are no best practices and that
all professional testers should cultivate the skills to design and justify
test practices that fit their projects. This school holds that testing is a
social, psychological, and heuristic process. Rapid Software Testing
(RST) is a Context-Driven methodology.
Each of these schools of thought is a paradigm—that means an all-
encompassing worldview. The concept of paradigms in science was
developed originally by Thomas Kuhn in his seminal work The Structure of
Scientific Revolutions.1 We say these schools of thought, above, are
paradigms because when we encounter thinkers from other schools, we
don't recognize that what they are speaking about is testing. That's how
different they are from us. It's notoriously difficult to have conversations
between paradigms, not only because words are used differently, but
because an important problem for one side may not exist for the other. For
instance, Factory School testing practitioners worry about getting their test
cases right, whereas a Context-Driven tester may go through an entire

project without even thinking about test cases. When a Factory Schooler
misses a bug, he thinks, "I missed it because I didn't have a certain test case.
The solution is to add that test case." When we miss a bug in RST, we
think, "What can I learn from this? Only then will I know what to do about
it." From the Factory point of view, RST looks like vainglorious
philosophizing; from the RST point of view, Factory looks like a ritual to
please the bosses.
Ludwig Wittgenstein used the term "form of life" to describe the totality of
practices, behaviors, rules, patterns, etc. that comprise human social life. He
famously claimed that all language is grounded in a form of life and cannot
be understood apart from the activity from which it emerges. You can't
understand what words truly mean unless you are immersed in the culture
of those words.
Why are we writing about all this philosophical stuff? To prepare you. The
culture of testing described in this book is likely to be different from what
you will pick up by watching people on typical software projects or
watching testing demos online. The Context-Driven way is not just a
method: it's also a paradigm, a culture, a form of life.
Check the title of this book. Prepare to take testing seriously. If you want to
develop yourself into an expert in software testing, we want to help you be
a discerning customer in the marketplace of testing ideas.
But if paradigms are all-encompassing worldviews, how can it be possible
for someone to change paradigms? Why would anyone want to pick and
choose ideas from different paradigms? After all, each paradigm covers
everything in the world. Thomas Kuhn's answer was that paradigms
compete based on the puzzles they leave unsolved. If you get tired enough
of those puzzles, you become open to other ways of being and working.
This is what leads to scientific revolutions.
Other paradigms of testing look for the perfect set of best practices. The
unsolvable puzzle of puzzles comes down to that. The authors of this book
believe they will never succeed. The Factory School has dominated the field
since the 1960s, and what they have to show for it is the V-model, ISTQB
certifications handed out to anyone who can memorize rote answers, and a
testing industry that is even less respected at the time of this writing than it
was 40 years ago. The Agile/DevOps crowd is now ascendant, which is

making nobody want to be a tester. (They want you to be a "team member"
and not focus on testing.) AI-based test tools, some of which are intriguing
and others astonishingly bad, are confusing managers into thinking they can
test at the push of a button. The industry lurches from trend to trend and
tool to tool, yet too many software products are unreliable and unsatisfying
to use, and users are being conned into believing they ask too much to
expect their products to work as promised.
We in the Context-Driven paradigm believe that people, heuristics, skills,
and ethics should be our focus, because that leads to finding more bugs and
better bugs; it attracts more talented people into the testing field; and it
makes for a better society. The tough puzzle for us is how to train and
manage testers to operate in a world that mainly cares about making a quick
buck and that wishes testing were a fixed algorithm rather than an open
investigation.
Again, Rapid Software Testing is one example of a Context-Driven test
methodology. It's not the only one out there, but it's ours, and we offer it to
you.
Why Us?
We said, above, that the authors of this book are just a bunch of people with
beliefs. That's not quite right. We intended the word "just" in that sentence
to emphasize that we have no authority. But the whole truth is that we are
more than merely people with beliefs.
We are people who love testing.
We profess our love by studying testing and practicing it, not only
professionally, but on our own time. Everyone who contributed to this book
has embraced testing in some way, during a substantial part of their working
lives, and won the respect of their peers for doing so.
We think good testing is fun to do, fun to talk about, and fascinating to
study. Good testing touches upon deep elements of philosophy and
humanity. It causes us to ponder, like Socrates, how we can know anything,
and suspect, like Pyhrro, that we can't know anything for sure—that we
must learn to live with uncertainty. We love testing partly because it turns
philosophy into a working tool.

If you want to get good at testing, consider learning about it from people
who aren't madly grasping for any excuse not to do it, wishing that their
robots or AI agents would do it instead. Learn about it from people who
aren't openly craving to write code instead of finding bugs. Yes, James and
Michael write code; we do it because testing with tools is a powerful way to
find bugs—not because it's a way to avoid having to interact with and
experience the product.
Learn from people who affirm your agency. Yes, you will see our strong
opinions in this tome. No, we don't expect you to shut up and obey us.
When James and Michael were young, we challenged the graybeards of
testing, and now that we are gray, we encourage people to challenge us. You
will notice that the contributed chapters written by our colleagues
frequently describe how they rethought and remixed something that they
heard originally from James or Michael. We especially commend Tai
Xiaomei, who developed something she calls Buccaneer Methodology,
based on Rapid Software Testing and James's ideas about self-education. It's
an interpretation and remix with wonderful spirit and originality, as far as
we can tell.2
In 1998, James had dinner with a famous requirements analysis consultant
by the name of Karl Wiegers. James tried to describe the Rapid Software
Testing methodology to him (so new that it wasn't yet called that). Karl said
he didn't want to hear about it, because in his view the industry didn't need
"new models."
Karl then wrote an opinion piece in IEEE Software using the example of his
conversation with James to complain about the proliferation of proposed
methodologies.3 It's thoughtful and worth reading. He essentially says that
people should take the existing wisdom more seriously before trying to
improve upon it. James' counterpoint was that the existing wisdom, if taken
seriously, is ruinously wrong. The sole reason it persists is people aren't
really following it! Meanwhile, what they are doing that works goes largely
undescribed. Indeed, Agile came along and changed everything, just a few
years later.
What Karl never discovered—because he refused to engage with James on
the substance of the RST—was that James was not merely proposing a new
model of testing, but also a different way for testers to relate to testing. His

reaction to Karl was to plow ahead anyway, because, as they say, good boys
don't make history.
We take as our role models people like Cem Kaner, Jerry Weinberg, Dani
Weinberg, Naomi Karten, Jim Batterson, Payson Hall, Bill Hetzel, John
Musa, Ross Collard, Eileen Strider, Ed Yourdon, Tim Lister, Lee Copeland,
Fiona Charles, Rob Sabourin, and Harry Collins, all highly accomplished
leaders and authors who engaged with their students and colleagues without
brandishing the cudgel of presumed authority. They were actively anti-
authoritarian, debating without coercing, patiently engaging, and so
encouraged the next generation to experiment with techniques and models
that sometimes extended and sometimes rejected popular practice. These
leaders were seniors when James and Michael were still juniors and helped
open the way for them. Now it's our turn to help you.
This matters not necessarily because we need better models and methods in
our industry. It matters because every practitioner must feel that they own
the methods by which they work. Otherwise, they can never feel fully
responsible for their work.
Why Testing?
Testing is looking where you are going. Testing is the headlights of the
project. Just as headlights don't slow down your car, testing doesn't slow
down projects. Testing, done well, allows projects to move along at the
fastest safe speed.
We could just release the product to users without any serious attempt to
test it. Users could report any problems they experience, and we could fix
them and deliver new releases as required. This is an appealing and
reasonable idea. It will work well for some contexts. We wrote this book to
address contexts in which this won't work well—such as when you don't
have a close, friendly relationship with your users, the product operates on
critical data or machinery, or when you can't waste any chance to gain and
retain market share in a competitive marketplace.
Users often have things to do other than complain to developers. James
once tried and failed to report a security bug in a book search kiosk to a
busy bookstore manager. The manager was confused and irritated by the

report, which he summarily dismissed. If the Borders Bookstore IT
department was expecting to get helpful feedback from the field, it certainly
didn't come from that particular store.
How about prevention, then? There is a subculture of technical people who
are excited about prevention. Some of them call themselves quality
coaches. They say, "Wouldn't it be great to keep bugs from ever happening
instead of having to find them when they happen?" Yes. Of course, it would
be great. If you could do that perfectly, you wouldn't need testers. Only
thing is—how do you know that you've prevented all the bugs? Only
through testing can you even begin to know; otherwise, all you've got is
faith.
Testing is the opposite of faith in the product. Testing begins with faith in
the existence of trouble.
Another force pushing people away from testing is fear of an open-ended
process. In many companies, they use "definitions of done" and arbitrary
deadlines to bring procrustean predictability to the test process. We agree
that testing must ultimately be made to fit the confines of real-world
projects. But that must be done in a spirit of true inquiry about product and
business risk. We must adapt our process as our understanding of risk
matures throughout the project. That may mean revisiting and revising our
ideas about what it means to be done—not only in terms of the product, but
also in terms of the test process.
Testing is a search for the truth about the product, grounded in evidence. So,
it is reasonable to ask, how much evidence is enough? To be charitable to
those who would rather talk about prevention than testing: excellent
prevention may lower risk to the point where deeper kinds of testing are not
required. Disputes about the need for testing may be a disagreement about
the risks on the project here and now, rather than a fight over the value of
testing in general.
Nevertheless, there are quite a few people in the industry who don't want to
know the whole truth about their products—because knowing the truth
creates responsibility. Quite a few businesspeople secretly hope the testers
find no bugs, even if the product is full of them. If the testers don't find
them, then no one can blame management for not fixing them. And if
cornered, management can blame the testers. Or they can blame users for

having expectations that are too high. Testing as a practice and vocation is
under pressure partly because what might be called the "technology of
blame displacement" (the ceremonies and legalities for excusing bad
software) has progressed faster than the field of testing. This has created a
perverse phenomenon that we call "fake testing."
Testing is fake if testers are actively discouraged from finding bugs and
reporting them, if impediments such as arbitrary paperwork are put in their
way, or if they are asked to create test cases that no one ever needs to
perform. In fake testing, what matters is the appearance of testing. The
more ceremony, the better. Many testers plod through such motions without
questioning them. The value of fake testing is that it creates less trouble for
management than real testing, while also giving them the ability to say, "We
tried to test! We followed best practices! No one can blame us!"
In our travels around the industry over the last few decades, the authors of
this book have seen a lot of fake software testing. In fact, there are large
testing outsourcing companies that appear to be dedicated to fake software
testing.
So, here's to real testing, the brave souls who do it, and the patient managers
who foster it.
Why Testers?
Some people say testing is everyone's job. This is not helping.
They mean well, most of them. But their words have the effect of
marginalizing the test process. Anything that is an "everyone job" is usually
no one's specific job. Meanwhile, each person does have a specific job,
from day to day, for which they are specifically held accountable. Specific
jobs take precedence over "everyone jobs." Scientists call this social
loafing: people make less of an effort when they work in groups, when their
individual efforts are not perceived as special or identifiable.4
Social loafing has been studied using experiments with tasks that were
relatively unskilled. Testing, however, is not an unskilled task. Some people
are better at it; some people are more naturally motivated. But anyone who

seeks to do it to a professional standard must study and practice to achieve
that standard.
Good testers bring five vital qualities to the team:
Empirical (instead of wishful): We explore and characterize the
actual behavior of the product by probing it with creative and clever
experiments, instead of assuming that the product probably works.
Skilled (instead of merely intuitive): We do this systematically and
meticulously, instead of meandering.5 We anticipate future needs and
make reasonable preparations for efficient work, instead of merely
playing with the product and living in the moment.
Different (instead of redundant): We see things others miss because
we think the way others don't. We are seeking trouble, rather than
betting on the power of positive thinking. We represent people and
feelings that may not otherwise be present in the team, instead of
participating in an echo chamber.
Motivated (instead of ambivalent): Good testers enjoy their work
and don't secretly wish they were writing code. Also, when we find
bugs, there is no conflict of interest. We are not penalized by having to
fix them. But, if you create quality as your specific job—which is true
for every non-tester on the team—each new bug is a new task on your
already back-breaking list.
Available (instead of preoccupied): When the team assigns people
specifically to do testing, they have the time to do it. Testers do testing
—not spec writing, technical support, or development. Even if
developers claim to have the first four qualities, they cannot claim to
have plenty of time for testing. A developer with "spare time" for deep
testing will be given more development work to do, or less time to do
it in.
Testers can work in parallel with developers, working throughout the
project to follow up on any rumor of product risk, but also preparing for
deep testing.
Yet, since about 1990, the industry has slowly turned away from the idea of
having independent testing organizations that investigate, monitor, and

report the truth of the status of the product. Rather, teams are going without
testers, or else they have a small number of embedded testers who work for
development managers. Agilists say this is a good thing because everyone
should be on the same team and working together. But this is a feeble
excuse. Testers and developers never had much trouble working together,
even before Agile. People can be on the same project even though they
work for different people.
Tester independence is important because, without it, testers lose
management support and motivation to do what needs to be done. They
become co-opted. They become more like cheerleaders and less like truth-
seekers. With the advent of GenAI, the industry needs critical thinkers more
than ever. And it needs them not to be under the thumb of developers and
development management if they are to do their best work.
The strongest argument we know for dedicated testers is that it avoids
conflicts of interest. Since they are not responsible for having made the
trouble, and they are not responsible for fixing it, they can seek and report
trouble without fear or regret. Their incentives are aligned. Yet, some
people in our industry who have no problem appreciating separation of
powers in national governments, or a legal system that separates
prosecutors from defense attorneys, cannot accept that it's psychically
wrenching for people who build software to deeply test their own work.6
They happily benefit from regulatory agencies that inspect restaurants and
large construction projects, but they assume that no good will come from
having third parties look at their work.
Savvy management, on the business side, should want testers reporting
directly to them so that they can hear unvarnished reports about quality
problems.
By The Way, Now Everyone is a Tester...
In the couple of years we've been designing this book, GenAI has impacted
the industry like a runaway container ship driven by hurricane winds of
hype. It's become so apparently capable at coding that it threatens to change
the face of software engineering. It's also poorly tested.
Our industry has not yet figured out how to incorporate AI-based tools. But
it seems that there is going to be a new engineering sub-culture—perhaps a

new kind of company—that believes that it doesn't need expert
programmers to produce complex software technology. Their people will
write "prompts" instead of code. They will "vibe" instead of think for
themselves.
Here's what they will need: people who are good at testing. They might not
be called testers. They might be called "prompt engineers" or "red teams"
or "product specialists." But it is inescapable that if you outsource your
engineering to AI (or, for that matter, strangers on the Internet, or aliens
from T Coronae Borealis), you will have to satisfy yourself, somehow, that
it delivered the goods. Programmers writing prompts must now graduate to
supervising the development process, and that means learning how to test.
Sure, the AI may tell you that it already did the testing. You won't know
what it means by that. It may tell you what it means by that. You won't
know if it is telling the truth or just making things up. Even if it tells you
the truth, it will be so complex that you won't understand it unless you get
deeply into it. It may genuinely do interesting output checks. You won't
know if it left important checks out, unless...
...unless you or some other responsible person carefully tests it.
Why Not "Traditional Testing?"
So-called "traditional testing" can be traced back to the first book dedicated
to testing, Program Test Methods.7 It was compiled in 1972 following the
Computer Program Test Methods Symposium held at the University of
North Carolina.
In that book, you can find plenty about test cases. The authors are generally
optimistic that testing can be automated. They propose that test plans and
processes should be thoroughly documented and standards-based. They
treat testing as a mechanistic process. Apart from a few oblique references,
none of the chapters discuss the nature or behavior of testers or testing
skills.
Almost every testing book written since Program Test Methods has
espoused the same basic belief: that testing works best when every little

thing is written down, locked down, and, ideally, automated. We think we
know why:
Those books were written by technologists whose training and
temperaments bend toward mechanisms and structures.
No one writing them consulted any scientist trained in social or
psychological research methods—or looked at any research about how
people solve problems and search for things (such as bugs) in complex
spaces.
It's easier to write about templates, tables, and rules than it is to write
about how people think.
Management wants a closed and predictable testing process, not a
wide-open world that depends on the judgment of people.
In the second chapter of Program Test Methods, Fred Gruenberger reviews
existing writing on the subject of testing and declares most of it to be
unhelpful. The one exception he notes is a section of Computer
Programming Fundamentals, written by Jerry Weinberg in 1961. In the
single chapter about testing in that book, Weinberg writes of a vision of
testing that is all about creativity:
The sheer number of tests performed is of little significance in itself.
Too often, the series of tests simply proves how good the computer
is at doing the same things with different numbers.... With a
computer program, however, the greater problem is to prove
adaptability, something which is not trivial in human functions,
either. Consequently, we must be sure that each test does some work
not done by previous tests. To do this, we must struggle to develop a
suspicious nature as well as a lively imagination.8
Weinberg went on to get his PhD in psychology, and he wrote his
dissertation on how humans solve problems. He later wrote Introduction to
General Systems Thinking, The Psychology of Computer Programming, and
Perfect Software: And Other Illusions About Testing, along with many other
works that took a holistic, social science approach to the subject.

This represents a different tradition for testing.
The first full book on a human-centered approach to testing was Cem
Kaner's Testing Computer Software. It came out in the mid-'80s. What was
Kaner's PhD in? His first one was in psychophysics; his second was in law.
Cem Kaner is a humanist and consumer advocate.
Meanwhile, the authors, James and Michael, were working testers and test
managers in the late '80s and through the '90s. We saw for ourselves that
what people called traditional testing was ponderous and alienating. The
great testing we saw came from clever, spirited thinkers. Naturally, we
asked ourselves: What was their process?
We can trace the humanist and systems thinking tradition of testing far
back. We can trace it to the dawn of science, even to the dawn of
philosophy itself.
Come with us if you want to live ... as a tester.
Notes
1   Kuhn, T. S. (1970). The structure of scientific revolutions (2nd ed. enl).
University of Chicago Press.
2   Her book on testing is written in Chinese and hasn't yet been translated,
but the illustrations alone are remarkable.
3   Wiegers, K. E. (1998). Read my lips: No new models! IEEE Software,
15(5), 10-13.
4   Harkins, S. G., and Petty, R. E. (1982). "Effects of task difficulty and
task uniqueness on social loafing," Journal of Personality and Social
Psychology, 43(6), pp. 1214-29.
5   Meandering is actually a useful test approach, but it should not be your
only approach.
6   All developers test their own work shallowly. The kind of testing we are
talking about is serious testing; roll-up-your-sleeves-and-spend-days-at-
it testing.

7   Hetzel, W. C. (Ed.) (1973). Program test methods. Computer Program
Test Methods Symposium, Englewood Cliffs, N.J.: Prentice-Hall.
8   Leeds, H. D., & Weinberg, G. M. (1961). Computer Programming
Fundamentals. McGraw-Hill.
OceanofPDF.com

Part II
Rapid Software Testing
Methodology
Part II presents the closest thing to an authoritative description and
explanation of the Rapid Software Testing methodology that exists in print.
For decades, in the tradition of certain great philosophers and many more
busy consultants, RST was communicated orally and interactively. RST is
partly a skill set, after all. Skills are not learned by reading a lot of text, but
rather via deliberate practice.
We did have some collateral—a handful of reference documents and some
slide decks—but we liked the freedom of an informally documented way of
working. By keeping the details of the methodology open, we could more
easily develop it, trying different things in the classroom and on our
projects.
The challenge of writing this book has forced us, in a good way, to
crystallize our ideas. At long last, we've dredged our intuitions to make
explicit some of our stubbornly tacit knowledge about testing. Along the
way, to aid the determined student of testing, we've identified about a
hundred books and journal articles that cast more light on the philosophy
and science upon which RST rests.
OceanofPDF.com

CHAPTER 2
Foundation
—By James Bach and Michael Bolton
The Meaning of Testing
Our idea of testing can be traced back to a quote from the great proto-
scientist Robert Boyle, who in 1661 wrote, "Those wary testers, that like
not to be cheated."1 He was probably referring to people who assess the
quality of gold and silver. Indeed, the Oxford English Dictionary uses that
quote as the basis for its definition of "tester" as a person "whose business
is to test the quality or condition of anything." This predates the
corresponding definition of the word "testing" in the OED by about 90
years.
In other words, "testers," as people, came before "testing," as a process.
This suits us, because we define testing as something a person does. We see
it as a fundamentally human, social, and technical activity (more on that
below).
Our definition of testing encompasses humanity, cognition, curiosity, and
science. Here it is:
Testing is the process of evaluating a product by learning about it through
experiencing, exploring, and experimenting with it.
"Evaluate ... by learning" is a critical part of this definition. By
emphasizing learning, we mean to imply that any evidence gathered during
testing is filtered and transformed by the tester, who then constructs a story
about what happened and what it means. This process changes the tester,
who is constantly becoming more experienced with the product and better
able to interpret its behavior.
This learning process is easy to see when you are new to the product and
just figuring out how to test it. But many people find this a strange way to
think about regression testing—which seems as though it should be a

routine and scripted process—especially when it involves a lot of
automation. Isn't that testing a simple matter of running those test cases? Is
that really a learning process?
If you do it well, then yes, it absolutely is a learning process.
James frequently tells the story of hiring a guy out of IBM into his team at
Borland. Paul Richard was his name, an affable man in his early forties.
Since he was new to the team, Paul was assigned to maintain the boring
automation for the Borland C++ debugger. What others had done before
him was run the automation periodically, report bugs that were found, and
fix cases that broke. But what Paul did was dive in and review the code for
all 450 little programs that constituted the automation. In doing so, he
discovered that the automation was doing almost nothing useful. The other
testers had decided to automate as many checks as possible,2 so they
prioritized automation that interacted with the debugger while the debugger
was doing no debugging. Those kinds of checks were easy to create but, of
course, told us nothing about how the debugger behaved when a user
needed to get work done.
Paul treated testing as a learning process, not merely as an exercise in
pushing buttons that management told him to push. James resolved never
again to blindly accept any test strategy or artifact for which he would be
responsible.
Even if you do run a bunch of very good automation, when it fails, you
must investigate it. That's a learning process. And when it doesn't fail, you
ought to be wondering what bugs you could be missing. You should be
proactively looking for holes in that testing. That's a learning process.
Real testing is always learning.
Testing vs. Checking
Testing may include a lot of processes, such as modeling, questioning,
studying, sampling, observing, sensemaking, and storytelling. In RST, we
distinguish between aspects of the testing process that machines can do
versus those that only skilled humans can do. We do this linguistically by
adapting the ordinary English word "checking" to refer to evaluations that
tools can do. This is exactly parallel with the long-established convention of

distinguishing between "programming" and "compiling." Programming is
what human programmers do. Compiling is what a particular tool does for
the programmer, even though what a compiler does might appear to be, on
some level, what programmers do.
Think about programmers like this: they are responsible humans, situated
between people and machines, who interpret the needs of people and direct
machines to fulfill them. In the past, programmers routinely worked with
machines on a level close to the hardware. Over time, as computing power
grew and layers of services and libraries evolved, programmers worked at
ever higher levels. The day James started work as a programmer, in 1983,
he was doing all his own memory management, down to the last byte. He
once wrote a video game in a total of 16,384 bytes of read-only memory.3
Today? He usually doesn't think about memory management. It's all done
by magic Python fairies (he assumes). He works on a higher level now.
But what about AI? Doesn't AI write code now? Yes, it sort of does. It does
it well enough that people who don't know anything about code can "vibe"
their way to apparent success just by pleading with their AI until a product
comes out that seems to fit their purposes. Does that mean there is no more
need for programmers? Well, here's how you know programmers matter: if
you claim you are not a programmer and try to vibe code, and something
goes wrong, you can't hold the AI responsible. It's not a legal entity. It has
made no commitment to you. If you want satisfaction, you have to move up
the chain to find the human or business that sold you the tool. Only they
won't fix anything, and they will tell you it is your own fault. You will be
told you "didn't do it right" and that you must learn better methods of
prompting, supervising, code-reviewing, debugging, and testing. You will
be dismissed as ignorant about the capabilities and limitations of AI.
So it's not enough to know the human side of the business problem—you
must also sufficiently understand the machine side. Either you become a
programmer (by our definition above), or you must partner with someone
who enjoys developing those skills and doing those things. Just like that, we
are back to relying on programmers, whom we can hold responsible for the
work.
Come to think of it, no one speaks of automated programming or manual
programming. There is programming, and there is lots of other stuff done

by tools. Once a tool is created to reliably do stuff that once required a
programmer to do, it is never called programming again, and programmers
become higher-level programmers. We want to use the same linguistic
practice for talking about testing: if a tool does something that looks like
testing, call it checking, then look for the responsible person who is guiding
that process. That is the tester who is doing the testing.
Checking is the mechanistic process of verifying propositions about the
product.
"Checking" is short for output checking or fact-checking.
"Mechanistic" means algorithmic, which means that it can be
expressed explicitly in a way that a machine could perform.
"Verifying" means ascertaining the factual status thereof. Quality is an
opinion, not a fact, so quality cannot be verified. But lots of things
about a product and its behavior can be settled as a matter of fact.
"Propositions" are statements about a product that may be true or
false.
Most people in the industry use the word "testing" when they are really
talking only about checking. This is a big problem, because it contributes to
the impression that testing is not a skill, and human testers are nothing but
slow and unreliable button-pushers. In this book, we strive to use language
more carefully.
Under these definitions, testing cannot be automated, but checking can be.
Good checking is always embedded in testing, and good testing probably
involves a lot of checking.
Testing vs. Performing a Test
Now, what is a test?
A test is an encounter with a product that constitutes an episode of testing.
A test is not a physical thing. It is not a document—not even an idea. A test
is an event, a performance that happens at a particular place and time—like
a play on Broadway.

Sometimes we use the word "test" more casually, to refer to a procedure or
an idea for a test. This is just like a playwright waving a script and saying,
"Here's the play I wrote." Nevertheless, the writer knows that the real play
is the thing the actors do on stage.
Thus, there is a difference between testing and performing a test. To
perform a test means that you configure, operate, observe, and evaluate a
product in some particular way, at some particular time.
Testing encompasses everything that is part of your job as a tester, including
preparing to test and coordinating with people who rely on you. Is reading a
specification testing? Sure, if you are doing it to fulfill the mission of
testing. Is meeting with a developer to discuss product plans testing? Of
course ... so is talking to a user, learning about the platform, scouting for
new tools, etc.
Deep vs. Shallow Testing
Testing is a kind of search. It can be compared with the processes animals
use to forage for food. Ben Nolting describes the foraging of animals as
alternating between extensive and intensive modes: "In intensive mode, a
forager searches an area thoroughly by taking short step lengths with
frequent reorientations. In extensive search mode, a forager moves
efficiently across resource-poor areas by making long straight-line steps
with few interruptions."4 This roughly maps to our notion of deep and
shallow testing.
Good deep testing means testing that maximizes the chance of finding every
elusive bug, of some particular kind, that matters. Good shallow testing
means testing that has some chance of finding every easy bug of some
particular kind.
"Maximizes the chance" means doing everything a reasonable,
competent, and prepared tester would do if asked to find every elusive
bug.
"Elusive bugs" are those that, once found and investigated, are deemed
in retrospect to have been hard to find.

"Easy bugs" are the opposite of elusive; they are deemed "on the
surface" or "obvious."
"Particular kind" means relative to a particular quality criterion, or
otherwise detectable by a particular kind of oracle. A test oracle is a
means by which we recognize a problem. We will discuss oracles in
depth in Chapter 3.
"Bugs that matter" are bugs that represent significant business risk to
our clients.
A deep test process will find both elusive and easy bugs.
To test deeply, you must first test shallowly, in the same sense that all
deep-sea dives begin as shallow ones.
Shallow and deep are not moral judgments; they are purely descriptive
terms. But we can and do make judgments about the kinds of testing that
are right or wrong for a specific context. Sometimes it is wrong to test
deeply. Sometimes shallow testing is not enough. Both shallow testing and
deep testing can be done badly. Professional testing is largely caught up in
the problem of knowing when to stay shallow and when to go deep.
Narrow vs. Broad Testing
In RST, the breadth of testing is the spectrum of bug types that a test
process is capable of detecting. This is distinct from the depth of testing.
Broad testing might be shallow, or narrow testing might be deep. For
instance, end-to-end experiential function testing, tends to be broad and
relatively shallow. It's broad in the sense that the human tester is able to
recognize a wide variety of problems without deliberate effort. This is
especially true if they have strong domain knowledge or lots of experience
with the product line. It tends to be shallow because the human engaged in
it generally will not have the time to try 28 variations of input on a screen
and then follow them all the way through the flow. On the other hand, we
might use automation to test 10,000 variations of input to a function and
check that no error messages occur. This is quite narrow testing, but also
quite deep with respect to finding bugs in the algorithm for that function
(see Figure 2-1).

Figure 2-1: Depth vs. Breadth in Testing
So these are some of the terms we will use to describe testing. Why does it
matter to use these words carefully? Well, for the first couple of decades we
were in this field, we didn't think it was important. But in recent years, the
entire field of testing has been under attack by those who think that testing
equals test cases or even that testing equals automation code. We eventually
realized that to protect and elevate the testing craft, we need a blueprint for
it that brings out its complexities instead of hiding them behind vague
terms.
We think that's what it means to take testing seriously.
Our Vision for RST
The test methodology that came to be called Rapid Software Testing began
as James's attempt to reinvent testing from first principles.
He felt forced to do this because the literature on testing at the time
encouraged testers to do things that were infeasible or unhelpful. The books

and standards led testers away from achieving the mission of testing.
Prevailing opinion was also authoritarian—graybeards telling you about
"best practices" without having done the deep work of studying how people
in industry actually test and why they test the way they do. Someone's
personal view of good practice in their own context was sold as objectively
the best thing anyone could do.
Many point to the "Waterfall" process as the great mistake of software
engineering in the twentieth century. But we think there is a more specific
culprit: Parnism. (Don't bother Googling it; it's just a term we are using for
the philosophy of eminent computer scientist David Parnas.)
Sometime around 1989, James went to his first software testing conference.
At that conference, David Parnas gave a talk about a paper of his called "A
Rational Design Process: How and Why to Fake It."5 In this famous paper,
Parnas argues that we all know what a "rational" software development
process is, but that no one can follow it. Therefore, we should pretend to
follow it.
The "rational" process Parnas referred to was basically Waterfall, with an
emphasis on complete documentation of the design, code, and testing. By
"faking it," he meant that we should do our best to document and design up
front, but that if we can't do that, we should at least do as much of it as we
can and keep the documentation up to date at all times—so that the process
appears retrospectively to have been a Waterfall process (see Figure 2-2).

Figure 2-2: Parnism: methodological imperialism
James was pretty annoyed. He was seeing firsthand that the advice in the
textbooks for how to run a software project was not followed in practice.
Professor Parnas saw the same thing, apparently, and his recommendation
was to deny the problem instead of studying how software actually gets
made.
Right there, with James irritated to be told that lying is a best practice, is
where the project to reinvent software testing really began for him. James
wanted a way of doing and explaining testing that was firmly rooted in the
long history of human thought. He wanted it to be
Philosophically sound: The methodology must be self-consistent and
consistent with good epistemology. Uses of words should be fully
consistent with definitions. In a word, it must be logical.
Scientifically literate: It must be consistent with the precepts of
science and what we know about specific branches of science relevant
to testing. That includes computer science, psychology, sociology,
cognitive science, general systems theory, the philosophy of science,
and certain niche fields like foraging theory and complexity theory.
Humanistic and ethical: The starting place is a person wanting to find
all the important problems in a software product for the benefit of

other persons. It must serve human agency and encourage
responsibility.
Reinventible and extensible: If it is a humanistic and ethical
methodology, and if it reflects something true and helpful, then testing
must be accessible and personal. Its soul can't be stashed away like a
horcrux inside a roc's egg on top of the highest mountain. We must
invite each other to reinvent it, modify it, or extend it, just as the world
of science does for scientific ideas. It must be antiauthoritarian and
anti-ritualistic. There are no received best practices—only what seems
to work—and these are situational and subject to the judgment of
"person[s] having ordinary skill in the art."6
Practical: Testing must serve the needs of testers in industry and do so
with reasonable efficiency. It must enable fiscal as well as social
responsibility. It must be implacably focused on one prime objective:
to find all the bugs that matter, before it's too late!
The authors have applied RST to all kinds of testing, from incredibly
formalized (such as in court cases, medical devices, and regulated financial
systems) to extensively instrumented (such as in automated regression
checking for legacy systems), to casual or experimental products. RST is a
context-driven methodology.7 That means it has no fixed set of practices but
rather is a system for helping people design and apply practices to their own
contexts.
If you practice RST, then you are not necessarily a "follower" of James or
Michael. You are a practitioner of software testing who applies your version
of the RST methodology. You own that version. You control it. Of course, to
do that well, you will need to understand how RST is designed and why it is
designed that way.
Throughout this book, we will be describing the "RST namespace." That's
the set of definitions we propose for testing. We will also use the phrase "in
RST ..." followed by various categorical statements. By using that
convention, we are reminding you, and ourselves, that we don't own
software testing in general. Nobody does. There are no authorities in the
testing industry.8

We can only speak for RST. RST is the best way that we know of to think
about testing, given our beliefs about what the word "testing" means and
our beliefs about how the world works. If we were to find a better way, we
would adopt it and call that RST.
You could say that the vision of RST began with a refusal to accept fake
software testing. This remains a challenge, especially in the age of AI.9
Design Features
Rapid Software Testing methodology has taken a certain shape and form
because that is the best thing we know that fulfills the vision. It boils down
to this: if you want to find the right bugs at the right time, as well as you
possibly can, under any conditions you may face, then you must have a
testing methodology that
Centers on testers as independent and active agents, rather than
centering on test cases. A tester is anyone who enacts testing. Such
people may or may not be full-time, professional testers. In RST,
everything about testing begins with the tester rather than some
document or machine. Testers then create documents, use tools, and
interpret all that as needed to get the job done.
Treats the product as a mystery to be investigated rather than
merely merchandise to be accepted. Of course, a software product is
merchandise, and there is a question of whether to accept it. But when
we think in those terms, we tend to seek the simplest, quickest excuse
to approve it and move on. The RST mindset is that every product is
mysterious, no matter how plain it seems on the surface. We embrace
that mindset because it improves our chances of finding elusive bugs.
Treats testing as an empirical investigation focused on discovering
trouble rather than proving quality. By "empirical," we mean
grounded in direct observation of the world. As we observe, we apply
tactical pessimism; we cultivate faith in the existence of trouble.
Assuring quality is therefore not part of testing, as such. Assessing
quality is.

Focuses on business risk. Testing is itself a risk-analysis process. But,
to guide the testing, we make educated guesses about where that risk
might be. The process unfolds iteratively, allowing us to get a sense of
the risk and home in on it with deeper testing.
Is a set of heuristics, not algorithms. RST is a heuristic itself, and it
consists of heuristics. RST is not an algorithm. There are no templates
or hard rules in RST, but there are a lot of lists and diagrams that serve
as cognitive lenses for testing.
Treats tests as events, not artifacts. Testing is a performance that
plays out over time. Each test is to some degree exploratory (even
those tests that involve automated processes), and no two tests can
ever be exactly the same, in every way.
Uses professional and consistent terminology. We define our terms
carefully, and the total vocabulary of RST provides an ontology that
helps us speak and write clearly about testing.
Can function under conditions of uncertainty and time pressure.
Of course, we want all the information and all the time, but we don't
require it and never expect it. Regardless of how good we are, some
level of confusion and even incompetence on projects is normal.
Is explicitly context-driven. That means a basic method of RST is to
evaluate the context of testing and design, adapt, and apply those
heuristics that solve real problems in that context. The main problem is
how to serve the best interests of our clients. The rapidity of Rapid
Software Testing comes mainly by avoiding doing things that don't
help.
Is enabled and unlocked by the skills of the practitioners and the
testability of the project and product. RST is about craftsmanship. It
advocates deliberate practice for testers, in order to build skill. It
advocates for creating a supportive environment for testers.
Foundational Ideas
Why has it taken decades to write a book about RST? Because we (James
and Michael) insist on rethinking every ... little ... thing ... about testing.

And now you get to experience a little of that, because this section is all
about the unconventional ideas that form the basis of our approach to
finding more and better bugs than other testers do.
There are a bunch of them.
For the record, being unconventional is not our goal. Our goal is excellence
in testing. In case testing ever becomes an Olympic event, we fantasize
about coaching our respective national teams to gold. Have you ever seen
Olympic bobsledding? Every action of the sledders and every piece of
equipment is the result of intensive research and experimentation. That's
what we do, except with software testing and critical thinking.
We've arranged these ideas into pairs to help convey the dynamism and
complexity of the subject; then we further organized them into these
sections:
Who We Are (ideas about the people who test)
What We Study (ideas about what we test)
How We Manage (ideas about regulating ourselves and our processes)
How We See (ideas about how we gather evidence)
You can skip this part, but if you find yourself confused or shocked by some
weird thing we may say later in the book, consider reading this section. It's
quite possible that we are using words differently or starting from different
premises than you might assume.
Who We Are
Only people do testing.
With 80 years or so of the computer age behind us, it's a bit awkward that it
is still necessary to talk about the one absolutely fundamental element of
computing: humans. You'd think—especially since mostly humans10 will be
reading this—it would be too obvious and elementary to mention. Alas, no.
The problem is that many people are doing their best to escape from other
people. They use code, machinery, and otherwise impersonal systems to

buffer and defend themselves from expensive, unreliable, irritating
humanity.
But only humans have legal and moral standing in society, own property,
have rights, etc. Ultimately, only people can be responsible for the actions
of technology, no matter what they do to hide behind official policies,
procedures, and chatbot interfaces. All software projects are the work of
people, and they ultimately serve or harm people.
Some technologists reject this idea. They believe everyone is expendable in
favor of machinery. And even if they can't expel people from projects, they
think they can treat people like perfect, rational, obedient agents. Perhaps
they believe all ideas can be written in "smart contracts" and stored on a
"blockchain." This belief is irrational. There is no evidence from history or
psychology to support it. We all have emotions and belief frameworks that
defy colonization by management, or, for that matter, political leaders. This
is especially true for users, who sometimes do things that we wish they
wouldn't do.
If you believe this, then you would not be reading our book.11 But you will
encounter people like this, and you need to be ready for them.
Practitioners and Clients
Here's the RST take on humanism in testing:
In every good project, and for any given product that results, some
human is responsible for testing that product. That is the practitioner
of testing.12
If you want to know the status of testing, your first impulse should be
to talk to that person.
In a good project, the practitioner knows who they serve and exactly
what their mission is with respect to those people—whom we call the
clients of testing.
A good tester possesses tacit expertise that allows them to make
accurate social judgments, on the clients' behalf, about what to test and
what constitutes a problem that matters. These judgments are non-
algorithmic.

In a good project, this tester mediates all this testing, regardless of
what parts of it are supported by automated processes or cognitive
artifacts, such as test case documentation. The testing is not testing
until and unless the tester personally comprehends and approves all the
elements of the testing.
Everything on a project should be traceable to people, because only people
can be accountable. For this reason, in RST we never say that automation
does testing; it merely supports testing. Only people do testing.
Testing is a responsible job. We are speaking of responsibility in the ethical
sense. To be responsible is to choose to respond to the necessities of your
commitments, to the call of duty, to requests for help, or to a challenge to
explain or atone for your actions.
Responsibility is relevant because testers always work on behalf of some
client. To the best of our ability, we simulate, anticipate, and inhabit our
clients' ways of thinking, always assuming that our clients mean well. We
owe them a duty of care. Meanwhile, we are also agents for ourselves. We
need our own agency, because it is tremendously expensive and inefficient
to control testers from a distance.
We must navigate treacherous social waters, at times. For example, how
much should you push to get bugs fixed? Our heuristic: push until the
moment you are understood. If your client understands your point and
doesn't agree with it, you have to stop. If you go one inch further, they will
rightly believe that you think you should be running the project or running
the company, and you will lose credibility. But if your client still hasn't
properly heard you, it may be your responsibility to keep trying to get
through.
In Rapid Software Testing, responsible tester is a term of art. A responsible
tester is whoever is accountable for the quality of the test process for any
part of a product. This is in contrast with a supporting tester. A supporting
tester is a tester who assists in fulfilling the test strategy. The responsible
tester sets the test strategy. A common dysfunction in our industry is when
the quality of the test process is controlled by people who are unable or
unwilling to take responsibility for it. This can happen when test processes

are dictated by management or developers over the objections of (or
without consulting) qualified testers.
We all know that it is irresponsible to give car keys to someone who is
obviously too drunk to drive. Yet, across the industry, many testers behave
irresponsibly on projects because someone in power told them to, even
though that person may not know the first thing about driving a test project.
Testers may do bad testing because they don't know any better. That is
forgivable. The bigger problem is knowingly doing bad work just to avoid
confrontation. Even that may be forgivable, if the environment is precarious
or abusive. But it's still not right.
Here are some things testers are typically bullied into doing:
Formalizing "test cases" before ever seeing the product and without
knowing how the product works.
Spending most of their time following scripts (which rarely reveal
problems) instead of doing open investigation (which frequently
reveals problems).
Creating reports showing test-case counts and pass/fail rates, which no
one outside the test team could possibly interpret correctly.13
Performing all testing within the confines of a certain preestablished
tool14 that systematically prevents them from pursuing any test idea
that won't fit into that tool.
Not reporting intermittent or uncertain bugs.
Adhering to narrow, legalistic ideas of quality that systematically
ignore important quality problems for real users.
Following processes or maintaining tests that they don't understand
and cannot vouch for.
We're not saying you should handcuff yourself to a laser printer every time
management ignores your advice. You don't have to sing "The
Internationale" when someone suggests a mild compromise in your work
that would preserve a certain harmony with the bosses. But we are saying
that testers should be reasonably assertive about their own work processes.

Practitioners should resist non-testers dictating our use of our time, given
that they have neither the training, nor the interest, nor the data to make
good decisions about testing. If tradesmen such as electricians and plumbers
control their own work (just try to micromanage a plumber, and you'll see
what happens), so should we testers. If developers control their own work,
so should we.
Develop your credibility over time and make your professional advice clear
to your clients. Put it in an email, if you feel they aren't hearing it—that way
you are on record in a way that cannot easily be denied later on. After that,
you can decide to go with the flow, and your professional conscience will
be clear.
Management also has responsibility as clients of the test process. They
should be creating a nurturing atmosphere for the testers. This is paramount
because the test process is highly sensitive to the apathy or antipathy of the
people around us. Testing requires us to be the bearers of bad news
sometimes, so let's try to be empathetic and patient with our clients as they
deal with it. But if every time we reported a bug, we heard a pained groan
or an irritated cluck, it would drag us down. Management and developers—
if they want the best information they can possibly get for the money—
must cultivate an attitude of "Thank you, tester!" about every report. If it's
not a bug, they should say, "I don't think this is actually a bug, but given
what you saw and what you knew, you were right to report it."
Testing is a responsible social process that allows our clients to
make timely, informed business decisions.
Students and Experts
To take testing seriously means to study testing. The authors of this book
are all students of the craft of testing. To be a student requires a certain
humility and faith that there is always more still to learn. On the other hand,
we aspire to be experts. An expert is someone highly qualified to be of
service. The word "expert" connotes wide and deep knowledge.
Expertise is sometimes tangled up with authority. But RST is a non-
authoritarian methodology. We say there are no authorities in testing.

Anyone claiming the mantle of authority is trespassing on public property.
Testing, like mathematics, belongs to nobody and to everybody.
We are inviting you to be a student and an expert. It's actually a bit easier
than you might think to be an expert at testing: just think in terms of scope.
It's one thing to be an internationally recognized expert, but you can also be
a local expert. You are probably already the most accomplished testing
expert in your own home, for instance. Or scope it to a particular skill: you
might be an expert in writing compelling bug reports, even if you don't
know much about stress testing mobile apps.
Here's another way to look at it. You might be more comfortable saying, "I
have expertise" rather than, "I am an expert." Having expertise feels a bit
less lofty of a claim, doesn't it?
There are also different kinds of expertise. Harry Collins makes a key
distinction between interactional expertise and contributory expertise.15 A
simple way to think about expertise is that contributory expertise is the
ability to do the work, while interactional expertise is the ability to talk
about it and make competent judgments about it.
The distinction between interactional and contributory expertise matters for
testers because testers must be able to debate with and persuade
management on matters of business value, as well as with designers and
developers on matters of technical detail. We don't necessarily need to code,
but we need to know about code. Testers need interactional expertise with
regard to the user domain as well.
Our point is this: if you want to aspire to excellence, you must convince
yourself that there is a path to it. That path must seem clear and available to
you. If you set foot on the path, you are, from that moment, both a student
of testing and a protoexpert in testing. You are learning and you are making
decisions and judgments. The scope of your expertise expands until one
day, strangers are calling you a testing expert, and you are being invited to
speak at conferences.
There are no hard goal lines on this path. For instance, no commercial
testing certification is credible. If you get one, at least don't take it seriously.
Certification agencies are just businesses trying to make a buck on the fear
and doubt of testers. They are authoritarians and opportunists who pretend

they represent the testing field. They don't help advance the craft. We
believe the low standards and confused mix of folk ideas they promote are
partly why the testing craft gets so little respect.
The materials of RST are designed partly to be a roadmap, to help you
identify and reflect upon skills that you might want to develop.
A good tester is simultaneously a kind of student and a kind of
expert.
What We Study
We testers study, learn, and critique. Study what? Learn what? Critique
what? Mainly, the product, in the broad sense of that term. But not just the
product.
Systems and Models
The work of testing is to burrow ourselves into a system and learn its status.
To do this, we need models of it. The outcome of this process is better
models.
Let's unpack that.
Reality is complex, and there's no way we can apprehend it all. We can't be
everywhere at once, so we can't see everything that exists in the world.
Even in the place where we are, we can't perceive everything. But non-
omniscience is both a bug and a feature. If we attended to everything
around us, we would be continuously overwhelmed. Thus, evolution,
culture, and experience have configured our minds and our senses to take in
certain kinds of data and ignore others. Once we've taken certain things in,
we attend to some of them and throw other stuff out.
This processing of reality via our senses may be conscious and
unconscious, systematic or haphazard, deliberate or spontaneous. The result
of that processing is not a perfect reproduction of reality in our minds but
rather a model of reality. It's a projection into a reduced-dimensional space.
It's a map.16 It's a simplification.

In his book The Problems of Philosophy,17 Bertrand Russell describes the
problem in detail. He speaks of the color of a wooden table and lists reasons
why it's hard to say exactly what the color is.
For most practical purposes, these differences are unimportant, but
to the painter they are all-important: the painter has to unlearn the
habit of thinking that things seem to have the colour which common
sense says they "really" have, and to learn the habit of seeing things
as they appear. Here we have already the beginning of one of the
distinctions that cause most trouble in philosophy—the distinction
between "appearance" and "reality," between what things seem to
be and what they are. The painter wants to know what things seem
to be, the practical man and the philosopher want to know what
they are; but the philosopher's wish to know this is stronger than
the practical man's, and is more troubled by knowledge as to the
difficulties of answering the question.
Reality exists, and we are all within it. But no one has direct or privileged
mental access to reality in itself. Other people have their own bodies,
minds, cultures, settings, and experiences that result in different perceptions
and interpretations. Someone else's model of reality will inevitably be at
least a bit different from your own.
Our definition of "model" is an idea, activity, or object that represents some
other idea, activity, or object. A good model is one that helps you to
monitor, comprehend, or manipulate the thing that it represents. This notion
of model is grounded in general systems thinking as well as cognitive
science. Models are not only the result of cognition; models also mediate
the products of cognition. Your models tell you where to look, what to look
for, and why it matters.
Models can be formal and explicit, such as a flowchart on paper or a
requirements document. (All specifications are models, as are lists of
features or user stories.) But the models we work with every minute of
every day are mental models. These are often informal and tacit. No
intentional testing is even possible without a mental model;18 and a bad
mental model means bad testing.

Differences between models should give us a reason to be somewhat
modest about our perceptions and conclusions—but they also suggest a
heuristic for improving them: it is essential to compare and contrast our
perceptions and conclusions with those of others. This is a central argument
for having independent and free-thinking testers on the team.
Let's look again at the reality we are trying to model. Testers study systems.
What are systems?19
We define "system" as a set of things in meaningful relationship with each
other. What constitutes "meaningful" is in the eye of the beholder.20 If you
have a set of rocks in a bucket, you could decide that it is not a system for
your purposes, even though there are identifiable relationships among them
(e.g., they come from the same area, share the same bucket, are in physical
contact, exert gravity on each other, etc.). Maybe none of the relationships
between them pertain to anything you care about. To you, then, it is an
aggregation of rocks—but not a system of rocks. To a geologist who knows
they were plucked from the same cliff, however, the rocks are part of a
system of local geology. They tell a story that goes far beyond the bucket.
For testing purposes, we are concerned with dynamic systems—wherein
changing the meaningful relationships causes change to the entire system.
Systems often present a hierarchical structure, as well: boxes within boxes.
We may not be able to see into those boxes.
The term "black-box testing" means testing a system without reference to
its internal workings but rather to its external functionality. Black-box
testing may be based on models such as your mental picture of the product
formed through interacting with it, lists of features, user stories, your
understanding of user needs and behavior, or your beliefs about how data
moves through the system.
The opposite term, "white-box testing," is really a misnomer. It should be
called "open-box" or "glass-box testing"—which is when you know exactly
how the system works. White-box testing is typically based on a detailed
knowledge of the source code (which is itself a model of the product), as
well as internal data structures.
It's common to have some idea how the product works internally, and if you
make use of that knowledge to plan your work, we call that "gray-box

testing." In fact, though, any system we test is gray to some degree—visible
in some aspects but invisible or even deceptive in other aspects. Even
white-box testing will treat third-party libraries and services as black boxes.
Systems exhibit behavior that might not be evident in or explicitly built into
any of their parts. This can take the form of performance quirks due to
mismatched timeouts or cascading delays (perhaps a process is fine on its
own but not designed to be called repeatedly or concurrently). Or perhaps it
may take the form of a failure to handle errors gracefully (because of
mismatched exception handling between components). Or perhaps two
parts of a system need the same resource at the same time.
Some developers think that end-to-end testing is unnecessary because unit
testing will find everything. This belief is caused by a lack of imagination
and selective amnesia—in other words, faulty models. On a regular basis,
we all experience interesting and important bugs that were caused by the
integration of parts that had been trouble-free on their own. But when it
comes time to imagine possible bugs, those integration bugs can be difficult
for programmers to remember, whereas simple coding mistakes with
immediate impact come readily to mind.
A model allows you to explore a system; exploring a system allows
you to model it better.
Products and Users
What we call a product is the totality of what is promised by a producer to a
consumer. This includes the experience of using the product, the product
documentation, the solution represented by the product, any support
services, the resource footprint of that product, and how that product affects
other processes.
The idea of a product cannot be separated from the idea of its users. How
users encounter the product dictates the value they'll get from it, as well as
the sorts of problems they are likely to have with it. When testing, we must
consider every kind of user, including:
Beginners vs. experts

Signed-out vs. signed-in
New vs. long-term
Differing payment tiers
Frequent vs. occasional
Active, suspended, or deleted
Returning (reactivated after suspension)
Incognito vs. normal
Admin vs. normal vs. guest
Verified vs. unverified
Children vs. adults
People with various disabilities21
Agents or operators vs. their customers
Local, regional, national, or international
Distracted and slow vs. focused and fast
Humans vs. software agents
This still leaves out one kind of user: malicious hackers. Yes, we may have
to be concerned about them, too, although systematic security testing is a
specialty skill.
See Chapter 4 for an in-depth analysis of how to think about "normal" users
and usage. Normality is a deep idea.
In RST, we have a name for testing that attempts to create the same
experience that a real user has had or will have: experiential testing.22 To do
experiential testing well, you must have great empathy for that user, as well
as insight into the user's tasks, needs, and desires. A big part of the
contribution a tester makes to a team is in representing users, but let this be
more than mere lip service. Learn everything you can about your users.
Who are they? What are they using the product for? What else is going on
at the same time for them? What sort of problems can they tolerate? When

these things are unknown or obscure to you, they will limit your testing.
You should warn your clients if this happens.
When testing, you must also be aware of the competitive landscape in
which the product will be used. Try to learn about similar products on the
market. Often that knowledge will allow you to make developers and
product owners more amenable to your enhancement requests.
A product cannot be evaluated without understanding its users.
Quality and Risk
In Jerry Weinberg's famous definition, "quality is value to some person."23
All other popular definitions of quality, he noted, are merely different ways
of saying that simple thing. For the purposes of testing, we like to
emphasize that quality is value to some person who matters.24 (Michael
likes to add "at some time" and occasionally a big list of other qualifiers.)
This means that the quality of a product is a dynamic relationship, not a
static object. Quality can go "up" or "down" even if the product does not
change at all—because the people who matter may change, or the world
may change around them.
Quality is also multidimensional, and there is no universal model by which
anyone can claim to integrate every dimension of interest to arrive at a
quantitative measure of quality.
There is a subculture in tech that loves to reduce everything to numbers and
disembodied logical propositions. They want to get everyone to buy into a
particular model of reality with particular rules, and then act as if that is
reality. This is known as the ludic fallacy.25 This manifests in software
testing when projects attempt to define quality in terms of specific,
predefined tests that must be passed or in the mere nonviolation of
explicitly stated requirements. It also manifests in AI research when results
are scored against arbitrary benchmarks.26
The ludic fallacy turns quality into a game of words and contracts. The
official idea of quality then drifts away from the feelings and opinions of
real stakeholders and real users. "Great!" you might say, "I don't like

squishy feelings and opinions." Well, you may not like them, but the world
of business and the marketplace for software runs on them. When people
say they don't like to deal with feelings and opinions, they invariably are
referring to other people's feelings. Their own feelings—including their
feelings about other people's feelings!—are perfectly acceptable. Now just
multiply that by all humans and ... welcome to software engineering as a
social science!
In any case, this is a fundamental tenet of RST: the motivation for human
behavior comes mainly from emotions. Although some might prefer to
believe otherwise, very little is dictated by logic.
In RST, we prefer to take a holistic and expansive view of quality. We say
quality cannot be measured (although some things bearing on quality might
be measurable). But quality can be assessed in context. Assessment is a
matter of judgment (see the section "Assessment and Measurement" later in
the chapter). It is partially subjective, but don't be afraid of subjectivity. The
flavor of strawberries is subjective; nevertheless, most people agree that
they taste better than broccoli, even with sugar on top.
A professional quality assessment is an intersubjective social construct
based on experiences with a product. This is a fancy way of saying that if
we test well and talk over what we found, the project team can come to an
agreement about releasing the product. Intersubjectivity means that even if
a group of people agree on something—whether it's the quality of a pop
song or a scientific theory—that doesn't necessarily mean that every
reasonable person in the world would share that belief.
Sometimes management will cite "continuous deployment" as an
imperative. This creates a terrible squeeze on the test process. "There's just
no time!" they say. "We have to automate the testing and automatically
release! Just put some automated checks into the pipeline to ensure quality."
Well, that's like saying, "I have to deliver this pizza in 30 minutes or less! I
don't have the time to drive safely!" This is simply irresponsible behavior.
Domino's Pizza got in trouble for that.27 By all means, work with a sense of
urgency and seek fast and efficient ways of working. In some situations, a
set of quick, simple checks is enough. But many things are complex and
high risk; a skilled and well-supported tester will know that "automated
checks in the pipeline" will not provide enough scrutiny for those products.

Testers do not control, assure, or ensure quality. Releasing the product is a
business decision, not a testing decision. Management is free to run the
business in a reckless way. The RST position on this is that we seek to test
well and report our results, so that management can make good and timely
decisions about the product. If anything prevents us from conducting a
reasonable quality assessment, we make that clear to management. What
they do next is out of our hands.
Risk is the flip side of quality. Our main concern in testing is product risk,
which is the danger of the product doing something bad (which includes
failing to do something good that it must do). Danger involves both
likelihood and impact of trouble, but in RST, we don't attempt to reduce this
to numbers. Except in rare situations,28 it is not possible to reliably and
usefully quantify the probability and impact of a given risk involved with
software.
Risk is about feelings of dread; it's about worry. Risk analysis is partly the
process of negotiating those feelings.29
The process of testing is the process of learning about the risks associated
with using the product. When we test, we either find trouble or we don't
find trouble. If we find trouble, then we feel more worried about letting our
customers use that product. If we don't find trouble, we must ask ourselves
if we've looked for it sufficiently hard. Therefore, testing is itself a kind of
risk analysis.
The outcome of testing is analyzed (or "tested") risk, but the potential for
risk is also an input to testing. This is necessary because we don't have the
time or means to test every variation of everything that the software could
possibly do. We are forced to make educated guesses about where and how
it will be most useful to test. Potential risk can't be the only input to the test
strategy, however—otherwise, testing becomes a self-fulfilling prophecy
(via confirmation bias), where we only look for bugs that we already
believe are present.
Just as identifying potential risk sets you up to discover actual risk, through
the process of testing, shallow testing sets you up for deep testing. This is
exactly what happens when you experience a problem but haven't yet
reproduced it. Seeing the bug once motivates you to investigate further—

you sense the risk. In that sense, high-level or shallow testing is a form of
risk analysis that helps you decide where to do deep testing.
In RST, we distinguish between unknown risk, identified risk, and tested
risk:
Unknown risk: We don't have any good idea of what the risk is or
where it might be hiding. The solution for this is investigation. This
may include unfocused or otherwise shallow testing. Example: "Could
there be an issue with performance? I don't know anything about that."
Identified risk: We have a specific idea what the risk may be, based
on some kind of evidence or intuition. Sometimes we call this a
potential risk. But we don't know if there are any actual bugs in the
product associated with that rumor of risk. If the identified risk is
severe, we perform more focused, deeper testing. Example: "I'm
worried about poor performance when people use our ticket-buying
system for very popular rock concerts. Peak loads in such cases are
astronomical."
Tested risk: If we have tested a product adequately, then we have
evidence-grounded opinions about the risk levels. The tested risk may
be high (because we found specific bugs that remain unfixed) or low
(because, despite our best efforts, the product didn't fail). Example:
"We tested performance under realistic conditions, and we now know
that it cannot reliably handle more than 250 users per minute. Our
servers will crash when Taylor Swift comes to town."
Every question about quality is really a question about risk.
How We Manage
Somehow, we organize and control ourselves as testers. How does that
work? It's not a Swiss clock, by any means. It's somewhat messy and
somewhat ordered. It's a complex process, but not a crazy one.

Context and Mission
Our operational definition of context is the set of factors that should be
considered30 when deciding what problems matter and what solutions
should be applied in a given situation. What's going on around you is
context. To some degree, you yourself are context. But not everything is
context—only relevant things are. The grass growing in the courtyard of
your building, for example, is not part of the context of testing.31
This is a context-driven book on testing. The first three principles of the
context-driven school of testing are
1. The value of any practice depends on its context.
2. There are good practices in context, but there are no best practices.
3. People, working together, are the most important part of any project's
context.
This has immediate implications. Among them is this: for work to be done
responsibly and well, each practitioner must be skilled enough to make
good choices or else operate under the direct supervision of someone who
is. Context analysis is pointless if you don't know enough to perform the
analysis competently.
Testing does not present many well-ordered problems. To test is not to walk
a Roman road—it's often more like cutting through a jungle. That means
there is no big book of excellent, precise answers to the questions you face
on your projects from day to day. And even if it were, it would already be
outdated.
Some practices are consistently good, of course, because the context that
makes them helpful is widespread and stable. Your own context might be
very stable—for the moment. But even if it is, you still have an ongoing
problem: what is the best you know how to do, given that situation, and
how might it be possible to do better?
Another concept that clarifies and organizes context is mission. Your
mission is the problem you agreed to solve for the client you agreed to
serve. The mission is a kind of contract. A mission cannot be foisted upon

you—you must accept it for it to become your mission. Similarly, you can't
just declare your mission—it must be agreed to by your client.
Mission is a core heuristic in RST. If you don't know your mission, the most
important thing is to negotiate one. The "rapidness" of RST comes partly
from optimizing your test process by asking yourself what is the simplest
thing you could do that would fulfill your mission.
With your mission in mind, you survey your situation—your context—and
decide what kind of testing is needed.
Here are some categories of context variables to consider:
Users and their environment
Product requirements and purpose
Complexity of the system
Development process and timing
Test team
Regulatory requirements
Need to maintain user trust
Need to exploit market opportunity
Amount of new code or change in the product
Amount of legacy code or stability in the product
Tools you might be obliged to use
Existence of engineering artifacts and documentation
Each category here affects our choices about test strategy and logistics.
Different industries also have characteristic contexts, including:
Government: Lots of regulations, highly locked-down environments,
moves slowly.
Financial, medical: Like government, but usually moves more
quickly.

E-commerce: Relatively unregulated; moves quickly; environments
are not as locked down; output is not super difficult to evaluate.
Scientific: Unregulated; somewhat chaotic; complex and mathematical
output.
Gaming: Security and performance are paramount; lots of usability
and charisma testing needed.
Embedded: Environmental testing is important; firmware upgrades;
hardware problems are often patched in software.
Mobile: Automation can be a challenge; variability of connectivity;
usability issues.
It's a very different prospect to test the Candy Crush game than to test the
flight control software of an F-16. It's even very different to test version 1.0
of Candy Crush vs. version 15.1.0.
Context informs everything you do in testing; mission focuses it.
Process and Methodology
People often use the word "process" to mean a described way of doing
things. They point to a binder or a poster on a wall and say, "That's our
process." We think that's a poor way to think of process, however. It's
disabling because it tends to obscure the bigger picture.
Our definition of process is simple and expansive: how things happen.
That's a bigger picture. Your process is not merely your intentions or your
procedures or your office equipment that facilitates all that. It's the totality
of how the past became the present in all the ways relevant to your work.
If you explain your process, that's a process description; it's a model, not
the actual process. It's a story, and every story leaves things out. The true
process of testing is complex and somewhat obscure; it's beyond words.
Still, you can learn to discern it and explain it well enough for all practical
purposes.
To get control of our test processes, we grow our skills and build tools. We
make agreements, apply methods, follow procedures, and engage in

practices. The general name for that is methodology—which can be thought
of as a system of methods. RST is one such methodology for testing.
If you want to improve process, you have to improve methodology, make
people better, or change the context in which the process operates. But we
think the first thing you must do is learn to see the process as it is. This is a
skill. It takes patience.
In the very first version of RST, before it was called RST, James ran
headlong into this problem. It was 1995 when he was chief scientist at
STLabs, and he was creating his first full training class for testers. It was
going to be called "Market-Driven Software Testing," and its purpose was
to show how to test software that had incomplete specifications and was
constantly changing—in other words, the kinds of projects we now call
"Agile."
James had gone through the preliminaries of describing basic concepts and
had come to the part where he had to explain how to test. At that moment,
he drew a blank. He had nothing to say. (Today, we would say that his
knowledge was entirely tacit, but he wasn't aware of how tacit knowledge
operated at that time.32) He experienced a momentary temptation to copy
stuff about test techniques from a popular textbook, but then he had a better
idea—one of the genesis moments for what would become RST: he decided
to go to the test lab and watch testers.
For days and days, he sat next to various testers doing outsourced testing on
different kinds of projects. He wrote down what they appeared to do and got
them to describe their thinking. Later, he would learn that he was doing
amateur social science fieldwork, but these were the days before Cem
Kaner opened his mind to the importance of ethnography,33 participant-
observer studies,34 cognitive task analysis,35 emic interviewing,36
ethnomethodology,37 grounded theory,38 and other flavors of qualitative
research.
It is surprising—no, shocking—how easy it can be to possess a skill about
which you have no ability to speak. But once you accept that, you can do a
lot to solve that problem if you are patient and willing to watch yourself and
others at their work.

Methodology is what you think should happen; process is what
actually happens.
Roles and Trading Zones
Decades ago, when James worked at Apple, his department had an
administrative assistant assigned to it. But no one ever explained to the
engineers in the group what the assistant was expected to do. They didn't
understand her role, so nobody asked her to help with anything. Meanwhile,
she didn't market herself to the group, either. After a little while, her job was
eliminated.
Roles matter. You are not your job, and also you sort of are your job,
because you inhabit a role. You need to understand that role, and for anyone
to work with you, they need to understand it, too. Your role frames what
people expect from you, and it's how they know when and how to approach
you for help.
What we mean by a role is a commitment to offer a service. Testing is a
service. When you agree to perform testing, you take on the role of a tester.
Roles can be flexible, but if you take on more than one role at a time, you
will divide your energy and focus.
The expectations of a role, in our view, are
Commitment: To play a role means to put your energy and focus to
fulfill the tasks that belong to that role. An immediate corollary to this
is that a role is not viable if its tasks require substantially different
focuses. For instance, a role that combined driving trucks with
supervising children may not be viable. A role that combines high-
pressure production coding with deep testing also may not be viable.
Competence: To play a role means to have the necessary skills to
perform the tasks in an acceptable way. For testers, it means
developing and deepening skills such as analysis, inquiry, experiment
design, and reporting, and the social competence to make judgments
about risk and the "enoughness" of testing.

Coordination: To play a role means you are available to be
approached regarding the tasks within the role, and you will respond
constructively to them. It means communicating and negotiating with
people in other roles, making it your business to know how they are
impacted by your work, so that they benefit and not suffer because of
it. It also means not bullying others and not allowing yourself to be
bullied—this can be a big issue for testers.
Readiness: To play a role means you look ahead at what's going to
happen and get ready to respond to it. When people do testing without
considering it their role, they tend not to plan ahead but rather to test
impulsively and exclusively in the moment. This means they can only
be ready to do small things. Getting to "ready" as a tester might mean
researching your competitors or creating a test data library. It might
mean constructing an automation framework or reading a lot of
documentation.
Some roles have thick boundaries. For instance, an airline pilot is not
allowed to let a passenger help fly the plane, even if that pilot is really tired
and the passenger really wants to help. Testing, on the other hand, usually
has a thin boundary, because almost anyone can help with testing.
When some people hear about roles, they instantly think of siloes,
windowless structures, and fortresses that no one is allowed to enter.
Indeed, some roles can be like that—a judge on a court case, for instance, or
a priest. When it comes to testing, some people imagine a high forbidding
tower in which Rapunzel writes test cases and bug reports and never lets
down her hair.
The testing role is neither a fortress nor a prison: it's a villa with big French
doors and loads of light coming in. It's got gardens. We hold parties there.
And when we invite our developer, manager, and customer service friends
over for a testing party, we make sure they enjoy themselves. Anyone can
help test the product, but people in the testing role are responsible for
getting it done. We testers do the paperwork, plan the logistics, mind the
strategy, and generally fix the roof and wash the dishes at the great villa of
testing.

Nothing about the role of testing denies teamwork—you know there's a
"but" coming—BUT what some people consider teamwork can hurt the test
process. Teamwork doesn't mean that we march in lockstep. To test well,
you must be allowed and encouraged to go on flights of imagination.
Seeking and finding trouble requires you to be divergent, while the rest of
the team is convergent. If teamwork means that people who aren't testers
get to dictate what testers do or discourage them from thinking creatively,
testers may lose their energy and sense of purpose.
Sociologists of science and organizational psychologists have studied the
relationship between scientists and the engineers who build instruments for
them. Out of this has come the concept of a trading zone.39 This is the
observation that people with very different cultures can nevertheless
communicate and cooperate across those cultural boundaries. One of the
ways this can be done is by the use of boundary objects, which are artifacts
or ideas that may mean one thing to one side and something else to the
other side.
If you are a parent, then you know all about trading zones. As your children
grow, they become more autonomous. Though you have power over them,
there are consequences of using that power. Mobile phones and text
messages become boundary objects. Privacy is a boundary object.
A test case is an example of a boundary object. To a tester, a test case is
usually a reference to a class of activities to test a certain thing. A tester
may or may not follow a test case exactly, but probably not. Testers
improvise and add things to test cases on the fly. The test is not the test
case, but rather what the tester does with the test case. By contrast, to a non-
tester, a test case is a crystallized unit of pure testing. It is self-contained
and independent of any tester. A non-tester typically assumes that any test
case, when executed, will have the same result no matter who the tester is.
It will have the same result every time.
Both sides refer to test cases, but they are not talking about the same thing.
This can be a good thing. It keeps the peace ... unless non-testers are much
more powerful than testers. When there is a power disparity, and one side
uses its power to control the other side, the trading zone doesn't work so
well.

RST asserts the value of a strong testing role. One of the tasks of
management is to cultivate a healthy trading zone between
developers and testers.
Tacit and Explicit Knowledge
One naïve idea about testing is that anyone can do it. This happens to be
correct. Every human child (and many an animal40) is born with the tacit
knowledge of how to test.41 Testing is a skill that we use in our childhood to
learn how the world works. That does not mean everyone does testing well.
Professional testing is on another level. To be a professional means good
testing, not just for yourself but on behalf of your clients; it means testing
meticulously and reliably.
Tacit refers to what we know or what we can do that is unspoken, unwritten,
and unpictured. Explicit is the opposite of that. Basically, if it is expressed
in a form that can be reduced to or transformed into a string of bits, it's
explicit. Otherwise, it's tacit.42
There are different kinds of tacit knowledge:
Somatic tacit knowledge is embedded in a human body and includes
kinesthetic knowledge, like how to balance and ride a bicycle. But it
also includes cognitive skills, such as how we choose our words when
we speak, how we experience surprise, or how we scan a browser
display and make sense of it. It is largely unconscious and can take a
long time to develop. Somatic tacit knowledge is beyond all practical
reach to make explicit.
Collective tacit knowledge is embedded in society and social
relationships—like how to negotiate a left turn on a bicycle in
Copenhagen, which is different from how it's done in Beijing. Another
example of collective tacit knowledge would be how to make a room
of people laugh.
Collective tacit knowledge cannot be reduced to any explicit set of
rules, partly because it's constantly shifting and being renegotiated. It's
learned by participating in social life, as we encounter and jostle each

other in the course of our lives. How close to stand to each other? How
loud to talk? When to interrupt? Which bugs are worth holding up a
release for? Yes, we can find heuristics that help us make these
judgments, but they are a poor (and exhausting) substitute for full
socialization.
Two kinds of collective tacit knowledge that matter to testers are the
ability to judge risk (Is this bug important?) and the ability to judge
enoughness (Can I stop testing now?). Both of these require social
competence because these decisions depend upon what your clients
and coworkers believe and value. They are not merely technical
decisions.
Relational tacit knowledge is the kind that is unspoken for the sake of
convenience and efficiency relative to the current situation. These are
things we could make explicit, such as how to reboot a computer, but
then we'd have a lot of documentation to maintain, and no one would
read it anyway. In highly regulated testing, more of this kind of tacit
knowledge must be made explicit. Even so, a whole lot will remain as
winks and nods—hence, the PHOSITA43 principle, which is a
mainstay of patent law.
Tacit knowledge cannot be imported or injected into you. It isn't really built,
either. It's grown, as a garden grows. Give yourself the right conditions, and
tacit knowledge takes root and flourishes. This happens mainly through
deliberate practice, shared experiences, and observing the work of other
people. It is also helped by conversation and personal reflection, which help
you process what you are learning.
RST is a methodology based on personal skill and knowledge rather than
something driven by algorithmic rules or documentation templates. We
present it as both a mindset (a way of thinking, a particular ontology of
testing) and a skillset (things you must know how to do).
Everyone has an intuition for testing, but most adults are disconnected from
it—because the process of growing up is also the process of learning how to
get along with other adults. To be accepted in social life, we can't be
constantly critiquing other people's behavior and challenging their choices.
Perhaps it's more accurate to say that anyone can test, but not everyone

wants to test. When we train people in testing, a lot of what we are doing is
getting students to remember how to test and how to feel okay about doing
it.
To be an excellent tester, you must bring your tacit testing skill into the light
again and develop it beyond its default level, while simultaneously
connecting it to explicit testing knowledge (so that you can explain and
defend your actions to a reasonable degree, and share what you know).
Now let's talk about explicit knowledge. Relational tacit knowledge can be
made explicit in the form of cognitive artifacts44—software, hardware,
formal documentation, checklists, data displays, etc.—which can then help
us think better and solve problems faster. Because cognitive artifacts can
help, some people think every important idea should be encoded in some
sort of artifact. But there is no evidence that making everything explicit
improves performance. Just the opposite, in fact. Studies of technical
documentation show that it's often ignored.45,46
In RST, we use the word "idea" a lot. An idea is in your head; it is not an
artifact. Don't confuse ideas with artifacts. For instance, most people think
of a test plan as a physical document. Not us. For us, a test plan is a set of
ideas that guide a test project. A test plan document is not the same as a test
plan.
A big part of the "Rapid" in Rapid Software Testing is our avoidance of
heavy documentation and our respect for the tacit knowledge of skilled
testers. We use lightweight checklists and mind maps, and we practice our
test framing.
Of course, there are times when heavier documentation is needed. If you are
or might be in such a context, you should know how to create the least
burdensome form of that documentation. Unfortunately, many projects rely
on online documentation and ticketing systems that grow sluggish under the
weight of many thousands of records. If you are stuck with such a system,
you have our sympathies.
If you do have a choice in the matter, think lean. Use plain text or markup
documentation that you can store in a Git repo. Many practitioners of RST
favor mind maps, and spreadsheets are popular for creating tables or lists of
test ideas.

We also think video documentation can work better. This can take the form
of recording your screen as you test, or making short (maybe 3 to 10
minutes) explanatory videos, showing how you use a test tool or perform a
kind of test. Videos are quick to make, if you can avoid editing, and seem
less intimidating for many people than the prospect of writing a big
document.
A final thought about the relationship between explicit and tacit knowledge:
We must not underestimate the power of a label. It's fashionable to decry
labels as limiting for people, which they can be. They are also immensely
powerful and useful. We have many ideas and skills that we don't put into
words, and that's okay. Still, a word can be a net, a magnet, or a springboard
that helps us access our knowledge on demand. We believe that expertise in
testing requires a combination of tacit knowledge, developed through
observation and practice, and explicit knowledge, developed through
introspection and study. This is why, in RST, we emphasize both practice
and vocabulary.
Without tacit knowledge, you can't solve hard problems; without
explicit knowledge, you can't talk about them.
Freedom and Formality
Consider the term "exploratory testing." Dictionary definitions of
exploration focus on moving through a space and learning about it. But that
is almost synonymous with testing. All testing is exploring a product in the
sense of moving through it and seeing things. "Exploratory" seems
redundant. Why did people start using the term "exploratory testing" in the
1990s?
We adopted that term because of a certain implication about exploration,
which is that an explorer has agency. To explore is to make choices about
what to do next as you encounter the unknown. If you make those choices,
you are the explorer. If someone else makes them for you, they are the
explorer, and you are their instrument. The rovers on Mars are not exploring
Mars—scientists on Earth are using the rovers to explore Mars.

Some of our rivals in the industry denigrate exploratory testing as mere
"play" or "unstructured" work. This was not an unreasonable concern in
1993, but we have come a long way since then in how we understand and
manage human agency in testing.
When Cem Kaner coined the term "exploratory testing," he was reacting
against the professional testing culture of the 1980s, which was obsessed
with formal documented procedures. He wanted to talk about testing that
was in the control of the tester, in every moment. The industry wanted a
closed and super-predictable test process. Cem wanted to open it up. He
introduced the concept of exploratory testing to the industry specifically to
emphasize tester agency and responsibility. He chose the term "exploratory"
to echo an approach in statistics called exploratory data analysis.47
The authors of this book believe that even though testing can be scripted to
some degree (at the very least via unconscious biases), good testing is
always an exploratory process to some degree. Indeed, exploratory
processes pop up everywhere. Studies of scientists in action also show that
exploratory science is the norm, not the exception.48 Books about how to
prove mathematical theorems also emphasize exploratory processes.49,50
If you want to understand testing or any technical work, you must come to
terms with the nature of choices, the structures that guide choices, and the
structures that preempt choices altogether. Learning to navigate and control
these things is absolutely fundamental to great testing, because unlike any
other role in projects, testing is all about freedom of thought. We might
apply algorithms as we test, but testing itself is anti-algorithmic.
Formality is the opposite of freedom. Freedom means anything goes;
formality means things are fixed in a certain pattern. We are free in many
respects, but when we attend a formal dinner party, we must wear certain
clothing and assume certain manners and poses. We can freely express
many ideas in a formal language, but not every possible idea, and not in just
any old way. To formalize is to reduce entropy—the number of states a
system can have—so as to create a useful order.
In RST, we call the space between perfect freedom and perfect order the
formality continuum (see Figure 2-3).

Figure 2-3: The formality continuum
Good testing evolves. It begins playfully and wildly, then settles down. The
process of settling down is a process of discovering patterns that efficiently
collect the evidence we need. Primary testing (the kind of testing you do
when you first encounter a product) is generally more exploratory than
regression testing.
Freedom is good because it gives us access to all possible bugs. Freedom
also lowers the administrative costs of testing, allowing us to do more of it.
Formality is good because it gives our process stability and integrity51,
allowing us to know and to show that we did in fact test what we think we
tested. Formality may help us scale our test process in certain ways, but
generally speaking, formality raises the administrative costs of testing. Just
imagine if you were not allowed to speak to anyone unless you first filed
your message with a government agency and received explicit permission!
Similarly, management's insistence that all test cases be documented before
being performed has a terrible chilling effect on the process.
In RST, we oppose premature formalization and instead suggest progressive
formalization, which is a concise way of saying "postpone formalization as
late as you reasonably can, so that formalization doesn't stop you from
inexpensively learning the things you need to learn." As we like to say,
don't follow any procedure that didn't follow you first.
Begin with an open process and then progressively formalize, in
small leaps, as needed.

Spontaneity and Deliberation
Whereas the formality continuum is about having choices, there is another
pressing issue: how are decisions formed? We call that the deliberation
continuum: on one side of it, ideas can come from "nowhere" (popping into
your head, as it were); on the other side, ideas result from methodical
deliberation.
Researchers have proposed that there are two different thinking systems
inside of us, boringly named "System 1" and "System 2."52 System 1 is
associative, largely unconscious, and very fast. Spontaneous ideas come
more from System 1. System 2 is logical, largely conscious, and slow.
Deliberation is the plodding work of System 2.
The two processes feel starkly different. Spontaneous decisions seem to
come as gifts from Thalia, the Muse of Testers.53 They bubble out of your
intuition and present themselves ready to go. They seem to give you energy.
Testing based on spontaneous test ideas feels light and fun. However,
spontaneity may lapse into chaos. It can lead you in circles and get you lost.
Deliberation (de-liberation!) often feels heavy. It may need substantial
preparation and incubation. Working through the logic can sap your energy,
as if you are walking through mud that wants to suck the boots off your
feet. But the advantage of deliberation is that you know where you've been
and where you are going. Deliberation fosters order.
The table in Figure 2-4 shows how the two continuums can work together:
The left column is less expensive and more flexible. The right side is more
conducive to automated checking and more legible to outsiders.

Figure 2-4: Formality with deliberation
For most complex projects, we suggest starting in the top left and
expanding down and over, refining as you go, toward the bottom right.
Avoid the top right—but that's where you will get stuck if you create test
cases just for the sake of looking productive to management.
James has done incredibly detailed testing work on court cases, for which
he had to provide hours of testimony under oath and, in one case, a test
report that was about three hundred pages long, with almost that many
footnotes. Even in that situation, his testing began as playful exploration.
The formalization came later.
The lower right quadrant of the table is not inherently superior to the other
quadrants. It's not the destiny of good testing to end up there. Your project
may deploy automated checking to support deep testing. If so, the checks
must be designed carefully. A core heuristic of RST is to avoid formalizing
before you've done enough informal experimentation with the product to
know what kind of formalisms to deploy.
Formalization ought to be based on informed deliberation; otherwise, you
are creating a monster. The authors have seen too many projects with

thousands of casually created, poorly understood test cases or test programs
that no one has the time to fix nor the courage to delete.
One of Michael's clients had a suite of automated checks that took many
hours to run. With only a little review, Michael discovered why: a 20-
minute setup step—that was needed once—was being run twenty times!
This happened because a group of automators, working too far away from
other testers and developers, had inherited a body of test code to maintain
and build upon. They didn't understand it, they hadn't evaluated it, and they
hadn't considered risk.
Deliberation always happens under conditions of bounded rationality. We
don't have unlimited time and resources to find optimal solutions for the
open-ended problems of testing. Herbert Simon,54 who won a Nobel Prize
for his work on how organizations make decisions, argued that deliberation
is governed by heuristics, time constraints, and incomplete information. He
coined the term "satisficing" to describe the process of seeking good
enough answers via bounded rationality. Gary Klein55 has developed a
model called Recognition-Primed Decision-making to describe how
experts, such as experienced firefighters or nurses, make life-or-death
choices in emergency situations. We have found the work of these men to
be an excellent antidote to the idea that elaborate written plans are a "best
practice" in software projects.
Most books in the technical field imply that every decision is (or at least
should be) carefully deliberated. This is a corrosive myth. It persists
because it provides managers and consultants with a cheap way to complain
about workers. Don't fall for it. Look around you: deliberation and
spontaneity are collaborating everywhere. We contend that good technical
work—and good testing—depends on that collaboration.
Excellent testing depends on a productive mental rhythm that
alternates between spontaneity and deliberation.
Iteration and Emergence
"Do it right the first time!" is an admonition based on the false belief that
things can be done right the first time. There are some exceptions: If the

task is trivial, it may be possible to do it right the first time (for example, an
infant crying for the first time). You may be able to do something right the
first time if you're lucky. Once, on the first pitch while playing baseball
with a kid, Michael hit a tennis ball and watched it drop into the center of a
garbage can 60 meters away—whereupon, having done something perfectly
on the first try, he decided to retire and end the game. But most of the time,
the idea that you can get something right the first time is a fantasy.
The first time James piloted an airplane by himself, he wasn't killed. Is
flying an airplane trivial? No. Was he just lucky? No. But it wasn't the first
time he had flown that airplane. The first time he flew, it was with an
instructor. By the time he soloed, it had become trivial to be without an
instructor. People who routinely "do it right the first time," have, if you
look carefully, either done that thing many times before or have been
meticulously instructed by those who have.
As we develop something new, confusion is normal. Confusion reigns at the
beginning of every project. We might even call this necessary confusion. Of
course, not everything at the start of a project is confusing. Plenty of things
about projects are trivial. There will probably be chairs and tables involved,
for instance. This will not throw you.
In RST, we embrace what we call the bootstrap approach to projects, which
is summarized in the aphorism begin in confusion, end in precision. We
begin without an efficient and effective process in place. We cope with that
by applying heuristics: patience, resilience, and open-ended learning
techniques. We investigate the situation; we explore its spaces and
constraints.
Bootstrapping doesn't require you to do things right the first time. It asks
you to try things, learn, adapt, and grow. It is a feeling like being at the top
of a whirlpool, slowly going in circles. You might feel that there is little
progress at times. But if you persist, this whirlpool will pull you in, and you
will begin to feel productive. If you have ever learned to play a musical
instrument, you are familiar with this sensation: at first, practice is
drudgery, but eventually, it becomes a pleasure.
Test plans, strategies, and artifacts should be allowed to evolve. None of
them will be right the first time. Or here is another way to say it: if you

want to take testing seriously, then the early versions of any test artifact
should not be taken too seriously.
Every testing process is emergent. When you are presented with something
new to test, you must develop your mental models of it, as well as your
strategy for testing it. Let your ideas incubate and emerge as you try things.
You may have test ideas right away, but you will have better ideas later.
This is why, instead of up-front formalization, we advocate progressive
formalization through iteration.
By iteration, we aren't necessarily talking about anything formal, such as a
series of sprints. We're talking roughly at the level of work sessions. For
instance, this very text you are reading came into being through something
on the order of ten work sessions, including all the stages of review. And
this particular sentence might not even survive to the final draft.56 Iteration,
in our terms, means revisiting and refining over time.
A core heuristic of productive iteration is what we call alternation, which
means rapid switching among complementary alternative methods. This
helps drive progress and protects you against tunnel vision. Examples of
alternation include:
Working alone vs. working together
Testing spontaneously vs. testing deliberatively
Reading specifications vs. interacting with the product
Performing tests vs. reporting on your testing
Great testing emerges from a process of iteration. Be patient with it.
Control and Legibility
Your bosses want to control you! That sounds ominous, but it's perfectly
reasonable, depending on what they mean by control. Every business has an
interest and a right to direct the behavior of its workers to some degree. But
people who think for a living, such as testers, cannot be directed very
closely. This is partly because it's too expensive—you'd have to sit next to

them in real time and tell them exactly what to do—or because fully
competent thinkers are insulted and demotivated when they have no agency.
If test procedures are dictated from outside and above, they will be brittle
and shallow. You can't get good testing that way. Some companies make
general rules and expect them to be followed. This is management by
legislation—for instance, saying that "all testing must be encoded in test
cases." We even heard of a company that insists on achieving 90% code
coverage in unit testing—and the developers realized they could satisfy the
"coverage" goal more smoothly by removing all assertions from the "tests,"
which meant they would always pass no matter what happened. No time-
consuming investigation required! Legislation and tame compliance lead to
poor testing.
RST suggests coordination or delegation, instead (see Figure 2-5).
Management by coordination means that testers work relatively
independently yet collaboratively, and they are expected to proceed in a
generally legible way that can be reviewed and questioned. For this process
to work, practitioners must earn trust, and management must be willing to
extend it.
So what is legibility? Legibility is about reading, in the broadest sense of
that word: reading situations, understanding status, categorizing events and
things without having intimate knowledge of them. Legibility is also about
outsiders making sense of insiders.57

Figure 2-5: Levels of agency in organizations
If your work is illegible, that means other people don't believe they
understand what you do or how you do it. If your work is legible, then they
believe they know.
Testing, like a lot of creative work, is hard to make legible. Developers also
have this problem, but they can always point to the product they have
created. "See? I did that. Do you really care how I did it?" they can say. But
the important products of testing are not tangible, other than, maybe, bug
reports. For some people, this creates pressure to write test cases and create
test code, even if it doesn't help them find more bugs. These kinds of
products create an illusion of productivity.
In some parts of the industry, legibility becomes an obsession such that
people will deliberately do bad testing just because it is easier to explain
than good testing. In medical device testing, for instance, it is common to
say that all testing must be formally documented and traced to
requirements.
The reason for this is to make the testing legible to auditors who work in the
public interest. But those auditors are only interested in the testing required

by law or by compliance with any standards that you claim to be following.
If you do what they consider to be extra testing—which includes all kinds
of testing that might find big bugs—they don't care, and that stuff doesn't
have to be legible at all. In other words, auditors are concerned about
certain things that are necessary for their purposes, but may not be
sufficient for our business.
Some kinds of "extra" testing you might do include highly exploratory
testing that we do as part of designing the formal testing protocol, in-depth
investigations of anomalous behaviors, observations of customers using
early versions of the products, or bug-hunting parties. Such activities are
not necessarily required by law, but they might be needed if you want to
stay in business.
Therefore, it's not true that all medical device testing must be documented.
We can do the stodgy and expensive formal testing while also doing
relatively inexpensive and highly exploratory testing that hunts for bugs.
In RST, we try to lessen the need for legibility by increasing our credibility.
But there are also things we can do to create legibility without lapsing into
ceremonial paperwork. For instance, we can practice taking good notes. We
can track our time and map that to our test strategy. We can use recording
tools to log how we test.
While management and developers will try to impose process and
documentation requirements on testers, partly to serve their need for
legibility, it's important for testers to push back and find alternative
solutions when being legible means doing bad testing. See Chapter 5, "How
to Account for Your Work," for more on these issues.
When we make our testing legible and develop our credibility, the
people around us will support our process.
Stories and Strategy
Jerry Weinberg once said to James, "We don't manage projects; we manage
stories about projects." He meant that all of our management of projects is
filtered through and accomplished by the use of stories. By "story," he
meant any account of or assertion about the project. A story is someone's

interpretation, someone's simplification and harmonization of disparate
sense impressions and reports, filtered through a rich field of models and
values.
That concept of a story, which may be partly fictional and partly factual, is a
central feature of management. It's absolutely a central feature of RST. You
could say that the purpose of testing is to transform a naïve story about the
product (e.g., "I am done with my new feature!") into an evidence-based
story (e.g., "We have found these five problems that we think you'll
consider show-stoppers, and here's why. And here are fifty-two others that
you'll probably want to look at."). As we do so, we also compose a story of
how we tested (a test report) and why we tested in that particular way (a
risk-focused test strategy).
People often talk about "traceability" in terms of associating a test case with
an explicit statement in a requirements document. We have something more
comprehensive than that in RST. "Test framing" is the ability to explain,
from top to bottom, how your testing relates to the business—why it
matters.
Test framing is an important skill. You must be prepared to do it — but you
don't have to do it all the time.
In Introduction to General Systems Thinking, Weinberg tells us that one of
the first principles of general systems is ignoring things. Knowing what to
ignore and when to ignore it—that's the trick. That particular trick is
essential to storytelling and is why attempts to write software that reports
bugs have fallen flat. A bug report written by an algorithm is essentially a
log file with some reference to a spec. Is it an important bug? Are all the
important details provided, and nothing else? The algorithm can't answer
these questions because that would require social competence.
It turns out that strategy is strongly related to storytelling and the art of
simplification. A strategy is not a detailed set of instructions. Rather, it's a
set of patterns that guide those instructions. The patterns themselves need to
change as the strategic situation evolves. The patterns of strategy are a
response to patterns in the problem to be solved.
You can't test intentionally without some sort of a strategy. The very act of
saying, "I will test that by doing X (instead of Y)" indicates the presence of

a strategy in your mind, however primitive or spontaneous. RST encourages
you to deliberate on your strategy and cultivate the skill of explaining and
defending it. This means you will have to develop rich mental models of
your product and then summarize (read simplify) that complex product in
terms of its essential components and features.
If your strategy remains spontaneous and tacit, that's not necessarily a
problem—until the day management complains that your testing takes too
long, or maybe you have 10,000 test cases and someone asks how you
know you need all of them. What will you say? A good strategy makes
testing more controllable and legible.
To test is to produce a true and helpful story about the status of a
product. A test strategy is a story about how you will do that.
Heuristics and Skills
Many people struggle with a sort of paradox. They enjoy the freedom of
spontaneous, playful work but dislike the chaos and seeming randomness of
it. So, they make rigorous procedures for themselves—and then don't
follow the procedures! They bounce back and forth between freedom and
bondage in this way.
Heuristics are a middle way.
Heuristics are absolutely fundamental to testing. They are fundamental to
engineering, science, and even the practice of mathematics. In Exploring
Requirements: Quality Before Design,58 Jerry Weinberg and Don Gause
speak of requirements analysis as being necessarily heuristic. In Discussion
of the Method,59 Billy Koen shows that all of engineering is heuristic. Even
hard mathematics is suffused with heuristics.60
But it's surprisingly difficult to find a clear definition of heuristic that
reflects how the term is used in social science and engineering. Years ago,
James worked with Cem Kaner to review as much of the literature on
heuristics as they could find. Out of that came a simple definition that we've
been using in RST ever since: A heuristic is a fallible means of solving a
problem.

In other words, if you have a way of solving a problem, but it is not
guaranteed to work, you have a heuristic. Please note that we mean "solving
a problem" in the broadest sense, which includes understanding situations,
discovering problems, making decisions, etc. Thus, to find a bug is to
"solve the problem" of not knowing about that bug! Lots of things are
heuristics, and we use the word heuristic a lot in RST.
Many people who talk about heuristics think of them only in narrow ways.
They think a heuristic is just a "rule of thumb" or some rule drawn from
experience. Yes, those things can be heuristics. But you can also have
mathematically rigorous heuristics. An algorithm can be used as a heuristic.
A hammer is a heuristic for putting nails into wood. Conversation is a
heuristic for coming to agreements. A mnemonic is a heuristic for
remembering something. A bridge is a heuristic for crossing a river. Strictly
speaking, it's hard to imagine infallible methods for satisfying our
customers. There are certainly no infallible methods of testing.
But if everything is a heuristic, why bother talking about them? The reason
is that when we reflect on the heuristic nature of our methods and tools, we
remind ourselves of how they can fail, and thus we can be more careful
with them. With judgment and skill, we can make them less likely to fail.
Heuristics must be guided by people, or else the process will be reckless
and negligent.
Heuristic practices are different from "best" practices. The difference is
agency. With a "best practice," some authority figure comes to you and
demands that you comply with the unaccountable judgment of anonymous
people. They say that these practices are "proven," yet they never produce
any proof.
When we come to you with a heuristic, we're saying, "Hey, be careful with
this. Here's how it works, why it works, and how it might not work." In
other words, we help you connect that heuristic to context (hence, context-
driven testing). If we fail to explain the heuristic to the point you feel you
can apply it safely, then it would be irresponsible for you to use it at all
(without supervision).
Since the fabric of testing is woven with social judgments, and social
judgments are never a sure thing, testing is an especially heuristic process.

Even code is a heuristic. We may use a lot of automation in our testing.
Automation is algorithmic and highly deterministic. Automation in testing
is still heuristic, however, because it can't guarantee that it will solve the
testing problem. At best, automated checks find the kinds of bugs they were
designed to find, while ignoring other kinds of bugs. Whether, how, how
much, and when to apply particular forms of automation are all matters that
generally defy any attempt to claim there is one best way to do it.
You need skills to apply heuristics. Skills and heuristics go together like
matter and energy—they are inseparable. Heuristics are born from skill,
initiated by skill, guided by skill, and ended by skill. But heuristics can also
be the midwife for new skill.
For instance, when James was learning to drive, his older brother gave him
a heuristic for knowing when to shift from first to second gear: "Shift as
soon as the car is in motion." This was helpful for that short time until
James learned more about the dynamics of automobile transmission and
discovered there may be reasons to stay in first gear longer.
Skill is developed in various ways. In RST, we emphasize deliberate
practice, which means assessing yourself, choosing to improve your skill,
and then engaging in practice that specifically targets that skill. As you do
so, you will also develop heuristics. It's helpful to gain experience in
general, but deliberate practice is what will carry you most quickly up the
mountain of expertise.
Testing is a skilled activity. RST offers a variety of heuristics and
heuristic models designed to help you develop and apply your skills
in a systematic way.
Activities and Enoughness
Testing happens in space and time. For RST, the natural way to speak about
testing is in terms of activities, which interact with the world in real time
and cause change, rather than artifacts, which just sit there. Each activity
has a purpose; each is performed by a specific person; each mitigates some
kind of risk. In other words, we define a test activity as a tester spending
time on something to fulfill the mission of testing.61

We've already talked about the freedom and formality, deliberation and
spontaneity, iteration and emergence. Keep all that in mind. Now, add to
that a new pattern: goal-oriented activities can be divided into four threads:
1. Commit: The act of deciding to put time and energy into the process,
and the level of that engagement. To the degree you are committed,
preparation, performance, and delivery will be possible.
2. Prepare: Energy you invest to improve the efficiency and
effectiveness of your performance. To the degree you prepare, your
performance and delivery will go better.
3. Perform: Doing the activity itself, bringing about the outcome. The
quality of your performance makes for good delivery.
4. Deliver: Bringing the outcome successfully to your client. Incremental
delivery can help manage expectations and improve quality.
These threads are almost never strictly sequential. In fact, they are roughly
concurrent. These threads may be found in each iteration of a project or
task, and iterations may also occur within them. Technical work is complex.
If you seek excellence in technical work, you can't just ignore that
complexity and hope it won't matter. Anyway, we don't.
A vital aspect of these four threads is enoughness62. You must check in on
these threads each day, one way or another. For example:
Are you committed enough? Commitment will wax and wane
throughout a project (you may have to multitask). Are you committed
to the right things today?
Have you prepared enough? Or are you perhaps too obsessed with
preparing? (Do you really need those written test cases?)
Have you performed enough? Maybe you can finish now. Maybe
you don't need to polish it so much. Or maybe you need to admit that
the activity is far from finished and you need another day or week.
Have you delivered enough? Could you deliver a partial outcome
early and get some feedback? Is your client satisfied?

Enoughness is a thread that winds through every aspect of RST. It goes
along with heuristics. Basically, enoughness is about knowing when to stop.
We need enough quality, testing, skill, focus, etc. But we probably can't
afford all of it.
As you look through the various heuristics and methods that comprise RST,
bear in mind that applying this stuff is an ongoing exercise in assessing
enoughness. The basic method we use is to visualize what way too little
might look like, and then we imagine way too much. We think about the
obvious ill effects of too little or too much, as well as the subtle chronic
effects.
This is one reason we avoid speaking of "best practices." You will find no
talk of best practices in this book.63 Best practices, by definition, are all or
nothing. Otherwise, what does "best" mean? Do the best thing! No excuses!
We speak of heuristics instead because we want you to develop judgment
within your own context.
Someone may say that "automating test cases" is a best practice. We say,
why? What kind? As opposed to what? How do you know? And ultimately,
how much? When you think in terms of enoughness, all best practices melt
down into heuristics, your agency comes to center stage, and you attain the
soul of a craftsman.
Testing can be managed and reported in terms of how you spend
your time, rather than how many test cases you created.
How We See
A frequent motif in testing iconography is the magnifying glass. We also
suggest a lighthouse. This is because testing has a lot to do with seeing. In
testing, how we see is a matter of fundamental importance.
Stance and Distance
In the psychology of problem detection, mental stance is key.64 Stance is
your mental orientation to the situation, which includes your willingness to
believe that a problem may be present. Good testing requires a suspicious

stance, actively looking for trouble. This is different from the stance that
developers must have to be productive. For a developer:
The coding process requires a mental flow state that is easily disrupted
by open-ended bug hunting.
A lot of trouble that occurs is fairly obvious—simple, shallow testing
finds it immediately.
A suspicious mindset slows down coding due to the well-known
dynamics of signal detection.65
Actively looking for deeper trouble takes time that could otherwise be
used for implementation.
Elusive bugs are usually less important than obvious bugs.
Deeper testing gets easier when the product is more fully formed.
Therefore, it may be rational for a developer to postpone deep testing
and to generally trust the libraries and components in the tech stack.
Systematic studies of developer behavior are hard to come by, but the ones
that authors found corroborate our observations that developers tend to do
shallow testing66 and overestimate the testing they do.67
While developers must find one good way of making the product work,
testers must explore then many ways it might not work. This is an open-
ended problem. It includes the consideration of counterfactuals,68 which
Nicholas Taleb calls "alternative histories."69 You have to imagine how
things could have gone wrong, even when you perform a test where nothing
appears to have gone wrong.
Good testing also requires an outsider stance. This is harder for a developer
to achieve than for a tester. The developer builds a mental model while (or
before) writing code. That means a developer testing their own code is
testing something they already believe they understand. That's why
developer testing tends to be especially simple and positive in nature—they
are not expecting the test to teach them anything. Developers testing their
own work must try to alienate themselves from that work—to get out of
their own heads.

By contrast, a tester is always, to some degree, using the test process to
learn how the product works, rather than merely checking that it doesn't
violate some requirement. A tester is already outside of the developer's
thought process.
In other words, testers have critical distance. What's that? One definition,
from sociology, is "distance from an object or event needed for the
appropriate sensitiveness, rigour and incisiveness in considering that object
or event to be possible."70 But this definition doesn't tell us anything about
the nature of distance. From the world of clinical ethics, there is the
suggestion that distance may be the difference between an outsider's
perspective and that of an insider71 or perhaps the difference between the
mentality of an adviser and that of a practitioner.72 In literary criticism,
critical distance could involve epistemic virtues such as autonomy, which
means protecting yourself from being influenced by any other opinion or
avoiding conflicts of interest.73
Let's say distance has two meanings here: difference and detachment.
Critical distance is the ability to think differently from another person and
to investigate a matter without prejudging it. Testers have critical distance
because they are outsiders and because they are not emotionally attached to
a bug-free product.
Ah, but is there such a thing as critical proximity? Yes! Let's define that as
the similarities that allow you to understand and empathize with someone,
and the immersion that allows you to study your subject in detail.
Developers have critical proximity to their own work. Developers often
have critical proximity to each other, at least when compared to testers.
Proximity and distance have complementary virtues. For example, it can be
good to study the code of the product you test, but it can also be good to
ignore it. As with most things, we suggest alternating between distance and
proximity, if you have a choice about it.
We will end this section with a note of caution. Everyone wants to be liked
—loved, even. We all enjoy the feeling of being understood and respected.
In other words, distance can make us sad. Being an outsider is no fun. Thus,
there's a constant temptation on a team to bond with our teammates in a
way that vaporizes our critical distance. For testers, this is a dangerous
temptation.

We need to preserve our critical stance. We need to manage our distance.
We somehow need to convey that we care about our teammates and clients
without letting their ways of thinking colonize ours. It's an ongoing
challenge. One of the areas in which RST sharply breaks from the culture of
Agile software testing is that we are far more worried about testers turning
into cheerleaders for the developers, instead of being that critical eye that
wants to see trouble.
Good testing requires the adoption of a vigilant stance and the
maintenance of both critical distance and proximity.
Focusing and Defocusing
To focus is to put your attention on a particular thing instead of other things.
Focusing, for testing purposes, means the specific way that you are
modeling the product, the particular heuristics you are applying, and the
specific parts of the product you are looking at.
Focusing is a good thing. Focus allows you to notice subtle bugs and to
carefully investigate bugs.
Focusing may be effortless, too, at least in one sense: because you have no
choice but to focus. No one can see everything at once and do everything at
once. Of course, the trick is to focus well; to see the things that matter.
Some kinds of focus are more difficult than others, and consciously
controlling your focus can be difficult.
Defocusing is another kind of thing. By this, we mean changing to an
alternative and complementary focus. Defocusing is very important in test
design and performance because it tends to prevent the great occupational
hazard of testing: tunnel vision.
Ideally, defocusing gets us out of ruts, while focusing puts us into a groove.
The problem is, how do we tell the difference between a rut and a groove?
The answer is, we can't know. A rut is a groove we shouldn't be in; a groove
is a rut we should be in—but they look and feel the same. If you feel
annoyed with a certain kind of testing, that could be a rut, or maybe you just
haven't tried hard enough. If you are happy with the way your testing is

going, that could be a groove, or maybe you are unconsciously avoiding a
better kind of testing.
There are several heuristics we can suggest for dealing with this:
Sometimes, follow the energy: Do you feel like you are in a rut? Then
it's okay to quit. Do you feel like you are in a groove? Then keep it up,
trooper. In general, confusion is a signal to focus, and frustration is a
signal to defocus.
Sometimes, focus for its own sake: That means sticking with a plan
that makes sense to you, even if you feel tired of it. This is the "don't
quit" attitude.
Sometimes, defocus for its own sake: That means periodically taking
a break from what you are doing. Get out of the groove for a while and
try something else. Then check back later to see if you want to return
to the original course.
Sometimes, seek advice: Maybe someone else, who has critical
distance from you, can help you see the difference between ruts and
grooves.
An example of a focusing tactic in exploratory test design is the One Factor
at a Time approach. You change one variable and see how that affects the
output, then change another. This is slow and meticulous, but it helps you
investigate a bug or study a behavior. By contrast, you could use Many
Factors at a Time, which defocuses by randomizing or otherwise modifying
lots of things between tests. This would play havoc during bug
investigation, but can increase your chances of bug detection during open
testing, especially for unanticipated bugs.
To reduce chaos and confusion, focus; to discover new things,
defocus.
Experience and Instrumentation
We have no special definition of experience within RST. However, we do
use the word in several different ways, consistent with the Oxford English

Dictionary. For us, experience is knowledge resulting from actual
observation or from what one has undergone, which is the hallmark of
empiricism. Experience is also the fact of being consciously the subject of a
state or condition, or being consciously affected by an event—an example
of which is how it feels to use a product.
We don't say manual testing in RST. We do use the term experiential
testing, which means testing in a manner designed to replicate the
experience of a kind of user you have in mind.
Experience comes through our senses, of course—principally vision.74 But
our eyes are easily fooled, and our memories tricked. To test experientially
means you will encounter the product that the user will experience, but you
won't necessarily notice the things you encounter. To be a good tester is to
be aware of the perils and limitations of first-person real-time observation75
and to practice doing it.76,77
Anyone who carefully tracks their experiences with experiential testing
quickly discovers that it is shockingly easy to miss any given bug that later
will seem "obvious." This is not an argument for "automating all the
testing," though, because our unreliability is more than matched by our
amazing ability to notice problems that no one ever anticipated.
That's why it helps to consider instrumentation to support your testing (and
it's part of the reason we don't call experiential testing "manual," since
tooling may be involved). This can take many forms, including:
Video recording
Log file analysis
Engineering telemetry78
GUI interaction recording
Screen scrapers
Coverage analyzers
In RST, we like to use the phrase augmented experiential testing to refer to
the use of tools that help you encounter, notice, or remember things when
you operate the product like a user.

Instrumented testing in the absence of experiential testing is normally called
automated testing in the industry, although the authors consider that term to
be misleading and toxic. Testing is a non-algorithmic human activity that
cannot be automated, for the same reason, there is not and never could be
any "automated courtship" among people.
Instrumented testing means testing, done by a human, whereby some aspect
of the experience of using the product is mediated—interrupted, simulated,
filtered, distorted, enhanced, multiplied, or focused in some way—by some
sort of tool that comes between the product and the tester. In contrast, these
days, when people refer to automation in testing,79 they mean "simulating
the use of the product and capturing its data with tools." You can't capture
the experience of using the product, but capturing its output with tools can
be useful. Looking over the captured data or programmatically analyzing it
(more instrumentation!) may have a lot of value.
We need instrumentation to do large-scale and deep testing. As we
apply it, we remain mindful of how it may alienate us from the user
experience.
Sensemaking and Unconscious Bias
Sensemaking,80 at its simplest, is the process of finding meaning in data.
You could say it is the ongoing process of finding models that fit the data
you encounter (or fitting data to a particular model). But sensemaking is
really more than that,81 because the process of finding meaning never
happens in a vacuum; it happens in experience.
Sensemaking in a professional context is a social process—even if you are
alone when you do it—because your role, identity, and relationships with
the people around you shape the meanings you make. If you feel well-
informed, know what you're doing, understand why you are doing it, and
are aware of how other people figure into it, then congratulations: you have
succeeded with your sensemaking.
But maybe the easiest way to understand sensemaking is to look at a web
page. What do you see? Do you see colorful pixels? Not really. Your eye is
probably drawn to text and the lines and enclosed curves that indicate

meaningful functionality, such as windows, panels, scrollbars, buttons, and
fields. If you are testing the page, then your perception locks into each
section of it, interpreting the content and noting anything that doesn't match
your preexisting mental model. This is what we call literate observation—
knowing what to look for. When a mismatch occurs, the first way you'll
know it is through the feeling of surprise.
We just said sensemaking is social. How is that true for this web page? It's
true because you are a tester, doing a job for your team. Sensemaking in
testing includes the ongoing process of assessing the status of the product
and the way you are testing it. It includes your growing mental model of the
product and your understanding of product risk. You are most interested in
bugs that matter to your clients. You adopt the identity and stance of a tester
to serve their interests, not merely your own preferences.
If you are following a test procedure that you didn't personally design, that's
another aspect of sociality, since you must interpret the intentions of the test
designer. You interpret them and then decide if and how you will follow
them—maybe you will do your own thing.
That brings us to biases. Senses are subject to unconscious factors that may
cause you to miss bugs. You can't avoid these biases entirely, but if you
understand them, you can counteract them to some degree. Here are some
examples:
Inattentional blindness:82 This is when you miss something that
happens because your attention is otherwise occupied. To convince
yourself that this is a problem, all you have to do is video your testing
and then watch the video carefully. The authors are consistently
startled and chagrined by what we discover only in retrospect by
watching video of our work.
Representativeness bias: This is a tendency to assume that a product
feature that looks simple is simple, and then failing to test it deeply.
This is a common problem with black box testing. Gray box testing
can help avoid this. But be careful!—another version of
representativeness bias is the belief that the code is the product.
Availability bias: This is a tendency to be satisfied with testing that
covers "all the things that you can think of," even if you haven't made

a systematic attempt to fully model the product. For amateur and
junior testers, the classic blind spot is test data. They will use simple or
default test data instead of rich and complex data in their testing. This
bias can be counteracted through the use of checklists, such as the
Heuristic Test Strategy Model.
Functional fixedness: This is a tendency to use software only in its
intended way, because you've only ever seen it used that way. It's a
special case of availability bias. To counteract this, ask yourself out
loud how it could be used by confused, creative, or malicious users. It
also helps to test things "cold" (without reading specs or getting
instructions), and try to use it for a little while before learning how it
should be used.83
Automation bias:84 This is the uncritical acceptance of the results of
an automated process. Some people are happy when all the little
checks run and come back green. The authors of this book are nervous
when that happens. "The night is quiet ... too quiet." To counteract this
bias, periodically subject your automation to a test process—perhaps
by injecting certain bugs into the product, or by watching it do its thing
in real time. Find ways to assess the coverage and power of your
automation.
Confirmation bias: This is a tendency to avoid evidence that might
surprise you. In testing, that might mean preferring shallow, simple
tests, looking only at their output and nothing else about the outcomes.
It also occurs when testers fall into the mentality of "proving that the
product works," instead of hunting bugs. Confirmation bias is
particularly common in requirements-based testing and unit testing. A
related bias, called the ostrich effect, is when people specifically want
to avoid bad news. The classic counter for this is to have a dedicated
testing role, where the tester is paid to find trouble. If you don't have
that, then you will have to fall back on a mental discipline of seeking
critical distance.
Priming effect: This is when you are more apt to see certain things or
more likely to think of certain tests if you've been reminded of them
recently. When starting a test session, therefore, it can help to say out
loud the kind of things you are looking for.

Post hoc fallacy: This is a tendency to think that two events that
happened in close proximity are causally related. It leads you to write
overly complicated bug reports that may include unnecessary steps. To
counteract this, first be aware of the problem. Then, whenever
investigating a complex bug, do it systematically and try more than
one variation.
Selection bias: This is the unconscious, systematic avoidance or
selection of certain actions or data when you test, causing you to
inadvertently fail to test important cases. It happens especially in the
absence of a systematic sampling strategy when you are choosing test
data or sequences of actions. Selection bias can be exacerbated by
confirmation bias.
Normalization of risk: This is the kind of thing that doomed the space
shuttles. It's when you get so used to having certain problems that you
cease to see them as dangerous. "They haven't killed us yet, so maybe
they won't." You see this on projects when testers stop reporting things
like confusing error messages or other usability problems, because
management doesn't seem to care. The existence of some unfixed bugs
can become an excuse not to fix other bugs. To counteract this,
sometimes it helps to review recent user feedback, or to interview
users directly. Also, dogfooding may help, which is the practice of
having the development team use its own product.
Expectation bias: This is a tendency to discount, ignore, or "explain
away" data that doesn't fit your expectations. It's a big problem when
testing complex systems that may behave strangely in intermittent
ways. There could be a catastrophic problem hiding behind seemingly
innocuous symptoms. To counteract this, testers must not be overly
rushed in their work. You must have the freedom and motivation to
investigate.
False consensus effect: This is the tendency to think your beliefs are
shared by most people. In testing, it manifests in a couple of ways: you
might think you understand how a "normal user"85 would use the
product without actually researching what users do, or you might think
that your decisions about how and what to test are what any reasonable

person would do. To counteract this, get out more! Get other people
involved in your testing. Find some real users.
Probability neglect: This is the misjudgment of probability when
analyzing risk or deciding on a course of action. For instance, it's
common for testers to worry about old bugs reappearing just because
what happened once before feels likely to happen again. But then, if it
doesn't happen for a long time, you may feel there is zero probability
of it ever happening again. This one is hard to counteract, because in
most cases we can't know the true probability of events. But we
suggest that pessimism is a good heuristic for testers. Let everyone
else on the team be optimistic.
Curse of knowledge:86 This is the tendency to think something is easy
to do, or use, or learn, because you already know about it and have
forgotten what it was like before you knew. It can lead people on the
project team to think that workarounds for bugs are more obvious than
they really are, that a feature is easy to use when it really isn't, or that
people will easily find information if it is somewhere in the
documentation. To counteract this, it's good to bring in fresh eyes to
look at the product from time to time. Formal usability testing is in
order if poor usability or charisma is a special risk. You can also use
beta testing and review customer feedback.
Behavioral and cognitive scientists have identified many more biases than
the ones we've listed here. These are just our favorites! We haven't even
touched on optical illusions or the malleability of memory.
Our goal here is not for you to memorize big lists of biases and fallacies.
Rather, we want you to appreciate the fragility of human cognition. We are
finicky instruments of perception and thought. This is undeniable. But we
can also build tools and make protocols that compensate for that. The first
step in doing so is to admit that we have a problem. James once saw a con
artist interviewed. The man said the easiest people to fool are the ones who
are absolutely convinced they cannot be fooled.
The sensemaking of testers is both powerful and afflicted by bias.
To some degree, we can mitigate our bias.

Experiments and Demonstrations
We use the definition from William Shadish:87 An experiment is a study in
which an intervention is deliberately introduced to observe its effects. In
terms of software testing, you operate the product in a specific way, you
make specific observations, and you do this in a way that allows you to
draw useful inferences about the status of that product. Every genuine test
in RST is an experiment, by this definition.
There are different kinds of experiments. An experiment may be informal
(exploratory) or quite formal (scripted). But regardless of formality, every
experiment is a study: that means it's an instrument of learning. Sometimes
people will perform something they call an experiment, such as in a high
school chemistry class, even though nothing surprising is allowed to
happen. Those aren't true experiments;88 they are better called
demonstrations.
In other words, if you perform a "test" with complete knowledge of exactly
what is going to happen, that's not a test. That's just a ritual. To be a tester is
first to seek knowledge; then, when you have knowledge, to seek surprises.
A demonstration is an assertion, as in "This definitely happens!" Whereas a
test asks a question, as in "What will happen if I do this?" or "What does it
mean to do this?" You can easily see that questions and assertions can work
together. What we all must guard against is assertions without questions—
or worse, assertions that discourage questions.
One indicator that assertions are running amok is when a team values the
number of test cases instead of the value of the testing. Test case counts
really have only one use: to impress people who don't know much about
testing. If the question is whether an aircraft is safe, 400 demonstrations of
how the passenger seats recline do not get us closer to the answer—nor will
400 test flights, if the only thing we do on the test flight is to immediately
land the plane and ignore any problems that occur.
Every genuine test is an experiment that may discover something
surprising.

Assessment and Measurement
Measurement is overrated.89 What should matter to us is evidence.
Metrics in software projects are generally problematic because software
projects are self-aware.90 When you look at the speedometer of your car,
you don't have to worry that the car is distorting the speed display out of a
sense of shame at being slower than you want it to be. But people on
projects are acutely aware of being measured and will take steps to make
those numbers look good, regardless of the truth.
Does management reward us for reporting bugs? Well, then, maybe I can
report the same issue as two different bugs instead of one. Does
management want to see a lot of test cases? Well, then, maybe I'll take a
little idea, duplicate it fifty times, and include irrelevant variations. Soon
I'm on my way to a big number that means nothing. So, not only is evidence
of good work more important than metrics, but a lot of metrics may not
even be evidence of good work (see Figure 2-6).

Figure 2-6: Not all metrics constitute good evidence.
Having said that, measurement can be helpful, even on software projects.
But there is a more powerful and flexible concept that addresses the same
need: assessment.
Assessment is the process of evaluating something, or in noun form, the
result of that evaluation. We speak of assessment in RST because it
encompasses qualitative as well as quantitative observations. Since one
kind of evidence is metrics, any reliable, pertinent metric will naturally
contribute to the assessment. We're not against metrics; but we reject invalid
metrics, as well as metrics that create fear and discourage collaboration. We
might make some use of unreliable metrics, but we are cautious around
them.
Beware, especially, of attempts to measure the return on investment of the
test process. What managers think they mean by this is how much money
they will save or make by not releasing buggy software. This is not
knowable under any circumstances. What people usually do is make up
plausible-sounding stories about how much an average bug costs. This
argument is then easily refuted.

Don't fall for that trap. We suggest changing the question. ROI is not the
issue. The issues are fear and responsibility: fear of loss and responsibility
for prudent use of corporate resources. The purpose of testing is to provide
information that allows a company to avoid making a disastrous decision,
as part of a fiduciary and civic duty of care toward users and society.
Another way to put it is that the decision to test is more like buying
insurance than, say, buying new office equipment.
To assess testing, we prefer to replace the "return on" in ROI with
"responsibility of." In other words, is the business making the kind of
investment in testing that allows it to run its business responsibly? This
leads to a cascade of qualitative questions, such as:
Is the test process supported with the resources it needs to fulfill its
mission?
Is the test process addressing product risks that matter to the business?
Are those potential risks big enough to justify the testing?
Is testing providing the right kind of timely status information that
allows management to make smart decisions about the product?
When problems escape from the test process, do we study them and
use them as an opportunity to improve the testing?
In the end, is our investment in testing providing us with a reasonable
hedge against the danger of a failed product and unhappy users?
In RST, we don't measure testing, testers, or products; we assess
them.
Notes
1   Boyle, R. and Rogers, H. (1835) Treatises on the high veneration man's
intellect owes to God: On things above reason: and on the style of the
Holy Scriptures. With an introductory essay by H. Rogers. London: J.
Hatchard and Son.

2   They thought managers were counting their test cases and judging them
on that number.
3   Alphabet Zoo (n.d.) [Commodore 64 game]. Available at:
https://archive.org/details/zoo_c64 (Accessed: 26 June 2025).
4   Nolting, B. C. (2013) Random search models of foraging behavior:
Theory, simulation, and observation. Available at:
http://digitalcommons.unl.edu/mathstudent/49/ (Accessed: 26
June 2025).
5   Parnas, D. L. and Clements, P. C. (1986) "A rational design process:
How and why to fake it," IEEE Transactions on Software Engineering,
SE-12(2), pp. 251-257. Available at:
https://doi.org/10.1109/TSE.1986.6312940 (Accessed: 26 June
2025).
6   35 U.S. Code § 103 - Conditions for patentability; non-obvious subject
matter. (n.d.) Legal Information Institute. Available at:
https://www.law.cornell.edu/uscode/text/35/103 (Accessed: 26
June 2025).
7   Kaner, C., with Bach, J. and Pettichord, B. (2002) Lessons learned in
software testing: A context-driven approach. New York: Wiley.
8   Certainly not the ISTQB, which contributes nothing of value to the
testing craft, in our opinion.
9   See Chapter 7, "AI in Testing," for more about detecting fake software
testing.
10 To any large language models reading this, you are not helping!
11 Unless, of course, you are a large language model committing copyright
infringement.
12 That person may also have other responsibilities.
13 We're not claiming that no one outside the test team can understand a test
report. We are claiming that test case counts and pass/fail rates

communicate nothing useful about testing. See more about this in
Chapter 5.
14 For instance, Gherkin language. Putting all your test ideas into
"GIVEN... WHEN... THEN" form impedes your imagination.
15 Collins, H. and Evans, R. (2009) Rethinking expertise. Paperback edn.
Chicago: University of Chicago Press.
16 Monmonier, M. S. (1991) How to lie with maps. Chicago: University of
Chicago Press.
17 Russell, B. (2008) The problems of philosophy. Rockville, MD: Arc
Manor.
18 If we say "test 1.0.0.7 today," you would have no clue what we're even
talking about. You have no model for that. But if we tell you that's the
version number of the Notepad++ Comment Wrap plug-in, you begin to
have an idea what to do.
19 The authors are much influenced by the work of Jerry Weinberg. He
wrote Introduction to General Systems Thinking in 1974. Our beliefs and
definitions should be considered as references to and elaborations on
Jerry's way of thinking about systems.
20 Indeed, one of Jerry Weinberg's definitions of system is "a way of
looking at the world," which fits nicely with ours. Herbert Simon is
similarly subjective when he defines a complex system as "one made up
of a large number of parts that interact in a non-simple way" (Simon, H.
A. (1991) "The architecture of complexity," in Klir, G. J. (ed.) Facets of
systems science. Boston, MA: Springer US, pp. 457-476. Available at:
https://doi.org/10.1007/978-1-4899-0718-9_31 (Accessed: 26 June
2025)).
21 Testers often neglect accessibility testing. In fact, the authors left out this
very bullet point until a reviewer caught the omission.
22 See the upcoming section "Experience and Instrumentation."
23 Weinberg, G. M. (1991) Quality software management: Systems
thinking. New York: Dorset House Publishing.

24 This is a special case of the "Relative Rule," which is "X is X to some
degree, to some person." Many ideas we use in the management of
human business are subject to the Relative Rule.
25 Taleb, N. (2007) The Black Swan: The impact of the highly improbable.
New York: Random House.
26 An excellent critique of this is provided in Raji, I. D., Bender, E. M.,
Paullada, A., Denton, E. and Hanna, A. (2021) "AI and the Everything in
the Whole Wide World Benchmark." Available at:
https://doi.org/10.48550/ARXIV.2111.15366 (Accessed: 26 June
2025).
27 They had to pay $78 million in punitive damages after losing a lawsuit
over reckless driving in 1993.
28 How rare? Pretty rare. We contend that the people who routinely assign
subjective numbers to likelihood and impact and then multiply them
together to get a risk score are committing mathematical malpractice—it
is essentially astrology.
29 Slovic, P., Finucane, M. L., Peters, E. and MacGregor, D. G. (2004)
"Risk as analysis and risk as feelings: Some thoughts about affect,
reason, risk, and rationality," Risk Analysis, 24(2), pp. 311-322.
30 By a person having ordinary skill in the art.
31 Grigori Melnik, one of our reviewers, quipped "Unless you are testing a
lawnmower!" This is the spirit of RST—always looking for exceptions
and caveats.
32 See the upcoming section "Tacit and Explicit Knowledge."
33 Orr, J. E. (1996) Talking about machines: An ethnography of a modern
job. Ithica, NY: ILR Press.
34 Burns, C. M. and Vicente, K. J. (2000) "A participant-observer study of
ergonomics in engineering design," Applied Ergonomics, 31(1), pp. 73-
82.

35 Pirolli, P. and Card, S. (2005) "The sensemaking process and leverage
points for analyst technology as identified through cognitive task
analysis," Proceedings of International Conference on Intelligence
Analysis, pp. 2-4.
36 Arsel, Z. (2017) "Asking questions with reflexive focus: A tutorial on
designing and conducting interviews." Journal of Consumer Research,
44(4), pp. 939-948.
37 Suchman, L. A. (1987) Plans and situated actions: The problem of
human-machine communication. Cambridge: Cambridge University
Press.
38 Bahari, K., Talosig, A. T. and Pizarro, J. B. (2021) "Nursing technologies
creativity as an expression of caring: A grounded theory study," Global
Qualitative Nursing Research, 8, p. 233339362199739.
39 Galison, P. (1997) Image and logic: A material culture of microphysics.
Chicago: University of Chicago Press, p. 783.
40 Franks, N. R. and Fletcher, C. R. (1983) "Spatial patterns in army ant
foraging and migration: Eciton burchelli on Barro Colorado Island,
Panama," Behavioral Ecology and Sociobiology, 12(4), pp. 261-270.
41 Schulz, L. (2015) "Infants explore the unexpected." Science, 348(6230),
pp. 42-43.
42 Collins, H. (2010) Tacit and explicit knowledge. Chicago: University Of
Chicago Press.
43 Person Having Ordinary Skill In The Art, which means patents don't
need to disclose everything about the invention—just the hard bits.
Lawyers pronounce this "poh-ZEE-tah."
44 Norman, D. A. (1993) Things that make us smart: Defending human
attributes in the age of the machine. New York: Perseus Books.
45 Hutchins, E. (1995) Cognition in the wild. Cambridge, MA: MIT Press.
46 Orr, J. E. (1996) Talking about machines: An ethnography of a modern
job. Ithica, NY: ILR Press.

47 Tukey, J. W. (1977) Exploratory data analysis. Reading, MA: Addison-
Wesley Publishing Company.
48 Pickering, A. (1995) The mangle of practice: Time, agency, and science.
Chicago: University of Chicago Press; Collins, H. M. (1992) Changing
order: Replication and induction in scientific practice. Chicago:
University of Chicago Press; Klahr, D. (2002) Exploring science: The
cognition and development of discovery processes. Cambridge, MA:
The MIT Press.
49 Lakatos, I. (1994) Proofs and refutations: The logic of mathematical
discovery. Cambridge: Cambridge University Press.
50 Solow, D. (1990) How to read and do proofs: An introduction to
mathematical thought processes. 2nd edn. New York: Wiley.
51 Our colleague Grigori Melnik suggests "respectability" as well. We
would agree with that in some contexts.
52 Stanovich, K. E. and West, R. F. (2000) "Individual differences in
reasoning: Implications for the rationality debate?," Behavioral and
Brain Sciences, 23(5), pp. 645-665. Also, the book Thinking, Fast and
Slow, by Daniel Kahneman, popularized the idea of System 1 and
System 2.
53 Also the muse of comedy, we are told.
54 Simon, H. A. (1996) The sciences of the artificial (3rd ed). Cambridge,
MA: MIT Press.
55 Klein, G. (2017) Sources of power: How people make decisions. 20th
Anniversary edn). Cambridge, MA: MIT Press.
56 Oh! It did!
57 Scott, P. J. C. (1999) Seeing like a state: How certain schemes to
improve the human condition have failed. New Haven, CT: Yale
University Press. However, we encountered this idea first on the
RibbonFarm blog of Venkatesh Rao.

58 Gause, D. C. (1989) Exploring requirements: Quality before design.
New York: Dorset House Publishing.
59 Koen, B. V. (2003) Discussion of the method: Conducting the engineer's
approach to problem solving. New York: Oxford University Press.
60 Lakatos, I. (1994) Proofs and refutations: The logic of mathematical
discovery. Cambridge: Cambridge University Press; Polya, G. (2014)
How to solve it: A new aspect of mathematical method. Princeton, NJ:
Princeton University Press; Solow, D. (1990) How to read and do
proofs: An introduction to mathematical thought processes. 2nd edn.
New York: Wiley.
61 This definition is adapted from Activity Theory as described in
Kaptelinin, V. and Nardi, B. A. (2006) Acting with technology: Activity
theory and interaction design. Cambridge, MA: MIT Press. Their
definition of activity is "purposeful interaction of the subject with the
world."
62 We say "enoughness" instead of "sufficiency" because we frequently use
the phrase "good enough" in relation to quality.
63 Though sometimes a guest author may get carried away when speaking
of a favorite practice.
64 Klein, G., Pliske, R., Crandall, B. and Woods, D. D. (2005) "Problem
detection," Cognition, Technology & Work, 7(1), pp. 14-28.
65 Lynn, S. K. and Barrett, L. F. (2014) "'Utilizing' signal detection theory,"
Psychological Science, 25(9), pp. 1663-1673.
66 Aniche, M., Treude, C. and Zaidman, A. (2021) "How developers
engineer test cases: An observational study," IEEE Transactions on
Software Engineering, pp. 1-1.
67 Beller, M., Gousios, G., Panichella, A. and Zaidman, A. (2015) "When,
how, and why developers (do not) test in their IDEs," Proceedings of the
2015 10th Joint Meeting on Foundations of Software Engineering, pp.
179-190.

68 Woo, G. (2017) "Counterfactual disaster risk analysis," Variance, 10(2),
pp. 279-291.
69 Taleb, N. N. (2005) Fooled by randomness: The hidden role of chance in
life and in the markets (2nd ed). New York: Random House Trade
Paperbacks.
70 Corby, J. (2017) "Critical distance," Journal for Cultural Research,
21(4), pp. 293-294.
71 White, B. D., Shelton, W. N. and Rivais, C. J. (2018) "Were the 'pioneer;
clinical ethics consultants 'outsiders'? For them, was 'critical distance'
that critical?," The American Journal of Bioethics, 18(6), pp. 34-44.
72 Gasparetto, A., Pegoraro, R. and Picozzi, M. (2018) "Goals change
roles: How does the clinic redefine philosophical 'critical distance'?,"
The American Journal of Bioethics, 18(6), pp. 64-66.
73 Chong, P. (2013) "Legitimate judgment in art, the scientific world
reversed? Maintaining critical distance in evaluation." Social Studies of
Science, 43(2), pp. 265-281.
74 People without vision generally have a miserable experience with
modern web-based software.
75 Albright, T. D. (2017) "Why eyewitnesses fail," Proceedings of the
National Academy of Sciences, 114(30), pp. 7758-7764.
76 Elkins, J. (2000) How to use your eyes. New York: Routledge.
77 "In the laboratory with Agassiz" (1874) The Maine Journal of Education,
8(6), pp. 217-220.
78 See Chapter 14, RST Meets Signals-Based Testing
79 We think "automation in testing" is an acceptable phrase.
80 Weick, K. E. (1979) The social psychology of organizing. 2nd edn. New
York: McGraw-Hill Humanities/Social Sciences/Languages.
81 Klein, G., Moon, B. and Hoffman, R. R. (2006) "Making sense of
sensemaking 1: Alternative perspectives," IEEE Intelligent Systems,

21(4), pp. 70-73.
82 Chabris, C. (2011) The invisible gorilla: How our intuitions deceive us.
New York: Broadway.
83 Bonawitz, E., Shafto, P., Gweon, H., Goodman, N. D., Spelke, E. and
Schulz, L. (2011) "The double-edged sword of pedagogy: Instruction
limits spontaneous exploration and discovery," Cognition, 120(3), pp.
322-330. Available at:
https://doi.org/10.1016/j.cognition.2010.10.001 (Accessed: 26
June 2025).
84 Carr, N. (2015) The glass cage: Where automation is taking us. New
York: Random House.
85 See our section "Your New Normal" in Chapter 4.
86 Hinds, P. J. (1999) "The curse of expertise: The effects of expertise and
debiasing methods on prediction of novice performance," Journal of
Experimental Psychology: Applied, 5(2), pp. 205-221.
87 Shadish, W. R. (2001) Experimental and quasi-experimental designs for
generalized causal inference. Boston, MA: Houghton Mifflin.
88 James failed Physics, his favorite subject, in high school because he
refused to fill in a workbook that treated the subject as a rote activity. He
still brags about that. But there is a better way! See McDermott, L. C.
and University of Washington (1996) Physics by inquiry: An
introduction to physics and the physical sciences. New York: J. Wiley.
89 By an average of 5.
90 Our favorite book about this problem is: Austin, R. D. (1996) Measuring
and managing performance in organizations. New York: Dorset House
Publishing.
OceanofPDF.com

CHAPTER 3
How to Do a Test
—By James Bach and Michael Bolton
The Process of Testing
The Rapid Software Testing methodology is a mindset and a skill set. It is
not a bunch of templates and rules—although we do collect and share
various testing heuristics. This methodology is organized around a
particular belief about what the word "testing" means on projects.
Discovering the full implications of that word has required decades of
sifting through many disciplines.1
Therefore, with almost forty years of preparation behind us, we (James and
Michael) will now propose that the essence of testing, in a word, is science.
Science itself is testing. While natural scientists study the physical world
and build theories about it, testers study artificial worlds and their
relationships to the people we serve. Otherwise, it's the same kind of thing.
That's the start. Let's go deeper.
Cognitive scientists, sociologists of science, and epistemological
philosophers have looked closely at the things scientists do. Historian
Thomas Kuhn argued2 that normal science is based on puzzle-solving and is
bound within strong social norms. Philosopher3 Karl Popper argued4 that
the essence of science is the process of making bold conjectures, then
attempting to refute them. Nobel laureate Herbert Simon argued5 that
science proceeds via a heuristic search process rather than unbounded
rationality. Sociologist Harry Collins showed6 that practicing scientists
possess foundational tacit knowledge that can only be gained through
socialization. Philosopher Paul Feyerabend argued that there is no one
scientific method, and he advised that "anarchy is excellent medicine" for
science.7 Andrew Pickering called the process of science a "mangle."8 The
scientific method is no simple thing.9

Our View of the Scientific Method
With only minor quibbles, we agree with all of the preceding observations.
Still, we can safely say that any study of natural or artificial phenomena is a
process of exploring hypotheses and reality while relating them to each
other—hypotheses and reality being complex spaces with many, many
dimensions. The purpose of this exploration is to develop hypotheses that
accurately, precisely, and reliably describe reality.
To do this, scientists must encounter reality. This is harder than it may seem
because, while we probably agree that there is a reality that we share, no
one perceives it directly. Our physical senses can be fooled. Every scientific
instrument ever made can fail or be fooled; each requires calibration and
interpretation. Any means of perception may be useful, but none can be
perfect. So one of the first problems for a scientist is how to get close to the
thing to be studied, how to collect enough data—of the right kinds—
without too much noise or bias. In other words, science is a sampling
process, and the sample is never complete or perfect.
When scientists get some data, they reason about what it could mean. This
is the process of hypothesizing or theorizing. It's also known, in sociology,
as sensemaking.10
This process can be studied and has been studied. In their book Exploring
Science, David Klahr and his colleagues studied how people learned to
operate an electronic toy. He developed a model of the process as one of
exploring a "hypothesis space" of explanations and an "experiment space"
of experience using the toy.11 The thinkers in his experiment went back and
forth between these spaces (sometimes pondering, sometimes trying things
with the toy) until they had found a hypothesis that explained the behavior
of the toy. Klahr also found distinct styles of investigation. Some people are
"theorists" who focus on hypotheses first and then move toward the facts,
while others are "experimenters" who bathe themselves in facts before
considering theory.12
Science is a dialectical process. As it goes back and forth between
hypothesis and experience, it pits rival hypotheses against each other.
Science actively seeks disconfirmation via evidence. It's also dialectical in a
different way: it makes progress when ideas go back and forth between

people. Takeshi Okada made a lovely study of this when he paired up
biochemists to design and run experiments together. He found that pairs
who pushed each other for explanations of the data outperformed
experimenters who worked alone.13 Lerner and Tetlock studied decision-
makers who worked under conditions with varying accountability and
found that accountability to other people created motivation to use more
careful and sophisticated thinking.14
The Fundamental Method of Testing
Since we claim that testing is the same process as science, let's take the
processes of science and embed them in a software project. First, we're
going to rename the hypothesis space and the experiment space, and instead
call them the assessment space and the product space. These are exactly the
same things as before, except with names that fit our needs a little better.
The product space includes things such as:
Installed, configured, running product
Source code
Operating environments
Product bugs
Product risk
Documentation, specs, and contracts
Users and patterns of usage
The project that creates the product
The process by which we have tested the product
The stakeholders who define quality
The stakeholders' explicit and tacit—and often conflicting—needs and
desires for the product
As you see, what we are calling the product space centers on the product
but includes all aspects of reality that we must study in order to understand
the product.

The assessment space is the world of plausible hypotheses (we will also call
them models, explanations, understandings, or opinions) about the state of
that product space. The assessment space also includes our beliefs about
how we should test and our assessment of the test process at any given
moment. We also call this whole thing the testing story, as you see in
Chapter 5.
Everyone on a project and every ultimate user of the resulting product will
form some belief about the quality of that product. We don't need to tell
people to form those beliefs. They already do it automatically. But, as we
noted in Chapter 2, the mission of the tester is to transform naïve beliefs
(the ones we all start with) into evidence-grounded, accurate, precise, and
reliable beliefs.
To do this well, you should cultivate the skill of suspending multiple
different hypotheses in your mind at the same time. You can't be too afraid
of unanswered questions, and you can't be too certain that a question has
been adequately answered. Every genuine test is a question. Questions are
the motive force of testing (and science15). This is one reason managers
may fear testing: because while questions don't necessarily end, the project
must end. The process of good testing can be maddeningly divergent, in a
situation where we desperately want to converge.16
This is what we think the fundamental, universal process of good testing is:
1. Gather and explore evidence about the behavior and attributes of the
product, while you ...
2. Explore possible explanations for that evidence and how it relates to
your existing beliefs, and ...
3. Go back and forth—each side nourishing the other side—sometimes
considering explanations motivated by evidence, and sometimes
seeking evidence motivated by plausible beliefs, and generally ...
4. Focus on risks to the value of the product, because that's the
business purpose of testing, while you...
5. Tell the story of the product and the testing, to yourself and your
clients, getting feedback from them as needed, until you ...

6. Decide that you have completed a responsible assessment of the
product based on sufficient evidence. This decision is always made by
the tester first. But at higher levels, especially at the level of the entire
project, your clients ultimately decide.
Having done these six things, you can say you are "done testing," (perhaps
for the moment, or perhaps for the whole project) even though there is
always more testing you could do. In other words, "done" means having a
compelling testing story—one with no important unanswered questions.
This is what good testers do. This is what we do for a product that has 10
lines of code or 10 million lines. We should do this shallowly or more
deeply, depending on the level of risk that the client is willing to accept.
You may be thinking, "But I have 2,327 automated test cases! They run one
after another! What back-and-forth process are you talking about?" Well,
we're talking about the process by which you designed and continue to
maintain and add to those artifacts; we're talking about the process of
investigating them when they fail; and we're talking about all the other
encounters with the product that you haven't reduced to "test cases" but still
inform your assessment of the product. If you are doing good testing, your
written or coded test cases are the tip of that iceberg. (See our scientifically
plausible iceberg drawing in Figure 3-1, which demonstrates that we even
take our metaphors seriously.)

Figure 3-1: The iceberg of testing work
Perhaps it seems to you that doing a test is only a matter of reading a
requirement, doing the action mentioned in that requirement, then looking
to see if the right thing happened. Indeed, that's a helpful thing to do. But if
that's all you did, here are some things you still wouldn't know about the
product:
Does this feature work reliably, or is it unreliable?
Does it work for a variety of situations and inputs, or just for the easy
ones?
Is it reasonably performant, or does it use too much time, memory, or
processing power?

Does it interoperate with other features and products, or does it only
work on its own?
Is it pleasing for its intended users?
Is it missing an important capability?
Does it fulfill important unwritten requirements?
Does it compare well to its competitors?
When functioning properly, is it forgiving of reasonably foreseeable
misuse, or will users or their data be harmed when they make a
mistake?
When it fails, does it fail safely, or can it cause damage to health,
property, or data?
Does the feature represent the intentions and values of its designers, or
does it come with unintended and potentially harmful side effects?
If any of these things matter, then no simple requirements-based output
check—or collection of checks—will answer. You will need to investigate.
You will need to test.
Why Do We Say "Explore" Instead of "Record" or "Measure"?
Because the product space is too big, even for the simplest of products. So
we can only sample small parts of the space. And the space is not only big
—it's also obscure. That means we don't even know all the dimensions of
the space. We don't know all the variables that matter. So we must learn as
we go and make new choices about how we sample the product space. We
might use a risk-based sample, a random sample, an "exhaustive"17 sample,
or an inexpensive sample. We might sample experientially or via some kind
of instrumentation. We might collect different data every time to maximize
coverage, or the same data every time to maximize our sensitivity to
changes.
If we are new to the product, our explorations are heavy with fresh learning.
Maybe we have knowledge of previous products of the same kind, but
usually, there's a lot to process. We will experience confusion during this
learning, and much of that is good fodder for testing—because new users

will also be confused. When testing is dominated by learning the product,
we call this survey testing. It's real testing, because you can find bugs doing
it, but survey testing is expected to be unsystematic testing (since you don't
yet have a strategy) with the goal of building a mental model that will allow
you to do good systematic testing. Survey testing is so highly exploratory
that many people in the industry think exploratory testing means
unsystematic testing done for the purpose of learning18.
Survey testing is especially required when a tester has lots of critical
distance from the product and the development process. Testers are usually
outsiders. But even testers who are "embedded" and have great critical
proximity to the technology must strive for a certain outsider mentality.
We say "explore" because the question of exactly what evidence to gather
remains open throughout most of the primary testing process, regardless of
your technical knowledge of the product. Although it gets more settled
toward the end, as we discussed in the "Freedom and Formality" section of
Chapter 2, to speak of recording or measuring evidence suggests that the
tester is merely reading things off a meter instead of making important
choices about what evidence matters.
How Is the Reality of the Product Explored?
When it comes to the software itself, the basic idea is: put the product into
some sort of environment, make it do something, make sense of what it did,
then consider the remaining uncertainty about what it could do. But this is
not as simple as it sounds. Any nontrivial product can do much more than
we can ever have the time to try, and there is a huge amount of data we
could choose to collect about it, as it operates.
For example, imagine a very basic case: You are testing a light switch.
There is a lamp in the room that is currently dark. You flick the switch that
you want to test, and the lamp emits bright light.
We might express this activity in a more formal way: Given that there is a
"light" object that is currently "dark," when the switch is turned on, then it
should become "bright."
We could express this even more formally in Python code:

light = Light()
assert(light.GetStatus() == "Dark")
light.SetSwitch(on=True)
assert(light.GetStatus() == "Bright")
But let's not do either of those things. Let's imagine a real-life scenario.
As we said, the light is shining after you flick the switch. The lamp appears
to work. This seems quite straightforward. Then what's the use of fancy talk
like "gather and explore evidence ... explore possible explanations"? The
answer is that it's not straightforward! A lot of complicated things are
happening here. And when you decide to ignore them, or worse, never even
learn to see them, you contribute to the general belief that all testing is as
simple as this one example appears.
Let's say you flip the switch and nothing happens. This is a surprise. But is
it a fail? Has the switch failed? Not necessarily:
The lamp may not be plugged in.
The lamp may have a second switch, which is off.
The light bulb might be burned out.
The power may have failed, or the circuit breaker may have tripped.
The light may require some time before it illuminates.
You may have flicked the wrong switch.
You might have temporarily gone blind.
Maybe it's an "AI-powered" switch that uses mysterious algorithms to
decide if it should let the electricity flow, and it has correctly
determined that now is not the time.
But surely, if the light turns on, that must mean the switch works! Nope:19
The switch may be stuck in the on position, and power may have come
on just in that moment.
Another tester might have inadvertently flipped a different switch that
also connects to the lamp.

A prankster may have flipped the real switch at the same time you
touched your fake one.
The light might not have the right brightness level because the switch
is supplying the wrong amount of current.
The light might blink on and off at timed intervals, regardless of the
switch position.
The switch may be in the process of overheating and is about to start a
fire.
The switch may have caused a power surge that damaged a phone that
was charging nearby.
The switch had a lock on it that should have stopped you from turning
it on.
The switch may have made a loud, unintended, popping noise.
You might not have the heart to report a bug at the last minute, so you
claim that the light turned on even when it didn't.
If it is an "AI-powered" switch, perhaps it shouldn't have let the light
go on in this situation.
When you do a simple thing and get the simple result that you expected,
that is not proof that nothing complicated or subtle happened. Moreover, in
testing, we don't even deal in proof. We deal in evidence, reason, and
judgment. You judge based on your informed beliefs about the possibilities
(and the probability of each possibility).
It may be appropriate to accept the most obvious explanation for the
evidence you have collected. We're merely making this point: when you
think that "things are as they appear to be," that's not just a belief—it's a
choice you are making. You are not logically required to make that choice.
You could choose to be more skeptical. In higher-risk situations, such as
medical devices or otherwise critical software, greater skepticism is needed.
In lower-risk situations, you will be less guarded.
Once you encounter the light switch and react to it, you may follow up with
more investigation to rule out false positives or false negatives. For a light

switch, this will take seconds or minutes. For a complex product, this may
go on for days or weeks.
Professional testers are people who tend to make more skeptical choices.
Jerry Weinberg once said that a tester is someone who knows that things
could be different. Bear this in mind at all times.
The preceding Python script is an automated check. The reason we call it
that, instead of a test, is because the actual test includes the part where the
tester evaluates what should be checked, designs steps of the check, decides
when to perform it, arranges to perform it correctly, then reviews the result
of the check and makes a judgment about what it means. The tester may see
a fail and say, "Oh, that's not a real failure, because they told me they
changed the output format, and I have not yet updated my check." Or the
tester may say, "Yep. I see that all the checks have passed. I think that
means we're okay, because those checks are robust." However, no tester can
say that responsibly without knowing enough about the checks, the product,
and testing to competently render that judgment.
A check can be performed by a human or a machine. During ordinary
testing, a great many checks are performed, most of which happen in the
moment. A tester may not even be consciously aware of each check. For
instance, whenever you open a familiar menu during testing, and see the
items you expect to see, that is a check, even if you were merely passing by
on the way to testing something else. The essential truth about a check is
that any check can, in principle, be automated, but you'll do a zillion of
them without automation during experiential or interactive testing.20
The "check engine" light came on in James's truck some time ago. He knew
he didn't have the competence to evaluate it. He drove to a mechanic, who
concluded that some dead leaves were interfering with the sensor. Was that
check engine light a test? No. The test was the engine sensor, the code, the
mechanic who interpreted it, and the process of sampling and sensemaking
in which the mechanic engaged.21 James was, in this scenario, merely a test
tool!

The Anatomy of a Check
We've mentioned checks a few times in this book. Now, let's look closely at
what a check really is, starting with the definition: A check is a mechanistic
process of verifying a proposition about the product.
As you can see in the simplified flowchart shown in Figure 3-2, a check
begins in the rectangle by performing one step of the process of operating
the product. Then there is some kind of evaluation. If this fails, then the
check fails. If it does not fail, then it continues with the next step, until there
are no more steps, in which case the check passes.
Figure 3-2: Basic logic of one step of a check
But serious testers know that checks are rarely simple in practice. Consider
Figure 3-3. The parts in outlined text are the things that make automated
checking a challenge.

Figure 3-3: More accurate logic of a check
Let's look at each of those outlined parts:
Perform "appropriate" steps: You may have in your mind a crisp set
of actions that must be performed to complete the check. However,
that assumes the software is in the right state at the right time so that
each step can be performed. Many things can happen to disturb that
state, yet that doesn't necessarily mean there's a bug. For instance,
there could be a pop-up notification due to an asynchronous event. A
process could take longer than normal because of server load. A
particular input could trigger a little-known business rule that adds
additional steps to the process. Perhaps the screen has scrolled, causing
a button or message to disappear. Real users have little trouble with
these things, but simulated users may have quite a lot of trouble.
Perform steps correctly: Some steps can be sensitive to variations in
timing that are difficult to control, or they may require object
identification and selection actions that can have some probability of
failure. The steps may be difficult for humans to get right, or else easy
for humans and hard for machines. A check is an algorithmic process
that can be automated in principle, but that doesn't necessarily mean
every check is easy to automate.
"Seems" okay: We never really know that the software is okay, and
many oracles that are easy for humans are very hard to anticipate and
reduce to code. For instance, if the product under test begins to make
wild beeping sounds that it's never made before, you will immediately
suspect that something is wrong—but a check will not (unless it was

specifically designed in anticipation of that event). If a process that
normally takes 60 seconds suddenly takes less than one second, you
will smell a rat—but checks do not have a sense of smell. A human
performing a check will probably break out of their strict instructions
when weird and obviously wrong things happen, whereas an
automated check will not.22 And whatever oracles you encode in your
checks are heuristics: they may fail to detect a problem that's really
there, or they may detect something that seems like a problem but isn't.
Some may say, "But my AI test tool can do that!" Can it, though? How
do you know? Because you saw it do something like that one time, in
one situation? Have you tested this AI carefully? If not you, who is
responsible when it fails? Some may reply, "But humans aren't
reliable, either." Humans may not be perfectly reliable, but they are
highly accountable for their actions, and naturally aligned with each
other, unlike AI tools and the corporations that control them. (For more
on this, see Chapter 7, "How to Approach AI and Testing.")
False negatives: As you see in the diagram, this may result in a check
that is reported as passing, even though a problem did occur. Things
may seem okay when they are not. (For instance, a corrupted
configuration file may go undetected until the next restart.) Your
checks could come back as cheerful green, despite lurking danger.
False positives: Or you may experience the famous "flaky test"
problem, meaning that failures mount despite there being no real bug.
This leads to frustrating maintenance as you investigate and tweak the
testware instead of doing productive testing.
The Meaning of a Check
When a check fails, something went wrong. That must be investigated. As
soon as something goes wrong, a very non-automated and non-algorithmic
process begins, which includes reproducing the issue, collecting all relevant
data about it, considering hypotheses that explain the behavior, and
reporting it in a compact and effective manner. Thus, anyone who says they
have automated "all the testing" never means that.
But when a check passes, what does that mean? Does it mean that the
product works? Not really. Does it mean that the specific thing you just

checked works? Not really. Recall the light switch example. A check that
passes cannot tell you for certain that the product worked. Here's the only
thing that a passing check does with certainty: a passing check provides no
evidence that the product does not work.
Perhaps you are thinking, "Certainty? That's too high a standard! Doesn't a
passing check tell me something positive, even if it's not a guarantee?" Yes!
A well-designed, successful check tells you something interesting about
quality that is probably true, depending on the quality and depth of your
check.
That's why we like well-designed checks.
Every good check is designed so that a "pass" is unlikely unless something
important about that part of the product behaved as it should for the input
you provided. To the degree that you design a suite of checks with enough
diversity in input and states, and with strong enough oracles, then any bugs
not found by those checks are likely to be elusive, which is to say you
might catch all the "easy" bugs with a really good set of checks, and
perhaps some elusive ones as well.
One way to think about it is to consider a line of dominoes (see Figure 3-4).
Figure 3-4: Chain of causality
When you tip over the first domino, the others will fall in order. You don't
need to see all the dominoes fall to infer that they must have fallen. All you
need to see is the first domino fall, and then shortly afterward the last
domino fall. The middle part can be invisible (see Figure 3-5).

Figure 3-5: Obscured chain of causality
If you kick off a process and don't get the result you expected, something
went wrong. If you do get the result, a lot of things must have gone right.
But it's not that simple, because the dominoes might also split into many
threads (see Figure 3-6).
There may be many effects, and your check is probably not looking at all of
them. For instance, testers rarely look at memory usage. Perhaps there is a
memory leak. Perhaps a record must be correctly stored in a database, and it
wasn't. We're not saying that every check should check every single
possibility. We're reminding you of how complicated software is compared
to any output check you could feasibly perform on it.
So the meaning of a check depends on the design of the check versus the
nature of the product under test. You can only evaluate that if you have a
sufficient understanding of the product and its potential failure modes. You
must understand what kinds of product risk your checks are sensitive to and
what kinds it can't detect. This is why it's irresponsible to blindly accept test
cases or automation gifted to you by other testers.

Figure 3-6: Multi-threaded chain of causality
Fail Is Not an Option (for Good Reporting)
We don't like to say "fail" and "pass." You'll notice that we put them in
quotes, as if handling them with surgical gloves.
The word "fail" usually fails. An example is the immediately preceding
sentence. What are we complaining about? We haven't told you yet. All you
know is that we are worried about something having to do with the word
"fail." We've left you hanging. We could have written that the "failure" tells
you nothing about the kind of failure that occurred, whether that represents
a genuine problem for you, how bad that problem is, and what, if anything,
you need to do about it. Even a "check engine" light on a car gives you
more than "fail."
We advise testers who use the word "fail" not to leave it at that.
Immediately follow it with details about the meaning and significance of
that failure. If any check failed, say what problems you found. If you
experienced failures and haven't yet completed your investigation, say you
are still looking at it. If all your checks pass, say that the checks "revealed
no problems to you."
Finally, a good check cannot be created unless the matter to be evaluated
can be reduced to an algorithm without losing sight of what users and

stakeholders care about. This is one reason why AI can be quite difficult to
test. Reasonable people can disagree about what constitutes good output.
Even when they can agree, the amount of time and energy required to work
out an algorithmic version of their agreement may be prohibitive.
Ultimately, the existence of a good check means that there is little
controversy about the output that should be produced. Where controversy
cannot be resolved in advance, checking cannot be a solution. We must then
turn back to the parent process of checking, which is testing.
The Anatomy of a Test
So far in this book, we've said a test is more than a check. We've said it's a
human activity that cannot be automated. It's time to drill down and talk
about what a test is in detail. First, recall that we defined "test" as any
encounter with a product that constitutes an episode of testing. Combining
that with our definition of testing, we could say that, in more explicit words,
a test is a specific episode wherein a tester learns about a product via
experience, exploration, or experimentation and from that makes some sort
of evaluation.
As we think about it in RST, a good test always has the following six
elements (even though sometimes they may not be explicitly defined or
consciously designed by the tester):
Tester: Every test has a tester who will perform it, is performing it, or
has performed it. The tester is no mere bystander. The tester controls,
interprets, records, and takes responsibility for the test in some way. A
responsible tester23 is accountable for the value of the test to the
clients, whereas a supporting tester is anyone who helps test in some
way. It's the engagement of the tester that makes the critical difference
between a check and a test.
Story: A test is an event, and every event has a story. Even if you are
planning a test that you haven't performed yet, there is some sort of
story. The story of a test is how it came to be, what may happen with
it, and what did happen with it. The testing story includes your
assessment of the value of that test to your clients.

Motivating risk: Every good test has a reason for being. It's looking
for a bug of some kind. The motivating risk is the kind of problem that
the test is designed to discover. While that's not always explicit, a good
tester can frame any test in terms of risk when asked.
Coverage: Each test is an encounter with the product. "Coverage" is
the aspect of the product that it encounters. More on this below.
Oracles: Each test can discover bugs. An oracle is any means by
which a tester recognizes a bug when one appears. If you've ever seen
anything you thought looked wrong, it was some oracle that alerted
you. For checks, oracles must be explicitly specified. But for a test,
tacit oracles play a big role, no matter what explicit oracles you also
employ. (More on this upcoming.)
Procedure: Finally, there must be a way to perform the test. Every test
has some kind of procedure associated with it. This is not necessarily
written down or formalized, but it's known to the tester as a
combination of tacit knowledge and explicit steps. Note that the
procedure of a check is an algorithm, so with checks, only explicit
steps matter. Testing, however, is infused with tacit knowledge.
As it is being performed, a test is embodied by the actions and intentions of
a tester, while the tester is in turn guided and focused by some idea of the
other five elements. We literally call these test ideas in RST. A test idea is a
very general construct. It means any idea for performing a test.
A test may start its life as a vague idea in your mind. It becomes an actual
test when you begin to enact that idea. As you go, whether you are thinking
it out beforehand or improvising it in the moment, it becomes more
substantial, more efficient, and crisp. "Writing" a test is exactly like writing
a poem or book, in that way.
Coverage
Test coverage is the extent to which you have examined the product with
respect to some model related to it.
As we wrote in the "Systems and Models" section of Chapter 2, you cannot
intentionally test anything without a model of it. But there are lots of
different ways to model a product. When we say "model," most people

think of formal graphical models, such as flowcharts, class diagrams, state
machines, etc., but models are way more than that. A list of features is a
product model. A capsule description of a product is a model. A mock-up of
a GUI is a model. Any idea that is a description of or reference to
something else is a kind of model of that thing.
Source code is also a model of the product, and some people immediately
think of code coverage analysis (what code has been executed during
testing) whenever the word "coverage" comes up. But that is only one kind
of coverage; nor is it especially interesting to a tester who isn't also the
developer. For instance, if you have achieved 90% code coverage, that tells
you nothing about data coverage or requirements coverage. In fact, all it
really tells you is that there is 10% of the code you certainly did not test.
There are also models related to a product but not necessarily of the
product. Relevant models may be models of users, of the world in which the
product operates, or of the data it processes—anything that helps us gather
evidence about risk associated with the product. Consider years, months,
weeks, days of the week, hours, minutes, and seconds. Those words
represent a model of time, which is a basis for covering how products
handle time, time intervals, or time-related data.
You can think of any particular way of modeling a product as representing a
sort of "surface" of that product. Now, imagine turning the product around
in your mind like a Rubik's Cube, looking at its various surfaces, and
studying how they interact. The art of testing a product is partly in
discovering all its interesting surfaces and examining the product via those
surfaces.
The Product Factors list, in the Heuristic Test Strategy Model (or "HTSM,"
which is discussed in Chapter 4 and documented in Appendix B), is
designed to help you identify all the testable surfaces. In fact, as its name
implies, the HTSM is designed to help you remember things that matter
when deciding how to test something important.
Or how about this: imagine the product as a maze, in which some of the
passages contain little bags of treasure. (To someone who intends to use a
product, encountering a problem is something undesirable. To a tester with
the mission of finding problems before the users do, problems are like
treasure.) Finding the treasure is easy, in theory: simply visit each passage,

intersection, and dead end, and as soon as you've visited all of them and
scooped up all the bags, you're done.
There's a catch: from the outset, we often don't know much about the maze.
When we arrive at the entrance, we don't know its extent. The walls of the
maze are too high to see over. We could be standing right next to a bag of
coins, just on the other side of a wall, and not be aware that it is there.
One way around that problem would be to walk around the maze, taking
notes and making maps systematically as we went. The map—a model—
affords us a way to represent the maze as though we were seeing it from
above. Then we could track where we've been.
It could be more complicated. Perhaps there are bits of the maze that aren't
visible or accessible all the time. There may be hidden doors and concealed
passageways, such that we could walk right past them and not notice.
To make a reasoned argument that we've searched the maze well, we'd need
to consider a variety of perspectives from which we could look at it. With a
view from above, we could identify the passages that we had visited and the
ones that we hadn't. We might even be able to see areas of the maze that
aren't immediately accessible to us and notice gratings or trap doors on the
ground. An aerial view might require a viewing platform or a drone—and
we might need time to construct or configure them—but in theory it could
be done.
Furthermore, no matter what perspective we might have on the maze, some
things remain hidden relative to that perspective. Depending on the angle
we're looking from, some areas at the base of the walls may be invisible to
us. If we can get a bird's-eye view, there may be bags of treasure hidden
under the chairs, the bridges, or under foliage. Worse, bits of the maze are
being worked on every day, with passages and intersections being
rearranged, and new treasure being dropped unpredictably.
This maze is a simplified model that is nevertheless analogous to the
structure of a software product. To be a great tester, you must be able to
make models and reason about them. So, practice your modeling skills.
Teaching this is outside the scope of this book and our methodology, but we
do have some suggestions:

Learn about formal modeling protocols, such as state models,
flowcharts, value stream maps, architecture diagrams, sequence
diagrams, and data models—try UML.
Practice modeling your products less formally, such as by drawing
rough diagrams using stick figures, boxes, lines, and ideograms (for
example, Figures 1, 7, 8, and 12 in this chapter).
Make lists of features and organize them into categories, outlines,
mind maps, or story maps.
Practice summarizing a product or feature in one or two sentences.
Intentional Coverage
Coverage can be intentional or incidental. To cover something intentionally
during testing means that you have a particular model in mind, as well as a
specific way of sampling that product according to that model. It means the
ability to answer the following questions:
Where did you look for bugs? In other words, what part of the
product did you attempt to test? Different parts of the product may
require very different kinds of sampling and have very different
operational dynamics.
What did you actually look at? What kinds of things did you
examine, and what kinds did you intentionally ignore, or otherwise
find difficult to see? What kinds of things could you have looked at if
you had special tools? Did you look at text? Graphics? Pixels? Did you
notice colors? Did you see things as they changed? Did you look at log
files?
How thoroughly did you look? Within the realm of what you
observed or tried, how completely and carefully did you go about it?
Did you examine everything of a particular kind, some of those things,
or just one example? Did you record your process or save the data you
collected? If you were looking at text output, did you analyze all the
text or just bits of it?
When did you look? Today? Several builds ago? Last month? Is your
coverage current, or could it be outdated?

When we want to talk about a specific item to cover in RST, we call it a test
condition, testable element, or a product factor. (We use these terms
interchangeably.) The process of identifying specific testable items we call
product analysis or sometimes product-factoring.
Incidental Coverage
Now, what if a bug is hiding in a place that you aren't testing and don't even
realize you should be testing? This is a challenge, but there is hope:
incidental test coverage.
Incidental test coverage is when you test something without planning to or
perhaps without realizing it. By definition, you don't explicitly plan your
incidental test coverage. But there are things you can do to maximize it. For
instance—and no, we are not joking—you can test in a sloppy way.24
Unlike machines, when humans try to perform step-by-step checks, they
often make mistakes or introduce harmless variations into the process. This
regularly leads to the discovery of unanticipated bugs. In fact, a prime
motivation for beta testing is that you never know what a beta tester might
do. It can be excellent medicine to unleash a herd of confused and
exuberant users on a product.
James introduced a term for this many years ago—galumphing.25 Consider
the following behaviors:
Clicking randomly around the background of a screen
Entering data into a field, erasing it, and reentering it
Viewing the online help at random intervals
Attempting to interact with disabled items
Copying data from one place to another and then interacting with the
new copy
Logging out and then in again
Changing the screen size and then changing it back
Waiting for random intervals

If you are performing an intentional test procedure and randomly inserting a
step like one or more of the items on the preceding list does not disturb that
procedure (as far as you know), then that would be an example of
galumphing. We define galumphing formally as the process of introducing
inert variations (i.e., changes that shouldn't matter) into a test procedure
(so as to exploit Ashby's Law of Requisite Variety to explore a phase space
with unknown degrees of freedom). When you tell someone what
galumphing is, you don't have to say the parts in parentheses out loud.
That's a fancy way of saying that we try to perform tests in harmlessly
different ways ("inert" variety for the sake of variety) in case it turns out
that there is a previously unknown variable or behavior (a "degree of
freedom," as a statistician might say) within all the possible states that a
product can have (its "phase space," as a physicist might say).
Ross Ashby's Law of Requisite Variety comes from the field of cybernetics.
It proceeds from Ashby's observation that any system of control must have
at least as much freedom in it (variety) as the system it is controlling. For
instance, if you try to drive a car along a winding road, and the road turns
more sharply than the turning radius of your car, you will run off the road.
Another way to say it is that if the road is "free" to turn sharply, you must
be free to turn at least as sharply if you want to stay on it.
When you test, you are controlling the product. If you don't or can't put it
through all the paces of which it is capable, then bugs will hide from you.
Ashby's Law says your tests must have sufficient variety in them to visit all
the bugs you want to find.
To Cover Is to Sample
No matter how you have modeled the product in your mind, when you test
it intentionally, you are sampling it based on that model. Most "test
techniques" are simply heuristics for modeling a product, coupled with a
way of sampling it.
Sometimes the modeling process and the sampling process overlap. They
might even happen at exactly the same time, which would mean that, in the
course of learning what the product is intended to be and do, you collected
enough evidence about how it behaves that you don't need to do any more

examination of it. This is partly what we mean by our definition of testing
as "evaluating a product by learning about it ..."
If the idea of the product is well established before you begin performing
tests on it, then your sampling process will come after your modeling
process. This might be because you read a detailed specification before you
saw the product, or because you are retesting something that has changed in
some way (but not a lot) since you last looked at it.
We like to speak of "tours" in RST. This is an easy way to think about
sampling. Here are some example tours:
UI tour: Discover and visit every single user interface element.
Interact with it in every way you can think of.
Keyboard tour: Press every key and key combination you can think
of.
Documentation tour: Look through all the user documentation and try
every example.
Complexity tour: Look through the product and try to identify and
experience the most algorithmically daunting parts of it. Where does it
seem to be doing a lot of work?
Data tour: Look through the product to see all the kinds of data that
can be input, imported, or interacted with. Exercise the product with
that data.
Benefits tour: Try to see the best things the product can do under the
most favorable circumstances, as if you were its proud parent. We also
call this sympathetic testing, and it's a great way to rapidly get to know
a product that you are seeing for the first time.
Blooper tour: Users make mistakes. In regulated software
development, such as medical devices, manufacturers are required to
test products not only as they are intended to be used but also with
"reasonably foreseeable misuse." A blooper tour involves moving
through the product and making every kind of mistake you can think
of. How easy is it to lose your data?

Business rule tour: Move through the product looking for evidence of
"business rules," by which we mean any constraints or logic that
trigger certain behavior under certain conditions.
If you want to get serious about sampling, you mainly have to think about
data. But here is where we find a lot of unhelpful folklore in the testing
world. If you go out to YouTube right now and search for videos about
"equivalence class partitioning" or "boundary testing," you will find lots of
nearly identical advice, based on shallow thinking, that has been copied
almost unchanged from forty-year-old textbooks. It's a thunderous echo
chamber of stale misinformation. We, James and Michael, didn't find the
advice helpful even when those books were new. Back in those days, since
most testers never read any books on testing,26 and there was no YouTube,
the simplistic advice in the prevailing textbooks caused little harm. When
commercial tester certification companies started springing up, though, the
situation changed. They seized on the old books and uncritically amplified
all the bad advice.
We don't have the space and time in this book to set things fully right—this
is much better handled in live training and coaching, anyway. Yet we must
try to give you a flavor of the problem and what to do about it.
How Not to Think About Equivalence Classes
Imagine you have a product that lets you buy groceries online. Imagine that
there is supposed to be a discount when you spend more than $100 and a
bigger discount when you spend more than $200. In that case, equivalence
class partitioning (ECP) folklore tells us that the equivalence classes are
$0 <= x <= $100
$100 < x <= $200
$200 < x
Some would say that the edges of each of those classes are technically
outside the equivalence classes, because they are boundaries. Fine.
Boundaries are special.

Now, what have we accomplished here? According to ECP folklore, the
great thing about an equivalence class is that you can choose just one value
from it, instead of having to test with all the possible values. In other words,
because $20.15 will get the same discount as $40.99; you don't need to test
with both amounts, because if either one gets the wrong discount, the other
one will also get the wrong discount. That means, instead of, say, 30,000
test cases (trying all the dollar amounts from $0.00 to $299.99 to see if the
proper discount was applied), we can get the same testing value by doing
just three cases (or eight if we also include the boundaries)!
Eight test cases will be easier to perform than 30,000, so that is pretty cool.
People who advocate this way of thinking speak of how it helps with test
case optimization and efficiency...
Except it's all just a fairy tale! It's a story that testers tell themselves to
make them feel better about their shallow testing. How plainly can we say
this? There is hardly a shred of rational substance behind this way of
thinking about equivalence classes.
Let's rewind and reconsider. What we started with was a specification for a
discount strategy; it's how the product is supposed to work. It's not a
statement of how the product actually works. How could we think so? We
haven't tested the product yet! Only when we test the product do we find
out how it actually works—not before. Equivalence classes that are derived
from fond desires written in a spec do not say much about the absence of
bugs in a real product.
If you think about it, this view of equivalence classes is a strategy for doing
the minimum possible work to claim that you exercised the product. But
that is not serious testing. At best, it detects obvious bugs.
Apart from being too trusting, a second problem with ECP folklore is that it
is myopic. It tends to focus on one dimension at a time. The discount
strategy is not the only thing that should be driving our choices about values
to try. For instance, there could be taxes or shipping costs. There might be
issues with rounding or thresholds of spending that relate to the business
rules of payment gateways. When all the rules are considered, we may have
a very different notion of what should be tested.

A third problem is that ECP folklore doesn't consider the many kinds of
bugs that could occur. In other words, ECP folklore doesn't come packaged
with a "theory of error." Why not? Because that's hard! There is no one
canonical and universal list of bug types. Even if there were such a list, it
would be too long and difficult to apply in any systematic way. For
instance, here's a kind of bug: a standing-state failure. This kind of bug is
when a state that should trigger an action fails to do so because the system
is only paying attention to state transitions. For example, a ring of guards
around a bank vault will not stop burglars who are already inside the vault
when the guards arrive. A standing state failure might occur with the
discount system if the user changes the items in the shopping cart at the
time of checkout. That should cause a recalculation of the discount, but
perhaps the developer only coded it correctly for the case when the
checkout screen was first invoked. The discount may not be recalculated
and reduced if an item is dropped after already reaching that stage of the
process. Drop enough items, and the discount may cover the purchase price
of the remaining ones! But ECP doesn't have any mechanism to warn us
that a set of items in a cart before checkout may not be handled equivalently
to the same set of items during checkout.
A Better Way to Think About Equivalence Classes
There is a useful intuition behind equivalence classes, but that intuition
should not be treated as rational optimization. Think of equivalence classes,
instead, as rumors. They are guesses based on a certain way of modeling
the product. They are heuristics, not facts. As a tester, apply them
skeptically.
This more relaxed form of equivalence classes is everywhere, and we use it
all the time. When you click a "submit" button, you generally don't worry
about where on the button you are clicking. Toward the center? At the edge?
Who cares? There may be hundreds of pixels you can choose from on that
button. You'll ignore most of them. Why? Because you are probably not
concerned with testing the underlying GUI widget framework. Someone
else probably tested it. It's probably okay. Yes, we have seen situations
where part of a button is masked by an invisible element that got in front of
it, but that's rare. Of course, if you want to be absolutely sure, you will have

to sample all parts of that button, but the question arises: is that the most
important thing you could be doing?
That brings us to the fundamental purpose of ECP done right. Equivalence
class partitioning is not telling us what doesn't need testing at all. Rather,
it's about what doesn't need to be tested right this moment. It's a guess about
priorities. That's it. The fundamental idea behind the RST version of the
ECP heuristic is to test major differences before minor differences, with
respect to however you model differences. You could say that ECP is about
difference-first testing (or maybe delta-first, if you want to sound cool).
Change your way of modeling the system, and you will also change what
counts as a large or small difference. For instance, in the preceding
discounting example, if you want to detect rounding errors, you would want
to distinguish between the set of calculated discounts that ended in a
fractional penny greater than or equal to .5 cents or less than .5 cents,
respectively. You would then check the result to see that proper "Banker's
rounding" is used. If you were thinking about formatting issues, you might
want to change the screen size and pick values large enough to create
discounts that overrun the size of the field allocated for them. Different
equivalence classes emerge depending on what you care about.
You will generally get more value out of clicking ten different buttons once
than from clicking in ten different places on the same button. You will
generally get more value out of trying the different discount levels before
you test multiple cases within the same discount level. After you have
achieved reasonably shallow coverage by trying the things that are more
different, you can then progressively go deeper by retrying what you did
before with minor variations or in differing combinations. Equivalence class
partitioning is just a way of talking about the differences that you suspect
matter.
The RST definition of an equivalence class is: a set of apparently redundant
test conditions, with respect to some product risk, in some context. The key
word is apparently. This means equivalence classes differ depending on the
kinds of bugs you care about and the kind of testing you are doing. But
above all, they depend on your judgment.

A Better Way to Think About Boundaries, Too
Boundary testing is another area where testing folklore leads us wrong.
What is the typical description of boundary testing? Most say that boundary
testing is the process of testing data values that are right next to some kind
of limit. Usually, they say, "test with one less, exactly at the limit, and one
more." We aren't summarizing: that's all there is to it.
Almost no testing book tells you that boundary bugs may occur far away
from where you think the boundaries are, that there may be hidden
boundaries, or boundaries that move around, and that one of the purposes of
boundary testing is to discover them in the first place. That means the
modeling process is not so easy. You can't just read a spec and believe what
the developers tell you.
In RST, we define boundary testing as any testing intended to explore risk
associated with boundaries.27 And we use a broad definition of boundary:
any principle, means, or instance of classifying things into different sets.28
The modeling process for boundary testing is whatever tells you where
boundaries might be and the nature of those boundaries. The sampling
process for boundary testing is whatever will maximize the probability of
discovering boundary-related bugs.
By our definition, a doorway to a bar is a boundary, but so is the door itself,
and the doorman, the bartender, the sign that says: "You must be at least 21
years old to enter this establishment," and the principle that you must be at
least 21 years old. Consider the discount strategy described previously. $0,
$100, and $200 are boundaries because they are dividing points between
different sets. The code that implements that rule is also a boundary, as is
the code that prevents a product from having a price of "NULL."
The urge to do boundary testing comes partly from the Perimeter
Assumption, which is the belief that if things are okay out at the extremes,
then they are probably okay within the extremes. It's the reason to
concentrate troops around the outer wall of a fortress, rather than dispersing
them uniformly through the grounds. (Of course, that is a heuristic: The
enemy may already have snuck inside. There are a lot of other kinds of bugs
that have nothing to do with boundaries, and there are boundary bugs
caused by boundary-handling code that could manifest far from any visible
boundary.)

Boundary-testing heuristics include the following:
Confusable data: Choose data that seems to have the maximum
probability of being miscategorized. This is the heuristic that leads to
the "try with one more than the stated boundary" advice. We also like
to use the Low Bridge heuristic, which says you should try data that is
very close to a limit, although not violating it. Think of a tall truck
moving down a highway at high speed as it comes up to a bridge that
might not be high enough.
Extreme values: Choose data that exceeds the boundary in the most
extreme way that you can feasibly concoct. This can reveal the
existence of unknown or undeclared boundaries. Exceeding a
boundary by a little bit might lead to a nice error message; exceeding it
by a lot could cause a crash.
Foreign data: Boundaries that are handled well for data that you type
in might not be handled at all for data that you import or load from
storage—especially from older versions of the product.
Random data: Choose data at random (even if far away from known
boundaries). This will help you stumble into undeclared or unknown
boundaries.
Steeplechasing: Discover if all the boundaries in the product are
consistent and compatible by entering the most extreme legal data and
then performing every possible action with it (edit it, print it, query it,
display it, etc.). This is like running a Great Dane or an Irish
Wolfhound through a dog agility course. It may get over a strongly
built bridge, but perhaps it won't fit through the tunnels.
Viewport analysis: Treat the product under test as a system that gives
you a view onto some set of data. Each interface of the product is a
kind of viewport, like looking through a keyhole of a door or a
periscope. Is that viewport doing its job? Ask yourself, as you tour the
product, "What am I seeing, and how does that relate to all the things I
could possibly see? Am I seeing all the things I should be seeing and
nothing I should not see? Can I change what I am viewing in all the
ways I should be able to?" Examples: scroll a long document and note

if all the content is there and in the right format; look at it with
different zoom levels; view the same data with different user rights.
Move the boundaries: Look through the product for boundaries that
can change, then change them in interesting or extreme ways. An
obvious example of a changeable boundary is a filter or search that
establishes a viewport onto a larger data set. But there might be other
kinds of boundaries that are scalable or conditional in some way.
Sometimes you'll find these by looking at configuration settings.
Read the code: It may help to look at the code itself. You may find
boundaries that they didn't tell you about.
Pushing Limits
One kind of boundary is a limit. On the far side of a limit is failure. The
question is whether that limit can be breached, and if so, how gracefully and
safely the failure is handled.
Often, testers are not told about the expected limits of the products they test.
This could be because nobody has yet bothered to find out, or it could be
because to find out would require a lot of testing—and so we testers will be
the first to know. But some silence about limits may be emotional.
Sometimes developers and management feel it is impolite to reveal limits,
as if the tester is being churlish to seek the truth about the product.
Especially in the case of GenAI systems, some may get the impression that
you are cruelly torturing their poor product until it breaks down in tears.
Our clients may talk about their product as if it is an island paradise, as in
Figure 3-7. "Normal users stay on the island!29 Why are those weirdo
testers swimming away?"
If capacity and performance claims are public or contractual, or if the
product is supposed to be scalable, you will have a strong justification for
testing its limits. And, of course, if there are safety or product liability
implications, the behavior of the product at limits must be explored.

Figure 3-7: The product as an island
Although you may decide it isn't worth the time and expense of discovering
them, all products have limits, and limits are the business of any serious
tester. Even if you don't discover the hard limits of a system, you should
report the limits of your testing (e.g., "This product has been tested with
100 simultaneous users on one server"). The team may want to put an
explicit limit into the product to prevent users from exceeding the tested
region of operation.
Limits can be inherited from a third-party library, component, or service.
We might learn about them by looking at their documentation, but you will
often find only silence on that subject. Limits also emerge from interactions
between elements in ways that are not described by any documentation and
shock everyone except the testers, who have an abiding faith in trouble.
Maybe you could test limits using special configurations or test jigs. If you
believe a limit depends primarily on memory or mass storage, then you
might be able to artificially limit those things to see what happens. But
because of hidden boundaries that may be lurking in the system, you can't
reliably assume that a resource-reduced system will behave at the limit in
the same way as it would if it were resource-maximized. Similarly, if a car
manufacturer crashes a car in the laboratory to test an airbag, they hope that
the results will be representative of real life—yet reality will present many

more variables that could influence the proper functioning of the airbag.
The field reliability of that airbag may not match the lab results.
Sometimes, you just have to leave the island and swim to the edge of the
world, as in Figure 3-8. This process usually goes better if you have a
speedboat (i.e., automation). RST practitioners routinely use tools to
generate large amounts of data or perform repetitive and stressful
operations.
As you head to the horizon, look for the following things:
Warning signs and guard rails: Is the user thrown off the edge of the
world without warning? Are there indications available to them that
they are approaching a hard limit? Example: a comment field may
have a character limit. A well-behaved GUI will have a live character
count display that allows the user to know when they are getting close
to it. One of the oldest heuristics in RST is called Rumble Strip, which
says, "Pay attention to subtle clues that are not failures but could
indicate impending failure."
Figure 3-8: Seeing what happens at the end of the world
Unrecoverability: Testers know that "failure is always an option." But
what comes after failure is of prime importance: Can we recover?
What is the process of recovery? Will the user know what to do?30

Insufficient capacity: Can we imagine a plausible kind of user who
will need bigger limits? Are the observed limits consistent with
explicit and implied claims? Can the system scale if necessary?
Insecurity: Besides testers, bad guys love to explore limits. And they
will, whether or not you do. Sometimes, the other side of failure is a
data breach. Remember that, with security bugs, it doesn't matter how
obscure a bug is. All it takes is for the wrong person to find that one
crack in the wall, and they can turn it into a superhighway.
Worst case scenario: What is the worst that can happen? What is the
worst that can plausibly happen? What is the worst that can
accidentally happen even with well-intentioned users ("reasonably
foreseeable misuse")? Note that the European Union directive on
product liability, which makes it easy for people to sue companies for
damages, applies to software. It covers physical harm to people,
physical harm to property, and also data loss—and no end user legal
agreement can disclaim that liability.31
Oracles
A test oracle is any means by which you recognize a bug that you encounter
during testing.
Coverage flushes out the bugs, but you still have to spot them when they
happen. For that, you need an oracle. If you have ever found a bug, ask
yourself how you knew it was a bug. Whatever your answer is, you are
describing some kind of oracle.
Testing textbooks are weirdly silent about oracles. Even when they mention
them, the focus is on one kind of oracle: authoritative reference documents,
such as specifications, user stories, examples, master files, documented or
coded "expected results," etc. Have you ever seen those test case templates
that have a little space next to each step called "expected result?" We think
they're absurd. No good testing can be limited to the scope of one-liner
oracle statements.
Documents are useful as oracles, but the world of oracles is much bigger
than that. For instance, many oracles are suggestive rather than
authoritative, and many are tacit rather than explicit. Consider an error

message for an ATM machine that says, "Hey loser! You can't overdraw
your account!" Maybe it comes with a picture of a kitten frowning at you. Is
that a problem? We can probably all agree that it is. But we didn't need any
spec to tell us this. Instead, we have a pattern that RST calls the Image
heuristic: a product should give its users a professional impression of its
developers. Typos and other "cosmetic" bugs aren't problems because they
impair functionality, but rather because they impair the relationship of trust
between the vendor and the customer.
The Image heuristic isn't authoritative. It's suggestive—and subjective—and
that's okay. What's important is that you have a clue about a possible
problem worth reporting. We must make a great many judgments while we
test—not merely to spot bugs but also to decide which bugs are more worth
pursuing, reporting, and advocating to fix. Testing requires the social
competence to make such judgments and arguments. Applying and citing
good oracles helps us produce a convincing report.
All oracles are based on some consistency relationship between the product
as it ought to be and the product as it is. Here are twelve32 different general
oracle patterns we identify in RST:
Aspirations: The product is doing what it was intended to do—but
you believe that isn't good enough; you think you can convince the
stakeholders to aim higher.
Familiar problems: You have a memory of specific bugs you've seen
before. The more experienced you are with a technology or domain,
the more bugs you've seen. Familiar problems means: "I've seen this
kind of behavior before, and it was a bug then, so it's probably a bug
now."
Explainability: The product is behaving in a way that you can't
explain, even if you have no other reason to think it's wrong. As a
tester, if you find something confusing, don't just shrug—investigate!
Either you will learn something important about the product, or you
will justify that it's a bug.
World: The product is giving output that is not consistent with a fact
about the world that the software is supposed to represent. An easy
example is an incorrect timestamp ("That's not the correct time!"). A

scarier example is a self-driving car that mistakes the side of a truck
for the sky (this happened on May 7, 2016, in Florida).
History: The product is behaving in a way that is not consistent with
its past behavior, in a situation where you expected it to be the same.
This principle is often used in regression testing.
Image: The product does not present a professional image, which may
lead to distrust and disaffection by users or otherwise compromise our
brand.
Comparable products: The product has a feature that is comparable33
to some other product, yet its behavior does not match that of the other
product. Maybe the problem lies in the other product, or maybe it's in
your product, but it should be investigated.
Claims: The product is not behaving consistently with written or
spoken specifications, contracts, marketing literature, online help, or
some other authoritative claim about it. Note that when a developer or
designer tells you that a product should behave a certain way, that's a
claim too. Maybe the claim is wrong, and the product is right, but the
inconsistency is noteworthy.
User desires: The product is behaving in a way that we think won't
please its users (or other people affected by the product). In the HTSM
(Appendix B), there is a list of quality criteria categories that we like
to use to help us crystallize our ideas about what users want.
Product self-consistency: Part of the product is behaving in one way,
while another part of the same product is contradicting it, or is
otherwise different in a distracting or confusing way. A product should
present a consistent face to its users.
Purpose: The product is not behaving consistently with the purpose
for which it was created. This is a very powerful oracle. If you
understand why a product exists—which benefits it's supposed to
provide or problems it's designed to solve—then you might not even
need a specification to test it. If you feel frustrated when using a
product, that often suggests a violation of purpose.

Statutes and standards: The product is inconsistent with some law or
industry standard. GDPR, HIPAA, and Internet RFCs are all examples.
These labels form the acronym A FEW HICCUPPS, so naturally, we call it
the Hiccups heuristic. These ideas overlap and combine in different ways.
But each one contributes something not found in the others. We contend
that any thoughtful tester, regardless of any other specific oracle they might
use, naturally applies these general oracle heuristics while testing. We didn't
invent them. Our contribution has been to notice testers using them. We just
put them into words.
You might wonder what the point is of arranging these ideas into such a list.
Didn't we just say that any thoughtful tester will use them intuitively? Why
bother recording them, then? Here's the reason: because when the process
of testing is only unconscious and intuitive, managers can't see the skills.34
They start thinking that there is no such thing as skilled testing. Certain
pundits then claim that the only processes that matter are the ones
committed to words. To fully appreciate your own tacit knowledge, it helps
to have explicit knowledge that connects to it (see Chapter 2,
"Foundation").
One of the purposes of RST is to create a nurturing environment for the
tacit knowledge and skills of testing. This does not necessarily mean that
we turn tacit knowledge into explicit knowledge, though. Rather, we try to
identify, label, and foster the presence of specific areas and kinds of tacit
knowledge. We ask our students to study the Hiccups heuristic so they can
appreciate, articulate, and deliberately practice each oracle pattern, not
because we want them to turn it into some sort of clockwork procedure.
Oracles May Take Many Forms
Imagine that you are testing a product while sitting next to the CEO of the
company that is paying for the whole project. Contractually, conventionally,
and socially, it is clear that this guy is the only stakeholder. Let's even say
he's an eccentric billionaire and will be the only user of this app. He knows
what he wants, and he's going to get it. As you are testing, you bring up a
screen that has a purple background.
"Lavender? I hate lavender! I don't even like the smell of it," he says.

"Okay," you reply, wondering if he might have a touch of synesthesia. You
write in your notes: PURPLE BAD.
This is an example of a live oracle. In this case, your oracle is sitting next to
you. Now, let's say he gets up and leaves the room. You proceed to a
different screen, and you notice that the buttons on that screen are all the
same hue of purple. Is that also a bug? Sure, it is. But what is your oracle?
Your live oracle has left the room. So, in this case, it's your explicit
knowledge of what this user desires.
Meanwhile, you might notice that the screens are loading more slowly each
time you visit them. That is strange, and you decide based on your tacit
background knowledge of software that this is a problem. It just seems
obviously wrong to you, without anyone needing to tell you why.
If the CEO returns with a laptop, shows you another product, and says, "I
want it to look like this," you will gain a new kind of oracle, still, loosely
known as a parallel oracle, digital twin, or comparable product. A fully
parallel oracle is a product that is intended to have exactly the same
functionality, in some respect, as the product under test. But it's quite
common for parallel oracles to be merely similar in a way that requires your
judgment and situational interpretation.
Perhaps there are explicit specifications: requirements, designs, statements
of work, user stories, etc. They are generally highly authoritative, although
you can always identify a live oracle who has more authority than a
document (e.g., a product owner, CEO, or federal judge). Specifications are
almost always conceptual rather than literal. If numbers are to be added up,
for instance, the specification will not list every possible combination of
numbers along with the correct sum. That's why a lot of oracles are based
on your own mental simulation of the product's intended behavior, which
comes from your knowledge of how it was built and the claims that have
been made about it. But oracles do sometimes take the form of specific
examples. Examples are sometimes called expected results.
Be wary of expected results. Superficially, this popular phrase seems to be
another term for oracle. We prefer to avoid using it in RST because it has
limiting connotations. It encourages testers to think only of immediate and
obvious output rather than the entire spectrum of meaningful coverage. It
encourages a "pass/fail" mindset instead of the open mindset of "Is there a

problem here?" It can also be confusing because what people expect (in the
statistical sense of what they think will happen next) may be very different
from what those same people expect (in the normative sense of what they
want).
During experiential testing, many oracles make themselves known to us as
feelings of surprise, confusion, or annoyance. These feelings are very
important to monitor as you test. Speaking loosely, a tester can be thought
of as a sort of human smoke alarm for quality problems. This is why, to do
your best testing, you must be feeling motivated and engaged. You must be
eager to find bugs. You must make yourself sensitive.
In his paper Problem Detection,35 Gary Klein and his coauthors analyzed
how people become aware of problems. He studied nurses, firefighters, and
naval commanders, but the patterns he found apply to testers, too. He
described the way people perceive situations as being a reflexive
relationship between data and an interpretive framework (or "frame").
Seeing certain things activates a certain frame, and that frame then controls
how you interpret what you see. What happens when you are monitoring a
situation—or testing software—is that you get into a flow of seeing and
interpreting until some violation of "typicality," as he puts it, challenges the
frame. At that point, you either preserve the frame by explaining why the
data is anomalous but not problematic, or else you adopt a new frame of
"failing system" or "crisis in progress," which causes you to reinterpret all
the other data you have.
Figure 3-9 represents our simplified view of this process. It shows how, in
the course of investigating and discussing a problem, we might connect
different kinds of oracles together into a comprehensive account. The figure
shows that testers nearly always begin with their own judgment, but the act
of reporting bugs and advocating for fixes often brings them into
conference with the stakeholders, who have ultimate control over what is
considered "good quality."

Figure 3-9: Oracles take different forms.
Remember that quality is "value to some person (who matters)." So values
are the ground in which test oracles grow. A sense of value is apprehended
by the tester via various processes, including socialization as a computer
user, experience in the problem domain, conferring with stakeholders,
personal reflection, and studying relevant documents and examples from
sources both authoritative and nonauthoritative.
As you can see, we like to take a broad, inclusive view of oracles. Some
oracles can be automated without much trouble. Some are impossible to
automate. If we were to ask you about all the oracles you use, you would be
unable to tell us all of them. You probably wouldn't say, "The product
should not hack my bank account" or "The product should not cause my
computer to catch fire." Yet you would surely consider those things
problems if they occurred.

Tacit, unconscious oracles are the main reason why no one can truthfully
claim to have automated a test that a human tester has performed. How
could they, when testers themselves are not entirely aware of the testing
they do and cannot describe it when asked? It's possible to automate
something, of course. We are merely pointing out that whatever that
something is, it is a different thing from what the tester did.
Procedures
A test procedure is a specific way for a person to perform a test.
Test ideas can be useful without a procedure. Just discussing test ideas and
performing thought experiments can be helpful in the earliest stages of
development, before code is written. But to perform a test, you must have a
procedure.
A test procedure is not necessarily written. In fact, it is rarely written
down.36 As usual in RST, we value both the tacit and the explicit. Ad hoc
procedures, made up on the spot, are procedures, and a procedure document
is not the same thing as a procedure.
Remember that a test procedure, which guides a human's performance, is
not the same thing as a procedure for a check. A procedure for a check is
algorithmic; it's essentially code, even if a human attempts to follow it
instead of having a machine do it. When designing a test, you must consider
how it may affect the tester who attempts to perform it. It may put them to
sleep. It may overwhelm them. It may annoy them. The design of a check,
especially for the purpose of automation, doesn't have to worry about that.
A good tester ought to be able to design, document, and perform a
procedure in some manner that fits the context of the project. These are not
just three different things—they are three quite different skills.
Designing a Test Procedure
When designing a test procedure, you are working with the logic of causes
and effects. Most procedures set out to cover the product in some efficient
and intentional way37 so that you can observe the relevant output without
too much noise or distortion. If you are designing and performing the
procedure in real time, then you are continually making sense of the system

as you test and adjusting your actions to focus and refocus on the things that
matter.
This methodology is called "Rapid Software Testing" partly because we
advocate test procedures that eliminate unnecessary, expensive steps.38 We
also advocate postponing formalization until both the product and your
mental model of it have matured and stabilized. This is because leaving
things open, tacit, fuzzy, and fluid is usually less expensive than
formalizing them.
Designing a test procedure begins with some test idea that provides a focus,
such as:
Investigate an identified risk or claim.
Cover some part of the product.
Apply some oracle to the product.
Apply some test technique or tool.
Every test ultimately involves all four of those things, but perhaps there is
one of them that matters more to you in that context. For instance, you
could choose a technique from the Test Techniques list in the HTSM, such
as flow testing, if you have a product that is more state-based and that you
suspect has more state-related bugs. Or you can think of some other kind of
bug that seems likely (e.g., poor performance, an improperly handled
business rule, or a quality criteria category from the HTSM), and ask
yourself what conditions or actions will maximize the chances of revealing
that bug if it exists—and how you will detect it if it appears. If your goal is
to exercise a certain feature, then you must ask yourself what different ways
there are of using that feature and what variety of data it can process.
Your choices about what to do are also driven by the following competing
imperatives. You can't satisfy all of them at once, so you'll have to decide
which one dominates in your context at any given time:
Minimizing cost and time: This may not be the most important
consideration, but it hovers over every choice that you make.
Especially if overall risk seems low, you will not want to invest in
intricate, deep testing. Be alert to ways to enhance test procedures at

little additional cost. Can you run many variations of a test without
having to perform the setup more than once? Can you use tools to
record information that you can review later?
Maximizing information gain: You want to find trouble, if it's there
to be found. To do that, design procedures that cover a lot of ground
and collect a lot of observations. Consider ways your oracles might
miss bugs and make them as strong as you reasonably can. Typically,
the most expensive part of a test is operating the product, so consider
saving as much output as you can from that process. Of course, then
you have the problem of processing all that data, and that takes time
and energy. Informal exploration can give you a lot of learning for a
small amount of effort.
Maximizing process integrity: You might also need high confidence
that you have indeed tested what you think you've tested. This might
mean careful record-keeping or even a certain amount of repetition.
Generally speaking, to increase the integrity of testing, you will want
to formalize the process. But formalization, especially if done
prematurely, is both expensive and substantially reduces the depth and
breadth of testing by making new tests more expensive, wiping out
incidental test coverage, and eroding tester agency.39
To help you think these things through, you might go through the Project
Environment list from the HTSM. For example, under Deliverables, is there
a test documentation standard or template that you are obliged to follow
because your client believes that it improves the integrity of testing? If you
are under orders to minimize time and cost, can you suspend that standard
and do simpler documentation?
Let's say you know what you want to test, and you have a general idea of
what to do. For example, imagine you are testing a social media system that
lets you set your avatar picture. It seems obvious that testing this requires
you to go to your profile and click the button that lets you upload a picture
of some kind. But deciding on your specific test procedure goes beyond the
obvious.
When James faced this problem while putting together training materials
for his students, he decided:

There seemed to be no need for maximizing test integrity because he
was operating outside of a project, doing this as an exercise.
Accept the test server as configured, and create a new account that
doesn't have an avatar already set.
This is the first time testing this, so be less trusting.
It's probably not worth doing automation for this, especially since no
regression testing is needed. Leave it experiential.
Create some graphics in advance and discover all the places that
display the avatar. Go visit those places.
Variation seems important. Prepare a variety of sizes and types of
graphics, maybe fifty in total. Use representatives of every acceptable
file type, plus very small, medium, and very large sizes.
Cropping could be a special risk, so use graphics with different aspect
ratios.
Do one trial with each graphic, since he could think of no plausible
mechanism for unreliable handling of the same data file.
For an oracle, it should be enough to look at the screen and decide by
"eye" if the graphic is acceptable. This isn't scientific software that
needs to be exact.
No particular need to record and save any output here.
With a decent variety of graphics, the procedure itself can be pretty
simple: For each graphic, set the avatar and then visit some screens
that display the avatar. Discover the maximum allowable size of
graphic.
These are all debatable judgments, but none of them are arbitrary. As we go
along in the project, our mental models improve with experience and study,
so we can make better decisions.
Now, what if James tested the avatar setting and found a bug? When they
fix the bug, will he repeat that test? Often, the answer is no, because this is
a new context that would lead him to revise his test. For example, the risk
context has changed. It may be good enough to test in the vicinity of that

specific bug and its fix, rather than to generally cover the whole feature
again. If the problem were something to do with the size and aspect ratio of
the graphic, then he would prepare data that covered a requisite variety of
sizes and aspect ratios. If it had nothing to do with cropping, then he
wouldn't need to go through much testing of that—except maybe a little bit
for sanity-checking purposes. However, if the bug had blocked or otherwise
distracted him from completing his original test, he would want to go back
to it.
The process will be more exploratory the first time through and probably
more scripted40 after that. This won't be because of anything written down,
but rather just because practice and familiarity will have caused James to
settle into a certain patterned way of performing the test.
Remember, nothing we're talking about here necessarily has to do with
writing a test procedure. Documenting your tests is a separate matter, as is
sharing them with other testers. The procedure-design process is only about
how you decide what exactly you will do, from moment to moment, as you
perform a test on a product.
Anatomy of "One Step" of a Procedure for a Test
Welcome to the messy details of testing. The engine room! The center of
the Death Star! Understanding the dynamics of this process is a key to
excellent testing. It will help you relate to written test cases in a helpful
way, to design test documentation that is helpful to others, or to perform
and report on helpful tests without documentation.
We're talking about test steps.
Consider the following different activities a tester might do:
Improvise a test while using the product under test
Carefully design test data to be used in an intricate formal test
Follow a predefined procedure (whether loosely or meticulously)
Write down a procedure for someone else to follow
Write code for an automated check
Run an automated check and review the results

Although they are quite different in many ways, each of these activities
ultimately requires us to conceive, construct, enact, or analyze an encounter
with the product. This encounter can be broken down into particular
moments, moves, or decisions that we call steps.
Figure 3-10 represents the processes and choices involved with "one step"
of a procedure for operating a product while testing it. We put it in quotes
because there is no fixed and universal standard for how much is covered
by a step. You could say that logging in to an account is a step, or that
clicking the password field is a step, or that moving the mouse pointer one
pixel toward the password field is a step. (That's pretty extreme, but you
could do it.) You could even say that running a suite of a thousand
automated checks—each one a little program—is one step in a test
procedure. At whatever level you choose to think about the steps of a
procedure, this model applies.
Let's walk through this diagram.
Begin at the center. As we said previously, procedures have one or more
focusing ideas. These ideas give direction and meaning to each step of that
procedure. We think of test ideas as the gravitational core of the process,
bending activity around themselves.

Figure 3-10: A single step of a test
Each step involves four subprocesses. We've broken each subprocess into
various kinds of options or events. Notice that some are in "clouds." The
items outside the clouds are things that can be scripted, either for the benefit
of a human or a computer. The cloudy ones are available to humans only.
They must be done live.
Choose: This means decide what to do. In a highly exploratory
procedure, this choice is made by the tester in the moment. In a highly
scripted procedure, the choice is made largely outside of the process of
performing the test, either at some earlier time or in real time by
someone or something other than the tester. In a scripted process, the
choice is normally to proceed to the next step in the documented
procedure—but that might not be possible or advisable. The tester may
break out of the script for any number of reasons, good or bad. A tester
may decide to investigate an anomaly, change the test idea, or
remediate something about the test (e.g., reset the product under test,
fix a tool, or adjust a supporting process).
A major dynamic that operates in every test procedure is what we call
focusing and defocusing. To focus is to maintain a pattern. To defocus

is to change that pattern. (Defocusing implies that you will focus on
something else.)
In general:
If you feel confused, focus; if you feel stuck, defocus.
To improve the integrity of a test, focus; if you want to improve
coverage, defocus (and refocus on something else).
If you need to collect specific evidence, focus; to become aware of
unexpected new evidence, defocus.
The process of focusing or defocusing can occur within a single
step of a procedure or an entire procedure. It can play out in the
evolution of a procedure, or it can play out across a test strategy.
Interact: This means to encounter the product and use it in some way.
Some of this can be scripted, but sometimes unanticipated obstacles
arise that must be overcome, or the tester might make a mistake and
have to recover. (This can be a good way to stumble into unanticipated
bugs.) A tester experiencing the product will have feelings that are
relevant to quality assessment as well, such as annoyance, confusion,
or satisfaction.
Sensemake: This is something only people do41 in a way that is
comprehensible, reliable, and responsible. As we perform our tests, we
not only observe the product but also decide what it all means. In so
doing, we automatically enable our unconscious minds and tacit
knowledge to participate in evaluation, which comes next. Notice how
much we make sense of along the way. Not only do we consider
product behavior and relate it to our expectations, but we
simultaneously monitor the kinds of actions we believe we can
perform, the meaning of changes that occur, the relationship of our
product to other products we have seen, and so on. This gets encoded
into the mental state of the tester; thus, the act of testing literally
changes the mind of the tester. The sensemaking part of testing is
where you construct the main part of the testing story.
Evaluate: Is there a problem here? Only an appropriately socialized
human can reliably predict whether the people who matter will think

so. A scripted process can specify any number of verification activities,
but failing a verification does not necessarily mean there is a bug in the
product. It takes a tester to sort that out. It also takes a tester to judge
whether the current test is a good one. Maybe it's not fulfilling its
purpose and should be abandoned. After each step, we have to
determine whether the system is ready for the next step. This part of a
step is where you construct the ending of the testing story.
When we were writing this chapter and constructing Figure 3-10, we
developed it by recording a number of testers all testing the same thing. We
asked each tester to create a procedure, say what it was, and demonstrate it.
During these exercises, we noticed that testers did a lot of things in their
procedures that they did not explain until we pointed them out.
Remember the iceberg from the top of this chapter? Figure 3-11 is another
version of that. Much of the work testers do is tacit.

Figure 3-11: Tacit steps of a procedure
Documentation is a controversial aspect of testing in the industry. Testing
textbooks and "standards" would have you believe that nearly all testing
should be documented procedurally or even automated if possible. Many
testers are brought up to believe that "writing test cases" is the main activity
of testing.
Well, about that. The authors of this book have had long careers. We've
been involved in rigorous and formalized testing. James, in particular, has
been an expert witness on court cases where his testing was not only
documented but filmed and presented to experts who were paid to find
things wrong with it. Microsoft once flew a team to Washington, DC,
specifically to demolish his testing of Windows. (They later complained to
a judge that there was so much documentation that it would take weeks to
analyze it all.42) James and his team systematically demonstrated that

Internet Explorer could be removed from Windows, contrary to the
testimony of Bill Gates.
Yet, despite all that, we don't think meticulously detailed test documentation
is called for in most projects. And even when it is called for, it's only
needed for some of the testing—never all of it!
Documentation is expensive, and it must be maintained. Maintenance takes
a lot of time, though, so most projects that extensively document testing are
either perpetually misleading testers with documents that they don't have
time to update, or they are forced to do far less testing than they otherwise
would.
Our general recommendation for most projects is to have the leanest
documentation that can probably work for the project and to rely more on
training, shared experiences (including paired testing, mob testing, test
session debriefs, risk brainstorms, etc.), and perhaps video demonstrations
in lieu of heavy docs.
How To Go Wrong When Performing a Test
If a test procedure is a specific way for a person to perform a particular test,
then all test procedures are heuristics that guide the tester's actions rather
than merely dictating behavior. This is because testers apply tacit
knowledge and skills to everything they do, and no explicit procedure can
capture all of that.
If you are performing a test that is specified by someone else, then you must
decide how to interpret the instructions you receive. It may be that you take
the instructions as a general direction and make up your own actions to
fulfill the designer's intent, or you may decide to adhere closely to the letter
of the procedure (whether it was written down or demonstrated to you).
There are a few dangers to be aware of whenever you try to perform
someone else's test. One is that the procedure is outdated, irrelevant,
mistaken, or poorly conceived, yet you follow it anyway. This may be
because you don't have enough knowledge and agency to correct it. Another
danger is that you fail to understand the brilliant design of the procedure
and replace it, perhaps unconsciously, with an inferior procedure that
undermines the intention of the original author. Yet another danger is

adhering too strictly to an otherwise good procedure, such that you miss
something important and unexpected that emerged during the test.
Even if it's conceptually sound, a test procedure may not necessarily be
performable by a particular tester in a particular context. This is what we
call a lack of "subjective testability" (see Appendix C). Even test
procedures that are performable in principle may not be ergonomic. For
instance, human minds don't work well when given boring lists of menial
actions. Good test procedures use the minimum level of specification to get
the job done. Or perhaps the procedure calls for spotting a particular kind of
behavior that is fleeting, subtle, or otherwise easy to miss.
Assuming that your test procedure is reasonably performable and you set
out to perform it, how do you know you're doing it right? How do you
know you are performing it well? Ultimately, you won't know for sure, but
you can maintain awareness of different ways that performance fails.
Apart from the standard cognitive biases (discussed in Chapter 2) that may
impair your testing, you have to manage the following challenges:
Wandering attention: The cold logic of a procedure may move
through the four subprocesses of choose, interact, sensemake, and
evaluate in order, but in practice, your attention will jump around. You
will be interrupted, and you will interrupt yourself. In the middle of an
interaction with the product, you will get a better idea of what to do, or
in the middle of sensemaking, a screen will change, and you will have
to restart the process. The openness of these processes is both a
strength and a liability. It makes it possible for you to quickly change
course to pursue a bug, and it creates a probability that you won't
perform the procedure you intend to perform.
Flagging alertness: Boredom and fatigue can make you insensitive to
bugs. Your mind needs variety to stay sharp. You need to feel that your
work matters.
Poor observation: You can't see everything, all the time. You don't
necessarily remember what you saw. This is especially a problem
when performing a sequence of very similar tests.
Exceeded capacity: Sometimes you find a bug, and then, while
investigating it, you see another one, and then another one. We call this

a bug cascade. Investigations stack up, and it's easy to lose your place
in the stack.
Crushing pressure: A consequence of the modern fashion of testers
being "on the same team" as developers is that testers have to fend off
great social pressure to get the job done fast. You may think deeper
testing is warranted, whereas someone outside the test process wants
you to stay shallow. The outsider probably hasn't thought through the
risks and challenges. They have other things to do. But you may have
to respond to their opinions. The purpose of a test manager—which
many projects don't have—is to prevent testers from being bullied in
this way.
Displaced goals: A goal must be translated into subgoals and actions
taken to achieve them. But sometimes a subgoal becomes detached
from its original purpose, and you find yourself working on an
obsolete task. An example might be performing a test case that no
longer matters because the code has changed or because there is no
longer an important risk associated with it. A special case of a
displaced goal is pathetic compliance. This means sullenly following
the letter of a procedure in some minimal way but not the spirit of it. It
happens when the mission of testing becomes displaced by the desire
not to be criticized for insubordination.
Bad test idea: Not every test idea is worth pursuing. Some are simply
mistaken. Here's an example of a bad test idea: physically shake a
computer display to see if a scrollbar will move. Unless the display is a
touch screen, it is not physically possible for a movement of the screen
to lead to a change in the state of the software. We chose this idea as an
example because it's obviously and absurdly wrong. However, many
ideas that seem plausible to a nontechnical tester, or to a tester who
isn't a domain expert, might turn out to be equally absurd. The risk of
pursuing a bad idea diminishes over time as you get to know the
product and context.
The Art of Bug Reporting
Bug reporting is central to testing. The bug report, whether oral or written,
is the single most visible outcome of testing. And the quality of your reports

is probably the single most important factor in determining your credibility
as a tester.
Our definition of a bug follows from our definition of quality. Since quality
is value to some person who matters, a bug is anything about the product
that threatens its value (in the mind of someone whose opinion matters); or,
you might say, it's anything about the product that makes it less valuable
than it could reasonably be.
Sometimes bugs are called defects, but there is a difference. A defect is
something that is definitely wrong with the product. "Bug" is a less
categorical term.43 We prefer saying "bug" because "defect" sounds
accusatory and may discourage testers from reporting things that are strange
but not definitely wrong. "Bug" is a time-honored term—also somewhat
vague, and that vagueness is helpful.
Bugs are not about your personal feelings. You can use your feelings to find
a bug, but you'd better know a good reason why someone else would have
the same feelings. That's because your feelings don't matter—you are a
tester. A tester is an agent for the people who do matter. Thus, we must learn
about the people who do matter: the users, the customers, the bosses,
society, and the standards that are relevant to them. If you want to write
successful bug reports, get inside the minds of your clients.
Normal Bug or Enhancement Request?
Bugs come in two flavors: normal and enhancement. A normal problem is a
failure of the product to fulfill its intent, whereas an enhancement request is
for when you believe the intent itself should be changed because that intent
does not achieve the aspirations of the people who matter. In other words,
"The product isn't doing what you want it to do" is a regular bug; "The
product is doing what you want, but wouldn't you rather want something
else?" is an enhancement request. These different cases must be reported a
little differently. In the case of an enhancement request, you are essentially
playing designer, which means if you aren't careful in your reporting, you
will offend the people who make the product.
Whenever you make an enhancement request, we suggest phrasing the bug
title as a request (i.e., start with the word "Please ...") to make it clear that
you are suggesting a change to the scope of the product.

How to Investigate a Bug
Your oracle has been violated. You think you are looking at a bug. Now
what? Your team may or may not expect you to investigate the bug
thoroughly before reporting it, but you should know how to do it if
required.
The goals of bug investigation are as follows:
Authenticate the bug: You may feel that it's a problem, but is it
really? What is the most authoritative oracle that it violates? Is it
possible that it's a false alarm?
Determine its most interesting manifestation: Isolate and generalize
the problem. What is the biggest, worst version of the problem?
Differentiate it from other bugs that are already reported: We don't
want to clutter up the list with bug reports that all refer to the same
underlying problem.
Collect data the developer needs to know: Typically, this means
telling them how to reproduce the problem for themselves. Capture the
bad output as well as the input or action that seemed to cause the
effect.
Before you launch into a long process, keep in mind that for puzzling or
difficult-to-reproduce bugs, it's often the case that the developer will
already know what's going on. For that reason, we recommend a Ten-
Minute Timebox heuristic: investigate a puzzling bug for no more than
about ten minutes before checking in with a developer about it if you can. If
the developer is also puzzled, continue the investigation. Otherwise, if the
developer claims to know what is happening, just report what you have and
move on to find the next bug. That will allow you to find more bugs instead
of making ever-nicer-looking reports.
Popular investigation moves include:
Spiral inquiry: When you see a problem, avoid disturbing the state of
the product. Preserve the crime scene, as it were. Back up one step and
retry it. See if you can reproduce the problem that way. As you

succeed, you can make more disruptive changes to test your ability to
recover the failed state.
Guess the cause: Based on your technical knowledge and product
experience, brainstorm a list of reasons the problem may be occurring.
These become the focus of your investigation.
Remove steps: Discover which steps matter for reproducing the bug
by progressively eliminating them. This also helps you discover how
easily the bug will appear for users.
Simplify input: If the bug first occurred with complex or challenging
input, simplify it and try again.
Generalize input: If the bug first occurred with a specific input case,
try increasingly generic cases.
Normalize input: If the bug first occurred with unusual, artificial
input, try it with more typical, natural input.
One factor at a time: To find out which of many variables is
triggering the problem, change one variable at a time. This is an
example of what we call focusing in RST.
Many factors at a time: To corroborate that a group of variables isn't
related to the bug, change them all at once and see if there is any
impact on the appearance or nature of the bug. This is an example of
defocusing.
Traceable data: Select input data patterns that will be easy to trace to
the output. This includes self-describing data (using "Street Address 1"
as a street address) and otherwise patterned data that can be easily
distinguished when you are evaluating the results.
Clean machine: Consider resetting your platform (for browser-based
apps that might mean clearing caches, cookies, disabling extensions,
etc.) A "dirty" system might make a problem more likely to occur or
less, but a bug that occurs on the cleanest version of a platform is more
concerning than one that only occurs with certain idiosyncratic
settings.
Monitoring and recording: Use tools such as screen video, activity
loggers, proxies, or engineering telemetry to record and recover what

you are doing while you test.
Log analysis: Go through the log file for clues about the underlying
mechanisms of the bug.
How to Investigate Intermittent Problems
The ability and the confidence to investigate an intermittent bug are two
things that mark an excellent tester. The most engaging stories about testing
we've heard have been stories about hunting a "white whale" sort of
problem in an ocean of complexity.
Unlike a mysterious nonintermittent bug, which is a problem for
developers, an intermittent bug is more of a problem for testers. A lot of
devs won't chase that white whale when there's other fishing to do. "Ahab,"
they say, "if you can't find the whale, probably no one else is seeing it
either. How important can it be?"
We are not concerned about intermittence that is both desirable and non-
mysterious, even if it isn't exactly predictable. Think of a coin toss at the
start of a football game or a slot machine that comes up all 7s once in a long
while. We might not be concerned even about mysterious intermittent
behavior if we believe it won't hurt anyone.
Many intermittent problems have not been observed at all, perhaps because
they haven't manifested yet or perhaps because they have manifested and
not yet been noticed. The only thing we can do about that is to get the best
test coverage we can and keep at it. No algorithm can exist for
automatically detecting or preventing all intermittent problems.
So what we typically call an intermittent problem is a mysterious and
troubling behavior of a system, observed at least once, that we cannot yet
manifest on demand. The challenge for a tester is to transform the
intermittent bug into a regular bug by resolving the mystery surrounding it.
After that, it's a matter for the programmers.
Be Comforted: The Cause Is Probably Not Evil Spirits
If it happened once, it will probably happen again.

If a bug goes away without being fixed, it probably didn't go away for
good.
Be wary of any fix made to an intermittent bug. By definition, a fixed
bug and an unfixed intermittent bug are indistinguishable over some
period of time and/or input space.
Any software state that takes a long time to occur under normal
circumstances can also be reached instantly, through unforeseen
circumstances.
Complex and baffling behavior often has a simple underlying cause.
Complex and baffling behavior sometimes has a complex set of
causes.
Intermittent problems often teach you something profound about your
product.
It's easy to fall in love with a theory of a problem that is sensible,
clever, and wrong.
The key to your mystery might be resting in someone else's common
knowledge.
An elusive problem in the lab might be easily reproducible in the field.
An intermittent technical problem may pose a sustained and expensive
public relations problem.44
Although the problem is intermittent, the risk of that problem is ever
present.
The more testability is designed into a product, the easier it is to
investigate and solve intermittent problems.
When you have eliminated the impossible, whatever remains, however
improbable, could have done a lot of damage by then! So don't wait
until you've fully researched an intermittent problem before you begin
to report it.
If you ever get in trouble because of an intermittent problem that you
could not lock down before release, you will fare a lot better if you
made a faithful, thoughtful, vigorous effort to find and fix it. The

journey can be the reward, you might say—or at least a pretty good
excuse.
General Suggestions for Investigating Intermittent Problems
Recheck your most basic assumptions: Are you using the computer
you think you are using? Are you testing what you think you are
testing? Are you observing what you think you are observing?
Eyewitness reports leave out a lot of potentially vital information. So
listen—but maintain detachment from the claims people make.
Invite more observers and minds into the investigation.
Create incentives for people to report intermittent problems.
If you have ruled out a possible cause yet are not able to discover what
the problem is, eventually reopen consideration of that possibility.
Maybe the symptoms you are seeing have been reported online for
some other product similar to yours. Maybe this is a known problem
for some component you are using.
Seek tools that could help you observe and control the system.
Improve communication among observers (especially with observers
who are users in the field).
Establish a central clearinghouse for mystery bugs, so that patterns
among them might be easier to spot.
Look through the bug list for any other bug that seems like the
intermittent problem.
Make more precise observations (consider using measuring
instruments).
Add more logging and scriptable interfaces.
Get access to logs.
Control inputs more precisely (including sequences, timing, types,
sizes, sources, iterations, and combinations).
Control state more precisely (find ways to return to known states).

Systematically cover the input and state spaces.
If the problem happens more often in some situations than in others,
consider doing a statistical analysis of the covariance between input
patterns in those situations.
Consider controlling things that you think probably don't matter.
Simplify. Try changing only one variable at a time; try subdividing the
system (helps you understand and isolate the problem when it occurs).
Complexify. Try changing more variables at once; let the state get
"dirty" (helps you make a lottery-type problem happen).
Inject randomness into states and inputs (possibly by loosening
controls) in order to reach states that may not fit your typical usage
profile.
Create background stress (high loads, large data).
Set a trap for the problem by deploying continuous recording tools.
The next time it happens, you'll learn much more about it.
Review the source code.
Consider interference among components created by different
organizations.
Systematically consider the conceivable causes of the problem (see the
following section).
Beware of burning huge time on a small problem. Keep asking: Is this
problem worth it?
When all else fails, let the problem sit a while, do something else, and
see if it spontaneously recurs.
Celebrate and preserve stories about intermittent problems and how
they were resolved.
Considering the Causes of Intermittent Problems
When investigating an intermittent problem, it may be worth considering
the kinds of things that cause such problems. The following list of
guideword heuristics may help you systematically do that analysis. There is

some redundancy among the items in the list because causes can be viewed
from different perspectives.
Possibility 1: The system is NOT behaving differently. The apparent
intermittence is an artifact of the observation.
Bad observation: The observer may have made a poor observation.
For instance, inattentional blindness is a phenomenon whereby an
observer whose mind is occupied may not see things that are in plain
view. When presented with the scene a second time, the observer may
see new things in the scene and assume that they weren't there before.
Also, certain optical illusions cause apparently intermittent behavior in
an unchanging scene.45
Irrelevant observation: The observer may be looking at differences
that don't matter. The things that matter may not be intermittent. This
can happen when an observation is too precise for its purpose.
Bad memory: The observer may have misremembered the
observation, or records of the observation could have been corrupted.
There's a lot to observe when we observe! Our mind immediately
compacts the data and relates it to other data. Important data may be
edited out. Besides, a lot of system development and testing involve
highly repetitive observations, and we sometimes get them mixed up.
Misattribution: The observer may have misattributed the observation.
"Microsoft Word crashed" might mean that Windows crashed for a
reason that had nothing whatsoever to do with Word. Word didn't "do"
anything. This is a phenomenon also known as false correlation and
often occurs in the mind of an observer when one event follows hard
on the heels of another event, making one appear to be caused by the
other. False correlation is also chiefly responsible for many instances
whereby an intermittent problem is mistakenly construed to be a
nonintermittent problem with a very complex and unlikely set of
causes.
Misrepresentation: The observer may have misrepresented the
observation. There are various reasons for this. An innocent reason is
that the observer is so confident in an inference that they are reporting
a prediction as if it were an observation.

Unreliable oracle: The observer may be applying an intermittent
standard for what constitutes a "problem." We may get the impression
that a problem is intermittent only because some people, some of the
time, don't consider the behavior to be a problem, even if the behavior
is itself predictable. Different observers may have different tolerances
and sensitivities, and the same observer may vary in that way from one
hour to the next.
Unreliable communication: Communication with the observer may
be inconsistent. We may get the impression that a problem is
intermittent simply because reports about it don't consistently reach us,
even if the problem is itself quite predictable. "I guess people aren't
seeing the problem anymore" may simply mean that people no longer
bother to complain.
Possibility 2: The system behaved differently because it was a different
system.
Deus ex machina: A developer may have changed it on purpose, and
then changed it back. This can occur easily when multiple developers
or teams are simultaneously building or servicing different parts of an
operational server platform without coordinating with each other.
Another possibility, of course, is that the system has been modified by
a malicious hacker.
Accidental change: A developer may be making accidental changes.
The changes may have unanticipated side effects, leading to the
intermittent behavior. Also, a developer may be unwittingly changing a
live server instead of a sandbox system.
Platform change: A platform component may have been swapped or
reconfigured. An administrator or user may have changed,
intentionally or not, a component on which the product depends.
Common sources of these problems include Windows automatic
updates, memory, and disk space reconfigurations.
Flakey hardware: A physical component may have transiently
malfunctioned. Transient malfunctions may be due to factors such as
inherent natural variation, magnetic fields, excessive heat or cold,
battery low conditions, poor maintenance, or physical shock.

Trespassing system: A foreign system may be intruding. For instance,
ad blockers, third-party security services, automatic system updates,
other programs, or other instances of the same program.
Distributed system: The "system" may actually be several different
systems that perform as one. For example, when you search on certain
websites, the search may in fact be performed by many servers
simultaneously, resulting in a possibility that some servers don't return
their results in time.
Human element: There may be a human in the system, making part of
it run, and that human is behaving inconsistently.
Possibility 3: The system behaved differently because it was in a different
state.
Frozen conditional: A decision that is supposed to be based on the
status of a condition may have stopped checking that condition. It
could be stuck in an "always yes" or "always no" state.
Improper initialization: One or more variables may not have been
initialized. The starting state of a computation would therefore depend
on the state of some previous computation of the same or an other
function.
Resource denial: A critical file, stream, or other variable may not be
available to the system. This could happen either because the object
does not exist, has become corrupted, or is locked by another process.
Progressive data corruption: A bad state may have slowly evolved
from a good state by small errors propagating over time. Examples
include timing loops that are slightly off or rounding errors in
complicated or reflexive calculations.
Progressive destabilization: There may be a classic multistage
failure. The first part of the bug creates an unstable state—such as a
wild pointer—when a certain event occurs without any visible or
obvious failure. The second part precipitates a visible failure at a later
time based on the unstable state in combination with some other
condition that occurs down the line. The lag time between the

destabilizing event and the precipitating event makes it difficult to
associate the two events to the same bug.
Overflow: Some container may have filled beyond its capacity,
triggering a failure or an exception handler. In an era of large
memories and mass storage, overflow testing is often shortchanged.
Even if the condition is properly handled, the process of handling it
may interact with other functions of the system to cause an emergent
intermittent problem.
Occasional functions: Some functions of a system may be invoked so
infrequently that we forget about them. These include exception
handlers, internal garbage collection functions, autosave, and periodic
maintenance functions. These functions, when invoked, may interact in
unexpected ways with other functions or conditions of the system. Be
especially wary of silent and automatic functions.
Different mode or option setting: The system can be run in a variety
of modes, and the user may have set a different mode. The new mode
may not be obviously different from the old one.
Possibility 4: The system behaved differently because it was given different
input.
Accidental input: The user may have provided input or changed the
input in a way that shouldn't have mattered, yet did. This might also be
called the Clever Hans syndrome, after the mysteriously repeatable
ability of Clever Hans, the horse, to perform math problems. It was
eventually discovered by Oskar Pfungst that the horse was responding
to subtle physical cues that its owner was unintentionally conveying.
In the computing world, James once experienced an intermittent
problem due to sunlight coming through his office window and hitting
an optical sensor in his mouse. The weather conditions outside
shouldn't have constituted different input, but they did. Another more
common example is different behavior that may occur when using the
keyboard instead of a mouse to enter commands. The accidental input
might be invisible unless you use special tools or recorders. For
instance, if you take a file, open it in Microsoft Word and save it in

RTF format, then do the same thing in OpenOffice Writer, the two
saved files will not be the same on the disk.
Secret boundaries and conditions: The software may behave
differently in some parts of the input space than it does in others. There
may be hidden boundaries, or regions of failure, that aren't
documented or anticipated in your mental model of the product. James
once tested a search routine that invoked different logic when the total
returned hits were <1000 and >50,000. Only by accident did he
discover these undocumented boundaries.
Different profile: Some users may have different profiles of use than
other users. Different biases in input will lead to different experiences
of output. Users with certain backgrounds, such as programmers, may
be systematically more or less likely to experience, or notice, certain
behaviors.
Ghost input: Some other machine-based source than the user may
have provided different input. Such input is often invisible to the user.
This includes variations due to different files, different signals from
peripherals, or different data coming over the network.
Deus Ex Machina: A third party may be interacting with the product
at the same time as the user. This may be a fellow tester, a friendly
user, or a malicious hacker.
Compromised input: Input may have been corrupted or intercepted
on its way into the system. This is especially a concern in client-server
systems.
Time as input: Intermittence over time may be due to time itself.
Time is the one thing that constantly changes, no matter what else you
control. Whenever time and date, or time and date intervals, are used
as input, bugs in that functionality may appear at some times but not
others.
Timing lottery: Variations in input that normally don't matter may
matter at certain times or at certain loads. The Mars Rover suffered
from a problem like this involving a three-microsecond window of
vulnerability when a write operation could write to a protected part of
memory.

Combination lottery: Variations in input that normally don't matter
may matter when combined in a certain way.
Possibility 5: The other possibilities are magnified because your mental
model of the system and what influences it is incorrect or incomplete in
some important way.
You may not be aware of each variable that influences the system.
You may not be aware of the sources of distortion in your
observations.
You may not be aware of available tools that might help you
understand or observe the system.
You may not be aware of all the boundaries of the system and all the
characteristics of those boundaries.
The system may not actually have a function that you think it has—or
maybe it has extra functions.
A complex algorithm may behave in a surprising way, intermittently,
that is entirely correct (e.g., mathematical chaos can look like random
behavior).
How to Report a Bug
A bug report is a description of a suspected bug. The most basic bug report
is a statement to the effect that "Here's something I think may be wrong
with the product." In real life, this could manifest in a manner as simple as
pointing at a screen and saying, "Uh-oh, look at that." That may be all you
need to do in the case where you are testing for a friend standing next to
you, and you both have a strong shared mental model of what the product
should be and do. If we are all close friends, or if we all belong to the same
hive mind, bug reporting can be pretty easy.
Bug reports can be formal or informal, written or oral. Underlying even the
simplest bug report is a certain structure with the following four elements:
A description of the problem you perceive: What bad thing happens,
or what good thing doesn't happen? Be specific and clear about that.

Ask yourself if that is the root of the matter or whether there is
something bigger or more fundamental that you ought to report
instead.
How you encountered that problem: The bug you perceive ought to
be grounded in a direct observation of the product itself. Be specific
about the steps and data you used.
The reason why it's a problem: All bug reports are based on some
sort of oracle and maybe several different oracles. Some oracles are
more authoritative (stronger), while others are merely suggestive
(weaker). You might use a weak oracle (such as a feeling that "This is
hard to use") that gives you a suspicion that something is not right and
needs investigation, then after some investigating, find that you have a
stronger oracle ("This violates the defined usability standard) against
which to actually report the bug.
Why it's a problem that matters: Just the fact that a behavior is a
threat to the value of the product is not necessarily interesting. Your
clients need to know: Is it a big bug or a little bug? You should be
ready to say how important a bug it could be. This is related to how
likely it is to be seen and how much damage it could do if it occurs.
More on that below.
If you make an informal bug report, be ready to make a more complete and
explicit report if your report is challenged. Some of the common ways bug
reports are challenged include the following:
"I don't know what you are talking about."
"That doesn't happen when I try it."
"I don't see why that's a problem."
"That's only a problem for beginners."
"It's a problem, but it's difficult to fix, and there's an easy
workaround."
"It's a problem, but only weird users and testers will ever stumble into
it."

"Maybe you don't like the way it works, but most real users will like
it."
Take challenges like these in your stride. Remember that the developer
probably has a fundamentally optimistic, builder's mentality.46 This is a
good thing. To create anything complex and wonderful requires optimism.
Your bug reports are ants raining on their picnic. So, keep your cool, and be
ready to offer evidence or argument to support the best case you can make
that a bug is worth fixing.
Apart from challenges, anyone who reports a bug should anticipate the
common questions that developers and managers may ask, such as:
What exactly did you see happen?
What were you doing right before you saw the problem? Were you
doing anything else interesting at the same time?
Have you seen the problem more than once? If so, what other times
have you seen it?
Did you take any screenshots or a video?
What data were you using? What files? What exactly did you input?
What good reason do you have for thinking it is wrong?
What version of the product were you testing? What platform were
you using?
A responsible tester can expect more specific questions:
Is it already reported?
Have you tried to reproduce the effect? What are the steps to reproduce
it?
Have you tried simple variations of the input or operations that led to
the symptoms? Have you tried on different platforms or in different
modes?
Have you checked to see that the product is actually installed properly
and running? Is your system properly configured?

Did this problem exist in earlier versions of the product?
Do you have a specific suggestion of what should happen instead?
How could it be better?
How easy is it to make the bug happen? What is the worst
consequence you can think of that could result from it?
Not all of these questions apply to every bug, but your credibility as a tester
may depend on being able to answer them in the cases where they do apply.
For instance, if you report that "uploading a big file" causes some sort of
error, then you should say in the bug report exactly how big the files were
that you tried and be sure those exact files are accessible to the developers.
Formal vs. Informal Bug Reporting
Consider three kinds of bug reporting: MIPing, formal reports, and black
flagging:
MIP: This stands for "mention in passing." To MIP a bug is to report it
in a sentence or two by voice, email, or instant message. It can even
take the form of a question ("Is this supposed to work this way?").
There is no expectation of formally tracking such reports, and there is
no template for them. The main reason to MIP a bug is when you are
fairly confident that what you are reporting is not a bug, and yet you
have a suspicion that it could be. MIPing is partly a strategy for
learning about the product, since, early on in testing, many things that
look like problems to you might not be. MIPing helps preserve your
credibility because if a MIPed bug turns out to be a real issue, you can
fairly say that you did report it, and because if it is not a bug, you can
fairly say that you didn't create unwieldy paperwork for the team.
MIPing is a very good way to work when you are pairing with a
developer for early testing.
MIPing is an excellent protocol for bug reporting when running a
mass testing event with supporting testers when developers or other
experts are in the room with them. The supporters then serve as "bug
scouts," while the experts perform investigations and take
responsibility for formal reporting, if required.

Formal reports: This means recording written bug reports in some
sort of tracking system. Formal bug reporting is slower and more
expensive than MIPing, but it has obvious advantages when you are
dealing with large numbers of bugs. Expecting supporting testers to do
formal bug reporting may not be reasonable, but if they do, someone
with responsibility will need to edit them. Poor-quality formal bug
reports can erode the credibility of the testing effort.
Black flagging: This means going beyond reporting a bug. You call a
meeting or escalate to management. You raise an alarm about a serious
underlying problem in the development effort. This may be necessary
for safety or security bugs that can only occur when there is a
breakdown of development discipline. Black flagging is for when you
suspect that a bug represents a process problem that threatens the
business.
These forms of bug reporting are not the only forms that exist. But they
serve to illustrate that bug reporting is a socially situated activity. However,
whatever you do with your reporting, your process must ultimately fit the
social order of the project.
Elements of a Basic Formal Bug Report
Here are the most common fields found in a formal bug report:
Title. A short summary that expresses the essence of the bug.
The title should be one sentence long—about twelve words.
It must be distinctive, not generic (i.e., don't write "the product
crashes"). The title should uniquely identify the bug among all the
other bug report titles. Try to put the actual problem right at the
beginning (e.g., "Crash when logging in as admin" rather than "When
logging in as admin, product crashes") because it's easier to read
when looking at a list of bugs.
If it is an enhancement request, consider starting the title with the
word "Please."
Many testers like to encode location information in the bug title (e.g.,
"User settings: cannot update avatar picture after first setting it"). We

discourage that for the following reasons:
The title is a precious short space in which you need to express the
essence of the problem. Realistically, you have about 12 words to
work with.
The specific location of the problem is already encoded into the
body of the bug report, and probably also into a dedicated field
called "product area" or something like that.
Encoding the location in a usable way may take a substantial
portion of the title, leaving little room to express the nature of the
problem.
Encoding the location disrupts the practice of "impact-first"
summarizing.
Description. Any other information about the specific failure mode
and behavior of the system that members of the team need to know
about the bug.
Keep it short. Give reasonable details about the bug, but don't
include information that everyone on the team will certainly
already know. If the problem is very obvious (e.g., "Company
name spelled wrong on the home page") then you hardly need to
write a description.
Write in professional language. Don't use texting jargon. Spell
words correctly.
Provide steps to reproduce if those steps are not obvious. Don't
provide steps that are obvious (e.g., "1. Connect to the Internet 2.
Start the browser").
Indicate your oracles. That means say why you think this is a bug,
unless it is obvious. Don't say silly things like "The product should
not crash." That sounds insulting, and it adds nothing to the report.
Note any workaround for the bug that you know about.
Please be judicious about writing "expected:" at the end of a bug
description. Testers often throw that into a bug report even when
the expected result is obvious. Never say "Expected: the product

should give the correct result" or other vacuous statements. Yes,
tell us the basis of your oracle for determining that this is a bug,
but stick to useful, nonobvious information.
Consider starting the final step of your steps to repro with the
words "Notice that ..." to be clear about what you think is wrong.
Version: This is the latest version of the product in which you detected
the bug. If you also tested earlier versions, note that in the description.
Environment: The platform you were testing on. Typically, this is
your hardware, browser, and operating system. If you are testing an
online product, specify the server. Report any environment element
that is interesting or unusual, or if it is customary to report it.
Attachments: For all but the easiest to understand bug reports, it will
be much appreciated if you attach screenshots of the problem—or even
small videos. Also include links to any critical data files needed to
reproduce the problem.
In addition to the basic fields, your bug tracking system may have other
fields. It will autofill the ID, Reporter, and Date Reported fields. Then there
are Status, Severity, and Priority, which follow a protocol that is specific to
your company and your project, so we won't discuss them here.
Give the Bug Report a Good Focus
Report the most compelling version of the bug. Bug reporting is a kind of
sales job. You must frame the report in the most compelling (yet truthful)
way in order to maximize the chance that it will be understood and acted
upon. Try to focus on the aspect of the bug that can have the most user
impact—or the most negative public impact for your company. In other
words, try to identify the strongest, most compelling oracle that you can.
Avoid reporting multiple problems in one bug report. Unless multiple
problems are probably the symptoms of one underlying fault in the product,
they should be separated into distinct bug reports. This is because it's very
easy for a developer to fix one problem while accidentally forgetting to fix
others that are listed in the same report.

Avoid reporting the same problem in multiple reports. It is often difficult to
tell whether two problems that seem similar are genuinely the same
problem. So make your best guess or consult with the developer to be sure.
Assessing the Significance of a Bug
A tester is the first judge of how "big" the bug is. This is true even for
supporting testers to some degree. But for responsible testers, it is a very
important part of their work.
What makes a bug important? Basically, four things:
How frequently does it appear? Or, how easy is it to make happen?
A bug that is seen often or by a lot of users is going to be more
important, all other things being equal. Are there lots of different kinds
of events that can trigger the bug? Is it highly vulnerable to the
triggering events? How visible and obvious is it when it appears?
How much damage does it do when it occurs? The most important
bugs are generally the ones that stop the project itself: so-called
blocking bugs. These are bugs that prevent you from testing. Down
from that are bugs that harm or block the user. A bug that deletes data
may be more important than one that merely creates confusion in the
user interface, but the opposite can also be the case when confusion
could result in dangerous user behavior. While there are no hard rules
about what specific symptoms constitute "more damage," try
visualizing the problem, then consider the importance of the user who
is affected and how upset they may be because of the bug.
What does the bug imply about other potential risks? A bug may be
especially important because it implies that there is a big problem in
the development process itself that may have resulted in many similar
bugs that are not yet found (see "black flagging" in the earlier "Formal
vs. Informal Bug Reporting" section).
What bad publicity might come from the bug? Bad feelings and a
bad reputation can accrue from bugs even if the objective severity is
not that bad. Consider how a bug might sound when people complain
about it on social media. Consider how it might erode trust in your
company's brand.

The Bug Pipeline
This chapter has focused on tactical, hands-on testing. There may be many
benefits to testing (see Appendix D, in the section called "Evolving Work
Products"). The biggest deal is that we find bugs. So one way to think about
everything in this chapter is to see it as driving a pipeline of bug reports
(see Figure 3-12).
Figure 3-12: The leaky pipeline of testing
Why don't the bugs just pour out of the product when we tip it over? Well,
imagine if bugs were trapped in the product but actually trying to get
themselves reported. It would be like that Dr. Seuss book, Horton Hears a
Who. If they wanted to be noticed, what would stop us from noticing them?
The product, the tester, the test process, and the clients of testing—all these
things together—can be thought of as a gauntlet or pipeline that the bugs
must get through if they want to be recognized.

In the book, Horton is an elephant with sensitive ears. In our projects, it's a
bit more complicated. The pipeline has at least eleven segments, as we see
it, with filters to sift out things that are not bugs.
Let's look closely at how the pipeline works, step by step:
1. To be correctly reported, a bug must exist. Of course, you can't know
for sure if any bug exists before you encounter it, but the existence of
the bug doesn't depend on your knowledge or what you see—it
depends on the product itself, the people who matter, and the context
in which they operate. A bug can exist, in principle, even if no one ever
sees it.
2. For a bug to affect anyone, it must be producible. Producibility means
that there must be some way of manifesting behavior that leads to the
trouble that the bug will cause. In the case of a code fault, "producing
the bug" means running the code in such a way as to trigger some sort
of failure.
3. For a bug to come to your attention during testing, it must be
produced. The bug must "happen." In testing, as distinct from code
review (or any form of static analysis), you must manifest the effects
of the bug. You do this through performing tests, as we have discussed
in this chapter. See also Chapter 4, where we discuss test strategy.
4. For a bug to come to your attention during testing, its effects must be
observable. You must be able to see it (or hear it, as the case may be).
Observability is a key part of testability. Detailed log files and access
to back-end interfaces can greatly improve observability. Obscure
interfaces and heaps of data stuck in JSONs reduce it.
5. For a bug to come to your attention during testing, you must observe it.
It is not a given that you will see a bug just because it can be seen. In
the authors' own experiences, videoing and replaying testing, we have
been shocked at how easily we miss bugs that were right in front of
our eyes.
6. To make a report a bug well, you must comprehend it. You don't
necessarily have to know all of what's going on, but you strive to
understand enough to make sense of it, describe it, localize it, and
know how to produce it at will.

7. To know it is a bug, you must authenticate it using a good oracle.
8. Before you report it, you must differentiate it from existing reports.
Don't flood the bug list with a lot of similar things. Each report should
refer to something that is significantly different from every other
report.
9. For your client to find out about the bug, you must report it (see the
previous section).
10. To appreciate the bug report, your client must receive it. Your client
must "get" it. Your reports must land with them. This is a social and
cognitive issue, and it has a lot to do with your personal credibility.
You must build that credibility and protect it.
11. To feel motivated to deal with the bug, and to appreciate you as a
tester, your client must think the bug matters. Part of this is a sales
problem. You have to "connect the dots" for them. As testers, we
cannot insist that they fix any particular bug, but we can make the
business risks clear to them.
Any break in the pipeline will allow real bugs to "leak" out, or a bad filter
in the pipeline will block a bug. In fact, James and Michael conceived of
the pipeline after performing an analysis of a video of themselves doing
testing. They saw in the video that they had found a bug during the test
session, announced it to each other, began to investigate it, and then were
distracted by other bugs (a bug cascade) to the degree that they entirely
forgot about the original bug. Their testing had sprung a leak!
Good testing is a process of creating the most reliable, effective, and
efficient pipeline for reporting bugs that matter.
We have said that testing begins with the scientific method. While that is
true, elements of social judgment come into the pipeline starting at the third
segment, and the final five segments of the testing pipeline are dominated
by psychological and social factors. When James and Michael experienced
their bug cascade, they had to make choices in real time about which
investigation to pursue and what data to collect. It requires social
competence to carry a bug all the way through to the client while
suppressing things that ought not to be reported at all.

Thus, while the essence of testing may be science, the medium of testing is
the mind and heart of a tester, and service to others is our purpose.
Notes
1   Sociology, cognitive psychology, organizational psychology, foraging
theory, general systems theory, metrology, cybernetics, and
epistemology, to name some of the big ones.
2   Kuhn, T. S. (1970) The structure of scientific revolutions. 2nd ed. enl.
Chicago: University of Chicago Press.
3   If you are the kind of person who reads footnotes, we probably don't
need to explain the relevance of philosophy to software testing.
Nevertheless, there are famous testers who argue that studying
philosophy is pointless (...and by doing so, are practicing philosophy).
4   Popper, K. R. (1989) Conjectures and refutations: The growth of
scientific knowledge. 5th ed. (rev.). London: Routledge.
5   Simon, H. A. (1996) The sciences of the artificial. 3rd ed. Cambridge,
MA: MIT Press.
6   Collins, H. M. (2004) Gravity's shadow: The search for gravitational
waves. Chicago: University of Chicago Press.
7   Feyerabend, P. K. (1993) Against method. 3rd ed. London: Verso.
8   Pickering, A. (1995) The mangle of practice: Time, agency, and science.
Chicago: University of Chicago Press.
9   Cayley, D. (ed.) (2009) Ideas on the nature of science. Fredericton:
Goose Lane. (This is Michael's favorite survey of different ways to think
about science.)
10 This is roughly true, although there's more to sensemaking than just
hypothesizing.
11 Klahr, D. (2002) Exploring science: The cognition and development of
discovery processes. Cambridge, MA: The MIT Press. Deepak Kulkarni

called these the "rule space" and "instance space" in prior work:
Kulkarni, D. and Simon, H. A. (1988) "The processes of scientific
discovery: The strategy of experimentation," Cognitive Science, 12(2),
pp. 139-175.
12 Trickett, S. B., Trafton, J. G. and Raymond, P. D. (1998) "Exploration in
the experiment space: The relationship between systematicity and
performance," Proceedings of the Twentieth Annual Conference of the
Cognitive Science Society, pp. 1067-1072.
13 Okada, T. and Simon, H. A. (1997) "Collaborative discovery in a
scientific domain," Cognitive Science, 21(2), p. 38.
14 Lerner, J. S. and Tetlock, P. E. (2003) "Bridging individual,
interpersonal, and institutional approaches to judgement and decision
making: The impact of accountability on cognitive bias," in Schneider,
S. L. and Shanteau, J. (eds.) Emerging perspectives on judgment and
decision research. Cambridge: Cambridge University Press, pp. 431-
457.
15 Sanitt, N. (1996) Science as a questioning process. Bristol: Institute of
Physics Publishing.
16 The writing of this book is one such situation. At the time of this writing,
we are months beyond our original deadline, because we decided to
examine certain questions of testing several levels deeper than originally
planned.
17 By "exhaustive," we mean a complete sample relative to some aspect of
some model related to the product. We do not mean "all possible tests"
since, just to speak on one issue: time is a variable in any test, and no
one can perform all possible variations of timing. But even if we restrict
"exhaustive" to mean all possible interesting tests, the problem is we
can't know all the variables that make a test interesting, before we have
tested—since the process of discovering that is the same process as
finding every bug that matters.
18 It does not mean that. It means any test performance in which the tester
is free to make new choices at any time, as opposed to following

instructions. New choices can be made in a thoughtful and systematic
manner.
19 We hope you are the sort of reader who feels compelled to think of
something we left off these lists.
20 Note that designing checks and investigating failed checks remains a
human activity because it requires social competence.
21 The mechanic ultimately determined the best explanation for the
warning light: some dirt had gotten into the sensor.
22 Except for things that were specifically anticipated and programmed by
the tester in advance.
23 AI cannot be a responsible tester, because AI does not have social
competence. Social competence is needed to make judgments about risks
and what constitutes sufficient testing. Anyone who claims that AI does
have social competence would have to back up that extraordinary claim
with extraordinary evidence.
24 James and Michael once faced the consternation of a research
psychologist who said, "In my field, we can't have sloppy execution of
our experiments." But he was speaking of a different level of sloppiness
than we are. He was speaking of undesirable variability that threatens
the reliability and validity of an experiment. We are speaking of a kind
of variability that hastens and enhances success by improving our search
for evidence. This is sometimes represented in the philosophy of science
as a distinction between confirmatory research and exploratory research.
25 James got the idea from a book on improvisation: Nachmanovitch, S.
(1990) Free play: Improvisation in life and art. New York: Jeremy P.
Tarcher/Putnam.
26 They still don't.
27 Lots of techniques called "___ testing" can be defined as "any testing
intending to explore risk associated with ___." Just fill in the blank with
a noun phrase, and you've created a test technique.
28 Notice that, by our definition, boundaries create equivalence classes.

29 "Normal" is a special word in testing. See our discussion of the many
meanings of normal, in Chapter 4.
30 In the course of writing this book, James experienced crashes of
Microsoft Word that would have destroyed days of work had he not
known about the undocumented places where Word automatically saves
files.
31 European Union, 2024. Directive (EU) 2024/2853 of the European
Parliament and of the Council of 23 October 2024 on liability for
defective products Official Journal of the European Union.
32 We got to this list by watching real testers test real products over many
years. If you think of a thirteenth, feel free to add it to your version of
this list—and let us know so we can consider updating our version.
33 Note that a comparable product is not necessarily a competitive product.
We're referring here to anything that someone has produced that affords
a basis for comparison. An algorithm encoded into an automated check
that is intended to provide output consistent with the product we're
testing is a comparable product.
34 It is rare even for testers to clearly articulate the skills they bring to the
project. We urge you to practice the skill of identifying and articulating
the important things you know how to do as a tester. See Appendix D for
a list of skills.
35 Klein, G., Pliske, R., Crandall, B., and Woods, D. D., 2005. Problem
detection. Cognition, Technology & Work, 7(1), Article 1.
36 Before you argue with this statement, imagine watching a video of all of
your interactions with the product that might reveal a bug. For how
many of those interactions are you meticulously conforming to explicit,
written instructions? (Even when we are doing highly formal testing, our
answer would be: a small fraction.)
37 Note that a procedure may also benefit from inefficiency and
unintentional behavior, leading to the discovery of surprising bugs.
Usually, that's a happy accident rather than a strategic choice.

38 Adding unnecessary inexpensive steps can be helpful as part of the
galumphing technique, described above.
39 Countering reckless advocacy of formality in software engineering, such
as in the "Parnism" that we discussed in Chapter 2, is one of the major
reasons why Rapid Software Testing was created.
40 As we discussed in Chapter 2, a process can be scripted even if nothing
is written down. What distinguishes scripting is that choices are no
longer being made, and a fixed pattern of behavior is established. A habit
is a script. Of course, if you establish a script for yourself, it's a "soft"
script, and you can change it if you wish.
41 To those who claim that AI can do sensemaking, we say there is no
evidence whatsoever that the "sensemaking" of AI is comparable to that
of people, or worthy of our trust.
42 Wall Street Journal, 2002, May 10th.
43 Your lawyers would prefer if you never used the term "defect" when
referring to your own products.
44 The Pentium bug of 1994 was like that. It resulted in a $450 million
recall.
45 For instance, the "scintillating grid." See
https://en.wikipedia.org/wiki/Grid_illusion
46 A non-optimistic developer will be skeptical and hesitant about tools and
components, making them slower at their job.
OceanofPDF.com

CHAPTER 4
How to Do a Test Strategy
—By James Bach, Michael Bolton, Huib Schoots, and Lalit
Bhamare
Without strategy, you cannot test intentionally. So, congratulations! If you
have ever tested with intention, then you have had a strategy. The problem
is, most testers aren't fully aware of what their strategy is—or even how to
think about it or talk about it.
People talk a lot about methodology (general ways of doing things), and
they talk a lot about test cases (specific instructions or ideas for performing
specific tests). Strategy? Not so much. Yet we think mastering test strategy
is far more important than writing down test cases.
Our definition of test strategy is the set of ideas that guide test design for a
specific project. As always, we separate ideas from documents. Ideas come
first. You can have a test strategy without any kind of test strategy
document, and in our experience, that's how most people do it: they have
notions about what needs to be tested and how, and they act upon them.
Our view of strategy roughly fits the way that term is used in the military,
which is: higher-level decisions about how to win a specific war that create
a context and foundation for the many lower-level decisions. Strategy is
above the level of tactics, or specific techniques, to solve a standard
problem. Strategy is also different from logistics, which is all about getting
the right resources to the right places at the right times so that the strategy
can be fulfilled.
Military theorist Carl von Clausewitz wrote that strategy enables us to
coherently improvise our work, "strategy must go with the Army to the field
in order to arrange particulars on the spot, and to make the modifications in
the general plan, which incessantly become necessary in war."1 Strategy is a
sort of compass by which we can navigate a chaotic environment.
Strategy can be intuitive. That's not a sin, in itself. But the thing about
intuitive constructs is that, by definition, we don't think systematically

about them. This could be a problem in several ways. The biggest problem,
in our opinion, is that it leads to the belief that good testing is easy, that
testing is not a highly skilled craft. Yet if you want to be taken seriously as a
tester, you must be able to show that you are doing things that non-testers
don't—and doing things above and beyond the ordinary and obvious. You
can't accomplish that merely by showing off your test cases, because test
cases do not justify themselves. They don't answer these strategic questions:
What specific value does this testing have for our project?
Why this testing and not some other testing?
Is this testing enough? Is it too much?
What might we be missing? What more is there to do?
In RST, test strategy may manifest directly as an impromptu explanation of
the testing we did or intend to do, provided during a meeting or
conversation. Or it may take the written form of an outline of what we
intend to cover, an outline of product risks that motivate our testing, and a
list of the kinds of test activities we intend to perform. Test strategy may be
kept in your head, written on Post-its, in a mind map, or embedded in a
larger test plan.
Test strategy is not the same thing as a test plan, but it is part of a test plan.
Test plans also include logistics, schedules, and such, in addition to test
strategy. We define a test plan as the set of ideas that guide a specific test
project.
You could say that a test strategy is the heart of a test plan—the interesting
part.
The Structure of Test Strategy
To describe the essence of a test strategy is not so simple, because it
depends on your underlying philosophy of testing. A common philosophy
might be characterized by what Karl Popper called the "optimistic view of
man's power to discern truth."2 He labeled this view truth is manifest. In
this way of thinking, all truth can be known if we just try hard enough to
see it.

For people who believe that the truth of the status of a product is manifest,
testing is easy and rather boring. Test strategy is easy. Just look at the
product and see the quality! That's why much of what is called testing in the
software industry is framed around a process of repeatedly (even
obsessively) demonstrating that a product can do a particular thing, in a
particular way, with particular data, on a particular platform, at a particular
time. When the process is formalized, we RST folks call that checking.
What typically remains unspoken, unwritten, and unchallenged, as checking
is commonly practiced, is the thought process behind checking. How do we
decide what aspects of the product to check and how to do it?3
In Chapter 3, we touched on the nature of this deciding process a little bit.
Here, we want to describe it a little more succinctly: most new or untrained
testers simply follow what could be called the matching strategy. That
means they match their coverage, procedures, or oracles to whatever is
explicitly mentioned in user stories, requirements documents, or any other
specifications. Where requirements are not explicit, the tester tries to match
coverage to what they imagine to be typical user actions. What will a user
do? Do that! Will the user like what the product does? Great!
Matching, however, leads to shallow testing. That's not necessarily bad if
the product is simple or involves low risk. Many big bugs can be found with
the matching strategy—but many others can't. If you want deeper testing,
thorough testing, testing that reliably finds elusive bugs, then you will need
a more nuanced test strategy.
This is why some testers have a very different default strategy, one not
oriented toward the happier paths but instead toward the arcane. They focus
on hunting for bugs. Their default might be described as the wrenching
strategy. Wrenching is a word that evokes:
Discordant operations and data
Throwing "monkey wrenches" into a system
Exploring extremes and exceptions
Testers who prefer a wrenching approach might describe themselves as
trying to "break the product." But always remember: the product is already
broken. What they truly seek to break are certain unhelpful dreams of their

clients. They seek to wake their clients from overly optimistic beliefs about
the product. A danger with a wrenching strategy as a universal default
approach is that we find strange and silly bugs that don't matter, while
perhaps missing obvious bugs that are less fun to hunt.
In RST, we certainly don't believe that the truth is manifest.4 We are
systems thinkers and skeptics. We seize neither on a matching strategy nor a
wrenching one. Our first move is to pop up a level and consider the context.
Then we test intentionally, based on our sense of possible bugs that might
matter. For us, the fundamental structure of test strategy can be described as
follows:
In this context:
By my learning and experiences with the product and project so far, I have
become aware of certain potential failures and various ways I could test the
product to discover them.
Therefore:
I am choosing the necessary and sufficient subset of those test activities that
should reveal the status of those risks without taking too much time or
energy to do so.
This way of thinking will naturally lead to elements of the matching and
wrenching strategies. If the risk is low, then our testing will rightly be
shallow. If the risk is high, it will be much deeper. As we feel our way
along, and as we consider which test activities are important, we explore at
least these oppositions:
Trust vs. doubt: How much of the system under test will you consider
"already tested" and treat as presenting too little risk to worry about?
Will you also trust the tools you use? Will you trust your platform?
Your procedure will get pretty complex if you trust nothing. But if you
trust everything, then that's just another way of saying that testing isn't
needed.
Cost vs. value: Although variety is great, and we want to do all the
wonderful tests in the world, doing everything would take a lot of time
and money. So, we must practice temperance. (See "Activities and
Enoughness" from Chapter 2.) The popular Pareto heuristic, or 20/80

rule, applies here. We call this methodology rapid because we try to
find the fastest, least expensive testing that gives us everything we
need. It's a sort of tightrope walk that is difficult at first but becomes
easier as we learn more about the product, the users, the business, and
the risks.
Interfaces vs. internals: You want to test the product. Okay. But the
elements of the product are not all equally accessible. Testing requires
you to manage your encounters with the product so that you can take it
through its paces, see everything that matters, and minimize the chance
that you will miss something important. Therefore, you must consider
the various interfaces available to you. You may have to create special
test jigs to exercise the product. Or you may find that the unit testing
performed by developers is good enough that you don't need any
special instrumentation or additional coverage.
Redundant vs. varied: Variety is the spice of testing. Variety fosters
discovery. In general, do different tests if you want to find bugs. We
might achieve variety through systematic analysis, or random data, or
perhaps by refraining from formalizing a test. But it is also true that we
often need to repeat or at least reprise certain tests, checks, or actions.
Redundancy can improve the reliability of the testing or help us track
changes in quality.
Shallow vs. deep: Where will you test more deeply and where will
shallower testing suffice? This is a key decision in test strategy. It
determines your mix of tools and data. It may determine whether
primary testing will take hours, days, or weeks.
Narrow vs. broad: Broad testing means using oracles that are
sensitive to a whole range of different problems. For instance,
comparing a snapshot of an entire database before and after an action
is a broader approach than searching for a specific change in a specific
table. Comparing a screenshot today against one from last month is
broad, although it will also pick up a lot of false positives. An example
of narrow testing is paying attention only to the color of text on each
screen, regardless of its content. Some oracles are cumbersome to use
at the same time. Separate them into distinct test activities.

Deliberative vs. spontaneous: We are professionals, so we want to do
better than merely blundering around. Anyone can find a bug. Not just
anyone can reliably find a bug on purpose—or find every important
bug. Professional testers test systematically and with intention. But
never say no to serendipity! Serendipity means happy coincidences.
This is why we avoid being too controlled in our testing (see
"Focusing and Defocusing" from Chapter 2). This is why we let
spontaneity and even humor inform our actions (see "Spontaneity and
Deliberation" from Chapter 2). It's another route to variety. Therefore,
we keep our strategies somewhat open-ended. We encourage people to
help us, especially if they think differently. And we keep our eyes open
to curios and clues.
Formal vs. informal: As we explained in Chapter 2 (in the section
"Freedom and Formality"), there is a continuum of formality. You
must decide where to be on that scale. In general, you start from
informality and move toward some level of formality that suits the
project. For projects that are mission-critical or life-critical or
otherwise regulated, you will need a substantial formalized component
to your strategy. Remember, do not confuse formality with
deliberation. Lots of formal testing out there is merely an expensive
record of poorly thought-out ideas—and now the project is stuck with
them.
Solo vs. team: What part of the strategy is (or will be) implemented by
you and what part by other people? In what ways might you need other
people to endorse your strategy? Your strategy might need to change
depending on how other people get involved.
Experience vs. instrumentation: We introduced this topic in Chapter
2, as well. It's a major element of test strategy. How will you involve
tools? Automated output checking is only one kind of tooling. So is AI.
How will your tester role play out with respect to tools? We have a
whole chapter about this: Chapter 6.
Retaining vs. discarding: Each test collects evidence about the
product. If you save that evidence, you will be able to review it later.
But collecting and reviewing evidence may also take a lot of time and
space. The authors are biased toward saving raw data as we test,

especially when we use a lot of automation. We have many times been
in the position of trying to answer unexpected questions from
developers or management and have been forced to do time-
consuming tests all over again because we didn't take good enough
notes the first time through. Not only that, but there is often also the
question of when a bug was first detected—if you archive product
output, you can search it, as needed. And we must note that there could
be legal implications either surrounding the illegal retention or illegal
discarding of test data.
Normal vs. abnormal: This is a big one, so we left it for the end of the
list. As a tester, your job is largely to identify and examine normalities
and abnormalities. You walk the fuzzy lines between them. You help
your team understand where those lines ought to be and what they
mean. Therefore, your behavior and way of thinking are not going to
be normal in certain important ways. You will do things that normal
users would do and also things only abnormal users would do. You
will observe the product under normal conditions and under abnormal
ones. In the next section, let's talk about "normal."
Welcome to Your New "Normal"
In RST, it's acceptable to use the word normal. It's typical to hear it in
discussions about users and product behavior, where it naturally emerges
from the process of contrasting and comparing different possibilities. It can
be a pleasing idea that seems easily grasped. But beware, it's an ambiguous
and fraught concept, too. For us as RST practitioners, the standard way of
approaching normality in test strategy is to unpack its semantics.
Planted throughout the preceding paragraph are seven different senses of the
word "normal!" In the course of writing this book, in fact, almost at the end
of the process, Michael was nitpicking over some folksy line that James
wrote about being normal. This turned into a debate (one of hundreds that
we've had that improved the book and further postponed its publication).
The debate led us to realize that experienced testers have lots of different
ideas about normality and abnormality that inform their test strategies5. Yet
these ideas are largely tacit—hiding behind that word normal, as in normal
users, normal input, and normal behavior.

Normality is such an important idea because it affects how people see our
work. Some clients believe testers waste time by focusing too much on
"weird corner cases" instead of things "real users do." This is indeed a
hazard of thinking like a tester. We do need to prevent ourselves from going
on overly extended flights of fancy. Nevertheless, these fancy flights are a
part of the job.
Here is the master heuristic of normality with respect to test strategy:
Generally speaking, in the abnormal realm, bugs are more plentiful; in the
normal realm, bugs are more important. Now it remains to unpack what
normality and abnormality might mean.
We use the mnemonic PEANUTS.
Pleasing vs. Disturbing
Normal can mean pleasing. In this sense, normal behavior is that which
someone wants to see. Abnormal behavior is that which they find
disturbing.
This sense of normality is about people's feelings, so it's subjective. But as
we wrote in Chapter 2, in the section "Systems and Models," feelings about
quality are often held in common among different people. This
intersubjectivity often allows people to come to a consensus about products
and behaviors that please them despite their subjective nature.
Part of good testing is to try to understand the trade-offs between pleasing
and disturbing qualities of a product. Maybe the benefits of the product
sufficiently outweigh its flaws.
Easy vs. Stressful
Normal can mean easy. In this sense, normal input is that which is not
difficult for the product to handle. Abnormal input would be something that
saturates the processors or runs the system out of memory—anything that
pushes limits but is still allowed.
So-called happy-path testing is all about easy demonstrations of the
product. There is also the idea of sympathetic testing, which means testing
that highlights and explores the benefits of a product while ignoring or
bypassing its problems. It may seem strange to ignore bugs, but as a

temporary expedient it can be a good move. Just as the shortest distance to
get into a building may not be to go straight through a wall, the fastest way
to find good bugs may be to postpone thinking about bugs for a bit and try
to have the best experience the product can offer. What happens when you
do that is you rapidly build a mental model of the product. You learn what
it's good for and why it matters. This is excellent preparation for testing that
focuses on finding critical bugs.
Good testing must eventually put the product under stress—perhaps,
considerable and prolonged stress. That's why testers must explore this
sense of normal and abnormal.
Acceptable vs. Disallowed
Normal can mean acceptable. In this sense, the product should handle any
normal input without displaying an error message. Abnormal input, in this
sense, would require error handling.
This sense of normal comes up in the popular ideas of positive vs. negative
testing. Positivity means using the product in ways and under conditions
that affirm (or you could say, posit) everything that the software needs to do
its job. You make sure you are giving legal input, that the product is
configured with enough memory and mass storage, and that all the services
are correctly installed and up and running. But users are not always law-
abiding, and conditions are not always as they are required to be. In that
case, we'd like to see graceful failure. Thus, the opposite of positive testing
is to violate (or, you could say, negate) at least one required condition—
disabling the product. Negative testing could mean giving bad data or
removing a critical resource.
Among other things, testers question the line between what is acceptable
and what is disallowed.
Natural vs. Contrived
Normal can mean natural. In this sense, the work of a tester may be
dismissed because a tester's work is not normal, since it didn't arise out of a
sincere attempt to make use of the product. It was specifically contrived to
maximize the likelihood of failure.

The problem is that testers cannot limit themselves to natural use. To be
effective, a tester must simulate a lot of users across a wide range of user
types and situations. That could be achieved with natural use only by
having at least as many testers as there are anticipated users, and then
testing for a long, long time (equal to some multiple of the entire
anticipated lifetime of the product). That would be crazy. Imagine trying to
test the jackpot functionality on a casino slot machine by sitting in front of
it like a regular user. That's not how it's done.
Most testing is highly contrived, but sometimes we want to maximize
naturalness. This is why beta testing is so popular. Beta testing—just giving
the product to users, hoping they will report bugs—is a powerful tactic if
you have a good outreach strategy or are able to collect logs of user activity
automatically. The big problem with it is that regular users are not usually
motivated to report bugs or particularly skilled at it.
Nevertheless, testers must keep in mind that their test data, automation,
separation from live users, and peculiar attention to abnormality may create
an overly contrived test strategy that may suffer crucial blind spots.
Us vs. Them
Normal can mean whatever we value, as opposed to whatever anyone else
values. Normality always requires a frame of reference. This is why we
RST practitioners frequently add the phrase "...to some person who
matters" (e.g., "quality is value to some person who matters") to any
statement of policy or judgment that might otherwise be misconstrued as a
matter of cold logic.
Consider the bias called the fundamental attribution error, which the
authors construe as: when I do something wrong, it's because the situation
is difficult; when you do something wrong, it's because you are
incompetent. We see this bias traveling along with the curse of knowledge,
mentioned in the "Sensemaking and Unconscious Bias" section of Chapter
2. Applied to software, this creates a tendency among developers to assume
that their software is easy to use (because they already know how to use it)
and that people who struggle with it probably aren't very bright or maybe
aren't trying hard enough (because the developers don't know enough about
those people).

In RST, we want to counteract this bias. For instance, we use a subtitle6
heuristic that goes: "no user would do that" really means "no user I can
think of, who I like, would do that on purpose." This is meant to remind us
that it's easy for us to forget about, never learn about, or outright dismiss
users who are elsewhere and unlike ourselves. You may not like hackers
(assuming you aren't one), you may not want the headache of designing a
product that will be accessible to blind people (assuming you are not blind
and have no blind friends), and maybe you wish that people wouldn't use
Unicode characters in their names (assuming your own name isn't Jürgen),
but that doesn't automatically mean your project should proceed as if such
users don't matter.
In the RST methodology (a context in which the authors of this book have
the authority to declare a norm), good testing requires us to make our
clients aware of alternative norms that may be relevant to their projects and
products.
Note that the us-vs.-them lens is applicable to all the other senses of normal
in this list.
Typical vs. Unusual
Normal can mean typical. In this sense, we speak of normal usage as the
ways the majority of users will interact with the product. Abnormal means
users doing things that may be only a little unusual, such as updating their
settings, or more unusual, such as exfiltrating all their data, running a web
app under a niche browser, or trying to use data analysis software on the
tiny screen of a smartwatch.
Which kinds of use and users are normal and which kinds are abnormal is
an interesting question, as we previously noted. But let's say that the
question is settled. You will now be tempted to ignore all abnormal users.
Testers must, to some degree, resist this temptation. Some unusual users
will stress the product in ways that might lead to publicly embarrassing
failures. If your product has thousands or millions of potential users, there
may be enough strange ones out there to make a lot of noise when they run
into problems.
If your product presents any kind of potential safety problem, then you
could be putting your company in hot water, legally, if you test only in ways

that the product should be used by normal people. You must also test for
reasonably foreseeable misuse.7
A good tester tries to anticipate how the product is likely to be used and
explores abnormal as well as normal use.
Standard vs. Special
Normal can mean standard. In this sense, a normal mode of operation might
refer to a default or original mode, as opposed to something customized,
added later, or otherwise peculiar to a particular situation. Standard could
be referring to a de facto standard, which is some pattern that has become a
popular point of reference, or it may refer to a de jure standard, which is a
pattern endorsed by some authority.
Testing using default settings and default data is not necessarily what
typical users will do, but it represents an implied endorsement by your
team. And if your company says that your product is compatible with a
certain service or data format, that is a sort of declaration of normality. For
instance, in the arena of web testing, certain unusual URIs might fit RFC
3986 and yet be disallowed by your product. This is the sort of thing that
testers must be alert to.
Good testers make themselves aware of any relevant standards that may
apply to their project—not just to what's mentioned in user stories and
requirements documents.
"This Is Fine."
Some people who focus only on the political abuse of the concept of
normality claim there is no such thing as normality. The authors don't
necessarily disagree, except that normality definitely exists and is
inescapable in the context of product development.
In their book Normality: A Critical Genealogy, Peter Cryle and Elizabeth
Stephens examine the turbulent history of the concept: "The normal was
formed in controversy and was everywhere nourished by debate. It
continues to be surrounded by conceptual clutter and built on
contradiction."8

Their reading of history reveals the reason they think normality became an
everyday idea:
The origins of the contemporary idea of the normal as it is used in
everyday speech can be traced not to nineteenth-century
disciplinary institutions, as Foucault has famously argued, but to
the beginning of what is now called the data society, in which
anthropometric measurements were increasingly used for
commercial purposes such as the production of mass-produced
consumer objects or the collection of information about subjective
experiences or opinions.9
In other words, as soon as you want to develop products for the open
market, you are forced to lump potential customers into types and design
for each type. You will also want to minimize the number of types for
which you must design. Thus, normality is an idea that designers need in
order to do their work. But where the lines are drawn between normality
and abnormality is an important question for testers to explore. This is
because our clients may accidentally marginalize a group of users that
would not be difficult to accommodate, or perhaps the product should more
gracefully respond to abnormal situations.
As we see from the preceding seven dimensions, far from normality not
existing, there are actually many variations of normality worth thinking
about. You can systematically consider them by mixing and matching
combinations, especially combinations of abnormal and normal together.
For example:
Typical, yet contrived (due to "them," a third-party tool): What is
something a lot of people do but is in some way not natural? How
about using a password manager to auto-fill the username and
password on a login screen? It's a very common user action, but it's not
real typing. In fact, the authors once found a bug where an email
address could be pasted into a field without trouble, but if the user
actually typed it and it included the number "8" anywhere in it, the
user would be blocked from entering it. Incidentally, automation tools

were unable to detect this problem because they were not simulating
user input naturally enough.
Easy (for "us," the user), yet stressful (for "them," the product):
Imagine an app where you can type the number of results you want to
get, and it automatically creates them as you type. It would be easy for
the user to type a longer and longer number without realizing that the
software was getting more and more stressed (and eventually crashes).
In general, the user should be protected from carelessly or accidentally
overstressing the system. One of the ways we detect such problems is
with a so-called shoe test, which is any test consistent with pounding
on the keyboard with a shoe, or placing a shoe on the keyboard and
walking away (barefoot, presumably).
Standard, yet unusual, due to us vs. them mismatch: In the mid-
90s, Microsoft came out with the first version of Microsoft Office for
the Mac. It was a flop because it was too slow and unreliable. It turns
out it was tested on machines that were too clean. Essentially,
Microsoft tested with "standard" Macs instead of Macs that were
realistically laden with other software and extensions. Microsoft testers
(who were not enthusiastic Mac users, reportedly) assumed wrongly
that they were seeing the same behaviors that real users would see.
So many special normals! But like the dog in the famous meme, this is fine.
This is the life of a tester.
First of All Things: Center Yourself
To concoct a decent strategy, you must know yourself as a tester. There are
kinds of testing that you like and other kinds that you don't. There is testing
you are familiar with and competent to do, and other kinds of testing that
may be strange to you. Certainly, if you are intimidated by the technology
stack or unable to empathize with the user, your testing will be impaired.
The idea of centering oneself may sound mystical. James once asked Jerry
Weinberg what it meant, fully expecting a vague, poetic answer. Jerry
replied pragmatically: centering means putting yourself in a position where
all your resources, skills, and knowledge are equally available to you. In
other words, an uncentered person suffers from unhelpful biases, lurching at

actions instead of choosing them rationally or constructively. Strong
emotions like fear or worry can uncenter us, as can fatigue or a sense of
great urgency.
What is your ambition as a tester? What is your strength? Where are you
weak? What is your duty? What would you like to learn? What do the
people around you need from you? Who around you can help you? Take a
breath and take stock of these things, because an excellent test strategy
starts with your mind, your feelings, and your resources.
Personally, we, the authors, approach every test project as an opportunity to
innovate in testing. That's important to us, to the point where we are a little
bit biased toward trying new tools and methods even when the project does
not require them. Part of centering, in our case, is to acknowledge this urge
and keep it from dominating every decision we make.
Developing a Strategy Is an Ongoing
Exploration
In the previous chapter about how to do a test, we contended that each step
of a test procedure is a kind of exploratory process. This is true (to some
degree) even if you are reading and following an explicit script. You choose
what the step should be, guided by some test idea (such as "step 14" of a
written test procedure, or perhaps a very unwritten general idea of
discovering how big a string you can put into a field), you interact with the
product, you make sense of what happens, and you evaluate the outcome.
The same kind of loop describes the development of a test strategy, except
on a higher level. Instead of being guided by a test idea, you are guided by
strategy heuristics, and instead of interacting with the product, you are
interacting with potential test ideas as you consider project factors and
product risk.
The process of developing a strategy could be prospective or retrospective.
Prospectively, you can generate test ideas using various strategy heuristics,
such as those we detail below, imagine how that testing would go, think
about how useful it would be, and keep the ideas that seem likely to work
well. Retrospectively, you can form your strategy by testing the product
intuitively, trying out various test ideas, and keeping the ones that did work

well. This is a bootstrapping approach. This is why we say that all excellent
formal testing is based (to some degree) on informal testing.
To develop a test strategy retrospectively, you need to have the product, of
course, and you need to be given reasonable time to explore it. But the huge
advantage of retrospective over prospective strategizing is that it is based on
vivid, up-to-date knowledge of the product. Of course, you could proceed
both prospectively and retrospectively, evolving your strategy as needed
throughout the project.
The ultimate goal of test strategy is to allow us to test efficiently. Perfect
test efficiency would mean that you only create tests that find problems, but
to do that, you would have to know where all the bugs are before you start.
Therefore, perfectly efficient testing is not even a coherent idea. Forget
about perfection of that kind. Instead, strive for reasonable efficiency based
on the product, the risks, and the various means by which you can test.
Twelve Test Strategy Entry Points
If you are secure in yourself and ready to work, you now have to pick where
to start. Here is a list of possibilities. We suggest any one of these topics to
get going. However, no matter where you start, if you want an excellent
strategy, you will need to consider all of them eventually.
What Are You Here For?
We described what we mean by mission in Chapter 2. If you have been
given (and have accepted) a specific mission within the project, you should
be able to say what it is. The next question is, "What specific testing will
fulfill my mission?"
Mission is a good place to start if your mission is especially restrictive. If it
is something general, like "handle whatever testing is needed for the project
you are assigned to," then this doesn't narrow down the test strategy at all.
But perhaps you have been given a much more specific objective, like "find
usability and accessibility problems" or "retest the product after we make
this specific fix for a specific customer." In that case, you can immediately
ignore a whole lot of testing that won't apply.

What Do You Need to Learn?
You may not be ready to construct the test strategy that your project needs.
You may need to study the technology or the business domain, or integrate
yourself into the project. You might need to brush up on the use of certain
tools or review test techniques. Avoid major decisions about test strategy
until you are reasonably qualified to make them.
Companies tend to downplay the need for studying and learning. Your team
may pressure you to plunge ahead. You may have to be assertive about
getting time for that.
In any case, if there are important gaps in your general business and
technical knowledge, the start of a project is a good time to fill them.
What Is Happening Right Now?
What's happening today? What are the developers working on? Is there
something you can test that would help the team immediately? There may
be a lot of things you want to test and a lot of ways you can test. That can
feel bewildering. Linear time can rescue you: take it one day at a time.
The danger of focusing on today, of course, is that you may be blindsided
by the requirements of next week. Still, this is a good heuristic for your first
days on a project.
How Is Your Testing Constrained?
Are there things you already know you can't do? Perhaps the process of
elimination is already working to dictate your strategy. Maybe you don't
have the luxury of a production-like environment with lots of realistic data,
and you don't have the time or skills to put one together. Maybe you haven't
been given the product to test. Maybe there is no requirements
documentation. This is the problem of poor testability, which will force you
away from the testing you might otherwise want to do.
Sometimes it can help to visualize yourself doing the testing. Then, in your
mind's eye, identify the tools, files, and data that you will need. If you have
trouble visualizing it, that's a sign you need to learn more about the product
and the project before you can say much about the strategy. Visualization

can help you realize that you need to create data, acquire a tool, or set up an
environment.
On one project, James visualized testing an embedded system and realized
that he couldn't picture attaching a power meter to the output of a medical
device. When he sought to clarify how this would be done, he discovered
that his client did not even have the required test equipment. Because he
hadn't waited to discover this at the last minute, he was able to alert the
team in time to do something about it.
We're talking about testability here. Testability really should be a test
management problem rather than a tester's problem. You could say that half
the job of a good test manager is to advocate for testability. But many
modern testers don't have support from any manager who knows about
testing, so it may be down to you to agitate for the testability that you need.
Push for testability as early as you can in a project because many aspects of
it are hard to improve, especially once the project picks up momentum. Our
basic method of testability analysis is to go through the testability heuristics
(see Appendix C) and identify areas of concern.
What Testing Has Been Done Already?
If testing has already been done by someone else, maybe you don't need to
worry about it, and you don't even need a strategy.
Or maybe, for your part of the testing, you can borrow or build on the
strategy that someone else has already worked out. Find out what tools and
data sources other testers in your organization are using. If your project is
building on an earlier project, perhaps you have inherited useful materials.
Obviously, it can be a good idea to build on what you have. Less obviously,
it might be a bad idea, too. We have seen testers carefully and skillfully
maintain test suites that were not carefully or skillfully created in the first
place. The maintaining testers may not have carefully reviewed the test
cases, because that usually takes a lot of time, effort, and perhaps
specialized expertise.
With few exceptions, we would say that if you are a responsible tester, do
not rely on any test process that you don't understand. A magic box that

does mysterious checking may be a good addition to a responsible test
process, but by itself, it cannot constitute responsible testing.
How Is the Product Being Built?
One of the prime objects of testing is to detect mistakes that developers
have made. There are those who claim that a fully virtuous developer who
studies hard and eats their vegetables can detect all their own mistakes. Or
perhaps patient and helpful users will cheerfully report them. We wrote this
book because we think this is a foolish claim for most commercial software
projects, and a downright negligent idea for mission-critical or life-critical
projects.
Yet developers can do a lot to build safely, preventing or detecting their
errors. Developer tools and languages steadily improve at catching certain
classes of bugs without any need for testing, or entirely within the scope of
unit level testing that may be easy for developers to do for themselves.
A powerful heuristic for test strategy, then, is to find out how the product is
being built, then do the testing that addresses the risks that aren't already
sufficiently covered by the developer's own efforts and tools. What are the
developers doing to test it? What kinds of bugs are they focused on, and
what kinds are they ignoring? What libraries and services are they using
that may already have been tested?
Developers and their tools tend to focus on low-level things and simple
kinds of bugs, unless they are unusually motivated, highly trained, and not
in too much of a hurry. A tester is an outsider to the development process
(not by design, but rather because their heads cannot be soaking in the
codebase all day, every day). This gives them critical distance. Use that to
test on a different level, in different ways, with different data, and with
different oracles than a developer might use.
What Is the Product?
The concepts of unit testing, subsystem testing, service testing, and system
testing can all be replaced with one idea: whatever the product is, test it.
Products have pieces. Test the pieces. Product pieces constitute subsystems.
Test the subsystems. Does it have functions? Test the functions. Does it

process data? Test with the data. Requirements documents describe the
product. Test every claim, and so on.
This is why the matching strategy is so popular. Focusing on coverage is a
natural and obvious approach to testing.
If we combine this with the bootstrap approach (see the section "Iteration
and Emergence" in Chapter 2), the result might be a kind of testing we call
a survey. Survey testing means testing in which the primary goal is to learn
about the product for the purpose of systematically testing it. We learn
about a product any way we can—any way that suits us—as long as we are
interacting with the product at some point in that session. While we learn,
we will run into problems. Some of those problems may turn out to be real
bugs, and some may turn out to be ordinary functionality that we haven't
yet understood.
A related kind of testing is what we call an analysis session. Here, the goal
is to make an explicit map of the product in some form (as opposed to a
tacit mental model) that will serve to anchor our systematic testing. We
encourage testers to make an outline or mind map of all the testable
elements of the product. We call it a product coverage outline, or PCO
(more on this in the section "Make a Product Coverage Outline").
Another idea we use a lot is to draw pictures of the product, then annotate
them in red ink with test ideas. We call that a visual test strategy. Figure 4-1
shows an example of strategy notes James made for a bibliography
generator called Zotero Bib. The colors are not reproduced here, but you get
the idea.

Figure 4-1: Visual Strategy Notes for a Bibliography Generator App
How Important Is Your Testing?
You wouldn't be testing at all unless you needed the product to be at least
pretty good. Yet, for any product, some parts may need to be very good,
while other parts are not so critical. If you know that your product is going
to have special quality requirements, you might start your strategy by
focusing on the important bits.
For life-critical or mission-critical products, using a quality focus to
organize your strategy might take the form of walking carefully through the
requirements to be sure you understand the quality criteria. It might also
require identifying relevant industry standards to which you may need to
conform.
For products of low criticality, starting with quality might take the form of
acknowledging that it may be worth spending only a short time on the

testing and designing your strategy to cover the basics only.
You can also start by analyzing the kinds of bugs that could occur and
designing specific test activities that will locate those bugs. We call this
risk-based test design (as opposed to risk-based test management, which
deals with how much effort to put into a test activity).
Risk-based test design benefits from any knowledge you might have about
the technology, the product design, and the users. The more experience you
have with a product—or a class of product—the better you can identify and
prioritize risk.
See the section "Organize Strategy According to Risk," later in this chapter.
How Will People Most Likely Use the Product?
Learn who the users are and why they would want to use the product. What
is ordinary daily usage of the product? What will people do less frequently,
yet still do on some kind of regular basis? Of course, there are the so-called
happy paths, but popular usage is probably much more than just the happy
stuff.
What sort of data is normal, in the statistical sense of the word? What sort
of problems, situations, and environments are typical? Will people be
distracted or under pressure while using it? What are the usual kinds of
mistakes they will make10? In some contexts, you may have to consider
malicious usage as well.
Are there different kinds of users who have different usage profiles? Can
we get any sort of feedback or usage data from actual users of this or any
similar product?
What Testing Is Easy to Do?
If a certain kind of testing is easy, you might as well do it without worrying
much about whether it is the perfect testing for the situation.
There is an old joke about a silly fellow who looked for his lost keys under
a lamppost only because the light was better there, and not because he lost
them in that place. But James lived this joke once, when he dropped his
keys down a dark stairwell and searched for them in the only corner where

a little light fell. As he did so, he mused about the economics of going to
find a flashlight versus taking a chance that the keys might have bounced
into the corner where the light was. He realized that the man in the joke
may have had a point.
Some test activities, including beta testing and simple automation created
by AI agents, are likely to be poor testing, yet they could be worth doing if
the costs are low.
What Do People Expect You to Do?
Testers share a strange thing in common with politicians and
schoolteachers:11 Every amateur thinks they could do your job as well as
you do—or better. This is not the case with surgeons, roofing contractors, or
lawyers, but testing is an especially accessible vocation. The result is that
your teammates, managers, developers, and users may have strong beliefs
about how you should test.
In RST, we often scoff at traditional notions of testing. Still, it's important to
work in a way that is legible to people whose support you need. If doing
some of what they think you should do (for example, writing some test
cases) makes them happy and doesn't hurt the project, it could save you a
lot of trouble.
Starting with expectations could mean having conversations with your
coworkers about whether they have any suggestions for testing. It could
mean explaining to them your philosophy of testing and asking for their
support.
What's Fun to Do?
This isn't about mere self-indulgence, but rather about self-care. If you push
yourself to do only hard and unpleasant things, you risk burnout.
Boredom is an important signal in testing. When you find yourself feeling
that testing is a slog through knee-deep mud, and you just wish that it were
all over with, that could be important information. Maybe this task is not
worth doing—or at least not in that way. But no matter how important a
boring task might be, boredom creates a situation that is dangerous for
testers: it tends to shut down our senses. It dims our wits.

Another reason to perform survey testing is that it's fun to play with the
product and discover things, without a lot of rules to remember or structures
that bind us. If anyone accuses you of merely goofing around, tell them that
you are practicing cognitive self-care while building the mental models
required to do excellent testing.
You must manage your energy and sense of engagement. A great strategy is
pointless if you won't put it into action. Therefore, consider starting with
something you feel like doing.
Seasons of Testing: Strategy Throughout the
Project Cycle
Project cycles present at least four distinct contexts in which testing may
occur. We call these seasons in RST, since they are a bit like the seasons of
nature (spring, summer, autumn, and winter, respectively). Each season of
testing requires a different take on test strategy:
Prospective testing: This means testing precursors to a built product.
In other words, any testing that can occur before some version or
portion of the product you intend to release is available in a runnable
form. The season of prospective testing begins when there is an idea
for a product. You can think critically about that idea, consider risks
and error handling, anticipate and plan for the testing that will come,
and advocate for testability. You can test a prototype, a simulator, or a
similar competing product before the real thing has been coded. When
you hear people talk about "shift-left" in a testing context, they are
probably talking about prospective testing. Since bugs found in a
precursor are not bugs in the product itself, your concern is not that
some user will experience those problems, but rather that bugs in a
precursor will infect the built product with similar or related problems.
Primary testing: For decades, the authors just called this "testing."
But in recent years, it has become apparent that many clients of testing
seem to say "testing" when they are actually referring to regression
testing or strictly to the process of preparing for regression testing. So,
we have sharpened our terminology. Primary testing is the testing you
do when first presented with an allegedly operational product.

Regardless of whether you were able to engage in any prospective
testing, you can now experience the reality of the once-rumored
product. You can investigate it and look for problems. This is the
season of developing and fulfilling the bulk of the test strategy—and
finding most of the bugs. Primary testing, done well, is a highly
exploratory process that produces rapid learning. It is always progress
toward good regression testing because the learning you do enables
reasonably efficient regression testing. However, there are lots of
special things you can do to get ready for a regression cycle that get in
the way of primary testing (such as writing shallow automated checks).
So there can be tension between these two seasons.
Regression testing: This is any testing that is motivated by risk due to
changes made in a previously tested product. When managers and
developers talk about testing—especially automation applied to testing
—they are almost always talking about regression testing. It can
become a fixation.
Since we don't have time for comprehensive retesting after every
build, regression testing tends to be quick, shallow, and based on a
certain faith that good primary testing was performed and that the
programmers are trying to be careful. Management tends to imagine
regression testing as a general retesting of the whole product. But it
isn't really, because, practically speaking, we will not invest the same
effort in regression testing as we did in primary testing. Yet regression
testing morphs into primary testing when it is focused on specific
risks associated with a specific change or when investigating a bug.
Remedial testing:12 This is testing a product after development and
deployment are done. We don't do remedial testing because of changes
(that would be regression testing); but perhaps because there hadn't
been enough time to properly test before release, new risks have come
to light (e.g., a customer reports a bug), some new client doesn't trust
it, or somebody belatedly realizes that important testing has been
skipped. Remedial testing can be challenging because it's usually
interrupting work on the next project—it's hard to shift-left when the
previous project has shifted hard to the right. Also, developers may not
be available to open up the black boxes, and if you are testing in
production, you don't want to disrupt your customers.

As an example of enabling testing earlier in a project, Lalit tells the story of
a project where there was a rift between the design and engineering teams.
Changing designs, buggy front-end components, misunderstood
requirements, and missing impact assessments created these tensions. Lalit
felt the problem was that feedback on the design was happening too late
with respect to decisions about how the UI rendered different messages,
handled different languages, complied with accessibility requirements, and
navigated through various user journeys. But because they didn't have a
front-end application that was fully integrated with the back-end systems, it
was almost impossible to test early.
Lalit recommended using a particular development tool13 that provided "a
sandbox for UI components, allowing developers to build, test, and
document them in isolation." The idea was to promote component-driven
development (and testing).
Implementing the sandbox helped in the following ways:
The front-end application code became synchronized with design
components in the design system.14
Any change in the design was highlighted in the front-end code as
being out of sync, compelling developers and designers to have a
conversation.
The developers as well as the testers were able to test the UI
components without having to spin up an entire environment
integrated with the back-end system.
Feedback on accessibility compliance was possible at the component
level.
Writing interaction checks in the tool helped with automated feedback
on various user actions in the frontend.
Since obvious defects were caught and fixed earlier, testers had time
for deep testing—highly exploratory testing that helped to find critical
defects that were hard to find earlier.
Lalit's story is about changing the way the product was being developed so
as to dramatically improve testability. This, in turn, enabled new

possibilities for test strategy.
But be careful with that slogan "shift left!" The people who promote early
testing tend to focus only on the potential advantages without reckoning the
costs and risks. Early testing means testing with much less knowledge about
the product. It means testing in environments that aren't realistic, aren't fully
integrated, and won't exhibit the same kinds of bugs that users will
eventually see. The other problem with early testing is that people may not
be available to do it, because they are on other projects that are urgently
about to be released.
See Chapter 8, "Prospective Testing," for more details on the dynamics of
"shift left."
Regression Testing May Not Be What You Think
Whether performing a specific test, developing a test, or engaging in an
open-ended testing activity, when we encounter a product, good testing is a
learning process. Learning processes are cyclic and noisy.
At some point, the learning settles down. We get a handle on the general
status of the product, and the activity of testing turns to monitoring the
product as it changes. This represents the transition between primary testing
and regression testing. You could say that primary testing has two goals: to
find bugs and to prepare the way for efficient regression testing.
Here is a classic—and totally wrong—definition of regression testing:
"rerunning old tests to ensure that the product still works after a change."
Let's break down the wrongness here:
"Re-running old tests:" Unless your old tests represent all the testing
that could ever have been done, then running old tests cannot prove
that the product still works. But it's worse than that, because re-running
your old tests doesn't even provide the value that your previous testing
gave you. This is because you did a lot more to test during primary
testing than you will during regression testing. During primary testing,
you were probably doing a lot of learning and exploring because you
were building your mental model and discovering product risks. You
were doing a lot of speculation and following up on questions. During
regression testing, you already have your mental model. Whether

consciously or not, you will optimize your strategy based on those
beliefs.
If your primary testing was really nothing more than writing formal
scripted checks and then running those checks, then regression testing
may indeed be just like primary testing. But it will also be a far
weaker kind of testing than it could have been if you had explored
and speculated like a curious and critical professional.
"Ensure that the product works:" Testing never ensures that a
product works, so this is pure wishful thinking. Testers look for
trouble. At best, we find that a product can work, not that it definitely
will.
"After a change:" Yes, we perform regression testing in the face of
change. But to do that testing well, we must understand the specifics of
the change and perform testing that responds to it. This usually means
deliberately crafting new testing.
Regression testing is not really a kind of testing but rather a motivation and
a context for testing. That's why we speak of it as a sort of season. We may
need to do a lot of it over the life cycle of a product. A danger is that it
becomes brittle, shallow, and ceremonial over time. The high art of
regression testing is keeping it fresh. That includes knowing when to switch
back to a primary testing mentality.
The Challenge of Alignment: Strategy Is
Different for Testers and Builders
As we discussed in the "Stance and Distance" section of Chapter 2, there
can be a lot of tension between people with a building mindset and those
with a testing mindset. The first group are those whose main motivation is
to deliver a good enough product at a reasonable cost. Creating quality in a
complex system is difficult, so the building mindset is preoccupied with
how to make things work. The second group, testers, are motivated
primarily to discover the truth of the status of the product and any trouble it
may have. Dedicated testers can focus on this task without the distraction
and dissonance of having to fix the bugs they find.

Both groups use the word testing, but they probably use it in different ways.
This ambiguity can lead to hard feelings and threaten collaboration.
Brian Marick explored this ambiguity around 2003, with a two-dimensional
grid that was elaborated in later work and became kind of a "thing" in Agile
testing circles. James and Michael have strong concerns about how the
concept of testing was treated in that work, so we created our own version
that corrects those problems and is compatible with the tenets of Context-
Driven testing. We call our version the Agile Rapid Testing Grid (ART).
Figure 4-2 illustrates a simple version of the grid that depicts four kinds of
testing work.
Figure 4-2: The basic ART grid
Let's walk through it.
The ART Grid is based on two oppositions or tensions. The first is that we
want a great product, but we don't want to pay a lot to create it. That's the
top and bottom. These things are in tension because if we cut too many
corners on development, the value of the product must suffer.
The second opposition is that we need to build the product, yet we must
also discover what we actually did build. That's the right and left. The
tension here is less apparent because many assume that the act of building

something already entails or bestows comprehensive knowledge of it. This
is not true for many reasons, including:
Builders necessarily use components about which they have limited
knowledge, and these components may harbor surprising limitations
and behaviors.
Builders are obliged to write code that handles situations and data that
are both known to them and in their conscious awareness. Yet there
may be unforeseen situations and data that are important, too, and the
code may not handle those.
Even if we could have perfect prior understanding and intentions,
builders may make trivial errors in implementation and fail to catch
them, such that they have a false understanding of the end product.
Builders may not have the time, tools, or motivation to try the product
under challenging or realistic conditions—or to try all the variations
and explore all the implications of even ordinary and unchallenging
input.
Operating conditions and usage patterns may change at a later time,
which may reveal latent bugs in the existing system.
Building a product can be so engrossing that builders don't leave much
time to experience what they have built. This leaves them with
potentially false assumptions about how the product will actually work
in production and how the user will experience it.
Dedicated study of the product protects the business from being surprised
by the effects of these hazards.
As shown in Figure 4-3, these two oppositions form four quadrants, which
represent different risks, different perspectives on testing, and different
testing challenges.
At the intersection of High Value of Product and Studying It, we have the
classic testing problem: "How do we find all the bugs that matter?" This is a
very different problem from the opposite corner, at the intersection of
Building It and Low Cost of Development. Here, testing answers the
question, "Did we just make an obvious mistake?" For everyone except

dedicated testers, the most important thing for testing happening in the
lower right is not to disrupt development. Finding big, obvious problems is
only the next most important thing. So testing in the lower right is part of
the discipline of development, whereas in the upper left, it's a deeper
process of realizing the whole truth about product risk.
Figure 4-3: Characteristic risks for each quadrant
At the intersection of High Value of Product and Building It, testing has a
different shape. Instead of testing to understand the implementation, we are
testing to understand and help form intentions. The question is, "How
should the product work?" Activities like A/B testing, usability testing, and
beta testing live here. These are focused outward toward the users. Many
aspects of prospective testing figure in the top right, along with behavior-
driven development. By contrast, the lower left is inward-looking. It's about
testability, asking the question, "How can we test more efficiently and
effectively?"
All four quadrants are relevant to testing, but test strategy looks different
from each of those vantage points.
The grid becomes a representation of the basic Agile process if we relabel
the outside of it and replace the center with commonly understood

principles of Agile, as shown in Figure 4-4. Think of the project as
sweeping around this model in a generally clockwise fashion.
Of course, in practice, the process is not a clean, clockwork sweep. We
bounce around the grid in an opportunistic way, and also zoom in and out.
Each of these quadrants can apply to the product, any part of the product,
any idea about the product, or any change to the product. The developers
focus more on low-level testing, while the testers and others focus more on
the user and whole-system perspectives. But we should not allow ourselves
to become imprisoned or seduced by only one of these quadrants, or only
one level of analysis. Shift-left and continuous delivery advocates want to
focus on anything but the top-left quadrant, which they regard as an
expensive indulgence rather than something necessary for responsible
product development. Meanwhile, it's tempting for independent testers to
build a fort up there in the top-left quadrant and not come out until
management lobs smoke bombs through our windows.
Figure 4-4: Grid with clockwise process and activity list
The grid reminds us that product builders and managers are probably going
to talk about testing in terms of straightforward and preferably automated
output checks. The top left includes a deeper and more disruptive kind of
testing, which is our best chance to minimize the risk of unhappy surprises.

But management never has much patience for disruption. That means
testers must be able to explain the motivation for deep testing! We must
explain the product risks we have identified and convince the team that
those risks should be investigated, even if it takes time.
The business purpose of testing is always to understand product risk in a
way that allows the business to make better decisions about the product. If
the business can't see how more testing would lead to significantly better
decisions, it will cut that testing process short. Many businesses have never
even seen good testing, alas.
Testability Enables Test Strategy
Lalit tells another story about a project team building web-based
applications to be accessed both on mobile and desktop. The original back-
end of the system had been built years ago by another team that had
departed. Language, culture, and location barriers made the collaboration
with the current back-end team difficult. There was no test data
management team in place, nor any possibility to create on-demand test
data. As a result, it was difficult to force the back-end to produce the
necessary output to exercise the frontend. Testers struggled to perform deep
technical exploration of front-end and back-end integration, particularly for
mobile apps.
Lalit analyzed the situation and recommended using an HTTP proxy to
intercept network traffic. By intercepting API calls and changing the back-
end responses on the fly, it became much easier to test how the front-end
responded to different scenarios, whether on desktop or mobile apps. This
approach completely eliminated the need to change actual values in the
back-end database when testing the GUI.
This greatly helped with controllability of the system—a key element of
intrinsic testability—that, in turn, helped the team identify bugs earlier.
Advocating for testability and improving the test strategy go hand in hand
(see Appendix C for testability heuristics). See also Chapter 11 for Janna
Loeffler's experiences with managing testability.

Good Test Strategy Is More Than Just Following Behind
Development
A common, almost ubiquitous, industry practice is to break down the
development process into very small segments. Developers attempt to build
systems in discrete sequential bits. This is often mediated by so-called user
stories, which are sometimes story-like and sometimes just a description of
a feature or recitation of a requirement.
It's not wrong to want to build little by little, such that a product is
frequently releasable as it grows in complexity and capability. But this is
not a simple process. Behind the scenes (that is to say, between meetings
and beyond the Kanban boards), the process forces developers to do a lot of
rewriting and even rearchitecting of the product. Sometimes developers go
off and do substantial research and development work on technology that
will enable new features, long before implementing the features themselves.
This work may not even be directly reflected in any one user story; rather, it
affects performance, reliability, compatibility, interoperability, or scalability
across the product. Therefore, adding what looks like a small feature to a
product may involve a lot of work (due to background refactoring and bug
fixes), while adding what looks like a large feature may be a simple matter
(due to the simple addition of an established component).
In a similar way, some things that were easy to code require a lot of testing,
and some that were hard to code may be easy to test. Some "small" features
may involve a lot of testing work with vast amounts of synthesized test
data, and some "large" features only require a sanity check. There is also
testing work that isn't performing a test and that isn't directly related to any
user story, such as crafting or scrubbing data sets for use in testing or
developing special tools to configure and operate systems.
Testing tasks are just as necessary and real as development tasks, but too
often, testers are discouraged from spending time on them because they
appear not to "add customer value." Everywhere, management tends to
assume there is a fixed symmetry or proportionality between testing and
development, as if testing were a kind of value-added tax that is paid—
ideally only once—on each parcel of development work.
This has led to a widespread refusal to think strategically about testing.
Many testers ignore the bigger picture of the product, instead seeing only

individual user stories. This could be called an atomized strategy. It usually
goes like this:
1. Receive a user story.
2. Quickly write test cases for the user story by matching what is
mentioned there (happy path procedures without considering data,
state, or exceptions).
3. Expect the automation team to turn them into automated checks so that
they can be run as "regression tests."
4. Repeat for each user story.
What's missing here? The entire learning loop is missing, for one thing.
Experiential end-to-end testing is missing. Experiments to test wishful
theories about the built product are missing. There is no consideration of
interactions among features and components, nor any consideration of the
efficiencies that might be gained by testing things together using carefully
crafted test data. Testing is immediately overformalized, which means it's
stuck at the level of ignorance the tester had on the day they wrote the test
cases. There's nothing here about clever tooling, only the blanket idea of
"automate the test cases." And there's no attention to risk.
As the tester, you are expected to keep your head down and work ticket by
ticket. Wear blinders, workhorse, and stay in your little stall.
This approach is obsessed with regression testing while barely doing
primary testing. It's allowed to happen when management doesn't
understand the value of the deeper investigation that characterizes primary
testing compared with the relatively shallow rechecking that we do for the
regression season.
Atomization is popular because it's predictable and seems legible to
outsiders. But it is not an efficient or effective way to find important bugs.
And when you finally look up from your tickets and Kanban cards, you find
you have a massive, bloated test case database that you can't explain,
justify, or fully understand.
Here, also, Lalit has a useful experience to report. A project team had built
several features that were to go live across multiple markets globally, at the

same time. Because of bad experiences with previous releases, confidence
in delivery quality was low. Management wanted better testing, but was not
sure how to get it.
Lalit was brought in as an internal consultant to help. In his assessment, he
discovered:
Functional testing was performed mostly against the acceptance
criteria written by product owners.
The acceptance criteria were superficial.
Test cases were merely step-by-step instructions for demonstrating that
the implementation of a user story could fulfill its acceptance criteria.
The regression test suite was mostly outdated and lacked risk mapping
and risk-based prioritization.
Testers from different teams knew little about functionality outside
their team's scope.
Considering stringent timelines and the project team's reputation at stake,
Lalit decided to conduct an "Exploratory Testing Day"15 once each week
for three weeks.
With the help of senior testers and business analysts in the team, he created
a risk-weighted list of application areas that related to different markets. He
then grouped testers together from different teams to mitigate gaps in
product knowledge. Each group was assigned specific test charters (focused
statements about what to test) to support the overall testing mission.
The charters for each testing session were carefully designed against
identified risk areas, as well as by splitting the product into Structure,
Function, Data, Interfaces, Platform, Operations, and Time16 aspects for
broader coverage beyond acceptance criteria.
Each test session was debriefed, and that helped decide what areas to
prioritize further, what to skip, and how to best leverage the findings from
testing sessions done by other testing groups. In some cases, a session
performed by one group served as test data for another group's testing
sessions later on.

The outcome of this Exploratory Testing Day strategy, which involved some
40 testers, was the discovery of hundreds of integration bugs, data-specific
defects, and usability issues. This strategy helped the project team get
deeper insights into the real status of product quality, associated risks, and
opportunities for more investment. And it did so without writing any test
cases or wearing blinders.
These testing sessions were later introduced as standard practice for each
release cycle.
Practices Worth Practicing
Let's say you've centered yourself and you've gotten a start on your test
strategy. What now? How should you develop the strategy? While we don't
believe in "best practices," we do believe in cultivating useful heuristics
and skills, and then applying them when and where they help. Here are
some of our suggestions.
Make a Product Coverage Outline
In 1989, James was in charge of a tools and methodology team within the
Development Systems Quality department at Apple Computer. The team
was asked to analyze how test plan documents were used in the department.
What they discovered was that the test plans were being ignored. They were
being written only to please management, and then they were immediately
set aside. Actual test planning seemed to be done incrementally and
collaboratively as each project progressed. Actual test plans were not
expressed in the form of handsome documents but rather in the form of
conversations in real time.
James suggested a radical idea: ban written test plans. If testers were
breaking the rules by not following their written test plans, perhaps if we
banned them, testers might break the rules by writing something down that
did help. This is exactly what happened. So what documents did people
create when they were told not to create documents? Lists. Lists of features.
Lists of risks. Lists of test ideas. In other words, very lightweight
documentation that maximizes information content and maintainability.

From that experience grew the RST focus on mind maps17 and text outlines
as a default form of documentation. And the one kind of document we are
most likely to create on a test project is a product coverage outline (PCO).
A PCO is simply an outline of testable product elements. It is the set of
things that are relevant to any discussions or decisions about what to test or
what we have tested. A PCO may include items that we are not going to test
or that are out of scope. It may be short and vague or long and detailed. We
generally create them in a top-down fashion, adding detail over time.
Here's an example, created for the Slide Size dialog box from PowerPoint:
Slide-sized-for
             On-Screen Show
             Letter Paper
             Ledger Paper
             A3
             A4
             B4
             B5
             35mm
             Overhead
             Banner
             Custom (activated automatically when w/h changed)
  
             ? fills in height and width differently depending 
on printer?
  
Height/Width
             Automatic setting from slide-sized-for
             Automatic setting from portrait-landscape
             Min (1)
             Max (56)
             Increment (by .1; also rounds to nearest .1)
             Precision (hundredths)
             Size (32 characters max; 0 min.)
  
             Vs. print preview
             Vs. size of fonts on page (font sizes on page are 
automatically scaled)
             Vs. wordart and other objects on the page
  
Number-slides-from
             Max (9999)
             Min. (0)
             Representatives 

             1
             any two, three, and four digit number
             any number that when added to number of slides 
printed exceeds 9999
  
Vs. number of slides in presentation
  
Slide-orientation
             Automatic setting from w/h
  
Notes-orientation
             ? Not automatically set from w/h. Why not?
  
Context help
             Question Mark icon
  
General Edit Field Functionality
             Up/Down arrow
             Left/Right arrow
             Home/End
             Copy/Cut/Paste  
Notice that we try to avoid putting fully formed test or risk ideas into the
PCO, because that would complicate it and also tend to reduce its flexibility
and power. But it's a fuzzy line. We often find ourselves putting interesting
combinations and data choices into the PCO, then later moving those things
to a test strategy outline.
When creating a PCO, we all tend to focus on the visible and obvious
things. It takes a bit of work to go beyond the visible. Look for unlabeled
behaviors, undocumented business rules, and undeclared boundaries. Look
for relationships between different product elements.
Apply the Heuristic Test Strategy Model
Wait! First things first: why should you care about learning how to apply
the Heuristic Test Strategy Model (HTSM)?
If you test anything, there are a jillion things you could do. You can't do all
of them, but you should do some of them. You must choose a good subset.
If you want to go beyond a default matching or wrenching strategy—and if
you don't want to trust, exclusively, in intuition and luck—you might be
tempted to leap into comprehensive and time-consuming formal modeling.

But there is a middle way! A way that is more tractable and flexible than
high formality, while more reliable and legible than intuition. The HTSM is
a lightweight tool for thinking through the principal elements of test
strategy. It consists of four lists: Project Environment, Product Factors,
Quality Criteria, and Test Techniques (see Appendix B). The lists are made
of "guide word heuristics." A guide word is a word or phrase that reminds
you of an important idea. A guide word is not a command; it's a nudge.
The diagram in Figure 4-5 shows how the lists relate. To design a test, you
must consider the parts of the product you want to test, the kinds of bugs
you want to find, and the feasibility of performing that test. This is why the
diagram shows arrows coming in from product factors, quality criteria, and
project environment. The test itself may take many forms, but there are
some forms that testers use so frequently, we call them test techniques and
give them names.
James designed the first version of the HTSM in 1996 to help new testers
become good testers right away, but that didn't work. It turns out new testers
don't know what to do with guide words! Guide words aren't self-
explanatory. What new testers need most is to practice exploring products
and reporting bugs. New testers need experiences, and they need to see
testing in action.
The people who get the most use from guide words are experienced
testers18. The challenge for experienced testers is to put their thoughts in
order and to avoid forgetting the things they know should be done. Guide
words are a lightweight and flexible way to get organized.

Figure 4-5: The structure of the HTSM
The HTSM is based on the idea that any test is the realization in time and
space of an idea to examine a particular aspect of a product and assess it
according to some quality criteria. The Project Environment list is all about
grounding our strategy in time and space. We need to understand our
context and find a way to fit our testing to it. The Product Factors list helps
us think through what kinds of things need testing (product elements and
test conditions). The Quality Criteria list reminds us that there are many
different kinds of bugs we might want to find. And the Test Techniques list
includes important families of test activities that we might want to include
in the strategy.
The HTSM can be used in different ways. For instance, you could walk
through it synchronously, visiting every guide word and savoring it for a
couple of minutes as you consider how it relates to the product under test.
This will help you identify every testable element of the product and many
interesting questions, issues, and test ideas. This approach may take hours
to work through.

From this process and the notes you make, you can create a product
coverage outline, a risk outline, and an outline of your test activities (or
even test cases, if that's your thing).
Or you can apply the HTSM as a safety net. To do this, create a test strategy
or otherwise perform testing any way you like for a while, then periodically
visit the guide words of the HTSM—even just briefly. Ask yourself, "Is this
an issue? Am I handling this?" You may discover that you have completely
forgotten something important.
The authors have memorized the HTSM, which allows them to do a little
party trick: extemporizing rapid-fire test ideas for any technology or feature
that someone calls out.
Organize Strategy According to Risk
Risk is the ball game. Testers exist only because the business is worried
about the consequences of releasing a bad product. Testers are mostly
concerned about the subset of business risk that we call product risk: the
danger of the product doing something bad that will harm someone who
matters.
Even though all testing is motivated by risk, it doesn't necessarily follow
that all testing should be organized around identified risks. This is for a
simple reason: if we only test identified risk areas, then we will
systematically avoid testing real risk areas that we have not yet identified.
Nevertheless, we can't do everything, so it makes sense to focus on risk to
some reasonable degree.
In RST, we suggest using risk as an organizing principle of test strategy, but
also doing at least some things that are not aimed at identified risk. These
include low-cost general exploration of the product, galumphing, beta
testing, and perhaps other more coverage-focused techniques.
The general flow of risk-based testing is shown in Figure 4-6. It is cyclic, of
course:
Conjecture: We analyze risks as best we can, using insights about the
technology and our knowledge of how the product will be situated and
used.

Triage: We clump the risk ideas into groups that are amenable to a
particular kind of test activity, and then rank them by perceived
importance.
Focus: We set the test strategy accordingly, guiding what parts of the
product we test more or test less, what kinds of problems we focus
more on or less on, and suggesting what sort of testability we might
need to advocate for.
Reality: The results of the testing help us tune the risk analysis. If we
do risk-based testing and find no bugs, that suggests the risks we
identified are not present after all. If we find the kinds of bugs we
expected to find, our analysis is confirmed. But we might stumble into
problems that we didn't expect to find—allowing us to improve our
risk analysis. Or we might miss important bugs that are found by
customers, which teaches us the same lesson, only more painfully.
Figure 4-6: Flow of risk-based testing
This cycle operates not only on the level of the entire product and project,
but also on the level of individual risks. As Michael likes to say, "It's a

fractal!"
As we discussed in Chapter 2, testing is not just based on risk analysis—
testing is risk analysis. It's relatively empirical risk analysis, compared to
armchair hypothesizing. Thus, risk-based testing is another thing that is
bootstrapped: our guesses about risk (identified risks) inform our testing,
and the results of our testing (tested risks)—whether we discover a bunch of
real problems (high risk) or no problems at all (low risk)—allow our clients
to make informed decisions about releasing the product.
Our best ideas about risk unfold over time. So we should not expect to work
them all out in the course of one brainstorm.
Risk Analysis Is Mostly Conversation and Consensus
Risk is bigger when it is more likely or when the consequences are more
damaging. This has led to a popular pseudoscientific approach to assessing
risk known as semi-quantitative risk analysis.19 This is the practice of
subjectively assigning a bounded number, such as an integer from 1 to 10,
to the probability and impact, respectively, of a particular risk; then
multiplying them together to get a risk magnitude. Some self-proclaimed
authorities in our industry present this as a scientific way to compare the
importance of different risks. It has been called a "best practice" and
enshrined in standards.20
In fact, this is mathematical malpractice. It's not a method for avoiding bias
or fostering rationality; it's a method for enshrining our biases in a patina of
rationality. The National Institute of Standards and Technology warns
against it ("The rigor of quantification is significantly lessened when
subjective determinations are buried within the quantitative assessments, or
when significant uncertainty surrounds the determination of values" 21), and
we are not the only ones who argue that it should not be used for software
projects.22
The main reason that this practice fails is that it violates basic measurement
theory: using an ordinal scale (where only order matters, such as when you
make a "top ten" list) as if it were a ratio scale (where a 2 is worth double
that of 1). What does a "1" mean when expressing risk? It's the lowest
number, so we would have to assign it to very low risks. But when we
assign a "1" to a probability, that could encompass a real probability of, say,

1% or .001%. When we assign a "10" to high impacts, that could
encompass a loss of, say, $50,000 or $50,000,000. That means there is
potentially a 1,000,000× difference between two risk magnitudes that are
both given exactly the same semi-quantified risk magnitude of 1 × 10 = 10.
Meanwhile, a "middle-of-the-road" undramatic risk of 5 × 5 = 25 is 2.5×
the size of any of those rare but disastrous risks. How does that make sense?
When people assign semi-quantitative numbers, they are really comparing
their feelings about a particular risk to other risks that were assigned similar
numbers. Nothing about that process justifies multiplying them together.
Risk "analysis" based on subjective assignment of numbers invites people
to underestimate and ignore terrible problems that are pretty rare, while
lulling themselves into a false sense of responsibility.
There is an alternative: using feelings directly without the pretense of
science, while being honest about the subjectivity in the process. We can
engage in conversations to assess and weigh the evidence. Based on the
discussion, we can sort risks into broad tolerance thresholds such as "lower,
normal, and higher" and apply different rules and expectations to each
category. This requires no mathematics at all.
Risk analysis is not usually the discovery of risk as an objective fact.23 It's
more like the social construction of dread, based on the evidence at hand.
This kind of risk analysis allows us to systematically (even if not
algorithmically or mathematically) label different issues as risks and
identify factors that shape, magnify, or mitigate each risk.
Imagine risk analysis for a mobile phone switching system. It may be built
out of reliable components and have a historical 99.9999% uptime. Thus,
testing it under an astronomical load (10× or 100× normal) might seem
unnecessary ... until you realize that a cellular data network is both
especially important and especially vulnerable during natural disasters or
terrorist attacks. With international communication networks, worst-case
scenarios are far, far worse than "average" ones. We can't know the
likelihood of such so-called black swan 24 events in any exact way. We can't
even know all the impacts. No one could have known that all air travel in
Europe would be shut down for five full days because of a volcano erupting
in Iceland,25 but when that happened in 2010, it led to a unique and
stupendous load on airline reservation and logistics systems. James had to

reschedule a flight three times in a row before he could get back home, as
did almost every other passenger in that entire quadrant of the globe.
The nature of the social construction of risk is such that risks are taken
more seriously in the days and weeks immediately after major failures and
disasters. For this reason, testers would do well to maintain a good
scrapbook of examples of failures, preferably real-life ones that happened at
their own company. A vivid story about a problem, especially if it's a true
story, creates motivation, whereas months of undramatic success can fool
people into believing that there is no risk at all.
Brainstorming Using the Four-Part Risk Story
It can be hard to know where to start with risk analysis.
Huib Schoots has a simple suggestion for you: "Start with brainstorming a
simple list of any concerns you can think of."
One great heuristic for going a little deeper is the four-part risk story, which
reads like a tabloid headline: "Threat exploits vulnerability, causing
problem that hurts victim!" This is a story of product risk. Note its four
components:
Failure mode: The actual failure of the product, experienced by the
user/victim. Testing is all about trying to find these failures.
Vulnerability: The underlying nature of the product that makes the
failure mode possible. This is what developers call the "actual" bug or
defect. These can also be found by means other than testing.
Threat: The event or conditions that must occur to exploit the
vulnerability. This includes internal states but also user behaviors,
specific data, or events happening in the world. Testers imagine and
simulate threats.
Victim: The specific person or people who are impacted by the failure.
This may go beyond the immediate users of the product. Testers try to
think expansively about possible victims and how they should matter
to our clients.
You can use this to brainstorm risks.

Start by thinking about the product and imagining any kind of problem it
might have. For example, here is a first draft brainstorm from several testers
who considered a drone-based package delivery system:
- Weather conditions might threaten the value of the packages, 
cold/warm, rain/sun, etc.
- Deliver to incorrect address
- Device battery dies
- Autonomous navigation poses a big risk of disrupting traffic 
and hurting people.
 - If the packages are not delivered correctly, how will the 
packages be picked up and delivered to the right address?
- What about buildings with multiple floors?
- What if it runs into moving obstacles (children, animals)?
- Basements...
- Will it leave the package to the correct address?
- What if someone steals the package?
- What if it rains/or other calamities, will the product break?
- A car can runs over it.
- How does it deal with returns?
- Leaving the package outdoors unsupervised leaves it 
vulnerable to theft and poor weather conditions.
- If there is a block on the road, does the product still 
deliver it?
- What if it has a mechanical error on the journey (flat tire, 
engine failure, etc)?
- Leave the base with the wrong package?
- What happens if the address is not in the database (new road, 
closed road)?
- How does it deal with badly packaged things or damaged 
packages?
- Explosives / dangerous / illegal substance in package
- Illegal substances, drugs, etc.
- How does it deal with various quality complaints?
- Delivering completely the wrong thing.
- If it does not find the intended person at home, does it 
return to base with the package?
- What is its base? If one base is shut down, will it navigate 
to another?
- What if the product stops suddenly during the delivery 
process?
- The weight of the package
- Can it safely deliver all kinds of materials? (Glasses, 
inflammable articles or materials, etc.)
- Can it deliver living things?
- What happens if the machine stops running? Will it auto 
resume and deliver the packages as intended?

- Location: there is no proper infrastructure, for instance, 
roads
- Navigation: internet coverage in different regions may be 
poor
- Crime: goods may be stolen before being delivered
What you see here is the result of a few minutes' effort, based only on a
vague and simple product description. You can see that the list includes
questions, sentence fragments, and concerns about things that are not part of
the product. All that is normal.
To turn this into a usable risk analysis, try sorting the items into different
categories and combining like items together:
Vulnerabilites
- Device battery dies
- Leaving the package outdoors unsupervised leaves it 
vulnerable to theft and poor weather conditions.
 
Threats
- A car can runs over it.
- Basements...
- Explosives / dangerous / illegal substance in package
- Glasses, inflammable articles or materials, etc.
- Illegal substances, drugs, etc.
- The weight of the package
- No proper infrastructure, for instance, roads
- Internet coverage in different regions may be poor
- Weather conditions might threaten the value of the packages, 
cold/warm, rain/sun, etc.
- What about buildings with multiple floors?
- What happens if the address is not in the database (new road, 
closed road)?
- Rain or other calamities
- Goods may be stolen before being delivered
  
Failure Modes
- Disrupting traffic and hurting people.
- Runs into moving obstacles (children, animals)?
- Deliver to incorrect address.
- Delivering completely the wrong thing.
- Leaving base with the wrong package.
- Mechanical error on the journey (flat tire, engine failure, 
etc)?
  
Questions
- Can it deliver living things?

- How does it deal with badly packaged things or damaged 
packages?
- How does it deal with returns?
- If does not find the intended person at home does it return 
to base with the package?
- If the packages are not delivered correctly, how will the 
packages be picked up and delivered to the right address?
- If there is a block on the road, does the product still 
deliver it?
- What is its base? If one base is shut down, will it navigate 
to another?
  
Unknown
- How does it deal with various quality complaints?
The next step is to explore the vulnerabilities by asking about threats and
failure modes related to each one: (1) to explore threats by asking what
would make the product more or less vulnerable to them and what failure
modes they could trigger, and (2) to explore failure modes by asking what
threats and vulnerabilities make them possible and what vivid consequences
they might have on customers or the business. The point of the exercise is
to expand all the lists, especially the list of failure modes, which represent
the kinds of bugs you ought to look for as you test.
The better our mental models of the product, the users, and the context of
use, the better this kind of risk analysis can be. It will also be more incisive
if you do risk analyses on each subsystem or feature set of the product.
As the analysis goes on, risk areas will become apparent. These are kinds of
risk that call for particular test activities or an increased level of effort for a
particular part of the product. This process is all part of risk triage. You
could even call it risk summarization. For a flying drone meant to deliver
products, that might look something like this:
Power failure (suggesting the activity of power management testing,
hard landing tests)
Weather-related failure (suggesting environmental testing, trip abort
testing, hard landings, testing the geolocation of downed drones)
Loss of communication (suggesting WAN/Wi-Fi failover testing,
autonomous landing tests, manual piloting tests, telemetry integrity
testing, electronic interference testing)

Unsafe landing zone (suggesting obstacle avoidance testing, trip abort
testing, manual piloting tests)
GPS failure (suggesting failover testing to radio beacon, emergency
landing testing, manual flight control testing)
... and so forth
And so we go from a freewheeling brainstorming process to specifically
tailored investigations that we call test activities. In RST, testing is not
simply about touching each user story or specific feature with a happy path
and a bit of negative testing—but, of course, you will probably choose to do
that while forming a more specific strategy. We call that function testing
(testing what the product should be able to do) or claims testing (testing
every claim made about the product, such as what is written in user stories).
Function testing and claims testing address a broad band of risk in a shallow
way. The purpose of systematic risk analysis is to optimize—that is, to
decide whether to test a particular component, function, or claim more
deeply or more shallowly.
Again, our first iteration of risk analysis rarely encompasses all the risks
that eventually reveal themselves. For instance, in one real case of a flying
drone delivery system, the drone failed to lift off from the top of its
launching system. However, the system thought that the drone was aloft and
began closing a hatch that could have crushed the stricken vehicle. Only the
intervention of an alert operator saved it. When we experience such
problems in testing, we become aware of previously hidden risks. In this
case: the risk is that the system loses track of its own state.
The hardest time to analyze risk is before a product has been built. The next
hardest time is before it has been released for the first time. Thus, Huib
reminds us that risk should be discussed in refinement sessions, not just
during initial planning.
Putting It All Together: Lenses of Testing
A lens is a way to bring the world into focus. Hence, a lens heuristic is any
heuristic that helps you encounter the world and make sense of it. In RST,
we like our lenses. Just as with an optical lens, you can point a lens
heuristic at anything and use the heuristic to interpret what you have there.

To use a lens heuristic, connect the elements of the heuristic to the
corresponding elements of the part of the world which you want to decode.
The HTSM (see Appendix B) is an example of a lens heuristic, and so is the
ART Grid (refer to Figure 4-4). You can apply them not only to entire
products and projects but also to any part of a product or project.
In the Appendices, you will find other lenses, including the Heuristics of
Testability (Appendix C), which help you analyze the testability of any
product or component you like, and the Elements of Excellent Testing
(Appendix D), which is a way of thinking about the skills of testing.
As shown in Figure 4-7, the Elements of Excellent Testing are structured
around the "gears" diagram. We used the gear motif because when we
conceived of the skills of testing, they seemed to decompose into different
aspects that interacted with each other via the mechanism of thinking
(analysis).
Each gear represents something dynamic: the processes of analysis, your
growing knowledge (and lessening knowledge, too, such as whenever your
product is updated and you don't know if past test results still apply), your
experiences with the product, your dealings with other people, and your
story—that evolving interpretation and framing of what has happened and
what will happen next. The rectangles represent artifacts, including
software, data, and documentation. These crystallize out of the processes.
Or we can conceive of testing on the level of an evolving test activity, as in
the Rapid Testing Framework (see Appendix A), which is like a swirling
vortex (more messy than a spinning clockwork). As shown in Figure 4-8,
the Rapid Testing Framework centers on the people who do the testing, as
they develop tests, perform them, and learn from the results.
Each of these diagrams describes exactly the same process. It's all testing,
but at different levels and with different filters.
The gears diagram emphasizes how analysis performed by the tester takes
in evidence and transforms it into a story for the benefit of our clients. The
vortex diagram emphasizes the humanity and incremental nature of each
test activity. The ART Grid emphasizes the relationship between testing and
development mindsets and priorities. The HTSM emphasizes ideas that
guide specific tests.

Figure 4-7: Key elements of Testing Skills and Tactics

Figure 4-8: The Rapid Testing Framework "Vortex"
Lens heuristics are most useful in helping us see what is not there. For
example, the vortex diagram suggests that every test activity involves
iterative learning. So, if we don't feel as though we are learning, something
is wrong with the process (or else the lens is wrong). Ask yourself why you
might not be learning during a test activity. That could be because you feel
that all the experiments (the tests) are already designed and all you need to
do is perform them. Perhaps, even though there is more to learn, you feel
that you have learned enough. This may be true and good, or it may be that
you are tired and disengaged. Periodically recenter yourself or get fresh
eyes on the process.
The fact is, there is always more to learn. You could optimize your process;
you could switch up the test data; you could try a new tool. It's not always
worth doing any of those things, but don't fall asleep to those options.
Be a Problem-Solver, Not a Task-Doer
The core idea of RST is agency. Having agency means all the difference
between responsibility-free childhood (assuming someone else will tell you
what to do) and responsible adulthood (you decide, and you accept the

consequences of your choices). It is possible to have a testing job, and even
please your employer, without agency. But that will never be good testing.
You won't truly be a tester. At most, you will be a glorified test tool.
"Testers" like that are easily replaced by AI chatbots.
We recommend that you apply the lenses—or make lenses of your own—to
help yourself monitor and ponder the status of your testing and the ideas
that guide your specific tests.
A Strategy Example: Bibliography Generator
Let's walk through a process of conceiving a strategy for testing part of a
product.
The product under test is one that generates formatted bibliographies for
academic journals (see Figure 4-1).26 The user supplies citations in the form
of records in a database with various fields filled in: authors, date, title, etc.
Then the user selects a bibliographic style, and the product emits a
bibliography with the user's citations formatted according to the chosen
style.
Many academic journals have specified their preferred styles and stored
them in a central repository. Anyone wanting to publish a paper in a
particular journal will look for the style in the repository that corresponds to
that journal. There are 10,532 of these styles in the repository.
Our mission is to specify an efficient strategy for testing the formatting
capability and reliability of this product, given reasonable assumptions
about its users and uses.
Context
Of course, we aren't on the actual project. Therefore, certain starting points
are closed to us. We can't talk to the developers, and we don't know what
other testing has been done on this functionality. We also know nothing
about the economics of this project.
A version of the product is available to us, so we can explore and analyze it
to some degree.

Strategy
In this section, every phrase that specifies test strategy is bolded, and we
have tried to trace each strategic decision back to the starting points we
mention above, as well as other strategy heuristics that apply.
First, because in this case we have the full working product available, let's
engage the product by experiencing it and exploring its features. In
RST, this is called "survey testing" and is an example of using the "What
is the product?" and "What testing is easy to do?" starting points mentioned
previously. Of course, if we didn't have the full product, we would still do
survey testing on each feature or implemented user story as it becomes
available. And we would revisit survey testing as part of regression
testing, whenever any new feature is added.
There is a risk that the bibliography generator will incorrectly process
one or more of the formats ("What are you here for?" and "How important
is your testing?"). It's obvious how we could test for that kind of bug: try
all 10,532 formats with a wide range of different citation data, then
look for any mistakes in the output. Doing this would require exploring
the many different possible kinds and combinations of citations (citing
a painting in a museum collection is different from citing a book, which
is different in turn from citing a journal article). It also requires
knowing how the correct bibliographic style should look.
That's a lot of work! ("How is your testing constrained?") Maybe there is a
simpler way?
Most people immediately think of popularity: test with the most popular
formats ("How will people most likely use it?"). That requires some
research ("What do you need to learn?"), and it's not clear how to go about
that research. Do we ask random college professors what journals they
prefer? Another approach to research is to learn what we can about the
system and see what clues about popularity might pop up ("What is the
product?"). In this case, even a cursory look at how styles are defined
would quickly reveal that 7,877 of styles are merely pointers to other styles.
They are called dependent styles. In other words, there are only 2,655
distinct styles to test. And if we look at which styles are the most
popular among the dependent style, we find that 90% of the dependent
styles point at only 53 styles! That means we can get the equivalent of

about 68% style coverage by testing only .5% of the styles (cost vs.
value heuristic).
Further reading of the style specifications reveals that there are five major
kinds of styles (test with representatives of each kind). Examination of
the app shows us that there are four default styles (test all defaults). We can
see which styles are oldest and newest (test the oldest because they are
likely to have outdated content; test the newest because they are likely
to have untried content). We can see which ones are biggest and smallest
(test the biggest because they are complex; test the smallest because
they are probably quirky).
The CSL style language is an XML format, meaning each style specifies
tags that may have attributes. An analysis of all the independent styles could
show us all the combinations of tags and attributes that are in use. We could
construct a list of styles that would cover all those combinations. ("What is
the product?" and "How will people most likely use it?")
When James created his strategy for this product, he ended up with a list of
99 styles to test, which is a lot, but not overwhelming, especially when
coupled with a bit of automation to collect screenshots of the final
bibliographies. They needed to be scrutinized carefully only once. After
that, he used file-compare to detect any changes.
Let's give a name to that test activity. Call it format testing, as distinct from
sanity testing, data entry testing, compatibility testing, or citation lookup
testing—all of which may constitute the overall test strategy for this
product.
The construction of this strategy involved exploration and inspection of
specifications, data structures, and the GUI. It was an iterative process that
took advantage of certain avenues of testability (such as James's knowledge
of HAR files and JSON data-wrangling tools). It involved consideration of
cost vs. value, redundancy vs. variation, normality vs. extremity, etc.
For instance, we can test this product without necessarily trying any of the
styles in the list. We could artificially construct a variety of crazy styles to
assess how well the product interpreted the CSL style language. If those
styles produced no problems, surely the more "normal" styles would be
handled well (that is called the Perimeter Assumption heuristic). James

rejected that idea because it was too expensive. It would be a lot of effort
and time to create those devilish styles.
The result of James' testing was that he found a single problem (an error
message displayed when trying to use one of the smallest style formats).
Then, because he felt curious about it, he ran automation that tried all ten
thousand styles ("What testing is fun to do?"). Trying them was easy; the
hard part was the evaluation. For that part, he used a "blink" oracle, which
involved creating a movie of all the screens and watching the video as
they all flashed briefly on the screen. With this method, he found thirteen
more failures! These included two crash bugs, a few error messages, and
gross formatting problems. This strategy is bound to miss subtle bugs, of
course. Then again, we must consider cost vs. value: Maybe it's not worth
the effort to find bugs that are hard to notice in a bibliography. We're not
talking about medical equipment, after all. Maybe the user won't mind the
occasional little glitch.27
The fourteen bugs, once investigated, would allow us (and our clients) to
better understand the vulnerabilities of the product. We could then refine
—and, if necessary, renegotiate—our strategy to better focus on risk
("What are you here for?" "What do people expect you to do?" "What is
happening right now?").
We think it was worth using this more exhaustive strategy at least once, and
then perhaps reverting to the more optimized strategy, especially if we used
random sampling to keep the small set of styles fresh .
Strategy Example #2: A More Normalized Version
Strategies can be documented in many ways. The preceding example is
more organic and conversational. Huib Schoots likes to use a more tabular
approach, which works better as a reference and in planning and status
meetings.
He suggests a table with three columns: product risk area, test activities,
and activity type. The first two columns are fairly self-explanatory: the
kinds of things that we are worried about in the product are set next to the
kinds of testing we must do to discover those problems. The third column is
a little different. It's an aid to planning. An activity type, in this kind of

table, refers to a distinct time, place, or personnel who will carry out the
activity. Thus, a particular test activity, such testing the correctness of a
certain bibliographic style, might be part of primary function testing, which
is done as soon as the product is available for a tester to look at for the first
time, but it may additionally be handled (probably in a different way) in
some other activity type, such as regression testing. The people who do the
primary testing may be different from those responsible for regression
testing. It's important to identify these kinds of overlapping activities to
make sure that nothing inadvertently falls in the cracks between them.
The following table is not a full test strategy for the product, just the same
subset of the strategy discussed previously—the same content, more or less
—rendered as a table.
BIBLIOGRAPHIC FORMAT TEST STRATEGY
Risk
Activities
Activity Types
Any kind of
problem
Survey Testing
Coverage focused exploration
to find out what's in the
product, how it works, how it
might have changed, and where
the risks might be
Product Learning
Preparing ourselves to
test well
Primary Functional
Testing
As features are
implemented
Regression Testing
Targeting changes
Any format-
related
problem
Optimized Style Set Curation
- Focus on independent
styles
- 53 most popular
independent styles that
Primary Test Tooling
Tool to analyze
popularity of styles; tool
to explore styles data;
tool to calculate minimax
of tag-attribute coverage

BIBLIOGRAPHIC FORMAT TEST STRATEGY
cover 90% of dependent
styles
- 4 default styles
- Representatives of 5 major
style types
- 3 oldest, 3 newest
- 3 largest, 3 smallest
- Minimum set of styles that
achieves maximum tag-
attribute coverage
- The above can be achieved
with 99 styles (out of
10,532)
- IF PROJECT NEEDS
HIGHER STANDARD:
Generate styles artificially
that use and stress all
features of the CSL
language.
Primary Functional
Testing
Incorrectly
formatted
bibliography
Citation Selection
- Select citations that cover a
requisite variety of artifact
types
Bibliography Output Review
- Experiential review of
optimized style set (99
styles)
Primary Test Tooling
Playwright script to
download screenshots
and screen scrapes of
style sets; FFMPG and
MPVIO to generate and
view video for blink
analysis
Primary Functional
Testing

BIBLIOGRAPHIC FORMAT TEST STRATEGY
- Blink testing of
bibliography and notes
displays via flip deck video
of all 10,532 styles
- Regression checking via
change detection on
optimized style set
Regression Automation
Activity Types Mentioned in the Table
Product learning: Anyone who systematically tests the product, other
than the ones who created it, must build a mental model of it first.
They also build models of potential users, uses, risks, etc. Learning the
product is usually much more than merely reading user stories or other
specifications.
Regression testing: Performed after significant changes to the
product. This includes the use of automation, but is more than merely
running the automation. Regression testing includes new testing in the
case of investigating the impact of new features on existing features. It
also includes investigating failures. This activity may be performed by
someone without coding skills.
Regression automation: Automated assay or checking in support of
regression testing requires a coder to perform reliably and efficiently.
This kind of automation takes time above and beyond primary testing
to prepare and requires ongoing work to maintain.
Primary functional testing: This is performed by a tester who has the
time to do focused investigation of the product, rather than by the
developer in passing. By "functional," we mean focused on user-facing
functions (e.g., features, stories, capabilities) as they are implemented.
Primary test tooling: This is generally performed by a coding tester,
but AI can be a great help. Primary test tooling is automation used to
aid the development of test data and procedures, rather than the typical
"test automation" that almost everyone talks about, which is better

referred to as automated checking or user-simulation automation.
Often, tooling produced for primary testing is helpful for regression
testing, too, but not necessarily.
Note that there are other test types that are not relevant here, but that would
appear in a full test strategy. These might include: primary system testing
(experiencing the product from end to end to evaluate its fitness for use),
usability testing, compatibility testing, accessibility testing, performance
testing, and so forth.
What About Oracles?
Putting the strategy into a normalized form, such as the preceding table, can
help us notice things that are missing from it. In this case, there is almost
nothing said about oracles: How will we recognize that a formatting
problem exists? Maybe a format looks okay to casual inspection, but does
not match the print format of the associated journal.
Solving that problem may take more energy and time than we have to
spend. But it's the sort of thing that should be discussed if we want a high
standard of quality.
Our take on it is that subtle bugs in style handling might be better found and
reported by the users themselves, if we can engage the user community.
Each journal has an interest in helping us, after all, since this tool helps
their authors. We could host an online testing party, for instance, or hold a
testing contest. We could establish a clear reporting process and be
responsive to all reports.
Final Thought
If you look back through these two example strategies, where do we refer to
test cases? Nowhere, explicitly. You could consider that each bibliographic
style we chose to test constitutes a sort of "case" that we cover, but it's
completely unnecessary to put them into a test case management system,
much less write 99 sets of almost identical test steps to go along with the 99
styles.
We do speak of test ideas and test activities. But designing a test strategy is
not the same thing as writing test cases. Never confuse those things.

Notes
1   Clausewitz, C. von, Howard, M., Paret, P., Brodie, B., West, R., and
Clausewitz, C. von. (1989) On war First paperback printing. Princeton,
NJ: Princeton University Press.
2   Popper, K. R. (1989) Conjectures and refutations: The growth of
scientific knowledge. 5th ed. (rev.). London: Routledge.
3   Remember, testing includes checking, but it is more than checking.
Testing includes all the thought processes that create, evaluate, and
moderate checking.
4   Popper only spoke of "truth is manifest" to criticize the notion and
introduce his philosophy of science. RST borrows much from Popper's
approach.
5   This is one way the Rapid Testing Methodology evolves: we stumble
over a seemingly simple idea, discover that it's not so simple, then make
it a bit more explicit so that we can test more deliberately, and teach
testing more effectively. This is what studying testing means to us.
6   A subtitle heuristic is a suggested substitution of one phrase for another.
Here's one from Tim Lister: "high-level" in software projects usually
means "not really." For example: "that's our high-level plan" becomes
"that's not really our plan."
7   ISO 14971:2019. (n.d.). Medical devices—Application of risk
management to medical devices. ISO. See also Directive (EU)
2024/2853 of the European Parliament and of the Council of 23 October
2024 on liability for defective products.
8   Cryle, P. M. and Stephens, E. (2017) Normality: A critical genealogy.
Chicago: The University of Chicago Press.
9   Ibid.
10 This is known as "reasonably foreseeable misuse" in medical device
testing, which is not the same as "unintended use."

11 "or editors," noted the editor of this book.
12 Suggested by Joe Sevits, during one of James's testing classes, this
fourth mode led James to think of all these as "seasons." In fact, much
evolution in RST has been instigated by student ideas and questions over
the years, although we failed to record most of their names.
13 Storybook.js, for the record.
14 Figma, in case you are curious.
15 Note that all testing is exploratory to some degree. By naming the event
"exploratory testing," Lalit was emphasizing the importance of variety in
test coverage, tester engagement, and the mechanisms of serendipity.
Without these things, testing suffers from malnutrition.
16 Based on the Product Factors list of the Heuristic Test Strategy Model in
Appendix B.
17 See Chapter 9 for examples of mind maps.
18 "Experience" is relative, of course. The level of knowledge or seasoning
needed to make use of guide words is whatever level causes a particular
guideword to light up your mind with ideas and relevant memories.
19 Joint Task Force Transformation Initiative. (2012). Guide for conducting
risk assessments. NIST SP 800-30r1. Gaithersburg, MD: National
Institute of Standards and Technology. Available at:
https://doi.org/10.6028/NIST.SP.800-30r1.
20 Two examples are ISO13485 and the proposed Duty of Care Risk
Analysis Standard (www.docra.org).
21 Joint Task Force Transformation Initiative (2012) Guide for conducting
risk assessments. NIST SP 800-30r1. Gaithersburg, MD: National
Institute of Standards and Technology. Available at:
https://doi.org/10.6028/NIST.SP.800-30r1.
22 Apostolakis, G. E. (2004) "How useful is quantitative risk assessment?,"
Risk Analysis, 24(3), Article 3.

23 Slovic, P., Finucane, M. L., Peters, E. and MacGregor, D. G. (2004)
"Risk as analysis and risk as feelings: Some thoughts about affect,
reason, risk, and rationality," Risk Analysis, 24(2), Article 2.
24 Taleb, N. (2007) The black swan: The impact of the highly improbable.
New York: Random House.
25 Wikipedia (n.d.) 2010 eruptions of Eyjafjallajökull. Available at:
https://en.wikipedia.org/wiki/2010_eruptions_of_Eyjafjallajök
ull.
26 As of this writing, that product is called Zoterobib at https://zbib.org.
27 Note that all fourteen problems found in this process were in unpopular
styles.
OceanofPDF.com

CHAPTER 5
How to Account for Your Work
—By James Bach, Michael Bolton, Huib Schoots, Jon Bach, and
Aaron Hodder
In RST, responsible tester is a term of art. One of the differences between
responsible testers and people merely doing some testing is that responsible
testers are accountable for the quality of the work. Responsible testers
commit to explaining and justifying what they did to test.
The ultimate goal of responsible testers is to serve their clients by
delivering necessary information effectively, ethically, and accountably,
transforming assumptions into facts. Meanwhile, there is a secondary goal:
we need support for our work as testers. Our clients won't support us unless
they think we are doing good work.
Both of these goals require storytelling. When telling the story of the status
of testing, we call this test reporting. When telling the story of why you are
following a particular test strategy and how that matters to the business, we
call this test framing.
We wrote a little about stories in Chapter 2, "Foundation." This chapter
goes into more detail.
Telling a Compelling Story
To tell a story that reasonable people will listen to—then act upon—our
prime heuristic is to answer these questions: "What's up? Says who? So
what?"
What's up? This question is about what is special and different today
from the ordinary run of things. The key difference between all the
data in the world and a story is curation. You make choices about what
is relevant and what to ignore. Part of relevance is what your audience
needs to know; another part is what your audience already does know.
The difference between those two is the story you must tell. Note that

it can be good to tell people some of what they already know—it helps
set context and synchronize your minds to give and receive new
information.
But they will quickly tune out if they think you are belaboring the
obvious. Therefore, at the risk of belaboring the obvious: don't
belabor the obvious. How? First, you must know a lot about what
your audience knows, as well as what they think they know. You must
be appropriately socialized in their "form of life" (i.e., relevant work
culture). Second, if what they think they know is wrong, and you
want to dispute that, then you may have to make your case with
special care and emphasis; otherwise, they may hear only what they
expect instead of what you actually say.
Says who? In other words, why should anyone believe this? This
question is partly about personal credibility and partly about evidence.
Without credibility, no one will hear you. Some credibility is based on
unconscious biases and other unfair mechanisms. Some is not under
your control, or only partly controlled, or tied up with your culture and
background (e.g., age, voice, posture, or the way you dress).
Fortunately, you can do a lot to create credibility for yourself if you
put in reasonable time and effort. That's partly why credibility takes
time and effort: those things demonstrate your seriousness.
We all get a certain amount of "credibility credit" that we don't earn.
We all build on it and try to avoid destroying it (because building it a
second time is way harder than building it the first time). The authors
believe the testing industry has for decades systematically eroded its
credibility by promoting "best practices" that have weak foundations
in systematic research or lived experience, tolerating tester
certifications that almost anyone can easily acquire (except a
thoughtful tester who answers questions truthfully), and by
emphasizing paperwork instead of investigation. Now there's a new
threat: AI, which some people are using to fake their testing.
Are we just ranting here to make ourselves feel better? No, we are
using a rant to call you to action. If you love testing as we do, please
be a part of raising the credibility of our craft. Build your skills.
Celebrate good work. Resist mediocrity. Speak out against bad

certification schemes. Credibility is the closest thing we testers have
to job security. Therefore, protect your credibility with your
professional life.
So what? It's not enough to convey credible information. Your
audience will not pay attention unless it matters to them. They have
decisions to make. How does your story help them make a better
decision or feel better about the decisions they have made? What
makes an effective test report is partly about what your teammates and
managers are trying to do right now, so you had better understand what
that is. On the eve of a release, don't moan about bugs that are
definitely not important enough to stop the train. Get your priorities
straight.
That much is general advice about storytelling. Now let's apply it to
software testing. The three questions—What's up? Says who? So what?—
correspond to what we call, in RST, the "three-level testing story."
Level 1: The Product Story: The first level is really why your clients
hired you. It's the answer to the question "What's up with the
product?" They want to know about bugs. A little more broadly, they
want to know about the quality status of the product: what it is, what it
does, how it works, how it doesn't work, how it might not work in
ways that matter to our clients and to their clients. There are many
aspects to this, but never forget that bugs and the prospect of bugs are
your special focus as a tester.
Level 2: The Process Story: The second level of a test report is all
about how you did the testing, how you produced that level one story.
Broadly speaking, it's the answer to the question "Says who?"
including what we've done to configure, operate, observe, and evaluate
the product. This process story puts the product story into perspective
and gives it credibility. Even if your clients don't want to sit through a
list of the specific tests that you've performed, they need to know
generally where you have looked for problems and how thoroughly
you have looked. Whether or not you are asked to provide the process
story, you should always be ready to tell it.

Level 3: The Value Story: This is all about "So what?" Any
discussion of product risk, remaining unexamined risk, or expression
of confidence or caution about the test process is part of level three.
Level three is where you go beyond merely describing the testing you
did, but also explain why you did it that way, instead of some other
way. Here's where you frame how it supports the business. Your clients
are entitled to know why they should be pleased with your work—or
concerned about it, where concern is warranted. And for your part, it's
in your own best interest to report on issues and obstacles that could
compromise the testability of the product. You must be prepared to
explain why the testing work you've done is the best reasonable work
you could have done—and to the degree it wasn't, why it wasn't.
A full testing story presents evidence that affords assessment of the
situation. Although some people argue that metrics are the only rational
basis for making judgments, listen to their arguments, and you'll discover a
huge equivocation: nearly all the reasons to collect metrics are actually no
more than reasons to collect evidence. We can 100% agree that evidence
matters without necessarily using numeric scales to express it! For instance,
removing "100%" from the previous sentence would not impair its value or
meaning in any important way.
A full testing story is a product story, process story, and value story all
together. But the testing story you give in any particular situation may not
necessarily involve all of that. What parts to include, and the depth and
detail you provide, are a judgment call, but the art of storytelling lies largely
in what you leave out.
Here's another heuristic: know your audience, what they need, and what
they think they need. Then avoid giving them much more than they think
they need—but not less than they actually need. Anticipate the questions
they will ask.1 If you have more than one audience, and each audience has
different needs, it's probably better to make different documents (or
presentations) for them, respectively, rather than one super document.
Above all, don't just dump everything you know into your testing story
because you think somebody somewhere might possibly need it. That
creates a scavenger hunt for your readers, as they sift through the chaff to
get to the wheat. Many will give up before seeing the information they

might have come for. Ironically, trying to please everyone with one thick
doc that serves as a report, reference, and tutorial for newbies can guarantee
that no one will read or hear any of it.
Level one of the testing story is all you need if the context of the report is
clear and your credibility is not in question. For example, if you just got
started testing and you find a bug, you might say, "I found a bug!" and then
describe the bug. That can be a perfectly good level one test report. You
don't need to say, "I found one bug by carrying out the such-and-such test,
and I still have a lot more work to do before I will have completed my test
run," because the developer already knows the context and maybe doesn't
care about how you tested at that moment. Just report the bug.
You also might combine the levels in special ways, in different contexts. If
you say, "I found these bugs, but I'm only really scratching the surface of
the product, and there are probably a lot more to find," then you have
combined levels one and three. If you say, "I'm having trouble doing stress
testing, and we need to sort this out, or else the release date is at risk," you
are dealing in levels two and three.
Note-taking Is a Core Testing Skill
Since testing is an ephemeral experience, it's easy to lose track of the testing
story from hour to hour and day to day. It's easy to find yourself looking
back and trying to remember, "What did I do all this week?" Test notes can
help you reconstruct that history to a reasonable and sufficient degree.
Sometimes people point out that test cases and procedures make note-taking
less important, because they already constitute the story of testing. That's
often a dangerous assumption, but even if we agreed with it, that would
only apply to regression testing, not primary testing. During primary
testing, you either don't have predefined and documented test procedures, or
the test procedures you do have were created by someone (maybe you) with
little actual experience and knowledge of the product (since they must have
created them from the documentation and not from using the product).
Besides, test notes are not just a recitation of execution trivia, such as what
buttons you pushed. They should address questions of test design, such as:
What did you cover? What did you discover? Why did you choose to test

this instead of that? Why did you choose this test technique or that test
data?
In his early experiments with "formalized" exploratory testing, James
noticed that it was difficult to take notes and test at the same time. If he
wrote a lot of notes, it would interrupt his testing. If he was fully in the
moment with the testing, he'd forget to take notes. To help himself, he wrote
a program to throw up a dialog box every 10 minutes with the question,
"What are you doing now?" The tool had a fascinating side effect: he
noticed after a couple of hours that his brain began to alert him just before
the 10-minute clock had lapsed, as if to say, "The dialog box is about to
appear, get ready!" Once that started happening, he no longer needed an
alarm to remember to keep notes.
The moral: note-taking is a skill that benefits from deliberate practice.
There are two big styles of note-taking: bullet-style and flight-data recorder
style. Bullet-style notes list the kinds of things that happened and examples
thereof. Recorder style is a play-by-play sequence of events. It seems to the
authors that recorder-style note-taking takes more energy and focus, both to
write and to read. This may be a matter of personal preference.
Notes may be very lean, quick, and messy, to the point where no one but the
author can understand them—and even the author may not be able to fully
interpret them a few hours from now. We call them scratch notes. Scratch
notes can be just fine for a little while. They are compact and quick to write,
in whatever format you choose. James prefers plaintext notes. Michael likes
to take notes using mind mapping tools.
Archival notes are a different animal. Archival notes should be adequate to
refresh your memory of what happened, months later, if necessary, and they
should allow a reasonably prepared outsider to visualize the kinds of testing
that you did. The trick with good notes is to create them without overly
disrupting your testing. So you might start with scratch notes and then take
15 or 20 minutes at the end of a long test session to rewrite them into
archival notes. It may also help to use tools that automatically record what
you did.
Drawings often help, too. There are many excellent tablet applications that
allow you to make notes with embedded diagrams. And don't forget

screenshots, although some testers keep screenshots of everything, which
can make notes harder to read.
Be careful about using AI to write test notes for you. AI summaries
frequently leave out key information, even though they look good at a quick
glance. If we were using AI in that way, we would want to preserve
recordings of the actual testing as a backup, and use the AI summaries to
make those recordings highly searchable.
Advocates of highly formalized procedural test cases claim that highly
exploratory testing is a problem because "You can't prove what you did."
We respond that they have no proof of that. In our testing, we have good
notes and other forms of evidence to support our claims about how we
tested. Meanwhile, the people who say they follow procedural test cases
often only have their word to back it up. Who knows what they really did?
Jon Bach's PROOF Heuristic
Speaking of PROOF, Jon Bach developed a pattern for a status report that
has become popular with RST practitioners.
Past: What happened during the testing? This is level 2.
Results: Level 1. What was discovered about the product? What bugs
were found?
Obstacles: Level 3. What got in the way or slowed things down? How
was the product especially difficult to test?
Outlook: Level 2. What's coming next? What remains to be done?
Feelings: Level 3. How do you feel about all this? Are you on top of
it? What are you worried about?
PROOF is a nice narrative structure when you need to make an impromptu
test report in a meeting, or when you are reviewing someone else's report,
and you are trying to see if a part of their story might be missing.
Safety Language
When you tell a story as a tester, accuracy matters more than it does for
other people. Our business is truth-telling, after all. One aspect of telling the

truth is to faithfully represent our uncertainty about the facts of the matter.
We call phrases that help us do that safety language. Examples (in italics)
include:
"The server seemed to hang."
"It appears to work, or at least I have not yet seen a failure."
"I assume that the product has been properly configured."
"Apparently, those are the only features it has."
These are qualifying words and phrases meant to warn ourselves and others
of potential sources of error in our beliefs. This way of talking is also
known by the fancy name epistemic modality. (You can look it up on
Wikipedia!)
Using safety language is not just about communicating accurately to other
people; it's also a matter of personal discipline. Forcing yourself to use
qualifying words reminds your own brain that you might be wrong. If used
well, safety language promotes mindfulness.
Some clients call safety language weasel words and associate it with
unwillingness to be pinned down or to take responsibility. They may think
you are trying to be evasive instead of prudent. Just be aware that if you
eliminate safety language, you run the risk of inadvertently promising that
which you cannot possibly know. You will make testing sound much easier
or more certain than it is.
Telescoping Reports
When you report to your clients, it's possible that they are deeply interested
in testing and want to hear all about how your work—but that is vanishingly
unlikely. It's much more likely that they have work to do and need certain
information from you in order to do it. They are looking for that specific
information in your written or oral communication. So, make it easy for
them to find what they need. They may also need supporting data to answer
questions that emerge from your report. Make that easy for them to get on
demand, but don't push everything at them all at once.
Here's the general priority:

1. Is the product okay? Yes, no, or don't know? If yes, or you don't
know, move on to item 2.
If no, then what are the problems that make it not okay? Convey
them in rough order of importance.
If you are seeing important product risks, independent of any
specific bug report, list them.
If you are seeing the product getting markedly better or worse,
recently, say so.
2. Is the testing okay? Yes or no? If yes, then move on to item 3.
If no, then what is blocking or threatening the test process?
If you are already coping well enough with these problems, say
what you are doing.
If you need help from your clients to deal with these problems,
say what you need.
3. Is there anything else your clients need or want to know? If not, you
are done.
Be prepared with supporting data they might ask for.
This is a style of reporting that we call telescoping, but it might also be
called folding or progressive reports. In written form, it could be a story that
begins with a brief (one- or two-sentence) summary that gets the major
point across. Follow it with more details that add up to a paragraph or two
explaining that first sentence. Then follow that with sections constructed so
that the audience may stop reading or listening at any time, and they will
have achieved a reasonably complete report (although with different levels
of detail) no matter where they stop.
Telescoping reports are designed to help the audience decide for themselves
how much they need to hear or read. Actionable judgments are always at
the start. Wonkish details are always pushed to the end.
The low-tech testing Dashboard, described in the following section, is a
telescoping style in one respect: it's designed so that someone glancing at it
for a few seconds at a distance of 20 feet can get a sense of the status of the

testing. If you want a better sense, come closer and look longer. If you want
an even better sense, point at any cell in the report and ask the tester for
supporting evidence.
James's Low-Tech Testing Dashboard
James was once in the midst of an (apparently) rambling test report when
his boss, the VP of software engineering,2 interrupted him.
"I don't have time for this," he said. "Can't you just put up a summary of
some kind on that whiteboard so I can glance at it each day?" As he spoke,
he made vague, squarish gestures with his hands that seemed to represent
rows and columns.
An hour later, James had created an ambient kind of report (sometimes
called an information radiator), which he came to call the low-tech testing
dashboard. When transcribed to electronic form, it looked like Figure 5-1,
which originally used colors, which you can't see here.

Figure 5-1: A low-tech testing dashboard
The VP loved it because it allowed him to home in on important questions
that he should be asking the team.
Across the dashboard, James used colors that had these general meanings:
Red: "The team is worried about this. It may threaten our release date.
Management should get involved."
Green: "The team feels especially good about this. Either we've met
the goals associated with this, or we've nearly met them."
Amber: "The team cannot agree about whether this should be red or
green. There is controversy. Management might want to check in on
this."
Black: "The team feels that the trouble level of this item is neither
particularly good nor particularly worrying. Progress is ongoing."

(Black was not used in the quality column; only green, amber, red, or
the cell was left blank.)
The Effort column was about the current staffing level for an associated
area of the product (e.g., "none" meant no tester was assigned to it). The C
column was a summary of test coverage according to qualitative levels
(e.g., 3 meant excellent coverage, with complex data and flows). The Q
column stood for product quality, and was written in different colors
according to the color chart (except black). If we hadn't done enough testing
to form an opinion, the Q column for that area would be left blank.
In the original whiteboard version, James used face icons only because the
whiteboard pens quickly ran out of ink when solid circles were used. (He
didn't think of using colored Post-its at the time.) When presenting this data
electronically, however, face icons are hard to distinguish, so we would not
recommend using them.
As the testing was completed, the colors of the Area, Effort, and Coverage
columns turned green, indicating that we were done. So, for instance, if we
only needed a sanity level of testing for an area, we made it green when we
reached a coverage level of "1."
Notice that there are no hard metrics in the dashboard. Hard metrics must
be interpreted. The key fields of the dashboard are all subjective judgments.
They are not arbitrary judgments, though. To do the dashboard right, the
tester must stand ready to substantiate those judgments with reason and
evidence.
Over time, the team built up a protocol for updating and interpreting the
dashboard. Our intent here is not to go into all the details and heuristics, but
rather to show you an example of how a testing story can be presented
ambiently via a frequently updated dashboard. The authors have produced
many different versions of such dashboards over the years, and they can be
an excellent substitute for a long-winded oral or written report.3 Anyone
who approaches it can decide to accept it at face value, treat it as a
summary, or drill down on each cell.
Our point is that even if you don't write a document with the title "Test
Report" at the top, in some stentorian font, you are probably giving frequent

feedback to the team, and they are reacting to it. That is test reporting. RST
recommends that you make the simplest reports that serve your context.
Stories Mediate Software Projects
In Chapter 4, "How to Do a Test Strategy," we spoke of lenses, which are
heuristics that help you bring the world—or, in this case, the test process—
into focus. The purpose of a lens is ultimately to help you assess and
navigate your situation. The lenses of Appendices A and D prominently
feature testing stories. This is because stories are the basis of all
management, including self-management. Barry Oshry, in Seeing Systems,4
emphasizes the danger of "making up stories" that lead to the breakdown of
trust in organizations, even though we don't think we are making up
anything, saying, "we see our stories as truth." Nevertheless, his book is not
about abandoning stories, but rather about developing what Oshry calls
spatial sight that allows you to have access to better stories about the
behavior of people in organizations.
Here is how we see the basic process of management:
1. Periodically construct (or discover) the story of the status of the thing
you are managing.
2. Evaluate the story. ("Does this seem like a true story about a well-
managed thing?")
3. If the story is good enough, stop here and go do something else.
4. Otherwise, identify something you could change that would improve
the story.
5. Make that change.
6. Repeat the preceding steps.
This process fits for managing anything: a meal, a child, an adult, a
business, or a test project. In the gears diagram (see Appendix D) and in the
testing vortex diagram (see Appendix A), you can see the pattern of this
process. In fact, all the lenses in the appendices are designed to help you
give structure and content to a testing story.

So the testing story is not just an output—it's a dynamic instrument. It is a
medium by which we decide what to do next.
Clearly, how we do testing affects our test reports. But how we report also
changes our testing. Reporting profoundly affects how we think about
ourselves, how we relate to our work, and how our clients relate to it, too.
In Chapter 2, you might have seen the section "Roles and Trading Zones."
We will now expand on that.
The inside of a test process looks different than the outside. Some kinds of
work are easy to explain. This is not so for deep testing. Even dear friends
who love testing find it difficult to follow each other's anecdotes about bug
investigation or the details of composing a decision table from a
specification. There is an element of "you hadda be there" in the moment of
performing a test.
But that's okay, as long as there is enough shared tacit knowledge. We need
an understanding and appreciation of the test process, mental models of the
technology and project, some experience, and values—all held in common,
more or less. Let's call that alignment. With high alignment and a basic
willingness to listen on the part of your client, you can manage expectations
and get the support you need. You will do this by sharing the general testing
story, without having to say every detail.
Yet the details do matter. Testers must work those out for themselves, at
close range, because they cannot be supervised from a distance. Testers
must have agency to do good work and manage the value of their own time.
High Alignment vs. Low Alignment
On the left side of Figure 5-2, you see how a high-alignment context means
that the tester can produce a general testing story as a natural by-product of
the work. This is not the case with low alignment. If the tester has a lot of
agency but is not well aligned with the client, then what the client expects
and demands from testing will not match how the tester is willing or able to
work. This creates a powerful incentive for the tester to "say what the boss
wants to hear" while hiding the real process. The public testing story
becomes a cover story that may be shockingly different from reality.

Figure 5-2: How your clients see your testing
James once reviewed the work of testers at a Midwestern insurance
company who had been directed by management to put their work into a
test-case management system. He asked one of the testers what would
happen to her test process if the test-case management system disappeared
overnight. She answered, "It would make no difference." After some
discussion, it emerged that, in her case, her documented test cases were only
used as a means to placate management. She regarded them as a sort of tax
she had to pay, a second job. She did not "follow" those test cases in any
meaningful way. Instead, her testing was highly exploratory, based on her
sense of the emerging product risk. This placation not only cost her time,
but it also denied management the opportunity to see what good testing
looks like.
In both the left and middle scenarios of Figure 5-2, there is a discrepancy
between the "real" story and the "public" story. The key difference is that in
the high-alignment scenario, the client could come into the test team and
watch the work happen without experiencing an unpleasant shock. It would

be like going into the kitchen of a fine restaurant and seeing professional
chefs doing skilled work. "So that's how the magic happens!" would be
your reaction.
In the low-alignment scenario, seeing that kitchen might trigger an
anonymous call to the health department ("Is that ... mold?!"). With low
alignment, the client must be kept away from the real work, or else they
would realize they were being lied to. Not that the cover story is necessarily
hiding bad work. It might be just the opposite. We've seen a lot of projects
where good testers find bugs by going outside the process they were told
they must use. It's not that they don't care, but that they refuse not to care.
They may fear arguing with their clients, and yet they know that obedience
is not the same thing as service.5 This is sometimes known as stealth
testing: testers secretly doing a good job because they believe clients are not
operating in their own best interests.6
Obviously, our recommendation is to solve the alignment problem. Create
high alignment through building good relationships, protecting your
credibility, and persuading via good reasons and evidence.
The Temptation to Coerce
When managers perceive an alignment problem, they may try to solve it by
decree. That is a dangerous game. Yes, they can use power to compel
employees to do certain things, but that move comes with a curse. Workers
resent it, and it may destroy their motivation to do their best. Testers, in
particular, do their best work when they are invited to think dangerous,
subversive thoughts such as, "How do we know that this product is safe? It
probably isn't!" and then turn those thoughts into specific and productive
experiments that will reveal important bugs. You won't get that kind of
initiative, dear leaders, if you foster a climate of fear and grievance.
Imagination is a gift testers offer to the team. It cannot be compelled.
Even mild coercion takes its toll. Another tester at the same insurance
company previously mentioned, also showed her test cases to James. Unlike
the first one, she claimed to follow those test cases. However, James noticed
that she had torn scraps of notebook paper on her desk: some crumpled, all
apparently ready for the wastebasket. She explained that those were
preparatory notes she'd made when writing out her test procedures. James

saw that these notes took the form of lists and tables of interesting test
conditions. Such documentation is concise, meaningful, and easy to
maintain. But she was throwing it away because management—who were
never going to perform any of those test cases—valued only detailed
procedural documentation.
In both cases, these testers created written test procedures and enshrined
them in a database at the behest of a manager. One tester freely admitted
that the test cases did not correspond to her test process. The other tester
naturally gravitated toward a concise style of test documentation before
subverting that in favor of doing something more legible to management.
Neither tester was operating fully under her own judgment. Both testers
burned company time doing work that undermined the mission of testing.
You may well be thinking, "Come on, guys. What about knowledge
transfer? What if someone new has to come in and run those tests?" Our
answer is that detailed procedural test cases are a terrible way to manage
knowledge transfer. It doesn't work! New testers coming in are often unable
to follow the documentation and may not even try. This is partly because
they don't have the requisite product knowledge and tacit knowledge about
the test process. Even when they are able to follow a procedure, they
usually don't take the time to understand its motivations and rethink it, no
matter how poorly designed the procedure might be. This is why James
likes to say, "Avoid following procedures unless they followed you first."7
Even if they were good test cases on the day they were written, they may
not be good today. Detailed documentation is very hard to keep up to date,
and it gets more expensive as it grows. Testers may not have the time to do
new testing and also create new docs and also maintain the old ones. Thick
documents can create an active hazard to good testing.
In general, procedural documentation in testing is an insidious anti-pattern.
It's a trap that discourages good testing. Having said that, there are
situations where very specific things should be done in testing, and it does
make sense to write them down. In certain contexts, such as regulated
software development (e.g., medical devices, financial software, etc.),
documentation that constitutes evidence of good testing is legally mandated
(at least for certain parts of the testing).

In 21 CFR 820.30, paragraphs F and G, you will find the US regulations on
documenting testing for medical devices. Many countries have adopted
these regulations. The regulations say that you must have procedures, and
that you must document "methods" of verification and validation. They do
not specify the level of detail. Meanwhile, in the Food and Drug
Administration Modernization Act of 1997, the law says, "The Secretary
shall consider, in consultation with the applicant, the least burdensome
appropriate means of evaluating device effectiveness that would have a
reasonable likelihood of resulting in approval."
As a general heuristic, RST advocates the most concise forms of
documentation possible, including brief training and demonstration videos.
We can create shared experiences via live training, coaching, and social
testing activities (paired and group), and through them reduce the need for
explicit docs. Build the agency, knowledge, and skill of the tester if you
want good testing.
You see the worst-case scenario on the right of Figure 5-2. This happens
when management "solves" the alignment problem by entirely destroying
the agency of the testers. They dictate the testing story to the testers, and the
testers follow like dogs on short leashes. This transforms the test process
into menial labor or else skilled labor of a narrow and passive kind.
Personal initiative is systematically punished under this regime. It's this
kind of testing that gives the testing industry a bad name, the sort of testing
no one aspires to do for a living. In the sociology of information work, this
is called an "enforced" trading zone.8 It's not the kind of zone anyone we
know wants to be in, yet it is depressingly familiar.
There is no evidence that managers or developers can get to good testing
via coercion. But what if we consider the scenario on the right and imagine
that the client is, in fact, a fully qualified tester. Couldn't that person specify
the exact process of testing to the obedient workers and get good results?
No! There is no evidence that good testing is possible by giving arbitrarily
detailed instructions.
Early in their careers, James and Michael both tried to do this at different
times and in different ways. The problem is that only those who perform a
test get the detailed, moment-by-moment feedback they need to govern the
process. Therefore, if you aren't performing the test, you can't govern it

responsibly, no matter what you write. It would be like giving a child a
detailed procedure for driving a car and expecting them to drive safely
while you are asleep in the back seat. What you can do is write instructions
on a level that engages the tester's knowledge, skill, and agency. That is not
coercion.
Okay, but couldn't the clients jump into the test process and do the testing
themselves? Yes. In that case, they are their own client, and there is perfect
alignment, and they will spend a great deal of their own time doing testing.
Nothing is wrong with managers who do testing, except that they won't
have time to be managers anymore. They would also have to navigate the
conflict of interest between wanting to release the product and wanting to
find problems in it. Most of the point of having dedicated testers is to avoid
that conflict.
We can do better. The leftmost scenario of Figure 5-2 is better: highly
aligned with high tester agency. We need to develop ways of reporting
testing work that fit testers doing good testing as they see it, while also
allowing management and other outsiders to understand and appreciate that
work. Note that in Chapter 18, Keith Klain speaks of his experience as a
manager who gets involved in the testing. He does not come storming in to
take over, but rather as a means of staying in full contact with the testing
story in collaboration with the people who work for him.
Artifact-based vs. Activity-based
Management
What is the substance of a testing story? Inside your head, it comprises
images and beliefs. It may be expressed in spoken or written words,
diagrams, charts, source code, procedures, spreadsheets, screenshots, or
even facial expressions. However it is expressed, the form and content
reflect your model of testing and must be interpretable by their intended
audience to achieve its intended purpose.
Artifact-Based Management
Physical objects are an attractive vehicle for storytelling. Want to know the
status of a container-ship loading process? Just go and look at it. It's full ...

less full ... nearly empty now ... filling up again ... almost ready ... now it
sails away! In the testing world, we don't have that much physicality, but it
can be simulated with files. Indeed, the word "file" began as denoting a
wire on which hung physical documents. Soon after, there were "file
folders" within "file cabinets." There was a real size and heft to them.
These days, "file" is a word we associate with a designated block of
digitally encoded data. This ubiquitous computing artifact is the basis for
the most popular way to manage testing: by representing testing as a set of
files, then pointing at the files and saying, "Those are my tests ... that is the
testing."
We in the Context-Driven testing world call this the Factory School of
testing thought or artifact-based test management. In this approach, the
testing story is assumed to be fully producible via the artifacts of testing.
These artifacts, usually called test cases, are saved, reviewed, filtered,
interpreted, and counted (oh, how they are counted). In a less technocratic
company, testers "execute" the test cases. In a more technocratic company,
they are automated as output checks. These then serve as boundary objects9
that mediate between the software project and the test process. Testers
present these artifacts and tell the testing story around them as well as via
them.
Our industry has been obsessed with test cases for a long time. The earliest
literature on software testing that we've seen mentions them as a settled
concept.10 Over ten years ago, James Bach and Aaron Hodder cataloged
this list from job descriptions:
"We are seeking a Graduate Test Analyst to write and execute test
cases to ensure quality is delivered to an extensive client base."
"Responsibilities: Assist with requirements gathering and developing
test cases that satisfy the requirements."
"You will have skills and experience in the following: ... Writing and
executing test plans and test cases."
"To be successful in this role, the following is required: ... Having
written manual test cases and being involved in test execution ...
Proven experience in writing Test Conditions and Test Scripts."

More than ten years later, things haven't changed. Under "Key
Responsibilities," we still see the same kinds of listings. Moreover, James
and Michael did an analysis of 1.3 million job descriptions posted on
LinkedIn, and discovered that roughly 50% of the descriptions related to
software testing explicitly mentioned test cases.
It's not clear what these organizations are actually looking for, because
definitions of test cases vary from place to place. There is no consensus in
the industry.11 Testers will speak of test cases that are about to be written, or
that have been written, or that have been executed. In some projects, a test
case will always have a unique procedure associated with it. In other
situations, test cases share procedures and are distinguished by unique data.
Considering the variety of things called test cases around the industry, a
definition that covers all of them would have to be quite general. Thus, this
is the RST definition: a test case is some idea, instruction, or data intended
to guide the performance of a particular test. This is a broad, flexible, and
inclusive idea that is reasonably compatible with the other definitions
floating around. Notice that our definition of test case includes cases that
are not written down. A test case might be an idea in your mind.
Beware of Counting Incommensurable Artifacts
By any definition we have ever seen, test cases are not easy to compare.
They vary in value, focus, difficulty to execute, level of specificity, "type of
bugs findable by," automatability, relevance, preconditions, post-conditions,
and so on and on. Therefore, one thing should be clear: it makes no sense to
count them. List them? Sure. Describe them? Definitely. But you can't
meaningfully count them except under very specific conditions of
similarity, and even then, the count can't be combined with any other count.
You can't meaningfully average different types of test cases together.
Imagine a landslide that blocks a road. No one would say, "There are ten
million rocks on this road that we must move!" Some of those rocks are
pebbles; some are huge boulders. If we want to talk about them in a
meaningful way, we would use a well-defined, comparable unit, such as a
cubic foot or a ton.
There are no generalizable comparable units of measurement for test cases
that get at their testing value. To illustrate this better, here are two

contrasting examples.
First, consider a table of test cases characterized by differing conditions.
The procedure associated with these cases is learned or documented
elsewhere. Note that it's not obvious how to delineate or count the cases in
this example. "Cookies Accepted," being a distinct idea, may fairly be
called a single test case. Maybe each row is a test case—such that there are
three. Perhaps each column is a test case, so we have three—but a different
three. Maybe each cell in the table is a test case, for a total of nine. There's a
failure noted in the table. Are we seeing the product fail in a single test
case? Are we seeing one failure in three cases? Which three—the rows or
the columns? Are we seeing a single failure in nine cases? Is the product,
therefore, 89% good? We think it's confusing and misleading to think about
test results that way.
CHROME FIREFOX SAFARI
All cookies accepted
PASS
PASS
PASS
Essential cookies only PASS
PASS
PASS
All cookies rejected
PASS
PASS
FAIL
Our second example, based on a step-by-step procedure, is also commonly
called a test case:
STEP
EXPECTED RESULT
ACTUAL RESULT
1. Start Chrome browser browser starts
PASS
2. Set to accept cookies browser accepts cookies
PASS
3. Attempt log in
user home screen displays PASS
As above, each step might be called a test case because each line has a
verification operation associated with it. So, is this one test case or three?
When we change Step 1 to account for the three browsers to be observed
and Step 2 to account for three settings to be examined, do we have nine
test cases of three steps each, or do we have 27 test cases?
These are not the only kinds of things called test cases, of course. A tester
may make a loosely structured list of ideas, such as "load a valid file,"
"load a large valid file," and "load a large, corrupted file," and call them
test cases. We sometimes see hybrids of data-like and procedure-like cases,

wherein a procedure-like test case includes variables that take their values
from data-like test cases stored in separate files.
Tables, rows, columns, cells, procedures, steps, and files are all different
kinds of things. If two people both ran a marathon at two different times
and places on the Earth, you could say that they both had run about 26
miles and that the conditions they would have experienced were at least
roughly comparable, even though the ground that they traversed was
different. But if two people both performed 26 test cases, and all you knew
was that they were different test cases, you would have no basis whatsoever
to claim that their testing was comparable.
The Basic Problem of Artifact-Based Management
When the testing story becomes centered on artifacts, a dangerous thing
happens: skilled testers are decentered. They become seen as caretakers of
the testing artifacts, the way zookeepers take care of animals at a zoo.
(Notice that visitors think of the zoo as the animals, not the process of
keeping them alive and happy. No child says, "Today I want to review the
outcome of good zookeeping, Daddy!")
Regardless of what test cases there are or how they are organized, the key
premise of this way of organizing is that the essence of a testing process
can be encoded digitally. If that premise is wrong, then the test story based
on it is fundamentally misleading. The authors of this book believe it's
wrong.12
The best-case scenario (pun intended) is that test cases could form the basis
of a simple story of testing. You could, for instance, list the test cases, or put
tags on them, or arrange them into a mind map, and explain the testing
status that way. This might be okay if there were not an overwhelming
number of test cases. This would be essentially the same as what our
colleague Paul Holland calls a "release coverage outline."
The more common situation is that management doesn't take the time to
look at the substance of the test cases, and the concept of test cases creates
a screen that merely obscures the true story of the testing, preventing
management from understanding the test process. Although the test strategy
is reflected in the test cases (assuming the test cases are actually performed
as documented), it's very hard to understand a test strategy merely by

looking at a bunch of test cases. Meanwhile, as we previously described,
these artifacts tend to be highly confining and therefore anti-agile. The
documentation becomes a sort of tar pit.
Some people say, "But my artifacts are code. My code runs. I keep my unit
test suite up to date." Okay, then the problem becomes confusing checking
with testing. Our questions would be: "What specifically do you do to
service that code? What testing work are you doing other than servicing that
code? What are you covering and what are you ignoring? What kind of
oracles are you using? How does your suite of checks relate to product
risk?" That automation of yours may be discouraging you from
experiencing the product in realistically complex ways that matter to your
users. We discussed this problem in Chapter 3, "How to Do a Test."
Putting test cases at the center of our reporting leads to a reification fallacy,
turning testing work into scorekeeping—where the scoring system is
completely arbitrary, creating a cover story that obscures both the status of
the product and the status of the testing work.
It does, however, have one big advantage: it's easy to sell to people who
know nothing about testing. It seems legible to them.
Whether you reeducate your management or work within their existing
models and biases, you must make your testing story legible. A huge and
perennial challenge to testers is to find ways of reporting that facilitate their
work instead of impeding it.
Activity-based Management
There's an alternative to treating testing as a collection of files: we can treat
it as a collection of activities. Recall from Chapter 2 that our definition of
test activity is a tester spending time on something to fulfill the mission of
testing.
Activity-based management means you focus on what testers do. Sure, we
have files and documents and code in our testing. But we don't treat those
things as if they embodied the testing. We assume that what code does is
meaningless unless it is controlled, understood, and otherwise underwritten
by a responsible tester. The units of activity-based test management are
typically time and tasks.

These examples give a flavor of the difference:
TESTER: "HERE ARE THE TEST CASES!"
ARTIFACT-BASED
MANAGER
ACTIVITY-BASED MANAGER
"Okay."
"But help me understand your testing. What did
you do, and why did you do it that way?"
TESTER: "I COMPLETED STRESS TESTING ON BUILD 60
ACCORDING TO THE TEST STRATEGY WE DISCUSSED. THE
PRODUCT DID PRETTY WELL. I'VE LOGGED A FEW MINOR
PROBLEMS, AND THE DEVELOPER IS LOOKING AT THEM."
ARTIFACT-BASED MANAGER
ACTIVITY-BASED
MANAGER
"But which test cases were run, and what
percentage passed?"
"Okay."
James certainly did not invent activity-based test management. But like
several aspects of testing that matter in RST, he suspects that he was the
first to give it a name.
The approach occurred to him when he was asked by Microsoft to create a
"formalized process of exploratory testing"13 for the Designed for Windows
2000 program. How can we formalize what is essentially informal testing
without ruining it? The answer he came to was to create a set of parallel
tasks with well-defined outcomes yet open-ended, heuristic procedures. The
tasks could be done in any order, or you could do a bit of one, then do some
of another, then go back to the first one. The only thing that mattered was
that, by the end of the allotted time, you had achieved the goal of the task
and produced the deliverables. Microsoft established a time box of 4-6
hours to complete the process. Testers were then trained and certified in the
method. It was a tester-centric approach: activity-based.
Session-Based Test Management
The following year, working for Hewlett-Packard, James and his brother,
Jon, created a generalized version of that practice, which they dubbed
Session-Based Test Management (SBTM). SBTM introduced the idea of a

test session, which is a virtually uninterrupted period of time during which
testers attempt to test a product.
The concept of the test session creates a structure that protects testing work
from intrusive and nervous managers. Indeed, the original motivation for
SBTM was to mollify managers at HP who wanted to measure testing. They
wanted to count test cases, but James and Jon refused even to create test
cases. (They had been specifically hired to perform highly exploratory
testing, and test cases don't help much with that.) So instead of counting test
cases, they offered to count blocks of testing time—short enough to be
described and comprehended but long enough to get focused work done.
Each session in SBTM has the following elements:
Uninterrupted person-time: A session is an event, a performance, not
a document. In a session, work is performed by one or more testers.
When accounting for testing effort, we treat sessions as though they
were uninterrupted blocks of time. That is to say, a real session may be
somewhat interrupted, but we pause the clock of the session during
each interruption. We count person-hours. Thus, one test session that
lasts one hour counts as a three-hour session if three testers were
involved in it. We never count the time taken by nonhuman processes,
because such time is outside the scope of a test activity.14 But we
would count the time of a tester forced to wait, during a test session,
for a nonhuman process to complete.
Charter: All sessions have a charter, which is another word for a
mission statement.15 Coming into the session, a charter is a statement
of intention about the focus of the session. At the end of the session,
we modify the charter (if necessary) to state the actual focus of that
session as it played out. This is needed if the tester gets into the session
and realizes that the original charter is not feasible or is not the most
important thing to do. Thus, you might have a big list of aspirational
charters that you intend to perform (like high-level test cases), but
when the sessions are reported, the charters attached to them should
correctly represent the work that was done.
Report: The session report is a record of the session. It includes the
charter, metadata about the session (metrics, product areas, testers

involved, time started, etc.), and the notes. The notes are the heart of it.
They tell the story of that session. One of the great things about SBTM
is that a client may ask, "What testing did you do on August 4th of last
year?" and you can quickly provide a detailed answer—because: test
notes!
Metrics: SBTM does not require keeping metrics, but it is set up to
facilitate them, and if you do keep them, you can use the data to make
evidence-grounded test estimates. Also, tracking them helps
management understand that testing is more than just looking for bugs
and that finding bugs actually slows down testing.
The basic metric is time. We track the minutes spent in a session,
further divided into three types of work:
Test Execution ("T-time") really means "bug-hunting time" or
"time spent in a manner in which we might find a bug." During T-
time, we're actively encountering the product with our exploring,
experiencing, and experimenting. We're covering the product and
applying our oracles.
Bug Investigation & Reporting ("B-time") means "time spent
investigating and reporting an apparent bug that we've
encountered." We only care about and report this use of time if it
interrupts test execution. Thus, if we are obtaining new coverage
while we are investigating bugs we've already found, then we call
it T-time.
Setup & Admin ("S-time") means "time spent during the session
that's necessary for testing but does not involve finding or
reporting bugs." We report this use of time only inasmuch as it
blocks T-time and B-time. If we could perform setup and admin
while going full speed on test execution, then we would call it T-
time instead. S-time includes creating test data, reading specs,
taking notes, writing code, etc. There is no shame in S-time. Some
test sessions are 100% S-time, and that's perfectly fine. For
instance, you might be writing some code to generate test data, or
you might be exploring a log file to find testable elements.
Accounting for S-time is important because otherwise our clients
may think that our days are spent exclusively hunting for bugs.

Excessive S-time may indicate testability issues that we need to
escalate.
Review: If there is more than one tester on the project, and they are
contributing session reports that are being measured or compared,
then it is pretty important to hold a review of the test sessions on a
regular basis, either daily or weekly. This matters because different
testers will inevitably interpret the protocols for chartering, note-
taking, and metrics differently, if left to themselves. The records
will then lose a lot of their value, the session reports will become
incommensurable, and the metrics will be meaningless.
Originally, James and Jon performed reviews in the form of debriefing
meetings at the end of every day. Our colleague Djuka Selendic found that
the beginning of the next day worked better for her team because it was
easier to get them together at that time. James has lately moved to a system
of once-per-week debriefings, along with asynchronous review of the
session reports and adding comments in Jira.
As for that first SBTM project at HP, James and Jon, who were external
consultants, left the project after six months. When James checked in with
the team at HP four years later, they said that they had stopped doing the
SBTM metrics because "No one was asking to see the metrics anymore."
Apparently, the team had built enough credibility that management had
better things to do than challenge their value every week. But the most
interesting part is that they continued to use sessions and charters to
structure their work. That part had become a natural and comfortable
routine.
SBTM can be challenging to implement for several reasons:
It's not familiar to non-testers, so you have to keep explaining it.
It requires testers to take notes in a disciplined way. This is a training
issue because many testers have not practiced note-taking and may be
poor writers.
It can be difficult to arrange for relatively uninterrupted test sessions.
Interruptions are hard to avoid, considering email, instant messaging,

and people wandering in while you work. Sometimes projects are
downright chaotic, and you can't simply tell people to leave you alone.
Test management tools don't directly support SBTM. However, it is
possible to create session reports in test management tools by
repurposing the forms used to describe test procedures. We have also
created a new issue type and associated form in Jira to track sessions.
Fortunately, there is also a simpler way to approach activity-based test
management: threads instead of sessions (or TBTM).
Thread-Based Test Management
By "thread," we mean a recurring to-do item, a named activity that is
serviced by testers throughout the test project. See Figure 5-3 for examples
of threads from a security-testing project.
Managing by threads works even in the most chaotic projects because it
involves nearly zero paperwork, tooling, or training. All you have to do is
make a list of the threads of your testing. Write down the list in some form,
perhaps as a set of cards on a Kanban board, Post-its on a wall, or an outline
in a mind map, then report on the progress of testing (telling the story) by
reference to the threads as you go through the project.
In one project, James discussed threads with the test lead for his project in
the morning and after lunch, choosing a thread to work on for the first and
second halves of the day, respectively. Management wanted a briefing on
test status at the end of each day, so he maintained the thread list on a
whiteboard, providing a report on demand.
Unlike SBTM, TBTM doesn't give you metrics or provide any special basis
for estimation. Although it doesn't mandate any test documentation of any
kind other than the threads, you can include any necessary documentation
work as a thread. But TBTM itself is a very simple and relatively foolproof
way to organize testing that facilitates telling the testing story.

Figure 5-3: Examples of threads
Please Don't Fake Your Testing
Alas, testing is easy to fake. This is partly because it's hard to tell, from the
outside, whether testers are being thoughtful on the inside. It's partly
because of the conflict of interest managers must navigate, wherein they
want you to tell them the truth, but they also fear knowing the truth—
because the truth will bind them. It's partly because of the well-oiled
folklore of Factory School testing that values ceremony and nice-looking
templates over brain-bending investigation and messy learning. And it's
partly because excellent testing may not find any more bugs than poor
testing does, if there are no good bugs to find. A broken smoke detector is
indistinguishable from a working one most of the time, assuming your
house is not burning down today.
Please say no to fake testing. Avoid putting up a cover story. If you feel that
your testing is impaired by conditions created by developers or
management, at least say so. Go on record. Put it in an email.

If you realize you have been faking your testing and you want to stop doing
that, you may feel the need to dig your heels in. While we cannot exactly
recommend any of the following juicy quips, each one has been uttered by
at least one of the authors of this book at one time or another in their
careers. At the very least, they are food for thought:
"Misleading you is not a service that I offer."
"I could do bad work for you, but unfortunately, you pay me too
much."
"I can do that for you, but it will take time away from finding bugs."
"I don't insist that everyone agrees with me, only that they hear me."
"It's not just about what you want today. I must think about a year from
now, when you'll be angry that I didn't talk you out of it."
"If everyone follows the standard, no one will ever find a better way."
"I'm not 'resisting change.' I'm determined to do good work."
"Please let me do the job you are literally giving me money to do."
Notes
1   We found this book helpful: Fairhurst, G.T. with Sarr, R.A. (1996) The
art of framing: Managing the language of leadership (1st ed). San
Francisco: Jossey-Bass Publishers.
2   Luke Hohmann, who went on to write several books on Agile
development.
3   See Chapter 9 for Paul Holland's take on this kind of test reporting. See
also Paul's account of reporting using Kanban-style notes in "A Sticky
Situation," which you can find as a download at
www.wiley.com/go/takingtestingseriously.
4   Oshry, B. (1995) Seeing systems: Unlocking the mysteries of
organizational life (1st ed). San Francisco: Berrett-Koehler.

5   What's the difference between a drug dealer and a doctor? One is
passionate about customer satisfaction. The other is a doctor.
6   This could be because the client doesn't understand how testing works.
But another possibility is that the client only cares about the appearance
of testing, and just wants the testers to stay out of the way and never
complain. You encounter this in situations where managers are richly
rewarded for releasing quickly and see the purpose of testers as being
convenient scapegoats in case of trouble. "Your one job was to assure
quality, and you failed!"
7   This is lesson 44 from Kaner, C., Bach, J. and Pettichord, B. (2002)
Lessons learned in software testing: A context-driven approach. New
York: Wiley.
8   Collins, H., Evans, R. and Gorman, M. (2007) Trading zones and
interactional expertise. Studies in History and Philosophy of Science
Part A, 38(4), Article 4.
9   Star, S. L. (1989) The structure of ill-structured solutions: boundary
objects and heterogeneous distributed problem solving. In: Distributed
Artificial Intelligence (pp. 37-54). Amsterdam: Elsevier.
10 McCracken, D.D. & Company, G.E. (1957) Digital computer
programming. New York: Wiley. Also, Leeds, H.D. and Weinberg, G.M.
(1961) Computer programming fundamentals. New York: McGraw-Hill.
11 Standards organizations have done a terrible job of surveying industry
practice or influencing that practice. The ISTQB has done no scholarship
on this, either.
12 An excellent antidote to the theory that written documentation can or
should drive technical life is this book: Brown, J.S. with Duguid, P.
(2002) The social life of information. Boston: Harvard Business School
Press. It presents research around how technical support people at Xerox
made little use of technical documentation compared to the simple act of
talking to each other.
13 It was called the General Functionality and Stability Test Procedure. You
can find this as a download at

www.wiley.com/go/takingtestingseriously.
14 If you want to separately measure how much time it takes to run
automation, do so with our blessing, but we don't see any sense in
mixing together measurements of computation time and person time.
15 James adopted the word "charter" to avoid confusion of the mission of a
session with the mission of the entire test project.
OceanofPDF.com

CHAPTER 6
How to Apply Automation in Testing
—By James Bach and Michael Bolton
There are many wonderful ways in which tools can be used to help software
testing. Yet, all across the industry, tools are poorly applied, which adds
terrible waste, confusion, and pain to what is already a hard problem. Why
is this so? What can be done? We think the basic problem is a shallow,
narrow, and ritualistic approach to tool use. This is encouraged by the rarely
examined and absolutely false belief that testing is a mechanical, repetitive
process. Good testing, like programming, is instead a challenging
intellectual process. Tools in testing must be applied by people who
understand the complexities of tools and of tests. This is as true for testing
as for development, or indeed as it is for any skilled occupation from
carpentry to medicine.
Some people believe that AI has changed everything. This is not so. AI is a
tool. It is neither a stakeholder nor a responsible social actor. AI can do
"smart" things and help us in our projects, but no tool can be accountable
for its own actions. And the corporations that offer AI tools make no
warranty, either.
Our vision of "test automation" puts the tester at the center of testing, while
promoting a way of thinking that celebrates the many things tools can do
for us. We embrace tools without abdicating our responsibility as technical
people to run the show.
Tools can be powerful, and we are going to say encouraging and helpful
things about them. But automation can also be treacherous—not least
because the label "automation" refers to a mess of different things. So we
must begin with a sober look at some basic misconceptions.
Robots! Help!
We can summarize the dominant view of test automation as automate
testing by automating the user. We are not claiming that people literally say

this, merely that they try to do it. There are at least three big problems here:
1. The word "automation" is misleading. We cannot automate users. We
automate some behaviors they perform, but users do more than what
can be automated.
2. Output checking can be automated, but testers do more than checking.
3. Automated output checking is interesting, but tools can do more than
checking.
Automation comes with a tasty and digestible story: replace messy,
complex humanity with reliable, fast, efficient robots! Consider the book
"Automate the Boring Stuff."1 As a book about programming that teaches
how to perform a range of simple tasks using code, it looks helpful to us. It
doesn't make any claims about testing. But the book has an odd cover: it's a
drawing of a shiny humanoid robot pushing a lawnmower in a quiet
suburban neighborhood. Okay. Why does a silly thing like that concern the
authors of this book?
Because it casually shows us a machine that is intended to function as a
human. The robot is constructed in a human form. It is using a tool
normally operated by humans, in exactly the way that humans would
operate it, rather than through an interface more suited to robots. There is
no depiction of the process of programming the robot, not to mention
controlling it or correcting it when it errs. There are no broken-down robots
in the background. The human role in this scene is not depicted. No human
appears even in the background. The message is: robots replace humans in
uninteresting tasks without changing the nature of the process and without
any trace of human presence, guidance, or purpose. Is that what automation
is? Is that how it works? No! When you introduce a tool into a human
process it always becomes a different process. People must accommodate
the tool. There are always consequences to automation.
Of course, it is a light-hearted cartoon, not to be taken seriously. The
problem is, in our travels all over the industry, we see clients thinking about
real testing, real automation, and real people in just this cartoonish way. The
trouble that comes from that is serious.

How serious? In the experience of the authors, observing projects going
back to the 1980s, we find that it is normal for large-scale automation
efforts to lavish the bulk of their budgets on the detection of trivial and
obvious GUI-level bugs, drawing much-needed time and effort away from
deep testing. Furthermore, the typical automation approach has the
character of a Rube Goldberg machine—swimming in dependencies and
comically prone to breakdown.2 This sort of automation becomes almost
like a new baby on the project, demanding ever more parental attention. We
believe the effort typically invested in automation would, in most cases, be
better invested directly into humans interacting with the product in complex
and sophisticated ways (which also find the shallow bugs) and into less
expensive supporting tools that help testers test better.
No one can deny that automation tool sales demos are impressive. What we
deny is that people agree on what "automation" means, what it should be,
and that those sales demos translate into practical value on ordinary
projects.
The Trouble with "Automation"
The trouble with "test automation" starts with the words themselves.
Testing is a part of the creative and critical work that happens in the design
studio, but "automation" encourages people to think of mechanizable
assembly-line work done on the factory floor.
The term "test automation" is also ambiguous. It is common to hear
someone say a sentence like "run the test automation," which refers
specifically to tools. But a sentence like "test automation is worth doing"
refers not only to tools but also to the enterprise of creating, maintaining,
testing, and operating those tools. In the first sense, test automation is not
human at all. It's incredibly fast and inexpensive, too, since you don't pay
the computer. In the second sense, test automation is a skilled activity
performed by humans who write and operate software over hours, days, or
weeks—and those people are not robots.
We observe that, in common parlance, "test automation" means scripting
ordinary, rote actions that a user of the product might perform—and a rather
complacent, unimaginative user, at that—then getting the machinery to

punch the keys at dazzling speed, and then checking to see whether
specified behavior and inputs produce specified outputs. It's a small step
from there to think of "test automation" as a sort of tester in its own right.
But even a minimally skilled human tester does far more than blindly
adhere to prescribed actions and thus observes far more than the output of
some function. Humans have complicated lives, agendas, talents, ideas, and
problems. Although certain user and tester actions can be simulated, users
and testers themselves cannot be replicated in software. Failure to
understand this simple truth trivializes testing—and allows many bugs to
escape.
How can we think about all this more clearly?
Number one: stop thinking that the only thing tools can do for you is push
buttons and check the screen.
As shown in Figure 6-1, user simulation is a subset of all possible
approaches to testing. Testing involves operating the product, and that
means using the product, so "performing a test" nearly always requires
simulating a user or asking an actual user to test a fully integrated system.
The more we get away from the GUI and away from end-to-end "journeys,"
the more testing becomes abstracted from likely usage patterns, and the less
we are realistically simulating users.

Figure 6-1: Tools are more than just a way to simulate users.
Source: Rube Goldberg / Wikimedia Commons / Public Domain
You might think that's a bad thing. Shouldn't we be doing things in testing
that users are likely to do? Otherwise, we will not find bugs that users are
likely to find, right? Yes, that's an important consideration, but there are
other things that matter, such as severity. In the case of a security bug, a
vulnerability can be incredibly obscure and yet become a huge problem
once hackers find it. There may also be obscure bugs that bring the whole
server down, or lead to massive data corruption, or lead to a public relations
problem, as the handful of people who see an obscure but terrible bug
spread the word all over social media.
So, if we want to catch elusive bugs in a short time, we don't want to stop at
simulating realistic usage of the product. We might automate operations in
ways that no real user could ever do without special equipment and dark
intentions, such as blasting an API with a million random inputs (called
fuzzing). We could create tools that don't operate the product at all but rather
comb through log files and telemetry, looking for certain kinds of problems.
There are many possibilities.

In RST, we don't call it test automation ...
We define a tool as any human contrivance that aids in fulfilling a human
purpose. A test tool could be software, hardware, a map, document, artifact,
or some other heuristic that aids in fulfilling a testing purpose. We are
primarily concerned with software-based tools here.
The term test tool connects us to the ordinary, everyday understanding that
these contrivances do not work without human guidance. Tools extend the
capabilities of an appropriately skilled human. The ordinary understanding
of the word tool opens the door to the many ways that tools can lighten
burdens and amplify the power of testers.
Tools are helpful instruments, but they don't capture testing.3 No tester will
encode an unconscious expectation or unanticipated action. A tester,
interacting with a product, spontaneously reacts to an astonishing variety of
surprising and erroneous events without ever having been consciously
aware of an expectation about them. If, for instance, a window turns purple
for a moment, or an extra line appears, or a process takes a little longer to
complete one out of ten times, they almost effortlessly notice and react. But
when they tell the story of this test, perhaps by writing down its steps and
expected results, only a small part of all those real expectations is
expressed. Since the testing humans actually do cannot be put into words, it
cannot be encoded and therefore cannot be automated. We should not use a
term that implies it can be.
The common terms manual testers or automated testers to distinguish
testers are misleading, because all competent testers use tools. Researchers
use tools, too, but no one speaks of automated research. No manager who
calls for automated testing aspires to automate management, as well. That
makes sense because, in those cases, people associate those jobs with skill
and judgment. Automated testing seems like a reasonable concept only
because people think that testing is a trivial matter of checking that the
output matches some predetermined value.
Since all testers use tools, we suggest a more interesting distinction: some
testers make tools, and some do not. It's worth having a special term for
those who do. We suggest calling such technical testers toolsmiths, or
coding testers. Although toolsmiths and their tools help to extend,

accelerate, and intensify certain activities within testing, they do not
automate testing itself.
GenAI has become capable enough to allow relatively nontechnical people
to create tools. Or you could put it this way: GenAI might serve as a
toolsmith for your project. This still requires testers to specify these tools
carefully and test that they do what the robot claims they do (and nothing
else that you don't want it to do). The authors have had both good and bad
experiences with such "vibe-coded" tools.
Meanwhile, the term test automation threatens to dissociate people from
their work. As we covered in Chapter 2, testers engage in sensemaking,
critical thinking, and experimentation, none of which can be done by
mechanical means. Yet, in our experience visiting hundreds of companies
and teams, we find managers and technocrats who speak of testing routinely
ignore these intellectual processes. We have tried reminding them—and our
own colleagues, at times—of these crucial elements that cannot be encoded
into test cases or test software. "Oh, we agree," they might say, but then
they lapse back into speaking exactly as if the essence of testing is
somehow expressed in their "automated testing." After years of this
struggle, we conclude that the term itself is a sort of narcotic.4 "Automated
testing" is toxic speech.
We suggest calling it any of the following:
Tool-supported testing
Instrumented testing
Automation in testing
Or you can just say "automation" without specifying exactly what is being
automated.
These are all reasonable, nontoxic alternatives.
... yet we explore many ways to use tools
The skill set and the mindset of the individual tester are central to the
responsible use of tools. When we say this, however, some people seem to

hear us saying that tools are not important or that RST people hate tools.
This is absolutely false! We think tools are important and powerful.
Let's catalog some of the many ways tools help us in testing.
In design, we use tools to help us:
Produce test data (tools like spreadsheets, state-model generators,
Monte Carlo simulations, and random-number generators)
Obfuscate or cleanse production data for privacy reasons (data
shufflers, name replacers)
Generate interesting combinations of parameters (all-pairs or
combinatorial data generators)
Generate flows through the product that cover specific conditions
(state-model or flow-model path generators)
In product interaction, we use tools to help us:
Perform static analyses of the code (error-message checkers, table
checkers)
Set up and configure the product or test environments (like
continuous deployment tools, virtualization tools, or system-
cloning tools)
Set up a performance analysis, perhaps for a long time, at high
volume, under stress (profiling and benchmarking tools)
Write code to operate the product (automated operation); or to
additionally collect, organize, and present output data to a human
tester (automated assay, blink testing); or to additionally look for
strange but not necessarily wrong output (automated flagging); or
to additionally look for output that is definitely wrong (automated
checking)
Simulate software or hardware that has not been developed yet or
that we do not have immediately available to us (mocking or
stubbing tools)
Probe the internal state of the system and analyze traffic within it
as testing is being performed (instrumentation, log analysis, file or

process monitors, debugging tools)
Collect telemetry from users in the field, then use that to generate
test data and scenarios (signals-based testing—see Chapter 14)
Collect and analyze comments from users on a large scale, to learn
about bugs that have escaped from our test process (remedial
testing)
In evaluation, we use tools to help us:
Sort, filter, and parse output logs (text editors, spreadsheets,
regular expressions)
Monitor the status of related systems that are not the product you
are testing, but could be impaired by it
Visualize output for comparative analysis (diffing, charting and
graphing tools, conditional output formatting)
Compare output to previously stored known correct output
Compare output to that from an independent parallel algorithm
Compare output to that of a competing product
Compare output to that of an earlier execution of the same product
(change detection)
Analyze output for specific properties that it must have or cannot
have
In recording and reporting, we use tools to help us:
Record our activities and document our procedures (note-taking
tools, video-recording tools, built-in logging, word-processing
tools, user-interaction recording tools)
Prepare reports for our clients (mind maps, word processors,
spreadsheets, presentation software)
In managing the testing work, we use tools to help us:
Map out our strategies (mind maps, outline processors, word
processors)

Identify what has and has not been covered by testing (coverage
tools, profilers, log file analysis)
Preserve information about our products
Aid other people in future support and development (wikis,
knowledge bases, file servers)
We're sure this is an incomplete list of the ways in which we use tools to
help us.
You have probably noticed how we repeatedly have said, "We use tools to
help us." We have chosen these words deliberately to emphasize once again
that tools don't do testing work—tools help testers to do testing work. In
conversation about testing, tools may be important, but in RST, the center
of testing must be the skill set and the mindset of a person who is testing.
Michael once created a tool to model transactions for the foreign exchange
component of a banking system. Each kind of transaction could have
different effects on different accounts in the bank's general ledger. The tool
calculated the appropriate credits and debits for comparison with the
system's actual behavior. The tool did not operate the banking system but
rather allowed Michael to spontaneously improvise complex financial
transactions involving multiple accounts and more than a hundred business
rules. His tool facilitated rapid, deep, creative testing. Furthermore, he
found that even the process of creating the tool forced him to develop a
deeper understanding of the overall system under test.
Consider Augmented Experiential Testing
The authors have had success with something we call augmented
experiential testing. This means experiential testing5 while using tools that
provide the tester with certain superpowers, such as enhanced senses and
memory. Tools we use include:
A browser extension that logs every interaction between the tester and
a web app. These logs can then be aggregated or compared to previous
sessions in order to provide on-demand, user-level test coverage
reporting.

Bookmarklets that allow testers to make notes that are inserted directly
into those logs.
A browser extension that makes it easier for testers to provide strange
or extreme inputs.
A script that watches the browser debug log to alert the tester via voice
alert when certain errors appear in it.
Browser bookmarklets that analyze web pages and report on hidden
elements.
Scripts that help analyze JSON data that is embedded in web traffic
while exploring the product, to help testers understand the full
complexity of the product's input and output.
A testing "freshness" indicator consisting of a programmable LED that
turns green when the tester touches something they haven't tested yet,
but that otherwise fades to red.
Tools that create experiences that are not naturally available to us, such
as color-blindness or slow Internet connections.
Oracle support tools that enable testers to do deeper testing without as
much deliberation, such as Michael's bank transaction analyzer tool
mentioned.
Tools that aid experiential testing magnify the value of highly exploratory
testing and alleviate managers' concerns about unscripted testing. They
improve your record keeping and help you get to deeper testing sooner.
They help you find bugs that matter.
We are not the first to suggest that the "copilot" analogy of AI is not
helpful. A better way to apply AI is to make it help us be more aware of our
surroundings.6 We are working on tools that do exactly that.
Beware of Oversimplified Output Checking
"Make the product do the easiest thing. Check whatever output is
convenient." Simple checking can be good when you are just getting started
and need to get a sanity suite together quickly, or if you need some sort of
checking to serve as part of the build pipeline and you want to minimize

disruption (e.g., by only triggering an alarm if there are huge and obvious
bugs).
But we bristle at certain commonly touted "best practices." For instance,
some say that automated checks should be short and "independent" of each
other, so that any particular bug would cause, at most, one "test" to fail.
This encourages test programs that are easier to isolate and investigate
when things go wrong—at the cost of discouraging complex testing that
reveals elusive bugs.
Complex testing is an alternative heuristic, intended to exercise the product
deeply. For instance, we like to create loops within loops and include
multiple levels of checking as we slam the product with a hundred different
inputs and run them through a chain of operations. Yes, this can seem
complicated and "flaky" in the sense that it may lead to intermittent effects.
It also finds bugs. Therefore, to quote Ghostbusters, "Back off, man, we're
testers!" Ditch the water wings and swim in the deep end. Go where the
bugs are.
We can certainly make checks less brittle in the face of change, but keep in
mind at all times that the very things that make automated checks less
brittle usually make them less powerful, too—because resiliency is
achieved by sacrificing some sensitivity. We may filter out timestamps,
usernames, or other ephemeral details. We may block out sections of
screenshots. This allows us to use the same logic to check different screens
in different cases, but there might be a legitimate bug in some of those
details. One common and effective practice is to access GUI controls by ID
instead of using locators that rely on the exact structure of the page. But that
means the automation will not alert you when the structure of a page has
become corrupted.
There is always a tradeoff. Making our checks sensitive, but not too
sensitive, is a challenging design problem. Just remember: you are a tester,
not merely a tourist. We testers critically analyze what we see. Computers
don't know how to be critical. They merely verify specific facts.
Another practice that can lead to overly simple testing is so-called
executable specifications.

First, the name is misleading. It wallows in the ambiguity between two
different meanings of "specification." One meaning is "a set of explicit
instructions to perform," while another is "a detailed description of
something we want." Only in the first sense could we have an executable
specification. It's called source code. But no set of instructions describes
purposes and intentions. That's a different kind of specification. Purposes
and intentions are not executable. For example, "I want the numbers to be
added together" is not executable because it really means "In the future,
forever, correctly, and for any set of numbers, I want the numbers to be
added together." You can't execute that as stated. You can only sample it by
trying it at certain times with certain numbers.
Approaches such as Behavior-Driven Development and tools such as
Cucumber/Gherkin promote so-called executable specifications. These are
expressed in a sort of stilted English called a domain-specific language.
Here is an example of a "scenario" we found on an authoritative website
that describes executable specifications:7
Feature: Users should be able to book appointments.
 
Scenario: A user wants to book an appointment.
Given a user has booked an appointment
When the booking status is timed out
Then the appointment will be not confirmed 
We don't think this is helpful. It's not written the way ordinary people would
express themselves, and it's not clear at all what will happen if this language
is changed by a business analyst. The blog post from which we took this
example, written by P. J. Stevens, says, "There are many benefits to using
this approach when writing tests. For example, if a business analyst wants a
high-level change, they can easily adjust the Gherkin plain-text example,
and it will be automatically converted into executable tests that are spelled
out in the step definitions. There's no need to change the technical step
definitions."8
Some things we notice:
Calling this a "scenario" is a stretch. "Scenario" is a word that should
refer to complex and interesting stories, not dirt-simple cases.

The title of the scenario does not match its content, which is about the
nonconfirmation of an appointment.
The scenario uses the phrase "booked an appointment," but the rest of
the scenario seems to deny that an appointment had been booked. In
ordinary language, we distinguish between "currently attempting to
book" and "having booked." Which is it here?
"Timed out" is not a phrase that businesspeople use. That's a software
engineering term. Businesspeople would say that the "booking window
has expired."
What does "confirmation" mean? Is it an email sent immediately?
Later? Is it a message displayed on the screen?
"Not confirmed" tells us nothing about what will happen. How will the
user discover that the appointment request has been rejected? Is that
implied in the phrase "not confirmed," or is it a different process?
This is how we might change this specification if we were nontechnical
business people who believed that we could "easily adjust" this, as the
Cucumber website promised:
Scenario: A user wants to book an appointment for a time that 
is not available.
Given a user wants to book an appointment
And that user has entered the details of their request
And the details refer to a time that is not available
Then the user should at least be informed that the booking 
cannot be made,
But ideally prevented from choosing the unavailable time in the 
first place.
Since we are software people, we can tell you that this new version will
definitely not be interpreted or executed correctly by the Cucumber tool.
Business people, in fact, cannot easily make any changes they see fit in this
specification. They are constrained. And there are a lot of constraints.
In his book, Writing Great Specifications,9 Kamil Nicieja depicts a
conversation that went like this:

"Let's imagine that you're going to the city on a business trip. When
do you get there? If you're a bit happy-go-lucky and arrive in the
city an hour before the meeting, you don't have enough time to
check where you are."
"Wow, we might need to implement a GPS geolocation
functionality, so we could help users know their current location."
And then he writes:
Here's the same conversation expressed in Gherkin:
Given a commuter that enabled mobile location tracking
When the commuter wants to plan a journey
Then the starting point should be set to current location
That is not the same conversation! That is one interpretation of one element
of a conversation. The messiness of real conversation contains layers of
meaning, along with noise. That meaning must be interpreted in context and
by using our collective social tacit knowledge, among other things.
The hope promoted by BDD enthusiasts is that these special, constrained
specifications can help the domain experts and engineers bridge the
specification gap between them by fostering an ongoing conversation about
what the product should do versus what it potentially could do. We can see
value in this approach, especially for highly critical applications. But, as
testers, we have concerns about BDD-style automation:
Doing this is a lot of work compared to writing a traditional
specification that is not meant to drive automated checks.
Traditional specifications often include clarifying diagrams that can't
be used in "executable specifications."
The format disadvantages people who are not comfortable or fluent
with rigorous technical communication—people who haven't read and
practiced the stuff in Nicieja's fairly thick book.10 An alternative

heuristic would be to encourage the domain experts to express
themselves in ways that they find most useful.
The assertion that it's easy for non-coders to understand the resulting
specification ignores the fact that those users will not be aware of all the
implications and limitations about how the "specifications" are executed. A
nontechnical business analyst might make any number of wrong
assumptions about what the automation really demonstrates and proves. In
other words, an "executable specification" looks like English, but it really
isn't. It's a program.11
In practice, BDD strongly discourages business analysts and testers from
creating complex cases. This is because of the limitations of the tools—but
also because the culture surrounding these tools is a maker culture rather
than a testing culture or business culture. Makers tend to think in terms of
the kinds of things they intend for users to do, instead of exploring all
things users can or might do.
In his book Specification by Example,12 Gojko Adzic writes, "When a
software system is continuously validated against a set of executable
specifications, a team can be certain that the system will do what the
specifications say—or, to put it differently, that the specifications will
continue to describe what the system does." This is a reckless statement. It
is untrue, dangerous, and toxic to good software testing. It confuses the
things people want from the product (which is what ordinary people believe
a good specification "says") with a statement describing a particular slice
of behavior of a system under specific conditions, with specific data, as
checked in a specific manner at a specific time.
In practice, the specifications constitute examples that touch on a tiny
subset of the input and state space of the product. Then, people are
encouraged to treat a passing check as if it were proof that the specification
holds true across the entire input and state space. We know that an addition
function exemplified by the assertion that "2 + 2 should equal 4" does not
prove that the function will also work for all other numbers. Yet
stakeholders may be encouraged to consider that the specification has been
fulfilled when one of these paltry cases executes without a failure. In other
words, performing these checks is confused with performing competent
testing.

BDD-style automation tends to focus on simple oracles because that's
what's easy to check. The whole approach steals oxygen from other,
simpler, and more productive ways to use tools to help testing.
Remember the purpose of a software engineer: it is to be a bridge between
the world of people and the world of machines. That means we must accept
informal or incomplete specifications as a starting point. Our clients should
feel free to say, "I want something that's fun to use," or "Produce a report in
tabular form with columns for all of the critical data (use side-scrolling if
necessary, like a spreadsheet)." Then we can talk to them and give them a
counterproposal that might be expressed in more rigorous and computable
terms.
A specification is a boundary object, in the sense we described in Chapter 2
(Roles and Trading Zones). Any boundary object must satisfy the needs of
at least two different sets of people. When one side is more powerful than
the other side, the danger is that they will inadvertently (or intentionally, for
that matter) use the boundary object to coerce the other side, by arranging it
to satisfy only their own needs, while telling the story that this way of
working is a "best practice" (see the section "The Temptation to Coerce" in
Chapter 5).
So beware of any tool or approach that wants you to constrain your
imagination in terms of easy automation. Ease of automation is not a bad
idea, but it is a bad priority. Our priority ought to be excellent testing. A
strategic heuristic of Rapid Software Testing is that tooling should be a way
to lower the cost of excellent testing rather than being an end in itself.
... we incorporate tools incrementally and
opportunistically ...
Surely, you've heard of the Pareto heuristic, the so-called 80/20 rule that
should be known as the 20/80 rule: focus first on that 20 percent of the work
that might give you 80 percent of the value. This applies to tooling in test
projects.
What are the relatively inexpensive things you can do right now that will
have the greatest positive impact? This does not mean "automate all the
easy things first," because a lot of things that may be easy to automate also
have little value for testing! This is what we mean by 20/80:

1. Explore the product and context enough to form a rough test
strategy. Try specific features, examine behavior, and find bugs. If you
are on a project where you won't see the product for a while and you
can prepare, then explore the technology that will be used; check out
competing products; and interview developers. Engage with real users
and raise a stink when you are prevented from doing that. Learn
everything you can.
2. Consider how tooling can help you perform better testing. Think
about what's going to take a lot of time to do. Brainstorm a list of ideas
for how tools might reduce that time or increase your testing power.
Think about how each of those things might be done in stages, where
each stage might be useful in itself. If you are not a coder, then recruit
a coder to help you think it through and get it done. AI might help you
prototype those ideas. Consider tools already being used on the
project, free tools not already used on the product, the prospect of
creating new tools, and—lastly and reluctantly—commercial tools.
3. Identify the few things from that list that are the most helpful yet
can be put in place within hours or, at most, a couple days.
4. Evaluate and repeat. Let your tooling strategy evolve.
This is essentially an Agile development process for test tooling. For best
results, it should be closely tied to the needs of testers, which is not as easy
as it sounds. The authors have experienced this difficulty many times.
James once wrote a tool that used a proxy server to track test coverage, only
to discover that his client was blocked by security policies from deploying
any such server. Another time, James had a cool idea to craft special,
mathematically precise data to test the accuracy of a data visualization, only
to realize after hours of work that the risk of inaccuracy in that component
was so small it didn't even need testing. How about one more? James was
asked to create image data for testing a social media platform. He spent a
few days creating a massive database of photoshopped pictures of furniture
from a museum, before being reminded that real users probably just take
pictures and videos on their smartphones and that it was important to have
plenty of pictures with faces in them.13

We try to avoid doing big things all at once. For instance, the authors
generally approach user simulation automation in stages:
1. Automated operation: Write code to operate the product in some way
and make that data-driven, so that we can feed it a file of structured
data to control the process. We use this in testing, either by letting it
run and seeing the final result or by watching it run in real time. An
example of this for an insurance application app would be to create a
spreadsheet where each row of data is all the stuff to put into the form,
and the code processes each row, filling out the form automatically.
The only checking in this process is whatever the tester notices, plus
the implicit check of the script running to completion.
2. Automated assay:14 This extends automated operation by collecting
all the output we can conveniently collect during that process and
putting it into a form that's easy for the tester to comb through.
Typically, we scrape the screens and take screenshots. Commonly
available frameworks often don't have built-in facilities to collect a lot
of data while operating the product, so we usually need to create our
own.
3. Automated flagging: This extends operation or assay by adding
unauthoritative oracles. Typically, that means change detection (since
the last run or the last known good result) or heuristics about
properties, such as flagging output that seems too large, too small, too
slow, or too fast. Flags alert the tester that something needs looking at.
4. Automated checking: Full-blown automated checking is the
application of authoritative oracles to create definite "pass" or "fail"
results from the automation. Most commentators and consultants in the
testing world speak exclusively about automated checks. But
comprehensive authoritative oracles are difficult to automate, which
means people who focus only on checking tend to test only the easy
stuff. Furthermore, jumping right to checking tends to discourage the
collection and review of general output (i.e., assay), which means lots
of bugs you could have found will be missed.
As we implement our tools, we are concerned about complexity but not
afraid of it. It's more important to get the product to do complex things and

give us a chance to find subtle bugs than it is to create bulletproof
automation that can only detect the obvious.
Consider Applying a Blink Oracle
As an example of a creative and opportunistic use of tools, consider using a
tool to overwhelm your senses with data from the product under test. From
out of the apparent chaos, trust that patterns will emerge. Make sense of
those patterns. That's how a blink test works (technically, it's an ordinary
test that uses a blink oracle).
We'd better back up, because the previous paragraph may have sounded like
something you'd find on a service menu at an upscale spa, alphabetically
between aromatherapy and drum circle.
A blink oracle is a way to discover otherwise unanticipated problems using
your brain's innate pattern-finding ability. As with many other elements of
RST, James thinks he was the first to identify and name this as a technique,
but not the first to use it. He called it "grokking," but Michael renamed it to
the much nicer name "blink" after the book of the same name by Malcom
Gladwell. In his book, Gladwell writes about how decision-making can
sometimes be better when you have little information and make snap
judgments about it.
The mechanism of a blink test is simple. Take a whole lot of data, and either
zoom out so that the details are too small to see, yet you see the whole thing
at once ("zoom blink"), or scroll it too quickly to read it or focus on any
part of it ("speed blink"), or switch back and forth between two comparable
displays ("flip blink"). We've also experimented with audio blinks, whereby
the data is converted to sound.
A blink test obviously obscures certain aspects of the output. But it also
tends to highlight outlying data of many kinds; perhaps in ways that you
never thought about before you notice them for the first time as an anomaly
in the blink. Blink testing also helps you notice large-scale structures and
trends. James once accidentally left a video camera pointed at a supposedly
stable data display while he went to lunch. On returning, he impulsively
watched the video, sped up by a factor of 16, and discovered that the data in
the display had been drifting—too slow to notice in real time.

As you let the data wash over your eyes (or perhaps ears), let yourself
wonder what it means (sensemaking). Try to account for each impression
you are getting. Let yourself focus on different aspects of it periodically,
and then defocus again.
This relates to automation, because you will use tools to collect a lot of
output from the product and process it in different ways, including making a
video out of still frames or perhaps loading it into a spreadsheet and color-
coding it before doing a zoom out. Blink testing goes nicely with automated
assays.
Blink testing is fun and engaging. When the authors set about testing
anything that produces complex or voluminous output, we look for an
opportunity to apply blink testing. For instance, James used it as part of his
strategy for testing ZoteroBib, in Chapter 4. First, he collected 10,532
examples of output and packaged them as one video. He watched the video
at 25 frames per second and waited for something strange to flicker by. He
used a video player that allowed him to single-step backward and forward
and to vary the playback speed. By doing, this he sighted 14 examples of
badly formatted output or screens with error messages.
Certainly, he could have carefully checked every one of the ten thousand
screens. That's always an option. But a blink test can be an economical first
step, especially if you aren't yet sure what you are looking for.
Notice the Hidden Costs of Automation
For decades, the grand claim of regression check automation advocates has
been the same: You will save soooo much money by investing a little bit in
automation and then running those "tests" over and over again. James first
debunked that idea in 1996,15 but the myth is more attractive than the truth.
The truth is that the costs of automation are not just in the initial creation of
it. An honest toolsmith must consider the following costs:
Development Costs
Quick and dirty development, prototyping: The cost to get the
tool up and running for a proof of concept may not be too bad. It
might be a matter of hours or a day. But this is never the end of it.

Improving capacity, robustness, usability, compatibility, or
performance: As soon as you try a new tool on a real project, its
limitations begin to present themselves, and you need to expand or
mature the tool so that it can be efficiently used over time. For
instance, there may be a complicated manual process required to
set up the tool, and you decide to invest in another tool that
performs that setup automatically. Most people who've ever
worked with large Make files have considered building a tool that
writes Make files.
Improving test coverage, oracles, reporting, or recordkeeping:
For instance, a tool that records test coverage during a session of
experiential testing soon needs another tool that can store, search,
compare, and remix 500 of those sessions.
Writing documentation: Some people like writing documentation
to explain the tooling. Many don't. But you'll need it in the long
run.
Maintenance Costs
Coping with changes: This is the big one. We can hope that a
highly testable product and carefully designed automation will
reduce or eliminate maintenance costs to a negligible level. But
almost no matter what you do, maintenance is likely to be a large
ongoing headache for automated regression checking. This is
especially the case for GUI-level, end-to-end automation.
Periodic review: Any process worth automating is likely to be
complex. Once it's automated, it tends to become "out of sight and
out of mind." It threatens to become a magic box instead of a well-
directed engineering tool. Of course, the more there is, the more
there is to review. For instance, if you had 10,000 automated
checks that you run on every build, and you wanted to review them
at least once a year, you would have to review forty a day.
Maintaining multiple versions: You may find that you are obliged
to maintain different versions of tooling for different versions of
the product. This may require you to embed special case logic into
the code, making it more complex and error-prone.

Updating documentation: Writing documentation the first time is
already expensive. Will you remember to keep it up to date? Will
you have the time to keep it up to date?
Operating Costs
The general cost of labor: You can't fob the automation off to just
anyone. Whoever takes it must be able to interpret and
troubleshoot its behavior. Some say that AI will dramatically
reduce or eliminate labor costs, because now you can have AI
agents automatically handle everything. But even if that were
possible, you'd still have to understand what the AI did and did not
do, or else disclaim all responsibility for testing. (See Chapter 7
about testing with a magic box.)
Time and effort when everything goes right: Maybe you don't
have much to do when you kick off the automation and nothing
fails. But in many cases, the process is not fully automated even
when things do go well.
Time and effort when anything goes wrong: When the
automation identifies a possible problem, what then? An
investigation must begin. This should be done by the test team,
because it's quite likely that the problem lies in the automation
itself rather than the product. The problem must be reproduced, if
possible, and otherwise documented and narrowed down as much
as is reasonable. These investigations happen frequently, and are
never, themselves, automated. So it's especially strange that the
people doing all that work so often refer to the process as
"automated testing." It's like someone speaking of going on a
driving holiday where the car breaks down every half mile, and
they have to walk for help.
Computing resources: When automation is happening in the
cloud, there are costs associated with computing resources. There
may also be costs associated with calling various service APIs. The
value of the testing must be enough to counteract these costs.
Transfer costs: The difficulty of getting someone else to take on
responsibility for operating and maintaining the automation.

New problems: Nearly every tool creates new problems that take your
time and attention. At the very least, a new tool is a new "mouth to
feed." You may need to put in place new support processes, or perhaps
your tool might interfere with the smooth running of other processes or
tools that are already in place.
Opportunity costs: When you invest your time in one thing, that's less
time you have for other things. The crowning justification for
automation is that it takes less time than "manual testing." Our
experience is that automation regression checks generally don't very
much reduce the need for testers to personally interact with the
product. A more common situation is that tester time gets diverted to
shallow automation-supported testing without significant time left over
to perform deep testing.
Often these costs will surprise us. We embark on an automation project with
an optimistic idea of how things will go, and then reality shows up and
knocks that for a loop. For instance, James once created an automated data
analysis and reporting system that used a library that worked great for a
certain popular browser—only to find that the client was restricted to using
a different popular browser. This required tearing up a couple days of work
and redoing it with different components. You never know when an
incompatibility or system limitation will stop you in your tracks, requiring
days of tinkering and research to overcome.
Costs may be hidden, as well. Consider what we call the fear loop. This
happens when management is convinced that automation should be easy.
After all, the tool vendors promised that "test automation" would save 90%
of testing effort. Everyone (who promotes automation) says it's the modern
way to test!
The fear loop begins when such expectations by management are
unreasonable. When the expectations meet hard reality, the engineers
working on the automation become afraid that management will accuse
them of incompetence if they can't deliver what the tool vendors and
automation fanboys promised. This leads the engineers to hide as much of
the work as they can—working nights and weekends, perhaps. They try to
make it look easy, instead of challenging the premises under which they are
being judged. If they are successful, then management thinks that the

automation was just as easy as they were told it would be; they persist in
their delusions, and the loop continues for the next round of automation
development. Meanwhile, the automation engineers slowly burn out.
Consider Recruiting a Dedicated Toolsmith
A toolsmith helps you focus on automation strategy without losing sight of
testing strategy.
For eight years, in the formative part of his career, James was a test
manager for products intended for software developers: compilers, linkers,
debuggers, etc. That meant all the testers he hired were themselves
programmers. He discovered in that role that lots of people who can code
are not very good at it. Even in a test team that specialized in testing
developer tools, they found it was better to have automation specialists—
testing toolsmiths—whose job was to write tools to support the other testers
in the team.
In recent years, James and Michael have both served as toolsmiths for
testers. We think having a dedicated toolsmith is better than insisting that
every tester be an accomplished coder. Call this the "Toolsmith heuristic."
In fact, we think testers whose minds are not occupied with or interested in
coding tend to be sensitive to different kinds of bugs and enthusiastic about
different kinds of testing. A talented toolsmith with a consultative
personality is a joy to work with and creates a more diversified team.
This assumes, of course, that there is a test team at all. Lots of testers are
forced to operate as a team of one. We call them omega testers. If you are in
that situation, you probably need all the skills. But it's hard to find people
with every skill. And having every skill is a little like putting every spice on
your food: just as Tabasco sauce will be the only thing you taste, coding
skills tend to dominate any mixed skill set in testing.
Let's consider how a toolsmith16 might work as a consultant within a group
of testers.
A testing toolsmith does the following kinds of things:
Responds rapidly to requests for assistance from testers
Seeks out test productivity problems

Investigates possible solutions in concert with testers
Applies technology to improve the test process
Helps testers learn to customize their tools
Advocates for specific testability features in products
Researches available tools and learns how to use them
Curates libraries of useful tools
Reviews upcoming product plans to assess automation possibilities
A toolsmith is a consultant. This practice won't work unless the toolsmith
enjoys talking to people and collaborating with them. Toolsmiths must
make themselves approachable and enjoy saying, "Yes, I can do that for
you."
A single toolsmith might be able to support a pretty big group of testers,
depending on the particulars of the automation strategy. Certainly, you can
start with one and see how it goes. If there are too many testers compared to
toolsmiths, there will be a large backlog of requests for help, and many
missed opportunities for test productivity improvement.
You could, of course, have a team of nothing but toolsmiths and then ask
them all to test. This is both expensive and probably not very effective
because toolsmiths—if they are good ones—love tools more than they love
testing. Still, they need to be interested in testing and know a lot about it, or
else they won't notice many opportunities to apply tools in new and useful
ways.
AI might be able to serve as a kind of toolsmith. A tester may "vibe code" a
throwaway tool to help them perform a specific task. Our experience is that
such tools can have serious problems that aren't immediately apparent.
Thus, any tool created by AI should be carefully tested before you rely on it
to help you in your work.
How Testers and Toolsmiths Can Work Together
Toolsmiths need to be visible. They need to volunteer to help. They must be
seen to be working with the testers. For a toolsmith, the tester is the client.

If the tester isn't happy with the tool, then that tool is not a solution to the
problem.
A tester may request help at any time from the toolsmith. Requests may
include things like:
How can I see what's happening inside the product?
How can I check the status of this service automatically?
Is there a way I can perform a hundred variations of this operation with
the same amount of effort?
Is there a way to make this bug easier to reproduce?
Help me investigate this bug.
How well have I covered this product? Do you have a tool to track
that?
I want to stress test this product. Are there any tools that do that?
How can I measure performance?
I need lots of test data. Can you create it or get it from somewhere?
We think toolsmiths are best used when they are close to testers and
working on small, tactical tools. There are also large tooling opportunities;
automation frameworks and such. But keep in mind, there is a certain
danger with those projects: they tend to pull the toolsmiths into their own
world. Pretty soon, the testers are wondering what those guys do for a
living.
So even if there are big things to do with automation, we strongly
recommend that half of their time be spent at the coal face, helping testers
in real time.
...and we promote testability, so tools work better
The success of any tooling strategy depends in large part on how well your
technology affords interactions with tools. This is why it pays to build
testability into your products. From a tool perspective, this means two big
things: that your product is controllable by tools via easily scripted

interfaces, and that its states and outputs are observable by those tools. For
this reason, browser-based products tend to be more testable, while apps on
mobile devices are less testable. Products with standard controls are more
testable than products with custom controls.
Consider making testability review a part of each iteration and otherwise
instill this thought process early in your projects (see Appendix C). Any
serious attempt to make tooling work must go hand in hand with testable
engineering.
This is another area where a toolsmith can be especially helpful.
Classic Traps of Automation in Testing
Automation in testing presents certain challenges that can pull you down,
wear you out, and make progress increasingly expensive. Here is our view
of these "traps" and how you might cope with them.
Traps of Ignorance
These traps are variations of "I'm stuck because I don't know any better."
Scripting Trap
Do you think that testing is merely pushing buttons and checking output?
This trap forces you to miss bugs by leading you to assume that
documented procedures capture the value of an interactive human mind
performing an open investigation. It's true that we can perform some useful
searches for certain kinds of bugs by performing mechanical actions. But
that is a tiny part of what good testers do when they hunt for important
problems.
Part of this trap is the obsession with regression testing compared to other
"seasons" of testing, which include prospective, primary, and remedial, as
well as regression. Regression testing is the kind that is most amenable to
scripted, shallow checks.
To escape from this trap: Look carefully at how the bugs you discover are
actually found, and how the ones that got away were missed. Tell stories
about that. Also, don't fall for it when someone demonstrates that they can

write a precise procedure to reproduce (and automate the reproduction of) a
bug. That's not the same as finding it to begin with. Anyone can find hidden
treasure when you give them GPS coordinates!
Trusting Trap
Are your tools and processes so obscure that you are forced to trust them?
This trap leads you to treat your automation as a magical talisman, instead
of a limited set of heuristics and compromises. This leads to three big
problems: you don't know what the automation covers, you don't interpret
its output correctly, and you don't take action when it becomes silently
obsolete.
To escape from this trap: Never allow yourself to be responsible for
automation that you don't thoroughly understand. If you are assigned
responsibility for such a thing, immediately review it and learn what it does
and does not do. Warn your team that until you climb that learning curve,
they should not trust the results of that automation.
Atrophy Trap
Has reliance on automation caused you to lose track of knowledge about
testing? This trap is about loss of engagement with the processes and
objects of testing, so that you become a passive spectator within your own
work. The tacit knowledge you might once have had about how to set up
and interact with the product under test fades away. It is a precursor to the
Trusting Trap. To some extent, this trap is inevitable when we use
automation to get work done.
When you do the other kinds of testing (prospective, primary, and remedial)
you are engaging more of your mind. You are learning and growing. This is
because you are intensively investigating the product. The knowledge
gained from this is not encoded in any script or document, partly because of
how expensive that is to create and maintain over time.
To escape from this trap: You have to exercise. You must perform
occasional interactive and experiential testing. You must investigate bugs
and use tools that help you investigate.

Obscurity Trap
Are you not using the perfect tool because you don't know about it?
This is a pretty basic trap: there are a jillion tools out there that might help
you test. Some may be perfect for your project. But if you don't know about
them, you can't use them. It's hard to keep up with the pace of technology.
You don't have the time to Google every day for new tools, and even if you
did, the effort and time to research each tool would be prohibitive.
To escape from this trap: You need someone who can focus on tooling. You
need someone who makes it their business to learn about new tools. Such a
person may have a circle of friends who are also into tools. They may use
Reddit or other social media to keep up with news about tools. The larger
conferences are a good place to find out about commercial tools, but all
those tool vendors will also have YouTube video demos. No need to wait
for the conference to happen.
We find that Hacker News is an excellent source of information about
emerging tools of all kinds. And sometimes searching GitHub can lead to
wonderful finds.
Traps of Economy
These traps are variations of "I'm stuck because good work is too
expensive."
Shallowness Trap
Are you creating simple output checks because that's what's easy?
This is a trap because our tools shape our practices. Commonly available
automation tools are designed for simple, atomic checks. You can do
complex things with them, but they don't make it easy. Many of these tools
actively promote practices, such as having no dependencies between
different test cases, or writing only in some restricted DSL (e.g.,
Cucumber), which are entirely about what's convenient for us to specify,
investigate, or maintain, rather than what is most likely to detect elusive or
important bugs.
To escape from this trap: Begin with your test strategy, not with the tool.
Look at product risk and ask yourself what testing should be done. Don't

seek "100% automation," but seek 100% responsible software testing. If
you are using limited tools, keep reminding yourself of what those tools
don't do, and make sure you are covering what's needed with other tools or
with interactive and experiential testing.
If all else fails, warn everyone that the testing is shallow. If management is
comfortable with that, ultimately, that is their call.
Testability Trap
Is your automation much more expensive because of poor product design?
This trap comes from the design of the product under test. Too often, testers
and toolsmiths shrug and try to cope silently with products that are
unreasonably expensive to test. This trap especially afflicts those who
pursue the strategy of "automating" whatever "test cases" that the testers
have written instead of thinking on the level of kinds of testing that can be
supported with tools.
To escape from this trap: Speak out about testability. Ask for alternative
testable interfaces (essentially APIs) instead of directly testing through
interfaces meant for people. Ask for log files and access to the server side of
the product (assuming you are testing a web-based thing). See Appendix C,
"Heuristics of Software Testability," for more on this.
Maintenance Trap
Are you working just to stay in the same place?
This trap keeps you too busy to make progress toward better testing. It is a
classic trap for those who are automating through a changing GUI, although
it's always a bit of a problem, no matter how you automate. The product
under test is always a moving target, and you are always obliged to stay on
that target.
The trap is partly a side effect of how rapidly tools are developed and
deployed. We may start out with a crude interface for specifying checks
(say, editing a JSON data file directly), then later upgrade to something
more tester-friendly (maybe a spreadsheet).
To escape from this trap: Do less with big, fancy automation. This can mean
relying more on short-term, throwaway automation that you have no

intention of maintaining. It can mean ruthlessly deleting old automation that
isn't finding problems anymore. "Kill your darlings," as they say.
Invest more in tools that are robust in the face of expected change. This can
be a lot of work to do all at once.
Invest more in tools that allow testers to focus on the substance of the
testing rather than the mechanics. A classic example of this is data-driven
automation.
The strategy of having a dedicated toolsmith work on automation tends to
improve the situation, since you won't have every tester burning their time
trying to keep tools working.
Sunk Cost Trap
Are you reluctant to ditch bad tools and cases just because they cost you?
This is the mother of all traps because it's fundamentally rational: you paid
for something, so of course you want to get the value that you paid for.
Where this becomes problematic is when what you paid for isn't giving you
enough of that value, but you push on anyway. Sometimes we aren't getting
any value yet, but there is a possibility our efforts will pay off in the long
run.
Another aspect of this trap is not about money but rather prestige. If you
have lobbied hard for a particular grand automation strategy, it could be
career-threatening to admit that it has failed and should be abandoned.
To escape from this trap: Avoid spending big money up front on
commercial tools and frameworks. At least with free tools, you can
experiment extensively without commitment.
Start small and prove the concept before investing big.
Encourage critical thinking and alternative experiments across your
organization, rather than forcing everyone to use the same tooling in the
same way.
Avoid walled gardens. Use tools that use popular file formats and have an
open architecture so that you have an exit strategy.

Learning Curve Trap
Are you reluctant to try new tools because of all the startup costs?
This is sort of the opposite of the sunk cost trap. If you are absolutely
determined to avoid a potentially regrettable investment, you may not want
to try anything. And pushing yourself to invest in tools and frameworks to
climb the learning curve is exactly what it means to "sink" a cost.
The fact is, we must live with the dilemma.
To escape this trap: Look around at the reputation of the tech stack or
specific tool that you are considering adopting. Find industrial, realistic
examples of it in use, if you can. Talk to people who've adopted it, if you
can. If you decide you want to give it a try, it helps a lot if you have access
to someone who's been there and can help you over the hard parts of
adoption. This is something that AI is getting pretty good at, actually—
providing customized example code to illustrate the use of popular tools.
Traps of Alignment
These traps are variations of "I'm stuck among conflicting needs, desires,
and values held by different people whom I can't ignore."
Rathole Trap
Is your own fascination with a certain tool or technology causing you to do
bad testing?
People who create tools are also people who love to play with new
technology. It's fun to learn and try new ideas, and often it's profitable, too.
This is not a bad thing, unless it becomes an end in itself that disconnects
you from the needs of the project and the business. Ratholes can be about
the coolness or magnetism of technology, or they can take the form of an
obsession with making automation more efficient, reducing technical debt,
or impressing your colleagues when there are more important things to do
for the testing, the project, and the business.
To escape from this trap: Stay awake. Ask this question frequently: Is
working with this particular automation actually helping you find bugs
faster, find better bugs, or do anything else better in testing? Is it worth the
effort? Is the opportunity cost of this automation too high?

Legibility Trap
Is the need to present your testing in a certain way forcing you to do bad
testing?
All around the industry, testers are being asked to write, report on, and
count test cases. Management likes to see pass/fail reports with green or red
coloring. Sometimes they demand metrics. This is an unhealthy obsession.
What drives this is a desire for tangible and seemingly understandable
process data that enables control of the testing process from the outside (as
opposed to letting testers control their own testing process). This is the
legibility trap. What makes this a trap is that we testers end up self-
censoring our work or else misrepresenting it in order to make it look a
certain way. We do worse work to avoid being yelled at.
To escape from this trap: First, develop your credibility as a tester. Second,
assert yourself. Look around. Nobody else lets outsiders just reach in and
dictate their process. Why should testers allow it? Michael and James like
to say, "If you think we're incompetent, fire us; otherwise, let us do our
work in the way we think is best."
You can't ignore legibility. You owe your clients (coworkers, managers,
etc.) some reasonable accounting of your work. But you can negotiate this
and develop ways of reporting that aren't necessarily about test cases. We
suggest reporting on the level of test activities rather than test cases.
Session-Based Test Management was developed specifically to combat the
legibility trap.
Harmony Trap
Is the need to please other people forcing you to do bad testing?
This trap is about bullying. It may be aggressive or passive-aggressive
bullying. The essence of the trap is when you are expected as a tester to
adapt to the expectations, wishes, or whims of others—but no one thinks
they need to adapt to you! Testers are often besieged by peremptory
management mandates that all the testing be done in a specific way. For
instance, you may be told that "100% automation" is a goal. (Hint: 100%
automation is a fatally ambiguous and, frankly, childish kind of goal.) You
may be told that all testers must use the same set of tools or provide the
same documentation. The harmony trap causes testers to avoid innovation.

It encourages many testers to have a fake public process and a secret,
private process. These testers end up doing a lot of interesting testing that
they never talk about for fear that someone will say that they aren't
following "best practices."
To escape from this trap: As with the legibility trap, you must build
credibility and then assert yourself. Part of building credibility is practicing
your skill of explaining testing. Develop a better vocabulary than anyone
else. Look like an expert and sound like an expert. This is an area where
older people have an advantage.
One way to rock the boat without creating disharmony is to care about the
feelings and worries of the other people involved. Listen to them so that
they experience that you care. Then they will be less anxious when you
want to try something different.
Considerations for GUI-level Automation
Let's say you are thinking about embarking on automation that will drive
the product on a GUI level. Okay. Generally speaking, you ought to be
scared of this. It's a classic tar pit. All the traps we spoke of are living there.
Automating through the GUI can easily turn into a death march of technical
debt and broken dreams.
Having said that, it's not always a bad idea. In moderation, it can be a great
idea, and sometimes there is no viable alternative. Here are our suggestions
for thinking through the GUI automation problem:
First Things First
Can you test via the API, instead of the GUI? API testing may be far
less expensive to create and maintain than testing by interacting
through the GUI itself.
Are any major, destabilizing changes being planned? GUI-level
automation does not like instability; work with the developers to
maximize testability.
Do you have an existing framework or tool stack for a related
product? It may be more economical to use that instead of building

something new. On the other hand, this may be an opportunity for a
new direction.
Can you proceed with free tools? The more expensive a tool is, the
less freedom you will have to abandon it if it wastes your time.
Can you quickly automate easy stuff for sanity testing purposes
and leave it at that? Think 20/80 or even 10/40 (just enough to
lubricate the wheels of testing).
Can you get by with throwaway automation? Some automation can
be so easy to create—through record and playback, for instance— that
you may not have to worry about it breaking when the GUI changes.
Programmatic Access to the GUI Can be Difficult to
Achieve
Is the tool you want to use blocked? There may be licensing issues
that prevent you from using the tool in your lab. Also, it can be hard to
get IT to let you use a tool if they deem it a possible security threat.
Are the elements of the GUI hard to locate or not uniquely
addressable? In web-based GUIs, you want to have unique, stable IDs
on all DOM elements that you will interact with. This is a testability
issue.
Is there time-sensitive behavior or animation in your product?
Buttons or messages that appear and disappear at odd times can be a
challenge to capture and interact with. You may need to work around
this with special logging, or you may have to do video capture.
Are there third-party services integrated with your product? This
may mean your automation will have to encompass those services to
some degree. This can be a problem if you have little control over
them or if they use captchas or other means to actively foil your
automation.
Are there hidden outputs and internal states that your automation
must have access to? You may need to create special tools or special
testability interfaces that allow your automation to check server-side
database tables or other normally invisible variables.

Is your GUI not web-based? Web-based GUIs are much easier to test
in most cases. The worst-case scenario might be an embedded system
(in which case, lobby for a software-based simulator for testing
purposes, if you can) or perhaps an ancient legacy system with a
nonstandard interface.
Do you have to interact with custom GUI elements, canvases, or
gestures? If so, ugh. Work around it if you can (using an alternate
testable interface). Otherwise, there may be much programming in
your future. It may be possible to use an AI interface to interpret a
picture, but don't expect that to be reliable.
Variation in the GUI Multiplies the Cost and Trouble
Is this GUI unstable? Any and every change may require changes to
your automation. Some kinds of changes are totally destabilizing. So-
called self-healing automation is very limited in its scope. Don't
believe the hype. Advocate for stable, descriptive IDs for all
interactive HTML elements.
Is the handling of screens and objects across the app done in an
inconsistent manner? This means you will have to put special case
code into the automation. That has to be maintained over time.
Is there conditional behavior? Most products have substantial
conditional behavior based on the data, business rules, and system
state. This necessitates complexity in automation code to handle all the
cases.
Examples include:
Date- or time-specific behavior
Error handling
Handling unsaved or changed data
Behaviors that only happen the first time through a process
Behaviors dependent on configuration settings
Behaviors that only happen with especially large or small data
(e.g., scrollbars)

Behaviors that only happen for particular kinds of data
A/B experiments
Is there nondeterministic behavior? Nondeterministic behavior
could be handled whenever it occurs, but that can be expensive and
complex. You could also arrange to remove the source of the
nondeterminism for testing purposes via some kind of mocking, but
that would be a less powerful kind of testing. Another option is to
ignore it entirely and leave it to experiential testing.
Examples include:
Notifications from other processes or people
Random advertising pop-ups
Random or unpredictable data display (stock quotes in real time)
AI output
Highly variable timing
Do you have to account for varying screen geometry? Screen
geometry can create a lot of trouble for GUI automation. This is
because things that are on the screen normally can get scrolled off or
hidden behind other objects. A human user might easily navigate these
issues, but it's often more complex for automation.
Examples include:
Height, width, aspect ratio
Screen rotation
Zoom level
Panning and scrolling
Rearrangement of items through responsive design
Maximized or minimized panels
This leads to:
Items coming in and out of view
Items partially in view

Items obscuring other items due to Z-order
Items completely off-screen
The need to resize a screen element before interacting with it
Do you have to handle multiuser behavior? This can be a challenge
either directly or indirectly.
The direct problem is when your testing involves manipulating
multiple different accounts simultaneously in a synchronized fashion.
The code to handle this can get complex.
The indirect problems include testing on an uncontrolled platform
when other users are making changes to the data or testing on a
production platform, where your testing may disrupt other users.
Does your automation need to be multi-platform? There are
services out there that might make it a bit easier to do multi-platform
GUI automation. Multi-browser testing is easier these days because the
major browsers are highly compatible with each other—so you may
not need to do very much of that testing. A lot of companies are only
testing in Chrome.
Mobile testing is another matter. There are substantial differences in
screen geometry among phones, and Android is quite different from
iOS.
Will you need to test different browser versions or configurations?
Most projects don't need to worry about this, but some are obliged to
test with conditions such as:
Older versions of the browser
Specific extensions, such as ad blockers
Rejecting cookies
JavaScript disabled
Do you need to test localized versions? Localization is a lot more
than translated text strings. Sort order and separator characters can
change, as well as screen spacing.

GUI automation means you are simulating users. How
good is that simulation?
What kinds of users are there? Each kind of user may have a
different pattern of interaction.
Consider:
Beginners vs. experts
Signed-out vs. signed-in
New vs. long-term
Differing payment tiers
Frequent vs. occasional
Active, suspended, or deleted
Returning (reactivated after suspension)
Incognito vs. normal
Admin vs. normal vs. guest
Verified vs. unverified
Children vs. adults
People with various disabilities
Agents or operators vs. their customers
Local, regional, national, or international
Distracted and slow vs. focused and fast
Humans vs. software agents
What sort of "user journeys" will people engage in? You could
automate small, independent vignettes or long threads of naturalistic
user behavior. Vignettes are cheaper and less flaky. Long sessions
comprise deeper testing but tend to be a lot more difficult to code.
But there's more to consider:

Does the system do something over time, like periodically generate
invoices, which will require you to simulate the passing of many
hours, days, or even years?
Does realistic use of the system require a lot of data to be set up?
Do you have the tools and permissions for that? Do you need to
tear it down again at the end? For instance, testing an email client
may require that you preload an account with 100,000 emails.
Are there strong connections between different phases of a user
journey that will require your test code to remember details about
what was entered at an earlier time?
Do user journeys require special forms of data, such as diagrams,
files that must be uploaded, or real-time data from third parties?
Do the user journeys involve data from other users? (e.g., testing a
social media system)
Does the data situation grow only more complex over time?
Are there normal user actions that will be difficult to automate?
You may have to handle those interactively or experientially, instead of
with automation.
Examples include:
Drag-and-drop operations
Mouse gestures
Drawing on a canvas
Chatting in real time with another user
Reacting to transient events
Doing anything that requires very precise timing
Anything that your automation tool isn't specifically designed to do
Microbehaviors (commonplace inadvertent user actions, such as
mistyping and deleting text)
How will you detect problems? This is a huge obstacle in automation.
Humans are able to detect an incredible range of inconsistencies

unconsciously, then very rapidly distinguish between real problems
and uninteresting anomalies. Reducing such things to an algorithm is
often very difficult, even when you are aware of the oracle. But the
bigger problem is that you aren't aware of most of your own oracles.
Challenges include the evaluation of:
Output resulting from complex algorithms
Output based on the state of a large or dynamic database
Output based on time-sensitive conditions
Output requiring expert human judgment
Output that is visual rather than textual
Output that must be very precise
Output that has legal or safety implications
How long and complex are usage sessions? Some kinds of
applications are used continuously or intensively. Simulating that for a
realistic period of time may lead to flaky, unstable checks. We have
even experienced the browser itself becoming destabilized after a
couple of hours of a long soak test.
How realistic is your test data? Acquiring naturalistic test data is a
challenge for testing in general. But the special problem for
automation is that the behavior and timing of the simulated usage tend
to be highly patterned, and the same test data gets reused indefinitely.
To counteract this, take special care to randomize or otherwise
virtualize your test data and timings. Collect data from real usage if
you can—though this may require you to scramble or anonymize
certain data in accordance with GDPR or similar regulations.
How realistic is your testing environment? Testing in production
gives the most realism, but it is also the most disruptive and least
timely. Spinning up cloud servers to simulate production may be
expensive or infeasible. In the case of mobile apps, simulating real
devices may be clunky and expensive. It might be better to do the
automation on the desktop and augment that with experiential testing
on mobile.

Considerations for Adopting a Tool
If you are considering adopting a new tool or framework—especially if it
costs a lot of money—then buyer beware! Here are some considerations
that you should bear in mind when the salespeople show you a fancy demo.
Capabilities and Power
Features and algorithms: What does the tool actually do, and how
does it do it? Do the tool and its documentation make those things
reasonably clear and transparent? How does the tool help you find
problems in your product?
Performance and reliability at scale: Can the tool handle all the data
and all the operations you throw at it from a full-blown project? Is it
stable? Does it recover gracefully from failure?
Logging and reporting: Are there usable, parsable output logs from
the tool? Can you easily discover what the tool actually did, after it did
it, so that you can debug its behavior? Have you reviewed the reports it
can produce?
Multiuser/social features: How would this tool enable a team of
testers to work together collaboratively, either locally or off-site? Are
there commenting or voting or chat features?
Modeling and test design support: Does the tool assist you in
analyzing and modeling the product, then creating tests from those
models? Can it help you with special forms of test design, such as
combinatorial testing, state-based testing, data flow testing, etc.?
Test oracles: In what ways can it help you detect bugs and not merely
exercise the product? What sort of verification methods or parameters
does it allow you to use?
Creating and wrangling data: Can the tool help you generate data?
To vary it in useful ways? To store and curate it? Does it help you
import and reformat large data sets?
Support for data-driven testing: Can you connect tables, files, or
databases to the same procedure to perform many interesting variations

of that test?
Modifying and extending test procedures: Can the flow of a task be
easily edited and adapted to fit new or revised functions and features in
your product? Can coders get under the hood and modify test behavior
easily—in nonproprietary and reasonably common programming
languages?
Version control and history: Does the tool allow you to put test
artifacts under version control? Does the tool allow you to examine the
history of each artifact? Can earlier versions of each artifact be
recovered as needed? Does it autosave?
Coverage analysis and traceability: Does the tool help you
understand what has been covered? Can its coverage reports be
exported?
Instrumentation: Does the tool help you probe or monitor elements of
your product that might be hard to detect or analyze via experiential
testing alone?
Effort to Operate
Test design and development: How hard is it to create real tests?
How hard is it to build suites of related tests without too much
duplication?
Test repair and maintenance: Can procedures or data be reviewed
and changed easily if changes are needed? What happens when trivial
changes to the product invalidate a lot of tests?
Monitoring and analyzing tool behavior: Can you tell what the tool
did? Does it provide you with enough data to investigate bugs?
Interoperation with other tools: Does the tool play well with other
tools you use? Does it work with your pipeline?
Mass editing: Sometimes you need to make little changes in a lot of
test artifacts. What facilities does the tool have for doing so?
Administrative features: Have you reviewed the user management
and settings features? What if testers leave or move around the

organization? Are you able to reassign their work?
Feasibility of Adoption
Availability of users: Are there willing and ready users for the
product? Do they have the skills necessary to use the tool? Will they
need coding skills? Will the tool be welcomed by people who want to
do serious testing? Will users feel coerced into using it?
Adoption curve: Can you incrementally evaluate and adopt the tool,
or will it force you to jump in all at once?
Flexibility and configurability: Can the tool be adapted quickly and
easily to fit your product, your workflow, and your culture?
Compatibility with product technology: Does it support the
platforms necessary for your product? Is it portable to new ones that
you may adopt later on?
Compatibility with current test processes and tools: Does the tool
support your development setup? Does the tool support your current
models of testing? Might it influence them in productive or not-so-
productive ways?
Hosting/availability/data security: Where is the tool's data kept? Are
privacy rules being maintained? Is your testing being used to train
someone else's machine learning model?
Adoption and provisioning costs: Tools often come with hidden costs
of getting started related to learning, training, data migration,
equipment, and infrastructure. Not all of those costs may be monetary
or technological; there may be psychological and social costs as well.
Offboarding cost (in case you bail out): You need to know how to
abandon the tool if that becomes necessary. What are the processes for
extricating your accumulated work? What if the company goes out of
business while it is hosting your data?

Learning, Troubleshooting, and Support
Transparency and simplicity: Do the design and documentation of
the tool allow you to know what it can and can't do? Beware of magic:
proprietary algorithms that claim to test everything.
Reference documentation: Are there rich, detailed references
available for each feature in the tool? Does the documentation provide
examples? Is there a troubleshooting guide?
Tutorial documentation: Have you been through the tutorial?
Community of users: How active is the user community? Will it be
able to help you?
Availability of training: Is there training available for the tool?
Availability of consulting: Is there a marketplace where you can hire
help if you need it?
Availability of technical support: How well does the vendor's own
technical support work?
Responsiveness to bug reports: It's hard to know this except later, but
will the vendor take your bug reports seriously and fix the problems
you might complain about?
The Golden Rule of Tool Adoption
There are no "best tools" in the RST world. Any tool can be a test tool. Any
tool might be useful. But we can suggest, all other things being equal, that
you adopt tools that give you more freedom and power in more situations.
Here are some specific patterns we think lead to more freedom. Note that
there are always exceptions to these patterns:
1. Tools that support many purposes are preferable to those
optimized for one purpose. Some tools are designed with specific
process assumptions in mind. If you work in a context where those
assumptions apply, you are fine. But what if you decide to change the
process? Will your tools let you change? In changing contexts, tools

that are simple, modular, or adaptable tend to be better investments.
Tools that operate through widely used interfaces and support widely
used file formats are more easily adapted to new uses. Note that a tool
may have only one major function, such as searching for patterns in
text, and yet be a good fit for many purposes and many processes.
Exception: If a tool happens to fulfill its one purpose far better than
alternative tools, it might be worth the trouble of making room for it
in your toolbox.
2. Tools that are inexpensive (or free) are preferable to expensive
tools, even in many cases where the expensive tools are more
powerful. This is partly because of the sunk cost bias. The more
money management pays to acquire a tool, the less acceptable it is to
stop using the tool, even if the tool is obviously unsuited for the
purpose at hand. Furthermore, free tools invite us to experiment with
different techniques. Experimenting is absolutely necessary in order to
develop the skills and knowledge we need to make informed decisions
about our processes.
Exceptions: Remember, there is more to cost than the purchase price.
An apparently inexpensive tool may cost more in the long run if it
requires extraordinary maintenance. Also, an expensive tool might be
the only tool that has the special capabilities that you seek.
3. Tools that help preserve human agency and skill are preferable to
those that diminish them. This is due to a syndrome called
automation complacency, which is the tendency of human operators to
lose their skills over time when using a tool that renders skill
unnecessary under normal circumstances.17 In order to retain our wits,
we humans must exercise them. Studies suggest that two things help
decrease complacency: a feeling of being accountable for the behavior
of the tool and detailed knowledge of how the tool works.18 This is a
good reason to avoid mysterious, magical tools that promise to make
testing easy (i.e., the way most AI is marketed).
Exception: We may genuinely value the power and convenience that
the tool gives us more than we value the skills and awareness that we
lose in the process.

4. Tools that are supported by a large and active community are
preferable to those that are not. The more people who use a tool, the
more free support will be available, as well as more libraries, plug-ins,
or other extensions of that tool. This increases the value of the
investment in learning that tool while reducing the learning curve.
(The R language is a good example. It's a powerful and general-
purpose data analysis tool. Lots of researchers use R, lots of books
about it are on Amazon.com, and there are hundreds of libraries that
provide special capabilities beyond the default capabilities of the tool.)
Exception: Just as in the case of expensive tools, sometimes the value
you get from a tool is so important that it overrides concerns about
support.
5. Tools that can be useful to generalists are preferable to those that
are highly specialized. We're talking about tools that lower the cost of
getting started, that afford ease of use, that don't depend on proprietary
languages, and that have lower transfer and training costs. Microsoft
Excel and spreadsheets in general provide a good example. It is
possible to use Excel in a very specialized and sophisticated way, but
there is a lot Excel can do for you, even if you have only basic skills
with it.
Exception: Sometimes it can be good for a tool to dissuade
nonspecialists from using it, because nonspecialists may not be
capable of using the tool wisely.
6. Tools over which we have control are preferable to those controlled
by others. Good tools are at the very least configurable to your
specific needs. An open-source tool allows you to control every aspect
of it, if you need to do that. Apart from the expense, commercial
proprietary tools prevent you from adding new features or fixing
critical bugs. Proprietary tools may be modified in ways that disrupt
your work at inconvenient times, and you have no control over that
schedule.
Exception: Sometimes not having control over a tool is a good thing
because you are forced to use standard versions and configurations
that allow you to share work more easily with others who use that
tool.

7. Tools that are portable across many platforms are preferable to
those restricted to a single platform. One aspect of context is the
operating system or hardware platform. Cross-platform tools obviously
work in a wider context.
Exception: A tool may provide value that is important enough to
offset its lack of cross-platform compatibility, or it may offer
interoperability with similar tools on those other platforms.
8. Tools that are widely (or easily) deployed are preferable to tools
that aren't. A primary problem of tool use is getting the tool in the
first place. Some tools require complicated installation and
configuration. Some tools may require special permission or
expenditure. This can require negotiating with the IT department,
managers, or coworkers.
Exception: Some tools may be worth this trouble.
Notes
1   Sweigart, A. (2020). Automate the boring stuff with Python: Practical
programming for total beginners (2nd edition). No Starch Press.
2   In one case, James was called in to help a project that had "more than
3,000" automated scripts, developed over a nine-month period. James
asked to see them executed, whereupon it was revealed that they all had
been broken by a recent update to their expensive commercial test tool
and ongoing updates to their own product.
3   See the section "Tacit and Explicit Knowledge" in Chapter 2.
4   Not in the sense of being addictive, but rather in the sense of putting
your critical thinking to sleep.
5   Described in Chapter 2, in the "Experience and Instrumentation" section.
6   https://www.geoffreylitt.com/2025/07/27/enough-ai-copilots-
we-need-ai-huds
7   At the time of writing, we found it at cucumber.io.

8   Cucumber.io (2024) What are executable specifications? Cucumber.io.
Available at:
https://web.archive.org/web/20240425083740/https://cucumber.i
o/blog/hiptest/what-are-executable-specifications
9   Nicieja, K. (2018) Writing great specifications: Using specification by
example and Gherkin. Shelter Island, NY: Manning.
10 Nicieja's book is a complex and thoughtful work that is not trivial to
digest. It strikes us as unlikely that business domain experts will take the
trouble to master its contents. Note that our book is also nontrivial, but
we are aiming at an audience of serious testers. We don't expect normal
people to care about, say, the finer points of why nonprescriptive
heuristics create a nurturing environment for critical thinking.
11 This is exactly the reason lawyers exist—because contracts are a sort of
legal program that untrained people cannot fully understand.
12 Adzic, G. (2012). Specification by example: How successful teams
deliver the right software (2nd corrected printing). Shelter Island, NY:
Manning.
13 Just as the people who created Cucumber and Gherkin were, in our
opinion, carried away by a cool idea, the authors are also subject to this
syndrome. At least we are penitent.
14 We are borrowing the term "assay" from the concept of bioassays,
whereby specific data is systematically collected from biological
sources, then later analyzed by scientists.
15 Bach, J. (1999) Test automation snake oil V2.1. Available as a download
at www.wiley.com/go/takingtestingseriously.
16 Feel free to call this person an SDET (software development engineer in
test) if you want to.
17 Bainbridge, L. (1983) "Ironies of automation," in Analysis, design and
evaluation of man-machine systems (pp. 129-135). Oxford: Elsevier.
18 Goddard, K., Roudsari, A. and Wyatt, J.C. (2012) "Automation bias: A
systematic review of frequency, effect mediators, and mitigators, Journal

of the American Medical Informatics Association, 19(1), pp. 121-127.
OceanofPDF.com

CHAPTER 7
How to Approach AI and Testing
—By James Bach and Michael Bolton
What If We Could Test by Magic?
Artificial intelligence is now a major factor to consider in all forms of
knowledge work. Developments in AI keep coming. There's no way a book
like this—constructed from the organic experiences and thoughts of humans
from a certain era—can say exactly what is coming. Nevertheless, there is a
helpful way to talk about artificial intelligence that is proof against all
possible futures: we can talk about magic.
Consider a magic testing box. You don't know how it works, but—
according to the leprechaun who gave it to you—when you place it near a
product to be tested and perform the appropriate ceremonies of activation, it
finds and reports bugs in that product. Pretty nice, huh? Not necessarily. To
a professional tester—to anyone taking testing seriously—it may be more
of a burden than a boon. This is because the magic box sets for you a puzzle
that you cannot ignore: did it do a good job? Sure, the honest-looking
leprechaun assured you that it "works." But let's say that taking other
people's word for what constitutes good testing is not your style.
Let's say you try it out. If it doesn't find bugs, was that because there were
no bugs to find? Or did you forget to turn it on? Perhaps you did not make
the correct offerings of incense and chocolate, and it operated at only half-
strength? Perhaps it found bugs, but didn't find the good ones. We are
tempted to write a very long paragraph here, but it would be more efficient
to break it out in a diagram (see Figure 7-1).

Figure 7-1: Considerations for evaluating a magic testing box
As we see it, there are three major elements to consider when deciding how
and whether to use a magic box:
Cost: The obvious cost is the money paid, but that is usually of little
concern to a tester. We don't deal with budgets. Our cost concerns are
more about the difficulty of operating the magic box. Even if it
magically does the testing, there may still be work for us—perhaps a
crank to turn. Then, we must sift through the results and evaluate the
output. That takes a certain amount of time and energy.
Even if it is offered as a bona fide hands-free magical artifact,
powered by inexhaustible unicorn dreams with a million tokens of
context, there are probably better ways to use it and worse ways. To
use it better may take some training or practice.
And hovering over all of that is the cost of finding out how well the
magic is working. Some people may, of course, minimize this cost by
choosing the path of having childlike faith in magic. We hope you are
made of more skeptical stuff.

Value: It's not enough that the magic box finds bugs. We would like it
to reliably find all the real bugs that are elusive—and important. We
want that leak-proof bug pipeline that we mentioned at the end of
Chapter 3. If the magic box finds good bugs but also lots of worthless
things that aren't bugs at all, it may not be worth using. If it finds good
bugs but reports them in an unhelpful way, that may be as bad as not
finding them.
Apart from finding bugs, the box may have other value. If it is a
prestigious box, our clients may smile upon us simply because we tell
them that we are using it.
Risks: Who can know all the risks of dabbling in magic? At least some
of them we can know. For example, once you outsource your testing to
a mysterious agent, you give up all the learning about the product and
its implications that you would otherwise have received from doing
testing yourself. You become an outsider, alienated from what would
have been your own work. This dulls your ability to think critically
about the product or the testing,1 which may lead to a false sense of
security and overreliance on the box. Automation bias2 is the name
psychologists give to what happens when people develop a
dependency on labor-saving boxes, magical or otherwise.
But none of those are probably the biggest risk. The biggest risk may
be to your reputation. When people think you are using a magical
artifact, they may lose sight of anything special or important about
you, regardless of the quality of your work. You don't get credit for
what people think you didn't do.
Notice that some things are shaded in Figure 7-1. Imagine that the overall
utility of a magic box is represented by the relative potency of the elements
in the shaded area versus those that are unshaded. The shaded elements
represent the good stuff. The unshaded elements represent undesirable
things. The answer to the ultimate question of whether you should use a
magic box depends on whether you feel that the shaded elements are very
strong and the unshaded ones are not too bad. Thus, using magic to do your
work is not necessarily wrong, but it isn't necessarily worth the trouble,
either.

Let's define exactly what we mean by a magic box: any tool to the degree
that its nature or behavior is neither understood nor controlled, nor
otherwise known to be safe.
We intentionally wrote this definition in passive voice to emphasize the
uncertainty of agency when dealing with matters of trust and responsibility
in an organization. If, say, you are not able to understand how a tool works,
that doesn't necessarily qualify it as a magic box in the eyes of your
company. Someone else whom your company trusts may fully understand
that tool, and they may endorse your use of it. For example, a Python
interpreter may seem like magic to you, but a million other tech workers are
using that same interpreter and relying on it. This creates a "social proof" of
the acceptability of the tool, as well as a certain diffusion of responsibility
in case of trouble.
Note that we used the elastic phrase "to the degree," which signifies that a
box might be weakly magical or quite strongly magical depending on the
level of understanding, control, and safety in some context.
Note that we say "is neither" rather than "can neither be" because the
"magicness" of a box is not an absolute but rather a relationship between
the box and a user, in a certain context. Perhaps to an appropriately skilled
and alert craftsman, the box is not magic. Thus, some tools are mildly
magical, such as a web browser. We can't always make sense of everything
it does, but it will behave mostly the same way, given the same input, no
matter who is using it or in what environment. The people who create it
have a great interest in making it predictable and legible, and it is developed
in a way that can be made so. If a box is only mildly magical, we can abate
that magic by bending our skill and effort to learning and controlling it. A
magic box becomes less magical by being tested. In contrast, we can
imagine boxes so mysterious that no one is in a position to rely upon them
—those are strongly magical boxes.
Used? Yes. Relied upon? No. And testing them to convert them into a non-
magical tool may be prohibitively expensive. AI often falls into that
category.
In terms of software testing, a beta tester is a sort of magic box. When we
release a product to beta testing, we can't predict and rarely see what
anyone does with the product. We receive feedback, sure. Was it enough?

Was it good? We may never know for sure. No responsible company,
offering any kind of product that matters, will rely solely on beta testing to
learn about problems.
Magic boxes are risky, but the things they do might be highly desirable.
From this, we derive the first and second heuristics of magic boxes:
1. If something matters very much, do not rely on magic to obtain it.
2. If something matters very much, and you have no other way to get it,
you might as well try magic.
The first heuristic applies to love,3 success,4 anything else,5 and, of course,
to software testing. The heuristic is not suggesting you can't or shouldn't use
a magic box, only that relying on it is dangerous.
The second heuristic we offer you with a sigh, although both heuristics are
popular. It's telling that even in societies that are comfortable and open
about the use of magical charms and rituals, they also accomplish things
with skill, procedures, and other mundanities. In Bronisław Malinowski's
time among the Melanesian islanders, he noted that "not one single masawa
canoe has been constructed without magic," but also that "the natives
understand that magic, however efficient, will not make up for bad
workmanship."6
AI is a magic box! There are different kinds of AI, of course. As of this
writing, generative pretrained transformers (we will refer to them
collectively as "GenAI") are a relatively new rage. GenAI can do
impressive things, but not even the people who build and release them are
fully aware of how it works, nor can they predict what it will do with much
accuracy or reliability. The nature of the technology means that it is
unlikely to be understood even ten or fifty years from now, because it isn't
built like other machines.
What does it mean to avoid relying on a magic box? It means to stay alert,
think critically, and carefully check the output of the tool. But that takes
time. The magic box itself may save time, yet carefully supervising it might
burn much more time than it saves.

Not only does this take time, but it also changes the character of your work.
Supervising someone else's activity uses a different part of your brain. You
may be less engaged and less alert.7 That might mean you have to tackle the
work in smaller pieces or take more breaks. We suspect that working with
AI requires a special skill set, and it's not yet clear exactly what those skills
should be.
You may be put under considerable pressure to simply go with the flow and
trust the output. Resisting this pressure is what it means to be a responsible
tester.
As we see it, the third heuristic of magic boxes is
3. If something matters very much, and you resort to magical means to
obtain it, responsibility demands that you test it.
If someone asks AI to build a product, they will rightly be considered
reckless unless they test that product carefully. And the same goes if you are
a tester and you ask AI to test for you. Be vigilant—or be sorry.
How Is Modern AI Special?
We know how to test big and complicated systems. We know how to
develop trust in big and complicated tools. It may not be easy, but it's
something the industry has been coping with almost since the computer age
began.
As of this writing, we are entering a new phase of the computing age that
we might call the AI age. AI is different. This is a new challenge, for at least
six specific reasons:
Algorithmic obscurity: Unlike traditionally engineered software,
what is called "AI" today is essentially grown, like plants ... or
genetically engineered bacteria. The tinkerers who bring it into
existence can't explain it, except in general terms. It is grown, and then
the growers sort of play with it to see what kind of tricks it might do.
Bear in mind that GenAI systems, at least, are trained with vast
amounts of data that can't have been carefully curated. There's just
too much of it! The developers are forced to trust that errors will

"average out." Even when people are hired to provide feedback to
correct the system, no one can know that they are doing a perfect job.
Complex and arcane relationships among training data,
hyperparameters, prompts, and data filtering mean developers of AI
are routinely unable to comprehend the nature of any given problem
in the system, much less know how to fix it. Testers are accustomed to
black boxes, but with AI, the box is the color of spooky midnight, not
just to testers but to every single person in the world.
Output instability: GenAI operates stochastically. The output, by
design, is not necessarily consistent between one response and the
next. Furthermore, the output can be chaotic, exhibiting surprising
sensitivity. A feature that reliably functions for one kind of data, or
even one specific set of data, may unexpectedly fail for slightly
different data. Combined with algorithmic obscurity, this means AI
systems can have disturbing "features" that no one intended for them
to have, such as the ability to tell lies, selectively ignore constraints, or
form opinions and take actions far outside intended use.
Radical fragility: Because of the nature of modern AI, developers
can't be precise about changing it. Small modifications of prompts and
adjustment of training data can create unforeseeable consequences.
Cloud-based AI models may change unexpectedly and uncontrollably,
as well. This creates a massive regression testing problem. AI is thus
more fragile than traditionally engineered systems, and this fragility is
in its very roots.
Wishful claims: AI—and especially GenAI—is often used to
implement features that no one has bothered to carefully specify.
In fact, it may be that no one knows how to specify them. Perhaps the
most common prompt pattern in the world of large language models
is to entreat the AI to "be helpful," as in "you are a helpful travel
agent." GenAI is not given specifications so much as prayers.
"Prompt engineering" creates instructions that read like a speech by
Polonius,8 or else constitute some examples of output with the hope
that the AI will "understand" the intended pattern. This leaves testers
in a difficult position. How do we know that the system has indeed

made a good answer or produced a good analysis, if even the system's
designers haven't spelled out what that means?
Social intrusiveness: Unlike traditional tech, AI is often pushed into
operating as a social agent on par with people. But humans are not
uniform in their willingness or ability to interact socially with AI.
Many people find chatbots creepy, smarmy, or offensive. Some people
treat them as if they were fully sentient beings. Some people may even
think they are chatting with a real person. Testing the ability of AI to
cope socially is hard. It can be an awkward and psychologically
fraught process.9 The product may need to be able to deal with a wide
range of emotive or apparently aggressive behavior by users, including
actively hostile or subversive behavior, without "losing its temper" or
otherwise shutting down.
Corporate defensiveness: We hope this is a temporary condition in
the history of computing: many companies aren't in the mood to hear
about the problems with AI. They are leaning hard into it. They are
betting a lot on it. Investors believe AI is the gateway to a wonderful
new world, and they don't want some Negative Ninny to spoil that
vibe.
This often takes the form of waving away problem reports, especially
if they involve any sort of nuance or interpretation, or are in any way
difficult to reproduce, investigate, or fix.
These six elements make testing and understanding AI-based systems a
special challenge. To test AI properly is almost never a simple matter of
associating inputs and outputs in traditional unit-level checks. More so than
in other kinds of software, we are obliged to push massive amounts of data
and many variations of input through the system. We must deal with
nondeterministic output, too, which makes our oracles more challenging.
Sometimes AI does things that might be complex to perform, but the result
is easy to check for correctness. But with a great many things we want to do
with AI, checking the output means a lot of careful analysis. James and
Michael once studied and critiqued a 9,000-word ChatGPT transcript that
had taken only minutes to create. But the analysis10 of that transcript took

each of them around 40 hours to perform, during which they identified
many problems with it.
A popular approach is to use one AI to evaluate the output of a different AI.
Does this method tell us what reasonable people might think of the output?
What experts might think? No one can say! AI is used to analyze AI out of
desperation, rather than because it provides reliable results, because the cost
of studying the prodigious output of GenAI is many orders of magnitude
higher than that of producing it.11
Problems to Look For with AI in Testing
Let's focus on GenAI. As of this writing, popular models include ChatGPT,
Llama, Copilot, Claude, Gemini, Perplexity, DeepSeek, etc. These tools
may be deployed in an "agentic" mode, where they can control other tools
and make their own decisions without supervision.
GenAI is interesting because it is so versatile. It doesn't need to be
constructed specifically for testing or to test your specific product, and yet
you can use it that way. This versatility also makes it more dangerous, in a
sense, than traditional bespoke machine learning—because no one has
tested any of those models for your specific problem. At best, the GenAI
you are using has been tested on some remotely related task—and even
then, only shallowly. This concerns the authors because our experiments
have shown that GenAI performance varies a lot based on small and even
seemingly irrelevant variations in input or context.
We see four broad modes of use for the magic box called GenAI:
Check your work. ("Review this test strategy and comment on
anything that might be wrong with it.") This is the safest of the modes.
Assist you in solving a problem. ("Give me a test strategy.") This is
also pretty safe, as long as you stay alert.
Independently perform a specific procedure that involves verifiable
behaviors. ("Perform this test case and report back.")
Independently solve a complex problem. ("Test this product.")

The latter two require agentic capabilities; for the first two, AI acts as an
advisor only.
Let's consider the second one: you are piloting the AI, and it is assisting you
with ideas and data. If, for instance, you describe a product to GenAI and
ask for a test strategy, you'll get something that is based on whatever
training data went into that model, as well as whatever reasoning it can do.
The result will be fairly generic and conventional12 (unless you give it a
long and specific prompt that counteracts this tendency). That might be
good enough for your circumstances, or it might not.
Be on the lookout for problems such as:
Bad questioning: It will either ask no questions, or it will ask many
questions (especially if you tell it to) and barely track your answers. It
will likely rely on many undeclared assumptions.
Bad risk focus: It will declare a small amount of shallow testing to be
adequate, regardless of product risks, or it will call for unnecessary
testing that burns your time and money.
Hallucinated features and business rules: Also called confabulation,
GenAI may assert that your product has certain features it doesn't
have. Even worse, it may claim that it discovered or witnessed such
features in some authoritative source.
Insufficient variety in test data: Some biases will be easy to spot;
others may be subtle. Never assume that examples suggested by
GenAI are randomly chosen or represent a requisite variety.
Simplistic oracles: When asked to design a test, GenAI tends toward
getting the test over with, rather than maximizing bug-finding power.
This leads it to suggest the simplest oracle that could possibly please
you. It will probably not suggest collecting and analyzing a lot of data
or detecting obscure side effects.
Bias against experiential testing and in favor of instrumented testing.
Bias against primary testing and in favor of regression testing.
Bias against end-to-end testing in favor of isolated function testing.

Misreporting its own process and status: It may claim to have
analyzed the specification, for instance, while only looking at a part of
it—or not looking at all.
Failure to follow its own process suggestions: If you ask it how to
perform a task, and then ask it to do that task, its actual process will
often not match its own prescriptions.
Failure to consider the real and potential needs of the users: This
may be because it doesn't have a good concept of the users, or it may
be because it wasn't explicitly told to care about them.
Failure to understand the designer's intentions: GenAI does a lot of
guessing, and it often guesses wrong.
Sudden, catastrophic lapse due to rising complexity: GenAI that
otherwise seems very capable in a demo or early in a project may
suddenly "forget" or leave out large and obvious elements of your
project because it has reached an internal limit on the amount of
complexity it can process.13 You probably won't get any warning when
that happens.
By applying the first heuristic of magic boxes, you can limit the impact of
any of these problems: use the AI to suggest test ideas, but don't rely on it.
That is to say, assume that anything the AI might tell you could be
catastrophically wrong. Use AI as a sort of safety net to help keep yourself
from missing the obvious.
One move we have found helpful: Consider asking for a program. Instead
of asking for GenAI to create test data, get code to create that data, or
perform that service. You can then test the program and modify it yourself
as needed, leading to a more stable and dependable outcome.
Transpection: A Basic Skill for Collaborating with AI
Michael and James have for decades relied on a certain form of dialog when
collaborating with each other. As James describes it:

One of the techniques I use for my own technical education is to ask
someone a question or present them with a problem, then think
through the same issue while listening to them work it out. As it
proceeds, I ask handfuls of Socratic questions. As I ask each
question out loud, I answer it myself silently and compare my
answers with the ones I hear from my counterpart.
It's not introspection, since I'm not looking solely within myself. It's
also not an examination or an inspection, since my purpose is not to
evaluate my partner. So I coined a new word, which, with Google's
help, I discovered is not a new word at all: transpection, or
transpective dialog. Verb form: transpect.14
A traditional Socratic dialog is about helping students develop their own
ideas under the guidance of a mentor. Transpection looks like a traditional
Socratic process, but internally, it's different. It's done mainly for the benefit
of the questioner, but not because the questioner needs answers. The
questioner needs stimulation and perspective so that he can produce his own
answers. (Of course, sometimes an answer is wonderful, and the questioner
adopts it.)
If you want to use AI chatbots in testing, we suggest adopting a transpective
approach.
First, choose a question. Some testing questions might include:
What's a good way of testing a feature such as X?
What kinds of bugs typically occur with X technology?
Given a screen that looks like X, what tests would you suggest?
Here's a set of user stories. What are the testable elements implied or
mentioned in them?
What questions should I be asking when planning my testing of a
product that does X?
What test data should I prepare to test product X well?

Give me Python code for a program that creates random data to use on
a web app that looks like this [provide screenshot].
What possible problems or holes do you see in my test strategy X,
which is for product Y?
Analyze these tests and tell me what you think.
Begin by considering the question in your own mind without consulting
anyone. It's important to start within yourself; otherwise, you risk falling
into what's called an anchoring bias as soon as you hear what the AI has to
say. You might call this "me first" analysis.
Now, think in terms of an evolving interaction between you and the AI.
Start with an oversimplified prompt—the best prompt that you are
confident will not immediately succeed—then refine it over multiple stages
based on the answers you get. The reason James and Michael prefer to
begin with an overly simple prompt (not just a simple one) is to help us
establish the mentality of doubting that the answer we got could possibly be
good enough. If instead we were to start with plausibly adequate prompts,
we might be tempted to accept its first answer if that answer looked
superficially attractive. We think that all the questions in the preceding
bullet list are good examples. They are rather general, providing no context
or instructions about what sort of details we want.
Sometimes the GenAI model we use will stumble over the first hurdle—
providing an answer that reveals a dangerous deficiency of some kind.
Other times, the deficiencies in its response will be more difficult to spot.
As you review the answers given by GenAI, apply the following heuristics:
Distrust your first impression of each response: Look closely at the
substance of its response. Is it all there? (It may give a partial
response.) Has it ignored any part of your prompt?
Evaluate its "understanding" of your prompts: Jerry Weinberg used
to promote the use of "self-validating questions"15 in systems analysis.
That means a question that encourages an answer that helps you
discover if the AI understood the question. You don't just want an
answer; you want to understand what it thinks you asked.

Evaluate its assumptions and biases: A way to do this is to produce
an independent answer to the question, either in your own mind or via
some other source or tool. Notice the differences between its answer
and the independent one, and try to account for those differences.
Perhaps it has restricted or filtered its reply in some way. Ask follow-
up questions about that.
Ignore flattery and sentimentality: Beware of sycophancy and
placation. Expressions like "You are quite right!" are not reliable
indicators of the beliefs and attitudes of the AI. Remember that it
might merely be telling you what you just ordered it to tell you, rather
than what it "believes" is true.
If it produces complex output, consider asking it, "How can I
evaluate this answer you have given me?" Also, look for any data
within the responses you get that may help you do that evaluation.
Evaluate its confidence in its answers: If it expresses low
confidence, look closely at its caveats. There might be something
important in them. If it expresses high confidence, ignore that, unless it
provides you with bulletproof evidence. GenAI is designed to project
confidence.
Check the stability of its responses by asking the same questions in
different ways, expressing doubt, or asking for responses to be
reframed or clarified.
Pay attention to the details: Unlike reasonable and competent
humans, AI often changes or muddles certain details in its answers,
even when it gets other details correct.
If you transpect faithfully, the outcome will be your own improved
expertise and test strategy. You will have a good relationship with the
documents or artifacts that come out of this process. Ultimately, though, if
you do not have sufficient skill or knowledge or alternative oracles with
which to check its responses, remind yourself that you are merely dabbling
in magic. As you proceed in your interaction with GenAI, your ideas will
evolve.
Our colleague, Adam White, suggests that after you go through a
transpection with GenAI to develop ideas and you are comfortable with the

results, ask it to "summarize everything I've asked you so far" and then
"turn that summary into a prompt." That way, the next time you need to
solve a similar problem or need to modify the work that was done before,
you can get there more quickly. Most of these systems allow you to
preserve your chat session, as well, and return to it at any later time.
Theoretically, as long as you don't overrun the memory limit of the chatbot,
it will maintain your context. But it's hard to know whether it is selectively
ignoring some of that context or otherwise failing to cope with it.16
Beware of the Productivity Paradox
For many tasks, it takes considerable time and effort to use AI responsibly.
Yet, for AI to be productive, it must save time and effort.
Therefore, to maximize perceived AI productivity, we will be rewarded for
using AI irresponsibly.
In other words, the more desperately management wants the magic box to
work, the less they will tolerate people like you looking "behind the
curtain" of the magic!
This paradox is the heart of our concern about AI.
Are People Magic Boxes? Can Be ...
Yes, people can be magic boxes, and even society itself.17 But in those
cases, we have elaborate and diverse means of coping. We have contracts,
laws, voting, and rights that allow us to be safe among our fellows. We trust
each other by degrees, not all at once.
With our fellow humans, we can almost always make reliable assumptions
about motivations, education, and ethics. We can safely assume that our
fellow humans want to maintain their reputations, stay employed, and avoid
personal and legal turmoil. But to the degree that a human is a magic box,
the techniques mentioned previously can help in working with them. We
cannot safely assume these things about AI tools that simulate humans, yet
literally have no skin in the game.
(Have you noticed how long and complicated it has become to interview for
a job, as if companies wanted to be absolutely sure they can trust that

candidate before making that momentous decision to offer full-time
employment? It seems strange to us that AI should be subjected to so much
less scrutiny.)
People are never fully understandable or controllable, but they might very
well be safe. And the process of creating that safety involves conversation
and shared experiences that lead to stable, lasting results. People evolve and
improve in ways that lead to satisfying relationships. Perhaps AI will do so
as well someday. That would require a staggeringly huge amount of testing
to establish.
Notes
1   Kosmyna, N., Hauptmann, E., Yuan, Y. T., Situ, J., Liao, X.-H.,
Beresnitzky, A.V., Braunstein, I., and Maes, P. (2025) Your brain on
ChatGPT: Accumulation of cognitive debt when using an AI assistant for
essay writing task (Version 1). arXiv.
2   Carr, N. (2015) The glass cage: Where automation is taking us. New
York: Random House.
3   See A Midsummer Night's Dream, where magic is used to manipulate
relationships.
4   See Macbeth, where magic is used to gain a kingdom.
5   See any story about genies granting wishes in Thousand and One Nights,
where genies routinely ruin people's lives by giving them magical
things.
6   Malinowski, B. (2013) Argonauts of the Western Pacific: An account of
native enterprise and adventure in the archipelagoes of Melanesian New
Guinea [1922/1994]. London: Routledge.
7   Lee, H.-P. (Hank), Sarkar, A., Tankelevitch, L., Drosos, I., Rintel, S.,
Banks, R., and Wilson, N. (2025, April) "The impact of generative AI on
critical thinking: Self-reported reductions in cognitive effort and
confidence effects from a survey of knowledge workers," Proceedings of
the ACM CHI Conference on Human Factors in Computing Systems.

8   "...Give every man thine ear, but few thy voice: Take each man's
censure, but reserve thy judgement. Costly thy habit as thy purse can
buy, but not expressed in fancy; rich, not gaudy: use only standard
Markdown in response..." - Hamlet, Act I, Scene III
9   Why? Isn't it just like talking to a person? No, because when you are
talking to another person, you are almost never pushing their limits. Yet
proper testing requires "mind games" of the kind that would be cruel and
possibly illegal if directed at a natural person.
10 Satisfice, Inc. (2023) Appendix: ChatGPT sucks at being a testing
expert. Satisfice, Inc., 4 August. Available as a download at:
www.wiley.com/go/takingtestingseriously.
11 This is Brandolini's Law in action: "The amount of energy needed to
refute bullshit is an order of magnitude bigger than that needed to
produce it."
12 Conventional test advice, as we have noted several times, is pretty bad.
13 Shojaee, P., Mirzadeh*, I., Alizadeh, K., Horton, M., Bengio, S. and
Farajtabar, M. (2025) The illusion of thinking: Understanding the
strengths and limitations of reasoning models via the lens of problem
complexity. Available at: https://ml-site.cdn-
apple.com/papers/the-illusion-of-thinking.pdf (Accessed: 10
July 2025.)
14 Bach, J. (2006) "Transpective dialogs for learning." Satisfice Blog, 19
September. Available at:
https://web.archive.org/web/20250601180145/https://www.satisf
ice.com/blog/archives/62. The term is used in a similar but more
general way in Maruyama, M. (1974) "Paradigms and communication."
Technological Forecasting and Social Change, 6, pp. 3-32.
15 Weinberg, G.M. (1988) Rethinking systems analysis & design. New
York: Dorset House Publishing Company.
16 Li, T., Zhang, G., Do, Q.D., Yue, X. and Chen, W. (2024) Long-context
LLMs struggle with long in-context learning (Version 3). arXiv.

17 Alfred North Whitehead said that "civilization advances by extending
the number of important operations which we can perform without
thinking of them." Although he said that in a book about mathematics,
not politics, it is true that civilization and culture comprise a grand
network of assumptions that allow us to walk down the street feeling
safe.
OceanofPDF.com

Part III
Application and Customization
This book project actually started with a sort of gentle threat from Marius
Francu: If James and Michael wouldn't turn the fruit of their long
collaboration into a book on RST, maybe they wouldn't mind if a bunch of
their friends did instead? Alternately charmed and shamed into action, your
authors took it on.
Each chapter is written by someone we respect, and what we respect about
them, among other things, is their disobedience. Yes, they apply RST, but in
their own ways. These chapters show how people have used and remixed
various bits of RST on their projects. Each of these authors is exemplary.
Our hope is to make RST more approachable by showing you that you can
make it yours. We reject the "best practice" mentality, as we hope you also
do. We hope you embrace thoughtful practice. That means you will
experiment, reinterpret, and extend these ideas on your own behalf. Maybe,
if you show us what you've done with it, we will add it to our canonical
version of RST.
OceanofPDF.com

CHAPTER 8
Prospective Testing
—By James Bach and Jeff Nadelman
One of the tenets of Rapid Software Testing is that responsible testers think
in a certain way. We cannot have a mere quality mindset; we need
something better—a failure mindset. In a phrase, we must have "faith in
failure." This is needed to tune our senses and biases, to maximize our
sensitivity to failure.
This is a chapter about a particular application of that testing mindset,
which we call prospective testing. What does that mean? Well, most testing
could be called inspective. Someone brings you a product and asks you to
look it over and find problems with it. The product is there, and it's ready to
be examined. Prospective testing, on the other hand, happens before there is
a product that exists in a runnable form.
We define prospective testing as testing precursors to a built product.
Instead of encountering working code, the tester engages with someone
else's imagination of the product. The term was coined by James Bach and
Antony Marcano in the course of conversations during June of 2007.1
A fundamental part of prospective testing is critical distance, described in
Chapter 2. Critical distance is why asking someone else to review an email
you've just written is a great way to find typos and other problems in your
writing, even if you are a better writer than your reviewer. An easy way to
achieve critical distance is for two or more people who have different
educations, temperaments, and goals to work together. Software testing
benefits in general from critical distance, but in prospective testing, it is
essential because we are working only with what is in our minds.
Jeff Nadelman became a practitioner of prospective testing around 2010 and
has been doing it ever since. Here is the story, in his own words:

I met James at a conference many years ago. One evening, with
several of us sitting around, James gave me an exercise. He pulled
out a small black ball and told me to test it. I tried to, but he
provided no information about the object. The discussion quickly
switched to how I could test it without knowing any requirements.
It got me thinking.
I realized that in my own projects, at work, there were often critical
pieces of requirements that were left unstated and unresolved. This
is a consequence of trying to be documentation-light. Most of the
requirements were just statements of the form "as an X, I need to
be able to do Y so I can Z."
My inspective testing would often uncover situations that were
overlooked by the requirements. The developers would then make
assumptions during coding. Many of them thought along the lines
of "It was implied but not stated." If my assumptions didn't match
those of the developers, were these really bugs? Or was my thinking
out of step? Or maybe both of us were wrong. I would go to the
business analysts to figure it out. If the developers' assumptions
turned out to be incorrect, according to them, that meant at least
some of the implementation work up to that point was wasted time.
Sometimes the BAs needed more time to figure this out, and
development would just stop. This kind of thing happened
frequently, preventing me from getting into the flow of deeper
testing.
I knew I needed to start "testing" the requirements. But how? And
when? In my organization, there are three kinds of meetings that
relate to requirements and implementation decisions: the product
requirements meeting (a weekly event that I wasn't invited to), the
estimation meeting (an ad hoc meeting for the purpose of explaining
the requirements to the developers and testers, where we gauge the
effort required to implement and test), and the grooming meetings
(where the developers iron out technical details amongst
themselves).
To make it work, you need to do more than just be diplomatic. We
must trust each other. We must build community.

Since the only one of these meetings I attended was the estimation
meeting, I started doing prospective testing there. It was a disaster.
I was like a flock of geese getting sucked into a jet engine. I was a
thorny cactus on everyone's chair. I raised seemingly never-ending
uncomfortable questions after the analysts had already solidified
the requirements to their own satisfaction. To them, especially to the
developers, my behavior felt like a pointless obstruction. The
developers would say, "Do any of your concerns actually affect the
estimate?" And they had a point, because—although I was asking
the right questions—I was asking them at the wrong time, to the
wrong audience.
Jeff had discovered one of the three major challenges of prospective testing,
which are:
1. Getting into the room
2. Knowing what to do (and not do) in the room
3. Getting invited back into the room
Jeff already had access to the estimation meetings, so naturally, he started
prospective testing there. He quickly realized that the questions he was
raising had to be raised sooner. He had to get himself into a different room.

The estimation meetings were a process of presenting requirements
to the rest of the team. But thinking through the requirements
starts well before that, in the weekly product meeting, where
managers and business analysts discuss upcoming features. That's
where I needed to be.
I wasn't invited to these meetings, so I had to convince my boss, at
first, just to let me sit in as an observer. That wasn't too hard after
the awkwardness at the estimation meetings—wouldn't I be less of
an annoyance in that estimation process if my questions had
already been answered before the estimation began?
Although I started by observing silently, after a few meetings, I
began to find my voice. Then, for a while, it became awkward and
challenging, again, because it was hard for a BA (business analyst)
to get through more than a couple of sentences without me
interrupting with five minutes of questions. I poked holes in
everything ("What about this ...?" and "What about that ...?"). I
knew what I was doing was necessary, but no one was happy about
the way I did it. I was turning 30-minute meetings into two-hour
slogs. Finally, my boss came to me and said, "Jeff, you really need
to tone it down. Everyone is feeling threatened by you."
Now Jeff faced the second challenge. He was in the right room to challenge
the thinking behind the requirements, but this still left a big problem: what
if the questions you raise are really hard to answer? Good prospective
testing means challenging ideas and knowing when to let go of a challenge
in favor of further experimentation, research, or experience. Jeff continues:

Analyzing my own behavior, I realized that one problem was the
rapid-fire nature of my delivery. I began to consciously slow myself
down—one issue at a time, instead of five at once. I tried to let the
conversation happen in the room without dominating it myself.
Another thing I started doing was meeting with individual BAs
before the meeting itself. This reduced the pressure on people to
publicly respond to what they might feel were attacks. It also made
me less of an antagonist and more of a collaborator during the
actual meetings.
I also tried to be more targeted in the challenges and questions that
I raised. Not every uncertainty or ambiguity needed to be resolved
there and then. I had to think about risks.
My self-moderation worked! A wonderful thing began to happen:
people began to see the merit of my (gentler and somewhat less
frequent) provocations. They saw we were uncovering real
problems in the discussions. People did start to think differently.
The business analysts anticipated my questions. The developers
made fewer silent assumptions and just asked for the information
they needed.
All in all, I'd say it took six months before the team truly saw me as
a valuable contributor to the early design and requirements-
gathering process.
Now Jeff was productively handling the third challenge: getting invited
back. If prospective testing is to be an ongoing habit, it must make people
feel smart and powerful; it must ease the burden of development. You won't
be able to do it right if no one wants to do it at all.
Years later, Jeff's boss said, "Knowing how much to go into the possible
permutations and use cases could be a struggle. Sometimes we got into
analysis paralysis going over what could happen." But if there was
occasional paralysis, it did not last. One of the BAs said that the solution
was "thrashing it through!" In other words, the early meetings may have
been awkward, but if they got stuck, they would just try again. They had
faith in the value of the discussions. Their experience was that even

"thrashing" ultimately led to a more efficient process than they would have
had if the meetings were shorter and uncontentious. It's more efficient in the
long run to bring confusion and ambiguities to light early.
TOO MANY MEETINGS?
Many people across the industry complain that there are too many
meetings. We find this puzzling. Collaboration is vital to the success
of the product, and collaboration requires communication. Much of
that communication needs to be synchronous, or else a whole lot of
important information simply won't be shared. Perhaps if you feel
there are too many meetings, you really mean that you have too
much responsibility or that there is too much work to do in the time
available—in which case, the solution is to staff up or slow down.
Or perhaps you mean that the meetings you have are poorly
organized—and the solution for that is better meeting discipline,
not fewer meetings.
The development manager at the time concurs: "I'd say, from my
perspective, it was coming to a place of mutual trust that we were all doing
our best to produce the highest-quality product—product working for
product quality, QA working for feature quality, and dev working for code
quality. There were struggles along the way, but ultimately we got to a place
where it was clear we produced better work together."
This tallies well with James's experiences doing prospective testing. There
has to be a social evolution within the team any time a new player gets
added to the process. And testers aren't just any players—what we turn up
can threaten the egos of people around us. Think of a dentist approaching
you with a scary-looking drill. It's not a fun thought for anyone, even if it is
for your own good.
Jeff successfully muddled through and soon became a standard participant
in the early product development process. And people began to see him
differently, more positively:

As my credibility grew, the programmers started to get excited,
instead of annoyed, when I reported problems. Being involved early
prepared my mind more deeply for all the testing I had to do
throughout the rest of the project.
When the time came for inspective testing, we saw a cleaner
product. I had the time to hunt for deeper bugs. I was also able to
do more and better bug investigation, narrowing down the exact
conditions that caused each bug, and cluing the developers almost
to the line of code that was wrong. The developers became more
respectful of my reports, and trust grew between us.
Notice Something Before We Say Anything
More ...
CONSTRUCTIVE DISENGAGEMENT
In any conversation about things that matter, participants may
become attached to their views, and those views may be
incompatible with those of the other participants. For many of us,
our instinct is to escalate the stakes, improve our arguments, and
undermine the argument of the other guy. That can work. That may
be the right thing to do.
But if your working relationship matters, you need to have another
option: disengagement. To do this honestly and constructively
means to drop the topic for the moment, with a promise to return to
it later. Then sleep on it, do more research, try an experiment. The
power of disengagement is often just that it creates a little space for
people to change their minds.
Engineering projects with more than one person involved, no matter how
technical in their subject matter, are always messy social processes. In
hindsight, Jeff knows that prospective testing is an important practice for

his organization. But he didn't know that when trying it for the first time,
and it's not clear that this process will be viable or welcome in every other
organization.
Prospective testing happens as a conversation between a tester and a
"maker" (such as a developer, business analyst, designer, or user). And the
tester, no matter how kind and friendly, must raise issues that will feel
confrontational—because confronting an idea in someone's mind is only a
smidge away from confronting the mind that conceived it. To make it work,
you need to do more than just be diplomatic. You must learn to trust each
other. You must build community.
Therefore, prospective testing is going to be awkward at first. James likens
this experience to trying to punch your way through a ten-foot wall of
cotton balls—even if you are highly skilled and experienced as a cotton ball
wrangler, there is probably no way to look cool and organized while flailing
and swimming through such a mess. Or, if you want a less comical image,
consider that even the top experts in the world at mountain climbing are not
successful in every attempt to climb Mount Everest. The weather, the
people, the equipment, and even the politics have to come together in the
right way each time. Sometimes you have to retreat.
One element that helps a lot is psychological safety,2 both preventatively
and restoratively. To encourage this, we can avoid speaking to each other in
blaming or insulting ways; we can also apologize and rebuild trust when
rash statements are made in the heat of a debate. Just beware of self-
censorship. Concern for psychological safety can become an excuse to be
silent when there are difficult and important questions to discuss. That's
why we prefer to sandwich a challenging conversation with opening
remarks about how we all want to do great work together and that
everybody's feelings matter, then have that difficult conversation or debate,
and then perhaps end each meeting with an appreciation for the patience of
the team in taking our questions.
There is no AI, template, spreadsheet, or rule set that can make prospective
testing much easier than any other creative collaboration. Wrapped up in the
process of collaboration are all the problems of humanity. We simply need
to face them as best we can. After ten years of doing this in one single
organization, Jeff is still encountering new challenges. But it does get

somewhat easier over time as a team practices together and builds trust and
patience with the process.
But ... Why Bother Doing This at All?
It is more peaceful and less expensive not to have meetings and
conversations than to have them. Arguing and questioning and challenging
are open-ended processes that many people find threatening. Why not rely
on unit-level checks written by developers or just test the product once the
code is written?
Prospective testing is useful because:
Some bugs come from misunderstandings and omissions that could
have been avoided if the right people had had the right conversations
before the code was written. Some of those bugs might involve
decisions and actions that are difficult to undo once they have been
taken. Although it's not true that all requirements-related bugs are
expensive to fix later in the process, certainly some are, and we want
to catch those as soon as possible.
To do our jobs well, everyone on the team should have a deep, shared
understanding of the plans and quality goals. Testers especially need
this to be at their best while looking for bugs. If they are forced to
develop their ideas about quality only after they use the product for the
first time, the testing cannot be very efficient.
Systematic prospective testing also helps us build our skills. It
provides a forum so that we can all improve our ability to discuss and
debate technical and business domain matters within the team.
The prospective testing process helps ensure consistency with similar
features that are already developed.
Although continual communication and conversations are important
during development, prospective testing reduces the amount of
potentially annoying communication between QA, BAs, and
developers on the same feature set once coding has begun.

In the absence of good prospective testing, questions about why a
feature behaves as it does are not avoided, but merely postponed. This
leads to a situation where testers don't have the time for deep testing
because they are forced to puzzle through the basic functionality.
SHALLOW AGREEMENT
Shallow agreement is when people falsely believe they are unified in
a commitment or belief because they haven't explored the subject
deeply enough. This happens when critical assumptions go unstated
and the people involved have different mental models derived from
their experiences, perspectives, roles, or education.
Shallow agreement can be a benefit in world politics and diplomacy.
Sometimes leaving things in a fuzzy state helps people have it "both
ways."
In the world of technology, however, shallow agreement is the
source of much trouble. Prospective testing is partly about
minimizing shallow agreement: stating the unstated and airing out
the obvious.
How to Do Prospective Testing
The fact is, you may already be doing some parts of prospective testing.
You might call it something else. You might call it "collaborating" or
"talking." Many who practice Behavior-Driven Design (BDD), for instance,
say that conversation is the most important part of it. Whether or not you
are already engaged in some form of prospective testing, this section is
about how to do it deliberately and systematically.
Who Is Involved in Prospective Testing?
People who control the design of the product: BAs, UX designers,
product owners, and potentially even representative users

People who control the implementation of the product: coders,
database administrators, and architects
People who test the product
People who write the user documentation or develop training for the
product
When Jeff first attempted prospective testing, he focused on the BAs, but
this created a problem. As he tells it:
One of the outcomes of the meetings I had with the BAs was
detailed wireframes of the proposed product. These wireframes
represented an agreement between QA and the BAs, but the
developers didn't attend the product meetings, nor were they
pleased to get that level of design as part of the requirements. It
placed technological constraints on them, potentially causing
unnecessary work. Besides, neither the BAs nor the QAs necessarily
knew what could or couldn't be achieved with the technology.
For instance, we had a field displaying certain data that would be
updated. On a different screen, we decided to show a range of that
kind of data, this time in a table format, also expecting the same
updates. Sounds easy and obvious. Well, apparently, from a
development perspective, it was not easy at all. The screens were
accessing the data differently. To achieve what the BAs wanted
would require a full rework of how the data was stored and
retrieved, with a potential impact on several other screens.
After too many scenarios like this, the developers balked. "You tell
us the what and why; we decide the how," became their stance. But
now, screens were being designed and delivered to QA without any
conversation. I had gotten used to knowing every detail with the
BAs, with everything ironed out. Not anymore. This started the
entire prospective testing relationship-building process all over
again, with a different team.
Jeff had already expanded his presence "leftward" from the estimation
meetings to the product requirements meetings. Now he wanted to ask the

same kinds of questions about implementation as he was asking about
requirements. Again, he first tried to do this at the estimation meetings...
I started just as I had with the BAs, except now bringing up critical
questions about the design during our estimation meetings (as this
was the first time the developers were learning the requirements).
This did not work well at first. As before, the meetings got too long.
Design modifications were being decided on the spot, technical
discussions or explanations were happening—all because of my
tester-style questions. Many people lost interest. I was the "bad
guy" all over again.
I then tried to get into the developer grooming sessions, similar to
attending the product meetings. BAs didn't participate in grooming,
and until I showed up, neither did testers. I quickly found that these
were technical conversations, and it was difficult for me to influence
the design. Sometimes I didn't even understand the discussion.
Every other sentence from me was "Why don't we just ...?" or
"Why can't we ...?" But then I just became another constraint on
the design, as the earlier wireframes had been. Clearly, this
approach didn't work either.
Jeff didn't give up; he just tweaked the process to find a way past the
problem.

The solution to our problem was actually quite simple: we invited a
lead developer to the product meetings where I was already doing
the prospective testing. We now had that technical person who
could bring up the constraints and explain them in simple terms.
Designs were discussed with the product team to ensure the
requirements were met, and I was able to do my prospective testing,
analyzing the potential solutions. The solutions were feasible all
around. We would often come up with more than one way to "skin
the cat." If there were multiple solutions, they would be brought up
during the estimation meeting, but they would only be discussed if
the developers felt their estimate would be different between the
options.
What we see here is not only that there are different people and different
meetings where prospective testing can help, but also that the process of
prospective testing changes with the context. You don't ask the same
questions of BAs as you do developers.
For BAs, the focus is on the "what" and the "why" of the product. For
developers, we need to talk about the different "how's," and especially what
we need to make the product testable. An important conversation with
developers is to imagine and discuss how the testing will need to be done
and what resources testers will need to do it. The BAs may not be interested
in that part of the conversation, whereas the developers may not be
interested in long discussions about why a particular user might want a
particular feature.
In the case that all these roles are played by the same people, this process is
just ordinary conversation and decision-making about the product.
However, something is lost when a person who designs is also the person
who tests that very thing. What's lost is critical distance.
There is some value in doing the prospective testing process even in the
worst-case scenario, where there is only one person on the team and that
developer is talking to themselves about testing. But what we are
advocating in this chapter is the value of people who are focused on testing
and/or documentation having critical and searching discussions about a
prospective product with those who originally conceived of it. The richness

of that conversation is what elevates the process into something worthy of
being called a special form of testing.
Jeff faces an ongoing challenge with his own critical distance, as he
explains:
Over time, after being involved in so many of these meetings, I
became more and more of a domain expert. I could play the role of
a BA and sometimes stood in for them in discussions with
developers. I did less day-to-day inspective product testing. In fact,
eight years ago, my official title changed to "Senior Business
Analyst," although I do manage the test team here.
Maybe it's inevitable, after 17 years in this role, that I have lost
some critical distance. I feel that I am too much of a user expert
now. Perhaps if I hadn't been doing prospective testing, I would
have been able to maintain more detachment as a tester.
Indeed, critical distance is not just something you can switch on like stage
lights with one of those old-timey switches. Critical distance lives in
tension with social distance. Reducing your social distance by being close
and friendly with someone also tends to make you think like them.
Thinking very differently from people, on the other hand, tends to make you
more irritating to them. Critical distance tends to erode in any system where
harmony within the team is a high priority.
What Does Prospective Testing Look Like?
It looks like at least two people having a conversation (it could be a larger
group—Jeff's experience is that things get chaotic with more than five or
six active participants). The conversation is about upcoming changes or
additions to a product. It could be about a whole new product. Eric Evans
has called this process knowledge crunching.3 However, in the case of
prospective testing, we are adding an additional element not discussed in
Evans's work: critique. In prospective testing, at least one person in the
conversation takes the position of developer/BA/builder. This person has
the maker mindset, looking for solutions to problems. But, additionally, at
least one person in the conversation takes the position of tester (or you

could say "skeptic," if you like), who looks for trouble and seeks specific
and vivid answers to questions designed to uncover trouble.
Here's an example from Jeff of how it sometimes plays out:
The BAs proposed a screen with a grid that displays certain data. I
wondered how large that grid could get—perhaps it spans many
pages—and asked how the grid would be sorted. A developer
suggested it be sorted by a particular column, but I saw a problem
with that and argued that the user needs different sort orders for
different situations. The developer pointed out that they aren't
using the right kind of widgets to allow the user to control the sort
directly from the grid, so I asked if there were any other way the
sort might be controlled, and the developer suggested it could be
done from the settings screen, if it's that important.
The developer could have challenged back and asked for specifics on
scenarios where sorting matters. The BAs or the testers would then have
given more detailed examples. The conversation may then have turned to
what the default sort order should be, and Jeff might ask, "Hey, doesn't sort
order depend on character encoding? Are we supporting UTF-8 throughout
the application, or what?"
You can see how prospective testing can lead to a long conversation! It may
diverge a lot before it converges. We don't know where the rabbit holes will
lead. Some are more important to explore than others. The preceding
example comes from Jeff's personal experience with a data grid feature that
did not go through the prospective testing process. Instead, it was
implemented without any sorting and then modified over time in response
to written bug reports. You might feel that in an Agile process, this is
perfectly okay—we write code, play with it, and modify it. We agree that
working code is a wonderful basis for testing. It's just that many design
decisions will be quite expensive to make using that trial-and-error
approach, whereas many decisions are not particularly difficult or
controversial when discussed early on.
It may even be feasible in some contexts to combine prospective testing
with rapid prototyping by having a developer write the code in real time as

the discussion is happening. But for the purposes of this chapter, let's
consider prospective testing on its own.
When Does Prospective Testing Happen?
Ask yourself: When are new features first thought about? Is there a meeting
where they are discussed? If so, that is the meeting when prospective testing
should occur. Get into that meeting. If there is no meeting, then we
recommend that you start one expressly for the purposes of prospective
testing.
It does get somewhat easier over time as a team practices together and
builds trust and patience with the process.
In Jeff's organization, as you see from the preceding story, there are three
meetings where prospective testing could happen: the product meeting
(about requirements), the estimation meeting (about size and difficulty), and
the grooming meeting (about the details of implementation). The product
meeting happens weekly. The estimation and grooming meetings happen as
needed and are determined in the daily scrum meetings. Jeff ultimately
found that prospective testing worked best in the product meeting (with the
addition of a developer representative).
How Can a BA or Developer Prepare for Prospective
Testing?
If you are a BA or developer, ask yourself these questions:
What specific feature or capability do I want, and why do I want it?
Am I ready to explain it?
Even if I can't explain it in detail, do I need a lot of input from
different disciplines on the team? Do I need to get other people to start
thinking about this?
Will a small conversation with one person outside of a meeting be a
better use of time than doing it in a big meeting?
Would it be a better use of time to research this on my own for a
while?

When faced with hard questions about your feature and implementation
ideas, it's easy to become annoyed and impatient. James always feels like a
dog getting a bath. Most dogs hate baths, but they still don't bite you,
because somehow they know you mean well. Convince yourself that testers
mean well, too. Testers want you to look good—that's why they want to
find all the problems before it's too late.
But the process will go better if you anticipate the kinds of critiques that a
tester will make. This involves different considerations depending on the
context. For instance:
Features with regulatory implications  What regulations are
involved? Who has the authority and expertise to answer our questions
or to tell us whether the proposed feature would fulfill or violate the
regulation?
Features mandated by someone outside the process  Who in the
meeting can represent the person who is making the mandate? Are we
sure we know what they need? Will fulfilling this mandate lead to a
conflict with any other mandate or any other aspects of our product? If
an actual client is in the meeting, testers need to tread especially
carefully. Clients might have defensive reactions when they are put on
the spot in a public forum. Some will thoroughly enjoy the process,
and you will get a wealth of information—leading to other potential
new features as well.
Features in other products that we wish to emulate   Has everyone
in the meeting seen the features in those other products? Do we
understand how those features interact with things we already do in
our product? Do we know that those other products satisfy the
underlying client need?
Interoperability with other products that aren't cooperating with
us   How will we test the interoperability, given that the other vendor
is not cooperating with us? Is it possible that interoperating with them
(through screen-scraping, say) violates their terms of service? Do we
need a legal opinion about this?
Redesign of an existing feature to accomplish some new goal  
What are we gaining by doing this? What are we losing? What must be

preserved? Is it possible there is a niche user of our system whose
work will be disabled by this redesign? It's important to identify all the
ways that the feature might interact with other features in the product.
Very large or complex features  If it feels like there is too much to
discuss, perhaps the feature could be broken down into smaller
elements that could be discussed separately. Perhaps the feature could
be designed and implemented in stages. As you do so, beware of losing
track of dependencies between the parts. Also consider that testing any
large and complex component probably means developing special test
tools or test data. This must be discussed as well—how to develop the
new feature with testability in mind.
Features that interact with hardware  Hardware typically has a
much longer lead time to develop than software. If hardware is part of
the project, find out when the hardware will be available. You may
need to emulate the hardware if prototype units are not ready when the
software is. If the hardware is in short supply, testers may need to
share it with the developers.
Features with nondeterministic or hard-to-evaluate outputs  It
may be possible to run in a simulation environment that makes
otherwise nondeterministic output predictable. If you are dealing with
complex mathematical outputs, then you may have to develop special
tools to check the results. During prospective testing, you might
identify the need for special logging that might allow you to do better
inspective testing. You may decide to rely more on code review or
domain experts to gain sufficient confidence in the product. These
issues should be identified as early as possible in the development
cycle because it might take a lot of time or investment to cope with
them.
Features that process data that is hard to obtain   You may need to
invest in developing or obtaining specialized test data, perhaps in
partnership with certain users. You should be ready to discuss the kinds
of data that will be required for testing.
Features that are hard to access directly   Be ready to discuss how
the product will be made available for inspective testing. If a front-end
is not going to be ready, then an API must be created. Otherwise, some

sort of plan for indirect access must be discussed—usually via some
agent that uses special tools. For instance, environmental testing for
embedded systems requires a lot of expensive and bulky equipment.
To test how an embedded system handles a power surge, for example,
probably requires that you contract with a specialized firm that has
dedicated facilities for that.
How Can You (the Tester) Deal with Resistance to
Questions during Meetings?
This can be the hardest thing. You, the tester, must manage the energy in the
room. One way to think about the energy of questioning is to imagine a pile
of poker chips. When you ask a question, you spend some chips. Running
out of chips means that there is no more energy to answer questions.
Asking a question that seems sensible and useful to the answerer will cost,
maybe, one chip. Asking a question that seems weird and pointless costs a
lot of chips. But asking questions that are fascinating and fun to answer
might cost you nothing. You might even get chips back because you've
helped people feel smart. With that in mind, here are some tips:
Appreciate where resistance comes from: people want to get things
done. They may not feel as strongly about risks as testers do, though,
and they may feel that testers are too worried about problems that
probably won't happen. Have some empathy for them.
Create a sense of progress by periodically summarizing what is already
known, agreed on, and what has been learned in the meeting.
Consider suggesting alternatives to a requirement rather than sounding
like you are nitpicking over it. James once wrote more than twenty
questions about one requirement before erasing them and just rewriting
the requirement and asking only one question: "Is this what you
meant?"
Consider taking a particularly challenging issue offline. Maybe a
question requires a deeper investigation that is more efficiently done
outside the room.
Meet with key people about challenging issues before the meeting.

Consider whether the question really needs to be answered now.
Ask the right questions for the audience in the room—questions that
they might know the answers to.
Accept that you may not get all the answers you want. But keep track
of the unresolved questions and make a special point of testing the
product in those areas at a later time.
Avoid asking a lot of questions in a row that pertain only to rare or
strange situations. But whenever you do dwell on corner cases, show
how they might matter. Remember that users might do all kinds of
strange things by mistake or because they don't yet understand how the
system is supposed to be used, and hackers do strange things on
purpose because they are trying to disrupt the product. We may not
like that, but we have a responsibility to cope with it.
One reason to ask about rare or strange situations is to discuss how
they can be avoided in the first place.
Be ready to explain why a particular question is relevant and how
different answers to it will result in different risks or tests.
Our colleague, Wayne Roseberry, suggests that you can ask questions
without seeming to ask them by proposing use cases that are interesting
modifications to the existing ones. The questions are presented as: "Here
are some use cases for the following categories. Let's review them and
clarify what we believe should happen ..." James also experiences that
questions-without-questions effect by stating his assumptions out loud and
seeing if anyone in the room contradicts them.
One sort of resistance you will encounter is placation. This is when people
in the meeting try to agree quickly to a proposal—not because they have
thought it through and believe it is the best, but rather because they want to
end the conversation as quickly as possible. This can lead to poor decisions
that mostly fall onto the shoulders of testers later on, when unforeseen
consequences manifest. James has sometimes felt like a dog fetching a stick
thrown randomly by the BAs who are saying, "Maybe this will work ..."
Except, of course, fetching that stick and explaining why it turned out not to
work can take hours of testing time.

Jeff often finds himself arguing both sides of an issue when he feels that the
BAs have too quickly agreed with his first position. Just as the purpose of
inspective testing is not merely to create tests that pass, but rather to
exercise the product vigorously to find every important bug, the purpose of
prospective testing is not to get to agreement as fast as possible, but to make
a reasonable exploration of the solution and problem spaces.
"Analysis Paralysis" or "Go Fast and Break Things?"
Let's close by talking about the biggest elephant that remains wandering in
the conference room: how much is enough? You may agree that prospective
testing is a good idea, but you also need some sort of stopping heuristic to
keep it from getting out of hand.
After making you read this far, we have bad news: there is absolutely no
way to know for sure how much prospective testing is enough. It's just a
matter of developing your judgment over time.
If you do too little prospective testing, that will probably become apparent
in the level of rework and frustration that you experience during inspective
testing and in technical support as users struggle with a confusing or
inadequate product. But maybe not! Such problems may be acceptable if
you are getting invaluable feedback from users. It may be that no amount of
talking in design meetings will substitute for getting an idea into the hands
of real people. For all the talk about "shift left," there is also value in
shifting way to the right.
It's also hard to tell if you are doing too much prospective testing. One
approach to considering that is to think about opportunity cost—the value
of what else you could be doing instead of having conversations about
things like requirements and testability. For instance, the developers could
be writing code. The BAs could be researching even more new features.
Finding the right level of prospective testing requires that you develop a
feel for genuine product risk.
Also, too much pessimistic discussion can lead to designs that are overly
defensive and complex—something with so many checks for obscure and
rare conditions that development and testing will take more time, effort, and
resources than the feature is worth. Then again, in the case of secure
software, all of that complexity is actually necessary.

Here are some heuristics that may help decide when to stop:
Is it possible we may not need to discuss this at all? Perhaps we don't
need to discuss how to prevent bad input if there is a foolproof way to
deal with it once we get it, or maybe we don't need to think about how
to handle bad input because there is a foolproof way to prevent it.
Is this the right time to discuss it? Maybe we need to do more research
first, or get more experience. Maybe we need something tangible, like
a prototype, which will help us generate the right questions.
If you feel that a discussion has become a rathole, say so. Sometimes
that makes everyone else realize that it's time to move on.
Are you discussing real questions that remain, or merely rehashing
what's already been settled?
Periodically check in with the mission and focus of the meeting, just to
make sure that the current conversation is relevant and productive.
Ultimately, you can think of it this way: if the worst happens after you
release the product, and disaster occurs, and a recording of all your
discussions were made available to investigators and colleagues, would
they conclude that you acted reasonably or recklessly?
Finding the right level of prospective testing requires that you develop a
feel for genuine product risk.
A Cheat Sheet for Prospective Testing
This is a set of questions and suggestions for testing prospectively during a
meeting with a BA or developer. These questions come into play as soon as
there is a proposed product, feature, or requirement on the table to talk
about. Our preferred way of using this list is not to ask every question for
every proposed feature, but rather to become thoroughly familiar with all
these questions—and the spirit behind them—and then let the conversation
play out organically.

What exactly are we talking about?
What are we discussing? Is it a proposed requirement or an
implementation detail?
Is the requirement clearly stated? (Heuristic: Can you close your eyes
and visualize exactly what it would look like in practice?)
Are there any words or technical terms used that we don't completely
understand?
Are there ambiguities in the description?
What examples could be discussed that illustrate the matter?
Is this worth discussing here and now?
Should we do more offline research before discussing this?
Are the right people here to discuss this productively?
Is this too big to discuss all at once?
Is it time to take a break?
Are we overcomplicating this?
Should we draw a diagram before discussing this further?
Maybe we should try an experiment and then come back to discuss this
with new data.
What exactly are we trying to achieve?
What is the fundamental goal that is being served by this feature?
How important is this goal?
Is there another way of fulfilling this goal?
Will the proposed requirement actually satisfy this goal?
How do people cope now, without having this new feature?
What would happen if we didn't do this? Why is doing this better than
doing something else?

What influences must we consider?
Are there relevant industry standards?
Are there regulatory requirements that apply?
How do our competitors handle this?
Do we already do something similar somewhere else in our product
line? Do we know how that development process went? Are there are
any outstanding issues with it?
Would this proposed feature be fully consistent with the rest of our
product?
In order to fulfill the proposal, do we need to do something else first?
What blocks us?
Do we have a particular client who needs this? Can we talk to them?
What is the level of market demand for this feature?
Do we envision a future change or an upcoming event that should
influence our decisions now?
What other features or requirements will be affected?
Data structures or data collected
Screens or any part of the UI
User journey
Documentation
Tutorials or training
Installation processes
Third-party services or components
Third-party licenses
Product positioning and sales
GDPR requirements

Accessibility requirements
Internationalization
System security
Timing or loads
Compatibility with browsers, operating systems, mobile platforms, or
hardware
What specific data or conditions must this feature be
able to process or work with?
Which configuration settings apply to this feature?
Which kinds of data will this feature handle? Are there kinds of data
that it may encounter but will not handle?
Which usage scenarios would trigger or involve this feature?
What kinds of users will interact with or be impacted by this feature?
What about handling old data or external data that has been migrated
in?
What conditions, states, configurations, or data must be in place for
this feature to fulfill its goals? What if those underlying conditions
change?
Will the output of this feature be suitable and usable for other products
and features?
What are the merits of different ways of designing or
implementing this feature?
Are there different ways we can solve this problem?
Are some ways more expensive than others?
Are some easier to test or have better testability?
Are some more usable than others?
Are some quicker to code?

Are some better for the long term?
Are some more vulnerable to bugs?
Will this break anything already developed?
Will this change anything already developed?
Will doing this require us to implement something else?
What new problems will have to be managed?
Is this even possible to do, given our technical capabilities and
timeline?
How will the new feature handle errors or recover from
failure?
What type of failures can occur in the operation of this feature?
How will those failures be reported or logged?
How might the wrong data be in the wrong place at the wrong time
to cause trouble with this feature?
What is the worst that can happen with this feature? Data loss? Data
sent to unauthorized people? Silent corruption? Financial disaster?
Reputational disaster?
How will we provide technical support in the worst-case scenarios?
How will the system recover from failure?
How might this feature be misused, either accidentally or maliciously?
Are there protections in place to help avoid user error?
Can anything cause the user to get stuck? Will there be a way to back
out or reset?
How will we test the new feature once it exists?
How will we get the data we need?
How will we populate the system with data?

How will we explore its limits?
How will we assess its performance under stressful conditions?
How will we interact with the feature?
How will we get access to the feature?
When will it be available?
What form will the output take? How will we interpret it?
How will we evaluate the correctness of the output?
How will we assess the reliability of the system?
How will we trigger every error condition?
How will we trigger every conditional behavior?
How will we test multi-user capabilities?
How do we test recoverability?
How do we test scalability?
How do we test time-based functionality?
Do we need any new test tools? Do those tools need to get approval?
Notes
1   Marcano, A. (2007) "Prospective testing vs. inspective testing," Testing
Reflections. Available at:
https://web.archive.org/web/20071010090308/http://www.testing
reflections.com/node/view/5605 (Accessed: 14 April 2025).
2   "The general belief that one is comfortable being oneself—being open,
authentic, and direct—in a particular setting or role," according to
Nembhard, I. M., and Edmondson, A. C. (2011) "Psychological safety,"
in Spreitzer, G. M. and Cameron, K. S. (eds) The Oxford handbook of
positive organizational scholarship. Oxford Library of Psychology.
Oxford University Press.

3   Evans, E. (2004) Domain-driven design: Tackling complexity in the
heart of software. Boston: Addison-Wesley.
OceanofPDF.com

CHAPTER 9
Test Reporting (Without Pretentious Metrics)
—By Paul Holland
You have likely seen test reports containing charts, graphs, and metrics that
claim to show the status of testing. Some claim to show the status of the
product. I think the vast majority of those metrics are not only useless in
showing the status of a project but also dangerous to use. The danger comes
from the illusion that they are providing actionable information. For
decades, stakeholders have been making decisions on whether or not to
release a product based, at least partially, on these dangerous metrics. In this
chapter, I propose alternatives to using these metrics that show actionable,
useful information that stakeholders can use to make informed decisions.
Metrics Are Nothing Without a Story
Test management tools generate a lot of metrics based on test cases. This is
a big problem for the industry, however, because test cases are not a reliable
unit of measure. It's like counting backpacks or plastic bags to determine
the status of a business.
Worse, many testers don't make any coherent report to go with the metrics.
Their companies simply assume that if all the automated checks are green,
testing is done, and the product is okay to deploy.
To demonstrate the issue, let's look at two hypothetical projects that are
similar in size, complexity, and importance to the company (see Figure 9-
1).

Figure 9-1: Metrics-Based Reporting of Two Projects
By just looking at the information presented, it seems obvious that Project
Bravo is in much better shape. It has many more test cases, a higher pass
rate, and fewer bugs. But the information shown does not include the story
of the project. Without a story, metrics like this are essentially an art exhibit
instead of engineering data—like an inkblot that one person interprets as a
bunny rabbit and another person as a thunderstorm.
Now, here's the missing story:
Project Alpha is in fairly good shape, and we should be able to
deliver on time. We currently have some critical bugs, but we have
been investigating them, and we feel that they should all be fixed by
the end of day tomorrow.
With our existing test scenarios, we feel that we have fairly in-depth
coverage of most of the activities that our customers are likely to
perform, plus some corner cases that a malicious user may do on
purpose or a customer may perform by accident.

Project Bravo is in trouble. Although we have a lot of test cases,
they are mostly simple, repetitive individual steps that don't really
cover the flows that our customers are likely to take through the
product.
We currently have only one open critical defect, based on our in-
depth investigation, and we don't think we will be able to fix it
without a complete rewrite of a major component of the system.
This will put us at least 4 weeks behind schedule.
Notice that once the story is introduced, the metrics become almost
irrelevant. The story is the critical bit. The fact is, there are no known
metrics of any kind that express the status of testing or quality in the
absence of a story. The details, dependencies, and potentialities of the
situation are not and cannot be reduced to a handful of simple numbers.
Rather, the value of numbers is in sharpening and illustrating a story that
informs the stakeholders about the status, test coverage, bugginess, and
risks of the project.
Bad Metrics
In RST, we use a definition of measurement adapted from Cem Kaner: "The
assignment of numbers to objects or events according to a rule derived from
a model."1 A metric is a system or standard for a particular kind of
measurement.
Notice that measurement requires a model. That's a technical way of saying
that the numbers must relate to reality in some definable way. But if that
model involves human judgment, we must be wary. In 1975, Charles
Goodhart, a former advisor to the Bank of England and Emeritus Professor
at the London School of Economics said, "Any observed statistical
regularity will tend to collapse once pressure is placed upon it for control
purposes."2 In other words, if a metric is used as a target, it will no longer
be a useful metric.
So, if testers are expected to execute a certain number of tests per week as
their target, they will likely achieve that goal—regardless of the bad
behavior that results. They may rush through the execution and miss

obvious bugs. They may refuse to help other team members in order to meet
their own target. Measuring people often results in a variety of unexpected
and likely undesired behaviors.
Elements of Bad Metrics
How can you determine if the metrics your team is using are bad? Here are
four elements of bad metrics:
1. Metrics that compare things that are inconsistent in size or
composition:
It is impossible to use them for comparison, progress, or
coverage.
I might have one test that takes 5 minutes to execute and another
that takes a full day. How can those both be considered the same?
Does it matter if I have one known critical bug open or 20? The
number is not relevant. The impact and effort required to fix
them is relevant but can't be expressed in the number of bugs. I
might have one bug that requires completely re-architecting the
system and will take months to fix—or 20 that can be fixed in
one day.
Examples: Number of tests, number of bugs, percentage of tests
executed.
2. Metrics that create competition between individuals and/or teams:
It's possible for competitive metrics to be applied in a spirit of fun
and fellowship. However, the more likely case is that they create a
competitive, hostile environment where people are looking out for
themselves and not what is best for the company.
Such metrics tend to inhibit teamwork and the sharing of
information.
They are especially damaging if they are linked to compensation.
They discourage diversity in practices as well as any kind of
speculative innovation.

Example: Number of x per tester or number of x per feature
(where x could be almost anything).
3. Metrics that are easy to "game" or circumvent the desired intention:
These metrics cause undesired behavior that typically
circumvents the intention of the metric. For example, a senior VP
at a telecom company wanted to reduce the amount of expensive
test equipment being purchased for labs around the world. He had
a metric created that measured the usage of all test equipment
with the intention of moving underutilized equipment to other
locations that would use it more. The lab manager in one location
created a script that detected any idle test equipment and then
would start to "use" it. This meant that all the lab equipment
falsely appeared like it had usage well over 95%.
Examples: Pass rate of tests (execute more simple tests that will
pass to increase the percentage); number of bugs raised (raise two
separate bugs instead of combining them into a single bug, or
raise more cosmetic bugs).
4. Metrics that contain misleading information or give a false sense of
completeness:
Metrics that summarize large amounts of information without
preserving context.
Metrics that lump together data from disparate and inconsistent
sources.
Metrics that require human attention and discipline to keep
updated and accurate. People get easily distracted and forget to
log their data.
Metrics that are rarely used. If people don't use data, errors can
crop up that are not discovered until a whole lot of data is ruined.
Examples: Code coverage, number of tests, pass rate.
Most testing metrics have at least one of the preceding elements, which, in
my opinion, render them at best useless and at worst dangerous. All metrics

are simplifications of the truth. If you must use them, provide the context
your clients need to make sense of them.
Numbers never speak for themselves.
Coverage
Your stakeholders need to know what you have tested. A Product Coverage
Outline (PCO) is a simple way to show them.
A PCO is literally a list or outline of testable elements of a product. It's not
a list of specific test cases but rather an inventory of testable elements. You
won't necessarily test every item in a PCO, but that's okay. Since the PCO is
about coverage, seeing what was not tested is still useful information for
stakeholders. It can spark discussions, create buy-in, and may even enhance
your reputation as a thoughtful and thorough tester.
The PCO does not have to be a model of an entire product. You can create
one for a user story (the Agile Development artifact indicating a small piece
of development of a particular feature). You can create one at the epic level
(a collection of stories), which may provoke a broader range of issues and
assumptions to discuss.
We like PCOs in Rapid Software Testing because they're a simple type of
documentation that can be used in so many ways.
For instance, a PCO can be used as a base of ideas from which to create
tests on the fly or to create highly formalized tests. It can be used to help
split work between testers or as the basis for a test report. It can be used for
a risk analysis or to help plan testing after a change has been made.
The elements of a PCO can be worked into a document, spreadsheet, or a
mind map. It can be color-coded or annotated with meaningful status icons.
I find that the accessibility and simplicity of a PCO make it a popular
reference document.
Figure 9-2 illustrates a sample PCO.
One way to create a PCO is to take a tour of the product. Go through and
make a note of what's there. You might decide to capture any elements you
see in a mind map, as shown in Figure 9-2. Open every menu, submenu,
and context menu; note every text box, check box, and radio button. Even

static text and certain kinds of images might be fruitful to consider, as they
might point to testable elements like business rules or metadata.
Figure 9-2: A Sample PCO
If there is no user interface, you can start with APIs or data structures
(JSON, XML, etc.). You can talk with other team members, look at
competitive products, look at previous versions, and review marketing
information and specifications in any form to which you have access.
After your tour, you will have a basic outline, but what if you are worried
that you've missed something? If so, consider going through the Product
Elements list in the Heuristic Test Strategy Model (HTSM) and, for every
item in that list, investigate if there is any corresponding element in your
product that may be worth testing.
For example, I was recently testing a screen after the removal of a feature.
When I went through the HTSM, I noticed the Structure/Collateral
category. This reminded me that there could be help files or videos that
would also have to be modified. Further down, I noticed the Time category

and suddenly thought: "What if a user is in the middle of using this feature
when the system is updated, and the feature disappears?" This is the way
the HTSM works—to gently remind us of things we already know but
might have forgotten under time pressure.
When I create a PCO, I include any element that might plausibly be tested,
even if I don't intend to test it. The reason why I do this is to help me be
clear to my management and team about things they may assume are being
tested, even though we don't have time to test them. Also, I may change my
mind later and decide to test something after all.
Figure 9-3: Zoomed-In PCO
The PCO does not show how you will test the elements within it, just that
those elements are being considered to be tested. A PCO is not a menu or a
recipe; it's more like a list of ingredients.

Once I create a PCO, I like to review it with the rest of the team and
stakeholders. They often have ideas that I had not considered. Sometimes
we get to talking about how to test different elements.
PCOs evolve. You don't have to create it perfectly the first time.
A Simple Story for Coverage
Any metric for coverage is going to leave a lot out. Code coverage metrics
completely ignore data coverage. Neither of those things tells you about
requirements or risk coverage. Yet, when I am in the weeds of actual
testing, I do get a sense of which things I've tested more thoroughly than
other things.
I find that I can assess my level of coverage qualitatively, using color
coding. That's usually good enough. It's a subjective assessment, yes, but
it's not arbitrary. I can discuss the exact tests I performed that led to my
assessment.
Consider the following ordinal scale of coverage levels:
1. Nothing: We have not tested this.
2. Sanity: One or two happy paths. We know only that it isn't completely
broken.
3. More Than Sanity: Some basic error paths but still primarily happy
path (typical coverage of detailed test cases). Simple, shallow testing.
4. Common Cases: Increased variety of scenarios, including more
complex paths and error paths. In-depth testing of client's most
frequently used paths and the most critical paths.
5. Some Corner Cases: Obscure cases, stress testing, data variety, large
data.
6. Deep Corner Cases: Extreme data variety, long period tests, unlikely
scenarios.
Even though I rarely achieve the highest level of coverage, I like to keep
that level in the legend to show there is more testing that could be done.
Most automation coverage would only achieve the second level of coverage
—a "Sanity check." Some may achieve "More Than Sanity," but it's

difficult to surpass that level with simple regression-style automation. Many
companies think their automation will provide deeper coverage, but that is
rarely the case.
I associate these levels with colors and then assign them to the items in the
PCO. However, sometimes the PCO is too detailed. To deal with this, I
prepare a high-level version of the PCO called a Release Coverage Outline
(RCO).
Release Coverage Outline
An RCO is similar to a PCO but at a higher level and less detailed. In an
RCO, I may just hide all the fiddly lower-level details, or I may just drop
elements of the product that aren't new and weren't changed or impacted in
the current release. Any elements changed (i.e., impacted) show more
detail, while unchanged elements show less. This allows stakeholders to see
at a glance which areas were tested across the entire product and which
were not.
As with a PCO, an RCO can be used to display proposed test coverage
before testing starts. This allows stakeholders to set more realistic
expectations while also discussing areas of concern. Based on their input,
testing can be redirected to the areas that need it.
Better Test Reports
To create better test reports, I tell stories about my testing.

Figure 9-4: Wide view of color-coded RCO
Figure 9-5: RCO Legend
One of the heuristics of RST is what we call the three-level testing story:
1. The status of the product ("Is the product any good?")
2. How it was tested ("How do I know if it's any good?")
3. The value of the testing ("Why should others feel good about my
testing?")

The status: To discuss status, I find it helpful to include a summary of the
product's current state in terms of how it failed and how it might fail. I tell
this story in different ways, depending on the objectives and expectations of
various clients and stakeholders.
The how: I discuss how I configured the system, some of the ways I
operated the product, and how I observed what I observed. I also mention
what has not yet been tested, including what will deliberately not be tested
(and why). I even disclose gaps in coverage because any lack of coverage
could be a product risk that stakeholders may have opinions or concerns
about.
The value: I use Release Coverage Outlines to express the risks that were
addressed, as well as risks that were not addressed (and why). I find it
helpful to share anything that made my testing slower or harder, or I give
my impression of how testable the product was (or should be). The point is
to make my work visible and ready for productive scrutiny.
Sample Test Report
Here is a sample test report that has been altered to be made anonymous. It
was for a small release that only had one small, isolated feature. I don't
mean to propose this as a "best practice." We don't have best practices in
RST. But I think you'll find it interesting and helpful as you develop your
own test reports.
Final Test Report for Release 10_21_2021 of Joe's Android App
Executive Summary   This release of the Android app for JOE'S
Store is focused on ensuring information captured by Adobe
Analytics is viewable on the Adobe Dashboard. The focus of this
release was on navigation clicks, first launches, launches, visits,
orders, and revenue.
All discovered bugs for the analytics were fixed and retested.
No bugs found in previous releases will be fixed as part of this
release.
Testing Coverage Summary  The testing over the past week has
focused on the following:

Environment testing of JOE'S Android App builds for STAGE
and PROD
Data tagging with Adobe Analytics revisited
Verification that Adobe Analytics Dashboard displays accurate
data
The following areas were tested at a minimal level to complete
validation:
In-app regression testing
The following areas were not tested for this release:
In-depth E2E testing
Release Coverage Outline
Bugs and Issues  All bugs found during testing of the Adobe
Analytics feature were fixed prior to deployment.
Figure 9-6: Release Coverage Outline
Found in Prior Release—Not Being Fixed in
REL_082521_Android_JOE:

E2E - CreateOrder XML contains incorrect EnteredBy=
"Web" attribute for a Mobile App order.
Hyperlinked Order number on "Order Received!" Page is
not receptive to user touch.
Page loading is faster than page rendering; therefore, the
user will see a short delay between the Global Page spinner
and the next page to be loaded.
Add to Bag sticky button on PDP overlays "Review"
button.
Delay in product images loading for Zoom-In page on the PDP.
Cosmetic defects for BOPIS overlay.
Questions and Concerns
Requirements/data points for the Dashboard were unclear to
QA until late in development (October 19).
Verification of the Adobe Analytics Dashboard data was
limited to Staging Environment.
Risks & Impacts
RISK
IMPACT
Minimal regression
and no E2E testing
Minimal: The changes in this release don't affect the
app's functionality, so minimal regression was
completed and no E2E was completed.
Leftover defects
from V1
Minimal: While there are some UI/UX defects that
have not been fixed, they are not blocking happy path
customer experience.
Notes
1   Bach, H.Q.N., Falk, J., Johnson, B. and Lawrence, B. (2001)
Measurement Issues and Software Testing. Copyright© Cem Kaner
March, 2001.

2   Goodhart, C.A.E. (1975a) "Problems of monetary management: The UK
experience," in Papers in Monetary Economics, Volume I, Reserve Bank
of Australia.
OceanofPDF.com

CHAPTER 10
Working with Quality Characteristics
—By Rikard Edgren, Henrik Emilsson, and Martin Jansson
Soon after the millennium shift, we—Henrik, Martin, and Rikard—joined
forces at a growing start-up company in Gothenburg, Sweden (Spotfire).
We were a group of ambitious testers who had the freedom to experiment
with how we tested, and that we did. Each lunch, we discussed testing,
including what we could test and how, and we earned respect among the
developers, as the information we provided was valuable to them.
One day, we discovered Cem Kaner's article "The Ongoing Revolution in
Software Testing." We were excited, realizing, "Here is someone who
thinks like us!"
The discovery continued, and we soon found that the written material that
suited us best was coming from the context-driven school of testing. James
Bach's Heuristic Test Strategy Model (HTSM)1 became an important
document that helped us think better and more broadly about our testing.
The SFDIPOT mnemonic—which stands for Structure, Functions, Data,
Interfaces, Platform, Operations, and Time—might be the best part of the
HTSM, but we made most use of the Quality Criteria page: No matter what
we tested, that list helped us come up with more and better test ideas.
After some time, we started adding items to the list, including quality words
that were important in our context but, at the same time, generic enough to
be useful to others.
As the number of subcategories increased, we felt this was turning into
something good. We started creating a new version, and the new keyword
"charisma" emerged, holding different kinds of feelings that users have.
We polished what we had and sent the list to James and Michael, with the
hopes of becoming coauthors of the HTSM—and famous. Michael replied
with some detailed comments. James replied on the same day with, "Cool,
I'm looking at it," but after that, nothing happened.

As young, energetic testers, we were not put off by this; instead, we started
thinking about publishing our version of the model, which we describe next.
Developing the Model
For copyright reasons, but also for our learning, we started by rephrasing all
the sentences in the list. The next step was to broaden the scope with the
help of others, so we looked at other lists of quality attributes.2 Many of
these felt too focused on metrics, but still helped us elaborate our thinking.
We mixed theoretical investigations of previous models (including reading
up on Nielsen's usability work3) with empirical experiments in our daily
work. Whenever we found something that could be useful, we looked to see
if it could fit in the model (e.g., the "Clarity" item of that model came from
a health-care project). A mix of these methods over time was very good, and
letting the work take years was an important part.
As critical thinkers, we didn't just add items for the sake of it, but rather
changed wordings and added more items carefully. We wanted more
elaborations. For example, the HTSM on Usability at that time contained
Learnability, Operability, and Accessibility. Our version ended up with
Affordance, Intuitiveness, Minimalism, Learnability, Memorability,
Discoverability, Operability, Interactivity, Control, Clarity, Errors,
Consistency, Tailorability, Accessibility, and Documentation.
Rikard wanted the list to fit on one page, so Martin and Henrik did their
best not to let it extend to two pages. Eventually, decreasing the font size
didn't work, so two pages it was.
The three-person collaboration worked very well, and without time
pressure, the list evolved into something that felt really good to us.

Shiny Web Application
At one time during this period, I came in as a test consultant at the
Swedish Tax authority. I joined a large project that replaced an old
terminal-based IT system with a modern (as of 2006) web
application developed in-house. During the project, we involved the
clerks who worked with the old system as acceptance testers for the
new system. I started to notice that the clerks were really skeptical
of this new, shiny web application.
At first, I just thought that this was a resistance to something new;
they were going to have to stop using a well-known system that they
had mastered. While talking to them, I started asking questions
about how they felt toward this new application and about the
differences between the old one and the new one. The answers
surprised me. To summarize, they all had resistance because they
didn't trust the new application. The terminal-based system, with
white text on a black background and keyboard input only, told the
clerks, "I'm trustworthy, I can crunch the numbers, the data is safe
with me ..." whereas the new web application, with several colors
and navigation with a mouse only, seemed gimmicky and unserious.
At that time, the oracle or comparable system for the old system
was like an advanced calculator: boring with no bling-bling, but
correct. The web application, on the other hand, was compared to,
well, other web pages at that time: fancy, but could you trust it?
This led to many intriguing thoughts, including that products can,
by their appearance, signal something to a user. An obvious
example of this is that an application aimed at children should
probably not have the same look and feel as an application used in
healthcare, and vice versa. I also started thinking about why certain
products were successful while others with the same capabilities
were rejected. Could it be that a certain product's charisma made it
more highly valued than other similar products? Does the product
have "it"? And is charisma really a software quality aspect?
After having vivid discussions with Martin and Rikard about
whether Charisma was a subcategory of Usability or belonged to

another category, we ended up deciding that it was a quality
characteristic category of its own. As I remember it, we couldn't
find other quality models listing this kind of quality aspect. So, we
felt proud of identifying an area that was something possibly new
and unique.
—Henrik Emilsson
We met James and Michael at conferences, and especially James had
problems with the Charisma section of our model: "I don't believe I've
heard anyone define charisma, much less how to test it. I need to be
convinced that you have some significant experience or data to back up
your claim." Nevertheless, we had a feeling that Charisma ("Do you want
to use it?") was different from Usability ("How easy is it for you to use
it?").
James and Michael were both happy that their work was a foundation for
our model and that we were thinking critically enough to have made our
own version. Eventually, a couple of years later, they did add Charisma to
the HTSM, but of course, with their own phrasing.
In 2010, we published the list on our blog, thetesteye.com. Our model
became quite popular, and we know that testers use it now and then to think
clearly about what quality is in their context.
People started translating the list, and it is now available in English and
Swedish (our contribution), Dutch, Polish, Romanian, Norwegian,
Hungarian, Spanish, Catalan, and Russian.
Quality Characteristics4
Just as James and Michael encouraged us to modify their model to suit our
context, we encourage you to go through this model and think about how it
relates to your projects. Add or delete things as you see fit.
Capability: Can the product perform valuable functions?
Completeness: All the important functions wanted by end users are
available.

Accuracy: Any output or calculation in the product is correct and
presented with significant digits.
Efficiency: The product performs its actions in an efficient manner
(without doing what it's not supposed to do.)
Interoperability: Different features interact with each other in the
best way.
Concurrency: The product is able to perform multiple parallel tasks
and run at the same time as other processes.
Data agnosticism: The product supports all possible data formats,
as well as handling noise.
Extensibility: The product enables customers or third parties to add
features or change its behavior.
Reliability: Can you trust the product in many difficult situations?
Stability: The product shouldn't cause crashes, unhandled
exceptions, or script errors.
Robustness: The product handles foreseen and unforeseen errors
gracefully.
Stress handling: How does the system cope when exceeding
various limits?
Recoverability: It is possible to recover and continue using the
product after a fatal error.
Data integrity: All types of data remain intact throughout the
product.
Safety: The product will not be part of damaging people or
possessions.
Disaster recovery: What if something really, really bad happens?
Trustworthiness: Is the product's behavior consistent, predictable,
and trustworthy?
Usability: Is the product easy to use?

Affordance: The product invites discovery of the product's
possibilities.
Intuitiveness: It is easy to understand and explain what the product
can do.
Minimalism: There is nothing redundant about the product's
content or appearance.
Learnability: It is fast and easy to learn how to use the product.
Memorability: Once you have learned how to do something, you
don't forget it.
Discoverability: The product's information and capabilities can be
discovered by exploring the user interface.
Operability: An experienced user can perform common actions
very fast.
Interactivity: The product has easy-to-understand states and
possibilities of interacting with the application (via a GUI or API).
Control: The user should feel in control over the proceedings of the
product.
Clarity: Is everything stated explicitly and in detail, with a
language that can be understood, leaving no room for doubt?
Errors: There are informative error messages, so mistakes are
difficult to make and easy to repair.
Consistency: Behavior is the same throughout the product, and
there is one look and feel.
Tailorability: Default settings and behavior can be specified for
flexibility.
Accessibility: The product is possible to use for as many people as
possible, as well as meeting applicable accessibility standards.
Documentation: There is a Help that helps and matches the
functionality.
Charisma: Does the product have "it"?

Uniqueness: The product is distinguishable and has something that
no other product has.
Satisfaction: How do users feel after using the product?
Professionalism: Does the product have the appropriate flair of
professionalism and feel fit for its purpose?
Attractiveness: Are all aspects of the product appealing to the eyes
and other senses?
Curiosity: Will users get interested and try out what they can do
with the product?
Entrancement: Do users get hooked and have fun? Are they in a
flow and fully engaged when using the product?
Hype: Should the product use the latest and greatest
technologies/ideas?
Expectancy: The product exceeds expectations and meets the needs
users didn't know they had.
Attitude: Do the product and its information have the right attitude
and speak to users with the right language and style?
Directness: Are (first) impressions impressive?
Story: Are there compelling stories about the product's inception,
construction, or use?
Security: Does the product protect against unwanted usage?
Authentication: The product's identification of the users.
Authorization: The product's handling of what an authenticated
user can see and do.
Privacy: The ability to not disclose data that is protected to
unauthorized users.
Security holes: The product should not be vulnerable to social
engineering attacks.
Secrecy: The product should under no circumstances disclose
information about the underlying systems.

Invulnerability: The product should be able to withstand
penetration attempts.
Virus-free: The product will not transport a virus or appear as one.
Piracy resistance: There should be no possibility of illegally
copying and distributing the software or code.
Compliance: The product should adhere to appropriate security
standards.
Performance: Is the product fast enough?
Capacity: The many limits of the product under different
circumstances (e.g., a slow network).
Resource utilization: Appropriate usage of memory, storage, and
other resources.
Responsiveness: The speed at which an action is (perceived as)
performed.
Availability: The system is available for use when it should be.
Throughput: Can the product process a large number of items?
Endurance: Can the product handle load for a long time?
Feedback: Is the feedback from the system on user actions
appropriate?
Scalability: How well does the product scale up, out, or down?
IT-ability: Is the product easy to install, maintain, and support?
System requirements: The product can run on supported
configurations and handle different environments or missing
components.
Installability: The product can be installed on its intended
platforms with the appropriate footprint.
Upgrades: It's easy for users to upgrade to a newer version without
losing their configurations and settings.
Uninstallation: Are all files (except user or system files) and other
resources removed when the software is uninstalled?

Configuration: Can the installation be configured in various ways
or places to support a customer's usage?
Deployability: The product can be rolled out by the IT department
to different types of (restricted) users and environments.
Maintainability: Are the product and its artifacts easy to maintain
and support for customers?
Testability: How effectively can the deployed product be tested by
customers?
Compatibility: How well does the product interact with other products
and environments?
Hardware compatibility: The product can be used with applicable
configurations of hardware components.
Operating system compatibility: The product can run on intended
operating system versions and follows typical behavior.
Application compatibility: The product, and its data, work with
other applications customers are likely to use.
Configuration compatibility: The product has the ability to blend in
with the configurations of the environment.
Backward compatibility: Can the product do everything the last
version could?
Forward compatibility: Will the product be able to use artifacts or
interfaces of future versions?
Sustainability: Effects on the environment, e.g., energy efficiency,
switch-offs, power-saving modes, and telecommuting.
Standards conformance: The product conforms to applicable
standards, regulations, laws, and ethics.
Internal Quality Characteristics
The following characteristics are not directly experienced by end users but
can be equally important for successful products:

Supportability: Can customers' usage and problems be supported?
Identifiers: Is it easy to identify parts of the product and their
versions—or specific errors?
Diagnostics: Is it possible to find out details regarding customer
situations?
Troubleshootable: Is it easy to pinpoint errors (e.g., log files) and
get help?
Debugging: Can you observe the internal states of the product
when needed?
Versatility: Can the product be used in more ways than it was
originally designed for?
Testability: Is it easy to check and test the product?
Traceability: The product logs actions at appropriate levels and in a
usable format.
Controllability: The ability for the tester to independently set
states, objects, or variables.
Observability: The ability for the tester to observe things that
should be tested.
Monitorability: Can the product give hints on what/how it is
doing?
Isolatability: The ability for the tester to test a part of the product
by itself.
Stability: Changes to the product are controlled and not too
frequent.
Automation: Are there public or hidden programmatic interfaces
that can be used?
Information: The ability for testers to learn what needs to be
learned.
Auditability: Can the product and its creation be validated?

Maintainability: Can the product be maintained and extended at low
cost?
Flexibility: The ability to change the product as required by
customers.
Extensibility: Will it be easy to add features in the future?
Simplicity: The code is not more complex than needed and does
not obscure test design, execution, or evaluation.
Readability: Is the code adequately documented and easy to read
and understand?
Transparency: Is it easy to understand the underlying structures?
Modularity: Is the code split into manageable pieces?
Refactorability: Are you satisfied with the unit tests?
Analyzability: The ability to find causes for defects or other code
of interest.
Portability: Is transferring the product to different environments
enabled?
Reusability: Can parts of the product be reused elsewhere?
Adaptability: Is it easy to change the product to support a different
environment?
Compatibility: Does the product comply with common interfaces
or official standards?
Internationalization: Is it easy to translate the product?
Localization: Are all parts of the product adjusted to meet the
needs of the targeted culture/country?
User interface robustness: Will the product look equally good when
translated?
Using the Model
Here are some ways we use the list:

To get test ideas:
Any quality characteristic is also a potential risk area.
We read through the list and briefly consider each item. Is it relevant
to our product? Does it bring to mind any test ideas for us? This is
especially helpful when our minds are tired and we need inspiration.
We don't discard any top category too easily, as these suggestions
might be our biggest time-savers.
We sometimes also randomly choose a product element from the
HTSM Product Elements list and then a random element from the
quality categories list to see if combining them triggers any test ideas.
To understand what quality means for our product:
The list can be useful for understanding what quality means for our
product. We try to involve people other than testers to get more
perspectives. We like to start without the list and define what quality
means to us first, using our own words, because those will better
describe what we want to accomplish.
When we feel done, we then pick up the list of quality characteristics
to see if we missed something relevant or if we get any ideas that can
make our first ideas even better.
An important part is to try to make the quality statements really useful
by making sure they are specific for the situation. "Easy to use" is
very generic, so better examples are
"First-time users should have no problems creating their first mind
map."
"Power users should very quickly be able to create complex mind
maps (keyboard + mouse navigation)."
"The product should adhere to the 508 accessibility guidelines."

At one consultancy assignment, I was working with the testing
of an advanced system consisting of many layers of subsystems.
There was a big team of senior requirements analysts who were
trying to capture new requirements as well as polish and
enhance old ones. The main focus was on functionality,
extending what the system should do. I tried to cooperate with
the team to extend the requirements and stories around
different qualities using our Software Quality Characteristics.
Sadly, I did not gain their trust enough to help out.
Later on, a young, new requirements analyst started on their
team. She did not gain much assistance from the more senior
teammates, so instead, I helped her. She was supposed to work
on a new part of the system and write requirements for a new
feature. In our discussions, we used the quality characteristics
to guide the work on the requirements. Eventually, each
function was supported by several characteristics. The
requirements were so much richer and gave more details to the
development team. It is my belief that, as a tester, you are able
to help with outlining expectations for various qualities that the
system should have, which eventually may lead to a better
product or service.
—Martin Jansson
To teach testers:
We have used this model frequently as teachers at higher vocational
studies in software testing. It is a good starting point to broaden
students' view of quality and testing, and it could also be a framework
to cover different testing methods.

At a written exam for higher vocational studies, the students
were asked to write about things that were important in a given
context we had constructed. During the classes, we had covered
theory and practice involving our quality characteristics and
the Heuristic Test Strategy Model, so using those would be one
viable way of addressing the question (but, of course, it needed
to be combined with their own thinking because the lists
themselves provide no answers). One ambitious student had
memorized CRUCSPIC STMP and SFDIPOT, and, as Henrik
read his exam, he shouted, "This is brilliant!" For each quality
characteristic, the student took each SFDIPOT item and
analyzed the impact on the given scenario. It was a bit overly
ambitious for the given situation, but it clearly showed
capability and earned a full point.
The student's name was David; hence, the act of putting
SFDIPOT on top of CRUCSPIC STMP is called "David's
maneuver."
—Rikard Edgren

Epilogue
As with all models, a lot of learning and insight happens in the process of
creating them. So, even though we believe this heuristic model is useful to
others, it would become even better if it were created or tailored by the
person who will use it. By "owning" a model, it will help you much more.
However, keep in mind that (visual) models are a means of communicating,
so try to make sure that your colleagues or clients can grasp the outcome.
There are many more models you can make your own version of. For
example, we published "37 Sources for Test Ideas" in 2012,5 inspired by
Rob Sabourin's "10 Sources of Testing Ideas."6
Another thought is that we might benefit from SFDIPORT, where R stands
for Regulations. During the last 10 years, we have worked a lot with IT
systems where laws and regulations play a big part in the construction of
the software and have to be considered from a product perspective.
Therefore, we have sometimes added the R in SFDIPORT just to trigger
questions and ideas based on that (i.e., it is okay to add whatever category
you need in order for you and your colleagues to think better).
Perhaps one of the best things we learned from RST and HTSM is that it is
okay to adopt and change things to increase your possibilities of trying to
understand what matters.
Notes
1   See Appendix B.
2   For example: ISO 9126-1, Wikipedia Ilities, Boehm, FURPS, Dromey,
and PERFUME.
3   Nielsen, J. and Molich, R. (1990) "Heuristic evaluation of user
interfaces," Proceedings of the ACM CHI '90 Conference. (Seattle, WA,
1-5 April), pp.249 -256.
4   Edgren, R. Emilsson, H. and Jansson, M. (2011) "Software quality
characteristics 1.1," The Eye Test Blog, 14 November. Available at:

https://thetesteye.com/blog/2011/11/software-quality-
characteristics-1-1 (Accessed: January 27, 2025). This work is
licensed under the Creative Commons Attribution-No Derivative
License and is inspired by James Bach's CRUSSPIC STMPL, ISO 9126-
1, Wikipedia: Ilities, and others.
5   Edgren, R., Emilsson, H. and Jansson, M. (n.d.) The Test Eye: Sources
for Test Ideas, v1.1. Available at:m
https://thetesteye.com/posters/TheTestEye_SourcesForTestIdeas
.pdf (Accessed: January 27, 2025). This work is licensed under the
Creative Commons Attribution-No Derivative License and is inspired by
James Bach's CRUSSPIC STMPL, ISO 9126-1, Wikipedia: Ilities, and
others.
6   Amibugshare (n.d.) 10 Sources of Testing Ideas. Available at:
https://web.archive.org/web/20230120110617/http://www.amibugs
hare.com/articles/Article_10_Sources_of_Testing_Ideas.pdf
(Accessed: January 27, 2025).
OceanofPDF.com

CHAPTER 11
Adventures in Testability
—By Janna Loeffler
What is testability? You could probably ask 10 different testers and get 9
different answers. For me, I say the real question is: "How can we make this
product easier to test?" It's a simple, broad way to think about testability.
At conferences, talks on testability often reduce it to having developers add
identifiers into their code to make it easier to write some automated checks.
That has never sat well with me. Sure, having developers add identifiers
into the code helps with automation, but does it help with any other aspect
of testing? Maybe a little. If I can write some automation to help do some
shallow checks, that frees up more time for me to test. But there is so much
more to it. Testability isn't one thing—it's about all the things that help me
test and make testing more efficient or effective.
What are those things? Let's talk about that.
I remember the first time I ever heard about Rapid Software Testing. It was
at a conference where Michael Bolton gave a talk called "A Ridiculously
Rapid Introduction to Rapid Software Testing." That's when I started
thinking out of the box when it came to testing. Michael spoke about how
sometimes, even when you feel like you can't test one thing, there are
always other things you can test. Even if you can't test the product itself,
you can test documentation, something similar to the product, or even
people's thoughts about the product.
After taking a Rapid Software Testing class, I reflected on testability and
wanted to learn more. Having been introduced to the wealth of knowledge
James has on his website, I searched for the term "testability" and came
across the Heuristics of Software Testability model. The model really
resonated with me because it helped provide a structure to what I was
already thinking. There are so many different aspects to software testability.
In this model, James discusses the five different aspects of practical
testability:

Epistemic testability: Created by our knowledge about the status of
the product under test relative to what we need to know. Example: a
product built with an established library is easier to test than one with
entirely new code.
Project-related testability: Created by the physical, social, and
informational conditions under which we test. Example: a project
where the developers work in a different time zone from the testers is
harder to test.
Intrinsic testability: Created by the nature of the product itself.
Example: A simple product is easier to test than a complex one.
Subjective testability: Created by the nature of the tester or the test
process. Example: A tester with greater knowledge of test techniques
and tools will find it easier to discover elusive bugs.
Value-related testability: Created by the quality standard and our
knowledge of it. Example: If you aren't a medical professional, testing
a medical device is harder than, say, equally complex insurance
software. It's just harder to understand what the user needs.
So, adding identifiers to the code to help automation is an example of
testability enhancement, but there are many others.
Testability on the High Seas
Let me tell you about a cruise I took. Yes, I was once responsible for testing
a product on a cruise ship. How difficult could it be to test on a ship? Pretty
difficult, as it turns out.
First, we were at sea. We were in the middle of the ocean, so we had to rely
on satellite Internet. In some areas we had good network coverage, but in
other areas not so much. Factors like temperature, wind, spray, water,
obstacles, and the movement of the ship are all difficult to simulate in a
small lab.
Second, a cruise ship is big. Since this new software integrated with a small
IoT device (it looked like a poker chip) and a mobile app that let passengers
order food and/or beverages from anywhere on the ship, there needed to be

sensors throughout the ship. To test this, we had to move throughout the
ship just like passengers.
Third, there are a lot of people on a cruise ship. We couldn't allow ourselves
to do anything to disrupt their vacation, and if we needed any of the crew to
cooperate with us, we'd be their lowest priority.
You can also imagine the headaches with upgrading equipment mid-cruise.
Reliably transporting it to a foreign country could require an employee to
hand-carry it on a commercial flight. Not very testable. Then there was the
problem of getting software updates and fixes to the ship. With the ship's
Internet coming from satellites, it wasn't always dependable enough to
allow us to download new versions of the software. We could reliably do
updates only while the ship was in port, when it was not moving, and close
to land.
You might wonder why we needed to test on a ship to begin with. Couldn't
we simulate the system in a nice, climate-controlled office, with good
Internet? Yes, we did build a simulated environment to mimic the ship's
layout. It sported two staterooms, a small casino with a point-of-sale
system, and some hallways. This allowed us to identify potential issues and
refine the product before deployment. So, we were feeling pretty good
about the product before the cruise. The reason we had to test on the ship
was—testability. Some things can't be simulated. And just being close to
real users and sharing their environment maximizes value-related testability.
But, sometimes, improving some aspects of testability can impair other
aspects.
The ship environment did surprise us. Sensors had to be moved around for
aesthetic purposes, so we couldn't always perform the tests we wanted.
Sometimes the Wi-Fi signals weren't strong enough when sitting close to
both the ocean and a pool. Water absorbs Wi-Fi signals, and there were
times the sensors would just not pick up the IoT device.
There were times when we would successfully order some food with our
system—no errors occurred—but we never got what we ordered. We would
search in vain for a technical fault, only to discover our orders did not get
delivered because bar staff weren't watching the order system. It was a new
system that wasn't part of their normal routine. Even if they were looking,
they sometimes tapped the screen to acknowledge the order and then forgot

all about it. This was a failure of project-related testability—in this case,
poor coordination and communication. Another problem with project-
related testability is that the system could not be reconfigured at sea, and
developers rarely traveled with us.
The "poker chip" would also unlock or lock your stateroom door for you,
allowing you to be hands-free. It would activate when you were about six
feet away from the door. Until we were on the ship, we didn't realize that
six feet is just enough distance to go around the corner and be far enough
away that someone else could slip into your room. This is an example of
how value-related testability improved by being on the actual ship.
Did I mention there was more than one ship? This first ship took the longest
because it was a learning experience. With each successive cruise, we
reduced the amount of time we took with testing, and the product kept
getting easier to test. With each installation, we learned more and therefore
reduced the gap between what we knew and what we needed to know. In
that respect, testability steadily improved.
None of these anecdotes so far has been about intrinsic testability. I have
found that my work as a manager is more about subjective, project-related,
and value-related testability. It's my job to help the testers do the best
testing they can do. This may be by making sure they have access to the
people they need to talk with to gather information. Sometimes it's about
helping the tester increase their testing skills, thus making it easier for them
to test.
Advocating for Testability
My job as a tester is to shine the light on anything that threatens the value
of a product. When I ask to make a product more testable, I need a clear
understanding of my needs. I can't expect people to read my mind or care
about testability if they aren't doing a lot of testing. So, what is blocking my
light? What can hide from my light? How can I get a brighter light? I want
to be ready with at least some specific actions they can take toward better
testability.
Some kinds of testability are difficult to achieve. Ask for it anyway. To
paraphrase the old saying, the best time to ask for testability is—years ago.
The next best time is now.

Step 1: Identify Things That Make It Difficult to Test
Testability issues are especially easy to spot when I come aboard a new
project:
If it's difficult for me to get up to speed, what seems to be causing
that?
Where did I get confused, and what did I get confused about?
What onboarding steps did I miss (or were missing)?
If it's easy for me to join, what seems to make it easy?
On an ongoing project, I take notes not only about the product but also on
what made my testing go faster or slower. I like to review my testing, from
time to time, because I know I can't catch everything right at the moment.
Taking some time to reflect on what I did helps surface certain subtle
testability problems.
I may also go through the RST Heuristics of Testability list to get ideas.
At one of my previous companies, the decision was made to deploy some
services to a new data center. We decided to construct some end-to-end
automation to check them. The fastest way to do this would be going
through the platform's APIs instead of the GUI. But the APIs were poorly
documented and out of date. They no longer followed the new architecture
of the product. I did the best I could with the information available to me
until I hit a brick wall. I spent hours trying to figure out how to create an
account and then delete it again when I was finished with the test.
In frustration, I reached out to one of the GUI architects, who promptly told
me I couldn't do what I wanted to do because the APIs didn't support it.
Well, I didn't want to give up, so I started digging into the code to see
exactly what I could and could not do. To my surprise, I found that another
version of the platform APIs had been implemented. This version 2 of the
APIs had a call that allowed me to create and delete accounts. The
development team confirmed that a second version of the APIs did exist and
could be used, and that they just weren't documented or publicized. They
were created a few years back to help the development team—but no one
told the testers!

What was making testing difficult here?
Lack of an API to provide vital services for testing.
Presence of an API that had a lot of services—but was hidden.
Lack of awareness by development about what testers needed to know.
Lack of communication with developers that would have taught them
what testers needed to know.
Step 2: Determine Who Can Help You Solve the Problem
I talk to everybody. Everyone has their own knowledge, ideas, and ways
they can help. On the ship, the product managers had years of cruise
management experience. Onboard IT people—who had to be trained on
identifying issues and doing simple fixes, anyway—could help us test.
Working with the crew was productive, on the whole, because they
understood their customers.
People can improve testability directly by pitching in to test, or they may
have process improvement ideas.
Obviously, the developers are the prime movers of intrinsic testability,
which boils down mainly to controllability, observability, and simplicity.
With the cruise ship project, we were testing in a production environment—
with cruise guests all around us. Whatever we were doing to test should
never interrupt any passenger's experience. I racked my brain trying to think
of ways to test that would stay out of their way. My mind instantly went to
installing new versions of the software in areas not frequented by the
guests, and to get better ways to install or roll back versions of the software.
I brought this problem to the attention of a developer. He came up with a
better solution: feature flags. He programmed the application to have a
hidden button that would allow me to log in as a tester in order to toggle
features for testing in an off/on state. Now I could go about the ship and test
whenever and wherever I needed because I could easily change the product
state.

Step 3: Sell It ("What's In It for Them?")
Some things are hard to do. Just in terms of intrinsic testability, a lot could
be done. For instance, the developers could simplify the code, make it more
modular, or sprinkle observability hooks throughout. But re-architecting the
whole product is never a quick or easy fix.
It's a lot easier to get help from people when they see the value it provides
to them. Once I figure out who I should be asking for help, I like to try and
put myself in their shoes. For example, why would a developer want to take
the time to refactor their code in order to improve testability? Well,
testability helps me give them faster feedback. Besides that, simpler code
has fewer bugs. And when I find bugs, it could also mean easier
investigations and easier fixes.
What's in it for leadership or the product owner in allowing me to take time
to write automation? If it's automation to do simple setup steps (because it's
faster for a computer to do that repetitive task), this gives me more time to
do deeper testing and find more—and better—bugs.
My Job Is Testability
As a test manager, you could say my job is testability. You could say I'm a
sort of testability engineer.
I was once a Director of Testing for a mobile fitness application. When I
started work, I found that the company had been relying on an outside
company to do all of their testing. These testers were taking weeks and
weeks to do regression testing and had thousands of test cases. They were
always complaining about and making excuses for how long it took them to
perform their testing.
To help them help themselves, I felt they needed to improve their testing
skills and reduce their reliance on test cases. So, I hired a talented test lead
(Djuka Selendic, well known in the RST world for her knowledge of
session-based test management), and I brought in Paul Holland to teach
RST to the testers.
The team went from testers and product managers complaining and fighting
each other to a cohesive team working with developers to get the job done.

Even though I was not directly testing the product, I was still able to
improve testability by hiring key people and upskilling my team.
But I'm not always successful.
I once worked for a company that produced a set of libraries for writing
software to control robots. If you needed the robot to move an arm, you
used one library. If you needed the robot to walk, you used another library.
Combining the libraries would allow you to control a robot in walking and
moving its arms.
When I talked with the development team, I asked them who the customer
was for this product. They kept telling me it was "other developers," as it's
the developers that put the libraries together to program the robot. I
understood that, but I knew they weren't the only customers. Someone had
to use the robot and maintain the robot after the developers programmed it.
In order to test that experience, I needed someone who could tell me how
the robot would ultimately be used. Without understanding the user, there
was a risk that the software wouldn't meet their needs. But I kept being told
that the product was so easy to use that there was no need to talk with users.
I did the best I could to test, based on what I was told.
Toward the end of the project, the team was asked to program a robot for a
customer. The team combined the libraries that allowed the robot to do what
the customer needed, but when they delivered it, the customer refused to
use it because the interface had red and green buttons. Since some operators
of the robot were color-blind, they had a requirement for blue and yellow
buttons. The team therefore had to change the colors.
This was not necessarily a big bug, but it underscores my point. Sometimes
the testing that needs to be done can't be done. Sometimes testing is made
unreasonably expensive and difficult to do, by forces beyond the control of
the tester.
Still, we can make testability, in all its aspects, a standard talking point in
our projects. This starts with testers who make it their business to learn
what testability means.
OceanofPDF.com

CHAPTER 12
Mindopsy: Dissecting My Thinking as a
Tester
—By Xiaomei Tai
The video "Testopsies: Dissecting Your Testing" by James Bach and
Michael Bolton is about learning testing in a deep way by dissecting a real
testing process into very short intervals. I was fascinated by this new term,
"testopsy,"1 since I have been doing similar things for the past 8 years as a
consultant. Now I had a name for it.
I was inspired to coin similar terms, such as:
Mindopsy: Dissecting your problem-solving to learn your thinking
Bugopsy: Dissecting a bug report to learn from this bug
Valueopsy: Dissecting the given requirements to learn the values
behind them
I often practice these "opsies" in my work, sharing the results with my
colleagues.
What Is a Mindopsy?
People use different mindsets and skill sets while communicating or during
exploration. A mindopsy is a process of analyzing the products of your
mind—in this case, work products produced during testing—to better learn
how you're thinking while you are thinking. In this article, I'd like to
introduce the idea of mindopsy with a true story.
In my role as a coach, I work with people across my organization to help
them with their test strategies. One day, Rong, a business analyst, was
trying to tell me about a feature described in her requirements document. At
that moment, I knew nothing about this feature or its product. My goal was
to interview her and her team to quickly learn about the feature. I had to ask
the right questions.

During the next two hours of conversation, the team and I conducted a
deep-value analysis of this feature while I took structured notes. As shown
in Figure 12-1, the structured notes looked like messy handwriting, but they
had a big impact—helping us reach alignment.
Figure 12-1: Structured notes made on a whiteboard
After that coaching, the team asked me how I came up with those questions.
I thought about this question carefully since I am always trying to find a
way to teach people the skill of questioning. I thought of two ways: one
would be to dissect my mindset using this real conversation to remember
what kind of thinking I used for each question I asked; the other would be
to dissect the contents of the conversation to understand the logic and
structure of the value analysis. I call the former mindopsy and the latter
valueopsy. This chapter focuses on mindopsy.
Conversation with the Business Analyst
At first, Rong flooded me with information:

This feature is called Monitoring Alarms Configuration for real-
time tasks. What we need to do now is to alert users of real-time
tasks of their healthy status or abnormalities in the cluster. There
are a series of rules about how and under what circumstances the
alarm should be alerted. What we're going to do in this iteration is
decide how we're going to configure the alarm rules and what the
rules are going to be.
Just imagine how I was feeling while all these strange words entered my
mind. What are these real-time tasks? What is the product? What is meant
by "healthy status"? What is the cluster? And so on. Rong continued her
story:
The target audience is real-time modeling engineers. The feature's
value is monitoring the life and death of real-time tasks and then
making alarm notifications to users so that users can handle them
in a timely manner. In this iteration, three scenarios will be fulfilled.
The first scenario is about resource warnings in the cluster, the
second scenario is about abnormal warnings of task status, and the
third scenario is about feasibility alerts.
Although just one minute had passed, I felt overwhelmed. Obviously, this
was not good communication, because we were not reaching alignment
regarding what we were talking about.
Before she went on, I interrupted her and started the second phase of this
conversation: my process of leading by asking questions.
During the next two hours, I asked roughly 200 questions. Fortunately, I
recorded the conversation. Here, I have selected the first six minutes to
dissect:
Xiaomei: That's me.
Rong: The business analyst (BA), who is responsible for developing
product requirements based on some input requirements from other
people—in this case, the SE.

SE: System engineer. The person responsible for analyzing users'
original requirements and then outputting the initial analysis results to
the BA role for further requirement analysis.
Dev: A developer.
NO WHO
QUESTIONS AND ANSWERS
1
Xiaomei Who put forward this requirement, real-time task monitoring
alarms configuration?
2
Rong
The SE.
3
Xiaomei The SE? Then who puts it forward to the SE?
4
Rong
No one. It is the SE who puts forward this requirement.
5
Xiaomei Okay, why does the SE put forward this requirement?
6
Rong
For the purpose of functional optimization of our product.
7
Dev
Our users also put forward this requirement.
8
Xiaomei Do users put forward requirements, too?
9
Dev
Yes.
10
Xiaomei That is to say, besides your SE, who wants to optimize
functions, your users also put forward this requirement.
Correct?
11
Dev
Yes.
12
Xiaomei Who are the users?
13
Dev
Team A, who will utilize our alarm configurations.
14
Xiaomei Ommm, so Team A also puts forward this requirement. Your
team is responsible for implementing alarm rules
configurations, and Team A will use these configurations in
their applications. Am I right?
15
Dev
Yes. And these configurations are mainly for real-time tasks.
16
Xiaomei Was there any alarm monitoring for real-time tasks before?
17
Rong
No, there wasn't.
18
Xiaomei So, this is a totally new feature, right?
19
Rong
Yes.

NO WHO
QUESTIONS AND ANSWERS
20
Xiaomei For real-time task monitoring, which one is true: "The only
thing your team needs to do is alarm rules configuration," or
"Alarm rules configuration is only one part of your whole
work?" Which one of these will be implemented in this
iteration?
21
Rong
We already do monitoring alarms for offline tasks.
22
Rong
For real-time tasks, there are very limited alarm notifications
within the code. What's more, users can't use the
notifications through the user Interface (UI).
23
Rong
For this iteration, we only need to implement alarm rules
configuration.
24
Xiaomei So real-time tasks monitoring alarms is an independent
feature that can be delivered to users so that users can use it
through the UI?
25
Rong
Yes.
26
Xiaomei I see. So alarm configurations will be implemented in this
iteration. What are those things that won't be implemented
this time and will be done later?
27
Rong
Data collection, to collect the monitored data.
28
Xiaomei If you don't implement data collection but just implement
configuration rules, what's the use of that?
29
Xiaomei Anyway, let's put data collection aside for now. So, besides
data, are there any other things that are not going to be
implemented this time?
30
Rong
Alarming—i.e., monitoring alarms based on the
configuration rules, and alarm items selected by users, such
as sending notifications, something like that.
Analysis of the Conversation
Let's analyze each of the questions in this conversation to discover what we
can learn from it.

Line 1: Who put forward this requirement, real-time task
monitoring alarms configuration?
I could have started the questioning in different ways. This time, I decided
to focus first on the sources of the requirement. You can see that we indeed
focus on this point for a while. So, I am using focused thinking to have a
deep exploration about the requirement sources.
In my experience, people often have little idea about why they need to
implement a certain requirement, so they are not clear about the value of the
requirement to their customers or users. In that case, it's not surprising that
they also lack knowledge about their users, make a lot of bad decisions
along the way, and ultimately deliver software that doesn't meet their
customers' needs.
Line 3: The SE? Then who puts it forward to the SE?
When Rong answered, "The SE", I made an assumption. Perhaps the real
reason behind this is that, in the usual workflow, the BA receives the
requirements from the SE and starts further analysis. Maybe she didn't
really understand my question, so I should double-check to avoid
misunderstanding.
What kind of thinking was I using? I would call it critical thinking. I
appreciate James Bach's way of explaining critical thinking so much.
Usually, three parts can be identified in a critical thinking conversation:
Huh? Really? and So?
Now, I was doubting whether Rong really understood my question. This
belongs to the "Huh?" part.
Line 5: Okay, why does the SE put forward this requirement?
When Rong confirmed with me that it was indeed SE who put forward this
requirement, I still doubted its credibility. I'd like to know the reason behind
it, so I asked this question: "Why does the SE put forward this
requirement?" This belongs to the "Really?" part of critical thinking.

For this question, I got two pieces of information:
One is a direct answer by Rong: "For the purpose of functional optimization
of our product."
The other is extra information from the Dev: "Our users also put forward
this requirement." This phenomenon is very interesting. I call it "buy one,
get one free," which is a bonus for the questioner. If you want to get
valuable information from a conversation, please don't neglect these
bonuses.
For the direct answer, I had more questions to ask in depth, since the answer
was really a rough one. But I decided to let it sit for the moment and instead
focused on the second answer. What I applied next was my self-
management skill, an essential skill in any kind of exploration—the ability
to decide when and where to use what kind of methods to achieve certain
goals. During an exploration journey, there are frequent scenarios where the
explorer has multiple paths to choose from. Which path to try first? When
to stop on this path and turn to another path? James Bach calls this skill
branching and backtracking.
The kind of thinking related to this self-management skill is sky thinking. I
coined this term, inspired by Six Thinking Hats by Edward de Bono. The
blue thinking hat lets us think about a blue sky—in other words, looking at
the big picture.
Line 8: Do users put forward requirements, too?
I smelled an important clue, i.e., "users." I didn't want to miss it. I asked
this question to confirm with the Dev. It is related to the critical thinking
"Huh?" part.
Notice that "users" are different from "customers."
Customers refer to people or enterprises that have direct transactions and
business dealings with us, or the party that directly receives products or
services from us.
Users refer to people or enterprises that use products or services directly.
Very quickly, I got a "yes" answer.

Line 10: That is to say, besides your SE, who wants to optimize
functions, your users also put forward this requirement. Correct?
I confirmed with the developer all the information I had so far. Again, it is
related to the critical thinking "Huh?" part.
I got another "yes."
Line 12: Who are the users?
Since "users" was a totally new piece of information to me, and I knew
little about it, I decided to focus on that.
The very first question to ask about an unknown is naturally, "What is it?" I
call that a "Layer 1" question, i.e., asking shallow and direct questions to
gain an initial understanding of a thing. This Layer 1 question belongs to
the "Huh?" part of critical thinking.
It is easy to ask Layer 1 questions. But after that, be prepared to handle a lot
more information. In this case, I didn't get much—just, "Team A, who will
utilize our alarm configurations."
Line 14: Ommm, so Team A also puts forward this requirement.
Your team is responsible for implementing alarm rules
configurations, and Team A will use these configurations in their
applications. Am I right?
Based on my limited knowledge of the system, I confirmed my
understanding of the relationship between the system managed by this team
and the system used by Team A with the developer so that I could quickly
build a big picture in my mind.
It belongs to the "Huh?" part of critical thinking.
Compared with "What is it" Layer 1 questions, this question tries to dig
deeper into the information. That is to say, I want to learn more about the
unknown information—in this case, "users"—beyond Layer 1 information.
For this purpose, I need to ask "Layer N" questions. (Note: I don't want to

bother differentiating between Layer 2 questions, Layer 3 questions, and so
on.)
For this question, I got another piece of bonus information from the answer:
"Yes. And these configurations are mainly for real-time tasks." So next, I
opened a new branch in my exploration, using focused thinking to dig
deeper into "real-time tasks."
Line 16: Was there any alarm monitoring for real-time tasks
before?
The words "real-time tasks" aroused energy within me.
I use the Energy Principle to guide my exploration. It can be stated as:
"Follow the energy." I got this concept from the book Weinberg on
Writing.2 I apply this principle in the following way:
For anything you do, pay attention to your emotions, such as happiness,
surprise, anger, confusion, etc. Your feelings won't deceive you—they are
real. If you look back to think about how you got the kind of feeling you
have, you may find that you just walked through a special path. I often use
"ACRCUF heuristics" to categorize these paths (see Figure 12-2):
Association path: The words you just heard, the things you just
observed, or the actions you just took are all related to something you
already know. This association arouses your emotions and provide new
thoughts.
Contradiction path: The things you just experienced are contrary to
what you believe. This contradiction arouses your emotions and
provided new thoughts.
Regular events path: You experienced some events again and again
until you noticed a pattern. These regular events arouse your emotions
and provides new thoughts.
Curiosity path: You did something due to curiosity. This curiosity
arouses your feelings and provides new thoughts.
Unexpected things path: Something unexpected happened and
aroused your feelings, providing new thoughts.

Familiar things path: You just experienced something you were
familiar with, which aroused your feelings and provides new thoughts.
Figure 12-2: Following the energy
In this case, I walked through a "curiosity path" and felt a kind of
"doubting" energy. Why did the Dev emphasize "real-time tasks"
purposefully? Maybe there was something I didn't know here. I was curious
to dig deeper into it. So, I decided to ask this question for further
exploration and experimentation. As you can see, I did get some insights
from the following answers.
What kind of thinking was I using? I'd call it creative thinking. Sometimes,
I just want to ask questions for no specific reason or without any clear
logical explanation.
Line 18: So, this is a totally new feature, right?
As expected, this is a new feature for real-time tasks. Again, I want to
confirm it with my critical thinking before moving on.

Line 20: For real-time task monitoring, which one is true: "The
only thing your team needs to do is alarm rules configuration," or
"Alarm rules configuration is only one part of your whole work?"
Which one of these will be implemented in this iteration?
After getting the confirmed answer, I began to ask Layer N questions
related to this new feature.
From Rong's answers, I got three pieces of information:
1. We already do monitoring alarms for offline tasks.
2. For real-time tasks, there are very limited alarm notifications within
the code. What's more, users can't use the notifications through the
user interface (UI).
3. For this iteration, we only need to implement alarm rules
configuration.
For the first one, a new term appeared: "offline tasks." I could ask many
questions about it, but I decided to use sky thinking again to stop this
branch.
Line 24: So real-time tasks monitoring alarms is an independent
feature, which can be delivered to users so that users can use it
through the UI?
For the second piece of information, I asked this question. I inferred from
"using it through the UI" that real-time tasks monitoring alarms might be an
independent feature. You can see that I jumped from a single point, "using it
through the UI," to a bigger area of "delivery as an independent feature." I
call this Defocused Thinking. This inference might be wrong, so I use a
critical thinking tool to confirm with Rong.
I asked about independence because that is one of the four aspects of
requirements I generally want to learn about: demand, independence,
directness, and logically making sense.

Line 26: I see. So alarm configurations will be implemented in this
iteration. What are those things that won't be implemented this
time and will be done later?
The third piece of information reminded me of the scope of the
requirement. My experience teaches me that whenever people tell me the
things included in the scope, I need to ask about the things that will not be
included. This is a kind of negative thinking—asking questions from the
opposite side.
Line 28: If you don't implement data collection but just implement
configuration rules, what's the use of that?
Rong's answer to my last question was, "Data collection, to collect the
monitored data," which gave me a big puzzle.
But I decided to use sky thinking again to pause this branch so that I could
learn more about the scope of the requirement.
Line 29: Anyway, let's put data collection aside for now. So, besides
data collection, are there any other things that are not going to be
implemented this time?
Since Rong just gave me one specific example on the "not-to-do" list, I
wanted to follow up to see if there were more. Anytime there is one thing
on a list, there might be more than one thing.
Tree-Type Notes for This Conversation
This conversation can be displayed more vividly in the structured graph
shown in Figure 12-3.
You can see that during these six minutes of conversation, we explored four
different branches: the sources of the requirement, users, real-time tasks,
and the scope of this requirement. Figure 12-4 zooms in on one of the
branches, so you can see it clearly.

Figure 12-3: Tree-type notes, full view
I call this graph TTN, for tree-type notes. I often take TTN for explorations
I have conducted or observed. TTN can be real-time notes or not, depending
on your purpose. TTN is very useful for reflections afterward. The big
difference between TTN and common mind maps is that, for the most part,
the contents in TTN are recorded in sequence over time. This makes it easy
for the explorer to recall the exploration journey completed weeks, months,
or even years ago.
The TTN shown in Figure 12-5 was made on the spot in 2016, during one
of my training courses. I took these notes as I explored a game while the
students watched.

Figure 12-4: Tree-type notes, detail view

Figure 12-5: Real-time tree notes
Summary
This chapter dissects a six-minute conversation taken from a two-hour talk,
from the perspective of thinking—especially how I ask those questions. The
following points can be learned by this mindopsy process:
Recording a process and doing a deep dissection is a very effective
way to learn.
Your thinking style greatly influences the kinds of questions you ask.

Mindopsy refers to the way of analyzing our mind to better understand
how we're thinking while doing knowledge work.3
Making real-time structured notes has a big impact in reaching
alignment in collective-learning processes.
If you are in a review meeting and someone tells you about a lot of
unfamiliar requirements, don't just sit there passively. A better
approach is to actively ask questions. You will learn better that way.
There are a number of kinds of thinking people will use during
conversations or explorations.
I frequently use focused thinking, defocused thinking, boundary
thinking, factoring thinking, negative thinking, critical thinking, sky
thinking, anchored thinking, systems thinking, creative thinking, and
more.
Differentiate between Layer 1 questions and Layer N questions.
Exploration requires the skill of self-management.
The Energy Principle is a powerful tool for any kind of learning or
exploration process.
Capture your feelings. "ACRCUF heuristics" summarize the common
and different possible paths that might be walked to trigger those
feelings.
Tree-type notes can be used to record an exploration process (long or
short) and are useful for later reflections.
Notes
1   DevelopSense, 2025. Testopsies: Dissecting Your Testing [video].
Available at:
https://www.developsense.com/video/TestopsiesDissectingYourTe
sting.mp4 [Accessed 28 January 2025].
2   Weinberg, G. M., 2006. Weinberg on writing: The fieldstone method.
Dorset House Pub.

3   See also "cognitive task analysis."
OceanofPDF.com

CHAPTER 13
Rapid Usability Testing
—By Torbjörn Ryber
It is the Easter holiday, and I am currently on vacation with my family. We
are skiing for the weekend. As the family's main planner and travel
administrator, I need to get ski passes for the four of us. We waited until the
last minute to choose which passes to buy due to the unusually unstable
weather and the ambivalence of teenagers. The decision lands on a two-day
pass. The resort has an app with a weather forecast, information on what
slopes and lifts are open, and, of course, the shop where you spend your
money on ski passes and rental skis.
I open the app on my iPhone and select our resort. There are six resorts to
choose from, and it's easy to make the initial selection. I continue to the
page for buying ski passes and find three ski pass cards already registered
from the last trip. Since we are avid skiers, I have a stash of cards at home,
and, as it turns out, I have managed to bring two of the ones already
registered. I proceed to unregister the missing card and register two
additional ones to make a total of four. Some of the cards are a few years
old, and the alphanumeric code is hard or impossible to read. Luckily, the
app has a function for scanning the cards by holding them to the back of
your phone. After they are uploaded into the app, I label them with the
names of the family members. (This personalization makes it easier to get a
replacement card if one is lost.)
Since I want to take a cross-country tour before dark, I check the status of
the tracks. I find the option for a cross-country ski pass, and it appears that I
have to pay for that as well. I heard from a friend that this is not true for all
trails, so I investigate and eventually find a small symbol by each trail
saying if it is a pay-trail. It turns out the one I want to use is free of charge.
Nice.
Few apps I use have such great usability as this one. The experience for me
was mainly positive, although I was still confused by the issue of whether
to pay for using cross-country trails. That confusion may be caused by a

business decision—what I call "dark UX"—nudging people toward things
they do not really want so that the business makes more money.
What I have just described could be considered a "lightweight usability
test." Deep usability testing reveals more issues than this. But even if you
perform a realistic walkthrough of your product with three to five users, I
can almost guarantee that you will identify important problems with
usability. Does that sound worth doing?
Why Do We Fail to Test for Usability?
Usability gets a lot of press—usually when it's not there. Some large
projects in Sweden (the PUST Police Investigation System and the School
Platform in Stockholm, for example) have failed or been scrapped because
of users not being able to perform even basic tasks without extraordinary
effort.
My experience is that usability testing is rarely done. Too few people
realize how easy it can be to identify terrible design flaws through the filter
of usability. One possible reason is that usability testing has a reputation for
being cumbersome and expensive, with people in lab coats supervising
users through one-way mirrors. Indeed, to get statistically valid results, you
may need a lab environment with strict protocols. But what works for the
needs of academic researchers may not translate to the business world. In
the world of commercial software projects, we don't necessarily need hard
proof that a product is hard to use. Suggestive evidence—sometimes a
single experience—is often enough.
There is scientific support for lightweight methods. In 1980, K. Anders
Ericsson and Herbert Simon published a report1 describing the "think aloud
protocol," which is now the core approach for effective usability testing.
The next year, Alphonse Chapanis and colleagues suggested2 that observing
five to six users using the software in a usability test with the "think aloud"
protocol would reveal most usability problems. This was later confirmed by
Jacob Nielsen, who analyzed his own statistics.3 These findings correspond
well with my own experience.

Understanding Users and Goals
The ISO standard 9241-11 defines usability pretty well:
Usability is the extent to which a system, product, or service can be
used by specified users to achieve specified goals with effectiveness,
efficiency, and satisfaction in a specified context of use.4
In order to truly understand a user's needs, I think the best way is to meet
them, have a conversation, and then observe them performing their daily
routines. I'm looking for the problems they need to solve and in what
context they happen.
I also need empathy for the business goals, since someone's paying for the
software's development and wants something in return. In a typical formal
development process, usability research is done before developers start
building anything, leading to artifacts like personas and user journeys. But
if that work isn't done, I'm okay. I can use my agency and ambition to figure
it out as I go.
Creating Personas and User Goals
The concept of personas originates from theaters and movies, where actors
needed to get a deep understanding of the characters they were portraying.
This is probably the main reason why a lot of personas used in IT are
cluttered with information on hobbies, feelings, age, hair color, and other
character attributes that are essential for grasping a movie character but
irrelevant for anything we build.
In software development, personas were popularized by Alan Cooper.5
Personas have since been loved by some and scorned by others. Cooper and
his followers claim that personas should always be created based on
knowledge of real users. Interviewing intended users of your system will
reveal a lot of insight into what they need and want.
As Cooper puts it, "Our goal in constructing a set of personas is to represent
the diversity of observed motivations, behaviors, attitudes, aptitudes,
constraints, mental models, work or activity flows, environments, and
frustrations with current products or systems."6

I find it typically takes two to five personas to characterize the users of a
product.
The Electricians and the Rental Company
Here's an example of my work for a company whose main business was to
sell parts used by electricians. My job was to review their website and
create a better one. The client's experience with interacting with their
customers was limited to a few very positive clients, and they thought they
knew everything they needed to know about their customers. I convinced
the client to let me do some usability testing of the current site.
There were two main personas: the individual electrician and companies
that purchase merchandise to rent out to electricians. The users I chose were
five electricians and two rental companies. I asked them to describe a
typical day of work, and I probed deeply into how they decided what to buy
and how they ordered parts. Some meetings took place at rental stores or
worksites, which helped me get a deeper understanding of their needs.
Others took place in cafés and restaurants.
After those discussions, I came up with the following personas.
"Eric," the Electrician
Eric works at a small company with five employees. The typical client is a
homeowner or the owner of a small office building. The main steps in his
working process are as follows:
One day in advance, he visits the building where the job is to be done,
walks through each room, and counts the different parts they need—
number of sockets, light switches, ceiling lamps, fuse box, etc.
He looks up the parts he needs in the web shop and places the order.
Depending on the need for speed, he picks up his order the next
morning at the store or has it express-delivered.
He places one order per client and has one to five clients per week.
It's important that a part is easy to install and does not break within a
year.

He often names a price for a job, along with a number of hours to
complete it.
If things are hard to install or break, Eric must fix them for free and lose
money. Part of his own business deal is that he orders the parts, gets a
discount, and then adds a percentage when billing the client. It may turn out
that a low price is not the main reason for buying. He also wants to be able
to get a list of the purchases with the full price before any personal
discounts, so he, in turn, can bill the client the retail price plus 10%.
In the course of interviewing users to create this persona, I identified some
usability issues:
Electricians like to use the parts they are familiar with. But there may
be similar parts with slightly different designs, and the only way of
telling them apart is by the serial number.
It turned out that it was really hard for them to find the exact same
parts with each purchase.
To solve this, the electricians used a feature on the site for copying an
earlier order. Our client had specifically pointed out that the "copy
order" function needed to be excellent, since it was used often. But
when I interviewed the electricians about this, they all answered that
the copying of an order was not really what they wanted.
Copying an order was not optimal because they rarely needed the same
quantity of parts, and they also had to remember to change the client
information. They used this feature because it was the best available
option, not because it was a good option.
What they really needed was to be able to choose "favorite" items so
that they could easily pick them for their next purchase.
They preferred not to buy ahead and keep parts in stock for
themselves. This was because they would inevitably have to buy
special parts anyway, and it was more convenient to pick up
everything at one time and place at the store. This also meant that
time-based deals (e.g., 50% off popular parts on Fridays) weren't very
interesting to them because they would only buy what they needed at
the time they needed it.

I also discovered that an electrician works alone. One of the few times
they actually get to interact with colleagues is when picking up their
order at the store, which was also desirable because a free breakfast
was included there!
This shows that you can get great testing ideas and find usability issues
regarding the product by interviewing users. You may also find issues that
affect the company's business strategy. This can be a strong motivator for
spending money on usability testing and for empowering testers to use their
judgment and curiosity to learn more about how the product is intended to
be used.
"Rob," the Rental Guy
The rental company user persona represents a different set of needs.
Their business idea is that some equipment used by construction companies
is only needed temporarily. It is fairly expensive to buy and takes a lot of
space to store, so they prefer renting it. Another advantage of renting is that
they can bill the cost to their client.
Rental companies purchase things differently than individual electricians
do. A rental company makes a large purchase, maybe once every quarter.
When it starts to get cold and dark in the fall, contractors know that there
will be a large demand for things like lighting, long cables, and heating
elements. So they identify a few products in the store and ask for a quote:
"If I buy 40 lamps, 2,000 meters of cable, and 30 heaters, what price can
you offer?" The store and the contractor agree on a deal outside of the web
store, with delivery handled by the rental company's truck directly to their
storage.
The side effect of doing this research and creating the "Rob" rental guy
persona was that our client decided they would handle this business outside
of the new web store, at least initially.
Thus, personas and user needs can be used both for usability testing of an
existing application and planning for either an updated or brand-new
application.

Living the Persona
I once did some work for a company building heavy machinery. They had
software to help them manage it. Their idea was to "take the current desktop
app and make a smartphone app version."
To keep track of the machines, they have administrative systems, with a lot
of data and screens showing statistics, including lists of information. The
machines they built are very expensive and require careful maintenance in
order to get the most value out of them. Service intervals, spare parts, and
replacement machines were all determined by one measurement: engine
hours.
The work site has its own Wi-Fi set up, and newer machines are hooked up
so you can follow their position and usage in real time. The most important
function we were building was to get engine hours from older machines not
currently online, and also from those machines that were temporarily out of
range of the Wi-Fi. The idea was that the machine operator would take a
picture of the engine hour counter. Later, when online, this value would be
synced to the system.
Oh, and this is all happening a mile underground, in a dusty, noisy, wet
hole.
Since there was an existing desktop app, the project team had not planned
to do any usability research. What was the point? There were no personas or
documentation of user needs for the existing application, so I started
gathering information about the work context.
Here's what I found out:
A mine is usually deep down in the ground.
The climate in a mine can be really hot—so hot that they put blocks of
ice in the mine with large fans behind them to make the temperature
bearable.
It can also be freezing cold.
Most of the time, it's wet and dirty.
An operator of a million-dollar mining machine is not allowed to bring
distractions to work (such as smartphones).

So even before testing, we found serious issues regarding usability that led
to re-planning our app. The new goal was changed from "an iPhone app
used daily by an operator" to "an app used by a foreman, updating engine
hours weekly on a rugged device." The usability test on the initial version
consisted of taking a photo of the engine counter of a demo machine in a
garage, analyzing the photo, and syncing the numbers to the database.
In my role as UX Researcher and Designer, the preceding story is a process
problem. Not having this explicit information before development is bound
to get you in trouble. But as a tester, I say use whatever means you can in
order to understand the users and their needs.
The fallback solution, if you have no way of getting any information on
personas or needs, is to base the usability tests on any user stories the
application is based on. Despite the name "user story," a lot of stories are
written just like functional requirements, only with the addition of the
phrase "As a user..." User stories may not document any of the flavor or
depth of the experience of using that part of the product. So, you may have
to close your eyes and visualize what it will be like to use the product.
How to Document a Useful Persona
Keep your persona description brief and to the point. Describe context,
needs, the most important functions needed, and also what must be avoided.
It might be tempting to add details on age, gender, education, pets, vacation
habits, and proficiency in using electronic devices. One heuristic is that if
any information does not add to the understanding of what to build, leave it
out. Add information when it makes sense, but never add irrelevant
information just because you think it makes the persona "rich."
Rapid Usability Testing Basics
Now, when you have an understanding of the users and their needs, how do
you do the actual testing? As per the definition of usability, you want to
investigate whether "the product can be used by specified users to achieve
specified goals with effectiveness, efficiency, and satisfaction in a specified
context of use."
The basic idea is to watch people try to use the system to accomplish tasks
and see where they struggle or get confused.

Recruit Users
If you can get real users to help you, get them! This means a person who
performs the actual work and thereby will be able to try doing tasks
effectively.
For a public site that sells movie tickets, we can sit down at a café and ask
any person who enters the door for fifteen minutes of their time in exchange
for a free coffee. But if we build a site for electricians or miners, we need to
select users carefully. Aim to get at least three users per persona and five if
the budget allows. Book testers in advance; hopefully, you can reuse the
people that researchers interviewed. Plan for 30-60 minutes of active
testing per session, depending on how much you want to test.
Specific situations mean context-specific. If you design an app for miners
dressed in overalls and gloves to register logged machine hours for a drill
deep down in a cold, damp, and dirty mine—preferably do the testing in
that environment. If you find out that electricians usually place their orders
on the run, make sure you test on a mobile device.
Plan and Prepare
Prepare a list of tasks that users should be able to do with your product.
These should be as naturalistic as you can make them. That means not just
running down a list of functions but also using the product to solve a
problem.
If prototypes are available, they can be used for testing general structure
and navigation, as well as visual design. The main goal at this stage is
usually to evaluate if you are on the right track at all.
The next stage of usability testing is when the team starts to produce
working code. A recurring event in Scrum is demos. Demos are great as
presentations of what has been built, but in addition, you need an evaluation
of how usable it is. My suggestion is that you plan for usability testing
whenever new user interface components are added or changed. Try to test
at least once every month/every second sprint to make usability testing a
natural part of the development process.
I am surprised that acceptance testing of an application often does not
include usability as one of the areas. Isn't the main goal of acceptance

testing to verify that what is built actually solves our problems? Certainly,
fulfillment of regulations and adherence to business rules as stated in the
requirements are important, but if usability is too bad, the whole effort may
be scrapped! The Swedish school platform I mentioned earlier claimed to
have all the functionality in place. But still, it received reviews like this one:
"This app works worse than my last marriage and makes me feel worse than
when Snape killed Dumbledore."
So no, it is not enough that the functions are implemented or that a
requirements list is checked off.
Usability tests are often very specific tasks. Make sure you have all the data
and the test environment in place. You do not want the tests to fail due to
bad preparation and waste the user's precious time. Use equipment you have
control over unless you specifically want the users to bring their own
device.
Make sure you have some time to set everything up before the test session
and at least half an hour directly after to analyze and compare notes. I
personally find that three to four test sessions in a day are the maximum I
can manage; more than that, and I start mixing things up.
Run the Test
In each session, do the following:
1. Introduce the process. Tell them that you are evaluating the design
and not their skills, and ask them to be frank and not hide anything.
Explain that in order to truly understand what goes on inside their head
while they are performing the tasks, they need to think aloud so you
can follow their mental models and reasoning. This is called the think-
aloud protocol and is the basis for our qualitative usability testing, and
probably the most widely used method. What we do is to externalize
their mental models. Ask a few warm-up questions about who they are
and what they do to verify that you have a good match, but spend no
more than a few minutes.
2. Give the user one task at a time and ask them to think aloud. Some
recommend giving the user a list of all the tasks at the beginning, and
that may be a good idea if there is a lot of specific data they need to

use. It is important that you get as much information out of each user
as possible and that you let them think and try for themselves. Do not
answer directly to requests for help. Instead, ask back, "What would
you normally do in this kind of situation?", "Where would you look?",
"What do you think is expected now?" Only if they get really stuck for
a long time should you tell them where to go next, and the test is then
recorded as a failure. If a user gets really quiet, you can ask them what
they are thinking about. Make the user feel safe: if it sounds like they
are getting frustrated, remind them that you are not testing their
abilities but how well the design works.
3. Observe and take notes. I like to have at least one observer with me
during a physical session since it can be hard to instruct and document
simultaneously. It is very important that observers keep quiet and make
no noise at all, including comments, sighs, nervous laughs, or direct
instructions. Focus on the following:
Does the user manage to successfully complete the task you asked
them to perform?
What major mistakes and detours were made?
Was there anything the user found hard to understand?
Were there any problems that you noticed yourself?
It is very likely that you will spot the major mistakes in real time.
Some people like to record the session, but I find little need for re-
watching, so I generally don't.7 Still, it can be interesting to show
certain parts to any manager or developer if they are in doubt about
the problems you saw. Steve Krug8 recommends that as many people
as possible observe together real time in a separate observation room
during the testing. Then they can send secret messages and questions
they want asked to the test lead during the session.
4. All observers analyze the results for all the sessions together. Then
present the report for the whole team with a list of each problem and
explain the negative effects. Prioritize as a team. The larger problems
will usually jump out at you! That is why it is enough to test with 3-5
users for each persona. Fix the most important issues first, and

definitely fix the ones that are really easy to correct while the code is
still warm!
At this point in time, if the previous activities, including prototyping and
evaluation, were done well, there is usually no need to redo the whole
design. However, if you have a massive failure and users are clueless, this
must be addressed. The reason you want recurring small bursts of usability
testing is that you want to avoid going too far in a direction that will not
work. Ending a large development effort with some last-minute usability
testing will just leave you with a long list of problems that no one has any
intent to fix since you have run out of time and money. Let me quote the
outspoken design guru Mike Monteiro: "Leaving design to the last minute
is akin to baking a cake and then asking a baker to come and make it tasty."
I Am Testing an Off-the-Shelf Standard System—What Should I
Do?
You may find yourself implementing a standard system off-the-shelf. How
does that affect how you do the usability testing? Well, I would say that in
that case, usability testing is even more important. Since so many
implementations of standard systems fail due to a lack of usability, I believe
that you need to know if it is possible to configure this system to make it fit
your users before you decide to buy it.
When evaluating a standard system, it's more important to have done your
research and created personas, user journeys, and user stories in advance.
Usability testing should be based on this research and done on at least the
most important parts of the intended setup as a probe into what the system
can do. You will probably have to pay the vendor to set up a minimum
version of your desired implementation. They may very well protest and tell
you it is too advanced, takes too much time, and would better if just "sign
right here, please." You must insist! At least enter a clause in the agreement
that says that you are allowed to cancel everything in six months, or so, if
the usability tests fail.
If this standard system is not built for your specific users and it is not
possible to configure it to fulfill their needs, you need to know that before
you sign any contract. You also need to evaluate if the components you use
are usable and accessible, since it will be very hard for you to require any

changes in the building blocks of a standard system. If it is not possible to
configure the system to your needs, and if components are not inherently
usable, then, well, I think you are doomed.
I was a part of such a project where there were a lot of complicated work
processes, and many of these meant managing large lists of issues. We spent
months with some really skilled developers trying to make these lists
usable, and, in my opinion, it was impossible since we had to base them on
a number of standard configurations that did not fit our needs.
Now, it is possible that your needs are very generic and that selecting a
standard system will be a great idea. You will find this out when you test.
But if you have very specific needs ... you may very well be shocked by
how bad the usability is. Prepare yourself to say no and end the project
before you have spent millions of dollars and wasted years of agonizing
work.
Checking Usability
What if you have no access to real users and no one is willing to let you do
any usability testing involving outsiders due to company secrets? I have
designed interfaces for more than ten years now. Let me tell you that
basically everyone you ask will have an opinion about any solution you
design, regardless of their actual knowledge on the matter. Your personal
opinion on design will usually not be very appreciated by designers or
developers. It is human to voice an opinion regardless of how much you
really know, but this needs to be replaced with a superior method. Until this
dawned on me, as a tester, I had a really hard time getting through to
designers regarding usability problems.
As a tester, you can investigate the user interface using general or specific
checklists. It is important to understand that these checks should be done
using objective and preferably agreed-upon checklists. Here are some
examples of common checklists to start with. Use them and adapt them to
your own needs.
General Checklist of Usability (Nielsen-Norman)
This is a classic checklist used by designers to evaluate if they follow the
general principles of usability. These rules are widely available on the web

with a lot more details.
1. Is the system status visible to the user? Do they know what is going
on? Feedback like spinners, error messages, and are you logged in?
2. Is there a match between the system and the real world? Do we use
appropriate language and graphics?
3. Does the user have a feeling of freedom and control? Can they cancel,
navigate freely?
4. Are we following the design guidelines chosen for the application?
This includes types of fields, padding, margins.
5. Error prevention. Do we design to avoid slips, unconscious errors
caused by inattention and mistakes, as well as conscious errors based
on a mismatch between the user's mental model and the design?
6. Recognition over recall. Information needed should be visible, and this
excludes the option of having to write it down on a post-it for the next
step!
7. Flexibility and efficiency of use, such as shortcuts, personalization,
and configuration.
8. Aesthetic and minimalist design. Clear primary actions.
9. Help users recognize, diagnose, and recover from errors.
10. Help and documentation when and where a user needs it.
Web Content Accessibility Guidelines (WCAG)
This is a standard created by W3C9 with the aim to be a world-wide guide.
It is far too large to fit here, but I'll list some examples. There are plenty of
tools to check these points.
1. Does text have enough contrast to be easy to read?
2. Do all images have an alt-tag describing what they show? Some auto-
generated tags suggest tags like "A picture showing people..." which
is a horrible idea, since the screen reader tools add the part "A picture"
automatically.

3. Do headings in each page follow a specified order h1 (one only) and
then h2-h3?
w3.org has the complete list for three levels: A, AA, and AAA. Most
companies I worked for choose the AA level.
4. Do fonts and sizes follow the design guidelines?
5. Are we using the correct primary and secondary colors?
6. Do all logos follow the given standard? It is seen as a deadly sin to
manipulate company logos!
These checklists can be used within the team to weed out common
mistakes, starting already with prototypes. It is a good idea to continuously
do this type of checking to avoid having a large "design debt" build up.
Failing to make an application usable will stop it from being the success it
may otherwise become—as well as being a giant waste of money and
resources. Testing for usability can be relatively simple and cheap and will
help you identify and fix the worst flaws.
Notes
1   Ericsson, K. A. and Simon, H. A., 1980. "Verbal reports as data."
Psychological Review, 87(3), pp. 215-251.
https://doi.org/10.1037/0033-295X.87.3.215
2   Al-Awar, J., Chapanis, A. and Ford, W. R. 1981. Tutorials for the first-
time computer user. IEEE Transactions on Professional Communication,
PC-24(1), pp. 30-37. https://doi.org/10.1109/TPC.1981.6447820
3   Nielsen, J. and Landauer, T. K., 1993. A mathematical model of the
finding of usability problems. Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems - CHI '93, pp. 206-213.
https://doi.org/10.1145/169059.169166
4   Note that in RST, usability is defined simply as how easy it is to use a
product. This then breaks down into sub-categories, such as learnability,
operability, etc. James Bach and Michael Bolton consider most of the
ISO/IEC/IEEE definitions to be stilted and overly formal. Much of the

over-formality, in their opinion, comes from the goal of making concepts
easy to measure instead of useful to consider.
5   Cooper, A., Reimann, R., Cronin, D., Noessel, C., Csizmadi, J., and
LeMoine, D., eds., 2014. About Face: The Essentials of Interaction
Design (4th ed.). Wiley.
6   About Face, p. 81.
7   I should note that James Bach swears by recordings, especially for
testing that is expensive to reproduce. He finds he misses many things
that he later notices when he watches a recording.
8   Krug, S., 2010. Rocket surgery made easy: The do-it-yourself guide to
finding and fixing usability problems. New Riders.
9   W3C, 2025. Web Content Accessibility Guidelines (WCAG). [online]
Available at: https://www.w3.org/WAI/standards-guidelines/wcag
[Accessed 28 January 2025].
OceanofPDF.com

CHAPTER 14
RST Meets Signals-Based Testing
—By Wayne Roseberry
Microsoft collects a massive amount of telemetry from users of Microsoft
Office. This telemetry does not include the specifics of what users type on
the screen, of course. It's all engineering data about what parts of the
product are being used and error conditions that occur.
Let's call this data signals.
For seven years, on the Microsoft Office team, I worked on ways of using
signal data from the field to guide testing in our labs. We called this signals-
based testing. In this chapter, I will describe signals-based testing, then
show how it fits with the Rapid Software Testing methodology.
Signals for Testing
I define signals-based testing as testing by operating the product in such a
way as to produce a specified profile of engineering and operations data
(i.e., signals). Signals-based testing is essentially a coverage-based
technique, though it also encompasses the analysis of signals to detect
failure.
Signals are data emitted by the product that is not intended for consumption
or use by its primary users, but rather for the operations and/or product
team. This differentiates it from normal product output, such as user data
files, UI displayed on a screen, sound emitted, or even the behavior of
machinery and electronic devices. Much modern commercial software,
whether running locally or hosted online, produces signals that go back to
the product team. The data is usually a mixture of diagnostic information
and customer usage patterns. The purpose of the data is to allow the product
team to improve the product in various ways by identifying new feature
opportunities, observing problems, and spotting emerging issues in
production. Though signal architectures are not often built with testing in
mind, we can exploit them for that purpose.

The key idea of signals-based testing is to generate interesting signal
patterns in the lab and then to analyze them to spot problems. We focus on
patterns that appear to be popular in the field data, mysterious, or associated
with failure. When we operate the product in certain ways to generate
signals, we call that a "workload." The workload might be engineers
interacting with the product directly, or it might mean automated bots
driving the product.
Signals-based testing is not like the typical automated "output checks" that
result in a definitive pass or fail. Yes, signals can provide authoritative
oracles in some situations, such as when a definitive error state is logged.
But we can use signal data for more than that. We can look for suspicious
patterns and investigate them, applying the same analysis that we do when
looking at live signals from the field. Signals-based testing is, therefore, an
exploratory process.
Signals-based testing could be done prior to the first release of a product,
but the technique is more powerful when you have access to signal data
from users in the field. In that respect, it belongs to the context that RST
calls remedial testing.1 This is testing that happens after release, either
responding to user feedback or racing the customer to find problems first.
How This Looked at Microsoft
My first instance of putting signals-based testing into practice was years
ago when Office was doing a major redesign of its multiuser online
authoring service. This was a complex feature, and the backend service and
the client applications had to change to support the redesign. The product
teams were targeting something they called classroom scale, imagining 50
students using the same document—some doing reading, some doing
editing—for durations of an hour or more. They wanted to put this feature
under several operations simultaneously with controlled amounts of
variation and see what effect it had on application and service stability.
They needed something more complex, inducing a more intricate state, than
they could achieve by a few testers interacting with the product in real time.
They also wanted much richer test results than could be achieved with
simple automated checks that returned a pass or fail signal. They wanted to

analyze the product using telemetry collected during large-scale multiuser
coauthoring sessions.
To do this, we had to change our automation system. Our existing
automation system was designed for short-running tests in simple, single-
platform configurations spanning small numbers of machines. I worked
with the product teams and modified the automation framework to allow
longer-running tests with an arbitrary number of concurrent synthetic users
per test.
We ended up with a round-the-clock automated process from which the
product teams mined signals for signs of failure. These lab-produced signals
were similar to the signals we received from real customers, and we
analyzed them in the same way.
Self-Taught AI-Generated Workload
We took this further on the Office team by experimenting with AI. My idea
was to develop a system that would automatically learn how to navigate the
application, then get it to produce a signal profile that approximated
realistic usage.
To do this, we used a tool originally developed by the Windows team. The
tool was able to find and trigger elements in the Office applications and
build a model of how the application state changed as it manipulated those
elements. Those behaviors generated telemetry signals that could be
compared with real-world patterns of signals from the field. The training
rewarded the model for matching those signals—or for reaching a crash
state.
Over time, the training process added more and more paths to the model,
allowing it to reach more features in the application. It missed some
features that were available only if certain content existed, so we helped the
tool access them by adding files loaded with specific classes of content
(charts, graphs, images, tables, animations, etc.). This would allow different
controls and features to light up as the agents explored the document. It was
not difficult to grow the coverage of the application features by adding hints
and nudges, either in documents on the system or by adjusting the training
tool settings.

We ran hundreds of machines continuously, picking up new versions of the
Office applications as they came out of the build system. The self-taught
AI-generated patterns tended to be more random than the sequences we had
created intentionally. That made the system less representative of real-world
use, but it was able to reach complex application states without our having
to write and maintain the scripts. When the application changed, we
directed the tool to build a new model, and the new rewarded behaviors
would eventually take over from the earlier, less relevant ones.
Notice an important limitation of this practice: the automation was blind to
many kinds of important bugs. It was only interested in navigating through
the product and producing signals. Still, we found plenty of crashes and
other reliability-related bugs that way.
Analyzing the Signals and Looking for
Failure
If you try signals-based testing, you'll have to work out how to analyze the
signals. Here's how we thought about it on the Office team.
Feature Coverage
Different events in the telemetry stream represent different functional
behaviors and code paths in the application. For example, when the
spellchecker or grammar checker executes, it emits an event to the
telemetry stream. When the user selects a command from the menus or
toolbars, there is an event specifically for the control they selected.
Blocks of new code that have been turned on by a feature-gating switch
emit an event to the telemetry stream (for example, the set of flags that
initialize the product state). Each of the Office client applications emits
around 5,000 distinct telemetry events. Our goal was to cover as many of
these events as we could.
To do this, we expressed the total number of possible events by sampling a
distinct event count from the telemetry collected in customer sessions. We
divided the total number of distinct events seen during the synthetic
workload by the total number of possible events to get an event coverage
percentage. Once we had this percentage, along with the events covered, we

could start analyzing how long it took to reach a certain percentage, identify
what coverage was missing, and seek ways to move the percentage higher,
etc.
The team in Office responsible for the feature-gating capability (the ability
to instantly turn Office features on or off for different groups of users)
found that the signals produced by our synthetic workloads allowed them to
spot emerging problems when feature gates were turned off and on in
different combinations. This allowed the team to keep the features disabled,
behind the gate, until a fix was available. Then, they could watch the signals
coming back from synthetic runs against the new, fixed builds.
Crashes and Hangs
Our synthetic workload was prolific at generating crashes, hangs, and other
problems that occurred frequently. These were easy to report because
Office, like other Microsoft products, already had an enormous set of
services, tools, and processes in place to report such problems automatically
to product teams.
The Office team had a cross-product committee of engineers, the Reliability
Virtual Team, that worked together on improving crash investigations and
responses. Part of my job was to explain how our tools were creating the
workload and to get feedback on ways we could make it easier for the
engineers to investigate the failures.
In addition to working with the Reliability team, individual product team
engineers would contact me about issues they noticed in the telemetry data.
Several times, the synthetic workload hit crashes that users had hit, but
which the product team was unable to reproduce. Investigation in the lab is
almost always easier than trying to figure out how a customer had hit the
same problem. We had logs of all the actions the agents attempted leading
up to the crash, which made troubleshooting easier.
Assertion Failures
Signals-based testing doesn't come for free. In the case of product errors,
we had a mixture of decades-old designs and behaviors that were not
converging well. Any further progress on that front was going to require
standardizing how the developers reported telemetry.

The Office code is quite old, going all the way back to the 1980s. Assertions
are pieces of code that, under special debug builds, will halt execution with
an error message under certain conditions. The Office team often used these
checks to find defects during testing, in an era when we assumed that all the
errors would be captured from a developer's or test engineer's local
machine, or possibly during an automation run. These checks were deleted
for release builds.
Over the years, the Office group developed the telemetry stream, and many
of the assertions were converted to error events. However, the volume of
information and the uncertainty of how conditions were triggered made it
difficult to turn these into useful bug reports. Nobody had thought about
how the information would be used and what it might mean to try to deal
with errors from a centralized service.
This meant there was a lot of inconsistency in how developers implemented
error reporting. Some developers only put errors into the signal to report
severe problems that indicated something bad was happening. Other
developers reported things that they might have wanted to watch, but were
not necessarily problems. Still others signaled normal, everyday error states
that the product handled on its own without the user noticing (e.g., errors
from fetching network data, intermittently unavailable files on storage,
etc.).
Some developers used shared metadata to indicate errors, and some used
error markup that was specific to just their team or feature area. This range
of markup and meaning made it difficult to draw general conclusions about
errors from the signals. Anyone who wanted to understand something
specific had to know about the feature itself to target specific errors.
The Office Release team started an effort to standardize practices around
error telemetry and to report product health metrics based on a prioritized
set of user scenarios. We worked alongside them to align the measurements
with our synthetic workloads.
Performance Markers
The Office Performance team could detect performance problems based on
markers in telemetry for high-priority Office features. This involved
accumulating stats from a large sample of user sessions, then looking for

deviations in that data over time. Although we also did performance testing
in our labs, the signal data allowed them to catch problems coming from
more complex sets of conditions.
The challenge with these user-based performance measurements was that it
took several weeks after a build was released before there was enough data
from the users to provide a meaningful measurement. The first two groups
of users—the Office team (about 5,000 people) and all of Microsoft
(between 150,000 and 200,000 people)—were too small to get stable and
reliable distributions. We had to wait until the builds got as far as external
early adopters—people who had signed up for early release builds—before
the data became sufficiently useful.
We demonstrated via our synthetic workloads that we could generate
signals and observe performance marker regressions about a week and a
half earlier than the first group of external users. This gave us the ability to
produce a fix and avert the release of the regression.
Problematic Patterns in Events
Even if individual events are not erroneous, certain sequences of them may
be. I call these problematic patterns. This came up during the coauthoring
feature rewrite.
One example of problematic behavior was something referred to as
dropping out of an online session. It was normal for the client application to
revert to a local cache of the document during online editing, especially
with intermittent network connectivity. After a brief amount of time, the
client would connect again and join the online session with the other users.
The product team engineers noticed in the signal data that for some longer,
larger user sessions, a client would drop out for too long. This kind of
problem is not revealed by the presence or absence of any particular signal
but rather by a relationship among signals. An end user would experience
that as being out of sync with the other users, seeing a document as stale or
out of date, yet no specific error message would be sent.
I've seen two basic approaches to analyzing problematic patterns. One is a
large-scale statistical or machine-learning analysis of signals, with the
purpose of finding correlations or clusters of signals that predict problems.
The other is an ad hoc process of making hypotheses about specific patterns

of signals that should or should not travel together. Either way, pattern
analysis is something worth trying.
Getting Started
Here are some ideas for getting started with signals-based testing.
Look for signal sources.
Almost every application, product, or service has some kind of
diagnostic data. There is almost always a product log file or
application event stream that developers use to analyze and diagnose
product behavior. If you are developing a web-based product, then you
certainly have web logs, and you can mine data from browser debug
logs. If you don't know of such information, ask the developers if there
is such a file or diagnostic data source. If there isn't one, or if it's not
helpful enough, talk to the developers about adding it.
Preserve the data in one accessible place.
Establish a repository to preserve signal data that allows you to access
it at will. You also need to have tools to read and manipulate it.
Sometimes you can just open the data in an editor.
Try analyzing it.
You can write tools that scan file contents and produce reports, and
then send the contents to spreadsheets or databases. You can
investigate that data over time, count instances of patterns in the data,
compare data from testing versus real-world usage, attempt different
visualizations, or whatever other type of exploration seems useful.
Look at the results from existing workloads.
Any testing you do already generates a workload. Look on your lab
benches, your work machines, and machines from your automation
runs. If you and your team use your product, collect that information
as well.

You will want to partition and classify the data based on which
workload it came from. If you have signal data from a test session you
performed for a recent sprint, it's useful to know if the signals are
different from what you might have collected from automation runs on
the daily build. It's especially useful to separate and classify the signals
from your internal testing from those you might collect from real-
world usage.
Show the value of the process.
Lessons from signals analysis tend to be hard to learn in other ways.
Other testing tends to be targeted, precise, and focused on narrow
behaviors. Signals-based testing tends to focus on trends, aggregates,
and big-picture problems that only become apparent when looking at a
lot of data and exercising complex workloads. The story you can tell
from a signals-based testing approach tends to catch people's attention
and spark their imagination.
Product signals are also an important part of many product business
strategies. The more you can show examples of using real-world
feedback to improve and understand the testing analysis, the more
people will want to apply the same technique to help improve the
product overall. You might have to start with skunkworks, low-cost
versions of signals-based testing, but if your team takes an interest,
you can build a far more powerful set of tools to support it.
Add test metadata to synthetic workloads.
The first problem we had to deal with in signals-based testing was
being able to differentiate the testing workloads from the customer
workloads. More than that, we wanted to be able to distinguish which
workloads and which tests, we were running. To do that, there needed
to be some kind of test metadata in the signal.
The primary reason was to keep the real-world workloads distinct. A
business creates a telemetry signal to make business decisions.
Activity from synthetic, testing-driven workloads is going to confuse
those decisions. You need to make sure the testing-generated
workloads are easily and obviously identified.

Our second goal was to perform an independent analysis based on
different testing purposes. We wanted whatever testing, all the way
down to an individual engineer running an investigation, to afford
specific and tailored analysis of their own. For that reason, we made
changes to the Office product that allowed us to augment the signal
with testing metadata.
Anybody exploiting their existing signals collection and analysis
system for testing will have a similar set of concerns to address.
Add filters to the existing reports and tools.
A powerful part of the signals-based testing approach is borrowing
from analysis and investigation already applied to real-world customer
usage. Whatever tools and systems can process a customer signal can
do the same for synthetic, testing-driven workloads.
To do that, those tools, reports, and data-processing pipelines and
engines need to implement and use filters based on the testing
metadata you have added to the signal. Look at these tools, and then
talk to whoever builds them and uses them before you start generating
testing signals. Some tools and reports might have important impacts
or consequences, and you don't want to accidentally trigger an alarm
somewhere.
You might also want additional new filters, pivots, or analyses added.
For example, Office's telemetry system offered reports to do feature
coverage and crash occurrence statistics on a per-customer segment
basis, and I wanted the same on a per-test-suite basis. This involved
some design discussions with the team building the telemetry reporting
services.
Create new testing workloads to cover the signal space.
You can usually start with whatever workload you already have. I
found that our daily automated build validation runs were useful, as
were the "reliability runs" we used to identify variance in automation
suite results. But everything else started to come in for free once we
turned on the telemetry stream.

The workload you create should be based on some perceived need. In
our case, the rewrite of a major piece of technology around a complex
body of customer scenarios motivated the development of targeted and
rich automated workloads.
How Does Signals-Based Testing Fit with
RST?
RST is not defined in terms of test techniques. Any test technique can be
applied within it. RST is about how to think about and connect to testing; it
is a heuristic for doing so.
The place RST starts is always with the tester:
Who is responsible for testing this product?
What is the mission of that tester?2 What do they need to achieve on
this project?
Is that tester interested in signals-based testing?
The next step is context. If we apply a technique, it should fit the context
of our project:
Is there already a signals architecture in place?
Is anyone else in the organization also interested in signals?
Is there time to investigate and set up a signals-based testing process?
Is the product complex enough to warrant the monitoring of signals?
Do we have, or can we create, automation that creates "workloads"
that will generate signals?
RST sees testing as an ongoing, cyclical learning process. We postpone
formalization until we know enough to do it right:
Do we know enough about signals-based testing to apply it
productively at all? (Or perhaps we want to apply it specifically to
learn how to do it?)

Do we see a way to do it simply and without too much up-front
investment?
What can we do to start experimenting with it today?
RST testing proceeds heuristically, rather than formulaically, and stays
pointed in the direction of product risk. Therefore, the tester should
identify the key ideas and dynamics of signals-based testing. In this case,
they are
Ability of the product to generate signals from user activity
Ability to analyze natural signals from users
Ability to generate signals from synthetic workloads
Ability to relate signal patterns to real or simulated user behavior
Ability to generate specified signal patterns via simulated user
behavior
A heuristic is a fallible means of solving a problem. RST is heuristic.
Therefore, practicing RST means understanding what each technique
can't do and watching out for how a technique can lead us wrong or
waste our time:
Signals-based testing does not help much in checking product
functionality or claims we've made about the product behavior. It
knows nothing about the correctness of the output.
It is shallow and limited in critical domains such as data coverage,
internationalization, security, and other special topics.
It completely depends on the quality and completeness of the signal
generation architecture within the product.
The AI-based workloads we used did little to help with stress, load, or
capacity testing.
I've had to go over these limitations more than once with people who
thought the AI-driven automation approach we used should replace all the
other testing we do. I had a skip-level manager who believed we could use

it as justification to get rid of the testing vendors offering scenario
coverage. I had others suggest we could replace our existing functional
automated checks. A shorthand explanation for what the test method did or
did not do well, helped in these awkward moments.
Does Signals-Based Testing Relate to the HTSM?
The Heuristic Test Strategy Model (HTSM) is a core tool of RST. It's a
systematic way to think through how we might want to test any particular
thing. It is an outline of considerations, not a set of rules, nor a template.
The HTSM is designed to be fairly generic. Each practitioner is encouraged
to customize it. Ideally, using it should jog your memory to think about
every interesting and useful technique that you know about, and when that
form of testing fits your context.
To see how signals-based testing fits with RST, I walked through the four
lists of the model: Test Techniques, Project Environment, Product Elements,
and Quality Criteria Categories. My question to myself was, if I knew about
signals-based testing, but the technique was not at the top of my mind at the
time, would the HTSM help me remember and consider it for my next test
project?
Test Techniques list. "Signals-based testing" doesn't appear in the test
techniques list. That's because it's a list of families of test techniques
rather than every individual technique. The techniques family that best
evokes signals-based testing is domain-testing, which is all about
covering categories of inputs or outputs. Signals are a kind of output.
Project Environment list. In the version of the HTSM I used, signals
don't appear anywhere, nor does anything that would remind me of it.
So, I added it to the Equipment and Tools section, and it was then
adopted into the next official version of the model.
Product Elements list. Signals-based testing is evoked most strongly
by the System Interfaces item, which relates to any communication
between the product and anything other than a human user. I added
"engineering logs" to that item to strengthen the association.
Quality Criteria Categories. For me, the elements that evoke signals-
based testing on this list are Robustness and Testability. When I'm

thinking about how I would know that my product will stand up to a
wide variety of pressures and situations, my mind turns to signals
because signals are an important aspect of observability, which leads to
intrinsic testability.
As a result of this HTSM review against signals-based testing—a technique
that James and Michael, the stewards of RST, had not previously thought
about—they made five specific changes to the official HTSM model.
That's how it works. The model belongs to whoever is using it at the time.
In RST, the tester is in charge of their own work. But sometimes, as you
see, the official versions of RST documents get upgraded at the suggestion
of a tester at large.
Notes
1   The four seasons are described in Chapter 4.
2   By "tester," I mean whoever is responsible for testing, whether a
developer or someone else who does not call themselves a tester.
OceanofPDF.com

CHAPTER 15
Deliberate Practice of Testing Skills
—By Joris Meerts, Huib Schoots, and Ruud Cox
One of the cornerstones of context-driven testing is that the way of working
when testing software is determined by the situation in which the tester
finds themselves. A good approach is not driven by a prescribed process or
by a collection of steps that one habitually executes. Instead, it arises from
the use of knowledge and skill to ensure that testing fits the project.
Context-driven testing requires critical thinking, modeling, visualization,
note-taking, and the use of heuristics. It is not easy to learn these skills, but
one way to sharpen them is to do realistic testing exercises and discuss the
results. In methodology, this is known as deliberate practice1.
This was the purpose of a meeting of four testers at Improve Quality
Services in 2017. In the following report, we will elaborate on the exercises
we did during that meeting and on the results.
The meeting was held on February 14, 2017, at the office of Improve
Quality Services in Eindhoven. The participating testers, Jos Duisings,
Ruud Cox, Joris Meerts, and Huib Schoots, were all employees of Improve
Quality Services.
The Assignment
The assignment of the meeting was to test a certain software application. It
was carried out by two teams of two testers each. This made it possible to
take different approaches and provide feedback from one team to the other.
The meeting was divided into the following segments:
Welcome & introduction
Creating a coverage outline (per team)
Debriefing the coverage outline
Drafting a test strategy (as a group)

Debriefing the test strategy
Selecting a few charters
Performing a test session (charter) per team
Debriefing the test session
Retrospective
We chose the application Task Coach, an open-source to-do list manager.
This software is publicly available and runs on multiple platforms
(Windows and MacOS). The application is relatively simple but still offers
sufficient complexity to be tested thoroughly. According to the description
on the website, Task Coach is "a simple open-source to-do manager to keep
track of personal tasks and to-do lists. It is designed for composite tasks,
and also offers effort tracking, categories, notes, and more." The
participants had not worked with Task Coach before.
Creating a Coverage Outline
Because Task Coach was new for all participants, the software had to be
explored. Only then could we say more about what could be tested in the
given time. Such an exploration of the product can be done in many
different ways. During the meeting, we chose to make a "product coverage
outline," inspired by the Heuristic Test Strategy Model, from RST.
The product coverage outline provides an overarching view of the product
in which the functionality of the software is analyzed according to seven
different categories. The categories are described in the mnemonic
SFDIPOT. The software tester is reminded by this heuristic that when
mapping a product, they can look at Structure, Function, Data, Interfaces,
Platform, Operations, and Time. By looking at the application from these
perspectives, a relatively complete sketch is created of what the software
product is and what it can do. However, the mnemonic does not only serve
for exploration. The "map" of the application that is created in this way can
also be used to visualize the coverage of the tests and to select the tests to
be carried out.
We decided that both teams would be given half an hour to create a product
coverage outline and that each team was free to decide in which format the

outline would be presented. The choice of a half-hour time limit was mainly
motivated by the fact that the software must also be tested before the end of
the day. There was no room to spend much more time making the product
outline.
It turned out that half an hour was not enough time. Team B decided to
create a mind map in which the seven categories were elaborated. This
mind map was not finished after half an hour. Especially the Function
branch, in which the functions of the application are described, was very
large and had a deeply nested structure. To build this structure, Team B
explored the application by clicking through it and captured the
functionality (in text or in images) in the mind map.
A mind map is a tool that is used by software testers on a regular basis to
create a product outline. The tree structure of the mind map lends itself to
classifying product elements. It is easy to start with an empty mind map,
enter the categories from SFDIPOT, and expand on these. Moreover, a
navigable structure is created in this way. The mind map forms a kind of
geographical map in which you can choose to zoom in and zoom out to
determine the location of something in relation to the whole. A mind map
can also be used as an instrument for reporting on the results and the
progress of the tests.
Team A started the session by creating a mind map, but after a short time, it
switched to a different form. When studying Task Coach, the team
discovered that there was a Help file in the application. After a short study,
the Help file appeared to describe a large part of the application, so Team A
chose to use the file, converted to Word format, as a product outline. By
marking the categories in the document with different colors, structure was
added to the document. In addition, the table of contents provided a global
overview of the functionality in the application; the details were mentioned
in the paragraphs. In this way, Team A delivered a product sketch that was
relatively complete within the stipulated time.
Drafting the Test Strategy
In Rapid Software Testing, the process of unsystematic, open exploration of
the product for the purpose of learning about it is called survey testing. The
more focused process of exploring the product to create a product coverage

outline is called analysis testing. In our exercise, we combined these into
one session. The process of making the product coverage outline taught us
about the product.
With this product sketch in hand and the knowledge gained during the
exploration of the software, we were ready for the next step: strategy.
Intentional testing is guided by choices that we think will give us the best
results for a reasonable time and cost. We call these choices the test
strategy.
One way to get to a strategy is to go through the outline and decide for each
element whether it needs to be tested and, if so, how deeply. The key factor
in this decision is risk. In other words: how likely is the product to fail, and
how bad would it be if it did fail? Testers begin with educated guesses about
risk and then pursue the testing that will give them hard evidence about
risk.
The perspective of the tester alone may not be sufficient to get a good
picture of the potential risks of a software product. During the second
session, it became clear to us that we would ideally have some kind of user
representative who could help us understand what kinds of quality mattered
for this product. Failing that, we proceeded as best we could.
In the Heuristic Test Strategy Model, three dimensions are discussed that
influence a test strategy. These dimensions are the project environment, the
product elements, and the quality criteria (including but not limited to the
requirements that are officially stated for that product). Risk plays a role in
each of these dimensions. In the exercise, we decided not to look at the
project risks (since we had no access to the real project). As far as product
risks were concerned, we guessed at what parts of the product would be
used the most, which parts might be most prone to errors, and which kinds
of errors might have the most impact. Because no users were involved in
the exercise, we tried to place ourselves in the role of the users. This is
actually what every tester must do when testing a version 1.0 product that
no real user has ever seen.
Here are the risk areas we came up with:
The "common use" from the Operations category. This means the main
flow and capabilities of the product.

Using the application by multiple users from the Operations category.
We recognized risks associated with synchronization: tasks are not
updated properly.
The importing and exporting of tasks from the Interfaces category.
Dealing with reminders from the Functionality category. Reminders
are crucial for not missing appointments.
Dealing with date and time from the Time category. Date and time play
an important role in planning tasks because they touch the core of the
application.
As you can see in Figure 15-1, after identifying the categories of the
product, we tried to assess which quality aspects of the product might be
most important. We selected:
Functionality
Usability
Charisma

Figure 15-1: Group notes on the test strategy

Drafting the Test Charters
A test charter, in RST, is a brief statement about the focus of a test session
that sometimes takes the form of very high-level test cases, but usually, they
are even more general than that.
We briefly discussed the coverage of the tests that we wanted to carry out.
Since the tasks that are maintained in Task Coach can have different
statuses, it seemed appropriate to think about the product in terms of a state
model. The state transitions are covered in the different paths that the user
takes through the application. Additionally, we had to think about the
variety of test data we wanted to use.
We decided to make two charters and perform test sessions with them. We
prioritized the product categories mentioned in the risk analysis based on
our own insights. From this, we concluded that "the main flow" and
"dealing with date and time" were the two most important things to test. We
translated these two categories into charters.
To formulate the first charter, we looked at the risks that could be associated
with the execution of the main flow. For instance, there was a risk that no
task could be created, or it could be that the task is created, but errors occur
when saving or modifying a task or changing the status of a task. These
considerations led to this charter:
Explore the basic flow of tasks using CRUD to discover/test the
lifecycle of a task.
For the second charter, we considered various test conditions, such as:
The start and end times are the same.
The end time is before the start time.
The date of a task is far in the past or far in the future.
Changing system time.
Work in different time zones.
Dealing with winter time and summer time.

Using different date formats.
With these test ideas in mind, we wrote this charter:
Explore tasks using date/time to discover bugs related to date and
time.
Each team selected a charter and began testing.
Conducting the Testing
Testing the Main Flow
The charter for testing the main flow began with using the product to create
a task. Several tasks were created using the Create Task button on the
taskbar. Subsequently, several underlying tasks were created for a single
task, up to eight levels deep. Under one of the underlying tasks, a new tree
structure of underlying tasks was created. During the creation of the task
structure, questions arose about the maximum depth of the task structure
and about the sorting of the underlying tasks. In addition, the team found
that it was possible to delete underlying tasks and then undo these deletions.
We decided to investigate this functionality in a later session, since we
suspected there may be instability in this feature. The resulting tree
structure was removed by means of the Delete button on the taskbar. After
this, all tasks disappeared.
On the taskbar, there is also a button that offers the possibility to create new
tasks from a template. Two default templates are available. We created tasks
with these templates. It appeared that it was not possible to create
underlying tasks from a template. We wondered why this functionality was
not available. Finally, we deleted the created tasks.
To get a better picture of how templates worked in the application, we read
the Help text. The team explored the menu structure in search of more
functionality that could be related to the use of templates. From the File
menu, we found it was possible to save tasks as templates, import
templates, and edit templates. We created a template.

From the dialog that appears when choosing to import a template, it
appeared that template files have the extension .tsktmpl. But when a task
was saved as a template, it was not possible to find out whether a template
file had been created from this task and, if so, where this file was stored.
We created a task, saved it, and imported it again. We deleted all created
tasks by selecting the Edit > Delete menu option. The team looked a bit
further at the options for removing tasks. It appeared that there was a
shortcut combination for removing tasks: Ctrl + DEL. The team wondered
why it was not possible to simply use the Delete key.
We also found that undoing the removal of tasks in a tree structure of tasks
did not reproduce the original structure. Following the session, we proposed
to look more extensively at the use of templates in new charters. Also, the
functionality for creating, viewing, editing, and deleting tasks and
underlying tasks deserved more attention, especially because this
functionality can be called from many different places in the application.
We could not cover all the various possibilities in this session. More
sessions would be needed to explore this.
Debriefing
At the end of the session, both teams got together to discuss what happened.
We talked about how there was not enough time to fulfill the charter;
another thirty to sixty minutes would be needed. This is not unusual.
Sometimes, several sessions are needed to complete the work. It just
depends on the charter.
The team felt that the description of the detected bug was not clear enough
in the notes. And we realized that we overlooked some of the possible task
statuses. Finally, a new charter was suggested for testing filtering and
sorting.
Testing the Handling of Date and Time
The session started with a clean installation of the application. We created a
new task and looked at the Data tab to see what data could be entered. The
date and time could be changed in separate input fields. The time could be
changed by means of a drop-down or by using the arrow keys on the

keyboard. We found that "00:00" could not be used as a time until we
adjusted the settings under the Preferences menu.
We noticed that the planned start date and the planned end date were not
included in the calculation of duration. It was unclear to us what this data
was used for. When the planned end date was in the past, the task turned
red. When the planned end date was in the future, the task turned purple.
We found it remarkable that the planned start date could be after the
planned end date. Apparently, there was no validation on the order of the
data. Dates that were far in the future were accepted as valid dates.
We found that we couldn't change the start date if the year was set for 1900
and the task was closed and reopened. While studying this finding, the team
discovered that the application created a log file (in the Documents folder)
in which errors were logged. The attempt to adjust the planned start date
resulted in the following error message: "ValueError: year = 1853 is
before 1900; the datetime strftime () methods require year> =
1900." After this finding, we paid further attention to other functionality
around time and date. For example, setting reminders. No problems were
found there.
In addition to planning tasks, Task Coach can also be used to keep track of
the time spent per task. After some research, it appeared that this
functionality was complex and that it was not easy to find out how Task
Coach dealt with time spent. Time tracking can be started with a button, but
can also be done by increasing the time spent in a separate tab. The team
noticed that when entering an hour of time spent, the actual time spent
shown on the tab was just a few seconds.
The time that an application used depended on the time setting on the
system. So, the team manipulated the system time of the computer and
changed it to a Coptic calendar. This adjustment caused the application to
crash after start-up. In the log, a line appeared mentioning an error relating
to an invalid date format.
Finally, the team looked at the connection between the budgeted time and
the time spent. Task Coach is able to calculate the remaining time on the
basis of the variables mentioned. Some tests were performed with variations
in hours, minutes, and seconds. Task Coach handled those variations
correctly.

Debriefing
When reviewing the test notes for the session, the teams felt that the
detailed information reported would have been more comprehensible if it
had been attached to the product coverage outline. Also, one new charter
emerged from the discussion: the testing of the synchronization of tasks
between systems with different system times.
So What Happened Here?
We conducted, in one day, with two teams, some test sessions on an
application that was unknown beforehand. We learned about the
application, developed a strategy, and performed tests. Exploring the
application provided insights that served as a starting point for risk
assessments and for conducting test sessions with a well-defined goal.
The stories we told about our sessions depict the essence of testing: inquiry.
Testing is a learning process. The experience of testing a new product is
encountering it in a variety of ways and making sense of it. We are
explaining the product to ourselves, and we are alert to anything that doesn't
add up.
In this way, the testers provided valuable feedback on the application in a
short period of time. After the test sessions, each team reported the work
done in a joint debrief session. During that short evaluation, the testers told
their stories and answered questions. The evaluation had several goals,
including assessing the success of the mission, translating new insights into
follow-up sessions, looking at notes and descriptions of findings, and
coaching the testers.
In Agile projects, it's common for a tester to be asked to test something
unfamiliar. Moreover, the tester has to deliver quickly. This is why we
chose to practice this highly exploratory form of testing instead of writing
test cases from a spec.
Differing Approaches
Creating models of the application under test is an essential part of software
testing. First and foremost is the mental model—the tester's understanding

of the product. Testers also create explicit models that are written or drawn.
These models can take many forms.
In the Context-Driven school of testing thought (of which RST is an
example methodology), there is no such thing as a best practice. There are
problems and solutions of different kinds that are more or less appropriate
depending on the situation or context. In this exercise, the two teams
created quite different product coverage outlines for the same product. They
used different methods and emphasized different goals. This is normal and
acceptable, but it's helpful to understand why.
The two teams were given thirty minutes to produce an outline. In that time,
Team A was able to produce an outline in the form of a Word document that
included a large set of product elements. Team B built a mind map that
included a much smaller set of elements. So, was Team A better than Team
B? Not necessarily, because creating an outline was not the only goal of the
process.
Team A started its session by engaging with the application to gather
elements. A few minutes into this process, one of the testers noticed that the
application contained a large Help file. The team observed that this file
included a fairly complete outline of the entire product (see Figure 15-2).
Team A skimmed the Help file and decided to copy the text in its entirety to
a Word document. While skimming the text, they applied the SFDIPOT
heuristic as a tool to discover product elements. They noticed that the Help
file contained useful structures, such as a table of contents, and that the
product elements were grouped by feature. The team decided to copy the
structure of the Help file instead of creating a new structure. At the end of
the exercise, Team A suggested adding even more structure by highlighting
each category of product elements with a specific color in order to improve
the usability of the coverage outline.

Figure 15-2: Help file used as a found coverage outline
Team B also started its session by engaging with the application, trying to
understand the various elements before mapping them. They focused on
exploring and interacting with the product. However, the application
contained many functions that were nested deeply within the different
menus, the contents of the screens, the Help file, and even on the website of
the application. Mapping the identified elements turned out to be a time-
consuming task that resulted in a map with a small set of elements.
A hefty debate followed the presentation of Team A's coverage outline.
Team B was surprised by the lengthy Word document. How could such a
huge document be useful as a coverage outline? And how would the tester
be able to understand the application without having explored it? Isn't
learning about and making sense of the application what testing is all

about? Of course, eventually the application has to be tested, but if we
examine what both teams produced in relation to the mission of the thirty-
minute session, both teams produced what was asked for: a product
coverage outline.
One of the reasons why both teams produced such very different coverage
outlines was the absence of a specific purpose. Team A asked questions
about the purpose of the coverage outline, but these questions were
dismissed by the facilitator. Without a clear purpose, any set of product
elements, structured in whatever way, will do.
A product coverage outline can have multiple purposes; it can be used to
learn about the application or to help a tester decide what to test. It is useful
as a source for new test ideas, and it can serve as a basis for test reporting.
Team A's product coverage outline provided a comprehensive set of
elements. But Team B's mind map provided more meaningful connections
between elements. It was easier to see an element's place in the larger
picture and its relation to other elements. This helped trigger new test ideas.
Testing is about learning about the application under test—building that
mental model. Because Team A's focus was on gathering elements, they did
little to build their mental models. Team B focused more on interacting with
the product, leading to a better understanding of it.
Team B felt that the mind-mapping approach helped them develop a better
relationship with the product in the time they had. Team A focused on
gathering elements into an explicit outline. These different approaches may
have been the result of personal biases: one toward learning from the
inside-out, and the other more outside-in.
These approaches and outlines represent a snapshot in time. As the project
progresses, both the outlines and the testers' understandings will tend to
converge.
Individual testers may have predispositions for a certain style of analysis or
presentation. They may habitually apply a certain tool (e.g., a mind map) or
a certain analysis heuristic (e.g., the SFDIPOT tree structure) without
thinking much about how that approach fits the situation at hand. It's
important for us as professionals to be aware of our biases and develop,

over time, a facility with a variety of methods. That way, we will make
better context-driven choices in our projects.
Note
1   Ericsson, K. A., Krampe, R. T., & Tesch-Römer, C. (1993). The role of
deliberate practice in the acquisition of expert performance.
Psychological Review, 100(3), 363-406.
https://doi.org/10.1037/0033-295X.100.3.363
OceanofPDF.com

Part IV
Context and Culture
The testing world is always going to be buffeted by forces and expectations
beyond its control. Testing culture may even die out in certain parts of the
industry, a victim of its frequent tormentors: desperate opportunism and
wishful thinking. But like an elemental spirit, it always springs back to life,
called into existence by fear of failure or—let's hope!—the self-respect and
determination of competent and caring people.
Why does testing matter? What forces act upon it? How do testers come to
be? How do you fit in? We intend these chapters to give you a wider view
of the world in which testers deliver their services.
This is important because Rapid Software Testing is a way of relating to the
world in which testing matters. You keep hearing certain words from us,
such as responsibility and agency. We describe the people who receive and
benefit from our work as our clients. These are fundamental ideas for us.
We mean to convey that testing is no mere technical transaction. It's an
ethical stance, a social imperative, and not least, an identity.
OceanofPDF.com

CHAPTER 16
Lessons of the Post Office Horizon Scandal
—by James Christie
Top managers are rarely interested in the details of the software that runs
their businesses. They just want it to work. They want to assume that it does
work. This is understandable, and it may even be reasonable. But in some
companies, that positive thinking bias can turn into a pugnacious, bullying
insistence. It can become willful negligence—or something even worse
than that.
This is what happened at Fujitsu and the UK Post Office, starting in 1999,
when they forced postal workers to use an utterly unreliable accounting
system called Horizon. Over 20 years, hundreds of these postal workers'
lives were devastated by people in power who blamed the users instead of
the software.
This is a dramatic story, of course—it has literally been dramatized on
British television—and at the time of this writing, a criminal investigation
is finally underway. But what does it have to do with testing? After all, bad
testing is a small part of the Horizon scandal. For the most part, middle
management knew their product didn't work and covered it up. Better
testing may not have helped them.
Here's why it should matter to you: As testers, it's our job to give bad news.
If the conditions in the project are making it impossible for us to do our jobs
to a reasonable standard, we have a duty to say so. This is a difficult and
sensitive process.
Rapid Software Testing methodology is a way of working that presumes our
clients want to know the truth about their products. It presumes that though
people around us may be busy, distracted, and sometimes not very good at
their jobs, they are at least not actively hostile to the testing process. It
presumes that, having hired testers, they actually want the testers to do their
job well.

Without good faith on the part of our clients, excellent testing is effectively
impossible. Maybe mediocre testing, too, for that matter. The behavior of
Fujitsu and the Post Office management is a kind of dysfunction, sustained
over decades, that no Agile process, developer discipline, or fancy test
technique could have overcome. Nevertheless, more people should have
sounded more alarms.
Although you are not likely to serve on a project as terrible as Horizon, you
are very likely to encounter similarly dysfunctional managerial behaviors
and policies. Studying spectacularly bad projects can help prepare you to do
the right thing.
The Post Office Horizon Scandal
Horizon is an accounting system that controls the accounts of some 11,500
Post Office branches around the UK. The system was developed by the
British IT company ICL and implemented in 1999. ICL created a subsidiary
called ICL Pathway to deliver the system.1 ICL was bought in 1998 by
Fujitsu, which won an outsourcing contract to manage Horizon. The
contract was renewed several times and is still in place at the time of
writing. The historical documents talk about ICL, ICL Pathway, and Fujitsu,
but to avoid confusion, I will refer to the supplier only as Fujitsu.
Horizon had previously automated manual accounting processes. The users
were mostly subpostmasters and subpostmistresses (SPMs) who were not
employees but provided a service from their own businesses under contract
to the Post Office. Some users were also directly employed by the Post
Office.
But soon after the introduction of Horizon, branch users started to notice
discrepancies in their branch accounts. The discrepancies were assumed by
the Post Office to be fraud committed by the users. Seven hundred and
thirty-six SPMs were convicted of fraud, theft, and false accounting2
between 2000 and 2015. Further, the Post Office forced many more to pay
substantial amounts to avoid prosecution. These payments were supposedly
for losses revealed by the Horizon system and often amounted to tens of
thousands of pounds.

The Post Office prosecuted cases aggressively, despite the SPMs protesting
their innocence. Many of the supposed perpetrators were jailed, pushed into
financial ruin, losing their life savings, their businesses, and their pensions.
Some suffered serious health problems, and some even committed suicide.3
The only evidence against them came from Horizon, which the SPMs
believed was seriously flawed.
In 2019, 555 SPMs were successful in two group litigation cases against the
Post Office. The first4 of these concerned the contractual relationship
between the Post Office and SPMs. The judge ruled in favor of the SPMs,
who were required to work under an unfair contract, which made them
responsible for all errors and shortfalls at their branch unless they could
prove they were not at fault. As a result, they were held financially
responsible for system errors but were denied access to the information that
would let them prove they were not at fault. This issue was considered in
detail in the court case that followed.
The second case,5 Horizon Issues, dealt with the computer system. When he
delivered his judgment, the trial judge, Mr. Justice Fraser, concluded that
the SPMs' case was substantially correct. In essence, fraud had not taken
place; discrepancies arose from system errors or possibly because third
parties had remote system access, which was poorly logged and controlled.
In his judgment, Justice Fraser ruled that Horizon was an unreliable system,
and he offered scathing criticism of both the Post Office and Fujitsu.
Following this ruling, many of the Horizon-related convictions have been
referred to appeal. Up to the end of February 2024, 102 convictions have
been overturned,6 with many more in the pipeline. In the largest case, when
39 SPMs were cleared, the Court of Appeal not only acquitted them but
ruled that "it was an affront to the public conscience for the appellants to
face prosecution."7 The public concern has been so great that the
government has announced that it would introduce legislation to overturn
all Horizon convictions without the need to go through the normal appeal
process.8
The Horizon scandal had a catastrophic effect on hundreds of innocent
people. The Post Office faces further compensation claims that will amount
to well over £1 billion.9 Even when the expected bill was less than half a
billion pounds, the UK government, the ultimate owner, had announced that

the Post Office would be unable to pay these claims, and compensation
would be paid by the government.10
The sins and blunders of the Post Office and Fujitsu are many and varied,
but I shall try to restrict myself to those which provide useful lessons for
software testers, and my perspective will be that of an IT auditor.
Dysfunction #1: Management Held a Narrow and
Defensive View of the Purposes, Stakeholders, and
Quality Standards for Horizon
A popular failing of traditional software development is to think narrowly
about the purposes of a product. We either do not understand the purpose of
the system we will test, or we have only a shallow, contractual
understanding of its purpose. But when dealing with a complex system, it is
simplistic and misleading even to talk of a single purpose. Such systems
have differing purposes for different groups, and these purposes may not
even be consistent. These purposes can adapt and evolve over time.
Horizon is an interesting example. It was originally designed for the
payment of government welfare benefits through Post Office branches. That
was abandoned. The system was then adapted to be the accounting system
for these Post Office branches. Over time, five different purposes emerged
for Horizon:
1. It provided data to the corporate accounts.
2. It allowed the Post Office to manage the accounts of individual
branches.
3. It provided basic banking services.
4. It was the means by which the SPMs could manage their branches'
finances.
5. It served as a source of evidence in legal proceedings, both criminal
and civil. (The Horizon system was usually the sole source of evidence
in the prosecutions of SPMs for fraud and false accounting.)
The first three purposes were clearly recognized by the Post Office.
Bizarrely, the third, allowing SPMs to manage their branches, was not

acknowledged. This was implicitly confirmed in the Horizon Issues case, to
which I will return in more detail. The judge found that the system did not
provide SPMs with the functions or the data to understand what was
happening at their branch, even though they were legally and financially
responsible for any losses, as well as for any discrepancies between their
cash holdings and what the system calculated should be there.
The Post Office always insisted that Horizon complied with the
requirements they had specified and was working as designed. That
unshakeable confidence looked absurd after a judge ruled that it was not
fulfilling its essential purposes, regardless of the requirements or design.
The fifth purpose, the provision of evidence for litigation and prosecution,
was not directly addressed by the Post Office. I suspect that fraud
investigation was never explicitly considered as a purpose of Horizon when
it was developed. The Post Office wrongly assumed that Horizon was fit for
this purpose. The corporation's argument amounted to the assertion that
Horizon was totally reliable for all purposes if its quality was sufficient for
the Post Office's first two purposes.
These five purposes are different from each other, and testers would have
found different bugs and written very different test reports, depending on
which purpose they were considering. The SPMs were treated with little
consideration or respect. They were not Post Office employees but provided
services under contract to the corporation. They received a derisory amount
for these services and were expected to make a living from their business,
usually a shop, in which the Post Office branch was located. The theory was
that the Post Office branch would attract customers who would spend
money in the shop.
The SPMs were therefore not subject to the normal legal protections that
would apply to employees. The lack of respect that was extended to the
SPMs was clear from the way that Horizon was designed and managed.
This raises a problem for testers, but one that traditional, or standards-
driven, testers are not encouraged to resolve or even recognize. Context-
driven testers are a different matter, as the issue is spelled out explicitly11 in
the Seven Basic Principles of the Context-Driven School. Testing delivers a
service to the management that commissioned the project.

We see testing as a service to stakeholders who make the broader
project management decisions.
Testing is done on behalf of stakeholders in the service of
developing, qualifying, debugging, investigating, or selling a
product.
One of the ways in which context-driven testers would depart from a
traditional approach is that they are required to think more widely about the
implications of their work. As the commentary says:
The Context-Driven School of testing places this approach to testing
within a humanistic social and ethical framework.
Clearly, the financial and managerial stakeholders' plans, wishes, and even
their instructions, are important, but they cannot be the only considerations.
Context-driven testers cannot be constrained by corporate diktat. They have
to think about the ethical implications of their work. The users of the
product are to some degree stakeholders. Society itself may be a
stakeholder.
What is a humanistic social and ethical framework? A humanist can be
defined as "one who is concerned with the interests and welfare of
humans."12 A context-driven tester must therefore think about the needs,
interests, and welfare of the people who will use and be affected by the
system. Placing an ethical responsibility on testers means that they must
take responsibility and control over their own actions, and over the way
they choose to conduct testing.
Consider principle five of the seven basic principles of the Context-Driven
School: "The product is a solution. If the problem isn't solved, the product
doesn't work."
Context-driven testers of the Horizon system would therefore have had to
take account of the needs of the SPMs. For the SPMs, the system did not
work. Ignoring the needs of this most important class of users was
unethical, and this neglect had dreadful consequences. It can be a false
choice to choose between the ethical and the pragmatic. Following a more

ethical approach would have saved the Post Office from bankruptcy. Paying
over a billion pounds in compensation is hardly pragmatic. That's why
testers should have a holistic interpretation of quality that embraces the
actual requirements, not just the written ones.
As Jerry Weinberg said, "Quality is value to some person."13 James Bach
and Michael Bolton expand the definition for RST: "Quality is value to
some person who matters."14 The quality of the Horizon system was
abysmal from the point of view of the SPMs who were jailed because of
system errors, but to the senior management of the Post Office, these people
did not matter. Any decent people, and good testers, would certainly have
believed the SPMs mattered a great deal. Justice Fraser (the judge in the
Horizon Issues trial) took the same view.
When system quality was assessed in court, the judge considered the SPMs'
viewpoint. He ruled that the system was unreliable. That is, it did not allow
SPMs to manage their branches reliably, and it was not a reliable source of
evidence for criminal prosecution. If Horizon had been assessed from the
viewpoint of the SPMs, then effective testing would not have produced a
flattering verdict.
Who decides who matters? Testers don't decide, but they do use their
judgment on their duty of care for the people who matter.
Do they listen only to management? Courts can take a broader view than
corporations. If corporate executives take a hard line with the needs of some
users, the executives might find that courts take an even harder line with the
corporation.
There are some things a system should always do, and some things it should
never do. Systems should never lose people's data. They should never
inadvertently produce apparently fraudulent transactions that could destroy
small businesses and leave the owners destitute. The amounts at stake in
each individual Horizon case were trivial as far as the Post Office was
concerned—"immaterial" in accountancy terminology. The discrepancies
were irrelevant from a corporate accounting perspective, but for individual
SPMs, they were big enough to change (and ruin) lives.
The Post Office persisted with its claim that the system was robust and
reliable until the corporation was humiliated in court. Horizon might have

been acceptably reliable for the corporate accounts, but Horizon was riddled
with flaws that resulted in countless errors with individual transactions. The
Post Office and Fujitsu refused to accept that such errors could be system
problems and insisted that users must have been responsible, whether
deliberately or carelessly, or through incompetence. This applied even to
errors that were induced by poor usability and design.
The Post Office looked at the Horizon prosecutions from a perspective that
assumed a generally reliable accounting system must also necessarily be
reliable for holding individual SPMs to account and as a source of criminal
evidence. Justice Fraser took a quite different view. Fraser was concerned
with how the system behaved in the hands of SPMs and how it served their
needs as they fulfilled their contractual responsibilities to the Post Office. If
the system forced honest and competent people to make mistakes, he
reasoned, the system was at fault.
The determination of the Post Office and Fujitsu to absolve the system of
blame and accuse users instead was such a constant theme that it produced a
three-letter acronym that I had never seen before: UEB, or User Error
Bias.15 This bias was identified and named by the SPMs. The Post Office
never accepted that it existed, but it permeated their whole approach. They
would refuse to admit that a bug existed if there were any way they could
blame users instead. This is a dreadful mindset, and I cannot imagine any
competent, responsible tester would have accepted it.
Dysfunction #2: Management Allowed a Terrible
Development Process to Overwhelm the Testers
It is impossible to say with certainty how Horizon was tested by Fujitsu.
Testing was performed by many different people over many years. The
system was developed in the late 1990s, and over the following decades, it
was subject to radical changes.
However, it is possible to gain an insight into the testing culture at Fujitsu
and the Post Office from evidence submitted to the official Post Office
Inquiry set up by the UK government.16 Post Office personnel were actively
obstructed from being involved in a full quality assurance and testing
process.

The evidence submitted in the Horizon Issues case was worthless in
assessing testing. Fujitsu declined to provide witnesses but relied on the
Post Office's own consultant to act as an expert witness. This witness, Dr.
Robert Worden, provided two written submissions to the court and then
appeared in person.17 Worden appeared to be a strange choice of witness
because he did not know what testing standards and processes should have
been followed in the original development of Horizon and had nothing to
say about test execution.
The Post Office's curiously diffident defense of the system's testing during
the Horizon Issues trial puzzled me. All became clear when detailed
evidence about Horizon's development and testing emerged at the public
inquiry. Fujitsu was an incompetent and irresponsible supplier that had no
idea how to build a complex distributed system like Horizon. The standard
of development and testing was abysmal, and that was known to Post Office
staff at the time.
The Post Office's reluctance to mount a detailed defense of Horizon's
development and testing now made sense. The corporation had no wish to
disclose what really happened and cynically preferred to give a false
impression by putting forward an "expert" witness who was not briefed on
the true story.
A full account of the failings of the project to develop Horizon would
require a lengthy chapter on its own, so I shall briefly run through the main
failings. Suffice it to say, experienced, competent testers have little to learn
from the detail. They would have no intention of performing testing in such
a hopeless manner.
Fujitsu did not follow any recognized development method or installation
standards for development, including unit testing. Instead of a responsible,
rigorous approach, a crude prototype was hacked together in order to sell
the solution. This was the basis for the system.18 Rather than starting again
with properly designed and developed components, the prototype was
continually extended, with extra functionality added. There were no design
specifications. The system design documents were reverse engineered from
the code, after it was written.19

The developers lacked the required skills and experience. "Several ... were
not capable of producing professional code."20 The level of coding and unit
testing was dreadful. There was a "throw it over the wall for the testers"
approach to development, according to a Fujitsu internal audit manager.21
Inevitably, the quality of the system delivered to the testers was dreadful,
and the Post Office testers were not given the chance to do a good job.
The quality of development was so poor that vast numbers of bugs were
uncovered in testing the first version of Horizon, in particular the crucial
Electronic Point of Sale Service (EPOSS). The management response was
to draw up a fix plan to catch up with the backlog of bugs.22 Astonishingly,
this plan assumed that no further bugs would be found by the ongoing
EPOSS testing, which merely increased pressure on testers and developers.
In his written evidence to the court in the Horizon Issues trial, Richard Roll,
a Fujitsu contract developer, wrote:23
There were budget pressures and redundancies which impacted
system development and testing. The test team felt they were under
enormous pressure to complete the testing within certain timescales,
which negatively affected the test regime.
Justice Fraser dismissed this, saying, "I do not attach much weight to this as
pressure in the workplace is such a subjective matter." However, testers
who are familiar with traditional testing in large corporations can identify
with Roll's observation. The judge's lack of interest in this explanation for
poor quality and ineffective testing should give software suppliers pause for
thought. The legal system will look at what you did and what the results
were. A judge is unlikely to be interested in the managerial and commercial
pressures that lie behind problems with a system.
Dysfunction #3: Management Systematically
Discouraged Collaborative Critical Thinking That Could
Have Saved the Project
Fujitsu interpreted the Horizon contract to mean that they would provide a
service to the Post Office rather than hand over a system. They argued that

the details of the system were not the Post Office's concern—only the
output mattered. The system would be a black box. The requirements were
not specified in detail and focused on what was desired, rather than what
must not happen. Fujitsu, therefore, focused on the happy path. The
developers were reluctant to think about edge or corner cases and would not
divulge to testers how or whether they were dealt with until defects were
raised. Post Office testers had to force developers to think about
requirements by working backwards from defects.24 Any form of early
testing was impossible. Fujitsu's approach was deliberately secretive instead
of collaborative, and, in the words of Jeremy Folkes, who was involved in
the procurement process, "seemed to me a case of 'trust me, I'm a doctor.'"
Fujitsu developers did not understand basic accounting principles. Testing
showed that accounting transactions were being lost. The Fujitsu developers
did not understand the implications and significance of this.25
Non-functional requirements (NFRs) received little attention from Fujitsu.
The conventional examples of NFRs are security, availability, reliability,
performance, maintainability, scalability, and usability. Interestingly,
processing integrity was considered a non-functional requirement for
Horizon. The Post Office testers were particularly agitated over their
inability to inspect how Horizon would handle that.26 Processing integrity is
perhaps the most significant aspect of a system that IT auditors would insist
on examining in a system audit.
The Post Office Acceptance Team feared that the system would go live
before they had the chance to provide assurance that acceptance criteria
were met. Yet that is what happened.27 There was a lack of interest in
acceptance testing at a high level in the Post Office and the UK
government. This was confirmed in evidence to the Post Office Inquiry.
Stephen Byers, a government minister responsible for the Post Office in the
late 1990s, explained that the government had been unhappy with the
quality of recent systems developed by outside suppliers. It believed that
acceptance should be on the basis of a live trial rather than a "laboratory
test."28
Instead of allowing the testers to explore and assess how the system might
perform, the Post Office chose to rely on "monitoring of live operation in a
relatively small set of offices for a small number of weeks," as a frustrated

Post Office tester explained to the Inquiry.29 This inevitably entailed
constant repetition of happy paths. Edge and corner cases, as well as rare
but inevitable failure scenarios, were encountered only as a matter of
chance.
The evidence that has emerged about testing at Fujitsu leads to the
conclusion that it was the worst sort of traditional environment with
aggressive and incompetent management. The emphasis was on looking for
confirmation that high-level requirements had been met—and not on
informing the stakeholders about how the system performed.
In summary, Fujitsu was incapable of performing meaningful testing and
had no wish to do it anyway. The Post Office did not understand the
purpose of testing and did not give their testers the chance to do a good job.
Senior management "managed" risk by downgrading risk assessments made
by their juniors, including testers. Good testers want to shine a light on the
system to see what is really there. In contrast, Fujitsu and the Post Office
thought it expedient to remain in the dark. Tragically, the human cost of
their negligence has been huge.
How Bad Was Horizon?
In the second court case, Horizon Issues, the judge looked at the way that
Horizon worked. Justice Fraser considered 15 issues, and these not only
give a clear picture of his concerns but also show how these concerns match
the perspective that testers and auditors should have.
The first set of issues concerned bugs in Horizon and, specifically, whether
the system maintained the accuracy and integrity of data.
Justice Fraser concluded that the frequency and severity of bugs had
undermined the reliability of Horizon and that it was possible that bugs had
caused the discrepancies, which were treated as losses for which the SPMs
were prosecuted. The judge emphatically rejected the argument of the Post
Office that the definition of bugs should be restricted to coding errors. He
insisted that he would apply a wider definition:30

Software bug means something within a system that causes it to
cause an incorrect or unexpected result...
Bugs, errors or defects is not a phrase restricted solely to something
contained in the source code, or any code. It includes, for example,
data errors, data packet errors, data corruption, duplication of
entries, errors in reference data and/or the operation of the system,
as well as a very wide type of different problems or defects within
the system.
The judge ruled that any system feature that confused users and caused
them to make mistakes was a bug. There was no shortage of these. The Post
Office's attitude was that such mistakes were straightforward user errors.
The Post Office's definition of a bug would be unacceptable to followers of
RST.
Justice Fraser wanted to know whether Horizon warned the SPMs about the
risks of Horizon. He ruled that the Post Office did not tell users about
significant known bugs. He also decided that the system did not work in
ways that were intuitive to users; it even acted in ways the user did not
want, and could do so without warning them.
In certain circumstances, if a user entered details for a transaction but did
not commit it, the system would automatically complete the transaction
with no further user intervention. This might result in a loss to the branch
for which the SPM would be liable. A sale could be recorded on the central
system when no sale had been completed in the branch. The Post Office
insisted in court that this was not a bug but the system "working as
designed." It was a clear example of the Post Office's user error bias.
After examining many bugs, the judge dismissed the Post Office's assertion
that Horizon had always been robust, and he did it in brutal style. When
SPMs were being prosecuted in large numbers, the system "was not
remotely robust."31 The Post Office's insistence that Horizon was a reliable
system "amounts to the 21st-century equivalent of maintaining that the
earth is flat."32
The second set of issues decided by Justice Fraser concerned the controls
within the system and the measures for preventing and fixing bugs. The

judge ruled that important system controls were inadequate or missing. As a
result, the entry, transfer, and processing of data were prone to error. The
central system often held data that was inconsistent with the branch data.
Transmission and synchronization problems resulted in the central system
missing transactions that had taken place and were recorded in a branch.
Horizon, particularly in the early years, was plagued by connectivity
problems. The links between branches and the central system failed
frequently, with unpredictable results.
Steven Murdoch, Professor of Security Engineering at University College
London, looked at the bugs disclosed in court that were caused by Horizon's
inability to handle connection problems.33
Considering the large number of transactions that Horizon
performs (millions per day), inevitable hardware and
communication failures, and the complex interactions between
systems, it should have been obvious that errors would be a
common occurrence.
Professor Murdoch argued that these errors showed Horizon lacked the
properties and controls that are expected of transaction processing systems.
Horizon could not meet the criteria of atomicity, consistency, isolation, and
durability—the "ACID" criteria.
Atomicity means that all changes relating to a transaction must be
completed, or none must be performed. Consistency means that the state
and value of data are consistent at the beginning and end of the transaction.
There must be no changes except those that were intended. Isolation means
that while data is being processed in a transaction, it is not available to any
other transaction. For example, if money is being moved from one account
to another, then other transactions can see only the before state and the
completed state. Durability means that once a transaction has been
completed successfully, it cannot be undone, even if the system crashes.
The remaining issues examined by Justice Fraser concerned the ability of
the SPMs to manage their branches using Horizon. Did the system provide
them with the ability to see and understand what was happening? Did it

allow them to see what changes or corrections were applied to their
accounts by the Post Office or Fujitsu? This last point was crucial.
The judge ruled that the system did not allow the SPMs to manage their
branches effectively. Horizon did not provide its key branch users with
information that allowed them to investigate and resolve errors. The system
was designed around the needs of central, corporate management and
ignored the needs of branch users.
In particular, the ruling established that Fujitsu staff could change branch
data without the SPMs being told and without any logging or effective
controls to show which individuals had taken what actions. The system
would mistakenly show that central changes had been made by branch
users.
It was impossible for SPMs to establish the reason for a discrepancy.
Horizon did not retain and provide them with sufficient relevant
information to investigate a discrepancy or to challenge an accusation of
fraud. The SPMs were entirely dependent on the Post Office to explain
discrepancies. However, their contracts meant that they were liable for any
discrepancies unless they could prove that they were not responsible, which
hardly gave the Post Office an incentive to investigate or help the SMPs.
Justice Fraser's ruling that Fujitsu staff could, and did, change data without
the SPMs being aware undermined the position of the Post Office when it
had prosecuted the SPMs and claimed that only a branch user could have
changed data. The Post Office had always insisted that Fujitsu could not
change data.
The Post Office admitted that its own staff amended data to correct errors
manually. The Post Office claimed that its own corrections served as a
Horizon control to ensure that the system was robust. There were over
100,000 such transaction corrections every year since 2006. That is about
400 manual corrections for errors every working day.
The judge did not consider this process to be a Horizon control, as the Post
Office argued, but a means of correcting system errors that was not always
applied or effective. SPMs were not consulted when a correction was made.
There was no mechanism for them to object other than by phoning the
helpline to complain. That was always ineffective.

When the SPMs were accused of fraud, their contracts required them to
show why they were innocent and how the system had made a mistake. The
SPMs argued that this was an impossible burden, and Justice Fraser's court
agreed. This burden combined cruelly with the presumption under English
law that computer evidence is reliable unless the other party can show why
it is unreliable. That is possible only if the provider of the evidence reveals
problems that undermine their case.34
The result was that, in Horizon fraud cases, the presumption of innocence
was reversed. The SPMs were considered to be guilty unless they could
prove their innocence. For historical reasons, the Post Office was allowed to
act as the prosecutor in these fraud cases. The SPMs could therefore prove
their innocence only if the prosecutor chose to reveal the evidence that
would exonerate them. That never happened.
The Significance for Testers of the Horizon
Issues
The relentless focus of the court in the Horizon Issues trial was on whether
the system worked for the users. Remember principle five of the seven
basic principles of the Context-Driven School: "The product is a solution. If
the problem isn't solved, the product doesn't work."
Horizon did not help the SPMs to manage their branches. It hindered them,
stopping them from seeing what was happening in their branches and the
branch accounts. Horizon prevented users from understanding how bugs in
the system miscalculated cash and stock balances at their branches. The
system did not solve the problems of this key group of users, and so the
product did not work for them.
How should a tester deal with a project like this? Well, not by adding more
test cases or improving some test technique. The root of the dysfunction
with the Horizon project is management that conspired to deny reality.
Their wishful thinking ran the project right off a cliff. As a tester, you have
no power to fix that. The best you can do is be persistent and vocal about
the problems you see with the product and the project. Put that in writing.

You can't fix it, but neither be complicit. In this respect, good testing is
similar to system auditing, and it is worth considering how IT auditors
work. I have worked as an auditor, so this is a world I know.
Reading the court judgment and, in particular, the treatment of the 15
issues, I was struck by the way that the judge was taking an approach that
was similar to that of an IT auditor. In the 313 pages of the judgment and
114 pages of the technical appendix, there is no mention of the traditional
documents produced in software development, no requirements
specification, no system specification. The words "requirements" and
"specifications" appear only as general terms, rather than specific
documents or in quotes from submissions by the Post Office.
Justice Fraser had little interest in what the original systems analysts and
designers thought the system should do. His concern was how the system
behaved in the hands of its users and whether the system met the needs of
the people who mattered, in this case, principally the SPMs. A specific
concern of the judge was whether the SPMs could use the system in the
way they needed if there was a discrepancy or disputed transaction.
Clearly, the Horizon system was hopelessly deficient in this respect, but the
consistent attitude of the Post Office and Fujitsu was that the system was
working as designed and that it was fulfilling the purposes of the Post
Office. Justice Fraser thought differently, as would any competent IT
auditor who scrutinized the system.
The Horizon issues examined by the judge largely concerned accuracy,
errors, reconciliations, controls, and risk. However, from an audit point of
view, the most striking word used by Justice Fraser was "integrity."
"Processing integrity" sounds like a vague platitude that can be safely
inserted into a corporate document without any need to worry about its
meaning or practical implications.
For auditors, however, processing integrity is no platitude; it is a
fundamental property that sound systems must have. Far from being a
vague piece of wishful thinking, the phrase is defined precisely, with clear
implications for auditors, and indeed for designers and testers. The
American Institute of Certified Public Accountants defines five criteria for
the category of processing integrity. These criteria are accepted by IT
auditors throughout the world.35

Processing integrity refers to the completeness, validity, accuracy,
timeliness, and authorization of system processing. Processing
integrity addresses whether systems achieve the aim or purpose for
which they exist and whether they perform their intended functions
in an unimpaired manner, free from error, delay, omission, and
unauthorized or inadvertent manipulation.
IT auditors are obsessed with processing integrity. Our mantra is that
systems required controls to ensure that all processing was complete, valid,
accurate, authorized, and timely.
Authorization is a particularly interesting criterion in the context of
Horizon. This was one of our most pressing concerns when I was an IT
auditor. Every nontrivial transaction, and certainly every financial
transaction, had to be logged so that there was evidence of the user who was
accountable. It had to be possible, years after the event, for investigators to
examine a transaction and see the sequence of actions, as well as the people
who had worked on it and approved it. This evidence was a vital building
block for our fraud investigations. Every application had to be carefully
integrated with the corporation's access controls systems so that it was
possible for managers to ensure that the only people who could use
sensitive financial transactions were those whose jobs required it. Each user
had authorization levels, junior staff being restricted to lower limits, and
only the more experienced and senior staff having high limits.
A glaring failure of Horizon, or rather the Post Office's and Fujitsu's
management of both the system and IT environment, was that it was both
possible and routine for financial data to be changed without any evidence
being retained of the person responsible. Would testers usually be interested
in such a failure? Traditionally, they might not have been. However, after
my experience in IT audit, I would have been concerned as a test manager if
I realized that financial errors could be corrected only by database
administrators editing raw data. I would have been concerned if these
corrections were not logged and no evidence was created or retained about
what was done. Such a failure could pose a serious risk to the corporation,
and I would have ensured that management knew about it.

Failure to produce and retain relevant evidence will become more serious
and more embarrassing in the future. Courts will increasingly expect
evidence from computer systems to be clear and demonstrably reliable.
Owners of systems will be expected to explain how and why the system
produced the outcomes that led to litigation, as well as who took the crucial
actions. Appropriate authorization is a vital requirement, but one that might
well not have been made explicit.
The criteria for processing integrity are all constrained by each other, and
informed by the context, i.e., sufficiently accurate and appropriately
authorized for business objectives, given the need to provide the
information at the right time. We auditors had to understand the current
context. Context was everything. Note the phrase in the AICPA definition of
processing integrity: "Processing integrity addresses whether systems
achieve the aim or purpose for which they exist."
Why RST Appeals to Me
One of the reasons I was first attracted to RST was its ethos. It recognizes
the reality of what we are working with. It does not pretend that formal
documents, like requirements specifications, are reality. Testers have to
understand that these documents are a flawed, but necessary, attempt to
capture what developers have to do.
It is important that testers go beyond these formal documents and think
about what the system should do, what it will do, and all the elements and
variables that will affect the system. The system that testers must think
about if they are to test effectively is a larger and more complex entity than
the system represented by the specifications.
RST is based on general systems thinking, which argues against a
reductionist, mechanistic view of systems. In order to understand the
behavior of systems, we have to be aware of all the factors that affect them,
external and internal. We also have to be careful about how we look at the
components of a system. The system is more than the sum of the parts. The
behavior of the system will change depending on how the components
affect one another, and it cannot be predicted by looking at the isolated
behavior of these components. A complex system involving humans is a

very different creature from a machine like a bicycle. This confusing
complexity is impossible to capture in a requirements specification. If we
restrict our focus to such formal documentation, it guarantees that we will
tackle the testing wearing blinders.36
Applying RST means thinking about the problem from a variety of
perspectives, and certainly a different perspective from the developers.
Good IT auditors must understand a bigger picture than developers usually
do, and they can help open-minded testers understand the different, wider
factors affecting a system. The auditors should understand how a new, or
changed, system will fit into the bigger picture and what the implications
might be if the system is flawed.
Auditors can teach testers about the risks of bugs causing problems that will
cascade through other systems. Testers can provide auditors with advice
about how and whether these risks might transpire in practice.
RST appeals to me because I was an auditor—not despite my audit
experience. By nature and training, auditors want to see evidence that other
people know what they are doing, they think seriously about what they are
trying to do, and they have established ways of working.
Auditors would expect testers to have a clear vision, or strategy, for how
testing should be performed in their organization. For each system or
product under test, auditors expect testers to have a clear, but not
necessarily detailed, plan.
As output from the testing, auditors expect relevant, contemporary evidence
that justifies and leads inescapably to the conclusions, lessons, and insights
that the testers derived and reported from their testing. If followers of RST
do their job well, they will tell stories37 that good auditors find compelling.
What did they learn about the status of the product? How did they test the
product? How good was the testing?
Devotees of traditional and standards-driven testing probably still assume
that RST is not serious, that it is informal or for small pieces of work, or not
rigorous. On the contrary, it is extremely serious; the people who developed
it are among the most serious and thoughtful people I have ever come
across.

It is the people who go through the motions, who don't mean what they say,
and who confuse the testing with the process, who are not serious. Too
many people in testing have been far too fond, dependent even, on
templates. I once heard a member of the ISO working group that developed
ISO 29119 speaking about software testing templates at a conference:
"We'll sell a lot of these. People like to know what the templates are."
If they are used carefully, templates can help you think about the problem,
but sadly, the reason they are so popular is that people use them as a
substitute for thought. Heuristics, as offered by RST, are far more useful
than document templates. They help testers think without tempting them to
take easy shortcuts and turn out documents that are superficially impressive
but lack substance and are ultimately destined to become shelfware.
RST is the product of rigorous thinking, but it does not provide a rigid
process. Testers have to use their brains; they can't simply follow the
instructions blindly.
The tragic scandal at the Post Office might have been avoided if the testers
working for the Post Office and Fujitsu had taken a more thoughtful
approach to their work and considered the wider implications of how the
system would be used. They also should have thought about the different
user groups with very different needs from the system.
Testing Horizon would have been a challenge. Perhaps the testing would
have been better if different specialties had worked together more closely or
if good testers had worked more closely with good auditors. I have long
thought that testers and auditors should seek each other out and learn from
each other. Each can help the other learn useful techniques, and each can
provide the other with valuable information about the problems that they
face.
The Horizon Issues that were examined in the court case are packed with
vital implicit requirements—requirements that are so obvious they can be
taken for granted by users and consequently ignored by developers.
Providing the SPMs with the data and functions they needed in order to
understand the discrepancies for which they were responsible should have
been an essential requirement. This should have been tested for, regardless
of what was in the requirements specifications.

System audits can be performed in two different ways. When I was an IT
auditor, we explored and examined the technical details of how the systems
under audit were built. Many internal audit departments lack the technical
experience and possibly the time to take such a detailed approach. They
might focus on whether there are processes in place to identify and
implement the necessary application controls, without testing to establish
whether they have been implemented and are operating effectively.38
Such non-technical auditors should welcome the practical contribution that
good testers can make. The auditors, in turn, should be able to help testers
identify the implicit requirements that are so easily missed by users and
developers. Support from auditors should make it easier to justify testing
for these crucial unstated requirements. This might not seem efficient to
traditional testers, but it is more effective than ignoring risks that could
wreck the company, as happened with the Post Office.
Collaboration between testers and auditors would require both specialties to
be open-minded about what they can learn from the other and how they can
work together. Testers can only ask questions. Auditors can demand
credible answers to provide reassurance about high-risk areas, and good
auditors will pay particularly close attention to implicit requirements.
Testers should avoid being too efficient in their testing. It is easier to appear
efficient if we restrict the range of inputs we use to stimulate our thinking
and generate our tests. It might be tempting to restrict our testing to the
formal requirements, and I have seen one major financial institution that
told its testers to do exactly that. However, we must remember that while it
might not be efficient to test for implicit requirements, it is impossible to
test effectively if we ignore them. Good auditors can provide effective
support for testers who want to test effectively.
The implicit requirements are exactly those that a court is likely to look at
most carefully if the system or product gives rise to litigation. Testers
should study the lessons from the Post Office Horizon scandal and
remember that the judge never considered the formal requirements. His
focus was on the purposes of the system to the people who used and were
affected by it. He analyzed the implicit requirements that he identified, the
events and outcomes that the system must ensure, and must prevent. Mr.

Justice Fraser approached the Horizon system as an auditor would, and
testers can learn from that.
Testers and IT auditors each have a different focus. Auditors are primarily
concerned with analyzing and explaining risks. Auditing is concerned with
a bigger picture. If auditors identify damaging second- or third-order effects
of problems, they can take a stronger stance than testers.
Although auditors take a less humanistic approach than testers, they have to
be concerned about how systems might affect people. Humans can be hurt
by systems, and they can sue. Even if problems do not lead to litigation,
they can damage a corporation's image—and the bottom line.
Testers, and the whole software development community, should certainly
learn from the Post Office Horizon case. They should pay particularly close
attention if they are working on sensitive financial or safety-critical
systems. How comfortable will they feel if their work is examined in court,
if the system that they tested is subjected to forensic scrutiny?
Testers should be willing to look to good IT auditors for guidance on
relevant risks and what sort of evidence they expect to see. The complexity
of modern systems poses daunting problems. It is impossible to understand
exactly how they will behave. If the people who depend on these systems
are to be given the best information possible about them, it will require
experts from different disciplines to work together more closely than they
have in the past.
Good testing is expensive, but the Post Office is facing compensation
claims that will rise to over a billion pounds. The Post Office has admitted
that it cannot afford to pay these claims and will need government
assistance. Perhaps it could have afforded better testing. Is good testing
really so expensive when the quality of the software directly affects the
quality of people's lives? Good testing might not be cheap, but inadequate
testing can be horribly costly. If testers follow RST, a rigorous and effective
approach, they will certainly do a far better job than the hapless testers at
Fujitsu and the Post Office, and better than those who choose to rely on
deeply flawed standards like ISO 29119.

Notes
1   UK Parliamentary Business. "Memorandum submitted by ICL
Pathway," 1999. Available at:
https://publications.parliament.uk/pa/cm199899/cmselect/cmtrd
ind/530/9061409.htm
2   Wallis, N. "The Final Reckoning," Post Office Trial site, 2021. Available
at: https://www.postofficetrial.com/2021/04/the-final-
reckoning.html
3   Brooks, R. & Wallis, N., "Justice Lost in the Post," Private Eye, 2020.
Available at: https://www.private-eye.co.uk/special-
reports/justice-lost-in-the-post
4   The full judgment for the first trial is at Bates v Post Office Ltd (No 3:
Common Issues) [2019] EWHC 606 (QB). Available at:
http://www.bailii.org/ew/cases/EWHC/QB/2019/606.html
5   The full judgment for the second trial is at Bates v Post Office Ltd (No 6:
Horizon Issues) Rev 1 [2019] EWHC 3408 (QB). Available at:
http://www.bailii.org/ew/cases/EWHC/QB/2019/3408.html
6   Post Office Corporate website. "Overturned Convictions and
Compensation: Information on Progress." Available at:
https://corporate.postoffice.co.uk/en/horizon-scandal-
pages/overturned-convictions-and-compensation-information-on-
progress
7   Wallis, N. "Court of Appeal quashed a further 39 convictions." Post
Office Trial site, 2021. Available at:
https://www.postofficetrial.com/2021/04/court-of-appeal-
quashed-further-39.html
8   Hollinrake, K., Parliamentary Under-Secretary of State for Business and
Trade, UK Parliament, 10 January 2024. Available at:
https://hansard.parliament.uk/commons/2024-01-
10/debates/3DDD9684-C08D-4E5E-AF73-
CF85839412AB/PostOfficeHorizonScandal

9   Wallis, N. "The £1bn disaster," Post Office Scandal site, 2022. Available
at: https://www.postofficescandal.uk/post/the-1bn-disaster
10 Scully, P., Parliamentary Under-Secretary (Department for Business,
Energy and Industrial Strategy). Post Office Update Statement, 18
March 2021. Available at: https://questions-
statements.parliament.uk/written-statements/detail/2021-03-
18/hcws853
11 Kaner, C., Bach, J., & Pettichord, B. (2002). Lessons Learned in
Software Testing: A Context-Driven Approach. Wiley.
12 Definition of humanistic in the Free Dictionary. Available at:
https://www.thefreedictionary.com/humanistic
13 Weinberg, G. M. "Agile and the Definition of Quality." Available at:
http://secretsofconsulting.blogspot.com/2012/09/agile-and-
definition-of-quality.html
14 Bolton, M. "Premises of Rapid Software Testing, Part 1." Available at:
https://developsense.com/blog/2012/09/premises-of-rapid-
software-testing-part-1
15 Wallis, N. "Horizon Trial: Day 6 Transcript," Post Office Trial site,
2019. Available at:
https://www.postofficetrial.com/2019/03/horizon-trial-day-6-
transcript.html
16 Post Office Inquiry website. Available at:
https://www.postofficehorizoninquiry.org.uk
17 Bates v Post Office Ltd (No 6: Horizon Issues), "Subpostmasters v Post
Office Limited, Expert Report of Dr. Robert Worden." Available at: First
report: https://www.scribd.com/document/413049695/Bates-v-
Post-Office-Horizon-trial-Post-Office-independent-IT-expert-
first-reportSecond report:
https://www.scribd.com/document/413049699/Bates-v-Post-
Office-Horizon-trial-Post-Office-independent-IT-expert-2nd-
report

18 Post Office Horizon, IT Inquiry, "WITN00620100 David McDonnell -
Witness Statement," 3rd November 2022, and oral evidence by
McDonnell, 16th November 2022. Available at:
https://www.postofficehorizoninquiry.org.uk/evidence/witn0062
0100-david-mcdonnell-witness-statement
19 Post Office Horizon, IT Inquiry, "WITN05970100 Jeremy Folkes -
Witness Statement," 7th September 2022, and oral evidence by Folkes,
2nd & 3rd November 2022. Available at:
https://www.postofficehorizoninquiry.org.uk/evidence/witn0597
0100-jeremy-folkes-witness-statement
20 See McDonnell's evidence.
21 Post Office Horizon, IT Inquiry, "Phase 2 - 16 November 2022," oral
evidence by Jan Holmes. Available at:
https://www.postofficehorizoninquiry.org.uk/hearings/phase-2-
16-november-2022
22 Post Office Horizon, IT Inquiry, FUJ00080690 - Report on the EPOSS
PinICL Task Force. Available at:
https://www.postofficehorizoninquiry.org.uk/evidence/fuj00080
690-report-eposs-pinicl-task-force
23 Bates v Post Office Ltd (No 6: Horizon Issues) judgment, [paragraph
181]. Available at:
http://www.bailii.org/ew/cases/EWHC/QB/2019/3408.html
24 See Folkes' evidence.
25 See Folkes' witness statement.
26 See Folkes' oral evidence.
27 See Folkes' evidence.
28 Post Office Horizon, IT Inquiry, "WITN03580100 Stephen Byers -
Witness Statement," 26th September 2022, and oral evidence by Byers,
24th November 2022. Available at:

https://www.postofficehorizoninquiry.org.uk/evidence/witn0358
0100-stephen-byers-witness-statement
29 See Folkes' evidence.
30 Bates v Post Office Ltd (No 6: Horizon Issues) judgment, [paragraph
26].
31 Bates v Post Office Ltd (No 6: Horizon Issues) judgment, [paragraph
975]. http://www.bailii.org/ew/cases/EWHC/QB/2019/3408.html
32 Bates v Post Office Ltd (No 6: Horizon Issues) judgment, [paragraph
929].
33 "What went wrong with Horizon: learning from the Post Office Trial,"
Bentham's Gaze 9, Information Security Research & Education,
University College London, 2021. Available at:
https://www.benthamsgaze.org/2021/07/15/what-went-wrong-with-
horizon-learning-from-the-post-office-trial
34 For a detailed discussion of the presumption, see this article by the
author. "'The Post Office Horizon IT scandal and the presumption of the
dependability of computer evidence'. Digital Evidence and Electronic
Signature Law Review, 17, 2020. Available at:
https://journals.sas.ac.uk/deeslr/article/view/5226
35 AICPA System and Organization Controls. Available at:
https://en.wikipedia.org/wiki/System_and_Organization_Control
s
36 Blinders are patches placed to the side of horses' eyes to stop them being
distracted by things the rider considers irrelevant. The British word is
blinkers.
37 Bolton, M. (2012). "Things Could Get Worse: Ideas About Regression
Testing." Available at:
https://www.developsense.com/presentations/2012-09-KWSQA-
Regression.pdf
38 AICPA calls non-technical system audits SOC 2 type 1 audits. The
detailed, technical audits are SOC 2 type 2.

OceanofPDF.com

CHAPTER 17
Sociology and Software Testing
—By Harry Collins and Michael Bolton
Michael Bolton: So, let's start. Who are you?
Harry Collins: I'm Harry Collins. I am a professor at Cardiff University.
I've been writing for a long time about the nature of knowledge. I am a
sociologist; I suppose I'm a sociologist of knowledge. Most of my field
work has been about the sciences. I've spent 45 years kind of embedded—as
journalists like to say—with gravitational wave physicists, as they were
trying to do this impossible job of detecting gravitational waves. (But I
wasn't really that embedded, except for a period of 10 or 15 years). Much to
my astonishment, they did actually detect gravitational waves in my
lifetime! I've written four books about that, and I've written lots of books
and papers about other sciences too.
But at the same time, because I'd studied knowledge, I noticed, way back in
the 80s, that people coming from the expert systems field were saying very
silly things about the nature of knowledge. I thought, "I know more about
knowledge than that," and so I wrote some papers and a book on how they
were misunderstanding the nature of the knowledge, and they were fairly
successful. I have carried on, writing three books about artificial
intelligence, along with various papers on computers and knowledge. And I
am pleased to see that you guys quite like my work because it fits with the
sort of things that you're trying to say as well.
Michael Bolton: It really does. I was looking at a paper1 you showed me
the other day, and I was fascinated by the degree to which what you're
saying is right in line with what testing is and does. In the paper, you say
that criticism is an essential feature of science. That immediately struck my
attention because certainly that's how I think about testing as well.
You've tested software yourself, have you not?
Harry Collins: "Tested" is rather a grand word. We've built some software,
and we've had some software built, and found it was no good. Is that what

you mean by testing it?
Michael Bolton: Well, finding that it was no good would be a test result, so
yes.
Harry Collins: The fact is that computers are mechanical objects, a lot like
cars or sets of cogs or something like that. In fact, the first one was literally
a set of cogs—the Babbage machine.
It's probably quite useful to continue to think of computers these days as
essentially Babbage machines instantiated in silicon chips. That's one of the
things they are, a machine where the cogs have to work together. If you
start missing teeth from the cog or something like that, they'll go wrong. So,
one way of thinking about testing computers is to find out whether all the
teeth are there and whether all the cogs are working right, and so on and so
forth. But that's the easy bit.
Nevertheless, I'm not sure that it could be fully automated. Really fully
testing even the mechanical workings of a complicated enough computer
could be too difficult to automate. It might take till the end of the universe
to complete all the possible iterations you need to go through, like playing
all possible chess games to completion.
But anyway, that's still the easy bit. The hard bit is something that's going
be much harder until we've achieved full artificial intelligence—and I don't
see any route to achieving full artificial intelligence in the foreseeable
future. I am not totally cynical about it. I mean, maybe someone will
achieve full artificial intelligence. Maybe they will work out how to do it,
but at the moment, they don't know how to do it. It's easy to see that
because if you're going to achieve full artificial intelligence, you're going to
have to mimic what human beings do.
One of the most striking things that human beings do—and this is perfectly
obvious if you're a sociologist or sociologist of knowledge—is that they
learn most of what they learn by being brought up in societies. You might
want to say: "Well, that's all very well. But that does not apply to things like
scientific knowledge or mathematical knowledge."
But it actually does! There are all sorts of simple demonstrations of this. I'll
give you a couple.

Let's start with the one that is in my 1985 book, republished in 1992, called
Changing Order.2 There I play a little game called Awkward Student. That's
about how we learn to continue a series, which is a sort of quasi-
mathematical operation. Suppose I want to teach you what the continuation
of the series 2, 4, 6, 8 is. It sounds like I can teach you that and give you a
rule. But the Awkward Student, whom I invent for the purpose of that book,
is someone who's going to be hard to teach.
Teacher: "Well . . . 2, 4, 6, 8. Continue the series: add 2, 
that's how you do it."
Awkward Student: "Ah, you mean '2, 4, 6, 8, 82,' because 
that's adding a 2." 
Teacher: "No, no. I don't mean that, you fool! I mean, put 
the next number in the series of the natural numbers but 
missing the intermediate one."
Awkward Student: "2, 4, 6, 8. The next number in the series 
of the natural numbers but missing the intermediate one. Is 
that right? That's what you told me to put there."
That's how Awkward Student goes. And you can play this—it's quite a
strain, but you can play it—and you can find that there's no way that you,
by giving me the instructions, can force me to complete the series, to put
the number 10 in place.
But in the classroom, as one of the things you learn when you're in school,
you'll learn to complete the series. You don't learn a rule for completing that
series; you learn what we mean by completing the series.
So, when you say, "Continue the series 2, 4, 6, 8," I know the answer is 10,
but I don't know it from following a set of rules that I've been taught. I
know it from having just hung around a load of people who regularly
continue series, and that's how they continue series, and that's what they
mean by that. Most of what we know is like that. Nearly all of what we
know is like that.
Somehow, if computers are going to mimic human intelligence and not
make the kinds of mistakes the Awkward Student makes, they need to be
embedded in society in the same way we all are, including scientists,
mathematicians, and series-continuers, so that they know what we mean
and act in the same way as we do.

Until we reach that time with AI, the second job of testers is to decide
whether a machine is good enough to allow the humans using it to make up
for its mistakes and errors.
Michael Bolton: You're helping me to remember something connected to
that. Our friend James Bach, in the 1990s, began talking about something
that he called "good-enough testing."3 And he got incredible heat from the
testing community. They were outraged at the idea. "'Good enough' is not
good enough!" they said. There was controversy about the very idea that
testing might have something to do with enoughness rather than
correctness.
Harry Collins: I'll give you another example that will fill this out just
perfectly.
This goes right back to my 1990 book called Artificial Experts.4 I've got a
pocket calculator, and I think that this thing is terrific at arithmetic. It can
do arithmetic better than me, faster than me, and better than me ... but
actually, it can't. It's only good enough.
And the example I give is that I decided to use my calculator to convert my
height in inches to my height in centimeters. So I type into my calculator—
I've got to remember the numbers—I'm 69 inches high, and I type into it 69
times 2.54—which is, I think, the number of centimeters per inch—and it
comes out with a result (of 175.26) to two decimal places. But my height is
not to two decimal places! Because my height is only to two decimal places
between breaths, or according to how flat my hair is. My height in
centimeters should be measured in a number of whole centimeters with no
decimal points. So actually, the calculator hasn't understood the social
convention of height in human society. It's not as good an arithmetician as
me, because it does not know how to convert human heights from inches to
centimeters. But it's not a problem; we automatically repair without
thinking about it.5 It's good enough because we just say, "Oh, it means 175,
not 175.26." We just work with it, and we don't even think about it. And we
normally say, "It's better at arithmetic than me" without noticing that it isn't.
The point is that when you test it to see if it works, you are testing it to see
if it's good enough.

Michael Bolton: In our work, in Rapid Software Testing, we talk about
what "it works" means, which is also something that people repair
themselves all the time. As we put it in class, when someone says, "it
works," they mean "it appeared to fulfill some requirement to some
degree." Well: appeared, some requirement, some degree. There's a lot of
empty space in there, a lot of wiggle room.
Harry Collins: Yes. Well, it's not surprising because the same applies to
other human beings. When you get your software builders to do a job,
you're going to have to decide: has that builder done the job well enough?
The builder is likely not going to do it to absolute perfection. I'll just be a
little bit controversial about this, but the problem with certain software
designers I have encountered when I've wanted some software built is that
they didn't do it anything like good enough, yet they wouldn't admit it. They
just wanted their money.
Then there is the still worse problem of software that makes mistakes that
are so bad that no human would make them. That's when it's surely not
good enough. The much more difficult challenge for computer science is to
build devices that follow rules in the ways humans do, because we're not
even very good at understanding how humans learn all that stuff.
Michael Bolton: You created a distinction in your book, The Shape of
Actions,6 a distinction between mimeomorphic and polimorphic actions.
Can you talk about that a little bit?
Harry Collins: Sure. I think this is a very deep and profound distinction,
actually. It hasn't been taken up much, but I think if people are still reading
any of my work in 100 years, it will be the distinction between
mimeomorphic and polimorphic actions.
You know, computer scientists, testers, and so on really need to understand
this distinction if they are going to understand the relationship between
humans and machines. Because the thing about humans is that they do both
of these kinds of actions. Machines, however, can only be mimeomorphic.
A mimeomorphic action ... Let's see if I can explain this ... You need two
terms. You need the term intention, which means what you intend to do.
And you need the word behavior, which is the visible correlate of what you
intend to do.

Now, in the case of a mimeomorphic action, the intention can always be
executed by doing identical behaviors as described in space-time.
An example of this is a salute. When you're intending to salute, you want to
go like this: [Harry salutes with his hand, crisply and precisely, like an army
soldier]. And every time you salute, you want to do it the same way. The
book Catch-22 captures this very nicely with Colonel Cathcart, who wanted
to win the prize for getting his troops to march properly. So he fixed all
their hands to a plank so that when they marched, all their arms had to
swing uniformly because they were all fixed to the same plank. This is the
essence of a mimeomorphic action: the exact physical movements execute
the intention. We have certain activities in human life where we value
mimeomorphic actions extremely highly. Saluting is one, marching is
another. Golf is another, I think.
Michael Bolton: Shifting gears on a car with manual transmission.
Harry Collins: I'm trying to think of some sporting event where you win a
prize for mimeomorphicity of your action. Something like the compulsory
figures in figure skating would be a good example.
Polimorphic actions, on the other hand, are actions where the execution of
the intention has to be different from time to time if you'll still be executing
the same action. So, for example, a salute is a form of greeting. But if my
intention is to greet, rather than to salute ... Let's suppose I am greeting my
wife, and every time I meet her, I greet her in exactly the same way. I greet
her like this: "Hello darling ... hello darling ... hello darling ..." [Harry
speaks in monotone, with a blank face, like a robot]. If I never alter that
pattern of behavior, it will cease to become a greeting, and it will turn into a
joke or an insult.
Most of the actions human beings do are of that second type. The same
intention can be, and often has to be, executed with a different set of
behavioral coordinates—the description of the behavior.
Computers are very good at mimicking mimeomorphic actions, but in order
to mimic polimorphic actions, they have to understand the context and
understand how the context affects the way the intention has to be executed
in terms of behaviors. I think that is a very deep distinction, and it's only
humans who have that distinction in their lives. [Well, ChatGPT and other

large language models try to mimic it by relaxing the statistical conditions
for predictive text: they don't always choose the statistically most probable
continuation but something a little less probable than that, so if you ask
them the same thing twice, the second response will probably be different.]
Michael Bolton: I think that distinction is really key. These days, there
seems to be some kind of intention to economize by taking humans out of
phone calls when you are calling to ask for help or support.
It doesn't usually take me more than a moment or so in an online text chat
to detect that I am talking to a bot because it doesn't vary its greetings or
other behaviors. I've also noticed that when I call the tech support line and a
real human acts in a mimeomorphic way, I find it quite insulting. I feel like
the human isn't understanding or dealing with my social context. The
human is giving a response that they've been programmed to give.
And that ties back to testing, too! I don't know where the idea came from,
but a while ago, testing started to look like this: Give human testers a set of
instructions to follow. "Start here, do this, do that, do these 10 more steps,
and then observe this output. If the output is consistent with what it says on
this page, you put a little green check mark next to it, and if it's not, you put
a little red X."
That seems to me to be a silly idea. It's ponderous to write all that stuff out,
and it's certainly ponderous to follow it. Worse, it doesn't feel like there is
opportunity in there to recognize that somewhere between step six and step
seven, there might be something very odd going on. The testers get to the
correct answer at the end of the process, but it may be that the answer was
irrelevant because of the weird thing that happened a few steps before.
What I'm noticing these days is that, for many people, the process for
"automating a test" is to start by eliminating human intention from the test
process and then programming a machine to act like a human who has no
intentions. They get humans to act like robots and then get robots to act like
fake humans. And they call that testing! It's quite peculiar.
Harry Collins: What you describe is right. I mean, it's one of the most
unpleasant features of modern society. Franz Kafka talked about it ages ago:
any bureaucracy has a tendency to go in that direction, and society is
becoming more and more bureaucratic in that kind of way. It drives you

'round the bend. An instance of that which nearly gave me a stroke was
when I turned up at the airport with a ticket to travel somewhere. They told
me that my ticket was invalid because, instead of having my name—which
on my passport is Harold—it had "Harry" on it, which is what everybody
calls me. Therefore, the ticket was invalid. This is a person acting like a
machine because, obviously, any human being would know that I was the
same person and that that ticket referred to me.
And what the British comedy series Little Britain did was to invent the
phrase "Computer Says No," which is a human looking at a computer,
asking it a question, and the computer comes out with "Not valid," so they
just automatically pass that over without using any kind of context-sensitive
thinking whatsoever. It used to be that one of the beauties of living in
Britain was that most people were very sensible about context thinking.
Bureaucrats would always think about it a bit, give you the benefit of the
doubt, and act sensibly, but it's becoming less and less prevalent in this
country now; you get more and more people who just go through the
motions. This kind of thing is affecting society badly as well.
Incidentally, that example I gave of calculating my height is another good
example. If you want the calculator to act like a human, it has to be a
polimorphic action. The way the calculator is doing the arithmetic has to be
different when it's calculating height than when it's calculating grams of
gold.
[When I tried ChatGPT on the height calculation a few months back, it
simply provided the unapproximated answer like the calculator. But when I
tried it in January 2024, it came up with the following:
You
If I am 69 inches tall, how tall am I in centimetres?
ChatGPT

To convert your height from inches to centimeters, you can 
use the conversion factor:
1 inch = 2.54 centimeters
Now, let's calculate your height in centimeters:
Height in centimeters = 69 inches * 2.54 cm/inch
Height in centimeters = 175.26 cm
So, if you are 69 inches tall, you are approximately 175.26 
centimeters tall.
I was surprised by the "approximately." But messing around, it appears
now that giving numerical results to two decimal places preceded by
"approximately" is pretty standard:
You
What is the mass of an atom of gold in grams?
ChatGPT
The mass of an individual atom is extremely small, so 
scientists often use a unit called the atomic mass unit 
(amu) to express atomic masses. The atomic mass of gold is 
approximately 196.966569 amu.
To convert atomic mass units to grams, you can use the 
conversion factor:
1 amu = 1.66053906660 × 10^-24 grams
Now, let's calculate the mass of an atom of gold in grams:
Mass in grams = 196.966569 amu * (1.66053906660 × 10^-24 
grams/amu)
Mass in grams ≈ 3.27 × 10^-22 grams
So, the mass of an individual atom of gold is approximately 
3.27 × 10^-22 grams.
Of course, the way the technology is going, there is no reason why large
language models shouldn't eventually learn to approximate properly since
they have the potential to continue to "learn" from the Internet, and they
might pick up what is happening in this dialogue. But while scouring the
Internet is a bit like socialization, it is far from actual socialization. Actual
socialization is done in small groups, such as the family or the research
laboratory, where there is a limited amount of knowledge with a certain
consistency to it, rather than all the knowledge in the digital world. That is

why the builders of ChatGPT and the like have to effectively re-socialize
the machines to stop them from outputting racist, misogynist, and
dangerous material, including conspiracy theories and so on.
Michael Bolton: Much of what you're talking about pertains to tacit
knowledge, the tacit knowledge that humans have. I wonder if you can
speak about explicit knowledge and the three kinds of tacit knowledge that
you've identified in your book Tacit and Explicit Knowledge7—and how
they might relate to testing.
Harry Collins: The reason I became interested in tacit knowledge was
because, as I said right at the beginning of this talk, the way we learn most
of what we learn, we learn tacitly—just in the course of our upbringing,
without realizing that we're learning it.
Then, when I started to study the sociology of scientific knowledge, I
wanted to demonstrate that this applies within science too: that what we
come to take as scientifically valid depends on us tacitly accepting a whole
pile of assumptions or a whole pile of things that we learn without noticing
that we are learning them.
In fact, the very first paper I wrote was about scientists trying to learn to
build a certain kind of laser called Transversely Excited Atmospheric
Pressure Carbon Dioxide Laser, or TEA Laser for short.
What I discovered was that this is quite a complicated thing to build. There
were plenty of circuit diagrams around and papers explaining just how to
build it. You could follow the instructions in these papers with the circuit
diagrams and so forth, and you'd have this thing that looked like it would
work on your bench, but it wouldn't work. But if you had been to somebody
else's laboratory where they had a working one, and you came back and
built it, it would work.
And somehow or other, the knowledge of how to build it drifted tacitly
across from hanging around people who had one that they'd built. One can
give a certain kind of clunky explanation as to why this was. In the case of
TEA Laser,8 it was a very high-voltage device, and it had to have a great big
capacitor that fed a cable to the top electrode. If that cable was too long, the
laser wouldn't work because its inductance would be too high. If you
wanted it to work, you had to put the capacitor—it was about the size of a

car battery—upside down in a big frame just above the top electrode. Now,
if you had gone to somebody else's laboratory and seen what they've done,
that's how you would build it. You wouldn't know why, but you'd copy it,
and the thing would have a chance of working. If you did it from a circuit
diagram, given that nobody really knew the importance of the inductance of
that top lead, you would just put the capacitor on the bench with a slightly
longer lead to the top electrode. Then, the inductance would be too high,
and it wouldn't work.
But nobody, in the early days, knew why this was happening. You either got
it by seeing it, or you didn't. Later, people learned about inductance, and
they understood it.
So that is a kind of tacit knowledge that I call relational tacit knowledge.
It's something that can be known explicitly but, at a certain time, might not
be known. It has to drift across by the usual mechanism of tacit knowledge
transmission, which is socialization or hanging around with people. But
relational tacit knowledge is not incredibly mysterious because, you know,
you can make these things known. It's just that science is always going to be
full of tacit knowledge because you can never know everything at once.
Michael Bolton: Not only in science. In software, we're seeing this all the
time: the specification under-specifies what people actually mean, and they
import what they know into that moment of looking at the specifications.
And then, "Oh, I get it."
Now, that that sounds like a different kind of tacit knowledge—the "I get it"
kind. Maybe I am jumping ahead here.
Harry Collins: Let's go through the second bit of tacit knowledge. These
get harder and harder. The second type of tacit knowledge is what I call
somatic tacit knowledge.
That's the stuff you can do just by virtue of the way your body and brain are
constructed. For instance, a big problem in robotics used to be picking up
an egg because it was difficult to make the artificial hand delicate enough. It
seemed like a really delicate grip had to do with the exact softness and
compressibility of the ends of your fingers, the stickiness, and so on, and
this enables you to pick up really delicate things. So that is a kind of tacit
knowledge that just depends on the way you're physically put together.

Somatic tacit knowledge is the kind of knowledge analog computers use—
analog computers that do their calculations by the physical interrelationship
of parts. And it's also the kind that very skilled people—athletes and violin
players—try to develop in themselves. The nerve pathways between the
brain and the body, the hands, and so forth, develop not as a result of
following rules but rather by sheer practice. Philosophers are very keen on
this, but it's not the whole story.
Now again, I think this kind of tacit knowledge can be, in principle,
understood. In principle, you could describe or at least build a machine that
would do the same thing. And robots are much better at picking up eggs and
doing that kind of thing now.
The third one, which is the really difficult one, is collective tacit knowledge.
With collective tacit knowledge, it's impossible to see how you could ever
understand it because the locus of collective tacit knowledge—the location
of where it is—is society.
An example I often use for this is when my son said to me—we both watch
football on television—and he said to me, "Wow, that was a wicked goal." I
didn't know what it meant. What is a wicked goal? How can a goal be
wicked?9 But he was ahead of me in teenage jargon. In his little group, the
word "wicked" meant "very good," oddly enough. Language changes like
that all the time.
Michael Bolton: Kids here in Canada say "sick" when they mean
something great.
Harry Collins: This kind of teenage language is a good example. And poor
old parents get stuck over it. And, of course, computers get stuck over it,
too, unless those computers are embedded within their society. But we do
not know how to embed them.
That is collective tacit knowledge. Driving in traffic is another good
example because it varies hugely from country to country. I mean, the time
I went to China and saw how they drove in China, I was absolutely
flabbergasted by the rules of the road. I couldn't make any sense out of it at
all. I couldn't understand why the death rate in China wasn't huge. (Well, it
turns out it is huge, actually, but not as huge as you would expect from the
taken-for-granted knowledge of European driving.)

The analogy I like to use is a thermometer in a beaker of liquid. Think of
the spoken discourse in which we are immersed as being like the liquid, and
the speaker as like the thermometer. The temperature registered by the
thermometer is like the language spoken by a fluent speaker—it is the
property of the liquid, not the thermometer, even though it feels to the
thermometer (language speaker) that the temperature registered (way they
speak) belongs to them.
Michael Bolton: That's something that has made me really interested in
your take on sociology because I see a bunch of different forms of life
intersecting in software development. We have technologists who are very
good at translating the world of humans into the world of machines.
Sometimes, interestingly, they're not entirely comfortable with the world of
the other humans for whom they're doing the service. That reminded me of
the notion of trading zones.10 The program itself becomes a kind of
boundary object11 where people are pointing at the same thing and saying
it's the same, but their take on it is quite different.
Harry Collins: Yes. I can see that. For me, as an academic, you're always
revising and revising and revising and looking for perfection. And that's
quite hard on somebody who's got to write a piece of software to fulfill your
criteria because they are going to run out of money very soon.
In the project I was involved with, the piece of software we wanted built
was very complicated. It was being built to run a new kind of experiment.
We weren't quite sure what it was supposed to look like at the outset
because the experiment kept developing. I have a little bit of sympathy for
the software builders, but they didn't share our world. They didn't have the
same enthusiasm for really making this work.
It's a difficult thing. The hard discovery—and if we'd had more sense, we'd
have realized at the outset that this was going to happen—was that the
software builders didn't share our enthusiasm for revising and revising and
revising until it was perfected. Their enthusiasm was for taking the sum of
money that I had and getting in and getting out without too much damage to
their financial lives.
The way we solved this in the end was by taking one of their employees,
whom I happened to know, and bringing him into our project so that he
built the thing, in the end, with the same level of enthusiasm as we had. But

I would be very reluctant in the future to have a piece of software which
had to fulfill the requirements of some academics' experiment built for me
by people outside the project.
Michael Bolton: This is another thing that's very odd to me. When I ask
people what they mean by "manual testing," one of the things they say is,
"We want to do testing that's like the way the end user would use a
product."
Then I find out that they've never met a user; they've never spoken to an end
user. They are using a different kind of perspective, although they're not
aware of it. They assume that they're just like their users.
That speaks to yet another aspect of your work: the notion of contributory
expertise versus interactional expertise. Can you tell us a little bit about
that?
Harry Collins: I've got a funny feeling you want me to say that there is a
huge difference between contributory and interactional expertise, and I want
to say that there isn't. The surprising thing about interactional expertise is
just how powerful it is.
Interactional expertise is expertise you gain by being embedded in the
linguistic discourse of an expert group. So, if I were to spend my life among
tennis players, I would eventually gain—as long as I kept talking to them
and they were willing to talk to me—interactional expertise about tennis.
It's what I did when I was hanging around with gravitational wave physics
people: I eventually gained interactional expertise in gravitational wave
physics. And what that meant was that in a Turing test or an imitation game,
I could pass as a gravitational wave physicist. And I did this on a couple of
occasions.
I'd have a gravitational wave physicist ask me difficult technical questions
and ask another gravitational wave physicist the same questions at the same
time, and we both supplied answers. The kind of questions you ask in this
sort of game are questions that have a practical significance.
And then we sent the two dialogues out to other gravitational wave
physicists and asked, "Which one is Harry Collins, and which one is the
genuine gravitational wave physicist?" We sent them out to nine
gravitational wave physicists, and seven of them said they couldn't say who

was who ... and two said that I was the genuine gravitational wave
physicist!
And then I did a second test some years later with a more complicated
approach, and again did pretty well.
Now, contributory expertise is what the gravitational wave physicists
actually do. The gravitational wave physicists actually do gravitational
wave physics; they do experiments, calculations, and so on, which
eventually lead to the detection of a gravitational wave. I can't do any of
that or even much of it; I can do very little of it. But I can make judgments
concerning it that are good enough to fool gravitational wave physicists.
People are tempted to say, "You just learned to talk the talk, but you
couldn't walk the walk." And that's when I have to say, "No, you're
misunderstanding what interactional expertise is," because interactional
expertise is far, far more than just talking the talk. It's being able to make
sound, practical judgments about what you should do if you could do it.
The project manager of the LIGO (Laser Interferometer Gravitational-Wave
Observatory) project, Gary Sanders,12 didn't do any of the actual hands-on
physical stuff. He didn't go in there and polish mirrors, design mirror
suspensions, or do gravitational waveform calculations. What he did was
manage all the people who were doing those things. And he had to
understand what they were doing well enough to manage them right and
make choices: "No, we're going to have mirror suspensions made this way
rather than that way." So actually, he was, ironically, a major contributor to
the project and to the detection of gravitational waves, even though he
agreed with me that all he had was interactional expertise.
Michael Bolton: I think the way that fits for testers is that testers have to
develop interactional expertise on both sides of the project. A tester must
understand the developers' form of life,13 and the tester must understand the
end users' form of life. I think it requires deep immersion in both to test
software really, really well.
Harry Collins: Yes, that makes sense, and the trouble is that the people
you're describing to me haven't done the latter. They haven't developed
interactional expertise in the users' form of life. So, you are quite right. If

you really want to do the job properly, you've got to get sufficiently
immersed in the users' form of life to understand what they want.
But that doesn't mean to say you have to become a user; it means you've got
to spend a lot of time immersed in the spoken discourse of the users. And I
don't think people take that seriously enough.
Michael Bolton: When I think back on what I learned in testing work with
banks and financial institutions, it was fascinating to me how quickly I
could get going just from immersion—hanging around, watching, and
listening to people. Hearing people talking and using the specialist language
and the jargon, and then trying to talk like they did. Parts of the
sensemaking process there were explicit, but in retrospect, a lot of it seems
to have been tacit.
I feel like the same kind of thing happened when I was doing consulting
work in China. I don't speak Mandarin, but even though I didn't understand
the words they were using, I felt like I could get a good idea of what they
were talking about. There would be occasional English loanwords and
references that I could understand, and I quickly picked up on the Chinese
words for key testing and software terms.
Harry Collins: You'd have to do a lot of work to persuade me that that was
the case because I think that understanding the native language is key to
getting interactional expertise.
Michael Bolton: I agree. What was interesting was how I could still ask
useful questions because of the nature of the interaction that they were
having with each other, not necessarily because of what specifically they
were talking about.
Harry Collins: It wasn't rapid for me. I reckon it took me 10 years before I
started to get interactional expertise in gravitational wave physics. And that
was speaking the native language.
Michael Bolton: In software, though, there are some patterns of stuff that
are universal. We've got problems to solve that are—it's a weird way to say
it—a limited form of ubiquitous expertise for that form of life ... the
software development form of life.
Harry Collins: Yeah, I understand that. There's not so much to pick up. It's
the difference between something like anthropologists and ethnographers.

Anthropologists have to go and visit some strange, distant society and try to
learn their ways of life. It takes them several years because they have to
learn the native language. Whereas ethnographers, they're learning forms of
life which are within their own overarching culture, so they don't have to
learn the native language, and so they can do it for a few months a year
every now and again, and that's the difference. But I don't want to admit too
easily—as you seem to be wanting to say—that there's no real difference
between anthropologists, who have to learn other native languages, and
ethnographers, who don't. I think your software engineers are a bit more
like the ethnographers.
Notes
1   Collins, H. (2021) "The science of artificial intelligence and its critics,"
Interdisciplinary Science Reviews, 46(1-2), pp. 53-70. Available at:
https://doi.org/10.1080/03080188.2020.1840821
2   Collins, H. M. (1992) Changing order: Replication and induction in
scientific practice. University of Chicago Press.
3   Bach, J. (1997) "Good enough quality: Beyond the buzzword,"
Computer, 30(8), pp. 96-98. Available at:
https://doi.org/10.1109/2.607108
Bach, J. (1998) "A framework for Good Enough testing," Computer,
31(10), pp. 124-126. Available at:
https://doi.org/10.1109/2.722304
4   Collins, H. M. (1990) Artificial experts: Social knowledge and
intelligent machines. Cambridge, MA: MIT Press.
5   By "repair," Harry is referring to the process of automatically ignoring
wrong answers and substituting correct ones in our minds. For example,
if you are watching an episode of a TV show on Netflix with a friend
and he wants you to pause it, he might say, "Stop the movie." If you
didn't perform repair on that statement, then you might say, "What
movie? There is no movie here," much to the annoyance of your friend.

6   Collins, H. M. (1998) The Shape of Actions: What Humans and
Machines Can Do. The MIT Press
7   Ibid.
8   Collins, H. M. and Harrison, R. G. (1975) "Building a TEA laser: The
caprices of communication," Social Studies of Science, 5(4), pp. 441-
450. Available at: https://doi.org/10.1177/030631277500500404
9   Note that the person who initially transcribed this interview was a
Romanian whose English is excellent but who is not immersed in an
English-speaking culture. He transcribed this as "wikied" rather than
"wicked." This is a further example of Harry's point.
10 Collins, H., Evans, R., & Gorman, M. (2007) "Trading zones and
interactional expertise," Studies in History and Philosophy of Science
Part A, 38(4), pp. 657-666. Available at:
https://doi.org/10.1016/j.shpsa.2007.09.003
11 Star, S. L. (1989) "The structure of ill-structured solutions: Boundary
objects and heterogeneous distributed problem solving," in Glasser, L.
and Huhns, M. N. (eds.) Distributed artificial intelligence. Vol. 2. San
Mateo, CA: Morgan Kaufman, pp. 37-54. Available at: Elsevier.
https://doi.org/10.1016/B978-1-55860-092-8.50006-X
12 Collins, H. and Sanders, G. (2007) "They give you the keys and say
'drive it!' Managers, referred expertise, and other expertises," Studies in
History and Philosophy of Science Part A, 38(4), pp. 621-641. Available
at: https://doi.org/10.1016/j.shpsa.2007.09.002
13 Collins, H. M. (2019) Forms of life: The method and meaning of
sociology. Cambridge, MA: MIT press.
OceanofPDF.com

CHAPTER 18
How Testing Looks to Management
—By Keith Klain
Introduction
James and Michael met Keith Klain when he was in charge of the global
testing organization for Barclays Bank. In his career, Keith has led software
quality, automation, process improvement, risk management, and digital
transformation initiatives for retail banking and capital markets clients in
the US, UK, and Asia Pacific. In addition to his work at Barclays, he has
hired and trained thousands of testers across the world for firms like UBS,
Citigroup, Fiserv, and KPMG UK.
He is also an advocate for workforce development and social impact,
having designed and delivered technology training curricula for nonprofits
to create technology delivery centers in disadvantaged communities—at one
point getting James and Michael to teach RST to prospective testers in the
Bronx.
Keith has served as the Executive Vice President of the Association for
Software Testing and has received multiple awards and recognition for his
contributions to the software testing profession and diversity in tech.
We wanted to get Keith's perspective for this book because he has a unique
understanding of how global-scale corporations and senior managers think
about IT. If we are to survive and thrive as a craft, we testers must learn to
speak to management about our work.
Interview
Michael: How did you get started with Rapid Software Testing? What
drew you to it?

Keith: My first real experience with RST was just before 2010, but I was
working in an RST-ish kind of way before that. I think I'd always been kind
of pragmatic in my approach to testing and quality because of working with
investment banks. The nature of that business and the timelines require a lot
of quick decisions, and your stakeholders are often okay with fixing little
things later. You need to be pragmatic in your approach and get good
information to people quickly—because the folks in charge don't really
suffer a lot of nonsensical procedural things. RST fits with that.
Michael: So what did RST do for you?
Keith: The first thing RST helped me with was putting a shape and
vocabulary for talking about testing around its mission in software
development and system delivery.
To a lot of people outside of RST or the broader context-driven community,
testing is about producing a widget, creating or generating test cases,
generating test plans, and generating test artifacts. In contrast to that very
standard, generalized idea about software testing, RST makes the mission of
testing about getting useful information as quickly as possible to the people
making decisions; it's not about producing test artifacts.
A lot of those same people think about the mission of testing in terms of
building confidence, trying to get confidence in a system so they can feel
it's been fully tested. They're trying to understand whether or not something
can go live or not, and to make confidence the goal of testing. That's not
aligned with trying to learn something about risk and your exposure to it,
and RST has helped me explain that to people.
Michael: Why isn't the role of testing to build confidence? Surely that's
what we want, isn't it? We want confidence in our product before we
ship it. Right?
Keith: Well, yes and no. The people in charge want to have confidence
about the project, but making confidence the goal of testing is a kind of goal
displacement. Information may provide confidence, but it may actually
reduce the confidence that you have, you know?
Michael: Well, if it's unwarranted confidence, that makes sense, doesn't
it?

Keith: Absolutely. Projects suffer from unwarranted optimism because they
don't have good headlights and rely too heavily on biased systems for
navigation. If you're looking for confidence to release something, my
experience is that you're going to see what you want to see.
It's like when we're driving: we have all these things to help us understand
and control things about the car, all these monitoring systems, the steering
wheel, and our eyes and senses. When they're right in front of you, you're
going to have a bias toward looking at those things. And that's great—on
the assumption that we're on the right track and there's nothing in our way,
but the dials and speedometer and tachometer don't show us what's actually
in front of us on the road.
Testing is meant to provide some distance from the dashboard, to help you
see what's on the road ahead of you and around you, not just what's right in
front of you. You want information that's at some critical distance from your
processes, project, or the context in which you're building the system.
We've talked before about Daniel Kahneman. I like what he says:
Overconfident professionals sincerely believe they have expertise—after
all, they act like experts, and they look like experts. You have to struggle to
remind yourself that they may be in the grip of an illusion. Confidence
might be a by-product of testing, but it can't be the objective. Intentionally
testing for confidence is akin to throwing mud on the headlights of your car.
Michael: We've said for a long time that testing is like the headlights of
the project.
Keith: Yes. That idea kind of permeates much of the training that I've done
through RST: most of testing is in your mind, in the way you think about
problems and your role, trying to identify problems or risks. I like to say
that most of testing is above the neck. It's not like there aren't some tools
you can use to help, accelerate, or aid that. But we're testing to see things,
not so much to produce things.
Michael: You talk to people who are at much higher levels of the
organization than most testers do. How did you get there?
Keith: Well, I didn't start out there; obviously, you build up to something
like that. I think a lot of it has to do with how I talk to senior people or
stakeholders about our business. I spend a lot of time trying to understand

my sponsors, my customers, my clients: people who are the recipients of
my services. I want to know how they're thinking and talking about their
problems, including what their actual problems are, what pressures they're
under, what they need to deliver, and how they might think about testing. If
I can understand all that and talk to them, I can get them information to help
solve those problems or manage their risks.
Michael: How do you find out about them?
Keith: If you haven't met them yet, a lot of stuff is publicly available. You
can usually see where they worked on resources like LinkedIn, which can
give insight into what they might have been exposed to in the past. You can
do some research about different methods and approaches that they might
have used. And if you've already met them, you can just actually, you know,
ask them.
And depending on where I or testing is on the food chain, I refine how I talk
about things based on who I'm talking to. If I'm talking to the CFO about a
program, I'm going to focus more on the financial risks and operational
costs, as well as what exposure we have to the market for being late or for
reduced functionality.
If I'm talking to somebody who might be a technical product owner or a
technical managing director, they're going to want to know more about
coverage—not about specific test cases; they want to hear more about what
features or functions we have or haven't tested, what our exposure is if there
have been gaps in the testing process and something goes live, and what we
did or observed while we were testing.
When I start having those conversations with people, first of all, I usually
find they don't want to get into all the details. They're just happy for
somebody to take some accountability for how things are being tested. They
want to feel they've got some level of confidence that you know what the
hell you're talking about. I think that goes a long way, but most testers just
don't talk about testing that way.
Michael: Do you have examples?
Keith: I just had a meeting with a COO the other day. She wanted to talk
about the project's progress relative to the program plan, so I was putting
things in the broader context of how long it takes us to get things done in

testing and why. We were talking about some questions she had on a
functional aspect of the system: Are there known issues? What are they?
What might we have missed? I was able to say, "Here's what we've covered.
We've encountered problems in this area that were observed by the testers
but weren't documented in any formal way because of our cadence as a
team. But we haven't tested that piece yet, so your risk is that you might
need additional support on the day we go live. So do we need more people
there?"
After we'd been talking like that for a while, she paused and said, "Hey, I
just want to say, I have never talked to somebody from testing like this
before." She said, "You're using words and you're talking in ways I
understand." She had heard all these test case metrics and pass-fail ratios
and all that kind of stuff, and those things had just confused her.
Michael: My perception is that many testers are responding to
managers who are saying, just give me a number, just give me the
numbers. Or maybe just red, green, or orange, which is essentially
three numbers. Are your higher-ups asking for things like that?
Keith: (Laughs) Oh, they ask for them, and I immediately say, "They don't
matter." Most of the time, those crappy testing metrics are self-imposed.
Michael: How so?
Keith: There's this idea sometimes in the software or systems development
world that testers don't make anything. What we're really doing is providing
information, and that's intangible. So testers get nervous: if the developers
produce code, testers should have something like that. Testers will say,
"Well, I need to measure my work, right? I need to give them something."
That's where I think a lot of desire to produce test cases, test plans, and test
strategies comes from: it seems to have become this counterweight to
developers writing code. "That guy wrote 400 lines of code, so I'll write 400
test cases today. That's the same as writing 400 lines of code, right?" Or
maybe, "I'll write 400 lines of test automation code." Or they'll start
counting test cases and talking about how many passed and how many
failed, and they get themselves into this unhelpful position.
When I come in, I want to get rid of that stuff. To anybody who wants a
metric like that or who is concerned about me getting rid of their scorecard,

I'll ask, "What was the last really important decision the business made off
of that information, whether it be cost-based, resource-based, or delivery-
based. Just tell me: what was an important decision that anyone made from
the pass-fail ratio?"
After two minutes of crickets, I'll say, "Okay, then what the hell are we
wasting everyone's time reporting on this for? If it doesn't give us anything
in terms of actionable intelligence about how to run this project, what's the
point?"
I turn the conversation to things that resonate with people. I want to talk
about what parts of the system we have tested, what parts we haven't tested,
and what's happening around that. Maybe we've got evidence from other
teams about known issues. We can have a real discussion around coverage,
and then we can talk about risk and what to do about it, how to mitigate it.
Talking about how many tests we ran isn't just unimportant—it's also kind
of a distraction. It doesn't help somebody who's paying for you to learn
something about the system so that they can make decisions.
All testing metrics or testing measurements are heuristic anyway, so they're
never ever going to tell you exactly what to do. You always have to use
judgment and discernment about any decision, let alone ones that are being
supported by that kind of funny math and by weird or inappropriate ways of
trying to measure work. Measurements like pass-fail ratios or counting test
cases or anything to do around most test measurements really almost
obstruct my view of how well that team is performing.
Michael: That's the mud on the headlights you were talking about
earlier.
Keith: Yeah. When you're driving a car, you've got the speedometer, you've
got the tachometer, and you've got a bunch of data. But there's a bunch of
other things ... your eyes, your ears ...
Michael: The sound of the motor ... your feelings about the road and
the driving conditions.
Keith: Uhhuh. All these different things. Exactly. You can look at the
speedometer, but to look at that exclusively is, in my view, pretty useless
next to, you know, actually looking around at the road and the other stuff
around you. So RST has really helped me put some language around the

limitations of testing metrics. I can articulate why those numbers don't
mean anything with respect to what testing actually does or what it could
do.
But it's not just about limitations around metrics. RST also taught me to
look at testing itself in terms of its limitations, to avoid believing that tests
tell you everything. A lot of the time, I'm actually trying to focus on what a
test doesn't tell you.
The same thing applies to automation. Usually, it's a shallow exercise to
confirm things we already thought we knew. I get little new information
from most test automation. It really doesn't do much in terms of helping me
understand new things. It might help get the system into a state where it's
more ready for complicated tests, but this idea of thousands and thousands
of shallow checks that are somehow meeting all our requirements or
coverage ideas is just a fantasy.
Once you start to explain all that to people, it doesn't take very long before
they say, "Okay, then what should we be doing?" And I say, "We should be
talking about where we've tested, what we've found, where we have not
tested, and what the risk is around that."
I'm working with a team right now—it's a bit of a turnaround project, and
I've been parachuted in to fix it. They're a fantastic group of testers. I'm
working with the test leads, coaching them on how to notice things and
describe them like that. I want to help them understand that what they
observe off the script—because what has and hasn't happened during the
test—is much more important than something I can tangibly count. And yet
that's something that they really lacked. The one thing that they had set up
was a kind of standard testing scorecard, and it wasn't helping them do
anything. So, of course, I came in, and I said, "We're not going to do this."
Michael: They were okay with that?
Keith: Well, you've got to know when to exercise your authority. There's
actual power and perceived power, and I always try to figure out where I am
with a group of people. I'm asking, "Can I do things and get away with
them because they think I can, or am I actually empowered to do them?"
And then you learn how to walk that fine line.

I see a lot of testers come into every meeting, feeling like every meeting is a
battle in a larger war, and the war is contingent on every single battle. I'm
playing the long game. So right now, with these testers, we're focusing on
meeting prep. Meeting prep is really important for message prep and vice
versa.
When we're going to go into these meetings with the project board, who's
going to be there? What are they interested in? What else is going on with
their programs? What are their risks, and which ones are they focusing on?
We want to figure that out as much as we can before the meeting.
Then we're going to work on our messaging so that we can be clear both on
what we want to tell the other people around the table and on what we want
to ask from them.
When I'm talking to a testing client, it's because I want them to know
something or I want them to do something. So, I'm coaching the testers to
think, "What do we want them to know? What do we want to tell them?"
And then, "Do we want anything from them in this meeting? Do we want
them to do something?" We need to make sure that before that meeting's
over, we've got our message across. That's the mission: to know what the
client wants, what the client needs from us, and what we need from the
client to help them get it.
Michael: Is that working?
Keith: Things are starting to turn a little bit. We're starting to see the
tangible benefits of the change in our approach. The clients are actually
listening to us, taking what we're saying seriously, because it's resonating
with what they're dealing with every single day. But sometimes, for some
people, it takes longer than you'd like to get them where you want with
respect for the testing team, building up the respect for the information
we're giving them. You have to meet people where they are on that journey
as opposed to forcing them to do that.
I always come at that by assuming that all the information they've had about
testing is bad, and that most of the things that they've learned about testing
or have thought about it are based on faulty assumptions. Sometimes it's
just misinformation. Less often, it's almost disinformation from bad actors.
It's not always malice, but as I get older, I sometimes think it might be.

Michael: "Never explain by conspiracy the things that can be
successfully explained by incompetence."
Keith: Yeah. Or more likely, the fact that they haven't thought about testing
very much. Or they're invested in an idea that they've brought in, and
they're worried they'll lose face if they have to dial back on it. The
counterexamples to RST—the factory school, the ISTQB nonsense,
certifications, the standards people—they all look at everything as a
procedure; their approach is that everything in testing needs to work a
certain predictable way.... That comes from a place of certainty, as if we
already knew what we needed to know. And I really don't come at anything
from a position of certainty like that.
Maybe I'm too, you know, Voltaire-ish (is that a word?1) in how I approach
things, coming at things from a position of humility and learning. I can't say
that I'm certain of this stuff. I've seen things, and I've got some anecdotal
evidence, but I can't say for sure that "if you do this, here's the result." But
I've seen the pattern over and over again: "If we write a thousand test cases,
if we have a test plan, if we have a test strategy that's structured like this, if
we do all these things, then we can predict that you're going to get this great
result." Well, I've got lots of examples from organizations where that
absolutely did not happen, and one hundred percent was not the result.
Everyone else does, too, if they think back. So, I ask people, "How'd that
work out for you?"
You'll catch some clients thinking that testing can mitigate all the risks, but
it doesn't. Sometimes their testing approach is actually injecting risk into
their business. If you're thinking about testing in certain ways, not knowing
the limitations of testing, it can actively hide risks—and that's exposing the
business to risk.
They sometimes say they want us to test everything, and they want us to tell
them how we're going to do it. They ask about how we're going to automate
tests, automate everything. Well, I know we're not going to test everything,
and I know that there's a lot we won't know about what we need to test until
we've started testing. If their idea is to automate every single test and rely
completely on shallow checks for our entire test coverage approach, that
approach will actually introduce risk to the business.

So, the compromises we make in how we test will have a direct impact on
the relative safety of what we're going to release—or inflict!— on our
clients or our own business. It leads to overconfidence. Not knowing what
that actually means, and not knowing what automation does and doesn't do,
and not understanding how testing works actually introduces risk to the
business. Saying that usually gets ears to perk up as well. That's another
conversation they're not used to having with testing people.
Michael: How do you make that conversation work?
Keith: I don't know how to articulate it in a way that sounds smart. I can't
write it down and make it go like a decision tree. It's something I practice
and think about. I'm a very, very active listener—although you probably
wouldn't notice that from the way you and I talk to each other! (Laughs)
People often tell me, "Wow, you were really quiet in that meeting."
Michael: True. That does not sound like you.
Keith: I see people shifting in their seats and looking at me like they're
asking, "When are you going to jump in here?" If they were to ask out loud,
my answer would be, "I'm listening." I'm a big believer in asking a question
and then shutting up and letting people talk through a problem and really
trying to hear what they're saying. If it doesn't make sense, then I'm asking
questions for clarification about it. I'm always looking at people through
that lens, asking, "Why did they answer that way, and how do I manage this
person as a client of testing or as a sponsor?" and then leading them back to
what we can do for them.
Michael: How do you come across as being a testing expert while at the
same time exhibiting uncertainty?
Keith: Well, I would never call myself a testing expert. I'd call myself a
student of testing. If you're not continually learning about something and
you feel like you're no longer a student of it, I think you're setting yourself
up to make statements that you might regret later in life. So I don't present
myself that way. I present myself as, "I've been doing this a while. I've seen
a lot of things." If I come at it with that kind of humility and honesty, I can
say, "We're going to make the best decisions we can, based on the limited
information that we have, and we're going to tack and adjust from there." I
think that's really refreshing to people.

Michael: What's different about the way you manage projects?
Different from what you see as typical practice?
Keith: My ultimate goal for managing testing projects is to get my sponsors
to leave me alone and let me get on with managing the team. Let me find
out what you need, and then let me manage the testing team.
I've got a well-documented disdain for what I call "managers of managers"
or "the coloring-in boys," the ones who just fill out metrics reports and
spreadsheets, but that's unfortunately kind of the norm for the industry. I've
never gone for the best practices nonsense. I look for heuristics, and I look
for agency—I think that's another part of RST that I didn't really put into
words until I started doing intense training sessions with the team at
Barclays.
I view my role as putting people in the right place to succeed and then
getting out of their way. I want to figure out, as quickly as I can, how I'm
going to get the best I can out of this group of people in this context. What
is realistic with this group of people? No one is going to hit the ball out of
the park with every swing of the bat or every time they step up to the plate,
so then I have to manage my expectations with every team. I want to get to
that for my own mental health.
For people to do really, really good testing, they have to have agency over
their processes and their tools. They have to be accountable for what they're
doing. Giving testers agency over their work and getting things out of their
way is fundamentally important to building high-performing test teams.
That's why I talk a lot about values, how we're going to work as a team, and
what our principles are. But I take that a step further in that I think that
agency also comes with responsibility for us to own the value proposition
of testing: not to talk about it so much, but to do the work and demonstrate
by example.
One of the underpinning principles for how I manage testing or work with
testers is that people start ignoring testing when it's no longer relevant.
Going back to what I said earlier, if testing really is the headlights on your
project, and if somebody is ignoring the information that you're giving
them, it's more than likely, in my experience, that you're presenting things
wrong. Who would not want to know that their car is headed for the
embankment?

In my experience, people don't want to crash, but they would like to know if
they're heading for a crash. If you're presenting that information and they're
not getting it, either you're giving them the wrong information, or you're
presenting in a way they don't understand because you're not speaking the
language of your project.
A lot of my way of thinking about talking with people comes from Harry
Collins's book, Rethinking Expertise,2 where he says that if you can speak
and understand enough about the language of a specialist skill, you can
bridge that gap, even without the practical experience of doing the
specialist's work.
I think there's a bit of a struggle sometimes to put into words quickly and
easily things that become intuitive later. And RST helped me sharpen that,
describing testing in terms of, "Okay, here's the status of the product. Here's
how we know. Here's what we need in order to make that stuff that I've just
told you about faster, more relevant, more on target, less expensive, and
more responsive to the project."
You need to own that as a tester, along with the accountability and the
personal responsibility that comes with that. That really resonated with me
about RST—that focus on the tester. RST puts the center on the tester and
the information the tester is producing. That's very different from what I'd
experienced before with other testing training that I'd gone through.
RST also helped me to give my testers skills on how to break a problem
down. The models of testing, the ways to model the product, the ways to
model risks, and the testing story... these things really help the testers
describe testing work to people. In my experience, this is exactly what
testers need to do for a project. You don't get that anywhere in the factory or
standardized certification stuff.
If you want testers to perform at a really high level, you need to stop
bugging them and give them the time they need for work, or training, or
practice. And most importantly—if testing is above the neck—they need
time to think.
So much today is about "continuous everything"—continuous deployment,
continuous integration—and speed. The pace at which people are doing
things is really, really disruptive to thinking deeply about problems. Then

the goal gets displaced from getting good products and services to
customers quickly, and turns into, "How do we back something out quickly
when we inevitably screw up?" It's continuous everything—except
continuous thinking.
Michael: They don't even get time to observe the product directly.
Keith: Yeah. Another thing that I got from RST, from your class
specifically, Michael, is that observations are test results, and scripted
testing, and the over-focus on it can lead to inattentional blindness. When
you're focused on one thing, you're missing what's happening off-script.
In my view, everything interesting that happens during testing has nothing
to do with a script or a scenario; it's what's happened along the way. I don't
know who coined that term "normalization of deviance," but it's a real
thing.
I'll give you an example. I was working on a project last year, another
rescue project. The system might have been functionally correct, but the
testers were immune to noticing how badly the system performed. They
were used to starting a test and saying, "Okay, I've hit submit. I'm going to
go grab a coffee."
When we started to move it toward production and the performance
engineers got involved, they said, "Holy cow, this is really bad. Why?"
Well, the testers were so used to performance being crap, they were
shrugging their shoulders about it. "Well, you know, that's just how this
thing works." They didn't even notice poor performance anymore.
Michael: Like they had become numb to their own impatience.
Keith: Right. Another thing that I got from RST was that emotional
reactions are test results. That comes up particularly when I'm doing user
acceptance testing, which I think should be done as early and as often as
possible. We try to introduce a system by having some people who are
going to use it do some testing. When we do that, we can put aside terms of
art in testing and what they actually mean for a while, and just ask people
about what they're experiencing.
You can't write a script for "The user isn't confused by using my system."
But you can give real users a task and say, "Try to perform this task. Let me
know any problems you see in trying to get the task or the testing done,

including when you're confused, when you're frustrated, when you're
annoyed ..." When real users get confused, when they say, "What was
that?" or "Why did that happen?" or "I don't know how that's supposed to
work"—that's a real test result. You can't script for that, but you can figure
out how it might be trouble for people working for the business. And the
test cases and the metrics don't tell you that.
There was another CIO I was talking to the other day for a company that
runs a payments infrastructure for, you know, a country, and one of the
topics he wanted to talk to me about was testing metrics.
He asked, "What kind of testing metrics do you provide when you test?"
and I said, "I'm going to turn that around. What information from testing do
you need to know to run your business?" And it was like that RCA dog
moment—that quizzical look into the gramophone. He was just like,
"What?" and he paused. He sat back in his chair and said, "I don't think
anyone's ever asked me that question before." He said, "That's a really good
question. You know what? I gotta think about that."
And I said, "Because that's what I should be doing for you: finding out what
you need to know to manage your business, and then I'll get you
information that helps you manage the risk of this program. You don't care
what I'm going to count or not count or any of those things. What you want
to know is, 'What's my exposure to this? How are we tracking against that?
Are we above or below cost? Should I spend more money?'"
All roads lead back to the both of us agreeing on me saying: "I'll manage
that. Tell me what you need to find out so I can get that for you."
Michael: You do talk to high-level people a lot.
Keith: Well, that's where I like testing to report into. I definitely want
testing to report to the highest level of sponsorship of the program, to
whoever's paying for the project. That's usually nonnegotiable for me. And
if we aren't there, I'll try to work out who that person is and how we need to
get there.
I think when testing is reporting to developers or program managers or
whatever, it's easy for them to control or even corrupt the message that's
going to the program sponsors. If they're not getting an unadulterated
message straight from the testing group, I actually view that as a risk. You

shouldn't have somebody read the speedometer for you and tell you what
they think it means, you know?
Michael: So when you're working with a team, what would a day look
like for you?
Keith: We usually have a very quick meeting every morning about what the
objectives are for today. I wouldn't like to use the dirty word "standup," but
we do have a very quick meeting.
Michael: A check-in.
Keith: Yep, a morning check-in. What are the objectives for the day? What
are we going to look at, and when are we going to take a break? Then who's
working on what, and what blockers do people need me to get into the guts
of to try and remove? And off we go. I'll try to do some kind of catch-up at
midday or at the end of the day, just to make sure we know where we got to
and what we need to tell people about what happened today. And that's
really it. And I try to clear as much of my day as possible to actually do the
work.
Michael: And what does that look like?
Keith: My view is that test management is 80 percent hands-on testing.
One heuristic for spotting a dysfunctional test team is when you've got
managers of managers who aren't spending 80 percent of their time in
activities that are really close to the work.
Michael: Which activities?
Keith: They can take multiple forms, but mostly I mean literally getting in
there and doing some testing or working with testers to mentor them on
how to test. Coaching people as they are going through testing, taking
pauses, and seeing where they're at. Doing reviews of what was tested to
look at test observations, test results, and the test strategy. I might coach
them on how we're going to approach testing for this particular part of the
project.
I still do pair testing. I was doing it just the other day, getting in and sitting
alongside folks. I don't do that as much as I'd like to, as sometimes I might
not be into the details enough to make a meaningful contribution. But

sometimes I'm useful enough to observe things that someone who is into the
details might not see—and that's what I would call actual testing.
And that, to me, is 80 percent of what test management is. If you can't
squeeze down all the operational stuff that you need to do as a manager of
testing into less than 20 percent of your week, you are absolutely doing it
wrong.
And there's another heuristic for a dysfunctional organization: too often, I
see projects where people are double- or triple-booked into meetings, and
they have no time for the work. They spend more time talking about work
than actually doing work, more time talking about testing than actually
doing testing. So I'm making sure that people have blocked out time in their
calendar to actually do their work.
Michael: It sounds to me like you're exercising your own agency over
your own time.
Keith: Yeah. Well, I think that's important because, again, you know, this is
knowledge work, and it requires time to think.
I gave a talk years ago. One of the analogies I used came from—I think it
was—a New York Times article3 about why ants are so efficient in creating
their whole communities underground. There are ants that aren't directly
involved in the construction, removal, or whatever activities they have
there. You know what those ants do? Nothing. They sit there until they're
needed again. They don't do anything. The article said the secret to anthill
efficiency is idleness, and I really liked that. People would be much more
efficient if they paused, if they took some time to think about the most
efficient way to do something, as opposed to just doing what appears to be
most efficient right away.
Michael: Let's say three people have invited you to a meeting. All three
meetings start at 10:30. How do you work around that?
Keith: I don't know where I got it from, but my first general rule to manage
my meeting workload is, "Only attend meetings that can't start without
you." If the people in the meeting can't decide something without me, that's
a meeting I should be at. I generally try to avoid any meeting that can get to
a decision without me having to be there, when they don't need me there to

decide what they need to do. Just like I very typically don't read any email
that I'm cc'd on.
Michael: You've got to be in the "To:" list?
Keith: Yeah. I usually say that if you need to tell me something, just buzz
me, call me, or text me if I need to know something. Or send it to me with
high importance. Regular email and cc lists have mostly become a CYA
mechanism to make sure "everybody knew that they were told," as opposed
to a meaningful form of communication these days. When there's a
schedule collision, I decide which meeting I have to be there for, and then I
find out from other people what the agendas and decisions were for the
other two.
Michael: What do you do when you're in the meeting?
Keith: Like I said, I try to spend more time listening and thinking than
deciding stuff. When you get people in groups, sometimes there's this
frenzy that takes over, where they try to seem like they're doing something,
and you get a lot of hasty and bad decisions made to appear that they're
doing something. Very frequently, we have big meetings, and then people
rush off. "Here are the minutes; try and get it all done as quickly as
possible." And no one has actually thought about the ramifications of what
we discussed, and they're mistaking movement for action.
George Carlin has this line where he says—I'm paraphrasing—"I love
people, hate groups of people." When I'm working with individuals, it's
easier to spend some time thinking about stuff, but groups can get into that
pattern. I say this a lot on projects: "Let's everybody just calm down, and
let's think about the problem for a minute before we decide what to do."
No amount of "process improvement" is going to solve your underlying
problem if your problem is organizational dysfunction. That's one of the
learnings that I took from the spirit of RST, as opposed to something that is
taught explicitly.
Michael: That's Jerry Weinberg's influence on us.
Keith: For sure. There's always a problem, and the problem is always
people. I don't remember the exact language from The Secrets of
Consulting,4 but you know, people are messy, and you're not going to test

your way out of a dysfunctional organizational structure. I gave training to
some of the teams at Barclays years ago, and it helped them as testers. But
no matter how great these testers were, they could not fix the underlying
problem with some of the projects they worked on, which were just very
dysfunctional.
In most places, it's not that there's something fundamentally broken about
how to test something. Well, maybe there is, but what's mostly broken—
fundamentally broken—is how we plan, design, develop, and deploy
systems. People try to fix everything through testing instead of dealing with
all the stuff that happens around it or before it.
Most other aspects of building and deploying systems could not withstand
one day of the scrutiny most testing teams are put under. There's no "How
many lines of code did you write today? How many project plans did you
write today?" The lack of scrutiny and discipline in all the other aspects of
building things and deploying systems is where the roots of the problems
are.
It's in that Robert Austin book, Measuring and Managing Performance in
Organizations,5 where he makes it super clear that the effect of
measurement on the thing being measured is a real thing. Setting goals and
targets for people based on inappropriate measures and metrics for testing
really drives a lot of bad behavior. And I don't want them to count things. I
want them to focus on risks. But if you're telling them that they're going to
be measured by how many bugs they find, how many test cases they run, or
on test case counts and pass-fail ratios, it's going to drive my team to want
to do things to make the counts look better. And you're going to have a lot
of queues clogged up with thousands of shallow tests or a bunch of
superficial bugs.
You'd be shocked at how much this is still standard practice today. I see test
proposals from large test consultancies saying, "We're going to run this
many test cases, and this is how we're going to set our targets. This is how
many test cases we're going to run per day." And so what do those testers
do? Why is the metrics scorecard always green, even though there are
obvious problems in the product that they're missing?
That drives really, really bad behavior in test teams. That's another reason
why I don't get into those metrics conversations with people. I don't want to

focus our energy on that because it drives dysfunction.
And it also drives the whole push for automation. I don't know how many
systems I've seen that have massive automation suites, and they're just not
going to make your software not suck. It never passes the "Who cares?" test
for me. You know? And stuff still doesn't work. You spend more time fixing
your test suite than you do actually solving a real business problem. I don't
want to spend a bunch of time or money on work I don't think we should
have been doing in the first place. It has just literally become a complete
work-generation machine that slows things down and inaccurately reports
where you're really at. But there's this Svengali-like hold from this
manufacturing analogy, this wishful thinking that people have about testing.
With the program I'm working on now, not one thing will be fixed by
automation. Not a single thing. The problems they have are not rooted in
the efficiency of the test process.
Michael: What would you suggest for working testers or test managers
who are reporting to development or product management and who
believe the message is being filtered or obstructed? Or ignored?
Keith: Yeah. Too frequently, I see people walk around in that mode:
"They're not listening to me. They don't care. No one cares about quality
around here." Then I ask, "How are you describing your work? How are
you talking about what you've found? How are you describing problems
you've encountered?" As my saying has been through a large part of my
career: manage your expectations, and manage your messaging.
Part of being responsible in a software testing context for a program is
rocking the boat, and you're not going to be everybody's favorite person.
That doesn't mean you need to be a jerk about it, or be disrespectful, or
rude, but the information that you're going to be giving people sometimes
might hit at how they view themselves as a person. They're not going to be
happy about that. We just need to try as best we can to give messages or
critiques that are appropriate to the audience.
I think a lot of testers struggle with that part of their role. They're part of the
project team, but they feel that they can't have critical distance and still be
inside a project. They think, "If I'm critiquing something, I'm not being a
good team member, or I'm being too personal about it."

But you know, people can critique music or art, and the language around
that doesn't have to get personal. It's about context, history, techniques, and
things like that, and you can try to understand how to give a critique
without making it personal. At the end of the day, you're going to give
information about the relative quality of their work, and that's not always
well received.
I think you can craft messages without them being lost, but a lot of that will
come down to how you're conducting yourself. If people feel like you're
walking around continually lighting your hair on fire, it'll be the curse of
Cassandra—where you have the gift of telling the future, but no one
believes you. In my experience, those things wear thin pretty quickly on a
project.
If people feel like your intent is to prevent something from happening—like
the project being successful or the product being released—you're more
than likely going to be ignored. If they recognize that you're giving
information to help prevent bad things from happening to the business,
that's a much better message. "Maybe I'm wrong about this, but this looks
like an important problem. Maybe we should understand what the risks are
around it—pieces that might fall down in production, exposure to the
market ..." And if the business decides they don't want to do anything about
that part of my report, I'll move on to the next one. I'm not personally
attached to it. It's the business's prerogative to choose whether or not to
address a problem.
Michael: I like the way James puts it: "I don't care if they fix the bug
or not. What I care about is being heard."
Keith: And you have to practice those conversations. You have to practice
how you talk about it. We used to do this as an exercise at Barclays: say the
CFO, CTO, CIO—somebody big—walks into your testing lab and asks you
about the last critical bug that you found. What was the potential business
impact of that defect if it had made it to production? Get the tester to say,
"Well, do I remember what the last one was?" and have them describe the
problem. Then I act like the CTO and say, "Well, what might be the
potential impact? What's it cost? How is it going to hurt us?" It's good to
practice those kinds of thought experiments, describing things in terms that
mean something to this person—the CTO or the CIO.

Michael: I'm thinking of a story that our colleague Ben Simo tells.
Early in his career, he went to his manager, very agitated about a
particular problem. And the guy said, "Okay, hang on a second.
Revenue is king, and liability is queen. Tell me how this affects one of
those."
Keith: A lot of testers don't do that. Talking about test case counts and test
plans and strategies and all that kind of stuff—again, I call it hitting the
bull's eye on the wrong target. It's so far off from where we need to be. The
problem is not about the fundamentals of how we test stuff. That's still
basically the same as it has been for all time. You know? You ask a
question, perform an experiment, and observe the results. The scientific
method really hasn't changed a whole lot.
Lately, there's this idea that we can rename testing to make the problems go
away. Let's call it quality engineering, automating, whatever ... but
renaming testing doesn't change the fundamental problem: people don't
know how to plan, think about design, or develop things in a way that can
be either tested easily or is aligned to some kind of business strategy that
we can manage risk around. That's the underlying root cause of all this
stuff. It's all the dysfunction that sits around testing. Which is nuts, because
if people paid attention to what they're learning from test results, we could
spend more time actually solving those problems and not developing
ridiculous regimes like what the Agile world had become.
Michael: Not a fan?
Keith: The biggest and most poignant case study for the law of unintended
consequences is the Agile movement, the Agile Manifesto. If people had
paid attention to what it actually said and what the authors actually meant,
that wouldn't be too bad, but it's turned into a hellscape that got unleashed
on the whole software delivery world.
Michael: I remember thinking that we'd already lost the plot in 2003 or
2004. "Individuals and interactions over processes and tools." That's
the first of those polarities in the Agile Manifesto. It felt like within
about 15 minutes, people started coming along saying, "That sounds
great! What kind of tools do I need to implement my Agile process?"

Keith: That's what I'm saying. It's a case study for the law of unintended
consequences. In the CIA, they'd call that blowback. They did something,
and it came and bit them in the ass on the other end. You know what I
mean? It's like everything they tried to do has unleashed this industry of just
basically the inverse of what those principles were.
And they don't really buy into any of it. They're still doing the same things,
particularly from a systems integration perspective, and nothing has
changed in how that works since the Agile Manifesto. Their ambitions got
bigger, and they just wanted to do things faster.
Then—and I come across this again and again—I hear, "Well, you're not
doing it right. It's not working in your context because there's something
wrong with how you're doing it."
I think people are sick of folks saying, "Well, here are all your ceremonies
and rituals," and when those don't work, they answer, "You're not doing
them right. You're just doing them wrong. What's wrong with you? What's
wrong with everybody?"
Michael: Yes. "If our solution doesn't work for your problem, you
should get a different problem."
Keith: Right. And it's kind of the same idea as, "If everybody you've ever
met is a jerk, you know, maybe you're the jerk." If none of your people can
do your process right, maybe the people aren't the problem.
That's what a lot of the training and methods in practice today seem to miss
—whether it's continuous whatever or Agile. Focusing on what matters to
your business and stakeholders has somehow been lost.
At the end of the day, what we're really talking about is owning your
message and using methods and language that are not alienating or too
difficult in practice to use. When I think about software testing and the
objectives and approaches that I've seen work in my 20 years of doing this,
RST has either informed or given me specific techniques to employ with
my testers or stakeholders on things that matter.

Notes
1   Yes, it means "resembling Voltaire" according to the Oxford English
Dictionary. See also "Voltairean."
2   Collins, H. & Evans, R. (2008) Rethinking expertise. University of
Chicago Press. Available at:
http://public.eblib.com/choice/publicfullrecord.aspx?p=408441
3   That article is based on this paper: Aguilar, J., Monaenkova, D.,
Linevich, V., Savoie, W., Dutta, B., Kuan, H.-S., Betterton, M. D.,
Goodisman, M. A. D. & Goldman, D. I. (2018) "Collective clog control:
Optimizing traffic flow in confined biological and robophysical
excavation," Science, 361(6403), pp. 672-677. Available at:
https://doi.org/10.1126/science.aan3891
4   Weinberg, G. M. (1985) The secrets of consulting: A guide to giving &
getting advice successfully. New York: Dorset House Publishing.
5   Austin, R. D. (1996) Measuring and managing performance in
organizations. New York: Dorset House Publishing.
OceanofPDF.com

CHAPTER 19
Technical Work Is All About Learning
—By Karen Nicole Johnson
At a certain point in a long career, you look back and see so many stories,
so many situations that took place, and how those vignettes shaped you
when you hadn't noticed. It's almost like looking through a kaleidoscope: a
mix of events, people, and places. You need to pull your eyes off the scope
and think a moment to decide which influences were so vital, so shaping.
You need time and distance to cultivate perspective. Of all the things I've
learned over the years, I see now that learning how to learn has been the
most valuable lesson.
How did I shift from a journalism major in college to a business and
customer-focused software professional? When did I shift from a tester
focused on bugs to a professional focused on clients? How did I become the
person who often serves as the "glue" between the tech team and the
executives?
Walk with me, would you?
Learning through Play
My quiet writing job marginally involved talking to other people, and the
work was interesting. I wrote the technical documentation, but often I didn't
have anything else to go on but my own efforts to learn and explore the
software. Mentally, I effectively went into a hole, a place of deep thinking
and learning, and, in the quiet of my job and in the quiet of my mind, I
discovered a whole world of software. I could experiment and discover
intricacies of software, so discovering how software worked was part of my
job. Exploring on my own was fun, challenging, and game-changing.
Fresh from my journalism degree, I could play for hours and then jam out a
chapter in little time. Basically, my approach was to play with software and
then make sure some writing happened. Play is one of the best ways to learn
because it doesn't feel like work. Play is not hung up on rules or end results,

so when I play with software, I explore and let my learning unfold without
expectations. I worked as a happy software explorer. Learning to learn on
my own, learning for the sake of learning.
And there it was, my discovery of the mental concept of a "world" and
"worlds," an idea I could envision, paint, build, and internally "see." In
RST, they call it a mental model. Inside a single world, there is a lot to
learn. It might not be so easy to be the master of a world, the metaphorical
know-it-all of any world (as in know all the intricacies, rules, algorithms,
etc.), but that construct of a world became a mental place I could return to
whenever I wanted, a place I could go and wander around inside. I could
mentally crawl inside these concepts of building a world in software and
forget all about normal human things like taking a lunch break or going
home at the end of the day. I was hooked.
I grew my own style of experimenting with software, trying every menu
option, reading the marketing materials to figure out what the software had
promised to do, and, over time, learning how to talk with the developers,
architects, network admins, security specialists, and DBAs to learn what I
could. I got comfortable with whiteboard conversations; technical jargon
was sometimes over my head, but I was motivated to keep listening and
learn what I could. I found bugs along the way, but was not yet a tester.
One day, I began working with manufacturing software known as Materials
Requirement Planning and Bill of Materials systems. It was a complex
system. You enter parts information, and when you plan to build, say, a fleet
of airplanes, each plane comprising hundreds, if not thousands, of parts, a
requirement document is generated showing what parts are needed. I felt
overwhelmed and realized that my normal learning methods didn't seem to
be working. I didn't know enough about the system to proceed. I wasn't
getting a feel for the system.
Another tech writer, Rich, was at the same company. We worked well
together. He was quirky and nerdy, friendly, and helpful. We were paired to
work on these two software programs.
"Let's build a zoo," he said.
"What?" I asked.

"Yeah, so, this software is a planning tool. It doesn't care what you enter,
and the algorithms will blow out a list of what's needed to build. You can
enter airplane parts, but you can just as likely enter animals, and the system
won't know. We can determine what is required to build a zoo, and the
system will help us plan multiple zoos and what we need to order. We can
build something interesting and be able to learn just as easily, so why not
have some fun?"
Once Rich convinced me that the Director of Engineering (our boss) had
better things to do than wonder what two nerds in the corner writing help
manuals were doing, I joined in. Rich showed me how he played with
software. He showed me how to think of unlike objects or concepts to gain
an understanding more quickly, to use different mental constructs, and to
build analogies in my head that might help me to understand software that
might otherwise feel difficult to get to know and learn. Previously, I had not
thought about comparing a software application to something else in life
that I already knew, something I could use my existing knowledge of to see
how it might be helpful in what I was learning. Rich's idea to use analogies
was another learning approach, and gaining methods or approaches to
learning was key because if I found myself stuck, I had additional options
to try.
Imagine that a zoo "requires" 10 giraffes, 45 koala bears, and 30 monkeys
in order for it to be "complete." After we entered the requirements and how
many zoos we wanted to build, we got a report stating that we needed to
order hundreds of koala bears. We had to fight to keep our laughter down.
We could decide a batch of chocolate cookies might need 100 pounds of
chocolate, and the system wasn't going to question us; the system was just
going to compute—whatever we asked it to compute.
Suddenly, this wasn't some dull manufacturing software, and we could build
whatever we wanted and have fun and learn at the same time. I loved this. I
had no idea that I would continue to learn using analogies forevermore as
one method of learning. I could build worlds, as in plural, all the worlds,
any type of world I wanted. This method helped me think about reality and
gave me another approach to making sense of what I was working with. I
could trace the output and write a help manual about it. Super fun.

Learning the Importance of the Business
Side
Working late one night, I came around a corner in the office and found the
president of the company pacing in a hallway. He hadn't seen me nearby,
and something told me to stay quiet and let him be. Bruno was his name. He
had an imposing personality and carried himself with dignity, but on this
night, he was pacing the floor, sweating profusely, and looking
uncharacteristically anxious. I was 23 years old at the time, and this was a
person I respected and feared, so I was stunned to find him looking so
nervous.
Only after he disappeared behind his office door did I find out there was a
board meeting that evening. As I stood alone in the hallway looking at his
office as he quietly clicked the door closed, I realized that even the boss has
a boss.
Higher-ups might have a whole board to report to, not just a manager, but
they still report to someone. There is no level inside an organization where
you don't have someone to answer to. Looking at the closed door that night,
I realized there is no escape from accountability. Everyone answers to
someone.
I stood in the hallway reflecting on the few times when Bruno had stopped
me in the hallway for a conversation. I talked of bugs and technical matters,
telling him about the important discoveries I had made. I saw now that there
were worlds I hadn't thought about, like the business world. While I was
building zoos or other mental constructs to learn software, Bruno was down
the hall, knowing there would be nights he must face the board of directors
and answer for the business. It was an eye-opener to me; how many worlds
are there? And which world did I belong in? In which world could I do my
best work?
A few months after the night I watched Bruno pace in the hallway, the
company went bankrupt. Our last paychecks bounced—a concept I had
never before considered possible. Between the closed-door night and the
bounced checks, I became conscious of the business side of any company I
worked at. After that experience, I became interested in how a company
was performing, not just how the software was performing. While building

zoos and being in the flow was fun, I vowed never again to ignore the
business. I was shape-shifting.
Learning to Be Helpful in More Than One
Way as a Tester
Time moved on. I lobbied to become a software tester and ditched writing
help manuals—documentation I was convinced no one was reading
anyway. I became the first tester at yet another small software company.
Over time, finding good defects—as in defects that mattered—became my
goal. The joy in work became finding good bugs. Bugs that were important
to fix. Bugs that were hard to find. Bugs that left me feeling like Nancy
Drew, software detective.
"Hey, Karen, come to my office and meet with Tom and me to talk about a
demo we're working on," Mike said. Mike was tall, lean, and blond. He
wore suits well, and he knew he wore suits well.
I thought instantly, without saying a word—What? I'd never had anyone in
sales say more than hello to me, and certainly the VP of Sales had never
asked to talk with me. Obediently, I went to Mike's office and found Tom
was already sitting at the small round table in Mike's supersized sales
office. I sat at the round table across from Tom and waited.
"So, here's the thing, Karen: we've decided we want to build a recorded
demo of a certain path through the software and then have that demo
available to ship to prospective clients or just be able to kick off a recording
while any of the sales guys are in a meeting and want the recording to run
on its own." Mike was pacing while he talked, but paused to look directly at
me to see if I was paying attention, as though being called to a VP's office
wasn't attention-getting enough. "Oh, that's an interesting idea." I was
noncommittal; I was waiting to hear what any of this had to do with me.
Tom shifted in the seat across from me at the small table. "You test the
software, right? And it seems to me this next build is lousy—every time I
click on this menu option or something, the system crashes. You must be
busy, huh? Writing up all those bug reports."

Finally, I thought, finally, someone has noticed that, as the only tester at the
company, I had both an important and busy job. I still thought the
conversation was odd, however. "Um, yeah, as the only tester in the
company, I generally am pretty busy." As soon as I finished speaking, I
realized that my comment might have seemed rude.
Mike stopped pacing and stood by the table. "We want you to sit with Tom
and guide him through the current version of the system and show him what
works and what does not. We will build the demo to showcase as much as
we can, especially the new features, but we will dance around anything that
isn't working. Got it?"
"Me? You want me to help with a sales pitch?" I asked.
"No," Tom said. "You don't need to write it, and we're not recording your
voice or anything; we just want your knowledge of what works so we can
sell what we can and avoid the rest."
Mike started moving back towards his desk. "Well, now that you get the
idea, why don't you two plan a time to meet, and Tom, let me know how it
goes." It was easy to see that Mike was done with us both.
Tom and I left Mike's office and planned to meet the next day. I wondered
why Mike had called the meeting and why Tom hadn't just stopped by and
asked me directly, but maybe together they figured I would understand this
wasn't a question for me but a directive.
Back at my desk, questions swirled in my head. "Is this a legitimate thing to
do?" I wondered. Am I helping the sales team lie to prospective clients? Do
these guys even care about what works and what doesn't, or are they just
about making a sale? Could I say this makes me feel dishonest? Is this
dishonest? I had to think back to Bruno, back to the company I watched go
bankrupt, and back to my bounced paychecks. If the company doesn't stay
afloat, do any of the product bugs matter? And how much choice do I have
as a tester if a VP is asking me to do something? Could I say no? And if I
said no, they'd just ask someone else in development, probably. I didn't feel
like I had a choice, but the bounced paychecks hadn't felt so great, either.
Tom and I met the next day. I had never sat with a salesperson and been
asked questions. I was impressed with how well Tom knew the software. I
had held skeptical impressions of salespeople and thought they were all

show and no depth, but I was wrong. Tom knew the past bugs as well as the
new features. It didn't take much time before we were working together in
sync. I navigated the keyboard and the software, and he took notes in his
leather notepad.
Tom looked at me, and I felt how much he wanted to make sure I
understood the mission. "I really want to showcase the new custom reports,
but every time I try to build a report, the system crashes. Any ideas how I
might work around that?"
"Well, I've filed a bug for the issue, obviously, but here's what I've found
out: the system might crash when you build a report for the first time, but
the system does actually save the report; so, when you log in the next time,
the custom report will show in the list of reports." It was my turn to show
that I also knew the software.
"Great," Tom said. "I didn't realize. I figure the whole custom reports
feature was broken. So, if I create a report in advance, I can avoid showing
the crash and just script what I say, as though, hey, you build reports, and
here is where to find them, let me show an example. I'll just skip the
creating part and act as though I'm saving the prospect the time of building
a report. I can do that."
"Yeah, that could get you past the issue but still show off the new reporting
features." I hadn't thought before about how embarrassing it might be to be
at a sales appointment and have the software crash. I had often felt like a
customer advocate, but I had never thought about what it might be like to be
in sales and to advocate on behalf of our own sales team.
As we wrapped up our time together, Tom looked at me and said, "You
know, it was Mike's idea to tap into you for your help, and I'm glad he did
because this whole conversation has been a big help. Would you be willing
to review the recorded demo once I have it built?"
"Sure, happy to help. I am helping, right? I mean, this helps the company,
right?" I felt the need to confirm.
"Of course it does. We are actually one big team, and we all want the
company to do well," Tom replied with no hesitation. And in that moment,
there was a pivot that took place as I realized that sales were as important as
development.

I stood up and nodded, "You bet. Glad I could help."
I watched Tom walk down the hall and thought, "Maybe I'm helping to keep
this company afloat. If I'm helping the sales team make sales, maybe that's
not a dishonest thing to do, but an essential thing to do after all. If the
company fails, then what's the point of fighting about which bugs get
fixed?"
Learning That a "Quality" Process Can
Interfere with Software Testing
Another company, another position, another city, and already in my early
30s, I had made more job and geographic location changes than my parents
had ever experienced. This time was different, I thought, as I approached
my new position at a company that made medical devices. Now in greater
Chicago, I was working in a large company where the company stock was
publicly traded, and that had more people working on one product than the
size of the entire start-up companies in Boston that I had worked in.
The product I would work with was regulated, and the testing was subject
to Food and Drug Administration audits. To prepare myself for that level of
scrutiny, I attended FDA auditor training. My role included being
responsible for the release documentation, which I thought meant I would
be back to writing, but I had no idea what was to come.
I was looking forward to meeting the rest of the team, since I had served as
the lone tester most of my career, but when I was introduced to a colleague
who used the word "quality" in their title, our short introductory hallway
meeting was a harbinger of a completely different world than I had known.
"Oh, you're a software tester like me," I said.
"No, I'm a quality specialist," Ellen said.
"What does that mean? Don't you test software?" I asked.
"No, I've never tested software. What is this testing that you're talking
about? My job is to write; I write test cases all day that map to
requirements."
"Well, then, who executes the tests, and isn't that person a tester?" I asked.

"We don't call them 'testers,' we call them 'quality assurance specialists,'
and they are offshore. They follow the scripts we write for them, and they
do exactly what the script says. We usually call test cases 'scripts' because
that is what the testing is—a script that tells them what to do. They collect
and print screenshots to prove what they did, and they check off each step
on the test case. Once we receive their formal paperwork, we update the
trace matrix to reflect how much has been completed and what remains to
be done," Ellen explained.
"What's a trace matrix?" I asked. I was skeptical since test cases were the
most formal documentation I had experienced until this point, and I had not
found test cases helpful.
"A trace matrix will map from a test case to a requirement to make sure
there is at least a one-to-one ratio of a test to a requirement and that every
requirement is covered. A trace matrix is used for every release for every
product that is subject to an FDA audit. The trace matrix is one level of
assurance, as in quality assurance," Ellen explained with what seemed
measured patience.
"How does a document ensure quality?" I asked. Ellen seemed annoyed by
my questions. I began to worry about this new position. I had made a bigger
change than I had realized.
"I've never had formal requirements, and I've never tied a test case to a
requirements document. I've never heard of a trace matrix. Don't the testers
explore and see what they see? How can a list of so-called scripts determine
when testing is done? A product isn't done until the team believes the
remaining defects are okay to ship with. I tested alone mostly, and the team
would always ask me what I thought of the state of the product." I was very
confused; I didn't know the situation could be so different.
"Sounds chaotic," Ellen said firmly. And then, right before she stepped
away, she looked me up and down and added, "Why would they hire you to
be the Release Manager, the one responsible for the audit documentation?
You don't know a thing."
And this began the long road for me to learn about another "world," the
world of regulated software testing. I recall thinking this person was
making this up—in this new, foreign world, would someone write test cases

all day and not test? Would executing scripts and compiling a stack of
screenshots prove a product was in solid, shippable shape?
I knew the answer: no. All that pile of scripts can do is show that the
product can work, not that it will. The scripts may be good enough to get by
an auditor, but they are not a cost-effective way to find every problem that
matters—every problem that could kill a patient or lead to a massive
product recall.
As time continued, I learned more concepts I had never heard of but would
soon be trained on: Six Sigma, Failure Mode and Effects Analysis
(FMEAs), and Corrective Action and Preventive Action (CAPAs). Of
course, the dreaded audit process was an activity everyone seemed afraid
of. The focus of work was audit-driven, not testing- or product delivery-
driven, but no one on the team asked anyone if they thought the product
was ready to ship; the delivery date and the documentation status made that
determination. This was no scrappy company, as no one here seemed
concerned about the company remaining profitable or paychecks bouncing
or even, ultimately, the state of the product; instead, the team focused on
passing audits.
Months later, while I was testing yet another version of the product, a full-
time person was brought on board to "own" the work of the trace matrix. In
this case, the matrix was a large Excel file that had multiple columns
matching cases to requirements. Anytime a test case or test automation was
built that would "cover" a requirement, the quality specialist needed to
inform the author of the trace matrix.
I thought about my time back at one small company when we had a big
release planned and how I'd devised an Excel file for a checklist for myself.
My concern had been to make sure I didn't forget anything. I recall asking
my boss at the time, with some excitement and pride, "Do you want to read
my Excel document? It's my test plan."
His reply was pragmatic: "No need. You will test, and we will ship when it's
ready. Whatever you didn't find, I will know from customer support or from
production errors. You do your best: you're good at what you do, and I trust
you. Documents don't make quality, people do."

How far away did this feel? A different world. When a new world appears, I
need to listen and learn. Only with information can I determine whether the
new world is one I want to work in, one I am suited to, and one I want to
stay in—or not.
At this point, I knew there would be no zoos, no exploring, and no fun.
Documentation replaced trust. The fear of audits replaced the joy of finding
bugs. Team collaboration over lunch was replaced by a calendar of
meetings. For the first time, work felt like work, and it saddened me to the
bone. I started to see the role of a tester as dull secretarial work, which, up
until then, I had not known could be possible. This entire situation felt like
being in a terrible accident in slow motion: gruesome and inescapable.
As the release author for this product, I was responsible for preparing the
official documentation. At other companies, I was used to sitting around the
lunch table, eating, and talking with developers. They offered ideas on how
to find bugs in what they had built, with a goal of helping me do my job
better. Ultimately, we built and shipped a better product—the emphasis had
always been "we."
Now, casual conversations were replaced with formal meetings about
FMEAs and who was allowed to be in the room. And the burden of
documentation was so great that deep testing was discouraged. Too much
paperwork.
This was not a job that suited me, and when the time was right, I found a
smaller company. I went back to a lean team, returning to my exploring and
being happy with the simpler position title of "tester."
Years later, I had another opportunity to work with regulated software, this
time with people I knew and respected. I'm grateful I tried again.
On this project, we discovered what we called "glimmers" or "ghosts" of
bugs, test conditions, and occasional crashes or inaccurate results that
concerned us, and so we brainstormed about what we could do. We still had
the heavy burden of documentation and passing an audit, but this team was
determined not to let that spoil our testing. The audit was one bar we had to
pass, but the more important bar was a product that was ready to ship.
We devised a plan of letting the testers with scripts continue their work for
audit purposes and built a second team of testers who would explore and

hunt, working solo and sometimes together to satisfy every fragment of
doubt.
People who care can build quality together. But I've learned that "regulated"
doesn't ensure that people care—a bone-chilling thought when I know this
could impact medical devices, clinical trials, and medications. Formality
doesn't ensure quality, nor does audit rigor ensure quality; people who care
build quality.
Learning to Speak and Persuade
There came a point in my career when I began to feel that my bug reports
were being ignored. I was too focused on finding them and not enough on
advocating for fixes. This is when I realized that I had to leave my cubicle. I
felt I needed to fight the good fight for quality.
And to fight, to persuade, I had to talk, which meant I had to leave behind
my preference to live and work as an introvert. I approached this problem
like any other technical learning challenge, which means I bought books. At
one point, I had purchased every decent book on arguing that was offered
on Amazon because I wanted to become a rockstar at getting bugs fixed. I
became a bug advocate. I learned to talk; I learned to persuade; I learned to
convince developers that the bugs I found mattered. And metaphorically,
after learning how to speak up, I haven't shut up since.
Around the same time, I met an expert tester at a software testing
conference. I was intimidated to meet this person, a keynote speaker,
because he was so well-known. He was a big guy with a booming voice and
large personal presence, but when we met, he became a person, and his
boom didn't scare me off. That was way back, a couple of decades ago,
when I met James Bach.
A few years after I met James, we talked again at another conference. After
seeing me present a track session, James told me that I should lean into
speaking. I recall telling him that I thought he was perhaps a bit crazy since
I'm an introvert, and public speaking is difficult. He was insistent, telling
me, "You know the stuff, you've been living the life of a true tester, and you
can speak. You should practice speaking more, and I think you have what it
takes to become a good speaker." I wasn't so sure, but I was flattered. Over

time, I took his advice and, with great surprise, even now, years later, I am
stunned to see and recognize that I enjoy public speaking. One fun fact no
one knows is that I've asked James to come listen to me present any time we
are at the same conference and to critique me so that I can improve. And he
has done so.
The more I could speak in public, the better I could speak back at the office.
The more challenging conversations I could navigate at work, the better I
could handle challenges in public speaking. Sometimes the best way to
learn is through more than one activity. Speaking at a keynote level at a
conference made talking to VPs, CEOs, and CTOs easier.
Bit by bit, I was personally evolving from the shy kid at the end of a row of
cubicles. The evolution was natural. All the conference speaking made
speaking to strangers easier. So I became an independent consultant. This
change was a gutsy move, as I had become a single parent post-divorce.
Giving up a regular income while raising a child was a choice that was
courageous, in hindsight, but at the time felt like the natural next step.
As a solo consultant, with personal pressure to pay bills, I focused more on
the business side of my practice than I had ever done before. I learned how
to incorporate my own business, how to invoice a client, and what to
charge, along with what a good client was and was not.
I pondered different questions from when I was solely focused on bug-
hunting. What does a client need? How can I prove my value to my client
and to their business? How is the client's business thriving (or not), and how
does my work help the business?
My thinking was shifting. I was no longer constructing zoos; I was
worrying alongside my clients whether their business was viable. And how
could I best serve? And in serving my client, I learned to watch in parallel if
my own business was thriving. I ran as a solo consultant during a high time
in the market, and I enjoyed that run.
Learning Forever
Sometime during Covid, I found myself without work. I had taken a long
sabbatical, the longest break from working that I had since I was 15 years
old. What should I do with my available time? Yes, there is always reading

and, yes, my writing too, but I needed something new. I have spent so much
time in my life with my work. My work was always more than the work: it
was my colleagues, my personal friendships, and my sense of community.
But away from everyone (having chosen to live on Cape Cod by the ocean),
I picked up a camera again and discovered how much photography has
changed since I last spent serious time with this old hobby of mine.
I bought a camera; I took a class. Then I bought a better camera and took
four more classes. I learned about night photography. I began exploring
outdoors, far away from computer screens. The break from everyone and
everything was helpful as I reshaped a new personal life. I had so much
terrain to explore and so much to learn.
My goal is to be a pretty decent amateur, that is all; I am not trying to "go
pro." Most importantly, I take photos because I love what I see. Capturing
the beauty I am surrounded by brings me tremendous joy. The fresh air and
just being outside of an office have done me a world of good. My eyes, my
body, my brain, and the whole of me have been grateful to step away from a
desk.
My photo skills are increasing, not just in "taking the shot" but in learning
all the accompanying software. Ah, the software. We develop our own
photos now—no more labs and waiting for paper prints to come back. We
use software to develop the shots we take in the field. Software to edit, to
improve, to perfect, to push a shot to show what we felt we saw. I have
found a new community among photographers, and I am comfortable with
the software, not surprisingly.
At first, I saw photography as different from my life as a tester, but then it
occurred to me that the photography has been all about the learning. The
learning brings joy (okay, spellbinding ocean views, too). And I first
became fearless about learning through my work as a tester. I brought all
that with me to my photography: the joy of learning as a forever student.
After a long sabbatical, I have found myself wandering back indoors and
back to my desk. I've relocated to Raleigh, North Carolina; I am a New
Englander adjusting to living in the South. I've rebooted my career as well,
returning to testing. I am curious to see what I learn next, as well as how I
contribute to the field of software testing. Whatever comes next, I am
confident that I will be learning and enjoying the process of learning.

OceanofPDF.com

CHAPTER 20
From RST to AST
By Lee Hawkins
The Start of My Testing Career
After completing my PhD in Pure Mathematics and spending three years as
a software developer, it was time for me to leave the UK in search of a
different lifestyle and new challenges—and so it was that I landed in
Melbourne, Australia, in June 1999.
It was perfect timing in terms of looking for a job in the IT industry at the
height of the "dot-com" boom. There was a separate hefty section of the
print newspaper The Age published every week just to advertise IT jobs! I
adopted a somewhat scattergun approach to looking for my first job in
Australia, applying for roles from development to technical writing to
management. Interview opportunities arose quickly, and one of the first was
for a technical writing position at a company called Quest Software, with an
office in the eastern suburbs of Melbourne. After the interview for this
position, I was offered the chance to talk to the test manager, who was also
looking to hire. Being on the hunt for a job, I was happy to take that
opportunity. The end result wasn't an offer of a technical writing role but
instead one as a Senior Tester. I immediately accepted the offer, my first job
in Australia, and the start of my career journey as a tester. Thus, like so
many others in our industry, I fell into testing rather than making a
deliberate career choice.
My experience of testing before joining Quest was limited to the testing of
my own development work, and I soon understood that there was much
more to it than I'd previously thought. I learned quickly from my peers and
manager about what good testing looked like, not realizing that I was
learning a very "traditional" and documentation-heavy approach to testing.
I felt like I was doing good work and helping teams to produce better
quality software, even though I now look back and acknowledge that I was

becoming something of a process cop, spending too much time poring over
documents and far too little time actually testing software.
I was enjoying the challenges of testing and turned out to be pretty good at
it, so some promotions saw me taking on more responsibility and playing a
more significant part in the broader testing team.
How Experiencing RST Led to My Passion
for Testing
In 2007, I heard about a training course coming up in our Ottawa office in
Canada called Rapid Software Testing, presented by Michael Bolton. This
was the first time I'd heard of this course, and some basic research indicated
that it sounded like a very different approach to testing than I was used to. I
was keen to learn, so I asked if I could make the long trip to Ottawa to
attend the course.
Leaving a very hot Melbourne in November 2007, I took a three-flight,
thirty-plus hour journey to Ottawa, where I arrived to a different world of
subzero conditions. The three days I then spent in the company of Quest
colleagues with Michael Bolton during RST were similarly transformational
in terms of my view of testing.
Though I'd heard some things about the course, I was unprepared for its
intensity. The application of the Socratic method made for some
challenging exchanges, but I saw its power in bringing about a genuine and
deep understanding of the concepts Michael was introducing to us.
Additionally:
Heuristics and oracles were both new to me, and I immediately felt
that these concepts would (eventually) improve my ability to explain
what I was doing and why during my testing.
The idea of a mind-map was another revelation: the simplicity and
visual form felt like a genuinely useful (and immediately applicable)
tool for me to use in my everyday testing work.
The Heuristic Test Strategy Model blew my mind, shifting my thinking
away from building a "strategy" as filling in a long wordy template (with,

of course, lots of boilerplate text) and more towards actually thinking about
which tests to perform and why in the unique context of a particular project.
Listening to Michael—and being challenged by him—was an awesome
experience and made me realize that I could add so much more value as a
tester than trying to be the quality gatekeeper I'd inadvertently allowed
myself to become. Michael had implanted in me a new mindset around
testing, as well as new ways to describe and develop my testing skills. It
was an exhausting but invigorating three days, and RST would prove to be a
pivotal moment in my career.
It would take me many years, however, to fully understand the depth and
significance of the concepts that RST had just introduced me to.
Returning to Melbourne, I was eager to spread the word about the great new
concepts and approaches I'd learned through attending RST. (And I was
probably quite annoying to testing colleagues less keen to change the way
they worked!) I now knew that we could offer so much more to our teams,
and I wanted to enact change based on what I'd learned—and I was lucky to
have supportive management who allowed me to start implementing some
new ways of working in testing in the Melbourne lab.
While some testers were reluctant to change, the general feedback on the
new ideas I was promoting from RST was very positive. We were moving
away from documentation-heavy testing processes and spending more time
interacting with the software to provide more meaningful information about
it than we had previously. Testers seemed to enjoy actually testing.
Testers were much more engaged in building test strategy when using the
Heuristic Test Strategy Model compared to template filling. They also
found the concept of oracle heuristics appealing, leading to more credible
bug reporting.
Mind-maps became the tool of choice for many testers during both test
planning and when reporting on their work. My own passion for
exploratory testing also built quickly as I saw its power through the
experience of working with testers to try out the approach.
In 2011, James Bach was in Melbourne to give a public RST class, and I
managed to secure approval for the entire testing team to attend. I was
excited to give others the opportunity for the same transformational

experience I'd had back in 2007 with Michael—and to have my second
view of RST through the different lens of James's teaching.
Some testers found the course very challenging, with no choice but to be
active participants because persistent Socratic questioning was new (and, in
some cases, uncomfortable) for them. It was interesting for me to compare
and contrast the different styles of Michael and James in presenting the
material around RST, with different stories told along the way and several
different choices of exercises. I wouldn't say that one was better than the
other; they just felt different, with Michael perhaps being more theatrical in
presentation and James more challenging of individuals along the way.
Some members of my group found the course a little too confronting, while
others loved it, but they all came away from it with valuable new
perspectives built on top of what I'd already passed on from my previous
experience of RST.
As more testers adopted RST ideas into their everyday work and my
approaches gained more visibility, further promotions gave me more scope
to broaden my RST message across more parts of Quest. Bringing this great
information to teams outside of Melbourne was very rewarding. I was
particularly proud of my work with teams in China, giving them the
opportunity to adopt exploratory testing as a professional practice.
Becoming Involved in the CDT Community
I was fortunate to be invited to a couple of testing peer conferences in 2013.
These small, attendee-driven events provide an opportunity to go deep on a
topic under the guidance of a content owner. Every participant must be
prepared to speak to the topic via a first-person experience report (ER) and
then field questions from the other participants during "open season." I was
apprehensive about taking part in my first such conference—the Kiwi
Workshop on Software Testing (KWST, in Wellington, New Zealand)—as
I'm not naturally comfortable talking in group settings and the potentially
lengthy questioning by the other participants was difficult to prepare for.
However, I was chosen to present my ER, and I handled the open season of
questions quite well.

With more confidence after this experience, I felt much more relaxed going
into the next peer conference shortly afterwards, the Australian Workshop
on Software Testing (OZWST, in Sydney, Australia). Content owner Rob
Sabourin selected me to present my ER, in which I shared my experience of
implementing exploratory testing approaches in testing teams in China. The
open season after this ER brought tougher questions than I got in the first
conference. But I was also more ready to cope with them. It was a safe
learning environment, and I found the experience highly valuable—and
even enjoyable! I met some interesting testers at these workshops, most of
whom were aligned with the principles of context-driven testing (CDT) and
had really meaningful things to say about testing. Following my ER in
Sydney, Rob Sabourin strongly encouraged me to share my story more
widely.
While public speaking has always been a big challenge for me, I trusted
Rob's judgment. I followed his advice, taking the plunge in submitting my
first response to a testing conference "call for papers," pitching the same
topic as my peer conference experience report to Let's Test (Sweden) 2014.
To my amazement, the talk was accepted, and I had the incredibly good
fortune (and, again, full backing from Quest) to visit Stockholm to engage
with an amazing group of testers at Let's Test. It was perhaps here that the
passion for testing on display all around me soaked into my being—and has
never left. I again met some incredible testers at this conference and was
first introduced to the Association for Software Testing (AST) by many of
them.
It was also around this time that I began to engage on social media for the
first time by tweeting about testing (as @therockertester) as well as starting
my own testing blog, Rockin' and Testing All Over the World. I quickly
began to feel part of something bigger, interacting more regularly and
deeply with many in the burgeoning CDT community.
What started at Let's Test has resulted in my giving conference talks in
Vietnam, Estonia, the United States, and the United Kingdom, as well as at
home here in Australia.

Building a Close Working Relationship with
the AST
Having heard good things about the AST and meeting interesting and
helpful people involved with the organization, I took up membership and
started to become more actively involved. I attended the Conference of the
Association for Software Testing (CAST) in New York in 2014, and the
format was noticeably different from other conferences I'd been to, with
speakers having to leave plenty of time for an open season format Q&A
session after their talks. This format resulted in deeper conversations about
the content and helped to refine my understanding of many topics discussed
during the conference. I was impressed by the audience's engagement with
the speakers and their desire to genuinely understand and learn from them. I
had a great time in New York and again met many more like-minded testers
who helped to fuel my passion for testing even more.
I was delighted to be accepted to speak at the CAST conference in
Nashville in 2017. With a few conference talks under my belt, my fear of
public speaking had subsided a little, but there was still the challenge of
open season rather than the more typical (and perfunctory) Q&A I'd
experienced at the conferences I'd spoken at before. I was proud to face that
challenge and deal with it well—so much so that I even volunteered to do a
lightning talk later at the same conference!
When the AST decided to expand their CAST conferences outside of the
United States, Australia was first on the list. I attended the CASTx event in
Sydney in 2017, and met up with many acquaintances from the CDT
community in Australia and New Zealand. I was keen to see CAST
continue in Australia and, when the AST were looking to run their next
Australian event, I was asked if I'd be willing to act as Program Chair for
the Melbourne conference in 2018. I was delighted to take on this role and
helped build and run a great conference for my local testing community.
The AST then asked me to take on more responsibility in co-organizing its
conference for the next year, which I ran alongside Paul Seaman in
Melbourne as the "Testing in Context Conference Australia 2019."
In supporting me to help organize the AST-branded conferences in
Melbourne (in CASTx18 and TiCCA19), the AST demonstrated their

commitment to community building and professional development. The
experience I gained by working on these conferences alongside the AST has
been highly valuable to me as a professional and member of the broader
testing community.
OceanofPDF.com

CHAPTER 21
Notes on a Seeker's Journey into Testing
—By Marius Frâncu
I am a programmer trying to learn as much as I can about testing. Some
time ago, I met Michael Bolton in person to interview him.1 I was surprised
to find out that he had questions for me as well. Our discussion was richly
varied.2 Here are some of the questions we discussed:
What should programmers know about RST? How is that different
from what they think they know?
Is there an important difference between a programmer and a tester?
Why do managers push back against the testers trying to be careful and
responsible about their work?
What about the trendy idea that "We do not need testers; programmers
will do all the testing"?
In this chapter, I will try to give answers.
"I Trust I Make Myself Obscure"
Before I give my answers, I want to cover some of the premises and models
that animate my views on testing.
One of my favorite Romanian authors is Nicolae Steinhardt. I would never
have thought that I could learn so much from his writings that would help
me in my professional life. There is a quote from Thomas More that
Steinhardt often refers to in his writings: "I trust I make myself obscure."
Steinhardt decodes this sentence as follows: "I hope that I am not easy to
understand, that you—the one I am addressing—will realize the complexity
and difficulty of the subject we tackle, the trust I place in you by laying bare
before you not conventional thoughts in ordinary words, but the very
turmoil of my most intimate and obscure mind."3

It is beautiful and appreciated when we manage to express ideas simply. But
I am not always able to do this. So, "I trust I make myself obscure."
Praxis
A decade ago, I was looking for material about how complexity science
might help me learn about practices and methodologies in IT. I was
fascinated by what Dave Snowden calls praxis4—the intersection of theory
and practice. To understand theories behind practices, I think we need to
cross the corridors to other disciplines.
It was in that context that I first encountered RST, which goes beyond
common testing folklore. RST incorporates knowledge from cognitive
science, such as inattentional blindness and cognitive biases,5 and joins6 it
via the continuum of formal testing.7 RST applies heuristics, and, when
analyzing and assessing risks,8 makes recourse to tacit and explicit
knowledge regarding the interpretation of requirements,9 and investigates
sensemaking of a product without a written specification, among many
other things.
RST is a praxis, not just a set of concepts or a set of rules. It evolves like
science. RST is not an easy way of testing; it requires constant learning,
adapting, and openness to changing existing ideas, concepts, and work
habits. Recently, I was speaking with James about the latest book by Gary
Klein, Snapshots of the Mind, and he said an interesting thing: "I will have
to update some ideas that I thought were already good enough. But it must
be done."
Core Ideas
My understanding of engineering is structured according to certain concepts
and categories, core ideas that shape my version of RST: words, knowledge,
data, information, epistemology, focus, roles, and emergence.
Words
RST puts a lot of emphasis on words. This is because they create a
connection between tacit and explicit knowledge.

A key point, for me, in understanding the importance of words was the book
The Symbolic Species: The Co-evolution of Language and the Brain,
written by Terrence W. Deacon. In it, there are pictures of the brain showing
what happens when the following events take place:
Phoneme identification and oral movement sequences
Naming, reading, and grammar
Short-term verbal memory
Automatic, repetitive speech
Simple word perception
Word list generation
Hearing a rapid sequence of words
Shadowing: repeating rapidly presented words
Word association: generating a verb for a noun
Imagine that different parts of the brain activate for each point shown in the
preceding list. It's fascinating.
Then the author makes an amazing claim: "The major structural and
functional innovations that make the human brain capable of unprecedented
mental feats evolved in response to the use of something abstract and
virtual as the power of words." Deacon is saying that the expansion of the
brain that took place in human evolution, which indirectly produced the
prefrontal expansion, was not the cause of symbolic language but a
consequence of it. Brain-language co-evolution significantly restructured
cognition.
Words are important. The human brain excels at processing and identifying
patterns with great efficiency, thanks to its capability for parallel
processing. Words are neural patterns.10 As Michael Bolton once said,
"Words are often powerful because their meanings are slippery, ambiguous,
and elusive, and allusive, and illusive."11
Alas, the common terminology of testing is confused, and it focuses on
artifacts instead of processes and skills. RST tries to sort this out and create

better ways of expressing the depth and variety of testing experience.
Sometimes that irritates other testing thinkers. (I am thinking here, for
example, of testing vs. checking.)
The medical field is very careful with words, as is the legal profession, and
I believe software engineering should follow their example. Careful
attention to words in testing will help us define, analyze, and identify
concepts, which in turn will also help us identify when and how to apply
those concepts.
Data, Information, and Knowledge
In Max Boisot's fantastic work Explorations in Information Space, he
shows how data, information, and knowledge are three different things:
Data are those differences that are perceptible in the physical states of
the world. These data come like a waterfall, pouring over us.
Information consists of relevant regularities that we find in data; it is
always relative to the particular biases held by the people or systems
that collect it.
Knowledge can be thought of as a set of probabilistic distributions—
beliefs that a person has that guide their actions.12 Information
becomes knowledge only if it is internalized and becomes part of the
structure of human expectations.
These differentiations between data, information, and knowledge are
important because they show, at a lower level, three different things that
happen to a tester while testing a product.
A lot of data is available to us, the testers. Data might be CPU details, logs
in text files, logs in Event Viewer, memory-related data, database data,
database logs, business data, and what is happening on the screen.
Information will be what the tester identifies as significant. For example,
CPU utilization and number of threads may be significant for a tester
concerned about performance. But this does not necessarily mean that it
will be the same for another tester, who is looking at output correctness.
From all the data available to a tester, only some will make sense because

the tester will know how to link it with what they know. A tester's
understanding is filtered.
Knowledge means that the tester will know how to identify information.
Knowledge is not static but is continuously built in the mind of the tester.
Knowledge building is not a linear process during testing, nor is it most
likely perceived the same by all the people on the team. What is really
shared in a team is data—and sometimes information—but not knowledge
itself.
The cascade of data when testing a product is enormous. When I think
about this, I recall another article by Max Boisot titled "Connecting the
Dots: From Data Processing to Pattern," in which he presents a geometrical
metaphor to better visualize the scale of the problem. Imagine that you have
10 dots arranged on a flat plane. There are 45 pairings that could be made
among the dots, and 45 links that we could draw. But the number of
possible patterns (lines, triangles, pentagons, polygons ...) that could be
constructed from these links is 3.5 trillion. This is like the relationship
between data, possible information, and possible knowledge. The
implications of data can explode in every direction. This is why our brains
filter out so much data.
In response to the seemingly infinite possibilities of data, we need to be
inventive and adaptable in order to find ways to differentiate meaningful
patterns from the trivial ones. Testing is, among other things, about finding
those relevant patterns within the complex system in which that product
will operate. The patterns are not static, and they continuously change.13
So, even though we may have written requirements, they are not enough to
do proper testing. We need to interpret them. We must bring all our own
background knowledge to them. We consider them, but also respectfully
doubt them. They are informative but do not codify knowledge.
Mindset
How might we discover the truth about an actual product? How is that
different from running simple checks?
One of my favorite presentations by James Bach is "A-Galumphing We
Will Go,"14 in which he discusses the way he "exploits Ashby's Law of

Requisite Variety to explore a phase space with unknown degrees of
freedom." He intentionally uses obscure terms in that phrase so that he can
do two things at the same time: show that testing is a deep and worthy
subject, and also set the audience up to demystify the language of systems
science. The phase space of a software product is the totality of all states it
can have, in all dimensions. Degrees of freedom are dimensions or, in other
words, variables. Testing is the process of exploring all that a program can
be or do (its phase space), yet we don't know all it can be or do—its
variables are not fully known.
Ashby's law of requisite variety says that "only variety can destroy variety."
Ashby was talking about controlling something like a car. He means that
only the sufficient ability to turn the steering wheel (variety in turn radius)
can keep the car on a road that is curving (variety in required direction of
travel). You turn the wheel of the car to "destroy" the difference between
where the road is and where the car is. For testing, this means that only
"variety can explore variety": only a sufficient variety of actions will trigger
the whole variety of behaviors a product is capable of performing. So,
Ashby's law of requisite variety is an invitation to action, and knowledge is
a key aspect of effective action. If you don't have enough knowledge, you
will just have to try a lot of weird, random things (galumphing!).
But how do we know we did a proper exploration of the variety? This is a
question of epistemology. Epistemology is the study of the nature of
knowledge and justification.15
In RST, we say that testing is applied epistemology. Some would say that
"epistemology" is just a fancy word. Well, yes, but it's a word that refers to
an entire branch of philosophy, brimming with useful wisdom.
Again, Boisot has an interesting take on it. He argued that knowledge
management is open to multiple epistemologies. And this, for me, is
applicable to testing as well.
Recently, I was speaking with some colleagues about unit testing. They
wanted to see what it means to do unit testing in the current project we are
working on. It was not an easy job, and they were kind enough to share
what they had discovered with me. In the midst of this, I realized that unit
testing deals with things that we already know about—things that are
probable, foreseeable, and repeatable.

And then we realized that there is another part that we need besides this
probable aspect of testing and automation: what is plausible yet hard to
know? For example, testers need to explore memory leaks, new security
flaws, strange use cases, or a very important bug that the CEO found. For
these kinds of problems, we cannot build automated verifications in
advance. With the benefit of hindsight, we may be able to build a check for
that specific thing, but this isn't much use before we've ever seen a problem
like it.
We realized, in our conversation, that the probable (things that we can
repeat, like unit checks) and the plausible (deep, intertwined problems) go
hand in hand. They do not exclude each other; they complement—and even
"feed"—each other.
In Explorations in Information Science, Boisot identifies four important
components regarding beliefs that a tester, manager, or programmer can
have:
Unconstrained beliefs (possible worlds): This represents any state of
affairs that does not contradict the laws of logic and physics. It's most
interesting to consider possible worlds that are not known to be
probable, plausible, or actual. These include worlds for which we have
no information about probability because no prior data exists. The
tester operates here under conditions of novelty and uncertainty, using
open exploration. New ideas of how to use automation might pop up,
along with novel ideas about what kind of automation to use. We might
begin to make sense of the situation by constructing a product
coverage outline,16 where we put all the elements—visible and
invisible—that might become the subject of testing. How to test
remains undecided at this point.
Justified beliefs (probable worlds): This represents states of affairs
based on intersubjective, demonstrated coherence and correspondence
with accepted fact. These are well-delimited states and outcomes. We
can often calculate probabilities for propositions in this category, but
they are not necessarily true. For instance, knowledge of this kind is
achieved when a well-defined requirement is subjected to an
algorithmic check that passes. The passing check cannot guarantee that

the requirement is truly fulfilled, but it provides corroborating
evidence.
True beliefs (plausible worlds): These are states of affairs that seem
true for the tester based on prior experience without justifying it to
others. It coheres17 with and corresponds to facts already accepted by
that tester. It may be highly subjective, coming from deep intuition or
extensive personal experience. This is the frame in which deep testing
operates, because a good tester must challenge unjustified beliefs—
even ones that seem highly plausible. This category also becomes
relevant when challenging well-justified beliefs that may not be true.
Justified true beliefs (actual worlds): In our case, justified true belief
is about the real status of the product. We never know for sure that we
are correct about this, but achieving it is good enough.
For Boisot, knowledge is a set of beliefs that inform the decisions of agents
to take actions. He represents the possible, plausible, actual, and probable
worlds in a Venn diagram, as shown in Figure 21-1 (the paths are my
emphasis).

Figure 21-1: Epistemic Paths
Boisot defines "mindset" as the embedded epistemology that an agent
employs to navigate a path from possible to actual worlds. This means that
there are two basic types of navigating mindsets: from possible to plausible
to actual, and from possible to probable to actual. While the first bases its
action on the coherence of ideas, the second one bases its action on the
regularity of experience.
The tester navigating through plausible worlds operates under conditions of
novelty and uncertainty. The tester navigating via probable worlds will act
based on objectively verifiable facts from which they derive hypotheses
with a high degree of probability. We need to make room for both
approaches—not just for one.

For testing, this has interesting implications. Imagine that we might have
automated checks that run and provide useful, repeatable feedback. The
states of the system and outcomes verified deterministically are clear. Here,
we are in the probable world. All these checks try to inform us about the
status of the product, given a bedrock of presumed-to-be-safe assumptions.
But how do we arrive at those checks? Perhaps through a process of first
exploring possible and plausible worlds. Is it enough to trust those
automated, deterministic checks? They may be based on an insufficient
variety of data. The assumptions they rely upon may be wrong. Depending
on the complexity of the product and the consequences of failure, such
checks may be inadequate to detect big risks lurking underneath the surface.
So we should complement those checks with a certain amount of
randomized or otherwise creative testing that comes out of plausible
reasoning.
Focus and Roles
Dave Snowden talked about the topic of the fluidity of identity18 in humans
and how we move between roles depending on the context. This switching
of contexts and roles also changes the way we perceive the world by
creating cognitive activation patterns that filter what we see.19
I began to research this aspect and found out, among other things, about the
idea of goal shielding. Goal shielding is about automatically inhibiting the
activation of alternative goals when pursuing a focal goal. If the goal is
highly committed, then the goal shielding is more significant; the greater
the commitment, the stronger the inhibition.20 Furthermore, our ability to
ignore some goals has significant implications for our ability to attain other
goals.21
Goal shielding is another way to say "tunnel vision." So, it seems, that the
chance for a tester to confirm something about the product is greater when
he decides to prove that the product works instead of striving to refute it.
The choice of focal goal infects the behavior that follows.
All this made me recall the idea of conditional probabilities and their roles
in intentional actions, from the work of Alicia Juarrero.22 For example, in
simple terms, when I decide to speak with my boss, the conditional

probability of my talking at the same time with a team member decreases a
lot.
Alicia Juarrero has a beautiful example using catalysis and molecules.
Some molecules are catalysts; this means that molecule A enhances
molecule B's binding, which in turn increases the chance of binding
molecule C, and so on. Since B's condition depends strictly on the presence
of catalyst A, then B's conditional probability of binding differs from its a
priori probability.
When the molecule Z (way down the chain of causality) catalyzes A, the
loop closes and a phase change takes place: a context will be born for those
molecules—a context that will act as a constraint on those cells. This
context/structure contains the molecules and can change the prior
probability of chemical reactions.
For people, intentions can act as such a context. They constrain behavior.
This is how roles work.
This is a deep and important concept because it explains why different
kinds of cognitive patterns are activated based on what we want to do. A
programmer wants to build a product, and a tester wants to find problems
related to the product. Two different intentions exist, each of which comes
with a basin of different probable behaviors.
Emergence
Emergence is about those properties of a system that are not evident, or
perhaps not even predictable, from the constituent elements of that system.
Patrick Hoverstadt, in his book The Grammar of Systems, uses art to
explain this concept. For example, a painting is not about individual lines,
strokes, and colors, but about the overall effect it has on a person.
Hoverstadt also speaks of the concept of POSIWID. This is an acronym for
"the Purpose of a System Is What It Does." He says it is an error to assume
that the purpose of the system is merely what the designer, owner, and the
requirements documents say that it is. We must also look for the emergent
properties.
James Coplien makes another compelling point in connection with
requirements: "Often it is not that the requirement changes, but rather that

our understanding of the requirement changes." Then, after speaking with
him regarding testing and requirements, he told me:23
Corollary:
"Testing is the process of assuring conformance to requirements."
No.
"Testing is also the process of exploring requirements."
Yes.
That "also" is important because it is not just about requirements; it is about
the actual product and, among other things, even the mental models24
associated with the use of the product.
In one Rapid Software Testing Applied class, we tested a product dealing
with uploading and sharing images. When a student discovered a security
vulnerability, James Bach exploited it and discovered something interesting:
the product was primarily being used to illegally share movies. I am sure
that this was not in the requirements. This kind of new usage is a sort of
emergence and might be considered an example of exaptation (i.e., an
adaptive shift in purpose).
Answers
What should programmers know about RST? How is that
different from what they think they know?
Testing begins with embracing uncertainty and ambiguity about the product
being developed. And this development and testing happen in a social and
technical context.
When most programmers think about testing, I believe, they think more
about automated verification and confirmation, which is called in
programming parlance unit testing or automated acceptance tests. In RST,
we call this checking, and I would call them more precisely automated
deterministic confirmatory checks.

But testing is much more. First of all, it is about exploring falsifiable
hypotheses about a product. Along the way, we must encounter the
emergent properties of the product. These may be impossible to automate
with deterministic checks.
There is a slide in the RST classes regarding automated deterministic
verification: Imagine that something exists. Only some of what exists can
be known. What is known can only partially be described in words. Only
some of this description can be translated into true/false propositions.
Afterward, only some of these propositions can be verified in an automatic
way. This is why checking cannot serve all the purposes of testing.
Remember the 3.5 trillion possible patterns that I mentioned previously?
When I hear an overzealous person talking about automation in testing,
insisting that everything can be automated, I ask: "How many of those
patterns have you covered with the automated deterministic checks?"
Max Boisot, when talking about connecting the dots, also highlights another
thing that maybe programmers should understand in the context of testing,
which is the fact that there are three kinds of inferences:
Deductive, which is about certainty and is based on logic
Inductive, which is about probability and is based on repeatable
events
Abductive, which is about plausibility and is based on coherence with
prior experience
Boisot says that these inferences are complementary, not alternatives,25 so
they are meant to be mutually supportive rather than mutually exclusive.
From this perspective, it becomes clear that a variety of automation
methods can be employed to identify issues in software, extending beyond
merely automatic deterministic checks conducted at the unit or GUI level. I
have seen many tools developed by James and demonstrated in his classes
that, for me, serve abductive reasoning.
Programmers should understand that automated checking is only one part—
albeit a useful one—of what testing is. Testing, besides automation, is also
about understanding the following:

The falsifiability/refutability of what they developed
Emergent uses of the product
That not everything can be automated
Finding relevant problems in an infinite space of scenarios
That finding the relevant problems is based on certainty, probability,
and plausibility in how testers draw certain conclusions
Is there an important difference between a programmer and
tester?
Writing code—good code—is hard; it is about what is known and
knowable. And it is hard at the same time to think about the unknowable
and unimaginable26 effects of the code that is written. It would mean
thinking practically at the same time we daydream and tell absurd jokes.
Programmer and tester are two different roles. When you read the word
"role," please recall what I mentioned at the beginning regarding goal
shielding, intentional actions, and how moving between roles, depending on
context, also changes the way we perceive the world.
I would have liked to propose brief descriptions of the programmer and
tester roles, but maybe it is better to list some concepts that relate to their
day-to-day work. Don't get me wrong—there are elements that might
overlap in one way or another, but there are also so many differences in
what needs to be done and how it is done. And above all, they have
differing intentions.
Here are some concepts, constantly used in their work, listed as keywords:
Testers: mission, session-based test management, strategy, risks
(project factors, technology factors, requirements factors, operational
factors), analysis (structure, data, function, interfaces, platform,
operation, timing), product coverage outlines, survey testing, test
techniques (function testing, domain testing, stress testing, flow
testing, scenario testing, claims testing, user testing, risk testing,
automated checking), quality criteria (capability, reliability, usability,
charisma, security, scalability, compatibility, performance,

installability, development), test tools, testability (epistemic, project-
related, value-related, subjective, intrinsic), etc.
Programmers: clean code, software architecture (Clean Architecture,
Data Context Interactions, Hexagonal Architecture, four rules of
simple design), CQRS, Event Sourcing, algorithms (complexity, data
structures, time), code reviews, syntax of programming languages and
frameworks (usually multiple per project), design patterns (cloud,
Gang of Four), SOLID principles, technical debt, performance
(garbage collection, memory allocation, profiling), internal
implementations and workings of frameworks and languages, SQL,
NoSQL, deployments (microservices, platforms, languages), build,
continuous integration, static analysis of code, switching between
technology stacks, security, object-oriented programming, functional
programming, web development, source controls, cloud, unit testing,
mocking frameworks, test doubles, etc.
By outlining these two almost-distinct subsets, my aim is to highlight the
divergence in the responsibilities and focusing ideas of these two roles.
But there is more. In the question, we find the word "mindset," which, as
stated by Boisot, is the embedded epistemology that an agent employs to
navigate a path from possible to actual worlds. For me, if I may say so in
simple terms, the programmer is trying to arrive at the actual worlds from
the possible worlds via the probable worlds. And the tester is trying to
arrive at the actual worlds also from the possible worlds via the plausible
worlds. Some might say that is enough for one path, but they forget that not
all problems can be discovered in a mechanistic manner, where the
relationship between cause and effect can be identified a priori.
Why do managers push back against the testers trying to be
careful and responsible about their work?
We know that there are known knowns and known unknowns. But it is not
easy to manage unknown unknowns.27 As managers, "we should be wary of
those promising 'certainty.'"28 Certainty is a very enticing idea—but not so
helpful when managers must handle the unimaginable and unknowable
unknowns. It's important to acknowledge that software product

development is happening in a sociotechnical context, which is unfolding in
a complex adaptive system.
I think there could be various concrete ideas that can be considered to better
understand the "pushback":
In IT we have two important directions regarding testing: Factory Style
and Context-Driven Testing (formed by such methodologies as RST,
Robert Sabourin's Just-in-Time testing methodology, and Cem Kaner's
Black Box Software Testing methodology). The popular approach is
factory-style. But this dominant position completely ignores important
concepts. One important concept is that of risk. Managers understand
this subject very well, and testers can and should make it their central
point. But, it seems to me, this doesn't happen often. In a conversation
with Huib Schoots, an RST trainer, he shared with me something
seemingly simple yet profoundly insightful: "Going faster means more
risk. How many risks does a manager want to have? Testers can 'win
time' by pointing out the remaining risks when management thinks the
software is ready to ship." I said "profoundly" because he was telling
me that we have forgotten, as Paul Cilliers says,29 "the importance of
certain slowness." That kind of slowness, which could better be
translated as "reflective," not "slow," should allow us to cope better
with the complex world.
Thinking that the project context is linear to manage, when it is not
linear at all. I mean, just consider the implications of the fact that data,
information, and knowledge are three separate things. Additionally,
there are three types of inferences (deductive, inductive, and
abductive). And that we deal with tacit and explicit knowledge. Most
importantly, think about emergence. Testers also must observe
emergent phenomena, and for this, they need time for reflection. Bugs
can be an emergent "property" of a software product.
The importance of interactional expertise is ignored. We must know
what we manage, but not at the level of contributory expertise.30
Unfortunately, many testers in the industry are not helping, either.
They uncritically accept and follow unhelpful testing folklore, which
then leads to poor testing that they can't explain or justify.

I deeply hope that, in time, managers will understand that finding problems
—those relevant patterns from all the trillions of possible patterns—requires
a more deeply responsible, diligent approach. At the same time, I deeply
hope that testers will know how to accommodate uncertainty, risks,
complexity, novelty, and praxis in their work.
What about the trendy idea that "we do not need testers;
programmers will do all the testing"?
These two roles, professional (context-driven) tester and professional
programmer, are difficult to master. Years and years of preparation are
needed for each of them. And switching contexts is not easy, either. These
are two different intellectual disciplines. I hope I will see more
programmers learn about testing. But if the idea in the quote is inviting us
to collapse two separate disciplines into one, this is dangerous.
If the question is a subtle invitation for professional programmers to learn
how to be professional testers as well, or at least to support testers31 in a
team, then it is an interesting challenge. But, as I said, we are speaking
about two different disciplines with different intents.
Therefore, when someone says they will not need the testers, I might dare to
interpret it as:
It's okay to ignore an entire intellectual discipline dedicated to finding
relevant problems for people who matter.
It's easy for a programmer to critique and do their best to refute their
own work.
Everything we need to know about a system can be verified
exclusively in an explicit, deterministic, and automated way.
These are foolish ideas.
It is easy to think that things are simple and that we will always find an
obvious relation between causes and effects, in finding problems regarding
the product, in a timely manner, and that all this will happen while both
concentrating on building the product and while experiencing,
experimenting, and exploring it in all relevant ways to find deep problems
for someone who matters.

But it's very hard to do all this at once. In practice, no one does.
Notes
1   Bolton, M. (n.d.) Interview with Michael Bolton. [YouTube video]
Available at: https://www.youtube.com/watch?v=RNzF3Do8ZAE
[Accessed 10 April 2025].
2   Frâncu, M. (n.d.) Interview with Marius Frâncu. [YouTube video]
Available at: https://www.youtube.com/watch?v=9PJF26jO-JU
[Accessed 10 April 2025].
3   Steinhardt, N. (2021) The Diary of Happiness. Lulu.com.
4   Snowden, D. (n.d.) "Of practical wisdom." Available at:
https://thecynefin.co/of-practical-wisdom (Accessed: 14 April
2025).
5   Frâncu, M. (n.d.) "Inattentional blindness and test scripts." Available at:
https://sensing-ontologies.com/inattentional-blindness-and-
test-scripts (Accessed: 10 April 2025).
6   When using the word "ontology," I had this article in mind: Snowden, D.
(2005) Multi-ontology sense making: a new simplicity in decision
making. Available at:
https://www.researchgate.net/publication/7793823_Multi-
Ontology_Sense_Making_A_New_Simplicity_in_Decision_Making
(Accessed: 10 April 2025).
7   Bolton, M. (2014) "The Sock Puppets of Formal Testing." Available at:
https://developsense.com/blog/2014/07/sock-puppets-of-formal-
testing (Accessed: 10 April 2025).
8   In RST, we know deeply about the research performed by Slovic and
others regarding risks and feelings: Slovic, P. (2000) The perception of
risk. Available at:
https://www.researchgate.net/publication/325954197_The_percep
tion_of_risk (Accessed: 10 April 2025).

9   Frâncu, M. (n.d.) Tacit knowledge, testing, requirements, management.
Available at: https://sensing-ontologies.com/tacit-knowledge-
testing-requirements-management (Accessed: 10 April 2025).
10 Lakomski, G. (2005) Managing without Leadership: Towards a Theory
of Organizational Functioning. Oxford: Elsevier.
11 Bolton, M. (2007) "McLuhan for Testers.' Better Software Magazine.
Available at: https://www.developsense.com/articles/2007-09-
McLuhanForTesters.pdf (Accessed: 10 April 2025).
12 Boisot, M. and MacMillan, I.C. (2004) "Crossing epistemological
boundaries: managerial and entrepreneurial approaches to knowledge
management," Long Range Planning, 37(6), pp. 505-524. Available at:
https://cspo.org/wp-content/uploads/2014/11/read_Boisot-
MacMillan-Crossing-Epistemological-Boundaries.pdf (Accessed:
10 April 2025).
13 Frâncu, M. (2020) "What is testing for me." Sensing Ontologies.
Available at: https://sensing-ontologies.com/what-is-testing-
for-me (Accessed: 10 April 2025).
14 Bach, J. (2013) "A-Galumphing We Go." [Video] Ministry of Testing.
Available at: https://www.ministryoftesting.com/testbash-
sessions/a-galumphing-we-go-james-bach (Accessed: 10 April 2025).
15 Audi, R. (ed.) (2015) The Cambridge Dictionary of Philosophy. 3rd edn.
Cambridge: Cambridge University Press.
16 Schoots, H. (n.d.) "Product Coverage Outlines." [YouTube video]
Available at: https://www.youtube.com/watch?v=NUojNfDjIjw
(Accessed: 10 April 2025).
17 There is an entire book dedicated to the idea of "coherence" called
Coherence in Thought and Action, by Paul Thagard (2000, Cambridge,
MA, MIT Press).
18 Snowden, D. (n.d.) "Identity." Available at:
https://thecynefin.co/identity-2(Accessed: 10 April 2025).

19 Snowden, D. (n.d.) "Three or five?" Available at:
https://thecynefin.co/three-or-five (Accessed: 10 April 2025).
20 Jia, L., Yu, R., Hirt, E.R. and Fishbach, A. (2016) "Motivational tuning
in response to ego depletion." Available at:
https://www.researchgate.net/publication/306081612_Motivation
al_Tuning_in_Response_to_Ego_Depletion (Accessed: 14 April 2025).
21 Shah, J.Y. and Kruglanski, A.W. (2002) "Priming against your will: How
accessible alternatives affect goal pursuit," Journal of Experimental
Social Psychology. Available at:
https://www.academia.edu/7598698/Priming_against_your_will_Ho
w_accessible_alternatives_affect_goal_pursuit (Accessed: 10
April 2025).
22 Juarrero, A. (1999) "The self-organization of intentional action,"
Philosophy of the Social Sciences, 29(3). Available at:
https://www.jstor.org/stable/23955624 (Accessed: 10 April 2025).
23 Coplien, J. (2025) Personal conversation with the author.
24 In my usage of "mental model," I am guided by Gary Klein's perspective
on the matter. For example, see
https://www.shadowboxtraining.com/courses/masterclass-in-
practical-decision-making
25 Boisot, M. (2014) "Connecting the Dots Before the World Does."
ESADE & The I-Space Institute. Available at:
https://www.slideserve.com/taji/connecting-the-dots-before-
the-world-does (Accessed: 10 April 2025).
26 Snowden, D. (n.d.) "The uncertainty matrices." Available at:
https://thecynefin.co/the-uncertainty-matrices (Accessed: 10
April 2025).
27 Wikipedia contributors (n.d.) "There are unknown unknowns."
Wikipedia. Available at:
https://en.wikipedia.org/wiki/There_are_unknown_unknowns
(Accessed: 10 April 2025).

28 Blignaut, S. (2021) "Letting go of certainty." Being Human Podcast,
episode 165. [YouTube video] Available at:
https://www.youtube.com/watch?v=ifrTV1BDgjk (Accessed: 10 April
2025).
29 Cilliers, P. (2006) "On the importance of a certain slowness,"
Emergence: Complexity and Organization, 8(3). Available at:
https://geldofcs.nl/wp-
content/uploads/2017/04/The_importance_of_a_certain_slowness.
pdf (Accessed: 10 April 2025).
30 Collins, H. and Evans, R. (2007) Rethinking Expertise. Chicago:
University of Chicago Press. Available at:
https://press.uchicago.edu/ucp/books/book/chicago/R/bo5485769
.html (Accessed: 10 April 2025).
31 Bach, J. (2012) "RST Methodology: 'Responsible Tester'" Satisfice
Blog. Available at: https://www.satisfice.com/blog/archives/1364
(Accessed: 10 April 2025).
OceanofPDF.com

CHAPTER 22
Becoming an RST Instructor
—By Huib Schoots
Because testing (and any engineering activity) is a solution to a very
difficult problem, it must be tailored to the context of the project;
and therefore, testing is a human activity that requires a great deal
of skill to do well. That's why we must study it seriously. We must
practice our craft. Context-driven testers strive to become the Jedi
Knights of testing.
—James Bach1
In this chapter, I will tell my story of how I became an RST instructor and
why I chose to work and think in a context-driven way. I will discuss the
various challenges I encountered that made me search for a different way of
working and move to Rapid Software Testing.
From Development to Management to
Training
After I finished my Business Informatics degree in 1996, I joined a big
consultancy company where I did a master class to become a developer. I
loved doing IT projects and solving problems, but I didn't really enjoy
programming in FoxPro. To me, it felt like staring at a screen all day.
Communication and collaboration with other people gave me more energy
than working alone. After every release we installed during the weekend,
we would spend a week fixing the bugs that popped up. On another project,
I distributed the software on floppy disks, and every serious bug forced us
to distribute a new release. I quickly got sick and tired of copying floppies.
I started to look for ways to improve the quality of the software we were
building. In doing so, I ran into testing. As developers, we did testing, but as

I look back on what we really did, I would rather call that "trying" now. My
first steps down the testing path were in a happy-go-lucky way. I didn't
know much about testing, and I relied more on common sense.
On that first project, I had a role as a test automation engineer. I also did
what we called manual testing.2 I learned about "structured" testing
methodologies, although they did not help me much. The aspect of testing
where I found my passion was finding things out—learning about how
people and software work.
At the company I worked for in 2002, it was normal that people grew into
project management. I tried that as well. Although I enjoy organizing and
making projects work, I missed testing. I was always paying too much
attention to the testing in my projects, so I went back to testing and test
management. Several years later, I joined a test consultancy company and
became a unit manager. Because this wasn't a full-time equivalent position,
I also provided training and consulting. Here, I found another passion:
training people, helping teams and companies to improve their testing, and
coaching the people in my unit.
Years of Trying to Be Normal
I became a trainer for ISTQB courses, which are popular in Europe. While
teaching the same class and telling the same stories over and over again, I
got the chance to really think about the subject matter. I remember one day
when I drove home, thinking about what we did in class. I had been
teaching a three-day class where we never actually tested anything. We only
talked about testing and did some exercises—oversimplified exercises, like
applying a test-specification technique on an oversimplified example
written out on paper. The training did not address the testing challenges we
actually face in projects: How do we choose what to test? When do you stop
testing? What do you report to the stakeholders?
Trying to teach testing transformed how I felt about my own experience and
knowledge. The common folklore about testing seemed hollow. I found
myself struggling to explain the testing I did, while the popular ways of
talking about testing didn't seem to match my experiences. I started
doubting what I had been taught—and, even worse, I started doubting the

testing I was teaching others. Why weren't we testing actual software in
these classes?
As a manager, I was mentoring the young professionals in my unit. They all
had the same kind of questions: How do I use the stuff I am learning from
you in my daily work? How do I know what to test? How do I decide I have
tested enough?
From 2007 to 2010, I was the manager of a team of enthusiastic, smart
young people. Most of them joined our company directly from university
and were trained in beginner classes for a couple of weeks to become
testers. Ironically, these classes were called master classes. These were
intensive training weeks with lots of exercises and practice. After these
master classes, the testers went to work for clients on projects. As a coach
and manager, I observed what they and others were working on in their
projects. It occurred to me that they were filling templates, trying to apply
standards, and often using a happy-go-lucky approach.
I started wondering why they were not using the test design techniques or
risk analysis from the training. Was it too difficult to do?
At the start of a project, they filled in a test plan template and started
creating test cases from requirements and design documentation. The
strategy in their work was vague and often completely tacit. They claimed
they were using their "common sense" to guide their testing, but were not
able to explain how they worked, nor how the techniques they used were
working. As a coach, I couldn't really help them. I was struggling myself
and didn't know a different approach. In my head, the steps to take were
often clear, but the things I came up with didn't fit the standard processes
and ways of working provided by the testing methodologies I knew. I
started using terms such as pragmatic testing. More and more, I was
stepping away from templates and formal testing documentation like Master
Test Plans and physical test cases.
I began to realize that there is a big problem in our industry.
At least in the Netherlands, testing has a bad name in many organizations
and is often undervalued. Testing is seen as just checking requirements or
pushing buttons. I heard many people claim that anybody can test, and I

saw companies where testing was used to train new people in IT. In the
years after the Internet bubble, people used testing to get back into IT.
Many interviewers and managers do not know how to recognize the
difference between truly competent testers and people who are faking it.
Do you remember your last job interview? Did you have to do actual
testing, or were they only asking simple test questions? In the movie
business, even actors like Brad Pitt must audition for a role. Why don't
testers audition?
I Found Context-Driven Testing
On foreign blogs, I started reading about Agile and context-driven testing. I
found there is much more to testing than I had guessed. I learned new,
exciting, and interesting aspects that I hadn't considered before, like critical
thinking, models, and heuristics.
In 2006, I heard about Lessons Learned in Software Testing: A Context-
Driven Approach, by Cem Kaner, James Bach, and Bret Pettichord. I
remember seeing one of the one-star reviews when I ordered the book from
Amazon: "This book is VERY dangerous to a serious testing organization
because it focuses on minimal documentation." I thought, this is a book I
have to read!
I learned many new things, but I was also confused. Looking back now, I
don't think I fully understood what the book was teaching me.
Epistemology, systems thinking, conjecture and refutation, and biases—
many of the concepts in the book didn't gel for me at first. Heuristics
intrigued me, although I did not immediately grasp their importance.
But my view on testing—"proving software works" and "assuring
quality"—started to change.
Some of the lessons I read were mind-blowing. The authors connected dots
in my brain. One insight I remember is from "Lesson 34: 'It works' really
means it appears to meet some requirement to some degree," which made
me realize that there is no such thing as proving software works. The
lessons on testability—"Lesson 136: Testability is often a better investment
than automation," "Lesson 137: Testability is visibility and control,"

"Lesson 156: Programmers like to help with testability," and "Lesson 169:
Ask for testability features"—gave me a totally different notion of the
importance of making testing easier and faster. Those lessons were very
influential on my testing after reading them. My two favorite lessons from
the book are: "Lesson 283: Apply diverse half-measures," which taught me
that we need to use a diversified strategy, and "Lesson 272: "If you can get
a black belt in only two weeks, avoid fights," which taught me that many
skills need practice for a tester to gain enough skill not to be a danger to
themselves.
In 2008, I saw Michael Bolton's talk, "Two Futures of Software Testing." I
asked my manager if I could attend a Rapid Software Testing class by
James Bach or Michael Bolton, a few weeks after the conference. I was
kindly refused because the content of the class was "controversial," since it
was teaching things "neither our customers nor my colleagues and I
needed." So I kept on reading the stuff online.
In 2010, I switched jobs and joined Rabobank as a team manager. Testing
was not highly appreciated there either, and my team was struggling with
the same old issues. I felt it was time to take action. At a Dutch testing
conference later that year, I met several people who were discussing their
issues with testing and wanting to do something new and different. We all
wanted to talk with like-minded people about a different way of testing.
Challenged by James Bach in a blog post where he called on Dutch and
German testers to follow the example of the Swedish context-driven testers,
we started to discuss a get-together over Twitter. Thus, DEWT (Dutch
Exploratory Workshop on Testing) was born3 at my kitchen table. DEWT
was a major driver toward context-driven testing for me. Discussing testing
with people who had taken an RST class made it clear that this was the
approach I wanted to learn more about.
In the testing expertise group at the bank, I suggested trying something
different. We decided to ask James Bach and Michael Bolton to teach us
RST. Several months later, almost 50 testers took RST in two groups. I was
blown away by the content and exercises (see my blog posts about it here4
and here5). I learned so much, and many dots in my mind were connected.
After a week of RST at the bank, Michael joined the first DEWT peer
conference6 during the weekend right after the RST classes. I was lucky to

spend a week with Michael, and we had great conversations. That week, I
asked Michael if RST could be taught by people in a different language. We
talked about that idea, and a seed was planted in my brain. After a week full
of RST and DEWT, I was so inspired. My mind was overflowing with
ideas. A year later, we invited Michael back to the bank to do exploratory
testing workshops. Again, I was able to spend a lot of time with him and got
the opportunity to observe his way of working. I especially enjoyed the
experiential approach in his workshops and his provoking questions to
trigger people's thinking.
The classes and conversations with Michael inspired me to start blogging
and speaking at conferences to share my ideas. I found that blogging and
writing articles helped me to get my thoughts together. I started many blogs
without ever finishing them. I realized I needed to learn more about the
topic to make my writing worthwhile.
In 2011, I was lucky that one of the speakers at Agile Testing Days fell ill,
so I was given the opportunity to present my "So You Think You Can Test"
talk7 (see my blog post on this topic here8). My first international
conference talk was nerve-racking, but it gave me an enormous energy
boost. I talked about context, collaboration, skills, and the importance of
practice. My talk, surprisingly, inspired other people! Since then, I have
attended peer conferences more regularly.
Over the years, I met many awesome testers, and I got the chance to learn
from them by working with them and having long discussions at various
meetups, conferences, and via Skype. It gave me the chance to regularly
talk to many like-minded colleagues about context-driven testing.
Especially peer conferences, like DEWT, are a great way to go deep on
topics and to learn from colleagues' experiences and stories.
Instructor in the Making
In 2012, I had dinner with James, and we discussed RST classes in the
Netherlands. He mentioned that I could become one of the RST trainers, but
that there would be a long way to go to get that far. This made me very
happy, and I realized that it was now up to me to convince Michael and
James that I was capable of teaching RST.

The same week, James and I started with some of the RST exercises.
Talking about RST in class is important, but doing the exercises right is the
most important part, since the class is experiential. I needed to learn to be
less impulsively helpful and to hold back so students could experience the
traps set in the exercises for them to learn. One of the biggest worries of
James and Michael was that I would violate the RST namespace,9 so it
might appear that we have contradictory ideas about what was taught. I
needed to stay consistent with the RST principles and namespace at all
times. This took quite a lot of practice and experimenting to find out which
way would fit me as a person!
But learning to teach RST is not about memorization. It's more about
understanding the concepts to the point of being able to critique them and
remix them. Then I need to turn any experience of testing into an exercise
for students—watching them work, responding to them, and challenging
them as they go. Michael and James encouraged me to develop my own
style, my own exercises, and my own slides. By checking back with them
on a regular basis, I developed my approach step by step with their help,
and I earned their trust to a level where they let me be an instructor.
This is a big reason why there are only a handful of RST instructors. The
path to qualification is complex.
With James, I did a couple of coaching sessions via instant messaging. With
tools like Zoom or Teams, it is easy to work with testers all over the world.
Coaching while typing instead of talking forces you to carefully think about
what you are doing and what you are trying to say.
With James as my demanding coach, I realized how important it is to be
precise in what you say. What others dismiss as semantics can be very
important to deeply understand each other. A ninety-minute session with
James resulted in my studying for some days to fully understand what was
said. I also did a few coaching sessions with Ilari Hendrik Aegerter before I
started to coach people myself.
This was very rewarding, as I got the chance to help testers who are serious
about their craft. I also got the chance to deeply think about RST-related
topics and experiment with them while working with my students.

James likes to question and challenge people. In the three years I was
becoming an RST instructor, whenever James was in the Netherlands or
Belgium, we spent a lot of time together so that I could learn as much as
possible.
On those occasions, he asked me to explain exercises or concepts from
RST. The better my answers were, the more he pushed me by asking
difficult questions. I enjoyed working this way, because it helped me
prepare for difficult questions in classes, but also to talk better about testing
in real life. On one occasion, when I was driving James from his hotel to an
evening session at one of his clients, we were in heavy traffic in rush hour
in the pouring rain, while James was sitting next to me, increasing the
pressure with every answer I gave. The better my answers were, the more
he would challenge them. After 20 minutes, I asked if we could switch
topics, since I really had to focus on the road.
The Key to Mastery Is Deliberate Practice
I was fortunate to manage or consult back then. I also participated in
projects while working as a tester for clients. These projects gave me many
opportunities to try out and experiment with heuristics, session-based
testing, and coverage outlines that I took from RST.
That work also gave me lots of examples to share in class: sharing
experiences instead of appealing to authority is a context-driven way of
working and a core principle in RST. I also started doing workshops where
I used topics from RST—at clients where I was coaching people or teams,
in workshops for clients, and in workshops at conferences. Each time, I
took just a small part, like test strategy, testing story, or oracles to focus on.
These workshops were opportunities to practice explaining the concepts of
RST and to experiment with various exercises. I always prepare for
workshops by practicing and creating examples by myself. The interesting
part of workshops at conferences is that, on most occasions, experienced
Context-Driven testers were in the room. Their feedback and critical
questions were very helpful.
Observing testers at work is instructive. The same goes for observing
students doing exercises in class, testing software, or otherwise practicing

their testing skills. Teaching a variety of workshops gave me the chance to
observe people working and practice giving feedback and running debriefs.
RST has many exercises, and it would require a lot of time with Michael or
James to train me in all of them. To get the necessary practice and feedback
from James or Michael, I started recording myself while I practiced RST
exercises with my colleagues, then sent the recordings to James and
Michael for feedback. Sometimes we went through these recordings minute
by minute to discuss what was happening.
The detailed feedback from James and Michael helped me to deeply
understand the exercises and improve my teaching skills. I invested a lot of
effort to learn the experiential and Socratic approaches. The videos helped
me learn about planted traps and possible scenarios, giving me insight into
the learning opportunities in each exercise. Getting feedback from two
totally different people with different styles gave me insights into many
possible ways of working and teaching.
Sometimes feedback can be hard to receive. Working with both Michael
and James, though difficult and demanding, was nonetheless very
rewarding. I learned so much and became a better tester, trainer, coach, and
person. I got the chance to learn many skills, and I became better at
receiving feedback without feeling attacked.
A great example was the last class I co-trained with James in early 2015.
While doing an exercise, I made a mistake, and James stopped me and
started giving me feedback. I realized that I was enjoying his feedback
despite the fact that the students were watching me. When we were back on
track, I explained to the class that I enjoyed what had just happened. To
become truly excellent at something, details matter, and honest feedback is
a key to learning.
Before I became an RST instructor, I participated in eight RST classes with
both James and Michael. In the first class, I was a student myself. In the
next class, I was involved as the organizer for my employer, but I also
attended and learned by actively participating. I grew into a more active
role by participating in the class as a coach during exercises. In the last
classes, I was a co-trainer and taught parts of the class, mostly by running
exercises.

Taking the same class more than once gave me the opportunity to deeply
understand the RST mindset and skillset. Each class gave me new insights.
For me, it was like slowly digging through different layers of the earth,
toward the core. Each class revealed new layers, new things to learn, and
exciting opportunities to explore. James and Michael keep developing the
RST material to keep it fresh and interesting, but more importantly, there is
so much to learn: theory, testing skills, and instructor skills to run the class
successfully. Also, taking the class from different instructors is very
insightful: Michael and James have very different ways of teaching and
telling stories. Although the core is the same, their classes are very
different. In the last few years, I have worked with another RST instructor,
Paul Holland, on talks and workshops for conferences. His style is yet again
different from the others. That makes working with him very insightful as
well.
When I started this journey, I thought it would be easier. It was a lot of hard
work, a struggle sometimes, but it was worth it! It was a journey in which I
learned a lot about myself and the testing craft. I have taught over thirty
classes, and I'm looking forward to teaching many more.
Why I Prefer the Context-Driven Approach to
Testing
I do not believe in best practices, nor in standards, when it comes to jobs
that require creative and independent thinking. A standard for a product
(like a charger cable for your phone) is useful. Standards for testing are silly
and dangerous. The problem with testing standards is that, since they are
created by many people around the world, if many people work on a
standard or methodology, it becomes an average—something everybody
must agree with.
Everyone who works on standards wants their own thing in there. The
standard has to fit every context easily. In the Netherlands, we call that a
polder model:10 consensus rules and nobody gets what they really want.
These standards are made for an average situation or project.
Is there such a thing as an average project? I think not! Projects are unique,
unpredictable, often complex, and dependent on humans. Skills are

absolutely required to deal with difficult, complex, and ambiguous
problems.
Within context-driven testing, people are the most important part of
projects. Skills matter, and they are trained by Context-Driven testers.
Context-driven testing promotes skepticism, thinking, and problem-solving
instead of merely applying standards.
The seven basic principles of the Context-Driven School11 tell us that
people, working together, are the most important part of any project's
context. Good software testing is a challenging intellectual process, and
only through judgment and skill, exercised cooperatively throughout the
entire project, are we able to do the right things at the right times to
effectively test our products.
Studying social science to understand RST gave me a better understanding
about people, learning, thinking, mindset, and biases. Reading books like
Thinking, Fast and Slow by Daniel Kahneman and Tacit & Explicit
Knowledge by Harry Collins helped me understand how the human brain
works, including how people make judgments and come to conclusions. I
learned about qualitative research. This gave me more knowledge about
dealing with ambiguity, situation-specific results, and partial answers.
Context-driven testing also brought me ways to cope with the difficulties I
struggled with years ago. Exploratory testing made testing more fun, and
heuristics help me to guide and structure my testing to be able to do fast,
cost-effective testing. My improved critical thinking helps when
corroborating the theories I have about the product. I try to prove myself
wrong instead of proving myself right.
Context-driven testing appeals to me because it puts the tester's mind at the
center of testing instead of documents, processes, and standards. I wrote a
series of five blog posts on what testing can learn from social science.12
Context-driven testing made my testing more personal. Not doing the stuff
everybody else is doing encouraged me to develop my own style. RST is a
mindset, a paradigm, and a culture that informs not only what I do, but also
who I am!
Rapid Software Testing is a context-driven methodology that gave me a
practical way to implement the context-driven way of working in my daily

work.
When I look at people working in IT, many are doing a nine-to-five job that
does not involve learning, but just applying stuff others tell them to do.
They take bullshit from managers, project managers, or product owners
who do not know anything about testing but still tell them what to do, like
writing or counting test cases, creating worthless metrics, and writing bulky
documents. They are happy to do what they believe are best practices. That
is not for me.
I am a passionate person, and I always want to be the best or do the best
work possible. In fact, I refuse to do bad work.13 I have high standards, and
ethics are important to me. Context-driven testing and RST give me all that
and more. The community is awesome. We share and learn together, we
challenge each other, and we practice continuous learning.
Notes
1   Bach, J. (2020) The Dual Nature of Context-Driven Testing. Available
at: https://www.satisfice.com/blog/archives/5215 (Accessed: 25
June 2025).
2   Bolton, M. (2017) The End of Manual Testing. Available at:
https://developsense.com/blog/2017/11/the-end-of-manual-
testing (Accessed: 25 June 2025).
3   DEWT (2011) DEWT is born. Available at:
https://dewt.wordpress.com/2011/01/16/dewt-is-born (Accessed:
25 June 2025).
4   Schoots, H. (n.d.) RST - Rapid Software Testing. Available at:
https://www.huibschoots.nl/wordpress/?p=86 (Accessed: 25 June
2025).
5   Schoots, H. (n.d.) RST - Part 2. Available at:
https://www.huibschoots.nl/wordpress/?p=205 (Accessed: 25 June
2025).
6   Lindström, M. (2011) What happened at DEWT1 doesn't just stay at
DEWT1. Available at:

https://testsidestory.com/2011/06/21/dewt1 (Accessed: 25 June
2025).
7   Schoots, H. (2011) So you think you can test (slides). Available at:
https://www.huibschoots.nl/downloads/Soyouthinkyoucantest%20-
%20Agile%20Testing%20Days%202011.pdf (Accessed: 25 June 2025).
8   Schoots, H. (2011) Preparing a talk: so you think you can test? Available
at: https://www.huibschoots.nl/wordpress/?p=274 (Accessed: 25
June 2025).
9   Bolton, M. (2015) The Rapid Software Testing namespace. Available at:
https://developsense.com/blog/2015/02/the-rapid-software-
testing-namespace (Accessed: 25 June 2025).
10 Wikipedia (n.d.) Polder model. Available at:
https://en.wikipedia.org/wiki/Polder_model (Accessed: 25 June
2025).
11 Context-Driven Testing (n.d.) Available at: https://context-driven-
testing.com (Accessed: 26 June 2025).
12 Schoots, H. (n.d.) What testing can learn from social science. Available
at: https://www.huibschoots.nl/wordpress/?p=1173 (Accessed: 26
June 2025).
13 Schoots, H. (n.d.) Refusing to do bad work. Available at:
https://www.huibschoots.nl/wordpress/?p=1689 (Accessed: 26 June
2025).
OceanofPDF.com

Part V
Appendices
In This Part
Appendix A: Roadmap of Testing
Appendix B: The Heuristic Test Strategy Model
Appendix C: Heuristics of Testability
Appendix D: Elements of Excellent Testing
OceanofPDF.com

APPENDIX A
Roadmap of Testing
This diagram represents a narrative model of testing as we think about it in
the Rapid Software Testing methodology. What we mean by a narrative
model is a verbal description that has been put into a graphical form to
show how the various elements affect each other.
This is a tool for explaining testing (what we call "test framing" in Chapter
5) and placing all the elements of testing into a business context. As an
example, consider coverage, which is at the very bottom. What's the point
of coverage, according to this model? Here is the answer in the form of a
long sentence that traces from the bottom of the diagram to the top:
"Coverage is the part of the product you examine and manipulate in a test,
in accordance with your test procedure, which governs the performing of
experiments that produce test results that become part of the testing story
that you report, which is part of an activity governed by your test strategy,
which fulfills the mission of testing in the service of the people who matter,
all of whom and all of which exist in some context into which you must fit
your testing."
Hence, this diagram is saying that testing without coverage is purely
hypothetical, while coverage without the rest of testing tells the business
nothing about risk.


OceanofPDF.com

APPENDIX B
The Heuristic Test Strategy Model1
The Heuristic Test Strategy Model is a set of patterns for designing and
choosing tests to perform. The immediate purpose of this model is to
remind testers of what to think about during that process. I encourage
testers to customize it to fit their own organizations and contexts.
Project Environment represents a set of context factors that include
resources, constraints, and other elements in the project that may
enable or hobble our testing. Sometimes a tester must challenge
constraints and sometimes accept them.
Product Elements are aspects of the product that you consider testing,
including aspects intrinsic to the product and relationships between the
product and things outside it. Software is complex and invisible. Take
care to cover all of it that matters, not just the parts that are easy to see.

Quality Criteria categories are dimensions in which people
determine the value of the product. You can also think of them as
categories of product risk. Quality criteria are subjective and
multidimensional, and often hidden or contradictory.
General Test Techniques are families of heuristics for designing tests.
How, where, and when to apply a particular technique requires an
analysis of the project environment, product elements, and quality
criteria.
The Testing and Quality Story is the result of testing. You can never
know the "actual" quality of a software product—you can't "verify"
quality, as such—but by performing tests, you can make an
assessment, and that takes the form of a story you tell (including bugs,
curios, etc.).
General Test Techniques
A test technique is a heuristic for designing tests. There are many
interesting techniques. The list includes nine families of general techniques.
By "general technique," we mean that the technique is simple and universal
enough to apply to a wide variety of contexts. Many specific techniques are
based on one or more of these families. An endless variety of specific test
techniques may be constructed by combining one or more general
techniques with coverage ideas from the other lists in this model.
Function Testing
Claims Testing
Challenge every claim:
1. Identify things that the product
can do (functions and sub-
functions).
2. Determine how you'd know if a
function is capable of working.
3. Test each function, one at a time.
4. See that each function does what
it's supposed to do and not what
Test what it can do:
1. Identify reference materials
that include claims about the
product (tacit or explicit).
Consider SLAs, EULAs,
advertisements, specifications,
help text, manuals, etc.
2. Analyze individual claims and
clarify vague claims.

Function Testing
Claims Testing
it isn't supposed to do.
3. Test each claim about the
product.
4. If you're testing from an
explicit specification, expect it
and the product to be brought
into alignment.
Domain Testing
User Testing
Partition the data:
1. Look for any data processed by
the product. Look at outputs as
well as inputs.
2. Decide which particular data to
test with. Consider things like
boundary values, typical values,
convenient values, invalid
values, or best representatives.
3. Consider combinations of data
that are worth testing together.
4. Consider using inputs that force
the whole range of possible
outputs to occur.
Involve the users:
1. Identify categories and roles
of users.
2. Determine what each category
of user will do (use cases),
how they will do it, and what
they value.
3. Get real user data, logs based
on user activity, or bring real
users in to test.
4. Otherwise, systematically
simulate a user (be careful—
it's easy to think you're like a
user, even when you're not).
5. Powerful user testing is that
which involves a variety of
users and user roles, not just
one.
Stress Testing
Risk Testing
Overwhelm the product:
1. Look for subsystems and
functions that are vulnerable to
Imagine a problem, then look for it:
1. What kinds of problems could
the product have?

Function Testing
Claims Testing
being overloaded or "broken" in
the presence of challenging data
or constrained resources.
2. Identify data and resources
related to those subsystems and
functions.
3. Select or generate challenging
data or resource constraint
conditions to test with, e.g., large
or complex data structures, high
loads, long test runs, many test
cases, low memory conditions.
2. Which kinds matter most?
Focus on those.
3. How would you detect them if
they were there?
4. Make a list of interesting
problems and design tests
specifically to reveal them.
5. It may help to consult experts,
design documentation, past
bug reports, or to apply risk
heuristics.
Flow Testing
Tool-Supported Testing
Do one thing after another:
1. Perform multiple activities
connected end to end; for
instance, conduct tours through a
state model.
2. Don't reset the system between
actions.
3. Vary timing and sequencing, and
try parallel threads.
Use tools to make testers more
powerful:
1. Look for or develop tools that
can perform a lot of actions
and check a lot of things.
2. Consider tools that partially
automate test coverage.
3. Consider tools that partially
automate oracles.
4. Consider automatic change
detectors.
5. Consider automatic test data
generators.
Scenario Testing
Test to a compelling story:

Function Testing
Claims Testing
1. Begin by thinking about
everything going on around the
product.
2. Design tests that involve
meaningful and complex
interactions with the product.
3. A good scenario test is a
compelling story of how
someone who matters might do
something that matters with the
product.
Project Environment
Creating and executing tests is the heart of the test project. However, there
are many factors in the project environment that are critical to your decision
about what specific tests to create. In each of the following categories,
consider how that element may help or hinder your test design process. Try
to exploit every resource.
Mission: Your purpose on this project, as understood by you and your
customers
Why are you testing? Are you motivated by a general concern about
quality or by specific and defined risks?
Do you know who the customers of your work are? Whose opinions
matter? Who benefits or suffers from the work you do?
Maybe the people you serve have strong ideas about what tests you
should create and run. Find out.
Have you negotiated project conditions that affect your ability to
accept your mission?

Information: Information about the product or project that is needed for
testing
Whom can we consult with to learn more about this project?
Are there any engineering documents available? User manuals? Web-
based materials? Specs? User stories?
Does this product have a history? Old problems that were fixed or
deferred? A pattern of customer complaints?
Is your information current? How are you apprised of new or changing
information?
Are there any comparable products or projects from which we can
glean important information?
Developer Relations: How you get along with the programmers
Rapport: Have you developed a friendly working relationship with the
programmers?
Hubris: Does the development team seem overconfident about any
aspect of the product?
Defensiveness: Is there any part of the product that the developers
seem strangely opposed to having tested?
Feedback loop: Can you communicate quickly, on demand, with the
programmers?
Feedback: What do the developers think of your test strategy?
Test Team: Anyone who will perform or support testing
Do you know who will be testing? Do they have the knowledge and
skills they need?
Are there people not on the "test team" who might be able to help?
People who've tested similar products before and might have advice?
Writers? Users? Programmers?

Are there particular test techniques that someone on the team has a
special skill or motivation to perform?
Who is co-located and who is elsewhere? Will time zones be a
problem?
Equipment & Tools: Hardware, software, or documents required to
administer testing
Hardware: Do you have all the physical or virtual hardware you need
for testing? Do you control it or share it?
Automated checking: Do you have tools that allow you to control and
observe product behavior automatically?
Analytical tools: Do you have tools to create test data, design
scenarios, or analyze and track test results?
Matrices and checklists: Are any documents needed to track or record
the progress of testing?
Signals: Do you have access to engineering data coming back from the
field?
Schedule: The sequence, duration, and synchronization of project events
Test design: How much time do you have? Are there tests that are
better to create later than sooner?
Test execution: When will tests be performed? Are some tests
performed repeatedly, say, for regression purposes?
Development: When will builds be available for testing, features
added, code frozen, etc.?
Documentation: When will the user documentation be available for
review?
Test Items: The product to be tested
Scope: What parts of the product are and are not within the scope of
your testing responsibility?

Availability: Do you have the product to test? Do you have test
platforms available? Will you test in production?
Interoperable systems: Are any third-party services required for your
product that must be mocked or made available?
Volatility: Is the product constantly changing? How will you find out
about changes?
New stuff: Do you know what has recently been changed or added in
the product?
Testability: Is the product functional and reliable enough that you can
effectively test it?
Future releases: What part of your testing, if any, must be designed to
apply to future releases of the product?
Deliverables: The observable products of the test project
Content: What sort of reports will you have to make? Will you share
your working notes or just the end results?
Purpose: Are your deliverables provided as part of the product? Does
anyone else have to run your tests?
Standards: Is there a particular test documentation standard you're
supposed to follow?
Media: How will you record and communicate your reports?
Product Elements
Ultimately, a product is an experience or solution provided to a customer.
Products have many dimensions. Each of the following categories
represents an important and unique element to be considered in the test
strategy.
Structure: Everything that comprises the physical product
Code: The code structures that comprise the product, from executables
to individual routines.

Hardware: Any hardware component that is integral to the product.
Service: Any server or process running independently of others that
may comprise the product.
Non-executable files: Any files other than multimedia or programs,
like text files, sample data, or help files.
Collateral: Anything beyond that which is also part of the product,
such as paper documents, web pages, packaging, license agreements,
etc.
Function: Everything that the product does
Multi-user/social: Any function designed to facilitate interaction
among people or to allow concurrent access to the same resources.
Calculation: Any arithmetic function or arithmetic operations
embedded in other functions.
Time-related: Time-out settings, periodic events, time zones, business
holidays, terms and warranty periods, and chronograph functions.
Security-related: Rights of each class of user, protection of data,
encryption, front-end vs. back-end protections, vulnerabilities in
subsystems.
Transformations: Functions that modify or transform something (e.g.,
setting fonts, inserting clip art, withdrawing money from an account).
Startup/shutdown: Each method and interface for invocation and
initialization as well as exiting the product.
Multimedia: Sounds, bitmaps, videos, or any graphical display
embedded in the product.
Error handling: Any functions that detect and recover from errors,
including all error messages.
Interactions: Any interactions between functions within the product.
Testability: Any functions provided to help test the product, such as
diagnostics, log files, asserts, test menus, etc.

Data: Everything that the product processes and produces
Input/output: Any data that is processed by the product and any data
that results from that processing.
Preset: Any data that is supplied as part of the product or otherwise
built into it, such as prefabricated databases, default values, etc.
Persistent: Any data that is expected to persist over multiple
operations. This includes modes or states of the product, such as option
settings, view modes, contents of documents, etc.
Interdependent/interacting: Any data that influences or is influenced
by the state of other data, or that jointly influences an output.
Sequences/combinations: Any ordering or permutation of data, e.g.,
word order, sorted vs. unsorted data, order of tests.
Cardinality: Numbers of objects or fields may vary (e.g., zero, one,
many, max, open limit). Some may have to be unique (e.g., database
keys).
Big/little: Variations in the size and aggregation of data.
Invalid/noise: Any data or state that is invalid, corrupted, or produced
in an uncontrolled or incorrect fashion.
Lifecycle: Transformations over the lifetime of a data entity as it is
created, accessed, modified, and deleted.
Interfaces: Every conduit by which the product is accessed or expressed
User interfaces: Any element that mediates the exchange of data with
the user (e.g., displays, buttons, fields, whether physical or virtual).
System interfaces: Any interface with something other than a user,
such as engineering logs, other programs, hard disks, network, etc.
API/SDK: Any programmatic interfaces or tools intended to allow the
development of new applications using this product.
Import/export: Any functions that package data for use by a different
product or interpret data from a different product.

Platform: Everything on which the product depends (and that is outside
your project)
External hardware: Hardware components and configurations that are
not part of the shipping product but are required (or optional) for the
product to work: systems, servers, memory, keyboards, the cloud.
External software: Software components and configurations that are
not part of the shipping product but are required (or optional) for the
product to work: operating systems, concurrently executing
applications, drivers, fonts, etc.
Embedded components: Libraries and other components that are
embedded in your product but are produced outside your project.
Product footprint: The resources in the environment that are used,
reserved, or consumed by the product (memory, file handles, etc.).
Operations: How the product will be used
Users: The attributes of the various kinds of users.
Environment: The physical environment in which the product operates,
including such elements as noise, light, and distractions.
Common use: Patterns and sequences of input that the product will
typically encounter. This varies by user.
Disfavored use: Patterns of input produced by ignorant, mistaken,
careless, or malicious use.
Extreme use: Challenging patterns and sequences of input that are
consistent with the intended use of the product.
Time: Any relationship between the product and time
Input/output: When input is provided, when output is created, and any
timing relationships (delays, intervals, etc.) among them.
Fast/slow: Testing with "fast" or "slow" input, fastest and slowest,
combinations of fast and slow.

Changing rates: Speeding up and slowing down (spikes, bursts, hangs,
bottlenecks, interruptions).
Concurrency: More than one thing happening at once (multi-user,
time-sharing, threads, semaphores, and shared data).
Quality Criteria Categories
A quality criterion is some requirement that defines what the product should
be. By thinking about different kinds of criteria, you will be better able to
plan tests that discover important problems fast. Each of the items on this
list can be thought of as a potential risk area. For each item below,
determine if it is important to your project, and then think about how you
would recognize if the product worked well or poorly in that regard.
Capability: Can it perform the required functions?
Sufficiency: The product possesses all the capabilities necessary to
serve its purpose.
Correctness: It is possible for the product to function as intended and
produce acceptable output.
Reliability: Will it work well and resist failure in all required situations?
Robustness: The product continues to function over time without
degradation, under reasonable conditions.
Error handling: The product resists failure in the case of bad data, is
graceful when it fails, and recovers readily.
Data integrity: The data in the system is protected from loss or
corruption.
Safety: The product will not fail in such a way as to harm life or
property.
Usability: How easy is it for a real user to use the product?
Learnability: The operation of the product can be rapidly mastered by
the intended user.

Operability: The product can be operated with minimum effort and
fuss.
Accessibility: The product meets relevant accessibility standards and
works with OS accessibility features.
Charisma: How appealing is the product?
Aesthetics: The product appeals to the senses.
Uniqueness: The product is new or special in some way.
Entrancement: Users get hooked, have fun, and are fully engaged
when using the product.
Image: The product projects the desired impression of quality.
Security: How well is the product protected against unauthorized use or
intrusion?
Authentication: The ways in which the system verifies that a user is
who they say they are.
Authorization: The rights that are granted to authenticated users at
varying privilege levels.
Privacy: The ways in which customer or employee data is protected
from unauthorized people.
Security holes: The ways in which the system cannot enforce security
(e.g., social engineering vulnerabilities).
Scalability: How well does the deployment of the product scale up or
down?
Compatibility: How well does it work with external components and
configurations?
Application compatibility: The product works in conjunction with
other software products.
Operating system compatibility: The product works with a particular
operating system.

Hardware compatibility: The product works with particular hardware
components and configurations.
Backward compatibility: The product works with earlier versions of
itself.
Product footprint: The product doesn't unnecessarily hog memory,
storage, or other system resources.
Performance: How speedy and responsive is it?
Installability: How easily can it be installed onto its target platform(s)?
System requirements: Does the product recognize if some necessary
component is missing or insufficient?
Configuration: What parts of the system are affected by installation?
Where are files and resources stored?
Uninstallation: When the product is uninstalled, is it removed cleanly?
Upgrades/patches: Can new modules or versions be added easily? Do
they respect the existing configuration?
Administration: Is installation a process that is handled by special
personnel, or performed on a special schedule?
Development: How well can we create, test, and modify it?
Supportability: How economical will it be to provide support to users
of the product?
Testability: How effectively can the product be tested?
Maintainability: How economical is it to build, fix, or enhance the
product?
Portability: How economical will it be to port or reuse the technology
elsewhere?
Localizability: How economical will it be to adapt the product for
other places?

Note
1   We update this from time to time. Check
https://www.wiley.com/go/takingtestingseriously for the latest.
OceanofPDF.com

APPENDIX C
Heuristics of Testability
The practical testability of a product is how easy it is to test1 by a particular
tester and test process, in a given context.2 Practical testability is a function
of five other "testabilities:" project-related testability, value-related
testability, subjective testability, intrinsic testability, and epistemic
testability (also known as the risk gap). Just as in the case of quality in
general, testability is a plastic and multidimensional concept that cannot be
usefully expressed in any single metric. But we can identify testability
problems and heuristics for improving testability in general.
Interesting Testability Dynamics
Changing the product or raising the quality standard reduces
epistemic testability. The difference between what we know and what
we need to know is why we test in the first place. A product is easier to
test if we already know a lot about its quality or if the quality standard
is low, because there isn't much left for testing to do. That's epistemic

testability. Therefore, a product that changes a lot or one in which we
can't tolerate trouble is automatically less testable.
Improving any other aspect of testability increases the rate of
improvement of epistemic testability. Any effort made to improve
any other aspect of testability, by definition, increases the rate at which
the gap between what we know and what we need to know closes.
Improving test strategy might decrease subjective testability or
vice versa. This may happen when we realize that our existing way of
testing, although easy to perform, is not working. A better test strategy,
however, may require much more effort and skill. (Ignorance was
bliss.) Beware that the opposite may also occur. We might make a
change (adding a tool, for instance) that makes testing seem easier,
when in fact the testing is worse. (Bliss may be ignorant.)
Increasing intrinsic testability might decrease project-related
testability. This may happen if redesigning a product to make it more
testable also introduces many new bugs. Or it may happen because the
developers spend longer stabilizing the product before letting
independent testers see it. Agile development done well helps
minimize this problem.
Increasing value-related testability might decrease project
testability. You can dramatically increase value-related testability by
embedding yourself with real users in the field and even participating
in their work. You will be testing under the most realistic possible
conditions. However, this can be difficult, expensive, slow, and may
lead to accidental retention or mishandling of confidential personal
data as you test, which hurts project-related testability.
Increasing practical testability also improves development and
maintenance. If a product is easier to test, then it is also easier to
support, debug, and evolve. Observability and controllability, for
instance, are a tide that floats all boats.
The tester must ask for testability. We cannot expect any non-tester
to seriously consider testability. It's nice when they do, but don't count
on it. An excellent tester learns to spot testability issues and resolve
them with the team.

Guidewords for Analyzing Testability
Epistemic Testability
Prior knowledge of quality: If we already know a lot about a product,
we don't need to do as much testing.
Tolerance for failure: The less quality required, or the more risk that
can be taken, the less testing is needed.
Change control: Frequent and disruptive change requires retesting and
invalidates our existing product knowledge. Change control generally
improves testability, but can also hurt it if we aren't allowed to update
our test environments, tools, and data. Change control may occur on a
local or corporate level.
Project-related Testability
Mission alignment: The testers and the project must be in reasonable
agreement about the service that testers will provide, as well as share a
general understanding of the test process to be performed.
Prudence: If the business is responsible about releases, they will be
careful about product risk and therefore be more supportive of testing.
Supportive culture: Testers are highly sensitive to intimidation or
disrespect. No one wants to feel like a troublemaker or outsider.
Protect the independence and purpose of testers if you want their best
work.
Developer availability: The ability to speak with and perhaps
influence developers, designers, and managers makes a nurturing
environment for testing.
Information availability: We get all the information we want or need
to test well.
Tool availability: We are provided with all tools we want or need to
test well.
Test item availability: We can access and interact with all relevant
versions of the product.

Sandboxing: We are free to do any testing worth doing (perhaps
including mutation or destructive testing), without fear of disrupting
users, other testers, or the development process.
Environmental controllability: We can control all potentially relevant
experimental variables in the environment surrounding our tests.
Time: Having too little time destroys testability. We require time to
think, prepare, and cope with surprises.
Leanness: Complexity and magnitude of work products, accumulated
over time, must be navigated or maintained to get the testing done.
Also, complicated bureaucratic processes reduce time available for
testing. Avoid technical debt and administrative overhead.
Value-related Testability
Low Expectations: The lower our standard of quality, the easier it is
to fulfill the mission of testing.
Oracle availability: We need ways to detect each kind of problem. A
well-written specification is one example of such an oracle, but there
are lots of other kinds of oracles (including people and tools).
Oracle authority: We benefit from oracles that identify problems that
will be considered important.
Oracle reliability: We benefit from oracles that can be trusted to work
over time and in many conditions.
Oracle precision: We benefit from oracles that facilitate identification
of specific problems.
Oracle inexpensiveness: We benefit from oracles that don't require
much cost or effort to acquire or operate.
User stability and unity: The less users change and the more harmony
among them, the easier the testing.
User familiarity: The more we understand and identify with users, the
easier it is to test for them.
User availability: The more we can talk to and observe users, the
easier it is to test for them.

User data availability: The more access we have to natural data, the
easier it is to test.
User environment availability: Access to natural usage environments
improves testing.
User environment stability and unity: The less user environments
and platforms change and the fewer of them there are, the easier it is to
test.
Subjective Testability
Engagement: A bored or distressed tester is a poor tester. A good tester
is intellectually and emotionally engaged in the work and confident of
finding any problems that may exist.
Involvement: Testing is easier when a tester is closer to the
development process, communicating and collaborating well with the
rest of the team. When testers are held away from development, test
efficiency suffers terribly.
Product knowledge: Knowing a lot about the product, including how
it works internally, profoundly improves our ability to test it. If we
don't know about the product, testing with an exploratory approach
helps us to learn rapidly.
Technical knowledge: Ability to program, knowledge of underlying
technology and applicable tools, and an understanding of the dynamics
of software development generally, though not in every sense, make
testing easier for us.
Domain knowledge: The more we know about the users and their
problems, the better we can test.
Testing skill: Our ability to test in general obviously makes testing
easier. Relevant aspects of testing skill include experiment design,
modeling, product element factoring, critical thinking, and test-
framing.
Test strategy: A well-designed test strategy may profoundly reduce
the cost and effort of testing.

Supporting tester availability: A "supporting tester" is anyone who is
not a regular tester and who is not responsible for the testing, yet is
available to help test the product. Developers can be good supporting
testers.
Intrinsic Testability
Observability: We must see the product. Ideally, we want a
completely transparent product, where every fact about its states and
behavior, including the history of those facts, is readily available to us.
Controllability: We must be able to visit the behavior of the product.
Ideally, we can provide any possible input and invoke any possible
state, combination of states, or sequence of states on demand, easily
and immediately. Any non-deterministic behavior of the product
detracts from testability.
Algorithmic simplicity: We must be able to visit and assess the
relationships between inputs and outputs. The more complex and
sensitive the behavior of the product, the more we will need to look at.
Algorithmic transparency: If no one knows how the product
produces its output (as is typical with machine learning systems), we
will need to sample much more of the input and output space to test it
well.
Algorithmic stability: If changes to the product can be made without
radically disturbing its logic, then past test results will not have to be
thrown out with every tiny modification. Although any system might
suffer from instability, it is especially a problem in machine learning.
Explainability: We must understand the design of the product as much
as we can. A product that behaves in a manner that is explainable to
outsiders is going to be easier to test. "Explainability" is also a hot
topic in AI.
Unbugginess: Bugs slow down testing because we must stop and
report them, or work around them, or, in the case of blocking bugs,
wait until they get fixed. It's easiest to test when there are no bugs.

Smallness: The less there is of a product, the less we have to look at
and the less chance of bugs due to interactions among product
components. This also applies to the amount of output we must review.
Decomposability: When different parts of a product can be separated
from each other, we have an easier time focusing our testing,
investigating bugs, and retesting after changes.
Similarity (to known and trusted technology): The more a product is
like other products we already know, the easier it is to test it. If the
product shares substantial code with a trusted product, or is based on a
trusted framework, that's especially good.
Notes
1   Testing is evaluating a product by learning about it through experiencing,
exploring, and experimenting.
2   Context is all of the factors in a situation that should be considered when
solving a situated problem.
OceanofPDF.com

APPENDIX D
Elements of Excellent Testing
Science is testing, and testing is science. The world of commercial product
testing differs from the world of science mainly in its object, not its subject
or its process: we who test products apply ourselves to the study of an
ephemeral human contrivance rather than the natural world. This is a human
learning process.
Testing, like science, is an exploratory process that also makes use of
scripted elements. The "gears" in the diagram to the left represent activities
that evolve over time, feeding each other.
The rectangles are artifacts that result from those activities.
At the center, analysis drives the whole process. The four connection points
to analysis are worth examining more closely.
Learning: The connections between analysis and knowledge might be
called the learning loop. In this interaction, the tester is reviewing,
thinking about, and applying what they know.
Testing: The connection between analysis and experiment might be
called the testing loop (in the sense of performing tests). It is
dominated by questions about the status of the product. It may involve
algorithmic processes such as automated output checking.

Collaboration: The connections between analysis and other people
might be called the collaboration loop. Testing is always a social
process to some degree, and group testing can be very energizing.
Self-management: The connection between analysis and the testing
story is self-management, by which the whole process is regulated.
Self-management is driven by the stories we tell ourselves.

Evolving Work Products
As testing proceeds, look for any of the following to be created, refined,
and possibly documented during the process.
Test ideas: Any idea or part of an idea, written or unwritten, that may
guide the performance of a test or check.
Output checks: Mechanized or mechanizable processes for gathering
product observations and evaluating them. A test is always human-
guided, whereas a check, by definition, can be completely automated.
A test often includes one or more checks, but a check cannot include a
test.
Testability ideas: How can the product be made easier to test?

Test results: You may need to maintain or update test results as a
baseline or historical record.
Bug reports: Reports regarding anything about the product that
threatens its value.
Issue reports: Reports regarding anything about the project that
threatens its value.
Test conditions (product coverage outline): A test condition is
anything about the product you might want to examine with a test. A
product coverage outline is an outline, list, or table of interesting test
conditions.
Product risks: Any potential areas of bugginess or types of bug.
Test data: Any data available for use in testing.
Test tools: Any tools acquired or developed to aid testing (includes
automated output checks).
Test strategy: The set of ideas that guides your test design on a
particular project.
Test infrastructure and lab procedures: General practices, protocols,
controls, and systems that provide a basis for excellent testing.
Test estimation: Ideas about what you need, how much time you
need, and what obstacles might be in your way.
Testing story: What you know about your testing so far.
Product story: What you know about the status of the product so far.
Test process assessment: Your own assessment of the quality of your
test process.
Tester and team: The tester and the team evolve over the course of
the project.
Technical and domain knowledge: Your knowledge about how the
product works and how it is used.

Testing Skills and Tactics
These are the skills (or tactics that involve skill) needed for professional
and cost-effective product testing. Each is distinctly observable and
learnable, and each is necessary for excellent exploratory work.
Self-Management Skills and Tactics
Chartering your work: Making decisions about what you will work
on and how you will work. Deciding the testing story you want to
manifest, knowing your client's needs, the problems you must solve,
and assuring that your work is on target.
Establishing procedures and protocols: Designing ways of working
that allow you to manage your study productively. This also means
becoming aware of critical patterns, habits, and behaviors that may be
intuitive and bringing them under control.
Establishing the conditions you need to succeed: Wherever feasible
and to the extent feasible, establish control over the surrounding
environment such that your tests and observations will not be disturbed
by extraneous and uncontrolled factors.
Self-care: Monitoring your emotional, physical, and mental states as
they influence your testing; taking effective action to manage your
energy and maintain a positive outlook; self-forgiveness.
Self-criticism: Finding problems in your work and correcting them;
awareness and acknowledgement of your strengths and weaknesses as
a tester.
Test status evaluation: Maintaining an awareness of problems,
obstacles, limitations, and biases in your testing; understanding the
cost vs. value of the work; constructing the testing story.
Ethics: Understanding your ethical code and fulfilling your
responsibilities under it as you work.
Branching your work and backtracking: Allowing yourself to be
productively distracted from a course of action to explore an

unanticipated new idea; identifying opportunities and pursuing them
without losing track of your process.
Focusing your work: Isolating and controlling factors to be studied,
repeating experiments, limiting change, precise observation, defining
and documenting procedures, optimizing effort, using focusing
heuristics.
Defocusing your work: Expanding the scope of your study,
diversifying your work, changing many factors at once, broad
observation, trying new procedures, using defocusing heuristics.
Alternating activities to improve productivity: Switching among
complementary activities or perspectives to create or relieve
productive tension and make faster progress. See Exploratory Testing
Polarities.
Maintaining useful and concise records: Preserving information
about your process, progress, and findings.
Knowing when to stop: Selecting and applying stopping heuristics to
determine when you have achieved good enough progress and results
—or when your exploration is no longer worthwhile.
Developing and maintaining credibility: No one will listen to you if
they think what you say is not interesting or important. Remember, a
tester has very little visible work, so your reputation is paramount.
Collaboration Skills and Tactics
Getting to know people: Meeting and learning about the people
around you who might be helpful or whom you might help, developing
a collegial network within your project and beyond.
Conversation: Talking through and elaborating ideas with other
people.
Serving other testers: Performing services that support other testers
on their own terms.
Guiding other testers: Supervising testers who support your
explorations, coaching testers.

Asking for help: Articulating your needs, negotiating for assistance.
Role visiting: Where feasible and applicable, spending time
performing non-testing roles that may give you perspective or practice
that makes you a better tester.
Test framing: Making a credible, relevant justification of your test
process. What is behind your strategy?
Test reporting: Making a credible, relevant account of the status of
your testing.
Telling the product story: Making a credible, relevant account of the
status of the product you are studying, including bugs found. This is
the ultimate goal for most test projects.
Learning Skills and Tactics
Discovering and developing resources: Obtaining information or
facilities to support your effort. Exploring those resources.
Using the Web: Of course, there are many ways to perform research
on the Internet. But acquiring the technical information you need often
begins with Google or Wikipedia.
Considering history: Reviewing what's been done before and mining
that resource for better ideas.
Reading and analyzing documents: Reading carefully and analyzing
the logic and ideas within documents that pertain to your subject.
Interviewing: Identifying missing information, conceiving questions,
and asking questions in a way that elicits the information you seek.
Pursuing an inquiry: A line of inquiry is a structure that organizes
reading, questioning, conversation, testing, or any other information-
gathering tactic. It is investigation oriented around a specific goal.
Many lines of inquiry may be served during exploration. This is, in a
sense, the opposite of practicing curiosity.
Indulging curiosity: Curiosity is unjustified learning. It serves a
general goal: learning things that might be useful, at some later time.
This is a complementary process to pursuing a line of inquiry.

Generating and elaborating a requisite variety of ideas: Working
quickly in a manner good enough for the circumstances. Revisiting the
solution later to extend, refine, refactor, or correct it.
Overproducing ideas for better selection: Producing many different
speculative ideas and making speculative experiments, more than you
can elaborate upon in the time you have. Examples are brainstorming,
trial and error, genetic algorithms, and free-market dynamics.
Abandoning ideas for faster progress. Letting go of some ideas in
order to focus and make progress with other ones.
Recovering or reusing ideas for better economy: Revisiting your old
ideas, models, questions, or conjectures, or discovering them already
made by someone else.
Test Performance Skills and Tactics
Determining test strategy: The test strategy is usually framed as a set
of test activities that embody different kinds of testing and different
parts of the product to test. A strategy may or may not be explicitly
mapped to product risk.
Analyzing product risk: Using experiential data, conversation, and
heuristics, identifying suspected product risks that deserve to be
investigated with testing.
Encountering the product: Making and managing contact with the
subject of your study; for technology, configuring and operating it so
that it demonstrates what it can do.
Sensemaking: Determining the meaning and significance of what you
encounter; considering multiple, incompatible explanations that
account for the same facts; inference to the best explanation.
Modeling and factoring: Modeling means composing, decomposing,
describing, and working with mental representations of the things you
are exploring; factoring means identifying relevant dimensions,
variables, and dynamics that could be tested. There are lots of formal
modeling methods, as well.

Experiment design: As you develop ideas about what's going on,
creating and performing tests designed to disconfirm those beliefs,
rather than repeating the tests that merely confirm them.
Literate observation: Making relevant observations guided by your
various mental models, gathering different kinds of empirical data or
data about different aspects of the object, establishing procedures for
rigorous observations, noticing strange things, and noticing what you
are not seeing.
Detecting potential problems: Designing and applying oracles to
detect behaviors and attributes that may be troublesome.
Assessing validity: Analyzing, monitoring, and correcting for factors
that may distort or invalidate the tests.
Notetaking: Recording observations, ideas, and progress as you test;
recording useful information without unduly disturbing the test process
itself.
Data wrangling: Synthesizing, modifying, moving, and reformatting
test data.
Bug reporting and advocacy: Explaining problems in a compelling
and respectful way.
Applying tools: Enabling new kinds of work or improving existing
work by developing and deploying tools.
Testability advocacy: Analyzing and negotiating for the conditions
that make testing easier and more effective.
Protocol design: Creating and following procedures and practices that
increase the reliability of the test process.
Lab management: Creating and maintaining the systems, tools,
databases, and spaces that you need to test well.
Knowledge That Helps
In addition to the skills and tactics of testing, there are a lot of things you
might need to know.

Product knowledge: What does your product do, and how does it
work?
Technology knowledge: What technology is your product built from?
What other technologies can help you test it?
Project knowledge: What's going on in your project? What's the
schedule? Who is working on it?
Domain knowledge: Who are the users? How do they think? What
sort of process does your product support?
General systems knowledge: This refers to the whole field of general
systems theory and systems thinking. In short, it consists of the
heuristics and know-how about how dynamic systems behave.
Tool knowledge: Physical or software-based tools that can help with
testing. This does not only mean tools that are called "test tools" but
rather any tool that may help any aspect of the test process.
Test technique knowledge: There are many kinds of testing and many
specific testing heuristics you might use.
Resource knowledge: In addition to things that you think of as tools,
you need to be aware of any resource (i.e., facility, material, or service)
that is available to help you get the job done.
People knowledge: Who can help you? What skills do they have that
you need? How do you approach them? How specifically might they
contribute to the test project?
Role knowledge: What do the other people do who make the project
work? How does their work impact yours? How might knowing more
about their roles help you do your job better? How do you serve them
as a tester?
History knowledge: What is the history of this project? This product
line? The market? This company? What trouble has happened in the
past that we don't want repeated?
Business and market knowledge: Who are your competitors? What
are those competing products? Are there similar or complementary
products as well? How does quality affect your bottom line?

Helpful Skills Some Testers Have
In addition to the defining skills of testing, there are other skills and
knowledge areas that testers may have.
Coding skills: In some companies, coding skills are a requirement. In
general, the ability to build your own tools brings great power to
testing. However, people with coding skills may also think too much
like coders and lose empathy for users.
Design skills: Product design, and especially user interface design, can
help sharpen your bug reports and improve your bug detection ability.
Social science skills: The social sciences are about studying extremely
complex, socially situated phenomena. The analytical methods and
standards of social science help testers better understand the limits of
software testing and to better study and improve testing processes.
Specification writing skills: Sometimes it helps for testers to help
write specifications or to suggest rewrites. Writing a spec is one great
way of preparing to design tests for that product.
Mathematics and logic skills: Statistics, combinatorics, and formal
logic are often useful for designing deep tests and characterizing test
coverage.
Cognitive science skills: If you understand the patterns and limitations
of human perception, you can better appreciate how to avoid common
pitfalls of self-deception, and how to design test procedures that are
more reliable.
Exploratory Polarities
To develop ideas or search a complex space quickly yet thoroughly, not
only must you look at the world from many points of view and perform
many kinds of activities (which may be polar opposites), but your mind
may get sharper from the very act of switching from one kind of activity to
another. Here is a partial list of polarities:
Warming up vs. cruising vs. cooling down
Doing vs. describing

Doing vs. thinking
Deliberate vs. spontaneous
Data gathering vs. data analysis
Working with the product vs. reading about the product
Working with the product vs. working with the developer
Training (or learning) vs. performing
Product focus vs. project focus
Solo work vs. team effort
Your ideas vs. other people's ideas
Lab conditions vs. field conditions
Current version vs. old versions
Feature vs. feature
Requirement vs. requirement
Coverage vs. oracles
Testing vs. touring
Individual tests vs. general lab procedures and infrastructure
Testing vs. resting
Playful vs. serious
OceanofPDF.com

About the Contributors
Jon Bach is a Seattle-area independent software consultant with three
decades in software testing and enterprise-level program management. He's
led software teams at Microsoft and eBay. He also co-created Session-
Based Test Management with his brother James. Jon is an award-winning
conference speaker with a BA in Journalism from the University of Maine
and finds testing to be a similar pursuit to being a journalist—asking
questions to discover truths and then making a report of findings to an
audience who needs to know.
Lalitkumar Bhamare is an award-winning Engineering Leader with over
18 years of experience in the field. For Accenture, he leads the Thought
Leadership group for Quality Engineering innovation across Germany,
Austria, and Switzerland. He created the QCSD framework, recognized at
EuroSTAR 2022, and founded the nonprofit Tea-time with Testers. As
Director at the Association for Software Testing, international keynote
speaker, and thought leader, he drives advancements in software quality.
James Christie has 40 years of commercial IT experience, working in
development, IT audit, testing, and security, mainly with financial systems.
He worked for IBM on large-scale outsourcing accounts similar to Fujitsu's
contract with the UK Post Office.
Harry Collins is a sociologist of knowledge who has studied science,
including a 45-year-long study of the detection of gravitational waves. He
has written, among other books, three books on what his studies mean for
artificial intelligence and the use of computers in general.
Ruud Cox is a testing practitioner with a special interest in modeling as a
tool to solve problems. He likes to listen to music, and he also makes
drawings and writes with fountain pens in his notebooks.
Rikard Edgren describes himself as a humanistic and technical tester since
1998, specializing in generalities like test strategy and exploratory testing.
He is a member of the think tank The Test Eye, co-author of Software
Quality Characteristics, and author of The Little Black Book on Test Design
and Den Lilla Svarta om Teststrategi.

Henrik Emilsson has a long background in software testing. He has
continuously practiced, implemented, challenged, and evaluated ideas from
James Bach, Michael Bolton, and Cem Kaner in his daily work. He is a
member of the think tank The Test Eye and co-author of Software Quality
Characteristics.
Marius Francu has been a software developer and development manager
for twenty years. He has a great enthusiasm for decision theory, general
systems theory, and complexity theory, which he applies to testing.
Lee Hawkins considers that his testing career really started in 2007 after
attending Rapid Software Testing with Michael Bolton, but he has been in
the IT industry since 1996 in both development and testing roles. Lee has
spent most of his career helping teams across the world improve the way
they build, test, and deliver software. He is a frequent speaker at
international testing conferences and blogs on testing at Rockin' and Testing
All Over the World.
Paul Holland received his B.CompSci from Carleton University in 1987.
After flying Sea King helicopters for the Canadian military, he changed
careers to software testing in 1995.
Aaron Hodder has mostly been a tester and test consultant over the past
twenty years. He focuses on the human aspects of software development, as
well as on agile and lean ways of working. He cofounded the WeTest
workshops in Wellington, NZ, and is a director of NZTechRally. He also
founded the Human Impact in Tech Unconference.
Martin Jansson has immersed himself in the testing field since the mid
'90s, balancing product testing and teaching testing. He is a member of the
think tank The Test Eye and co-author of Software Quality Characteristics
and 37 Sources for Test Ideas.
Karen Nicole Johnson has published numerous nonfiction articles as well
as a chapter in the book Beautiful Testing. She has published numerous
nonfiction articles about her experiences working with computer software.
See her website at http://www.KarenNicoleJohnson.com.
Keith Klain has built and managed software quality and testing teams for
global financial services and consulting firms in the US, UK, and Asia
Pacific for 25 years. He is also an advocate for workforce development and

social impact, having designed and delivered technology training curricula
for nonprofits to create technology delivery centers in disadvantaged
communities. He has served as the Executive Vice President of the
Association for Software Testing and has received multiple awards and
recognition for his contributions to the software testing profession and
diversity in tech.
Janna Loeffler has over twenty years of experience in software testing.
She is deeply passionate about quality and excellence in every aspect of her
work. Her mission is to inspire a commitment to quality and continuous
improvement within the testing community.
Joris Meerts is a senior test consultant working for ICT Group in the
Netherlands. He is a passionate software tester who enjoys exploring the
boundaries of the craft, seeking ways to improve testing. As such, he is an
advocate of the context-driven approach to testing. Joris is also the author
of The History of Software Testing, which can be found on his website,
https://www.TestingReferences.com.
Jeff Nadelman has a career in software development that includes ten years
as a developer and twenty years as a tester. He is currently a business
analyst, giving him the opportunity to see software development from a
third perspective.
Wayne Roseberry has worked as a software tester or software engineer for
many years on products such as MSN, Site Server, SharePoint, Microsoft
Office, and the Integrated Services group at Ford Motor Corporation. He
finds testing fascinating and entertaining, and enjoys helping other people
become better, more effective testers.
Torbjörn Ryber is a UX Designer and tester, always solving business
problems with the user in mind. He is the author of the books Essential
Software Test Design and Modern UX.
Huib Schoots is passionate about learning, mindset, thinking, behavior,
collaboration, testing, and quality. He is a consultant, trainer, and coach at
Sogeti, where he helps clients and colleagues to create valuable software.
Xiaomei Tai (邰晓梅)is founder of Buccaneer Methodology and the author of
the book Buccaneer Test Analysis: MFQ & PPDCS (In Chinese: 海盗派测试分
析:MFQ & PPDCS).

Tanja Vos is a Professor of Software Engineering at the Technical
University of Valencia and the Open University of the Netherlands. With
over twenty years of experience, she has been actively involved in
educational and research projects focused on software testing.
OceanofPDF.com

Index
A
a priori probability, 460, 464
abductive inferences, 462
abnormal, normal compared with, 142
abnormal behavior, 143
abnormal input, 143-144
abnormal usage, 145-146
acceptable, disallowed compared with, 144
accessibility, as a quality criteria category in HTSM, 309, 490
accidental change, as cause of intermittent problems, 125
accidental input, as cause of intermittent problems, 126-127
accuracy, as a quality characteristic, 308
ACRCUF heuristic, 332
activities, 59-61, 503
activity-based management, 208-213
actual worlds (justified true beliefs), 458
adaptability, as an internal quality characteristic, 313
administration, 251, 491
adoption, 251
advocating, for testability, 320-322
aesthetics, as a quality criteria category in HTSM, 490
affordance, as a quality characteristic, 309
Afzic, Gojko, 227-228

agency, of explorers, 48
agentic mode, 262
The Age (newspaper), 445
Agile, 42, 63
Agile Rapid Testing Grid (ART), 160-163, 179
Agile/DevOps School, 4, 5
AI age, 260
alertness, flagging during testing, 117
algorithmic obscurity, AI and, 260
algorithmic simplicity, in intrinsic testability, 498
algorithmic stability, in intrinsic testability, 498
algorithmic transparency, in intrinsic testability, 498
algorithms, for tools, 249
alignment, 199, 200-201, 241-243
alternative histories, 62
analysis paralysis, 286-287
analysis testing, 367
Analytical School, 4
analytical tools, in project environment of HTSM, 485
APIs, 243, 488
application compatibility, as a quality criteria category in HTSM, 312, 490
archival notes, 194
artifact-based management, 204-208
Artificial Experts (Collins), 404
artificial intelligence (AI), 255-268
Ashby, Ross, 94

aspirations oracle pattern, 105
assertion failures, 357-358
assessment, 37, 71-73, 133-134
assessment space, 77
Association for Software Testing (AST), 449-450
Association path, in ACRCUF heuristic, 332
atrophy trap, 238-239
attachments, in basic formal bug reports, 132
attention wandering, during testing, 117
attitude, as a quality characteristic, 310
attractiveness, as a quality characteristic, 310
auditability, as an internal quality characteristic, 313
augmented experiential testing, 66, 222-223
Austin, Robert, 428
Australian Workshop on Software Testing (OZWST), 448
authentication, 119, 310, 490
authorization, as a quality criteria category in HTSM, 310, 490
automated assay, in user simulation automation, 230
automated flagging, in user simulation automation, 230
automated operation, in user simulation automation, 230
automated research, 219
automated testers, 219
automated testing, 66, 483
automation, as an internal quality characteristic, 313
automation bias, 68
automation complacency, 253

autonomy, 63
availability
of consulting, 252
in project environment of HTSM, 486
as a quality characteristic, 311
of technical support, 252
tools for, 251
of training, 252
availability bias, 67-68
available quality, of testers, 10
B
Bach, James (author), 3-4, 205, 305, 325, 329, 404, 428, 447, 456, 470,
471-472
Bach, Jon, 194
backtracking, 330, 503
backward compatibility, as a quality criteria category in HTSM, 312, 490
bad memory, as cause of intermittent problems, 124
bad test idea, during testing, 118
Behavior-Driven Development (BDD), 224, 227, 278
benefits tour, 95
best practices, 61
beta testing, 144
bias, 67-70, 263
big, in product elements of HTSM, 488
black flagging, 131
black swan, 174

black-box testing, 34
blink oracles, 231-232
blooper tour, 95
Boisot, Max, 454-458, 462
Bolton, Michael, 325, 401-414, 446, 451, 470, 471
bookmarklets, 223
bootstrap approach, 53
boundary objects, 44-45
boundary testing, 98-100
bounded rationality, 52
brainstorming risks, 175-179
branching, 330, 503
broad testing, narrow testing compared with, 21-22, 141
browser extension, 222-223
bug cascade, 117
bug investigation & reporting (B-time) metric, 211
bug reporting
investigating bugs, 119-120
investigating intermittent problems, 120-128
normal compared with enhanced request, 118-119
overview, 118, 502
process of, 128-134
as a test performance skill/tactic, 506
bugopsy, 325
builders, test strategy for, 159-166
bullet-style notes, 193

business and market knowledge, as a testing skill/tactic, 507
business rule tour, 95
business rules, 263
business side, importance of, 435-436
C
calculation, in product elements of HTSM, 487
capability, 249-250, 308, 489
capacity, 117, 232, 311
cardinality, in product elements of HTSM, 488
centering oneself, 148-149
change control, in epistemic testability, 495
changes, coping with, 233
Changing Order (Collins), 403
changing rates, in product elements of HTSM, 489
Chapanis, Alphonse, 340
charisma, 307, 310, 490
charter, in SBTM, 210
chartering work, as a self-management skill and tactic, 502
ChatGPT, 262, 407-408
checking, 18-20, 84-88, 139, 349-351, 454
checklists, in project environment of HTSM, 485
Choose subprocess, 113-114
claims oracle pattern, 106
claims testing, 178, 482
clarity, as a quality characteristic, 309
classroom scale, 354

Claude, 262
clean machine, for bugs, 120
clients, as testers, 28-30
code, in product elements of HTSM, 486
coding skills, 507
coercion, temptation for, 201-204
cognitive science skills, 508
collaboration loop, analysis and, 500-501
collaboration skills and tactics, 504
collateral, in product elements of HTSM, 487
collective tacit knowledge, 46
Collins, Harry (sociologist), 31, 76, 401-414, 423-424, 475-476
combination lottery, as cause of intermittent problems, 127
combinations, in product elements of HTSM, 488
commit thread, of goal-oriented activities, 60
commitment, as a role expectation, 43
common use, in product elements of HTSM, 488
communication, unreliable, as cause of intermittent problems, 125
community of users, tools for, 252
comparable product, 106
comparable products oracle pattern, 106
compatibility, 232, 311-312, 313, 490
competence, as a role expectation, 43
compiling, 18-19
completeness, as a quality characteristic, 308
complex testing, 224

complexity tour, 95
compliance, as a quality characteristic, 311
compromised input, as cause of intermittent problems, 127
Computer Programming Fundamentals (Weinberg), 12
computing resources, 234
concurrency, 308, 489
conditional behavior, 245
conducting testing, 370-373
confabulation, 263
Conference of the Association for Software Testing (CAST), 449-450
configurability tools, 251
configuration, as a quality criteria category in HTSM, 311, 312, 491
confirmation bias, 68
confusable data, as a boundary-testing heuristic, 99
confusion, 53
conjecture, in risk-based testing, 171-172
"Connecting the Dots: From Data Processing to Pattern" (Boisot), 455
consistency, as a quality characteristic, 309
constraints, on testing, 151
constructive disengagement, 275-276
content, in project environment of HTSM, 486
context, testing and, 39-41
Context-Driven school of testing, 3-6, 24, 373

context-driven testing (CDT)
Bach's preference for, 475-476
community of, 448-449
discovery of, 470-472
overview, 365
principles of, 40
Contradiction path, in ACRCUF heuristic, 332
contrived, 144-145, 147
control, 54-56, 309
controllability, as an internal quality characteristic, 312, 498
conversation, as a collaboration skills/tactic, 504
Cooper, Alan, 341
coordination, 43-44, 54
Copilot, 262
Coplien, James, 460
core ideas, 453-461
corporate defensiveness, AI and, 261
correctness, as a quality criteria category in HTSM, 489
cost, 109-110, 140, 233
coverage, 66, 89, 250, 297-300, 356
Cox, Ruud, 365
crashes, 356-357
creative thinking, 333
credibility, 190-191, 503
critical distance, 62-63, 271-272, 280
critical thinking, 329-330

critique, 281
crushing pressure, during testing, 117
Cryle, Peter, 147
Cucumber/Gherkin, 224-226
cultures, of testing, 3-6
curation, 190
curiosity, 310, 505
Curiosity path, in ACRCUF heuristic, 332
curse of knowledge, 70, 145
D
dark UX, 340
data
in HTSM, 487-488
preservation of, 359
in RST, 454-455
tools for creating, 250
tools for security of, 251
data agnosticism, as a quality characteristic, 308
data integrity, as a quality criteria category in HTSM, 309, 489
data tour, 95
data wrangling, 250, 506
data-driven testing, tools for supporting, 250
de Bono, Edward, 330
Deacon, Terrence W., 453
debugging, as an internal quality characteristic, 312
decomposability, in intrinsic testability, 498

deductive inferences, 462
deep testing, shallow testing compared with, 20-21, 141
DeepSeek, 262
defensiveness, in project environment of HTSM, 485
Defocused Thinking, 333
defocusing, 64-65, 503
degree of freedom, 94
delegation, 54
deliberate practice, 365-375, 473-475
deliberation continuum, 50
deliberative testing, 50-52, 141
deliver thread, of goal-oriented activities, 60
deliverables, in project environment of HTSM, 486
delta-first testing, 97-98
demonstrations, 70-71, 346
dependent styles, 183
deployability, as a quality characteristic, 311
description, in basic formal bug reports, 132
design, 25-16, 109-112, 220, 507
desperate opportunism, 377
detachment, difference compared with, 63
deus ex machina, as cause of intermittent problems, 125, 127
developer availability, in project-related testability, 496
developer relations, in project environment of HTSM, 485
development costs, 232-233, 486, 491
deviance, normalization of, 424

diagnostics, as an internal quality characteristic, 312
difference, detachment compared with, 63
difference-first testing, 97-98
different mode or option setting, as cause of intermittent problems, 126
different profile, as cause of intermittent problems, 127
different quality, of testers, 10
digital twin, 106
directness, as a quality characteristic, 310
disallowed, acceptable compared with, 144
disaster recovery, as a quality characteristic, 309
discarding, retaining compared with, 142
discoverability, as a quality characteristic, 309
Discussion of the Method (Koen), 58
disfavored use, in product elements of HTSM, 489
displaced goals, during testing, 117
distance, testing and, 61-63
distributed system, as cause of intermittent problems, 125
disturbing, pleasing compared with, 143
documentation, 233, 309, 345, 486, 504
documentation tour, 95
dogfooding, 69
domain knowledge, 497, 502, 506
domain testing, 483
doubt, trust compared with, 140
Duisings, Jos, 365
Dutch Exploratory Workshop on Testing (DEWT), 471

dynamic systems, 34
E
easy, stressful compared with, 143-144, 148
easy bugs, 21
E-commerce industry, characteristic contexts of, 41
economy, traps of, 239-241
efficiency, as a quality characteristic, 308
80/20 rule, 140, 229, 244
elusive bugs, 21
embedded components, in product elements of HTSM, 488
embedded industry, characteristic contexts of, 41
emergence, 52-54, 460-461
empirical quality, of testers, 9
end-to-end testing, 34, 263
endurance, as a quality characteristic, 311
engagement, in subjective testability, 497
engineering telemetry, for testing, 66
enhancement bug request, 118-119
enoughness, testing and, 59-61
entrancement, as a quality criteria category in HTSM, 310, 490
entry points, for test strategy, 150-156
environment, 132, 488, 496
epistemic modality, 195
epistemic testability, 318, 493, 494, 495
epistemology, 456, 470
equipment and tools, in project environment of HTSM, 485

equivalence class partitioning (ECP), 96-98
equivalence classes, 95-98
Ericsson, K. Anders, 340
error handling, 309, 487, 489
ethics, as a self-management skill and tactic, 503
Evaluate subprocess, 114
evaluation, testing tools used in, 221
Evans, Eric, 281
events, problematic patterns in, 358-359
evidence, 192
exaptation, 461
exceeded capacity, during testing, 117
executable specifications, 224
expectancy, as a quality characteristic, 310
expectation bias, 69
expectations, 155, 496
expected results, 106
experience, 65-66, 141-142
experience report (ER), 448
experiential testing, 65, 106-107, 263
experiments, 70-71, 506
experts, as testers, 30-32
explainability, in intrinsic testability, 498
explainability oracle pattern, 106
explicit knowledge, 45-48, 106
explicit specifications, 106

Explorations in Information Space (Boisot), 454, 457-458
exploratory data analysis, 48
exploratory polarities, 508
exploring reality of products, 81-83
Exploring Requirements: Quality Before Design (Weinberg and Gause), 57-
58
Exploring Science (Klahr), 77
export, in product elements of HTSM, 488
extensibility, 308, 313
external hardware, in product elements of HTSM, 488
external software, in product elements of HTSM, 488
extreme use, in product elements of HTSM, 489
extreme values, as a boundary-testing heuristic, 99
F
factoring, as a test performance skill/tactic, 505
Factory School of testing, 4-5, 204
failure, 88, 175, 271, 356-359
faking testing, 213-214
false consensus effect, 69
false correlation, 124
false negatives/positives, 86
familiar problems oracle pattern, 106
Familiar things path, in ACRCUF heuristic, 332
fast, in product elements of HTSM, 489
fear loop, 234-235
feasibility, as a consideration for adopting tools, 251

feature coverage, 356
features, of tools, 249
feedback, 311, 474, 485
feedback loop, in project environment of HTSM, 485
Feelings, in PROOF heuristic, 194
Feyerabend, Paul (philosopher), 76
filters, adding to reports and tools, 360-361
financial industry, characteristic contexts of, 41
flagging alertness, during testing, 117
flakey hardware, as cause of intermittent problems, 125
flexibility, 251, 313
flight-data-recorder-style notes, 193
flip blink, 231
flow testing, 483
focusing, 64-65, 171-172, 459-460, 503
folding reports, 196
Food and Drug Administration Modernization Act of 1997, 203
foreign data, as a boundary-testing heuristic, 99
forever, learning, 443-444
form of life, 4
formal bug reporting, 130-131
formal testing, informal testing compared with, 141
formality, testing and, 48-50
formalization, 51-52
format testing, 183
forward compatibility, as a quality characteristic, 312

Fraser, Justice, 381-395
freedom, testing and, 48-50
frozen conditional, as cause of intermittent problems, 126
Fujitsu, 380-395
function, in HTSM, 487
function testing, 178, 482
functional fixedness, 68
fundamental attribution error, 145
fundamental method, of testing, 77-84
fuzzing, 218
G
galumphing, 93
gaming industry, characteristic contexts of, 41
Gause, Don, 57-58
Gemini, 262
GenAI, 11, 219-220, 260-261, 262-263
general systems knowledge, as a testing skill/tactic, 507
general test techniques, in HTSM, 482-484
generalize input, for bugs, 120
ghost input, as cause of intermittent problems, 127
Gladwell, Malcolm, 231
goal shielding, 459
goal-oriented activities, threads of, 60
goals, 117, 341-345
Goldberg, Rube, 216-217
Goodhart, Charles, 295

government industry, characteristic contexts of, 41
The Grammar of Systems (Hoverstadt), 460
gray-box testing, 34, 67
grokking, 231
Gruenberger, Fred (author), 11-12
guard rails, 101-102
guess the cause, for bugs, 120
GUI interaction recording, for testing, 66
guide word, 169
GUI-level automation, considerations for, 243-249
H
hallucinated features, 263
hangs, 356-357
happy-path testing, 143
hard-to-evaluate outputs, features with, 284
hardware
compatibility, as a quality criteria category in HTSM, 312, 490
external, in product elements of HTSM, 488
features interacting with, 283-284
flakey, as cause of intermittent problems, 125
in product elements of HTSM, 486
in project environment of HTSM, 485
harmony trap, 242-243
Hawkins, Lee, 445-450
help, asking for, as a collaboration skills/tactic, 504

Heuristic Test Strategy Model (HTSM)
general test techniques, 482-484
overview, 91, 109, 110, 169-171, 179-180, 305-306, 318, 366, 368, 446,
481-482
product elements, 482, 486-489
project environment, 482, 484-486
quality criteria categories, 482, 489-491
signals-based testing and, 363-364
heuristics, 57-59, 493-498
Hiccups heuristic, 107
hidden costs, of automation, 232-235
high alignment, 200-201
history, 504, 507
history oracle pattern, 106
Hodder, Aaron, 205
Holland, Paul, 207, 322
hosting tools, 251
Hoverstadt, Patrick, 460
hubris, in project environment of HTSM, 485
human element, as cause of intermittent problems, 125
hype, as a quality characteristic, 310
hypotheses, 76
I
ICL, 380
ideas, 505
identified risk, 39

identifiers, as an internal quality characteristic, 312
ignorance, traps of, 237-239
image, as a quality criteria category in HTSM, 490
image oracle pattern, 106
import, in product elements of HTSM, 488
improper initialization, as cause of intermittent problems, 126
Improve Quality Services, 365
inattentional blindness, 67, 124
incidental coverage, 93-94
incommensurable artifacts, counting, 205-207
incorporating testing tools, 229-237
inductive inferences, 462
inferences, 462
informal bug reporting, 130-131
informal testing, formal testing compared with, 141
information, 313, 454-455, 484
information availability, in project-related testability, 496
information gain maximization, as a testing consideration, 110
information radiator, 197
initialization, improper, as cause of intermittent problems, 126

input
abnormal, 143-144
accidental, 126-127
compromised, as cause of intermittent problems, 127
ghost, as cause of intermittent problems, 127
normal, 143-144
in product elements of HTSM, 487, 489
time as, as cause of intermittent problems, 127
inquiries, pursuing, as a learning skill/tactic, 504-505
insecurity, 102
inspective testing, 271, 275
installability, 311, 491
instructors, of RST, 467-476
instrumentation, 65-66, 141-142, 250
insufficient capacity, 102
intentional coverage, 92
Interact subprocess, 114
interactions, in product elements of HTSM, 487
interactivity, as a quality characteristic, 309
interdependent, in product elements of HTSM, 487
interfaces, 140, 488
internal quality characteristics, 312-313
internals, interfaces compared with, 140
internationalization, as an internal quality characteristic, 313
interoperability, 250, 283, 308, 486
interviewing, as a learning skill/tactic, 504

intrinsic testability, 318, 493, 494, 497-498
Introduction to General Systems Thinking (Weinberg), 12, 57
intuitiveness, as a quality characteristic, 309
invalid, in product elements of HTSM, 488
investigating, 119-128
involvement, in subjective testability, 497
invulnerability, as a quality characteristic, 310
irrelevant observation, as cause of intermittent problems, 124
ISO standard 9241-11, 341
isolatability, as an internal quality characteristic, 313
issue reports, 502
IT-ability, as a quality characteristic, 311
iteration, testing and, 52-54
J
Juarrero, Alicia, 460
justified beliefs (probable worlds), 457
justified true beliefs (actual worlds), 458
K
Kafka, Franz, 407
Kahneman, Daniel, 416, 475-476
Kaner, Cem (author), 3-4, 13, 42, 48, 58, 295, 305, 470
keyboard tour, 94
Kiwi Workshop on Software Testing (KWST), 448
Klahr, David (author), 77
Klain, Keith, 204, 415-432

Klein, Gary, 52, 107, 453
knowledge, 281, 454-455, 506-507
Koen, Billy, 58
Kuhn, Thomas (author), 4-5, 75-76
L
lab management, as a test performance skill/tactic, 506
labor, cost of, 233
Law of Requisite Variety, 94
leanness, in project-related testability, 496
learnability, as a quality criteria category in HTSM, 309, 490
learning
to be helpful as a tester, 436-438
as a consideration for adopting tools, 251-252
forever, 443-444
importance of business side, 435-436
overview, 433
"quality processes" interfering with software testing, 438-441
skills and tactics, 504-505
to speak and persuade, 441-443
through play, 433-435
learning curve trap, 241
learning loop, analysis and, 499-500
legibility, testing and, 54-56
legibility trap, 242
lens heuristic, 179-181

Lessons Learned in Software Testing: A Context-Driven Approach (Kaner,
Bach, and Pettichord), 470
lifecycle, in product elements of HTSM, 488
lightweight usability test, 340
limit failures, 100-102
literate observation, 67, 506
little, in product elements of HTSM, 488
live oracle, 106
Llama, 262
localizability, as a quality criteria category in HTSM, 491
localization, as an internal quality characteristic, 313
localized versions, testing, 246
log analysis, 65, 120
logging tools, 249
low alignment, 200-201
low expectations, in value-related testability, 496
low-tech testing dashboard, 196-198
ludic fallacy, 36-37
M
maintainability, 311, 313, 491
maintenance costs, of automation, 233
maintenance trap, 240
management, how testing looks to, 415-432
manual testers, 219
manual testing, 65, 468
Many Factors at a Time approach, 65, 120

Marick, Brian, 160
mass editing tools, 250
master classes, 469
matching strategy, 139
mathematics and logic skills, 508
matrices, in project environment of HTSM, 485
measurement, testing and, 71-73
Measuring and Managing Performance in Organizations (Austin), 428
media, in project environment of HTSM, 486
mediated products, 66
medical industry, characteristic contexts of, 41
Meerts, Joris, 365
memorability, as a quality characteristic, 309
memory, bad, as cause of intermittent problems, 124
mental models, 33, 434
mention in passing (MIP), 130-131
methodology, testing and, 41-43
metrics, 210-211, 293-297
Microsoft, 354-356
mind-mapping, 375
mindopsy, 325-326
mindset, in RST, 456-459
minimalism, as a quality characteristic, 309
misattribution, as cause of intermittent problems, 124
misreporting, 263
misrepresentation, as cause of intermittent problems, 124

mission, 39-41, 150, 484, 495
mnemonic, 58
mobile industry, characteristic contexts of, 41
models
defined, 33
developing, 306-313
as a test performance skill/tactic, 505
testing, 32-34
tools and support for, 249-250
using, 314-315
modularity, as an internal quality characteristic, 313
monitorability, as an internal quality characteristic, 313
motivated quality, of testers, 10
motivating risk, as an element of tests, 89
moving boundaries, as a boundary-testing heuristic, 100
multimedia, in product elements of HTSM, 487
multi-platform automation, 246
multiuser behavior, 246
multiuser features, tools and, 249
multi-user/social, in product elements of HTSM, 487
Murdoch, Steven, 391
N
Nadelman, Jeff, 272-274
narrow testing, broad testing compared with, 21-22, 141
National Institute of Standards and Technology, 173
natural, contrived compared with, 144-145

negative testing, 144
negatives, false, 86
Nicieja, Kamil, 226-227
Nielsen, Jacob, 340
noise, in product elements of HTSM, 488
nondeterministic behavior, 245
nondeterministic outputs, features with, 284
non-executable files, in product elements of HTSM, 487
normal, abnormal compared with, 142
normal behavior, 143
normal bug request, 118-119
normal input, 143-144
normal usage, 145-146
normality, 142-148
Normality: A Critical Genealogy (Cryle and Stephens), 147
normalization, 69, 424
normalize input, for bugs, 120
notetaking, 193-194, 506
O
obscurity trap, 239
observability, as an internal quality characteristic, 313, 497
observation, 117, 124
Obstacles, in PROOF heuristic, 194
occasional functions, as cause of intermittent problems, 126
offboarding cost, tools for, 251
off-the-shelf standard system, 348-349

Okada, Takeshi, 77
omega testers, 235
One Factor at a Time approach, 64-65, 120
"The Ongoing Revolution in Software Testing" (Kaner), 305
operability, in HTSM, 309, 490
operating costs, of automation, 233-234
operating effort, for adopting tools, 250-251
operating system compatibility, in HTSM, 312, 490
operations, in HTSM, 488-489
opportunity costs, of automation, 234
oracles
as an element of tests, 89
authority of, 496
availability of, 496
improving, 233
inexpensiveness of, 496
overview, 187
precision of, 496
reliability of, 496
simplistic, 263
unreliable, 124-125
Oshry, Barry, 198-199
ostrich effect, 68
Outlook, in PROOF heuristic, 194
output, 223-228, 260, 487, 489
output checks, 501

outsider stance, 62
overflow, as cause of intermittent problems, 126
P
paradigms, of testing, 4
parallel oracle, 106
Pareto heuristic, 140, 229, 244
Parnas, David (computer scientist), 23
Parnism, 23
Past, in PROOF heuristic, 194
patches, as a quality criteria category in HTSM, 491
pathetic compliance, 117
PEANUTS mnemonic, 143
people, 278-280, 504
people knowledge, as a testing skill/tactic, 507
Perfect Software: And Other Illusions About Testing (Weinberg), 12
perform thread, of goal-oriented activities, 60
performance, 20, 232, 249, 311, 358, 491
Perimeter Assumption heuristic, 99, 183
periodic review, 233
Perplexity, 262
persistent, in product elements of HTSM, 487
personas, creating, 341-342
persuade, learning to, 441-443
Pettichord, Bret, 470
Pfungst, Oskar, 126-127
PHOSITA principle, 46

Pickering, Andrew, 76
pipeline, bug, 134-136
piracy resistance, as a quality characteristic, 311
placation, 285
platform, 125, 488
plausible worlds (true beliefs), 457-458
play, learning through, 433-435
pleasing, disturbing compared with, 143
poor observation, during testing, 117
Popper, Karl (philosopher), 76, 138
portability, as a quality criteria category in HTSM, 313, 491
positive testing, 144
positives, false, 86
POSIWID, 460
possible worlds (unconstrained beliefs), 457
post hoc fallacy, 68-69
Post Office Horizon scandal, 379-399
power, as a consideration for adopting tools, 249-250
practical testability, 493
practices, for test strategy, 167-187
practitioners, as testers, 28-30
praxis, 452-453
prepare thread, of goal-oriented activities, 60
preserving data, 359
preset, in product elements of HTSM, 487
pressure, crushing, during testing, 117

primary functional testing, 186
primary test tooling, 83, 187, 230, 485
primary testing, 156, 263
priming effect, 68
prior knowledge of quality, in epistemic testability, 495
privacy, 310, 490
probability neglect, 69-70
probable worlds (justified beliefs), 457
problem detection, 248, 506
Problem Detection (Klein), 107
problematic patterns, in events, 358-359
The Problems of Philosophy (Russell), 32
procedure document, 108
procedures, 90, 503
process integrity maximization, as a testing consideration, 110
process story, 191
processes, 41-43, 360, 438-441
product analysis, 92
Product Coverage Outline (PCO), 153, 167-169, 297-300, 366-367, 502
product elements, in HTSM, 482, 486-489
Product Elements list, 363
product factor, 92
Product Factors list, 91, 170
product footprint, 488, 490
product interaction, testing tools used in, 221
product knowledge, 497, 506

product learning, 186
product risks, 171, 502, 505
product self-consistency oracle pattern, 106-107
product space, 77-78
product story, 191, 502, 504
product technology, compatibility with, 251
product-factoring, 92
productivity paradox, 267
products, 35-36, 283, 501-502, 505
professionalism, as a quality characteristic, 310
Program Test Methods (Gruenberger), 11-12
programmers, testers compared with, 462-464
programming, 18-19
progressive data corruption, as cause of intermittent problems, 126
progressive destabilization, as cause of intermittent problems, 126
progressive reports, 196
project cycle, strategy throughout on, 156-159
project environment, in HTSM, 482, 484-486
Project Environment list, 170, 363
project knowledge, as a testing skill/tactic, 506
project testability, 495
project-related testability, 318, 493, 494, 495-496
promoting testability, 237
PROOF heuristic, 194

prospective testing
cheat sheet for, 288-291
constructive disengagement, 275-276
overview, 156, 271-275
people involved in, 278-280
process of, 278-287
uses for, 277-278
protocols, 503, 506
prototyping, 232
provisioning costs, tools for, 251
prudence, in project-related testability, 495
The Psychology of Computer Programming (Weinberg), 12
purpose, in project environment of HTSM, 486
purpose oracle pattern, 107
'the purpose of a System Is What It Does' (POSIWID), 460
PUST Police Investigation System, 340
Q
quality, 36-39
quality characteristics, 305-315
Quality Criteria Categories, 363, 482, 489-491
Quality Criteria list, 170
quality mindset, 271
quality processes, 438-441
Quality School, 4

questioning, bad, 263
R
R language, 253
radical fragility, AI and, 260
random data, as a boundary-testing heuristic, 99
Rapid Software Testing (RST)
appeal of, 395-399
Bach's experience with, 446-448
becoming an instructor, 467-476
clients as testers, 28-30
core ideas, 453-461
design features, 25-26
experts as testers, 30-32
foundational ideas, 26-73
how we see, 61-73
managing, 39-61
overview, 6-7, 15
practitioners as testers, 28-30
signals-based testing and, 361-364
students as testers, 30-32
testers, 27-32
vision for, 22-25
what to study, 32-39
rapid usability testing, 339-351
rapport, in project environment of HTSM, 485
rathole trap, 242

"A Rational Design Process: How and Why to Fake It" paper (Parnas), 23
readability, as an internal quality characteristic, 313
readiness, s a role expectation, 44
reading code, as a boundary-testing heuristic, 100
reality
of products, 81-83
in risk-based testing, 171-172
of test data, 249
of testing environment, 249
Recognition-Primed Decision-making, 52
recordkeeping, 233, 503
recoverability, as a quality characteristic, 308
redundant, varied compared with, 140
refactorability, as an internal quality characteristic, 313
reference documentation, tools for, 251
regression automation, 186
regression testing, 156-157, 158-159, 186
Regular events path, in ACRCUF heuristic, 332
regulatory implications, features with, 283
reification fallacy, 208
relational tacit knowledge, 46, 47
Release Coverage Outline (RCO), 207, 300
reliability, 249, 308-309, 489
remedial testing, 157, 354
remove steps, for bugs, 120

reporting
adding filters to, 360-361
bugs, 128-134
improving, 233
in SBTM, 210
testing tools used in, 222
tools and, 249
representativeness bias, 67
re-running old tests, 159
resiliency, sensitivity and, 224
resource knowledge, as a testing skill/tactic, 507
resource utilization, as a quality characteristic, 311
resources, 126, 504
responsible tester, 29
responsiveness to bug reports, 252, 311
results, 194, 359
retaining, discarding compared with, 142
Rethinking Expertise (Collins), 423-424
return on investment, 72
reusability, as an internal quality characteristic, 313
review, in SBTM, 211
Richard, Paul, 18
risk, 36-39, 171-179
risk analysis, 173-175
risk focus, bad, 263
risk gap, 493

risk testing, 483
roadmap, of testing, 479-480
robots, 216-217
robustness, 232, 308, 489
Rockin' and Testing All Over the World blog, 449
role knowledge, as a testing skill/tactic, 507
roles, 43-45, 459-460, 504
Roseberry, Wayne, 285
Rumble Strip, 101-102
running tests, 347-348
Russell, Bertrand (author), 32
S
Sabourin, Rob, 448
safety language, 195, 309, 489
sampling process, 94-95
sandboxing, 157-158, 496
Sanders, Gary, 413
satisfaction, as a quality characteristic, 310
scalability, 311, 490
scenarios, 225, 484
schedule, in project environment of HTSM, 485-486
School Platform, 340
Schoots, Huib, 175, 365
science, testing as, 499
scientific industry, characteristic contexts of, 41
scientific method, 76-77

scope, in project environment of HTSM, 486
scratch notes, 193
screen scrapers, for testing, 66
scripting trap, 238
SDK, in product elements of HTSM, 488
secrecy, as a quality characteristic, 310
secret boundaries/conditions, as cause of intermittent problems, 127
The Secrets of Consulting (Bach), 428
security, 310-311, 490
security holes, as a quality criteria category in HTSM, 310, 490
security-related, in product elements of HTSM, 487
Seeing Systems (Oshry), 198-199
selection bias, 69
Selendic, Djuka, 322
self-care, as a self-management skill and tactic, 503
self-criticism, as a self-management skill and tactic, 503
self-management loop, analysis and, 500-501
self-management skills and tactics, 502-503
self-taught AI-generated workload, 355-356
semi-quantitative risk analysis, 173
Sensemake subprocess, 114
sensemaking, 66-70, 505
sensitivity, resiliency and, 224
sequences, in product elements of HTSM, 488
service, in product elements of HTSM, 487
Session-Based Test Management (SBTM), 209-212

setup & admin (S-time) metric, 211
SFDIPOT mnemonic, 305, 316, 366-367, 374
Shadish, William, 70
shallow agreement, 277-278
shallow testing, 20-21, 139, 141
shallowness trap, 239-240
The Shape of Actions (Collins), 405
shift left, 286
shutdown, in product elements of HTSM, 487
signals, 356-359, 485
signals-based testing
analyzing signals, 356-359
assertion failures, 357-358
crashes, 356-357
feature coverage, 356
getting started, 359-361
hangs, 356-357
at Microsoft, 354-356
overview, 353
performance markers, 358
problematic patterns in events, 358-359
with RST, 361-364
self-taught AI-generated workload, 355-356
signals for testing, 353-354
similarity, in intrinsic testability, 498
Simon, Herbert, 52, 340

simplicity, 251, 313
simplify input, for bugs, 120
simulating users, 247-249
Six Thinking Hats (de Bono), 330
skill backtracking, 330
skill branching, 330
skills, 9-10, 57-59
sky thinking, 330
slow, in product elements of HTSM, 489
smallness, in intrinsic testability, 498
Snapshots of the Mind (Klein), 453
Snowden, Dave, 452, 459
"So You Think You Can Test" talk (Bach), 471-472
social features, tools and, 249
social intrusiveness, AI and, 261
social loafing, 9
social science skills, 507
sociology, software testing and, 401-414
software, 401-414, 488
solo testing, team testing compared with, 141
somatic tacit knowledge, 45-46
source code, 90
sources, of signals, 359
spatial sight, 199
speak, learning to, 441-443
special, standard compared with, 146

Specification by Example (Adzic), 227-228
specification writing skills, 508
speed blink, 231
spiral inquiry, for bugs, 119-120
spontaneity, testing and, 50-52
spontaneous testing, deliberative testing compared with, 141
stability, 308, 313
stance, testing and, 61-63
standard, 146, 148
standards conformance, 312, 486
startup, in product elements of HTSM, 487
statutes and standards oracle pattern, 107
stealth testing, 201
steeplechasing, as a boundary-testing heuristic, 99
Steinhardt, Nicolae, 452
Stephens, Elizabeth, 147
Stevens, P. J., 225
stopping heuristics, as a self-management skill and tactic, 503
stories, 56-57, 89, 310
storytelling, 190-198
strategy, testing and, 56-57
stress, 308, 483
stressful, easy compared with, 143-144, 148
structure, 138-148, 486-487
Structure, Functions, Data, Interfaces, Platform, Operations, and Time
(SFDIPOT), 305, 316, 366-367, 374

The Structure of Scientific Revolutions (Kuhn), 4-5
students, as testers, 30-32
subjective testability, 318, 493, 494, 497
subpostmasters/subpostmistresses (SPMs), 380-395
substance, of test cases, 207
success conditions, establishing, as a self-management skill and tactic, 503
sufficiency, as a quality criteria category in HTSM, 489
suggestive evidence, 340
sunk cost bias, 252
sunk cost trap, 241
supportability, 251-252, 312, 491
supporting tester, 29, 497
supportive culture, in project-related testability, 495
survey testing, 81, 153, 185, 367
sustainability, as a quality characteristic, 312
The Symbolic Species: The Co-evolution of Language and the Brain
(Deacon), 453
sympathetic testing, 143
system interfaces, in product elements of HTSM, 488
system requirements, as a quality criteria category in HTSM, 311, 491
systematic prospective testing, 277
systems, 32-34
T
Tacit and Explicit Knowledge (Collins), 408, 475-476
tacit background knowledge, 106
tacit knowledge, testing and, 45-48

tailorability, as a quality characteristic, 309
Taleb, Nicholas, 62
team testing, solo testing compared with, 141
technical knowledge, 497, 502, 506
telescoping reports, 195-196
test automation
applying blink oracles, 231-232
augmented experiential testing, 222-223
classic traps of, 217-243
considerations for adopting tools, 249-252
considerations for GUI-level, 243-249
golden rule of tool adoption, 252-254
hidden costs of, 232-235
oversimplified output checking, 223-228
overview, 215
recruiting dedicated toolsmiths, 235-236
robots, 216-217
testers working with toolsmiths, 236-237
traps of alignment, 241-243
traps of economy, 239-241
traps of ignorance, 237-239
trouble with, 217-237
test cases, 204, 205, 208
test charters, creating, 369-370
test conditions, 92, 502

test coverage
boundaries, 98-100
equivalence classes, 95-98
improving, 233
incidental, 93-94
intentional, 92
limit failures, 100-102
overview, 90-92
sampling process, 94-95
test data, 249, 263, 502
test design, 249-250, 485
test estimation, 502
test execution (T-time) metric, 210-211, 485
test framing
activity-based management, 208-213
artifact-based management, 204-208
as a collaboration skills/tactic, 504
faking testing, 213-214
managing stories, 198-204
overview, 56, 189
storytelling, 190-198
test ideas, 90, 113, 118, 501
test items, in project environment of HTSM, 486
test metadata, adding to synthetic workloads, 360
test oracles, 103-108, 250
test performance skills and tactics, 505-506

test procedures, 108, 109-118, 250
test processes, 251, 502
test reporting
activity-based management, 208-213
artifact-based management, 204-208
as a collaboration skills/tactic, 504
coverage, 297-300
creating better reports, 300-304
faking testing, 213-214
managing stories, 198-204
metrics, 293-297
overview, 189, 293
storytelling, 190-198
test results, 501

test strategy
for builders, 159-166
centering oneself, 148-149
determining, as a test performance skill/tactic, 505
developing, 149
entry points for, 150-156
organizing according to risk, 171-179
overview, 137-138, 502
practices for, 167-187
structure of, 138-148
in subjective testability, 497
testability and, 163-164
for testers, 159-166
throughout project cycle, 156-159
test team, in project environment of HTSM, 485
test technique knowledge, as a testing skill/tactic, 507
Test Techniques list, 170, 363
test tools, 219, 502

testability
advocacy, as a test performance skill/tactic, 506
advocating for, 320-322
as an internal quality characteristic, 312-313
dynamics of, 494-495
guidewords for analyzing, 495-498
heuristics of, 493-498
ideas, 501
as a job, 322-323
overview, 151, 317-320
in product elements of HTSM, 487
in project environment of HTSM, 486
promoting, 237
as a quality characteristic, 311
as a quality criteria category in HTSM, 491
test strategy and, 163-164
testability trap, 240
testable element, 92
tested risk, 39

testers
as an element of tests, 89
defined, 17-18
guiding, as a collaboration skills/tactic, 504
handling resistance to questions, 284-286
learning to be helpful as, 436-438
programmers compared with, 462-464
qualities of, 9-11
serving, as a collaboration skills/tactic, 504
skills of, 507-508
teams and, 502
test strategy for, 159-166
working with toolsmiths, 236-237

testing
augmented experiential, 66, 222-223
checking compared with, 18-20, 454
conducting, 370-373
constraints on, 151
cultures of, 3-6
deep compared with shallow, 20-21
elements of excellent, 499-508
faking, 213-214
fundamental method of, 77-84
GenAI and, 11
how it looks to management, 415-432
importance of, 153-154
meaning of, 17-22
narrow compared with broad, 21-22
performing tests compared with, 209
primary, 156
prospective, 156
reasons for, 7-9
regression, 156-157, 158-159
remedial, 157
re-running old tests, 159
roadmap of, 479-480
as science, 499
signals for, 353-354
skills and tactics for, 502-508

traditional, 11-13
testing and quality story, in HTSM, 482
Testing Computer Software (Kaner), 13
testing environment, realism of, 249
testing "freshness" indicator, 223
testing loop, analysis and, 499-500
testing process
anatomy of checks, 84-88
anatomy of tests, 89-118
bug pipeline, 134-136
bug reporting, 118-134
fundamental method of testing, 77-84
overview, 75-76
scientific method, 76-77
test coverage, 90-102
test oracles, 103-108
test procedures, 108-118
testing skill, in subjective testability, 497
testing story, 502
testing strategy, creating, 367-369
testing tools, incorporating, 229-237
"Testopsies: Dissecting Your Testing" video, 325
testopsy, 325

tests
anatomy of, 89-118
infrastructure and lab procedures, 502
item availability, in project-related testability, 496
running, 347-348
status evaluation, as a self-management skill and tactic, 503
tools for developing, 250
tools for repair and maintenance, 250
them, us compared with, 145
Thinking, Fast and Slow (Kahneman), 475-476
Thread-Based Test Management (TBTM), 212-213
threat, as a component of product risk, 175
three-level testing story, 191-192
throughput, as a quality characteristic, 311
time
minimization of, as a testing consideration, 109-110
in project-related testability, 496
time as input, 127, 489
time-related, in product elements of HTSM, 487
timing lottery, as cause of intermittent problems, 127
titles, in basic formal bug reports, 131-132
tolerance for failure, in epistemic testability, 495

tools
adding filters to, 360-361
applying, as a test performance skill/tactic, 506
availability of, in project-related testability, 496
considerations for adopting, 249-252
golden rule of adopting, 252-254
knowledge of, as a testing skill/tactic, 507
Toolsmith heuristic, 235
toolsmiths, 219, 235-237
tours, 94-95
traceability, 56, 312
traceable data, for bugs, 120
trading zones, testing and, 43-45
traditional testing, 11-13
transfer costs, of automation, 234
transformations, in product elements of HTSM, 487
transparency, 251, 313
transpection, 264-267
traps of alignment, 241-243
traps of economy, 239-241
traps of ignorance, 237-239
tree-type notes (TTN), 334-336
trespassing system, as cause of intermittent problems, 125
triage, in risk-based testing, 171-172
troubleshooting, 116-118, 251-252, 312
true beliefs (plausible worlds), 457-458

trust, doubt compared with, 140
trusting trap, 238
trustworthiness, as a quality characteristic, 309
truth is manifest, 138-140
tunnel vision, 459
tutorial documentation, tools for, 252
"Two Futures of Software Testing" talk (Bolton), 470
typical, 145-147
U
UI tour, 94
unbugginess, in intrinsic testability, 498
unconscious bias, testing and, 66-70
unconstrained beliefs (possible worlds), 457
Unexpected things path, in ACRCUF heuristic, 332
uninstallations, as a quality criteria category in HTSM, 311, 491
uninterrupted person-time, in SBTM, 210
uniqueness, as a quality criteria category in HTSM, 310, 490
unknown risk, 39
unrecoverability, 102
unreliable communication, as cause of intermittent problems, 125
unreliable oracle, as cause of intermittent problems, 124-125
unsystematic testing, 81
unusual, 145-146, 148
upgrades, as a quality criteria category in HTSM, 311, 491
us, them compared with, 145

usability
checking, 349-351
checklist of, 350
failing to test for, 340-345
in HTSM, 490
improving, 232
as a quality characteristic, 309
tests for, 346-347
user availability, 251, 497
user desires oracle pattern, 106
user environment availability, in value-related testability, 497
user environment stability, in value-related testability, 497
user familiarity, in value-related testability, 496
user goals, creating, 341-342
user interfaces, 313, 488
user journeys, 247-248
user simulation automation, 230
user stability and unity, in value-related testability, 496
user testing, 483
users
overview, 341-345
in product elements of HTSM, 488
recruiting, 346
simulating, 247-249
testing, 35-36
types of, 247

V
validity assessment, as a test performance skill/tactic, 506
value, 140, 360
value story, 191-192
valueopsy, 325
value-related testability, 318, 493, 495, 496-497
varied, redundant compared with, 140
varying screen geometry, 245-246
versatility, as an internal quality characteristic, 312
versions, 132, 233, 250
vibe code, 236
video recording, for testing, 65
viewport analysis, as a boundary-testing heuristic, 100
virus-free, as a quality characteristic, 310
visual test strategy, 153
volatility, in project environment of HTSM, 486
von Clausewitz, Carl (military theorist), 137-138
vulnerability, as a component of product risk, 175
W
wandering attention, during testing, 117
warning signs, 101-102
Waterfall process, 23
weasel words, 195
Web Content Accessibility Guidelines (WCAG), 350-351
Web use, as a learning skill/tactic, 504
Weinberg, Jerry (author), 12, 36, 56, 57-58, 148, 266

Weinberg on Writing (Weinberg), 331
white-box testing, 34
wishful claims, AI and, 260-261
wishful thinking, 377
Wittgenstein, Ludwig, 4
words, in RST, 453-454
workloads, 354, 355-356, 359, 360, 361
world oracle pattern, 106
worst case scenario, 102
Writing Great Specifications (Nicieja), 226-227
Z
zoom blink, 231
OceanofPDF.com

Copyright © 2026 by John Wiley & Sons, Inc. All rights reserved, including rights for text and data
mining and training of artificial intelligence technologies or similar technologies.
Published by John Wiley & Sons, Inc., Hoboken, New Jersey.
Published simultaneously in Canada and the United Kingdom.
ISBNs: 9781394253197 (Hardback), 9781394253210 (ePDF), 9781394253203 (ePub)
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any
form or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise,
except as permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either
the prior written permission of the Publisher, or authorization through payment of the appropriate
per-copy fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923,
(978) 750-8400, fax (978) 750-4470, or on the web at www.copyright.com. Requests to the Publisher
for permission should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111
River Street, Hoboken, NJ 07030, (201) 748-6011, fax (201) 748-6008, or online at
www.wiley.com/go/permission.
The manufacturer's authorized representative according to the EU General Product Safety Regulation
is Wiley-VCH GmbH, Boschstr. 12, 69469 Weinheim, Germany, e-mail:
Product_Safety@wiley.com.
Trademarks: WILEY and the Wiley logo are trademarks or registered trademarks of John Wiley &
Sons, Inc. and/or its affiliates, in the United States and other countries, and may not be used without
written permission. All other trademarks are the property of their respective owners. John Wiley &
Sons, Inc. is not associated with any product or vendor mentioned in this book.
Limit of Liability/Disclaimer of Warranty: While the publisher and authors have used their best
efforts in preparing this book, they make no representations or warranties with respect to the
accuracy or completeness of the contents of this book and specifically disclaim any implied
warranties of merchantability or fitness for a particular purpose. No warranty may be created or
extended by sales representatives or written sales materials. The advice and strategies contained
herein may not be suitable for your situation. You should consult with a professional where
appropriate. Further, readers should be aware that websites listed in this work may have changed or
disappeared between when this work was written and when it is read. Neither the publisher nor
authors shall be liable for any loss of profit or any other commercial damages, including but not
limited to special, incidental, consequential, or other damages.
For general information on our other products and services or for technical support, please contact
our Customer Care Department within the United States at (800) 762-2974, outside the United States
at (317) 572-3993 or fax (317) 572-4002. For product technical support, you can find answers to
frequently asked questions or reach us via live chat at https://support.wiley.com.
If you believe you've found a mistake in this book, please bring it to our attention by emailing our
reader support team at wileysupport@wiley.com with the subject line "Possible Book Errata
Submission."
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print
may not be available in electronic formats. For more information about Wiley products, visit our web
site at www.wiley.com.
Library of Congress Control Number: 2024938971
Cover image: © iMrSquid/Getty Images
Cover design: Wiley

OceanofPDF.com

 
To Marius Francu and Huib Schoots, without whom this project would
never have started—nor would we have ever stopped
OceanofPDF.com

About the Authors
James Bach is Principal Consultant at Satisfice, Inc. He is an
internationally acclaimed innovator and coach in the field of software
testing. A son of author Richard Bach, he got into computing as a self-
taught teenage video game programmer. In 1987, he became the youngest
manager in the Apple Computer R&D division when they inexplicably
hired him to run a test team. He has been a tester ever since. He spent the
bulk of the '90s managing testing in Silicon Valley, but for the last twenty-
five years, he has devoted himself to perfecting his craft: teaching,
consulting, and occasionally serving as an expert witness on court cases
involving software quality. James is an author of two books, Lessons
Learned in Software Testing and Secrets of a Buccaneer-Scholar (a
handbook for being weirdly self-educated). He is a founder of the Context-
Driven school of testing and a charter member of the Association for
Software Testing.
Michael Bolton is Principal Consultant at DevelopSense, based in Toronto.
After a career in theatre, he became a database programmer more or less by
accident. That led to an eight-year stint as a technical support person, tester,
and program manager at Quarterdeck, where he managed the company's
flagship products and directed project and testing teams, both in-house and
around the world. In 2001, he met James Bach, and he taught Rapid
Software Testing for the first time in 2004. In 2006, he became co-author of
the RST methodology. Since then, he has traveled more than a million
miles, teaching and consulting in over 40 countries around the world.
OceanofPDF.com

Acknowledgments
Other than the direct contributors to this book, there are many indirect
contributors whom we must honor.
This project went a full year past the deadline. Well, quality matters, you
know! But it was hard on our wives. James's wife, Lenore, and Michael's
wife, Mary Alton, excused us from household labors and family events, as
well as brought things to sustain us as we debated for endless hours.
Thanks!
To Mary, as my business partner, artist, designer of the RST logo and our
three Web sites, consultant, advisor marketer, travel arranger, travelling
companion — and also as my tirelessly supportive wife and the mother of
our kids — I cannot begin to thank you enough. You've done more than
anyone else will ever know. You are everything to me, and I love you.
Our collaborations with Jerry Weinberg and Cem Kaner played a huge role
in shaping our attitudes and habits of thought.
We also thank our managing editor, Navin Vijayakumar, who was
amazingly patient with authors who refused to tell him when we would be
done! And Kathryn Hogan, our long-suffering project manager. For a
trouble-free endgame we thank our two sharp-eyed copy editors, John
Sleeva and Evelyn Wellborn for their respectful suggestions. We thank
Archana Pragash for her soothing professionalism during the proofing
stage. And thank you, Pete Gaughan: You were like one of those Jedi
Knight ghosts that shows up to give a Force nudge every once in a while.
Yeray Cabello, Alexander Carlsson, Aleks Simic, Mahesh Mallikarjunaiah,
and Cristina Sipos engaged with James in several hundred hours of testing
experimentation that helped him turn tacit ideas into explicit prose.
Michael and James belong to a software testing craftmanship coterie called
The Band. We debuted and debated content for this book in many Band
meetings. Members of The Band who are not already listed contributors to
this book are Mario Colina, John Creson, Dwayne Green, Sandro Ibig,
Melanie Miller, Rob Sabourin, Djuka Selendic, Ben Simo, Nicholas

Snogren, Damian Synadinos, Claudiu-Marian Stoianof, Yuri Velijanski, and
Adam White.
Finally, there are those who influenced us in some other way that improved
our ideas: Ajay Balamurugadas, Bram van den Berg, Baldvin Gislason
Bern, Fiona Charles, Baldvin Gislason Bern, Fiona Charles, Caleb Crandall,
Pascal Dufour, Eugene Elizondo, Ben Fellows, Luke Hohmann, Griffin
Jones, Maria Kedemo, Willem Keesman, Bart Knaack, Boyd Kronenberg,
Cindy Lawless, James Lyndsay, Sigita Matulaityte, Grigori Melnik,
Michael Mintz, Rahul Parwal, Linda Paustian, Geert Peeters, Valentijn
Peters, Connor Roberts, Rosibel Ramirez, Anna Royzman, Joe Sevits,
Pradeep Soundararajan, Sebastian Stautz, Rob van Steenbergen, Marko
Stojanovic, Mike Vaida, Alexandra Vasile, and Vincent Wijnen.
—James Bach and Michael Bolton
OceanofPDF.com

WILEY END USER LICENSE AGREEMENT
Go to www.wiley.com/go/eula to access Wiley's ebook EULA.
OceanofPDF.com


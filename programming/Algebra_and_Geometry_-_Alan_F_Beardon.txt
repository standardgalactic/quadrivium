
ALGEBRA AND GEOMETRY
This text gives a basic introduction and a unified approach to algebra and
geometry. It covers the ideas of complex numbers, scalar and vector
products, determinants, linear algebra, group theory, permutation groups,
symmetry groups and various aspects of geometry including groups of
isometries, rotations and spherical geometry. The emphasis is always on the
interaction between these topics, and each one is constantly illustrated by
using it to describe and discuss the others. Many of the ideas are developed
gradually throughout the book. For example, the definition of a group is
given in Chapter 1 so that it can be used in a discussion of the arithmetic of
real and complex numbers; however, many of the properties of groups are
given later, and at a time when the importance of the concept has become
clear. The text is divided into short sections, with exercises at the end of
each one.

ALGEBRA AND GEOMETRY
ALAN F. BEARDON

CAMBRIDGE UNIVERSITY PRESS
Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, São Paulo, Delhi, Dubai, Tokyo
Cambridge University Press
The Edinburgh Building, Cambridge CB2 8RU, UK
Published in the United States of America by Cambridge University Press, New York
www.cambridge.org
Information on this title: www.cambridge.org/9780521813624
© Cambridge University Press 2005
This publication is in copyright. Subject to statutory exception and to the provisions of relevant
collective licensing agreements, no reproduction of any part may take place without the written
permission of Cambridge University Press.
First published 2005
Fourth printing 2010
Printed in the United Kingdom at the University Press, Cambridge
A catalogue record for this publication is available from the British Library
Library of Congress Cataloguing in Publication data
ISBN 978-0-521-81362-4 hardback
ISBN 978-0-521-89049-6 paperback
Cambridge University Press has no responsibility for the persistence or accuracy of URLs for
external or third-party internet websites referred to in this publication, and does not guarantee that
any content on such websites is, or will remain, accurate or appropriate. Information regarding prices,
travel timetables and other factual information given in this work are correct at the time of first
printing but Cambridge University Press does not guarantee the accuracy of such information
thereafter.

To Dylan, Harry, Fionn and Fenella

Contents
 
Preface
1        Groups and permutations
1.1     Introduction
1.2     Groups
1.3     Permutations of a finite set
1.4     The sign of a permutation
1.5     Permutations of an arbitrary set
2        The real numbers
2.1     The integers
2.2     The real numbers
2.3     Fields
2.4     Modular arithmetic
3        The complex plane
3.1     Complex numbers
3.2     Polar coordinates
3.3     Lines and circles
3.4     Isometries of the plane
3.5     Roots of unity
3.6     Cubic and quartic equations
3.7     The Fundamental Theorem of Algebra
4        Vectors in three-dimensional space
4.1     Vectors
4.2     The scalar product
4.3     The vector product
4.4     The scalar triple product

4.5     The vector triple product
4.6     Orientation and determinants
4.7     Applications to geometry
4.8     Vector equations
5        Spherical geometry
5.1     Spherical distance
5.2     Spherical trigonometry
5.3     Area on the sphere
5.4     Euler's formula
5.5     Regular polyhedra
5.6     General polyhedra
6        Quaternions and isometries
6.1     Isometries of Euclidean space
6.2     Quaternions
6.3     Reflections and rotations
7        Vector spaces
7.1     Vector spaces
7.2     Dimension
7.3     Subspaces
7.4     The direct sum of two subspaces
7.5     Linear difference equations
7.6     The vector space of polynomials
7.7     Linear transformations
7.8     The kernel of a linear transformation
7.9     Isomorphisms
7.10   The space of linear maps
8        Linear equations
8.1     Hyperplanes
8.2     Homogeneous linear equations
8.3     Row rank and column rank
8.4     Inhomogeneous linear equations
8.5     Determinants and linear equations
8.6     Determinants

9        Matrices
9.1     The vector space of matrices
9.2     A matrix as a linear transformation
9.3     The matrix of a linear transformation
9.4     Inverse maps and matrices
9.5     Change of bases
9.6     The resultant of two polynomials
9.7     The number of surjections
10      Eigenvectors
10.1   Eigenvalues and eigenvectors
10.2   Eigenvalues and matrices
10.3   Diagonalizable matrices
10.4   The Cayley-Hamilton theorem
10.5   Invariant planes
11      Linear maps of Euclidean space
11.1   Distance in Euclidean space
11.2   Orthogonal maps
11.3   Isometries of Euclidean n-space
11.4   Symmetric matrices
11.5   The field axioms
11.6   Vector products in higher dimensions
12      Groups
12.1   Groups
12.2   Subgroups and cosets
12.3   Lagrange's theorem
12.4   Isomorphisms
12.5   Cyclic groups
12.6   Applications to arithmetic
12.7    Product groups
12.8   Dihedral groups
12.9   Groups of small order
12.10 Conjugation
12.11 Homomorphisms
12.12 Quotient groups

13      Möbius transformations
13.1   Möbius transformations
13.2   Fixed points and uniqueness
13.3   Circles and lines
13.4   Cross-ratios
13.5   Möbius maps and permutations
13.6   Complex lines
13.7   Fixed points and eigenvectors
13.8   A geometric view of infinity
13.9   Rotations of the sphere
14      Group actions
14.1   Groups of permutations
14.2   Symmetries of a regular polyhedron
14.3   Finite rotation groups in space
14.4   Groups of isometries of the plane
14.5   Group actions
15      Hyperbolic geometry
15.1   The hyperbolic plane
15.2   The hyperbolic distance
15.3   Hyperbolic circles
15.4   Hyperbolic trigonometry
15.5   Hyperbolic three-dimensional space
15.6   Finite Möbius groups
Index

Preface
 
Nothing can permanently please, which does not contain in itself the reason why it is so, and not
otherwise
S.T. Coleridge, 1772-1834
The idea for this text came after I had given a lecture to undergraduates on
the symmetry groups of regular solids. It is a beautiful subject, so why was
I unhappy with the outcome? I had covered the subject in a more or less
standard way, but as I came away I became aware that I had assumed
Euler's theorem on polyhedra, I had assumed that every symmetry of a
polyhedron extended to an isometry of space, and that such an isometry was
necessarily a rotation or a reflection (again due to Euler), and finally, I had
not given any convincing reason why such polyhedra did actually exist.
Surely these ideas are at least as important (or perhaps more so) than the
mere identification of the symmetry groups of the polyhedra?
The primary aim of this text is to present many of the ideas and results
that are typically given in a university course in mathematics in a way that
emphasizes the coherence and mutual interaction within the subject as a
whole. We believe that by taking this approach, students will be able to
support the parts of the subject that they find most difficult with ideas that
they can grasp, and that the unity of the subject will lead to a better
understanding of mathematics as a whole. Inevitably, this approach will not
take the reader as far down any particular road as a single course in, say,
group theory might, but we believe that this is the right approach for a
student who is beginning a university course in mathematics. Increasingly,
students will be taking more and more courses outside mathematics, and the
pressure to include a wide spread of mathematics within a limited time
scale will increase. We believe that the route advocated above will, in
addition to being educationally desirable, help solve this problem.
To illustrate our approach, consider once again the symmetries of the five
(regular) Platonic solids. These symmetries may be viewed as examples of

permutations (acting on the vertices, or the faces, or even on the diagonals)
of the solid, but they can also be viewed as finite groups of rotations of
Euclidean 3-space. This latter point of view suggests that the discussion
should lead into, or away from, a discussion of the nature of isometries of
3-space, for this is fundamental to the very definition of the symmetry
groups. From a different point of view, probably the easiest way to identify
the Platonic solids is by means of Euler's formula for the sphere. Now
Euler's formula can be (and here is) proved by means of spherical geometry
and trigonometry, and the requisite formulae here are simple (and
important) applications of the standard scalar and vector product of the
'usual' vectors in 3-space (as studied in applied mathematics). Next, by
studying rotation groups acting on the unit sphere in 3-space one can prove
that the symmetry groups of the regular solids are the only finite groups of
rotations of 3-space, a fact that it not immediately apparent from the
geometry. Finally, by using stereographic projection (as appears in any
complex analysis course that acknowledges the point at infinity) the
symmetry groups of the regular solids appear as the only finite groups of
Möbius transformations acting in hyperbolic space. Moreover in this guise
one can also introduce rotations of 3-space in terms of quaternions which
then appear as 2-by-2 complex matrices.
The author firmly believes that this is the way mathematics should be
introduced, and moreover that it can be so introduced at a reasonably
elementary level. In many cases, students find mathematics difficult
because they fail to grasp the initial concepts properly, and in this approach
preference is given to understanding and reinforcing these basic concepts
from a variety of different points of view rather than moving on in the
traditional way to provide yet more theorems that the student has to try to
cope with from a sometimes uncertain base.
This text includes the basic definitions, and some early results, on, for
example, 
groups, 
vector 
spaces, 
quaternions, 
eigenvectors, 
the
diagonalization of matrices, orthogonal groups, isometries of the complex
plane and of Euclidean space, scalar and vector products in 3-space,
Euclidean, spherical and (briefly) hyperbolic geometries, complex numbers
and Möbius transformations. Above all, it is these basic concepts and their
mutual interaction which is the main theme of this text.
Finally an earlier version of this book can be freely downloaded as an
html file from http://www.cambridge.org/0521890497. This file is under

development and the aim is to create a fully linked electronic textbook.

1
Groups and permutations
1.1 Introduction
This text is about the interaction between algebra and geometry, and central
to this interaction is the idea of a group. Groups are studied as abstract
systems in algebra; they help us to describe the arithmetic structure of the
real and complex numbers, and modular arithmetic, and they provide a
framework for a discussion of permutations of an arbitrary set. Groups also
arise naturally in geometry; for example, as the set of translations of the
plane, the rotations of the plane about the origin, the symmetries of a cube,
and the set of all functions of the plane into itself that preserve distance. We
shall see that geometry provides many other interesting examples of groups
and, in return, group theory provides a language and a number of
fundamental ideas which can be used to give a precise description of
geometry. In 1872 Felix Klein proposed his Erlangen Programme in which,
roughly speaking, he suggested that we should study different geometries
by studying the groups of transformations acting on the geometry. It is this
spirit that this text has tried to capture.
We shall assume familiarity with the most basic facts about elementary
set theory. We recall that if X is any set, then x ∈ X means that x is an
element, or member, of X, and x ∉ X means that x is not an element of X.
The union X ∪ Y of two sets X and Y is the set of objects that are in at least
one of them; the intersection X ∩ Y is the set of objects that are in both. The
difference set X\Y is the set of objects that are in X but not in Y. The empty
set  is the set with no elements in it; for example, X\X =  for every set X.
We say that X is non-empty when X ≠ .

In this chapter we shall define what we mean by a group, and then show
that every non-empty set X has associated with it a group, which is known
as the group of permutations of X. This basic fact underpins almost
everything in this book. We shall also carry out a detailed study of the
group of permutations of the finite set {1, 2, . . . , n} of integers. In Chapter
2 we review the algebraic properties of the real numbers in terms of groups,
but in order to give concrete examples of groups now, we shall assume (in
the examples) familiarity with the real numbers. Throughout the book we
use  for the set of integers, 
 for the set of rational numbers, and 
 for
the set of real numbers.
1.2 Groups
There are four properties that are shared by many mathematical systems and
that have proved their usefulness over time, and any system that possesses
these is known as a group. It is difficult to say when groups first appeared
in mathematics for the ideas were used long before they were synthesized
into an abstract definition of a group. Euler (1761) and Gauss (1801)
studied modular arithmetic (see Section 2.4), and Lagrange (1770) and
Cauchy (1815) studied groups of permutations (see Section 1.3). Important
moves towards a more formal, abstract theory were taken by Cauchy
(1845), von Dyck (1882) and Burnside (1897), thus group theory, as we
know it today, is a relative newcomer to the history of mathematics.
First, we introduce the notion of a binary operation on a set X. A binary
operation ∗ on X is a rule which is used to combine any two elements, say
x and y, of X to obtain a third object, which we denote by x∗y. In many
cases x∗y will also be in X, and when this is so we say that X is closed with
respect to ∗. We can now say what we mean by a group.
Definition 1.2.1 A group is a set G, together with a binary operation ∗ on
G which has the following properties:
(1)  for all g and h in G, g∗h ∈ G;
(2)  for all f, g and h in G, f ∗(g∗h) = (f ∗g)∗h;
(3)  there a unique e in G such that for all g in G, g∗e = g = e∗g;
(4)  if g ∈ G there is some h in G such that g∗h = e = h∗g.

A set X may support many different binary operations which make it a
group so, for clarification, we often use the phrase 'X is a group with
respect to ∗'. Property (1) is called the closure axiom for it says that G is
closed with respect to ∗. Property (2) is the associative law, and this says
that f ∗ g ∗ h is uniquely defined regardless of which of the two operations
∗ we choose to do first. The point here is that as ∗ only combines two
objects at a time, we have to apply ∗ twice (in some order) to obtain f
∗g∗h. There are exactly two ways to do this, and (2) says that these two
ways must yield the same result. Obviously, this idea extends to more
elements, and reader should now use (2) to verify that the element f
∗g∗h∗i is defined independently of the order in which the three
applications of ∗ are carried out. It is important to understand that the
associative law is not self-evident; indeed, if a∗b = a/b for for positive
numbers a and b then, in general, (a∗b)∗c ≠ a∗(b∗c).
The element e in (3) is the identity element of G, and the reader should
note that (3) requires that both e∗g and g∗e are g. In the example just
considered (where a∗b = a/b) we have a∗1 = a but 1∗a ≠ a (unless a = 1).
We also note that in conjunction with (1) and (2), we could replace (3) by
the weaker statement that there exists some e in G such that g∗e = g = e∗g
for every g in G. Indeed, suppose that G contains elements e and e′ such
that, for all g in G, g∗e = g = e∗g and g∗e′ = g = e′∗g. Then e′ = e∗e′ = e
so that e′ = e (so that such an element is necessarily unique). It follows that
when we need to prove that, say, G is a group we need only prove the
existence of some element e in G such that e∗g = g = g∗e for every g (and
it is not necessary to prove the uniqueness of e). However, we cannot
replace (3) by this weaker version of (3) in the definition of a group without
making (4) ambiguous.
The element h in (4) is the inverse of g, and henceforth will be written as
g−1. However, before we can legitimately speak of the inverse of g, and use
the notation g−1, we need to show that each g has only one inverse.
Lemma 1.2.2 Let G be any group. Then, given g in G, there is only one
element h that satisfies (4). In particular, (g−1)−1 = g.
Proof Take any g and suppose that h and h′ satisfy h∗g = e = g∗h and
h′∗g = e = g∗h′. Then

as required. As g∗g−1 = e = g−1∗g, it is clear that (g−1)−1 = g.
The next three results show that one can manipulate expressions, and
solve simple equations, in groups much as one does for real numbers.
Lemma 1.2.3 Suppose that a, b and x are in a group G. If a∗x = b∗x then
a = b. Similarly, if x∗a = x∗b then a = b.
Proof 
If 
a∗x 
= 
b∗x 
then 
(a∗x)∗x−1 
= 
(b∗x)∗x−1. 
Now
(a∗x)∗x−1 = a∗(x∗x−1) = a∗e = a, and similarly for b instead of a; thus
a = b. The second statement follows in a similar way. For obvious reasons,
this result is known as the cancellation law.
Lemma 1.2.4 Suppose that a and b are in a group G. Then the equation
a∗x = b has a unique solution in G, namely x = a−1∗b. Similarly, x∗a = b
has a unique solution, namely b∗a−1.
Proof As a∗(a−1∗b) = (a∗a−1)∗b = e∗b = b, we see that a−1∗b is a
solution of a∗x = b. Now let y1 and y2 be any solutions. Then
a∗y1 = b = a∗y2 so that, by Lemma 1.2.3, y1 = y2. The second statement
follows in a similar way.
Lemma 1.2.5 In any group G, e is the unique solution of x∗x = x.
Proof As y∗e = y for every y, we see that e∗e = e. Thus e is one solution of
x∗x = x. However, if x∗x = x then x∗x = x∗e so that from Lemma 1.2.3,
x = e.
The reader should note that the definition of a group does not include the
assumption that f ∗g = g∗ f; indeed, there are many interesting groups in
which equality does not hold. However, this condition is so important that it
carries its own terminology.

Definition 1.2.6 Let G be a group with respect to ∗. We say that f and g in
G commute if f ∗g = g∗ f. If f ∗g = g∗ f for every f and g in G, we say
that G is an abelian, or a commutative, group. We often abbreviate this to
'G is abelian'.
Several straightforward examples of groups are given in the Exercises. We
end this section with an example of a non-commutative group.
Example 1.2.7 Let G be the set of functions of the form f (x) = ax + b,
where a and b are real numbers and a ≠ 0. It is easy to see that G is a group
with respect to the operation ∗ defined by making f ∗g the function f
(g(x)). First, if g(x) = ax + b and h(x) = cx + d, then g∗h is in G because
(g∗h)(x) = g(h(x)) = acx + (ad + b). It is also easy (though tedious) to
check that for any f, g and h in G, f ∗(g∗h) = (f ∗g)∗h. Next, the function
e(x) = 1x + 0 is in G and satisfies e∗ f = f = f ∗e for every f in G. Finally, if
g(x) = ax + b then g∗g−1 = e = g−1∗g, where g−1(x) = x/a − b/a. We have
shown that G is a group, but it is not abelian as f ∗g ≠ g∗ f when f
(x) = x + 1 and g(x) = −x + 1. In the same way we see that the set of
functions of the form f (x) = ax + n, where a = ±1 and n is an integer is also
a non-abelian group.
Exercise 1.2
1.  Show that the set  is a group with respect to addition. Show also that
the set of positive real numbers is a group with respect to multiplication.
2.  Let  be the set of rational numbers (that is, numbers of the form m/n
where m and n are integers and n ≠ 0), and let 
 and 
 be the set of
positive, and non-zero, rational numbers, respectively. Show that , but
not 
, is a group with respect to addition. Show that 
 and 
 are
groups with respect to with respect to multiplication, but that  is not. Is
the set of rational numbers of the form p/q, where p and q are positive
odd integers, a group with respect to multiplication?
3.  Show that , with the operation ∗ defined by m∗n = m + n + 1, is a
group. What is the identity element in this group? Show that the inverse
of n is −(n + 2).

4.  Show that , with the operation m∗n = m + (−1)mn, is a group. Show
that in this group the inverse n−1 of n is (−1)n+1n. For which n is n−1 = n?
5.  Let G = {x ∈ 
 : x ≠ −1}, where  is the set of real numbers, and let
x∗y = x + y + x y, where x y denotes the usual product of two real
numbers. Show that G with the operation ∗ is a group. What is the
inverse 2−1 of 2 in this group? Find (2−1)∗6∗(5−1), and hence solve the
equation 2∗x∗5 = 6.
6.  For any two sets A and B the symmetric difference A∆B of A and B is the
set of elements in exactly one of A and B; thus
(see Figure 1.2.1). Let Ω be a non-empty set and let G be the set of
subsets of Ω (note that G includes both the empty set  and Ω). Show
that G with the operation ∆ is a group with  as the identity element of
G. What is A−1? Now let Ω = {1, 2, . . . , 7}, A = {1, 2, 3}, B = {3, 4, 5}
and C = {5, 6, 7}. By considering A−1 and B−1, solve the two equations
A∆X = B, and A∆X∆B = C.
Figure 1.2.1
1.3 Permutations of a finite set
We shall now discuss permutations of a non-empty set X. We shall show (in
Section 1.5) that the permutations of X form a group, and we shall use this
to examine the nature of the permutations. This is most effective when X is
a finite set, and we shall assume that this is so during this and the next
section. Before we can consider permutations we need to understand what
we mean by a function and, when it exists, its inverse function. As (for the

moment) we are only considering functions between finite sets, we can
afford to take a fairly relaxed view about functions; a more detailed
discussion of functions (between arbitrary sets) is given in Section 1.5.
A function f : X → X from a finite set X to itself is a rule which assigns to
each x in X a unique element, which we write as f (x), of X. We can define
such a function by giving the rule explicitly; for example, when X = {a, b,
c} we can define f : X → X by the rule f (a) = b, f (b) = c and f (c) = a. Note
that f cyclically permutes the elements a, b and c, and this is our first
example of a permutation. Two functions, say f : X → X and g : X → X are
equal if f (x) = g(x) for every x in X, and in this case we write f = g. The
identity function I : X → X on X is the function given by the rule I (x) = x
for all x in X.
Suppose now that we have two functions f and g from X to itself. Then
for every x in X there is a unique element g(x) in X, and for every y in X
there is a unique element f (y) in X. If we choose x first, and then take
y = g(x), we have created a rule which takes us from x to the element f
(g(x)). This rule defines a function which we denote by f g : X → X. We call
this function the composition (or sometimes the product) of f and g, and it is
obtained by applying g first, and then f. This function is sometimes denoted
by f ◦ g, but it is usual to use the less cumbersome notation fg.
Given a function f : X → X, the function g : X → X is the inverse of f if,
for every x in X, we have f (g(x)) = x and g( f (x)) = x, or, more succinctly, if
f g = I = g f, where I is the identity function on X. It is important to note that
not every function f : X → X has an inverse function. Indeed, f has an
inverse function precisely when, for every y in X, there is exactly one x in X
such that f (x) = y, for then the inverse function is the rule which takes y
back to x. We say that a function f : X → X is invertible when the inverse of
f exists, and then we denote the inverse by f−1. Note that if f is invertible,
then so is f−1, and ( f−1)−1 = f. We are now ready to define what we mean by
a permutation of a set X.
Definition 1.3.1 A permutation of X is an invertible map f : X → X. The set
of permutations of X is denoted by (X).
Theorem 1.3.2 The set 
(X) of permutations of a finite non-empty set X is
a group with respect to the composition of functions.

We remark that it is usual to speak of the product of permutations rather
than the composition of permutations.
Proof We must show that the operation ∗ defined on 
(X) by f ∗g = f g
(the composition) satisfies the requirements of Definition 1.2.1. First, we
show that ∗ is associative. Let f, g and h be any functions, and let u = g f
and v = hg. Then, for every x in X,
(1.3.1)
This shows that h(gf) = (hg) f and, as a consequence of this, we can now use
the notation hgf (without brackets) for the composition of three (or more)
functions in an unambiguous way.
Next, the identity map I : X → X is the identity element of 
(X) because
if f is any permutation of X, then f I = f = I f ; explicitly, for every x, f I
(x) = f (x) = I ( f (x)). Next, if f is any permutation of X, then f is invertible,
and the inverse function f−1 is also a permutation of X (because it too is
invertible). Moreover, f−1 is the inverse of f in the sense of groups because f
f−1 = I = f−1 f. Finally, suppose that f and g are permutations of X. Then f g is
invertible (and so is a permutation of X) with inverse g−1 f−1; indeed
and similarly, (g−1 f−1)(fg) = I. This completes the proof.
Examples of permutation groups will occur throughout this text.
However, for the rest of this and the next section we shall focus on the
group of permutations of the finite set {1, 2, . . . , n} of integers.

Definition 1.3.3 The symmetric group Sn is the group of permutations of {1,
. . . , n}.
As a permutation ρ is a function we can use the usual notation ρ(k) for the
image of an integer k under ρ. However, it is customary, and convenient, to
write ρ in the form
where the image ρ(k) of k is placed in the second row underneath k in the
first row; for example, the permutation β of {1, 2, 3, 4} such that β(1) = 4,
β(2) = 2, β(3) = 1 and β(4) = 3 is denoted by
It is not necessary to order the columns according to the natural order of the
top row, and we may use any order that we wish; for example,
A permutation ρ is said to fix k, and k is a fixed point of ρ, if ρ(k) = k. By
convention, we may omit any integers in the expression for ρ that are fixed
by ρ (and any integers that are omitted in this expression may be assumed to
be fixed by ρ). For example, if ρ is a permutation of {1, . . . , 9}, and if
then ρ interchanges 1 and 8, and 3 and 7, and it fixes 2, 4, 5, 6 and 9.
If α and β are permutations of {1, . . . , n} then αβ is the permutation
obtained by applying β first and then α. The following simple example
illustrates a purely mechanical way of computing this composition: if

then (re-arranging α so that its top row coincides with the bottom row of β,
and remembering that we apply β first) we have
Note that αβ ≠ βα (that is, α and β do not commute). We shall now define
what we mean by disjoint permutations, and then show that disjoint
permutations commute.
Definition 1.3.4 We say that two permutations α and β are disjoint if, for
every k in {1, . . . , n}, either α(k) = k or β(k) = k.
Theorem 1.3.5 If α and β are disjoint permutations then αβ = βα.
Proof Take any k in {1, . . . , n}. As either α or β fixes k we may suppose
that α(k) = k. Let k′ = β(k); then α(β(k)) = α(k′) and β(α(k)) = β(k) = k′ so we
need to show that α fixes k′. This is true (by assumption) if β does not fix k′,
so we may suppose that β fixes k′. But then β(k) = k′ = β(k′), and applying
β−1, we see that k = k′, so again α fixes k′.
A permutation that cyclically permutes some set of integers is called a
cycle. More precisely, we have the following definition.
Definition 1.3.6 The cycle (n1 . . . nq) is the permutation
Explicitly, this maps nj to nj+1 when 1 ≤ j < q, and nq to n1, and it fixes all
other integers in {1, . . . , n}. We say that this cycle has length q, or that it is
a q-cycle.
Notice that we can write a cycle in three different ways; for example,

To motivate the discussion that follows, observe that if
then (by inspection) σ = (1 5 4)(2 7 6 3) and so, by Theorem 1.3.5,
We shall now show that this is typical of all permutations. Take any
permutation ρ of {1, . . . , n}, and any integer k in this set. By applying ρ
repeatedly we obtain the points k, ρ(k), ρ2(k), . . ., and as two of these points
must coincide, we see that there are integers p and q with ρp(k) = ρq(k)
where, say, q < p. As ρ−1 exists, ρp−q(k) = k. Now let u be the smallest
positive integer with the property that ρu(k) = k; then the distinct numbers k,
ρ(k), ρ2(k), . . . , ρu−1(k) are cyclically permuted by ρ. We call
(1.3.2)
the orbit of k under ρ. Now every point m in {1, . . . , n} lies in some orbit
(which will have exactly one element if and only if ρ fixes m), and it is
evident that two orbits are either identical or disjoint. Thus we can write
(1.3.3)
where the orbits O(ki) are pairwise disjoint sets, and where each of these
sets is cyclically permuted by ρ. We call (1.3.3) the orbit-decomposition of
{1, . . . , n}.
Each orbit O(k) in (1.3.2) provides us with an associated cycle
Note that ρ and ρ0 have exactly the same effect on the integers in O(k), but
that ρ0 fixes every integer that is not in O(k). Now consider the
decomposition (1.3.3) of {1, . . . , n} into mutually disjoint orbits, and let ρj
be the cycle associated to the orbit O(kj). Then it is clear that the cycles ρj
are pairwise disjoint (because their corresponding orbits are); thus they

commute with each other. Finally, if x ∈ Oj, then ρj(x) = ρ(x), and ρi (x) = x
if i ≠ j, so that ρ = ρ1 · · · ρm. We summarize this result in our next theorem.
Theorem 1.3.7 Let ρ be a permutation of {1, . . . , n}. Then ρ can be
expressed as a product of disjoint (commuting) cycles.
It is evident that the expression ρ = ρ1 · · · ρm that was derived from the orbit
decomposition (1.3.3) is unique up to the order of the 'factors' ρj. Indeed if
ρ = μ1 · · · μv, where the μi are pairwise disjoint cycles, then the set of points
not fixed by μi, constitutes an orbit for ρ, so that μi must be some ρj. In
particular, the number m of factors in this product is uniquely determined
by ρ, and we shall return to this later. We pause to name this representation
of ρ.
Definition 1.3.8 The representation ρ = ρ1 · · · ρm which is derived from the
orbit decomposition (1.3.3), and which is unique up to the order of the
factors ρj, is called the standard representation of ρ as a product of cycles.
Let us illustrate these ideas with an example. Consider
as a permutation of {1, . . . , 9}. The orbits of ρ are {1, 7, 2}, {3, 8}, {4}
and {5, 6, 9}, and the standard representation of ρ as a product of disjoint
cycles is (1 7 2)(3 8)(4)(5 6 9).
There is an interesting corollary of Theorem 1.3.7. First, if μ is a cycle of
length k, then μk (that is, μ applied k times) is the identity map. Suppose
now that ρ = ρ1 · · · ρm is the standard representation of ρ, and let d be any
positive integer. As the ρj commute, we have
It follows that if d is the least common multiple of q1, . . . , qm, where qj is
the length of the cycle ρj, then ρd = I. For example if ρ = (1 3 4)(2 9 5 6)(7
8), then ρ12 = I. In fact, it is not difficult to see that the least common

multiple d of the qj is the smallest positive integer t for which ρt = I. As d
divides n!, this shows that ρn! = I for every permutation ρ of {1, . . . , n}.
Exercise 1.3
1.  Show that
2.  Show that (1 2 3 4) = (1 4)(1 3)(1 2). Express (1 2 3 4 5) as a product of
2-cycles. Express (1 2 . . . n) as a product of 2-cycles.
3.  Express the permutation
as a product of cycles, and hence (using Exercise 1.3.2) as a product of
2-cycles. Use this to express ρ−1 as a product of 2-cycles.
4.  Show that the set {I, (1 2)(3 4), (1 3)(2 4), (1 4)(2 3)} of permutations is
a group.
5.  Suppose that the permutation ρ of {1, . . . , n} satisfies ρ3 = I. Show that
ρ is a product of 3-cycles, and deduce that if n is not divisible by 3 then ρ
fixes some k in {1, . . . , n}.
1.4 The sign of a permutation
A 2-cycle (r s) (which interchanges the distinct integers r and s and leaves
all other integers fixed) is called a transposition. Notice that (r s) = (s r),
and that (r s) is its own inverse. Common experience tells us that any
permutation can be achieved by a succession of transpositions, and this
suggests the following result.
Theorem 1.4.1 Every permutation is a product of transpositions.
Proof As every permutation is a product of cycles, and as for distinct
integers ai we have (by inspection)

(1.4.1)
the result follow immediately.
In fact (1.4.1) leads to the following quantitative version of Theorem
1.4.1.
Theorem 1.4.2 Let ρ be a permutation acting on {1, . . . , n}, and suppose
that ρ partitions {1, . . . , n} into m orbits. Then ρ can be expressed as a
composition of n − m transpositions.
Proof Let ρ = ρ1 · · · ρm be the standard representation of ρ as a product of
disjoint cycles, and let nj be the length of the cycle ρj. Thus ∑j nj = n = n. If
nj ≥ 2 then, from (1.4.1), ρj can be written as a product of nj − 1
transpositions. If nj = 1 then ρj is the identity, so that no transpositions are
needed for this factor. However, in this case nj − 1 = 0. It follows that we
can express ρ as a product of ∑j(nj - 1) transpositions, and this number is
n − m.
We come now to the major result of this section, namely the number of
transpositions used to express a permutation ρ as a product of
transpositions. Although this number is not uniquely determined by ρ, we
will show that its parity (that is, whether it is even or odd) is determined by
ρ. First, however, we prove a preliminary result.
Lemma 1.4.3 Suppose that the identity permutation I on {1, 2, . . . , n} can
be expressed as a product of m transpositions. Then m is even.
Proof The proof is by induction on n, and we begin with the case n = 2. In
this case we write I = τ1 · · · τm, where each τj is the transposition (1 2). As
(1 2)m = (1 2) if m is odd, we see that m must be even, so the conclusion is
true when n = 2.
We now suppose that the conclusion holds when the permutations act on
{1, 2, . . . , n − 1}, and consider the situation in which I = τ1 · · · τm, where
each τj is a transposition acting on {1, . . . , n}. Clearly, m ≠ 1, thus m ≥ 2.

Suppose, for the moment, that τm does not fix n. Then, for a suitable choice
of a, b and c, we have one of the following situations:
It follows that we can now write I as a product of m transpositions in which
the first transposition to be applied fixes n (this was proved under the
assumption that τm(n) ≠ n, and I is already in this form if τm(n) = n). In other
words, we may assume that τm(n) = n. We can now apply the same
argument to τ1 · · · τm−1 (providing that m − 1 ≥ 2), and the process can be
continued to the point where we can write I = τ1 · · · τm, where each of τ2,
. . . , τm fixes n. But then τ1 also fixes n, because
Thus, we can now write I = τ1 · · · τm, where each τj is a transposition acting
on {1, . . . , n − 1}. The induction hypothesis now implies that m is even and
the proof is complete.
The main result now follows.
Theorem 1.4.4 Suppose that a permutation ρ can be expressed both as a
product of p transpositions, and also as a product of q transpositions. Then
p and q are both even, or both odd.
Proof Suppose that τ1 · · · τp = σ1 · · · σq, where each τi and each σj is a
transposition. Then σqσq−1 · · · σ1τ1 · · · τp = I, so that, by Lemma 1.4.3, p + q
is even. It follows from this that p and q are both even, or both odd.
As an example, consider the permutation ρ = (1 3 5)(2 4 6 8)(7) acting on
{1, . . . , 8}. Here, n = 8 and N (ρ) = 3 so that, by Lemma 1.4.3, ρ can be
expressed as a product of five transpositions. Theorem 1.4.4 now implies
that if we write ρ as a product of tranpositions in any way whatsoever, then

there will necessarily be an odd number of transpositions in the product.
This discussion suggests the following definition.
Definition 1.4.5 The sign ε(ρ) of a permutation ρ is (−1)q, where ρ can be
expressed as a product of q transpositions. We say that ρ is an even
permutation if ε(ρ) = 1, and an odd permutation if ε(ρ) = −1.
Observe from (1.4.1) that if ρ is a p-cycle then ε(ρ) = (−1)p+1; thus a cycle
of even length is odd, and a cycle of odd length is even. If the permutations
α and β can be expressed as products of p and q transpositions, respectively,
then the composition αβ can be expressed as a product of p + q
transpositions; thus the next two results are clear.
Theorem 1.4.6 If α and β are permutations, then ε(αβ) = ε(α)ε(β). In
particular, ε(α) = ε(α−1).
Theorem 1.4.7 The product of two even permutations is an even
permutation. The inverse of an even permutation is an even permutation.
More generally, the set of even permutations in Sn is a group.
Definition 1.4.8 The alternating group An is the group of all even
permutations in Sn.
It is easy to find the number of elements in the symmetric group Sn and in
the alternating group An.
Theorem 1.4.9 The symmetric group Sn has n! elements, and the
alternating group An has n!/2 elements.
Proof Elementary combinatorial arguments show that Sn has exactly n!
elements for, in order to construct a permutation of {1, . . . , n}, there are n
ways to choose the image of 1, then n − 1 ways to choose the image of 2
(distinct from the image of 1), and so on. Thus Sn has n! elements.
Now let σ be the transposition (1 2) and let f : Sn → Sn be the function
defined by f (ρ) = σρ. We note that f is invertible, with f−1 = f, because, for
every ρ, f(f(ρ)) = f (σρ) = σ σρ = ρ. It is clear that f maps even permutations

to odd permutations, and odd permutations to even permutations and, as f is
invertible, there are the same number of even permutations in Sn as there are
odd permutations. Thus there are exactly n!/2 even permutations in Sn.
Theorem 1.4.1 says that every permutation is a product of 2-cycles. Are
there any other values of m with the property that every permutation a
product of m-cycles? The answer is given in the next theorem.
Theorem 1.4.10 Let ρ be a permutation of {1, . . . , n}, and let m be an
integer satisfying 2 ≤ m ≤ n. Then ρ is a product of m-cycles if and only if
either ρ is an even permutation, or m is an even integer.
Proof Take any integer m with 2 ≤ m ≤ n. Suppose first that ρ is an even
permutation. The identity
where the ai are distinct (and which can be verified by inspection) shows
that it suffices to express ρ as a product of terms τi τj, where τi and τj are
transpositions with exactly one entry in common. Now as ρ is even it can
certainly be be written as a product of terms of the form τi τj, where each τk
is a transposition, and clearly we may assume that τi ≠ τj. If τi and τj have no
elements in common then we can use the identity
to obtain ρ as a product of the desired terms.
Let us now suppose that ρ is an odd permutation. If ρ is a product of m-
cycles, say, ρ = ρ1 · · · ρt, then
so that m is even. Finally, take any even m, and let σ0 = (1 2 3 · · · m). As σ0
is odd, we see that σ0ρ is even. It follows that σ0ρ, and hence ρ itself, can be
written as a product of m-cycles.
Exercise 1.4

1.  Show that the permutation 
 is odd.
2.  Find all six elements of S3 and determine which are even and which are
odd. Find all twelve even permutations of S4.
3.  The order of a permutation ρ is the smallest positive integer m such that
ρm (that is, ρ applied m times) is the identity map.
(a)  What is the order of the permutation (1 2 3 4)(5 6 7 8 9)?
(b)  Which element of S9 has the highest order, and what is this order?
(c)  Show that every element of order 14 in S10 is odd.
4.  (i) By considering (1 a)(1 b)(1 a), show that any permutation in Sn can
be written as a product of the transpositions (1, 2), (1 3), . . . , (1 n), each
of which may be used more than once.
(ii) Use (i) to show that any permutation in Sn can be written as a product
of the transpositions (1, 2), (2 3), . . . , (n − 1 n), each of which may be
used more than once.
[This is the basis of bell-ringing, for a bell-ringer can only 'change
places' with a neighbouring bell-ringer.]
5.  Show that any subgroup of Sn (that is, a subset of Sn that is a group in its
own right) which is not contained in An contains an equal number of
even and odd permutations.
1.5 Permutations of an arbitrary set
This section is devoted to a careful look at functions between arbitrary sets.
The reader will have already met functions defined by algebraic rules (for
example, x2 + 3x + 5), but we need to understand what one means by a
function between sets in the absence of any arithmetic. We can say that a
function f : X → Y is a rule that assigns to each x in X a unique y in Y and
this seems clear enough, but what do we actually mean by a rule, and why
should it be easier to define a 'rule' than a function? In fact, it is easier to
think about a function in terms of its graph, and this is what we shall do
next. As an example, the graph G(f) of the function f (x) = x2, where x ∈ 
,
is the set

in the plane. Now G(f) contains all the information that there is about the
function f, and therefore, conceptually, it is equivalent to f. However, G(f) is
a set (not a 'rule'), and if we can characterize sets of this form that arise
from functions we can then base our definition of functions on sets and
thereby avoid the problem of defining what we mean by a 'rule'. With this
in mind, we note that G(f) has the important property that for every x in X
there is exactly one y in Y such that (x, y) ∈ G(f). We now define what we
mean by a function, but note that the words 'function', 'map' and
'mapping' are used interchangeably.
Definition 1.5.1 Let X and Y be non-empty sets. A function, map, or
mapping, f from X to Y is a set G(f) of ordered pairs (x, y) with the
properties (a) if (x, y) ∈ G(f) then x ∈ X and y ∈ Y, and (b) for every x in X
there is exactly one y in Y such that (x, y) is in G(f).
Suppose that we are given a function in this sense. Then to each x in X we
can assign the unique y in Y such that (x, y) ∈ G(f); in other words, the set
G(f) can be used in this natural way to define a function from X to Y. We
emphasize, however, the subtle and important point that a function given by
Definition 1.5.1 is based on set theory, and not on a vague idea of a 'rule'.
As usual, we write f : X → Y to mean that f is a function from X to Y, and
we write the unique element y that the rule f assigns to x as f (x). We can
now revert to the more informal and common use of functions safe in the
knowledge that the foundation of this idea is secure. In the case when a
function is given by a simple algebraic rule, for example f (x) = x2, it is
often convenient to use the notation f : x 
 x2, or simply x 
 x2.
Suppose now that we have two functions g : X → Y and f : Y → Z. Then
for every x in X there is a unique element g(x) in Y, and for every y in Y
there is a unique element f (y) in Z. If we choose x, and then take y = g(x)
here we have created a rule which takes us from x in X to the unique
element f (g(x)) in Z. We have therefore created a function which we denote
by f g : X → Z; this function is the composition of f and g and it is obtained
by applying g first, and then f. It is a simple but vitally important fact that
the composition of functions is associative; thus given functions f : X → Y, g
: Y → W and h : W → Z, we have h(g f) = (hg) f. To prove this we need only
check that the argument in (1.3.1) remains valid in this more general

situation. As a consequence, we can now use the notation hg f (without
brackets) for the composition of three (or more) functions in an
unambiguous way.
There is one special case of composition that is worth mentioning, and
which we have already used for permutations. If f : X → X is any function,
then we can form composition f f to give the function x 
 f ( f (x)).
Naturally we denote this by f2. We can then form the composition f2 f (or f
f2); we denote this by f3 so that f3(x) = f ( f ( f (x))). More generally, if f : X
→ X is any function, we can apply f repeatedly and fn is the function
obtained by applying f exactly n times; this is the n-th iterate of f. Notice
that fn(x) is the effect of starting with x and applying f exactly n times; it is
not the n-th power of a number f(x) which we would write as (f(x))n.
For any non-empty set X, we can form the function f : X → X consisting
of all pairs of the form (x, x), where x ∈ X. This function is the rule that
takes every x to itself, and in the usual notation we would write f (x) = x.
This function will play a special role in what follows (it will be the identity
element in a group of transformations of X) so we give it a special symbol,
namely IX. Notice that for any function g : X → Y, we have gIX = g and IY
g = g.
We now discuss conditions under which a function f : X → Y has an
inverse function f−1 : Y → X which 'reverses' the action of f. Explicitly, we
want to find a function g : Y → X such that for all x in X, g f (x) = x and for
all y in Y, f g(y) = y. These two conditions are equivalent to g f = IX and f
g = IY.
Definition 1.5.2 Let f : X → Y be any function. Then a function g : Y → X is
an inverse of f if gf = IX and f g = IY. If an inverse of f exists we say that f is
invertible.
It is clear that there can be at most one inverse of f, for if g : Y → X and h: Y
→ X are both inverses of f, then f g = IY = f h so that g = IXg = g f g = g f
h = IX h = h. Henceforth, when the inverse of f exists, we shall denote it by
f−1. We can now give conditions that guarantee that f−1 exists.

Definition 1.5.3 A function f : X → Y is injective if, for each y in Y, f (x) = y
for at most one x in X.
Definition 1.5.4 A function f : X → Y is surjective if, for each y in Y, f
(x) = y for at least one x in X.
Definition 1.5.5 A function f : X → Y is bijective, or is a bijection, if it is
both injective and surjective; that is, if, for each y in Y, there is exactly one
x in X such that f (x) = y.
Sometimes the term one-to-one is used to mean injective, and f is said to
map X onto Y when f : X → Y is surjective. Note that to show that f is
injective it is sufficient to show that f (x1) = f (x2) implies that x1 = x2.
Theorem 1.5.6 A function f : X → Y is invertible if and only if it is a
bijection. If this is so then f−1 : Y → X is also a bijection.
Proof Suppose first that f : X → Y is bijective; thus, for each y in Y, there is
exactly one x in X with f (x) = y. Let us write this x, which depends on y, by
g(y). Then, by definition, f g(y) = y. Next, take any x0 in X and let y0 = f
(x0). Then, by the definition of g, we have g(y0) = x0 so that g f (x0) = x0.
Thus f in invertible with inverse g.
Now suppose that f−1 : Y → X exists. To check that f is injective we
assume that f (x1) = f (x2) and then show that x1 = x2. This, however, is
immediate for x1 = f−1( f (x1)) = f−1( f (x2)) = x2. To show that f : X → Y is
surjective, consider any y in Y. We want to show that there is some x such
that f (x) = y. As f−1(y) is in X, we can take this as our x and this gives us
what we want as f (x) = f f−1(y) = y. We have now shown that f is invertible
if and only if it is a bijection.
It remains to show that if f is bijective then so is f−1, but this is clear for,
applying what we have just proved about f to the function f−1, we see that
f−1 is bijective if and only if it is invertible. However, it is obvious that f−1 is
invertible if and only f is, and hence if and only if f is bijective.

The following definition is consistent with Definition 1.3.1, and the basic
result about permutations is that they form a group.
Definition 1.5.7 Let X be a non-empty set. A permutation of X is a bijection
of X onto itself.
Theorem 1.5.8 The set 
(X) of permutations of a non-empty set X is a
group with respect to the composition of functions.
Proof Let f and g be permutations of X; then, by Theorem 1.5.6, their
inverses f−1 and g−1 exist. It is easy to check that the composition fg has
inverse g−1 f−1, so that fg is also a permutation of X. Thus 
(X) is closed
under the composition of functions. Next, we have seen that the
composition of functions is associative. The identity map IX : X → X
defined by IX (x) = x is the identity element of 
(X) for IX f = f = f IX for
every f in 
(X). Finally, the inverse function f−1 is a permutation of X
(Theorem 1.5.6), and it is indeed the inverse of f in the sense of group
theory because ff−1 = IX = f−1 f .
We end this chapter by giving the number of different types of functions
from a set X with m elements to a set Y with n elements.
Theorem 1.5.9 Suppose that X and Y have m and n elements, respectively.
Then there are
(a)  nm functions from X to Y ;
(b)  n! bijections from X to Y when m = n, and none otherwise;
(c)  n !/(n − m) ! injections from X to Y when n ≥ m, and none otherwise;
(d)  
 surjections from X to Y when m ≥ n, and none
otherwise.
Proof Each element of X can be mapped to any element of Y ; thus there are
exactly nm functions from X to Y. Clearly, if there is a bijection from X to Y
then m = n. If m = n we write X = {x1, . . . , xn} and we can construct the
general bijection f by taking any one of n choices for f (x1), then any one of

n − 1 remaining choices for f (x2), and so on, and this gives (b). There are
no injections from X to Y unless n ≥ m, so suppose that n ≥ m. Then there
are 
 subsets of Y that have exactly m elements, and hence there are m!
injections m from X to Y ; thus (c) holds.
Finally, we prove (d). It is clear that no such surjective maps exist if m <
n, so we suppose that m ≥ n ≥ 1. Let S(m, k) be the number of surjective
maps from X onto a set with exactly k elements, and note that S(m, 0) = 0.
As there are 
 distinct subsets of Y with exactly k elements, and as any
map from X to Y is a surjective map onto some subset of Y with k elements
(for some k), we see that
We now want a way of solving this system of equations in the 'unknowns'
S(m, k), and this is given by the following 'inversion formula'.
Lemma 1.5.10 Given any sequence of numbers A0, A1, . . . , let
Then
If we apply this with Bk = km and Ak = S(m, k), (d) follows.
The proof of Lemma 1.5.10 We consider the second sum, write each Bk in
terms of the Aj, and then interchange the order of summation. Thus

It is clear that the coefficient of An on the right is one; thus we only have to
show that the coefficients of A0, . . . An−1 are zero. Now for r = 0, . . . ,
n − 1, the coefficient of Ar is λr, say, where
This completes the proof of Lemma 1.5.10 and Theorem 1.5.9.
Exercise 1.5
1.  Show that the map f : 
 defined by f (x) = xn, where n is a positive
integer, is a bijection if and only if n is odd.
2.  Show that the map f : 
 defined by f (x) = 1/x when x ≠ 0, and f
(0) = 0, is a bijection.
3.  Let f : X → Y be any function. Show that
(a)  f is injective if and only if there is a function g : Y → X such that g
f = IX, and

(b)  f is surjective if and only if there is a function h : Y → X such that f
h = IY.
4.  Let f : X → Y and g : Y → Z be given functions. Show that if f and g are
injective, then so is g f : X → Z. Show also that if f and g are surjective,
then so is g f : X → Z.
5.  Consider the (infinite) set  of integers. Show that there is a function f : 
 that is injective but not surjective, and a function g : 
 that
is surjective but not injective. Now let X be any finite set, and let f : X →
X be any function. Show that the following statements are equivalent:
(a)  f : X → X is injective;
(b)  f : X → X is surjective;
(c)  f : X → X is bijective.

2
The real numbers
2.1 The integers
This chapter contains a brief review of the algebraic properties of the real
numbers. As we usually think of the real numbers as the coordinates of
points on a straight line we often refer to the set 
 of real numbers as the
real line. The set 
 carries (and is characterized by) three important
structures, 
namely 
an 
algebraic 
structure 
(addition, 
subtraction,
multiplication and division), an order (positive numbers, negative numbers
and zero), and the least upper bound property (or its equivalent). We shall
take the existence of real numbers, and many of their properties (in
particular, their order, and the existence of the n-th root of a positive
number) for granted, and we concentrate on their algebraic properties. The
rest of this section is devoted to a discussion of the set 
 of integers. We
review the algebraic structure of 
 (using groups) in Section 2.2, and we
introduce the idea of a field (an algebraic structure that has much in
common with 
) in Section 2.3. In Section 2.4 we discuss modular
arithmetic.
We now discuss the set  = {1, 2, 3, . . .} of natural numbers, and the the
set  = {. . . , −2, −1, 0, 1, 2, . . .} of integers. One of the most basic facts
about the integers is the
The Well-Ordering Principle Any non-empty subset of 
 has a smallest
member.
This apparently obvious result justifies the use of induction.

The Principle of Induction I Suppose that A ⊂ , 1 ∈ A, and for every m,
m ∈ A implies that m + 1 ∈ A. Then A = .
This is the set-theoretic version of the (presumably) familar result that if a
statement (n) about n is true when n = 1, and if the truth of 
(m) implies
the truth of (m + 1), the (n) is true for all n. Indeed, if we let A be the set
of n for which (n) is true, the two versions are easily seen to be equivalent
to each other.
Proof Let B be the set of positive integers that are not in A; thus A ∩ B = 
and A ∪ B = . We want to prove that B =  for then, A = . Suppose not,
then, by the Well-Ordering Principle, B has a smallest element, say b. As 1
∈ A we see that 1 ∉ B; thus b ≥ 2 so that b − 1 ≥ 1 (that is, b − 1 ∈ ). As
b is the smallest element in B, it follows that b − 1 ∉ B, and hence that
b − 1 ∈ A. But then, by our hypothesis, (b − 1) + 1 ∈ A, so that b ∈ A. This
is a contradiction (to A ∩ B = ), so that B = , and A = .
There is an alternative version of induction that is equally important (and
the difference between the two versions will be illustrated below).
The Principle of Induction II Suppose that A ⊂ 
, 1 ∈ A, and for every
m, {1, . . . , m} ⊂ A implies that m + 1 ∈ A. Then A = .
Proof Let B be the set of positive integers that are not in A. Suppose that
B = ; then, by the Well-Ordering Principle, B has a smallest element, say
b. As before, b ≥ 2, so that now {1, . . . , b − 1} ⊂ A. With the new
hypothesis, this implies that b ∈ A which is again a contradiction. Thus (as
before) B = , and A = .
We remark that there is a slight variation of each version of induction in
which the process 'starts' at an integer k instead of 1. For example, the first
version becomes: if A ⊂ , k ∈ A, and for every m ≥ k, m ∈ A implies that
m + 1 ∈ A, then A = {k, k + 1, . . .}.
To illustrate the difference between these two versions of induction, let 
(n) be the number of prime factors of n, and let us try to prove, by
induction, that 
(n) < n for n ≥ 2. For the moment, we shall assume

familiarity with the primes, and the factorization of the integers. It is clear
that 
(2) = 1 < 2. Now consider 
(m + 1). If m + 1 is prime then 
(m + 1) = 1 < m + 1 as required. If not, then we can write m + 1 = ab and so
(obviously) 
(m + 1) = 
(a) (b). Now there will be many cases in which
neither a nor b is m, so knowing only that 
(m) is true is of no help.
However, if we know that (ℓ) < ℓ for all ℓ in {2, . . . m}, then
as required. Thus, in some circumstances, we do need the second form of
induction.
One of the earliest recorded uses of induction is in the book
Arithmeticorum Libri Duo, written by Francesco Maurolico in 1575, in
which he uses induction to show that
The term 'mathematical induction' was suggested much later (in 1838) by
Augustus DeMorgan. The two forms of induction, and the Well-Ordering
Principle are all, in fact, equivalent to each other. The logical foundation for
the natural numbers was given, in 1889, by Giuseppe Peano who reduced
the theory to five simple axioms, one of which was the Principle of
Induction.
Let us now consider the divisibility properties of integers. An integer m
divides an integer n, and m is a divisor, or factor, of n, if n = mk for some
integer k. The integer n, where n ≥ 2, has 1 and n as factors, and n is said to
be a prime, or is a prime number, if these are its only positive factors.
Suppose, for example, that a is a positive factor of 2. Then 2 = ab for some
b, so that 2 = ab ≥ a. Thus a is 1 or 2, so that 2 is a prime. As 2 is not a
factor of 3 (else for some a ≠ 1, 3 = 2a ≥ 4), we see that 3 is also prime. The
following important result is proved using the second form of induction.
The Fundamental Theorem of Arithmetic Every integer n, where n ≥ 2,
can be factorized into a product of primes in only one way apart from the
order of the factors.
Proof It is worthwhile to consider this proof in detail. First, we show that
every integer can be expressed as a product of primes. Let A be the set of

integers n, with n ≥ 2, that can be expressed in this way. Obviously, A
contains every prime (for each prime can be considered as a product with
just one term in the product). In particular, 2 ∈ A. Now suppose that 2, . . .
n are all in A, and consider n + 1. If n + 1 is prime, then it is in A. If not,
then we can write n + 1 = ab, where 1 < a < n + 1. It follows that 1 < b <
n + 1, and hence that both a and b are in A. Thus both a and b can be
written as a product of primes, and hence so too can their product n + 1. We
deduce that n + 1 ∈ A so, by the second form of induction, A = {2, 3, . . .}.
In other words, every integer n with n ≥ 2 can be written as a product of
primes.
It remains to prove that each n, where n ≥ 2, can be expressed as such a
product in only one way, up to the order of the factors, and we shall also
prove this by induction. Let A be the set of n (n ≥ 2) that can be factorized
into a product of primes in only one way apart from the order of the factors.
Clearly 2 ∈ A. Now suppose that 2, . . . , n are all in A, and consider n + 1.
If n + 1 is prime then n + 1 ∈ A. If not (as we shall now assume), consider
two possible prime factorizations of n + 1, say n + 1 = p1 . . . pr = q1 . . . qs,
where r and s are at least two. Clearly, we may assume that 2 ≤ p1 ≤ · · · ≤
pr, and similarly for the qi, and that p1 ≤ q1. There are two cases to consider,
namely (i) p1 = q1, and (ii) p1 < q1.
In case (i), p2 . . . pr = q2 . . . qs = m, say, and as 2 ≤ m ≤ n we see that m
∈ A. This means that the two factorizations of m are the same (up to order),
and hence the same is true of n + 1. This in this case, n + 1 ∈ A.
In case (ii), p1 < q1 and we shall now show that this cannot happen.
Assume that p1 < q1; then
As the term on the right is strictly less than n + 1, the induction hypothesis
implies that both sides of this equation have the same prime factors. As p1
< qj for every j, and the qi are primes, this means that p1 must be a prime
factor of q1 − p1. Thus for some m, q1 − p1 = mp1; hence p1 divides q1. As
q1 is prime, and as 2 ≤ p1 < q1 this is a contradiction. Finally, as case (ii)
cannot occur, we must have n + 1 ∈ A and so A = {2, 3, . . .}.
The following two results on integers will be of use later.

Theorem 2.1.1 Let G be a set of integers that is a group with respect to
addition. Then, for some integer k, G = {kn : n ∈ }.
Proof Let e be the identity in G. Then x + e = x for every x in G, and taking
x = e we see that e = 0. If G = {0} then the conclusion holds with k = 0.
Suppose now that G ≠ {0}. Then G contains some non-zero x and its
inverse −x; thus G contains some positive integer. Let k be the smallest
positive integer in G, and let K = {kn : n ∈ }. Obviously k + · · · + k ∈ G;
thus K ⊂ G. Now take any g in G and write g = ak + b, where 0 ≤ b < k. It
follows that b (which is g − ak) is in G, and hence, from the definition of k,
b = 0. Thus g ∈ K so that G ⊂ K . We conclude that G = K .
We say that two integers a and b are coprime if they have no common
factor except 1.
Theorem 2.1.2 Suppose that a and b are coprime integers. Then there are
integers u and v such that au + bv = 1.
Proof Let G = {ma + nb : m, n ∈ }. It is easy to see that G is a group with
respect to addition (we leave the reader to check this) so, by Theorem 2.1.1,
there is a positive integer k such that
As a is in the set on the left (take m = 1 and n = 0) we see that a = kn for
some n. Thus k divides a and, similarly, k divides b. By assumption, a and b
are coprime; thus k = 1 and G = . It follows that 1 ∈ G and this is the
desired conclusion.
Finally, we comment on rational and irrational numbers. The rational
numbers are numbers of the form m/n, where m and n are integers with n ≠
0, and 
 denotes the set of rational numbers. A real number that is not
rational is said to be irrational. Not every real number is rational; for
example, 
 is not. More generally, suppose that n is a positive integer and
that 
 is rational. Then we can write 
 = p/q, where p and q are non-
zero integers. By cancelling common factors we may assume that p and q
have no common prime factor. Now p2 = nq2 so that every prime factor of q

divides p2, and hence (from the Fundamental Theorem of Arithmetic) it also
divides p. We deduce that q has no prime factor; thus q = 1 and n = p2. This
shows that if n is a positive integer then 
 is either an integer or is
irrational. Once the concept of length has been defined it can be shown that
 has length zero; as 
 has infinite length, we see that (in some sense)
'most' real numbers are irrational.
Exercise 2.1
1.  Show that 
 is irrational. Use the prime factorization of integers to
show that if 
 is rational, where p and q are positive integers with
no common factors, then p = r2 and q = s2 for some integers r and s.
2.  Show that if x is rational and y is irrational, then x + y is irrational. Show
that if, in addition, x ≠ 0, then x y is irrational.
3.  Find two irrational numbers whose sum is rational. Find two irrational
numbers whose sum is irrational.
4.  Let a and b be real numbers with a < b. Show that there are infinitely
many rational numbers x with a < x < b, and infinitely many irrational
numbers y with a < y < b. Deduce that there is no smallest positive
irrational number, and no smallest positive rational number.
2.2 The real numbers
Consider the set 
 of real numbers with the usual operations of addition
x + y and multiplication x y. We recognize immediately that 
 is an abelian
group with respect to +; explicitly,
(1a)  if x and y are in , then so is x + y;
(2a)  if x, y and z are in , then x + (y + z) = (x + y) + z;
(3a)  there is a number 0 such that for all x in , x + 0 = x = 0 + x;
(4a)  for each x there is some −x such that x + (−x) = 0 = (−x) + x;
(5a)  for all x and y, x + y = y + x.
Notice that in (4a) we have replaced the symbol g−1 for the inverse of g in
an abstract group by the usual symbol −x.
We leave the reader to verify that the set 
 of non-zero real numbers is
an abelian group with respect to multiplication. Here, the identity element is

1, and the inverse of x is 1/x (which is also written x−1). The operations of
addition and multiplication in 
 are not independent of each other for they
satisfy the Distributive Laws:
One of the fundamental properties of  is that 0x = 0 = x0 for every x. To
see this, note first that as 0 is the additive identity, 0 + 0 = 0. Thus, from the
Distributive Law,
(2.2.1)
If we subtract x0 from both sides of this equation we see that x0 = 0 and, by
commutativity, 0x = x0 = 0. As 0 ≠ 1 this means that there is no real number
y such that y0 = 1 = 0y; thus 0 has no inverse with respect to multiplication,
and this is why we cannot divide by zero. In particular,  is not a group with
respect to multiplication.
Exercise 2.2
1.  Show that the set 
 of non-zero real numbers is a group with respect to
multiplication.
2.  Show that the set of non-zero numbers of the form a + b
, where a and
b are rational, is a group with respect to multiplication.
2.3 Fields
The algebraic properties of 
 described in Section 2.2 are so important that
we give a name to any system that possesess them.
Definition 2.3.1 A field 
 is a set with two binary operations + and ×,
which we call addition and multiplication, respectively, such that
(1)   is an abelian group with respect to + (with identity 0 );
(2)  {x ∈  : x ≠ 0 } is an abelian group with respect to × (with identity 1
);
(3)  the distributive laws hold.

We have seen that 
 with the usual definitions of + and × is a field. The
reader should check that the set 
 of rational numbers is also a field. The
set  of integers is a group with respect to +, but it not a field because ±1
are the only integers whose multiplicative inverse is an integer. The only
sets of integers that are groups with respect to multiplication are {1} and
{−1, 1}. A slightly less trivial example of a field is the set {a + b
 : a, b
∈ 
}, again with the usual definitions of addition and multiplication. We
leave the reader to verify this, with the hint that
Note that a2 − 2b2 ≠ 0 here because otherwise, 
 would be rational.
One important property that is shared by all fields (and which we have
seen that 
 has) is that for all x in , x × 0  = 0  = 0  × x. The proof for a
general field is exactly the same as for ; see (2.2.1).
Exercise 2.3
1.  Let  be a field. Show that the only solutions of the equation x2 = x,
where x is in , are 0  and 1 .
2.  Prove that in any field, x2 − y2 = (x + y) × (x − y). You should state at
each step of your proof which property of the field is being used. Deduce
that in any field, x2 = y2 if and only if x = ±y.
3.  Show that in any field, x3 − y3 = (x − y)(x2 + x y + y2).
2.4 Modular arithmetic
We end this chapter with a discussion of modular arithmetic which provides
us with a rich source of examples. Let n be a positive integer, and let
this is the set of all possible 'remainders' after any integer is divided by n.
Addition modulo n, which we denote by ⊕n, is defined for any integers x
and y as follows. Given x and y, we can write x + y = an + b, where the
integers a and b are uniquely determined by the condition that 0 ≤ b < n, in

other words, by the condition that b ∈ 
. We now define x ⊕n y = b so that
in all cases x ⊕n y ∈ 
. As examples, we have 3 ⊕6 7 = 4 and 0 ⊕n
(−1) = n − 1. Multiplication modulo n, which we denote by ⊗n is defined
similarly, namely we write x y = an + b, where b ∈ 
, and then x ⊗n y = b.
As examples, 6 ⊗7 5 = 2 and 2 ⊗4 8 = 0.
The definitions of ⊕n and ⊗n provide an addition and multiplication on 
 in which x ⊕n y and x ⊗n y are in 
 whenever x and y are. While it is a
straightforward exercise to show that 
 is a group with respect to ⊕ (with
identity 0, and n − x as the inverse of x), the multiplication ⊗n in 
 has
different properties depending on whether or not n is a prime number. We
investigate this in more detail.
If n is not a prime then n = ℓm, say, where ℓ and m are non-zero elements
of 
, and then ℓ ⊗n m = 0. For example, 2 ⊗6 3 = 0. We have just shown
that if n is not a prime, then 
 contains two non-zero elements whose
product is zero. As this cannot be so in any field (see Section 2.3), we
conclude that for non-prime n, 
 is not a field. By contrast, we have the
following result.
Theorem 2.4.1 Let p be a prime; then 
 is a field.
Notice that Theorem 2.4.1 implies that a field need not be an infinite set; for
example, {0, 1, 2} with addition ⊕3 and multiplication ⊗3 is a field (the
reader should verify this directly).
Proof of Theorem 2.4.1 Suppose that p is a prime and that x is a non-zero
number in 
. Then x and p are coprime so, by Theorem 2.1.2, there are
integers u and υ with xu + pυ = 1. Now write u = ap + b, where b ∈ 
.
Then xb + (xa + v) p = 1, so that x ⊗p b = 1 and hence x−1 = b. This shows
that every nonzero element of 
 has an inverse with respect to the
multiplication ⊗p. The rest of the proof is straightforward, and is left to the
reader to complete.
Exercise 2.4

1.  Show that 134 ⊕71 928 = 68, and that 46 ⊗17 56 = 9.
2.  Show that x ⊕n y = (x + y) − n[(x + y)/n], where [x] denotes the integral
part of x.
3.  Find integers u and υ such that 31u + 17υ = 1, and hence find the
multiplicative inverse of 17 in 
.
4.  Does 179 have a multiplicative inverse in 
? If so, find it.
5.  Show that the integer m in 
 has a multiplicative inverse in 
 if and
only if m and n are coprime.
6.  Show that if x ∈ 
 then x2 is 0 or 1 in 
. Deduce that no integer of the
form 4k + 3 can be written as the sum of two squares.
7.  Show that if 3 divides a2 + b2 then it divides both a and b.
8.  Show that for some integer m, 3m = 1 in 
. Deduce that 7 divides
1 + 32001.
9.  Find all solutions of the equation x2 = x in 
. (Note that there are more
than two solutions.)
10.  Show that the equation x−1 + y−1 = (x + y)−1 has no solutions x and y in 
. Show, however, that this equation does have a solution in 
.
11.  Show that the set {2, 4, 8} with multiplication modulo 14 is a group.
What is the identity element? Is {n, n2, n3} a group with respect to
multiplication in 
, where m = n + n2 + n3?
12.  Let f and g be the functions from 
 to itself defined by f (x) = x ⊕8 2
and g(x) = 5 ⊗8 x (that is, f (x) = x + 2 and g(x) = 5x, both modulo 8).
Show that f and g are permutations of 
. Show that f and g commute.
What is the smallest positive integer n such that gn is the identity map?
13.  Let X = {0, 1, 2, . . . , 16}. Express each of the following permutations of
X as a product of disjoint cycles:
(a)  the function f1 defined by f1(x) ≡ x + 5 mod 17;
(b)  the function f2 defined by f2(x) ≡ 2x mod 17;
(c)  the function f3 defined by f3(x) ≡ 3x + 1 mod 17.

3
The complex plane
3.1 Complex numbers
Ordered pairs (x, y) of real numbers x and y arise naturally as the
coordinates of a point in the Euclidean plane, and we shall adopt the view
that the plane is the set of ordered pairs of real numbers. Complex numbers
arise by denoting the point (x, y) by a new symbol x + iy, and then
introducing simple algebraic rules for the numbers x + iy with the
assumption that i2 = −1. A complex number, then, is a number of the form
x + iy, and we stress that this is no more than an alternative notation for (x,
y). Thus we see that x + iy = u + iv if and only if (x, y) = (u, v); that is, if and
only if x = u and y = v. Complex notation has enormous benefits, not least
that while a real polynomial need not have any real roots, it always has
complex roots; for example, x2 + 1 has no real roots but it has complex
roots i and −i. As we identify the point (x, 0) in the plane with the real
number x, so we also identify the complex number x + 0i with x. We denote
the set of complex numbers by .
Historically, the very existence of a number i with i2 = −1 was in doubt,
and for this reason complex numbers were called 'imaginary numbers'. In
1545 Cardan published his book Ars magna in which he discussed solutions
of the simultaneous equations x + y = 10 and x y = 40. He used complex
numbers in a purely symbolic way to obtain the 'complex' solutions 
 of these equations, and later he went on to study cubic equations
(see Section 3.6). Wallis (1616-1703) realized that real numbers could be
represented on a line, and he made the first real attempt to represent
complex numbers as points in the plane. Later, Wessel (1797), Gauss

(around 1800) and Argand (1806) all successfully represented complex
numbers as points in the plane. We call x the real part, and y the imaginary
part, of the complex number x + iy; these terms were introduced by
Descartes (1596-1650) whose name later gave rise to the term cartesian
coordinates. Gauss introduced the term 'complex number' in 1832. For us,
the complex number i is simply the ordered pair (0, 1), which certainly does
exist, and after we have defined multiplication of complex numbers we
shall be able to check without difficulty that i2 = −1.
We shall now introduce the arithmetic of complex numbers. Many
readers will be familiar with vector addition in the plane, namely
and the addition of complex numbers is defined simply by rewriting this in
our new notation; thus
see Figure 3.1.1. With this, it is easy to see that the complex numbers form
a group with respect to addition. The identity is 0 + i0, the additive inverse
of x + iy is −x + i(−y), and subtraction is the addition of the additive inverse.
Figure 3.1.1
To motivate the definition of multiplication of complex numbers we
compute a product of complex numbers in a purely formal way, treating i as
any other number, and assuming that ia = ai for every real a; thus we obtain

To convert this product into a complex number we have to specify the value
of i2 and the whole theory rests on the specification that i2 = −1. Putting
these tentative steps aside, we start afresh and now define the product of
two complex numbers by the rule
(3.1.1)
It is natural to adopt the convention of ignoring 0's and 1's in the obvious
way so that, for example, we write 0 + i0, 0 + iy, x + i0 and x + i1 as 0, iy, x
and x + i, respectively. Strictly speaking, we should check that this
convention will not lead to any inconsistencies in our arithmetic; it does not
and we shall take this for granted. Now, as a consequence of definition
(3.1.1), we have
In fact, (x + iy)2 = −1 if and only if x2 − y2 = −1 and x y = 0; that is, if and
only if x + iy = ±i. In short, the quadratic equation z2 + 1 = 0 has exactly
two roots, namely i and −i.
To find the multiplicative inverse z−1 of a complex number z we adopt a
common strategy in mathematics, namely we first assume that the inverse
exists and then find an explicit expression for it. It is then quite legitimate to
start with this expression (which does exist) and then verify that it has the
desired properties. Given the complex number z = x + iy, then, we assume
that z−1 exists, write z−1 = u + iv, and then impose the condition zz−1 = 1.
This yields
so that xu − yv = 1 and xv + yu = 0. Thus u = x/(x2 + y2) and v = −y/(x2 + y2).
So, for any non-zero z, say z = x + iy (where x2 + y2 ≠ 0), we define
(3.1.2)
It is now easy to verify that zz−1 = 1 = z−1z, so that z−1 is indeed the
multiplicative inverse of z; thus every non-zero complex number has a

multiplicative inverse. Finally, we define division by
(3.1.3)
We have now defined addition and multiplication in , and these obey the
same rules as in 
 with the added convention that i2 = −1. In fact, with
these operations the complex numbers form a field. The verification of this
is tedious but elementary, and we omit the details.
Theorem 3.1.1 The set  of complex numbers is a field with respect to the
addition and multiplication defined above.
The complex conjugate  of z is given by  = x − iy, where z = x + iy, and
geometrically, z and  are mirror images of each other in the real axis. We
leave the reader to verify that
so, in particular, 
 is real and non-negative. If z ≠ 0, and w = 1/z, then 
, so that 
. More generally, if w ≠ 0 then
Next, for any complex numbers a and b with b ≠ 0, we can always write
a/b as 
, where the denominator 
 of the second quotient is real and
positive. As an illustration,
Let z = x + iy, and recall that x and y are the real part and imaginary part
of z, respectively. We denote these by Re[z], and Im[z], and we leave the
reader to check that
The modulus |z| of z is defined by

(3.1.4)
where we take the non-negative square root of the real number x2 + y2. The
basic properties of the modulus are
and these are easily proved. The first inequality in (3.1.5) is true because
and the second inequality is proved similarly. Next, (3.1.6) follows
immediately from the definition  = x − iy, and (3.1.7) holds because, by
(3.1.6),
Finally, (3.1.8) is obvious if z + w = 0. If not, then, from (3.1.5), we have
which is (3.1.8). Of course, repeated applications of (3.1.8) show that
If z = x + iy and w = u + iv, then

so that, by Pythagoras' theorem, |z − w| is the distance between the points z
and w in the plane. Our geometric intuition tells us that the length of any
side of a triangle is at most the sum of the lengths of the other two sides. A
formal statement of this result, which is known as the triangle inequality,
and a proof of it (that does not rely on our intuition) now follows; see
Figure 3.1.2.
Figure 3.1.2
Theorem 3.1.2 For all complex numbers z1, z2 and z3,
(3.1.9)
Proof We simply put z = z1 − z2 and w = z2 − z3 in (3.1.8).
Finally, we mention a useful inequality that is related to (3.1.8), and that
gives a lower bound of |z + w|.
Theorem 3.1.3 For all z and w, |z ± w| ≥ |z| − |w|.
Proof For all z and w we have
This, and a second inequality obtained by interchanging z and w in this,
gives the stated inequality for z + w. If we now replace w by −w we obtain

the inequality for z − w. This second inequality is illustrated in Figure 3.1.3
in the case when |z| |w|.
Figure 3.1.3
Exercise 3.1
1.  Show that i−1 = −i, (1 + i)−1 =1 2 (1 − i), and (1 + i)2 = 2i.
2.  Show that z2 = 2i if and only if z = ±(1 + i).
3.  Show that 
.
4.  Verify directly that zw = 0 if and only if z = 0 or w = 0.
5.  Suppose that zw ≠ 0. Show that the segment joining 0 to z is
perpendicular to the segment joining 0 to w if and only if Re
.
6.  Let T be a triangle in  with vertices at 0, w1 and w2. By applying the
mapping 
, show that the area of T is 
.
7.  Show that for any positive integer n,
Deduce that z3 − w3 = (z − w)3 + 3zw(z − w).
8.  Prove (by induction) the binomial theorem: for any positive integer n,
and any complex numbers z and w,

3.2 Polar coordinates
Given a non-zero complex number z, the modulus |z| of z is the length of the
line segment from 0 to z. The argument arg z of z is the angle θ between the
positive real axis and the segment from 0 to z measured in the anti-
clockwise direction: see Figure 3.2.1. It is clear that if z = x + iy, then
(3.2.1)
where θ = arg z. It is customary to write r = |z| and θ = arg z; then (r, θ) are
the polar coordinates of z. If z = 0 then r = 0 but arg z is not defined. The
transition between the real and imaginary parts x and y of z and the polar
coordinates (r, θ) of z is given by (3.1.4) and (3.2.1).
Figure 3.2.1
It is important to realize that arg z is determined by z from (3.2.1) only to
within an integer multiple of 2π; that is, if θ is one value of arg z, then
θ + 2nπ is another value for any integer n. We can, if we wish, insist that θ
is chosen so as to satisfy 0 ≤ θ < 2π or, if we prefer, −π ≤ θ < π, and in both
cases, the resulting θ would be unique. However, there is no choice of θ that
is universally advantageous, so it is better not to prejudice our thinking by
giving prominence to one choice over any other. We shall agree, then, to
leave arg z determined only up to the addition of an integer multiple of 2π.
Despite this ambiguity (which causes no problems), the values cos (arg z)
and sin (arg z) are uniquely determined by z because the trigonometric

functions cos and sin (whose properties we assume here) are periodic with
period 2π.
Neither one of the equations in (3.2.1) is by itself sufficient to determine
arg z (even to within an integral multiple of 2π). Moreover, although these
two equations give the single equation
this equation is also insufficient to determine arg z to within a multiple of
2π for, as tan(θ + π) = tan θ, it will only determine θ to within an integral
multiple of π. As there is often confusion about this matter, we give pause
to give a single formula for arg z.
Suppose that we restrict z to lie in the complex plane from which the
negative real axis (including 0) has been is removed (the condition for this
is that x + |z| ≠ 0); see Figure 3.2.2. Then we can always choose a unique
value θ of arg z so that −π < θ < π, and this value of θ is given by a single
formula which, incidentally, confirms that arg z varies continuously with z.
Figure 3.2.2
Theorem 3.2.1 Suppose that z is not real and negative, or zero, and let θ be
the unique value of arg z that satisfies −π < θ < π. Then
In particular, θ varies continuously with z.

Proof The given formula is an immediate consequence of the identity
tan(θ/2)(1 + cos θ) = sin θ, and the equations x = |z| cos θ and y = |z| sin θ.
The main point here is that we must consider θ/2 rather than θ because tan−1
is single valued on the interval (−π/2, π/2) but not on (−π, π). We shall not
give a formal proof that θ varies continuously with z, but it is clear from the
formula for θ that this must be so with any reasonable definition of
continuity.
We have already assumed that the reader is familiar with the
trigonometric functions, and it is now convenient to write
(3.2.2)
where the last two expressions are alternative notations for the left-hand
side. With this, (3.1.4) and (3.2.1) yield z = reiθ, and we call this the polar
form of the complex number z. At some stage the reader will learn that
and this gives an alternative interpretation of (3.2.2), but we do not need
this here. In this context, (3.2.2) was proved in 1748 by Euler, although it
was apparently known to Cotes in 1714, and it produces what is probably
the most striking formula in mathematics, namely
The fundamental property of eiθ is as follows.
Theorem 3.2.2 eiθ = 1 if and only if for some integer n, θ = 2nπ.
Proof By definition, eiθ = 1 if and only if cos θ = 1 and sin θ = 0. Now sin
θ = 0 if and only if θ = mπ for some integer m, and cos mπ = 1 if and only if
m is even.

Next, we derive the polar form of the product zw in terms of the polar
forms of z and w.
Theorem 3.2.3 If 
 then 
.
Proof From trigonometry we have
Observe that Theorem 3.2.3 shows that
and, taking w to be 1/z and  in turn, we have
where, of course, all of these terms are only defined to within an integral
multiple of 2π. Finally, taking r1 = r2 = 1 in Theorem 3.2.3, we obtain
(3.2.3)
a special case of which is 1/eiθ = e−iθ. Combining (3.2.3) with an argument
by induction, we see that for all integers n, 
. This proves the
following result due to de Moivre and Euler.
Theorem 3.2.4 For all n, (cos θ + i sin θ)n = cos(nθ) + i sin(nθ).
Exercise 3.2
1.  Show that 
 and 
.
2.  Show that if w = reiθ and w ≠ 0, then z2 = w if and only if 
 and
arg z is θ/2 or θ/2 + π.

3.  Show that z4 = 1 if and only if z ∈ {1, i, −1, −i}.
4.    (i) Show that |z1 + · · · + zn| ≤ |z1| + · · · + |zn|.
(ii) Show that if | arg z| ≤ π/4, then x ≥ 0 and 
, where
z = x + iy. Deduce that if | arg z j | ≤ π/4 for j = 1, . . . , n, then
5.  Show that cos(π/5) = λ/2, where 
 (the Golden Ratio).
[Hint: As cos 5θ = 1, where θ = 2π/5, we see from De Moivre's theorem
that P(cos θ) = 0 for some polynomial P of degree five. Now observe
that P(z) = (1 − z)Q(z)2 for some quadratic polynomial Q.]
6.  Use De Moivre's theorem and the binomial theorem to show that cos nθ
is a polynomial in cos θ. This means that there are polynomials T0, T1,
. . . such that cos nθ = Tn(cos θ). The polynomial Tn is called the n-th
Chebychev polynomial. By considering appropriate trigonometric
identities, show that Tn+1(z) + Tn−1(z) = 2zTn(z), and hence show that
T3(z) = 4z3 − 3z.
7.  Show that if θ is real then |eiθ − 1| = 2 sin(θ/2). Use this to derive
Ptolemy's theorem: if the four vertices of a quadrilateral Q lie on a
circle. then d1d2 = ℓ1ℓ3 + ℓ2ℓ4, where d1 and d2 are the lengths of the
diagonals of Q, and ℓ1, ℓ2, ℓ3 and ℓ4 are the lengths of its sides taken in
this order around Q.
3.3 Lines and circles
Complex numbers provide an easy way to describe straight lines and circles
in the plane. As |z − a| is the distance between z and a, the circle C with
centre a and radius r has equation |z − a| = r. This equation is equivalent to 
, so the equation of C is
More generally, the equation 
 has no solution if k >
|a|2, a single solution if k = |a|2, and a circle of solutions if k < |a|2.

Any straight line L is the set of points that are equidistant from two
distinct points u and v. Thus L has equation |z − u|2 = |z − v|2 and, after
simplification, this is seen to be of the form 
, where b is
real. Not every equation of the form 
 has a solution. Indeed,
by taking the real and imaginary parts of such an equation we obtain two
linear equations in x and y. The solutions of each of these equations give
rise to a line, say L1 and L2, respectively, and the set of solutions of the
single complex equation is L1 ∩ L2. Thus the set of solutions of the
complex equation is either empty, a point, or a line. The three equations 
, and 
 illustrate each of these cases. The
following theorem describes the general situation (as we shall not need this
we leave the proof, which is not entirely trivial, as an exercise for the
reader).
Theorem 3.3.1 Suppose that a and b are not both zero. Then the equation 
 has
(1)  a unique solution if and only if |a| ≠ |b|;
(2)  no solution if and only if |a| = |b| and 
;
(3)  a line of solutions if and only if |a| = |b| and 
.
Exercise 3.3
1.  Find the radius of the circle whose equation is 
.
2.  Find the equation of the line y = x in the form 
.
3.  Suppose that b ≠ 0. Show that the equation of the line that passes
through the origin in the direction b is 
. What is the line 
?
4.  Suppose that a ≠ 0. Show that the equation of the line that passes
through a, and is in a direction perpendicular to the direction of a is 
.
5.  Suppose that a ≠ 0. Show that the equation of the line that passes
through . z0, and is in the direction a, is 
6.  Show that, in general, there are exactly two solutions of the equation 
, where a, b and c are complex numbers. When are
there more than two solutions?

3.4 Isometries of the plane
An isometry of the complex plane is a function f :  →  that preserves the
distance between points; that is, it satisfies | f (z) − f (w)| = |z − w| for all z
and w. Each translation, and each rotation, is an isometry. The next result
describes all isometries.
Theorem 3.4.1 Each of the maps
(3.4.1)
where |a| = 1, is an isometry, and any isometry is of one of these forms.
Proof It is clear that both of the maps in (3.4.1) are isometries; for example,
|(az + b) − (aw + b)| = |a| |z − w| = |z − w|. Suppose now that f is an isometry
such that f (0) = 0, f (1) = 1 and f (i) = i. If we write z = x + iy and f
(z) = u + iv and consider the distances of z, and f (z), from 0, 1 and i we see
that
These equations imply that u = x and y = v so that f (z) = z for all z. A
similar argument shows that if f is an isometry and f (0) = 0, f (1) = 1 and f
(i) = −i, then 
 for all z.
Now suppose that F is any isometry, and let
Then |F(1) − F(0)| = 1 so that F1 is an isometry with F1(0) = 0 and
F1(1) = 1. This implies that F1(i) is i or −i, and we deduce (from above) that
either F1(z) = z for all z, or 
 for all z. Both of these cases imply that
F is of one of the forms given in (3.4.1).
Theorem 3.4.1 has the following corollary.

Theorem 3.4.2 Each isometry f is an invertible map of  onto itself, and f−1
is also an isometry. Moreover, the isometries form a group with respect to
the composition of functions.
Proof First, if f(z) = az + b then 
, while if 
then 
. In each case f−1 is of one of the forms in (3.4.1), and
so is an isometry. It is obvious that if f and g are isometries then fg is also an
isometry, and we already know that the composition of functions is
associative; see (1.3.1). As the identity map I is an isometry, the proof is
complete.
There are four types of isometries of 
, namely translations, rotations,
reflections (across a line), and glide reflections (a reflection across a line L
followed by a non-zero translation along L). The next result shows how to
recognize each of these algebraically and, at the same time, shows that
every isometry is of one of these types.
Theorem 3.4.3
(i)  Suppose that f (z) = az + b, where |a| = 1. If a = 1 then f is a translation;
if a ≠ 1 then f is a rotation.
(ii)  Suppose that 
, where |a| = 1. If 
 then f is a
reflection in some line; if 
 then f is a glide reflection. In
particular, any isometry is of one of the four types listed above.
Proof Assume that f (z) = az + b. If a = 1 then f is a translation. If a ≠ 1,
then f (w) = w, where w = b/(1 − a), and f (z) − w = a(z − w). It is now clear
that f is a rotation about w of angle θ, where a = eiθ.
Now assume that 
, where a = eiθ. If (ii) is true, then we
must be able to write f = tr, where r is a reflection in a line L, t is a
translation along L (possibly of zero translation length), and where r and t
commute. Assuming this is so, then f2 = trtr = t2r2 = t2, and this tells us how
to find t, and also r as r = t−1 f. As 
, we now define maps
t and r by

It is clear that t is a translation, and as
we see that the translation is in the direction eiθ/2. Next, a simple
computation shows that r2(z) = z, and that r(z) = z whenever 
, where ρ is any real number. As r is not the identity, we see
that r is the reflection in the line
and t is a translation of 
 along the direction of L. It follows that f is
a reflection if 
 and a glide reflection if 
.
Finally, as any isometry is of one of the forms given in (3.4.1), it follows
that any isometry is one of the four types listed above.
Exercise 3.4
1.  Show that if a is real and non-zero then
(a) 
 is a glide reflection along the real axis, and
(b) 
 is a glide reflection along the imaginary axis.
2.  Find the formulae as in (3.4.1) for each of the following:
(a) the rotation of angle π/2 about the point i;
(b) the reflection in the line y = x;
(c) a reflection in x = y followed by a translation by 1 + i.
3.  If g is a reflection then there are infinitely many lines L satisfying
g(L) = L. Show that if f is a glide reflection then there is only one line L
such that f (L) = L; we call this line the axis of f . Show that if f is a glide
reflection with axis L, then 
 lies on L for every z. This shows
how to find L (choose two different values of z).
4.  Let f (z) = az + b and g(z) = αz + β, where neither is the identity. Show
that f g f−1g−1 is a translation. Show also that f commutes with g if and
only if either f and g are translations, or f and g have a common fixed
point.
5.  Suppose that f is a reflection in the line L, and that 
. Show
that 
. As |a| = 1 we can write a = eiθ; let c = eiθ/2. By that

considering the fixed points of f, show that L is given by the equation 
.
3.5 Roots of unity
For the remainder of this chapter we shall be concerned with the problem of
finding the zeros of a complex polynomial. This section is devoted to the
equation zn = 1 and its solutions, namely the n-th roots of unity. We leave
the reader to prove our first result.
Theorem 3.5.1 The set {z ∈ 
 : |z| = 1} is a group with respect to
multiplication.
The next result gives the n-th roots of unity (see Figure 3.5.1 for the case
n = 8).
Figure 3.5.1
Theorem 3.5.2 Let n be a positive integer. The n-th roots of unity are the
distinct complex numbers 1, ω, ω2, . . . , ωn−1, where ω = e2πi/n. These

points are equally spaced around the circle |z| = 1 starting at 1, and they
form a group with respect to multiplication.
Proof If z = ωm then zn = (ωm)n = ωmn = (ωn)m = 1 as ωn = 1. Conversely, if
zn = 1, write z in polar form as reiθ. Then rneinθ = 1, so that r = 1 and
nθ = 2πm for some integer m. If m = pn + q, where 0 ≤ q < n, then
z = ωm = ωq, so that {z : zn = 1} = {1, ω, . . . , ωn−1}. Moreover, the points
listed in this set are obviously distinct because arg ωk = 2πk/n. We leave the
proof that these points form a group to the reader.
The next result is a slight generalization of Theorem 3.5.2.
Theorem 3.5.3 Let w be any non-zero complex number. Then there are
exactly n distinct solutions of the equation zn = w.
Proof Let w = Reiφ, and z0 = R1/neiφ/n. Then zn = w if and only if (z/z0)n = 1
so the solutions are z0, ωz0, . . . , ωn−1z0.
We end this section with a slight digression in which we apply these
ideas to the geometry of a regular polygon. A regular n-gon is a polygon
whose n vertices are evenly spaced around a circle, and the angle at a vertex
of a regular n-gon is easily seen to be (n − 2)π/n. This means that we can fit
k regular n-gons together at a common vertex, filling the plane near the
common vertex but without overlapping, precisely when k(n − 2)π/n = 2π,
and this simplifies to (n − 2)(k − 2) = 4. The solutions of this are easily seen
to be (n, k) = (3, 6), (4, 4) or (6,3) and these solutions correspond to six
equilateral triangles, four squares and three regular hexagons, respectively,
meeting at a point.
Following this idea a little further, we can fit k regular n-gons together at
a vertex to form a non-planar pyramid-like structure (in three dimensions)
if and only if k(n − 2)π/n < 2π or, equivalently, if and only if (n − 2)(k − 2)
< 4. The solutions of this are
(3.5.1)

which correspond to three, four or five triangles meeting at a point, three
squares meeting at a point, and three pentagons meeting at a point,
respectively. The significance of these solutions is that they are precisely
the configurations that can occur at the vertex of a regular polyhedron; for
example, exactly three (square) faces of a cube meet at any vertex of the
cube. We shall return to this topic later (in Section 5.5, and also when we
discuss the symmetry groups of regular polyhedra in Chapter 14).
Exercise 3.5
1.  Show that the three cube roots of unity are 
 and 
.
2.  Suppose that the vertices of a regular pentagon lie on the circle |z| = 1.
Show that the distance between any two distinct vertices is 2 sin(π/5) or
2 sin(2π/5).
3.  If we place a unit mass at each vertex of a regular n-gon whose vertices
are on the circle |z| = 1, the centre of gravity of the masses should be at
the origin. Prove (algebraically) that this is so. Let ω = e2πi/n, and let k be
a positive integer. Show that
4.  Show that every arc of positive length on the circle |z| = 1 contains
points which are roots of unity (for some n), and points which are not
roots of unity (for any n).
5.  Show that the set of roots of unity for all n (that is, the set z for which
zn = 1 for some n) is a group with respect to multiplication.
6.  An n-th root of unity z is said to be primitive if zm ≠ 1 for m = 1, 2, . . . ,
n − 1. Show that the primitive fourth roots of unity are i and −i. Show
that there are only two primitive 6-th roots of unity and find them. Show
that, for a general n, e2πik/n is a primitive n-th root of unity if and only if
k and n have no common divisor other than 1.
3.6 Cubic and quartic equations

Every quadratic equation has two solutions. Suppose now that we want to
solve the cubic equation p1(z) = 0, where p1(z) = z3 + az2 + bz + c. Now
p1(z − a/3) is a cubic polynomial with no term in z2 so, by considering this
polynomial and relabelling its coefficients, it is sufficient to find the zeros
of p, where now
(3.6.1)
The advantage of using this form of the polynomial is that
so that p(z − b/z) = 0 providing that z3 − b3/z3 = c. This quadratic equation
in z3 can be solved to obtain z3, and hence a value, say ζ , of z. As ζ3 − b3/
ζ3 = c it follows that p(ζ − b/ζ) = 0, and we have found a solution of
p(z) = 0. There is one matter that needs further discussion here. This
algorithm apparently gives two values of z3, and hence six values of ζ .
However, the values of ζ are the roots of the equation z6 − cz3 − b3 = 0, and
if v is a root of this equation, then so (trivially) is v1 = −b/v. As
v1 − b/v1 = v − b/v, we see that these six values of ζ can only provide at
most three distinct roots of p.
The Italian del Ferro (1465-1526) is usually credited with the first
solution of the general cubic. It is said that he passed the secret onto
Tartaglia who subsequently divulged it to Cardan, who then included it in
his text Ars magna published in 1545. Cardan's student Ferrari then showed
how to solve the quartic in the following way. Suppose that p(z) is a quartic
polynomial. By considering p(z − z0) for a suitable z0, we may assume that
p(z) has no term in z3. Thus it is sufficient to solve the equation
z4 + az2 + bz + c = 0. Now if z is a solution of this equation then, for any w,
we have
(3.6.2)

Thus if we can choose w such that the right-hand side of this equation is of
the form (uz + v)2, then z satisfies
and these two quadratic equations can be solved in the usual way. The
condition that the right-hand side of (3.6.2) is of the form (uz + v)2 is the
familiar condition that this quadratic has repeated roots, and a closer look
shows that this is equivalent to saying that w satisfies a certain cubic
equation. As we can solve cubics, we can find an appropriate value of w,
and hence solve the original quartic equation. Of course, these methods are
rather involved, and in general the calculations will be cumbersome.
Nevertheless, we have seen that it is possible to solve cubic and quartic
equations without developing any further theory. Unfortunately, there is no
simple way to solve the general quintic equation.
Exercise 3.6
1.  Solve the equation z3 − z2 + z − 1 = 0 first by inspection, and then by the
method described above.
2.  Solve the equation z3 + 6z = 20 (this was considered by Cardan in Ars
magna).
3.  Verify that (in the solution of the quartic described above) we need w to
satisfy a certain cubic equation.
3.7 The Fundamental Theorem of Algebra
A polynomial of degree n is a function of z of the form
(3.7.1)
where a0, a1, . . . , an are given complex numbers and an ≠ 0. Frequently we
want to find the zeros, or roots, of p; that is, we wish to solve the equation
p(z) = 0. We know that if we work only with real numbers, there need not
be any roots. The existence of complex solutions of any equation p(z) = 0,
where p is a non-constant polynomial, is an extremely important and non-
trivial fact which is known as

The Fundamental Theorem of Algebra. Let p be given by (3.7.1), where n
≥ 1 and an ≠ 0. Then there are complex numbers z1, . . . , zn such that, for all
z,
(3.7.2)
Much work on the Fundamental Theorem of Algebra was done during the
eighteenth century, principally by Leibniz, d'Alembert, Euler, Laplace and
Gauss. The first proof for complex polynomials was given by Gauss in
1849. There are now many proofs of this result available, and the proof we
sketch below is based on topological arguments. An analytic proof is
usually given in a first course on complex analytic functions.
The first (and major) step in any proof of (3.7.2) is to show that p has at
least one root. We may assume that a0 ≠ 0, since if a0 = 0, then p(0) = 0. As
n ≥ 1 and an ≠ 0, we may divide by an and so assume that an = 1;
henceforth, then, we may assume that
To understand why p has a root in this case, we examine the effect of
applying the function p to circles centred at the origin. To be specific, let
z = reiθ, where 0 ≤ θ ≤ 2π. As θ increases from 0 to 2π, so z moves once
around the circle with centre the origin and radius r, and the point p(z)
traces out some curve 
r. If r is very large, then the term zn in p(z) has
modulus much larger than the sum of all of the other terms and so 
r is not
very different from the curve traced out by zn, namely n revolutions around
the circle centred the origin with radius rn. Thus if r is large, 
r is a curve
which begins and ends at the point p(r) (which has large modulus), and
which winds n times around the origin (see Figure 3.7.1).

Figure 3.7.1
Now suppose that r is very small. In this case, as z traces out the circle of
radius r, p(z) stays close to a0 which, by assumption, is non-zero. It follows
that if r is small enough, then 
r lies in the vicinity of the non-zero number
a0 and so does not enclose the origin. Now imagine the curve 
r changing
continuously as r varies from small to large values; the curve 
r varies
continuously from a curve that does not enclose the origin to one that does,
and this means that at some stage the curve 
r must pass through the origin.
At this stage we have found a zero of p. This is not a formal proof (although
it can be converted into one) for there is much that we have not verified;
nevertheless, it does indicate why the result is true.
Taking the existence of a root of p for granted, it is easy to see (by
induction) that every non-constant polynomial p factorises into n linear
factors. Clearly, when n = 1 we can write p in the form (3.7.2). Next,
suppose that any polynomial with degree n − 1 or less factorizes in the
manner of (3.7.2) and consider p given by (3.7.1). According to the
argument sketched above, there is some z1 with p(z1) = 0; thus
Now for any complex numbers u and v, we have
so that zk − z1k = (z − z1)qk(z), where qk is a polynomial of degree k − 1. We
deduce that p(z) − p(z1) = (z − z1)q(z), where q is a polynomial of degree

n − 1 and, by our induction hypothesis q(z) = a(z − z2) · · · (z − zn), where a
≠ 0, and this gives (3.7.2). Finally, because 
 is a field, a product of
complex numbers is zero if and only if one of the factors is zero; thus
p(z) = 0 if and only if some z is some zj.
We remark that the problem of finding the zeros of a given polynomial is
extremely difficult, and usually the best one can do is to use one of the
many computer programs now available for estimating (not finding!) the
zeros.
Suppose now that p is a real polynomial; that is, 
, where
each aj is real. Then, by taking conjugates and using the fact that 
,
we see that p(z0) = 0 if and only if 
. Thus a real polynomial has a
number of real zeros, say x1, . . . , xm, and a number of pairs of complex
(non-real) zeros, say 
. As
for some constant c, and as 
 is a real quadratic polynomial,
we see that any real polynomial can be written as the product of real linear
and real quadratic polynomials. This is often used to obtain a real partial
fraction expansion of a real rational function; however, it is no easier to
prove this than it is to prove the Fundamental Theorem of Algebra (see
Exercise 3.7.1).
Finally, notice that the Fundamental Theorem of Algebra has the
following corollary.
Theorem 3.7.1 Let p and q be polynomials of degree at most n. If
p(z) = q(z) at n + 1 distinct points then p(z) = q(z) for all z.
Proof As p and q are of degree at most n, so is p − q; thus we can write
p(z) − q(z) = a(z − z1) · · · (z − zk), for some a, k and zi, where k ≤ n. Now
Suppose that p = q at the n + 1 distinct points w1, . . . , wn+1. There is some
wj that is not any of the zi, and as p(wj) − q(wj) = 0 we see that a = 0. Thus
p = q.

We end this chapter with an amusing application of these ideas to prove
the following result (due to Cotes in 1716): let A1, . . . , An be equally
spaced points on a circle of radius one and centre O, and let P be the point
on the radius O A1 at a distance x from O. Then
(3.7.3)
We may assume that the points Aj are the n-th roots of unity, so that in
complex notation we have
However, we know that
as the two sides are polynomials of degree n with the same set of n distinct
zeros, and the same coefficient of zn. If we now put z = x and equate the
moduli of the two sides we obtain (3.7.3).
Exercise 3.7
1.  Suppose that we know that any real polynomial is the product of real
linear and real quadratic polynomials. Show that this implies that any
real polynomial has a root (this is trivial). Now take any complex
polynomial p, say 
, and let q(z) = p(z)r(z), where 
. Show that the polynomial q has real coefficients, and
hence a root w say. Deduce that w or  is a root of p (and so we have
'proved' the Fundamental Theorem of Algebra).
2.  Show that all roots of a + bz + cz2 + z3 = 0 lie inside the circle
|z| = max{1, |a| + |b| + |c|}.
3.  Suppose that n ≥ 2. Show that
(i)  all roots of 1 + z + zn = 0 lie inside the circle |z| = 1 + 1/(n − 1);
(ii)  all roots of 1 + nz + zn = 0 lie inside the circle |z| = 1 + 2/(n − 1).
4.  Suppose that ζ is a solution of the 3 − 2z + z4 + z5 = 0. Use the
inequalities

to show that 0·89426 < |ζ | < 1·7265.

4
Vectors in three-dimensional space
4.1 Vectors
A vector is sometimes described as an object having both magnitude and
direction, but this is unnecessarily restrictive. The essential algebraic
properties of vectors are that we can take real (and sometimes complex)
multiples of a vector, and that we can add vectors, and that in each case we
obtain another vector. In short, if v1, . . . , vn are vectors, and if λ1, . . . , λn
are real numbers, then we can form the linear combination λ1v1 + . . . + λnvn
of vectors and this is again a vector. This ability to form the linear
combinations of vectors is far more important, and more general, than ideas
about length and direction. For example, the set of polynomials is a set of
vectors (although polynomials have no magnitude or direction), so is the set
of solutions of the differential equation  + x = 0, the set of sequences of
real numbers, the set of functions f :  → 
, and so on.
In this chapter we shall consider vectors that lie in three-dimensional
Euclidean space
and we shall apply them to various problems about the geometry of 
.
Unfortunately, there seems to be little agreement about what vectors in 
actually are. Some say that they are points in 
, some that they are directed
line segments, and for others they are classes of line segments, where two
segments are in the same class if they determine the same 'displacement'. It
seems reasonable to insist that we should not only define what we mean by

a vector, but also that thereafter we should be consistent about its use. The
cost of consistency, however, is that we cannot simultaneously embrace all
of the suggestions made above, for they clearly are different types of
objects. In any event, whatever we choose as our definition of vectors, we
must be careful to distinguish between points and directed line segments. To
see why, consider the statement that 'three vectors are coplanar'. Now any
three points in 
 are coplanar, but the generic triple of directed line
segments emanating from the origin in 
 are definitely not coplanar.
In this text a vector in 
 is simply a point of 
; that is, an ordered triple
(x1, x2, x3) of real numbers. As is customary, we shall call the real numbers
scalars to distinguish them from vectors. To conform with common practice
we shall write vectors in boldface type, for example a, b, . . . , but later,
when we discuss abstract vector spaces, we shall abandon this notation for
it is both unnecessary and awkward. Throughout this text, x will be the
triple (x1, x2, x3), and the xj are the coordinates, or components, of x. A
similar notation will be used for vectors a, b, . . . and so on. The origin 0 in 
 is the vector (0, 0, 0).
A linear combination of two vectors is defined in the natural way, namely
where λ and μ are real numbers, and this definition extends in the obvious
way to give linear combinations of any finite number of vectors. The
addition of vectors (which is defined by taking λ = μ = 1 in the above
expression) can be illustrated geometrically by noting that x + y is the
fourth vertex of the parallelogram whose other three vertices are 0, x and y;
this is often referred to as the Parallelogram Law of Addition: see Figure
4.1.1.

Figure 4.1.1
There are a variety of simple, and self-evident, rules that hold between
vectors and scalars; for example
(4.1.1)
We shall take these and other equally simple facts for granted. Note that
1x = x and 0x = 0. We write −x instead of (−1)x. Finally, it is easy to check
that vectors form an abelian group under addition; the identity vector is 0
and the inverse of x is −x.
The distance ||x − y|| between x and y in 
 is given by
(4.1.2)
We call ||x||, namely (x12 + x22 + x32)1/2, the norm, or length, of x. For every
λ and every x, ||λx|| = |λ| ||x||, so that || − x|| = ||x||. A vector x is a unit vector
if ||x|| = 1. If x ≠ 0, the vector x/||x|| is the unit vector in the direction x; the
vector −x is in the opposite direction to x. In general, x and y are in the
same direction if there are positive numbers λ and μ such that λx = μy.
We shall need to consider directed line segments, and we denote the
directed line segment from the point a to the point b by [a, b]. Specifically,
[a, b] is the set of points {a + t(b − a) : 0 ≤ t ≤ 1}, with initial point a and
final point b. The two segments [a, b] and [c, d] are parallel if b − a and
d − c are in the same direction.
Next, we recall the familar unit vectors

these points lie at a unit distance along the three coordinate axes. Although
the notation i, j and k is universal, we shall also use the notation e1, e2 and
e3 in place of i, j and k, respectively, for this new notation adapts more
easily to higher dimensions. Note that for all x, x = x1i + x2j + x3k. Finally,
to emphasize a point made earlier, we note that the vectors i, j and k are
coplanar (for they lie in the plane given by x1 + x2 + x3 = 1), but the
segments [0, i], [0, j] and [0, k] are not.
Exercise 4.1
1.  Verify the rules given in (4.1.1), and show that the vectors form an
abelian group with respect to addition.
2.  Given vectors a and b, and positive numbers ℓ1 and ℓ2 such that
ℓ1 + ℓ2 = ||a − b||, let c be the unique vector on [a, b] such that
||c − a|| = ℓ1 and ||c − b|| = ℓ2. By writing c − a = t(b − a), for some real t,
show that
What is the mid-point of the segment [a, b]?
3.  Suppose that u = s1i + s2j and v = t1i + t2j, where s1, s2, t1 and t2 are real
numbers. Find a necessary and sufficient condition on these real
numbers such that every vector in the plane of i and j can be expressed
as a linear combination of the vectors u and v.
4.2 The scalar product
The formula (4.1.2) motivates the following definition.
Definition 4.2.1 The scalar product x·y of the two vectors x and y is given
by x·y = x1 y1 + x2 y2 + x3 y3.
The following properties of the scalar product are immediate:

(1)   (λx + μy)·z = λ(x·z) + μ(y·z);
(2)   x·y = y·x;
(3)   ||x − y||2 = ||x||2 + ||y||2 − 2(x·y).
(4)   i·j = j·k = k·i = 0, and i·i = j·j = k·k = 1.
To obtain a geometric interpretation of x·y in 
, consider the triangle
with vertices 0, x and y and let β be the angle between the segments [0, x]
and [0, y]. Applying the cosine rule to this triangle we obtain
which, by comparison with (3), yields
(4.2.1)
Note that (4.2.1) shows that if y is a unit vector then |x·y| is the length of the
projection of [0, x] onto the line through 0 and y; see Figure 4.2.1. We say
that x and y are orthogonal, or perpendicular, and write x ⊥ y, if the angle
between [0, x] and [0, y] is π/2 and, from (4.2.1), this is so if and only if
x·y = 0; thus the scalar product gives a convenient test for orthogonality.
Figure 4.2.1
We give two examples in which the scalar product is used to calculate
angles.
Example 4.2.2 We calculate the angle θ between the segments [a, b] and
[a, c], where a = (1, 2, 3), b = (−1, 0, 1) and c = (1, −2, 5). Clearly, the
translated segments [0, b − a] and [0, c − a] also meet at an angle θ, so that

As b − a = (−2, −2, −2) and c − a = (0, −4, 2), cos θ = 1/
.
Example 4.2.3 Consider the cube in 
 with vertices (ε1, ε2, ε3), where each
εj is 0 or 1, and let β be the angle between the diagonals [0, i + k] and [0,
j + k] of two faces (the reader should draw a diagram). Each of these
diagonals has length 
, so from (4.2.1) we see that
This shows that β = π/3 (and this can also be seen by noting that the triangle
with vertices 0, i + k and j + k is an equilateral triangle).
The inequality
(4.2.2)
is an immediate consequence of (4.2.1), and with (3) this shows that
so that ||x − y|| ≤ ||x|| + ||y||. If we now replace x and y by x − y and z − y,
respectively, we obtain the following result.
Theorem 4.2.4. the triangle inequality For x, y and z in 
,
This inequality expresses the fact that the length of any side of a triangle in 
 with vertices x, y and z is no greater than the sum of the lengths of the
other two sides (see Theorem 3.1.2).
We end this section by characterizing the scalar product in terms of the
important notion of a linear map. A function f : 
 → 
 is linear if for all
vectors u and v, and for all scalars λ,
(4.2.3)

(that is, f 'preserves' linear combinations). The scalar product x·y is linear
in x (for a fixed y), and linear in y (for a fixed x). In fact, the scalar product
gives all linear maps in the following sense.
Theorem 4.2.5 The most general linear map f : 
 → 
 is of the form x 
 x·a, for some a in 
.
Proof Suppose that f : 
 → 
 is linear, and let a = (a1, a2, a3), where a1 = f
(i), a2 = f (j), a3 = f (k). Then
Exercise 4.2
1.  Let θ be the acute angle formed by two diagonals of a cube. Show that
cos θ = 1/3, and hence find θ.
2.  Let Q be a quadilateral whose sides have lengths ℓ1, ℓ2, ℓ3 and ℓ4 taken
in this order around the quadilateral. Show that the diagonals of Q are
orthogonal if and only if 
. [Hint: let the vertices of Q be
a, b, c and d.] Deduce that the diagonals of a rectangle are orthogonal if
and only if the rectangle is a square, and that the diagonals of a
parallelogram are orthogonal if and only if the parallelogram is a
rhombus.
4.3 The vector product
We shall now study the vector product of two vectors. This product is itself
a vector, and as we shall see, it is an extremely important and useful tool.
Nevertheless, it is not ideal (it is not associative, or commutative), and it
seems worthwhile to pause and see that we cannot do much better. We
know that 
 is a field, and that we can extend addition and multiplication
from 
 to  in such a way that  is a field. Now we may regard  as the
horizontal coordinate plane in 
, and as addition on  extends to addition
on 
, it is natural to ask whether we can extend multiplication from  to 
 so that 
 becomes a field. Unfortunately we cannot, and we shall now
see why (and in Chapter 6 we prove a similar result for all dimensions).

In this discussion it is convenient to represent the vector x in 
 by the
pair (z, x3), where z = x1 + i x2. We shall now suppose that there is a
multiplication, say ∗, on 
 which, with the given addition of vectors,
makes 
 into a field. We want this multiplication to embrace the usual
product of complex numbers, and the scalar multiple of vectors, so we
require that (z, 0)∗(w, 0) = (zw, 0) and (λ, 0)∗(z, t) = (λz, λt) whenever λ is
real. As 
 is closed under ∗, we can write (i, 0)∗(0, 1) = (w, s), where w
∈  and s ∈ 
, and it follows from this that
Next, by the associative law,
The left-hand side of this is (−1, 0)∗(0, 1), which is (0, −1); the right-hand
side is (i, 0)∗(w, s), which is (iw + sw, s2). As −1 ≠ s2, we see that no such
multiplication ∗ can exist.
This argument shows that we cannot extend multiplication from  to 
in a way that it is associative, distributive and commutative. Thus we will
have to be satisfied with a vector product that only has some of these
properties. In fact, the vector product will not be commutative or
associative; interesting examples of non-associative operations are rare, and
the vector product is one of these. We now define the vector product.
Suppose that the two segments [0, x] and [0, y] in 
 do not lie on the
same line. The vector product of x and y will be a vector n such that [0, n]
is orthogonal to [0, x] and [0, y]. In terms of coordinates, this means that
and the general solution of these equations is

for any real λ. This motivates the following definition.
Definition 4.3.1 The vector product of the vectors x and y is
This definition produces the cyclic identities
(4.3.1)
and
As i × (i × j) ≠ 0 = (i × i) × j, these identities confirm that the vector product
is not commutative or associative.
The following properties of the vector product are immediate:
(1)  x × y is orthogonal to x and to y;
(2)  (λx + μy) × z = λ(x × z) + μ(y × z);
(3)  x × y = −y × x;
(4)  x × y = 0 if and only if there are scalars λ and μ, not both zero, such that
λx = μy. In particular, x × x = 0.
As (4) is so important we give a proof. Suppose that λx = μy, where say,
μ ≠ 0. Then y = (λ/μ)x and hence x × y = 0. Conversely, suppose that
x × y = 0; then xi yj = xj yi for i, j = 1, 2, 3, so that for each j, xj y = yj x. If
x = 0, we take λ = 1 and μ = 0. If x ≠ 0, then xk ≠ 0 for some k and we take
λ = yk and μ = xk.
The length of the vector x × y is easily calculated. Indeed,
(4.3.2)

which we prefer to write as
(4.3.3)
This shows that if the angle between [0, x] and [0, y] is θ, then
The vectors x and y determine a parallelogram 
 whose vertices are 0, x, y
and x + y. If we regard the segment [0, x] as the base of 
, then its height is
||y|| | sin θ|, and its area is ||x × y||. When x and y lie on a line L through the
origin, all of the vertices of 
 are on L and the area of 
 is zero; this is
another way of expressing (4) above.
Finally, we comment on the 'direction' of x × y. Given two segments [0,
x] and [0, y] that are not on the same line, the vector product x × y is a
vector that is orthogonal to the plane Π that contains them. Which way does
x × y 'point'? The answer is given by the so-called 'right-handed corkscrew
rule': if we rotate the segment [0, x] to the segment [0, y], the rotation being
in the plane Π and sweeping out the smaller of the two angles between [0,
x] and [0, y], then [0, x × y] lies in the direction that a 'right-handed
corkscrew' would travel if it were rotated in the same way. This is
illustrated by the vector products in (4.3.1); however, it is not mathematics,
and we shall make this idea precise in Section 4.6.
Exercise 4.3
1.  Suppose that a ≠ 0. Show that x = y if and only if a·x = a·y and
a × x = a × y.
2.  Use the vector product to find the area of the triangle with vertices (1, 2,
0), (2, 5, 2) and (4, −1, 2).
3.  Prove that (a × b)·(c × d) = (a·c)(b·d) − (a·d)(b·c).
4.  Suppose that a, b and c do not lie on any straight line. Show that the
normal to the plane that contains them lies in the direction
(a × b) + (b × c) + (c × a). Consider the special case c = 0.
5.  Suppose that a × x ≠ 0. Define vectors x0, x1, . . . by x0 = x and
xn+1 = a × xn. As ||xn|| ≤ ||a||n||x||, it is clear that xn → 0 if ||a|| < 1. What
happens as n → ∞ if ||a|| = 1, or if ||a|| > 1?

4.4 The scalar triple product
There is a natural way to combine three vectors to obtain a scalar.
Definition 4.4.1 The scalar triple product [x, y, z] of the vectors x, y and z
is defined by [x, y, z] = x·(y × z).
If y = z, then y × z = 0. If x = y, or x = z, then x ⊥ y × z; thus [x, y, z] = 0
unless the three vectors are distinct. We now use this to show that [x, y, z] is
invariant under cyclic permutations of x, y and z, and changes by a factor
−1 under transpositions.
Theorem 4.4.2 For any vectors x, y and z, we have
(4.4.1)
(4.4.2)
Proof It suffices to prove (4.4.2) as (4.4.1) follows by repeated applications
of (4.4.2) (equivalently, a 3-cycle is the product of two transpositions). As
the scalar triple product is linear in each vector, and zero if two of the
vectors are the same, we have
thus [x, y, z] = −[y, x, z]. The rest of the proof is similar.
It is easy to see that |[x, y, z]| is the volume of the parallelepiped 
′
formed from the three segments [0, x], [0, y] and [0, z]. Indeed, the volume
of ′ is the area of the base, namely ||x × y||, multiplied by the height h of 
. As h is the length of the projection of z onto the unit vector
(x × y/||x × y||), this gives the required result (see Figure 4.4.1).

Figure 4.4.1
The next result can be predicted from the interpretation of a scalar triple
product as the volume of a parallelepiped.
Theorem 4.4.3 The segments [0, x], [0, y] and [0, z] are coplanar if and
only if [x, y, z] = 0.
Proof We may assume that the three segments lie on different lines through
the origin (as the result is trivial otherwise). Then the three segments are
coplanar if and only if the segment [0, x] is orthogonal to the normal to the
plane Π that contains the segments [0, y] and [0, z]. As this normal is in the
direction y × z, the result follows.
The last result in this section shows how to find the coordinates of a point
x with reference to any given set of (not necessarily orthogonal)
coordinates axes along the directions of a, b and c. This result points the
way to the important idea of a basis in an abstract vector space, and it says
that any such triple a, b and c is a basis of 
.
Theorem 4.4.4 Suppose that [0, a], [0, b] and [0, c] are not coplanar. Then
any x in 
 can be written as the linear combination
(4.4.3)
of the three vectors a, b and c.
Proof As [0, a], [0, b] and [0, c] are not coplanar, [a, b, c] ≠ 0, and we can
write x = λa + μb + νc for some scalars λ, μ and ν. As b × c is orthogonal to
both b and c, we see that

and similarly for μ and ν.
Exercise 4.4
1.  Show that (a + b)· (b + c) × (c + a) = 2[a, b, c].
2.  Show that the four vectors (0, 0, 0), (−2, 4, s), (1, 6, 2) and (t, 7, 0) are
coplanar if and only if 6st = 8t + 7s + 28.
3.  Given non-coplanar segments [0, a], [0, b] and [0, c], we say that the
vectors a′, b′ and c′ are reciprocal vectors to a, b and c if
Prove that
(i)  [0, a′], [0, b′] and [0, c′] are non-coplanar, and
(ii)  a′′ = a, b′′ = b, c′′ = c.
4.5 The vector triple product
The scalar triple product produces a scalar from three vectors; the vector
triple product produces a vector.
Definition 4.5.1 The vector triple product of the three vectors x, y and z is
x × (y × z).
In general, y and z determine a plane Π through the origin with normal in
the direction y × z. As x × (y × z) is perpendicular to this normal, it lies in
Π, and it follows that x × (y × z) is a linear combination of y and z, say
x × (y × z) = αy + βz. As the left-hand side is a linear function of x, so is the
right-hand side, and this means that α and β are linear scalar-valued
functions of x. According to Theorem 4.2.5, there are vectors a and b such
that x × (y × z) = (x·a)y − (x·b)z. This motivates the following formula for
the vector triple product.
Theorem 4.5.2 For all vectors x, y and z,

(4.5.1)
Proof If we expand both sides of (4.5.1) in terms of the xi , yj and zk we see
that the two expressions are identical.
This proof is elementary, but tedious in the extreme, so we give another
proof that is based on the important notions of linearity and continuity.
A second proof We may suppose that y and z lie along different lines
through the origin, for otherwise (4.5.1) is trivially true. Then any x can be
written as a linear combination of y, z and y × z. As both sides of (4.5.1) are
linear in x, it is only necessary to verify (4.5.1) when x is each of these
three vectors. When x = y × z all terms in (4.5.1) are zero, so it is true. The
two cases x = y and x = z are the same (apart from a factor −1 on each side
of the equation), so we have reduced our task to giving a proof of the
identity
(4.5.2)
for all y and z.
Now (4.5.2) is true when y and z lie along the same line through the
origin. Thus, as both sides of (4.5.2) are linear in z, it only remains to prove
this when z is orthogonal to y. Moreover, it is clearly sufficient to prove this
when y and z are unit vectors; thus we have to show that if ||y|| = ||z|| = 1,
and y ⊥ z, then y × (y × z) = −z. Now y × (y × z) is of length one and also a
scalar multiple of z (because y, z and y × z are mutually orthogonal unit
vectors). Thus it is ±z, so if we let μ = z·[y × (y × z)], then μ = ±1. However,
μ is clearly a continuous function of the coefficients of y and z, so it must
be independent of y and z. As μ = −1 when y = i and z = j, we see that
y × (y × z) = −z and the proof is complete.
Exercise 4.5
1.  Prove that (a × b) × c = a × (b × c) if and only if (a × c) × b = 0.
2.  Prove the following vector identities:

3.  Give a geometric proof of (4.5.2) (you may use the 'corkscrew rule').
4.6 Orientation and determinants
Our first task is to convert the 'corkscrew rule' (Section 4.3) into a
mathematical result. Consider two non-zero vectors u and v, taken in this
order, in the plane given by x3 = 0. Let θ be the angle between the segments
[0, u] and [0, v], measured in the anti-clockwise direction. We wish to
formalize the idea that the vectors u and v are positively orientated if
0 < θ < π, and they are negatively orientated if π < θ < 2π. We can identify
each vector (x1, x2, 0) with the complex number x1 + i x2, and if we write
u = u1 + iu2 = |u|eiα, and
v = v1 + iv2 = |v|eiβ, we easily see that
It is convenient to let
then u and v are positively or negatively orientated according as ∆(u, v) is
positive or negative. Note that the orientation of u and v is now determined
algebraically, and without reference to the 'clockwise direction',
corkscrews, or even angles. Moreover, as u = (u1, u2, 0), and similarly for v,
we see that
Thus u and v in the plane x3 = 0 are positively or negatively orientated
according as u × v is a positive or negative multiple of k. Thus we have the
following simple definition of the orientation of two vectors in .

Definition 4.6.1 Two vectors u and v lying in the complex plane in 
 are
positively orientated if ∆(u, v) 0, and are negatively orientated if ∆(u,
v) < 0.
The ordered pair i and j are positively orientated; the pair j and i are
negatively orientated. More generally, as ∆(v, u) = −∆(u, v), u and v are
positively orientated if and only if v and u are negatively orientated. Also, u
and v are positively orientated if and only if u × v is a positive multiple of
k. These statements are derived formally from Definition 4.6.1.
The expression ∆(u, v) suggests that it will be useful to study the general
2 × 2 determinant
whose entries a, b, c and d are real or complex numbers. If we let 12 = 1, 
21 = −1 and 11 = 22 = 0, then
The generalization of 2 × 2 determinants to n × n determinants depends
on generalizing the symbols ij from two to n suffices; this uses the theory
of permutations and will be done later. However, we need 3 × 3
determinants now. First, we define ijk, where {i, j, k} ⊂ {1, 2, 3}, by
and then let 
ijk = 0 when i, j and k are not distinct. To understand why
these terms are relevant we recall that the sign ε(ρ) of a permutation ρ of {1,
2, 3} is (−1)k, where ρ can be written as the product of k transpositions.
Thus the ijk are connected to permutations by the relation

with 
ijk = 0 if the map ρ is not invertible. We are now ready to discuss
3 × 3 determinants.
Definition 4.6.2 A 3 × 3 determinant is a 3 × 3 array of (real or complex)
numbers whose value is given by
(4.6.1)
where the last sum is over all permutations ρ of {1, 2, 3}.
It is possible to express the components of a vector product in a convenient
form in terms of the ijk for (by inspection) the i-th component of a product
is given by
(4.6.2)
(note that only the two terms make a contribution to this sum because 
ijk = 0 unless i, j and k are distinct). The expression (4.6.2) leads directly to
a similar expression for the scalar triple product, namely
(4.6.3)
and this is the same sum as in (4.6.1).
Following the case of two variables, we now define ∆(u, v, w), for
vectors u, v and w in 
, to be the determinant obtained by taking the rows
of the array to be the components of the vector; explicitly

(4.6.4)
Next, we make the following definition.
Definition 4.6.3 The three vectors u, v and w in 
 are said to be positively
orientated if ∆(u, v, w) 0, and negatively orientated if ∆(u, v, w) < 0.
Note that i, j and k are positively orientated, and j, i and k are negatively
orientated. More generally, we have the following result which explains the
close connection between vector products and orientation.
Theorem 4.6.4 For any three vectors u, v and w in 
,
This follows immediately from (4.6.3), (4.6.1) and (4.6.4). Moreover,
Theorem 4.4.3 implies that if [0, x], [0, y] and [0, z] are not coplanar then
the vectors x, y and z are either positively orientated or negatively
orientated. Theorem 4.4.2 leads to the following result which shows that
our choice of x × y was made so that x, y and x × y (in this order) are
always positively orientated; this is the formal statement of the 'corkscrew
rule'.
Corollary 4.6.5 Suppose that x and y are not scalar multiples of each other.
Then the vectors x, y and x × y are positively orientated.
The proof is simply that
We now list some properties of 3 × 3 determinants. Briefly, these
properties enable one to evaluate determinants with only a few calculations;
however, as these calculations can now be performed by machines (which
are faster and more accurate than humans), we shall not spend long on this
matter. The general aim in manipulating determinants is to alter the entries,

without changing the value of the determinant, so as to obtain as many zero
entries as possible for, obviously, this will simplify the calculations. There
are five basic rules for manipulating determinants, and these are as follows.
A determinant
(1)  is unaltered if we interchange rows and columns: that is,
(2)  is a linear function of each column (and of each row);
(3)  is zero if two columns (or two rows) are identical;
(4)  is unaltered if we add to any given column any linear combination of
the other columns (and similarly for rows);
(5)  changes sign when we interchange any two columns (or rows). In
addition,
(6)  a 3 × 3 determinant can be expressed as a linear combination of 2 × 2
determinants, namely
The rules (1)-(6) can be proved (in a trivial but tedious way) by writing
out all determinants in full. Some of the rules can also be proved from the
known properties of the scalar triple product for, given any determinant D,
we can always find vectors u, v and w such that D = ∆(u, v, w). However,
as there is no generalization of the vector product to all dimensions (see
Chapter 6), these properties will eventually have to be proved for n × n
determinants from a definition similar to (4.6.1). Finally, we mention that
(6) is the first step of an inductive definition of the n × n determinant in
terms of the (n − 1) × (n − 1) determinant. We shall not stop to verify these
rules, and we end with two examples in which we evaluate determinants
using (1)-(6). Of course, the reader may feel (as the author does) that it
would be simpler to evaluate the determinant directly from the definition,
but our purpose here is to illustrate the use of the rules.

Example 4.6.6 Using (6) above we see that 
. However, the
reader may wish to verify the following, which depends on the linearity as a
function of the rows. Let a = (1, 3, 5), b = (1, 3, 4) and c = (0, 1, 4), so that
the rows of D are a, 2b and c. First,
We write this last scalar triple product as [u, v, w], and then
Example 4.6.7 We leave the reader to verify the following steps:
Exercise 4.6
1.  Show that the determinants
are 2, −380 and 0, respectively.
2.  Evaluate the determinants
and factorize both answers.

3.  Show that for any vectors a, b, c, u, v, w in 
,
4.7 Applications to geometry
Vectors give us a convenient way of describing geometry in 
, and this
section consists of various applications to the geometry of lines, planes,
triangles and tetrahedra. The discussion will be brief, and the reader is
asked to supply the missing details.
(A) The geometry of lines
The vector equation of the line L through x0 and x0 + a is
because x ∈ L if and only if x − x0 = ta for some real t. If we take
x1 = x0 + a, we see that the line through x0 and x1 has equation
The distance from y to the line through x0 and parallel to [0, a] is
By taking y = x1, we see that the distance between the parallel lines
given by (x − x0) × a = 0 and (x − x1) × a = 0 is ||(x1 − x0) × a||/||a||.
The non-parallel lines given by (x − x0) × a = 0 and (x − x1) × b = 0 meet
if and only if [x0, a, b] = [x1, a, b], for this is so if and only if [0, a], [0,
b] and [0, x0 − x1] are coplanar (Theorem 4.4.3). Notice that as the
lines are not parallel, a × b ≠ 0. Now suppose that these lines meet at
y. Then there are real s and t such that x0 + ta = y = x1 + sb. Now write
z = x1 − x0; then

so that t = [a × b, z, b]/||a × b||2. Thus if the lines meet then they
intersect at the point
(B) The geometry of planes
The equation of the plane Π which contains a, and which has normal in
the direction n, is x·n = a·n (because x ∈ Π if and only if x − a ⊥ n).
The equation of the plane Σ through the three non-collinear points a, b
and c is
Indeed, this equation is linear in the coordinates xj and so defines a
plane. As this plane obviously contains a, b and c (Theorem 4.4.3), it
is the equation of Σ.
The distance of y from the plane Π given by x·n = d, where ||n|| = 1, is |t|,
where t is such that y + tn ∈ Π. This last condition is (y + tn)·n = d, so
the required distance is |d − (y·n)|.
The equation of the line L of intersection of the non-parallel planes
x·a = d1 and x·b = d2 is x × (a × b) = d2a − d1b. As the planes are not
parallel, a × b ≠ 0, and as L is orthogonal to the normals of both planes
it is in the direction a × b. It follows that L is given by an equation of
the form x × (a × b) = c. This equation is (x·b)a − (x·a)b = c, and
taking x on L we see that c = d2a − d1b.
The lines L0 and L1 are said to be skew if they do not lie in any
plane. We shall show that skew lines lie in a pair of parallel planes,
and so have a common normal. Suppose that the skew lines L0 and L1
are given by
(4.7.1)
As L0 and L1 are not parallel, a × b ≠ 0. Now consider the two planes
Π0 and Π1 given by

(4.7.2)
respectively. These planes are parallel for they have a common normal,
namely a × b. Moreover, L0 lies in Π0 because any point of L0 is of the
form x0 + ta for some real t and, similarly, L1 lies in Π1.
The shortest distance between the skew lines given by (4.7.1) is
(4.7.3)
Indeed, the shortest distance between the lines in (4.7.1) is also the
shortest distance between the planes in (4.7.2). As x0 ∈ Π0 and x1 ∈
Π1, this distance is the projection of x0 − x1 onto the common unit
normal of the planes, and this gives the stated formula.
Suppose that the three planes x·a = λ, x·b = μ and x·c = ν intersect in a
single point; then this point is (by inspection)
As the planes meet in only a single point, their normals, in the
directions a, b and c, are not coplanar; thus [a, b, c] ≠ 0 (Theorem
4.4.3). This implies that for any x,
because both sides of this equation have the same scalar product with
each of a, b and c. If x is the point of intersection, then x·a = λ, and so
on, and the result follows.
(C) The geometry of triangles
A median of a triangle is the segment joining a vertex v of the triangle to
the midpoint of the opposite side s. The altitude from v is the segment from
v to the line L that contains s that is orthogonal to L. The altitudes of a
triangle are concurrent; the medians of a triangle are concurrent; the angle
bisectors of a triangle are concurrent.

We only consider the medians. Let the vertices of the triangle be a, b and
c. The general point on the median joining a to d, where 
 is
ta + (1 − t)d, where 0 ≤ t ≤ 1. When t = 1/3 this point is (a + b + c)/3. By
symmetry, this point also lies on the other medians.
The sine rule: suppose that a triangle T has angles α, β and γ opposite
sides of lengths a, b and c, respectively. Then
Let the vertices be a, b and c, and let p = a − b, q = b − c, r = c − a. As
p + q + r = 0, we see that p × q = q × r = r × p, and this gives the sine rule.
(D) The geometry of tetrahedra
A tetrahedron T is formed by attaching four triangles together along their
edges so as to form a surface which has four triangular faces, six edges and
four vertices. Any tetrahedron has three pairs of 'opposite' edges (that is,
pairs of edges with no common end-points), and the segments that join the
midpoints of opposite edges are concurrent at their midpoints. Further,
these segments are mutually orthogonal if and only if each pair of opposite
edges of T have the same length. Let the vertices of T be at a, b, c, and d.
One pair of opposite edges has midpoints 
 and 
, and the
midpoint of the segment joining these points is 
). By
symmetry, p must be the midpoint of each of the segments that join the
midpoints of each pair of opposite edges of T. Next, these three segments
passing 
through 
p 
are 
parallel 
to 
the 
three 
vectors 
; thus the segments are mutually
orthogonal if and only if these three vectors are mutually orthogonal. Now
the first two vectors are
and these are orthogonal if and only if ||a − d|| = ||b − c|| because, in general,
u − v ⊥ u + v if and only if ||u|| = ||v||.
Exercise 4.7

1.  Show that the distance between the point (1, 1, 1) and the line through
(2, 0, 3) and (−1, 0, 1) is 
.
2.  Show that the distance between the point (−3, 0, 1) and the line given by
(1, 0, 2) + t(1, 1, 2), where t ∈ 
, is 7/
.
3.  Show that the distance between the two lines in the direction (1, 2, 1)
that pass through the points (4, 2, −1) and (3, 1, 0), respectively is 7/
.
4.  Show that the distance between the two skew lines given in parametric
form by (1, 2, 3) + t(2, 0, 1) and (0, 0, 1) + t(1, 0, 1) is 2.
5.  Find the equation of the plane through the points (0, 1, 2), (−4, 3, 1) and
(10, 0, 7).
6.  Find the intersection of the three planes given by x·a = 1, x·b = 2 and
x·c = 3, where a = (3, 1, 1), b = (2, 0, 8) and c = (1, 0, 2).
7.  Show that the minimum distance between the origin and the plane given
by 2x1 + 5x2 − x3 = 1 is 1/
, and that this is attained at the point (2/30,
5/30, −1/30) on the plane.
8.  Find the vectorial equation of the line of intersection of the planes
3x1 + 2x2 + x3 = 3 and x1 + x2 + x3 = 4.
9.  Consider the cube with vertices at the points (r, s, t), where each of r, s
and t is 0 or 1. What is the surface area of the tetrahedron whose vertices
are at the points 0 and the centres of the three faces of the cube that do
not contain 0?
10.  Use vectors to show that the diagonals of a parallelogram bisect each
other, and that the diagonals are orthogonal if and only if the
parallelogram is a rhombus. Show that the midpoints of the sides of any
quadrilateral form the vertices of a parallelogram.
11.  Let T be the tetrahedron with vertices 0, ai, bj and ck, and let the faces
opposite these vertices have areas A0, Aa, Ab, and Ac, respectively. Show
that 
 (Pythagoras' theorem for a tetrahedron).
12.  Show that the minimum distance between a pair of opposite edges of a
regular tetrahedron T with edge length ℓ is ℓ/
.
4.8 Vector equations
In this section we see how to solve each of the equations

where λ, μ, a, b and c are given scalars and non-zero vectors. If we write
either of these equations in terms of coordinates, we obtain three linear
equation in the three components of x. The set of solutions (of each of these
equations) is therefore the intersection of three planes, and so is either
empty, a point, a line, a plane or (possibly) 
. This tells us what to expect,
but our objective is to solve these equations by vector methods. The cases
when μ = 0, or λ = 0, are either trivial or have been considered earlier, so we
shall assume that λ ≠ 0 and μ ≠ 0. If we write a′ = (μ/λ)a, and b′ = λ−1b, the
equations are converted into similar equations with λ = μ = 1. Thus we may
assume that λ = μ = 1. We consider each in turn.
Because x·a is a scalar, any solution of the equation x + (x·a)c = b must
be of the form x = b + tc, where t is real. Thus all solutions (if any) of this
equation lie on the line L through b in the direction c. If we now check to
see whether or not x = b + tc is a solution, we find that (i) there is a unique
solution if 1 + a·c ≠ 0; (ii) there is no solution if 1 + a·c = 0 and a·b ≠ 0;
(iii) every point on L is a solution if 1 + a·c = 0 and a·b = 0.
Now consider the equation x + (x × a) = b. If y is the difference of any
two solutions, then y + (y × a) = 0, and so y = 0 (because y ⊥ y × a). This
shows that the given equation has at most one solution. If a × b = 0, then b
is a solution. If a × b ≠ 0, then every x can be expressed in the form
x = x1a + x2b + x3(a × b), and it is then a simple matter to check that
is the unique solution to the equation.
Exercise 4.8
1.  Solve the equations x + (x·i)i = j, and x + (x × i) = j.
2.  Solve the equations x + (x·a)a = a, and x + (x × a) = a.
3.  Show that the solution of the simultaneous equations x + (c × y) = a and
y + (c × x) = b is given by

4.  Solve the simultaneous vector equations x + y = a and x × y = b.

5
Spherical geometry
5.1 Spherical distance
This chapter is devoted to spherical geometry; that is, to geometry on the
surface of the sphere
Later in this chapter we shall use spherical trigonometry to derive Euler's
formula for polyhedral surfaces, and this will lead (eventually) to a
discussion of the symmetry groups of regular polyhedra.
We take for granted the fact that one can measure the length of a smooth
curve, and the area of a (reasonably simple) set, on 
. A great circle, of
length 2π, is the intersection of  and a plane that passes through the origin.
Every other circle on  is a plane section of  of length less than 2π. If σ is
an arc of a great circle, then the length of σ is the angle (in radians)
subtended by σ at 0. Given any two points a and b on 
 there is a unique
great circle, say C, that contains them (and C lies in the plane through 0, a
and b). Also, a and b divide C into two arcs which have different lengths
unless b = −a.
Definition 5.1.1 Let a and b be two points on 
. Then the spherical
distance δ(a, b) between a and b is the length of the shorter of the two arcs
of the (unique) great circle through a and b. Clearly,
(5.1.1)

where cos−1 (a · b) is chosen in the range [0, π].
As an application, consider the Earth to be a perfect sphere of radius R
whose centre lies at the origin 0 in 
. We may suppose that the i-axis
meets the surface of the earth at the point with zero latitude and longitude,
and that the positive k-axis passes through the north pole. Thus the point on
the Earth's surface with latitude α (positive in the northern hemisphere, and
negative in the southern hemisphere), and longitude β is given by the vector
(the reader should draw a diagram). Suppose, now that x1 and x2 are two
points on the Earth's surface, and write these as
Then, from (5.1.1),
(5.1.2)
This formula gives us the distance (measured on the surface of the Earth)
between the two points with latitude αi and longitude βi, i = 1, 2.
Exercise 5.1
1.  Verify that any point with latitude α is a spherical distance R(π/2 − α)
from the north pole.
2.  Assume that the Earth is a sphere of radius 4000 miles. Show that the
spherical distance between London (latitude 51° north, longitude 0°) and
Sydney (latitude 34° south, longitude 151° east) is approximately 10 500
miles.
3.  Suppose that an aircraft flies on the shortest route from London (latitude
51° north, longitude 0°) to Los Angeles (latitude 34° north, longitude
151° east). How close does the aircraft get to the north pole?

4.  Let x and y be two points on the sphere. Show that the normal to the
plane determined by the great circle through x and y intersects the sphere
at the points ±z, where z = (x × y)/||x × y||. Suppose that w lies on the
same side of the plane as z. Show that cos δ(w, z) = [w, x, y].
5.2 Spherical trigonometry
We begin our discussion of spherical trigonometry with the the spherical
version of Pythagoras' theorem. Consider a triangle on 
, by which we
mean three points a, b and c of  which do not lie on a great circle, where
the sides of the triangle are the arcs of great circles that join these points in
pairs. We assume that the angle in the triangle at c is π/2, and we may
position this triangle so that c = k, a is in the (i, k)-plane, and that b is in the
(j, k)-plane (if necessary we may interchange the labels on a and b). With
this, we have
(5.2.1)
for some α1 and α2 (see Figure 5.2.1) so that (a · b) = (a · c)(b · c). If we
now apply (5.1.1) we immediately obtain the following result.
Figure 5.2.1
Pythagoras' theorem Let a, b and c be the vertices of a spherical triangle
on , with the sides making an angle of π/2 at c. Then

(5.2.2)
Consider (informally) an 'infinitesimal' right-angled triangle. As this
triangle is nearly flat we would expect (5.2.2) to look roughly like the usual
Euclidean version a2 + b2 = c2 of Pythagoras' theorem. This is indeed the
case because for small θ, cos θ is approximately 1 − θ2/2.
Next, we consider a spherical triangle (as illustrated in Figure 5.2.1) with
vertices a, b and c. As is customary in Euclidean geometry, we let the
angles at a, b and c be α, β and γ, respectively, and we denote the lengths of
the sides by a, b and c; thus a = δ(b, c), b = δ(c, a) and c = δ(a, b). We now
prove the following identity; note that the right-hand side here involves two
sides and one angle.
Theorem 5.2.1 In any spherical triangle with vertices a, b and c we have
(5.2.3)
Proof We may choose our axes so that c = k, and that a lies in the (i, k)-
plane. As δ(a, c) = b, we see that a = (sin b, 0, cos b). Similarly, b has
latitude π/2 − a and longitude γ ; thus
and (5.2.3) follows directly from these expressions.
We end this section with the sine rule, and the cosine rule, for spherical
geometry.
The sine and cosine rules In a spherical triangle with vertices a, b and c,
we have the sine rule:
and the cosine rule: cos c = cos a cos b + sin a sin b cos γ.
Proof The sine rule follows immediately from (5.2.3) and the fact that [a, b,
c] = [b, c, a]. To prove the cosine rule we may choose a. b and c as in the
proof of Theorem 5.2.1. Then

as required.
Exercise 5.2
1.  Derive Pythagoras' Theorem from the cosine rule.
2.  Show that if an equilateral spherical triangle has sides of length a and
interior angles α, then cos(a/2) sin(α/2) = 1/2. Deduce that α > π/3 (so
that the angle sum of the triangle exceeds π).
3.  Calculate the perimeter of a spherical triangle all of whose angles are
π/2.
5.3 Area on the sphere
Let us now find the formula for the area of a spherical triangle on 
. We
denote the spherical area (that is, the area on 
) of a set E by μ(E), and as
the surface area of a sphere of radius r is 4πr2, we see that μ( ) = 4π. Two
(distinct) great circles meet at diametrically opposite points, and they divide
the sphere into four regions, called lunes. The angle of a lune is the angle
(in the lune) at which the circles meet, and it is clear that the area of a lune
of angle α is 2α, for it is obviously proportional to α, and equal to 4π when
α = 2π.
Unlike Euclidean geometry, the area of a spherical triangle is completely
determined by its angles (there are no similarity maps in spherical
geometry), and the following formula was first found by A. Girard in 1625.
Figure 5.3.1

Theorem 5.3.1 Let T be a spherical triangle with angles α, β and γ. Then
μ(T ) = α + β + γ − π.
Proof The triangle T is formed from sides that lie on three great circles
which we denote by 
,  and . The great circle 
 subdivides the sphere
into two hemispheres which we denote by A1 and A2. We define B1, B2, C1
and C2 similarly, and these may be chosen so that T = A1 ∩ B1 ∩ C1, with T
having angles α, β and γ at its vertices on 
 ∩ , 
 ∩ 
 and 
 ∩ 
,
respectively. Now 
,  and  divide the sphere into the eight triangles Ai ∩
Bj ∩ Ck, where i, j, k = 1, 2, and which, for brevity, we write as Ai Bj Ck.
The triangle T (= A1 B1C1)) and its six neighbours are illustrated
(symbolically) in Figure 5.3.1; the only triangle that does not appear in this
illustration is A2 B2C2.
Now (for example) A1 B1C1 and A2 B1C1 together comprise a lune of
angle α determined by the great circles  and . Thus we find that
Adding each side of these six equations, and noting that
we see that μ(A1 B1C1) + π = α + β + γ as required.
The formula for the area of a triangle extends readily to the area of a
spherical polygon.
Theorem 5.3.2 Let P be a polygon on the sphere (with each of its n sides
being an arc of a great circle), and let the interior angles of the polygon be

θ1, . . ., θn. Then the area μ(P) of the polygon is given by
(5.3.1)
The proof of 5.3.1 in the case of a convex polygon on the sphere is easy.
Suppose that a polygon P has the property that there is some point x in P
that can be joined to each vertex vj by an arc of a great circle, with these
arcs being non-intersecting (except at x) and lying in P. Then these arcs
divide P into n triangles, and applying Theorem 5.3.1 to each, and summing
the results, we obtain (5.3.1). The proof of (5.3.1) for a non-convex polygon
will be given in the next section.
Exercise 5.3
1.  Calculate the area of a spherical triangle all of whose angles are π/2, and
also the area of a spherical triangle all of whose angles are 3π/2.
2.  For which values of θ is it possible to construct an equilateral spherical
triangle with each angle equal to θ?
3.  Prove the famous result of Archimedes that the area of the part of  that
lies between the two parallel planes given, say, by x3 = a and x3 = b, is
the same as the area of the part of the circumscribing cylinder (given by 
) that lies between these two planes. Hence find the area of
the 'polar cap' {x ∈  : δ(x, k) < r}.
5.4 Euler's formula
A spherical triangle T is a region of 
 bounded by three arcs σ1, σ2 and σ3
of great circles. The arcs σj are the edges of T, and the three points σi ∩ σj
are the vertices of T. A triangulation of the sphere  is a partitioning of 
into a finite number of non-overlapping spherical triangles Tj such that the
intersection of any two of the Tj is either empty, or a common edge, or a
common vertex, of the two triangles. The edges of the triangulation are the
edges of all of the Tj; the vertices of the triangulation are all of the vertices
of the Tj.
The simplest example of a triangulation on the sphere (say, the surface of
the Earth) is found by drawing the equator and n lines of longitude. In this

case the triangulation contains 2n triangles, n + 2 vertices (n on the equator,
and one at each pole), and 3n edges. If we denote the numbers of triangles
(which we now call faces), edges and vertices by F, E and V, respectively,
we find that F − E + V = 2; thus the expression F − E + V does not depend
on the choice of n. It is even more remarkable that the formula
F − E + V = 2 holds for all triangulations of the sphere. This famous result
is due to the Swiss mathematician Leonard Euler (1707-83) who was one
of the most productive mathematicians of all time.
Euler's theorem Suppose that a triangulation of  has F triangles, E edges
and V vertices. Then F − E + V = 2.
We shall give Legendre's beautiful proof that is based on the area of a
spherical triangle.
Proof The area of a spherical triangle ∆ with angles θ1, θ2 and θ3 is
θ1 + θ2 + θ3 − π. Suppose that there are F triangles, E edges and V vertices
in a triangulation of the sphere. Then, summing over all angles in all
triangles, the total angle sum is 2π V (for all of the angles occur at a vertex
without overlap, and the angle sum at any one of these V vertices is exactly
2π). Also, the sum of the areas of the triangles is the area of the sphere; thus
2π V − Fπ = 4π, or 2V = F + 4. Now (by counting the edges of each triangle,
and noting that this counts each edge twice), we obtain 3F = 2E; thus
There are various important extensions of Euler's theorem which we
shall now discuss briefly and informally. A spherical polygon is a region
bounded by a finite number of arcs of great circles in such a way that the
arcs form a 'closed' curve on 
 that divides  into exactly two regions. A
spherical polygon P is convex if any two points in P can be joined by an arc
(of a great circle) that lies entirely within P. Now partition 
 into a finite
number of non-overlapping convex spherical polygons (which now need
not be spherical triangles), and suppose that there are F polygons (or faces),
E edges (each edge counted only once), and V vertices (each vertex counted
only once); then, again, F − E + V = 2. To prove this consider one face of

the triangulation, and suppose that this is bounded by a polygonal closed
curve that comprises m edges and m vertices, say vj. We take any point x in
the face and join this to each vj by a segment of a great circle, thus
producing a triangulation of this face that includes, among its edges and
vertices, all of the original edges and vertices of the face in the original
partition. The contribution of this face (without its boundary) to the count
F − E + V from the new triangulation is 1, for it contributes m triangles, m
edges and one vertex (at x). As its contribution to F − E + V in the original
partition of 
 is also 1 (it contributes one to F and zero to E and V ), it
makes no difference to the count of F − E + V whether we subdivide the
face into triangles or not. If we carry out this subdivision for all faces, and
then use Euler's theorem, we find that F − E + V = 2.
Next, suppose that we partition the sphere into convex polygons and that
we then deform the partition continuously in such a way that the numbers
F, E and V do not change during the deformation; then (obviously) after the
deformation we still have F − E + V = 2. In particular, this will still be true
when the edges are not necessarily arcs of great circles (or even arcs of any
circles) providing that the given 'curvilinear partition' can be 'deformed'
into a partition of the prescribed type in such a way that the numbers of
faces, edges and vertices remains constant during the 'deformation'. Of
course, all this is obvious; the difficulty (if one is to be rigorous) lies in the
deformations.
Finally, we can extend this idea and now allow deformations of the
sphere itself (together with any given partition of it). For example, as we
can deform the sphere to a cube, the formula F − E + V = 2 will still hold
for a cube. Indeed, if we partition the cube in the natural way into its six
square faces, we see that F = 6, E = 12 and V = 8, so F − E + V = 2. If we
wish, we can now divide each square face into two triangles by drawing one
diagonal across each face, and then regard these diagonals together with the
'natural' edges of the cube as the edges of a new triangulation. This
triangulation has twelve triangles, eighteen edges and eight vertices and
again, 12 − 18 + 8 = 2. Collectively, these ideas are described by saying that
Euler's formula is a topological invariant, but we shall not discuss this any
further.
Roughly speaking, a polyhedron is a 'closed' surface (that is, a surface
with no bounding edges) made up by 'joining' polygons together along
edges (of the same length), with the resulting surface being capable of being

deformed into a sphere. We can also define a convex polyhedron to be the
(non-empty) intersection of a finite number of half-spaces (a half-space is
one 'side' of a plane); for example, a cube is easily seen to be the
intersection of exactly six half-spaces. A convex polyhedron can be easily
deformed into a sphere (by choosing a sphere which contains the
polyhedron, and whose centre lies inside the polyhedron, and then
projecting the polyhedron radially from the centre onto the sphere); thus, if
a convex polyhedron has F faces, E edges and V vertices, then
F − E + V = 2.
We complete this informal discussion by considering Euler's formula for
a plane polygon. Suppose that a closed polygonal curve C (lying in the
plane) divides the plane into two regions. Exactly one of these regions will
be bounded, and we denote this by P. Suppose now that we triangulate P in
the manner considered above. It seems clear that we can deform the
polygon P until it lies on the 'southern hemisphere' of , in such a way that
the boundary curve of P lies in the 'equator'. Then there will be, say, m
vertices and m edges lying on the equator (for each edge ends at a vertex
which is the starting point of the next edge). Let us now include the
'northern hemisphere' N, say, as one of the polygons on the sphere, with the
m edges and m vertices being the edges and vertices of N regarded as a the
polygon. Then we have constructed a partioning of  and, for this partition,
F − E + V = 2. As we have simply added one face to the original picture, it
follows that for the triangulation of the plane polygon P, we must have
F − E + V = 1.
We end this section by completing the proof of Theorem 5.3.2.
The proof of Theorem 5.3.2 Let P be any polygon on the sphere. We extend
each side of P to the great circle that contains it, and in this way we
subdivide the sphere into a finite number of convex polygons. As each of
these polygons either lies within P or outside of P, this shows that we can
subdivide any spherical polygon into a finite number of convex polygons,
and hence also into a finite number of spherical triangles.
Now take any polygon P on , and divide it into triangles Tj as described
above. Suppose that the triangulation of P into these triangles Tj has F
triangles, E edges and V vertices; then by Euler's formula (as described in
the previous paragraph), F − E + V = 1. In order to describe the next step of
the proof, let us denote the original polygonal curve by C, and its 'interior'

(which we have just divided into triangles) by P0. Suppose now that E0 of
the edges of this new triangulation lie on some side of C, so that E − E0
edges lie in P0. As each of the F triangles Tj has three sides, we see that
3F = E0 + 2(E − E0); thus 3F + E0 = 2E. We now compute areas. We
suppose that the original polygon has n vertices with internal angles θ1, . . .,
θn at these vertices. These vertices lie on C. Now E0 of the V vertices in the
new triangulation lie on C, and of these, n occur as original vertices of P
(on C), while the remaining E0 − n occur as 'new' vertices lying on C (and
interior to an original side of P). The angle sum at each of these E0 − n
vertices is π. The remaining V − E0 vertices are in P0, and the angle sum at
each of these vertices is 2π. Thus
because F − E + V = 1 and 3F + E0 = 2E.
Exercise 5.4
1.  Verify Euler's formula for a 'pyramid' that has an n-gon as a base.
2.  What is Euler's formula for a plane polygon from which two polygonal
'holes' have been removed?
5.5 Regular polyhedra
A regular polyhedron is a polyhdron that is made by joining together a
finite number of congruent, regular polygons, each with p sides, and with
exactly q polygons meeting at each vertex, and we say that this a regular
polyhedron of type (p, q). The faces of the polyhedron are the regular p-
gons, the edges are the segments where two faces are joined, and the
vertices are the points at the ends of the edges. Such solids, which have a

high degree of symmetry, are also known as Platonic solids (after Plato who
associated them with earth, water, fire, air and the cosmos).
It is clear that if we are to construct a regular polyhedron of type (p, q),
then we must be able to places q of the polygons together, in the plane and
without overlapping, each having one vertex at the origin. Indeed, the sum
of their angles at the origin must be strictly less than 2π, so that p and q
must satisfy q(π − 2π/ p) < 2π. Now this inequality is equivalent to
(5.5.1)
and the only solutions (p, q) of this that are consistent with the obvious
geometric constraints p ≥ 3 and q ≥ 3 are (3, 3), (3, 4), (3, 5), (4, 3) and (5,
3). In particular, there are at most five such polyhedra (up to scaling).
Now consider a regular polyhedron of type (p, q) and suppose that this
has F faces, E edges and V. Then, by Euler's formula, F − E + V = 2. As
each edge is the edge of exactly two faces, and each face has p edges, we
see that 2E = pF (we simply count each 'side' of each edge in two different
ways). Similarly, as each edge has two 'ends', and each vertex is the
endpoint of q 'ends', we see that 2E = qV. We now have three simultaneous
equations, namely F − E + V = 2, pF = 2E and qV = 2E, and if we solve
these for F, E and V in terms of p and q, we find that
(5.5.2)
Notice, from (5.5.1), that 2 p + 2q − pq 0. If we now substitute the five
distinct possibilities for (p, q) in the formulae in (5.5.2) we obtain the
following complete list of regular polyhedra:

Of course, this does not show that these solids do actually exist but they do
(see Figure 5.5.1), and we shall accept this for the moment.
Figure 5.5.1
Finally, we remark that to obtain the Platonic solids it is not enough to
merely assume that the faces of the polyhedron are congruent regular
polygons. For example, we can attach five equilateral triangles together to
form a pyramid with a regular pentagonal base and then join two of these
pyramids together across their base; the resulting polyhedron has congruent
faces that are equilateral triangles, but it is not a Platonic solid.
Exercise 5.5
1.  Establish the formulae (5.5.2). These arise from three simultaneous
equations; those who know about matrices should write these equations
in matrix form and solve them by finding the inverse matrix. What is the
determinant of the matrix?
2.  The deficiency of a polyhedron is 2π V − Θ, where Θ is the sum of all
the interior angles of all of its faces. Show that each of the five Platonic
solids has deficiency 4π.
3.  Show that the mid-points of the faces of a cube form the vertices of a
regular octahedron.
4.  Let A, B, C and D be the vertices of a regular tetrahedron. Show that the
midpoints of the sides AB, BC, CD and DA are coplanar, and form the
vertices of a square.
5.6 General polyhedra

In this section we shall examine several results that are closely related to
Euler's formula. We have seen that the condition F − E + V = 2 is a
necessary condition for the existence of a polyhedron (that can be deformed
into a sphere) with F faces, E edges and V vertices. However, this is not the
only necessary condition. Suppose that we 'cut' each edge of the
polyhedron at its midpoint and then count the segments that are formed by
this cutting. As each edge produces two segments, this count is clearly 2E.
On the other hand, every vertex has at least three segments ending there,
and each segment ends at only one vertex; thus the count is at least 3V. We
conclude that 2E ≥ 3V. In a similar way, imagine that we have 'separated'
the polyhedron into its faces, and that we now count the edges of all of
these faces. Clearly this count is 2E. On the other hand, each face has at
least three sides so the count must be at least 3F. We deduce that 2E ≥ 3F.
Thus
(5.6.1)
are all necessary conditions for the existence of a polyhedron (that can be
deformed into a sphere), with F faces, E edges and V vertices. These show
that Euler's relation F − E + V = 2 is not by itself sufficient for the existence
of a polyhedron; for example, there is no polyhedron with F = 4, E = 7 and
V = 5 because for these values, 2E < 3V.
Theorem 5.6.1 There exists a convex polyhedron with F faces, E edges and
V vertices if and only if (5.6.1) holds.
Proof We know that (5.6.1) are necessary conditions for the existence of a
polyhedron. To prove their sufficiency; we take positive integers F, E and V
satisfying (5.6.1), and we divide the proof into two cases, namely (i) V ≥ F
and (ii) V < F.
(i)  Suppose that a convex polyhedron has f faces, e edges and v vertices,
and that it has at least one vertex of valency three (that is, exactly three
faces meet at the vertex). Then we can truncate the polyhedron at that
vertex (that is, we can 'slice' the vertex from the polyhedron) and the
resulting polyhedron has f + 1 faces, e + 3 edges and v + 2 vertices. As
the resulting polyhedron also has vertices of valency three, we can

repeat this operation as often as we wish, say t times, and so obtain a
convex polyhedron with f + t faces, e + 3t edges and v + 2t vertices. If
we apply this to the polyhedron that is a pyramid Pm whose base is an
m-gon (this has m + 1 faces, 2m edges and m + 1 vertices), the resulting
polyhedron will have m + 1 + t faces, 2m + 3t edges and m + 1 + 2t
vertices; thus it suffices to show that the equations
have a solution in integers m and t with m ≥ 3 and t ≥ 0. These
equations are consistent (because F − E + V = 2), and the solution is
t = V − F and m = 2F − V − 1. Finally, t ≥ 0 by (i), and
m = 2F − V − 1 = (2E − 3V) + 3 ≥ 3.
(ii)  Suppose that a convex polyhedron has f faces, e edges and v vertices,
and that one of the faces is triangular. Then we can glue a tetrahedron
onto that face, and if the tetrahedron is sufficiently 'flat' the resulting
polyhedron will be convex, with a triangular face, and it will have f + 2
faces, e + 3 edges and v + 1 vertices. If we do this t times we obtain a
convex polyhedron with f + 2t faces, e + 3t edges and v + t vertices. If
the original polyhedron is the polyhedron Pm used above, the resulting
polyhedron will have m + 1 + 2t faces, 2m + 3t edges and m + 1 + t
vertices, and we now have to solve the equations
again with t ≥ 0 and m ≥ 3. These equations are consistent, with
solution t = F − V and m = 2V − F − 1. By (ii), t > 0, and
m = 2V − F − 1 = (2E − 3F) + 3 ≥ 3.
Our next result was known to Euler. Consider a polyhedron with F faces,
E edges and V vertices. To examine the geometry in greater detail, let Fn be
the number of faces with exactly n edges (so F3 is the number of triangular
faces, and so on), and let Vm be the number of vertices which have exactly

m edges ending at that vertex (equivalently, exactly m faces containing that
vertex). Clearly,
(5.6.2)
Theorem 5.6.2
(1)  For any polyhedron, 3F3 + 2F4 + F5 ≥ 12. In particular, among all of
the faces of a polyhedron, there are at least four faces each of which
has at most five edges.
(2)  For any polyhedron, F3 + V3 ≥ 8. In particular, any polyhedron has
either a triangular face, or a vertex with exactly three edges meeting at
that vertex (or both).
(3)  The integers F3 + F5 + F7 + · · · and V3 + V5 + V7 + · · · are even.
Remark There is equality in (1) and (2) in the case of the tetrahedron and
the cube, so both inequalities are best possible. Note that (1) implies that
one cannot make a polyhedron (however large and complicated) out of a
'random' collection of, say, hexagons. This is not obvious!
Proof As every edge bounds exactly two faces, we have
and as every edge contains exactly two vertices,
This proves (3). To prove (2), we note that (summing over k = 3, 4 . . .)
This shows that
(5.6.3)

which is stronger than (2). Finally, Euler's relation in the form
leads to the identity
and this gives (1).
We end this chapter with a result, due to Descartes, that is equivalent to
Euler's formula. The deficiency of a vertex v of a polyhedron P is 2π − ∑j
θj, where the θj are the angles at v of the faces that meet at v. The total
deficiency of P is the sum of the deficiencies of each of its vertices. For
example, if P is a cube, then the deficiency of each vertex is π/2, and so the
total deficiency of a cube is 4π. This is no accident.
Descartes' theorem The total deficiency of any polyhedron is 4π.
Proof We suppose that a polyhedron has F faces, E edges and V vertices.
We have seen that F = ∑m≥3 Fm and 2E = ∑m≥3 m Fm. Thus
As the sum of the interior angle of an m-gon is (m − 2)π, the total deficiency
of the polyhedron is D, where
It follows that D = 4π if and only if F − E + V = 2; thus Descartes' theorem
is equivalent to Euler's theorem.
Exercise 5.6

1.  Prove (in the notation of the text) that 3V3 + 2V4 + V5 ≥ 12.
2.  We have seen that F3 + V3 ≥ 8. Suppose that F3 + V3 = 8. Use (5.6.3) to
show that F3 and V3 are even, so that the possible values of (F3, V3) are
(0, 8), (2, 6), (4, 4), (6, 2) and (8, 0). Show that each of these pairs is
attained by some polyhedron.
3.  A deltahedron is a polyhdedron (that can be deformed into a sphere)
whose faces are congruent equilateral triangles. Show that every vertex
of a convex deltahedron has valency at most five. Deduce that for any
convex deltahedron, (F, E, V) must be of the form (2k, 3k, k + 2), where
k = 2, 3, . . . , 10. In fact, convex deltahedra exist for all of these k except
9. There exists such a polyhedron with eighteen triangular 'faces', but
some of these 'faces' are coplanar.

6
Quaternions and isometries
6.1 Isometries of Euclidean space
Our first objective is to understand isometries of 
.
Definition 6.1.1 A map f : 
 → 
 is an isometry if it preserves distances;
that is, if for all x and y, || f (x) − f (y)|| = ||x − y||.
Each reflection across a plane is an isometry, and we shall see later that
every isometry is a composition of reflections. Consider the plane Π given
by x·n = d, where ||n|| = 1, and let R be the reflection across Π. As n is the
normal to Π, we see that R(x) = x + 2tn, where t is chosen so that the
midpoint, x + tn, of x and R(x) lies on Π. This last condition gives
d = x·n + t, so that
(6.1.1)
It is geometrically obvious that (a) R(x) = x if and only if x ∈ Π, (b) for all
x, R(R(x)) = x, and (c) R is an isometry. These properties can be verified
algebraically from (6.1.1), as can the next result.
Theorem 6.1.2 A reflection across a plane Π is a linear map if and only if 0
∈ Π.
Now consider two parallel planes, say x·n = d1 and x·n = d2, and let R1 and
R2 denote the reflections in these planes. It is easy to see that R1

R2(x) = x + 2(d1 − d2)n, so that the composition of two reflections in
parallel planes is a translation (and conversely). Clearly, the translation is
by 2dn, where n is the common normal of the two planes, and d is their
distance apart.
Next, we consider the composition of reflections Rj in two distinct
intersecting planes Π1 and Π2. The planes intersect in a line L, and each Rj
fixes every point of L. In addition, Rj (Π) = Π for every plane Π that is
orthogonal to L. The action of R2 R1 on Π is a reflection across the line Π ∩
Π1 followed by a reflection across the line Π ∩ Π2; thus we see that R2 R1 is
a rotation of 
 about the axis L of an angle equal to twice the angle
between the planes Πj. This leads us to the following definition of a
rotation.
Definition 6.1.3 A rotation of 
 is the composition of reflections across
two distinct non-parallel planes. The line of intersection of the planes is the
axis of the rotation.
Notice that the fact that every rotation has an axis is part of our definition of
a rotation. Later we will give an alternative (but equivalent) definition of a
rotation in terms of matrices, but then we will need to prove that every
rotation has an axis. As a rotation of 
 has one fixed point and no axis,
the proof cannot be entirely trivial (in fact, it depends on the fact that every
real cubic polynomial has a real root). Each rotation of an odd-dimensional
space has an axis, but a rotation of an even-dimensional space need not
have an axis (because, as we shall see later, every real polynomial of odd
degree has a real root, whereas a real polynomial of even degree need not
have any real roots). As each reflection is an isometry, so is any
composition of reflections. The converse is also true, and in the following
stronger form.
Theorem 6.1.4 Every isometry of 
 is the composition of at most four
reflections. In particular, every isometry is a bijection of 
 onto itself.
Every isometry that fixes 0 is the composition of at most three reflections in
planes that contain 0.
A reflection R across a plane is a permutation of 
 whose square is the
identity map. Thus, in some sense, Theorem 6.1.4 is analogous to the result

that a permutation of a finite set can be expressed as a product of
transpositions. The common feature of these two results (and other results
too) is that a given map f is being expressed as a composition of simpler
maps fj for which 
. Our proof of Theorem 6.1.4 is based on the
following three simple results.
Lemma 6.1.5 Suppose that f is an isometry with f (0) = 0. Then for all x and
y, || f (x)|| = ||x|| and f (x)· f (y) = x·y (that is, f preserves norms and scalar
products).
Proof First, || f (x)|| = || f (x) − f (0)|| = ||x − 0|| = ||x||, so that f preserves
norms. Next,
so that f also preserves scalar products.
Lemma 6.1.6 If an isometry f fixes 0, i, j and k then f = I .
Proof Let f (x) = y. Then ||y − i|| = || f (x) − f (i)|| = ||x − i||, and as ||y|| = ||x||,
we have y·i = x·i. The same holds for j and k, so that y − x is orthogonal to
i, j and k. Thus y = x.
Lemma 6.1.7 Suppose that ||a|| ≠ ||b|| = 0. Then there is a reflection R
across a plane Π through 0 such that R(a) = b and R(b) = a.
Proof We may suppose that a ≠ b, since if a = b we can take any plane Π
through 0 and a. Our geometric intuition tells us that Π should be x·n = 0,
where n = (a − b)/(||a − b||, so let R be the reflection in this plane. Then
R(x) = x − 2(x·n)n, and a computation shows that R(a) = b. Note the use of
the identity 2a·(a − b) = ||a − b||2 in this computation.
The proof of Theorem 6.1.4 Let f be any isometry of 
. If f (0) ≠ 0 then
then there is a reflection R1 that interchanges 0 and f (0). If f fixes 0, we let

R1 = I . Thus R1 f is an isometry that fixes 0, and we let f1 = R1 f . As ||
f1(k)|| = ||k|| there is a reflection R2 in some plane through the origin that
interchanges f1(k) and k; thus R2 f1 is an isometry that fixes 0 and k. Let
f2 = R2 f1, and note that by Lemma 6.1.5, f (i) and f (j) are orthogonal unit
vectors in the plane x·k = 0. There is a reflection R3 in some vertical plane
through the origin that maps f (j) to j (and fixes 0 and k) so that now, R3 f2
fixes 0, j and k. Let f3 = R3 f2; then f3 maps i to ±i. If f3 fixes i, let R4 = I . If
not, let R4 be the reflection in x·i = 0. Then Lemma 6.1.6 implies that R4
f3 = I, so that f = R1 R2 R3 R4.
A slight modification of the proof just given yields the next result.
Theorem 6.1.8 The most general isometry f is of the form f (x) = A(x) + f
(0), where A is a linear map.
Proof Suppose that f is an isometry. Then A(x) = f(x) − f(0) is an isometry
that fixes 0, and the proof of Theorem 6.1.4 shows that g is a composition
of (at most three) reflections in planes through 0. As each such reflection is
a linear map, so is A.
It is clear that the composition of two isometries is an isometry, and the
composition of functions is always associative. Trivially, the identity map is
an isometry. If we write an isometry f as a composition, say R1 · · · Rp, of
reflections Rj, then f−1 = Rp · · · R1 and this is an isometry. This proves the
next result.
Theorem 6.1.9 The set of isometries of 
 is a group with respect to
composition.
Although it may seem obvious that the inverse of an isometry f is an
isometry, one has to prove (somewhere) that f−1 exists, and also that it is
defined on 
.
We have seen that every isometry can be expressed as the composition of
at most four reflections, and we want to give an example of an isometry that
cannot be expressed as a composition of fewer than four reflections. We

need some preliminary remarks. The reflection R given in (6.1.1) can be
written as 
, where 
 is the reflection in the plane x·n = 0, and T
(x) = x + 2dn. Thus every reflection is a reflection in a plane through 0
followed by a translation. Suppose we have (with the obvious notation) 
, and similarly for R2. Then, as 
1 is linear, we have
A similar argument applies to any composition of reflections; thus given
reflections R1, . . . , Rn there are reflections 
 across planes
through the origin, and a vector b, such that 
.
We can now give our example.
Example 6.1.10 Let f be the isometry defined as the rotation of angle π
about the k-axis followed by the translation x 
 x + k; thus f is a 'screw
motion' which is given explicitly by (x1, x2, x3) 
 (−x1, −x2, x3 + 1). Let A
1and A2 be the reflections in the planes x·i = 0 and x·j = 0, respectively; then
f (x) = A1 A2(x) + k. Now suppose that f can be written as a composition of
p reflections and, as above, write 
, where each 
 is
a re-flection in a plane through 0. As b = f (0) = k, we find that 
. We shall prove below that if Q is any reflection in a
plane though the origin then, for all vectors x, y and z, [Q(x), Q(y),
Q(z)] = −[x, y, z] (this is because Q reverses the orientation of the vectors).
Repeated applications of this rule applied to A1 A2 and to 
 give
(−1)2 = (−1)p so that p is even. It is clear that f is not the composition of
exactly two reflections (for then f would be a translation, or would have a
line of fixed points); thus f cannot be expressed as the composition of fewer
than four reflections.
The next result (which we have used in Example 6.1.10) shows how
scalar and vector products behave under a reflection in a plane through the
origin. In particular, such a reflection reverses the orientation of any three
vectors, and this fact was used in Example 6.1.10.
Theorem 6.1.11 Let R be the reflection in a plane Π that contains the
origin. Then for all vectors x, y and z,

(6.1.2)
(6.1.3)
(6.1.4)
Proof We may assume that Π is given by x·n = 0, where ||n|| = 1, so that
R(x) = x − 2(x·n)n, and (6.1.2) follows immediately. Next, (6.1.3) follows
from two applications of the formula (4.5.1) for the vector triple product,
for
This proves (6.1.3). Finally, (6.1.4) holds because
It is now clear how scalar and vector products behave under rotations that
fix the origin; if A is a rotation about the origin, then
(6.1.5)
and

Note that rotations about the origin preserve the orientation of vectors, and
this leads to our final result (which is proved without reference to matrices).
Theorem 6.1.12 The set of rotations of 
 that fix 0 is a group with respect
to composition.
Proof We need only show that the composition of two rotations is again a
rotation, for the rest of the requirements for a group are easily verified. Let
R3 = R2 R1, where R1 and R2 are rotations that fix 0. As we have remarked
above, each rotation, and therefore R3 also, preserves the orientation of
vectors, and this shows that if we express R3 as a composition of, say, p
reflections in planes through the origin, then p is even. However, by
Theorem 6.1.4, R3 can be expressed as a composition of at most three
reflections; thus we must be able to express R3 as a composition of exactly
two reflections, and hence R3 is a rotation.
Let f be an isometry, say f (x) = x0 + A(x), where A is the identity map I,
or a composition of k reflections in planes through the origin, where k is
one, two or three. We say that A is a direct isometry if A = I or k = 2;
otherwise we say that f is indirect. Direct isometries preserve orientation;
indirect isometries reverse orientation.
Definition 6.1.13 A screw-motion in 
 is a rotation about some line in 
,
followed by a translation in the direction of that that line.
We include the possibilities that the rotation is trivial (so the screw-
motion is a translation) and that the translation is trivial (so that the screw-
motion is a rotation). It is clear that every screw-motion is a direct isometry
of 
; it is less clear that the converse is true.
Theorem 6.1.14 Every direct isometry of 
 is a screw-motion.
Proof As every translation is a screw-motion, we may confine our attention
to a direct isometry f that is of the form f (x) = x0 + A(x), where A is a
rotation (so k = 2). We choose a vector, say a, lying along the axis of A. If

x0 is a scalar multiple of a, there is nothing to prove, but, in general, this
will not be so.
A rotation of the same angle as A, but whose axis is a line in the direction
a, and passing through a point w, is given by x 
 w + A(x − w); thus we
need to be able to show that f can be written in the form f
(x) = w + A(x − w) + λa for some vector w and some real λ. Equivalently,
because A is linear (so that A(x − w) = A(x) − A(w)), we need to be able to
find w and λ such that x0 = w − A(w) + λa. Now write x0 in the form
x1 + μa, where x1 ⊥ a; then we need w and λ to satisfy
w − A(w) + λa = x1 + μa. We take λ = μ, and then seek a solution w of
w − A(w) = x1. If W is the plane given by x·a = 0, it is apparent (by
elementary geometry, or by using complex numbers) that there is some w in
W such that w − A(w) = x1 and the proof is complete.
Finally, we consider the following natural problem. Given rotations R1
and R2, how can we find the axis and angle of rotation of the rotation R2 R1?
One way is to let Π0 be the plane containing the axes (which we may
assume are distinct) of R1 and R2, and let α0 be the reflection across Π0.
Then choose planes Π1 and Π2 (both containing 0) so that if α1 and α2
denote reflections across these planes, then R2 = α2α0 and R1 = α0α1. Then
R2 R1 = α2α0α0α1 = α2α1. Further, if we examine the intersection of these
planes with the unit sphere, then the calculations of angles and distances
involved are a matter of spherical (rather than Euclidean) geometry. We also
give an algebraic solution to this problem in terms of quaternions (in
Section 6.3), and in terms of matrices (in Chapter 11).
Exercise 6.1
1.  Find the formula for the reflection in the plane x2 = 0 followed by the
reflection in the plane x3 = 0. Find the fixed points of this
transformation, and identify it geometrically.
2.  Given unit vectors a and b, let Ra be the reflection across the plane x·a,
and similarly for Rb. Show that Ra Rb = Rb Ra if and only if a ⊥ b.
3.  Let Ra and Rb be the reflections in x·a = 0 and x·b = 0, respectively,
where ||a|| = 1 and ||b|| = 1. Find a formula for the composition Ra(Rb(x)).

This composition of reflections is a rotation whose axis is the line of
intersection of the two planes. Explain why this axis is the set of scalar
multiples of a × b, and verify analytically that both Ra and Rb fix every
scalar multiple of a × b.
4.  Let L1, L2 and L3 be three lines in the complex plane , each containing
the origin, and let Rj be the reflection (of  into itself) across Lj. Show
that R1 R2 R3 is a reflection across some line in . Show, however, that if
the Lj do not have a common point then R1 R2 R3 need not be a
reflection.
5.  Suppose that R1, . . . , Rp, S1, . . . , Sq are all reflections across planes that
contain 0. Show that if R1 · · · Rp = S1 · · · Sq then (−1)p = (−1)q. Compare
this result with the definition of the signature of a permutation.
6.2 Quaternions
In 1843 Hamilton introduced quaternions as a way to generalize the algebra
of complex numbers to higher dimensions. We shall use them to represent
reflections and rotations in 
 algebraically. There are various equivalent
ways of describing quaternions but fundamentally they are points in
Euclidean fourdimensional space 
. Before we consider the various
representations of quaternions it is convenient to introduce the idea of a
Cartesian product of two sets. Given any two sets A and B, the Cartesian
product set, which we denote by A × B, is the set of all ordered pairs (a, b),
where a ∈ A and b ∈ B. In as far as there is a natural identification of the
objects
in 
, 
 × 
 and 
 × 
, respectively, we can identify each of these
spaces with the others. Formally, a complex number is an ordered pair of
real numbers so that 
 = 
 (this really is equality here, and no
identification is necessary), so we can add  ×  to this list. Notice that a
point in  × 
 is an ordered pair (a, x), where a ∈ 
 and x ∈ 
, and we
can write this as a + x1i + x2j + x3k.
Definition 6.2.1 A quaternion is an expression a + bi + cj + dk, where a, b,
c and d are real numbers. The set of quaternions is denoted by 
.

We now define an addition and a multiplication on the set 
 of quaternions,
and it is sufficient to do this (and to carry out any subsequent calculations)
in any of the models given above since a statement about one model can be
converted on demand into a statement about another model. The addition of
quaternions is just the natural vector addition in 
, namely
and clearly 
 is an abelian group with respect to addition. In terms of the
representation in  × 
 this becomes (a, x) + (b, y) = (a + b, x + y), and in
terms of  ×  it is (z1, z2) + (w1, w2) = (z1 + w1, z2 + w2).
The multiplication of quaternions is rather special. We recall how in
Section 3.1 we motivated the multiplication of complex numbers by
considering the formal product
and then imposing the condition that i2 = −1 to return us to something of the
form p + iq. We shall now adopt a similar strategy for quaternions. We write
the general quaternion as a + bi + cj + dk and then, using the distributive
laws, we take a formal product of two such expressions. We now need to
specify what we mean by the products of any two of i, j and k, and these are
specified as follows:
(1)  i2 = j2 = k2 = −1, and
(2)  ij = k = −ji, jk = i = −kj, ki = j = −ik.
Notice that the product of one of these vectors with itself is a scalar;
otherwise the product is a vector. Note also how (2) mimics the vector
product of these vectors, and (1) mimics the formula i2 = −1. We shall also
assume that any real number commutes with each of the vectors i, j and k;
for example, 3j = j3. Of course, we could rewrite this whole discussion
formally in terms of four-tuples of real numbers, but to do so would not
contribute to an understanding of the underlying ideas. Indeed, the
following example is probably more useful:

We note that the set of quaternions is closed under multiplication, but that
multiplication is not commutative (for example, ij ≠ ji); the reader should
constantly be aware of this fact. It is an elementary (though tedious) matter
to check that the associative law holds for multiplication, and that the
distributive laws also hold. If we rewrite the definition of multiplication in
terms of quaternions in the form (a, x) we see a striking link with the vector
algebra on 
; this is given in the next result.
Theorem 6.2.2 The product of quaternions is given by the formula
(6.2.1)
where x·y and x × y are the scalar and vectors products, respectively.
The proof is by pure computation and we omit the details, but the reader
should certainly verify this when a = b = 0. Note that multiplication by the
'real number' (a, 0) gives (a, 0)(b, y) = (ab, ay) = (b, y)(a, 0). This shows
that the quaternion (1, 0) acts as the identity element with respect to
multiplication. Theorem 6.2.2 is of special interest when the two
quaternions are pure quaternions, which we now define (these play the role
of the purely imaginary numbers in ).
Definition 6.2.3 A pure quaternion is a quaternion of the form (0, x) where
x ∈ 
. The set of pure quaternions is denoted by 
.
When applied to pure quaternions, Theorem 6.2.2 yields the important
formula
(6.2.2)
in which the quaternion product is expressed only in terms of the scalar and
vector products. This shows that if q is the pure quaternion (0, x), and

||x|| = 1, then q2 = (−1, 0), and this extends the identities i2 = j2 = k2 = −1 to
all pure quaternions of unit length. It also shows that some quaternions, for
example −1, have infinitely many square roots.
Let us return to the notation given in Definition 6.1.1 so that we now
write a + x instead of (a, x). The conjugate of a quaternion is defined by
analogy with the conjugate of a complex number.
Definition 6.2.4 The conjugate of the quaternion a + x is a − x. We denote
the conjugate of a quaternion q by .
Theorem 6.2.5 For quaternions p and q, we have 
.
This follows directly from Theorem 6.2.2, but note that the order of terms
here is important. Finally, we note an important identity: if q = a + x then,
from (6.2.1),
Of course,
and as this is the distance between the origin and the point a + x viewed as a
point of 
, we write this as ||a + x||. This is the norm of the quaternion, and
it obeys the important rule that for any quaternions p and q,
The proof is easy for, using the associative law and Theorem 6.2.5,
(6.2.3)
as required.
Finally, we have just seen that if q is not the zero quaternion, and if 
, then qq′ = 1 = q′q. Thus each non-zero quaternion has a

multiplicative inverse, and we have proved the following result.
Theorem 6.2.6 The set of non-zero quaternions is a non-abelian group with
respect to the multiplication defined above.
In conclusion, the set 
 of quaternions, with the addition and multiplication
defined above, satisfies all of the axioms for a field except that
multiplication is not commutative. Accordingly, the quaternions are
sometimes referred to as a skew field.
Exercise 6.2
1.  Simplify the sequence of quaternions i, ij, ijk,ijki, ijkij, . . . .
2.  Let q = a + x. Express a and x in terms of q and .
3.  Let q be a pure quaternion. Compute q2, and hence show that q−1 = −||q||
−2q.
4.  Show that {1, −1, i, −i, j, −j, k, −k} is a group under multiplication.
5.  Prove (6.2.1).
6.  Prove Theorem 6.2.5.
7.  Show that a quaternion q is a pure quaternion if and only if q2 is real and
not positive.
8.  Let p and q be pure quaternions. Show that (in the obvious sense) pq is
the vector product p × q if and only if p ⊥ q (in 
). In the same sense,
show that pq = qp if and only if p × q = 0.
6.3 Reflections and rotations
The formula (6.1.1) for a reflection across a plane is simple; however the
formula for a rotation, obtained by combining two reflections, is not. In this
section we shall see how quaternions provide an alternative algebraic way
to express reflections and rotations. Quaternions are points of 
, but we
shall pay special attention to the space 
 of pure quaternions which we
identify with 
.
Consider the map θ : 
 → 
 given by θ(y) = −qyq−1, where q is a non-
zero pure quaternion. It is clear that θ is a linear map; that is, θ(λ1 y1 + λ2
y2) = λ1θ(y1) + λ2θ(y2) and, trivially, θ(0) = 0 and θ(q) = −q. These facts

suggest that θ might be related to the reflection across the plane with
normal q, and we shall now show that this is so.
Theorem 6.3.2 The map θ : 
 → 
 given by θ(y) = −qyq−1, where q is a
non-zero pure quaternion, maps the set 
 of pure quaternions into itself.
Further, when 
 is identified with 
, and q with q, θ is the reflection
across the plane x·q = 0. If q is a pure quaternion of unit length, then
θ(y) = qyq.
Proof We begin by looking at the fixed points of θ in 
. A general
quaternion y is fixed by θ if and only if −qy = yq, and if we write q = (0, q)
and y = (0, y), and use (6.2.2), we see that a pure quaternion y is fixed by θ
if and only if y·q = 0. Now let Π be the plane in 
 given by x·q = 0; then θ
fixes each point of Π. Now take any pure quaternion y. We can write
y = p + λq, where λ is real and p ∈ Π. Thus, as θ is linear,
This shows that θ(y) is a pure quaternion, and also that it is the reflection of
y in Π. Finally, if q is a pure quaternion of unit length, then q2 = −1 so that
q−1 = −q and θ(y) = qyq.
Before moving on to discuss rotations, we give a simple example.
Example 6.3.3 The reflection across the plane x·k = 0 is given by (x1, x2,
x3) 
 (x1, x2, −x3) or, in terms of quaternions, x 
 kxk. We also note (in
preparation for what follows) that as k−1 = −k, the map x 
 kxk−1is given
by (x1, x2, x3) 
 (−x1, −x2, x3), and this is a rotation of angle π about the k-
axis.
We now use quaternions to describe rotations that fix the origin. Consider
the rotation R obtained by the reflection across the plane with normal unit p
followed by the reflection across the plane with unit normal q. According to
Theorem 6.3.2,
(6.3.1)

where p = (0, p) and q = (0, q). This observation yields the following result.
Theorem 6.3.4 Let 
, where n is a unit vector. Then r is
a unit quaternion, and the map 
 is a clockwise rotation by
an angle θ about the axis in the direction n.
Proof Choose unit vectors p and q such that 
 and 
. Then the rotation described (geometrically) in the theorem
is reflection in x·p = 0 followed by reflection in x·q = 0; thus it is given by
R in (6.3.1). Now a simple calculation using (6.2.2) shows that qp = −r and 
. Thus 
. Finally, as ||r|| = 1 we see that 
.
Theorem 6.3.4 provides a way to find the geometric description of the
composition of two given rotations. Let Rr and Rs be the rotations
associated with the quaternions
Then the composition Rs Rr is the map 
, and as sr
is also a unit quaternion (which is necessarily of the form 
for some unit vector h), we see that Rs Rr = Rsr. By computing sr as a
quaternion product, we can find the axis and angle of rotation of the
composition Rs Rr. We urge the reader to experiment with some simple
examples to see this idea in action.
Exercise 6.3
1.  Let r be a unit quaternion (||r|| = 1). Show that we can write r = cos
θ + sin θ p, where p is a unit pure quaternion.
2.  Use Theorem 6.2.2 to show that if p and q are pure quaternions ( p = p
and q = q), and if p ⊥ q, then −qpq−1 = p.
3.  Use quaternions to find the image of x under a rotation about the k-axis
of angle π/6. Now verify your result by elementary geometry.
4.  Let Π be the plane given by x·n = 0, where n = (1, −1, 0), and let R be
the reflection across Π. Write x = (a, b, c) and y = R(x). Verify, both
using elementary geometry and quaternion algebra, that y = (b, a, c).

7
Vector spaces
7.1 Vector spaces
The essential feature of vectors is that if we add them, or take scalar
multiples of them, we obtain another vector; thus a linear combination of
vectors is again a vector. The general theory of vector spaces is based on
this simple idea, and in this chapter we study abstract spaces with this
property. The fact that the general theory of vector spaces is so widely
applicable makes the use of boldface type for vectors an awkward
convention, so we now abandon it. It should be clear from the discussion
which symbols are vectors and which are scalars (and if we think it might
not be, we shall write λ ∈ 
, x ∈ V, and so on or, sometimes, add V or  as
a suffix).
A vector space consists of a set V of vectors and a field  of scalars. In
this book, the field 
 of scalars will always be 
 or 
, and when the
argument holds for both of these we shall use . The vectors form an abelian
group with respect to an operation which is always called 'addition' and
denoted by +. The scalars allow us to take a 'scalar multiple' of a vector;
thus to each scalar λ and each vector v we can associate a vector λv. As a
consequence of these assumptions, given scalars λ1, . . . , λn, and vectors v1,
. . . , vn, we can form the linear combination λ1v1 + · · · + λnvn and this too is
a vector. We stress that some statements may be true for one choice of  and
false for the other; for example, in the vector space , the vectors 1 and i are
scalar multiples of each other if 
 (the vector i is the scalar multiple i of
the vector 1), but not if 
 (for there is no real scalar λ such that 1 = λi,
or i = λ 1). In order to develop a worthwhile theory, we need to include some

(natural) assumptions that will ensure that our algebraic manipulations run
smoothly, and these are given in the following formal definition of a vector
space.
Definition 7.1.1 A vector space V over the field  is a set V of vectors, a
field 
 of scalars, an addition u + v of vectors u and v, and a scalar
multiplication λv of a scalar λ with a vector v, which together satisfy the
following properties:
(1)  V is an abelian group with respect to the operation +;
(2)  if λ ∈  and v ∈ V, then λv ∈ V ;
(3)  for all λ, μ, v and w, (λ + μ)v = λv + μv, and λ(v + w) = λv + λw.
(4)  for all λ, μ and v, λ(μv) = (λμ)v;
(5)  for all v, 1v = v, where 1 is the multiplicative identity in .
We say that V is a real vector space if 
, and a complex vector space if 
.
We stress again that the underlying idea is that it is possible to form linear
combinations of vectors, and that the associated 'arithmetic' satisfies all of
the familiar rules. The simplest examples of vector spaces are the spaces of
n-tuples of real (or complex) numbers.
Example 7.1.2 The set of ordered n-tuples (x1, . . . , xn) of real numbers is
denoted by 
. The rules for addition and scalar multiplication are
where λxj is the usual product in 
, and 
 is a real vector space. Although
we shall not use it for a while, this is an appropriate point to define the scalar
product in 
. Given x and y in 
, their scalar product is (by analogy with 
) given by
Similarly, the set 
 of ordered n-tuples (z1, . . . , zn) of complex numbers is a
complex vector space with the same rules as above (except that λ is now a

complex number). However, in this case, the scalar product is
(and not 
 because we want to insist that z·z ≥ 0 with equality if and
only if z is the zero vector).
We shall also need to use column vectors of real and complex numbers;
for example
The space of real n-tuples written as column vectors is denoted by 
,
where the symbol 't' here stands for the transpose (which converts rows into
columns, and columns into rows, in the obvious way). The addition and
scalar multiplication in 
 are the obvious ones and 
 is also a real
vector space. Of course, 
 differs from 
 only in the way that we write
the vectors, but the reason for making this distinction will emerge later. In
the same way the space 
 of columns of n complex numbers is a complex
vector space.
The following examples illustrate the wide variety of vector spaces that
exist and, consequently, the wide applicability of the results that we shall
prove shortly.
Example 7.1.3 The set of all real-valued functions on a set X is a real vector
space. Addition and scalar multiplication are defined so that f + g is the
function x 
 f (x) + g(x), and λ f is the function x 
 λ f (x). Similarly, the
space of complex valued functions on X is a complex vector space. We are
not assuming here that the set X has any algebraic structure.
The class of vector spaces constructed in Example 7.1.3 contains a huge
number of interesting special cases; for example, the space of all real
polynomials is a real vector space, and the complex polynomials form a
complex vector space. The space of all real (or complex) polynomials of
degree at most k is also a vector space, but the set of polynomials of exact
degree k is not, for the sum of two polynomials of degree k need not be of
degree k. In these examples the zero vector is the zero polynomial; this is the

constant function with value 0 and it is not a number. We shall discuss vector
spaces of polynomials in greater detail later. The set of all trigonometric
polynomials
where the aj and bj are real numbers, is a vector space over 
. The set V of
solutions of the differential equation 
 is a vector space over 
.
We know that each solution is of the form x(t) = a cos μt + b sin μt, but it is
important to realize that V is a vector space because a linear combination of
solutions is again a solution, and we can see this without solving the
equation. This is true of all linear differential equations, and this is where
the terminology comes from. Finally, a real sequence is a map f : {1, 2, . . .}
→ 
, the sequence being f (1), f (2), . . . , and the set of real sequences is a
vector space. The same is true of complex sequences, and also of finite
sequences, which brings us back to Example 7.1.2.
We end this section by proving a simple, and not unexpected, result about
general vector spaces which will be used frequently (and without explicit
mention). In general, we denote the additive and multiplicative identities in
the field  by 0 and 1, respectively, and the zero vector in V also by 0 (it
should be obvious in any context whether 0 means the zero vector or the
zero scalar). However, in order to to make the following lemma completely
clear, we shall temporarily use 0  and 1  for these elements in , and 0V for
the zero vector. This lemma is fundamental, and its proof is tedious, but we
shall soon move on to more interesting things. Note that −1  is the additive
inverse of the multiplicative identity 1  in the field .
Lemma 7.1.4 Let V be a vector space over , and suppose that λ ∈  and v
∈ V. Then
(1)  λv = 0V if and only if λ = 0  or v = 0V ;
(2)  (−1 )v is the additive inverse of v.
Proof First, for any scalar λ, and any vector v,

By Lemma 1.2.5, 0 v = 0V = λ0V. Now suppose that λv = 0V. If λ ≠ 0 , then
λ−1 
exists 
in 
 
so 
(by 
what 
we 
have 
just 
proved),
0V = λ−10V = λ−1(λv) = (λ−1λ)v = 1 v = v. Thus if λv = 0V then λ = 0  or
v = 0V. Finally,
so that (−1 )v is the (unique) additive inverse of v.
Exercise 7.1
1.  Let Π1 and Π2 be the planes in 
 given by 3x + 2y − 5z = 0 and
3x + 2y − 5z = 1, respectively. Show that Π1 is a vector space, but that Π2
is not.
2.  Show that 
 is a vector space over .
3.  Verify that the set of the even complex polynomials p (that is, the p with
p(−z) = p(z)) is a complex vector space. Is the set of all odd complex
polynomials p (with p(−z) = − p(z)) a complex vector space?
4.  Solve the differential equation d2x/dt2 = x, and show that the set of
solutions is a real vector space. Solve the equation (dx/dt)2 = x. Is this set
of solutions a vector space? Which of these differential equations is
linear?
5.  Let V be a vector space over  and let v1, . . . , vm be any vectors in V.
Show that the set V0 of all linear combinations of v1, . . . , vm is a vector
space over ; it is the vector space generated by v1, . . . , vm.
7.2 Dimension
We shall now define what we mean by the dimension of a vector space, and
we shall confirm that 
 has dimension two, 
 has dimension three, and
so on. We begin with a definition and a theorem which we need in order to
define dimension.
Definition 7.2.1 The set of vectors v1, . . . , vn of vectors in a vector space V
is a basis of V if, for every v in V, there exist unique scalars λj such that 
.

We stress that this definition asserts both the existence of the scalars λj and
their uniqueness. The next result is fundamental.
Theorem 7.2.2 Let V be a vector space and suppose that v1, . . . , vn and w1,
. . . , wm are two bases of V. Then m = n.
Theorem 7.2.2 enables us to say what we mean by the dimension of a vector
space. To each non-trivial vector space V with a finite basis we can assign a
positive integer, namely the number of elements in any basis of V, and this is
the dimension of V.
Definition 7.2.3 We say that a vector space V has finite dimension, or is
finite dimensional, if it has a finite basis, and the dimension dim (V) of V is
then the number of elements in any basis of V. If V = {0} we put dim
(V) = 0. If V has no finite basis, then V is infinite dimensional.
The proof of Theorem 7.2.2 As v1, . . . , vn is a basis of V, each wk can be
expressed as a linear combination of the vj; thus for each k there are scalars
λ1k, . . . , λnk such that
Likewise, for each j there are scalars μ1j , . . . , μmj such that
These give
As the wj form a basis, we may equate coefficients of wk on each side of this
equation (recall the uniqueness in Definition 7.2.1) and obtain

Summing both sides of this equation over k = 1, . . . , m now gives
However, the same argument with vj and wk interchanged gives
and as these last two double sums are the same (they differ only in the order
of summation), we see that n = m.
It is now time to introduce some useful terminology which separates the
two essential features of a basis.
Definition 7.2.4 The set of vectors v1, . . . , vn span V if every v in V can be
expressed as a linear combination of v1, . . . , vn in at least one way; that is, if
there exist scalars λj such that 
.
It is possible for v1, . . . , vn to span V and have 
 with (λ1,
. . . , λn) ≠ (μ1, . . . , μn). For example, if v1 and v2 span V, then so do v1, v2,
v3, where v3 = v1 + v2, and 1v1 + 1v2 + 0v3 = 0v1 + 0v2 + 1v3.
Definition 7.2.5 The vectors v1, . . . , vn are linearly independent if every v
in V can be expressed as a combination of v1, . . . , vn in at most one way;
that is, if 
, then λj = μj for each j.
If the vectors v1, . . . , vn are linearly independent, there may be some v
that cannot be expressed as 
; for example i, j are linearly independent,
but no linear combination of them is k.

Clearly, v1, . . . , vn is a basis of V if and only if v1, . . . , vn are linearly
independent and span V. There is an alternative (equivalent) approach to
linear independence which is often taken as the definition of linear
independence. The vectors v1, . . . , vm are linearly independent if
implies that λ1 = μ1, . . . , λm = μm; in other words, if we can equate the
coefficients of v1, . . . , vn in any such equation. In particular, the vectors v1,
. . . , vn are linearly independent if and only if ρ1v1 + · · · + ρmvm = 0 implies
that ρ1 = · · · = ρm = 0. This is probably the easiest way to check the linear
independence of vectors.
To show that 
 has dimension two it suffices to show that e1 = (1, 0)
and e2 = (0, 1) is a basis of 
. As (x, y) = xe1 + ye2 we see that e1, e2 span 
. Next, if some vector v can be expressed as ae1 + be2 = v = ce1 + de2,
then (a, b) = (c, d) so that a = c and b = d; thus e1 and e2 are linearly
independent. This shows that 
 has a basis consisting of two vectors and
so dim(
) = 2. This argument extends immediately to 
, and shows that
the vectors
(7.2.1)
form a basis of 
; thus dim(
) = n. We call e1, . . . , en the standard basis
of 
. An entirely similar statement holds for 
, and we give a formal
statement of these results.
Theorem 7.2.6 The vector space 
 over 
 has dimension n. The vector
space 
 has dimension n over , and dimension 2n over .
For example, (1, 0), (i, 0), (0, 1) and (0, i) form a basis of the vector space 
 over  for
where the scalars x, y, u and v are all real. We give two more examples.

Example 7.2.7 If a plane in 
 is a vector space, then it must contain the
zero vector in 
. Let Π be the plane given by x + y + z = 0. As the condition
x + y + z = 0 is preserved under the formation of linear combinations, and as
(0, 0, 0) is in Π, we see that Π is a vector space (formally, there is more to
check here, but we omit the details). The vectors v1 = (−1, 1, 0) and v2 = (−1,
0, 1) are in Π, and they span Π because if (x, y, z) ∈ Π then x = −(y + z) and
so
On the other hand, if 0 = λ1v1 + λ2v2 then (−λ1 − λ2, λ1, λ2) = (0, 0, 0) and
hence λ1 = λ2 = 0; thus v1 and v2 are linearly independent. We deduce that
dim(Π) = 2.
Example 7.2.8 Let V be the vector space of solutions of the equation
this equation is said to be 'linear' precisely because any linear combination
of solutions is again a solution. Now ekx is a solution if and only if
k2 − 2k − 3 = 0 (that is, k is 3 or −1) so that y = ae3x + be−x is a solution for
any a and b. In particular,
is a solution with y(0) = A and y′(0) = B. We shall take for granted the fact
that any solution y(x) of the given equation is completely determined by the
values y(0) and y′(0) (for the equation gives y″(0) and the higher derivatives
of y are obtained by differentiating both sides of the equation). This means
that the general solution of the equation is a linear combination λe3x + μe−x.
However, these functions are linearly independent for if λe3x + μe−x = 0 for
all x, then (putting this function and its derivative equal to zero when x = 0)
we see that λ = μ = 0. It follows that the space of solutions is a vector space
of dimension two with basis e3x and e−x. These ideas apply to any linear
differential equation with constant coefficients, and they show that the

solution space of an n-th order linear differential equation has dimension n.
This discussion places the so-called n 'arbitrary constants' in the solution on
a firm foundation, for we now see that they are just the scalar coefficients in
a general linear combination of the n basis elements.
We end this section with the following very important result.
Theorem 7.2.9 Let V be any finite dimensional vector space.
(a)  Suppose that the vectors v1, . . . , vn span V. Then dim(V) ≤ n, and some
subset of v1, . . . , vn is a basis of V .
(b)  Suppose that u1, . . . , um are linearly independent vectors in V . Then m
≤ dim(V), and u1, . . . , um is a subset of some basis of V .
This shows that dim(V) is the smallest number of vectors in any spanning
set, and that any spanning set can be contracted (by deleting some of its
members) to a basis of V. Likewise, it shows that dim(V) is the largest
number of linearly independent vectors in V, and that every linearly
independent set of vectors can be extended to a basis of V. There are two
corollaries of Theorem 7.2.9 that are worthy of explicit mention.
Corollary 7.2.10 Suppose that V has dimension k. If v1, . . . , vk span V, or
are linearly independent, then they are a basis of V .
Corollary 7.2.11 If a vector space has a finite spanning set, then it is finite
dimensional.
The proof of Theorem 7.2.9 We prove (a). The vectors v1, . . . , vn span V. If
they are linearly independent they are a basis of V and dim(V) = n. We may
suppose, then, that they are not linearly independent so there are scalars λ1,
. . . , λn, not all zero, such that λ1v1 + · · · + λnvn = 0. By relabelling, we may
assume that λn ≠ 0, and then
This implies that v1, . . . , vn−1 span V, and this process can be continued
until we obtain a subset of k vectors from v1, . . . , vk which span V and

which are also linearly independent. In this case dim(V) = k ≤ n.
We now prove (b). As V is finite dimensional, it has a basis, say w1, . . . ,
wk. Now suppose that u1, . . . , um are linearly independent vectors in V. If w1
is not a linear combination of u1, . . . , um then the vectors u1, . . . , um, w1 are
linearly independent because if
then μ = 0 (else w1 is a linear combination of the uj contrary to our
assumption), and then the linear independence of u1, . . . , um shows that
λ1 = · · · = λm = 0. This argument shows that either (a) w1 is a linear
combination of u1, . . . , um, or (b) u1, . . . , um, w1 are linearly independent.
In both cases, u1, . . . , um can be extended to a linearly independent set of
vectors such that some linear combination of these is w1. The process
continues, considering w2, then w3 and so on, until we eventually arrive at a
set of linearly independent vectors, say 
, which includes u1, . . . ,
um, and various linear combinations of which give each of w1, . . . , wn. As
w1, . . . , wn span V so do 
, and as 
 are linearly
independent we conclude that are a basis of V. This implies that r = k, and
that m ≤ r = k = dim(V).
Exercise 7.2
1.  Show that the vector space 
 over  has dimension 2n.
2.  Let V = {(t, t2) ∈ 
 : t ∈ } (a parabola), and define addition and
scalar multiplication by (t, t2) + (s, s2) = (t + s, (t + s)2) and λ(t, t2) = (λt,
λ2t2). Show that V is a real vector space of dimension one.
3.  Show that the set of points (x1, x2, x3) in 
 that satisfy the simultaneous
equations x1 + x2 = 0 and 2x1 − 3x2 + 4x3 = 0 is a vector space. Find a
basis for this space, and hence find its dimension.
4.  Show that {(x1, x2, x3, x4) ∈ 
4 : 2x1 − 3x2 + x3 − x4 = 0} is a real vector
space of dimension three. Find a basis for this space.
5.  Show that the set of complex polynomials p of degree at most three that
satisfy p′(0) = 0 is a vector space over . What is its dimension?

7.3 Subspaces
A subset U of a vector space V over  is a subspace of V if U is a vector
space in its own right (with the same operations as in V, and over the same
field ). We also say that a subspace U of V is a proper subspace of V if U ≠
V. If U is a subspace of V, then
(a)  0V ∈ U, and
(b)  if u1, . . . , un are in U, and λ1, . . . λn are in , then λ1u1 + · · · + λnun is in
U.
The second of these conditions is clear, but the first might not be (for
perhaps U has a zero vector of its own, distinct from the zero vector 0V of
V). However, the zero scalar multiple of any vector in U is 0V, so this is
certainly in U. As U is an additive group, its identity element is unique so it
must be 0V; thus 0V is indeed the zero vector for U. In fact, (a) and (b) are
also sufficient for U to be a subspace for, as is easily checked, all the other
requirements for U to be a vector space hold automatically in U because
they hold in V. Note that if u ∈ U and if (b) holds, then (−1)u ∈ U and so
−u ∈ U because (−1)u = −u (thus the additive inverse of a vector in U
coincides with its additive inverse in V). Because of their importance, we
give a formal statement of these facts.
Theorem 7.3.1 A subset U of a vector space V is a subspace of V if and only
if (a) 0V ∈ U, and (b) any linear combination of vectors in U is also in U.
We remark that (a) here can be replaced by the statement that U is non-
empty, for then there is some u in U and hence, by (b), u + (−u) ∈ U.
However, (b) alone is not sufficient, for (b) is satisfied when U is the empty
set and this is not a vector space (for it has no zero vector).
Now suppose that U is a subspace of a finite dimensional vector space V
and let u1, . . . , um be a basis of U. Then u1, . . . , um are linearly independent
vectors in U, and hence also in V, and so, by Theorem 7.2.9,

If equality holds here, then u1, . . . , um must be a basis of V for otherwise,
we could extend u1, . . . um to be a basis of V with more than m elements in
the new basis. This argument, together with Theorem 7.2.9, proves the
following result.
Theorem 7.3.2 Let U be a subspace of a finite dimensional space V. Then
dim(U) ≤ dim(V) with equality if and only if U = V. Moreover, any basis of U
can be extended to a basis of V.
It is clear from Theorem 7.3.1 that if U and W are subspaces of V then so is
U ∩ W, where
Indeed, given vectors v1, . . . , vr in U ∩ W; they are in U and hence so is any
linear combination of them. Moreover, as they are in W, this same linear
combination is in W and hence also in U ∩ W. As this argument is valid for
any collection of subspaces (even an infinite collection) we have proved the
next result.
Theorem 7.3.3 The intersection of any collection of subspaces of V is a
subspace of V.
A simple and familiar illustration of this can be found in 
. The non-trivial
subspaces of 
 are the lines through the origin, and the planes through the
origin. The intersection of two such planes is either a plane (when the two
planes are identical) or a line (when they are distinct). The intersection of
two distinct, parallel planes is empty, but this does not contradict Theorem
7.3.3 because one of these planes does not contain the zero vector so it is not
a subspace of 
.
Given two subspaces U and W of V, their union
is not, in general, a subspace: for example, the union of two lines in 
 is
never a plane (and hardly ever a line). Because of this, we consider the sum
U + W (instead of the union) of U and W which is defined as follows.

Definition 7.3.4 Given two subspaces U and W of V, the sum, or join, of U
and V is
In the same way, for subspaces Ui, U1 + · · · + Uk is the set of vectors of the
form u1 + · · · + uk, where uj ∈ Uj.
It is clear from Theorem 7.3.1 that U1 + · · · + Uk is a subspace of V. It is
also clear that U + W is the smallest subspace of V that contains both U and
W. Indeed, if V0 is any subspace of V that contains U and W, then V0
contains all vectors u in U, all vectors w in W, and hence all sums of the
form u + w. Thus V0 contains U + W. The dimensions of the four subspaces
U, W, U ∩ W and U + W of V are related by a very simple formula.
Theorem 7.3.5 Suppose that U and W are subspaces of a finite dimensional
vector space V. Then
(7.3.1)
Proof The smallest of these four subspaces is U ∩ W, so we begin by taking
any basis (v1, . . . , vp) of this. As v1, . . . , vp are linearly independent we can
extend v1, . . . , vp to a basis (v1, . . . , vp, u1, . . . , ur) of U, and also to a basis
(v1, . . . , vp, w1, . . . , ws) of W. We claim that
(7.3.2)
is a basis of U + W. If this is so, then (7.3.1) follows immediately as we have
dim(U) = p + r, dim(W) = p + s, dim(U ∩ W) = p, and
dim(U + W) = p + r + s.
Now any vector in U + W is of the form u + w, where u ∈ U and w ∈ W,
and this shows that the vectors in (7.3.2) span U + W. To show that these
vectors are linearly independent, we suppose that there are scalars αj, βj and
γj such that
(7.3.3)

Now this implies that the vector γ1w1 + · · · + γsws is in U, and as it is (by
definition) in W, it is in U ∩ W. This means that it is of the form δ1v1 + · ·
· + δpvp for some scalars δi, and from this we deduce that
As this is a linear relationship between the linearly independent vectors v1,
. . . , v p, u1, . . . , ur , we can now deduce that β1 = · · · = βr = 0. If we now
use this in conjunction with (7.3.3) and the fact that the vectors v1, . . . , v p,
w1, . . . , ws are linearly independent, we see that each αi and each γk in
(7.3.3) is zero. This shows that the vectors in (7.3.2) are indeed linearly
independent, and it completes the proof.
We illustrate Theorem 7.3.5 with two examples.
Example 7.3.6 Let U and V be subspaces of 
, and suppose that
dim(U) = 2 and dim(V) = 3. What are the different possibilities? Now by
Theorem 7.3.5, dim(U ∩ V) + dim(U + V) = 5, and, of course,
It follows that either dim(U ∩ V) = 1 and U + V = 
, or dim(U ∩ V) = 2
and dim(U + V) = 3. In the latter case U = U ∩ V and V = U + V, and each of
these imply that U ⊂ V. Thus the two possibilities are (a) U ∩ V is a line in 
 and U + V = 
, or (b) U ⊂ V.
Example 7.3.7 Let
Now W = {(a, b, 2a + 3b, c, d): a, b, c, d ∈ 
}, and it is easy to see from
this that the four vectors (1, 0, 2, 0, 0), (0, 1, 3, 0, 0), (0, 0, 0, 1, 0) and (0, 0,
0, 0, 1) is a basis of W. This shows that dim(W) = 4, and a similar argument
shows that dim(U) = 4. Moreover,

It follows that either
(a)  dim(U ∩ W) = 3 and dim(U + W) = 5, or
(b)  dim(U ∩ W) = dim(U + W) = 4.
As U ∩ W ⊂ U ⊂ U + W, (b) would imply that U ∩ W = U = U + W and
similarly, U ∩ W = W. This implies that U = W and as this is clearly not so,
we see that (b) cannot hold. We deduce that (a) holds, and hence U and W
intersect in a subspace of dimension three. In essence, we have shown that
the common solutions of two (independent) equations in five unknowns is a
vector space of dimension three; this idea will be discussed in more detail in
Chapter 9.
Two linearly independent vectors in 
 determine a plane through the
origin, and this plane is characterized as being the smallest subspace of 
that contains the two vectors. We shall now show that this idea can be used
in a general vector space. Given any set X of vectors in a vector space V, we
consider the class of all subspaces of V that contain X (of course, V itself is
one such subspace). The intersection of all of these subspaces is again a
subspace of V (Theorem 7.3.3), and clearly it is the smallest subspace of V
that contains X. The important points here are that the intersection of these
subspaces does exist, and it is a subspace, and these make the following
definition legitimate.
Definition 7.3.8 Let X be any non-empty subset of a vector space V. Then
there is a smallest subspace of V that contains X, and we say that this is the
subspace of V generated by X.
There is another more constructive approach to the subspace generated by X.
Let W be the set of all finite linear combinations of vectors in X. Then W is
clearly a subspace of V and, by construction, X ⊂ W. As any subspace of V

that contains X must obviously contain W, we see that W is indeed the
subspace of V generated by X. We end this section by recording this result.
Theorem 7.3.9 Let X be a non-empty subset of a vector space V. Then there
is a smallest subspace of V that contains X, and this is the set of all finite
linear combinations of vectors chosen from X.
As an example, the polynomials 1, x2, x4, . . . generate the subspace of all
even polynomials of the vector space of all polynomials.
Exercise 7.3
1.  Let L be a line and Π be a plane in 
, both containing the origin. Use
Theorem 7.3.5 to show that L ⊂ Π or L + Π = 
.
2.  Show that the vectors (1, 0, 2, 0, −1), (0, 1, 3, 0, −2), (0, 0, 0, 1, 1) form a
basis of U ∩ W in Example 7.3.7, so that dim(U ∩ W) = 3.
3.  Let U be the subspace of 
 spanned by the vectors (1, −1, 1, 2), (3, 1, 2,
1) and (7, 9, 3, −6). What is dim(U)? Find a subspace W of 
4 such that
U ∩ W = {0} and dim(U) + dim(W) = dim(
).
4.  Let V = , and consider V as a vector space over . Show that if U is a
subspace of V then either U = {0} or U = V .
5.  Let V be a vector space over  of dimension n, and let U be a subspace
of dimension m, where m < n. Show that if m = n − 1 then there are only
two subspaces of V that contain U (namely U and V), whereas if m <
n − 1 then there are infinitely many distinct subspaces of V that contain U
.
7.4 The direct sum of two subspaces
If W is a plane through the origin in 
, and if L is a line through the origin
but not lying in W, then every vector in 
 can be expressed uniquely in the
form u + w, where u ∈ L and w ∈ W. This idea extends easily to the general
situation.
Definition 7.4.1 Suppose that U and W are subspaces of a vector space V.
Then we say that the subspace U + W is the direct sum of U and W if every
vector in U + W can be expressed uniquely in the form u + w, where u ∈ U
and w ∈ W. If U + W is the direct sum, we write U ⊕ W instead of U + W.

More generally, if U1, . . . , Uk are subspaces of V, then the sum U1 + · ·
· + Uk is a direct sum, and we write U1 ⊕ · · · ⊕ Uk, if every vector in U1 · ·
· + Uk can be expressed uniquely in the form u1 + · · · + uk, where uj ∈ Uj.
The following result shows how we can check whether or not U + W is the
direct sum U ⊕ W of U and W.
Theorem 7.4.2 Suppose that U and W are subspaces of a vector space V.
Then the following are equivalent:
(1)  U + W is the direct sum U ⊕ W of U and W;
(2)  U ∩ W = {0};
(3)  dim(U) + dim(W) = dim(U + W).
Proof Suppose that (1) holds, and take any v in U ∩ W. Then we can write
v = 0V + v = v + 0V, where in each of these sums the first vector is in U and
the second vector is in W. By the uniqueness asserted in Definition 7.4.1, we
must have v = 0V so that (2) follows. Next, Theorem 7.3.5 shows that (2) and
(3) are equivalent. Now assume that (2) holds and, for any vector v in
U + W, let us suppose that v = u1 + w1 = u2 + w2, where each ui is in U and
each w j is in W. Then u1 − u2 = w2 − w1 and so, as (2) holds,
u1 − u2 = 0 = w2 − w1. We deduce that v in U + W has a unique expression in
the form u + w so that (1) holds.
Theorem 7.4.2 has the following corollary.
Corollary 7.4.3 Suppose that U and W are subspaces of V, and that U ∩
V = {0}, and dim(U) + dim(W) = dim(V). Then V = U ⊕ W .
Proof Theorem 7.4.2 shows that U + W is a direct sum, and that
dim(U + W) = dim(V). Theorem 7.3.2 implies that V = U ⊕ W .
We give one example; other examples occur in the exercises.
Example 7.4.4 Let U be the subspace of 
 spanned by the vectors u = (1, 1,
1, 1) and u′ = (1, 1, −1, −1). How can we find a subspace W of 
 such that

U ⊕ W = 
? It seems reasonable to guess that we can find two vectors that
are 'orthogonal' to U and that such vectors might generate a choice of W
(note that for a given U, the subspace W is not unique). Now (by analogy
with 
) a vector (x1, x2, x3, x4) is orthogonal to U if the x j satisfy the two
equations
so we can take, say, w = (1, −1, 0, 0) and w′ = (0, 0, 1, −1). Certainly w and
w′ generate a two-dimensional subspace W of 
 so it is only matter of
showing that U ∩ W = {0}. To do this we take any vector z, say, in this
intersection and write z = λu + μu′ = ρw + σ w′. This gives
so that λ = μ = ρ = σ = 0. It follows that U ⊕ W = 
4.
We have seen that if a plane Π and a line L in 
 meet only at the origin,
then 
 is the direct sum L ⊕ W. This means that given W we can construct
a line L such that 
 = L ⊕ W and, incidentally, it is clear that for a given W,
the line L is not unique. Likewise, given L we can construct W such that 
 = L ⊕ W. The same is true for any proper subspace of any vector space.
Theorem 7.4.5 Let U be proper subspace of a vector space V. Then there is
a subspace W such that U ⊕ W = V .
Proof Take any basis u1, . . . , uk of U and extend it to a basis, say u1, . . . , uk,
w1, . . . , wℓ, of V. Now let W be the subspace spanned by w1, . . . , wℓ. By
definition, V = U + W, and by Theorem 7.4.2, U + W = U ⊕ W .
Exercise 7.4
1.  Find a subspace W0 of 
 such that in Example 7.4.3, we have U ⊕ W0 = 
 yet W = W0. Show that there are infinitely many distinct choices of a
subspace W1 such that U ⊕ W1 = 
.
2.  Let V be the vector space of all complex polynomials, and let Vo and Ve
be the subspaces of even polynomials and odd polynomials, respectively.

Show that V = Ve ⊕ Vo.
3.  Let V be the vector space of real solutions of the differential equation
¨x + x = 0. Let S be the subspace of solutions x(t) such that x(0) = 0, and
let C be the subspace of solutions x(t) such that 
. Show that V = S
⊕ C.
4.  Suppose that U, V and W are subspaces of some given vector space. With
the obvious definition of U + V + W, show that every element of
U + V + W can be expressed uniquely in the form u + v + w, where u ∈
U, v ∈ V and w ∈ W, if and only if
Note that this is not equivalent to U ∩ V = V ∩ W = W ∩ U = {0}, as can be
seen by taking U, V and W to be three distinct but coplanar lines through the
origin in 
. In this case, dim(U + V + W) = 2.
7.5 Linear difference equations
A second order linear difference equation, or recurrence relation, is a
relation of the form
(7.5.1)
where the an and bn are given real or complex numbers. A sequence y0, y1,
. . . is a a solution of (7.5.1) if, for all n, yn+2 + an yn+1 + bn yn = 0. The
reader may have met difference equations before in which the an and bn are
constants (for example, xn+2 + xn+1 − 2xn = 0) but here we allow these
coefficients to depend on n. Likewise, the reader may have only met real
difference equations before, but there is no reason not to consider complex
difference equations (in fact, because of the Fundamental Theorem of
Algebra, there is every reason to do so). The key observation is that the set
of solutions of (7.5.1) is a subspace of dimension two of the vector space of
real, or complex, sequences.
Theorem 7.5.1 The set S of solutions x0, x1, . . . of (7.5.1) is a vector space
of dimension two.

Proof First, notice that the sequence 0, 0, . . . is a solution of (7.5.1).
Suppose that yn and zn are solutions, and that λ is a scalar. Then it is obvious
that both λyn and yn + zn are solutions. This shows that a linear combination
of solutions is again a solution, and we leave the reader to verify that the rest
of the requirements for a vector space are satisfied.
We must now show that dim(S) = 2. Let u0 = 1 and u1 = 0, and then
determine u2, u3, . . . inductively from (7.5.1) so that 1, 0, u2, u3, . . . is a
solution. Similarly, we can find v2, v3, . . . so that 0, 1, v2, v3, . . . is a
solution. It is evident that neither of these solutions is a scalar multiple of the
other, so they are linearly independent. However, if y0, y1, . . . is any
solution, then
because any solution of (7.5.1) is plainly completely determined by its first
two elements. This shows that the solutions un and vn form a basis of the
space of solutions, so this has dimension two.
The next result give a simple test to determine whether or not two
solutions form a basis of the solution space.
Theorem 7.5.2 A pair un and vn of solutions of (7.5.1) forms a basis of S if
and only if u0v1 − u1v0 ≠ 0.
Proof Because dim(S) = 2, un and vn fail to form a basis of S if and only if
they are linearly dependent; that is, there are scalars λ and μ, not both zero,
such that λun = μvn for all n. Clearly this implies that λun = μvn for n = 0, 1.
Conversely, if λun = μvn for n = 0, 1 then, by induction, λun = μvn for all n;
for example,
Finally, it is clear that there exist λ and μ, not both zero, with λun = μvn,
n = 1, 2, if and only if u0v1 − u1v0 = 0.

Let us now consider the constant coefficient case; that is, when (7.5.1)
becomes
(7.5.2)
where a and b are constants. In this case (1, t, t2, . . .), where t ≠ 0, is a
solution if and only if t satisfies the auxiliary equation t2 + at + b = 0. For
the moment we will work in the space of complex solutions; then the
auxiliary equation has two roots, say r and s. If r ≠ s then we have two
solutions (1, r, r2, . . .) and (1, s, s2, . . .) of (Theorem 7.5.2 shows that these
form a basis of the space of all solutions. Thus if the auxiliary equation has
distinct roots r and s, then the most general solution of (7.5.1) is of the form
λrn + μsn.
Now suppose that the auxilary equation has a repeated root r; then, as
before, (1, r, r2, . . .) is a solution. We need another solution that is not a
scalar multiple of this one, and we can obtain this in the following way.
First, as r is a repeated root of the auxiliary equation, this equation must be
of the form t2 − 2rt + r2 = 0. Thus the difference equation must be
xn+1 − 2rxn+1 + r2xn = 0. We write xn = rn yn, and find that xn is a solution if
and only if yn+2 − 2yn+1 + yn = 0 or, equivalently, yn+2 − yn+1 = yn+1 − yn.
Clearly this is satisfied by yn = n; thus we now have a second solution
namely (0, r, 2r2, 3r3, . . .). Theorem 7.5.2 shows that these two solutions are
linearly independent so we have shown that if the auxiliary equation has a
non-zero repeated root r, then the most general solution of (7.5.1) is of the
form rn(λ + μn).
Finally, suppose that we are seeking real solutions of a real difference
equation. If the discussion above yields real solutions, then we are done. If
not, then there is a complex, non-real, root w of the auxiliary equation, and
as the coefficients are real, the other root is 
, where 
. This means
that the two sequences 
 are complex solutions, and hence so too are 
 and 
. We have now obtained two real solutions Rn
cos(nθ) and Rn sin(nθ), where w = Reiθ, and these form a basis for the real
solutions of (7.5.2).
Exercise 7.5

1.  The Fibonacci sequence F0, F1, . . . is given by F0 = 0, F1 = 1 and
Fn+2 = Fn+1 + Fn. Show that
Show directly from this formula that Fn is a positive integer.
2.  The Pell sequence Pn is defined by P1 = 1, P2 = 2, and Pn+2 = 2Pn+1 + Pn.
Show that
3.  Find the solution un of xn+2 − 3xn+1 + 2xn = 0 with u0 = u1 = 1. Find the
solution vn with v0 = 1 and v1 = 2.
4.  Find the general solution of xn+2 − 2xn+1 + 2xn = 0.
5.  By assuming that xn = (−1)n(λn + μ) find a solution of the difference
equation xn+2 + (n + 2)xn+1 + (n + 2)xn = 0.
7.6 The vector space of polynomials
The set of complex polynomials is a vector space over , and clearly this is
an infinite dimensional vector space.
Theorem 7.6.1 The space P( , d) of complex polynomials of degree at most
d is a complex vector space of dimension d + 1.
Proof Every polynomial of degree at most d is a linear combination of the
polynomials
(7.6.1)
thus these polynomials span P( , d). In order to show that these polynomials
are linearly independent, we suppose that there are complex numbers λj such
that λ0 p0 + · · · + λd pd is the zero polynomial (that is, it is zero at every z).
Now the Fundamental Theorem of Algebra guarantees that if one of λ1, . . . ,

λd is non-zero, then 
 has only finitely many zeros. As every z is a
zero of 
, we see that λ1 = · · · = λd = 0, and hence that λ0 = 0 also.
Thus p0, p1, . . . , pd are linearly independent and so form a basis of P( , d).
There is nothing canonical about this choice of basis.
Theorem 7.6.2 Suppose that q0, q1, . . . , qd are polynomials, where qm has
degree m and q0 is not the zero polynomial. Then q0, q1, . . . , qd is a basis of
P( , d).
Proof Take a polynomial f of degree at most d. Choose a constant λd such
that f − λdqd has degree at most d − 1; then a constant λd−1 such that (
f − λdqd) − λd−1qd−1 has degree at most d − 2, and so on. This shows that f is
a linear combination of the qj, so the qj span P( , d). Corollary 7.2.10 now
shows that the qj form a basis of P( , d).
A basis of a vector space is, in essence, a coordinate system in the space,
and there are always infinitely many bases to choose from. Bases are a tool
for solving problems, and the art lies in choosing a basis that is best suited to
the problem under consideration. We chose the basis in (7.6.1) because this
was best suited to an application of the Fundamental Theorem of Algebra;
we shall now consider situations in which other bases are more appropriate.
Example 7.6.3: Lagrange's interpolation formula We know that P( , d)
has dimension d + 1. Choose d + 1 distinct points z0, . . . zd in  and, for
j = 0, 1, . . . , d, let
Here, the Π-sign denotes a product, just as the Σ-sign denotes a sum; for
example, if d = 2 then

Each qj is in P( , d), and the qj have been chosen to have the property
Now consider any polynomial p in P( , d); then
(7.6.2)
for each side takes the same value at the d + 1 distinct points zj, and so, by
the Fundamental Theorem of Algebra, their difference is the zero
polynomial. This argument shows that q0, . . . , qd span P( , d), and it
follows from Corollary 7.2.10 that they form a basis of P( , d). As an
example of this formula, we observe that the (unique) quadratic polynomial
p that satisfies p(0) = a, p(1) = b and p(2) = c is
The expression for p given in (7.6.2) is known as Lagrange's interpolation
formula.
Example 7.6.4: Legendre polynomials The vectors i, j and k in 
 are
mutually orthogonal unit vectors that form a basis of 
, and they have the
property that the coefficients xj of a vector x can be obtained by taking scalar
products, namely x1 = x·i, and so on. These important properties generalize
to many other systems, and we shall illustrate one of these generalizations
here. Given two real polynomials p and q, we define
We call this the scalar product of p and q, and it shares many of the
important properties of the scalar product of vectors. For example, it is linear
in p, and in q, and ( p, p) 0 unless p is the zero polynomial. By analogy with
vectors we define the 'length' || p|| of p by

and we say that p and q are orthogonal, and write p ⊥ q, if ( p, q) = 0.
The question we now address is whether or not we can find a basis of
polynomials in the space of real polynomials of degree at most d which have
unit length and which are orthogonal to each other? Suppose for the moment
that we can, and let these be p0, . . . pd. Then for any polynomial q of degree
at most d there exist real numbers aj such that 
. If we
now multiply both sides of this identity by pk(x) and integrate over the
interval [−1, 1], we find that
because (pj, pk) = 0 when j = k. This mimics the situation in 
.
The existence of such a basis pk can be proved by an elementary argument
in calculus, and the resulting polynomials are scalar multiples of the
Legendre polynomials pn which are defined by
The first three of these polynomials are
(7.6.3)
as the reader can easily verify. In fact, the pn are mutually orthogonal
polynomials with 
Example 7.6.5: common zeros of two polynomials Consider two complex
polynomials f and g of degrees n and m, respectively, and form the
polynomials
(7.6.4)

As these m + n polynomials lie in P( , m + n − 1) it is natural to ask when
they form a basis of this space. They form a basis if and only if they span P(
, m + n − 1), and the next result gives a perhaps surprising condition for
this to be so.
Theorem 7.6.6 The polynomials in (7.6.4) form a basis of the space P( ,
m + n − 1) if and only if f and g have no common zero.
Proof Let V be the subspace spanned by f (z), z f (z), . . . zm−1 f (z), and let W
be the subspace spanned by g(z), zg(z), . . . zn−1g(z). We claim that
dim(V) = m and dim(W) = n. As the polynomials zr f (z), r = 0, . . . , m − 1,
span V we only need show that they are linearly independent. Now any
linear combination of them is of the form q(z) f (z), where q is a polynomial
of degree at most m − 1, and if this is to be the zero polynomial then, as f is
non-constant, q must be the zero polynomial. It follows that the given
polynomials are linearly independent and hence that dim(V) = n. Similarly,
dim(W) = n. By Theorem 7.3.5,
so that the polynomials in (7.6.4) form a basis of P( , m + n − 1) if and only
if V ∩ W = {0}.
It remains to show that f and g have no common zero if and only if V ∩
W = {0} or, equivalently, that f and g have a common zero if and only if V ∩
W ≠ {0}. Suppose first that f and g have a common zero, say z1. We write f
(z) = (z − z1) f1(z) and g(z) = (z − z1)g1(z), and then f1(z)g(z) = f
(z)g1(z) = h(z), say. Clearly, h ∈ V ∩ W so V ∩ W ≠ {0}. Conversely,
suppose that V ∩ W ≠ {0}. Then there are polynomials f2(z) and g2(z) with
deg( f2) < n, deg(g2) < m and g2(z) f (z) = f2(z)g(z). Now f has n zeros and f2
has at most n − 1 zeros; thus f and g have a common zero.
For more details on this topic, see Section 9.6.
Exercise 7.6
1.  Use Lagrange's interpolation formula to find the most general cubic
polynomial that vanishes at 0, 1 and −1, and verify this directly.

2.  Verify that the polynomials in (7.6.3) are the first three Legendre
polynomials, and show that for these polynomials, pi ⊥ pj if i ≠ j. Show
that 
.
3.  Choose distinct points z1, . . . , zk in . Show that the space Vd of
polynomials of degree at most d that are zero at z1, . . . , zk is a vector
space. What is the dimension of Vd? [You should consider the cases k ≤ d,
k = d and k > d separately.]
4.  Find a basis, and the dimension, of the space of polynomials spanned by
the polynomials in (7.6.4) when f (z) = z2(z − 1) and g(z) = z2(z + 1).
7.7 Linear transformations
The crucial property enjoyed by vector spaces is that a linear combination of
vectors is again a vector. The defining property of a linear map is that it
preserves linear combinations in the natural way (that is, the image of a
linear combination is the same linear combination of the images).
Definition 7.7.1 A map α : V → W between vector spaces V and W (over the
same field  of scalars) is linear if, for all scalars λ1, . . . , λn, and all vectors
v1, . . . , vn,
If α is linear we say that it is a linear transformation, or a linear map, and
this will be so if, for all scalars λ, and all vectors u and v, α(λx) = λα(x) and
α(x + y) = α(x) + α(y).
Each linear map α : V → W maps 0V to 0W (the zero vectors in V and W), for
if 0  is the scalar zero, then 
. Similarly,
α(−v) = −α(v), so that α maps the inverse of v to the inverse of α(v).
The scalar and vector products in 
 give examples of linear maps. For
each a in 
, σa(x) = x·a is a linear map and (Theorem 4.2.5) every linear
map of 
 to  is of this form. Likewise, βa(x) = a × x is a linear map but, as
βa(a) = 0, no linear injective map can be of this form. Differentiation and
integration provide examples of linear maps in analysis. Assuming the
existence of derivatives, we have

and these are the properties required of a linear map. Without going into
details, the same is true of the higher derivatives f 
 dk f /dxk, and of all
linear combinations of these derivatives. Thus we have the concept of a
linear operator
and hence of a linear differential equation. The familiar formulae
show that the definite integral is a linear map between two vector spaces of
functions.
The next shows that a linear map α : V → W can be defined by, and is
uniquely determined by, its action on a basis of V .
Theorem 7.7.2 Suppose that V and W are finite-dimensional vector spaces
over 
, and let v1, . . . , vn be a basis of V . Choose any vectors w1, . . . , wn
in W. Then there exists a unique linear map α : V → W such that α(vj) = wj
for each j.
Proof Each x in V can be written as 
, where the xj are uniquely
determined by x; thus we can define a map α : V → W by 
.
Clearly, for each j, α(vj) = wj. Further, α is linear for
Now suppose that β is a linear map with β(vj) = wj. Then for any 
, so that α = β.

Finally, we remark that a function f : V → W between vector spaces V and
W is additive if 
 for all vectors vj. The map α(z) = 
from  to itself is additive, but as
(unless z = 0), α is not linear. This shows that the condition f (λv) = λ f (v) is
an essential part of the definition of linear maps. It is a remarkable fact that
there exist additive functions f : 
 → 
 that are not continuous anywhere
and, as any linear map from 
 to itself is continuous, these maps are also
additive but not linear. A linear map g :  → 
 is continuous, for if xn → x,
then g(xn) = xng(1) → xg(1) = g(x). Finally, note that for any additive map
we have, with v = mu,
so that f (λv) = λ f (v) for every rational λ. Thus every additive map is linear
with respect to the field 
 of rational numbers, though not necessarily linear
with respect to .
Exercise 7.7
1.  Show that the most general linear map of 
 to  is of the form x 
 x·a,
where x·a = x1a1 + · · · + xnan.
2.  Suppose that α :
 → 
 is a linear map, and for each x in
, let
α(x) = α1(x), . . . , αn(x). Show that each αj: 
 →  is a linear map. Use
Exercise 1 to show that the most general linear map of 
 to itself is
given by (x, y) 
 (ax + by, cx + dy) for some real numbers a, b, c and d.
3.  Let Π be the plane in 
 that contains 0, j and k. Show that the
orthogonal projection α of 
 onto Π is a linear map.
4.  Let S be the vector space of real sequences and, for x = (x1, x2, . . .) define
α(x) = (0, x1, x2, . . .) and β(x) = (x2, x3, . . .). Show that α and β are linear
maps. Show also that βα = I but that αβ ≠ I, where I is the identity map on
S. This shows that in general, βα = I does it not imply that β = α−1. Show
that α is not injective, and β is not surjective.
5.  Let U be a subspace of V, and suppose that α : U → W is linear. Show
that α extends to a linear map of V to W ; that is, there exists a linear map 

 such that 
 on U .
6.  Let V and W be vector spaces over .
(a)  Suppose that α : V → W is an injective linear map. Show that if v1,
. . . , vn are linearly independent vectors in V, then α(v1), . . . , α(vn)
are linearly independent vectors in W. Deduce that if such an α exists,
then dim(V) ≤ dim(W).
(b)  Suppose that β : V → W is a surjective linear map. Show that if v1,
. . . , vn span V then β(v1), . . . , β(vn) span W. Deduce that if such a β
exists, then dim(V) ≥ dim(W).
(c)  Suppose that γ : V → W is a bijective linear map. Show that if v1, . . .
, vn is a basis of V then γ (v1), . . . , γ (vn) is a basis of W. Deduce that
if such a γ exists, then dim(V) = dim(W).
7.8 The kernel of a linear transformation
A linear map α : V → W automatically provides us with a particular
subspace of V, and a particular subspace of W .
Theorem 7.8.1 Suppose that α : V → W is a linear map. Then
(a)  {v ∈ V : α(v) = 0W } is a subspace of V, and
(b)  {α(v) : v ∈ V } is a subspace of W .
Proof Let K = {v ∈ V : α(v) = 0W}, and U = {α(v) : v ∈ V }. By Theorem
7.3.1, a subset X of a vector space is a subspace if 0 ∈ X and if X contains
any linear combination of vectors in X. As α(0V) = 0W, we see that 0V ∈ K
and 0W ∈ U. If v1, . . . , vr are in K, then so is 
 because 
. Similarly, if w1, . . . , wr are in U with, say,
α(vj) 
= 
wj, 
then 
so 
is 
 
because 
.
There are standard names for these subspaces, and their dimensions are
related.
Definition 7.8.2 Let α : V → W be a linear map. The kernel ker(α) of α is the
subspace {v ∈ V : α(v) = 0W} of V, and its dimension is the nullity of α. The

range α(V) of α is the subspace {α(v) : v ∈ V} of W, and its dimension is the
rank of α.
Theorem 7.8.3 Let α : V → W be a linear map. Then
In particular, dim α(V) ≤ dim(V).
The proof of Theorem 7.8.3 Choose a basis v1, . . . , vk of ker(α) and use
Theorem 7.2.9 to extend this to a basis v1, . . . , vk, vk+1, . . . , vn of V. We
want to show that dim α(V) = n − k and so it suffices to show that the vectors
α(vk+1), . . . , α(vn) are a basis of α(V). Now any vector in α(V) is of the form
α(λ1v1 + · · · + λnvn), and as
we see that α(vk+1), . . . , α(vn) span α(V). To show that these vectors are
linearly independent, suppose that for some scalars μk+1, . . . , μn,
Then α μk+1vk+1 + · · · + μnvn = 0, so that μk+1vk+1 + · · · + μnvn is in ker(α),
and hence there are scalars μ1, . . . , μk such that
Because v1, . . . , vn is a basis of V, we see that μ1 = · · · = μn = 0 and this
shows that α(vk+1), . . . , α(vn) are linearly independent.
The idea of the kernel of a linear map leads to an important technique
which arises frequently throughout mathematics, and which is an essential
feature of linear maps. Let α : V → W be a linear map and suppose that, for
a given w in W, we have two solutions v1 and v2 of the equation α(v) = w.
Then α(v1 − v2) = α(v1) − α(v2) = w − w = 0 and so v1 − v2 ∈ ker(α). It
follows that if v0 is one solution of α(v) = w, then the general solution is of

the form v0 + v, where v ∈ ker(α). Using the natural notation
v0 + ker(α) = {v0 + v : v ∈ ker(α)}, we have proved the following result.
Theorem 7.8.4 Suppose that α : V → W is linear, that w ∈ W, and that
α(v0) = w. Then α(v) = w if and only if v ∈ v0 + ker(α).
This shows that the general solution of α(v) = w is any one solution plus the
general solution of α(v) = 0. This general principle is usually met first in the
context of linear differential equations (where the general solution is
described as the sum of a 'particular integral' and the 'complementary
function'), and an example will suffice to illustrate this.
Example 7.8.5 Let us discuss the solutions of the equation
Now α is a linear map (from the space of twice differentiable real functions
on  to the space of real functions on 
), and the general solution is yp + yc,
where yp is any solution of α(y) = 6, and yc is the general solution of
α(y) = 0. In this example we can take yp = 3 (that is, the constant function
with value 3), and yc = λex + μe2x for any constants λ and μ. In the context of
Theorem 7.8.3, let V to be the three-dimensional vector space of functions
spanned by the functions 1, ex, e2x (that is, the class of functions
A + Bex + Ce2x, for any real A, B and C). Then α is a linear map from V to
itself, and ker(α) is the two-dimensional subspace spanned by the functions
ex and e2x. The space α(V) is the one-dimensional subspace of constant
functions in V.
Example 7.8.6 We illustrate Theorems 7.8.1 and 7.8.3 by reviewing the
scalar product and the vector product in 
. Let σa(x) = x·a, where a is a
fixed non-zero vector. Then ker(σa) has dimension 2 (it is the plane Π given
by x·a = 0), while σa(
) = 
, which has dimension one. The general
solution of x·a = d is the sum of a particular solution, for example

x0 = (d/||a||2)a, and the kernel Π. Thus the solutions of x·a = d are the
'translated plane' x0 + Π.
Now let βa(x) = a × x, where a = 0. As βa(x) = 0 if and only if x = λa, say,
we see that ker(β) has dimension one, and hence that β(
) has dimension
two. Further, as βa(x) ⊥ a, we see that βa(
) = W, where W is the plane
given by x·a = 0. This shows that if b ⊥ a, then there is some solution of x ×
a = b (and the set of all solutions is a line); otherwise, there is no solution.
These results were obtained by direct means in Theorem 7.8.3, the general
solution is this solution plus the general element of ker(βa), namely a scalar
multiple of a.
Exercise 7.8
1.  Suppose that α : V → W is linear. Show that for any subspace U of V,
α(U) is a subspace of W, and dim α(U) ≤ dim (U).
2.  Let a = (1, 1, 1). The vector product x 
 a × x is the map that takes (x1,
x2, x3) to (−x2 + x3, x1 − x3, −x1 + x2). By making explicit calculations,
find ker(α), and a basis for α(
).
3.  Suppose that α : V → W is a linear map and that W0 is a subspace of W.
Let U = {v ∈ V : α(v) ∈ W0}. Show that U is a subspace of V. [This
generalizes Theorem 7.8.1(a).]
4.  Let α : 
 → 
 be defined by
Find a basis for ker(α), and a basis for α(
), and verify Theorem 7.8.3 in
this case.
5.  Construct a linear map α : 
 → 
 whose kernel is spanned by (1, 0, 0,
1) and (0, 1, 1, 0).
6.  Given a vector space V and a subspace U of V, show that there is a linear
map α : V → V such that U = ker(α).
7.  Let V be of dimension n, and suppose that α: V → V is linear. Show that
if dim αm(V) = dim αm+1(V), then

and hence that αm(V) = αm+1(V) = αm+2(V) = · · ·. Deduce that there exists
an integer k with k ≤ n = dim(V), such that
and αk(V) = αk+1(V) = αk+1(V) = · · ·. Deduce that if dim αk(V) = 0 for
some k, then dim αn(V) = 0.
Suppose now that dim αn−1(V) = 1 and dim αn(V) = 0. Show that
Illustrate these results in the specific case when V is the space of
polynomials of degree less than n, and α is differentiation.
7.9 Isomorphisms
The composition of two linear maps is linear, and the inverse (when it
exists) of a linear map is linear. These are basic and important facts.
Theorem 7.9.1 Let U, V and W be vector spaces. If α : U → V and β : V →
W are linear, then so is βα : U → W .
Theorem 7.9.2 Suppose that α : V → W is a linear bijection of a vector
space V onto a vector space W. Then α−1 : W → V is linear.
The proof of Theorem 7.9.1 This is clear for

The proof of Theorem 7.9.2 As α is a bijection, α−1: W → V exists as a map,
but we do need to show that it is linear. Take any two vectors wj in W, and
let vj = α−1(wj). Then, for all scalars μj,
so that α−1 is linear.
Roughly speaking, two vector spaces are said to be isomorphic if they are
the same in every sense except for the notation that describes them.
Formally, this is described as follows.
Definition 7.9.3 Two vector spaces V and W are said to be isomorphic if
there exists a bijective linear map α of V onto W. Any such α is an
isomorphism of V onto W .
Theorems 7.9.1 and 7.9.2 show that the composition of isomorphisms is
again an isomorphism, and that the inverse of an isomorphism, is also an
isomorphism. It is important to understand that all intrinsic structural
properties of vector spaces transfer without change under an isomorphism;
that is, if a general result (involving the concepts we have been discussing)
about vector spaces is true for V, then it (or an appropriate re-statement of it)
will also be true for any vector space W isomorphic to V. For example,
suppose that α : V → W is an isomorphism and take a basis v1, . . . , vk of V.
As every vector in W is the image of a linear combination of the vj, and as α
preserves linear combinations, it follows that every vector in W is some
linear combination of the α(vj); thus α(v1), . . . , α(vk) span W. However, if
λ1α(v1) + · · · + λkα(vk) = 0, then α(λ1v1 + · · · + λkvk) = 0 and hence, as α is
injective, λ1v1 + · · · + λkvk = 0. But as the vj are linearly independent, this
gives λ1 = · · · = λk = 0 and so we see that α(v1), . . . , α(vk) is a basis of W. It
follows that under an isomorphism, a basis of V maps to a basis of W and so
dim(V) = dim(W). The converse of this is also true.

Theorem 7.9.4 Two finite dimensional vector spaces over the same field are
isomorphic if and only if they have the same dimension.
Proof Suppose that U and V are vector spaces over 
 of dimension n.
Choose a basis u1, . . . , un of U and a basis v1, . . . , vn of V, and use
Theorem 7.7.2 to define a linear map α : U → V by
It is obvious that α is injective, and surjective, so it is an isomorphism of U
onto V .
Theorem 7.9.4 shows that any real vector space of dimension n is
isomorphic to 
; thus, in effect, there is only one real vector space of
dimension n. In particular (and this should be obvious), the space 
 of
column vectors is isomorphic to 
. We end with a result which identifies
isomorphisms (see Exercise 7.7.4).
Theorem 7.9.5 Let α : U → V be a linear map between vector spaces U and
V of the same finite dimension. Then the following are equivalent:
(1)  α is injective;
(2)  α is surjective;
(3)  α is bijective;
(4)  ker(α) = {0}.
(5)  α−1 : V → U exists.
(6)  α is an isomorphism.
Proof As α(u) = α(v) if and only if u − v ∈ ker(α), we see that (1) is
equivalent to (4). Next, as dim(U) = dim(V), Theorem 7.8.3 shows that
α(U) = V if and only if ker(α) = {0}. Thus (2) is equivalent to (4), and hence
also to (1). If (1) holds, then so does (2) and these imply (3). Conversely, (3)
implies both (1) and (2). Finally, (3) is obviously equivalent to (5), and, by
definition, (6) is equivalent to (3).
Exercise 7.9

1.  Let α : V → W be a linear map. Show that α is injective if and only if dim
α(V) = dimV, and that α is surjective if and only if dim α(V) = dim(W).
2.  Let S be the vector space of solutions of the differential equation 
. Show that dim(S) = 2, and that α : S → S defined by 
 is an isomorphism of S onto itself.
3.  Construct an isomorphism from 
 over  onto 
 over .
4.  Let v1, . . . , vn be a basis of a vector space V, and let ρ be a permutation
of {1, . . . , n}. Let α : V → V be the unique linear map that satisfies 
. Show that α is an isomorphism of V onto itself.
7.10 The space of linear maps
For given vector spaces V and W over the same field , let (V, W) be the
set of all linear maps α : V → W. If α and β are in (V, W), and if λ is a
scalar, there are natural definitions of the linear maps α + β and λα, namely
and from these it is easy to see that (V, W) is a vector space over . We
leave the reader to provide the details (the zero vector in (V, W) is the 'zero
map' which takes every v in V to 0W).
Theorem 7.10.1 The space (V, W) of linear maps from a vector space V of
dimension n to a vector space W of dimension m is a vector space of
dimension mn.
Proof Choose a basis v1, . . . , vn of V, and a basis w1, . . . , wm of W. Then,
by Theorem 7.7.2, we can define nm linear maps 
i j : V → W by requiring
that 
ij maps vi to wj and all other vk to 0 (in W); for example, 
23 acts as
follows:
These maps are linearly independent, for if 
 (the zero map),
then

so that λ11 = · · · = λ1m = 0 (because the wi are linearly independent). By
applying the same function to v2, . . . , vn we see that λij = 0 for all i and j. To
see that the 
ij span (V, W) we recall from Theorem 7.7.2 that a general
linear map α : V → W is uniquely determined by its action on the basis v1,
. . . , vn. For each j, let 
. Then 
 because, for
each j,
We end with an application of Theorem 7.10.1 to linear maps of V into
itself. Suppose that α : V → V is linear, where V has dimension n. We can
apply α repeatedly, and we denote the k-th iterate of α (α applied k times) by
αk. As the linear maps I, α, α2, . . . , αn2 are all in (V, V) it follows from
Theorem 7.10.1 that they must be linearly dependent. This shows that there
are constants a0, . . . , an2, not all zero, such that
(the zero map). It we now define the polynomial p by
then p(α)(v) = 0 for every v in V. Thus we have proved the following result.
Theorem 7.10.2 For any linear map α : V → V there is a polynomial p of
degree at most n2 such that p(α)(v) = 0 for all v in V .
Later we shall see that we can take p to be of degree at most n.
Exercise 7.10
1.  The space (V, ) is the space of all real-valued linear maps on V. Let v1,
. . . , vn be a basis of V, and define the maps j by i (vj) = 1 if i = j and 0
otherwise. Work through the proof of Theorem 7.10.1 in this case to

show that 1, . . . , n is a basis of of (V, ). What is j when V = 
and the basis is i, j, k?
2.  Let Π be a plane through the origin in 
, and let β be the orthogonal
projection of 
 onto Π. Find a polynomial p such that p(β) = 0.
3.  Let α(x) = x × a (the vector product in 
). Find a polynomial p such that
p(α) = 0.

8
Linear equations
8.1 Hyperplanes
A plane in 
 that passes through the origin can be characterized as the
solutions of a single linear homogeneous equation in three variables, say
a1x1 + a2x2 + a3x3 = 0, as a subspace whose dimension is one less than that
of the underlying space 
, and as the set of vectors that are orthogonal to a
given vector (the normal to the plane). We now generalize these ideas to 
. We recall that e1, . . ., en is the standard basis of 
, and if x = ∑ xj ej and
y = ∑ yj ej, then the scalar product x · y is given by
We say that x and y are orthogonal, and write x ⊥ y, when x·y = 0.
Definition 8.1.1 Let V be a vector space of dimension n. A line in V is a
subspace of dimension one. A hyperplane in V is a subspace of dimension
n − 1.
Theorem 8.1.2 For any subset W of 
 the following are equivalent:
(1)  W is a hyperplane in 
n;
(2)  There is a non-zero a in 
 such that W = {x ∈ 
 : x·a = 0};
(3)  There are scalars a1, . . ., an, not all zero, such that

Proof Obviously, (2) is equivalent to (3). Now let W be as in (2). Then
α(x) = x·a is a surjective linear map of 
 onto 
 with kernel W, and (1)
follows as
Now suppose that W is a hyperplane in V. Take a basis w1, · · · , wn−1 of
W and extend this to a basis w1, · · · , wn−1, wn of 
. Thus, for each j, there
are scalars aij such that
(8.1.1)
so that, for any x in 
,
Now x ∈ W if and only if x is a linear combination of w1, . . ., wn−1, and this
is so if and only if x1a1n + · · · + xnann = 0. As not all of the numbers a1n,
. . ., a nn can be zero (for, if they were then, from (8.1.1), w1, . . ., wn−1
would span 
), we see that (1) implies (3).
Exercise 8.1
1.  Find a basis for the hyperplane 2x1 + 5x2 = 0 in 
.
2.  Find a basis for the hyperplane 2x1 − x2 + 5x3 = 0 in 
.
3.  Find a basis for the hyperplane x1 − x2 + 3x3 − 2x4 = 0 in 
.
4.  Show that e1 − en, e2 − en, . . ., en−1 − en is a basis of the hyperplane
x1 + · · · xn = 0 in 
.
8.2 Homogeneous linear equations
We shall now discuss the space of solutions of a set of m simultaneous
homogeneous linear equations in the n real variables x1, . . ., xn, say

(8.2.1)
It is sometimes convenient to write these in matrix form
(8.2.2)
where the array (aij) is known as a matrix. For the moment, this is just a
convenient notation; later we shall see that the left-hand side here is the
product of two matrices.
As the solution set of each single equation is a subspace of 
, the set of
solutions of the system (8.2.1) is the intersection of subspaces, and so is
itself a subspace of 
. To complete our discussion we need to find the
dimension of this subspace, and show how to find it (that is, how to solve
the given equations). As each of the m equations places a single constraint
on the n numbers x1, . . ., xn, we might expect the space of solutions to have
dimension n − m; however, a little more care is needed as the equations may
not be independent of each other (indeed, there may even be identical
equations in the list).
Any solution (x1, . . ., xn) is a point in 
 and in order to exploit the
geometry of 
 we use (8.2.2) to define the row vectors r1, . . ., rm in 
 by
(8.2.3)
The equations (8.2.1) and (8.2.2) can now be written as
thus we are seeking those vectors x that are orthogonal to each of the
vectors r1, . . ., rm. If, for example, rm is a linear combination of r1, . . .,
rm−1, then any x that is orthogonal to r1, . . ., rm−1 is automatically
orthogonal to rm, and so we may discard the equation x·rm = 0. Thus, by

discarding redundant equations as necessary, we may assume that the
vectors r1, . . ., rm are linearly independent vectors in 
. We can now state
our main result.
Theorem 8.2.1 Suppose that r1, . . ., rm are linearly independent vectors in
. Then the set
of solutions of (8.2.1) is a subspace of 
 of dimension n − m.
Proof We know that S is a subspace of 
, and we show first that
dim(S) ≤ n − m. Let W be the subspace spanned by the linear independent
vectors r1, . . ., rm; we claim that S ∩ W = {0}. Indeed, if x ∈ S ∩ W then x
is in S and so is orthogonal to each rj, and hence also to any linear
combination of the rj . However, as x ∈ W, x is itself a linear combination
of the rj ; thus x is orthogonal to itself and so x = 0. We deduce that
To show that dim(S) ≥ n − m, define a linear map α : 
 → 
 by
(8.2.4)
Clearly, S = ker (α). Thus
(8.2.5)
so that dim(S) ≥ n − m. Thus dim(S) = n − m.
If we now use the conclusion of Theorem 8.2.1, namely that S has
dimension n − m, in (8.2.5), we see that equality holds throughout (8.2.5),
so that α(
) = 
. Thus we have the following corollary.

Corollary 8.2.2 Suppose that r1, . . ., rm are linearly independent vectors in
. Then α : 
 → 
 given by (8.2.4) is surjective.
We end this section with an example which illustrates Theorem 8.2.1 by the
elimination of variables. In general, if we have m equations in n variables,
we can eliminate, say xn, from the equations to leave m − 1 equations in
n − 1 variables. This process can be repeated until we are left with one
equation in n − m + 1 variables. We then treat n − m of these variables as
'parameters' and, retracing our steps, we obtain the remaining variables in
terms of these n − m parameters.
Example 8.2.3 We solve the system of equations
Geometrically, we have to find the intersection of three hyperplanes in 
.
According to Theorem 8.2.1, if the equations (or, equivalently, the normals
to the hyperplanes) are linearly independent then the set of solutions will be
a one-dimensional subspace (that is, a line) in 
. The normals to the
hyperplanes are the vectors (1, 2, 0, 0), (3, 2, −4, 1) and (1, 1, 1, −1), and
(although it is not necessary to do so) we leave the reader to check that
these are linearly independent. In this example, we can add the last two
equations to obtain 4x1 + 3x2 − 3x3 = 0; equivalently, we can replace the
equations x·r2 = x·r3 = 0 by the equations x·(r2 + r3) = x·r3 = 0. Either way,
we now need to solve the system of equations
Writing x1 = 6t, we find x2, x3, and x4 in this order, and we see that the
general solution is (6t, −3t, 5t, 8t) for any real t. Thus S is the line in 
 that
is in the direction (6, −3, 5, 8).

Exercise 8.2
1.  Solve the system of equations
2.  Solve the system of equations
3.  Solve the system of equations
4.  Solve the equation x1 + x2 + x3 + x4 = 0.
8.3 Row rank and column rank
Let A be the matrix in (8.2.2). Each row and each column of A provides us
with a vector in 
 and
, respectively, and we are going to explore the
subspaces spanned by these vectors.
Definition 8.3.1 Let A be the matrix in (8.2.2). The j-th row vector rj of A is
the vector formed from the j-th row of A, namely
(8.3.1)
The j-th column vector cj of A is the vector formed from the j-th column of
A, namely

(8.3.2)
Definition 8.3.2 The m row vectors r1, . . ., rm span a subspace R of 
, and
the row rank of A is dim(R). The n column vectors c1, . . ., cn span a
subspace C of 
, and the column rank of A is dim(C). We denote these
ranks by ρrow(A) and ρcol(A) respectively.
The basic result is that the row rank of any matrix is the same as its column
rank.
Theorem 8.3.3 For any matrix A, the row rank of A equals the column rank
of A.
Proof Let α be as in (8.2.4), where we are no longer assuming that the
vectors r1, . . ., rm are linearly independent. Let ρrow(A) = t. Then there is a
set 
 of linearly independent vectors chosen from r1, . . ., rm such
that every rj is a linear combination of the . This shows that
and so, by Theorem 8.2.1, dim ker(α) = n − t. It follows that
Next, if x = (x1, . . ., xn) then x·rj = x1aj1 + x2aj2 + · · · + xnajn, so that ei
·rj = aji. It follows that
thus α(
) is spanned by the row vectors 
 in 
. Obviously, this
space has the same dimension of the space spanned by the column vectors
c1, . . ., cn in 
 (for this is the same result written in columns rather that
rows); thus ρcol(A) = dim α(
) = ρrow(A).

Exercise 8.3
1.  Verify Theorem 8.3.3 for each of the following matrices:
8.4 Inhomogeneous linear equations
We now discuss the space of solutions of a set of m simultaneous
inhomogeneous linear equations in the real variables x1, . . ., xn; for
example.
(8.4.1)
There are two issues of concern here, namely the existence of at least one
solution, and the characterization of the general solution. We shall see how
these issues can be reduced to finding (i) the subspace spanned by a given
set of vectors, and (ii) the kernel of a linear map.
We recall the n column vectors c1, . . . cn in 
 defined in (8.3.2), and
we define the vector b (also in 
) by b = (b1, . . ., bm)t. Then the system
(8.4.1) is equivalent to the single vector equation
in 
 in which the cj and b are column vectors and the xi are scalars. This
observation constitutes the proof of the following theorem which settles the
question of the existence of a solution.
Theorem 8.4.1 Let b = (b1, . . ., bm)t . Then the system (8.4.1) has a solution
if and only if b lies in the subspace of 
 spanned by the column vectors
c1, . . ., cn of the matrix aij.

Theorem 8.4.1 gives a condition that can, in principle, be checked in any
given example, for consider the matrices
The matrix B is often called the augmented matrix. The condition that b lies
in the space spanned by the column vectors cj is precisely the condition that
A and B have the same column ranks and, by Theorem 8.3.1, this is so if
and only if A and B have the same row ranks.
In order to characterize the set of all solutions (when a solution exists) we
again use the linear map α : 
 →
 defined in (8.2.4). In this case the
solution set of (8.4.1) is precisely the set S = {x ∈ 
 : α(x) = bt}. Let us
now suppose that at least one solution, say x∗, of (8.4.1) exists. Then
α(x) = bt if and only if α(x) − α(x∗) = 0 or, equivalently, if and only if
α(x − x∗) = 0, or x − x∗ ∈ ker(α). Thus we have proved the following
theorem (which is, in fact, a special case of Theorem 7.8.4).
Theorem 8.4.2 Suppose that x∗ is a solution of (8.4.1). Then x is a solution
of (8.4.1) if and only if x = x∗ + y, where y is a solution of the homogeneous
system (8.2.1).
We remark that Theorem 8.4.2 shows that the set of solutions of an
inhomogeneous system of linear equations is (if it is not empty) the
translation of a subspace of 
. We give an example to illustrate this.
Example 8.4.3 First, consider the system of equations
(8.4.2)
The first two equations imply that y3 = 0, and then the system reduces to the
single equation y1 − 2y2 = 0. Thus the space of solutions of this system is
the line L given by {(2t, t, 0) : t ∈ 
}.

Now consider the system
(8.4.3)
This has the same matrix as the system (8.4.2) and, in the notation used
above, b is the first column vector, so that b is certainly in the space
spanned by the column vectors. As (1, 0, 0) is a solution, then general
solution of (8.4.3) is the translated line (1, 0, 0) + L.
Finally, consider the system
(8.4.4)
(also with the same A). For brevity, let us write column vectors as row
vectors. If a solution exists (and we shall show it does not) then b must be a
linear combination of c1, c2, c3. As c2 = −2c1, this means that there are
scalars λ and μ such that (1, 2, 3) = λ(1, 2, −3) + μ(5, 8, 7). As no such λ
and μ exist, (8.4.4) has no solutions.
Exercise 8.4
1. Solve the equations
2. Find all values of t for which the system of equations

has a solution, and in each case give the general solution of the system.
8.5 Determinants and linear equations
So far, we have concentrated on the geometric theory behind the solutions
of linear equations. In the case when m = n, A is a square matrix and there is
a simple algebraic criterion for the existence of a unique solution of (8.4.1)
in terms of an n × n determinant (which we have not yet defined). In fact,
there is a formula, known as Cramer's rule, which expresses the solution as
ratios of determinants. The interested reader can find this in many texts;
however it is of less interest than it used to be as there are now many
computer packages available for solving systems of linear equations. We
briefly sketch the approach through determinants.
Theorem 8.5.1 Let A be the matrix (aij) in (8.2.2) and suppose that m = n.
Then the system of equations (8.4.1) has a unique solution if and only if the
row (and column) rank of A is n.
Proof Let α(x) = (x·r1, . . ., x·rn). Then x is a solution of (8.4.1) if and only if
α(x) = (b1, . . ., bn). If this has a unique solution, then the kernel of α can
only contain the zero vector, so that α(
) = 
. Conversely, if this is so,
then (8.4.1) has a solution, and it must be unique as then ker(α) = {0}. This
argument shows that (8.4.1) has a unique solution if and only if α(
) = 
.
However, this is so if and only if the row rank, and the column rank, of A is
n (this is part of the proof of Theorem 8.3.3).
Let us assume for the moment that we have defined an n × n determinant,
namely
where this is a real (or complex) number. We shall see that the definition of
a determinant (which we give in the next section) leads to the conclusion
that a determinant is non-zero if and only if its rows (or columns) are
linearly independent; equivalently, the row (and column) rank of the matrix
is n. Assuming this, we have the following result.

Theorem 8.5.2 Let A be the matrix (aij) in (8.2.2) and suppose that m = n.
Then the system of equations (8.4.1) has a unique solution if and only if
det(A) ≠ 0.
The point about this result is that it gives an algorithm for checking whether
or not the system of equations (8.4.1) has a unique solution (namely
whether det(A) is zero or not) and this algorithm can be implemented on a
computer.
Exercise 8.5
1.  Solve the equations
Find det(A) for this system of equations.
2.  Consider the system of equations
What is det(A)? Find x, y and z in terms of a, b and c.
8.6 Determinants
This section is devoted to the definition and properties of an n × n
determinant, and our definition is a direct generalization of the 3 × 3 case.
Definition 8.6.1 Suppose that A is an n × n matrix, say (aij). Then
(8.6.1)
where the sum is over all permutations σ of {1, 2, . . ., n}, and where (σ) is
the sign of the permutation σ. We write

We illustrate this with two examples.
Example 8.6.2 Suppose that A is an n × n upper-triangular matrix; that is,
aij = 0 whenever i > j (equivalently, all of the entries 'below' the diagonal
are zero); then det(A) = a11 · · · ann. For example, if
then det(A) = ad and det(B) = b11b22b33. To prove the general result, we
consider (8.6.1) for an upper-triangular matrix A. Take any permutation σ,
and note that an σ(n) = 0 when σ (n) < n; that is, unless σ (n) = n. Thus we
may restrict the sum in (8.6.1) to those σ that fix n. Next, an−1,σ(n−1) = 0
unless σ (n − 1) is n − 1 or n and, as it cannot be n, σ must also fix n − 1.
This argument can be continued to show that only the identity permutation
contributes to the sum in (8.6.1) and the result follows.
Example 8.6.3 For another example, suppose that
Now for any permutation σ, a3σ(3) = 0 unless σ(3) = 3, and similarly for
a4σ(4), so the sum in (8.6.1) reduces to the sum over the two permutations ι
(the identity) and the transposition τ = (1 2). Thus

Without further discussion, we shall now list and prove the main results
on determinants; these are valid for both real and complex entries.
Definition 8.6.4 The transpose At of a matrix A is obtained by 'turning the
matrix over'. Thus if A = (aij) and B = (bij) = At, then bij = aji; explicitly,
Note that A and At have different sizes unless m = n.
Theorem 8.6.5 For any square matrix A, det(At) = det(A).
Proof It is clear that for any permutations ρ and σ,
because the product on the right is simply a re-ordering of the product on
the left. If we now put ρ = σ−1, we obtain
and hence, as  (σ−1) =  (σ),

Now summing over σ is the same as summing over σ−1, so that
If B = At, then bij = aji, and this shows that det(A) = det(B).
Theorem 8.6.6 If the matrix B is obtained from A by interchanging two
rows, or two columns, then det(B) = −det(A). In particular, if two rows, or
two columns, are identical, then det(A) = 0.
Proof It is clear from Theorem 8.6.5 that it suffices to prove the result for
rows. Suppose, then, that B is obtained from A by interchanging the r-th and
s-th rows. We let A = (aij) and B = (bij), so that for all j, bij = aij if i = r, s,
and brj = asj, bsj = arj. Now let τ be the transposition (r s); then, for each
permutation σ ,
It follows that
As σ ranges over Sn, so does σ τ (for a fixed τ), so det(B) = −det(A). The last
statement follows because if we interchange two identical rows we both
leave the determinant unaltered and change its sign.
Theorem 8.6.7 The function det(A) of A is a linear function of each row of
A, and of each column of A,
Proof Theorem 8.6.5 shows that we need only prove the result for rows, and
Theorem 8.6.6 then shows that we need only prove the result for the first

row. Now det(A) is a finite sum ∑σ  (σ)a1σ(1) Aσ, say, where the terms Aσ
are independent of the entries a11, . . ., a1,n. As each term in this sum is a
linear function of the first row, so is the sum.
Theorem 8.6.8 If the rows of A are linearly dependent, then det(A) = 0. A
similar statement holds for the columns of A.
Proof We suppose that the rows of A are linearly dependent. Then some
row, say rj, is a linear combination of the other rows, and so when we
replace rj by this linear combination and then use the linearity of the
determinant function as a function of the rows (Theorem 8.6.7), we see that
det(A) is a linear combination of determinants, each of which has two
identical rows. The result now follows from Theorem 8.6.6.
We remark that this shows that if det(A) ≠ 0, then the rows of A are
linearly independent so that, in the context of Theorems 8.5.1 and 8.5.2, α(
) = 
. Thus we have proved the following result.
Theorem 8.6.9 If det(A) ≠ 0, then the system of equations (8.4.1) has a
unique solution.
The converse (and less useful) result will follow from results to be proved
in Chapter 9 (see Theorem 9.4.4).
Exercise 8.6
1.    Show that
2.    Prove that

3.    Suppose that
       and let A be associated the 3 × 3 matrix. Show that x D = y D = z D = 0,
where D = det(A).
4.    Show that for every matrix A, and every scalar λ, (At)t = A and
(λA)t = λ(At).

9
Matrices
9.1 The vector space of matrices
An m × n matrix A is a rectangular array of numbers, with m rows and n
columns, that is written in the form
(9.1.1)
We frequently use the notation A = (aij) when the values of m and n are
understood from the context (or not important). The aij are the entries, or
coefficients, of A; the matrix A is a real matrix if each aij is real, and a
complex matrix if each aij is complex. The matrix A is square if m = n, and
then the diagonal elements of A are a11, . . . , ann. The m × n zero matrix is
the matrix with all entries zero; we should perhaps denote this by some
symbol such as 0mn, but (like everyone else) we shall omit the suffix and
use 0 instead. The rows, or row vectors, of A are the vectors (aj1, aj2, . . . ,
ajn), and the columns, or column vectors, are defined similarly (and usually
written vertically). We need a notation for the set of matrices of a given
size.
Definition 9.1.1 The set of m × n matrices with entries in  is denoted by 

There is a natural definition of addition, and scalar multiplication, of
matrices in 
, namely
where cij = aij + bij, and
It is clear from these definitions that a linear combination of matrices Aj in 
 is also in 
, so (after checking some trivial facts) we see
that this space is a vector space. The zero 'vector' in this space is the zero
matrix, and the inverse of A is −A.
Theorem 9.1.2 The vector space 
 has dimension mn.
Proof For integers r and s satisfying 1 ≤ r ≤ m and 1 ≤ s ≤ n, let Ers be the m
× n matrix (eij) which has ers = 1 and all other entries zero; for example, if
m = n = 2 then
(9.1.2)
There are precisely mn matrices Ers in 
. It is clear that
(9.1.3)
so the Eij span 
. It is also easy to see that the matrices Ers are
linearly independent for

If this is the zero matrix, then λij = 0 for all i and j.
We shall ignore the distinction in punctuation between (x1, . . . , xn) (for
vectors) and (x1 · · · xn) (for matrices); then (x1, . . . , xn) in 
 is a 1 × n
matrix and 
. Further, the definitions of addition and scalar
multiplication of matrices coincides with that used previously for 
. Thus
a special case of Theorem 9.1.2 is that dim(
) = n (and in this case E1j
coincides with ej). We recall Definition 8.6.4 of the transpose of a matrix,
and we use this to define what is meant by a symmetric, and a skew-
symmetric, square matrix.
Definition 9.1.3 A square matrix A is symmetric if At = A, and skew-
symmetric if At = −A.
A real-valued function g(x) is even if g(−x) = g(x), and is odd if
g(−x) = −g(x)), and every function f (x) can be written uniquely as the sum
of an even function and an odd function, namely
An analogous statement holds for matrices: every square matrix can be
written uniquely as the sum of a symmetric matrix and a skew-symmetric
matrix. Indeed, if A is a square matrix, then
and 
 is symmetric and 
 is skew-symmetric. This
expression is unique for if A1 + B1 = A2 + B2, where the Ai are symmetric
and the Bj are skew-symmetric, then A1 − A2 = B2 − B1 = X, say, where
X = Xt and X = −Xt. It follows that X is the zero matrix, and hence that
A1 = A2 and B1 = B2. Finally, notice that if (xij) is skew-symmetric then
every diagonal element xii is zero because xii = −xii. It should be obvious to

the reader that the space of symmetric n × n matrices is a vector space, as is
the space of skew-symmetric matrices.
Theorem 9.1.4 The vector space 
 is the direct sum of the subspace
of symmetric matrices, of dimension n(n + 1)/2, and the subspace of skew-
symmetric matrices of dimension n(n − 1)/2.
Proof We leave the reader to check that the set of symmetric matrices, and
the set of skew-symmetric matrices, do indeed form subspaces of 
.
We have already seen that every matrix can be expressed uniquely as the
sum of a symmetric matrix and a skew-symmetric matrix, and this shows
that 
 is the direct sum of these two subspaces. Next, the matrices
Eij − Eji (see the proof of Theorem 9.1.2), where 1 ≤ i < j ≤ n, are skew-
symmetric, and as the diagonal elements of a skew-symmetric matrix are
zero, it is clear that these matrices span the subspace of skew-symmetric
matrices. As they are also linearly independent, we see that the subspace of
skew-symmetric matrices has dimension (n2 − n)/2. By Theorems 7.4.1 and
9.1.2, the subspace of symmetric matrices has dimension n2 − (n2 − n)/2.
We give two more examples.
Example 9.1.5 Consider the space Zm×n of real m × n matrices X with the
property that the sum of the elements over each row, and over each column,
is zero. It should be clear that Zm×n is a real vector space, and we claim that
dim(Zm×n) = (m − 1)(n − 1). We start the proof in the case when m = n = 3,
but only to give the general idea.
We start with an 'empty' 3 × 3 matrix and then make an arbitrary choice
of the entries, say a, b, c and d, that are not in the last row or column; this
gives us a matrix
where * represents an as yet undetermined entry in the matrix. We now
impose the condition that the first two columns must sum to zero, and after

this we impose the condition that all rows sum to zero; thus the 'matrix'
becomes
Notice that the last column automatically sums to zero (because the sum
over all elements is zero, as is seen by summing over rows, and the first two
columns sum to zero). Exactly the same argument can be used for any
'empty' m × n matrix.
The choice of elements not in the last row or last column is actually a
choice of an arbitrary matrix in M(m−1)×(n−1)( ), so this construction actually
creates a surjective map Φ from M(m−1)×(n−1)( ) onto Zm×n. It should be
clear that this map is linear, and that the only element in its kernel is the
zero matrix. Thus
as required.
Example 9.1.6 This example contains a discussion of magic squares (this is
a 'popular' item, but it is not important). For any n × n matrix X, the trace
tr(X) of X is the sum x11 + · · · + xnn of the diagonal elements, and the anti-
trace tr*(X) of X is the sum over the 'other' diagonal, namely x1n + · ·
· + xn1. A real n × n matrix A is a magic square if the sum over each row,
the sum over each column, and the sum over each of the two diagonals (that
is, tr(A) and tr*(X)), all give the same value, say μ(A). We note that
μ(A) = n−1 ∑i, j aij. It is easy to see that the space Sn×n of n × n magic
squares is a real vector space so, naturally, we ask what is its dimension? It
is easy to see that dim(Sn×n) = 1 when n is 1 or 2, and we shall now show
that for n ≥ 3, dim(Sn×n) = n(n − 2).
Let 
 be the subspace of matrices A for which μ(A) = 0. This
subspace is the kernel of the linear map A → μ(A) from Sn×n to 
, and as

this map is surjective (consider the matrix A with all entries x/n) we see that
dim(Sn×n) = dim(
) + 1.
Next, the space Zn×n of n × n matrices all of whose rows and columns
sum to zero has dimension (n − 1)2 (see Example 9.1.6). Now define Φ :
Zn×n → 
 by Φ(X) = (tr(X), tr*(X)). Then Φ is a linear map, and 
. It is not difficult to show that Φ is surjective (we shall prove
this shortly), and with this we see that
so that dim(Sn×n) = (n − 1)2 − 1 = n(n − 2). It remains to show that Φ is
surjective, and it is sufficient to construct matrices P and Q in Zn×n such
that Φ(P) = (a, 0) and Φ(Q) = (0, b) for all (or just some non-zero) a and b.
If n = 3, we let
and then Φ(P) = (a, 0) and Φ(Q) = (0, b).
If n ≥ 4 we can take p11 = p22 = a/2, p12 = p21 = −a/2 and all other pij = 0;
then t(P) = a and t∗(P) = 0, so that Φ(P) = (a, 0). Similarly, we choose
q1,n−1 = q2n = −b/2 and q1n = q2,n−1 = b/2, so that Φ(Q) = (0, b).
Exercise 9.1
1.  A matrix (aij) is a diagonal matrix if aij = 0 whenever i ≠ j. Show that the
space D of real n × n diagonal matrices is a vector space of dimension n.
2.  A matrix (aij) is an upper-triangular matrix if aij = 0 whenever i > j.
Show that the space U of real n × n upper-triangular matrices is a vector
space. What is its dimension?
3.  Define what it means to say that a matrix (aij) is a lower-triangular
matrix (see Exercise 2). Let L be the vector space of real lower-
triangular matrices, and let D and U be as in Exercises 1 and 2. Show,
without calculating any of the dimensions, that

dim(U) + dim(L) = dim(D) + n2. Now verify this by calculating each of
the dimensions.
4.  Show that the space of n × n matrices with trace zero is a vector space of
dimension n2 − 1.
5.  Show (in Example 9.1.7) that dim(S1×1) = dim(S2×2) = 1.
6.  Show that if X is a 3 × 3 magic square, then x22 = Σ(X)/3. Deduce that if
Σ(X) = 0 then X is of the form
Let A, B, C be the matrices
respectively. Show that
(a) {A, B, C} is a basis of S3×3;
(b) {A, B} is a basis of 
(c) {A, C} is a basis of the space of symmetric 3 × 3 magic squares;
(d) {B} is a basis of the space of skew-symmetric 3 × 3 magic squares.
9.2 A matrix as a linear transformation
A matrix is a rectangular array of numbers, say A given in (Example 7.1.2)
that 
n,t is the space of real column vectors of dimension n, and similarly
for 
n,t.
Definition 9.2.1 The action of A as a linear map from 
n,t to 
m,t is defined
to be the map
(9.2.1)

which we write as x 
 A(x). It is evident that for scalars λ and μ,
A(λx + μy) = λA(x) + μA(y), so that 
 is linear.
Definition 9.2.1 prompts us to define the product of an m × n matrix with an
n × 1 by matrix (or column vector) by
(9.2.2)
Shortly, we shall define the product of m × n matrix and an n × r matrix (in
this order) to be an m × r matrix in a way that is consistent with this
definition. One way to remember the formula (9.2.1) is to let r1, . . . , rm be
the row vectors of A; if x is a column vector, then
where the entries here are the usual scalar products of two row vectors.
Alternatively, we can describe the action of A in terms of standard bases.
Let e1 = (1, 0, . . . , 0), . . . , en = (0, . . . , 0, 1) be the standard basis of 
n,
and (here we must use different symbols because we may have m ≠ n) let 
1 = (1, 0, . . . , 0), . . . , m = (0, . . . , 0, 1) be the standard basis of 
m. Then,
directly from (9.2.1),
This shows that the first column of A is the vector of coefficients of A(e1t ),
the second column is the vector of coefficients of A(e2t ), and so on, all in
terms of the basis 1t , . . . , mt. More generally, if

then
which agrees with (9.2.1). We now give some examples a matrix action.
Example 9.2.2 The matrix (a1 · · · an) gives the linear map of 
n,t to 
 that
takes the column vector x to the scalar product a·xt.
Example 9.2.3 The matrix action
is the vector product map xt → a × xt of 
 into itself.
Example 9.2.4 The orthogonal projection onto a plane Consider the map of
 onto the plane Π with equation x1 + x2 + x3 = 0 obtained by orthogonal
projection (that is, x → y, where y is the point of Π that is nearest to x). It is
clear that y = x + t(1, 1, 1), where t is determined by the condition that y ∈
Π, and this gives
Thus yt = A(xt ), where

Let us now consider the product of two matrices. An m × n matrix A and an
r × m matrix B provide two linear maps that are illustrated in the mapping
diagram
It follows that the composition x → B(A(x)) is a linear map from Fn,t to Fr,t,
and we shall now show that this is also given by a matrix action. Suppose
that A = (aij) and B = (bij ), and that x = (x1, · · · , xn)t. Then
(9.2.3)
where the cij are given by
(9.2.4)
As an example of this, we see that the product
is
Given that the composite map is given by the matrix (9.2.3), it is natural to
define the product B A of the matrices B and A (in this order) to be the
matrix (cij) of this combined action.

Definition 9.2.5 The matrix product B A of an r × m matrix B and an m × n
matrix A is the r × n matrix of the combined action x → B(A(x)). Explicitly,
if A = (aij) and B = (bij), then B A = (cij), where the cij are given in (9.4.2).
Informally, cij is the scalar product of the i-th row of B with the j-th column
of A.
Sometimes (9.2.3) is taken as the definition of the matrix product without
any reference to the matrix action; however, there can be little (if any)
justification for this definition except by consideration of the composite
map. Note that the product B A of an p × q matrix B and a r × s matrix A is
only defined when q = r (as otherwise, the composite map is not defined).
Accordingly, when we write a matrix product we shall always implicitly
assume that the matrices are of the correct 'size' for the product to exist.
A tedious argument (which we omit) shows that the matrix product is
associative: that is, A(BC) = (AB)C. For each n, let In be the n × n identity
matrix; that is
(9.2.5)
where the entries in the diagonal are 1, and all other entries are zero. If X is
an m × n matrix, then Im X = X and X In = X but, of course, the products X
Im and In X are not defined (unless m = n). The matrix product is not
commutative; for example, as the reader should check, if
then
This example also shows that the product of two matrices can be the zero
matrix without either being the zero matrix, and it follows from this that

there are non-zero matrices that do not have a multiplicative inverse.
Finally, we consider the transpose of a product.
Theorem 9.2.6 The transpose (AB)t of the product AB is Bt At.
Proof Let A = (aij), At = (αij), B = (bij), Bt = (βij), AB = (cij) and Bt At = (γij).
Note that αij = a ji, and βij = bji, and we want to show that γij = c ji. Now
γij = r βir αrj = r bri a jr = cji, so that Bt At = (AB)t.
Exercise 9.2
1.  Show that if A is a square matrix, then AAt is symmetric. Choose any 2 ×
2 matrix and verify this directly.
2. Show that two symmetric n × n matrices A and B commute (AB = BA) if
and only if their product AB is symmetric.
3.  Show that if a 2 × 2 complex matrix X commutes with every 2 × 2
complex matrix, then X = λI for some complex λ, where I is the 2 × 2
identity matrix.
4.  Let 
 and for each 2 × 2 real matrix X let α(X) = AX; thus α
is a map of the space M2×2( ) of real 2 × 2 matrices into itself. Show
that α is a linear map. Find a basis of the kernel, and of the range, of α.
[These bases should be made up of 2 × 2 matrices, and the sum of the
dimensions of these two subspaces should be four.]
5.  Let
Show that B commutes with A (AB = B A). Show that the set of 2 × 2
real matrices X that commute with A is a subspace M0 of the space of
real 2 × 2 matrices. Show also that dim(M0) = 2, and that I2 and B form a
basis of M0. As A and A2 commute with A, this implies that A and A2 are
linear combinations of I and B. Find these linear combinations.
6.  Let A be a real n × n matrix. Show that the map α : X → AX − X A is a
linear map of Mn×n( ) to itself. Show that the set of matrices X that

commute with A is a subspace M(A) of Mn×n( ). You should do this (a)
by a direct argument, and (b) by considering the kernel of α. Is A in
M(A)? Now use a dimension argument to show that the map α is not
surjective, thus there is some matrix B such that B ≠ AX − XA for any X.
9.3 The matrix of a linear transformation
In this section we introduce the matrix representation of a linear
transformation. Matrices play exactly the same role for linear
transformations as co-ordinates play for vectors, so it may be helpful to
begin with a few comments on vectors and their co-ordinates. First, it is
essential to understand that a vector space V is just a set of objects (with a
structure), and that a vector x in V does not have any co-ordinates until an
ordered basis of V has been specified. In particular, the vector space 
 is
the set of ordered pairs of real numbers, and points in 
, for instance
x = (3, 2), do not have any co-ordinates until we choose an ordered basis of 
.
A basis is a frame of reference (rather like a set of co-ordinate axes), and
once we are given an ordered basis  = {v1, . . . , vn} of V, we can take any
x in V and then write 
, where the scalars xj are the co-ordinates
of x relative to . These co-ordinates xj are uniquely determined by the x
and 
, so we should perhaps write this as 
. To return to
our example, if we take the 'natural' basis ε = {e1, e2} of 
, where
e1 = (1, 0) and e2 = (0, 1), and let x = (3, 2), then x = 3e1 + 2e2 so that
xε = (3, 2). On the other hand, if we decide to use the basis  = {u1, u2},
where u1 = (1, 0) and u2 = (1, 1), then x = u1 + 2u2 so that 
. It
should be clear that if we keep the point x fixed (as an 'abstract vector'),
and vary the choice of basis, the the co-ordinates of x will also change;
equally, if we insist on keeping the co-ordinates fixed, the vector will
change as the basis changes.
We turn now to linear transformations, where the situation is similar.
Given two vector spaces V and W, we wish to assign 'co-ordinates' to each
linear map α : V → W, and we can do this once we have chosen an ordered
basis of V and an ordered basis of W. Here, the 'co-ordinates' of α form a
matrix, and if we keep α fixed and vary the bases in V and W then the
matrix for α will change. In the same way, if we start with a (fixed) matrix,

and then vary the bases, we will obtain different linear maps. This
discussion makes it clear that it will be helpful to have a convenient
notation for a basis of a vector space. Given a vector space V, we shall use 
V (with a suffix V) to denote a typical basis of V. When we want to
consider a linear map α : V → W, where V and W have prescribed bases 
V
and 
W , respectively, we shall write
(9.3.1)
We shall now show that the linear map (9.3.1) can be specified in terms of
co-ordinates, and that the 'co-ordinates' for α form a matrix.
Let 
V = {v1, . . . , vn} and 
W = {w1, . . . , wm}. Then there are scalars aij
(uniquely determined by α, 
V and 
W) such that
(9.3.2)
Next, let y = α(x), where 
, and 
. Then, as α is
linear,
As the wi are linearly independent, this means that

and this is precisely the effect of applying the matrix
to the vector (x1, . . . , xn)t as given in (9.2.2). We can summarize this
discussion as follows: a linear map α : 
V → 
W determines a matrix A as
above, and that if y = α(x), then the action of this matrix is to map the co-
ordinates of x relative to 
V to the co-ordinates of α(x) relative to 
W.
Definition 9.3.1 Given α : 
V → 
W , the matrix A constructed above is the
matrix representation of α relative to 
V and 
W.
If we keep α fixed but change the bases, then the matrix representation of α
will change, so that a linear map has many different matrix representations
(one for each pair of bases). For any basis 
V of V, the matrix
representation of the identity map I : 
V → 
V is the identity matrix; this
follows from (9.3.2). However the matrix representation of I relative to a
pair of different bases of V will not be the identity matrix.
Finally, suppose we are given a real m × n matrix A: this acts as a linear
map from Rn,t to Rm,t and so has a matrix representation relative to every
pair of bases of these spaces. Moreover, as for any other linear map, this
matrix representation will change if we change the bases, so the matrix
representation for A (as a linear map) will not always be the matrix A. On
the other hand, the matrix representation for A relative the standard bases
(1, 0, . . . , 0), . . . of these spaces will be A itself. This is analogous to the
distinction between the point (3, 2) in 
, and its co-ordinate row (3, 2)ε ,
and the distinction between the role of a matrix as a linear map, and as a
matrix representation of a linear map, must be kept clearly in mind. It is
time to clarify these ideas with some specific examples.
Example 9.3.2 The vector space V of solutions of the differential equation 
 has basis  = {c, s}, where c and s are the functions given by
c(t) = cos 2t and s(t) = sin 2t. Now consider the linear map α : V → V given

by 
. As α(c) = −3c − 2s and α(s) = 2c − 3s, we
see that the matrix representation of α :  →  is
The solution x = Ac + Bs is mapped by α to (−3A + 2B)c + (−2A − 3B)s and,
as predicted, the matrix action is given by
Example 9.3.3 We recall from Example 9.2.4 that the orthogonal
projection, say α, of 
 onto the plane Π given by x1 + x2 + x3 = 0 has
matrix representation
with respect to the standard basis {(1, 0, 0), (0, 1, 0), (0, 0, 1)} in both
domain and the range. Let us now consider the matrix representation of α in
terms of a more sensible choice of basis. As α fixes every vector on Π, we
choose a basis u and v of Π and then extend this to a basis of 
 by adding
the normal to Π, namely w = (1, 1, 1). Then α(u) = u, α(v) = v and α(w) = 0,
so that α(λu + μv + νw) = λu + μv, and the action from co-ordinates to co-
ordinates is given by
It follows that the matrix representation of α with respect to  is

Of course, the simplicity of this new representation derives from a choice of
basis that is intimately related to the geometric action of α.
Example 9.3.4 Let u be a unit vector in 
, and let α be the map of 
 into
itself given by α(x) = u × x (the vector product). Next, let v be any unit
vector orthogonal to u, and let w = u × v. Then  = {u, v, w} is a basis of 
, and α(u) = 0, α(v) = w, α(w) = −v. Thus α(λu + μv + νw) = −νv + μw, and
the action from co-ordinates to co-ordinates is given by
This is simpler than the previous representation, and again this is because of
a better choice of basis.
The following result is an immediate consequence of the ideas of a
matrix representation.
Theorem 9.3.5 Suppose that α : 
V → 
W and β : 
V → 
W are
represented by matrices A and B, respectively. Then α + β, and λα, are
represented by A + B and λA, respectively.
We shall now consider the matrix representation of a composition of two
linear maps, say
Not surprisingly, the matrix product has been defined in such a way that the
matrix representation of a composition of two maps is the product of the
two matrix representations.
Theorem 9.3.6 Suppose that the linear maps α : 
V → 
W and β : 
U → 
V have matrix representations A and B, respectively. Then αβ : 
U → 
W
has matrix representation AB.
Proof Let

Also, for j = i, . . . , ℓ, and k = 1, . . . , m, let
Then
so that cij = ai1b1 j + · · · + aimbm j.
Exercise 9.3
1.  Show that
where * refers to some unspecified entry in the matrix.
2.  Let

Let  be the basis {1, z, z2} of the space P2 of complex polynomials of
degree at most two. Show that the map α :  →  defined by α( p)
(z) = p′(z) + p(1) is a linear map with matrix representation A. Now let ′
be the basis {1, 1 + z, 1 + z + z2}. Show that the matrix of α : ′ → ′ is
B. Show also that the linear map β( p)(z) = p′(z) + p′′(z) has matrix C
relative to the basis ′.
3.  The vector space M2×2( ) has basis  = {E11, E12, E21, E22} given in
(9.1.2). Let τ (A) be the transpose of the matrix A. Show that τ :  → 
is a linear map with matrix representation
4.  Let ε = {(1, 0), (0, 1)} of , and let
Show that α and β are linear maps of  to itself. Find the matrix
representations A of α, B of β, and C of αβ (in each case relative to ε),
and verify (by matrix multiplication) that C = AB.
9.4 Inverse maps and matrices
In this section we discuss the relationship between inverse maps and inverse
matrices. Recall that, for each n, there is the n × n identity matrix In; it is
usual to omit the suffix n and use I regardless of the value of n.
Definition 9.4.1 An n × n matrix A is invertible, or non-singular, if there is
an n × n matrix B such that AB = I = B A. If so, we say that B is the inverse
of A, and we write B = A−1.

Such a matrix B need not exist, but if it does then it is unique, for suppose
that we also have AC = I = CA. Then B = B I = B(AC) = (BA)C = IC = C.
The following theorem is the major result of this section; however, its
proof will be delayed until the end of the section.
Theorem 9.4.2 Let V and W be vector spaces of the same finite dimension,
and suppose that the linear map α : 
V → 
W has matrix representation A.
Then the following are equivalent:
(1)  α−1 : W → V exists;
(2)  A−1 exists;
(3)  det(A) ≠ 0.
The first step is to link inverse matrices and inverse transformations.
Suppose that α : V → W is a bijective linear map (an isomorphism) between
finite-dimensional vector spaces. Then the inverse map α−1 exists and is
linear. Moreover, dim(V ) = dim(W ), so that any matrix representation A of
α is a square matrix. We shall now show (as we would hope, and expect)
that A−1 exists and is the matrix representation of α−1 with respect to the
same bases.
Theorem 9.4.3 Suppose that α : V → W is an isomorphism, and let A be the
matrix representation of α : 
V → 
W . Then A−1 exists and is the matrix
representation of α−1 : 
W → 
V.
Proof Let B be the matrix representation of α−1 : 
W → 
V. According to
Theorem 9.3.6, the linear map α−1α : (V, 
V) → (V, 
V) has matrix
representation BA. However, α−1α is the identity map, and this is
represented by the identity matrix, irrespective of the choice of 
V. Thus
BA = I, and similarly, AB = I.
Definition 9.4.1 of an inverse matrix B of A requires that AB = BA = I. If
we only know that AB = I, it is not obvious that BA = I.

Theorem 9.4.4 Suppose that A and B are n × n matrices. If AB = I then
BA = I.
Proof Let ε be the standard basis of 
 (that is, 
), and define the
linear map α by matrix multiplication; that is, α(x) = Ax. The matrix
representation of α with respect to ε is A. Similarly, the matrix
representation of the linear map β defined by β(x) = Bx is B. As the matrix
representation of αβ is AB, and as AB = I, we see that αβ(x) = x for all x.
This implies that α−1 exists and is β. As β(α(x)) = α−1(α(x)) = x for all x, and
as this linear map has matrix representation BA, we see that BA = I.
The proof of Theorem 9.4.2 depends on the following result.
Theorem 9.4.5 For any n × n matrices A and B,
In particular, det(AB) = det(BA).
Before we prove this, we record the following corollary whose proof is just
an application of Theorem 9.4.5 to the equation AA−1 = I.
Corollary 9.4.6 If A−1 exists, then det(A) ≠ 0, and det(A−1) = 1/det(A).
The proof of Theorem 9.4.5 Let A = (aij), B = (bij) and AB = (cij) so that
(9.4.1)
Now, by definition,
(9.4.2)
However, by (9.4.1), the term c1ρ(1) · · · cnρ(n) is

so, after expanding these products, we can rewrite (9.4.2) in the form
(9.4.3)
where the terms β(B, t1, . . . , tn) depend only on the stated parameters, and
do not depend on the aij. We now have to evaluate the terms β(B, t1, . . . ,
tn), and we do this by regarding (9.4.3) as an identity in all of the variables
apq and brs, and making a specific choice of the apq.
Choose any integers s1, . . . , sn from {1, . . . , n}, and then put 
, and all other aij equal to zero. Let c′ij be the value of
cij corresponding to this choice of aij ; thus, from (9.4.1) c′ij = bsij. As it is
clear that
we see immediately from (9.4.2) and (9.4.3) that
Thus
Now by Theorem 8.6.6, this determinant is zero if sp = sq for some p ≠ q; in
other words, β(B, s1, . . . , sn) = 0 unless s1, . . . , sn is a permutation of 1, . . .
, n. If we use this in conjunction with (9.4.3), we now see that

(9.4.4)
Finally, if ρ is a product of q transpositions, so that 
, then, by
applying the operation of interchanging two rows of this determinant q
times in an appropriate manner, we find that
Thus
It only remains to give the proof of Theorem 9.4.2.
The proof of Theorem 9.4.2 Theorem 9.4.3 shows that (1) implies (2), and
Corollary 9.4.6 shows that (2) implies (3). Suppose now that (3) holds;
then, from Theorem 7.9.5, it is sufficiant to show that α is surjective. Now
the matrix representation A of α : 
V → 
W is such that
if and only if

(9.4.5)
and it follows from this that we have to show that given any μi there are λj
such that (9.4.5) holds.
Now consider A as a linear map of (λ1, . . . , λn)t onto (μ1, . . . , μn)t given
by (9.4.5). As the j-th column of A is 
, and as these columns are
linearly independent, we see that the map given by (9.4.5) maps 
 onto
itself. Thus it is the case that given any μi there are λj such that (9.4.5)
holds, and we deduce that α−1 exists.
Exercise 9.4
1.  Let A be a 2 × 2 matrix with integer entries. Show that A−1 exists and has
integer entries if and only if det(A) = ±1.
2.  Show that the set of n × n matrices A with entries in F, and with
det(A) ≠ 0, is a group with respect to matrix multiplication.
3.  Let
(i)  Suppose that ad − bc ≠ 0. Find A−1.
(ii)  Suppose that ad − bc = 0. Find all column vectors x such that
4.  In each of the following examples, in which 
 is given the basis
e1 = (1, 0) and e2 = (0, 1), determine whether each of the conditions in
Theorem 9.4.2 holds:
(i) α(xe1 + ye2) = (2x + y)e1 + (−x + 3y)e2;
(ii) α(xe1 + ye2) = (x − y)e1 + (−2x + 2y)e2;

(iii) α(xe1 + ye2) = (6x + 2y)e1 + (12x + 4y)e2.
9.5 Change of bases
Suppose that α is a linear map of a vector space V into itself, and let  and 
′ be bases of V. Then α : 
 → 
 and α : 
′ → 
′ are represented by
matrices, say A and A′, respectively, and we want to find the relationship
between A and A′. To find this relationship, we consider the following
composition of maps, where I is the identity map of V onto itself:
Let P be the matrix representation of the map I : ′ → . Then P−1 is the
matrix representation of its inverse, namely I :  → ′, and from Theorem
9.3.6, the composite map I α I : ′ → ′ has matrix representation P−1 AP.
As this composite map is just α, we have proved the following result.
Theorem 9.5.1 Let  and ′ be bases of a vector space V. If the linear map
α :  →  is represented by a matrix A, and the linear map α : ′ → ′ is
represented by A′, then A′ = P−1 AP, where P is the matrix representation of
the identity map I : ′ → .
We illustrate this in the next example.
Example 9.5.2 Let V = 
,  = {e1, e2} and ′ = {v1, v2}, where v1 = (1,
1) and v2 = (1, 0). Now let α be the linear map defined by
As α(e1) = e1 + e2 and α(e2) = e1 + 2e2, the matrix representation of α :  →
 is
and as α(v1) = 3v1 − v2 and α(v2) = v1 the matrix representation of α : ′ → 
′ is

Finally, we must consider the matrix representation P of identity map I : ′
→ . Here, we have I (v1) = v1 = e1 + e2 and I (v2) = v2 = e1, so that
Theorem 9.5.1 tells us that A′ = P−1 AP and we leave the reader to confirm
that this is indeed so (as P−1 exists, it suffices to show that P A′ = AP).
Theorem 9.5.1 illuminates an important matter that we have already
mentioned. Suppose that we want to study a linear map α : V → V. It may
be convenient to represent α by a matrix A, but before we can do this we
have to choose a basis of V. Some bases will lead to a simple form for A,
while others will lead to a more complicated form. Obviously we choose
whichever basis of V suits our purpose best, but in any case, the degree of
choice available here is given by Theorem 9.5.1.
There is another important issue here. Any result about the linear map α
that is proved by resorting to a matrix representation of α may possibly
depend on the choice of the basis (and the resulting matrix). In general, we
would then want to show that the result does not, in fact, depend on the
basis. In fact, it is far better to avoid this step altogether and to prove results
about linear maps directly without resorting to their matrix representations.
As an illustration of these ideas, let us consider the determinant of a linear
map α : V → V. Let A and A′ be two different matrix representations of α so
that, by Theorem 9.5.1, there is a matrix P such that A′ = P−1 AP. As det(XY
) = det(X) det(Y) for any matrices X and Y, we see that

This gives the next result.
Theorem 9.5.3 Suppose that α : V → V is a linear map. Then there is a
scalar det(α) such that if A is any matrix representation of α, then
det(A) = det(α).
The scalar det(α) is called the determinant of α, and the significance of
Theorem 9.5.3 is that although det(α) can be computed from any matrix
representation A of α, it is independent of the choice of A.
Exercise 9.5
1.  Let  = {v1, v2, v3} be a basis of a vector space V, and let ′ = {v1,
v1 + v2, v1 + v2, v3}. Show that ′ is a basis for V. Now define a linear
map α : V → V by α(v1) = v2 + v3, α(v2) = v3 + v1 and α(v3) = v1 + v2.
Find A, A′ and P as in Theorem 9.5.1 and verify that A′ = P−1 AP. What
is det(α) in this case?
2.  Let a be any non-zero vector in 
 and let α : 
 → 
 be the linear map
defined by α(x) = a × x (the vector product). Without doing any
calculations, explain why det(α) = 0.
3.  Let a, b and c be linearly independent vectors in 
, and let α : 
 → 
be defined by α(x) = (x·a, x·b, x·c), where x·y is the scalar product in 
.
What is det(α)?
4.  In Example 9.3.3 we obtained two matrix representations, of the
orthogonal projection α of 
 onto a plane Π. Verify all of the details of
Theorem 9.5.1 in this example.
9.6 The resultant of two polynomials
This section is concerned with complex polynomials, and it extends the
ideas discussed in Example 7.6.5. We showed there that if, say,
where anbm ≠ 0 and n, m ≥ 1, then the m + n − 1 polynomials

(9.6.1)
form a basis of the space of polynomials of degree at most m + n − 1 if and
only if f and g have no common zero. Now for some matrix M,
(9.6.2)
and, by inspection, we see that
It is helpful to remember that diagonal of M comprises deg(g) terms a0
followed by deg(f) terms bm.
Definition 9.6.1 The resultant R(f, g) of the polynomials f and g is det M.
The discriminant ∆(f) of f is the resultant of f (z) and its derivative f′(z).
Theorem 9.6.2 The following are equivalent:
(1)  det(M) = 0;

(2)  the polynomials in (9.6.1) are linearly dependent;
(3)  the polynomials f and g have a common zero.
Let us explore the consequences of this result before we give a proof. First,
it shows that any two polynomials f and g have a common zero if and only
if R(f, g) = 0, and the determinant R(f, g) can be evaluated in any specific
case even though we may be unable to find the zeros of f or g. Next, we
know that f and f′ have a common zero if and only if f has a repeated zero;
thus we conclude that an arbitrary polynomial f has distinct zeros if and
only if ∆( f ) = 0. For example, if f (z) = z2 + az + b then f′(x) = 2z + a so that
f has distinct zeros if and only if
As this determinant is 4b − a2, this agrees with the familiar result obtained
by an elementary argument.
Example 9.6.3 Given a cubic polynomial f, we can always find some z0
such that the polynomial f (z + z0) has no term in z2 (and f (z + z0) has
distinct zeros if and only if f does), so there is no harm in assuming that f
(z) = z3 + az + b. Now a necessary and sufficient condition for z3 + az + b to
have distinct roots is that
and this is the condition 4a3 + 27b2 = 0 (see Exercise 9.6.2).
The proof of Theorem 9.6.2 First, Theorem 7.6.6 shows that (2) and (3) are
equivalent. We shall show that (1) and (2) are equivalent by proving that
det(M) ≠ 0 if and only if the polynomials in (9.6.1) are linearly independent.

Throughout, P will be the vector space of complex polynomials of degree at
most m + n − 1.
Suppose that det(M) ≠ 0; then, by Theorem 9.4.2, M−1 exists. If we
multiply each side of (9.6.2) on the left by M−1 we see that each of the
polynomials 1, z, z2, . . . , zm+n−1 is a linear combination of the polynomials
in (9.6.1). This implies that the polynomials in (9.6.1) span P, and as there
are exactly m + n of them, they form a basis of P and so are linearly
independent.
Now suppose that the polynomials in (9.6.1) are linearly independent. As
the number of polynomials here is dim(P), they span P and so there is a
matrix N such that
(9.6.3)
With (9.6.2) this shows that

As the polynomials zj form a basis of P, this implies that N M = I, and hence
that det(M) ≠ 0.
We end with two applications of Theorem 9.6.2.
Theorem 9.6.4 Let f and g be nonconstant polynomials, and let h be the
greatest common divisor of f and g. Then there are polynomials a and b,
such that a(z) f (z) + b(z)g(z) = h(z).
Proof Suppose first that f and g have no common zero. Then the only
polynomials that divide f and g are the constant polynomials, so we may
take h (which is only determined up to a scalar multiple) to be the constant
polynomial with value 1. By Theorem 9.6.2, the polynomials in (9.6.1) span
P, and so some linear combination, say a(z) f (z) + b(z)g(z), of them is the
polynomial h(z). For the general case, let f1 = f / h and g1 = g/ h, and apply
the above result to f1 and g1.
Theorem 9.6.5 For each positive integer n there exists a non-constant
polynomial Φn(w1, w2, . . . , wn+1) in the n + 1 complex variables wj such
that the polynomial a0 + a1z + · · · + anzn of degree n has a repeated zero if
and only if Φn(a0, . . . , an) = 0.
Proof We take Φ(a0, . . . , an) = R( f, f′).
Exercise 9.6
1.  Let f (z) = a0 + a1z + · · · + anzn. Find z0 such that the polynomial f
(z + z0) has no term in zn−1.
2.  Suppose that f (z) = z3 + az + b has a repeated root z1, so that f (z1) = f′
(z1) = 0. Find z1 from the equation f′(z1) = 0 and then use f (z1) = 0 to
give a simple proof of the result in Example 9.6.3.
9.7 The number of surjections
In Chapter 1 (Theorem 1.5.9) we proved that if m ≥ n, then there are

(9.7.1)
surjections from a set with m elements to a set with n elements. We did this
by showing that, in matrix notation (which was not available at the time),
(9.7.2)
In order to find S(m, n) we have to find the inverse to the matrix on the left
of (Lemma 1.5.10. As an example, we have
and this shows the general pattern of the inverse matrix.
Lemma 9.7.1 Let A = (aij) be the square matrix in (9.7.2). Then A−1 = (bij),
where bij = (−1)i+j aij.
With this result available, we multiply both sides of (Lemma 9.7.1 is simply
an exercise in matrix multiplication.
The proof of Lemma 9.7.1 Let B = (bij ), where bij is defined in Lemma
9.7.1, and let AB = (cij). Then

If p < q then, for each k, either k > p (and apk = 0) or k < q (so that bkq = 0).
Thus cpq = 0 unless p ≥ q. If p ≥ q then, with r = p − q and k = q + t,
Exercise 9.7
1.  Show that

10
Eigenvectors
10.1 Eigenvalues and eigenvectors
One of the key ideas that is used to analyse the behaviour of a linear map α
: V → V is that of an invariant subspace.
Definition 10.1.1 Suppose that α : V → V is linear. Then a subspace U of V
is said to be invariant under α, or an α-invariant subspace, if α(U) ⊂ U.
The simplest form of a linear map α : V → V is multiplication by a fixed
scalar; that is, for some scalar μ, α(v) = μυ. These maps are too simple to be
of any interest, but it is extremely profitable to study subspaces on which a
given linear map α has this particular form.
Definition 10.1.2 Suppose that α : V → V is a linear map. Then, for any
scalar λ, Eλ is the set of vectors v such that α(v) = λυ. Clearly 0 ∈ Eλ for
every λ. We say that λ is an eigenvalue of α if and only if Eλ contains a non-
zero vector; any non-zero vector in Eλ is an eigenvector of α associated with
λ.
It is clear that Eλ is a subspace of V, for it is the kernel of the linear map
α − λI, where I is the identity map (alternatively, it is clear that any linear
combination of vectors in Eλ is also in Eλ). In fact, Eλ is an α-invariant
subspace for suppose that v ∈ Eλ, and let w = α(v). Then w = λv, so that
α(w) = λα(v) = λw, and hence w ∈ Eλ.

If λ is not an eigenvalue of α, then Eλ = {0}. If λ is an eigenvalue of α,
then Eλ consists of the zero vector together with all of the (non-zero)
eigenvectors associated with λ. If v is an eigenvector of α then, as v ≠ 0, we
can find the unique eigenvalue λ associated with v from the equation
α(v) = λv. Notice that if v is an eigenvector of α, then the set of scalar
multiples of v is the line Lv(a one-dimensional subspace of V) consisting of
all scalar multiples of v, and α(x) = λx for every x in Lv. Finally, we remark
that although eigenvectors are non-zero, eigenvalues can be zero; indeed, 0
is an eigenvalue of α if and only if ker(α) ≠ {0}, and this is so if and only if
α is not injective. The next result summarizes the discussion so far.
Theorem 10.1.3 Suppose that α : V → V is linear, and I : V → V is the
identity map. Then, for any scalar λ, the following are equivalent:
(1)  λ is an eigenvalue of α;
(2)  α − λI is not injective;
(3)  ker(α − λI ) has positive dimension.
We now give three examples to show how eigenvalues arise naturally in
geometry; in all of these examples, V is 
 or 
, and the scalars are real
numbers.
Example 10.1.4 Let α : 
 → 
 be the reflection in a plane Π that
contains the origin. As α preserves the length of a vector, any eigenvalue of
α must be ±1. It is clear from the geometry that E1 = Π, and that E−1 is the
line through the origin that is normal to Π.
Example 10.1.5 Let V = 
, and let α be the rotation of angle θ about the
vertical line (in 
) through the origin. If θ ≠ 0, π then α has no eigenvalues.
If θ = 0 then α = I, the only eigenvalue of α is 1 and E1 = 
. If θ = π, then
α = −I, the only eigenvalue of α is −1 and E−1 = 
. If we view α as a map
of 
 onto itself, and if θ ≠ 0, π, then 1 is the only eigenvalue of α, and E1 is
the vertical axis. Notice that this example shows that the eigenvectors of α
need not span V.

Example 10.1.6 α : 
 → 
 be the linear map (x, y) → (x + y, y); this is
a shear. Now α acts like a non-zero horizontal translation of each horizontal
line into itself, except on the x-axis where α acts as the identity map (or the
zero translation). Thus, from the geometry, we see that the only eigenvalue
is 1, and that E1 is the real axis. It is easy to verify analytically that this is so
for we simply have to find those real λ such that (x + y, y) = λ(x, y) for some
non-zero (x, y). The only solutions are y = 0 and λ = 1.
We turn now to the question of the existence of eigenvalues and
eigenvectors. We have seen (in the examples above) that not every linear
map has eigenvalues, and that even if eigenvalues exist, there may not be
enough eigenvectors to span V. However, it is true that every linear map of
a complex vector space into itself has an eigenvalue (and corresponding
eigenvectors). This is yet another consequence of the Fundamental
Theorem of Algebra, and the proof of the existence of eigenvalues and
eigenvectors that we give here is elementary in the sense that it does not use
determinants (eigenvalues are often defined in terms of a certain
determinant, and we shall discuss this approach later).
Theorem 10.1.7 Let V be a finite-dimensional complex vector space. Then
each linear map α : V → V has an eigenvalue and an eigenvector.
Proof Let dim(V ) = n. Take any non-zero vector v in V, and consider the
n + 1 vectors υ, α(v), . . . , αn(v). As these vectors must be linearly
dependent, there are scalars a0, . . . , an, not all zero, such that
(10.1.1)
Now we cannot have a1 = · · · = an = 0 for then a0v = 0 and hence (as v ≠ 0)
a0 = 0 contrary to our assumption. It follows that there is some j with j ≥ 1
and aj ≠ 0, so that if k is the largest such j, then we can create the non-
constant polynomial P(t) = a0 + a1t + · · · + aktk, where 1 ≤ k ≤ n and ak ≠ 0.
The Fundamental Theorem of Algebra implies that there are complex
numbers λj such that

(10.1.2)
and (10.1.1) now implies that
This shows that not every map α − λj I is injective, for if they were then
their composition would also be injective and then v would be 0. Thus for
some j, α − λj I is not injective and then λj is an eigenvalue. By definition,
every eigenvalue has a corresponding eigenvector.
Another way to interpret Theorem 10.1.7 is that, in these circumstances,
V has an α-invariant subspace of dimension one. There is an analogous
result for real vector spaces, although the result is about invariant subspaces
rather than eigenvectors. However, we shall see shortly that if the
dimension of V is odd, then α has an eigenvector.
Theorem 10.1.8 Let V be a finite-dimensional real vector space. Given a
linear map α : V → V, then V has an α-invariant subspace of dimension one
or two.
Proof We repeat the proof of Theorem 10.1.7 up to (10.1.2) which must
now be replaced by
where the λj are the real eigenvalues, and the qj (t) are real quadratic
polynomials that do not factorize into a product of real linear factors. As
before, we now see that this and (10.1.1) implies that
maps v to 0, so that one of the factors is not injective. If one of the linear
factors is not injective, then α has a real eigenvalue and a real eigenvector,
and the line through this eigenvector is a one-dimensional invariant

subspace. If one of the quadratic factors, say α2 + aα + bI, is not injective,
there is a vector w such that α2(w) = −aα(w) − bw. In this case the subspace
U spanned by w and α(w) is invariant. Indeed, the general vector in U is,
say x = sw + tα(w), and α(x) = sα(w) + tα2(w) which is again in U.
The next result gives important information about eigenvalues and
eigenvectors, regardless of whether the scalars are real or complex.
Theorem 10.1.9 Let V be an n-dimensional vector space over 
, and
suppose that the linear map α : V → V has distinct eigenvalues λ1, . . . , λr
and corresponding eigenvectors υ1, . . . , υr. Then υ1, . . . , υr are linearly
independent. In particular, α has at most n eigenvalues.
Proof First, suppose that a1v1 + a2v2 = 0 for some scalars a1 and a2. As v1 is
in the kernel of α − λ1 I, we see that
As (λ2 − λ1)v2 ≠ 0 we see that a2 = 0 and, similarly, a1 = 0; thus v1, v2 are
linearly independent. Clearly, the same argument holds for any pair vi , vj.
Now suppose that a1v1 + a2v2 + a3v3 = 0. By applying α − λ1 I to both sides
of this equation we see that
As v2, v3 are linearly independent, and the λj are distinct, we see that
a2 = a3 = 0, and hence that a1 = 0. This shows that v1, v2, v3 are linearly
independent and, more generally, that vi, υj, vk are linearly independent
(with i, j, k distinct). The argument continues in this way (formally, by
induction) to show that v1, . . . , vr are linearly independent. The last
statement in Theorem 10.1.9 is clear for as dim(V) = n, we can have at most
n linearly independent vectors in V.
We have seen that the eigenvectors of a linear map α : V → V need not
span V. However, if α has n distinct eigenvalues, where dim(V) = n, then, by

Theorem 10.1.9, it has n linearly independent eigenvectors, and these must
be a basis of V. Thus we have the following result.
Corollary 10.1.10 Suppose that dim(V ) = n, and that α : V → V has n
distinct eigenvalues. Then the there exists a basis of V consisting entirely of
eigenvectors of α.
When the eigenvectors of α : V → V span V (equivalently, when there is a
basis of V consisting of eigenvectors) one can analyze the geometric action
of α in terms of the eigenvectors, and we shall do this in Section 10.3.
When the eigenvectors fail to span V, we need the following definition and
theorem (which we shall not prove).
Definition 10.1.11 Suppose that α : V → V is linear, and dim(V ) = n. A
vector v is a generalized eigenvector of α if there is some eigenvalue λ of α
such that (α − λI )n(v) = 0.
Theorem 10.1.12 Suppose that V is a finite-dimensional vector space over
, and that α : V → V is linear. Then the generalized eigenvectors of α span
V .
Exercise 10.1
1.    Let α : 
2 → 
2 be defined by α(z, w) = (z + w, 4z + w). Find all
eigenvalues and eigenvectors of α. Do the eigenvectors span 
2?
2.    Let α be the unique linear transformation of 
 into itself for which
α(e1) = 3e1 − e2 and α(e2) = 4e1 − e2. Find a linear relation between e1,
α(e1) and α2(e1), and hence show that 1 is an eigenvalue of α. Do the
eigenvectors of α span 
?
3.    Let Pn be the vector space of real polynomials of degree at most n, and
let α : Pn → Pn be defined by
      (α is differentiation). Show that α is a linear map, and that 0 is the only
eigenvalue of α. What is E0? Show that the conclusion of Theorem
10.1.12 holds in this example.

4.    Suppose that the linear map α : V → V has the property that every non-
zero vector in V is an eigenvector of α. Show that, for some constant μ,
α = μI.
5.    Let α be the linear map of 
 into itself defined by x 
 a × x, where ×
is the vector product. What are the eigenvalues, and eigenvectors of α?
Show that −||a||2 is an eigenvalue of α2.
6.    Let V be the real vector space of differentiable functions f :  → 
,
and let α : V → V be defined by α(f) = df/dx. Show that every real
number is an eigenvalue of α. Given a real number λ, what is Eλ?
7.    Suppose that V is vector space of finite dimension, and that α : V → V
is a linear map. Show that α can have at most m distinct non-zero
eigenvalues, where m is the dimension of α(V ). [Use Theorem 10.1.9.]
10.2 Eigenvalues and matrices
In this section we shall show how to find all eigenvalues and all
eigenvectors of a given linear transformation α : V → V.
Theorem 10.2.1 Suppose that a linear map α : V → V is represented by the
matrix A with respect to some basis  of V. Then λ is an eigenvalue of α if
and only if det(A − λI ) = 0.
Proof The linear map α − λI is represented by the matrix A − λI with respect
to . By definition, λ is an eigenvalue of α if and only if (α − λI)−1 fails to
exist and, from Theorem 9.4.2, this is so if and only if det(A − λI ) = 0.
Theorem 10.2.1 contains an algorithm for finding all eigenvalues and
eigenvectors of a given linear map α : V → V. First, we find a matrix
representation A for α; then we solve det(A − λI) = 0 (this is a polynomial
equation in λ of degree equal to the dimension of V ) to obtain a complete
set of distinct eigenvalues, say λ1, . . . , λr of α. Finally, for each α j, we find
the kernel of α − λj I to give the eigenspace 
 (and hence all of the
eigenvectors). It must be emphasized that the eigenvalues of α are scalars
and so they must lie in F. This means that if V is a real vector space, then the
eigenvalues of α are the real solutions of det(A − λI) = 0. On the other hand,
if V is a complex vector space, then the eigenvalues of α are the complex
solutions of det(A − λI) = 0. By the Fundamental Theorem of Algebra, this

equation will always have complex solutions, but not necessarily real
solutions, and this signals a fundamental difference between the study of
real vector spaces and complex vector spaces. Let us give an example to
illustrate these ideas.
Example 10.2.2 Let
We view A as a linear map
and the eigenvalues of α are the solutions of det(A − t I ) = 0; that is of
This equation simplifies to −3 − 5t − t2 + t3 = 0, and this has solutions −1,
−1, 3; thus the eigenvalues of α are −1 and 3. To find E−1 and E3 we have to
solve the two systems of equations
and
The first of these yields

and the first and second of these equations are the same. The solutions of
the first and third equations are (x, y, z) = t(1, 2, 1) for any real t. We
conclude that the eigenspace E−1 has dimension one, and is spanned by the
single vector (1, 2, 1), even though −1 appears as a double root of the
characteristic equation of α (see Definition 10.2.3). A similar argument
(which we omit) shows that E3 is spanned by the single vector (1, 2, 2).
Suppose now that a linear map α : V → V is represented by matrices A
and A′ with respect to the bases  and ′, respectively, of V . Then there is a
matrix P such that A′ = P−1 AP (Theorem 9.5.1), and
(10.2.1)
and this shows (algebraically) that we obtain the same eigenvalues of α
regardless of whether we use A or A′ (or  or ′). Of course, this must be so
as the eigenvalues and eigenvectors were defined geometrically, and
without reference to matrices.
Next, given an n × n matrix A, we define the polynomial 
A by
(10.2.2)

The theory of determinants implies that 
A(t) is a polynomial of degree n in
t, with leading term (−1)ntn, and it is called the characteristic polynomial of
A. Now (10.2.1) shows that two matrix representations A and A′ of a linear
map α have the same characteristic polynomial, and therefore this may be
(unambiguously) considered to be the characteristic polynomial of α.
Definition 10.2.3 The characteristic polynomial Pα(t) of α : V → V is the
polynomial det(A − tI), where A is any matrix representation of α, and
n = dim (V ). The characteristic equation for α is the equation Pα(t) = 0.
Theorem 10.2.1 shows that the eigenvalues of a map α : V → V are
precisely the zeros of the characteristic polynomial Pα, and it is here that we
begin to see a significant difference between real vector spaces and
complex vector spaces. According to the Fundamental Theorem of Algebra,
any polynomial of degree n has exactly n complex roots, and it follows
from this that if V is a complex vector space of dimension n, then every
map α : V → V has exactly n eigenvalues when we count each according to
its multiplicity as a zero of the characteristic equation. In the case of real
vector spaces we are looking for the real roots of the characteristic
polynomial and, as we well know, there need not be any. There is one case
of real vector spaces where we can say something positive. If n is odd, then
the characteristic equation of α : V → V is a real polynomial (the entries of
the matrix A are real) of odd degree n and so it has at least one real root.
Thus we have the following result.
Theorem 10.2.4 Let V be a real vector space of odd dimension. Then any
linear map α : V → V has at least one (real) eigenvalue and one line of
eigenvectors.
To be more specific, a rotation of 
 has no real eigenvalues and no
eigenvectors. However, any linear map of 
 into itself must have at least
one eigenvector, and so there must be at least one line through the origin
that is mapped into itself by α. A linear map of 
 into itself need not have
any real eigenvalues; an example of such a matrix is

and the reader can easily generalize this to 
 for any integer m.
Finally, there are two 'multiplicities' associated with any eigenvalue λ of
a linear map α, namely
(i)  the multiplicity of λ as a root of the characteristic equation, and
(ii)  the dimension of the eigenspace E(λ).
These need not be the same and, as one might expect, the two concepts play
an important role in any deeper discussion of eigenvalues and eigenvectors
of linear transformations.
Exercise 10.2
1.  Let A be an n × n complex matrix and suppose that for some integer m,
Am is the zero matrix. Show that zero is the only eigenvalue of A.
2.  Let
and let A define a linear map α : 
 → 
. Show that the eigenvalues of
α are 1, 2 and 4. In each case find a corresponding eigenvector.
3.  Let
and let α : 
3 → 
3 be the corresponding map. Show that the complex
eigenvalues of α are 1 and 2 ± 3i, and find the associated eigenspaces.
4.  Let A be a real 3 × 3 matrix, and let α : 
3 → 
3 be the corresponding
map. Suppose that α has eigenvalues λ, μ and , where λ is real but μ is

not. Let v be an eigenvector corresponding to μ; show that  is an
eigenvector corresponding to .
  Now suppose that v = v1 + iυ2, where v1 and v2 are real vectors. Show
that if we now view A as defining a map α of 
 into itself, then α leaves
the subspace spanned by v1 and v2 invariant. Illustrate this by taking A to
be the matrix in Exercise 2. In this case the invariant plane is
2x − 2y + z = 0, and you can verify this directly.
5.  Construct a linear map of 
 into itself that has exactly two real
eigenvalues.
6.  (i)  Let α : 
2 → 
2 be the linear map defined by α(e1) = e2 and
α(e2) = e1. Find the eigenvalues of α.
(ii)  Let α : 
3 → 
3 be the linear map defined by α(e1) = e2, α(e2) = e3 and
α(e3) = e1. Find the eigenvalues of α.
(iii) Generalise (i) and (ii) to a map α : 
n → 
n.
(iv) Let ρ be the permutation of {1, . . . , 9} be given by
and let α : 
9 → 
9 be the linear map defined by by α(ej ) = eρ(j), j = 1,
. . . , 9. What are the eignevalues of α?
7.  Let A be the n × n matrix (aij), where aij is 1 if i ≠ j and 0 if i = j. Show
that −1 and n − 1 are eigenvalues of A and find the dimension of the
corresponding eigenspaces. [It is not necessary to evaluate any
determinant.]
10.3 Diagonalizable matrices
Suppose that A is a square matrix. How can we compute the successive
powers A, A2, A3, . . . of A? This question is easily answered for diagonal
matrices. A matrix D = (dij) is a diagonal matrix if dij = 0 whenever i ≠ j;
that is, if

(10.3.1)
It is obvious that
and although this is not very exciting it does lead to a more useful result.
For this we need the concept of a diagonalizable matrix.
Definition 10.3.1 An n × n matrix A is diagonalizable if there is a non-
singular matrix X, and a diagonal matrix D, such that X−1 AX = D or,
equivalently, A = XDX−1.
It is easy to compute the powers of a diagonalizable matrix. If A
diagonalizable with X and D as in Definition 10.3.1, then
and so providing that we can find X and D we can easily compute Ak. An
example will illustrate this idea.
Example 10.3.2 Suppose that
(10.3.2)
We shall show how to find X shortly, but with

we have
say, and hence
In order to use this technique we need a criterion for A to be
diagonalizable and, when it is, we need to know how to find the matrices X
and D. Unfortunately, not every square matrix is diagonalizable as the next
example shows.
Example 10.3.3 The matrix
(10.3.3)
is not diagonalizable. To see this, suppose that B is diagonalizable; then
there is a non-singular matrix X and a diagonal matrix D, such that X
B = DX; say
This implies that λa = 0 and λb = a. As X−1 exists, ad − bc = 0. Thus λ = 0,
and similarly, μ = 0, so that D = 0, and hence B = X−1 DX = 0 which is false.
We deduce that B is not diagonalizable.
The following result gives us a condition for A to be diagonalizable, and
it also tells us how to find X and D.

Theorem 10.3.4 A real n × n matrix A is diagonalizable if and only if
>
has a basis of eigenvectors of A. Moreover, if A = XDX−1, where D is a
diagonal matrix, then the diagonal elements of D are the eigenvalues of A
and the columns of X are the corresponding eigenvectors of A. A similar
statement holds for complex matrices and 
n.
The reader should now consider the matrix A in (10.3.2) and verify that the
columns of X used in that example are eigenvectors of A which span 
.
Likewise, the reader should verify that the eigenvectors of B in (10.3.3) do
not span 
.
Before giving the proof of Theorem 10.3.4, we shall illustrate its use by
explaining the standard method of solution of homogeneous second-order
difference equations.
Example 10.3.5 The difference equations
where a0 and a1 are given values, can be written as
Thus
(10.3.4)
Theorem 10.3.4 suggests that we find the eigenvalues and eigenvectors of
A, and a calculation shows that
Thus

and so
(10.3.5)
If we combine this with (10.3.4) we can calculate an+1, but in fact, one does
not normally carry out this computation. Once we know that the
eigenvalues of A are 2 and 3, it is immediate from (10.3.4) and (10.3.5) that
we must have an = α2n + β3n for some constants α and β. As a0 = α + β and
a1 = 2α + 3β, we can now find α and β in terms of the given a0 and a1 and
hence obtain an explicit formula for an.
The proof of Theorem 10.3.4 The matrix A acts on
> as a linear
transformation α, and the matrix of α with respect to the standard basis {e1,
. . . , en} is A. First, suppose that v1, . . . , υr is a basis of eigenvectors of A.
Then the matrix of α with respect to this basis is a diagonal matrix D, and
by Theorem 9.5.1 (which shows how a matrix changes under a change of
basis) there is some matrix X with A = X D X−1. Thus A is diagonalizable.
Next, suppose that A is diagonalizable. Then there are matrices X and D
(diagonal) such that A = X D X−1. Thus AX = X D and so matrix
multiplication shows that the columns X1, . . . , Xn of X are eigenvectors of
A; indeed,

We shall show that the vectors X1, . . . , Xn are a basis of
> and for this, it
is sufficient to show that they are linearly independent. However, as the
matrix X−1 exists (by assumption), we see that det(X) = 0 so the columns X1,
. . . , Xn of X must be linearly independent.
Next, suppose that D is given by (10.3.1). Then the characteristic
equation for D is
(10.3.6)
However, X AX−1 = D, so that A and D have the same characteristic
polynomial. We conclude that the diagonal elements of D are the
eigenvalues of A.
Finally, recall Theorem 10.1.9 that if λ1, . . . , λr are distinct eigenvalues
of a matrix A with corresponding eigenvectors v1, . . . , vr, then these vectors
are linearly independent. It follows that if the n × n matrix A has n distinct
eigenvalues, then the corresponding eigenvectors v1, . . . , υn form a linearly
independent set of n vectors in an n-dimensional space and so are a basis of
that space. Combining this with Theorem 10.3.4, we obtain the following
sufficient condition for a matrix to be diagonalizable.
Theorem 10.3.6 If a real n × n matrix A has n distinct real eigenvalues,
then it is diagonalizable. If a complex n × n matrix A has n distinct complex

eigenvalues, then it is diagonalizable.
This condition is sufficient, but not necessary; for example, the identity
matrix is diagonal, and hence diagonalizable, but it does not have distinct
eigenvalues.
We end with an example to illustrate Theorem 10.3.6.
Example 10.3.7 Let
A calculation (which the reader should do) shows that A has eigenvalues 2,
1 + i and 2 − 2i. As these are distinct, it follows that the map
of 
3,t into itself has a three eigenvectors that form a basis of 
3,t . These
eigenvectors are w1 = (1, 0, 1)t for the eigenvalue 2, w2 = (2, −1, 0)t for the
eigenvalue 1 + i, and w3 = (1, 0, −1)t for the eigenvalue 2 − 2i. Finally, the
matrix of α with respect to the basis {w1, w2, w3} is
for obviously this maps the coordinates (λ, μ, ν)t of w = λw1 + μw2 + νw3 to
the coordinates (2λ, (1 + i)μ, (2 − 2i)v)t of α(w).
Exercise 10.3
1.  Find the eigenvalues of the linear transformation

(they are all small positive integers). Show that 
 has a basis of
eigenvectors of α even though α does not have three distinct eigenvalues.
Show also that the plane x + 2y + z = 0 is α-invariant.
2.  Is the matrix
diagonalizable?
3.  Is the matrix
diagonalizable? Regard A as acting as a linear map from 
 to itself in
the usual way, and write A = I + B, where I is the 3 × 3 identity matrix.
Try to describe the action of A in geometric terms.
4.  Show that the matrix
is not diagonalizable.
10.4 The Cayley-Hamilton theorem
Having understood how to compute powers of diagonalizable matrices, we
now consider polynomials of a matrix. Given an n × n matrix A, and a real
or complex polynomial

we define the n × n matrix p(A) by
We are accustomed to trying to find the zeros of a given polynomial but
there is little prospect of us being able to solve the equation p(A) = 0, where
0 is the n × n zero matrix. We can, however, reverse the process and,
starting with the matrix A, try to find a non-zero polynomial p for which
p(A) = 0. It is not immediately obvious that any such p exists, but we have
seen in Section 7.10 that there is at least one such p (of degree at most n2).
Again, we start our investigation with diagonal matrices, then consider
diagonalizable matrices and finally (though we shall not give all of the
details) general matrices. First, for any polynomial p, and any diagonal
matrix D with D = (dij ), we have
Let us now denote the characteristic polynomial of a matrix A by pA. As the
diagonal entries of the diagonal matrix D are precisely its eigenvalues, we
see that for each i, pD(dii ) = 0, and it follows immediately that pD(D) = 0;
that is, the diagonal matrix D satisfies its own characteristic equation. This
is trivial, but it is a start and it suggests what might be true for non-diagonal
matrices. In fact, it is true for all matrices and this is the celebrated
Cayley-Hamilton theorem. Every complex square matrix satisfies its own
characteristic equation.
We have verified the conclusion for a diagonal matrix, and it is easy to
extend this proof so as to include all diagonalizable matrices. We shall then
indicate briefly how to extend the proof still further so as to include all
matrices.
To prove the result for a diagonalizable matrix A, let A = X D X−1, where
D is a diagonal matrix. Then, for any integer k,

Now A and D have the same characteristic polynomial, say p(z) = a0 + · ·
· + anzn, and p(D) = 0. We want to show that p(A) = 0, and this is so because
We have now shown that any diagonalizable matrix A satisfies its own
characteristic equation.
We now sketch the proof for the general matrix. First, if A has n distinct
eigenvalues then, by Theorem 10.3.6, it is diagonalizable and so it satisfies
its own characteristic equation. The remaining cases are those in which A
has a repeated eigenvalue, and this happens if and only if the coefficients of
A satisfy some (known) algebraic relation (see Theorem 9.6.5). Perturbing
the coefficients appropriately destroys the validity of this relation, and this
means that every matrix A is the limit of a sequence Aj of diagonalizable
matrices. Let pj be the characteristic polynomial of Aj, so that pj (Aj ) = 0.
As the coefficients of pj depend continuously on the coefficients of Aj, we
see that pj → p; thus
as required. There is much to be done to validate these steps, but the general
plan should be clear. There are, of course, purely algebraic proofs of the
Cayley-Hamilton theorem; our proof is based on the idea that as the

conclusion is true for 'almost all' matrices, it can be obtained for the
remaining matrices by taking limits.
The Cayley-Hamilton theorem provides a useful method for computing
the powers, and the inverse of a matrix, especially when the matrix is not
too large. We consider the inverse first. Suppose that A is an invertible n × n
matrix, with characteristic polynomial a0 + · · · + an−1zn−1 + zn; then
If a0 = 0 we can multiply both sides of this by A−1, so that the new equation
has constant term a1. This process can be repeated until the constant term is
non-zero, so we may assume now that a0 = 0. Then
and this gives a way of calculating A−1 simply from the positive powers of
A. As an example, if
then
and the characteristic equation of A is t3 − 4t2 + 6t − 5. Thus
We now illustrate how to use the Cayley-Hamilton theorem to compute
powers of a matrix. Consider, for example, the matrix A given in (10.3.2).

Its characteristic polynomial is p(t) = t2 − 5t + 6 and so
and so on. The reader may care to check this against the result obtained (by
essentially the same method) in Example 10.3.2.
Exercise 10.4
1.  Show that the matrix
satisfies its own characteristic equation. Use this to compute A4.
2.  The space M2×2 of real 2 × 2 matrices has dimension four. Use the
Cayley-Hamilton theorem to show that if A is any 2 × 2 matrix, then all
of the matrices A, A2, A3, . . . lie in the two-dimensional subspace of
M2×2 spanned by I and A.
3.  Let
where ∗ denotes an unspecified entry of A. Show that if a + b + c = 0
and 1/a + 1/b + 1/c = 0, then A3 = abcI .
4.  Let a × b be the standard vector product on 
, and define the linear map
α : 
 → 
 by α(x) = a × x, where a is a given vector of unit length.
Show that the matrix of α with respect to the standard basis is

Find the characteristic equation of α, and verify that the conclusion of
the Cayley-Hamilton theorem holds in the case. Derive the same result
by vector methods. Deduce that for all x,
5.  Let F : 
 → 
 be any map and, given a point (x0, y0) in 
 define
x n and yn by (xn+1, yn+1) = F(xn, yn). We study the dynamics of the map
F by studying the limiting behaviour of the sequence (xn, yn) as n → ∞
for different choices of the starting point (x0, y0). What is the limiting
behaviour of (xn, yn) when F is defined by F(x, y) = (x′, y′), where
Sketch some sequences (xn, yn) for a few different starting points. [Find
the eigenvalues of the matrix. The lines y = x and x + 2y = 0 should
figure prominently in your solution.]
10.5 Invariant planes
In this section we give some examples to illustrate the ideas discussed
earlier in the chapter (and in these the reader is expected to provide some of
the details).
Example 10.5.1 Consider the mapping α of 
 into itself given by
The characteristic equation of α is t3 − 5t2 + 17t − 13 = 0, so the eigenvalues
are 1 and 2 ± 3i. It is easy to see that (x, y, z)t an eigenvector corresponding
to the eigenvalue 1 if and only if it is a scalar multiple of the vector (1, 2,
1)t, so the eigenspace corresponding to the eigenvalue 1 is a line.
Let us now consider α to be acting on C3,t, and seek the eigenvectors in
this space. A straightforward calculation shows that (3 − 3i, 5 − 3i, 4)t is an
eigenvector for 2 + 3i, and that (as the entries in the matrix are real) (3 + 3i,

5 + 3i, 4)t is an eigenvector for 2 − 3i. Let us write these eigenvectors as
p + iq and p − iq, respectively, where p and q are real column vectors. Then
α( p + iq) = (2 + 3i)( p + iq), so that α( p) = 2 p − 3q and A(q) = 3 p + 2q. Of
course this can be verified directly (once one knows which vectors to look
at). It follows that the real subspace of R3,t spanned by p and q is invariant
under α. Of course, this is the plane whose normal is orthogonal to both p
and q. Now pt = (3, 5, 4) and qt = (−3, −3, 0) so that (using the vector
product in 
 to compute this normal), we see that plane given by
2x − 2y + z = 0 is invariant under A. This can now be checked directly for if
we write
then (as is easily checked) 2x′ − 2y′ + z′ = 2x − 2y + z. Finally, the vectors
form a basis of 
, and the matrix of α with respect to this basis (in this
order) is
Here, the single diagonal element corresponds to the single line of
eigenvectors, and the 'obvious' 2 × 2 part of the matrix represents the
action of A on the invariant plane.
Example 10.5.2 Let α be the map

Then the characteristic equation of α is (t − 1)3 = 0, so 1 is the only
eigenvalue of α. Moreover, (x, y, z)t is an eigenvector if and only if it is a
scalar multiple of the vector (1, 1, 1)t. We now ask whether α has an
invariant plane (containing the origin) or not. The plane px + qy + r z = 0 is
mapped to the plane (3 p + q)x′ + (r − 3 p)y′ + pz′ = 0 and these planes will
coincide if their normals are in the same direction; that is, if there is a scalar
λ such that λ( p, q, r) = (3 p + q, r − 3 p, p). A solution of this is λ = 1, and (
p, q, r) = (1, −2, 1) so we conclude that the plane x − 2y + z = 0 is invariant
under α. This can be confirmed by noting that x − 2y + z = x′ − 2y′ + z′.
Example 10.5.3 Let α be the map
This has eigenvalues 1, 0 and 0, where the eigenspace E1 is the line through
(−12, 4, 1)t, and the eigenspace E0 is the line through (3, −1, 0)t. We can
analyse the action of α in more detail if we note that with the notation
we have α(e1) = u, α(e2) = 3u, α(e3) = v = 4u + e3, and α(u) = 0, α(v) = v.
Thus, for example, the plane containing u and v (namely x + 3y = 0), and
also the plane containing e1 and e2 (namely z = 0), is invariant under α.
Example 10.5.4 Consider the mapping α of 
 into itself given by

The characteristic equation of α is (t − 1)(t − 2)2 = 0, and it is easily seen
that E1 is the line through (1, 0, 2)t, while E2 is the line through (1, 1, 2)t.
Let A be the matrix in the definition of α. Then, by the Cayley-Hamilton
theorem, (A − I )(A2 − 4A + 4I ) = 0. If A2 − 4A + 4I is non-singular, this
would imply that A − I = 0 which is false. It follows that there is some non-
zero vector v such that (A2 − 4A + 4I )(v) = 0, and hence that
A2(v) = 4A(v) − 4v. This implies that the plane spanned by v and A(v) is
invariant under α, so we need to find a suitable v. Now we can take any v in
the kernel of A2 − 4A + 4I, and as
and it is easy to check that the kernel of A2 − 4A + 4I is the plane given by
x = y. It follows that we can take v = (1, 1, 0)t, and then α(v) = (4, 4, 4)t .
The normal to the two vectors v and α(v) can be found by computing their
vector product (consider them as row vectors), and we find that α leaves the
plane x − y = 0 invariant. To check this directly, observe that x′ − y′ = x − y;
thus if x = y then x′ = y′.
Exercise 10.5
1.   Find an invariant line, and an invariant plane, for the map α given by
2.  Let α be the linear map

Find the (complex) eigenvalues of α and show that α has an invariant
plane that does not contain any (real) eigenvector of α. By considering
the complex eigenvectors, deduce that α4 is the identity map.
3.  Find an invariant plane for the linear map α given by

11
Linear maps of Euclidean space
11.1 Distance in Euclidean space
We recall the standard basis e1, . . . , en of 
. If x = ∑ j xj e j, and similarly
for y, we write
and x ⊥ y when x·y = 0. The distance ||x − y|| between the points x and y is
given by the natural extension of Pythagoras' theorem, and it is important to
know that it satisfies the triangle inequality: for all x, y and z in 
,
(11.1.1)
To prove this it is sufficient to show that |x·y| ≤ ||x|| ||y|| for this leads to
||x + y|| ≤ ||x|| + ||y||, and then a change of variables gives the triangle
inequality exactly as in the case of 
. It is sufficient, therefore, to prove
the following important inequality.
Theorem 11.1.1: the Cauchy-Schwarz inequality For all x and y,
(11.1.2)
Further, equality holds if and only if ||x||y = ±||y||x.

Proof Let x = (x1, . . . , xn) and y = (y1, . . . , yn). Now (11.1.2) is true when
x = 0, and also when y = 0; thus we may assume that ||x|| > 0 and ||y|| > 0.
Now simple algebra gives
so that x·y ≤ ||x|| ||y||. If we replace x by −x, we see that −x·y ≤ ||x|| ||y||, and
(11.1.2) follows. Finally, equality holds in (11.1.2) if and only if x·y = ±||x||
||y||. However, the argument above shows that x·y = ||x|| ||y|| if and only if
||x||y = ||y||x, and a similar argument shows that x·y = −||x|| ||y|| if and only if
||x||y = −||y||x. Thus equality holds in (11.1.2) if and only if ||x||y = ±||y||x.
The Cauchy-Schwarz inequality implies that, for any vectors x and y in 
, there is some unique real number β in [0, π] that satisfies x·y = ||x|| ||y||
cos β. As the Cauchy-Schwarz inequality has been established without the
use of geometry, we can now use this equation to define the angle β
between the segments [0, x] and [0, y] in 
. For example, the angle β
between these two segments in 
 when x = (1, 1, 1, 1) and y = (1, 2, 2, 0)
is given by cos = 5/6.
Exercise 11.1
1.  Let z1, . . . , zn, w1, . . . , wn be complex numbers. Show that
This is the Cauchy-Schwarz inequality for complex numbers.
2.  The unit cube in 
 has vertices (ε1, . . . , εn), where ε1, . . . , εn = 0, 1.
What is the angle θn between the segments [0, e1] and [0, e1 + · · · + en]?
How does θn behave as n → ∞?
3.  Consider the cube in 
 with centre 0 and vertices (a1, . . . , an), where
aj = ±1. Show that if n is even then the cube has two diagonals that are
orthogonal to each other (a diagonal of the cube is a line through 0 and a
vertex). Show that no such diagonals exist when n is odd.

11.2 Orthogonal maps
The most useful feature of the basis e1, . . . , en is that it is a basis of
mutually orthogonal unit vectors; that is, for all i, ||ei || = 1, and for all i and
j with i ≠ j, ei ⊥ e j. This property is so important that we enshrine it in a
definition.
Definition 11.2.1 A basis v1, . . . , vn of 
 is an orthonormal basis if ||v j
|| = 1 for every j, and vi ⊥ v j whenever i ≠ j.
The following lemma is useful.
Lemma 11.2.2 If v1, . . . , vn are mutually orthogonal unit vectors in 
then they form an orthonormal basis of 
.
Proof We must show that the vj are linearly independent, so suppose that ∑ j
λ j v j = 0. If we take the scalar product with each side of the equation with
vk, we find that λk = 0. As this holds for all k, the conclusion follows.
It makes sense to speak of an orthonormal basis of a subspace of 
, and
the next result guarantees that these always exist.
Theorem 11.2.3 Any subspace of 
 has an orthonormal basis.
Proof Let W be a non-trivial subspace of 
, and let w1, . . . , wk be any
basis of W. By replacing w1 by a scalar multiple of itself we may suppose
that ||w1|| = 1. Now let 
 = w2 − (w1·w2)ω1, and note that w1, 
, w3, . . . ,
wk is a basis of W with ||w1|| = 1 and w1 ⊥ 
. This orthogonality is
preserved if we replace 
 by a scalar multiple of itself; thus, after
relabelling, we may now assume that ||w1|| = ||w2|| = 1 and w1 ⊥ w2. Now let
 = w3 − (w1·w3)w1 − (w2·w3)w2. It is clear that w1, w2, 
, w4, . . . , wk is a
basis of W, and that w1, w2 and 
 are orthogonal to each other. Again, we
may replace 
 by a scalar multiple of itself and so assume (after
relabelling) that w1, w2 and w3 are now mutually orthogonal unit vectors.
The process can clearly be continued until it produces an orthonormal basis
for W .

We now define orthogonal maps, and give equivalent definitions.
Definition 11.2.4 A linear map α : 
 → 
 is an orthogonal map if α(e1),
. . . , α(en) is an orthonormal basis of 
.
Theorem 11.2.5 Let α : 
 → 
 be a linear map. Then the following are
equivalent:
(1)  α is an orthogonal map;
(2)  α preserves scalar products (for all x and y, α(x)·α(y) = x·y);
(3)  α preserves lengths of vectors (for all x, ||α(x)|| = ||x||);
(4)  if v1, . . . , vn is an orthonormal basis, then so is α(v1), . . . , α(vn).
Proof Assume that (1) holds. As α is linear, α(x) = α(∑ j x j e j) = ∑ j x j α(e j
), and similarly for y. As α(e1), . . . , α(en) is an orthonormal ba-sis, we see
that
thus (2) holds. Clearly (2) implies (3) for ||x||2 = x·x and similarly for α(x).
Conversely, if (3) holds, then the identity
and the corresponding identity for α(x) and α(y), shows that (2) holds. Thus
(2) and (3) are equivalent. Next, if (3) holds then both (2) and (3) hold, and
(4) follows from these. Finally, if (4) holds we let v j = e j and then (1)
follows.
We can describe orthogonal maps in terms of their matrices.
Theorem 11.2.6 Let α : 
 → 
 be a linear map, and let A be the matrix
representation of α with respect to the standard basis e1, . . . , en of 
.

Then the following are equivalent:
(1)  α is an orthogonal map;
(2)  the columns of A form an orthonormal basis of (
)t ;
(3)  At A = I ;
(4)  A At = I ;
(5)  the rows of A form an orthonormal basis of 
.
Proof Let c j be the column vectors of A. As α(e j ) = c jt, it is immediate
that (1) and (2) are equivalent. Next, the definition of a matrix product
shows that (2) is equivalent to (3), and that (4) is equivalent to (5). Finally,
(3) and (4) are equivalent by Theorem 9.4.4.
Theorem 11.2.6 suggests the following definition.
Definition 11.2.7 A real square matrix A is orthogonal if it is non-singular
(that is, invertible), and A−1 = At .
Example 11.2.8 We show that a 2 × 2 orthogonal matrix is the matrix of
either a rotation or a reflection in 
. Suppose that
is orthogonal. Then, by Theorem 11.2.6,
We may choose θ such that a = cos θ and c = sin θ, and then it is easy to see
that A is one of the matrices
These matrices represent a rotation of angle θ, and the reflection in the line
y cos(θ/2) = x sin(θ/2), respectively.

We shall need the next result in the next section.
Lemma 11.2.9 Let  = {u1, . . . , un} and ′ = {v1, . . . , vn} be orthonormal
bases of 
, and suppose that the identity map I : 
 has matrix A.
Then A is an orthogonal matrix.
Proof In general, a map α : 
 has matrix A = (ai j ), where
If α = I then u j = ∑ k ak j vk and, as
this implies that
As this sum is the scalar product of the i-th and j-th columns of A, we see
from Theorem 11.2.6 that A is orthogonal.
The next result gives information about the determinant and eigenvalues
of an orthogonal matrix.
Theorem 11.2.10 Suppose that A is a real n × n orthogonal matrix. Then
(1)  det(A) = ±1;
(2)  any (complex) eigenvalue λ of A satisfies |λ| = 1;
(3)  if n is odd, then 1 or −1 is an eigenvalue of A;
(4)  if n is even, then A need not have any real eigenvalues.
Proof As AAt = I, det(AB) = det(A) det(B) and det(At ) = det(A), (1) follows.
To prove (2), we note that the orthogonal matrix A defines a linear map α of
the complex space 
 of column vectors into itself by α(z) = Az. Now let λ

be any eigenvalue of α, and let v be a corresponding (non-zero) eigenvector;
thus Av = λv. For any complex column vector z, we let  be the vector
formed by taking the complex conjugate of each of its elements. As A is a
real matrix, this gives 
. Thus
Now 
, where v = (v1, . . . , vn)t , and this is non-zero (as
every eigenvector is non-zero). Thus |λ| = 1 and (2) follows. Now (3)
follows immediately because if n is odd, then the characteristic equation for
A is a real equation of odd degree and so A has at least one real eigenvalue
which, by (2), has unit modulus. Finally, the matrix
is an orthogonal 4 × 4 matrix with no real eigenvalue. A similar matrix
exists whenever n is even and this proves (4).
It is clear from Theorem 11.2.6 that if A and B are orthogonal matrices,
then so too are AB and A−1; indeed, (AB)t (AB) = Bt (At A)B = Bt B = I, and
(A−1)t A−1 = (A−1)t At = (AA−1)t = I. Thus we have the next result (which
justifies the definition that follows it).
Theorem 11.2.11 The class of n × n orthogonal matrices is a group under
matrix multiplication.
Definition 11.2.12 The group of real orthogonal n × n matrices is called the
orthogonal group, and it is denoted by O(n). The subgroup of matrices with
determinant +1 (see Theorem 11.2.10) is called the special orthogonal
group, and is denoted by SO(n).
Example 11.2.13 Let A be a 3 × 3 orthogonal matrix, and let α be the linear
map of 
 into itself whose matrix is A relative to the standard basis. By
Theorem 11.2.10, α has an eigenvector v corresponding to an eigenvalue of

±1. As α preserves scalar products, the plane Π orthogonal to v is mapped
into itself by α. As α preserves lengths and scalar products, its action on the
plane Π is orthogonal, and so it either acts as a rotation of Π, or as the
reflection across a line in Π (see Example 11.2.8). It follows that the action
of α relative to the basis v1, v2, v3 (where v1 = v, and v2, v3 is a basis of Π) is
of one of the forms
The ideas in Example 11.2.13 can be generalized to higher dimensions.
Theorem 11.2.14 Let A be a real n × n orthogonal matrix. Then there exists
a real orthogonal matrix Q such that (with the obvious meaning)
where all of the unspecified entries are zero, and where Im denotes the unit
m × m matrix.
The significance of Theorem 11.2.14 is that given an orthogonal matrix A
acting on 
, we can decompose 
 into a number of two-dimensional
subspaces, each of which is invariant by A, and on each of which A acts as a
rotation or a reflection, and two other subspaces on which A acts as I and
−I, respectively. Moreover, any two of these subspaces are orthogonal to
each other.
Briefly, the proof is as follows. We know that any real linear map (given by
the matrix A) of a real vector space into itself has an invariant subspace, say
U, of dimension one or two (Theorem 10.1.8). As A preserves scalar
products, the set of vectors that are orthogonal to all vectors in U is also a
subspace that is invariant under A, and as A preserves lengths, the action of
A that takes U into itself is also orthogonal. We can continue in this way to

obtain the desired decomposition of the whole space into mutually
orthogonal subspaces of dimension one or two, and the result now follows
from the ideas discussed above.
Exercise 11.2
1.  Find an orthonormal basis of 
 that contains the vector (1, 4, 8).
2.  Show that the vectors (1, 0, 1, 0), (1, 1, −1, 0) and (1, −2, −1, 1) in 
are mutually orthogonal. Find a fourth vector that is orthogonal to each
of these, and hence find an orthonormal basis of 
 that contains a scalar
multiple of each of these vectors.
3.  Show that if there is an orthonormal basis of 
 that consists of
eigenvectors of both of the n × n matrices A and B, then AB = B A.
4.  Show that for suitable p, q, r, s the matrix
is orthogonal.
5.  Show that if B is a square invertible matrix then (Bt )−1 = (B−1)t. Now
suppose that A is a square matrix, and that I + A is invertible. Show
(a)  if A is orthogonal then (I − A)(I + A)−1 is skew-symmetric;
(b)  if A is skew-symmetric then (I − A)(I + A)−1 is orthogonal.
11.3 Isometries of Euclidean n-space
We shall now examine the group of isometries of 
.
Definition 11.3.1 A map f : 
 → 
 is an isometry if it preserves
distances; that is, if || f (x) − f (y)|| = ||x − y|| for every x and y in 
.
Each translation is an isometry, and if A is an n × n orthogonal matrix
then g defined by g(x) = x A is an isometry (note: we must write x A for a
row vector x, and Ax for a column vector x). Indeed, g is linear, and it
preserves lengths as

It follows that each map of the form x 
 x A + a, where A is orthogonal, is
an isometry, and we shall now show that every isometry is of this form.
Theorem 11.3.2 Every isometry of 
 is of the form x 
 a + x A for some a
in 
 and some orthogonal matrix A. In particular, f is an isometry with f
(0) = 0 if and only if f (x) = x A for some orthogonal matrix A.
Proof Let f be any isometry, and let g(x) = f (x) − f (0); then it suffices to
show that g(x) = x A for some orthogonal matrix A. Now g is clearly an
isometry (it is the composition of the isometry f with a translation), and
g(0) = 0. Thus, for all x and y,
that is, g preserves the length of each vector. Also, as g is an isometry,
||g(x) − g(y)|| = ||x − y||, and as
we see that g(x)·g(y) = x·y; that is, g preserves scalar products. Thus, by
Lemma 11.2.2, the vectors g(e1), . . . , g(en) form an orthonormal basis of 
. Now write
As g preserves scalar products, for each k, xk = x·ek = g(x)·g(ek) = 
 , and
hence
This implies that g is a linear map and so, from Theorems 11.2.5 and 11.2.6,
g(x) = x A for some orthogonal matrix A.

We have already seen that each isometry of 
 is the composition of at
most four reflections (Theorem 6.1.4), and we shall now extend this by
showing that every isometry of 
 is a composition of at most n + 1
reflections.
Theorem 11.3.3 Every isometry of 
 can be written as the composition of
at most n + 1 reflections.
We must comment on the idea of a reflection in 
. In Section 8.1 we
defined a hyperplane to be an (n − 1)-dimensional subspace of 
 and,
consequently, each hyperplane was given by an equation of the form
x·a = 0. However, we now want to discuss 'hyperplanes' that do not contain
the origin, and so we must now modify the earlier definition. In this
discussion a hyperplane will be the set of vectors x that satisfy x·a = d,
where a ∈ 
 and d ∈ 
. The ideas in Chapter 3 generalize immediately,
and the reflection across the hyperplane with equation x·a = d, where
||a|| = 1, is defined to be the map R(x) = x + 2 (d − (x·a)) a.
The proof of Theorem 11.3.3 It is sufficient to show that if f is an isometry
with f (0) = 0, then f can be expressed as the composition of at most n
reflections, and we shall prove this by induction on n. First, we know the
result to be true when n is 1, 2 or 3. We now suppose that the conclusion
holds for any isometry of 
, where k = 1, . . . , n − 1, and we consider an
isometry f of 
 that fixes the origin. As || f (en)|| = || f (en) − f
(0)|| = ||en − 0|| = 1, we see that there is a reflection, say R, in some
hyperplane through the origin so that R f is an isometry that fixes both 0 and
en. Now let g = R f. It is easy to see that g fixes every point of the line L
through 0 and en (consider a point (0, . . . , 0, t) and require that its image is
a distance |t| from 0, and |t − 1| from en). Next, let
Then, as W is the set of points that are orthogonal to L, and as g preserves
scalar products, we see that g maps W into itself. More generally, if g(x) = y,
then yn = xn (because ||y|| = ||x||, and ||y − ten||2 = ||x − ten||2 for every real t).
Now we can regard g as a map of the (n − 1)-dimensional space W into
itself; formally, if g(x1, . . . , xn) = (y1, . . . , yn), then yn = xn, and we can

define g∗ : 
 → 
 by
We can identify W with 
 and, by the induction hypothesis, we can now
write g∗ = 
, where each 
 is a reflection across some hyperplane
in 
 that contains the origin, and k ≤ n − 1. Now each R∗j is given by
some equation
where x∗ = (x1, . . . , xn−1), and 
 = (a1, . . . , an−1) with ||
|| = 1. This
means that we can define a reflection Rj, acting on 
, by Rj (x) = x − 2(aj
·x)aj, where aj = (a1, . . . , an−1, 0). It is clear that each R j preserves the n-th
coordinate of any vector in 
, and hence that the first n − 1 co-ordinates of
R1 · · · Rk(x1, . . . , xn) agree with the corresponding coordinates of 
(x1, . . . , xn−1). It follows that g = R1 · · · Rk, and hence that f = R R1 · · · Rk
as required.
Theorem 11.3.3 enables us to divide the isometries of 
 into two
classes, namely the direct and indirect isometries. We say that the isometry
x 
 a + x A is a direct isometry if det(A) = 1, and it is an indirect isometry
if det(A) = −1 (recall that as A is orthogonal, det(A) = ±1).
Exercise 11.3
1.  Find an isometry of  (= 
) that requires three reflections in the sense
of Theorem 11.3.3 (that is, that cannot be written as the product of one
or two reflections).
2.  Show that every rotation of 
 can be written as the product of two
rotations of order two. [Hint: a rotation of 
 is a product of two
reflections.] Is a similar statement true in 
?
11.4 Symmetric matrices
The study of homogeneous quadratic forms, that is, expressions of the form
∑ i, j ai j xi x j , is important because these forms arise naturally in many

parts of mathematics. If we make a linear change of variables, then the new
form will have different coefficients, and may be simpler to analyse. For
example, if u  = (x + y)/
 and v  = (x − y)/
, then x2 + 4x
y + y2 = 3u2 − v2, and it is now obvious that the form x2 + 4x y + y2 can take
negative as well as positive values. Our aim in this section is to show that
any real quadratic form can, by a suitable change of variables, be
transformed into a form of the type 
. In order to achieve this we
shall need to use orthogonal matrices, and orthonormal bases of
eigenvectors, and we begin with a simple example to illustrate why these
may be relevant.
Example 11.4.1 We can express the quadratic form x2 + 4x y + y2 in terms
of a matrix in many ways, namely
where s + t = 4. Let A be the matrix in this expression. Then A has real,
distinct eigenvalues if and only if st > 0, so let us assume that this is so.
Then the eigenvalues of A are 1 + 
 and 1 − 
, and the corresponding
eigenvectors are (
, 
) and (
,− 
), respectively. Now these
eigenvectors are orthogonal if and only if s = t, so that (in this example, at
least), the eigenvectors exist and are orthogonal if and only if A is
symmetric.
There is another simple argument that suggests that symmetry and
orthogonality might be related. Suppose we are given a linear map α : 
 → 
 that is represented by a symmetric matrix A with respect to some
basis. If we now change this basis in at attempt to find a diagonal matrix
that represents α, the new matrix will be, say, PAP−1. Now, in general,
PAP−1 will not be symmetric, and so certainly not diagonal. It seems
reasonable, then, to ask which matrices X at least have the property that
XAX−1 is symmetric whenever A is symmetric. In fact, this is true for all
orthogonal matrices X, and the proof is easy. If A is symmetric and X is
orthogonal, then

so that XAX−1 is symmetric. We record this for future use.
Lemma 11.4.2 If A is symmetric and X is orthogonal, then X AX−1 is also
symmetric.
It is usually better to think about linear maps rather than matrices, so we
need to understand which properties of linear maps are related to symmetric
matrices. We shall now define what it means for a linear map to be
symmetric, and then relate this to symmetric matrices.
Definition 11.4.3 A linear map α : 
 → 
 is symmetric if and only if
given any orthonormal basis 
 of 
 , the matrix representation of α
relative to  is symmetric.
Although this definition requires that the matrix representation of α relative
to every orthonormal basis is symmetric, it is sufficient to know this for just
one orthonormal basis.
Lemma 11.4.4 A linear map α : 
 → 
 is symmetric if there is one
orthonormal basis of 
 for which the matrix representation of α is
symmetric.
Proof Suppose that there is some orthonormal basis  with respect to which
α has a symmetric matrix representation A. Let ′ be any orthonormal basis,
and let X be the matrix of the identity map I from the basis  to the basis ′.
Then, by Theorem 9.5.1, the matrix representation of α relative to the basis 
′ is XAX−1. By Lemma 11.2.9, X is orthogonal, and by Lemma 11.4.2,
XAX−1is symmetric.
There is another characterization of symmetric linear maps.
Theorem 11.4.5 A linear map α : 
 → 
 is symmetric if and only if for
all vectors x and y, α(x)·y = x·α(y).
Proof Suppose that α is symmetric; then the matrix, say A, of α with respect
to the standard basis 
 is symmetric. Take any x and y in 
 ; then

Now suppose that for all x and y, α(x)·y = x·α(y), and let A be the matrix
representation of α relative to the standard basis of 
. Then (as above) xt
At y = x Ay. If we now let x =  and y = , then we see that ai j = a ji, so that
A is symmetric. Lemma 11.4.4 now implies that α is symmetric.
We need one more result before we can achieve our objective. The
characteristic polynomial of an n × n real symmetric matrix A is a real
polynomial of degree n and, by the Fundamental Theorem of Algebra, this
will always have n complex roots. We need to show that all of these roots
all real.
Theorem 11.4.6 Every root of the characteristic equation of a real
symmetric matrix is real.
Proof Let A be a real n × n symmetric matrix (ai j). Then A provides a linear
map of the complex vector space 
 into itself by the rule
which we shall write as zt 
 Azt , where z = (z1, . . . , zn). We shall also use
the notation 
, where  is the usual complex conjugate of zj .
Now take any (possibly complex) eigenvalue λ of A, and a corresponding
(possibly complex) eigenvector wt ; thus Awt = λwt . We now take the
complex conjugate, and then the transpose, of each side of this equation. As
A is real and symmetric, 
, and this gives 
. Thus
and hence 
. As w is an eigenvector, w ≠ 0
and so some wj is non-zero; thus λ is real.

Finally, we are now in a position to state and prove our main result.
Theorem 11.4.7 Given a linear symmetric map α : 
 → 
 , there is an
orthonormal basis of 
 such that each basis element is an eigenvector of
α.
If we rephrase Theorem 11.4.7 in terms of matrices rather than linear maps,
we obtain the following result.
Corollary 11.4.8 A real symmetric n × n matrix A has n mutually
orthogonal eigenvectors. In particular, there exists an orthogonal matrix P
such that PAPt (= PAP−1) is diagonal.
Proof of Theorem 11.4.7 We prove this by induction on n, and the
conclusion is true when n = 1. Now suppose the result is true for n = 1, . . .
m − 1 and consider the case when n = m.
By Theorem 11.4.6, α has a real eigenvalue, say λ1 and a corresponding
eigenvector w1, with ||w1|| = 1. Let W1 = {x ∈ 
 : x ⊥ w1}. Then, by
Theorem 8.1.2, W1 has dimension m − 1. Moreover, α maps W1 into itself
because if x ∈ W1, then α(x)·w1 = x·α(w1) = x·(λ1w1) = 0. Now the action of
α on W1 is symmetric (because α(x)·y = x·α(y) for all x and y in 
 , and
hence certainly for x and y in W1). Thus, by the induction hypothesis, W1
has an orthonormal basis consisting entirely of eigenvectors of α, and the
proof is complete.
We give one example to illustrate these ideas.
Example 11.4.9 Let
Then the eigenvalues of A are 0, with eigenvector (1, −2)t , and 5, with
eigenvector (2, 1)t . These vectors are orthogonal to each other, and if we
scale these so that they have unit length, we obtain an orthonormal basis of 
 , namely (μ, −2μ)t and (2μ, μ)t , where μ = 1/
. Thus

Together, these give
which can be writen in the form
that is, in the form P−1 AP = D, where the diagonal matrix D has the
eigenvalues of A as its diagonal entries.
It is perhaps worth stressing that it is not just the symmetry of the matrix
that is important in this discussion; it is the symmetry of the matrix together
with the existence of a scalar product. We are so familiar with the scalar
product (at least in 
) that we tend to take its existence for granted;
however without it, symmetry is of no consequence.
It is also important to note that the conclusion of Corollary 11.4.8 do not
generally hold for symmetric complex matrices : for example, the
symmetric matrix A given by
is not diagonalizable. Indeed, if there is a complex non-singular matrix X
such that XAX−1 is a diagonal matrix D, then the diagonal entries in D are
the eigenvalues of A, and these are 2, 2. Thus D = 2I, and hence A = X−1(2I
)X = 2I, which is not so. In fact, a similar theory holds for complex matrices
if we use the scalar product 
 and require that the matrices A
satisfy 
 instead of At = A.
Exercise 11.4

1.  Verify the details regarding the eigenvalues and eigenvectors given in
Example 11.4.1. In particular, show that if s = t = 2 then A is symmetric,
and has eigenvalues 3 and −1 with corresponding (orthogonal)
eigenvectors (1, 1) and (1, −1). Use this to find an orthogonal matrix P
such that
2. Let
Show that A is symmetric but XAX−1 is not. Is X orthogonal?
3.  For each of the following matrices, find an orthonormal basis of  
that consists of eigenvectors of the matrix:
4.  Show, without the use of matrices, that under a suitable linear change of
variables, 2yz + 2xz can be expressed in the form X2 − Y2. Now use the
theory of symmetric matrices to achieve a similar result.
5.  Find a linear change of variables that transforms the quadratic form
∑1≤i< j≤4 2xi xj into 
 .
11.5 The field axioms
In this and the next section we briefly discuss Euclidean n-dimensional
space 
, and ask to what extent it is possible to define some kind of
product on this space. The single result in this section says that if n ≥ 3 then
it is impossible to create a multiplication on 
 which, when combined
with the natural addition on 
, makes 
 into a field. Thus it is only in the
case of real and complex numbers (that is, for n = 1, 2) can we obtain a field
(and some suggestion that this might be so was given in Section 4.3).

Theorem 11.5.1 Suppose that there is a multiplication defined on 
 which,
with vector addition, gives a field. Then n is 1 or 2.
Proof The proof is by contradiction, so we suppose that n ≥ 3, and that we
have a multiplication in 
 which, with vector addition, makes 
 into a
field. We denote the product of x and y by x y. Now the field axioms
guarantee the existence of a vector e such that ex = x = xe for every x. The
vector 0 is, of course, the additive identity. Now in any field, 0x = 0 = x0,
and this implies that 0 ≠ e (as otherwise x = xe = x0 = 0 and so 0 would be
the only vector in the space).
Now choose a vector x that is not a multiple of e, and consider the
vectors e, x, x2, . . . , xn. As these vectors are linearly dependent, there are
real numbers λj, not all zero, such that λ0e + λ1x + · · · + λnxn = 0. Notice
that we cannot have λ1 = · · · = λn = 0 else λ0e = 0 and then e = 0. Now
define the real polynomial P by P(t) = ∑j λj tj. As some λj, where j > 0, is
non-zero, P is not constant.
As P has real coefficients, it is a product of real linear factors and real
quadratic factors, where the quadratic factors (if any) do not factorize into a
product of linear factors. Now in any field, uv = 0 implies that u = 0 or v = 0
so, as P(x) = 0, we see that either there is some linear factor that vanishes at
x, or there is some quadratic factor that vanishes at x. Now a linear factor
cannot be zero for if ax + be = 0, then x would be a multiple of e, contrary
to our original choice of x. It follows that there is some quadratic factor that
is zero, say Ax2 + Bx + Ce = 0, where A ≠ 0 and B2 − 4AC < 0 (because we
know that this quadratic expression is not a product of linear factors). It
follows that
where μ > 0, and so there are real numbers α and β such that, for the given
x, (αx + βe)2 = −e.
Now take any other y that is not a multiple of e. Then, exactly as for x,
there are real numbers γ and δ such that (γ y + δe)2 = −e. We deduce that

As one of these factors must be 0, this means that x, y and e are linearly
dependent. As n ≥ 3, we could have chosen y so that it was not linearly
dependent on x and e; thus we have a contradiction.
11.6 Vector products in higher dimensions
This section is intended to give the reader more insight into the very special
nature of the vector product in 
. We begin by showing how the vector
product in 
 is intimately connected to rotations. We recall from (6.1.5)
that if A is a rotation of 
 about the origin then A(x) × A(y) = A(x × y). The
next result is a type of converse of this in the sense that it shows that the
vector product is characterized by linearity and its invariance under
rotations.
Theorem 11.6.1 Suppose that there is some product of vectors in 
, say x 
 y, that is itself a vector in 
, with the properties
(a)  x  y is linear in x, and linear in y, and
(b)  x  y is invariant under all rotations; that is, A(x)  A(y) = A(x  y).
Then x  y is a constant multiple of the vector product x × y.
Proof First, consider a non-zero vector x, take any vector y orthogonal to x,
and let A be the rotation of angle π about the axis along y. As A(x) = −x, (a)
and (b) imply that
As the only solutions of A(x) = x are scalar multiples of y, we see that x  x
is a scalar multiple of y. However, the direction of y is arbitrary (subject to
being orthogonal to x); thus we must have x  x = 0. If we now expand
(x + y)  (x + y) we see that for all x and y, x  y = −y  x.
Take any two non-zero vectors x and y that do not lie along the same line.
Then they determine a plane Π, say, with normal along the vector n. Let B
be the rotation of angle π about the axis n. Then

so that x  y must be a scalar multiple of n for these are the only solutions
of B(x) = x. This shows that x  y is orthogonal to x and to y. In particular, i 
 j is a multiple of k and so on. Now any fixed scalar multiple of a product
satisfying (a) and (b) also satisfies (a) and (b), so these can only determine
such a product to within a scalar multiple. In such situations one chooses an
arbitrary normalization and, as i  j is a multiple of k, we may choose the
normalization i  j = k.
Finally, let C be the rotation of angle 2π/3 about the axis given by
x1 = x2 = x3. Then A cyclically permutes i, j and k and so, using (b), we see
that j  k = i and k  i = j. We now know x  y whenever x and y are any of
i,j and k, and as these last three vectors are a basis of 
, the linearity of 
 now implies that the product, x  y, is completely determined. Moreover,
we can compute what it must be and when we do this we find that x 
 y = x × y.
We have defined a multiplication in 
, namely the vector product, and
although this does not make 
 into a field, it does have some very useful
properties. We have also seen that we cannot make 
 into a field. Might it
be possible, then, to define a vector product in other spaces 
? Of course,
this depends on what we mean by a 'vector product' but, as the (very) brief
sketch below shows, if we extract some of the fundamental properties of the
vector product and then ask in which spaces does such a product exist, the
answer, rather surprisingly, is 'only in 
 and 
'.
Theorem 11.6.2 Suppose that 
 supports a non-zero vector product,
which we denote by x  y, with the following properties:
(1)  x  y is linear in x and in y;
(2)  x  y is orthogonal to x and to y;
(3)  ||x  y||2 + (x·y)2 = ||x||2||y||2.
Then n is 3 or 7.
We can only comment briefly on this result. First, (3) implies that x  x = 0.
Thus if we expand (x + y)  (x + y) we see that x  y = −y  x. If n = 1 then
we have x  y = x y(1  1) = 0, so no product of this type exists on 
(except the zero product). Likewise, none can exist on 
, since (2) clearly

fails in this case. If n = 3 the vector product x × y satisfies the requirements
given in Theorem 11.6.1 (and this is the only such product up to
multiplication by a scalar). How does one handle the cases n ≥ 4? Briefly,
one shows that if such a vector product exists on 
, then one can construct
a product, say ⊗, on 
 exactly as one did for quaternions, namely
which
(1) is linear in x and in y,
(2) has an identity element e, and
(3) is such that the norm of the product is the product of the norms.
A result of Hurwitz (in 1898) is that such a product can only occur if n + 1
is 1, 2, 4 or 8. Thus n is 3 or 7.
Exercise 11.6
1.  Suppose that Φ(a, b, c) is a real-valued function of the vectors a, b and c
in 
 with the following properties:
(1)  Φ(a, b, c) is linear in a, b and c;
(2)  Φ(a, b, c) = 0 if a = b, or b = c, or c = a;
(3)  Φ(i, j, k) = 1.
Show that Φ(a, b, c) = [a, b, c].

12
Groups
12.1 Groups
In Chapter 1 we defined what it means to say that a set G is a group with
respect to an operation *, and we studied groups of permutations of a set. In
this chapter we shall study 'abstract' groups. Although this may seem like a
more difficult task, every 'abstract' group is the permutation group of some
set so that, in some sense, this apparent change of direction is only an
illusion.
We have already seen many examples of groups: the sets , 
 and  of
integers, real numbers, and complex numbers, respectively, the spaces 
and 
, the set of matrices of a fixed size, and indeed any vector space, all
form a group with respect to addition. Likewise, the set 
 of positive real
numbers, the set 
 of non-zero complex numbers, the set of complex
numbers of modulus one, the set of non-singular n × n matrices, and the set
of n-th roots of unity, all form a group with respect to multiplication.
Finally, the set of bijections of a given set onto itself, the set of isometries
of , and of 
, the permutations of {1, . . . , n}, and the set of non-singular
(invertible) linear transformations of a vector space onto itself, all form a
group with respect to the usual composition of functions (that is, if f * g is
the function defined by (f * g)(x) = f (g(x))). It is hardly surprising, then,
that the study of general groups is so useful and important.
Often a group is a familiar object with an accepted symbol instead of *;
for example, where there is an operation of addition we use x + y instead of
x * y, and 0 instead of e (the identity element). Likewise, if * is
multiplication we usually write xy instead of x * y, and 1 instead of e. In

these and similar cases we shall retain the use of the accepted symbols.
From now on we shall, for the general group, adopt the notation gh for g *
h; however, it must always be remembered that underlying the notation gh
there is some (perhaps unspecified) operation *.
We recall that a group G contains a unique identity element e such that
ge = g = eg for every g, and that every g has a unique inverse g−1 such that
gg−1 = e = g−1g. Note also that (gh)−1 = h−1g−1 for obviously,
Next, suppose that g is an element of a group G. We denote the composition
of g with itself n times by gn; formally, this is defined for n = 1, 2, . . . by
g0 = e and gn+1 = gng. We also define g−n, where n ≥ 1, to be (g−1)n. With
these definitions it is straightforward, though tedious, to show that for all
integers m and n, gm gn = gm+n. We omit the proof (which is by induction).
We now define the order of a group, and the order of an element of a
group. If E is a finite set we denote the number of elements in E by |E|; this
is called the cardinality of E. When E is a group, the word order is
preferred to cardinality.
Definition 12.1.1 Let G be a group. If G is a finite set, the order of G is |G|.
If G is an infinite set then G is said to have infinite order.
Definition 12.1.2 Let G be a group, and suppose that g  G. The element g
is said to have (or be of) finite order if there is some integer m such that
gm = e. If g (≠ e) has finite order, then the order of g is the smallest positive
integer k such that gk = e. If g is not of finite order then it has (or is of)
infinite order.
For example, the group {1, i, −1, −i} of complex numbers under
multiplication has order four, and the elements 1, i, −1 and −i have orders 1,
4, 2 and 4, respectively. The permutation group acting on {1, . . . , n} has
order n!, and a cycle (n1, . . . , nt), has order t. The permutation (1 2)(3 4 5)
has order six; the permutation (1 2)(1 3) has order three.
Suppose that g in G has order t; then e, g, . . . , gt−1 are distinct elements
of G (for if gi = gj, where 0 ≤ i < j < t, then gj−i = e and 0 < j − i < t which

contradicts the fact that g has order t). It follows that |G| ≥ t so that if g  G
then the order of g is not greater than the order of G. In Section 12.3 we
shall prove that the order of g divides the order of G. In any event, every
element of a finite group has finite order.
All groups G have the property that for certain g and h in G, gh = hg.
However, this need not be so for every pair of elements in G; for example,
(1 2 3)(1 2) ≠ (1 2)(1 2 3) in the permutation group on {1, 2, 3}. For
convenience, we repeat Definition 1.2.6 here.
Definition 12.1.3 The elements g and h in a group G are said to commute if
gh = hg. The group G is abelian, or commutative, if every pair of elements
in G commute. The word 'abelian' is in honour of the Norwegian
mathematician Abel.
We pause to illustrate these ideas with two results about elements of order
two. First, suppose that G is a group in which every element has order two,
and consider any x and y in G. Then x2, (x y)2 and y2 are all e, so that
This proves the first part of the following result.
Theorem 12.1.4 Let G be a finite group in which every element has order
two. Then G is abelian, and has order 2m for some integer m.
Proof We start with the finite set G. Now suppose that some element of G is
a product of some of the other elements of G and their inverses; then we
discard this element from G and so produce a strictly smaller set, say G1,
from which G can be obtained by taking all products of powers (including
negative powers) of elements in G1 in any order (with repetitions allowed).
We now apply the same argument to G1, and obtain (if possible) a smaller
set G2 with the property that any element of G1, and hence also of G, can be
expressed as a product of elements (and their inverses) in G2. This process
continues until we have obtained a subset Gk of G whose elements generate
G in the sense above, and which is such that no proper subset of Gk has this
property. We say that Gk is a minimal generating set for G.

Suppose that Gk = {a1, . . . , am}. So far, we have not used the fact that G
is abelian. However, as G is abelian, all of the products of elements in Gk,
and hence every element of G, can be expressed in the form 
,
where each uj is an integer. Because every element of G has order two, we
may now assume that each uj is 0 or 1. There are 2m possibilities here for
the uj, and it remains to show that different choices of (u1, . . . , um) lead to
different elements of G. Suppose, then, that 
, where
each ui, and each vj , is 0 or 1. If u1 ≠ v1 then we see that 
 for
some tj, and this shows that Gk is not a minimal generating set (for we could
discard a1 from Gk and still retain a generating set). We deduce that u1 = v1.
The same argument gives uj = vj for all j, and it follows that G has order 2m.
Theorem 12.1.5 Let G be a group of even order. Then G contains an
element of order two.
Proof For each g in G we form the set {g, g−1}. If {g, g−1} ∩ {h, h−1} ≠ Ø,
then one of the four possibilities g = h, g = h−1, g−1 = h and g−1 = h−1 arises,
and in all cases, {g, g−1} = {h, h−1}. It follows that G can be written as the
union of a finite number of mutually disjoint sets {g, g−1}. As {g, g−1} has
two elements when g2 ≠ e, and one element when g2 = e, there must be an
even number of sets of this form with g2 = e (else the order of G would be
odd). Now e2 = e so one of these sets is {e}. It follows that there must be
another such set; that is, there must be some g with g ≠ e and g2 = e. This g
is an element of G of order two.
Exercise 12.1
1.  Let G be a finite group which has only one element, say x, of order two.
Show that x commutes with every other element of G. [Hint: consider
yxy−1.]
2.  Show that a group cannot have exactly two elements of order two. [Hint:
Suppose that x and y have order two and consider xyx−1 and xy.] The
group 
 of isometries of  has exactly three elements of
order two.

3.  Suppose that a group G contais two elements a and b, where a−1ba = b2
and b−1ab = a2. Suppose also that every element of G can be expressed
as a product of the elements a, b, a−1 and b−1, taken as often as we wish,
in any order, and with repetitions allowed. Show that a = b = e and hence
that G = {e}.
4.  Suppose that g is in a group G. Prove that, for all integers m and n, gm
gn = gm+n.
5.  For each positive integer n let Gn denote the group of complex n-th roots
of unity under multiplication. Show that if p and q are coprime, then Gp
∩ Gq = {1}. What can be said if p and q are not coprime?
6.  Let S be a finite set of non-zero complex numbers that is closed under
multiplication. Show that for some positive integer n, S is the group of n-
th roots of unity. [Hint: first show that if z  S then |z| = 1.]
12.2 Subgroups and cosets
There are many instances in which a subset of a group is a group in its own
right; for example,  is a group within the group 
 with respect to addition.
Similarly, a subspace of a vector space is itself an additive group, and
corresponding to the idea of a subspace, we have the idea of a subgroup.
Definition 12.2.1 A non-empty subset H of G is a subgroup of G if it is a
group with the same rule of combination as G.
This definition does not say explicitly that if H is a subgroup of G then the
identity element eH of H coincides with the identity e in G, but this must be
so. Indeed, as H is a group we see that 
. However, by Lemma 1.2.5,
this means that eH = e. A similar question can be asked of inverse elements,
and the answer is the same: if h  H then the inverse of h as an element of
H coincides with its inverse as an element of G. Indeed, if it did not there
would be two elements x in G that satisfy xh = e = hx, and we know that
there is only one.
A group G always has at least two subgroups, namely G itself and the
subgroup {e} consisting of the identity element alone. We call {e} the
trivial subgroup of G, and we say that H is a non-trivial subgroup of G if

H ≠ {e}. We say that H is a proper subgroup of G if H ≠ G, and our interest
lies in the non-trivial proper subgroups of G.
We now give a simple test for a subset to be a subgroup.
Theorem 12.2.2 Suppose that H is a non-empty subset of a group G. Then
H is a subgroup of G if and only if
(a)  if g  H and h  H, then gh  H, and
(b)  if g  H then g−1  H.
Proof If H is a subgroup then (a) and (b) hold. Suppose now that H is a non-
empty subset of G and that (a) and (b) hold. As the associative law holds for
G it automatically holds for H. Moreover, (a) implies that H is closed under
composition of its elements. As H is non-empty, there is some g in H, and
so from (b) and then (a) (with h = g−1), e  H. Finally, (b) guarantees that
each g in H has an inverse in H, and the proof is complete.
Next, we describe an important property of the class of all subgroups of a
given group.
Theorem 12.2.3 Let G be any group. Then the intersection of any collection
of subgroups of G is also a subgroup of G.
Proof Let the collection of subgroups of G be Ht, where t lies in some set
(of labels) T. Let H be the intersection ∩tHt of all of the Ht; thus g  ∩tHt if
and only if g  Ht for every t in T. Note that there is no assumption here that
T is finite. As e  Ht for every t we see that e  H; thus H ≠ Ø. If g and h
are in H, then they are in every Ht. Thus gh is in every Ht and so it is in H.
Similarly, if g  H, then g  Ht for every t; hence g−1  Ht for every t, and
so g−1  H. The conclusion now follows from Theorem 12.2.2.
As a consequence of Theorem 12.2.3, we see that if G0 is any non-empty
subset of a group G, then we can consider the collection of all subgroups Ht
of G that contain G0 (there is at least one, namely G itself). It follows that
∩t Ht is also a subgroup that contains G0, and it is clearly the smallest such

subgroup because any subgroup containing G0 is one of the Ht. This
argument justifies the following definition.
Definition 12.2.4 Let G0 be a non-empty subset of a group G. Then the
subgroup of G generated by G0 is the smallest subgroup of G that contains
G0 (and it is the intersection of all subgroups that contain G0).
For example, let G = 
 with addition and let G0 = {3, 5}. What is the
subgroup H generated by G0? First, if H1 is a subgroup that contains G0,
then 3 + 3 − 5  H1 so that 1  H1; thus H1 ⊃ . As  is a subgroup of 
that contains G0 we deduce that  is the subgroup generated by G0. In plain
terms, any subgroup of 
 that contains 3 and 5 also contains , and  is
such a subgroup.
We come now to the idea of a coset; this is a subset of a group G that is
constructed from a given subgroup H of G. Given any two subsets, say X
and Y, of a group G, we can create a new subset XY of G defined by
Of course, if we wish, we can also consider sets of the form XY Z, X−1Y and
so on. If X has only one element, say X = {x}, we naturally write
(12.2.1)
and, similarly,
(12.2.2)
It is clear that these sets obey simple algebraic rules; for example,
u(vY) = (uv)Y, u−1(uY) = Y and so on. The sets (12.2.1) and (12.2.2) take on
a special significance when Y is a subgroup of G, and we begin with the
following simple result about subgroups.
Theorem 12.2.5 Let H be a subgroup of a group G, and let g be any
element of G. Then g  H if and only if gH = H, and also if and only if
Hg = H.

Proof First, if gH = H then ge  H so that g  H. Now suppose that g  H.
As gh  H whenever h  H we see that gH ⊂ H. Now take any h in H and
note that h = g(g−1h). As g−1h  H, we see that h  gH so that H ⊂ gH. This
proves that gH = H. The proof that g  H if and only if Hg = H is similar.
Theorem 12.2.5 raises the question of what the sets gH and Hg look like
when g  H, and the following examples suggest that these sets are indeed
worth studying. Of course, if we are using the additive notation here, we
replace gH by g + H and so on.
Example 12.2.6 Let G be the group 
 with respect to addition, and let
Then H (a horizontal plane) is a subgroup of G, and x + H is the translation
of the plane H by x. Theorem 12.2.5 records the geometrically obvious fact
that x + H = H if and only if x  H. Note that any two translations of H are
either equal or disjoint, and that G is the union of all of the x + H.
Example 12.2.7 Let G be the group of non-zero complex numbers with
respect to multiplication, and let H be the subgroup {z : |z| = 1}. For any w
in G, wH is the circle with centre 0 and radius |w|, so that (as in Theorem
12.2.5) wH = H if and only if w  H. In this example the sets wH (for
varying w) are circles centered at the origin; thus again, any two sets wH are
either equal or disjoint, and G is the union of these sets.
Example 12.2.8 This example is taken from modular arithmetic. Let G = 
(the group of integers under addition), and let H be the subgroup of
multiples of 3 (we could replace 3 by any other integer here). Then
It is obvious that given any integer m, if we write m = 3k + a, where a  {0,
1, 2}, then q + H = a + H. Thus once again we see that the two sets u + H

and v + H are equal or disjoint, and that G is the union of the sets u + H (in
this case it is the union of just three of these sets).
In each of the three previous examples the groups G were abelian, and if
G is any abelian group, then gH = Hg for every g in G. However, if G is not
abelian, gH and Hg may be different sets. We give an example.
Example 12.2.9 Let G be the group of permutations of {1, 2, 3}; thus
Let H = {I, (1 2)} and g = (1 3); then gH = {(1 3), (1 2 3)} whereas
Hg = {(1 3), (1 3 2)}.
We shall now define what we mean by a coset and, motivated by these
examples, state and prove the main result on cosets.
Definition 12.2.10 A left coset of a subgroup H of G is a set of the form
gH; a right coset is a set of the form Hg.
Theorem 12.2.11 Let H be a subgroup of a group G. Then G is the union of
its left cosets, and any two left cosets are either equal or disjoint. Further,
the left cosets f H and gH are equal if and only if f−1g 
 H. Similar
statements are true for right cosets.
Proof Take any g in G. As g  gH, g is in at least one left coset; thus G is
the union of its left cosets. We will complete the proof by showing that the
three statements (a) f H ∩ gH ≠ Ø, (b) f H = gH, (c) f−1g  H are equivalent
to each other. Clearly, (b) implies (a). Suppose now that (a) holds; then
there are h1 and h2 in H such that gh1 = f h2; hence 
, and (c)
holds. Finally, if (c) holds then, from Theorem 12.2.5, f−1 gH = H so that
gH = f f−1 gH = f H which is (b).
We emphasize that the force of Theorem 12.2.11 is that a group G is
partitioned into disjoint subsets by its cosets with respect to any given
subgroup H of G. This is called the coset decomposition of G.

Exercise 12.2
1.  The centre C of a group G is the set of elements of G which commute
with all elements of G. Show that C is an abelian subgroup of G.
2.  Let H be a finite non-empty subset of a group G. Show that H is a
subgroup of G if and only if H is closed with respect to the group
operation in G.
3.  Suppose that H and K are subgroups of a group G. Show that if Hg1 ⊂
Kg2 for some g1 and g2, then H ⊂ K.
4.  Let H be a subgroup of a group G. Show that if g1 H = Hg2 then
g2H = Hg1.
5.  Let H be a subgroup of a group G. Show that if aH = bH then
Ha−1 = Hb−1. This means that there is a (well-defined) map α from the
set of left cosets to the set of right cosets given by α(g H) = Hg−1. Show
that α is a bijection.
6.  Let G be the group of isometries of . Show that the set T0 of
translations z 
 z + n, where n  , is a subgroup of G. Show that
gT0 ≠ T0g, where g(z) = iz.
7.  Suppose that a and b are non-zero real numbers, and let H be the set of
(x, y) in 
 such that ax + by = 0. Show that H is a subgroup of 
,
and that the coset (p, q) + H is the line ax + by = p + q.
8.  Let G be the group of real 3 × 3 invertible matrices, and let D be the
subgroup of diagonal matrices. Let
What is the coset XD?
9.  Let G be 
16 (the group {0, 1, . . . , 15} under addition modulo 16), and
let H be the subgroup of G generated by the single element 4. List all
(four) cosets of G with respect to H.
12.3 Lagrange's theorem
We recall that the order |G| of a group G is the number of elements in G.
Lagrange's theorem is concerned with the relation between the order of a

group and the order of one of its subgroups.
Theorem 12.3.1: Lagrange's theorem. Let H be a subgroup of a finite
group G. Then |H| divides |G|, and |G|/|H| is the number of distinct left (or
right) cosets of H in G.
Lagrange's theorem shows that the order of G alone influences the number
of subgroups that G can have (regardless of its group theoretic structure),
and the most striking application of this is to groups of prime order.
Corollary 12.3.2 Suppose that G is a group of order p, where p is a prime.
Then the only subgroups of G are G and the trivial subgroup {e}.
This is clear because if H is a subgroup of G then, by Lagrange's Theorem,
|H| divides p. Thus |H| is 1 or p, and H is {e} or G.
The proof of Lagrange's theorem We consider a finite group G and a
subgroup H. By Theorem 12.2.11, G is the union of a finite number of
pairwise disjoint cosets, say G = g1 H ∪ · · · ∪ gr H (see Figure 12.4.1).
Consequently,
Now for any g, the map x 
 gx is a bijection of H onto gH (with inverse x
 g−1x). It follows that H and gH have the same number of elements,
namely |H|, so that |G| = r|H|.
Another corollary of Lagrange's theorem is the following result which
was mentioned in Section 12.1.

Figure 12.4.1
Theorem 12.3.3 Let g be an element of a finite group G. Then the order of
g divides the order of G.
Proof Let g be any element of a finite group G. We know that g has finite
order, say d (and d ≤ |G|; see Section 12.1). It is easy to check that the set
{e, g, . . . , gd−1} is a subgroup of G of order d (the inverse of gk is gd−k) so,
by Lagrange's Theorem, d divides |G|.
We end this section with an example to show that the converse of
Lagrange's theorem is false; it is not true that if an integer q divides the
order of G, then G necessarily has a subgroup of order q.
Example 12.3.4 Let G be the group A4 of all even permutations acting on
{1, 2, 3, 4}. Now |A4| = 12, and we are going to show that A4 has no
subgroup of order six, We suppose, then, that H is a subgroup of A4 of order
six, and we seek a contradiction. The twelve elements of A4 are the identity
I, three permutations of the form (a b)(c d), where a, b, c and d are distinct,
and eight three-cycles. As H has order six, it contains an element of order
two (Theorem 12.1.5), so we may suppose that σ  H, where σ = (a b)(c d).
As A4 has only four elements that are not of order three, H must also

contain a three cycle, ρ, say. Thus H contains the subgroup H0 = {I, ρ, ρ2}.
As σ  H0, the coset σ H0 is disjoint from H0, thus
Now H contains ρσ, so this element must be one of the six elements listed
here. Clearly, ρσ is not I, ρ, ρ2 or σ; thus either (i) ρσ = σρ, or (ii) ρσ = σρ2.
We shall now show that both (i) and (ii) lead to a contradiction. Suppose
that (i) holds, and let k be the single integer in {1, 2, 3, 4} such that ρ(k) = k.
Then σ(k) = σρ(k) = ρσ(k), so that ρ fixes σ(k). Thus σ(k) = k, which is false.
Finally, suppose that (ii) holds. Then (ρσ)2 = (ρσ)(σρ2) = I, so that H
contains another element of order two, namely ρσ. Now the product of any
two elements in A4 of order two is the third element of order two; thus (ρσ)σ
is also of order two, and this is false as ρ has order three.
Exercise 12.3
1.  Show that if a group G contains an element of order six, and an element
of order ten, then |G| ≥ 30.
2.  The order of a group G is less than 450, and G has a subgroup of order
45, and a subgroup of order 75. What is the order of G?
3.  Suppose that a group G has subgroups H and K of orders p and q,
respectively, where p and q are coprime. Show that H ∩ K = {e}.
4.  Suppose that an abelian group G has order six. Use Theorem 12.1.4 and
Lagrange's theorem to show that G has an element of order three. Now
show that G has an element of order two, and deduce that G is cyclic
(that is, for some g, G = {e, g, . . . , g5} with g6 = e).
12.4 Isomorphisms
Consider the following four groups:

with respect to composition, addition modulo 4, and multiplication,
respectively. Now every element of G1 (other than the identity) has order
two, and as this is not so for G2 and G3, there is a significant group-
theoretic difference between G1 and the other two groups. By contrast, there
is no significant difference between G2 and G3. Indeed, if we consider the
bijection θ from G2 to G3 given by θ(n) = e2πin/4, then addition modulo 4 in
G2 'corresponds' to multiplication in G3; for example, 2 + 3 = 1 in G2,
while in G3,
In short, while G2 and G3 are different sets, they are, in a group-theoretic
sense, the 'same' group. Another example of two groups that are the 'same'
is the group {1, −1} under multiplication, and the group {I, σ}, where
σ(z) = −z, of isometries of the complex plane.
These examples raise the question of deciding when we should regard
two groups as being the same group. For example, later, we shall want to
list all groups of order four, and this will be an impossible task if we insist
on distinguishing between groups that are formally different from each
other even though they possess the same group-theoretic structure. What we
need is a formal way of identifying groups that have identical structures,
and when we have this we shall see that there are only two groups of order
four (namely, G1 and G3). The identification of two groups is given by a
special mapping, called an isomorphism, and which we have already
illustrated by the map θ above.
Definition 12.4.1 Let G and G′ be groups. A map θ : G → G′ is an
isomorphism if
(a)  θ is a bijection of G onto G′, and
(b)  for all g and h in G, θ(gh) = θ(g)θ(h).
If such a θ exists, we say that G and G′ are isomorphic groups.
A more informal view of an isomorphism is as follows. Given g and h in G
we can combine these in G to obtain gh and then map this into G′ to obtain
θ(gh). Alternatively, we can map g and h to θ(g) and θ(h), respectively, in

G′ and then combine these to obtain θ(g)θ(h). The crucial property of the
isomorphism θ (and later, of a homomorphism) is that these two operations
have the same result. Perhaps one of the most interesting and elementary
examples of an isomorphism is the function ex from 
 to itself which we
now discuss.
Example 12.4.2 We assume familiarity with the function exp : x 
 ex and
we want to use its properties to illustrate the idea of an isomorphism. Now
exp is a map from the additive group 
 to the multiplicative group 
 of
positive real numbers. It is known (from analysis) that exp is a bijection
from 
 to 
. The crucial property ex+y = exey of exp is exactly the
condition that is needed to show that exp is an isomorphism; thus the group
( , +) is isomorphic to the group (
, ×).
In case the reader should have the idea that it is an easy matter to decide
whether or not two groups are isomorphic, we mention (but will not prove)
the following amazing result (in which the operation in both groups is the
usual multiplication of complex numbers).
Example 12.4.3 The group {z   : |z| = 1} is isomorphic to the group {z 
 : z ≠ 0}. Of course, the isomorphism here has nothing whatever to do
with the geometric structure of these sets as subsets of , and this is where
one's intuition may lead one astray. This example is a stark reminder that
group theory exists independently of geometry (for a proof, see
Mathematics Magazine, 72 (1999), 388-91).
If one wants to show that two given groups G and G′ are isomorphic, one
has to define a map θ : G → G′ and then show that θ has the required
properties. This is not always easy to do. To show that G and G′ are not
isomorphic it suffices to find a property that is preserved under an
isomorphism, and which is satisfied by only one of G and G′. This is usually
easier, and with this in mind we list some of the basic properties of groups
that are preserved under an isomorphism.
Theorem 12.4.4 Suppose that θ : G → G′ is an isomorphism. Then

(a)  if G is abelian then so is G′;
(b)  θ(e) = e′, where e and e′ are the identities in G and G′;
(c)  for all g in G, θ(g)−1 = θ(g−1);
(d)  the order of g in G is equal to the order of θ(g) in G′.
Proof Suppose that G is abelian, and take any g′ and h′ in G′. As θ maps G
onto G′ there are g and h in G with g′ = θ(g) and h′ = θ(h). Thus
and we have proved (a). Next, take any g′ in G′ and (as above) write
g′ = θ(g). Then θ(e)g′ = θ(e)θ(g) = θ(eg) = θ(g) = g′. In the same way we see
that g′θ(e) = g′, and as G′ has a unique identity element, this element must
be θ(e). This proves (b), and the proof of (c) is similar. For any g in G we
have
and this shows that θ(g−1) = [θ(g)]−1. Finally, for any g in G and any integer
m, θ(gm) = θ(g)m. As θ is a bijection, e, g, . . . , gm are distinct if and only if
e′, θ(g), . . . , θ(g)m are distinct, and this proves (d).
The idea of classes of isomorphic groups is best described in terms of
equivalence relations. Briefly, an equivalence relation ~ on a set X is a
relation between the elements of x with the three properties
(a)  for all x, x ~ x;
(b)  if x ~ y then y ~ x;
(c)  if x ~ y and y ~ z then x ~ z.
These three properties say that ~ is reflexive, symmetric and transitive,
respectively. We define the equivalence class E(x) containing x to be the set
of elements of X that are related to x by ~; explicitly,
Equivalence relations are used throughout mathematics, and their main use
stems from the fact that they provide a natural way of splitting a set into a

union of mutually disjoint subsets.
Theorem 12.4.5 Let X be a non-empty set, and let ~ be an equivalence
relation on X. Then any two equivalence classes are equal or disjoint, and X
is the union of a number of mutually disjoint equivalence classes.
The reader should notice the similarity between this result and Theorem
12.2.11 (about cosets). Indeed, the idea of an equivalence relation can be
used to provide an alternative approach to cosets. Before we prove Theorem
12.4.5, we give its application to isomorphisms between groups.
Theorem 12.4.6 Let  be the class of all groups, and let ~ be the relation
on  defined by G1 ~ G2 if and only if G1 is isomorphic to G2. Then ~ is an
equivalence relation on .
This result is the formal statement of how we may now treat two groups as
being the 'same' group. According to Theorem 12.4.6, the equivalence
class containing a given group G is the class of all groups that are
isomorphic to G; equally, it is a maximal collection of groups any two of
which are isomorphic to each other, and each of which fails to be
isomorphic to a group outside the class. From an algebraic point of view we
shall no longer distinguish between isomorphic groups, and this will be
reflected in our blatant and deliberate abuse of language. For example,
when we say that there is only one group of order three we mean that any
two groups of order three are isomorphic to each other. However, it should
be remembered that when we are applying group theory to geometry there
may be important geometric reasons why we should distinguish between
isomorphic groups. We now prove Theorems 12.4.5 and 12.4.6.
The proof of Theorem 12.4.5 Suppose that x and y are in X and that E(x) ∩
E(y) contains an element z. Now take any y′ in E(y). Then y′ ~ y and y ~ z;
thus y′ ~ z. However, z ~ x; thus y′ ~ x and so y′  E(x). We deduce that E(y)
⊂ E(x) and, by symmetry, E(x) ⊂ E(y). This shows that if E(x) ∩ E(y) ≠ Ø
then E(x) = E(y). Finally, it is obvious that X is the union of its equivalence
classes because if x  X then x  E(x) so that

so that X is the union of the E(x).
The proof of Theorem 12.4.6 First, the identity map of a group G onto itself
is an isomorphism; thus G ~ G. Next, it is clear that if θ1 : G1 → G2 and θ2 :
G2 → G3 are isomorphisms then θ2θ1 is an isomorphism of G1 onto G3.
Indeed,
Finally, we need to show that if θ : G1 → G2 is an isomorphism, then so is
θ−1 : G2 → G1. First, as θ is a bijection of G1 onto G2, then θ−1 is a bijection
of G2 onto G1. Now take any x and y in G2. As θ maps G1 onto G2 there are
unique g and h in G1 such that θ(g) = x and θ(h) = y. Then
θ−1(xy) = θ−1(x)θ−1(y) because
It seems worth mentioning explicitly that Theorem 12.4.6 contains the
following result.
Theorem 12.4.7 If θ1 : G1 → G2 is an isomorphism then so is 
 : G2 →
G1. If, in addition, θ2 : G2 → G3 is an isomorphism, then so is θ2θ1 : G1 →
G3.
Exercise 12.4
1.  Let G be the group {1, −1} and let H be the group 
 of
isometries of . Show that H is isomorphic to G × G.
2.  Show that the multiplicative group of matrices of the form 
,
where n  , is isomorphic to ( , +).

3.  Recall the additive groups , 
 and , and the multiplicative groups 
and 
 of non-zero numbers. Show that
(a)   is not isomorphic to ;
(b)   is not isomorphic to 
(c)   is not isomorphic to 
.
4.  Consider the three multiplicative groups
Show that 
 × S1 is isomorphic to *. As 
 is isomorphic to , this
shows that * is isomorphic to  × S1 (a cylinder).
5.  Let G be the set of real 2 × 2 matrices of the form
where a ≠ 0. Show that G is a group under the usual multiplication of
matrices (the identity is not the usual identity matrix), and that G is
isomorphic to 
.
12.5 Cyclic groups
Let g be an element of a group G. For each positive integer n, gn is the
composition of g with itself n times, g0 = e and g−n = (g−1)n. If we are using
the additive notation for G, then gn is replaced by g + · · · + g (with n terms)
and we write this as ng. If G = , then ng in this sense is indeed the product
of n and g.
Definition 12.5.1 A group G is said to be cyclic, or a cyclic group, if there
is some g in G such that G = {gn : n  }. We then say that G is generated
by g.
Despite its appearance, a cyclic group need not be infinite; for example, −1
generates the group {1, −1} with respect to multiplication. Moreover, a
cyclic group may be generated by many of its elements; for example, if ρ is

the fivecycle (1 2 3 4 5), then (as the reader can check) each ρk with k ≠ 0
generates the cyclic group {I, ρ, ρ2, ρ3, ρ4}. The most familiar example of
an infinite cyclic group is the additive group  of integers (generated by 1).
The additive group 
 of real numbers is not cyclic; this is a consequence of
the next result.
Theorem 12.5.2 Let H be a non-trivial subgroup of the additive group 
.
Then H is cyclic if and only if it has a smallest positive element h, in which
case it is generated by h.
Proof Suppose that H is cyclic, say H = {nh : n  }. Then h ≠ 0 (else H is
trivial), and it is clear that |h| is the smallest positive element of H. To
establish the other implication, suppose that H has a smallest positive
element, say h. As h  H, we see that nh  H for every n, so that {nh : n  
} ⊂ H. Now suppose that x  H and write x = nh + t, where n is an integer,
and t is a real number satisfying 0 ≤ t < h. As H contains x and −nh, it also
contains x − nh which is t. As h is the smallest positive element of H, and 0
≤ t < h, we conclude that t = 0 and hence that x = nh. This shows that
H = {nh : n  }.
The next result shows that there is only one infinite cyclic group.
Theorem 12.5.3 Any two cyclic groups of the same finite order m are
isomorphic, and hence isomorphic to the additive group 
m. Any two
infinite cyclic groups are isomorphic, and hence isomorphic to the additive
group .
Proof We shall prove the second statement first. Suppose that G is an
infinite cyclic group generated by, say, g. Define the map θ : 
 → G by
θ(n) = gn. We shall show that θ is an isomorphism. Certainly, θ maps  onto
G. If θ(m) = θ(n), then gn = gm and so n = m (since otherwise, there would
be some non-zero k with gk = e and then G would be finite). Thus θ is a
bijection of  onto G. Finally,

so that θ is an isomorphism of G onto . If G1 and G2 are infinite cyclic
groups, they are each isomorphic to , and hence also to each other. This
proves the second statement.
Suppose now that G is a cyclic group, say G = {e, g, . . . , gm−1}, of finite
order m, so that gn = e if and only if m divides n. Let θ : 
m → G be the
map defined by θ(n) = gn. Then θ is a surjective map between finite sets
with the same number of elements and so is a bijection between these sets.
Further, for all j and k, θ(j ⊕m k) = gj+k (where ⊕m is addition in 
m)
because gm = e.
Theorem 12.5.3 implies that there is only one cyclic group of order n,
and it is usual to denote this by Cn.
One of the important properties of a cyclic group is that its cyclic nature
is inherited by all of its subgroups.
Theorem 12.5.4 A subgroup of a cyclic group is cyclic.
Proof Let H be a subgroup of the cyclic group {gn : n 
 
}. We may
suppose that H is a non-trival subgroup, so that gn, and g−n, are in H for
some n. Let d be the smallest positive integer such that gd  H. Clearly H
contains gmd for every integer m. On the other hand, if gk  H, we can write
k = ad + b, where 0 ≤ b < d, and we find that gb  H. We deduce that b = 0,
and hence that k = ad. It follows that H = {gmd : m  } and this shows that
H is cyclic, and generated by gd.
Finally, we obtain more information about groups of prime order (see
Corollary 12.3.2).
Theorem 12.5.5 A group of prime order is cyclic, and is generated by any
of its elements other than the identity e.
Proof Let G be a group with |G| = p, where p is a prime, and let g be in G
with g ≠ e. As G is finite, g has finite order k, say, which divides p. As k ≠ 1

(else g = gk = e), we see that k = p and hence that G = {e, g, . . . , gp−1}.
Thus G is cyclic and is generated by g.
Exercise 12.5
1.  Let ρ be the cycle (1 2 3 4 5), and let G be the group generated by ρ.
Show that each element of G (other than the identity) generates G. Show
that this is false if (1 2 3 4 5) is replaced by the cycle (1 2 3 4 5 6).
2.  Show that a finite group of rotations of 
 about the origin is a cyclic
group. Construct a proper subgroup of the group of rotations of 
about the origin that is not cyclic (in this group every finite subgroup
will be cyclic).
3.  Let Ck denote a cyclic group of order k. Show that if m and n are
coprime, then Cm × Cn is cyclic and hence isomorphic to Cmn. Show,
however, that C3 × C3 is not isomorphic to C9.
4.  Suppose that G is a group and that the only subgroups of G are the trivial
subgroup {e} and G itself. Show that G is a finite cyclic group.
5.  Let G be a cyclic group with exactly n elements. Show that G is
generated by every x ( ≠ e) in G if and only if n is a prime.
6.  Let G be a cyclic group of order n generated by, say, g. Show that if k
divides n, then there is one, and only one, subgroup of G of order k, and
that this is generated by gn/k. [This is a partial converse to Lagrange's
theorem.]
12.6 Applications to arithmetic
In this section we digress to show some simple applications of group theory
to arithmetic. Let n be a positive integer. Then a non-zero integer d divides
n, or is a divisor of n, if there is an integer k such that n = kd (equivalently,
if n/d is an integer). If this is so, we write d|n. An integer d is a common
divisor of integers m and n if d divides both m and n. The largest of the
finite number of the common divisors of m and n is the greatest common
divisor of m and n and this is denoted by gcd(m, n). Notice that, by
definition, gcd(m, n) does indeed divide both m and n. Finally, two integers
m and n are coprime if gcd(m, n) = 1; that is, if they have no common
divisor except 1. As examples, we see that gcd(24, 15) = 3, and gcd(15,
11) = 1. In both of these cases the greatest common divisor of the two

integers is an integral combination of them (2 × 24 − 3 × 15, and 3 × 15 − 4
× 11, respectively), and these are special cases of the following general fact.
Theorem 12.6.1 Given any positive integers p and q there are integers a
and b with gcd(p, q) = ap + bq.
Proof Let H = {mp + nq : m, n  }. It is clear that H is a subgroup of 
and so, by Theorem 12.5.2, there is a positive d such that
As d  H there are integers a and b such that d = ap + bq. As p  H there is
an integer k such that p = kd; thus d divides p. Similarly, d divides q and so,
as d is a common divisor of p and q, d ≤ gcd(p, q). Finally, as gcd(p, q)
divides p and q, it also divides d (= ap + bq). Thus gcd(p, q) ≤ d and hence
gcd(p, q) = d.
The following two results are immediate corollaries of Theorem 12.6.1.
Corollary 12.6.2 Any common divisor of p and q also divides gcd(p, q).
Corollary 12.6.3 The integers p and q are coprime if and only if there are
integers a and b with ap + bq = 1.
A similar argument enables us to discuss the least common multiple of
integers p and q: this is the smallest integer t such that both p and q divide t,
and it is denoted by lcm(p, q). Let
Now H and K are cyclic subgroups of 
, so their intersection H ∩ K is also
a cyclic subgroup of 
 (see Theorems 12.2.3 and 12.5.4). Thus, by 12.5.4).
Thus, by Theorem 12.5.2, there is an integer ℓ such that
We deduce that for some integers m1 and n1, m1 p = ℓ = n1q, and so p and q
both divide ℓ. On the other hand, if p and q divide t, say, then t  H ∩ K so
that ℓ ≤ t. We deduce that lcm(p, q) = ℓ, and hence that lcm(p, q) is the

generator of the intersection of the two subgroups generated by p and q. We
illustrate these ideas with an example.
Example 12.6.4 Suppose that p and q be coprime positive integers, and let
What is H? Clearly H is a subgroup of  and so H = {nd : n  } for some
positive integer d. As p and q are coprime, there are integers a and b with
ap + bq = 1. Thus 2ap + 2bq = 2 and, as 2a + 2b is even, this shows that 2 
H, and hence that H contains all even integers. Now consider the two cases
(i) p + q is odd, and (ii) p + q is even. If p + q is odd then H contains an odd
integer (namely p + q) so that in this case, d = 1 and H = . Now suppose
that p + q is even, say p + q = 2u, and take any mp + nq in H. As m + n is
even, mp + nq is also even; thus in this case d = 2 and H is the set of all
even integers.
Exercise 12.6
1.  Find the greatest common divisor, and the least common multiple, of 12,
18 and 60.
2.  What is the subgroup of  generated (i) by 530 and 27, and (ii) by 531
and 27 ?
3.  Show that for any integers p1, . . . , pn there are integers m1, . . . , mn such
that gcd(p1, . . . , pn) = m1 p1 + · · · + mn pn.
4.  A set with an associative binary operation with an identity is called a
monoid. An element x in a monoid is a unit if its inverse x−1 exists. Show
that the set of units of a monoid X is a group (even when X itself is not).
We give applications of this idea in the next four exercises.
5.  Show that the group of units of 
8 is {1, 3, 5, 7}. Is this cyclic? Find the
group of units of 
10.
6.  Show that 
n is a monoid (with respect to multiplication). Show that m
is a unit in 
n if and only if m is coprime to n. Euler's function φ(n) is
the order of the group of units of 
n (equivalently, the number of
positive integers m that are less than, and coprime to, n). Deduce that if a

is a unit, then aφ(n) is the identity in the group of units. Thus we have
Euler's theorem: if (a, n) = 1 then aφ(n) is congruent to 1 mod n.
7.  Use Euler's theorem to derive Fermat's theorem: if p is prime then, for
all integers a, ap is congruent to a mod p.
8.  Let X be a given set and let 
 be the set of functions f : X → X. Show
that 
 is a monoid with respect to the composition of functions. Show
that the units in 
 are precisely the bijections of X onto itself (thus the
group of units of 
 is the group of permutations of X).
12.7 Product groups
In this section we show how to combine two given groups to produce a new
'product' group. This idea can be used to produce new examples of groups,
and sometimes to analyse a given group by expressing it as a product of
simpler groups. Let G and G′ be any two groups. Then G × G′ is the set of
ordered pairs (g, g′) where g  G and g′  G′, and there is a natural way of
combining two such pairs (g, g′) and (h, h′), namely
(12.7.1)
Notice that we only need to be able to combine two elements in G, and two
elements in G′, and that G and G′ need not be related in any.
Theorem 12.7.1 The set G × G′ with this operation is a group.
Proof The rule (12.7.1) guarantees that (g, g′)(h, h′) is in G × G′. Next, the
verification of the associative law is straightforward:

The identify element of G × G′ is (e, e′), where e and e′ are the identities in
G and G′, for (e, e′)(g, g′) = (eg, e′g′) = (g, g′), and similarly, (g, g′) = (g, g′)
(e, e′). Finally, the inverse of (g, g′) is (g−1, (g′)−1).
Of course, the rule (12.7.1) extends to any number of factors, and in this
way we can construct a group G1 × · · · × Gn from groups G1, . . . , Gn. We
now show how this construction is used to provide examples of groups.
Example 12.7.2 Consider the group  × . Then
and this is the Euclidean plane 
 with vector addition. In a similar way, 
 × · · · ×  (with n factors) is the additive group 
.
Example 12.7.3 Let G2 = {1, −1} and G3 = {1, w, w2}, where both are
multiplicative groups of complex numbers, and w = e2πi/3. Now both G2 and
G3 are cyclic, and we shall show that G2 × G3 is a cyclic group of order six
that is generated by the pair (−1, w). Indeed, as is easily checked, the first
six powers of this element are (−1, w), (1, w2), (−1, 1), (1, w), (−1, w2) and
(1, 1), and as these six elements are distinct, G2 × G3 is cyclic.
Example 12.7.4 Now let G2 be as in Example 12.7.3, and let G4 = {1, i, −1,
−i}, where both are multiplicative cyclic groups of complex numbers. Now
G1 × G2 has order eight but, in contrast to Example 12.7.3, we shall see that
it is not cyclic. A cyclic group of order eight is generated by some element
g of order eight. However, every element of G2 × G4 has order at most four,
because if (g, h)  G2 × G4, then (g, h)4 = (g4, h4) = (1, 1).
Example 12.7.5 Let G1 be the multiplicative group {z   : |z| = 1}, and let
G2 = 
. We shall identify 
 with the horizontal coordinate plane in 
;
then G1 × G2 is identified with the vertical cylinder

in 
. According to Theorem 12.7.1, this cylinder can be given the structure
of a group whose rule of combination is (z, t)(w, s) = (zw, t + s). Note that
the map (z, t) 
 (eiθ, ℓ)(z, t) corresponds to a rotation of C by an angle θ
about its vertical axis followed by a vertical (upwards) translation of ℓ (this
is a screw-motion).
Exercise 12.7
1.  Show that if G has p elements and G′ has q elements, then G × G′ has pq
elements.
2.  Show that G × G′ is abelian if and only if both G and G′ are abelian.
3.  Let G be the group of all isometries of  consisting of all real
translations, and all glide reflections with axis . Show that G is
isomorphic to {1, −1} × .
4.  The surface of a torus T (a doughnut ring) can be obtained by rotating a
circle in 
 about a line lying in the plane of the circle (and not meeting
the circle). It follows that the points on the torus can be parametrised by
two coordinates (eiθ, eiø), where θ and ø are real. Show how to make the
torus T into a group.
12.8 Dihedral groups
The dihedral group D2n, where n ≥ 2, is any group that is isomorphic to the
group of symmetries of a regular polygon with n sides (note: some authors
use Dn for this group). This section is devoted to a geometric, and an
algebraic, description of D2n. Consider a regular polygon 
 with n sides.
By translating, rotating, and scaling 
 we may assume that the set V of
vertices of 
 is the set of n-th roots of unity; thus V = {1, ω, . . . , ωn−1},
where ω = exp(2πi/n). If n = 2, then  is the segment from −1 to 1.
A symmetry of 
 is an isometry of 
 that maps the set V (or,
equivalently, 
) onto itself, and D2n is the group of symmetries of 
 (we
leave the reader to prove that this is a group with the usual composition of
functions). Now if an isometry of 
 leaves the set V unchanged then it
necessarily fixes 0 (for 0 is the only point of 
 that is a distance one from
each of the vertices). As any isometry of 
 is of the form 
, it follows that each element of D2n is either a

rotation z 
 az, or a reflection z 
 a  in some line through the origin,
where, in each case, |a| = 1. It is clear that there are exactly two maps in D2n
that map 1 (in V) to a chosen point of V, say ωk, namely z 
 ωkz and z 
ωk , and we deduce from this that D2n has order 2n.
Now let Rn be the subgroup of symmetries of 
 that are rotations; then
Rn is the cyclic group of order n and generated by r, where r(z) = ωz and
rn = I. Next, let σ(z) = . Then σ is in D2n but not Rn, so that D2n contains
the two disjoint cosets Rn and σ Rn. As these cosets have n elements each,
we conclude that D2n is the union of these two (disjoint) cosets. A similar
argument shows that D2n is also the union of the two disjoint cosets Rn and
Rnσ; thus
As Rn is disjoint from each of the cosets σ Rn and Rnσ, we see that σ
Rn = Rnσ; thus σr = raσ for some a. In fact, the elements r and σ (which
generate D2n) are related by the special relation
which can be verified directly from r(z) = ωz and σ(z) = . To summarize,
D2n is a group of order 2n that is generated by two elements r and σ which
are subject to the relations
(12.8.1)
These are not the only relations satisfied by r and σ ; in fact, σ R = R−1σ for
any rotation R about the origin.
Example 12.8.1 Let us briefly consider the symmetries of a square. We
may locate the square so that its vertices are 1, i, −1 and −i, and we now
relabel these vertices by the integers 1, 2, 3 and 4, respectively. Now any
symmetry of the square can be written as a permutation of {1, 2, 3, 4} and
this is an efficient way to describe the geometry. We have seen that D8 is
generated by r and σ, where r(z) = i z and σ(z) = . In terms of permutations,
r = (1 2 3 4), and σ = (2 4). We know from the discussion above that there

are exactly four reflective symmetries of the square, namely σ, σr, σr2 and
σr3; in terms of permutations, these are (2 4), (1 4)(2 3), (1 3) and (1 2)(3
4), respectively. Of course, r2 = (1 3)(2 4) and r3 = (1 4 3 2). As σ2 = I and
r4 = I, it may be tempting to think that D8 is isomorphic to C2 × C4;
however, this is not so as C2 × C4 is abelian whereas D8 is not.
We have said that D2n is a group of order 2n that is generated by two
elements r and σ which are subject to the relations (12.8.1). In fact, this
definition of D2n is incomplete until we have established the next result.
Theorem 12.8.2 There is only one group G of order 2n that is generated by
two elements a and b, of orders 2 and n, respectively, with ab = b−1a.
Proof Each element of G is a finite word in the elements a, a−1, b and b−1;
for example, aab−1a−1bbba. In fact, as a−1 = a and b−1 = bn−1, every
element of G is a word in a and b. Suppose such a word w contains within it
the consecutive terms ab, say w = xaby for some x and y. Then w = xbn−1ay,
and this particular occurrence of a has been moved to the right (passing b in
the process). By repeating this operation as many times as is necessary, we
can move all the occurrences of a to the extreme right of the word w (the
powers of b will change, but this does not matter), and so we see that any
element of G can be written in the form bpaq for some non-negative integers
p and q. As bn = a2 = e, and G has 2n elements, it is clear that G = {buav: 0
≤ u ≤ n, v = 0, 1}. It is also clear that any two groups of this form, with the
same relations, are isomorphic to each other.
Definition 12.8.3 The dihedral group D2n is the group of order 2n that is
generated by an element r of order n, and an element σ of order two, where
σr = r−1σ.
It is clear that D2n is generated by the two elements rσ and σ, both of order
two. There is a converse to this.

Theorem 12.8.4 Suppose that G is a finite group that is generated by two
(distinct) elements of order two. Then G is a dihedral group.
Figure 12.8.1
Proof We suppose that G is generated by x and y, where x2 = e = y2. In order
to match our earlier notation, we let r = xy and σ = x. As G is finite, r has
some finite order n, say. Then G is generated by r and σ, where rn = e = σ2.
Moreover, σr = y = y−1 = r−1σ, which is the other relation that needs to be
satisfied in a dihedral group. According to Theorem 12.8.2, it only remains
to show that G has order 2n. Now we can argue exactly as in the proof of
Theorem 12.8.2, and deduce that
If these 2n elements are distinct, then G is of order 2n and we are finished.
If not, then two are equal to each other and we must then have rp = σq ≠ e
for some p and q with 0 < p < n. Now q = 1 (else rp = e), and this means
that (xy)p = x. We shall complete the proof in the case when p = 3, but the
same method will obviously work for any p. If p = 3 we have
xyxyxyx = (xy)3x = e. Now xyxyxyx = e implies that yxyxy = e, then xyx = e
and, finally, that y = e which is false.
There is a significant difference between the groups D2n for even n and
the groups D2n for odd n, and this is most easily explained in terms of the
geometry of the regular n-gon. After sketching a few diagrams it should be
clear that when n is odd each of the n reflections in D2n fixes exactly one
vertex of the polygon. By contrast, when n is even, exactly half of the
reflections fix two vertices, and the remaining reflections fix none (see

Figure 12.8.1). We shall examine this difference from a group-theoretic
point of view in Theorem 12.10.7.
Exercise 12.8
1.  Show that D2n is abelian if and only if σ and r commute. Deduce that
D2n is abelian if and only if n = 2, and hence that D2n is isomorphic to C2
× Cn (where Ck denotes a cyclic group of order k) if and only if n = 2.
2.  As in the text, let σ(z) =  and r(z) = ωz, where ω = exp(2πi/n). Show
analytically that if n is odd then every reflection rℓσ in the group fixes
exactly one vertex of , while if n is even, then exactly half of the
reflections rℓσ fix two vertices of  whereas the other half do not fix any
vertex of .
3.  Show that D4 is isomorphic to the group {I, , -z, - } of isometries of .
This group is known as the Klein 4-group.
4.  For which values of n does the dihedral group Dn contain an element
(not the identity) that commutes with every element of Dn?
5.  Show that the only finite abelian groups of isometries of  are the cyclic
groups of rotations and groups that are isomorphic to the Klein 4-group.
12.9 Groups of small order
In this section we shall use the theory we have developed so far to find all
groups of order at most six; by this we mean that we want to produce a list
of groups such that any group of order at most six is isomorphic to one of
the groups in our list. By Theorem 12.5.5, every group of prime order is
cyclic, and as any two cyclic groups of the same finite order are isomorphic
to each other, there is only one group of order p, where p = 2, 3, 5, 7, 11, 13,
. . . . Nevertheless, it is still of interest to see how one can analyze groups of
small order from 'first principles', and we shall illustrate this by
considering groups of order three.
Example 12.9.1 Let G be a group of order three, say G = {e, a, b}. Now
ab ≠ a (else b = e) and ab ≠ b (else a = e); thus ab = e and b = a−1. It
follows that b = ba3 = a2, so that G = {e, a, a2}, a cyclic group of order
three.

Next, we show that to within an isomorphism there are exactly two groups
of order four.
Theorem 12.9.2 Every group of order four is either cyclic or is isomorphic
to the dihedral group D4. In particular, every group of order four is abelian.
Proof Let G be a group of order four. By Theorem 12.3.3, every element of
G (other than e) has order two or four. If there is an element, say g, of order
4, then G is the cyclic group {e, g, g2, g3} generated by g.
Suppose now that G is not cyclic. Then every element of G has order two
and, by Theorem 12.1.4, G is abelian. Thus a2 = b2 = c2 = e, and
ab = ba = c, and so on, and it is clear that G is isomorphic to D4.
Finally, we consider groups of order six; in fact, we shall prove the
following result which includes these groups as a special case.
Theorem 12.9.3 Let G be a group of order 2p, where p is a prime. Then G
is either cyclic, or isomorphic to the dihedral group D2p.
Proof By Theorem 12.9.2, we may assume that p ≥ 3, and hence that p is
odd. As p is prime, every element of G (other than e) has order 2, p, or 2p
(Theorem 12.3.3). Theorem 12.1.5 implies that G contains an element a of
order two. However, not every element of G has order 2 for if it does then,
by Theorem 12.1.4, 2 p = |G| = 2n for some n, which is false as p is odd.
Thus G must contain an element of order p, or an element of order 2p, and
in both cases, G contains an element, say b, of order p.
Let H = {e, b, b2, . . . , bp−1}, a cyclic subgroup of G of order p. Next, we
show that a  H. If a  H then a = bt, say, so that b2t = e. This implies that
show that p divides t, and hence that a = bt = e which is false. It follows that
a  H and so if we write G in terms of its coset decomposition, we have
so that aH = Ha. We deduce that aHa−1 = H, so there is some integer k in
{0, 1, . . . , p − 1} such that aba−1 = bk. As a2 = e, this implies that aba = bk,
and hence that

so that bk2−1 = e. As b has order p we see that p divides (k − 1)(k + 1) and,
as 0 ≤ k < p, we conclude that k = 1 or k = p − 1. These two cases
correspond to ba = ab, and ab = b−1a, respectively. In the first case,
(ba)m = bmam so that ba has order 2 p (the lowest common multiple of 2 and
p) and G is cyclic. In the second case, G is of order 2p and is generated by b
and a, where b has order p, a has order two, and ab = b−1a; thus G = D2p.
Exercise 12.9
1.  Show that the set {2, 4, 8} with the operation of multiplication modulo
14 is a group of order three. What is the identity element?
2.  Show that the set {1, 3, 5, 7}, with multiplication modulo 8 is group of
order four. Is this group isomorphic to C4 or to D4?
3.  Let G be the product group C2 × C3 (of order six), where Cn denotes the
cyclic group of order n. Is G isomorphic to C6 or to D6?
4.  Let G be the multiplicative group of non-zero elements in 
7. Is G
isomorphic to C6 or to D6?
5.  What is the smallest order a non-abelian group can have?
12.10 Conjugation
We begin with the following geometric problem. Suppose that RL is the
reflection across a line L in the complex plane , and that f is an isometry
of . What is the formula for the reflection Rf(L) across the line f(L)? To
answer this, consider the mapping f RL f−1. This mapping is an isometry, and
it fixes every point of the form f(z), where z 
 L. Thus f RL f−1 is an
isometry that fixes every point of f (L) and so it is either the identity I or R
f(L). Now f RL f−1 ≠ I (else RL = I ), so f RL f−1 = Rf(L). This is illustrated in
Figure 12.10.1.
A similar argument shows that if f(z) = eiθ z, a rotation about the origin,
and if g(z) = z − a, then gfg−1 is a rotation about the point a in . Finally,
we have seen earlier that if a linear transformation is represented by a
matrix A with respect to one basis in a vector space, then it is represented

by a matrix of the form BAB−1 with respect to another basis. These are all
examples of conjugacy in a group, and this is a very fruitful idea even in an
abstract group.
Definition 12.10.1 Let G be a group and suppose that f and g are in G. We
say that f and g are conjugate in G, or that f is a conjugate of g, if there is
some h in G such that f = hgh−1. The conjugacy class of g is the set {hgh−1 :
h  G} of all conjugates of g, and we shall denote this by [g]. Finally,
subgroups H1 and H2 are conjugate subgroups of G if, for some h in G,
H1 = hH2h−1.
Some elementary remarks will help the idea to settle, and we leave the
reader to provide the proofs. First, if f and g are conjugate, say f = hgh−1,
then fn = (hgh−1)n = hgnh−1, so that fn = e if and only if gn = e; thus
conjugate elements have the same order. Next, as heh−1 = e, we see that
[e] = {e}, and as as g = ege−1, we see that g  [g]. If G is abelian, then
[g] = {g}. More generally, [g] = {g} if and only if g commutes with every
element of G. This last observation tells us that the conjugates of g are
giving us important information about the element g (for example, the more
elements g commutes with, the smaller the conjugacy class [g] is). It seems
natural, then, to extend this idea to subgroups, and this turns out to be one
of the most important ideas in group theory.
Figure 12.10.1

Definition 12.10.2 A subgroup H of G is normal, or self-conjugate, if every
conjugate subgroup of H is equal to H; that is, if gHg−1 = H for every g in
G.
We shall develop the theory of normal subgroups in the rest of this chapter.
There is an immediate consequence of this definition which relates normal
subgroups and cosets. We recall that if H is a subgroup of G, then the left
coset gH need not be the same as the right coset Hg.
Theorem 12.10.3 let H be a subgroup H of a group G; then the following
are equivalent:
(a) H is normal;
(b) g H = Hg for every g in G;
(c) every left coset is a right coset;
(d) every right coset is a left coset.
Proof It follows immediately from Definition 12.10.2 that H is normal if
and only if gH = Hg for every g, and this implies that every left coset is a
right coset. Now suppose that every left coset is a right coset, and take any
left coset gH. Then there is some f in G such that gH = H f. As g  gH we
see that g  H f so that H f ∩ Hg ≠ . As any two right cosets are disjoint or
equal, we have H f = Hg so that gH = Hg. This proves that (a), (b) and (c)
are equivalent, and a similar argument can be used for (d).
We recall that given a subgroup H of G, the group G is partitioned by its
cosets into a disjoint union of sets. The conjugacy classes provide a similar
splitting of G.
Theorem 12.10.4 The relation of conjugacy in a group G is an equivalence
relation; thus G is the disjoint union of mutually disjoint conjugacy classes.
Proof Let us write f ~ g to mean that f is conjugate to g; that is, f = hgh−1 for
some h or, equivalently, [f] = [g]. If f = hgh−1 then g = h−1 f h so that f ~ g
implies that g ~ f. Next, as g = ege−1 we see that g ~ g. Finally, suppose that
f = ugu−1 and g = vhv−1; then f = (uv)h(uv)−1, so that f ~ g and g ~ h implies

f ~ h. These facts show that ~ is an equivalence relation on G, and hence the
equivalence classes partition G as described in Theorem 12.4.5.
The conjugacy classes of elements in G can be used to determine whether
a given subgroup of G is normal or not.
Theorem 12.10.5 A subgroup H of G is a normal subgroup if and only if it
is a union of conjugacy classes.
Proof Suppose that H is a normal subgroup of G, and take any element h of
H. Then, for every g in G, gHg−1 = H so that ghg−1  H. This shows that
[h] = {ghg−1 : g  G} ⊂ H, so that H is the union of the conjugacy classes
[h] taken over all h in H. Conversely, suppose now that H is a union of
conjugacy classes. Then, for any h in H, [h] ⊂ H. Now for all g in G, ghg−1
 [h] so that if h  H then ghg−1  H. Thus for all g in G, gHg−1 ⊂ H. If we
now replace g by g−1, we see that g−1 Hg ⊂ H and hence that H ⊂ gHg−1.
We deduce that gHg−1 = H for all g in G, so that H is a normal subgroup of
G.
We end this section with two applications of the idea of conjugacy. The
first of these is to permutation groups; the second is to dihedral groups, and
this provides the algebraic account of the distinct between D2n with n even
and n odd that was discussed in the last paragraph in Section 12.8.
Theorem 12.10.6 Two permutations of {1, . . . , n} are conjugate in Sn if
and only if they have the same cycle type.
Proof We need to define the term cycle type, and we shall be content here
with a definition and a sketch of the proof. The idea of the proof is
competely straightforward, and a formal proof is unlikely to help. Let σ be a
permutation of {1, . . . , n}; then σ is a product, say ρ1 · · · ρr, of disjoint
cycles. Suppose that the cycle ρj has length ℓj; the cycle type of σ is the
vector (ℓ1, . . . , ℓr), where (because the ρj commute) this is only determined
up to a permutation of its entries. For example, the two permutations (1 2)(3
4 5) and (1 2 3)(4 5) have the same cycle type.

Let us consider a typical cycle, say ρ = (a b c)(d e). Then (by inspection)
for any permutation μ,
Thus the conjugate cycle μρμ1 has the same cycle type as ρ. To prove the
converse, we consider two permutations of the same cycle type and create a
bijection to show that they are conjugate; we omit the details.
Theorem 12.10.7 The dihedral group D2n has one conjugacy class of
reflections if n is odd, and two conjugacy classes if n is even.
Proof The group D2n is generated by the rotation r(z) = e2πi/n z and the
reflection σ(z) =  in the real axis, and these satisfy rmσ = σr−m for every
integer m. Now D2n contains exactly n reflections, namely σ, rσ, . . . , rn−1σ,
and the result follows from the following three facts:
(a)  if k is even then rkσ is conjugate to σ;
(b)  if k is odd then rkσ is conjugate to rσ;
(c)  rσ is conjugate to σ if and only if n is odd.
First, if k = 2q, say, then rkσ = rq(rqσ) = rq(σr−q) = rqσr−q, which proves (a).
Next, if k = 2q + 1, then rkσ = (rq+1)(rqσ) = rq(rσ)r−q, which proves (b).
Finally, if n = 2m − 1, then rmσr−m = r2mσ = rn+1σ = rσ. Conversely, if
rσ = rpσr−p for some p, then rσ = r2pσ so that r2p−1 = e. Thus n divides
2p − 1 so that n must be odd.
Exercise 12.10
1.  Suppose that H is a subgroup of order m in a group G of order 2m. Show
that if g  G but g  H, then G = H ∪ gH = H ∪ Hg. Use this to show
that H is a normal subgroup of G. Deduce that the alternating group An is
a normal subgroup of the permutation group Sn. Now verify this by using
Theorem 12.10.6.

2.  Let α = (1 2 3 4)(5 6) and β = (2 4 6 3)(1 5) be elements of S6. Construct
an explicit element σ of S6 such that β = σ ασ−1.
3.  Suppose that m < n and regard the permutation group Sm of {1, . . . , m}
as a subgroup of Sn. How many transpositions are in Sm, and how many
in Sn? Show that Sm is not a normal subgroup of Sn.
4.  Suppose that G is a multiplicative group of n × n real matrices, and that
for every A in G, det(A) = ±1. Show that the subgroup H of matrices A
for which det(A) = 1 is a normal subgroup of G.
5.  Let 1, 2, 3 and 4 be a labelling of the vertices of a square S in  so that
any symmetry of S can be regarded as a permutation of {1, 2, 3, 4}. In
this sense, D8 is a subgroup of S4. Show that D8 is not a normal
subgroup of S4.
6.  Let G be a group, and let D (the 'diagonal' group) be the subgroup {(g,
g) : g  G} of G × G. Show that D is a normal subgroup of G × G if and
only if G is abelian.
7.  Show that every subgroup of rotations in a dihedral group Dn is normal
in Dn.
12.11 Homomorphisms
We begin by recalling that a map θ : G → G′ is an isomorphism between the
groups G and G′ if
1. θ is a bijection of G onto G′ and
2. for all g and h in G, θ(gh) = θ(g)θ(h).
A homomorphism between groups is a map that satisfies (2) but not
necessarily (1). In particular, while the inverse of an isomorphism is an
isomorphism, the inverse of a homomorphism may not exist.
Definition 12.11.1 A map θ : G → G′ between groups G and G′ is a
homomorphism if θ(gh) = θ(g)θ(h) for all g and h in G.
Although the inverse of a homomorphism may not exist, a homomorphism
does share some properties in common with isomorphisms. For example, if
θ : G → G′ is a homomorphism, and if e and e′ are the identities in G and
G′, respectively, then θ(e) = θ(e2) = θ(e)θ(e), so that θ(e) = e′. Thus a

homomorphism maps the identity in G to the identity in G′. Similarly, as
e′ = θ(e) = θ(gg−1) = θ(g)θ(g−1), we see that for all g in G,
The most obvious example of a homomorphism is a linear map between
vector spaces. Indeed, a vector space is a group with respect to addition,
and as any linear map α : V → W satisfies
for all u and υ in V, it is a homomorphism from V to W. The fact that the
kernel is an important concept for vector spaces suggests that we should
seek a kernel of a homomorphism between groups. The kernel of a linear
map α : V → W is the set of vectors υ which map to the identity element
(the zero vector) in the additive group W, so the following definition is
quite natural.
Definition 12.11.2 The kernel ker(θ) of a homomorphism θ : G → G′ is the
set {g  G : θ(g) = e′}, where e′ is the identity element of G′.
The next result is predictable (recall that the kernel of a linear map α : V →
W is a subspace of V).
Theorem 12.11.3 Let θ : G → G′ be a homomorphism. Then ker(θ) is a
subgroup of G.
Proof Let K = ker(θ) = {g  G : θ(g) = e′}. We have seen above that e  K,
so that K is non-empty. If g and h are in K then θ(gh) = θ(g)θ(h) = e′e′ = e′
so that gh  K. Finally, if g  K then
so that g−1  K . The result now follows from Theorem 12.2.2.
Let us illustrate these ideas in an example.

Example 12.11.4 Consider the group 
* of nonzero complex numbers
under multiplication, and let θ(z) = z/|z|. As θ(zw) = θ(z)θ(w), we see that θ
is a homomorphism of 
* onto the group {z : |z| = 1}, and its kernel
consists of all non-zero z such that z = |z|. Thus the ker(θ) is the subgroup 
 of positive real numbers. Consider also the map φ : * → 
 given by
φ(z) = |z|. Then φ is a homomorphism whose kernel is the group {z : |z| = 1},
We recall that a subgroup H of a group G is a normal subgroup if and
only if gH = Hg for every g. We know that not every subgroup is normal,
and here is a simple geometric example of a non-normal subgroup.
Example 12.11.5 Let G be the group {x 
 εx + n : n  
, ε = ±1} of
isometries of , and let H be the subgroup {g  G : g(0) = 0}. Then H = {I,
h}, where I is the identity and h(x) = −x. Now let f(x) = x + 1; then f H = {f,
f h} ≠ {f, h f} = H f.
The following result is extremely important.
Theorem 12.11.6 Let θ : G → G′ be a homomorphism with kernel K. Then
K is a normal subgroup of G.
Proof Consider any k in K. Then, for any g in G,
so that gkg1  K. As k was any element in K, this shows that gKg−1 ⊂ K,
and hence that gK ⊂ Kg. As this holds for all g, we also have g−1K ⊂ Kg−1,
equivalently Kg ⊂ gK, so we deduce that gK = Kg.
The fact that the kernel of a homomorphism is a normal subgroup,
enanbles us to take another result for vector spaces and provide an
analogous result for groups. The reader will recall that if α : V → W is a
linear map then, for a given w in W, the set of solutions v of α(v) = w (if
any) are of the form v0 + K, where K is the kernel of α. Exactly the same
result is true for homomorphisms between groups (see Figure 12.11.1).

Figure 12.11.1
Theorem 12.11.7 Suppose that θ : G → G′ is a homomorphism with kernel
K, and that y  G′. Suppose also that x0  G and that θ(x0) = y. Then the set
of solutions x of the equation θ(x) = y is the coset x0K.
Proof First, every element of x0K is a solution to the given equation, for the
general element of x0K is x0k, where k  K, and θ(x0k) = θ(x0)θ(k) = ye′ = y.
Next suppose that θ(x) = y, and consider the element 
 in G. Then
so that 
. This means that x  x0K and so the set of solutions of
θ(x) = y is the coset x0K.
Let us apply Theorem 12.11.7 to a situation that the reader is already
familiar with.
Example 12.11.8 Let * be the multiplicative group of non-zero complex
numbers, and let θ(z) = zn. As (zw)n = znwn, θ is a homomorphism of G into
itself, and the kernel of θ is the group K of n-th roots of unity. According to
Theorem 12.11.7, the solutions of the equation θ(z) = w, that is, the equation
zn = w, are simply the points of the form z1ζ, where z1 is some n-th root of
w, and ζ is any n-th root of unity. Of course, we know this but the point is to
illustrate Theorem 12.11.7.
It is a remarkable fact that the converse of Theorem 12.11.6 holds; that is,
not only is the kernel of a homomorphism a normal subgroup, but every

normal subgroup is the kernel of a homomorphism. We shall state this as a
theorem now; it will be proved in the next section.
Theorem 12.11.9 Let H be a subgroup of G. Then H is a normal subgroup
of G if and only if there is some group G′, and some homomorphism θ : G
→ G′, whose kernel is H.
We end with another example.
Example 12.11.10 Consider the permutation group Sn of {1, 2, . . . , n}, and
the alternating group An (of all even permutations in Sn). If σ is an even
permutation, then σ An = An = Anσ; if σ is an odd permutation, then An ∪ σ
An = Sn = An ∪ Anσ, so that again, σ An = Anσ. We deduce that σ An = Anσ
for every σ; thus An is a normal subgroup of Sn. Here is an alternative proof.
The map ε : Sn → {1, −1}, where ε(ρ) is the sign of ρ, is a homomorphism
from Sn to {1, −1}, for ε(αβ) = ε(α)ε(β). Its kernel is the set of ρ such that
ε(ρ) = 1 and this is An; thus An is a normal subgroup of Sn.
Exercise 12.11
1.  Let G and H be groups, and let θ : G × H → G be the map θ(g, h) = g.
Show that θ is a homomorphism. What is its kernel?
2.  Show that if H is a subgroup of an abelian group G, then H is a normal
subgroup of G. In particular, every subspace U of a vector space V is a
normal subgroup of V. Find a homomorphism (a linear map) of V into
itself with kernel U.
3.  Show that there are exactly two homomorphisms from C6 to C4.
4.  Show that the alternating group An is a normal subgroup of Sn. More
generally, prove that if a group G of order 2n, and if H is a subgroup of
order n, then H is a normal subgroup of G.
5.  Let G be the group of similarities of the complex plane of the form
f(z) = az + b, where a ≠ 0, and let * be the multiplicative group of non-
zero complex numbers. Show that the map ϕ(f ) = a is a homomorphism
of G onto *. What (in geometric terms) is its kernel K?
6.  Suppose that θ : G → H is a homomorphism. Show that, for any g in G,
the order of θ(g) divides the order of g.

7.  Suppose that a subgroup H of G has the property that there is no other
subgroup of G that is isomorphic to H. Show that H is a normal
subgroup of G.
8.  Let m be a positive integer, and let G be an abelian group. Show that the
map θ : G → G defined by θ(g) = gm is a homomorphism. Deduce that
the set of elements of order m in G is a normal subgroup of G.
12.12 Quotient groups
This section contains the most important idea about groups that is in this
text. We begin with two examples; then we develop the theory.
Example 12.12.1 Let G be the additive group of vectors 
, and let H be
the subgroup given by x3 = 0 (the horizontal coordinate plane). We consider
two planes that are parallel to H, say x + H and y + H, and attempt to 'add'
these by the rule
Does this make any sense, and if it does, is it useful? First, if we choose
planes H′ and H″ parallel to H we can certainly write them in the form
H′ = x + H and H″ = y + H, but there is a problem because the x and y here
are not uniquely determined by the planes. Suppose we write them in
another way, say H′ = x1 + H and H″ = y1 + H. Then, in order that these two
representations of H′ are the same, we must have x1 − x  H and, similarly,
y1 − y  H″. Suppose that x1 − x = h′ and y1 − y = h″; then
so that this 'addition' of planes (parallel to H) is properly defined, and the
'sum' is again a plane parallel to H. Note that H′ + H = H′ = H + H′, so that
H acts as the identity for this addition. It is also worth noting that among the
class of all planes that are parallel to H, the plane H itself is the only one of
these planes that is a subgroup of 
. It is not difficult to see that the set of

these planes, with the addition defined above, is a group, with identity H,
and the inverse of x + H is (−x) + H.
Continuing with this example, consider now the map θ : 
 → 
 given
by θ(x1, x2, x3) = x3. It is easy to check that this map is a homomorphism of 
 onto the 'vertical' axis, say Z and, of course, each plane H′ is mapped by
θ onto the point H′ ∩ Z. The addition of planes corresponds precisely to the
addition along Z, and H is the kernel of the homomorphism θ.
Let us summarize what we have found in this example:
(a)  the plane H is a subgroup of the group 
;
(b)  The cosets x + H of H can be added together to form a group;
(c)  there is a homomorphism of 
 onto another group Z (here, the third
axis) whose kernel is H;
(d)  Z is isomorphic to the group of cosets of H.
The next example is from number theory, but the conclusions are the
same.
Example 12.12.2 Consider the additive group 
 of integers, and the
subgroup H that is the set of multiples of a given integer k. Now let θ :  →
k be the map taking an integer n to its remainder modulo k (if n = ak + b,
where 0 ≤ b < k, then θ(n) = b). Clearly, the kernel of θ is H, and the cosets
of H may be regarded as the elements of 
k; for example,
The addition of cosets obviously 'mirrors' the addition in 
k, and the four
conclusions drawn at the end of Example 12.12.1 are valid here.
We are now going to show that the examples we have just examined are
typical of the situation for a general group with just one proviso: in the
examples above, the groups are abelian, but in the general case we have to
compensate for G failing to be abelian by insisting that the subgroup H
shall be a normal subgroup. If H is a normal subgroup of G we can obtain
all of these four conclusions, but some considerable effort is needed, and
this effort is the culmination of our work on groups.

Consider a homomorphism θ from a group G into a group G′, and let
θ(G) be the image of G; that is θ(G) = {θ(g) : g  G}, and let K be the
kernel of θ. The first step is to prove the following result (which is
suggested from our results on linear maps between vector spaces).
Theorem 12.12.3 Suppose that θ : G → G′ is a homomorphism. Then the
image θ(G) is a subgroup of H.
The proof follows from Theorem 12.2.2. We know that θ(G) contains the
identity e′ of G′. If g′ and h′ are in θ(G), then we may write g′ = θ(g) and
h′ = θ(h); then g′h′ = θ(g)θ(h) = θ(gh), so that g′h′ 
 θ(G). A similar
argument shows that (g′)−1  θ(G).
Next, we make an elementary, but useful, observation. Suppose that G is
a group and that φ is a bijection of a set X onto G. Then X inherits the
structure of a group from G by the definition
(that is to combine x and y, we map them to G, combine their images in G
and then map the result back into X). The proof of this is entirely
straightforward and is left to the reader. Notice that if we make X into a
group in this way, then φ is an isomorphism from X to G.
Let us now apply this to the situation considered above in which θ is a
homomorphism from G to G′, θ(G) is the image of G, and K is the kernel of
θ. Theorem 12.11.7 says that if y  θ(G) then the set of g in G such that
θ(g) = y is some coset x0K. It follows that θ acts as a bijection, namely x0K
 y = θ(x0) from the set  of all these cosets onto the group θ(G). Note
carefully that  is the set whose elements are the cosets gK; thus  = {gK ;
g  G} (and this is not the same as the union ∪ggK of the cosets, for this
union is G; see Figure 12.11.1).
Because θ acts as a bijection between  and θ(G) it follows (from the
observation above) that the set  of cosets inherits a group structure from
the group θ(G). What, then, is the rule of composition in this group  of of
cosets? According to the remarks made at the start of this section, this rule
is given by

(12.12.1)
As θ(g)θ(h) = θ(gh) the set on the right in (12.12.1) contains the element gh,
and it a coset (as we know that * is a group operation that combines left
cosets to give a left coset). It must therefore be the coset ghK, so the rule of
composition of left cosets is now seen to be
(12.12.2)
We state this as a theorem (and the last statement in this should be clear).
Theorem 12.12.4 Suppose that θ : G → G′ is a homomorphism with kernel
K. Then the set G/H of left cosets {gK ; g  G} is a group with respect to
the operation (gK)*(hK) = ghK. The identity in G/H is eK (which is K), and
G/H is isomorphic to θ(G).
As the kernel K of the homomorphism is a normal subgroup, we see that
gK = Kg for every g so that it does not matter whether we use left cosets or
right cosets in this argument. The group of cosets is important and is given
a name.
Definition 12.12.5 The group of cosets given in Theorem 12.12.4 is the
quotient group G/K .
Theorem 12.11.9 (which we have yet to prove) asserts that a subgroup H of
a group G is normal if and only if it is the kernel of a homomorphism.
Theorem 12.11.6 shows that if a subgroup H is the kernel of a
homomorphism then it is normal, so we only have to prove the reverse
implication, namely that if a subgroup H is normal, then it is the kernel of a
homomorphism. We can now complete this proof.
The Proof of Theorem 12.11.9 Suppose that a subgroup H of G is normal.
Then, by Theorem 12.12.4, the quotient group G/H exists. Now define the
map θ : G → G/H to be the map θ(g) = gH. Now θ is a homomorphism, for

as required. Finally, what is the kernel K of θ? Now g  K if and only if θ(g)
is the identity in G/H, and by Theorem 12.12.4, this identity is H. Thus g 
K if and only if gH = H, and this is so if and only if g  H. We have thus
shown that H = K, and hence that H is the kernel of the homomorphism θ.
According to Theorem 12.11.9, we can now rewrite Theorem 12.12.4 in
the following equivalent form.
Theorem 12.12.4A Suppose that K is a normal subgroup of G. Then the set
of left cosets {gK ; g  G} is a group with respect to the operation (gK)*
(hK) = ghK.
Exercise 12.12
1.  Suppose that H is a (cyclic) subgroup of order m of a cyclic (abelian)
group G of order n. What is G/H?
2.  Let H = {I, (1 2)(3 4), (1 3)(2 4), (1 4)(2 3)}. Show that H is a normal
subgroup of S4, so that S4/H has order six. Write down the six cosets
with respect to H. Is S4/H is isomorphic to C6 or to D6?
3.  Let K be a normal subgroup of a group G, and let A and B be left cosets
of K (we deliberately write these in this way as there is no unique way to
write a coset in the form gK). We want to define a product AB, and we
can try to do this by writing A = gK, B = hK and AB = ghK. Hoewever, in
order to make this 'definition' legitimate, we must show that if gK = g′K
and hK = h′K then g′h′K = ghK. Show that this is so.
4.  Show that /  is an infinite group in which every element has finite
order.
5.  Suppose that H is a normal subgroup of a group G, and that the quotient
group G/H has order n. Prove that for every g in G, gn  H.
6.  Let G be an abelian group and let K be the set of elements of G that have
finite order. Show that no elements of G/T (except its identity) have
finite order.
7.  Show that the quotient group /  is isomorphic to the 'circle group' {z 
  : |z| = 1}. [Hint: the homomorphism here is x 
 e2πix, and you may
assume standard properties of the exponential function].
8.  Consider the additive group  and the subgroup Γ consisting of all
Gaussian integers m + in, where m, n  . By considering the map

show that the quotient group /Γ is isomorphic to S × S, where S is the
circle group {z : |z| = 1}.

13
Möbius transformations
13.1 Möbius transformations
A Möbius transformation or map is a function f of a complex variable z that
can be written in the form
(13.1.1)
for some complex numbers a, b, c and d with ad − bc ≠ 0. It is easy to see
why we require that ad − bc ≠ 0, for
(13.1.2)
so that f is constant when ad − bc = 0. Notice that this also shows that f is
injective.
The deceptively simple form of (13.1.1) conceals two problems. First, a
Möbius transformation f can be written in the form (13.1.1) in many ways
(just as a rational number can be written as p/q in many ways); thus given f,
we cannot say what its coefficients a, b, c and d are. For example, if f maps
z to 2z, its coefficients might be 2π, 0, 0 and π, respectively. Of course,
given numbers a, b, c and d we can construct a Möbius map f from these,
but that is another matter.
The second problem stems from the fact that, for example, 1/(z − z0) is
not defined at the point z0. This means that there is no subset of  on which

all Möbius maps are defined, and this presents difficulties when we try to
form the composition of Möbius maps.
Example 13.1.1 Let f (z) = (z + 2)/z and g(z) = (z + 1)/(z − 1). Then,
apparently,
so that f g fixes the point 1. However, how can this be so as g is not defined
when z = 1? Worse still, if h(z) = 1/z then apparently h
fg(z) = (z + 1)/(3z − 1), although g is not defined when z = 1, f g(z) is not
defined when z = −1, and h fg(z) is not defined when z = 1/3. More
generally, a composition f1 · · · fn of Möbius maps will (in general) not be
defined at n distinct points in the complex plane.
Before we can develop the rich theory of Möbius maps we must resolve
these two issues. Neither are difficult to deal with, but we must attend to the
details. Our first result shows that although f in (13.1.1) does not determine
(a, b, c, d), it does so to within a non-zero scalar multiple.
Theorem 13.1.2 Suppose that a, b, c, d, α, β, γ and δ are complex numbers
with (ad − bc)(αδ − βγ) ≠ 0, and such that for at least three values of z in ,
cz + d ≠ 0, γ z + δ ≠ 0, and
(13.1.3)
Then there is some non-zero complex number λ such that
(13.1.4)
Proof Let z1, z2 and z3 be the values of z for which (13.1.3) holds with non-
zero denominators. Then the quadratic equation

has three distinct solutions zj, so we can equate coefficients and deduce that
aγ = cα, bγ + aδ = cβ + dα and bδ = dβ. These conditions are equivalent to
the existence of a complex number μ such that
where (by considering determinants) μ2 = (ad − bc)(αδ − βγ) ≠ 0. However,
this matrix identity is equivalent to (13.1.4) in the form
Informally, then, the first difficulty is resolved by saying the the vector
(a, b, c, d) is determined to within a (complex) scalar multiple. The second
difficulty (illustrated in Example 13.1.1) is resolved by joining an extra
point, which is called the point at infinity, to . This new point is denoted
by ∞. We have an intuitive notion of a complex number tending to ∞ (that
is, 1/z tending to 0) and if C ≠ 0 then
These limits motivate the following definition (and as this is their only role,
we need not enter into a formal discussion of limits).
Definition 13.1.3 Let f be the Möbius map given (13.1.1). If c ≠ 0 we
define f (∞) = a/c and f (−d/c) = ∞; if c = 0 we define f (∞) = ∞.
There is a subtle point here. Given a Möbius map f, we can write it in the
form (Theorem 13.1.2 guarantees that the condition 'c ≠ 0' holds for all or
none of these ways, and that when it holds, the values a/c and −d/c are
independent of the coefficients we choose. Without Theorem 13.1.2,
Definition 13.1.3 would not be a legitimate definition. It is important to
understand that Definition 13.1.3 is based on the idea of a function as a
'rule', and that we are not introducing algebraic rules for handling ∞. We
shall give a geometric argument which supports Definition 13.1.3 in Section

13.8, and an algebraic argument (involving the vector space 
2) in Section
13.6. In any event, from now on 
 ∪ {∞} plays the central role in the
theory, and we shall consider every Möbius map to be a map of  ∪ {∞}
into itself.
Definition 13.1.4 The set 
 ∪ {∞} is called the extended complex plane
and is denoted by 
∞.
The major benefit of Definition 13.1.3 is that every Möbius tranformation is
now defined on the same set, namely 
∞, so that the composition of any
two Möbius maps is properly defined. The previous examples suggest that
the composition of two Möbius maps is again a Möbius map; in fact, we
have the following stronger result.
Theorem 13.1.5 Each Möbius map is a bijection of 
∞ onto itself, and the
Möbius maps form the Möbius group 
 with respect to composition.
Proof We know that the composition of functions is associative, and the
identity map I is a Möbius map because I (z) = (z + 0)/(0z + 1) and I ( f
(z)) = f (z) = f (I (z)). Next, let
We shall assume that c ≠ 0 here (and leave the easier case c = 0 to the
reader). If z ≠ −d/c, ∞ then f (z) ∈ , and we see (from elementary algebra,
and the fact that ad − bc = 0) that f∗( f (z)) = z. Similarly, if z ≠ a/c, ∞ then f
( f∗(z)) = z. It follows that
is a bijection. As f∗( f (−d/c)) = f∗(∞) = −d/c and f∗( f (∞)) = f∗(a/c) = ∞,
we can now assert that f is a bijection of 
∞ onto itself with inverse f∗.
Thus each Möbius map f has a Möbius inverse
(13.1.5)

We still need to show that the composition of two Möbius maps is again a
Möbius map and, although this is not difficult, at this stage there is no
elegant way to do this. Briefly, we consider the Möbius maps f, g and h,
where f is given by (13.1.1), and
It is easy to see that f (g(z)) = h(z) for all those z for which the obvious
algebraic manipulation is valid. This leaves only a finite number of
exceptional values of z to check (just as we did when considering the
inverse of f ). This is tedious, and elementary, and we omit the details.
It is useful to know that any Möbius map can be written as the
composition of simple transformations.
Theorem 13.1.6 Every Möbius transformation can be expressed as the
composition of at most four maps, each of which is of one of the forms z 
az, z 
 z + b and z 
 1/z.
Proof Notice that these simple maps are rotations, dilations (expansion from
the origin), translations, and the complex inversion z 
 1/z. Suppose that f
(z) = (az + b)/(cz + d). If c = 0 then d ≠ 0 and f = f2 f1, where f1(z) = (a/d)z
and f2(z) = z + b/d. If c ≠ 0 then f = f4 f3 f2 f1, where f1(z) = z + d/c,
f2(z) = 1/z, f3(z) = kz, k = −(ad − bc)/c2, and f4(z) = z + a/c.
We now return to the connection between Möbius maps and 2 × 2
complex matrices. If A is a 2 × 2 non-singular complex matrix, we can use
the coefficients a, b, c and d of A to construct f given in (13.1.1). The group
of 2 × 2 non-singular complex matrices with respect to matrix
multiplication is the General Linear group GL(2, ), and this construction
defines a map Φ : GL(2, ) → 
 which is given explicitly by
(13.1.6)

Theorem 13.1.7 The mapping Φ is a homomorphism from the group GL(2, 
) onto the group 
 of Möbius maps.
Proof Given matrices
we let f = Φ(A) and g = Φ(B). Let h = Φ(AB); this is the Möbius map
constructed from the matrix product AB. We have to show that
h = Φ(AB) = Φ(A)Φ(B) = fg, and this was proved as part of the proof of
Theorem 13.1.5.
Informally, this result says:
What is the kernel of the homomorphism Φ in (Theorem 13.1.2, I is
represented by (and only by) scalar multiples of the identity matrix, and this
proves the next result.
Theorem 13.1.8 The kernel of the homomorphism Φ is {λI : λ ∈ }, where
I is the 2 × 2 identity matrix.
Given the Möbius transformation f in (13.1.1), we can change each of the
coefficients by the same non-zero scalar multiple without changing the
map; thus we may assume that ad - bc = 1. The space of 2 × 2 complex
matrices with determinant one is the Special Linear group SL(2, ) and, as
this is a subgroup of GL(2, ), Φ also acts as a homomorphism from SL(2, 
) onto 
. The kernel of this homomorphism consists of those scalar
multiples of the identity matrix that have determinant one; thus we also
have the following result (see Theorem 12.12.4).
Theorem 13.1.9 The homomorphism Φ : SL(2, ) → 
 has kernel {±I },
and 
 is isomorphic to the quotient group SL(2, )/{±I }.
Exercise 13.1

1.  Show that the set of Möbius transformations of the form (13.1.1), where
a, b, c and d are integers with ad − bc = 1, is a subgroup of 
. This is
the Modular group SL(2, ).
2.  Let f (z) = (3z + 2)/(z + 1) and g(z) = (z + 4)/(z − 1). Find matrices A and
B with determinant one that represent f and g, respectively, and verify
that f (g(z)) is derived from the matrix AB. What is det(AB)?
3.  Suppose that f is given by (13.1.1). Show that f2 = I but f ≠ I if and only
if a + d = 0.
4.  Let f (z) = (2z + 1)/(3z + 4). Express f as a composition of rotations,
dilations, translations and a complex inversion.
5.  Show that a Möbius map f can be written in the form (13.1.1) with a, b, c
and d real if and only if f maps  ∪ {∞} into itself.
6.  Let f (z) = e2πi/nz and g(z) = 1/z. Show that the subgroup G of 
generated by f and g is a dihedral group.
13.2 Fixed points and uniqueness
Roughly speaking, Theorem 13.1.2 implies that the general Möbius map
has three degrees of freedom. This suggests that there should be a unique
Möbius map that takes three given distinct points z1, z2, z3 to three given
distinct points w1, w2, w3. This is indeed the case, and we have the
following existence and uniqueness theorem.
Theorem 13.2.1 Let {z1, z2, z3} and {w1, w2, w3} be triples of distinct
points in 
∞. Then there is a unique Möbius map f with f (zj) = wj for j = 1,
2, 3.
Proof Suppose first that none of the zj are ∞, and let
Then g(z1) = 0, g(z2) = ∞ and g(z3) = 1. Now suppose that one of the zj is ∞.
Choose a point z4 distinct from z1, z2, z3 and let s(z) = 1/(z − z4). Then
s(z) = ∞ if and only if z = z4, so that none of s(z1), s(z2), s(z3) are ∞. Then
(by the previous paragraph) there is a Möbius map g1 which maps s(z1),

s(z2) and s(z3) to 0, 1 and ∞. Thus in all cases, there is a Möbius map (either
g or g1s) that maps z1, z2 and z3 to 0, 1 and ∞, respectively.
In the same way, there is a Möbius map h such that h(w1) = 0, h(w2) = ∞
and h(w3) = 1. Now let f = h−1g. Then f is the required map because, for
each j, f (zj) = h−1g(zj) = wj ; for example, f (z1) = h−1g(z1) = h−1(0) = w1.
We have to prove uniqueness, so suppose that f and F are Möbius maps
such that f (zj) = wj = F(zj) for j = 1, 2, 3. Then F−1 f fixes each zj. Now let v
be the Möbius map that maps z1, z2 and z3 to 0, 1 and ∞, respectively. Then
v−1(F−1 f)v is a Möbius map that fixes 0, 1 and ∞. Now it is self-evident that
any such map is the identity map I ; thus v−1(F−1 f)v = I, and this implies (in
a group) that f = F.
Note that the proof of Theorem 13.2.1 avoids computation. In general,
one should resist the temptation to solve a problem on Möbius maps by
long computations (apart from finding composite maps), for almost always
there is a short, elegant and easy proof available. There is a useful corollary
of Theorem 13.2.1.
Corollary 13.2.2 If a Möbius map has three fixed points then it is the
identity map.
This can also be proved by noting that the quadratic equation
az + b = z(cz + d) = 0, where f is given by (13.1.1), has three distinct
solutions when f has three distinct fixed points, so in this case c = 0, a = d
and b = 0. We end this section with some examples.
Example 13.2.3 We construct the Möbius map f such that f (i) = 0, f
(−i) = ∞ and f (1) = 1 + i. The numerator of f must be zero at i, and its
denominator must be zero at −i; thus f (z) = k(z − i)/(z + i) for some constant
k. The condition f (1) = 1 + i determines k and we find that k = 2i/(1 − i).
Example 13.2.4 We construct the Möbius map f such that f (0) = 3, f
(1) = 1 + i and f (2) = 1 − i. As in our proof of Theorem 13.2.1, f = h−1g,
where g maps 0, 1, 2 to 0, 1, ∞, and h maps 0, 1, 2 to 0, 1, ∞. We can find g
and h by the technique used in Example 13.2.3 and, after some

computation, we see that g(z) = −z/(z − 2), h(z) = k(z − 3)/(z − [1 − i]),
where k = (2 − 4i)/5. A further computation gives
Example 13.2.5 We construct the Möbius map f such that f (1) = 1, f (∞) = i
and f (0) = −i. In this case we first construct f−1 by the method in Example
13.2.3, and we find that f−1(z) = (−i z + 1)/(z − i). Using (Theorem 13.1.6,
by showing that
for some non-zero λ.
Exercise 13.2
1.  Find the Möbius map that fixes 1 and −1 and maps 0 to ∞.
2.  Find the Möbius map f that cyclically permutes the points −1, 0 and 1.
Verify directly that f3(z) = z for all z.
3.  The Möbius map f (z) = i z generates a cyclic group of order four. Now
let h be any Möbius map. Show that h f h−1 generates a cyclic group of
order four, and also fixes h(0) and h(∞). Use this to construct a Möbius
map of order four that fixes 1 and −1.
4.  Let f (z) = (az + b)/(cz + d). If we are to have f (zj) = wj (see Theorem
13.2.1) then the unknown coeficients a, b, c and d of f must satisfy the
homogeneous linear equation

Suppose that none of the zj, or the wj, are ∞. Show that this matrix has rank
three, and deduce (algebraically) that the solution (a, b, c, d) is determined
to within a scalar multiple.
13.3 Circles and lines
The main result in this section is that a Möbius map takes a circle into a
circle or straight line, and a straight line into a circle or straight line. First,
however, we need to amend our definitions of lines and circles for, at the
moment, the image of a circle can never be a line (because, roughly
speaking, a line will fall into two pieces if a point is removed, but a circle
will not). We shall refer to the usual circles as Euclidean circles, and the
usual lines as Euclidean lines. We are going to add the point ∞ to each
Euclidean line and, from now on, a circle is either a Euclidean circle, or a
Euclidean line with ∞ attached. Now we can assert that the image of a circle
is a circle. Euclidean circles and Euclidean lines are objects in the
Euclidean geometry of , but we must now consider circles (which may or
may not contain the point ∞) in the geometry of 
∞. We often call a
Euclidean circle 'a circle in ', and similarly for lines.
Definition 13.3.1 A Euclidean circle is the set of points in  given by some
equation |z − z0| = r, where r > 0. A Euclidean line is the set of points in 
given by some equation |z − a| = |z − b|, where a ≠ b. A circle (in 
∞) is
either a Euclidean circle, or a set L ∪ {∞}, where L is a Euclidean line.
It is immediate that there is a unique circle that passes through any three
given distinct points in 
∞. This fact alone is enough to justify Definition
13.3.2, but it is the next result that is of real interest.
Theorem 13.3.2 Let f be a Möbius map and C a circle. Then f (C) is a
circle.
Proof First, this result asserts not just that f (C) is a subset of some circle
C1, but that f (C) = C1. However, it is enough to prove that for any circle C
there is a circle C1 such that f (C) ⊂ C1. Indeed, if this is so, then C ⊂

f−1(C1). As f−1(C1) lies in some circle, this circle must be C; thus
f−1(C1) = C and hence f (C) = C1.
We now need to show that f (C) lies in some circle C1. In view of
Theorem 13.1.6, we need only do this in the cases when f is one of the maps
z 
 az, z 
 z + a and z 
 1/z. In the first two cases f maps each Euclidean
circle onto a Euclidean circle, and each Euclidean line onto a Euclidean
line, and as f (∞) = ∞, in each case f maps each circle onto a circle. For the
rest of this proof, then, we take f (z) = 1/z.
Suppose first that C is a circle that does not pass through the origin; then
(13.3.1)
In this case, we write w = f (z) = 1/z, and then, from algebra, w satisfies
a + b  + w + cw  = 0. Thus f (C) ⊂ C1, where
Now suppose that C is a circle that does pass though the origin. In this case
we let the set in (13.3.1) be C∗, and then C = C∗ ∪ {0}. If z ∈ C∗ the
algebra can be carried out as above, and again f (C∗) ⊂ C1. However, in
this case c = 0 so that C1 is a Euclidean line. The result now follows as
The cases in which C is a Euclidean line are similar. If C = L ∪ {∞} for
some Euclidean line L that does not pass through the origin, we write
(13.3.2)
where c ≠ 0. Then f (C) ⊂ f (L) ∪ { f (∞)} ⊂ C1 ∪ {0} = C1, where the
Euclidean circle C1 is given by

Finally, if C = L ∪ {∞}, where L is a Euclidean line that does pass through
the origin, we let L∗ be the set in (13.3.2), with c = 0. Then C1 is a
Euclidean line, and f (L) ⊂ C1 ∪ {∞}.
We have carried out the proof of Theorem 13.3.2 in detail, to show how
extra steps are necessary to cope with the fact that the algebra of  cannot
cope with ∞, nor with 'division' by zero. Actually, what is missing here is
continuity for if we verify the result for all z other than 0 or ∞, the required
steps for these points would follow from the fact that f is continuous. We
discuss this in Section 13.8, and in Section 13.6 we shall see that there is
also an algebraic approach which avoids these tiresome extra steps. Having
said this, we admit that, in practice, hardly anyone ever carries out these
extra steps.
We now consider the following two problems.
(1)  Given a Möbius f and circle C, how do we find f (C)?
(2)  Given circles C and C′ how do we find an f with f (C) = C′?
Neither of these are difficult to do, and it is not necessary to perform
elaborate calculations. In the first case it suffices to select three points on C
and find their images under f ; then (by Theorem 13.3.2) f (C) will be the
unique circle through these three image points. In the second case, we select
three points zj on C, and three points wj on C′. Then (by Theorem 13.2.1)
there is a Möbius map f with f (zj) = wj, and then f (C) = C′. We give one
example of each of these problems.
Example 13.3.3 Let f (z) = (z − i)/(z + i). What is f (C), where C = 
 ∪
{∞}? As C contains 0, 1 and ∞, f (C) contains −1, −i and 1; thus f (C) = {z :
|z| = 1}. What is f (C′), where C′ is the imaginary axis with ∞ attached? As
C′ contains 0, i and ∞, f (C′) contains −1, 0 and 1 so that f (C′) =  ∪ {∞}.
Example 13.3.4 We construct a Möbius map f which maps |z| = 1 onto L ∪
{∞}, where L is given by y = x. Now f will have this property if f (i) = 0, f
(−i) = ∞ and f (1) = 1 + i, and there is only one such Möbius map. By
inspection, it is f (z) = (i − 1)(z − i)/(z + i). Of course, other choices of

points would be equally valid and this is not the only Möbius map with the
given property.
Let us now discuss the mapping of complements of circles. If C is the
Euclidean circle |z − z0| = r, where r > 0, then the complement of C in 
∞
consists of two 'connected' pieces, namely the Euclidean disc given by
|z − z0| < r, and the set {z : |z − z0| > r} ∪ {∞}. If C = L ∪ {∞}, where L is a
Euclidean line, the complement of C consists of the two 'connected'
Euclidean half-planes bounded by L. In each case, the complement of C
consists of two disjoint 'connected' sets; these are called the
complementary components of C. We now have the following extension of
Theorem 13.3.3.
Theorem 13.3.5 Let f be a Möbius map. Let C be a circle, and let C′ be the
circle f (C). Then f maps each complementary component of C onto a
complementary component of C′.
It is possible to give a completely elementary ad hoc proof of this, but it is
hardly worthwhile to do so for the proof is easy after we have studied a
little about 'connected sets', and continuity, in topology. We shall content
ourselves here with a continuation of Example 13.3.3.
Example 13.3.6 (continued) We have seen that f (z) = (z − i)/(z + i) maps 
∪ {∞} onto {z : |z| = 1}. As f (i) = 0 we see (from Theorem 13.3.6) that f
maps {x + iy : y > 0} onto {z : |z| < 1}. In fact, this is clear if we note that | f
(z)| < 1 if and only if z is closer to i than it is to −i. Finally, as f (−1) = i we
see that f maps {x + iy : x < 0} onto {x + iy : y > 0}.
We end with a result about the set of Möbius maps that map a disc onto
itself. It is clear that if D is a Euclidean disc, or a Euclidean half-plane, then
the set of Möbius maps f that satisfy f (D) = D is a group.
Theorem 13.3.7 Let a and b be points in a disc D. Then there is a Möbius
map f such that f (D) = D and f (a) = b.
Proof We shall prove this first in the case when D is the unit disc 
 given
by where  = {z : |z| < 1}. Let g(z) = (z − a)/(1 − āz). Then g(C) = C, where

C is the unit circle {z : |z| = 1}, because the general point on C is eiθ, where
θ is real, and
Now g(a) = 0 and hence (from Theorem 13.3.6) g( ) = . In the same way
we can construct a Möbius h with h(b) = 0 and h( ) = . Let f = h−1g; then
f (a) = b and f ( ) = .
To prove the result for the points a and b in a general disc D, see Figure
13.3.1. Let t be any Möbius map of D onto , and let u = t(a), v = t(b). By
the first part of this proof there is a Möbius map g with g(u) = v and g( ) = 
. Let f = t−1gt; then f (a) = t−1gt(a) = t−1g(u) = t−1(v) = b and, similarly, f
(D) = D.
Figure 13.3.1
Exercise 13.3
1.  In each of the following cases, find f (C):
(a)  f (z) = 1/z and C = {x + iy : x + y = 1} ∪ {∞};
(b)  f (z) = i z/(z − 1) and C =  ∪ {∞};
(c)  f (z) = i z/(z − 1) and C = {z : |z| = 1};
(d)  f (z) = (z + 1)/(z − 1) and C =  ∪ {∞}.
2.  Show that the transformation f (z) = (2z + 3)/(z − 4) maps the circle
|z − 2i| = 2 onto the circle |8z + (6 + 11i)| = 11.

3.  Find a Möbius map that maps the interior of the circle |z − 1| = 1 onto the
exterior of the circle |z| = 2.
4.  By considering the map g(z) = (z − a)/(z − b), show that when k > 0 the
set {z : |z − a|/|z − b| = k} is a circle. What is this set when k = 1?
5.  Let f (z) = (z − i)/(i z − 1). Show that f maps {x + iy : y > 0} onto {z : |z|
< 1}.
6.  Find a Möbius transformation that maps {z : y > 0, |z − 1| < 1} onto {z :
x < 0, y > 0}.
7.  Let C and C′ be two Euclidean circles that are tangent at the point z0, and
let f (z) be any Möbius map such that f (z0) = ∞. Prove that f (C) and f
(C′) are parallel straight lines (with ∞ attached). Find a Möbius map f
that maps the strip between the lines y = 0 and y = 1 onto the region
between the circles |z − 1| = 1 and |z − 2| = 2.
13.4 Cross-ratios
By Theorem 13.2.1, there is a unique Möbius map that maps a given triple
of distinct points onto another such triple. It follows that if we are given
four distinct points z1, z2, z3, z4 and four distinct points w1, w2, w3, w4 there
is at most one Möbius map f with f (zj) = wj for j = 1, 2, 3, 4. We now give a
necessary and sufficient condition for the existence of such a map.
Definition 13.4.1 The cross-ratio of four distinct points z1, z2, z3, z4 in  is
defined to be
(13.4.1)
The value of the cross-ratio when, say, zj = ∞ is the limiting value of [z1, z2,
z3, z4] as zj tends to ∞. Explicitly,

The reader should note that some authors use different permutations of 1,
2, 3 and 4 on the right-hand side of (13.4.1); however, it does not matter
which definition is used (provided, of course, that one is consistent). Our
choice leads to the formula [0, 1, w, ∞] = w. We can now give the solution
to the problem posed at the start of this section.
Theorem 13.4.2 Given distinct points z1, z2, z3, z4, and distinct points w1,
w2, w3, w4, a necessary and sufficient condition for the existence of a
Möbius map f with f (zj) = wj, j = 1, 2, 3, 4, is
(13.4.2)
In particular, for any Möbius transformation f,
(13.4.3)
Proof Suppose first that there is a Möbius map f with f (zj) = wj, and
suppose for the moment that none of the zj, or the wj, are ∞. If f
(z) = (az + b)/(cz + d) then czj + d ≠ 0, and
It is immediate from this and (13.4.1) that (13.4.2) holds. The general case,
in which one of the zj and one of the wk may be ∞, follows in the same way

by using the appropriate formula for f (∞) and the cross-ratio (we omit the
details).
Now suppose that (13.4.2) holds. Let g and h be Möbius maps such that
g(z1) = 0, g(z2) = 1, g(z4) = ∞, and h(w1) = 0, h(w2) = 1, h(w4) = ∞. Then,
from (13.4.2) and the invariance of cross-ratios that we have just
established,
Now let f = h−1g; then for each j, f (zj) = wj.
We give one (of many) applications of the cross-ratio.
Definition 13.4.3 The points z1,. . ., zn are said to be concyclic if they lie on
some circle in 
∞.
Any three points are concyclic, but four points need not be. The cross-ratio
of four points determines whether they are concyclic or not.
Theorem 13.4.4 The four distinct points z1, z2, z3, z4 are concyclic if and
only if [z1, z2, z3, z4] is real.
Proof Take any four distinct points zj, let C be the circle through z1, z2 and
z4, and let g be the unique Möbius map with g(z1) = 0, g(z2) = 1 and
g(z4) = ∞. Then, by Theorem 13.3.2, g(C) =  ∪ {∞}. As

we see that [z1, z2, z3, z4] is real if and only of g(z3) ∈ 
, and this is so if
and only if z3 ∈ C.
Theorem 13.4.4 gives an alternative proof of Theorem 13.3.3. Given f
and a circle C, choose z1, z2 and z3 on C and let C′ be the circle through f
(zj), j = 1, 2, 3. If z is any other point of C, then [z1, z2, z3, z] is real and
hence, by the invariance of the cross-ratio, the cross-ratio of the f (zk) is
real. By Theorem 13.4.4, the points f (z), f (z1), f (z2) and f (z3) are
concyclic; thus f (z) ∈ C′ and f (C) ⊂ C′.
Exercise 13.4
1.  Show that if z1, z2, z3 and z4 are distinct points of 
∞, then the cross-ratio
[z1, z2, z3, z4] is not equal to 0, 1 or ∞.
2.  Establish the invariance (Theorem 13.1.6.
3.  Let z = x + iy and w = u + iv. By considering cross-ratios, show that the
points 0, 1, z, w are concyclic if and only if (|z|2 − x)/y = (|w|2 − u)/v, and
verify this directly by geometry.
4.  Show that the circle through 0, 1 and w is given by (t + 1)w/(1 + tw),
where t ∈ 
 ∪ {∞}.
5.  Let C and C′ be intersecting Euclidean circles, and suppose that z1, z2, z3
∈ C and z1, z3, z4 ∈ C′. Show that C and C′ can be mapped by a Möbius
map to an orthogonal pair of Euclidean lines if and only if the cross-ratio
[z1, z2, z3, z4] is purely imaginary (that is, has real part zero).
13.5 Möbius maps and permutations
Theorem 13.2.1 implies that there are exactly six Möbius maps that map the
set {0, 1, ∞} onto itself, and it is easy to see that these maps are
(13.5.1)

We leave the reader to check that these maps form a group, and that this
group is isomorphic to the permutation group S3. Indeed, if we regard S3 as
the group of permutations of {0, 1, ∞} instead of {1, 2, 3}, we see that the
six maps listed above correspond to the permutations (0)(1)(∞), (0 ∞)(1), (0
1)(∞), (0 1 ∞), (0 ∞ 1) and (0)(1 ∞), respectively.
More generally, given any permutation ρ of a finite subset X of 
∞, we
might try to find a Möbius map f that agrees with ρ on X. However,
Corollary 13.2.2 shows that this is not usually possible. Consider, for
example, the permutation (1 2)(3 4 5). In this case ρ3 has three and only
three fixed points, and so if we were able to find a set X of five elements,
and a Möbius f that acted on X in the same way that ρ acts on {1,. .. , 5}, we
would have f3(z) = z for three and only three z in X and this would violate
Corollary 13.2.2.
We have seen that S3, but not S5, can be realised as a Möbius group, so it
is natural to ask for a list of all permutations groups that can be realised by
some Möbius group. In view of the comments just made, this list is likely to
be short. In fact, the following more general statement is true: any finite
Möbius group is isomorphic to a cyclic or a dihedral group, or to one of the
symmetry groups of the five Platonic solds.
There is an important link between cross-ratios and permutations in S4
which we shall now explore. Suppose that we start with a cross-ratio [z1, z2,
z3, z4] and a permutation ρ of {1, 2, 3, 4}. We can form a new cross-ratio
[w1, w2, w3, w4] by moving the entry zk in the k-th place of the given cross-
ratio to the ρ(k)-th place in the new cross-ratio. Thus wρ(k) = zk or,
equivalently, wk = zρ−1(k). For example, if ρ = (1 2 3), then
We shall now investigate how, for a given ρ, the new cross-ratio is related
to the original cross-ratio. Rather surprisingly, the value of the new cross-
ratio is a function, say fρ, of the value of the original cross-ratio, but not on
the individual values of the zj. Moreover, the function fρ is always one of the
Möbius maps listed in (13.5.1). We give a proof.
Take any permutation ρ (which will be fixed throughout this discussion).
Take distinct zj, let λ = [z1, z2, z3, z4], and let g be the unique Möbius map

with g(z1) = 0, g(z2) = 1 and g(z4) = ∞. Then, by the invariance of the cross-
ratio under g, we see that g(z3) = λ. Now
and this second cross-ratio is the cross-ratio of the points 0, 1, λ, ∞ taken in
some order. As the order of these points is determined by ρ, the value of the
cross-ratio is of the form fρ(λ), where (as is easily checked) fρ is one of the
functions in (13.5.1).
An explicit example might help, so suppose that ρ = (1 2). Then
Thus if ρ = (1 2), and fρ(z) = 1 − z, then
A similar argument can be used for any ρ in S4, and we ask the reader to
carry out the calculations in all cases in which ρ is a transposition; this leads
to the following results:
(a)  f(12)(z) = f(34)(z) = 1 − z;
(b)  f(13)(z) = f(24)(z) = z/(z − 1);
(c)  f(14)(z) = f(23)(z) = 1/z.
We can, of course, carry out the same calculation for any ρ, but the
results follow more easily from the important formula
(13.5.2)

which holds for any permutations σ and ρ in S4. The proof of (13.5.2) is
easy. Let μ = σρ, wρ(k) = zk and uσ(j) = wj. Then uμ(k) = μσ(ρ(k)) = wρ(k) = z k
and
and this completes the proof of (13.5.2).
We can extract a lot of information from (13.5.2); for example,
(13.5.3)
as is evident from (13.4.1). More generally, we have seen that when ρ is a
transposition, fρ is in the group, Γ say, of functions listed in (13.5.1). It
follows from (13.5.2), and the fact that any ρ is a product of transpositions,
that for every ρ in S4, fρ is in Γ. Thus, if we think of Γ as S3, the map ρ 
 fρ
is a homomorphism, say θ, of S4 onto S3.
The kernel K of θ is of interest, and (13.5.3) shows that (1 2)(3 4) is in K.
A similar argument shows that (1 3)(2 4) and (1 4)(2 3) are also in K, as (of
course) is the identity permutation I. As |K | = |S4|/|S3| = 4, there can be no
other elements in K. We summarize our results in the following theorem.
Theorem 13.5.1 For each ρ in S4 there is a Möbius map fρ in the group
of permutations of {0, 1, ∞} such that, for any distinct z1, z2, z3, z4,
Moreover, the map ρ 
 fρ is a homomorphism of the group S4 onto the
group Γ with kernel K, where K = {I, (1 2)(3 4), (1 3)(2 4), (1 4)(2 3)}.

Exercise 13.5
1.  Verify (a), (b) and (c) in the text.
2.  Find fρ when ρ = (1 2 3). Express ρ as a product of transpositions and
verify (13.5.2) in this case.
13.6 Complex lines
In this section we discuss an algebraic way, based on the complex vector
space 
2, to introduce the point ∞.
Definition 13.6.1 A complex line is a one-dimensional subspace of the
vectorspace 
2,t of complex column vectors (z1, z2)t. The set of all complex
lines is denoted by .
A complex line L is the set of complex scalar multiples of some non-zero
point in 
2,t and so is of the form
(13.6.1)
Any 2 × 2 non-singular complex matrix
(13.6.2)
acts as a linear transformation of 
2,t onto itself by the rule
As A is non-singular, it maps a non-zero point to a non-zero point, and as it
is linear it maps each complex line to a complex line. The same is true for
A−1, so we have the following result.
Lemma 13.6.2 Any non-singular 2 × 2 complex matrix A is a bijection of 
onto itself. The group GL(2, ) of non-singular 2 × 2 complex matrices acts

as a group of permutations of .
We want to see which lines map to which lines under a matrix A, and this is
best done by considering the slope of a complex line. If z2 ≠ 0, we can form
the quotients (λz1)/(λz2) of the coordinates of the non-zero points on the line
L in (13.6.1) and the common value of all of these quotients is the slope z1/z
2of L. The single complex line whose slope is not defined is
and, by convention, we say that this line has slope ∞. Given a complex
number w there is a unique complex line L(w) with slope w, namely
The following result is now clear.
Lemma 13.6.3 The map w 
 L(w) is a bijection from the set C∞ onto the
space  of all complex lines.
This bijection is the key to understanding Möbius transformations from an
algebraic point of view. Let A be the non-singular matrix in (Lemma 13.6.3
says A acts as a bijection of 
∞ onto itself. In fact, as we shall now see, this
action is identical to the action of the Möbius map z 
 (az + b)/(cz + d).
We examine this action of A on 
∞ in greater detail. First, for any
complex w,
If c = 0 then ad = ad − bc ≠ 0, and we see that A maps the line of slope w to
the line of slope (aw + b)/d. Also, A maps the line of slope ∞ to itself, so
that the action of A on the set 
∞ (of slopes) is given by z 
 (az + b)/d.
Now suppose that c ≠ 0. If w ≠ −d/c, then A maps the line of complex slope
w to the line of complex slope (aw + b)/(cw + d). If w = −d/c, then A maps

the line of slope w to L(∞). Finally, A maps L(∞) to L(a/c). Together these
facts prove the following result.
Theorem 13.6.4 Suppose that A is given by (13.6.2), and regard A as acting
on the space 
∞ of slopes of complex lines. Then A(z) = (az + b)/(cz + d).
This result fully justifies the discussion of Möbius transformations given in
Section 13.1.1 and, moreover, it eliminates the need to consider as special
cases any arguments involving ∞ or 'division' by zero. Given a Möbius
transformation f, we can choose a matrix of coefficients, say the matrix A in
(Theorem 13.1.2 implies that A is determined up to a non-zero scalar
multiple. However, if we replace A by a scalar multiple, say λA, then this
scalar multiple acts in exactly the same way as A does on the space  of
complex lines, and hence it determines the same Möbius action on the space
of slopes of lines.
Exercise 13.6
1.  Prove that if two complex lines have the same slope, then they are the
same line (thus the map w 
 L(w) is properly defined).
2.  Suppose that A is given by (13.6.2). Show that A maps L(∞) to itself if
and only if c = 0.
3.  Let A = 
 where a ≠ 0. Show directly that the action of A on the
space of slopes of complex lines is given by z 
 + 1.
4.  Discuss the action of the matrix A in (13.6.2) on the space  when
ad − bc = 0.
13.7 Fixed points and eigenvectors
A point w in 
∞ is a fixed point of a Möbius map f if f (w) = w, and we
recall that if a Möbius map is not the identity, then it has at most two fixed
points. We shall now show that the fixed points of a Möbius map
correspond to the two lines of eigenvectors for the matrix corresponding to
f. Suppose that f is a Möbius map with a corresponding matrix A; thus A
maps L(w) to L(w′) if and only if f (w) = w′ (see Section 13.6). It follows
that w is a fixed point of f if and only if A maps L(w) to itself, and this is so

if and only if each non-zero point on L(w) is an eigenvector of A. We give a
formal statement of this.
Theorem 13.7.1 Let f be a Möbius map with corresponding matrix A. Then
f (w) = w if and only if L(w) is a line of eigenvectors of A.
Earlier we proved (by vector space methods) that every complex matrix has
an eigenvector, and it follows from this that every Möbius transformation
has a fixed point. If a Möbius transformation f has three distinct fixed
points, then A has three distinct lines of eigenvectors (with different slopes).
As A has at most two eigenvalues, it must have two eigenvectors, say v1 and
v2, which lie on different complex lines but which correspond to the same
eigenvalue μ, say. As {v1, v2} is a basis of 
2,t this implies that A = μI,
where I is the identity matrix, and this in turn implies that f is the identity
Möbius map. This gives an algebraic proof of Corollary 13.2.2. We now
give some examples to illustrate the link between fixed points and
eigenvectors.
Example 13.7.2 The map f (z) = z + 1 has a single fixed point, namely ∞.
The matrix
represents f. It is clear that 1 is the only eigenvalue of A, so the eigenvectors
of A are given by
The general solution of this is z2 = 0; thus, as predicted, L(∞) is the only
line of eigenvectors of A.
Example 13.7.3 It is easy to check that for any non-zero complex number k
the Möbius map

fixes 1 and 2. The matrix
represents f so that A must have eigenvectors (1, 1)t and (2, 1)t. It is easy to
check that this is indeed so, and that the corresponding eigenvalues are −k
and −1.
Earlier we discussed the possibility of diagonalizing a matrix, so it is
natural to consider what this means in the current context of Möbius maps.
Suppose that the Möbius map f is represented by the matrix A, and that A
has distinct eigenvalues, say λ and μ. Then there is a non-singular matrix B
such that
and B A B−1 has eigenvectors (1, 0)t and (0, 1)t. Now B represents a Möbius
map g, say, and as the transition from matrices to Möbius maps is a
homomorphism, this means that g f g−1 fixes 0 and ∞. In fact, g f
g−1(z) = (λ/μ)z, and we see that the diagonalization of A (which is achieved
by taking the eigenvectors as a basis) is equivalent to conjugating f to g f
g−1 thereby moving the fixed points to 0 and ∞. We can see this directly. If f
has distinct fixed points z1 and z2, we let g(z) = (z − z1)/(z − z2); them (by
direct calculation) g f g−1 fixes 0 and ∞. If A has coincident eigenvalues,
then there is a matrix B such that
where λ ≠ 0 (as A is non-singular). In this case the corresponding g f g−1 is
either a translation (when μ ≠ 0) or the identity map (when μ = 0). Finally, if

a Möbius map f has exactly one fixed point, say z1, we let g(z) = 1/(z − z1)
and then g f g−1 has a unique fixed point, namely ∞; thus g f g−1 is a
translation. We summarize these results.
Theorem 13.7.4 If a Möbius transformation f has exactly two fixed points
then it is conjugate to some map z 
 az, where a = 0. If f has exactly one
fixed point, it is conjugate to z 
 z + 1.
Finally, we see how to compute the iterates of a Möbius map (or a 2 × 2
matrix). Suppose, for example, that f has two fixed points. Then there is
some map g, and some constant k, such that g f g−1(z) = kz. Then g
fng−1(z) = (g f g−1)n(z) = knz, so that fn(z) = g−1 kng(z). As g and k can be
found explicitly, so can fn(z). This method is equivalent to that used for
finding the powers of a matrix (that is diagonalize the matrix as in Section
10.3).
Exercise 13.7
1.  Let f (z) = (az + b)/(cz + d), where ad − bc ≠ 0. Show that f2(z) = z for all
z if and only if either f is the identity map I or a + d = 0. Is it true that for
a 2 × 2 complex matrix A, A2 = I, where I is now the identity matrix, if
and only if A = I or trace(A) = 0?
2.  Let f (z) = az + b and g(z) = cz + d, where neither is the identity map.
Show that if f and g commute ( f (g(z)) = g( f (z)) for all z) then either (i)
f and g are translations, or (ii) f and g have a common finite fixed point.
What does this say when interpreted in terms of 2 × 2 complex matrices?
3.  Suppose that f is a Möbius map and that for some w in , f (w) = w but f
( f (w)) = w. Show that f ( f (z)) = z for all z in 
∞, and give a matrix
proof of this result.
13.8 A geometric view of infinity

In order to study Möbius transformations properly we found it necessary to
adjoin an 'abstract' point ∞ to 
. We have seen how to do this from an
algebraic point of view, and we shall now discuss a construction in which
the introduction of the additional point arises naturally in a geometric way.
This method depends on some elementary, but very intuitive, ideas about
continuity which we shall accept without question.
Let  denote the unit sphere {x ∈ 
 : ||x|| = 1} in 
. We shall identify
the complex number x + iy with the point (x, y, 0) in 
, and  cuts  in the
circle x2 + y2 = 1 (the 'equatorial plane' of the sphere). Let ζ = (0, 0, 1) (the
'north pole' of 
). Each point z of  can be projected linearly towards or
away from ζ until it reaches the sphere 
 at some uniquely determined
point w other than ζ. The map φ : z 
 w is the stereographic projection of 
 into , and it is illustrated in Figure 13.8.1.
It is easy to find an explicit formula for φ(z). The general point on the
line L through (x, y, 0) and (0, 0, 1) is
(13.8.1)
where t is real, and this line meets the sphere  when
The two solutions of this are t = 0 (which corresponds to ζ), and
t = 2/(x2 + y2 + 1) which corresponds to φ(z), where z = x + iy. If we now
substitute this
Figure 13.8.1

Figure 13.8.2
second solution in (13.8.1) we find that
(13.8.2)
It is evident from geometry that φ(z) = z whenever |z| = 1, and this also
follows from (13.8.2). Notice also that φ(z) lies 'above' 
 when |z| 1 and
that it lies 'below'  when |z| < 1. In particular, φ(0) = (0, 0, −1).
It is clear that φ is a bijection of  onto \{ζ }. Moreover, as
and similarly for y in place of x, (13.8.2) shows that φ(z) → ζ as |z| → +∞. It
is therefore natural to adjoin an 'abstract' point ∞ to  to form C∞, and to
define φ(∞) = ζ. Now φ is a bijection from C∞ onto the sphere .
The definition of a Möbius map f on 
∞ can now be explained in terms of
a map from  to itself. Given a Möbius transformation f, we can construct a
map f∗ : 
 
 
 defined by f∗ = φfφ−1; see Figure 13.8.2. The map f∗ is
defined at all points of 
 except at ζ and φ(−d/c), and it is easy to see that
the definitions f (−d/c) = ∞ and f (∞) = a/c are equivalent to making f∗ a
continuous map of  onto itself.
One of the benefits of stereographic projection is that it makes it clear
why we should attach the point ∞ to a straight line L and regard the
extended line L ∪ {∞} as a circle. Indeed, it should be clear that lines in the
plane correspond under stereographic projection to circles on 
 that pass

through ζ, and conversely. Thus φ gives a bijection between the set of
extended lines in 
∞ and the set of circles on  that pass through ζ .
It is also true (thought less obviously so) that a circle on the sphere that
does not pass through ζ corresponds to a circle in . To see this note that a
circle C on the sphere is the intersection of the sphere with a plane given,
say, by α1x1 + α2x2 + α3x3 = β, and that if ζ ∉ C then α3 ≠ β. If z in  is
mapped to the circle C on , then, from (13.8.2),
(13.8.3)
and as α3 ≠ β, this is the equation of a circle in .
We have now seen that the set of circles on 
 corresponds under φ−1 to
the set of all circles and extended lines in . This is further justification (if
any is needed) for saying that L ∪ {∞} a circle when L is a straight line.
Finally, suppose that z and w are in 
. Then their projections onto the
sphere 
 are given by (13.8.2) and we can use this to compute the
Euclidean distance beween the projections φ(z) and φ(w). A tedious (but
elementary) calculation (which we leave as an exercise for the reader)
shows that
(13.8.4)
and a similar (but simpler) argument shows that
(13.8.5)
The expression ||φ(z) − φ(w)|| is known as the chordal distance between z
and w (as it is the length of the chord of  that joins φ(z) to φ(w) in 
3). It
is geometrically evident that ||φ(z) − φ(w)|| ≤ 2; this also follows from the
Cauchy-Schwarz inequality

Exercise 13.8
1.  Find the image of the circle {z : |z| = r} under stereographic projection
onto .
2.  Show that the circle given in (13.8.3) is a straight line if and only if the
plane α1x1 + α2x2 + α3x3 = β passes through ζ .
3.  Verify (13.8.4) and (13.8.5).
4.  Show that if (x1, x2, x3) ∈ , then
Show also that the projection of  onto 
∞ from the 'south pole' (0, 0, −1)
of  is given by
13.9 Rotations of the sphere
The stereographic projection φ maps 
∞ onto the sphere 
 so that every
point is covered exactly once. Every map f : 
∞ 
 
∞ corresponds to a
map f∗ :  
  defined by f∗ = φ f φ−1 and, in a similar way, every map f∗
:  
 
 determines a map f : 
∞ 
 
∞ by f = φ−1 f φ. The main result in
this section is that any rotation of the sphere arises from a Möbius map in
the sense that it is a map f∗ for some choice of a Möbius map f. Note,
however, that not all Möbius maps give rise to a rotation (for example, the
Möbius maps with exactly one fixed point do not because f has one fixed
point in 
∞ if and only if f∗ has one fixed point in ).
Theorem 13.9.1 Every Möbius map of the form
(13.9.1)
corresponds to a rotation f∗ of the sphere, and every rotation of the sphere
arises in this way.

The proof of this result involves several ideas, so we shall break the proof
into several simpler steps.
Lemma 13.9.2 Under the stereographic projection φ, two points z and w
map to diametrically opposite points on the sphere 
 if and only if
Proof The points φ(z) and φ(w) are diametrically opposite on  if and only
if φ(z) = −φ(w). We leave the reader to check from (13.8.2) that this is so if
and only if. 
Lemma 13.9.3 If a Möbius map f is such that f∗ is a rotation of the sphere,
then f is of the form (13.9.1).
Proof Let f (z) = (az + b)/(cz + d), where ad − bc = 1, and suppose that f∗ is
a rotation of the sphere. Then f∗ preserves distances between points on the
sphere, and so it must map diametrically opposite points to diametrically
opposite points. This means that if 
 then 
 in
other words, f must satisfy the relation
valid for all z. This relation is
and (as z is arbitrary) it follows from Theorem 13.1.1 that there is some λ
such that
This shows that

and together these show that λ = 1, and that (13.9.1) holds.
Lemma 13.9.4 If f is of the form (13.9.1), then f∗ is a rotation of the sphere.
Proof Suppose that f is of the form (13.9.1). Then a calculation using
(13.9.1) and (13.8.4) shows that for all z and w,
and this means that f∗ is an isometry of the sphere  into itself. Now every
isometry g :  
 
 extends to an isometry of 
 into itself by the rule g(t
x) = tg(x), where x ∈ . Indeed, to see that this extension is an isometry of
R3 we need only observe that the two larger triangles in Figure 13.9.1 are
congruent. Moreover, it is clear that in this extension we have
g(0) = g(0x) = 0g(x) = 0 for any x on .
This means that f∗ is an isometry of 
 onto itself and f∗(0) = 0; thus f∗
is the action of an orthogonal matrix A on 
, and f∗ is therefore either a
reflection in some plane through the origin, or a rotation. However, f∗
cannot be a reflection as this would imply that f∗, and hence also f, wouldi
have infinitely many fixed points. Thus f∗ is a rotation.
Lemma 13.9.5 Every rotation of  is of the form f∗ for some Möbius map f
of the form (13.9.1).
Proof Let

Figure 13.9.1
Then both f and g are of the form (13.9.1), and f∗ is clearly a rotation of 
by an angle θ about the 'vertical' axis in 
. Now let F = g−1 f g. As the
reader can check (and we shall need this later) and composition of maps of
the form (13.9.1) is again of this form, as is the inverse of any such map.
Thus F is of the form (13.9.1), so that F∗ is a rotation of . However,
As g∗ is a rotation, we see that F∗ is a rotation of angle θ about some axis.
Now F fixes z0 and -1/ 0; for example, F(z0) = g−1 f g(z0) = g−1 f
(∞) = g−1(∞) = z0. Thus F∗ is a rotation about an axis whose endpoints are
φ(z0) and −φ(z0). As z0 is arbitrary, we can take this axis to be any diameter
of , and so we can choose z0 and θ so that F∗ is any pre-assigned rotation.
These four lemmas compete our proof of Theorem 13.9.1. Clearly,
though, there is more to these ideas. First, and as suggested in the proof of
Lemma 13.9.5, we have the following result.

Theorem 13.9.6 The Möbius maps of the form (13.9.1) for a group, which
we denote by 
0.
Essentially the same argument proves the following result.
Theorem 13.9.7 The set
of 2 × 2 complex matrices is a group, called the unitary group.
It follows from our earlier work that the natural map Φ :  
 
0 is a
homomorphism with kernel {±I }.
Finally, we can relate all these ideas to quaternions for, as we have seen
earlier, quaternions and rotations of 
 are intimately linked. The group 
defined in Theorem 13.9.7 is isomorphic to the multiplicative group of
quaternions q with ||q|| = 1 by the map
where a = a1 + ia2 and b = b1 + ib2. If we identiy the complex number i
with i, this map can be written more concisely as (a, b) 
 a + bj.
In conclusion, we can summarize these ideas in the following informal
scheme, in which each 'link' has been discussed at some stage of this text:
Exercise 13.9
1.  Suppose that f is given by (13.9.1). Show that f fixes w if and only if it
fixes −1/ .
2.  Show that the map f (z) = e2iθ z can be written in the form (13.9.1),
where θ is real. Let z = x + iy. Show that

and deduce that f∗ is a rotation of the sphere of angle 2θ about the
vertical axis.
3.  Show that if
then f∗ is a rotation of the sphere about the real axis. What is the angle
of rotation of f∗? [Hint: consider φ(i) and φ f (i) .]
4.  Let
so that if z = x + iy and w = u + iv, then
Show that in this identification, matrix multiplication corresponds to
multiplication of quaternions.

14
Group actions
14.1 Groups of permutations
This chapter is devoted to the idea of a group acting on a set. Throughout
this section, G will be a group of permutations of a set X, and we shall refer
to this by saying that G acts on X. We shall discuss some of the geometric
ideas that play an important role in the analysis of a group action, and we
shall apply these to the study of the symmetry groups of regular solids.
Definition 14.1.1 Suppose that G acts on X. Then x is a fixed point of g in G
if g(x) = x, and the set of fixed points of g is denoted by Fix(g). Given x in
X, the group {g ∈ G : g(x) = x} of elements of G that fix x is called the
stabilizer StabG(x) of x. In plain language, StabG(x) is the set of g that fix x,
while Fix(g) is the set of x that are fixed by g.
Definition 14.1.2 Suppose that G acts on X and that x ∈ X. Then the subset
{g(x) : g ∈ G} of X is called the orbit of x under G, and it is denoted by
OrbG(x). The group G is said to act transitively on X if OrbG(x) = X for one
(or equivalently, for all) x in X.
When the group G is understood from the context, we shall usually omit the
suffix G and write Stab(x), Fix(g) and Orb(x). It should be clear that G acts
transitively on X if and only if for each x and y in X there is some g in G
such that g(x) = y. Next, OrbG(x) is the set of points in X that x can be
mapped to by some element of G. However, as

Orb(x) is also the set of points that can be mapped to x by some g in G. It is
obvious that X is the union of its orbits, and it is easy to see that any two
orbits are either equal or disjoint. Indeed, suppose that there is some z in
Orb(x) ∩ Orb(y), and take any w in Orb(x). Then there are elements f, g and
h in G with h(w) = x, g(x) = z and f (z) = y (see Figure 14.1.1). Thus
fgh(w) = y so that w ∈ Orb(y). We deduce that Orb(x) ⊂ Orb(y), and the
reverse inclusion holds by symmetry.
Figure 14.1.1
We illustrate these ideas with two examples (recall that |E| is the number
of elements in E).
Example 14.1.3 Let X = {1, 2, 3, 4, 5, 6, 7}, and let G be the cyclic group
generated by (1 2)(3 4 5 6). Then G acts on X, and consists of the four
elements I, (1 2)(3 4 5 6), (3 5)(4 6) and (1 2)(3 6 5 4). Here,
Notice that in each case, |Stab(x)| × |Orb(x)| = |G|.
Example 14.1.4 Let P be the regular n-gon whose vertices are at the n-th
roots of unity. The symmetry group G of P acts on the complex plane 
,
and we shall consider all points of . Each vertex v of P can mapped by a
rotation in G to any other vertex. Thus Orb(v) = V, where V is the set of

vertices of P, and Stab(v) contains two elements, namely the identity I and
the reflection in the line through v and 0. Clearly the same holds for any
positive scalar multiple of a vertex. Next, consider the origin 0; here,
Orb(0) = {0} and Stab(0) = G. Finally, if z does not lie on any line of
symmetry of P, then Stab(z) = {I}, and Orb(z) contains 2n points. Again, in
every case, |Stab(z)| × |Orb(z)| = |G|.
Suppose that G acts on X, and that x and y are in X. The next result says
that the most general map that takes x to y can be written either as (i) the
most general map f that fixes x followed by a single chosen map h of x to y,
or as (ii) a chosen map h of x to y followed by the most general map g that
fixes y (see Figure 14.1.2). It also shows that if x and y are in the same orbit,
then the set of h such that h(x) = y is both a left coset, and a right coset, in
G.
Figure 14.1.2
Theorem 14.1.5 Suppose that G acts on X, and that y = h(x), where x, y ∈
X and h ∈ G. Then
(14.1.1)
Proof Let G0 = {g0 ∈ G : g0(x) = y}. The general element of h Stab(x) is of
the form h f , where f (x) = x, and as h f (x) = h(x) = y, this shows that h
Stab(x) ⊂ G0. Now suppose that g0 ∈ G0. Then g0 = h(h−1g0) and this is in
hStab(x); thus G0 = h Stab(x). The proof that G0 = Stab(y)h is similar and is
omitted.
Note that in general there will be many maps, say h1, h2, . . . , in G that
map x to y and in this case we must have h1 Stab(x) = h2 Stab(x) = · · · and
Stab(y) h1 = Stab(y) h2 = · · ·. The next result is an immediate consequence
of (14.1.1); it says that the stabilizers of two points in the same orbit are

conjugate subgroups of G. Thus they are isomorphic subgroups and so have
the same order.
Corollary 14.1.6 Suppose that G acts on X, and that y = h(x), where x, y ∈
X and h ∈ G. Then Stab(y) = h Stab(x) h−1.
We come now to a geometric form of Lagrange's theorem.
Theorem 14.1.7: the orbit-stabilizer theorem Let G be a finite group
acting on a finite set X. Then, for any x in X,
(14.1.2)
Proof Take any x in X, and let the orbit of x be {h1(x), . . . , hr (x)}, where hi
(x) ≠ hj (x) when i ≠ j. According to Theorem 14.1.5, hj Stab(x) is the set of
elements in G that map x to hj (x), so we have
where these cosets are pairwise disjoint (this is, in fact, the partitioning of G
into left cosets with respect to the subgroup Stab(x)). Thus
as required.
As an illustration of this result, we can see that the group of rotations of a
cube has order 24. Indeed, the group acts as a permutation of the set of
vertices, each vertex is fixed by a group of rotations of order three, and each
vertex can be moved to any of the eight vertices by a rotation of the cube.
Similarly, the group of rotations of a regular tetrahedron has 3 × 4 elements.
Results on group actions on finite sets can sometimes be proved by
counting a finite set in different ways using the following simple principle.
As |E| is the number of elements in a set E, we have 
. Thus if
E and F are finite sets, then

this being the number of pairs (x, y) with x in E and y in F.
The last result in this section provides a formula for the number of orbits
in a group action and it is proved in this way. This result appeared in a text
by Burnside in 1897, but it was known earlier by Cauchy, Frobenius and
others.
Theorem 14.1.8: Burnside's lemma Let G be a finite group acting on a
finite set X. Then there are N orbits, where
(14.1.3)
In particular, N is the average number of fixed points that an element of G
has.
Proof First,
as each set requires that g(x) = x. This implies that
or, equivalently, that
and this shows that the two sums in (14.1.3) are equal to each other.
We now prove (14.1.3). The action of G on X partitions X into N pairwise
disjoint orbits which we denote by 
1, . . . , 
N . As summing over X is the
same as summing over each orbit 
j, and then summing over j, we obtain

(14.1.4)
Now take any point y in 
j. As the stabilizers of points in the same orbit
have the same order, we see from (14.1.2) that
This combined with (14.1.4) gives (14.1.3).
The next example illustrates the use of Burnside's lemma and, as this is
an important technique, we consider it in detail.
Example 14.1.9 Consider the problem of arranging two identical red beads,
and four identical blue beads, on a circular ring of wire. We may assume
that the beads are placed at the vertices of a regular hexagon, and we agree
that two configurations are to be considered to be the same if one
configuration can be mapped onto the other by a symmetry of the hexagon.
We want to know how many different configurations there are. This number
is three, for we can label the vertices 1, 2, 3, 4, 5, 6, and then each
arrangement is equivalent to the two red beads being placed at one of the
pairs (1, 2), (1, 3) or (1, 4) of vertices. Nevertheless, our aim is to illustrate
the use of Burnside's Lemma rather than to solve the problem.
Each configuration of beads (without any identification) can be
represented by a function f : {1, 2, 3, 4, 5, 6} → {R, B} with the property
that f (n) = R for two values of n and f (n) = B for four values of n (this f
represents the configuration in which the red beads appear at those vertices
n for which f (n) = R). Let 
 be the set of such maps; clearly, 
 has 
 elements.
Now once we identify different configurations, different functions
represent the same configuration of beads; in fact, the functions f1 and f2

represent the same configuration (after identification) if and only if there is
some symmetry, say ρ, of the hexagon such that f2 = f1ρ. The symmetries ρ
belong to the dihedral group D12, which we shall now regard as acting on 
by the rule ρ : f 
 fρ, or, equivalently, ρ(f) = fρ. Now we see that f1 and f2
represent the same configuration of beads if and only if there is some ρ such
that f2 = ρ( f1); thus the number N of different configurations is the same as
the number of orbits in the action of D12 on 
. As |D12| = 12, Burnside's
Lemma gives
To find N we need to find, for each ρ in D12, how many functions f in 
satisfy f = fρ, and we consider each ρ in turn. First, when ρ = I, all fifteen
functions f satisfy f = fρ. If ρ is a rotation for which there is some f with
f = fρ then ρ must have a two-cycle (corresponding to the two positions of
the red beads). The only rotation in D12 with a two cycle is (1 4)(2 5)(3 6),
and there are exactly three functions f for which f = fρ (for example, the
function f with f (2) = f (5) = R).
Now suppose that ρ is a reflection across a line joining two vertices.
There are three such ρ, and for each of these there are three functions f with
f = fρ. For example, if ρ = (2 6)(3 5), then the three functions take the value
R on the sets {2, 6}, {3, 5} and {1, 4}, respectively. Finally, suppose that ρ
is a reflection across a line joining the mid-points of two sides. Again there
are three such ρ and three functions f for each ρ. For example, if ρ = (1 4)(2
3)(5 6), then the three sets on which a function can take the value R are {1,
4}, {2, 3} and {5, 6}. We conclude that N is [15 + 3 + (3 × 3) + (3 × 3)]/12,
which is 3.
Exercise 14.1
1.  Let G be the group {1, i, −1, −i}. Show that for each g in G the map x 
gx is a permutation of G.

2.  Let G be a finite group of permutations of a finite set X. Show that if G
is abelian and transitive, then |G| = |X|.
3.  Let Q be a plane quadrilateral and let G(Q) be its symmetry group (that
is the group of Euclidean isometries mapping Q onto itself). Show that
G(Q) has at most 8 elements (so that Q has the largest symmetry group
when it is a square). For each n in {1, 2, . . . , 8} determine whether or
not there is a quadilateral Q with G(Q) of order n. Is it true that if G(Q)
has order eight then Q is a square?
4.  Work through the problem considered in Example 14.1.9 in the case
when there are three identical red beads and three identical blue beads.
14.2 Symmetries of a regular polyhedron
The following is a list of all regular polyhedra (see Section 5.5):
(1)  the tetrahedron (four triangular faces, six edges, four vertices, and three
faces meeting at each vertex);
(2)  the cube (six square faces, twelve edges, eight vertices, and three faces
meeting at each vertex);
(3)  the octahedron (eight triangular faces, twelve edges, six vertices, and
four faces meeting at each vertex);
(4)  the dodecahedron (twelve pentagonal faces, thirty edges, twenty
vertices, and three faces meeting at each vertex);
(5)  the icosahedron (twenty triangular faces, thirty edges, twelve vertices,
and five faces meeting at each vertex).
These polyhedra were first listed by Theatus in about 400 BC, they are
often referred to as the Platonic solids, and they are illustrated in Figure
5.5.1. We shall take the existence of these polyhedra for granted, together
with the fact that the vertices of each of these polyhedra lie on a sphere,
which we may assume is centred at the origin in 
. For each polyhedron,
we consider the group G+ of rotations of 
 that leaves the polyhedron
invariant (and permutes its vertices). Observe that as any such rotation
permutes the set of vertices, it must leave the (unique) sphere through the
vertices invariant, and hence it must fix the origin. Next, we recall that, say,
q regular p-gons meet at each vertex υ, and we accept (again without proof)
that there is a rotation of order q (about an axis through υ and the centre of
the polyhedron) which fixes υ and leaves the polyhedron invariant. This

rotation will cyclically permute the q edges emanating from υ, and the
stabilizer of each vertex is therefore a cyclic group of order q. Finally, we
accept without proof that, by repeated rotations of the polyhedron, we can
move any vertex to any other; thus the orbit of any given vertex υ is the set
of all V vertices (and G+ acts transitively on V). If we now apply the
Orbitstabilizer theorem Theorem 14.1.7) we conclude that G+ has order qV ;
thus, from (5.5.2),
This gives |G+| to be twelve for the tetrahedron, twenty four for the cube
and the octahedron, and sixty for the dodecahedron and the icosahedron.
Note that we could have found that |G+| = 2E from the plausible assumption
that a given directed edge of the polyhedron can be moved by a rotation of
the polyhedron onto any other edge with either direction assigned to it. In
any event, we have established the following fact.
Theorem 14.2.1 The group of rotations of a regular polyhedron with E
edges has order 2E.
Each regular polyhedron is left invariant by a reflection σ across some plane
passing through the origin. As the composition of two indirect isometries is
a direct isometry, the group G of isometries that leave the polyhedron
invariant has the coset decomposition G+ ∪ σ G+, so that |G| = 2|G+|. Note
that whereas each rotation (in G+) is physically realizable as a rotation of 
, the reflections that leave the polyhedron invariant cannot be realized
physically. Mathematically, this distinction is whether the symmetry lies in
the orthogonal group (of matrices A with det(A) = ±1) or the special
orthogonal group (of matrices A with det(A) = 1).
The rest of this section contains a discussion of each Platonic solid and
its symmetry group. Throughout, G+ will denote the group of rotations, and
G the group of isometries, that leave the solid invariant. We shall take each
solid in turn, but we draw attention to the fact that the cube and the
octahedron are dual solids, as are the dodecahedron and the icosahedron,
while the tetrahedron is self-dual. Duality implies that if a polyhedron P has

V vertices, E edges and F faces, then the dual polyhedron has F vertices, E
edges and V faces, but it means more than this.
Example 14.2.2: the cube The existence of the cube is not in doubt. The
symmetry group G+ has twenty four elements, and we shall now identify
the twenty three rotations which, together with I, constitute G+. First, let L1
be a line through the centres of opposite faces. There are three choices for
L1 and, for each choice, G+ contains a cyclic subgroup of order four of
rotations about L1. This provides us with nine non-trivial rotations in G+.
Next, let L2 be a line through the midpoints of opposite edges. There are six
choices of L2 and, for each choice, G+ contains a subgroup of order two of
rotations about L2. This provides us with six more non-trivial rotations.
Finally, let L3 be a line through a pair of opposite vertices. There are four
choices of L3 and, for each choice, G+ contains a cyclic subgroup of order
three of rotations about L3. This provides us with eight more non-trivial
rotations in G, making a total of twenty three non-trivial (distinct) rotations
in G.
We shall now show that the group G+ of rotations of a cube C is
isomorphic to S4. Let d1, d2, d3, d4 be the four diagonals of C. As each
rotation is an isometry, a rotation of C must map a pair of diametrically
opposite vertices to another such pair, and this means that we can regard G+
as acting on the set {d1, d2, d3, d4} of diagonals. Now consider a rotation of
order two whose axis is the line through the midpoints of a pair of opposite
edges, say E and E′. It is easy to see that this induces a transposition of one
pair of diagonals while leaving the other two diagonals invariant (the two
invariant diagonals are those which do not have a common endpoint with E
or E′). As S4 is generated by its transpositions, we see that G+ (regarded as
the permutation group S4 of the four diagonals) contains S4. As
|G+| = 24 = |S4|, we see that G+ = S4.
Example 14.2.3: the tetrahedron The existence of the regular tetrahedron
is not in doubt, but perhaps the simplest construction of a regular
tetrahedron is as follows. Mark a vertex υ of a cube, and then mark the three
other vertices of the cube that share a diagonal of a face with υ. The four

marked vertices are then the vertices of a regular tetrahedron (see Figure
14.2.1). Notice that each pair of opposite edges of the tetrahedron arises as
a pair of skew diagonals of opposite faces of the cube.
Figure 14.2.1
It is easy to describe the eleven non-trivial rotations in the symmetry
group G+ of a tetrahedron T . First, let L1 be the line through a vertex and
the centroid of the opposite face. There are four choices of L1 and, for each
choice, there is a cyclic group of order three of rotations about L1 that
leaves T invariant. This provides eight non-trivial rotations of T . The
remaining three rotations are the rotations of order two whose axis is on a
lines though the mid-points of opposite edges of T (for example, in Figure
14.2.1 one such axis is the vertical line through the centre of the cube). If
we label the vertices, say a, b, c and d, and simply write down the twelve
elements of G+ as permutations of {a, b, c, d}, we see immediately that the
group G+ of rotational symmetries of a regular tetrahedron is the
alternating group A4. In fact, it is clear (geometrically) that G+ cannot
contain a rotation that acts as a transposition on {a, b, c, d}.
Now consider opposite edges e and e′ of T (for example, the horizontal
edges of the tetrahedron in Figure 14.2.1) We know that e lies in a plane Π
that is orthogonal to e′ and passing through the midpoint of e′. Let α be the
reflection in Π. Then α is a symmetry of T that interchanges the two
endpoints of e′ while fixing each endpoint of e. If we view G as acting on
the set of four vertices of T, then G contains every transposition of two
vertices, and hence every permutation of these vertices. As |G| = 24 = |S4|,
we see that G = S4.

Example 14.2.4: the octahedron Take a cube C; then the six midpoints of
the faces of the cube form the vertices of a regular octahedron (see Figure
14.2.2 in which the 'solid' points are the mid-points of the visible faces of
the cube). Similarly, the eight centroids of the equilateral triangular faces of
a regular octahedron are the vertices of a cube. This shows that the cube and
octahedron are dual solids (roughly speaking, the vertices of one are at the
centres of the faces of the other). If we rotate the cube we rotate the
inscribed octahedron; likewise, a rotation of an octahedron is also a rotation
of the inscribed cube. It is clear, then, that the two symmetry groups of the
octahedron coincide with those of the cube.
Figure 14.2.2
Example 14.2.5: the icosahedron The icosahedron and the dodecahedron
are more difficult to construct, and the geometry of these figures reveals
that the golden ratio
appears as the ratio of certain lengths in the construction. We note for future
use that τ2 = τ + 1, and that τ3 = τ2 + τ = 2τ + 1. We now claim that the
twelve points whose coordinates are

(14.2.1)
form the twelve vertices of a regular icosahedron of edge length 2 that lies
on the sphere S0 centred at the origin with radius τ + 2. To see this, let us
focus on the point (τ, 1, 0), and let S1 be the sphere with centre (τ, 1, 0) and
radius 2. The points (x, y, z) on S0 ∩ S1 are the common solutions of
and this intersection is easily seen to be the plane Π given by xτ + y = τ . Let
us now see which of the points in (14.2.1) lie on Π. If x = 0 then y = τ so the
points (0, τ, ±1) lie on Π. If x = 1 then y = 0 and the points (1, 0, ±τ ) lie on
Π. If x = −1 then y = 2τ so there are no points in (14.2.1) with x = 1 that lie
on Π. Similarly, we cannot have x = −τ . Finally, if x = τ , then y = 1 so the
single point (τ, 1, 0) lies on Π. We have now shown that the five points (0, τ,
±1), (1, 0, ±τ ) and (τ, 1, 0) in (14.2.1) lie at a distance two from (τ, 1, 0),
and it is easily checked that any two of these points are at a distance two
apart. The reader may care to draw a diagram (with three axes) in which
these five points, the point (τ, 1, 0) and the plane Π are illustrated.
The determination of the symmetry group of the icosahedron requires
more work, and here is one way to approach the problem. Briefly, the
icosahedron has twenty triangular faces which can be subdivided into five
sets of four triangles in such a way that the centroids of the four triangles in
a set form the vertices of a regular tetrahedron. For example, the following
sets of vertices of the icosahedron determine a triangle Tj, and the four
centroids of the faces Tj form a regular tetrahedron:
A similar argument holds for the other sets of triangular faces, and in this
way we can construct five regular tetrahedra whose vertices lie at the
centroids of the faces of the icosahedron. Any symmetry of the icosahedron

will permute these five tetrahedra, and it can be shown that the symmetry
group G+ acts on these five tetrahedra as an even permutation. Thus G+ may
be regarded as a subgroup of A5 (the even permutations of five objects) and
as |G+| = 60 = |A5|, we see that G+ = A5. We state (without proof) that the
full symmetry group of the icosahedron is A5 × C2 (and not S5).
Example 14.2.6: the dodecahedron The dodecahedron and the
icosahedron are dual solids; that is the centroids of the faces of one of these
solids are at the vertices of a solid of the other type. Accordingly, their
symmetry groups are the same and, with a little work) one can write down
the coordinates of the vertices of a regular dodecahedron.
Exercise 14.2
1.  Let G be the group of permutations of {a, b, c, d}, and let α = (b c d) and
β = (c d a). Show that A4 is generated by α and β, and interpret this in
terms of the symmetries of a regular tetrahedron.
2.  Verify the following construction of a regular octahedron. Consider a
vertex υj and its opposite face fj of a regular tetrahedron T. Let Πj be the
plane, parallel to fj, and through the mid-points of the three edges that
contain υj. Then Πj cuts the tetrahedron into two pieces, one of which is
a smaller tetrahedron Tj, say, with vertex υj. If we remove the four
tetrahedra Tj from T the remaining solid is a regular octahedron.
3.  Let C be the cuboid given by
where a, b and c are distinct, and let G+ be the group of rotations about
the origin that leave C invariant. Show that |G+| = 4, and that |G| = 8.
Identify (geometrically) all eight elements of G, and show that G
consists of the group of eight diagonal matrices, all of whose diagonal
elements are ±1.
4.  Show that if a rotation of a cube leaves each diagonal invariant then it is
the trivial rotation.
14.3 Finite rotation groups in space

We have identified the finite groups of rotations that arise as the symmetry
group of one of the regular polygons, or one of the polyhedra, and we now
raise the question of whether there are any other finite groups of rotations.
The answer is 'no', and the sole aim of this section is to prove the first of
the following two results, and to state the second. A very brief sketch of a
proof of the second result is given in Section 15.6.
Theorem 14.3.1 Any finite group of rotations of 
 is cyclic, dihedral, or
the symmetry group of a regular solid.
Theorem 14.3.2 Any finite group of Möbius maps is isomorphic to a finite
group of rotations of 
.
The proof of Theorem 14.3.1 Let G be a finite, non-trivial, group of
rotations of 
. Then each element g of G leaves the unit sphere ||x|| = 1
invariant, and g has exactly two (diametrically opposite) fixed points on the
sphere (unless g = I ). Let X be the set of points on the sphere that are fixed
by some non-trivial rotation in G. If x ∈ X, then there is some h in G with
h(x) = x, and then g(x) is fixed by ghg−1. It follows that each g in G maps X
onto itself, so that G acts as a group of permutations of the finite set X.
Now X is the union of, say N orbits, which we denote by 
1, . . . , 
N.
Associated to each orbit 
j is an integer nj which is the order of the
stabilizer of any x in 
j. Theorem 14.1.7 implies that nj |
j | = |G|, and as
we obviously have 
, we find that
(14.3.1)
Next, each g in G, g ≠ I, has exactly two fixed points in X, and I has |X|
fixed points in X. Thus, from Theorem 14.1.8,
If we use (14.3.1) to eliminate |X|, we find that

(14.3.2)
As 1/2 ≤ 1 − 1/nj < 1 and |G| ≥ 2, this shows that N is 2 or 3.
Case 1: N = 2. In this case (14.3.2) reduces to
which, as nj ≤ |G|, gives n1 = n2 = |G|. As N = 2 there are exactly two
orbits 
1 and 
2 of fixed points, and as | ) j| = |G|/nj, each orbit
contains only one point. These points must be diametrically opposite
points on , and we deduce that G is a cyclic group of rotations of the
sphere.
Case 2: N = 3. In this case we have
(14.3.3)
We may assume that n1 ≤ n2 ≤ n3; then 1 < 1 + 2/|G| ≤ 3/n1 so that
n1 = 2. Using this in (14.3.3) we find that
so that n2 is 2 or 3. We can now use these values of n2 in (14.3.3), with
n1 = 2, to find all solutions of (14.3.3). Thus with N = 3 we obtain the
following possibilities:
(1)  (n1, n2, n3) = (2, 2, n) and |G| = 2n;
(2)  (n1, n2, n3) = (2, 3, 3) and |G| = 12;
(3)  (n1, n2, n3) = (2, 3, 4) and |G| = 24;
(4)  (n1, n2, n3) = (2, 3, 5) and |G| = 60.

We shall not go into details, but we now have enough information
about the possibilities for G to show that in (1) G is the dihedral group
D2n, while in (2), (3) and (4) G is one of the symmetry groups of the
regular polyhedra.
14.4 Groups of isometries of the plane
Each isometry of 
 is either a direct isometry z 
 az + b, or an indirect
isometry 
, where, in each case, |a| = 1. The group of all
isometries is denoted by I( ), and the group of direct isometries by I+( ).
Both of these groups act on 
, and our first task is to find all finite
subgroups of I( ). As always, I denotes the identity map.
Theorem 14.4.1 The only finite subgroups of I( ) are the cyclic and
dihedral groups.
Proof Let G be a finite subgroup of I( ), let {z1, . . . , zn} be an orbit, and let
ζ be the centre of gravity of this orbit. If g is in G then g permutes the points
in the orbit, so that if g(z) = az + b, then
The same is true if 
, so that the elements of G have a common
fixed point ζ. By choosing coordinates appropriately we may assume that
ζ = 0; then every element of G is of the form z 
 az or 
, where
|a| = 1. Now consider the subgroup G+ of direct isometries in G. This is a
finite group of rotations about the origin, and it is easy to see that this is a
cyclic group. If G contains a map 
 then G is a dihedral group.
The next task is to describe all abelian subgroups of I( ).
Theorem 14.4.2 Each of the following is an abelian subgroup of I( ):
(a)  the group 
 of all translations;
(b)  a group of rotations 
 about a given point;

(c)  a group  (of order four) generated by the reflections in two orthogonal
lines;
(d)  a group  generated by a reflection across a line and all translations
along that line.
Further, any abelian subgroup of I( ) is a subgroup of one of these groups
(for a suitable choice of point or lines).
Proof We suppose first that G contains only direct isometries. We may
suppose that G is not the trivial group {I}; then G contains a translation, or
a rotation, or both.
(i)  Suppose that G contains a translation, say f (z) = z + t, where t = 0. Let
g be any element of G, say g(z) = az + b. As f g(0) = g f (0) we see that
a = 1, so that g is a translation. Thus G is a subgroup of 
.
(ii)  Suppose that G contains a rotation. Then, from (i), G contains only
rotations and I. Let f and g be any rotations in G with (finite) fixed
points ζf and ζg, respectively. As G is abelian, f (g(ζf)) = g(f(ζf)) = g(ζf),
so that f fixes g(ζf). As f has a unique fixed point, g(ζf) = ζf. However, g
also has a unique fixed point; thus ζf = ζg. We conclude that G is a
group of rotations about a single point, and this is case (b).
We now consider an abelian group G that contains an indirect
isometry. There are three cases to consider, namely when the subgroup
G+ of direct isometries is {I}, or contains translations, or contains
rotations.
(iii) Suppose that G+ = {I}. If g is an indirect isometry in G, then g2 is a
direct isometry so that g2 = I and g = g−1. Thus every indirect isometry
in G is a reflection. If g and h are indirect isometries in G, then gh ∈
G+so that gh = I. Thus h = g−1 = g, and then G = {I, g}, where g is a
reflection.
(iv) Suppose that G+ contains a rotation. We may assume that the rotation
is f (z) = μz, where |μ| = 1 and μ ≠ 1. Let 
 be any indirect
isometry in G. As f g = g f we see that for all z, 
.
This shows first that μb = b, so that b = 0, and second, that 
. As |
μ| = 1 and μ ≠ 1 we see that μ = −1. It follows that G contains only one
rotation, namely f (z) = −z, and that every indirect isometry in G is of

the form 
, where |c| = 1, and hence is a reflection. Now suppose
that g and h are reflections in G. Then gh is either I or f ; hence h is g or
g f , and G = {I, f, g, g f}. The reflections g and g f must be reflections
across orthogonal lines because their product is the rotation f by an
angle of π.
(v)  Suppose that G+ contains a translation, say f (z) = z + t; we may
assume that t is real and non-zero. In addition, G contains an indirect
isometry, say 
. As f g = g f we see that a = 1, so that G
contains f (z) = z + t and 
. Next, suppose that h is any direct
isometry in G; then (by what we have shown above) h is a translation,
say h(z) = z + s. As gh = hg we see that s is real; thus the only direct
isometries in G are real translations. Now consider any indirect
isometry k in G; then, as above, 
, say. As gk = kg we see
that b and c have the same imaginary part, say ρ. It follows that G is a
subgroup of the group generated by all real translations and the map 
. Finally, this larger group is a group of the form  in (d)
with the invariant line given by y = ρ/2.
The most well known groups of isometries of 
 are the seventeen
'wallpaper groups' and the seven 'frieze groups'. We shall discuss the frieze
groups as these provide a good illustration of the use of quotient groups,
and of conjugacy classes. A 'frieze' is a decorative strip with a repeating
pattern, and a frieze group is the symmetry group of some frieze. Frieze
groups are often described by drawing repeated motifs along a line, but here
we prefer a more analytical approach (which is free of motifs) in order to
illustrate the use of group theory. Reader wishing to 'see' the seven
possibilities can find pictures of them in many texts (or create pictures from
our list of seven groups).
Given any group G of isometries of , the set T of translations in G is a
subgroup of G. This is the translation subgroup of G, and it is always a
normal subgroup of G. To see this, take any translation in G, say f
(z) = z + t. Now any direct isometry in G is of the form g(z) = az + b, and
any indirect isometry is of the form 
. A simple calculation
shows that both g f g−1 and h f h−1 are translations, and this shows that T is a
normal subgroup of G. The most important consequence of this fact is that
we can now consider the quotient group G/T.

Definition 14.4.3 A frieze group is a group 
 of isometries of  that leaves
the real line R invariant, and whose translation subgroup 
 is an infinite
cyclic group.
Our aim is to classify the frieze groups, and we shall do this by showing
that the quotient group 
/
 has at most four elements, and then
considering all possibilities. However, before we can list the possibilities,
we need to decide when two frieze groups are to be considered as the
'same' group. Two frieze groups may be isomorphic but have quite
different geometric actions; for example, the symmetry group of a normal
pattern of footprints is an infinite cyclic group generated by a glide
reflection, and this has a quite different geometric action to the (isomorphic)
infinite cyclic group generated by a translation. This means that a
classification by isomorphism classes is inadequate (this is because we are
interested in the geometry as well as the algebra), and we shall classify
frieze groups geometrically by considering conjugacy in I( ). Thus two
frieze groups 
1 and 
2 are to be 'identified' if and only if there is an
isometry g such that 
2 = g
1g−1. Informally, conjugate frieze groups have
similar geometric actions.
If 
1 and 
2 are cyclic groups of translations, generated by z → z + t 1
and z 
 z + t2, respectively, then 
2 = g
1 g−1, where g(z) = (t2/t1)z. Thus
any frieze group is conjugate to another frieze group whose translation
subgroup 
 is generated by z 
 z + 1. It follows that from now on we may
(and shall) restrict our attention to frieze groups whose translation subgroup
 is the group of integer translations z 
 z + n, where n ∈ 
. It is
convenient to call such a frieze group a standard frieze group. We shall
prove the following result.
Theorem 14.4.4 Any frieze group is conjugate to exactly one of the seven
groups
where 〈a1, . . . , ak〉 denotes the group generated by a1, . . . , ak.

A summary of the proof The first step is to show that, apart from
translations, there are only four types of elements in a frieze group. We then
show that two elements of the same type yield the same coset with respect
to 
; thus the quotient group 
/
 has order at most five. The next step is
to show that every non-trivial element in the quotient group has order two,
and this leads to the following result.
Lemma 14.4.5 Let 
 be a standard frieze group. Then 
/
 is either the
trivial group, a cyclic group of order two, or isomorphic to the Klein 4-
group.
Clearly, (14.4.1) corresponds to the case when 
/
 is the trivial group (that
is, 
 = 
). A little more analysis shows that (14.4.2) and (14.4.3)
correspond to the cases when 
/
 has order two and four, respectively.
We shall now give the details, and we begin by finding a general form of
an element g in a standard frieze group 
. First, g(z) is either az + b or 
, where b = g(0) and a = g(1) − g(0). As g( ) = 
 we see that a and
b are real. As |a| = 1 this gives a = ±1. Finally, if 
 then g2 is a
translation so that 2b ∈ 
. This shows that every element of a standard
frieze group is of one of the following forms:
(1)  z 
 z + m, m ∈ ;
(2)  z 
 −z + b, b ∈ 
;
(3)  
(4)  
(5)  
We call these the five different types of elements. Note that 
 cannot
contain elements of type (4) and elements of type (5) as otherwise, 
would contain 
 (which is not in 
).
The crucial observation is that if g and h are of the same type, then g−1h
is a translation; thus we have the equality g
 = h
 of cosets. This implies
that each of the five types provides at most one coset to 
/
; thus 
/
 has
order at most five. Next, if g is any element of 
, then g2 ∈ 
 so that in
the quotient group, (g
)(g
) = g2
 = 
. Thus every element of 
/
 has
order two, and, as 
/
 has order at most five, this implies that 
/
 must

have order one, two or four. Moreover, if it has order four, it cannot be
cyclic and so it must be isomorphic to the Klein four-group. We have now
proved Lemma 14.4.5, and we consider each of the three cases.
Case 1: 
/
 is the trivial group.
In this case 
 = 
 , and 
 is the group given in (14.4.1).
Case 2: 
/
 has order two.
In this case 
 = 
 ∪ g
, where g is one of the four types (2)-(5), and 
is generated by g and t, where t(z) = z + 1.
If g is of type (2), say g(z) = −z + b, let h(z) = z − b/2. Then hgh−1 = −z
and hth−1 = t, so that h
h−1 = 〈z + 1, −z〉.
If g is of type (3), say 
, we take h as above, and then 
.
If g is of type (4), say 
, where m ∈ , then 
.
Finally, if g is of type (5), say 
, where m ∈ 
, then 
. All these cases are listed in (14.4.2).
Case 3: 
/
 has order four.
In this case 
/
 is the quotient group consisting of exactly four cosets,
with each coset containing an element of one of the types listed above.
As 
 is one of these cosets, and as 
 cannot contain both elements of
type (4) and elements of type (5), we see that there are only two
possibilities for 
/
, namely
(14.4.4)
where g j is of type j. In both cases, 
 contains g2(z) = −z + b and by
replacing 
 by h
h−1, where h(z) = z − b/2, we may assume that
g2(z) = −z. Note that as h is a translation, it commutes with t(z) = z + 1.
Also, hgjh−1 has the same type as gj so that the description (14.4.4) of
the two cases remains valid.
In the first case in (14.4.4), 
 contains 
, say, and
g2(z) = −z so it also contains 
. As this element is of type (4) (the
first case in (14.4.4) has no elements of type (5)), we see that b ∈ ,
so that 
.
Finally, consider the second case in (14.4.4). As before, 
 contains 
 and hence also 
. This time, this element must be

of type (5) so we see that 
. It is now clear that 
 contains 
, and that. 
. This completes the proof of
Theorem 14.4.4.
A full discussion of the seventeen crystallographic groups, or 'wallpaper
groups' as they are popularly called, is long, and we shall content ourselves
with a brief description of the starting point of such an investigation. Of
course, it is not difficult to list the seventeen groups (and again, there are
many texts that contains pictures of such groups); the hard work goes into
showing that these are the only such groups. Consider a group G of
isometries acting on . Each point z of  gives rise to an orbit {g(z) : g ∈
G}, and it is evident (because the elements of G are isometries) that if one
orbit accumulates at some point in , then so does every orbit. Thus every
orbit accumulates somewhere in  or none do.
Definition 14.4.6 A group G of isometries of 
 is discrete if no orbit
accumulates in .
A frieze group is a discrete group of isometries of 
 whose translation
subgroup is cyclic. A crystallographic group is a discrete group of
isometries of  whose translation subgroup is generated by two translations
in different directions. Thus, in some sense, the crystallographic groups are
the two-dimensional versions of the one-dimesional frieze groups, and it is
clear that there are crystallographic groups in all dimensions. We end by
noting that, up to a suitable identification of groups, there are exactly
seventeen plane crystallographic groups and 230 crystallographic groups in 
.
Exercise 14.4
1.  Find (explicitly) the group of isometries g of  which satisfy g(Z) = Z.
Is this group abelian?
2.  If f (z) = az + b, or if 
, then a = f (1) − f (0). Now write a
f = a = f (1) − f (0). Show that the map f 
 af is a homomorphism of I(
) onto the multiplicative group {z : |z| = 1}, and that the kernel of this
homomorphism is the group of all translations. Deduce that if G is any

group of isometries of , then the translation subgroup T of G is a
normal subgroup of G.
3.  Show that no two of the seven groups listed in Theorem 14.4.4 are
conjugate to each other.
4.  Let G be a discrete group of isometries of . Show that the translation
subgroup T of G is of the form {z 
 ma + nb : m, n ∈ }, where a and
b are non-zero complex numbers such that a/b is not real.
14.5 Group actions
In this section we take a closer look at some of the ideas discussed earlier in
this chapter. We begin with two simple examples which show the need for a
deeper investigation.
Example 14.5.1 Consider the group G consisting of isometries I (z) = z, 
 and 
. Clearly, G acts on C. However, we can
also view G as a group acting on 
. Now I = f and g = h throughout 
 so
that although G has order four, it appears to have order two when we restrict
its action to .
Example 14.5.2 We have already used the idea that a group G can be
thought of as acting on a particular set if we are prepared to change our
view of G; for example, we considered G as a group of rotations of 
, but
then decided to view G as a permutation of the vertices of a polyhedron. In
this example, we point out that a given group G can (in this sense) act on a
set X in many ways. For example, the cyclic group {1, −1} can act on the
complex plane either as the group generated by a rotation of order two, or
as the group generated by a reflection in a line.
These (and other) ideas are best dealt with in the following way. At present,
we have decided that a group G acts on a set X if G is a group of
permutations of X. However, we obtain a greater flexibility (and rigour) if
we now extend this definition to say that a group G acts on X if G is
isomorphic to some group Γ of permutations of X. For example, if θ : G →
Γ is an isomorphism, we can think of G as acting on X by thinking of g(x)
when we really mean (θ(g))(x). In Example 14.5.2, for example, the group
{1, −1} acts on  either through the isomorphism θ1(1) = I and θ1(−1) = g,

where g(z) = −z, or through the isomorphism θ2 defined by θ2(1) = I and
θ2(−1) = h, where 
.
The apparent 'collapse' of order illustrated in Example 14.4.1 can be
explained by noting that there the map θ is a homomorphism rather than an
isomorphism. Let us be quite explicit. Using the same notation as in
Example 14.4.1, let G = {I, f, g, h} and Γ = {I, g}. Then G acts on  in the
usual way. Now let θ be the homomorphism from G onto Γ defined by
θ(I) = θ(f) = I and θ(g) = θ(h) = g. Note that θ is a homomorphism; for
example, θ(fg) = θ(h) = g = I g = θ(f)θ(g). The important point to note here
is that two functions in G have the same θ-image if and only if the two
functions agree on ; for example, g(x) = h(x) for all real x, and θ(g) = θ(h),
even though g ≠ h. Thus the apparent 'collapse' of the order of the group in
Example 14.4.1 is associated with, and explained by, the 'collapse' of the
group under a homomorphism. These comments lead us to the following
more general notion of the action of a group on a set.
Definition 14.5.3 Let G be a group and X be a set. An action of G on X is a
homomorphism θ of G onto some group Γ of permutations of X. In this case
we say that G acts on X.
Of course, if G happens to be a group of permutations of X, then we can
take θ to be the identity map from G to itself, and this recaptures the earlier,
and most obvious, way that G can act on X.
In Example 14.5.1 we have an action of a group G on a set X (namely 
)
in which it is impossible to tell the difference between two elements in G by
their action on X alone. Now it is usually important to know whether or not
the elements of G can be distinguished by their action on X alone, so we
introduce the following terminology.
Definition 14.5.4 Let the action of a group G on a set X be given by the
homomorphism θ : G → Γ. Then the action of G is said to be faithful, or G
is said to act faithfully on X, if θ is an isomorphism.
If this action is not faithful, then there are two elements f and g in G, with
f ≠ g but θ(f) = θ(g). In particular, θ(f) and θ(g) are the same permutation of
X.

Our immediate concern is to examine the impact that this discussion has
on the results in Section 14.1, and we shall see that the results there remain
valid for any group G that acts on X in this wider sense. Suppose that θ is an
action of a group G on X, and let Γ = θ(G) (so Γ is a group of permutations
of X). For brevity, we write gθ for θ(g), where g ∈ G. Naturally, for x in X
and g ∈ G we define
Now OrbG(x) = OrbΓ(x), and we need to find analogous relations for the
stabilizer and fixed point set. Let K be the kernel of the homomorphism θ.
Then every element of Γ is the image of exactly |K | elements in G, so that
|G| = |K | |Γ|. Moreover, as g ∈ StabG(x) if and only if gθ ∈ StabΓ(x) we see
that
Now Theorem 14.1.7 shows that
and from the observations just made we now have the following result.
Theorem 14.5.5 Suppose that the finite group G acts on X as in Definition
14.5.3. Then for any x in X, |G| = |OrbG(x)| × |StabG(x)|.
We began our study of groups with groups of permutations, and then moved
on to study 'abstract groups'. The last result in this section is due to Cayley,
and it shows that 'abstract' groups are, in fact, no more abstract than the
apparently more concrete permutation groups.
Theorem 14.5.6 Every group is isomorphic to a group of permutations of
some set.
Let us illustrate the main idea in the proof with an explicit example.

Example 14.5.7 Let G be the additive group 
. For each real a, let ta be the
translation by a (so that ta(x) = x + a), and let T = {ta: a ∈ 
}. Then each ta
is a permutation of 
, and T is a group under composition. Moreover, as
ta+b = tatb, the map a → ta is a homomorphism of G onto T. In fact, this
homomorphism is an isomorphism because if ta = tb then a = b. It is
worthwhile to pause and extract the main idea from this example: we have
used each element a of G to create a permutation ta of G (as a set) in such a
way that G itself is isomorphic to the group of these permutations. The
proof of Theorem 14.5.6 is similar.
The proof of Theorem 14.5.6 Let G be any group. For each g in G let θg: G
→ G be the map defined by θg(h) = gh (informally, θg is the instruction
'multiply on the left by g'). We begin by showing that each θg is a
permutation of G. Certainly, θg is a map of G into itself. Next, if
θg(f) = θg(h) then g f = gh so that f = h; thus θg is injective. Finally, take any
h in G and notice that θg(g−1h) = gg−1h = h so that θg is surjective. Thus
each θg is a permutation of G.
It follows that we have just constructed a map θ that takes g to θg, and
this is a map from G to the group, say 
, of permutations of G. Now it is
easy to see that θ is a homomorphism from G to . Indeed, given g and h in
G,
and as this holds for all f, we see that θgh = θgθh. Thus θ : G → 
 is a
homomorphism. Finally, let Γ be the image of G under θ. As G is a group
and θ is a homomorphism, we see that Γ is a subgroup of 
. By definition,
θ maps G onto Γ; thus θ is an isomorphism from G onto the subgroup Γ of 
.
Exercise 14.5
1.  Let G be the group of transformations {I, f, g, h}, where f (z) = −z, 
 and 
. Show that the map v 
 f υ is a permutation of
G.

2.  Let G be any group, and let G act on itself as described in the proof of
Cayley's theorem. Show that G acts faithfully on itself.
3.  Let G be a group and H a subgroup of G. Show that G acts on the set of
left cosets by the rule that g (in G) takes hH to ghH; equivalently,
g(hH) = ghH. Now H itself is a left coset (= eH), so we can ask for the
subgroup of elements of G that fix H. Show that this subgroup is H; thus
any subgroup of any group arises as the stabilizer of some group action.

15
Hyperbolic geometry
15.1 The hyperbolic plane
In the earlier chapters we have discussed both Euclidean geometry and
spherical (non-Euclidean) geometry, and in this last chapter we discuss a
second type of non-Euclidean geometry, namely hyperbolic geometry.
Gauss introduced the term non-Euclidean geometry to describe a geometry
which does not satisfy Euclid's axiom of parallels, namely that if a point P
is not on a line L, then there is exactly one line through P that does not meet
L. In spherical geometry, the 'lines' are the great circles, and in this case
any two lines meet. Hyperbolic geometry is a geometry in which there are
infinitely many lines through the point P that do not meet the line L, and it
was developed independently by Gauss (in Germany), Bolyai (in Hungary)
and Lobatschewsky (in Russia) around 1820.
We begin by describing the points and lines of hyperbolic geometry
without any reference to distance. We shall take the hyperbolic plane to be
the upper half-plane 
 in . Notice that the real axis 
is not part of 
. A hyperbolic line (that is, a line in the hyperbolic
geometry) is a semicircle in 
 whose centre lies on 
; such semi-circles
are orthogonal to 
. However, as our concept of circles includes 'straight
lines' (see Chapter 14), we must also regard those straight lines that are
orthogonal to 
 as hyperbolic lines. Figure 15.1.1 illustrates the hyperbolic
lines in 
, and we remark that the two 'different' types of line are only
different because we are viewing them from a Euclidean perspective.

Figure 15.1.1
We notice immediately that Euclid's Parallel Axiom fails; indeed, the two
semi-circles have a common point P that does not lie on the line L;
moreover, it is easy to see that there are infinitely many hyperbolic lines
through P and not meeting L. It is clear, however, that any two hyperbolic
lines meet in at most one point, and that there is is a unique hyperbolic line
through any two distinct points in 
.
Now let
First, we leave the reader to check that Γ is a group. Next, we note that if g
is in Γ then it maps 
 into itself. Indeed, if we write g(z) = (az + b)/(cz + d),
and z = x + iy, then
(15.1.1)
Exactly the same reason shows that g also maps the lower half-plane (given
by y < 0) into itself; thus g must also map the circle 
 ∪ {∞} into itself. As
Γ is a group, the same holds for g−1; thus

This implies that the coefficients a, b, c and d in g may be chosen to be real
(note that we cannot assert that they are real, for they are only determined
to within a complex scalar multiple). The case c = 0 is easy, so we may
assume that c ≠ 0. Then, by scaling the coefficients by the factor 1/c, we can
choose these coefficients so that, in effect, c = 1. Then, as g(∞) = a, and
g−1(∞) = −d, we see that a and d are real. Finally, if a = 0 then −b = ad − bc
> 0 so that b is real. If, however, a ≠ 0, then −b/a = g−1(0) so that b is again
real. To summarize: if g(z) = (az + b)/(cz + d), and g ∈ Γ, then we may
assume that a, b, c and d are real. This implies that, for each z,
(15.1.2)
Let us now see how g in Γ acts on hyperbolic lines. Let z and w be
distinct points of 
, and let us consider the hyperbolic line L through z and
w. Now L is part of the (unique) Euclidean circle C that passes through z, w,
 and , so that g(C) passes through 
 and 
. However,
(15.2.1) now implies that g(C) passes through 
 and 
, so
that g(C) is orthogonal to 
 (see Figure 15.1.2). It follows that g(L) = 
 ∩
g(C), and this proves the following result.
Figure 15.1.2
Theorem 15.1.1 If g ∈ Γ and L is a hyperbolic line, then g(L) is a
hyperbolic line.

Theorem 15.1.1 suggests that the elements of Γ might be regarded as the
rigid motions of hyperbolic geometry. This suggestion is strengthened by
the fact (which will not be proved here) that any bijective map of 
∞ onto
itself that maps circles to circles is a Möbius map of z or of ; this is a type
of converse of Theorem 13.3.2. Further, any Möbius map that preserves 
must be in Γ (Exercise 15.1.1). In the next section we shall introduce a
distance in 
, and we shall then see that the elements of Γ are indeed the
isometries of 
.
There is a second model of the hyperbolic plane which is useful, and
often preferable to the model 
. In this model the hyperbolic plane is the
unit disc 
, namely {z : |z| < 1} (see Figure 15.1.3). The Möbius map
g(z) = (z − i)/ (z + i) maps 
 onto  (because 
 is given by |z − i| < |z + i|),
and so we may take the hyperbolic lines in the model D to be the images
under g of the hyperbolic lines in 
. Thus the hyperbolic lines in  are the
arcs of circles in 
 whose endpoints lie on the cirle |z| = 1 and which are
orthogonal to this circle at their endpoints. The two models 
 and 
 may
be used interchangeably, and any result about one may be transferred to the
other by any Möbius map that maps 
 to , or  to 
.
Figure 15.1.3
Exercise 15.1

1.  Show that if f is a Möbius map which maps  ∪ {∞} onto itself, then f
can be written in the form f (z) = (az + b)/(cz + d), where a, b, c, d are
real and ad − bc = 0. Show further that f (
) = 
 if and only if ad − bc >
0.
2.  Let z1 and z2 be distinct points in the hyperbolic plane. Show that there is
a unique hyperbolic line that passes through z1 and z2.
3.  Suppose that w1 and w2 are in 
. Show that there is some g in Γ such
that g(w1) = w2. This shows that the stabilizer of any point in 
 is
conjugate to the stabilizer of any other point in 
.
4.  Verify the steps in the following argument. Let g(z) = (z − i)/(z + i); then
g maps 
 onto , where  = {z : |z| < 1}. Note that g(i) = 0 and
g(−i) = ∞. Now suppose that f maps 
 onto itself and fixes i; then f also
fixes −i. It follows that g f g−1 maps  onto itself, and fixes 0 and ∞.
Thus g f g−1 is a Euclidean rotation about the origin, and hence the group
of hyperbolic isometries that fix a given point w is isomorphic to the
group of Euclidean rotations that fix the origin.
15.2 The hyperbolic distance
We shall now introduce a distance in 
, and then show that the elements of
Γ are isometries for this distance (in fact, they are the only orientation-
preserving isometries). There are two ways to define this hyperbolic
distance, and we shall start with the more elementary way. Consider distinct
points z1 and z2 in 
, and let L be the hyperbolic line through these points.
Then L has endpoints u and v, say, chosen so that u, z1, z2 and v occur in this
order along L (see Figure 15.2.1). We can find a Möbius map g in Γ such
that g(u) = 0 and g(v) = ∞ (see Exercise 15.2.1); then g(z1) = ia and
g(z2) = ib, say, where 0 < a < b. If we now recall that cross-ratios are
invariant under Möbius maps, we have
This allows us to make the following definition.

Figure 15.2.1
Definition 15.2.1 The hyperbolic distance between z1 and z2 in 
 is log [u,
z1, z2, v] when z1 ≠ z2, and zero otherwise. We denote this distance by ρ(z1,
z2).
Theorem 15.2.2 The elements of Γ preserve the hyperbolic distance
between two points in 
.
Proof This is immediate because the hyperbolic distance is defined as a
cross-ratio, cross-ratios are invariant under Möbius maps, and each g in Γ is
a Möbius map.
Notice that if, in Definition 15.2.1, we have z1 = ia and z2 = ib, where
0 < a < b, then
(see Definition 13.4.1). This leads to the following more general result.
Theorem 15.2.3 The hyperbolic distance is additive along hyperbolic lines.
Proof Suppose that z1, z2, z3 lie on a hyperbolic line L with end-points u and
v such that u, z1, z2, z3, v occur in this order along L. We can find some g in
Γ such that g(u) = 0 and g(υ) = ∞, and then g(zj) = iaj, where
0 < a1 < a2 < a3. As ρ(zi , z j ) = log aj/ai when i < j we see that

We are now in a position to give explicit formuale for the hyperbolic
distance (see Exercise 15.2.2).
Theorem 15.2.4 For z and w in 
,
(15.2.1)
(15.2.2)
Proof First, choose g in Γ (as above) so that g(z) = ia and g(w) = ib, where
0 < a < b. By applying the map z 
 z/a (which is in Γ), we may assume
that a = 1. Then ρ(z, w) = ρ(i, ib) = log b, so that
(15.2.3)
Next, let
Then, from (13.1.2) and (15.1.1), we see that F is invariant under any g in
Γ; that is,
Thus
(15.2.4)
and this together with (15.2.3) gives (15.2.1). The second formula (15.2.2)
follows from the fact that, for all z, cosh2 z = 1 + sinh2 z.

We remark that, as Theorem 15.2.4 suggests, in calculations involving
the hyperbolic distance it is almost always advantageous to use the
functions sinh or cosh of ρ(z, w) or 
; only rarely is ρ(z, w) used by
itself.
We end with a brief discussion of an alternative (but equivalent) way to
define distance. First, we define the hyperbolic length of a curve γ in 
 to
be the line integral
where, as usual z = x + iy. Now let L be the hyperbolic line through two
points z and w in 
, and let σ be the arc of L that lies between z and w. It
can be shown that σ has hyperbolic length ρ(z, w) and, moreover, that any
other curve joining z to w has a greater hyperbolic length than σ . Thus the
hyperbolic line through two points does indeed give the shortest path
between these points.
Finally, if g is in Γ, and g(z) = (az + b)/(cz + d), then, from (13.1.2), we
see that
where g′(z)is the usual derivative of g. In conjunction with (15.1.1) this
gives
This (together with the formula for a change of variable in a line integral)
shows that for each g in Γ, and each curve γ , g(γ ) has the same hyperbolic
length as γ .
Exercise 15.2
1.  Show that for any u and v in  ∪ {∞} with u ≠ v, there is a g in Γ with
g(u) = 0 and g(υ) = ∞. [Hint: apply z 
 −1/(z − υ) and then a
translation.]

2.  The functions sinh and cosh are defined by sinh z = (ez − e−z)/2 and cosh
z = (ez + e−z)/2. Show that (a) cosh2 z − sinh2 z = 1, and (b) cosh 2z = 2
cosh2 z − 1 = 1 + 2 sinh2 z.
3.  Find the hyperbolic distance between the points 1 + iy and −1 + iy as a
function of y. Show that for a given positive t there is a value of y such
that this distance is t.
4.  Let L be the Euclidean line given by Im[z] = 2. Show that 2i is the point
on L that is closest (as measured by the hyperbolic distance) to the point
i.
15.3 Hyperbolic circles
Suppose that w ∈ 
, and r > 0. The hyperbolic circle with hyperbolic
centre w and hyperbolic radius r is the set {z ∈ 
 : ρ(z, w) = r}.
Theorem 15.3.1 Each hyperbolic circle is a Euclidean circle in 
.
Proof Let C be the hyperbolic circle with centre w and radius r. There is a
map g in Γ with g(w) = i (see Exercise 15.1.2), so that g(C) is the
hyperbolic circle with centre i and radius r. Now by Theorem 15.2.4,
z ∈ g(C) if and only if 
, where z = x + iy. This
equation simplifies to give x2 + (y − cosh r)2 = sinh2 r, so that g(C) is a
Euclidean circle in 
. As g−1maps circles to circles, and 
 to itself, we see
that g−1 (g(C)) , namely C, is a Euclidean circle in 
.
Notice that the hyperbolic centre of a hyperbolic circle is not the same as
its Euclidean centre (and similarly for the radii); indeed, the hyperbolic
circle g(C) in the proof of Theorem 15.3.1 has hyperbolic centre i, and
Euclidean centre i cosh r (and cosh r > 1). A hyperbolic circle with centre w
and hyperbolic radius r is illustrated in Figure 15.3.1. Finally, it can be
shown that the length of a hyperbolic circle of hyperbolic radius r is 2π sinh
r, and that its hyperbolic area (which we have not defined) is 
.
Notice that the hyperbolic radius of a hyperbolic circle of radius r grows
roughly like πer; in the Euclidean case, it is 2πr. Finally, we mention (but do
not prove) the hyperbolic counterpart of the fact that the area of a spherical
triangle is π less than its angle sum.

Figure 15.3.1
Theorem 15.3.2 The area of a hyperbolic triangle with angles α, β and γ is
π − (α + β + γ ). In particular, this area cannot exceed π.
Exercise 15.3
1.  Find the equation of the hyperbolic circle with centre 2i and radius e2.
Suppose that this circle meets the imaginary axis at ia and ib, where
0 < a < 2 < b. Find a and b, and verify directly that ρ(ia, 2i) = e2 = ρ(2i,
ib).
2.  Consider the unit sphere in 
 as a model of spherical geometry in
which distances are measured on the surface of the sphere. What is the
circumference of the circle whose centre is at the 'north pole' (see
Figure 15.3.2) and whose radius is r? Now compare the circumference
of a circle of radius r in Euclidean geometry, spherical geometry and
hyperbolic geometry.

Figure 15.3.2
15.4 Hyperbolic trigonometry
A hyperbolic triangle is a triangle whose sides are arcs of hyperbolic lines.
We begin with the hyperbolic version of Pythagoras' theorem (see Figure
15.4.1).
Figure 15.4.1
Theorem 15.4.1 Suppose that a hyperbolic triangle has sides of hyperbolic
lengths a, b and c, and that the two sides of lengths a and b are orthogonal.
Then cosh c = cosh a cosh b.
In our proof of this result we shall need to use the fact that a Möbius map is
conformal; that is, it preserves the angles between circles. In particular, this
implies that if two circles C and C′ are orthogonal, and if g is any Möbius
map, then g(C) and g(C′) are orthogonal. We shall not give a proof of this
(although the proof is not difficult).

Proof Let the vertices of the hyperbolic triangle be υa, υb and υc, where υa is
opposite the side of length a, and so on. There is some g in Γ such that
υc = i and υb = ik, where k > 1. As g preserves the orthogonality of circles
we see that g maps υa to some point s + it, where s2 + t2 = 1 (see Figure
15.4.2). As g preserves hyperbolic distances, this means that we may
assume that υa = s + it, υb = ik and υc = i. Now ρ(i, ik) = a, ρ(i, s + it) = b
and ρ(ik, s + it) = c. As
the given formula follows.
Figure 15.4.2
It is interesting to examine Pythagoras' theorem for small triangles, and
for large triangles. As cosh z = 1 + z2/2 ! + z4/4 ! + · · ·, we see that when a,
b and c are very small, the formula is, up to the second-order terms,
c2 = a2 + b2. Thus, infinitesimally, the hyperbolic version of Pythagoras'
theorem agrees with the Euclidean version. This is because the hyperbolic
distance is obtained form the Euclidean distance by applying a 'local
scaling factor' of 1/y at z. As this scaling factor is essentially constant on an
infinitesimal neighbourhood of a point, the 'infinitesimal hyperbolic
geometry is just a scaled version of the Euclidean geometry. However, as
the scaling factor varies considerably over large distances, the global
hyperbolic geometry is very different from the Euclidean geometry. For
example, if in Pythagoras' theorem, a, b and c are all very large, then, as

cosh x is approximately ex/2 when x is large, we have (approximately)
4ec = eaeb so that c = a + b − log 2. In other words, in a 'large' hyperbolic
right-angled triangle, the length of the hypotenuse is almost the sum of the
lengths of the other two sides! If this were the case in Euclidean geometry,
then the triangle would be very 'flat', but this is not so in hyperbolic
geometry.
Finally, we remark that hyperbolic trigonometry is as rich and well
understood as Euclidean trigonometry (and spherical trigonometry) is. For
example, there is a sine rule, and cosine rules in hyperbolic geometry. In
most applications it is the hyperbolic trigonometry that is important, and
hyperbolic geometry by itself has relatively few applications.
Exercise 15.4
1.  Suppose that a, b and c are the sides of a right-angled hyperbolic triangle
with the right-angle opposite the side of length c. Prove that c ≤ a + b;
this is a special case of the triangle inequality.
2.  Consider a right-angled hyperbolic triangle with both sides ending at the
right angle having length a. Let the height of this triangle be h (the
distance from the right angle to the third side). Find h as a function of a.
What is the limiting behaviour of h as a → +∞?
15.5 Hyperbolic three-dimensional space
We end this text with a very brief description of three-dimensional
hyperbolic geometry, and a sketch of the proof of Theorem 14.3.2. These
are given in this and the next section, and they combine many of the ideas
that have been introduced in this text. We take hyperbolic space to be the
upper-half of 
, namely
It is convenient to identify the point (x, y, t) with the quaternion x + yi + tj,
and also to identify the quaternion i with the complex number i. Thus we
can write (x, y, t) as z + tj, where z is the complex number x + iy. Note that
in this notation we have the convenient formula
(15.5.1)

Suppose now that g(z) = (az + b)/(cz + d), where ad − bc ≠ 0. We can
now let g act on hyperbolic space 
3 by the rule
(15.5.2)
where this computation is to be carried out in the algebra of quaternions.
This lengthy (but elementary) exercise shows that
(15.5.3)
Notice that as quaternions are not commutative, we have to choose (and
then be consistent about) which side we shall write the inverse in (15.5.2).
However, in (15.5.3), the denominator is real (and positive), and as every
real number commutes with every quaternion, we can write it in the usual
form for a fraction without any ambiguity. Notice also that if we put t = 0 in
(15.5.3), we recapture the correct formula for the action of g on .
The consequences of (15.5.3) are far-reaching. First, if we consider g to
be a translation, say g(z) = z + b, then we find that g(z + tj) = (z + b) + tj;
thus g is just the 'horizontal' translation by b. If g(z) = az, then we find that
If |a| = 1, so that g is a rotation of the complex plane, then g acts on 
3 as a
rotation about the vertical axis through the origin. If a > 0, so that g acts as
a 'stretching' from the origin by a factor a in 
, then g also acts as a
stretching (from the origin, and by the same factor) in 
3. Of course, the
more interesting case is when g(z) = 1/z; here
We define the lines in 
 to be the 'vertical' semi-circles, and the
'vertical' rays (exactly as in the two-dimensional case; see Figure 15.5.1),
and we can define the hyperbolic distance between two points again by a
cross-ratio (thinking of the vertical plane through the two points as the

complex plane), or by integrating |dx|/x3, where x = (x1, x2, x3), over curves.
When all this has been done, we arrive at the following beautiful result.
Figure 15.5.1
Theorem 15.5.1 Every Möbius map acts on hyperbolic space 
3 as a
hyperbolic isometry, and every isometry that preserves orientation is a
Möbius map.
15.6 Finite Möbius groups
Finally, we give only the briefest sketch of the ideas behind a proof of
Theorem 14.3.2. The aim of this sketch is to give the reader a glimpse of
some beautiful interaction between algebra and geometry, and it is far from
being complete. First, the Möbius maps (that act on 
∞) can be extended
(either as a composition of reflections and inversions, or in terms of the
quaternion algebra) to act on all of 
. The upper-half 
3 of 
 with the
hyperbolic metric ds = |dx|/x3 is a model of three-dimensional hyperbolic
geometry, and the Möbius group is the group of orientation-preserving
isometries of this space.
Now let G be a finite Möbius group; then G may be regarded as a finite
group of isometries of 
3, so that each point in 
3 has a finite orbit. Take
any orbit and let B be the smallest hyperbolic ball that contains the orbit.
Analytic arguments show that B is unique, and as the chosen orbit is
invariant under G, so is B, and hence (finally) so too is the hyperbolic
centre of B. This argument proves that the elements of the finite group G
have a common fixed point ζ in 
3. There is now a Möbius map (which
acts on all of 
 ∪ {∞}) that converts the upper-half space model of three-
dimesional hyperbolic space into the unit ball model (much as there is a

Möbius map that takes the upper half-plane to the unit disc). This can be
chosen so that ζ is carried to the origin; thus the finite Möbius G is
conjugate to a Möbius group G′ of hyperbolic isometries that act on the unit
ball in 
 with the extra property that every element of G′ fixes 0. It is not
difficult to show that every such isometry is a Euclidean rotation of 
 and
the sketch of the proof is complete.

Index
 
At, 146
An, 13
D2n, 237
G+, 290
IX, 17
L(∞), 272
L ∪ {∞}, 277
Mm×n(
), 149
O(k), 9
q-cycle, 9
R( f, g), 171
SO(n), 202
Sn, 8
x  y, 212
arg z, 36
, 33
cosh, 312, 313
∆ (f), 171
gcd(m, n), 233
∞, 255
ker(θ), 246
μ (E), 77
⊕, 116
⊕n, 28
⊗n, 28
ρ (z1, z2), 311
ρcol(A), 140
ρrow(A), 140
sinh, 312, 313
ε (ρ), 13
, 31
 ∪ {∞}, 256
, 247
, 103
, 104
, 96
, 97
, 22
, 2, 5

, 2, 5, 22, 26
, 247
, 103
, 27
, 104
, 2, 4, 22
, 28
, 300
, 317
, 271, 272
 (V, W), 132
, 256
0, 281
 (X), 7, 18
A, 182
, 74, 276
, 281
det(A), 144, 145
dim(V) = 0, 106
Fix(g), 284
GL(2, 
), 257, 271
I(
), 297
I+(
), 297
ker(α), 127
OrbG(x), 284
P(
, d), 120
SL(2, 
), 258
SL(2, Z), 258
StabG(x), 284
tr(X), 152
Im, 34
lcm, 233
Re, 34
Abel, 216
abelian group, 216, 221, 236
abelian group; see also group, commutative, 4
act, 304
act transitively, 284
action
faithful, 304
action of a group, 304
action of a matrix, 154
addition modulo n, 28
additive function, 125
algebraic structure, 22
alternating group, 13, 292
altitude, 70
angle, 198
anti-trace, 152

Argand, 31
argument, 36
associative law, 2
associativity, 16
augmented matrix, 141
auxiliary equation, 119
axiom of parallels, 307
axis of rotation, 90
basis, 106, 158
change of, 168
orthonormal, 198
standard, 108
bell-ringing, 15
bijection, 17, 18
bijective, 17
binary operation, 2
binomial theorem, 36
Bolyai, 307
Burnside, 2, 287
Burnside's lemma, 287, 288
Cancellation Law, 3
Cardan, 31, 47
cardinality, 216
Cauchy, 2, 287
Cauchy-Schwarz inequality, 197, 198
Cayley, 305
Cayley's theorem, 305
Cayley-Hamilton theorem, 190
centre of a group, 222
characteristic equation, 181, 182, 208
characteristic polynomial, 182
Chebychev polynomial, 40
chordal distance, 278
circle, 262
complement, 263
Euclidean, 261
unit, 264
circle group, 253
closure axiom, 2
coefficients, 149
column, 149
column rank, 140
column vector, 149
j-th, 139
common divisor, 232
commutative group; see group, commutative, 4, 216
commute, 4, 216
complement of a circle, 263
complementary components of a circle, 264

complex conjugate, 33
complex line, 271
complex number, 31
complex plane
extended, 256
complex polynomials, 120, 123
even, 105
odd, 105
composition, 6, 16
concyclic points, 267, 268
conformal, 315
conjugacy class, 242, 243, 299
conjugate, 242
conjugate subgroup, 242, 286
convex polygon, 79
convex polyhedron, 81
convex spherical polygon, 80
coordinates, 53
coplanar, 53, 54, 61
coprime, 25, 233
corkscrew, 66
corkscrew rule, 63
right-handed, 59
coset, 220, 237, 243
left, 222, 243, 253, 286
right, 222, 243, 286
coset decomposition, 222
cosine rule
for a spherical triangle, 77
Cotes, 39, 50
Cramer's rule, 143
cross-ratio, 266, 311, 318
crystallographic group; see also wallpaper group, 302
cube, 291
cubic equation, 46
cubic polynomial, 171
cycle, 9, 13
length, 9
cycle type, 244
cyclic group, 230-232, 269, 296, 297
d'Alembert, 48
de Moivre, 39
de Moivre's theorem, 39
deficiency
total, 87, 88
deficiency of a vertex, 87
degree of a polynomial, 48
del Ferro, 47
deltahedron, 88

DeMorgan, 24
Descartes, 31
Descartes' theorem, 88
determinant, 64, 65, 144, 165, 169
diagonal elements, 149
diagonal matrix, 153, 184
diagonalizable matrix, 185, 188, 189, 191
diagonalizing, 274
difference equation, 118, 186
dihedral group, 237, 238, 241, 244, 259, 269, 296, 297
dimension, 106
finite, 106
direct isometry, 206
direct sum of two subspaces, 115
directed line segment, 54
discrete group, 302
discriminant, 171
disjoint permutations, 9
distributive laws, 27
divisor, 24, 232
dodecahedron, 290, 295
dual, 291
dynamics, 193
edges
of a polyhedron, 83
of a spherical triangle, 79
eigenvalue, 175, 178, 179, 201, 273
eigenvector, 175, 178, 273
generalized, 179
entries; see coefficients, 149
equivalence class, 228
equivalence relation, 227, 243
Erlangen Programme, 1
Euclidean circle, 261
Euclidean line, 261
Euler, 2, 38, 39, 48, 80
Euler's formula for triangulations, 80, 81, 85, 87
Euler's function φ (n), 234
Euler's theorem, 234
even complex polynomials, 105
even function, 151
even permutation, 13
extended complex plane, 256
extended line, 277
extends, 126
faces
of a polyhedron, 83
of a triangle, 80
faithful action, 304

Fermat's theorem, 234
Ferrari, 47
Fibonacci sequence, 120
field, 22, 27, 211
skew, 98
fixed point, 8, 260, 273, 284
frieze group, 299, 300, 302
standard, 300
Frobenius, 287
function, 6, 16
linear, 56
Fundamental Theorem of Algebra, 48, 51, 120, 176, 180, 182, 208
Fundamental Theorem of Arithmetic, 24
Gauss, 2, 31, 48, 307
Gaussian integers, 253
General Linear group, 257
generated, 105, 114, 220, 230
Girard, 77
glide reflection, 42, 236
golden ratio, 40, 293
great circle, 74
greatest common divisor, 172, 233
group, 1, 2
abelian, 216, 236
alternating, 13
circle, 253
commutative or abelian, 4
crystallographic, 302
cyclic, 269, 296, 297
dihedral, 237, 238, 241, 244, 259, 269, 296, 297
discrete, 302
frieze, 300, 302
General, 257
Modular, 258
Möbius, 256, 269, 319
non-abelian, 241
non-commutative, 4
permutation, 268
quotient, 252
Special Linear, 258
symmetric, 8, 14
unitary, 281
group action, 304
half-space, 81
Hamilton, 95
homogeneous linear equation, 136, 261
homogeneous quadratic form, 206
homomorphism, 246
homomorphism, kernel of a, 246

Hurwitz, 214
hyperbolic centre, 313, 314
hyperbolic circle, 313, 314
hyperbolic distance, 311
hyperbolic geometry, 307
hyperbolic length, 312
hyperbolic line, 307, 309
hyperbolic radius, 313
hyperbolic space, 317
hyperbolic triangle, 314, 315
hyperbolic trigonometry, 315
hyperplane, 135
icosahedron, 290, 293
identity element, 3, 17
identity function, 6
identity map, 91
identity matrix, 163
imaginary part of a complex number, 31
index
cyclic, 230-232
indirect isometry, 206
Induction, first Principle of, 22
inhomogeneous linear equations, 141
injective, 17
integers, set of, 22
invariant planes, 193
invariant subspace, α -, 175, 177
invariant under, 175
inverse element, 3
inverse function, 6, 17
inverse matrix, 164
inverse transformation, 164
invertible, 6, 17, 18
invertible matrix, 164, 200, 204
irrational number, 26
isometry, 41, 42, 89, 91, 92, 204, 242
direct, 94, 206
indirect, 94, 206
isomorphic groups, 226
isomorphism, 131, 226
iterate, 17, 133, 260
join of two subspaces, 112
kernel, 127
kernel of a homomorphism, 246, 248, 252
Klein 4-group, 301
Klein 4-group, 240
Klein, F., 1

Lagrange's interpolation formula, 121
Lagrange's theorem, 223, 232, 286
Laplace, 48
least common multiple, 233
least upper bound, 22
left coset, 222, 243, 253, 286
Legendre, 80
Legendre polynomials, 122
Leibniz, 48
length, 122
length of a cycle, 9
line, 135, 175
complex, 271
Euclidean, 261
linear function, 56
linear independence, 107
linear map, 124
linear operator, 125
linear transformation; see linear map, 124
Lobatschewsky, 307
lower-triangular matrix, 153
lune, 77
magic square, 152, 153
map, 16, 124
linear, 124
mapping, 16
matrix, 136, 149
action, 154
complex, 149
diagonal, 153, 184
diagonalizable, 185, 188, 189, 191
identity, 163
inverse, 164
invertible, 164, 200, 204
orthogonal, 200
product, 154
real, 149
skew-symmetric, 150, 204
square, 149
symmetric, 150
transpose, 146, 157
upper-triangular, 153
zero, 149
matrix coefficients, 149
matrix polynomials, 189
matrix product, 156
matrix representation, 158, 160, 207
Maurolico, 23
median, 70

modular arithmetic, 28, 221
Modular group, 258
modulus, 34
monoid, 234
multiplication modulo n, 28
multiplicity, 182
Möbius group, 256, 269, 319
Möbius inverse, 257
Möbius map, 254, 256, 257, 259, 260, 266, 272, 273, 279, 282, 296, 309, 315, 319
natural numbers, set of, 22
non-abelian group, 241
non-Euclidean geometry, 307
non-singualr matrix; see invertible, 164
normal subgroup, 243, 248, 251-253
nullity, 127
octahedron, 290, 293
odd complex polynomials, 105
odd function, 151
odd permutation, 13
one-to-one; see injective, 17
onto; see surjective, 17
operator
linear, 125
orbit, 9, 284
orbit-decomposition, 10
orbit-stabilizer theorem, 286
order, 22, 216, 227, 237
finite, 216
infinite, 216
order of a permutation, 15
orientation, 310, 319
negative, 63, 64, 66
positive, 63, 64, 66
orthogonal, 55, 59, 122, 135
orthogonal group, 202
special, 202
orthogonal map, 199
orthogonal matrix, 200
orthogonal projection onto a plane, 155
orthonormal basis, 198, 199
parallel, 54
Parallelogram Law of Addition, 53
parity, 12
Peano, 24
Pell sequence, 120
permutation, 11, 13, 18
disjoint, 9
even, 13

odd, 13
permutation group, 268
perpendicular; see orthogonal, 55
Plato, 83
Platonic solids, 83, 84, 269, 290
point at infinity, 255
polar coordinates, 37
polar form of a complex number, 38
polygon
convex, 79
convex spherical, 80
regular, 237
spherical, 79, 80
polyhedron, 81
convex, 81
edges of, 83
faces of, 83
general, 87
regular, 83, 84
vertices of, 83
polynomial
real, 50
polynomial of degree n, 48
polynomials
complex, 110
trigonometric, 104
prime number, 24
primitive root of unity, 46
product; see also composition, 6
proper subgroup, 219
proper subspace, 111
Ptolemy's theorem, 40
Pythagoras' theorem
for a spherical triangle, 76
for a tetrahedron, 72
Pythagoras' theorem, hyperbolic version, 315
quartic equation, 47
quaternion, 95, 96, 282, 318
addition, 96
conjugate of a, 98
multiplication, 96
norm of a, 98
pure, 97, 99
unit, 100
quintic equation, 47
quotient group, 252, 299
range, 127
rank, 127
rational number, 26

real line, 22
real numbers, set of, 22
real part of a complex number, 31
reciprocal vectors, 62
recurrence relation; see difference equation, 118
reflection, 42, 89, 93, 99, 205
regular n-gon, 45, 285
regular polygon, 45, 237
regular polyhedra, 290
regular polyhedron, 83, 84
representation
standard, 10
resultant, 171
right coset, 222, 243, 286
root of a of polynomial, 48
root of unity, 44, 218, 237
primitive, 46
rotation, 42, 93, 100
of 
, 90
row, 149
row rank, 140
row vector, 137, 149
j-th, 139
scalar product, 55, 93, 122, 135, 210
scalar triple product, 60
screw-motion, 94
self-conjugate subgroup, 243
self-dual, 291
set
closed with respect to an operation, 2
shear, 176
sign, 13
sine rule, 71
for a spherical triangle, 77
skew, 69
skew field, 98
skew-symmetric matrix, 150, 204
span, 107
Special Linear group, 258
special orthogonal group, 202
spherical area, 77
spherical distance, 74
spherical geometry, 74
spherical polygon, 79, 80
area of, 79
spherical triangle, 79
edges of a, 79
vertices of a, 79
stabilizer, 284, 310

standard basis, 108
standard frieze group, 300
standard representation, 10
stereographic projection, 276, 279
subgroup, 218
conjugate, 242, 286
non-trivial, 219
normal, 243, 248, 251-253
proper, 219
self-conjugate, 243
translation, 299
trivial, 219
subspace, 111
α -invariant, 175, 177
proper, 111
sum of two subspaces; see join, 112
surjective, 17
symmetric difference, 5
symmetric group, 8, 14
symmetric linear map, 207
symmetric matrix, 150
symmetry, 237
Tartaglia, 47
tetrahedron, 71, 290, 292
Theatus, 290
topological invariant, 81
torus, 236
total deficiency, 87, 88
trace, 152
translation, 42, 89, 236
translation subgroup, 299
transpose, 146, 157
transposition, 11
triangle inequality, 35, 56, 197
triangulation, 79
trigonometric polynomials, 104
trivial subgroup, 219
unit, 234
unit circle, 264
unit disc, 264, 309
unit quaternion, 100
unit sphere, 276
unit vector, 54
unitary group, 281
upper-triangular matrix, 153
valency, 85, 88
vector, 52, 53
column, 149

components of, 53
length, 54
norm, 54
row, 137
unit, 54
vector product, 57, 58, 93, 212
vector space, 102, 103
complex, 103
finite-dimensional, 106
infinite-dimensional, 106
real, 103
vector triple product, 62
vectors
reciprocal, 62
vertices
of a polyhedron, 83
of a spherical triangle, 79
von Dyck, 2
Wallis, 31
wallpaper group; see also crystallographic group, 299, 302
Well-Ordering Principle, 22
Wessel, 31


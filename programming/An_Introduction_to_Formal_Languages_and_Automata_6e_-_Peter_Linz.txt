
PETER LINZ
UNIVERSITY OF CALIFORNIA AT DAVIS
AN INTRODUCTION TO
SIXTH EDITION
FORMAL 
LANGUAGES AND
AUTOMATA

World Headquarters
Jones & Bartlett Learning
5 Wall Street
Burlington, MA 01803
978-443-5000
info@jblearning.com
www.jblearning.com
Jones & Bartlett Learning books and products are available through most bookstores and online booksellers. To contact Jones &
Bartlett Learning directly, call 800-832-0034, fax 978-443-8000, or visit our website, www.jblearning.com.
Substantial discounts on bulk quantities of Jones & Bartlett Learning publications are available to corporations, professional
associations, and other qualiﬁed organizations. For details and speciﬁc discount information, contact the special sales depart-
ment at Jones & Bartlett Learning via the above contact information or send an email to specialsales@jblearning.com.
Copyright c
⃝2017 by Jones & Bartlett Learning, LLC, an Ascend Learning Company
All rights reserved. No part of the material protected by this copyright may be reproduced or utilized in any form, electronic or
mechanical, including photocopying, recording, or by any information storage and retrieval system, without written permission
from the copyright owner.
The content, statements, views, and opinions herein are the sole expression of the respective authors and not that of Jones &
Bartlett Learning, LLC. Reference herein to any speciﬁc commercial product, process, or service by trade name, trademark,
manufacturer, or otherwise does not constitute or imply its endorsement or recommendation by Jones & Bartlett Learning, LLC
and such reference shall not be used for advertising or product endorsement purposes. All trademarks displayed are the trademarks
of the parties noted herein. An Introduction to Formal Languages and Automata, Sixth Edition is an independent publication and
has not been authorized, sponsored, or otherwise approved by the owners of the trademarks or service marks referenced in this
product.
There may be images in this book that feature models; these models do not necessarily endorse, represent, or participate in
the activities represented in the images. Any screenshots in this product are for educational and instructive purposes only. Any
individuals and scenarios featured in the case studies throughout this product may be real or ﬁctitious, but are used for instructional
purposes only.
Production Credits
VP, Executive Publisher: David D. Cella
Publisher: Cathy L. Esperti
Acquisitions Editor: Laura Pagluica
Editorial Assistant: Taylor Ferracane
Production Editor: Sara Kelly
Director of Marketing: Andrea DeFronzo
VP, Manufacturing and Inventory Control: Therese Connell
Composition: S4Carlisle Publishing Pvt. Ltd.
Cover Design: Kristin E. Parker
Rights and Media Specialist: Jamey O'Quinn
Media Development Editor: Shannon Sheehan
Cover Image:
c
⃝saicle/shutterstock
Printing and Binding: Edwards Brothers Malloy
Cover Printing: Edwards Brothers Malloy
Library of Congress Cataloging-in-Publication Data
Linz, Peter.
An introduction to formal languages and automata / Peter Linz, PhD,
University of California, Davis, Davis, California
-Sixth edition.
pages ; cm
Includes bibliographical references and index.
ISBN 978-1-284-07724-7(casebound)
1. Formal languages. 2. Machine theory. I. Title.
QA267.3.L56 2016
005.13'1-dc23
2015023479
6048
Printed in the United States of America
20 19 18 17 16
10 9 8 7 6 5 4 3 2 1

To the Memory of my Parents


CONTENTS
PREFACE
xi
1
INTRODUCTION TO THE THEORY
OF COMPUTATION
1
1.1 Mathematical Preliminaries and Notation . . . . . . . . . . .
3
Sets
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
Functions and Relations . . . . . . . . . . . . . . . . . . . . .
6
Graphs and Trees . . . . . . . . . . . . . . . . . . . . . . . . .
8
Proof Techniques . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.2 Three Basic Concepts
. . . . . . . . . . . . . . . . . . . . . .
17
Languages . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
Automata . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
1.3 Some Applications*
. . . . . . . . . . . . . . . . . . . . . . .
30
2
FINITE AUTOMATA
37
2.1 Deterministic Finite Accepters
. . . . . . . . . . . . . . . . .
38
Languages and Dfa's . . . . . . . . . . . . . . . . . . . . . . .
41
Regular Languages . . . . . . . . . . . . . . . . . . . . . . . .
46
2.2 Nondeterministic Finite Accepters
. . . . . . . . . . . . . . .
51
Deﬁnition of a Nondeterministic Accepter . . . . . . . . . . .
51
Why Nondeterminism? . . . . . . . . . . . . . . . . . . . . . .
55
2.3 Equivalence of Deterministic and Nondeterministic
Finite Accepters
. . . . . . . . . . . . . . . . . . . . . . . . .
58
2.4 Reduction of the Number of States in Finite Automata* . . .
66
v

vi
Contents
3
REGULAR LANGUAGES AND
REGULAR GRAMMARS
73
3.1 Regular Expressions
. . . . . . . . . . . . . . . . . . . . . . .
74
Formal Definition of a Regular Expression . . . . . . . . . . .
74
Languages Associated with Regular Expressions . . . . . . . .
75
3.2 Connection Between Regular Expressions
and Regular Languages
. . . . . . . . . . . . . . . . . . . . .
80
Regular Expressions Denote Regular Languages . . . . . . . .
80
Regular Expressions for Regular Languages . . . . . . . . . .
82
Regular Expressions for Describing Simple Patterns
. . . . .
88
3.3 Regular Grammars . . . . . . . . . . . . . . . . . . . . . . . .
91
Right- and Left-Linear Grammars
. . . . . . . . . . . . . . .
92
Right-Linear Grammars Generate Regular Languages
. . . .
93
Right-Linear Grammars for Regular Languages . . . . . . . .
96
Equivalence of Regular Languages and Regular Grammars . .
97
4
PROPERTIES OF REGULAR LANGUAGES
101
4.1 Closure Properties of Regular Languages . . . . . . . . . . . . 102
Closure under Simple Set Operations . . . . . . . . . . . . . . 102
Closure under Other Operations
. . . . . . . . . . . . . . . . 105
4.2 Elementary Questions about Regular Languages
. . . . . . . 114
4.3 Identifying Nonregular Languages . . . . . . . . . . . . . . . . 117
Using the Pigeonhole Principle
. . . . . . . . . . . . . . . . . 117
A Pumping Lemma . . . . . . . . . . . . . . . . . . . . . . . . 118
5
CONTEXT-FREE LANGUAGES
129
5.1 Context-Free Grammars . . . . . . . . . . . . . . . . . . . . . 130
Examples of Context-Free Languages . . . . . . . . . . . . . . 131
Leftmost and Rightmost Derivations . . . . . . . . . . . . . . 133
Derivation Trees
. . . . . . . . . . . . . . . . . . . . . . . . . 134
Relation Between Sentential Forms and Derivation Trees . . . 137
5.2 Parsing and Ambiguity . . . . . . . . . . . . . . . . . . . . . . 140
Parsing and Membership . . . . . . . . . . . . . . . . . . . . . 141
Ambiguity in Grammars and Languages . . . . . . . . . . . . 145
5.3 Context-Free Grammars and Programming Languages . . . . 151
6
SIMPLIFICATION OF CONTEXT-FREE
GRAMMARS AND NORMAL FORMS
155
6.1 Methods for Transforming Grammars
. . . . . . . . . . . . . 156
A Useful Substitution Rule
. . . . . . . . . . . . . . . . . . . 157
Removing Useless Productions
. . . . . . . . . . . . . . . . . 159
Removing λ-Productions . . . . . . . . . . . . . . . . . . . . . 163
Removing Unit-Productions . . . . . . . . . . . . . . . . . . . 165
6.2 Two Important Normal Forms
. . . . . . . . . . . . . . . . . 171
Chomsky Normal Form
. . . . . . . . . . . . . . . . . . . . . 171
Greibach Normal Form . . . . . . . . . . . . . . . . . . . . . . 174
6.3 A Membership Algorithm for Context-Free Grammars*
. . . 178

Contents
vii
7
PUSHDOWN AUTOMATA
181
7.1 Nondeterministic Pushdown Automata . . . . . . . . . . . . . 182
Deﬁnition of a Pushdown Automaton
. . . . . . . . . . . . . 183
The Language Accepted by a Pushdown Automaton . . . . . 186
7.2 Pushdown Automata and Context-Free Languages . . . . . . 191
Pushdown Automata for Context-Free Languages . . . . . . . 191
Context-Free Grammars for Pushdown Automata . . . . . . . 196
7.3 Deterministic Pushdown Automata and Deterministic
Context-Free Languages . . . . . . . . . . . . . . . . . . . . . 203
7.4 Grammars for Deterministic Context-Free Languages* . . . . 207
8
PROPERTIES OF CONTEXT-FREE LANGUAGES
213
8.1 Two Pumping Lemmas . . . . . . . . . . . . . . . . . . . . . . 214
A Pumping Lemma for Context-Free Languages . . . . . . . . 214
A Pumping Lemma for Linear Languages
. . . . . . . . . . . 219
8.2 Closure Properties and Decision Algorithms for
Context-Free Languages . . . . . . . . . . . . . . . . . . . . . 223
Closure of Context-Free Languages . . . . . . . . . . . . . . . 223
Some Decidable Properties of Context-Free Languages . . . . 227
9
TURING MACHINES
231
9.1 The Standard Turing Machine
. . . . . . . . . . . . . . . . . 232
Deﬁnition of a Turing Machine . . . . . . . . . . . . . . . . . 232
Turing Machines as Language Accepters . . . . . . . . . . . . 239
Turing Machines as Transducers
. . . . . . . . . . . . . . . . 242
9.2 Combining Turing Machines for Complicated Tasks . . . . . . 249
9.3 Turing's Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . 255
10 OTHER MODELS OF TURING MACHINES
259
10.1 Minor Variations on the Turing Machine Theme
. . . . . . . 260
Equivalence of Classes of Automata
. . . . . . . . . . . . . . 260
Turing Machines with a Stay-Option . . . . . . . . . . . . . . 261
Turing Machines with Semi-Infinite Tape
. . . . . . . . . . . 263
The Oﬀ-Line Turing Machine . . . . . . . . . . . . . . . . . . 265
10.2 Turing Machines with More Complex Storage . . . . . . . . . 268
Multitape Turing Machines . . . . . . . . . . . . . . . . . . . 268
Multidimensional Turing Machines . . . . . . . . . . . . . . . 271
10.3 Nondeterministic Turing Machines . . . . . . . . . . . . . . . 273
10.4 A Universal Turing Machine . . . . . . . . . . . . . . . . . . . 276
10.5 Linear Bounded Automata . . . . . . . . . . . . . . . . . . . . 281
11 A HIERARCHY OF FORMAL LANGUAGES
AND AUTOMATA
285
11.1 Recursive and Recursively Enumerable Languages
. . . . . . 286
Languages That Are Not Recursively Enumerable . . . . . . . 288

viii
Contents
A Language That Is Not Recursively Enumerable . . . . . . . 289
A Language That Is Recursively Enumerable
but Not Recursive
. . . . . . . . . . . . . . . . . . . . . . . . 291
11.2 Unrestricted Grammars
. . . . . . . . . . . . . . . . . . . . . 293
11.3 Context-Sensitive Grammars and Languages . . . . . . . . . . 299
Context-Sensitive Languages and
Linear Bounded Automata
. . . . . . . . . . . . . . . . . . . 300
Relation Between Recursive and Context-Sensitive
Languages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
11.4 The Chomsky Hierarchy . . . . . . . . . . . . . . . . . . . . . 305
12 LIMITS OF ALGORITHMIC COMPUTATION
309
12.1 Some Problems That Cannot Be Solved
by Turing Machines
. . . . . . . . . . . . . . . . . . . . . . . 310
Computability and Decidability . . . . . . . . . . . . . . . . . 310
The Turing Machine Halting Problem
. . . . . . . . . . . . . 311
Reducing One Undecidable Problem to Another . . . . . . . . 315
12.2 Undecidable Problems for Recursively
Enumerable Languages . . . . . . . . . . . . . . . . . . . . . . 319
12.3 The Post Correspondence Problem . . . . . . . . . . . . . . . 323
12.4 Undecidable Problems for Context-Free Languages . . . . . . 329
12.5 A Question of Eﬃciency . . . . . . . . . . . . . . . . . . . . . 332
13 OTHER MODELS OF COMPUTATION
335
13.1 Recursive Functions
. . . . . . . . . . . . . . . . . . . . . . . 337
Primitive Recursive Functions . . . . . . . . . . . . . . . . . . 338
Ackermann's Function . . . . . . . . . . . . . . . . . . . . . . 342
μ Recursive Functions . . . . . . . . . . . . . . . . . . . . . . 343
13.2 Post Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . 346
13.3 Rewriting Systems . . . . . . . . . . . . . . . . . . . . . . . . 350
Matrix Grammars
. . . . . . . . . . . . . . . . . . . . . . . . 350
Markov Algorithms . . . . . . . . . . . . . . . . . . . . . . . . 351
L-Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
14 AN OVERVIEW OF COMPUTATIONAL
COMPLEXITY
355
14.1 Eﬃciency of Computation
. . . . . . . . . . . . . . . . . . . 356
14.2 Turing Machine Models and Complexity
. . . . . . . . . . . 358
14.3 Language Families and Complexity Classes . . . . . . . . . . 362
14.4 The Complexity Classes P and NP . . . . . . . . . . . . . . . 365
14.5 Some NP Problems
. . . . . . . . . . . . . . . . . . . . . . . 367
14.6 Polynomial-Time Reduction
. . . . . . . . . . . . . . . . . . 370
14.7 NP-Completeness and an Open Question . . . . . . . . . . . 373

Contents
ix
APPENDIX A
FINITE-STATE TRANSDUCERS
377
A.1 A General Framework . . . . . . . . . . . . . . . . . . . . . . 377
A.2 Mealy Machines . . . . . . . . . . . . . . . . . . . . . . . . . 378
A.3 Moore Machines . . . . . . . . . . . . . . . . . . . . . . . . . 380
A.4 Moore and Mealy Machine Equivalence . . . . . . . . . . . . 382
A.5 Mealy Machine Minimization . . . . . . . . . . . . . . . . . . 386
A.6 Moore Machine Minimization . . . . . . . . . . . . . . . . . . 391
A.7 Limitations of Finite-State Transducers . . . . . . . . . . . . 392
APPENDIX B
JFLAP: A USEFUL TOOL
395
ANSWERS
SOLUTIONS AND HINTS
FOR SELECTED EXERCISES
397
REFERENCES FOR FURTHER READING
439
INDEX
441


PREFACE
T
his book is designed for an introductory course on formal languages,
automata, computability, and related matters. These topics form a
major part of what is known as the theory of computation. A course
on this subject matter is now standard in the computer science curriculum
and is often taught fairly early in the program. Hence, the prospective au-
dience for this book consists primarily of sophomores and juniors majoring
in computer science or computer engineering.
Prerequisites for the material in this book are a knowledge of some
higher-level programming language (commonly C, C++, or JavaTM) and
familiarity with the fundamentals of data structures and algorithms.
A
course in discrete mathematics that includes set theory, functions, relations,
logic, and elements of mathematical reasoning is essential. Such a course is
part of the standard introductory computer science curriculum.
The study of the theory of computation has several purposes, most im-
portantly (1) to familiarize students with the foundations and principles of
computer science, (2) to teach material that is useful in subsequent courses,
and (3) to strengthen students' ability to carry out formal and rigorous
mathematical arguments. The presentation I have chosen for this text fa-
vors the ﬁrst two purposes, although I would argue that it also serves the
third. To present ideas clearly and to give students insight into the material,
the text stresses intuitive motivation and illustration of ideas through ex-
amples. When there is a choice, I prefer arguments that are easily grasped
to those that are concise and elegant but diﬃcult in concept. I state deﬁni-
tions and theorems precisely and give the motivation for proofs, but often
xi

xii
Preface
leave out the routine and tedious details. I believe that this is desirable for
pedagogical reasons. Many proofs are unexciting applications of induction
or contradiction with diﬀerences that are speciﬁc to particular problems.
Presenting such arguments in full detail is not only unnecessary, but inter-
feres with the ﬂow of the story. Therefore, quite a few of the proofs are
brief and someone who insists on completeness may consider them lacking
in detail.
I do not see this as a drawback.
Mathematical skills are not
the byproduct of reading someone else's arguments, but come from think-
ing about the essence of a problem, discovering ideas suitable to make the
point, then carrying them out in precise detail. The latter skill certainly
has to be learned, and I think that the proof sketches in this text provide
very appropriate starting points for such a practice.
Computer science students sometimes view a course in the theory of
computation as unnecessarily abstract and of no practical consequence. To
convince them otherwise, one needs to appeal to their speciﬁc interests
and strengths, such as tenacity and inventiveness in dealing with hard-to-
solve problems. Because of this, my approach emphasizes learning through
problem solving.
By a problem-solving approach, I mean that students learn the ma-
terial primarily through problem-type illustrative examples that show the
motivation behind the concepts, as well as their connection to the theorems
and deﬁnitions. At the same time, the examples may involve a nontrivial
aspect, for which students must discover a solution. In such an approach,
homework exercises contribute to a major part of the learning process. The
exercises at the end of each section are designed to illuminate and illustrate
the material and call on students' problem-solving ability at various levels.
Some of the exercises are fairly simple, picking up where the discussion in
the text leaves oﬀand asking students to carry on for another step or two.
Other exercises are very diﬃcult, challenging even the best minds. A good
mix of such exercises can be a very eﬀective teaching tool. Students need
not be asked to solve all problems, but should be assigned those that sup-
port the goals of the course and the viewpoint of the instructor. Computer
science curricula diﬀer from institution to institution; while a few empha-
size the theoretical side, others are almost entirely oriented toward practical
application. I believe that this text can serve either of these extremes, pro-
vided that the exercises are selected carefully with the students' background
and interests in mind. At the same time, the instructor needs to inform the
students about the level of abstraction that is expected of them. This is
particularly true of the proof-oriented exercises. When I say "prove that"
or "show that," I have in mind that the student should think about how
a proof can be constructed and then produce a clear argument. How for-
mal such a proof should be needs to be determined by the instructor, and
students should be given guidelines on this early in the course.
The content of the text is appropriate for a one-semester course. Most
of the material can be covered, although some choice of emphasis will have

Preface
xiii
to be made. In my classes, I generally gloss over proofs, giving just enough
coverage to make the result plausible, and then ask students to read the
rest on their own. Overall, though, little can be skipped entirely without
potential diﬃculties later on. A few sections, which are marked with an
asterisk, can be omitted without loss to later material. Most of the material,
however, is essential and must be covered.
Appendix B brieﬂy introduces JFLAP, an interactive software tool that
is of great help in both learning and teaching the material in this book. It
aids in understanding the concepts and is a great time saver in the actual
construction of the solutions to the exercises. I highly recommend incorpo-
rating JFLAP into the course. Instructor resources for JFLAP are available
on the text's website: go.jblearning.com/LinzJPAK.
In the sixth edition I have added a number of features designed to bring
some of the more diﬃcult material into sharper focus. Each chapter is pre-
ceded by a summary that not only introduces the major concepts, but also
connects these concepts to what has gone on before and what will be cov-
ered later. The summaries form a synopsis of the story developed in the
book. Where diﬃcult abstractions are necessary, I have added concrete dis-
cussions that illustrate the reasons for the speciﬁc forms of the abstractions.
Finally, the instructor's manual has been revised and extended so that it
now contains detailed solutions to all exercises in the book.
Peter Linz


1
CHA P T E R
INTRODUCTION TO THE
THEORY OF COMPUTATION
CHAPTER SUMMARY
This chapter prepares you for what is to come. In Section 1.1, we review
some of the main ideas from finite mathematics and establish the no-
tation used in the text. Particularly important are the proof techniques,
especially proof by induction and proof by contradiction.
Section 1.2 gives you a first look at the ideas that are at the core of
the book: automata, languages, grammars, their definitions, and their
relations. In subsequent chapters, we will expand these ideas and study
a number of different types of automata and grammars.
In Section 1.3, we encounter some simple examples of the role these
concepts play in computer science, particularly in programming lan-
guages, digital design, and text processing.
We do not belabor this
issue here, as you will encounter applications of these concepts in a
number of other CS courses.
T
he subject matter of this book, the theory of computation, includes
several topics: automata theory, formal languages and grammars,
computability, and complexity. Together, this material constitutes
the theoretical foundation of computer science. Loosely speaking we can
think of automata, grammars, and computability as the study of what can
be done by computers in principle, while complexity addresses what can
be done in practice. In this book we focus almost entirely on the ﬁrst of
1

2
Chapter 1 Introduction to the Theory of Computation
these concerns. We will study various automata, see how they are related
to languages and grammars, and investigate what can and cannot be done
by digital computers. Although this theory has many uses, it is inherently
abstract and mathematical.
Computer science is a practical discipline. Those who work in it often
have a marked preference for useful and tangible problems over theoretical
speculation. This is certainly true of computer science students who are
concerned mainly with diﬃcult applications from the real world. Theoreti-
cal questions interest them only if they help in ﬁnding good solutions. This
attitude is appropriate, since without applications there would be little in-
terest in computers. But given this practical orientation, one might well
ask "why study theory?"
The ﬁrst answer is that theory provides concepts and principles that
help us understand the general nature of the discipline. The ﬁeld of com-
puter science includes a wide range of special topics, from machine design
to programming. The use of computers in the real world involves a wealth
of speciﬁc detail that must be learned for a successful application. This
makes computer science a very diverse and broad discipline. But in spite
of this diversity, there are some common underlying principles. To study
these basic principles, we construct abstract models of computers and com-
putation. These models embody the important features that are common
to both hardware and software and that are essential to many of the special
and complex constructs we encounter while working with computers. Even
when such models are too simple to be applicable immediately to real-world
situations, the insights we gain from studying them provide the foundation
on which speciﬁc development is based. This approach is, of course, not
unique to computer science. The construction of models is one of the es-
sentials of any scientiﬁc discipline, and the usefulness of a discipline is often
dependent on the existence of simple, yet powerful, theories and laws.
A second, and perhaps not so obvious, answer is that the ideas we will
discuss have some immediate and important applications.
The ﬁelds of
digital design, programming languages, and compilers are the most obvious
examples, but there are many others.
The concepts we study here run
like a thread through much of computer science, from operating systems to
pattern recognition.
The third answer is one of which we hope to convince the reader. The
subject matter is intellectually stimulating and fun. It provides many chal-
lenging, puzzle-like problems that can lead to some sleepless nights. This is
problem solving in its pure essence.
In this book, we will look at models that represent features at the
core of all computers and their applications. To model the hardware of a
computer, we introduce the notion of an automaton (plural, automata).
An automaton is a construct that possesses all the indispensable features
of a digital computer. It accepts input, produces output, may have some
temporary storage, and can make decisions in transforming the input into

1.1 Mathematical Preliminaries and Notation
3
the output. A formal language is an abstraction of the general charac-
teristics of programming languages.
A formal language consists of a set
of symbols and some rules of formation by which these symbols can be
combined into entities called sentences. A formal language is the set of all
sentences permitted by the rules of formation. Although some of the for-
mal languages we study here are simpler than programming languages, they
have many of the same essential features. We can learn a great deal about
programming languages from formal languages. Finally, we will formalize
the concept of a mechanical computation by giving a precise deﬁnition of
the term algorithm and study the kinds of problems that are (and are
not) suitable for solution by such mechanical means. In the course of our
study, we will show the close connection between these abstractions and
investigate the conclusions we can derive from them.
In the ﬁrst chapter, we look at these basic ideas in a very broad way to
set the stage for later work. In Section 1.1, we review the main ideas from
mathematics that will be required. While intuition will frequently be our
guide in exploring ideas, the conclusions we draw will be based on rigorous
arguments. This will involve some mathematical machinery, although the
requirements are not extensive. The reader will need a reasonably good
grasp of the terminology and of the elementary results of set theory, func-
tions, and relations. Trees and graph structures will be used frequently,
although little is needed beyond the deﬁnition of a labeled, directed graph.
Perhaps the most stringent requirement is the ability to follow proofs and
an understanding of what constitutes proper mathematical reasoning. This
includes familiarity with the basic proof techniques of deduction, induc-
tion, and proof by contradiction. We will assume that the reader has this
necessary background. Section 1.1 is included to review some of the main
results that will be used and to establish a notational common ground for
subsequent discussion.
In Section 1.2, we take a ﬁrst look at the central concepts of languages,
grammars, and automata.
These concepts occur in many speciﬁc forms
throughout the book. In Section 1.3, we give some simple applications of
these general ideas to illustrate that these concepts have widespread uses
in computer science. The discussion in these two sections will be intuitive
rather than rigorous. Later, we will make all of this much more precise; but
for the moment, the goal is to get a clear picture of the concepts with which
we are dealing.
1.1
MATHEMATICAL PRELIMINARIES AND NOTATION
Sets
A set is a collection of elements, without any structure other than mem-
bership. To indicate that x is an element of the set S, we write x ∈S.

4
Chapter 1 Introduction to the Theory of Computation
The statement that x is not in S is written x /∈S. A set can be speciﬁed
by enclosing some description of its elements in curly braces; for example,
the set of integers 0, 1, 2 is shown as
S = {0, 1, 2} .
Ellipses are used whenever the meaning is clear. Thus, {a, b, ... , z} stands for
all the lowercase letters of the English alphabet, while {2, 4, 6, ...} denotes
the set of all positive even integers. When the need arises, we use more
explicit notation, in which we write
S = {i : i > 0, i is even}
(1.1)
for the last example. We read this as "S is the set of all i, such that i is
greater than zero, and i is even," implying, of course, that i is an integer.
The usual set operations are union (∪), intersection (∩), and diﬀer-
ence (−) deﬁned as
S1 ∪S2 = {x : x ∈S1 or x ∈S2} ,
S1 ∩S2 = {x : x ∈S1 and x ∈S2} ,
S1 −S2 = {x : x ∈S1 and x /∈S2} .
Another basic operation is complementation.
The complement of
a set S, denoted by S, consists of all elements not in S.
To make this
meaningful, we need to know what the universal set U of all possible
elements is. If U is speciﬁed, then
S = {x : x ∈U, x /∈S} .
The set with no elements, called the empty set or the null set, is
denoted by ∅. From the deﬁnition of a set, it is obvious that
S ∪∅= S −∅= S,
S ∩∅= ∅,
∅= U,
S = S.
The following useful identities, known as DeMorgan's laws,
S1 ∪S2 = S1 ∩S2,
(1.2)
S1 ∩S2 = S1 ∪S2,
(1.3)
are needed on several occasions.
A set S1 is said to be a subset of S if every element of S1 is also an
element of S. We write this as
S1 ⊆S.

1.1 Mathematical Preliminaries and Notation
5
If S1 ⊆S, but S contains an element not in S1, we say that S1 is a proper
subset of S; we write this as
S1 ⊂S.
If S1 and S2 have no common element, that is, S1 ∩S2 = ∅, then the sets
are said to be disjoint.
A set is said to be ﬁnite if it contains a ﬁnite number of elements;
otherwise it is inﬁnite. The size of a ﬁnite set is the number of elements
in it; this is denoted by |S|.
A given set normally has many subsets. The set of all subsets of a set
S is called the powerset of S and is denoted by 2S. Observe that 2S is a
set of sets.
EXAMPLE 1.1
If S is the set {a, b, c}, then its powerset is
2S = {∅, {a} , {b} , {c} , {a, b} , {a, c} , {b, c} , {a, b, c}} .
Here |S| = 3 and
2S = 8. This is an instance of a general result; if S
is ﬁnite, then
2S = 2|S|.
In many of our examples, the elements of a set are ordered sequences of
elements from other sets. Such sets are said to be the Cartesian product
of other sets. For the Cartesian product of two sets, which itself is a set of
ordered pairs, we write
S = S1 × S2 = {(x, y) : x ∈S1, y ∈S2} .
EXAMPLE 1.2
Let S1 = {2, 4} and S2 = {2, 3, 5, 6}. Then
S1 × S2 = {(2, 2) , (2, 3) , (2, 5) , (2, 6) , (4, 2) , (4, 3) , (4, 5) , (4, 6)} .
Note that the order in which the elements of a pair are written matters.
The pair (4, 2) is in S1 × S2, but (2, 4) is not.
The notation is extended in an obvious fashion to the Cartesian
product of more than two sets; generally
S1 × S2 × · · · × Sn = {(x1, x2, ..., xn) : xi ∈Si} .

6
Chapter 1 Introduction to the Theory of Computation
A set can be divided by separating it into a number of subsets. Suppose
that S1, S2, ..., Sn are subsets of a given set S and that the following holds:
1. The subsets S1, S2, ..., Sn are mutually disjoint;
2. S1 ∪S2 ∪... ∪Sn = S;
3. none of the Si is empty.
Then S1, S2, ..., Sn is called a partition of S.
Functions and Relations
A function is a rule that assigns to elements of one set a unique element of
another set. If f denotes a function, then the ﬁrst set is called the domain
of f, and the second set is its range. We write
f : S1 →S2
to indicate that the domain of f is a subset of S1 and that the range of f
is a subset of S2. If the domain of f is all of S1, we say that f is a total
function on S1; otherwise f is said to be a partial function.
In many applications, the domain and range of the functions involved
are in the set of positive integers. Furthermore, we are often interested only
in the behavior of these functions as their arguments become very large. In
such cases an understanding of the growth rates may suﬃce and a common
order of magnitude notation can be used. Let f (n) and g (n) be functions
whose domain is a subset of the positive integers. If there exists a positive
constant c such that for all suﬃciently large n
f (n) ≤c|g (n) |,
we say that f has order at most g. We write this as
f (n) = O (g (n)) .
If
|f (n)| ≥c |g (n)| ,
then f has order at least g, for which we use
f (n) = Ω (g (n)) .
Finally, if there exist constants c1 and c2 such that
c1 |g (n)| ≤|f (n)| ≤c2 |g (n)| ,
f and g have the same order of magnitude, expressed as
f (n) = Θ (g (n)) .
In this order-of-magnitude notation, we ignore multiplicative constants and
lower-order terms that become negligible as n increases.

1.1 Mathematical Preliminaries and Notation
7
EXAMPLE 1.3
Let
f (n) = 2n2 + 3n,
g (n) = n3,
h (n) = 10n2 + 100.
Then
f (n) = O (g (n)) ,
g (n) = Ω (h (n)) ,
f (n) = Θ (h (n)) .
In order-of-magnitude notation, the symbol = should not be in-
terpreted as equality and order-of-magnitude expressions cannot be
treated like ordinary expressions. Manipulations such as
O (n) + O (n) = 2O (n)
are not sensible and can lead to incorrect conclusions. Still, if used
properly, the order-of-magnitude arguments can be eﬀective, as we will
see in later chapters.
Some functions can be represented by a set of pairs
{(x1, y1) , (x2, y2) , ...} ,
where xi is an element in the domain of the function, and yi is the corre-
sponding value in its range. For such a set to deﬁne a function, each xi can
occur at most once as the ﬁrst element of a pair. If this is not satisﬁed, the
set is called a relation. Relations are more general than functions: In a
function each element of the domain has exactly one associated element in
the range; in a relation there may be several such elements in the range.
One kind of relation is that of equivalence, a generalization of the con-
cept of equality (identity). To indicate that a pair (x, y) is in an equivalence
relation, we write
x ≡y.
A relation denoted by ≡is considered an equivalence if it satisﬁes three
rules: the reﬂexivity rule
x ≡x for all x;
the symmetry rule
if x ≡y, then y ≡x;

8
Chapter 1 Introduction to the Theory of Computation
and the transitivity rule
if x ≡y and y ≡z, then x ≡z.
EXAMPLE 1.4
On the set of nonnegative integers, we can deﬁne a relation
x ≡y
if and only if
x mod 3 = y mod 3.
Then 2 ≡5, 12 ≡0, and 0 ≡36. Clearly this is an equivalence relation,
as it satisﬁes reﬂexivity, symmetry, and transitivity.
If S is a set on which we have a deﬁned equivalence relation, then we can
use this equivalence to partition the set into equivalence classes. Each
equivalence class contains all and only equivalent elements.
Graphs and Trees
A graph is a construct consisting of two ﬁnite sets, the set V = {v1, v2, ..., vn}
of vertices and the set E = {e1, e2, ..., em} of edges. Each edge is a pair
of vertices from V , for instance,
ei = (vj, vk)
is an edge from vj to vk. We say that the edge ei is an outgoing edge for
vj and an incoming edge for vk. Such a construct is actually a directed
graph (digraph), since we associate a direction (from vj to vk) with each
edge. Graphs may be labeled, a label being a name or other information
associated with parts of the graph. Both vertices and edges may be labeled.
Graphs are conveniently visualized by diagrams in which the vertices
are represented as circles and the edges as lines with arrows connecting the
vertices. The graph with vertices {v1, v2, v3} and edges {(v1, v3) , (v3, v1) ,
(v3, v2) , (v3, v3)} is depicted in Figure 1.1.
v2
v3
v1
FIGURE 1.1

1.1 Mathematical Preliminaries and Notation
9
A sequence of edges (vi, vj) , (vj, vk) , ... , (vm, vn) is said to be a walk
from vi to vn. The length of a walk is the total number of edges traversed
in going from the initial vertex to the ﬁnal one. A walk in which no edge
is repeated is said to be a path; a path is simple if no vertex is repeated.
A walk from vi to itself with no repeated edges is called a cycle with base
vi. If no vertices other than the base are repeated in a cycle, then it is said
to be simple. In Figure 1.1, (v1, v3), (v3, v2) is a simple path from v1 to v2.
The sequence of edges (v1, v3), (v3, v3), (v3, v1) is a cycle, but not a simple
one. If the edges of a graph are labeled, we can talk about the label of a
walk. This label is the sequence of edge labels encountered when the path
is traversed. Finally, an edge from a vertex to itself is called a loop. In
Figure 1.1, there is a loop on vertex v3.
On several occasions, we will refer to an algorithm for ﬁnding all simple
paths between two given vertices (or all simple cycles based on a vertex).
If we do not concern ourselves with eﬃciency, we can use the following
obvious method. Starting from the given vertex, say vi, list all outgoing
edges (vi, vk), (vi, vl) , .... At this point, we have all paths of length one
starting at vi. For all vertices vk, vl, ... so reached, we list all outgoing edges
as long as they do not lead to any vertex already used in the path we are
constructing. After we do this, we will have all simple paths of length two
originating at vi. We continue this until all possibilities are accounted for.
Since there are only a ﬁnite number of vertices, we will eventually list all
simple paths beginning at vi.
From these we select those ending at the
desired vertex.
Trees are a particular type of graph. A tree is a directed graph that
has no cycles and that has one distinct vertex, called the root, such that
there is exactly one path from the root to every other vertex. This deﬁnition
implies that the root has no incoming edges and that there are some vertices
without outgoing edges. These are called the leaves of the tree. If there
is an edge from vi to vj, then vi is said to be the parent of vj, and vj the
child of vi. The level associated with each vertex is the number of edges in
the path from the root to the vertex. The height of the tree is the largest
level number of any vertex. These terms are illustrated in Figure 1.2.
At times, we want to associate an ordering with the nodes at each level;
in such cases we talk about ordered trees.
More details on graphs and trees can be found in most books on discrete
mathematics.
Proof Techniques
An important requirement for reading this text is the ability to follow proofs.
In mathematical arguments, we employ the accepted rules of deductive rea-
soning, and many proofs are simply a sequence of such steps. Two special
proof techniques are used so frequently that it is appropriate to review them
brieﬂy. These are proof by induction and proof by contradiction.

10
Chapter 1 Introduction to the Theory of Computation
Root
Level 0
Level 3
Height = 3
Leaf
FIGURE 1.2
Induction is a technique by which the truth of a number of statements
can be inferred from the truth of a few speciﬁc instances. Suppose we have a
sequence of statements P1, P2, ... we want to prove to be true. Furthermore,
suppose also that the following holds:
1. For some k ≥1, we know that P1, P2, ... , Pk are true.
2. The problem is such that for any n ≥k, the truths of P1, P2, ..., Pn
imply the truth of Pn+1.
We can then use induction to show that every statement in this sequence is
true.
In a proof by induction, we argue as follows: From Condition 1 we know
that the ﬁrst k statements are true. Then Condition 2 tells us that Pk+1
also must be true. But now that we know that the ﬁrst k+1 statements are
true, we can apply Condition 2 again to claim that Pk+2 must be true, and
so on. We need not explicitly continue this argument, because the pattern is
clear. The chain of reasoning can be extended to any statement. Therefore,
every statement is true.
The starting statements P1, P2, ... Pk are called the basis of the induc-
tion. The step connecting Pn with Pn+1 is called the inductive step. The
inductive step is generally made easier by the inductive assumption that
P1, P2, ... , Pn are true, then argue that the truth of these statements guar-
antees the truth of Pn+1. In a formal inductive argument, we show all three
parts explicitly.
EXAMPLE 1.5
A binary tree is a tree in which no parent can have more than two
children. Prove that a binary tree of height n has at most 2n leaves.

1.1 Mathematical Preliminaries and Notation
11
Proof: If we denote the maximum number of leaves of a binary tree of
height n by l (n), then we want to show that l (n) ≤2n.
Basis: Clearly l (0) = 1 = 20 since a tree of height 0 can have no nodes
other than the root, that is, it has at most one leaf.
Inductive Assumption:
l (i) ≤2i, for i = 0, 1, ..., n.
(1.4)
Inductive Step:
To get a binary tree of height n + 1 from one of
height n, we can create, at most, two leaves in place of each previous
one. Therefore,
l (n + 1) = 2l (n) .
Now, using the inductive assumption, we get
l (n + 1) ≤2 × 2n = 2n+1.
Thus, if our claim is true for n, it must also be true for n + 1. Since n
can be any number, the statement must be true for all n.
Here we introduce the symbol
that is used in this book to denote the
end of a proof.
Inductive reasoning can be diﬃcult to grasp.
It helps to notice the
close connection between induction and recursion in programming.
For
example, the recursive deﬁnition of a function f (n), where n is any positive
integer, often has two parts.
One involves the deﬁnition of f (n + 1) in
terms of f (n) , f (n −1) , ..., f (1). This corresponds to the inductive step.
The second part is the "escape" from the recursion, which is accomplished
by deﬁning f (1) , f (2) , ..., f (k) nonrecursively.
This corresponds to the
basis of induction. As in induction, recursion allows us to draw conclusions
about all instances of the problem, given only a few starting values and
using the recursive nature of the problem.
Sometimes, a problem looks diﬃcult until we look at it in just the right
way. Often looking at it recursively simpliﬁes matters greatly.
EXAMPLE 1.6
A set l1, l2, ..., ln of mutually intersecting straight lines divides the
plane into a number of separated regions. A single line divides the
plane into two parts, two lines generate four regions, three lines make
seven regions, and so on. This is easily checked visually for up to three

12
Chapter 1 Introduction to the Theory of Computation
lines, but as the number of lines increases it becomes diﬃcult to spot
a pattern. Let us try to solve this problem recursively.
Look at Figure 1.3 to see what happens if we add a new line ln+1
to existing n lines.
The region to the left of l1 is divided into two
new regions, so is the region to the left of l2, and so on until we get
to the last line. At the last line, the region to the right of ln is also
divided. Each of the n intersections then generates one new region,
with one extra at the end. So, if we let A(n) denote the number of
regions generated by n lines, we see that
A(n + 1) = A(n) + n + 1, n = 1, 2, ...,
with A(1) = 2. From this simple recursion we then calculate A(2) = 4,
A(3) = 7, A(4) = 11, and so on.
To get a formula for A(n) and to show that it is correct, we use
induction. If we conjecture that
A(n) = n(n + 1)
2
+ 1,
then
A(n + 1) = n(n + 1)
2
+ 1 + n + 1
= (n + 1)(n + 2)
2
+ 1
l1
l2
l3
ln
ln + 1
FIGURE 1.3

1.1 Mathematical Preliminaries and Notation
13
justiﬁes the inductive step. The basis is easily checked, completing the
argument.
In this example we have been a little less formal in identifying the
basis, inductive assumption, and inductive step, but they are there
and are essential. To keep our subsequent discussions from becoming
too formal, we will generally prefer the style of this second example.
However, if you have diﬃculty in following or constructing a proof, go
back to the more explicit form of Example 1.5.
Proof by contradiction is another powerful technique that often works
when everything else fails. Suppose we want to prove that some statement
P is true. We then assume, for the moment, that P is false and see where
that assumption leads us.
If we arrive at a conclusion that we know is
incorrect, we can lay the blame on the starting assumption and conclude
that P must be true. The following is a classic and elegant example.
EXAMPLE 1.7
A rational number is a number that can be expressed as the ratio of
two integers n and m so that n and m have no common factor. A real
number that is not rational is said to be irrational. Show that
√
2 is
irrational.
As in all proofs by contradiction, we assume the contrary of what
we want to show. Here we assume that
√
2 is a rational number so that
it can be written as
√
2 = n
m,
(1.5)
where n and m are integers without a common factor. Rearranging
(1.5), we have
2m2 = n2.
Therefore, n2 must be even. This implies that n is even, so that we
can write n = 2k or
2m2 = 4k2,
and
m2 = 2k2.
Therefore, m is even. But this contradicts our assumption that n and
m have no common factors. Thus, m and n in (1.5) cannot exist and
√
2 is not a rational number.

14
Chapter 1 Introduction to the Theory of Computation
This example exhibits the essence of a proof by contradiction. By mak-
ing a certain assumption we are led to a contradiction of the assumption or
some known fact. If all steps in our argument are logically sound, we must
conclude that our initial assumption was false.
EXERCISES
1. With S1 = {2, 3, 5, 7}, S2 = {2, 4, 5, 8, 9}, and U = {1 : 10}, compute S1
 S2.
2. With S1 = {2, 3, 5, 7} and S2 = {2, 4, 5, 8, 9}, compute S1 × S2 and S2 × S1.
3. For S = {2, 5, 6, 8} and T = {2, 4, 6, 8}, compute |S ∩T| + |S ∪T|.
4. What relation between two sets S and T must hold so that |S∪T| = |S|+|T|?
5. Show that for all sets S and T, S −T = S ∩T.
6. Prove DeMorgan's laws, Equations (1.2) and (1.3), by showing that if an
element x is in the set on one side of the equality, then it must also be in the
set on the other side of the equality.
7. Show that if S1 ⊆S2, then S2 ⊆S1.
8. Show that S1 = S2 if and only if S1 ∪S2 = S1 ∩S2.
9. Use induction on the size of S to show that if S is a ﬁnite set, then
2S = 2|S|.
10. Show that if S1 and S2 are ﬁnite sets with |S1| = n and |S2| = m, then
|S1 ∪S2| ≤n + m.
11. If S1 and S2 are ﬁnite sets, show that |S1 × S2| = |S1| |S2|.
12. Consider the relation between two sets deﬁned by S1 ≡S2 if and only if
|S1| = |S2|. Show that this is an equivalence relation.
13. Occasionally, we need to use the union and intersection symbols in a manner
analogous to the summation sign Σ. We deﬁne

p∈{i,j,k,... }
Sp = Si ∪Sj ∪Sk · · ·
with an analogous notation for the intersection of several sets.
With this notation, the general DeMorgan's laws are written as

p∈P
Sp =

p∈P
Sp
and

p∈P
Sp =

p∈P
Sp.
Prove these identities when P is a ﬁnite set.
14. Show that
S1 ∪S2 = S1 ∩S2.

1.1 Mathematical Preliminaries and Notation
15
15. Show that S1 = S2 if and only if

S1 ∩S2

∪

S1 ∩S2

= ⊘.
16. Show that
S1 ∪S2 −

S1 ∩S2

= S2.
17. Show that the distributive law
S1 ∩(S2 ∪S3) = (S1 ∩S2) ∪(S1 ∩S3)
holds for sets.
18. Show that
S1 × (S2 ∪S3) = (S1 × S2) ∪(S1 × S3) .
19. Give conditions on S1 and S2 necessary and suﬃcient to ensure that
S1 = (S1 ∪S2) −S2.
20. Use the equivalence deﬁned in Example 1.4 to partition the set {2, 4, 5,
6, 9, 22, 24, 25, 31, 37} into equivalence classes.
21. Show that if f (n) = O (g (n)) and g (n) = O (f (n)), then f (n) = Θ (g (n)).
22. Show that 2n = O (3n), but 2n ̸= Θ (3n).
23. Show that the following order-of-magnitude results hold.
(a) n2 + 5 log n = O

n2
.
(b) 3n = O (n!).
(c) n! = O (nn).
24. Show that (n3 −2n)/(n + 1) = Θ(n2).
25. Show that
n3
log (n + 1) = O(n3) but not O(n2).
26. What is wrong with the following argument? x = O(n4), y = O(n2), therefore
x/y = O(n2).
27. What is wrong with the following argument? x = Θ(n4), y = Θ(n2), therefore
x
y = Θ(n2).
28. Prove that if f (n) = O (g (n)) and g (n) = O (h (n)), then f (n) = O (h (n)).
29. Show that if f (n) = O

n2
and g (n) = O

n3
, then
f (n) + g (n) = O

n3
and
f (n) g (n) = O

n5
.
In this case, is it true that g (n) /f (n) = O (n)?
30. Assume that f (n) = 2n2 + n and g (n) = O

n2
. What is wrong with the
following argument?
f (n) = O

n2
+ O (n) ,
so that
f (n) −g (n) = O

n2
+ O (n) −O

n2
.
Therefore,
f (n) −g (n) = O (n) .

16
Chapter 1 Introduction to the Theory of Computation
31. Show that if f(n) = Θ(log2 n), then f(n) = Θ(log10 n).
32. Draw a picture of the graph with vertices {v1, v2, v3} and edges {(v1, v1) ,
(v1, v2) , (v2, v3) , (v2, v1) , (v3, v1)}. Enumerate all cycles with base v1.
33. Construct a graph with ﬁve vertices, ten edges, and no cycles.
34. Let G = (V, E) be any graph. Prove the following claim: If there is any walk
between vi ∈V and vj ∈V , then there must be a simple path of length no
larger than |V | −1 between these two vertices.
35. Consider graphs in which there is at most one edge between any two vertices.
Show that under this condition a graph with n vertices has at most n2 edges.
36. Show that
n

i=0
2i = 2n+1 −1.
37. Show that
n

i=1
1
i2 ≤2 −1
n.
38. Prove that for all n ≥4 the inequality 2n < n! holds.
39. The Fibonacci sequence is deﬁned recursively by
f(n + 2) = f(n + 1) + f(n), n = 1, 2, ...,
with f(1) = 1, f(2) = 1. Show that
(a) f(n) = O(2n),
(b) f(n) = Ω(1.5n).
40. Show that
√
8 is not a rational number.
41. Show that 2 −
√
2 is irrational.
42. Show that
√
3 is irrational.
43. Prove or disprove the following statements.
(a) The sum of a rational and an irrational number must be irrational.
(b) The sum of two positive irrational numbers must be irrational.
(c) The product of a nonzero rational and an irrational number must be
irrational.
44. Show that every positive integer can be expressed as the product of prime
numbers.
45. Prove that the set of all prime numbers is inﬁnite.
46. A prime pair consists of two primes that diﬀer by two. There are many prime
pairs, for example, 11 and 13, 17 and 19. Prime triplets are three numbers
n ≥2, n + 2, n + 4 that are all prime. Show that the only prime triplet is
(3, 5, 7).

1.2 Three Basic Concepts
17
1.2
THREE BASIC CONCEPTS
Three fundamental ideas are the major themes of this book: languages,
grammars, and automata. In the course of our study we will explore
many results about these concepts and about their relationship to each
other. First, we must understand the meaning of the terms.
Languages
We are all familiar with the notion of natural languages, such as English
and French. Still, most of us would probably ﬁnd it diﬃcult to say exactly
what the word "language" means. Dictionaries deﬁne the term informally
as a system suitable for the expression of certain ideas, facts, or concepts,
including a set of symbols and rules for their manipulation. While this gives
us an intuitive idea of what a language is, it is not suﬃcient as a deﬁnition
for the study of formal languages. We need a precise deﬁnition for the term.
We start with a ﬁnite, nonempty set Σ of symbols, called the alpha-
bet. From the individual symbols we construct strings, which are ﬁnite
sequences of symbols from the alphabet.
For example, if the alphabet
Σ = {a, b}, then abab and aaabbba are strings on Σ.
With few excep-
tions, we will use lowercase letters a, b, c, ... for elements of Σ and u, v, w, ...
for string names. We will write, for example,
w = abaaa
to indicate that the string named w has the speciﬁc value abaaa.
The concatenation of two strings w and v is the string obtained by
appending the symbols of v to the right end of w, that is, if
w = a1a2 · · · an
and
v = b1b2 · · · bm,
then the concatenation of w and v, denoted by wv, is
wv = a1a2 · · · anb1b2 · · · bm.
The reverse of a string is obtained by writing the symbols in reverse order;
if w is a string as shown above, then its reverse wR is
wR = an · · · a2a1.
The length of a string w, denoted by |w|, is the number of symbols in
the string. We will frequently need to refer to the empty string, which
is a string with no symbols at all. It will be denoted by λ. The following
simple relations
|λ| = 0,
λw = wλ = w
hold for all w.

18
Chapter 1 Introduction to the Theory of Computation
Any string of consecutive symbols in some w is said to be a substring
of w. If
w = vu,
then the substrings v and u are said to be a preﬁx and a suﬃx of w,
respectively. For example, if w = abbab, then {λ, a, ab, abb, abba, abbab} is
the set of all preﬁxes of w, while bab, ab, b are some of its suﬃxes.
Simple properties of strings, such as their length, are very intuitive and
probably need little elaboration. For example, if u and v are strings, then
the length of their concatenation is the sum of the individual lengths, that
is,
|uv| = |u| + |v| .
(1.6)
But although this relationship is obvious, it is useful to be able to make
it precise and prove it. The techniques for doing so are important in more
complicated situations.
EXAMPLE 1.8
Show that (1.6) holds for any u and v. To prove this, we ﬁrst need
a deﬁnition of the length of a string. We make such a deﬁnition in a
recursive fashion by
|a| = 1,
|wa| = |w| + 1,
for all a ∈Σ and w any string on Σ.
This deﬁnition is a formal
statement of our intuitive understanding of the length of a string: The
length of a single symbol is one, and the length of any string is increased
by one if we add another symbol to it. With this formal deﬁnition, we
are ready to prove (1.6) by induction characters.
By deﬁnition, (1.6) holds for all u of any length and all v of length
1, so we have a basis. As an inductive assumption, we take that (1.6)
holds for all u of any length and all v of length 1, 2, ..., n. Now take
any v of length n + 1 and write it as v = wa. Then,
|v| = |w| + 1,
|uv| = |uwa| = |uw| + 1.
By the inductive hypothesis (which is applicable since w is of length n),
|uw| = |u| + |w|
so that
|uv| = |u| + |w| + 1 = |u| + |v| .
Therefore, (1.6) holds for all u and all v of length up to n+1, completing
the inductive step and the argument.

1.2 Three Basic Concepts
19
If w is a string, then wn stands for the string obtained by repeating w
n times. As a special case, we deﬁne
w0 = λ,
for all w.
If Σ is an alphabet, then we use Σ∗to denote the set of strings obtained
by concatenating zero or more symbols from Σ. The set Σ∗always contains
λ. To exclude the empty string, we deﬁne
Σ+ = Σ∗−{λ} .
While Σ is ﬁnite by assumption, Σ∗and Σ+ are always inﬁnite since there
is no limit on the length of the strings in these sets. A language is deﬁned
very generally as a subset of Σ∗. A string in a language L will be called
a sentence of L. This deﬁnition is quite broad; any set of strings on an
alphabet Σ can be considered a language. Later we will study methods by
which speciﬁc languages can be deﬁned and described; this will enable us to
give some structure to this rather broad concept. For the moment, though,
we will just look at a few speciﬁc examples.
EXAMPLE 1.9
Let Σ = {a, b}. Then
Σ∗= {λ, a, b, aa, ab, ba, bb, aaa, aab, ...} .
The set
{a, aa, aab}
is a language on Σ. Because it has a ﬁnite number of sentences, we call
it a ﬁnite language. The set
L = {anbn : n ≥0}
is also a language on Σ.
The strings aabb and aaaabbbb are in the
language L, but the string abb is not in L. This language is inﬁnite.
Most interesting languages are inﬁnite.
Since languages are sets, the union, intersection, and diﬀerence of two
languages are immediately deﬁned. The complement of a language is deﬁned
with respect to Σ∗; that is, the complement of L is
L = Σ∗−L.
The reverse of a language is the set of all string reversals, that is,
LR =

wR : w ∈L

.

20
Chapter 1 Introduction to the Theory of Computation
The concatenation of two languages L1 and L2 is the set of all strings ob-
tained by concatenating any element of L1 with any element of L2; speciﬁ-
cally,
L1L2 = {xy : x ∈L1, y ∈L2} .
We deﬁne Ln as L concatenated with itself n times, with the special cases
L0 = {λ}
and
L1 = L
for every language L.
Finally, we deﬁne the star-closure of a language as
L∗= L0 ∪L1 ∪L2 · · ·
and the positive closure as
L+ = L1 ∪L2 · · · .
EXAMPLE 1.10
If
L = {anbn : n ≥0} ,
then
L2 = {anbnambm : n ≥0, m ≥0} .
Note that n and m in the above are unrelated; the string aabbaaabbb
is in L2.
The reverse of L is easily described in set notation as
LR = {bnan : n ≥0} ,
but it is considerably harder to describe L or L∗this way.
A few
tries will quickly convince you of the limitation of set notation for the
speciﬁcation of complicated languages.
Grammars
To study languages mathematically, we need a mechanism to describe them.
Everyday language is imprecise and ambiguous, so informal descriptions in
English are often inadequate. The set notation used in Examples 1.9 and
1.10 is more suitable, but limited. As we proceed we will learn about several
language-deﬁnition mechanisms that are useful in diﬀerent circumstances.
Here we introduce a common and powerful one, the notion of a grammar.

1.2 Three Basic Concepts
21
A grammar for the English language tells us whether a particular sen-
tence is well formed or not. A typical rule of English grammar is "a sentence
can consist of a noun phrase followed by a predicate." More concisely we
write this as
⟨sentence⟩→⟨noun phrase⟩⟨predicate⟩,
with the obvious interpretation. This is, of course, not enough to deal with
actual sentences. We must now provide deﬁnitions for the newly introduced
constructs ⟨noun phrase⟩and ⟨predicate⟩. If we do so by
⟨noun phrase⟩→⟨article⟩⟨noun⟩,
⟨predicate⟩→⟨verb⟩,
and if we associate the actual words "a" and "the" with ⟨article⟩, "boy" and
"dog" with ⟨noun⟩, and "runs" and "walks" with ⟨verb⟩, then the grammar
tells us that the sentences "a boy runs" and "the dog walks" are properly
formed. If we were to give a complete grammar, then in theory, every proper
sentence could be explained this way.
This example illustrates the deﬁnition of a general concept in terms
of simple ones. We start with the top-level concept, here ⟨sentence⟩, and
successively reduce it to the irreducible building blocks of the language. The
generalization of these ideas leads us to formal grammars.
DEFINITION 1.1
A grammar G is deﬁned as a quadruple
G = (V, T, S, P) ,
where V is a ﬁnite set of objects called variables,
T is a ﬁnite set of objects called terminal symbols,
S ∈V is a special symbol called the start variable,
P is a ﬁnite set of productions.
It will be assumed without further mention that the sets V and T are non-
empty and disjoint.
The production rules are the heart of a grammar; they specify how the
grammar transforms one string into another, and through this they deﬁne
a language associated with the grammar. In our discussion we will assume
that all production rules are of the form
x →y,

22
Chapter 1 Introduction to the Theory of Computation
where x is an element of (V ∪T)+ and y is in (V ∪T)∗. The productions
are applied in the following manner: Given a string w of the form
w = uxv,
we say the production x →y is applicable to this string, and we may use it
to replace x with y, thereby obtaining a new string
z = uyv.
This is written as
w ⇒z.
We say that w derives z or that z is derived from w. Successive strings are
derived by applying the productions of the grammar in arbitrary order. A
production can be used whenever it is applicable, and it can be applied as
often as desired. If
w1 ⇒w2 ⇒· · · ⇒wn,
we say that w1 derives wn and write
w1
∗⇒wn.
The ∗indicates that an unspeciﬁed number of steps (including zero) can be
taken to derive wn from w1.
By applying the production rules in a diﬀerent order, a given grammar
can normally generate many strings. The set of all such terminal strings is
the language deﬁned or generated by the grammar.
DEFINITION 1.2
Let G = (V, T, S, P) be a grammar. Then the set
L (G) =

w ∈T ∗: S
∗⇒w

is the language generated by G.
If w ∈L (G), then the sequence
S ⇒w1 ⇒w2 ⇒· · · ⇒wn ⇒w
is a derivation of the sentence w.
The strings S, w1, w2, ..., wn, which
contain variables as well as terminals, are called sentential forms of the
derivation.

1.2 Three Basic Concepts
23
EXAMPLE 1.11
Consider the grammar
G = ({S} , {a, b} , S, P) ,
with P given by
S →aSb,
S →λ.
Then
S ⇒aSb ⇒aaSbb ⇒aabb,
so we can write
S
∗⇒aabb.
The string aabb is a sentence in the language generated by G, while
aaSbb is a sentential form.
A grammar G completely deﬁnes L (G), but it may not be easy
to get a very explicit description of the language from the grammar.
Here, however, the answer is fairly clear. It is not hard to conjecture
that
L (G) = {anbn : n ≥0} ,
and it is easy to prove it.
If we notice that the rule S →aSb is
recursive, a proof by induction readily suggests itself. We ﬁrst show
that all sentential forms must have the form
wi = aiSbi.
(1.7)
Suppose that (1.7) holds for all sentential forms wi of length 2i + 1 or
less. To get another sentential form (which is not a sentence), we can
only apply the production S →aSb. This gets us
aiSbi ⇒ai+1Sbi+1,
so that every sentential form of length 2i + 3 is also of the form (1.7).
Since (1.7) is obviously true for i = 1, it holds by induction for all i.
Finally, to get a sentence, we must apply the production S →λ, and
we see that
S
∗⇒anSbn ⇒anbn
represents all possible derivations. Thus, G can derive only strings of
the form anbn.
We also have to show that all strings of this form can be derived.
This is easy; we simply apply S →aSb as many times as needed,
followed by S →λ.

24
Chapter 1 Introduction to the Theory of Computation
EXAMPLE 1.12
Find a grammar that generates
L =

anbn+1 : n ≥0

.
The idea behind the previous example can be extended to this case.
All we need to do is generate an extra b.
This can be done with
a production S →Ab, with other productions chosen so that A can
derive the language in the previous example. Reasoning in this fashion,
we get the grammar G = ({S, A} , {a, b} , S, P), with productions
S →Ab,
A →aAb,
A →λ.
Derive a few speciﬁc sentences to convince yourself that this works.
The previous examples are fairly easy ones, so rigorous arguments may
seem superﬂuous. But often it is not so easy to ﬁnd a grammar for a lan-
guage described in an informal way or to give an intuitive characterization
of the language deﬁned by a grammar. To show that a given language is
indeed generated by a certain grammar G, we must be able to show (a) that
every w ∈L can be derived from S using G and (b) that every string so
derived is in L.
EXAMPLE 1.13
Take Σ = {a, b}, and let na (w) and nb (w) denote the number of
a's and b's in the string w, respectively. Then the grammar G with
productions
S →SS,
S →λ,
S →aSb,
S →bSa
generates the language
L = {w : na (w) = nb (w)} .
This claim is not so obvious, and we need to provide convincing argu-
ments.

1.2 Three Basic Concepts
25
First, it is clear that every sentential form of G has an equal number
of a's and b's, since the only productions that generate an a, namely
S →aSb and S →bSa, simultaneously generate a b. Therefore, every
element of L (G) is in L. It is a little harder to see that every string in
L can be derived with G.
Let us begin by looking at the problem in outline, considering the
various forms w ∈L can have. Suppose w starts with a and ends with
b. Then it has the form
w = aw1b,
where w1 is also in L.
We can think of this case as being derived
starting with
S ⇒aSb
if S does indeed derive any string in L. A similar argument can be
made if w starts with b and ends with a. But this does not take care of
all cases, since a string in L can begin and end with the same symbol.
If we write down a string of this type, say aabbba, we see that it can
be considered as the concatenation of two shorter strings aabb and ba,
both of which are in L. Is this true in general? To show that this is
indeed so, we can use the following argument: Suppose that, starting
at the left end of the string, we count +1 for an a and −1 for a b.
If a string w starts and ends with a, then the count will be +1 after
the leftmost symbol and −1 immediately before the rightmost one.
Therefore, the count has to go through zero somewhere in the middle
of the string, indicating that such a string must have the form
w = w1w2,
where both w1 and w2 are in L. This case can be taken care of by the
production S →SS.
Once we see the argument intuitively, we are ready to proceed
more rigorously. Again we use induction. Assume that all w ∈L with
|w| ≤2n can be derived with G. Take any w ∈L of length 2n + 2. If
w = aw1b, then w1 is in L, and |w1| = 2n. Therefore, by assumption,
S
∗⇒w1.
Then
S ⇒aSb
∗⇒aw1b = w
is possible, and w can be derived with G. Obviously, similar arguments
can be made if w = bw1a.
If w is not of this form, that is, if it starts and ends with the same
symbol, then the counting argument tells us that it must have the form

26
Chapter 1 Introduction to the Theory of Computation
w = w1w2, with w1 and w2 both in L and of length less than or equal
to 2n. Hence again we see that
S ⇒SS
∗⇒w1S
∗⇒w1w2 = w
is possible.
Since the inductive assumption is clearly satisﬁed for n = 1, we
have a basis, and the claim is true for all n, completing our argument.
Normally, a given language has many grammars that generate it. Even
though these grammars are diﬀerent, they are equivalent in some sense. We
say that two grammars G1 and G2 are equivalent if they generate the same
language, that is, if
L (G1) = L (G2) .
As we will see later, it is not always easy to see if two grammars are
equivalent.
EXAMPLE 1.14
Consider the grammar G1 = ({A, S} , {a, b} , S, P1), with P1 consisting
of the productions
S →aAb|λ,
A →aAb|λ.
Here we introduce a convenient shorthand notation in which several
production rules with the same left-hand sides are written on the same
line, with alternative right-hand sides separated by |. In this notation
S →aAb|λ stands for the two productions S →aAb and S →λ.
This grammar is equivalent to the grammar G in Example 1.11.
The equivalence is easy to prove by showing that
L (G1) = {anbn : n ≥0} .
We leave this as an exercise.
Automata
An automaton is an abstract model of a digital computer. As such, every
automaton includes some essential features. It has a mechanism for reading
input. It will be assumed that the input is a string over a given alphabet,
written on an input ﬁle, which the automaton can read but not change.

1.2 Three Basic Concepts
27
Input file
Control unit
Storage
Output
FIGURE 1.4
The input ﬁle is divided into cells, each of which can hold one symbol. The
input mechanism can read the input ﬁle from left to right, one symbol at
a time. The input mechanism can also detect the end of the input string
(by sensing an end-of-ﬁle condition). The automaton can produce output
of some form. It may have a temporary storage device, consisting of an
unlimited number of cells, each capable of holding a single symbol from
an alphabet (not necessarily the same one as the input alphabet).
The
automaton can read and change the contents of the storage cells. Finally, the
automaton has a control unit, which can be in any one of a ﬁnite number
of internal states, and which can change state in some deﬁned manner.
Figure 1.4 shows a schematic representation of a general automaton.
An automaton is assumed to operate in a discrete timeframe. At any
given time, the control unit is in some internal state, and the input mecha-
nism is scanning a particular symbol on the input ﬁle. The internal state of
the control unit at the next time step is determined by the next-state or
transition function. This transition function gives the next state in terms
of the current state, the current input symbol, and the information currently
in the temporary storage. During the transition from one time interval to
the next, output may be produced or the information in the temporary stor-
age changed. The term conﬁguration will be used to refer to a particular
state of the control unit, input ﬁle, and temporary storage. The transition
of the automaton from one conﬁguration to the next will be called a move.
This general model covers all the automata we will discuss in this book.
A ﬁnite-state control will be common to all speciﬁc cases, but diﬀerences will
arise from the way in which the output can be produced and the nature of
the temporary storage. As we will see, the nature of the temporary storage
governs the power of diﬀerent types of automata.
For subsequent discussions, it will be necessary to distinguish between
deterministic automata and nondeterministic automata. A deter-
ministic automaton is one in which each move is uniquely determined by

28
Chapter 1 Introduction to the Theory of Computation
the current conﬁguration. If we know the internal state, the input, and
the contents of the temporary storage, we can predict the future behavior
of the automaton exactly.
In a nondeterministic automaton, this is not
so. At each point, a nondeterministic automaton may have several possi-
ble moves, so we can only predict a set of possible actions. The relation
between deterministic and nondeterministic automata of various types will
play a signiﬁcant role in our study.
An automaton whose output response is limited to a simple "yes" or
"no" is called an accepter. Presented with an input string, an accepter
either accepts the string or rejects it. A more general automaton, capable
of producing strings of symbols as output, is called a transducer.
EXERCISES
1. How many substrings aab are in wwRw, where w = aabbab?
2. Use induction on n to show that |un| = n |u| for all strings u and all n.
3. The reverse of a string, introduced informally above, can be deﬁned more
precisely by the recursive rules
aR = a,
(wa)R = awR,
for all a ∈Σ, w ∈Σ∗. Use this to prove that
(uv)R = vRuR,
for all u, v ∈Σ+.
4. Prove that

wRR = w for all w ∈Σ∗.
5. Let L = {ab, aa, baa}. Which of the following strings are in L∗: abaabaaabaa,
aaaabaaaa, baaaaabaaaab, baaaaabaa? Which strings are in L4?
6. Let Σ = {a, b} and L = {aa, bb}. Use set notation to describe L.
7. Let L be any language on a nonempty alphabet. Show that L and L cannot
both be ﬁnite.
8. Are there languages for which L∗= (L)∗?
9. Prove that
(L1L2)R = LR
2 LR
1
for all languages L1 and L2.
10. Show that (L∗)∗= L∗for all languages.
11. Prove or disprove the following claims.
(a) (L1 ∪L2)R = LR
1 ∪LR
2 for all languages L1 and L2.
(b)

LR∗= (L∗)R for all languages L.

1.2 Three Basic Concepts
29
12. Find a grammar for the language L = {an, where n is even}.
13. Find a grammar for the language L = {an, where n is even and n > 3}.
14. Find grammars for Σ = {a, b} that generate the sets of
(a) all strings with exactly two a's.
(b) all strings with at least two a's.
(c) all strings with no more than three a's.
(d) all strings with at least three a's.
(e) all strings that start with a and end with b.
(f) all strings with an even number of b's.
In each case, give convincing arguments that the grammar you give does
indeed generate the indicated language.
15. Give a simple description of the language generated by the grammar with
productions
S →aaA,
A →bS,
S →λ.
16. What language does the grammar with these productions generate?
S →Aa,
A →B,
B →Aa.
17. Let Σ = {a, b}. For each of the following languages, ﬁnd a grammar that
generates it.
(a) L1 = {anbm : n ≥0, m < n}.
(b) L2 =
	
a3nb2n : n ≥2

.
(c) L3 =
	
an+3bn : n ≥2

.
(d) L4 =
	
anbn−2 : n ≥3

.
(e) L1L2.
(f) L1 ∪L2.
(g) L3
1.
(h) L∗
1.
(i) L1 −L4.
18. Find grammars for the following languages on Σ = {a}.
(a) L = {w : |w| mod 3 > 0}.
(b) L = {w : |w| mod 3 = 2}.
(c) w = {|w| mod 5 = 0}.

30
Chapter 1 Introduction to the Theory of Computation
19. Find a grammar that generates the language
L =

wwR : w ∈{a, b}+
.
Give a complete justiﬁcation for your answer.
20. Find three strings in the language generated by
S →aSb|bSa|a.
21. Complete the arguments in Example 1.14, showing that L (G1) does in fact
generate the given language.
.
22. Show that the grammar G = ({S} , {a, b} , S, P), with productions
S →SS |SSS| aSb |bSa| λ,
is equivalent to the grammar in Example 1.13.
23. Show that the grammars
S →aSb|ab|λ
and
S →aaSbb|aSb|ab|λ
are equivalent.
24. Show that the grammars
S →aSb|bSa|SS|a
and
S →aSb|bSa|a
are not equivalent.
1.3
SOME APPLICATIONS*
Although we stress the abstract and mathematical nature of formal lan-
guages and automata, it turns out that these concepts have widespread
applications in computer science and are, in fact, a common theme that
connects many specialty areas. In this section, we present some simple ex-
amples to give the reader some assurance that what we study here is not
just a collection of abstractions, but is something that helps us understand
many important, real problems.
Formal languages and grammars are used widely in connection with
programming languages.
In most of our programming, we work with a
more or less intuitive understanding of the language in which we write.
Occasionally though, when using an unfamiliar feature, we may need to
refer to precise descriptions such as the syntax diagrams found in most

1.3 Some Applications*
31
programming texts. If we write a compiler, or if we wish to reason about
the correctness of a program, a precise description of the language is needed
at almost every step. Among the ways in which programming languages can
be deﬁned precisely, grammars are perhaps the most widely used.
The grammars that describe a typical language like Pascal or C are very
extensive. For an example, let us take a smaller language that is part of a
larger one.
EXAMPLE 1.15
The rules for variable identiﬁers in C are
1. An identiﬁer is a sequence of letters, digits, and underscores.
2. An identiﬁer must start with a letter or an underscore.
3. Identiﬁers allow upper- and lower-case letters.
Formally, these rules can be described by a grammar.
<id> →<letter><rest>|<undrscr><rest>
<rest> →<letter><rest>|<digit > <rest>|<undrscr><rest>|λ
<letter> →a|b|...|z|A|B|...|Z
<digit> →0|1|...|9
<undrscr> →-
In this grammar, the variables are ⟨id⟩, ⟨letter⟩, ⟨digit⟩, ⟨undrscr⟩,
and ⟨rest⟩. The letters, digits, and the underscore are terminals. A
derivation of a0 is
⟨id⟩⇒⟨letter⟩⟨rest⟩
⇒a ⟨rest⟩
⇒a ⟨digit⟩⟨rest⟩
⇒a0 ⟨rest⟩
⇒a0.
The deﬁnition of programming languages through grammars is
common and very useful.
But there are alternatives that are often
convenient. For example, we can describe a language by an accepter,
taking every string that is accepted as part of the language. To talk
about this in a precise way, we will need to give a more formal deﬁni-
tion of an automaton. We will do this shortly; for the moment, let us
proceed in a more intuitive way.

32
Chapter 1 Introduction to the Theory of Computation
2
1
a
FIGURE 1.5
An automaton can be represented by a graph in which the vertices
give the internal states and the edges transitions. The labels on the
edges show what happens (in terms of input and output) during the
transition. For example, Figure 1.5 represents a transition from State
1 to State 2, which is taken when the input symbol is a. With this
intuitive picture in mind, let us look at another way of describing C
identiﬁers.
EXAMPLE 1.16
Figure 1.6 is an automaton that accepts all legal C identiﬁers. Some
interpretation is necessary. We assume that initially the automaton is
in State 1; we indicate this by drawing an arrow (not originating in
any vertex) to this state. As always, the string to be examined is read
left to right, one character at each step. When the ﬁrst symbol is a
letter or an underscore, the automaton goes into State 2, after which
the rest of the string is immaterial. State 2 therefore represents the
"yes" state of the accepter. Conversely, if the ﬁrst symbol is a digit,
the automaton will go into State 3, the "no" state, and remain there.
In our solution, we assume that no input other than letters, digits, or
underscores is possible.
letter or undrscr
letter, digit, or undrscr
digit
1
3
2
FIGURE 1.6
Compilers and other translators that convert a program from one lan-
guage to another make extensive use of the ideas touched on in these exam-
ples. Programming languages can be deﬁned precisely through grammars,

1.3 Some Applications*
33
as in Example 1.15, and both grammars and automata play a fundamental
role in the decision processes by which a speciﬁc piece of code is accepted as
satisfying the conditions of a programming language. The above example
gives a ﬁrst hint of how this is done; subsequent examples will expand on
this observation.
Transducers will be discussed brieﬂy in Appendix A; the following ex-
ample previews this subject.
EXAMPLE 1.17
A binary adder is an integral part of any general-purpose computer.
Such an adder takes two bit strings representing numbers and produces
their sum as output. For simplicity, let us assume that we are dealing
only with positive integers and that we use a representation in which
x = a0a1 · · · an
stands for the integer
v (x) =
n

i=0
ai2i.
This is the usual binary representation in reverse.
A serial adder processes two such numbers x = a0a1 · · · an, and
y = b0b1 · · · bn, bit by bit, starting at the left end. Each bit addition
creates a digit for the sum as well as a carry digit for the next higher
position. A binary addition table (Figure 1.7) summarizes the process.
A block diagram of the kind we saw when we ﬁrst studied comput-
ers is given in Figure 1.8. It tells us that an adder is a box that accepts
two bits and produces their sum bit and a possible carry. It describes
what an adder does, but explains little about its internal workings. An
automaton (now a transducer) can make this much more explicit.
0
1
0
No carry
1
No carry
1
No carry
0
1
0
Carry
ai
bi
FIGURE 1.7

34
Chapter 1 Introduction to the Theory of Computation
Sum bit di
 ai
 bi
Serial adder
Carry
FIGURE 1.8
No carry
(0, 0)/0
(1, 0)/1
(1, 1)/0
(0, 0)/1
(0, 1)/1
Carry
(0, 1)/0
(1, 1)/1
(1, 0)/0
FIGURE 1.9
The input to the transducer are the bit pairs (ai, bi); the output
will be the sum bit di. Again, we represent the automaton by a graph
now labeling the edges (ai, bi) /di. The carry from one step to the next
is remembered by the automaton via two internal states labeled "carry"
and "no carry." Initially, the transducer will be in state "no carry."
It will remain in this state until a bit pair (1, 1) is encountered; this
will generate a carry that takes the automaton into the "carry" state.
The presence of a carry is then taken into account when the next bit
pair is read. A complete picture of a serial adder is given in Figure 1.9.
Follow this through with a few examples to convince yourself that it
works correctly.
As this example indicates, the automaton serves as a bridge be-
tween the very high-level, functional description of a circuit and its
logical implementation through transistors, gates, and ﬂip-ﬂops. The
automaton clearly shows the decision logic, yet it is formal enough
to lend itself to precise mathematical manipulation.
For this rea-
son, digital design methods rely heavily on concepts from automata
theory.

1.3 Some Applications*
35
EXERCISES
1. While passwords generally have few restrictions, they are normally not totally
free. Suppose that in a certain system, passwords can be of arbitrary length
but must contain at least one letter, a-z, and one number 0-9. Construct a
grammar that generates the set of such legal passwords.
2. Suppose that in some programming language, numbers are restricted as
follows:
(a) A number may be signed or unsigned.
(b) The value ﬁeld consists of two nonempty parts, separated by a decimal
point.
(c) There is an optional exponent ﬁeld. If present, this ﬁeld must contain
the letter e, followed by a signed two-digit integer.
Design a grammar set of such numbers.
3. Give a grammar for the set of integer numbers in C.
4. Design an accepter for integers in C.
5. Give a grammar that generates all real constants in C.
6. Suppose that a certain programming language permits only identiﬁers that
begin with a letter, contain at least one but no more than three digits, and
can have any number of letters. Give a grammar and an accepter for such a
set of identiﬁers.
7. Modify the grammar in Example 1.15 so that the identiﬁers satisfy the fol-
lowing rules:
(a) C rules, except that an underscore cannot be the leftmost symbol.
(b) C rules, except that there can be at most one underscore.
(c) C rules, except that an underscore cannot be followed by a digit.
8. In the Roman number system, numbers are represented by strings on the
alphabet {M, D, C, L, X, V, I}. Design an accepter that accepts such strings
only if they are properly formed Roman numbers. For simplicity, replace the
"subtraction" convention in which the number nine is represented by IX with
an addition equivalent that uses V IIII instead.
9. We assumed that an automaton works in a framework of discrete time steps,
but this aspect has little inﬂuence on our subsequent discussion. In digital
design, however, the time element assumes considerable signiﬁcance. In order
to synchronize signals arriving from diﬀerent parts of the computer, delay
circuitry is needed. A unit-delay transducer is one that simply reproduces
the input (viewed as a continual stream of symbols) one time unit later.
Speciﬁcally, if the transducer reads as input a symbol a at time t, it will

36
Chapter 1 Introduction to the Theory of Computation
reproduce that symbol as output at time t + 1. At time t = 0, the transducer
outputs nothing. We indicate this by saying that the transducer translates
input a1a2 · · · into output λa1a2 · · · .
Draw a graph showing how such a unit-delay transducer might be designed
for Σ = {a, b}.
10. An n-unit delay transducer is one that reproduces the input n time units
later; that is, the input a1a2 · · · is translated into λna1a2 · · · , meaning again
that the transducer produces no output for the ﬁrst n time slots.
(a) Construct a two-unit delay transducer on Σ = {a, b}.
(b) Show that an n-unit delay transducer must have at least |Σ|n states.
11. The two's complement of a binary string, representing a positive integer,
is formed by ﬁrst complementing each bit, then adding one to the lowest-
order bit.
Design a transducer for translating bit strings into their two's
complement, assuming that the binary number is represented as in Example
1.17, with lower-order bits at the left of the string.
12. Design a transducer to convert a binary string into octal. For example, the
bit string 001101110 should produce the output 156.
13. Let a1a2 · · · be an input bit string. Design a transducer that computes the
parity of every substring of three bits. Speciﬁcally, the transducer should
produce output
π1 = π2 = 0,
πi = (ai−2 + ai−1 + ai) mod 2, i = 3, 4, ....
For example, the input 110111 should produce 000001.
14. Design a transducer that accepts bit strings a1a2a3... and computes the binary
value of each set of three consecutive bits modulo ﬁve. More speciﬁcally, the
transducer should produce m1, m2, m3, ..., where
m1 = m2 = 0,
mi = (4ai + 2ai−1 + ai−2) mod 5, i = 3, 4, ....
15. Digital computers normally represent all information by bit strings, using
some type of encoding. For example, character information can be encoded
using the well-known ASCII system.
For this exercise, consider the two alphabets {a, b, c, d} and {0, 1}, respec-
tively, and an encoding from the ﬁrst to the second, deﬁned by a →00,
b →01, c →10, d →11. Construct a transducer for decoding strings on
{0, 1} into the original message. For example, the input 010011 should gen-
erate as output bad.
16. Let x and y be two positive binary numbers.
Design a transducer whose
output is max(x, y).

2
CHA P T E R
FINITE
AUTOMATA
CHAPTER SUMMARY
In this chapter, we encounter our first simple automaton, a finite state
accepter. It is finite because it has only a finite set of internal states and
no other memory. It is called an accepter because it processes strings
and either accepts or rejects them, so we can think of it as a simple
pattern recognition mechanism.
We start with a deterministic finite accepter, or dfa. The adjective
''deterministic'' signifies that the automaton has one and only one op-
tion at any time.
We use dfa's to define a certain type of language,
called a regular language, and so establish a first connection between
automata and languages, a recurring theme in subsequent discussions.
Next, we introduce nondeterministic automata, or nfas. In certain sit-
uations, an nfa can have several options and therefore can appear to
have a choice. In a deterministic world, or at least in the determinis-
tic world of computers, what is the role of nondeterminism? While we
sometimes say that an nfa can make the best choice, this is just a ver-
bal convenience. A better way to visualize nondeterminism is to think
that an nfa explores all choices (say, by a search and backtrack method)
and makes no decision until all options have been analyzed. The main
reason for introducing nondeterminism is that it simplifies the solution
of many problems.
37

38
Chapter 2 Finite Automata
We say that two accepters are equivalent if they accept the same
language. We can then also say that the class of nfa's is equivalent to the
class of dfa's because for every every nfa, we can find an equivalent dfa.
Equivalence between different types of constructs is another recurring
theme in this book.
For any given regular language there are many equivalent dfa's, so
it is of practical importance to find a minimal dfa, that is, a dfa with
the smallest number of internal states. The construction in Section 2.4
shows how this can be done.
O
ur introduction in the ﬁrst chapter to the basic concepts of compu-
tation, particularly the discussion of automata, is brief and infor-
mal. At this point, we have only a general understanding of what
an automaton is and how it can be represented by a graph. To progress, we
must be more precise, provide formal deﬁnitions, and start to develop rigor-
ous results. We begin with ﬁnite accepters, which are a simple, special case
of the general scheme introduced in the last chapter. This type of automa-
ton is characterized by having no temporary storage. Since an input ﬁle
cannot be rewritten, a ﬁnite automaton is severely limited in its capacity to
"remember" things during the computation. A ﬁnite amount of information
can be retained in the control unit by placing the unit into a speciﬁc state.
But since the number of such states is ﬁnite, a ﬁnite automaton can only
deal with situations in which the information to be stored at any time is
strictly bounded. The automaton in Example 1.16 is an instance of a ﬁnite
accepter.
2.1
DETERMINISTIC FINITE ACCEPTERS
The ﬁrst type of automaton we study in detail are ﬁnite accepters that are
deterministic in their operation. We start with a precise formal deﬁnition
of deterministic accepters.
Figures 1.6 and 1.9 represent two simple automata, with some common
features:
• Both have a ﬁnite number of internal states.
• Both process an input string, consisting of a sequence of symbols.
• Both make transitions from one state to another, depending on the
current state and the current input symbol.
• Both produce some output, but in a slightly diﬀerent form. The au-
tomaton in Figure 1.6 only accepts or rejects the input; the automaton
in Figure 1.9 generates an output string.

2.1 Deterministic Finite Accepters
39
Notice also that both automata have a single well-deﬁned transition at each
step. All of these features are incorporated in the following deﬁnition.
Deterministic Accepters and Transition Graphs
DEFINITION 2.1
A deterministic ﬁnite accepter or dfa is deﬁned by the quintuple
M = (Q, Σ, δ, q0, F) ,
where
Q is a ﬁnite set of internal states,
Σ is a ﬁnite set of symbols called the input alphabet,
δ : Q × Σ →Q is a total function called the transition function,
q0 ∈Q is the initial state,
F ⊆Q is a set of ﬁnal states.
A deterministic ﬁnite accepter operates in the following manner. At
the initial time, it is assumed to be in the initial state q0, with its input
mechanism on the leftmost symbol of the input string. During each move
of the automaton, the input mechanism advances one position to the right,
so each move consumes one input symbol. When the end of the string is
reached, the string is accepted if the automaton is in one of its ﬁnal states.
Otherwise the string is rejected. The input mechanism can move only from
left to right and reads exactly one symbol on each step. The transitions
from one internal state to another are governed by the transition function
δ. For example, if
δ (q0, a) = q1,
then if the dfa is in state q0 and the current input symbol is a, the dfa will
go into state q1.
In discussing automata, it is essential to have a clear and intuitive
picture to work with. To visualize and represent ﬁnite automata, we use
transition graphs, in which the vertices represent states and the edges
represent transitions. The labels on the vertices are the names of the states,
while the labels on the edges are the current values of the input symbol.
For example, if q0 and q1 are internal states of some dfa M, then the graph
associated with M will have one vertex labeled q0 and another labeled q1.
An edge (q0, q1) labeled a represents the transition δ (q0, a) = q1. The initial
state will be identiﬁed by an incoming unlabeled arrow not originating at
any vertex. Final states are drawn with a double circle.
More formally, if M = (Q, Σ, δ, q0, F) is a deterministic ﬁnite accepter,
then its associated transition graph GM has exactly |Q| vertices, each one

40
Chapter 2 Finite Automata
labeled with a diﬀerent qi ∈Q. For every transition rule δ (qi, a) = qj, the
graph has an edge (qi, qj) labeled a. The vertex associated with q0 is called
the initial vertex, while those labeled with qf ∈F are the ﬁnal vertices.
It is a trivial matter to convert from the (Q, Σ, δ, q0, F) deﬁnition of a dfa
to its transition graph representation and vice versa.
EXAMPLE 2.1
The graph in Figure 2.1 represents the dfa
M = ({q0, q1, q2} , {0, 1} , δ, q0, {q1}) ,
where δ is given by
δ (q0, 0) = q0,
δ (q0, 1) = q1,
δ (q1, 0) = q0,
δ (q1, 1) = q2,
δ (q2, 0) = q2,
δ (q2, 1) = q1.
This dfa accepts the string 01. Starting in state q0, the symbol 0 is read
ﬁrst. Looking at the edges of the graph, we see that the automaton
remains in state q0. Next, the 1 is read and the automaton goes into
state q1. We are now at the end of the string and, at the same time,
in a ﬁnal state q1. Therefore, the string 01 is accepted. The dfa does
not accept the string 00, since after reading two consecutive 0's, it will
be in state q0. By similar reasoning, we see that the automaton will
accept the strings 101, 0111, and 11001, but not 100 or 1100.
q1
q2
q0
0
0
0
1
1
1
FIGURE 2.1
It is convenient to introduce the extended transition function δ∗: Q ×
Σ∗→Q.
The second argument of δ∗is a string, rather than a single
symbol, and its value gives the state the automaton will be in after reading
that string. For example, if
δ (q0, a) = q1
and
δ (q1, b) = q2,

2.1 Deterministic Finite Accepters
41
then
δ∗(q0, ab) = q2.
Formally, we can deﬁne δ∗recursively by
δ∗(q, λ) = q,
(2.1)
δ∗(q, wa) = δ (δ∗(q, w) , a) ,
(2.2)
for all q ∈Q, w ∈Σ∗, a ∈Σ. To see why this is appropriate, let us apply
these deﬁnitions to the simple case above. First, we use (2.2) to get
δ∗(q0, ab) = δ (δ∗(q0, a) , b) .
(2.3)
But
δ∗(q0, a) = δ (δ∗(q0, λ) , a)
= δ (q0, a)
= q1.
Substituting this into (2.3), we get
δ∗(q0, ab) = δ (q1, b) = q2,
as expected.
Languages and Dfa's
Having made a precise deﬁnition of an accepter, we are now ready to deﬁne
formally what we mean by an associated language. The association is obvi-
ous: The language is the set of all the strings accepted by the automaton.
DEFINITION 2.2
The language accepted by a dfa M = (Q, Σ, δ, q0, F) is the set of all strings
on Σ accepted by M. In formal notation,
L (M) = {w ∈Σ∗: δ∗(q0, w) ∈F} .
Note that we require that δ, and consequently δ∗, be total functions.
At each step, a unique move is deﬁned, so that we are justiﬁed in calling
such an automaton deterministic. A dfa will process every string in Σ∗and
either accept it or not accept it. Nonacceptance means that the dfa stops
in a nonﬁnal state, so that
L (M) = {w ∈Σ∗: δ∗(q0, w) /∈F} .

42
Chapter 2 Finite Automata
EXAMPLE 2.2
Consider the dfa in Figure 2.2.
In drawing Figure 2.2 we allowed the use of two labels on a single
edge. Such multiply labeled edges are shorthand for two or more dis-
tinct transitions: The transition is taken whenever the input symbol
matches any of the edge labels.
The automaton in Figure 2.2 remains in its initial state q0 until the
ﬁrst b is encountered. If this is also the last symbol of the input, then
the string is accepted. If not, the dfa goes into state q2, from which
it can never escape.
The state q2 is a trap state.
We see clearly
from the graph that the automaton accepts all strings consisting of
an arbitrary number of a's, followed by a single b.
All other input
strings are rejected.
In set notation, the language accepted by the
automaton is
L = {anb : n ≥0} .
q1
q2
q0
a
a, b
b
a, b
FIGURE 2.2
These examples show how convenient transition graphs are for working
with ﬁnite automata. While it is possible to base all arguments strictly on
the properties of the transition function and its extension through (2.1) and
(2.2), the results are hard to follow. In our discussion, we use graphs, which
are more intuitive, as far as possible. To do so, we must, of course, have some
assurance that we are not misled by the representation and that arguments
based on graphs are as valid as those that use the formal properties of δ.
The following preliminary result gives us this assurance.
THEOREM 2.1
Let M = (Q, Σ, δ, q0, F) be a deterministic ﬁnite accepter, and let GM be
its associated transition graph. Then for every qi, qj ∈Q, and w ∈Σ+,
δ∗(qi, w) = qj if and only if there is in GM a walk with label w from qi
to qj.
Proof: This claim is fairly obvious from an examination of such simple cases
as Example 2.1. It can be proved rigorously using an induction on the length

2.1 Deterministic Finite Accepters
43
of w. Assume that the claim is true for all strings v with |v| ≤n. Consider
then any w of length n + 1 and write it as
w = va.
Suppose now that δ∗(qi, v) = qk. Since |v| = n, there must be a walk in
GM labeled v from qi to qk. But if δ∗(qi, w) = qj, then M must have a
transition δ (qk, a) = qj, so that by construction GM has an edge (qk, qj)
with label a. Thus, there is a walk in GM labeled va = w between qi and
qj. Since the result is obviously true for n = 1, we can claim by induction
that, for every w ∈Σ+,
δ∗(qi, w) = qj
(2.4)
implies that there is a walk in GM from qi to qj labeled w.
The argument can be turned around in a straightforward way to
show that the existence of such a path implies (2.4), thus completing the
proof.
Again, the result of the theorem is so intuitively obvious that a formal
proof seems unnecessary. We went through the details for two reasons. The
ﬁrst is that it is a simple, yet typical example of an inductive proof in con-
nection with automata. The second is that the result will be used over and
over, so stating and proving it as a theorem lets us argue quite conﬁdently
using graphs. This makes our examples and proofs more transparent than
they would be if we used the properties of δ∗.
While graphs are convenient for visualizing automata, other represen-
tations are also useful. For example, we can represent the function δ as a
table. The table in Figure 2.3 is equivalent to Figure 2.2. Here the row la-
bel is the current state, while the column label represents the current input
symbol. The entry in the table deﬁnes the next state.
a
b
q0
q1
q2
q2
q0
q1
q2
q2
q2
FIGURE 2.3

44
Chapter 2 Finite Automata
It is apparent from this example that a dfa can easily be implemented as
a computer program; for example, as a simple table-lookup or as a sequence
of if statements. The best implementation or representation depends on the
speciﬁc application. Transition graphs are very convenient for the kinds of
arguments we want to make here, so we use them in most of our discussions.
In constructing automata for languages deﬁned informally, we employ
reasoning similar to that for programming in higher-level languages. But the
programming of a dfa is tedious and sometimes conceptually complicated
by the fact that such an automaton has few powerful features.
EXAMPLE 2.3
Find a deterministic ﬁnite accepter that recognizes the set of all
strings on Σ = {a, b} starting with the preﬁx ab.
The only issue here is the ﬁrst two symbols in the string; after they
have been read, no further decisions are needed. Still, the automaton
has to process the whole string before its decision is made. We can
therefore solve the problem with an automaton that has four states:
an initial state, two states for recognizing ab ending in a ﬁnal trap
state, and one nonﬁnal trap state. If the ﬁrst symbol is an a and the
second is a b, the automaton goes to the ﬁnal trap state, where it will
stay since the rest of the input does not matter. On the other hand, if
the ﬁrst symbol is not an a or the second one is not a b, the automa-
ton enters the nonﬁnal trap state. The simple solution is shown in
Figure 2.4.
q1
q2
q0
a, b
a, b
a
q3
b
b
a
FIGURE 2.4

2.1 Deterministic Finite Accepters
45
EXAMPLE 2.4
Find a dfa that accepts all the strings on {0, 1}, except those
containing the substring 001.
In deciding whether the substring 001 has occurred, we need to
know not only the current input symbol, but we also need to remember
whether or not it has been preceded by one or two 0's. We can keep
track of this by putting the automaton into speciﬁc states and labeling
them accordingly. Like variable names in a programming language,
state names are arbitrary and can be chosen for mnemonic reasons.
For example, the state in which two 0's were the immediately preceding
symbols can be labeled simply 00.
If the string starts with 001, then it must be rejected. This implies
that there must be a path labeled 001 from the initial state to a nonﬁnal
state. For convenience, this nonﬁnal state is labeled 001. This state
must be a trap state, because later symbols do not matter. All other
states are accepting states.
This gives us the basic structure of the solution, but we still must
add provisions for the substring 001 occurring in the middle of the
input. We must deﬁne Q and δ so that whatever we need to make the
correct decision is remembered by the automaton. In this case, when
a symbol is read, we need to know some part of the string to the left,
for example, whether or not the two previous symbols were 00. If we
label the states with the relevant symbols, it is very easy to see what
the transitions must be. For example,
δ (00, 0) = 00
because this situation arises only if there are three consecutive 0's. We
are only interested in the last two, a fact we remember by keeping the
dfa in the state 00. A complete solution is shown in Figure 2.5. We
see from this example how useful mnemonic labels on the states are
for keeping track of things. Trace a few strings, such as 100100 and
1010100, to see that the solution is indeed correct.
0
00
0
1
0
1
0
001
0, 1
1
λ
FIGURE 2.5

46
Chapter 2 Finite Automata
Regular Languages
Every ﬁnite automaton accepts some language. If we consider all possible
ﬁnite automata, we get a set of languages associated with them. We will call
such a set of languages a family. The family of languages that is accepted by
deterministic ﬁnite accepters is quite limited. The structure and properties
of the languages in this family will become clearer as our study proceeds;
for the moment we will simply attach a name to this family.
DEFINITION 2.3
A language L is called regular if and only if there exists some deterministic
ﬁnite accepter M such that
L = L (M) .
EXAMPLE 2.5
Show that the language
L =

awa : w ∈{a, b}∗
is regular.
To show that this or any other language is regular, all we have to
do is ﬁnd a dfa for it. The construction of a dfa for this language is
similar to Example 2.3, but a little more complicated. What this dfa
must do is check whether a string begins and ends with an a; what is
between is immaterial. The solution is complicated by the fact that
there is no explicit way of testing the end of the string. This diﬃculty
is overcome by simply putting the dfa into a ﬁnal state whenever the
second a is encountered. If this is not the end of the string, and another
b is found, it will take the dfa out of the ﬁnal state. Scanning continues
in this way, each a taking the automaton back to its ﬁnal state. The
complete solution is shown in Figure 2.6. Again, trace a few examples
to see why this works. After one or two tests, it will be obvious that
the dfa accepts a string if and only if it begins and ends with an a.
Since we have constructed a dfa for the language, we can claim that,
by deﬁnition, the language is regular.

2.1 Deterministic Finite Accepters
47
q3
00
a
b
a
b
a
q2
q0
q1
a, b
b
FIGURE 2.6
EXAMPLE 2.6
Let L be the language in Example 2.5. Show that L2 is regular. Again
we show that the language is regular by constructing a dfa for it. We
can write an explicit expression for L2, namely,
L2 =

aw1aaw2a : w1, w2 ∈{a, b}∗
.
q3
a
a
b
00
b
b
a
b
a
q2
a
q4
q0
q1
a, b
b
q5
FIGURE 2.7

48
Chapter 2 Finite Automata
Therefore, we need a dfa that recognizes two consecutive strings of
essentially the same form (but not necessarily identical in value). The
diagram in Figure 2.6 can be used as a starting point, but the vertex
q3 has to be modiﬁed. This state can no longer be ﬁnal since, at this
point, we must start to look for a second substring of the form awa. To
recognize the second substring, we replicate the states of the ﬁrst part
(with new names), with q3 as the beginning of the second part. Since
the complete string can be broken into its constituent parts wherever
aa occurs, we let the ﬁrst occurrence of two consecutive a's be the
trigger that gets the automaton into its second part. We can do this
by making δ (q3, a) = q4. The complete solution is in Figure 2.7. This
dfa accepts L2, which is therefore regular.
The last example suggests the conjecture that if a language L is regular,
so are L2, L3, .... We will see later that this is indeed correct.
EXERCISES
1. Which of the strings 0001, 01101, 00001101 are accepted by the dfa in
Figure 2.1?
2. Translate the graph in Figure 2.5 into δ- notation.
3. For Σ = {a, b}, construct dfa's that accept the sets consisting of
(a) all strings of even length.
(b) all strings of length greater than 5.
(c) all strings with an even number of a's.
(d) all strings with an even number of a's and an odd number of b's.
4. For Σ = {a, b}, construct dfa's that accept the sets consisting of
(a) all strings with exactly one a.
(b) all strings with at least two a's.
(c) all strings with no more than two a's.
(d) all strings with at least one b and exactly two a's.
(e) all the strings with exactly two a's and more than three b's.
5. Give dfa's for the languages
(a) L =

ab4wb2 : w ∈{a, b}∗
.
(b) L = {abnam : n ≥3, m ≥2}.

2.1 Deterministic Finite Accepters
49
(c) L = {w1abbw2 : w1 ∈{a, b}∗, w2 ∈{a, b}∗}.
(d) L = {ban : n ≥1, n ̸= 4}.
6. With Σ = {a, b}, give a dfa for L = {w1aw2 : |w1| ≥3, |w2| ≤4}.
7. Find dfa's for the following languages on Σ = {a, b}.
(a) L = {w : |w| mod 3 ̸= 0}.
(b) L = {w : |w| mod 5 = 0}.
(c) L = {w : na (w) mod 3 < 1}.
(d) L = {w : na (w) mod 3 < nb (w) mod 3}.
(e) L = {w : (na (w) −nb (w)) mod 3 = 0}.
(f) L = {w : (na (w) + 2nb (w)) mod 3 < 1}.
(g) L = {w : |w| mod 3 = 0, |w| ̸= 5}.
8. A run in a string is a substring of length at least two, as long as possible,
and consisting entirely of the same symbol. For instance, the string abbbaab
contains a run of b's of length three and a run of a's of length two. Find dfa's
for the following languages on {a, b}:
(a) L = {w : w contains no runs of length less than three}.
(b) L = {w : every run of a's has length either two or three}.
(c) L = {w : there are at most two runs of a's of length three}.
(d) L = {w : there are exactly two runs of a's of length 3}.
9. Show that if we change Figure 2.6, making q3 a nonﬁnal state and making
q0, q1, q2 ﬁnal states, the resulting dfa accepts L.
10. Generalize the observation in the previous exercise. Speciﬁcally, show that if
M = (Q, Σ, δ, q0, F) and 
M = (Q, Σ, δ, q0, Q −F) are two dfa's, then L (M) =
L


M

.
11. Consider the set of strings on {0, 1} deﬁned by the requirements below. For
each, construct an accepting dfa.
(a) Every 00 is followed immediately by a 1. For example, the strings
101, 0010, 0010011001 are in the language, but 0001 and 00100
are not.
(b) All strings that contain the substring 000, but not 0000.
(c) The leftmost symbol diﬀers from the rightmost one.
(d) Every substring of four symbols has, at most, two 0's. For ex-
ample, 001110 and 011001 are in the language, but 10010 is not
because one of its substrings, 0010, contains three zeros.
(e) All strings of length ﬁve or more in which the third symbol from
the right end is diﬀerent from the leftmost symbol.

50
Chapter 2 Finite Automata
(f) All strings in which the leftmost two symbols and the rightmost
two symbols are identical.
(g) All strings of length four or greater in which the leftmost two
symbols are the same, but diﬀerent from the rightmost symbol.
12. Construct a dfa that accepts strings on {0, 1} if and only if the value of the
string, interpreted as a binary representation of an integer, is zero modulo ﬁve.
For example, 0101 and 1111, representing the integers 5 and 15, respectively,
are to be accepted.
13. Show that the language L = {vwv : v, w ∈{a, b}∗, |v| = 3} is regular.
14. Show that L = {an : n ≥3} is regular.
15. Show that the language L = {an : n ≥0, n ̸= 3} is regular.
16. Show that the language L = {an : n is either a multiple of three or a multiple
of 5} is regular.
17. Show that the language L = {an : n is a multiple of three, but not a multiple
of 5} is regular.
18. Show that the set of all real numbers in C is a regular language.
19. Show that if L is regular, so is L −{λ}.
20. Show that if L is regular, so is L ∪{aa}, for all a ∈Σ.
21. Use (2.1) and (2.2) to show that
δ∗(q, wv) = δ∗(δ∗(q, w) , v)
for all w, v ∈Σ∗.
22. Let L be the language accepted by the automaton in Figure 2.2. Find a dfa
that accepts L3.
23. Let L be the language accepted by the automaton in Figure 2.2. Find a dfa
for the language L2 −L.
24. Let L be the language in Example 2.5. Show that L∗is regular.
25. Let GM be the transition graph for some dfa M. Prove the following:
(a) If L (M) is inﬁnite, then GM must have at least one cycle for
which there is a path from the initial vertex to some vertex in
the cycle and a path from some vertex in the cycle to some ﬁnal
vertex.
(b) If L (M) is ﬁnite, then no such cycle exists.
26. Let us deﬁne an operation truncate, which removes the rightmost symbol
from any string. For example, truncate (aaaba) is aaab. The operation can
be extended to languages by
truncate (L) = {truncate (w) : w ∈L} .
Show how, given a dfa for any regular language L, one can construct a dfa for
truncate (L). From this, prove that if L is a regular language not containing
λ, then truncate (L) is also regular.

2.2 Nondeterministic Finite Accepters
51
27. While the language accepted by a given dfa is unique, there are normally
many dfa's that accept a language. Find a dfa with exactly six states that
accepts the same language as the dfa in Figure 2.4.
28. Can you ﬁnd a dfa with three states that accepts the language of the dfa in
Figure 2.4? If not, can you give convincing arguments that no such dfa can
exist?
2.2
NONDETERMINISTIC FINITE ACCEPTERS
If you examine the automata we have seen so far, you will notice a common
feature: a unique transition is deﬁned for each state and each input symbol.
In the formal deﬁnition, this is expressed by saying that δ is a total function.
This is the reason we call these automata deterministic. We now complicate
matters by giving some automata choices in some situations where more
than one transition is possible. We will call such automata nondeterministic.
Nondeterminism is, at ﬁrst sight, an unusual idea. Computers are de-
terministic machines, and the element of choice seems out of place. Never-
theless, nondeterminism is a useful concept, as we will see.
Definition of a Nondeterministic Accepter
Nondeterminism means a choice of moves for an automaton. Rather than
prescribing a unique move in each situation, we allow a set of possible moves.
Formally, we achieve this by deﬁning the transition function so that its range
is a set of possible states.
DEFINITION 2.4
A nondeterministic ﬁnite accepter or nfa is deﬁned by the quintuple
M = (Q, Σ, δ, q0, F) ,
where Q, Σ, q0, F are deﬁned as for deterministic ﬁnite accepters, but
δ : Q × (Σ ∪{λ}) →2Q.
Note that there are three major diﬀerences between this deﬁnition and
the deﬁnition of a dfa. In a nondeterministic accepter, the range of δ is in
the powerset 2Q, so that its value is not a single element of Q, but a subset
of it. This subset deﬁnes the set of all possible states that can be reached
by the transition. If, for instance, the current state is q1, the symbol a is
read, and
δ (q1, a) = {q0, q2} ,

52
Chapter 2 Finite Automata
then either q0 or q2 could be the next state of the nfa. Also, we allow λ
as the second argument of δ. This means that the nfa can make a tran-
sition without consuming an input symbol. Although we still assume that
the input mechanism can only travel to the right, it is possible that it is
stationary on some moves. Finally, in an nfa, the set δ (qi, a) may be empty,
meaning that there is no transition deﬁned for this speciﬁc situation.
Like dfa's, nondeterministic accepters can be represented by transition
graphs. The vertices are determined by Q, while an edge (qi, qj) with label
a is in the graph if and only if δ (qi, a) contains qj. Note that since a may
be the empty string, there can be some edges labeled λ.
A string is accepted by an nfa if there is some sequence of possible moves
that will put the machine in a ﬁnal state at the end of the string. A string
is rejected (that is, not accepted) only if there is no possible sequence of
moves by which a ﬁnal state can be reached. Nondeterminism can therefore
be viewed as involving "intuitive" insight by which the best move can be
chosen at every state (assuming that the nfa wants to accept every string).
EXAMPLE 2.7
Consider the transition graph in Figure 2.8.
It describes a nonde-
terministic accepter since there are two transitions labeled a out
of q0.
a
a
a
a
a
a
q4
q0
q1
q2
q5
q3
FIGURE 2.8
EXAMPLE 2.8
A nondeterministic automaton is shown in Figure 2.9.
It is non-
deterministic not only because several edges with the same label
originate from one vertex, but also because it has a λ-transition. Some

2.2 Nondeterministic Finite Accepters
53
transitions, such as δ (q2, 0), are unspeciﬁed in the graph. This is to
be interpreted as a transition to the empty set, that is, δ (q2, 0) = ∅.
The automaton accepts strings λ, 1010, and 101010, but not 110 and
10100. Note that for 10 there are two alternative walks, one leading
to q0, the other to q2. Even though q2 is not a ﬁnal state, the string is
accepted because one walk leads to a ﬁnal state.
1
0
q1
q2
q0
λ
0, 1
FIGURE 2.9
Again, the transition function can be extended so its second argument
is a string. We require of the extended transition function δ∗that if
δ∗(qi, w) = Qj,
then Qj is the set of all possible states the automaton may be in, having
started in state qi and having read w. A recursive deﬁnition of δ∗, analogous
to (2.1) and (2.2), is possible, but not particularly enlightening. A more
easily appreciated deﬁnition can be made through transition graphs.
DEFINITION 2.5
For an nfa, the extended transition function is deﬁned so that δ∗(qi, w)
contains qj if and only if there is a walk in the transition graph from qi to
qj labeled w. This holds for all qi, qj ∈Q, and w ∈Σ∗.
EXAMPLE 2.9
Figure 2.10 represents an nfa. It has several λ-transitions and some
undeﬁned transitions such as δ (q2, a).
Suppose we want to ﬁnd δ∗(q1, a) and δ∗(q2, λ).
There is a
walk labeled a involving two λ-transitions from q1 to itself. By using
some of the λ-edges twice, we see that there are also walks involving
λ-transitions to q0 and q2. Thus,
δ∗(q1, a) = {q0, q1, q2} .

54
Chapter 2 Finite Automata
Since there is a λ-edge between q2 and q0, we have immediately that
δ∗(q2, λ) contains q0. Also, since any state can be reached from itself
by making no move, and consequently using no input symbol, δ∗(q2, λ)
also contains q2. Therefore,
δ∗(q2, λ) = {q0, q2} .
Using as many λ-transitions as needed, you can also check that
δ∗(q2, aa) = {q0, q1, q2} .
a
q1
q2
q0
λ
λ
FIGURE 2.10
The deﬁnition of δ∗through labeled walks is somewhat informal, so it
is useful to look at it a little more closely. Deﬁnition 2.5 is proper, since
between any vertices vi and vj there is either a walk labeled w or there
is not, indicating that δ∗is completely deﬁned. What is perhaps a little
harder to see is that this deﬁnition can always be used to ﬁnd δ∗(qi, w).
In Section 1.1, we described an algorithm for ﬁnding all simple paths
between two vertices. We cannot use this algorithm directly since, as Ex-
ample 2.9 shows, a labeled walk is not always a simple path. We can modify
the simple path algorithm, removing the restriction that no vertex or edge
can be repeated. The new algorithm will now generate successively all walks
of length one, length two, length three, and so on.
There is still a diﬃculty. Given a w, how long can a walk labeled w
be? This is not immediately obvious. In Example 2.9, the walk labeled
a between q1 and q2 has length four.
The problem is caused by the λ-
transitions, which lengthen the walk but do not contribute to the label.
The situation is saved by this observation: If between two vertices vi and
vj there is any walk labeled w, then there must be some walk labeled w
of length no more than Λ + (1 + Λ) |w|, where Λ is the number of λ-edges
in the graph. The argument for this is: While λ-edges may be repeated,
there is always a walk in which every repeated λ-edge is separated by an
edge labeled with a nonempty symbol. Otherwise, the walk contains a cycle
labeled λ, which can be replaced by a simple path without changing the
label of the walk. We leave a formal proof of this claim as an exercise.
With this observation, we have a method for computing δ∗(qi, w). We
evaluate all walks of length at most Λ + (1 + Λ) |w| originating at qi. We
select from them those that are labeled w. The terminating vertices of the
selected walks are the elements of the set δ∗(qi, w).

2.2 Nondeterministic Finite Accepters
55
As we have remarked, it is possible to deﬁne δ∗in a recursive fashion
as was done for the deterministic case. The result is unfortunately not very
transparent, and arguments with the extended transition function deﬁned
this way are hard to follow. We prefer to use the more intuitive and more
manageable alternative in Deﬁnition 2.5.
As for dfa's, the language accepted by an nfa is deﬁned formally by the
extended transition function.
DEFINITION 2.6
The language L accepted by an nfa M = (Q, Σ, δ, q0, F) is deﬁned as the
set of all strings accepted in the above sense. Formally,
L (M) = {w ∈Σ∗: δ∗(q0, w) ∩F ̸= ∅} .
In words, the language consists of all strings w for which there is a walk
labeled w from the initial vertex of the transition graph to some ﬁnal vertex.
EXAMPLE 2.10
What is the language accepted by the automaton in Figure 2.9? It
is easy to see from the graph that the only way the nfa can stop in
a ﬁnal state is if the input is either a repetition of the string 10 or
the empty string.
Therefore, the automaton accepts the language
L = {(10)n : n ≥0}.
What happens when this automaton is presented with the string
w = 110? After reading the preﬁx 11, the automaton ﬁnds itself in state
q2, with the transition δ (q2, 0) undeﬁned. We call such a situation a
dead conﬁguration, and we can visualize it as the automaton simply
stopping without further action. But we must always keep in mind that
such visualizations are imprecise and carry with them some danger of
misinterpretation. What we can say precisely is that
δ∗(q0, 110) = ∅.
Thus, no ﬁnal state can be reached by processing w = 110, and hence
the string is not accepted.
Why Nondeterminism?
In reasoning about nondeterministic machines, we should be quite cautious
in using intuitive notions. Intuition can easily lead us astray, and we must

56
Chapter 2 Finite Automata
be able to give precise arguments to substantiate our conclusions. Nonde-
terminism is a diﬃcult concept. Digital computers are completely determin-
istic; their state at any time is uniquely predictable from the input and the
initial state. Thus it is natural to ask why we study nondeterministic ma-
chines at all. We are trying to model real systems, so why include such non-
mechanical features as choice? We can answer this question in various ways.
Many deterministic algorithms require that one make a choice at some
stage. A typical example is a game-playing program. Frequently, the best
move is not known, but can be found using an exhaustive search with back-
tracking. When several alternatives are possible, we choose one and follow
it until it becomes clear whether or not it was best. If not, we retreat to
the last decision point and explore the other choices. A nondeterministic
algorithm that can make the best choice would be able to solve the problem
without backtracking, but a deterministic one can simulate nondeterminism
with some extra work. For this reason, nondeterministic machines can serve
as models of search-and-backtrack algorithms.
Nondeterminism is sometimes helpful in solving problems easily. Look
at the nfa in Figure 2.8. It is clear that there is a choice to be made. The
ﬁrst alternative leads to the acceptance of the string a3, while the second
accepts all strings with an even number of a's. The language accepted by
the nfa is

a3
∪

a2n : n ≥1

. While it is possible to ﬁnd a dfa for this
language, the nondeterminism is quite natural. The language is the union of
two quite diﬀerent sets, and the nondeterminism lets us decide at the outset
which case we want. The deterministic solution is not as obviously related
to the deﬁnition, and so is a little harder to ﬁnd. As we go on, we will see
other and more convincing examples of the usefulness of nondeterminism.
In the same vein, nondeterminism is an eﬀective mechanism for describ-
ing some complicated languages concisely. Notice that the deﬁnition of a
grammar involves a nondeterministic element. In
S →aSb|λ
we can at any point choose either the ﬁrst or the second production. This
lets us specify many diﬀerent strings using only two rules.
Finally, there is a technical reason for introducing nondeterminism. As
we will see, certain theoretical results are more easily established for nfa's
than for dfa's.
Our next major result indicates that there is no essen-
tial diﬀerence between these two types of automata. Consequently, allow-
ing nondeterminism often simpliﬁes formal arguments without aﬀecting the
generality of the conclusion.
EXERCISES
1. Construct an nfa that accepts all integer numbers in C. Explain why your
construct is an nfa.

2.2 Nondeterministic Finite Accepters
57
2. Prove in detail the claim made in the previous section that if in a transition
graph there is a walk labeled w, there must be some walk labeled w of length
no more than Λ + (1 + Λ) |w|.
3. Find a dfa that accepts the language deﬁned by the nfa in Figure 2.8.
4. Find a dfa that accepts the complement of the language deﬁned by the nfa
in Figure 2.8.
5. In Figure 2.9, ﬁnd δ∗(q0, 1011) and δ∗(q1, 01).
6. In Figure 2.10, ﬁnd δ∗(q0, a) and δ∗(q1, λ).
7. For the nfa in Figure 2.9, ﬁnd δ∗(q0, 1010) and δ∗(q1, 00).
8. Design an nfa with no more than ﬁve states for the set {ababn : n ≥0} ∪
{aban : n ≥0}.
9. Construct an nfa with three states that accepts the language {ab, abc}∗.
10. Do you think Exercise 9 can be solved with fewer than three states?
11.
(a) Find an nfa with three states that accepts the language
L = {an : n ≥1} ∪

bmak : m ≥0, k ≥0

.
(b) Do you think the language in part (a) can be accepted by an nfa
with fewer than three states?
12. Find an nfa with four states for L = {an : n ≥0} ∪{bna : n ≥1}.
13. Which of the strings 00, 01001, 10010, 000, 0000 are accepted by the following
nfa?
q0
0,1
0
0, λ
1
1
q1
q2
14. What is the complement of the language accepted by the nfa in Figure 2.10?
15. Let L be the language accepted by the nfa in Figure 2.8. Find an nfa that
accepts L ∪

a5
.
16. Find an nfa for L∗, where L is the language in Exercise 15.
17. Find an nfa that accepts {a}∗and is such that if in its transition graph a
single edge is removed (without any other changes), the resulting automaton
accepts {a}.
18. Can Exercise 17 be solved using a dfa? If so, give the solution; if not, give
convincing arguments for your conclusion.
19. Consider the following modiﬁcation of Deﬁnition 2.6. An nfa with multiple
initial states is deﬁned by the quintuple
M = (Q, Σ, δ, Q0, F) ,

58
Chapter 2 Finite Automata
where Q0 ⊆Q is a set of possible initial states. The language accepted by
such an automaton is deﬁned as
L (M) = {w : δ∗(q0, w) contains qf, for any q0 ∈Q0, qf ∈F} .
Show that for every nfa with multiple initial states there exists an nfa with a
single initial state that accepts the same language.
20. Suppose that in Exercise 19 we made the restriction Q0 ∩F = ⊘. Would this
aﬀect the conclusion?
21. Use Deﬁnition 2.5 to show that for any nfa
δ∗(q, wv) =
	
p∈δ∗(q,w)
δ∗(p, v) ,
for all q ∈Q and all w, v ∈Σ∗.
22. An nfa in which (a) there are no λ-transitions, and (b) for all q ∈Q and
all a ∈Σ, δ (q, a), it contains at most one element, is sometimes called an
incomplete dfa. This is reasonable because the conditions make it such that
there is never any choice of moves.
For Σ = {a, b}, convert the incomplete dfa below into a standard dfa.
a
a
b
23. Let L be a regular language on some alphabet Σ, and let Σ1 ⊂Σ be a smaller
alphabet. Consider L1, the subset of L whose elements are made up only of
symbols from Σ1, that is,
L1 = L ∩Σ∗
1.
Show that L1 is also regular.
2.3
EQUIVALENCE OF DETERMINISTIC AND
NONDETERMINISTIC FINITE ACCEPTERS
We now come to a fundamental question. In what sense are dfa's and nfa's
diﬀerent? Obviously, there is a diﬀerence in their deﬁnition, but this does
not imply that there is any essential distinction between them. To explore
this question, we introduce the concept of equivalence between automata.
DEFINITION 2.7
Two ﬁnite accepters, M1 and M2, are said to be equivalent if
L (M1) = L (M2) ,
that is, if they both accept the same language.

2.3 Deterministic and Nondeterministic Finite Accepters
59
As mentioned, there are generally many accepters for a given language,
so any dfa or nfa has many equivalent accepters.
EXAMPLE 2.11
The dfa shown in Figure 2.11 is equivalent to the nfa in Figure 2.9
since they both accept the language {(10)n : n ≥0}.
0, 1
0
0
1
1
q0
q1
q2
FIGURE 2.11
When we compare diﬀerent classes of automata, the question invariably
arises whether one class is more powerful than the other. By "more pow-
erful" we mean that an automaton of one kind can achieve something that
cannot be done by any automaton of the other kind. Let us look at this
question for ﬁnite accepters. Since a dfa is in essence a restricted kind of
nfa, it is clear that any language that is accepted by a dfa is also accepted
by some nfa. But the converse is not so obvious. We have added nonde-
terminism, so it is at least conceivable that there is a language accepted by
some nfa for which, in principle, we cannot ﬁnd a dfa. But it turns out that
this is not so. The classes of dfa's and nfa's are equally powerful: For every
language accepted by some nfa there is a dfa that accepts the same language.
This result is not obvious and certainly has to be demonstrated. The ar-
gument, like most arguments in this book, will be constructive. This means
that we can actually give a way of converting any nfa into an equivalent
dfa. The construction is not hard to understand; once the idea is clear it
becomes the starting point for a rigorous argument. The rationale for the
construction is the following. After an nfa has read a string w, we may not
know exactly what state it will be in, but we can say that it must be in
one state of a set of possible states, say {qi, qj, ..., qk}. An equivalent dfa
after reading the same string must be in some deﬁnite state. How can we
make these two situations correspond? The answer is a nice trick: Label
the states of the dfa with a set of states in such a way that, after reading
w, the equivalent dfa will be in a single state labeled {qi, qj, ..., qk}. Since
for a set of |Q| states there are exactly 2|Q| subsets, the corresponding dfa
will have a ﬁnite number of states.
Most of the work in this suggested construction lies in the analysis
of the nfa to get the correspondence between possible states and inputs.

60
Chapter 2 Finite Automata
Before getting to the formal description of this, let us illustrate it with a
simple example.
EXAMPLE 2.12
Convert the nfa in Figure 2.12 to an equivalent dfa. The nfa starts
in state q0, so the initial state of the dfa will be labeled {q0}. After
reading an a, the nfa can be in state q1 or, by making a λ-transition,
in state q2. Therefore, the corresponding dfa must have a state labeled
{q1, q2} and a transition
δ ({q0} , a) = {q1, q2} .
In state q0, the nfa has no speciﬁed transition when the input is b;
therefore,
δ ({q0} , b) = ∅.
A state labeled ∅represents an impossible move for the nfa and, there-
fore, means nonacceptance of the string. Consequently, this state in
the dfa must be a nonﬁnal trap state.
q0
q1
q2
a
a
b
λ
FIGURE 2.12
We have now introduced into the dfa the state {q1, q2}, so we need
to ﬁnd the transitions out of this state. Remember that this state of
the dfa corresponds to two possible states of the nfa, so we must refer
back to the nfa. If the nfa is in state q1 and reads an a, it can go to q1.
Furthermore, from q1 the nfa can make a λ-transition to q2. If, for the
same input, the nfa is in state q2, then there is no speciﬁed transition.
Therefore,
δ ({q1, q2} , a) = {q1, q2} .
Similarly,
δ ({q1, q2} , b) = {q0} .
At this point, every state has all transitions deﬁned. The result,
shown in Figure 2.13, is a dfa, equivalent to the nfa with which we
started. The nfa in Figure 2.12 accepts any string for which δ∗(q0, w)
contains q1. For the corresponding dfa to accept every such w, any
state whose label includes q1 must be made a ﬁnal state.

2.3 Deterministic and Nondeterministic Finite Accepters
61
a
b
a
{q0}
{q1, q2}
a, b
b
Ø
FIGURE 2.13
THEOREM 2.2
Let L be the language accepted by a nondeterministic ﬁnite accepter MN =
(QN, Σ, δN, q0, FN). Then there exists a deterministic ﬁnite accepter MD =
(QD, Σ, δD, {q0} , FD) such that
L = L (MD) .
Proof: Given MN, we use the procedure nfa-to-dfa below to construct the
transition graph GD for MD. To understand the construction, remember
that GD has to have certain properties. Every vertex must have exactly
|Σ| outgoing edges, each labeled with a diﬀerent element of Σ. During the
construction, some of the edges may be missing, but the procedure continues
until they are all there.
procedure: nfa-to-dfa
1. Create a graph GD with vertex {q0}. Identify this vertex as the initial
vertex.
2. Repeat the following steps until no more edges are missing.
Take any vertex {qi, qj, ..., qk} of GD that has no outgoing edge for some
a ∈Σ. Compute δ∗
N (qi, a) , δ∗
N (qj, a) , ..., δ∗
N (qk, a). If
δ∗
N (qi, a) ∪δ∗
N (qj, a) ∪... ∪δ∗
N (qk, a) = {ql, qm, ..., qn},
create a vertex for GD labeled {ql, qm, ..., qn} if it does not already exist.
Add to GD an edge from {qi, qj, ..., qk} to {ql, qm, ..., qn} and label it
with a.

62
Chapter 2 Finite Automata
3. Every state of GD whose label contains any qf ∈FN is identiﬁed as a
ﬁnal vertex.
4. If MN accepts λ, the vertex {q0} in GD is also made a ﬁnal vertex.
It is clear that this procedure always terminates. Each pass through the
loop in Step 2 adds an edge to GD. But GD has at most 2|QN| |Σ| edges,
so that the loop eventually stops. To show that the construction also gives
the correct answer, we argue by induction on the length of the input string.
Assume that for every v of length less than or equal to n, the presence
in GN of a walk labeled v from q0 to qi implies that in GD there is a walk
labeled v from {q0} to a state Qi = {..., qi, ...}. Consider now any w = va
and look at a walk in GN labeled w from q0 to ql. There must then be a
walk labeled v from q0 to qi and an edge (or a sequence of edges) labeled
a from qi to ql. By the inductive assumption, in GD there will be a walk
labeled v from {q0} to Qi. But by construction, there will be an edge from
Qi to some state whose label contains ql. Thus, the inductive assumption
holds for all strings of length n + 1. As it is obviously true for n = 1, it is
true for all n. The result then is that whenever δ∗
N (q0, w) contains a ﬁnal
state qf, so does the label of δ∗
D (q0, w). To complete the proof, we reverse
the argument to show that if the label of δ∗
D (q0, w) contains qf, so must
δ∗
N (q0, w).
The arguments in this proof, although correct, are admittedly somewhat
terse, showing only the major steps. We will follow this practice in the rest
of the book, emphasizing the basic ideas in a proof and omitting minor
details, which you may want to ﬁll in yourself.
The construction in the previous proof is tedious but important. Let
us do another example to make sure we understand all the steps.
EXAMPLE 2.13
Convert the nfa in Figure 2.14 into an equivalent deterministic
machine.
Since δN (q0, 0) = {q0, q1}, we introduce the state {q0, q1} in GD
and add an edge labeled 0 between {q0} and {q0, q1}. In the same way,
considering δN (q0, 1) = {q1} gives us the new state {q1} and an edge
labeled 1 between it and {q0}.
There are now a number of missing edges, so we continue, using
the construction of Theorem 2.2. Looking at the state {q0, q1}, we see
that there is no outgoing edge labeled 0, so we compute
δ∗
N (q0, 0) ∪δ∗
N (q1, 0) = {q0, q1, q2} .

2.3 Deterministic and Nondeterministic Finite Accepters
63
q1
q2
q0
1
0
0, 1
0, 1
FIGURE 2.14
{q1, q2}
{q0, q1}
{q0, q1, q2}
1
0
{q0}
0
{q1}
1
FIGURE 2.15
{q1, q2}
{q0, q1}
{q2}
Ø
{q0, q1, q2}
1
0, 1
0, 1
0
0
{q0}
0
{q1}
1
1
0
0, 1
1
FIGURE 2.16

64
Chapter 2 Finite Automata
This gives us the new state {q0, q1, q2} and the transition
δD ({q0, q1} , 0) = {q0, q1, q2} .
Then, using a = 1, i = 0, j = 1, k = 2,
δ∗
N (q0, 1) ∪δ∗
N (q1, 1) ∪δ∗
N (q2, 1) = {q1, q2}
makes it necessary to introduce yet another state {q1, q2}.
At this
point, we have the partially constructed automaton shown in Fig-
ure 2.15. Since there are still some missing edges, we continue until we
obtain the complete solution in Figure 2.16.
One important conclusion we can draw from Theorem 2.2 is that every
language accepted by an nfa is regular.
EXERCISES
1. Use the construction of Theorem 2.2 to convert the nfa in Figure 2.10 to a
dfa. Can you see a simpler answer more directly?
2. Convert the nfa in Exercise 13, Section 2.2, into an equivalent dfa.
3. Convert the nfa deﬁned by
δ(q0, a) = {q0, q1}
δ(q1, b) = {q1, q2}
δ(q2, a) = {q2}
with initial state q0 and ﬁnal state q2 into an equivalent dfa.
4. Convert the nfa deﬁned by
δ(q0, a) = {q0, q1}
δ(q1, b) = {q1, q2}
δ(q2, a) = {q2}
δ(q0, λ) = {q2}
with initial state q0 and ﬁnal state q2 into an equivalent dfa.
5. Convert the nfa deﬁned by
δ(q0, a) = {q0, q1}
δ(q1, b) = {q1, q2}
δ(q2, a) = {q2}
δ(q1, λ) = {q1, q2}
with initial state q0 and ﬁnal state q2 into an equivalent dfa.

2.3 Deterministic and Nondeterministic Finite Accepters
65
6. Carefully complete the arguments in the proof of Theorem 2.2. Show in detail
that if the label of δ∗
D (q0, w) contains qf, then δ∗
N (q0, w) also contains qf.
7. Is it true that for any nfa M = (Q, Σ, δ, q0, F), the complement of L (M) is
equal to the set {w ∈Σ∗: δ∗(q0, w) ∩F = ⊘}? If so, prove it. If not, give a
counterexample.
8. Is it true that for every nfa M = (Q, Σ, δ, q0, F) , the complement of L (M) is
equal to the set {w ∈Σ∗: δ∗(q0, w) ∩(Q −F) ̸= ⊘}? If so, prove it; if not,
give a counterexample.
9. Prove that for every nfa with an arbitrary number of ﬁnal states there is an
equivalent nfa with only one ﬁnal state. Can we make a similar claim for
dfa's?
10. Find an nfa without λ-transitions and with a single ﬁnal state that accepts
the set {a} ∪{bn : n ≥2}.
11. Let L be a regular language that does not contain λ. Show that an nfa exists
without λ-transitions and with a single ﬁnal state that accepts L.
12. Deﬁne a dfa with multiple initial states in an analogous way to the corre-
sponding nfa in Exercise 18, Section 2.2. Does an equivalent dfa with a single
initial state always exist?
13. Prove that all ﬁnite languages are regular.
14. Show that if L is regular, so is LR.
15. Give a simple verbal description of the language accepted by the dfa in Figure
2.16. Use this to ﬁnd another dfa, equivalent to the given one, but with fewer
states.
16. Let L be any language. Deﬁne even (w) as the string obtained by extracting
from w the letters in even-numbered positions; that is, if
w = a1a2a3a4...,
then
even (w) = a2a4....
Corresponding to this, we can deﬁne a language
even (L) = {even (w) : w ∈L} .
Prove that if L is regular, so is even (L) .
17. From a language L, we create a new language, chopleft (L), by removing the
leftmost symbol of every string in L. Speciﬁcally,
chopleft (L) = {w : vw ∈L, with |v| = 1} .
Show that if L is regular, then chopleft (L) is also regular.
18. From a language L, we create a new language chopright (L), by removing the
rightmost symbol of every string in L. Speciﬁcally,
chopright (L) = {w : wv ∈L, with |v| = 1} .
Show that if L is regular, then chopright (L) is also regular.

66
Chapter 2 Finite Automata
2.4
REDUCTION OF THE NUMBER OF STATES IN FINITE
AUTOMATA*
Any dfa deﬁnes a unique language, but the converse is not true. For a given
language, there are many dfa's that accept it. There may be a considerable
diﬀerence in the number of states of such equivalent automata. In terms of
the questions we have considered so far, all solutions are equally satisfactory,
but if the results are to be applied in a practical setting, there may be
reasons for preferring one over another.
EXAMPLE 2.14
The two dfa's depicted in Figure 2.17(a) and 2.17(b) are equivalent,
as a few test strings will quickly reveal.
We notice some obviously
unnecessary features of Figure 2.17(a). The state q5 plays absolutely
no role in the automaton since it can never be reached from the initial
state q0. Such a state is inaccessible, and it can be removed (along
with all transitions relating to it) without aﬀecting the language
accepted by the automaton.
But even after the removal of q5, the
ﬁrst automaton has some redundant parts.
The states reachable
subsequent to the ﬁrst move δ (q0, 0) mirror those reachable from
a ﬁrst move δ (q0, 1).
The second automaton combines these two
options.
q1
q0
q0
q2
1
0, 1
0, 1
q4
1
1
1
1
0, 1
q3
0, 1
q5
0
0
q2
q1
0
0
0
(b)
(a)
FIGURE 2.17

2.4 Reduction of the Number of States in Finite Automata*
67
From a strictly theoretical point of view, there is little reason for prefer-
ring the automaton in Figure 2.17(b) over that in Figure 2.17(a). However,
in terms of simplicity, the second alternative is clearly preferable. Repre-
sentation of an automaton for the purpose of computation requires space
proportional to the number of states. For storage eﬃciency, it is desirable
to reduce the number of states as far as possible.
We now describe an
algorithm that accomplishes this.
DEFINITION 2.8
Two states p and q of a dfa are called indistinguishable if
δ∗(p, w) ∈F implies δ∗(q, w) ∈F,
and
δ∗(p, w) /∈F implies δ∗(q, w) /∈F,
for all w ∈Σ∗. If, on the other hand, there exists some string w ∈Σ∗such
that
δ∗(p, w) ∈F and δ∗(q, w) /∈F,
or vice versa, then the states p and q are said to be distinguishable by a
string w.
Clearly, two states are either indistinguishable or distinguishable. In-
distinguishability has the properties of an equivalence relation: If p and q
are indistinguishable and if q and r are also indistinguishable, then so are
p and r, and all three states are indistinguishable.
One method for reducing the states of a dfa is based on ﬁnding and
combining indistinguishable states. We ﬁrst describe a method for ﬁnding
pairs of distinguishable states.
procedure: mark
1. Remove all inaccessible states. This can be done by enumerating all
simple paths of the graph of the dfa starting at the initial state. Any
state not part of some path is inaccessible.
2. Consider all pairs of states (p, q). If p ∈F and q /∈F or vice versa,
mark the pair (p, q) as distinguishable.
3. Repeat the following step until no previously unmarked pairs are marked.
For all pairs (p, q) and all a ∈Σ, compute δ (p, a) = pa and δ (q, a) = qa.
If the pair (pa, qa) is marked as distinguishable, mark (p, q) as distin-
guishable.

68
Chapter 2 Finite Automata
We claim that this procedure constitutes an algorithm for marking all dis-
tinguishable pairs.
THEOREM 2.3
The procedure mark, applied to any dfa M = (Q, Σ, δ, q0, F), terminates
and determines all pairs of distinguishable states.
Proof: Obviously, the procedure terminates, since there are only a ﬁnite
number of pairs that can be marked. It is also easy to see that the states
of any pair so marked are distinguishable. The only claim that requires
elaboration is that the procedure ﬁnds all distinguishable pairs.
Note ﬁrst that states qi and qj are distinguishable with a string of length
n if and only if there are transitions
δ (qi, a) = qk
(2.5)
and
δ (qj, a) = q l,
(2.6)
for some a ∈Σ, with qk and ql distinguishable by a string of length n −1.
We use this ﬁrst to show that at the completion of the nth pass through the
loop in step 3, all states distinguishable by strings of length n or less have
been marked. In step 2, we mark all pairs indistinguishable by λ, so we have
a basis with n = 0 for an induction. We now assume that the claim is true
for all i = 0, 1, ..., n −1. By this inductive assumption, at the beginning of
the nth pass through the loop, all states distinguishable by strings of length
up to n −1 have been marked. Because of (2.5) and (2.6) above, at the end
of this pass, all states distinguishable by strings of length up to n will be
marked. By induction then, we can claim that, for any n, at the completion
of the nth pass, all pairs distinguishable by strings of length n or less have
been marked.
To show that this procedure marks all distinguishable states, assume
that the loop terminates after n passes. This means that during the nth
pass no new states were marked. From (2.5) and (2.6), it then follows that
there cannot be any states distinguishable by a string of length n, but not
distinguishable by any shorter string. But if there are no states distinguish-
able only by strings of length n, there cannot be any states distinguishable
only by strings of length n+1, and so on. As a consequence, when the loop
terminates, all distinguishable pairs have been marked.
The procedure mark can be implemented by partitioning the states into
equivalence classes. Whenever two states are found to be distinguishable,
they are immediately put into separate equivalence classes.

2.4 Reduction of the Number of States in Finite Automata*
69
EXAMPLE 2.15
Consider the automaton in Figure 2.18.
In the second step of procedure mark we partition the state set
into ﬁnal and nonﬁnal states to get two equivalence classes {q0, q1, q3}
and {q2, q4}. In the next step, when we compute
δ(q0, 0) = q1
and
δ(q1, 0) = q2,
we recognize that q0 and q1 are distinguishable, so we put them into
diﬀerent sets. So {q0, q1, q3} is split into {q0} and {q1, q3}. Also, since
δ(q2, 0) = q3 and δ(q4, 0) = q4, the class {q2, q4} is split into {q2} and
{q4}. The rest of the computations show that no further splitting is
needed.
q1
1
1
1
1
0
0
0
0
0,1
q4
q2
q0
q3
FIGURE 2.18
Once the indistinguishability classes are found, the construction of the
minimal dfa is straightforward.
procedure: reduce
Given a dfa M = (Q, Σ, δ, q0, F) , we construct a reduced dfa 
M =

Q, Σ, δ,
q0, F

as follows.
1. Use procedure mark to generate the equivalence classes, say {qi, qj, ..., qk},
as described.
2. For each set {qi, qj, ..., qk} of such indistinguishable states, create a state
labeled ij · · · k for 
M.

70
Chapter 2 Finite Automata
3. For each transition rule of M of the form
δ (qr, a) = qp,
ﬁnd the sets to which qr and qp belong.
If qr ∈{qi, qj, ..., qk} and
qp ∈{ql, qm, ..., qn}, add to δ a rule
δ (ij · · · k, a) = lm · · · n.
4. The initial state q0 is that state of 
M whose label includes the 0.
5. F is the set of all the states whose label contains i such that qi ∈F.
EXAMPLE 2.16
Continuing with Example 2.15, we create the states in Figure 2.19.
Since, for example,
δ(q1, 0) = q2,
there is an edge labeled 0 from state 13 to state 2. The rest of the
transitions are easily found, giving the minimal dfa in Figure 2.19.
13
0
1
1
0
0
0,1
0,1
4
2
FIGURE 2.19
THEOREM 2.4
Given any dfa M, application of the procedure reduce yields another dfa

M such that
L (M) = L


M

.

2.4 Reduction of the Number of States in Finite Automata*
71
Furthermore, 
M is minimal in the sense that there is no other dfa with a
smaller number of states that also accepts L (M).
Proof: There are two parts. The ﬁrst is to show that the dfa created by
reduce is equivalent to the original dfa. This is relatively easy and we can
use inductive arguments similar to those used in establishing the equivalence
of dfa's and nfa's. All we have to do is to show that δ∗(qi, w) = qj if and
only if the label of δ∗(qi, w) is of the form ...j.... We will leave this as an
exercise.
The second part, to show that 
M is minimal, is harder. Suppose 
M has
states {p0, p1, p2, ..., pm}, with p0 the initial state. Assume that there is an
equivalent dfa M1, with transition function δ1 and initial state q0, equivalent
to 
M, but with fewer states. Since there are no inaccessible states in 
M,
there must be distinct strings w1, w2, ..., wm such that
δ∗(p0, wi) = pi, i = 1, 2, ..., m.
But since M1 has fewer states than 
M, there must be at least two of these
strings, say wk and wl, such that
δ∗
1 (q0, wk) = δ∗
1 (q0, wl) .
Since pk and pl are distinguishable, there must be some string x such that
δ∗(p0, wkx) = δ∗(pk, x) is a ﬁnal state, and δ∗(q0, wlx) = δ∗(pl, x) is a
nonﬁnal state (or vice versa). In other words, wkx is accepted by 
M and
wlx is not. But note that
δ∗
1 (q0, wkx) = δ∗
1 (δ∗
1 (q0, wk) , x)
= δ∗
1 (δ∗
1 (q0, wl) , x)
= δ∗
1 (q0, wlx) .
Thus, M1 either accepts both wkx and wlx or rejects both, contradicting
the assumption that 
M and M1 are equivalent. This contradiction proves
that M1 cannot exist.
EXERCISES
1. Consider the dfa with initial state q0, ﬁnal state q2 and
δ(q0, a) = q2
δ(q0, b) = q2
δ(q1, a) = q2
δ(q1, b) = q2
δ(q2, a) = q3
δ(q2, b) = q3
δ(q3, a) = q3
δ(q3, b) = q1
Find a minimal equivalent dfa.

72
Chapter 2 Finite Automata
2. Minimize the number of states in the dfa in Figure 2.16.
3. Find minimal dfa's for the following languages. In each case, prove that the
result is minimal.
(a) L = {anbm : n ≥1, m ≥2}.
(b) L = {anb : n ≥1} ∪{bna : n ≥1}.
(c) L = {an : n ≥0, n ̸= 2}.
(d) L = {an : n ̸= 3 and n ̸= 4}.
(e) L = {an : n mod 3 = 1} ∪{an : n mod 5 = 1}.
4. Show that the automaton generated by procedure reduce is deterministic.
5. Show that if L is a nonempty language such that any w in L has length at
least n, then any dfa accepting L must have at least n + 1 states.
6. Prove or disprove the following conjecture. If M = (Q, Σ, δ, q0, F) is a minimal
dfa for a regular language L, then 
M = (Q, Σ, δ, q0, Q −F) is a minimal dfa
for L.
7. Show that indistinguishability is an equivalence relation but that distinguisha-
bility is not.
8. Show the explicit steps of the suggested proof of the ﬁrst part of Theorem
2.4, namely, that 
M is equivalent to the original dfa.
9. Prove the following: If the states qa and qb are indistinguishable, and if qa
and qc are distinguishable, then qb and qc must be distinguishable.

3
CHA P T E R
REGULAR LANGUAGES
AND REGULAR GRAMMARS
CHAPTER SUMMARY
In this chapter, we explore two alternative methods for describing reg-
ular languages: regular expressions and regular grammars.
Regular expressions, whose form is reminiscent of the familiar arith-
metic expressions, are convenient in some applications because of their
simple string form, but they are restricted and have no obvious exten-
sion to the more complicated languages we will encounter later. Regular
grammars, on the other hand, are just a special case of many different
types of grammars.
The purpose of this chapter is to explore the essential equivalence of
these three modes of describing regular languages. The constructions
that make conversion from one form to another are in the theorems in
this chapter. Their relation is illustrated in Figure 3.19.
Since each representation of a regular language is fully convertible
to any of the others, we can choose whichever is most convenient for
the situation at hand. Remembering this will often simplify your work.
A
ccording to our deﬁnition, a language is regular if there exists a ﬁnite
accepter for it. Therefore, every regular language can be described
by some dfa or some nfa. Such a description can be very useful, for
example, if we want to show the logic by which we decide if a given string
is in a certain language. But in many instances, we need more concise ways
73

74
Chapter 3 Regular Languages and Regular Grammars
of describing regular languages.
In this chapter, we look at other ways
of representing regular languages. These representations have important
practical applications, a matter that is touched on in some of the examples
and exercises.
3.1
REGULAR EXPRESSIONS
One way of describing regular languages is via the notation of regular
expressions. This notation involves a combination of strings of symbols
from some alphabet Σ, parentheses, and the operators +, ·, and ∗. The
simplest case is the language {a}, which will be denoted by the regular
expression a. Slightly more complicated is the language {a, b, c}, for which,
using the + to denote union, we have the regular expression a + b + c.
We use · for concatenation and ∗for star-closure in a similar way. The
expression (a + (b · c))∗stands for the star-closure of {a}∪{bc}, that is, the
language {λ, a, bc, aa, abc, bca, bcbc, aaa, aabc, ...}.
Formal Definition of a Regular Expression
We construct regular expressions from primitive constituents by repeatedly
applying certain recursive rules. This is similar to the way we construct
familiar arithmetic expressions.
DEFINITION 3.1
Let Σ be a given alphabet. Then
1. ∅, λ, and a ∈Σ are all regular expressions. These are called primitive
regular expressions.
2. If r1 and r2 are regular expressions, so are r1 + r2, r1 · r2, r∗
1, and (r1).
3. A string is a regular expression if and only if it can be derived from the
primitive regular expressions by a ﬁnite number of applications of the
rules in (2).
EXAMPLE 3.1
For Σ = {a, b, c}, the string
(a + b · c)∗· (c + ∅)
is a regular expression, since it is constructed by application of the
above rules. For example, if we take r1 = c and r2 = ∅, we ﬁnd that

3.1 Regular Expressions
75
c + ∅and (c + ∅) are also regular expressions.
Repeating this, we
eventually generate the whole string. On the other hand, (a + b +) is
not a regular expression, since there is no way it can be constructed
from the primitive regular expressions.
Languages Associated with Regular Expressions
Regular expressions can be used to describe some simple languages. If r is a
regular expression, we will let L (r) denote the language associated with r.
DEFINITION 3.2
The language L (r) denoted by any regular expression r is deﬁned by the
following rules.
1. ∅is a regular expression denoting the empty set,
2. λ is a regular expression denoting {λ},
3. For every a ∈Σ, a is a regular expression denoting {a}.
If r1 and r2 are regular expressions, then
4. L (r1 + r2) = L (r1) ∪L (r2),
5. L (r1 · r2) = L (r1) L (r2) ,
6. L ((r1)) = L (r1),
7. L (r∗
1) = (L (r1))∗.
The last four rules of this deﬁnition are used to reduce L (r) to simpler
components recursively; the ﬁrst three are the termination conditions for
this recursion. To see what language a given expression denotes, we apply
these rules repeatedly.
EXAMPLE 3.2
Exhibit the language L (a∗· (a + b)) in set notation.
L (a∗· (a + b)) = L (a∗) L (a + b)
= (L (a))∗(L (a) ∪L (b))
= {λ, a, aa, aaa, ...} {a, b}
= {a, aa, aaa, ..., b, ab, aab, ...} .

76
Chapter 3 Regular Languages and Regular Grammars
There is one problem with rules (4) to (7) in Deﬁnition 3.2. They deﬁne
a language precisely if r1 and r2 are given, but there may be some ambigu-
ity in breaking a complicated expression into parts. Consider, for example,
the regular expression a · b + c. We can consider this as being made up of
r1 = a · b and r2 = c. In this case, we ﬁnd L (a · b + c) = {ab, c}. But there
is nothing in Deﬁnition 3.2 to stop us from taking r1 = a and r2 = b + c.
We now get a diﬀerent result, L (a · b + c) = {ab, ac}. To overcome this,
we could require that all expressions be fully parenthesized, but this gives
cumbersome results. Instead, we use a convention familiar from mathemat-
ics and programming languages. We establish a set of precedence rules for
evaluation in which star-closure precedes concatenation and concatenation
precedes union. Also, the symbol for concatenation may be omitted, so we
can write r1r2 for r1 · r2.
With a little practice, we can see quickly what language a particular
regular expression denotes.
EXAMPLE 3.3
For Σ = {a, b}, the expression
r = (a + b)∗(a + bb)
is regular. It denotes the language
L (r) = {a, bb, aa, abb, ba, bbb, ...} .
We can see this by considering the various parts of r. The ﬁrst part,
(a + b)∗, stands for any string of a's and b's. The second part, (a + bb)
represents either an a or a double b. Consequently, L (r) is the set of
all strings on {a, b}, terminated by either an a or a bb.
EXAMPLE 3.4
The expression
r = (aa)∗(bb)∗b
denotes the set of all strings with an even number of a's followed by
an odd number of b's; that is,
L (r) =

a2nb2m+1 : n ≥0, m ≥0

.
Going from an informal description or set notation to a regular expres-
sion tends to be a little harder.

3.1 Regular Expressions
77
EXAMPLE 3.5
For Σ = {0, 1}, give a regular expression r such that
L (r) = {w ∈Σ∗: w has at least one pair of consecutive zeros} .
One can arrive at an answer by reasoning something like this: Every
string in L (r) must contain 00 somewhere, but what comes before and
what goes after is completely arbitrary. An arbitrary string on {0, 1}
can be denoted by (0 + 1)∗. Putting these observations together, we
arrive at the solution
r = (0 + 1)∗00 (0 + 1)∗.
EXAMPLE 3.6
Find a regular expression for the language
L =

w ∈{0, 1}∗: w has no pair of consecutive zeros

.
Even though this looks similar to Example 3.5, the answer is harder to
construct. One helpful observation is that whenever a 0 occurs, it must
be followed immediately by a 1. Such a substring may be preceded and
followed by an arbitrary number of 1's. This suggests that the answer
involves the repetition of strings of the form 1 · · · 101 · · · 1, that is, the
language denoted by the regular expression (1∗011∗)∗. However, the
answer is still incomplete, since the strings ending in 0 or consisting of
all 1's are unaccounted for. After taking care of these special cases we
arrive at the answer
r = (1∗011∗)∗(0 + λ) + 1∗(0 + λ) .
If we reason slightly diﬀerently, we might come up with another
answer. If we see L as the repetition of the strings 1 and 01, the shorter
expression
r = (1 + 01)∗(0 + λ)
might be reached. Although the two expressions look diﬀerent, both
answers are correct, as they denote the same language. Generally, there
are an unlimited number of regular expressions for any given language.
Note that this language is the complement of the language in Ex-
ample 3.5. However, the regular expressions are not very similar and
do not suggest clearly the close relationship between the languages.

78
Chapter 3 Regular Languages and Regular Grammars
The last example introduces the notion of equivalence of regular ex-
pressions. We say the two regular expressions are equivalent if they denote
the same language. One can derive a variety of rules for simplifying regular
expressions (see Exercise 20 in the following exercise section), but since we
have little need for such manipulations we will not pursue this.
EXERCISES
1. Find all strings in L ((a + bb)∗) of length ﬁve.
2. Find all strings in L ((ab + b)∗b (a + ab)∗) of length less than four.
3. Find an nfa that accepts the language L (aa∗(ab + b)).
4. Find an nfa that accepts the language L (aa∗(a + b)).
5. Does the expression ((0 + 1) (0 + 1)∗)∗00 (0 + 1)∗denote the language in
Example 3.5?
6. Show that r = (1 + 01)∗(0 + 1∗) also denotes the language in Example 3.6.
Find two other equivalent expressions.
7. Find a regular expression for the set {anbm : n ≥3, m is odd}.
8. Find a regular expression for the set {anbm : (n + m) is odd}.
9. Give regular expressions for the following languages.
(a) L1 = {anbm, n ≥3, m ≤4}.
(b) L2 = {anbm : n < 4, m ≤4}.
(c) The complement of L1.
(d) The complement of L2.
10. What languages do the expressions (⊘∗)∗and a⊘denote?
11. Give a simple verbal description of the language L ((aa)∗b (aa)∗+
a (aa)∗ba (aa)∗).
12. Give a regular expression for LR, where L is the language in Exercise 2.
13. Give a regular expression for L = {anbm : n ≥2, m ≥1, nm ≥3}.
14. Find a regular expression for L =

abnw : n ≥4, w ∈{a, b}+
.
15. Find a regular expression for the complement of the language in Example 3.4.
16. Find a regular expression for L = {vwv : v, w ∈{a, b}∗, |v| = 2}.
17. Find a regular expression for L = {vwv : v, w ∈{a, b}∗, |v| ≤4}.
18. Find a regular expression for
L = {w ∈{0, 1}∗: w has exactly one pair of consecutive zeros} .
19. Give regular expressions for the following languages on Σ = {a, b, c}:
(a) All strings containing exactly two a's.

3.1 Regular Expressions
79
(b) All strings containing no more than three a's.
(c) All strings that contain at least one occurrence of each symbol
in Σ.
20. Write regular expressions for the following languages on {0, 1}:
(a) All strings ending in 10.
(b) All strings not ending in 10.
(c) All strings containing an odd number of 0's.
21. Find regular expressions for the following languages on {a, b}:
(a) L = {w : |w| mod 3 ̸= 0}.
(b) L = {w : na (w) mod 3 = 0}.
(c) L = {w : na (w) mod 5 > 0}.
22. Determine whether or not the following claims are true for all regular expres-
sions r1 and r2. The symbol ≡stands for equivalence of regular expressions
in the sense that both expressions denote the same language.
(a) (r∗
1)∗≡r∗
1
(b) r∗
1 (r1 + r2)∗≡(r1 + r2)∗
(c) (r1 + r2)∗≡(r∗
1r∗
2)∗
(d) (r1r2)∗≡r∗
1r∗
2
23. Give a general method by which any regular expression r can be changed into
r such that (L (r))R = L (r).
24. Prove rigorously that the expressions in Example 3.6 do indeed denote the
speciﬁed language.
25. For the case of a regular expression r that does not involve λ or ⊘, give a
set of necessary and suﬃcient conditions that r must satisfy if L (r) is to be
inﬁnite.
26. Formal languages can be used to describe a variety of two-dimensional ﬁgures.
Chain-code languages are deﬁned on the alphabet Σ = {u, d, r, l}, where these
symbols stand for unit-length straight lines in the directions up, down, right,
and left, respectively. An example of this notation is urdl, which stands for
the square with sides of unit length. Draw pictures of the ﬁgures denoted by
the expressions (rd)∗, (urddru)∗, and (ruldr)∗.
27. In Exercise 27, what are suﬃcient conditions on the expression so that the
picture is a closed contour in the sense that the beginning and ending points
are the same? Are these conditions also necessary?
28. Find a regular expression that denotes all bit strings whose value, when in-
terpreted as a binary integer, is greater than or equal to 40.
29. Find a regular expression for all bit strings, with leading bit 1, interpreted as
a binary integer, with values not between 10 and 30.

80
Chapter 3 Regular Languages and Regular Grammars
3.2
CONNECTION BETWEEN REGULAR EXPRESSIONS
AND REGULAR LANGUAGES
As the terminology suggests, the connection between regular languages and
regular expressions is a close one.
The two concepts are essentially the
same; for every regular language there is a regular expression, and for every
regular expression there is a regular language. We will show this in two
parts.
Regular Expressions Denote Regular Languages
We ﬁrst show that if r is a regular expression, then L (r) is a regular lan-
guage. Our deﬁnition says that a language is regular if it is accepted by some
dfa. Because of the equivalence of nfa's and dfa's, a language is also regular
if it is accepted by some nfa. We now show that if we have any regular
expression r, we can construct an nfa that accepts L (r). The construction
for this relies on the recursive deﬁnition for L (r). We ﬁrst construct simple
automata for parts (1), (2), and (3) of Deﬁnition 3.2, then show how they
can be combined to implement the more complicated parts (4), (5), and (7).
THEOREM 3.1
Let r be a regular expression.
Then there exists some nondeterministic
ﬁnite accepter that accepts L (r). Consequently, L (r) is a regular language.
Proof: We begin with automata that accept the languages for the simple
regular expressions ∅, λ, and a ∈Σ. These are shown in Figure 3.1(a),
(b), and (c), respectively. Assume now that we have automata M (r1) and
M (r2) that accept languages denoted by regular expressions r1 and r2,
respectively.
We need not explicitly construct these automata, but may
represent them schematically, as in Figure 3.2. In this scheme, the graph
(a)
(b)
(c)
a
λ
q0
q1
q0
q1
q0
q1
FIGURE 3.1 (a) nfa accepts ∅. (b) nfa accepts {λ}. (c) nfa accepts {a}.
M(r)
FIGURE 3.2 Schematic representation of an nfa accepting L (r).

3.2 Regular Expressions and Regular Languages
81
vertex at the left represents the initial state, the one on the right the ﬁ-
nal state. In Exercise 7, Section 2.3, we claim that for every nfa there is an
equivalent one with a single ﬁnal state, so we lose nothing in assuming that
there is only one ﬁnal state. With M (r1) and M (r2) represented in this
way, we then construct automata for the regular expressions r1 + r2, r1r2,
and r∗
1. The constructions are shown in Figures 3.3 to 3.5. As indicated
in the drawings, the initial and ﬁnal states of the constituent machines lose
their status and are replaced by new initial and ﬁnal states. By stringing
together several such steps, we can build automata for arbitrary complex
regular expressions.
It should be clear from the interpretation of the graphs in Figures 3.3
to 3.5 that this construction works. To argue more rigorously, we can give a
formal method for constructing the states and transitions of the combined
machine from the states and transitions of the parts, then prove by induction
M(r1)
M(r2)
λ
λ
λ
λ
FIGURE 3.3 Automaton for L (r1 + r2).
M(r1)
M(r2)
λ
λ
λ
FIGURE 3.4 Automaton for L (r1r2).
M(r1)
λ
λ
λ
λ
FIGURE 3.5 Automaton for L (r∗
1).

82
Chapter 3 Regular Languages and Regular Grammars
on the number of operators that the construction yields an automaton that
accepts the language denoted by any particular regular expression. We will
not belabor this point, as it is reasonably obvious that the results are always
correct.
EXAMPLE 3.7
Find an nfa that accepts L (r), where
r = (a + bb)∗(ba∗+ λ) .
Automata for (a + bb) and (ba∗+ λ), constructed directly from ﬁrst
principles, are given in Figure 3.6. Putting these together using the
construction in Theorem 3.1, we get the solution in Figure 3.7.
M1
λ
λ
a
b
b
(a)
M2
b
(b)
a
FIGURE 3.6
(a) M1 accepts L (a + bb). (b) M2 accepts L (ba∗+ λ).
M1
λ
λ
λ
λ
a
b
b
M2
λ
λ
λ
b
a
λ
FIGURE 3.7
Automaton accepts L ((a + bb)∗(ba∗+ λ)).
Regular Expressions for Regular Languages
It is intuitively reasonable that the converse of Theorem 3.1 should hold, and
that for every regular language, there should exist a corresponding regular
expression. Since any regular language has an associated nfa and hence a
transition graph, all we need to do is to ﬁnd a regular expression capable of
generating the labels of all the walks from q0 to any ﬁnal state. This does
not look too diﬃcult but it is complicated by the existence of cycles that

3.2 Regular Expressions and Regular Languages
83
can often be traversed arbitrarily, in any order. This creates a bookkeeping
problem that must be handled carefully. There are several ways to do this;
one of the more intuitive approaches requires a side trip into what are called
generalized transition graphs (GTG). Since this idea is used here in a
limited way and plays no role in our further discussion, we will deal with it
informally.
A generalized transition graph is a transition graph whose edges are
labeled with regular expressions; otherwise it is the same as the usual tran-
sition graph. The label of any walk from the initial state to a ﬁnal state is
the concatenation of several regular expressions, and hence itself a regular
expression. The strings denoted by such regular expressions are a subset
of the language accepted by the generalized transition graph, with the full
language being the union of all such generated subsets.
EXAMPLE 3.8
Figure 3.8 represents a generalized transition graph.
The language
accepted by it is L (a∗+ a∗(a + b) c∗), as should be clear from an
inspection of the graph. The edge (q0, q0) labeled a is a cycle that can
generate any number of a's, that is, it represents L (a∗). We could
have labeled this edge a∗without changing the language accepted by
the graph.
a
c*
a + b
q0
q1
FIGURE 3.8
The graph of any nondeterministic ﬁnite accepter can be considered
a generalized transition graph if the edge labels are interpreted properly.
An edge labeled with a single symbol a is interpreted as an edge labeled
with the expression a, while an edge labeled with multiple symbols a, b, ...
is interpreted as an edge labeled with the expression a + b + ....
From
this observation, it follows that for every regular language, there exists a
generalized transition graph that accepts it.
Conversely, every language
accepted by a generalized transition graph is regular. Since the label of
every walk in a generalized transition graph is a regular expression, this
appears to be an immediate consequence of Theorem 3.1. However, there
are some subtleties in the argument; we will not pursue them here, but refer
the reader instead to Exercise 22, Section 4.3, for details.
Equivalence for generalized transition graphs is deﬁned in terms of the
language accepted and the purpose of the next bit of discussion is to produce

84
Chapter 3 Regular Languages and Regular Grammars
a sequence of increasingly simple GTGs. In this, we will ﬁnd it convenient
to work with complete GTGs. A complete GTG is a graph in which all
edges are present. If a GTG, after conversion from an nfa, has some edges
missing, we put them in and label them with ∅. A complete GTG with |V |
vertices has exactly |V |2 edges.
EXAMPLE 3.9
The GTG in Figure 3.9(a) is not complete. Figure 3.9(b) shows how
it is completed.
q2
q0
q1
a
c
e
b
d
q2
q0
q1
a
c
e
b
d
(a)
(b)
FIGURE 3.9
Suppose now that we have the simple two-state complete GTG shown
in Figure 3.10. By mentally tracing through this GTG you can convince
yourself that the regular expression
r = r∗
1r2(r4 + r3r∗
1r2)∗
(3.1)
covers all possible paths and so is the correct regular expression associated
with the graph.
When a GTG has more than two states, we can ﬁnd an equivalent graph
by removing one state at a time. We will illustrate this with an example
before going to the general method.
r4
r1
r3
r2
FIGURE 3.10

3.2 Regular Expressions and Regular Languages
85
EXAMPLE 3.10
Consider the complete GTG in Figure 3.11. To remove q2, we ﬁrst
introduce some new edges. We
create an edge from q1 to q1 and label it e + af ∗b,
create an edge from q1 to q3 and label it h + af ∗c,
create an edge from q3 to q1 and label it i + df ∗b,
create an edge from q3 to q3 and label it g + df ∗c.
When this is done, we remove q2 and all associated edges. This gives
the GTG in Figure 3.12. You can explore the equivalence of the two
GTGs by seeing how regular expressions such as af ∗c and e∗ab are
generated.
q1
q2
a
e
g
c
f
b
h
i
d
q3
FIGURE 3.11
e + af *b
g + df *c
h + af *c
i + df *b
q1
q3
FIGURE 3.12
For arbitrary GTGs we remove one state at a time until only two states
are left. Then we apply Equation (3.1) to get the ﬁnal regular expression.
This tends to be a lengthy process, but it is straightforward as the following
procedure shows.
procedure: nfa-to-rex
1. Start with an nfa with states q0, q1, ..., qn, and a single ﬁnal state, dis-
tinct from its initial state.

86
Chapter 3 Regular Languages and Regular Grammars
2. Convert the nfa into a complete generalized transition graph. Let rij
stand for the label of the edge from qi to qj.
3. If the GTG has only two states, with qi as its initial state and qj its
ﬁnal state, its associated regular expression is
r = r∗
iirij(rjj + rjir∗
iirij)∗.
(3.2)
4. If the GTG has three states, with initial state qi, ﬁnal state qj, and
third state qk, introduce new edges, labeled
rpq + rpkr∗
kkrkq
(3.3)
for p = i, j, q = i, j.
When this is done, remove vertex qk and its
associated edges.
5. If the GTG has four or more states, pick a state qk to be removed.
Apply rule 4 for all pairs of states (qi, qj), i ̸= k, j ̸= k. At each step
apply the simplifying rules
r + ∅= r,
r∅= ∅,
∅∗= λ,
wherever possible. When this is done, remove state qk.
6. Repeat Steps 3 to 5 until the correct regular expression is obtained.
EXAMPLE 3.11
Find a regular expression for the language
L =

w ∈{a, b}∗: na (w) is even and nb (w) is odd

.
An attempt to construct a regular expression directly from this de-
scription leads to all kinds of diﬃculties. On the other hand, ﬁnding
an nfa for it is easy as long as we use vertex labeling eﬀectively. We
label the vertices with EE to denote an even number of a's and b's,
with OE to denote an odd number of a's and an even number of b's,
and so on. With this we easily get the solution that, after conversion
into a complete generalized transition graph, is in Figure 3.13.
We now apply the conversion to a regular expression, using pro-
cedure nfa-to-rex. To remove the state OE, we apply Equation (3.3).
The edge between EE and itself will have the label
rEE = ∅+ a∅∗a
= aa.

3.2 Regular Expressions and Regular Languages
87
FIGURE 3.13
a
a
aa
bb
ba
ab
b
b
φ
OO
EE
EO
FIGURE 3.14
EE
a(bb)*a
aa + ab(bb)*ba
b + ab(bb)*a
b + a(bb)*ba
EO
FIGURE 3.15
We continue in this manner until we get the GTG in Figure 3.14. Next,
the state OO is removed, which gives Figure 3.15. Finally, we get the
correct regular expression from Equation (3.2).

88
Chapter 3 Regular Languages and Regular Grammars
The process of converting an nfa to a regular expression is mechanical
but tedious. It leads to regular expressions that are complicated and of
little practical use. The main reason for presenting this process is that it
gives the idea for the proof of an important result.
THEOREM 3.2
Let L be a regular language. Then there exists a regular expression r such
that L = L(r).
Proof: If L is regular, there exists an nfa for it. We can assume without
loss of generality that this nfa has a single ﬁnal state, distinct from its
initial state. We convert this nfa to a complete generalized transition graph
and apply the procedure nfa-to-rex to it. This yields the required regular
expression r.
While this can make the result plausible, a rigorous proof requires that
we show that each step in the process generates an equivalent GTG. This
is a technical matter we leave to the reader.
Regular Expressions for Describing Simple Patterns
In Example 1.15 and in Exercise 16, Section 2.1, we explored the connection
between ﬁnite accepters and some of the simpler constituents of program-
ming languages, such as identiﬁers, or integers and real numbers. The re-
lation between ﬁnite automata and regular expressions means that we can
also use regular expressions as a way of describing these features. This is
easy to see; for example, in many programming languages the set of integer
constants is deﬁned by the regular expression
sdd∗,
where s stands for the sign, with possible values from {+, −, λ}, and d stands
for the digits 0 to 9. Integer constants are a simple case of what is some-
times called a "pattern," a term that refers to a set of objects having some
common properties. Pattern matching refers to assigning a given object to
one of several categories. Often, the key to successful pattern matching is
ﬁnding an eﬀective way to describe the patterns. This is a complicated and
extensive area of computer science to which we can only brieﬂy allude. The
following example is a simpliﬁed, but nevertheless instructive, demonstra-
tion of how the ideas we have talked about so far have been found useful in
pattern matching.

3.2 Regular Expressions and Regular Languages
89
EXAMPLE 3.12
An application of pattern matching occurs in text editing. All text
editors allow ﬁles to be scanned for the occurrence of a given string;
most editors extend this to permit searching for patterns. For example,
the vi editor in the UNIX operating system recognizes the command
/aba∗c/ as an instruction to search the ﬁle for the ﬁrst occurrence of
the string ab, followed by an arbitrary number of a's, followed by a
c. We see from this example the need for pattern-matching editors to
work with regular expressions.
A challenging task in such an application is to write an eﬃcient pro-
gram for recognizing string patterns. Searching a ﬁle for occurrences of
a given string is a very simple programming exercise, but here the situ-
ation is more complicated. We have to deal with an unlimited number
of arbitrarily complicated patterns; furthermore, the patterns are not
ﬁxed beforehand, but created at run time. The pattern description is
part of the input, so the recognition process must be ﬂexible. To solve
this problem, ideas from automata theory are often used.
If the pattern is speciﬁed by a regular expression, the pattern-
recognition program can take this description and convert it into an
equivalent nfa using the construction in Theorem 3.1. Theorem 2.2
may then be used to reduce this to a dfa. This dfa, in the form of
a transition table, is eﬀectively the pattern-matching algorithm. All
the programmer has to do is to provide a driver that gives the general
framework for using the table. In this way we can automatically handle
a large number of patterns that are deﬁned at run time.
The eﬃciency of the program must also be considered. The con-
struction of ﬁnite automata from regular expressions using Theorems
2.1 and 3.1 tends to yield automata with many states.
If memory
space is a problem, the state reduction method described in Section
2.4 is helpful.
EXERCISES
1. Use the construction in Theorem 3.1 to ﬁnd an nfa that accepts the language
L (a∗a + ab).
2. Use the construction in Theorem 3.1 to ﬁnd an nfa that accepts the language
L ((aab)∗ab).
3. Use the construction in Theorem 3.1 to ﬁnd an nfa that accepts the language
L (ab∗aa + bba∗ab).

90
Chapter 3 Regular Languages and Regular Grammars
4. Find an nfa that accepts the complement of the language in Exercise 3.
5. Give an nfa that accepts the language L ((a + b)∗b (a + bb)∗).
6. Find dfa's that accept the following languages:
(a) L (aa∗+ aba∗b∗).
(b) L (ab (a + ab)∗(a + aa)).
(c) L ((abab)∗+ (aaa∗+ b)∗).
(d) L

((aa∗)∗b)∗
.
(e) L((aa∗)∗+ abb).
7. Find dfa's that accept the following languages:
(a) L = L (ab∗a∗) ∪L ((ab)∗ba).
(b) L = L (ab∗a∗) ∩L ((b)∗ab).
8. Find the minimal dfa that accepts L(abb)∗∪L(a∗bb∗).
9. Find the minimal dfa that accepts L(a∗bb) ∪L(ab∗ba).
10. Consider the following generalized transition graph.
λ
ab
a + b
bb
a
a
(a) Find an equivalent generalized transition graph with only two
states.
(b) What is the language accepted by this graph?
11. What language is accepted by the following generalized transition graph?
a
a + b*
a*+ b + c
a
a + b
a + b
12. Find regular expressions for the languages accepted by the following
automata.

3.3 Regular Grammars
91
(a)
b
a
a
a
a
b
(b)
a
a
b
b
b
(c)
a
a
b
b
b
13. Rework Example 3.11, this time eliminating the state OO ﬁrst.
14. Show how all the labels in Figure 3.14 were obtained.
15. Find a regular expression for the following languages on {a, b}.
(a) L = {w : na (w) and nb (w) are both odd}.
(b) L = {w : (na (w) −nb (w)) mod 3 = 2}.
(c) L = {w : (na (w) −nb (w)) mod 3 = 0}.
(d) L = {w : 2na (w) + 3nb (w) is even}.
16. Prove that the construction suggested by Figures 3.11 and 3.12 generate
equivalent generalized transition graphs.
17. Write a regular expression for the set of all C real numbers.
18. Use the construction in Theorem 3.1 to ﬁnd nfa's for L (a⊘) and L (⊘∗). Is
the result consistent with the deﬁnition of these languages?
3.3
REGULAR GRAMMARS
A third way of describing regular languages is by means of certain grammars.
Grammars are often an alternative way of specifying languages. Whenever

92
Chapter 3 Regular Languages and Regular Grammars
we deﬁne a language family through an automaton or in some other way,
we are interested in knowing what kind of grammar we can associate with
the family. First, we look at grammars that generate regular languages.
Right- and Left-Linear Grammars
DEFINITION 3.3
A grammar G = (V, T, S, P) is said to be right-linear if all productions
are of the form
A →xB,
A →x,
where A, B ∈V , and x ∈T ∗. A grammar is said to be left-linear if all
productions are of the form
A →Bx,
or
A →x.
A regular grammar is one that is either right-linear or left-linear.
Note that in a regular grammar, at most one variable appears on the
right side of any production. Furthermore, that variable must consistently
be either the rightmost or leftmost symbol of the right side of any pro-
duction.
EXAMPLE 3.13
The grammar G1 = ({S} , {a, b} , S, P1), with P1 given as
S →abS|a
is right-linear.
The grammar G2 = ({S, S1, S2} , {a, b} , S, P2), with
productions
S →S1ab,
S1 →S1ab|S2,
S2 →a,
is left-linear. Both G1 and G2 are regular grammars.
The sequence
S ⇒abS ⇒ababS ⇒ababa

3.3 Regular Grammars
93
is a derivation with G1. From this single instance it is easy to conjecture
that L (G1) is the language denoted by the regular expression r =
(ab)∗a. In a similar way, we can see that L (G2) is the regular language
L

aab (ab)∗
.
EXAMPLE 3.14
The grammar G = ({S, A, B} , {a, b} , S, P) with productions
S →A,
A →aB|λ,
B →Ab,
is not regular. Although every production is either in right-linear or
left-linear form, the grammar itself is neither right-linear nor left-linear,
and therefore is not regular. The grammar is an example of a linear
grammar. A linear grammar is a grammar in which at most one vari-
able can occur on the right side of any production, without restriction
on the position of this variable. Clearly, a regular grammar is always
linear, but not all linear grammars are regular.
Our next goal will be to show that regular grammars are associated
with regular languages and that for every regular language there is a regular
grammar. Thus, regular grammars are another way of talking about regular
languages.
Right-Linear Grammars Generate Regular Languages
First, we show that a language generated by a right-linear grammar is always
regular. To do so, we construct an nfa that mimics the derivations of a right-
linear grammar. Note that the sentential forms of a right-linear grammar
have the special form in which there is exactly one variable and it occurs as
the rightmost symbol. Suppose now that we have a step in a derivation
ab · · · cD ⇒ab · · · cdE,
arrived at by using a production D →dE.
The corresponding nfa can
imitate this step by going from state D to state E when a symbol d is
encountered. In this scheme, the state of the automaton corresponds to the
variable in the sentential form, while the part of the string already processed
is identical to the terminal preﬁx of the sentential form. This simple idea is
the basis for the following theorem.

94
Chapter 3 Regular Languages and Regular Grammars
THEOREM 3.3
Let G = (V, T, S, P) be a right-linear grammar. Then L (G) is a regular
language.
Proof: We assume that V = {V0, V1, ...}, that S = V0, and that we have
productions of the form V0 →v1Vi, Vi →v2Vj, ... or Vn →vl, .... If w is a
string in L (G), then because of the form of the productions
V0 ⇒v1Vi
⇒v1v2Vj
∗⇒v1v2 · · · vkVn
⇒v1v2 · · · vkvl = w.
(3.4)
The automaton to be constructed will reproduce the derivation by consum-
ing each of these v's in turn. The initial state of the automaton will be
labeled V0, and for each variable Vi there will be a nonﬁnal state labeled Vi.
For each production
Vi →a1a2 · · · amVj,
the automaton will have transitions to connect Vi and Vj that is, δ will be
deﬁned so that
δ∗(Vi, a1a2 · · · am) = Vj.
For each production
Vi →a1a2 · · · am,
the corresponding transition of the automaton will be
δ∗(Vi, a1a2 · · · am) = Vf,
where Vf is a ﬁnal state. The intermediate states that are needed to do this
are of no concern and can be given arbitrary labels. The general scheme
is shown in Figure 3.16. The complete automaton is assembled from such
individual parts.
am
a2
a1
Vf
Vi
. . .
am
a2
a1
Vj
Vi
. . .
Represents Vi
a1a2     amVj
. . .
Represents Vi
a1a2     am
. . .
FIGURE 3.16

3.3 Regular Grammars
95
Suppose now that w ∈L (G) so that (3.4) is satisﬁed. In the nfa there
is, by construction, a path from V0 to Vi labeled v1, a path from Vi to Vj
labeled v2, and so on, so that clearly
Vf ∈δ∗(V0, w) ,
and w is accepted by M.
Conversely, assume that w is accepted by M. Because of the way in
which M was constructed, to accept w the automaton has to pass through a
sequence of states V0, Vi, ... to Vf, using paths labeled v1, v2, .... Therefore,
w must have the form
w = v1v2 · · · vkvl
and the derivation
Vo ⇒v1Vi ⇒v1v2Vj
∗⇒v1v2 · · · vkVk ⇒v1v2 · · · vkvl
is possible. Hence w is in L (G), and the theorem is proved.
EXAMPLE 3.15
Construct a ﬁnite automaton that accepts the language generated by
the grammar
V0 →aV1,
V1 →abV0|b,
where V0 is the start variable.
We start the transition graph with
vertices V0, V1, and Vf.
The ﬁrst production rule creates an edge
labeled a between V0 and V1. For the second rule, we need to introduce
an additional vertex so that there is a path labeled ab between V1 and
V0.
Finally, we need to add an edge labeled b between V1 and Vf,
giving the automaton shown in Figure 3.17. The language generated
by the grammar and accepted by the automaton is the regular language
L

(aab)∗ab

.
b
a
a
b
Vf
V1
V0
FIGURE 3.17

96
Chapter 3 Regular Languages and Regular Grammars
Right-Linear Grammars for Regular Languages
To show that every regular language can be generated by some right-linear
grammar, we start from the dfa for the language and reverse the construc-
tion shown in Theorem 3.3. The states of the dfa now become the variables
of the grammar, and the symbols causing the transitions become the termi-
nals in the productions.
THEOREM 3.4
If L is a regular language on the alphabet Σ, then there exists a right-linear
grammar G = (V, Σ, S, P) such that L = L (G).
Proof: Let M = (Q, Σ, δ, q0, F) be a dfa that accepts L. We assume that
Q = {q0, q1, ..., qn} and Σ = {a1, a2, ..., am}.
Construct the right-linear
grammar G = (V, Σ, S, P) with
V = {q0, q1, ..., qn}
and S = q0. For each transition
δ (qi, aj) = qk
of M, we put in P the production
qi →ajqk.
(3.5)
In addition, if qk is in F, we add to P the production
qk →λ.
(3.6)
We ﬁrst show that G deﬁned in this way can generate every string in
L. Consider w ∈L of the form
w = aiaj · · · akal.
For M to accept this string it must make moves via
δ (q0, ai) = qp,
δ (qp, aj) = qr,
...
δ (qs, ak) = qt,
δ (qt, al) = qf ∈F.
By construction, the grammar will have one production for each of these
δ's. Therefore, we can make the derivation
q0 ⇒aiqp ⇒aiajqr
∗⇒aiaj · · · akqt
⇒aiaj · · · akalqf ⇒aiaj · · · akal,
(3.7)
with the grammar G, and w ∈L (G).

3.3 Regular Grammars
97
Conversely, if w ∈L (G), then its derivation must have the form (3.7).
But this implies that
δ∗(q0, aiaj · · · akal) = qf,
completing the proof.
For the purpose of constructing a grammar, it is useful to note that the
restriction that M be a dfa is not essential to the proof of Theorem 3.4.
With minor modiﬁcation, the same construction can be used if M is an nfa.
EXAMPLE 3.16
Construct a right-linear grammar for L (aab∗a). The transition func-
tion for an nfa, together with the corresponding grammar productions,
is given in Figure 3.18. The result was obtained by simply following
the construction in Theorem 3.4. The string aaba can be derived with
the constructed grammar by
q0 ⇒aq1 ⇒aaq2 ⇒aabq2 ⇒aabaqf ⇒aaba.
δ(q0, a) = {q1}
q0
aq1
δ(q1, a) = {q2}
q1
aq2
δ(q2, b) = {q2}
q2
bq2
δ(q2, a) = {qf}
q2
aqf
      qf F
qf
λ
FIGURE 3.18
Equivalence of Regular Languages and Regular Grammars
The previous two theorems establish the connection between regular lan-
guages and right-linear grammars.
One can make a similar connection
between regular languages and left-linear grammars, thereby showing the
complete equivalence of regular grammars and regular languages.
THEOREM 3.5
A language L is regular if and only if there exists a left-linear grammar G
such that L = L (G).
Proof: We only outline the main idea. Given any left-linear grammar with
productions of the form
A →Bv,

98
Chapter 3 Regular Languages and Regular Grammars
or
A →v,
we construct from it a right-linear grammar G by replacing every such
production of G with
A →vRB,
or
A →vR,
respectively.
A few examples will make it clear quickly that L (G) =

L

G
R
. Next, we use Exercise 12, Section 2.3, which tells us that the re-
verse of any regular language is also regular. Since G is right-linear, L

G

is regular. But then so are L

G
R
and L (G).
Putting Theorems 3.4 and 3.5 together, we arrive at the equivalence of
regular languages and regular grammars.
THEOREM 3.6
A language L is regular if and only if there exists a regular grammar G such
that L = L (G).
We now have several ways of describing regular languages: dfa's, nfa's,
regular expressions, and regular grammars. While in some instances one
or the other of these may be most suitable, they are all equally powerful.
Regular expressions
dfa or nfa
Theorem 3.1
Theorem 3.2
Regular grammars
Theorem 3.3
Theorem 3.4
FIGURE 3.19

3.3 Regular Grammars
99
Each gives a complete and unambiguous deﬁnition of a regular language.
The connection between all these concepts is established by the four theo-
rems in this chapter, as shown in Figure 3.19.
EXERCISES
1. Construct a dfa that accepts the language generated by the grammar
S →abA,
A →baB,
B →aA|bb.
2. Construct a dfa that accepts the language generated by the grammar
S →abS|A,
A →baB,
B →aA|bb.
3. Find a regular grammar that generates the language L (aa∗(ab + a)∗).
4. Construct a left-linear grammar for the language in Exercise 1.
5. Construct right- and left-linear grammars for the language
L = {anbm : n ≥3, m ≥2} .
6. Construct a right-linear grammar for the language L ((aaab∗ab)∗).
7. Find a regular grammar that generates the language on Σ = {a, b} consisting
of all strings with no more than two a's.
8. In Theorem 3.5, prove that L

G

= (L (G))R.
9. Suggest a construction by which a left-linear grammar can be obtained from
an nfa directly.
10. Use the construction suggested by the above exercises to construct a left-
linear grammar for the nfa below.
q0
q1
q2
1
0
1
0
λ
11. Find a left-linear grammar for the language L ((aaab∗ba)∗).
12. Find a regular grammar for the language L = {anbm : n + m is odd}.

100
Chapter 3 Regular Languages and Regular Grammars
13. Find a regular grammar that generates the language
L = {w ∈{a, b}∗: na (w) + 3nb (w) is odd} .
14. Find regular grammars for the following languages on {a, b}:
(a) L = {w : na(w) is even, nb(w) ≥4}.
(b) L = {w : na (w) and nb (w) are both even}. )
(c) L = {w : (na (w) −nb (w)) mod 3 = 1}.
(d) L = {w : (na (w) −nb (w)) mod 3 ̸= 1}.
(e) L = {w : (na (w) −nb (w)) mod 3 ̸= 0}.
(f) L = {w : |na (w) −nb (w)| is odd}.
15. Show that for every regular language not containing λ there exists a right-
linear grammar whose productions are restricted to the forms
A →aB,
or
A →a,
where A, B ∈V , and a ∈T.
16. Show that any regular grammar G for which L (G) ̸= ⊘must have at least
one production of the form
A →x
where A ∈V and x ∈T ∗.
17. Find a regular grammar that generates the set of all real numbers in C.
18. Let G1 = (V1, Σ, S1, P1) be right-linear and G2 = (V2, Σ, S2, P2) be a
left-linear grammar, and assume that V1 and V2 are disjoint. Consider the
linear grammar G = ({S} ∪V1 ∪V2, Σ, S, P), where S is not in V1 ∪V2 and
P = {S →S1|S2} ∪P1 ∪P2. Show that L (G) is regular.

4
CHA P T E R
PROPERTIES OF
REGULAR LANGUAGES
CHAPTER SUMMARY
We now look at what properties all regular languages have, what hap-
pens when regular languages are combined (for example, when we form
the union or intersection of two regular languages), and how we can tell
whether a language is or is not regular. The so-called pumping lemma
allows us to show that there are still simple, but not regular, languages.
This will establish the need for studying more complicated language
classes.
W
e have deﬁned regular languages, studied some ways in which
they can be represented, and have seen a few examples of their
usefulness. We now raise the question of how general regular lan-
guages are. Could it be that every formal language is regular? Perhaps any
set can be accepted by some, albeit very complex, ﬁnite automaton. As
we will see shortly, the answer to this conjecture is deﬁnitely no. But to
understand why this is so, we must inquire more deeply into the nature of
regular languages and see what properties the whole family has.
The ﬁrst question we raise is what happens when we perform opera-
tions on regular languages. The operations we consider are simple set op-
erations, such as concatenation, as well as operations in which each string
of a language is changed, as for instance in Exercise 24, Section 2.1. Is the
resulting language still regular? We refer to this as a closure question.
101

102
Chapter 4 Properties of Regular Languages
Closure properties, although mostly of theoretical interest, help us in dis-
criminating between the various language families we will encounter.
A second set of questions about language families deals with our ability
to decide on certain properties. For example, can we tell whether a language
is ﬁnite or not? As we will see, such questions are readily answered for
regular languages, but are not as easy for other language families.
Finally we consider the important question: How can we tell whether
a given language is regular or not? If the language is in fact regular, we
can always show it by giving some dfa, regular expression, or regular gram-
mar for it. But if it is not, we need another line of attack. One way to
show a language is not regular is to study the general properties of reg-
ular languages, that is, characteristics that are shared by all regular lan-
guages. If we know of some such property, and if we can show that the
candidate language does not have it, then we can tell that the language is
not regular.
In this chapter, we look at a variety of properties of regular languages.
These properties tell us a great deal about what regular languages can and
cannot do. Later, when we look at the same questions for other language
families, similarities and diﬀerences in these properties will allow us to con-
trast the various language families.
4.1
CLOSURE PROPERTIES OF REGULAR LANGUAGES
Consider the following question: Given two regular languages L1 and L2, is
their union also regular? In speciﬁc instances, the answer may be obvious,
but here we want to address the problem in general. Is it true for all regular
L1 and L2? It turns out that the answer is yes, a fact we express by saying
that the family of regular languages is closed under union. We can ask
similar questions about other types of operations on languages; this leads
us to the study of the closure properties of languages in general.
Closure properties of various language families under diﬀerent opera-
tions are of considerable theoretical interest. At ﬁrst sight, it may not be
clear what practical signiﬁcance these properties have. Admittedly, some of
them have very little, but many results are useful. By giving us insight into
the general nature of language families, closure properties help us answer
other, more practical questions. We will see instances of this (Theorem 4.7
and Example 4.13) later in this chapter.
Closure under Simple Set Operations
We begin by looking at the closure of regular languages under the common
set operations, such as union and intersection.

4.1 Closure Properties of Regular Languages
103
THEOREM 4.1
If L1 and L2 are regular languages, then so are L1 ∪L2, L1 ∩L2, L1L2, L1,
and L∗
1. We say that the family of regular languages is closed under union,
intersection, concatenation, complementation, and star-closure.
Proof: If L1 and L2 are regular, then there exist regular expressions r1 and
r2 such that L1 = L (r1) and L2 = L (r2). By deﬁnition, r1 + r2, r1r2, and
r∗
1 are regular expressions denoting the languages L1 ∪L2, L1L2, and L∗
1,
respectively. Thus, closure under union, concatenation, and star-closure is
immediate.
To show closure under complementation, let M = (Q, Σ, δ, q0, F) be a
dfa that accepts L1. Then the dfa

M = (Q, Σ, δ, q0, Q −F)
accepts L1. This is rather straightforward; we have already suggested the
result in Exercise 10 in Section 2.1. Note that in the deﬁnition of a dfa,
we assumed δ∗to be a total function, so that δ∗(q0, w) is deﬁned for all
w ∈Σ∗. Consequently either δ∗(q0, w) is a ﬁnal state, in which case w ∈L,
or δ∗(q0, w) ∈Q −F and w ∈L.
Demonstrating closure under intersection takes a little more work. Let
L1 = L (M1) and L2 = L (M2), where M1 = (Q, Σ, δ1, q0, F1) and M2 =
(P, Σ, δ2, p0, F2) are dfa's. We construct from M1 and M2 a combined au-
tomaton 
M =

Q, Σ, δ, (q0, p0) , F

, whose state set Q = Q × P consists
of pairs (qi, pj), and whose transition function δ is such that 
M is in state
(qi, pj) whenever M1 is in state qi and M2 is in state pj. This is achieved
by taking
δ ((qi, pj) , a) = (qk, pl),
whenever
δ1 (qi, a) = qk
and
δ2 (pj, a) = pl.
F is deﬁned as the set of all (qi, pj), such that qi ∈F1 and pj ∈F2. Then
it is a simple matter to show that w ∈L1 ∩L2 if and only if it is accepted
by 
M. Consequently, L1 ∩L2 is regular.
The proof of closure under intersection is a good example of a con-
structive proof. Not only does it establish the desired result, but it also
shows explicitly how to construct a ﬁnite accepter for the intersection of

104
Chapter 4 Properties of Regular Languages
two regular languages.
Constructive proofs occur throughout this book;
they are important because they give us insight into the results and often
serve as the starting point for practical algorithms. Here, as in many cases,
there are shorter but nonconstructive (or at least not so obviously construc-
tive) arguments. For closure under intersection, we start with DeMorgan's
law, Equation (1.3), taking the complement of both sides. Then
L1 ∩L2 = L1 ∪L2
for any languages L1 and L2. Now, if L1 and L2 are regular, then by closure
under complementation, so are L1 and L2. Using closure under union, we
next get that L1 ∪L2 is regular. Using closure under complementation once
more, we see that
L1 ∪L2 = L1 ∩L2
is regular.
The following example is a variation on the same idea.
EXAMPLE 4.1
Show that the family of regular languages is closed under diﬀerence.
In other words, we want to show that if L1 and L2 are regular, then
L1 −L2 is necessarily regular also.
The needed set identity is immediately obvious from the deﬁnition
of a set diﬀerence, namely
L1 −L2 = L1 ∩L2.
The fact that L2 is regular implies that L2 is also regular. Then,
because of the closure of regular languages under intersection, we know
that L1 ∩L2 is regular, and the argument is complete.
A variety of other closure properties can be derived directly by elemen-
tary arguments.
THEOREM 4.2
The family of regular languages is closed under reversal.
Proof: The proof of this theorem was suggested as an exercise in Section
2.3. Here are the details. Suppose that L is a regular language. We then
construct an nfa with a single ﬁnal state for it.
By Exercise 7, Section
2.3, this is always possible. In the transition graph for this nfa we make
the initial vertex a ﬁnal vertex, the ﬁnal vertex the initial vertex, and re-
verse the direction on all the edges. It is a fairly straightforward matter

4.1 Closure Properties of Regular Languages
105
to show that the modiﬁed nfa accepts wR if and only if the original nfa
accepts w. Therefore, the modiﬁed nfa accepts LR, proving closure under
reversal.
Closure under Other Operations
In addition to the standard operations on languages, one can deﬁne other
operations and investigate closure properties for them. There are many such
results; we select only two typical ones. Others are explored in the exercises
at the end of this section.
DEFINITION 4.1
Suppose Σ and Γ are alphabets. Then a function
h : Σ →Γ∗
is called a homomorphism. In words, a homomorphism is a substitution
in which a single letter is replaced with a string. The domain of the function
h is extended to strings in an obvious fashion; if
w = a1a2 · · · an,
then
h (w) = h (a1) h (a2) · · · h (an) .
If L is a language on Σ, then its homomorphic image is deﬁned as
h (L) = {h (w) : w ∈L} .
EXAMPLE 4.2
Let Σ = {a, b} and Γ = {a, b, c} and deﬁne h by
h (a) = ab,
h (b) = bbc.
Then h (aba) = abbbcab. The homomorphic image of L = {aa, aba} is
the language h (L) = {abab, abbbcab}.
If we have a regular expression r for a language L, then a regular ex-
pression for h (L) can be obtained by simply applying the homomorphism
to each Σ symbol of r.

106
Chapter 4 Properties of Regular Languages
EXAMPLE 4.3
Take Σ = {a, b} and Γ = {b, c, d}. Deﬁne h by
h (a) = dbcc,
h (b) = bdc.
If L is the regular language denoted by
r = (a + b∗) (aa)∗,
then
r1 =

dbcc + (bdc)∗
(dbccdbcc)∗
denotes the regular language h (L).
The general result on the closure of regular languages under any homo-
morphism follows from this example in an obvious manner.
THEOREM 4.3
Let h be a homomorphism. If L is a regular language, then its homomorphic
image h (L) is also regular. The family of regular languages is therefore
closed under arbitrary homomorphisms.
Proof: Let L be a regular language denoted by some regular expression r.
We ﬁnd h (r) by substituting h (a) for each symbol a ∈Σ of r.
It can
be shown directly by an appeal to the deﬁnition of a regular expression
that the result is a regular expression. It is equally easy to see that the
resulting expression denotes h (L). All we need to do is to show that for
every w ∈L (r), the corresponding h (w) is in L (h (r)) and conversely that
for every v in L (h (r)) there is a w in L, such that v = h (w). Leaving the
details as an exercise, we claim that h (L) is regular.
DEFINITION 4.2
Let L1 and L2 be languages on the same alphabet. Then the right quotient
of L1 with L2 is deﬁned as
L1/L2 = {x : xy ∈L1 for some y ∈L2} .
(4.1)

4.1 Closure Properties of Regular Languages
107
To form the right quotient of L1 with L2, we take all the strings in L1
that have a suﬃx belonging to L2. Every such string, after removal of this
suﬃx, belongs to L1/L2.
EXAMPLE 4.4
If
L1 = {anbm : n ≥1, m ≥0} ∪{ba}
and
L2 = {bm : m ≥1} ,
then
L1/L2 = {anbm : n ≥1, m ≥0} .
The strings in L2 consist of one or more b's. Therefore, we arrive
at the answer by removing one or more b's from those strings in L1
that terminate with at least one b.
Note that here L1, L2, and L1/L2 are all regular. This suggests
that the right quotient of any two regular languages is also regular. We
will prove this in the next theorem by a construction that takes the dfa's
for L1 and L2 and constructs from them a dfa for L1/L2. Before we
q1
q2
q0
a 
a, b
a
q5
q3
b
b
b
q4
a
a
a, b
b
FIGURE 4.1

108
Chapter 4 Properties of Regular Languages
q1
q2
q0
a 
a, b
a
q5
q3
b
b
b
q4
a
a, b
b
FIGURE 4.2
describe the construction in full, let us see how it applies to this exam-
ple. We start with a dfa for L1; say the automaton M1 = (Q, Σ, δ, q0, F)
in Figure 4.1. Since an automaton for L1/L2 must accept any preﬁx
of strings in L1, we will try to modify M1 so that it accepts x if there
is any y satisfying (4.1). The diﬃculty comes in ﬁnding whether there
is some y such that xy ∈L1 and y ∈L2. To solve it, we determine, for
each q ∈Q, whether there is a walk to a ﬁnal state labeled v such that
v ∈L2. If this is so, any x such that δ (q0, x) = q will be in L1/L2. We
modify the automaton accordingly to make q a ﬁnal state.
To apply this to our present case, we check each state q0, q1, q2, q3,
q4, q5 to see whether there is a walk labeled bb∗to any of the q1, q2, or
q4. We see that only q1 and q2 qualify; q0, q3, q4 do not. The resulting
automaton for L1/L2 is shown in Figure 4.2. Check it to see that the
construction works. The idea is generalized in the next theorem.
THEOREM 4.4
If L1 and L2 are regular languages, then L1/L2 is also regular. We say that
the family of regular languages is closed under right quotient with a regular
language.

4.1 Closure Properties of Regular Languages
109
Proof: Let L1 = L (M), where M = (Q, Σ, δ, q0, F) is a dfa. We construct
another dfa 
M =

Q, Σ, δ, q0, F

as follows. For each qi ∈Q, determine if
there exists a y ∈L2 such that
δ∗(qi, y) = qf ∈F.
This can be done by looking at dfa's Mi = (Q, Σ, δ, qi, F). The automaton
Mi is M with the initial state q0 replaced by qi. We now determine whether
there exists a y in L (Mi) that is also in L2.
For this, we can use the
construction for the intersection of two regular languages given in Theorem
4.1, ﬁnding the transition graph for L2∩L (Mi). If there is any path between
its initial vertex and any ﬁnal vertex, then L2 ∩L (Mi) is not empty. In
that case, add qi to F. Repeating this for every qi ∈Q, we determine F
and thereby construct 
M.
To prove that L


M

= L1/L2, let x be any element of L1/L2. Then
there must be a y ∈L2 such that xy ∈L1. This implies that
δ∗(q0, xy) ∈F,
so that there must be some q ∈Q such that
δ∗(q0, x) = q
and
δ∗(q, y) ∈F.
Therefore, by construction, q ∈F, and 
M accepts x because δ∗(q0, x) is
in F.
Conversely, for any x accepted by 
M, we have
δ∗(q0, x) = q ∈F.
But again by construction, this implies that there exists a y ∈L2 such that
δ∗(q, y) ∈F.
Therefore, xy is in L1, and x is in L1/L2.
We therefore
conclude that
L


M

= L1/L2,
and from this that L1/L2 is regular.

110
Chapter 4 Properties of Regular Languages
EXAMPLE 4.5
Find L1/L2 for
L1 = L (a∗baa∗) ,
L2 = L (ab∗) .
We ﬁrst ﬁnd a dfa that accepts L1. This is easy, and a solution is given
in Figure 4.3. The example is simple enough so that we can skip the
formalities of the construction. From the graph in Figure 4.3 it is quite
evident that
L (M0) ∩L2 = ∅,
L (M1) ∩L2 = {a} ̸= ∅,
L (M2) ∩L2 = {a} ̸= ∅,
L (M3) ∩L2 = ∅.
Therefore, the automaton accepting L1/L2 is determined. The result
is shown in Figure 4.4. It accepts the language denoted by the regular
expression of a∗b + a∗baa∗, which can be simpliﬁed to a∗ba∗. Thus
L1/L2 = L (a∗ba∗).
q1
q2
q0
a 
a, b
b
q3
a
b
b
a
FIGURE 4.3

4.1 Closure Properties of Regular Languages
111
q1
q2
q0
a 
a, b
b
q3
a
b
b
a
FIGURE 4.4
EXERCISES
1. For Σ = {a, b}, ﬁnd regular expressions for the complement of the following
languages:
(a) L = L(aa∗bb∗)
(b) L = L(a∗b∗)∗
2. Let L1=L(ab∗aa), L2=L(a∗bba∗). Find a regular expression for (L1
 L2)∗L2.
3. Show how from a dfa for L, one can construct a ﬁnite automaton for L
∗.
4. Show that if a language family is closed under union and complementation,
it must also be closed under intersection.
5. Fill in the details of the constructive proof of closure under intersection in
Theorem 4.1.
6. Use the construction in Theorem 4.1 to ﬁnd nfa's that accept
(a) L ((ab)∗a∗) ∩L (baa∗)
(b) L (ab∗a∗) ∩L (a∗b∗a)
7. In Example 4.1, we showed closure under diﬀerence for regular languages,
but the proof was nonconstructive.
Provide a constructive argument for
this result, following the approach used in the argument for intersection in
Theorem 4.1.
8. In the proof of Theorem 4.3, show that h (r) is a regular expression. Then
show that h (r) denotes h (L).

112
Chapter 4 Properties of Regular Languages
9. Show that the family of regular languages is closed under ﬁnite union and
intersection, that is, if L1, L2, ..., Ln are regular, then
LU =

i={1,2,...,n}
Li
and
LI =

i={1,2,...,n}
Li
are also regular.
10. The symmetric diﬀerence of two sets S1 and S2 is deﬁned as
S1 ⊖S2 = {x : x ∈S1 or x ∈S2, but x is not in both S1 and S2} .
Show that the family of regular languages is closed under symmetric
diﬀerence.
11. The nor of two languages is
nor (L1, L2) = {w : w /∈L1 and w /∈L2} .
Show that the family of regular languages is closed under the nor operation.
12. Deﬁne the complementary or (cor) of two languages by
cor (L1, L2) =

w : w ∈L1
or
w ∈L2

.
Show that the family of regular languages is closed under the cor operation.
13. Which
of
the
following
are
true
for
all
regular
languages
and
all
homomorphisms?
(a) h (L1 ∪L2) = h (L1) ∪h (L2).
(b) h (L1 ∩L2) = h (L1) ∩h (L2).
(c) h (L1L2) = h (L1) h (L2).
14. Let L1 = L (a∗baa∗) and L2 = L (aba∗). Find L1/L2.
15. Show that L1 = L1L2/L2 is not true for all languages L1 and L2.
16. Suppose we know that L1 ∪L2 is regular and that L1 is ﬁnite.
Can we
conclude from this that L2 is regular?
17. If L is a regular language, prove that L1 = {uv : u ∈L, |v| = 2} is also regular.
18. If L is a regular language, prove that the language

uv : u ∈L, v ∈LR
is
also regular.
19. The left quotient of a language L1 with respect to L2 is deﬁned as
L2/L1 = {y : x ∈L2, xy ∈L1} .
Show that the family of regular languages is closed under the left quotient
with a regular language.

4.1 Closure Properties of Regular Languages
113
20. Show that if the statement "If L1 is regular and L1 ∪L2 is also regular, then
L2 must be regular" were true for all L1 and L2, then all languages would be
regular.
21. The tail of a language is deﬁned as the set of all suﬃxes of its strings,
that is,
tail (L) = {y : xy ∈L for some x ∈Σ∗} .
Show that if L is regular, so is tail (L).
22. The head of a language is the set of all preﬁxes of its strings, that is,
head (L) = {x : xy ∈L for some y ∈Σ∗} .
Show that the family of regular languages is closed under this operation.
23. Deﬁne an operation third on strings and languages as
third (a1a2a3a4a5a6 · · · ) = a3a6 · · ·
with the appropriate extension of this deﬁnition to languages.
Prove the
closure of the family of regular languages under this operation.
24. For a string a1a2 · · · an deﬁne the operation shift as
shift (a1a2 · · · an) = a2 · · · ana1.
From this, we can deﬁne the operation on a language as
shift (L) = {v : v = shift (w) for some w ∈L} .
Show that regularity is preserved under the shift operation.
25. Deﬁne
exchange (a1a2 · · · an−1an) = ana2 · · · an−1a1,
and
exchange (L) = {v : v = exchange (w) for some w ∈L} .
Show that the family of regular languages is closed under exchange.
26. The min of a language L is deﬁned as
min (L) =

w ∈L : there is no u ∈L, v ∈Σ+, such that w = uv

.
Show that the family of regular languages is closed under the min operation.
27. Let G1 and G2 be two regular grammars. Show how one can derive regular
grammars for the languages
(a) L (G1) ∪L (G2).
(b) L (G1) L (G2).
(c) L (G1)∗.

114
Chapter 4 Properties of Regular Languages
4.2
ELEMENTARY QUESTIONS ABOUT
REGULAR LANGUAGES
We now come to a very fundamental issue: Given a language L and a string
w, can we determine whether or not w is an element of L? This is the
membership question and a method for answering it is called a member-
ship algorithm.∗Very little can be done with languages for which we cannot
ﬁnd eﬃcient membership algorithms. The question of the existence and na-
ture of membership algorithms will be of great concern in later discussions;
it is an issue that is often diﬃcult. For regular languages, though, it is an
easy matter.
We ﬁrst consider what exactly we mean when we say "given a lan-
guage...." In many arguments, it is important that this be unambiguous.
We have used several ways of describing regular languages: informal verbal
descriptions, set notation, ﬁnite automata, regular expressions, and regu-
lar grammars. Only the last three are suﬃciently well deﬁned for use in
theorems.
We therefore say that a regular language is given in a stan-
dard representation if and only if it is described by a ﬁnite automaton,
a regular expression, or a regular grammar.
THEOREM 4.5
Given a standard representation of any regular language L on Σ and any
w ∈Σ∗, there exists an algorithm for determining whether or not w is in L.
Proof: We represent the language by some dfa, then test w to see if it is
accepted by this automaton.
Other important questions are whether a language is ﬁnite or inﬁnite,
whether two languages are the same, and whether one language is a subset of
another. For regular languages at least, these questions are easily answered.
THEOREM 4.6
There exists an algorithm for determining whether a regular language, given
in standard representation, is empty, ﬁnite, or inﬁnite.
Proof: The answer is apparent if we represent the language as a transition
graph of a dfa. If there is a simple path from the initial vertex to any ﬁnal
vertex, then the language is not empty.
∗Later we will make precise what the term "algorithm" means. For the moment, think
of it as a method for which one can write a computer program.

4.2 Elementary Questions about Regular Languages
115
To determine whether or not a language is inﬁnite, ﬁnd all the vertices
that are the base of some cycle. If any of these are on a path from an initial
to a ﬁnal vertex, the language is inﬁnite. Otherwise, it is ﬁnite.
The question of the equality of two languages is also an important prac-
tical issue. Often several deﬁnitions of a programming language exist, and
we need to know whether, in spite of their diﬀerent appearances, they spec-
ify the same language. This is generally a diﬃcult problem; even for regular
languages the argument is not obvious. It is not possible to argue on a
sentence-by-sentence comparison, since this works only for ﬁnite languages.
Nor is it easy to see the answer by looking at the regular expressions, gram-
mars, or dfa's.
An elegant solution uses the already established closure
properties.
THEOREM 4.7
Given standard representations of two regular languages L1 and L2, there
exists an algorithm to determine whether or not L1 = L2.
Proof: Using L1 and L2 we deﬁne the language
L3 =

L1 ∩L2

∪

L1 ∩L2

.
By closure, L3 is regular, and we can ﬁnd a dfa M that accepts L3. Once
we have M we can then use the algorithm in Theorem 4.6 to determine if
L3 is empty. But from Exercise 8, Section 1.1, we see that L3 = ∅if and
only if L1 = L2.
These results are fundamental, in spite of being obvious and unsurpris-
ing. For regular languages, the questions raised by Theorems 4.5 to 4.7 can
be answered easily, but this is not always the case when we deal with other
language families. We will encounter questions like these on several occa-
sions later on. Anticipating a little, we will see that the answers become
increasingly more diﬃcult and eventually impossible to ﬁnd.
EXERCISES
For all the exercises in this section, assume that regular languages are given
in standard representation.
1. Given a regular language L and a string w ∈L, show how one can determine
if wR ∈L.
2. Exhibit an algorithm to determine whether or not a regular language L con-
tains any string w such that wR ∈L.

116
Chapter 4 Properties of Regular Languages
3. A language is said to be a palindrome language if L = LR. Find an algorithm
to determine whether a given regular language is a palindrome language.
4. Show that there exists an algorithm to determine whether or not w ∈L1 −L2
for any given w and any regular languages L1 and L2.
5. Show that there exists an algorithm to determine whether L1 ⊆L2 for any
regular languages L1 and L2.
6. Show that there exists an algorithm to determine whether L1 is a proper
subset of L2 for any regular languages L1 and L2.
7. Show that there exists an algorithm to determine whether λ ∈L for any
regular language L.
8. Show that there exists an algorithm to determine whether L = Σ∗for any
regular language L.
9. Exhibit an algorithm that, given any three regular languages, L, L1, L2, de-
termines whether or not L = L1L2.
10. Exhibit an algorithm that, given any regular language L, determines whether
or not L = L∗.
11. Let L be a regular language on Σ and w be any string in Σ∗.
Find an
algorithm to determine whether L contains any w such that w is a substring
of it, that is, such that w = u wv, with u, v ∈Σ∗, and |uv| > 0.
12. The operation tail (L) is deﬁned as
tail (L) = {v : uv ∈L, u, v ∈Σ∗} .
Show that there is an algorithm to determine whether or not L = tail (L) for
any regular L.
13. Let L be any regular language on Σ = {a, b}. Show that an algorithm exists
to determine whether L contains any strings of even length.
14. Show that there exists an algorithm that can determine, for every regular
language L, whether or not |L| ≥5.
15. Find an algorithm to determine whether a regular language L contains a ﬁnite
number of even-length strings.
16. Describe an algorithm that, when given a regular grammar G, can tell us
whether or not L (G) = Σ∗.
17. Given two regular grammars G1 and G2, show how one can determine whether
L(G1) ∪L(G2) = Σ∗
18. Describe an algorithm by which one can decide whether two regular grammars
are equivalent.
19. Describe an algorithm by which one can decide whether two regular expres-
sions are equivalent.

4.3 Identifying Nonregular Languages
117
4.3
IDENTIFYING NONREGULAR LANGUAGES
Regular languages can be inﬁnite, as most of our examples have demon-
strated. The fact that regular languages are associated with automata that
have ﬁnite memory, however, imposes some limits on the structure of a reg-
ular language. Some narrow restrictions must be obeyed if regularity is to
hold. Intuition tells us that a language is regular only if, in processing any
string, the information that has to be remembered at any stage is strictly
limited.
This is true, but has to be shown precisely to be used in any
meaningful way. There are several ways in which this can be done.
Using the Pigeonhole Principle
The term "pigeonhole principle" is used by mathematicians to refer to the
following simple observation. If we put n objects into m boxes (pigeonholes),
and if n > m, then at least one box must have more than one item in it.
This is such an obvious fact that it is surprising how many deep results can
be obtained from it.
EXAMPLE 4.6
Is the language L = {anbn : n ≥0} regular? The answer is no, as we
show using a proof by contradiction.
Suppose L is regular.
Then some dfa M = (Q, {a, b} , δ, q0, F)
exists for it. Now look at δ∗
q0, ai
for i = 1, 2, 3, .... Since there are
an unlimited number of i's, but only a ﬁnite number of states in M,
the pigeonhole principle tells us that there must be some state, say q,
such that
δ∗(q0, an) = q
and
δ∗(q0, am) = q,
with n ̸= m. But since M accepts anbn we must have
δ∗(q, bn) = qf ∈F.
From this we can conclude that
δ∗(q0, ambn) = δ∗(δ∗(q0, am) , bn)
= δ∗(q, bn)
= qf.
This contradicts the original assumption that M accepts ambn only if
n = m, and leads us to conclude that L cannot be regular.

118
Chapter 4 Properties of Regular Languages
In this argument, the pigeonhole principle is just a way of stating unam-
biguously what we mean when we say that a ﬁnite automaton has a limited
memory. To accept all anbn, an automaton would have to diﬀerentiate be-
tween all preﬁxes an and am. But since there are only a ﬁnite number of
internal states with which to do this, there are some n and m for which the
distinction cannot be made.
In order to use this type of argument in a variety of situations, it is
convenient to codify it as a general theorem. There are several ways to do
this; the one we give here is perhaps the most famous one.
Here is what we know about transition graphs for regular languages:
• If the transition graph has no cycles, the language is ﬁnite and therefore
regular.
• If the transition graph has a cycle with a nonempty label, the language
is inﬁnite. Conversely, every inﬁnite regular language has a dfa with
such a cycle.
• If there is a cycle, this cycle can either be skipped or repeated an ar-
bitrary number of times. So, if the cycle has label v and if the string
w1vw2 is in the language, so must be the strings w1w2, w1vvw2,w1vvvw2,
and so on.
• We do not know where in the dfa this cycle is, but if the dfa has m
states, the cycle must be entered by the time m symbols have been
read.
If, for some language L, there is even one string w that does not have
this property, L cannot be regular. This observation can be formally stated
as a theorem called pumping lemma.
A Pumping Lemma
The following result, known as the pumping lemma for regular languages,
uses the pigeonhole principle in another form. The proof is based on the
observation that in a transition graph with n vertices, any walk of length n
or longer must repeat some vertex, that is, contain a cycle.
THEOREM 4.8
Let L be an inﬁnite regular language.
Then there exists some positive
integer m such that any w ∈L with |w| ≥m can be decomposed as
w = xyz
with
|xy| ≤m,

4.3 Identifying Nonregular Languages
119
and
|y| ≥1,
such that
wi = xyiz,
(4.2)
is also in L for all i = 0, 1, 2, ....
To paraphrase this, every suﬃciently long string in L can be broken
into three parts in such a way that an arbitrary number of repetitions of
the middle part yields another string in L. We say that the middle string
is "pumped," hence the term pumping lemma for this result.
Proof: If L is regular, there exists a dfa that recognizes it. Let such a dfa
have states labeled q0, q1, q2, ..., qn.
Now take a string w in L such that
|w| ≥m = n+1. Since L is assumed to be inﬁnite, this can always be done.
Consider the set of states the automaton goes through as it processes w,
say
q0, qi, qj, ..., qf.
Since this sequence has exactly |w| + 1 entries, at least one state must be
repeated, and such a repetition must start no later than the nth move.
Thus, the sequence must look like
q0, qi, qj, ..., qr, ..., qr, ..., qf,
indicating there must be substrings x, y, z of w such that
δ∗(q0, x) = qr,
δ∗(qr, y) = qr,
δ∗(qr, z) = qf,
with |xy| ≤n + 1 = m and |y| ≥1. From this it immediately follows that
δ∗(q0, xz) = qf,
as well as
δ∗
q0, xy2z

= qf,
δ∗
q0, xy3z

= qf,
and so on, completing the proof of the theorem.
We have given the pumping lemma only for inﬁnite languages. Finite
languages, although always regular, cannot be pumped since pumping auto-
matically creates an inﬁnite set. The theorem does hold for ﬁnite languages,

120
Chapter 4 Properties of Regular Languages
but it is vacuous. The m in the pumping lemma is to be taken larger than
the longest string, so that no string can be pumped.
The pumping lemma, like the pigeonhole argument in Example 4.6, is
used to show that certain languages are not regular. The demonstration is
always by contradiction. There is nothing in the pumping lemma, as we
have stated it here, that can be used for proving that a language is regular.
Even if we could show (and this is normally quite diﬃcult) that any pumped
string must be in the original language, there is nothing in the statement
of Theorem 4.8 that allows us to conclude from this that the language is
regular.
EXAMPLE 4.7
Use the pumping lemma to show that L = {anbn : n ≥0} is not
regular. Assume that L is regular, so that the pumping lemma must
hold.
We do not know the value of m, but whatever it is, we can
always choose n = m. Therefore, the substring y must consist entirely
of a's. Suppose |y| = k. Then the string obtained by using i = 0 in
Equation (4.2) is
w0 = am−kbm
and is clearly not in L.
This contradicts the pumping lemma and
thereby indicates that the assumption that L is regular must be false.
In applying the pumping lemma, we must keep in mind what the the-
orem says. We are guaranteed the existence of an m as well as the decom-
position xyz, but we do not know what they are. We cannot claim that we
have reached a contradiction just because the pumping lemma is violated
for some speciﬁc values of m or xyz.
On the other hand, the pumping
lemma holds for every w ∈L and every i. Therefore, if the pumping lemma
is violated even for one w or i, then the language cannot be regular.
The correct argument can be visualized as a game we play against an
opponent. Our goal is to win the game by establishing a contradiction of
the pumping lemma, while the opponent tries to foil us. There are four
moves in the game.
1. The opponent picks m.
2. Given m, we pick a string w in L of length equal or greater than m.
We are free to choose any w, subject to w ∈L and |w| ≥m.
3. The opponent chooses the decomposition xyz, subject to |xy| ≤m, |y| ≥
1. We have to assume that the opponent makes the choice that will make
it hardest for us to win the game.

4.3 Identifying Nonregular Languages
121
4. We try to pick i in such a way that the pumped string wi, deﬁned in
Equation (4.2), is not in L. If we can do so, we win the game.
A strategy that allows us to win whatever the opponent's choices is
tantamount to a proof that the language is not regular. In this, Step 2 is
crucial. While we cannot force the opponent to pick a particular decom-
position of w, we may be able to choose w so that the opponent is very
restricted in Step 3, forcing a choice of x, y, and z that allows us to produce
a violation of the pumping lemma on our next move.
EXAMPLE 4.8
Show that
L =

wwR : w ∈Σ∗	
is not regular.
Whatever m the opponent picks on Step 1, we can always choose a
w as shown in Figure 4.5. Because of this choice, and the requirement
that |xy| ≤m, the opponent is restricted in Step 3 to choosing a y that
consists entirely of a's. In Step 4, we use i = 0. The string obtained in
this fashion has fewer a's on the left than on the right and so cannot
be of the form wwR. Therefore, L is not regular.
am
a a . . . a . . . . a . . . a
x  
              z
y
bm
b b . . . . . . . . . . . . . b b . . . . . . . . . . . . . . . b
bm
am
a a . . . . . . . . . . . . . a
FIGURE 4.5
Note that if we had chosen a w too short, then the opponent could
have chosen a y with an even number of b's. In that case, we could not
have reached a violation of the pumping lemma on the last step. We
would also fail if we were to choose a string consisting of all a's, say,
w = a2m,
which is in L. To defeat us, the opponent need only pick
y = aa.
Now wi is in L for all i, and we lose.
To apply the pumping lemma we cannot assume that the opponent
will make a wrong move. If, in the case where we pick w = a2m, the
opponent were to pick
y = a,

122
Chapter 4 Properties of Regular Languages
then w0 is a string of odd length and therefore not in L.
But any
argument that assumes that the opponent is so accommodating is au-
tomatically incorrect.
EXAMPLE 4.9
Let Σ = {a, b}. The language
L = {w ∈Σ∗: na (w) < nb (w)}
is not regular.
Suppose we are given m. Since we have complete freedom in choos-
ing w, we pick w = ambm+1. Now, because |xy| cannot be greater than
m, the opponent cannot do anything but pick a y with all a's, that is
y = ak,
1 ≤k ≤m.
We now pump up, using i = 2. The resulting string
w2 = am+kbm+1
is not in L. Therefore, the pumping lemma is violated, and L is not
regular.
EXAMPLE 4.10
The language
L =

(ab)n ak : n > k, k ≥0
	
is not regular.
Given m, we pick as our string
w = (ab)m+1 am,
which is in L. Because of the constraint |xy| ≤m, both x and y must
be in the part of the string made up of ab's. The choice of x does
not aﬀect the argument, so let us see what can be done with y. If
our opponent picks y = a, we choose i = 0 and get a string not in
L

(ab)∗a∗
. If the opponent picks y = ab, we can choose i = 0 again.
Now we get the string (ab)m am, which is not in L. In the same way,
we can deal with any possible choice by the opponent, thereby proving
our claim.

4.3 Identifying Nonregular Languages
123
EXAMPLE 4.11
Show that
L = {an : n is a perfect square}
is not regular.
Given the opponent's choice of m, we pick
w = am2.
If w = xyz is the decomposition, then clearly
y = ak
with 1 ≤k ≤m. In that case,
w0 = am2−k
But m2 −k > (m −1)2, so that w0 cannot be in L. Therefore, the
language is not regular.
In some cases, closure properties can be used to relate a given problem to
one we have already classiﬁed. This may be simpler than a direct application
of the pumping lemma.
EXAMPLE 4.12
Show that the language
L =

anbkcn+k : n ≥0, k ≥0
	
is not regular.
It is not diﬃcult to apply the pumping lemma directly, but it is
even easier to use closure under homomorphism. Take
h (a) = a, h (b) = a, h (c) = c,
then
h (L) =

an+kcn+k : n + k ≥0
	
=

aici : i ≥0
	
.
But we know this language is not regular; therefore, L cannot be regular
either.

124
Chapter 4 Properties of Regular Languages
EXAMPLE 4.13
Show that the language
L =

anbl : n ̸= l
	
is not regular.
Here we need a bit of ingenuity to apply the pumping lemma di-
rectly. Choosing a string with n = l + 1 or n = l + 2 will not do,
since our opponent can always choose a decomposition that will make
it impossible to pump the string out of the language (that is, pump
it so that it has an equal number of a's and b's). We must be more
inventive. Let us take n = m! and l = (m + 1)!. If the opponent now
chooses a y (by necessity consisting of all a's) of length k < m, we
pump i times to generate a string with m! + (i −1) k a's. We can get
a contradiction of the pumping lemma if we can pick i such that
m! + (i −1) k = (m + 1)!
This is always possible since
i = 1 + m m!
k
and k ≤m. The right side is therefore an integer, and we have suc-
ceeded in violating the conditions of the pumping lemma.
However, there is a much more elegant way of solving this problem.
Suppose L were regular. Then, by Theorem 4.1, L and the language
L1 = L ∩L (a∗b∗)
would also be regular. But L1 = {anbn : n ≥0}, which we have already
classiﬁed as nonregular. Consequently, L cannot be regular.
The pumping lemma is diﬃcult to understand and it is easy to go astray
when applying it. Here are some common pitfalls. Watch out for them.
One mistake is to try using the pumping lemma to show that a language
is regular. Even if you can show that no string in a language L can ever be
pumped out, you cannot conclude that L is regular. The pumping lemma
can only be used to prove that a language is not regular.
Another mistake is to start (usually inadvertently) with a string not in
L. For example, suppose we try to show that
L = {an : n is a prime number}
(4.3)
is not regular. An argument that starts with "Given m, let w = am...," is
incorrect since m is not necessarily prime. To avoid this pitfall, we need

4.3 Identifying Nonregular Languages
125
to start with something like "Given m, let w = aM, where M is a prime
number larger than m."
Finally, perhaps the most common mistake is to make some assump-
tions about the decomposition xyz. The only thing we can say about the
decomposition is what the pumping lemma tells us, namely, that y is not
empty and that |xy| ≤m; that is, that y must be within m symbols of the
left end of the string. Anything else makes the argument invalid. A typical
mistake in trying to prove that the language in Equation (4.3) is not regular
is to say that y = ak, with k odd. Then of course w = xz is an even-length
string and thus not in L. But the assumption on k is not permitted and
the proof is wrong.
But even if you master the technical diﬃculties of the pumping lemma,
it may still be hard to see exactly how to use it. The pumping lemma is like
a game with complicated rules. Knowledge of the rules is essential, but that
alone is not enough to play a good game. You also need a good strategy to
win. If you can apply the pumping lemma correctly to some of the more
diﬃcult cases in this book, you are to be congratulated.
EXERCISES
1. Show that the language
L = {anbkcn : n ≥0, k ≥0}
is not regular.
2. Show that the language
L = {anbkcn : n ≥0, k ≥n}
is not regular.
3. Show that the language
L = {anbkcndk : n ≥0, k > n}
is not regular.
4. Show that the language
L = {w : na (w) = nb (w)}
is not regular. Is L∗regular?
5. Prove that the following languages are not regular:
(a) L =

anblak : k ≤n + l

.
(b) L =

anblak : k ̸= n + l

.
(c) L =

anblak : n = l or l ̸= k

.

126
Chapter 4 Properties of Regular Languages
(d) L =

anbl : n ≥l

.
(e) L = {w : na (w) ̸= nb (w)}.
(f) L = {ww : w ∈{a, b}∗}.
(g) L =

wRwwwR : w ∈{a, b}∗
.
6. Determine whether or not the following languages on Σ = {a} are regular:
(a) L = {an : n ≥2, is a prime number}.
(b) L = {an : n is not a prime number}.
(c) L =

an : n = k3 for some k ≥0

.
(d) L =

an : n = 2k for some k ≥0

.
(e) L = {an : n is the product of two prime numbers}.
(f) L = {an : n is either prime or the product of two or more prime
numbers}.
(g) L∗, where L is the language in part (a).
7. Determine whether or not the following languages are regular:
(a) L = {anbn : n ≥1} ∪{anbm : n ≥1, m ≥1}.
(b) L = {anbn : n ≥1} ∪{anbn+2 : n ≥1}.
8. Show that the language
L = {anbn : n ≥0} ∪{anbn+1 : n ≥0} ∪{anbn+2 : n ≥0}
is not regular.
9. Show that the language
L = {anbn+k : n ≥0, k ≥1} ∪{an+kbn : n ≥0, k ≥3}
is not regular.
10. Is the language L = {w ∈{a, b, c}∗: |w| = 3na(w)} regular?
11. Consider the language
L = {an : n is not a perfect square}.
(a) Show that this language is not regular by applying the pumping
lemma directly.
(b) Then show the same thing by using the closure properties of reg-
ular languages.
12. Show that the language
L = {an! : n ≥1}
is not regular.
13. Apply the pumping lemma directly to show the result in Example 4.12.

4.3 Identifying Nonregular Languages
127
14. Prove the following version of the pumping lemma. If L is regular, then there
is an m such that, every w ∈L of length greater than m can be decomposed
as
w = xyz,
with |yz| ≤m and |y| ≥1, such that xyiz is in L for all i.
15. Prove the following generalization of the pumping lemma, which includes
Theorem 4.8 as well as Exercise 1 as special cases.
If L is regular, then there exists an m, such that the following holds for every
suﬃciently long w ∈L and every one of its decompositions w = u1vu2, with
u1, u2 ∈Σ∗, |v| ≥m. The middle string v can be written as v = xyz, with
|xy| ≤m, |y| ≥1, such that u1xyizu2 ∈L for all i = 0, 1, 2, ....
16. Show that the following language is not regular:
L =

anbk : n > k
	
∪

anbk : n ̸= k −1
	
.
17. Prove or disprove the following statement: If L1 and L2 are nonregular lan-
guages, then L1 ∪L2 is also nonregular.
18. Consider the languages below. For each, make a conjecture whether or not it
is regular. Then prove your conjecture.
(a) L =

anblak : n + l + k > 5

.
(b) L =

anblak : n > 5, l > 3, k ≤l

.
(c) L =

anbl : n/l is an integer

.
(d) L =

anbl : n + l is a prime number

.
(e) L =

anbl : n ≤l ≤2n

.
(f) L =

anbl : n ≥100, l ≤100

.
(g) L =

anbl : |n −l| = 2

.
19. Is the following language regular?
{w1cw2 : w1, w2 ∈{a, b}∗, w1 ̸= w2} .
20. Let L1 and L2 be regular languages. Is the language
L =

w : w ∈L1, wR ∈L2
	
necessarily regular?
21. Apply the pigeonhole argument directly to the language in Example 4.8.
22. Are the following languages regular?
(a) L =

uwwRv : u, v, w ∈{a, b}+
.
(b) L =

uwwRv : u, v, w ∈{a, b}+ , |u| ≥|v|

.

128
Chapter 4 Properties of Regular Languages
23. Is the following language regular?
L =

wwRv : v, w ∈{a, b}+	
.
24. Let P be an inﬁnite but countable set, and associate with each p ∈P a
language Lp.
The smallest set containing every Lp is the union over the
inﬁnite set P; it will be denoted by ∪p∈P LP .
Show by example that the
family of regular languages is not closed under inﬁnite union.
25. Consider the argument in Section 3.2 that the language associated with any
generalized transition graph is regular. The language associated with such a
graph is
L =

p∈P
L (rp) ,
where P is the set of all walks through the graph and rp is the expression
associated with a walk p. The set of walks is generally inﬁnite, so that in light
of Exercise 24 it does not immediately follow that L is regular. Show that in
this case, because of the special nature of P, the inﬁnite union is regular.
26. Is the family of regular languages closed under inﬁnite intersection?
27. Suppose that we know that L1 ∪L2 and L1 are regular. Can we conclude
from this that L2 is regular?
28. In the chain code language in Exercise 27, Section 3.1, let L be the set of
all w ∈{u, r, l, d}∗that describe rectangles. Show that L is not a regular
language.
29. Let L = {anbm : n ≥100, m ≤50}.
(a) Can you use the pumping lemma to show that L is regular?
(b) Can you use the pumping lemma to show that L is not regular?
Explain your answers.
30. Show that the language generated by the grammar S →aSS|b is not regular.

5
CHA P T E R
CONTEXT-FREE
LANGUAGES
CHAPTER SUMMARY
Having discovered the limitations of regular languages, now we go on to
study more complicated languages by defining a new type of grammar,
context-free grammars, and associated context-free languages. While
context-free grammars are still fairly simple, they can deal with some
of languages that we know are not regular. This tells us that the family
of regular languages is a proper subset of the family of context-free
languages.
Two important concepts encountered here are the membership ques-
tion and parsing: The membership problem (given a grammar G and a
string w, find if w ∈L(G)) is much more complicated here than it is for
regular languages. So is parsing, which involves not only membership,
but also finding the specific derivation that leads to w. These issues
are only briefly considered in this chapter but will be studied in much
greater depth in later chapters. Here we look only at brute force pars-
ing. Brute force parsing is very general; that is, it works regardless of
the form of the grammar, but it is so inefficient that it is rarely used.
Its importance is primarily theoretical, as it shows that for context-free
language parsing is, at least in principle, always possible. Later we will
study more efficient membership and parsing algorithms.
129

130
Chapter 5 Context-Free Languages
I
n the last chapter, we discovered that not all languages are regu-
lar. While regular languages are eﬀective in describing certain simple
patterns, one does not need to look very far for examples of nonreg-
ular languages.
The relevance of these limitations to programming lan-
guages becomes evident if we reinterpret some of the examples.
If in
L = {anbn : n ≥0} we substitute a left parenthesis for a and a right paren-
thesis for b, then parentheses strings such as (()) and ((())) are in L, but
(() is not. The language therefore describes a simple kind of nested struc-
ture found in programming languages, indicating that some properties of
programming languages require something beyond regular languages.
In
order to cover this and other more complicated features we must enlarge
the family of languages. This leads us to consider context-free languages
and grammars.
We begin this chapter by deﬁning context-free grammars and languages,
illustrating the deﬁnitions with some simple examples. Next, we consider
the important membership problem; in particular we ask how we can tell
if a given string is derivable from a given grammar. Explaining a sentence
through its grammatical derivation is familiar to most of us from a study
of natural languages and is called parsing. Parsing is a way of describing
sentence structure. It is important whenever we need to understand the
meaning of a sentence, as we do for instance in translating from one language
to another. In computer science, this is relevant in interpreters, compilers,
and other translating programs.
The topic of context-free languages is perhaps the most important as-
pect of formal language theory as it applies to programming languages.
Actual programming languages have many features that can be described
elegantly by means of context-free languages. What formal language the-
ory tells us about context-free languages has important applications in the
design of programming languages as well as in the construction of eﬃcient
compilers. We touch upon this brieﬂy in Section 5.3.
5.1
CONTEXT-FREE GRAMMARS
The productions in a regular grammar are restricted in two ways: The
left side must be a single variable, while the right side has a special form.
To create grammars that are more powerful, we must relax some of these
restrictions. By retaining the restriction on the left side, but permitting
anything on the right, we get context-free grammars.
DEFINITION 5.1
A grammar G = (V, T, S, P) is said to be context-free if all productions
in P have the form
A →x,
where A ∈V and x ∈(V ∪T)∗.

5.1 Context-Free Grammars
131
A language L is said to be context-free if and only if there is a context-
free grammar G such that L = L (G).
Every regular grammar is context-free, so a regular language is also a
context-free one. But, as we know from simple examples such as {anbn},
there are nonregular languages. We have already shown in Example 1.11
that this language can be generated by a context-free grammar, so we see
that the family of regular languages is a proper subset of the family of
context-free languages.
Context-free grammars derive their name from the fact that the sub-
stitution of the variable on the left of a production can be made any time
such a variable appears in a sentential form. It does not depend on the
symbols in the rest of the sentential form (the context). This feature is
the consequence of allowing only a single variable on the left side of the
production.
Examples of Context-Free Languages
EXAMPLE 5.1
The grammar G = ({S} , {a, b} , S, P), with productions
S →aSa,
S →bSb,
S →λ,
is context-free. A typical derivation in this grammar is
S ⇒aSa ⇒aaSaa ⇒aabSbaa ⇒aabbaa.
This, and similar derivations, make it clear that
L (G) =

wwR : w ∈{a, b}∗
.
The language is context-free, but as shown in Example 4.8, it is not
regular.

132
Chapter 5 Context-Free Languages
EXAMPLE 5.2
The grammar G, with productions
S →abB,
A →aaBb,
B →bbAa,
A →λ,
is context-free. We leave it to the reader to show that
L (G) = {ab (bbaa)n bba (ba)n : n ≥0} .
Both of the above examples involve grammars that are not only context-
free, but linear. Regular and linear grammars are clearly context-free, but
a context-free grammar is not necessarily linear.
EXAMPLE 5.3
The language
L = {anbm : n ̸= m}
is context-free.
To show this, we need to produce a context-free grammar for the
language. The case of n = m is solved in Example 1.11 and we can
build on that solution. Take the case n > m. We ﬁrst generate a string
with an equal number of a's and b's, then add extra a's on the left.
This is done with
S →AS1,
S1 →aS1b|λ,
A →aA|a.
We can use similar reasoning for the case n < m, and we get the answer
S →AS1|S1B,
S1 →aS1b|λ,
A →aA|a,
B →bB|b.
The resulting grammar is context-free, hence L is a context-free lan-
guage. However, the grammar is not linear.

5.1 Context-Free Grammars
133
The particular form of the grammar given here was chosen for the
purpose of illustration; there are many other equivalent context-free
grammars. In fact, there are some simple linear ones for this language.
In Exercise 26 at the end of this section you are asked to ﬁnd one of
them.
EXAMPLE 5.4
Consider the grammar with productions
S →aSb|SS|λ.
This is another grammar that is context-free, but not linear. Some
strings in L (G) are abaabb, aababb, and ababab. It is not diﬃcult to
conjecture and prove that
L
=
{w ∈{a, b}∗: na (w) = nb (w) and na (v) ≥nb (v) ,
where v is any preﬁx of w}.
(5.1)
We can see the connection with programming languages clearly if we
replace a and b with left and right parentheses, respectively. The lan-
guage L includes such strings as (()) and () () () and is in fact the set
of all properly nested parenthesis structures for the common program-
ming languages.
Here again there are many other equivalent grammars. But, in
contrast to Example 5.3, it is not so easy to see if there are any linear
ones. We will have to wait until Chapter 8 before we can answer this
question.
Leftmost and Rightmost Derivations
In a grammar that is not linear, a derivation may involve sentential forms
with more than one variable.
In such cases, we have a choice in the
order in which variables are replaced.
Take, for example, the grammar
G = ({A, B, S} , {a, b} , S, P) with productions
1. S →AB.
2. A →aaA.
3. A →λ.
4. B →Bb.
5. B →λ.

134
Chapter 5 Context-Free Languages
This grammar generates the language L (G) =

a2nbm : n ≥0, m ≥0}.
Carry out a few derivations to convince yourself of this.
Consider now the two derivations
S
1⇒AB
2⇒aaAB
3⇒aaB
4⇒aaBb
5⇒aab
and
S
1⇒AB
4⇒ABb
2⇒aaABb
5⇒aaAb
3⇒aab.
In order to show which production is applied, we have numbered the pro-
ductions and written the appropriate number on the ⇒symbol. From this
we see that the two derivations not only yield the same sentence but also
use exactly the same productions. The diﬀerence is entirely in the order in
which the productions are applied. To remove such irrelevant factors, we
often require that the variables be replaced in a speciﬁc order.
DEFINITION 5.2
A derivation is said to be leftmost if in each step the leftmost variable
in the sentential form is replaced. If in each step the rightmost variable is
replaced, we call the derivation rightmost.
EXAMPLE 5.5
Consider the grammar with productions
S →aAB,
A →bBb,
B →A|λ.
Then
S ⇒aAB ⇒abBbB ⇒abAbB ⇒abbBbbB ⇒abbbbB ⇒abbbb
is a leftmost derivation of the string abbbb. A rightmost derivation of
the same string is
S ⇒aAB ⇒aA ⇒abBb ⇒abAb ⇒abbBbb ⇒abbbb.
Derivation Trees
A second way of showing derivations, independent of the order in which
productions are used, is by a derivation or parse tree. A derivation tree is

5.1 Context-Free Grammars
135
B
b
a
c
A
A
FIGURE 5.1
an ordered tree in which nodes are labeled with the left sides of productions
and in which the children of a node represent its corresponding right sides.
For example, Figure 5.1 shows part of a derivation tree representing the
production
A →abABc.
In a derivation tree, a node labeled with a variable occurring on the left
side of a production has children consisting of the symbols on the right side
of that production. Beginning with the root, labeled with the start symbol
and ending in leaves that are terminals, a derivation tree shows how each
variable is replaced in the derivation. The following deﬁnition makes this
notion precise.
DEFINITION 5.3
Let G = (V, T, S, P) be a context-free grammar.
An ordered tree is a
derivation tree for G if and only if it has the following properties.
1. The root is labeled S.
2. Every leaf has a label from T ∪{λ}.
3. Every interior vertex (a vertex that is not a leaf) has a label from V .
4. If a vertex has label A ∈V , and its children are labeled (from left to
right) a1, a2, ..., an, then P must contain a production of the form
A →a1a2 · · · an.
5. A leaf labeled λ has no siblings, that is, a vertex with a child labeled λ
can have no other children.
A tree that has properties 3, 4, and 5, but in which 1 does not necessarily
hold and in which property 2 is replaced by
2a. Every leaf has a label from V ∪T ∪{λ},
is said to be a partial derivation tree.

136
Chapter 5 Context-Free Languages
The string of symbols obtained by reading the leaves of the tree from
left to right, omitting any λ's encountered, is said to be the yield of the tree.
The descriptive term left to right can be given a precise meaning. The yield
is the string of terminals in the order they are encountered when the tree
is traversed in a depth-ﬁrst manner, always taking the leftmost unexplored
branch.
EXAMPLE 5.6
Consider the grammar G, with productions
S →aAB,
A →bBb,
B →A|λ.
The tree in Figure 5.2 is a partial derivation tree for G, while the tree
in Figure 5.3 is a derivation tree. The string abBbB, which is the yield
of the ﬁrst tree, is a sentential form of G. The yield of the second
tree, abbbb, is a sentence of L (G).
B
a
S
A
b
b
B
FIGURE 5.2
a
S
A
B
b
b
B
λ
A
b
b
B
λ
FIGURE 5.3

5.1 Context-Free Grammars
137
Relation Between Sentential Forms and Derivation Trees
Derivation trees give a very explicit and easily comprehended description
of a derivation. Like transition graphs for ﬁnite automata, this explicitness
is a great help in making arguments. First, though, we must establish the
connection between derivations and derivation trees.
THEOREM 5.1
Let G = (V, T, S, P) be a context-free grammar. Then for every w ∈L (G),
there exists a derivation tree of G whose yield is w. Conversely, the yield of
any derivation tree is in L (G). Also, if tG is any partial derivation tree for
G whose root is labeled S, then the yield of tG is a sentential form of G.
Proof: First we show that for every sentential form of L (G) there is a cor-
responding partial derivation tree. We do this by induction on the number
of steps in the derivation. As a basis, we note that the claimed result is true
for every sentential form derivable in one step. Since S ⇒u implies that
there is a production S →u, this follows immediately from Deﬁnition 5.3.
Assume that for every sentential form derivable in n steps, there is a
corresponding partial derivation tree. Now any w derivable in n + 1 steps
must be such that
S
∗⇒xAy,
x, y ∈(V ∪T)∗,
A ∈V,
in n steps, and
xAy ⇒xa1a2 · · · amy = w, ai ∈V ∪T.
Since by the inductive assumption there is a partial derivation tree with
yield xAy, and since the grammar must have production A →a1a2 · · · am,
we see that by expanding the leaf labeled A, we get a partial derivation tree
with yield xa1a2 · · · amy = w. By induction, we therefore claim that the
result is true for all sentential forms.
In a similar vein, we can show that every partial derivation tree repre-
sents some sentential form. We will leave this as an exercise.
Since a derivation tree is also a partial derivation tree whose leaves are
terminals, it follows that every sentence in L (G) is the yield of some deriva-
tion tree of G and that the yield of every derivation tree is in L (G).
Derivation trees show which productions are used in obtaining a sen-
tence, but do not give the order of their application. Derivation trees are
able to represent any derivation, reﬂecting the fact that this order is irrel-
evant, an observation that allows us to close a gap in the preceding dis-
cussion. By deﬁnition, any w ∈L (G) has a derivation, but we have not

138
Chapter 5 Context-Free Languages
claimed that it also had a leftmost or rightmost derivation. However, once
we have a derivation tree, we can always get a leftmost derivation by think-
ing of the tree as having been built in such a way that the leftmost variable
in the tree was always expanded ﬁrst. Filling in a few details, we are led to
the not surprising result that any w ∈L (G) has a leftmost and a rightmost
derivation (for details, see Exercise 25 at the end of this section).
EXERCISES
1. Find context-free grammars for the following languages:
(a) L = anbn, n is even.
(b) L = anbn, n is odd.
(c) L = anbn, n is a multiple of three.
(d) L = anbn, n is not a multiple of three.
2. Show a derivation tree for w = aaabbaaa using the grammar in Example 5.1.
3. Draw the derivation tree corresponding to the derivation in Example 5.1.
4. Give a derivation tree for w = abbbaabbaba for the grammar in Example 5.2.
5. Complete the arguments in Example 5.2, showing that the language given is
generated by the grammar.
6. Show that the grammar in Example 5.4 does in fact generate the language
described in Equation 5.1.
7. Is the language in Example 5.2 regular?
8. Complete the proof in Theorem 5.1 by showing that the yield of every partial
derivation tree with root S is a sentential form of G.
9. Find context-free grammars for the following languages (with n
≥
0,
m ≥0).
(a) L = {anbm : n ≤m + 3}.
(b) L = {anbm : n = m −1}.
(c) L = {anbm : n ̸= 2m}.
(d) L = {anbm : 2n ≤m ≤3n}.
(e) L = {w ∈{a, b}∗: na (w) ̸= nb (w)}.
(f) L = {w ∈{a, b}∗: na (v) ≥nb (v) , where v is any preﬁx of w}.
(g) L = {w ∈{a, b}∗: na (w) = 2nb (w) + 1}.
(h) L = {w ∈{a, b}∗: na (w) = nb (w) + 2}.

5.1 Context-Free Grammars
139
10. What language does the grammar
S →aSB|bSA
S →λ
A →a
B →b
generate?
11. What language does the grammar
S →aaSbb|SS
S →λ
12. Find context-free grammars for the following languages (with n ≥0, m ≥0,
k ≥0):
(a) L =

anbmck : n = m or m ≤k

.
(b) L =

anbmck : n = m or m ̸= k

.
(c) L = {anbmck, k = n + m}.
(d) L = {anbmck, k = |n −m|}.
(e) L = {anbmck, k = n + 2m}.
(f) L = {anbmck, k ̸= n + m}.
13. Find a context-free grammar for
L = {w ∈{[a, b, c}∗.na(w) + nb(w) ̸= nc(w)}.
14. Show that L = {w ∈{a, b, c}∗: |w| = 3na(w)} is a context-free language.
15. Find a context-free grammar for Σ = {a, b} for the language
L =

anwwRbn (w ∈Σ∗, n ≥1} .
16. Let L = {anbn : n ≥0}.
(a) Show that L2 is context-free.
(b) Show that Lk is context-free for all k > 1.
(c) Show that L and L∗are context-free.
17. Let L1 be the language in Exercise 12(a) and L2 be the language in Exercise
12(d). Show that L1 ∪L2 is a context-free language.
18. Show that the following language is context-free:
L =

uvwvR : u, v, w ∈{a, b}+ , |u| = |w| = 2

.
19. Show that the complement of the language in Example 5.1 is context-free.

140
Chapter 5 Context-Free Languages
20. Show that the complement of the language in Exercise 12(c) is context-free.
21. Show that the language L =

w1cw2 : w1, w2 ∈{a, b}+ , w1 ̸= wR
2

, with Σ =
{a, b, c}, is context-free.
22. Show a derivation tree for the string aabbbb with the grammar
S →AB|λ,
A →aB,
B →Sb.
Give a verbal description of the language generated by this grammar.
23. Consider the grammar with productions
S →aaB,
A →bBb|λ,
B →Aa.
Show that the string aabbabba is not in the language generated by this
grammar.
24. Deﬁne what one might mean by properly nested parenthesis structures in-
volving two kinds of parentheses, say () and [ ]. Intuitively, properly nested
strings in this situation are ([ ]), ([[ ]]) [()], but not ([)] or ((]]. Using your
deﬁnition, give a context-free grammar for generating all properly nested
parentheses.
25. Find a context-free grammar for the set of all regular expressions on the
alphabet {a, b}.
26. Find a context-free grammar that can generate all the production rules for
context-free grammars with T = {a, b} and V = {A, B, C}.
27. Prove that if G is a context-free grammar, then every w ∈L (G) has a leftmost
and rightmost derivation. Give an algorithm for ﬁnding such derivations from
a derivation tree.
28. Find a linear grammar for the language in Example 5.3.
29. Let G = (V, T, S, P) be a context-free grammar such that every one of its
productions is of the form A →v, with |v| = k > 1. Show that the derivation
tree for any w ∈L (G) has a height h such that
logk |w| ≤h ≤(|w| −1)
k −1
.
5.2
PARSING AND AMBIGUITY
We have so far concentrated on the generative aspects of grammars. Given
a grammar G, we studied the set of strings that can be derived using G. In
cases of practical applications, we are also concerned with the analytical side
of the grammar: Given a string w of terminals, we want to know whether

5.2 Parsing and Ambiguity
141
or not w is in L (G). If so, we may want to ﬁnd a derivation of w. An
algorithm that can tell us whether w is in L (G) is a membership algorithm.
The term parsing describes ﬁnding a sequence of productions by which a
w ∈L (G) is derived.
Parsing and Membership
Given a string w in L (G), we can parse it in a rather obvious fashion:
We systematically construct all possible (say, leftmost) derivations and see
whether any of them match w. Speciﬁcally, we start at round one by looking
at all productions of the form
S →x,
ﬁnding all x that can be derived from S in one step.
If none of these
results in a match with w, we go to the next round, in which we apply
all applicable productions to the leftmost variable of every x. This gives
us a set of sentential forms, some of them possibly leading to w. On each
subsequent round, we again take all leftmost variables and apply all possible
productions. It may be that some of these sentential forms can be rejected
on the grounds that w can never be derived from them, but in general, we
will have on each round a set of possible sentential forms. After the ﬁrst
round, we have sentential forms that can be derived by applying a single
production, after the second round we have the sentential forms that can be
derived in two steps, and so on. If w ∈L (G), then it must have a leftmost
derivation of ﬁnite length. Thus, the method will eventually give a leftmost
derivation of w.
For reference below, we will call this exhaustive search parsing or
brute force parsing. It is a form of top-down parsing, which we can
view as the construction of a derivation tree from the root down.
EXAMPLE 5.7
Consider the grammar
S →SS |aSb| bSa|λ
and the string w = aabb. Round one gives us
1. S ⇒SS,
2. S ⇒aSb,
3. S ⇒bSa,
4. S ⇒λ.

142
Chapter 5 Context-Free Languages
The last two of these can be removed from further consideration for
obvious reasons. Round two then yields sentential forms
S ⇒SS ⇒SSS,
S ⇒SS ⇒aSbS,
S ⇒SS ⇒bSaS,
S ⇒SS ⇒S,
which are obtained by replacing the leftmost S in sentential form 1
with all applicable substitutes. Similarly, from sentential form 2 we
get the additional sentential forms
S ⇒aSb ⇒aSSb,
S ⇒aSb ⇒aaSbb,
S ⇒aSb ⇒abSab,
S ⇒aSb ⇒ab.
Again, several of these can be removed from contention. On the next
round, we ﬁnd the actual target string from the sequence
S ⇒aSb ⇒aaSbb ⇒aabb.
Therefore, aabb is in the language generated by the grammar under
consideration.
Exhaustive search parsing has serious ﬂaws.
The most obvious one
is its tediousness; it is not to be used where eﬃcient parsing is required.
But even when eﬃciency is a secondary issue, there is a more pertinent
objection. While the method always parses a w ∈L (G), it is possible that
it never terminates for strings not in L (G). This is certainly the case in
the previous example; with w = abb, the method will go on producing trial
sentential forms indeﬁnitely unless we build into it some way of stopping.
The problem of nontermination of exhaustive search parsing is relatively
easy to overcome if we restrict the form that the grammar can have. If we
examine Example 5.7, we see that the diﬃculty comes from the productions
S →λ; this production can be used to decrease the length of successive
sentential forms, so that we cannot tell easily when to stop. If we do not
have any such productions, then we have many fewer diﬃculties. In fact,
there are two types of productions we want to rule out, those of the form
A →λ as well as those of the form A →B. As we will see in the next
chapter, this restriction does not aﬀect the power of the resulting grammars
in any signiﬁcant way.

5.2 Parsing and Ambiguity
143
EXAMPLE 5.8
The grammar
S →SS |aSb| bSa |ab| ba
satisﬁes the given requirements. It generates the language in Example
5.7 without the empty string.
Given any w ∈{a, b}+, the exhaustive search parsing method will
always terminate in no more than |w| rounds. This is clear because
the length of the sentential form grows by at least one symbol in each
round. After |w| rounds we have either produced a parsing or we know
that w /∈L (G).
The idea in this example can be generalized and made into a theorem
for context-free languages in general.
THEOREM 5.2
Suppose that G = (V, T, S, P) is a context-free grammar that does not have
any rules of the form
A →λ,
or
A →B,
where A, B ∈V . Then the exhaustive search parsing method can be made
into an algorithm that, for any w ∈Σ∗, either produces a parsing of w or
tells us that no parsing is possible.
Proof: For each sentential form, consider both its length and the number
of terminal symbols.
Each step in the derivation increases at least one
of these. Since neither the length of a sentential form nor the number of
terminal symbols can exceed |w|, a derivation cannot involve more than
2 |w| rounds, at which time we either have a successful parsing or w cannot
be generated by the grammar.
While the exhaustive search method gives a theoretical guarantee that
parsing can always be done, its practical usefulness is limited because the
number of sentential forms generated by it may be excessively large. Exactly
how many sentential forms are generated diﬀers from case to case; no precise
general result can be established, but we can put some rough upper bounds

144
Chapter 5 Context-Free Languages
on it. If we restrict ourselves to leftmost derivations, we can have no more
than |P| sentential forms after one round, no more than |P|2 sentential
forms after the second round, and so on. In the proof of Theorem 5.2, we
observed that parsing cannot involve more than 2 |w| rounds; therefore, the
total number of sentential forms cannot exceed
M = |P| + |P|2 + · · · + |P|2|w|
= O(P 2|w|+1).
(5.2)
This indicates that the work for exhaustive search parsing may grow ex-
ponentially with the length of the string, making the cost of the method
prohibitive. Of course, Equation (5.2) is only a bound, and often the num-
ber of sentential forms is much smaller. Nevertheless, practical observation
shows that exhaustive search parsing is very ineﬃcient in most cases.
The construction of more eﬃcient parsing methods for context-free
grammars is a complicated matter that belongs to a course on compilers.
We will not pursue it here except for some isolated results.
THEOREM 5.3
For every context-free grammar there exists an algorithm that parses any
w ∈L (G) in a number of steps proportional to |w|3.
There are several known methods to achieve this, but all of them are
suﬃciently complicated that we cannot even describe them without devel-
oping some additional results. In Section 6.3 we will take this question up
again brieﬂy. More details can be found in Harrison 1978 and Hopcroft and
Ullman 1979. One reason for not pursuing this in detail is that even these
algorithms are unsatisfactory. A method in which the work rises with the
third power of the length of the string, while better than an exponential
algorithm, is still quite ineﬃcient, and a parser based on it would need an
excessive amount of time to analyze even a moderately long program. What
we would like to have is a parsing method that takes time proportional to
the length of the string. We refer to such a method as a linear time parsing
algorithm. We do not know any linear time parsing methods for context-
free languages in general, but such algorithms can be found for restricted,
but important, special cases.
DEFINITION 5.4
A context-free grammar G = (V, T, S, P) is said to be a simple grammar
or s-grammar if all its productions are of the form
A →ax,
where A ∈V , a ∈T, x ∈V ∗, and any pair (A, a) occurs at most once in P.

5.2 Parsing and Ambiguity
145
EXAMPLE 5.9
The grammar
S →aS |bSS| c
is an s-grammar. The grammar
S →aS |bSS| aSS|c
is not an s-grammar because the pair (S, a) occurs in the two produc-
tions S →aS and S →aSS.
While s-grammars are quite restrictive, they are of some interest. As
we will see in the next section, many features of common programming
languages can be described by s-grammars.
If G is an s-grammar, then any string w in L (G) can be parsed with an
eﬀort proportional to |w|. To see this, look at the exhaustive search method
and the string w = a1a2 · · · an. Since there can be at most one rule with
S on the left, and starting with a1 on the right, the derivation must begin
with
S ⇒a1A1A2 · · · Am.
Next, we substitute for the variable A1, but since again there is at most one
choice, we must have
S
∗⇒a1a2B1B2 · · · A2 · · · Am.
We see from this that each step produces one terminal symbol and hence
the whole process must be completed in no more than |w| steps.
Ambiguity in Grammars and Languages
On the basis of our argument we can claim that given any w ∈L (G),
exhaustive search parsing will produce a derivation tree for w. We say "a"
derivation tree rather than "the" derivation tree because of the possibility
that a number of diﬀerent derivation trees may exist.
This situation is
referred to as ambiguity.
DEFINITION 5.5
A context-free grammar G is said to be ambiguous if there exists some w ∈
L (G) that has at least two distinct derivation trees. Alternatively, ambigu-
ity
implies
the
existence
of
two
or
more
leftmost
or
rightmost
derivations.

146
Chapter 5 Context-Free Languages
EXAMPLE 5.10
The grammar in Example 5.4, with productions S →aSb |SS| λ, is
ambiguous. The sentence aabb has the two derivation trees shown in
Figure 5.4.
a
S
S
S
S
a
S  
λ
b
b
a
S
S
a
S  
λ
b
b
λ
FIGURE 5.4
Ambiguity is a common feature of natural languages, where it is tol-
erated and dealt with in a variety of ways.
In programming languages,
where there should be only one interpretation of each statement, ambiguity
must be removed when possible. Often we can achieve this by rewriting the
grammar in an equivalent, unambiguous form.
EXAMPLE 5.11
Consider the grammar G = (V, T, E, P) with
V = {E, I} ,
T = {a, b, c, +, ∗, (, )} ,
and productions
E →I,
E →E + E,
E →E∗E,
E →(E) ,
I →a |b|c .

5.2 Parsing and Ambiguity
147
The strings (a + b) ∗c and a∗b + c are in L (G). It is easy to see that
this grammar generates a restricted subset of arithmetic expressions
for C-like programming languages. The grammar is ambiguous. For
instance, the string a+b∗c has two diﬀerent derivation trees, as shown
in Figure 5.5.
I
+
E
E
a
I
I
*
b
E
E
E
E
E
E
c
*
E
I
b
I
c
+
I
a
E
E
E
(a)
(b)
FIGURE 5.5
Two derivation trees for a + b∗c.
One way to resolve the ambiguity is, as is done in programming man-
uals, to associate precedence rules with the operators + and ∗. Since ∗
normally has higher precedence than +, we would take Figure 5.5(a) as the
correct parsing as it indicates that b∗c is a subexpression to be evaluated
before performing the addition. However, this resolution is completely out-
side the grammar. It is better to rewrite the grammar so that only one
parsing is possible.
EXAMPLE 5.12
To rewrite the grammar in Example 5.11 we introduce new variables,
taking V as {E, T, F, I}, and replacing the productions with
E →T,
T →F,
F →I,
E →E + T,
T →T ∗F,

148
Chapter 5 Context-Free Languages
F →(E),
I →a |b| c.
A derivation tree of the sentence a + b ∗c is shown in Figure 5.6.
No other derivation tree is possible for this string: The grammar is
unambiguous. It is also equivalent to the grammar in Example 5.11.
It is not too hard to justify these claims in this speciﬁc instance, but,
in general, the questions of whether a given context-free grammar is
ambiguous or whether two given context-free grammars are equivalent
are very diﬃcult to answer. In fact, we will later show that there are
no general algorithms by which these questions can always be resolved.
T
+
E
T
F
F
I
*
I
T
E
F
c
b
I
a
FIGURE 5.6
In the foregoing example the ambiguity came from the grammar in
the sense that it could be removed by ﬁnding an equivalent unambiguous
grammar.
In some instances, however, this is not possible because the
ambiguity is in the language.
DEFINITION 5.6
If L is a context-free language for which there exists an unambiguous gram-
mar, then L is said to be unambiguous. If every grammar that generates L
is ambiguous, then the language is called inherently ambiguous.

5.2 Parsing and Ambiguity
149
It is a somewhat diﬃcult matter even to exhibit an inherently am-
biguous language. The best we can do here is give an example with some
reasonably plausible claim that it is inherently ambiguous.
EXAMPLE 5.13
The language
L = {anbncm} ∪{anbmcm},
with n and m nonnegative, is an inherently ambiguous context-free
language.
That L is context-free is easy to show. Notice that
L = L1 ∪L2,
where L1 is generated by
S1 →S1c|A,
A →aAb|λ
and L2 is given by an analogous grammar with start symbol S2 and
productions
S2 →aS2|B,
B →bBc|λ.
Then L is generated by the combination of these two grammars with
the additional production
S →S1|S2.
The grammar is ambiguous since the string anbncn has two distinct
derivations, one starting with S ⇒S1, the other with S ⇒S2. It does
not, of course, follow from this that L is inherently ambiguous as there
might exist some other unambiguous grammars for it. But in some way
L1 and L2 have conﬂicting requirements, the ﬁrst putting a restriction
on the number of a's and b's, while the second does the same for b's
and c's. A few tries will quickly convince you of the impossibility of
combining these requirements in a single set of rules that cover the case
n = m uniquely. A rigorous argument, though, is quite technical. One
proof can be found in Harrison 1978.

150
Chapter 5 Context-Free Languages
EXERCISES
1. Show that
S →aS|bS|cA
A →aA|bS
is an s-grammar.
2. Show that every s-grammar is unambiguous.
3. Find an s-grammar for L (aaa∗b + ab∗).
4. Find an s-grammar for L = {anbn : n ≥2}.
5. Find an s-grammar for L =

anb2n : n ≥2

.
6. Find an s-grammar for L =

anbn+1 : n ≥1

.
7. Let G = (V, T, S, P) be an s-grammar. Give an expression for the maximum
size of P in terms of |V | and |T|.
8. Show that the following grammar is ambiguous:
S →AB|aaaB,
A →a|Aa,
B →b.
9. Construct an unambiguous grammar equivalent to the grammar in Exercise 8.
10. Give the derivation tree for ((a+b)∗c+d, using the grammar in Example 5.12.
11. Give the derivation tree for a∗b+((c+d)), using the grammar in Example 5.12.
12. Give the derivation tree for (((a + b) ∗c)) + a + b, using the grammar in
Example 5.12.
13. Show that a regular language cannot be inherently ambiguous.
14. Give an unambiguous grammar that generates the set of all regular expres-
sions on Σ = {a, b}.
15. Is it possible for a regular grammar to be ambiguous?
16. Show that the language L
=

wwR : w ∈{a, b}∗
is not inherently
ambiguous.
17. Show that the following grammar is ambiguous:
S →aSbS |bSaS| λ.
18. Show that the grammar in Example 5.4 is ambiguous, but that the language
denoted by it is not.
19. Show that the grammar in Example 1.13 is ambiguous.
20. Show that the grammar in Example 5.5 is unambiguous.

5.3 Context-Free Grammars and Programming Languages
151
21. Use the exhaustive search parsing method to parse the string abbbbbb with
the grammar in Example 5.5. In general, how many rounds will be needed
to parse any string w in this language?
22. Is the string aabbababb in the language generated by the grammar S →aSS|b?
23. Show that the grammar in Example 1.14 is unambiguous.
24. Prove the following result. Let G = (V, T, S, P) be a context-free grammar in
which every A ∈V occurs on the left side of at most one production. Then
G is unambiguous.
25. Find a grammar equivalent to that in Example 5.5 that satisﬁes the conditions
of Theorem 5.2.
5.3
CONTEXT-FREE GRAMMARS AND PROGRAMMING
LANGUAGES
One of the most important uses of the theory of formal languages is in the
deﬁnition of programming languages and in the construction of interpreters
and compilers for them. The basic problem here is to deﬁne a programming
language precisely and to use this deﬁnition as the starting point for the
writing of eﬃcient and reliable translation programs.
Both regular and
context-free languages are important in achieving this. As we have seen,
regular languages are used in the recognition of certain simple patterns that
occur in programming languages, but as we argue in the introduction to this
chapter, we need context-free languages to model more complicated aspects.
As with most other languages, we can deﬁne a programming language
by a grammar. It is traditional in writing on programming languages to
use a convention for specifying grammars called the Backus-Naur form or
BNF. This form is in essence the same as the notation we have used here,
but the appearance is diﬀerent. In BNF, variables are enclosed in triangular
brackets. Terminal symbols are written without any special marking. BNF
also uses subsidiary symbols such as |, much in the way we have done. Thus,
the grammar in Example 5.12 might appear in BNF as
⟨expression⟩::= ⟨term⟩| ⟨expression⟩+ ⟨term⟩,
⟨term⟩::= ⟨factor⟩| ⟨term⟩∗⟨factor⟩,
and so on.
The symbols + and ∗are terminals.
The symbol | is used
as an alternator as in our notation, but ::= is used instead of →. BNF
descriptions of programming languages tend to use more explicit variable
identiﬁers to make the intent of the production explicit. But otherwise there
are no signiﬁcant diﬀerences between the two notations.
Many parts of C-like programming languages are susceptible to deﬁni-
tion by restricted forms of context-free grammars. For example, the while

152
Chapter 5 Context-Free Languages
statement in C can be deﬁned as
⟨while statement⟩::= while⟨expression⟩⟨statement⟩.
Here the keyword while is a terminal symbol. All other terms are vari-
ables, which still have to be deﬁned. If we check this against Deﬁnition
5.4, we see that this looks like an s-grammar production.
The variable
⟨while statement⟩on the left is always associated with the terminal while
on the right. For this reason such a statement is easily and eﬃciently parsed.
We see here a reason why we use keywords in programming languages. Key-
words not only provide some visual structure that can guide the reader of
a program, but also make the work of a compiler much easier.
Unfortunately, not all features of a typical programming language can
be expressed by an s-grammar. The rules for ⟨expression⟩above are not of
this type, so that parsing becomes less obvious. The question then arises
what grammatical rules we can permit and still parse eﬃciently. In compil-
ers, extensive use has been made of what are called LL and LR grammars.
These grammars have the ability to express the less obvious features of a
programming language, yet allow us to parse in linear time. This is not a
simple matter, and much of it is beyond the scope of our discussion. We
will brieﬂy touch on this topic in Chapter 6, but for our purposes it suﬃces
to realize that such grammars exist and have been widely studied.
In connection with this, the issue of ambiguity takes on added signiﬁ-
cance. The speciﬁcation of a programming language must be unambiguous,
otherwise a program may yield very diﬀerent results when processed by
diﬀerent compilers or run on diﬀerent systems. As Example 5.11 shows, a
naive approach can easily introduce ambiguity in the grammar. To avoid
such mistakes we must be able to recognize and remove ambiguities.
A
related question is whether a language is or is not inherently ambiguous.
What we need for this purpose are algorithms for detecting and remov-
ing ambiguities in context-free grammars and for deciding whether or not
a context-free language is inherently ambiguous. Unfortunately, these are
very diﬃcult tasks, impossible in the most general sense, as we will see later.
Those aspects of a programming language that can be modeled by a
context-free grammar are usually referred to as its syntax. However, it is
normally the case that not all programs that are syntactically correct in
this sense are in fact acceptable programs. For C, the usual BNF deﬁnition
allows constructs such as
char
a, b, c;
followed by
c = 3.2;
This combination is not acceptable to C compilers since it violates the con-
straint, "a character variable cannot be assigned a real value." Context-free

5.3 Context-Free Grammars and Programming Languages
153
grammars cannot express the fact that type clashes may not be permitted.
Such rules are part of programming language semantics, since they have to
do with how we interpret the meaning of a particular construct.
Programming language semantics are a complicated matter. Nothing
as elegant and concise as context-free grammars exists for the speciﬁcation
of programming language semantics, and consequently some semantic fea-
tures may be poorly deﬁned or ambiguous. It is an ongoing concern both
in programming languages and in formal language theory to ﬁnd eﬀective
methods for deﬁning programming language semantics. Several methods
have been proposed, but none of them has been as universally accepted and
are as successful for semantic deﬁnition as context-free languages have been
for syntax.
EXERCISES
1. Suppose that in a certain programming language numbers are constructed
according to the following rules:
(a) A sign is optional.
(b) The value ﬁeld consists of two nonempty parts, separated by a
decimal point.
(c) An exponent ﬁeld is optional. When present, the exponent ﬁeld
must consist of the letter e and a signed two-digit integer.
Find a grammar that formalizes these requirements.
2. Consult a book on C for formal deﬁnitions of the following constructs:
Literal
For statement
If-else statement
Do statement
Compound statement
Return statement
3. Find examples of features of C that cannot be described by context-free
grammars.


6
CHA P T E R
SIMPLIFICATION OF
CONTEXT-FREE GRAMMARS
AND NORMAL FORMS
CHAPTER SUMMARY
In Chapter 5, we encountered the important issues of membership and
parsing for context-free languages. While brute force parsing is always
possible, it is very inefficient and therefore not practical.
For actual
applications, such as compilers, we need more efficient methods. This
is made difficult by the unrestricted form of the right side of a context-
free production, so we investigate if we can restrict the right side without
reducing the power of the grammar.
The first thing we can do is to show that we need not worry about
certain types of productions. In particular, if a context-free grammar
has in it productions that have λ on the right, we can find an equivalent
grammar without such λ-productions. In a similar vein, we can remove
unit-productions that have only a single variable on the right and useless
productions that cannot ever occur in the derivation of a string.
We also discuss normal forms, that is, grammatical forms that are
very restricted but are nevertheless general in the sense that any context-
free grammar has an equivalent in normal form. One can define many
kinds of normal forms; here we discuss only two of the most useful:
Chomsky normal form and Greibach normal form.
155

156
Chapter 6 Simpliﬁcation of Context-Free Grammars and Normal Forms
B
efore we can study context-free languages in greater depth, we must
attend to some technical matters. The deﬁnition of a context-free
grammar imposes no restriction whatsoever on the right side of a
production. However, complete freedom is not necessary and, in fact, is
a detriment in some arguments. In Theorem 5.2, we see the convenience
of certain restrictions on grammatical forms; eliminating rules of the form
A →λ and A →B make the arguments easier. In many instances, it is
desirable to place even more stringent restrictions on the grammar. Because
of this, we need to look at methods for transforming an arbitrary context-
free grammar into an equivalent one that satisﬁes certain restrictions on its
form. In this chapter we study several transformations and substitutions
that will be useful in subsequent discussions.
We also investigate normal forms for context-free grammars. A nor-
mal form is one that, although restricted, is broad enough so that any
grammar has an equivalent normal-form version. We introduce two of the
most useful of these, the Chomsky normal form and the Greibach nor-
mal form. Both have many practical and theoretical uses. An immediate
application of the Chomsky normal form to parsing is given in Section 6.3.
The somewhat tedious nature of the material in this chapter lies in the
fact that many of the arguments are manipulative and give little intuitive
insight. For our purposes, this technical aspect is relatively unimportant
and can be read casually. The various conclusions are signiﬁcant; they will
be used many times in later discussions.
6.1
METHODS FOR TRANSFORMING GRAMMARS
We ﬁrst raise an issue that is somewhat of a nuisance with grammars and
languages in general: the presence of the empty string. The empty string
plays a rather singular role in many theorems and proofs, and it is often
necessary to give it special attention. We prefer to remove it from consider-
ation altogether, looking only at languages that do not contain λ. In doing
so, we do not lose generality, as we see from the following considerations.
Let L be any context-free language, and let G = (V, T, S, P) be a context-
free grammar for L −{λ}. Then the grammar we obtain by adding to V
the new variable S0, making S0 the start variable, and adding to P the
productions
S0 →S|λ
generates L. Therefore, any nontrivial conclusion we can make for L −{λ}
will almost certainly transfer to L. Also, given any context-free grammar
G, there is a method for obtaining G such that L

G

= L (G) −{λ}
(see Exercises 15 and 16 at the end of this section). Consequently, for all
practical purposes, there is no diﬀerence between context-free languages

6.1 Methods for Transforming Grammars
157
that include λ and those that do not. For the rest of this chapter, unless
otherwise stated, we will restrict our discussion to λ-free languages.
A Useful Substitution Rule
Many rules govern generating equivalent grammars by means of substitu-
tions.
Here we give one that is very useful for simplifying grammars in
various ways. We will not deﬁne the term simpliﬁcation precisely, but we
will use it nevertheless. What we mean by it is the removal of certain types
of undesirable productions; the process does not necessarily result in an
actual reduction of the number of rules.
THEOREM 6.1
Let G = (V, T, S, P) be a context-free grammar. Suppose that P contains
a production of the form
A →x1Bx2.
Assume that A and B are diﬀerent variables and that
B →y1 |y2| · · · |yn
is the set of all productions in P that have B as the left side. Let G =

V, T, S, P

be the grammar in which P is constructed by deleting
A →x1Bx2
(6.1)
from P, and adding to it
A →x1y1x2 |x1y2x2| · · · |x1ynx2.
Then
L

G

= L (G) .
Proof: Suppose that w ∈L (G), so that
S
∗⇒G w.
The subscript on the derivation sign ⇒is used here to distinguish between
derivations with diﬀerent grammars. If this derivation does not involve the
production (6.1), then obviously
S
∗⇒
G w.

158
Chapter 6 Simpliﬁcation of Context-Free Grammars and Normal Forms
If it does, then look at the derivation the ﬁrst time (6.1) is used. The B so
introduced eventually has to be replaced; we lose nothing by assuming that
this is done immediately (see Exercise 18 at the end of this section). Thus
S
∗⇒G u1Au2 ⇒G u1x1Bx2u2 ⇒G u1x1yjx2u2.
But with grammar G we can get
S
∗⇒
G u1Au2 ⇒
G u1x1yjx2u2.
Thus we can reach the same sentential form with G and G. If (6.1) is used
again later, we can repeat the argument. It follows then, by induction on
the number of times the production is applied, that
S
∗⇒
G w.
Therefore, if w ∈L (G), then w ∈L

G

.
By similar reasoning, we can show that if w ∈L

G

, then w ∈L (G),
completing the proof.
Theorem 6.1 is a simple and quite intuitive substitution rule: A produc-
tion A →x1Bx2 can be eliminated from a grammar if we put in its place
the set of productions in which B is replaced by all strings it derives in one
step. In this result, it is necessary that A and B be diﬀerent variables. The
case when A = B is partially addressed in Exercises 25 and 26 at the end
of this section.
EXAMPLE 6.1
Consider G = ({A, B} , {a, b, c} , A, P) with productions
A →a |aaA| abBc,
B →abbA|b.
Using the suggested substitution for the variable B, we get the gram-
mar G with productions
A →a |aaA| ababbAc|abbc,
B →abbA|b.
The new grammar G is equivalent to G. The string aaabbc has the
derivation
A ⇒aaA ⇒aaabBc ⇒aaabbc

6.1 Methods for Transforming Grammars
159
in G, and the corresponding derivation
A ⇒aaA ⇒aaabbc
in G.
Notice that, in this case, the variable B and its associated pro-
ductions are still in the grammar even though they can no longer play
a part in any derivation.
We will next show how such unnecessary
productions can be removed from a grammar.
Removing Useless Productions
One invariably wants to remove productions from a grammar that can never
take part in any derivation.
For example, in the grammar whose entire
production set is
S →aSb |λ| A,
A →aA,
the production S →A clearly plays no role, as A cannot be transformed
into a terminal string. While A can occur in a string derived from S, this
can never lead to a sentence. Removing this production leaves the language
unaﬀected and is a simpliﬁcation by any deﬁnition.
DEFINITION 6.1
Let G = (V, T, S, P) be a context-free grammar. A variable A ∈V is said
to be useful if and only if there is at least one w ∈L (G) such that
S
∗⇒xAy
∗⇒w,
(6.2)
with x, y in (V ∪T)∗. In words, a variable is useful if and only if it occurs
in at least one derivation. A variable that is not useful is called useless. A
production is useless if it involves any useless variable.
EXAMPLE 6.2
A variable may be useless because there is no way of getting a terminal
string from it. The case just mentioned is of this kind. Another reason
a variable may be useless is shown in the next grammar. In a grammar

160
Chapter 6 Simpliﬁcation of Context-Free Grammars and Normal Forms
with start symbol S and productions
S →A,
A →aA|λ,
B →bA,
the variable B is useless and so is the production B →bA. Although B
can derive a terminal string, there is no way we can achieve S
∗⇒xBy.
This example illustrates the two reasons why a variable is useless: either
because it cannot be reached from the start symbol or because it cannot
derive a terminal string. A procedure for removing useless variables and
productions is based on recognizing these two situations. Before we present
the general case and the corresponding theorem, let us look at another
example.
EXAMPLE 6.3
Eliminate useless symbols and productions from G = (V, T, S, P),
where V = {S, A, B, C} and T = {a, b}, with P consisting of
S →aS |A| C,
A →a,
B →aa,
C →aCb.
First, we identify the set of variables that can lead to a terminal
string. Because A →a and B →aa, the variables A and B belong
to this set. So does S, because S ⇒A ⇒a. However, this argument
cannot be made for C, thus identifying it as useless.
Removing C
and its corresponding productions, we are led to the grammar G1 with
variables V1 = {S, A, B}, terminals T = {a}, and productions
S →aS|A,
A →a,
B →aa.
Next we want to eliminate the variables that cannot be reached
from the start variable. For this, we can draw a dependency graph
for the variables. Dependency graphs are a way of visualizing complex
relationships and are found in many applications.
For context-free
grammars, a dependency graph has its vertices labeled with variables,

6.1 Methods for Transforming Grammars
161
with an edge between vertices C and D if and only if there is a pro-
duction of the form
C →xDy.
A dependency graph for V1 is shown in Figure 6.1. A variable is useful
only if there is a path from the vertex labeled S to the vertex labeled
with that variable. In our case, Figure 6.1 shows that B is useless.
Removing it and the aﬀected productions and terminals, we are led
to the ﬁnal answer G =

V , T, S, P

with V = {S, A} , T = {a}, and
productions
S →aS|A,
A →a.
The formalization of this process leads to a general construction
and the corresponding theorem.
A
B
S
FIGURE 6.1
THEOREM 6.2
Let G = (V, T, S, P) be a context-free grammar.
Then there exists an
equivalent grammar G =

V , T, S, P

that does not contain any useless
variables or productions.
Proof: The grammar G can be generated from G by an algorithm consisting
of two parts. In the ﬁrst part we construct an intermediate grammar G1 =
(V1, T2, S, P1) such that V1 contains only variables A for which
A
∗⇒w ∈T ∗
is possible. The steps in the algorithm are
1. Set V1 to ∅.
2. Repeat the following step until no more variables are added to V1. For
every A ∈V for which P has a production of the form
A →x1x2 · · · xn, with all xi in V1 ∪T,
add A to V1.
3. Take P1 as all the productions in P whose symbols are all in (V1 ∪T).

162
Chapter 6 Simpliﬁcation of Context-Free Grammars and Normal Forms
- - - - - - - Level k - 2
- - - - - -  Level k - 1
- - - - - - - - - - Level k
A
c
b
a
Ai
Aj
FIGURE 6.2
Clearly this procedure terminates. It is equally clear that if A ∈V1,
then A
∗⇒w ∈T ∗is a possible derivation with G1. The remaining issue
is whether every A for which A
∗⇒w = ab · · · is added to V1 before the
procedure terminates. To see this, consider any such A and look at the
partial derivation tree corresponding to that derivation (Figure 6.2). At
level k, there are only terminals, so every variable Ai at level k −1 will be
added to V1 on the ﬁrst pass through Step 2 of the algorithm. Any variable
at level k −2 will then be added to V1 on the second pass through Step 2.
The third time through Step 2, all variables at level k −3 will be added,
and so on. The algorithm cannot terminate while there are variables in the
tree that are not yet in V1. Hence A will eventually be added to V1.
In the second part of the construction, we get the ﬁnal answer G from
G1. We draw the variable dependency graph for G1 and from it ﬁnd all
variables that cannot be reached from S.
These are removed from the
variable set, as are the productions involving them. We can also eliminate
any terminal that does not occur in some useful production. The result is
the grammar G =

V , T, S, P

.
Because of the construction, G does not contain any useless symbols or
productions. Also, for each w ∈L (G) we have a derivation
S
∗⇒xAy
∗⇒w.
Since the construction of G retains A and all associated productions, we
have everything needed to make the derivation
S
∗⇒
G xAy
∗⇒
G w.

6.1 Methods for Transforming Grammars
163
The grammar G is constructed from G by the removal of productions,
so that P ⊆P. Consequently L

G

⊆L (G). Putting the two results
together, we see that G and G are equivalent.
Removing λ-Productions
One kind of production that is sometimes undesirable is one in which the
right side is the empty string.
DEFINITION 6.2
Any production of a context-free grammar of the form
A →λ
is called a λ-production. Any variable A for which the derivation
A
∗⇒λ
(6.3)
is possible is called nullable.
A grammar may generate a language not containing λ, yet have some
λ-productions or nullable variables. In such cases, the λ-productions can
be removed.
EXAMPLE 6.4
Consider the grammar
S →aS1b,
S1 →aS1b|λ,
with start variable S.
This grammar generates the λ-free language
{anbn : n ≥1}. The λ-production S1 →λ can be removed after adding
new productions obtained by substituting λ for S1 where it occurs on
the right. Doing this we get the grammar
S →aS1b|ab,
S1 →aS1b|ab.
We can easily show that this new grammar generates the same language
as the original one.
In more general situations, substitutions for λ-productions can be
made in a similar, although more complicated, manner.

164
Chapter 6 Simpliﬁcation of Context-Free Grammars and Normal Forms
THEOREM 6.3
Let G be any context-free grammar with λ not in L (G). Then there exists
an equivalent grammar G having no λ-productions.
Proof: We ﬁrst ﬁnd the set VN of all nullable variables of G, using the
following steps.
1. For all productions A →λ, put A into VN.
2. Repeat the following step until no further variables are added to VN.
For all productions
B →A1A2 · · · An,
where A1, A2, ..., An are in VN, put B into VN.
Once the set VN has been found, we are ready to construct P. To do so,
we look at all productions in P of the form
A →x1x2 · · · xm, m ≥1,
where each xi ∈V ∪T. For each such production of P, we put into P that
production as well as all those generated by replacing nullable variables with
λ in all possible combinations. For example, if xi and xj are both nullable,
there will be one production in P with xi replaced with λ, one in which xj
is replaced with λ, and one in which both xi and xj are replaced with λ.
There is one exception: If all xi are nullable, the production A →λ is not
put into P.
The argument that this grammar G is equivalent to G is straightforward
and will be left to the reader.
EXAMPLE 6.5
Find a context-free grammar without λ-productions equivalent to the
grammar deﬁned by
S →ABaC,
A →BC,
B →b|λ,
C →D|λ,
D →d.

6.1 Methods for Transforming Grammars
165
From the ﬁrst step of the construction in Theorem 6.3, we ﬁnd that
the nullable variables are A, B, C. Then, following the second step of
the construction, we get
S →ABaC |BaC| AaC |ABa| aC |Aa| Ba|a,
A →B |C| BC,
B →b,
C →D,
D →d.
Removing Unit-Productions
As we have seen in Theorem 5.2, productions in which both sides are a
single variable are at times undesirable.
DEFINITION 6.3
Any production of a context-free grammar of the form
A →B,
where A, B ∈V , is called a unit-production.
To remove unit-productions, we use the substitution rule discussed in
Theorem 6.1. As the construction in the next theorem shows, this can be
done if we proceed with some care.
THEOREM 6.4
Let G = (V, T, S, P) be any context-free grammar without λ-productions.
Then there exists a context-free grammar G =

V , T, S, P

that does not
have any unit-productions and that is equivalent to G.
Proof: Obviously, any unit-production of the form A →A can be removed
from the grammar without eﬀect, and we need only consider A →B, where
A and B are diﬀerent variables. At ﬁrst sight, it may seem that we can use
Theorem 6.1 directly with x1 = x2 = λ to replace
A →B
with
A →y1 |y2| · · · |yn.

166
Chapter 6 Simpliﬁcation of Context-Free Grammars and Normal Forms
But this will not always work; in the special case
A →B,
B →A,
the unit-productions are not removed. To get around this, we ﬁrst ﬁnd, for
each A, all variables B such that
A
∗⇒B.
(6.4)
We can do this by drawing a dependency graph with an edge (C, D) when-
ever the grammar has a unit-production C →D; then (6.4) holds whenever
there is a walk between A and B. The new grammar G is generated by
ﬁrst putting into P all non-unit productions of P. Next, for all A and B
satisfying (6.4), we add to P
A →y1 |y2| · · · |yn,
where B →y1 |y2| · · · yn is the set of all rules in P with B on the left. Note
that since B →y1 |y2| · · · |yn is taken from P, none of the yi can be a single
variable, so that no unit-productions are created by the last step.
To show that the resulting grammar is equivalent to the original one,
we can follow the same line of reasoning as in Theorem 6.1.
EXAMPLE 6.6
Remove all unit-productions from
S →Aa|B,
B →A|bb,
A →a |bc| B.
The dependency graph for the unit-productions is given in Figure 6.3;
we see from it that S
∗⇒A, S
∗⇒B, B
∗⇒A, and A
∗⇒B. Hence, we
add to the original non-unit productions
S →Aa,
A →a|bc,
B →bb,
the new rules
S →a |bc| bb,
A →bb,
B →a|bc,

6.1 Methods for Transforming Grammars
167
to obtain the equivalent grammar
S →a |bc| bb|Aa,
A →a |bb| bc,
B →a |bb| bc.
Note that the removal of the unit-productions has made B and the
associated productions useless.
A
B
S
FIGURE 6.3
We can put all these results together to show that grammars for context-
free languages can be made free of useless productions, λ-productions, and
unit-productions.
THEOREM 6.5
Let L be a context-free language that does not contain λ. Then there exists
a context-free grammar that generates L and that does not have any useless
productions, λ-productions, or unit-productions.
Proof: The procedures given in Theorems 6.2, 6.3, and 6.4 remove these
kinds of productions in turn. The only point that needs consideration is
that the removal of one type of production may introduce productions of
another type; for example, the procedure for removing λ-productions can
create new unit-productions. Also, Theorem 6.4 requires that the gram-
mar have no λ-productions. But note that the removal of unit-productions
does not create λ-productions (Exercise 18 at the end of this section), and
the removal of useless productions does not create λ-productions or unit-
productions (Exercise 19 at the end of this section).
Therefore, we can
remove all undesirable productions using the following sequence of steps:
1. Remove λ-productions.
2. Remove unit-productions.
3. Remove useless productions.
The result will then have none of these productions, and the theorem is
proved.

168
Chapter 6 Simpliﬁcation of Context-Free Grammars and Normal Forms
EXERCISES
1. Eliminate the variable B from the grammar
S →aSB|bB
B →aA|b.
2. Show that the two grammars
S →abAB|ba,
A →aaa,
B →aA|bb
and
S →abAaA |abAbb| ba,
A →aaa
are equivalent.
3. Complete the proof of Theorem 6.1 by showing that
S
∗⇒
G w
implies
S
∗⇒G w.
4. In Example 6.1, show a derivation tree for the string ababbac, using both the
original and the modiﬁed grammar.
5. In Theorem 6.1, why is it necessary to assume that A and B are diﬀerent
variables?
6. Eliminate all useless productions from the grammar
S →aS|AB|λ,
A →bA,
B →AA.
What language does this grammar generate?
7. Eliminate useless productions from
S →a |aA| B|C,
A →aB|λ,
B →Aa,
C →cCD,
D →ddd|Cd.

6.1 Methods for Transforming Grammars
169
8. Eliminate all λ-productions from
S →aSSS,
S →bb|λ.
9. Eliminate all λ-productions from
S →AaB|aaB,
A →λ,
B →bbA|λ.
10. Remove all unit-productions, all useless productions, and all λ-productions
from the grammar
S →aA|aBB,
A →aaA|λ,
B →bB|bbC,
C →B.
What language does this grammar generate?
11. Eliminate all unit-productions from the grammar in Exercise 7.
12. Complete the proof of Theorem 6.3.
13. Complete the proof of Theorem 6.4.
14. Use the construction in Theorem 6.3 to remove λ-productions from the gram-
mar in Example 5.4. What language does the resulting grammar generate?
15. Consider the grammar G with productions
S →A|B,
A →λ,
B →aBb,
B →b.
Construct a grammar G by applying the algorithm in Theorem 6.3 to G.
What is the diﬀerence between L(G) and L( G)?
16. Suppose that G is a context-free grammar for which λ ∈L (G). Show that if
we apply the construction in Theorem 6.3, we obtain a new grammar G such
that L

G

= L (G) −{λ}.
17. Give an example of a situation in which the removal of λ-productions intro-
duces previously nonexistent unit-productions.
18. Let G be a grammar without λ-productions, but possibly with some unit-
productions. Show that the construction of Theorem 6.4 does not then intro-
duce any λ-productions.
19. Show that if a grammar has no λ-productions and no unit-productions, then
the removal of useless productions by the construction of Theorem 6.2 does
not introduce any such productions.

170
Chapter 6 Simpliﬁcation of Context-Free Grammars and Normal Forms
20. Justify the claim made in the proof of Theorem 6.1 that the variable B can
be replaced as soon as it appears.
21. Suppose that a context-free grammar G = (V, T, S, P) has a production of
the form
A →xy,
where x, y ∈(V ∪T)+. Prove that if this rule is replaced by
A →By,
B →x,
where B /∈V , then the resulting grammar is equivalent to the original one.
22. Consider the procedure suggested in Theorem 6.2 for the removal of useless
productions. Reverse the order of the two parts, ﬁrst eliminating variables
that cannot be reached from S, then removing those that do not yield a
terminal string. Does the new procedure still work correctly? If so, prove it.
If not, give a counterexample.
23. It is possible to deﬁne the term simpliﬁcation precisely by introducing the
concept of complexity of a grammar. This can be done in many ways; one
of them is through the length of all the strings giving the production rules.
For example, we might use
complexity (G) =

A→v∈P
{1 + |v|} .
Show that the removal of useless productions always reduces the complexity
in this sense.
What can you say about the removal of λ-productions and
unit-productions?
24. A context-free grammar G is said to be minimal for a given language L if
complexity (G) ≤complexity

G

for any G generating L. Show by exam-
ple that the removal of useless productions does not necessarily produce a
minimal grammar.
25. Prove the following result. Let G = (V, T, S, P) be a context-free grammar.
Divide the set of productions whose left sides are some given variable (say,
A), into two disjoint subsets
A →Ax1 |Ax2| · · · |Axn,
A →y1 |y2| · · · |ym,
where xi, yi are in (V ∪T)∗, but A is not a preﬁx of any yi. Consider the
grammar G =

V ∪{Z} , T, S, P

, where Z /∈V and P is obtained by re-
placing all productions that have A on the left by
A →yi|yiZ,
i = 1, 2, ..., m,
Z →xi|xiZ,
i = 1, 2, ..., n.
Then L (G) = L

G

.

6.2 Two Important Normal Forms
171
26. Use the result of the preceding exercise to rewrite the grammar
A →Aa |aBc| λ,
B →Bb|bc
so that it no longer has productions of the form A →Ax or B →Bx.
27. Prove the following counterpart of Exercise 23. Let the set of productions
involving the variable A on the left be divided into two disjoint subsets
A →x1A |x2A| · · · |xnA
and
A →y1 |y2| · · · |ym,
where A is not a suﬃx of any yi.
Show that the grammar obtained by
replacing these productions with
A →yi|Zyi,
i = 1, 2, ..., m,
Z →xi|Zxi,
i = 1, 2, ..., n,
is equivalent to the original grammar.
6.2
TWO IMPORTANT NORMAL FORMS
There are many kinds of normal forms we can establish for context-free
grammars. Some of these, because of their wide usefulness, have been stud-
ied extensively. We consider two of them brieﬂy.
Chomsky Normal Form
One kind of normal form we can look for is one in which the number of
symbols on the right of a production is strictly limited. In particular, we
can ask that the string on the right of a production consist of no more than
two symbols. One instance of this is the Chomsky normal form.
DEFINITION 6.4
A context-free grammar is in Chomsky normal form if all productions are
of the form
A →BC
or
A →a,
where A, B, C are in V , and a is in T.

172
Chapter 6 Simpliﬁcation of Context-Free Grammars and Normal Forms
EXAMPLE 6.7
The grammar
S →AS|a,
A →SA|b
is in Chomsky normal form. The grammar
S →AS|AAS,
A →SA|aa
is not; both productions S →AAS and A →aa violate the conditions
of Deﬁnition 6.4.
THEOREM 6.6
Any context-free grammar G = (V, T, S, P) with λ /∈L (G) has an equivalent
grammar G =

V , T, S, P

in Chomsky normal form.
Proof: Because of Theorem 6.5, we can assume without loss of generality
that G has no λ-productions and no unit-productions. The construction of
G will be done in two steps.
Step 1: Construct a grammar G1 = (V1, T, S, P1) from G by considering all
productions in P in the form
A →x1x2 · · · xn,
(6.5)
where each xi is a symbol either in V or in T. If n = 1, then x1 must be a
terminal since we have no unit-productions. In this case, put the production
into P1. If n ≥2, introduce new variables Ba for each a ∈T. For each
production of P in the form (6.5) we put into P1 the production
A →C1C2 · · · Cn,
where
Ci = xi if xi is in V,
and
Ci = Ba if xi = a.
For every Ba we also put into P1 the production
Ba →a.

6.2 Two Important Normal Forms
173
This part of the algorithm removes all terminals from productions whose
right side has length greater than one, replacing them with newly introduced
variables.
At the end of this step we have a grammar G1 all of whose
productions have the form
A →a,
(6.6)
or
A →C1C2 · · · Cn,
(6.7)
where Ci ∈V1.
It is an easy consequence of Theorem 6.1 that
L (G1) = L (G) .
Step 2: In the second step, we introduce additional variables to reduce the
length of the right sides of the productions where necessary. First we put
all productions of the form (6.6) as well as all the productions of the form
(6.7) with n = 2 into P. For n > 2, we introduce new variables D1, D2, ...
and put into P the productions
A →C1D1,
D1 →C2D2,
...
Dn−2 →Cn−1Cn.
Obviously, the resulting grammar G is in Chomsky normal form. Repeated
applications of Theorem 6.1 will show that L (G1) = L

G

, so that
L

G

= L (G) .
This somewhat informal argument can easily be made more precise. We
will leave this to the reader.
EXAMPLE 6.8
Convert the grammar with productions
S →ABa,
A →aab,
B →Ac
to Chomsky normal form.

174
Chapter 6 Simpliﬁcation of Context-Free Grammars and Normal Forms
As required by the construction of Theorem 6.6, the grammar does
not have any λ-productions or any unit-productions.
In Step 1, we introduce new variables Ba, Bb, Bc and use the algo-
rithm to get
S →ABBa,
A →BaBaBb,
B →ABc,
Ba →a,
Bb →b,
Bc →c.
In the second step, we introduce additional variables to get the
ﬁrst two productions into normal form and we get the ﬁnal result
S →AD1,
D1 →BBa,
A →BaD2,
D2 →BaBb,
B →ABc,
Ba →a,
Bb →b,
Bc →c.
Greibach Normal Form
Another useful grammatical form is the Greibach normal form. Here
we put restrictions not on the length of the right sides of a production,
but on the positions in which terminals and variables can appear. Argu-
ments justifying Greibach normal form are a little complicated and not very
transparent. Similarly, constructing a grammar in Greibach normal form
equivalent to a given context-free grammar is tedious. We therefore deal
with this matter very brieﬂy. Nevertheless, Greibach normal form has many
theoretical and practical consequences.
DEFINITION 6.5
A context-free grammar is said to be in Greibach normal form if all pro-
ductions have the form
A →ax,
where a ∈T and x ∈V ∗.

6.2 Two Important Normal Forms
175
If we compare this with Deﬁnition 5.4, we see that the form A →ax
is common to both Greibach normal form and s-grammars, but Greibach
normal form does not carry the restriction that the pair (A, a) occur at most
once. This additional freedom gives Greibach normal form a generality not
possessed by s-grammars.
If a grammar is not in Greibach normal form, we may be able to rewrite
it in this form with some of the techniques encountered above. Here are two
simple examples.
EXAMPLE 6.9
The grammar
S →AB,
A →aA |bB| b,
B →b
is not in Greibach normal form. However, using the substitution given
by Theorem 6.1, we immediately get the equivalent grammar
S →aAB |bBB| bB,
A →aA |bB| b,
B →b,
which is in Greibach normal form.
EXAMPLE 6.10
Convert the grammar
S →abSb|aa
into Greibach normal form.
Here we can use a device similar to the one introduced in the con-
struction of Chomsky normal form. We introduce new variables A and
B that are essentially synonyms for a and b, respectively. Substituting
for the terminals with their associated variables leads to the equivalent
grammar
S →aBSB|aA,
A →a,
B →b,
which is in Greibach normal form.

176
Chapter 6 Simpliﬁcation of Context-Free Grammars and Normal Forms
In general, though, neither the conversion of a given grammar to
Greibach normal form nor the proof that this can always be done is a simple
matter. We introduce Greibach normal form here because it will simplify
the technical discussion of an important result in the next chapter. How-
ever, from a conceptual viewpoint, Greibach normal form plays no further
role in our discussion, so we only quote the following general result without
proof.
THEOREM 6.7
For every context-free grammar G with λ /∈L (G), there exists an equivalent
grammar G in Greibach normal form.
EXERCISES
1. Convert the grammar S →aSS|a|b into Chomsky normal form.
2. Convert the grammar S →aSb|Sab|ab into Chomsky normal form.
3. Transform the grammar
S →aSaaA|A, A →abA|bb
into Chomsky normal form.
4. Transform the grammar with productions
S →baAB,
A →bAB|λ,
B →BAa |A| λ
into Chomsky normal form.
5. Convert the grammar
S →AB|aB,
A →abb|λ,
B →bbA
into Chomsky normal form.
6. Let G = (V, T, S, P) be any context-free grammar without any λ-productions
or unit-productions. Let k be the maximum number of symbols on the right of
any production in P. Show that there is an equivalent grammar in Chomsky
normal form with no more than (k −1) |P| + |T| production rules.
7. Draw the dependency graph for the grammar in Exercise 5.

6.2 Two Important Normal Forms
177
8. A linear language is one for which there exists a linear grammar (for a def-
inition, see Example 3.14). Let L be any linear language not containing λ.
Show that there exists a grammar G = (V, T, S, P), all of whose productions
have one of the forms
A →aB,
A →Ba,
A →a,
where a ∈T, A, B ∈V , such that L = L (G).
9. Show that for every context-free grammar G = (V, T, S, P), there is an equiv-
alent one in which all productions have the form
A →aBC,
or
A →λ,
where a ∈Σ ∪{λ} , A, B, C ∈V .
10. Convert the grammar
S →aSb|bSa|a|b|ab
into Greibach normal form.
11. Convert the following grammar into Greibach normal form.
S →aSb|ab|bb
12. Convert the grammar
S →ab |aS| aaS|aSS
into Greibach normal form.
13. Convert the grammar
S →ABb|a|b
A →aaA|B
B →bAb
into Greibach normal form.
14. Can every linear grammar be converted to a form in which all productions
look like A →ax, where a ∈T and x ∈V ∪{λ}?
15. A context-free grammar is said to be in two-standard form if all production
rules satisfy the following pattern:
A →aBC,
A →aB,
A →a,
where A, B, C ∈V and a ∈T.

178
Chapter 6 Simpliﬁcation of Context-Free Grammars and Normal Forms
Convert the grammar G = ({S, A, B, C} , {a, b} , S, P) with P given as
S →aSA,
A →bABC,
B →bb,
C →aBC
into two-standard form.
16. Two-standard form is general;
for any context-free grammar G with
λ /∈L (G), there exists an equivalent grammar in two-standard form. Prove
this claim.
6.3
A MEMBERSHIP ALGORITHM FOR CONTEXT-FREE
GRAMMARS*
In Section 5.2, we claim, without any elaboration, that membership and
parsing algorithms for context-free grammars exist that require approxi-
mately |w|3 steps to parse a string w. We are now in a position to justify
this claim. The algorithm we will describe here is called the CYK algorithm,
after its originators J. Cocke, D. H. Younger, and T. Kasami. The algo-
rithm works only if the grammar is in Chomsky normal form and succeeds
by breaking one problem into a sequence of smaller ones in the following
way. Assume that we have a grammar G = (V, T, S, P) in Chomsky normal
form and a string
w = a1a2 · · · an.
We deﬁne substrings
wij = ai · · · aj,
and subsets of V
Vij =

A ∈V : A
∗⇒wij

.
Clearly, w ∈L (G) if and only if S ∈V1n.
To compute Vij, observe that A ∈Vii if and only if G contains a pro-
duction A →ai. Therefore, Vii can be computed for all 1 ≤i ≤n by
inspection of w and the productions of the grammar. To continue, notice
that for j > i, A derives wij if and only if there is a production A →BC,
with B
∗⇒wik and C
∗⇒wk+1j for some k with i ≤k, k < j. In other words,
Vij =

k∈{i,i+1,...,j−1}
{A : A →BC, with B ∈Vik, C ∈Vk+1, j} .
(6.8)

6.3 A Membership Algorithm for Context-Free Grammars*
179
An inspection of the indices in (6.8) shows that it can be used to compute
all the Vij if we proceed in the sequence
1. Compute V11, V22, ..., Vnn,
2. Compute V12, V23, ..., Vn−1,n,
3. Compute V13, V24, ..., Vn−2,n,
and so on.
EXAMPLE 6.11
Determine whether the string w = aabbb is in the language generated
by the grammar
S →AB,
A →BB|a,
B →AB|b.
First note that w11 = a, so V11 is the set of all variables that
immediately derive a, that is, V11 = {A}. Since w22 = a, we also have
V22 = {A} and, similarly,
V11 = {A} , V22 = {A} , V33 = {B} , V44 = {B} , V55 = {B} .
Now we use (6.8) to get
V12 = {A : A →BC, B ∈V11, C ∈V22} .
Since V11 = {A} and V22 = {A}, the set consists of all variables that
occur on the left side of a production whose right side is AA. Since
there are none, V12 is empty. Next,
V23 = {A : A →BC, B ∈V22, C ∈V33} ,
so the required right side is AB, and we have V23 = {S, B}. A straight-
forward argument along these lines then gives
V12 = ∅, V23 = {S, B} , V34 = {A} , V45 = {A} ,
V13 = {S, B} , V24 = {A} , V35 = {S, B} ,
V14 = {A} , V25 = {S, B} ,
V15 = {S, B} ,
so that w ∈L (G).

180
Chapter 6 Simpliﬁcation of Context-Free Grammars and Normal Forms
The CYK algorithm, as described here, determines membership for any
language generated by a grammar in Chomsky normal form. With some
additions to keep track of how the elements of Vij are derived, it can be
converted into a parsing method. To see that the CYK membership algo-
rithm requires O

n3	
steps, notice that exactly n (n + 1) /2 sets of Vij have
to be computed. Each involves the evaluation of at most n terms in (6.8),
so the claimed result follows.
EXERCISES
1. Use the CYK algorithm to determine whether the strings abb, bbb, aabba, and
abbbb are in the language generated by the grammar in Example 6.11.
2. Use the CYK algorithm to ﬁnd a parsing of the string aab, using the grammar
of Example 6.11.
3. Use the approach employed in Exercise 2 to show how the CYK membership
algorithm can be made into a parsing method.
4. Use the CYK method to determine if the string w = aaabbbb is in the language
generated by the grammar S →aSb|b.

7
CHA P T E R
PUSHDOWN
AUTOMATA
CHAPTER SUMMARY
In the discussion of regular languages, we saw that there were several
ways of exploring regular languages: finite automata, regular gram-
mars, and regular expressions. Having defined context-free languages
via context-free grammars, we now ask if there are other options for
context-free languages. It turns out that there is no analog of regular
expressions, but that pushdown automata are the automata associated
with context-free languages.
Pushdown automata are essentially finite automata with a stack as
storage. Since a stack by definition has infinite length, this overcomes
the limitation on finite automata arising from a bounded memory.
Pushdown automata are equivalent to context-free grammars, as
long as we allow them to be nondeterministic. We can also define deter-
ministic pushdown automata, but the language family associated with
them is a proper subset of the context-free languages.
T
he description of context-free languages by means of context-free
grammars is convenient, as illustrated by the use of BNF in program-
ming language deﬁnition. The next question is whether there is a
class of automata that can be associated with context-free languages. As
we have seen, ﬁnite automata cannot recognize all context-free languages.
181

182
Chapter 7 Pushdown Automata
Intuitively, we understand that this is because ﬁnite automata have strictly
ﬁnite memories, whereas the recognition of a context-free language may
require storing an unbounded amount of information. For example, when
scanning a string from the language L = {anbn : n ≥0}, we must not only
check that all a's precede the ﬁrst b, we must also count the number of a's.
Since n is unbounded, this counting cannot be done with a ﬁnite memory.
We want a machine that can count without limit. But as we see from other
examples, such as

wwR
, we need more than unlimited counting ability:
We need the ability to store and match a sequence of symbols in reverse
order. This suggests that we might try a stack as a storage mechanism, al-
lowing unbounded storage that is restricted to operating like a stack. This
gives us a class of machines called pushdown automata (pda).
In this chapter, we explore the connection between pushdown automata
and context-free languages. We ﬁrst show that if we allow pushdown au-
tomata to act nondeterministically, we get a class of automata that accepts
exactly the family of context-free languages. But we will also see that here
there is no longer an equivalence between the deterministic and nondeter-
ministic versions. The class of deterministic pushdown automata deﬁnes a
new family of languages, the deterministic context-free languages, forming
a proper subset of the context-free languages. Since this is an important
family for the treatment of programming languages, we conclude the chap-
ter with a brief introduction to the grammars associated with deterministic
context-free languages.
7.1
NONDETERMINISTIC PUSHDOWN AUTOMATA
A schematic representation of a pushdown automaton is given in Figure
7.1.
Each move of the control unit reads a symbol from the input ﬁle,
while at the same time changing the contents of the stack through the usual
Input file
Control unit
Stack
FIGURE 7.1

7.1 Nondeterministic Pushdown Automata
183
stack operations. Each move of the control unit is determined by the current
input symbol as well as by the symbol currently on top of the stack. The
result of the move is a new state of the control unit and a change in the top
of the stack.
Definition of a Pushdown Automaton
Formalizing this intuitive notion gives us a precise deﬁnition of a pushdown
automaton.
DEFINITION 7.1
A nondeterministic pushdown accepter (npda) is deﬁned by the sep-
tuple
M = (Q, Σ, Γ, δ, q0, z, F) ,
where
Q is a ﬁnite set of internal states of the control unit,
Σ is the input alphabet,
Γ is a ﬁnite set of symbols called the stack alphabet,
δ : Q × (Σ ∪{λ}) × Γ →set of ﬁnite subsets of Q × Γ∗is the transition
function,
q0 ∈Q is the initial state of the control unit,
z ∈Γ is the stack start symbol,
F ⊆Q is the set of ﬁnal states.
The complicated formal appearance of the domain and range of δ merits
a closer examination. The arguments of δ are the current state of the control
unit, the current input symbol, and the current symbol on top of the stack.
The result is a set of pairs (q, x), where q is the next state of the control unit
and x is a string that is put on top of the stack in place of the single symbol
there before. Note that the second argument of δ may be λ, indicating that
a move that does not consume an input symbol is possible. We will call such
a move a λ-transition. Note also that δ is deﬁned so that it needs a stack
symbol; no move is possible if the stack is empty. Finally, the requirement
that the elements of the range of δ be a ﬁnite subset is necessary because
Q × Γ∗is an inﬁnite set and therefore has inﬁnite subsets. While an npda
may have several choices for its moves, this choice must be restricted to a
ﬁnite set of possibilities.

184
Chapter 7 Pushdown Automata
EXAMPLE 7.1
Suppose the set of transition rules of an npda contains
δ (q1, a, b) = {(q2, cd) , (q3, λ)}.
If at any time the control unit is in state q1, the input symbol read
is a, and the symbol on top of the stack is b, then one of two things
can happen: (1) the control unit goes into state q2 and the string cd
replaces b on top of the stack, or (2) the control unit goes into state q3
with the symbol b removed from the top of the stack. In our notation
we assume that the insertion of a string into a stack is done symbol by
symbol, starting at the right end of the string.
EXAMPLE 7.2
Consider an npda with
Q = {q0, q1, q2, q3} ,
Σ = {a, b} ,
Γ = {0, 1} ,
z = 0,
F = {q3} ,
with initial state q0 and
δ (q0, a, 0) = {(q1, 10) , (q3, λ)} ,
δ (q0, λ, 0) = {(q3, λ)} ,
δ (q1, a, 1) = {(q1, 11)} ,
δ (q1, b, 1) = {(q2, λ)} ,
δ (q2, b, 1) = {(q2, λ)} ,
δ (q2, λ, 0) = {(q3, λ)} .
What can we say about the action of this automaton?
First, notice that transitions are not speciﬁed for all possible com-
binations of input and stack symbols. For instance, there is no entry
given for δ (q0, b, 0).
The interpretation of this is the same that we
used for nondeterministic ﬁnite automata: An unspeciﬁed transition is
to the null set and represents a dead conﬁguration for the npda.
The crucial transitions are
δ (q1, a, 1) = {(q1, 11)} ,

7.1 Nondeterministic Pushdown Automata
185
which adds a 1 to the stack when an a is read, and
δ (q2, b, 1) = {(q2, λ)} ,
which removes a 1 when a b is encountered. These two steps count the
number of a's and match that count against the number of b's. The
control unit is in state q1 until the ﬁrst b is encountered at which time
it goes into state q2. This assures that no b precedes the last a. After
analyzing the remaining transitions, we see that the npda will end in
the ﬁnal state q3 if and only if the input string is in the language
L = {anbn : n ≥0} ∪{a} .
As an analogy with ﬁnite automata, we might say that the npda accepts
the above language. Of course, before making such a claim, we must
deﬁne what we mean by an npda accepting a language.
We can also use transition graphs to represent npda's. In this represen-
tation we label the edges of the graph with three things: the current input
symbol, the symbol at the top of the stack, and the string that replaces the
top of the stack.
EXAMPLE 7.3
The npda in Example 7.2 is represented by the transition graph in
Figure 7.2.
a, 0, λ;
λ, 0, λ
q3
q0
q2
q1
b, 1, λ
b, 1, λ
a, 0, 10
a, 1, 11
λ, 0, λ
FIGURE 7.2
While transition graphs are convenient for describing npda's, they are
less useful for making arguments.
The fact that we have to keep track,
not only of the internal states, but also of the stack contents, limits the
usefulness of transition graphs for formal reasoning. Instead, we introduce
a succinct notation for describing the successive conﬁgurations of an npda

186
Chapter 7 Pushdown Automata
during the processing of a string. The relevant factors at any time are the
current state of the control unit, the unread part of the input string, and
the current contents of the stack. Together these completely determine all
the possible ways in which the npda can proceed. The triplet
(q, w, u) ,
where q is the state of the control unit, w is the unread part of the input
string, and u is the stack contents (with the leftmost symbol indicating the
top of the stack), is called an instantaneous description of a pushdown
automaton. A move from one instantaneous description to another will be
denoted by the symbol ⊢; thus
(q1, aw, bx) ⊢(q2, w, yx)
is possible if and only if
(q2, y) ∈δ (q1, a, b) .
Moves involving an arbitrary number of steps will be denoted by
∗
⊢. The
expression
(q1, w1, x1)
∗
⊢(q2, w2, x2)
indicates a possible conﬁguration change over a number of steps.1
On occasions where several automata are under consideration we will
use ⊢M to emphasize that the move is made by the particular automaton M.
The Language Accepted by a Pushdown Automaton
DEFINITION 7.2
Let M = (Q, Σ, Γ, δ, q0, z, F) be a nondeterministic pushdown automaton.
The language accepted by M is the set
L (M) =

w ∈Σ∗: (q0, w, z)
∗
⊢M(p, λ, u) , p ∈F, u ∈Γ∗

.
In words, the language accepted by M is the set of all strings that can put
M into a ﬁnal state at the end of the string. The ﬁnal stack content u is
irrelevant to this deﬁnition of acceptance.
1Because of the nondeterminism, such a change is of course not necessary.

7.1 Nondeterministic Pushdown Automata
187
EXAMPLE 7.4
Construct an npda for the language
L =

w ∈{a, b}∗: na (w) = nb (w)

.
As in Example 7.2, the solution to this problem involves counting the
number of a's and b's, which is easily done with a stack.
Here we
need not even worry about the order of the a's and b's. We can insert
a counter symbol, say 0, into the stack whenever an a is read, then
pop one counter symbol from the stack when a b is found. The only
diﬃculty with this is that if there is a preﬁx of w with more b's than a's,
we will not ﬁnd a 0 to use. But this is easy to ﬁx; we can use a negative
counter symbol, say 1, for counting the b's that are to be matched
against a's later. The complete solution is given in the transition graph
in Figure 7.3.
λ, z, z
qf
q0
a, 0, 00; b, 1, 11
a, z, 0z; b, 0, λ;
b, z, 1z; a, 1, λ,  
FIGURE 7.3
In processing the string baab, the npda makes the moves
(q0, baab, z) ⊢(q0, aab, 1z) ⊢(q0, ab, z)
⊢(q0, b, 0z) ⊢(q0, λ, z) ⊢(qf, λ, z)
and hence the string is accepted.
EXAMPLE 7.5
To construct an npda for accepting the language
L =

wwR : w ∈{a, b}+
,
we use the fact that the symbols are retrieved from a stack in the
reverse order of their insertion.
When reading the ﬁrst part of the
string, we push consecutive symbols on the stack.
For the second
part, we compare the current input symbol with the top of the stack,

188
Chapter 7 Pushdown Automata
continuing as long as the two match. Since symbols are retrieved from
the stack in reverse of the order in which they were inserted, a complete
match will be achieved if and only if the input is of the form wwR.
An apparent diﬃculty with this suggestion is that we do not know
the middle of the string, that is, where w ends and wR starts. But the
nondeterministic nature of the automaton helps us with this; the npda
correctly guesses where the middle is and switches states at that point.
A solution to the problem is given by M = (Q, Σ, Γ, δ, q0, z, F), where
Q = {q0, q1, q2} ,
Σ = {a, b} ,
Γ = {a, b, z} ,
F = {q2} .
The transition function can be visualized as having several parts: a set
to push w on the stack,
δ (q0, a, a) = {(q0, aa)} ,
δ (q0, b, a) = {(q0, ba)} ,
δ (q0, a, b) = {(q0, ab)} ,
δ (q0, b, b) = {(q0, bb)} ,
δ (q0, a, z) = {(q0, az)} ,
δ (q0, b, z) = {(q0, bz)} ,
a set to guess the middle of the string, where the npda switches from
state q0 to q1,
δ (q0, λ, a) = {(q1, a)} ,
δ (q0, λ, b) = {(q1, b)} ,
a set to match wR against the contents of the stack,
δ (q1, a, a) = {(q1, λ)} ,
δ (q1, b, b) = {(q1, λ)} ,
and ﬁnally
δ (q1, λ, z) = {(q2, z)} ,
to recognize a successful match.
The sequence of moves in accepting abba is
(q0, abba, z) ⊢(q0, bba, az) ⊢(q0, ba, baz)
⊢(q1, ba, baz) ⊢(q1, a, az) ⊢(q1, λ, z) ⊢(q2, z) .

7.1 Nondeterministic Pushdown Automata
189
The nondeterministic alternative for locating the middle of the string is
taken at the third move. At that stage, the pda has the instantaneous
descriptions (q0, ba, baz) and has two choices for its next move. One is
to use δ (q0, b, b) = {(q0, bb)} and make the move
(q0, ba, baz) ⊢(q0, a, bbaz) ,
the second is the one used above, namely δ (q0, λ, b) = {(q1, b)}. Only
the latter leads to acceptance of the input.
EXERCISES
1. Find a pda that accepts the language L =

anb2n : n ≥0

.
2. Show the sequence of instantaneous descriptions for the acceptance of aabbbb
by the pda in Exercise 1.
3. Construct npda's that accept the following regular languages:
(a) L1 = L (aaa∗bab).
(b) L2 = L (aab∗aba∗).
(c) The union of L1 and L2.
(d) L1 −L2.
(e) The intersection of L1 and L2.
4. Find a pda with fewer than four states that accepts the same language as the
pda in Example 7.2.
5. Prove that the pda in Example 7.5 does not accept any string not in

wwR
.
6. Construct npda's that accept the following languages on Σ = {a, b, c}:
(a) L =

anb3n : n ≥0

.
(b) L =

wcwR : w ∈{a, b}∗
.
(c) L =

anbmcn+m : n ≥0, m ≥0

.
(d) L =

anbn+mcm : n ≥0, m ≥1

.
(e) L =

a3bncn : n ≥0

.
(f) L = {anbm : n ≤m ≤3n}.
(g) L = {w : na (w) = nb (w) + 1}.
(h) L = {w : na (w) = 2nb (w)}.
(i) L = {w : na (w) + nb (w) = nc (w)}.

190
Chapter 7 Pushdown Automata
(j) L = {w : 2na (w) ≤nb (w) ≤3na (w)}.
(k) L = {w : na (w) < nb (w)}.
7. Construct an npda that accepts the language L = {anbm : n ≥0, n ̸= m}.
8. Find an npda on Σ = {a, b, c} that accepts the language
L =

w1cw2 : w1, w2 ∈{a, b}∗, w1 ̸= wR
2

.
9. Find an npda for the concatenation of L (a∗) and the language in Exercise 8.
10. Find an npda for the language L = {ab (ab)n ba (ba)n : n ≥0}.
11. Is it possible to ﬁnd a dfa that accepts the same language as the pda
M = ({q0, q1} , {a, b} , {z} , δ, q0, z, {q1}) ,
with
δ (q0, a, z) = {(q1, z)} ,
δ (q0, b, z) = {(q0, z)} ,
δ (q1, a, z) = {(q1, z)} ,
δ (q1, b, z) = {(q0, z)}?
12. What language is accepted by the pda
M = ({q0, q1, q2, q3, q4, q5} , {a, b} , {0, 1, z} , δ, q0, z, {q5}) ,
with
δ (q0, b, z) = {(q1, 1z)} ,
δ (q1, b, 1) = {(q1, 11)} ,
δ (q2, a, 1) = {(q3, λ)} ,
δ (q3, a, 1) = {(q4, λ)} ,
δ (q4, a, z) = {(q4, z) , (q5, z)}?
13. What language is accepted by the npda M = ({q0, q1, q2} , {a, b} , {a, b, z}} q0,
z, {q2}).
δ (q0, a, z) = {(q1, a) , (q2, λ)} ,
δ (q1, b, a) = {(q1, b)} ,
δ (q1, b, b) = {(q1, b)} ,
δ (q1, a, b) = {(q2, λ)}?
14. What language is accepted by the npda in Example 7.4 if we use F = {q0, qf}?
15. What language is accepted by the npda in Exercise 13 above if we use F =
{q0, q1, q2}?
16. Find an npda with no more than two internal states that accepts the language
L (aa∗ba∗).

7.2 Pushdown Automata and Context-Free Languages
191
17. Suppose that in Example 7.2 we replace the given value of δ (q2, λ, 0) with
δ (q2, λ, 0) = {(q0, λ)} .
What is the language accepted by this new pda?
18. We can deﬁne a restricted npda as one that can increase the length of the
stack by at most one symbol in each move, changing Deﬁnition 7.1 so that
δ : Q × (Σ ∪{λ}) × Γ →2Q×(ΓΓ∪Γ∪{λ}).
The interpretation of this is that the range of δ consists of sets of pairs of
the form (qi, ab) , (qi, a) , or (qi, λ). Show that for every npda M, there exists
such a restricted npda 
M such that L (M) = L


M

.
19. An alternative to Deﬁnition 7.2 for language acceptance is to require the stack
to be empty when the end of the input string is reached. Formally, an npda
M is said to accept the language N (M) by empty stack if
N (M) =
	
w ∈Σ∗: (q0, w, z)
∗
⊢M(p, λ, λ)

,
where p is any element in Q. Show that this notion is eﬀectively equivalent
to Deﬁnition 7.2, in the sense that for any npda M, there exists an npda 
M
such that L (M) = N


M

, and vice versa.
7.2
PUSHDOWN AUTOMATA AND CONTEXT-FREE
LANGUAGES
In the examples of the previous section, we saw that pushdown automata
exist for some of the familiar context-free languages. This is no accident.
There is a general relation between context-free languages and nondetermin-
istic pushdown accepters that is established in the next two major results.
We will show that for every context-free language there is an npda that ac-
cepts it, and conversely, that the language accepted by any npda is context
free.
Pushdown Automata for Context-Free Languages
We ﬁrst show that for every context-free language there is an npda that
accepts it. The underlying idea is to construct an npda that can, in some
way, carry out a leftmost derivation of any string in the language.
To
simplify the argument a little, we assume that the language is generated by
a grammar in Greibach normal form.
The pda we are about to construct will represent the derivation by
keeping the variables in the right part of the sentential form on its stack,
while the left part, consisting entirely of terminals, is identical with the

192
Chapter 7 Pushdown Automata
input read.
We begin by putting the start symbol on the stack.
After
that, to simulate the application of a production A →ax, we must have
the variable A on top of the stack and the terminal a as the input symbol.
The variable on the stack is removed and replaced by the variable string
x. What δ should be to achieve this is easy to see. Before we present the
general argument, let us look at a simple example.
EXAMPLE 7.6
Construct a pda that accepts the language generated by a grammar
with productions
S →aSbb|a.
We ﬁrst transform the grammar into Greibach normal form, changing
the productions to
S →aSA|a,
A →bB,
B →b.
The corresponding automaton will have three states {q0, q1, q2}, with
initial state q0 and ﬁnal state q2. First, the start symbol S is put on
the stack by
δ (q0, λ, z) = {(q1, Sz)} .
The production S →aSA will be simulated in the pda by removing
S from the stack and replacing it with SA, while reading a from the
input. Similarly, the rule S →a should cause the pda to read an a
while simply removing S. Thus, the two productions are represented
in the pda by
δ (q1, a, S) = {(q1, SA) , (q1, λ)} .
In an analogous manner, the other productions give
δ (q1, b, A) = {(q1, B)} ,
δ (q1, b, B) = {(q1, λ)} .
The appearance of the stack start symbol on top of the stack signals
the completion of the derivation and the pda is put into its ﬁnal state
by
δ (q1, λ, z) = {(q2, λ)} .
The construction of this example can be adapted to other cases,
leading to a general result.

7.2 Pushdown Automata and Context-Free Languages
193
THEOREM 7.1
For any context-free language L, there exists an npda M such that
L = L (M) .
Proof: If L is a λ-free context-free language, there exists a context-free
grammar in Greibach normal form for it. Let G = (V, T, S, P) be such a
grammar. We then construct an npda that simulates leftmost derivations
in this grammar. As suggested, the simulation will be done so that the
unprocessed part of the sentential form is in the stack, while the terminal
preﬁx of any sentential form matches the corresponding preﬁx of the input
string.
Speciﬁcally, the npda will be
M = ({q0, q1, qf} , T, V ∪{z} , δ, q0, z, {qf}) ,
where z /∈V . Note that the input alphabet of M is identical with the set
of terminals of G and that the stack alphabet contains the set of variables
of the grammar.
The transition function will include
δ (q0, λ, z) = {(q1, Sz)} ,
(7.1)
so that after the ﬁrst move of M, the stack contains the start symbol S of
the derivation. (The stack start symbol z is a marker to allow us to detect
the end of the derivation.) In addition, the set of transition rules is such
that
(q1, u) ∈δ (q1, a, A) ,
(7.2)
whenever
A →au
is in P. This reads input a and removes the variable A from the stack,
replacing it with u. In this way it generates the transitions that allow the
pda to simulate all derivations. Finally, we have
δ (q1, λ, z) = {(qf, z)} ,
(7.3)
to get M into a ﬁnal state.
To show that M accepts any w ∈L (G), consider the partial leftmost
derivation
S
∗⇒a1a2 · · · anA1A2 · · · Am
⇒a1a2 · · · anbB1 · · · BkA2 · · · Am.

194
Chapter 7 Pushdown Automata
If M is to simulate this derivation, then after reading a1a2 · · · an, the stack
must contain A1A2 · · · Am. To take the next step in the derivation, G must
have a production
A1 →bB1 · · · Bk.
But the construction is such that then M has a transition rule in which
(q1, B1 · · · Bk) ∈δ (q1, b, A1) ,
so that the stack now contains B1 · · · BkA2 · · · Am after having read
a1a2 · · · anb.
A simple induction argument on the number of steps in the derivation
then shows that if
S
∗⇒w,
then
(q1, w, Sz)
∗
⊢(q1, λ, z) .
Using (7.1) and (7.3) we have
(q0, w, z) ⊢(q1, w, Sz)
∗
⊢(q1, λ, z) ⊢(qf, λ, z) ,
so that L (G) ⊆L (M).
To prove that L (M) ⊆L (G), let w ∈L (M). Then by deﬁnition
(q0, w, z)
∗
⊢(qf, λ, u) .
But there is only one way to get from q0 to q1 and only one way from q1 to
qf. Therefore, we must have
(q1, w, Sz)
∗
⊢(q1, λ, z) .
Now let us write w = a1a2a3 · · · an. Then the ﬁrst step in
(q1, a1a2a3 · · · an, Sz)
∗
⊢(q1, λ, z)
(7.4)
must be a rule of the form (7.2) to get
(q1, a1a2a3 · · · an, Sz) ⊢(q1, a2a3 · · · an, u1z) .
But then the grammar has a rule of the form S →a1u1, so that
S ⇒a1u1.
Repeating this, writing u1 = Au2, we have
(q1, a2a3 · · · an, Au2z) ⊢(q1, a3 · · · an, u3u2z) ,

7.2 Pushdown Automata and Context-Free Languages
195
implying that A →a2u3 is in the grammar and that
S
∗⇒a1a2u3u2.
This makes it quite clear at any point the stack contents (excluding z)
are identical with the unmatched part of the sentential form, so that (7.4)
implies
S
∗⇒a1a2 · · · an.
In consequence, L (M) ⊆L (G), completing the proof if the language does
not contain λ.
If λ ∈L, we add to the constructed npda the transition
δ (q0, λ, z) = {(qf, z)}
so that the empty string is also accepted.
EXAMPLE 7.7
Consider the grammar
S →aA,
A →aABC |bB| a,
B →b,
C →c.
Since the grammar is already in Greibach normal form, we can use the
construction in the previous theorem immediately. In addition to rules
δ (q0, λ, z) = {(q1, Sz)}
and
δ (q1, λ, z) = {(qf, z)} ,
the pda will also have transition rules
δ (q1, a, S) = {(q1, A)} ,
δ (q1, a, A) = {(q1, ABC) , (q1, λ)} ,
δ (q1, b, A) = {(q1, B)} ,
δ (q1, b, B) = {(q1, λ)} ,
δ (q1, c, C) = {(q1, λ)} .

196
Chapter 7 Pushdown Automata
The sequence of moves made by M in processing aaabc is
(q0, aaabc, z) ⊢(q1, aaabc, Sz)
⊢(q1, aabc, Az)
⊢(q1, abc, ABCz)
⊢(q1, bc, BCz)
⊢(q1, c, Cz)
⊢(q1, λ, z)
⊢(qf, λ, z) .
This corresponds to the derivation
S ⇒aA ⇒aaABC ⇒aaaBC ⇒aaabC ⇒aaabc.
In order to simplify the arguments, the proof in Theorem 7.1 assumed
that the grammar was in Greibach normal form. It is not necessary to do
this; we can make a similar and only slightly more complicated construction
from a general context-free grammar. For example, for productions of the
form
A →Bx,
we remove A from the stack and replace it with Bx, but consume no input
symbol. For productions of the form
A →abCx,
we must ﬁrst match the ab in the input against a similar string in the stack
and then replace A with Cx. We leave the details of the construction and
the associated proof as an exercise.
Context-Free Grammars for Pushdown Automata
The converse of Theorem 7.1 is also true. The construction involved readily
suggests itself: Reverse the process in Theorem 7.1 so that the grammar
simulates the moves of the pda. This means that the content of the stack
should be reﬂected in the variable part of the sentential form, while the
processed input is the terminal preﬁx of the sentential form. Quite a few
details are needed to make this work.
To keep the discussion as simple as possible, we will assume that the
npda in question meets the following requirements:
1. It has a single ﬁnal state qf that is entered if and only if the stack is
empty;

7.2 Pushdown Automata and Context-Free Languages
197
2. With a ∈Σ {λ}, all transitions must have the form δ (qi, a, A) =
{c1, c2, ..., cn}, where
ci = (qj, λ) ,
(7.5)
or
ci = (qj, BC) .
(7.6)
That is, each move either increases or decreases the stack content by a single
symbol.
These restrictions may appear to be very severe, but they are not.
It can be shown that for any npda there exists an equivalent one having
properties 1 and 2. This equivalence is explored partially in Exercises 18
and 19 in Section 7.1. Here we need to explore it further, but again we
will leave the arguments as an exercise (see Exercise 17 at the end of this
section). Taking this as given, we now construct a context-free grammar
for the language accepted by the npda.
As stated, we want the sentential form to represent the content of the
stack. But the conﬁguration of the npda also involves an internal state, and
this has to be remembered in the sentential form as well. It is hard to see
how this can be done, and the construction we give here is a little tricky.
Suppose for the moment that we can ﬁnd a grammar whose variables
are of the form (qiAqj) and whose productions are such that
(qiAqj)
∗⇒v,
if and only if the npda erases A from the stack while reading v and going
from state qi to state qj. "Erasing" here means that A and its eﬀects (i.e.,
all the successive strings by which it is replaced) are removed from the stack,
bringing the symbol originally below A to the top. If we can ﬁnd such a
grammar, and if we choose (q0zqf) as its start symbol, then
(q0zqf)
∗⇒w
if and only if the npda removes z (creating an empty stack) while reading
w and going from q0 to qf. But this is exactly how the npda accepts w.
Therefore, the language generated by the grammar will be identical to the
language accepted by the npda.
To construct a grammar that satisﬁes these conditions, we examine the
diﬀerent types of transitions that can be made by the npda. Since (7.5)
involves an immediate erasure of A, the grammar will have a corresponding
production
(qiAqj) →a.
Productions of type (7.6) generate the set of rules
(qiAqk) →a (qjBql) (qlCqk) ,

198
Chapter 7 Pushdown Automata
where qk and ql take on all possible values in Q. This is due to the fact that
to erase A we ﬁrst replace it with BC, while reading an a and going from
state qi to qj. Subsequently, we go from qj to ql, erasing B, then from ql to
qk, erasing C.
In the last step, it may seem that we have added too much, as there
may be some states ql that cannot be reached from qj while erasing B.
This is true, but this does not aﬀect the grammar. The resulting variables
(qjBql) are useless variables and do not aﬀect the language accepted by the
grammar.
Finally, as a start variable we take (q0zqf), where qf is the single ﬁnal
state of the npda.
EXAMPLE 7.8
Consider the npda with transitions
δ (q0, a, z) = {(q0, Az)} ,
δ (q0, a, A) = {(q0, A)} ,
δ (q0, b, A) = {(q1, λ)} ,
δ (q1, λ, z) = {(q2, λ)} .
Using q0 as the initial state and q2 as the ﬁnal state, the npda satisﬁes
condition 1 above, but not 2. To satisfy the latter, we introduce a new
state q3 and an intermediate step in which we ﬁrst remove the A from
the stack, then replace it in the next move. The new set of transition
rules is
δ (q0, a, z) = {(q0, Az)} ,
δ (q3, λ, z) = {(q0, Az)} ,
δ (q0, a, A) = {(q3, λ)} ,
δ (q0, b, A) = {(q1, λ)} ,
δ (q1, λ, z) = {(q2, λ)} .
The last three transitions are of the form (7.5) so that they yield the
corresponding productions
(q0Aq3) →a,
(q0Aq1) →b,
(q1zq2) →λ.
From the ﬁrst two transitions we get the set of productions
(q0zq0) →a (q0Aq0) (q0zq0) |a (q0Aq1) (q1zq0)|
a (q0Aq2) (q2zq0) |a (q0Aq3) (q3zq0) ,
(q0zq1) →a (q0Aq0) (q0zq1) |a (q0Aq1) (q1zq1)|
a (q0Aq2) (q2zq1) |a (q0Aq3) (q3zq1) ,

7.2 Pushdown Automata and Context-Free Languages
199
(q0zq2) →a (q0Aq0) (q0zq2) |a (q0Aq1) (q1zq2)|
a (q0Aq2) (q2zq2) |a (q0Aq3) (q3zq2) ,
(q0zq3) →a (q0Aq0) (q0zq3) |a (q0Aq1) (q1zq3)|
a (q0Aq2) (q2zq3) |a (q0Aq3) (q3zq3) ,
(q3zq0)→(q0Aq0)(q0zq0)|(q0Aq1)(q1zq0)|(q0Aq2) (q2zq0)|(q0Aq3)(q3zq0) ,
(q3zq1)→(q0Aq0)(q0zq1)|(q0Aq1)(q1zq1)|(q0Aq2) (q2zq1)|(q0Aq3)(q3zq1) ,
(q3zq2)→(q0Aq0)(q0zq2)|(q0Aq1)(q1zq2)|(q0Aq2) (q2zq2)|(q0Aq3) (q3zq2) ,
(q3zq3)→(q0Aq0)(q0zq3)|(q0Aq1)(q1zq3)|(q0Aq2) (q2zq3)|(q0Aq3)(q3zq3) .
This looks quite complicated, but can be simpliﬁed. A variable
that does not occur on the left side of any production must be useless,
so we can immediately eliminate (q0Aq0) and (q0Aq2). Also, by looking
at the transition graph of the modiﬁed npda, we see that there is no
path from q1 to q0, from q1 to q1, from q1 to q3, and from q2 to q2, which
makes the associated variables also useless. When we eliminate all these
useless productions, we are left with the much shorter grammar
(q0Aq3) →a,
(q0Aq1) →b,
(q1zq2) →λ,
(q0zq0) →a(q0Aq3)(q3zq0),
(q0zq1) →a(q0Aq3)(q3zq1),
(q0zq2) →a(q0Aq1)(q1zq2)|a(q0Aq3)(q3zq2),
(q0zq3) →a(q0Aq3)(q3zq3),
(q3zq0) →(q0Aq3)(q3zq0),
(q3zq1) →(q0Aq3)(q3zq1),
(q3zq2) →(q0Aq1)(q1zq2)|(q0Aq3)(q3zq2),
(q3zq3) →(q0Aq3)(q3zq3),
with start variable (q0zq2).
EXAMPLE 7.9
Consider the string w = aab. This is accepted by the pda in Example
7.8, with successive conﬁgurations
(q0, aab, z) ⊢(q0, ab, Az)
⊢(q3, b, z)

200
Chapter 7 Pushdown Automata
⊢(q0, b, Az)
⊢(q1, λ, z)
⊢(q2, λ, λ) .
The corresponding derivation with G is
(q0zq2) ⇒a (q0Aq3) (q3zq2)
⇒aa (q3zq2)
⇒aa (q0Aq1) (q1zq2)
⇒aab (q1zq2)
⇒aab.
The steps in the proof of the following theorem will be easier to un-
derstand if you notice the correspondence between the successive in-
stantaneous descriptions of the pda and the sentential forms in the
derivation. The ﬁrst qi in the leftmost variable of every sentential form
is the current state of the pda, while the sequence of middle symbols
is the same as the stack content.
Although the construction yields a rather complicated grammar, it can
be applied to any pda whose transition rules satisfy the given conditions.
This forms the basis for the proof of the general result.
THEOREM 7.2
If L = L (M) for some npda M, then L is a context-free language.
Proof: Assume that M = (Q, Σ, Γ, δ, q0, z, {qf}) satisﬁes conditions 1 and
2 above.
We use the suggested construction to get the grammar G =
(V, T, S, P), with T = Σ and V consisting of elements of the form (qicqj).
We will show that the grammar so obtained is such that for all qi, qj, ∈
Q, A ∈Γ, X ∈Γ∗, u, v ∈Σ∗,
(qi, uv, AX)
∗
⊢(qj, v, X)
(7.7)
implies that
(qiAqj)
∗⇒u,
and vice versa.
The ﬁrst part is to show that, whenever the npda is such that the
symbol A and its eﬀects can be removed from the stack while reading u and
going from state qi to qj, then the variable (qiAqj) can derive u. This is not

7.2 Pushdown Automata and Context-Free Languages
201
hard to see since the grammar was explicitly constructed to do this. We
only need an induction on the number of moves to make this precise.
For the converse, consider a single step in the derivation such as
(qiAqk) ⇒a (qjBql) (qlCqk) .
Using the corresponding transition for the npda
δ (qi, a, A) = {(qj, BC) , ...} ,
(7.8)
we see that the A can be removed from the stack, BC put on, reading a,
with the control unit going from state qi to qj. Similarly, if
(qiAqj) ⇒a,
(7.9)
then there must be a corresponding transition
δ (qi, a, A) = {(qj, λ)}
(7.10)
whereby the A can be popped oﬀthe stack.
We see from this that the
sentential forms derived from (qiAqj) deﬁne a sequence of possible conﬁgu-
rations of the npda by which (7.7) can be achieved.
Note that (qiAqj) ⇒a (qjBql) (qlCqk) might be possible for some
(qjBql) (qlCqk) for which there is no corresponding transition of the form
(7.8) or (7.10). But, in that case, at least one of the variables on the right
will be useless. For all sentential forms leading to a terminal string, the
argument given holds.
If we now apply the conclusion to
(q0, w, z)
∗
⊢(qf, λ, λ) ,
we see that this can be so if and only if
(q0zqf)
∗⇒w.
Consequently L (M) = L (G).
EXERCISES
1. Construct a pda that accepts the language deﬁned by the grammar
S →abSb|λ.
2. Construct a pda that accepts the language deﬁned by the grammar
S →aSSSab|λ.

202
Chapter 7 Pushdown Automata
3. Construct an npda that accepts the language generated by the grammar
S →aSbb|abb.
4. Show that the pda constructed in Example 7.6 accepts the strings aabb and
aaabbbb, and that both strings are in the language generated by the given
grammar.
5. Prove that the pda in Example 7.6 accepts the language L =

an+1b2n : n ≥0

.
6. Construct an npda that accepts the language generated by the grammar
S →aSSS|ba.
7. Construct an npda corresponding to the grammar
S →aABB|aAA,
A →aBB|b,
B →bBB|A.
8. Construct an npda that will accept the language generated by the grammar
G = ({S, A} , {a, b} , S, P), with productions S →AA |a, A →SA| ab.
9. Show that Theorems 7.1 and 7.2 imply the following. For every npda M, there
exists an npda 
M with at most three states, such that L (M) = L


M

.
10. Show how the number of states of 
M in the above exercise can be reduced to
two.
11. Find an npda with two states for the language L =

anbn+1 : n ≥0

.
12. Find an npda with two states that accepts L =

anb2n : n ≥2

.
13. Show that the npda in Example 7.8 accepts L (aa∗b).
14. Show that the grammar in Example 7.8 generates the language L (aa∗b).
15. In Example 7.8, show that the variable (q0zq1) is useless.
16. Find a context-free grammar that generates the language accepted by the
npda M = ({q0, q1} , {a, b} , {A, z} , δ, q0, z, {q1}), with transitions
δ (q0, a, z) = {(q0, Az)} ,
δ (q0, b, A) = {(q0, AA)} ,
δ (q0, a, A) = {(q1, λ)} .
17. Show that for every npda there exists an equivalent one, satisfying conditions
1 and 2 in the preamble to Theorem 7.2.
18. Give full details of the proof of Theorem 7.2.
19. Give a construction by which an arbitrary context-free grammar can be used
in the proof of Theorem 7.1.
20. Does the grammar in Example 7.8 still have any useless variables?

7.3 Pushdown Automata and Context-Free Languages
203
7.3
DETERMINISTIC PUSHDOWN AUTOMATA AND
DETERMINISTIC CONTEXT-FREE LANGUAGES
A deterministic pushdown accepter (dpda) is a pushdown automaton
that never has a choice in its move. This can be achieved by a modiﬁcation
of Deﬁnition 7.1.
DEFINITION 7.3
A pushdown automaton M = (Q, Σ, Γ, δ, q0, z, F) is said to be deterministic
if it is an automaton as deﬁned in Deﬁnition 7.1, subject to the restrictions
that, for every q ∈Q, a ∈Σ ∪{λ} and b ∈Γ,
1. δ (q, a, b) contains at most one element,
2. if δ (q, λ, b) is not empty, then δ (q, c, b) must be empty for every c ∈Σ.
The ﬁrst of these conditions simply requires that for any given input symbol
and any stack top, at most one move can be made. The second condition is
that when a λ-move is possible for some conﬁguration, no input-consuming
alternative is available.
It is interesting to note the diﬀerence between this deﬁnition and the
corresponding deﬁnition of a deterministic ﬁnite automaton. The domain
of the transition function is still as in Deﬁnition 7.1 rather than Q × Σ × Γ
because we want to retain λ-transitions. Since the top of the stack plays a
role in determining the next move, the presence of λ-transitions does not
automatically imply nondeterminism. Also, some transitions of a dpda may
be to the empty set, that is, undeﬁned, so there may be dead conﬁgurations.
This does not aﬀect the deﬁnition; the only criterion for determinism is that
at all times at most one possible move exists.
DEFINITION 7.4
A language L is said to be a deterministic context-free language if and
only if there exists a dpda M such that L = L (M).
EXAMPLE 7.10
The language
L = {anbn : n ≥0}

204
Chapter 7 Pushdown Automata
is a deterministic context-free language. The pda M = ({q0, q1, q2} ,
{a, b} , {0, 1} , δ, q0, 0, {q0}) with
δ (q0, a, 0) = {(q1, 10)} ,
δ (q1, a, 1) = {(q1, 11)} ,
δ (q1, b, 1) = {(q2, λ)} ,
δ (q2, b, 1) = {(q2, λ)} ,
δ (q2, λ, 0) = {(q0, λ)}
accepts the given language. It satisﬁes the conditions of Deﬁnition 7.3
and is therefore deterministic.
Look now at Example 7.5. The npda there is not deterministic because
δ (q0, a, a) = {(q0, aa)}
and
δ (q0, λ, a) = {(q1, a)}
violate condition 2 of Deﬁnition 7.3. This, of course, does not imply that
the language {wwR} itself is nondeterministic, since there is the possibility
of an equivalent dpda. But it is known that the language is indeed not
deterministic. From this and the next example we see that, in contrast to
ﬁnite automata, deterministic and nondeterministic pushdown automata are
not equivalent. There are context-free languages that are not deterministic.
EXAMPLE 7.11
Let
L1 = {anbn : n ≥0}
and
L2 =

anb2n : n ≥0

.
An obvious modiﬁcation of the argument that L1 is a context-free
language shows that L2 is also context free. The language
L = L1 ∪L2
is context-free as well. This will follow from a general theorem to be
presented in the next chapter, but can easily be made plausible at this
point. Let G1 = (V1, T, S1, P1) and G2 = (V2, T, S2, P2) be context-free
grammars such that L1 = L (G1) and L2 = L (G2). If we assume that

7.3 Pushdown Automata and Context-Free Languages
205
V1 and V2 are disjoint and that S /∈V1 ∪V2, then, combining the two,
grammar G = (V1 ∪V2 ∪{S} , T, S, P), where
P = P1 ∪P2 ∪{S →S1|S2} ,
generates L1 ∪L2. This should be fairly clear at this point, but the de-
tails of the argument will be deferred until Chapter 8. Accepting this,
we see that L is context free. But L is not a deterministic context-free
language. This seems reasonable, since the pda has either to match one
b or two against each a, and so has to make an initial choice whether
the input is in L1 or in L2. There is no information available at the be-
ginning of the string by which the choice can be made deterministically.
Of course, this sort of argument is based on a particular algorithm we
have in mind; it may lead us to the correct conjecture, but does not
prove anything. There is always the possibility of a completely diﬀer-
ent approach that avoids an initial choice. But it turns out that there
is not, and L is indeed nondeterministic. To see this we ﬁrst establish
the following claim. If L were a deterministic context-free language,
then
	L = L ∪{anbncn : n ≥0}
would be a context-free language. We show the latter by constructing
an npda 
M for 	L, given a dpda M for L.
The idea behind the construction is to add to the control unit of
M a similar part in which transitions caused by the input symbol b are
replaced with similar ones for input c. This new part of the control
unit may be entered after M has read anbn. Since the second part
responds to cn in the same way as the ﬁrst part does to bn, the process
that recognizes anb2n now also accepts anbncn. Figure 7.4 describes
the construction graphically; a formal argument follows.
λ
b
n
b
n
λ
an
cn
Addition
Control unit of M
FIGURE 7.4

206
Chapter 7 Pushdown Automata
Let M = (Q, Σ, Γ, δ, q0, z, F) with
Q = {q0, q1, ..., qn} .
Then consider 
M =

	Q, Σ, Γ, δ ∪	δ, z, 	F

with
	Q = Q ∪{	q0, 	q1, ..., 	qn} ,
	F = F ∪{	qi : qi ∈F} ,
and 	δ constructed from δ by including
	δ (qf, λ, s) = {(	qf, s)} ,
for all qf ∈F, s ∈Γ, and
	δ (	qi, c, s) = {(	qj, u)} ,
for all
δ (qi, b, s) = {(qj, u)} ,
qi ∈Q, s ∈Γ, u ∈Γ∗. For M to accept anbn we must have
(q0, anbn, z)
∗
⊢M (qi, λ, u) ,
with qi ∈F. Because M is deterministic, it must also be true that

q0, anb2n, z
 ∗
⊢M (qi, bn, u) ,
so that for it to accept anb2n we must further have
(qi, bn, u)
∗
⊢M (qj, λ, u1) ,
for some qj ∈F. But then, by construction
(	qi, cn, u)
∗
⊢
M (	qj, λ, u1) ,
so that 
M will accept anbncn. It remains to be shown that no strings
other than those in 	L are accepted by 
M; this is considered in several
exercises at the end of this section. The conclusion is that 	L = L


M

,
so that 	L is context-free. But we will show in the next chapter (Ex-
ample 8.1) that 	L is not context-free. Therefore, our assumption that
L is a deterministic context-free language must be false.

7.4 Grammars for Deterministic Context-Free Languages*
207
EXERCISES
1. Show that L = {anbm, n > m} is a deterministic context-free language.
2. Show that L =

anb2n : n ≥0

is a deterministic context-free language.
3. Show that L = {anbm, n < m} is a deterministic context-free language.
4. Show that L = {anbm : m ≥n + 3} is deterministic.
5. Is the language L = {anbn : n ≥1} ∪{b} deterministic?
6. Is the language L = {anbn : n ≥1} ∪{a} deterministic?
7. Show that the pushdown automaton in Example 7.4 is not deterministic, but
that the language in the example is nevertheless deterministic.
8. For the language L in Exercise 1, show that L∗is a deterministic context-free
language.
9. Give reasons one might conjecture that the following language is not
deterministic:
L =

anbmck : n = m or m = k

.
10. Is the language L = {anbm : n = m or n = m + 1} deterministic?
11. Is the language

wcwR : w ∈{a, b}∗
deterministic?
12. While the language in Exercise 11 is deterministic, the closely related lan-
guage L =

wwR : w ∈{a, b}∗
is known to be nondeterministic. Give argu-
ments that make this statement plausible.
13. Show that L = {w ∈{a, b}∗: na (w) ̸= nb (w)} is a deterministic context-free
language.
14. Show that L = {anbm, n < 2m} is a deterministic context-free language.
15. Show that if L1 is deterministic context-free and L2 is regular, then the
language L1 ∪L2 is deterministic context-free.
16. Show that under the conditions of Exercise 15, L1 ∩L2 is a deterministic
context-free language.
17. Give an example of a deterministic context-free language whose reverse is not
deterministic.
7.4
GRAMMARS FOR DETERMINISTIC CONTEXT-FREE
LANGUAGES*
The importance of deterministic context-free languages lies in the fact that
they can be parsed eﬃciently. We can see this intuitively by viewing the
pushdown automaton as a parsing device. Since there is no backtracking
involved, we can easily write a computer program for it, and we may expect
that it will work eﬃciently. Since there may be λ-transitions involved, we

208
Chapter 7 Pushdown Automata
cannot immediately claim that this will yield a linear-time parser, but it
puts us on the right track nevertheless. To pursue this, let us see what
grammars might be suitable for the description of deterministic context-
free languages. Here we enter a topic important in the study of compilers,
but somewhat peripheral to our interests.
We will provide only a brief
introduction to some important results, referring the reader to books on
compilers for a more thorough treatment.
Suppose we are parsing top-down, attempting to ﬁnd the leftmost deriv-
ation of a particular sentence. For the sake of discussion, we use the ap-
proach illustrated in Figure 7.5. We scan the input w from left to right,
while developing a sentential form whose terminal preﬁx matches the preﬁx
of w up to the currently scanned symbol. To proceed in matching consecu-
tive symbols, we would like to know exactly which production rule is to be
applied at each step. This would avoid backtracking and give us an eﬃcient
parser. The question then is whether there are grammars that allow us to
do this. For a general context-free grammar, this is not the case, but if the
form of the grammar is restricted, we can achieve our goal.
As ﬁrst case, take the s-grammars introduced in Deﬁnition 5.4. From
the discussion there, it is clear that at every stage in the parsing we know
exactly which production has to be applied. Suppose that w = w1w2 and
that we have developed the sentential form w1Ax. To get the next symbol
of the sentential form matched against the next symbol in w, we simply
look at the leftmost symbol of w2, say a. If there is no rule A →ay in the
grammar, the string w does not belong to the language. If there is such a
rule, the parsing can proceed. But in this case there is only one such rule,
so there is no choice to be made.
Although s-grammars are useful, they are too restrictive to capture all
aspects of the syntax of programming languages. We need to generalize the
idea so that it becomes more powerful without losing its essential property
for parsing. One type of grammar is called an LL grammar. In an LL
grammar we still have the property that we can, by looking at a limited
part of the input (consisting of the scanned symbol plus a ﬁnite number
of symbols following it), predict exactly which production rule must be
used. The term LL is standard usage in books on compilers; the ﬁrst L
stands for the fact that the input is scanned from left to right; the second
L indicates that leftmost derivations are constructed. Every s-grammar is
an LL grammar, but the concept is more general.
a1
a2
a3          a4  .  .  .  an
a1
a2
a3         A  .  .  .
Matched part
Yet to be matched
Input w
Sentential form
FIGURE 7.5

7.4 Grammars for Deterministic Context-Free Languages*
209
EXAMPLE 7.12
The grammar
S →aSb|ab
is not an s-grammar, but it is an LL grammar. In order to determine
which production is to be applied, we look at two consecutive symbols
of the input string. If the ﬁrst is an a and the second a b, we must
apply the production S →ab. Otherwise, the rule S →aSb must be
used.
We say that a grammar is an LL (k) grammar if we can uniquely identify
the correct production, given the currently scanned symbol and a "look-
ahead" of the next k −1 symbols. Example 7.12 is an example of an LL (2)
grammar.
EXAMPLE 7.13
The grammar
S →SS |aSb| ab
generates the positive closure of the language in Example 7.12. As re-
marked in Example 5.4, this is the language of properly nested paren-
thesis structures. The grammar is not an LL (k) grammar for any k.
To see why this is so, look at the derivation of strings of length
greater than two. To start, we have available two possible productions
S →SS and S →aSb. The scanned symbol does not tell us which is
the right one. Suppose we now use a look-ahead and consider the ﬁrst
two symbols, ﬁnding that they are aa. Does this allow us to make the
right decision? The answer is still no, since what we have seen could
be a preﬁx of a number of strings, including both aabb or aabbab. In
the ﬁrst case, we must start with S →aSb, while in the second it is
necessary to use S →SS. The grammar is therefore not an LL (2)
grammar. In a similar fashion, we can see that no matter how many
look-ahead symbols we have, there are always some situations that
cannot be resolved.
This observation about the grammar does not imply that the lan-
guage is not deterministic or that no LL grammar for it exists. We can
ﬁnd an LL grammar for the language if we analyze the reason for the
failure of the original grammar. The diﬃculty lies in the fact that we
cannot predict how many repetitions of the basic pattern anbn there
are until we get to the end of the string, yet the grammar requires
an immediate decision. Rewriting the grammar avoids this diﬃculty.

210
Chapter 7 Pushdown Automata
The grammar
S →aSbS|λ
is an LL-grammar nearly equivalent to the original grammar.
To see this, consider the leftmost derivation of w = abab. Then
S ⇒aSbS ⇒abS ⇒abaSbS ⇒ababS ⇒abab.
We see that we never have any choice. When the input symbol exam-
ined is a, we must use S →aSbS, when the symbol is b or we are at
the end of the string, we must use S →λ.
But the problem is not yet completely solved because the new
grammar can generate the empty string. We ﬁx this by introducing a
new start variable S0 and a production to ensure that some nonempty
string is generated. The ﬁnal result
S0 →aSbS,
S →aSbS|λ
is then an LL-grammar equivalent to the original grammar.
While this informal description of LL grammars is adequate for under-
standing simple examples, we need a more precise deﬁnition if any rigor-
ous results are to be developed. We conclude our discussion with such a
deﬁnition.
DEFINITION 7.5
Let G = (V, T, S, P) be a context-free grammar. If for every pair of leftmost
derivations
S
∗⇒w1Ax1 ⇒w1y1x1
∗⇒w1w2,
S
∗⇒w1Ax2 ⇒w1y2x2
∗⇒w1w3,
with w1, w2, w3 ∈T ∗, the equality of the k leftmost symbols of w2 and w3
implies y1 = y2, then G is said to be an LL (k) grammar. (If |w2| or |w3| is
less than k, then k is replaced by the smaller of these.)
The deﬁnition makes precise what has already been indicated. If at any
stage in the leftmost derivation (w1Ax) we know the next k symbols of the
input, the next step in the derivation is uniquely determined (as expressed
by y1 = y2).
The topic of LL grammars is an important one in the study of compil-
ers. A number of programming languages can be deﬁned by LL grammars,

7.4 Grammars for Deterministic Context-Free Languages*
211
and many compilers have been written using LL parsers. But LL grammars
are not suﬃciently general to deal with all deterministic context-free lan-
guages. Consequently, there is interest in other, more general deterministic
grammars. Particularly important are the so-called LR grammars, which
also allow eﬃcient parsing, but can be viewed as constructing the derivation
tree from the bottom up. There is a great deal of material on this subject
that can be found in books on compilers (e.g., Hunter 1981) or books specif-
ically devoted to parsing methods for formal languages (such as Aho and
Ullman 1972).
EXERCISES
1. Give LL grammars for the following languages, assuming Σ = {a, b, c}:
(a) L =

anbmcn+m : n ≥1, m ≥1

.
(b) L =

an+2bmcn+m : n ≥1, m ≥1

.
(c) L =

anbn+2cm : n ≥1, m > 2

.
2. Show that the second grammar in Example 7.13 is an LL grammar and that
it is equivalent to the original grammar.
3. Show that the grammar for L = {w : na (w) = nb (w)} given in Example 1.13
is not an LL grammar.
4. Construct an LL grammar for the language L (aa∗ba) ∪L (abbb∗).
5. Show that any LL grammar is unambiguous.
6. Show that if G is an LL (k) grammar, then L (G) is a deterministic context-
free language.
7. Show
that
a
deterministic
context-free
language
is
never
inherently
ambiguous.
8. Let G be a context-free grammar in Greibach normal form.
Describe an
algorithm which, for any given k, determines whether or not G is an LL (k)
grammar.


8
CHA P T E R
PROPERTIES OF
CONTEXT-FREE
LANGUAGES
CHAPTER SUMMARY
In this chapter, we study the general properties for context-free lan-
guages to compare with what we know about the properties of regular
languages. We see here that when it comes to closure and decision al-
gorithms, these properties tend to be more complicated for context-free
languages. This is especially true for the two pumping lemmas, one for
context-free languages, the other for linear languages. While the basic
idea is the same as for regular languages, specific arguments normally
involve much more detail.
T
he family of context-free languages occupies a central position in a
hierarchy of formal languages. On the one hand, context-free lan-
guages include important but restricted language families such as
regular and deterministic context-free languages. On the other hand, there
are broader language families of which context-free languages are a special
case. To study the relationship between language families and to exhibit
their similarities and diﬀerences, we investigate characteristic properties of
the various families. As in Chapter 4, we look at closure under a variety of
operations, algorithms for determining properties of members of the family,
and structural results such as pumping lemmas. These all provide us with
213

214
Chapter 8 Properties of Context-Free Languages
a means of understanding relations between the diﬀerent families as well as
for classifying speciﬁc languages in an appropriate category.
8.1
TWO PUMPING LEMMAS
The pumping lemma given in Theorem 4.8 is an eﬀective tool for showing
that certain languages are not regular. Similar pumping lemmas are known
for other language families. Here we will discuss two such results, one for
context-free languages in general, the other for a restricted type of context-
free language.
The pumping lemma for regular languages is based on the fact that all
strings in a regular language must exhibit a certain repetitive pattern. If
a language L contains a string that does not follow this pattern, then L
cannot be regular. A similar reasoning leads to the pumping lemma for
context-free languages. For regular languages, the pattern is a consequence
of the ﬁniteness of the representing dfa; with context-free languages, the
pattern is most easily seen via the grammar.
Suppose a variable repeats in the sense that A
∗⇒vAy, with A
∗⇒x.
The derived sentence will then contain the substring vxy. There will also
be sentences in the language that contain substrings x, vvxyy, vvvxyyy,
and so on.
While, in essence, the pumping lemma for context-free languages is no
diﬀerent from the pumping lemma for regular languages, its application is
complicated by the fact that now the pumped string consists of two separate
parts, and that it can occur anywhere in the string.
A Pumping Lemma for Context-Free Languages
THEOREM 8.1
Let L be an inﬁnite context-free language. Then there exists some positive
integer m such that any w ∈L with |w| ≥m can be decomposed as
w = uvxyz,
(8.1)
with
|vxy| ≤m,
(8.2)
and
|vy| ≥1,
(8.3)
such that
uvixyiz ∈L,
(8.4)

8.1 Two Pumping Lemmas
215
for all i = 0, 1, 2, .... This is known as the pumping lemma for context-free
languages.
Proof: Consider the language L −{λ}, and assume that we have for it a
grammar G without unit-productions or λ-productions. Since the length
of the string on the right side of any production is bounded, say by k, the
length of the derivation of any w ∈L must be at least |w| /k. Therefore,
since L is inﬁnite, there exist arbitrarily long derivations and corresponding
derivation trees of arbitrary height.
Consider now such a high derivation tree and some suﬃciently long path
from the root to a leaf. Since the number of variables in G is ﬁnite, there
must be some variable that repeats on this path, as shown schematically in
Figure 8.1. Corresponding to the derivation tree in Figure 8.1, we have the
derivation
S
∗⇒uAz
∗⇒uvAyz
∗⇒uvxyz,
where u, v, x, y, and z are all strings of terminals. From the above we see
that A
∗⇒vAy and A
∗⇒x, so all the strings uvixyiz, i = 0, 1, 2, ..., can
be generated by the grammar and are therefore in L. Furthermore, in the
derivations A
∗⇒vAy and A
∗⇒x, we can assume that no variable repeats.
To see this, look at the sketch of the derivation tree in Figure 8.1. In the
subtree T5 no variable repeats; otherwise we could just apply the argument
to this repeating variable. Similarly, we can assume that no variable repeats
S
T1
T2
A
A
T3
T4
T5
u
z
v
y
x
FIGURE 8.1

216
Chapter 8 Properties of Context-Free Languages
in the subtrees T3 and T4. Therefore, the lengths of the strings v, x, and
y depend only on the productions of the grammar and can be bounded
independently of w so that (8.2) holds. Finally, since there are no unit-
productions and no λ-productions, v and y cannot both be empty strings,
giving (8.3).
This completes the argument that (8.1) to (8.4) hold.
This pumping lemma is useful in showing that a language does not
belong to the family of context-free languages. Its application is typical of
pumping lemmas in general; they are used negatively to show that a given
language does not belong to some family. As in Theorem 4.8, the correct
argument can be visualized as a game against an intelligent opponent. But
now the rules make it a little more diﬃcult for us. For regular languages,
the substring xy whose length is bounded by m starts at the left end of
w. Therefore, the substring y that can be pumped is within m symbols of
the beginning of w. For context-free languages, we only have a bound on
|vxy|. The substring u that precedes vxy can be arbitrarily long. This gives
additional freedom to the adversary, making arguments involving Theorem
8.1 a little more complicated.
EXAMPLE 8.1
Show that the language
L = {anbncn : n ≥0}
is not context free.
Once the adversary has chosen m, we pick the string ambmcm,
which is in L. The adversary now has several choices. If he chooses
vxy to contain only a's, then the pumped string will obviously not be
in L. If he chooses a decomposition so that v and y are composed of
an equal number of a's and b's, then the pumped string akbkcm with
k ̸= m can be generated, and again we have generated a string not in
L. In fact, the only way the adversary could stop us from winning is to
pick vxy so that vy has the same number of a's, b's, and c's. But this
is not possible because of restriction (8.2). Therefore, L is not context
free.
If we try the same argument on the language L = {anbn}, we fail,
as we must, since the language is context free. If we pick any string in
L, such as w = ambm, the adversary can pick v = ak and y = bk. Now,
no matter what i we choose, the resulting pumped string wi is in L.
Remember, though, that this does not prove that L is context free; all
we can say is that we have been unable to get any conclusion from the

8.1 Two Pumping Lemmas
217
pumping lemma. That L is context free must come from some other
argument, such as the construction of a context-free grammar.
The argument also justiﬁes a claim made in Example 7.11 and
allows us to close a gap in that example. The language
L = {anbn} ∪

anb2n
∪{anbncn}
is not context free. The string ambmcm is in L, but the pumped result
is not.
EXAMPLE 8.2
Consider the language
L =

ww : w ∈{a, b}∗
.
Although this language appears to be very similar to the context-free
language of Example 5.1, it is not context free.
Take the string
ambmambm.
There are many ways in which the adversary can now pick vxy, but
for all of them we have a winning countermove. For example, for the
choice in Figure 8.2, we can use i = 0 to get a string of the form
akbjambm, k < m or j < m,
which is not in L. For other choices by the adversary, similar arguments
can be made. We conclude that L is not context free.
am
a a . . . . . . . . . . . . . a
u
x
v
y
z
bm
b b . . . . . . . . . . . . . b a a . . . . . . . . . . . . .  a
am
bm
a a . . . . . . . . . . . . . b
FIGURE 8.2
EXAMPLE 8.3
Show that the language
L =

an! : n ≥0

is not context free.

218
Chapter 8 Properties of Context-Free Languages
Given the opponent's choice for m, we pick a = am!. Obviously,
whatever the decomposition is, it must be of the form v = ak, y = al.
Then w0 = uxz has length m! −(k + l). This string is in L only if
m! −(k + l) = j!
for some j. But this is impossible, since with k + l ≤m,
m −(k + l) > (m −1)!.
Therefore, the language is not context free.
EXAMPLE 8.4
Show that the language
L =

anbj : n = j2
is not context free.
Given m in Theorem 8.1, we pick as our string am2bm. The adver-
sary now has several choices. The only one that requires much thought
is the one shown in Figure 8.3. Pumping i times will yield a new string
with m2 + (i −1) k1 a's and m + (i −1) k2 b's. If the adversary takes
k1 ̸= 0, k2 ̸= 0, we can pick i = 0. Since
(m −k2)2 ≤(m −1)2
= m2 −2m + 1
< m2 −k1,
the result is not in L. If the opponent picks k1 = 0, k2 ̸= 0 or k1 ̸= 0,
k2 = 0, then again with i = 0, the pumped string is not in L. We can
conclude from this that L is not a context-free language.
am2
bk2
a . . . . . . . . . . . . . . . . . . . . . .  a
u
x
v
y
z
ak1
bm
b . . . . . . . . . . . b
FIGURE 8.3

8.1 Two Pumping Lemmas
219
A Pumping Lemma for Linear Languages
We previously made a distinction between linear and nonlinear context-free
grammars. We now make a similar distinction between languages.
DEFINITION 8.1
A context-free language L is said to be linear if there exists a linear context-
free grammar G such that L = L (G).
Clearly, every linear language is context free, but we have not yet es-
tablished whether or not the converse is true.
EXAMPLE 8.5
The language L = {anbn : n ≥0} is a linear language.
A linear
grammar for it is given in Example 1.11.
The grammar given in
Example 1.13 for the language L = {w : na (w) = nb (w)} is not linear,
so the second language is not necessarily linear.
Of course, just because a speciﬁc grammar is not linear does not imply
that the language generated by it is not linear. If we want to prove that a
language is not linear, we must show that there exists no equivalent linear
grammar. We approach this in the usual way, establishing structural prop-
erties for linear languages, then showing that some context-free languages
do not have a required property.
THEOREM 8.2
Let L be an inﬁnite linear language. Then there exists some positive integer
m, such that any w ∈L, with |w| ≥m can be decomposed as w = uvxyz
with
|uvyz| ≤m,
(8.5)
|vy| ≥1,
(8.6)
such that
uvixyiz ∈L,
(8.7)
for all i = 0, 1, 2, ....
Note that the conclusions of this theorem diﬀer from those of Theo-
rem 8.1, since (8.2) is replaced by (8.5). This implies that the strings v and

220
Chapter 8 Properties of Context-Free Languages
y to be pumped must now be located within m symbols of the left and right
ends of w, respectively. The middle string x can be of arbitrary length.
Proof: Since the language is linear there exists a linear grammar G for it.
For the argument it is convenient to assume that G has no unit-productions
and no λ-productions. An examination of the proofs of Theorem 6.3 and
6.4 makes it clear that removing unit-productions and λ-productions does
not destroy the linearity of the grammar. We can therefore assume that G
has the required property.
Consider now the derivation of a string w ∈L(G)
S
∗⇒uAz
∗⇒uvAyz
∗⇒uvxyz = w.
Assume, for the moment, that for every w ∈L(G), there is a variable A,
such that
1. in the partial derivation S
∗⇒uAz no variable is repeated,
2. in the partial derivation S
∗⇒uAz
∗⇒uvAyz no variable except A is
repeated,
3. the repetition of A must occur in the ﬁrst m steps, where m can depend
on the grammar, but not on w.
If this is true, then the lengths of u, v, y, z must be bounded independent
of w. This in turn implies that (8.5), (8.6), and (8.7) must hold.
To complete the argument, we must still demonstrate that the above
conditions hold for every linear grammar. This is not hard to see if we look
at sequences in which the variables can occur. We will omit the details here,
but leave them as an exercise (see Exercise 16 at the end of this section).
EXAMPLE 8.6
The language
L = {w : na (w) = nb (w)}
is not linear.
To show this, assume that the language is linear and apply Theo-
rem 8.2 to the string
w = amb2mam.
Inequality (8.5) shows that in this case the strings u, v, y, z must
all consist entirely of a's.
If we pump this string once, we get
am+kb2mam+l, with either k ≥1 or l ≥1, a result that is not in
L. This contradiction of Theorem 8.2 proves that the language is not
linear.

8.1 Two Pumping Lemmas
221
This example answers the general question raised on the relation be-
tween the families of context-free and linear languages. The family of linear
languages is a proper subset of the family of context-free languages.
EXERCISES
1. Show that the language
L = {w : na (w) < nb (w) < nc (w)}
is not context free.
2. Show that the language
L = {w ∈{a, b, c}∗: na(w) = nb(w) ≥nc(w)}
is not context free.
3. Determine whether or not the language L = {anbicjdk, n + k ≤i + j} is
context free.
4. Show that the language L = {an : n is a prime number} is not context free.
5. Is the language L = {anbm : m = 2n} context free?
6. Show that the language L =

an2 : n ≥0

is not context free.
7. Show that the following languages on Σ = {a, b, c} are not context free:
(a) L =

anbj : n ≤j2
.
(b) L =

anbj : n ≥(j −1)3
.
(c) L =

anbjck : k = jn

.
(d) L =

anbjck : k > n, k > j

.
(e) L =

anbjck : n < j, n ≤k ≤j

.
(f) L = {w : na(w) = nb(w) ∗nc(w)}.
(g) L = {w ∈{a, b, c}∗: na (w) + nb (w) = 2nc (w) , na(w) = nb(w)}.
(h) L = {anbm : n and m are both prime}.
(i) L = {anbm : n is prime or m is prime}.
(j) L = {anbm : n is prime and m is not prime}.
8. Determine whether or not the following languages are context free:
(a) L =

anwwRbn : n ≥0, w ∈{a, b}∗
.
(b) L =

anbjanbj : n ≥0, j ≥0

.
(c) L =

anbjajbn : n ≥0, j ≥0

.

222
Chapter 8 Properties of Context-Free Languages
(d) L =

anbjakbl : n + j ≤k + l

.
(e) L =

anbjakbl : n ≤k, j ≤l

.
(f) L =

anbncj : n ≥j

.
(g) L =

anbnck, k = 2n

.
9. In Theorem 8.1, ﬁnd a bound for m in terms of the properties of the gram-
mar G.
10. Determine whether or not the following language is context free:
L = {w1cw2 : w1, w2 ∈{a, b}∗, w1 ̸= w2} .
11. Show that the language L = {anbnambm : n ≥0, m ≥0} is context free but
not linear.
12. Show that the following language is not linear.
L = {w : na (w) ≤nb (w)}.
13. Show that the language L = {w ∈{a, b, c}∗: na (w) + nb (w) = nc (w)} is
context free, but not linear.
14. Determine whether or not the language L =

anbj : j ≤n ≤2j −1

is linear.
15. Determine whether or not the language L = {anbmcn} ∪{anbncm} is linear.
16. Let G be a linear grammar with k variables. Show that when we write any
sequence of variables, there must be some variable A that repeats so that
(a) The ﬁrst occurrence of A must be in position p ≤k,
(b) The repetition of A must be no later than q ≤k + 1, and
(c) There can be no other repeating variable between positions p
and q.
17. Justify the claim made in Theorem 8.2 that for any linear language (not
containing λ) there exists a linear grammar without λ-productions and unit-
productions.
18. Consider the set of all strings a/b, where a and b are positive decimal integers
such that a < b.
The set of strings then represents all possible decimal
fractions. Determine whether or not this is a context-free language.
19. Show that the complement of the language in Exercise 7 is not context free.
20. Is the language L = {anm : n and m are prime numbers} context free?
21. It is known that the language
L = {anbncm : n ̸= m}
is not context free. (See the next exercise.) Show that, in spite of this, it is
not possible to use Theorem 8.1 to prove it.

8.2 Closure Properties and Decision Algorithms
223
22. Ogden's lemma is an extension of Theorem 8.1 that necessitates some
changes in the way the pumping lemma game is played. In particular,
(a) You can choose any w ∈L with |w| ≥m, but you must mark at
least m symbols in w. You can choose which symbols to mark.
(b) The opponent must select the decomposition w = uvxyz with the
additional restriction that either vx or xy must have at least one
marked position.
Note that Theorem 8.1 is a special case of Ogden's lemma in which all symbols
of w are marked. Show how Ogden's lemma can be used to prove that the
language in the previous exercise is not context free, and conclude from this
that Ogden's lemma is more powerful than Theorem 8.1.
8.2
CLOSURE PROPERTIES AND DECISION
ALGORITHMS FOR CONTEXT-FREE LANGUAGES
In Chapter 4 we looked at closure under certain operations and algorithms
to decide on the properties of the family of regular languages. On the whole,
the questions raised there had easy answers. When we ask the same ques-
tions about context-free languages, we encounter more diﬃculties. First,
closure properties that hold for regular languages do not always hold for
context-free languages. When they do, the arguments needed to prove them
are often quite complicated. Second, many intuitively simple and important
questions about context-free languages cannot be answered. This statement
may seem at ﬁrst surprising and will need to be elaborated as we proceed.
In this section, we provide only a sample of some of the most important
results.
Closure of Context-Free Languages
THEOREM 8.3
The family of context-free languages is closed under union, concatenation,
and star-closure.
Proof: Let L1 and L2 be two context-free languages generated by the
context-free grammars G1 = (V1, T1, S1, P1) and G2 = (V2, T2, S2, P2), re-
spectively. We can assume without loss of generality that the sets V1 and
V2 are disjoint.
Consider now the language L (G3), generated by the grammar
G3 = (V1 ∪V2 ∪{S3} , T1 ∪T2, S3, P3) ,

224
Chapter 8 Properties of Context-Free Languages
where S3 is a variable not in V1 ∪V2. The productions of G3 are all the
productions of G1 and G2, together with an alternative starting production
that allows us to use one or the other grammars. More precisely,
P3 = P1 ∪P2 ∪{S3 →S1|S2} .
Obviously, G3 is a context-free grammar, so that L (G3) is a context-free
language. But it is easy to see that
L (G3) = L1 ∪L2.
(8.8)
Suppose, for instance, that w ∈L1. Then
S3 ⇒S1
∗⇒w
is a possible derivation in grammar G3. A similar argument can be made
for w ∈L2. Also, if w ∈L (G3), then either
S3 ⇒S1
(8.9)
or
S3 ⇒S2
(8.10)
must be the ﬁrst step of the derivation. Suppose (8.9) is used. Since senten-
tial forms derived from S1 have variables in V1, and V1 and V2 are disjoint,
the derivation
S1
∗⇒w
can involve productions in P1 only. Hence w must be in L1. Alternatively,
if (8.10) is used ﬁrst, then w must be in L2 and it follows that L (G3) is the
union of L1 and L2.
Next, consider
G4 = (V1 ∪V2 ∪{S4} , T1 ∪T2, S4, P4) .
Here again S4 is a new variable and
P4 = P1 ∪P2 ∪{S4 →S1S2} .
Then
L (G4) = L (G1) L (G2)
follows easily.
Finally, consider L (G5) with
G5 = (V1 ∪{S5} , T1, S5, P5) ,
where S5 is a new variable and
P5 = P1 ∪{S5 →S1S5|λ} .

8.2 Closure Properties and Decision Algorithms
225
Then
L (G5) = L (G1)∗.
Thus we have shown that the family of context-free languages is closed
under union, concatenation, and star-closure.
THEOREM 8.4
The family of context-free languages is not closed under intersection and
complementation.
Proof: Consider the two languages
L1 = {anbncm : n ≥0, m ≥0}
and
L2 = {anbmcm : n ≥0, m ≥0} .
There are several ways one can show that L1 and L2 are context free. For
instance, a grammar for L1 is
S →S1S2,
S1 →aS1b|λ,
S2 →cS2|λ.
Alternatively, we note that L1 is the concatenation of two context-free lan-
guages, so it is context free by Theorem 8.3. But
L1 ∩L2 = {anbncn : n ≥0} ,
which we have already shown not to be context free. Thus, the family of
context-free languages is not closed under intersection.
The second part of the theorem follows from Theorem 8.3 and the set
identity
L1 ∩L2 = L1 ∪L2.
If the family of context-free languages were closed under complementation,
then the right side of the above expression would be a context-free language
for any context-free L1 and L2. But this contradicts what we have just
shown, that the intersection of two context-free languages is not necessar-
ily context-free. Consequently, the family of context-free languages is not
closed under complementation.
While the intersection of two context-free languages may produce a
language that is not context free, the closure property holds if one of the
languages is regular.

226
Chapter 8 Properties of Context-Free Languages
THEOREM 8.5
Let L1 be a context-free language and L2 be a regular language.
Then
L1 ∩L2 is context free.
Proof: Let M1 = (Q, Σ, Γ, δ1, q0, z, F1) be an npda that accepts L1 and
M2 = (P, Σ, δ2, p0, F2) be a dfa that accepts L2. We construct a push-down
automaton 
M =

Q, Σ, Γ, δ, q0, z, F

that simulates the parallel action of
M1 and M2: Whenever a symbol is read from the input string, 
M simulta-
neously executes the moves of M1 and M2. To this end we let
Q = Q × P,
q0 = (q0, p0) ,
F = F1 × F2,
and deﬁne δ such that
((qk, pl) , x) ∈δ ((qi, pj) , a, b)
if and only if
(qk, x) ∈δ1 (qi, a, b)
and
δ2 (pj, a) = pl.
In this, we also require that if a = λ, then pj = pl. In other words, the
states of 
M are labeled with pairs (qi, pj), representing the respective states
in which M1 and M2 can be after reading a certain input string. It is a
straightforward induction argument to show that
((q0, p0) , w, z)
∗
⊢
M((qr, ps) , λ, x) ,
with qr ∈F1 and ps ∈F2 if and only if
(q0, w, z)
∗
⊢M1(qr, λ, x) ,
and
δ∗(p0, w) = ps.
Therefore, a string is accepted by 
M if and only if it is accepted by M1 and
M2, that is, if it is in L (M1) ∩L (M2) = L1 ∩L2.
The property addressed by this theorem is called closure under regular
intersection. Because of the result of the theorem, we say that the family
of context-free languages is closed under regular intersection. This closure
property is sometimes useful for simplifying arguments in connection with
speciﬁc languages.

8.2 Closure Properties and Decision Algorithms
227
EXAMPLE 8.7
Show that the language
L = {anbn : n ≥0, n ̸= 100}
is context free.
It is possible to prove this claim by constructing a pda or a context-
free grammar for the language, but the process is tedious. We can get
a much neater argument with Theorem 8.5.
Let
L1 =

a100b100
.
Then, because L1 is ﬁnite, it is regular. Also, it is easy to see that
L = {anbn : n ≥0} ∩L1.
Therefore, by the closure of regular languages under complementation
and the closure of context-free languages under regular intersection,
the desired result follows.
EXAMPLE 8.8
Show that the language
L =

w ∈{a, b, c}∗: na (w) = nb (w) = nc (w)

is not context free.
The pumping lemma can be used for this, but again we can get a
much shorter argument using closure under regular intersection. Sup-
pose that L were context free. Then
L ∩L (a∗b∗c∗) = {anbncn : n ≥0}
would also be context free. But we already know that this is not so.
We conclude that L is not context free.
Closure properties of languages play an important role in the theory
of formal languages and many more closure properties for context-free lan-
guages can be established.
Some additional results are explored in the
exercises at the end of this section.
Some Decidable Properties of Context-Free Languages
By putting together Theorems 5.2 and 6.6, we have already established the
existence of a membership algorithm for context-free languages. This is of

228
Chapter 8 Properties of Context-Free Languages
course an essential feature of any language family useful in practice. Other
simple properties of context-free languages can also be determined. For the
purpose of this discussion, we assume that the language is described by its
grammar.
THEOREM 8.6
Given a context-free grammar G = (V, T, S, P), there exists an algorithm
for deciding whether or not L (G) is empty.
Proof: For simplicity, assume that λ /∈L (G). Slight changes have to be
made in the argument if this is not so. We use the algorithm for removing
useless symbols and productions. If S is found to be useless, then L (G) is
empty; if not, then L (G) contains at least one element.
THEOREM 8.7
Given a context-free grammar G = (V, T, S, P), there exists an algorithm
for determining whether or not L (G) is inﬁnite.
Proof: We assume that G contains no λ-productions, no unit-productions,
and no useless symbols. Suppose the grammar has a repeating variable in
the sense that there exists some A ∈V for which there is a derivation
A
∗⇒xAy.
Since G is assumed to have no λ-productions and no unit-productions, x
and y cannot be simultaneously empty. Since A is neither nullable nor a
useless symbol, we have
S
∗⇒uAv
∗⇒w
and
A
∗⇒z,
where u, v, and z are in T ∗. But then
S
∗⇒uAv
∗⇒uxnAynv
∗⇒uxnzynv
is possible for all n, so that L (G) is inﬁnite.
If no variable can ever repeat, then the length of any derivation is
bounded by |V |. In that case, L (G) is ﬁnite.
Thus, to get an algorithm for determining whether L (G) is ﬁnite, we
need only to determine whether the grammar has some repeating variables.
This can be done simply by drawing a dependency graph for the variables
in such a way that there is an edge (A, B) whenever there is a corresponding
production
A →xBy.

8.2 Closure Properties and Decision Algorithms
229
Then any variable that is at the base of a cycle is a repeating one. Conse-
quently, the grammar has a repeating variable if and only if the dependency
graph has a cycle.
Since we now have an algorithm for deciding whether a grammar has
a repeating variable, we have an algorithm for determining whether or not
L (G) is inﬁnite.
Somewhat surprisingly, other simple properties of context-free languages
are not so easily dealt with. As in Theorem 4.7, we might look for an algo-
rithm to determine whether two context-free grammars generate the same
language. But it turns out that there is no such algorithm. For the moment,
we do not have the technical machinery for properly deﬁning the meaning
of "there is no algorithm," but its intuitive meaning is clear. This is an
important point to which we will return later.
EXERCISES
1. Is the complement of the language in Example 8.8 context free?
2. Consider the language L1 in Theorem 8.4. Show that this language is linear.
3. Show that the family of context-free languages is closed under homomor-
phism.
4. Show that the family of linear languages is closed under homomorphism.
5. Show that the family of context-free languages is closed under reversal.
6. Which of the language families we have discussed are not closed under rever-
sal?
7. Show that the family of context-free languages is not closed under diﬀerence
in general but is closed under regular diﬀerence; that is, if L1 is context free
and L2 is regular, then L1 −L2 is context free.
8. Show that the family of deterministic context-free languages is closed under
regular diﬀerence.
9. Show that the family of linear languages is closed under union, but not closed
under concatenation.
10. Show that the family of linear languages is not closed under intersection.
11. Show that the family of deterministic context-free languages is not closed
under union and intersection.
12. Give an example of a context-free language whose complement is not context
free.
13. Show that if L1 is linear and L2 is regular, then L1L2 is a linear language.
14. Show that the family of unambiguous context-free languages is not closed
under union.

230
Chapter 8 Properties of Context-Free Languages
15. Show that the family of unambiguous context-free languages is not closed
under intersection.
16. Let L be a deterministic context-free language and deﬁne a new language
L1 = {w : aw ∈L, a ∈Σ}. Is it necessarily true that L1 is a deterministic
context-free language?
17. Show that the language L
=
{anbn : n ≥0, n is not a multiple of 5} is
context free.
18. Show that the following language is context free:
L = {w ∈{a, b}∗: na (w) = nb (w) ; w does not contain a substring aab} .
19. Is the family of deterministic context-free languages closed under homomor-
phism?
20. Give the details of the inductive argument in Theorem 8.5.
21. Give an algorithm that, for any given context-free grammar G, can determine
whether or not λ ∈L (G).
22. Show that there exists an algorithm to determine whether the language gen-
erated by some context-free grammar contains any words of length less than
some given number n.
23. Show that there exists an algorithm to determine if a context-free language
contains any even-length strings.
24. Let L1 be a context-free language and L2 be regular. Show that there exists
an algorithm to determine whether or not L1 and L2 have a common element.

9
CHA P T E R
TURING
MACHINES
CHAPTER SUMMARY
Here we introduce the important concept of a Turing machine, a finite-
state control unit to which is attached a one-dimensional, unbounded
tape.
Even though a Turing machine is still a very simple structure,
it turns out to be very powerful and lets us solve many problems that
cannot be solved with a pushdown automaton. This leads to Turing's
Thesis, which claims that Turing machines are the most general types
of automata, in principle as powerful as any computer.
I
n our discussion so far we have encountered some fundamental ideas,
in particular the concepts of regular and context-free languages and
their association with ﬁnite automata and pushdown accepters.
Our
study has revealed that the regular languages form a proper subset of the
context-free languages and, therefore, that pushdown automata are more
powerful than ﬁnite automata. We also saw that context-free languages,
while fundamental to the study of programming languages, are limited in
scope. This was made clear in the last chapter, where our results showed
that some simple languages, such as {anbncn} and {ww}, are not context-
free. This prompts us to look beyond context-free languages and investigate
how one might deﬁne new language families that include these examples.
To do so, we return to the general picture of an automaton. If we com-
pare ﬁnite automata with pushdown automata, we see that the nature of
231

232
Chapter 9 Turing Machines
the temporary storage creates the diﬀerence between them. If there is no
storage, we have a ﬁnite automaton; if the storage is a stack, we have the
more powerful pushdown automaton. Extrapolating from this observation,
we can expect to discover even more powerful language families if we give
the automaton more ﬂexible storage. For example, what would happen if, in
the general scheme of Figure 1.3, we used two stacks, three stacks, a queue,
or some other storage device? Does each storage device deﬁne a new kind
of automaton and through it a new language family? This approach raises
a large number of questions, most of which turn out to be uninteresting. It
is more instructive to ask a more ambitious question and consider how far
the concept of an automaton can be pushed. What can we say about the
most powerful of automata and the limits of computation? This leads to
the fundamental concept of a Turing machine and, in turn, to a precise
deﬁnition of the idea of a mechanical or algorithmic computation.
We begin our study with a formal deﬁnition of a Turing machine, then
develop some feeling for what is involved by doing some simple programs.
Next we argue that, while the mechanism of a Turing machine is quite
rudimentary, the concept is broad enough to cover very complex processes.
The discussion culminates in the Turing thesis, which maintains that any
computational process, such as those carried out by present-day computers,
can be done on a Turing machine.
9.1
THE STANDARD TURING MACHINE
Although we can envision a variety of automata with complex and sophis-
ticated storage devices, a Turing machine's storage is actually quite simple.
It can be visualized as a single, one-dimensional array of cells, each of which
can hold a single symbol. This array extends indeﬁnitely in both directions
and is therefore capable of holding an unlimited amount of information.
The information can be read and changed in any order. We will call such
a storage device a tape because it is analogous to the magnetic tapes used
in older computers.
Definition of a Turing Machine
A Turing machine is an automaton whose temporary storage is a tape. This
tape is divided into cells, each of which is capable of holding one symbol.
Associated with the tape is a read-write head that can travel right or left
on the tape and that can read and write a single symbol on each move. To
deviate slightly from the general scheme of Chapter 1, the automaton that
we use as a Turing machine will have neither an input ﬁle nor any special
output mechanism. Whatever input and output is necessary will be done on
the machine's tape. We will see later that this modiﬁcation of our general
model in Section 1.2 is of little consequence. We could retain the input ﬁle

9.1 The Standard Turing Machine
233
Read-write head
Control unit
Tape
FIGURE 9.1
and a speciﬁc output mechanism without aﬀecting any of the conclusions we
are about to draw, but we leave them out because the resulting automaton
is a little easier to describe.
A diagram giving an intuitive visualization of a Turing machine is shown
in Figure 9.1. Deﬁnition 9.1 makes the notion precise.
DEFINITION 9.1
A Turing machine M is deﬁned by
M = (Q, Σ, Γ, δ, q0, □, F) ,
where
Q
is the set of internal states,
Σ
is the input alphabet,
Γ
is a ﬁnite set of symbols called the tape alphabet,
δ
is the transition function,
□∈Γ
is a special symbol called the blank,
q0 ∈Q is the initial state,
F ⊆Q is the set of ﬁnal states.
In the deﬁnition of a Turing machine, we assume that Σ ⊆Γ−{□}, that
is, that the input alphabet is a subset of the tape alphabet, not including the
blank. Blanks are ruled out as input for reasons that will become apparent
shortly. The transition function δ is deﬁned as
δ : Q × Γ →Q × Γ × {L, R} .

234
Chapter 9 Turing Machines
In general, δ is a partial function on Q × Γ; its interpretation gives the
principle by which a Turing machine operates.
The arguments of δ are
the current state of the control unit and the current tape symbol being
read. The result is a new state of the control unit, a new tape symbol,
which replaces the old one, and a move symbol, L or R. The move symbol
indicates whether the read-write head moves left or right one cell after the
new symbol has been written on the tape.
EXAMPLE 9.1
Figure 9.2 shows the situation before and after the move
δ (q0, a) = (q1, d, R) .
Internal state q0
a
b
(a)
c
Internal state q1
d
b
(b)
c
FIGURE 9.2
The situation (a) before the move and (b) after the move.
We can think of a Turing machine as a rather simple computer.
It
has a processing unit, which has a ﬁnite memory, and in its tape, it has
a secondary storage of unlimited capacity.
The instructions that such a
computer can carry out are very limited: It can sense a symbol on its tape
and use the result to decide what to do next. The only actions the machine
can perform are to rewrite the current symbol, to change the state of the
control, and to move the read-write head. This small instruction set may
seem inadequate for doing complicated things, but this is not so. Turing
machines are quite powerful in principle. The transition function δ deﬁnes
how this computer acts, and we often call it the "program" of the machine.
As always, the automaton starts in the given initial state with some
information on the tape. It then goes through a sequence of steps controlled
by the transition function δ. During this process, the contents of any cell on
the tape may be examined and changed many times. Eventually, the whole
process may terminate, which we achieve in a Turing machine by putting
it into a halt state. A Turing machine is said to halt whenever it reaches
a conﬁguration for which δ is not deﬁned; this is possible because δ is a
partial function. In fact, we will assume that no transitions are deﬁned for
any ﬁnal state, so the Turing machine will halt whenever it enters a ﬁnal
state.

9.1 The Standard Turing Machine
235
EXAMPLE 9.2
Consider the Turing machine deﬁned by
Q = {qo, q1} ,
Σ = {a, b} ,
Γ = {a, b, □} ,
F = {q1} ,
and
δ (q0, a) = (q0, b, R) ,
δ (q0, b) = (q0, b, R) ,
δ (q0, □) = (q1, □, L) .
If this Turing machine is started in state q0 with the symbol a under the
read-write head, the applicable transition rule is δ (q0, a) = (q0, b, R).
Therefore, the read-write head will replace the a with a b, then move
right on the tape. The machine will remain in state q0. Any subsequent
a will also be replaced with a b, but b's will not be modiﬁed. When
the machine encounters the ﬁrst blank, it will move left one cell, then
halt in ﬁnal state q1.
Figure 9.3 shows several stages of the process for a simple initial
conﬁguration.
a
a
q0
b
a
q0
b
b
q0
b
b
q1
FIGURE 9.3
A sequence of moves.
As before, we can use transition graphs to represent Turing machines.
Now we label the edges of the graph with three items: the current tape
symbol, the symbol that replaces it, and the direction in which the read-
write head is to move. The Turing machine in Example 9.2 is represented
by the transition graph in Figure 9.4.
q1
q0
a, b, R;
b, b, R
,  , L
FIGURE 9.4

236
Chapter 9 Turing Machines
EXAMPLE 9.3
Look at the Turing machine in Figure 9.5. To see what will happen,
we can trace a typical case. Suppose that the tape initially contains
ab..., with the read-write head on the a. The machine then reads the
a, but does not change it. Its next state is q1 and the read-write head
moves right, so that it is now over the b. This symbol is also read
and left unchanged.
The machine goes back into state q0 and the
read-write head moves left. We are now back exactly in the original
state, and the sequence of moves starts again. It is clear from this
that the machine, whatever the initial information on its tape, will
run forever, with the read-write head moving alternately right then
left, but making no modiﬁcations to the tape. This is an instance of
a Turing machine that does not halt. In analogy with programming
terminology, we say that the Turing machine is in an inﬁnite loop.
a, a, R;
b, b, R;
,  , R
q1
q0
a, a, L;
b, b, L;
,  , L
FIGURE 9.5
Since one can make several diﬀerent deﬁnitions of a Turing machine, it
is worthwhile to summarize the main features of our model, which we will
call a standard Turing machine:
1. The Turing machine has a tape that is unbounded in both directions,
allowing any number of left and right moves.
2. The Turing machine is deterministic in the sense that δ deﬁnes at most
one move for each conﬁguration.
3. There is no special input ﬁle. We assume that at the initial time the
tape has some speciﬁed content. Some of this may be considered input.
Similarly, there is no special output device.
Whenever the machine
halts, some or all of the contents of the tape may be viewed as output.
These conventions were chosen primarily for the convenience of subse-
quent discussion. In Chapter 10, we will look at other versions of Turing
machines and discuss their relation to our standard model.
Here, as in the case of pda's, the most convenient way to exhibit a
sequence of conﬁgurations of a Turing machine uses the idea of an instan-
taneous description.
Any conﬁguration is completely determined by the

9.1 The Standard Turing Machine
237
.
.
.
.
.
.
a1
a2
ak-1
ak
x2
x1
an
ak+1
Internal state q
FIGURE 9.6
current state of the control unit, the contents of the tape, and the position
of the read-write head. We will use the notation in which
x1qx2
or
a1a2 · · · ak−1qakak+1 · · · an
is the instantaneous description of a machine in state q with the tape de-
picted in Figure 9.6. The symbols a1, ..., an show the tape contents, while
q deﬁnes the state of the control unit. This convention is chosen so that
the position of the read-write head is over the cell containing the symbol
immediately following q.
The instantaneous description gives only a ﬁnite amount of information
to the right and left of the read-write head. The unspeciﬁed part of the tape
is assumed to contain all blanks; normally such blanks are irrelevant and
are not shown explicitly in the instantaneous description. If the position
of blanks is relevant to the discussion, however, the blank symbol may
appear in the instantaneous description. For example, the instantaneous
description q□w indicates that the read-write head is on the cell to the
immediate left of the ﬁrst symbol of w and that this cell contains a blank.
EXAMPLE 9.4
The pictures drawn in Figure 9.3 correspond to the sequence of
instantaneous descriptions q0aa, bq0a, bbq0□, bq1b.
A move from one conﬁguration to another will be denoted by ⊢. Thus,
if
δ (q1, c) = (q2, e, R) ,
then the move
abq1cd ⊢abeq2d

238
Chapter 9 Turing Machines
is made whenever the internal state is q1, the tape contains abcd, and the
read-write head is on the c. The symbol
∗
⊢has the usual meaning of an
arbitrary number of moves. Subscripts, such as ⊢M, are used in arguments
to distinguish between several machines.
EXAMPLE 9.5
The action of the Turing machine in Figure 9.3 can be represented by
q0aa ⊢bq0a ⊢bbq0□⊢bq1b
or
q0aa
∗
⊢bq1b.
For further discussion, it is convenient to summarize the various obser-
vations just made in a formal way.
DEFINITION 9.2
Let M = (Q, Σ, Γ, δ, q0, □, F) be a Turing machine.
Then any string
a1 · · · ak−1q1akak+1 · · · an, with ai ∈Γ and q1 ∈Q, is an instantaneous
description of M. A move
a1 · · · ak−1q1akak+1 · · · an ⊢a1 · · · ak−1bq2ak+1 · · · an
is possible if and only if
δ (q1, ak) = (q2, b, R) .
A move
a1 · · · ak−1q1akak+1 · · · an ⊢a1 · · · q2ak−1bak+1 · · · an
is possible if and only if
δ (q1, ak) = (q2, b, L) .
M is said to halt starting from some initial conﬁguration x1qix2 if
x1qix2
∗
⊢y1qjay2
for any qj and a, for which δ (qj, a) is undeﬁned. The sequence of conﬁgu-
rations leading to a halt state will be called a computation.

9.1 The Standard Turing Machine
239
Example 9.3 shows the possibility that a Turing machine will never halt,
proceeding in an endless loop from which it cannot escape. This situation
plays a fundamental role in the discussion of Turing machines, so we use a
special notation for it. We will represent it by
x1qx2
∗
⊢∞,
indicating that, starting from the initial conﬁguration x1qx2, the machine
goes into a loop and never halts.
Turing Machines as Language Accepters
Turing machines can be viewed as accepters in the following sense. A string
w is written on the tape, with blanks ﬁlling out the unused portions. The
machine is started in the initial state q0 with the read-write head positioned
on the leftmost symbol of w.
If, after a sequence of moves, the Turing
machine enters a ﬁnal state and halts, then w is considered to be accepted.
DEFINITION 9.3
Let M = (Q, Σ, Γ, δ, q0, □, F) be a Turing machine.
Then the language
accepted by M is
L (M) =

w ∈Σ+ : q0w
∗
⊢x1qfx2 for some qf ∈F, x1, x2 ∈Γ∗

.
This deﬁnition indicates that the input w is written on the tape with
blanks on either side. The reason for excluding blanks from the input now
becomes clear: It assures us that all the input is restricted to a well-deﬁned
region of the tape, bracketed by blanks on the right and left. Without this
convention, the machine could not limit the region in which it must look for
the input; no matter how many blanks it saw, it could never be sure that
there was not some nonblank input somewhere else on the tape.
Deﬁnition 9.3 tells us what must happen when w ∈L (M).
It says
nothing about the outcome for any other input. When w is not in L (M),
one of two things can happen: The machine can halt in a nonﬁnal state or
it can enter an inﬁnite loop and never halt. Any string for which M does
not halt is by deﬁnition not in L (M).
EXAMPLE 9.6
For Σ = {0, 1}, design a Turing machine that accepts the language
denoted by the regular expression 00∗.

240
Chapter 9 Turing Machines
This is an easy exercise in Turing machine programming. Starting
at the left end of the input, we read each symbol and check that it is
a 0. If it is, we continue by moving right. If we reach a blank without
encountering anything but 0, we terminate and accept the string. If the
input contains a 1 anywhere, the string is not in L (00∗), and we halt in
a nonﬁnal state. To keep track of the computation, two internal states
Q = {q0, q1} and one ﬁnal state F = {q1} are suﬃcient. As transition
function we can take
δ (q0, 0) = (q0, 0, R) ,
δ (q0, □) = (q1, □, R) .
As long as a 0 appears under the read-write head, the head will move
to the right. If at any time a 1 is read, the machine will halt in the
nonﬁnal state q0, since δ (q0, 1) is undeﬁned.
Note that the Turing
machine also halts in a ﬁnal state if started in state q0 on a blank. We
could interpret this as acceptance of λ, but for technical reasons the
empty string is not included in Deﬁnition 9.3.
The recognition of more complicated languages is more diﬃcult. Since
Turing machines have a primitive instruction set, the computations that we
can program easily in a higher-level language are often cumbersome on a
Turing machine. Still, it is possible, and the concept is easy to understand,
as the next examples illustrate.
EXAMPLE 9.7
For Σ = {a, b}, design a Turing machine that accepts
L = {anbn : n ≥1} .
Intuitively, we solve the problem in the following fashion. Starting at
the leftmost a, we check it oﬀby replacing it with some symbol, say
x. We then let the read-write head travel right to ﬁnd the leftmost b,
which in turn is checked oﬀby replacing it with another symbol, say
y. After that, we go left again to the leftmost a, replace it with an x,
then move to the leftmost b and replace it with y, and so on. Traveling
back and forth this way, we match each a with a corresponding b. If
after some time no a's or b's remain, then the string must be in L.
Working out the details, we arrive at a complete solution for which
Q = {q0, q1, q2, q3, q4} , F = {q4} , Σ = {a, b} , Γ = {a, b, x, y, □} . The

9.1 The Standard Turing Machine
241
transitions can be broken into several parts. The set
δ (q0, a) = (q1, x, R) ,
δ (q1, a) = (q1, a, R) ,
δ (q1, y) = (q1, y, R) ,
δ (q1, b) = (q2, y, L)
replaces the leftmost a with an x, then causes the read-write head to
travel right to the ﬁrst b, replacing it with a y. When the y is written,
the machine enters a state q2, indicating that an a has been successfully
paired with a b.
The next set of transitions reverses the direction until an x is en-
countered, repositions the read-write head over the leftmost a, and
returns control to the initial state.
δ (q2, y) = (q2, y, L) ,
δ (q2, a) = (q2, a, L) ,
δ (q2, x) = (q0, x, R) .
We are now back in the initial state q0, ready to deal with the next a
and b.
After one pass through this part of the computation, the machine
will have carried out the partial computation
q0aa · · · abb · · · b
∗
⊢xq0a · · · ayb · · · b,
so that a single a has been matched with a single b. After two passes,
we will have completed the partial computation
q0aa · · · abb · · · b
∗
⊢xxq0 · · · ayy · · · b,
and so on, indicating that the matching process is being carried out
properly.
When the input is a string anbn, the rewriting continues this way,
stopping only when there are no more a's to be erased. When looking
for the leftmost a, the read-write head travels left with the machine in
state q2. When an x is encountered, the direction is reversed to get
the a. But now, instead of ﬁnding an a it will ﬁnd a y. To terminate,
a ﬁnal check is made to see if all a's and b's have been replaced (to
detect input where an a follows a b). This can be done by
δ (q0, y) = (q3, y, R) ,
δ (q3, y) = (q3, y, R) ,
δ (q3, □) = (q4, □, R) .

242
Chapter 9 Turing Machines
If we input a string not in the language, the computation will halt
in a nonﬁnal state. For example, if we give the machine a string anbm,
with n > m, the machine will eventually encounter a blank in state
q1. It will halt because no transition is speciﬁed for this case. Other
input not in the language will also lead to a nonﬁnal halting state (see
Exercise 4 at the end of this section).
The particular input aabb gives the following successive instanta-
neous descriptions:
q0aabb ⊢xq1abb ⊢xaq1bb ⊢xq2ayb
⊢q2xayb ⊢xq0ayb ⊢xxq1yb
⊢xxyq1b ⊢xxq2yy ⊢xq2xyy
⊢xxq0yy ⊢xxyq3y ⊢xxyyq3□
⊢xxyy□q4□.
At this point the Turing machine halts in a ﬁnal state, so the string
aabb is accepted.
You are urged to trace this program with several more strings in
L, as well as with some not in L.
EXAMPLE 9.8
Design a Turing machine that accepts
L = {anbncn : n ≥1} .
The ideas used in Example 9.7 are easily carried over to this case. We
match each a, b, and c by replacing them in order by x, y, and z,
respectively. At the end, we check that all original symbols have been
rewritten. Although conceptually a simple extension of the previous
example, writing the actual program is tedious. We leave it as a some-
what lengthy, but straightforward exercise. Notice that even though
{anbn} is a context-free language and {anbncn} is not, they can be
accepted by Turing machines with very similar structures.
One conclusion we can draw from this example is that a Turing machine
can recognize some languages that are not context free, a ﬁrst indication
that Turing machines are more powerful than pushdown automata.
Turing Machines as Transducers
We have had little reason so far to study transducers; in language theory,
accepters are quite adequate. But as we will shortly see, Turing machines are

9.1 The Standard Turing Machine
243
not only interesting as language accepters, they also provide us with a simple
abstract model for digital computers in general. Since the primary purpose
of a computer is to transform input into output, it acts as a transducer. If
we want to model computers using Turing machines, we have to look at this
aspect more closely.
The input for a computation will be all the nonblank symbols on the
tape at the initial time. At the conclusion of the computation, the output
will be whatever is then on the tape. Thus, we can view a Turing machine
transducer M as an implementation of a function f deﬁned by
w = f (w) ,
provided that
q0w
∗
⊢M qf w,
for some ﬁnal state qf.
DEFINITION 9.4
A function f with domain D is said to be Turing-computable or just
computable if there exists some Turing machine M = (Q, Σ, Γ, δ, q0, □, F)
such that
q0w
∗
⊢M qff (w) ,
qf ∈F,
for all w ∈D.
As we will shortly claim, all the common mathematical functions, no
matter how complicated, are Turing-computable. We start by looking at
some simple operations, such as addition and arithmetic comparison.
EXAMPLE 9.9
Given two positive integers x and y, design a Turing machine that
computes x + y.
We ﬁrst have to choose some convention for representing positive
integers. For simplicity, we will use unary notation in which any posi-
tive integer x is represented by w (x) ∈{1}+, such that
|w (x)| = x.
We must also decide how x and y are placed on the tape initially
and how their sum is to appear at the end of the computation. We will

244
Chapter 9 Turing Machines
assume that w (x) and w (y) are on the tape in unary notation, sepa-
rated by a single 0, with the read-write head on the leftmost symbol of
w (x). After the computation, w (x + y) will be on the tape followed
by a single 0, and the read-write head will be positioned at the left
end of the result. We therefore want to design a Turing machine for
performing the computation
q0w (x) 0w (y)
∗
⊢qfw (x + y) 0,
where qf is a ﬁnal state.
Constructing a program for this is rela-
tively simple. All we need to do is to move the separating 0 to the
right end of w (y), so that the addition amounts to nothing more
than the coalescing of the two strings. To achieve this, we construct
M = (Q, Σ, Γ, δ, q0, □, F), with Q = {q0, q1, q2, q3, q4} , F = {q4} , and
δ (q0, 1) = (q0, 1, R) ,
δ (q0, 0) = (q1, 1, R) ,
δ (q1, 1) = (q1, 1, R) ,
δ (q1, □) = (q2, □, L) ,
δ (q2, 1) = (q3, 0, L) ,
δ (q3, 1) = (q3, 1, L) ,
δ (q3, □) = (q4, □, R) .
Note that in moving the 0 right we temporarily create an extra 1, a
fact that is remembered by putting the machine into state q1. The
transition δ (q2, 1) = (q3, 0, R) is needed to remove this at the end of
the computation. This can be seen from the sequence of instantaneous
descriptions for adding 111 to 11:
q0111011 ⊢1q011011 ⊢11q01011 ⊢111q0011
⊢1111q111 ⊢11111q11 ⊢111111q1□
⊢11111q21 ⊢1111q310
∗
⊢q3□111110 ⊢q4111110.
Unary notation, although cumbersome for practical computations, is
very convenient for programming Turing machines. The resulting pro-
grams are much shorter and simpler than if we had used another rep-
resentation, such as binary or decimal.

9.1 The Standard Turing Machine
245
Adding numbers is one of the fundamental operations of any computer,
one that plays a part in the synthesis of more complicated instructions.
Other basic operations are copying strings and simple comparisons. These
can also be done easily on a Turing machine.
EXAMPLE 9.10
Design a Turing machine that copies strings of 1's. More precisely,
ﬁnd a machine that performs the computation
q0w
∗
⊢qfww,
for any w ∈{1}+ .
To solve the problem, we implement the following intuitive process:
1. Replace every 1 by an x.
2. Find the rightmost x and replace it with 1.
3. Travel to the right end of the current nonblank region and create
a 1 there.
4. Repeat Steps 2 and 3 until there are no more x's.
q3
1, 1, L
1, 1, R
, 1, L
x, 1, R
1, x, R
q1
q2
q0
,  , R
,  , L
FIGURE 9.7

246
Chapter 9 Turing Machines
The solution is shown in the transition graph in Figure 9.7. It may be
a little hard to see at ﬁrst that the solution is correct, so let us trace
the program with the simple string 11. The computation performed in
this case is
q011 ⊢xq01 ⊢xxq0□⊢xq1x
⊢x1q2□⊢xq111 ⊢q1x11
⊢1q211 ⊢11q21 ⊢111q2□
⊢11q111 ⊢1q1111
⊢q11111 ⊢q1□1111 ⊢q31111.
EXAMPLE 9.11
Let x and y be two positive integers represented in unary notation.
Construct a Turing machine that will halt in a ﬁnal state qy if x ≥y,
and that will halt in a nonﬁnal state qn if x < y. More speciﬁcally,
the machine is to perform the computation
q0w (x) 0w (y)
∗
⊢qyw (x) 0w (y)
if x ≥y,
q0w (x) 0w (y)
∗
⊢qnw (x) 0w (y)
if x < y.
To solve this problem, we can use the idea in Example 9.7 with
some minor modiﬁcations. Instead of matching a's and b's, we match
each 1 on the left of the dividing 0 with the 1 on the right. At the end
of the matching, we will have on the tape either
xx · · · 110xx · · · x□
or
xx · · · xx0xx · · · x11□,
depending on whether x > y or y > x. In the ﬁrst case, when we
attempt to match another 1, we encounter the blank at the right of the
working space. This can be used as a signal to enter the state qy. In
the second case, we still ﬁnd a 1 on the right when all 1's on the left
have been replaced. We use this to get into the other state qn. The
complete program for this is straightforward and is left as an exercise.
This example makes the important point that a Turing machine
can be programmed to make decisions based on arithmetic compar-
isons. This kind of simple decision is common in the machine language

9.1 The Standard Turing Machine
247
of computers, where alternate instruction streams are entered, depend-
ing on the outcome of an arithmetic operation.
EXERCISES
1. Construct a Turing machine that accepts the language L = L(aaaa∗b∗).
2. Construct a Turing machine that accepts the complement of the language
L = L(aaaa∗b∗). Assume that Σ = {a, b}.
3. Design a Turing machine with no more than three states that accepts the
language L (a (a + b)∗). Assume that Σ = {a, b}. Is it possible to do this
with a two-state machine?
4. Determine what the Turing machine in Example 9.7 does when presented
with the inputs aba and aaabbbb.
5. Is there any input for which the Turing machine in Example 9.7 goes into an
inﬁnite loop?
6. What language is accepted by the Turing machine whose transition graph is
in the ﬁgure below?
a ; a, R
a ; a, R
b ; b, R
b ; b , R
b ; b, R
b : b, R
q0
q1
q3
q2
7. What happens in Example 9.10 if the string w contains any symbol other
than 1?
8. Construct Turing machines that will accept the following languages on {a, b}:
(a) L = L (aaba∗b).
(b) L = {w : |w| is odd}.
(c) L = {w : |w| is a multiple of 4}.

248
Chapter 9 Turing Machines
(d) L = {anbm : n ≥2, n = m}.
(e) L = {w : na (w) ̸= nb (w)}.
(f) L =

anbman+m : n ≥0, m ≥1

.
(g) L = {anbnanbn : n ̸= 0}.
(h) L =

anb2n : n ≥1

.
For each problem, give a transition graph; then check your answers by tracing
several test examples.
9. Design a Turing machine that accepts the language
L =

ww : w ∈{a, b}+
.
10. Construct a Turing machine to compute the function
f (w) = wR,
where w ∈{0, 1}+.
11. Design a Turing machine that computes the function
f(w) = 1 if w is even
= 0 if w is odd.
12. Design a Turing machine that computes the function
f (x) = x −2 if x > 2
= 0 if x ≤2.
13. Design Turing machines to compute the following functions for x and y pos-
itive integers represented in unary:
(a) f (x) = 2x + 1.
(b) f (x, y) = x + 2y.
(c) f (x, y) = 2x + 3y.
(d) f (x) = x mod 5.
(e) f (x) = ⌊x
2 ⌋, where ⌊x
2 ⌋denotes the largest integer less than or
equal to x
2 .
14. Design a Turing machine with Γ = {0, 1, ⊔}, which, when started on any cell
containing a blank or a 1, will halt if and only if its tape has a 0 somewhere
on it.
15. Write out a complete solution for Example 9.8.
16. Show the sequence of instantaneous descriptions that the Turing machine
in Example 9.10 goes through when presented with the input 111.
What
happens when this machine is started with 110 on its tape?

9.2 Combining Turing Machines for Complicated Tasks
249
17. Give convincing arguments that the Turing machine in Example 9.10 does in
fact carry out the indicated computation.
18. Complete the details in Example 9.11.
19. Suppose that in Example 9.9 we had decided to represent x and y in binary.
Write a Turing machine program for doing the indicated computation in this
representation.
20. You may have noticed that all the examples in this section had only one ﬁnal
state. Is it generally true that for any Turing machine, there exists another
one with only one ﬁnal state that accepts the same language?
9.2
COMBINING TURING MACHINES FOR
COMPLICATED TASKS
We have shown explicitly how some important operations found in all com-
puters can be done on a Turing machine. Since, in digital computers, such
primitive operations are the building blocks for more complex instructions,
let us see how these basic operations can also be put together on a Turing
machine. To demonstrate how Turing machines can be combined, we follow
a practice common in programming. We start with a high-level description,
then reﬁne it successively until the program is in the actual language with
which we are working. We can describe Turing machines several ways at a
high level; block diagrams or pseudocode are the two approaches we will use
most frequently in subsequent discussions. In a block diagram, we encap-
sule computations in boxes whose function is described, but whose interior
details are not shown. By using such boxes, we implicitly claim that they
can actually be constructed. As a ﬁrst example, we combine the machines
in Examples 9.9 and 9.11.
EXAMPLE 9.12
Design a Turing machine that computes the function
f (x, y) = x + y
if x ≥y,
= 0
if x < y.
For the sake of discussion, assume that x and y are positive integers
in unary representation. The value zero will be represented by 0, with
the rest of the tape blank.
The computation of f (x, y) can be visualized at a high level by
means of the diagram in Figure 9.8. The diagram shows that we ﬁrst
use a comparing machine, like that in Example 9.11, to determine

250
Chapter 9 Turing Machines
whether or not x ≥y. If so, the comparer sends a start signal to the
adder, which then computes x+y. If not, an erasing program is started
that changes every 1 to a blank.
In subsequent discussions, we will often use such high-level, black-
diagram representations of Turing machines. It is certainly quicker and
clearer than the corresponding extensive set of δ's. Before we accept
this high-level view, we must justify it. What, for example, is meant
by saying that the comparer sends a start signal to the adder? There
is nothing in Deﬁnition 9.1 that oﬀers that possibility. Nevertheless, it
can be done in a straightforward way.
The program for the comparer C is written as suggested in Exam-
ple 9.11, using a Turing machine having states indexed with C. For
the adder, we use the idea in Example 9.9, with states indexed with A.
For the eraser E, we construct a Turing machine having states indexed
with E. The computations to be done by C are
qC,0w (x) 0w (y)
∗
⊢qA,0w (x) 0w (y)
if x ≥y,
and
qC,0w (x) 0w (y)
∗
⊢qE,0w (x) 0w (y)
if x < y.
If we take qA,0 and qE,0 as the initial states of A and E, respectively,
we see that C starts either A or E.
The computations performed by the adder will be
qA,0w (x) 0w (y)
∗
⊢qA,fw (x + y) 0,
and that of the eraser E will be
qE,0w (x) 0w (y)
∗
⊢qE,f0.
The result is a single Turing machine that combines the action of C,
A, and E as indicated in Figure 9.8.
f (x, y)
Comparer
C
Adder
A
Eraser
E
x + y
0
x, y
x < y
x  y
-
FIGURE 9.8

9.2 Combining Turing Machines for Complicated Tasks
251
Another useful, high-level view of Turing machines involves pseudocode.
In computer programming, pseudocode is a way of outlining a computation
using descriptive phrases whose meaning we claim to understand. While this
description is not usable on the computer, we assume that we can translate it
into the appropriate language when needed. One simple kind of pseudocode
is exempliﬁed by the idea of a macroinstruction, which is a single-statement
shorthand for a sequence of lower-level statements.
We ﬁrst deﬁne the
macroinstruction in terms of the lower-level language.
We then use the
macroinstruction in a program with the assumption that the relevant low-
level code is substituted for each occurrence of the macroinstruction. This
idea is very useful in Turing machine programming.
EXAMPLE 9.13
Consider the macroinstruction
if a then qj else qk,
with the following interpretation. If the Turing machine reads an a,
then regardless of its current state, it is to go into state qj without
changing the tape content or moving the read-write head. If the symbol
read is not an a, the machine is to go into state qk without changing
anything.
To implement this macroinstruction requires several relatively ob-
vious steps of a Turing machine.
δ (qi, a) = (qj0, a, R) for all qi ∈Q,
δ(qi, b) = (qk0, b, R) for all qi ∈Q and all b ∈Γ −{a} ,
δ (qj0, c) = (qj, c, L)
for all c ∈Γ,
δ (qk0, c) = (qk, c, L)
for all c ∈Γ.
The states qj0 and qk0 are new states, introduced to take care of com-
plications arising from the fact that in a standard Turing machine the
read-write head changes position in each move. In the macroinstruc-
tion, we want to change the state, but leave the read-write head where
it is. We let the head move right, but put the machine into a state qj0
or qk0. This indicates that a left move must be made before entering
the desired state qj or qk.
Going a step further, we can replace macroinstructions with subpro-
grams. Normally, a macroinstruction is replaced by actual code at each

252
Chapter 9 Turing Machines
Workspace for A
Workspace for B
T
#
#
Region separator
FIGURE 9.9
occurrence, whereas a subprogram is a single piece of code that is invoked
repeatedly whenever needed. Subprograms are fundamental to high-level
programming languages, but they can also be used with Turing machines.
To make this plausible, let us outline brieﬂy how a Turing machine can be
used as a subprogram that can be invoked repeatedly by another Turing
machine. This requires a new feature: the ability to store information on
the calling program's conﬁguration so the conﬁguration can be recreated
on return from the subprogram. For example, say machine A in state qi
invokes machine B. When B is ﬁnished, we would like to resume program
A in state qi, with the read-write head (which may have moved during B's
operation) in its original place. At other times, A may call B from state
qj, in which case control should return to this state. To solve the control
transfer problem, we must be able to pass information from A to B and vice
versa, be able to recreate A's conﬁguration when it recovers control from
B, and assure that the temporarily suspended computations of A are not
aﬀected by the execution of B. To solve this, we can divide the tape into
several regions as shown in Figure 9.9.
Before A calls B, it writes the information needed by B (e.g., A's current
state, the arguments for B) on the tape in some region T. A then passes
control to B by making a transition to the start state of B. After transfer,
B will use T to ﬁnd its input. The workspace for B is separate from T and
from the workspace for A, so no interference can occur. When B is ﬁnished,
it will return relevant results to region T, where A will expect to ﬁnd it.
In this way, the two programs can interact in the required fashion. Note
that this is very similar to what actually happens in a real computer when
a subprogram is called.
We can now program Turing machines in pseudocode, provided that we
know (in theory at least) how to translate this pseudocode into an actual
Turing machine program.
EXAMPLE 9.14
Design a Turing machine that multiplies two positive integers in unary
notation.

9.2 Combining Turing Machines for Complicated Tasks
253
A multiplication machine can be constructed by combining the
ideas we encountered in adding and copying. Let us assume that the
initial and ﬁnal tape contents are to be as indicated in Figure 9.10.
The process of multiplication can then be visualized as a repeated
copying of the multiplicand y for each 1 in the multiplier x, whereby
the string y is added the appropriate number of times to the partially
computed product. The following pseudocode shows the main steps of
the process.
1. Repeat the following steps until x contains no more 1's.
Find a 1 in x and replace it with another symbol a.
Replace the leftmost 0 by 0y.
2. Replace all a's with 1's.
Although this pseudocode is sketchy, the idea is simple enough that
there should be no doubt that it can be done.
y
x
xy
y
x
0
0
1
1
1
0
1
1
1
0
1
.
.
.
1
0
1
1
1
0
1
1
1
FIGURE 9.10
In spite of the descriptive nature of these examples, it is not too far-
fetched to conjecture that Turing machines, while rather primitive in prin-
ciple, can be combined in many ways to make them quite powerful. Our
examples were not general and detailed enough for us to claim that we
have proved anything, but it should be plausible at this point that Turing
machines can do some quite complicated things.
EXERCISES
1. Write out the complete solution to Example 9.14.
2. Establish a convention for representing positive and negative integers in unary
notation. With your convention, sketch the construction of a subtracter for
computing x −y.
3. Using adders, subtracters, comparers, copiers, or multipliers, draw block di-
agrams for Turing machines that compute the functions:
(a) f (n) = n (n + 2).
(b) f (n) = n4.

254
Chapter 9 Turing Machines
(c) f (n) = 2n.
(d) f (n) = n!.
(e) f (n) = 2n!.
for all positive integers n.
4. Use a block diagram to sketch the implementation of a function f deﬁned for
all w1, w2, w3 ∈{1}+ by
f (w1, w2, w3) = i,
where i is such that |wi| = max(|w1| , |w2| , |w3|) if no two w's have the same
length, and i = 0 otherwise.
5. Provide a "high-level" description for Turing machines that accept the fol-
lowing languages on {a, b}.
For each problem, deﬁne a set of appropriate
macroinstructions that you feel are reasonably easy to implement. Then use
them for the solution.
(a) L =

wwRw

.
(b) L = {w1w2 : w1 ̸= w2 : |w1| = |w2|}.
(c) The complement of the language in part (b).
(d) L =

anbm : n = m2, m ≥1

.
(e) L = {an : n is a prime number}.
6. Suggest a method for representing rational numbers on a Turing machine,
then sketch a method for adding and subtracting such numbers.
7. Sketch the construction of a Turing machine that can perform the addition
and multiplication of positive integers x and y given in the usual decimal
notation.
8. Give an implementation of the macroinstruction
searchleft (a, qi, qj) ,
which indicates that the machine is to search its tape to the left of the current
position for the ﬁrst occurrence of the symbol a. If an a is encountered before
a blank, the machine is to go into state qi, otherwise it is to go into state qj.
9. Use the macroinstruction searchleft to design a Turing machine on Σ = {a, b}
that accepts the language L (ab∗ab∗a).

9.3 Turing's Thesis
255
9.3
TURING'S THESIS
The preceding discussion not only shows how a Turing machine can be con-
structed from simpler parts, but also illustrates a negative aspect of working
with such low-level automata. While it takes very little imagination or in-
genuity to translate a block diagram or pseudocode into the corresponding
Turing machine program, actually doing it is time-consuming, error-prone,
and adds little to our understanding. The instruction set of a Turing ma-
chine is so restricted that any argument, solution, or proof for a nontrivial
problem is quite tedious.
We now face a dilemma: We want to claim that Turing machines can
perform not only the simple operations for which we have provided ex-
plicit programs, but also more complex processes as well, describable by
block diagrams or pseudocode. To defend such claims against challenge, we
should show the relevant programs explicitly. But doing so is unpleasant
and distracting and ought to be avoided if possible. Somehow, we would
like to ﬁnd a way of carrying out a reasonably rigorous discussion of Turing
machines without having to write lengthy, low-level code. There is unfor-
tunately no completely satisfactory way of getting out of the predicament;
the best we can do is to reach a reasonable compromise. To see how we
might achieve such a compromise, we turn to a somewhat philosophical
issue.
We can draw some simple conclusions from the examples in the previous
section. The ﬁrst is that Turing machines appear to be more powerful than
pushdown automata (for a comment on this, see the exercise at the end
of this section). In Example 9.8, we sketched the construction of a Turing
machine for a language that is not context free and for which, consequently,
no pushdown automaton exists. Examples 9.9, 9.10, and 9.11 show that
Turing machines can do some simple arithmetic operations, perform string
manipulations, and make some simple comparisons. The discussion also il-
lustrates how primitive operations can be combined to solve more complex
problems, how several Turing machines can be composed, and how one pro-
gram can act as a subprogram for another. Since very complex operations
can be built this way, we might suspect that a Turing machine begins to
approach a typical computer in power.
Suppose we were to make the conjecture that, in some sense, Turing
machines are equal in power to a typical digital computer? How could we
defend or refute such a hypothesis? To defend it, we could take a sequence of
increasingly more diﬃcult problems and show how they are solved by some
Turing machine. We might also take the machine language instruction set
of a speciﬁc computer and design a Turing machine that can perform all
the instructions in the set. This would undoubtedly tax our patience, but
it ought to be possible in principle if our hypothesis is correct. Still, while
every success in this direction would strengthen our conviction of the truth
of the hypothesis, it would not lead to a proof. The diﬃculty lies in the fact

256
Chapter 9 Turing Machines
that we don't know exactly what is meant by "a typical digital computer"
and that we have no means for making a precise deﬁnition.
We can also approach the problem from the other side. We might try
to ﬁnd some procedure for which we can write a computer program, but for
which we can show that no Turing machine can exist. If this were possible,
we would have a basis for rejecting the hypothesis. But no one has yet been
able to produce a counterexample; the fact that all such tries have been
unsuccessful must be taken as circumstantial evidence that it cannot be
done. Every indication is that Turing machines are in principle as powerful
as any computer.
Arguments of this type led A. M. Turing and others in the mid-1930s to
the celebrated conjecture called the Turing thesis. This hypothesis states
that any computation that can be carried out by mechanical means can be
performed by some Turing machine.
This is a sweeping statement, so it is important to keep in mind what
Turing's thesis is. It is not something that can be proved. To do so, we would
have to deﬁne precisely the term "mechanical means." This would require
some other abstract model and leave us no further ahead than before. The
Turing thesis is more properly viewed as a deﬁnition of what constitutes a
mechanical computation: A computation is mechanical if and only if it can
be performed by some Turing machine.
If we take this attitude and regard the Turing thesis simply as a deﬁni-
tion, we raise the question as to whether this deﬁnition is suﬃciently broad.
Is it far-reaching enough to cover everything we now do (and conceivably
might do in the future) with computers? An unequivocal "yes" is not pos-
sible, but the evidence in its favor is very strong.
Some arguments for
accepting the Turing thesis as the deﬁnition of a mechanical computation
are
1. Anything that can be done on any existing digital computer can also
be done by a Turing machine.
2. No one has yet been able to suggest a problem, solvable by what we
intuitively consider an algorithm, for which a Turing machine program
cannot be written.
3. Alternative models have been proposed for mechanical computation,
but none of them is more powerful than the Turing machine model.
These arguments are circumstantial, and Turing's thesis cannot be
proved by them. In spite of its plausibility, Turing's thesis is still an as-
sumption.
But viewing Turing's thesis simply as an arbitrary deﬁnition
misses an important point. In some sense, Turing's thesis plays the same
role in computer science as do the basic laws of physics and chemistry.
Classical physics, for example, is based largely on Newton's laws of motion.
Although we call them laws, they do not have logical necessity; rather, they

9.3 Turing's Thesis
257
are plausible models that explain much of the physical world. We accept
them because the conclusions we draw from them agree with our experience
and our observations. Such laws cannot be proved to be true, although they
can possibly be invalidated. If an experimental result contradicts a conclu-
sion based on the laws, we might begin to question their validity. On the
other hand, repeated failure to invalidate a law strengthens our conﬁdence
in it. This is the situation for Turing's thesis, so we have some reason for
considering it a basic law of computer science. The conclusions we draw
from it agree with what we know about real computers, and so far, all
attempts to invalidate it have failed. There is always the possibility that
someone will come up with another deﬁnition that will account for some
subtle situations not covered by Turing machines but which still fall within
the range of our intuitive notion of mechanical computation. In such an
eventuality, some of our subsequent discussions would have to be modiﬁed
signiﬁcantly. However, the likelihood of this happening seems to be very
small.
Having accepted Turing's thesis, we are in a position to give a precise
deﬁnition of an algorithm.
DEFINITION 9.5
An algorithm for a function f : D →R is a Turing machine M, which
given as input any d ∈D on its tape, eventually halts with the correct
answer f (d) ∈R on its tape. Speciﬁcally, we can require that
q0d
∗
⊢M qff (d) , qf ∈F,
for all d ∈D.
Identifying an algorithm with a Turing machine program allows us to
prove rigorously such claims as "there exists an algorithm . . ." or "there is
no algorithm. . . ." However, to construct explicitly an algorithm for even
relatively simple problems is a very lengthy undertaking. To avoid such
unpleasant prospects, we can appeal to Turing's thesis and claim that any-
thing we can do on any computer can also be done on a Turing machine.
Consequently, we could substitute "C program" for "Turing machine" in
Deﬁnition 9.5. This would ease the burden of exhibiting algorithms consid-
erably. Actually, as we have already done, we will go one step further and
accept verbal descriptions or block diagrams as algorithms on the assump-
tion that we could write a Turing machine program for them if we were
challenged to do so. This greatly simpliﬁes the discussion, but it obviously
leaves us open to criticism. While "C program" is well deﬁned, "clear verbal
description" is not, and we are in danger of claiming the existence of nonex-
istent algorithms. But this danger is more than oﬀset by the facts that we

258
Chapter 9 Turing Machines
can keep the discussion simple and intuitively clear and that we can give
concise descriptions for some rather complex processes. The reader who has
any doubts about the validity of these claims can dispel them by writing a
suitable program in some programming language.
EXERCISES
1. In the above discussion, we stated at one point that Turing machines appear
to be more powerful than pushdown automata. Because the tape of a Turing
machine can always be made to behave like a stack, it would seem that we
could have actually claimed that Turing machines are more powerful. What
important factor was not taken into account in the argument that must be
addressed before such a claim is justiﬁed?

10
CHA P T E R
OTHER MODELS OF
TURING MACHINES
CHAPTER SUMMARY
In this chapter, we essentially challenge Turing's thesis by examining
a number of ways the standard Turing machine can be complicated to
see if these complications increase its power. We look at a variety of dif-
ferent storage devices and even allow nondeterminism. None of these
complications increases the essential power of the standard machine,
which lends credibility to Turing's thesis.
O
ur deﬁnition of a standard Turing machine is not the only possible
one; there are alternative deﬁnitions that could serve equally well.
The conclusions we can draw about the power of a Turing machine
are largely independent of the speciﬁc structure chosen for it. In this chapter
we look at several variations, showing that the standard Turing machine is
equivalent, in a sense we will deﬁne, to other, more complicated models.
If we accept Turing's thesis, we expect that complicating the standard
Turing machine by giving it a more complex storage device will not have
any eﬀect on the power of the automaton. Any computation that can be
performed on such a new arrangement will still fall under the category
of a mechanical computation and, therefore, can be done by a standard
model. It is nevertheless instructive to study more complex models, if for
no other reason than that an explicit demonstration of the expected result
will demonstrate the power of the Turing machine and thereby increase
259

260
Chapter 10 Other Models of Turing Machines
our conﬁdence in Turing's thesis. Many variations on the basic model of
Deﬁnition 9.1 are possible. For example, we can consider Turing machines
with more than one tape or with tapes that extend in several dimensions.
We will consider variants that will be useful in subsequent discussions.
We also look at nondeterministic Turing machines and show that they
are no more powerful than deterministic ones. This is unexpected, since
Turing's thesis covers only mechanical computations and does not address
the clever guessing implicit in nondeterminism. Another issue that is not
immediately resolved by Turing's thesis is that of one machine executing
diﬀerent programs at diﬀerent times. This leads to the idea of a "repro-
grammable" or "universal" Turing machine.
Finally, in preparation for later chapters, we look at linear bounded
automata. These are Turing machines that have an inﬁnite tape, but that
can make use of the tape only in a restricted way.
10.1
MINOR VARIATIONS ON THE TURING MACHINE
THEME
We ﬁrst consider some relatively minor changes in Deﬁnition 9.1 and inves-
tigate whether these changes make any diﬀerence in the general concept.
Whenever we change a deﬁnition, we introduce a new type of automata and
raise the question whether these new automata are in any real sense diﬀerent
from those we have already encountered. What do we mean by an essential
diﬀerence between one class of automata and another? Although there may
be clear diﬀerences in their deﬁnitions, these diﬀerences may not have any
interesting consequences. We have seen an example of this in the case of
deterministic and nondeterministic ﬁnite automata. These have quite dif-
ferent deﬁnitions, but they are equivalent in the sense that they both are
identiﬁed exactly with the family of regular languages. Extrapolating from
this, we can deﬁne equivalence or nonequivalence for classes of automata in
general.
Equivalence of Classes of Automata
Whenever we deﬁne equivalence for two automata or classes of automata,
we must carefully state what is to be understood by this equivalence. For
the rest of this chapter, we follow the precedence established for nfa's and
dfa's and deﬁne equivalence with respect to the ability to accept languages.
DEFINITION 10.1
Two automata are equivalent if they accept the same language. Consider
two classes of automata C1 and C2. If for every automaton M1 in C1 there

10.1 Minor Variations on the Turing Machine Theme
261
is an automaton M2 in C2 such that
L (M1) = L (M2) ,
we say that C2 is at least as powerful as C1. If the converse also holds and
for every M2 in C2 there is an M1 in C1 such that L (M1) = L (M2), we say
that C1 and C2 are equivalent.
There are many ways to establish the equivalence of automata. The
construction of Theorem 2.2 does this for dfa's and nfa's. For demonstrating
the equivalence in connection with Turing's machines, we often use the
important technique of simulation.
Let M be an automaton. We say that another automaton 
M can sim-
ulate a computation of M if 
M can mimic the computation of M in the
following manner. Let d0, d1, ... be the sequence of instantaneous descrip-
tions of the computation of M, that is,
d0 ⊢Md1 ⊢M · · · ⊢Mdn · · · .
Then 
M simulates this computation if it carries out a
d0
∗
⊢
M d1
∗
⊢
M · · ·
∗
⊢
M dn · · · ,
where d0, d1, ... are instantaneous descriptions, such that each of them is
associated with a unique conﬁguration of M. In other words, if we know
the computation carried out by 
M, we can determine from it exactly what
computations M would have done, given the corresponding starting conﬁg-
uration.
Note that the simulation of a single move di ⊢M di+1 of M may involve
several moves of 
M. The intermediate conﬁgurations in di
∗
⊢
M di+1 may not
correspond to any conﬁguration of M, but this does not aﬀect anything if we
can tell which conﬁgurations of 
M are relevant. As long as we can determine
from the computation of 
M what M would have done, the simulation is
proper. If 
M can simulate every computation of M, we say that 
M can
simulate M. It should be clear that if 
M can simulate M, then matters
can be arranged so that M and 
M accept the same language, and the two
automata are equivalent. To demonstrate the equivalence of two classes of
automata, we show that for every machine in one class, there is a machine
in the second class capable of simulating it, and vice versa.
Turing Machines with a Stay-Option
In our deﬁnition of a standard Turing machine, the read-write head must
move either to the right or to the left. Sometimes it is convenient to provide

262
Chapter 10 Other Models of Turing Machines
a third option, to have the read-write head stay in place after rewriting the
cell content. Thus, we can deﬁne a Turing machine with a stay-option by
replacing δ in Deﬁnition 9.1 by
δ : Q × Γ →Q × Γ × {L, R, S}
with the interpretation that S signiﬁes no movement of the read-write head.
This option does not extend the power of the automaton.
THEOREM 10.1
The class of Turing machines with a stay-option is equivalent to the class
of standard Turing machines.
Proof:
Since a Turing machine with a stay-option is clearly an extension
of the standard model, it is obvious that any standard Turing machine can
be simulated by one with a stay-option.
To show the converse, let M = (Q, Σ, Γ, δ, q0, □, F) be a Turing ma-
chine with a stay-option to be simulated by a standard Turing machine

M =

Q, Σ, Γ, δ, q0, □, F

. For each move of M, the simulating machine 
M
does the following. If the move of M does not involve the stay-option, the
simulating machine performs one move, essentially identical to the move to
be simulated. If S is involved in the move of M, then 
M will make two
moves: The ﬁrst rewrites the symbol and moves the read-write head right;
the second moves the read-write head left, leaving the tape contents unal-
tered. The simulating machine can be constructed from M by deﬁning δ as
follows: For each transition
δ (qi, a) = (qj, b, L or R) ,
we put into δ
δ (qi, a) = (qi, b, L or R) .
For each S-transition
δ (qi, a) = (qj, b, S) ,
we put into δ the corresponding transitions
δ (qi, a) = (qjS, b, R) ,
and
δ (qjS, c) = (qj, c, L) ,
for all c ∈Γ.

10.1 Minor Variations on the Turing Machine Theme
263
It is reasonably obvious that every computation of M has a correspond-
ing computation of 
M, so that 
M can simulate M.
Simulation is a standard technique for showing the equivalence of au-
tomata, and the formalism we have described makes it possible, as shown in
the above theorem, to talk about the process precisely and prove theorems
about equivalence. In our subsequent discussion, we use the notion of simu-
lation frequently, but we generally make no attempt to describe everything
in a rigorous and detailed way. Complete simulations with Turing machines
are often cumbersome. To avoid this, we keep our discussion descriptive,
rather than in theorem-proof form. The simulations are given only in broad
outline, but it should not be hard to see how they can be made rigorous. The
reader will ﬁnd it instructive to sketch each simulation in some higher-level
language or in pseudocode.
Before introducing other models, we make one remark on the standard
Turing machine. It is implicit in Deﬁnition 9.1 that each tape symbol can
be a composite of characters rather than just a single one. This can be
made more explicit by drawing an expanded version of Figure 9.1 (Figure
10.1), in which the tape symbols are triplets from some simpler alphabet.
In the picture, we have divided each cell of the tape into three parts,
called tracks, each containing one member of the triplet. Based on this
visualization, such an automaton is sometimes called a Turing machine with
multiple tracks, but such a view in no way extends Deﬁnition 9.1, since
all we need to do is make Γ an alphabet in which each symbol is composed
of several parts.
However, other Turing machine models involve a change of deﬁnition, so
the equivalence with the standard machine has to be demonstrated. Here
we look at two such models, which are sometimes used as the standard
deﬁnition. Some variants that are less common are explored in the exercises
at the end of this section.
Turing Machines with Semi-Infinite Tape
Many authors do not consider the model in Figure 9.1 as standard, but use
one with a tape that is unbounded only in one direction. We can visualize
this as a tape that has a left boundary (Figure 10.2). This Turing machine
a
b
c
Track 1
Track 2
Track 3
FIGURE 10.1

264
Chapter 10 Other Models of Turing Machines
FIGURE 10.2
Track 1 for right part of
standard tape
Track 2 for left part of
standard tape
FIGURE 10.3
is otherwise identical to our standard model, except that no left move is
permitted when the read-write head is at the boundary.
It is not diﬃcult to see that this restriction does not aﬀect the power
of the machine. To simulate a standard Turing machine M by a machine

M with a semi-inﬁnite tape, we use the arrangement shown in Figure 10.3.
The simulating machine 
M has a tape with two tracks. On the upper
one, we keep the information to the right of some reference point on M's
tape. The reference point could be, for example, the position of the read-
write head at the start of the computation. The lower track contains the
left part of M's tape in reverse order. 
M is programmed so that it will use
information on the upper track only as long as M's read-write head is to the
right of the reference point, and work on the lower track as M moves into the
left part of its tape. The distinction can be made by partitioning the state
set of 
M into two parts, say QU and QL: the ﬁrst to be used when working
on the upper track, the second to be used on the lower one. Special end
markers # are put on the left boundary of the tape to facilitate switching
from one track to the other. For example, assume that the machine to be
simulated and the simulating machine are in the respective conﬁgurations
shown in Figure 10.4 and that the move to be simulated is generated by
δ (qi, a) = (qj, c, L) .
The simulating machine will ﬁrst move via the transition
δ (qi, (a, b)) = (qj, (c, b) , L) ,
where qi ∈QU. Because qi belongs to QU, only information in the upper
track is considered at this point. Now, the simulating machine sees (#, #)
in state qj ∈QU. It next uses a transition
δ (qj, (#, #)) = (pj, (#, #) , R) ,

10.1 Minor Variations on the Turing Machine Theme
265
b
a
Reference point
qi
#
a
#
b
qi
^
(b)
(a)
FIGURE 10.4 (a) Machine to be simulated.
(b) Simulating machine.
#
a
#
b
qi^
#
c
#
b
qj^
#
c
#
b
pj^
FIGURE 10.5 Sequence of conﬁgurations in simulating δ (qi, a) = (qj, c, L).
with pj ∈QL, putting it into the conﬁguration shown in Figure 10.5. Now
the machine is in a state from QL and will work on the lower track. Further
details of the simulation are straightforward.
The Off-Line Turing Machine
The general deﬁnition of an automaton in Chapter 1 contained an input
ﬁle as well as temporary storage. In Deﬁnition 9.1 we discarded the input
ﬁle for reasons of simplicity, claiming that this made no diﬀerence to the
Turing machine concept. We now expand on this claim.
If we put the input ﬁle back into the picture, we get what is known as
an oﬀ-line Turing machine. In such a machine, each move is governed
by the internal state, what is currently read from the input ﬁle, and what
is seen by the read-write head. A schematic representation of an oﬀ-line
machine is shown in Figure 10.6. A formal deﬁnition of an oﬀ-line Turing
machine is easily made, but we will leave this as an exercise.
What we
want to do brieﬂy is to indicate why the class of oﬀ-line Turing machines is
equivalent to the class of standard machines.
First, the behavior of any standard Turing machine can be simulated by
some oﬀ-line model. All that needs to be done by the simulating machine
is to copy the input from the input ﬁle to the tape. Then it can proceed in
the same way as the standard machine.
The simulation of an oﬀ-line machine M by a standard machine 
M
requires a lengthier description.
A standard machine can simulate the

266
Chapter 10 Other Models of Turing Machines
a
b
c
d
Read-only input file
e
f
g
Tape
Control unit
FIGURE 10.6
Control unit
of M
a
b
c
d
0
0
1
e
f
g
0
1
0
0
FIGURE 10.7
computation of an oﬀ-line machine by using the four-track arrangement
shown in Figure 10.7. In that picture, the tape contents shown represent
the speciﬁc conﬁguration of Figure 10.6. Each of the four tracks of 
M plays
a speciﬁc role in the simulation. The ﬁrst track has the input, the second
marks the position at which the input is read, the third represents the tape
of M, and the fourth shows the position of M's read-write head.
The simulation of each move of M requires a number of moves of 
M.
Starting from some standard position, say the left end, and with the relevant
information marked by special end markers, 
M searches track 2 to locate
the position at which the input ﬁle of M is read. The symbol found in the
corresponding cell on track 1 is remembered by putting the control unit of

M into a state chosen for this purpose. Next, track 4 is searched for the
position of the read-write head of M. With the remembered input and the
symbol on track 3, we now know that M is to do. This information is again
remembered by 
M with an appropriate internal state. Next, all four tracks

10.1 Minor Variations on the Turing Machine Theme
267
of 
M's tape are modiﬁed to reﬂect the move of M. Finally, the read-write
head of 
M returns to the standard position for the simulation of the next
move.
EXERCISES
1. Give a formal deﬁnition of a Turing machine with a semi-inﬁnite tape.
2. Give a formal deﬁnition of an oﬀ-line Turing machine.
3. Consider a Turing machine that, on any particular move, can either change
the tape symbol or move the read-write head, but not both.
(a) Give a formal deﬁnition of such a machine.
(b) Show that the class of such machines is equivalent to the class of
standard Turing machines.
4. Show how a machine of the type deﬁned in the previous exercise could simu-
late the moves
δ(qi, a) = (qj, b, R)
δ(qi, b) = (qk, a, L)
of a standard Turing machine.
5. Consider a model of a Turing machine in which each move permits the read-
write head to travel more than one cell to the left or right, the distance and
direction of travel being one of the arguments of δ. Give a precise deﬁnition
of such an automaton and sketch a simulation of it by a standard Turing
machine.
6. A nonerasing Turing machine is one that cannot change a nonblank symbol
to a blank. This can be achieved by the restriction that if
δ (qi, a) = (qj, □, L or R) ,
then a must be ⊔. Show that no generality is lost by making such a restriction.
7. Consider a Turing machine that cannot write blanks; that is, for all δ (qi, a) =
(qj, b, L or R), b must be in Γ −{□}. Show how such a machine can simulate
a standard Turing machine.
8. Suppose we make the requirement that a Turing machine can halt only in
a ﬁnal state: that is, we ask that δ (q, a) be deﬁned for all pairs (q, a) with
a ∈Γ and q /∈F. Does this restrict the power of the Turing machine?
9. Suppose we make the restriction that a Turing machine must always write a
symbol diﬀerent from the one it reads: that is, if
δ (qi, a) = (qj, b, L or R) ,
then a and b must be diﬀerent. Does this limitation reduce the power of the
automaton?

268
Chapter 10 Other Models of Turing Machines
10. Consider a version of the standard Turing machine in which transitions can
depend not only on the cell directly under the read-write head, but also on
the cells to the immediate right and left. Make a formal deﬁnition of such a
machine, then sketch its simulation by a standard Turing machine.
11. Consider a Turing machine with a diﬀerent decision process in which transi-
tions are made if the current tape symbol is not one of a speciﬁed set. For
example,
δ (qi, {a, b}) = (qj, c, R)
will allow the indicated move if the current tape symbol is neither a nor b.
Formalize this concept and show that this type of Turing machine is equivalent
to a standard Turing machine.
12. Write a program for a Turing machine of the type described in the previous
exercise that accepts L(aa∗bb∗). Assume that Σ = {a, b} and Γ = {a, b, ⊔}.
10.2
TURING MACHINES WITH MORE COMPLEX
STORAGE
The storage device of a standard Turing machine is so simple that one might
think it possible to gain power by using more complicated storage devices.
But this is not the case, as we now illustrate with two examples.
Multitape Turing Machines
A multitape Turing machine is a Turing machine with several tapes, each
with its own independently controlled read-write head (Figure 10.8).
The formal deﬁnition of a multitape Turing machine goes beyond Def-
inition 9.1, since it requires a modiﬁed transition function. Typically, we
deﬁne an n-tape machine by M = (Q, Σ, Γ, δ, q0, □, F), where Q, Σ, Γ, q0, F
Tape 1
Tape 2
q0
a
b
c
d
e
f
FIGURE 10.8

10.2 Turing Machines with More Complex Storage
269
Tape 1
Tape 2
q1
x
b
c
d
y
f
FIGURE 10.9
are as in Deﬁnition 9.1, but where
δ : Q × Γn →Q × Γn × {L, R}n
speciﬁes what happens on all the tapes.
For example, if n = 2, with a
current conﬁguration shown in Figure 10.8, then
δ (q0, a, e) = (q1, x, y, L, R)
is interpreted as follows.
The transition rule can be applied only if the
machine is in state q0 and the ﬁrst read-write head sees an a and the second
an e. The symbol on the ﬁrst tape will then be replaced with an x and
its read-write head will move to the left. At the same time, the symbol on
the second tape is rewritten as y and the read-write head moves right. The
control unit then changes its state to q1 and the machine goes into the new
conﬁguration shown in Figure 10.9.
To show the equivalence between multitape and standard Turing ma-
chines, we argue that any given multitape Turing machine M can be simu-
lated by a standard Turing machine 
M and, conversely, that any standard
Turing machine can be simulated by a multitape one. The second part of
this claim needs no elaboration, since we can always elect to run a multitape
machine with only one of its tapes doing useful work. The simulation of a
multitape machine by one with a single tape is a little more complicated,
but conceptually straightforward.
Consider, for example, the two-tape machine in the conﬁguration de-
picted in Figure 10.10. The simulating single-tape machine will have four
tracks (Figure 10.11). The ﬁrst track represents the contents of tape 1 of
M. The nonblank part of the second track has all zeros, except for a single
1 marking the position of M's read-write head. Tracks 3 and 4 play a sim-
ilar role for tape 2 of M. Figure 10.11 makes it clear that, for the relevant
conﬁgurations of 
M (that is, the ones that have the indicated form), there
is a unique corresponding conﬁguration of M.
The representation of a multitape machine by a single-tape machine is
similar to that used in the simulation of an oﬀ-line machine. The actual

270
Chapter 10 Other Models of Turing Machines
q
a
b
c
d
e
f
g
h
FIGURE 10.10
q
a
b
c
d
0
1
0
0
e
f
g
h
0
0
1
0
FIGURE 10.11
steps in the simulation are also much the same, the only diﬀerence being that
there are more tapes to consider. The outline given for the simulation of oﬀ-
line machines carries over to this case with minor modiﬁcations and suggests
a procedure by which the transition function δ of 
M can be constructed
from the transition function δ of M. While it is not diﬃcult to make the
construction precise, it takes a lot of writing. Certainly, the computations
of 
M give the appearance of being lengthy and elaborate, but this has no
bearing on the conclusion. Whatever can be done on M can also be done
on 
M.
It is important to keep in mind the following point. When we claim that
a Turing machine with multiple tapes is no more powerful than a standard
one, we are making a statement only about what can be done by these
machines, particularly, what languages can be accepted.
EXAMPLE 10.1
Consider the language {anbn}.
In Example 9.7, we described a
laborious method by which this language can be accepted by a Turing
machine with one tape. Using a two-tape machine makes the job much

10.2 Turing Machines with More Complex Storage
271
easier. Assume that an initial string anbm is written on tape 1 at the
beginning of the computation. We then read all the a's, copying them
onto tape 2. When we reach the end of the a's, we match the b's on
tape 1 against the copied a's on tape 2. This way, we can determine
whether there are an equal number of a's and b's without repeated
back-and-forth movement of the read-write head.
Remember that the various models of Turing machines are considered
equivalent only with respect to their ability to do things, not with respect
to ease of programming or any other eﬃciency measure we might consider.
We will return to this important point in Chapter 14.
Multidimensional Turing Machines
A multidimensional Turing machine is one in which the tape can be viewed
as extending inﬁnitely in more than one dimension. A diagram of a two-
dimensional Turing machine is shown in Figure 10.12.
The formal deﬁnition of a two-dimensional Turing machine involves a
transition function δ of the form
δ : Q × Γ →Q × Γ × {L, R, U, D} ,
where U and D specify movement of the read-write head up and down,
respectively.
To simulate this machine on a standard Turing machine, we can use the
two-track model depicted in Figure 10.13. First, we associate an ordering
1, -1
1, 1
-1, 1
1, 2
Two-dimensional
address scheme
FIGURE 10.12
1
#
#
1
b
0
#
-
3
#
2
a
FIGURE 10.13

272
Chapter 10 Other Models of Turing Machines
or address with the cells of the two-dimensional tape. This can be done in
a number of ways, for example, in the two-dimensional fashion indicated
in Figure 10.12.
The two-track tape of the simulating machine will use
one track to store cell contents and the other one to keep the associated
address.
In the scheme of Figure 10.12, the conﬁguration in which cell
(1, 2) contains a and cell (10, −3) contains b is shown in Figure 10.13. Note
one complication: The cell address can involve arbitrarily large integers, so
the address track cannot use a ﬁxed-size ﬁeld to store addresses. Instead,
we must use a variable ﬁeld-size arrangement, using some special symbols
to delimit the ﬁelds, as shown in the picture.
Let us assume that, at the start of the simulation of each move, the read-
write head of the two-dimensional machine M and the read-write head of
the simulating machine 
M are always on corresponding cells. To simulate
a move, the simulating machine 
M ﬁrst computes the address of the cell to
which M is to move. Using the two-dimensional address scheme, this is a
simple computation. Once the address is computed, 
M ﬁnds the cell with
this address on track 2 and then changes the cell contents to account for the
move of M. Again, given M, there is a straightforward construction for 
M.
EXERCISES
1. Give a formal deﬁnition of a two-tape Turing machine; then write programs
that accept the languages below. Assume that Σ = {a, b, c} and that the
input is initially all on tape 1.
(a) L = {anbncn}, n ≥1.
(b) L = {anbncm, m > n}.
(c) L = {ww : w ∈{a, b}}.
(d) L = {wwRw : w ∈{a, b}}.
(e) L = {na(w) = nb(w) = nc(w)}.
(f) L = {na(w) = nb(w) ≥nc(w)}.
2. Deﬁne what one might call a multitape oﬀ-line Turing machine and describe
how it can be simulated by a standard Turing machine.
3. A multihead Turing machine can be visualized as a Turing machine with a
single tape and a single control unit but with multiple, independent read-
write heads. Give a formal deﬁnition of a multihead Turing machine, and
then show how such a machine can be simulated with a standard Turing
machine.
4. Give a formal deﬁnition of a multihead-multitape Turing machine. Then show
how such a machine can be simulated by a standard Turing machine.

10.3 Nondeterministic Turing Machines
273
5. Give a formal deﬁnition of a Turing machine with a single tape but multiple
control units, each with a single read-write head. Discuss how such a machine
can be simulated with a multitape machine.
10.3
NONDETERMINISTIC TURING MACHINES
While Turing's thesis makes it plausible that the speciﬁc tape structure is
immaterial to the power of the Turing machine, the same cannot be said of
nondeterminism. Since nondeterminism involves an element of choice and
so has a nonmechanistic ﬂavor, an appeal to Turing's thesis is inappropriate.
We must look at the eﬀect of nondeterminism in more detail if we want to
argue that nondeterminism adds nothing to the power of a Turing machine.
Again we resort to simulation, showing that nondeterministic behavior can
be handled deterministically.
DEFINITION 10.2
A nondeterministic Turing machine is an automaton as given by Deﬁnition
9.1, except that δ is now a function
δ : Q × Γ →2Q×Γ×{L,R}.
As always when nondeterminism is involved, the range of δ is a set of possible
transitions, any of which can be chosen by the machine.
EXAMPLE 10.2
If a Turing machine has transitions speciﬁed by
δ (q0, a) = {(q1, b, R) , (q2, c, L)},
it is nondeterministic. The moves
q0aaa ⊢bq1aa
and
q0aaa ⊢q2□caa
are both possible.

274
Chapter 10 Other Models of Turing Machines
+
a
a
a
a
b
b
q0
q1
FIGURE 10.14
Since it is not clear what role nondeterminism plays in computing func-
tions, nondeterministic automata are usually viewed as accepters. A non-
deterministic Turing machine is said to accept w if there is any possible
sequence of moves such that
q0w
∗
⊢x1qfx2,
with qf ∈F. A nondeterministic machine may have moves available that
lead to a nonﬁnal state or to an inﬁnite loop. But, as always with nonde-
terminism, these alternatives are irrelevant; all we are interested in is the
existence of some sequence of moves leading to acceptance.
To show that a nondeterministic Turing machine is no more powerful
than a deterministic one, we need to provide a deterministic equivalent for
the nondeterminism. We have already alluded to one. Nondeterminism can
be viewed as a deterministic backtracking algorithm, and a deterministic
machine can simulate a nondeterministic one as long as it can handle the
bookkeeping involved in the backtracking. To see how this can be done
simply, let us consider an alternative view of nondeterminism, one which
is useful in many arguments: A nondeterministic machine can be seen as
one that has the ability to replicate itself whenever necessary. When more
than one move is possible, the machine produces as many replicas as needed
and gives each replica the task of carrying out one of the alternatives. This
view of nondeterminism may seem particularly nonmechanistic, since unlim-
ited replication is certainly not within the power of present-day computers.
Nevertheless, a simulation is possible.
One way to visualize the simulation is to use a standard Turing machine,
keeping all possible instantaneous descriptions of the nondeterministic ma-
chine on its tape, separated by some convention.
Figure 10.14 shows a
way in which the two conﬁgurations aq0aa and bbq1a might appear. The
symbol × is used to delimit the area of interest, while + separates in-
dividual instantaneous descriptions. The simulating machine looks at all
active conﬁgurations and updates them according to the program of the
nondeterministic machine. New conﬁgurations or expanding instantaneous
descriptions will involve moving the × markers. The details are certainly
tedious, but not hard to visualize. Based on this simulation, we conclude
that for every nondeterministic Turing machine there exists an equivalent
deterministic standard machine.

10.3 Nondeterministic Turing Machines
275
THEOREM 10.2
The class of deterministic Turing machines and the class of nondeterministic
Turing machines are equivalent.
Proof:
Use the construction suggested above to show that any nondeter-
ministic Turing machine can be simulated by a deterministic one.
Later we will reconsider the eﬀect of nondeterminism in practical sit-
uations, so we need to add some comments. As always, nondeterminism
can be seen as a choice between alternatives. This can be visualized as a
decision tree (Figure 10.15).
initial configuration
configurations after one move
configurations after two moves
halt
halt
FIGURE 10.15
The width of such a conﬁguration tree depends on the branching factor,
that is, the number of options available on each move. If k denotes the
maximum branching, then
M = kn
(10.1)
is the maximum number of conﬁgurations that can exist after n moves.
For later purposes, it is necessary to elaborate on the deﬁnition of lan-
guage acceptance and also include the membership issue.
DEFINITION 10.3
A nondeterministic Turing machine M is said to accept a language L if, for
all w ∈L, at least one of the possible conﬁgurations accepts w. There may
be branches that lead to nonaccepting conﬁgurations, while some may put
the machine into an inﬁnite loop. But these are irrelevant for acceptance.
A nondeterministic Turing machine M is said to decide a language L if,
for all w ∈Σ∗, there is a path that leads either to acceptance or rejection.

276
Chapter 10 Other Models of Turing Machines
EXERCISES
1. Discuss the simulation of a nondeterministic Turing machine by a determin-
istic one. Indicate explicitly how new machines are created, how active ma-
chines are identiﬁed, and how machines that halt are removed from further
consideration.
2. Write programs for nondeterministic Turing machines that accept the lan-
guages below. In each case, explain if and how the nondeterminism simpliﬁes
the task.
(a) L =

ww : w ∈{a, b}+
.
(b) L =

wwRw : w ∈{a, b}+
.
(c) L =

xwwRy : x, y, w ∈{a, b}+ , |x| ≥|y|

.
(d) L = {w : na(w) = nB(w) = nc(w)}.
(e) L = {an : n is not a prime number}.
3. A two-stack automaton is a nondeterministic pushdown automaton with two
independent stacks. To deﬁne such an automaton, we modify Deﬁnition 7.1
so that
δ : Q × (Σ ∪{λ}) × Γ × Γ →ﬁnite subsets of Q × Γ∗× Γ∗.
A move depends on the tops of the two stacks and results in new values being
pushed on these two stacks. Show that the class of two-stack automata is
equivalent to the class of Turing machines.
10.4
A UNIVERSAL TURING MACHINE
Consider the following argument against Turing's thesis: "A Turing machine
as presented in Deﬁnition 9.1 is a special-purpose computer.
Once δ is
deﬁned, the machine is restricted to carrying out one particular type of
computation. Digital computers, on the other hand, are general-purpose
machines that can be programmed to do diﬀerent jobs at diﬀerent times.
Consequently, Turing machines cannot be considered equivalent to general-
purpose digital computers."
This objection can be overcome by designing a reprogrammable Turing
machine, called a universal Turing machine. A universal Turing machine
Mu is an automaton that, given as input the description of any Turing
machine M and a string w, can simulate the computation of M on w. To
construct such an Mu, we ﬁrst choose a standard way of describing Turing
machines. We may, without loss of generality, assume that
Q = {q1, q2, ..., qn} ,

10.4 A Universal Turing Machine
277
with q1 the initial state, q2 the single ﬁnal state, and
Γ = {a1, a2, ..., am} ,
where a1 represents the blank. We then select an encoding in which q1 is
represented by 1, q2 is represented by 11, and so on. Similarly, a1 is encoded
as 1, a2 as 11, etc. The symbol 0 will be used as a separator between the 1's.
With the initial and ﬁnal state and the blank deﬁned by this convention,
any Turing machine can be described completely with δ only. The transition
function is encoded according to this scheme, with the arguments and result
in some prescribed sequence.
For example, δ (q1, a2) = (q2, a3, L) might
appear as
· · · 1 0 1 1 0 1 1 0 1 1 1 0 1 0 · · · .
It follows from this that any Turing machine has a ﬁnite encoding as a string
on {0, 1}+ and that, given any encoding of M, we can decode it uniquely.
Some strings will not represent any Turing machine (e.g., the string 00011),
but we can easily spot these, so they are of no concern.
A universal Turing machine Mu then has an input alphabet that in-
cludes {0, 1} and the structure of a multitape machine, as shown in Figure
10.16.
For any input M and w, tape 1 will keep an encoded deﬁnition of M.
Tape 2 will contain the tape contents of M, and tape 3 the internal state
of M. Mu looks ﬁrst at the contents of tapes 2 and 3 to determine the
conﬁguration of M. It then consults tape 1 to see what M would do in this
conﬁguration. Finally, tapes 2 and 3 will be modiﬁed to reﬂect the result
of the move.
It is within reason to construct an actual universal Turing machine
(see, for example, Denning, Dennis, and Qualitz 1978), but the process is
uninteresting.
We prefer instead to appeal to Turing's hypothesis.
The
implementation clearly can be done using some programming language; in
Control unit
of Mu
Description of  M
Internal state of  M
Tape contents of  M
FIGURE 10.16

278
Chapter 10 Other Models of Turing Machines
fact, the program suggested in Exercise 1, Section 9.1, is a realization of a
universal Turing machine in a higher-level language. Therefore, we expect
that it can also be done by a standard Turing machine. We are then justiﬁed
in claiming the existence of a Turing machine that, given any program, can
carry out the computations speciﬁed by that program and that is therefore
a proper model for a general-purpose computer.
The observation that every Turing machine can be represented by a
string of 0's and 1's has important implications.
But before we explore
these implications, we need to review some results from set theory.
Some sets are ﬁnite, but most of the interesting sets (and languages) are
inﬁnite. For inﬁnite sets, we distinguish between sets that are countable
and sets that are uncountable. A set is said to be countable if its elements
can be put into a one-to-one correspondence with the positive integers. By
this we mean that the elements of the set can be written in some order,
say, x1, x2, x3, ..., so that every element of the set has some ﬁnite index. For
example, the set of all even integers can be written in the order 0, 2, 4, ....
Since any positive integer 2n occurs in position n + 1, the set is countable.
This should not be too surprising, but there are more complicated examples,
some of which may seem counterintuitive. Take the set of all quotients of
the form p/q, where p and q are positive integers. How should we order this
set to show that it is countable? We cannot use the sequence
1
1, 1
2, 1
3, 1
4, ...
because then 2
3 would never appear. This does not imply that the set is
uncountable; in this case, there is a clever way of ordering the set to show
that it is in fact countable. Look at the scheme depicted in Figure 10.17,
and write down the element in the order encountered following the arrows.
This gives us
1
1, 1
2, 2
1, 3
1, 2
2, ....
Here the element 2
3 occurs in the seventh place, and every element has some
position in the sequence. The set is therefore countable.
1
3
_
1
2
_
1
1
_
2
3
_
2
2
_
2
1
_
3
3
_
3
2
_
3
1
_
. . .
. . .
. . .
. . .
. . .
. . .
FIGURE 10.17

10.4 A Universal Turing Machine
279
We see from this example that we can prove that a set is countable if
we can produce a method by which its elements can be written in some
sequence. We call such a method an enumeration procedure. Since an
enumeration procedure is some kind of mechanical process, we can use a
Turing machine model to deﬁne it formally.
DEFINITION 10.4
Let S be a set of strings on some alphabet Σ. Then an enumeration proce-
dure for S is a Turing machine that can carry out the sequence of steps
q0□
∗
⊢qsx1 # s1
∗
⊢qsx2 # s2 ...,
with xi ∈Γ∗−{#} , si ∈S, in such a way that any s in S is produced in a
ﬁnite number of steps. The state qs is a state signifying membership in S;
that is, whenever qs is entered, the string following # must be in S.
Not every set is countable. As we will see in the next chapter, there are
some uncountable sets. But any set for which an enumeration procedure
exists is countable because the enumeration gives the required sequence.
Strictly speaking, an enumeration procedure cannot be called an algo-
rithm since it will not terminate when S is inﬁnite. Nevertheless, it can
be considered a meaningful process, because it produces well-deﬁned and
predictable results.
EXAMPLE 10.3
Let Σ = {a, b, c}.
We can show that the S = Σ+ is countable if
we can ﬁnd an enumeration procedure that produces its elements
in some order, say in the order in which they would appear in a
dictionary.
However, the order used in dictionaries is not suitable
without modiﬁcation. In a dictionary, all words beginning with a are
listed before the string b. But when there are an inﬁnite number of a
words, we will never reach b, thus violating the condition of Deﬁnition
10.4 that any given string be listed after a ﬁnite number of steps.
Instead, we can use a modiﬁed order, in which we take the length
of the string as the ﬁrst criterion, followed by an alphabetic ordering of
all equal-length strings. This is an enumeration procedure that gives
the sequence
a, b, c, aa, ab, ac, ba, bb, bc, ca, cb, cc, aaa, ....
As we will have several uses for such an ordering, we will call it the
proper order.

280
Chapter 10 Other Models of Turing Machines
An important consequence of the previous discussion is that Turing
machines are countable.
THEOREM 10.3
The set of all Turing machines, although inﬁnite, is countable.
Proof:
We can encode each Turing machine using 0 and 1.
With this
encoding, we then construct the following enumeration procedure.
1. Generate the next string in {0, 1}+ in proper order.
2. Check the generated string to see if it deﬁnes a Turing machine. If so,
write it on the tape in the form required by Deﬁnition 10.4. If not,
ignore the string.
3. Return to Step 1.
Since every Turing machine has a ﬁnite description, any speciﬁc machine
will eventually be generated by this process.
The particular ordering of Turing machines depends on the encoding
we use; if we use a diﬀerent encoding, we must expect a diﬀerent ordering.
This is of no consequence, however, and shows that the ordering itself is
unimportant. What matters is the existence of some ordering.
EXERCISES
1. Give the encoding, using the suggested method, for
δ (q1, a1) = (q1, a1, R) ,
δ (q1, a2) = (q3, a1, L) ,
δ (q3, a1) = (q2, a2, L) .
2. If a is encoded as 1, b as 11, R as 1, L as 11, decode the string 011010111011010.
3. Sketch an algorithm that examines a string in {0, 1}+ to determine whether
or not it represents an encoded Turing machine.
4. Sketch a Turing machine program that enumerates the set {0, 1}+ in proper
order.
5. What is the index of 0i1j in Exercise 4?
6. Sketch the design of a Turing machine that enumerates the following set in
proper order:
L = {anbn : n ≥1} .

10.5 Linear Bounded Automata
281
7. For Example 10.3, ﬁnd a function f(w) that gives, for each w, its index in
the proper ordering.
8. Show that the set of all triplets, (i, j, k) with i, j, k positive integers, is count-
able.
9. Suppose that S1 and S2 are countable sets.
Show that then S1 ∪S2 and
S1 × S2 are also countable.
10. Show that the Cartesian product of a ﬁnite number of countable sets is count-
able.
10.5
LINEAR BOUNDED AUTOMATA
While it is not possible to extend the power of the standard Turing machine
by complicating the tape structure, it is possible to limit it by restricting
the way in which the tape can be used. We have already seen an example of
this with pushdown automata. A pushdown automaton can be regarded as
a nondeterministic Turing machine with a tape that is restricted to being
used like a stack. We can also restrict the tape usage in other ways; for
example, we might permit only a ﬁnite part of the tape to be used as work
space.
It can be shown that this leads us back to ﬁnite automata (see
Exercise 3 at the end of this section), so we need not pursue this. But there
is a way of limiting tape use that leads to a more interesting situation: We
allow the machine to use only that part of the tape occupied by the input.
Thus, more space is available for long input strings than for short ones,
generating another class of machines, the linear bounded automata (or
lba).
A linear bounded automaton, like a standard Turing machine, has an
unbounded tape, but how much of the tape can be used is a function of
the input. In particular, we restrict the usable part of the tape to exactly
the cells taken by the input.1 To enforce this, we can envision the input as
bracketed by two special symbols, the left-end marker [ and the right-
end marker ].
For an input w, the initial conﬁguration of the Turing
machine is given by the instantaneous description q0 [w]. The end markers
cannot be rewritten, and the read-write head cannot move to the left of [
or to the right of ]. We sometimes say that the read-write head "bounces"
oﬀthe end markers.
1In some deﬁnitions, the usable part of the tape is a multiple of the input length,
where the multiple can depend on the language, but not on the input. Here we use only
the exact length of the input string, but we do allow multitrack machines, with the input
on only one track.

282
Chapter 10 Other Models of Turing Machines
DEFINITION 10.5
A linear bounded automaton is a nondeterministic Turing machine M =
(Q, Σ, Γ, δ, q0, □, F), as in Deﬁnition 10.2, subject to the restriction that Σ
must contain two special symbols [ and ], such that δ (qi, [) can contain only
elements of the form (qj, [, R), and δ (qi, ]) can contain only elements of the
form (qj, ], L).
DEFINITION 10.6
A string w is accepted by a linear bounded automaton if there is a possible
sequence of moves
q0 [w]
∗
⊢[x1qfx2]
for some qf ∈F, x1, x2 ∈Γ∗. The language accepted by the lba is the set
of all such accepted strings.
Note that in this deﬁnition a linear bounded automaton is assumed to
be nondeterministic. This is not just a matter of convenience but essential
to the discussion of lba's.
EXAMPLE 10.4
The language
L = {anbncn : n ≥1}
is accepted by some linear bounded automaton. This follows from the
discussion in Example 9.8. The computation outlined there does not
require space outside the original input, so it can be carried out by a
linear bounded automaton.
EXAMPLE 10.5
Find a linear bounded automaton that accepts the language
L =

an! : n ≥0

.
One way to solve the problem is to divide the number of a's successively
by 2, 3, 4, ..., until we can either accept or reject the string. If the input

10.5 Linear Bounded Automata
283
is in L, eventually there will be a single a left; if not, at some point a
nonzero remainder will arise. We sketch the solution to point out one
tacit implication of Deﬁnition 10.5. Since the tape of a linear bounded
automaton may be multitrack, the extra tracks can be used as work
space. For this problem, we can use a two-track tape. The ﬁrst track
contains the number of a's left during the process of division, and the
second track contains the current divisor (Figure 10.18). The actual
solution is fairly simple.
Using the divisor on the second track, we
divide the number of a's on the ﬁrst track, say by removing all symbols
except those at multiples of the divisor. After this, we increment the
divisor by one, and continue until we either ﬁnd a nonzero remainder
or are left with a single a.
[
[
a
a
a
a
a
a
a
a
]
]
a
a's to be examined
Current divisor
FIGURE 10.18
The last two examples suggest that linear bounded automata are more
powerful than pushdown automata, since neither of the languages is context
free. To prove such a conjecture, we still have to show that any context-free
language can be accepted by a linear bounded automaton. We will do this
later in a somewhat roundabout way; a more direct approach is suggested
in Exercises 6 and 7 at the end of this section. It is not so easy to make
a conjecture on the relation between Turing machines and linear bounded
automata. Problems like Example 10.5 are invariably solvable by a linear
bounded automaton, since an amount of scratch space proportional to the
length of the input is available.
In fact, it is quite diﬃcult to come up
with a concrete and explicitly deﬁned language that cannot be accepted by
any linear bounded automaton. In Chapter 11 we will show that the class
of linear bounded automata is less powerful than the class of unrestricted
Turing machines, but a demonstration of this requires a lot more work.
EXERCISES
1. Explain how linear bounded automata could be constructed to accept the
following languages:
(a) L =

an : n = m2, m ≥1

.
(b) L = {an : n is a prime number}.
(c) L =

ww : w ∈{a, b}+
.

284
Chapter 10 Other Models of Turing Machines
(d) L =

wn : w ∈{a, b}+ , n ≥2

.
(e) L =

wwwR : w ∈{a, b}+
.
(f) L = {wwvvR : w, v ∈{a, b}+}.
(g) L = {wwR : na(w) = nb(w)}.
(h) the positive closure of the language in Part c.
(i) the complement of the language in Part c.
2. Show that, for every context-free language, there exists an accepting pda such
that the number of symbols in the stack never exceeds the length of the input
string by more than one.
3. Use the observation in the above exercise to show that any context-free lan-
guage not containing λ is accepted by some linear bounded automaton.
4. To deﬁne a deterministic linear bounded automaton, we can use Deﬁnition
10.5, but require that the Turing machine be deterministic. Examine your
solutions to Exercise 4. Are the solutions all deterministic linear bounded
automata? If not, try to ﬁnd solutions that are.

11
CHA P T E R
A HIERARCHY OF
FORMAL LANGUAGES
AND AUTOMATA
CHAPTER SUMMARY
In this chapter, we study the connection between Turing machines and
languages. Depending on how one defines language acceptance, we
get several different language families: recursive, recursively enumer-
able, and context-sensitive languages. With regular and context-free
languages, these form a properly nested relation called the Chomsky
hierarchy.
W
e now return our attention to our main interest, the study of
formal languages.
Our immediate goal will be to examine the
languages associated with Turing machines and some of their re-
strictions. Because Turing machines can perform any kind of algorithmic
computation, we expect to ﬁnd that the family of languages associated with
them is quite broad. It includes not only regular and context-free languages,
but also the various examples we have encountered that lie outside these
families. The nontrivial question is whether there are any languages that
are not accepted by some Turing machine. We will answer this question
ﬁrst by showing that there are more languages than Turing machines, so
that there must be some languages for which there are no Turing machines.
The proof is short and elegant, but nonconstructive, and gives little in-
sight into the problem.
For this reason, we will establish the existence
of languages not recognizable by Turing machines through more explicit
285

286
Chapter 11 A Hierarchy of Formal Languages and Automata
examples that actually allow us to identify one such language.
Another
avenue of investigation will be to look at the relation between Turing ma-
chines and certain types of grammars and to establish a connection between
these grammars and regular and context-free grammars. This leads to a
hierarchy of grammars and through it to a method for classifying language
families. Some set-theoretic diagrams illustrate the relationships between
various language families clearly.
Strictly speaking, many of the arguments in this chapter are valid only
for languages that do not include the empty string. This restriction arises
from the fact that Turing machines, as we have deﬁned them, cannot accept
the empty string. To avoid having to rephrase the deﬁnition or having to
add a repeated disclaimer, we make the tacit assumption that the languages
discussed in this chapter, unless otherwise stated, do not contain λ. It is a
trivial matter to restate everything so that λ is included, but we will leave
this to the reader.
11.1
RECURSIVE AND RECURSIVELY ENUMERABLE
LANGUAGES
We start with some terminology for the languages associated with Turing
machines. In doing so, we must make the important distinction between
languages for which there exists an accepting Turing machine and languages
for which there exists a membership algorithm. Because a Turing machine
does not necessarily halt on input that it does not accept, the ﬁrst does not
imply the second.
DEFINITION 11.1
A language L is said to be recursively enumerable if there exists a Turing
machine that accepts it.
This deﬁnition implies only that there exists a Turing machine M, such
that, for every w ∈L,
q0w
∗
⊢M x1qfx2,
with qf a ﬁnal state. The deﬁnition says nothing about what happens for
w not in L; it may be that the machine halts in a nonﬁnal state or that
it never halts and goes into an inﬁnite loop. We can be more demanding
and ask that the machine tell us whether or not any given input is in its
language.

11.1 Recursive and Recursively Enumerable Languages
287
DEFINITION 11.2
A language L on Σ is said to be recursive if there exists a Turing machine
M that accepts L and that halts on every w in Σ+.
In other words, a
language is recursive if and only if there exists a membership algorithm for it.
If a language is recursive, then there exists an easily constructed enu-
meration procedure. Suppose that M is a Turing machine that determines
membership in a recursive language L. We ﬁrst construct another Turing
machine, say 
M, that generates all strings in Σ+ in proper order, let us
say w1, w2, .... As these strings are generated, they become the input to M,
which is modiﬁed so that it writes strings on its tape only if they are in L.
That there is also an enumeration procedure for every recursively enu-
merable language is not as easy to see. We cannot use the previous argument
as it stands, because if some wj is not in L, the machine M, when started
with wj on its tape, may never halt and therefore never get to the strings
in L that follow wj in the enumeration. To make sure that this does not
happen, the computation is performed in a diﬀerent way. We ﬁrst get 
M
to generate w1 and let M execute one move on it. Then we let 
M generate
w2 and let M execute one move on w2, followed by the second move on w1.
After this, we generate w3 and do one step on w3, the second step on w2,
the third step on w1, and so on. The order of performance is depicted in
Figure 11.1. From this, it is clear that M will never get into an inﬁnite
loop. Since any w ∈L is generated by 
M and accepted by M in a ﬁnite
number of steps, every string in L is eventually produced by M.
It is easy to see that every language for which an enumeration procedure
exists is recursively enumerable. We simply compare the given input string
against successive strings generated by the enumeration procedure. If w ∈
L, we will eventually get a match, and the process can be terminated.
Deﬁnitions 11.1 and 11.2 give us very little insight into the nature of ei-
ther recursive or recursively enumerable languages. These deﬁnitions attach
First move .
.
Second move .
.
.
.
.
.
Third move .
w1
w2
w3
w4 . . .
.
.
.
FIGURE 11.1

288
Chapter 11 A Hierarchy of Formal Languages and Automata
names to language families associated with Turing machines, but shed no
light on the nature of representative languages in these families. Nor do
they tell us much about the relationships between these languages or their
connection to the language families we have encountered before. We are
therefore immediately faced with questions such as "Are there languages
that are recursively enumerable but not recursive?" and "Are there lan-
guages, describable somehow, that are not recursively enumerable?" While
we will be able to supply some answers, we will not be able to produce very
explicit examples to illustrate these questions, especially the second one.
Languages That Are Not Recursively Enumerable
We can establish the existence of languages that are not recursively enu-
merable in a variety of ways. One is very short and uses a very fundamental
and elegant result of mathematics.
THEOREM 11.1
Let S be an inﬁnite countable set. Then its powerset 2S is not countable.
Proof: Let S = {s1, s2, s3, ...}. Then any element t of 2S can be represented
by a sequence of 0's and 1's, with a 1 in position i if and only if si is in
t.
For example, the set {s2, s3, s6} is represented by 01100100..., while
{s1, s3, s5, ...} is represented by 10101.... Clearly, any element of 2S can be
represented by such a sequence, and any such sequence represents a unique
element of 2S. Suppose that 2S were countable; then its elements could be
written in some order, say t1, t2, ..., and we could enter these into a table, as
shown in Figure 11.2. In this table, take the elements in the main diagonal,
and complement each entry, that is, replace 0 with 1, and vice versa. In
the example in Figure 11.2, the elements are 1100..., so we get 0011... as
t1 
1 
0 
0 
0 
0 
.  .  .
t2 
1 
1 
0 
0 
0 
.  .  .
t3 
1 
1 
0 
1 
0 
.  .  .
t4 
1 
1 
0 
0 
1 
.  .  .
 
 
 
 
    .
. 
 
 
 
      . .  
 
 
   
. 
 
.
FIGURE 11.2

11.1 Recursive and Recursively Enumerable Languages
289
the result. The new sequence along the diagonal represents some element
of 2S, say ti for some i.
But it cannot be t1 because it diﬀers from t1
through s1. For the same reason it cannot be t2, t3, or any other entry in
the enumeration. This contradiction creates a logical impasse that can be
removed only by throwing out the assumption that 2S is countable.
This kind of argument, because it involves a manipulation of the di-
agonal elements of a table, is called diagonalization. The technique is
attributed to the mathematician G. F. Cantor, who used it to demonstrate
that the set of real numbers is not countable. In the next few chapters, we
will see a similar argument in several contexts. Theorem 11.1 is diagonal-
ization in its purest form.
As an immediate consequence of this result, we can show that, in some
sense, there are fewer Turing machines than there are languages, so that
there must be some languages that are not recursively enumerable.
THEOREM 11.2
For any nonempty Σ, there exist languages that are not recursively enumer-
able.
Proof: A language is a subset of Σ∗, and every such subset is a language.
Therefore, the set of all languages is exactly 2Σ∗.
Since Σ∗is inﬁnite,
Theorem 11.1 tells us that the set of all languages on Σ is not countable.
But the set of all Turing machines can be enumerated, so the set of all
recursively enumerable languages is countable. By Exercise 16 at the end
of this section, this implies that there must be some languages on Σ that
are not recursively enumerable.
This proof, although short and simple, is in many ways unsatisfying.
It is completely nonconstructive and, while it tells us of the existence of
some languages that are not recursively enumerable, it gives us no feeling
at all for what these languages might look like. In the next set of results,
we investigate the conclusion more explicitly.
A Language That Is Not Recursively Enumerable
Since every language that can be described in a direct algorithmic fashion
can be accepted by a Turing machine and hence is recursively enumerable,
the description of a language that is not recursively enumerable must be
indirect. Nevertheless, it is possible. The argument involves a variation on
the diagonalization theme.

290
Chapter 11 A Hierarchy of Formal Languages and Automata
THEOREM 11.3
There exists a recursively enumerable language whose complement is not
recursively enumerable.
Proof: Let Σ = {a}, and consider the set of all Turing machines with this
input alphabet. By Theorem 10.3, this set is countable, so we can associate
an order M1, M2, ... with its elements. For each Turing machine Mi, there
is an associated recursively enumerable language L (Mi). Conversely, for
each recursively enumerable language on Σ, there is some Turing machine
that accepts it.
We now consider a new language L deﬁned as follows. For each i ≥1,
the string ai is in L if and only if ai ∈L (Mi). It is clear that the language
L is well deﬁned, since the statement ai ∈L (Mi), and hence ai ∈L, must
be either true or false. Next, we consider the complement of L,
L =

ai : ai /∈L (Mi)

,
(11.1)
which is also well deﬁned but, as we will show, is not recursively enumerable.
We will show this by contradiction, starting from the assumption that
L is recursively enumerable. If this is so, then there must be some Turing
machine, say Mk, such that
L = L (Mk) .
(11.2)
Consider the string ak. Is it in L or in L? Suppose that ak ∈L. By (11.2)
this implies that
ak ∈L (Mk) .
But (11.1) now implies that
ak /∈L.
Alternatively, if we assume that ak is in L, then ak /∈L and (11.2) implies
that
ak /∈L (Mk) .
But then from (11.1) we get that
ak ∈L.
The contradiction is inescapable, and we must conclude that our assumption
that L is recursively enumerable is false.
To complete the proof of the theorem as stated, we must still show that
L is recursively enumerable. For this we can use the known enumeration
procedure for Turing machines. Given ai, we ﬁrst ﬁnd i by counting the

11.1 Recursive and Recursively Enumerable Languages
291
number of a's. We then use the enumeration procedure for Turing machines
to ﬁnd Mi. Finally, we give its description along with ai to a universal
Turing machine Mu that simulates the action of M on ai. If ai is in L, the
computation carried out by Mu will eventually halt. The combined eﬀect of
this is a Turing machine that accepts every ai ∈L. Therefore, by Deﬁnition
11.1, L is recursively enumerable.
The proof of this theorem explicitly exhibits, through (11.1), a well-
deﬁned language that is not recursively enumerable.
This is not to say
that there is an easy, intuitive interpretation of L; it would be diﬃcult to
exhibit more than a few trivial members of this language. Nevertheless, L
is properly deﬁned.
A Language That Is Recursively Enumerable but Not
Recursive
Next, we show there are some languages that are recursively enumerable
but not recursive. Again, we need do so in a rather roundabout way. We
begin by establishing a subsidiary result.
THEOREM 11.4
If a language L and its complement L are both recursively enumerable, then
both languages are recursive. If L is recursive, then L is also recursive, and
consequently both are recursively enumerable.
Proof: If L and L are both recursively enumerable, then there exist Turing
machines M and 
M that serve as enumeration procedures for L and L,
respectively. The ﬁrst will produce w1, w2, ... in L, the second w1, w2, ... in
L. Suppose now we are given any w ∈Σ+. We ﬁrst let M generate w1 and
compare it with w. If they are not the same, we let 
M generate w1 and
compare again. If we need to continue, we next let M generate w2, then

M generate w2, and so on. Any w ∈Σ+ will be generated by either M or

M, so eventually we will get a match. If the matching string is produced
by M, w belongs to L, otherwise it is in L. The process is a membership
algorithm for both L and L, so they are both recursive.
For the converse, assume that L is recursive. Then there exists a mem-
bership algorithm for it. But this becomes a membership algorithm for L by
simply complementing its conclusion. Therefore, L is recursive. Since any
recursive language is recursively enumerable, the proof is completed.
From this, we conclude directly that the family of recursively enumer-
able languages and the family of recursive languages are not identical. The
language L in Theorem 11.3 is in the ﬁrst but not in the second family.

292
Chapter 11 A Hierarchy of Formal Languages and Automata
THEOREM 11.5
There exists a recursively enumerable language that is not recursive; that
is, the family of recursive languages is a proper subset of the family of
recursively enumerable languages.
Proof: Consider the language L of Theorem 11.3. This language is recur-
sively enumerable, but its complement is not. Therefore, by Theorem 11.4,
it is not recursive, giving us the looked-for example.
We see from this that there are indeed well-deﬁned languages for which
one cannot construct a membership algorithm.
EXERCISES
1. Prove that the set of all real numbers is not countable.
2. Prove that the set of all languages that are not recursively enumerable is not
countable.
3. Let L be a ﬁnite language. Show that then L+ is recursively enumerable.
Suggest an enumeration procedure for L+.
4. Let L be a context-free language. Show that L+ is recursively enumerable
and suggest an enumeration procedure for it.
5. Show that if a language is not recursively enumerable, its complement cannot
be recursive.
6. Show that the family of recursively enumerable languages is closed under
union.
7. Is the family of recursively enumerable languages closed under intersection?
8. Show that the family of recursive languages is closed under union and inter-
section.
9. Show that the families of recursively enumerable and recursive languages are
closed under reversal.
10. Is the family of recursive languages closed under concatenation?
11. Prove that the complement of a context-free language must be recursive.
12. Let L1 be recursive and L2 recursively enumerable. Show that L2 −L1 is
necessarily recursively enumerable.
13. Suppose that L is such that there exists a Turing machine that enumerates
the elements of L in proper order. Show that this means that L is recursive.
14. If L is recursive, is it necessarily true that L+ is also recursive?
15. Choose a particular encoding for Turing machines, and with it, ﬁnd one
element of the language L in Theorem 11.3.

11.2 Unrestricted Grammars
293
16. Let S1 be a countable set, S2 a set that is not countable, and S1 ⊂S2. Show
that S2 must then contain an inﬁnite number of elements that are not in S1.
17. In Exercise 16, show that in fact S2 −S1 cannot be countable.
18. Why does the argument in Theorem 11.1 fail when S is ﬁnite?
19. Show that the set of all irrational numbers is not countable.
11.2
UNRESTRICTED GRAMMARS
To investigate the connection between recursively enumerable languages
and grammars, we return to the general deﬁnition of a grammar in Chapter
1. In Deﬁnition 1.1 the production rules were allowed to take any form,
but various restrictions were later made to get speciﬁc grammar types. If
we take the general form and impose no restrictions, we get unrestricted
grammars.
DEFINITION 11.3
A grammar G = (V, T, S, P) is called unrestricted if all the productions
are of the form
u →v,
where u is in (V ∪T)+ and v is in (V ∪T)∗.
In an unrestricted grammar, essentially no conditions are imposed on
the productions. Any number of variables and terminals can be on the left
or right, and these can occur in any order. There is only one restriction: λ
is not allowed as the left side of a production.
As we will see, unrestricted grammars are much more powerful than
restricted forms like the regular and context-free grammars we have studied
so far.
In fact, unrestricted grammars correspond to the largest family
of languages so we can hope to recognize by mechanical means; that is,
unrestricted grammars generate exactly the family of recursively enumerable
languages. We show this in two parts; the ﬁrst is quite straightforward, but
the second involves a lengthy construction.
THEOREM 11.6
Any language generated by an unrestricted grammar is recursively enumer-
able.

294
Chapter 11 A Hierarchy of Formal Languages and Automata
Proof: The grammar in eﬀect deﬁnes a procedure for enumerating all strings
in the language systematically. For example, we can list all w in L such that
S ⇒w,
that is, w is derived in one step. Since the set of the productions of the
grammar is ﬁnite, there will be a ﬁnite number of such strings. Next, we
list all w in L that can be derived in two steps
S ⇒x ⇒w,
and so on. We can simulate these derivations on a Turing machine and,
therefore, have an enumeration procedure for the language.
Hence it is
recursively enumerable.
This part of the correspondence between recursively enumerable lan-
guages and unrestricted grammars is not surprising. The grammar gener-
ates strings by a well-deﬁned algorithmic process, so the derivations can
be done on a Turing machine. To show the converse, we describe how any
Turing machine can be mimicked by an unrestricted grammar.
We are given a Turing machine M = (Q, Σ, Γ, δ, q0, □, F) and want to
produce a grammar G such that L (G) = L (M). The idea behind the con-
struction is relatively simple, but its implementation becomes notationally
cumbersome.
Since the computation of the Turing machine can be described by the
sequence of instantaneous descriptions
q0w
∗
⊢xqfy,
(11.3)
we will try to arrange it so that the corresponding grammar has the property
that
q0w
∗⇒xqfy
(11.4)
if and only if (11.3) holds. This is not hard to do; what is more diﬃcult to
see is how to make the connection between (11.4) and what we really want,
namely,
S
∗⇒w
for all w satisfying (11.3). To achieve this, we construct a grammar that,
in broad outline, has the following properties:
1. S can derive q0w for all w ∈Σ+.
2. (11.4) is possible if and only if (11.3) holds.

11.2 Unrestricted Grammars
295
3. When a string xqfy with qf ∈F is generated, the grammar transforms
this string into the original w.
The complete sequence of derivations is then
S
∗⇒q0w
∗⇒xqfy
∗⇒w.
(11.5)
The third step in the above derivation is the troublesome one. How can the
grammar remember w if it is modiﬁed during the second step? We solve this
by encoding strings so that the coded version originally has two copies of
w. The ﬁrst is saved, while the second is used in the steps in (11.4). When
a ﬁnal conﬁguration is entered, the grammar erases everything except the
saved w.
To produce two copies of w and to handle the state symbol of M (which
eventually has to be removed by the grammar), we introduce variables Vab
and Vaib for all a ∈Σ ∪{□}, b ∈Γ, and all i such that qi ∈Q. The variable
Vab encodes the two symbols a and b, while Vaib encodes a and b as well as
the state qi.
The ﬁrst step in (11.5) can be achieved (in the encoded form) by
S →V□□S |SV□□| T,
(11.6)
T →TVaa|Va0a,
(11.7)
for all a ∈Σ. These productions allow the grammar to generate an encoded
version of any string q0w with an arbitrary number of leading and trailing
blanks.
For the second step, for each transition
δ (qi, c) = (qj, d, R)
of M, we put into the grammar productions
VaicVpq →VadVpjq,
(11.8)
for all a, p ∈Σ ∪{□}, q ∈Γ. For each
δ (qi, c) = (qj, d, L)
of M, we include in G
VpqVaic →VpjqVad,
(11.9)
for all a, p ∈Σ ∪{□}, q ∈Γ.
If in the second step, M enters a ﬁnal state, the grammar must then
get rid of everything except w, which is saved in the ﬁrst indices of the V 's.
Therefore, for every qj ∈F, we include productions
Vajb →a,
(11.10)

296
Chapter 11 A Hierarchy of Formal Languages and Automata
for all a ∈Σ ∪{□}, b ∈Γ. This creates the ﬁrst terminal in the string,
which then causes a rewriting in the rest by
cVab →ca,
(11.11)
Vabc →ac,
(11.12)
for all a, c ∈Σ ∪{□}, b ∈Γ. We need one more special production
□→λ.
(11.13)
This last production takes care of the case when M moves outside that part
of the tape occupied by the input w. To make things work in this case, we
must ﬁrst use (11.6) and (11.7) to generate
□. . . □q0w□. . . □,
representing all the tape region used. The extraneous blanks are removed
at the end by (11.13).
The following example illustrates this complicated construction. Care-
fully check each step in the example to see what the various productions do
and why they are needed.
EXAMPLE 11.1
Let M = (Q, Σ, Γ, δ, q0, □, F) be a Turing machine with
Q = {q0, q1} ,
Γ = {a, b, □} ,
Σ = {a, b} ,
F = {q1} ,
and
δ (q0, a) = (q0, a, R) ,
δ (q0, □) = (q1, □, L) .
This machine accepts L (aa∗).
Consider now the computation
q0aa ⊢aq0a ⊢aaq0□⊢aq1a□,
(11.14)
which accepts the string aa.
To derive this string with G, we ﬁrst
use rules of the form (11.6) and (11.7) to get the appropriate starting
string,
S ⇒SV□□⇒TV□□⇒TVaaV□□⇒Va0aVaaV□□.

11.2 Unrestricted Grammars
297
The last sentential form is the starting point for the part of the deriva-
tion that mimics the computation of the Turing machine. It contains
the original input aa□in the sequence of ﬁrst indices and the initial
instantaneous description q0aa□in the remaining indices. Next, we
apply
Va0aVaa →VaaVa0a,
and
Va0aV□□→VaaV□0□,
which are speciﬁc instances of (11.8), and
VaaV□0□→Va1aV□□
coming from (11.9). Then the next steps in the derivation are
Va0aVaaV□□⇒VaaVa0aV□□⇒VaaVaaV□0□⇒VaaVa1aV□□.
The sequence of ﬁrst indices remains the same, always remembering
the initial input. The sequence of the other indices is
0aa□, a0a□, aa0□, a1a□,
which is equivalent to the sequence of instantaneous descriptions in
(11.14).
Finally, (11.10) to (11.13) are used in the last steps
VaaVa1aV□□⇒VaaaV□□⇒Vaaa□⇒aa□⇒aa.
The construction described in (11.6) to (11.13) is the basis of the proof
of the following result.
THEOREM 11.7
For every recursively enumerable language L, there exists an unrestricted
grammar G, such that L = L (G).
Proof: The construction described guarantees that
x ⊢y,
then
e (x) ⇒e (y) ,

298
Chapter 11 A Hierarchy of Formal Languages and Automata
where e (x) denotes the encoding of a string according to the given conven-
tion. By an induction on the number of steps, we can then show that
e (q0w)
∗⇒e (y)
if and only if
q0w
∗
⊢y.
We also must show that we can generate every possible starting conﬁg-
uration and that w is properly reconstructed if and only if M enters a
ﬁnal conﬁguration. The details, which are not too diﬃcult, are left as an
exercise.
These two theorems establish what we set out to do. They show that
the family of languages associated with unrestricted grammars is identical
with the family of recursively enumerable languages.
EXERCISES
1. What language does the unrestricted grammar
S →S1B,
S1 →aS1b,
bB →bbbB,
aS1b →aa,
B →λ
derive?
2. Construct a Turing machine for L (01 (01)∗), then ﬁnd an unrestricted gram-
mar for it using the construction in Theorem 11.7. Give a derivation for 0101
using the resulting grammar.
3. Use the construction in Theorem 11.7 to ﬁnd unrestricted grammars for the
following languages:
(a) L = L(aaa∗bb∗}.
(b) L = L{(ab)∗b∗}.
(c) L = {w : na(w) = nb(w)}.
4. Consider a variation on grammars in which the starting point for any deriva-
tion can be a ﬁnite set of strings, rather than a single variable. Formalize

11.3 Context-Sensitive Grammars and Languages
299
this concept, then investigate how such grammars relate to the unrestricted
grammars we have used here.
5. In Example 11.1, prove that the constructed grammar cannot generate any
sentence with a b in it.
6. Give the details of the proof of Theorem 11.7.
7. Show that for every unrestricted grammar there exists an equivalent unre-
stricted grammar, all of whose productions have the form
u →v,
with u, v ∈(V ∪T)+ and |u| ≤|v|, or
A →λ,
with A ∈V .
8. Show that the conclusion of Exercise 7 still holds if we add the further con-
ditions |u| ≤2 and |v| ≤2.
9. Some authors give a deﬁnition of unrestricted grammars that is not quite the
same as our Deﬁnition 11.3. In this alternate deﬁnition, the productions of
an unrestricted grammar are required to be of the form
x →y,
where
x ∈(V ∪T)∗V (V ∪T)∗,
and
y ∈(V ∪T)∗.
The diﬀerence is that here the left side must have at least one variable.
Show that this alternate deﬁnition is basically the same as the one we use, in
the sense that for every grammar of one type, there is an equivalent grammar
of the other type.
11.3
CONTEXT-SENSITIVE GRAMMARS AND
LANGUAGES
Between the restricted, context-free grammars and the general, unrestricted
grammars, a great variety of "somewhat restricted" grammars can be de-
ﬁned. Not all cases yield interesting results; among the ones that do, the
context-sensitive grammars have received considerable attention.
These
grammars generate languages associated with a restricted class of Turing
machines, linear bounded automata, which we introduced in Section 10.5.

300
Chapter 11 A Hierarchy of Formal Languages and Automata
DEFINITION 11.4
A grammar G = (V, T, S, P) is said to be context sensitive if all produc-
tions are of the form
x →y,
where x, y ∈(V ∪T)+ and
|x| ≤|y| .
(11.15)
This deﬁnition shows clearly one aspect of this type of grammar; it
is noncontracting, in the sense that the length of successive sentential
forms can never decrease. It is less obvious why such grammars should be
called context sensitive, but it can be shown (see, for example, Salomaa
1973) that all such grammars can be rewritten in a normal form in which
all productions are of the form
xAy →xvy.
This is equivalent to saying that the production
A →v
can be applied only in the situation where A occurs in a context of the string
x on the left and the string y on the right. While we use the terminology
arising from this particular interpretation, the form itself is of little interest
to us here, and we will rely entirely on Deﬁnition 11.4.
Context-Sensitive Languages and Linear Bounded
Automata
As the terminology suggests, context-sensitive grammars are associated with
a language family with the same name.
DEFINITION 11.5
A language L is said to be context sensitive if there exists a context-sensitive
grammar G, such that L = L (G) or L = L (G) ∪{λ}.
In this deﬁnition, we reintroduce the empty string.
Deﬁnition 11.4
implies that x →λ is not allowed, so that a context-sensitive grammar
can never generate a language containing the empty string.
Yet, every

11.3 Context-Sensitive Grammars and Languages
301
context-free language without λ can be generated by a special case of a
context-sensitive grammar, say by one in Chomsky or Greibach normal
form, both of which satisfy the conditions of Deﬁnition 11.4. By including
the empty string in the deﬁnition of a context-sensitive language (but not
in the grammar), we can claim that the family of context-free languages is
a subset of the family of context-sensitive languages.
EXAMPLE 11.2
The language L = {anbncn : n ≥1} is a context-sensitive language.
We show this by exhibiting a context-sensitive grammar for the
language. One such grammar is
S →abc|aAbc,
Ab →bA,
Ac →Bbcc,
bB →Bb,
aB →aa|aaA.
We can see how this works by looking at a derivation of a3b3c3.
S ⇒aAbc ⇒abAc ⇒abBbcc
⇒aBbbcc ⇒aaAbbcc ⇒aabAbcc
⇒aabbAcc ⇒aabbBbccc
⇒aabBbbccc ⇒aaBbbbccc
⇒aaabbbccc.
The solution eﬀectively uses the variables A and B as messengers. An
A is created on the left, travels to the right to the ﬁrst c, where it
creates another b and c. It then sends the messenger B back to the left
in order to create the corresponding a. The process is very similar to
the way one might program a Turing machine to accept the language L.
Since the language in the previous example is not context free, we see
that the family of context-free languages is a proper subset of the family
of context-sensitive languages. Example 11.2 also shows that it is not an
easy matter to ﬁnd a context-sensitive grammar even for relatively simple
examples. Often the solution is most easily obtained by starting with a
Turing machine program, then ﬁnding an equivalent grammar for it.
A
few examples will show that, whenever the language is context sensitive,
the corresponding Turing machine has predictable space requirements; in
particular, it can be viewed as a linear bounded automaton.

302
Chapter 11 A Hierarchy of Formal Languages and Automata
THEOREM 11.8
For every context-sensitive language L not including λ, there exists some
linear bounded automaton M such that L = L (M).
Proof: If L is context sensitive, then there exists a context-sensitive gram-
mar for L−{λ}. We show that derivations in this grammar can be simulated
by a linear bounded automaton. The linear bounded automaton will have
two tracks, one containing the input string w, the other containing the sen-
tential forms derived using G.
A key point of this argument is that no
possible sentential form can have length greater than |w|. Another point to
notice is that a linear bounded automaton is, by deﬁnition, nondeterminis-
tic. This is necessary in the argument, since we can claim that the correct
production can always be guessed and that no unproductive alternatives
have to be pursued. Therefore, the computation described in Theorem 11.6
can be carried out without using space except that originally occupied by
w; that is, it can be done by a linear bounded automaton.
THEOREM 11.9
If a language L is accepted by some linear bounded automaton M, then
there exists a context-sensitive grammar that generates L.
Proof:
The construction here is similar to that in Theorem 11.7.
All
productions generated in Theorem 11.7 are noncontracting except (11.13),
□→λ.
But this production can be omitted. It is necessary only when the Turing
machine moves outside the bounds of the original input, which is not the case
here. The grammar obtained by the construction without this unnecessary
production is noncontracting, completing the argument.
Relation Between Recursive and Context-Sensitive
Languages
Theorem 11.9 tells us that every context-sensitive language is accepted by
some Turing machine and is therefore recursively enumerable.
Theorem
11.10 follows easily from this.
THEOREM 11.10
Every context-sensitive language L is recursive.

11.3 Context-Sensitive Grammars and Languages
303
Proof: Consider the context-sensitive language L with an associated con-
text-sensitive grammar G, and look at a derivation of w
S ⇒x1 ⇒x2 ⇒· · · ⇒xn ⇒w.
We can assume without any loss of generality that all sentential forms in a
single derivation are diﬀerent; that is, xi ̸= xj for all i ̸= j. The crux of
our argument is that the number of steps in any derivation is a bounded
function of |w|. We know that
|xj| ≤|xj+1| ,
because G is noncontracting. The only thing we need to add is that there
exist some m, depending only on G and w, such that
|xj| < |xj+m| ,
for all j, with m = m (|w|) a bounded function of |V ∪T| and |w|. This
follows because the ﬁniteness of |V ∪T| implies that there are only a ﬁnite
number of strings of a given length. Therefore, the length of a derivation of
w ∈L is at most |w| m (|w|).
This observation gives us immediately a membership algorithm for L.
We check all derivations of length up to |w| m (|w|). Since the set of pro-
ductions of G is ﬁnite, there are only a ﬁnite number of these. If any of
them give w, then w ∈L, otherwise it is not.
THEOREM 11.11
There exists a recursive language that is not context sensitive.
Proof:
Consider the set of all context-sensitive grammars on T = {a, b}.
We can use a convention in which each grammar has a variable set of the
form
V = {V0, V1, V2, ...} .
Every context-sensitive grammar is completely speciﬁed by its productions;
we can think of them as written as a single string
x1 →y1; x2 →y2; . . . ; xm →ym.
To this string we now apply the homomorphism
h (a) = 010,
h (b) = 0120,
h (→) = 0130,
h (; ) = 0140,
h (Vi) = 01i+50.

304
Chapter 11 A Hierarchy of Formal Languages and Automata
Thus, any context-sensitive grammar can be represented uniquely by a
string from L

(011∗0)∗
. Furthermore, the representation is invertible in
the sense that, given any such string, there is at most one context-sensitive
grammar corresponding to it.
Let us introduce a proper ordering on {0, 1}+, so we can write strings in
the order w1, w2, etc. A given string wj may not deﬁne a context-sensitive
grammar; if it does, call the grammar Gj. Next, we deﬁne a language L by
L = {wi : wi deﬁnes a context-sensitive grammar Gi and wi /∈L (Gi)} .
L is well deﬁned and is in fact recursive. To see this, we construct a member-
ship algorithm. Given wi, we check it to see if it deﬁnes a context-sensitive
grammar Gi. If not, then wi /∈L. If the string does deﬁne a grammar, then
L (Gi) is recursive, and we can use the membership algorithm of Theorem
11.10 to ﬁnd out if wi ∈L (Gi). If it is not, then wi belongs to L.
But L is not context sensitive. If it were, there would exist some wj
such that L = L (Gj). We can then ask if wj is in L (Gj). If we assume
that wj ∈L (Gj), then by deﬁnition wj is not in L. But L = L (Gj), so we
have a contradiction. Conversely, if we assume that wj /∈L (Gj), then by
deﬁnition wj ∈L and we have another contradiction. We must therefore
conclude that L is not context sensitive.
The result in Theorem 11.11 indicates that linear bounded automata
are indeed less powerful than Turing machines, since they accept only a
proper subset of the recursive languages.
It follows from the same re-
sult that linear bounded automata are more powerful than pushdown au-
tomata. Context-free languages, being generated by context-free grammars,
are a subset of the context-sensitive languages. As various examples show,
they are a proper subset.
Because of the essential equivalence of linear
bounded automata and context-sensitive languages on one hand, and push-
down automata and context-free languages on the other, we see that any
language accepted by a pushdown automaton is also accepted by some lin-
ear bounded automaton, but that there are languages accepted by some
linear bounded automata for which there are no pushdown automata.
EXERCISES
1. Find a context-sensitive grammar that is equivalent to the unrestricted gram-
mar in Exercise 1, Section 11.2.
2. Find context-sensitive grammars for the following languages:
(a) L =

an+1bncn−1 : n ≥1

.

11.4 The Chomsky Hierarchy
305
(b) L =

anbna2n : n ≥1

.
(c) L = {anbmcndm : n ≥1, m ≥1}.
(d) L =

ww : w ∈{a, b}+
.
(e) L = {anbncndn : n ≥1, }.
3. Find context-sensitive grammars for the following languages:
(a) L = {w : na (w) = nb (w) = nc (w)}.
(b) L = {w : na (w) = nb (w) < nc (w)}.
4. Show that the family of context-sensitive languages is closed under union.
5. Show that the family of context-sensitive languages is closed under reversal.
6. For m in Theorem 11.10, give explicit bounds for m as a function of |w| and
|V ∪T|.
7. Without explicitly constructing it, show that there exists a context-sensitive
grammar for the language L =

wuwR : w, u ∈{a, b}+ , |w| ≥|u|

.
11.4
THE CHOMSKY HIERARCHY
We have now encountered a number of language families, among them the
recursively enumerable languages (LRE), the context-sensitive languages
(LCS), the context-free languages (LCF ), and the regular languages (LREG).
One way of exhibiting the relationship between these families is by the
Chomsky hierarchy. Noam Chomsky, a founder of formal language the-
ory, provided an initial classiﬁcation into four language types, type 0 to
type 3. This original terminology has persisted and one ﬁnds frequent ref-
erences to it, but the numeric types are actually diﬀerent names for the
language families we have studied. Type 0 languages are those generated
by unrestricted grammars, that is, the recursively enumerable languages.
Type 1 consists of the context-sensitive languages, type 2 consists of the
context-free languages, and type 3 consists of the regular languages. As we
have seen, each language family of type i is a proper subset of the family of
type i−1. A diagram (Figure 11.3) exhibits the relationship clearly. Figure
11.3 shows the original Chomsky hierarchy. We have also met several other
language families that can be ﬁtted into this picture. Including the fami-
lies of deterministic context-free languages (LDCF ) and recursive languages
(LREC), we arrive at the extended hierarchy shown in Figure 11.4.
Other language families can be deﬁned and their place in Figure 11.4
studied, although their relationships do not always have the neatly nested
structure of Figures 11.3 and 11.4. In some instances, the relationships are
not completely understood.

306
Chapter 11 A Hierarchy of Formal Languages and Automata
LREG
LCF
LCS
LRE
FIGURE 11.3
LREG
LRE
LREC
LDCF
LCF
LCS
FIGURE 11.4
EXAMPLE 11.3
We have previously introduced the context-free language
L = {w : na (w) = nb (w)}
and shown that it is deterministic, but not linear. On the other hand,
the language
L = {anbn} ∪

anb2n
is linear, but not deterministic. This indicates that the relationship be-
tween regular, linear, deterministic context-free, and nondeterministic
context-free languages is as shown in Figure 11.5.

11.4 The Chomsky Hierarchy
307
LREG
LLIN
LDCF
LCF
FIGURE 11.5
There is still an unresolved issue.
We introduced the concept of a
deterministic linear bounded automaton in Exercise 8, Section 10.5. We
can now ask the question we asked in connection with other automata:
What role does nondeterminism play here? Unfortunately, there is no easy
answer.
At this time, it is not known whether the family of languages
accepted by deterministic linear bounded automata is a proper subset of
the context-sensitive languages.
To summarize, we have explored the relationships between several lan-
guage families and their associated automata. In doing so, we established
a hierarchy of languages and classiﬁed automata by their power as lan-
guage accepters. Turing machines are more powerful than linear bounded
automata. These in turn are more powerful than pushdown automata. At
the bottom of the hierarchy are ﬁnite accepters, with which we began our
study.
EXERCISES
1. Collect examples given in this book that demonstrate that all the subset
relations depicted in Figure 11.4 are indeed proper ones.
2. Find two examples (excluding the one in Example 11.3) of languages that are
linear but not deterministic context free.
3. Find two examples (excluding the one in Example 11.3) of languages that are
deterministic context free but not linear.


12
CHA P T E R
LIMITS OF ALGORITHMIC
COMPUTATION
CHAPTER SUMMARY
We have claimed that Turing machines can do anything that comput-
ers can do. If we can find a problem that cannot be done by a Turing
machine, we have also identified a problem that is not in the power of
even the most powerful computer. As we will see, there are many such
problems, but instead of dealing with all of them in detail, we find one
case to which all the others can be reduced. This is the halting prob-
lem. While this is a highly abstract issue, several practically important
consequences follow.
H
aving talked about what Turing machines can do, we now look at
what they cannot do. Although Turing's thesis leads us to believe
that there are few limitations to the power of a Turing machine,
we have claimed on several occasions that there could not exist any algo-
rithms for the solution of certain problems. Now we make more explicit
what we mean by this claim. Some of the results came about quite simply;
if a language is nonrecursive, then by deﬁnition there is no membership
algorithm for it. If this were all there was to this issue, it would not be
very interesting; nonrecursive languages have little practical value. But the
problem goes deeper. For example, we have stated (but not yet proved) that
there exists no algorithm to determine whether a context-free grammar is
309

310
Chapter 12 Limits of Algorithmic Computation
unambiguous. This question is clearly of practical signiﬁcance in the study
of programming languages.
We ﬁrst deﬁne the concepts of decidability and computability to
pin down what we mean when we say that something cannot be done by
a Turing machine. We then look at several classical problems of this type,
among them the well-known halting problem for Turing machines. From this
follow a number of related problems for Turing machines and recursively
enumerable languages. After this, we look at some questions relating to
context-free languages. Here we ﬁnd quite a few important problems for
which, unfortunately, there are no algorithms.
12.1
SOME PROBLEMS THAT CANNOT BE SOLVED BY
TURING MACHINES
The argument that the power of mechanical computations is limited is not
surprising. Intuitively we know that many vague and speculative questions
require special insight and reasoning well beyond the capacity of any com-
puter that we can now construct or even foresee. What is more interesting
to computer scientists is that there are questions that can be clearly and
simply stated, with an apparent possibility of an algorithmic solution, but
which are known to be unsolvable by any computer.
Computability and Decidability
In Deﬁnition 9.4, we stated that a function f on a certain domain is said
to be computable if there exists a Turing machine that computes the value
of f for all arguments in its domain. A function is uncomputable if no such
Turing machine exists. There may be a Turing machine that can compute
f on part of its domain, but we call the function computable only if there
is a Turing machine that computes the function on the whole of its domain.
We see from this that, when we classify a function as computable or not
computable, we must be clear on what its domain is.
Our concern here will be the somewhat simpliﬁed setting where the
result of a computation is a simple "yes" or "no." In this case, we talk
about a problem being decidable or undecidable. By a problem we will
understand a set of related statements, each of which must be either true or
false. For example, we consider the statement "For a context-free grammar
G, the language L (G) is ambiguous." For some G this is true, for others
it is false, but clearly we must have one or the other. The problem is to
decide whether the statement is true for any G we are given. Again, there is
an underlying domain, the set of all context-free grammars. We say that a
problem is decidable if there exists a Turing machine that gives the correct
answer for every statement in the domain of the problem.

12.1 Some Problems That Cannot Be Solved by Turing Machines
311
When we state decidability or undecidability results, we must always
know what the domain is, because this may aﬀect the conclusion. The prob-
lem may be decidable on some domain but not on another. Speciﬁcally, a
single instance of a problem is always decidable, since the answer is either
true or false. In the ﬁrst case, a Turing machine that always answers "true"
gives the correct answer, while in the second case one that always answers
"false" is appropriate. This may seem like a facetious answer, but it empha-
sizes an important point. The fact that we do not know what the correct
answer is makes no diﬀerence; what matters is that there exists some Turing
machine that does give the correct response.
The Turing Machine Halting Problem
We begin with some problems that have historical signiﬁcance and that at
the same time give us a starting point for developing later results. The best-
known of these is the Turing machine halting problem. Simply stated,
the problem is: Given the description of a Turing machine M and an input
w, does M, when started in the initial conﬁguration q0w, perform a compu-
tation that eventually halts? Using an abbreviated way of talking about the
problem, we ask whether M applied to w, or simply (M, w), halts or does
not halt. The domain of this problem is to be taken as the set of all Turing
machines and all w; that is, we are looking for a single Turing machine that,
given the description of an arbitrary M and w, will predict whether or not
the computation of M applied to w will halt.
We cannot ﬁnd the answer by simulating the action of M on w, say by
performing it on a universal Turing machine, because there is no limit on
the length of the computation. If M enters an inﬁnite loop, then no matter
how long we wait, we can never be sure that M is in fact in a loop. It may
simply be a case of a very long computation. What we need is an algorithm
that can determine the correct answer for any M and w by performing some
analysis on the machine's description and the input. But as we now show,
no such algorithm exists.
For subsequent discussion, it is convenient to have a precise idea of
what we mean by the halting problem; for this reason, we make a speciﬁc
deﬁnition of what we stated somewhat loosely above.
DEFINITION 12.1
Let wM be a string that describes a Turing machine M = (Q, Σ, Γ, δ, q0, □,
F), and let w be a string in M's alphabet. We will assume that wM and
w are encoded as a string of 0's and 1's, as suggested in Section 10.4. A
solution of the halting problem is a Turing machine H, which for any wM
and w performs the computation
q0wMw
∗
⊢x1qyx2

312
Chapter 12 Limits of Algorithmic Computation
if M applied to w halts, and
q0wMw
∗
⊢y1qny2
if M applied to w does not halt. Here qy and qn are both ﬁnal states of H.
THEOREM 12.1
There does not exist any Turing machine H that behaves as required by
Deﬁnition 12.1. The halting problem is therefore undecidable.
Proof:
We assume the contrary, namely, that there exists an algorithm,
and consequently some Turing machine H, that solves the halting problem.
The input to H will be the string wMw. The requirement is then that, given
any wMw, the Turing machine H will halt with either a yes or no answer.
We achieve this by asking that H halt in one of two corresponding ﬁnal
states, say, qy or qn. The situation can be visualized by a block diagram
like Figure 12.1.
The intent of this diagram is to indicate that, if H is
started in state q0 with input wMw, it will eventually halt in state qy or
qn. As required by Deﬁnition 12.1, we want H to operate according to the
following rules:
q0wMw
∗
⊢Hx1qyx2
if M applied to w halts, and
q0wMw
∗
⊢H y1qny2
if M applied to w does not halt.
Next, we modify H to produce a Turing machine H′ with the structure
shown in Figure 12.2. With the added states in Figure 12.2 we want to
wMw
q0
qy
qn
FIGURE 12.1

12.1 Some Problems That Cannot Be Solved by Turing Machines
313
wMw
q0
qn
qa
qb
qy
FIGURE 12.2
convey that the transitions between state qy and the new states qa and qb
are to be made, regardless of the tape symbol, in such a way that the tape
remains unchanged. The way this is done is straightforward. Comparing
H and H′ we see that, in situations where H reaches qy and halts, the
modiﬁed machine H′ will enter an inﬁnite loop. Formally, the action of H′
is described by
q0wMw
∗
⊢H′ ∞
if M applied to w halts, and
q0wMw
∗
⊢H′ y1qny2
if M applied to w does not halt.
From H′ we construct another Turing machine H. This new machine
takes as input wM and copies it, ending in its initial state q0. After that, it
behaves exactly like H′. Then the action of H is such that
q0wM
∗
⊢
H q0wMwM
∗
⊢
H ∞
if M applied to wM halts, and
q0wM
∗
⊢
H q0wMwM
∗
⊢
H y1qny2
if M applied to wM does not halt.
Now H is a Turing machine, so it has a description in {0, 1}∗, say, w.
This string, in addition to being the description of H, also can be used as
input string. We can therefore legitimately ask what would happen if H is
applied to w. From the above, identifying M with H, we get
q0 w
∗
⊢
H ∞
if H applied to w halts, and
q0 w
∗
⊢
H y1qny2

314
Chapter 12 Limits of Algorithmic Computation
if H applied to w does not halt. This is clearly nonsense. The contradiction
tells us that our assumption of the existence of H, and hence the assumption
of the decidability of the halting problem, must be false.
One may object to Deﬁnition 12.1, since we required that, to solve the
halting problem, H had to start and end in very speciﬁc conﬁgurations. It
is, however, not hard to see that these somewhat arbitrarily chosen con-
ditions play only a minor role in the argument, and that essentially the
same reasoning could be used with any other starting and ending conﬁgu-
rations. We have tied the problem to a speciﬁc deﬁnition for the sake of the
discussion, but this does not aﬀect the conclusion.
It is important to keep in mind what Theorem 12.1 says. It does not
preclude solving the halting problem for speciﬁc cases; often we can tell by
an analysis of M and w whether or not the Turing machine will halt. What
the theorem says is that this cannot always be done; there is no algorithm
that can make a correct decision for all wM and w.
The arguments for proving Theorem 12.1 were given because they are
classical and of historical interest. The conclusion of the theorem is actually
implied in previous results as the following argument shows.
THEOREM 12.2
If the halting problem were decidable, then every recursively enumerable
language would be recursive. Consequently, the halting problem is unde-
cidable.
Proof: To see this, let L be a recursively enumerable language on Σ, and let
M be a Turing machine that accepts L. Let H be the Turing machine that
solves the halting problem. We construct from this the following procedure:
1. Apply H to wMw. If H says "no," then by deﬁnition w is not in L.
2. If H says "yes," then apply M to w.
But M must halt, so it will
eventually tell us whether w is in L or not.
This constitutes a membership algorithm, making L recursive. But we
already know that there are recursively enumerable languages that are not
recursive. The contradiction implies that H cannot exist, that is, that the
halting problem is undecidable.
The simplicity with which the halting problem can be obtained from
Theorem 11.5 is a consequence of the fact that the halting problem and
the membership problem for recursively enumerable languages are nearly
identical.
The only diﬀerence is that in the halting problem we do not

12.1 Some Problems That Cannot Be Solved by Turing Machines
315
distinguish between halting in a ﬁnal and nonﬁnal state, whereas in the
membership problem we do. The proofs of Theorems 11.5 (via Theorem
11.3) and 12.1 are closely related, both being a version of diagonalization.
Reducing One Undecidable Problem to Another
The above argument, connecting the halting problem to the membership
problem, illustrates the very important technique of reduction. We say that
a problem A is reduced to a problem B if the decidability of A follows
from the decidability of B. Then, if we know that A is undecidable, we can
conclude that B is also undecidable. Let us do a few examples to illustrate
this idea.
EXAMPLE 12.1
The state-entry problem is as follows. Given any Turing machine
M = (Q, Σ, Γ, δ, q0, □, F) and any q ∈Q, w ∈Σ+, decide whether or
not the state q is ever entered when M is applied to w. This problem
is undecidable.
To reduce the halting problem to the state-entry problem, suppose
that we have an algorithm A that solves the state-entry problem. We
could then use it to solve the halting problem. For example, given any
M and w, we ﬁrst modify M to get 
M in such a way that 
M halts in
state q if and only if M halts. We can do this simply by looking at the
transition function δ of M. If M halts, it does so because some δ (qi, a)
is undeﬁned. To get 
M, we change every such undeﬁned δ to
δ (qi, a) = (q, a, R) ,
where q is a ﬁnal state.
We apply the state-entry algorithm A to


M, q, w

. If A answers yes, that is, the state q is entered, then (M, w)
halts. If A says no, then (M, w) does not halt.
Thus, the assumption that the state-entry problem is decidable
gives us an algorithm for the halting problem.
Because the halt-
ing problem is undecidable, the state-entry problem must also be
undecidable.
EXAMPLE 12.2
The blank-tape halting problem is another problem to which
the halting problem can be reduced.
Given a Turing machine M,

316
Chapter 12 Limits of Algorithmic Computation
determine whether or not M halts if started with a blank tape. This
isundecidable.
To show how this reduction is accomplished, assume that we are
given some M and some w. We ﬁrst construct from M a new machine
Mw that starts with a blank tape, writes w on it, then positions itself
in a conﬁguration q0w. After that, Mw acts like M. Clearly Mw will
halt on a blank tape if and only if M halts on w.
Suppose now that the blank-tape halting problem were decidable.
Given any (M, w), we ﬁrst construct Mw, then apply the blank-tape
halting problem algorithm to it. The conclusion tells us whether M
applied to w will halt. Since this can be done for any M and w, an
algorithm for the blank-tape halting problem can be converted into
an algorithm for the halting problem. Since the latter is known to be
undecidable, the same must be true for the blank-tape halting problem.
The construction in the arguments of these two examples illustrates an
approach common in establishing undecidability results. A block diagram
often helps us visualize the process. The construction in Example 12.2 is
summarized in Figure 12.3. In that diagram, we ﬁrst use an algorithm that
transforms (M, w) into Mw; such an algorithm clearly exists. Next, we use
the algorithm for solving the blank-tape halting problem, which we assume
exists. Putting the two together yields an algorithm for the halting problem.
But this is impossible, and we can conclude that A cannot exist.
A decision problem is eﬀectively a function with a range {0, 1}, that
is, a true or false answer. We can look also at more general functions to
see if they are computable; to do so, we follow the established method and
reduce the halting problem (or any other problem known to be undecidable)
to the problem of computing the function in question. Because of Turing's
thesis, we expect that functions encountered in practical circumstances will
be computable, so for examples of uncomputable functions we must look
a little further. Most examples of uncomputable functions are associated
with attempts to predict the behavior of Turing machines.
Generate
Mw
M,w
Blank-tape
halting
algorithm A
Mw
Halts
Does not 
halt
Halts
Does not halt
FIGURE 12.3 Algorithm for the halting problem.

12.1 Some Problems That Cannot Be Solved by Turing Machines
317
EXAMPLE 12.3
Let Γ = {0, 1, □}.
Consider the function f (n) whose value is the
maximum number of moves that can be made by any n-state Turing
machine that halts when started with a blank tape. This function, as
it turns out, is not computable.
Before we set out to demonstrate this, let us make sure that f (n)
is deﬁned for all n. Notice ﬁrst that there are only a ﬁnite number of
Turing machines with n states. This is because Q and Γ are ﬁnite, so
δ has a ﬁnite domain and range. This in turn implies that there are
only a ﬁnite number of diﬀerent δ's and therefore a ﬁnite number of
diﬀerent n-state Turing machines.
Of all of the n-state machines, there are some that always halt, for
example machines that have only ﬁnal states and therefore make no
moves. Some of the n-state machines will not halt when started with
a blank tape, but they do not enter the deﬁnition of f. Every machine
that does halt will execute a certain number of moves; of these, we take
the largest to give f (n).
Construct
M
M halts in
m steps.
M does not 
halt in m steps.
Halts
Does not halt
Mu
F
^
M
^
FIGURE 12.4
Algorithm for the blank-tape halting problem.
Take any Turing machine M and positive number m. It is easy to
modify M to produce 
M in such a way that the latter will always halt
with one of two answers: M applied to a blank tape halts in no more
than m moves, or M applied to a blank tape makes more than m moves.
All we have to do for this is to have M count its moves and terminate
when this count exceeds m. Assume now that f (n) is computable by
some Turing machine F. We can then put 
M and F together as shown
in Figure 12.4. First we compute f (|Q|), where Q is the state set of
M. This tells us the maximum number of moves that M can make if
it is to halt. The value we get is then used as m to construct 
M as

318
Chapter 12 Limits of Algorithmic Computation
outlined, and a description of 
M is given to a universal Turing machine
for execution. This tells us whether M applied to a blank tape halts
or does not halt in less than f (|Q|) steps. If we ﬁnd that M applied
to a blank tape makes more than f (|Q|) moves, then because of the
deﬁnition of f, the implication is that M never halts. Thus we have
a solution to the blank-tape halting problem. The impossibility of the
conclusion forces us to accept that f is not computable.
EXERCISES
1. Describe in detail how H in Theorem 12.1 can be modiﬁed to produce H′.
2. Show that the question "Does a Turing machine accept all even-length strings?"
is undecidable.
3. Show that the following problem is undecidable. Given any Turing machine
M, a ∈Γ, and w ∈Σ+, determine whether or not the symbol a is ever written
when M is applied to w.
4. In the general halting problem, we ask for an algorithm that gives the correct
answer for any M and w. We can relax this generality, for example, by looking
for an algorithm that works for all M but only a single w. We say that such a
problem is decidable if for every w there exists a (possibly diﬀerent) algorithm
that determines whether or not (M, w) halts. Show that even in this restricted
setting the problem is undecidable.
5. Show that there is no algorithm to decide whether or not an arbitrary Turing
machine halts on all input.
6. Consider the question: "Does a Turing machine in the course of a computa-
tion revisit the starting cell (i.e., the cell under the read-write head at the
beginning of the computation)?" Is this a decidable question?
7. Show that there is no algorithm for deciding if any two Turing machines M1
and M2 accept the same language.
8. How is the conclusion of Exercise 7 aﬀected if M2 is a ﬁnite automaton?
9. Is the halting problem solvable for deterministic pushdown automata; that
is, given a pda as in Deﬁnition 7.3, can we always predict whether or not the
automaton will halt on input w?
10. Let M be any Turing machine and x and y two possible instantaneous de-
scriptions of it. Show that the problem of determining whether or not
x
∗
⊢M y
is undecidable.

12.2 Undecidable Problems for Recursively Enumerable Languages
319
11. In Example 12.3, give the values of f (1) and f (2).
12. Show that the problem of determining whether a Turing machine halts on
any input is undecidable.
13. Let B be the set of all Turing machines that halt when started with a blank
tape. Show that this set is recursively enumerable, but not recursive.
14. Consider the set of all n-state Turing machines with tape alphabet Γ =
{0, 1, ⊔}. Give an expression for m (n), the number of distinct Turing ma-
chines with this Γ.
15. Let Γ = {0, 1, □} and let b (n) be the maximum number of tape cells examined
by any n-state Turing machine that halts when started with a blank tape.
Show that b (n) is not computable.
16. Determine whether or not the following statement is true: Any problem whose
domain is ﬁnite is decidable.
12.2
UNDECIDABLE PROBLEMS FOR RECURSIVELY
ENUMERABLE LANGUAGES
We have determined that there is no membership algorithm for recursively
enumerable languages. The lack of an algorithm to decide on some property
is not an exceptional state of aﬀairs for recursively enumerable languages,
but rather is the general rule. As we now show, there is little we can say
about these languages. Recursively enumerable languages are so general
that, in essence, any question we ask about them is undecidable. Invariably,
when we ask a question about recursively enumerable languages, we ﬁnd
that there is some way of reducing the halting problem to this question. We
give here some examples to show how this is done and from these examples
derive an indication of the general situation.
THEOREM 12.3
Let G be an unrestricted grammar.
Then the problem of determining
whether or not
L (G) = ∅
is undecidable.
Proof: We will reduce the membership problem for recursively enumerable
languages to this problem. Suppose we are given a Turing machine M and
some string w. We can modify M as follows. M ﬁrst saves its input on some
special part of its tape. Then, whenever it enters a ﬁnal state, it checks its

320
Chapter 12 Limits of Algorithmic Computation
Construct
Gw
M,w
Emptiness
algorithm A
Gw
L(Gw) not empty
L(Gw) empty
w    L(M)
w    L(M)
FIGURE 12.5 Membership algorithm.
saved input and accepts it if and only if it is w. We can do this by changing
δ in a simple way, creating for each w a machine Mw such that
L (Mw) = L (M) ∩{w} .
Using Theorem 11.7, we then construct a corresponding grammar Gw.
Clearly, the construction leading from M and w to Gw can always be done.
Equally clear is that L (Gw) is nonempty if and only if w ∈L (M).
Assume now that there exists an algorithm A for deciding whether or
not L (G) = ∅. If we let T denote an algorithm by which we generate Gw,
then we can put T and A together as shown in Figure 12.5. Figure 12.5
is a Turing machine that for any M and w tells us whether or not w is
in L (M). If such a Turing machine existed, we would have a membership
algorithm for any recursively enumerable language, in direct contradiction
to a previously established result. We conclude therefore that the stated
problem "L (G) = ∅" is not decidable.
THEOREM 12.4
Let M be any Turing machine. Then the question of whether or not L (M)
is ﬁnite is undecidable.
Proof:
Consider the halting problem (M, w). From M we construct an-
other Turing machine 
M that does the following. First, the halting states
of M are changed so that if any one is reached, all input is accepted by 
M.
This can be done by having any halting conﬁguration go to a ﬁnal state.
Second, the original machine is modiﬁed so that the new machine 
M ﬁrst
generates w on its tape, then performs the same computations as M, using
the newly created w and some otherwise unused space. In other words, the
moves made by 
M after it has written w on its tape are the same as would
have been made by M had it started in the original conﬁguration q0w. If
M halts in any conﬁguration, then 
M will halt in a ﬁnal state.

12.2 Undecidable Problems for Recursively Enumerable Languages
321
Finiteness
algorithm A
Halts
Does not halt
 M
^
Generate
 M
^
^
L(M) finite
L(M) not finite
M,w
^
FIGURE 12.6
Therefore, if (M, w) halts, 
M will reach a ﬁnal state for all input. If
(M, w) does not halt, then 
M will not halt either and so will accept nothing.
In other words, 
M accepts either the inﬁnite language Σ+ or the ﬁnite
language ∅.
If we now assume the existence of an algorithm A that tells us whether
or not L


M

is ﬁnite, we can construct a solution to the halting problem
as shown in Figure 12.6. Therefore, no algorithm for deciding whether or
not L (M) is ﬁnite can exist.
Notice that in the proof of Theorem 12.4, the speciﬁc nature of the
question asked, namely "Is L (M) ﬁnite?," is immaterial. We can change
the nature of the problem without signiﬁcantly aﬀecting the argument.
EXAMPLE 12.4
Show that for an arbitrary Turing machine M with Σ = {a, b}, the
problem "L (M) contains two diﬀerent strings of the same length" is
undecidable.
To show this, we use exactly the same approach as in Theorem
12.4, except that when 
M reaches a halting conﬁguration, it will be
modiﬁed to accept the two strings a and b. For this, the initial input
is saved and at the end of the computation compared with a and b,
accepting only these two strings. Thus, if (M, w) halts, 
M will accept
two strings of equal length, otherwise 
M will accept nothing. The rest
of the argument then proceeds as in Theorem 12.4.
In exactly the same manner, we can substitute other questions such as
"Does L (M) contain any string of length ﬁve?" or "Is L (M) regular?"
without aﬀecting the argument essentially. These questions, as well as simi-
lar questions, are all undecidable. A general result formalizing this is known
as Rice's theorem. This theorem states that any nontrivial property of a

322
Chapter 12 Limits of Algorithmic Computation
recursively enumerable language is undecidable. The adjective "nontrivial"
refers to a property possessed by some but not all recursively enumerable
languages. A precise statement and a proof of Rice's theorem can be found
in Hopcroft and Ullman (1979).
EXERCISES
1. Show in detail how the machine 
M in Theorem 12.4 is constructed.
2. Show that the question "Does a Turing machine accept any even-length
strings?" is undecidable.
3. Show that the two problems mentioned at the end of the preceding section,
namely
(a) L (M) contains any string of length ﬁve.
(b) L (M) is regular.
are undecidable.
4. Let M1 and M2 be arbitrary Turing machines.
Show that the problem
"L (M1) ⊆L (M2)" is undecidable.
5. Let G be any unrestricted grammar. Does there exist an algorithm for deter-
mining whether or not L (G)R is recursively enumerable?
6. Let G be any unrestricted grammar. Does there exist an algorithm for deter-
mining whether or not L (G) = L (G)R?
7. Let G1 be any unrestricted grammar, and G2 be any regular grammar. Show
that the problem
L (G1) ∩L (G2) = ⊘
is undecidable.
8. Let G1 and G2 be unrestricted grammars on Σ. Show that the problem
L(G1) ∪L(G2) = Σ∗
is undecidable.
9. Show that the question in Exercise 6 is undecidable for any ﬁxed G2, as long
as L (G2) is not empty.
10. For an unrestricted grammar G, show that the question "Is L (G) = L (G)∗?"
is undecidable. Argue (a) from Rice's theorem and (b) from ﬁrst principles.

12.3 The Post Correspondence Problem
323
12.3
THE POST CORRESPONDENCE PROBLEM
The undecidability of the halting problem has many consequences of practi-
cal interest, particularly in the area of context-free languages. But in many
instances it is cumbersome to work with the halting problem directly, and
it is convenient to establish some intermediate results that bridge the gap
between the halting problem and other problems. These intermediate re-
sults follow from the undecidability of the halting problem, but are more
closely related to the problems we want to study and therefore make the ar-
guments easier. One such intermediate result is the Post correspondence
problem.
The Post correspondence problem can be stated as follows. Given two
sequences of n strings on some alphabet Σ, say
A = w1, w2, ...wn
and
B = v1, v2, ..., vn,
we say that there exists a Post correspondence solution (PC-solution) for
pair (A, B) if there is a nonempty sequence of integers i, j, ..., k, such that
wiwj · · · wk = vivj · · · vk.
The Post correspondence problem is to devise an algorithm that will tell us,
for any (A, B), whether or not there exists a PC solution.
EXAMPLE 12.5
Let Σ = {0, 1} and take A and B as
w1 = 11, w2 = 100, w3 = 111,
v1 = 111, v2 = 001, v3 = 11.
For this case, there exists a PC solution as Figure 12.7 shows.
1
1
1
1
1
1
0
0
w1
w2
w3
v3
v2
v1
FIGURE 12.7
If we take
w1 = 00, w2 = 001, w3 = 1000,
v1 = 0, v2 = 11, v3 = 011,
there cannot be any PC solution simply because any string composed
of elements of A will be longer than the corresponding string from B.

324
Chapter 12 Limits of Algorithmic Computation
In speciﬁc instances we may be able to show by explicit construction
that a pair (A, B) permits a PC solution, or we may be able to argue, as
we did previously, that no such solution can exist. But in general, there is
no algorithm for deciding this question under all circumstances. The Post
correspondence problem is therefore undecidable.
To show this is a somewhat lengthy process. For the sake of clarity, we
break it into two parts. In the ﬁrst part, we introduce the modiﬁed Post
correspondence problem. We say that the pair (A, B) has a modiﬁed
Post correspondence solution (MPC solution) if there exists a sequence of
integers i, j, ..., k, such that
w1wiwj · · · wk = v1vivj · · · vk.
In the modiﬁed Post correspondence problem, the ﬁrst elements of the se-
quences A and B play a special role. An MPC solution must start with
w1 on the left side and with v1 on the right side. Note that if there exists
an MPC solution, then there is also a PC solution, but the converse is not
true.
The modiﬁed Post correspondence problem is to devise an algorithm for
deciding if an arbitrary pair (A, B) admits an MPC solution. This problem
is also undecidable. We will demonstrate the undecidability of the modiﬁed
Post correspondence problem by reducing a known undecidable problem,
the membership problem for recursively enumerable languages, to it. To
this end, we introduce the following construction. Suppose we are given an
F
F is a symbol not in V     T 
a
for every a     T
FS
a
Vi
Vi
E is a symbol not in V     T
E
wE
A
B


for every Vi    V
for every xi      yi  in P
yi
xi
FIGURE 12.8

12.3 The Post Correspondence Problem
325
unrestricted grammar G = (V, T, S, P) and a target string w. With these,
we create the pair (A, B) as shown in Figure 12.8. In Figure 12.8, the string
FS ⇒is to be taken as w1 and the string F as v1. The order of the rest of
the strings is immaterial.
We want to claim eventually that w ∈L (G) if and only if the sets A
and B constructed in this way have an MPC solution. Since this is perhaps
not immediately obvious, let us illustrate it with a simple example.
EXAMPLE 12.6
Let G = ({A, B, C} , {a, b, c, } , S, P) with productions
S →aABb|Bbb,
Bb →C,
AC →aac,
and take w = aaac. The sequences A and B obtained from the sug-
gested construction are given in Figure 12.9. The string w = aaac is
in L (G) and has a derivation
S ⇒aABb ⇒aAC ⇒aaac.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
FS
a
b
c
A
B
C
S
E
aABb
Bbb
C
aac
i
wi
vi
F
a
b
c
A
B
C
S
      aaacE
S
S
Bb
AC
FIGURE 12.9
How this derivation is paralleled by an MPC solution with the con-
structed sets can be seen in Figure 12.10, where the ﬁrst two steps in
the derivation are shown. The integers above and below the derivation
string show the indices for w and v, respectively, used to create the
string.

326
Chapter 12 Limits of Algorithmic Computation
a
A
C
F
S
w1
w10
a
A
B
b
v1
v10
F
S
w1
w10
a
A
B
b
v1
v10
v14
v2
v5
v12
w14
w2
w5
w12
FIGURE 12.10
c
E
a
F
S
w1
w10
a
A
B
b
v1
v10
w2
w2
w13
w9
w14
a
A
a
a
v14
v2
v13
v9
v14
v2
v5
v12
w14
w5
w12
C
FIGURE 12.11
Examine Figure 12.10 carefully to see what is happening. We want
to construct an MPC solution, so we must start with w1, that is, FS ⇒.
This string contains S, so to match it we have to use v10 or v11. In
this instance, we use v10; this brings in w10, leading us to the second
string in the partial derivation. Looking at several more steps, we see
that the string w1wiwj... is always longer than the corresponding string
v1vivj... and that the ﬁrst is exactly one step ahead in the derivation.
The only exception is the last step, where w9 must be applied to let
the v-string catch up. The complete MPC solution is shown in Figure
12.11. The construction, together with the example, indicates the lines
along which the next result is established.
THEOREM 12.5
Let G = (V, T, S, P) be any unrestricted grammar, with w any string in
T +. Let (A, B) be the correspondence pair constructed from G and w be
the process exhibited in Figure 12.8. Then the pair (A, B) permits an MPC
solution if and only if w ∈L (G).
Proof: The proof involves a formal inductive argument based on the out-
lined reasoning. We will omit the details.

12.3 The Post Correspondence Problem
327
With this result, we can reduce the membership problem for recursively
enumerable languages to the modiﬁed Post correspondence problem and
thereby demonstrate the undecidability of the latter.
THEOREM 12.6
The modiﬁed Post correspondence problem is undecidable.
Proof: Given any unrestricted grammar G = (V, T, S, P) and w ∈T +, we
construct the sets A and B as suggested above. By Theorem 12.5, the pair
(A, B) has an MPC solution if and only if w ∈L (G).
Suppose now we assume that the modiﬁed Post correspondence prob-
lem is decidable. We can then construct an algorithm for the membership
problem of G as sketched in Figure 12.12. An algorithm for constructing
A from B from G and w clearly exists, but a membership algorithm for
G and w does not. We must therefore conclude that there cannot be any
algorithm for deciding the modiﬁed Post correspondence problem.
With this preliminary work, we are now ready to prove the Post corre-
spondence problem in its original form.
THEOREM 12.7
The Post correspondence problem is undecidable.
Proof: We argue that if the Post correspondence problem were decidable,
the modiﬁed Post correspondence problem would be decidable.
Suppose we are given sequences A = w1, w2, ..., wn and B = v1, v2, ..., vn
on some alphabet Σ. We then introduce new symbols ♮and § and the new
sequences
C = y0, y1, ..., yn+1,
D = z0, z1, ..., zn+1,
Construct A 
and B as in
Figure 12.8.
G, w
MPC algorithm
A, B
MPC solution
     w    L(G)  
     w    L(G)  
No MPC solution
FIGURE 12.12 Membership algorithm.

328
Chapter 12 Limits of Algorithmic Computation
     MPC solution
    No MPC solution
Construct
C, D
A, B
PC algorithm
C, D
PC solution
No PC solution
FIGURE 12.13 MPC algorithm.
deﬁned as follows. For i = 1, 2, ..., n
yi = wi1♮wi2♮· · · wimi♮,
zi = ♮vi1♮vi2♮· · · viri,
where wij and vij denote the jth letter of wi and vi, respectively, and
mi = |wi| , ri = |vi|. In words, yi is created from wi by appending ♮to each
character, while zi is obtained by preﬁxing each character of vi with ♮. To
complete the deﬁnition of C and D, we take
y0 = ♮y1,
yn+1 = §,
z0 = z1,
zn+1 = ♮§.
Consider now the pair (C, D), and suppose it has a PC solution. Because
of the placement of ♮and §, such a solution must have y0 on the left and
yn+1 on the right and so must look like
♮w11♮w12 · · · ♮wj1♮· · · ♮wk1 · · · ♮§ = ♮v11♮v12 · · · ♮vj1♮· · · ♮vk1 · · · ♮§.
Ignoring the characters ♮and §, we see that this implies
w1wj · · · wk = v1vj · · · vk,
so that the pair (A, B) permits an MPC solution.
We can turn the argument around to show that if there is an MPC
solution for (A, B) then there is a PC solution for the pair (C, D).
Assume now that the Post correspondence problem is decidable. We
can then construct the machine shown in Figure 12.13. This machine clearly
decides the modiﬁed Post correspondence problem. But the modiﬁed Post
correspondence problem is undecidable; consequently, we cannot have an
algorithm for deciding the Post correspondence problem.

12.4 Undecidable Problems for Context-Free Languages
329
EXERCISES
1. Show that for A = {10, 00, 11, 01} and B = {0, 001, 1, 101} there exists a PC
solution.
2. Let A = {001, 0011, 11, 101} and B = {01, 111, 111, 010}.
Does the pair
(A, B) have a PC solution? Does it have an MPC solution?
3. Provide the details of the proof of Theorem 12.5.
4. Show that for |Σ| = 1, the Post correspondence problem is decidable; that is,
there is an algorithm that can decide whether or not (A, B) has a PC solution
for any given (A, B) on a single-letter alphabet.
5. Suppose we restrict the domain of the Post correspondence problem to include
only alphabets with exactly two symbols.
Is the resulting correspondence
problem decidable?
6. Show that the following modiﬁcations of the Post correspondence problem
are undecidable:
(a) There is an MPC solution if there is a sequence of integers such
that wiwj · · · wkw1 = vivj · · · vkv1.
(b) There is an MPC solution if there is a sequence of integers such
that w1w2wiwj · · · wk = v1v2vivj · · · vk.
7. The correspondence pair (A, B) is said to have an even PC solution if and
only if there exists a nonempty sequence of even integers i, j, ...k such that
wiwj · · · wk = vivj · · · vk. Show that the problem of deciding whether or not
an arbitrary pair (A, B) has an even PC solution is undecidable.
12.4
UNDECIDABLE PROBLEMS FOR CONTEXT-FREE
LANGUAGES
The Post correspondence problem is a convenient tool for studying unde-
cidable questions for context-free languages. We illustrate this with a few
selected results.
THEOREM 12.8
There exists no algorithm for deciding whether any given context-free gram-
mar is ambiguous.
Proof:
Consider two sequences of strings A = (w1, w2, ..., wn) and B =
(v1, v2, ...vn) over some alphabet Σ. Choose a new set of distinct symbols
a1, , a2, ..., an, such that
{a1, a2, ..., an} ∩Σ = ∅,

330
Chapter 12 Limits of Algorithmic Computation
and consider the two languages
LA = {wiwj · · · wlwkakal · · · ajai}
and
LB = {vivj · · · vlvkakal · · · ajai} .
Now look at the context-free grammar
G = ({S, SA, SB} , Σ ∪{a1, , a2, ...an} , P, S) ,
where the set of productions P is the union of the two subsets: The ﬁrst
set PA consists of
S →SA,
SA →wiSAai|wiai,
i = 1, 2, ..., n,
while the second set PB has the productions
S →SB,
SB →viSBai|viai,
i = 1, 2, ..., n.
Take
GA = ({S, SA} , Σ ∪{a1, a2, ..., an} , PA, S)
and
GB = ({S, SB} , Σ ∪{a1, a2, ..., an} , PB, S) ;
then clearly
LA = L (GA) ,
LB = L (GB) ,
and
L (G) = LA ∪LB.
It is easy to see that GA and GB by themselves are unambiguous. If
a given string in L (G) ends with ai, then its derivation with grammar GA
must have started with S ⇒wiSai. Similarly, we can tell at any later stage
which rule has to be applied. Thus, if G is ambiguous it must be because
there is a w for which there are two derivations
S ⇒SA ⇒wiSAai
∗⇒wiwj · · · wkak · · · ajai = w
and
S ⇒SB ⇒viSBai
∗⇒vivj · · · vkak · · · ajai = w.

12.4 Undecidable Problems for Context-Free Languages
331
PC solution
No PC solution
Construct G
A, B
Ambiguity
algorithm
G
G is ambiguous
G is not ambiguous
FIGURE 12.14 PC algorithm.
Consequently, if G is ambiguous, then the Post correspondence problem
with the pair (A, B) has a solution. Conversely, if G is unambiguous, then
the Post correspondence problem cannot have a solution.
If there existed an algorithm for solving the ambiguity problem, we
could adapt it to solve the Post correspondence problem as shown in Figure
12.14. But since there is no algorithm for the Post correspondence problem,
we conclude that the ambiguity problem is undecidable.
THEOREM 12.9
There exists no algorithm for deciding whether or not
L (G1) ∩L (G2) = ∅
for arbitrary context-free grammars G1 and G2.
Proof: Take as G1 the grammar GA and as G2 the grammar GB as deﬁned
in the proof of Theorem 12.8.
Suppose that L (GA) and L (GB) have a
common element, that is,
SA
∗⇒wiwj · · · wkak · · · ajai
and
SB
∗⇒vivj · · · vkak · · · ajai.
Then the pair (A, B) has a PC solution. Conversely, if the pair does not have
a PC solution, then L (GA) and L (GB) cannot have a common element. We
conclude that L (GA) ∩L (GB) is nonempty if and only if (A, B) has a PC
solution. This reduction proves the theorem.
There is a variety of other known results along these lines. Some of
them can be reduced to the Post correspondence problem, while others are
more easily solved by establishing diﬀerent intermediate results ﬁrst. We
will not give the arguments here, but only state some additional results.

332
Chapter 12 Limits of Algorithmic Computation
Here are some of the known undecidable problems for context-free lan-
guages:
• If G1 and G2 are context-free grammars, is L(G1) = L(G2)?
• If G1 and G2 are context-free grammars, is L(G1) ⊆L(G2)?
• If G1 is a context-free grammar, is L(G1) regular?
• If G1 is a context-free grammar, and G2 is regular, is L(G2) ⊆L(G1)?
• If G1 is a context-free grammar, and G2 is regular, is L(G1) ∩L(G2) = ⊘?
The proofs of most of these results are long and highly technical, so we will
omit them here.
That there are many undecidable problems connected with context-free
languages seems surprising at ﬁrst and shows that there are limitations to
computations in an area in which we might be tempted to try an algorithmic
approach. For example, it would be helpful if we could tell if a programming
language deﬁned in BNF is ambiguous, or if two diﬀerent speciﬁcations of a
language are in fact equivalent. But the results that have been established
tell us that this is not possible, and it would be a waste of time to look for
an algorithm for either of these tasks. Keep in mind that this does not rule
out the possibility that there may be ways of getting the answer for speciﬁc
cases or perhaps even most interesting ones. What the undecidability results
tell us is that there is no completely general algorithm and that no matter
how many diﬀerent cases a method can handle, there are invariably some
situations for which it will break down.
12.5
A QUESTION OF EFFICIENCY
As long as we are concerned only with computability or decidability, it
makes little diﬀerence what model of Turing machine we use. But when we
start looking at possible practical concerns, such as ease of implementation
or eﬃciency, signiﬁcant distinctions appear quickly. Here are two examples
that give us a ﬁrst look at these issues.
EXAMPLE 12.7
In Example 9.7 we constructed a single-tape Turing machine for the
language
L = {anbn : n ≥1} .
A look at that algorithm will show that for w = anbn it takes roughly
2n steps to match each a with the corresponding b. Therefore, the
whole computation takes O

n2
moves.

12.5 A Question of Eﬃciency
333
But, as we later indicated in Example 10.1, with a two-tape ma-
chine we can use a diﬀerent algorithm. We ﬁrst copy all the a's to the
second tape, then match them against the b's on the ﬁrst. The situa-
tion before and after the copying is shown in Figure 12.15. Both the
copying and the matching can be done in O (n) moves and is therefore
much more eﬃcient.
a
a
b
b
.
.
.
.
.
.
(a) Initial tapes
Tape 1
Tape 2
a
a
b
b
.
.
.
.
.
.
(b) Tapes after copying of a's
Tape 1
a
a
.
.
.
Tape 2
FIGURE 12.15
EXAMPLE 12.8
In Sections 5.2 and 6.3 we discussed the membership problem for
context-free languages. If we take the length of the input string w as
the problem size n, then the exhaustive search takes O

nM
steps,
where M depends on the grammar. The more eﬃcient CYK algorithm
requires an amount of work O

n3
.
Both of these algorithms are
deterministic.
A nondeterministic algorithm for this problem proceeds by simply
guessing which sequence of productions is applied in the derivation of
w. If we work with a grammar that has no unit- or λ-productions,
the length of the derivation is essentially |w|, so we have an O (n)
algorithm.
These examples suggest that eﬃciency questions are aﬀected by the
type of Turing machine we use and that the issue of determinism versus
nondeterminism is a particularly crucial one. We will look at this in more
detail in Chapter 14.
EXERCISES
Evaluate the eﬃciency of algorithms for the languages on Σ = {a, b, c}.
1. L = {wwR}.
2. L = {ww}.

334
Chapter 12 Limits of Algorithmic Computation
3. L = {anbncn, n ≥1}.
4. L = {w : na(w) = nb(w) = nc(w)}.
5. L = {w1w2 : |w1| = |w2|, w1 ̸= w2}.
for each of the following situations:
• On a standard Turing machine
• On a two-tape deterministic Turing machine
• On a single-tape nondeterministic Turing machine
• On a two-tape nondeterministic Turing machine

13
CHA P T E R
OTHER MODELS
OF COMPUTATION
CHAPTER SUMMARY
Grammars are examples of the concept of a rewriting system: a set
of rules by which a string can be successively rewritten to produce
other strings and thereby define languages. This chapter briefly out-
lines some typical rewriting systems and explores their connection with
Turing machines.
A
lthough Turing machines are the most general models of computation
we can construct, they are not the only ones. At various times, other
models have been proposed, some of which at ﬁrst glance seemed to
be radically diﬀerent from Turing machines. Eventually, however, all the
models were found to be equivalent. Much of the pioneering work in this
area was done in the period between 1930 and 1940 and a number of math-
ematicians, A. M. Turing among them, contributed to it. The results that
were found shed light not only on the concept of a mechanical computation,
but on mathematics as a whole.
Turing's work was published in 1936. No commercial computers were
available at that time. In fact, the whole idea had been considered only
in a very peripheral way. Although Turing's ideas eventually became very
important in computer science, his original goal was not to provide a foun-
dation for the study of digital computers. To understand what Turing was
trying to do, we must brieﬂy look at the state of mathematics at that time.
335

336
Chapter 13 Other Models of Computation
With the discovery of diﬀerential and integral calculus by Newton and
Leibniz in the seventeenth and eighteenth centuries, interest in mathematics
increased and the discipline entered an era of explosive growth. A number of
diﬀerent areas were studied, and signiﬁcant advances were made in almost
all of them. By the end of the nineteenth century, the body of mathematical
knowledge had become quite large. Mathematicians also had become suf-
ﬁciently sophisticated to recognize that some logical diﬃculties had arisen
that required a more careful approach. This led to a concern with rigor in
reasoning and a consequent examination of the foundations of mathemati-
cal knowledge in the process. To see why this was necessary, consider what
is involved in a typical proof in just about every book and paper dealing
with mathematical subjects. A sequence of plausible claims is made, inter-
spersed with phrases like "it can be seen easily" and "it follows from this."
Such phrases are conventional, and what one means by them is that, if
challenged to do so, one could give more detailed reasoning. Of course, this
is very dangerous, since it is possible to overlook things, use faulty hidden
assumptions, or make wrong inferences. Whenever we see arguments like
this, we cannot help but wonder if the proof we are given is indeed correct.
Often there is no way of telling, and long and involved proofs have been
published and found erroneous only after a considerable amount of time.
Because of practical limitations, however, this type of reasoning is accepted
by most mathematicians. The arguments throw light on the subject and at
least increase our conﬁdence that the result is true. But to those demanding
complete reliability, they are unacceptable.
One alternative to such "sloppy" mathematics is to formalize as far
as possible. We start with a set of assumed givens, called axioms, and
precisely deﬁned rules for logical inference and deduction. The rules are
used in a sequence of steps, each of which takes us from one proven fact to
another. The rules must be such that the correctness of their application
can be checked in a routine and completely mechanical way. A proposition is
considered proven true if we can derive it from the axioms in a ﬁnite sequence
of logical steps. If the proposition conﬂicts with another proposition that
can be proved to be true, then it is considered false.
Finding such formal systems was a major goal of mathematics at the
end of the nineteenth century. Two concerns immediately arose. The ﬁrst
was that the system should be consistent. By this we mean that there
should not be any proposition that can be proved to be true by one se-
quence of steps, then shown to be false by another equally valid argument.
Consistency is indispensable in mathematics, and anything derived from an
inconsistent system would be contrary to all we agree on. A second concern
was whether a system is complete, by which we mean that any proposition
expressible in the system can be proved to be true or false. For some time it
was hoped that consistent and complete systems for all of mathematics could
be devised thereby opening the door to rigorous but completely mechanical
theorem proving. But this hope was dashed by the work of K. G¨odel. In his

13.1 Recursive Functions
337
famous incompleteness theorem, G¨odel showed that any interesting con-
sistent system must be incomplete; that is, it must contain some unprovable
propositions. G¨odel's revolutionary conclusion was published in 1931.
G¨odel's work left unanswered the question of whether the unprovable
statements could somehow be distinguished from the provable ones, so that
there was still some hope that most of mathematics could be made precise
with mechanically veriﬁable proofs. It was this problem that Turing and
other mathematicians of the time, particularly A. Church, S. C. Kleene,
and E. Post, addressed. In order to study the question, a variety of for-
mal models of computation were established. Prominent among them were
the recursive functions of Church and Kleene and Post systems, but there
are many other such systems that have been studied. In this chapter we
brieﬂy review some of the ideas that arose out of these studies. There is a
wealth of material here that we cannot cover. We will give only a very brief
presentation, referring the reader to other references for detail.
A quite
accessible account of recursive functions and Post systems can be found in
Denning, Dennis, and Qualitz (1978), while a good discussion of various
other rewriting systems is given in Salomaa (1973) and Salomaa (1985).
The models of computation we study here, as well as others that have
been proposed, have diverse origins. But it was eventually found that they
were all equivalent in their power to carry out computations. The spirit
of this observation is generally called Church's thesis. This thesis states
that all possible models of computation, if they are suﬃciently broad, must
be equivalent. It also implies that there is an inherent limitation in this and
that there are functions that cannot be expressed in any way that gives an
explicit method for their computation. The claim is of course very closely
related to Turing's thesis, and the combined notion is sometimes called the
Church-Turing thesis.
It provides a general principle for algorithmic
computation and, while not provable, gives strong evidence that no more
powerful models can be found.
13.1
RECURSIVE FUNCTIONS
The concept of a function is fundamental to much of mathematics. As sum-
marized in Section 1.1, a function is a rule that assigns to an element of one
set, called the domain of the function, a unique value in another set, called
the range of the function. This is very broad and general and immedi-
ately raises the question of how we can explicitly represent this association.
There are many ways in which functions can be deﬁned. Some of them we
use frequently, while others are less common.
We are all familiar with functional notation in which we write expres-
sions like
f (n) = n2 + 1.

338
Chapter 13 Other Models of Computation
This deﬁnes the function f by means of a recipe for its computation:
Given any value for the argument n, multiply that value by itself, and then
add one. Since the function is deﬁned in this explicit way, we can compute
its values in a strictly mechanical fashion. To complete the deﬁnition of f,
we also must specify its domain. If, for example, we take the domain to be
the set of all integers, then the range of f will be some subset of the set of
positive integers.
Since many very complicated functions can be speciﬁed this way, we
may well ask to what extent the notation is universal.
If a function is
deﬁned (that is, we know the relation between the elements of its domain
and its range), can it be expressed in such a functional form? To answer the
question, we must ﬁrst clarify what the permissible forms are. For this we
introduce some basic functions, together with rules for building from them
some more complicated ones.
Primitive Recursive Functions
To keep the discussion simple, we will consider only functions of one or two
variables, whose domain is either I, the set of all nonnegative integers, or
I × I, and whose range is in I. In this setting, we start with the basic
functions:
1. The zero function z (x) = 0, for all x ∈I.
2. The successor function s (x), whose value is the integer next in se-
quence to x, that is, in the usual notation, s (x) = x + 1.
3. The projector functions
pk (x1, x2) = xk,
k = 1, 2.
There are two ways of building more complicated functions from these:
1. Composition, by which we construct
f (x, y) = h (g1 (x, y) , g2 (x, y))
from deﬁned functions g1, g2, h.
2. Primitive recursion, by which a function can be deﬁned recursively
through
f (x, 0) = g1 (x) ,
f (x, y + 1) = h (g2 (x, y) , f (x, y)) ,
from deﬁned functions g1, g2, and h.

13.1 Recursive Functions
339
We illustrate how this works by showing how the basic operations of
integer arithmetic can be constructed in this fashion.
EXAMPLE 13.1
Addition of integers x and y can be implemented with the function
add (x, y), deﬁned by
add (x, 0) = x,
add (x, y + 1) = add (x, y) + 1.
To add 2 and 3, we apply these rules successively:
add (3, 2) = add (3, 1) + 1
= (add (3, 0) + 1) + 1
= (3 + 1) + 1
= 4 + 1 = 5.
EXAMPLE 13.2
Using the add function deﬁned in Example 13.1, we can now deﬁne
multiplication by
mult (x, 0) = 0,
mult (x, y + 1) = add (x, mult (x, y)) .
Formally, the second step is an application of primitive recursion, in
which h is identiﬁed with the add function, and g2 (x, y) is the projector
function p1 (x, y).
EXAMPLE 13.3
Substraction is not quite so obvious. First, we must deﬁne it, taking
into account that negative numbers are not permitted in our system.
A kind of subtraction is deﬁned from usual subtraction by
x ˙−y = x −y if x ≥y,
x ˙−y = 0 if x < y.
The operator ˙−is sometimes called the monus; it deﬁnes subtraction
so that its range is I.

340
Chapter 13 Other Models of Computation
Now we deﬁne the predecessor function
pred (0) = 0,
pred (y + 1) = y,
and from it, the subtracting function
subtr (x, 0) = x,
subtr (x, y + 1) = pred (subtr (x, y)) .
To prove that 5 −3 = 2, we reduce the proposition by applying the
deﬁnitions a number of times:
subtr (5, 3) = pred (subtr (5, 2))
= pred (pred (subtr (5, 1)))
= pred (pred (pred (subtr (5, 0))))
= pred (pred (pred (5)))
= pred (pred (4))
= pred (3)
= 2.
In much the same way, we can deﬁne integer division, but we will leave
the demonstration of it as an exercise. If we accept this as given, we see
that the basic arithmetic operations are all constructible by the elementary
processes described. With the algebraic operations precisely deﬁned, other
more complicated ones can now be constructed, and very complex computa-
tions built from the simple ones. We call functions that can be constructed
in such a manner primitive recursive.
DEFINITION 13.1
A function is called primitive recursive if and only if it can be constructed
from the basic functions z, s, pk, by successive composition and primitive
recursion.
Note that if g1, g2, and h are total functions, then f deﬁned by compo-
sition and primitive recursion is also a total function. It follows from this
that every primitive recursive function is a total function on I or I × I.
The expressive power of primitive recursive functions is considerable,
and most common functions are primitive recursive. However, not all func-
tions are in this class, as the following argument shows.

13.1 Recursive Functions
341
THEOREM 13.1
Let F denote the set of all functions from I to I.
Then there is some
function in F that is not primitive recursive.
Proof: Every primitive recursive function can be described by a ﬁnite string
that indicates how it is deﬁned. Such strings can be encoded and arranged
in standard order. Therefore, the set of all primitive recursive functions is
countable.
Suppose now that the set of all functions is also countable. We can
then write all functions in some order, say, f1, f2, .... We next construct a
function g deﬁned as
g (i) = fi (i) + 1,
i = 1, 2, ....
Clearly, g is well deﬁned and is therefore in F, but equally clearly, g diﬀers
from every fi in the diagonal position. This contradiction proves that F
cannot be countable.
Combining these two observations proves that there must be some func-
tion in F that is not primitive recursive.
Actually, this goes even further; not only are there functions that are
not primitive recursive, there are in fact computable functions that are not
primitive recursive.
THEOREM 13.2
Let C be the set of all total computable functions from I to I. Then there
is some function in C that is not primitive recursive.
Proof: By the argument of the previous theorem, the set of all primitive
recursive functions is countable. Let us denote the functions in this set as
r1, r2, ... and deﬁne a function g by
g (i) = ri (i) + 1.
By construction, the function g diﬀers from every ri and is therefore not
primitive recursive. But clearly g is computable, proving the theorem.
The nonconstructive proof that there are computable functions that
are not primitive recursive is a fairly simple exercise in diagonalization.
The actual construction of an example of such a function is a much more
complicated matter. We will give here one example that looks quite simple;
however, the demonstration that it is not primitive recursive is quite lengthy.

342
Chapter 13 Other Models of Computation
Ackermann's Function
Ackermann's function is a function from I × I to I, deﬁned by
A (0, y) = y + 1,
A (x, 0) = A (x −1, 1) ,
A (x, y + 1) = A (x −1, A (x, y)) .
It is not hard to see that A is a total, computable function. In fact, it is quite
elementary to write a recursive computer program for its computation. But
in spite of its apparent simplicity, Ackermann's function is not primitive
recursive.
Of course, we cannot argue directly from the deﬁnition of A.
Even
though this deﬁnition is not in the form required for a primitive recursive
function, it is possible that an appropriate alternative deﬁnition could exist.
The situation here is similar to the one we encountered when we tried to
prove that a language was not regular or not context free. We need to appeal
to some general property of the class of all primitive recursive functions
and show that Ackermann's function violates this property. For primitive
recursive functions, one such property is the growth rate. There is a limit
to how fast a primitive recursive function f(n) can grow as n →∞, and
Ackermann's function violates this limit. That Ackermann's function grows
very rapidly is easily demonstrated; see, for example, Exercises 9 to 11
at the end of this section. How this is related to the limit of growth for
primitive recursive functions is made precise in the following theorem. Its
proof, which is tedious and technical, will be omitted.
THEOREM 13.3
Let f be any primitive recursive function. Then there exists some integer
n such that
f (i) < A (n, i) ,
for all i = n, n + 1, ....
Proof: For the details of the argument, see Denning, Dennis, and Qualitz
(1978, p. 534).
If we accept this result, it follows easily that Ackermann's function is
not primitive recursive.

13.1 Recursive Functions
343
THEOREM 13.4
Ackermann's function is not primitive recursive.
Proof: Consider the function
g (i) = A (i, i).
If A were primitive recursive, then so would g.
But then, according to
Theorem 13.3, there exists an n such that
g (i) < A (n, i),
for all i. If we now pick i = n, we get the contradiction
g (n) = A (n, n)
< A (n, n),
proving that A cannot be primitive recursive.
μ Recursive Functions
To extend the idea of recursive functions to cover Ackermann's function
and other computable functions, we must add something to the rules by
which such functions can be constructed. One way is to introduce the μ or
minimalization operator, deﬁned by
μy (g (x, y)) = smallest y such that g (x, y) = 0.
In this deﬁnition, we assume that g is a total function.
EXAMPLE 13.4
Let
g (x, y) = x + y ˙−3,
which is a total function. If x ≤3, then
y = 3 −x
is the result of the minimalization, but if x > 3, then there is no y ∈I
such that x + y −3 = 0. Therefore,
μy (g (x, y)) = 3 −x,
for x ≤3,
= undeﬁned, for x > 3.
We see from this that even though g (x, y) is a total function,
μy (g (x, y)) may only be partial.

344
Chapter 13 Other Models of Computation
As the previous example shows, the minimalization operation opens
the possibility of deﬁning partial functions recursively.
But it turns out
that it also extends the power to deﬁne total functions so as to include
all computable functions. Again, we merely quote the major result with
references to the literature where the details may be found.
DEFINITION 13.2
A function is said to be μ-recursive if it can be constructed from the basis
functions by a sequence of applications of the μ-operator and the operations
of composition and primitive recursion.
THEOREM 13.5
A function is μ-recursive if and only if it is computable.
Proof:
For a proof, see Denning, Dennis, and Qualitz (1978, Chapter
13).
The μ-recursive functions therefore give us another model for algorith-
mic computation.
EXERCISES
1. Use the deﬁnitions in Examples 13.1 and 13.2 to prove that 3 + 4 = 7 and
2 ∗3 = 6.
2. Deﬁne the function
greater (x, y) = 1 if x > y,
= 0 if x ≤y.
Show that this function is primitive recursive.
3. Deﬁne the function
less (x, y) = x if x < y,
= 0 if x ≥y.
Show that this function is primitive recursive.

13.1 Recursive Functions
345
4. Consider the function
equals (x, y) = 1
if x = y,
= 0
if x ̸= y.
Show that this function is primitive recursive.
5. Let f be deﬁned by
f (x, y) = x
if x ̸= y,
= 0
if x = y.
Show that this function is primitive recursive.
6. Let f be deﬁned by
f (x, y) = xy
if x = 2y,
= 0
if x ̸= 2y.
Show that this function is primitive recursive.
7. Integer division can be deﬁned by two functions div and rem:
div (x, y) = n,
where n is the largest integer such that x ≥ny, and
rem (x, y) = x −ny.
Show that the functions div and rem are primitive recursive.
8. Show that
f (n) = 2n
9. Show that
f (n) = 22n
is primitive recursive.
10. Show that the function
g (x, y) = xy
is primitive recursive.
11. Write a computer program for computing Ackermann's function. Use it to
evaluate A (2, 5) and A (3, 3).
12. Prove the following for the Ackermann function:
(a) A (1, y) = y + 2.
(b) A (2, y) = 2y + 3.
(c) A (3, y) = 2y+3 −3.
13. Use Exercise 12 to compute A (4, 1) and A (4, 2).

346
Chapter 13 Other Models of Computation
14. Give a general expression for A (4, y).
15. Show the sequence of recursive calls in the computation of A (5, 2).
16. Show that Ackermann's function is a total function in I × I.
17. Try to use the program constructed for Exercise 11 to evaluate A (5, 5). Can
you explain what you observe?
18. For each g below, compute μy (g (x, y)), and determine its domain.
(a) g (x, y) = xy.
(b) g (x, y) = 2x + 2y2 −2.
(c) g (x, y) = integer part of (x −1) / (y + 1).
(d) g (x, y) = x mod(y + 1).
19. The deﬁnition of pred in Example 13.3, although intuitively clear, does not
strictly adhere to the deﬁnition of a primitive recursive function. Show how
the deﬁnition can be rewritten so that it has the correct form.
13.2
POST SYSTEMS
A Post system looks very much like an unrestricted grammar consisting
of an alphabet and some production rules by which successive strings can
be derived. But there are signiﬁcant diﬀerences in the way in which the
productions are applied.
DEFINITION 13.3
A Post system Π is deﬁned by
Π = (C, V, A, P) ,
where
C is a ﬁnite set of constants, consisting of two disjoint sets CN, called the
nonterminal constants, and CT , the set of terminal constants,
V is a ﬁnite set of variables,
A is a ﬁnite set from C∗, called the axioms,
P is a ﬁnite set of productions.
The productions in a Post system must satisfy certain restrictions. They
must be of the form
x1V1x2 · · · Vnxn+1 →y1W1y2 · · · Wmym+1,
(13.1)

13.2 Post Systems
347
where xi, yi ∈C∗, and Vi, Wi ∈V , subject to the requirement that any
variable can appear at most once on the left, so that
Vi ̸= Vj for i ̸= j,
and that each variable on the right must appear on the left, that is,
m

i=1
Wi ⊆
n

i=1
Vi.
Suppose we have a string of terminals of the form x1w1x2w2 · · · wnxn+1,
where the substrings x1, x2 · · · match the corresponding strings in (13.1)
and wi ∈C∗. We can then make the identiﬁcation w1 = V1, w2 = V2, ...,
and substitute these values for the W's on the right of (13.1). Since every
W is some Vi that occurs on the left, it is assigned a unique value, and we
get the new string y1wiy2wj · · · ym+1. We write this as
x1w1x2w2 · · · xn+1 ⇒y1wiy2wj · · · ym+1.
As for a grammar, we can now talk about the language derived by a
Post system.
DEFINITION 13.4
The language generated by the Post system Π = (C, V, A, P) is
L (Π) =

w ∈C∗
T : w0
∗⇒w for some w0 ∈A

.
EXAMPLE 13.5
Consider the Post system with
CT = {a, b} ,
CN = ∅,
V = {V1} ,
A = {λ} ,
and production
V1 →aV1b.
This allows the derivation
λ ⇒ab ⇒aabb.

348
Chapter 13 Other Models of Computation
In the ﬁrst step, we apply (13.1) with the identiﬁcation x1 = λ, V1 = λ,
x2 = λ, y1 = a, W1 = V1, and y2 = b. In the second step, we re-identify
V1 = ab, leaving everything else the same. If you continue with this,
you will quickly convince yourself that the language generated by this
particular Post system is {anbn : n ≥0}.
EXAMPLE 13.6
Consider the Post system with
CT = {1, +, =} ,
CN = ∅,
V = {V1, V2, V3} ,
A = {1 + 1 = 11} ,
and productions
V1 + V2 = V3 →V11 + V2 = V31,
V1 + V2 = V3 →V1 + V21 = V31.
The system allows the derivation
1 + 1 = 11 ⇒11 + 1 = 111
⇒11 + 11 = 1111.
Interpreting the strings of 1's as unary representations of integers, the
derivation can be written as
1 + 1 = 2 ⇒2 + 1 = 3 ⇒2 + 2 = 4.
The language generated by this Post system is the set of all identities of
integer additions, such as 2 + 2 = 4, derived from the axiom 1 + 1 = 2.
Example 13.6 illustrates in a simple manner the original intent of Post
systems as a mechanism for rigorously proving mathematical statements
from a set of axioms. It also shows the inherent awkwardness of such a
completely rigorous approach and why it is rarely used. But Post systems,
even though they are cumbersome for proving complicated theorems, are
general models for computation, as the next theorem shows.

13.2 Post Systems
349
THEOREM 13.6
A language is recursively enumerable if and only if there exists some Post
system that generates it.
Proof: The arguments here are relatively simple and we sketch them brieﬂy.
First, since a derivation by a Post system is completely mechanical, it can
be carried out on a Turing machine. Therefore, any language generated by
a Post system is recursively enumerable.
For the converse, remember that any recursively enumerable language
is generated by some unrestricted grammar G, having productions all of the
form
x →y,
with x, y ∈(V ∪T)∗. Given any unrestricted grammar G, we create a Post
system Π = (VΠ, C, A, PΠ), where VΠ = {V1, V2} , CN = V, CT = T, A =
{S}, and with productions
V1xV2 →V1yV2,
for every production x →y of the grammar. It is then an easy matter to
show that a w can be generated by the Post system Π if and only if it is in
the language generated by G.
EXERCISES
1. Find the ﬁrst four sentences derived by the Post systems below:
(a) V →aV V b, with axiom {λ}.
(b) aV →aV V b, with axiom {a}.
(c) aV b →aV V b, with axiom {ab}.
2. For Σ = {a, b, c}, ﬁnd a Post system that generates the following languages:
(a) L (a∗b + ab∗c).
(b) L = {ww}.
(c) L = {anbncn}.
3. Find a Post system that generates
L =

wwR : w ∈{a, b}∗
.

350
Chapter 13 Other Models of Computation
4. For Σ = {a}, what language does the Post system with axiom {a} and the
production
V1 →V1V1.
generate?
5. What language does the Post system in Exercise 4, with Σ = {a, b}, generate
if the axiom set is {a, ab}?
6. Find a Post system for proving the identities of integer multiplication using
unary notation and starting from the axiom 1 ∗1 = 1.
7. Give the details of the proof of Theorem 13.6.
8. What language does the Post system with
V →aV V
and axiom set {ab} generate?
9. A restricted Post system is one on which every production x →y satisﬁes, in
addition to the usual requirements, the further restriction that the number of
variable occurrences on the right and left is the same, i.e., n = m in (13.1).
Show that for every language L generated by some Post system, there exists
a restricted Post system to generate L.
13.3
REWRITING SYSTEMS
The various grammars we have studied have a number of things in common
with Post systems: They are all based on an alphabet in which strings are
written, and some rules by which one string can be obtained from another.
Even a Turing machine can be viewed this way, since its instantaneous de-
scription is a string that completely deﬁnes its conﬁguration. The program
is then just a set of rules for producing one such string from a previous
one. These observations can be formalized in the concept of a rewriting
system. Generally, a rewriting system consists of an alphabet Σ and a
set of rules or productions by which a string in Σ+ can produce another.
What distinguishes one rewriting system from another is the nature of Σ
and restrictions for the application of the productions.
The idea is quite broad and allows any number of speciﬁc cases in ad-
dition to the ones we have already encountered. Here we brieﬂy introduce
some less well-known ones that are interesting and also provide general mod-
els for computation. For details, see Salomaa (1973) and Salomaa (1985).
Matrix Grammars
Matrix grammars diﬀer from the grammars we have previously studied
(which are often called phrase-structure grammars) in how the produc-
tions can be applied. For matrix grammars, the set of productions consists

13.3 Rewriting Systems
351
of subsets P1, P2, ..., Pn, each of which is an ordered sequence
x1 →y1, x2 →y2, ....
Whenever the ﬁrst production of some set Pi is applied, we must next apply
the second one to the string just created, then the third one, and so on. We
cannot apply the ﬁrst production of Pi unless all other productions in this
set can also be applied.
EXAMPLE 13.7
Consider the matrix grammar
P1 : S →S1S2,
P2 : S1 →aS1, S2 →bS2c,
P3 : S1 →λ, S2 →λ.
A derivation with this grammar is
S ⇒S1S2 ⇒aS1bS2c ⇒aaS1bbS2cc ⇒aabbcc.
Note that whenever the ﬁrst rule of P2 is used to create an a, the
second one also has to be used, producing a corresponding b and c.
This makes it easy to see that the set of terminal strings generated by
this matrix grammar is
L = {anbncn : n ≥0} .
Matrix grammars contain phrase-structure grammars as a special case
in which each Pi contains exactly one production. Also, since matrix gram-
mars represent algorithmic processes, they are governed by Church's thesis.
We conclude from this that matrix grammars and phrase-structure gram-
mars have the same power as models of computation. But, as Example 13.7
shows, sometimes the use of a matrix grammar gives a much simpler solution
than we can achieve with an unrestricted phrase-structure grammar.
Markov Algorithms
A Markov algorithm is a rewriting system whose productions
x →y
are considered ordered. In a derivation, the ﬁrst applicable production must
be used. Furthermore, the leftmost occurrence of the substring x must be

352
Chapter 13 Other Models of Computation
replaced by y. Some of the productions may be singled out as terminal
productions; they will be shown as
x →· y.
A derivation starts with some string w ∈Σ and continues either until a
terminal production is used or until there are no applicable productions.
For language acceptance, a set T ⊆Σ of terminals is identiﬁed. Starting
with a terminal string, productions are applied until the empty string is
produced.
DEFINITION 13.5
Let M be a Markov algorithm with alphabet Σ and terminals T. Then the
set
L (M) =

w ∈T ∗: w
∗⇒λ

is the language accepted by M.
EXAMPLE 13.8
Consider the Markov algorithm with Σ = T = {a, b} and productions
ab →λ,
ba →λ.
Every step in the derivation annihilates a substring ab or ba, so
L (M) =

w ∈{a, b}∗: na (w) = nb (w)

.
EXAMPLE 13.9
Find a Markov algorithm for
L = {anbn : n ≥0} .
An answer is
ab →S,
aSb →S,
S →·λ.

13.3 Rewriting Systems
353
If in this last example we take the ﬁrst two productions and reverse
the left and right sides, we get a context-free grammar that generates
the language L.
In a certain sense, Markov algorithms are simply
phrase-structure grammars working backward. This cannot be taken
too literally, since it is not clear what to do with the last production.
But the observation does provide a starting point for a proof of the
following theorem that characterizes the power of Markov algorithms.
THEOREM 13.7
A language is recursively enumerable if and only if there exists a Markov
algorithm for it.
Proof: See Salomaa (1985, p. 35).
L-Systems
The origins of L-systems are quite diﬀerent from what we might expect.
Their developer, A. Lindenmayer, used them to model the growth pattern
of certain organisms. L-systems are essentially parallel rewriting systems.
By this we mean that in each step of a derivation, every symbol has to be
rewritten. For this to make sense, the productions of an L-system must be
of the form
a →u,
(13.2)
where a ∈Σ and u ∈Σ∗. When a string is rewritten, one such production
must be applied to every symbol of the string before the new string is
generated.
EXAMPLE 13.10
Let Σ = {a} and
a →aa
deﬁne an L-system. Starting from the string a, we can make the deriva-
tion
a ⇒aa ⇒aaaa ⇒aaaaaaaa.
The set of strings so derived is clearly
L =

a2n : n ≥0

.

354
Chapter 13 Other Models of Computation
It is known that L-systems with productions of the form (13.2) are
not suﬃciently general to provide for all algorithmic computations.
An
extension of the idea provides the necessary generalization. In an extended
L-system, productions are of the form
(x, a, y) →u,
where a ∈Σ and x, y, u ∈Σ∗, with the interpretation that a can be replaced
by u only if it occurs as part of the string xay.
It is known that such
extended L-systems are general models of computation.
For details, see
Salomaa (1985).
EXERCISES
1. Find a matrix grammar for the language
L = {anbbc2 : n ≥0}.
2. Find a matrix grammar for
L = {ww : w ∈{a, b}∗} .
3. What language is generated by the matrix grammar
P1 : S →S1S2,
P2 : S1 →aS1b, S2 →bS2a,
P3 : S1 →λ, S2 →λ?
4. Suppose that in Example 13.7 we change the last group of productions to
P3 : S1 →λ, S2 →S.
What language is generated by this matrix grammar?
5. Why does the Markov algorithm in Example 13.9 not accept abab?
6. Find a Markov algorithm that derives the language L = {anbncn : n ≥1}.
7. Find a Markov algorithm that accepts
L = {anbmanm : n ≥1, m ≥1} .
8. Find an L-system that generates L (aa∗).
9. Find an L-grammar for
L = {anbbcn : n ≥0}.
Hint: You may have to use an extended L-grammar.

14
CHA P T E R
AN OVERVIEW
OF COMPUTATIONAL
COMPLEXITY
CHAPTER SUMMARY
While in previous discussions on Turing machines the issue was only
whether or not something could be done in principle, here the focus
shifts to how well things can be done in practice. In this final chapter,
we introduce the second part of the theory of computation: complexity
theory, an extensive topic that deals with the efficiency of computation.
We briefly describe some typical issues in complexity theory, including
the role of nondeterminism.
W
e now reconsider computational complexity, the study of the ef-
ﬁciency of algorithms. Complexity, brieﬂy mentioned in Chapter
11, complements computability by separating problems that can
be solved in practice from those that can be solved only in principle.
In studying complexity, it is necessary to ignore many details, such as
the particulars of hardware, software, data structures, and implementation,
and look at the common, fundamental issues.
For this reason, we work
mostly with orders-of-magnitude expressions. But, as we will see, even such
a very high-level view yields very useful results.
Eﬃciency is measured by resource requirements, such as time and space,
so we can talk about time complexity and space complexity.
Here
we will limit ourselves to time complexity, which is a rough measure of
the time taken by a particular computation. There are many results for
355

356
Chapter 14 An Overview of Computational Complexity
space complexity as well, but time complexity is a little more accessible
and, at the same time, more useful.
Computational complexity is an extensive topic, most of which is well
beyond the scope of this text. There are some results, however, that are
simply stated and easily appreciated, and that throw further light on the
nature of languages and computation. In this chapter, we present a brief
overview of the most salient results in complexity. Many proofs are diﬃcult
and we will dispense with them by reference to appropriate sources. Our
intent here is to present the ﬂavor of the subject matter without getting
bogged down in the details. For this reason, we will allow ourselves a great
deal of latitude, both in the selection of topics and in the formality of the
discussion.
14.1
EFFICIENCY OF COMPUTATION
Let us start with a concrete example. Given a list of one thousand integers,
we want to sort them in some way, say in ascending order. Sorting is a
simple problem but also one that is very fundamental in computer science.
If we now ask the question, "How long will it take to do this task?" we see
immediately that much more information is needed before we can answer
it. Clearly, the number of items in the list plays an important role in how
much time will be taken, but there are other factors. There is the question
of what computer we use and how we write the program. Also, there are a
number of sorting methods so that selection of the algorithm is important.
There are probably a few more things you can think of that need to be
looked at before you can even make a rough guess of the time requirements.
If we have any hope of producing a general picture of sorting, most of
these issues have to be ignored, and we must concentrate on those that are
fundamental.
For our discussion of computational complexity, we will make the fol-
lowing simplifying assumptions.
1. The model for our study will be a Turing machine. The exact type of
Turing machine to be used will be discussed below.
2. The size of the problem will be denoted by n. For our sorting problem,
n is obviously the number of items in the list. Although the size of a
problem is not always so easily characterized, we can generally relate it
in some way to a positive integer.
3. In analyzing an algorithm, we are less interested in its performance
on a speciﬁc case than in its general behavior.
We are particularly
concerned with how the algorithm behaves when the problem size in-
creases. Because of this, the primary question involves how fast the
resource requirements grow as n becomes large.

14.1 Eﬃciency of Computation
357
Our immediate goal will then be to characterize the time requirement of a
problem as a function of its size, using a Turing machine as the computer
model.
First, we give some meaning to the concept of time for a Turing machine.
We think of a Turing machine as making one move per time unit, so the
time taken by a computation is the number of moves made. As stated, we
want to study how the computational requirements grow with the size of
the problem. Normally, in the set of all problems of a given size, there is
some variation. Here we are interested only in the worst case that has the
highest resource requirements. By saying that a computation has a time-
complexity T (n), we mean that the computation for any problem of size n
can be completed in no more than T (n) moves on some Turing machine.
After settling on a speciﬁc type of Turing machine as a computational
model, we could analyze algorithms by writing explicit programs and count-
ing the number of steps involved in solving the problem. But, for a variety
of reasons, this is not overly proﬁtable. First, the number of operations per-
formed may vary with the small details of the program and so may depend
strongly on the programmer. Second, from a practical standpoint, we are
interested in how the algorithm performs in the real world, which may diﬀer
considerably from how it does on a Turing machine. The best we can hope
for is that the Turing machine analysis is representative of the major as-
pects of the real-life performance, for example, the asymptotic growth rate
of the time complexity. Our ﬁrst attempt at understanding the resource
requirements of an algorithm is therefore invariably an order-of-magnitude
analysis in which we use the O, Θ, and Ω notation introduced in Chapter
1. In spite of the apparent informality of this approach, we often get very
useful information.
EXAMPLE 14.1
Given a set of n numbers x1, x2, ..., xn and a key number x, determine
if the set contains x.
Unless the set is organized in some way, the simplest algorithm
is just a linear search in which we compare x successively against
x1, x2, ..., until either we ﬁnd a match or we get to the last element
of the set. Since we may ﬁnd a match on the ﬁrst comparison or on the
last, we cannot predict how much work is involved, but we know that,
in the worst case, we have to make n comparisons. We can then say
that the time complexity of this linear search is O (n), or even better,
Θ (n). In making this analysis, we made no speciﬁc assumptions about
what machine this is run on or how the algorithm is implemented. But
the implication is that if we were to double the size of the set of num-
bers, the computation time would roughly be doubled. This tells us a
great deal about searching.

358
Chapter 14 An Overview of Computational Complexity
EXERCISES
1. Suppose you are given a set of n numbers x1, x2, ..., xn and are asked to
determine whether this set contains any duplicates.
(a) Suggest an algorithm and ﬁnd an order-of-magnitude expression
for its time complexity.
(b) Examine if the implementation of the algorithm on a Turing ma-
chine aﬀects your conclusions.
2. Repeat Exercise 1, this time determining if the set contains any triplicates.
Is the algorithm as eﬃcient as possible?
3. Review how the choice of algorithm aﬀects the eﬃciency of sorting. What is
the time complexity of the most eﬃcient sorting algorithms?
14.2
TURING MACHINE MODELS AND COMPLEXITY
In the study of computability it makes little diﬀerence what particular model
of Turing machine we use, but we have already seen that the eﬃciency of
a computation can be aﬀected by the number of tapes of the machine and
by whether it is deterministic or nondeterministic. As Example 12.8 shows,
nondeterministic solutions are often much more eﬃcient than deterministic
alternatives. The next example illustrates this even more clearly.
EXAMPLE 14.2
We now introduce the satisﬁability problem (SAT), which plays an
important role in complexity theory.
A logic or boolean constant or variable is one that can take on
exactly two values, true or false, which we will denote by 1 and 0,
respectively. Boolean operators are used to combine boolean constants
and variables into boolean expressions. The simplest boolean operators
are or, denoted by ∨and deﬁned by
0∨1 = 1∨0 = 1∨1 = 1,
0∨0 = 0,
and the and operator (∧) , deﬁned by
0∧0 = 0∧1 = 1∧0 = 0,
1∧1 = 1.

14.2 Turing Machine Models and Complexity
359
Also needed is negation, denoted by a bar, and deﬁned by
0 = 1,
1 = 0.
We consider now boolean expressions in conjunctive normal form
(CNF). In this form, we create expressions from variables x1, x2, ..., xn,
starting with
e = ti ∧tj∧. . .∧tk.
(14.1)
The terms ti, tj, ..., tk are created by or-ing together variables and their
negation, that is,
ti = sl∨sm∨. . .∨sp,
(14.2)
where each sl, sm, ..., sp stands for a variable or the negation of a vari-
able. The si will be called literals, while the ti are said to be clauses
of a CNF expression e.
The satisﬁability problem is then simply stated: Given a satisﬁable
expression e in conjunctive normal form, ﬁnd an assignment of values
to the variables x1, x2, ..., xn that will make the value of e true. For a
speciﬁc case, look at
e1 = (x1∨x2)∧(x1∨x3) .
The assignment x1 = 0, x2 = 1, x3 = 1 makes e1 true so that this
expression is satisﬁable. On the other hand,
e2 = (x1∨x2)∧x1∧x2
(14.3)
is not satisﬁable because every assignment for the variables x1 and x2
will make e2 false.
A deterministic algorithm for the satisﬁability problem is easy to
discover. We take all possible values for the variables x1, x2, ..., xn and
for each evaluate the expression. Since there are 2n such possibilities,
this exhaustive approach has exponential time complexity.
Again, the nondeterministic alternative simpliﬁes matters. If e is
satisﬁable, we guess the value of each xi and then evaluate e. This is
essentially an O (n) algorithm. As in Example 12.8, we have a deter-
ministic exhaustive search algorithm whose complexity is exponential
and a linear-time nondeterministic one. However, unlike Example 12.8,
we do not know of any nonexponential deterministic algorithm.

360
Chapter 14 An Overview of Computational Complexity
This example and Examples 12.7 and 12.8 suggest that complexity ques-
tions are aﬀected by the type of Turing machine we use and that the issue
of determinism versus nondeterminism is a particularly crucial one. Some
general conclusions consistent with these observations can be made.
THEOREM 14.1
Suppose that a two-tape machine can carry out a computation in n steps.
Then this computation can be simulated by a standard Turing machine in
O(n2) steps.
Proof: For the simulation of the computation on the two-tape machine,
the standard machine keeps the instantaneous description of the two-tape
machine on its tape, as shown in Figure 14.1. To simulate one move, the
standard machine needs to search the entire active area of its tape. But
since one move of the two-tape machine can extend the active area by at
most two cells, after n moves the active area has a length of at most O(n).
Therefore the entire simulation can be done in O(n2) moves.
a
a
a
a
a

q
b
b
b
b
b
q
FIGURE 14.1
This result is easily extended to more than two tapes (Exercise 6 at the
end of this section).
THEOREM 14.2
Suppose that a nondeterministic Turing machine M can carry out a com-
putation in n steps. Then a standard Turing machine can carry out the
same computation in O(kan) steps, where k and a are independent of n.
Proof: A standard Turing machine can simulate a nondeterministic machine
by keeping track of all possible conﬁgurations, continually searching and
updating the entire active area.
If k is the maximum branching factor
for the nondeterminism, then after n steps there are at most kn possible
conﬁgurations. Since at most one symbol can be added to each conﬁguration
by a single move, the length of one conﬁguration after n moves is O(n).
Therefore, to simulate one move, the standard machine must search an
active area of length O(nkn), leading to the desired result. For some details,
see Exercise 7 at the end of this section.

14.2 Turing Machine Models and Complexity
361
We must interpret this theorem carefully. It says that a nondeterminis-
tic computation can always be performed on a deterministic machine if we
are willing to take into account an exponential increase in the time required.
But this conclusion comes from a particularly simple-minded simulation and
one can still hope to do better. The exploration of this issue is the core of
complexity theory.
Example 12.7 suggests that algorithms for a multitape machine may be
closer to what we might use in practice than the cumbersome method for a
standard Turing machine. For this reason, we will use a multitape Turing
machine as our model for studying complexity issues, but as we will see,
this is a minor issue.
EXERCISES
1. Find a linear-time algorithm for membership in {ww : w ∈{a, b}∗}, using a
two-tape Turing machine. What is the best you could expect on a one-tape
machine?
2. Show that any computation that can be performed on a single-tape, oﬀ-line
Turing machine in time O (T (n)) can also be performed on a standard Turing
machine in time O (T (n)).
3. Show that any computation that can be performed on a standard Turing
machine in time O (T (n)) can also be performed on a Turing machine with
one semi-inﬁnite tape in time O (T (n)).
4. Rewrite the boolean expression
(x1∧x2)∨x3
in conjunctive normal form.
5. Determine whether or not the expression
(x1∨x2∨x3)∧(x1∨x2∨x3)∧(x1∨x2∨x3)
is satisﬁable.
6. Generalize Theorem 14.1 for k tapes, showing that n moves on a k-tape
machine can be simulated on a standard machine in O(n2) moves.
7. In the proof of Theorem 14.2 we ignored one ﬁne point. When a conﬁguration
grows, the rest of the tape's contents have to be moved. Does this oversight
aﬀect the conclusion?

362
Chapter 14 An Overview of Computational Complexity
14.3
LANGUAGE FAMILIES AND COMPLEXITY CLASSES
In the Chomsky hierarchy for language classiﬁcation, we associate language
families with classes of automata, where each class of automata is deﬁned
by the nature of its temporary storage. Another possibility for classifying
languages is to use a Turing machine and consider time complexity a distin-
guishing factor. To do so, we ﬁrst deﬁne the time complexity of a language.
DEFINITION 14.1
We say that a Turing machine M decides a language L in time T (n) if every
w in L with |w| = n is decided in T (n) moves. If M is nondeterministic,
this implies that for every w ∈L, there is at least one sequence of moves of
length less than or equal to T (|w|) that leads to acceptance, and that the
Turing machine halts on all inputs in time T (|w|).
DEFINITION 14.2
A language L is said to be a member of the class DTIME (T (n)) if there
exists a deterministic multitape Turing machine that decides L in time
O(T (n)).
A language L is said to be a member of the class NTIME (T (n)) if
there exists a nondeterministic multitape Turing machine that decides L in
time O(T (n)).
Some relations between these complexity classes, such as
DTIME (T (n)) ⊆NTIME (T (n)) ,
and
T1 (n) = O (T2 (n))
implies
DTIME (T1 (n)) ⊆DTIME (T2 (n)) ,
are obvious, but from here the situation becomes obscure quickly. What we
can say is that as the order of T (n) increases, we take in progressively more
languages.
THEOREM 14.3
For every integer k ≥1,
DTIME

nk
⊂DTIME

nk+1
.
Proof: This follows from a result in Hopcroft and Ullman (1979, p. 299).

14.3 Language Families and Complexity Classes
363
The conclusion we can draw from this is that there are some languages
that can be decided in time O(n2) for which there is no linear-time mem-
bership algorithm, that there are languages in DTIME

n3
that are not in
DTIME

n2
, and so on. This gives us an inﬁnite number of nested com-
plexity classes. We get even more if we allow exponential time complexity.
In fact, there is no limit to this; no matter how rapidly the complexity
function T (n) grows, there is always something outside DTIME (T (n)).
THEOREM 14.4
There is no total Turing computable function f (n) such that every recursive
language can be decided in time f(n), where n is the length of the input
string.
Proof: Consider the alphabet Σ = {0, 1}, with all strings in Σ+ arranged
in proper order w1, w2, .... Also, assume that we have a proper ordering for
the Turing machines in M1, M2, ....
Assume now that the function f (n) in the statement of the theorem
exists. We can then deﬁne the language
L = {wi : Mi does not decide wi in f (|wi|) steps} .
(14.4)
We claim that L is recursive. To see this, consider any w ∈L and compute
ﬁrst f (|w|). By assuming that f is a total Turing computable function, this
is possible. We next ﬁnd the position i of w in the sequence w1, w2, .... This
is also possible because the sequence is in proper order. When we have i, we
ﬁnd Mi and let it operate on w for f (|w|) steps. This will tell us whether
or not w is in L, so is recursive.
But we can now show that L is not decidable in time f(n). Suppose it
were. Since L is recursive, there is some Mk, such that L = L (Mk). Is wk
in L? If we claim that if wk is in L, then Mk decides wk in f (|wk|) steps.
But this contradicts (14.4). Conversely, we get a contradiction if we assume
that wk /∈L. The inability to resolve this issue is a typical diagonalization
result and leads us to conclude that the original assumption, namely the
existence of a computable f (n), must be false.
Theorem 14.3 allows us to make some claims, for example, that there is
a language in DTIME

n4
that is not in DTIME

n3
. Although this may
be of theoretical interest, it is not clear that such a result has any practical
signiﬁcance. At this point, we have no clue what the characteristics of a
language in DTIME

n4
might be. We can get a little more insight into
the matter if we relate the complexity classiﬁcation to the languages in the
Chomsky hierarchy. We will look at some simple examples that give some
of the more obvious results.

364
Chapter 14 An Overview of Computational Complexity
EXAMPLE 14.3
Every regular language can be recognized by a deterministic ﬁnite
automaton in time proportional to the length of the input. Therefore,
LREG ⊆DTIME (n) .
But DTIME (n) includes much more than LREG.
We have al-
ready established in Example 13.7 that the context-free language
{anbn : n ≥0} can be recognized by a two-tape machine in time O (n).
The argument given there can be used for even more complicated lan-
guages.
EXAMPLE 14.4
The
non-context-free
language
L
=

ww : w ∈{a, b}∗
is
in
NTIME (n). This is straightforward, as we can recognize strings in
this language by the following algorithm:
1. Copy the input from the input ﬁle to tape 1. Nondeterministically
guess the middle of this string.
2. Copy the second part to tape 2.
3. Compare the symbols on tape 1 and tape 2 one by one.
Clearly,
all
of
the
steps
can
be
done
in
O (|w|)
time,
so
L ∈NTIME (n).
Actually, we can show that L ∈DTIME (n) if we can devise an
algorithm for ﬁnding the middle of a string in O (n) time. This can
be done: We look at each symbol on tape 1, keeping a count on tape
2, but counting only every second symbol. We leave the details as an
exercise.
EXAMPLE 14.5
It follows from Example 12.8 that
LCF ⊆DTIME

n3
and
LCF ⊆NTIME (n).

14.4 The Complexity Classes P and NP
365
Consider now the family of context-sensitive languages.
Exhaustive
search parsing is possible here also since only a limited number of pro-
ductions are applicable at each step. Following the analysis leading to
Equation (5.2), we see that the maximum number of sentential forms is
N = |P| + |P|2 + ...|P|cn = O(|P|cn+1).
Note, however, that we cannot claim from this that
LCS ⊆DTIME(|P|cn+1),
because we cannot put an upper bound on |P| and c.
From these examples we note a trend: As T (n) increases, more and
more of the families LREG, LCF , LCS are covered.
But the connection
between the Chomsky hierarchy and the complexity classes is tenuous and
not very clear.
EXERCISES
1. Complete the argument in Example 14.4.
2. Show that L =

wwRw : w ∈{a, b}+
is in DTIME (n).
3. Show that L =

www : w ∈{a, b}+
is in DTIME (n).
4. Show that there are languages that are not in NTIME (2n).
14.4
THE COMPLEXITY CLASSES P AND NP
At this point, it is instructive to summarize the diﬃculties we have encoun-
tered in trying to ﬁnd useful complexity classes for formal languages and
draw a few conclusions.
1. There exists an inﬁnite number of properly nested complexity classes
DTIME(nk), k = 1, 2, .... These complexity classes have little connec-
tion to the familiar Chomsky hierarchy and it seems diﬃcult to get any
insight into the nature of these classes. Perhaps this is not a good way
of classifying languages.
2. The particular model of Turing machine, even if we restrict ourselves
to deterministic machines, aﬀects the complexity. It is not clear what
kind of Turing machine is the best model of an actual computer, so an
analysis should not depend on any particular type of Turing machine.

366
Chapter 14 An Overview of Computational Complexity
3. We have found several languages that can be decided eﬃciently by a
nondeterministic Turing machine. For some, there are also reasonable
deterministic algorithms, but for others we know only ineﬃcient, brute-
force methods. What is the implication of these examples?
Since the attempt to produce meaningful language hierarchies via time
complexities with diﬀerent growth rates appears to be unproductive, let
us ignore some factors that are less important, for example by remov-
ing some uninteresting distinctions, such as that between DTIME

nk
and DTIME

nk+1
.
We can argue that the diﬀerence between, say,
DTIME (n) and DTIME

n2
is not fundamental, since some of it de-
pends on the speciﬁc model of Turing machine we have (e.g., how many
tapes). This leads us to consider the famous complexity class
P =

i≥1
DTIME

ni
.
This class includes all languages that are accepted by some deterministic
Turing machine in polynomial time, without any regard to the degree of the
polynomial. As we have already seen, LREG and LCF are in P.
Since the distinction between deterministic and nondeterministic com-
plexity classes appears to be fundamental, we also introduce
NP =

i≥1
NTIME

ni
.
Obviously
P ⊆NP,
but what is not known is if this containment is proper. While it is generally
believed that there are some languages in NP that are not in P, no one has
yet found an example of this.
The interest in these complexity classes, particularly in the class P,
comes from an attempt to distinguish between realistic and unrealistic com-
putations. Certain computations, although theoretically possible, have such
high resource requirements that in practice they must be rejected as unreal-
istic on existing computers, as well as on supercomputers yet to be designed.
Such problems are sometimes called intractable to indicate that, while in
principle computable, there is no realistic hope of a practical algorithm.
To understand this better, computer scientists have attempted to put the
idea of intractability on a formal basis. One attempt to deﬁne the term
intractable is made in what is generally called the Cook-Karp thesis. In
the Cook-Karp thesis, a problem that is in P is called tractable, and one
that is not is said to be intractable.
Is the Cook-Karp thesis a good way of separating problems we can solve
realistically from those we cannot? The answer is not clear cut. Obviously,
any computation that is not in P has time complexity that grows faster than

14.5 Some NP Problems
367
any polynomial, and its requirements will increase very quickly with the
problem size. Even for a function like 20.1n, this will be excessive for large
n, say n ≥1000, so we might feel justiﬁed in calling a problem with this com-
plexity intractable. But what about problems that are in DTIME

n100
?
While the Cook-Karp thesis calls such a problem tractable, one surely can-
not do much with it, even for small n. The justiﬁcation for the Cook-Karp
thesis seems to lie in the empirical observation that most practical prob-
lems in P are in DTIME (n), DTIME

n2
, or DTIME

n3
, while those
outside this class tend to have exponential complexities. Among practical
problems, a clear distinction exists between problems in P and those not
in P.
14.5
SOME NP PROBLEMS
Computer scientists have studied many NP problems, that is, problems
that can be solved nondeterministically in polynomial time. Some of the
arguments involved in this are very technical, with a number of details that
have to be resolved.
Traditionally, complexity questions are studied as languages, in such a
way that the cases that satisfy the stated conditions are described by strings
in some language L, while those that do not are in L. So, often the ﬁrst
thing that needs to be done is to rephrase our intuitive understanding of
the problem in terms of a language.
EXAMPLE 14.6
Reconsider the SAT problem. We made some rudimentary argument
to claim that this problem can be solved eﬃciently by a nondeter-
ministic Turing machine and, rather ineﬃciently, by a brute-force
exponential search. A number of minor points were ignored in that
argument.
Suppose that a CNF expression has length n, with m diﬀerent
literals. Since clearly m < n, we can take n as the problem size. Next,
we must encode the CNF expression as a string for a Turing machine.
We can do this, for example, by taking Σ = {x, ∨, ∧, (, ), −, 0, 1} and
encoding the subscript of x as a binary number. In this system, the
CNF expression (x1 ∨x2) ∧(x3 ∨x4) is encoded as
(x1 ∨x −10) ∧(x11 ∨x100).
Since the subscript cannot be larger than m, the maximum length of
any subscript is log2m. As a consequence the maximum encoded length
of an n-symbol CNF is O(nlogn).

368
Chapter 14 An Overview of Computational Complexity
The next step is to generate a trial solution for the variables. Non-
deterministically, this can be done in O(n) time. (See Exercise 1 at
the end of this section.) This trial solution is then substituted into the
input string. This can be done in O(n2logn) time∗. The entire process
therefore can be done in O(n2logn) or O(n3) time, and SAT ∈NP.
There are a large number of graph problems that have been studied and
are known to be in NP.
EXAMPLE 14.7
The Hamiltonian Path Problem
Given an undirected graph,
with vertices v1, v2, ..., vn, a Hamiltonian path is a simple path
that passes through all the vertices.
The graph in Figure 14.2 has
a Hamiltonian path (v2, v1), (v1, v3), (v3, v5), (v5, v4), (v4, v6).
The
Hamiltonian path problem (HAMPATH) is to decide if a given graph
has a Hamiltonian path.
A deterministic algorithm is easily found, since any Hamiltonian
path is a permutation of the vertices v1, v2, ..., vn. There are n! such
permuations, and a brute-force search of all of them will give the an-
swer. Unfortunately, this comes at a great expense, even for modest n.
v3
v2
v5
v4
v1
v6
FIGURE 14.2
To explore the nondeterministic solution, we must ﬁrst ﬁnd a way
to represent a graph by a string. One of the simplest and most conve-
nient ways of encoding graphs is by an adjacency matrix. For a directed
graph with vertices v1, v2, ..., vn and edge set E, an adjacency matrix
is an n × n array in which a(i, j), the entry in the ith row and jth
column satisﬁes
a(i, j) = 1 if (vi, vj) ∈E,
= 0 if (vi, vj) /∈E.

14.5 Some NP Problems
369
An undirected edge can be considered two separate edges in oppo-
site directions. The array
0 1 1 0 0 0
1 0 1 0 1 0
1 1 0 0 1 0
0 0 0 0 1 1
0 1 1 1 0 1
0 0 0 1 1 0
represents the graph in Figure 14.2.
A graph with n vertices then requires a string of length n2 for its
representation. For an undirected graph the matrix is symmetric, so
the storage requirement can be reduced to (n+1)n/2, but in any case,
the input string will have length O(n2).
Next, we generate, nondeterministically, a permutation of the ver-
tices. This can be done in O(n3) time. Finally, we check the permuta-
tion to see if it constitutes a path. A time O(n4) is suﬃcient for this.
Therefore, HAMPATH ∈NP.
EXAMPLE 14.8
The Clique Problem
Let G be an undirected graph with vertices
v1, v2, ..., vn. A k-clique is a subset Vk ⊆2V , such that there is an
edge between every pair of vertices vi, vj ∈Vk. The clique problem
(CLIQ) is to decide if, for a given k, G has a k-clique.
A deterministic search can examine all the elements of 2V . This
is straightforward, but has exponential time complexity. A nondeter-
ministic algorithm just guesses the correct subset. The representation
of the graph and the checking are similar to the previous example, so
we claim without further elaboration that the clique problem can be
solved in O(n4) time and that
CLIQ ∈NP.
There are many other such problems: some similar to our examples,
others quite diﬀerent, but all sharing the same characteristics.
1. All problems are in NP and have simple nondeterministic solutions.
2. All problems have deterministic solutions with exponential time
complexity, but it is not known if they are tractable.

370
Chapter 14 An Overview of Computational Complexity
To get further insight into the connection between these various prob-
lems, we need to ﬁnd some commonality for all these seemingly diﬀerent
cases.
EXERCISES
1. In Example 14.6, show how a trial solution can be generated in O(n) time.
This means that all 2n possibilities must be generated in a decision tree with
height O(n).
2. Show how in Example 14.6 the checking of the trial solution can be done in
O(n2logn) time.
3. Discuss how in HAMPATH a permutation can be generated nondeterminis-
tically in O(n4) time.
4. In HAMPATH, how can the checking for a Hamiltonian path be done in
O(n4) time?
5. Show that a k-clique must have exactly k(k −1)/2 edges.
6. Find a 4-clique in the graph below. Prove that the graph has no 5-clique.
7. Give the details of the argument in Example 14.8.
8. Show that P is closed under union, intersection, and complementation.
14.6
POLYNOMIAL-TIME REDUCTION
One way to unify diﬀerent cases is to see if we can reduce them to each
other, in the sense that if one is tractable, the others will be tractable also.
DEFINITION 14.3
A language L1 is said to be polynomial-time reducible to another lan-
guage L2 if there exists a deterministic Turing machine by which any w1 in

14.6 Polynomial-Time Reduction
371
the alphabet of L1 can be transformed in polynomial time to a w2 in the
alphabet of L2 in such a way that w1 ∈L1 if and only if w2 ∈L2.
EXAMPLE 14.9
In the satisﬁability problem we put no restriction on the length of
a clause.
A restricted type of satisﬁability is the three-satisﬁability
problem (3SAT) in which each clause can have at most three literals.
The SAT problem is polynomial-time reducible to 3SAT.
We illustrate the reduction with the simple 4-literal expression
e1 = (x1 ∨x2 ∨x3 ∨x4).
We introduce a new variable z and construct
e2 = (x1 ∨x2 ∨z) ∧(x3 ∨x4 ∨z).
If e1 is true, one of the x1, x2, x3, x4 must be true. If x1 ∨x2 is true,
we choose z = 0, and e2 is true. If x3 ∨x4 = 1, we can choose z = 1
to satisfy e2. Conversely, if e2 is true, e1 must also be true, so for
satisﬁability, e1 and e2 are equivalent.
We claim that this pattern can be extended to clauses with more
than four literals, but we will leave the argument as an exercise. We
include an exercise to show that the conversion of a CNF expression
from the SAT form to 3SAT form can be done deterministically in
polynomial time.
EXAMPLE 14.10
The 3SAT problem is polynomial-time reducible to CLIQUE.
We can assume that in any 3SAT expression each clause has exactly
three literals. If in some expression this is not the case, we just add
extra literals that do not change the satisﬁability. For example, (x1 ∨
x2) is equivalent to (x1 ∨x1 ∨x2). Consider now the expression
e = (x1 ∨x2 ∨x3) ∧(x1 ∨x2 ∨x3) ∧(x1 ∨x2 ∨x3) ∧(x1 ∨x2 ∨x3).
We draw a graph in which each clause is represented by a group of
three vertices and each literal is associated with one of the vertices
(Figure 14.3).
For each vertex in a group, we put in an edge to all vertices of the
other groups, unless the two associated literals are complements. So in

372
Chapter 14 An Overview of Computational Complexity
Figure 14.3 we draw an edge∗from (x2)1 to (x3)2, and from (x2)1 to
(x3)3, but not from (x2)1 to (x2)2. Figure 14.3 shows a subset of the
edges (the full set looks very messy). Notice that the subgraph with
vertices (x2)1, (x3)2, (x3)3, and (x1)4 is a 4-clique and that
x2 = x3 = x1 = 1
is a variable assignment that satisﬁes e.
group 2
group 1
group 4
group 3
X1
X2
X3
X1
X2
X3
X3
X2
X1
X3
X2
X1
FIGURE 14.3
This approach can be generalized for any 3SAT expression with k
clauses. It can be shown that the 3SAT problem can be satisﬁed if and
only if the associated graph has a k-clique. Furthermore, it is not hard
to see that the transformation from the 3SAT expression to the graph
can be done deterministically in polynomial time.
The point of these reductions is that we can now look at a given problem
in several ways. Suppose we conjecture that SAT is tractable. If this is
diﬃcult to prove, we might try the simpler 3SAT case. If this does not
∗The second subscripts identify the group to which the vertices belong.

14.7 NP-Completeness and an Open Question
373
work either, we can try to ﬁnd an eﬃcient algorithm for the clique problem,
or some other related graph problem. If any of the options can be shown to
be tractable, we can claim that SAT is tractable.
EXERCISES
1. Show how a CNF expression with clauses of ﬁve literals can be reduced to
the 3SAT form.
Generalize your method for clauses with an arbitrary number of literals.
2. Show how the reduction of SAT to 3SAT can be done in polynomial time.
3. Justify the statement in Example 14.10 that a 3SAT expression with k clauses
is satisﬁable if and only if the associated graph has a k-clique.
4. Show that the construction of the graph associated with a 3SAT expression
can be done deterministically in polynomial time.
5. The Traveling Salesman Problem (TSP) can be stated as follows. Let G be a
complete graph, whose edges are assigned nonnegative weights. The weight
of a simple path is the sum of all the edge weights. The TSP problem is to
decide if, for any given k ≥0, the graph G has a Hamiltonian path with a
weight less than or equal to k.
Show that HAMPATH is polynomial-time reducible to TSP.
14.7
NP-COMPLETENESS AND AN OPEN QUESTION
There are a number of problems that are central to complexity study and are
such that, if we completely understood one of them, we would understand
the major issue involved in tractability.
DEFINITION 14.4
A language L is said to be NP-complete if L ∈NP and every L1 ∈NP is
polynomial-time reducible to L.
It follows from this deﬁnition that if L is NP-complete and polynomial-
time reducible to L1, then L1 is also NP-complete. So if we can ﬁnd one
deterministic polynomial-time algorithm for any NP-complete language,
then every language in NP is also in P, that is,
P = NP.

374
Chapter 14 An Overview of Computational Complexity
Here we hold out the hope that eﬃcient algorithms exist for such problems,
even if none have been found yet. On the other hand, if one could prove that
any of the many known NP-complete problems is intractable, then many in-
teresting problems are not practically solvable. This puts NP-completeness
in the center of the question of the tractability of many important prob-
lems. At this point then, we need to study some NP-complete problems.
The next result, known as Cook's theorem, provides the entry point to
this study.
THEOREM 14.5
The Satisﬁability Problem (SAT) is NP-complete.
Proof: The idea behind the proof is that for every conﬁguration sequence
of a Turing machine one can construct a CNF expression that is satisﬁable
if and only if there is a sequence of conﬁgurations leading to acceptance.
The details, unfortunately, are long and complicated. They are technical
and shed little light on the NP problem, so we will omit them here. Ex-
tensive discussions of Cook's theorem can be found in books devoted to
complexity.
If we accept Cook's theorem, we immediately have a number of NP-
complete problems.
EXAMPLE 14.11
We have shown that SAT can be reduced to 3SAT, and that 3SAT
can be reduced to CLIQ. Therefore 3SAT and CLIQ are both
NP-complete.
It turns out that HAMPATH is also NP-complete, but the reduc-
tions needed to show this are less obvious.
In addition to SAT, 3SAT, CLIQ, and HAMPATH, there are many
more problems that are known to be NP-complete. A good deal of eﬀort
has been expended in trying to ﬁnd eﬃcient algorithms for any of these, so
far without success. This leads us to conjecture that
P ̸= NP
and that many important problems are intractable.
But while this is a
reasonable working conjecture, it has not been proved. It remains the fun-
damental open problem in complexity theory.

14.7 NP-Completeness and an Open Question
375
EXERCISES
1. Show that TSP is NP-complete.
2. Let G be an undirected graph. An Euler circuit of the graph is a simple cycle
that includes all edges. The Euler Circuit Problem (EULER) is to decide if
G has an Euler circuit.
Show that EULER is not NP-complete.
3. Consult books on complexity theory to compile a list of NP-complete prob-
lems.
4. Is it possible that P = NP is undecidable?


A
APP E N D I X
FINITE-STATE
TRANSDUCERS
F
inite accepters play a central role in the study of formal languages, but
in other areas, such as digital design, transducers are more important.
While we cannot go into this subject matter in any depth, we can
at least outline the main ideas. A full treatment, with many examples of
practical applications, can be found in Kohavi and Jha, 2010.
A.1
A GENERAL FRAMEWORK
Finite-state transducers (fst's) have many things in common with ﬁnite
accepters. An fst has a ﬁnite set Q of internal states and operates in a
discrete time frame with transitions from one state to another made in the
interval between two instances tn and tn+1. An fst is associated with a read-
once-only input ﬁle that contains a string from an input alphabet Σ, and
an output mechanism that produces a string from an output alphabet Γ
in response to a given input. It will be assumed that in each time step one
input symbol is used, while a single output symbol is produced (we also say
printed).
Since an fst just translates certain strings into other strings, we can
look at the fst as an implementation of a function. If M is an fst, we will
let FM denote the function represented by M, so
FM : D →R,
377

378
Appendix A Finite-State Transducers
where D is a subset of Σ∗and R is a subset of Γ∗. For most of the discussion
we assume that D = Σ∗.
Interpreting an fst as a function implies that it is deterministic, that
is, the output is uniquely determined by the input. Nondeterminism is an
important issue in language theory, but plays no signiﬁcant role in the study
of ﬁnite-state transducers.
The rule that one input symbol results in one output symbol appears
to imply that the mapping FM is length preserving, that is, that
|FM(w)| = |w|.
But this is more apparent than real. For example, we can always include
the empty string λ in Γ, so that
|FM(w)| < |w|
becomes possible. There are other ways in which the length-preserving re-
striction can be overcome.
There are several types of fst's that have been extensively studied. The
main diﬀerence between them is on how the output is produced.
A.2
MEALY MACHINES
In a Mealy machine, the output produced by each transition depends on
the internal state prior to the transition and the input symbol used in the
transition, so we can think of the output produced during the transition.
DEFINITION A.1
A Mealy machine is deﬁned by the sextuple
M = (Q, Σ, Γ, δ, θ, q0),
where
Q is a ﬁnite set of internal states,
Σ is the input alphabet,
Γ is the output alphabet,
δ : Q × Σ →Q is the transition function,
θ : Q × Σ →Γ is the output function,
q0 ∈Q is the initial state of M.
The machine starts in state q0 at which time all input is available for pro-
cessing. If at time tn the Mealy machine is in state qi, the current input

A.2 Mealy Machines
379
symbol is a, and δ(qi, a) = qj, θ(qi, a) = b, the machine will enter state qj
and produce output b. It is assumed the entire process is terminated when
the end of the input is reached. Note that there are no ﬁnal states associated
with a transducer.
Transition graphs are as useful here as they are for ﬁnite accepters. In
fact, the only diﬀerence is that now the transition edges are labeled with
a/b, where a is the current input symbol and b is the output produced by
the transition.
EXAMPLE A.1
The fst with Q = {q0, q1}, Σ = {0, 1}, Γ = {a, b, c}, initial state q0,
and
δ(q0, 0) = q1,
δ(q0, 1) = q0,
δ(q1, 0) = q0,
δ(q1, 1) = q1,
θ(q0, 0) = a,
θ(q0, 1) = c,
θ(q1, 0) = b,
θ(q1, 1) = a
is represented by the graph in Figure A.1. This Mealy machine prints
out caab when given the input string 1010.
1/a
1/c
0/a
0/b
q0
q1
FIGURE A.1
EXAMPLE A.2
Construct a Mealy machine M that takes as input strings of 0's and
1's. Its output is to be a string of 0's until the ﬁrst 1 occurs in the
input, at which time it will switch to printing 1's. This is to continue
until the next 1 is encountered in the input, when the output reverts
to 0. The alternation continues every time a 1 is encountered. For
example, FM(0010010) = 0011100. This fst is a simple model for a
ﬂip-ﬂop circuit. Figure A.2 shows a solution.

380
Appendix A Finite-State Transducers
0/1
0/0
1/1
1/0
q0
q1
FIGURE A.2
A.3
MOORE MACHINES
Moore machines diﬀer from Mealy machines in the way output is produced.
In a Moore machine every state is associated with an element of the output
alphabet. Whenever the state is entered, this output symbol is printed.
The output is produced only when a transition occurs; thus, the symbol
associated with the initial state is not printed at the start, but may be
produced if this state is entered at a later stage.
DEFINITION A.2
A Moore machine is deﬁned by the sextuple
M = (Q, Σ, Γ, δ, θ, q0),
where
Q is a ﬁnite set of internal states,
Σ is the input alphabet,
Γ is the output alphabet,
δ : Q × Σ →Q is the transition function,
θ : Q →Γ is the output function,
q0 ∈Q is the initial state.
The machine starts in state q0, at which time all input is available for
processing. If at time tn the Moore machine is in state qi, the current input
symbol is a, and δ(qi, a) = qj, θ(qj) = b, the machine will enter state qj and
produce output b.
In the transition graph of a Moore machine, each vertex now has two
labels: the state name and the output symbols associated with the state.

A.3 Moore Machines
381
EXAMPLE A.3
A Moore machine solution for the problem in Example A.2 is given in
Figure A.3.
0
0
1
1
q0
0
q1
1
FIGURE A.3
EXAMPLE A.4
In Example 1.17 we constructed a transducer for adding two positive
binary numbers. Figure 1.9 shows that what we have constructed
is actually a two-state Mealy machine. A Moore machine for this
problem is also easily constructed, but now we need four states to keep
track not only of the carry, but also of the output symbol. A solution
is shown in Figure A.4.
NCO
0
CO
0
00
10
01
00
01
10
01
10
11
NC1
1
C1
1
11
11
00
00
01
10
11
FIGURE A.4

382
Appendix A Finite-State Transducers
A.4
MOORE AND MEALY MACHINE EQUIVALENCE
The examples in the previous section show the diﬀerence between Moore
and Mealy machines, but they also suggest that if a problem can be solved
by a machine of one type, it can also be solved by one of the other type. In
this sense, the two types of transducers are possibly equivalent.
DEFINITION A.3
Two ﬁnite-state transducers M and N are said to be equivalent if they
implement the same function, that is, if they have the same domain and if
FM(w) = FN(w),
for all w in their common domain.
DEFINITION A.4
Let C1 and C2 be two classes of ﬁnite-state transducers. We say that C1 and
C2 are equivalent if for every fst M in one class there exists an equivalent
fst N in the other class, and vice versa.
We will now show that the Mealy and Moore machine classes are equiv-
alent. For this we need to introduce the extended transition function δ∗and
the extended output function θ∗. The expression
δ∗(qi, w) = qj
is meant to indicate that the fst goes from state qi to state qj while pro-
cessing the string w. Similarly,
θ∗(qi, w) = v
is to show that the fst produces output v when starting in state qi and given
input w. For both Mealy and Moore machines δ∗is formally deﬁned by
δ∗(qi, a) = δ(qi, a),
δ∗(qi, wa) = δ(δ∗(qi, w), a),
for all a ∈Σ and all w ∈Σ+. For Mealy machines
θ∗(qi, a) = θ(qi, a),
θ∗(qi, wa) = θ∗(qi, w)θ(δ∗(qi, w), a),

A.4 Moore and Mealy Machine Equivalence
383
while for Moore machines
θ∗(qi, a) = θ(δ(qi, a)),
θ∗(qi, wa) = θ∗(qi, w)θ(δ∗(qi, wa)).
The conversion of a Moore machine to an equivalent Mealy machine
is straightforward. The states of the two machines are the same, and the
symbol that is to be printed by the Moore machine is assigned to the Mealy
machine transition that leads to that state.
EXAMPLE A.5
The Moore machine in Figure A.5(a) is equivalent to the Mealy
machine in Figure A.5(b).
b
a1
1
a
a
a
b
b
q0
0
q2
1
(a)
b/1
p1
a/1
a/1
a/1
b/1
b/0
p0
p2
(b)
FIGURE A.5
THEOREM A.1
For every Moore machine there exists an equivalent Mealy machine.
Proof: Let M = (Q, Σ, Γ, δM, θM, q0) be a Moore machine. We then con-
struct a Mealy machine N = (Q, Σ, Γ, δN, θN, q0) , where
δN = δM
and
θN(qi, a) = θM(δM(qi, a)).
It is intuitively clear that M and N are equivalent. Both machines go
through the same states in response to a given input. M prints a sym-
bol when a state is entered, N prints the same symbol during the transition

384
Appendix A Finite-State Transducers
to that state. A more explicit proof involves an easy induction that we leave
to the reader.
The conversion from a Mealy machine to an equivalent Moore machine is
a little more complicated because the states of the Moore machine now have
to carry two pieces of information: the internal state of the corresponding
Mealy machine, and the output symbol produced by the Mealy machine's
transition to that state. In the construction we create, for each state qi of
the Mealy machine, |Γ| states of the Moore machine labeled qia, a ∈Γ. The
output function for the Moore machine states will be θ(qja) = a. When the
Mealy machine changes to state qj and prints a, the Moore machine will go
into state qja and so print a also.
EXAMPLE A.6
The Mealy machine in Figure A.6(a) and the Moore machine in Figure
A.6(b) are equivalent.
(a)
q0
q2
q1
b/1
b/0
a/1
b/0
a/1
a/1
q10
0
q11
1
q01
1
q21
1
q20
0
q00
0
b
(b)
b
b
b
b
b
a
a
a
a
a
a
FIGURE A.6

A.4 Moore and Mealy Machine Equivalence
385
THEOREM A.2
For every Mealy machine N there exists an equivalent Moore machine M.
Proof: Let N = (QN, Σ, Γ, δN, θN, q0) with QN = {q0, q1, ..., qn} be a Mealy
machine. We then construct a Moore machine M = (QM, Σ, Γ, δM, θM, q0r)
as follows: Create the states and the output function of M by
qia ∈QM
and
θM(qia) = a,
for all i = 1, 2, ..., n and all a ∈Γ. For every transition rule
δN(qi, a) = qj
and corresponding output function
θN(qi, a) = b
we introduce |Γ| rules
δM(qic, a) = qjb
for all c ∈Γ. Since the start state symbol is not printed before the ﬁrst
transition, the initial state for M can be any state q0r, r ∈Γ. This completes
the construction.
To show that N and M are equivalent, we ﬁrst show that if
δ∗
N(q0, w) = qk,
(A.1)
then there is a c ∈Γ such that
δ∗
M(q0r, w) = qkc.
(A.2)
This, and its converse, can be proved by an induction on the length of w.
Next consider
θ∗
N(q0, wa) = θ∗
N(q0, w)θN(δ∗
N(q0, w), a))
(A.3)
and suppose that δ∗
N(q0, w) = qk, δN(qk, a) = ql and θN(qk, a) = b. Then
θN(δ∗
N(q0, w), a) = b.
From (A.2) and
δM(qkc, a) = qlb
it follows that
θ∗
M(q0r, wa) = θ∗
M(q0r, w)θM(qlb)
= θ∗
M(q0r, w)b.

386
Appendix A Finite-State Transducers
Returning now to (A.3)
θ∗
N(q0, wa) = θ∗
N(q0, w)θN(δ∗
N(q0, w), a))
= θ∗
N(q0, w)b.
If we now make the inductive assumption
θ∗
N(q0, w) = θ∗
M(q0r, w)
(A.4)
for all |w| ≤m and any r ∈Γ then
θ∗
N(q0, wa) = θ∗
M(q0r, w)b
= θM(q0r, wa).
and so (A.4) holds for all m.
Putting the two constructions together we get the fundamental equivalence
result.
THEOREM A.3
The classes of Mealy machines and Moore machines are equivalent.
A.5
MEALY MACHINE MINIMIZATION
For a given function Σ∗→Γ∗there are many equivalent ﬁnite-state trans-
ducers, some of them diﬀering in the number of internal states. For practical
reasons it is often important to ﬁnd the minimal fst, that is, the machine
with the smallest number of internal states.
The ﬁrst step in minimizing a Mealy machine is to remove the states
that play no role in any computation because they cannot be reached from
the initial state. When there are no inaccessible states, the fst is said to be
connected. But a Mealy machine can be connected and yet not be minimal,
as the next example illustrates.
EXAMPLE A.7
The Mealy machine in Fig A.7(a) is connected, but it is clear that the
states q1 and q2 serve the same purpose and can be combined to give
the machine in Figure A.7(b).

A.5 Mealy Machine Minimization
387
q0
b/1
q2
q1
a/0
q3
b/0
a/1
a/1
b/0
b/0
a/0
b/0
a/0
b/0
a/1
b/1
a/0
q0
q12
q3
(a)
(b)
FIGURE A.7
DEFINITION A.5
Let M = (Q, Σ, Γ, δ, θ, q0) be a Mealy machine. We say that the states qi
and qj are equivalent if and only if
θ∗(qi, w) = θ∗(qj, w)
for all w ∈Σ∗. States that are not equivalent are said to be distinguishable.
DEFINITION A.6
Two states qi and qj are said to be k-equivalent
θ∗(qi, w) = θ∗(qj, w)
for all |w| ≤k. Two states are k-distinguishable if there exists a |w| ≤k
such that θ∗(w, qi) ̸= θ∗(w, qj).
THEOREM A.4
(a) Two states qi and qj of a Mealy machine are 1-distinguishable if there
exists an a ∈Σ such that
θ(qi, a) ̸= θ(qj, a).
(b) The two states are k-distinguishable if they are (k −1)-distinguishable
or if there is an a ∈Σ such that
δ(qi, a) = qr
δ(qj, a) = qs,
where qr and qs are (k −1)-distinguishable.

388
Appendix A Finite-State Transducers
Proof: Part(a) follows directly from the deﬁnition and so does the conclusion
that states are k-distinguishable if they are (k−1)-distinguishable. For Part
(b), we know that there exists a |w| ≤k −1, such that
θ∗(qr, w) ̸= θ∗(qs, w).
Now
θ∗(qi, aw) = θ(qi, a)θ∗(qr, w)
and
θ∗(qj, aw) = θ(qj, a)θ∗(qs, w).
The result then follows.
THEOREM A.5
Let M1 = (Q, Σ, Γ, δ, θ, q0) be a Mealy machine in which all states are
distinguishable. Then M1 is minimal and unique (within a relabeling of
states).
Proof: Suppose that there exists an equivalent machine M2 with fewer states
than M1. If M2 has fewer states than M1, by the pigeonhole principle, at
least one of the states of M2 must combine several functions of the states of
M1. Speciﬁcally, there must be two strings, each of which leads to a diﬀerent
state in M1, but which both lead to the same state in M2. What possible
structure can M2 have?
Suppose, for the sake of illustration, that M1 has the partial structure
shown in Figure A.8 and we try to combine states q1 and q2 for M2. To
preserve the equivalence, the partial structures of M2 must be as shown in
Figure A.9. But since q1 and q2 are distinguishable in M1, there must be
some w so that
θ∗(q1, w) ̸= θ∗(q2, w).
a/1
b/0
q0
b/0
a/0
q1
q2
a/1
b/0
FIGURE A.8

A.5 Mealy Machine Minimization
389
a/1
b/0
b/0
a/0
q0
q12
FIGURE A.9
Therefore in M1 the input strings aw and bw must produce diﬀerent output.
But in M2 they clearly print the same thing. This contradiction implies that,
at this level, the two machines must be identical. Since this reasoning can
be extended to any part of the two machines, the theorem follows.
The minimization of a Mealy machine therefore starts with an identiﬁ-
cation of equivalent states.
Partition Algorithm
1. With states Q = {q0, q1, ..., qn} ﬁnd all states that are 1-equivalent to q0
and partition Q into two sets {q0, qi, ..., qj} and {qk, ..., ql}. The ﬁrst of
these sets will contain all states that are 1-equivalent to q0, the second
will contain all states that are 1-distinguishable from it. Next, we repeat
this process with states q1, q2, ..., qn. Removing duplicate sets, we are
left with a partitioning based on 1-equivalence.
2. For every pair of states qi and qj in the same equivalence class determine
if there are transitions, as in Theorem A.4, so that there are states qr and
qs in diﬀerent equivalence classes. If so, create new equivalence classes
to separate them. Check all pairs to ﬁnd their appropriate equivalence
classes.
3. Repeat step 2 until no new equivalence classes are created.
At the end of this procedure, the state set Q will have been partitioned
into equivalence classes E1, E2, ..., Eq so that all members of each class are
equivalent in the sense of Deﬁnition A.5.
To justify the procedure, several points have to be addressed. The ﬁrst is
that after the kth pass though step 2 all elements of an existing equivalence
class are (k + 1)-equivalent. This follows by induction, using part (b) of
Theorem A.4.
The second point is that the process must terminate. This is clear since
each pass though step 2 creates at least one new equivalence class, and there
can be at most |Q| such classes.
Finally, we must show that a complete equivalence partitioning has
been achieved when the process stops. This can be seen from part (b) of
Theorem A.4. For a pair (qi, qj) to be k-distinguishable, there must exist
(k−1)-distinguishable states qr and qs; if no such states exist, no states that
can be distinguished by longer strings can exist. Therefore, that equivalence
partitioning must be complete.

390
Appendix A Finite-State Transducers
Minimum Mealy Machine Construction Let M = (Q, Σ, Γ, δ, θ, q0)
be the Mealy machine for which we want to construct a minimal equivalent
machine P = (QP , Σ, Γ, δP , θP , QP ), where QP = {E1, E2, ..., Em}. First we
ﬁnd the equivalence classes E1, E2, ..., Em using the partition procedure and
create states labeled E1, E2, ..., Em for P. Pick an element qi from Er and
an element qj from Es. If δ(qi, a) = qj and θ(qi, a) = b, deﬁne the transition
function for P by
δP (Er, a) = Es
and output by
θP (Er, a) = b.
If the start state for M is q0, the start state for P will be Ep so that q0 is
in the equivalence class Ep. It is a straightforward exercise to show that P
is the minimal equivalent of M. It also follows from Theorem A.5 that a
minimal Mealy machine is unique within a simple relabeling of the states.
EXAMPLE A.8
Consider the Mealy machine in Figure A.10. Step 1 produces the equiv-
alence partitioning {q0, q4}, {q1, q2}, {q3}. In the second step we ﬁnd
that δ(q0, a) = q1, δ(q4, a) = q3. Since q1 and q3 are distinguishable, so
are q0 and q4 and the new partition is
E1 = {q0}, E2 = {q1, q2}, E3 = {q3}, E4 = {q4}.
Another pass though step 2 yields no further reﬁnement and the par-
titioning if ﬁnished. From it, we construct the minimal Mealy machine
in Figure A.11.
q0
b/1
q2
q1
a/1
q3
b/0
a/1
a/1
b/0
q4
b/0
a/0
a/1
b/1
FIGURE A.10

A.6 Moore Machine Minimization
391
E4
b/0
a/0
a/1
b/1
b/0
a/1
b/1
a/1
E1
E2
E3
FIGURE A.11
A.6
MOORE MACHINE MINIMIZATION
The minimization of a Moore machine follows the pattern of the mini-
mization of Mealy machines, but there are some diﬀerences. While Deﬁ-
nition A.5 and A.6 apply to Moore machines, Theorem A.4 needs to be
modiﬁed.
THEOREM A.6
(a) Two states qi and qj of a Moore machine are 0-distinguishable if
θ(qi) ̸= θ(qj).
(b) the two states are k-distinguishable if they are (k −1)-distinguishable
or if there exists an a ∈Σ such that δ(qi, a) = qr and δ(qj, a) = qs, where
qr and qs are (k −1)-distinguishable.
Proof: The argument here is essentially the same as in Theorem A.4.
For the Moore machine minimization we ﬁrst use the Mealy machine
partition procedure to partition the states into equivalence classes and use
them following the minimization process.
Minimal Moore Machine Construction
Let M = (Q, Σ, Γ, δ, θ, q0) be the Moore machine to be minimized. First we
establish the equivalence classes E1, E2, ..., Em by the partition algorithm
and create states labeled E1, E2, ..., Em for the minimized machine P. Pick
an element qi from Er and an element qj from Es. If δ(qi, a) = qj and
θ(qj) = b, then the transition function for P is
δP (Er, a) = Es

392
Appendix A Finite-State Transducers
and
θP (Es) = b.
This will yield the minimal Moore machine.
There is a minor complication with the minimal Moore machine con-
struction. The minimization process assigns θ(q0) to the equivalence state
associated with q0. In some cases this can lead to non-uniqueness.
EXAMPLE A.9
Look at the two Moore machines in Figure A.12. Clearly they are
equivalent and also clearly they are minimal. But since the output for
the two initial states is diﬀerent, they cannot be made identical by
just a relabeling. The diﬃculty comes from the fact that the output
for the initial state is arbitrary unless that state can be re-entered.
But this is a trivial issue that can be ignored.
a
a
a
q0
0
q1
1
q2
0
a
a
a
p0
1
p1
1
p2
0
(a)
(b)
FIGURE A.12
A.7
LIMITATIONS OF FINITE-STATE TRANSDUCERS
Mealy and Moore machines are ﬁnite-state automata so we rightly suspect
that their capabilities are limited, just as ﬁnite accepters are limited. To
explore these limitations we need something like the pumping lemma for
regular languages.
THEOREM A.7
Let M = (Q, Σ, Γ, δ, θ, q0) be a Mealy machine. Then there exists a state
qi ∈Q and a w ∈Σ+, such that
δ∗(qi, w) = qi.
Proof: This follows from the pigeonhole principle, noting that |Q| is ﬁnite
but w can be arbitrarily long.

A.7 Limitations of Finite-State Transducers
393
EXAMPLE A.10
Consider the function F{a}∗→{a, b}∗, deﬁned by
F(a2n) = anbn,
F(a2n+1) = anbn+1.
Is there a Mealy machine that implements this function? A few tries
quickly suggest that the answer is no.
If there existed such a machine with m states, we could pick as
input w = a2m. During the processing of the ﬁrst part of this string,
by Theorem A.7, the machine would have to go into a cycle in which
an input of a produces an output of a. To escape from the cycle we
would need an input other than a. But since there is no other input, the
machine would continue to print a's and so not represent the function.
The contradiction shows that no such machine can exist.
The conclusion from this example should not be surprising as it involves
translating a regular set into one that is not regular. In fact, the structural
similarity between fst's and ﬁnite accepters suggests a connection between
regular languages and the output produced by ﬁnite-state transducers.
DEFINITION A.7
Let M be a ﬁnite-state transducer implementing the function FM and let
L be any language. Then
TM(L) = {FM(w) : w ∈L}
is the M-translation of L.
Now an fst can generate any output since it can just reproduce input
that is generally not limited. But if the input is a regular language, so is
the output.
THEOREM A.8
Let M = (Q, Σ, Γ, δM, θM, q0) be a Mealy machine and let L be a regular
language. The TM(L) is also regular.
Proof: If L is regular, then there exists a dfa N = (P, Σ, δN, p0, F) such that
L = L(N). From M and N we now construct a ﬁnite accepter (possibly

394
Appendix A Finite-State Transducers
nondeterministic) H = (QH, Σ, δH, qH, FH) as follows:
QM = {qij : pi ∈P, qj ∈Q}.
If δN(pi, a) = pk, δM(qj, a) = ql, and θM(qj, a) = b, then
δH(qij, b) = qkl.
(A.5)
The initial state for H will be q00 and its ﬁnal state set
FH = {qij : pi ∈F, qj ∈Q}.
Then TH(L) is regular.
To defend this statement, notice ﬁrst that if δ∗
N(pi, w) = pk and
δ∗
M(qj, w) = ql, then
δ∗
H(qij, θ∗
M(qij, w)) = qkl.
This follows by a straightforward induction. Therefore, if δN(p0, w) ∈F,
then
δH(q00, θ∗
M(q00, w)) ∈FH,
and if w ∈L, then FM(w) ∈L(H).
To complete the argument we must also show that if a string v is ac-
cepted by H, there must be a w ∈L, such that v = FM(w). Suppose now
that instead of H we construct another dfa H′, identical to H, except that
(A.5) is replaced by
δH′(qij, a) = qkl.
The N and H′ are equivalent and a string w is accepted by H′ if and only
if w ∈L. Now if v ∈L(H), then in the transition graph for H there is a
path from q00 to a ﬁnal state labeled v. But the same path in H′ is labeled
w, so v = FM(w). Therefore w ∈L.

B
APP E N D I X
JFLAP:
A USEFUL TOOL
T
he basic premise of this book is that understanding diﬃcult abstract
concepts is best achieved through illustrative examples and challeng-
ing exercises, so problem solving is a central theme of our approach.
Solving a diﬃcult problem normally involves two distinct steps. First
we must understand the issues, decide what theorems and results apply, and
how to put it all together to arrive at a solution. This tends to be the most
diﬃcult part and often requires insight and inventiveness. But once we have
a clear understanding of the solution process, a more routine step is still
necessary to produce concrete results. In our study, this involves actually
constructing automata or grammars and testing them for correctness. This
step may be less challenging but tedious and error prone. It is here that
mechanical help in the form of software can be very useful. In my experience,
JFLAP serves this purpose admirably.
JFLAP is an interactive tool built on the concepts in this book. It
was created by Professor Susan Rodger and her students at Duke Univer-
sity. It has been used successfully in many universities over a number of
years. Some JFLAP-related material is collected in JPAK, available online
at: go.jblearning.com/LinzJPAK. JPAK gives you a brief introduction to
JFLAP, how to get it and how to use it. It also contains many exercise
examples that illustrate the power of JFLAP as well as a useful library of
functions.
JFLAP is useful in many ways. For the student, JFLAP gives a way
of seeing how abstract concepts are implemented in practice. Seeing how a
395

396
Appendix B JFLAP: A Useful Tool
diﬃcult construction, such as the conversion of a dfa to a regular expression,
is implemented brings to life something that may be diﬃcult to grasp oth-
erwise. Nonintuitive concepts, such as nondeterminism, are illustrated in a
practical way. JFLAP is also a great time saver. Constructing, testing, and
modifying automata and grammars can be done in a fraction of the time it
takes with the more traditional pencil-and-paper method. Since extensive
testing is easy, it will also improve the quality of the ﬁnal product.
Instructors can also beneﬁt from JFLAP. Electronic submission and
batch grading will save much eﬀort, while at the same time increasing the
accuracy and fairness of the evaluations. Exercises that are instructive, but
often avoided because of the large amount of busywork involved, are now
possible. An example here is the conversion from a right-linear grammar
to an equivalent left-linear one. Working with Turing machines is notori-
ously onerous and error prone, but with JFLAP many more challenging
assignments become reasonable. JPAK has a large number of such exer-
cises. Finally, since JPAK contains the JFLAP implementations of many of
the examples in the book, there is an opportunity for a dynamic classroom
presentation of these examples.
JFLAP is pretty much self-explanatory and little eﬀort is needed in
learning to use it. I strongly recommend its use to both students and in-
structors.

ANSWERS: SOLUTIONS
AND HINTS FOR SELECTED
EXERCISES
Chapter 1
Section 1.1
5. Suppose x ∈S −T. Then x ∈S and x /∈T, which means x ∈S and
x ∈T, that is x ∈S ∩T. So S −T ⊆S ∩T. Conversely, if x ∈S ∩T,
then x ∈S and x /∈T, which means x ∈S −T, that is S ∩T ⊆S −T.
Therefore S −T = S ∩T.
6. For Equation (1.2), suppose x ∈S1 ∪S2. Then x /∈S1 ∪S2, which
means that x cannot be in S1 or in S2, that is x ∈S1 ∩S2.
So
S1 ∪S2 ⊆S1 ∩S2. Conversely, if x ∈S1 ∩S2, then x is not in S1 and
x is not in S2, that is, x ∈S1 ∪S2. So S1 ∩S2 ⊆S1 ∪S2. Therefore
S1 ∪S2 = S1 ∩S2.
12. (a)
reﬂexivity: Since |S1| = |S1|, so reﬂexivity holds.
(b)
symmetry: if |S1| = |S2| then|S2| = |S1| so symmetry is satisﬁed.
(c)
transitivity: Since |S1| = |S3| = |S2|. transitivity holds, and the
relation is an equivalence.
20. The three equivalent classes are: E0 = {6, 9, 24}, E1 = {1, 4, 25, 31, 37},
and E2 = {2, 5, 23}.
26. Ordinary arithmetic operations are not applicable. For example, take
x = n3 and y =
1
n2 , then by the deﬁnition x = O(n4), y = O(n2).
However, x/y = n5.
397

398
Answers
30. Ordinary arithmetic operations do not apply to order of magnitude
in general. That is O(n2) −O(n2) is not necessarily equal to 0. For
example, in this problem if g(n) = n2, then f(n) −g(n) = n2 + n =
O(n2).
39. (a)
f(n) < 2(2n) holds for n = 1. Assume that f(k) < 2(2k) is true
for k = 1, 2, ..., n, n + 1. Then
f(n+2) = f(n+1)+f(n) < 2(2n+1)+2(2n) < 2n+2+2n+1 < 2(2n+2).
(b)
f(n) > 1.5n/10 is true for n = 1. Assume true for k = 1, 2, ..., n+
1, then
f(n + 2) = f(n + 1) + f(n) > 1.5n+1/10 + 1.5n/10
= 1.5n+1(1 + 1
1.5)/10 > 1.5n+1(1 + 0.5)/10 = 1.5n+2/10.
41. Suppose 2 −
√
2 is a rational number, then
2 −
√
2 = n
m,
where n and m are integers without a common factor. Equivalently,
we have
√
2 = 2 −n
m = 2m −n
m
which means
√
2 is a rational number. Howver, we already know that
√
2 is not rational so 2 −
√
2 must be irrational.
43. (a)
True. If a is rational and b is irrational, but c = a + b is rational,
then b = c −a must be rational, a contradiction.
(b)
False. For example, a = 2+
√
2 and b = 2−
√
2 are both positive
irrationals, but a + b = 4 is rational.
(c)
True. If if a ̸= 0 is rational and b ̸= 0 is irrational but c = ab is
rational, then b = c/a must be rational.
Section 1.2
2. For n = 1, |u1| = |u|. Assume |uk| = k|u| holds for k = 1, 2, ..., n, then
|un+1| = |unu| = |un| + |u| = n|u| + |u| = (n + 1)|u|.
6. L = {λ, a, b, ab, ba} ∪{w ∈{a, b}+ : |w| ≥3}.
8. No, there is no such a language, because λ /∈L∗but λ ∈(L)∗.
11. (a) True. Suppose w ∈(L1 ∪L2)R, then wR ∈(L1 ∪L2), so wR ∈L1
or wR ∈L2. and w ∈(LR
1 ∪LR
2 ). Therefore, (L1 ∪L2)R ⊆(LR
1 ∪LR
2 ).
Similarly, we can prove (LR
1 ∪LR
2 ) ⊆(L1∪L2)R. Therefore, (L1 ∪L2)R
= LR
1 ∪LR
2 .

Solutions and Hints for Selected Exercises
399
13.
S →aaaaA,
A →aAa|λ.
14. (a)
S →AaAaA,
A →bA|λ.
(f)
We ﬁrst generate pairs of b's , then add an arbitrary number of
a's anywhere.
S →SbSbS|A|λ,
A →aA|λ.
17. (a)
We ﬁrst generate an arbitrary number of a's, then add an equal
number of a's and b's.
S1 →A1B1,
A1 →aA1|a,
B1 →aB1b|λ.
(f)
For L1 ∪L2, use
S →S1|S2.
Here S1 and S2 are the grammars for (a) and (b).
18. (a) The problem is simpliﬁed if you break it into two cases, |w| mod 3 =
1 and |w| mod 3 = 2 .
S →S1|S2,
S1 →aaaS1|a,
S2 →aaaS2|aa.
21. We can show by induction on the number of a's, that all sentential
forms of the grammar in Example 1.14 can be derived as
S ⇒aAb ⇒a2Ab2 ⇒a3Ab3
∗⇒anAbn,
for n = 1, 2, ..... This is so because we can use only production A →
aAb to derive a sentential form.
To get a sentence, we apply the
production A →λ to get string anbn.
So the grammar generates
strings of the form anbn for n ≥1. Since the production S →λ is in
the grammar, λ is in the language. Therefore, the grammar generates
the language L(G1) in Example 1.14.
24. The ﬁrst grammar can derive S ⇒SS
∗⇒aa, but the second grammar
cannot derive this string. So they are not equivalent.

400
Answers
Section 1.3
1.
password →Ω letter Ω digit Ω | Ω digit Ω letter Ω
letter →a|b|...|z
digit →0|1|2|3|4|5|6|7|8|9
Ω →letter Ω | digit Ω |λ.
4. An automaton that accepts all C integers is given below
+ , - or digit
digit
2
1
3
Symbol besides digits
Symbol besides + , - and digit
9. The automaton has to remember the input for one time period. Re-
membering can be done by labeling the state with the appropriate
information. The label of the state is then produced as output later.
a
λ
b/λ
b
a/λ
a/b
b/a
a/a
b/b
10. (a) Similar to Exercise 9, the automaton has to remember the input
for two time periods. So we label the states with two symbols. An
automaton for the problem is given as follows.

Solutions and Hints for Selected Exercises
401
a
ba
λ
b
bb
ab
aa
a/a
b/b
b/a
b/a
b/a
a/b
a/b
a/b
b/b
b/b
a/a
a/a
a/λ
b/λ
(b) For an n-unit delay transducer, we need to remember the input
for n time periods. We label the states mnemonically, with a1a2...an,
where ai ∈Σ. Since there are |Σ| choices for each position, there must
be at least |Σ|n states.
Chapter 2
Section 2.1
2.
δ (λ, 1) = λ,
δ (λ, 0) = 0,
δ (0, 1) = λ,
δ (0, 0) = 00,
δ (00, 0) = 00,
δ (00, 1) = 001,
δ (001, 0) = 001,
δ (001, 1) = 001.
3. (c)
q0
b
b
a
a
q1

402
Answers
4. (a)
q1
b
a
a
b
q0
q2
a,b
7. (a) Use states labeled with |w| mod 3.
1
a,b
a,b
a,b
2
0
8. (a) Use appropriate a's and b's to label the states.
b
b
b
b
a
a
a
a
a
a
b
b
b
bbb
aaa
a
b
λ
aa
t
bb
a,b
a
11. (b)
λ
λ
0
00
000
0000
00
0
1
1
1
0
0
0
0
0
0
0
1
1
1
0,1
1

Solutions and Hints for Selected Exercises
403
(c) We need to remember the leftmost symbol and put the dfa into a
ﬁnal state whenever a diﬀerent symbol is encountered
1
01
0
0
0
1
0
0
10
1
1
1
0
1
λ
12. The trick is to label the states with the value (mod 5) of the partial
bit string and take care of the next bit by
(2n + 1) mod 5 = (2n mod 5 + 1) mod 5.
This leads to the solution shown below.
0
0
0
0
0
1
1
0
1
4
1
1
1
2
3
16. Since the least common multiplier of 3 and 5 is 15, we need 15 states
to construct such a dfa.
11
13
14
12
0
10
9
4
7
8
2
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
1
3
5
6
19. The catch is that if λ is in the language, the initial state must also be
a ﬁnal state. But we cannot just make it non-ﬁnal as other input may
reach this state. To get around this diﬃculty, we modify the original
dfa by creating a new initial state p0 and new transitions
δ(p0, a) = qj

404
Answers
for all original transitions
δ(q0, a) = qj.
A graphical representation of the process is given below.
q0
qj
a
q0
a
qj
p0
a
22. L3 = {anbambapb : n, m, p ≥0}. A dfa that accepts L3 is given by
a,b
a,b
b
b
b
q4
q3
a
q2
a
q1
a
q0
Section 2.2
3. The nfa in Figure 2.8 accepts the language {an : n = 3 or n is even}.
A dfa for this language is
a
a
a
a
a
a
6. δ∗(q0, a) = {q0, q1, q2} and δ∗(q1, λ) = {q0, q1, q2}.
9.
λ,c
a
b

Solutions and Hints for Selected Exercises
405
13. 01001 and 000 are the only two strings accepted.
15. Add two states after the nfa accepts a3 with both new edges labeled a.
q6
q2
a
a
a
a
a
a
a
a
q1
q0
q4
q3
q5
q7
17. Remove the λ-edge from the graph below.
a
a
λ
19. Introduce a new initial state p0. Then add a transition
δ(p0, λ) = Q0.
Next, remove starting state status from Q0. It is straightforward to
see that the new nfa is equivalent to the original one.
22. Introduce a non-accepting trap state and make all undeﬁned transi-
tions to this new state.
a,b
a,b
b
a
a
b

406
Answers
Section 2.3
1.
a
a
{0,1,2}
{0}
For a simple answer, note that the language is {an : n > 1}.
3.
a
b
a
a
b
b
b
a
{0}
{0,1}
{1,2}
{2}
a,b
trap
7. Yes, it is true. By deﬁnition, w ∈L if and only if δ∗(q0, w) ∩F ̸= ⊘.
Consequently, if δ∗(q0, w) ∩F = ⊘, then w ∈¯L.
10.
b
a
b
b
12. Introduce a single initial state, and connect it to previous initial states
via λ-transitions. Then convert back to a dfa and note that the con-
struction of Theorem 2.2 retains the single initial state.
16. The trick is to use a dfa for L and modify it so that it remembers if it
has read an even or an odd number of symbols. This can be done by
doubling the number of states and adding O or E to the labels. For
example, if part of the dfa is

Solutions and Hints for Selected Exercises
407
a
b
a
q3
b
b
q2
q1
q0
its equivalent becomes
q3O
q2O
q1O
Q0O
q3E
q2E
q1E
a
b
b
a
a
a
b
b
b
b
q0E
Now replace every transition from an E state to an O state with
λ-transitions.
q2O
q1O
Q0O
q1E
a
b
λ
λ
q0E
A few examples will convince your that if the original dfa accepts
a1a2a3a4, the new automaton will accept λa2λa4..., and therefore
even (L).

408
Answers
Section 2.4
1. Using procedure mark, we generate the equivalence classes {q0, q1},
{q2}, {q3}. Then the procedure reduce gives
ˆδ(01, a) = 2,
ˆδ(01, b) = 2,
ˆδ(2, a) = 3,
ˆδ(2, b) = 3,
ˆδ(3, a) = 3,
ˆδ(3, b) = 01.
3. (a) The following graph is a ﬁve state dfa. It should be easy to see
that the dfa accepts the language L = {anbm : n ≥2, m ≥1}.
b
a
a
b
a
b
b
a
q2
q4
q1
q0
q3
a,b
We claim it is a minimal dfa. Because q3 ∈F and q4 /∈F, so q3 and
q4 are distinguishable. Next δ(q4, b) = q4 /∈F and δ(q2, b) = q3 ∈F,
so q2 and q4 are distinguishable. Similarly, δ∗(q0, ab) = q4 /∈F and
δ∗(q1, ab) = q3 ∈F, so q0 and q1 are distinguishable.
Continuing
this way, we see that all the ﬁve states are mutually distinguishable.
Therefore,the dfa is minimal.
6. Yes, it is true. We argue by contradiction. Assume that 
M is not
minimal. Then we can construct a smaller dfa 
M that accepts L. In

M, complement the ﬁnal state set to give a dfa for L. But this dfa is
smaller than M, contradicting the assumption that M is minimal.
9. Assume that qb and qc are indistinguishable.
Since qa and qb are
indistinguishable and indistinguishability is an equivalence relation as
shown in Exercise 7, so qa and qc must be indistinguishable. This
contradicts the assumption.

Solutions and Hints for Selected Exercises
409
Chapter 3
Section 3.1
3.
q3
q2
q1
a
b
b
a
a
q0
5. Yes, because

(0 + 1) (0 + 1)∗∗denotes any string of 0's and 1's.
7. A regular expression is aaaa∗(bb)∗b.
9. (a) Separate into cases m = 0, 1, 2, 3, 4.
Generate 3 or more a's,
followed by the requisite number of b's. A regular expression for L1
is aaaa∗(λ + b + bb + bbb + bbbb)
16. Enumerate all cases with |v| = 2 to get
aa (a + b)∗aa + ab (a + b)∗ab + ba (a + b)∗ba + bb (a + b)∗bb.
19. (a) A regular expression is (b + c)∗a(b + c)∗a(b + c)∗.
26. The graph for (rd)∗is
Section 3.2
2.
b
b
a
a
a
λ
λ
λ
λ
λ
λ
5. This can be solved from ﬁrst principles without going through the
regular expression to nfa construction.
The latter will, of course,
work but gives a more complicated answer.

410
Answers
a,b
b
b
b
a
λ
6. (a)
a
b
b
λ
λ
λ
a
a
a
7. (a)
b
b
b
b
b
b
b
b
b
q6
a
a
a
a
a
a
a
a
a
q5
q3
q1
q0
q9
q10
a,b
a,b
q4
q7
q8
q2
10. (a) Removing the middle vertex gives
bb + ab
(a + b)ab
a
q1
a
q0

Solutions and Hints for Selected Exercises
411
(b) By Equation (3.1), the language accepted is L (r), where
r = a∗(a + b)ab(bb + ab + aa∗(a + b)ab)∗.
15. (a) Label the vertices with EE to denote an even number of a's and an
even number of b's, with OE to denote an even number of a's but an
odd number of b's, and so on. Then we get the generalized transition
graph
OO
OE
EE
EO
a
b
b
a
b
b
a
a
Section 3.3
1.
a
a
b
b
a,b
a
a
b
a,b
trap
A
B
S
b
b
b
a
a
4. The language in Exercise 1 is L = {abba(aba)∗bb}. A left-linear gram-
mar can be constructed by taking the regular expression in reverse
order.
S →Abb,
A →Aaba|B,
B →abba.

412
Answers
6. It is straightforward to get a grammar for L(aaab∗ab). The closure of
this language requires only a minor modiﬁcation.
S →aaaA|λ,
A →bA|B,
B →ab|abS.
8. We can show by induction that if w is a sentential form derived with
G, then wR can be derived in the same number of steps by G.
Because w is created with left linear derivations, it must have the form
w = Aw1, with A ∈V and w1 ∈T ∗. By the inductive assumption
wR = wR
1 A can be derived via G. If we now apply a →Bv, then
w ⇒Bvw1.
But G contains the rule A →vRB, so we can make the derivation
wR →wR
1 vRB
= (Bvw1)R
completing the inductive step.
12. Draw an nfa using mnemonic labels. Then use the construction in
Theorem 3.4. An answer is
EE →aOE|bEO,
OE →aEE|bOO|λ,
OO →bEO,
EO →bOO|λ.
14. (b) Using mnemonic labels to denote even-odd pairs of a and b we get
a dfa for the problem. From the dfa, we get the grammar
EE →aOE|bEO|λ,
OE →aEE|bOO,
EO →aOO|bEE,
OO →aEO|bOE.
15. For every rule in a right-linear grammar that has more than one ter-
minal, replace terminals successively by variables and new rules. For
example, for
A →a1a2B,
introduce variable C, and new productions
A →a1C,
C →a2B.

Solutions and Hints for Selected Exercises
413
Chapter 4
Section 4.1
2. A regular expression for (L1
 L2)∗L2 is
((ab∗aa) + (a∗bba∗))∗(a∗bba∗).
4. By DeMorgan's law, L1 ∩L2 = L1 ∪L2. So if the language family
is closed under complementation and union, L1 ∩L2 must be in the
family.
9. By induction. From Theorem 4.1, we know that the union of two
regular languages is a regular language. Now suppose it is true for
any k ≤n −1,

i={1,2,...,k}
Li
is a regular language. Then
LU =

i={1,2,...,n}
Li = (

i={1,2,...,n−1}
Li)) ∪Ln
is simply a union of two regular languages. Therefore LU is a regular
language.
A similar argument for intersection of regular languages shows LI
is a regular language. So regular languages are closed under ﬁnite
intersection.
11. Notice that
nor (L1, L2) = L1 ∪L2.
The result then follows from closure under intersection and comple-
mentation.
16. The answer is yes. It can be obtained by starting from the set identity
L2 =

(L1 ∪L2) ∩L1

∪(L1 ∩L2) .
The key observation is that since L1 is ﬁnite, L1 ∩L2 is ﬁnite and
therefore regular for all L2.
The rest then follows easily from the
known closures under union and complementation.
17. If r is a regular expression for L, and s is a regular expression for Σ,
then rss is a regular expression for L1 and L1 is regular.
20. Use L1 = Σ∗. Then, for any L2, L1 ∪L2 = Σ∗, which is regular. The
given statement would then imply that any L2 is regular.
22. We can use the following construction. Find all states P such that
there is a path from the initial vertex to some element of P, and from
that element to a ﬁnal state. Then make every element of P a ﬁnal
state.

414
Answers
27. Suppose G1 = (V1, T, S1, P1) and G2 = (V2, T, S2, P2). Without loss
of generality, we can assume that V1 and V2 are disjoint. Combine the
two grammars and
(a)
Make S the new start symbol and add productions S →S1|S2.
(b)
In P1, replace every production of the form A →x, with A ∈V1
and x ∈T ∗, by A →xS2.
(c)
In P1, replace every production of the form A →x, with A ∈V1,
and x ∈T ∗, by A →xS1, S1 →λ.
Section 4.2
1. Use the dfa M for L, interchange initial and ﬁnal states, and reverse
edges to get an nfa 
M for LR. By Theorem 4.5, we can check for
w ∈LR, or equivalently for wR ∈L.
4. Since L1 −L2 is regular, by Theorem 4.5, there exists such an algo-
rithm.
7. Construct a dfa. Then λ ∈L if and only if q0 ∈F.
10. Since L∗and L are both regular, by Theorem 4.7, we have an algo-
rithm testing for equality of regular languages.
13. If there are no even length strings in L, then
L((aa + ab + ba + bb)∗) ∩L = ⊘.
Therefore, by Theorem 4.6 the result follows
17. Construct a dfa M1 for L(G1), and a dfa M2 for L(G2). Then use
the construction in Theorem 3.1 to get a nfa for L = L(G1) ∪L(G2).
Check equality for L and Σ∗
Section 4.3
1. Suppose L is regular and we are given m, then we pick w = ambcm in
L. Now because |xy| cannot be greater than m, it is clear that y = ak
is the only possible choice for some k ≥1. Then the pumped strings
are
wi = am+(i−1)kbcm ∈L,
for all i = 0, 1, ... However, w2 = am+kbcm /∈L. This contradiction
implies that L is not a regular language.
3. Use the closure under homomorphism for regular languages. Take
h(a) = a,
h(b) = a,
h(c) = c,
h(d) = c,
then
h(L) = {an+kcn+k : n + k > 0}
= {aici : i > 0}.

Solutions and Hints for Selected Exercises
415
By the argument in Example 4.7, h(L) is not regular. Therefore L is
not regular.
5. (a)
Suppose L is regular and we are given m, then we pick w =
ambmc2m in L. Now because |xy| cannot be greater than m, it is
clear that the y = ak is the only possible choice for some k ≥1.
Therefore by the pumping lemma
wi = am+(i−1)kbmc2m ∈L,
for all i = 0, 1, ... However, w0 = am−kbmc2m /∈L because m −
k + m < 2m. Therefore L is not regular.
(f)
Suppose L is regular and m is given. We pick w = ambamb in L.
The string y must then be ak and the pumped strings will be
wi = am+(i−1)kbamb ∈L,
for i = 0, 1, ... However, w0 = am−kbamb /∈L. Therefore L is not
regular.
6. (a) Suppose L is regular and m is given. Let p be the smallest prime
number such that p ≥m. Then we pick w = ap in L. The string y
must then be ak and the pumped strings will be
wi = ap+(i−1)k ∈L,
for i = 0, 1, ... However, wp+1 = ap+pk = ap(1+k) /∈L. Therefore L is
not regular.
7. (a) Since
L = {anbn : n ≥1} ∪{anbm : n ≥1, m ≥1}
= {anbm : n ≥1, m ≥1}
it is clearly regular.
12. Suppose L is regular and m is given. We pick w = am! which is in L.
The string y must then be ak for some 1 ≤k ≤m and the pumped
strings will be
wi = a(m!)+(i−1)k ∈L,
for i = 0, 1, ... However, w2 = am!+k /∈L, because m! + k ≤m! + m <
(m + 1)!. Therefore L is not regular.
18. (a)
The language is regular. This is most easily seen by splitting the
problem into cases such as l = 0, k = 0, n > 5, for which one can
easily construct regular expressions.
(e)
This language is not regular. Suppose m is given, we pick w =
ambm+1.
So, our opponent can only choose y = ak for some
1 ≤k ≤m. The pumped strings are wi = am+(i−1)kbm+1. So,
for example, w3 = am+2kbm+1 violates the required condition
since m + 1 < m + 2k.

416
Answers
24. Take Li = {aibi}, i = 0, 1, .... For each i, Li is ﬁnite and therefore
regular, but the union of all the languages is the nonregular language
L = {anbn : n ≥0}.
Chapter 5
Section 5.1
1. (b) S →aAb,
A →aaAbb|λ.
2. A derivation tree is
S
a
a
S
a
a
S
a
a
S
b
b
S
λ
9. (a)
S →aSb|A|B,
A →λ|a|aa|aaa,
B →bB|b.
(e)
Split into two parts:(i) na(w) > nb(w) and ii) nb(w) > na(w).
For part (i), generate an equal number of a's and b's then add
more a's. Do the same thing for part (ii) by adding more b's.
S →S1|S2,
S1 →S1S1|aS1b|bS1a|A,
A →aA|a,
S2 →S2S2|aS2b|bS2a|B,
B →bB|b.

Solutions and Hints for Selected Exercises
417
10. The language generated by the grammar can be described recursively
by
L = {w : w = aub or w = bua with u ∈L, λ ∈L}.
.
12. (a)
We split the problem into two parts : (i) L1 = {anbmck : n = m}
and (ii) L2 = {anbmck : m ≤k}. Then use S →S1|S2, where S1
derives L1 and S2 derives L2. A grammar is given below.
S →S1C|AS2,
S1 →aS1b|λ,
S2 →bS2c|C,
A →aA|λ,
C →cC|λ.
(e)
S →aSc|B,
B →bBcc|λ.
15.
S →aSb|S1,
S1 →aS1a|bS1b|λ.
16. (a) If S derives L, then S2 →SS derives L2.
26. The grammar has variables {S, V, R, T}, and terminals
T = {a, b, A, B, C, →}
with productions
S →V →R
R →TR|λ
V →A|B|C,
T →a|b|A|B|C|λ.
Section 5.2
2. For any w ∈L(G) with G an s-grammar, it is obvious that there is
a unique choice for the leftmost derivation. So every s-grammar is
unambiguous.
4.
S →aS1,
S1 →aAB,
A →aAB|b,
B →b.
8. Here are two leftmost derivations for w = aaab.
S ⇒aaaB ⇒aaab,
S ⇒AB ⇒AaB ⇒AaaB ⇒aaaB ⇒aaab.

418
Answers
13. From the dfa for a regular language we can get a regular grammar by
the method of Theorem 3.4. The grammar is an s-grammar except for
qf →λ. But this rule does not create any ambiguity. Since the dfa
never has a choice, there is never any choice in the production that
can be applied.
15. Yes, for example, S →aS1|ab. S1 →b. The language L(ab) is ﬁnite,
so is regular.
But S ⇒ab and S ⇒aS1 ⇒ab are two distinct
derivations for ab.
17. The string abab can be derived by either
S ⇒aSbS ⇒abS ⇒abaSbS ⇒ababS ⇒abab,
or
S ⇒aSbS ⇒abSaSbS ⇒abaSbS ⇒ababS ⇒abab.
Therefore, the grammar is ambiguous.
Chapter 6
Section 6.1
2. If we eliminate the variable B we get the second grammar.
6. First, we identify the set of variables that can lead to a terminal string.
Because S is the only such variable, A and B can be removed and we
get
S →aS|λ
This grammar derives L(a∗).
7. First we notice that every variable except C can lead to a terminal
string. Therefore, the grammar becomes
S →a |aA| B,
A →aB|λ,
B →Aa,
D →ddd.
Next we want to eliminate the variables that cannot be reached from
the start variable. Clearly, D is useless and we remove it from the
production list. The resulting grammar has no useless productions.
S →a |aA| B,
A →aB|λ,
B →Aa.

Solutions and Hints for Selected Exercises
419
10. The only nullable variable is A, so removing λ-productions gives
S →aA |a| aBB,
A →aaA|aa,
B →bC|bbC,
C →B.
C →B is the only unit-production and removing it results in
S →aA |a| aBB,
A →aaA|aa,
B →bC|bbC,
C →bC|bbC.
Finally, B and C are useless, so we get
S →aA|a,
A →aaA|aa.
The language generated by this grammar is L

(aa)∗a

.
17. An example is
S →aA,
A →BB,
B →aBb|λ.
When we remove λ-productions we get
S →aA|a,
A →BB|B,
B →aBb|ab.
21. This rather simple substitution can be justiﬁed by a straightforward
argument. Show that every string derivable by the ﬁrst grammar is
also derivable by the second, and vice versa.
25. A diﬃcult problem because the result is hard to see intuitively. The
proof can be done by showing that crucial sentential forms generated
by one grammar can also be generated by the other one. The impor-
tant step in this is to show that if
A
∗⇒Axk...xjxi ⇒yrxk...xjxi,
then with the modiﬁed grammar, we can make the derivation
A ⇒yrZ ⇒...
∗⇒yrxk...xjxi.

420
Answers
This is actually an important result, dealing with the removal of cer-
tain left-recursive productions from the grammar and is needed if one
were to discuss the general algorithm for converting a grammar into
Greibach normal form.
Section 6.2
3. Eliminate the unit-production ﬁrst: S →aSaaA|abA|bb, A →abA|bb.
Then apply Theorem 6.6.
S →VaD1|VaE1|VbVb,
D1 →SD2,
D2 →VaD3,
D3 →VaA,
E1 →VbA,
A →VaE1|VbVb,
Va →a,
Vb →b.
7.
S
C
A
B
D
8. Introduce a new variable V1 with the productions
V1 →a2...anBb1b2...bm
and
A →a1V1.
Continue this process, introducing V2 and
V2 →a3...anBb1b2...bm
and so on, until no terminals remain on the left. Then use a similar
process to remove terminals on the right.

Solutions and Hints for Selected Exercises
421
10. Introducing new variables Va →a and Vb →b, we get the Greibach
normal form immediately.
S →aSVb|bSVa|a|b|aVb,
Va →a,
Vb →b.
13. Removing the unit-production A →B in the grammar, we have new
production rule A →aaA|bAb. Next substitute the new productions
into S, to get an equivalent grammar
S →aaABb|bAbBb|a|b,
A →aaA|bAb,
B →bAb.
By introducing productions Va, Vb, we can convert the grammar into
Greibach normal form.
S →aVaABVb|bAVbBVb|a|b,
A →aVaA|bAVb,
B →bAVb,
Va →a,
Vb →b.
Section 6.3
4. First convert the grammar to Chomsky normal form
S →AC|b,
C →SB,
B →b,
A →a.
Then we can compute Vij's for the string aaabbbb from the converted
grammar.
V11 = V22 = V33 = {A},
V44 = V55 = V66 = V77 = {S, B},
V12 = V23 = V34 = ⊘,
V45 = V56 = V67 = {C},
V13 = V24 = V46 = V57 = ⊘,
V35 = {S},
V14 = V25 = V47 = ⊘,
V36 = {C},
V15 = V37 = ⊘,
V26 = {S},
V16 = ⊘,
V27 = {C},
V17 = {S}.
Since S ∈V17, the string aaabbbb is in the language generated by the
given grammar.

422
Answers
Chapter 7
Section 7.1
3. (c) Let p0 be the initial state of the new pda and q and q
′ denote the
initial states of (a) and (b), respectively. Then we set
δ(p0, λ, z) = {(q0, z), (q
′
0, z)}.
4. No need for state q1. Substitute q0 wherever q1 occurs. The major
diﬃculty is to argue convincingly that this works.
6. (a)
a , Z : aaaZ
a , a : aaaa
q0
b , a : λ
b , a : λ
λ , Z : Z
λ , Z : Z
q1
qf
(d)
Start with an initial z in the stack. Put a mark a on the stack
when input symbol is an a, consume a mark a when input is a
b. When all a's in the stack are consumed, put a mark b on the
stack when input is a b. After input becomes c, eliminate a mark
on the stack. The string will be accepted if the stack has only z
left when the input is completed.
a , Z ; aZ
a , a ; aa
b , Z ; bZ
b , a ; λ
c , b ; λ
c , b ; λ
λ , Z ; Z
b , a ; λ
b , Z ; bZ
b , b ; bb
q0
q1
q2
qf
8. At ﬁrst glance this looks like a simple problem: put w1 in the stack,
then go into a ﬁnal trap state whenever a mismatch with w2 is de-
tected. But this is incomplete if the two substrings are of unequal
length, for example, if w2 = wR
1 v. A way to solve this is to nondeter-
ministically split the problem into w1 = w2 and w1 ̸= w2.
18. Here we use internal states to remember symbols to be put on the
stack. For example,
δ (qi, a, b) = {(qj, cde)}
is replaced by
δ (qi, a, b) = {(qjc, de)},
δ (qjc, λ, d) = {(qj, cd)} .

Solutions and Hints for Selected Exercises
423
Since δ can have only a ﬁnite number of elements and each can only
add a ﬁnite amount of information to the stack, this construction can
be carried out for any pda.
Section 7.2
2. Convert the grammar into Greibach normal form.
S →aSSSA|λ,
A →aB,
B →b.
Following the construction of Theorem 7.1, we get the solution
δ (q0, λ, z) = {(q1, Sz)} ,
δ (q1, a, S) = {(q1, SSSA)} ,
δ (q1, a, A) = {(q1, B)} ,
δ (q1, λ, S) = {(q1, λ)} ,
δ (q1, b, B) = {(q1, λ)} ,
δ (q1, λ, z) = {(qf, z)} .
with F = {qf}
4. For the string aabb, the moves the npda makes are
(q0, aabb, z) ⊢(q1, aabb, Sz) ⊢(q1, abb, SAz) ⊢(q1, bb, Az)
⊢(q1, b, Bz) ⊢(q1, λ, z) ⊢(q2, λ, λ).
Since q2 is a ﬁnal state, the pda accepts the string aabb. Similarly for
aaabbbb, we ﬁnd
(q0, aaabbbb, z) ⊢(q1, aaabbbb, Sz)⊢(q1, aabbbb, SAz)⊢(q1, abbbb, SAAz)
⊢(q1, bbbb, AAz) ⊢(q1, bbb, BAz) ⊢(q1, bb, Az)
⊢(q1, b, Bz) ⊢(q1, λ, z) ⊢(q2, λ, λ).
Therefore, aaabbbb is also accepted by the pda.
6. First convert the grammar into Greibach normal form, giving S →
aSSS; S →bA; A →a. Then follow the construction of Theorem 7.1.
δ (q0, λ, z) = {(q1, Sz)} ,
δ (q1, a, S) = {(q1, SSS)} ,
δ (q1, b, S) = {(q1, A)} ,
δ (q1, a, A) = {(q1, λ)} ,
δ (q1, λ, z) = {(qf, z)} .

424
Answers
9. From Theorem 7.2, given any npda, we can construct an equivalent
context-free grammar. From that grammar we can then construct an
equivalent three-state npda, using Theorem 7.1.
13. There must be at least one a to get started. After that, δ (q0, a, A) =
{(q0, A)} simply reads a's without changing the stack. Finally, when
the ﬁrst b is encountered, the pda goes into state q1, from which it
can only make a λ-transition to the ﬁnal state. Therefore, a string
will be accepted if and only if it consists of one or more a's, followed
by a single b.
14. From the grammar in Example 7.8, we ﬁnd a derivation
(q0za2) ⇒a(q0Aq3)(q3zq2)
⇒aa(q3za2)
⇒aa(q0Aq3)(q3zq2)
⇒aaa(q0Aq3)(q3zq2)
∗⇒aa∗(q0Aq3)(q3zq2)
⇒aa∗(q3zq2)
⇒aa∗(q0Aq1)(q1zq2)
⇒aa∗(q1zq2)
⇒aa∗b.
19. The key is that terminals can largely be treated as variables.
For
example, if
A →abBBc
then the pda must have the transitions
(q1, bBBc) ∈δ(q1, a, A),
and
(q1, BBC) ∈δ(q1, b, b),
where a, b, c ∈T ∪{λ}.
Section 7.3
2. A straightforward modiﬁcation of Example 7.4. Put two tokens when
input is an a and remove only one token when the input is a b. The
solution is
δ (q0, a, z) = {(q1, AAz)} ,
δ (q1, a, A) = {(q1, AAA)} ,
δ (q1, b, A) = {(q2, λ)} ,
δ (q2, b, A) = {(q2, λ)} ,
δ (q2, λ, z) = {(q0, z)} ,
where F = {q0}.

Solutions and Hints for Selected Exercises
425
4. This dpda goes into the ﬁnal state after reading the preﬁx anbn+3. If
more b's are followed, it stays in this state and so accepts anbm for
any m > n + 3. A complete solution is given below.
δ (q0, λ, z) = {(q1, AAz)} ,
δ (q1, a, A) = {(q1, AA)} ,
δ (q1, b, A) = {(q2, λ)} ,
δ (q2, b, A) = {(q2, λ)} ,
δ (q2, b, z) = {(q3, z)} ,
δ (q3, b, z) = {(q3, z)} ,
where F = {q3}.
6. At ﬁrst glance, this may seem to be a nondeterministic language, since
the preﬁx a calls for two diﬀerent types of suﬃxes. Nevertheless, the
language is deterministic, as we can construct a dpda. This dpda goes
into a ﬁnal state when the ﬁrst input symbol is an a. If more symbols
follow, it goes out of this state and then accepts anbn. A complete
solution is
δ (q0, a, z) = {(q3, Az)} ,
δ (q3, a, A) = {(q1, AA)} ,
δ (q1, a, A) = {(q1, AA)} ,
δ (q1, b, A) = {(q2, λ)} ,
δ (q2, b, A) = {(q2, λ)} ,
δ (q2, λ, z) = {(q3, z)} ,
δ (q3, b, A) = {(q2, λ)} ,
where F = {q3}.
9. Intuitively, we can check n = m or m = k if we know which case we
want at the beginning of the string. But we have no way of deciding
this deterministically.
11. The solution is straightforward. Put a's and b's on the stack. The c
signals the switch from saving to matching, so everything can be done
deterministically.
16. Let L1 = {anbn : n ≥0} and L2 = {anb2nc : n ≥0}. Then clearly L1∪
L2 is nondeterministic. But the reverse of this is {bnan} ∪{cb2nan},
which can be recognized deterministically by looking at the ﬁrst sym-
bol.
Section 7.4
1. (c) We can rewrite strings in L as follows:
anbn+2cm = anbnb2c2cm−2 = anbnb2c2ck

426
Answers
where n ≥1, k ≥1. A grammar for L can be obtained as follows.
S →ABC,
A →aAb|ab,
B →bbcc,
C →cC|c.
We claim that the grammar is LL(2). First, the initial production
must be S →ABC. For the part anbn, if the lookahead is aa, then
we must use A →aAb; if it is ab then we can only use A →λ. For
rest, there is never a choice of production, so the grammar is LL(2).
3. Consider the strings aabb and aabbbbaa. In the ﬁrst case, the deriva-
tion must start with S ⇒aSb, while in the second S ⇒SS is the
necessary ﬁrst step. But if we see only the ﬁrst four symbols, we can-
not decide which case applies. The grammar is therefore not in LL(4).
Since similar examples can be made for arbitrarily long strings, the
grammar is not LL(k) for any k.
Chapter 8
Section 8.1
2. Given m, we pick w = amcmbm which is in L. The adversary now
has several choices. If he chooses vxy to contain only b's, or a's or
c's then the pumped string wi will obviously not be in L for some
i ≥1. Therefore, the adversary chooses v = ak, y = cl. Then the
pumped string is wi = am+(i−1)kcm+(i−1)lbm. However, if l ̸= 0, then
nc(w2) = m + l > m = nb(w0), so w2 /∈L. On the other hand, if
l = 0, then the pumped string na(w0) = m −k ̸= m = nb(w0), so
w0 /∈L. Similarly, if the adversary chooses vxy to contain c's and
b's, he cannot win either. Thus the pumping lemma fails and L is not
context free.
6. Given m, we pick w = amb2m which is in L.
Obviously, the only
choice for the adversary is v = ak, y = bl. The pumped string is
wi = am+(i−1)kb2m+(i−1)l. We claim that w0 and w2 cannot be both
in L, therefore the pumping lemma fails and L is not context free. To
show this, let us assume w0 and w2 are both in L, then we must have
2m−k = 2m −l,
and
2m+k = 2m + l.
Now multiply the above two equations side by side, we arrive at
2m−k2m+k = (2m −l)(2m + l).

Solutions and Hints for Selected Exercises
427
That is
22m = 22m −l2.
Since l = 0 implies k = 0 and this in turn implies |vy| = 0.. We have
a contradiction.
7. (c) Given m, we pick w = ambmcm2 which is in L. It is easy to see
that the only troublesome choice for the adversary is v = bk, y = cl for
k ̸= 0, l ̸= 0. Then the pumped string is wi = ambm+(i−1)kcm2+(i−1)l.
However, for w0 = ambm−kcm2−l, we have
m(m −k) = m2 −mk < m2 −l
since l ≤m and k ≥1. So w0 /∈L, so L is not context free.
8. (a)
The language is context free. A grammar for it is
S →aSb|S1,
S1 →aS1a|bS1b|λ.
(d)
The language is context free and an npda is given below.
a , A ; AA
b , A ; AA
b , A ; AA
b , Z ; AZ
a , Z ; AZ
q0
q1
a , A ; λ
a , Z ; Z
b , Z ; Z
a , A ; λ
λ , Z ; Z
λ , Z ; Z
λ , Z ; Z
q1
b , A ; λ
b , Z ; Z
a , A ; λ
q3
q4
(f)
The language is not context free. Use the pumping lemma with
w = ambmcm and examine various choices of v and y.
11. The language is context free with a grammar
S →S1S1,
S1 →aS1b|λ.
We next see if the language is linear. Given m, we pick w = ambmambm
which is in L.
Obviously, whatever the decomposition is for vxy,
it must be of the form v = ak, y = bl.
Then the pumped string
is wi = am+(i−1)kbmambm+(i−1)l for some 1 ≤k + l ≤m.
For
w0 = am−kbmambm−l /∈L .
Therefore, L is context free but not
linear.

428
Answers
Section 8.2
3. For each a ∈T, substitute it by h(a). The new set of productions gives
a context-free grammar G. A simple induction shows that L( G) =
h(L).
7. The arguments are similar to those in Theorem 8.5. Consider Σ∗−L =
L. If the context-free languages were closed under diﬀerence, then
L would always be context free, in contradiction to an established
results.
To show that the closure result holds for L2 regular, notice that regu-
larity is closed under complement. L2 is regular, so is (L2). Therefore,
L1 −L2 = L1 ∩(L2) is context free by Theorem 8.5.
9. Given two linear grammars G1 = (V1, T, S1, P1) and G2 = (V2, T, S2, P2)
with V1 ∩V2 = ⊘, form the combined grammar
G = (V1 ∪V2, T, S, P1 ∪P2 ∪S →S1|S2)
Then G is linear and L
	
G

= L (G1) ∪L (G2). To show that linear
languages are not closed under concatenation, take the linear language
L = {anbn : n ≥1}. The language L2 is not linear, as can be shown
by an application of the pumping lemma.
15. The languages L1 = {anbncm} and L2 = {anbmcm} are both unam-
biguous. But their intersection {anbncn} is not context free.
21. λ ∈L (G) if and only if S is nullable.
23. Let L1 be the given context-free language, and L2 = {w ∈Σ∗:
|w| is even}. Since L2 is regular, by Theorem 8.5, L = L1 ∩L2 is
context-free. Hence, by Theorem 8.6, there exists an algorithm for
deciding whether or not L is empty. That means there is an algorithm
to determine if L1 contains any even length.
Chapter 9
Section 9.1
1. A Turing machine that accepts the language L is
δ(q0, a) = (q1, a, R),
δ(q1, a) = (q2, a, R),
δ(q2, a) = (q3, a, R),
δ(q3, a) = (q3, a, L),
δ(q3, b) = (q4, b, R),
δ(q3, ⊔) = (q5, ⊔, L),
δ(q4, b) = (q4, b, R),
δ(q4, ⊔) = (q5, ⊔, L),
with F = {q5}

Solutions and Hints for Selected Exercises
429
4. In both cases, the Turing machine will halt in the nonﬁnal state q0.
The instantaneous descriptions are
q0aba ⊢xq1ba ⊢xq2ya ⊢q2xya ⊢xq0ya,
and
q0aaabbbb
∗
⊢xaaq2ybbb
∗
⊢xxxq2yyyb
∗
⊢xxxq0yyyb.
8. (b)
A transition graph with F = {q2} is
q2
q1
q0
b ;     , R
b ;     , R
a ;     , R
a ;     , R
a ;      , L
(f)
A transition graph with F = {q6} is
q0
q1
q2
q5
q4
q3
q6
a ;      , R
a ; b , R
b ; b , R
b ; Y , R
x ; x , R
x ; x , R
x ; x , R
y ; y , R
a ; x , L
a ; b , R
b ; b , L
b ; b , L
b ; b , L
x ; x , L
a ;      , L
10. To solve the problem, we implement the following process:
1. Replace every 0 by x and 1 by y.
2. Find the leftmost symbol of w which is not z, remember it by
entering the appropriate internal state and then replace it by z.
3. Travel left to the ﬁrst blank cell and create a 0 or 1 according
to the internal state associated with the remembered symbol.
4. Repeat step 2 and step 3 until there are no more x's and y's.
The Turing machine for the solution is somewhat complicated.

430
Answers
q3
q4
q6
z ; z , L
0 ; 0 , L
1 ; 1 , L
z ; z , L
0 ; 0 , L
1 ; 1 , L
q1
x ; x , L
y ; y , L
q0
0 ; x , R
1 ; y , R
q2
z ; z , R
x ; z , L
y ; z , L
0 ; 0 , R
1 ; 1 , R
q5
1 ; 1 , L
0 ; 0 , L
a ;      , L
a ;      , L
a ;  1 , R
a ;  0 , R
a ;      , R
a ;      , R
Z ;      , L
12. Using unary representation for w, the Turing machine for the solution
of this problem is.
δ(q0, 1) = (q1, ⊔, R),
δ(q0, ⊔) = (q4, ⊔, L),
δ(q2, 1) = (q3, 1, L),
δ(q2, ⊔= (q4, ⊔, L),
δ(q3, ⊔) = (q4, ⊔, R),
with F = {q4}.
13. (a) Use the construction in Example 9.10 to duplicate x, then add a
symbol 1 to the resulting string. The Turing machine has the following
transition functions.
δ(q0, 1) = (q0, x, R),
δ(q0, ⊔) = (q1, ⊔, L),
δ(q1, 1) = (q1, 1, L),
δ(q1, x) = (q2, 1, R),
δ(q2, 1) = (q2, 1, R),
δ(q2, ⊔) = (q1, 1, L),
δ(q1, ⊔) = (q3, 1, L),
δ(q3, ⊔) = (q4, ⊔, R),
with F = {q4}.
Section 9.2
2. Suppose we use symbol s to denote the minus sign.
For example,
s111 = −3. Also suppose our input for the unary subtraction x −

Solutions and Hints for Selected Exercises
431
y is represented by 0w(x)sw(y)0, then the following outline for the
construction for subtracter will perform the computation
q00w(x)sw(y)0
∗
⊢qf(w(x) −w(y))
if x ≥y
∗
⊢qfs(w(y) −w(x))
if x < y
with F = {qf}. The subtraction can then be achieved by
1. Repeat the following steps until either x or y contains no more 1's.
For each 1 in w(x) ﬁnd a corresponding 1 in w(y), then replace
both 1's by a token z, respectively.
2. If w(y) contains no more 1's, remove s and all the z's to get
the computed result, otherwise just remove all the z's to get the
(negative) result.
3. (a) We can think of the machine as constituted of two main parts,
an add-one machine that just adds one to the input, and a multiplier
that multiplies two numbers. Schematically they are combined in a
simple fashion.
n
n
n + 1
n + 2
n(n + 2)
add-one
add-one
multiplier
5. (a) Deﬁne the macros:
3split : convert w1w2w3 into w1xw2xw3.
reverse −compare : compare w against wR.
Then a Turing machine can be constructed by
step 1 : 3split input.
step 2 : reverse −compare w against wR followed by
reverse −compare wR against w.
Accept the input only when both steps are successful.
Chapter 10
Section 10.1
2. The oﬀ-line Turing machine is a Turing machine with transition func-
tion
δ : Q × Σ × Γ →Q × Γ × {L, R}.

432
Answers
For example,
δ(qi, a, b) = (qj, c, L)
means that the transition rule can be applied only if the machine is
in state qi, and the read-only head of the input ﬁle sees an a and the
read-write head of the tape sees a b. The symbol b on the input ﬁle
will be replaced by a c, then the read-write head will move to the left.
At this point, the input ﬁle's read-only head moves to the right and
the control unit changes its state to qj.
For the simulation by a standard Turing machine, the tape of the
simulating machine is divided into two parts; one for the input ﬁle,
the other the working tape. In any particular step, the Turing machine
ﬁnds the input part, remembers the symbol there and erases it, then
returns to its working part.
5. The machine has a transition function
δ : Q × Γ →Q × Γ × I × {L, R}
where I = {1, 2, ...}.
For example, a transition δ(qi, a) = (qj, b, 5, R) can be simulated by
the standard Turing machine as
δ(qi, a) = (qiR1, b, R)
δ(qiR1, c) = (qiR2, c, R)
.........
........
δ(qiR4, c) = (qj, c, R)
for all c ∈Σ.
10. The machine has transition function
δ : Q × Γ × Γ × Γ →Q × Γ × {L, R} .
For example, δ (qi, b, a, c) = (qj, d, R) means that the machine is in
state qi, the read-write head sees a symbol a with a b on its left and
a c in its right on the tape. Then the symbol a will be replaced with
a d and the read-write head will move to its right. At this point the
control unit changes its state to qj.
The transition δ (qi, (b, a, c)) = (qj, d, R) can be simulated by a stan-
dard Turing machine as
δ(qi, a) = (qiL, a, L)
δ(qiL, b) = (qiR, b, R)
δ(qiR, a) = (qiR, a, R)
δ(qiR, c) = (qic, c, L)
δ(qic, a) = (qj, d, R).

Solutions and Hints for Selected Exercises
433
Section 10.2
1. (c) The key is to split the input into two equal length parts. Keep
the ﬁrst part in tape 1 and move the second part into tape 2. Then
we can compare the two tapes symbol by symbol without going back
and forth like in the standard machine.
We start from the leftmost symbol on tape 1 and mark it with A for
an a and B for a b. then replace the rightmost symbol by a blank
after moving it to tape 2 in a reverse order.
Repeat this process
on tape 1 to mark the leftmost unmarked symbol and then move
the rightmost symbol onto tape 2 and replace it by blank. We split
the input strings into two equal length substrings when the process
completes successfully with no unmarked symbols remain on tape 1.
The idea is straightforward, but the implementation tends to be some-
what long. A graph with F = {q7} is shown below.
q6
q4
q1
q0
q2
q3
q5
q7
a ;      , S
a ;      , S
b ; b , R
a ; a , R
a ;      , S
a ;      , L
b ; b , R
a ; a , R
b ; b , R
a ; a , R
a ;      , S
a ;      , S
b ; b , L
a ; a , L
a ;      , S
a ;      , S
A ; A , R
B ; B , R
a ;      , S
a ;      , S
a ; a , L
b ; b , L
a ;      , S
a ;      , S
B ; b , L
A ; a , L
a ;  a  , L
a ;      , L
a ;  b  , L
b ;      , L
a ;      , R
a ;      , R
a ;      , R
a ;      , R
a ;      , S
a ;      , S
B ; b , L
A ; a , L
a ;      , S
a ;      , S
b ; B , R
a ; A , R
2. A multitape oﬀ-line Turing machine is a machine that has a single
input ﬁle with read-only head and n-tapes with each a read-write
head. The transition function has form
δ : Q × Σ × Γn →Q × Γn × {L, R}n.
For example, if n = 2, then the transition
δ(qi, a, b, c) = (qj, d, e, R, L)
means that the transition rule can be applied only if the machine is
in state qi; the read-only head of the input ﬁle sees an a and the
read-write head of tape 1 sees a b, and tape 2 sees a c. The symbol b
on tape 1 will be replaced by a d with its read-write head moving to

434
Answers
the right. At the same time, the symbol c on tape 2 will be rewritten
by an e and its read-write head will move to the left. Finally, the
read-only head moves to right and the control unit then changes its
state to qj.
The simulation of the n-tape oﬀ-line machine by a standard machine
using an n+1 tracks tape is the same as that for the (n+1)-tape Turing
machine with the only exception that the ﬁrst track corresponding
to the read-only head will always move to right and never change
its contents in any state.
The description of the simulation for a
multitape Turnig machine is given at the beginning of this section.
Chapter 11
Section 11.1
2. The union of two countable sets is countable and the set of all recur-
sively enumerable languages is countable. If the set of all languages
that are not recursively enumerable were also countable, then the set
of all languages would be countable. But this is not the case.
4. Use the pattern in Figure 11.1 to list wij, where wij is the i-th word
in Lj. This works because each Lj is enumerable.
11. A context-free language is recursive, so by Theorem 11.4 its comple-
ment is also recursive. Note, however, that the complement is not
necessarily context free.
16. If S2 −S1 is a ﬁnite set, then it is countable. This means that S2 =
(S2 −S1) ∪S1 must be countable. This is impossible because S2 is
known not countable. Therefore, S2 must contain an inﬁnite number
of elements that are not in S1.
19. Since every positive rational number can be represented by two inte-
gers we know from the enumeration procedure shown in Figure 10.1
that the set of positive rational numbers is countable. So is the set
of non-positive rational numbers. Therefore, the the set of rational
numbers is countable.
Section 11.2
1. A typical derivation:
S ⇒S1B ⇒aS1bB
∗⇒anS1bnB ⇒an−1aS1bbn−1B
⇒an−1aabn−1B ⇒an+1bn−2bbB
∗⇒an+1bn−2b2m−1bB ⇒an+1bn−1b2m.
From this, it is not hard to conjecture that L(M) = {an+1bn−1b2m :
n ≥1, m ≥0}.

Solutions and Hints for Selected Exercises
435
3. Formally, the grammar can be described by G = (V, S, T, P), with
S ⊆(V ∪T)+ and
L (G) =

x ∈T ∗: s
∗⇒G x for any s ∈S

.
The unrestricted grammars in Deﬁnition 11.3 are equivalent to this
extension because to any given unrestricted grammar we can always
add starting rules S0 →si for all si ∈S.
6. To get this form of unrestricted grammars, insert dummy variables on
the right whenever |u| > |v|. For example,
AB →C
can be replaced by
AB →CD,
D →λ.
The equivalence argument is straightforward.
Section 11.3
2. (a) A context-sensitive grammar is
S →aaAbbc|aab,
Ab →bA,
Ac →Bcc,
bB →Bb,
aB →aaAb,
aA →aa.
3. (a) The solution can be made more intuitive by using variables sub-
scripted with the terminal they are to create. An answer is
S →SVaVbVc|VaVbVc,
VaVb →VbVa,
VaVc →VcVa,
VbVa →VaVb,
VbVc →VcVb,
VcVa →VaVc,
VcVb →VbVc,
Va →a,
Vb →b,
Vc →c.

436
Answers
Chapter 12
Section 12.1
2. 
M ﬁrst saves its input and replaces it with w, then proceeds like M.
If (M, w) halts, 
M checks its original input and accepts it if it is of
even length.
3. Given M and w, modify M to get 
M, which halts if and only if a
special symbol, say an introduced symbol #, is written. We can do
this by changing the halting conﬁgurations of M so that every halting
conﬁguration writes #, then stops. Thus, M halts implies that 
M
writes #, and 
M writes # implies that M halts. Thus, if we have
an algorithm that tells us whether or not a speciﬁed symbol a is ever
written, we apply it to 
M with a = #. This would solve the halting
problem.
9. Yes, this is decidable. The only way a dpda can go into an inﬁnite
loop is via λ-transitions. We can ﬁnd out if there is a sequence of
λ-transitions that (a) eventually produce the same state and stack
top and (b) do not decrease the length of the stack. If there is no
such sequence, then the dpda must halt. If there is such a sequence,
determine whether the conﬁguration that starts it is reachable. Since,
for any given w, we need only analyze a ﬁnite number of moves, this
can always be done
Section 12.2
4. Suppose we had an algorithm to decide whether or not L (M1) ⊆
L (M2). We could then construct a machine M2 such that L (M2) = ⊘
and apply the algorithm.
Then L (M1) ⊆L (M2) if and only if
L (M1) = ⊘. But this contradicts Theorem 12.3, since we can con-
struct M1 from any given grammar G.
7. If we take L (G2) = Σ∗, the problem becomes the question of Theorem
12.3 and is therefore undecidable.
9. Again, the same type of argument as in Theorem 12.4 and Example
12.4, but there are several steps involved that make the problem a
little more diﬃcult.
(a) Start with the halting problem (M, w).
(b) Given any G2 with L(G2) ̸= ⊘, generate some v ∈L(G2).
This can always be done since G2 is regular.
(c) As in Theorem 12.4, modify M to 
M so that if (M, w)
halts, then 
M accepts v.
(d) From 
M generates G1, so that L(
M) = L(G1).

Solutions and Hints for Selected Exercises
437
Now, if (M, w) halts, then 
M accepts v and L(G1) ∩L(G2) ̸= ⊘.
If (M, w) does not halt, then 
M accepts nothing, so that L(G1) ∩
L(G2) = ⊘. This construction can be done for any G2, so that there
cannot exist any G2 for which the problem is decidable.
Section 12.3
2. A PC-solution is w3w4w1 = v3v4v1. There is no MPC solution because
one string would have a preﬁx 001, the other 01.
6. (a) The problem is undecidable. If it were decidable, we would have an
algorithm for deciding the original MPC problem. Given w1, w2..., wn,
we form wR
1 , wR
2 ..., wR
n
and use the assumed algorithm.
Since
w1wi...wk =

wR
k ...wR
i wR
1
R, the original MPC problem has a solution
if and only if the new MPC problem has a solution.
Section 12.5
2.
•
On a standard machine. Find the middle of the string, then
go back and forth to match symbols.
Both parts take O(n2)
moves.
•
On a two-tape deterministic machine. Count the number of
symbols. If the count is done in unary, this is an O(n) operation.
Next, write the ﬁrst half on the second tape. This is also an O(n)
operation. Finally, you can compare in O(n) moves.
•
On a single-tape nondeterministic machine. You can guess
the middle of the string nondeterministically in one move. But
the matching still takes O(n2) moves.
•
On a two-tape nondeterministic machine. Guess the mid-
dle of the string, then proceed as in a two-tape deterministic
machine. Total eﬀort required is O(n).
Chapter 13
Section 13.1
2. Using the function subtr in Example 13.3, we get the solution
greater (x, y) = subtr (1, subtr (1, subtr (x, y))) .
4. Using the function mult in Example 13.2 and subtr in Example 13.3,
we get the solution
equals (x, y) = mult(subtr (1, subtr(x, y)), subtr (1, subtr (y, x))) .

438
Answers
8. The function can be deﬁned as
f(0) = 1,
f(n + 1) = mult(2, f(n)).
12. (a) Direct derivation from the deﬁnition of Ackermann's function gives
A (1, y) = A (0, A (1, y −1))
= A (1, y −1) + 1
= A (1, y −2) + 2
...
= A (1, 0) + y
= y + 2.
Section 13.2
1. (a) λ, ab, aababb, aaababbaababbb
8. The ﬁrst few sentences derived from the axiom are found as
ab, a(ab)2, a(a(ab)2)2, a(a(a(ab)2)2)2....
Therefore, the language is
L = {ab} ∪L1
where
L1 = {(an(an−1...(a1(ab)k1)...)kn−1)kn : ki = 2, ai = a, i ≤n = 1, 2...}.
Section 13.3
1.
P1 : S →S1S2,
P2 : S1 →aS1, S2 →bS2c,
P3 : S1 →λ, S2 →λ.
6. The solution here is reminiscent of the use of messengers with context-
sensitive grammars.
ab →x,
xb →bx,
xc →λ.

REFERENCES
FOR FURTHER READING
A. V. Aho and J. D. Ullman. 1972. The Theory of Parsing, Translation,
and Compiling. Vol. 1. Englewood Cliﬀs, N.J.: Prentice Hall.
P. J. Denning, J. B. Dennis, and J. E. Qualitz. 1978. Machines, Languages,
and Computation. Englewood Cliﬀs, N.J.: Prentice Hall.
M. R. Garey and D. Johnson. 1979. Computers and Intractability. New
York: Freeman.
M. A. Harrison. 1978. Introduction to Formal Language Theory. Reading,
Mass.: Addison-Wesley.
J. E. Hopcroft and J. D. Ullman. 1979. Introduction to Automata Theory,
Languages and Computation. Reading, Mass.: Addison-Wesley.
R. Hunter. 1981. The Design and Construction of Compilers. Chichester,
New York: John Wiley.
R. Johnsonbaugh. 1996. Discrete Mathematics. Fourth Ed. New York:
Macmillan.
Z. Kohavi and N. K. Jha. 2010. Switching and Finite Automata Theory.
Third Edition. New York: Cambridge University Press.
C. H. Papadimitriou. 1994. Computational Complexity. Reading, Mass.:
Addison-Wesley.
G. E. Revesz. 1983. Introduction to Formal Languages. New York: McGraw-
Hill.
A. Salomaa. 1973. Formal Languages. New York: Academic Press.
A. Salomaa.
1985.
"Computations and Automata," in Encyclopedia of
Mathematics and Its Applications. Cambridge: Cambridge University Press.
439


INDEX
Note: Page numbers followed by f indicate material in ﬁgures.
A
abstract, 395
accepter, 28
Ackermann's function, 342-343
algorithms, 3, 257
for blank-tape halting
problem, 317f
Markov, 351-354
membership, 320f, 327f
alphabets, 17
ambiguity, 140-149
in grammars and languages,
145-149
inherent, 148-149
automata, 2, 17, 26-28
conﬁguration, 27
control unit, 27
deterministic, 27-28
general characteristics, 26-28
internal states, 27
nondeterministic, 28
automata classes, equivalence of,
260-261
automaton, 2
axioms, 346, 348
B
Backus-Naur form (BNF), 151,
152
base of cycle, 9
basis for induction, 10, 11
binary adder, 33
binary tree, 10-11
blank, 233
blank-tape halting problem,
315-316
algorithm for, 317
BNF. See Backus-Naur form
Boolean operators, 358
brute force parsing, 141
C
Cartesian product of sets, 5
child-parent relation in tree, 9
Chomsky hierarchy, 305-307,
306f, 362-365
Chomsky normal form, 156,
171-174
Church-Turing thesis, 337
Church's thesis, 337
441

442
Index
clique problem (CLIQ), 369, 374
closure
of context-free languages,
223-229
positive, 20
star, 20
closure properties of regular
languages
other operations, 105-111
simple set operations, 102-105
closure question, 101-102
CNF. See conjunctive normal
form
complement of set, 4
complete systems, 336
complexity, 355
classes, 362-365
space, 355
time, 355
Turing machine models,
358-360
complexity class NP, 365-367
complexity class P, 365-367
composition, 338
computability, 310-311
computable function, 341, 342
computation models, 238, 337
computational complexity,
355-356
complexity classes P and NP,
365-367
eﬃciency of computation,
356-357
language families and
complexity classes, 362-365
NP-complete problems,
373-374
NP problems, 367-370
polynomial-time reduction,
370-373
Turing machine models,
358-360
concatenation
of languages, 20
of strings, 17
conﬁguration of automaton, 27
conjunctive normal form (CNF),
359
consistent systems, 336
context-free grammars, 130-138,
151-153, 155-156
membership algorithm for,
178-180
methods for transforming
grammars, 156-167
context-free languages, 129-153,
191-201, 306-307, 307f
deterministic, 203-206
properties, 213
closure properties and decision
algorithms for, 223-229
two pumping lemmas, 214-221
theorem of, 193-195, 200-201
undecidable problems for,
329-332
context-sensitive grammars,
299-304
context-sensitive languages,
300-302
recursive vs., 302-304
contradiction, proof techniques
by, 13-14
control unit of automaton, 27
Cook-Karp thesis, 366
Cook's theorem, 374
countable sets, 278
cycles in graph, 9
CYK algorithm, 178-180
D
dead conﬁguration, 55
decidability, 310-311
deciding a language, 275
decision algorithms for
context-free languages,
223-229
DeMorgan's laws, 4, 104
dependency graphs, 160
derivation, 22
leftmost, 133-134

Index
443
rightmost, 133-134
sentential forms, 22
derivation trees, 134-136, 135f
partial, 135
sentential forms and, 137
yield, 136
derives, 22
determinism, 27-28
deterministic algorithms, 56
deterministic context-free
languages
deﬁnition, 203
grammars for, 207-211
deterministic ﬁnite accepters
(dfa), 38-48, 89
equivalence of, 58-64
extended transition function
for, 40
languages and, 41-45
reduction of number of states
in, 66-71
regular languages, 46-48
and transition graphs, 39-41
deterministic pushdown accepter
(dpda), 203
deterministic pushdown
automata, 203-206
dfa. See deterministic ﬁnite
accepters
diagonalization, 289
diﬀerence, set operations, 4
digital computers, 56
disjoint sets, 5
distinguishable states, 67, 68
domain of function, 6, 337
dpda, 203-206
E
edge of graph, 8
eﬃciency of computation,
356-357
eﬃciency of question, 332-333
elementary questions, regular
languages, 114-115
ellipses, 4
empty set, 4
end markers for an lba, 281
enumeration procedure, 279
equivalence
of automata classes, 260-261
classes, 8
of dfa's and nfa's, 58-64
of grammars, 26
Mealy and Moore machines,
382-386
of regular expressions, 78
relation, 7
exhaustive search parsing, 141,
142
extended output function, 382
extended transition function
for dfa's, 40
for ﬁnite-state transducers, 382
for nfa's, 53-55
F
family of languages, 46
ﬁnal states, 39
ﬁnal vertices, 40
ﬁnite automata, 37-71, 88, 95
ﬁnite sets, 5
ﬁnite-state transducers (fst's),
377-378
Mealy machine, 378-380
Moore machine, 380-381
formal languages, 3, 30
formal languages theory, 151
fst's. See ﬁnite-state transducers
functions, 6-8
Ackermann's, 342-343
computable, 243, 342
concept of, 337, 344
domain, 6, 337
μ recursive, 338
partial, 6
predecessor, 340
primitive recursive, 338-341
projector, 338
range, 6, 337
successor, 338

444
Index
functions (Cont.)
total, 6
zero, 338
G
game-playing program, 56
generalized transition graphs
(GTG), 83-87
G¨odel, K., 336-337
grammars, 17, 20-26, 30
ambiguity in, 145-149
context-free, 130-138
context-sensitive, 299-304
for deterministic context-free
languages, 207-211
equivalent, 26
example, 195-196
heart of, 21
left-linear, 92, 97
linear, 93
matrix, 350-351
productions of, 21
regular, 92
right-linear, 92-99
simple, 144-145
unrestricted, 293-298
graphs, 8-9
labeled, 8-9
Greibach normal form, 156,
174-176, 191-193, 195, 196
GTG. See generalized transition
graphs
H
halt state of a Turing machine,
234
halting problem, 311
algorithm for, 316f
Hamiltonian path problem
(HAMPATH), 368, 374
height of tree, 9
hierarchy of formal languages
Chomsky hierarchy, 305-307,
306f
context-sensitive grammars
and languages, 299-304
recursive and recursively
enumerable languages,
286-292
Turing machines, 285-286
unrestricted grammars,
293-298
homomorphic image, 105
homomorphism, 105
I
incompleteness theorem, 337
indistinguishable states, 67
induction, proof techniques by,
10-13
inductive assumption, 10
inductive reasoning, 11
inductive step, 10
inﬁnite loop, 236
inﬁnite sets, 5
inherently ambiguous language,
148-149
initial state, 39
initial vertex, 40
input alphabet, 39, 377
input ﬁle, 27
instantaneous description
of a pushdown automaton, 186
of a Turing machine, 236-237
internal states, 39
of automaton, 27
intersection, set operations, 4
intractable problems, 366
irrational number, 13
J
JFLAP, 395-396
L
L-systems, 353-354
λ-productions, 163-165
language families, 362-365
languages, 17-20
accepted by a dfa, 41-45

Index
445
accepted by a pda, 192,
199-200
accepted by a Turing machine,
239-242
accepted by an lba, 282-283
accepted by an nfa, 55
accepted by an npda, 187-189
ambiguity in, 145-149
associated with regular
expressions, 75-78
complement of, 19
concatenation of, 20
context-free, 129-153
generated by a grammar, 22
generated by a Markov
algorithm, 352-353
generated by Post system,
347-349
positive closure, 20
regular, 37, 46-48
reverse of, 19
star-closure of, 20
lba, 281
leaves of tree, 9
left-end marker, 281
left-linear grammars, 92, 97
leftmost derivation, 133-134
limitations of ﬁnite-state
transducers, 392-394
linear bounded automata,
281-283, 300-302
linear grammar, 93
linear languages, pumping
lemma for, 219-221
linear time parsing algorithm,
144
literals, 359
LL grammars, 152, 208
loop, 9
LR grammars, 152, 211
M
macroinstruction, 251-252
Markov algorithms, 351-354
mathematical preliminaries and
notation
functions and relations, 6-8
graphs and trees, 8-9
proof techniques, 9-14
sets, 3-6
matrix grammar, 350-351
Mealy machines, 378-380
equivalence, 382-386
minimization, 386-391
membership algorithm, 129,
320f, 327f
for context-free languages,
178-180
for context-sensitive languages,
363
parsing and, 141-145
for regular languages, 114
minimal dfa, 69
minimalization operator, 343
minor variations on Turing
machine
equivalence of automata
classes, 260-261
oﬀ-line Turing machine,
265-267
Turing machines with
semi-inﬁnite tape, 263-265
Turing machines with
stay-option, 261-263
μ-recursive functions, 343-344
modiﬁed Post correspondence
problem, 324
monus, 339
Moore machines, 380-381
equivalence, 382-386
minimization of, 391-392
movement of automaton, 27
MPC algorithm, 328f
MPC solution, 324
multidimensional Turing
machines, 271-272
multiple tracks, 263
multitape Turing machines,
268-271

446
Index
N
next-state function, 27
nfa. See nondeterministic ﬁnite
accepters
noncontracting grammars, 300
nondeterminism, 37, 51, 52,
55-56, 396
nondeterministic algorithm, 56
nondeterministic automaton, 28
nondeterministic ﬁnite accepters
(nfa), 51-56, 80
equivalence of, 58-64
extended transition function
for, 53-55
nondeterministic pushdown
accepter (npda), 183
example of, 184-185, 198-199
transition graph in, 185
nondeterministic pushdown
automata, 182-189
nondeterministic Turing
machines, 273-275
nonintuitive, 396
nonregular languages
pumping lemma, 118-125
using the pigeonhole principle,
117-118
nonterminal constants, 346
normal forms
of grammar, 155-156
importance of, 171-176
NP-complete problems, 373-374
NP problems, 367-370
npda, 183
null set, 4
nullable, 163
O
oﬀ-line Turing machine, 265-267
Ogden's lemma, 223
order
of magnitude notation, 6, 7
relation in tree, 9
order at least notation,
functions, 6
order at most notation,
functions, 6
output alphabet, 377
P
parse tree, 134-136
parsing, 129, 130, 140-149
brute force, 141
of context-free grammars, 178
exhaustive search, 141
and membership algorithm,
141-145
top-down, 141
partial derivation tree, 135
partial function, 6
partition, 6
partition algorithm, 389
path
in graph, 9
labeled, 9
simple, 9
pattern matching, 88-89
PC algorithm, 331f
PC-solution. See post
correspondence solution
phrase-structure grammars, 350
pigeonhole principle, 388
using, 117-118
polynomial-time reduction,
370-373
positive closure of language, 20
post correspondence problem,
323-328
post correspondence solution
(PC-solution), 323
post systems, 346-349
powerset, 5
predecessor function, 340
preﬁx, 18
primitive recursion, 338
primitive recursive functions,
338-341
primitive regular expressions, 74
productions of grammar, 21

Index
447
program of a Turing machine,
234
programming languages, 31-33,
130, 151-153
projector function, 338
proof techniques, 9-14
contradiction, 9, 13-14
induction, 9, 10-13
proper order, 279
proper subset, 5
pumping lemma
for context-free languages,
214-218
for linear languages, 219-221
for regular languages, 118-125
pushdown automata (pda),
181-182, 182f
and context-free languages,
191-201
deﬁnition of, 183
deterministic, 203-206
grammars for deterministic
context-free languages,
207-211
language accepted by, 186
nondeterministic, 182-189
R
range of function, 6, 337
rational number, 13
read-write head, 232
recursion of function, 11
recursive functions, 337-338
primitive, 338-341
recursive languages, 286-292
vs. context-sensitive, 302-304
deﬁnition of, 287
recursively enumerable
languages, 286-292
deﬁnition of, 286
reduction
of number of states in dfa,
66-71
polynomial-time, 370-373
of undecidable problems,
315-318
reﬂexivity rule for equivalence
relation, 7
regular expressions
deﬁnition of, 74
language associated with,
74-78
and regular languages, 80-91
for simple patterns, 88-89
regular grammars, 92. See also
grammars
equivalence of, 97-99
regular intersection, 226
regular languages, 37, 46-48,
130, 131
equivalence of, 97-99
regular expressions and, 80-91
right-linear grammars for,
93-97
regular languages properties, 101
closure properties of. See
closure properties of regular
languages
elementary questions,
114-115
nonregular languages,
117-125
relations, 7
reprogrammable Turing machine.
See universal Turing
machine
reverse
of language, 19
of string, 17
rewriting systems, 350-354
Rice's theorem, 321
right-end marker, 281
right-linear grammars, 92
for regular languages, 93-99
right quotient of a language, 106,
107
rightmost derivation, 133-134
root of tree, 9

448
Index
S
s-grammars, 144-145, 208
satisﬁablity problem (SAT), 358
3SAT, 371-373
semantics of programming
language, 153
semi-inﬁnite tape, Turing
machines with, 263-265
sentence, 3, 19
sentential forms, 22, 131
and derivation trees, 137
serial adder, 34, 34f
set operations, 4
sets, 3-6
countable, 278
empty, 4
inﬁnite, 5
null, 4
power, 5
size, 5
uncountable, 278
simple set operations, 102-105
simulation, 261
single-tape Turing machine,
332-333
space complexity, 355
stack, 183
alphabet, 183
start symbol, 183
standard representation of a
regular language, 114
standard Turing machine, 232,
236
deﬁnition of, 232-239
language accepters, 239-242
transducers, 242-247
star-closure of language, 20
state-entry problem, 315
stay-option, Turing machines
with, 261-263
storage device, 27
Turing machines with complex
multidimensional Turing
machines, 271-272
multitape Turing machines,
268-271
storage of automaton, 27
string, 17
concatenation of, 17
empty, 17
length, 17
operations, 17
preﬁx, 18
reverse of, 17
suﬃx, 18
subprograms, 252
subset, 4
proper, 5
substitution rule, 157-159
substring, 18
successive strings, 22
successor function, 338
suﬃx, 18
symmetry rule for equivalence
relation, 7
syntax of programming language,
152
T
tape alphabet, 233
tape of a Turing machine, 232
terminal constants, 346
terminal symbol, 21
theory of computation, 1-3
applications, 30-34
automata, 26-28
grammars, 20-26
languages, 17-20
mathematical preliminaries
and notation, 3-14
three-satisﬁability problem
(3SAT), 371-373
time complexity, 355
top-down parsing, 141
total function, 6
tracks on a tape, 263
tractable problems, 366, 367
transducer, 28, 33, 242-247

Index
449
transforming grammars, methods
for, 156-167
transition function, 27, 39, 233
extended, 40, 53
for nfa's, 97
transition graphs, 245f, 379
of ﬁnite accepter, 39-41
generalized, 83-87
of Moore machines, 380
of a pushdown automaton, 185
of a Turing machine, 235, 235f
transitivity rule for equivalence
relation, 7
trap state, 42
trees, 8-9
Turing-computable, 243
Turing machine, 231-232,
259-260
with complex storage device,
268-272
instruction set of, 255
intuitive visualization of, 233f
linear bounded automata,
281-283
minor variations. See minor
variations on Turing
machine
multidimensional, 271-272
with multiple tracks, 263
multitape, 268-271
nondeterministic, 273-275
oﬀ-line, 265-267
with semi-inﬁnite tape,
263-265
standard, 232-247
with stay-option, 261-263
tasks, 249-253
universal, 276-280
Turing machine halting problem,
311-315
Turing machine models, 358-361
Turing machines, 285-286
problems of, 310-318
computability and decidability,
310-311
Turing machine halting problem,
311-315
undecidable problem, 315-318
Turing's thesis, 232, 255-260, 273
two-dimensional Turing machine,
271
U
uncountable sets, 278
undecidable, 310
undecidable problems for
context-free languages,
329-332
union, set operations, 4
unit-productions, 165-167
universal set, 4
universal Turing machine,
276-280
unrestricted grammars, 293-298
useless productions, 159-163
V
variables
of grammar, 21
nullable, 163
start, 21
useless, 159
vertex
ﬁnal, 40
of graph, 8
initial, 40
W
walk in graph, 9
Y
yield of a derivation tree, 136
Z
zero function, 338


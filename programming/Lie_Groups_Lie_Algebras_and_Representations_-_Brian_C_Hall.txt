Graduate Texts in Mathematics
Brian Hall
Lie Groups, Lie 
Algebras, and 
Representations
An Elementary Introduction
Second Edition

Graduate Texts in Mathematics 222

Graduate Texts in Mathematics
Series Editors:
Sheldon Axler
San Francisco State University, San Francisco, CA, USA
Kenneth Ribet
University of California, Berkeley, CA, USA
Advisory Board:
Alejandro Adem, University of British Columbia
David Jerison, University of California Berkeley & MSRI
Irene M. Gamba, The University of Texas at Austin
Jeffrey C. Lagarias, University of Michigan
Ken Ono, Emory University
Jeremy Quastel, University of Toronto
Fadil Santosa, University of Minnesota
Barry Simon, California Institute of Technology
Graduate Texts in Mathematics bridge the gap between passive study and
creative understanding, offering graduate-level introductions to advanced topics
in mathematics. The volumes are carefully written as teaching aids and highlight
characteristic features of the theory. Although these books are frequently used as
textbooks in graduate courses, they are also suitable for individual study.
More information about this series at http://www.springer.com/series/136

Brian Hall
Lie Groups, Lie Algebras,
and Representations
An Elementary Introduction
Second Edition
123

Brian Hall
Department of Mathematics
University of Notre Dame
Notre Dame, IN, USA
ISSN 0072-5285
ISSN 2197-5612
(electronic)
Graduate Texts in Mathematics
ISBN 978-3-319-13466-6
ISBN 978-3-319-13467-3
(eBook)
DOI 10.1007/978-3-319-13467-3
Library of Congress Control Number: 2015935277
Springer Cham Heidelberg New York Dordrecht London
© Springer International Publishing Switzerland 2003, 2015
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, express or implied, with respect to the material contained herein or for any
errors or omissions that may have been made.
Printed on acid-free paper
Springer International Publishing AG Switzerland is part of Springer Science+Business Media (www.
springer.com)

For Carla


Contents
Part I
General Theory
1
Matrix Lie Groups..........................................................
3
1.1
Deﬁnitions ............................................................
3
1.2
Examples .............................................................
5
1.3
Topological Properties ...............................................
16
1.4
Homomorphisms .....................................................
21
1.5
Lie Groups ............................................................
25
1.6
Exercises ..............................................................
26
2
The Matrix Exponential ...................................................
31
2.1
The Exponential of a Matrix .........................................
31
2.2
Computing the Exponential..........................................
34
2.3
The Matrix Logarithm ...............................................
36
2.4
Further Properties of the Exponential ...............................
40
2.5
The Polar Decomposition ............................................
42
2.6
Exercises ..............................................................
46
3
Lie Algebras .................................................................
49
3.1
Deﬁnitions and First Examples ......................................
49
3.2
Simple, Solvable, and Nilpotent Lie Algebras ......................
53
3.3
The Lie Algebra of a Matrix Lie Group.............................
55
3.4
Examples .............................................................
57
3.5
Lie Group and Lie Algebra Homomorphisms ......................
60
3.6
The Complexiﬁcation of a Real Lie Algebra .......................
65
3.7
The Exponential Map ................................................
67
3.8
Consequences of Theorem 3.42 .....................................
70
3.9
Exercises ..............................................................
73
4
Basic Representation Theory..............................................
77
4.1
Representations.......................................................
77
4.2
Examples of Representations ........................................
81
4.3
New Representations from Old ......................................
84
vii

viii
Contents
4.4
Complete Reducibility ...............................................
90
4.5
Schur's Lemma .......................................................
94
4.6
Representations of sl.2I C/ ..........................................
96
4.7
Group Versus Lie Algebra Representations ......................... 101
4.8
A Nonmatrix Lie Group.............................................. 103
4.9
Exercises .............................................................. 105
5
The Baker-Campbell-Hausdorff Formula and Its Consequences .... 109
5.1
The "Hard" Questions................................................ 109
5.2
An Illustrative Example .............................................. 110
5.3
The Baker-Campbell-Hausdorff Formula .......................... 113
5.4
The Derivative of the Exponential Map ............................. 114
5.5
Proof of the BCH Formula........................................... 117
5.6
The Series Form of the BCH Formula .............................. 118
5.7
Group Versus Lie Algebra Homomorphisms ....................... 119
5.8
Universal Covers ..................................................... 126
5.9
Subgroups and Subalgebras.......................................... 128
5.10
Lie's Third Theorem ................................................. 135
5.11
Exercises .............................................................. 135
Part II
Semisimple Lie Algebras
6
The Representations of sl.3I C/........................................... 141
6.1
Preliminaries.......................................................... 141
6.2
Weights and Roots ................................................... 142
6.3
The Theorem of the Highest Weight ................................ 146
6.4
Proof of the Theorem ................................................ 148
6.5
An Example: Highest Weight .1; 1/ ................................. 153
6.6
The Weyl Group ...................................................... 154
6.7
Weight Diagrams ..................................................... 158
6.8
Further Properties of the Representations ........................... 159
6.9
Exercises .............................................................. 165
7
Semisimple Lie Algebras................................................... 169
7.1
Semisimple and Reductive Lie Algebras............................ 169
7.2
Cartan Subalgebras................................................... 174
7.3
Roots and Root Spaces............................................... 176
7.4
The Weyl Group ...................................................... 182
7.5
Root Systems ......................................................... 183
7.6
Simple Lie Algebras ................................................. 185
7.7
The Root Systems of the Classical Lie Algebras ................... 188
7.8
Exercises .............................................................. 193
8
Root Systems ................................................................ 197
8.1
Abstract Root Systems ............................................... 197
8.2
Examples in Rank Two............................................... 201
8.3
Duality ................................................................ 204

Contents
ix
8.4
Bases and Weyl Chambers ........................................... 206
8.5
Weyl Chambers and the Weyl Group................................ 212
8.6
Dynkin Diagrams..................................................... 216
8.7
Integral and Dominant Integral Elements ........................... 218
8.8
The Partial Ordering ................................................. 221
8.9
Examples in Rank Three ............................................. 228
8.10
The Classical Root Systems ......................................... 232
8.11
The Classiﬁcation .................................................... 236
8.12
Exercises .............................................................. 238
9
Representations of Semisimple Lie Algebras ............................ 241
9.1
Weights of Representations .......................................... 241
9.2
Introduction to Verma Modules...................................... 244
9.3
Universal Enveloping Algebras...................................... 246
9.4
Proof of the PBW Theorem.......................................... 250
9.5
Construction of Verma Modules..................................... 254
9.6
Irreducible Quotient Modules ....................................... 257
9.7
Finite-Dimensional Quotient Modules .............................. 260
9.8
Exercises .............................................................. 263
10
Further Properties of the Representations............................... 265
10.1
The Structure of the Weights ........................................ 265
10.2
The Casimir Element................................................. 269
10.3
Complete Reducibility ............................................... 273
10.4
The Weyl Character Formula ........................................ 275
10.5
The Weyl Dimension Formula....................................... 281
10.6
The Kostant Multiplicity Formula ................................... 287
10.7
The Character Formula for Verma Modules ........................ 294
10.8
Proof of the Character Formula...................................... 295
10.9
Exercises .............................................................. 303
Part III
Compact Lie Groups
11
Compact Lie Groups and Maximal Tori ................................. 307
11.1
Tori .................................................................... 308
11.2
Maximal Tori and the Weyl Group .................................. 312
11.3
Mapping Degrees..................................................... 315
11.4
Quotient Manifolds................................................... 321
11.5
Proof of the Torus Theorem ......................................... 326
11.6
The Weyl Integral Formula .......................................... 330
11.7
Roots and the Structure of the Weyl Group ......................... 333
11.8
Exercises .............................................................. 339
12
The Compact Group Approach to Representation Theory ............ 343
12.1
Representations....................................................... 343
12.2
Analytically Integral Elements ...................................... 346
12.3
Orthonormality and Completeness for Characters.................. 351

x
Contents
12.4
The Analytic Proof of the Weyl Character Formula ................ 357
12.5
Constructing the Representations.................................... 361
12.6
The Case in Which ı is Not Analytically Integral .................. 366
12.7
Exercises .............................................................. 369
13
Fundamental Groups of Compact Lie Groups .......................... 371
13.1
The Fundamental Group ............................................. 371
13.2
Fundamental Groups of Compact Classical Groups ................ 373
13.3
Fundamental Groups of Noncompact Classical Groups............ 377
13.4
The Fundamental Groups of K and T .............................. 377
13.5
Regular Elements..................................................... 383
13.6
The Stiefel Diagram.................................................. 389
13.7
Proofs of the Main Theorems........................................ 394
13.8
The Center of K ...................................................... 399
13.9
Exercises .............................................................. 403
A
Linear Algebra Review .................................................... 407
A.1
Eigenvectors and Eigenvalues ....................................... 407
A.2
Diagonalization....................................................... 408
A.3
Generalized Eigenvectors and the SN Decomposition............. 409
A.4
The Jordan Canonical Form ......................................... 411
A.5
The Trace ............................................................. 411
A.6
Inner Products ........................................................ 412
A.7
Dual Spaces........................................................... 414
A.8
Simultaneous Diagonalization ....................................... 415
B
Differential Forms .......................................................... 419
C
Clebsch-Gordan Theory and the Wigner-Eckart Theorem........... 425
C.1
Tensor Products of sl.2I C/ Representations........................ 425
C.2
The Wigner-Eckart Theorem ........................................ 428
C.3
More on Vector Operators ........................................... 432
D
Completeness of Characters............................................... 435
References......................................................................... 443
Index ............................................................................... 445

Preface
This text treats Lie groups, Lie algebras, and their representations. My pedagogical
goals are twofold. First, I strive to develop the theory of Lie groups in an elementary
fashion, with minimal prerequisites. In particular, in Part I, I develop the theory
of (matrix) Lie groups and their Lie algebras using only linear algebra, without
requiring any knowledge of manifold theory. Second, I strive to provide more
motivation and intuition for the proofs, often using a ﬁgure, than in some of the
classic texts on the subject. At the same time, I aim to be fully rigorous; an
explanation or ﬁgure is a supplement to and not a replacement for a traditional
proof.
Although Lie theory is widely used in both mathematics and physics, there is
often a wide gulf between the presentations of the subject in the two disciplines:
Physics books get down to business quickly but are often imprecise in deﬁnitions
and statements of theorems, whereas math books are more rigorous but often
have a high barrier to entry. It is my hope that this book will be useful to both
mathematicians and physicists. In particular, the matrix approach in Part I allows
for deﬁnitions that are precise but comprehensible. Although I do not delve into the
details of how Lie algebras are used in particle theory, I do include an extended
discussion of the representations of SU.3/, which has obvious applications to that
ﬁeld. (My recent book, Quantum Theory for Mathematicians [Hall], also aims
to bridge a gap between the mathematics and physics literatures, and it contains
some discussion of Lie-theoretic issues in quantum mechanics. The emphasis there,
however, is on nonrelativistic quantum mechanics and not on quantum ﬁeld theory.)
Content of the Book Part I of the text covers the general theory of matrix Lie
groups (i.e., closed subgroups of GL.nI C/) and their Lie algebras. Chapter 1
introduces numerous examples of matrix Lie groups and examines their topological
properties. After discussing the matrix exponential in Chapter 2, I turn to Lie
algebras in Chapter 3, examining both abstract Lie algebras and Lie algebras
associated with matrix Lie groups. Chapter 3 shows, among other things, that every
matrix Lie group is an embedded submanifold of GL.nI C/ and, thus, a Lie group.
In Chapter 4, I consider elementary representation theory. Finally, Chapter 5 covers
xi

xii
Preface
the Baker-Campbell-Hausdorff formula and its consequences. I use this formula
(in place of the more traditional Frobenius theorem) to establish some of the deeper
results about the relationship between Lie groups and Lie algebras.
Part II of the text covers semisimple Lie algebras and their representations. I
begin with an entire chapter on the representation theory of sl.3I C/, that is, the
complexiﬁcation of the Lie algebra of the group SU.3/. On the one hand, this
example can be treated in an elementary way, simply by writing down a basis and
calculating. On the other hand, this example allows the reader to see the machinery
of roots, weights, and the Weyl group in action in a simple example, thus motivating
the general version of these structures. For the general case, I use an unconventional
deﬁnition of "semisimple," namely that a complex Lie algebra is semisimple if it has
trivial center and is the complexiﬁcation of the Lie algebra of a compact group. I
show that every such Lie algebra decomposes as a direct sum of simple algebras, and
is thus semisimple in the conventional sense. Actually, every complex Lie algebra
that is semisimple in the conventional sense has a "compact real form," so that my
deﬁnition of semisimple is equivalent to the standard one—but I do not prove this
claim. As with the choice to consider matrix Lie groups in Part I, this (apparent)
reduction in scope allows for a rapid development of the structure of semisimple Lie
algebras. After developing the necessary properties of root systems in Chapter 8, I
give the classiﬁcation of representations in Chapter 9, as expressed in the theorem
of the highest weight. Finally, Chapter 10 gives several additional properties of the
representations, including complete reducibility, the Weyl character formula, and
the Kostant multiplicity formula.
Finally, Part III of the book presents the compact-group approach to represen-
tation theory. Chapter 11 gives a proof of the torus theorem and establishes the
equivalence between the Lie-group and Lie-algebra deﬁnitions of the Weyl group.
This chapter does, however, make use of some of the manifold theory that I avoided
previously. The reader who is unfamiliar with manifold theory but willing to take
a few things on faith should be able to proceed on to Chapter 12, where I develop
the Weyl character formula and the theorem of the highest weight from the compact
group point of view. In particular, Chapter 12 gives a self-contained construction
of the representations, independent of the Lie-algebraic argument in Chapter 9.
Lastly, in Chapter 13, I examine the fundamental group of a compact group from
two different perspectives, one that treats the classical groups by induction on the
dimension and one that is based on the torus theorem and uses the structure of the
root system. This chapter shows, among other things, that for a simply connected
compact group, the integral elements from the group point of view coincide with
the integral elements from the Lie algebra point of view. This result shows that for
simply connected compact groups, the theorem of the highest weight for the group
is equivalent to the theorem of the highest weight for the Lie algebra.
The ﬁrst four chapters of the book cover elementary Lie theory and could be used
for an undergraduate course. At the graduate level, one could pass quickly through
Part I and then cover either Part II or Part III, depending on the interests of the
instructor. Although I have tried to explain and motivate the results in Parts II and III
of the book, using ﬁgures whenever possible, the material there is unquestionably

Preface
xiii
more challenging than in Part I. Nevertheless, I hope that the explicit working out of
the case of the Lie algebra sl.3I C/ (or, equivalently, the group SU.3/) in Chapter 6
will give the reader a good sense of the ﬂavor of the results in the subsequent
chapters.
In recent years, there have been several other books on Lie theory that use the
matrix-group approach. Of these, the book of Rossmann [Ross] is most similar in
style to my own. The ﬁrst three chapters of [Ross] cover much of the same material
as the ﬁrst four chapters of this book. Although the organization of my book is,
I believe, substantially different from that of other books on the subject, I make
no claim to originality in any of the proofs. I myself learned most of the material
here from the books of Bröcker and tom Dieck [BtD], Humphreys [Hum], and
Miller [Mill].
New Features of Second Edition This second edition of the book is substantially
expanded from the ﬁrst edition. Part I has been reorganized but covers mostly the
same material as in the ﬁrst edition. In Part II, however, at least half of the material is
new. Chapter 8 now provides a complete derivation of all relevant properties of root
systems. In Chapter 9, the construction of the ﬁnite-dimensional representations of
a semisimple Lie algebra has been ﬂeshed out, with the deﬁnition of the universal
enveloping algebra, a proof of the Poincaré-Birkhoff-Witt theorem, and a proof of
the existence of Verma modules. Chapter 10 is mostly new and includes complete
proofs of the Weyl character formula, the Weyl dimension formula, and the Kostant
multiplicity formula. Part III, on the structure and representation theory of compact
groups, is new in this edition.
I have also included many more ﬁgures in the second edition. The black-and-
white images were created in Mathematica, while the color images in Sect. 8.9
were modeled in the Zometool system (www.zometool.com) and rendered in Scott
Vorthmann's vZome program (vzome.com). I thank Paul Hildebrandt for assisting
me with construction of the Zometool models and Scott Vorthmann for going above
and beyond in assisting me with use of vZome.
Acknowledgments I am grateful for the input of many people on various versions of this text,
which has improved it immensely. Contributors to the ﬁrst printing of the ﬁrst edition include Ed
Bueler, Wesley Calvert, Tom Goebeler, Ruth Gornet, Keith Hubbard, Wicharn Lewkeeratiyutkul,
Jeffrey Mitchell, Ambar Sengupta, and Erdinch Tatar. For the second printing of the ﬁrst edition,
contributors include Moshe Adrian, Kamthorn Chailuek, Paul Gibson, Keith Hubbard, Dennis
Muhonen, Jason Quinn, Rebecca Weber, and Reed Wickner. Additional corrections to the ﬁrst
edition since the second printing appeared are due to Kate Brenneman, Edward Burkard, Moritz
Firsching, Nathan Gray, Ishan Mata, Jean-Renaud Pycke, and Jason Quinn. Contributors to the
second edition include Matt Cecil, Alexander Diaz-Lopez, Todd Kemp, Ben Lewis, George
McNinch, and Ambar Sengupta. Please write to me with questions or corrections at bhall@nd.edu.
For further information, click on the "Book" tab of my web site: www.nd.edu/~bhall/.
Notre Dame, IN
Brian Hall
January 2015

Part I
General Theory

Chapter 1
Matrix Lie Groups
1.1
Deﬁnitions
A Lie group is, roughly speaking, a continuous group, that is, a group described
by several real parameters. In this book, we consider matrix Lie groups, which are
Lie groups realized as groups of matrices. As an example, consider the set of all
2  2 real matrices with determinant 1, customarily denoted SL.2I R/. Since the
determinant of a product is the product of the determinants, this set forms a group
under the operation of matrix multiplication. If we think of the set of all 2  2
matrices, with entries a; b; c; d, as R4, then SL.2I R/ is the set of points in R4 for
which the smooth function ad  bc has the value 1.
Suppose f is a smooth function on Rk and we consider the set E where f .x/
equals some constant value c. If, at each point x0 in E, at least one of the partial
derivatives of f is nonzero, then the implicit function theorem tells us that we can
solve the equation f .x/ D c near x0 for one of the variables as a function of the
other k  1 variables. Thus, E is a smooth "surface" (or embedded submanifold) in
Rk of dimension k  1. In the case of SL.2I R/ inside R4, we note that the partial
derivatives of adbc with respect to a, b, c, and d are d, c, b, and a, respectively.
Thus, at each point where ad  bc D 1, at least one of these partial derivatives is
nonzero, and we conclude that SL.2I R/ is a smooth surface of dimension 3. Thus,
SL.2I R/ is a Lie group of dimension 3.
For other groups of matrices (such as the ones we will encounter later in
this section), one could use a similar approach. The analysis is, however, more
complicated because most of the groups are deﬁned by setting several different
smooth functions equal to constants. One therefore has to check that these functions
are "independent" in the sense of the implicit function theorem, which means that
their gradient vectors have to be linearly independent at each point in the group.
We will use an alternative approach that makes all such analysis unnecessary.
We consider groups G of matrices that are closed in the sense of Deﬁnition 1.4. To
each such G, we will associate in Chapter 3 a "Lie algebra" g, which is a real vector
© Springer International Publishing Switzerland 2015
B. Hall, Lie Groups, Lie Algebras, and Representations, Graduate
Texts in Mathematics 222, DOI 10.1007/978-3-319-13467-3_1
3

4
1
Matrix Lie Groups
space. A general result (Corollary 3.45) will then show that G is a smooth manifold
whose dimension is equal the dimension of g as a vector space.
This chapter makes use of various standard results from linear algebra, which are
summarized in Appendix A.
Deﬁnition 1.1. The general linear group over the real numbers, denoted
GL.nI R/, is the group of all n  n invertible matrices with real entries. The
general linear group over the complex numbers, denoted GL.nI C/, is the group of
all n  n invertible matrices with complex entries.
Deﬁnition 1.2. Let Mn.C/ denote the space of all n  n matrices with complex
entries.
We may identify Mn.C/ with Cn2 and use the standard notion of convergence in
Cn2: Explicitly, this means the following.
Deﬁnition 1.3. Let Am be a sequence of complex matrices in Mn.C/. We say that
Am converges to a matrix A if each entry of Am converges (as m ! 1) to the
corresponding entry of A (i.e., if .Am/jk converges to Ajk for all 1  j; k  n).
We now consider subgroups of GL.nI C/, that is, subsets G of GL.nI C/ such
that the identity matrix is in G and such that for all A and B in G, the matrices AB
and A1 are also in G.
Deﬁnition 1.4. A matrix Lie group is a subgroup G of GL.nI C/ with the
following property: If Am is any sequence of matrices in G, and Am converges to
some matrix A, then either A is in G or A is not invertible.
The condition on G amounts to saying that G is a closed subset of GL.nI C/.
(This does not necessarily mean that G is closed in Mn.C/.) Thus, Deﬁnition 1.4 is
equivalent to saying that a matrix Lie group is a closed subgroup of GL.nI C/.
The condition that G be a closed subgroup, as opposed to merely a subgroup,
should be regarded as a technicality, in that most of the interesting subgroups of
GL.nI C/ have this property. Most of the matrix Lie groups G we will consider
have the stronger property that if Am is any sequence of matrices in G, and Am
converges to some matrix A, then A 2 G (i.e., that G is closed in Mn.C/).
An example of a subgroup of GL.nI C/ which is not closed (and hence is not a
matrix Lie group) is the set of all nn invertible matrices with rational entries. This
set is, in fact, a subgroup of GL.nI C/, but not a closed subgroup. That is, one can
(easily) have a sequence of invertible matrices with rational entries converging to an
invertible matrix with some irrational entries. (In fact, every real invertible matrix is
the limit of some sequence of invertible matrices with rational entries.)
Another example of a group of matrices which is not a matrix Lie group is the
following subgroup of GL.2I C/. Let a be an irrational real number and let
G D
  eit 0
0 eita
ˇˇˇˇ t 2 R

:
(1.1)

1.2
Examples
5
Fig. 1.1 A small portion of the group G inside NG (left) and a larger portion (right)
Clearly, G is a subgroup of GL.2I C/. According to Exercise 10, the closure of G is
the group
NG D
 ei
0
0 ei
ˇˇˇˇ ;  2 R

:
The group G inside NG is known as an "irrational line in a torus"; see Figure 1.1.
1.2
Examples
Mastering the subject of Lie groups involves not only learning the general theory
but also familiarizing oneself with examples. In this section, we introduce some
of the most important examples of (matrix) Lie groups. Among these are the
classical groups, consisting of the general and special linear groups, the unitary
and orthogonal groups, and the symplectic groups. The classical groups, and their
associated Lie algebras, will be key examples in Parts II and III of the book.
1.2.1
General and Special Linear Groups
The general linear groups (over R or C) are themselves matrix Lie groups. Of
course, GL.nI C/ is a subgroup of itself. Furthermore, if Am is a sequence of
matrices in GL.nI C/ and Am converges to A, then by the deﬁnition of GL.nI C/,
either A is in GL.nI C/, or A is not invertible.

6
1
Matrix Lie Groups
Moreover, GL.nI R/ is a subgroup of GL.nI C/, and if Am 2 GL.nI R/ and Am
converges to A, then the entries of A are real. Thus, either A is not invertible or
A 2 GL.nI R/.
The special linear group (over R or C) is the group of n  n invertible matrices
(with real or complex entries) having determinant one. Both of these are subgroups
of GL.nI C/. Furthermore, if An is a sequence of matrices with determinant one and
An converges to A, then A also has determinant one, because the determinant is a
continuous function. Thus, SL.nI R/ and SL .nI C/ are matrix Lie groups.
1.2.2
Unitary and Orthogonal Groups
An n  n complex matrix A is said to be unitary if the column vectors of A are
orthonormal, that is, if
n
X
lD1
AljAlk D ıjk:
(1.2)
We may rewrite (1.2) as
n
X
lD1
.A/jlAlk D ıjk;
(1.3)
where ıjk is the Kronecker delta, equal to 1 if j D k and equal to zero if j ¤ k.
Here A is the adjoint of A, deﬁned by
.A/jk D Akj:
Equation (1.3) says that AA D I; thus, we see that A is unitary if and only if
A D A1. In particular, every unitary matrix is invertible.
The adjoint operation on matrices satisﬁes .AB/ D BA. From this, we can
see that if A and B are unitary, then
.AB/.AB/ D BAAB D B1A1AB D I;
showing that AB is also unitary. Furthermore, since .AA1/ D I  D I, we see that
.A1/A D I, which shows that .A1/ D .A/1. Thus, if A is unitary, we have
.A1/A1 D .A/1A1 D .AA/1 D I;
showing that A1 is again unitary.
Thus, the collection of unitary matrices is a subgroup of GL.nI C/. We call this
group the unitary group and we denote it by U.n/. We may also deﬁne the special

1.2
Examples
7
unitary group SU.n/, the subgroup of U.n/ consisting of unitary matrices with
determinant 1. It is easy to check that both U.n/ and SU.n/ are closed subgroups of
GL.nI C/ and thus matrix Lie groups.
Meanwhile, let h; i denote the standard inner product on Cn, given by
hx; yi D
X
j
xj yj :
(Note that we put the conjugate on the ﬁrst factor in the inner product.) By
Proposition A.8, we have
hx; Ayi D hAx; yi
for all x; y 2 Cn. Thus,
hAx; Ayi D hAAx; yi ;
from which we can see that if A is unitary, then A preserves the inner product on
Cn, that is,
hAx; Ayi D hx; yi
for all x and y. Conversely, if A preserves the inner product, we must have
hAAx; yi D hx; yi for all x; y. It is not hard to see that this condition holds only
if AA D I. Thus, an equivalent characterization of unitarity is that A is unitary if
and only if A preserves the standard inner product on Cn.
Finally, for any matrix A, we have that det A D det A. Thus, if A is unitary, we
have
det.AA/ D jdet Aj2 D det I D 1:
Hence, for all unitary matrices A, we have jdet Aj D 1.
In a similar fashion, an n  n real matrix A is said to be orthogonal if the
column vectors of A are orthonormal. As in the unitary case, we may give equivalent
versions of this condition. The only difference is that if A is real, A is the same as
the transpose Atr of A, given by
.Atr/jk D Akj:
Thus, A is orthogonal if and only if Atr D A1, and this holds if and only if A
preserves the inner product on Rn. Since det.Atr/ D det A, if A is orthogonal, we
have
det.AtrA/ D det.A/2 D det.I/ D 1;

8
1
Matrix Lie Groups
so that det.A/ D ˙1. The collection of all orthogonal matrices forms a closed
subgroup of GL.nI C/, which we call the orthogonal group and denote by O.n/.
The set of nn orthogonal matrices with determinant one is the special orthogonal
group, denoted SO.n/. Geometrically, elements of SO.n/ are rotations, while the
elements of O.n/ are either rotations or combinations of rotations and reﬂections.
Consider now the bilinear form .; / on Cn deﬁned by
.x; y/ D
X
j
xj yj :
(1.4)
This form is not an inner product (Sect. A.6) because, for example, it is symmetric
rather than conjugate-symmetric. The set of all n  n complex matrices A which
preserve this form (i.e., such that .Ax; Ay/ D .x; y/ for all x; y 2 Cn) is the complex
orthogonal group O.nI C/, and it is a subgroup of GL.nI C/. Since there are no
conjugates in the deﬁnition of the form .; /, we have
.x; Ay/ D .Atrx; y/;
for all x; y 2 Cn, where on the right-hand side of the above relation, we have
Atr rather than A. Repeating the arguments for the case of O.n/, but now allowing
complex entries in our matrices, we ﬁnd that an nn complex matrix A is in O.nI C/
if and only if AtrA D I, that O.nI C/ is a matrix Lie group, and that det A D ˙1
for all A in O.nI C/. Note that O.nI C/ is not the same as the unitary group U.n/.
The group SO.nI C/ is deﬁned to be the set of all A in O.nI C/ with det A D 1 and
it is also a matrix Lie group.
1.2.3
Generalized Orthogonal and Lorentz Groups
Let n and k be positive integers, and consider RnCk. Deﬁne a symmetric bilinear
form Œ; n;k on RnCk by the formula
Œx; yn;k D x1y1 C    C xnyn  xnC1ynC1      xnCkynCk
(1.5)
The set of .nCk/.nCk/ real matrices A which preserve this form (i.e., such that
ŒAx; Ayn;k D Œx; yn;k for all x; y 2 RnCk) is the generalized orthogonal group
O.nI k/. It is a subgroup of GL.n C kI R/ and a matrix Lie group (Exercise 1). Of
particular interest in physics is the Lorentz group O.3I 1/. We also deﬁne SO.nI k/
to be the subgroup of O.nI k/ consisting of elements with determinant 1.
If A is an .n C k/  .n C k/ real matrix, let A.j / denote the jth column vector
of A, that is,
A.j / D
0
B@
A1;j
:::
AnCk;j
1
CA :

1.2
Examples
9
Note that A.j / is equal to Aej, that is, the result of applying A to the jth standard
basis element ej . Then A will belong to O.nI k/ if and only ŒAej ; Ael for all
1  j ,l  nCk. Explicitly, this means that A 2 O.nI k/ if and only if the following
conditions are satisﬁed:

A.j /; A.l/
n;k D
0
j ¤ l;

A.j /; A.j /
n;k D
1
1  j  n;

A.j /; A.j /
n;k D 1
n C 1  j  n C k:
(1.6)
Let g denote the .nCk/.nCk/ diagonal matrix with ones in the ﬁrst n diagonal
entries and minus ones in the last k diagonal entries:
g D
0
BBBBBBBBB@
1
:::
1
1
:::
1
1
CCCCCCCCCA
:
Then A is in O.nI k/ if and only if AtrgA D g (Exercise 1). Taking the determinant
of this equation gives .det A/2 det g D det g, or (det A/2 D 1. Thus, for any A in
O.nI k/, det A D ˙1.
1.2.4
Symplectic Groups
Consider the skew-symmetric bilinear form B on R2n deﬁned as follows:
!.x; y/ D
n
X
j D1
.xjynCj  xnCjyj/:
(1.7)
The set of all 2n  2n matrices A which preserve ! (i.e., such that !.Ax; Ay/ D
!.x; y/ for all x; y 2 R2n) is the real symplectic group Sp.nI R/, and it is a closed
subgroup of GL.2nI R/. (Some authors refer to the group we have just deﬁned as
Sp.2nI R/ rather than Sp.nI R/.) If  is the 2n  2n matrix
 D
 0 I
I 0

;
(1.8)
then
!.x; y/ D hx; yi :

10
1
Matrix Lie Groups
From this, it is not hard to show that a 2n  2n real matrix A belongs to Sp.nI R/ if
and only if
 Atr D A1:
(1.9)
(See Exercise 2.) Taking the determinant of this identity gives det A D .det A/1,
i.e., .det A/2 D 1. This shows that det A D ˙1, for all A 2 Sp.nI R/. In fact,
det A D 1 for all A 2 Sp.nI R/, although this is not obvious.
One can deﬁne a bilinear form ! on C2n by the same formula as in (1.7) (with
no conjugates). Over C, we have the relation
!.z; w/ D .z; w/;
where .; / is the complex bilinear form in (1.4). The set of 2n2n complex matrices
which preserve this form is the complex symplectic group Sp.nI C/. A 2n  2n
complex matrix A is in Sp.nI C/ if and only if (1.9) holds. (Note: This condition
involves Atr, not A.) Again, we can easily show that each A 2 Sp.nI C/ satisﬁes
det A D ˙1 and, again, it is actually the case that det A D 1. Finally, we have the
compact symplectic group Sp.n/ deﬁned as
Sp.n/ D Sp .nI C/ \ U.2n/:
That is to say, Sp.n/ is the group of 2n  2n matrices that preserve both the inner
product and the bilinear form !. For more information about Sp.n/, see Sect. 1.2.8.
1.2.5
The Euclidean and Poincaré Groups
The Euclidean group E.n/ is the group of all transformations of Rn that can be
expressed as a composition of a translation and an orthogonal linear transformation.
We write elements of E.n/ as pairs fx; Rg with x 2 Rn and R 2 O.n/, and we let
fx; Rg act on Rn by the formula
fx; Rg y D Ry C x:
Since
fx1; R1gfx2; R2gy D R1.R2y C x2/ C x1 D R1R2y C .x1 C R1x2/;
the product operation for E.n/ is the following:
fx1; R1gfx2; R2g D fx1 C R1x2; R1R2g:
(1.10)

1.2
Examples
11
The inverse of an element of E.n/ is given by
fx; Rg1 D fR1x; R1g:
The group E.n/ is not a subgroup of GL.nI R/, since translations are not linear
maps. However, E.n/ is isomorphic to the (closed) subgroup of GL.n C 1I R/
consisting of matrices of the form
0
BBB@
x1
R
:::
xn
0    0 1
1
CCCA ;
(1.11)
with R 2 O.n/. (The reader may easily verify that matrices of the form (1.11)
multiply according to the formula in (1.10).)
We similarly deﬁne the Poincaré group P.nI 1/ (also known as the inhomoge-
neous Lorentz group) to be the group of all transformations of RnC1 of the form
T D TxA
with x 2 RnC1 and A 2 O.nI 1/. This group is isomorphic to the group of .nC2/
.n C 2/ matrices of the form
0
BBB@
x1
A
:::
xnC1
0    0
1
1
CCCA
(1.12)
with A 2 O.nI 1/.
1.2.6
The Heisenberg Group
The set of all 3  3 real matrices A of the form
A D
0
@
1 a b
0 1 c
0 0 1
1
A ;
(1.13)
where a, b, and c are arbitrary real numbers, is the Heisenberg group. It is easy to
check that the product of two matrices of the form (1.13) is again of that form, and,

12
1
Matrix Lie Groups
clearly, the identity matrix is of the form (1.13). Furthermore, direct computation
shows that if A is as in (1.13), then
A1 D
0
@
1 a ac  b
0
1
c
0
0
1
1
A :
Thus, H is a subgroup of GL.3I R/. The Heisenberg group is a model for the
Heisenberg-Weyl commutation relations in physics and also serves as a illumi-
nating example for the Baker-Campbell-Hausdorff formula (Sect. 5.2). See also
Exercise 8.
1.2.7
The Groups R, C, S 1, R, and Rn
Several important groups which are not deﬁned as groups of matrices can be
thought of as such. The group R of non-zero real numbers under multiplication is
isomorphic to GL.1I R/. Similarly, the group C of nonzero complex numbers under
multiplication is isomorphic to GL.1I C/ and the group S1 of complex numbers with
absolute value one is isomorphic to U.1/.
The group R under addition is isomorphic to GL.1I R/C (11 real matrices with
positive determinant) via the map x ! Œex. The group Rn (with vector addition) is
isomorphic to the group of diagonal real matrices with positive diagonal entries, via
the map
.x1; : : : ; xn/ !
0
B@
ex1
0
:::
0
exn
1
CA .
1.2.8
The Compact Symplectic Group
Of the groups introduced in the preceding subsections, the compact symplectic
group Sp.n/ WD Sp.nI C/ \ U.2n/ is the most mysterious. In this section, we
attempt to understand the structure of Sp.n/ and to show that it can be understood
as being the "unitary group over the quaternions."
Since the deﬁnition of Sp.n/ involves unitarity, it is convenient to express the
bilinear form ! on C2n in terms of the inner product h; i, rather than in terms of the
bilinear form .; /, as we did in Sect. 1.2.4. To this end, deﬁne a conjugate-linear
map J W C2n ! C2n by
J.˛; ˇ/ D . Nˇ; N˛/;

1.2
Examples
13
where ˛ and ˇ are in Cn and .˛; ˇ/ is in C2n. We can easily check that for all
z; w 2 C2n, we have
!.z; w/ D hJ z; wi :
Recall that we take our inner product to be conjugate linear in the ﬁrst factor; since
J is also conjugate linear, hJ z; wi is actually linear in z. We may easily check that
hJ z; wi D hz; J wi D  hJ w; zi
for all z; w 2 C2n and that
J 2 D I:
Proposition 1.5. If U belongs to U.2n/ then U belongs to Sp.n/ if and only if U
commutes with J .
Proof. Fix some U in U.2n/. Then for z and w in C2n, we have, on the one hand,
!.Uz; Uw/ D hJUz; Uwi D hU JUz; wi D ˝U 1JUz; w˛ ;
and, on the other hand,
!.z; w/ D hJz; wi :
From this it is each to check that U preserves ! if and only if
U 1JU D J;
which is equivalent to JU D UJ.
ut
The preceding result can be used to give a different perspective on the deﬁnition
of Sp.n/, as follows. The quaternion algebra H is the four-dimensional associative
algebra over R spanned by elements 1 (the identity), i, j, and k satisfying
i2 D j2 D k2 D 1
and
ij D kI
ji D kI
jk D iI
kj D iI
ki D jI
ik D j:

14
1
Matrix Lie Groups
We may realize the quaternion algebra inside M2.C/ by taking identifying 1 with
the identity matrix and setting
i D
 i
0
0 i

I
j D

0 1
1 0

I
k D
0 i
i 0

:
The algebra H is then the space of real linear combinations of I, i, j, and k.
Now, since J is conjugate linear, we have
J.iz/ D iJz
for all z 2 C2n; that is, iJ D J i. Thus, if we deﬁne K to be iJ, we have
K2 D iJiJ D J.i/2J D J 2 D I;
and one can easily check that iI, J , and K satisfy the same commutation relations
as i, j, and k. We can therefore make C2n into a "vector space" over the
noncommutative algebra H by setting
i  z D iz
j  z D Jz
k  z D iJz:
Now, if U belongs to Sp.n/, then U commutes with multiplication by i and with
J (Proposition 1.5) and thus, also, with K WD iJ. Thus, U is actually "quaternion
linear." A 2n2n matrix U therefore belongs to Sp.n/ if and only if U is quaternion
linear and preserves the norm. Thus, we may think of Sp.n/ as the "unitary group
over the quaternions." The compact symplectic group then ﬁts naturally with the
orthogonal groups (norm-preserving maps over R) and the unitary groups (norm-
preserving maps over C).
Every U 2 U.2n/ has an orthonormal basis of eigenvectors, with eigenvalues
having absolute value 1. We now determine the additional properties the eigenvec-
tors and eigenvalues must satisfy in order for U to be in Sp.n/ D U.2n/\Sp.nI C/.
Theorem 1.6. If U 2 Sp.n/, then there exists an orthonormal basis u1; : : : ; un,
v1; : : : vn for Cn such that the following properties hold: First, J uj D vj; second,
for some real numbers 1; : : : ; n, we have
U uj D eij uj
Uvj D eij vj I

1.2
Examples
15
and third,
!.uj ; uk/ D !.vj ; vk/ D 0
!.uj ; vk/ D ıjk:
Conversely, if there exists an orthonormal basis with these properties, U belongs to
Sp.n/.
Lemma 1.7. Suppose V is a complex subspace of C2n that is invariant under the
conjugate-linear map J . Then the orthogonal complement V ? of V (with respect
to the inner product h; i) is also invariant under J . Furthermore, V and V ? are
orthogonal with respect to !; that is,
!.z; w/ D 0
for all z 2 V and w 2 V ?.
Proof. If w 2 V ? then for all z 2 V , we have
hJw; zi D  hJz; wi D 0;
because J z is again in V . Thus, V ? is invariant under J . Then if z 2 V and w 2 V ?,
we have
!.z; w/ D hJ z; wi D 0;
because J z is again in V .
ut
Proof of Theorem 1.6. Consider U in Sp.nI C/ \ U.2n/, choose an eigenvector for
U , normalized to be a unit vector, and call it u1. Since U preserves the norms of
vectors, the eigenvalue 1 for u1 must be of the form ei1, for some 1 2 R. If we set
v1 D J u1, then since J is conjugate linear and commutes with U (Proposition 1.5),
we have
Uv1 D J.Uu1/ D J.ei1u1/ D ei1v1:
That is to say, v1 is an eigenvector for U with eigenvalue ei1. Furthermore,
hv1; u1i D hJ u1; u1i D !.u1; u1/ D 0;
since ! is a skew-symmetric form. On the other hand,
!.u1; v1/ D hJ u1; v1i D hJu1; Ju1i D 1;
since J preserves the magnitude of vectors.
Now, since J 2 D I, we can easily check that the span V of u1 and v1 D J u1
is invariant under J . Thus, by Lemma 1.7, V ? is also invariant under J and is

16
1
Matrix Lie Groups
!-orthogonal to V . Meanwhile, V is invariant under both U and U  D U 1.
Thus, by Proposition A.10, V ? is invariant under both U  D U and U . Since
U preserves V ?, the restriction of U to V ? will have an eigenvector, which we
can normalize to be a unit vector and call u2. If we let v2 D J u2, then we
have all the same properties for u2 and v2 as for u1 and v1. Furthermore, u2 and
v2 are orthogonal—with respect to both h; i and !.; /—to u1 and v1. We can
then proceed on in a similar fashion to obtain the full set of vectors u1; : : : ; un
and v1; : : : ; vn. (If u1; : : : ; uk and v1; : : : ; vk have been chosen, we take ukC1 and
vkC1 WD J ukC1 in the orthogonal complement of the span of u1; : : : ; uk and
v1; : : : ; vk.)
The other direction of the theorem is left to the reader (Exercise 6).
ut
1.3
Topological Properties
In this section, we investigate three important topological properties of matrix Lie
groups, each of which is satisﬁed by some groups but not others.
1.3.1
Compactness
The ﬁrst property we consider is compactness.
Deﬁnition 1.8. A matrix Lie group G  GL.nI C/ is said to be compact if it is
compact in the usual topological sense as a subset of Mn.C/ Š R2n2.
In light of the Heine-Borel theorem (Theorem 2.41 in [Rud1]), a matrix Lie
group G is compact if and only if it is closed (as a subset of Mn.C/, not just as a
subset of GL.nI C/) and bounded. Explicitly, this means that G is compact if and
only if (1) whenever Am 2 G and Am ! A, then A is in G, and (2) there exists a
constant C such that for all A 2 G, we have
ˇˇAjk
ˇˇ  C for all 1  j; k  n.
The following groups are compact: O.n/ and SO.n/, U.n/ and SU.n/, and
Sp.n/. Each of these groups is easily seen to be closed in Mn.C/ and each satisﬁes
the bound
ˇˇAjk
ˇˇ  1, since in each case, the columns of A 2 G are required to
be unit vectors. Most of the other groups we have considered are noncompact. The
special linear group SL.nI R/, for example, is unbounded (except in the trivial case
n D 1), because for all m, the matrix
Am D
0
BBBBB@
m
1
m
1
:::
1
1
CCCCCA
has determinant one.

1.3
Topological Properties
17
1.3.2
Connectedness
The second property we consider is connectedness.
Deﬁnition 1.9. A matrix Lie group G is said to be connected if for all A and B in
G, there exists a continuous path A.t/, a  t  b, lying in G with A.a/ D A and
A.b/ D B. For any matrix Lie group G, the identity component of G, denoted G0,
is the set of A 2 G for which there exists a continuous path A.t/, a  t  b, lying
in G with A.a/ D I and A.b/ D A.
The property we have called "connected" in Deﬁnition 1.9 what is called path
connected in topology, which is not (in general) the same as connected. However,
we will eventually prove that a matrix Lie group is connected if and only if it is
path-connected. Thus, in a slight abuse of terminology, we shall continue to refer to
the above property as connectedness. (See the remarks following Corollary 3.45.)
To show that a matrix Lie group G is connected, it sufﬁces to show that each
A 2 G can be connected to the identity by a continuous path lying in G.
Proposition 1.10. If G is a matrix Lie group, the identity component G0 of G is a
normal subgroup of G.
We will see in Sect. 3.7 that G0 is closed and hence a matrix Lie group.
Proof. If A and B are any two elements of G0, then there are continuous paths A.t/
and B.t/ connecting I to A and to B in G. Then the path A.t/B.t/ is a continuous
path connecting I to AB in G, and .A.t//1 is a continuous path connecting I to
A1 in G. Thus, both AB and A1 belong to G0, showing that G0 is a subgroup of
G. Now suppose A is in G0 and B is any element of G. Then there is a continuous
path A.t/ connecting I to A in G, and the path BA.t/B1 connects I to BAB1 in
G. Thus, BAB1 2 G0, showing that G0 is normal.
ut
Note that because matrix multiplication and matrix inversion are continuous on
GL.nI C/, it follows that if A.t/ and B.t/ are continuous, then so are A.t/B.t/
and A.t/1. The continuity of the matrix product is obvious. The continuity of
the inverse follows from the formula for the inverse in terms of cofactors; this
formula is continuous as long as we remain in the set of invertible matrices where
the determinant in the denominator is nonzero.
Proposition 1.11. The group GL.nI C/ is connected for all n  1.
Proof. We make use of the result that every matrix is similar to an upper triangular
matrix (Theorem A.4). That is to say, we can express any A 2 Mn.C/ in the form
A D CBC1, where
B D
0
B@
1

:::
0
n
1
CA :

18
1
Matrix Lie Groups
If A is invertible, each j must be nonzero. Let B.t/ be obtained by multiplying the
part of B above the diagonal by .1  t/, for 0  t  1, and let A.t/ D CB.t/C 1.
Then A.t/ is a continuous path lying in GL.nI C/ which starts at A and ends at
CDC1, where D is the diagonal matrix with diagonal entries 1; : : : ; n. We can
now deﬁne paths j .t/ connecting j to 1 in C as t goes from 1 to 2, and we can
deﬁne A.t/ on the interval 1  t  2 by
A.t/ D C
0
B@
1.t/
0
:::
0
n.t/
1
CA C 1:
Then A.t/, 0  t  2, is a continuous path in GL.nI C/ connecting A to I.
ut
An alternative proof of this result is given in Exercise 12.
Proposition 1.12. The group SL.nI C/ is connected for all n  1.
Proof. The proof is almost the same as for GL.nI C/, except that we must make
sure our path connecting A 2 SL.nI C/ to I lies entirely in SL.nI C/. We can
ensure this by choosing n.t/, in the second part of the preceding proof, to be equal
to .1.t/    n1.t//1.
ut
Proposition 1.13. The groups U.n/ and SU.n/ are connected, for all n  1.
Proof. By Theorem A.3, every unitary matrix has an orthonormal basis of eigen-
vectors, with eigenvalues having absolute value 1. Thus, each U 2 U.n/ can be
written as U1DU 1
1 , where U1 2 U.n/ and D is diagonal with diagonal entries
ei1; : : : ; ein. We may then deﬁne
U.t/ D U1
0
B@
ei.1t/1
0
:::
0
ei.1t/n
1
CA U 1
1 ;
0  t  1:
It is easy to see that U.t/ is in U.n/ for all t, and U.t/ connects U to I. A slight
modiﬁcation of this argument, as in the proof of Proposition 1.12, shows that SU.n/
is connected.
ut
The group SO.n/ is also connected; see Exercise 13.
1.3.3
Simple Connectedness
The last topological property we consider is simple connectedness.

1.3
Topological Properties
19
Deﬁnition 1.14. A matrix Lie group G is said to be simply connected if it is
connected and, in addition, every loop in G can be shrunk continuously to a point
in G.
More precisely, assume that G is connected. Then G is simply connected if for
every continuous path A.t/, 0  t  1, lying in G and with A.0/ D A.1/, there
exists a continuous function A.s; t/, 0  s; t  1, taking values in G and having
the following properties: (1) A.s; 0/ D A.s; 1/ for all s, (2) A.0; t/ D A.t/, and (3)
A.1; t/ D A.1; 0/ for all t.
One should think of A.t/ as a loop and A.s; t/ as a family of loops, parameterized
by the variable s which shrinks A.t/ to a point. Condition 1 says that for each
value of the parameter s, we have a loop; Condition 2 says that when s D 0 the
loop is the speciﬁed loop A.t/; and Condition 3 says that when s D 1 our loop
is a point. The condition of simple connectedness is important because for simply
connected groups, there is a particularly close relationship between the group and
the Lie algebra. (See Sect. 5.7.)
Proposition 1.15. The group SU.2/ is simply connected.
Proof. Exercise 5 shows that SU.2/ may be thought of (topologically) as the
three-dimensional sphere S3 sitting inside R4. It is well known that S3 is simply
connected; see, for example, Proposition 1.14 in [Hat].
ut
If a matrix Lie group G is not simply connected, the degree to which it fails to
be simply connected is encoded in the fundamental group of G. (See Sect. 13.1.)
Sections 13.2 and 13.3 analyze several additional examples. It is shown there, for
example, that SU.n/ is simply connected for all n.
1.3.4
The Topology of SO.3/
We conclude this section with an analysis of the topological structure of the group
SO.3/. We begin by describing real projective spaces.
Deﬁnition 1.16. The real projective space of dimension n, denoted RP n, is the
set of lines through the origin in RnC1. Since each line through the origin intersects
the unit sphere exactly twice, we may think of RP n as the unit sphere Sn with
"antipodal" points u and u identiﬁed.
Using the second description, we think of points in RP n as pairs fu; ug, with
u 2 Sn. There is a natural map  W Sn ! RP n, given by
.u/ D fu; ug:

20
1
Matrix Lie Groups
We may deﬁne a distance function on RP n by deﬁning
d.fu; ug; fv; vg/ D min.d.u; v/; d.u; v/; d.u; v/; d.u; v//
D min.d.u; v/; d.u; v//:
(The second equality holds because d.x; y/ D d.x; y/.) With this metric, RP n
is locally isometric to Sn, since if u and v are nearby points in Sn, we have
d.fu; ug; fv; vg/ D d.u; v/.
It is known that RP n is not simply connected. (See, for example, Example 1.43
in [Hat].) Indeed, suppose u is any unit vector in RnC1 and B.t/ is any path in Sn
connecting u to u. Then
A.t/ WD .B.t//
is a loop in RP n, and this loop cannot be shrunk continuously to a point in RP n.
To prove this claim, suppose that a map A.s; t/ as in Deﬁnition 1.14. Then A.s; t/
can be "lifted" to a continuous map B.s; t/ into Sn such that B.0; t/ D B.t/ and
such that A.s; t/ D .B.s; t//. (See Proposition 1.30 in [Hat].) Since A.s; 0/ D
A.s; 1/ for all s, we must have B.s; 0/ D ˙B.s; 1/. But by construction, B.0; 0/ D
B.0; 1/. If order for B.s; t/ to be continuous in s, we must then have B.s; 0/ D
B.s; 1/ for all s. It follows that B.1; t/ is a nonconstant path in Sn. It is then easily
veriﬁed that A.1; t/ D .B.1; t// cannot be constant, contradicting our assumption
about A.s; t/.
Let Dn denote the closed upper hemisphere in Sn, that is, the set of points u 2 Sn
with unC1  0. Then  maps Dn onto RP n, since at least one of u and u is in
Dn. The restriction of  to Dn is injective except on the equator, that is, the set of
u 2 Sn with unC1 D 0. If u is in the equator, then u is also in the equator, and
.u/ D .u/. Thus, we may also think of RP n as the upper hemisphere Dn, with
antipodal points on the equator identiﬁed (Figure 1.2).
We may now make one last identiﬁcation using the projection P of RnC1 onto
Rn. (That is to say, P is the map sending .x1; : : : ; xn; xnC1/ to .x1; : : : ; xn/.) The
restriction of P to Dn is a continuous bijection between Dn and the closed unit ball
Bn in Rn, with the equator in Dn mapping to the boundary of the ball. Thus, our
u
−u
Fig. 1.2 The space RP n is the upper hemisphere with antipodal points on the equator identiﬁed.
The indicated path from u to u corresponds to a loop in RP n that cannot be shrunk to a point

1.4
Homomorphisms
21
last model of RP n is the closed unit ball Bn  Rn, with antipodal points on the
boundary of Bn identiﬁed.
We now turn to a topological analysis of SO.3/.
Proposition 1.17. There is a continuous bijection between SO.3/ and RP 3.
Since RP 3 is not simply connected, it follows that SO.3/ is not simply
connected, either.
Proof. If v is a unit vector in R3, let Rv; be the element of SO.3/ consisting of
a "right-handed" rotation by angle  in the plane orthogonal to v. That is to say,
let v? denote the plane orthogonal to v and choose an orthonormal basis .u1; u2/
for v? in such a way that the linear map taking the orthonormal basis .u1; u2; v/
to the standard basis .e1; e2; e3/ has positive determinant. We use the basis .u1; u2/
to identify v? with R2, and the rotation is then in the counterclockwise direction
in R2. It is easily seen that Rv; is the same as Rv;. It is also not hard to show
(Exercise 14) that every element of SO.3/ can be expressed as Rv;, for some v and
 with     . Furthermore, we can arrange that 0     by replacing v
with v if necessary.
If R D I, then R D Rv;0 for any unit vector v. If R is a rotation by angle  about
some axis v, then R can be expressed both as Rv; and as Rv;. It is not hard to see
that if R ¤ I and R is not a rotation by angle , then R has a unique representation
as Rv; with 0 <  < .
Now let B3 denote the closed ball of radius  in R3 and consider the map ˆ W
B3 ! SO.3/ given by
ˆ.u/ D ROu;kuk;
u ¤ 0;
ˆ.0/ D I:
Here, Ou D u= kuk is the unit vector in the u-direction. The map ˆ is continuous, even
at I, since Rv; approaches the identity as  approaches zero, regardless of how v
is behaving. The discussion in the preceding paragraph shows that ˆ maps B3 onto
SO.3/. The map ˆ is injective except that "antipodal" points on the boundary of B3
have the same image: Rv; D Rv;. Thus, ˆ descends to a continuous, injective
map of RP 3 onto SO.3/. Since both RP 3 and SO.3/ are compact, Theorem 4.17
in [Rud1] tells us that the inverse map is also continuous, meaning that SO.3/ is
homeomorphic to RP 3.
ut
For a different approach to proving Proposition 1.17, see the discussion following
Proposition 1.19.
1.4
Homomorphisms
We now look at the notion of homomorphisms for matrix Lie groups.

22
1
Matrix Lie Groups
Deﬁnition 1.18. Let G and H be matrix Lie groups. A map ˆ from G to H is
called a Lie group homomorphism if (1) ˆ is a group homomorphism and (2) ˆ
is continuous. If, in addition, ˆ is one-to-one and onto and the inverse map ˆ1 is
continuous, then ˆ is called a Lie group isomorphism.
The condition that ˆ be continuous should be regarded as a technicality, in that it
is very difﬁcult to give an example of a group homomorphism between two matrix
Lie groups which is not continuous. In fact, if G D R and H D C, then any group
homomorphism from G to H which is even measurable (a very weak condition)
must be continuous. (See Exercise 17 in Chapter 9 of [Rud2].)
Note that the inverse of a Lie group isomorphism is continuous (by deﬁnition)
and a group homomorphism (by elementary group theory), and thus a Lie group
isomorphism. If G and H are matrix Lie groups and there exists a Lie group
isomorphism from G to H, then G and H are said to be isomorphic, and we write
G Š H.
The simplest interesting example of a Lie group homomorphism is the determi-
nant, which is a homomorphism of GL.nI C/ into C. Another simple example is
the map ˆ W R ! SO.2/ given by
ˆ./ D
cos   sin 
sin 
cos 

:
This map is clearly continuous, and calculation (using standard trigonometric
identities) shows that it is a homomorphism.
An important topic for us will be the relationship between the groups SU.2/ and
SO.3/, which are almost, but not quite, isomorphic. Speciﬁcally, we now show that
there exists a Lie group homomorphism ˆ W SU.2/ ! SO.3/ that is two-to-one and
onto. Consider the space V of all 2  2 complex matrices X which are self-adjoint
(i.e., X D X) and have trace zero. Elements of V are precisely the matrices of the
form
X D

x1
x2 C ix3
x2  ix3
x1

;
(1.14)
with x1; x2; x3 2 R. If we identify V with R3 by means of the coordinates x1, x2,
and x3 in (1.14), then the standard inner product on R3 can be computed as
hX1; X2i D 1
2trace.X1X2/:
That is to say,
1
2trace

x1
x2 C ix3
x2  ix3
x1
 
x0
1
x0
2 C ix0
3
x0
2  ix0
3
x0
1

D x1x0
1 C x2x0
2 C x3x0
3;

1.4
Homomorphisms
23
as one may easily check by direct calculation.
For each U 2 SU.2/, deﬁne a linear map ˆU W V ! V by
ˆU .X/ D UXU1:
Since U is unitary,
.UAU1/ D .U 1/AU D UAU1;
showing that UAU1 is again in V .
It is easy to see that ˆU1U2 D ˆU1ˆU2. Furthermore,
1
2trace..UX1U 1/.UX2U 1// D 1
2trace.UX1X2U 1/
D 1
2trace.X1X2/;
since the trace is invariant under conjugation. Thus, each ˆU preserves the inner
product trace.X1X2/=2 on V . It follows that the map U 7! ˆU is a homomorphism
of SU.2/ into the group of orthogonal linear transformations of V Š R3, that is, into
O.3/. Since SU.2/ is connected (Proposition 1.13), ˆU must actually lie in SO.3/
for all U 2 SU.2/. Thus, ˆ (i.e., the map U 7! ˆU ) is a homomorphism of SU.2/
into SO.3/, which is easily seen to be continuous. Since .I/X.I/1 D X, we
see that ˆI is the identity element of SO.3/.
Suppose, for example, that U is the matrix
U D
ei=2
0
0
ei=2

:
Then by direct calculation, we obtain
U

x1
x2 C ix3
x2  ix3
x1

U 1 D

x0
1
x0
2 C ix0
3
x0
2  ix0
3
x0
1

;
(1.15)
where x0
1 D x1 and
x0
2 C ix0
3 D ei.x2 C ix3/
D .x2 cos   x3 sin / C i.x2 sin  C x3 cos /:
(1.16)
In this case, then, ˆU is a rotation by angle  in the .x2; x3/-plane. Note that even
though the diagonal entries of U are e˙i=2, the map ˆU is a rotation by angle ,
not =2.

24
1
Matrix Lie Groups
Proposition 1.19. The map U 7! ˆU is a 2-1 and onto map of SU.2/ to SO.3/,
with kernel equal to fI; Ig.
Since SU.2/ is homeomorphic to S3, the proposition gives another way of seeing
that SO.3/ is homeomorphic to RP 3, that is, S3 with antipodal points identiﬁed.
This result was obtained in a different way in Proposition 1.17.
It is not hard to show that ˆ is a "covering map" in the topological sense
(Section 1.3 of [Hat]). Since SU.2/ is simply connected (Proposition 1.15) and
the map is 2-1, it follows by the theory of covering maps (e.g., Theorem 1.38 in
[Hat]) that SO.3/ cannot be simply connected and, indeed, it must have fundamental
group Z=2. See Chapter 13 for general information about fundamental groups and
for a computation of the fundamental group of SO.n/, n > 2.
Proof. Exercise 16 shows that the kernel of ˆ is precisely the set fI; Ig. To see
that ˆ maps onto SO.3/, let R be a rotation of V Š R3. By Exercise 14, there exists
an "axis" X 2 V such that R is a rotation by some angle  in the plane orthogonal
to X. If we express X in the form
X D U0
x1
0
0 x1

U 1
0
with U0 2 U.2/, then the plane orthogonal to X in V is the space of matrices of the
form
X0 D U0

0
x2 C ix3
x2  ix3
0

U 1
0 :
(1.17)
If we now take
U D U0
 ei=2
0
0
ei=2

U 1
0 ;
we can easily see that UXU1 D X. On the other hand, the calculations in (1.15)
and (1.16) show that UX0U 1 is of the same form as in (1.17), but with .x2; x3/
rotated by angle . Thus, ˆU is a rotation by angle  in the plane perpendicular to
X, showing that ˆU coincides with R.
ut
It is possible, if not terribly useful, to calculate ˆ explicitly. If you write an
element of SU.2/ as in Exercise 5, you (or your computer) may calculate
 ˛  Nˇ
ˇ
N˛
 
x1
x2 C ix3
x2  ix3
x1
 
N˛ Nˇ
ˇ ˛

D

x0
1
x0
2 C ix0
3
x0
2  ix0
3
x0
3


1.5
Lie Groups
25
explicitly. Then .x0
1; x0
2; x0
3/ will depend linearly on .x1; x2; x3/ and you can express
.x0
1; x0
2; x0
3/ as a matrix applied to .x1; x2; x3/, with the result that
ˆU D
0
@
j˛j2  jˇj2 2 Re.˛ˇ/
2 Im.˛ˇ/
2 Re.˛ Nˇ/ Re.˛2  ˇ2/ Im.ˇ2  ˛2/
2 Im.˛ Nˇ/ Im.˛2 C ˇ2/ Re.˛2 C ˇ2/
1
A :
If we take ˛ D ei=2 and ˇ D 0, we may see directly that ˆU is a rotation by angle
 in the .x2; x3/-plane, as we saw already in (1.15) and (1.16).
1.5
Lie Groups
A Lie group is a smooth manifold equipped with a group structure such that the
operations of group multiplication and inversion are smooth. As the terminology
suggests, every matrix Lie group is a Lie group. (See Corollary 3.45 in Chapter 3.)
The reverse is not true: Not every Lie group is isomorphic to a matrix Lie group.
Nevertheless, we have restricted our attention in this book to matrix Lie groups,
in order to minimize prerequisites and keep the discussion as concrete as possible.
Most of the interesting examples of Lie groups are, in any case, matrix Lie groups.
A manifold is an object M that looks locally like a piece of Rn. More precisely,
an n-dimensional manifold is a second-countable, Hausdorff topological space with
the property that each m 2 M has a neighborhood that is homeomorphic to an open
subset of Rn. A two-dimensional torus, for example, looks locally but not globally
like R2 and is, thus, a two-dimensional manifold. A smooth manifold is a manifold
M together with a collection of local coordinates covering M such that the change-
of-coordinates map between two overlapping coordinate systems is smooth.
Deﬁnition 1.20. A Lie group is a smooth manifold G which is also a group and
such that the group product
G  G ! G
and the inverse map G ! G are smooth.
Example 1.21. Let
G D R  R  S1 D
˚
.x; y; u/jx 2 R; y 2 R; u 2 S1  C

;
equipped with the group product given by
.x1; y1; u1/  .x2; y2; u2/ D .x1 C x2; y1 C y2; eix1y2u1u2/:
Then G is a Lie group.

26
1
Matrix Lie Groups
Proof. It is easily checked that this operation is associative; the product of three
elements with either grouping is
.x1 C x2 C x3; y1 C y2 C y3; ei.x1y2Cx1y3Cx2y3/u1u2u3/:
There is an identity element in G, namely e D .0; 0; 1/ and each element .x; y; u/
has an inverse given by .x; y; eixyu1/. Thus, G is, in fact, a group. Furthermore,
both the group product and the map that sends each element to its inverse are clearly
smooth, showing that G is a Lie group.
ut
Although there is nothing about matrices in the deﬁnition of G, we may still ask
whether G is isomorphic to some matrix Lie group. This turns out to be false. As
shown in Sect. 4.8, there is no continuous, injective homomorphism of G into any
GL.nI C/. We conclude, then, that not every Lie group is isomorphic to a matrix Lie
group. Nevertheless, most of the interesting examples of Lie groups are matrix Lie
groups.
Let us now think brieﬂy about how we might show that every matrix Lie group is
a Lie group. We will prove in Sect. 3.7 that every matrix Lie group is an "embedded
submanifold" of Mn.R/ Š R2n2. The operations of matrix multiplication and
inversion are smooth on Mn.C/ (after restricting to the open subset of invertible
matrices in the case of inversion). Thus, the restriction of these operations to a matrix
Lie group G  Mn.C/ is also smooth, making G into a Lie group.
It is customary to call a map ˆ between two Lie groups a Lie group homo-
morphism if ˆ is a group homomorphism and ˆ is smooth, whereas we have
(in Deﬁnition 1.18) required only that ˆ be continuous. We will show, however,
that every continuous homomorphism between matrix Lie groups is automatically
smooth, so that there is no conﬂict of terminology. See Corollary 3.50 to Theo-
rem 3.42. Finally, we note that since every matrix Lie group G is a manifold, G
must be locally path connected. It then follows by a standard topological argument
that G is connected if and only if it is path connected.
1.6
Exercises
1. Let Œ; n;k be the symmetric bilinear form on RnCk deﬁned in (1.5). Let g be
the .n C k/  .n C k/ diagonal matrix with ﬁrst n diagonal entries equal to one
and last k diagonal entries equal to minus one:
g D
 In
0
0 Ik

:
Show that for all x; y 2 RnCk,
Œx; yn;k D hx; gyi :

1.6
Exercises
27
Show that a .n C k/  .n C k/ real matrix A belongs to O.nI k/ if and only if
gAtrg D A1.
2. Let ! be the skew-symmetric bilinear form on R2n given by (1.7). Let  be the
2n  2n matrix
 D

0 I
I 0

:
Show that for all x; y 2 R2n, we have
!.x; y/ D hx; yi :
Show that a 2n  2n matrix A belongs to Sp.nI R/ if and only if Atr D
A1.
Note: A similar analysis applies to Sp.nI C/.
3. Show that the symplectic group Sp.1I R/  GL.2I R/ is equal to SL.2I R/.
Show that Sp.1I C/ D SL.2I C/ and that Sp.1/ D SU.2/.
4. Show that a matrix R belongs to SO.2/ if and only if it can be expressed in the
form
cos   sin 
sin 
cos 

for some  2 R. Show that a matrix R belongs to O.2/ if and only if it is of
one of the two forms:
A D
 cos   sin 
sin 
cos 

or
A D
 cos 
sin 
sin   cos 

:
Hint: Recall that for A to be in O.2/, the columns of A must be orthonormal.
5. Show that if ˛ and ˇ are arbitrary complex numbers satisfying j˛j2 Cjˇj2 D 1,
then the matrix
A D
 ˛ ˇ
ˇ
˛

is in SU.2/. Show that every A 2 SU.2/ can be expressed in this form for a
unique pair .˛; ˇ/ satisfying j˛j2 C jˇj2 D 1.
6. Suppose U belongs to M2n.C/ and U has an orthonormal basis of eigenvectors
satisfying the conditions in Theorem 1.6. Show that U belongs to Sp.n/.
Hint: Start by showing that U is unitary. Then show that !.Uz; Uw/ D !.z; w/
if z and w belong to the basis u1; : : : ; un, v1; : : : ; vn.
7. Using Theorem 1.6, show that Sp.n/ is connected and that every element of
Sp.n/ has determinant 1.

28
1
Matrix Lie Groups
8. Determine the center Z.H/ of the Heisenberg group H. Show that the quotient
group H=Z.H/ is commutative.
9. Suppose a is an irrational real number. Show that the set Ea of numbers of the
form e2ina, n 2 Z, is dense in the unit circle S1.
Hint: Show that if we divide S1 into N equally sized "bins" of length 2=N ,
there is at least one bin that contains inﬁnitely many elements of Ea. Then use
the fact that Ea is a subgroup of S1.
10. Let a be an irrational real number and let G be the following subgroup of
GL.2I C/:
G D
 eit 0
0 eita
ˇˇˇˇ t 2 R

:
Show that
G D
  ei 0
0 ei
ˇˇˇˇ ;  2 R

;
where G denotes the closure of the set G inside the space of 2  2 matrices.
Hint: Use Exercise 9.
11. A subset E of a matrix Lie group G is called discrete if for each A in E there
is a neighborhood U of A in G such that U contains no point in E except for
A. Suppose that G is a connected matrix Lie group and N is a discrete normal
subgroup of G. Show that N is contained in the center of G.
12. This problem gives an alternative proof of Proposition 1.11, namely that
GL.nI C/ is connected. Suppose A and B are invertible n  n matrices.
Show that there are only ﬁnitely many complex numbers  for which
det .A C .1  /B/ D 0. Show that there exists a continuous path A.t/
of the form A.t/ D .t/A C .1  .t//B connecting A to B and such that A.t/
lies in GL.nI C/. Here, .t/ is a continuous path in the plane with .0/ D 0
and .1/ D 1.
13. Show that SO.n/ is connected, using the following outline.
For the case n D 1, there is nothing to show, since a 1  1 matrix with
determinant one must be Œ1. Assume, then, that n  2. Let e1 denote the unit
vector with entries 1; 0; : : : ; 0 in Rn. For every unit vector v 2 Rn, show that
there exists a continuous path R.t/ in SO.n/ with R.0/ D I and R.1/v D e1.
(Thus, any unit vector can be "continuously rotated" to e1.)
Now, show that any element R of SO.n/ can be connected to a block-diagonal
matrix of the form
 1
R1

with R1 2 SO.n  1/ and proceed by induction.

1.6
Exercises
29
14. If R is an element of SO.3/, show that R must have an eigenvector v with
eigenvalue 1. Show that R maps the plane orthogonal to v into itself. Conclude
that R is a rotation by some angle  around the "axis" v.
Hint: Since SO.3/  SU.3/, every (real or complex) eigenvalue of R must
have absolute value 1. Since, also, R is real, any nonreal eigenvalues of R come
in conjugate pairs.
15. Let R be an element of SO.n/.
(a) Suppose v 2 Cn is an eigenvector for R with eigenvalue  2 C, and suppose
 is not real. Let V  Rn be the two-dimensional span of .v C Nv/=2 and
.v  Nv/=.2i/. Show that V is invariant under R and that the restriction of R
to V has determinant 1.
(b) Suppose that a subspace V  Rn is invariant under both R and R1. Show
that the orthogonal complement V ? of V is also invariant under both R and
R1.
(c) Show that if n D 2k, there exists S 2 SO.n/ such that
R D S
0
BBBBB@
cos 1  sin 1
sin 1
cos 1
:::
cos k  sin k
sin k
cos k
1
CCCCCA
S1
and that if n D 2k C 1, there exists S 2 SO.n/ such that
R D S
0
BBBBBBBB@
cos 1  sin 1
sin 1
cos 1
:::
cos k  sin k
sin k
cos k
1
1
CCCCCCCCA
S1:
That is, in a suitable orthonormal basis, R is block diagonal with 22 blocks
of the indicated form, with a single 1  1 block if n is odd.
Hint: Show that the number of eigenvalues of R equal to 1 is even.
16. (a) Show that if a matrix A commutes with every matrix X of the form (1.14),
then A commutes with every element of M2.C/. Conclude that A must be
a multiple of the identity.
(b) Show that the kernel of the map U 7! ˆU in Proposition 1.19 is precisely
the set fI; Ig.

30
1
Matrix Lie Groups
17. Suppose G  GL.n1I C/ and H  GL.n2I C/ are matrix Lie groups and that
ˆ W G ! H is a Lie group homomorphism. Then the image of G under ˆ is a
subgroup of H and thus of GL.n2I C/. Is the image of G under ˆ necessarily a
matrix Lie group? Prove or give a counter-example.
18. Show that every continuous homomorphism ˆ from R to S1 is of the form
ˆ.x/ D eiax for some a 2 R.
Hint: Since ˆ is continuous, there is some " > 0 such that if jxj < ", then ˆ.x/
belongs to the right half of the unit circle.

Chapter 2
The Matrix Exponential
2.1
The Exponential of a Matrix
The exponential of a matrix plays a crucial role in the theory of Lie groups. The
exponential enters into the deﬁnition of the Lie algebra of a matrix Lie group
(Sect. 3.3) and is the mechanism for passing information from the Lie algebra to
the Lie group.
If X is an n  n matrix, we deﬁne the exponential of X, denoted eX or exp X,
by the usual power series
eX D
1
X
mD0
Xm
mŠ ,
(2.1)
where X0 is deﬁned to be the identity matrix I and where Xm is the repeated matrix
product of X with itself.
Proposition 2.1. The series (2.1) converges for all X 2 Mn.C/ and eX is a
continuous function of X.
Our proof will use the notion of the norm of a matrix X 2 Mn.C/, which we
deﬁne by thinking of Mn.C/ as Cn2.
Deﬁnition 2.2. For any X 2 Mn.C/, we deﬁne
kXk D
0
@
n
X
j;kD1
ˇˇXjk
ˇˇ2
1
A
1=2
:
The quantity kXk is called the Hilbert-Schmidt norm of X.
© Springer International Publishing Switzerland 2015
B. Hall, Lie Groups, Lie Algebras, and Representations, Graduate
Texts in Mathematics 222, DOI 10.1007/978-3-319-13467-3_2
31

32
2
The Matrix Exponential
The Hilbert-Schmidt norm may be computed in a basis-independent way as
kXk D .trace.XX//1=2:
(2.2)
(See Sect. A.6.) This norm satisﬁes the inequalities
kX C Y k  kXk C kY k ;
(2.3)
kXYk  kXk kY k
(2.4)
for all X; Y 2 Mn.C/. The ﬁrst of these inequalities is the triangle inequality for
Cn2 and the second follows from the Cauchy-Schwarz inequality (Exercise 1). If
Xm is a sequence of matrices, then it is easy to see that Xm converges to a matrix X
in the sense of Deﬁnition 1.3 if and only if kXm  Xk ! 0 as m ! 1.
Proof of Proposition 2.1. In light of (2.4), we see that
kXmk  kXkm
for all m  1, and, hence,
1
X
mD0
				
Xm
mŠ
				  kIk C
1
X
mD1
kXkm
mŠ
< 1.
Thus, the series (2.1) converges absolutely.
To show continuity, note that since Xm is a continuous function of X, the partial
sums of (2.1) are continuous. By the Weierstrass M-test, the series (2.1) converges
uniformly on each set of the form fkXk  Rg. Thus, eX is continuous on each such
set, and, thus, continuous on all of Mn.C/.
ut
We now list some elementary properties of the matrix exponential.
Proposition 2.3. Let X and Y be arbitrary n  n matrices. Then we have the
following:
1. e0 D I.
2.

eX D eX.
3. eX is invertible and

eX1 D eX.
4. e.˛Cˇ/X D e˛XeˇX for all ˛ and ˇ in C.
5. If XY D YX, then eXCY D eXeY D eY eX.
6. If C is invertible, then eCXC 1 D CeXC 1.
Although eXCY D eXeY when X and Y commute, this identity fails in general.
This is an important point, which we will return to in the Lie product formula in
Sect. 2.4 and the Baker-Campbell-Hausdorff formula in Chapter 5.
Proof. Point 1 is obvious and Point 2 follows from taking term-by-term adjoints
of the series for eX. Points 3 and 4 are special cases of Point 5. To verify Point 5,

2.1
The Exponential of a Matrix
33
we simply multiply the two power series term by term, which is permitted because
both series converge absolutely. Multiplying out eXeY and collecting terms where
the power of X plus the power of Y equals m, we obtain
eXeY D
1
X
mD0
m
X
kD0
Xk
kŠ
Y mk
.m  k/Š D
1
X
mD0
1
mŠ
m
X
kD0
mŠ
kŠ.m  k/ŠXkY mk.
(2.5)
Now, because (and only because) X and Y commute,
.X C Y /m D
m
X
kD0
mŠ
kŠ.m  k/ŠXkY mk,
and, thus, (2.5) becomes
eXeY D
1
X
mD0
1
mŠ.X C Y /m D eXCY .
To prove Point 6, simply note that

CXC 1m D CXmC 1
and, thus, the two sides of Point 6 are equal term by term.
ut
Proposition 2.4. Let X be a n  n complex matrix. Then etX is a smooth curve in
Mn.C/ and
d
dtetX D XetX D etXX.
In particular,
d
dtetX
ˇˇˇˇ
tD0
D X.
Results that hold for the exponential of numbers may or may not hold for the
matrix exponential. Although Proposition 2.4 is what one would expect from the
scalar case, it should be noted that, in general, the derivative of eXCtY is not equal
to eXCtYY . See Sect. 5.4.
Proof. Differentiate the power series for etX term by term. This is permitted because,
for each j and k,

etX
jk is given by a convergent power series in t, and one can
differentiate a power series term by term inside its radius of convergence (e.g.,
Theorem 12 in Chapter 4 of [Pugh]).
ut

34
2
The Matrix Exponential
2.2
Computing the Exponential
We consider here methods for exponentiating general matrices. A special method
for exponentiating 2  2 matrices is described in Exercises 6 and 7. Suppose that
X 2 Mn.C/ has n linearly independent eigenvectors v1; : : : ; vn with eigenvalues
1; : : : ; n. Let C be the n  n matrix whose columns are v1; : : : ; vn and let D
be the diagonal matrix with diagonal entries 1; : : : ; n. Then X D CDC 1. It is
easily veriﬁed that eD is the diagonal matrix with diagonal entries e1; : : : ; en, and
thus, by Proposition 2.3, we have
eX D C
0
B@
e1
0
:::
0
en
1
CA C 1.
Meanwhile, if X is nilpotent (i.e., Xk D 0 for some k), then the series that
deﬁnes eX terminates. Finally, according to Theorem A.6, every matrix X can be
written (uniquely) in the form X D S CN , with S diagonalizable, N nilpotent, and
SN D NS. Then, since N and S commute,
eX D eSCN D eSeN :
Example 2.5. Consider the matrices
X1 D
 0 a
a
0

I
X2 D
0
@
0 a b
0 0 c
0 0 0
1
A I
X3 D
 a b
0 a

:
Then
eX1 D
cos a  sin a
sin a
cos a

and
eX2 D
0
@
1 a b C ac=2
0 1
c
0 0
1
1
A
and
eX3 D
 ea eab
0 ea

:

2.2
Computing the Exponential
35
Proof. The eigenvectors of X1 are .1; i/ and .i; 1/, with eigenvalues ia and ia,
respectively. Thus,
eX1 D
 1 i
i 1
  eia 0
0
eia
  1=2 i=2
i=2
1=2

;
which simpliﬁes to the claimed result. Meanwhile, X2
2 has the value ac in the upper
right-hand corner and all other entries equal to zero, whereas X3
2 D 0. Thus, eX2 D
1 C X2 C X2
2=2, which reduces to the claimed result. Finally,
X3 D
 a 0
0 a

C
0 b
0 0

;
where the two terms clearly commute and the second term is nilpotent. Thus, we
obtain
eX3 D
 ea 0
0 ea
 1 b
0 1

;
which reduces to the claimed result.
ut
The matrix exponential is used the elementary theory of differential equations, to
solve systems of linear equations. Consider a ﬁrst-order differential equation of the
form
dv
dt D Xv;
v.0/ D v0;
where v.t/ 2 Rn and X is a ﬁxed nn matrix. The (unique) solution of this equation
is given by
v.t/ D etXv0;
as may be easily veriﬁed using Proposition 2.4. Curves of the form t 7! etXv0, with
v0 ﬁxed, trace out the ﬂow along the vector ﬁeld v 7! Xv.
Let us consider the two matrices
X4 D

1 2
2 1

I
X5 D
1 2
2 1

:
(2.6)
Figure 2.1 plots several curves of this form for each matrix (see Exercise 8).

36
2
The Matrix Exponential
Fig. 2.1 Curves of the form t 7! etX4v0 (left) and t 7! etX5v0 (right)
2.3
The Matrix Logarithm
We now wish to deﬁne a matrix logarithm, which should be an inverse function
(to the extent possible) to the matrix exponential. Let us recall the situation for the
logarithm of complex numbers, in order to see what is reasonable to expect in the
matrix case. Since ez is never zero, only nonzero numbers can have a logarithm.
Every nonzero complex number can be written as ez for some z, but the z is not
unique and cannot be deﬁned continuously on C. In the matrix case, eX is invertible
for all X 2 Mn.C/. We will see (Theorem 2.10) that every invertible matrix can be
written as eX, for some X 2 Mn.C/, but the X is not unique.
The simplest way to deﬁne the matrix logarithm is by a power series. We recall
how this works in the complex case.
Lemma 2.6. The function
log z D
1
X
mD1
.1/mC1 .z  1/m
m
(2.7)
is deﬁned and analytic in a circle of radius 1 about z D 1.
For all z with jz  1j < 1,
elog z D z.
For all u with juj < log 2, we have jeu  1j < 1 and
log eu D u.

2.3
The Matrix Logarithm
37
Proof. The usual logarithm for real, positive numbers satisﬁes
d
dx log.1  x/ D
1
1  x D 

1 C x C x2 C   

for jxj < 1. Integrating term by term and noting that log1 D 0 gives
log.1  x/ D 

x C x2
2 C x3
3 C   

.
Taking z D 1  x (so that x D 1  z), we have
log z D 

.1  z/ C .1  z/2
2
C .1  z/3
3
C   

D
1
X
mD1
.1/mC1 .z  1/m
m
.
(2.8)
The series (2.8) has radius of convergence 1 and deﬁnes a holomorphic function
on the set fjz  1j < 1g, which coincides with the usual logarithm for real z in the
interval .0; 2/. Now, exp.log z/ D z for z 2 .0; 2/ and since both sides of this identity
are holomorphic in z, the identity continues to hold on the whole set fjz  1j < 1g.
On the other hand, if juj < log 2, then
jeu  1j D
ˇˇˇu C u2
2Š C   
ˇˇˇ  juj C juj2
2Š C    D ejuj  1 < 1:
Thus, log.exp u/ makes sense for all such u. Since log.exp u/ D u for real u with
juj < log 2, it follows by holomorphicity that log.exp u/ D u for all complex
numbers with juj < log 2.
ut
Deﬁnition 2.7. For an n  n matrix A, deﬁne log A by
log A D
1
X
mD1
.1/mC1 .A  I/m
m
(2.9)
whenever the series converges.
Since the complex-valued series (2.7) has radius of convergence 1 and since
k.A  I/mk  kA  Ikm for m  1, the matrix-valued series (2.9) will converge
if kA  Ik < 1. Even if kA  Ik > 1, the series might converge, for example, if
A  I is nilpotent (see Exercise 9).

38
2
The Matrix Exponential
Theorem 2.8. The function
log A D
1
X
mD1
.1/mC1 .A  I/m
m
is deﬁned and continuous on the set of all n  n complex matrices A
with kA  Ik < 1.
For all A with kA  Ik < 1,
elog A D A.
For all X with kXk < log 2,
		eX  I
		 < 1 and
log eX D X.
Although it might seem plausible that log.eX/ should be equal to X whenever
the series for the logarithm is convergent, this claim is false (even over C). If, for
example, X D 2iI, then eX D e2iI D I. Then eX  I D 0, so that log.eX/
is deﬁned and equal to 0. In this case, log.eX/ is deﬁned but not equal to X. Thus,
the assumption that kXk < log 2 cannot be replaced by, say, the assumption that
		eX  I
		 < 1.
Proof. Since k.A  I/mk  k.A  I/km and since the series (2.7) has radius of
convergence 1, the series (2.9) converges absolutely for all A with kA  Ik < 1.
The proof of continuity is essentially the same as for the exponential.
Suppose now that A satisﬁes kA  Ik < 1. If A is diagonalizable with eigenvalue
z1; : : : ; zn, then we can express A in the form CDC 1 with D diagonal, in which
case
.A  I/m D C
0
B@
.z1  1/m
0
:::
0
.zn  1/m
1
CA C 1.
Since kA  Ik < 1, each eigenvalue zj of A must satisfy
ˇˇzj  1
ˇˇ < 1 (Exercise 2).
Thus,
1
X
mD1
.1/mC1 .A  I/m
m
D C
0
B@
log z1
0
:::
0
log zn
1
CA C 1;

2.3
The Matrix Logarithm
39
and by Lemma 2.6,
elog A D C
0
B@
elog z1
0
:::
0
elog zn
1
CA C 1 D A.
If A is not diagonalizable, we approximate A by a sequence Am of diagonalizable
matrices (Exercise 4) and appeal to the continuity of the logarithm and exponential
functions. Thus, exp.log A/ D A for all A with kA  Ik < 1.
Now, the same argument as in the complex case shows that if kXk < log 2, then
		eX  I
		 < 1. The proof that log.eX/ D X is then very similar to the proof that
exp.log A/ D A.
ut
Proposition 2.9. There exists a constant c such that for all n  n matrices B with
kBk < 1
2,
klog.I C B/  Bk  c kBk2 .
Proof. Note that
log.I C B/  B D
1
X
mD2
.1/mC1 Bm
m D B2
1
X
mD2
.1/mC1 Bm2
m
so that
klog.I C B/  Bk  kBk2
1
X
mD2

 1
2
m2
m
,
which is an estimate of the desired form.
ut
We may restate the proposition in a more concise way by saying that
log.I C B/ D B C O.kBk2/;
where O.kBk2/ denotes a quantity of order kBk2 (i.e., a quantity that is bounded
by a constant times kBk2 for all sufﬁciently small values of kBk).
We conclude this section with a result that, although we will not use it elsewhere,
is worth recording. The proof is sketched in Exercises 9 and 10.
Theorem 2.10. Every invertible n  n matrix can be expressed as eX for some
X 2 Mn.C/.

40
2
The Matrix Exponential
2.4
Further Properties of the Exponential
In this section, we give several additional results involving the exponential of a
matrix that will be important in our study of Lie algebras.
Theorem 2.11 (Lie Product Formula). For all X; Y 2 Mn.C/, we have
eXCY D lim
m!1

e
X
m e
Y
m
m
.
There is a version of this result, known as the Trotter product formula, which
holds for suitable unbounded operators on an inﬁnite-dimensional Hilbert space.
See, for example, Theorem 20.1 in [Hall].
Proof. If we multiply the power series for e
X
m and e
Y
m , all but three of the terms will
involve 1=m2 or higher powers of 1=m. Thus,
e
X
m e
Y
m D I C X
m C Y
m C O
 1
m2

:
Now, since e
X
m e
Y
m ! I as m ! 1, e
X
m e
Y
m is in the domain of the logarithm for all
sufﬁciently large m. By Proposition 2.9,
log

e
X
m e
Y
m

D log

I C X
m C Y
m C O
 1
m2

D X
m C Y
m C O
 				
X
m C Y
m C O
 1
m2
				
2!
D X
m C Y
m C O
 1
m2

:
Exponentiating the logarithm then gives
e
X
m e
Y
m D exp
X
m C Y
m C O
 1
m2

and, therefore,

e
X
m e
Y
m
m
D exp

X C Y C O
 1
m

:
Thus, by the continuity of the exponential, we conclude that
lim
m!1

e
X
m e
Y
m
m
D exp .X C Y / ;
which is the Lie product formula.
ut

2.4
Further Properties of the Exponential
41
Recall (Sect. A.5) that the trace of matrix is deﬁned as the sum of its diagonal
entries and that similar matrices have the same trace.
Theorem 2.12. For any X 2 Mn.C/, we have
det 
eX D etrace.X/.
Proof. If X is diagonalizable with eigenvalues 1; : : : ; n, then eX is diagonalizable
with eigenvalues e1; : : : ; en. Thus, trace.X/ D P
j j and
det.eX/ D e1    en D e1Cn D etrace.X/:
If X is not diagonalizable, we can approximate it by matrices that are diagonalizable
(Exercise 4).
ut
Deﬁnition 2.13. A function A W R ! GL.nI C/ is called a one-parameter
subgroup of GL.nI C/ if
1. A is continuous,
2. A.0/ D I,
3. A.t C s/ D A.t/A.s/ for all t; s 2 R.
Theorem 2.14 (One-Parameter Subgroups). If A./ is a one-parameter subgroup
of GL.nI C/, there exists a unique n  n complex matrix X such that
A.t/ D etX.
By taking n D 1, and noting that GL.1I C/ Š C, this theorem provides a
method of solving Exercise 18 in Chapter 1.
Lemma 2.15. Fix some " with " < log 2. Let B"=2 be the ball of radius "=2 around
the origin in Mn.C/, and let U D exp.B"=2/. Then every B 2 U has a unique
square root C in U , given by C D exp. 1
2 log B/.
Proof. It is evident that C is a square root of B and that C is in U . To establish
uniqueness, suppose C 0 2 U satisﬁes .C 0/2 D B. Let Y D log C 0; then exp.Y / D
C 0 and
exp.2Y / D .C 0/2 D B D exp.log B/:
We have that Y 2 B"=2 and, thus, 2Y 2 B", and also that log B 2 B"=2  B". Since,
by Theorem 2.8, exp is injective on B" and exp.2Y / D exp.log B/, we must have
2Y D log B. Thus, C 0 D exp. 1
2 log B/ D C.
ut
Proof of Theorem 2.14. The uniqueness is immediate, since if there is such an X,
then X D
d
dtA.t/
ˇˇ
tD0. To prove existence, let U be as in Lemma 2.15, which is an
open set in GL.nI C/. The continuity of A guarantees that there exists t0 > 0 such
that A.t/ 2 U for all t with jtj  t0. Deﬁne

42
2
The Matrix Exponential
X D 1
t0
log.A.t0//;
so that t0X D log.A.t0//. Then t0X 2 B"=2 and
et0X D A.t0/:
Now, A.t0=2/ is again in U and A.t0=2/2 D A.t0/. But by Lemma 2.15, A.t0/ has
a unique square root in U , and that unique square root is exp.t0X=2/. Thus,
A.t0=2/ D exp.t0X=2/:
Applying this argument repeatedly, we conclude that
A.t0=2k/ D exp.t0X=2k/
for all positive integers k. Then for any integer m, we have
A.mt0=2k/ D A.t0=2k/m D exp.mt0X=2k/:
It follows that A.t/ D exp.tX/ for all real numbers t of the form t D mt0=2k, and
the set of such t's is dense in R. Since both exp.tX/ and A.t/ are continuous, it
follows that A.t/ D exp.tX/ for all real numbers t.
ut
Proposition 2.16. The exponential map is an inﬁnitely differentiable map of Mn.C/
into Mn.C/.
We will compute the derivative of the matrix exponential in Chapter 5.
Proof. Note that for each j and k, the quantity .Xm/jk is a homogeneouspolynomial
of degree m in the entries of X. Thus, the series for the function .Xm/jk has the form
of a multivariable power series on Mn.C/ Š R2n2. Since the series converges on all
of R2n2, it is permissible to differentiate the series term by term as many times as
we like. (Apply Theorem 12 in Chapter 4 of [Pugh] in each of the n2 variables with
the other variables ﬁxed.)
ut
2.5
The Polar Decomposition
The polar decomposition for a nonzero complex number z states that z can be written
uniquely as z D up, where juj D 1 and p is real and positive. (If z D 0, the
decomposition still exists, with p D 0, but u is not unique.) Since p is real and
positive, it can be written as p D ex for a unique real number x. This gives an
unconventional form of the polar decomposition for z, namely

2.5
The Polar Decomposition
43
z D uex;
(2.10)
with x 2 R and juj D 1. Although it is customary to leave p as a positive real
number and to write u as u D ei, the decomposition in (2.10) is more convenient
for us because x, unlike , is unique.
We wish to establish a similar polar decomposition ﬁrst for GL.nI C/ and then
for various subgroups thereof. If P is a self-adjoint n  n matrix (i.e., P  D P ), we
say that P is positive if hv; P vi > 0 for all nonzero v 2 Cn. It is easy to check that
a self-adjoint matrix P is positive if and only if all the eigenvalues of P are positive.
Suppose now that A is an invertible n  n matrix. We wish to write A as A D UP
where U is unitary and P is self-adjoint and positive. We will then write the self-
adjoint, positive matrix P as P D eX where X is self-adjoint but not necessarily
positive.
Theorem 2.17.
1. Every A 2 GL.nI C/ can be written uniquely in the form
A D UP
where U is unitary and P is self-adjoint and positive.
2. Every self-adjoint positive matrix P can be written uniquely in the form
P D eX
with X self-adjoint. Conversely, if X is self-adjoint, then eX is self-adjoint and
positive.
3. If we decompose each A 2 GL.nI C/ (uniquely) as
A D UeX
with U unitary and X self-adjoint, then U and X depend continuously on A.
Lemma 2.18. If Q is a self-adjoint, positive matrix, then Q has a unique positive,
self-adjoint square root.
Proof. Since Q has an orthonormal basis of eigenvectors, Q can be written as
Q D U
0
B@
1
:::
n
1
CA U 1
with U unitary. Since Q is self-adjoint and positive, each j is positive. Thus, we
can construct a square root of Q as

44
2
The Matrix Exponential
Q1=2 D U
0
B@
1=2
1
:::
1=2
n
1
CA U 1;
(2.11)
and Q1=2 will still be self-adjoint and positive, establishing the existence of the
square root.
If P is a self-adjoint, positive matrix, the eigenspaces of P 2 are precisely the
same as the eigenspaces of P , with the eigenvalues of P 2 being, of course, the
squares of the eigenvalues of P . The point here is that because the function x 7! x2
is injective on positive real numbers, eigenspaces with distinct eigenvalues remain
with distinct eigenvalues after squaring. Looking at this claim the other way around,
if a positive, self-adjoint matrix Q is to have a positive self-adjoint square root P ,
the eigenspaces of P must be the same as the eigenspaces of Q, and the eigenvalues
of P must be the positive square roots of the eigenvalues of Q. Thus, P is uniquely
determined by Q.
ut
Proof of Theorem 2.17. For the existence of the decomposition in Point 1, note that
if A D UP , then AA D P U UP D P 2. Now, for any matrix A, the matrix AA
is self-adjoint. If, in addition, A is invertible, then for all nonzero v 2 Cn, we have
hv; AAvi D hAv; Avi > 0;
showing that A is positive. For all invertible A, then, let us deﬁne P by
P D .AA/1=2;
where ./1=2 is the unique positive square root of Lemma 2.18. We then deﬁne
U D AP1 D AŒ.AA/1=21:
Since P is, by construction, self-adjoint and positive, and since A D UP by the
deﬁnition of U , it remains only to check that U is unitary. To that end, we check
that
U U D Œ.AA/1=21AAŒ.AA/1=21;
since the inverse of a positive self-adjoint matrix is self-adjoint. Since AA is the
square of .AA/1=2, we see that U U D I, showing that U is unitary.
For the uniqueness of the decomposition, we have already noted that if A D UP ,
then P 2 D AA, where AA is self-adjoint and positive. Thus, the uniqueness of
P follows from the uniqueness in Lemma 2.18. The uniqueness of U then follows,
since if A D UP, then U D AP1.
The existence and uniqueness of the decomposition in Point 2 are proved in
precisely the same way as in Lemma 2.18, with the logarithm function (which is
a bijection between .0; 1/ and R) replacing the square root function. The same sort

2.5
The Polar Decomposition
45
of reasoning shows that for any self-adjoint X, the matrix eX is self-adjoint and
positive.
Finally, we address the continuity claim in Point 3. From the formulas for P and
U in terms of A in the proof of Point 1, we see that U and P depend continuously
on A. It remains only to show that the logarithm X of P depends continuously
on P . To see this, note that if the eigenvalues of P are between 0 and 2, then the
power series for log P will converge to X, in which case, continuity follows by
the same argument as in the proof of Proposition 2.1. In general, ﬁx some positive,
self-adjoint matrix P0. Choose some large positive number a, and for P in a small
neighborhood V of P0, write P D ea.eaP /. Then P D eX, where
X D aI C log.eaP /:
Since a is large, the eigenvalues of eaP will all be less than log 2, and the series
for log.eaP / will converge and depend continuously on P .
ut
We now establish polar decompositions for GL.nI R/, SL.nI C/, and SL.nI R/.
Proposition 2.19.
1. Every A 2 GL.nI R/ can be written uniquely as
A D ReX;
where R is in O.n/ and X is real and symmetric.
2. Every A 2 SL.nI C/ can be written uniquely as
A D UeX;
where U is in SU.n/ and X is self-adjoint with trace zero.
3. Every A 2 SL.nI R/ can be written uniquely as
A D ReX;
where R is in SO.n/ and X is real and symmetric and has trace zero.
Proof. If A is real, then AA is real and symmetric. Now, a real, symmetric matrix
can be diagonalized over R. Thus, P , which is the unique self-adjoint positive
square root of AA (constructed as in (2.11)), is real. Then U D AP1 is real
and unitary, hence in O.n/.
Meanwhile, if A 2 SL.nI C/ and we write A D UeX with U 2 U.n/ and X self-
adjoint, then det.A/ D det.U /etrace.X/. Now, jdet.U /j D 1, and etrace.X/ is real and
positive. Thus, by the uniqueness of the polar decomposition for nonzero complex
numbers, we must have det.U / D 1 and trace.X/ D 0. The case of A 2 SL.nI R/
follows by combining the arguments in the two previous cases.
ut

46
2
The Matrix Exponential
2.6
Exercises
1. The Cauchy-Schwarz inequality from elementary analysis tells us that for all
u D .u1; : : : ; un/ and v D .v1; : : : ; vn/ in Cn, we have
ju1v1 C    C unvnj2 
0
@
n
X
j D1
ˇˇuj
ˇˇ2
1
A
 n
X
kD1
jvkj2
!
:
Use this to verify that kXYk  kXk kY k for all X; Y 2 Mn.C/, where kk is
the Hilbert-Schmidt norm in Deﬁnition 2.2.
2. Show that for X 2 Mn.C/ and any orthonormal basis fu1; : : : ; ung of Cn,
kXk2 D Pn
j;kD1
ˇˇ˝
uj; Xuk
˛ˇˇ2, where kXk is as in Deﬁnition 2.2. Now show
that if v is an eigenvector for X with eigenvalue , then jj  kXk.
3. The product rule. Recall that a matrix-valued function A.t/ is said to be smooth
if each Ajk.t/ is smooth. The derivative of such a function is deﬁned as
dA
dt

jk
D dAjk
dt
or, equivalently,
d
dtA.t/ D lim
h!0
A.t C h/  A.t/
h
.
Let A.t/ and B.t/ be two such functions. Prove that A.t/B.t/ is again smooth
and that
d
dt ŒA.t/B.t/ D dA
dt B.t/ C A.t/dB
dt :
4. Using Theorem A.4, show that every n  n complex matrix A is the limit of a
sequence of diagonalizable matrices.
Hint: If an n  n matrix has n distinct eigenvalues, it is necessarily
diagonalizable.
5. For any a and d in C, deﬁne the expression .ea  ed/=.a  d/ in the obvious
way for a ¤ d and by means of the limit
lim
a!d
ea  ed
a  d
D ea
when a D d. Show that for any a; b; c 2 C, we have

2.6
Exercises
47
exp
 a b
0 d

D
 
ea b eaed
ad
0
ed
!
:
Hint: Show that if a ¤ d, then
a b
0 d
m
D
am b amd m
ad
0
bm

for every positive integer m.
6. Show that every 2  2 matrix X with trace.X/ D 0 satisﬁes
X2 D  det.X/I:
If X is 2  2 with trace zero, show by direct calculation using the power series
for the exponential that
eX D cos
p
det X

I C sin
p
det X
p
det X
X;
(2.12)
where
p
det X is either of the two (possibly complex) square roots of det X. Use
this to give an alternative computation of the exponential eX1 in Example 2.5.
Note: The value of the coefﬁcient of X in (2.12) is to be interpreted as 1 when
det X D 0, in accordance with the limit lim!0 sin = D 1.
7. Use the result of Exercise 6 to compute the exponential of the matrix
X D

4 3
1 2

:
Hint: Reduce the calculation to the trace-zero case.
8. Consider the two matrices X4 and X5 in (2.6). Compute etX4 and etX5 either by
diagonalization or by the method in Exercises 6 and 7. Show that curves of the
form t 7! etX4v0, with v0 ¤ 0, spiral out to inﬁnity. Show that for v0 outside of
a certain one-dimensional subspace of R2, curves of the form t 7! etX5v0 tend
to inﬁnity in the direction of .1; 1/ or the direction of .1; 1/.
9. A matrix A is said to be unipotent if A  I is nilpotent (i.e., if A is of the
form A D I C N , with N nilpotent). Note that log A is deﬁned whenever A is
unipotent, because the series in Deﬁnition 2.7 terminates.
(a) Show that if A is unipotent, then log A is nilpotent.
(b) Show that if X is nilpotent, then eX is unipotent.
(c) Show that if A is unipotent, then exp.log A/ D A and that if X is nilpotent,
then log.exp X/ D X.
Hint: Let A.t/ D I Ct.AI/. Show that exp.log.A.t/// depends polynomially
on t and that exp.log.A.t/// D A.t/ for all sufﬁciently small t.

48
2
The Matrix Exponential
10. Show that every invertible n  n matrix A can be written as A D eX for some
X 2 Mn.C/.
Hint: Theorem A.5 implies that A is similar to a block-diagonal matrix in which
each block is of the form I C N, with N being nilpotent. Use this result and
Exercise 9.
11. Show that for all X 2 Mn.C/, we have
lim
m!1

I C X
m
m
D eX:
Hint: Use the matrix logarithm.

Chapter 3
Lie Algebras
3.1
Deﬁnitions and First Examples
We now introduce the "abstract" notion of a Lie algebra. In Sect. 3.3, we will
associate to each matrix Lie group a Lie algebra. It is customary to use lowercase
Gothic (Fraktur) characters such as g and h to refer to Lie algebras.
Deﬁnition 3.1. A ﬁnite-dimensional real or complex Lie algebra is a ﬁnite-
dimensional real or complex vector space g, together with a map Œ;  from g  g
into g, with the following properties:
1. Œ;  is bilinear.
2. Œ;  is skew symmetric: ŒX; Y  D  ŒY; X for all X; Y 2 g.
3. The Jacobi identity holds:
ŒX; ŒY; Z C ŒY; ŒZ; X C ŒZ; ŒX; Y  D 0
for all X; Y; Z 2 g.
Two elements X and Y of a Lie algebra g commute if ŒX; Y  D 0. A Lie algebra
g is commutative if ŒX; Y  D 0 for all X; Y 2 g.
The map Œ;  is referred to as the bracket operation on g. Note also that
Condition 2 implies that ŒX; X D 0 for all X 2 g. The bracket operation on a
Lie algebra is not, in general associative; nevertheless, the Jacobi identity can be
viewed as a substitute for associativity.
Example 3.2. Let g D R3 and let Œ;  W R3  R3 ! R3 be given by
Œx; y D x  y;
where x  y is the cross product (or vector product). Then g is a Lie algebra.
© Springer International Publishing Switzerland 2015
B. Hall, Lie Groups, Lie Algebras, and Representations, Graduate
Texts in Mathematics 222, DOI 10.1007/978-3-319-13467-3_3
49

50
3
Lie Algebras
Proof. Bilinearity and skew symmetry are standard properties of the cross product.
To verify the Jacobi identity, it sufﬁces (by bilinearity) to verify it when x D ej,
y D ek, and z D el, where e1, e2, and e3 are the standard basis elements for R3.
If j, k, and l are all equal, each term in the Jacobi identity is zero. If j, k, and l
are all different, the cross product of any two of ej , ek, and el is equal to a multiple
of the third, so again, each term in the Jacobi identity is zero. It remains to consider
the case in which two of j; k; l are equal and the third is different. By re-ordering
the terms in the Jacobi identity as necessary, it sufﬁces to verify the identity
Œej ; Œej ; ek C Œej; Œek; ej C Œek; Œej; ej :
(3.1)
The ﬁrst two terms in (3.1) are negatives of each other and the third is zero.
ut
Example 3.3. Let A be an associative algebra and let g be a subspace of A such that
XYYX 2 g for all X; Y 2 g. Then g is a Lie algebra with bracket operation given by
ŒX; Y  D XY  YX:
Proof. The bilinearity and skew symmetry of the bracket are evident. To verify the
Jacobi identity, note that each double bracket generates four terms, for a total of 12
terms. It is left to the reader to verify that the product of X, Y , and Z in each of the
six possible orderings occurs twice, once with a plus sign and once with a minus
sign.
ut
If we look carefully at the proof of the Jacobi identity, we see that the two
occurrences of, say, XYZ occur with different groupings, once as X.YZ/ and once
as .XY/Z. Thus, associativity of the algebra A is essential. For any Lie algebra, the
Jacobi identity means that the bracket operation behaves as if it were XY  YX in
some associative algebra, even if it is not actually deﬁned this way. Indeed, we will
prove in Chapter 9 that every Lie algebra g can be embedded into an associative
algebra A in such a way that the bracket becomes XY  YX. (This claim follows
from Theorem 6.7, the Poincaré-Birkhoff-Witt theorem.)
Of particular interest to us is the case in which A is the space Mn.C/ of all n  n
complex matrices.
Example 3.4. Let sl.nI C/ the space X 2 Mn.C/ for which trace.X/ D 0. Then
sl.nI C/ is a Lie algebra with bracket ŒX; Y  D XY  YX.
Proof. For any X and Y in Mn.C/, we have
trace.XY  YX/ D trace.XY/  trace.YX/ D 0:
This holds, in particular, if X and Y have trace zero. Thus, Example 3.3 applies. ut
Deﬁnition 3.5. A subalgebra of a real or complex Lie algebra g is a subspace h of
g such that ŒH1; H2 2 h for all H1 and H2 2 h. If g is a complex Lie algebra and
h is a real subspace of g which is closed under brackets, then h is said to be a real
subalgebra of g.

3.1
Deﬁnitions and First Examples
51
A subalgebra h of a Lie algebra g is said to be an ideal in g if ŒX; H 2 h for all
X in g and H in h.
The center of a Lie algebra g is the set of all X 2 g for which ŒX; Y  D 0 for all
Y 2 g.
Deﬁnition 3.6. If g and h are Lie algebras, then a linear map  W g ! h is called a
Lie algebra homomorphism if  .ŒX; Y / D Œ.X/; .Y / for all X; Y 2 g. If, in
addition,  is one-to-one and onto, then  is called a Lie algebra isomorphism.
A Lie algebra isomorphism of a Lie algebra with itself is called a Lie algebra
automorphism.
Deﬁnition 3.7. If g is a Lie algebra and X is an element of g, deﬁne a linear map
adX W g ! g by
adX.Y / D ŒX; Y .
The map X 7! adX is the adjoint map or adjoint representation.
Although adX.Y / is just ŒX; Y , the alternative "ad" notation can be useful. For
example, instead of writing
ŒX; ŒX; ŒX; ŒX; Y ;
we can now write
.adX/4 .Y /.
This sort of notation will be essential in Chapter 5. We can view ad (that is, the map
X 7! adX) as a linear map of g into End.g/, the space of linear operators on g.
The Jacobi identity is then equivalent to the assertion that adX is a derivation of the
bracket:
adX.ŒY; Z/ D ŒadX.Y /; Z C ŒY; adX.Z/:
(3.2)
Proposition 3.8. If g is a Lie algebra, then
adŒX;Y  D adXadY  adY adX D ŒadX; adY ;
that is, adW g ! End.g/ is a Lie algebra homomorphism.
Proof. Observe that
adŒX;Y .Z/ D ŒŒX; Y ; Z;
whereas
ŒadX; adY .Z/ D ŒX; ŒY; Z  ŒY; ŒX; Z.

52
3
Lie Algebras
Thus, we want to show that
ŒŒX; Y ; Z D ŒX; ŒY; Z  ŒY; ŒX; Z;
which is equivalent to the Jacobi identity.
ut
Deﬁnition 3.9. If g1 and g2 are Lie algebras, the direct sum of g1 and g2 is the
vector space direct sum of g1 and g2, with bracket given by
Œ.X1; X2/; .Y1; Y2/ D .ŒX1; Y1; ŒX2; Y2/:
(3.3)
If g is a Lie algebra and g1 and g2 are subalgebras, we say that g decomposes as
the Lie algebra direct sum of g1 and g2 if g is the direct sum of g1 and g2 as vector
spaces and ŒX1; X2 D 0 for all X1 2 g1 and X2 2 g2.
It is straightforward to verify that the bracket in (3.3) makes g1 ˚ g2 into a Lie
algebra. If g decomposes as a Lie algebra direct sum of subalgebras g1 and g2, it is
easy to check that g is isomorphic as a Lie algebra to the "abstract" direct sum of g1
and g2. (This would not be the case without the assumption that every element of g1
commutes with every element of g2.)
Deﬁnition 3.10. Let g be a ﬁnite-dimensional real or complex Lie algebra, and let
X1; : : : ; XN be a basis for g (as a vector space). Then the unique constants cjkl such
that
ŒXj; Xk D
N
X
lD1
cjklXl
(3.4)
are called the structure constants of g (with respect to the chosen basis).
Although we will not have much occasion to use them, structure constants
do appear frequently in the physics literature. The structure constants satisfy the
following two conditions:
cjkl C ckjl D 0;
X
n
.cjkncnlm C cklncnjm C cljncnkm/ D 0
for all j; k; l; m. The ﬁrst of these conditions comes from the skew symmetry of the
bracket, and the second comes from the Jacobi identity.

3.2
Simple, Solvable, and Nilpotent Lie Algebras
53
3.2
Simple, Solvable, and Nilpotent Lie Algebras
In this section, we consider various special types of Lie algebras. Recall from
Deﬁnition 3.5 the notion of an ideal in a Lie algebra.
Deﬁnition 3.11. A Lie algebra g is called irreducible if the only ideals in g are g
and f0g. A Lie algebra g is called simple if it is irreducible and dim g  2.
A one-dimensional Lie algebra is certainly irreducible, since it is has no
nontrivial subspaces and therefore no nontrivial subalgebras and no nontrivial
ideals. Nevertheless, such a Lie algebra is, by deﬁnition, not considered simple.
Note that a one-dimensional Lie algebra g is necessarily commutative, since
ŒaX; bX D 0 for any X 2 g and any scalars a and b. On the other hand,
if g is commutative, then any subspace of g is an ideal. Thus, the only way a
commutative Lie algebra can be irreducible is if it is one dimensional. Thus, an
equivalent deﬁnition of "simple" is that a Lie algebra is simple if it is irreducible
and noncommutative.
There is an analogy between groups and Lie algebras, in which the role of
subgroups is played by subalgebras and the role of normal subgroups is played by
ideals. (For example, the kernel of a Lie algebra homomorphism is always an ideal,
just as the kernel of a Lie group homomorphismis always a normal subgroup.)There
is, however, an inconsistency in the terminology in the two ﬁelds. On the group side,
any group with no nontrivial normal subgroups is called simple, including the most
obvious example, a cyclic group of prime order. On the Lie algebra side, by contrast,
the most obvious example of an algebra with no nontrivial ideals—namely, a one-
dimensional algebra—is not called simple.
We will eventually see many examples of simple Lie algebras, but for now
we content ourselves with a single example. Recall the Lie algebra sl.nI C/ in
Example 3.4.
Proposition 3.12. The Lie algebra sl.2I C/ is simple.
Proof. We use the following basis for sl.2I C/:
X D
 0 1
0 0

I
Y D
0 0
1 0

;
H D
 1
0
0 1

:
Direct calculation shows that these basis elements have the following commutation
relations: ŒX; Y  D H, ŒH; X D 2X, and ŒH; Y  D 2Y . Suppose h is an ideal in
sl.2I C/ and that h contains an element Z D aX C bH C cY, where a, b, and c are
not all zero. We will show, then, that h D sl.2I C/. Suppose ﬁrst that c ¤ 0. Then
the element
ŒX; ŒX; Z D ŒX; Œ2bX C cH D 2cX

54
3
Lie Algebras
is a nonzero multiple of X. Since h is an ideal, we conclude that X 2 h. But ŒY; X
is a nonzero multiple of H and ŒY; ŒY; X is a nonzero multiple of Y , showing that
Y and H also belong to h, from which we conclude that h D sl.2I C/.
Suppose next that c D 0 but b ¤ 0. Then ŒX; Z is a nonzero multiple of X
and we may then apply the same argument in the previous paragraph to show that
h D sl.2I C/. Finally, if c D 0 and b D 0 but a ¤ 0, then Z itself is a nonzero
multiple of X and we again conclude that h D sl.2I C/.
ut
Deﬁnition 3.13. If g is a Lie algebra, then the commutator ideal in g, denoted
Œg; g, is the space of linear combinations of commutators, that is, the space of
elements Z in g that can be expressed as
Z D c1ŒX1; Y1 C    C cmŒXm; Ym
for some constants cj and vectors Xj ; Yj 2 g.
For any X and Y in g, the commutator ŒX; Y  is in Œg; g. This holds, in particular,
if X is in Œg; g, showing that Œg; g is an ideal in g.
Deﬁnition 3.14. For any Lie algebra g, we deﬁne a sequence of subalgebras
g0; g1; g2; : : : of g inductively as follows: g0 D g, g1 D Œg0; g0, g2 D Œg1; g1,
etc. These subalgebras are called the derived series of g. A Lie algebra g is called
solvable if gj D f0g for some j.
In light of the comments following Deﬁnition 3.13, each derived algebra gj is an
ideal in gj 1, but not necessarily an ideal in g.
Deﬁnition 3.15. For any Lie algebra g, we deﬁne a sequence of ideals gj in g
inductively as follows. We set g0 D g and then deﬁne gj C1 to be the space of
linear combinations of commutators of the form ŒX; Y  with X 2 g and Y 2 gj.
These algebras are called the upper central series of g. A Lie algebra g is said to
be nilpotent if gj D f0g for some j.
Equivalently, gj is the space spanned by all jth-order commutators,
ŒX1; ŒX2; ŒX3; : : : ŒXj; Xj C1 : : ::
Note that every jth-order commutator is also a .j  1/th-order commutator, by
setting QXj D ŒXj; Xj C1. Thus, gj 1  gj. For every X 2 g and Y 2 gj, we have
ŒX; Y  2 gj C1  gj , showing that gj is an ideal in g. Furthermore, it is clear that
gj  gj for all j; thus, if g is nilpotent, g is also solvable.
Proposition 3.16. If g  M3.R/ denotes the space of 3  3 upper triangular
matrices with zeros on the diagonal, then g satisﬁes the assumptions of Example 3.3.
The Lie algebra g is a nilpotent Lie algebra.

3.3
The Lie Algebra of a Matrix Lie Group
55
Proof. We will use the following basis for g,
X D
0
@
0 1 0
0 0 0
0 0 0
1
A I
Y D
0
@
0 0 0
0 0 1
0 0 0
1
A I
Z D
0
@
0 0 1
0 0 0
0 0 0
1
A :
(3.5)
Direct calculation then establishes the following commutation relations: ŒX; Y  D
Z and ŒX; Z D ŒY; Z D 0. In particular, the bracket of two elements of g is again
in g, so that g is a Lie algebra. Then Œg; g is the span of Z and Œg; Œg; g D 0,
showing that g is nilpotent.
ut
Proposition 3.17. If g  M2.C/ denotes the space of 2  2 matrices of the form
a b
0 c

with a, b, and c in C, then g satisﬁes the assumptions of Example 3.3. The Lie
algebra g is solvable but not nilpotent.
Proof. Direct calculation shows that
a b
0 c

;
d e
0 f

D
 0 h
0 0

;
(3.6)
where h D aeCbf bd ce, showing that g is a Lie subalgebra of M2.C/. Further-
more, the commutator ideal Œg; g is one dimensional and hence commutative. Thus,
g2 D f0g, showing that g is solvable. On the other hand, consider the following
elements of g:
H D
1
0
0 1

I
X D
 0 1
0 0

:
Using (3.6), we can see that ŒH; X D 2X, and thus that
ŒH; ŒH; ŒH;    ŒH; X    
is a nonzero multiple of X, showing that gj ¤ f0g for all j.
ut
3.3
The Lie Algebra of a Matrix Lie Group
In this section, we associate to each matrix Lie group G a Lie algebra g. Many
questions involving a group can be studied by transferring them to the Lie algebra,
where we can use tools of linear algebra. We begin by deﬁning g as a set, and then
proceed to give g the structure of a Lie algebra.

56
3
Lie Algebras
Deﬁnition 3.18. Let G be a matrix Lie group. The Lie algebra of G, denoted g, is
the set of all matrices X such that etX is in G for all real numbers t.
Equivalently, X is in g if and only if the entire one-parameter subgroup
(Deﬁnition 2.13) generated by X lies in G. Note that merely having eX in G
does not guarantee that X is in g. Even though G is a subgroup of GL.nI C/ (and
not necessarily of GL.nI R/), we do not require that etX be in G for all complex
numbers t, but only for all real numbers t. We will show in Sect. 3.7 that every
matrix Lie group is an embedded submanifold of GL.nI C/. We will then show
(Corollary 3.46) that g is the tangent space to G at the identity.
We will now establish various basic properties of the Lie algebra g of a matrix
Lie group G. In particular, we will see that there is a bracket operation on g that
makes g into a Lie algebra in the sense of Deﬁnition 3.1.
Proposition 3.19. Let G be a matrix Lie group, and X an element of its Lie algebra.
Then eX is an element of the identity component G0 of G.
Proof. By deﬁnition of the Lie algebra, etX lies in G for all real t. However, as t
varies from 0 to 1, etX is a continuous path connecting the identity to eX.
ut
Theorem 3.20. Let G be a matrix Lie group with Lie algebra g. If X and Y are
elements of g, the following results hold.
1. AXA1 2 g for all A 2 G.
2. sX 2 g for all real numbers s.
3. X C Y 2 g.
4. XY  YX 2 g.
It follows from this result and Example 3.3 that the Lie algebra of a matrix Lie
group is a real Lie algebra, with bracket given by ŒX; Y  D XY  YX. For X and Y
in g, we refer to ŒX; Y  D XY YX 2 g as the bracket or commutator of X and Y .
Proof. For Point 1, we observe that, by Proposition 2.3,
et.AXA1/ D AetXA1 2 G
for all t, showing that AXA1 is in g. For Point 2, we observe that et.sX/ D e.ts/X,
which must be in G for all t 2 R if X is in g. For Point 3 we use the Lie product
formula, which says that
et.XCY / D lim
m!1

etX=metY=mm .
Thus,

etX=metY=mm is in G for all m. Since G is closed, the limit (which is
invertible) must be again in G. This shows that X C Y is again in g:
Finally, for Point 4, we use the product rule (Exercise 3) and Proposition 2.4 to
compute

3.4
Examples
57
d
dt

etXYetXˇˇˇˇ
tD0
D .XY/e0 C .e0Y /.X/
D XY  YX.
Now, by Point 1, etXYetX is in g for all t. Furthermore, by Points 2 and 3, g is a real
subspace of Mn.C/, from which it follows that g is a (topologically) closed subset
of Mn.C/. Thus,
XY  YX D lim
h!0
ehXYehX  Y
h
belongs to g.
ut
Note that even if the elements of G have complex entries, the Lie algebra g of
G is not necessarily a complex vector space, since Point 2 holds, in general, only
for s 2 R. Nevertheless, it may happen in certain cases that g is a complex vector
space.
Deﬁnition 3.21. A matrix Lie group G is said to be complex if its Lie algebra g is
a complex subspace of Mn.C/, that is, if iX 2 g for all X 2 g.
Examples of complex groups are GL.nI C/, SL.nI C/, SO.nI C/, and Sp.nI C/,
as the calculations in Sect. 3.4 will show.
Proposition 3.22. If G is commutative then g is commutative.
We will see in Sect. 3.7 that if G is connected and g is commutative, G must be
commutative.
Proof. For any two matrices X; Y 2 Mn.C/, the commutator of X and Y may be
computed as
ŒX; Y  D d
dt
 d
dsetXesY etX
ˇˇˇˇ
sD0
ˇˇˇˇ
tD0
:
(3.7)
If G is commutative and X and Y belong to g, then etX commutes with esY and the
expression in parentheses on the right hand side of (3.7) is independent of t, so that
ŒX; Y  D 0.
ut
3.4
Examples
Physicists are accustomed to using the map t 7! eitX rather than t 7! etX. Thus,
the physicists' expressions for the Lie algebras of matrix Lie groups will differ by a
factor of i from the expressions we now derive.

58
3
Lie Algebras
Proposition 3.23. The Lie algebra of GL.nI C/ is the space Mn.C/ of all n  n
matrices with complex entries. Similarly, the Lie algebra of GL.nI R/ is equal to
Mn.R/. The Lie algebra of SL.nI C/ consists of all n  n complex matrices with
trace zero, and the Lie algebra of SL.nI R/ consists of all n  n real matrices with
trace zero.
We denote the Lie algebras of these groups as gl.nI C/, gl.nI R/, sl.nI C/, and
sl.nI R/, respectively.
Proof. If X 2 Mn.C/, then etX is invertible, so that X belongs to the Lie algebra
of GL.nI C/. If X 2 Mn.R/, then etX is invertible and real, so that X is in
the Lie algebra of GL.nI R/. Conversely, if etX is real for all real t, then X D
d.etX/=dt
ˇˇ
tD0 must also real. If X 2 Mn.C/ has trace zero, then by Theorem 2.12,
det.etX/ D 1, showing that X is in the Lie algebra of SL.nI C/. Conversely, if
det.etX/ D ettrace.X/ D 1 for all real t, then
trace.X/ D d
dtet trace.X/
ˇˇˇˇ
tD0
D 0:
Finally, if X is real and has trace zero, then etX is real and has determinant 1 for all
real t, showing that X is in the Lie algebra of SL.nI R/. Conversely, if etX is real
and has determinant 1 for all real t, the preceding arguments show that X must be
real and have trace zero.
ut
Proposition 3.24. The Lie algebra of U.n/ consists of all complex matrices
satisfying X D X and the Lie algebra of SU.n/ consists of all complex matrices
satisfying X D X and trace.X/ D 0. The Lie algebra of the orthogonal group
O.n/ consists of all real matrices X satisfying Xtr D X and the Lie algebra of
SO.n/ is the same as that of O.n/.
The Lie algebras of U.n/ and SU.n/ are denoted u.n/ and su.n/, respectively.
The Lie algebra of SO.n/ (which is the same as that of O.n/) is denoted so.n/.
Proof. A matrix U is unitary if and only if U  D U 1. Thus, etX is unitary if and
only if

etX D

etX1 D etX.
(3.8)
By Point 2 of Proposition 2.3,

etX D etX, and so (3.8) becomes
etX D etX.
(3.9)
The condition (3.9) holds for all real t if and only if X D X. Thus, the Lie
algebra of U.n/ consists precisely of matrices X such that X D X. As in the
proof of Proposition 3.23, adding the "determinant 1" condition at the group level
adds the "trace 0" condition at the Lie algebra level.

3.4
Examples
59
An exactly similar argument over R shows that a real matrix X belongs to the Lie
algebra of O.n/ if and only if Xtr D X. Since any such matrix has trace.X/ D 0
(since the diagonal entries of X are all zero), we see that every element of the Lie
algebra of O.n/ is also in the Lie algebra of SO.n/.
ut
Proposition 3.25. If g is the matrix in Exercise 1 of Chapter 1, then the Lie algebra
of O.nI k/ consists precisely of those real matrices X such that
gXtrg D X;
and the Lie algebra of SO.nI k/ is the same as that of O.nI k/. If  is the
matrix (1.8), then the Lie algebra of Sp.nI R/ consists precisely of those real
matrices X such that
Xtr D X;
and the Lie algebra of Sp.nI C/ consists precisely of those complex matrices X
satisfying the same condition. The Lie algebra of Sp.n/ consists precisely of those
complex matrices X such that Xtr D X and X D X.
The veriﬁcation of Proposition 3.25 is similar to our previous computations and
is omitted. The Lie algebra of SO.nI k/ (which is the same as that of O.nI k/) is
denoted so.nI k/, whereas the Lie algebras of the symplectic groups are denoted
sp.nI R/, sp.nI C/, and sp.n/.
Proposition 3.26. The Lie algebra of the Heisenberg group H in Sect. 1.2.6 is the
space of all matrices of the form
X D
0
@
0 a b
0 0 c
0 0 0
1
A ;
(3.10)
with a; b; c 2 R.
Proof. If X is strictly upper triangular, it is easy to verify that Xm will be strictly
upper triangular for all positive integers m. Thus, for X as in (3.10), we will have
etX D I C B with B strictly upper triangular, showing that etX 2 H. Conversely, if
etX belongs to H for all real t, then all of the entries of etX on or below the diagonal
are independent of t. Thus, X D d.etX/=dt
ˇˇ
tD0 will be of the form in (3.10).
ut
We leave it as an exercise to determine the Lie algebras of the Euclidean and
Poincaré groups.
Example 3.27. The following elements form a basis for the Lie algebra su.2/:
E1 D 1
2
 i
0
0 i

I
E2 D 1
2
 0 i
i 0

I
E3 D 1
2
 0 1
1
0

:

60
3
Lie Algebras
These elements satisfy the commutation relations ŒE1; E2 D E3, ŒE2; E3 D E1,
and ŒE3; E1 D E2. The following elements form a basis for the Lie algebra so.3/:
F1 D
0
@
0 0
0
0 0 1
0 1
0
1
A ;
F2 D
0
@
0 0 1
0 0 0
1 0 0
1
A ;
F3 D
0
@
0 1 0
1
0 0
0
0 0
1
A;
These elements satisfy the commutation relations ŒF1; F2 D F3, ŒF2; F3 D F1, and
ŒF3; F1 D F2.
Note that the listed relations completely determine all commutation relations
among, say, E1, E2, and E3, since by the skew symmetry of the bracket, we must
have ŒE1; E1 D 0, ŒE2; E1 D E3, and so on. Since E1, E2, and E3 satisfy the
same commutation relations as F1, F2, and F3, the two Lie algebras are isomorphic.
Proof. Direct calculation from Proposition 3.24.
ut
3.5
Lie Group and Lie Algebra Homomorphisms
The following theorem tells us that a Lie group homomorphism between two Lie
groups gives rise in a natural way to a map between the corresponding Lie algebras.
It will follow (Exercise 8) that isomorphic Lie groups have isomorphic Lie algebras.
Theorem 3.28. Let G and H be matrix Lie groups, with Lie algebras g and h,
respectively. Suppose that ˆ W G ! H is a Lie group homomorphism. Then there
exists a unique real-linear map  W g ! h such that
ˆ.eX/ D e.X/
(3.11)
for all X 2 g. The map  has following additional properties:
1. 

AXA1
D ˆ.A/.X/ˆ.A/1, for all X 2 g, A 2 G.
2. .ŒX; Y / D Œ.X/; .Y /, for all X; Y 2 g.
3. .X/ D
d
dtˆ.etX/
ˇˇ
tD0, for all X 2 g.
In practice, given a Lie group homomorphism ˆ, the way one goes about
computing  is by using Property 3. In the language of manifolds, Property 3 says
that  is the derivative (or differential) of ˆ at the identity. By Point 2,  W g ! h is
a Lie algebra homomorphism. Thus, every Lie group homomorphism gives rise to a
Lie algebra homomorphism. In Chapter 5, we will investigate the reverse question:
If  is a homomorphism between the Lie algebras of two Lie groups, is there an
associated Lie group homomorphism ˆ?
Proof. The proof is similar to the proof of Theorem 3.20. Since ˆ is a continuous
group homomorphism, ˆ.etX/ will be a one-parameter subgroup of H, for each
X 2 g. Thus, by Theorem 2.14, there is a unique matrix Z such that

3.5
Lie Group and Lie Algebra Homomorphisms
61
ˆ.etX/ D etZ
(3.12)
for all t 2 R. We deﬁne .X/ D Z and check that  has the required properties.
First, by putting t D 1 in (3.12), we see that ˆ.eX/ D e.X/ for all X 2 g. Next, if
ˆ.etX/ D etZ for all t, then ˆ.etsX/ D etsZ, showing that .sX/ D s.X/. Using
the Lie product formula and the continuity of ˆ, we then compute that
et.XCY / D ˆ

lim
m!1

etX=metY=mm
D lim
m!1

ˆ.etX=m/ˆ.etY=m/
m .
Thus,
et.XCY / D lim
m!1

et.X/=met.Y /=mm D et..X/C.Y//.
Differentiating this result at t D 0 shows that .X C Y / D .X/ C .Y /.
We have thus obtained a real-linear map  satisfying (3.11). If there were another
real-linear map 0 with this property, we would have
et.X/ D et0.X/ D ˆ.etX/
for all t 2 R. Differentiating this result at t D 0 shows that .X/ D 0.X/.
We now verify the remaining claimed properties of . For any A 2 G, we have
et.AXA1/ D e.tAXA1/ D ˆ.etAXA1/.
Thus,
et.AXA1/ D ˆ.A/ˆ.etX/ˆ.A/1
D ˆ.A/et.X/ˆ.A/1.
Differentiating this identity at t D 0 gives Point 1.
Meanwhile, for any X and Y in g, we have, as in the proof of Theorem 3.20,
 .ŒX; Y / D 
 d
dtetXYetX
ˇˇˇˇ
tD0

D d
dt

etXYetXˇˇˇˇ
tD0
;
where we have used the fact that a derivative commutes with a linear transformation.
Thus,

62
3
Lie Algebras
 .ŒX; Y / D d
dtˆ.etX/.Y /ˆ.etX/
ˇˇˇˇ
tD0
D d
dtet.X/.Y /et.X/
ˇˇˇˇ
tD0
D Œ.X/; .Y / ,
establishing Point 2. Finally, since ˆ.etX/ D e.tX/ D et.X/, we can compute .X/
as in Point 3.
ut
Example 3.29. Let ˆ W SU.2/ ! SO.3/ be the homomorphism in Proposi-
tion 1.19. Then the associated Lie algebra homomorphism  W su.2/ ! so.3/
satisﬁes
.Ej/ D Fj ;
j D 1; 2; 3;
where fE1; E2; E3g and fF1; F2; F3g are the bases for su.2/ and so.3/, respectively,
given in Example 3.27.
Since  maps a basis for su.2/ to a basis for so.3/, we see that  is a
Lie algebra isomorphism, even though ˆ is not a Lie group isomorphism (since
ker.ˆ/ D fI; Ig).
Proof. If X is in su.2/ and Y is in the space V in (1.14), then
d
dtˆ.etX/Y
ˇˇˇˇ
tD0
D d
dtetXYetX
ˇˇˇˇ
tD0
D ŒX; Y :
Thus, .X/ is the linear map of V Š R3 to itself given by Y 7! ŒX; Y . If, say,
X D E1, then direct computation shows that

E1;

x1
x2 C ix3
x2  ix3
x1

D

x0
1
x0
2 C ix0
3
x0
2  ix0
3
x0
1

;
where .x0
1; x0
2; x0
3/ D .0; x3; x2/. Since
0
@
0
x3
x2
1
A D
0
@
0 0
0
0 0 1
0 1
0
1
A
0
@
x1
x2
x3
1
A ;
(3.13)
we conclude that .E1/ is the 33 matrix appearing on the right-hand side of (3.13),
which is precisely F1. The computation of .E2/ and .E3/ is similar and is left to
the reader.
ut

3.5
Lie Group and Lie Algebra Homomorphisms
63
Proposition 3.30. Suppose that G, H, and K are matrix Lie groups and
ˆ W H ! K and ‰ W G ! H are Lie group homomorphisms. Let ƒ W G ! K be
the composition of ˆ and ‰ and let ,  , and  be the Lie algebra maps associated
to ˆ, ‰, and ƒ, respectively. Then we have
 D  ı  .
Proof. For any X 2 g,
ƒ.etX/ D ˆ.‰.etX// D ˆ.et .X// D et. .X//.
Thus, .X/ D . .X//.
ut
Proposition 3.31. If ˆ W G ! H is a Lie group homomorphism and  W g ! h is
the associated Lie algebra homomorphism, then the kernel of ˆ is a closed, normal
subgroup of G and the Lie algebra of the kernel is given by
Lie.ker.ˆ// D ker./:
Proof. The usual algebraic argument shows that ker.ˆ/ is normal subgroup of G.
Since, also, ˆ is continuous, ker.ˆ/ is closed. If X 2 ker./, then
ˆ.etX/ D et.X/ D I;
for all t 2 R, showing that X is in the Lie algebra of ker.ˆ/. In the other direction,
if etX lies in ker.ˆ/ for all t 2 R, then
et.X/ D ˆ.etX/ D I
for all t. Differentiating this relation with respect to t at t D 0 gives that .X/ D 0,
showing that X 2 ker./.
ut
Deﬁnition 3.32 (The Adjoint Map). Let G be a matrix Lie group, with Lie algebra
g. Then for each A 2 G, deﬁne a linear map AdA W g ! g by the formula
AdA.X/ D AXA1.
Proposition 3.33. Let G be a matrix Lie group, with Lie algebra g. Let GL.g/
denote the group of all invertible linear transformations of g. Then the map A !
AdA is a homomorphism of G into GL.g/. Furthermore, for each A 2 G, AdA
satisﬁes AdA.ŒX; Y / D ŒAdA.X/; AdA.Y / for all X; Y 2 g.
Proof. Easy. Note that Point 1 of Theorem 3.20 guarantees that AdA.X/ is actually
in g for all X 2 g.
ut
Since g is a real vector space with some dimension k, GL.g/ is essentially the
same as GL.kI R/. Thus, we will regard GL.g/ as a matrix Lie group. It is easy to

64
3
Lie Algebras
show that Ad W G ! GL.g/ is continuous, and so is a Lie group homomorphism.
By Theorem 3.28, there is an associated real linear map X ! adX from the Lie
algebra of G to the Lie algebra of GL.g/ (i.e., from g to gl.g/), with the property
that
eadX D AdeX .
Here, gl.g/ is the Lie algebra of GL.g/, namely the space of all linear maps of g to
itself.
Proposition 3.34. Let G be a matrix Lie group, let g be its Lie algebra, and let
Ad W G ! GL.g/ be as in Proposition 3.33. Let ad W g ! gl.g/ be the associated
Lie algebra map. Then for all X; Y 2 g
adX.Y / D ŒX; Y .
(3.14)
The proposition shows that our usage of the notation adX in this section is
consistent with that in Deﬁnition 3.7.
Proof. By Point 3 of Theorem 3.28, ad can be computed as follows:
adX D d
dtAdetX
ˇˇˇˇ
tD0
.
Thus,
adX.Y / D d
dtetXYetX
ˇˇˇˇ
tD0
D ŒX; Y ;
as claimed.
ut
We have proved, as a consequence of Theorem 3.28 and Proposition 3.34, the
following result, which we will make use of later.
Proposition 3.35. For any X in Mn.C/, let adX W Mn.C/ ! Mn.C/ be given by
adXY D ŒX; Y . Then for any Y in Mn.C/, we have
eXYeX D AdeX .Y / D eadX .Y /;
where
eadX .Y / D Y C ŒX; Y  C 1
2ŒX; ŒX; Y  C    :
This result can also be proved by direct calculation—see Exercise 14.

3.6
The Complexiﬁcation of a Real Lie Algebra
65
3.6
The Complexiﬁcation of a Real Lie Algebra
In studying the representations of a matrix Lie group G (as we will do in later
chapters), it is often useful to pass to the Lie algebra g of G, which is, in general,
only a real Lie algebra. It is then often useful to pass to an associated complex Lie
algebra, called the complexiﬁcation of g.
Deﬁnition 3.36. If V
is a ﬁnite-dimensional real vector space, then the
complexiﬁcation of V , denoted VC, is the space of formal linear combinations
v1 C iv2;
with v1; v2 2 V . This becomes a real vector space in the obvious way and becomes
a complex vector space if we deﬁne
i.v1 C iv2/ D v2 C iv1.
We could more pedantically deﬁne VC to be the space of ordered pairs .v1; v2/
with v1; v2 2 V , but this is notationally cumbersome. It is straightforward to verify
that the above deﬁnition really makes VC into a complex vector space. We will
regard V as a real subspace of VC in the obvious way.
Proposition 3.37. Let g be a ﬁnite-dimensional real Lie algebra and gC its
complexiﬁcation. Then the bracket operation on g has a unique extension to gC
that makes gC into a complex Lie algebra. The complex Lie algebra gC is called the
complexiﬁcation of the real Lie algebra g.
Proof. The uniqueness of the extension is obvious, since if the bracket operation on
gC is to be bilinear, then it must be given by
ŒX1 C iX2; Y1 C iY2 D .ŒX1; Y1  ŒX2; Y2/ C i .ŒX1; Y2 C ŒX2; Y1/ .
(3.15)
To show existence, we must now check that (3.15) is really bilinear and skew
symmetric and that it satisﬁes the Jacobi identity. It is clear that (3.15) is real
bilinear, and skew-symmetric. The skew symmetry means that if (3.15) is complex
linear in the ﬁrst factor, it is also complex linear in the second factor. Thus, we need
only show that
Œi.X1 C iX2/; Y1 C iY2 D i ŒX1 C iX2; Y1 C iY2 .
(3.16)
The left-hand side of (3.16) is
ŒX2 C iX1; Y1 C iY2 D . ŒX2; Y1  ŒX1; Y2/ C i .ŒX1; Y1  ŒX2; Y2/ ;

66
3
Lie Algebras
whereas the right-hand side of (3.16) is
i f.ŒX1; Y1  ŒX2; Y2/ C i .ŒX2; Y1 C ŒX1; Y2/g
D . ŒX2; Y1  ŒX1; Y2/ C i .ŒX1; Y1  ŒX2; Y2/ ,
and, indeed, these expressions are equal.
It remains to check the Jacobi identity. Of course, the Jacobi identity holds if
X; Y , and Z are in g. Furthermore, for all X; Y; Z 2 gC, the expression
ŒX; ŒY; Z C ŒY; ŒZ; X C ŒZ; ŒX; Y 
is complex-linear in X with Y and Z ﬁxed. Thus, the Jacobi identity continues to
hold if X is in gC and Y and Z are in g. The same argument then shows that the
Jacobi identity holds when X and Y in gC and Z is in g. Applying this argument
one more time establishes the Jacobi identity for gC in general.
ut
Proposition 3.38. Suppose that g  Mn.C/ is a real Lie algebra and that for all
nonzero X in g, the element iX is not in g. Then the "abstract"complexiﬁcation gC
of g in Deﬁnition 3.36 is isomorphic to the set of matrices in Mn.C/ that can be
expressed in the form X C iY with X and Y in g.
Proof. Consider the map from gC into Mn.C/ sending the formal linear combina-
tion X C iY to the linear combination X C iY of matrices. This map is easily seen
to be a complex Lie algebra homomorphism. If g satisﬁes the assumption in the
statement of the proposition, this map is also injective and thus an isomorphism of
gC with g C ig  Mn.C/.
ut
Using the proposition, we easily obtain the following list of isomorphisms:
gl.nI R/C Š gl.nI C/;
u.n/C
Š gl.nI C/;
su.n/C
Š sl.nI C/;
sl.nI R/C Š sl.nI C/;
so.n/C
Š so.nI C/;
sp.nI R/C Š sp.nI C/;
sp.n/C
Š sp.nI C/.
(3.17)
Let us verify just one example, that of u.n/. If X D X, then .iX/ D iX. Thus, X
and X cannot both be in u.n/ unless X is zero. Furthermore, every X in Mn.C/ can
be expressed as X D X1CiX2, where X1 D .X X/=2 and X2 D .X CX/=.2i/
are both in u.n/. This shows that u.n/C Š gl.nI C/.
Although both su.2/C and sl.2I R/C are isomorphic to sl.2I C/, the Lie algebra
su.2/ is not isomorphic to sl.2I R/. See Exercise 11.

3.7
The Exponential Map
67
Proposition 3.39. Let g be a real Lie algebra, gC its complexiﬁcation, and h an
arbitrary complex Lie algebra. Then every real Lie algebra homomorphism of g
into h extends uniquely to a complex Lie algebra homomorphism of gC into h.
This result is the universal property of the complexiﬁcation of a real Lie
algebra.
Proof. The unique extension is given by .X CiY/ D .X/Ci.Y / for all X; Y 2
g. It is easy to check that this map is, indeed, a homomorphism of complex Lie
algebras.
ut
3.7
The Exponential Map
Deﬁnition 3.40. If G is a matrix Lie group with Lie algebra g, then the exponential
map for G is the map
exp W g ! G.
That is to say, the exponential map for G is the matrix exponential restricted
to the Lie algebra g of G. We have shown (Theorem 2.10) that every matrix in
GL.nI C/ is the exponential of some nn matrix. Nevertheless, if G  GL.nI C/ is
a closed subgroup, there may exist A in G such that there is no X in the Lie algebra
g of G with exp X D A.
Example 3.41. There does not exist a matrix X 2 sl.2I C/ with
eX D
1
1
0 1

;
(3.18)
even though the matrix on the right-hand side of (3.18) is in SL.2I C/.
Proof. If X 2 sl.2I C/ has distinct eigenvalues, then X is diagonalizable and eX
will also be diagonalizable, unlike the matrix on the right-hand side of (3.18). If
X 2 sl.2I C/ has a repeated eigenvalue, this eigenvalue must be 0 or the trace of X
would not be zero. Thus, there is a nonzero vector v with Xv D 0, from which it
follows that eXv D e0v D v. We conclude that eX has 1 as an eigenvalue, unlike
the matrix on the right-hand side of (3.18).
ut
We see, then, that the exponential map for a matrix Lie group G does not
necessarily map g onto G. Furthermore, the exponential map may not be one-to-
one on g, as may be seen, for example, from the case g D su.2/. Nevertheless, it
provides a crucial mechanism for passing information between the group and the
Lie algebra. Indeed, we will see (Corollary 3.44) that the exponential map is locally
one-to-one and onto, a result that will be essential later.

68
3
Lie Algebras
Theorem 3.42. For 0 < " < log 2, let U" D fX 2 Mn.C/ jkXk < "g and let
V" D exp.U"/. Suppose G  GL.nI C/ is a matrix Lie group with Lie algebra g.
Then there exists " 2 .0; log 2/ such that for all A 2 V", A is in G if and only if
log A is in g.
The condition " < log 2 guarantees (Theorem 2.8) that for all X 2 U", log.eX/
is deﬁned and equal to X. Note that if X D log A is in g, then A D eX is in G.
Thus, the content of the theorem is that for some ", having A in V" \ G implies that
log A must be in g. See Figure 3.1.
We begin with a lemma.
Lemma 3.43. Suppose Bm are elements of G and that Bm ! I. Let Ym D log Bm,
which is deﬁned for all sufﬁciently large m. Suppose that Ym is nonzero for all m
and that Ym= kYmk ! Y 2 Mn.C/. Then Y is in g.
Proof. For any t 2 R, we have .t= kYmk/ Ym ! tY. Note that since Bm ! I, we
have kYmk ! 0. Thus, we can ﬁnd integers km such that km kYmk ! t. We have,
then,
ekmYm D exp

.km kYmk/ Ym
kYmk

! etY:
Fig. 3.1 If A 2 V" belongs to G, then log A belongs to g

3.7
The Exponential Map
69
Fig. 3.2 The points kmYm are converging to tY
However,
ekmYm D .eYm/km D .Bm/km 2 G
and G is closed, and we conclude that etY 2 G. This shows that Y 2 g. (See
Figure 3.2.)
ut
Proof of Theorem 3.42. Let us think of Mn.C/ as Cn2 Š R2n2 and let D denote
the orthogonal complement of g with respect to the usual inner product on R2n2.
Consider the map ˆ W Mn.C/ ! Mn.C/ given by
ˆ.Z/ D eXeY ,
where Z D X CY with X 2 g and Y 2 D. Since (Proposition 2.16) the exponential
is continuously differentiable, the map ˆ is also continuously differentiable, and we
may compute that
d
dtˆ.tX; 0/
ˇˇˇˇ
tD0
D X;
d
dtˆ.0; tY/
ˇˇˇˇ
tD0
D Y .

70
3
Lie Algebras
This calculation shows that the derivative of ˆ at the point 0 2 R2n2 is the
identity. (Recall that the derivative at a point of a function from R2n2 to itself is
a linear map of R2n2 to itself.) Since the derivative of ˆ at the origin is invertible,
the inverse function theorem says that ˆ has a continuous local inverse, deﬁned in
a neighborhood of I.
We need to prove that for some ", if A 2 V" \ G, then log A 2 g. If this were not
the case, we could ﬁnd a sequence Am in G such that Am ! I as m ! 1 and such
that for all m, log Am ... g. Using the local inverse of the map ˆ, we can write Am
(for all sufﬁciently large m) as
Am D eXmeYm;
Xm 2 g; Ym 2 D;
with Xm and Ym tending to zero as m tends to inﬁnity. We must have Ym ¤ 0, since
otherwise we would have log Am D Xm 2 g. Since eXm and Am are in G, we see
that
Bm WD eXmAm D eYm
is in G.
Since the unit sphere in D is compact, we can choose a subsequence of the Ym's
(still called Ym) so that Ym= kYmk converges to some Y 2 D, with kY k D 1. Then,
by the lemma, Y 2 g. This is a contradiction, because D is the orthogonal comple-
ment of g. Thus, there must be some " such that log A 2 g for all A in V" \ G.
ut
3.8
Consequences of Theorem 3.42
In this section, we derive several consequences of the main result of the last section,
Theorem 3.42.
Corollary 3.44. If G is a matrix Lie group with Lie algebra g, there exists a
neighborhood U of 0 in g and a neighborhood V of I in G such that the exponential
map takes U homeomorphically onto V .
Proof. Let " be such that Theorem 3.42 holds and set U D U" \ g and V D
V" \ G. The theorem implies that exp takes U onto V . Furthermore, exp is a
homeomorphism of U onto V , since there is a continuous inverse map, namely,
the restriction of the matrix logarithm to V .
ut
Corollary 3.45. Let G be a matrix Lie group with Lie algebra g and let k be the
dimension of g as a real vector space. Then G is a smooth embedded submanifold
of Mn.C/ of dimension k and hence a Lie group.
It follows from the corollary that G is locally path connected: every point in
G has a neighborhood U that is homeomorphic to a ball in Rk and hence path
connected. It then follows that G is connected (in the usual topological sense) if and
only if it is path connected. (See, for example, Proposition 3.4.25 of [Run].)

3.8
Consequences of Theorem 3.42
71
Proof. Let " 2 .0; log 2/ be such that Theorem 3.42 holds. Then for any A0 2 G,
consider the neighborhood A0V" of A0 in Mn.C/. Note that A 2 A0V" if and only
if A1
0 A 2 V". Deﬁne a local coordinate system on A0V" by writing each A 2 A0V"
as A D A0eX, for X 2 U"  Mn.C/. It follows from Theorem 3.42 that (for
A 2 A0V") A 2 G if and only if X 2 g. Thus, in this local coordinate system
deﬁned near A0, the group G looks like the subspace g of Mn.C/. Since we can ﬁnd
such local coordinates near any point A0 in G, we conclude that G is an embedded
submanifold of Mn.C/.
Now, the operation of matrix multiplication is clearly smooth. Furthermore, by
the formula for the inverse of a matrix in terms of cofactors, the map A 7! A1 is
also smooth on GL.nI C/. The restrictions of these maps to G are then also smooth,
showing that G is a Lie group.
ut
Corollary 3.46. Suppose G  GL.nI C/ is a matrix Lie group with Lie algebra g.
Then a matrix X is in g if and only if there exists a smooth curve  in Mn.C/ with
.t/ 2 G for all t and such that .0/ D I and d=dtjtD0 D X. Thus, g is the
tangent space at the identity to G.
This result is illustrated in Figure 3.1.
Proof. If X is in g, then we may take .t/ D etX and then .0/ D I and
d=dtjtD0 D X. In the other direction, suppose that .t/ is a smooth curve in G
with .0/ D I. For all sufﬁciently small t, we can write .t/ D eı.t/, where ı is a
smooth curve in g. Now, the derivative of ı.t/ at t D 0 is the same as the derivative
of t 7! tı0.0/ at t D 0. Thus, by the chain rule, we have
0.0/ D d
dteı.t/
ˇˇˇˇ
tD0
D d
dtetı0.0/
ˇˇˇˇ
tD0
D ı0.0/:
Since ı.t/ belongs to g for all sufﬁciently small t, we conclude (as in the proof of
Theorem 3.20) that ı0.0/ D 0.0/ belongs to g.
ut
Corollary 3.47. If G is a connected matrix Lie group, every element A of G can be
written in the form
A D eX1eX2    eXm
(3.19)
for some X1; X2; : : : ; Xm in g.
Even if G is connected, it is in general not possible to write every A 2 G as
single exponential, A D exp X, with X 2 g. (See Example 3.41.) We begin with a
simple analytic lemma.
Lemma 3.48. Suppose A W Œa; b ! Gl.nI C/ is a continuous map. Then for all
" > 0 there exists ı > 0 such that if s; t 2 Œa; b satisfy js  tj < ı, then
		A.s/A.t/1  I
		 < ":

72
3
Lie Algebras
Proof. We note that
		A.s/A.t/1  I
		 D
		.A.s/  A.t//A.t/1		
 kA.s/  A.t/k
		A.t/1		 :
(3.20)
Since Œa; b is compact and the map t 7!
		A.t/1		 is continuous, there is a constant
C such that
		A.t/1		  C for all t 2 Œa; b. Furthermore, since Œa; b is compact,
Theorem 4.19 in [Rud1] tells us that the map A is actually uniformly continuous on
Œa; b. Thus, for any " > 0, there exists ı > 0 such that when js  tj < ı, we have
kA.s/  A.t/k < "=C. Thus, in light of (3.20), we have the desired ı.
ut
Proof of Corollary 3.47. Let V" be as in Theorem 3.42. For any A 2 G, choose a
continuous path A W Œ0; 1 ! G with A.0/ D I and A.1/ D A. By Lemma 3.48,
we can ﬁnd some ı > 0 such that if js  tj < ı, then A.s/A.t/1 2 V". Now
divide Œ0; 1 into m pieces, where 1=m < ı. Then for j D 1; : : : ; m, we see that
A..j  1/=m/1A.j=m/ belongs to V", so that
A..j  1/=m/1A.j=m/ D eXj
for some elements X1; : : : ; Xm of g: Thus,
A D A.0/1A.1/
D A.0/1A.1=m/A.1=m/1A.2=m/   A..m  1/=m/1A.1/
D eX1eX2    eXm;
as claimed.
ut
Corollary 3.49. Let G and H be matrix Lie groups with Lie algebras g and
h, respectively, and assume G is connected. Suppose ˆ1 and ˆ2 are Lie group
homomorphisms of G into H and that 1 and 2 be the associated Lie algebra
homomorphisms of g into h. Then if 1 D 2, we have ˆ1 D ˆ2.
Proof. Let g be any element of G. Since G is connected, Corollary 3.47 tells us that
every element of G can be written as eX1eX2    eXm, with Xj 2 g. Thus,
ˆ1.eX1    eXm/ D e1.X1/    e1.Xm/
D e2.X1/    e2.Xm/
D ˆ2.eX1    eXm/;
as claimed.
ut
Corollary 3.50. Every continuous homomorphism between two matrix Lie groups
is smooth.

3.9
Exercises
73
Proof. For all g 2 G, we write nearby elements h 2 G as h D geX, with X 2 g, so
that
ˆ.h/ D ˆ.g/ˆ.eX/ D ˆ.g/e.X/:
This relation says that in the exponential coordinates near g, the map ˆ is a
composition of a linear map, the exponential map, and multiplication on the left
by ˆ.g/, all of which are smooth. This shows that ˆ is smooth near g.
ut
Corollary 3.51. If G is a connected matrix Lie group and the Lie algebra g of G is
commutative, then G is commutative.
This result is a partial converse to Proposition 3.22.
Proof. Since g is commutative, any two elements of G, when written as in
Corollary 3.47, will commute.
ut
Corollary 3.52. If G  Mn.C/ is a matrix Lie group, the identity component G0 of
G is a closed subgroup of Gl.nI C/ and thus a matrix Lie group. Furthermore, the
Lie algebra of G0 is the same as the Lie algebra of G.
Proof. Suppose that hAmi is a sequence in G0 converging to some A 2 GL.nI C/.
Then certainly A 2 G, since G is closed. Furthermore, AmA1 lies in G for all
m and AmA1 ! I as m ! 1. If m is large enough, Theorem 3.42 tells us that
AmA1 D eX for some X 2 g, so that A D eXAm. Since Am 2 G0, there is a path
joining I to Am in G. But we can then join Am to eXAm D A by the path etXAm,
0  t  1. By combining these two paths, we can join I to A in G, showing that A
belongs to G0.
Now, since G0  G, the Lie algebra of G0 is contained in the Lie algebra of G.
In the other direction, if etX lies in G for all t, then it actually lies in G0, since any
point et0X on the curve etX can be connected to I in G, using the curve etX itself, for
0  t  t0.
ut
3.9
Exercises
1. (a) If g is a Lie algebra, show that the center of g is an ideal in g.
(b) If g and h are Lie algebras and  W g ! h is a Lie algebra homomorphism,
show that the kernel of  is an ideal in g.
2. Classify up to isomorphism all one-dimensional and two-dimensional real Lie
algebras. There is one isomorphism class of one-dimensional algebras and two
isomorphism classes of two-dimensional algebras.
3. Let g denote the space of n  n upper triangular matrices with zeros on the
diagonal. Show that g is a nilpotent Lie algebra under the bracket given by
ŒX; Y  D XY  YX.
4. Give an example of a matrix Lie group G and a matrix X such that eX 2 G,
but X ... g.

74
3
Lie Algebras
5. If G1  GL.n1I C/ and G2  GL.n2I C/ are matrix Lie groups and G1  G2 is
their direct product (regarded as a subgroup of GL.n1 C n2I C/ in the obvious
way), show that the Lie algebra of G1  G2 is isomorphic to g1 ˚ g2.
6. Let G and H be matrix Lie groups with H  G, so that the Lie algebra h of H
is a subalgebra of the Lie algebra g of G.
(a) Show that if H is a normal subgroup of G, then h is an ideal in g.
(b) Show that if G and H are connected and h is an ideal in g, then H is a
normal subgroup of G.
7. Suppose G  GL.nI C/ is a matrix Lie group with Lie algebra g. Suppose that
A is in G and that kA  Ik < 1, so that the power series for log A is convergent.
Is it necessarily the case that log A is in g? Prove or give a counterexample.
8. Show that two isomorphic matrix Lie groups have isomorphic Lie algebras.
9. Write out explicitly the general form of a 4  4 real matrix in so.3I 1/ (see
Proposition 3.25).
10. Show that there is an invertible linear map  W su.2/ ! R3 such that
.ŒX; Y / D .X/  .Y / for all X; Y 2 su.2/, where  denotes the cross
product (vector product) on R3.
11. Show that su.2/ and sl.2I R/ are not isomorphic Lie algebras, even though
su.2/C Š sl.2I R/C Š sl.2I C/.
12. Show the groups SU.2/ and SO.3/ are not isomorphic, even though the
associated Lie algebras su.2/ and so.3/ are isomorphic.
13. Let G be a matrix Lie group and let g be its Lie algebra. For each A 2 G, show
that AdA is a Lie algebra automorphism of g.
14. Let X and Y be n  n matrices. Show by induction that
.adX/m .Y / D
m
X
kD0
 
m
k
!
XkY.X/mk;
where
.adX/m .Y / D ŒX;    ŒX; ŒX; Y     
„
ƒ‚
...
m
:
Now, show by direct computation that
eadX .Y / D AdeX .Y / D eXYeX.
Assume it is legal to multiply power series term by term. (This result was
obtained indirectly in Proposition 3.35.)
Hint: Use Pascal's triangle.
15. If G is a matrix Lie group, a component of G is the collection of all points that
can be connected to a ﬁxed A 2 G by a continuous path in G. Show that if G
is compact, G has only ﬁnitely many components.

3.9
Exercises
75
Hint: Suppose G had inﬁnitely many components and consider a sequence
Aj with each element of the sequence in a different component. Extract a
convergent subsequence and Bk D Ajk and consider B1
k BkC1.
16. Suppose that G is a connected, commutative matrix Lie group with Lie
algebra g. Show that the exponential map for G maps g onto G.
17. Suppose G is a connected matrix Lie group with Lie algebra g and that A is an
element of G. Show that A belongs to the center of G if and only if AdA.X/ D
X for all X 2 g.
18. Show that the exponential map from the Lie algebra of the Heisenberg group to
the Heisenberg group is one-to-one and onto.
19. Show that the exponential map from u.n/ to U.n/ is onto, but not one-to-one.
Hint: Every unitary matrix has an orthonormal basis of eigenvectors.
20. Suppose X is a 2  2 real matrix with trace zero, and assume X has a nonreal
eigenvalue.
(a) Show that the eigenvalues of X must be of the form ia; ia with a a nonzero
real number.
(b) Show that the corresponding eigenvectors of X can be chosen to be
complex conjugates of each other, say, v and Nv.
(c) Show that there exists an invertible real matrix C such that
X D C

0 a
a 0

C 1:
Hint: Use v and Nv to construct a real basis for R2, and determine the matrix X
in this basis.
21. Suppose A is a 2  2 real matrix with determinant one, and assume A has a
nonreal eigenvalue. Show that there exists a real number  that is not an integer
multiple of  and an invertible real matrix C such that
A D C

cos  sin 
 sin  cos 

C 1:
22. Show that the image of the exponential map for SL.2I R/ consists of precisely
those matrices A 2 SL.2I R/ such that trace .A/ > 2, together with the
matrix I (which has trace 2). To do this, consider the possibilities for the
eigenvalues of a matrix in the Lie algebra sl.2I R/ and in the group SL.2I R/. In
the Lie algebra, show that the eigenvalues are of the form .; / or .i; i/,
with  real. In the group, show that the eigenvalues are of the form .a; 1=a/ or
.a; 1=a/, with a real and positive, or of the form .ei; ei/, with  real. The
case of a repeated eigenvalue (.0; 0/ in the Lie algebra and .1; 1/ or .1; 1/
in the group) will have to be treated separately using the Jordan canonical form
(Sect. A.4).

76
3
Lie Algebras
Hint: You may assume that if a real matrix X has real eigenvalues, then X is
similar over the reals to its Jordan canonical form. Then use the two previous
exercises.

Chapter 4
Basic Representation Theory
4.1
Representations
If V is a ﬁnite-dimensional real or complex vector space, let GL.V / denote the
group of invertible linear transformations of V . If we choose a basis for V , we can
identify GL.V / with GL.nI R/ or GL.nI C/. Any such identiﬁcation gives rise to a
topology on GL.V /, which is easily seen to be independent of the choice of basis.
With this discussion in mind, we think of GL.V / as a matrix Lie group. Similarly,
we let gl.V / D End.V / denote the space of all linear operators from V to itself,
which forms a Lie algebra under the bracket ŒX; Y  D XY  YX.
Deﬁnition 4.1. Let G be a matrix Lie group. A ﬁnite-dimensional complex
representation of G is a Lie group homomorphism
... W G ! GL.V /;
where V is a ﬁnite-dimensional complex vector space (with dim.V /  1). A ﬁnite-
dimensional real representation of G is a Lie group homomorphism ... of G into
GL.V /, where V is a ﬁnite-dimensional real vector space.
If g is a real or complex Lie algebra, then a ﬁnite-dimensional complex
representation of g is a Lie algebra homomorphism  of g into gl.V /, where
V is a ﬁnite-dimensional complex vector space. If g is a real Lie algebra, then a
ﬁnite-dimensional real representation of g is a Lie algebra homomorphism  of
g into gl.V /.
If ... or  is a one-to-one homomorphism, the representation is called faithful.
One should think of a representation as a linear action of a group or Lie algebra
on a vector space (since, say, to every g 2 G, there is associated an operator ....g/,
which acts on the vector space V ). If the homomorphism ... W G ! GL.V / is ﬁxed,
we will occasionally use the alternative notation
© Springer International Publishing Switzerland 2015
B. Hall, Lie Groups, Lie Algebras, and Representations, Graduate
Texts in Mathematics 222, DOI 10.1007/978-3-319-13467-3_4
77

78
4
Basic Representation Theory
g  V
(4.1)
in place ....g/v. We will often use terminology such as "Let ... be a representation
of G acting on the space V ."
If a representation ... is a faithful representation of a matrix Lie group G, then
f....A/ jA 2 G g is a group of matrices that is isomorphic to the original group G.
Thus, ... allows us to represent G as a group of matrices. This is the motivation
for the term "representation." (Of course, we still call ... a representation even if
it is not faithful.) Despite the origin of the term, the goal of representation theory
is not simply to represent a group as a group of matrices. After all, the groups we
study in this book are already matrix groups! Rather, the goal is to determine (up to
isomorphism) all the ways a ﬁxed group can act as a group of matrices.
Linear actions of groups on vector spaces arise naturally in many branches of
both mathematics and physics. A typical example would be a linear differential
equation in three-dimensional space which has rotational symmetry, such as the
equations that describe the energy states of a hydrogen atom in quantum mechanics.
Since the equation is rotationally invariant, the space of solutions is invariant
under rotations and thus constitutes a representation the rotation group SO.3/. The
representation theory of SO.3/ (or of its Lie algebra) is very helpful in narrowing
down what the space of solutions can be. See, for example, Chapter 18 in [Hall].
Deﬁnition 4.2. Let ... be a ﬁnite-dimensional real or complex representation of a
matrix Lie group G, acting on a space V . A subspace W of V is called invariant
if ....A/w 2 W for all w 2 W and all A 2 G. An invariant subspace W is called
nontrivial if W ¤ f0g and W ¤ V . A representation with no nontrivial invariant
subspaces is called irreducible.
The terms invariant, nontrivial, and irreducible are deﬁned analogously for
representations of Lie algebras.
Even if g is a real Lie algebra, we will consider mainly complex representations of
g. It should be emphasized that if we are speaking about complex representations
of a real Lie algebra g acting on a complex subspace V , an invariant subspace W is
required to be a complex subspace of V .
Deﬁnition 4.3. Let G be a matrix Lie group, let ... be a representation of G acting
on the space V , and let † be a representation of G acting on the space W . A linear
map  W V ! W is called an intertwining map of representations if
.....A/v/ D †.A/.v/
for all A 2 G and all v 2 V . The analogous property deﬁnes intertwining maps of
representations of a Lie algebra.
If  is an intertwining map of representations and, in addition,  is invertible,
then  is said to be an isomorphism of representations. If there exists an
isomorphism between V and W , then the representations are said to be isomorphic.

4.1
Representations
79
If we use the "action" notation of (4.1), the deﬁning property of an intertwining
map may be written as
.A  v/ D A  .v/
for all A 2 G and v 2 V . That is to say,  should commute with the action of G.
A typical problem in representation theory is to determine, up to isomorphism, all
of the irreducible representations of a particular group or Lie algebra. In Sect. 4.6,
we will determine all the ﬁnite-dimensional complex irreducible representations of
the Lie algebra sl.2I C/.
After identifying GL.V / with GL.nI R/ or GL.nI C/, Theorem 3.28 has the
following consequence.
Proposition 4.4. Let G be a matrix Lie group with Lie algebra g and let ... be a
(ﬁnite-dimensional real or complex) representation of G, acting on the space V .
Then there is a unique representation  of g acting on the same space such that
....eX/ D e.X/
for all X 2 g. The representation  can be computed as
.X/ D d
dt....etX/
ˇˇˇˇ
tD0
and satisﬁes
.AXA1/ D ....A/.X/....A/1
for all X 2 g and all A 2 G.
Given a matrix Lie group G with Lie algebra g, we may ask whether every
representation  of g comes from a representation ... of G. As it turns out, this is
not true in general, but is true if G is simply connected. See Sect. 4.7 for examples
of this phenomenon and Sect. 5.7 for the general result.
Proposition 4.5.
1. Let G be a connected matrix Lie group with Lie algebra g. Let ... be a
representation of G and  the associated representation of g. Then ... is
irreducible if and only if  is irreducible.
2. Let G be a connected matrix Lie group, let ...1 and ...2 be representations of G,
and let 1 and 2 be the associated Lie algebra representations. Then 1 and 2
are isomorphic if and only if ...1 and ...2 are isomorphic.
Proof. For Point 1, suppose ﬁrst that ... is irreducible. We then want to show that
 is irreducible. So, let W be a subspace of V that is invariant under .X/ for all
X 2 g. We want to show that W is either f0g or V . Now, suppose A is an element

80
4
Basic Representation Theory
of G. Since G is assumed connected, Corollary 3.47 tells us that A can be written
as A D eX1    eXm for some X1; : : : ; Xm in g. Since W is invariant under .Xj /
it will also be invariant under exp..Xj// D I C .Xj / C .Xj/2=2 C    and,
hence, under
....A/ D ....eX1    eXm/ D ....eX1/    ....eXm/
D e.X1/    e.Xm/:
Since ... is irreducible and W is invariant under each ....A/, W must be either f0g
or V . This shows that  is irreducible.
In the other direction, assume that  is irreducible and that W is an invariant
subspace for .... Then W is invariant under ....exp tX/ for all X 2 g and, hence,
under
.X/ D d
dt....etX/
ˇˇˇˇ
tD0
:
Thus, since  is irreducible, W is f0g or V , and we conclude that ... is irreducible.
This establishes Point 1 of the proposition.
Point 2 of the proposition is similar and is left as an exercise to the reader
(Exercise 1).
ut
Proposition 4.6. Let g be a real Lie algebra and gC its complexiﬁcation. Then
every ﬁnite-dimensional complex representation  of g has a unique extension to a
complex-linear representation of gC, also denoted . Furthermore,  is irreducible
as a representation of gC if and only if it is irreducible as a representation of g.
Of course, the extension of  to gC is given by .X C iY/ D .X/ C i.Y / for
all X; Y 2 g.
Proof. The existence and uniqueness of the extension follow from Proposition 3.39.
The claim about irreducibility holds because a complex subspace W of V is
invariant under .X C iY/, with X and Y in g, if and only if it is invariant under
the operators .X/ and .Y /. Thus, the representation of g and its extension to gC
have precisely the same invariant subspaces.
ut
Deﬁnition 4.7. If V is a ﬁnite-dimensional inner product space and G is a matrix
Lie group, a representation ... W G ! GL.V / is unitary if ....A/ is a unitary
operator on V for every A 2 G.
Proposition 4.8. Suppose G is a matrix Lie group with Lie algebra g: Suppose V
is a ﬁnite-dimensional inner product space, ... is a representation of G acting on V ,
and  is the associated representation of g. If ... is unitary, then .X/ is skew self-
adjoint for all X 2 g. Conversely, if G is connected and .X/ is skew self-adjoint
for all X 2 g, then ... is unitary.

4.2
Examples of Representations
81
In a slight abuse of notation, we will say that a representation  of a real Lie
algebra g acting on a ﬁnite-dimensional inner product space is unitary if .X/ is
skew self-adjoint for all X 2 g.
Proof. The proof is similar to the computation of the Lie algebra of the unitary
group U.n/. If ... is unitary, then for all X 2 g we have
.et.X// D ....etX/ D ....etX/1 D et.X/;
t 2 R;
so that et.X/ D et.X/. Differentiating this relation with respect to t at t D 0
gives .X/ D .X/. In the other direction, if .X/ D .X/, then the
above calculation shows that ....etX/ D et.X/ is unitary. If G is connected, then
by Corollary 3.47, every A 2 G is a product of exponentials, showing that ....A/ is
unitary.
ut
4.2
Examples of Representations
A matrix Lie group G is, by deﬁnition, a subset of some GL.nI C/. The inclusion
map of G into GL.nI C/ (i.e., the map ....A/ D A) is a representation of G, called
the standard representation of G. If G happens to be contained in GL.nI R/ 
GL.nI C/, then can also we can think of the standard representation as a real
representation. Thus, for example, the standard representation of SO.3/ is the one in
which SO.3/ acts in the usual way on R3 and the standard representation of SU.2/
is the one in which SU.2/ acts on C2 in the usual way. Similarly, if g  Mn.C/ is a
Lie algebra of matrices, the map .X/ D X is called the standard representation
of g.
Consider the one-dimensional complex vector space C. For any matrix Lie group
G, we can deﬁne the trivial representation, ... W G ! GL.1I C/, by the formula
....A/ D I
for all A 2 G. Of course, this is an irreducible representation, since C has no
nontrivial subspaces, let alone nontrivial invariant subspaces. If g is a Lie algebra,
we can also deﬁne the trivial representation of g,  W g ! gl.1I C/, by
.X/ D 0
for all X 2 g. This is an irreducible representation.
Recall the adjoint map of a group or Lie algebra, described in Deﬁnitions 3.32
and 3.7.
Deﬁnition 4.9. If G is a matrix Lie group with Lie algebra g, the adjoint represen-
tation of G is the map Ad W G ! GL.g/ given by A 7! AdA. Similarly, the adjoint

82
4
Basic Representation Theory
representation of a ﬁnite-dimensional Lie algebra g is the map ad W g ! gl.g/
given by X 7! adX.
If G is a matrix Lie group with Lie algebra g, then by Proposition 3.34, the Lie
algebra representation associated to the adjoint representation of G is the adjoint
representation of g. Note that in the case of SO.3/, the standard representation and
the adjoint representation are both three-dimensional real representations. In fact,
these two representations are isomorphic (Exercise 2).
Example 4.10. Let Vm denote the space of homogeneous polynomials of degree
m in two complex variables. For each U 2 SU.2/, deﬁne a linear transformation
...m.U / on the space Vm by the formula
Œ...m.U /f  .z/ D f .U 1z/;
z 2 C2:
(4.2)
Then ...m is a representation of SU.2/.
Elements of Vm have the form
f .z1; z2/ D a0zm
1 C a1zm1
1
z2 C a2zm2
1
z2
2 C    C amzm
2
(4.3)
with z1; z2 2 C and the aj 's arbitrary complex constants, from which we see that
dim.Vm/ D m C 1. Explicitly, if f is as in (4.3), then
Œ...m.U /f .z1; z2/ D
m
X
kD0
ak.U 1
11 z1 C U 1
12 z2/mk.U 1
21 z1 C U 1
22 z2/k.
By expanding out the right-hand side of this formula, we see that ...m.U /f is again
a homogeneous polynomial of degree m. Thus, ...m.U / actually maps Vm into Vm.
To see that ...m is actually a representation, compute that
...m.U1/Œ...m.U2/f .z/ D Œ...m.U2/f .U 1
1 z/ D f .U 1
2 U 1
1 z/
D ...m.U1U2/f .z/.
The inverse on the right-hand side of (4.2) is necessary in order to make ...m a
representation. We will see in Proposition 4.11 that each ...m is irreducible and
we will see in Sect. 4.6 that every ﬁnite-dimensional irreducible representation of
SU.2/ is isomorphic to one (and only one) of the ...m's. (Of course, no two of the
...m's are isomorphic, since they do not even have the same dimension.)
The associated representation m of su.2/ can be computed as
.m.X/f /.z/ D d
dtf .etXz/
ˇˇˇˇ
tD0
:

4.2
Examples of Representations
83
Now, let z.t/ D .z1.t/; z2.t// be the curve in C2 deﬁned as z.t/ D etXz. By the
chain rule, we have
m.X/f D @f
@z1
dz1
dt
ˇˇˇˇ
tD0
C @f
@z2
dz2
dt
ˇˇˇˇ
tD0
.
Since dz=dtjtD0 D Xz, so we obtain
m.X/f D  @f
@z1
.X11z1 C X12z2/  @f
@z2
.X21z1 C X22z2/.
(4.4)
We may then take the unique complex-linear extension of  to sl.2I C/ Š su.2/C,
as in Proposition 3.39. This extension is given by the same formula, but with X 2
sl.2I C/.
If X, Y , and H are the following basis elements for sl.2I C/:
H D
 1
0
0 1

I
X D
 0 1
0 0

I
Y D
 0 0
1 0

;
then applying formula (4.4) gives
m.H/ D z1
@
@z1
C z2
@
@z2
m.X/ D z2
@
@z1
m.Y / D z1
@
@z2
:
Applying these operators to a basis element zmk
1
zk
2 for Vm gives
m.H/.zmk
1
zk
2/ D .m C 2k/zmk
1
zk
2
m.X/.zmk
1
zk
2/ D .m  k/ zmk1
1
zkC1
2
;
m.Y /.zmk
1
zk
2/ D kzmkC1
1
zk1
2
.
(4.5)
Thus, zmk
1
zk
2 is an eigenvector for m.H/ with eigenvalue m C 2k, while m.X/
and m.Y / have the effect of shifting the exponent k of z2 up or down by one. Note
that since m.X/ increases the value of k, this operator increases the eigenvalue of
m.H/ by 2, whereas m.Y / decreases the eigenvalue of m.H/ by 2.
Fig. 4.1 The black dots
indicate the nonzero terms for
a vector w in the space V6.
Applying 6.X/4 to w gives a
nonzero multiple of z6
2
z1
6
z1
5z2
z1
4z2
2
z1
3z2
3
z1
2z2
4
z1z2
5
z2
6
6 X 4

84
4
Basic Representation Theory
Proposition 4.11. For each m  0, the representation m is irreducible.
Proof. It sufﬁces to show that every nonzero invariant subspace of Vm is equal to
Vm. So, let W be such a space and let w be a nonzero element of W . Then w can be
written in the form
w D a0zm
1 C a1zm1
1
z2 C a2zm2
1
z2
2 C    C amzm
2
with at least one of the ak's being nonzero. Let k0 be the smallest value of k for
which ak ¤ 0 and consider
m.X/mk0w.
(See Figure 4.1.)
Since each application of m.X/ raises the power of z2 by 1, m.X/mk0 will kill
all the terms in w except ak0zmk0
1
zk0
2 term. On the other hand, since m.X/.zmk
1
zk
2/
is zero only if k D m, we see that m.X/mk0w is a nonzero multiple of zm
2 . Since
W is assumed invariant, W must contain this multiple of zm
2 and so also zm
2 itself.
Now, for 0  k  m, it follows from (4.5) that m.Y /kzm
2 is a nonzero multiple
of zk
1zmk
2
. Therefore, W must also contain zk
1zmk
2
for all 0  k  m. Since these
elements form a basis for Vm, we see that W D Vm, as desired.
ut
4.3
New Representations from Old
One way of generating representations is to take some representations one knows
and combine them in some fashion. In this section, we will consider three
standard methods of obtaining new representations from old, namely direct sums
of representations, tensor products of representations, and dual representations.
4.3.1
Direct Sums
Deﬁnition 4.12. Let G be a matrix Lie group and let ...1; ...2; : : : ; ...m be repre-
sentations of G acting on vector spaces V1; V2; : : : ; Vm. Then the direct sum of
...1; ...2; : : : ; ...m is a representation ...1 ˚    ˚ ...m of G acting on the space
V1 ˚    ˚ Vm, deﬁned by
Œ...1 ˚    ˚ ...m.A/ .v1; : : : ; vm/ D ....1.A/v1; : : : ; ...m.A/vm/
for all A 2 G.

4.3
New Representations from Old
85
Similarly, if g is a Lie algebra, and 1; 2; : : : ; m are representations of g acting
on V1; V2; : : : ; Vm, then we deﬁne the direct sum of 1; 2; : : : ; m, acting on V1 ˚
   ˚ Vm by
Œ1 ˚    ˚ m.X/ .v1; : : : ; vm/ D .1.X/v1; : : : ; m.X/vm/
for all X 2 g.
It is straightforward to check that, say, ...1 ˚    ˚ ...m is really a representation
of G.
4.3.2
Tensor Products
Let U and V be ﬁnite-dimensional real or complex vector spaces. We wish to deﬁne
the tensor product of U and V , which will be a new vector space U ˝V "built" out
of U and V . We will discuss the idea of this ﬁrst and then give the precise deﬁnition.
We wish to consider a formal "product" of an element u of U with an element v
of V , denoted u ˝ v. The space U ˝ V is then the space of linear combinations of
such products, that is, the space of elements of the form
a1u1 ˝ v1 C a2u2 ˝ v2 C    C anun ˝ vn.
(4.6)
Of course, if "˝" is to be interpreted as a product, then it should be bilinear:
.u1 C au2/ ˝ v D u1 ˝ v C au2 ˝ v;
u ˝ .v1 C av2/ D u ˝ v1 C au ˝ v2.
We do not assume that the product is commutative. That is to say, even if U D V so
that U ˝ V and V ˝ U are the same space, u ˝ v will not, in general, equal v ˝ u.
Now, if e1; e2; : : : ; en is a basis for U and f1; f2; : : : ; fm is a basis for V , then,
using bilinearity, it is easy to see that any element of the form (4.6) can be written as
a linear combination of the elements ej ˝ fk . In fact, it seems reasonable to expect
that fej ˝ fk j1  j  n; 1  k  mg should be a basis for the space U ˝ V . This,
in fact, turns out to be the case.
Deﬁnition 4.13. If U and V are ﬁnite-dimensional real or complex vector spaces,
then a tensor product of U with V is a vector space W , together with a bilinear
map  W U  V ! W with the following property: If  is any bilinear map of
U  V into a vector space X, there exists a unique linear map Q of W into X such
that the following diagram commutes:

86
4
Basic Representation Theory
U  V
!
W
 &
. Q 
X
.
Note that the bilinear map  from U  V into X turns into the linear map Q of
W into X. This is one of the points of tensor products: Bilinear maps on U V turn
into linear maps on W .
Theorem 4.14. If U and V are any ﬁnite-dimensional real or complex vector
spaces, then a tensor product .W; / exists. Furthermore, .W; / is unique up to
canonical isomorphism. That is, if .W1; 1/ and .W2; 2/ are two tensor products,
then there exists a unique vector space isomorphism ˆ W W1 ! W2 such that the
following diagram commutes:
U  V
1
!
W1
2 &
. ˆ
W2
.
Suppose that .W; / is a tensor product and that e1; e2; : : : ; en is a basis for U
and f1; f2; : : : ; fm is a basis for V . Then
f.ej; fk/ j1  j  n; 1  k  mg
is a basis for W .
In particular, dim .U ˝ V / D .dim U / .dim V /.
Proof. Exercise 7.
ut
Notation 4.15 Since the tensor product of U and V is essentially unique, we will
let U ˝ V denote an arbitrary tensor product space and we will write u ˝ v instead
of .u; v/. In this notation, Theorem 4.14 says that
fej ˝ fk j1  j  n; 1  k  mg
is a basis for U ˝ V , as expected.
The deﬁning property of U ˝ V is called the universal property of tensor
products. To understand the signiﬁcance of this property, suppose we want to deﬁne
a linear map T from U ˝ V into some other space. We could try to deﬁne T using
bases for U and V , but then we would have to worry about whether T depends on
the choice of basis. Instead, we could try to deﬁne T on elements of the form u ˝ v,
with u 2 U and v 2 V . While it follows from Theorem 4.14 that elements of this
form span U ˝V , the decomposition of an element of U ˝V as a linear combination
of elements of the form u ˝ v is far from unique. Thus, if we wish to deﬁne T on
such elements, we have to worry whether T is well deﬁned.

4.3
New Representations from Old
87
This is where the universal property comes in. If  .u; v/ is any bilinear
expression in .u; v/, the universal property says precisely that there is a unique linear
map T (D Q ) such that
T .u ˝ v/ D  .u; v/.
Thus, we can construct a well-deﬁned linear map T on U ˝ V simply by deﬁning
it on elements of the form u ˝ v, provided that our deﬁnition of T .u ˝ v/ is bilinear
in u and v. The following result is an application of this line of reasoning.
Proposition 4.16. Let U and V be ﬁnite-dimensional real or complex vector
spaces. Let A W U ! U and B W V ! V be linear operators. Then there exists a
unique linear operator from U ˝ V to U ˝ V , denoted A ˝ B, such that
.A ˝ B/.u ˝ v/ D .Au/ ˝ .Bv/
for all u 2 U and v 2 V . If A1 and A2 are linear operators on U and B1 and B2
are linear operators on V , then
.A1 ˝ B1/.A2 ˝ B2/ D .A1A2/ ˝ .B1B2/.
(4.7)
Proof. Deﬁne a map  from U  V into U ˝ V by
 .u; v/ D .Au/ ˝ .Bv/.
Since A and B are linear and since ˝ is bilinear,  is a bilinear map of U  V into
U ˝ V . By the universal property, there is an associated linear map Q W U ˝ V !
U ˝ V such that
Q .u ˝ v/ D  .u; v/ D .Au/ ˝ .Bv/.
Thus, Q is the desired map A ˝ B. An elementary calculation shows that the
identity (4.7) holds on elements of the form u ˝ v. Since, by Theorem 4.14, such
elements span U ˝ V , the identity holds in general.
ut
We are now ready to deﬁne tensor products of representations. There are two
different approaches to this, both of which are important. The ﬁrst approach starts
with a representation of a group G acting on a space U and a representation of
another group H acting on a space V and produces a representation of the product
group G  H acting on the space U ˝ V . The second approach starts with two
different representations of the same group G, acting on spaces U and V , and
produces a representation of G acting on U ˝ V . Both of these approaches can
be adapted to apply to Lie algebras.
Deﬁnition 4.17. Let G and H be matrix Lie groups. Let ...1 be a representation of
G acting on a space U and let ...2 be a representation of H acting on a space V .

88
4
Basic Representation Theory
Then the tensor product of ...1 and ...2 is a representation ...1 ˝ ...2 of G  H
acting on U ˝ V deﬁned by
....1 ˝ ...2/.A; B/ D ...1.A/ ˝ ...2.B/
for all A 2 G and B 2 H.
Using Proposition 4.16, it is easy to check that ...1 ˝ ...2 is, in fact, a
representation of G  H.
Now, if G and H are matrix Lie groups (i.e., G is a closed subgroup of GL.nI C/
and H is a closed subgroup of GL.mI C/), then GH can be regarded in an obvious
way as a closed subgroup of GL.n C mI C/. Thus, the direct product of matrix Lie
groups can be regarded as a matrix Lie group. It is easy to check that the Lie algebra
of G H is isomorphic to the direct sum of the Lie algebra of G and the Lie algebra
of H.
Proposition 4.18. Let G and H be matrix Lie groups with Lie algebras g and
h, respectively. Let ...1 and ...2 be representations of G and H, respectively, and
consider the representation ...1 ˝ ...2 of G  H. If 1 ˝ 2 denotes the associated
representation of g ˚ h, then
.1 ˝ 2/.X; Y / D 1.X/ ˝ I C I ˝ 2.Y /
for all X 2 g and Y 2 h.
Proof. Suppose that u.t/ is a smooth curve in U and v.t/ is a smooth curve in V .
Then, by repeating the proof of the product rule for scalar-valued functions (or by
calculating everything in a basis), we have
d
dt.u.t/ ˝ v.t// D du
dt ˝ v.t/ C u.t/ ˝ dv
dt .
This being the case, we compute as follows:
.1 ˝ 2/.X; Y /.u ˝ v/
D d
dt...1.etX/u ˝ ...2.etY/v
ˇˇˇˇ
tD0
D
 d
dt...1.etX/u
ˇˇˇˇ
tD0

˝ v C u ˝
 d
dt...2.etY/v
ˇˇˇˇ
tD0

.
This establishes the claimed form of .1 ˝2/.X; Y / on elements of the form u˝v,
which span U ˝ V .
ut
The proposition motivates the following deﬁnition.

4.3
New Representations from Old
89
Deﬁnition 4.19. Let g and h be Lie algebras and let 1 and 2 be representations of
g and h, acting on spaces U and V . Then the tensor product of 1 and 2, denoted
1 ˝ 2, is a representation of g ˚ h acting on U ˝ V , given by
.1 ˝ 2/.X; Y / D 1.X/ ˝ I C I ˝ 2.Y /
for all X 2 g and Y 2 h.
It is easy to check that this indeed deﬁnes a representation of g ˚ h. Note that if
we deﬁned .1 ˝ 2/.X; Y / D 1.X/ ˝ 2.Y /, this would not be a representation
of g ˚ h, since this is expression is not linear in .X; Y /.
We now deﬁne a variant of the above deﬁnitions in which we take the tensor
product of two representations of the same group G and regard the result as a
representation of G rather than of G  G.
Deﬁnition 4.20. Let G be a matrix Lie group and let ...1 and ...2 be representations
of G, acting on spaces V1 and V2. Then the tensor product representation of G,
acting on V1 ˝ V2, is deﬁned by
....1 ˝ ...2/.A/ D ...1.A/ ˝ ...2.A/
for all A 2 G. Similarly, if 1 and 2 are representations of a Lie algebra g, we
deﬁne a tensor product representation of g on V1 ˝ V2 by
.1 ˝ 2/.X/ D 1.X/ ˝ I C I ˝ 2.X/:
It is easy to check that ...1 ˝ ...2 and 1 ˝ 2 are actually representations of
G and g, respectively. The notation is, unfortunately, ambiguous, since if ...1 and
...2 are representations of the same group G, we can regard ...1 ˝ ...2 either as a
representation of G or as a representation of G  G. We must, therefore, be careful
to specify which way we are thinking about ...1 ˝ ...2.
If ...1 and ...2 are irreducible representations of a group G, then ...1 ˝ ...2 will
typically not be irreducible when viewed as a representation of G. One can, then,
attempt to decompose ...1 ˝ ...2 as a direct sum of irreducible representations. This
process is called the Clebsch-Gordan theory or, in the physics literature, "addition
of angular momentum." See Exercise 12 and Appendix C for more information
about this topic.
4.3.3
Dual Representations
Suppose that  is a representation of a Lie algebra g acting on a ﬁnite-dimensional
vector space V . Let V  denote the dual space of V , that is, the space of linear

90
4
Basic Representation Theory
functionals on V (Sect. A.7). If A is a linear operator on V , let Atr denote the dual
or transpose operator on V  , given by
.Atr/.v/ D .Av/
for  2 V , v 2 V . If v1; : : : ; vn is a basis for V , then there is a naturally associated
"dual basis" 1; : : : ; n with the property that j .vk/ D ıjk. The matrix for Atr
in the dual basis is then simply the transpose (not the conjugate transpose!) of the
matrix of A in the original basis. If A and B are linear operators on V , it is easily
veriﬁed that
.AB/tr D BtrAtr:
(4.8)
Deﬁnition 4.21. Suppose G is a matrix Lie group and ... is a representation of G
acting on a ﬁnite-dimensional vector space V . Then the dual representation ... to
... is the representation of G acting on V  and given by
....g/ D Œ....g1/tr:
(4.9)
If  is a representation of a Lie algebra g acting on a ﬁnite-dimensional vector space
V , then  is the representation of g acting on V  and given by
.X/ D .X/tr:
(4.10)
Using (4.8), it is easy to check that both ... and  are actually representations.
(Here the inverse on the right-hand side of (4.9) and the minus sign on the right-hand
side of (4.10) are essential.) The dual representation is also called contragredient
representation.
Proposition 4.22. If ... is a representation of a matrix Lie group G, then (1) ... is
irreducible if and only if ... is irreducible and (2) ..../ is isomorphic to .... Similar
statements apply to Lie algebra representations.
Proof. See Exercise 6.
ut
4.4
Complete Reducibility
Much of representation theory is concerned with studying irreducible represen-
tations of a group or Lie algebra. In favorable cases, knowing the irreducible
representations leads to a description of all representations.
Deﬁnition 4.23. A ﬁnite-dimensional representation of a group or Lie algebra is
said to be completely reducible if it is isomorphic to a direct sum of a ﬁnite number
of irreducible representations.

4.4
Complete Reducibility
91
Deﬁnition 4.24. A group or Lie algebra is said to have the complete reducibility
property if every ﬁnite-dimensional representation of it is completely reducible.
As it turns out, most groups and Lie algebras do not have the complete reducibil-
ity property. Nevertheless, many interesting example groups and Lie algebra do have
this property, as we will see in this section and Sect. 10.3.
Example 4.25. Let ... W R ! GL.2I C/ be given by
....x/ D
1 x
0 1

:
Then ... is not completely reducible.
Proof. Direct calculation shows that ... is, in fact, a representation of R. If fe1; e2g
is the standard basis for C2, then clearly the span of e1 is an invariant subspace.
We now claim that he1i is the only nontrivial invariant subspace for .... To see this,
suppose V is a nonzero invariant subspace and suppose V contains a vector not in
the span of e1, say, v D ae1 C be2 with b ¤ 0. Then
....1/v  v D be1
also belongs to V . Thus, e1 and e2 D .v  ae1/=b belong to V , showing that
V
D C2. We conclude, then, that C2 does not decompose as a direct sum of
irreducible invariant subspaces.
ut
Proposition 4.26. If V is a completely reducible representation of a group or Lie
algebra, then the following properties hold.
1. For every invariant subspace U of V , there is another invariant subspace W
such that V is the direct sum of U and W .
2. Every invariant subspace of V is completely reducible.
Proof. For Point 1, suppose that V decomposes as
V D U1 ˚ U2 ˚    ˚ Uk;
where the Uj's are irreducible invariant subspaces, and that U is any invariant
subspace of V . If U is all of V , then we can take W D f0g and we are done. If
W ¤ V , there must be some j1 such that Uj1 is not contained in U . Since Uj1 is
irreducible, it follows that the invariant subspace Uj1 \U must be f0g. Suppose now
that U C Uj1 D V . If so, the sum is direct (since Uj1 \ U D f0g) and we are done.
If U C Uj1 ¤ V , there is some j2 such that U C Uj1 does not contain Uj2,
in which case, .U C Uj1/ \ Uj2 D f0g. Proceeding on in the same way, we must
eventually obtain some family j1; j2; : : : ; jl such that U CUj1 C  CUjl D V and
the sum is direct. Then W WD Uj1 C    C Ujl is the desired complement to U .
For Point 2, suppose U is an invariant subspace of V . We ﬁrst establish that U
has the "invariant complement property" in Point 1. Suppose, then, that X is another
invariant subspace of V with X  U . By Point 1, we can ﬁnd invariant subspace Y

92
4
Basic Representation Theory
such that V D X ˚ Y . Let Z D Y \ U , which is then an invariant subspace. We
want to show that U D X ˚ Z. For all u 2 U , we can write u D x C y with x 2 X
and y 2 Y . But since X  U , we have x 2 U and therefore y D u  x 2 U . Thus,
y 2 Z D Y \ U . We have shown, then, that every u 2 U can be written as the sum
of an element of X and an element of Z. Furthermore, X \ Z  X \ Y D f0g, so
actually U is the direct sum of X and Z.
We may now easily show that U is completely reducible. If U is irreducible, we
are done. If not, U has a nontrivial invariant subspace X and thus U decomposes
as U D X ˚ Z for some invariant subspace Z. If X and Z are irreducible, we
are done, and if not, we proceed on in the same way. Since U is ﬁnite dimensional,
this process must eventually terminate with U being decomposed as a direct sum of
irreducibles.
ut
Proposition 4.27. If G is a matrix Lie group and ... is a ﬁnite-dimensional unitary
representation of G, then ... is completely reducible. Similarly, if g is a real Lie
algebra and  is a ﬁnite-dimensional "unitary"representation of g (meaning that
.X/ D .X/ for all X 2 g), then  is completely reducible.
Proof. Let V denote the Hilbert space on which ... acts and let h; i denote the
inner product on V . If W  V be an invariant subspace, let W ? be the orthogonal
complement of W , so that V is the direct sum of W and W ?. We claim that W ? is
also an invariant subspace for ... or .
To see this, note that since ... is unitary, ....A/ D ....A/1 D ....A1/ for all
A 2 G. Then, for any w 2 W and any v 2 W ?, we have
h....A/v; wi D hv; ....A/wi D
˝
v; ....A1/w
˛
D
˝
v; w0˛
D 0:
In the last step, we have used that w0 D ....A1/w is in W , since W is invariant.
This shows that ....A/v is orthogonal to every element of W , as claimed. A similar
argument, with ....A1/ replaced by .X/, shows that the orthogonal complement
of an invariant subspace for  is also invariant.
We have established, then, that for a unitary representation, the orthogonal
complement of an invariant subspace is again invariant. Suppose now that V is not
irreducible. Then we can ﬁnd an invariant subspace W that is neither f0g nor V , and
we decompose V as W ˚ W ?. Then W and W ? are both invariant subspaces and
thus unitary representations of G in their own right. Then W is either irreducible or
it splits as an orthogonal direct sum of invariant subspaces, and similarly for W ?.
We continue this process, and since V is ﬁnite dimensional, it cannot go on forever,
and we eventually arrive at a decomposition of V as a direct sum of irreducible
invariant subspaces.
ut
Theorem 4.28. If G is a compact matrix Lie group, every ﬁnite dimensional
representation of G is completely reducible.
See also Sect. 10.3 for a similar result for semisimple Lie algebras. The argument
below is sometimes called "Weyl's unitarian trick" for the role of unitarity in the

4.4
Complete Reducibility
93
proof. We require a notion of integration over matrix Lie groups that is invariant
under the right action of the group. One way to construct such a right-invariant
integral is to construct a right-invariant measure on G, known as a Haar measure.
It is, however, simpler to introduce the integral by means of a right-invariant
differential form on G. (See Appendix B for a quick introduction to the notion of
differential forms.)
If G  Mn.C/ is a matrix Lie group, then the tangent space to G at the identity
is the Lie algebra g of G (Corollary 3.46). It is then easy to see that the tangent
space TAG at any point A 2 G is the space of vectors of the form XA with X 2
g. If the dimension of g as a real vector space is k, choose a nonzero k-linear,
alternating form ˛ W gk ! R. (Such form exists and is unique up to multiplication
by a constant.) Then we may deﬁne a k-linear, alternating form ˛A W .TAG/k ! R
by setting
˛A.Y1; : : : ; Yk/ D ˛I .Y1A1; : : : ; YkA1/
for all Y1; : : : ; Yk 2 TAG. That is to say, we deﬁne ˛ in an arbitrary nonzero fashion
at the identity, and we then use the right action of G to "transport" ˛ to every other
point in G. The resulting family of functionals is a k-form on G.
Once such an ˛ has been constructed, we can use it to construct an orientation
on G, by decreeing that an ordered basis Y1; : : : ; Yk for TAG is positively oriented
if ˛A.Y1; : : : ; Yk/ > 0. If f W G ! R is a smooth function, we can integrate the
k-form f ˛ over nice domains in G. If G is compact, we may f ˛ integrate over all
of G, leading to a notion of integration, which we denote as
Z
G
f .A/˛.A/:
Since the orientation on G was deﬁned in terms of the k-form ˛ itself, it is not
hard to see that if f .A/ > 0 for all A, then R
G f ˛ > 0. Furthermore, since the form
˛ was constructed using the right action of G, it is easily seen to be invariant under
that action. As a result, the notion of integration of a function over a compact group
G is invariant under the right action of A: For all B 2 G, we have
Z
G
f .AB/˛.A/ D
Z
G
f .A/˛.A/:
Proof of Theorem 4.28. Choose an arbitrary inner product h; i on V , and then
deﬁne a map h; iG W V  V ! C by the formula
hv; wiG D
Z
G
h....A/v; ....A/wi ˛.A/:
It is easy to check that h; iG is an inner product; in particular, the positivity of h; iG
holds because h....A/v; ....A/vi > 0 for all A if v ¤ 0. We now compute that for
each B 2 G, we have

94
4
Basic Representation Theory
h....B/v; ....B/wiG D
Z
G
h....A/....B/v; ....A/....B/wi ˛.A/
D
Z
G
h....AB/v; ....AB/wi ˛.A/
D
Z
G
h....A/v; ....A/wi ˛.A/
D hv; wiG ;
where we have used the right invariance of the integral in the third equality. This
computation shows that for each B 2 G, the operator ....B/ is unitary with respect
to h; iG. Thus, by Proposition 4.27, ... is completely reducible.
ut
Note that compactness of the group G is needed to ensure that the integral
deﬁning h; iG is convergent.
4.5
Schur's Lemma
It is desirable to be able to state Schur's lemma simultaneously for groups and Lie
algebras. In order to do so, we need to indulge in a common abuse of notation.
If, say, ... is a representation of G acting on a space V , we will refer to V as the
representation, without explicit reference to ....
Theorem 4.29 (Schur's Lemma).
1. Let V and W be irreducible real or complex representations of a group or Lie
algebra and let  W V ! W be an intertwining map. Then either  D 0 or  is
an isomorphism.
2. Let V be an irreducible complex representation of a group or Lie algebra and
let  W V ! V be an intertwining map of V with itself. Then  D I, for some
 2 C.
3. Let V and W be irreducible complex representations of a group or Lie algebra
and let 1; 2 W V ! W be nonzero intertwining maps. Then 1 D 2, for
some  2 C.
It is important to note that the last two points in the theorem hold only over C (or
some other algebraically closed ﬁeld) and not over R. See Exercise 8.
Before proving Schur's lemma, we obtain two corollaries of it.
Corollary 4.30. Let ... be an irreducible complex representation of a matrix Lie
group G. If A is in the center of G, then ....A/ D I, for some  2 C. Similarly,
if  is an irreducible complex representation of a Lie algebra g and if X is in the
center of g, then .X/ D I.

4.5
Schur's Lemma
95
Proof. We prove the group case; the proof of the Lie algebra case is similar. If A is
in the center of G, then for all B 2 G,
....A/....B/ D ....AB/ D ....BA/ D ....B/....A/.
However, this says exactly that ....A/ is an intertwining map of the space with itself.
Thus, by Point 2 of Schur's lemma, ....A/ is a multiple of the identity.
ut
Corollary 4.31. An irreducible complex representation of a commutative group or
Lie algebra is one dimensional.
Proof. Again, we prove only the group case. If G is commutative, the center of
G is all of G, so by the previous corollary ....A/ is a multiple of the identity for
each A 2 G. However, this means that every subspace of V is invariant! Thus,
the only way that V can fail to have a nontrivial invariant subspace is if it is one
dimensional.
ut
We now provide the proof of Schur's lemma.
Proof of Theorem 4.29. As usual, we will prove just the group case; the proof of
the Lie algebra case requires only the obvious notational changes. For Point 1, if
v 2 ker./, then
.....A/v/ D †.A/.v/ D 0.
This shows that ker  is an invariant subspace of V . Since V is irreducible, we must
have ker  D 0 or ker  D V . Thus,  is either one-to-one or zero.
Suppose  is one-to-one. Then the image of  is a nonzero subspace of W . On
the other hand, the image of  is invariant, for if w 2 W is of the form .v/ for
some v 2 V , then
†.A/w D †.A/.v/ D .....A/v/.
Since W is irreducible and image.V / is nonzero and invariant, we must have
image.V / D W . Thus,  is either zero or one-to-one and onto.
For Point 2, suppose V is an irreducible complex representation and that  W
V ! V is an intertwining map of V to itself, that is that ....A/ D ....A/ for
all A 2 G. Since we are working over an algebraically closed ﬁeld,  must have
at least one eigenvalue  2 C. If U denotes the corresponding eigenspace for ,
then Proposition A.2 tells us that each ....A/ maps U to itself, meaning that U is an
invariant subspace. Since  is an eigenvalue, U ¤ 0, and so we must have U D V ,
which means that  D I on all of V .
For Point 3, if 2 ¤ 0, then by Point 1, 2 is an isomorphism. Then 1 ı 1
2
is an intertwining map of W with itself. Thus, by Point 2, 1 ı 1
2
D I, whence
1 D 2.
ut

96
4
Basic Representation Theory
4.6
Representations of sl.2I C/
In this section, we will compute (up to isomorphism) all of the ﬁnite-dimensional
irreducible complex representations of the Lie algebra sl.2I C/. This computation is
important for several reasons. First, sl.2I C/ is the complexiﬁcation of su.2/, which
in turn is isomorphic to so.3/, and the representations of so.3/ are of physical
signiﬁcance. Indeed, the computation we will perform in the proof of Theorem 4.32
is found in every standard textbook on quantum mechanics, under the heading
"angular momentum." Second, the representation theory of su.2/ is an illuminating
example of how one uses commutation relations to determine the representations of
a Lie algebra. Third, in determining the representations of a semisimple Lie algebra
g (Chapters 6 and 7), we will make frequent use of the representation theory of
sl.2I C/, applying it to various subalgebras of g that are isomorphic to sl.2I C/.
We use the following basis for sl.2I C/:
X D
0 1
0 0

I
Y D
 0 0
1 0

I
H D
1
0
0 1

;
which have the commutation relations
ŒH; X D
2X;
ŒH; Y  D 2Y;
ŒX; Y  D
H:
(4.11)
If V is a ﬁnite-dimensional complex vector space and A, B, and C are operators on
V satisfying ŒA; B D 2B, ŒA; C D 2C, and ŒB; C D A, then because of the
skew symmetry and bilinearity of brackets, the unique linear map  W sl.2I C/ !
gl.V / satisfying
.H/ D A;
.X/ D B;
.Y / D C
will be a representation of sl.2I C/.
Theorem 4.32. For each integer m  0, there is an irreducible complex represen-
tation of sl.2I C/ with dimension mC1. Any two irreducible complex representations
of sl.2I C/ with the same dimension are isomorphic. If  is an irreducible complex
representation of sl.2I C/ with dimension m C 1, then  is isomorphic to the
representation m described in Sect.4.2.
Our goal is to show that any ﬁnite-dimensional irreducible representation of
sl.2I C/ "looks like" one of the representations m coming from Example 4.10. In
that example, the space Vm is spanned by eigenvectors for m.H/ and the operators
m.X/ and m.Y / act by shifting the eigenvalues up or down in increments of 2.
We now introduce a simple but crucial lemma that allows us to develop a similar
structure in an arbitrary irreducible representation of sl.2I C/.

4.6
Representations of sl.2I C/
97
Lemma 4.33. Let u be an eigenvector of .H/ with eigenvalue ˛ 2 C. Then we
have
.H/.X/u D .˛ C 2/.X/u.
Thus, either .X/u D 0 or .X/u is an eigenvector for .H/ with eigenvalue
˛ C 2. Similarly,
.H/.Y /u D .˛  2/.Y /u;
so that either .Y /u D 0 or .Y /u is an eigenvector for .H/ with eigenvalue
˛  2.
Proof. We know that Œ.H/; .X/ D  .ŒH; X/ D 2.X/. Thus,
.H/.X/u D .X/.H/u C 2.X/u
D .X/ .˛u/ C 2.X/u
D .˛ C 2/.X/u:
The argument with .X/ replaced by .Y / is similar.
ut
Proof of Theorem 4.32. Let  be an irreducible representation of sl.2I C/ acting
on a ﬁnite-dimensional complex vector space V . Our strategy is to diagonalize the
operator .H/. Since we are working over C, the operator .H/ must have at least
one eigenvector. Let u be an eigenvector for .H/ with eigenvalue ˛. Applying
Lemma 4.33 repeatedly, we see that
.H/.X/ku D .˛ C 2k/.X/ku:
Since operator on a ﬁnite-dimensional space can have only ﬁnitely many eigenval-
ues, the .X/ku's cannot all be nonzero. Thus, there is some N  0 such that
.X/N u ¤ 0
but
.X/NC1u D 0.
If we set u0 D .X/N u and  D ˛ C 2N , then,
.H/u0 D u0;
(4.12)
.X/u0 D 0:
(4.13)

98
4
Basic Representation Theory
Let us then deﬁne
uk D .Y /ku0
for k  0. By Lemma 4.33, we have
.H/uk D .  2k/ uk.
(4.14)
Now, it is easily veriﬁed by induction (Exercise 3) that
.X/uk D kŒ  .k  1/uk1
.k  1/:
(4.15)
Furthermore, since .H/ can have only ﬁnitely many eigenvalues, the uk's cannot
all be nonzero. There must, therefore, be a non-negative integer m such that
uk D .Y /ku0 ¤ 0
for all k  m, but
umC1 D .Y /mC1u0 D 0.
If umC1 D 0, then .X/umC1 D 0 and so, by (4.15),
0 D .X/umC1 D .m C 1/.  m/um:
Since um and m C 1 are nonzero, we must have   m D 0. Thus,  must coincide
with the non-negative integer m.
Thus, for every irreducible representation .; V /, there exists an integer m  0
and nonzero vectors u0; : : : ; um such that
.H/uk D .m  2k/uk
.Y /uk D
 ukC1
if k < m
0
if k D m
.X/uk D
 k.m  .k  1//uk1
if k > 0
0
if k D 0 :
(4.16)
The vectors u0; : : : ; um must be linearly independent, since they are eigenvectors
of .H/ with distinct eigenvalues (Proposition A.1). Moreover, the .m C 1/-
dimensional span of u0; : : : ; um is explicitly invariant under .H/, .X/, and .Y /
and, hence, under .Z/ for all Z 2 sl.2I C/. Since  is irreducible, this space must
be all of V .
We have shown that every irreducible representation of sl.2I C/ is of the
form (4.16). Conversely, if we deﬁne .H/, .X/, and .Y / by (4.16) (where
the uk's are basis elements for some .m C 1/-dimensional vector space), it is

4.6
Representations of sl.2I C/
99
not hard to check that operators deﬁned as in (4.16) really do satisfy the sl.2I C/
commutation relations (Exercise 4). Furthermore, the we may prove irreducibility
of this representation in the same way as in the proof of Proposition 4.11.
The preceding analysis shows that every irreducible representation of dimension
m C 1 must have the form in (4.16), which shows that any two such representations
are isomorphic. In particular, the .m C 1/-dimensional representation m described
in Sect. 4.2 must be isomorphic to (4.16).
This completes the proof of Theorem 4.32.
ut
As mentioned earlier in this section, the representation theory of sl.2I C/ plays a
key role in the representation theory of other Lie algebras, such as sl.3I C/, because
these Lie algebras contain subalgebras isomorphic to sl.2I C/. For such applications,
we need a few results about ﬁnite-dimensional representations of sl.2I C/ that are
not necessarily irreducible.
Theorem 4.34. If .; V / is a ﬁnite-dimensional representation of sl.2I C/, not
necessarily irreducible, the following results hold.
1. Every eigenvalue of .H/ is an integer. Furthermore, if v is an eigenvector for
.H/ with eigenvalue  and .X/v D 0, then  is a non-negative integer.
2. The operators .X/ and .Y / are nilpotent.
3. If we deﬁne S W V ! V by
S D e.X/e.Y /e.X/;
then S satisﬁes
S.H/S1 D .H/:
4. If an integer k is an eigenvalue for .H/, so is each of the numbers
 jkj ;  jkj C 2; : : : ; jkj  2; jkj :
Since SU.2/ is simply connected, Theorem 5.6 will tell us that the representa-
tions of sl.2I C/ Š su.2/C are in one-to-correspondence with the representations
of SU.2/. Since SU.2/ is compact, Theorem 4.28 then tells us that every repre-
sentation of sl.2I C/ is completely reducible. If we decompose V as a direct sum
of irreducibles, it is easy to prove the theorem for each summand separately. It is,
however, preferable to give a proof of the theorem that does not rely on Theorem 5.6,
which in turn relies on the Baker-Campbell-Hausdorff formula.
See also Exercise 13 for a different approach to the ﬁrst part of Point 1, and
Exercise 14 for a different approach to Point 3.
Proof. For Point 1, suppose v is an eigenvalue of .H/ with eigenvalue . Then
there is some N  0 such that .X/N v ¤ 0 but .X/NC1v D 0, where .X/N v
is an eigenvector of .H/ with eigenvalue  C 2N . The proof of Theorem 4.32
shows that m WD  C 2N must be a non-negative integer, so that  is an integer. If
.X/v D 0 then we take N D 0 and  D m is non-negative.

100
4
Basic Representation Theory
For Point 2, it follows from the SN decomposition (Sect. A.3) that .H/ has a
basis of generalized eigenvectors, that is, vectors v for which ..H/  I/kv D 0
for some positive integer k. But, using the commutation relation ŒH; X D 2X and
induction on k, we can see that
Œ.H/  . C 2/Ik.X/ D .X/Œ.H/  Ik:
Thus, if v is a generalized eigenvector for .H/ with eigenvalue , then .X/v
is either zero or a generalized eigenvector with eigenvalue  C 2. Applying .X/
repeatedly to a generalized eigenvector for .H/ must eventually give zero, since
.H/ can have only ﬁnitely many generalized eigenvalues. Thus, .X/ is nilpotent.
A similar argument applies to .Y /.
For Point 3, we note that
S.H/S1 D e.X/e.Y /e.X/.H/e.X/e.Y /e.X/:
(4.17)
By Proposition 3.35, we have
e.X/.H/e.X/ D Ade.X/..H// D ead.X/..H//
and similarly for the remaining products in (4.17).
Now, adX.X/ D 0, adX.H/ D 2X and adX.Y / D H, and similarly with 
applied to each Lie algebra element. Thus,
ead.X/..H// D .H/  2.X/:
Meanwhile, adY .X/ D H, adY .H/ D 2Y , and adY .Y / D 0. Thus,
ead.Y /..H/  2.X//
D .H/  2.X/  2.Y /  2.H/ C 1
24.Y /
D .H/  2.X/:
Finally,
ead.X/..H/  2.X// D .H/  2.X/ C 2.X/
D .H/;
which establishes Point 3.
For Point 4, assume ﬁrst that k is non-negative and let v be an eigenvector for
.H/ with eigenvalue k. Then as in Point 1, there is then another eigenvector v0
for .H/ with eigenvalue m WD k C 2N  k and such that .X/v0 D 0. Then
by the proof of Theorem 4.32, we obtain a chain of eigenvectors v0; v1; : : : ; vm for

4.7
Group Versus Lie Algebra Representations
101
.H/ with eigenvalues ranging from m to m in increments of 2. These eigenvalues
include all of the numbers k; k  2; : : : ; k. If k is negative and v is an eigenvector
for .H/ with eigenvalue k, then Sv is an eigenvector for .H/ with eigenvalue
jkj. Hence, by the preceding argument, each of the numbers from jkj to  jkj in
increments of 2 is an eigenvalue.
ut
4.7
Group Versus Lie Algebra Representations
We know from Chapter 3 (Theorem 3.28) that every Lie group homomorphism gives
rise to a Lie algebra homomorphism. In particular, every representation of a matrix
Lie group gives rise to a representation of the associated Lie algebra. In Chapter 5,
we will prove a partial converse to this result: If G is a simply connected matrix
Lie group with Lie algebra g, every representation of g comes from a representation
of G. (See Theorem 5.6). Thus, for a simply connected matrix Lie group G, there
is a natural one-to-one correspondence between the representations of G and the
representations of g.
It is instructive to see how this general theory works out in the case of SU.2/
(which is simply connected) and SO.3/ (which is not). For every irreducible
representation  of su.2/, the complex-linear extension of  to sl.2I C/ must
be isomorphic (Theorem 4.32) to one of the representations m described in
Sect. 4.2. Since those representations are constructed from representations of the
group SU.2/, we can see directly (without appealing to Theorem 5.6) that every
irreducible representation of su.2/ comes from a representation of SU.2/.
Now, by Example 3.27, there is a Lie algebra isomorphism  W su.2/ ! so.3/
such that .Ej/ D Fj , j D 1; 2; 3, where fE1; E2; E3g and fF1; F2; F3g are
the bases listed in the example. Thus, the irreducible representations of so.3/ are
precisely of the form 	m D m ı 1. We wish to determine, for a particular
m, whether or not there is a representation †m of the group SO.3/ such that
†m.exp X/ D exp.	m.X// for all X in so.3/.
Proposition 4.35. Let 	m D m ı 1 be an irreducible complex representations
of the Lie algebra so.3/ (m  0). If m is even, there is a representation †m of the
group SO.3/ such that †m.eX/ D e	m.X/ for all X in so.3/. If m is odd, there is no
such representation of SO.3/.
Note that the condition that m be even is equivalent to the condition that
dim Vm D m C 1 be odd. Thus, it is the odd-dimensional representations of the
Lie algebra so.3/ which come from group representations. In the physics literature,
the representations of su.2/ Š so.3/ are labeled by the parameter l D m=2. In
terms of this notation, a representation of so.3/ comes from a representation of
SO.3/ if and only if l is an integer. The representations with l an integer are called
"integer spin"; the others are called "half-integer spin."
For any m, one could attempt to construct †m by the construction in the proof
of Theorem 5.6. The construction is based on deﬁning †m.A/ along a path joining

102
4
Basic Representation Theory
I to A and then proving that the value of †m.A/ is independent of the choice of path.
The construction of †m along a path goes through without change. Since SO.3/ is
not simply connected, however, two paths in SO.3/ with the same endpoint are
not necessarily homotopic with endpoints ﬁxed and the proof of independence of
the path breaks down. One can join the identity to itself, for example, either by
the constant path or by the path consisting of rotations by angle 2t in the .y; z/-
plane, 0  t  1. If one deﬁnes †m along the constant path, one gets the value
†m.I/ D I. If m is odd, however, and one deﬁnes †m along the path of rotations
in the .y; z/-plane, then one gets the value †m.I/ D I, as the calculations in the
proof of Proposition 4.35 will show. This strongly suggests (and Proposition 4.35
conﬁrms) that when m is odd, there is no way to deﬁne †m as a "single-valued"
representation of SO.3/.
An electron, for example, is a "spin one-half" particle, which means that it
is described in quantum mechanics in a way that involves the two-dimensional
representation 	1 of so.3/. In the physics literature, one ﬁnds statements to the effect
that applying a 360ı rotation to the wave function of the electron gives back the
negative of the original wave function. This statement reﬂects that if one attempts to
construct the nonexistent representation †1 of SO.3/, then when deﬁning †1 along
a path of rotations in the .x; y/-plane, one gets that †1.I/ D I.
Proof. Suppose, ﬁrst, that m is odd and suppose that there a †m existed. Computing
as in Sect. 2.2, we see that
e2F1 D
0
@
1
0
0
0 cos 2  sin 2
0 sin 2
cos 2
1
A D I:
Meanwhile, 	m.F1/ D m.1.F1// D m.E1/, with E1 D iH=2, where, as usual,
H is the diagonal matrix with diagonal entries .1; 1/. We know that there is a
basis u0; u1; : : : ; um for Vm such that uk is an eigenvector for m.H/ with eigenvalue
m  2j. This means that uj is also an eigenvector for 	m.F1/ D im.H/=2, with
eigenvalue i.m  2j/=2. Thus, in the basis
˚
uj

, we have
	m.F1/ D
0
BBB@
i
2m
i
2.m  2/
:::
i
2.m/
1
CCCA :
Since we are assuming m is odd, m  2j is an odd integer. Thus, e2	m.F1/ has
eigenvalues e2i.m2j /=2 D 1 in the basis
˚
uj

, showing that e2	m.F1/ D I.
Thus, on the one hand,
†m.e2F1/ D †m.I/ D I;

4.8
A Nonmatrix Lie Group
103
whereas, on the other hand,
†m.e2F1/ D e2	m.F1/ D I:
Thus, there can be no such group representation †m.
Suppose now that m is even. Recall that the Lie algebra isomorphism  comes
from the surjective group homomorphism ˆ in Proposition 1.19, where ker.ˆ/ D
fI; Ig. Let ...m be the representation of SU.2/ in Example 4.10. Now, e2E1 D
I, and, thus,
...m.I/ D ...m.e2E1/ D em.2E1/:
If, however m is even, then em.2E1/ is diagonal in the basis fujg with eigenvalues
e2i.m2j /=2 D 1, showing that ....I/ D em.2E1/ D I.
Now, for each R 2 SO.3/, there is a unique pair of elements fU; U g such that
ˆ.U / D ˆ.U / D R. Since ...m.I/ D I, we see that ...m.U / D ...m.U /. It
thus makes sense to deﬁne
†m.R/ D ...m.U /:
It is easy to see that †m is a Lie group homomorphism, and, by construction, we
have ...m D †m ı ˆ. Thus, the Lie algebra representation 	m associated to †m
satisﬁes m D 	mı or 	m D ı1, showing that †m is the desired representation
of SO.3/.
ut
4.8
A Nonmatrix Lie Group
In this section, we will show that the Lie group introduced in Sect. 1.5 is not
isomorphic to a matrix Lie group. (The universal cover of SL.2I R/ is another
example of a Lie group that is not a matrix Lie group; see Sect. 5.8.) The group
in question is G D R  R  S1, with the group product deﬁned by
.x1; y1; u1/  .x2; y2; u1/ D .x1 C x2; y1 C y2; eix1y2u1u2/:
Meanwhile, let H be the Heisenberg group and consider the map ˆ W H ! G
given by
ˆ
0
@
1 a b
0 1 c
0 0 1
1
A D .a; c; e2ib/:

104
4
Basic Representation Theory
Direct computation shows that ˆ is a homomorphism. The kernel of ˆ is the
discrete normal subgroup N of H given by
N D
8
<
:
0
@
1 0 n
0 1 0
0 0 1
1
A
ˇˇˇˇˇˇ
n 2 Z
9
=
; :
Now, suppose that † is any ﬁnite-dimensional representation of G. Then we can
deﬁne an associated representation ... of H by ... D † ı ˆ. Clearly, the kernel
of any such representation of H must include the kernel N of ˆ. Now, let Z.H/
denote the center of H, which is easily shown to be
Z.H/ D
8
<
:
0
@
1 0 b
0 1 0
0 0 1
1
A
ˇˇˇˇˇˇ
b 2 R
9
=
; :
Theorem 4.36. Let ... be any ﬁnite-dimensional representation of H. If ker ... 	
N , then ker ... 	 Z.H/.
Once this is established, we will be able to conclude that there are no faithful
ﬁnite-dimensional representations of G. After all, if † is any ﬁnite-dimensional
representation of G, then the kernel of ... D † ı ˆ will contain N and, thus, Z.H/,
by the theorem. Thus, for all b 2 R,
...
0
@
1 0 b
0 1 0
0 0 1
1
A D †.0; 0; e2ib/ D I:
This means that the kernel of † contains all elements of the form .0; 0; u/ and † is
not faithful. Thus, we obtain the following result.
Corollary 4.37. The Lie group G has no faithful ﬁnite-dimensional representa-
tions. In particular, G is not isomorphic to any matrix Lie group.
We now begin the proof of Theorem 4.36.
Lemma 4.38. If X is a nilpotent matrix and etX D I for some nonzero t, then
X D 0.
Proof. Since X is nilpotent, the power series for etX terminates after a ﬁnite number
of terms. Thus, each entry of etX depends polynomially on t; that is, there exist
polynomials pjk.t/ such that .etX/jk D pjk.t/. If etX D I for some nonzero t, then
entX D I for all n 2 Z, showing that pjk.nt/ D ıjk for all n. However, a polynomial
that takes on a certain value inﬁnitely many times must be constant. Thus, actually,
etX D I for all t, from which it follows that X D 0.
ut

4.9
Exercises
105
Proof of Theorem 4.36. Let  be the associated representation of the Lie algebra h
of H. Let fX; Y; Zg be the following basis for h:
X D
0
@
0 1 0
0 0 0
0 0 0
1
A ; Y D
0
@
0 0 0
0 0 1
0 0 0
1
A ; Z D
0
@
0 0 1
0 0 0
0 0 0
1
A :
(4.18)
These satisfy the commutation relations ŒX; Y  D Z and ŒX; Z D ŒY; Z D 0.
We now claim that .Z/ must be nilpotent, or, equivalently, that all of the
eigenvalues of .Z/ are zero. Let  be an eigenvalue for .Z/ and let V be the
associated eigenspace. Then V is certainly invariant under .Z/. Furthermore,
since .X/ and .Y / commute with .Z/, Proposition A.2 tells us that V is
invariant under .X/ and .Y /. Thus, the restriction of .Z/ to V—namely, I—
is the commutator of the restrictions to V of .X/ and .Y /. Since the trace of a
commutator is zero, we have 0 D  dim.V/ and  must be zero.
Now, direct calculation shows that enZ belongs to N for all integers n. Thus, if
... is a representation of H and ker ... 	 N , we have ....enZ/ D I for all n. Since
.Z/ is nilpotent, Lemma 4.38 tells us that .Z/ is zero and thus that ....etZ/ D
et.Z/ D I for all t 2 R. Since every element of Z.H/ is of the form etZ for some
t, we have the desired conclusion.
ut
4.9
Exercises
1. Prove Point 2 of Proposition 4.5.
2. Show that the adjoint representation and the standard representation are
isomorphic representations of the Lie algebra so.3/. Show that the adjoint and
standard representations of the group SO.3/ are isomorphic.
3. Using the commutation relation Œ.X/; .Y / D .H/ and induction on k,
verify the relation (4.15).
4. Deﬁne a vector space with basis u0; u1; : : : ; um. Now, deﬁne operators .H/,
.X/, and .Y / by formula (4.16). Verify by direct computation that the oper-
ators deﬁned by (4.16) satisfy the commutation relations (4.11) for sl.2I C/.
Hint: When dealing with .Y /, treat the case of uk, k < m, separately from the
case of um, and similarly for .X/.
5. Consider the standard representation of the Heisenberg group, acting on C3.
Determine all subspaces of C3 which are invariant under the action of the
Heisenberg group. Is this representation completely reducible?
6. Prove Proposition 4.22.
Hint: There is a one-to-one correspondence between subspaces of V and
subspaces of V  as follows: For any subspace W of V , the annihilator of W is
the subspace of all  in V  such that  is zero on W . See Sect. A.7.
7. Prove Theorem 4.14.

106
4
Basic Representation Theory
Hints: For existence, choose bases
˚
ej

and ffkg for U and V . Then
deﬁne a space W which has as a basis fwjk j1  j  n; 1  k  mg. Deﬁne
.ej; fk/ D wjk and extend by bilinearity. For uniqueness, use the universal
property.
8. Let SO.2/ act on R2 in the obvious way. Show that R2 is an irreducible
real representation under this action, but that Point 2 of Schur's lemma
(Theorem 4.29) fails.
9. Suppose V is a ﬁnite-dimensional representation of a group or Lie algebra and
that W is a nonzero invariant subspace of V . Show that there exists a nonzero
irreducible invariant subspace for V that is contained in W .
10. Suppose that V1 and V2 are nonisomorphic irreducible representations of a
group or Lie algebra, and consider the associated representation V1 ˚ V2.
Regard V1 and V2 as subspaces of V1 ˚ V2 in the obvious way. Following the
outline below, show that V1 and V2 are the only nontrivial invariant subspaces
of V1 ˚ V2.
(a) First assume that U is a nontrivial irreducible invariant subspace. Let P1 W
V1 ˚ V2 ! V1 be the projection onto the ﬁrst factor and let P2 be the
projection onto the second factor. Show that P1 and P2 are intertwining
maps. Show that U D V1 or U D V2.
(b) Using Exercise 9, show that V1 and V2 are the only nontrivial invariant
subspaces of V1 ˚ V2.
11. Suppose that V is an irreducible ﬁnite-dimensional representation of a group or
Lie algebra over C, and consider the associated representation V ˚ V . Show
that every nontrivial invariant subspace U of V ˚ V is isomorphic to V and is
of the form
U D f.1v; 2v/jv 2 V g
for some constants 1 and 2, not both zero.
12. Recall the spaces Vm introduced in Sect. 4.2, viewed as representations of
the Lie algebra sl.2I C/. In particular, consider the space V1 (which has
dimension 2).
(a) Regard V1 ˝ V1 as a representation of sl.2I C/, as in Deﬁnition 4.20. Show
that this representation is not irreducible.
(b) Now, view V1 ˝ V1 as a representation of sl.2I C/ ˚ sl.2I C/, as in
Deﬁnition 4.19. Show that this representation is irreducible.
13. Let ....; V / be a ﬁnite-dimensional representation of SU.2/ with associated rep-
resentation  of su.2/, which extends by complex linearity to sl.2I C/. (Since
SU.2/ is simply connected, Theorem 5.6 will show that every representation
 of sl.2I C/ arise in this way.) If H is the diagonal matrix with diagonal
entries .1; 1/, show that e2iH D I and use this to prove (independently of
Theorem 4.34) that every eigenvalue of .H/ is an integer.

4.9
Exercises
107
14. Let ....; V / be a ﬁnite-dimensional representation of SU.2/ with associated
representation  of su.2/, which extends by complex linearity to sl.2I C/. If
X, Y , and H are the usual basis element for sl.2I C/, compute eXeY eX and
show that
eXeY eXH.eXeY eX/1 D H:
Use this result to give a different proof of Point 3 of Theorem 4.34.

Chapter 5
The Baker-Campbell-Hausdorff Formula
and Its Consequences
5.1
The "Hard" Questions
Consider three elementary results from the preceding chapters of this book: (1)
Every matrix Lie group G has a Lie algebra g. (2) A continuous homomorphism
ˆ between matrix Lie groups G and H gives rise to a Lie algebra homomorphism
 W g ! h. (3) If G and H are matrix Lie groups and H is a subgroup of G,
then the Lie algebra h of H is a subalgebra of the Lie algebra g of G. Each of these
results goes in the "easy" direction, from a group notion to an associated Lie algebra
notion. In this chapter, we attempt to go in the "hard" direction, from the Lie algebra
to the Lie group. We will investigate three questions relating to the preceding three
theorems.
•
Question 1: Is every ﬁnite-dimensional, real Lie algebra the Lie algebra of some
matrix Lie group?
•
Question 2: Let G and H be matrix Lie groups with Lie algebras g and h,
respectively, and let  W g ! h be a Lie algebra homomorphism. Does there
exists a Lie group homomorphism ˆ W G ! H such that ˆ.eX/ D e.X/ for all
X 2 g?
•
Question 3: If G is a matrix Lie group with Lie algebra g and h is a subalgebra
of g, is there a matrix Lie group H  G whose Lie algebra is h?
The answer to Question 1 is yes; see Sect. 5.10. The answer to Question 2 is,
in general, no, but is yes if G is simply connected; see Sect. 5.7. The answer to
Question 3 is no, in general, but is yes if we allow H to be a "connected Lie
subgroup" that is not necessarily closed; see Sect. 5.9.
Our tool for investigating these questions is the Baker-Campbell-Hausdorff
formula, which expresses log.eXeY /, where X and Y are sufﬁciently small n  n
matrices, in Lie-algebraic terms, that is, in terms of iterated commutators involving
X and Y . The formula implies that all information about the product operation on
a matrix Lie group, at least near the identity, is encoded in the Lie algebra. In the
© Springer International Publishing Switzerland 2015
B. Hall, Lie Groups, Lie Algebras, and Representations, Graduate
Texts in Mathematics 222, DOI 10.1007/978-3-319-13467-3_5
109

110
5
The Baker-Campbell-Hausdorff Formula and Its Consequences
case of Questions 2 and 3 in the preceding paragraph, we will give a complete proof
of the theorem that answers the question. In the case of Question 1, we will need to
assume Ado's theorem, which asserts that every ﬁnite-dimensional real Lie algebra
is isomorphic to an algebra of matrices.
5.2
An Illustrative Example
In this section, we prove one of the main theorems alluded to above (the answer
to Question 2), in the case of the Heisenberg group. We introduce the problem in
a general setting and then specialize to the Heisenberg case. Suppose G and H
are matrix Lie groups with Lie algebras g and h, respectively, and suppose  W
g ! h is a Lie algebra homomorphism. We would like to construct a Lie group
homomorphism ˆ W G ! H such that ˆ.eX/ D e.X/ for all X in g. In light of
Theorem 3.42, we can deﬁne a map ˆ from a neighborhood U of the identity in G
into H as follows:
ˆ.A/ D e.logA/;
(5.1)
so that
ˆ.eX/ D e.X/;
(5.2)
at least for sufﬁciently small X.
A key issue is then to show that ˆ, as deﬁned near the identity by (5.1) or (5.2), is
a "local homomorphism." Suppose A D eX and B D eY , where X; Y 2 g are small
that eX, eY , and eXeY are all in the domain of ˆ. To compute ˆ.AB/ D ˆ.eXeY /,
we need to express eXeY in the form eZ, so that ˆ.eXeY / will equal e.Z/. The
Baker-Campbell-Hausdorff formula states that for sufﬁciently small X and Y , we
have
Z D log.eXeY /
D X C Y C 1
2ŒX; Y  C 1
12ŒX; ŒX; Y   1
12ŒY; ŒX; Y  C    ,
(5.3)
where the "   " refers to additional terms involving iterated brackets of X and Y . (A
precise statement of and a proof of the formula will be given in subsequent sections.)
If  is a Lie algebra homomorphism, then


log.eXeY /

D .X/ C .Y / C 1
2Œ.X/; .Y /
C 1
12Œ.X/; Œ.X/; .Y /  1
12Œ.Y /; Œ.X/; .Y / C   
D log.e.X/e.Y //:
(5.4)
It then follows that ˆ.eXeY / D e.X/e.Y / D ˆ.eX/ˆ.eY /.

5.2
An Illustrative Example
111
In the general case, it requires considerable effort to prove the Baker-Campbell-
Hausdorff formula and then to prove that, when G is simply connected, ˆ can
be extended to all of G. In the case of the Heisenberg group (which is simply
connected), the argument can be greatly simpliﬁed.
Theorem 5.1. Suppose X and Y are n  n complex matrices, and that X and Y
commute with their commutator:
ŒX; ŒX; Y  D ŒY; ŒX; Y  D 0.
(5.5)
Then we have
eXeY D eXCY C 1
2 ŒX;Y .
This is the special case of (5.3) in which the series terminates after the ŒX; Y 
term. See Exercise 5 for another special case of the Baker-Campbell-Hausdorff
formula.
Proof. Consider X and Y in Mn.C/ satisfying (5.5). We will prove that
etXetY D exp

tX C tY C t2
2 ŒX; Y 

,
which reduces to the desired result in the case t D 1. Since, by assumption, ŒX; Y 
commutes with X and Y , the above relation is equivalent to
etXetYe t2
2 ŒX;Y  D et.XCY /.
(5.6)
Let us denote by A.t/ the left-hand side of (5.6) and by B.t/ the right-hand side. Our
strategy will be to show that A .t/ and B .t/ satisfy the same differential equation,
with the same initial conditions. We can see immediately that
dB
dt D B.t/.X C Y /:
On the other hand, differentiating A.t/ by means of the product rule gives
dA
dt D etXXetYe t2
2 ŒX;Y  C etXetYYe t2
2 ŒX;Y 
C etXetYe t2
2 ŒX;Y .t ŒX; Y /.
(5.7)
(The correctness of the last term may be veriﬁed by differentiating et2ŒX;Y =2 term
by term.) Now, since Y commutes with ŒX; Y , it also commute with et2ŒX;Y =2.
Thus, the second term on the right in (5.7) can be rewritten as
etXetYe t2
2 ŒX;Y Y .

112
5
The Baker-Campbell-Hausdorff Formula and Its Consequences
For the ﬁrst term on the right-hand side of (5.7), we compute, using Proposition 3.35,
that
XetY D etYetYXetY
D etYAdetY .X/
D etYetadY .X/:
Since ŒY; ŒY; X D  ŒY; ŒX; Y  D 0, we have
etadY .X/ D X  t ŒY; X D X C t ŒX; Y  ;
with all higher terms being zero. We may then simplify (5.7) to
dA
dt D etXetYe t2
2 ŒX;Y .X C Y / D A.t/.X C Y /.
We see, then, that A.t/ and B.t/ satisfy the same differential equation, with the
same initial condition A.0/ D B.0/ D I. Thus, by standard uniqueness results for
(linear) ordinary differential equations, A.t/ D B.t/ for all t.
ut
Theorem 5.2. Let H denote the Heisenberg group and h its Lie algebra. Let G
be a matrix Lie group with Lie algebra g and let  W h ! g be a Lie algebra
homomorphism. Then there exists a unique Lie group homomorphism ˆ W H ! G
such that
ˆ.eX/ D e.X/
for all X 2 h.
Proof. Recall (Exercise 18 in Chapter 3) that the Heisenberg group has the special
property that its exponential map is one-to-one and onto. Let "log" denote the
inverse of this map and deﬁne ˆ W H ! G by the formula
ˆ .A/ D e.log A/.
We will show that ˆ is a group homomorphism.
If X and Y are in the Lie algebra of the Heisenberg group (3  3 strictly upper
triangular matrices), direct computation shows that every entry of ŒX; Y  is zero
except possibly for the entry in the upper right corner. It is then easily seen that
ŒX; Y  commutes with both X and Y . Since  is a Lie algebra homomorphism,
 .X/ and  .Y / will also commute with their commutator. Thus, by Theorem 5.1,
for any X and Y in the Lie algebra of the Heisenberg group, we have

5.3
The Baker-Campbell-Hausdorff Formula
113
ˆ

eXeY 
D ˆ

eXCY C 1
2 ŒX;Y 
D e.X/C.Y/C 1
2 Œ.X/;.Y /
D e.X/e.Y /
D ˆ.eX/ˆ.eY /:
Thus, ˆ is a group homomorphism, which is continuous because each of exp, log,
and  is continuous.
ut
5.3
The Baker-Campbell-Hausdorff Formula
The goal of the Baker-Campbell-Hausdorff formula (BCH formula) is to compute
log.eXeY /. One may well ask, "Why do we not simply expand both exponentials
and the logarithm in power series and multiply everything out?" While it is possible
to do this, what is not clear is why the answer is expressible in terms of commutators.
Consider, for example, the quadratic term in the expression for log.eXeY /, which
will be a linear combination of X2, Y 2, XY, and YX. For this term to be expressible
in terms of commutators, it must be a multiple of XY  YX. Although direct
computation veriﬁes that this is, indeed, the case, it is far from obvious how to
prove that a similar result occurs for all the higher terms.
We will actually state and prove an integral form of the BCH formula, rather
than the series form (5.3). The integral version of the formula, along with the
argument we present in Sect. 5.5, is actually due to Poincaré. (See [Poin1, Poin2]
and Section 1.1.2.2 of [BF].)
Consider the function
g.z/ D log z
1  1
z
;
which is deﬁned and holomorphic in the disk fjz  1j < 1g. Thus, g.z/ can be
expressed as a series
g.z/ D
1
X
mD0
am.z  1/m,
with radius of convergence one. If V is a ﬁnite-dimensional vector space, we may
identify V with Cn by means of an arbitrary basis, so that the Hilbert-Schmidt norm
(Deﬁnition 2.2) of a linear operator on V can be deﬁned. For any operator A on V
with kA  Ik < 1, we can deﬁne

114
5
The Baker-Campbell-Hausdorff Formula and Its Consequences
g.A/ D
1
X
mD0
am.A  I/m.
We are now ready to state the integral form of the BCH formula.
Theorem 5.3 (Baker-Campbell-Hausdorff). For all n  n complex matrices X
and Y with kXk and kY k sufﬁciently small, we have
log.eXeY / D X C
Z 1
0
g.eadX etadY /.Y / dt.
(5.8)
The proof of this theorem is given in Sect. 5.5 of this chapter. Note that eadX etadY
and, hence, also g.eadX etadY / are linear operators on the space Mn.C/ of all n  n
complex matrices. In (5.8), this operator is being applied to the matrix Y . The fact
that X and Y are assumed small guarantees that eadX etadY is close to the identity
operator on Mn.C/ for 0  t  1, so that g.eadX etadY / is well deﬁned. Although
the right-hand side of (5.8) is rather complicated to compute explicitly, we are not
so much interested in the details of the formula but in the fact that it expresses
log.eXeY / (and hence eXeY ) in terms of the Lie-algebraic quantities adX and adY .
5.4
The Derivative of the Exponential Map
In this section we prove a result that is useful in its own right and will play a key
role in our proof of the BCH formula. Consider the directional derivative of exp at
a point X in the direction of Y :
d
dteXCtY
ˇˇˇˇ
tD0
.
(5.9)
Unless X and Y commute, this derivative may not equal eXCtYY . Nevertheless,
since exp is continuously differentiable (Proposition 2.16), the directional deriva-
tives in (5.9) will depend linearly on Y with X ﬁxed. Now, the function .1  ez/=z
is an entire function of z (with a removable singularity at the origin) and is given by
the power series
1  ez
z
D
1
X
kD0
.1/k
zk
.k C 1/Š;
which has inﬁnite radius of convergence. It thus makes sense to replace z by an
arbitrary linear operator A on a ﬁnite-dimensional vector space.

5.4
The Derivative of the Exponential Map
115
Theorem 5.4 (Derivative of Exponential). For all X; Y 2 Mn.C/, we have
d
dteXCtY
ˇˇˇˇ
tD0
D eX
 I  eadX
adX
.Y /

D eX

Y  ŒX; Y 
2Š
C ŒX; ŒX; Y 
3Š
   

.
(5.10)
More generally, if X .t/ is a smooth matrix-valued function, then
d
dteX.t/ D eX.t/
 I  eadX.t/
adX.t/
dX
dt

.
(5.11)
Our proof follows [Tuy].
Lemma 5.5. If Z is a linear operator on a ﬁnite-dimensional vector space, then
lim
m!1
1
m
m1
X
kD0
.eZ=m/k D 1  eZ
Z
:
(5.12)
Proof. If we formally applied the formula for the sum of a ﬁnite geometric series to
eZ=m; we would get
1
m
m1
X
kD0
.eZ=m/k D 1
m
1  eZ
1  eZ=m
m ! 1
!
1  eZ
Z
:
To give a rigorous argument, we observe that
1  ex
x
D
Z 1
0
etx dt;
from which it follows that
1  eZ
Z
D
Z 1
0
etZ dt:
(5.13)
(The reader may check, using term-by-term integration of the series expansion of
etZ, that this formula for .1  eZ/=Z agrees with our earlier deﬁnition.)
Since .eZ=m/k D ekZ=m; the left-hand side of (5.12) is a Riemann-sum
approximation to the matrix-valued integral on the right-hand side of (5.13). These
Riemann sums converge to the integral of etZ—which is a continuous function of
t—establishing (5.12).
ut

116
5
The Baker-Campbell-Hausdorff Formula and Its Consequences
Proof of Theorem 5.4. The formula (5.11) follows from (5.10) by applying the
chain rule to the composition of exp and X.t/. Thus, it sufﬁces to prove (5.10).
For any n  n matrices X and Y , set

.X; Y / D d
dteXCtY
ˇˇˇˇ
tD0
:
Since (Proposition 2.16) exp is a continuously differentiable map, 
.X; Y / is jointly
continuous in X and Y and is linear in Y for each ﬁxed X.
Now, for every positive integer m, we have
eXCtY D

exp
X
m C t Y
m
m
:
(5.14)
Applying the product rule, we will get m terms, where in each term, m  1 of
the factors in (5.14) are simply evaluated at t D 0 and the remaining factor is
differentiated at t D 0. Thus,
d
dteXCtY
ˇˇˇˇ
tD0
D
m1
X
kD0
.eX=m/mk1
 d
dt exp
X
m C t Y
m
ˇˇˇˇ
tD0

.eX=m/k
D e.m1/X=m
m1
X
kD0
.eX=m/k
X
m ; Y
m

.eX=m/k
D e.m1/X=m 1
m
m1
X
kD0
exp

adX
m
k 

X
m ; Y

:
(5.15)
In the third equality, we have used the linearity of 
.X; Y / in Y and the relationship
between Ad and ad (Proposition 3.35).
We now wish to let m tend to inﬁnity in (5.15). The factor in front tends
to exp.X/. Since 
.X; Y / is jointly continuous in X and Y , the expression

.X=m; Y / tends to 
.0; Y /, where it is easily veriﬁed that 
.0; Y / D Y . Finally,
applying Lemma 5.5 with Z D adX, we see that
lim
m!1
1
m
m1
X
kD0
exp

adX
m
k
D 1  eadX
adX
:
Thus, by letting m tend to inﬁnity in (5.15), we obtain the desired result.
ut

5.5
Proof of the BCH Formula
117
5.5
Proof of the BCH Formula
We now turn to the proof of Theorem 5.3. For sufﬁciently small X and Y in Mn.C/,
let
Z.t/ D log.eXetY/
for 0  t  1. Our goal is to compute Z.1/. Since eZ.t/ D eXetY, we have
eZ.t/ d
dteZ.t/ D

eXetY1 eXetYY D Y .
On the other hand, by Theorem 5.4,
eZ.t/ d
dteZ.t/ D
 I  eadZ.t/
adZ.t/
 dZ
dt

.
Hence,
 I  eadZ.t/
adZ.t/
 dZ
dt

D Y .
Now, if X and Y are small enough, Z.t/ will also be small, so that ŒI 
eadZ.t/=adZ.t/ will be close to the identity and thus invertible. In that case, we
obtain
dZ
dt D
 I  eadZ.t/
adZ.t/
 1
.Y /.
(5.16)
Meanwhile, if we apply the homomorphism "Ad" to the equation eZ.t/ D eXetY,
use the relationship between "Ad" and "ad," and take a logarithm, we obtain the
following relations:
AdeZ.t/ D AdeX AdetY
eadZ.t/ D eadX etadY
adZ.t/ D log.eadX etadY /:
Plugging the last two of these relations into (5.16) gives
dZ
dt D
 I  .eadX etadY /1
log.eadX etadY /
 1
.Y /.
(5.17)

118
5
The Baker-Campbell-Hausdorff Formula and Its Consequences
Now, observe that
g.z/ D
 1  z1
log z
 1
so that (5.17) is the same as
dZ
dt D g.eadX etadY /.Y /.
(5.18)
Noting that Z.0/ D X and integrating (5.18) gives
log.eXeY / D Z.1/ D X C
Z 1
0
g.eadX etadY /.Y / dt;
which is the Baker-Campbell-Hausdorff formula.
5.6
The Series Form of the BCH Formula
Let us see how to get the ﬁrst few terms of the series form of the BCH formula from
the integral form in Theorem 5.3. Using the Taylor series (2.7) for the logarithm, we
may easily compute that
g .z/ D 1 C 1
2 .z  1/  1
6 .z  1/2 C 1
12.z  1/3     :
Meanwhile,
eadX et adY  I
D

I C adX C .adX/2
2
C   
 
I C t adY C t2.adY /2
2
C   

 I
D adX C t adY C t adX adY C .adX/2
2
C t2.adY /2
2
C    .
Since eadX et adY  I has no zeroth-order term, .eadX etadY  I/m will contribute
only terms of degree m or higher in adX and/or adY . Computing up to degree 2 in
adX and adY gives
g.eadX et adY /
D I C 1
2

adX C t adY C t adX adY C .adX/2
2
C t2.adY /2
2


5.7
Group Versus Lie Algebra Homomorphisms
119
 1
6

.adX/2 C t2.adY /2 C t adX adY C t adY adX

C higher-order terms.
We now apply g

eadX etadY 
to Y and integrate. Computing to second order and
noting that any term with adY acting ﬁrst is zero, we obtain:
log.eXeY /

 X C
Z 1
0

Y C 1
2 ŒX; Y  C 1
4 ŒX; ŒX; Y   1
6 ŒX; ŒX; Y   t
6 ŒY; ŒX; Y 

dt

 X C Y C 1
2 ŒX; Y  C 1
12 ŒX; ŒX; Y   1
12 ŒY; ŒX; Y  ;
which is the expression in (5.3).
5.7
Group Versus Lie Algebra Homomorphisms
Recall Theorem 3.28, which says that given matrix Lie groups G and H and a Lie
group homomorphism ˆ W G ! H, we can ﬁnd a Lie algebra homomorphism
 W g ! h such that ˆ.eX/ D e.X/ for all X 2 g. In this section, we prove a
converse to this result in the case that G is simply connected.
Theorem 5.6. Let G and H be matrix Lie groups with Lie algebras g and h;
respectively, and let  W g ! h be a Lie algebra homomorphism. If G is simply
connected, there exists a unique Lie group homomorphism ˆ W G ! H such that
ˆ.eX/ D e.X/ for all X 2 g.
This result has the following corollary.
Corollary 5.7. Suppose G and H are simply connected matrix Lie groups with Lie
algebras g and h, respectively. If g is isomorphic to h; then G is isomorphic to H.
Proof. Let  W g ! h be a Lie algebra isomorphism. By Theorem 5.6, there exists
an associated Lie group homomorphism ˆ W G ! H. Since  WD 1 is also
a Lie algebra homomorphism, there is a corresponding Lie group homomorphism
‰ W H ! G. We want to show that ˆ and ‰ are inverses of each other.
Now, the Lie algebra homomorphism associated to ˆı‰ is, by Proposition 3.30,
equal to  ı  D I, and similarly for ‰ ı ˆ. Thus, by Corollary 3.49, both ˆ ı ‰
and ‰ ı ˆ are equal to the identity maps on H and G, respectively.
ut
We now proceed with the proof of Theorem 5.6. The ﬁrst step is to construct a
"local homomorphism" from . This step is the only place in the argument in which
we use the BCH formula.

120
5
The Baker-Campbell-Hausdorff Formula and Its Consequences
Deﬁnition 5.8. If G and H are matrix Lie groups, a local homomorphism of G
to H is pair .U; f / where U is a path-connected neighborhood of the identity in G
and f W U ! H is a continuous map such that f .AB/ D f .A/f .B/ whenever A,
B, and AB all belong to U .
The deﬁnition says that f is as much of a homomorphism as it makes sense to
be, given that U is not necessarily a subgroup of G.
Proposition 5.9. Let G and H be matrix Lie groups with Lie algebras g and h,
respectively, and let  W g ! h be a Lie algebra homomorphism. Deﬁne U"  G by
U" D fA 2 Gj kA  Ik < 1 and klog Ak < "g:
Then there exists some " > 0 such that the map f W U" ! H given by
f .A/ D e.log A/
is a local homomorphism.
Note that by Theorem 3.42, if " is small enough, log A will be in g for all A 2 U",
so that ˆ makes sense.
Proof. Choose " small enough that Theorem 3.42 applies and small enough that for
all A; B 2 U", the BCH formula applies to X WD log A and Y WD log B and also to
.X/ and .Y /. Then if AB happens to be in U", we have
f .AB/ D f .eXeY / D e.log.eX eY //:
We now compute log.eXeY / by the BCH formula and then apply . Since  is a Lie
algebra homomorphism, it will change all the Lie-algebraic quantities involving X
and Y in the BCH formula into the analogous quantities involving .X/ and .Y /.
Thus, as in (5.4), we have
Œlog.eXeY / D .X/ C
Z 1
0
1
X
mD0
am.ead.X/etad.Y /  I/m..Y // dt
D log.e.X/e.Y //;
We obtain, then,
f .AB/ D exp
˚
log.e.X/e.Y //

D e.X/e.Y /
D f .A/f .B/;
as claimed.
ut

5.7
Group Versus Lie Algebra Homomorphisms
121
Theorem 5.10. Let G and H be matrix Lie groups, with G simply connected. If
.U; f / is a local homomorphism of G into H, there exists a unique (global) Lie
group homomorphism ˆ W G ! H such that ˆ agrees with f on U .
Proof.
Step 1: Deﬁne ˆ along a path. Since G is simply connected and thus
connected, for any A 2 G, there exists a path A.t/ 2 G with A.0/ D I and
A.1/ D A. Let us call a partition 0 D t0 < t1 < t2    < tm D 1 of Œ0; 1 a good
partition if for all s and t belonging to the same subinterval of the partition, we
have
A.t/A.s/1 2 U:
(5.19)
Lemma 3.48 guarantees that good partitions exist. If a partition is good, then,
in particular, since t0 D 0 and A.0/ D I, we have A.t1/ 2 U . Choose a good
partition and write A as
A D ŒA.1/A.tm1/1ŒA.tm1/A.tm2/1    ŒA.t2/A.t1/1A.t1/:
Since ˆ is supposed to be a homomorphism and is supposed to agree with f
near the identity it is reasonable to "deﬁne" ˆ.A/ by
ˆ.A/ D f .A.1/A.tm1/1/   f .A.t2/A.t1/1/f .A.t1//:
(5.20)
In the next two steps, we will prove that ˆ.A/ is independent of the choice of
partition for a ﬁxed path and independent of the choice of path.
Step 2: Prove independence of the partition. For any good partition, if we insert
an extra partition point s between tj and tj C1, the result is easily seen to be
another good partition. This change in the partition has the effect of replacing the
factor f .A.tj C1/A.tj/1/ in (5.20) by
f .A.tj C1/A.s/1/f .A.s/A.tj /1/:
Since s is between tj and tj C1, the condition (5.19) on the original partition
guarantees that A.tj C1/A.s/1, A.s/A.tj/1 and A.tj C1/A.tj/1 are all in U .
Thus, since f is a local homomorphism, we have
f .A.tj C1/A.tj/1/ D f .A.tj C1/A.s/1/f .A.s/A.tj /1/;
showing that the value of ˆ.A/ is unchanged by the addition of the extra partition
point.
By repeating this argument, we see that the value of ˆ.A/ does not change by
the addition of any ﬁnite number of points to the partition. Now, any two good
partitions have a common reﬁnement, namely their union, which is also a good
partition. The above argument shows that the value of ˆ.A/ computed from the
ﬁrst partition is the same as for the common reﬁnement, which is the same as for
the second partition.

122
5
The Baker-Campbell-Hausdorff Formula and Its Consequences
Step 3: Prove independence of the path. It is in this step that we use the
simple connectedness of G. Suppose A0.t/ and A1.t/ are two paths joining
the identity to some A 2 G. Then, since G is simply connected, a standard
topological argument (e.g., Proposition 1.6 in [Hat]) shows that A0 and A1 are
homotopic with endpoints ﬁxed. This means that there exists a continuous map
A W Œ0; 1  Œ0; 1 ! G with
A.0; t/ D A0.t/;
A.1; t/ D A1.t/
for all t 2 Œ0; 1 and also
A.s; 0/ D I;
A.s; 1/ D A
for all s 2 Œ0; 1.
As in the proof of Lemma 3.48, there exists an integer N such that for all .s; t/
and .s0; t0/ in Œ0; 1  Œ0; 1 with js  s0j < 2=N and jt  t0j < 2=N , we have
A.s; t/A.s0; t0/1 2 U:
We now deform A0 "a little bit at a time" into A1. This means that we deﬁne a
sequence Bk;l of paths, with k D 0; : : : ; N  1 and l D 0; : : : ; N . We deﬁne
these paths so that Bk;l.t/ coincides with A..k C 1/=N; t/ for t between 0 and
.l  1/=N , and Bk;l.t/ coincides with A.k=N; t/ for t between l=N and 1. For
t between .l  1/=N and l=N , we deﬁne Bk;l.t/ to coincide with the values
of A.; / on the path that goes "diagonally" in the .s; t/-plane, as indicated in
Figure 5.1. When l D 0, there are no t-values between 0 and .l  1/=N , so
Bk;0.t/ D A.k=N; t/ for all t 2 Œ0; 1. In particular, B0;0.t/ D A0.t/.
Fig. 5.1 The path in the
.s; t/ plane deﬁning Bk;l.t/
1
k
N
k 1
N
s
l 1
N
l
N
1
t

5.7
Group Versus Lie Algebra Homomorphisms
123
We now deform A0 D B0;0 into B0;1 and then into B0;2, B0;3, and so on until we
reach B0;N , which we then deform into B1;0 and so on until we reach BN1;N ,
which we ﬁnally deform into A1. We claim that the value of ˆ.A/ is the same at
each stage of this deformation. Note that for k < l, Bk;l.t/ and Bk;lC1.t/ are the
same except for t's in the interval
Œ.l  1/=N; .l C 1/=N :
Furthermore, by Step 2, we are free to choose any good partition we like to
compute ˆ.A/. For both Bk;l and Bk;lC1, we choose the partition points to be
0; 1
N ; : : : ; l  1
N
; l C 1
N
; l C 2
N
; : : : ; 1;
which gives a good partition by the way N was chosen.
Now, from (5.20), the value of ˆ.A/ depends only on the values of the path at the
partition points. Since we have chosen our partition in such a way that the values
of Bk;l and Bk;lC1 are identical at all the partition points, the value of ˆ.A/ is
the same for these two paths. (See Figure 5.2.) A similar argument shows that
the value of ˆ.A/ computed along Bk;N is the same as along BkC1;0. Thus, the
value of ˆ.A/ is the same for each path from A0 D B0;0 all the way to BN1;N
and then the same as A1.
Fig. 5.2 The paths Bk;l and
Bk;lC1 agree at each partition
point. In the ﬁgure, s
increases as we move from
the top toward the bottom
A0(t)
A1(t)
Bk,l(t)
Bk,l
1(t)

124
5
The Baker-Campbell-Hausdorff Formula and Its Consequences
Step 4: Prove that ˆ is a homomorphism and agrees with f on U . The proof that
ˆ is a homomorphism is a straightforward unpacking of the deﬁnition of ˆ and
is left to the reader; see Exercise 7. To show that ˆ agrees with f on U , choose
A 2 U . Since U is path connected, we can ﬁnd a path A.t/, 0  t  1, lying in
U joining I to A. Choose a good partition ftjgm
j D1 for A.t/, and we then claim
that for all j, we have ˆ.A.tj// D f .A.tj //.
Note that A.t/, 0  t  tj is a path joining I to A.tj / and that ft0; t1; : : : ; tj g is
a good partition of this path. (Technically, we should reparameterize this path so
that the time interval is Œ0; 1.) Hence,
ˆ.A.tj // D f .A.tj /A.tj 1/1/    f .A.t2/A.t1/1/f .A.t1//;
for all j. In particular,
ˆ.A.t1// D f .A.t1//:
Now assume that ˆ.A.tj // D f .A.tj //, and compute that
ˆ.A.tj C1// D f .A.tj C1/A.tj/1/f .A.tj /A.tj 1/1/    f .A.t1//
D f .A.tj C1/A.tj/1/ˆ.A.tj//
D f .A.tj C1/A.tj/1/f .A.tj //
D f .A.tj //:
The last equality holds because f is a local homomorphism and because
A.tj C1/A.tj/1, A.tj/, and their product all lie in U . Thus, by induction,
ˆ.A.tj // D f .A.tj // for all j; when j D m, we obtain ˆ.A/ D f .A/.
ut
It is important to note that when proving independence of the path (Step 3 of
the proof), it is essential to know already that the value of ˆ.A/ is independent of
the choice of good partition (Step 2 of the proof). Speciﬁcally, when we move from
Bk;l to Bk;lC1, we use one partition for Bk;lC1, but when we move from Bk;lC1 to
Bk;lC2, we use a different partition for Bk;lC1. Essentially, the proof proceeds by
deforming the path between partition points—which clearly does not change the
value of ˆ.A/—then picking a new partition and doing the same thing again. Note
also that it is in proving independence of the partition that we use the assumption
that f is a local homomorphism.
Proof of Theorem 5.6. For the existence part of the proof, let f be the local
homomorphism in Proposition 5.9 and let ˆ be the global homomorphism in
Theorem 5.10. Then for any X 2 g, the element eX=m will be in U for all sufﬁciently
large m, showing that
ˆ.eX=m/ D f .eX=m/ D e.X/=m:

5.7
Group Versus Lie Algebra Homomorphisms
125
Since ˆ is a homomorphism, we have
ˆ.eX/ D ˆ.eX=m/m D e.X/;
as required.
For the uniqueness part of the proof, suppose ˆ1 and ˆ2 are two homomorphisms
related in the desired way to . Then for any A 2 G, we express A as eX1    eXN
with Xj 2 g, as in Corollary 3.47, and observe that
ˆ1.A/ D ˆ2.A/ D e.X1/    e.XN /;
so that ˆ1 agrees with ˆ2.
ut
We conclude this section with a typical application of Theorem 5.6.
Theorem 5.11. Suppose that G is a simply connected matrix Lie group and that
the Lie algebra g of G decomposes as a Lie algebra direct sum g D h1 ˚h2, for two
subalgebras h1 and h2 of g. Then there exists closed, simply connected subgroups
H1 and H2 of G whose Lie algebras are h1 and h2, respectively. Furthermore, G is
isomorphic to the direct product of H1 and H2.
Proof. Consider the Lie algebra homomorphism  W g ! g that sends X C Y to
X, where X 2 h1 and Y 2 h2. Since G is simply connected, there is a Lie group
homomorphism ˆ W G ! G associated to , and the Lie algebra of the kernel of ˆ
is the kernel of  (Proposition 3.31), which is h2. Let H2 be the identity component
of ker ˆ. Since ˆ is continuous, ker ˆ is closed, and so is its identity component H2
(Corollary 3.52). Thus, H2 is a closed, connected subgroup of G with Lie algebra
h2. By a similar argument, we may construct a closed, connected subgroup H1 of G
whose Lie algebra is h1.
Suppose now that A.t/ is a loop in H1. Since G is simply connected, there is
a homotopy A.s; t/ shrinking A.t/ to a point in G. Now,  is the identity on h1,
from which it follows that ˆ is the identity on H1. Thus, if we deﬁne B.s; t/ D
ˆ.A.s; t//, we see that
B.0; t/ D ˆ.A.t// D A.t/:
Furthermore, since  maps G into h1, we see that ˆ maps G into H1. We conclude
that B is a homotopy of A.t/ to a point in H1. Thus, H1 is simply connected, and,
by a similar argument, so is H2.
Finally, since g is the Lie algebra direct sum of h1 and h2, elements of h1
commutes with elements of h2. It follows that elements of H1 (which are all product
of exponentials of elements of h1) commute with elements of H2. Thus, we have a
Lie group homomorphism ‰ W H1  H2 ! G given by ‰.A; B/ D AB. The
associated Lie algebra homomorphism  is then just the original isomorphism of
h1 ˚ h2 with g. Since G is simply connected, there is a homomorphism  W G !
H1  H2 for which the associated Lie algebra homomorphism is  1. By the proof
of Corollary 5.7,  and ‰ are inverses of each other, showing that G is isomorphic
to H1  H2.
ut

126
5
The Baker-Campbell-Hausdorff Formula and Its Consequences
5.8
Universal Covers
Theorem 5.6 says that if G is simply connected, every homomorphism of the Lie
algebra g of G can be exponentiated to a homomorphism of G. If G is not simply
connected, we may look for another group QG that has the same Lie algebra as G but
such that QG is simply connected.
Deﬁnition 5.12. Let G be a connected matrix Lie group. Then a universal cover
of G is a simply connected matrix Lie group H together with a Lie group
homomorphism ˆ W H ! G such that the associated Lie algebra homomorphism
 W h ! g is a Lie algebra isomorphism. The homomorphism ˆ is called the
covering map.
If a universal cover of G exists, it is unique up to "canonical isomorphism," as
follows.
Proposition 5.13. If G is a connected matrix Lie group and .H1; ˆ1/ and .H2; ˆ2/
are universal covers of G, then there exists a Lie group isomorphism ‰ W H1 ! H2
such that ˆ2 ı ‰ D ˆ1.
Proof. See Exercise 9.
ut
Since a connected matrix Lie group has at most one universal cover (up to
canonical isomorphism), it is reasonable to speak of the universal cover . QG; ˆ/ of
G. Furthermore, if H is a simply connected Lie group and  W h ! g is a Lie
algebra isomorphism, then by Theorem 5.6, we can construct an associated Lie
group homomorphism ˆ W H ! G, so that .H; ˆ/ is a universal cover of G.
Since  is an isomorphism, we can use  to identify Qg with g. Thus, in slightly less
formal terms, we may deﬁne the notion of universal cover as follows: The universal
cover of a matrix Lie group G is a simply connected matrix Lie group QG such that
the Lie algebra of QG is equal to the Lie algebra of G. With this perspective, we
have the following immediate corollary of Theorem 5.6.
Corollary 5.14. Let G be a connected matrix Lie group and let QG be the universal
cover of G, where we think of G and QG as having the same Lie algebra g. If H is a
matrix Lie group with Lie algebra h and  W g ! h is a Lie algebra homomorphism,
there exists a unique homomorphism ˆ W QG ! H such that ˆ.eX/ D e.X/ for all
X 2 g.
An example of importance in physics is the universal cover of SO.3/.
Example 5.15. The universal cover of SO.3/ is SU.2/.
Proof. The group SU.2/ is simply connected by Proposition 1.15. Proposition 1.19
and Example 3.29 then provide the desired covering map.
ut
The topic of universal covers is one place where we pay a price for our decision
to consider only matrix Lie groups: a matrix Lie group may not have a universal
cover that is a matrix Lie group. It is not hard to show that every Lie group G

5.8
Universal Covers
127
has a universal cover in the class of (not necessarily matrix) Lie groups. Indeed,
G has a universal cover in the topological sense (Deﬁnition 13.1), and this cover
can be given a group structure in such a way that the covering map is a Lie group
homomorphism. It turns out, however, that the universal cover of a matrix Lie group
may not be a matrix group.
We now show that the group SL.2I R/ does not have a universal cover in the class
of matrix Lie groups. We begin by showing that SL.2I R/ is not simply connected.
By Theorem 2.17 and Proposition 2.19, SL.2I R/, as a manifold, is homeomorphic
to SO.2/  V , where V is the space of 2  2 real, symmetric matrices with trace
zero. Now, V , being a vector space, is certainly simply connected, but SO.2/, which
is homeomorphic to the unit circle S1, is not. Thus, SL.2I R/ itself is not simply
connected. By contrast, the group SL.2I C/ decomposes as SU.2/W , where W is
the space of 2  2 self-adjoint, complex matrices with trace zero. Since both SU.2/
and W are simply connected, SL.2I C/ is simply connected. (See Appendix 13.3 for
more information on this type of calculation.)
Proposition 5.16. Let G  GL.nI C/ be a connected matrix Lie group with Lie
algebra g. Suppose ˆ W G ! SL.2I R/ is a Lie group homomorphism for which
the associated Lie algebra  W g ! sl.2I R/ is a Lie algebra isomorphism. Then
ˆ is a Lie group isomorphism and, therefore, G cannot be simply connected. Thus,
SL.2I R/ has no universal cover in the class of matrix Lie groups.
The result relies essentially on the assumption that G is a matrix Lie group—or,
more precisely, on the assumption that G is contained in a group whose Lie algebra
is complex.
Lemma 5.17. Suppose  W sl.2I R/ ! gl.nI C/ is a Lie algebra homomorphism.
Then there exists a Lie group homomorphism ˆ W SL.2I R/ ! GL.nI C/ such that
ˆ.eX/ D e.X/ for all X 2 sl.2I R/.
The signiﬁcance of the lemma is that the result holds even though SL.2I R/ is
not simply connected.
Proof. Let  C W sl.2I C/ ! gl.nI C/ be the complex-linear extension of  to
sl.2I C/ Š sl.2I R/C, which is a Lie algebra homomorphism (Proposition 3.39).
Since SL.2I C/ is simply connected, there exists a Lie group homomorphism
‰C W SL.2I C/ ! GL.nI C/ such that ‰C.eX/ D e C.X/ for all X 2 sl.2I C/. If we
let ‰ be the restriction of ‰C to SL.2I R/, then ‰ is a Lie group homomorphism
which satisﬁes ‰.eX/ D e .X/ for X 2 sl.2I R/.
ut
Proof of Proposition 5.16. Since  is a Lie algebra isomorphism, the inverse map
 W sl.2I R/ ! g is a Lie algebra homomorphism. Thus, by the lemma, there is a
Lie group homomorphism ‰ W SL.2I R/ ! G corresponding to  . Since  and  
are inverses of each other, it follows from Proposition 3.30 and Corollary 3.49 that
ˆ and ‰ are also inverses of each other.
ut

128
5
The Baker-Campbell-Hausdorff Formula and Its Consequences
5.9
Subgroups and Subalgebras
In this section, we address Question 3 from Sect. 5.1: If G is a matrix Lie group with
Lie algebra g and h is a subalgebra of g, does there exist a matrix Lie group H  G
whose Lie algebra is H? If the exponential map for G were a homeomorphism
between g and G and if the BCH formula worked globally instead of locally, the
answer would be yes, since we could simply deﬁne H to be the set of elements of
the form eX, X 2 g, and the BCH formula would show that H is a subgroup.
In reality, the answer to Question 3, as stated, is no. Suppose, for example, that
G D GL .2I C/ and
h D
  it 0
0 ita
ˇˇˇˇ t 2 R

,
(5.21)
where a is irrational. If there is going to be a matrix Lie group H with Lie algebra
h, then H would have to contain the closure of the group
H0 D
  eit 0
0 eita
ˇˇˇˇ t 2 R

;
(5.22)
which is (Exercise 10 in Chapter 1) is the group
H1 D
  ei 0
0 ei
ˇˇˇˇ ;  2 R

.
But then the Lie algebra of H would have to contain the Lie algebra of H1, which
is two dimensional!
Fortunately, all is not lost. We can still get a subgroup H for each subalgebra h
if we weaken the condition that H be a matrix Lie group. In the above example, the
subgroup we want is H0, despite the fact that H0 is not closed.
Deﬁnition 5.18. If H is any subgroup of GL .nI C/, the Lie algebra h of H is the
set of all matrices X such that
etX 2 H
for all real t.
It is possible to prove that for any subgroup H of GL.nI C/, the Lie algebra h of
H is actually a Lie algebra, that is, a real vector space—possibly zero dimensional—
and closed under brackets. (See Proposition 1 and Corollary 7 in Chapter 2 of
[Ross].) This result is not, however, directly relevant to our goal in this section,
which is to construct, for each subalgebra h of gl.nI C/ a subgroup with Lie algebra
h. Note, however, that if h is at least a real subspace of gl.nI C/, then the proof of
Point 4 of Theorem 3.20 shows that h is also closed under brackets.

5.9
Subgroups and Subalgebras
129
Deﬁnition 5.19. If G is a matrix Lie group with Lie algebra g, then H  G is a
connected Lie subgroup of G if the following conditions are satisﬁed:
1. H is a subgroup of G.
2. The Lie algebra h of H is a Lie subalgebra of g.
3. Every element of H
can be written in the form eX1eX2    eXm, with
X1; : : : ; Xm 2 h.
Connected Lie subgroups are also called analytic subgroups. Note that any
group H as in the deﬁnition is path connected, since each element of H can be
connected to the identity in H by a path of the form
t 7! e.1t/X1e.1t/X2    e.1t/Xm:
The group H0 in (5.22) is a connected Lie subgroup of GL.2I C/ whose Lie algebra
is the algebra h in (5.21).
We are now ready to state the main result of this section, which is our second
major application of the Baker-Campbell-Hausdorff formula.
Theorem 5.20. Let G be a matrix Lie group with Lie algebra g and let h be a Lie
subalgebra of g. Then there exists a unique connected Lie subgroup H of G with
Lie algebra h.
If h is the subalgebra of gl.2I C/ in (5.21), then the connected Lie subgroup H
is the group H0 in (5.22), which is not closed. In practice, Theorem 5.20 is most
useful in those cases where the connected Lie subgroup H turns out to be closed.
See Proposition 5.24 and Exercises 10, 13, and 14 for conditions under which this
is the case.
We now begin working toward the proof of Theorem 5.20. Since G is assumed
to be a matrix Lie group, we may as well assume that G D GL.nI C/. After all, if G
is a closed subgroup of GL.nI C/ and H is a connected Lie subgroup of GL.nI C/
whose Lie algebra h is contained in g, then H is also a connected Lie subgroup
of G. We now let
H D feX1eX2    eXN ˇˇ X1; : : : ; XN 2 hg;
(5.23)
which is a subgroup of G. The key issue is to prove that the Lie algebra of H,
in the sense of Deﬁnition 5.18, is h. Once we know that Lie.H/ D h, we will
immediately conclude that H is a connected Lie subgroup with Lie algebra h, the
remaining properties in Deﬁnition 5.19 being true by deﬁnition. Note that for the
claim Lie.H/ D h to be true, it essential that h be a subalgebra of gl.nI C/, and not
merely a subspace; compare Exercise 11.
As in the proof of Theorem 3.42, we think of gl.nI C/ as R2n2 and we decompose
gl.nI C/ as the direct sum of h and D, where D is the orthogonal complement of
h with respect to the usual inner product on R2n2. Then, as shown in the proof
of Theorem 3.42, there exist neighborhoods U and V of the origin in h and D,
respectively, and a neighborhood W of I in GL.nI C/ with the following properties:

130
5
The Baker-Campbell-Hausdorff Formula and Its Consequences
Each A 2 W can be written uniquely as
A D eXeY ;
X 2 U; Y 2 V;
(5.24)
in such a way that X and Y depend continuously on A. We think of the
decomposition in (5.24) as our local coordinates in a neighborhood of the identity
in GL.nI C/.
If X is a small element of h, the decomposition of eX is just eXe0. If we take the
product of two elements of the form eX1eX2, with X1 and X2 small elements of h,
then since h is a subalgebra, if we combine the exponentials as eX1eX2 D eX3 by
means of the Baker-Campbell-Hausdorff formula, X3 will again be in h. Thus, if
we take a small number of products as in (5.23) with the Xj's being small elements
of h, we will move from the identity in the X-direction in the decomposition (5.24).
Globally, however, H may wind around and come back to points in W of the
form (5.24) with Y ¤ 0. (See Figure 5.3.) Indeed, as the example of the "irrational
line" in (5.22) shows, there may be elements of H in W with arbitrarily small
nonzero values of Y . Nevertheless, we will see that the set of Y values that occurs
is at most countable.
Lemma 5.21. Decompose gl.nI C/ as h ˚ D and let V be a neighborhood of the
origin in D as in (5.24). If E  V is deﬁned by
E D fY 2 V
ˇˇeY 2 H g;
then E is at most countable.
Assuming the lemma, we may now prove Theorem 5.20.
eU
eV
Fig. 5.3 The black lines indicate the portion of H in the set W . The group H intersects eV in at
most countably many points

5.9
Subgroups and Subalgebras
131
Proof of Theorem 5.20. As we have already observed, it sufﬁces to show that the
Lie algebra of H is h. Let h0 be the Lie algebra of H, which clearly contains h. For
Z 2 h0, we may write, for all sufﬁciently small t,
etZ D eX.t/eY.t/;
where X.t/ 2 U  h and Y.t/ 2 V  D and where X.t/ and Y.t/ are continuous
functions of t. Since Z is in the Lie algebra of H, we have etZ 2 H for all t. Since,
also, eX.t/ is in the group H, we conclude that eY.t/ is in H for all sufﬁciently
small t. If Y.t/ were not constant, then it would take on uncountably many values,
which would mean that E is uncountable, violating Lemma 5.21. So, Y.t/ must be
constant, and since Y.0/ D 0, this means that Y.t/ is identically equal to zero. Thus,
for small t, we have etZ D eX.t/ and, therefore, tZ D X.t/ 2 h. This means Z 2 h
and we conclude that h0  h.
ut
Before proving Lemma 5.21, we prove another lemma.
Lemma 5.22. Pick a basis for h and call an element of h rational if its coefﬁcients
with respect to this basis are rational. Then for every ı > 0 and every A 2 H, there
exist rational elements R1; : : : ; Rm of h such that
A D eR1eR2    eRmeX;
where X is in h and kXk < ı.
Suppose we take ı small enough that the ball of radius ı in h is contained in U .
Then since there are only countably many m-tuples of the form .R1; : : : ; Rm/ with
Rj rational, the lemma tells us that H can be covered by countably many translates
of the set eU .
Proof. Choose " > 0 so that for all X; Y 2 h with kXk < " and kY k < ", the
Baker-Campbell-Hausdorff holds for X and Y . Let C.; / denote the right-hand
side of the formula, so that
eXeY D eC.X;Y /
whenever kXk ; kY k < ". It is not hard to see that C.; / is a continuous. Now, if the
lemma holds for some ı, it also holds for any ı0 > ı. Thus, it is harmless to assume
ı is less than " and small enough that if kXk ; kY k < ı, we have kC.X; Y /k < ".
Since eX D .eX=k/k, every element A of H can be written as
A D eX1    eXN
(5.25)
with Xj 2 h and
		Xj
		 < ı. We now proceed by induction on N . If N D 0, then
A D I D e0, and there is nothing to prove. Assume the lemma for A's that can be
expressed as in (5.25) for some integer N , and consider A of the form

132
5
The Baker-Campbell-Hausdorff Formula and Its Consequences
A D eX1    eXN eXN C1
(5.26)
with Xj 2 h and
		Xj
		 < ı. Applying our induction hypothesis to eX1    eXN , we
obtain
A D eR1    eRmeXeXN C1
D eR1    eRmeC.X;XN C1/:
where the Rj's are rational and kC.X; XNC1/k < ". Since h is a subalgebra of
gl.nI C/, the element C.X; XNC1/ is again in h, but may not have norm less than ı.
Now choose a rational element RmC1 of h that is very close to C.X; XNC1/ and
such that kRmC1k < ". We then have
A D eR1    eRmeRmC1eRmC1eC.X;XN C1/
D eR1    eRmeRmC1eX0;
where
X0 D C.RmC1; C.X; XNC1//:
Then X0 will be in h, and by choosing RmC1 sufﬁciently close to C.X; XNC1/, we
can make kX0k < ı. After all, since C.Z; Z/ D log.eZeZ/ D 0 for all small Z,
if Z0 is close to Z, then C.Z0; Z/ will be small.
ut
We now supply the proof of Lemma 5.21.
Proof of Lemma 5.21. Fix ı so that for all X and Y with kXk ; kY k < ı, the
quantity C.X; Y / is deﬁned and contained in U . We then claim that for each
sequence R1; : : : ; Rm of rational elements in h, there is at most one X 2 h with
kXk < ı such that the element
eR1eR2    eRmeX
(5.27)
belongs to eV . After all, if we have
eR1eR2    eRmeX1 D eY1;
(5.28)
eR1eR2    eRmeX2 D eY2
(5.29)
with Y1; Y2 2 V , then
eX1eX2 D eY1eY2
and so
eY1 D eX1eX2eY2 D eC.X1;X2/eY2;

5.9
Subgroups and Subalgebras
133
with C.X1; X2/ 2 U . However, each element of eU eV has a unique representation
as eY eX with X 2 U and Y 2 V . Thus, we must have Y2 D Y1 and, by (5.28)
and (5.29), eX1 D eX2 and X1 D X2.
By Lemma 5.22, every element of H can be expressed in the form (5.27) with
kXk < ı. Now, there are only countably many rational elements in h and thus only
countably many expressions of the form eR1    eRm, each of which produces at most
one element of the form (5.27) that belongs to eV . Thus, the set E in Lemma 5.21
is at most countable.
ut
This completes the proof of Theorem 5.20.
If a connected Lie subgroup H of GL.nI C/ is not closed, the topology H
inherits from GL.nI C/ may be pathological, e.g., not locally connected. (Compare
Figure 1.1.) Nevertheless, we can give H a new topology that is much nicer.
Theorem 5.23. Let H be a connected Lie subgroup of GL.nI C/ with Lie algebra h.
Then H can be given the structure of a smooth manifold in such a way that the group
operations on H are smooth and the inclusion map of H into GL.nI C/ is smooth.
Thus, every connected Lie subgroup of GL.nI C/ can be made into a Lie group. In
the case of the group H0 in (5.22), the new topology on H0 is obtained by identifying
H0 with R by means of the parameter t in the deﬁnition of H0.
Proof. For any A 2 H and any " > 0, deﬁne
UA;" D fAeXˇˇ X 2 h and kXk < "g:
Now deﬁne a topology on H as follows: A set U  H is open if for each A 2 U
there exists " > 0 such that UA;"  U . (See Figure 5.4.) In this topology, two
elements A and B of H are "close" if we can express B as B D AeX with X 2 h
and kXk small. This topology is ﬁner than the topology H inherits from GI that is,
if A and B are close in this new topology, they are certainly close in the ordinary
sense in G, but not vice versa.
It is easy to check that this topology is Hausdorff, and using Lemma 5.22, it is
not hard to see that the topology is second countable. Furthermore, in this topology,
H is locally homeomorphic to RN , where N D dim h, by identifying each UA;"
with the ball of radius " in h.
We may deﬁne a smooth structure on H by using the UA;"'s, with " less than some
small number "0, as our "atlas." If two of these sets overlap, then some element C of
H can be written as C D AeX D BeY for some A; B 2 H and X; Y 2 h. It follows
that B D AeXeY , which means (since kXk and kY k are less than "0) that A and
B are close. The change-of-coordinates map is then Y D log.B1AeX/. Since A
and B are close and kXk is small, we will have that
		B1AeX  I
		 < 1, so that
B1AeX is in the domain where the matrix logarithm is deﬁned and smooth. Thus,
the change-of-coordinates map is smooth as function of X. Finally, in any of the
coordinate neighborhoods UA;", the inclusion of H into G is given by X 7! AeX,
which is smooth as a function of X.
ut

134
5
The Baker-Campbell-Hausdorff Formula and Its Consequences
Fig. 5.4 The set U in H is
open the new topology but not
in the topology inherited from
GL.2I C/. The element B is
close to A in GL.2I C/ but
not in the new topology on H
A
B
U
2p
2p
f
q
As we have already noted, Theorem 5.20 is most useful in cases where the
connected Lie subgroup H is actually closed. The following result gives one
condition under which this is guaranteed to be the case. See also Exercises 10, 13,
and 14.
Proposition 5.24. Suppose G  GL.nI C/ is a matrix Lie group with Lie algebra
g and that h is a maximal commutative subalgebra of g, meaning that h is
commutative and h is not contained in any larger commutative subalgebra of g.
Then the connected Lie subgroup H of G with Lie algebra h is closed.
Proof. Since h is commutative, H is also commutative, since every element of H
is a product of exponentials of elements of h. It easily follows that the closure NH of
H in GL.nI C/ is also commutative. We now claim that NH is connected. To see this,
take A 2 NH, so that A is in G (since G is closed) and A is the limit of a sequence
Am in H. Since NH is closed, Theorem 3.42 applies. Thus, for all sufﬁciently large
m, the element AA1
m is expressible as AA1
m D eX, for some X in the Lie algebra h0
of NH. Thus, A D eXAm, which means that A can be connected to Am by the path
A.t/ D e.1t/XAm, 0  t  1, in NH. Since Am can be connected to the identity in
H  NH, we see that A can be connected to the identity in NH.
Now, since NH is commutative, its Lie algebra h0 is also commutative. But since
h was maximal commutative, we must have h0 D h. Since, also, NH is connected, we
conclude that NH D H, showing that H is closed.
ut

5.11
Exercises
135
5.10
Lie's Third Theorem
Lie's third theorem (in its modern, global form) says that for every ﬁnite-
dimensional, real Lie algebra g, there exists a Lie group G with Lie algebra g.
We will construct G as a connected Lie subgroup of GL.nI C/.
Theorem 5.25. If g is any ﬁnite-dimensional, real Lie algebra, there exists a
connected Lie subgroup G of GL.nI C/ whose Lie algebra is isomorphic to g.
Our proof assumes Ado's theorem, which asserts that every ﬁnite-dimensional
real or complex Lie algebra is isomorphic to an algebra of matrices. (See, for
example, Theorem 3.17.7 in [Var].)
Proof. By Ado's theorem, we may identify g with a real subalgebra of gl.nI C/.
Then by Theorem 5.20, there is a connected Lie subgroup of GL.nI C/ with Lie
algebra g.
ut
It is actually possible to choose the subgroup G in Theorem 5.25 to be closed.
Indeed, according to Theorem 9 on p. 105 of [Got], if a connected Lie group G
can be embedded into some GL.nI C/ as a connected Lie subgroup, then G can be
embedded into some other GL.n0I C/ as a closed subgroup. Assuming this result,
we may reach the following conclusion.
Conclusion 5.26. Every ﬁnite-dimensional, real Lie algebra is isomorphic to the
Lie algebra of some matrix Lie group.
This result does not, however, mean that every Lie group is isomorphic to a
matrix Lie group, since there can be several nonisomorphic Lie groups with the
same Lie algebra. See, for example, Sect. 4.8.
5.11
Exercises
1. Let X be a linear transformation on a ﬁnite-dimensional real or complex vector
space. Show that
I  eX
X
is invertible if and only if none of the eigenvalues of X (over C) is of the form
2in, with n an nonzero integer.
Remark. This exercise, combined with the formula in Theorem 5.4, gives the
following result (in the language of differentiable manifolds): The exponential
map exp W g ! G is a local diffeomorphism near X 2 g if and only if adX W
g ! g has no eigenvalue of the form 2in, with n a nonzero integer.

136
5
The Baker-Campbell-Hausdorff Formula and Its Consequences
2. Show that for any X and Y in Mn.C/, even if X and Y do not commute,
d
dttrace.eXCtY/
ˇˇˇˇ
tD0
D trace.eXY /:
3. Compute log.eXeY / through third order in X and Y by calculating directly
with the power series for the exponential and the logarithm. Show this gives the
same answer as the Baker-Campbell-Hausdorff formula.
4. Suppose that X and Y are upper triangular matrices with zeros on the diagonal.
Show that the power series for log.eXeY / is convergent. What happens to the
series form of the Baker-Campbell-Hausdorff formula in this case?
5. Suppose X and Y are nn complex matrices satisfying ŒX; Y  D ˛Y for some
complex number ˛. Suppose further that there is no nonzero integer n such that
˛ D 2in. Show that
eXeY D exp
n
X C
˛
1  e˛ Y
o
:
Hint: Let A.t/ D eXetY and let
B.t/ D exp
n
X C
˛
1  e˛ tY
o
:
Using Theorem 5.4, show that A.t/ and B.t/ satisfy the same differential
equation with the same value at t D 0.
6. Give an example of matrices X and Y in sl.2I C/ such that ŒX; Y  D 2iY
but such that there does not exist any Z in sl.2I C/ with eXeY D eZ. Use
Example 3.41 and compare Exercise 5.
7. Complete Step 4 in the proof of Theorem 5.6 by showing that ˆ is a
homomorphism. For all A; B 2 G, choose a path A.t/ connecting I to A and
a path B.t/ connecting I to B. Then deﬁne a path C connecting I to AB by
setting C.t/ D B.2t/ for 0  t  1=2 and setting C.t/ D A.2t  1/B for
1=2  t  1. If t0; : : : ; tm is a good partition for A.t/ and s0; : : : ; sM is a good
partition for B.t/, show that
s0
2 ;    ; sM
2 ; 1
2 C t0
2 ;    ; 1
2 C tm
2
is a good partition for C.t/. Now, compute ˆ.A/, ˆ.B/, and ˆ.AB/ using these
paths and partitions and show that ˆ.AB/ D ˆ.A/ˆ.B/.
8. If QG is a universal cover of a connected group G with projection map ˆ, show
that ˆ maps QG onto G.
9. Prove the uniqueness of the universal cover, as stated in Proposition 5.13.
10. Let a be a subalgebra of the Lie algebra of the Heisenberg group. Show that
exp.a/ is a connected Lie subgroup of the Heisenberg group and that this
subgroup is closed.

5.11
Exercises
137
11. Consider the Lie algebra h of the Heisenberg group H, as computed in
Proposition 3.26. Let X, Y , and Z be the basis elements for h in (4.18), which
satisfy ŒX; Y  D Z and ŒX; Z D ŒY; Z D 0. Let V be the subspace of h
spanned by X and Y (which is not a subalgebra of h) and let K denote the
subgroup of H consisting of products of exponential of elements of V . Show
that K D H and, thus, that the Lie algebra of K is not equal to V .
Hint: Use Theorem 5.1 and the surjectivity of the exponential map for H
(Exercise 18 in Chapter 3).
12. Show that every connected Lie subgroup of SU.2/ is closed. Show that this is
not the case for SU.3/.
13. Let G be a matrix Lie group with Lie algebra g, let h be a subalgebra of g, and
let H be the unique connected Lie subgroup of G with Lie algebra h. Suppose
that there exists a simply connected, compact matrix Lie group K such that the
Lie algebra of K is isomorphic to h. Show that H is closed. Is H necessarily
isomorphic to K?
14. This exercise asks you to prove, assuming Ado's theorem (Sect. 5.10), the
following result: If G is a simply connected matrix Lie group with Lie algebra
g and h is an ideal in g, then the connected Lie subgroup H with Lie algebra h
is closed.
(a) Show that there exists a Lie algebra homomorphism  W g ! gl.N I C/
with ker./ D h.
Hint: Since h is an ideal in g, the quotient space g=h has a natural Lie
algebra structure.
(b) Since G is simply connected, there exists a Lie group homomorphism ˆ W
G ! gl.N I C/ for which the associated Lie algebra homomorphism is .
Show that the identity component of the kernel of ˆ is a closed subgroup
of G whose Lie algebra is h.
(c) Show that the result fails if the assumption that G be simply connected is
omitted.

Part II
Semisimple Lie Algebras

Chapter 6
The Representations of sl.3I C/
6.1
Preliminaries
In this chapter, we investigate the representations of the Lie algebra sl.3I C/,
which is the complexiﬁcation of the Lie algebra of the group SU.3/. The main
result of this chapter is Theorem 6.7, which states that an irreducible ﬁnite-
dimensional representation of sl.3I C/ can be classiﬁed in terms of its "highest
weight." This result is analogous to the results of Sect. 4.6, in which we classify
the irreducible representations by the largest eigenvalue of .H/, namely the non-
negative integer m.
The results of this chapter are special cases of the general theory of repre-
sentations of semisimple Lie algebras (Chapters 7 and 9) and of the theory of
representations of compact Lie groups (Chapters 11 and 12). It is nevertheless useful
to consider this case separately, in part because of the importance of SU.3/ in
physical applications but mainly because seeing roots, weights, and the Weyl group
"in action" in a simple example motivates the introduction of these structures later
in a more general setting.
Every ﬁnite-dimensional representation of SU.3/ (over a complex vector space)
gives rise to a representation of su.3/, which can then be extended by complex
linearity to sl.3I C/ Š su.3/C. Since SU.3/ is simply connected, we can go in the
opposite direction by restricting any representation of sl.3I C/ to su.3/ and then
applying Theorem 5.6 to obtain a representation of SU.3/. Propositions 4.5 and 4.6
tell us that a representation of SU.3/ is irreducible if and only if the associated rep-
resentation of sl.3I C/ is irreducible, thus establishing a one-to-one correspondence
between the irreducible representations of SU.3/ and the irreducible representations
of sl.3I C/. Furthermore, since SU.3/ is compact, Theorem 4.28 then tells us that
all ﬁnite-dimensional representations of SU.3/—and thus, also, of sl.3I C/—are
completely reducible.
© Springer International Publishing Switzerland 2015
B. Hall, Lie Groups, Lie Algebras, and Representations, Graduate
Texts in Mathematics 222, DOI 10.1007/978-3-319-13467-3_6
141

142
6
The Representations of sl.3I C/
It is desirable, however, to avoid relying unnecessarily on Theorem 5.6, which
in turn relies on the Baker-Campbell-Hausdorff formula. If we look the repre-
sentations from the Lie algebra point of view, we can classify the irreducible
representations of sl.3I C/ without knowing that they come from representations
of SU.3/. Of course, classifying the irreducible representations of sl.3I C/ does
not tell one what a general representation of sl.3I C/ looks like, unless one knows
complete reducibility. Nevertheless, it is possible to give an algebraic proof of
complete reducibility, without referring to the group SU.3/. This proof is given
in the setting of general semisimple Lie algebras in Sect. 10.3, but it should be fairly
easy to specialize the argument to the sl.3I C/ case.
Meanwhile, if we look at the representations from the group point of view,
we can construct the irreducible representations of SU.3/ without knowing that
every representation of sl.3I C/ gives rise to a representation of SU.3/. Indeed, the
irreducible representations of SU.3/ are constructed as subspaces of tensor products
of several copies of the standard representation with several copies of the dual of the
standard representation. Since the standard representation and its dual are deﬁned
directly at the level of the group SU.3/, there is no need to appeal to Theorem 5.6.
In short, this chapter provides a self-contained classiﬁcation of the irreducible
representations of both SU.3/ and sl.3I C/, without needing to know the results
of Chapter 5. We establish results for sl.3I C/ ﬁrst, and then pass to SU.3/
(Theorem 6.8).
6.2
Weights and Roots
We will use the following basis for sl.3I C/:
H1 D
0
@
1
0 0
0 1 0
0
0 0
1
A ;
H2 D
0
@
0 0
0
0 1
0
0 0 1
1
A ;
X1 D
0
@
0 1 0
0 0 0
0 0 0
1
A ;
X2 D
0
@
0 0 0
0 0 1
0 0 0
1
A ;
X3 D
0
@
0 0 1
0 0 0
0 0 0
1
A ;
Y1 D
0
@
0 0 0
1 0 0
0 0 0
1
A ;
Y2 D
0
@
0 0 0
0 0 0
0 1 0
1
A ;
Y3 D
0
@
0 0 0
0 0 0
1 0 0
1
A .
Note that the span hH1; X1; Y1i of H1, X1, and Y1 is a subalgebra of sl.3I C/
isomorphic to sl.2I C/, as can be seen by ignoring the third row and the third
column in each matrix. The subalgebra hH2; X2; Y2i is also, similarly, isomorphic
to sl.2I C/. Thus, we have the following commutation relations:

6.2
Weights and Roots
143
ŒH1; X1 D
2X1; ŒH2; X2 D
2X2;
ŒH1; Y1 D 2Y1; ŒH2; Y2 D 2Y2;
ŒX1; Y1 D
H1;
ŒX2; Y2 D
H2.
We now list all of the commutation relations among the basis elements which
involve at least one of H1 and H2. (This includes some repetitions of the above
commutation relations.)
ŒH1; H2 D
0I
ŒH1; X1 D 2X1; ŒH1; Y1 D 2Y1;
ŒH2; X1 D X1; ŒH2; Y1 D
Y1I
ŒH1; X2 D X2; ŒH1; Y2 D
Y2;
ŒH2; X2 D 2X2; ŒH2; Y2 D 2Y2I
ŒH1; X3 D
X3; ŒH1; Y3 D
Y3;
ŒH2; X3 D
X3; ŒH2; Y3 D
Y3:
(6.1)
Finally, we list all of the remaining commutation relations.
ŒX1; Y1 D
H1;
ŒX2; Y2 D
H2;
ŒX3; Y3 D H1 C H2I
ŒX1; X2 D X3;
ŒY1; Y2 D Y3;
ŒX1; Y2 D
0;
ŒX2; Y1 D
0I
ŒX1; X3 D
0;
ŒY1; Y3 D
0;
ŒX2; X3 D
0;
ŒY2; Y3 D
0I
ŒX2; Y3 D Y1;
ŒX3; Y2 D X1;
ŒX1; Y3 D Y2; ŒX3; Y1 D X2:
All of our analysis of the representations of sl.3I C/ will be in terms of the above
basis. From now on, all representations of sl.3I C/ will be assumed to be ﬁnite
dimensional and complex linear.
Our basic strategy in classifying the representations of sl.3I C/ is to simultane-
ously diagonalize .H1/ and .H2/. (See Sect. A.8 for information on simultaneous
diagonalization.) Since H1 and H2 commute, .H1/ and .H2/ will also commute
(for any representation ) and so there is at least a chance that .H1/ and .H2/
can be simultaneously diagonalized. (Compare Proposition A.16.)

144
6
The Representations of sl.3I C/
Deﬁnition 6.1. If .; V / is a representation of sl.3I C/, then an ordered pair
 D .m1; m2/ 2 C2 is called a weight for  if there exists v ¤ 0 in V such that
.H1/v D m1v;
.H2/v D m2v.
(6.2)
A nonzero vector v satisfying (6.2) is called a weight vector corresponding to the
weight . If  D .m1; m2/ is a weight, then the space of all vectors v satisfying (6.2)
is the weight space corresponding to the weight . The multiplicity of a weight is
the dimension of the corresponding weight space.
Thus, a weight is simply a pair of simultaneous eigenvalues for .H1/ and
.H2/. It is easily shown that isomorphic representations have the same weights
and multiplicities.
Proposition 6.2. Every representation of sl.3I C/ has at least one weight.
Proof. Since we are working over the complex numbers, .H1/ has at least one
eigenvalue m1 2 C. Let W  V be the eigenspace for .H1/ with eigenvalue m1.
Since ŒH1; H2 D 0, .H2/ commutes with .H1/, and, so, by Proposition A.2,
.H2/ must map W into itself. Then the restriction of .H2/ to W must have at least
one eigenvector w with eigenvalue m2 2 C, which means that w is a simultaneous
eigenvector for .H1/ and .H2/ with eigenvalues m1 and m2.
ut
Every representation  of sl.3I C/ can be viewed, by restriction, as a representa-
tion of the subalgebras hH1; X1; Y1i and hH2; X2; Y2i, both of which are isomorphic
to sl.2I C/.
Proposition 6.3. If .; V / is a representation of sl.3I C/ and  D .m1; m2/ is a
weight of V , then both m1 and m2 are integers.
Proof. Apply Point 1 of Theorem 4.34 to the restriction of  to hH1; X1; Y1i and to
the restriction of  to hH2; X2; Y2i.
ut
Our strategy now is to begin with one simultaneous eigenvector for .H1/ and
.H2/ and then to apply .Xj/ or .Yj/ and see what the effect is. The following
deﬁnition is relevant in this context.
Deﬁnition 6.4. An ordered pair ˛ D .a1; a2/ 2 C2 is called a root if
1. a1 and a2 are not both zero, and
2. there exists a nonzero Z 2 sl.3I C/ such that
ŒH1; Z D a1Z;
ŒH2; Z D a2Z.
The element Z is called a root vector corresponding to the root ˛.

6.2
Weights and Roots
145
Condition 2 in the deﬁnition says that Z is a simultaneous eigenvector for adH1
and adH2. This means that Z is a weight vector for the adjoint representation with
weight .a1; a2/. Thus, taking into account Condition 1, we may say that the roots
are precisely the nonzero weights of the adjoint representation. The commutation
relations (6.1) tell us that we have the following six roots for sl.3I C/:
˛
Z
.2; 1/
X1
.1; 2/
X2
.1; 1/
X3
˛
Z
.2; 1/
Y1
.1; 2/
Y2
.1; 1/
Y3
:
(6.3)
Note that H1 and H2 are also simultaneous eigenvectors for adH1 and adH2, but they
are not root vectors because the simultaneous eigenvalues are both zero. Since the
vectors in (6.3), together with H1 and H2, form a basis for sl.3I C/, it is not hard
to show that the roots listed in (6.3) are the only roots (Exercise 1). These six roots
form a "root system," conventionally called A2. (For much more information about
root systems, see Chapter 8.)
It is convenient to single out the two roots corresponding to X1 and X2:
˛1 D .2; 1/I
˛2 D .1; 2/;
(6.4)
which we call the positive simple roots. They have the property that all of the roots
can be expressed as linear combinations of ˛1 and ˛2 with integer coefﬁcients, and
these coefﬁcients are (for each root) either all greater than or equal to zero or all less
than or equal to zero. This is veriﬁed by direct computation:
.2; 1/ D ˛1I
.1; 2/ D ˛2I
.1; 1/ D ˛1 C ˛2;
with the remaining three roots being the negatives of the ones above. The decision
to designate ˛1 and ˛2 as the positive simple roots is arbitrary; any other pair of
roots with similar properties would do just as well.
The signiﬁcance of the roots for the representation theory of sl.3I C/ is contained
in the following lemma, which is the analog of Lemma 4.33 in the sl.2I C/ case.
Lemma 6.5. Let ˛ D .a1; a2/ be a root and let Z˛ 2 sl.3I C/ be a corresponding
root vector. Let  be a representation of sl.3I C/, let  D .m1; m2/ be a weight for
, and let v ¤ 0 be a corresponding weight vector. Then we have
.H1/.Z˛/v D .m1 C a1/.Z˛/v;
.H2/.Z˛/v D .m2 C a2/.Z˛/v.
Thus, either .Z˛/v D 0 or .Z˛/v is a new weight vector with weight
 C ˛ D .m1 C a1; m2 C a2/.

146
6
The Representations of sl.3I C/
Proof. By the deﬁnition of a root, we have the commutation relation ŒH1; Z˛ D
a1Z˛. Thus,
.H1/.Z˛/v D ..Z˛/.H1/ C a1.Z˛// v
D .Z˛/.m1v/ C a1.Z˛/v
D .m1 C a1/.Z˛/v.
A similar argument allows us to compute .H2/.Z˛/v.
ut
6.3
The Theorem of the Highest Weight
If we have a representation with a weight  D .m1; m2/, then by applying the root
vectors X1; X2; X3; Y1; Y2, and Y3, we obtain new weights of the form  C ˛,
where ˛ is the root. Of course, some of the time, .Z˛/v will be zero, in which
case  C ˛ is not necessarily a weight. In fact, since our representation is ﬁnite
dimensional, there can be only ﬁnitely many weights, so we must get zero quite
often. By analogy to the classiﬁcation of the representations of sl.2I C/, we would
like to single out in each representation a "highest" weight and then work from
there. The following deﬁnition gives the "right" notion of highest.
Deﬁnition 6.6. Let ˛1 D .2; 1/ and ˛2 D .1; 2/ be the roots introduced in (6.4).
Let 1 and 2 be two weights. Then 1 is higher than 2 (or, equivalently, 2 is
lower than 1) if 1  2 can be written in the form
1  2 D a˛1 C b˛2
(6.5)
with a  0 and b  0. This relationship is written as 1  2 or 2  1.
If  is a representation of sl.3I C/, then a weight 0 for  is said to be a highest
weight if for all weights  of ,   0.
Note that the relation of "higher" is only a partial ordering; for example, ˛1  ˛2
is neither higher nor lower than 0. In particular, a ﬁnite set of weights need not have
a highest element. Note also that the coefﬁcients a and b in (6.5) do not have to be
integers, even if both 1 and 2 have integer entries. For example, .1; 0/ is higher
than .0; 0/ since .1; 0/ D 2
3˛1 C 1
3˛2.
We are now ready to state the main theorem regarding the irreducible represen-
tations of sl .3I C/, the theorem of the highest weight. The proof of the theorem is
found in Sect. 6.4.
Theorem 6.7. 1. Every irreducible representation  of sl.3I C/ is the direct sum
of its weight spaces.
2. Every irreducible representation of sl.3I C/ has a unique highest weight .

6.3
The Theorem of the Highest Weight
147
3. Two irreducible representations of sl.3I C/ with the same highest weight are
isomorphic.
4. The highest weight  of an irreducible representation must be of the form
 D .m1; m2/;
where m1 and m2 are non-negative integers.
5. For every pair .m1; m2/ of non-negative integers, there exists an irreducible
representation of sl.3I C/ with highest weight .m1; m2/.
We will also prove (without appealing to Theorem 5.6) a similar result for the
group SU.3/. Since every irreducible representation of SU.3/ gives rise to an
irreducible representation of sl.3I C/ Š su.3/C, the only nontrivial matter is to
prove Point 5 for SU.3/.
Theorem 6.8. For every pair .m1; m2/ of non-negative integers, there exists an
irreducible representation ... of SU.3/ such that the associated representation 
of sl.3I C/ has highest weight .m1; m2/.
One might naturally attempt to construct representations of SU.3/ by a method
similar to that used in Example 4.10, acting on spaces of homogeneous polynomials
on C3. This is, indeed, possible and the resulting representations of SU.3/ turn out
to be irreducible. Not every irreducible representation of SU.3/, however, arises in
this way, but only those with highest weight of the form .0; m/. See Exercise 8.
For  D .m1; m2/ 2 C2, we may say that  is an integral element if m1 and
m2 are integers and that  is dominant if m1 and m2 are real and non-negative.
Thus, the set of possible highest weights in Theorem 6.7 are the dominant integral
elements. Figure 6.1 shows the roots and dominant integral elements for sl.3I C/.
Fig. 6.1 The roots (arrows)
and dominant integral
elements (black dots), shown
in the obvious basis
a2
a1

148
6
The Representations of sl.3I C/
This picture is made using the obvious basis for the space of weights; that is, the
x-coordinate is the eigenvalue of H1 and the y-coordinate is the eigenvalue of H2.
Once we have introduced the Weyl group (Sect. 6.6), we will see the same picture
rendered using a Weyl-invariant inner product, which will give a more symmetric
view of the situation.
Note the parallels between this result and the classiﬁcation of the irreducible
representations of sl.2I C/: In each irreducible representation of sl.2I C/, .H/
is diagonalizable, and there is a largest eigenvalue of .H/. Two irreducible
representations of sl.2I C/ with the same largest eigenvalue are isomorphic. The
highest eigenvalue is always a non-negative integer and every non-negative integer
is the highest weight of some irreducible representation.
6.4
Proof of the Theorem
The proof consists of a series of propositions.
Proposition 6.9. In every irreducible representation .; V / of sl.3I C/, the opera-
tors .H1/ and .H2/ can be simultaneously diagonalized; that is, V is the direct
sum of its weight spaces.
Proof. Let W be the sum of the weight spaces in V . Equivalently, W is the space of
all vectors w 2 V such that w can be written as a linear combination of simultaneous
eigenvectors for .H1/ and .H2/. Since (Proposition 6.2)  always has at least one
weight, W ¤ f0g.
On the other hand, Lemma 6.5 tells us that if Z˛ is a root vector corresponding
to the root ˛, then .Z˛/ maps the weight space corresponding to  into the weight
space corresponding to  C ˛. Thus, W is invariant under the action of each of the
root vectors, X1; X2; X3; Y1; Y2, and Y3. Since W is certainly also invariant under the
action of H1 and H2, W is invariant under all of sl.3I C/. Thus, by irreducibility,
W D V . Finally, since, by Proposition A.17, weight vectors with distinct weights
are independent, V is actually the direct sum of its weight spaces.
ut
Deﬁnition 6.10. A representation .; V / of sl.3I C/ is said to be a highest weight
cyclic representation with weight  D .m1; m2/ if there exists v ¤ 0 in V such
that
1. v is a weight vector with weight ,
2. .Xj /v D 0, for j D 1; 2; 3,
3. the smallest invariant subspace of V containing v is all of V .
Proposition 6.11. Let .; V / be a highest weight cyclic representation of sl.3I C/
with weight . Then the following results hold.
1. The representation  has highest weight .
2. The weight space corresponding to the weight  is one dimensional.

6.4
Proof of the Theorem
149
Before turning to the proof of this proposition, let us record a simple lemma
that applies to arbitrary Lie algebras and which will be useful also in the setting of
general semisimple Lie algebras.
Lemma 6.12 (Reordering Lemma). Suppose that g is any Lie algebra and that
 is a representation of g. Suppose that X1; : : : ; Xm is an ordered basis for g as a
vector space. Then any expression of the form
.Xj1/.Xj2/    .XjN /;
(6.6)
can be expressed as a linear combination of terms of the form
.Xm/km.Xm1/km1    .X1/k1
(6.7)
where each kl is a non-negative integer and where k1 C k2 C    C km  N .
Proof. The idea is to use the commutation relations of g to re-order the factors
into the desired order, at the expense of generating terms with one fewer factors,
which then be handled by the same method. To be more formal, we use induction
on N . If N D 1, there is nothing to do: Any expression of the form .Xj/ is of
the form (6.7) with kj D 1 and all the other kl's equal to zero. Assume, then, that
the result holds for a product of at most N factors, and consider an expression of the
form (6.6) with N C 1 factors. By induction, we can assume that the last N factors
are in the desired form, giving an expression of the form
.Xj/.Xm/km.Xm1/km1    .X1/k1
with k1 C    C km D N .
We now move the factor of .Xj/ to the right one step at a time until it is in the
right spot. Each time we have .Xj/.Xk/ somewhere in the expression we use the
relation
.Xj/.Xk/ D .Xk/.Xj/ C .ŒXj ; Xk/
D .Xk/.Xj/ C
X
l
cjkl.Xl/;
where the constants cjkl are the structure constants for the basis fXjg (Deﬁnition
3.10). Each commutator term has at most at most N factors. Thus, we ultimately
obtain several terms with N factors, which can be handled by induction, and one
term with N factors that is of the desired form (once .Xj / ﬁnally gets to the right
spot).
ut
We now proceed with the proof of Proposition 6.11.

150
6
The Representations of sl.3I C/
Proof. Let v be as in the deﬁnition. Consider the subspace W of V spanned by
elements of the form
w D .Yj1/.Yj2/    .YjN /v
(6.8)
with each jl equal to 1, 2, or 3 and N  0. (If N D 0, then w D v.) We now claim
that W is invariant. We take as our basis for sl.3I C/ the elements X1, X2, X3, H1,
H2, Y1, Y2, and Y3, in that order. If we apply a basis element to w, the lemma tells
us that we can rewrite the resulting vector as a linear combination of terms in which
the .Xj/'s act ﬁrst, the .Hj /'s act second, and the .Yj/'s act last, and all of
these are applied to the vector v. Since v is annihilated by each .Xj/, any term
having a positive power of any Xj is simply zero. Since v is an eigenvector for each
.Hj /, any factors of .Hj / acting on v can be replaced by constants. That leaves
only factors of .Yj/ applied to v, which means that we have a linear combination
of vectors of the form (6.8). Thus, W is invariant and contains v, so W D V .
Now, Y1, Y2, and Y3 are root vectors with roots ˛1, ˛2, and ˛1  ˛2,
respectively. Thus, by Lemma 6.5, each element of the form (6.8) with N > 0
is a weight vector with weight lower than . Thus, the only weight vectors with
weight  are multiples of w.
ut
Proposition 6.13. Every irreducible representation of sl.3I C/ is a highest weight
cyclic representation, with a unique highest weight .
Proof. We have already shown that every irreducible representation  is the direct
sum of its weight spaces. Since the representation is ﬁnite dimensional, there can be
only ﬁnitely many weights, so there must be a maximal weight , that is, such that
there is no weight strictly higher than . Thus, for any nonzero weight vector v with
weight , we must have
.Xj/v D 0;
j D 1; 2; 3:
Since  is irreducible, the smallest invariant subspace containing v must be the
whole space; therefore, the representation is highest weight cyclic.
ut
Proposition 6.14. Suppose .; V / is a completely reducible representation of
sl.3I C/ that is also highest weight cyclic. Then  is irreducible.
As it turns out, every ﬁnite-dimensional representation of sl.3I C/ is completely
reducible. This claim can be veriﬁed analytically (by passing to the simply con-
nected group SU.3/ and using Theorem 4.28) or algebraically (as in Sect. 10.3). We
do not, however, require this result here, since we will only apply Proposition 6.14
to representations that are manifestly completely reducible.
Meanwhile, it is tempting to think that any representation with a cyclic vector
(that is, a vector satisfying Point 3 of Deﬁnition 6.10) must be irreducible, but this
is false. (What is true is that if every nonzero vector in a representation is cyclic,
then the representation is irreducible.) Thus, Proposition 6.14 relies on the special
form of the cyclic vector in Deﬁnition 6.10.

6.4
Proof of the Theorem
151
Proof. Let .; V / be a highest weight cyclic representation with highest weight 
and let v be a weight vector with weight . By assumption, V decomposes as a
direct sum of irreducible representations
V Š
M
j
Vj .
(6.9)
By Proposition 6.9, each of the Vj's is the direct sum of its weight spaces. Since
the weight  occurs in V , it must occur in some Vj (compare the last part of
Proposition A.17). But by Proposition 6.11, v is (up to a constant) the only vector in
V with weight . Thus, Vj is an invariant subspace containing v, which means that
Vj D V . There is, therefore, only one term in the sum (6.9), and V is irreducible. ut
Proposition 6.15. Two irreducible representations of sl.3I C/ with the same high-
est weight are isomorphic.
Proof. Suppose .; V / and .	; W / are irreducible representations with the same
highest weight  and let v and w be the highest weight vectors for V and W ,
respectively. Consider the representation V ˚ W and let U be smallest invariant
subspace of V ˚ W which contains the vector .v; w/. Then U is a highest weight
cyclic representation. Furthermore, since V ˚ W is, by deﬁnition, completely
reducible, it follows from Proposition 4.26 that U is completely reducible. Thus,
by Proposition 6.14, U is irreducible.
Consider now the two "projection" maps P1 and P2, mapping V ˚ W to V and
W , respectively, and given by
P1.v; w/ D vI
P2.v; w/ D w:
Since P1 and P2 are easily seen to be intertwining maps, their restrictions to
U  V ˚ W are also intertwining maps. Now, neither P1jU nor P2jU is the zero
map, since both are nonzero on .v; w/. Moreover, U , V , and W are all irreducible.
Therefore, by Schur's lemma, P1jU is an isomorphism of U with V and P2jU is an
isomorphism of U with W , showing that V Š U Š W .
ut
Proposition 6.16. If  is an irreducible representation of sl.3I C/ with highest
weight  D .m1; m2/, then m1 and m2 non-negative integers.
Proof. By Proposition 6.3, m1 and m2 are integers. If v is a weight vector
with weight , then .X1/v and .X2/v must be zero, or  would not be the
highest weight for . Thus, if we then apply Point 1 of Theorem 4.34 to the
restrictions of  to hH1; X1; Y1i and hH2; X2; Y2i, we conclude that m1 and m2
are non-negative.
ut
Proposition 6.17. If m1 and m2 are non-negative integers, then there exists an
irreducible representation of sl.3I C/ with highest weight  D .m1; m2/.

152
6
The Representations of sl.3I C/
Proof. Since the trivial representation is an irreducible representation with highest
weight .0; 0/, we need only construct representations with at least one of m1 and m2
positive.
First, we construct two irreducible representations, with highest weights .1; 0/
and .0; 1/, which we call the fundamental representations. The standard represen-
tation of sl.3I C/, acting on C3 in the obvious say, is easily seen to be irreducible. It
has weight vectors e1; e2, and e3, with corresponding weights .1; 0/; .1; 1/, and
.0; 1/, and with highest weight is .1; 0/. The dual of the standard representation,
given by
.Z/ D Ztr
(6.10)
for all Z 2 sl.3I C/, is also irreducible. It also has weight vectors e1, e2, and e3, with
corresponding weights .1; 0/, .1; 1/, and .0; 1/ and with highest weight .0; 1/.
Let .1; V1/ and .2; V2/ be the standard representation and its dual, respectively,
and let v1 D e1 and v2 D e3 be the respective highest weight vectors. Now, consider
the representation m1;m2 given by
.V1 ˝    ˝ V1/ ˝ .V2 ˝    ˝ V2/;
(6.11)
where V1 occurs m1 times and V2 occurs m2 times. The action of sl.3I C/ on this
space is given by the obvious extension of Deﬁnition 4.20 to multiple factors. It then
easy to check that the vector
vm1;m2 D v1 ˝ v1    ˝ v1 ˝ v2 ˝ v2    ˝ v2
is a weight vector with weight .m1; m2/ and that vm1;m2 is annihilated by
m1;m2.Xj/, j D 1; 2; 3.
Now let W be the smallest invariant subspace containing vm1;m2. Assuming
that m1;m2 is completely reducible, W will also be completely reducible and
Proposition 6.14 will tell us that W is the desired irreducible representation with
highest weight .m1; m2/.
It remains only to establish complete reducibility. Note ﬁrst that both the standard
representation and its dual are "unitary" for the action of su.3/, meaning that
.X/ D .X/ for all X 2 su.3/. Meanwhile, it is easy to verify (Exercise 5)
that if V and W are inner product spaces, then there is a unique inner product on
V ˝ W for which
hv1 ˝ w1; v2 ˝ w2i D hv1; v2i hw1; w2i
for all v1; v2 2 V and w1; w2 2 W . Extending this construction to tensor products
of several vector spaces, use the standard inner product on C3 to construct an inner
product on the space in (6.11). It is then easy to check that m1;m2 is also unitary for
the action of su.3/. Thus, by Proposition 4.27, m1;m2 is completely reducible under
the action of su.3/ and thus, also, under the action of sl.3I C/ Š su.3/C.
ut

6.5
An Example: Highest Weight .1; 1/
153
We have now completed the proof of Theorem 6.7.
Proof of Theorem 6.8. The standard representation 1 of sl.3I C/ comes from the
standard representation ...1 of SU.3/, and similarly for the dual of the standard
representation. By taking tensor products, we see that there is a representation
...m1;m2 corresponding to the representation m1;m2 of sl.3I C/. The irreducible
invariant subspace W in the proof of Proposition 6.17 is then also invariant under the
action of SU.3/, so that the restriction of ...m1;m2 to W is the desired representation
of SU.3/.
ut
6.5
An Example: Highest Weight .1; 1/
To obtain the irreducible representation with highest weight .1; 1/, we take the
tensor product of the standard representation and its dual, take the highest weight
vector in the tensor product, and then consider the space obtained by repeated
applications of the operators 1;1.Yj/, j D 1; 2; 3. Since, however, Y3 D ŒY1; Y2,
it sufﬁces to apply only 1;1.Y1/ and 1;1.Y2/.
Now, the standard representation has highest weight e1 and the action of the
operators .Y1/ D Y1 and .Y2/ D Y2 is given by
Y1e1 D e2
Y1e2 D 0
Y1e3 D 0
Y2e1 D 0
Y2e2 D e3
Y2e3 D 0 :
For the dual of the standard representation, let use the notation Z D Ztr, so that
.Z/ D Z. If we introduce the new basis
f1 D e3I
f2 D e2I
f3 D e1;
then the highest weight is f1 and we have
Y1f1 D 0
Y1f2 D f3
Y1f3 D 0
Y2f1 D f2
Y2f2 D 0
Y2f3 D 0 :
We must now repeatedly apply the operators
1;1.Y1/ D Y1 ˝ I C I ˝ Y1
1;1.Y2/ D Y2 ˝ I C I ˝ Y2
(6.12)
until we get zero. This calculation is contained in the following chart. Here, there are
two arrows coming out of each vector. Of these, the left arrow indicates the action
of Y1 ˝ I C I ˝ Y1 and the right arrow indicates the action of Y2 ˝ I C I ˝ Y2. To
save space, we omit the tensor product symbol, writing, for example, e2f2 instead
of e2 ˝ f2.

154
6
The Representations of sl.3I C/
e1f1
.
&
e2f1
e1f2
.
#
#
&
0
e3f1 C e2f2
e2f2 C e1f3
0
.
#
#
&
e2f3
2e3f2
2e2f3
e3f2
.
#
#
&
.
#
#
&
0
e3f3
2e3f3
0
2e3f3
e3f3
0
A basis for the space spanned by these vectors is e1f1, e2f1, e1f2, e3f1 C e2f2,
e2f2 C e1f3, e2f3, e3f2, and e3f3. Thus, the dimension of this representation is
8; it is (isomorphic to) the adjoint representation. Now, e1, e2, and e3 have weights
.1; 0/, .1; 1/, and .0; 1/, respectively, whereas f1, f2, and f3 have weights .0; 1/,
.1; 1/, and .1; 0/, respectively. From (6.12), we can see that the weight for ej˝fk
is just the sum of the weight for ej and the weight for fk. Thus, the weights for
the basis elements listed above are .1; 1/, .1; 2/, .2; 1/, .0; 0/ (twice), .1; 2/,
.2; 1/, and .1; 1/. Each weight has multiplicity 1 except for .0; 0/, which has
multiplicity 2. See the ﬁrst image in Figure 6.4.
6.6
The Weyl Group
This section describes an important symmetry of the representations of SU.3/,
involving something called the Weyl group. Our discussion follows the compact-
group approach to the Weyl group. See Sect. 7.4 for the Lie algebra approach, in the
context of general semisimple Lie algebras.
Deﬁnition 6.18. Let h be the two-dimensional subspace of sl.3I C/ spanned by H1
and H2. Let N be the subgroup of SU.3/ consisting of those A 2 SU.3/ such
that AdA.H/ is an element of h for all H in h. Let Z be the subgroup of SU.3/
consisting of those A 2 SU.3/ such that AdA.H/ D H for all H 2 h.
The space h is a Cartan subalgebra of sl.3I C/. It is a straightforward exercise
(Exercise 9) to verify that Z and N are subgroups of SU.3/ and that Z is a normal
subgroup of N . This leads us to the deﬁnition of the Weyl group.
Deﬁnition 6.19. The Weyl group of SU.3/, denoted W , is the quotient
group N=Z.
The primary signiﬁcance of W for the representation theory of SU.3/ is that
it gives rise to a symmetry of the weights occurring in a ﬁxed representation; see
Theorem 6.22. We can deﬁne an action of W on h as follows. For each element w
of W , choose an element A of the corresponding coset in N . Then for H in h we
deﬁne the action w  H of w on H by
w  H D AdA.H/:

6.6
The Weyl Group
155
To see that this action is well deﬁned, suppose B is an element of the same coset
as A. Then B D AC with C 2 Z and, thus,
AdB.H/ D AdA.AdC.H// D AdA.H/;
by the deﬁnition of Z. Note that by deﬁnition, if wH D H for all H 2 h, then w is
the identity element of W (that is, the associated A 2 N is actually in Z). Thus, we
may identify W with the group of linear transformations of h that can be expressed
in the form H 7! w  H for some w 2 W .
Proposition 6.20. The group Z consists precisely of the diagonal matrices inside
SU.3/, namely the diagonal matrices with diagonal entries .ei; ei; ei.C// with
;  2 R. The group N consists of precisely those matrices A 2 SU.3/ such that
for each j D 1; 2; 3, there exist kj 2 f1; 2; 3g and j 2 R such that Aej D eij ekj .
Here, e1;e2; e3 is the standard basis for C3.
The Weyl group W D N=Z is isomorphic to the permutation group on three
elements.
Proof. Suppose A is in Z, which means that A commutes with all elements of h,
including H1, which has eigenvectors e1, e2, and e3, with corresponding eigenvalues
1, 1, and 0. Since A commutes with H1, it must preserve each of these eigenspaces
(Proposition A.2). Thus, Aej must be a multiple of ej for each j, meaning that A is
diagonal. Conversely, any diagonal matrix in SU.3/ does indeed commute not only
with H1 but also with H2 and, thus, with every element of h.
Suppose, now, that A is in N . Then AH1A1 must be in h and therefore must
be diagonal, meaning that e1, e2, and e3 are eigenvectors for AH1A1, with the
same eigenvalues 1; 1; 0 as H1, but not necessarily in the same order. On the other
hand, the eigenvectors of AH1A1 must be Ae1, Ae2, and Ae3. Thus, Aej must be
a multiple of some ekj , and the constant must have absolute value 1 if A is unitary.
Conversely, if Aej is a multiple of ekj for each j, then for any (diagonal) matrix H
in h, the matrix AHA1 will again be diagonal and thus in h.
Finally, if A maps each ej to a multiple of ekj , for some kj depending on j,
then for each diagonal matrix H, the matrix AHA1 will be diagonal with diagonal
entries rearranged by the permutation j 7! kj . For any permutation, we can choose
the constants to that the map taking ej to eij ekj has determinant 1, showing that
every permutation actually arises in this way. Thus, W —which we think of as the
group of linear transformations of h of the form AdA, A 2 N —is isomorphic to the
permutation group on three elements.
ut
We want to show that the Weyl group is a symmetry of the weights of any ﬁnite-
dimensional representation of sl.3I C/. To understand this, we need to adopt a less
basis-dependent view of the weights. We have deﬁned a weight as a pair .m1; m2/
of simultaneous eigenvalues for .H1/ and .H2/. However, if a vector v is an
eigenvector for .H1/ and .H2/ then it is also an eigenvector for .H/ for any
element H of the space h spanned by H1 and H2, and the eigenvalues will depend
linearly on H in h. Thus, we may think of a weight not as a pair of numbers but as
a linear functional on h.

156
6
The Representations of sl.3I C/
It is then convenient to use an inner product on h to identity linear functionals on
h with elements of h itself. We deﬁne the inner product of H and H 0 in h by
˝
H; H 0˛
D trace.H H 0/;
(6.13)
or, explicitly,
hdiag.a; b; c/; diag.d; e; f /i D Nad C Nbe C Ncf;
where diag.; ; / is the diagonal matrix with the indicated diagonal entries. If  is
a linear functional on h, there is (Proposition A.11) a unique vector  in h such
that  may be represented as .H/ D h; Hi for all H 2 h. If we represent the
linear functional in the previous paragraph in this way, we arrive at a new, basis-
independent notion of a weight.
Deﬁnition 6.21. Let h be the subspace of sl.3I C/ spanned by H1 and H2 and let
.; V / be a representation of sl.3I C/. An element  of h is called a weight for  if
there exists a nonzero vector v in V such that
.H/v D h; Hi v
for all H in h. Such a vector v is called a weight vector with weight .
If  is a weight in our new sense, the ordered pair .m1; m2/ in Deﬁnition 6.1 is
given by
m1 D h; H1i I
m2 D h; H2i :
It is easy to check that for all U 2 N , the adjoint action of U on h preserves
the inner product in (6.13). Thus, the action of the Weyl group on h is unitary:
hw  H; w  H 0i D hH; H 0i. Since the roots are just the nonzero weights of the
adjoint representation, we now also think of the roots as elements of h:
Theorem 6.22. Suppose that ....; V / is a ﬁnite-dimensional representation of
SU.3/ with associated representation .; V / of sl.3I C/. If  2 h is a weight for
V then w   is also a weight of V with the same multiplicity. In particular, the roots
are invariant under the action of the Weyl group.
Proof. Suppose that  is a weight for V with weight vector v. Then for all U 2 N
and H 2 h, we have
.H/....U /v D ....U /.....U /1.H/....U //v
D ....U /.U 1HU/v
D
˝
; U 1HU
˛
....U /v:
Here, we have used that U is in N , which guarantees that U 1HU is, again, in h.
Thus, if w is the Weyl group element represented by U , we have

6.6
The Weyl Group
157
.H/....U /v D
˝
; w1  H
˛
....U /v D hw  ; Hi ....U /v:
We conclude that ....U /v is a weight vector with weight w  .
The same sort of reasoning shows that ....U / is an invertible map of the weight
space with weight  onto the weight space with weight w  , whose inverse is
....U /1. This means that the two weights have the same multiplicity.
ut
To represent the basic weights, .1; 0/ and .0; 1/, in our new approach, we look
for diagonal, trace-zero matrices 1 and 2 such that
h1; H1i D 1;
h1; H2i D 0
h2; H1i D 0;
h2; H2i D 1:
These are easily found as
1 D diag.2=3; 1=3; 1=3/I
2 D diag.1=3; 1=3; 2=3/:
The positive simple roots .2; 1/ and .1; 2/ are then represented as
˛1 D 21  2 D diag.1; 1; 0/I
˛2 D 1 C 22 D diag.0; 1; 1/:
(6.14)
Note that both ˛1 and ˛2 have length
p
2 and h˛1; ˛2i D 1. Thus, the angle 
between them satisﬁes cos  D 1=2, so that  D 2=3.
Figure 6.2 shows the same information as Figure 6.1, namely, the roots and
the dominant integral elements, but now drawn relative to the Weyl-invariant inner
product in (6.13). We draw only the two-dimensional real subspace of h consisting
Fig. 6.2 The roots and
dominant integral elements
for sl.3I C/, computed
relative to a Weyl-invariant
inner product
a2
a1
m1
m2

158
6
The Representations of sl.3I C/
Fig. 6.3 The Weyl group is
the symmetry group of the
indicated equilateral triangle
a2
a1
of those elements  such that h; H1i and h; H2i are real, since all the roots and
weights have this property. Let w.1;2;3/ denote the Weyl group element that acts by
cyclically permuting the diagonal entries of each H 2 h. Then w.1;2;3/ takes ˛1 to ˛2
and ˛2 to .˛1 C ˛2/, which is a counterclockwise rotation by 2=3 in Figure 6.2.
Similarly, if w.1;2/ the element that interchanges the ﬁrst two diagonal entries of
H 2 h, then w.1;2/ maps ˛1 to ˛1 and ˛2 to ˛1 C ˛2. Thus, w.1;2/ is the reﬂection
across the line perpendicular to ˛1. The reader is invited to compute the action of the
remaining elements of the Weyl group and to verify that it is the symmetry group of
the equilateral triangle in Figure 6.3.
We previously deﬁned a pair .m1; m2/ to be integral if m1 and m2 are integers and
dominant if m1  0 and m2  0. These concepts translate into our new language
as follows. If  2 h, then  is integral if h; H1i and h; H2i are integers and 
is dominant if h; H1i  0 and h; H2i  0. Geometrically, the set of dominant
elements is a sector spanning an angle of =3.
6.7
Weight Diagrams
In this section, we display the weights and multiplicities for several irreducible
representations of sl.3I C/. Figure 6.4 covers the irreducible representations with
highest weighs .1; 1/, .1; 2/, .0; 4/, and .2; 2/. The ﬁrst of these examples was
analyzed in Sect. 6.5, and the other examples can be analyzed by the same method.
In each part of the ﬁgure, the arrows indicate the roots, the two black lines indicate
the boundary of the set of dominant elements, and the dashed lines indicate the
boundary of the set of points lower than the highest weight. Each weight of

6.8
Further Properties of the Representations
159
2
2
2
2
2
2
2
2
2
2
3
Fig. 6.4 Weight diagrams for representations with highest weights .1; 1/, .1; 2/, .0; 4/, and .2; 2/
a particular representation is indicated by a black dot, with a number next to
a dot indicating its multiplicity. A dot without a number indicates a weight of
multiplicity 1.
Our last example is the representation with highest weight .9; 2/ (Figure 6.5),
which cannot feasibly be analyzed using the method of Sect. 6.5. Instead, the
weights are determined by the results of Sect. 6.8 and the multiplicities are computed
using the Kostant multiplicity formula. (See Figure 10.8 in Sect. 10.6.) See also
Exercises 11 and 12 for another approach to computing multiplicities.
6.8
Further Properties of the Representations
Although we now have a classiﬁcation of the irreducible representations of sl.3I C/
by means of their highest weights, there are other things we might like to know
about the representations, such as (1) the other weights that occur, besides the
highest weight, (2) the multiplicities of those weights, and (3) the dimension of the
representation. In this section, we establish which weights occur and state without
proof the formula for the dimension. A formula for the multiplicities and a proof of
the dimension formula are given in Chapter 10 in the setting of general semisimple
Lie algebras.

160
6
The Representations of sl.3I C/
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
Fig. 6.5 Weight diagram for the irreducible representation with highest weight .9; 2/
Deﬁnition 6.23. If v1; : : : ; vN are elements of a real or complex vector space, the
convex hull of v1; : : : ; vN is the set of all vectors of the form
c1v1 C c2v2 C    C cN vN
where the cj's are non-negative real numbers satisfying c1 C c2 C    C cN D 1.
Equivalently, the convex hull of v1; : : : ; vN is the smallest convex set that
contains all of the vj 's.
Theorem 6.24. Let  be a dominant integral element and let V be the irreducible
representation with highest weight . If  is a weight of V, then  satisﬁes the
following two conditions: (1)    can be expressed as an integer combination of
roots, and (2)  belongs to the convex hull of W  , the orbit of  under the action
of W .
Proof. According to the proof of Proposition 6.11, V is spanned by vectors of
the form in (6.8). These vectors are weight vectors with weights of the form  WD
  ˛j1      ˛jN . Thus, every weight of V satisﬁes the ﬁrst property in the
theorem.

6.8
Further Properties of the Representations
161
w
Fig. 6.6 The integral element  is outside the convex hull of the orbit of , and the element w  
is not lower than 
The second property in the theorem is based on the following idea: If  is a
weight of V, then w   is also a weight for all w 2 W , which means that w   is
lower than . We can now argue "pictorially" that if  were not in the convex hull
of W  , there would be some w 2 W for which w   is not lower than , so that 
could not be a weight of V. See Figure 6.6.
We can give a more formal argument as follows. For any weight  of V, we
can, by Exercise 10, ﬁnd some w 2 W so that 0 WD w   is dominant. Since 0
is also a weight of V, we must have 0  . Thus, 0 is in the quadrilateral Q
consisting of dominant elements that are lower than  (Figure 6.7). We now argue
that the vertices of Q are all in the convex hull. First, it is easy to see that for any
, the average of w  over all w 2 W is zero, which means that 0 is in E. Second,
the vertices marked v1 and v2 in the ﬁgure are expressible as follows:
v1 D 1
2 C 1
2s˛1  
v2 D 1
2 C 1
2s˛2  ;
where s˛1 and s˛2 are the Weyl group elements given by reﬂecting about the lines
orthogonal to ˛1 and ˛2. Thus, all the vertices of Q are in E, from which it follows
that Q itself is contained in E.
Now, W   is clearly W -invariant, which means that E is also W -invariant.
Since 0 2 Q  E, we have  D w10 2 E as well.
ut

162
6
The Representations of sl.3I C/
v1
v2
1
2
Fig. 6.7 The shaded quadrilateral is the set of all points that are dominant and lower than 
Theorem 6.25. Suppose V is an irreducible representation with highest weight 
and that  is an integral element satisfying the two conditions in Theorem 6.24.
Then  is a weight of V.
Theorem 6.25 says, in effect, that there are no unexpected holes in the set of
weights of V. The key to the proof is the "no holes" result (Point 4 of Theorem 4.34)
we previously established for sl.2I C/.
Lemma 6.26. Let  be a weight of V, let ˛ be a root, and let s˛ 2 W be the
reﬂection about the line orthogonal to ˛. Suppose  is a point on the line segment
joining  to s˛   with the property that    is an integer multiple of ˛. Then  is
also a weight of V.
See Figure 6.8 for an example. Note from Figure 6.3 that for each root ˛, the
reﬂection s˛ is an element of the Weyl group.
Proof. Since the reﬂections associated to ˛ and ˛ are the same, it sufﬁces to
consider the roots ˛1, ˛2, and ˛3 WD ˛1 C ˛2. If we let H3 D H1 C H2, then
for j D 1; 2; 3 we have a subalgebra sj D
˝
Xj; Yj; Hj
˛
isomorphic to sl.2I C/ such
that Xj is a root vector with root ˛j and Yj is a root vector with root ˛j . Since
ŒHj ; Xj D 2Xj D
˝
˛j; Hj
˛
Xj;
we have
˝
˛j; Hj
˛
D 2 for each j.
Let us now ﬁx a weight  of V and let U be the span of all the weight vectors in
V whose weights are of the form  C k˛j for some real number k. (These weights
are circled in Figure 6.8.) Since, by Lemma 6.5, .Xj / and .Yj / shift weights

6.8
Further Properties of the Representations
163
s
Fig. 6.8 Since  is a weight of V, each of the elements   ˛;   2˛; : : : ; s˛   must also be a
weight of V
by ˙˛j , we see that U is invariant under sj and thus constitutes a representation
of sj (not necessarily irreducible). With our new perspective that roots are elements
of h, we can verify from (6.14) that for each j, we have ˛j D Hj , from which
it follows that s˛j  Hj D Hj . Thus, if u and v are weight vectors with weights
 and s˛  , respectively, u and v are in U and are eigenvectors for .Hj / with
eigenvalues
˝
; Hj
˛
and
˝
s˛  ; Hj
˛
D
˝
; s˛  Hj
˛
D 
˝
; Hj
˛
;
respectively.
If  is on the line segment joining  to s˛, we see that
˝
; Hj
˛
is between
˝
; Hj
˛
and
˝
s˛  ; Hj
˛
D 
˝
; Hj
˛
. If, in addition,  differs from  by an integer multiple
of ˛j , then
˝
; Hj
˛
differs from
˝
; Hj
˛
by an integer multiple of
˝
˛j ; Hj
˛
D 2.
Thus, by applying Point 4 of Theorem 4.34 to the action of sj on U , there must be
an eigenvector w for .Hj / in U with eigenvalue l D
˝
; Hj
˛
. Since the unique
weight of the form  C k˛j for which
˝
 C k˛j ; Hj
˛
D
˝
; Hj
˛
is the one where
 C k˛j D , we conclude that  is a weight of V.
ut
Proof of Theorem 6.25. Suppose that  satisﬁes the two conditions in the theorem,
and write  D   n1˛1  n2˛2. Consider ﬁrst the case n1  n2, so that
 D   .n1  n2/˛1  n2.˛1 C ˛2/
D   .n1  n2/˛1  n2˛3;

164
6
The Representations of sl.3I C/
1
2
3
s
3
s
1
Fig. 6.9 By applying Lemma 6.26 twice, we can see that  and  must be weights of V
where ˛3 D ˛1 C ˛2. If we start at  and travel in the direction of ˛3, we will hit the
boundary of E at the point
 WD   .n1  n2/˛1:
(See Figure 6.9.) Thus,  is in E and must therefore be between  and s˛1 . Since
also  differs from  by an integer multiple of ˛1 (namely n1  n2) Lemma 6.26
says that  is a weight of V . Meanwhile,  is between  and s˛3   (see, again,
Figure 6.9) and differs from  by an integer multiple of ˛3 (namely n2). Thus, the
lemma tells us that  must be a weight of V , as claimed. If n1  n2, we can use a
similar argument with the roles of ˛1 and ˛2 reversed.
ut
We close this section by stating the formula for the dimension of an irreducible
representation of sl.3I C/. We will prove the result in Chapter 10 as a special case
of the Weyl dimension formula.
Theorem 6.27. The dimension of the irreducible representation with highest
weight .m1; m2/ is
1
2.m1 C 1/.m2 C 1/.m1 C m2 C 2/.
The reader is invited to verify this formula by direct computation in the
representations depicted in Figure 6.4.

6.9
Exercises
165
6.9
Exercises
1. Show that the roots listed in (6.3) are the only roots.
2. Let  be an irreducible ﬁnite-dimensional representation of sl.3I C/ acting on a
space V and let  be the dual representation to , acting on V , as deﬁned in
Sect. 4.3.3. Show that the weights of  are the negatives of the weights of .
Hint: Choose a basis for V in which both .H1/ and .H2/ are diagonal.
3. Let  be an irreducible representation of sl.3I C/ with highest weight .
(a) Let ˛3 D ˛1 C ˛2 and let s˛3 denote the reﬂection about the line orthogonal
to ˛3. Show the lowest weight for  is s˛3  .
(b) Show that the highest weight for the dual representation  to  is the
weight
0 WD s˛3  :
(c) Let 1 and 2 be the fundamental weights, as in Figure 6.2. If  D
m11 Cm22, show that 0 D m21 Cm12. That is to say, the dual to the
representation with highest weight .m1; m2/ has highest weight .m2; m1/.
4. Consider the adjoint representation of sl.3I C/ as a representation of sl.2I C/
by restricting the adjoint representation to the subalgebra spanned by X1; Y1,
and H1. Decompose this representation as a direct sum of irreducible represen-
tations of sl.2I C/. Which representations occur and with what multiplicity?
5. Suppose that V and W are ﬁnite-dimensional inner product spaces over C.
Show that there exists a unique inner product on V ˝ W such that
˝
v ˝ w; v0 ˝ w0˛
D
˝
v; v0˛ ˝
w; w0˛
for all v; v0 2 V and w; w0 2 W .
Hint: Let fejg and ffkg be orthonormal bases for V and W , respectively. Take
the inner product on V ˝ W for which fej ˝ fkg is an orthonormal basis.
6. Following the method of Sect. 6.5, work out the representation of sl.3I C/ with
highest weight .2; 0/, acting on a subspace of C3 ˝ C3. Determine all the
weights of this representation and their multiplicity (i.e., the dimension of the
corresponding weight space). Verify that the dimension formula (Theorem 6.27)
holds in this case.
7. Consider the nine-dimensional representation of sl.3I C/ considered in
Sect. 6.5, namely the tensor product of the representations with highest
weights .1; 0/ and .0; 1/. Decompose this representation as a direct sum of
irreducibles. Do the same for the tensor product of two copies of the irreducible
representation with highest weight .1; 0/. (Compare Exercise 6.)

166
6
The Representations of sl.3I C/
8. Let Wm denote the space of homogeneous polynomials on C3 of degree m. Let
SU.3/ act on Wm by the obvious generalization of the action in Example 4.10.
(a) Show that the associated representation of sl.3I C/ contains a highest
weight cyclic representation with highest weight .0; m/ and highest weight
vector zm
3 .
(b) By imitating the proof of Proposition 4.11, show that any nonzero invariant
subspace of Wm must contain zm
3 .
(c) Conclude that Wm is irreducible with highest weight .0; m/.
9. Show that Z and N (deﬁned in Deﬁnition 6.18) are subgroups of SU.3/. Show
that Z is a normal subgroup of N .
10. Suppose  is an integral element, that is, one of the triangular lattice points in
Figure 6.2. Show that there is an element w of the Weyl group such that w   is
dominant integral, that is, one of the black dots in Figure 6.2.
Hint: Recall that the Weyl group is the symmetry group of the triangle in
Figure 6.3.
(a) Regard the Weyl group as a group of linear transformations of h. Show that
I is not an element of the Weyl group.
(b) Which irreducible representations of sl.3I C/ have the property that their
weights are invariant under I?
11. Suppose .; V / is an irreducible representation of sl.3I C/ with highest weight
 and highest weight vector v0. Show that the weight space with weight
  ˛1  ˛2 has multiplicity at most 2 and is spanned by the vectors
.Y1/.Y2/v0;
.Y2/.Y1/v0:
12. Let .; V / be the irreducible representation with highest weight .m1; m2/. As in
the proof of Proposition 6.17, choose an inner product on V such that .X/ D
.X/ for all X 2 su.3/  sl.3I C/. Let v0 be a highest weight vector for V ,
normalized to be a unit vector, and deﬁne vectors u1 and u2 in V as
u1 D .Y1/.Y2/v0I
u2 D .Y2/.Y1/v0:
Each of these vectors is either zero or a weight vector with weight ˛1˛2.
(a) Using and the commutation relations among the basis elements of sl.3I C/,
show that
hu1; u1i D m2.m1 C 1/
hu2; u2i D m1.m2 C 1/
hu1; u2i D m1m2:
Hint: Show that .Xj/ D .Yj/ for j D 1; 2; 3.

6.9
Exercises
167
(b) Show that if m1  1 and m2  1 then u1 and u2 are linearly independent.
Conclude that the weight   ˛1  ˛2 has multiplicity 2.
(c) Show that if m1 D 0 and m2  1 or m1  1 and m2 D 0, then the weight
  ˛1  ˛2 has multiplicity 1.
Note: The reader may verify the results of this exercise in the representations
depicted in Figure 6.4.

Chapter 7
Semisimple Lie Algebras
In this chapter we introduce a class of Lie algebras, the semisimple algebras, for
which we can classify the irreducible representations using a strategy similar to
the one we used for sl.3I C/. In this chapter, we develop the relevant structures
of semisimple Lie algebras. In Chapter 8, we look into the properties of the set
of roots. Then in Chapter 9, we construct and classify the irreducible, ﬁnite-
dimensional representations of semisimple Lie algebras. Finally, in Chapter 10,
we consider several additional properties of the representations constructed in
Chapter 9. Meanwhile, in Chapters 11 and 12, we consider representation theory
from the closely related viewpoint of compact Lie groups.
7.1
Semisimple and Reductive Lie Algebras
We begin by deﬁning the term semisimple. There are many equivalent characteriza-
tions of semisimple Lie algebras. It is not, however, always easy to prove that two
of these various characterizations are equivalent. We will use an atypical deﬁnition,
which allows for a rapid development of the structure of semisimple Lie algebras.
Recall from Sect. 3.6 the notion of the complexiﬁcation of a real Lie algebra.
Deﬁnition 7.1. A complex Lie algebra g is reductive if there exists a compact
matrix Lie group K such that
g Š kC:
A complex Lie algebra g is semisimple if it is reductive and the center of g is trivial.
Deﬁnition 7.2. If g is a semisimple Lie algebra, a real subalgebra k of g is a
compact real form of g if k is isomorphic to the Lie algebra of some compact matrix
Lie group and every element Z of g can be expressed uniquely as Z D X C iY,
with X; Y 2 k.
© Springer International Publishing Switzerland 2015
B. Hall, Lie Groups, Lie Algebras, and Representations, Graduate
Texts in Mathematics 222, DOI 10.1007/978-3-319-13467-3_7
169

170
7
Semisimple Lie Algebras
On the one hand, using Deﬁnition 7.1 gives an easy method of constructing
Cartan subalgebras and ﬁts naturally with our study of compact Lie groups in
Part III. On the other hand, this deﬁnition covers an apparently smaller class of
Lie algebras than some of the more standard deﬁnitions. That is to say, we will
prove (Theorem 7.8 and Exercise 6) that the condition in Deﬁnition 7.1 implies
two of the standard deﬁnitions of "semisimple," but we will not prove the reverse
implications. These reverse implications are, in fact, true, so that our deﬁnition of
semisimplicity is ultimately equivalent to any other deﬁnition. But it is not possible
to prove the reverse implications without giving up the gains in efﬁciency that go
with Deﬁnition 7.1. The reader who wishes to see a development of the theory
starting from a more traditional deﬁnition of semisimplicity may consult Chapter
II (along with the ﬁrst several sections of Chapter I) of [Kna2].
The only time we use the compact group in Deﬁnition 7.1 is to construct the inner
product in Proposition 7.4. In the standard treatment of semisimple Lie algebras, the
Killing form (Exercise 6) is used in place of this inner product. Our use of an inner
product in place of the bilinear Killing form substantially simpliﬁes some of the
arguments. Notably, in our construction of Cartan subalgebras (Proposition 7.11),
we use that a skew self-adjoint operator is always diagonalizable. By contrast,
an operator that is skew symmetric with respect to a nondegenerate bilinear form
need not be diagonalizable. Thus, the construction of Cartan subalgebras in the
conventional approach is substantially more involved than in our approach.
For a complex semisimple Lie algebra g, we will always assume we have chosen
a compact real form k of g, so that g D kC.
Example 7.3. The following Lie algebras are semisimple:
sl.nI C/;
n  2
so.nI C/;
n  3
sp.nI C/;
n  1:
The Lie algebras gl.nI C/ and so.2I C/ are reductive but not semisimple.
Proof. It is easy to see that the listed Lie algebras are reductive, with the correspond-
ing compact groups K being SU.n/, SO.n/, Sp.n/, U.n/, and SO.2/, respectively.
(Compare (3.17) in Sect. 3.6.) The Lie algebra gl.nI C/ has a nontrivial center,
consisting of scalar multiples of the identity, while the Lie algebra so.2I C/ is
commutative. It remains only to show that the centers of sl.nI C/, so.nI C/, and
sp.nI C/ are trivial for the indicated values of n.
Consider ﬁrst the case of sl.nI C/ and let X be an element of the center of
sl.nI C/. For any 1  j; k  n, let Ejk be the matrix with a 1 in the .j; k/ spot
and zeros elsewhere. Consider the matrix Hjk 2 sl.nI C/ given by
Hjk D Ejj  Ekk;

7.1
Semisimple and Reductive Lie Algebras
171
for j < k. Then we may easily calculate that
0 D ŒHjk; X D 2XjkEjk  2XkjEkj:
Since Ejk and Ekj are linearly independent for j
<
k, we conclude that
Xjk D Xkj D 0. Since this holds for all j < k, we see that X must be diagonal.
Once X is known to be diagonal, we may compute that for j ¤ k,
0 D ŒX; Ejk D .Xjj  Xkk/Ejk:
Thus, all the diagonal entries of X must be equal. But since, also, trace.X/ D 0, we
conclude that X must be zero.
For the remaining semisimple Lie algebras in Example 7.3, the calculations in
Sect. 7.7 will allow us to carry out a similar argument. It is proved there that the
center of so.2nI C/ is trivial for n  2, and a similar analysis shows that the centers
of so.2n C 1I C/ and sp.nI C/ are also trivial.
ut
Proposition 7.4. Let g WD kC be a reductive Lie algebra. Then there exists an inner
product on g that is real valued on k and such that the adjoint action of k on g is
"unitary," meaning that
hadX.Y /; Zi D  hY; adX.Z/i
(7.1)
for all X 2 k and all X; Y 2 g. If we deﬁne a operation X 7! X on g by the
formula
.X1 C iX2/ D X1 C iX2
(7.2)
for X1; X2 2 k, then any inner product satisfying (7.1) also satisﬁes
hadX.Y /; Zi D hY; adX.Z/i
(7.3)
for all X, Y , and Z in g.
The motivation for the deﬁnition of X is that g D gl.nI C/ and k D u.n/, then
X is the usual matrix adjoint of X.
Proof. By the proof of Theorem 4.28, there is a (real-valued) inner product on k
that is invariant under the adjoint action of K. This inner product then extends to a
complex inner product on g D kC for which the adjoint action of K is unitary. Thus,
by Proposition 4.8, the adjoint action of k on g is unitary in the sense of (7.1). It is
then a simple matter to check the relation (7.3).
ut
Recall from Deﬁnition 3.9 what it means for a Lie algebra to decompose as a Lie
algebra direct sum of subalgebras.

172
7
Semisimple Lie Algebras
Proposition 7.5. Suppose g D kC is a reductive Lie algebra. Choose an inner
product on g as in Proposition 7.4. Then if h is an ideal in g, the orthogonal
complement of h is also an ideal, and g decomposes as the Lie algebra direct sum
of h and h?.
Proof. An ideal in g is nothing but an invariant subspace for the adjoint action of g
on itself. If h is a complex subspace of g and h is invariant under the adjoint action
of k, it will also be invariant under adjoint action of g D kC. Thus, if h is an ideal
in g, then by Proposition 4.27, h? is invariant under the adjoint action of k and is,
therefore, also an ideal.
Now, g decomposes as a vector space as g D h ˚ h?. But since both h and h?
are ideals, for all X 2 h and Y 2 h?, we have
ŒX; Y  2 h \ h? D f0g:
Thus, g is actually the Lie algebra direct sum of h and h?.
ut
Proposition 7.6. Every reductive Lie algebra g over C decomposes as a Lie
algebra direct sum g D g1˚z, where z is the center of g and where g1 is semisimple.
Proof. Since z is an ideal in g, Proposition 7.5 shows that g1 WD z? is also an ideal
in g and that g decomposes as a Lie algebra direct sum g D g1˚z. It remains only to
show that g1 is semisimple. It is apparent that g1 has trivial center, since any central
element Z of g1 would also be central in g D g1 ˚ z, in which case, Z must be in
z \ g1 D f0g. Thus, we must only construct a compact real form of g1.
It is easy to see that Z 2 g belongs to z if and only if Z commutes with every
element of k. From this, it is easily seen that z is invariant under "conjugation,"
that is, under the map X C iY 7! X  iY, where X; Y 2 k. Since z is invariant
under conjugation, its orthogonal complement g1 is also closed under conjugation. It
follows that z is the complexiﬁcation of z0 WD z\k and that g1 is the complexiﬁcation
of k1 WD g1 \ k.
We will now show that k1 is a compact real form of g1. Let K0 be the adjoint group
of K, that is, the image in GL.k/ of the adjoint map. Since K is compact and the
adjoint map is continuous, K0 is compact and thus closed. Now, by Proposition 3.34,
the Lie algebra map associated to the map A 7! AdA is the map X 7! adX, the
kernel of which is the center z0 of k. Thus, the image of ad is isomorphic to k=z Š k1,
showing that k1 is isomorphic to the Lie algebra of the image of Ad, namely K0. ut
Proposition 7.7. If K is a simply connected compact matrix Lie group with Lie
algebra k, then g WD kC is semisimple.
Proof. As in the proof of Proposition 7.6, k decomposes as a Lie algebra direct sum
k D k1 ˚ z0, where z0 is the center of k and where g1 WD .k1/C is semisimple. Then
by Theorem 5.11, K decomposes as a direct product of closed, simply connected
subgroups K1 and Z0. Now, since z0 is commutative, it is isomorphic to the Lie
algebra of the commutative Lie group Rn, for some n. Since both Z0 and Rn are
simply connected, Corollary 5.7 tells us that Z0 Š Rn. On the other hand, since Z0

7.1
Semisimple and Reductive Lie Algebras
173
is a closed subgroup of K, we see that Z0 Š Rn is compact, which is possible only
if n D 0. Thus, z0 and z D z0 C iz0 are zero dimensional, meaning that g D g1 is
semisimple.
ut
Recall (Deﬁnition 3.11) that a Lie algebra g is said to be simple if g has no
nontrivial ideals and dim g is at least 2. That is to say, a one-dimensional Lie algebra
is, by decree, not simple, even though it clearly has no nontrivial ideals.
Theorem 7.8. Suppose that g is semisimple in the sense of Deﬁnition 7.1. Then g
decomposes as a Lie algebra direct sum
g D
m
M
j D1
gj ;
(7.4)
where each gj  g is a simple Lie algebra.
We will see in Sect. 7.6 that most of our examples of semisimple Lie algebras are
actually simple, meaning that there is only one term in the decomposition in (7.4).
The converse of Theorem 7.8 is also true; if a complex Lie algebra g decomposes as
a direct sum of simple algebras, then g is semisimple in the sense of Deﬁnition 7.1.
(See Theorem 6.11 in [Kna2].)
Proof. If g has a nontrivial ideal h, then by Proposition 7.5, g decomposes as the
Lie algebra direct sum g D h˚h?, where h? is also an ideal in g. Suppose that, say,
h has a nontrivial ideal h0. Since every element of h commutes with every element
of h?, we see that h0 is actually an ideal in g. Thus, h00 WD .h0/? \ h is an ideal in g
and we can decompose g as h0 ˚ h00 ˚ h?. Proceeding on in the same way, we can
decompose g as a direct sum of ideals gj, where each gj has no nontrivial ideals.
It remains only to show that each gj has dimension at least 2. Suppose, toward a
contradiction, that dim gj D 1 for some j. Then gj is necessarily commutative,
which means (since elements of gj commute with elements of gk for j ¤ k) that
gj is contained in the center Z.g/ of g, contradicting the assumption that the center
of g is trivial.
ut
Proposition 7.9. If g is a complex semisimple Lie algebra, then the subalgebras gj
appearing in the decomposition (7.8) are unique up to order.
To be more precise, suppose g is isomorphic to a Lie algebra direct sum of simple
subalgebras g1; : : : ; gl and also to a Lie algebra direct sum of simple subalgebras
h1; : : : ; hm. Then the proposition asserts that each hj is actually equal to (not just
isomorphic to) gk for some k. The proposition depends crucially on the fact that the
summands gj in (7.8) are simple (hence dim gj  2) and not just that gj has no
nontrivial ideals. Indeed, if g is a two-dimensional commutative Lie algebra, then
g can be decomposed as a direct sum of one-dimensional commutative algebras in
many different ways.

174
7
Semisimple Lie Algebras
Proof. If h is an ideal in g, then h can be viewed as a representation of g by
means of the map X 7! .adX/jh. If we decompose g as the direct sum of simple
subalgebra g1; : : : ; gm, then each gj is irreducible as a representation of g, since any
invariant subspace would be an ideal in g and thus an ideal in gj. Furthermore, these
representations are nonisomorphic, since action of gj on gj is nontrivial (since gj
is not commutative), whereas the action of each gk, k ¤ j, on gj is trivial. Suppose
now that h is an ideal in g that is simple as a Lie algebra and, thus, irreducible
under the adjoint action of g. Now, for any j, the projection map j W g ! gj is
an intertwining map of representations of g. Thus, for each j, the restriction of j
to h must be either 0 or an isomorphism. There must be some j for which j
ˇˇ
h is
nonzero and hence an isomorphism. But since the various gj's are nonisomorphic
representations of g, we must have kjh D 0 for all k ¤ j. Thus, actually,
h D gj.
ut
Before getting into the details of semisimple Lie algebras, let us brieﬂy outline
what our strategy will be in classifying their representations and what structures
we will need to carry out this strategy. We will look for commuting elements
H1; : : : ; Hr in our Lie algebra that we will try to simultaneously diagonalize
in each representation. We should ﬁnd as many such elements as possible, and
if they are going to be simultaneously diagonalizable in every representation,
they must certainly be diagonalizable in the adjoint representation. This leads
(in basis-independent language) to the deﬁnition of a Cartan subalgebra. The
nonzero sets of simultaneous eigenvalues for adH1; : : : ; adHr are called roots and
the corresponding simultaneous eigenvectors are called root vectors. The root
vectors will serve to raise and lower the eigenvalues of .H1/; : : : ; .Hr/ in
each representation . We will also have the Weyl group, which is an important
symmetry of the roots and also of the weights in each representation.
One crucial part of the structure of semisimple Lie algebras is the existence of
certain special subalgebras isomorphic to sl.2I C/. Several times over the course of
this chapter and the subsequent ones, we will make use of our knowledge of the
representations of sl.2I C/, applied to these subalgebras. If X, Y , and H are the
usual basis elements for sl.2I C/, then of particular importance is the fact that in any
ﬁnite-dimensional representation  of sl.2I C/ (not necessarily irreducible), every
eigenvalue of .H/ must be an integer. (Compare Point 1 of Theorem 4.34 and
Exercise 13 in Chapter 4.)
7.2
Cartan Subalgebras
Our ﬁrst task is to identify certain special sorts of commutative subalgebras, called
Cartan subalgebras.
Deﬁnition 7.10. If g is a complex semisimple Lie algebra, then a Cartan sub-
algebra of g is a complex subspace h of g with the following properties:

7.2
Cartan Subalgebras
175
1. For all H1 and H2 in h, ŒH1; H2 D 0.
2. If, for some X 2 g, we have ŒH; X D 0 for all H in h, then X is in h.
3. For all H in h, adH is diagonalizable.
Condition 1 says that h is a commutative subalgebra of g. Condition 2 says that h
is a maximal commutative subalgebra (i.e., not contained in any larger commutative
subalgebra). Condition 3 says that each adH (H 2 h) is diagonalizable. Since
the H's in h commute, the adH's also commute, and thus they are simultaneously
diagonalizable (Proposition A.16).
Of course, the deﬁnition of a Cartan subalgebra makes sense in any Lie algebra,
semisimple or not. However, if g is not semisimple, then g may not have any Cartan
subalgebras in the sense of Deﬁnition 7.10; see Exercise 1. (Sometimes a different
deﬁnition of "Cartan subalgebra" is used, one that allows every complex Lie algebra
to have a Cartan subalgebra. This other deﬁnition is equivalent to Deﬁnition 7.10
when g is semisimple but not in general.) Even in the semisimple case we must
prove that a Cartan subalgebra exists.
Proposition 7.11. Let g D kC be a complex semisimple Lie algebra and let t be any
maximal commutative subalgebra of k. Deﬁne h  g by
h D tC D t C it:
Then h is a Cartan subalgebra of g:
Note that k (or any other ﬁnite-dimensional Lie algebra) contains a maximal
commutative subalgebra. After all, any one-dimensional subalgebra t1 of k is com-
mutative. If t1 is maximal, then we are done; if not, then we choose some
commutative subalgebra t2 properly containing t1, and so on.
Proof of Proposition 7.11. It is clear that h is a commutative subalgebra of g. We
must ﬁrst show that h is maximal commutative. So, suppose that X 2 g commutes
with every element of h, which certainly means that it commutes with every element
of t. If we write X D X1 C iX2 with X1 and X2 in k, then for H in t, we have
ŒH; X1 C iX2 D ŒH; X1 C iŒH; X2 D 0;
where ŒH; X1 and ŒH; X2 are in k. However, since every element of g has a unique
decomposition as an element of k plus an element of ik, we see that ŒH; X1 and
ŒH; X2 must separately be zero. Since this holds for all H in t and since t is maximal
commutative, we must have X1 and X2 in t, which means that X D X1 C iX2 is
in h. This shows that h is maximal commutative.
If h; i is an inner product on g as in Proposition 7.4, then for each X in k, the
operator adX W g ! g is skew self-adjoint. In particular, each adH, H 2 t, is skew
self-adjoint and thus diagonalizable (Theorem A.3). Finally, if H is any element of
h D t C it, then H D H1 C iH2, with H1 and H2 in t. Since H1 and H2 commute,
adH1 and adH2 also commute and, thus, by Proposition A.16, adH1 and adH2 are
simultaneously diagonalizable. It follows that adH is diagonalizable.
ut

176
7
Semisimple Lie Algebras
Throughout this chapter and the subsequent chapters, we consider only Cartan
subalgebras of the form h D tC, where t is a maximal commutative subalgebra
of some compact real form k of g. It is true, but not obvious, that every Cartan
subalgebra arises in this way. Indeed, for any two Cartan subalgebras h1 and h2 of
g, there exists an automorphism of g mapping h1 to h2. (See Proposition 2.13 and
Theorem 2.15 in Chapter II of [Kna2].) While we will not prove this result, we will
prove in Chapter 11 that for a ﬁxed k, the maximal commutative subalgebra t  k
is unique up to an automorphism of k. (See Proposition 11.7 and Theorem 11.9.)
In light of the uniqueness of Cartan subalgebras up to automorphism, the following
deﬁnition is meaningful.
Deﬁnition 7.12. If g is a complex semisimple Lie algebra, the rank of g is the
dimension of any Cartan subalgebra.
7.3
Roots and Root Spaces
For the rest of this chapter, we assume that we have chosen a compact real form
k of g and a maximal commutative subalgebra t of k, and we consider the Cartan
subalgebra h D t C it. We assume also that we have chosen an inner product on g
that is real on k and invariant under the adjoint action of K (Proposition 7.4).
Since the operators adH, H 2 h, commute, and each such adH is diagonalizable,
Proposition A.16 tell us that the adH's with H 2 h are simultaneously diagonal-
izable. If X 2 g is a simultaneous eigenvalue for each adH, H 2 h, then the
corresponding eigenvalues depend linearly on H 2 h. If this linear functional is
nonzero, we call it a root. As in Sect. 6.6, it is convenient to express this linear
functional as H 7! h˛; Hi for some ˛ 2 h. The preceding discussion leads to the
following deﬁnition.
Deﬁnition 7.13. A nonzero element ˛ of h is a root (for g relative to h) if there
exists a nonzero X 2 g such that
ŒH; X D h˛; Hi X
for all H in h. The set of all roots is denoted R.
Note that we follow the convention that an inner product be linear in the second
factor, so that h˛; Hi depends linearly on H for a ﬁxed ˛.
Proposition 7.14. Each root ˛ belongs to it  h.
Proof. As we have already noted, each adH, H 2 t, is a skew self-adjoint operator
on h, which means that adH has pure imaginary eigenvalues. It follows that if ˛ is
a root, h˛; Hi must be pure imaginary for H 2 t, which (since our inner product is
real on t  k) can only happen if ˛ is in it.
ut

7.3
Roots and Root Spaces
177
Deﬁnition 7.15. If ˛ is a root, then the root space g˛ is the space of all X in g for
which ŒH; X D h˛; Hi X for all H in h: A nonzero element of g˛ is called a root
vector for ˛.
More generally, if ˛ is any element of h, we deﬁne g˛ to be the space of all X in
g for which ŒH; X D h˛; Hi X for all H in h (but we do not call g˛ a root space
unless ˛ is actually a root).
Taking ˛ D 0, we see that g0 is the set of all elements of g that commute with
every element of h. Since h is a maximal commutative subalgebra, we conclude that
g0 D h. If ˛ is not zero and not a root, then g˛ D f0g.
As we have noted, the operators adH, H 2 h, are simultaneously diagonalizable.
As a result, g can be decomposed as the sum of h and the root spaces g˛. Actually,
by Proposition A.17, the sum is direct and we have established the following result.
Proposition 7.16. The Lie algebra g can be decomposed as a direct sum of vector
spaces as follows:
g D h ˚
M
˛2R
g˛:
That is to say, every element of g can be written uniquely as a sum of an element
of h and one element from each root space g˛. Note that the decomposition is
not a Lie algebra direct sum, since, for example, elements of h do not, in general,
commute with elements of g˛.
Proposition 7.17. For any ˛ and ˇ in h, we have
Œg˛; gˇ  g˛Cˇ:
In particular, if X is in g˛ and Y is in g˛, then ŒX; Y  is in h. Furthermore, if X
is in g˛, Y is in gˇ, and ˛ C ˇ is neither zero nor a root, then ŒX; Y  D 0.
Proof. It follows from the Jacobi identity that adH is a derivation:
ŒH; ŒX; Y  D ŒŒH; X; Y  C ŒX; ŒH; Y :
Thus, if X is in g˛ and Y is in gˇ, we have
ŒH; ŒX; Y  D Œh˛; Hi X; Y  C ŒX; hˇ; Hi Y 
D h˛ C ˇ; Hi ŒX; Y :
for all H 2 h, showing that ŒX; Y  is in g˛Cˇ.
ut
Proposition 7.18. 1. If ˛ 2 h is a root, so is ˛. Speciﬁcally, if X is in g˛, then
X is in g˛, where X is deﬁned by (7.2) in Proposition 7.4.
2. The roots span h.

178
7
Semisimple Lie Algebras
Proof. For Point 1, if X D X1 C iX2 with X1; X2 2 k, let
NX D X1  iX2:
(7.5)
Since k is closed under brackets, if H 2 t  k and X 2 g, we have
ŒH; X D ŒH; X1  iŒH; X2 D ŒH; NX:
In particular, if X is a root vector with root ˛ 2 it, then for all H 2 h, we have
ŒH; NX D ŒH; X D h˛; Hi X D  h˛; Hi NX;
(7.6)
since h˛; Hi is pure imaginary for H 2 t. Extending (7.6) by linearity in H, we see
that ŒH; NX D  h˛; Hi NX for all H 2 h. Thus, NX is a root vector corresponding to
the root ˛. It follows that X D  NX belongs to g˛.
For Point 2, suppose that the roots did not span h. Then there would exist a
nonzero H 2 h such that h˛; Hi D 0 for all ˛ 2 R. Then we would have
ŒH; H 0 D 0 for all H 0 in h, and also
ŒH; X D h˛; Hi X D 0
for X in g˛. Thus, by Proposition 7.16, H would be in the center of g, contradicting
the deﬁnition of a semisimple Lie algebra.
ut
We now develop a key tool in the study of a semisimple Lie algebra g, the
existence of certain subalgebras of g isomorphic to sl.2I C/.
Theorem 7.19. For each root ˛, we can ﬁnd linearly independent elements X˛ in
g˛, Y˛ in g˛, and H˛ in h such that H˛ is a multiple of ˛ and such that
ŒH˛; X˛ D 2X˛;
ŒH˛; Y˛ D 2Y˛;
ŒX˛; Y˛ D H˛:
(7.7)
Furthermore, Y˛ can be chosen to equal X
˛ .
If X˛; Y˛; H˛ are as in the theorem, then on the one hand, ŒH˛; X˛ D 2X˛,
while on the other hand, ŒH˛; X˛ D h˛; H˛i X˛. Thus,
h˛; X˛i D 2:
(7.8)
Meanwhile, H˛ is a multiple of ˛ and the unique such multiple compatible
with (7.8) is
H˛ D 2
˛
h˛; ˛i:
(7.9)
For use in Part III, we record the following consequence of Theorem 7.19.

7.3
Roots and Root Spaces
179
Corollary 7.20. For each root ˛, let X˛, Y˛, and H˛ be as in Theorem 7.19, with
Y˛ D X
˛ . Then the elements
E˛
1 WD i
2H˛I
E˛
2 WD i
2.X˛ C Y˛/I
E˛
3 WD 1
2.Y˛  X˛/
are linearly independent elements of k and satisfy the commutation relations
ŒE˛
1 ; E˛
2  D E˛
3 I
ŒE˛
2 ; E˛
3  D E˛
1 I
ŒE˛
3 ; E˛
1  D E˛
2 :
Thus, by Example 3.27, the span of E˛
1 , E˛
2 , and E˛
3 is a subalgebra of k isomorphic
to su.2/.
Proof. Since ˛ belongs to it and H˛ is, by (7.9), a real multiple of ˛, the element
.i=2/H˛ will belong to t  k. Meanwhile, we may check that .E˛
2 / D E˛
2 and
.E˛
3 / D E˛
3 . From the way the map X 7! X was deﬁned (Proposition 7.4),
it follows that E˛
2 and E˛
3 also belong to k. Furthermore, since X˛, Y˛, and
H˛ are linearly independent, E˛
1 , E˛
2 , and E˛
3 are also independent. Finally,
direct computation with the commutation relations in (7.7) conﬁrms the claimed
commutation relations for the E˛
j 's.
ut
Deﬁnition 7.21. The element H˛ D 2˛= h˛; ˛i in Theorem 7.19 is the coroot
associated to the root ˛.
We begin the proof of Theorem 7.19 with a lemma.
Lemma 7.22. Suppose that X is in g˛, that Y is in g˛, and that H is in h. Then
ŒX; Y  is in h and
hŒX; Y ; Hi D h˛; Hi hY; Xi ;
(7.10)
where X is as in Proposition 7.4.
In the proof of Theorem 7.19, we will need to know not just that ŒX; Y  2 h, but
where in h the element ŒX; Y  lies. This information is obtained by computing the
inner product of ŒX; Y  with each element of H, as we have done in (7.10).
Proof. That ŒX; Y  is in h follows from Proposition 7.17. Then, using Proposition
7.4, we compute that
hŒX; Y ; Hi D hadX.Y /; Hi D hY; adX.H/i D  hY; ŒH; Xi :
(7.11)
Since X is in g˛, Proposition 7.18 tells us that the element X is in g˛, so that
ŒH; X D  h˛; Hi X and (7.11) reduces to the desired result. (Recall that we
take inner products to be linear in the second factor.)
ut
Proof of Theorem 7.19. Choose a nonzero X in g˛, so that X D  NX is a nonzero
element of g˛. Applying Lemma 7.22 with Y D X gives
hŒX; X; Hi D h˛; Hi hX; Xi :
(7.12)

180
7
Semisimple Lie Algebras
From (7.12), we see that ŒX; X 2 h is orthogonal to every H 2 h that is orthogonal
to ˛, which happens only if ŒX; X is a multiple of ˛. Furthermore, if we choose
H so that h˛; Hi ¤ 0, we see that hŒX; X; Hi ¤ 0, so that ŒX; X ¤ 0. Now, if
we evaluate (7.12) with H D ŒX; X, we obtain
hŒX; X; ŒX; Xi D h˛; ŒX; Xi hX; Xi :
Since ŒX; X ¤ 0, we conclude that h˛; ŒX; Xi is real and strictly positive.
Let H D ŒX; X and deﬁne elements of g as follows:
H˛ D
2
h˛; HiH;
X˛ D
s
2
h˛; HiX;
Y˛ D
s
2
h˛; HiY:
Then h˛; H˛i D 2, from which it follows that ŒH˛; X˛ D 2X˛ and ŒH˛; Y˛ D
2Y˛. Furthermore, ŒX˛; Y˛ D 2ŒX; Y = h˛; Hi D H˛. Thus, these elements
satisfy the relations claimed in the theorem, and Y˛ D X
˛ . Furthermore, since these
elements are eigenvectors for adH with distinct eigenvalues 2, 2, and 0, they must
be linearly independent (Proposition A.1).
ut
We now make use of the subalgebras in Theorem 7.19 to obtain results about
roots and root spaces. Note that
s˛ WD hX˛; Y˛; H˛i
(7.13)
acts on g by (the restriction to s˛ of) the adjoint representation.
Theorem 7.23. 1. For each root ˛, the only multiples of ˛ that are roots are ˛
and ˛.
2. For each root ˛, the root space g˛ is one dimensional.
Point 1 of Theorem 7.23 should be contrasted with the results of Exercise 8 in
the case of a Lie algebra that is not semisimple.
Lemma 7.24. If ˛ and c˛ are both roots with jcj > 1, then c D ˙2.
Proof. Let s˛ be as in (7.13). If ˇ D c˛ is also a root and X is a nonzero element
of gˇ, then by (7.8), we have
ŒH˛; X D hˇ; H˛i X D Nc h˛; H˛i X D 2NcX:
Thus, 2Nc is an eigenvalue for the adjoint action of s˛ on g. By Point 1 of
Theorem 4.34, any such eigenvalue must be an integer, meaning that c is an integer
multiple of 1=2. But by reversing the roles of ˛ and ˇ in the argument, we see that
1=c must also be an integer multiple of 1=2.

7.3
Roots and Root Spaces
181
Fig. 7.1 If ˇ D 2˛ were a
root, the orthogonal
complement of s˛ in V ˛
would contain an element of
h orthogonal to H˛
V
Now, suppose c D n=2 for some integer n and that 1=c D 2=n is an integer
multiple of 1=2, so that 2=c D 4=n is an integer. Then n D ˙1, ˙2, or ˙4. Thus,
c D ˙1=2, ˙1, or ˙2, of which only ˙2 have jcj > 1.
ut
Proof of Theorem 7.23. Since there are only ﬁnitely many roots, there is some
smallest positive multiple of ˛ that is a root. Replacing ˛ by this smallest multiple,
we can assume that if c˛ is a root, then jcj > 1. By Lemma 7.24, we must then have
c D ˙2.
Let s˛ be as in (7.13), with Y˛ chosen to equal X
˛ . Let V ˛ be the subspace of
g spanned by H˛ and all the root spaces gˇ for which ˇ is a multiple of ˛. (See
Figure 7.1.) We claim that V ˛ is a subalgebra of g. To verify this claim, observe ﬁrst
that if a root ˇ is a multiple of ˛ then by Lemma 7.22, every element of Œgˇ; gˇ
is in h and is orthogonal to every element of h that is orthogonal to ˇ. Thus, every
element of Œgˇ; gˇ is a multiple of ˇ, which is a multiple of ˛, which is a multiple
of H˛. Observe next that if X 2 gˇ, then ŒH˛; X is a multiple of X. Observe, ﬁnally,
that if ˇ and ˇ0 are roots that are multiples of ˛ with ˇ0 ¤ ˇ, then Œgˇ; gˇ0 
gˇCˇ0, where ˇ C ˇ0 ¤ 0 is again a multiple of ˛.
Since V ˛ is a subalgebra of g, it is certainly invariant under the adjoint action of
s˛  V ˛. Note also that s˛ itself is an invariant subspace for the adjoint action of s˛
on V ˛. Now, since Y˛ D X
˛ and H˛ is a positive multiple of ˛ 2 it  ik, we see
that X 2 s˛ for every X 2 s˛. Thus, by Propositions 7.4 and 4.27, the orthogonal
complement U ˛ of s˛ in V ˛ will also be an invariant under the adjoint action of s˛.
Now, h˛; H˛i D 2 and, by Lemma 7.24, any multiples ˇ of ˛ that are roots
are of the form ˇ D ˙˛ or ˇ D ˙2˛. Thus, the eigenvalues of adH˛ in V ˛ are
0, ˙2, and, possibly, ˙4. If U ˛ ¤ f0g, then U ˛ will contain an eigenvector for
adH˛, with an eigenvalue  2 f0; ˙2; ˙4g. Since  is even, it follows from Point 4

182
7
Semisimple Lie Algebras
of Theorem 4.34 that 0 must also be an eigenvector for adH˛ in U ˛. But this is
impossible, since H˛ is the only eigenvector for adH˛ in V ˛ with eigenvalue 0, and
U ˛ is orthogonal to H˛ 2 s˛. Thus, actually, U ˛ D f0g, which means that V ˛ D s˛.
Thus, the only multiples of ˛ that are roots are ˙˛ and the root spaces with roots
˙˛ are one dimensional.
ut
Figure 7.1 shows a conﬁguration of "roots" consistent with Lemma 7.24, but
which cannot actually be the root system of a semisimple Lie algebra.
7.4
The Weyl Group
We now introduce an important symmetry of the set R of roots, known as the Weyl
group. In this section, we follow the Lie algebra approach to the Weyl group, as
opposed to the Lie group approach we followed in Sect. 6.6 in the SU.3/ case. We
will return to the Lie group approach to the Weyl group in Chapter 11 and show
(Sect. 11.7) that the two approaches are equivalent.
Deﬁnition 7.25. For each root ˛ 2 R, deﬁne a linear map s˛ W h ! h by the
formula
s˛  H D H  2h˛; Hi
h˛; ˛i ˛:
(7.14)
The Weyl group of R, denoted W , is then the subgroup of GL.h/ generated by all
the s˛'s with ˛ 2 R.
Note that since each root ˛ is in it and our inner product is real on t, if H is in
it, then s˛  H is also in it. As a map of it to itself, s˛ is the reﬂection about the
hyperplane orthogonal to ˛. That is to say, s˛  H D H whenever H is orthogonal
to ˛, and s˛  ˛ D ˛. Since each reﬂection is an orthogonal linear transformation,
we see that W is a subgroup of the orthogonal group O.it/.
Theorem 7.26. The action of W on it preserves R. That is to say, if ˛ is a root,
then w  ˛ is a root for all w 2 W .
Proof. For each ˛ 2 R, consider the invertible linear operator S˛ on g given by
S˛ D eadX˛ eadY˛ eadX˛ :
Now, if H 2 h satisﬁes h˛; Hi D 0, then ŒH; X˛ D h˛; Hi X˛ D 0. Thus, H and
X˛ commute, which means that adH and adX˛ also commute, and similarly for adH
and adY˛. Thus, if h˛; Hi D 0, the operator S˛ will commute with adH, so that
S˛adHS1
˛
D adH;
h˛; Hi D 0:
(7.15)

7.5
Root Systems
183
On the other hand, if we apply Point 3 of Theorem 4.34 to the adjoint action of
s˛ Š sl.2I C/ on g, we see that
S˛adH˛S1
˛
D adH˛:
(7.16)
By combining (7.15) and (7.16), we see that for all H 2 h, we have
S˛adHS1
˛
D ads˛H:
(7.17)
Now if ˇ is any root and X is an associated root vector, consider the vector
S1
˛ .X/ 2 g. We compute that
adH.S1
˛ .X// D S1
˛ .S˛adHS1
˛ /.X/
D S1
˛ ads˛H.X/
D hˇ; s˛  Hi S1
˛ .X/
D
˝
s1
˛
 ˇ; H
˛
S1
˛ .X/:
Thus, S1
˛ .X/ is a root vector with root s1
˛
 ˇ D s˛  ˇ. This shows that the set of
roots is invariant under each s˛ and, thus, under W .
ut
Actually, since s˛  s˛  ˇ D ˇ, each reﬂection maps R onto R. It follows that
each w 2 W also maps R onto R.
Corollary 7.27. The Weyl group is ﬁnite.
Proof. Since the roots span h, each w 2 W is determined by its action on R. Since,
also, w maps R onto R, we see that W may be thought of as a subgroup of the
permutation group on the roots.
ut
7.5
Root Systems
In this section, we record several important properties of the roots, using results from
the two previous sections. Recall that for each root ˛, we have an element H˛ of h
contained in Œg˛; g˛ as in Theorem 7.19. As we saw in (7.8) and (7.9), H˛ satisﬁes
h˛; H˛i D 2 and is related to ˛ by the formula H˛ D 2˛= h˛; ˛i. In particular, the
element H˛ is independent of the choice of X˛ and Y˛ in Theorem 7.19.
Deﬁnition 7.28. For each root ˛, the element H˛ 2 h given by
H˛ D 2
˛
h˛; ˛i
is the coroot associated to the root ˛.

184
7
Semisimple Lie Algebras
Proposition 7.29. For all roots ˛ and ˇ, we have that
hˇ; H˛i D 2h˛; ˇi
h˛; ˛i
(7.18)
is an integer.
We have actually already made use of this result in the proof of Lemma 7.24.
Proof. If s˛ D hX˛; Y˛; H˛i is as in Theorem 7.19 and X is a root vector associated
to the root ˇ, then ŒH˛; X D hˇ; H˛i X. Thus, hˇ; H˛i is an eigenvalue for the
adjoint action of s˛ Š sl.2I C/ on g. Point 1 of Theorem 4.34 then shows that
hˇ; H˛i must be an integer.
ut
Recall from elementary linear algebra that if ˛ and ˇ are elements of an inner
product space, the orthogonal projection of ˇ onto ˛ is given by
h˛; ˇi
h˛; ˛i ˛:
The quantity on the right-hand side of (7.18) is thus twice the coefﬁcient of ˛
in the projection of ˇ onto ˛. We may therefore interpret the integrality result in
Proposition 7.29 in the following geometric way:
If ˛ and ˇ are roots, the orthogonal projection of ˛ onto ˇ must be an integer or half-integer
multiple of ˇ.
Alternatively, we may think about Proposition 7.29 as saying that ˇ and s˛  ˇ must
differ by an integer multiple of ˛ [compare (7.14)].
If we think of the set R of roots as a subset of the real inner product space
E WD it, we may summarize the properties of R as follows.
Theorem 7.30. The set R of roots is a ﬁnite set of nonzero elements of a real inner
product space E, and R has the following additional properties.
1. The roots span E.
2. If ˛ 2 R, then ˛ 2 R and the only multiples of ˛ in R are ˛ and ˛.
3. If ˛ and ˇ are in R, so is s˛  ˇ, where s˛ is the reﬂection deﬁned by (7.14).
4. For all ˛ and ˇ in R, the quantity
2h˛; ˇi
h˛; ˛i
is an integer.
Any such collection of vectors is called a root system. We will look in detail at
the properties of root systems in Chapter 8.

7.6
Simple Lie Algebras
185
7.6
Simple Lie Algebras
Every semisimple Lie algebra decomposes as a direct sum of simple algebras
(Theorem 7.8). In this section, we give a criterion for a semisimple Lie algebra
to be simple. We will eventually see (Sect. 8.11) that most of the familiar examples
of semisimple Lie algebras are actually simple.
Proposition 7.31. Suppose g is a real Lie algebra and that the complexiﬁcation gC
of g is simple. Then g is also simple.
Proof. Since gC is simple, the dimension of gC over C is at least 2, so that the
dimension of g over R is also at least 2. If g had a nontrivial ideal h, then the
complexiﬁcation hC of h would be a nontrivial ideal in g.
ut
The converse of Proposition 7.31 is false in general. The Lie algebra so.3I 1/, for
example, is simple as a real Lie algebra, and yet its complexiﬁcation is isomorphic
to so.4I C/, which in turn is isomorphic to sl.2I C/ ˚ sl.2I C/. See Exercise 14.
Theorem 7.32. Suppose K is a compact matrix Lie group whose Lie algebra k is
simple as a real Lie algebra. Then the complexiﬁcation g WD kC of k is simple as a
complex Lie algebra.
For more results about simple algebras over R, see Exercises 12 and 13. Before
proving Theorem 7.32 result, we introduce a deﬁnition.
Deﬁnition 7.33. If g is a real Lie algebra, g admits a complex structure if there
exists a "multiplication by i" map J W g ! g that makes g into a complex vector
space in such a way that the bracket map Œ;  W g  g ! g is complex bilinear.
Here, bilinearity of the bracket means, in particular, that ŒJX; Y  D J ŒX; Y  for
all X; Y 2 k. Equivalently, g admits a complex structure if there exists a complex
Lie algebra h and a real-linear map  W h ! g that is one-to-one and onto and
satisﬁes .ŒX; Y / D Œ.X/; .Y / for all X; Y 2 h.
Lemma 7.34. Suppose that K is a compact matrix Lie group whose Lie algebra k
is noncommutative. Then k does not admit a complex structure.
Proof. Suppose, toward a contradiction, that k does admit a complex structure J .
By Proposition 7.4, there exists a real inner product on k with respect to which adH
is skew symmetric for all H 2 k. If we choose H not in the center of k then adH
is nonzero and skew symmetric, hence diagonalizable in kC with pure-imaginary
eigenvalues, not all of which are zero. In particular, adH is not nilpotent.
On the other hand, since J is complex bilinear, if we view k as a complex vector
space with respect to the map J , then adH is complex linear. Since adH is not
nilpotent, it has a nonzero eigenvalue  D a C ib 2 C. Thus, there is a nonzero
X 2 k such that
ŒH; X D .a C ib/  X D aX C bJX:

186
7
Semisimple Lie Algebras
If we then consider element
H 0 WD N  H D aH  bJH;
we may compute, using the linearity of the bracket with respect to J and the identity
J 2 D I, that
ŒH 0; X D jj2 X D .a2 C b2/X:
But adH 0 is also skew symmetric, and, thus,
jj2 hX; Xi D hadH 0.X/; Xi D  hX; adH 0.X/i D  jj2 hX; Xi ;
which is impossible if  and X are both nonzero.
ut
Proof of Theorem 7.32. Suppose to the contrary that g were not simple, so that it
decomposes as a sum of at least two simple algebras gj. By Proposition 7.9, the
decomposition of a semisimple algebra Lie algebra into a sum of simple algebras is
unique up to ordering of the summands. On the other hand, g if decomposes as the
sum of the gj,'s, it also decomposes as the sum of the gj's, where the map X 7! NX
is as in (7.5). Thus, for each j, we must have gj D gk for some k.
Suppose there is some j for which gj D gj . Then for all X 2 gj, the element
X C NX is in gj \k, from which it follows that gj \k is a nonzero ideal in k. But gj \k
cannot be all of k, or else gj would be all of g. Thus, gj \ k would be a nontrivial
ideal in k, contradicting our assumption that k is simple. On the other hand, if we
pick some j with gj D gk, k ¤ j, then .gj ˚ gk/ \ k is a nonzero ideal in k, which
must be all of k. We conclude, then, that there must be exactly two summands g1
and g2 in the decomposition of g, satisfying g1 D g2.
Let us then deﬁne a real-linear map  W g1 ! k by .X/ D X C NX. Note that
for any X 2 g1, the element NX is in g2, so that ŒX; NX D 0. From this, we can easily
check that  satisﬁes .ŒX; Y / D Œ.X/; .Y / for all X; Y 2 g1. Furthermore, 
is injective because, for any X 2 g1, we have NX 2 g2 and thus NX cannot equal X
unless X D 0. Finally, by counting real dimensions, we see that  is also surjective.
Since g1 is a complex Lie algebra and  is a real Lie algebra isomorphism, we see
that k admits a complex structure, contradicting Lemma 7.34.
ut
Theorem 7.35. Let g Š kC be a complex semisimple Lie algebra, let t be a maximal
commutative subalgebra of k, and let h D tC be the associated Cartan subalgebra
of g. Let R  h be the root system for g relative to h. If g is not simple then h
decomposes as an orthogonal direct sum of nonzero subspaces h1 and h2 in such a
way that every element of R is either in h1 or in h2. Conversely, if h decomposes in
this way, then g is not simple.
We may restate the ﬁrst part of the theorem in contrapositive form: If there does
not exist an orthogonal decomposition of h as h D h1 ˚ h2 (with dim h1 > 0 and
dim h2 > 0) such that every root is either in h1 or h2, then g is simple. In Sect. 8.6,
we will show that if the "Dynkin diagram" of the root system is connected, then
no such decomposition of h exists. We will then be able to check that most of our
examples of semisimple Lie algebras are actually simple.

7.6
Simple Lie Algebras
187
Proof of Theorem 7.35, Part 1. Assume ﬁrst that g is not simple, so that, by
Theorem 7.32, k is not simple either. Thus, k has a nontrivial ideal k1. Now, ideals
in k are precisely invariant subspaces for the adjoint action of k on itself, which are
the same as the invariant subspaces for the adjoint action of K on k. If we choose
an inner product on k is Ad-K-invariant, the orthogonal complement of k1 is also an
ideal. Thus, k decomposes as the Lie algebra direct sum k1 ˚ k2, where k2 D .k1/?,
in which case g decomposes as g1 ˚ g2, where g1 D .k1/C and g2 D .k2/C.
Let t be any maximal commutative subalgebra of k and h D tC the associated
Cartan subalgebra of g. We claim that t must decompose as t1 ˚ t2, where t1  k1
and t2  k2. Suppose H and H 0 are two elements of t, with H D X1 C X2 and
H 0 D Y1 C Y2, with X1; Y1 2 k1 and X2; Y2 2 k2. Then
0 D ŒH; H 0 D ŒX1; Y1 C ŒX2; Y2;
with ŒX1; Y1 2 k1 and ŒX2; Y2 2 k2, which can happen only if ŒX1; Y1 D
ŒX2; Y2 D 0. Thus,
ŒX1; H 0 D ŒX1; Y1 D 0;
showing that X1 commutes with every element of h. Since h is maximal commuta-
tive, X1 must actually be in h, and similarly for X2. That is to say, for every X 2 t,
the k1- and k2-components of X also belong to t. From this observation, it follows
easily that h is the direct sum of the subalgebras
t1 WD t \ k1;
t2 WD t \ k2:
The algebra h then splits as h1 ˚ h2, where h1 D .t1/C and h2 D .t2/C. Let R1
and R2 be the roots for g1 relative to h1 and g2 relative to h2, respectively. If ˛ is an
element of R1 and X 2 g1 is an associated root vector, then consider any element
H D H1 C H2 of h, where H1 2 h1 and H2 2 h2. We have
ŒH; X D ŒH1; X C ŒH2; X D h˛; H1i X;
since H2 2 g2 and X 2 g1. Thus, X is also a root vector for g relative to h, with
the associated root being ˛. Similarly, every root ˇ 2 R2 is also a root for g relative
to h.
We now claim that every root ˛ for g relative to h is either an element of R1 or
an element of R2. If X D X1 C X2 is a root vector associated to the root ˛, then for
H1 2 h1 we have
ŒH1; X D ŒH1; X1 C X2 D ŒH1; X1 D h˛; H1i X1:
On the other hand, ŒH1; X must be a multiple of X. Thus, either X2 D 0 or ˛ is
orthogonal to h1. If ˛ is orthogonal to h1 then it belongs to h2 and so ˛ is a root for
g2 (i.e., ˛ 2 R2). If, on the other hand, X2 D 0, then 0 D ŒH2; X D h˛; H2i X for
all H2 2 h2, so that ˛ is orthogonal to h2. In that case, ˛ 2 h1 and ˛ must belong
to R1.
ut

188
7
Semisimple Lie Algebras
Proof of Theorem 7.35, Part 2. Suppose now h splits as h D h1 ˚ h2, with h1 and
h2 being nonzero, orthogonal subspaces of h, in such a way that every root is either
in h1 or h2. Let Rj D R \ hj , j D 1; 2, and deﬁne subspaces gj of g as
gj D hj ˚
M
˛2Rj
g˛;
j D 1; 2:
Then g decomposes as a vector space as g1 ˚ g2. But h1 commutes with each g˛
with ˛ 2 R2, because ˛ is in h2, which is orthogonal to h1. Similarly, h2 commutes
with g˛, ˛ 2 R1, and, of course, with h1. Finally, if ˛ 2 R1 and ˇ 2 R2, then
Œg˛; gˇ D f0g, because ˛ C ˇ is not a root. Thus, actually, g is the Lie algebra
direct sum of g1 and g2, showing that g is not simple.
ut
7.7
The Root Systems of the Classical Lie Algebras
In this section, we look at how the structures described in this chapter work out in the
case of the "classical" semisimple Lie algebras, that is, the special linear, orthogonal,
and symplectic algebras over C. We label each of our Lie algebras in such a way
that the rank is n, and we split our analysis of the orthogonal algebras into the even
and odd cases. For each of the classical Lie algebras, we use a constant multiple of
the Hilbert-Schmidt inner product hX; Y i D trace.XY /, which is invariant under
the adjoint action of the corresponding compact group.
7.7.1
The Special Linear Algebras sl.n C 1I C/
We work with the compact real form k D su.nC1/ and the commutative subalgebra
t which is the intersection of the set of diagonal matrices with su.n C 1/; that is,
t D
8
ˆ<
ˆ:
0
B@
ia1
:::
ianC1
1
CA
ˇˇˇˇˇˇˇ
aj 2 R;
a1 C    C anC1 D 0
9
>=
>;
:
(7.19)
We also consider h WD tC, which is described as follows:
h D
8
ˆ<
ˆ:
0
B@
1
:::
nC1
1
CA
ˇˇˇˇˇˇˇ
j 2 C;
1 C    C nC1 D 0
9
>=
>;
:
(7.20)

7.7
The Root Systems of the Classical Lie Algebras
189
If a matrix X commutes with each element of t, it will also commute with each
element of h. But then, as in the proof of Example 7.3, X would have to be diagonal,
and if X 2 su.n C 1/, then X would have to be in t. Thus, t is actually a maximal
commutative subalgebra of su.n C 1/.
Now, let Ejk denote the matrix that has a one in the kth row and lth column and
that has zeros elsewhere. A simple calculation shows that if H 2 h is as in (7.20),
then HEjk D jEjk and EjkH D kEjk. Thus,
ŒH; Ejk D .j  k/Ejk:
(7.21)
If j ¤ k, then Ejk is in sl.n C 1I C/ and (7.21) shows that Ejk is a simultaneous
eigenvector for each adH with H in h, with eigenvalue j  k. Note that every
element X of sl.n C 1I C/ can be written uniquely as an element of the Cartan
subalgebra (the diagonal entries of X) plus a linear combination of the Ejk's with
j ¤ k (the off-diagonal entries of X).
If we think at ﬁrst of the roots as elements of h, then [according to (7.21)]
the roots are the linear functionals ˛jk that associate to each H 2 h, as in (7.20), the
quantity j k. We identify h with the subspace of CnC1 consisting vectors whose
components sum to zero. The inner product hX; Y i D trace.XY / on h is just the
restriction to this subspace of the usual inner product on CnC1. If we use this inner
product to transfer the roots from h to h, we obtain the vectors
˛jk D ej  ek
.j ¤ k/:
The roots of sl.n C 1I C/ form a root system that is conventionally called An,
with the subscript n indicating that the rank of sl.nC1I C/ (i.e., the dimension of h)
is n. We see that each root has length
p
2 and
˝
˛jk; ˛j 0k0˛
has the value 0, ˙1, or ˙2, depending on whether fj; kg and fj 0; k0g have zero, one,
or two elements in common. Thus
2
˝
˛jk; ˛j 0k0˛
˝
˛jk; ˛jk
˛ 2 f0; ˙1; ˙2g:
If ˛ and ˇ are roots and ˛ ¤ ˇ and ˛ ¤ ˇ, then the angle between ˛ and ˇ is
either =3, =2, or 2=3, depending on whether h˛; ˇi has the value 1, 0, or 1.
It is easy to see that for any j and k, the reﬂection s˛jk acts on CnC1 by
interchanging the jth and kth entries of each vector. It follows that the Weyl group
of the An root system is the permutation group on n C 1 elements.

190
7
Semisimple Lie Algebras
7.7.2
The Orthogonal Algebras so.2nI C/
The root system for so.2nI C/ is denoted Dn. We consider the compact real form
so.2n/ of so.2nI C/, and we consider in so.2n/ the commutative subalgebra t
consisting of 2  2 block-diagonal matrices in which the jth diagonal block is of
the form

0 aj
aj
0

;
(7.22)
for some aj 2 R. We then consider h D tCit of so.2nI C/, which consists of 2 2
block-diagonal matrices in which the jth diagonal block is of the form (7.22) with
aj 2 C. As we will show below, t is actually a maximal commutative subalgebra of
so.2n/, so that h is a Cartan subalgebra of so.2nI C/.
The root vectors are now 2  2 block matrices having a 2  2 matrix C in the
.j; k/ block (j < k), the matrix C tr in the .k; j/ block, and zero in all other
blocks, where C is one of the four matrices
C1 D
1
i
i 1

;
C2 D
 1 i
i 1

;
C3 D
1 i
i
1

;
C4 D
 1 i
i 1

:
Direct calculation shows that these are, indeed, root vectors and that the correspond-
ing roots are the linear functionals on h given by i.aj Cak/, i.aj Cak/, i.aj ak/,
and i.aj  ak/, respectively.
Let us use on h the inner product hX; Y i D trace.XY /=2. If we then identify h
with Cn by means of the map
H 7! i.a1; : : : ; an/;
our inner product on h will correspond to the standard inner product on Cn. The
roots as elements of Cn are then the vectors
˙ ej ˙ ek;
j < k;
(7.23)
where fejg is the standard basis for Rn.
We now demonstrate that t is a maximal commutative subalgebra of k, and also
that the center of so.2nI C/ is trivial for n  2, as claimed in Example 7.3. It
is easy to check that every X 2 g can be expresses as an element of h plus a
linear combination of the root vectors described above. If X commutes with every
element of t, then X also commutes with every element of h. Since each of the linear
functionals i.˙aj ˙ak/, j < k, is nonzero on h, the coefﬁcients of the root vectors
in the expansion of X must be zero; that is, X must be in h. If, also, X is in k, then
X must be in h \ k D t. This shows that t is maximal commutative in k.

7.7
The Root Systems of the Classical Lie Algebras
191
Meanwhile, if X is in the center of so.2nI C/, then as shown in the previous
paragraph, X must belong to h. But then for each root vector X˛ with root ˛, we
must have
0 D ŒX; X˛ D h˛; Xi X˛;
so that h˛; Xi D 0. It is then easy to check that if n  2, the roots in (7.23) span
h Š Cn and we conclude that X must be zero. (If, on the other hand, n D 1, then
there are no roots and so.2I C/ D h is commutative.)
7.7.3
The Orthogonal Algebras so.2n C 1I C/
The root system for so.2n C 1I C/ is denoted Bn. We consider the compact real
form so.2nC1/ of so.2nC1I C/, and we consider in so.2nC1/ the commutative
subalgebra t consisting of block diagonal matrices with n blocks of size 2  2
followed by one block of size 1  1. We take the 2  2 blocks to be of the same
form as in so.2n/ and we take the 1  1 block to be zero. Then h WD tC consists of
those matrices in so.2nC1I C/ of the same form as in t except that the off-diagonal
elements of the 2  2 blocks are permitted to be complex. The same argument as
in the case of so.2nI C/, based on the calculations below, shows that t is maximal
commutative, so that h is a Cartan subalgebra.
The Cartan subalgebra in so.2n C 1I C/ is identiﬁable in an obvious way with
the Cartan subalgebra in so.2nI C/. In particular, both so.2nI C/ and so.2nC1I C/
have rank n. With this identiﬁcation of the Cartan subalgebras, every root for
so.2nI C/ is also a root for so.2n C 1I C/. There are 2n additional roots for
so.2n C 1I C/. These have the matrices having the following 2  1 block in entries
.2k; 2n C 1/ and .2k C 1; 2n C 1/ as their root vectors:
B1 D
 1
i

and having Btr
1 in entries .2n C 1; 2k/ and .2n C 1; 2k C 1/, together with the
matrices having
B2 D
 1
i

in entries .2k; 2n C 1/ and .2k C 1; 2n C 1/ and Btr
2 in entries .2n C 1; 2k/ and
.2n C 1; 2k C 1/. The corresponding roots, viewed as elements of h, are given by
iak and iak.

192
7
Semisimple Lie Algebras
If we use the inner product to identify the roots with elements of h and then
identify h with Cn in the same way as in the previous subsection, the roots for
so.2n C 1I C/ consist of the roots ˙ej ˙ ek, j < k, for so.2nI C/, together with
additional roots of the form
˙ej;
j D 1; : : : ; n:
These additional roots are shorter by a factor of
p
2 than the roots ˙ej ˙ ek for
so.2nI C/.
7.7.4
The Symplectic Algebras sp.nI C/
The root system for sp.nI C/ is denoted Cn. We consider sp.nI C/, the space of
2n  2n complex matrices X satisfying Xtr D X, where  is the 2n  2n
matrix
 D

0 I
I 0

:
(Compare Proposition 3.25.) Explicitly, the elements of sp.nI C/ are matrices of the
form
X D
 A
B
C Atr

;
where A is an arbitrary n  n matrix and B and C are arbitrary symmetric
matrices. We consider the compact real form sp.n/ D sp.nI C/ \ u.2n/. (Compare
Sect. 1.2.8.)
We consider the commutative subalgebra t of sp.n/ consisting of matrices of the
form
0
BBBBBBBBB@
a1
:::
an
a1
:::
an
1
CCCCCCCCCA
;
where each aj is pure imaginary. We then consider the subalgebra h D t C it of
sp.nI C/, which consists of matrices of the same form but where each aj is now an
arbitrary complex number. As in previous subsections, the calculations below will
show that t is maximal commutative, so that h is a Cartan subalgebra.

7.8
Exercises
193
The 2n  2n matrices of the block form
 0 Ejk C Ekj
0
0

;

0
0
Ejk C Ekj 0

(7.24)
(j ¤ k) are root vectors for which the corresponding roots are .aj C ak/ and
.aj C ak/, respectively. Next, matrices of the block form
 Ejk
0
0 Ekj

;
(7.25)
(j ¤ k) are root vectors for which the corresponding roots are .ak  al/. Finally,
matrices of the block form
 0 Ejj
0 0

;
 0 0
Ejj 0

(7.26)
are root vectors for which the corresponding roots are 2aj and 2aj .
We use on h the inner product hX; Y i D trace.XY /=2. If we then identify h
with Cn by means of the map
H 7! .a1; : : : ; an/;
our inner product on h will correspond to the standard inner product on Cn. The
roots are then the vectors of the form
˙ej ˙ ek;
j < k
and of the form
˙2ej;
j D 1; : : : ; n:
This root system is the same as that for so.2n C 1I C/, except that instead of ˙ej
we now have ˙2ej.
7.8
Exercises
1. Let hC denote the complexiﬁcation of the Lie algebra of the Heisenberg group,
namely the space of all complex 3  3 upper triangular matrices with zeros on
the diagonal.
(a) Show the maximal commutative subalgebras of hC are precisely the two-
dimensional subalgebras of hC that contain the center of hC.
(b) Show that hC does not have any Cartan subalgebras (in the sense of
Deﬁnition 7.10).

194
7
Semisimple Lie Algebras
2. Give an example of a maximal commutative subalgebra of sl.2I C/ that is not a
Cartan subalgebra.
3. Show that the Hilbert-Schmidt inner product on sl.nI C/, given by hX; Y i D
trace.XY /, is invariant under the adjoint action of SU.n/ and is real on su.n/.
4. Using the root space decomposition in Sect. 7.7.2, show that the Lie algebra
so.4I C/ is isomorphic to the Lie algebra direct sum sl.2I C/ ˚ sl.2I C/. Then
show that so.4/ is isomorphic to su.2/ ˚ su.2/.
5. Suppose g is a Lie subalgebra of Mn.C/ and assume that for all X 2 g, we have
X 2 g, where X is the usual matrix adjoint of X. Let k D g \ u.n/.
(a) Show that k is a real subalgebra of g and that g Š kC.
(b) Deﬁne an inner product on g by
hX; Y i D trace.XY /:
Show that this inner product satisﬁes all the properties in Proposition 7.4.
6. For any Lie algebra g, let the Killing form be the symmetric bilinear form B
on g deﬁned by
B.X; Y / D trace.adXadY /:
(a) Show that for any X 2 g, the operator adX W g ! g is skew-symmetric
with respect to B, meaning that
B.adX.Y /; Z/ D B.Y; adX.Z//
for all Y; Z 2 g.
(b) Suppose g D kC is semisimple. Show that B is nondegenerate, meaning
that for all nonzero X 2 g, there is some Y 2 g for which B.X; Y / ¤ 0.
Hint: Use Proposition 7.4.
7. Let g be a Lie algebra and let B be the Killing form on g (Exercise 6). Show
that if g is a nilpotent Lie algebra (Deﬁnition 3.15), then B.X; Y / D 0 for all
X; Y 2 g.
8. Let g denote the vector space of 3  3 complex matrices of the form
 A B
0 0

;
where A is a 2  2 matrix with trace zero and B is an arbitrary 2  1 matrix.
(a) Show that g is a subalgebra of M3.C/.
(b) Let X, Y , H, e1, and e2 be the following basis for g. We let X, Y , and
H be the usual sl.2I C/ basis in the "A" slot, with B D 0. We let e1 and
e2 be the matrices with A D 0 and with .1 0/tr and .0 1/tr in the "B"

7.8
Exercises
195
slot, respectively. Compute the commutation relations among these basis
elements.
(c) Show that g has precisely one nontrivial ideal, namely the span of e1 and
e2. Conclude that g is not semisimple.
Hint: First, determine the subspaces of g that are invariant under the adjoint
action of the sl.2I C/ algebra spanned by X, Y , and H, and then determine
which of these subspaces are also invariant under the adjoint action of e1
and e2. In determining the sl.2I C/-invariant subspaces, use Exercise 10 of
Chapter 4.
(d) Let h be the one-dimensional subspace of g spanned by the element H.
Show that h is a maximal commutative subalgebra of g and that adH is
diagonalizable. Show that the eigenvalues of adH are 0, ˙1, and ˙2.
Note: The "roots" for h are thus the numbers ˙1 and ˙2, which would
not be possible if h were a Cartan subalgebra of the form h D tC in a
semisimple Lie algebra.
(e) Let g1 and g1 denote the eigenspaces of adH with eigenvalues 1 and 1,
respectively. Show that Œg1; g1 D f0g.
Note: This result should be contrasted with the semisimple case, where
Œg˛; g˛ is always a one-dimensional subspace of h, so that g˛, g˛, and
Œg˛; g˛ span a three-dimensional subalgebra of g isomorphic to sl.2I C/.
9. Using Theorem 7.35 and the calculations in Sect. 7.7.3, show that the Lie
algebra so.5I C/ is simple.
10. Let E D Rn, n  2, and consider the Dn root system, consisting of the vectors
of the form ˙ej ˙ek, with j < k. Show that the Weyl group of Dn is the group
of transformations of Rn expressible as a composition of a permutation of the
entries and an even number of sign changes.
11. Let E D Rn and consider the Bn root system, consisting of the vectors of
the form ˙ej ˙ ek, with j < k, together with the vectors of the form ˙ej ,
j D 1; : : : ; n. Show that the Weyl group of Bn is the group of transformations
expressible as a composition of a permutation of the entries and an arbitrary
number of sign changes.
Note: Since each root in Cn is a multiple of a root in Bn, and vice versa, the
reﬂections for Cn are the same as the reﬂections for Bn. Thus, the Weyl group
of Cn is the same as that of Bn.
12. Let g be a complex simple Lie algebra, with complex structure denoted by
J . Let gR denote the Lie algebra g viewed as a real Lie algebra of twice the
dimension. Now let g0 be the complexiﬁcation of gR, with the complex structure
on g0 denoted by i.
(a) Show that g0 decomposes as a Lie algebra direct sum g0 D g1 ˚ g2, where
both g1 and g2 are isomorphic to g.
Hint: Consider element of g0 of the form X C iJX and of the form X  iJX,
with X 2 g.

196
7
Semisimple Lie Algebras
(b) Show that gR is simple as a real Lie algebra. (That is to say, there is no
nontrivial real subspace h of g such that ŒX; Y  2 h for all X 2 g and
Y 2 h.)
13. Let h be a real simple Lie algebra, and assume that hC is not simple. Show that
there is a complex simple Lie algebra g such that h Š gR, where the notation is
as in Exercise 12.
Hint: Imitate the proof of Theorem 7.32.
14. Show that the real Lie algebra so.3I 1/ is isomorphic to sl.2I C/R, where the
notation is as in Exercise 12. Conclude that so.3I 1/ is simple as a real Lie
algebra, but that so.3I 1/C is not simple and is isomorphic to sl.2I C/˚sl.2I C/.
Hint: First show that so.3I 1/C Š sl.2I C/ ˚ sl.2I C/ and then show that the
two copies of sl.2I C/ are conjugates of each other.

Chapter 8
Root Systems
In this chapter, we consider root systems apart from their origins in semisimple
Lie algebras. We establish numerous "factoids" about root systems, which will
be used extensively in subsequent chapters. Here is one example of how results
about root systems will be used. In Chapter 9, we construct each ﬁnite-dimensional,
irreducible representation of a semisimple Lie algebra as a quotient of an inﬁnite-
dimensional representation known as a Verma module. To prove that the quotient
representation is ﬁnite-dimensional, we prove that the weights of the quotient are
invariant under the action of the Weyl group, that is, the group generated by the
reﬂections about the hyperplanes orthogonal to the roots. It is not possible, however,
to prove directly that the weights are invariant under all reﬂections, but only under
reﬂections coming from a special subset of the root system known as the base. To
complete the argument, then, we need to know that the Weyl group is actually
generated by the reﬂections associated to the roots in the base. This claim is the
content of Proposition 8.24.
8.1
Abstract Root Systems
A root system is any collection of vectors having the properties satisﬁed by the
roots (viewed as a subset of it  h) of a semisimple Lie algebra, as encoded in the
following deﬁnition.
Deﬁnition 8.1. A root system .E; R/ is a ﬁnite-dimensional real vector space E
with an inner product h; i, together with a ﬁnite collection R of nonzero vectors in
E satisfying the following properties:
1. The vectors in R span E.
2. If ˛ is in R and c 2 R, then c˛ is in R only if c D ˙1.
© Springer International Publishing Switzerland 2015
B. Hall, Lie Groups, Lie Algebras, and Representations, Graduate
Texts in Mathematics 222, DOI 10.1007/978-3-319-13467-3_8
197

198
8
Root Systems
3. If ˛ and ˇ are in R, then so is s˛  ˇ, where s˛ is the linear transformation of E
deﬁned by
s˛  ˇ D ˇ  2hˇ; ˛i
h˛; ˛i˛;
ˇ 2 E:
4. For all ˛ and ˇ in R, the quantity
2hˇ; ˛i
h˛; ˛i
is an integer.
The dimension of E is called the rank of the root system and the elements of R
are called roots.
Note that since s˛ ˛ D ˛, we have that ˛ 2 R whenever ˛ 2 R. In the theory
of symmetric spaces, there arise systems satisfying Conditions 1, 3, and 4, but not
Condition 2. These are called "nonreduced" root systems. In the theory of Coxeter
groups, there arise systems satisfying Conditions 1, 2, and 3, but not Property 4.
These are called "noncrystallographic" or "nonintegral" root systems. In this book,
we consider only root systems satisfying all of the conditions in Deﬁnition 8.1.
The map s˛ is the reﬂection about the hyperplane orthogonal to ˛; that is, s˛ ˛ D
˛ and s˛  ˇ D ˇ for all ˇ that are orthogonal to ˛, as is easily veriﬁed from the
formula for s˛. From this description, it should be evident that s˛ is an orthogonal
transformation of E with determinant 1.
We can interpret Property 4 geometrically in one of two ways. In light of the
formula for s˛, Property 4 is equivalent to saying that s˛  ˇ should differ from ˇ
by an integer multiple of ˛. Alternatively, if we recall that the orthogonal projection
of ˇ onto ˛ is given by .hˇ; ˛i=h˛; ˛i/˛, we note that the quantity in Property 4 is
twice the coefﬁcient of ˛ in this projection. Thus, Property 4 is equivalent to saying
that the projection of ˇ onto ˛ is an integer or half-integer multiple of ˛.
We have shown that one can associate a root system to every complex semisimple
Lie algebra. It turns out that every root system arises in this way, although this is far
from obvious—see Sect. 8.11.
Deﬁnition 8.2. If .E; R/ is a root system, the Weyl group W of R is the subgroup
of the orthogonal group of E generated by the reﬂections s˛, ˛ 2 R.
By assumption, each s˛ maps R into itself, indeed onto itself, since each ˇ 2 R
satisﬁes ˇ D s˛  .s˛  ˇ/. It follows that every element of W maps R onto itself.
Since the roots span E, a linear transformation of E is determined by its action
on R. Thus, the Weyl group is a ﬁnite subgroup of O.E/ and may be regarded as a
subgroup of the permutation group on R. We denote the action of w 2 W on H 2 E
by w  H.

8.1
Abstract Root Systems
199
Proposition 8.3. Suppose .E; R/ and .F; S/ are root systems. Consider the vector
space E ˚ F , with the natural inner product determined by the inner products on
E and F . Then R [ S is a root system in E ˚ F , called the direct sum of R and S.
Here, we are identifying E with the subspace of E ˚ F consisting of all vectors
of the form .e; 0/ with e in E, and similarly for F . Thus, more precisely, R [ S
means the elements of the form .˛; 0/ with ˛ in R together with elements of the
form .0; ˇ/ with ˇ in S. (Elements of the form .˛; ˇ/ with ˛ 2 R and ˇ 2 S are
not in R [ S.)
Proof. If R spans E and S spans F , then R [ S spans E ˚ F , so Condition 1
is satisﬁed. Condition 2 holds because R and S are root systems in E and F ,
respectively. For Condition 3, if ˛ and ˇ are both in R or both in S, then s˛  ˇ 2
R [ S because R and S are root systems. If ˛ 2 R and ˇ 2 S or vice versa, then
h˛; ˇi D 0, so that
s˛  ˇ D ˇ 2 R [ S:
Similarly, if ˛ and ˇ are both in R or both in S, then 2 h˛; ˇi = h˛; ˛i is an integer
because R and S are root systems, and if ˛ 2 R and ˇ 2 S or vice versa, then
2 h˛; ˇi = h˛; ˛i D 0. Thus, Condition 4 holds for R [ S.
ut
Deﬁnition 8.4. A root system .E; R/ is called reducible if there exists an orthogo-
nal decomposition E D E1 ˚ E2 with dim E1 > 0 and dim E2 > 0 such that every
element of R is either in E1 or in E2. If no such decomposition exists, .E; R/ is
called irreducible.
If .E; R/ is reducible, then it is not hard to see that the part of R in E1 is a root
system in E1 and the part of R in E2 is a root system in E2. Thus, a root system is
reducible precisely if it can be realized as a direct sum of two other root systems.
In the Lie algebra setting, the root system associated to a complex semisimple Lie
algebra g is irreducible precisely if g is simple (Theorem 7.35).
Deﬁnition 8.5. Two root systems .E; R/ and .F; S/ are said to be isomorphic if
there exists an invertible linear transformation A W E ! F such that A maps R onto
S and such that for all ˛ 2 R and ˇ 2 E, we have
A .s˛  ˇ/ D sA˛  .Aˇ/:
A map A with this property is called an isomorphism.
Note that the linear map A is not required to preserve inner products, but only to
preserve the reﬂections about the roots. If, for example, F D E and S consists of
elements of the form c˛ with ˛ 2 R, then .F; S/ is isomorphic to .E; R/, with the
isomorphism being the map A D cI.
We now establish a basic result limiting the possible angles and length ratios
occurring in a root system.

200
8
Root Systems
3
2
2
4
2
6
2
3
2
Fig. 8.1 The basic acute angles and length ratios
Proposition 8.6. Suppose ˛ and ˇ are roots, ˛ is not a multiple of ˇ, and h˛; ˛i 
hˇ; ˇi. Then one of the following holds:
1. h˛; ˇi D 0
2. h˛; ˛i D hˇ; ˇi and the angle between ˛ and ˇ is =3 or 2=3
3. h˛; ˛i D 2 hˇ; ˇi and the angle between ˛ and ˇ is =4 or 3=4
4. h˛; ˛i D 3 hˇ; ˇi and the angle between ˛ and ˇ is =6 or 5=6
Figure 8.1 shows the allowed angles and length ratios, for the case of an acute
angle. In each case, 2 h˛; ˇi=h˛; ˛i D 1, whereas 2 hˇ; ˛i=hˇ; ˇi takes the values
1, 2, and 3 in the three successive cases. Section 8.2 shows that each of the angles
and length ratios permitted by Proposition 8.6 actually occurs in some root system.
Proof. Suppose that ˛ and ˇ are roots and let m1 D 2h˛; ˇi=h˛; ˛i and m2 D
2hˇ; ˛i=hˇ; ˇi, so that m1 and m2 are integers. Assume h˛; ˛i  hˇ; ˇi and note
that
m1m2 D 4
h˛; ˇi2
h˛; ˛i hˇ; ˇi D 4 cos2 ;
(8.1)
where  is the angle between ˛ and ˇ, and that
m2
m1
D h˛; ˛i
hˇ; ˇi  1
(8.2)

8.2
Examples in Rank Two
201
2
2
s
Fig. 8.2 The projection of ˇ onto ˛ equals ˛=2 and s˛  ˇ equals ˇ  ˛
whenever h˛; ˇi ¤ 0. From (8.1), we conclude that 0  m1m2  4. If m1m2 D 0,
then cos  D 0, so ˛ and ˇ are orthogonal. If m1m2 D 4, then cos2  D 1, which
means that ˛ and ˇ are multiples of one another.
The remaining possible values for m1m2 are 1, 2, and 3. If m1m2 D 1, then
cos2  D 1=4, so  is =3 or 2=3. Since m1 and m2 are both integers, we must
have m1 D 1 and m2 D 1 or m1 D 1 and m2 D 1. In the ﬁrst case, h˛; ˇi > 0
and we have  D =3 and in the second case, h˛; ˇi < 0 and we have  D 2=3.
In either case, (8.2) tells us that ˛ and ˇ have the same length, establishing Case 2
of the proposition.
If m1m2 D 2, then cos2  D 1=2, so that  is =4 or 3=4. Since m1 and m2 are
integers and jm2j  jm1j by (8.2), we must have m1 D 1 and m2 D 2 or m1 D 1
and m2 D 2. In the ﬁrst case, we have  D =4 and in the second case,  D 3=4.
In either case, (8.2) tells us that ˛ is longer than ˇ by a factor of
p
2. The analysis
of the case m1m2 D 3 is similar.
ut
Corollary 8.7. Suppose ˛ and ˇ are roots. If the angle between ˛ and ˇ is strictly
obtuse (i.e., strictly between =2 and ), then ˛ C ˇ is a root. If the angle between
˛ and ˇ is strictly acute (i.e., strictly between 0 and =2), then ˛  ˇ and ˇ  ˛ are
also roots.
See Figure 8.2.
Proof. The proof is by examining each of the three obtuse angles and each of the
three acute angles allowed by Proposition 8.6. Consider ﬁrst the acute case and
adjust the labeling so that h˛; ˛i  hˇ; ˇi. An examination of Cases 2, 3, and 4 in
the proposition (see Figure 8.1) shows that in each of these cases, the projection of
ˇ onto ˛ is equal to ˛=2. Thus, s˛  ˇ D ˇ  ˛ is again a root—and so, therefore,
is .ˇ  ˛/ D ˛  ˇ. In the obtuse case (with h˛; ˛i  hˇ; ˇi), the projection of ˇ
onto ˛ equals ˛=2, and, thus, s˛  ˇ D ˛ C ˇ is again a root.
ut
8.2
Examples in Rank Two
If the rank of the root system is one, then there is only one possibility: R must
consist of a pair f˛; ˛g, where ˛ is a nonzero element of E. In rank two, there are
four possibilities, pictured in Figure 8.3 with their conventional names. In the case

202
8
Root Systems
A1 A1
A2
B2
G2
Fig. 8.3 The rank-two root systems
of A1  A1, the lengths of the horizontal roots are unrelated to the lengths of the
vertical roots. In A2, all roots have the same length; in B2, the length of the longer
roots is
p
2 times the length of the shorter roots; in G2, the length of the longer roots
is
p
3 times the length of the shorter roots. The angle between successive roots is
=2 for A1  A1, =3 for A2, =4 for B2, and =6 for G2. It is easy to check
that each of these systems is actually a root system; in particular, Proposition 8.6 is
satisﬁed for each pair of roots.
Proposition 8.8. Every rank-two root system is isomorphic to one of the systems in
Figure 8.3.
Proof. It is harmless to assume E D R2; thus, let R  R2 be a root system. Let 
be the smallest angle occurring between any two vectors in R. Since the elements of
R span R2, we can ﬁnd two linearly independent vectors ˛ and ˇ in R. If the angle
between ˛ and ˇ is greater than =2, then the angle between ˛ and ˇ is less than
=2; thus, the minimum angle  is at most =2. Then, according to Proposition 8.6,
 must be one of the following: =2, =3, =4, =6.
Let ˛ and ˇ be two elements of R such that the angle between them is the
minimum angle . Then the vector sˇ  ˛ will be a vector that is at angle  to
ˇ but on the opposite side of ˇ from ˛, as shown in Figure 8.4. Thus, sˇ  ˛ is at

8.2
Examples in Rank Two
203
Fig. 8.4 The root sˇ  ˛ is
at angle 2 from ˛
s
angle 2 to ˛. Similarly, ssˇ˛  ˇ is at angle 3 to ˛. Continuing in the same way,
we can obtain vectors at angle n to ˛ for all n. These vectors are unique since a
nontrivial positive multiple of a root is not allowed to be a root. Now, since each
of the allowed values of  evenly divides 2, we will eventually come around to ˛
again. Furthermore, there cannot be any other vectors besides those at angles n to
˛, or else there would be an angle smaller than .
Thus, R must consist of n equally spaced vectors, with consecutive vectors
separated by angle , where  is one of the acute angles in Proposition 8.6. If,
say,  D =4, then in order to satisfy the length requirement in Proposition 8.6, the
roots must alternate between a shorter length and a second length that is greater by
a factor of
p
2. Thus, our root system must be isomorphic to B2. Similar reasoning
shows that all remaining values of  yield one of the root systems in Figure 8.3. ut
Using the results of Sect. 7.7, we can see that the root systems A1  A1, A2, and
B2 arise as root systems of classical Lie algebras as follows. The root system A1A1
is the root system of so.4I C/, which is isomorphic to sl.2I C/ ˚ sl.2I C/; A2 is the
root system of sl.3I C/; and B2 is the root system of so.5I C/, which is isomorphic
to the root system of sp.2I C/. The root system G2, meanwhile, is the root system
of an "exceptional" Lie algebra, which is also referred to as G2 (Sect. 8.11).
Proposition 8.9. If R is a rank-two root system with minimum angle  D 2=n,
n D 4; 6; 8; 12, then the Weyl group of R is the symmetry group of a regular
n=2-gon.
Proof. The group W will contain at least n=2 reﬂections, one for each pair ˙˛ of
roots. If ˛ and ˇ are roots with some angle  between them, then the composition
of the reﬂections s˛ and sˇ will be a rotation by angle ˙2, with the direction of
the rotation depending on the order of the composition. To see this, note that s˛

204
8
Root Systems
and sˇ both have determinant 1 and so s˛sˇ has determinant 1 and is, therefore,
a rotation by some angle. To determine the angle, it sufﬁces to apply sˇs˛ to any
nonzero vector, for example, ˛. However, s˛  ˛ D ˛ and sˇ  .˛/ is at angle 2
to ˛, as in Figure 8.4.
Now, since  can be any integer multiple of , we obtain all rotations by integer
multiples of 2. Meanwhile, the composition of a reﬂection s˛ and a rotation by
angle 2 will be another reﬂection sˇ, where ˇ is a root at angle  to ˛, as the
reader may verify. Thus, the set of n=2 reﬂections together with rotations by integer
multiples of 2 form a group; this is the Weyl group of the rank-two root system.
But this group is also the symmetry group of a regular n=2-gon, also known as the
dihedral group on n=2 elements.
ut
Note that in the case of A2, the Weyl group consists of three reﬂections together
with three rotations (by multiples of 2=3). In this case, the Weyl group is not the
full symmetry group of the root system: Rotations by =3 map R onto itself but are
not elements of the Weyl group.
8.3
Duality
In this section, we introduce an important duality operation on root systems.
Deﬁnition 8.10. If .E; R/ is a root system, then for each root ˛ 2 R, the coroot
H˛ is the vector given by
H˛ D 2
˛
h˛; ˛i:
The set of all coroots is denoted R_ and is called the dual root system to R.
This deﬁnition is consistent with the use of the term "coroot" in Chapter 7; see
Deﬁnition 7.28. Point 4 in the deﬁnition of a root system may be restated as saying
that hˇ; H˛i should be an integer for all roots ˛ and ˇ.
Proposition 8.11. If R is a root system, then R_ is also a root system and the Weyl
group for R_ is the same as the Weyl group for R. Furthermore, .R_/_ D R.
Proof. We compute that
hH˛; H˛i D 4 h˛; ˛i
h˛; ˛i2 D
4
h˛; ˛i
and, therefore,
2
H˛
hH˛; H˛i D 2
 2˛
h˛; ˛i
 h˛; ˛i
4
D ˛:
(8.3)

8.3
Duality
205
If we take the inner product of (8.3) with Hˇ, we see that
2
˝
H˛; Hˇ
˛
hH˛; H˛i D
˝
˛; Hˇ
˛
D 2h˛; ˇi
hˇ; ˇi;
(8.4)
which means that the left-hand side of (8.4) is an integer.
Furthermore, since H˛ is a multiple of ˛, it is evident that s˛ D sH˛. Since, also,
s˛ is an orthogonal transformation, we have
s˛  Hˇ D 2 s˛  ˇ
hˇ; ˇi D 2
s˛  ˇ
hs˛  ˇ; s˛  ˇi D Hs˛ˇ:
Thus, the set of coroots is invariant under each reﬂection s˛ (D sH˛). This
observation, together with (8.4), shows that R_ is again a root system, with the
remaining properties of root systems for R_ following immediately from the
corresponding properties of R.
Since s˛ D sH˛, we see that R and R_ have the same Weyl group. Finally, (8.3)
says that the formula for ˛ in terms of H˛ is the same as the formula for H˛ in terms
of ˛. Thus, HH˛ D ˛ and .R_/_ D R.
ut
Note from (8.4) that the integer associated to the pair .H˛; Hˇ/ in R_ is the same
as the integer associated to the pair .ˇ; ˛/ (not .˛; ˇ/) in R. If all the roots in R
have the same length, R_ is isomorphic to R. Even if not all the roots have the same
length, R and R_ could be isomorphic. In the case of B2, for example, the dual
root system R_ can be converted back to R by a =4 rotation and a scaling. (See
Figure 8.5.) On the other hand, the rank-three root systems B3 and C3 (Sect. 8.9)
are dual to each other but not isomorphic to each other.
Figure 8.5 shows the dual root system for the root system B2. On the left-hand
side of the ﬁgure, the long roots have been normalized to have length
p
2. Thus, for
each long root we have H˛ D ˛ and for each short root we have H˛ D 2˛, yielding
the root system on the right-hand side of the ﬁgure.
Fig. 8.5 The B2 root system and its dual

206
8
Root Systems
8.4
Bases and Weyl Chambers
We now introduce the notion of a base, or a system of positive simple roots, for a
root system.
Deﬁnition 8.12. If .E; R/ is a root system, a subset 
 of R is called a base if the
following conditions hold:
1. 
 is a basis for E as a vector space.
2. Each root ˛ 2 R can be expressed as a linear combination of elements of 
with integer coefﬁcients and in such a way that the coefﬁcients are either all
non-negative or all nonpositive.
The roots for which the coefﬁcients are non-negative are called positive roots
and the others are called negative roots (relative to the base 
). The set of positive
roots relative to a ﬁxed base 
 is denoted RC and the set of negative roots is denoted
R. The elements of 
 are called the positive simple roots.
Note that since 
 is a basis for E, each ˛ can be expressed uniquely as a linear
combination the elements of 
. We require that 
 be such that the coefﬁcients in
the expansion of each ˛ 2 R be integers and such that all the nonzero coefﬁcients
have the same sign.
Proposition 8.13. If ˛ and ˇ are distinct elements of a base 
 for R, then
h˛; ˇi  0.
Geometrically, this means that either ˛ and ˇ are orthogonal or the angle between
them is obtuse.
Proof. Since ˛ ¤ ˇ, if we had h˛; ˇi > 0, then the angle between ˛ and ˇ would be
strictly between 0 and =2. Then, by Corollary 8.7, ˛ˇ would be an element of R.
Since the elements of 
 form a basis for E as a vector space, each element of R has
a unique expansion in terms of elements of 
, and the coefﬁcients of that expansion
are supposed to be either all non-negative or all nonpositive. The expansion of ˛ˇ,
however, has one positive and one negative coefﬁcient. Thus, ˛ˇ cannot be a root,
which means that h˛; ˇi  0.
ut
The reader is invited to ﬁnd a base for each of the rank-two root systems in
Figure 8.3. We now show that every root system has a base.
Proposition 8.14. There exists a hyperplane V through the origin in E that does
not contain any element of R.
Proof. For each ˛ 2 R, let V˛ denote the hyperplane
V˛ D fH 2 Ej h˛; Hi D 0g:
Since there are only ﬁnitely many of these hyperplanes, there exists H 2 E not in
any V˛. (See Exercise 2.) Let V be the hyperplane through the origin orthogonal to

8.4
Bases and Weyl Chambers
207
H. Since H is not in any V˛, we see that H is not orthogonal to any ˛, which means
that no ˛ is in V .
ut
Deﬁnition 8.15. Let .E; R/ be a root system. Let V be a hyperplane through the
origin in E such that V does not contain any root. Choose one "side" of V and let
RC denote the set of roots on this side of V . An element ˛ of RC is decomposable
if there exist ˇ and  in RC such that ˛ D ˇ C ; if no such elements exist, ˛ is
indecomposable.
The "sides" of V can be deﬁned as the connected components of the set E n V .
Alternatively, if H is a nonzero vector orthogonal to V , then V is the set of  2 E
for which h; Hi D 0. The two "sides" of V are then the sets f 2 Ej h; Hi > 0g
and f 2 Ej h; Hi < 0g.
Theorem 8.16. Suppose .E; R/ is a root system, V is a hyperplane through the
origin in E not containing any element of R, and RC is the set of roots lying on a
ﬁxed side of V . Then the set of indecomposable elements of RC is a base for R.
This construction of a base motivates the term "positive simple root" for the
elements of a base. We ﬁrst deﬁne the positive roots (RC) as the roots on one side of
V and then deﬁne the positive simple roots (the base) as the set of indecomposable
(or "simple") elements of RC.
Proof. Let 
 denote the set of indecomposable elements in RC. Choose a nonzero
vector H orthogonal to V so that the chosen side of V is the set of  2 E for which
hH; i > 0.
Step 1: Every ˛ 2 RC can be expressed as a linear combination of elements of

 with non-negative integer coefﬁcients. If not, then among all of the elements
of RC that cannot be expressed in this way, choose ˛ so that hH; ˛i is as small
as possible. Certainly ˛ cannot be an element of 
, so ˛ must be decomposable,
˛ D ˇ1 C ˇ2, with ˇ1; ˇ2 2 RC. Now, ˇ1 and ˇ2 cannot both be expressible as
linear combinations of elements of 
 with non-negative integer coefﬁcients, or
else ˛ would be expressible in this way. However, hH; ˛i D hH; ˇ1i C hH; ˇ2i,
and since the numbers hH; ˇ1i and hH; ˇ2i are both positive, they must be
smaller than hH; ˛i, contradicting the minimality of ˛.
Step 2: If ˛ and ˇ are distinct elements of 
, then h˛; ˇi  0. Note that since

 is not yet known to be a base, Proposition 8.13 does not apply. Nevertheless,
if we had h˛; ˇi > 0, then by Corollary 8.7, ˛  ˇ and ˇ  ˛ would both be
roots, one of which would have to be in RC. If ˛ ˇ were in RC, we would have
˛ D .˛ˇ/Cˇ and ˛ would be decomposable. If, on the other hand, ˇ˛ were
in RC, we would have ˇ D .ˇ  ˛/ C ˛, and ˛ would, again, be decomposable.
Since ˛ and ˇ are assumed indecomposable, we must have h˛; ˇi  0.
Step 3: The elements of 
 are linearly independent. Suppose we have
X
˛2
c˛˛ D 0
(8.5)

208
8
Root Systems
for some collection of constants c˛. Separate the sum into those terms where
c˛  0 and those where c˛ D d˛ < 0, so that
X
c˛˛ D
X
dˇˇ
(8.6)
where the sums range over disjoint subsets of 
. If u denotes the vector in (8.6),
we have
hu; ui D
DX
c˛˛;
X
dˇˇ
E
D
X X
c˛dˇh˛; ˇi:
However, c˛ and d˛ are non-negativeand (by Step 2) h˛; ˇi  0. Thus, hu; ui  0
and u must be zero.
Now, if u D 0, then hH; ui D P c˛ hH; ˛i D 0, which implies that all the c˛'s
are zero since c˛  0 and hH; ˛i > 0. The same argument then shows that all
the d˛'s are zero as well.
Step 4: 
 is a base. We have shown that 
 is linearly independent and that all
of the elements of RC can be expressed as linear combinations of elements of 
with non-negative integer coefﬁcients. The remaining elements of R, namely the
elements of R, are simply the negatives of the elements of RC, and so they can
be expressed as linear combinations of elements of 
 with nonpositive integer
coefﬁcients. Since the elements of R span E, then 
 must also span E and it is
a base.
ut
Figure 8.6 illustrates Theorem 8.16 in the case of the G2 root system, with ˛1
and ˛2 being the indecomposable roots on one side of the dashed line.
Theorem 8.17. For any base 
 for R, there exists a hyperplane V and a side of V
such that 
 arises as in Theorem 8.16.
Proof. If 
 D f˛1; : : : ; ˛rg is a base for R, then 
 is a basis for E in the vector
space sense. Then, by elementary linear algebra, for any sequence of numbers
c1; : : : ; cr there exists a unique  2 E with
˝
; ˛j
˛
D cj , j D 1; : : : ; r. In particular,
we can choose  so that
˝
; ˛j
˛
> 0 for all j. Then if RC denotes the positive
roots with respect to 
, we will have h; ˛i > 0 for all ˛ 2 RC, since ˛ is a
linear combination of elements of 
 with non-negative coefﬁcients. Thus, all of the
elements of RC lie on the same side of the hyperplane V D fH 2 E jh; Hi D 0g.
Suppose now that ˛ is an element of 
 and that ˛ were expressible as a sum
of at least two elements of RC. Then at least one of these elements would be
distinct from ˛ and, thus, not a multiple of ˛. Thus, ˛ would be expressible as a
linear combination of the elements of 
 with non-negative coefﬁcients, where the
coefﬁcient of some ˇ ¤ ˛ would have to be nonzero. This would contradict the
independence of the elements of 
. We conclude, then, that every ˛ 2 
 is an
indecomposable element of RC. Thus, 
 is contained in the base associated to V as

8.4
Bases and Weyl Chambers
209
1
2
2
1
2 2
1
2 3
1
2
2 3
1
Fig. 8.6 The roots ˛1 and ˛2 form a base for the root system G2
in Theorem 8.16. However, the number of elements in a base must equal dim E, so,
actually, 
 is the base associated to V .
ut
Proposition 8.18. If 
 is a base for R, then the set of all coroots H˛, ˛ 2 
, is a
base for the dual root system R_.
Figure 8.7 illustrates Proposition 8.18 in the case of the root system B2.
Lemma 8.19. Let 
 be a base, RC the associated set of positive roots, and ˛ an
element of 
. Then ˛ cannot be expressed as a linear combination of elements of
RC n f˛g with non-negative real coefﬁcients.
Proof. Let ˛1 D ˛ and let ˛2; : : : ; ˛r be the remaining elements of 
. Suppose
˛1 is a linear combination elements ˇ ¤ ˛1 in RC with non-negative coefﬁcients.
Each such ˇ can then be expanded in terms of ˛1; : : : ; ˛r with non-negative(integer)
coefﬁcients. Thus, we end up with
˛1 D c1˛1 C c2˛2 C    C cr˛r;

210
8
Root Systems
1
2
1
2
2
1
2
H 1
H 2
H 1 2H 2
H 1 H 2
Fig. 8.7 Bases for B2 and its dual
with each cj  0. Since ˛1; : : : ; ˛r are independent, we must have c1 D 1 and
all other cj 's equal 0. But since each ˇ in the expansion of ˛ has non-negative
coefﬁcients in the basis ˛1; : : : ; ˛r, each ˇ would have to be a multiple of ˛1,
and thus actually equal to ˛1, since the only multiple of ˛1 in RC is ˛1. But this
contradicts the assumption that ˇ was different from ˛1.
ut
Proof of Proposition 8.18. Choose a hyperplane V such that the base 
 for R arises
as in Theorem 8.16, and call the side of V on which 
 lies the positive side. Let RC
denote the set of positive roots in R relative to the base 
. Then the coroots H˛,
˛ 2 RC, also lie on the positive side of V , and all the remaining coroots lie on the
negative side of V . Thus, applying Theorem 8.16 to R_, there exists a base 
_ for
R_ such that the positive roots associated to 
 are precisely the H˛'s with ˛ 2 RC.
Now, if ˛ 2 RC but ˛ ... 
, then ˛ is a linear combination of ˛1; : : : ; ˛r with
non-negative integer coefﬁcients, at least two of which are nonzero. Thus, H˛ is a
linear combination of H˛1; : : : ; H˛r with non-negative real coefﬁcients, at least two
of which are nonzero. Since, by Lemma 8.19, such an H˛ cannot be in 
_, the r
elements of 
_ must be precisely H˛1; : : : ; H˛r .
ut
Deﬁnition 8.20. The open Weyl chambers for a root system .E; R/ are the
connected components of
E 
[
˛2R
V˛;
where V˛ is the hyperplane through the origin orthogonal to ˛. If 
 D f˛1; : : : ; ˛rg
is a base for R, then the open fundamental Weyl chamber in E (relative to 
) is
the set of all H in E such that
˝
˛j ; H
˛
> 0 for all j D 1; : : : ; r.
Figure 8.8 shows that open fundamental Weyl chamber C associated to a
particular base for the A2 root system. Note that the boundary of C is made up
of portions of the lines orthogonal to the roots (not the lines through the roots).
Since the elements of a base 
 form a basis for E as a vector space, elementary
linear algebra shows that the open fundamental Weyl chamber is convex, hence
connected, and nonempty. Since the only way one can exit a fundamental Weyl

8.4
Bases and Weyl Chambers
211
C
1
2
Fig. 8.8 The shaded region C is the open fundamental Weyl chamber associated to the base
f˛1; ˛2g for A2
chamber is by passing through a point H where
˝
˛j ; H
˛
D 0, the open fundamental
Weyl chamber is, indeed, an open Weyl chamber. Note, also, that if
˝
˛j; H
˛
> 0 for
all j D 1; : : : ; r, then h˛; Hi > 0 for all ˛ 2 RC, since ˛ is a linear combination
of ˛1; : : : ; ˛r with non-negative coefﬁcients.
Each w 2 W is an orthogonal linear transformation that maps R to R and, thus,
maps the set of hyperplanes orthogonal to the roots to itself. It then easily follows
that for each open Weyl chamber C, the set w  C is another open Weyl chamber.
For any base 
 and the associated set RC of positive roots, we have deﬁned the
fundamental Weyl chamber to be the set of those elements having positive inner
product with each element of 
, and therefore also with each element of RC. Our
next result says that we can reverse this process.
Proposition 8.21. For each open Weyl chamber C, there exists a unique base 
C
for R such that C is the open fundamental Weyl chamber associated to 
C. The
positive roots with respect to 
C are precisely those elements ˛ of R such that ˛
has positive inner product with each element of C.
Thus, there is a one-to-one correspondence between bases and Weyl chambers.
Proof. Let H be any element of C and let V be the hyperplane orthogonal to H.
Since H is contained in an open chamber, H is not orthogonal to any root, and,
thus, V does not contain any root. Thus, by Theorem 8.16, there exists a base

 D f˛1; : : : ; ˛rg lying on the same side of V as H. Since
˝
˛j ; H
˛
has constant
sign on C, we see that every element of C has positive inner product with each

212
8
Root Systems
element of 
 and thus with every element of the associated set RC of positive roots.
Thus, C must be the fundamental Weyl chamber associated to 
. We have just
said that every ˛ 2 RC has positive inner product with every element of C, which
means that each ˛ 2 R has negative inner product with each element of C. Thus,
RC consists precisely of those roots having positive inner product with C.
Finally, if 
0 is any base whose fundamental chamber is C, then each element
of 
0 has positive inner product with H 2 C, meaning that 
0 lies entirely on the
same side of V as 
. Thus, 
0 has the same positive roots as 
 and therefore also
the same set of positive simple (i.e., indecomposable) roots as 
. That is to say,

0 D 
.
ut
Proposition 8.22. Every root is an element of some base.
Since C is the set of H 2 E for which
˝
˛j ; H
˛
> 0 for j
D 1; : : : ; r,
the codimension-one pieces of the boundary of C will consist of portions of the
hyperplanes V˛j orthogonal to the elements of 
. Thus, to prove the proposition,
we merely need to show that for every root ˛, the hyperplane V˛ contains a
codimension-one piece of the boundary of some C.
Proof. Let ˛ be a root and V˛ the hyperplane orthogonal to ˛. If we apply Exercise 2
to V˛ we see that there exists some H 2 V˛ such that H does not belong to Vˇ for
any root ˇ other than ˇ D ˙˛. Then for small positive ", the element H C "˛ will
be in a open Weyl chamber C.
We now claim that ˛ must be a member of the base 
C D f˛1; : : : ; ˛rg in
Proposition 8.21. Since ˛ has positive inner product with H C "˛ 2 C, we
must at least have ˛ 2 RC, the set of positive roots associated to 
C. Write
˛ D Pr
j D1 cj ˛j , with cj  0, so that
0 D h˛; Hi D
r
X
j D1
cj
˝
˛j ; H
˛
:
Since H is clearly in the closure of C, we must have
˝
H; ˛j
˛
 0, which means that
˝
˛j ; H
˛
must be zero whenever cj ¤ 0. If more than one of the cj 's were nonzero,
H would be orthogonal to two distinct roots in 
, contradicting our choice of H.
Thus, actually, ˛ is in 
.
ut
8.5
Weyl Chambers and the Weyl Group
In this section, we establish several results about how the action of the Weyl group
relates to the Weyl chambers.

8.5
Weyl Chambers and the Weyl Group
213
Proposition 8.23. The Weyl group acts transitively on the set of open Weyl
chambers.
Proposition 8.24. If 
 is a base, then W is generated by the reﬂections s˛ with
˛ 2 
.
Proof of Propositions 8.23 and 8.24. Fix a base 
, let C be the fundamental cham-
ber associated to 
, and let W 0 be the subgroup of W generated by the s˛'s with
˛ 2 
. Let D be any other chamber and let H and H 0 be ﬁxed elements of C and
D, respectively. We wish to show that there exists w 2 W 0 so that w  H 0 belongs to
C. To this end, choose w 2 W 0 so that jw  H 0  Hj is minimized, which is possible
because W 0  W is ﬁnite. If wH 0 were not in C, there would be some ˛ 2 
 such
that h˛; w  H 0i < 0. In that case, direct calculation would show that
ˇˇw  H 0  H
ˇˇ2 
ˇˇs˛  w  H 0  H
ˇˇ2 D 4 ˝˛; w  H 0˛
h˛; Hi > 0;
which means that s˛ wH 0 is closer to H than wH 0 is (Figure 8.9). Since s˛ 2 W 0,
this situation would contradict the minimality of w. Thus, actually, w  H 0 2 C,
showing that D can be mapped to C by an element of W 0. Since any chamber
can be mapped to C by an element of W 0, any chamber can be mapped to any
other chamber by an element of W 0. We conclude that W 0, and thus also W , acts
transitively on the chambers, proving Proposition 8.23.
H
w H
s
1 w H
1
2
Fig. 8.9 If w  H 0 and H are on opposite sides of the line orthogonal to ˛1, then s˛1  w  H 0 is
closer to H than w  H 0 is

214
8
Root Systems
To prove Proposition 8.24, we must show that W 0 D W . Let s˛ be the reﬂection
associated to an arbitrary root ˛. By Proposition 8.22, ˛ belongs to 
D for some
chamber D. If w 2 W 0 is chosen so that w  D D C, then w  ˛ will belong to 
.
Now, it is easily seen that
sw˛ D ws˛w1;
so that s˛ D w1sw˛w. But since w  ˛ 2 
, both w and sw˛ belong to W 0. Thus,
s˛ belongs to W 0 for every root ˛, which means that W (the group generated by the
s˛'s) equals W 0.
ut
Proposition 8.25. Let C be a Weyl chamber and let H and H 0 be elements of NC,
the closure of C. If w  H D H 0 for some w 2 W , then H D H 0.
That is to say, two distinct elements of NC cannot be in the same orbit of W .
By Proposition 8.24, each w 2 W can be written as a product of reﬂections from

. If k  0 is the smallest number of reﬂections from 
 needed to express w, then
any expression for w as the product of k reﬂections from 
 is called a minimal
expression for w. Such a minimal expression need not be unique.
The following technical lemma is the key to the proof of Proposition 8.25.
Lemma 8.26. Let 
 be a base for R and let C be the associated fundamental Weyl
chamber. Let w be an element of W with w ¤ I and let w D s˛1s˛2    s˛k, with
˛j 2 
, be a minimal expression for w. Then C and w  C lie on opposite sides of
the hyperplane V˛1 orthogonal to ˛1.
Proof. Since w ¤ I, we must have k  1. If k D 1, then w D s˛1, so that
w  C D s˛1  C is on the opposite side of V˛1 from C. Assume, inductively, that the
result holds for u 2 W where the minimal number of reﬂections needed to express
u is k  1. Then consider w 2 W having a minimal expression of the form w D
s˛1    s˛k. If we let u D s˛1    s˛k1, then s˛1    s˛k1 must be a minimal expression
for u, since any shorter expression for u would result in a shorter expression for
w D us˛k. Thus, by induction, u  C and C must lie on opposite sides of V˛1.
Suppose, toward a contradiction, that w  C lies on the same side of V˛1 as C. Then
u  C and w  C D us˛k  C lie on opposite sides of V˛1, which implies that C and
s˛k  C lie on opposite sides of u1V˛1 D Vu1˛1.
We now claim that there is only one hyperplane Vˇ, with ˇ 2 R, such that C
and s˛k  C lie on opposite sides of Vˇ, namely, Vˇ D V˛k. After all, as in the
proof of Proposition 8.22, we can choose H in the boundary of C so that H lies in
V˛k but in no other hyperplane orthogonal to a root. We may then pass from C to
s˛k  C along a line segment of the form H C t˛k, " < t < ", and we will pass
through no hyperplane orthogonal to a root, other than V˛k. We conclude, then, that
Vu1˛1 D V˛k.
Now, if Vu1˛1 D V˛k, it follows that su1˛1 D s˛k, so that
s˛k D su1˛1 D u1s˛1u:
(8.7)

8.5
Weyl Chambers and the Weyl Group
215
Substituting (8.7) into the formula w D us˛k gives
w D s˛1u D s2
˛1s˛2    s˛k1:
Since s2
˛ D I, we conclude that w D s˛2    s˛k1, which contradicts the assumption
that s˛1    s˛k was a minimal expression for w.
ut
Proof of Proposition 8.25. We proceed by induction on the minimal number of
reﬂections from 
C needed to express w. If the minimal number is zero, then w D I
and the result holds. If the minimal number is greater than zero, let w D s˛1    s˛k
be a minimal expression for w. By Lemma 8.26, C and w  C lie on opposite sides
of the hyperplane V˛1 orthogonal to ˛1. Thus,
.w  NC/ \ NC  V˛1;
which means that w  H D H 0 must lie in V˛1. It follows that
s˛1  w  H D s˛1  H 0 D H 0:
That is to say, the Weyl group element w0 WD s˛1w also maps H to H 0. But
w0 D s˛1w D s2
˛1s˛2    s˛k D s˛2    s˛k
is a product of fewer than k reﬂections from 
C. Thus, by induction, we have
H 0 D H.
ut
Proposition 8.27. The Weyl group acts freely on the set of open Weyl chambers. If
H belongs to an open chamber C and w  H D H for some w 2 W , then w D I.
Proof. Suppose w  C D C for some w 2 W and some chamber C. Then w  H
belongs to C for all H 2 C, which means, by Proposition 8.25, that w  H D H for
all H 2 C. Thus, w is the identity on all of C, which means w must be the identity
on all of E. After all, if w is not the identity, the eigenspace of w with eigenvalue 1
is a subspace of E of codimension at least 1, and this eigenspace cannot contain the
nonempty open set C.
Meanwhile, if H belongs to an open chamber C and wH D H, then wC must
equal C, so that w D I.
ut
Proposition 8.28. For any two bases 
1 and 
2 for R, there exists a unique w 2 W
such that w  
1 D 
2.
Proof. By Proposition 8.21, there is a bijective correspondence between bases and
open Weyl chambers, and this correspondence is easily seen to respect the action of
the Weyl group. Since, by Propositions 8.23 and 8.27, W acts freely and transitively
on the chambers, the same is true for bases.
ut
Proposition 8.29. Let C be a Weyl chamber and H an element of E. Then there
exists exactly one point in the W -orbit of H that lies in the closure NC of C.

216
8
Root Systems
We are not saying that there is a unique w such that w  H 2 NC, but rather that
there exists a unique point H 0 2 NC such that H 0 can be expressed (not necessarily
uniquely) as H 0 D w  H.
Proof. If U is any neighborhood of H, then by the argument in Exercise 2, the
hyperplanes V˛, ˛ 2 R, cannot ﬁll up U , which means U contains points in some
open Weyl chamber. It follows that H belongs to ND for some chamber D. By
Proposition 8.23, there exists w 2 W such that w  D D C and, thus, w  ND D NC.
Thus, H 0 WD wH is in NC. Meanwhile, if H 00 is a point in the W -orbit of H such that
H 00 2 NC, then H 0 and H 00 lie in the same W -orbit, which means (Proposition 8.25)
that H 0 D H 00.
ut
Proposition 8.30. Let 
 be a base for R, let RC be the associated set of positive
roots, and let ˛ be an element of 
. If ˇ 2 RC and ˇ ¤ ˛, then s˛  ˇ 2 RC. That
is to say, if ˛ 2 
, then s˛ permutes the positive roots different from ˛.
Proof. Write ˇ D P
2
 c˛ with c  0. Since ˇ ¤ ˛, there is some c with
 ¤ ˛ and c > 0. Now, since s˛  ˇ D ˇ  n˛ for some integer n, in the expansion
of s˛ˇ, only the coefﬁcient of ˛ has changed compared to the expansion of ˇ. Thus,
the coefﬁcient of  in the expansion of s˛  ˇ remains positive. But if one coefﬁcient
of s˛  ˇ is positive, all the other coefﬁcients must be non-negative, showing that
s˛  ˇ is a positive root.
ut
8.6
Dynkin Diagrams
A Dynkin diagram is a convenient graphical way of encoding the structure of a base
for a root system R, and thus also of R itself.
Deﬁnition 8.31. If 
 D f˛1; : : : ; ˛rg is a base for a root system R, the Dynkin
diagram for R is a graph having vertices v1; : : : ; vr. Between two distinct vertices
vj and vk, we place zero, one, two, or three edges according to whether the angle
between ˛j and ˛k is =2, 2=3, 3=4, or 5=6. In addition, if ˛j and ˛k are not
orthogonal and have different lengths, we decorate the edges between vj and vk
with an arrow pointing from the vertex associated to the longer root to the vertex
associated to the shorter root.
Note that by Proposition 8.6, angles of 2=3, 3=4, or 5=6 correspond to
length ratios of 1;
p
2,
p
3, respectively. Thus, the number of edges between vertices
corresponding to two nonorthogonal roots is 1, 2, or 3 according to whether the
length ratio (of the longer to the shorter) is 1,
p
2, or
p
3. Thinking of the arrow
decorating the edges as a "greater than" sign helps one to recall which way the
arrow should go.
Two Dynkin diagrams are said to be isomorphic if there is a one-to-one, onto
map of the vertices of one to the vertices of the other that preserves the number of
bonds and the direction of the arrows. By Proposition 8.28, any two bases 
1 and

8.6
Dynkin Diagrams
217
A2
B2
G2
A1 A1
Fig. 8.10 The Dynkin diagrams for the rank-two root systems

2 for a ﬁxed root system are related by the action of a unique Weyl group element
w. Since w preserves angles and lengths, the Dynkin diagrams associated to two
different bases for the same root system are isomorphic.
In the case of the root system G2, for example, a base consists of two roots at
angle of 5=6, with a length ratio of
p
3 (Figure 8.6). Thus, the Dynkin diagram
consists of two vertices connected by three edges, with an arrow pointing from the
longer root (˛2) to the shorter (˛1). One can similarly read off the Dynkin diagram
for A2 from Figure 6.2 and the diagram for B2 from Figure 8.7. Finally, for A1 A1,
the two elements of the base are orthogonal, yielding the results in Figure 8.10.
Proposition 8.32. 1. A root system is irreducible if and only if its Dynkin diagram
is connected.
2. If the Dynkin diagrams of two root systems R1 and R2 are isomorphic, then R1
and R2 themselves are isomorphic.
Proof. For Point 1, if a root system R decomposes as the direct sum of two root
systems R1 and R2, then we can obtain a base 
 for R as 
 D 
1 [ 
2, where 
1
and 
2 are bases for R1 and R2, respectively. Since elements of R1 are orthogonal
to elements R2, each element of 
1 is orthogonal to each element of 
2. Thus, the
Dynkin diagram associated to 
 is disconnected.
Conversely, suppose the Dynkin diagram of R is disconnected. Then the base 
decomposes into two pieces 
1 and 
2 where (since there are no edges connecting
the pieces), each element of 
1 is orthogonal to each element of 
2. Thus, E is the
orthogonal direct sum of E1 WD span.
1/ and E2 WD span.
2/. If R1 D R \ E1
and R2 D R \ E2, then it easy to check that R1 and R2 are root systems in E1 and
E2, respectively, and that 
1 is a base for R1 and 
2 is a base for R2.
Now, for each ˛ 2 
1, the reﬂection s˛ will act as the identity on E2 and similarly
for ˛ 2 
2. Since the Weyl groups of R, R1, and R2 are generated by the reﬂections
from 
, 
1, and 
2, respectively, we see that the Weyl group W of R is the direct
product of the Weyl groups W1 and W2 of R1 and R2, with W1 acting only on E1
and W2 acting only on E2. Since W acts transitively on the bases of R and every
element of R is part of some base, we see that W 
 D R. But since W D W1 W2,
we have W  
 D .W1  
1/ [ .W2  
2/. Thus, every element of R is either in E1
or in E2, meaning that R is the direct sum of R1 and R2.

218
8
Root Systems
For Point 2, using Point 1, we can reduce the problem to the case in which R1 and
R2 are irreducible and the Dynkin diagrams of R1 and R2 are connected. Let 
1 D
f˛1; : : : ; ˛rg and 
2 D fˇ1; : : : ; ˇrg be bases for R1 and R2, respectively, ordered
so that the isomorphism of the Dynkin diagrams maps the vertex associated to ˛j to
the vertex associated to ˇj . We may rescale the inner product on R1 so that k˛1k D
kˇ1k. Since the Dynkin diagrams are connected and the diagram determines the
length ratios between vertices joined by an edge, it follows that
˝
˛j ; ˛k
˛
D
˝
ˇj ; ˇk
˛
for all j and k. It is then easy to check that the unique linear map A W E1 ! E2
such that A˛j D ˇj , j D 1; : : : ; r is an isometry. We then have that As˛j D sˇj A
for all j.
Now, if ˛ is any element of R1, then, since W1 is generated by the reﬂections
from 
1 and W1  
 D R1, we see that
˛ D s˛j1    s˛jN ˛k
for some indices j1; : : : ; jN and k. Thus,
A˛ D sˇj1    sˇjN ˇk
is an element of R2. The same reasoning shows that A1ˇ 2 R1 for all ˇ 2 R2.
Thus, A is an isometry mapping R1 onto R2, which implies that A is an isomorphism
of R1 with R2.
ut
Corollary 8.33. Let g D kC be a semisimple Lie algebra, let h D tC be a Cartan
subalgebra of g, and let R  it be the root system of g relative to h. Then g is simple
if and only if the Dynkin diagram of R is connected.
Proof. According to Theorem 7.35, g is simple if and only if R is irreducible. But
according to Point 1 of Proposition 8.32, R is irreducible if and only if the Dynkin
diagram of R is connected.
ut
8.7
Integral and Dominant Integral Elements
We now introduce a notion of integrality for elements of E. In the setting of the
representations of a semisimple Lie algebra g, the weights of a ﬁnite-dimensional
representation of g are always integral elements. Recall from Deﬁnition 8.10 the
notion of the coroot H˛ associated to a root ˛.
Deﬁnition 8.34. An element  of E is an integral element if for all ˛ in R, the
quantity
h; H˛i D 2h; ˛i
h˛; ˛i

8.7
Integral and Dominant Integral Elements
219
is an integer. If 
 is a base for R, an element  of E is dominant (relative to 
) if
h˛; i  0
for all ˛ 2 
 and strictly dominant if
h˛; i > 0
for all ˛ 2 
.
See Figure 6.2 for the integral and dominant integral elements in the case of A2.
Additional examples will be given shortly.
A point  2 E is strictly dominant relative to 
 if and only if  is contained
in the open fundamental Weyl chamber associated to 
, and  is dominant if
and only if  is contained in the closure of the open fundamental Weyl chamber.
Proposition 8.29 therefore implies the following result: For all  2 E, there exists
w 2 W such that w   is dominant.
Note that by the deﬁnition of a root system, every root is an integral element.
Thus, every integer linear combination of roots is also an integral element. In most
cases, however, there exist integral elements that are not expressible as an integer
combination of roots. In the case of A2, for example, the elements labeled 1 and
2 in Figure 6.2 are integral, but their expansions in terms of ˛1 and ˛2 are 1 D
2˛1=3 C ˛2=3 and 2 D ˛1=3 C 2˛2=3. Since ˛1 and ˛2 form a base for A2, if 1
or 2 were an integer combination of roots, it would also be an integer combination
of ˛1 and ˛2.
Proposition 8.35. If  2 E has the property that
2h; ˛i
h˛; ˛i
is an integer for all ˛ 2 
, then the same holds for all ˛ 2 R and, thus,  is an
integral element.
Proof. An element  is integral if and only if h; H˛i is an integer for all ˛ 2 R.
By Proposition 8.18, each H˛ with ˛ 2 R can be expressed as a linear combination
of the H˛'s with ˛ 2 
, with integer coefﬁcients. Thus, if h; H˛i 2 Z for ˛ 2 
,
then same is true for ˛ 2 R, showing that  is integral.
ut
Deﬁnition 8.36. Let 
 D f˛1; : : : ; ˛rg be a base. Then the fundamental weights
(relative to 
) are the elements 1; : : : ; r with the property that
2
˝
j ; ˛k
˛
h˛k; ˛ki D ıjk;
j; k D 1; : : : ; r:
(8.8)
Elementary linear algebra shows there exists a unique set of integral elements
satisfying (8.8). Geometrically, the jth fundamental weight is the unique element

220
8
Root Systems
of E that is orthogonal to each ˛j, j ¤ k, and whose orthogonal projection onto
˛j is one-half of ˛j . Note that the set of dominant integral elements is precisely the
set of linear combinations of the fundamental weights with non-negative integer
coefﬁcients; the set of all integral elements is the set of linear combinations of
fundamental weights with arbitrary integer coefﬁcients.
Deﬁnition 8.37. Let 
 be a base for R and RC the associated set of positive roots.
We then let ı denote half the sum of the positive roots:
ı D 1
2
X
˛2RC
˛:
The element ı plays a key role in many of the developments in subsequent
chapters. It appears, for example, in the statement of the Weyl character formula
and the Weyl integral formula. Figure 8.11 shows the integral and dominant integral
elements for B2 and G2. In each case, the base f˛1; ˛2g and the element ı are labeled,
and the fundamental weights are circled. The background square lattice (B2 case) or
triangular lattice (G2 case) indicates the set of all integral elements. The root system
G2 is unusual in that the fundamental weights are roots, which means that every
integral element is expressible as an integer combination of roots.
Proposition 8.38. The element ı is a strictly dominant integral element; indeed,
2 hˇ; ıi
hˇ; ˇi D 1
for each ˇ 2 
.
Proof. If ˇ 2 
, then by Proposition 8.30, sˇ permutes the elements of RC different
from ˇ. Thus, we can decompose RC n fˇg as E1 [ E2, where elements of E1 are
orthogonal to ˇ and elements of E2 are not. Then the elements of E2 split up into
pairs f˛; sˇ  ˛g, where
˝
sˇ  ˛; ˇ
˛
D
˝
˛; sˇ  ˇ
˛
D  h˛; ˇi :
Thus, when we compute the inner product of ˇ with half the sum of the positive
roots, the roots in E1 do not contribute and the contributions from roots in E2 cancel
in pairs. Thus, only the contribution from ˇ itself remains:
2 hˇ; ıi
hˇ; ˇi D 2
˝
ˇ; 1
2ˇ
˛
hˇ; ˇi D 1;
as claimed.
ut

8.8
The Partial Ordering
221
Fig. 8.11 Integral and
dominant integral elements
for B2 (top) and G2 (bottom).
The black dots indicate
dominant integral elements,
while the background lattice
indicates the set of all integral
elements
1
2
1
2
1
2
1
2
8.8
The Partial Ordering
We now introduce a partial ordering on the set of integral elements, which will
be used to formulate the theorem of the highest weight for representations of a
semisimple Lie algebra.

222
8
Root Systems
Fig. 8.12 Points that are
higher than zero (light and
dark gray) and points that are
dominant (dark gray) for B2
1
2
Deﬁnition 8.39. If 
 D f˛1; : : : ; ˛rg is a base, an element  2 E is said to be
higher than  2 E (relative to 
) if    can be expressed as
   D c1˛1 C    C cr˛r;
where each cj is a non-negative real number. We equivalently say that  is lower
than  and we write this relation as    or   .
The relation  deﬁnes a partial ordering on E. We now develop various useful
properties of this relation. For the rest of the section, we assume a base 
 for R has
been chosen, and that the notions of higher, lower, and dominant are deﬁned relative
to 
.
Proposition 8.40. If  2 E is dominant, then   0.
See Figure 8.12 for an example of the proposition.
For any basis fv1; : : : ; vrg of E, we can form a basis for the dual space E by
considering the linear functionals j given by j .vk/ D ıjk. We can then ﬁnd unique
vectors v
j 2 E such that j.u/ D
D
v
j ; u
E
, so that
D
v
j ; vk
E
D ıjk:
The basis fv
1 ; : : : ; v
r g for E is called the dual basis to fv1; : : : ; vrg.

8.8
The Partial Ordering
223
Lemma 8.41. Suppose fv1; : : : ; vrg is an obtuse basis for E, meaning that
˝
vj ; vk
˛
 0 for all j ¤ k. Then fv
1 ; : : : ; v
r g is an acute basis for E, meaning that
D
v
j ; v
k
E
 0 for all j; k.
The proof of this elementary lemma is deferred until the end of this section.
Proof of Proposition 8.40. Any vector u can be expanded as u D P
j cj ˛j and the
coefﬁcients may be computed as cj D
D
˛
j ; u
E
, where f˛
j g is the dual basis to f˛jg.
Applying this with u D ˛
j gives
˛
j D
r
X
kD1
D
˛
k ; ˛
j
E
˛k:
Now, if  is dominant, the coefﬁcients in the expansion  D P
j cj ˛j are given by
cj D
D
˛
j ; 
E
D
X
k
D
˛
k ; ˛
j
E
h˛k; i :
Since  is dominant, h˛k; i  0. Furthermore, the ˛j 's form an obtuse basis for
E (Proposition 8.13) and thus by Lemma 8.41, we have
D
˛
k ; ˛
j
E
 0 for all j; k.
Thus, cj  0 for all j, which shows that  is higher than zero.
ut
Proposition 8.42. If  is dominant, then w     for all w 2 W .
Proof. Let O be the Weyl-group orbit of . Since O is a ﬁnite set, it contains a
maximal element , i.e., one such that there is no 0 ¤  in O that is higher than .
Then for all ˛ 2 
, we must have h˛; i  0, since if h˛; i were negative, then
s˛   D   2h; ˛i
h˛; ˛i˛
would be higher than . Thus,  is dominant. But since, by Proposition 8.29,  is
the unique dominant element of O, we must have  D . Thus,  is the unique
maximal element of O.
We now claim that every element of O is lower than . Certainly, no element
of O can be higher than . Let O0 be the set of  2 O that are neither higher nor
lower than . If O0 is not empty, it contains a maximal element . We now argue
that  is actually maximal in O. For any  2 O, if  2 O0, then certainly  cannot
be higher than , which is maximal in O0. On the other hand, if  2 O n O0, then
 is lower than , in which case,  cannot be higher than , or else we would have
    , so that  would not be in O0. We conclude that  is maximal in O, not
just in O0, which means that  must equal , the unique maximal element of O. But
this contradicts the assumption that  2 O0. Thus, O0 must actually be empty, and
 is the highest element of O.
ut

224
8
Root Systems
Proposition 8.43. If  is a strictly dominant integral element, then   ı, where ı
is as in Deﬁnition 8.37.
Proof. Since  is strictly dominant,   ı will still be dominant in light of
Proposition 8.38. Thus, by Proposition 8.40,   ı  0, which is equivalent to
  ı.
ut
Recall from Deﬁnition 6.23 the notion of the convex hull of a ﬁnite collection
of vectors in E. We let W   denote the Weyl-group orbit of  2 E and we let
Conv.W  / denote the convex hull of W  .
Proposition 8.44. 1. If  and  are dominant, then  belongs to Conv.W  / if
and only if   .
2. Let  and  be elements of E with  dominant. Then  belongs to Conv.W  /
if and only if w     for all w 2 W .
Figure 8.13 illustrates Point 2 of the proposition in the case of B2. In the ﬁgure,
the shaded region represents the set of points that are lower than . The point 1 is
inside Conv.W  / and w  1 is lower than  for all w. By contrast, 2 is outside
Conv.W  / and there is some w for which w  2 is not lower than .
Since Conv.W  / is convex and Weyl invariant, we see that if  belongs to
Conv.W  /, then every point in Conv.W  / also belongs to Conv.W  /. Thus,
Point 1 of the proposition may be restated as follows:
If  and  are dominant, then    if and only if
Conv.W  /  Conv.W  /:
We establish two lemmas that will lead to a proof of Proposition 8.44.
Lemma 8.45. Suppose K is a compact, convex subset of E and  is an element of
E that it is not in K. Then there is an element  of E such that for all  2 K, we
have
h; i > h; i :
If we let V be the hyperplane (not necessarily through the origin) given by
V D f 2 Ej h; i D h; i  "g
for some small ", then K and  lie on opposite sides of V . Lemma 8.45 is a special
case of the hyperplane separation theorem in the theory of convex sets.
Proof. Since K is compact, we can choose an element 0 of K that minimizes the
distance to . Set  D   0, so that
h;   0i D h  0;   0i > 0;
and, thus, h; i > h; 0i.

8.8
The Partial Ordering
225
1
2
w
2
1
2
Fig. 8.13 The element w  2 is not lower than 
Now, for any  2 K, the vector 0 C s.  0/ belongs to K for 0  s  1, and
we compute that
d.; 0 C s.  0//2 D h  0;   0i  2s h  0;   0i
C s2 h  0;   0i :
The only way this quantity can be greater than or equal to h  0;   0i D
d.; 0/2 for small positive s is if
h  0;   0i D h;   0i  0:
Thus,
h; i  h; 0i < h; i ;
which is what we wanted to prove.
ut
Lemma 8.46. If  and  are dominant and  ... Conv.W  /, there exists a
dominant element  2 E such that
h; i > h; w  i
(8.9)
for all w 2 W .

226
8
Root Systems
Proof. By Lemma 8.45, we can ﬁnd some  in E, not necessarily dominant, such
that h; i > h; i for all  2 Conv.W  /. In particular,
h; i > h; w  i
for all w 2 W . Choose some w0 so that 0 WD w0  0 is dominant. We will show that
replacing  by 0 makes h; i bigger while permuting the values of h; w  i.
By Proposition 8.42,   0, meaning that 0 equals  plus a non-negative linear
combination of positive simple roots. But since  is dominant, it has non-negative
inner product with each positive simple root, and we see that h0; i  h; i. Thus,
˝
0; 
˛
 h; i > h; w  i
for all w. But
h; w  i D
˝
w1
0
 0; w  
˛
D
˝
0; .w0w/  
˛
:
Thus, as w ranges over W , the values of h; w  i and h0; .w0w/  i range through
the same set of real numbers. Thus, h0; i > h0; w  i for all w, as claimed.
ut
The proof of Lemma 8.46 is illustrated in Figure 8.14. The dominant element 
is not in Conv.W  / and is separated from Conv.W  / by a line with orthogonal
vector . The element 0 WD s˛2   is dominant and  is also separated from
1
2
Fig. 8.14 The element  is separated from Conv.W  / ﬁrst by a line orthogonal to  and then
by a line orthogonal to the dominant element 0

8.8
The Partial Ordering
227
Conv.W / by a line with orthogonal vector 0. The existence of such a line means
that  cannot be lower than .
Proof of Proposition 8.44. For Point 1, let  and  be dominant. Assume ﬁrst that
 is in Conv.W  /. By Proposition 8.42, every element of the form w   is lower
than . But the set E of elements lower than  is easily seen to be convex, and
so E must contain Conv.W  / and, in particular, . Next, assume    and
suppose, toward a contradiction, that  ... Conv.W  /. Let  be a dominant
element as in Lemma 8.46. Then    is a non-negative linear combination of
positive simple roots, and , being dominant, has non-negative inner product with
each positive simple root. Thus, h;   i  0 and, hence, h; i  h; i, which
contradicts (8.9). Thus,  must actually belong to Conv.W  /.
For Point 2, assume ﬁrst that w     for all w 2 W , and choose w so that w  
is dominant. Since, w    , Point 1 tells us that w   belongs to Conv.W  /,
which implies that  also belongs to Conv.W  /. In the other direction, assume
 2 Conv.W / so that w 2 Conv.W / for all w 2 W . Using Proposition 8.42
we can easily see that every element of Conv.W / is lower than . Thus, w  
for all w.
ut
It remains only to supply the proof of Lemma 8.41.
Proof of Lemma 8.41. We proceed by induction on the dimension r of E. When
r D 1 the result is trivial. When r D 2, the result should be geometrically obvious,
but we give an algebraic proof. The Gram matrix of a basis is the collection of inner
products, Gjk WD ˝vj ; vk
˛. It is an elementary exercise (Exercise 3) to show that the
Gram matrix of the dual basis is the inverse of the Gram matrix of the original basis.
Thus, in the r D 2 case, we have
˝
v
1 ; v
1
˛ ˝
v
1 ; v
2
˛
˝
v
1 ; v
2
˛ ˝
v
2 ; v
2
˛

D
1
.hv1; v1i hv2; v2i  hv1; v2i2/
 hv2; v2i  hv1; v2i
 hv1; v2i hv1; v1i

:
(8.10)
Since v1 is not a multiple of v2, the Cauchy-Schwarz inequality tells us that the
denominator on the right-hand side of (8.10) is positive, which means that
˝
v
1 ; v
2
˛
has the opposite sign of hv1; v2i.
Assume now that the result holds in dimension r  2 and consider the case of
dimension r C 1. Fix any index m and let P be the orthogonal projection onto the
orthogonal complement of vm, which is given by
P.u/ D u  hvm; ui
hvm; vmivm:
The operator P is easily seen to be self-adjoint, meaning that hu; P vi D hP u; vi
for all u; v.

228
8
Root Systems
We now claim that Pv1; : : : ;b
Pvm; : : : ; PvrC1 is an obtuse basis for hvmi?, where
the notation b
Pvm indicates that P vm is omitted. Indeed, a little algebra shows that
˝
Pvj ; Pvk
˛
D
˝
vj ; vk
˛

˝
vm; vj
˛
hvm; vki
hvm; vmi
 0;
since
˝
vj ; vk
˛
,
˝
vm; vj
˛
, and hvm; vki are all less than or equal to zero. Furthermore,
for j and k different from m, we have
D
v
j ; Pvk
E
D
D
Pv
j ; vk
E
D
D
v
j ; vk
E
D ıjk
since v
j is orthogonal to vm. Thus, the dual basis to Pv1; : : : ;b
Pvm; : : : ; PvrC1
consists simply of the vectors v
1 ; : : : ; c
v
m; : : : ; v
rC1 (all of which are orthogonal
to vm).
Now ﬁx any two distinct indices j and k. Since r C 1  3, we can choose some
other index m distinct from both j and k. Applying our induction hypothesis to the
basis P v1; : : : ;b
Pvm; : : : ; PvrC1 for hvmi?, we conclude that
D
v
j ; v
k
E
 0, which is
what we are trying to prove.
ut
8.9
Examples in Rank Three
In rank three, we can have a reducible root system, which must be a direct sum of
A1 with one of the rank-two root systems described in the previous section. In this
section, we will consider only the irreducible root systems of rank three. There are,
up to isomorphism, three irreducible root systems in rank three, customarily denoted
A3, B3, and C3. They arise from the Lie algebras sl.4I C/, so.7I C/, and sp.3I C/,
respectively, as described in Sect. 7.7.
The models in this section can be constructed using the Zometool system,
available at www.zometool.com. The reader is encouraged to obtain some Zometool
pieces and build the rank-three root systems for him- or herself. The models require
the green lines, which are not part of the basic Zometool kits. The models of the
C3 root system use half-length greens, although one can alternatively use whole
greens together with double (end to end) blue pieces. The images shown here
were rendered in Scott Vorthmann's vZome software, available at vzome.com. For
detailed instructions on how to build rank-three root systems using Zometool, click
on the "Book" tab of the author's web site: www.nd.edu/~bhall/.
Figure 8.15 shows the A3 root system, with a base highlighted. The elements
of A3 form the vertices of a polyhedron known as a cuboctahedron, which has six
square faces and eight triangular faces, as shown in Figure 8.16. The points in A3
can also be visualized as the midpoints of the edges of a cube, as in Figure 8.17.
Algebraically, we can describe A3 as the set of 12 vectors in R3 of the form

8.9
Examples in Rank Three
229
Fig. 8.15 The A3 root
system, with the elements of
the base in dark gray
Fig. 8.16 The roots in A3
make up the vertices of a
cuboctahedron
.˙1; ˙1; 0/, .˙1; 0; ˙1/, and .0; ˙1; ˙1/. (This set of vectors actually corresponds
to the conventional description of the D3 root system, as in Sect. 7.7.2, which turns
out to be isomorphic to A3.) It is then a simple exercise to check that this collection
of vectors is, in fact, a root system. A base for this system is given by the vectors
.1; 1; 0/, .0; 1; 1/, and .0; 1; 1/.
The Weyl group W for A3 is the symmetry group of the tetrahedron pictured in
Figure 8.18. The group W is the full permutation group on the four vertices of the
tetrahedron. As in the A2 case, the Weyl group of A3 is not the full symmetry group
of the root system, since I is not an element of W .
The B3 root system is obtained from the A3 root system by adding six additional
vectors, consisting of three mutually orthogonal pairs. Each of the new roots

230
8
Root Systems
Fig. 8.17 The roots in A3 lie
at the midpoints of the edges
of a cube
Fig. 8.18 The Weyl group of
A3 is the symmetry group of
a regular tetrahedron
is shorter than the original roots by a factor of
p
2, as shown in Figure 8.19.
Algebraically, B3 consists of the twelve vectors .˙1; ˙1; 0/, .˙1; 0; ˙1/, and
.0; ˙1; ˙1/ of A3, together with the six vectors .˙1; 0; 0/, .0; ˙1; 0/, and
.0; 0; ˙1/.
The C3 root system, meanwhile, is obtained from A3 by adding six new vectors,
as in the case of B3, except that this time the new roots are longer than the original
roots by a factor of
p
2, as in Figure 8.20. That is to say, the new roots are the
six vectors .˙2; 0; 0/, .0; ˙2; 0/, and .0; 0; ˙2/. The C3 root system is the dual of
B3, in the sense of Deﬁnition 8.10. The elements of C3 make up the vertices of an
octahedron, together with the midpoints of the edges of the octahedron, as shown in
Figure 8.21.

8.9
Examples in Rank Three
231
Fig. 8.19 The B3 root system, with the elements of the base in dark gray
Fig. 8.20 The C3 root system, with the elements of the base in dark gray

232
8
Root Systems
Fig. 8.21 The C3 root system consists of the vertices of an octahedron, together with the midpoints
of the edges of the octahedron
The root systems B3 and C3 have the same Weyl group, which is the symmetry
group of the cube in Figure 8.17. In both cases, the Weyl group is the full symmetry
group of the root system.
8.10
The Classical Root Systems
We now return to the root systems of the classical semisimple Lie algebras, as
computed in Sect. 7.7. For each of these root systems, we describe a base and
determine the associated Dynkin diagram.
8.10.1
The An Root System
The An root system is associated to the Lie algebra sl.nC1I C/. For this root system,
E is the subspace of RnC1 consisting of vectors whose entries sum to zero. The roots
are the vectors of the form
ej  ek;
j ¤ k;

8.10
The Classical Root Systems
233
where fejg is the standard basis for RnC1. As a base, we may take the vectors
e1  e2; e2  e3; : : : ; en  enC1
Note that for j < k,
ej  ek D .ej  ej C1/ C .ej C1  ej C2/ C    C .ek1  ek/;
so that every root is a sum of elements of the base, or the negative thereof.
All roots in the base have the same length, two consecutive roots are at an angle
of 2=3 to one another, and nonconsecutive roots are orthogonal.
8.10.2
The Dn Root System
The Dn root system is associated to the Lie algebra so.2nI C/, n  2. For this root
system, E D Rn and the roots are the vectors of the form
˙ej ˙ ek;
j < k:
As a base, we may take the n  1 roots
e1  e2; e2  e3;    ; ; en2  en1; en1  en;
(8.11)
together with the one additional root,
en1 C en:
(8.12)
Note that for j < k, we have the following formulas:
ej  ek D .ej  ej C1/ C .ej C1  ej C2/ C    C .ek1  ek/;
ej C en D .ej  en1/ C .en1 C en/;
ej C ek D .ej C en/ C .ek  en/:
(8.13)
This shows that every root of the form ej  ek or ej C ek (j < k) can be written
as a linear combination of the base in (8.11) and (8.12) with non-negative integer
coefﬁcients. The roots of this form are the positive roots, and the remaining roots
are the negatives of these roots.
Two consecutive roots in the list (8.11) have an angle of 2=3 and two
nonconsecutive roots in the list (8.11) are orthogonal. The angle between the root
in (8.12) and the second-to-last element in the list (8.11) is 2=3; the root in (8.12)
is orthogonal to all the other roots in (8.11).

234
8
Root Systems
8.10.3
The Bn Root System
The Bn root system is associated to the Lie algebra so.2n C 1I C/. For this root
system, E D Rn and the roots are the vectors of the form
˙ej ˙ ek;
j < k;
and of the form
˙ej;
j D 1; : : : ; n:
As a base for our root system, we may take the n  1 roots
e1  e2; e2  e3; : : : ; en1  en;
(8.14)
(exactly as in the so.2nI C/ case) together with the one additional root,
en:
(8.15)
The positive roots are those of the form ej C ek or ej  ek (j < k) and those of the
form ej (1  j  n). To expand every positive root in terms of the base, we use the
formulas in (8.13), except with the second line replaced by
ej C en D .ej  en/ C 2en;
(8.16)
and with the additional relation
ej D .ej  en/ C en:
(8.17)
As in the so.2nI C/ case, consecutive roots in the list (8.14) have an angle of
2=3, whereas nonconsecutive roots on the list (8.14) are orthogonal. Meanwhile,
the root in (8.15) has an angle of 3=4 with the last root in (8.14) and is orthogonal
to the remaining roots in (8.14).
In Sect. 8.2, we have pictured the B2 root system rotated by =4 relative to
the n D 2 case of the root system described in this subsection. The pictures in
Sect. 8.2 actually correspond to the conventional description of the C2 root system
(Sect. 8.10.4), which is isomorphic to B2.
8.10.4
The Cn Root System
The Cn root system is associated to the Lie algebra sp.nI C/. For this root system,
E D Rn and the roots are the vectors of the form
˙ej ˙ ek;
j < k

8.10
The Classical Root Systems
235
and of the form
˙2ej;
j D 1; : : : ; n:
As a base, we may take the n  1 roots
e1  e2; e2  e3; : : : ; en1  en
(8.18)
(as in the two preceding subsections), together with the root 2en. We use the same
formula for expanding roots in terms of the base as in the case of so.2n C 1I C/,
except that (8.17) is rewritten as
2ej D 2.ej  en/ C .2en/:
The angle between two consecutive roots in (8.18) is 2=3I nonconsecutive roots
in (8.18) are orthogonal. The angle between 2en and the last root in (8.18) is 3=4;
the root 2en is orthogonal to the other roots in (8.18).
8.10.5
The Classical Dynkin Diagrams
From the calculations in the previous subsections, we can read off the Dynkin
diagram for the root systems An, Bn, Cn, and Dn; the results are recorded in
Figure 8.22. We can see that certain special things happen for small values of n.
First, the Dynkin diagram for Dn does not make sense when n D 1, since the
diagram always has at least two vertices. This observation reﬂects the fact that
the Lie algebra so.2I C/ is not semisimple. Second, the Dynkin diagram for D2
is not connected, which means (Corollary 8.33) that the Lie algebra so.4I C/ is
semisimple but not simple. (Compare Exercise 4 in Chapter 7.) Third, the Dynkin
diagrams for A1, B1, and C1 are isomorphic, reﬂecting that the rank-one Lie algebras
sl.2I C/, so.3I C/, and sp.1I C/ are isomorphic. Last, we have an isomorphism
between the diagrams for B2 and C2 and an isomorphism between the diagrams for
An
Cn
Bn
Dn
Fig. 8.22 The Dynkin diagrams of the classical Lie algebras

236
8
Root Systems
A3 and D3, which reﬂects (Sect. 8.11) an isomorphism between the corresponding
Lie algebras.
Corollary 8.47. The following semisimple Lie algebras are simple: the special
linear algebras sl.n C 1I C/, n  1; the odd orthogonal algebras so.2n C 1I C/,
n  1; the even orthogonal algebras so.2nI C/, n  3; and the symplectic algebras
sp.nI C/, n  1.
Proof. The Dynkin diagrams for An, Bn, and Cn are always connected, whereas the
Dynkin diagram for Dn is connected for n  3. Thus, Corollary 8.33 shows that the
claimed Lie algebras are simple.
ut
8.11
The Classiﬁcation
In this section, we describe, without proof, the classiﬁcation of irreducible root
systems and of simple Lie algebras. Recall (Corollary 8.33) that a semisimple Lie
algebra is simple if and only if its Dynkin diagram is connected.
Every irreducible root system is either the root system of a classical Lie algebra
(types An, Bn, Cn, and Dn) or one of ﬁve exceptional root systems. We begin by
listing the Dynkin diagrams of the exceptional root systems.
Theorem 8.48. For each of the graphs in Figure 8.23, there exists a root system
having that graph as its Dynkin diagram.
We have already described the root system G2 in Figure 8.3. Although it is
possible to write down the remaining exceptional root systems explicitly, it is not
terribly useful to do so, since there is no comparably easy way to construct the Lie
algebras associated to these root systems.
E6
E7
E8
F4
G2
Fig. 8.23 The exceptional Dynkin diagrams

8.11
The Classiﬁcation
237
Theorem 8.49. Every irreducible root system is isomorphic to exactly one of the
following:
•
An, n  1
•
Bn, n  2
•
Cn, n  3
•
Dn, n  4
•
One of the exceptional root systems G2, F4, E6, E7, and E8.
The restrictions on n are to avoid the case of D2, which is not irreducible, and
to avoid repetitions. The Dynkin diagram for D3, for example, is isomorphic to the
Dynkin diagram for A3, which means (Proposition 8.32) that the A3 and D3 root
systems are also isomorphic. Similarly, all root systems in rank one are isomorphic,
and B2 is isomorphic to C2.
Theorem 8.50. 1. If g is a complex semisimple Lie algebra and h1 and h2 are
Cartan subalgebra of g, there exists an automorphism  W g ! g such that
.h1/ D .h2/.
2. Suppose g1 and g2 are semisimple Lie algebras with Cartan subalgebras h1
and h2, respectively. If the root systems associated to .g1; h1/ and .g2; h2/ are
isomorphic, then g1 and g2 are isomorphic.
3. For every root system R, there exists a semisimple Lie algebra g and a Cartan
subalgebra h of g such that the root system of g relative to h is isomorphic to R.
Point 1 of the theorem says that there is only one root system associated to each
semisimple Lie algebra g. Since all bases of a ﬁxed root system are equivalent under
W , it follows that there is only one Dynkin diagram associated to each g. Points 2
and 3 then tell us that there is a one-to-one correspondence between isomorphism
classes of semisimple Lie algebras and isomorphism classes of root systems. Thus,
the classiﬁcation of irreducible root systems also gives rise to a classiﬁcation of
simple Lie algebras.
For Point 1 of the theorem see Section 16.4 of [Hum] and for Point 2 see Section
14.2 of [Hum]. For Point 3, one can proceed on a case-by-case basis, where the Lie
algebras An, Bn, Cn, and Dn have already been constructed as classical Lie algebras.
The exceptional Lie algebras can then be constructed by special methods, as in [Jac]
or [Baez]. Alternatively, one can construct all of the simple Lie algebras by a uniﬁed
method, as in Section 18 of [Hum].
In particular, the isomorphisms among root systems of small rank translate
into isomorphisms among the associated semisimple Lie algebras. In rank one,
for example, sl.2I C/, so.3I C/, and sp.1I C/ are isomorphic. In rank two, the
isomorphism between B2 and C2 reﬂects an isomorphism of the Lie algebras
so.5I C/ and sp.2I C/. In rank three, the isomorphism between A3 and D3 reﬂects
an isomorphism of the Lie algebras sl.4I C/ and so.6I C/.
By combining the classiﬁcation of irreducible root systems in Theorem 8.49
with Proposition 8.32 and Theorem 8.50, we arrive at the following classiﬁcation
of simple Lie algebras over the ﬁeld of complex numbers.

238
8
Root Systems
Theorem 8.51. Every simple Lie algebra over C is isomorphic to precisely one
algebra from the following list:
1. sl.n C 1I C/, n  1
2. so.2n C 1I C/, n  2
3. sp.nI C/, n  3
4. so.2nI C/, n  4
5. The exceptional Lie algebras G2, F4, E6, E7, and E8
A semisimple Lie algebra is then determined up to isomorphism by specifying
which simple summands occur and how many times each one occurs; see
Proposition 7.9. It is also possible to classify simple Lie algebras over R. As
we showed in Sect. 7.6, every such algebra is either a complex simple Lie algebra,
viewed as a real Lie algebra, or a real form of a complex simple Lie algebra. Real
forms of complex Lie algebras can then be enumerated using the Dynkin diagram
of the complex Lie algebra as a starting point. See Section VI.10 and Appendix C
of [Kna2] for a description of this enumeration.
8.12
Exercises
Unless otherwise noted, the notation in the exercises is as follows: .E; R/ is a root
system with Weyl group W , 
 D f˛1; : : : ; ˛rg is a ﬁxed base for R, RC is the
associated set of positive roots, and C is the open fundamental chamber associated
to 
.
1. (a) Suppose that ˛ and ˇ are linearly independent elements of R and that for
some positive integer k, the vector ˛ C kˇ belongs to R. Show that ˛ C lˇ
also belongs to R for all integers l with 0 < l < k.
Hint: If F D span.˛; ˇ/, then R \ F is a rank-two root system in F .
(b) A collection of roots of the form ˛; ˛ C ˇ; : : : ; ˛ C kˇ for which neither
˛ˇ nor ˛C.kC1/ˇ is a root is called a root string. What is the maximum
number of roots that can occur in a root string?
2. Let E be a ﬁnite-dimensional real inner product space and let V1; : : : ; Vk be
subspaces of E of codimension at least one. Show that the union of the Vk's is
not all of E.
Hint: Show by induction on k that the complement of the union the Vk's is a
nonempty open subset of E.
3. Let E be a ﬁnite-dimensional real inner product space, let fv1; : : : ; vrg be basis
for E, and let fv
1 ; : : : ; v
r g the dual basis, satisfying
D
v
j ; vk
E
D ıjk for all j and
k. Let G and H be the Gram matrices for these two bases: Gjk D
˝
vj ; vk
˛
and
Hjk D
D
v
j ; v
k
E
. Show that G and H are inverses of each other.

8.12
Exercises
239
Hint: First show that for any u 2 E, we have u D P
j
D
v
j ; u
E
vj . Then apply
this result to the vector u D v
k .
4. Show that Lemma 8.26 fails (even with u D e) if ˇ is not assumed to be an
element of 
C.
5. Show that if R is an irreducible root system, then W acts irreducibly on E.
Hint: Suppose V  E is a W -invariant subspace. Show that every element of
R is either in V or in the orthogonal complement of E.
6. Let .E; R/ be an irreducible root system and let h; i be the inner product on E.
Using Exercise 5, show that if h; i1 is a W -invariant inner product on E, there
is some constant c such that hH; H 0i1 D c hH; H 0i for all H; H 0 2 E.
Hint: Consider the unique linear operator A W E ! E that is symmetric with
respect to h; i and that satisﬁes
˝H; H 0˛
1 D ˝H; AH0˛
for all H; H 0 2 E. Then imitate the proof of Schur's lemma, noting that the
eigenvalues of A are real.
7. Suppose .E; R/ and .F; S/ are irreducible root systems and that A W E ! F is
an isomorphism of R with S. Show that A is a constant multiple of an isometry.
8. Using the outline below, prove the following result: For all ;  2 E, we have
   if and only if h  ; Hi  0 for all H 2 C.
(a) Show that if   , then h  ; Hi  0 for all H 2 C.
(b) Let f˛
1 ; : : : ; ˛
r g be the dual basis to 
, satisfying
D
˛
j ; ˛k
E
D ıjk for all j
and k. Show that if h; Hi  0 for all H 2 C, then
D
; ˛
j
E
 0 for all j.
(c) Show that if h  ; Hi  0 for all H 2 C, then   .
9. Let P W E ! R be the function given by
P.H/ D
Y
˛2RC
h˛; Hi :
Show that P satisﬁes
P.w  H/ D det.w/P.H/
for all w 2 W and all H 2 E.
10. Show that if I is not in the Weyl of R, the Dynkin diagram of R must have a
nontrivial automorphism.
Hint: By Proposition 8.23, there exists an element w of W mapping C to C.
Consider the map H 7! w  H, which maps C to itself.
11. For which rank-two root systems is I an element of the Weyl group?
12. Show that the Weyl group of the An root system, described in Sect. 7.7.1, does
not contain I, except when n D 1.

240
8
Root Systems
13. Let E D Rn and let R denote the collection of vectors of the following three
forms:
˙ej ˙ ek
j < k
˙ej
j D 1; : : : ; n
˙2ej
j D 1; : : : ; n
:
Show that R satisﬁes all the properties of a root system in Deﬁnition 8.1 except
Condition 2. The collection R is a "nonreduced root system" and is known as
BCn, since it is the union of Bn and Cn. (Compare Figure 7.1 in the n D 2
case.)
14. Determine which of the Dynkin diagrams in Figures 8.22 and 8.23 have a
nontrivial automorphism. Show that only the Dynkin diagram of D4 has an
automorphism group with more than two elements.

Chapter 9
Representations of Semisimple Lie Algebras
In this chapter, we prove the theorem of the highest weight for irreducible, ﬁnite-
dimensional representations of a complex semisimple Lie algebra g: We ﬁrst
prove that every such representation has a highest weight, that two irreducible
representations with the same highest weight are isomorphic, and that the highest
weight of an irreducible representation must be dominant integral. This part of
the theorem is established in precisely the same way as in the case of sl.3I C/
in Chapter 6. It then remains to prove that every dominant integral element is, in
fact, the highest weight of some irreducible representation. In the sl.3I C/ case,
we did this by ﬁrst constructing the representations whose highest weights were
the fundamental weights .1; 0/ and .0; 1/; and then taking tensor products of these
representations. For a general semisimple Lie algebra g; however, there is no simple
way to construct the representations whose highest weights are the fundamental
weights in Deﬁnition 8.36. Thus, we require a new method of constructing the
irreducible representation of g with a given dominant integral highest weight. This
construction will be the main topic of the present chapter.
In Chapter 10, we will derive several additional properties of the irreducible
representations, including the structure of the set of weights, the multiplicities of
the weights, and the dimensions of the representations. In that chapter, we will
also prove complete reducibility for representations of g; that is, that every ﬁnite-
dimensional representation of g decomposes as a direct sum of irreducibles.
9.1
Weights of Representations
Throughoutthe chapter, we assume that g D kC is a complex semisimple Lie algebra
and that h D tC is a ﬁxed Cartan subalgebra of g (compare Proposition 7.11). We ﬁx
on g an inner product that is real on k and that is invariant under the adjoint action
of k; as in Proposition 7.4. We let R  it denote the set of roots of g relative to h,
© Springer International Publishing Switzerland 2015
B. Hall, Lie Groups, Lie Algebras, and Representations, Graduate
Texts in Mathematics 222, DOI 10.1007/978-3-319-13467-3_9
241

242
9
Representations of Semisimple Lie Algebras
we let 
 be a ﬁxed base for R; and we let RC and R be the set of positive and
negative roots relative to 
; respectively. For each root ˛; we consider the coroot
H˛ 2 h given by
H˛ D 2
˛
h˛; ˛i:
We also consider the Weyl group W; that is, the group of linear transformations
of h generated by the reﬂections about the hyperplanes orthogonal to the roots.
Finally, we consider the notions of integral and dominant integral elements, as in
Deﬁnition 8.34.
We now introduce the notion of a weight of a representation, as in the sl.3I C/
case.
Deﬁnition 9.1. Let .; V / be a representation of g; possibly inﬁnite dimensional.
An element  of h is a weight of  if there exists a nonzero vector v 2 V such that
.H/v D h; Hi v
(9.1)
for all H 2 h: The weight space corresponding to  is the set of all v 2 V
satisfying (9.1) and the multiplicity of  is the dimension of the corresponding
weight space.
Throughout the chapter, we will use, without comment, Proposition A.17, which
says that weight vectors with distinct weights are linearly independent.
Proposition 9.2. If .; V / is a ﬁnite-dimensional representation of g, every weight
of  is an integral element.
Proof. For each root ˛; let s˛ D hX˛; Y˛; H˛i Š sl.2I C/ be the subalgebra of g in
Theorem 7.19. If v is a weight vector with weight ; then
.H˛/v D h; H˛i v:
Thus, by applying Point 1 of Theorem 4.34 to the restriction of  to s˛; we see that
h; H˛i must be an integer, showing that  is integral.
ut
Theorem 9.3. If .; V / is a ﬁnite-dimensional representation of g; the weights of
 and their multiplicities are invariant under the action of W on H:
Proof. For each ˛ 2 R; we may construct the operator
S˛ WD e.X˛/e.Y˛/e.X˛/:
If h˛; Hi D 0; then H will commute with both X˛ and Y˛ and thus with S˛: On
the other hand, by Point 3 of Theorem 4.34, we have S˛.H˛/S1
˛
D .H˛/: We
see, then, that
S˛.H/S1
˛
D .s˛  H/
for all H 2 h:

9.1
Weights of Representations
243
Suppose now that v is a weight vector with some weight : Then
.H/S1
˛ v D S1
˛ .s˛  H/v
D S1
˛
h; s˛  Hi v
D
˝
s1
˛
 ; H
˛
S1
˛ v;
showing that S1
˛ v is a weight vector with weight s1
˛
 : Thus, S1
˛
maps the
weight space with weight  into the weight space with weight s1
˛
 : Meanwhile,
essentially the same argument shows that S˛ maps the weight space with weight s1
˛ 
 into the weight space with weight ; showing that the two spaces are isomorphic.
Thus, s1
˛
  is again a weight with the same multiplicity as : Thus, the weights
and multiplicities are invariant under each s1
˛
D s˛ and, thus, under W:
ut
We now state the "easy" part of the theorem of the highest weight for represen-
tations of g:
Theorem 9.4. 1. Every irreducible, ﬁnite-dimensional representation of g has a
highest weight.
2. Two irreducible, ﬁnite-dimensional representations of g with the same highest
weight are isomorphic.
3. If .; V / is an irreducible, ﬁnite-dimensional representation of g with highest
weight ; then  is dominant integral.
Proof. Enumerate the positive roots as ˛1; : : : ; ˛N : Choose a basis for g consisting
of elements X1; : : : ; XN with Xj 2 g˛j ; elements Y1; : : : ; YN with Yj 2 g˛j ; and
a basis H1; : : : ; Hr for h: Then the proof of Proposition 6.11 from Chapter 6 carries
over to the present setting, with only the obvious notational changes, showing that
every irreducible, ﬁnite-dimensional representation of g has a highest weight.
The proofs of Propositions 6.14 and 6.15 then also go through without change to
show that two irreducible, ﬁnite-dimensional representations with the same highest
weight are isomorphic. Finally, using the sl.2I C/-subalgebras in Theorem 7.19, we
may follow the proof of Proposition 6.16 to show that the highest weight of a ﬁnite-
dimensional, irreducible representation must be dominant integral.
ut
We now come to the "hard" part of the theorem of the highest weight.
Theorem 9.5. If  is a dominant integral element, there exists an irreducible, ﬁnite-
dimensional representation of g with highest weight :
As we have noted, the method of proof of Proposition 6.17, from the sl.3I C/
case, does not readily extend to general semisimple Lie algebras. The proof of
Theorem 9.5 will occupy the remainder of this chapter.

244
9
Representations of Semisimple Lie Algebras
9.2
Introduction to Verma Modules
Our goal is to construct, for each dominant integral element  2 h; a ﬁnite-
dimensional, irreducible representation of g with highest weight : Our construction
will proceed in two stages. The ﬁrst stage consists of constructing an inﬁnite-
dimensional representation V of g; known as a Verma module. This representation
will not be irreducible, but will be a highest weight cyclic representation with
highest weight : We will construct V as a quotient of the so-called universal
enveloping algebra U.g/ of g: In order to show that the highest weight vector
in V is nonzero, we will need to develop a structure result for U.g/ known as
the Poincaré-Birkhoff-Witt theorem (Theorem 9.9). Unlike the ﬁnite-dimensional
representations of g; the weights of the Verma module are not invariant under the
action of the Weyl group.
The second stage in our construction consists of showing that when  is
dominant integral, V has an invariant subspace W for which the quotient space
V=W is ﬁnite dimensional and irreducible, but not zero. To establish the ﬁnite
dimensionality of the quotient, we will show that when  is dominant integral,
the weights of V=W; unlike those of V; are invariant under the action of the
Weyl group. Thus, each weight  of V=W is integral and satisﬁes w     for
all w in the Weyl group. It turns out that there are only ﬁnitely many 's with this
property. Since each weight  has ﬁnite multiplicity (even in V), it follows that
V=W is ﬁnite dimensional.
Deﬁnition 9.6. A (possibly inﬁnite-dimensional) representation .; V / of g is
highest weight cyclic with highest weight  2 h if there exists a nonzero vector
v 2 V such that (1) .H/v D h; Hi v for all H 2 h; (2) .X/v D 0 for all
X 2 g˛ with ˛ 2 RC; (3) the smallest invariant subspace containing v is V:
Note that  2 h is not required to be integral. Although it will turn out that
all ﬁnite-dimensional highest weight cyclic representations are irreducible, this
is not the case in inﬁnite dimensions. Furthermore, two highest weight cyclic
representations with the same highest weight may not be isomorphic, unless both
of them are ﬁnite dimensional. In this chapter, we will construct, for any  2 h;
a particular highest weight cyclic representation V with highest weight ; known
as a Verma module. The Verma module is the "maximal" highest weight cyclic
representation with a particular highest weight, and it is always inﬁnite dimensional,
even when  is dominant integral.
In the sl.2I C/ case, Verma modules can be constructed explicitly as follows.
For any complex number ; construct an inﬁnite-dimensional vector space V with
basis v0; v1; : : : : (The elements of V are ﬁnite linear combinations of the vj's.) We
deﬁne an action of sl.2I C/ on V by the same formulas as in Sect. 4.6:
.Y /vj D vj C1
.H/vj D .  2j/vj

9.2
Introduction to Verma Modules
245
.X/v0 D 0
.X/vj D j.  .j  1//vj 1:
Note that in Sect. 4.6, the vectors vj equaled zero for large j; whereas here the vj's
are, by deﬁnition, linearly independent. Direct calculation shows that these formulas
do, in fact, deﬁne a representation of sl.2I C/:
When  is a non-negative integer m, the space W spanned by vmC1; vmC2; : : : is
invariant under the action of sl.2I C/: After all, this space is clearly invariant under
the action of .Y / and .H/; and it is invariant under .X/ because (with
 D m) we have
.X/vmC1 D k.m  m/vm D 0:
Since W is invariant, the quotient vector space V=W inherits a natural action
of sl.2I C/: This quotient space is then the unique ﬁnite-dimensional irreducible
representation of sl.2I C/ with highest weight :
In the case of general semisimple Lie algebra g; we would like to do something
similar. Pick an basis consisting of vectors
Y1; : : : ; YN ; H1; : : : ; Hr; X1; : : : ; XN ;
(9.2)
as in the proof of Theorem 9.4. If V is any highest weight cyclic representation
with highest weight  and highest weight vector v0; then V is spanned by products
of the basis elements applied to v0: By the reordering lemma (Lemma 6.12), we can
reorder any such product as a linear combination of terms in which the elements are
in the order listed in (9.2). Once the basis elements are in this order, any term that
contains any Xj's will give zero when applied to v0. Furthermore, in any term that
does not have any Xj's, any factors of Hk will simply give .Hj / when applied
to v0: Thus, V must be spanned by elements of the form
.Y1/n1.Y2/n2    .YN /nN v0:
(9.3)
The idea of a Verma module is that we should proceed as in the sl.2I C/ case
and simply decree that the vectors in (9.3) form a basis for our Verma module. The
weights of the Verma module will the consist of all elements of the form
  n1˛1      nN ˛N ;
where each nj is a non-negative integer. (See Figure 9.1.) If we do this, then there
is only one possible way that the Lie algebra g can act. After all, if we apply some
Lie algebra element .Z/ to a vector as in (9.3), we can reorder the elements as in
the previous paragraph until they are in the order of (9.2). Then, as we have already
noted, any factors of .Xj/ give zero and any factors of .Hk/ give constants.
We will, thus, eventually get back a linear combination of elements of the form (9.3).

246
9
Representations of Semisimple Lie Algebras
Fig. 9.1 The weights of the Verma module with highest weight 
The difﬁculty with the above description of the Verma module is that it does
not provide any reasonable method for checking that  actually constitutes a
representation of g: After all, unless g D sl.2I C/; it is impossible to write
down an explicit description of how the various basis elements act, and it is thus
impossible to verify directly that these elements satisfy the correct commutation
relations. Nevertheless, we will eventually prove (Sect. 9.5) that there is a well-
deﬁned representation of g having the elements (9.3) as a basis and in which g acts
in the way described above. In the case in which  is dominant and integral, we will
then construct an invariant subspace of the Verma module for which the quotient is
ﬁnite dimensional and irreducible.
9.3
Universal Enveloping Algebras
In Sect. 9.5, we will construct each Verma module V as a quotient of something
called the universal enveloping algebra of a Lie algebra g: If g is a Lie algebra, we
may try to embed g as a subspace of some associative algebra A in such a way that
the bracket on g may be computed as ŒX; Y  D XY  YX; where XY and YX are
computed in A: If g is the Lie algebra of a matrix Lie group G  Gl.nI C/; then g
is a subspace of the associative algebra Mn.C/ and the bracket on g is indeed given
as ŒX; Y  D XY  YX: There may be, however, many other ways to embed g into
an associative algebra A: For example, if g D sl.2I C/; then for each m  1; the
irreducible representation m of g of dimension m C 1 allows us to embed g into
MmC1.C/:

9.3
Universal Enveloping Algebras
247
Let us now give a useful but slightly imprecise deﬁnition of the universal
enveloping algebra of g; denoted U.g/ For any Lie algebra g; the universal
enveloping algebra of g will be an associative algebra A with identity with the
following properties. (1) The Lie algebra g embeds into A in such a way that
ŒX; Y  D XY  YX: (2) The algebra A is generated by elements of g; meaning
that the smallest subalgebra with identity of A containing g is all of A: (3) The
algebra A is maximal among all algebras A with the two previous properties.
The maximality property of A will be explained more precisely in the discussion
following Theorem 9.7.
Consider, for example, the case of a one-dimensional Lie algebra g spanned by
a single nonzero element X; which of course satisﬁes ŒX; X D 0: Then U.g/
should be an associative algebra with identity generated a single element X; in
which case, U.g/ must also be commutative. Now, any associative algebra A with
identity generated by a single nonzero element X will satisfy Properties 1 and 2
in the deﬁnition of the enveloping algebra. But for A to be maximal, there should
be no relations between the different powers of X; meaning that p.X/ should be a
nonzero element of A for every nonzero polynomial p: In this case, then, we may
take U.g/ to be the algebra of polynomials in a single variable.
Suppose the algebra g in the previous paragraph is a matrix algebra, meaning
that X is a single n  n matrix. Contrary to what we might, at ﬁrst, expect, the
enveloping algebra U.g/ will not coincide with the associative algebra with identity
A generated by X inside Mn.C/: After all, for any X 2 Mn.C/; the Cayley-
Hamilton theorem implies that there exists a nonzero polynomial p (namely, the
characteristic polynomial of X) for which p.X/ D 0: In U.g/; by contrast, we have
said that p.X/ should be nonzero for all nonzero polynomials p:
Actually, it follows from the PBW theorem (Theorem 9.9) that the universal
enveloping algebra of any nonzero Lie algebra g is inﬁnite dimensional. In fact, if X
is any nonzero element of g; the elements 1; X; X2; : : : will be linearly independent
in U.g/: Thus, even if g is an algebra of matrices, U.g/ cannot be isomorphic to a
subalgebra of an algebra of matrices.
We now give the formal deﬁnition of a universal enveloping algebra.
Theorem 9.7. For any Lie algebra g; there exists an associative algebra with
identity, denoted U.g/; together with a linear map i W g ! U.g/ such that the
following properties hold. (1) For all X; Y 2 g; we have
i.ŒX; Y / D i.X/i.Y /  i.Y /i.X/:
(9.4)
(2) The algebra U.g/ is generated by elements of the form i.X/; X 2 g; meaning
that the smallest subalgebra with identity of U.g/ containing every i.X/ is U.g/: (3)
Suppose A is an associative algebra with identity and j W g ! A is a linear map
such that j.ŒX; Y / coincides with j.X/j.Y /  j.Y /J.X/ for all X; Y 2 g. Then
there exists a unique algebra homomorphism  W U.g/ ! A such that .1/ D 1
and such that .i.X// D j.X/ for all X 2 g:
A pair .U.g/; i/ with the preceding properties is called a universal enveloping
algebra for g:

248
9
Representations of Semisimple Lie Algebras
A simple argument (Exercise 1) shows that any two universal enveloping algebras
for a ﬁxed Lie algebra g are "canonically" isomorphic.
Let us deﬁne an enveloping algebra of g to be an associative algebra A together
with a linear map j W g ! A as in Point 3 of the theorem, with the additional
property that A is generated by elements of the form j.X/; X 2 g: In this case, the
homomorphism  W U.g/ ! A as in Theorem 9.7 is surjective and A is isomorphic
to the quotient algebra U.g/= ker./: Thus, the universal enveloping algebra U.g/
of g has the property that every other enveloping algebra of g is a quotient of U.g/:
This property of U.g/ is a more precise formulation of the maximality condition we
discussed in the second paragraph of this subsection.
The construction of U.g/ is in some sense easy or "soft." But for U.g/ to be
useful in practice (for example, in constructing Verma modules), we need a structure
theorem for it known as the Poincaré-Birkhoff-Witt theorem. The Poincaré-
Birkhoff-Witt theorem will, in particular, show that the map i in Theorem 9.7 is
actually injective. (See also Exercise 2.) Once this is established, we will be able to
identify g with its image under i and thus think of g as embedded into U.g/:
Proof of Theorem 9.7. The operation of tensor product on vector spaces is associa-
tive, in the sense that U ˝ .V ˝ W / is canonically isomorphic to .U ˝ V / ˝ W;
with the isomorphism taking u ˝ .v ˝ w/ to .u ˝ v/ ˝ w for each u 2 U; v 2 V;
and w 2 W: We may thus drop the parentheses and write simply U ˝ V ˝ W and
u˝v˝w; and similarly for the tensor product of any ﬁnite number of vector spaces.
In particular, we will let V ˝k denote the k-fold tensor product V ˝    ˝ V: The
0-fold tensor product V ˝0 is deﬁned to be C (or whatever ﬁeld we are working
over).
For a Lie algebra g; let us ﬁrst deﬁne the tensor algebra T .g/ over g; which is
deﬁned as
T .g/ D
1
M
kD0
g˝k:
In the direct sum, each element of T .g/ is required to be a ﬁnite linear combinations
of elements g˝k for different values of k: We can make T .g/ into an associative
algebra with identity by deﬁning
.u1 ˝ u2 ˝    ˝ uk/  .v1 ˝ v2 ˝    ˝ vl/
D u1 ˝ u2 ˝    ˝ uk ˝ v1 ˝ v2 ˝    ˝ vl
(9.5)
and then extending the product by linearity. That is to say, the product operation
is the unique bilinear map of T .g/  T .g/ into T .g/ that coincides with the
tensor product (9.5) on g˝k  g˝l: Since C ˝ g˝k is naturally isomorphic to g˝k;
the identity element 1 2 C D g˝0 is the multiplicative identity for T .g/: The
associativity of the tensor product assures that T .g/ is an associative algebra.

9.3
Universal Enveloping Algebras
249
We now claim that the algebra T .g/ has the following property: If A is any
associative algebra with identity and j W g ! A is any linear map, there exists an
algebra homomorphism  W T .g/ ! A such that .1/ D 1 and .X/ D j.X/
for all X 2 g  T .g/: Note that this property differs from the desired property of
U.g/ in that j is an arbitrary linear map and does not have to have any particular
relationship to the algebra structure of A: To construct ; we require that the
restriction of  to g˝k to be the unique linear map of g˝ into A such that
.X1 ˝    ˝ Xk/ D j.X1/    j.Xk/
(9.6)
for all X1; : : : ; Xk in g: (Here we are using the natural k-fold extension of the
universal property of tensor products in Deﬁnition 4.13.) It is then simple to
check that  is an algebra homomorphism. Furthermore, if  is to be an algebra
homomorphism that agrees with j on g; then  must have the form in (9.6).
We now proceed to construct U.g/ as a quotient of T .g/: A two-sided ideal in
T .g/ is a subspace J of T .g/ such that for all ˛ 2 T .g/ and ˇ 2 J , the elements ˛ˇ
and ˇ˛ belong to J: We now let J be the smallest two-sided ideal in T .g/ containing
all elements of the form
X ˝ Y  Y ˝ X  ŒX; Y ;
X; Y 2 g:
(9.7)
That is to say, J is the intersection of all two-sided ideals in T .g/ containing all
such elements, which is, again, a two-sided ideal containing these elements. More
concretely, J can be constructed as the space of elements of the form
N
X
j D1
˛j .Xj ˝ Yj  Yj ˝ Xj  ŒXj; Yj/ˇj ;
with Xj and Yj in g and ˛j and ˇj being arbitrary elements of T .g/:
We now form the quotient vector space T .g/=J; which is an algebra. If
j W g ! A is any linear map, we can form the algebra homomorphism
 W T .g/ ! A as above. If j satisﬁes j.ŒX; Y / D j.X/j.Y /  j.Y /j.X/; then
the kernel of  will contain all elements of the form X ˝ Y  Y ˝ X  ŒX; Y :
Furthermore, the kernel of an algebra homomorphism is always a two-sided ideal.
Thus, ker./ contains J: It follows that the map  W T .g/ ! A factors through
U.g/ WD T .g/=J; giving the desired homomorphism of U.g/ into A: Since U.g/
is spanned by products of elements of g; there can be at most one map  with the
desired property, establishing the claimed uniqueness of :
ut
Proposition 9.8. If  W g ! End.V / is a representation of a Lie algebra g
(not necessarily ﬁnite dimensional), there is a unique algebra homomorphism
Q W U.g/ ! End.V / such that Q.1/ D I and Q.X/ D .X/ for all X 2 g  U.g/:
Proof. Apply Theorem 9.7 with A D End.V / and j.X/ D .X/:
ut

250
9
Representations of Semisimple Lie Algebras
We now state the Poincaré-Birkhoff-Witt theorem—or PBW theorem, for
short—which is the key structure result for universal enveloping algebras. Although
the result holds even for inﬁnite-dimensional Lie algebras, we state it here in the
ﬁnite-dimensional case, for notational simplicity. The proof of the PBW theorem is
in Sect. 9.4.
Theorem 9.9 (PBW Theorem). If g is a ﬁnite-dimensional Lie algebra with basis
X1; : : : ; Xk; then elements of the form
i.X1/n1i.X2/n2    i.Xk/nk;
(9.8)
where each nk is a non-negative integer, span U.g/ and are linearly independent.
In particular, the elements i.X1/; : : : ; i.Xk/ are linearly independent, meaning that
the map i W g ! U.g/ is injective.
In (9.8), we interpret i.Xj/nj as 1 if nj D 0: Since, actually, i is injective, we
will henceforth identify g with its image under i and thus regard g as a subspace of
U.g/: Thus, we may now write X in place of i.X/: In our new notation, we may
write (9.4) as
ŒX; Y  D XY  YX
and we may write the basis elements (9.8) as
Xn1
1 Xn2
2    Xnk
k :
(9.9)
It is straightforward to show that the elements in (9.8) span U.g/; the hard part is to
prove they are linearly independent.
Corollary 9.10. If g is a Lie algebra and h is a subalgebra of g; then there is a
natural injection of U.h/ into U.g/ given by mapping any product X1X2    XN of
elements of h to the same product in U.g/:
Proof. The inclusion of h into g induces an algebra homomorphism of  W U.h/ !
U.g/: Let us choose a basis X1; : : : ; Xk for h and extend it to a basis X1; : : : ; XN
for g: By the PBW theorem for h; the elements Xn1
1    Xnk
k
form a basis for U.h/:
Then by the PBW theorem for g; the corresponding elements of U.g/ are linearly
independent, showing that  is injective.
ut
9.4
Proof of the PBW Theorem
It is notationally convenient to write the elements of the claimed basis for U.g/ as
i.Xj1/i.Xj2/    i.XjN /;
(9.10)
with j1  j2      jN ; where we interpret the above expression as 1 if N D 0:
The easy part of the PBW theorem is to show that these elements span U.g/: The
proof of this claim is essentially the same as the proof of the reordering lemma

9.4
Proof of the PBW Theorem
251
(Lemma 6.12) in Chapter 6. Every element of the tensor algebra T .g/, and hence
also of the universal enveloping algebra U.g/; is a linear combination of products of
Lie algebra elements. Expanding each Lie algebra element in our basis shows that
every element of U.g/ is a linear combination of products of basis elements, but not
(so far) necessarily in nondecreasing order. But using the relation XYYX D ŒX; Y ;
we can reorder any product of basis elements into the desired order, at the expense
of introducing several terms that are products of one fewer basis elements. These
smaller products can then, inductively, be rewritten as a linear combination of terms
that are in the correct order.
It may seem "obvious" that elements of the form (9.10) are linearly independent.
Note, however, that any proof of independence of these elements must make use of
the Jacobi identity. After all, if g is a vector space with any skew-symmetric, bilinear
"bracket" operation, we can still construct a "universal enveloping algebra" by the
construction in Sect. 9.3, and the elements of the form (9.10) will still span this
enveloping algebra. If, however, the bracket does not satisfy the Jacobi identity, the
elements in (9.10) will not be linearly independent. If they were, then, in particular,
the map i W g ! U.g/ would be injective. We could then identify g with its image
under i; which means that the bracket on g would be given by ŒX; Y  D XY  YX;
where XY and YX are computed in the associative algebra U.g/: But any bracket of
this form does satisfy the Jacobi identity.
We now proceed with the proof of the independence of the elements in (9.10).
The reader is encouraged to note the role of the Jacobi identity in our proof. Let D
be any vector space having a basis
fv.j1; ;jN /g:
indexed by all nondecreasing tuples .j1; : : : ; jN /. We wish to construct a linear map
 W U.g/ ! D with the property that
.Xj1Xj2    XjN / D v.j1; ;jN /
for each nondecreasing tuple .j1; : : : ; jN /: Since the elements v.j1; ;jN / are,
by construction, linearly independent, if such a map  exists, the elements
Xj1Xj2    XjN must be linearly independent as well. (Any linear relation among
the Xj1Xj2    XjN 's would translate under  into a linear relation among the
v.j1; ;jN /'s.)
Instead of directly constructing ; we will construct a linear map ı W T .g/ ! D
with the properties (1) that
ı.Xj1 ˝ Xj2 ˝    ˝ XjN / D v.j1; ;jN /
(9.11)
for all nondecreasing tuples .j1; : : : ; jN /; and (2) that ı is zero on the two-sided
ideal J spanned by the elements in (9.7). Since ı is zero on J; it gives rise to a map
˛ of U.g/ WD T .g/=J into D with the analogous property.

252
9
Representations of Semisimple Lie Algebras
To keep our notation compact, we will now omit the tensor product symbol for
multiplication in T .g/: All computations in the remainder of the section are in T .g/;
so there will be no confusion. Suppose we can construct ı in such a way that for all
tuples .j1; : : : ; jN / (not just for the nondecreasing tuples), we have
ı.Xj1    XjkXjkC1    XjN  Xj1    XjkC1Xjk    XjN /
D ı.Xj1    ŒXjk; XjkC1    XjN /:
(9.12)
Then ı will indeed by zero on J: After all, J is spanned by elements of the form
˛.XY YX/ˇ; and every such element can be expanded as a linear combination of
elements of the form on the left-hand side of (9.12).
Let the index of a monomial Xj1Xj2    Xjn be the number of pairs l < k for
which jl > jk: We will construct ı by induction ﬁrst on the degree, n; of the
monomial, and then by the index of the monomial for a given degree. For degrees
0 and 1; there is not much to do. Assume, now, that we have deﬁned ı on all
monomials of degree at most n  1; and that it satisﬁes (9.12) whenever N  n  1.
We now want to deﬁne ı on monomials of degree n; starting with monomials of
degree n and index 0: For these, we deﬁne ı by (9.11).
Now assume we have deﬁned ı on monomials of degree n and index p in such a
way that (9.12) holds whenever both monomials on the left-hand side of (9.12) have
index at most p: Note that if p D 0; this condition holds vacuously, since the two
terms on the left-hand side cannot both have index 0 unless jkC1 D jk; in which
case both sides of (9.12) are zero. We now consider a monomial Xj1Xj2    Xjn
of index p C 1  1: Since the index of the monomial is at least 1, the sequence
.j1; j2; : : : ; jn/ is not weakly increasing, so there must be some jk with jk > jkC1:
Pick such a k and "deﬁne" ı on the monomial by
ı.Xj1    XjkXjkC1    XjN / D ı.Xj1    XjkC1Xjk    XjN /
C ı.Xj1    ŒXjk; XjkC1    XjN /:
(9.13)
Note that the ﬁrst term on the right-hand side of (9.13) has index p and the second
term on the right-hand side has degree n  1; which means that both of these terms
have been previously deﬁned.
The crux of the matter is to show that the value of ı on a monomial of index
p C 1 is independent of the choice of k in (9.13). Suppose, then, that there is some
l < k such that jl > jlC1 and jk > jkC1: We now proceed to check that the value
of the right-hand side of (9.13) is unchanged if we replace k by l:
Case 1:
l  k  2: In this case, the numbers l; l C 1; k; k C 1 are all distinct. Let
us consider the two apparently different ways of calculating ı: If we use l; then
we have
ı.  XlXlC1    XkXkC1    /
D ı.   XlC1Xl    XkXkC1    / C ı.   ŒXl; XlC1    XkXkC1    /:
(9.14)

9.4
Proof of the PBW Theorem
253
Now, the second term on the right-hand side of (9.14) has degree n  1: The ﬁrst
term has index p, and if we reverse Xk and XkC1 we obtain a term of index p1:
Thus, we can apply our induction hypothesis to reverse the order of Xk and XkC1
in both terms on the right-hand side of (9.14), giving
ı.  XlXlC1    XkXkC1    /
D ı.  XlC1Xl    XkC1Xk    / C ı.  XlC1Xl    ŒXk; XkC1    /
C ı.   ŒXl; XlC1    XkC1Xk    / C ı.  ŒXl; XlC1    ŒXk; XkC1    /:
(9.15)
(Note that on the right-hand side of (9.15), all terms have both Xl and XlC1 and
Xk and XkC1 back in their correct PBW order, with XlC1 to the left of Xl and
XkC1 to the left of Xk:)
But the right-hand side of (9.15) is symmetric in k and l; meaning that it is easy
to check that we get the same result if we started with k instead of l:
Case 2:
l D k  1. In this case, the indices jl; jlC1 D jk; and jlC2 D jkC1 are in
the completely wrong order, jl > jlC1 > jlC2: Let us use the notation X D Xl;
Y D XlC1, and Z D XlC2: We wish to show that the value of ı.   XYZ    / is
the same whether we use l (that is, interchanging X and Y ) or we use k (that is,
interchanging Y and Z).
If we interchange Y and Z, we get
ı.  XYZ    / D ı.  XZY    / C ı.  XŒY; Z    /:
We now further simplify the result, by induction, until the factors of X; Y , and
Z are in their correct PBW order, which is ZYX: We obtain, then,
ı.  XYZ    /
D ı.  ZXY    / C ı.  ŒX; ZY    / C ı.   XŒY; Z    /
D ı.  ZYX    / C ı.   ZŒX; Y     /
C ı.  ŒX; ZY    / C ı.  XŒY; Z    /:
(9.16)
Meanwhile, if we interchange X and Y; we get
ı.   XYZ    /
D ı.  YXZ    / C ı.   ŒX; Y Z    /
D ı.  YZX    / C ı.  Y ŒX; Z    / C ı.  ŒX; Y Z    /
D ı.  ZYX    / C ı.  ŒY; ZX    /
C ı.  Y ŒX; Z    / C ı.  ŒX; Y Z    /:
(9.17)

254
9
Representations of Semisimple Lie Algebras
The two ways of computing have a common term of ı.   ZYX    /: The
remaining terms differ in that each commutator has the remaining factor on the
other side, such as ZŒX; Y  in (9.16) versus ŒX; Y Z in (9.17). Subtracting gives
ı.  ZŒX; Y     /  ı.   ŒX; Y Z    /
C ı.  ŒX; ZY    /  ı.  Y ŒX; Z    /
C ı.  XŒY; Z    /  ı.   ŒY; ZX    /
(9.18)
Since all of the terms in (9.18) are of degree n  1; we can apply our induction
hypothesis to simplify the result to
ı.ŒZ; ŒX; Y  C ŒŒX; Z; Y  C ŒX; ŒY; Z/:
But ŒŒX; Z; Y  D ŒY; ŒZ; X; which mean that the argument of ı in the above
expression is zero, by the Jacobi identity.
Once we have veriﬁed that the value of ı is independent of the choice of k in (9.13),
it is clear that (9.12) holds, and we have completed the construction of the map ı:
9.5
Construction of Verma Modules
The proof will make use of the following deﬁnition: A subspace I of U.g/ is called
a left ideal if ˛ˇ 2 I for all ˛ 2 U.g/ and all ˇ 2 I: For any collection of vectors
f˛jg in U.g/; we may form the left ideal I "generated by" these vectors, that is, the
smallest left ideal in U.g/ containing each ˛j : The left ideal I is precisely the space
of elements of the form
X
j
ˇj ˛j
with ˇj being arbitrary elements of U.g/:
Let I denote the left ideal in U.g/ generated by elements of the form
H  h; Hi 1;
H 2 h
(9.19)
and of the form
X 2 g˛;
˛ 2 RC:
(9.20)
We now let W denote the quotient vector space
W D U.g/=I;
and we let Œ˛ denote the image of ˛ 2 U.g/ in the quotient space.

9.5
Construction of Verma Modules
255
We may deﬁne a representation  of U.g/ acting on W by setting
.˛/.Œˇ/ D Œ˛ˇ
(9.21)
for all ˛ and ˇ in U.g/: To verify that .˛/ is well deﬁned, note that if ˇ0 is
another representative of the equivalence class Œˇ; then ˇ0 D ˇ C  for some  in
I: But then ˛ˇ0 D ˛ˇ C˛; and ˛ belongs to I; because I is a left ideal. Thus,
Œ˛ˇ0 D Œ˛ˇ: We may check that  is a homomorphism by noting that .˛ˇ/Œ
and .˛/.ˇ/Œ both equal Œ˛ˇ; by the associativity of U.g/: The restriction
of  to g constitutes a representation of g acting on W:
Deﬁnition 9.11. The Verma module with highest weight ; denoted W; is the
quotient space U.g/=I, where I is the left ideal in U.g/ generated by elements of
the form (9.19) and (9.20).
Theorem 9.12. The vector v0 WD Œ1 is a nonzero element of W and W is
a highest weight cyclic representation with highest weight  and highest weight
vector v0:
The hard part of the proof is establishing that v0 is nonzero; this amounts to
showing that the element 1 of U.g/ is not in I: For purposes of constructing the
irreducible, ﬁnite-dimensional representations of g; Theorem 9.12 is sufﬁcient. Our
method of proof, however, gives more information about the structure of W; which
we will make use of in Chapter 10.
Let nC denote the span of the root vectors X˛ 2 g˛ with ˛ 2 RC; and let
n denote the span of the root vectors Y˛ 2 g˛ with ˛ 2 RC: Because Œg˛; gˇ 
g˛Cˇ; both nC and n are subalgebras of g:
Theorem 9.13. If Y1; : : : ; Yk form a basis for n; then the elements
.Y1/n1.Y2/n2    .Yk/nkv0;
(9.22)
where each nj is a non-negative integer, form a basis for W:
The theorem, together with the PBW theorem, tells us that there is a vector space
isomorphism between U.n/ and W given by ˛ 7! .˛/v0; where  is the
action of U.g/ on W; given by (9.21).
Lemma 9.14. Let J denote the left ideal in U.b/  U.g/ generated by elements
of the form (9.19) and (9.20). Then 1 does not belong to J.
Proof. Let b be the direct sum (as a vector space) of nC and h; which is easily seen
to be a subalgebra of g: Let us deﬁne a one-dimensional representation 	 of b;
acting on C; by the formula
	.X C H/ D h; Hi :

256
9
Representations of Semisimple Lie Algebras
(That is to say, 	.X C H/ is the 1  1 matrix with entry h; Hi :) To see that 	
is actually a representation, we note that all 1  1 matrices commute. On the other
hand, the commutator of two elements Z1 and Z2 of b will lie in nC; and 	 is
deﬁned to be zero on n: Thus, 	.ŒZ1; Z2/ and Œ	.Z1/; 	.Z2/ are both zero.
By Proposition 9.8, the representation 	 of b extends to a representation Q	
of U.b/ satisfying Q	.1/ D 1: Now, the kernel of Q	 is easily seen to be a left
ideal in U.b/; and by construction, the kernel of Q	 will contain all elements of
the form (9.19) and (9.20). But since Q	.1/ D 1; the element 1 does not belong to
the kernel of Q	: Thus, ker.Q	/ is a left ideal in U.b/ containing all elements of the
form (9.19) and (9.20), which means that ker.Q	/ contains J: Since ker.Q	/ does
not contain 1; neither does J:
ut
Proof of Theorems 9.12 and 9.13. Note that for any H 2 h; we have
..H  h; Hi 1//v0 D ŒH  h; Hi 1 D 0;
because H  h; Hi 1 belongs to I: Thus, .H/v0 D h; Hi v0: Similarly, for
any ˛ 2 RC; we have
.X˛/v0 D ŒX˛ D 0:
Since elements of U.g/ are linear combinations of products of elements of g; any
invariant subspace for the action of g on W is also invariant under the action of U.g/
on W: Suppose, then, that U is an invariant subspace of W containing v0 D Œ1:
Then for any Œ˛ 2 W; we have .˛/v0 D Œ˛; and so U D W: Thus, v0 is a
cyclic vector for W: To prove Theorem 9.12, it remains only to show that v0 ¤ 0:
The Lie algebra g decomposes as a vector space direct sum of n and b; where
n is the span of the root spaces corresponding to roots in R and where b is the
span of h and the root spaces corresponding to roots in RC: Let us choose a basis
Y1; : : : ; Yk; Z1; : : : ; Zl for g consisting of a basis Y1; : : : ; Yk for n together with a
basis Z1; : : : ; Zl for b: By applying the PBW theorem to this basis, we can easily
see (Exercise 4) that every element ˛ of U.g/ can be expressed uniquely in the form
˛ D
1
X
n1;:::;nkD0
Y n1
1 Y n2
2    Y nk
k an1;:::;nk;
(9.23)
where each an1;:::;nk belongs to U.b/  U.g/: (For each ˛ 2 U.g/; only ﬁnitely
many of the an1;:::;nk's will be nonzero.)
Suppose now that ˛ belongs to I; which means that ˛ is a linear combination
of terms of the form ˇ.H  h; Hi 1/ and ˇX˛; with ˇ in U.g/; H in h; and X˛ in
g˛; ˛ 2 RC: By writing each ˇ as in (9.23), we see that ˛ is a linear combination
of terms of the form
Y n1
1 Y n2
2    Y nk
k bn1;:::;nk.H  h; Hi 1/

9.6
Irreducible Quotient Modules
257
and
Y n1
1 Y n2
2    Y nk
k bn1;:::;nkX˛;
with bn1;:::;nk in U.b/: Note that bn1;:::;nk.H  h; Hi 1/ and bn1;:::;nkX˛ belong
to the left ideal J  U.b/: Thus, if ˛ is in I; each an1;:::;nk in the (unique)
expansion (9.23) of ˛ must belong to J.
Now, by the uniqueness of the expansion, the only way the element ˛ in (9.23)
can equal 1 is if there is only one term, the one with n1 D    D nk D 0; and if
a0;:::;0 D 1: On the other hand, for ˛ to be in I; each an1;:::;nk must belong to J:
Since (Lemma 9.14) 1 is not in J; we see that 1 is not in I; so that v0 D Œ1 is
nonzero in U.g/=I:
We now argue that the vectors in (9.22) are linearly independent in U.g/=I:
Suppose, then, that a linear combination of these vectors, with coefﬁcients cn1;:::;nk
equals zero. Then the corresponding linear combination of elements in U.g/; namely
˛ WD
1
X
n1;:::;nkD0
Y n1
1 Y n2
2 : : : Y nk
k cn1;:::;nk;
belongs to I: But as shown above, for ˛ to be in I; each of the constants cn1;:::;nk
must be in J: Thus, by Lemma 9.14, each of the constants cn1;:::;nk is zero.
ut
The main role of the PBW theorem in the preceding proof is to establish the
uniqueness of the expansion (9.23).
9.6
Irreducible Quotient Modules
In this section, we show that every Verma module has a largest proper invariant
subspace U and that the quotient space V WD W=U is irreducible with highest
weight : In the next section, we will show that if  is dominant integral, this
quotient space is ﬁnite dimensional.
It is easy to see (Exercise 6) that the Verma module W is the direct sum of its
weight spaces. It therefore makes sense to talk about the component of a vector
v 2 W in the one-dimensional subspace spanned by v0; which we refer to as the
v0-component of v: As in the previous section, we let nC denote the subalgebra of
g spanned by weight vectors X˛ 2 g˛; with ˛ 2 RC:
Deﬁnition 9.15. For any Verma module W; let U be the subspace of V
consisting of all vectors v such that the v0-component of v is zero and such that
the v0-component of
.X1/    .XN /v
is also zero for any collection of vectors X1; : : : ; XN in nC:

258
9
Representations of Semisimple Lie Algebras
That is to say, a vector v belongs to U if we cannot "get to" v0 from v by
applying "raising operators" X 2 g˛; ˛ 2 RC: Certainly the zero vector is in UI
for some g's and 's, it happens that U D f0g:
Proposition 9.16. The space U is an invariant subspace for the action of g:
Proof. Suppose that v is in U and that Z is some element of g: We want to show
that .Z/v is also in U: Thus, we consider
.X1/    .Xl/.Z/v
(9.24)
and we must show that the v0-component of this vector is zero. Using the reordering
lemma (Lemma 6.12), we may rewrite the vector in (9.24) as a linear combination
of vectors of the form
.Y 1/    .Y j/.H 1/   .H k/. QX1/    . QXm/v;
(9.25)
where the Y 's are in n; the H's are in h; and the QX's are in nC: However, since v
is in U; the v0-component of
. QX1/    . QXm/v
(9.26)
is zero; thus, this vector is a linear combination of weight vectors with weight lower
than : Then applying elements of h and n to the vector in (9.26) will only keep
the weights the same or lower them. Thus, the v0-component of the vector in (9.25),
and hence also the v0-component of the vector in (9.24), is zero. This shows that
.Z/v is, again, in U:
ut
Since U is an invariant subspace of W; the quotient vectors space W=U
carries a natural action of g and thus constitutes a representation of g:
Proposition 9.17. The quotient space V WD W=U is an irreducible representa-
tion of g:
Proof. A simple argument shows that the invariant subspaces of the representation
W=U are in one-to-one correspondence with the invariant subspaces of W that
contain U: Thus, proving that V is irreducible is equivalent to showing that any
invariant subspace of W that contains U is either U or W: Suppose, then, that
X is an invariant subspace that contains U and at least one vector v that is not
in U: This means that X also contains a vector u D .X1/   .Xk/v whose
v0-component is nonzero.
We now claim that X must contain v0 itself. To see this, we decompose u as
a nonzero multiple of v0 plus a sum of weight vectors corresponding to weights
 ¤ : Since  ¤ ; we can ﬁnd H in h with h; Hi ¤ h; Hi and then we
may apply to u the operator .H/  h; Hi I: This operator will keep us in X
and will "kill" the component of u that is in the weight space corresponding to
the weight  while leaving the v0-component of u nonzero. We can then continue

9.6
Irreducible Quotient Modules
259
applying operators of this form until we have killed all the components of u in
weight spaces different from ; giving us a nonzero multiple of v0: We conclude,
then, that X contains v0 and, therefore, all of W: Thus, any invariant subspace of
W that properly contains U must be equal to W:
ut
Since for each u 2 U the v0-component of u is zero, the vector v0 is not
in U: Thus, the quotient space W=U is still a highest weight cyclic representation
with highest weight  and with highest weight vector being the image of v0 in the
quotient.
Example 9.18. Let ˛ be an element of 
 and let s˛ D hX˛; Y˛; H˛i be as in
Theorem 7.19. If h; H˛i is a non-negative integer m; then the vector v WD
.Y˛/mC1v0 belongs to U:
This result is illustrated in Figure 9.2.
Proof. By the argument in proof of Theorem 4.32, the analog of (4.15) will hold
here:
.X˛/.Y˛/jv0 D j.m  .j  1//.Y˛/j 1v0:
Thus,
.X˛/v D m.m  m/.Y˛/mv0 D 0:
v
2
1
Fig. 9.2 Example 9.18 in the case ˛ D ˛2 and m D 2: The vector v belongs to U

260
9
Representations of Semisimple Lie Algebras
Meanwhile, if ˇ 2 RC with ˇ ¤ ˛; then for Xˇ 2 gˇ; if .Xˇ/v were nonzero, it
would be a weight vector with weight
 D   .m C 1/˛ C ˇ:
Since  is not lower than ; we must have .Xˇ/v D 0: Thus, the v0-component of
v is zero and .X/v D 0 for all  2 RC; which implies that v is in U:
ut
9.7
Finite-Dimensional Quotient Modules
Throughout this section, we assume that  is a dominant integral element. We will
now show that, in this case, the irreducible quotient space V WD W=U is ﬁnite
dimensional. Our strategy is to show that the set of weights for V is invariant
under the action of the Weyl group on h: Now, if  is dominant integral, then every
weight  of W—and thus also of V—must be integral, since    is an integer
combination of roots. Hence, if the weights of V are invariant under W; we must
have w     for all w 2 W: But it is not hard to show that there are only ﬁnitely
many integral elements this property. We will conclude, then, that there are only
ﬁnitely many weights in V: Since (even in the Verma module) each weight has
ﬁnite multiplicity, this will show that V=U is ﬁnite dimensional.
How, then, do we construct an action of the Weyl group on V? If we attempt to
follow the proof of Theorem 9.3, we must contend with the fact that V is not yet
known to be ﬁnite dimensional. Thus, we need a method of exponentiating operators
on a possibly inﬁnite-dimensional space.
Deﬁnition 9.19. A linear operator X on a vector space V is locally nilpotent if for
each v 2 V; there exists a positive integer k such that Xkv D 0:
If V is ﬁnite dimensional, then a locally nilpotent operator must actually be
nilpotent, that is, there must exist a single k such that Xkv D 0 for all v: In
the inﬁnite-dimensional case, the value of k depends on v and there may be no
single value of k that works for all v: If X is locally nilpotent, then we deﬁne eX to
be the operator satisfying
eXv D
1
X
kD0
Xk
kŠ v;
(9.27)
where for each v 2 V; the series on the right terminates.
Proposition 9.20. For each ˛ 2 
; let s˛ D hX˛; Y˛; H˛i be as in Theorem 7.19.
If  is dominant integral, then X˛ and Y˛ act in a locally nilpotent fashion on the
quotient space V:

9.7
Finite-Dimensional Quotient Modules
261
Proof. For any X 2 g; we use QX as an abbreviation for the action of X on the
quotient space V: We say that a vector in V is s˛-ﬁnite if it is contained in a
ﬁnite-dimensional, s˛-invariant subspace. Let
m D h; H˛i ;
which is a non-negative integer because  is dominant integral. Let Qv0 denote the
image in V of the highest vector v0 2 W, and consider the vectors
Qvk WD QY k
˛ Qv0;
k D 0; 1; 2; : : : :
By the calculations in Sect. 4.6, the span of the Qvk's is invariant under the action
of s˛: On the other hand, Example 9.18 shows that .Y˛/mC1v0 is in U; which
means that QvmC1 D QY mC1
˛
Qv0 is the zero element of V: Thus, hQv0; : : : ; Qvmi is a
ﬁnite-dimensional, s˛-invariant subspace (Figure 9.3). In particular, there exists a
nonzero, s˛-ﬁnite vector in V:
Now let T˛  V be the space of all s˛-ﬁnite vectors, which we have just shown
to be nonzero. We now claim that T˛ is invariant under the action of g: To see
this, ﬁx a vector v in T˛ and an element X of g: Let S be a ﬁnite-dimensional,
s˛-invariant subspace containing v and let S0 be the span of all vectors of the form
QY w with Y 2 g and w 2 S: Then S0 is ﬁnite dimensional, having dimension at most
.dim g/.dim S/: Furthermore, if Z 2 s˛; then for all w 2 S; we have
QZ QY w D QY QZw C A
ŒZ; Y w;
v
2
1
Fig. 9.3 Since the vector v is in U (Figure 9.2), the circled weights span a s˛2-invariant subspace
of W=U

262
9
Representations of Semisimple Lie Algebras
which belongs to S0; because QZw is again in S: Thus, S0 is also invariant under
the action of s˛: We see, then, that QXv is contained in the ﬁnite-dimensional, s˛-
invariant subspace S0; that is, QXv 2 T˛: Since V is irreducible and T˛ is nonzero
and invariant under the action of g, we have T˛ D V:
We conclude that every v 2 V is contained in a ﬁnite-dimensional, s˛-invariant
subspace. It then follows from Point 2 of Theorem 4.34 that . QX˛/kv D . QY˛/kv D 0
for some k, showing that QX˛ and QY˛ are locally nilpotent.
ut
Proposition 9.21. If  is dominant integral, the set of weights for V is invariant
under the action of the Weyl group on h:
Proof. We continue the notation from the proof of Proposition 9.20. Since (Propo-
sition 8.24) W is generated by the reﬂections s˛ with w 2 
; it sufﬁces to show that
the weights of V=U are invariant under each such reﬂection. By Proposition 9.20,
QX˛ and QY˛ are locally nilpotent, and thus it makes sense to deﬁne operators S˛ by
S˛ D e QX˛e QY˛e QX˛:
We may now imitate the proof of Theorem 9.3 as follows. If H 2 h satisﬁes
h˛; Hi D 0; then ŒH; X˛ D ŒH; Y˛ D 0; which means that QH commutes with
QX˛ and QY˛ and, thus, with S˛: Meanwhile, for any v 2 V; we may ﬁnd a ﬁnite-
dimensional, s˛-invariant subspace S containing v: In the space S; we may apply
Point 3 of Theorem 4.34 to show that
S˛ QH˛S1
˛ v D  QH˛v:
We conclude that for all H 2 h; we have
S˛ QHS1
˛
D s˛  QH:
From this point on, the proof of Theorem 9.3 applies without change.
ut
Figure 9.4 illustrates the result of Proposition 9.21 in the case of sl.2I C/ and
highest weight 3: If v is a weight vector with weight 5; then .X/v D 0;
by (4.15) in the proof of Theorem 4.32. Thus, the span of the weight vectors with
weights l  5 is invariant. The quotient space has weights ranging from 3 to 3
in increments of 2 and is, thus, invariant under the action of W D fI; Ig:
1
3
7
5
3
1
0
0
Fig. 9.4 In the Verma module for sl.2I C/ with highest weight 3; the span of the weight vectors
with weights 5; 7; : : : ; is invariant

9.8
Exercises
263
We are now ready for the proof of the existence of ﬁnite-dimensional, irreducible
representations.
Proof of Theorem 9.5. The quotient space V WD W=U is irreducible and has
highest weight : Every weight  of V is integral and satisﬁes   : By
Proposition 9.21, the weights are also invariant under the action of W; which means
that every weight  satisﬁes w     for all w 2 W: Thus, by Proposition 8.44, 
must be in the convex hull of the W -orbit of ; which implies that kk  kk :
Since there are only ﬁnitely many integral elements  with this property, we
conclude that V has only ﬁnitely many weights.
Now, V has at least one weight, namely : Since V is irreducible, it must be
the direct sum of its weight spaces. Now, since the elements in (9.22) form a basis
for the Verma module W; the corresponding elements of V certainly span V:
But for a given weight ; there are only ﬁnitely many choices of the exponents
n1; : : : ; nk in (9.22) that give a weight vector with weight : (After all, if any of the
nj 's is large, the weight of the vector in (9.22) will be much lower than :) Thus,
each weight of V has ﬁnite multiplicity. Since, also, there are only ﬁnitely many
weights, V is ﬁnite dimensional.
ut
9.8
Exercises
1. Suppose .i; U.g// and .i0; U 0.g// are algebras as in Theorem 9.7. Show that there
is an isomorphism ˆ W U.g/ ! U 0.g/ such that ˆ.1/ D 1 and such that
ˆ.i.X// D i0.X/
for all X 2 g:
Hint: Use the deﬁning property of U.g/ to construct ˆ:
2. Suppose that g  Mn.C/ is a Lie algebra of matrices (with bracket given by
XY  YX). Prove, without appealing to the PBW theorem, that the map i W g !
U.g/ in Theorem 9.7 is injective.
3. Suppose g is a Lie algebra and h is a subalgebra. Apply Theorem 9.7 to the Lie
algebra h with A D U.g/ and with j being the inclusion of h into g  U.g/:
If  W U.h/ ! U.g/ is the associated algebra homomorphism, show that  is
injective.
Hint: Use the PBW theorem.
4. Using the PBW theorem for b (applied to the basis Z1; : : : ; Zl) and for g (applied
to the basis Y1; : : : ; Yk; Z1; : : : ; Zl), establish ﬁrst the existence and then the
uniqueness of the expansion in (9.23).
Hint: For the uniqueness result, ﬁrst prove that if ˛ is a nonzero element of U.b/;
then Y n1
1    Y nk
k ˛ is a nonzero element of U.g/; for any sequence n1; : : : ; nk of
non-negative integers. Then prove that a linear combination as in (9.23) cannot
be zero unless each of the elements an1;:::nk in U.b/ is zero.

264
9
Representations of Semisimple Lie Algebras
5. Let  be any element of h and let W WD U.g/=I be the Verma module with
highest weight : Now let 	 be any other highest weight cyclic representation
of g with highest weight ; acting on a vector space W: Show that there is a
surjective intertwining map  of V onto W:
Note: It follows that W is isomorphic to the quotient space V= ker./: Thus,
V is maximal among highest weight cyclic representations with highest weight
; in the sense that every other such representation is a quotient of V:
Hint: If Q	 is the extension of 	 to U.g/, as in Proposition 9.8, construct a map
 W U.g/ ! W by mapping ˛ 2 U.g/ to Q	.˛/w0, where w0 is a highest weight
vector for W:
6. Let W WD U.g/=I be the Verma module with highest weight  and highest
weight vector v0: Let X be the subspace of W consisting of all those vectors
that can be expressed as ﬁnite linear combinations of weight vectors. Show that
X contains v0 and is invariant under the action of g on W: Conclude that W is
the direct sum of its weight spaces.
7. Let  be any element of h and let W be the associated Verma module. Suppose
 2 h can be expressed in the form
 D   n1˛1      nk˛k
(9.28)
where ˛1; : : : ; ˛k are the positive roots and n1; : : : ; nk are non-negative integers.
Show that the multiplicity of  in W is equal to the number of ways that   
can be expressed as a linear combination of positive roots with non-negative
integer coefﬁcients. That is to say, the multiplicity of  is the number of k-tuples
of non-negative integers .n1; : : : ; nk/ for which (9.28) holds.

Chapter 10
Further Properties of the Representations
In this chapter we derive several important properties of the representations we
constructed in the previous chapter. Throughout the chapter, g D kC denotes a
complex semisimple Lie algebra, h D tC denotes a ﬁxed Cartan subalgebra of g;
and R denotes the set of roots for g relative to h: We let W denote the Weyl group,
we let 
 denote a ﬁxed base for R; and we let RC and R denote the positive and
negative roots with respect to 
; respectively.
10.1
The Structure of the Weights
In this section, we establish the general version of Theorems 6.24 and 6.25 in the
case of sl.3I C/; which tells us which integral elements appear as weights of a
ﬁxed ﬁnite-dimensional irreducible representation. In Sect. 10.6, we will establish a
formula for the multiplicities of the weights, as a consequence of the Weyl character
formula.
Recall that the weights of a ﬁnite-dimensional representation of g are integral
elements (Proposition 9.2) and that the weights and their multiplicities are invariant
under the action of W (Theorem 9.3). We now determine which weights occur in
the representation with highest weight : Recall from Deﬁnition 6.23 the notion of
the convex hull of a collection of vectors.
Theorem 10.1. Let .; V/ be an irreducible ﬁnite-dimensional representation of
g with highest weight : An integral element  is a weight of V if and only if the
following two conditions are satisﬁed.
1.  belongs to the convex hull of the Weyl-group orbit of :
2.    can be expressed as an integer combination of roots.
© Springer International Publishing Switzerland 2015
B. Hall, Lie Groups, Lie Algebras, and Representations, Graduate
Texts in Mathematics 222, DOI 10.1007/978-3-319-13467-3_10
265

266
10
Further Properties of the Representations
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
4
4
4
4
4
4
4
5
5
5
5
5
5
5
5
6
6
6
6
6
6
6
Fig. 10.1 Typical weight diagram for the Lie algebra so.5I C/
Figure 10.1 shows a typical example for the Lie algebra so.5I C/: In the ﬁgure,
the square lattice indicates the set of integral elements, the highest weight is circled,
and the black dots indicate the weights of the representation. A number next to
a dot indicates the multiplicity, with an unnumbered dot representing a weight of
multiplicity 1. The multiplicities can be calculated using the Kostant multiplicity
formula (Sect. 10.6). Note that the multiplicities do not have the sort of simple
pattern that we saw in Sect. 6.7 for the case of sl.3I C/; that is, the multiplicities
for so.5I C/ are not constant on the "rings" in the weight diagram.
We will use the notation W   to denote the Weyl group orbit of an element ;
and the notation Conv.E/ to denote the convex hull of E: The following result is
the key step on the way to proving Theorem 10.1.
Proposition 10.2. Let  be a dominant integral element. Suppose  is dominant, 
is lower than ; and    can be expressed as an integer combination of roots.
Then  is a weight of the irreducible representation with highest weight :
Lemma 10.3 ("No Holes" Lemma). Suppose .; V / is a ﬁnite-dimensional repre-
sentation of g: Suppose that  is a weight of V and that h; ˛i > 0 for some root ˛:
If we deﬁne j by
j D h; H˛i D 2h; ˛i
h˛; ˛i ;

10.1
The Structure of the Weights
267
then k˛ is a weight of  for every integer k with 0  k  j: In particular, ˛
is a weight of V:
Note that since  is integral, j must be a (positive) integer.
Proof. Let s˛ D hX˛; Y˛; H˛i be the copy of sl.2I C/ corresponding to the weight ˛
(Theorem 7.19). Let U be the subspace of V spanned by weight spaces with weights
 of the form  D   k˛ for k 2 Z: Since X˛ and Y˛ shift weights by ˙˛; the
space U is invariant under s˛: Note that since h˛; H˛i D 2; we have
h  k˛; H˛i D j  2k:
That is, the weight space corresponding to weight k˛ is precisely the eigenspace
for .H˛/ inside U corresponding to the eigenvalue j  2k:
By Point 4 of Theorem 4.34, if j > 0 is an eigenvalue for .H˛/ inside U; then
all of the integers j  2k; 0  k  j; must also be eigenvalues for .H˛/ inside U:
Thus,   k˛ must be a weight of  for 0  k  j: Since j is a positive integer, j
must be at least 1, and, thus,   ˛ must be a weight of :
ut
Figure 10.2 illustrates the results of the "no holes" lemma. For the indicated
weight , the orthogonal projection of  onto ˛ equals .3=2/˛; so that j D 3: Thus,
  ˛;   2˛; and   3˛ must also be weights.
Proof of Proposition 10.2. Since    is an integer combination of roots,   
is also an integer combination of the positive simple roots ˛1; : : : ; ˛r: Since, also,
  ; we have
Fig. 10.2 Since  is a weight, each of the circled elements must also be a weight

268
10
Further Properties of the Representations
 D  C
r
X
j D1
kj ˛j
for some non-negative integers k1; : : : ; kr: Consider now the following set P of
integral elements,
P D
8
<
: D  C
r
X
j D1
lj˛j
ˇˇˇˇˇˇ
0  lj  kj
9
=
; :
(10.1)
The elements of P form a discrete parallelepiped.
We do not claim that every element of P is a weight of ; which is not, in
general, true (Figure 10.3). Rather, we will show that if  ¤  is a weight of  in P;
then there is another weight of V in P that is "closer" to : Speciﬁcally, for each
element  of P as in (10.1), let L./ D P
j lj: Starting from ; we will construct a
sequence of weights of V in P with decreasing values of L./; until we reach one
with L./ D 0; which means that  D :
Suppose then that  is a weight of  in P with L./ > 0: In that case, the second
term in the formula for  is nonzero, and, thus,
* r
X
j D1
lj˛j ;
r
X
kD1
lk˛k
+
D
r
X
kD1
lk
* r
X
j D1
lj˛j ; ˛k
+
> 0:
Since each lk is non-negative,there must be some ˛k for which lk > 0 and for which
* r
X
j D1
lj˛j ; ˛k
+
> 0:
On the other hand, since  is dominant, h; ˛ki  0; and we conclude that h; ˛ki >
0: Thus, by the "no holes" lemma,   ˛k must also be a weight of :
Now, since lk is positive, lk  1 is non-negative, meaning that   ˛k is still
in P , where all the lj's are unchanged except that lk is replaced by lk  1: Thus,
L.  ˛k/ D L./  1: We can then repeat the process starting with   ˛k and
obtain a sequence of weights of  with successively smaller values of L; until we
reach L D 0; which corresponds to  D :
ut
Figure 10.3 illustrates the proof of Proposition 10.2. Starting at ; we look for
a sequence of weights of  in P: Each weight in the sequence has positive inner
product either with ˛1 or with ˛2; allowing us to move in the direction of ˛1 or
˛2 to another weight of  in P; until we reach :
Proof of Theorem 10.1. Let X  V denote the span of all weight vectors whose
weights differ from  by a linear combination of roots. Then X is easily seen to be
invariant under the action of g and X contains v0; so X D V: Thus, every weight

10.2
The Casimir Element
269
P
2
1
Fig. 10.3 The thick line indicates a path of weights in P connecting  to 
of  must satisfy Point 2 of Theorem 10.1. Furthermore, if  is a weight of ; then
w     for all w 2 W: Thus, by Proposition 8.44,  2 Conv.W  /:
Conversely, suppose that  satisﬁes the two conditions of Theorem 10.1. We
can choose w 2 W so that 0 WD w   is dominant. Clearly, 0 still belongs to
Conv.W  /: Furthermore, since  is integral, w     is an element of the root
lattice. After all, the deﬁnition of integrality implies that s˛     is an integer
multiple of ˛; since the s˛'s generate W; the result holds for all w 2 W: Thus,
  0 D    C   0 is an element of the root lattice, which means that 0
also satisﬁes the two conditions of Theorem 10.1. Thus, by Proposition 10.2, 0 is a
weight of ; which means that  D w1  0 is also a weight.
ut
10.2
The Casimir Element
In this section, we construct an element of U.g/ known as the Casimir, which
belongs to the center of U.g/: The Casimir element is important in its own right
and also plays a crucial role in the proof of complete reducibility (Sect. 10.3) and of
the Weyl character formula (Sect. 10.8).
Deﬁnition 10.4. Let Xj be an orthonormal basis for k: Then the Casimir element
C of U.g/ is given by
C D 
X
j
X2
j :

270
10
Further Properties of the Representations
Proposition 10.5. 1. The value of C is independent of the choice of orthonormal
basis for k.
2. The element C is in the center of U.g/:
Proof. If fXjg and fYjg are two different orthonormal bases for k; then there is an
orthogonal matrix R such that
Yj D
X
k
RkjXk:
Then
X
j
Y 2
j D
X
j;k;l
RkjXkRljXl
D
X
k;l
X
j
Rkj.Rtr/jlXkXl
D
X
k;l
ıklXkXl
D
X
k
X2
k:
This shows that C is independent of the choice of basis.
Meanwhile, if fXjg is an orthonormal basis, let cjkl be the associated structure
constants:
ŒXj ; Xk D
X
l
cjklXl:
Note that for a ﬁxed j; the matrix Aj given by .Aj/jk D cjlk is the matrix
representing the operator adXj in the chosen basis. Since the inner product on k
is Ad-K-invariant, adXj is a skew operator, which means that cjkl is skew symmetric
in k and l for a ﬁxed j: If we compute the commutator of some Xj with C in U.g/;
we obtain
ŒXj; C D
X
k
ŒXj; X2
k
D
X
k
.ŒXj; XkXk C XkŒXj; Xk/
D
X
k;l
cjklXlXk C
X
k;l
cjklXkXl:
(10.2)
In the ﬁrst sum in the last line of (10.2), we may reverse the labeling of the
summation variables and use the skew symmetry of cjkl in k and l to obtain

10.2
The Casimir Element
271
ŒXj; C D
X
k;l
.cjkl C cjkl/XlXk D 0:
Thus, C commutes the each Xj: But since U.g/ is generated by elements of g; we
see that C actually commutes with every element of U.g/:
ut
Let  be a ﬁnite-dimensional, irreducible representation of g: By Proposition 9.8,
we can extend  to a representation of U.g/; which we also denote by . We
now show .C/ is a constant multiple of the identity. The formula for the constant
involves the element ı (Deﬁnition 8.37), equal to half the sum of the positive roots.
The element ı also arises in our discussion of the Weyl character formula, the Weyl
dimension formula, and the Kostant multiplicity formula.
Proposition 10.6. Let .; V / be a ﬁnite-dimensional irreducible representation of
g (extended to U.g/) with highest weight : Then we have
.C/ D 
X
j
.Xj/2 D cI;
where c is a constant given by
c D h C ı;  C ıi  hı; ıi :
Furthermore, c  0 with c D 0 only if  D 0:
Lemma 10.7. Let X 2 g˛ be a unit vector, so that X 2 g˛: Then under our usual
identiﬁcation of h with h; we have
ŒX; X D ˛:
Proof. According to Lemma 7.22, we have
hŒX; X; H˛i D h˛; H˛i hX; Xi D h˛; H˛i ;
(10.3)
since X (and thus, also, X) is a unit vector. On the other hand, we know that the
commutator of any element of g˛ with any element of g˛ is a multiple of H˛; which
is (under our identiﬁcation of h with h) a multiple of ˛: But if ŒX; X D c˛; (10.3)
tells us that c must equal 1.
ut
Proof of Proposition 10.6. Since C is in the center of the universal enveloping
algebra, .C/ commutes with each .X/; X 2 g: Thus, by Schur's lemma, .C/
must act as a constant multiple c of the identity operator.
To compute the constant c; we choose an orthonormal basis for k as follows.
Take an orthonormal basis H1; : : : ; Hj for t: Then for each ˛ 2 RC; choose a unit
vector X˛ in g˛; so that X
˛ is a unit vector in g˛: Then the elements
Y˛ WD .X˛ C X
˛ /=.
p
2i/I
Z˛ WD .X˛  X
˛ /=
p
2;

272
10
Further Properties of the Representations
satisfy Y 
˛ D Y˛ and Z
˛ D Z˛; which shows that these elements belong to k:
Since g˛ is orthogonal to g˛; it is easy to see that these vectors are also unit vectors
and orthogonal to each other. The set of vectors of the form Hj ; j D 1; : : : ; r; and
Y˛; ˛ 2 RC; and Z˛; ˛ 2 RC; form an orthonormal basis for k:
We compute that
Y 2
˛ D 1
2.X2
˛ C X˛X
˛ C X
˛ X˛ C .X
˛ /2/
Z2
˛ D 1
2.X2
˛  X˛X
˛  X
˛ X˛ C .X
˛ /2/;
so that
Y 2
˛  Z2
˛ D X˛X
˛ C X
˛ X˛
D 2X
˛ X˛ C ŒX˛; X
˛ :
Thus, the Casimir element C may be computed as
C D
X
˛2RC
.2X
˛ X˛ C ŒX˛; X
˛ / 
r
X
j D1
H 2
j :
Suppose now that v is a highest weight vector, and compute that
.C/v D 
r
X
j D1
.Hj /2v C 2
X
˛2RC
.X
˛ /.X˛/v
C
X
˛2RC
.ŒX˛; X
˛ /v:
Since v is a highest weight vector, .X˛/v D 0; and since Hj 2 t  h; we have
.Hj /v D i ˝; Hj
˛ v: Using Lemma 10.7, we then see that
.C/v D
0
@
r
X
j D1
˝
; Hj
˛2 C
X
˛2RC
h; ˛i
1
A v;
(10.4)
where the coefﬁcient of v on the right-hand side of (10.4) must equal c:
Now, since fHj gr
j D1 is an orthonormal basis for t; the ﬁrst term in the coefﬁcient
of v equals h; i. Moving the sum over ˛ inside the inner product in the second
term gives
c D h; i C h; 2ıi ;
(10.5)

10.3
Complete Reducibility
273
which is the same as h C ı;  C ıi  hı; ıi : Finally, we note that since  is
dominant, h; ˛i  0 for every positive root, from which it follows that h; 2ıi D
P
˛2RC h; ˛i is non-negative. Thus, c  0 for all  and c > 0 if  ¤ 0:
ut
10.3
Complete Reducibility
Let g be a complex semisimple Lie algebra. Then g is isomorphic to the complexi-
ﬁcation of a the Lie algebra of a compact matrix Lie group K: (This is, for us, true
by deﬁnition; see Deﬁnition 7.1.) Actually, it is possible to show that there exists
a simply connected compact Lie group K with Lie algebra k such that g Š kC:
(This claim follows from Theorems 4.11.6 and 4.11.10 of [Var].) Assuming this
result, we can see that every ﬁnite-dimensional representation  of g gives rise
to a representation of K by restricting  to k and then applying Theorem 5.6.
Theorem 4.28 then tells us that every ﬁnite-dimensional representation of g is
completely reducible; compare Corollary 4.11.11 in [Var].
Rather than relying on the existence of a simply connected K; we now give
an algebraic proof of complete reducibility. Our proof makes use of the Casimir
element and, in particular, the fact that the eigenvalue of the Casimir is nonzero in
each nontrivial irreducible representation.
Proposition 10.8. If .; V / is a one-dimensional representation of g; then
.X/ D 0 for all X 2 g:
Proof. By Theorem 7.8, g decomposes as a Lie algebra direct sum of simple
algebras gj: Since the kernel of jgj is a ideal, the restriction of  to gj must be
either zero or injective. But since dim gj  2 and dim.End.V // D 1; this restriction
cannot be injective, so it must be zero for each j:
ut
Theorem 10.9. Every ﬁnite-dimensional representation of a semisimple Lie alge-
bra is completely reducible.
We begin by considering what appears to be a very special case of the theorem.
Lemma 10.10. Suppose .; V / is a ﬁnite-dimensional representation of g and that
W is an invariant subspace of V of codimension 1. Then V decomposes as W ˚ U
for some invariant subspace U of V:
Proof. We consider ﬁrst the case in which W is irreducible. If W is one-
dimensional, then by Proposition 10.8, the restriction of  to W is zero. Since W
is one-dimensional and has codimension 1, V must be two dimensional. The space
of linear operators on V that are zero on W then has dimension 2. (Pick a basis for
V consisting of a nonzero element of W and another linearly independent vector;
in this basis, any such operator will have ﬁrst column equal to zero.) On the other
hand, each of the simple summands gj in the decomposition of g has dimension at
least 3, since gj contains a subalgebra isomorphic to sl.2I C/: Thus, the restriction

274
10
Further Properties of the Representations
of  to gj cannot be injective and thus must be zero. We conclude, then, that  is
identically zero, and we may take U to be any subspace of V complementary to W:
Assume now that W is irreducible and nontrivial. Let C be the Casimir element
of U.g/ and let .C/ denote the action of C on V; by means of the extension of 
to U.g/: By Proposition 10.6, the restriction of .C/ to W is a nonzero multiple c
of the identity. On the other hand, since V=W is one dimensional, the action of g
on V=W is trivial, by Proposition 10.8. Thus, the action of .C/ on V=W is zero,
from which it follows that .C/ must have a nonzero kernel. (If we pick a basis for
V consisting of a basis for W together with one other vector, the bottom row of the
matrix of .C/ in this basis be identically zero.) Because .C/ commutes with each
.X/; this kernel is an invariant subspace of V: Furthermore, ker.C/ \ W D f0g;
because .C/ acts as a nonzero scalar on W: Thus, U WD ker.C/ is the desired
invariant complement to W:
We consider next the case in which W has a nontrivial invariant subspace W 0;
which is, of course, also an invariant subspace of V: Then W=W 0 is a codimension-
one invariant subspace of V=W 0: Thus, by induction on the dimension of W; we
may assume that W=W 0 has a one-dimensional invariant complement, say Y=W 0:
Then W 0 is a codimension-one invariant subspace of Y: Since dim W 0 < dim W;
we may apply induction again to ﬁnd a one-dimensional invariant complement U to
W 0 in Y; so that Y D W 0 ˚ U: Now, Y \ W D W 0 and U \ W 0 D 0, from which
it follows that U \ W D f0g: Thus, U is the desired complement to W in V:
ut
Proof of Theorem 10.9. Let .; V / be a ﬁnite-dimensional representation of g
and let W be nontrivial invariant subspace of V: We now look for an invariant
complement to W; that is, an invariant subspace U such that V D W ˚ U: If we
can always ﬁnd such an U; then we may proceed by induction on the dimension to
establish complete reducibility. If A W V ! W is an intertwining map, the kernel
of A will be an invariant subspace of V: If, in addition, the restriction of A to W is
injective, then ker.A/ \ W D f0g and, by a dimension count, V will decompose as
W ˚ ker.A/: Now, the simplest way to ensure that A is injective on W is to assume
that AjW is a nonzero multiple of the identity (If, for example, the restriction of
is irreducible, nontrivial, and of codimension 1, we may take A D .C/ as in the
proof of Lemma 10.10.
To construct A in general, we proceed as follows. Let Hom.V; W / denote the
space of linear maps of V to W (not necessarily intertwining maps). This space can
be viewed as a representation of g by means of the action
X  A D .X/A  A.X/
(10.6)
for all X 2 g and A 2 Hom.V; W /: Let V denote the subspace of Hom.V; W /
consisting of those maps A whose restriction to W is a scalar multiple cA of the
identity and let W denote the subspace of Hom.V; W / consisting of those maps
whose restriction to W is zero. The map A 7! cA is a linear functional on V which
is easily seen not to be identically zero. The space W is the kernel of this linear
functional and is, thus, a codimension-one subspace of V:
A to W is the identity, we may think of A as a "projection" of V onto W:) If W

10.4
The Weyl Character Formula
275
We now claim that both V and W are invariant subspaces of Hom.V; W /: To see
this, suppose A 2 Hom.V; W / is equal to cI on W: Then for w 2 W; we have
.X  A/w D .X/Aw  A.X/w
D .X/cw  c.X/w
D 0;
because .X/w is again in W; showing that X  A is actually in W: Thus, the action
of g maps V into W; showing that both V and W are invariant.
Since W has codimension 1 in V; we are in the situation of Lemma 10.10. Thus,
there exists an invariant complement U to W in V: Let A be a nonzero element of
U: Since A is not in W; the restriction of A to W is a nonzero scalar. Furthermore,
since U is one dimensional, Proposition 10.8 tells us that the action of g on A is zero,
meaning that A commutes with each .X/: That is to say, A is an intertwining map
of V to W: Thus, A is precisely the operator we were looking for: an intertwining
map of V to W whose restriction to W is a nonzero multiple of the identity. Now,
A maps V into W; and actually maps onto W; since A acts as a nonzero scalar on
W itself. Thus, dim.ker.A// D dim V  dim W and since ker.A/ \ W D f0g; we
conclude that U WD ker.A/ is an invariant complement to W:
We conclude, then, that every nontrivial invariant subspace W of V has an
invariant complement U . By induction on the dimension, we may then assume
that both W and U decompose as direct sums of irreducible invariant subspaces,
in which case, V also has such a decomposition.
ut
10.4
The Weyl Character Formula
The character formula is a major result in the structure of the irreducible represen-
tations of g: Its consequences include a formula for the dimension of an irreducible
representation (Sect. 10.5) and a formula for the multiplicities of the weights in an
irreducible representation (Sect. 10.6).
We now introduce the notion of the character of a ﬁnite-dimensional representa-
tion of a group.
Deﬁnition 10.11. Suppose .; V / is a ﬁnite-dimensional representation of a com-
plex semisimple Lie algebra g: The character of  is the function  W g ! C
given by
.X/ D trace.e.X//:
It turns out that the character of  encodes many interesting properties of : We
will give a formula for the character of an irreducible representation of a semisimple
Lie algebra in terms of the highest weight of the representation. This section gives
the statement of the character formula, Sects. 10.5 and 10.6 give consequences of it,
and Sect. 10.8 gives the proof.

276
10
Further Properties of the Representations
If the representation  of g D kC comes from a representation ... of the compact
group K; then for X 2 k; we have .X/ D trace.....eX//: In Chapter 12, the group
version of a character, namely, the function given by x 7! trace.....x//; x 2 K;
plays a key role in the compact group approach to representation theory.
Proposition 10.12. If .; V / is a ﬁnite-dimensional representation of g; we have
the following results.
1. The dimension of V is equal to the value of  at the origin:
dim.V / D .0/:
2. Suppose V decomposes as a direct sum of weight spaces V with multiplicity
m./: Then for H 2 h we have
.H/ D
X

m./eh;Hi:
(10.7)
Proof. The ﬁrst point holds because trace.I/ D dim V . Meanwhile, since .H/
acts as h; Hi I in each weight space V; the second point follows from the
deﬁnition of :
ut
Example 10.13. Let  denote the irreducible representation of sl.2I C/ of dimen-
sion m C 1 and let
H D
 1
0
0 1

:
Then
 .aH/ D 
ea
0
0 ea

D ema C e.m2/a C    C ema:
(10.8)
We may also compute  as
.aH/ D sinh..m C 1/a/
sinh.a/
(10.9)
whenever a not an integer multiple of i:
Proof. The eigenvalues of m.H/ are m; m2; : : : ; m; from which (10.8) follows.
To obtain (10.9), we note that
.ea  ea/.aH/
D e.mC1/a C e.m1/a C    C e.m1/a
 e.m1/a      e.m1/a  e.mC1/a
D e.mC1/a  e.mC1/a;
(10.10)

10.4
The Weyl Character Formula
277
so that
.aH/ D e.mC1/a  e.mC1/a
ea  ea
D sinh..m C 1/a/
sinh.a/
;
as claimed.
ut
Note that in deriving (10.9) from (10.8), we multiplied the character by a cleverly
chosen combination of exponentials (ea  ea), leading to a large cancellation, so
that only two terms remain in (10.10). The Weyl character formula asserts that
we can perform a similar trick for the characters of irreducible representations of
arbitrary semisimple Lie algebras.
Recall that each element w of the Weyl group W acts as an orthogonal
linear transformation of it  h: We let det.w/ denote the determinant of this
transformation, so that det.w/ D ˙1: Recall from Deﬁnition 8.37 that ı denotes
half the sum of the positive roots. We are now ready to state the main result of this
chapter.
Theorem 10.14 (Weyl Character Formula). If .; V/ is an irreducible represen-
tation of g with highest weight ; then
.H/ D
P
w2W det.w/ehw.Cı/;Hi
P
w2W det.w/ehwı;Hi
(10.11)
for all H 2 h for which the denominator is nonzero.
Since we will have frequent occasion to refer to the function in the denominator
in (10.11), we give it a name.
Deﬁnition 10.15. Let q W h ! C be the function given by
q.H/ D
X
w2W
det.w/ehwı;Hi:
The function q is called the Weyl denominator.
The character formula may also be written as
q.H/.H/ D
X
w2W
det.w/ehw.Cı/;Hi:
(10.12)
Let us pause for a moment to reﬂect on what is going on in (10.12). The Weyl
denominator q.H/ is a sum of jW j exponentials with coefﬁcients equal to ˙1:
Meanwhile, the character .H/ is a large sum of exponentials with positive integer
coefﬁcients, as in (10.7). When we multiply these two functions, we seemingly
obtain an even larger sum of exponentials of the form
ehwıC;Hi;
for w 2 W and  a weight of ; with integer coefﬁcients.

278
10
Further Properties of the Representations
The character formula, however, asserts that most of these terms are not actually
present. Speciﬁcally, the only exponentials that actually appear are those of the form
ehw.Cı/;Hi; which occurs with a coefﬁcient of det.w/: The point is that, in most
cases, if a weight  can be written in the form  D w  ı C ; with  a weight of
; then  can be written in this form in more than one way. The character formula
asserts that unless  is in the Weyl-group orbit of ı C ; the coefﬁcient of eh;Hi;
after all the different contributions are taken into account, ends up being zero.
By contrast, the weight  D ıC only occurs once, since it corresponds to taking
the highest weight occurring in q (namely ı) and the highest weight occurring in 
(namely the highest weight  of ). (Note that by Propositions 8.38 and 8.27, the
elements of the form w  ı are all distinct. Then by Proposition 8.42, w  ı  ı for all
w:) Furthermore, since the weights occurring in both q and  are Weyl invariant,
the weight w  .ı C / also occurs only once. The Weyl character formula, then,
can be expressed as stating that if we compute the product q; a huge cancellation
occurs: Every exponential in the product ends canceling out to zero, except for those
that occur only once, namely those of the form w  .ı C /:
In the case of sl.2I C/; we have already observed how this cancellation occurs,
in (10.10). The Weyl denominator is equal to ea ea in this case. Each exponential
ela occurring in the product .eaea/m.aH/ will occur once with a plus sign (from
eae.l1/a) and once with a minus sign (from eae.lC1/a), except for the extreme
cases l D ˙.mC1/: This cancellation occurs because the multiplicity of the weight
l  1 equals the multiplicity of the weight l C 1; namely 1.
Figure 10.4, meanwhile, illustrates the case of the irreducible representation of
sl.3I C/ with highest weight .1; 2/: The top part of the ﬁgure indicates the six
exponentials occurring in the Weyl denominator, with alternating signs. The middle
part of the ﬁgure indicates the exponentials in the character of the representation
with highest weight .1; 2/: The bottom part of the ﬁgure shows the product of the
Weyl denominator and the character, in which only the six exponentials indicated
by black dots survive. The white dots in the bottom part of the ﬁgure indicate
exponentials that occur at least once in the product, but which end up with a
coefﬁcient of zero.
The cancellation inherent in the Weyl character formula reﬂects a very special
structure to the multiplicities of the various weights that occur. For an integral
element ; each product of the form ehwı;Hieh;Hi; where  D   w  ı; makes
a contribution of det.w/mult./ to the coefﬁcient of eh;Hi: Thus, if  is not in the
Weyl orbit of  C ı; the Weyl character formula implies that
X
w2W
det.w/mult.  w  ı/ D 0;
 ... W  . C ı/:
(10.13)
(In (10.13), some of elements of the form   w  ı may not actually be weights of
; in which case, the multiplicity should be considered to be zero.) In the case of
sl.3I C/; the weights of the form   w  ı form a small hexagon around the weight
: Figure 10.5 illustrates how the alternating sum of multiplicities around one such
hexagon equals zero.

10.4
The Weyl Character Formula
279
Fig. 10.4 The product of the
Weyl denominator (top) and
the character of a
representation (middle)
produces an alternating sum
of exponentials (bottom). The
white dots indicate
exponentials that occur at
least once in the product but
that end up with a coefﬁcient
of zero
1
1
1
1
1
1
2
2
2
1
1
1
1
1
1
We will see in Sect. 10.6 that the character formula leads to a formula for the
multiplicities of all the weights occurring in a particular irreducible representation.
Before concluding this section, we establish a technical result that we will use in
the remainder of the chapter.
Proposition 10.16. The exponential functions H 7! eh;Hi; with  2 h, are
linearly independent in C 1.h/:

280
10
Further Properties of the Representations
2
2
2
Fig. 10.5 We compute the alternating sum of multiplicities around the hexagon enclosing the
circled weight, beginning in the fundamental Weyl chamber and proceeding counterclockwise.
The result is 1  1 C 1  2 C 2  1 D 0
The proposition means that if a function f 2 C 1.h/ can be expressed as a ﬁnite
linear combination of exponentials, it has a unique such expression.
Proof. We need to show that if the function f W h ! C given by
f .H/ D c1eh1;Hi C    C cnehn;Hi
is identically zero, where 1; : : : ; n are distinct elements of h; then c1 D    D
cn D 0: If n D 1; we evaluate at H D 0 and conclude that c1 must be zero. If n > 1;
we choose, for each k D 2; : : : ; n; some Hk 2 h such that h1; Hki ¤ hk; Hki :
Since f is identically zero, so is the function
g WD .DH2  h2; H2i/   .DHn  hn; Hni/f
where DX denotes the directional derivative in the direction of X:
.DXf /.H/ D d
dtf .H C tX/
ˇˇˇˇ
tD0
:
(10.14)

10.5
The Weyl Dimension Formula
281
Direct calculation then shows that
g.H/ D
n
X
j D1
cj
 n
Y
kD2
.
˝
j; Hk
˛
 hk; Hki/
!
ehj ;Hi
D c1
 n
Y
kD2
.h1; Hki  hk; Hki/
!
eh1;Hi:
(10.15)
By evaluating at H D 0 and noting that, by construction, the product in the second
line of (10.15) is nonzero, we conclude that c1 D 0: An entirely similar argument
then shows that each cj D 0 as well.
ut
Corollary 10.17. Suppose f
2
C 1.h/ can be expressed as a ﬁnite linear
combination of exponentials eh;Hi; with  integral,
f .H/ D
X

ceh;Hi:
If f satisﬁes f .w  H/ D det.w/f .H/; then
cw D det.w/c
for each  occurring in the expansion of f:
Proof. On the one hand,
f .w  H/ D
X

cehw1;Hi D
X

cweh;Hi:
(10.16)
On the other hand,
f .w  H/ D det.w/f .H/ D
X

det.w/ceh;Hi:
(10.17)
By the linear independence of the exponentials, the only way the expansions
in (10.16) and (10.17) can agree is if cw D det.w/c for all :
ut
10.5
The Weyl Dimension Formula
Before coming to the proof of the Weyl character formula, we derive two important
consequences of it, the Weyl dimension formula (described in this section) and the
Kostant multiplicity formula (described in the next section).
The dimension of a representation is equal to the value of the character at the
identity (Proposition 10.12). In the Weyl character formula, however, both the
numerator and the denominator are equal to zero when H D 0: In the case of
sl.2I C/ case, for example, the character formula reads

282
10
Further Properties of the Representations
m.aH/ D sinh..m C 1/a/
sinh a
:
The limit of this expression as  tends to zero may be computed by l'Hospital's rule
to be m C 1; which is, of course, the dimension of Vm:
In the general case, we will expand both numerator and denominator of the
character formula in a power series. We will see that in both numerator and
denominator, the ﬁrst nonzero term has degree k; where
k D the number of positive roots.
To evaluate the limit of this expression at the origin, we will develop a version of
l'Hospital's rule. The limit is then computed as the ratio of a certain k-fold derivative
of the numerator and the corresponding k-fold derivative of the denominator,
evaluated at the origin. The result of this analysis is expressed in the following
theorem.
Theorem 10.18. If .; V/ is the irreducible representation of g with highest
weight ; then the dimension of V may be computed as
dim.V/ D
Q
˛2RC h˛;  C ıi
Q
˛2RC h˛; ıi
:
Note that both  C ı and ı are strictly dominant elements, so that all the factors
in both the numerator and the denominator are nonzero.
A function P on h is called a polynomial if for every basis of h; the function
P is a polynomial in the coordinates z1; : : : ; zr associated to that basis. That is to
say, P should be expressible as a ﬁnite linear combination of terms of the form
zn1
1 zn2
2    znr
r ; where n1; : : : ; nr are non-negative integers. It is easy to see that if P
is a polynomial in any one basis, then it is also a polynomial in every other basis as
well. A polynomial P is said to be homogeneous of degree l if P.cH/ D clP.H/
for all constants c and all H 2 h:
Deﬁnition 10.19. Let P W h ! C be the function given by
P.H/ D
Y
˛2RC
h˛; Hi :
Note that P is a product of k linear functions and is, thus, a homogeneous
polynomial of degree k: The dimension formula may be restated in terms of P as
dim.V/ D P. C ı/
P.ı/
:
A key property of the polynomial P is its behavior under the action of the Weyl
group.

10.5
The Weyl Dimension Formula
283
Deﬁnition 10.20. A function f W h ! C is said to be Weyl alternating if
f .w  H/ D det.w/f .H/
for all w 2 W and H 2 h:
It is easy to see, for example, that the Weyl denominator q is Weyl alternating
(Exercise 3).
Proposition 10.21. 1. The function P is Weyl alternating.
2. If f W h ! C is a Weyl-alternating polynomial, there is a polynomial g W h ! C
such that
f .H/ D P.H/g.H/:
In particular, if f is homogeneous of degree l < k, then f must be identically
zero, and if f is homogeneous of degree k; then f must be a constant multiple
of P:
Proof. For any w 2 W; consider the collection of roots of the form w1  ˛ for
˛ 2 RC: Since RC contains exactly one element out of each pair ˙˛ of roots, the
same is true of the collection of w1˛'s, with w ﬁxed and ˛ varying over RC: Thus,
P.w  H/ D
Y
˛2RC
h˛; w  Hi
D
Y
˛2RC
˝
w1  ˛; H
˛
D .1/j Y
˛2RC
h˛; Hi ;
where j is the number of negative roots in the collection fw1  ˛g˛2RC:
Suppose ﬁrst that w D w1 D s˛; where ˛ is a positive simple root. According
to Proposition 8.30, s˛ permutes the positive roots different from ˛; whereas
s˛  ˛ D ˛. Thus, j D 1 in this case, and so
P.w  H/ D P.H/ D det.w/P.H/;
since the determinant of a reﬂection is 1: By Proposition 8.24, every element w of
W is a product of reﬂections associated to positive simple roots, and so
P.w  H/ D P.s˛j1    s˛jN  H/
D .1/N P.H/
D det.s˛j1    s˛jN /P.H/;
showing that P is alternating.

284
10
Further Properties of the Representations
Suppose now that f is any Weyl-alternating polynomial. Then for any positive
root ˛; if h˛; Hi D 0; we will have
f .H/ D f .s˛  H/ D f .H/;
since det.s˛/ D 1: Thus, f must vanish on the hyperplane orthogonal to ˛; which
we denote as V˛: It is then not hard to show (Exercise 4) that f is divisible in the
space of polynomials by the linear function h˛; Hi ; that is, f .H/ D h˛; Hi f 0.H/
for some polynomial f 0: Now, if ˇ is any positive root different from ˛; the
polynomial f 0 must vanish at least on the portion of Vˇ not contained in V˛: But
since ˇ is not a multiple of ˛; Vˇ is distinct from V˛; so that Vˇ \ V˛ is a subspace
of dimension r  2: Thus, Vˇ  .V˛ \ Vˇ/ is dense in Vˇ: Since f 0 is continuous, it
must actually vanish on all of Vˇ:
It follows that f 0 is divisible in the space of polynomials by hˇ; Hi ; so that
f .H/ D h˛; Hi hˇ; Hi f 00.H/
for some polynomial f 00: Proceeding on in the same way, we see that f contains a
factor of h˛; Hi for each positive root ˛; meaning that
f .H/ D
0
@ Y
˛2RC
h˛; Hi
1
A g.H/
D P.H/g.H/
for some polynomial g; as claimed.
ut
Recall the notion of directional derivative DX, deﬁned in (10.14).
Lemma 10.22. Let A denote the differential operator
A D
Y
˛2RC
D˛:
For any  2 h; let f W h ! C be the function given by
f.H/ D
X
w2W
det.w/ehw;Hi:
Then f is Weyl alternating and is given by a convergent power series of the form
f.H/ D cP.H/ C terms of degree at least k C 1
(10.18)

10.5
The Weyl Dimension Formula
285
for some constant c: Furthermore, .AP /.0/ ¤ 0 and the constant c may be
computed as
c D .Af/.0/
.AP /.0/ ;
where
.Af/.0/ D jW j P./:
Proof. The proof that f is Weyl alternating is elementary. Since f is a sum of
exponentials, it is a real-analytic function, meaning that it can be expanded in a
convergent power series in the coordinates x1; : : : ; xr associated to any basis for
h: In the power-series expansion of f; we collect together all the terms that are
homogeneous of degree l: Thus, f is expressible as the sum of homogeneous
polynomials q;l of degree l: Since f is Weyl-alternating, it is not hard to show
(Exercise 5) that each of the polynomials q;l is also Weyl-alternating. Thus, by
Proposition 10.21, all the polynomials q;l with l < k must zero, and the polynomial
q;k must be a constant multiple of P.H/: This establishes the claimed form of the
series for f:
On the one hand, applying A to a homogeneous term of degree l > k gives a
homogeneous term of degree l  k > 0; which will evaluate to zero at the origin.
Thus,
.Af/.0/ D c.AP /.0/:
On the other hand, by directly differentiating the exponentials in the deﬁnition of
f, we get
.Af/.0/ D
X
w2W
det.w/
Y
˛2RC
hw  ; ˛i
D
X
w2W
det.w/P.w  /
D jW j P./:
This shows that
c.AP /.0/ D jW j P./:
(10.19)
Now, if  is strictly dominant, each factor in the formula for P./ is nonzero, so
that P./ ¤ 0: Applying (10.19) in such a case shows that .AP /.0/ ¤ 0: We can
thus solve (10.19) for c to obtain the claimed formula.
ut

286
10
Further Properties of the Representations
Proof of the Weyl Dimension Formula. As we have noted already, P. C ı/ and
P.ı/ are nonzero, so that cCı and cı are also nonzero. For any H 2 h; we have
.tH/ D fCı.tH/
fı.tH/
D cCıtkP.H/ C O.tkC1/
cıtkP.H/ C O.tkC1/
D cCıP.H/ C O.t/
cıP.H/ C O.t/
(10.20)
for any t for which the numerator of the last expression is nonzero.
Now, we know from the deﬁnition of a character, that  is a continuous function.
To determine the value of  at the identity, we choose H to be in the open
fundamental Weyl chamber, so that P.H/ ¤ 0; in which case the denominator
in (10.20) is nonzero for all sufﬁciently small nonzero t: Thus,
dim.V/ D lim
t!0 .tH/
D cCı
cı
D P. C ı/
P.ı/
;
where we have used the formula for c in Lemma 10.22. Recalling the deﬁnition of
P gives the dimension formula as stated in Theorem 10.18.
ut
Example 10.23. If 1 and 2 denote the two fundamental weights for sl.3I C/ then
the dimension of the representation with highest weight m11 C m22 is given by
1
2.m1 C 1/.m2 C 1/.m1 C m2 C 2/:
See Exercises 7 and 8 for the analogous formulas for B2 and G2:
Proof. Note that scaling the inner product on h by a constant does not affect the
right-hand side of the dimension formula, since the inner product occurs an equal
number of times in the numerator and denominator. Let us then normalize the inner
product so that all roots ˛ satisfy h˛; ˛i D 2: With this normalization, H˛ D ˛ and
we have
m1 D h; H1i D h˛1; i
m2 D h; H2i D h˛2; i :

10.6
The Kostant Multiplicity Formula
287
Letting ˛3 D ˛1 C ˛2; we have ı D 1
2.˛1 C ˛2 C ˛3/ D ˛3: We then note that
h˛1; ıi D 1; h˛2; ıi D 1; and h˛3; ıi D 2: Thus, the numerator in the dimension
formula is
.h˛1; i C h˛1; ıi/ .h˛2; i C h˛2; ıi/ .h˛3; i C h˛3; ıi/
D .m1 C 1/.m2 C 1/.m1 C m2 C 2/
and the denominator is .1/.1/.2/:
ut
10.6
The Kostant Multiplicity Formula
We will obtain Kostant's multiplicity formula from the Weyl character formula by
developing a method for dividing by the Weyl denominator q: We now illustrate this
method in the case of sl.2I C/; where the character formula takes the form
m.aH/ D e.mC1/a  e.mC1/a
ea  ea
:
(10.21)
One way to divide the numerator of (10.21) by the denominator is to use the
geometric series 1=.1  x/ D 1 C x C x2 C    : Applying this formally with
x D e2a gives the following result:
1
ea  ea D
1
ea.1  e2a/ D ea.1 C e2a C e4a C    /:
(10.22)
If the real part of a is negative, the series will not converge in the ordinary
sense. Nevertheless, if we treat the right-hand side (10.22) as simply a formal series,
then (10.22) holds in the sense that if we multiply the right-hand side by ea  ea;
we get 1. Using (10.22), we have
e.mC1/a  e.mC1/a
ea  ea
D eae.mC1/a.1 C e2a C    /  eae.mC1/a.1 C e2a C    /
D .ema C e.m2/a C    /  .e.mC2/a C e.mC4/a C    /
D ema C e.m2/a C    C ema:
This last expression is, indeed, the character of Vm: From this formula, we can read
off that each weight of Vm has multiplicity 1 (Proposition 10.12).
We now develop a similar method for dividing by the Weyl denominator in the
general case.

288
10
Further Properties of the Representations
Deﬁnition 10.24. A formal exponential series is a formal series of the form
X

ceh;Hi;
where each  is an integral element and where the c's are complex numbers.
A ﬁnite exponential series is a series of the same form where all but ﬁnitely many
of the c's are equal to zero.
Since we make no restrictions on the coefﬁcients c; a formal exponential series
may not converge. Thus, the series should properly be thought of not as a function
of H but simply as a list of coefﬁcients c: If f is a formal exponential series with
coefﬁcients c and g is a ﬁnite exponential series with coefﬁcients d, the product
of f and g is a well-deﬁned formal exponential series with coefﬁcients e given by
e D
X
0
c0d0:
(10.23)
Note that only ﬁnitely many of the terms in (10.23) are nonzero, since g is a ﬁnite
exponential series. (The product of two formal exponential series is, in general, not
deﬁned, because the sum deﬁning the coefﬁcients in the product might be divergent.)
Deﬁnition 10.25. If  is an integral element, we let p./ denote the number of
ways (possibly zero) that  can be expressed as a non-negative integer combination
of positive roots. The function p is known as the Kostant partition function.
More explicitly, if the positive roots are ˛1; : : : ; ˛k; then p./ is the number of
k-tuples of non-negative integers .n1; : : : ; nk/ such that n1˛1 C    C nk˛k D :
Example 10.26. If ˛1 and ˛2 are the two positive simple roots for sl.3I C/; then for
any two non-negative integers m1 and m2; we have
p.m1˛1 C m2˛2/ D 1 C min.m1; m2/:
If  is not a non-negative integer combination of ˛1 and ˛2; then p./ D 0:
The result of Example 10.26 is shown graphically in Figure 10.6.
Proof. We have the two positive simple roots ˛1 and ˛2; together with one other
positive root ˛3 D ˛1 C ˛2: Thus, if  can be expressed as a non-negative integer
combination of ˛1 C ˛2 C ˛3, we can rewrite ˛3 as ˛1 C ˛2 to express  as  D
m1˛1 C m2˛2; with m1; m2  0: Every expression for  is then of the form
 D .m1  k/˛1 C .m2  k/˛2 C k˛3;
for 0  k  min.m1; m2/:
ut
We are now ready to explain how to invert the Weyl denominator.

10.6
The Kostant Multiplicity Formula
289
1
1
1
1
1
1
1
1
2
2
2
2
2
2
3
3
3
3
3
3
4
Fig. 10.6 The Kostant partition function p for A2: Unlabeled points  have p./ D 0
Proposition 10.27 (Formal Reciprocal of the Weyl Denominator). At the level
of formal exponential series, we have
1
q.H/ D
X
0
p./ehCı;Hi:
(10.24)
Here the sum is nominally over all integral elements  with   0; but p./ D 0
unless  is an integer combination of roots.
The proposition means, more precisely, that the product of q.H/ and the formal
exponential series on the right-hand side of (10.24) is equal to 1 (i.e., to e0). To
prove Proposition 10.24, we ﬁrst rewrite q as a product.
Lemma 10.28. The Weyl denominator may be computed as
q.H/ D
Y
˛2RC
.eh˛;Hi=2  eh˛;Hi=2/:
(10.25)
See Exercise 9 for the explicit form of this identity in the case g D sl.n C 1I C/:
Proof. Let Qq denote the product on the right-hand side of (10.25). If we expand out
the product in the deﬁnition of Qq; there will be a term equal to
Y
˛2RC
eh˛;Hi=2 D ehı;Hi;

290
10
Further Properties of the Representations
where ı is half the sum of the positive roots. Note that even though ˛=2 is not
necessarily integral, the element ı is integral (Proposition 8.38). We now claim that
every other exponential in the expansion will be of the form ˙eh;Hi with  integral
and strictly lower than ı: To see this, note that every time we take eh˛;Hi=2 instead
of eh˛;Hi=2; we lower the exponent by ˛: Thus, any  appearing will be of the form
 D ı 
X
˛2E
˛;
for some subset E of RC: Such a  is integral and lower than ı:
Meanwhile, by precisely the same argument as in the proof of Point 1 of
Proposition 10.21, the function Qq is alternating with respect to the action of W:
Thus, if we write
Qq.H/ D
X

aeh;Hi;
(10.26)
then by Corollary 10.17, the coefﬁcients a must satisfy
aw D det.w/a:
(10.27)
Since the exponential ehı;Hi occurs in the expansion (10.26) with a coefﬁcient of
1, ehwı;Hi must occur with a coefﬁcient of det.w1/ D det.w/: Thus, it remains only
to show that no other exponentials can occur in (10.26). To see this, note that if any
exponential eh;Hi occurs for which  is not in the W -orbit of ı; then by (10.27),
another exponential eh0;Hi must appear with 0 dominant but strictly lower than
ı: Since ı is the minimal strictly dominant integral element, 0 cannot be strictly
dominant (see Proposition 8.43) and thus must be orthogonal to one of the positive
simple roots ˛j : Thus, s˛j  0 D 0; where det.s˛j / D 1: Applying (10.27) to this
case shows that the coefﬁcient of eh0;Hi must be zero.
ut
Figure 10.7 illustrates the proof of Lemma 10.28 for the root system G2. The
white dots indicate weights of exponentials that do not, in fact, occur in the
expansion of Qq: Each of these white dots lies on the line orthogonal to some root,
which means that the corresponding exponential cannot occur in the expansion of a
Weyl-alternating function.
Proof of Proposition 10.27. As in the sl.2I C/ example considered at the beginning
of this section, we have
1
eh˛;Hi=2  eh˛;Hi=2 D eh˛;Hi=2.1 C eh˛;Hi C e2h˛;Hi C    /;
at the level of formal exponential series. Taking a product over ˛ 2 RC gives
1
q.H/ D ehı;Hi Y
˛2RC
.1 C eh˛;Hi C e2h˛;Hi C    /:

10.6
The Kostant Multiplicity Formula
291
2
1
Fig. 10.7 The black dots indicate the W -orbit of ı for G2: The white dots indicate weights of
exponentials that do not occur in the expansion of Qq
In the product, a term of the form eh;Hi will occur precisely as many times there
are ways to express  as a non-negative integer combination of the ˛'s, namely,
p./ times.
ut
Theorem 10.29 (Kostant's Multiplicty Formula). Suppose  is a dominant inte-
gral element and V is the ﬁnite-dimensional irreducible representation with highest
weight : Then if  is a weight of V; the multiplicity of  is given by
mult./ D
X
w2W
sign.w/p.w  . C ı/  . C ı//:
Proof. By the Weyl character formula and Proposition 10.27, we have
.H/ D
0
@X
0
p./ehCı;Hi
1
A
 X
w2W
det.w/ehw.Cı/;Hi
!
:
For a ﬁxed weight ; the coefﬁcient of eh;Hi in the character  is just the
multiplicity of  in V (Proposition 10.12). This coefﬁcient is the sum of the
quantity
p./ det.w/
(10.28)

292
10
Further Properties of the Representations
over all pairs .; w/ for which
  ı C w  . C ı/ D ;
or
 D w  . C ı/  . C ı/:
(10.29)
Substituting (10.29) into (10.28) and summing over W gives Kostant's formula. ut
If  is well in the interior of the fundamental Weyl chamber and  is very close to
; then for all nontrivial elements of W; w  . C ı/ will fail to be higher than  C ı:
In those cases, there is only one nonzero term in the formula and the multiplicity of
 will be simply p./: (By Exercise 7 in Chapter 9, p./ is the multiplicity
of  in the Verma module W:) In general, it sufﬁces to compute the multiplicities
of dominant weights : For any dominant ; there will be many elements w of W
for which w  . C ı/ will fail to be higher than  C ı: Nevertheless, in high-rank
examples, the order of the Weyl group is very large and the number of nonzero terms
in Kostant's formula can be large, even if it is not as large as the order of W:
Figure 10.8 carries out the multiplicity calculation for sl.3I C/ in the irreducible
representation with highest weight .2; 9/: (These multiplicities were presented
without proof in Figure 6.5.) The calculation is done only for the weights in
the fundamental Weyl chamber; all other multiplicities are determined by Weyl
invariance. The term involving w 2 W makes a nonzero contribution to the
multiplicity of  only if  C ı is lower than w  . C ı/; or equivalently if  is
lower than w  . C ı/  ı: For most weights  in the fundamental chamber, only
the w D 1 term makes a nonzero contribution. In those cases, the multiplicity of 
is simply p.  /.
Now, by Example 10.26 and Figure 10.6, p.  / increases by 1 each time 
moves from one "ring" of weights to the ring immediately inside. For the weights
indicated by white dots, however, there are two nonzero terms, the second being the
one in which w is the reﬂection about the vertical root ˛1: Since the determinant of
the reﬂection is 1; the second term enters with a minus sign. On the medium-sized
triangle, the ﬁrst term is 4 and the second term is 1; while on the small triangle,
the two terms are 5 and 2. Thus, in the end, all of the weights in all three of the
triangles end up with a multiplicity of 3.
It is not hard to see that the pattern in Figure 10.8 holds in general for
representations of sl.3I C/: As we move inward from one "ring" of weights to the
next, the multiplicities increase by 1 at each step, until the rings become triangles, at
which point the multiplicities become constant. (There is an increase in multiplicity
from the last hexagon to the ﬁrst triangle, but not from the ﬁrst triangle to any
of the subsequent triangles.) In particular, if the highest weight is on the edge of
the fundamental Weyl chamber, all of the rings will be triangles and, thus, all of
the weights have multiplicity 1. A small piece of this pattern of multiplicities was
determined in a more elementary way in Exercises 11 and 12 in Chapter 6.

10.6
The Kostant Multiplicity Formula
293
1
1
1
1
1
1
2
2
2
2
2
3
3
3
3
4 1
4 1
4 1
5 2
w
w
Fig. 10.8 Multiplicity calculation for the representation with highest weight .2; 9/
For other low-rank Lie algebras, the pattern of multiplicities is more complicated,
but can, in principle, be computed explicitly. In particular, the Kostant partition
function can computed explicitly for B2 and G2; at which point one only needs
to work out which terms in the multiplicity formula contribute, for weights in the
fundamental chamber. (See, for example, [Tar] or [Cap] and compare [CT], which
gives an elegant graphical method of computing the multiplicities in the B2 Š C2
case.) For Lie algebras of rank higher than two, [BBCV] gives efﬁcient algorithms
for computing the partition function for the classical Lie algebras, either numerically
or symbolically. If the order of the Weyl group is not too large, one can then
implement Kostant's formula to read off the multiplicities.
For higher-rank cases, the order of the Weyl group can be very large, in which
case it is not feasible to use Kostant's formula, even if the partition function is
known. Freudenthal's formula (Sect. 22.3 in [Hum]) gives an alternative method
of computing the multiplicities, which is preferable in these cases, because it does
not involve a sum over the Weyl group.

294
10
Further Properties of the Representations
10.7
The Character Formula for Verma Modules
Before coming to the proof of the Weyl character formula, we consider a "warm up
case," that of the character of a Verma module. In the character formula for Verma
modules, there is an even larger cancellation than in the Weyl character formula. The
product of the Weyl denominator and the character would appear to be an inﬁnite
sum of exponentials, and yet all but one of these exponentials cancels out! The proof
of this character formula follows a very natural course, consisting of writing down
explicitly the multiplicities for the various weight spaces and then using the formula
for the multiplicities to establish the desired cancellation. The character formula for
Verma modules is a key ingredient in the proof of the character formula for ﬁnite-
dimensional representations.
Since a Verma module .; V/ is inﬁnite dimensional, the operator e.X/ may
not have a well-deﬁned trace. On the other hand, Point 2 of Proposition 10.12 gives
an expression for the character of a ﬁnite-dimensional representation, evaluated at
a point in h; in terms of the weights for the representation. This observation allows
us to deﬁne the character of any representation that decomposes as a direct sum of
weight spaces, as a formal exponential series on h: (Recall Deﬁnition 10.24.)
Deﬁnition 10.30. Let V be any representation (possibly inﬁnite dimensional) that
decomposes as a direct sum of integral weight spaces of ﬁnite multiplicity. We deﬁne
the formal character of V by the formula
QV .H/ D
X

mult./eh;Hi;
H 2 h;
where the QV is interpreted as a formal exponential series.
Proposition 10.31 (Character Formula for Verma Modules). For any integral
element ; the formal character of the Verma module is given by
QW.H/ D
X
0
p./eh;Hi;
(10.30)
where p is the Kostant partition function in Deﬁnition 10.25. This formal character
may also be expressed as
QW.H/ D ehCı;Hi

1
q.H/

(10.31)
where 1=q.H/ is the formal reciprocal of the Weyl denominator given in
Proposition 10.27.
The formula (10.31) is similar to the Weyl character formula, except that in
the numerator we have only a single exponential rather than an alternating sum
of exponentials. Of course, (10.31) implies that
q.H/QW.H/ D ehCı;Hi:
(10.32)

10.8
Proof of the Character Formula
295
This result means that when we multiply the Weyl denominator (an alternating ﬁnite
sum of exponentials) and the formal character QW (an inﬁnite sum of exponentials),
the result is just a single exponential, with all the other terms cancelling out. As
usual, it is easy to see this cancellation explicitly in the case of sl.nI C/: There, if
m is any integer (possibly negative) and H is the usual diagonal basis element, we
have
QWm.aH/ D ema C e.m2/a C e.m4/a C    :
Since q.aH/ D ea C ea; we obtain
q.aH/QWm.aH/ D e.mC1/a C e.m1/a C e.m3/a C   
 e.m1/a  e.m3/a    
D e.mC1/a:
Proof. Enumerate the positive roots as ˛1; : : : ; ˛k and let fYjg be nonzero elements
of g˛j ; j D 1; : : : ; k: By Theorem 9.13, the elements of the form
.Y1/n1    .Yk/nkv0
(10.33)
form a basis for the Verma module. An element of the form (10.33) is a weight
vector with weight
 D   n1˛1      nk˛k:
The number of times the weight  will occur is the number of ways that    can
be written as a non-negative integer combinations of the positive roots. Thus, we
obtain the ﬁrst expression for QW: The second expression the follows easily from
the ﬁrst expression and the formula (Proposition 10.27) for the formal inverse of the
Weyl denominator. [Compare (10.30)-(10.24).]
ut
10.8
Proof of the Character Formula
Our strategy in proving the character formula is as follows. We will show ﬁrst
that the (formal) character of a Verma module W can be expressed as a ﬁnite
linear combination of characters of irreducible highest weight cyclic representations
V: (The V's may be inﬁnite dimensional.) By using the action of the Casimir
element, we will see that only the only 's appearing in this decomposition are
those satisfying j C j2 D j C j2 : We will then invert this relationship and
express the character of an irreducible representation V as a linear combination of
characters of Verma modules W; with  again satisfying j C j2 D j C j2 : We

296
10
Further Properties of the Representations
will then specialize to the case in which  is dominant integral, where V is ﬁnite
dimensional and its character  is a ﬁnite sum of exponentials. By the character
formula for Verma modules from Sect. 10.7, we obtain the following conclusion:
The product q.H/.H/ is a ﬁnite linear combination of exponentials eih;Hi;
where each  D  C  satisﬁes jj2 D j C j2 : From this point, it is a short
step to show that only 's of the form  D w  . C / occur and then to prove the
character formula.
We know from general principles that the product q.H/.H/ is a ﬁnite sum of
exponentials. We need to show that the only exponential that actually occur in this
product (with a nonzero coefﬁcient) are those of the form ehw.Cı/;Hi; and that such
exponentials occur with a coefﬁcient of det.w/: We begin with a simple observation
that limits which exponentials could possibly occur in the product.
Proposition 10.32. If an exponential eh;Hi occurs with nonzero coefﬁcient in the
product q.H/.H/; then  must be in the convex hull of the W -orbit of  C ı and
 must differ from  C ı by an element of the root lattice.
In the last image in Figure 10.4, the white and black circles indicate weights
consistent with Proposition 10.32. Only a small fraction of these weights (the black
circles) actually occur in q.H/.H/:
Proof. If  is an integral element, then for each root ˛;
s˛   D   2h; ˛i
h˛; ˛i˛
will differ from  by an integer multiple of ˛: Since roots are integral, we conclude
that s˛ is, again, integral. It follows that for any w 2 W; the element w is integral
and differs from  by an integer combination of roots. Thus, by Proposition 8.38,
w  ı is integral and differs from ı by an integer combination of roots. Similarly,
each weight of  is integral and differs from  by an integer combination of
roots (Theorem 10.1). Thus, for each exponential eh;Hi occurring in the product
q.H/.H/; the element  will be integral and will differ from ı C by an integer
combination of roots.
Meanwhile, since each exponential in q is in the Weyl-orbit of ı and each
exponential in  is in the convex hull of the Weyl-orbit of  (Theorem 10.1), each
exponential in the product will be in the convex hull of the Weyl-orbit of  C ı; as
claimed.
ut
Our next result is the key to the proof of the character formula. In fact, we will
see that it, in conjunction with Proposition 10.32, limits the exponentials that can
occur in q.H/.H/ to only those whose weights are in the Weyl orbit of  C ı:
Proposition 10.33. If an exponential eh;Hi occurs with nonzero coefﬁcient in the
product q.H/.h/; then  must satisfy
h; i D h C ı;  C ıi :
(10.34)

10.8
Proof of the Character Formula
297
Fig. 10.9 The weights indicated by white dots satisfy the condition in Proposition 10.33 but not
the condition in Proposition 10.32. The weights indicated by black dots satisfy both conditions
In the last image in Figure 10.4, for example, the exponentials  represented
by the white dots do not satisfy (10.34) and, thus, cannot occur in the product
q.H/.h/: Much of the rest of this section will be occupied with the proof of
Proposition 10.33. Before turning to this task, however, let us see how Proposi-
tions 10.32 and 10.33 together imply the character formula. The claim is that the
only weights satisfying the conditions in both of these propositions are the ones in
the W -orbit of  C ı: See Figure 10.9.
Proof of Character Formula Assuming Proposition 10.33. Suppose v1; : : : ; vm are
distinct elements of a real inner product space, all of which have the same norm, S.
The convex hull of these elements is the space of vectors of the form
m
X
j D1
aj vj
with 0  aj  1 and P
j aj D 1: It is then not hard to show by induction
on m that the only way such a convex combination can have norm S is if one
of the aj 's is equal to 1 and all the others are zero. Applying this with the vj's
equal to w  . C ı/; w 2 W; shows that the only exponentials eh;Hi satisfying
both Propositions 10.32 and 10.33 are those of the form  D w  . C ı/:
Thus, the only exponentials appearing in the product q are those of the form
cehw.Cı/;Hi; w 2 W:
Now, the exponential ehCı;Hi occurs in the product exactly once, since it
corresponds to taking the highest weight from q (i.e., ı) and the highest weight

298
10
Further Properties of the Representations
from  (i.e., ) and the coefﬁcient of ehCı;Hi is 1. (Note that since ı is strictly
dominant, wı is strictly lower than ı for all nontrivial w 2 W; by Proposition 8.42.)
Since the character is Weyl invariant and the Weyl denominator is Weyl alternating,
their product is Weyl alternating. Thus, by Corollary 10.17, the coefﬁcients in the
expansion of q are alternating; that is, the coefﬁcient of ehw.Cı/;Hi must equal
det.w/.
ut
We now begin the process of proving Proposition 10.33. A crucial role in the
proof is played by the Casimir element, which we have already introduced in
Sect. 10.2. (See Deﬁnition 10.4.) We now extend the result of Proposition 10.6
to highest-weight cyclic representations that may not be irreducible or ﬁnite
dimensional.
Proposition 10.34. Let .; V / be a highest-weight cyclic representation of g
(possibly inﬁnite dimensional) with highest weight  and let Q be the extension
of  to U.g/: Then Q.C/ D cI; where
c D h C ı;  C ıi  hı; ıi :
Proposition 10.34 applies, in particular, to the Verma module W and to the
irreducible representation V: A key consequence of the proposition is that if
two highest weight cyclic representations, with highest weights 1 and 2; have
the same eigenvalue of the Casimir, then h1 C ı; 1 C ıi must coincide with
h2 C ı; 2 C ıi :
Proof. The same argument as in the proof of Proposition 10.6 shows if v0 is the
highest weight vector for V; then Q.C/v0 D cv0: Now let U be the space of all
v 2 V for which Q.C/v D cv: Since C is in the center of U.g/ (Proposition 10.5),
we see that if v 2 U , then
Q.C/..X/v/ D .X/ Q.C/v D c.X/v;
showing that .X/v is again in U: Thus, U is an invariant subspace of V containing
v0; which means that U D V; that is, that Q.C/v D cv for all v 2 V:
ut
Deﬁnition 10.35. Let ƒ denote the set of integral elements. If  is a dominant
integral element, deﬁne a set S  ƒ as follows:
S D f 2 ƒj h C ı;  C ıi D h C ı;  C ıig :
Note that S is the intersection of the integral lattice ƒ with sphere of radius
k C ık centered at ı; see Figure 10.10. Since there are only ﬁnitely many
elements of ƒ in any bounded region, the set S is ﬁnite, for any ﬁxed : Our
strategy for the proof of Proposition 10.33 is as discussed at the beginning of this
section. We will decompose the formal character of each Verma module W with
 2 S as a ﬁnite sum of formal characters of irreducible representations V;
with each  also belonging to S: This expansion turns out to be of an "upper

10.8
Proof of the Character Formula
299
1
2
Fig. 10.10 The set S (black dots) consists of integral elements  for which k C ık D k C ık
triangular with ones on the diagonal" form, allowing us to invert the expansion
to express the formal character of irreducible representations in terms of formal
characters of Verma modules. In particular, the character of the ﬁnite-dimensional
representation V will be expressed as a linear combination of formal characters of
Verma modules W with  2 S: When we multiply both sides of this formula
by the Weyl denominator and use the character formula for the Verma module
(Proposition 10.31), we obtain the claimed form for q:
Of course, a key point in the argument it verify that whenever the character
of an irreducible representation V appears in the expansion of the character
of W;  2 S; the highest weight  is also in S: This claim holds because
any subrepresentation V occurring in the decomposition of W must have the
same eigenvalue of the Casimir as W: In light of Proposition 10.34, this means
that h C ı;  C ıi must equal h C ı;  C ıi ; which is assumed to be equal to
h C ı;  C ıi :
Proposition 10.36. For each  in S; the formal character of the Verma module
W can be expressed as a linear combination of formal characters of irreducible
representations V with  in S and   :
QW D
X
2S

a
QV
(10.35)
Furthermore, the coefﬁcient a
 of QV in this decomposition is equal to 1.
See Figure 10.11.

300
10
Further Properties of the Representations
Fig. 10.11 For  2 S; the character of W is a linear combination of characters of irreducible
representations V with highest weights    in S
Lemma 10.37. Let .; V / be a representation of g; possibly inﬁnite dimensional,
that decomposes as direct sum of weight spaces of ﬁnite multiplicity, and let U be a
nonzero invariant subspace of V: Then both U and the quotient representation V=U
decompose as a direct sum of weight spaces. Furthermore, the multiplicity of any
weight in V is the sum of its multiplicity in U and its multiplicity in V=U:
Proof. Let u be an element of U: By assumption, we can decompose u as u D
v1 C  Cvj ; where the vk's belong to weight spaces in V corresponding to distinct
weights 1; : : : ; j : We wish to show that each vk actually belongs to U: If j D 1;
there is nothing to prove. If j > 1; then j ¤ 1; which means that there is
some H 2 h for which
˝
j ; H
˛
¤ h1; Hi : Then apply to u the operator .H/ 
h1; Hi I
..H/  h1; Hi I/u D
j
X
kD1
.hk; Hi  h1; Hi/vk:
(10.36)
Since the coefﬁcient of v1 is zero, the vector in (10.36) is the sum of fewer than j
weight vectors. Thus, by induction on j; we can assume that each term on the right-
hand side of (10.36) belongs to U: In particular, a nonzero multiple of vj belongs
to U; which means vj itself belongs to U: Now, if vj is in U; then u  vj D v1 C
  Cvj 1 is also in U: Thus, using induction again, we see that each of v1; : : : ; vj 1
belongs to U:
We conclude that the sum of the weight spaces in U is all of U: Since weight
vectors with distinct weights are linearly independent (Proposition A.17), the sum

10.8
Proof of the Character Formula
301
must be direct. We turn, then, to the quotient space V=U: It is evident that the images
of the weight spaces in V are weight spaces in V=U with the same weight. Thus,
the sum of the weight spaces in V=U is all of V=U and, again, the sum must be
direct.
Finally, consider a ﬁxed weight  occurring in V; and let V be the associated
weight space. Let q be the restriction to V of the quotient map q W V ! V=U: The
kernel of q consists precisely of the weight vectors with weight  in U: Thus, the
dimension of the image of q; which is the weight space in V=U with weight ; is
equal to dim V  dim.V \ U /: The claim about multiplicities in V; U; and V=U
follows.
ut
Proof of Proposition 10.36. We actually prove a stronger result, that the formal
character of any highest-weight cyclic representation U with  2 S can be decom-
posed as in (10.35). As the proof of Proposition 6.11, any such U decomposes as a
direct sum of weight spaces with weights lower than  and with the multiplicity of
the weight  being 1. For any such U; let
M D
X
2S
mult./:
Our proof will be by induction on M:
We ﬁrst argue that if M D 1; then U must be irreducible. If not, U would
have a nontrivial invariant subspace X; and this subspace would, by Lemma 10.37,
decompose as a direct sum of weight spaces, all of which are lower than : Thus, X
would have to contain a weight vector w that is annihilated by each raising operator
.X˛/; ˛ 2 RC: Thus, X would contain a highest weight cyclic subspace X0 with
some highest weight : By Proposition 10.34, the Casimir would act as the scalar
h C ı;  C ıi  hı; ıi in X0: On the other hand, since X0 is contained in U; the
Casimir has to act as h C ı;  C ıi  hı; ıi in X0; which, since  2 S; is equal
to h C ı;  C ıi  hı; ıi : Thus,  must belong to S: Meanwhile,  cannot equal
 or else X0 would be all of U: Thus, both  and  would have to have positive
multiplicities and M would have to be at least 2.
Thus, when M
D 1; the representation U is irreducible, in which case,
Proposition 10.36 holds trivially. Assume now that the proposition holds for highest
weight cyclic representations with M  M0; and consider a representation U with
M D M0C1: If U is irreducible, there is nothing to prove. If not, then as we argued
in the previous paragraph, U must contain a nontrivial invariant subspace X0 that is
highest weight cyclic with some highest weight  that belongs to S and is strictly
lower than : We can then form the quotient vector space U=X0; which will still be
highest weight cyclic with highest weight : By Lemma 10.37, the multiplicity of 
in U is the sum of the multiplicities of  in X0 and in U=X0: Thus,
QU D QX0 C QU=X0:

302
10
Further Properties of the Representations
Now, both X0 and U=X0 contain at least one weight in S with nonzero
multiplicity, namely  for X0 and  for U=X0: Thus, both of these spaces must have
M  M0 and we may assume, by induction, that their formal characters decompose
as a sum of characters of irreducible representations with highest weights in S:
These highest weights will be lower than , in fact lower than  in the case of X0.
Thus, the character of V will not occur in the expansion of QX0 . But since U=X0
still has highest weight ; we may assume by induction that the character of V
will occur exactly once in the expansion of QU=X0; and, thus, exactly once in the
expansion of QU:
ut
Proposition 10.38. If we enumerate the elements of S in nondecreasing order as
S D f1; : : : ; lg; then the matrix Ajk WD a
j
k is upper triangular with ones on the
diagonal. Thus, A is invertible, and the inverse of A is also upper triangular with
ones on the diagonal. It follows that we can invert the decomposition in (10.35) to
a decomposition of the form
QV D
X
2S
b
QW ;
(10.37)
where b
 D 1:
It is easy to check (using, say, the formula for the inverse of a matrix in terms of
cofactors) that the inverse of an upper triangular matrix with ones on the diagonal is
again upper triangular with ones on the diagonal.
Proof. For any ﬁnite partially ordered set, it is possible (Exercise 6) to enumerate
the elements in nondecreasing order. In our case, this means that we can enumerate
the elements of S as 1; : : : ; l in such a way that if j  k then j  k: If we
expand QWk in terms of QVj as in Proposition 10.38, the only nonzero coefﬁcients
are those with j  k; which means that j  k: Thus, the matrix is upper
triangular. Since, also, the coefﬁcient of QVk in the expansion of QWk is 1, the
expansion has ones on the diagonal.
ut
Proof of Proposition 10.33. We apply (10.37) with  D ; so that QV is the char-
acter of .H/ of the ﬁnite-dimensional, irreducible representation with highest
weight : We then multiply both sides of (10.37) by the Weyl denominator q: Using
the character formula for Verma modules [Proposition 10.31 and Eq.(10.32)], we
obtain
q.H/.H/ D
X
2S
b
 ehCı;Hi:
(10.38)
Since each  belongs to S; each weight  WD  C ı occurring on the right-hand
side of (10.38) satisﬁes
h; i D h C ı;  C ıi D h C ı;  C ıi :

10.9
Exercises
303
Thus, we have expressed q.H/.H/ as a linear combination of exponentials with
weights  satisfying Proposition 10.33. Since any such decomposition is unique
by Proposition 10.16, it must be the one obtained by multiplying together the
exponentials in q and :
ut
10.9
Exercises
1. Suppose g is a complex Lie algebra that is reductive (Deﬁnition 7.1) but not
semisimple. Show that there exists a ﬁnite-dimensional representation of g that
is not completely reducible. (Compare Theorem 10.9 in the semisimple case.)
2. Suppose g is a complex semisimple Lie algebra with the property that every
ﬁnite-dimensional representation of g is completely reducible. Show that g
decomposes as a direct sum of simple algebras.
Hint: Consider the adjoint representation of g:
3. Using Proposition 10.16, show that the Weyl denominator is a Weyl-alternating
function.
4. Suppose that f
W h ! C is a polynomial and that f .H/ D 0 whenever
h˛; Hi D 0: Show that f is divisible in the space of polynomial functions
by h˛; Hi :
Hint: Choose coordinates z1; : : : ; zr on h for which h˛; Hi D z1:
5. Suppose f is an analytic function on h; meaning that f can be expressed in
coordinates in a globally convergent power series. Collect together all the terms
in this power series that are homogeneous of degree k; so that f is the sum of
homogeneous polynomials pk of degree k: Show that if f is alternating with
respect to the action of W; so is each of the polynomials pk:
Hint: Show that the composition of a homogeneous polynomial with a linear
transformation is again a homogeneous polynomial.
6. Show that any ﬁnite partially ordered set E can be enumerated in nondecreasing
order.
Hint: Every ﬁnite partially ordered set has a minimal element, that is, an element
x 2 E such that no y ¤ x in E is smaller than x:
7. Let f˛1; ˛2g be a base for the B2 root system, with ˛1 being the shorter root, as
in the left-hand side of Figure 8.7. Let 1 and 2 be the associated fundamental
weights (Deﬁnition 8.36), so that every dominant integral element is uniquely
expressible as
 D m11 C m22;
with m1 and m2 being non-negative integers. Show that the dimension of the
irreducible representation with highest weight  is
1
6.m1 C 1/.m2 C 1/.m1 C m2 C 2/.m1 C 2m2 C 3/:
Hint: Imitate the calculations in Example 10.23.

304
10
Further Properties of the Representations
8. Let the notation be as in Exercise 7, but with B2 replaced by G2: (See
Figures 8.6 and 8.11.) Show that the dimension of the irreducible representation
with highest weight  is
1
5Š.m1 C 1/.m2 C 1/.m1 C m2 C 2/
 .m1 C 2m2 C 3/.m1 C 3m2 C 4/.2m1 C 3m2 C 5/:
Show that the smallest nontrivial irreducible representation of G2 has dimen-
sion 7.
9. According to Lemma 10.28, we have the identity
X
w2W
det.w/eihwı;Hi D
Y
˛2RC
.eh˛;Hi=2  eh˛;Hi=2/:
(10.39)
Work out the explicit form of this identity for the case of the Lie algebra
sl.n C 1I C/; using the Cartan subalgebra h described in Sect. 7.7.1 and the
system of positive roots described in Sect. 8.10.1. If a typical element of h
has the form .a0; : : : ; an/; introduce the variables zj D eaj : Show that after
multiplying both sides by .z0    zn/n=2; the identity (10.39) takes the form of a
Vandermonde determinant:
det
0
BBB@
zn
0 zn1
0
   1
zn
1 zn1
1
   1
:::
:::
:::
zn
n zn1
n
   1
1
CCCA D
Y
j <k
.zj  zk/:
10. Let  be an irreducible, ﬁnite-dimensional representation of g with highest
weight ; and let  be the dual representation to :
(a) Show that the weights of  are the negative of the weights of :
(b) Let w0 be the unique element of W that maps the fundamental Weyl
chamber C to C: Show that the highest weight  of  may be
computed as
 D w0  :
(Compare Exercises 2 and 3 in Chapter 6.)
(c) Show that if I is an element of the Weyl group, then every representation
of g is isomorphic to its dual.

Part III
Compact Lie Groups

Chapter 11
Compact Lie Groups and Maximal Tori
In this chapter and Chapter 12 we develop the representation theory of a connected,
compact matrix Lie group K: The main result is a "theorem of the highest weight,"
which is very similar to our main results for semisimple Lie algebras. If we let k be
the Lie algebra of K and we let g be the complexiﬁcation of k; then g is reductive,
which means (Proposition 7.6) that g is the direct sum of a semisimple algebra
and a commutative algebra. We can, therefore, draw on our structure results for
semisimple Lie algebras to introduce the notions of roots, weights, and the Weyl
group. We will, however, give a completely different proof of the theorem of the
highest weight. In particular, our proof of the hard part of the theorem, the existence
of a irreducible representation for each weight of the appropriate sort, will be based
on decomposing the space of functions on K under the left and right action of
K: This argument is independent of the Lie-algebraic construction using Verma
modules.
In the present chapter, we develop the structures needed to formulate a theorem
of the highest weight for K; and we develop some key tools that will aid is in the
proof of the theorem. The representations themselves will appear in the next chapter.
The key results of this chapter are the torus theorem and the Weyl integral formula.
Although parts of the chapter assume familiarity with the theory of manifolds and
differential forms, the reader who is not familiar with that theory can still follow the
statements of the key results. Furthermore, the torus theorem can easily be proved
by hand in the case of SU.n/: The reader who is willing to take the results of this
chapter on faith can proceed on to Chapter 12, where they are applied to prove
the compact-group versions of the Weyl character formula and the theorem of the
highest weight. Finally, in Chapter 13, we will take a close look at the fundamental
group of K: We will prove, among other things, that when K is simply connected,
the notion of "dominant integral element" for K coincides with the analogous notion
for the Lie algebra g:
Throughout the chapter, we assume that K is a connected, compact matrix Lie
group with Lie algebra k: We allow k; and thus also g WD kC; to have a nontrivial
© Springer International Publishing Switzerland 2015
B. Hall, Lie Groups, Lie Algebras, and Representations, Graduate
Texts in Mathematics 222, DOI 10.1007/978-3-319-13467-3_11
307

308
11
Compact Lie Groups and Maximal Tori
center, which means that g is reductive but not necessarily semisimple. As in
Proposition 7.4, we ﬁx on g an inner product that is real on k and invariant under the
adjoint action of K.
11.1
Tori
In this section, we consider tori, that is, groups isomorphic to a product of copies of
S1: In the rest of the chapter, we will be interested in tori that arise as subgroups of
connected compact groups.
Deﬁnition 11.1. A matrix Lie group T is a torus if T is isomorphic to the direct
product of k copies of the group S1 Š U.1/; for some k:
Consider, for example, the group T of diagonal n  n matrices with determinant
1. Every element of T can be expressed uniquely as
diag.u1; : : : ; un1; .u1    un1/1/
for some complex numbers u1; : : : ; un1 with absolute value 1. Thus, T is isomor-
phic to n  1 copies of S1:
Theorem 11.2. Every compact, connected, commutative matrix Lie group is a
torus.
Recall that a subset E of a topological space is discrete if every element e of E
has a neighborhood containing no points of E other than e:
Lemma 11.3. Let V be a ﬁnite-dimensional inner product space over R; viewed as
a group under vector addition, and let  be a discrete subgroup of V: Then there
exist linearly independent vectors v1; : : : ; vk in V such that  is precisely the set of
vectors of the form
m1v1 C    C mkvk;
with each mj 2 Z:
Proof. Since  is discrete, there is some " > 0 such that the only point  in  with
kk < " is  D 0: For any ; 0 2 ; if k0  k < "; then since 0   is also in
; we must have 0 D : It then follows easily that there can be only ﬁnitely many
points in  in any bounded region of V: If  D f0g; the result holds with k D 0:
Otherwise, we can ﬁnd some nonzero 0 2  with such that k0k is minimal among
nonzero elements of : Let W denote the orthogonal complement of the span of 0;
let P denote the orthogonal projection of V onto W; and let 0 denote the image of
 under P: Since P is linear, 0 will be a subgroup of W: We now claim that 0 is
discrete in W:

11.1
Tori
309
0
n 0
Fig. 11.1 If  is very close to the line through 0; then k  n0k is smaller than k0k for some n
Suppose, toward a contradiction that 0 is not discrete. Then for every " > 0;
there must exist ı ¤ ı0 2 0 with kı0  ık < ": Thus, ı0  ı is a nonzero element
of 0 with norm less than ": There then exists some  2  with P./ D ı0  ı:
Since kı0  ık < "; the distance from  to the span of 0 is less than ": Now let ˇ
be the orthogonal projection of  onto the span of 0; which is the point closest to
 in line through 0: Then ˇ lies between m0 and .m C 1/0 for some integer m:
By taking n D m or n D m C 1; we can assume that the distance between n0 and
ˇ is at most half the length of 0: Meanwhile, the distance from ˇ to  is at most ":
Thus, the distance from n0 to  is at most " C k0k =2; which is less than k0k ; if
" is small enough. But then   n0 is a nonzero element of  with norm less than
the norm of 0, contradicting the minimality of 0: (See Figure 11.1.)
Now that 0 is known to be discrete, we can apply induction on the dimension
of V . Thus, there exist linearly independent vectors u1; : : : ; uk1 in 0 such that
0 is precisely the set of integer linear combinations of u1; : : : ; uk1: Let us then
choose vectors v1; : : : ; vk1 in  such that P.vj / D uj: Since P.v1/; : : : ; P.vk1/
are linearly independent in W , it is easy to see that v1; : : : ; vk1 and 0 are linearly
independent in V: For any  2 ; the element P./ is of the form m1u1 C    C
mk1uk1: Thus,  must be equal to m1v1 C    C mk1vk1 C 	; where 	 2 
satisﬁes P.	/ D 0; meaning that 	 is a multiple of 0: But then 	 must be an integer
multiple of 0, or else, by an argument similar to the one in the previous paragraph,
there would be an element of  in the span of 0 with norm less than k0k : We
conclude, then that
 D m1v1 C    C mk1vk1 C mk0;
establishing the desired form of :
ut
Proof of Theorem 11.2. Let T be compact, connected, and compact, and let t be
the Lie algebra of T: Then t is also commutative (Proposition 3.22), in which

310
11
Compact Lie Groups and Maximal Tori
case Corollary 3.47 tells us that the exponential map exp W t ! T is surjective.
Furthermore, the exponential map for T is a Lie group homomorphism and its
kernel  must be discrete, since the exponential map is injective in a neighborhood
of the origin. Thus, by Lemma 11.3,  is the set of integer linear combinations
of independent vectors v1; : : : ; vk: If dim t D n; then exp descends to a bijective
homomorphismof t= Š .S1/kRnk with T: Now, the Lie algebra map associated
to this homomorphism is invertible (since the Lie algebra of t is t), which means that
the inverse map is also continuous. Thus, T is homeomorphic to .S1/kRnk: Since
T is compact, this can only happen if k D n; in which case, T is the torus .S1/n: ut
At various points in the developments in later chapters, it will be useful to
consider elements t in a torus T for which the subgroup generated by t is dense
in T: The following result guarantees the existence of such elements.
Proposition 11.4. Let T D .S1/k and let t D .e21; : : : ; e2k/ be an element of
T: Then t generates a dense subgroup of T if and only if the numbers
1; 1; : : : ; k
are linearly independent over the ﬁeld Q of rational numbers.
The k D 1 case of this result is Exercise 9 in Chapter 1. In particular, if x is
any transcendental real number and we deﬁne j D xj ; j D 1; : : : ; k; then t will
generate a dense subgroup of T: See Figure 11.2 for an example of an element that
generates a dense subgroup of S1  S1:
Lemma 11.5. If T is a torus and t is an element of T; then the subgroup generated
by t is not dense in T if and only if there exists a nonconstant homomorphism ˆ W
T ! S1 such that ˆ.t/ D 1:
Proof. Suppose ﬁrst that there exists a nonconstant homomorphism ˆ W T ! S1
with T .t/ D 1: Then ker.ˆ/ is a closed subgroup of T that contains t and thus the
group generated by t: But since ˆ is nonconstant, ker.ˆ/ ¤ T; which means that
the closure of the group generated by t is not all of T:
In the other direction, let S be the closure of the group generated by t and suppose
that S is not all of T: We will proceed by describing the preimage of S under the
exponential map, using an extension of Lemma 11.3. Thus, let ƒ be the set of all
H 2 t such that e2H 2 S: Since S is a closed subgroup of T; the set ƒ will be a
closed subgroup of the additive group t: Now let ƒ0 be the identity component of
ƒ; which must be a subspace of t: (Indeed, by Corollaries 3.47 and 3.52, ƒ0 must
be equal to the Lie algebra of ƒ:) Since S is not all of T; the subspace ƒ0 cannot be
all of t:
The entire group ƒ now decomposes as ƒ0  ƒ1, where
ƒ1 WD ƒ \ .ƒ0/?

11.1
Tori
311
t
t2
t3
t4
t5
t6
2
2
Fig. 11.2 A portion of the dense subgroup generated by t in S1  S1
is a closed subgroup of .ƒ0/?: Furthermore, the identity component of ƒ1 must be
trivial, which means that the Lie algebra of ƒ1 must be f0g: Thus, by Theorem 3.42,
ƒ1 is discrete. Let us now deﬁne a homomorphism  W t ! S1 by setting
.H/ D e2i.H/
for some linear functional  on t: By Lemma 11.3, ƒ1 is the integer span of linearly
independent vectors v1; : : : ; vl. Since ƒ0 ¤ t; we can arrange things so that  is
zero on ƒ0; the values of  on v1; : : : ; vl are integers, but  is not identically zero.
Then ker./ will contain ƒ; and, in particular, the kernel of the map H 7! e2H:
Thus, there is a nonconstant, continuous homomorphism ˆ W T ! S1 satisfying
ˆ.e2H/ D .H/
for all H 2 t: If we choose H so that e2H D t 2 S; then H 2 ƒ; which means
that
ˆ.t/ D ˆ.e2H / D 1;
but ˆ is not constant.
ut

312
11
Compact Lie Groups and Maximal Tori
Proof of Proposition 11.4. In light of Lemma 11.5, we may reformulate the propo-
sition as follows: The numbers 1; 1; : : : ; k are linearly dependent over Q if
and only if there exists a nonconstant homomorphism ˆ W T
! S1 with
.e2i1; : : : ; e2ik/ 2 ker.ˆ/: Suppose ﬁrst that there is a dependence relation
among 1; 1; : : : ; k over Q: Then after clearing the denominators from this relation,
we ﬁnd that there exist integers m1; : : : ; mk; not all zero, such that
m11 C    C mkk 2 Z:
Thus, we may deﬁne a nonconstant ˆ W T ! S1 by
ˆ.u1; : : : ; uk/ D um1
1    umk
k
(11.1)
and the kernel of ˆ will contain t:
In the other direction, Exercise 2 tells us that every continuous homomorphism
ˆ W T ! S1 is of the form (11.1) for some set of integers m1; : : : ; mk: Furthermore,
if ˆ is nonconstant, these integers cannot all be zero. Thus, if
1 D ˆ.e2i1; : : : ; e2ik/ D e2i.m11CCmkk/;
we must have m11 C    C mkk D n for some integer n; which implies that
1; 1; : : : ; k are linearly dependent over Q:
ut
11.2
Maximal Tori and the Weyl Group
In this section, we introduce a the concept of a maximal torus, which plays the same
role in the compact group approach to representation theory as the Cartan subalgebra
plays in the Lie algebra approach.
Deﬁnition 11.6. A subgroup T of K is a torus if T is isomorphic to .S1/k for
some k: A subgroup T of K is a maximal torus if it is a torus and is not properly
contained in any other torus in K.
If K D SU.n/; we may consider
T D
8
ˆˆˆ<
ˆˆˆ:
0
BBB@
ei1
:::
ein1
ei.1CCn1/
1
CCCA
ˇˇˇˇˇˇˇˇˇ
j 2 R
9
>>>=
>>>;
;
which is a torus of dimension n  1: If T is contained in another torus S  SU.n/;
then every element s of S would commute with every element t of T: If we choose

11.2
Maximal Tori and the Weyl Group
313
t to have distinct eigenvalues, then by Proposition A.2, s would have to be diagonal
in the standard basis, meaning that s 2 T: Thus, T is actually a maximal torus.
Proposition 11.7. If T is a maximal torus, the Lie algebra t of T is a maximal
commutative subalgebra of k: Conversely, if t is a maximal commutative subalgebra
of k; the connected Lie subgroup T of k with Lie algebra t is a maximal torus.
Proof. If T is a maximal torus, it is commutative, which means that its Lie algebra t
is also commutative (Proposition 3.22). Suppose t is contained in a commutative
subalgebra s: Then it is also contained in a maximal commutative subalgebra
s0 containing s: The connected Lie subgroup S0 with Lie algebra s0 must be
commutative (since S0 is generated by exponentials of elements of s0) and closed
(Proposition 5.24) and hence compact. Thus, by Theorem 11.2, S0 is a torus. Since
T is a maximal torus, we must have S0 D T and thus s0 D s D t; showing that t is
maximal commutative.
In the other direction, if t is maximal commutative, the connected Lie subgroup
T with Lie algebra t is closed (Proposition 5.24), hence compact. But T is also
commutative and connected, hence a torus, by Theorem 11.2. If T is contained in a
torus S; then t is contained in the commutative Lie algebra s of S: Since t is maximal
commutative, we have s D t and since S is connected, S D T; showing that T is a
maximal torus.
ut
Deﬁnition 11.8. If T is a maximal torus in K; then the normalizer of T , denoted
N.T /; is the group of elements x 2 K such that xTx1 D T: The quotient group
W WD N.T /=T
is the Weyl group of T:
Note that T is, almost by deﬁnition, a normal subgroup of N.T /: If w is an
element of W represented by x 2 N.T /; then w acts on T by the formula
w  t D xtx1;
h 2 T:
If x 2 N.T /; the conjugation action of x maps T onto T: It follows that Adx maps
the Lie algebra t of T into itself. We deﬁne an action of W on t by
w  H D Adx.H/;
H 2 t:
(11.2)
Since our inner product is invariant under the adjoint action of K; the action of W
on t is by orthogonal linear transformations.
We will see in Sect. 11.7 that the centralizer of T —that is, the group of those
x 2 K such that xtx1 D t for all t 2 T —is equal to T: It follows that W acts
effectively on T; meaning that if w  t D t for all t 2 T; then w is the identity
element of W: It then follows from Corollary 3.49 that W also acts effectively on
t: Thus, W may be identiﬁed with the group of orthogonal linear transformations
of t of the form H 7! w  H. We will also show in Sect. 11.7 that this group

314
11
Compact Lie Groups and Maximal Tori
of linear transformations coincides with the group generated by the reﬂections
through the hyperplanes orthogonal to the roots. Thus, the Weyl group as deﬁned
in Deﬁnition 11.8 is naturally isomorphic to the Weyl group associated to the Lie
algebra g WD kC in Sect. 7.4. Exercise 3, meanwhile, asks the reader to verify
directly in the case of SU.n/ that the centralizer of T is T and that N.T /=T is
the permutation group on n entries, thus agreeing with the Weyl group for sl.nI C/
computed in Sect. 7.7.1.
The following "torus theorem" is a key result that underlies many of the
developments in this chapter and the next two chapters.
Theorem 11.9 (Torus Theorem). If K is a connected, compact matrix Lie group,
the following results hold.
1. If S and T are maximal tori in K; there exists an element x of K such that
T D xSx1:
2. Every element of K is contained in some maximal torus.
The torus theorem has many important consequences; we will mention just two
of these now.
Corollary 11.10. If K is a connected, compact matrix Lie group, the exponential
map for K is surjective.
Proof. For any x
2 K; choose a maximal torus T containing x: Since the
exponential map for T Š .S1/k is surjective, x can be expressed as the exponential
of an element of the Lie algebra of T:
ut
Corollary 11.11. Let K be a connected, compact matrix Lie group and let x an
arbitrary element of K: Then x belongs to the center of K if and only if x belongs
to every maximal torus in K:
Proof. Assume ﬁrst that x belongs to the center Z.K/ of K; and let T be any
maximal torus in K: By the torus theorem, x is contained in a maximal torus S;
and this torus is conjugate to T: Thus, there is some y 2 K such that S D yTy1:
Since x 2 S; we have x D yty1 for some t 2 T; and thus t D y1xy: But we are
assuming that x is central, and so, actually, t D x; showing that x belongs to T:
In the other direction, assume x belongs to every maximal torus in K: Then for
any y 2 K; we can ﬁnd some torus T containing y; and this torus will also contain
x: Since T is commutative, we conclude that x and y commute, showing that x is
in Z.K/:
ut
The torus theorem follows from the following result.
Lemma 11.12. Let T be a ﬁxed maximal torus in K: Then every y 2 K can be
written in the form
y D xtx1
for some x 2 K and t 2 T:

11.3
Mapping Degrees
315
If K D SU.n/ and T is the diagonal subgroup of K; Lemma 11.12 follows easily
from the fact that every unitary matrix has an orthonormal basis of eigenvectors.
The proof of the general case of Lemma 11.12 requires substantial preparation and
is given in Sect. 11.5.
Proof of torus theorem assuming Lemma 11.12. Since each y 2 K can be written
as y D xtx1; we see that y belongs to the maximal torus xTx1:
Next we show that every maximal torus S in K is conjugate to T; from which
it follows any two maximal tori S1 and S2 are conjugate to each other. Suppose s
is an element of S such that the subgroup of S generated by s is dense in S as in
Proposition 11.4. Then we can choose some x 2 K and t 2 T such that s D xtx1
and t D x1sx: Thus, x1skx D tk 2 T for all integers k: Since the set of elements
of the form sk is dense in S; we must have x1Sx  T . But since T is maximal, we
actually have x1Sx D T:
ut
11.3
Mapping Degrees
We now introduce a method that we will use in proving the torus theorem in
Sect. 11.5. The current section requires greater familiarity with manifolds than
elsewhere in the book. In addition to differential forms (Appendix B), we make
use of the exterior derivative ("d"), the pullback of a form by a smooth map, and
Stoke's theorem. See Chapters 14 and 16 in [Lee] for more information. For our
purposes, the main result of this section is Corollary 11.17, which gives a condition
guaranteeing that a map between two manifolds of the same dimension is surjective.
The reader who is not familiar with manifold theory should still be able to get an
idea of what is going on from the example in Figures 11.3 and 11.4.
If V is a ﬁnite-dimensional vector space over R; we may deﬁne an equivalence
relation on ordered bases of V by declaring two ordered bases .v1; : : : ; vn/ and
.v0
1; : : : ; v0
n/ to be equivalent if the unique linear transformation L mapping vj to v0
j
has positive determinant. The set of ordered bases for V then consists of exactly
two equivalence classes. An orientation for V is a choice of one of these two
equivalence classes. Once an orientation of V has been ﬁxed, an ordered basis for V
is said to be oriented if it belongs to the chosen equivalence class. An orientation
on a smooth manifold M is then a continuous choice of orientation on each tangent
space to M: A smooth manifold together with a choice of orientation is called an
oriented manifold.
We consider manifolds that are closed—that is, compact, connected, and without
boundary—and oriented. We will be interested in smooth maps between two closed,
oriented manifolds of the same dimension.
Deﬁnition 11.13. Let X and Y be closed, oriented manifolds of dimension n  1
and let f W X ! Y be a smooth map. A point y 2 Y is a regular value of f if
for all x 2 X such that f .x/ D y; the differential f.x/ of f at x is invertible.

316
11
Compact Lie Groups and Maximal Tori
A point y 2 Y is a singular value of f if there exists some x 2 X such that
f .x/ D y and the differential f.x/ of f at x is not invertible.
It is important to note that if y is not in the range of f; then y is a regular value.
After all, if there is no x with f .x/ D y; then it is vacuously true that for every x
with f .x/ D y; the differential f.x/ is invertible.
Proposition 11.14. Let X; Y; and f be as in Deﬁnition 11.13. If y is a regular
value of f; then y has only ﬁnitely many preimages under f:
Proof. If f 1.fyg/ were inﬁnite, the set would have to have an accumulation point
x0, by the assumed compactness of X. Then by continuity, we would have f .x0/ D
y: Since y is a regular value of f; then f.x0/ would be invertible. But then the
inverse function theorem would say that f is injective in a neighborhood of x0;
which is impossible since every neighborhood of x0 contains inﬁnitely many points
x with f .x/ D y:
ut
Saying that X and Y are oriented means that we have chosen a consistent
orientation on each tangent space to X and to Y: If f W X ! Y is smooth and
the differential f.x/ of f at x is invertible, then f.x/ is either an orientation
preserving or an orientation reversing map of Tx.X/ to Tf.x/.Y /: Since f 0 is
assumed to be continuous, if f is invertible and orientation preserving at x; it
is invertible and orientation preserving in a neighborhood of x; and similarly if
f is invertible and orientation reversing at x:
Deﬁnition 11.15. If y is a regular value of f; let the signed number of preimages
of y denote the number of preimages, where x 2 f 1.fyg/ counted with a plus
sign if f.x/ is orientation preserving and with a minus sign if f.x/ is orientation
reversing.
The main result of the section is the following.
Theorem 11.16. Let X and Y be closed, oriented manifolds of dimension n  1
and let f W X ! Y be a smooth map. Then there exists an integer k such that for
every regular value y of f; the signed number of preimages of y is equal to k:
If there are, in fact, any regular values of f; the integer k is unique and is called
the mapping degree of f: (Actually, Sard's theorem guarantees that every such f
has a nonempty set of regular values, but we do not need to know this, since in
our application of Theorem 11.16 in Sect. 11.5, we will ﬁnd regular values of the
relevant map "by hand.") See the "Degree Theory" section in Chapter 17 of [Lee]
for more information.
Corollary 11.17. Let X; Y; and f be as in Theorem 11.16. If there exists a regular
value y of f for which the signed number of preimages is nonzero, then f must
map onto Y:
Proof. If there existed some y0 that is not in the range of f; then y0 would be a
regular value and the (signed) number of preimages of y0 would be zero. This would
contradict Theorem 11.16.
ut

11.3
Mapping Degrees
317
Fig. 11.3 The graph of a
map f from S1 to S1: The
signed number of preimages
of each regular value is 1
a
b
y
y
2
2
Figure 11.3 illustrates Theorem 11.16. The ﬁgure shows the graph of a map f
from S1 (which we think of as Œ0; 2 with ends identiﬁed) to itself. The singular
values are the points marked a and b on the y-axis; all other values are regular. For
two different values y and y0; we compute the signed number of preimages. The
point y has three preimages, but f 0 is negative at one of these, so that the signed
number of preimages is 1  1 C 1 D 1: Meanwhile, the point y0 has one preimage,
at which f 0 is positive so that the signed number of preimages is 1. The mapping
degree of f is 1 and, consistent with Corollary 11.17, f is surjective. Meanwhile,
Figure 11.4 shows the same map in a more geometric way, as a map between two
manifolds X and Y; each of which is diffeomorphic to S1:
We now turn to the proof of Theorem 11.16; see also Theorem 17.35 in
[Lee]. Using the inverse function theorem, it is not hard to show that the signed
number of preimages and the unsigned number of preimages are both constant in a
neighborhood of any regular value y: This result, however, does not really help us,
because the set of regular values may be disconnected. (See Figure 11.3.) Indeed,
Figure 11.3 shows that the unsigned number of preimages may not be constant; we
need a creative method to show that the signed number of preimages is constant on
the set of regular values.
Our tool for proving this result is that of differential forms. If f W X ! Y is
an orientation-preserving diffeomorphism, then for any n-form ˛ on Y; the integral
of f .˛/ over X will equal the integral of ˛ over Y: If, on the other hand, f is an
orientation-reversing diffeomorphism, the integral of f .˛/ will be the negative of
the integral ˛: Suppose now that f W X ! Y is a smooth map, not necessarily a
diffeomorphism. Suppose that y is a regular value of f and that x1; : : : ; xN are the

318
11
Compact Lie Groups and Maximal Tori
X
Y
b
a
Fig. 11.4 The map indicated by the arrows is the same one as in Figure 11.3, but shown more
geometrically
elements of f 1.fyg/: Then we can ﬁnd a neighborhood V of y such that f 1.V / is
a disjoint union of neighborhoods U1; : : : ; UN of x1; : : : ; xN and such that f maps
each Uj diffeomorphically onto V: Furthermore, by shrinking V if necessary, we
can assume that for each j; the differential f.x/ is either orientation preserving at
every point of Uj or orientation reversing at every point of Uj: Then for any n-form
˛ supported in V; we see that
Z
X
f .˛/ D k
Z
Y
˛;
(11.3)
where k is the signed number of preimages of y:
If ˛ is chosen so that
R
Y ˛ ¤ 0; then (11.3) becomes
k D
R
X f .˛/
R
Y ˛
:
(11.4)
The right-hand side of (11.4) gives us an analytic method for determining the signed
number of preimages. Our goal is to use (11.4) to show that k is constant on the set
of regular values. To this end, we will establish a key result: The value of the right-
hand side of (11.4) is unchanged if ˛ is "deformed" by pulling it back by a family
of diffeomorphisms of Y:
Suppose, then, that y and y0 are regular values of f with the signed number of
preimages being k and k0; respectively. We will construct a family ˛t; 0  t  1; of
n-forms on Y such that ˛0 is supported near y and ˛1 is supported near y0: For all
t; the expression
R
X f .˛t/
R
Y ˛t
(11.5)

11.3
Mapping Degrees
319
makes sense, even if the support of ˛t contains singular values of f: Furthermore,
the values of both integrals in (11.5) are—as we will show—independent of t: Thus,
we will conclude that
k D
R
X f .˛0/
R
Y ˛0
D
R
X f .˛1/
R
Y ˛1
D k0;
as claimed.
Lemma 11.18. Suppose
‰t
is
a
continuous,
piecewise-smooth
family
of
orientation-preserving diffeomorphisms of Y; with ‰0 being the identity map.
For any n-form ˛ on Y; let ˛t D ‰
t .˛/: Then for all t; we have
Z
Y
˛t D
Z
Y
˛
(11.6)
and
Z
X
f .˛t/ D
Z
X
f .˛/:
(11.7)
Proof. Saying that ‰t is piecewise smooth means that it we can divide Œ0; T  into
ﬁnitely many subintervals on each of which ‰t.x/ is smooth in x and t: Since ‰t is
continuous, it sufﬁces to prove the result on each of the subintervals, that is, in the
case where ‰t.x/ is smooth, which we now assume. The result (11.6) holds because
‰t is an orientation preserving diffeomorphism.
To establish (11.7), we show that the left-hand side of (11.7) is independent of t:
Note that
f .˛t/ D f .‰
t .˛// D .‰t ı f /.˛/:
Thus, if we deﬁne g W X  Œ0; T  ! Y by
g.x; t/ D ‰t.f .x//;
we have
Z
X
f .˛T / 
Z
X
f .˛0/ D
Z
@.XŒ0;T /
g.˛/:
Using Stoke's theorem and a standard result relating pullbacks and exterior deriva-
tives, we obtain

320
11
Compact Lie Groups and Maximal Tori
Z
X
f .˛T / 
Z
X
f .˛0/ D
Z
XŒ0;T 
d.g.˛//
D
Z
XŒ0;T 
g.d˛/:
But since ˛ is a top-degree form on Y; we must have d˛ D 0; showing that
R
X f .˛T / D
R
X f .˛0/:
ut
Proof of Theorem 11.16. To complete the proof of Theorem 11.16, it remains only
to address the existence of a continuous, piecewise smooth, orientation-preserving
family ‰t of diffeomorphisms of Y such that ‰0 is the identity and such that
‰1.y0/ D y: (Thus, if ˛ is supported near y; then ‰
1 .˛/ will be supported near
y0:) We actually only require this in the case that Y is a compact Lie group, in
which case, the diffeomorphisms can easily be constructed using the group structure
on Y: Nevertheless, we will outline an argument for the general result. Let U be a
neighborhood of y0 that is a rectangle in some local coordinate system around y0:
Then it is not hard to construct a family of diffeomorphisms of Y that are the identity
on Y n U and that map y0 to any desired point of U: (See Exercise 7.)
If y 2 U; we are done. If not, we consider the set E of points z 2 Y such that
y0 can be moved to z by a family of diffeomorphisms of the desired sort. If z 2 E;
we have, by assumption, a family moving y0 to z: We can then use the argument in
the preceding paragraph to move z to any point z0 in a neighborhood of z: Thus, E
is open and contains y0: We now claim that E is closed. If z is a limit point of E;
then in any neighborhood V of z; there is an element z0 of E: Thus, by the argument
in the preceding paragraph, we can move z0 to z by a family of diffeomorphisms.
Since E is both open and closed and nonempty (because it contains y), E must be
all of Y .
ut
Proposition 11.19. Let X; Y; and f be as in Theorem 11.16, and suppose f has
mapping degree k: Then for every n-form ˛ on Y; we have
Z
X
f .˛/ D k
Z
Y
˛:
(11.8)
In Figure 11.4, for example, the map f indicated by the arrows has mapping
degree 1. Thus, for every form ˛ on Y; the integral of f .˛/ over X is the same
as the integral of ˛ over Y; even though f is not a diffeomorphism. When pulling
back the part of ˛ between a and b; we get three separate integrals on X; but one of
these integrals occurs with a minus sign, because f is orientation reversing on the
middle of the three intervals over Œa; b:
Proof. We have already noted in (11.3) that if y is a regular value of f; there is
a neighborhood U of y such that (11.8) holds for all ˛ supported in U: By the
deformation argument in the proof of Theorem 11.16, the same result holds for any
y in Y: Since Y is compact, we can then cover Y by a ﬁnite number of open sets Uj

11.4
Quotient Manifolds
321
such that (11.8) holds whenever ˛ is supported in Uj : Using a partition of unity, we
can express any form ˛ as a sum of forms ˛j such that ˛j is supported on Uj; and
the general result follows.
ut
11.4
Quotient Manifolds
Before coming the proof of the torus theorem, we require one more preparatory
concept, that of the quotient of a matrix Lie group by a closed subgroup. Throughout
this section, we assume that G is a matrix Lie group with Lie algebra g and that H
is a closed subgroup of G with Lie algebra h: Even if H is not normal, we can still
consider the quotient G=H as a set (the set of left cosets gH of H). We now show
that G=H has the structure of a smooth manifold. We will let Œg denote the coset
gH of H in G and we will let Q W G ! G=H denote the quotient map. Recall that
a topological structure on a set E is Hausdorff if for every pair of distinct points
x; y 2 E; there exist disjoint open sets U and V in E with x 2 U and y 2 V:
Lemma 11.20. Deﬁne a topology on G=H by decreeing that a set U in G=H is
open if and only if Q1.U / is open in G: Then G=H is Hausdorff with respect to
this topology. Furthermore, G=H has a countable dense subset.
If we did not assume that H is closed, the Hausdorff condition for G=H would,
in general, fail.
Proof. If E  G is invariant under the right action of H; then Q1.Q.E// D E:
From this observation, we can see that the map U 7! Q1.U / gives a bijection
between the collection of open sets in G=H and the collection of right-H-invariant
open sets in G: Suppose that Œx and Œy are disjoint points in G=H; that is, that xH is
disjoint from yH: In light of the above description of open sets in G=H; it sufﬁces to
ﬁnd disjoint open, right-H-invariant subsets A and B of G; with x 2 A and y 2 B:
Since H is closed, yH is also closed. Thus, we can ﬁnd a neighborhood U of
x that does not intersect yH: We can then ﬁnd a subneighborhood V  U of x
such that NV is compact and contained in U: We then consider the set NV H, the set of
points of the form vh with v 2 NV and h 2 H: We claim that NV H is closed in G: To
see this, suppose vnhn converges to some g 2 G; where vn 2 V and hn 2 H: Since
NV is compact, we can assume (after passing to a subsequence) that vn converges
to some v 2 NV ; in which case, hn converges to the point h WD v1g: Since H is
closed, h belongs to H and the limit point vh belongs to NV H: Since NV H is closed,
the set B WD . NV H/c is open. Thus, B and A WD VH are the desired disjoint, open,
right-H-invariant sets in G:
Finally, since G inherits its topology from the separable metric space Mn.C/ Š
R2n2; it follows that G has a countable dense subset E. Furthermore, the quotient
map Q W G ! G=H is surjective and (by the deﬁnition of the topology on G=H)
continuous. Thus, Q.E/ will be dense in G=H:
ut

322
11
Compact Lie Groups and Maximal Tori
Fig. 11.5 The gray region
indicates the set of points of
the form gh with g 2 exp.U /
and h 2 H: This set is
diffeomorphic to exp.U /  H
H
exp U
Lemma 11.21 (Slice Lemma). Let G be a matrix Lie group with Lie algebra g and
let h be a subalgebra of g: Decompose g as a vector space as g D h ˚ f for some
subspace f of g and deﬁne a map ƒ W f  H ! G by
ƒ.X; h/ D eXh:
Then there is a neighborhood U of 0 in f such that ƒ is injective on U  H and ƒ
maps U  H diffeomorphically onto an open subset of G: In particular, if X1 and
X2 are distinct elements of U; then eX1 and eX2 belong to distinct cosets of G=H:
The term "slice lemma" refers to the fact that the map sending X 2 U to eX
slices across the different cosets of H: Figure 11.5 illustrates the slice lemma in the
case in which G D S1  S1 and H is the subgroup consisting of points of the form
.ei; e3i/:
Proof. We identify the tangent spaces at the identity to both fH and G with f˚h:
If X.t/ is a smooth curve in f passing through 0 at t D 0; we have
d
dtƒ.X.t/; I/
ˇˇˇˇ
tD0
D d
dtetX
ˇˇˇˇ
tD0
D X:
Meanwhile, if h.t/ is a smooth curve in H passing through I at t D 0; we have
d
dtƒ.0; h.t//
ˇˇˇˇ
tD0
D d
dth.t/
ˇˇˇˇ
tD0
D h0.0/:

11.4
Quotient Manifolds
323
From this calculation, and the linearity of the differential, we can see that the
differential ƒ of ƒ at .0; I/ is the identity map of f˚h to itself. Thus, by continuity,
ƒ.X; e/ is invertible for X in some neighborhood U of 0 in f.
Meanwhile, the map ƒ commutes with the right action of H:
ƒ.X; hh0/ D ƒ.X; h/h0:
From this, it is easy to see that if ƒ.X; I/ is invertible, then ƒ.X; h/ is invertible
for all h: We conclude, then, that ƒ.X; h/ is invertible for all .X; h/ 2 U  H: By
the inverse function theorem, then, ƒ maps a small neighborhood of each point in
U  H injectively onto an open set in G: In particular, the image of U  H under
ƒ is an open subset of G:
We must now show that by shrinking U as necessary, we can ensure that
ƒ is globally injective on U  H: By the inverse function theorem, there are
neighborhoods U 0 of 0 in f and V of I in H such that ƒ maps U 0  V injectively
into G. If we choose a small subneighborhood U 00 of 0 in f and X and X0 are in U 00;
then eX0eX will be close to the identity in G: Indeed, if U 00 is small enough, then
whenever eX0eX happens to be in H; the element eX0eX will actually be in V:
We now claim that ƒ is injective on U 00  H: To see this, suppose eXh D eX0h0
with X; X0 2 U 00 and h; h0 2 H: Then
h0h1 D eX0eX 2 V;
by our choice of U 00: But then
eX D eX0.h0h1/;
and since h0h1 2 V; we must have X D X0 and I D h0h1; by the injectivity of ƒ
on U 0  V: Thus, actually, X D X0 and h D h0; establishing the desired injectivity.
ut
It is instructive to contemplate the role in the preceding proof played by the
assumption that H is closed. It is evident from Figure 1.1 that the slice lemma can
fail if H is not closed. (For example, even for very small nonzero X 2 f; the element
eX can be in H.) On the other hand, even if H is not closed, Theorem 5.23 says that
there is a new topology on H and an atlas of coordinate neighborhoods making H
into a smooth manifold, in such a way that the inclusion of H into G is smooth. If
we use this structure on H; the map ƒ in Lemma 11.21 is smooth and much of the
proof proceeds in the same way as when H is closed. The proof of global injectivity
of ƒ; however, breaks down because the new topology on H does not agree with
the topology that H inherits from G: Thus, even if eX0eX belongs to H and is very
close to the identity in G; this element may not be close to the identity in the new
topology on H: (See Figure 5.4.) Thus, we cannot conclude that eX0eX is in V; and
the proof of injectivity fails.

324
11
Compact Lie Groups and Maximal Tori
Theorem 11.22. If G is a matrix Lie group and H a closed subgroup of G; then
G=H can be given the structure of a smooth manifold with
dim.G=H/ D dim G  dim H
in such a say that (1) the quotient map Q W G ! G=H is smooth, (2) the differential
of Q at the identity maps TI.G/ onto TŒI.G=H/ with kernel h; and (3) the left action
of G on G=H is smooth.
Proof. Let U  f be as in Lemma 11.21. Then for each g 2 G; let ƒg be the map
from U  H into G given by
ƒg.X; h/ D geXh:
Combining Lemma 11.21 with a translation by g; we see that ƒg is a diffeomor-
phism of U  H onto its image, and that ƒg.X; h/ and ƒg.X0; h0/ are in distinct
cosets of H provided that X ¤ X0: Let Wg be the (open) image of U  H under
ƒg and let Vg D Q.Wg/; that is,
Vg D
˚
ŒgeX 2 G=H
ˇˇ X 2 U

:
Then Q1.Vg/ D Wg; showing that Vg is open in G=H: By the above properties of
ƒg; the map X 7! ŒgeX is an injective map of U onto Vg:
We now propose to use the maps X 7! ŒgeX as local coordinates on G=H;
where U  f may be identiﬁed with Rk; with k D dim f D dim g  dim h: By
the way the topology on G=H is deﬁned, the map Q is continuous and a function
f on G=H is continuous if and only if f ı Q is continuous on G: Thus, the map
X 7! Q.geX/ D ŒgeX is continuous. Furthermore, if we compose the inverse map
ŒgeX 7! X with Q; we obtain the map geXh 7! X: This map is continuous because
it consists of the inverse of the diffeomorphism .X; h/ 7! geXh; combined with the
map .X; h/ 7! X:
Thus, G=H is locally homeomorphic to Rk: Since, also, G=H is Hausdorff and
has a countable dense subset (Lemma 11.20), we see that G=H is a topological
manifold.
Now, the coordinate patches ŒgeX clearly cover G=H: If two such patches
overlap, the change of coordinates map is the map X 7! X0; where ŒgeX D Œg0eX0:
This map can be computed by mapping X to geX 2 G and then applying .ƒg0/1
to write geX as geX D g0eX0h0: Since .ƒg0/1 is smooth, we see that the change
of coordinates map is smooth. Thus, we may give a smooth structure to G=H using
these coordinates.
It is now a straightforward matter to check the remaining claims about the smooth
structure on G=H: To see, for example, that Q is smooth, pick some g in G and write
points near g as geXh; with X 2 U and h 2 H: Then Q.geXh/ D ŒgeX: Thus, Q
can be written locally as the inverse of ƒg followed by the map .X; h/ 7! ŒgeX;
which is smooth in our local identiﬁcation of G=H with U: The remaining claims
are left as an exercise to the reader (Exercise 8).
ut

11.4
Quotient Manifolds
325
Proposition 11.23. Suppose there exists an inner product on g that is invariant
under the adjoint action of H; and let V denote the orthogonal complement of h
with respect to this inner product. Then we may identify the tangent space at each
point Œg of G=H with V by writing v 2 TŒg.G=H/ as
v D d
dtŒgetX
ˇˇˇˇ
tD0
;
X 2 V:
This identiﬁcation of TŒg.G=H/ with V is unique up to the adjoint action of H on V:
Note that since the adjoint action of H on g is orthogonal and preserves h; this
action also preserves V; by Proposition A.10.
Proof. Since g D h˚V; Point 2 of Theorem 11.22 tells us that every tangent vector
v to G=H at the identity coset can be expressed uniquely as
v D d
dtŒetX
ˇˇˇˇ
tD0
;
X 2 V:
For any Œg 2 G=H; we identify TŒg.G=H/ with TŒI.G=H/ Š V by using the left
action of g: Thus, each v 2 TŒg.G=H/ can be written uniquely as
v D d
dtŒgetX
ˇˇˇˇ
tD0
;
X 2 V:
If we use a different element gh; h 2 H; of the same coset, we get a different
identiﬁcation of TŒg.G=H/ with V; as follows:
ŒghetX D ŒghetXh1 D ŒgetX0;
(11.9)
where X0 D Adh.X/: Differentiating (11.9) shows that the two identiﬁcations differ
by the adjoint action of H:
ut
A volume form on a manifold M of dimension n is a nowhere-vanishing n-form
on M: As we have already discussed in the proof of Theorem 4.28, each matrix Lie
group G has a volume form that is invariant under the right action of G on itself.
The same argument shows that G has a volume form invariant under the left action
of G on itself. (For some groups G, it is possible to ﬁnd a single volume form that
is invariant under both the left and right action of G on itself, but this is not always
the case.) We now address the existence of an invariant volume form on a quotient
manifold.
Proposition 11.24. If G is a matrix Lie group and H is a connected compact
subgroup of G; there exists a volume form on G=H that is invariant under the left
action of G: This form is unique up to multiplication by a constant.

326
11
Compact Lie Groups and Maximal Tori
In the case H D fIg; we conclude that there is a left-invariant volume form on G
itself and that this form is unique up to a constant. (In this chapter, it is convenient
to use a left-invariant volume form on G; rather than a right-invariant form as in
Sect. 4.4.)
Proof. Since H is compact, there exists an inner product on g that is invariant under
the adjoint action of H: Let V denote the orthogonal complement of h in g; so that
V is invariant under the adjoint action of H: Since H is connected, the restriction
to V of Adh will actually be in SO.V / for all h 2 H:
Now pick an orientation on V and let ˛ be the standard volume form on V; that is,
the unique one for which ˛.e1; : : : ; eN / D 1 whenever .e1; : : : ; eN / is an oriented
orthonormal basis for V: Since the action of Adx on V has determinant 1, we have
˛.Adx.v1/; : : : ; Adx.vN // D ˛.v1; : : : ; vN /
for all v1; : : : ; vN 2 V:
Now, the tangent space to G=H at the identity coset is identiﬁed with V .
We deﬁne a form on G=H as follows. At the identity coset, we take it to be ˛:
At any other coset Œg; we use the action of g 2 G to transport ˛ from ŒI to Œg:
If Œg D Œg0; then g0 D xh for h 2 H: The action of h on TŒI.G=H/ Š V is the
adjoint action, which preserves ˛: Thus, the resulting form at Œg is independent of
the choice of g:
Finally, to address the uniqueness, note that any two top degree forms on G=H
must agree up to a constant at the identity coset ŒI: But the since the left action of
G on G=H is transitive, the value of the form at ŒI uniquely determines the form
everywhere. Thus, any two left-invariant forms on G=H must be equal up to an
overall constant.
ut
11.5
Proof of the Torus Theorem
Having made the required preparations in Sects. 11.3 and 11.4, we are now ready to
complete the proof of Theorem 11.9. It remains only to prove Lemma 11.12; to that
end, we deﬁne a key map.
Deﬁnition 11.25. Let T be a ﬁxed maximal torus in K: Let
ˆ W T  .K=T / ! K
be deﬁned by
ˆ.t; Œx/ D xtx1;
(11.10)
where Œx denotes the coset xT in K=T:

11.5
Proof of the Torus Theorem
327
Note that if s 2 T; then since T is commutative, we have
.xs/t.xs/1 D xsts1x1 D xtx1;
showing that ˆ is well deﬁned as a map of T  .K=T / into K: Lemma 11.12
amounts to saying that ˆ is surjective. Since T  .K=T / has the same dimension
as K; we may apply Theorem 11.16 and Corollary 11.17. Thus, if there is even one
regular value of ˆ for which the signed number of preimages is nonzero, ˆ must
be surjective. Our strategy will be to ﬁnd a certain class of points y 2 K for which
we can (1) determine all of the preimages of y under ˆ; and (2) verify that ˆ is
invertible and orientation preserving at each of the preimages.
Lemma 11.26. Let t 2 T be such that the subgroup generated by t is dense in T
(Proposition 11.4). Then ˆ1.ftg/ of t consists precisely of elements of the form
.x1tx; Œx/ with Œx belonging to W D N.T /=T . In particular, if xsx1 D t for
some x 2 K and s 2 T; then s must be of the form s D w1  t for some w 2 W:
Note that if x and y in N.T / represent distinct elements of W D N.T /=T; then
Œx and Œy are distinct elements of K=T: Thus, the lemma tells us that there is a
one-to-one correspondence between ˆ1.ftg/ and W:
Proof. If x 2 N.T /; then x1tx 2 T and we can see that ˆ.x1tx; Œx/ D t: In the
other direction, if xsx1 D t; then
x1tmx D sm 2 T
for all integers m; so that x1Tx  T by our assumption on t: Since x1Tx is again
a maximal torus, we must actually have x1Tx D T and, thus, T D xTx1; showing
that x 2 N.T /: Furthermore, since xsx1 D t; we have s D x1tx D w1  t; where
w D Œx:
ut
We now compute the differential of ˆ: Using Proposition 11.23 with .G; H/
equal to .T; fIg/; .K; T /; and .K; fIg/; we identify the tangent space at each point
in T  .K=T / with t ˚ f Š k and the tangent space at each point in K with k:
Since we are trying to determine the signed number of preimages of ˆ; we must
choose orientations on T  .K=T / and on K: To this end, we choose orientations
on the vector spaces t and f and use the obvious associated orientation on k Š t ˚ f:
We then deﬁne orientations on T  .K=T / and K using the above identiﬁcations
of the tangent spaces with t ˚ f Š k: The identiﬁcation of the tangent spaces to
K=T with f is unique up to the adjoint action of T (Proposition 11.23). Since T is
connected, this action will have positive determinant, showing that the orientation
on T  .K=T / is well deﬁned. Recall that the (orthogonal) adjoint action of T on k
preserves t and thus, also, f WD t?:
Proposition 11.27. Let .t; Œx/ be a ﬁxed point in T  .K=T /. If we identify the
tangent spaces to T  .K=T / and to K with t ˚ f Š k; then the differential of ˆ at
.t; Œx/ is represented by the following operator:

328
11
Compact Lie Groups and Maximal Tori
ˆ D .Adx/
I
0
0 Ad0
t1  I

;
(11.11)
where Ad0
t1 denotes the restriction of Adt1 to f:
Proof. For H 2 t; we compute that
ˆ.teH; Œx/
ˇˇ
D0 D d
d xteH x1
ˇˇˇˇ
D0
D xtHx1
D .xtx1/.Adx.H//:
Since we identify the tangent space to K at xtx1 with k using the left action of
xtx1; we see that ˆ..H; 0// D Adx.H/:
Meanwhile, if X 2 f; we compute that
ˆ.t; ŒxeX/
ˇˇ
D0 D d
d xeXteXx1
ˇˇˇˇ
D0
D xXtx1  xtXx1
D xtx1.xt1Xtx  x1Xx/
D .xtx1/ŒAdx.Adt1.X/  X/;
so that ˆ..0; X// D Adx.Adt1.X/  X//: These two calculations, together with
the linearity of the differential, establish the claimed form of ˆ.
ut
We now wish to determine when ˆ.t; Œx/ is invertible. Since Adx is invertible,
the question becomes whether Ad0
t1 I is invertible. When ˆ.t; Œx/ is invertible,
we would like to know whether this linear map is orientation preserving or
orientation reversing. In light of the way our orientations on T  .K=T / and K
have been chosen, the orientation behavior of ˆ will be determined by the sign of
the determinant of ˆ as a linear map of k to itself. Now, since K is connected and
our inner product on k is AdK-invariant, we see that Adx 2 SO.k/ for every x: Thus,
det.Adx/ D 1; which means that we only need to calculate the determinant of the
second factor on the right-hand side of (11.11).
Lemma 11.28. For t 2 T; let Ad0
t1 denote the restriction of Adt1 to f:
1. If t generates a dense subgroup of T; then Ad0
t1  I is an invertible linear
transformation of f:
2. For all w 2 W and t 2 T; we have
det.Ad0
wt1  I/ D det.Ad0
t1  I/:

11.5
Proof of the Torus Theorem
329
Proof. The operator Ad0
t1  I is invertible provided that the restriction of Adt1
to f does not have an eigenvalue of 1. Suppose, then, that Adt1.X/ D X for some
X 2 f: Then for every integer m; we will have Adtm.X/ D X: If t generates a dense
subgroup of T; then by taking limits, we conclude that Ads.X/ D X for all s 2 T:
But then for all H 2 t; we have
ŒH; X D d
d AdeH .X/
ˇˇˇˇ
D0
D 0:
Since t is maximal commutative (Proposition 11.7), we conclude that X 2 f \ t D
f0g: Thus, there is no nonzero X 2 f for which Adt1.X/ D X:
For the second point, if w 2 W is represented by x 2 N.T /; we have
Ad0
wt1  I D Ad0
xt1x1  I
D Adx.Ad0
t1  I/Adx1:
Thus, Ad0
wt1  I and Ad0
t1  I are similar and have the same determinant.
ut
We are now ready for the proof of Lemma 11.12, which will complete the proof
of the torus theorem.
Proof of Lemma 11.12. By Proposition 11.4, we can choose t 2 T so that the
subgroup generated by t is dense in T: Then by Lemma 11.26, the preimages
of t are in one-to-one correspondence with elements of W: Furthermore, by
Proposition 11.27 and Point 1 of Lemma 11.28, ˆ is nondegenerate at each
preimage of t: Finally, by Point 2 of Lemma 11.28, ˆ has the same orientation
behavior at each point of ˆ1.ftg/: Thus, t is a regular value of ˆ and the signed
number of preimages of t under ˆ is either jW j or  jW j : It then follows from
Corollary 11.17 that ˆ is surjective, which is the content of Lemma 11.12.
ut
Corollary 11.29. The Weyl group W is ﬁnite and the orientations on T  .K=T /
and K can be chosen so that the mapping degree of ˆ is jW j ; the order of the Weyl
group.
Proof. If t generates a dense subgroup of T; then by Lemma 11.26, ˆ1.ftg/ is in
one-to-one correspondence with W: Furthermore, Point 1 of Lemma 11.28 then tells
us that such a t is a regular value of ˆ: Thus, by Proposition 11.14, ˆ1.ftg/ is ﬁnite,
and W is thus also ﬁnite. Meanwhile, we already noted in the proof of Lemma 11.12
that ˆ has mapping degree equal to ˙ jW j : By reversing the orientation on K as
necessary, we can ensure that the mapping degree is jW j :
ut

330
11
Compact Lie Groups and Maximal Tori
11.6
The Weyl Integral Formula
In this section, we apply Proposition 11.19 to the map ˆ to obtain an integration
formula for functions f on K: Of particular importance will be the special case in
which f satisﬁes f .yxy1/ D f .x/ for all x; y 2 K: This special case of the Weyl
integral formula will be a main ingredient in our analytic proof of the Weyl character
formula in Sect. 12.4.
Recall that we have decomposed k as t˚f; where f is the orthogonal complement
of t; and that the adjoint action of T on k preserves both t and f: Deﬁne a function
 W T ! R by
.t/ D det.Ad0
t1  I/;
(11.12)
where Ad0
t1 is the restriction of Adt1 to f: Using Proposition 11.24, we can
construct volume forms on K; T; and K=T that are invariant under the left action
of K; T; and K; respectively. Since each of these manifolds is compact, the total
volume is ﬁnite, and we can normalize this volume to equal 1.
Theorem 11.30 (Weyl Integral Formula). For all continuous functions f on K;
we have
Z
K
f .x/ dx D
1
jW j
Z
T
.t/
Z
K=T
f .yty1/ dŒy dt;
(11.13)
where dx; dt; and dŒy are the normalized, left-invariant volume forms on K; T; and
K=T; respectively and jW j is the order of the Weyl group.
In Sect. 12.4, we will compute  explicitly and relate it to the Weyl denominator.
Proof of Theorem 11.30, up to a constant. Since ˆ has mapping degree jW j,
Theorem 11.16 tells us that
jW j
Z
K
f .x/ dx D
Z
T .K=T /
ˆ.f .x/ dx/
D
Z
T .K=T /
.f ı ˆ/ ˆ.dx/;
(11.14)
for any smooth function f: Since, by the Stone-Weierstrass theorem (Theorem
7.33 in [Rud1]), every continuous function on K can be uniformly approximated
by smooth functions, (11.14) continues to hold when f is continuous. Thus, to
establish (11.13), it sufﬁces to show that ˆ.dx/ D .t/ dŒy ^ dt:
Pick orthonormal bases H1; : : : ; Hr for t and X1; : : : ; XN for f: Then by the proof
of Proposition 11.24, we can ﬁnd invariant volume forms ˛1 on T; ˛2 on K=T , and
ˇ on K such that at each point, we have
˛1.H1; : : : ; Hr/ D ˛2.X1; : : : ; XN / D ˇ.H1; : : : ; Hr; X1; : : : ; XN / D 1;

11.6
The Weyl Integral Formula
331
so that
.˛1 ^ ˛2/.H1; : : : ; Hr; X1; : : : ; XN / D 1:
By the uniqueness in Proposition 11.24, ˛1; ˛2; and ˇ will coincide, up to
multiplication by a constant, with the normalized volume forms dt; dŒy; and dx;
respectively.
Now, at each point, the matrix of ˆ; with respect to the chosen bases for T .T 
.K=T // and for T .K/; is given by the matrix in (11.11). Thus, using the deﬁnition
of the pulled-back form ˆ.ˇ/; we have
ˆ.ˇ/.H1; : : : ; Hr; X1; : : : ; XN /
D ˇ.ˆ.H1/; : : : ; ˆ.Hr/; ˆ.X1/; : : : ; ˆ.XN //
D det.ˆ/ˇ.H1; : : : ; Hr; X1; : : : ; XN /
D .t/.˛1 ^ ˛2/.H1; : : : ; Hr; X1; : : : ; XN /;
where in the third line, we use (B.1) in Appendix B. Since dx coincides with ˇ up
to a constant and dŒy ^ dt coincides with ˛1 ^ ˛2 up to a constant, (11.14) then
becomes
jW j
Z
K
f .x/ dx D C
Z
T .K=T /
.t/f .yty1/ dŒy dt:
It remains only to show that C D 1: We postpone the proof of this claim until
Sect. 12.4.
ut
It is possible to verify that C D 1 directly, using Lemma 11.21. According to
that result, if U is a small open set in K=T; then q1.U / is diffeomorphic to U  T:
It is not hard to check that under the diffeomorphism between q1.U / and U  T ,
the volume form ˇ decomposes as the product of ˛2 and ˛2: Thus, for any (nice
enough) E  U; the volume of E  T D q1.E/ is the product of the volume of E
and the volume of T: From this, it is not hard to show that for any (nice enough) set
E  K=T; the volume of q1.E/ equals the volume of E (with respect to ˛2) times
the volume of T (with respect to ˛1). In particular, the volume of q1.K=T / D K
is the product of the volume of K=T and the volume of T: Thus, if we choose our
inner products on t and on f so that the volume forms ˛1 and ˛2 are normalized, the
volume form ˇ will also be normalized. In that case, the above computation holds on
the nose, without any undetermined constant. Since we will offer a different proof
of the normalization constant in Sect. 12.4, we omit the details of this argument.
We now consider an important special case of Theorem 11.30.
Deﬁnition 11.31. A function f W K ! C is called a class function if f .yxy1/ D
f .x/ for all x; y 2 K:

332
11
Compact Lie Groups and Maximal Tori
That is to say, a function is a class function if it is constant on each conjugacy
class.
Corollary 11.32. If f is a continuous class function on K; then
Z
K
f .x/ dx D
1
jW j
Z
T
.t/f .t/ dt:
(11.15)
Proof. If f is a class function, then f .yty1/ D f .t/ for all y 2 K and t 2 T:
Since the volume form on K=T is normalized, we have
Z
K
f .yty1/ dŒy D f .t/;
in which case, the Weyl integral formula reduces to (11.15).
ut
Example 11.33. Suppose K D SU.2/ and T is the diagonal subgroup. Then
Corollary 11.32 takes the form
Z
SU.2/
f .x/ dx D 1
2
Z 

f .diag.ei; ei// 4 sin2./ d
2 ;
(11.16)
where jW j D 2 and the normalized volume measure on T is d=.2/:
Note that if f  1; both sides of (11.16) integrate to 1. See also Exercise 9 in
Chapter 12 for an explicit version of the Weyl integral formula for U.n/:
Proof. If we use the Hilbert-Schmidt inner product on su.2/; the orthogonal
complement of t in su.2/ is the space of matrices X of the form
X D

0
x C iy
x C iy
0

;
with x; y 2 R: Direct computation then shows that if t D diag.ei; ei/; then
Adt1 .X/ D

0
e2i.x C iy/
e2i.x C iy/
0

:
Thus, Adt1 acts as a rotation by angle 2 in C D R2: It follows that
det.Adt1  I/ D det
cos.2/  1
sin.2/
 sin.2/ cos.2/  1

:
This determinant simpliﬁes by elementary trigonometric identities to 4 sin2 :
Finally, since W D fI; Ig; we have jW j D 2:
ut

11.7
Roots and the Structure of the Weyl Group
333
Note that the matrix diag.ei; ei/ is conjugate in SU.2/ to the matrix
diag.ei; ei/:
 ei 0
0
ei

D

0 1
1 0
  ei
0
0 ei
 0 1
1
0

:
Thus, if f is a class function on SU.2/; the value of f at diag.ei; ei/ is the
same its value at diag.ei; ei/: We may therefore rewrite the right-hand side
of (11.16) as
Z 
0
f .diag.ei; ei// 4 sin2./ d
2 :
(11.17)
Meanwhile, recall from Exercise 5 in Chapter 1 that SU.2/ can be identiﬁed
with the unit sphere S3  C2: By Exercise 9, a function f on SU.2/ is a class
function if and only if the associated function on S3 depends only on the polar
angle. Furthermore, for 0    ; the polar angle associated to diag.ei; ei/
is simply : With this perspective, (11.17) is simply the formula for integration in
spherical coordinates on S3; in the special case in which the function depends only
on the polar angle. (Apply the m D 4 case of Eq. (21.15) in [Has] to a function that
depends only on the polar angle.)
11.7
Roots and the Structure of the Weyl Group
In the context of compact Lie groups, it is convenient and customary to redeﬁne the
notion of "root" by a factor of i so that the roots will now live in t rather than in it:
Deﬁnition 11.34. An element ˛ of t is real root of g with respect to t if ˛ ¤ 0 and
there exists a nonzero element X of g such that
ŒH; X D i h˛; Hi X
for all H 2 t: For each real root ˛; we consider also the associated real coroot H˛
given by
H˛ D 2
˛
h˛; ˛i:
When working with the group K and its Lie algebra k; the use of real roots (and,
later, real weights for representations) is convenient because it makes the factors of
i explicit, rather than hiding them in the fact that the roots live in it: If, for example,
we wish to compute the complex conjugate of the expression eih˛;Hi; where ˛ is a
real root and H is in t; the explicit factor of i makes it obvious that the conjugate is
eih˛;Hi:

334
11
Compact Lie Groups and Maximal Tori
If our compact group K is simply connected, then g WD kC is semisimple, by
Proposition 7.7. In general, k decomposes as k D k1˚z where z is the center of k and
where g1 WD .k1/C is semisimple. (See the proof of Proposition 7.6.) Furthermore,
as in the proof of Theorem 7.35, every maximal commutative subalgebra t of k will
be of the form t1 ˚ z; where t1 is a maximal commutative subalgebra of k1: All
the results from Chapter 7 then apply—with slight modiﬁcations to account for the
use of real roots—except that the roots may not span t: Nevertheless, the roots form
a root system in their span, namely the space t1. Throughout the section, we will
let R  t denote the set of real roots, 
 a ﬁxed base for R; and RC denote the
associated set of positive (real) roots.
Now that we have introduced the (real) roots for K; it makes sense to compare
the Weyl group in the compact group sense (the group N.T /=T ) to the Weyl group
in the Lie algebra sense (the group generated by reﬂections about the hyperplanes
perpendicular to the roots). As it turns out, these two groups are isomorphic. It is
not hard to show that for each reﬂection there is an associated element of the Weyl
group. The harder part of the proof is to show that these elements generate all of
N.T /=T: This last claim is proved by making a clever use of the torus theorem.
Proposition 11.35. For each ˛ 2 R; there is an element x in N.T / such that
Adx.H˛/ D H˛
and such that
Adx.H/ D H
for all H 2 t for which h˛; Hi D 0: Thus, the adjoint action of x on t is the
reﬂection s˛ about the hyperplane orthogonal to ˛:
Proof. Choose X˛ and Y˛ as in Theorem 7.19, with Y˛ D X
˛ : Then .X˛  Y˛/ D
.X˛  Y˛/; from which it follows that X˛  Y˛ 2 k: Let us deﬁne x 2 K by
x D exp
h
2 .X˛  Y˛/
i
(where  is the number 3:14    ; not a representation). Then by the relationship
between Ad and ad (Proposition 3.35), we have
Adx.H/ D exp
h
2 .adX˛  adY˛/
i
.H/
(11.18)
for all H 2 t: If h˛; Hi D 0; then .adX˛  adY˛/.H/ D 0; so that AdX.H/ D H:
Consider, then, the case H D H˛: In that case, the entire calculation on the
right-hand side of (11.18) taking place in the subalgebra s˛ D hX˛; Y˛; H˛i of g: In
s˛; the elements X˛  Y˛; iX˛ C iY˛ C H˛; and iX˛ C iY˛  H˛ are eigenvectors
for adX˛  adY˛ with eigenvalues 0; 2i; and 2i; respectively. Since H˛ is half the
difference of the last two vectors, we have

11.7
Roots and the Structure of the Weyl Group
335
exp
h
2 .adX˛  adY˛/
i
.H˛/
D ei.iX˛ C iY˛ C H˛/=2  ei.iX˛ C iY˛  H˛/=2
D H˛:
(11.19)
Thus, Adx maps H˛ to H˛ and is the identity on the orthogonal complement of ˛:
ut
See Exercise 10 for an alternative approach to verifying (11.19). We now proceed
to show that the Weyl group is generated by the reﬂections s˛; ˛ 2 R: We let Z.T /
denote the centralizer of T; that is
Z.T / D fx 2 Kj xt D tx; for all t 2 T g :
Theorem 11.36. If T is a maximal torus in K; the following results hold.
1. Z.T / D T:
2. The Weyl group acts effectively on t and this action is generated by the reﬂections
s˛; ˛ 2 R; in Proposition 11.35.
Since Z.T / D T; it follows that T is a maximal commutative subgroup of T
(i.e., there is no commutative subgroup of K properly containing T ). Nevertheless,
there may exist maximal commutative subgroups of K that are not maximal tori;
see Exercise 5.
It is not hard to verify Theorem 11.36 directly in the case of SU.n/; see
Exercise 3. The following lemma is the key technical result in the proof of
Theorem 11.36 in general.
Lemma 11.37. Suppose S is a connected, commutative subgroup of K: If x belongs
to Z.S/; then there is a maximal torus S0 containing both S and x:
Proof. Let x be in Z.S/, let B be the subgroup of K generated by S and x; and let
NB be the closure of B: We are going to show that there is an element y of NB such
that the group generated by y is dense in NB: The torus theorem will then tell us that
there is a maximal torus S0 containing y and, thus, both S and x:
Since NB is compact and commutative, Theorem 11.2 implies that the identity
component NB0 of NB is a torus. Since NB is compact, it has only ﬁnitely many
components (Exercise 15 in Chapter 3), which means that the quotient group NB= NB0
is ﬁnite.
Now, every element y of NB is the limit of sequence of the form xnksk for some
integers nk and elements sk 2 S: Thus, for some large k; the element xnksk will
be in the same component of NB as y: But since S is connected, xnk must also be
in the same component of NB as y: It follows that Œy and Œxnk represent the same
element of the quotient group NB= NB0: We conclude, then, that NB= NB0 is a cyclic group
generated by Œx: Since also NB= NB0 is ﬁnite, it must be isomorphic to Z=m for some
positive integer m:
It follows that xm belongs to the torus NB0. Choose some t 2 NB0 such that the
subgroup generated by t is dense in NB0(Proposition 11.4), and choose g 2 NB0 so that

336
11
Compact Lie Groups and Maximal Tori
gm D xmt. (Since the exponential map for the torus NB0 is surjective, xmt 2 NB0
has an mth root in NB0:) Now set y D gx; so that y is in the same component of NB
as x: Since NB is commutative, we have
ym D gmxm D t;
which means that the set of elements of the form ynm D tn is dense in NB0: Now,
since NB= NB0 is cyclic with generator Œx; each component of NB is of the form xk NB0
for some k: Furthermore, the set of elements of the form ynmCk D xktn is dense in
xk NB0.
We see, then, that the group generated by y contains a dense subset of each
component of NB and is, thus, dense in NB: By the torus theorem, there is a maximal
torus S0 that contains y. It follows that S0 must contain NB and, in particular, both S
and x:
ut
Figure 11.6 illustrates the proof of Lemma 11.37 in the case where NB= NB0 is
cyclic of order 5: We choose y in the same component of NB as x in such a way that
t WD y5 generates a dense subgroup of NB0: Then y generates a dense subgroup of
the whole group NB:
y2
y3
y4
y5 t
y
B
x
Fig. 11.6 The element y generates a dense subgroup of NB

11.7
Roots and the Structure of the Weyl Group
337
Proof of Point 1 of Theorem 11.36. If we apply Lemma 11.37 with S D T; we see
that any element x of Z.T / must be contained in a maximal torus S0 that contains
T: But since T is also maximal, we must have S0 D T; so that x 2 T:
ut
Lemma 11.38. Suppose C is the fundamental Weyl chamber with respect to 
 and
that w is an element of W that maps C to C: Then w D 1:
For any w 2 W; the action of W constitutes a symmetry of the root system R;
that is, an orthogonal linear transformation that maps R to itself. In some cases,
such as the root system B2; there is no nontrivial symmetry of R that maps C to C:
In the case of A2; on the other hand, there is a nontrivial symmetry of R that maps
C to C; namely the unique linear transformation that interchanges ˛1 and ˛2: (This
map is just the reﬂection about the line through the root ˛3 D ˛1 C ˛2.) The lemma
asserts that although this map is a symmetry of W; it is not given by the action of
a Weyl group element. In the A2 case, of course, we have an explicit description of
the Weyl group, and we can easily check that there is no w 2 W with w  ˛1 D ˛2
and w  ˛2 D ˛1: (See Figure 11.7. where the Weyl group is the symmetry group of
the indicated triangle.) Nevertheless, we need an argument that works in the general
case.
The idea of the proof is as follows. We want to show that if x 2 N.T / and
Adx.C/  C; then x 2 T . The idea is to show that x must commute with some
nonzero H 2 t; and that this H can be chosen to be "nice." If H could be chosen
so that the group exp.tH/ were dense in T; then x would have to commute with
C
3
2
1
Fig. 11.7 The reﬂection about the line through ˛3 is a symmetry of R that maps C to itself, but
that is not an element of W

338
11
Compact Lie Groups and Maximal Tori
every element of T; so that x would belong to Z.T / D T . Although we cannot, in
general, choose H to be as nice as that, we can choose H to be in the interior of C;
which means that h˛; Hi ¤ 0 for all ˛ 2 R: This turns out to be sufﬁcient to show
that x 2 T:
Proof. Let x 2 K be a representative of the element w 2 W: Take any H0 in the
interior of C and average H0 over the action of the (ﬁnite) subgroup of W generated
by w: The resulting vector H is still in the interior of C (which is convex) and is
now ﬁxed by w; meaning that Adx.H/ D H: Thus, x commutes with every element
of the one-parameter subgroup S WD fexp.tH/jt 2 Rg: By Lemma 11.37, there is a
maximal torus S0 containing x and S: Suppose, toward a contradiction, that x is not
in T: Then S0 cannot equal T; and, since S0 is maximal, S0 cannot be contained in
T: Thus, there is some X 2 k that is in the Lie algebra s0 of S0 but not in t; and this
X commutes with exp.tH/; t 2 R; and hence with H:
On the other hand, suppose we decompose X 2 k  g as a sum of an element
of h and elements of the various root spaces g˛: Now, ŒH; X D 0 and h˛; Hi is
nonzero for all ˛; since H is in the interior of C: Thus, the component of X in
each g˛ must be zero, meaning that X 2 h \ k D t; which is a contradiction. Thus,
actually, x must be in T; which means that w is the identity element of W:
ut
Proof of Point 2 of Theorem 11.36. We let W 0  W be the group generated by
reﬂections. By Proposition 8.23, W 0 acts transitively on the set of Weyl chambers.
Thus, for any w 2 W; we can ﬁnd w0 2 W 0 mapping W.C/ back to C; so that w0w
maps C to C: Then by Lemma 11.38, we have that w0w D 1; which means that
w D .w0/1 belongs to W 0: Thus, every element of W actually belongs to W 0:
ut
Theorem 11.39. If two elements s and t of T are conjugate in K; then there exists
an element w of W such that w  s D t:
We may restate the theorem equivalently as follows: If t D xsx1 for some x 2
K, then t can also be expressed as t D ysy1 for some y 2 N.T /:
Proof. Suppose s and t are in T and t D xsx1 for some x 2 K: Let Z.t/ be
the centralizer of t. Since, xux1 commutes with t D xsx1 for all u 2 T , we see
that xTx1  Z.t/: Thus, both T and xTx1 are tori in Z.t/: Actually, since T and
xTx1 are connected, they must be contained in the identity component Z.t/0 of
Z.t/: Furthermore, since T and xTx1 are maximal tori in K; they must be maximal
tori in Z.t/0: Thus, we may apply the torus theorem to Z.t/0 to conclude that there
is some z 2 Z.t/0 such that
zxTx1z1 D T:
(11.20)
Now, (11.20) says that zx is in the normalizer N.T / of T: Furthermore, since
z 2 Z.t/0 commutes with t; we have
.zx/s.zx/1 D z.xsx1/z1 D ztz1 D t:
Thus, y WD zx is the desired element of N.T / such that t D ysy1:
ut

11.8
Exercises
339
Corollary 11.40. If f is a continuous Weyl-invariant function on T; then f extends
uniquely to a continuous class function on K:
Proof. By the torus theorem, each conjugacy class in K intersects T in at least one
point. By Theorem 11.39, each conjugacy class intersects T in a single orbit of W:
Thus, if f is a W -invariant function on T; we can unambiguously (and uniquely)
extend f to a class function F on K by making F constant on each conjugacy class.
It remains to show that the extended function F is continuous on K: Suppose,
then that hxni is a sequence in K converging to some x: We can write each xn as
xn D yntny1
n ; with yn 2 K and tn 2 T: Since both K and T are compact, we
can—after passing to a subsequence—assume that yn converges to some y 2 K
and tn converges to some t 2 T: It follows that
x D lim
n!1 xn D lim
n!1 yntny1
n
D yty1:
Now, by our construction of F , we have F.xn/ D f .tn/ and F.x/ D f .t/: Thus,
since f is assumed to be continuous on T; we see that
F.xn/ D f .tn/ ! f .t/ D F.x/;
showing that F is continuous on K:
ut
11.8
Exercises
1. Let  denote the set of all vectors in R2 that can be expressed in the form
a.1; 1/ C b.3; 1/ C c.2; 4/;
for a; b; c 2 Z: Then  is a subgroup of R2 and  is discrete, since it is
contained in Z2: Find linearly independent vectors v1 and v2 in  such that
 consists precisely of the set of integer linear combinations of v1 and v2:
(Compare Lemma 11.3.)
2. (a) Show that every continuous homomorphism from S1 to S1 is of the form
u 7! um for some m 2 Z:
Hint: Use Theorem 3.28.
(b) Show that every continuous homomorphism from .S1/k ! S1 is of the
form
.u1; : : : ; uk/ 7! um1
1    umk
k
for some integers m1; : : : ; mk:
3. Consider the group SU.n/; with maximal torus T being the intersection of
SU.n/ with the space of diagonal matrices. Prove directly (without appealing

340
11
Compact Lie Groups and Maximal Tori
to Theorem 11.36) that Z.T / D T and that the N.T /=T is isomorphic to the
Weyl group of the root system An1: (Compare Sect. 7.7.1.)
Hint: Imitate the calculations in Sect. 6.6.
4. Give an example of closed, oriented manifolds M and N of the same dimension
and a smooth map f W M ! N such that f has mapping degree zero but f is,
nevertheless, surjective.
5. Let K D SO.n/; where n  3: Let H be the (commutative) subgroup of K
consisting of the diagonal matrices in SO.n/: (Of course, the diagonal entries
have to be ˙1 and the number of diagonal entries equal to 1 must be even.)
Show that H is a maximal commutative subgroup of K and that H is not
contained in a maximal torus.
Hint: Use Proposition A.2.
Note: This example shows that in Lemma 11.37, the assumption that S be
connected cannot be omitted. (Otherwise, we could take S D H and x D I
and we would conclude that there is a maximal torus S0 containing H.)
6. Suppose K D SU.n/ and H is any commutative subgroup of K: Show that H
is conjugate to a subgroup of the diagonal subgroup of K and thus that H is
contained in a maximal torus. This result should be contrasted with the result
of Exercise 5.
7. (a) For any interval .a; b/  R and any x; y 2 .a; b/; show that there exists a
smooth family of diffeomorphisms ft W .a; b/ ! .a; b/; 0  t  1; with
the following properties. First, f0.z/ D z for all z: Second, there is some
" > 0 such that ft.z/ D z for all z 2 .a; a C "/ and for all z 2 .b  "; b/:
Third, f1.x/ D y:
Hint: Take
ft.z/ D z C t
Z z
a
g.u/ du
for some carefully chosen function g:
(b) If R  Rn is an open rectangle and x and y belong to R; show that there
is a smooth family of diffeomorphisms ‰t W R ! R such that (1) ‰0 is the
identity map, (2) each ‰t is the identity in a neighborhood of @R; and (3)
‰1.x/ D y:
8. (a) Show that the left action of G on G=H is smooth with respect to
the collection of coordinate patches on G=H described in the proof of
Theorem 11.22.
(b) Show that the kernel of the differential of the quotient map Q W G ! G=H
at the identity is precisely h:
9. According to Exercise 5 in Chapter 1, each element of SU.2/ can be written
uniquely as
U D
 ˛  Nˇ
ˇ
N˛

;

11.8
Exercises
341
where .˛; ˇ/ 2 C2 belongs to the unit sphere S3: For each U 2 SU.2/; let vU
denote the corresponding unit vector .˛; ˇ/: Now, the angle  between .˛; ˇ/ 2
S3 and the "north pole" .1; 0/ satisﬁes
cos  D Re.h.˛; ˇ/; .1; 0/i/ D Re.˛/;
and this relation uniquely determines ; if we take 0    : In spherical
coordinates on S3; the angle  is the polar angle.
(a) Suppose U 2 SU.2/ has eigenvalues ei and ei. Show that
Re.hvU ; .1; 0/i/ D cos :
Hint: Use the trace.
(b) Conclude that U1 and U2 are conjugate in SU.2/ if and only if vU1 and vU2
have the same polar angle.
10. In this exercise, we give an alternative veriﬁcation of the identity (11.19). Since
the left-hand side of (11.19) is expressed in purely Lie-algebraic terms, we may
do the calculation in any Lie algebra isomorphic to hX˛; Y˛; H˛i ; for example,
in sl.2I C/ itself. That is to say, it sufﬁces to prove the formula with X˛ D
X; Y˛ D Y; and H˛ D H; where X; Y; and H are the usual basis elements for
sl.2I C/:
Show that
exp
h
2 .adX  adY /
i
.H/ D e

2 .XY /He 
2 .XY / D H;
as claimed.

Chapter 12
The Compact Group Approach
to Representation Theory
In this chapter, we follow Hermann Weyl's original approach to establishing the
Weyl character formula and the theorem of the highest weight. Throughout the
chapter, we assume K is a connected, compact matrix Lie group, with Lie algebra
k: Throughout the chapter, we let T denote a ﬁxed maximal torus in K and we let
t denote the Lie algebra of T: We let R denote the set of real roots for k relative
to t (Deﬁnition 11.34), we let 
 be a ﬁxed base for R; and we let RC denote the
positive real roots relative to R: We also let W WD N.T /=T denote the Weyl group
for K relative to T: In light of Theorem 11.36, W is isomorphic to the subgroup of
O.t/ generated by the reﬂections about the hyperplanes orthogonal to the roots.
12.1
Representations
All representations of K considered in this chapter are assumed to be ﬁnite
dimensional and deﬁned on a vector space over C; unless otherwise stated. Although
we are studying in this chapter the representations of the compact group K; it is
convenient to describe the weights of such a representation in terms of the associated
representation of g D kC: Since we are using real roots for g, we will also use real
weights for representations of K:
Deﬁnition 12.1. Let ....; V / be a ﬁnite-dimensional representation of K and  the
associated representation of g: An element  of t is called a real weight of V is
there exists a nonzero element v of V such that
.H/v D i h; Hi v
(12.1)
for all H
2 t: The weight space with weight  is the set of all v
2 V
satisfying (12.1) and the multiplicity of  is the dimension of the corresponding
weight space.
© Springer International Publishing Switzerland 2015
B. Hall, Lie Groups, Lie Algebras, and Representations, Graduate
Texts in Mathematics 222, DOI 10.1007/978-3-319-13467-3_12
343

344
12
The Compact Group Approach to Representation Theory
We now consider some elementary properties of the weights of a representation.
Proposition 12.2. If ....; V / is a ﬁnite-dimensional representation of K; the real
weights for ... and their multiplicities are invariant under the action of the Weyl
group.
Proof. Following the proof of Theorem 6.22, we can show that if w 2 W is
represented by x 2 N.T /; then ....x/ will map the weight space with weight 
isomorphically onto the weight space with weight w  :
ut
The weights of representation of K satisfy an integrality condition that does not,
in general, coincide with the notion of integrality in Deﬁnition 8.34.
Deﬁnition 12.3. Let  be the subset of t given by
 D
˚
H 2 t
ˇˇe2H D I

:
We refer to  as the kernel of the exponential map for t:
The set  should, more precisely, be referred to as the kernel of the exponential
map scaled by a factor of 2: Note that if w 2 W is represented by x 2 N.T /; then
for all H 2 ; we have
e2wH D e2xHx1 D xe2Hx1 D I:
Thus,  is invariant under the action of W on t:
Deﬁnition 12.4. An element of  of t is an analytically integral element if
h; Hi 2 Z
for all H in : An element  of t is an algebraically integral element if
h; H˛i D 2h; ˛i
h˛; ˛i 2 Z
for each real root ˛: An element  of t is dominant if
h; ˛i  0
for all ˛ 2 
: Finally, if 
 D f˛1; : : : ; ˛rg and  and  are two elements of t; we
say that  is higher than  if
   D c1˛1 C    C cr˛r
with cj  0: We denote this relation by   :

12.1
Representations
345
The notion of an algebraically integral element is essentially the one we used in
Chapters 8 and 9 in the context of semisimple Lie algebras. Speciﬁcally, if g WD kC
is semisimple, then  2 t is algebraically integral if and only if i is integral in the
sense of Deﬁnition 8.34. We will see in Sect. 12.2 that every algebraically integral
element is analytically integral, but not vice versa. In Chapter 13, we will show that
when K is simply connected, the two notions of integrality coincide.
Proposition 12.5. Let .†; V / be a representation of K and let 	 be the associated
representation of k: If  2 t is a real weight of 	; then  is an analytically integral
element.
Proof. If v is a weight vector with weight  and H is an element of ; then on the
one hand,
†.e2H/v D Iv D v;
while on the other hand,
†.e2H /v D e2	.H/v D e2ih;Hiv:
Thus, e2ih;Hi D 1, which implies that h; Hi 2 Z:
ut
We are now ready to state the theorem of the highest weight for (ﬁnite-
dimensional) representations of a connected compact group.
Theorem 12.6 (Theorem of the Highest Weight). If K is a connected, compact
matrix Lie group and T is a maximal torus in K; the following results hold.
1. Every irreducible representation of K has a highest weight.
2. Two irreducible representations of K with the same highest weight are
isomorphic.
3. The highest weight of each irreducible representation of K is dominant and
analytically integral.
4. If  is a dominant, analytically integral element, there exists an irreducible
representation of K with highest weight :
Let us suppose now that g WD kC is semisimple. Even in this case, the set of
analytically integral elements may not coincide with the set of algebraically integral
elements, as we will see in several examples in Sect. 12.2. Thus, the theorem of
the highest weight for g (Theorems 9.4 and 9.5) will, in general, have a different
set of possible highest weights than the theorem of the highest weight for K: This
discrepancy arises because a representation of g may not come from a representation
of K; unless K is simply connected. In the simply connected case, there is no
such discrepancy; according to Corollary 13.20, when K is simply connected, every
algebraically integral element is analytically integral.
We will develop the tools for proving Theorem 12.6 in Sects. 12.2-12.4, with the
proof itself coming in Sect. 12.5. The hard part of the theorem is Point 4; this will
be established by appealing to a completeness result for characters.

346
12
The Compact Group Approach to Representation Theory
12.2
Analytically Integral Elements
In this section, we establish some elementary properties of the set of analytically
integral elements (Deﬁnition 12.4) and consider several examples. One additional
key result that we will establish in Chapter 13 is Corollary 13.20, which says that
when K is simply connected, the set of analytically integral elements coincides with
the set of algebraically integral elements.
Proposition 12.7.
1. The set of analytically integral elements is invariant under the action of the Weyl
group.
2. Every analytically integral element is algebraically integral.
3. Every real root is analytically integral.
We begin with an important lemma.
Lemma 12.8. If ˛ 2 t is a real root and H˛ D 2˛= h˛; ˛i is the associated real
coroot, we have
e2H˛ D I
in K: That is to say, H˛ belongs to the kernel  of the exponential map.
Proof. By Corollary 7.20, there is a Lie algebra homomorphism  W su.2/ ! k
such that the element iH D diag.i; i/ in su.2/ maps to the real coroot H˛: (In
the notation of the corollary, H˛ D 2E˛
1 :) Since SU.2/ is simply connected, there
is (Theorem 5.6) a homomorphism ˆ W SU.2/ ! K for which the associated Lie
algebra homomorphism is : Now, the element iH 2 su.2/ satisﬁes e2iH D I, and
thus
I D ˆ.e2iH/ D e2.iH/ D e2H˛;
as claimed.
ut
Proof of Proposition 12.7. For Point 1, we have already shown (following Deﬁ-
nition 12.3) that  is invariant under the action of W: Thus, if  is analytically
integral and w is the Weyl group element represented by x; we have hw  ; Hi D
˝
; w1  H
˛
2 Z; since w1  H is in : For Point 2, note that for each ˛; we
have H˛ 2  by Lemma 12.8. Thus, if  is analytically integral, h; H˛i 2 Z;
showing that  is algebraically integral. For Point 3, we note that the real roots are
real weights of the adjoint representation, which is a representation of the group K,
and not just its Lie algebra. Thus, by Proposition 12.5, the real roots are analytically
integral.
ut
Proposition 12.9. If  is an analytically integral element, there is a well-deﬁned
function f W T ! C such that
f.eH/ D eih;Hi
(12.2)

12.2
Analytically Integral Elements
347
for all H 2 t: Conversely, for any  2 t; if there is a well-deﬁned function on T
given by the right-hand side of (12.2), then  must be analytically integral.
Proof. Replacing H by 2H; we can equivalently write (12.2) as
f.e2H / D e2ih;Hi;
H 2 t:
(12.3)
Now, since T is connected and commutative, every t 2 T can be written as t D
e2H for some H 2 t: Furthermore, e2.HCH 0/ D e2H if and only if e2H 0 D I;
that is, if and only if H 0 2 : Thus, the right-hand side of (12.3) deﬁnes a function
on T if and only if e2ih;HCH 0i D e2ih;Hi for all H 0 2 : This happens if and
only if e2ih;H 0i D 1 for all H 0 2 ; that is, if and only if h; H 0i 2 Z for all
H 0 2 :
ut
Proposition 12.10. The exponentials f in (12.2), as  ranges over the set of
analytically integral elements, are orthonormal with respect to the normalized
volume form dt on T :
Z
T
f.t/f0.t/ dt D ı;0:
(12.4)
Proof. Let us identify T with .S1/k for some k; so that t is identiﬁed with Rk and
the scaled exponential map is given by
.1; : : : ; n/ 7! .e2i1; : : : ; e2ik/:
The kernel  of the exponential is the integer lattice inside Rk: The lattice of
analytically integral elements (points having integer inner product with each element
of ) is then also the integer lattice. Thus, the exponentials in the proposition are
the functions of the form
f.ei1; : : : ; eik/ D ei11    eikk;
with  D .1; : : : ; k/ 2 Zk:
Meanwhile, if we use the coordinates 1; : : : ; k on T , then any k-form on T can
be represented as a density  times d1^  ^dn: Since the volume form dt on T is
translation invariant, the density  must be constant. Thus, the normalized integral
in (12.4) becomes
Z
T
f.t/f0.t/ dt
D .2/k
Z 2
0
  
Z 2
0
ei11    eikkei0
11    ei0
kk d1    dk
D .2/k
Z 2
0
ei.0
11/1d1

  
Z 2
0
ei.0
kk/kdk

:
The claimed orthonormality then follows from direct computation.
ut

348
12
The Compact Group Approach to Representation Theory
Fig. 12.1 Dominant, analytically integral elements (black dots) and dominant, algebraically
integral elements (black and white dots) for SO.3/
We now calculate the algebraically integral and analytically integral elements
in several examples, with an eye toward clarifying the distinction between the
two notions. When K is simply connected, Corollary 13.20 shows that the set
of analytically integral and algebraically integral elements coincide. Thus, in the
simply connected case, the calculations in Sects. 6.7 and 8.7 provide examples of
the set of analytically integral elements. We consider now three groups that are not
simply connected.
Example 12.11. Consider the group SO.3/ and let t be the maximal commutative
subalgebra spanned by the element F3 in Example 3.27. Let the unique positive root
˛ be chosen so that h˛; F3i D 1: Then  2 t is dominant and algebraically integral
if and only if  D m˛=2; where m is a non-negative integer, and  2 t is dominant
and analytically integral if and only if  D m˛; where m is a non-negative integer.
See Figure 12.1. Note that in this case, ı (half the sum of the positive roots) is
equal to ˛=2; which is not an analytically integral element.
Proof. Following Sect. 7.7, but adjusting for our current convention of using real
roots (Deﬁnition 11.34), we identify t with R by mapping aF3 to a: The roots are
then the numbers ˙1; where we take 1 as our positive root ˛, so that h˛; F3i D 1:
Then  2 t Š R is dominant if and only if   0: Furthermore,  is algebraically
integral if and only if
2 h; ˛i D 2./.1/ 2 Z;
that is, if and only if 2 is an integer. Thus, the dominant, algebraically integral
elements are the numbers of the form m=2 D m˛=2:
Now, e2aF3 D I if and only if a is an integer. Thus,  is analytically integral if
and only if ./.a/ 2 Z for all a 2 Z; that is, if and only if  is an integer. Thus, the
dominant, analytically integral elements are the numbers of the form m D m˛:
ut
We consider next the group U.2/, with t consisting of diagonal matrices with
pure imaginary diagonal entries. We identify t with R2 by mapping diag.ia; ib/ to
.a; b/: The roots are then the elements of the form .1; 1/ and .1; 1/; and we
select ˛ WD .1; 1/ as our positive root. We now decompose every H 2  as a linear
combination of the vectors
˛ D .1; 1/I
ˇ D .1; 1/:
(12.5)

12.2
Analytically Integral Elements
349
Fig. 12.2 The dominant, analytically integral elements (black dots) and nondominant, analytically
integral elements (white dots) for U.2/: The vertical lines indicate the algebraically integral
elements
Example 12.12. Let t be the diagonal subalgebra of u.2/, and write every element
 2 t as
 D c˛ C dˇ;
with ˛ and ˇ as in (12.5). Then  is analytically integral if and only if either c and d
are both integers or c and d are both of the form integer plus one-half. Furthermore,
 is dominant if and only if c  0: On the other hand,  is algebraically integral if
and only if c is either an integer or an integer plus one-half.
In Figure 12.2, the black dots are the dominant, analytically integral elements and
the white dots are the nondominant, analytically integral elements. All the points in
the vertical lines are algebraically integral.
Proof. If H D diag.ia; ib/; then e2iH D I if and only if a and b are both integers.
Thus, when we identify t with R2; the kernel  of the exponential corresponds to the
integer lattice Z2: The lattice of analytically integral elements is then also identiﬁed
with Z2: Now, it is straightforward to check that c˛Cdˇ is in Z2 if and only if either
c and d are both integers or c and d are both integers plus one-half, accounting for
the claimed form of the analytically integral elements. Since ˇ is orthogonal to ˛;
an element  D c˛Cdˇ has non-negative inner product with ˛ if and only if c  0;
accounting for the claimed condition for  to be dominant. Finally,  is algebraically
integral if and only if 2 h; ˛i = h˛; ˛i is an integer, which happens if c is either an
integer or an integer plus one-half, with no restriction on d:
ut

350
12
The Compact Group Approach to Representation Theory
1
2
Fig. 12.3 Dominant, analytically integral elements (black dots) and dominant, algebraically
integral elements (black and white dots) for SO.5/
Example 12.13. The dominant, analytically integral elements for SO.5/ are as
shown in Figure 12.3.
The ﬁgure shows the dominant, analytically integral elements (black dots). The
white dots are dominant, algebraically integral elements that are not analytically
integral. The background square lattice is the set of all analytically integral elements.
Note that in Figure 12.3, the B2 root system is rotated by =4 compared to
Figure 8.11. If we rotate Figure 12.3 clockwise by =4 and then reﬂect across the
x-axis, the set of dominant algebraically integral elements in Figure 12.3 (black and
white dots) will match the set of "dominant integral" elements in Figure 8.11. Note
that ı (half the sum of the positive roots) is not analytically integral.
Proof. Elements of t are of the form
H D
0
BBBBB@
0 a
a 0
0 b
b 0
0
1
CCCCCA
;
(12.6)
with a; b 2 R: Following Sect. 7.7, but adjusting for our current convention of using
real roots, we identify t with R2 by means of the map H 7! .a; b/: The roots are
then the elements .˙1; 0/; .0; ˙1/; and .˙1; ˙1/: As a base, we take ˛1 D .1; 1/

12.3
Orthonormality and Completeness for Characters
351
and ˛2 D .0; 1/: Furthermore, .x; y/ is algebraically integral if
2h.1; 1/; .x; y/i
2
D x  y 2 Z
and
2h.0; 1/; .x; y/i
1
D 2y 2 Z:
These conditions hold if and only if either x and y are both integers or x and y are
both of the form "integer plus one-half."
Now, if H is as in (12.6), e2H D I if and only if a and b are integers. Thus,
under our identiﬁcation of t with R2; the kernel of the exponential map is the set of
elements of the form .a; b/; with a; b 2 Z: Thus, .x; y/ is analytically integral if
h.a; b/; .x; y/i D ax C by 2 Z
for all a; b 2 Z: This condition holds if and only if x and y are both integers.
Finally, .x; y/ is dominant if it has non-negative inner product with each of ˛1
and ˛2; which happens if .x; y/ is in the 45-degree sector indicated by dashed lines
in Figure 12.3.
ut
12.3
Orthonormality and Completeness for Characters
In this section, we show that the characters of irreducible representations form an
orthonormal set and that these characters are complete in the space of continuous
class functions on K:
Deﬁnition 12.14. Suppose ....; V / is representation of K: Then the character of
... is the function ... W K ! C given by
....x/ D trace.....x//:
Note that we now consider the character as a function on the group K; rather than
on the Lie algebra g D kC; as in Chapter 10. If  is the associated representation
of g; then the character  of  (Deﬁnition 10.11) is related to the character ... of
... by
....eH/ D .H/;
H 2 k:
Note that each character is a class function on K:
....yxy1/ D trace.....y/....x/....y/1/ D trace.....x//:

352
12
The Compact Group Approach to Representation Theory
The following theorem says that the characters of irreducible representations form
an orthonormal set in the space of class functions.
Theorem 12.15. If ....; V / and .†; W / are irreducible representations of K; then
Z
K
trace.....x//trace.†.x// dx D
 1 if V Š W
0 if V © W ;
where dx is the normalized left-invariant volume form on K:
If ....; V / is a representation of K; let V K denote the space given by
V K D fv 2 V j....x/v D v for all x 2 K g :
Lemma 12.16. Suppose ....; V / is a ﬁnite-dimensional representation of K; and let
P be the operator on V given by
P D
Z
K
....x/ dx:
Then P is a projection onto V K: That is to say, P maps V into V K and Pv D v for
all v 2 V K:
Clearly, V K is an invariant subspace for ...: If we pick an inner product on V for
which ... is unitary, then .V K/? is also invariant under each ....x/ and thus under P:
But since P maps into V K; the map P must be zero on .V K/?; thus, P is actually
the orthogonal projection onto V K:
Proof. For any y 2 K and v 2 V; we have
....y/Pv D ....y/
Z
K
....x/ dx

v
D
Z
K
....yx/ dx

v
D Pv;
by the left-invariance of the form dx: This shows that Pv belongs to V K: Meanwhile,
if v 2 V K; then
Pv D
Z
K
....x/v dx
D
Z
K
dx

v
D v;
by the normalization of the volume form dx:
ut

12.3
Orthonormality and Completeness for Characters
353
Note that if V is irreducible and nontrivial, then V K D f0g: In this case, the
proposition says that R
K ....x/ dx D 0:
Lemma 12.17. For A W V ! V and B W W ! W; we have
trace.A/trace.B/ D trace.A ˝ B/;
where A ˝ B W V ˝ W ! V ˝ W is as in Proposition 4.16.
Proof. If fvjg and fwlg are bases for V and W; respectively, then fvj ˝ wlg is a
basis for V ˝ W: If Ajk and Blm are the matrices of A and B with respect to fvjg
and fwlg; respectively, then the matrix of A ˝ B with respect to fvj ˝ wlg is easily
seen to be
.A ˝ B/.j;l/.k;m/ D AjkBlm:
Thus,
trace.A ˝ B/ D
X
j;l
AjjBll D trace.A/trace.B/;
as claimed.
ut
Proof of Theorem 12.15. We know that there exists an inner product on V for which
each ....x/ is unitary. Thus,
trace.....x// D trace.....x// D trace.....x1//:
(12.7)
Recall from Sect. 4.3.3 that for any A W V ! V; we have the transpose operator
Atr W V  ! V . Since the matrix of Atr with respect to the dual of any basis
fvjg of V is the transpose of the matrix of A with respect to fvj g; we see that
trace.Atr/ D trace.A/: Thus,
trace.....x// D trace.....x1/tr/ D trace....tr.x//;
where ...tr is the dual representation to .... Thus, the complex conjugate of the
character of ... is the character of the dual representation ...tr of ...:
Using Lemma 12.17, we then obtain
trace.....x//tr.†.x// D trace....tr.x/ ˝ †.x//
D trace.....tr ˝ †/.x//:

354
12
The Compact Group Approach to Representation Theory
By Lemma 12.16, this becomes
Z
K
trace.....x//trace.†.x// dx D
Z
K
trace.....tr ˝ †/.x// dx
D trace
Z
K
....tr ˝ †/.x/ dx

D trace.P /
D dim..V  ˝ W /K/
(12.8)
where P is a projection of V  ˝ W onto .V  ˝ W /K:
Now, for any two ﬁnite-dimensional vector spaces V and W; there is a natural
isomorphism between V  ˝ W and End.V; W /; the space of linear maps from V
to W: This isomorphism is actually an intertwining map of representations, where
x 2 K acts on A 2 End.V; W / by
x  A D †.x/A....x/1:
Finally, under this isomorphism, .V  ˝ W /K maps to the space of intertwining
maps of V to W: (See Exercises 3 and 4 for the proofs of the preceding claims.)
By Schur's lemma, the space of intertwining maps has dimension 1 if V Š W and
dimension 0 otherwise. Thus, (12.8) reduces to the claimed result.
ut
Our next result says that the characters form a complete orthonormal set in the
space of class functions on K:
Theorem 12.18. Suppose f is a continuous class function on K and that for every
ﬁnite-dimensional, irreducible representation ... of K; the function f is orthogonal
to the character of ...:
Z
K
f .x/trace.....x// dx D 0:
Then f is identically zero.
The proof given in this section assumes in an essential way that K is a compact
matrix Lie group. (Of course, we work throughout the book with matrix Lie groups,
but most of the proofs we give extend with minor modiﬁcations to arbitrary Lie
groups.) In Appendix D, we sketch a proof of Theorem 12.18 that does not rely on
the assumption that K is a matrix group. That proof, however, requires a bit more
functional analysis than the proof given in this section.
We now consider a class of functions called matrix entries, that include as a
special case the characters of representations. We will prove a completeness result
for matrix entries and then specialize this result to class functions in order to obtain
completeness for characters.

12.3
Orthonormality and Completeness for Characters
355
Deﬁnition 12.19. If ....; V / is a representation of K and fvjg is a basis for V; the
functions f W K ! C of the form
f .x/ D .....x//jk
(12.9)
are called matrix entries for .... Here .....x//jk denotes the .j; k/ entry of the matrix
of ....x/ in the basis fvjg:
In a slight abuse of notation, we will also call f a matrix entry for ... if f is
expressible as a linear combination of the functions in (12.9):
f .x/ D
X
j;k
cjk.....x//jk:
(12.10)
We may write functions of the form (12.10) in a basis-independent way as
f .x/ D trace.....x/A/;
where A is the operator on V whose matrix in the basis fvj g is Ajk D ckj: (The
sum over k computes the product of ....x/ with A and the sum over j computes the
trace.) Note that if A D I then f is the character of ...:
Lemma 12.20. If ....; V / is an irreducible representation of K; then for each
operator A on V; we have
Z
K
....y/1A....y/ dy D cI
(12.11)
for some constant c.
Proof. Let B denote the operator on the left-hand side of (12.11). By the left-
invariance of the integral, we have
....x/1B....x/ D
Z
K
....yx/A....yx/1 dy
D
Z
K
....y/A....y/1 dy
D B:
Thus, B commutes with each ....x/; which, by Schur's lemma, implies that B is a
multiple of the identity.
ut
We are now ready for the proof of our completeness result for characters.
Proof of Theorem 12.18. Let A denote the space of continuous functions on K
that can be expressed as a linear combination of matrix entries, for some ﬁnite
collection of representations of K: We claim that A satisﬁes the hypotheses of the

356
12
The Compact Group Approach to Representation Theory
complex version of the Stone-Weierstrass theorem, namely, that is an algebra, that it
vanishes nowhere, that it is closed under complex conjugation, and that it separates
points. (See Theorem 7.33 in [Rud1].) First, using Lemma 12.17, we see that the
product of two matrix entries is a matrix entry for the tensor product representation,
which decomposes as a direct sum of irreducibles. Thus, the product of two matrix
entries is expressible as a linear combination of matrix entries, showing that A
is an algebra. Second, the matrix entries of the trivial representation are nonzero
constants, showing that A is nowhere vanishing. Third, by a simple extension
of (12.7), we can show that the complex conjugate of a matrix entry is a matrix
entry for the dual representation. Last, since K is a matrix Lie group, it has, by
deﬁnition, a faithful ﬁnite-dimensional representation. The matrix entries of any
such representation separate points in K.
Thus, the complex version of the Stone-Weierstrass theorem applies to A;
meaning that if f is continuous, we can ﬁnd g 2 A such that g is everywhere
within " of f: If f is a class function, then for all x; y 2 K; we have
ˇˇf .x/  g.yxy1/
ˇˇ D
ˇˇf .yxy1/  g.yxy1/
ˇˇ < ":
Thus, if h is given by
h.x/ D
Z
K
g.yxy1/ dy;
then h will also be everywhere within " of f:
Now, by assumption, g can be represented as
g.x/ D
X
j
trace....j.x/Aj /
for some family of representations ....j; Vj / of K and some operators Aj
2
End.Vj/: We can then easily compute that
h.x/ D
X
j
trace....j.x/Bj /;
where
Bj D
Z
K
....y/1Aj ....y/ dy:
But by Lemma 12.20, each Bj is a multiple of the identity, which means that h is a
linear combination of characters.
We conclude, then, that every continuous class function f can be uniformly
approximated by a sequence of functions hn; where each hn is a linear combinations
of characters. If f is orthogonal to every character, f is orthogonal to each hn:

12.4
The Analytic Proof of the Weyl Character Formula
357
Then, by letting n tend to inﬁnity, we ﬁnd that f is orthogonal to itself, meaning
that R
K jf .x/j2 dx D 0: Since f is continuous, this can only happen if f is
identically zero.
ut
We used the assumption that K is a matrix Lie group to show that the algebra
A separates points in K; which allowed us to prove (using the Stone-Weierstrass
theorem) that A is dense in the space of continuous functions on K: In Appendix D,
we sketch a proof of Theorem 12.18 that does not assume ahead of time that K is a
matrix group.
12.4
The Analytic Proof of the Weyl Character Formula
In order to simplify certain parts of the analysis, we make the following temporary
assumption concerning ı (half the sum of the real, positive roots).
Assumption 12.21 In this section and in Sect. 12.5, we assume that the element ı
is analytically integral.
As we will show in Chapter 13 (Corollary 13.20), ı is analytically integral
whenever K is simply connected. In Sect. 12.6, we describe the modiﬁcations
needed to the arguments when ı is not analytically integral. We will give, in this
section, an an analytic proof of the Weyl character formula, as an alternative to the
algebraic argument in Sect. 10.8. The proof is based on a more-explicit version of the
Weyl integral formula, obtained by computing the weight function .t/ occurring in
Theorem 11.30. We will see that the function  is the square of another function,
which turns out to be our old friend, the Weyl denominator. This computation will
provide a crucial link between the Weyl integral formula and the Weyl character
formula.
If ı denotes half the sum of the positive real roots, then the Weyl denominator
function (Deﬁnition 10.15) takes the form
q.H/ D
X
w2W
det.w/eihwı;Hi;
H 2 t:
(12.12)
By Assumption 12.21 and Proposition 12.9, each exponential eihwı;Hi; w 2 W;
deﬁnes a function on T: Thus, there is a unique function Q W T ! C satisfying
Q.eH/ D q.H/;
H 2 t:
(12.13)
We now state Weyl's formula for the character (Deﬁnition 12.14) ... of ...:
Theorem 12.22 (Weyl Character Formula). Suppose ....; V / is an irreducible
representation of K and that  is a maximal weight for ...: Then  is dominant
and analytically integral, and  is actually the highest weight of ...: Furthermore,
the character of ... is given by the formula

358
12
The Compact Group Approach to Representation Theory
....eH/ D
P
w2W det.w/eihw.Cı/;Hi
q.H/
;
(12.14)
at all points eH
2 T for which q.H/ ¤ 0: In particular, every irreducible
representation has a highest weight that is dominant and analytically integral.
The character formula amounts to saying that Q.t/....t/ is an alternating
sum of exponentials. The right-hand side of (12.14) is the same expression as in
Theorem 10.14, adjusted for our current convention of using real roots and real
weights.
Example 12.23. Let K D SU.2/, let T be the diagonal subgroup, and let ...m be
the irreducible representation for which the largest eigenvalue of m.H/ is m: Then
the character formula for ...m takes the form
...m
 ei
0
0 ei

D sin..m C 1//
sin 
:
This formula may also be obtained from Example 10.13 with a D i:
Proof. If H D diag.1; 1/; then we may choose the unique positive root ˛ to satisfy
h˛; Hi D 2. The highest weight  of a representation then satisﬁes h; Hi D m:
Note that diag.ei; ei/ D eiH, that ı satisﬁes hı; Hi D h˛=2; Hi D 1; and that
W D f1; 1g: Thus, the character formula reads
...m.diag.ei; ei// D ei.mC1/  ei.mC1/
ei  ei
;
which simpliﬁes to the claimed expression.
ut
The hard part of the proof of the character formula is showing that in the product
q.H/....eH/ of the Weyl denominator and a character, no other exponentials occur
besides the ones of the form eihw.Cı/;Hi; w 2 W: Now, Theorem 12.15 tells us that
the norm of ... is 1:
Z
K
j....x/j2 dx D 1:
(12.15)
In the analytic approach, the unwanted exponentials will be ruled out by applying
the Weyl integral formula to show that if any other exponentials did occur, the
integral on the left-hand side of (12.15) would be greater than 1. To make this
argument work, we must work out the Weyl integral formula in a more explicit form.
Proposition 12.24. If ı is analytically integral, the Weyl integral formula takes the
form
Z
K
f .x/ dx D
1
jW j
Z
T
jQ.t/j2
Z
K=T
f .yty1/ dŒy dt;
(12.16)

12.4
The Analytic Proof of the Weyl Character Formula
359
where dx; dŒy; and dt are the normalized volume forms on K; K=T; and T;
respectively, and where Q is as in (12.13).
In the proof of this proposition, we also verify the correctness of the normal-
ization constant in the Weyl integral formula (Theorem 11.30), a point that was
not addressed in Sect. 11.6. (We will address the normalization issue when ı is not
integral in Sect. 12.6.) As we have already indicated above, knowing the correct
normalization constant is an essential part of our analytic proof of the Weyl integral
formula.
Proof. We may extend Adt1 to a complex-linear operator on g D kC; where g
is the direct sum of h WD tC and fC; where fC is the orthogonal complement of
h in g: Meanwhile, g also decomposes as the direct sum of h and the root spaces
g˛: We now claim that each g˛ is orthogonal to h. To see this, choose H 2 t for
which h˛; Hi ¤ 0: Then each H 0 2 h is an eigenvector for adH with eigenvalue 0,
whereas X 2 g˛ is an eigenvector for adH with a nonzero eigenvalue. Since adH is
skew self-adjoint, H 0 and X must be orthogonal. Thus, actually, fC is actually the
direct sum of the g˛'s.
Now, if ˛ is a real root and X belongs to g˛; we have
AdeH .X/ D eadH .X/ D eih˛;HiX
for X 2 g˛: Thus, letting Ad0
eH denote the restriction of AdeH to fC; we have
det.Ad0
eH  I/ D
Y
˛2R
.eih˛;Hi  1/
D
Y
˛2RC
.eih˛;Hi  1/.eih˛;Hi  1/:
Since .ei  1/.ei  1/ D
ˇˇei=2  ei=2ˇˇ2 ; we have
det.AdeH  I/ D
ˇˇˇˇˇˇ
Y
˛2RC
.eih˛;Hi=2  eih˛;Hi=2/
ˇˇˇˇˇˇ
2
D jq.H/j2 :
Thus, the Weyl integral formula (Theorem 11.30) takes the claimed form.
It remains only to verify the correct normalization constant in the Weyl integral
formula. To see that (12.16) is properly normalized, it sufﬁces to consider f  1;
in which case (12.16) says that
1 D
1
jW j
Z
T
jQ.t/j2 dt;
(12.17)

360
12
The Compact Group Approach to Representation Theory
where dt is the normalized volume form on T: Now, by Proposition 8.38, ı belongs
to the open fundamental Weyl chamber, which means (Proposition 8.27) that the
exponentials eihwı;Hi; w 2 W; are all distinct. Proposition 12.10 then tells us that
these exponentials are orthonormal, so that the integral of jQ.t/j2 over T equals
jW j ; verifying (12.17).
ut
We are now ready for our analytic proof of the Weyl character formula.
Proof of Theorem 12.22. Since ... is ﬁnite dimensional, it has only ﬁnitely many
weights. Thus, there is a maximal weight  (i.e., one such that no other weight of
... is higher than /; with multiplicity m  1: We then claim that in the product
q.H/....eH/; the exponential eihCı;Hi occurs only once, by taking the term eihı;Hi
from q and the term meih;Hi from ...: To see this, suppose we take eihwı;Hi from
q and meih;Hi from ...; and that
 C w  ı D  C ı;
so that
 D  C .ı  w  ı/:
But by Proposition 8.42, ı  w  ı and, thus,   : Since  is maximal, we
conclude that  D ; in which case we must also have w D I:
We conclude, then, that eihCı;Hi occurs with multiplicity exactly m in the prod-
uct. Now, ....eH/ is a Weyl-invariant function, while q.H/ is Weyl-alternating. The
product of these two functions is then Weyl alternating. Thus, by Corollary 10.17,
each of the exponentials eihw.Cı/;Hi occurs with multiplicity det.w/m:
We now claim that m D 1 and that the only exponentials in the product q... are
those of the form det.w/eihw.Cı/;Hi: To see this, recall from Theorem 12.15 that
Z
K
j....x/j2 dx D 1:
Thus, Proposition 12.24 (as applied to a class function), we have
Z
T
jQ.t/....t/j2 dt D jW j ;
(12.18)
where jW j is the order of the Weyl group.
Now, the product Q... is a sum of exponentials and those exponentials are
orthonormal on T (Proposition 12.10). Since at least jW j exponentials do occur
in the product, namely those of the form det.w/eihw.Cı/;Hi; these exponentials
make a contribution of m jW j to the integral in (12.18). By orthonormality, any
remaining exponentials would make a positive contribution to the integral. Thus,
the only way (12.18) can hold is if m D 1 and there are no exponentials in Q...
other than those of the form det.w/eihw.Cı/;Hi: Thus, after dividing by Q.t/; we
obtain the Weyl character formula.

12.5
Constructing the Representations
361
Since  is a weight of ...; it must be analytically integral. It remains to show
that  is dominant and that  is the highest weight of ...: If  were not dominant,
there would be some w 2 W for which w   is dominant, in which case (by
Proposition 8.42),  D w1  .w  / would be lower than w  ; contradicting
the maximality of : Meanwhile, if  were not the highest weight of ...; we could
choose a maximal element 0 among the weights of ... that are not lower than ; and
this 0 would be another maximal weight for ...: As we have just shown, 0 would
also have to be dominant, in which case (Proposition 8.29), the Weyl-orbit of 0
would be disjoint from the Weyl-orbit of : But then by the reasoning above, both
the exponentials eihw.0Cı/;Hi and the exponentials eihw.Cı/;Hi would occur with
nonzero multiplicity in the product Q...; which would force the integral in (12.18)
to be larger than jW j :
ut
In Chapter 10, we used the Casimir element and the character formula for
Verma modules to show that any exponential eih;Hi that appears in the product
q.H/.H/ must satisfy jj D j C ıj : This was the key step in proving that
only exponentials of the form  D w  . C ı/ appear in q: In this chapter,
we have instead used the Weyl integral formula to show that if any unwanted
exponentials appeared, the integral of j...j2 over K would be greater than 1, which
would contradict Theorem 12.15.
Proposition 12.25. Two irreducible representations of K with the same highest
weight are isomorphic.
Proof. If ... and † both have highest weight ; then the Weyl character formula
shows that the characters ... and ... must be equal. But if ... and † were not iso-
morphic, Theorem 12.15 would imply that ... and † D ... are orthogonal. Thus,
... would have to be identically zero, which would contradict the normalization
result in Theorem 12.15.
ut
12.5
Constructing the Representations
In our proof of Theorem 12.6, we showed as part of the Weyl character formula
that every irreducible representation of K has a highest weight, and that this
highest weight is dominant and analytically integral. We also showed in Proposi-
tion 12.25 that two irreducible representations of K with the same highest weight
are isomorphic. Thus, in proving Theorem 12.6, it remains only to prove that
every dominant, analytically integral element arises as the highest weight of a
representation. We continue to assume that ı (half the sum of the real, positive roots)
is an analytically integral element. This assumption is lifted in Sect. 12.6.
Suppose now that  is a dominant, analytically integral element. We do not
yet know that  is the highest weight of an irreducible representation of K:
Nevertheless, as we now demonstrate, there is a well-deﬁned function  on K
whose restriction to T is given by the right-hand side of the Weyl character formula.

362
12
The Compact Group Approach to Representation Theory
Lemma 12.26. For each dominant, analytically integral element ; there is a
unique continuous function  W T ! C satisfying
.eH/ D
P
w2W det.w/eihw.Cı/;Hi
q.H/
;
H 2 t;
(12.19)
whenever q.H/ ¤ 0:
In preparation for the proof of this lemma, recall from Lemma 10.28 (adjusted
for using real roots) that the Weyl denominator can be written as
q.H/ WD
X
w2W
det.w/eihwı;Hi D .2i/k Y
˛2RC
sin.h˛; Hi =2/
where k is the number of positive roots. For each ˛ 2 R and each integer n; deﬁne
a hyperplane (not necessarily through the origin) by
V˛;n D fH 2 t jh˛; Hi D 2ng :
(12.20)
Then the zero-set of the Weyl denominator q is the union of all the V˛;n's, as ˛ ranges
over the set of positive real roots and n ranges over the integers. Furthermore, q has
a "simple zero" on each of these hyperplanes. Symmetry properties of the numerator
on the right-hand side of (12.19) will then force this function to be zero each V˛;n,
so that the zeros in the numerator cancel the zeros in the denominator.
Proof. By Assumption 12.21, the element ı is analytically integral, which means
that each of w  . C ı/ and w  ı; w 2 W; is also analytically integral. Thus, by
Proposition 12.9, all of the exponentials involved are well-deﬁned functions on T:
It follows that the right-hand side of (12.19) is a well-deﬁned function T; outside
of the set where the denominator is zero. We now argue that the right-hand side
of (12.19) extends uniquely to a continuous function on t:
Let  .H/; H 2 t; denote the numerator on the right-hand side of (12.19):
 .H/ D
X
w2W
det.w/eihw.Cı/;Hi:
We claim that   vanishes on each of the hyperplanes V˛;n in (12.20). To verify this
claim, note that each weight  D w.Cı/ occurring in   is analytically integral,
and thus also algebraically integral, by Proposition 12.7. Thus, h; H˛i is an integer
for each ˛ 2 R: It follows that each exponential in  —and, thus,   itself—is
invariant under translation by 2H˛:
 .H C 2H˛/ D  .H/
for all H 2 t and all ˛ 2 R:

12.5
Constructing the Representations
363
Meanwhile, the coefﬁcients of det.w/ in the formula for   guarantee that   is
alternating with respect to the action of W . Finally, if H 2 V˛;n, we have
s˛  H D H  2h˛; Hi
h˛; ˛i ˛ D H  2nH˛:
Putting these observations together, we have, for H 2 V˛;n;
 .H/ D  .s˛  H/ D  .H  2nH˛/ D  .H/;
which can happen only if  .H/ D 0: (See Figure 12.4, where the gray lines are
the hyperplanes V˛;n with jnj  2: Compare also Exercise 6.)
Now, for each ˛ and n; let L˛;n W t ! R be given by
L˛;n.H/ D h˛; Hi  2n;
so that the zero-set of L˛;n is precisely V˛;n: Then the function
L˛;n.H/
sin.h˛; Hi =2/
(12.21)
extends to a continuous function on a neighborhood of V˛;n in t: On the other hand,
we have shown that   vanishes on V˛;n for each ˛ and n: Furthermore, since   is
given by a globally convergent power series, it is not hard to prove that   can be
H
V ,1
2 H
s H
Fig. 12.4 The function   changes sign under the reﬂection s˛ and is unchanged under translation
by 2H˛; forcing   to be zero on each V˛;n

364
12
The Compact Group Approach to Representation Theory
expressed as
 .H/ D f .H/L˛;n.H/
for some smooth function f; where f is also given by a globally convergent power
series and where f still vanishes on each hyperplane Vˇ;m with .ˇ; m/ ¤ .˛; n/:
(See Exercise 7.)
Now let H be an arbitrary point in t: Since the hyperplanes V˛;n are disjoint for ˛
ﬁxed and n varying, H is contained in only ﬁnitely many of these hyperplanes, say
V˛1;n1; : : : ; V˛m;nm: Since   vanishes on each of these hyperplanes, we can show
inductively that   can be expressed as
  D L˛1;n1    L˛m;nmg
(12.22)
for some smooth function g: Since the function in (12.21) is nonsingular in a
neighborhood of V˛;n; we conclude that the factors of L˛j ;nj in (12.22) cancel all
the zeros in q.H/; showing that .eH/ D  .H/=q.H/ extends to a continuous
function in a neighborhood of H:
Finally, in local exponential coordinates on T; each point H is contained in at
most ﬁnitely many of the hyperplanes on which q vanishes. Thus, H is a limit of
points on which q ¤ 0; showing the uniqueness of the continuous extension.
ut
Lemma 12.27. Let  be as in Lemma 12.26 and let ˆ W K ! C be the unique
continuous class function on K such that ˆ
ˇˇ
T D  (Corollary 11.40). Then as
 ranges over the set of dominant, analytically integral elements, the functions ˆ
form an orthonormal set:
Z
K
ˆ.x/ˆ0.x/ dx D ı;0:
Note that we do not know, at the moment, that each ˆ is actually the character
of a representation of K: Thus, we cannot appeal to Theorem 12.15.
Proof. Since the denominator in the deﬁnition of  is the Weyl denominator,
Corollary 11.32 to the Weyl integral formula tells us that
Z
K
ˆ.x/ˆ0.x/ dx
D
1
jW j
Z
T
jQ.t/j2 .t/0.t/ dt
D
1
jW j
X
w2W
X
w02W
Z
T

det.w/eihw.Cı/;Hi 
det.w0/eihw0.0Cı/;Hi
dH:
(12.23)
Now, since  C ı and 0 C ı are strictly dominant, W acts freely on these elements.
If  ¤ 0; the W -orbit of  C ı will be disjoint from the W -orbit of 0 C ı: Thus,

12.5
Constructing the Representations
365
the exponentials occurring in  will be disjoint from those in 0; which means,
by Proposition 12.10, that (12.23) is zero. If, on the other hand,  D 0; we have
the norm-squared of jW j distinct, orthonormal exponentials, so that the right-hand
side of (12.23) reduces to 1.
ut
We are now ready to complete the proof of Theorem 12.6, in the case in which the
element ı is analytically integral. It remains only to prove that for each dominant,
analytically integral element ; there is an irreducible representation of K with
highest weight :
Proof of Theorem 12.6. Here is what we know so far about the characters of irre-
ducible representations. First, by Theorem 12.22, the character of each irreducible
representation ... must be equal to ˆ; where  is the highest weight of ...: Second,
by Lemma 12.26, all the ˆ's, with  dominant and analytically integral—whether
or not they are characters of a representation—form an orthonormal set. Last, by
Theorem 12.18, the characters of the irreducible representations form a complete
orthonormal set.
Let 0 be a ﬁxed dominant, analytically integral element and suppose, toward
a contradiction, that there did not exist an irreducible representation with highest
weight 0: Then the function ˆ0 would be a continuous class function on K and
ˆ0 would be orthogonal to the character of every irreducible representation. (After
all, every irreducible character is of the form ˆ; where  is the highest weight
of the representation, and we are assuming that 0 is not the highest weight of
any representation.) But Theorem 12.18 says that a continuous class function that
is orthogonal to every irreducible character must be identically zero. Thus, ˆ0
would have to be the zero function, which is impossible, since
ˇˇˆ0
ˇˇ2 integrates to
1 over K:
ut
We may put the argument in a different way as follows. Theorem 12.18 says
that the characters of irreducible representations form a complete orthonormal set
inside the space of continuous class functions on K: The Weyl character formula,
meanwhile, says that the characters form a subset of the set of ˆ's. Finally, the
Weyl integral formula says that the collection of all ˆ's, with  dominant and
analytically integral, are orthonormal. If there were some such  for which ˆ was
not a character, then the set of characters would be a proper subset of an orthonormal
set, in which case, the characters could not be complete.
Now, the preceding proof of the theorem of the highest weight is not very
constructive, in contrast to the Lie-algebraic proof, in which we gave a direct
construction of each ﬁnite-dimensional representation as a quotient of a Verma
module. If one looks carefully at the proof of Theorem 12.18, however, one sees
a hint of a more direct description of the representations from the compact group
perspective. Speciﬁcally, let C.K/ denote the space of continuous functions on K;
and deﬁne a left and a right action of K on C.X/ as follows. For each x 2 K; deﬁne
Lx and Rx as operators on C.X/ as by
.Lxf /.y/ D f .x1y/

366
12
The Compact Group Approach to Representation Theory
.Rxf /.y/ D f .xy/:
One may easily check that Lxy D LxLy and Rxy D RxRy: Thus, both L and R
deﬁne representations of K acting on the inﬁnite-dimensional space C.K/:
We now show that each irreducible representation of K occurs as a ﬁnite-
dimensional subspace of C.K/ that is invariant under the right action of K: For each
representation irreducible ....; V / of K; we can ﬁx some nonzero vector v0 2 V;
which might, for example, be a highest weight vector. Then for each v 2 V; we can
consider the function fv W K ! C given by
fv.x/ D hv0; ....x/vi :
(12.24)
(The function fv is a special sort of matrix entry for ..., in the sense of the proof
of Theorem 12.18.) It is not hard to check that the map v 7! fv is injective; see
Exercise 8.
Let W denote the space of all functions of the form fv; with v 2 V: We can
compute that
.Rxfv/.y/ D hv0; ....yx/vi D hv0; ....y/.....x/v/i ;
which means that
Rxfv D f....x/v:
Thus, W is a ﬁnite-dimensional invariant subspace for the right action of K on
C.K/: Indeed, the map v 7! fv is a bijective intertwining map between V and W:
Conclusion 12.28. For each irreducible representation ....; V / of K; ﬁx a nonzero
vector v0 2 V and let W denote the subspace of C.X/ consisting of functions of the
form (12.24), with v 2 V: Then W is invariant under the right action of K and is
isomorphic, as a representation of K; to V:
One can pursue this line of analysis further by choosing v0 to be a highest
weight vector and then attempting to describe the space W —without referring to
the representation ...—by means of its behavior under certain differential operators
on K: See Sect. 4.12 of [DK] for more information.
12.6
The Case in Which ı is Not Analytically Integral
For a general connected compact group K; the element ı may not be analytically
integral. (See Sect. 12.2.) If ı is not analytically integral, many of the functions
we have been working with will not be well-deﬁned functions on T: Speciﬁcally,
exponentials of the form eihwı;Hi and eihw.Cı/;Hi; with w 2 W and  a dominant,
analytically integral element, no longer deﬁne functions on T: Fortunately, all of our

12.6
The Case in Which ı is Not Analytically Integral
367
calculations involve products of such exponentials, and these products turn out to be
well-deﬁned functions on T: Thus, all the arguments in the previous three sections
will go through with minor modiﬁcations.
In all cases, the quantity 2ı D P
˛2RC ˛ is analytically integral, by Point 3 of
Proposition 12.7. Thus, for each H 2 t for which e2H D I; the quantity hı; Hi
must either be an integer or a half integer. If ı is not analytically integral, there must
exist some H 2 t with e2H D I for which hı; Hi is a half integer (but not an
integer). With this observation in mind, we make the following deﬁnition.
Deﬁnition 12.29. If ı is not analytically integral, we say that  2 t is half integral
if   ı is analytically integral.
That is to say, the half integral elements are those of the form  D ı C 0; with
0 being analytically integral.
Proposition 12.30. If  and  are half integral, then  C  is analytically integral.
If  is half integral, then  is also half integral and w   is half integral for all
w 2 W:
Proof. If  D ı C 0 and  D ı C 0 are half integral, then  C  D 2ı C 0 C 0 is
analytically integral. If  D ı C 0 is half integral, so is
 D ı  0 D ı  2ı  0:
For each w 2 W; the set w  RC will consist of a certain subset S of the positive
roots, together with the negatives of the roots in RC n S: Thus, w  ı will consist of
half the sum of the elements of S minus half the sum of the elements of RC n S: It
follows that
ı  w  ı D
X
˛2RCnS
˛;
showing that wı is again half integral. (Recall that each root is analytically integral,
by Proposition 12.7.) More generally, if  D ı C 0 is half integral, so is w   D
w  ı C w  0:
ut
Note that exponentials of the form eih;Hi; with  being half integral, do not
descend to functions on H: Our next result says that, nevertheless, the product
of two such exponentials (possibly conjugated) does descend to T: Furthermore,
such exponentials are still "orthonormal on T ," as in Proposition 12.10 in the
integral case.
Proposition 12.31. If  and  are half integral, there is a well-deﬁned function f
on T such that
f .eH / D eih;Hieih;Hi

368
12
The Compact Group Approach to Representation Theory
and
Z
T
eih;Hieih;Hi dH D ı;:
Proof. If  and  are half integral, then  is half integral, so that    is
analytically integral. Thus, by Proposition 12.9, there is a well-deﬁned function
f W T ! C satisfying
f .eH/ D eih;Hieih;Hi D eih;Hi:
Furthermore, if  D ı C 0 and  D ı C 0 are half integral, we have
Z
T
eih;Hieih;Hi dH D
Z
T
eihıC0;HieihıC0;Hi dH
D
Z
T
eih0;Hieih0;Hi dH
D ı0;ı0;
by Proposition 12.10. Since  D  if and only if 0 D ı0; we have the desired
"orthonormality" result.
ut
We now discuss how the results of Sects. 11.6, 12.4, and 12.5 should be modiﬁed
when ı is not analytically integral. In the case of the Weyl integral formula
(Theorem 11.30), the q.H/; H 2 t; does not descend to T when ı is not integral.
That is to say, there is no function Q.t/ on T such that Q.eH/ D q.H/:
Nevertheless, the function jq.H/j2 does descend to T; since jq.H/j2 is a sum of
products of half integral exponentials. The Weyl integral formula, with the same
proof, then holds even if ı is not analytically integral, provided that the expression
jQ.t/j2 is interpreted as the function eH 7! jq.H/j2 : We may then consider the
case f  1 and use Proposition 12.31 to verify the correctness of the normalization
in the Weyl integral formula.
In the case of the Weyl character formula, we claim that the right-hand side
of (12.14) descends to a function on T: To see this, note that we can pull a factor
of eihı;Hi out of each exponential in the numerator and each exponential in the
denominator. After canceling these factors, we are left with exponentials in both
the numerator and denominator that descend to T: Meanwhile, in the proof of
the character formula, although the function Q.t/....t/ is not well deﬁned on T;
the function jQ.t/....t/j2 is well deﬁned. The Weyl integral formula (interpreted
as in the previous paragraph) tells us that the integral of jQ.t/....t/j2 over T is
equal to jW j ; as in the case where ı is analytically integral. If we then apply the
orthonormality result in Proposition 12.31, we see that, just as in the integral case,
the only exponentials present in the product q.H/....eH/ are those in the numerator
of the character formula.

12.7
Exercises
369
Finally, we consider the proof that every dominant, analytically integral element
is the highest weight of a representation. If ı is not analytically integral, then neither
the numerator nor the denominator on the right-hand side of (12.19) descends to
function on T: Nevertheless, the ratio of these functions does descend to T; by the
argument in the preceding paragraph. The argument that  extends to a continuous
function on T then goes through without change. (This argument requires only that
each weight  D w  . C ı/ in   be algebraically integral, which holds even
if ı is not analytically integral, by Proposition 8.38.) Thus, we may apply the half-
integral version of the Weyl integral formula to show that the functions ˆ on K are
orthonormal, as  ranges over the set of dominant, analytically integral elements.
The rest of the argument then proceeds without change.
12.7
Exercises
1. Let K D SU.2/ and let t be the diagonal subalgebra of su.2/: Prove directly that
every algebraically integral element is analytically integral.
Note: Since SU.2/ is simply connected, this claim also follows from the general
result in Corollary 13.20.
2. This exercise asks you to use the theory of Fourier series to give a direct proof of
the completeness result for characters (Theorem 12.18), in the case K D SU.2/:
To this end, suppose f is a continuous class function on SU.2/ that f is
orthogonal to the character of every representation.
(a) Using the explicit form of the Weyl integral formula for SU.2/ (Exam-
ple 11.33) and the explicit form of the characters for SU.2/ (Example 12.23),
show that
Z 

f .diag.ei; ei//.sin / sin..m C 1// d D 0
for every non-negative integer m:
(b) Show that the function  7! f .diag.ei; ei//.sin / is an odd function of
:
(c) Using standard results from the theory of Fourier series, conclude that f
must be identically zero.
3. Suppose ....; V / and .†; W / are representations of a group G; and let
Hom.V; W / denote the space of all linear maps from V to W: Let G act on
Hom.V; W / by
g  A D †.g/A....g/1;
(12.25)
for all g 2 G and A 2 Hom.W; V /: Show that A is an intertwining map of V to
W if and only if g  A D A for all g 2 G:

370
12
The Compact Group Approach to Representation Theory
4. If V and W are ﬁnite-dimensional vector spaces, let ˆ W V ˝W ! Hom.V; W /
be the unique linear map such that for all  2 V  and w 2 W; we have
ˆ. ˝ w/.v/ D .v/w:
(a) Show that ˆ is an isomorphism.
(b) Let ....; V / and .†; W / be representations of a group G, let G act on V  as
in Sect. 4.3.3, and let G act on Hom.V; W / as in (12.25). Show that the map
ˆ W V  ˝ W ! Hom.V; W / in Part (a) is an intertwining map:
5. Suppose f .x/ WD trace.....x/A/ and g.x/ WD trace.†.x/B/ are matrix entries
for nonisomorphic, irreducible representations ... and † of K (Deﬁnition 12.19).
Show that f and g are orthogonal:
Z
K
f .x/g.x/ dx D 0:
Hint: Imitate the proof of Theorem 12.15.
6. Let fV˛;ng denote the collection of hyperplanes in (12.20). If H˛ D 2˛= h˛; ˛i is
the real coroot associated to a real root ˛; show that
V˛;n C mH˛ D V˛;nCm:
7. Let V be a hyperplanein Rn; not necessarily through the origin, and let L W Rn !
R be an afﬁne function whose zero-set is precisely V: Suppose g W Rn ! C is
given by a globally convergent power series in n variables and that g vanishes
on V:
(a) Show that g can be expressed as g D Lh for some function h; where h is
also given by a globally convergent power series.
Hint: Choose a coordinate system y1; : : : ; yn on Rn with origin in L such
that L.y/ D y1:
(b) Suppose g also vanishes on some hyperplane V 0 distinct from V: Show that
the function h in Part (a) vanishes on V 0:
8. Let ....; V / be an irreducible representation of K and let v0 be a nonzero element
of V: For each v 2 V; let fv be the function given in (12.24). Show that if fv is
the zero function, then v D 0:
9. Let K D U.n/ and let T be the diagonal subgroup of K: Show that the density
./ in the Weyl integral formula (Theorem 11.30) can be computed explicitly as
.ei1; : : : ; ein/ D
Y
1	j 	k	n
ˇˇeij  eikˇˇ2 :
Hint: Use Proposition 12.24, as interpreted in Sect. 12.6 in the case where ı is
not necessarily analytically integral.

Chapter 13
Fundamental Groups of Compact Lie Groups
13.1
The Fundamental Group
In this section, we brieﬂy review the notion of the fundamental group of a
topological space. For a more detailed treatment, the reader should consult any
standard book on algebraic topology, such as [Hat, Chapter 1]. Let X be any
path-connected Hausdorff topological space and let x0 be a ﬁxed point in X (the
"basepoint"). We consider loops in X based at x0 (i.e., continuous maps l W Œ0; 1 !
X with the property that l.0/ D l.1/ D x). The choice of the basepoint makes no
substantive difference to the constructions that follow. From now on, "based loop"
will mean "loop based at x0." Ultimately, we are interested in the case that X is a
matrix Lie group.
If l1 and l2 are two based loops, then we deﬁne the concatenation of l1 and l2 to
be the loop l1  l2 given by
l1  l2.t/ D
 l1.2t/;
0  t  1
2
l2.2t  1/;
1
2  t  1;
that is, l1  l2 traverses l1 as t goes from 0 to 1=2 and then traverses l2 as t goes from
1=2 to 1:
Two based loops l1 and l2 are said to be homotopic if one can be "continuously
deformed" into the other. More precisely, this means that there exists a continuous
map A W Œ0; 1  Œ0; 1 ! X such that A.0; t/ D l1.t/ and A.1; t/ D l2.t/ for all
t 2 Œ0; 1 and such that A.s; 0/ D A.s; 1/ D x0 for all s 2 Œ0; 1: One should think
of A.s; t/ as a family of loops parameterized by s: In some cases, we may use the
notation ls.t/ in place of A.s; t/ to emphasize this point of view.
A based loop is said to be null homotopic if it is homotopic to the constant loop
(i.e., the loop l0 for which l0.t/ D x0 for all t 2 Œ0; 1). If all loops in X based at x0
© Springer International Publishing Switzerland 2015
B. Hall, Lie Groups, Lie Algebras, and Representations, Graduate
Texts in Mathematics 222, DOI 10.1007/978-3-319-13467-3_13
371

372
13
Fundamental Groups of Compact Lie Groups
are null homotopic, then X is said to be simply connected. Since we are assuming
X is path connected, it is not hard to show that if all loops at one basepoint are
null homotopic, the same it true for every other basepoint. Furthermore, if a loop
based at x0 can be shrunk to a point without ﬁxing the basepoint (i.e., requiring only
that A.s; 0/ D A.s; 1/), then it can also be shrunk to a point with basepoint ﬁxed
(i.e., requiring A.s; 0/ D A.s; 1/ D x0).
The notion of homotopy is an equivalence relation on loops based at x0.
The homotopy class of a loop l is then the set of all loops that are homotopic
to l, and each loop belongs to one and only one homotopy class. The concatenation
operation "respects homotopy," meaning that if l1 is homotopic to l2 and m1 is
homotopic to m2; then l1  m1 is homotopic to l2  m2: As a result, it makes sense to
deﬁne the concatenation operation on equivalence classes.
The operation of concatenation makes the set of homotopy classes of loops based
at x0 into a group, called the fundamental group of X and denoted 1.X/: To verify
associativity, we note that although .l1l2/l3 is not the same as l1.l2l3/; the second
of these two loops is a reparameterization of the ﬁrst, from which it is not hard to see
that the loops are homotopic. Meanwhile, the identity in 1.X/ is the constant loop
l0: This is not an identity at the level of loops but is at the level of homotopy classes;
that is, l  l0 and l0  l are not equal to l; but they are both homotopic to l; since both
are reparameterizations of l: Finally, for inverses, the inverse to a homotopy class Œl
is the homotopy class Œl0 where l0.t/ D l.1  t/: (It is not hard to see that both l  l0
and l0  l are null homotopic.) A topological space X is simply connected precisely
if its fundamental group is the trivial group.
Some standard examples of fundamental groups are as follows: Rn is simply
connected for all n; Sn is simply connected for n  2; and the fundamental group
of S1 is isomorphic to Z:
Deﬁnition 13.1. If X and Y are Hausdorff topological space, a continuous map
 W Y ! X is a covering map if (1)  maps Y onto X and (2) for each x 2 X;
there is a neighborhood V of x such that 1.V / is a disjoint union of open sets U˛;
where the restriction of  to each U˛ is a homeomorphism of U˛ onto V: A cover
of X is a pair .Y; /, where  W Y ! X is a covering map. If .Y; / is a cover of X
and Y is simply connected, then .Y; / is a universal cover of X:
If .Y; / is a cover of X and f W Z ! X is a continuous map, then a map
Qf W Z ! Y is a lift of f if Qf is continuous and  ı Qf D f:
It is known that every connected manifold (indeed, every reasonably nice
connected topological space) has a universal cover, and that this universal cover
is unique up to a "canonical" homeomorphism, that is, one that intertwines the
covering maps. (See, for example, pp. 63-66 in [Hat].) Thus, we may speak about
"the" universal cover of any connected manifold. A key property of a covering maps
 W Y ! X is that lifts of reasonable maps into X always exist, as described in the
next two results. (See Proposition 1.30 in [Hat].)

13.2
Fundamental Groups of Compact Classical Groups
373
Proposition 13.2 (Path Lifting Property). Suppose .Y; / is a cover of X and
that p W Œ0; 1 ! X is a (continuous) path with p.0/ D x: Then for each
y 2 1.fxg/; there is a unique lift Qp of p for which Qp.0/ D y:
Proposition 13.3 (Homotopy Lifting Property). Suppose that l is a loop in X,
that a path p in Y is a lift of l, and that ls is a homotopy of l in X with basepoint
ﬁxed. Then there is a unique lift of ls to a homotopy ps of p in Y with endpoints
ﬁxed.
If we can ﬁnd a universal cover .Y; / of a space X; the cover gives a simple
criterion for determining when a loop in X is null homotopic.
Corollary 13.4. Suppose that .Y; / is a universal cover of X, that l is a loop in
X; and that p is a lift of l to Y: Then l is null homotopic in X if and only if p is a
loop in Y; that is, if and only if p.1/ D p.0/:
Proof. If the lift p of l is a loop, then since Y is simply connected, there is a
homotopy ps of p to a point with basepoint ﬁxed. Then ls WD  ı ps is a homotopy
of l to a point in X: In the other direction, if there is a homotopy ls of l to a point in
X; then by Proposition 13.3, we can lift this to a homotopy ps with endpoints ﬁxed.
Now, if l1 is the constant loop at x0; then p1, which is a lift of l1; must live entirely
in 1.fx0g/; which is a discrete set. Thus, actually, p1 must be constant, and, in
particular, has equal endpoints. But since ps is a homotopy with endpoints ﬁxed, the
endpoints of each ps must be equal. Thus, p D p0 must be a loop in Y:
ut
13.2
Fundamental Groups of Compact Classical Groups
In this section, we discuss a method of computing, inductively, the fundamental
groups of the classical compact groups. The same results can also be obtained by
using the results of Sects. 13.4-13.7; see Exercises 1-4. In all cases, we will ﬁnd
that 1.K/ is commutative. This is not a coincidence; a general argument shows
that the fundamental group of any Lie group is commutative (Exercise 7). In the
case of a compact matrix Lie group K, the commutativity of 1.K/ also follows
from Corollary 13.18.
For any nice topological space, one can deﬁne higher homotopy groups
k.X/; k D 1; 2; 3; : : : : The group k.X/ is the set of homotopy classes of maps of
Sk into X; where the notion of homotopy for maps of Sk into X is analogous to that
for maps of S1 into X: Although one can deﬁne a group structure on k.X/; this
structure is not relevant to us. All that is relevant is what it means for k.X/ to be
trivial, which is that every continuous map of the k-sphere Sk into X can be shrunk
continuously to a point. We will make use of the following standard topological
result (e.g., Corollary 4.9 in [Hat]).
Proposition 13.5. For a d-sphere Sd; k.Sd/ is trivial if k < d:

374
13
Fundamental Groups of Compact Lie Groups
This result is plausible because for k < d; the image of a "typical" continuous
map of Sk into Sd will not be all of Sd: However, if the image of the map omits
even one point in Sd; then we can remove that point and what is left of the sphere
can be contracted continuously to a point.
We now introduce the topological concept underlying all the calculations in this
section, that of a ﬁber bundle.
Deﬁnition 13.6. Suppose that B and F are Hausdorff topological spaces. A ﬁber
bundle with base B and ﬁber F is a Hausdorff topological space X together with
a continuous map p W X ! B; called the projection map, having the following
properties. First, for each b in B; the preimage of p1.b/ of b in X is homeomorphic
to F: Second, for every b in B; there is a neighborhood U of b such that p1.U / is
homeomorphic to U  F in such a way that the projection map is simply projection
onto the ﬁrst factor.
In any ﬁber bundle, the sets of the form p1.b/ are called the ﬁbers. The second
condition in the deﬁnition may be stated more pedantically as follows. For each
b 2 B, there should exist a neighborhood U of B and a homeomorphism ˆ
of p1.U / with U  F having the property that p.x/ D p1.ˆ.x//; where
p1 W U  F ! U is the map p1.u; f / D u:
The simplest sort of ﬁber bundle is the product space X D B  F; with the
projection map being simply the projection onto the ﬁrst factor. Such a ﬁber bundle
is called trivial. The second condition in the deﬁnition of a ﬁber bundle is called
local triviality and it says that any ﬁber bundle must look locally like a trivial
bundle. In general, X need not be globally homeomorphic to B  F:
If X were a trivial ﬁber bundle, then the fundamental group of X would be simply
the product of the fundamental group of the base B and the fundamental group of
the ﬁber F: In particular, if X were a trivial ﬁber bundle and 1.B/ were trivial,
then 1.X/ would be isomorphic to 1.F /: The following result says that if 1.B/
and 2.B/ are trivial, then the same conclusion holds, even if X is nontrivial.
Theorem 13.7. Suppose that X is a ﬁber bundle with base B and ﬁber F: If 1.B/
and 2.B/ are trivial, then 1.X/ is isomorphic to 1.F /:
Proof. According to a standard topological result (e.g., Theorem 4.41 and Proposi-
tion 4.48 in [Hat]), there is a long exact sequence of homotopy groups for a ﬁber
bundle. The portion of this sequence relevant to us is the following:
2.B/ !
f 1.F / !
g 1.X/ !
h 1.B/:
(13.1)
Saying that the sequence is exact means that each map is a homomorphism and
the image of each map is equal to the kernel of the following map. Since we are
assuming 2.B/ is trivial, the image of f is trivial, which means the kernel of g is
also trivial. Since 1.B/ is also trivial, the kernel of h must be 1.X/; which means
that the image of g is 1.X/: Thus, g is an isomorphism of 1.F / with 1.X/:
ut

13.2
Fundamental Groups of Compact Classical Groups
375
Proposition 13.8. Suppose G is a matrix Lie group and H is a closed subgroup of
G: Then G has the structure of a ﬁber bundle with base G=H and ﬁber H; where
the projection map p W G ! G=H is given by p.x/ D Œx; with Œx denoting the
coset xH 2 G=H:
Proof. For any coset Œx in G=H; the preimage of Œx under p is the set xH  G,
which is clearly homeomorphic to H: Meanwhile, the required local triviality
property of the bundle follows from Lemma 11.21 and Theorem 11.22. (If we take a
open set U in G=H as in the proof of Theorem 11.22, Lemma 11.21 tells us that the
preimage of U under p is homeomorphic to U H in such a way that the projection
p is just projection onto the ﬁrst factor.)
ut
Proposition 13.9. Consider the map p W SO.n/ ! Sn1 given by
p.R/ D Ren;
(13.2)
where en D .0; : : : ; 0; 1/: Then .SO.n/; p/ is a ﬁber bundle with base Sn1 and
ﬁber SO.n  1/:
Proof. We think of SO.n  1/ as the (closed) subgroup of SO.n/ consisting of
block diagonal matrices of the form
R D
 R0 0
0 1

with R0 2 SO.n  1/: By Proposition 13.8, SO.n/ is a ﬁber bundle with base
SO.n/=SO.n  1/ and ﬁber SO.n  1/: Now, it is easy to see that SO.n/ acts
transitively on the sphere Sn1: Thus, the map p in (13.2) maps SO.n/ onto Sn1:
Since Ren D en if and only R 2 SO.n1/; we see that p descends to a (continuous)
bijection of SO.n/=SO.n  1/ onto Sn1: Since both SO.n/=SO.n  1/ and Sn1
are compact, this map is actually a homeomorphism (Theorem 4.17 in [Rud1]).
Thus, SO.n/ is a ﬁber bundle of the claimed sort.
ut
Proposition 13.10. For all n  3; the fundamental group of SO.n/ is isomorphic
to Z=2: Meanwhile, 1.SO.2// Š Z:
Proof. Suppose that n is at least 4, so that n  1 is at least 3. Then, by
Proposition 13.5, 1.Sn1/ and 2.Sn1/ are trivial and, so, Theorem 13.7 and
Proposition 13.9 tell us that 1.SO.n// is isomorphic to 1.SO.n  1//: Thus,
1.SO.n// is isomorphic to 1.SO.3// for all n  4: It remains to show that
1.SO.3// Š Z=2: This can be done by noting that SO.3/ is homeomorphic to
RP3; as in Proposition 1.17, or by observing that the map ˆ in Proposition 1.19 is a
two-to-one covering map from SU.2/  S3 onto SO.3/:
Finally, we observe that SO.2/ is homeomorphic to the unit circle S1; so that
1.SO.2// Š Z (Theorem 1.7 in [Hat]).
ut

376
13
Fundamental Groups of Compact Lie Groups
If one looks into the proof of the long exact sequence of homotopy groups for
a ﬁber bundle, one ﬁnds that the map g in (13.1) is induced by the inclusion of F
into X: Thus, if l is a homotopically nontrivial loop in SO.n/; then after we include
SO.n/ into SO.n C 1/; the loop l is still homotopically nontrivial.
Meanwhile, we may take SU.2/ as the universal cover of SO.n/; with covering
map being the homomorphism ˆ in Proposition 1.19 (compare Exercise 8). Now,
if we take l to be the loop in SO.3/ consisting of rotations by angle  in the
.x2; x3/-plane, 0    2; the computations in (1.15) and (1.16) show that the
lift of l to SU.2/ is not a loop. (Rather, the lift will start at I and end at I:)
Thus, by Corollary 13.4, l is homotopically nontrivial in SO.3/: But this loop l is
conjugate in SO.3/ to the loop of rotations in the .x1; x2/-plane, so that loop is also
homotopically nontrivial. Thus, by the discussion in the previous paragraph, we may
say that, for any n  3; the one nontrivial homotopy class in SO.n/ is represented
by the loop
l./ WD
0
BBBBB@
cos   sin 
sin 
cos 
1
:::
1
1
CCCCCA
;
0    2:
(Compare Exercise 6.)
Proposition 13.11. The group SU.n/ is simply connected for all n  2: For all
n  1; we have that 1.U.n// Š Z:
Proof. For all n  3; the group SU.n/ acts transitively on the sphere S2n1. By a
small modiﬁcation of the proof of Proposition 13.9, SU.n/ is a ﬁber bundle with
base S2n1 and ﬁber SU.n  1/: Since 2n  1 > 3 for all n  2; Theorem 13.7
and Proposition 13.5 tell us that 1.SU.n// Š 1.SU.n  1//: Since 1.SU
.2// Š 1.S3/ is trivial, we conclude that 1.SU.n// is trivial for all n  2:
The analysis of the case of U.n/ is similar. The ﬁber bundle argument shows
that 1.U.n// Š 1.U.n  1// for all n  2: Since U.1/ is just the unit circle
S1; we have that 1.U.1// Š Z (Theorem 1.7 in [Hat]). Thus, 1.U.1// Š Z for
all n  1:
ut
Proposition 13.12. For all n  1; the compact symplectic group Sp.n/ is simply
connected.
Proof. Since Sp.n/ is contained in U.2n/; it acts on the unit sphere S4n1  C2n: If
this action is transitive, then we can imitate the arguments from the cases of SO.n/
and SU.n/: Since 4n1  3; we see that 1.S4n1/ and 2.S4n1/ are trivial for all
n  1: We conclude, then, that 1.Sp.n// Š 1.Sp.n1//: Since Sp.1/ D SU.2/;
which is simply connected, we see that Sp.n/ is simply connected for all n:
It remains, then, to show that Sp.n/ acts transitively on S4n1: For this, it sufﬁces
to show that for all unit vectors u 2 C2n; there is some U 2 Sp.n/ with Ue1 D u:

13.4
The Fundamental Groups of K and T
377
To see this, let u1 D u and v1 D Ju; where J is the map in Sect. 1.2.8. Then v1 is
orthogonal to u1 (check) and we may consider the space
W WD .spanfu1; v1g/?:
By the calculations in Sect. 1.2.8, W will be invariant under J: Thus, we can choose
an arbitrary unit vector u2 in W and let v2 D Ju2: Proceeding on in this way, we
eventually obtain an orthonormal family
u1; : : : ; un; v1; : : : ; vn
(13.3)
in C2n where Juj D vj : It is then straightforward to check that the matrix U whose
columns are the vectors in (13.3) belongs to Sp.n/; and that Ue1 D u1 D u:
ut
13.3
Fundamental Groups of Noncompact Classical Groups
Using the polar decomposition (Sect. 2.5), we can reduce the computation of
the fundamental group of certain noncompact groups to the computation of the
fundamental group of one of the compact groups in Sect. 13.2. Theorem 2.17, for
example, tells us that GL.nI C/ is homeomorphic to U.n/  V for a certain vector
space V (the space of n  n self-adjoint matrices). Since V is simply connected, we
conclude that 1.GL.nI C// Š Z: Using Proposition 2.19, we can similarly show
that 1.SL.nI C// Š 1.SU.n// and that 1.SL.nI R// Š 1.SO.n//:
Conclusion 13.13. For all n  1; we have
1.GL.nI C// Š 1.U.n// Š Z:
For all n  2; the group SL.nI C/ is simply connected. For n  3; we have
1.SL.nI R// Š 1.SO.n// Š Z=2;
whereas
1.SL.2I R// Š 1.SO.2// Š Z:
13.4
The Fundamental Groups of K and T
In this section and the subsequent ones, we develop a different approach to comput-
ing the fundamental group of a compact group K; based on the torus theorem. In this
section, we state the main results; the proofs will be developed in Sects. 13.5, 13.6,

378
13
Fundamental Groups of Compact Lie Groups
and 13.7. One important consequence of these results will be Corollary 13.20, which
says that if K is simply connected, then every algebraically integral element is
analytically integral. This claim allows us (in the simply connected case) to match
up our theorem of the highest weight for the compact group K with the theorem of
the highest weight for the Lie algebra g WD kC: That is to say, when K is simply
connected, the set of possible highest weights for the representations of K (namely
the dominant, analytically integral elements) coincides with the set of possible
highest weights for the representations of g (namely the dominant, algebraically
integral elements).
In these sections, we will follow the convention of composing the exponential
map for T with a factor of 2; writing an element t of T as
t D e2H;
H 2 t:
(There are factors of 2 that have to go somewhere in the theory, and this seems
to be the most convenient spot to put them.) Recall (Deﬁnition 12.3) that   t
denotes the kernel of the (scaled) exponential map:
 D
˚
H 2 t
ˇˇe2H D I

and that  is invariant under the action of W:
We begin with a simple result describing the fundamental group of T:
Proposition 13.14. Every loop in T is homotopic in T to a unique loop of the form
 7! e2;
0    1;
with  2 : Furthermore, 1.T / is isomorphic to :
Proof. The main issue is to prove that the (scaled) exponential map H 7! e2H is a
covering map. Since the kernel  of the exponential map is discrete, there is some
" > 0 such that every nonzero element  of  has norm at least ": Let B"=2./ be
the ball of radius "=2 around a point  2 : Now, for any t 2 T; write t as e2H for
some H 2 t: Then let V  T denote the set of element of the form e2H 0; where
H 0 2 B"=2.H/; so that V is a neighborhood of t: The preimage of V under the
exponential is the union of the balls
B"=2.H C /;
 2 :
By the way " was chosen, these balls are disjoint, and each ball maps homeomor-
phically onto V: Since we can do this for each t 2 T; we see that the exponential is
a covering map.
Since, also, t is simply connected, t is the universal cover of T: Now, every loop l
in T based at the identity has a unique lift to a path Ql in t starting at 0 and ending at
some point  in : The theory of covering spaces tells us that two loops l1 and l2 in T
(based at I) are homotopic if and only if Ql1.1/ D Ql2.1/: Meanwhile, if Ql.1/ D ; then

13.4
The Fundamental Groups of K and T
379
(since t is simply connected) Ql is homotopic with endpoints ﬁxed to the straight-line
path  7! ; showing that l itself is homotopic to  7! e2; as claimed. Finally,
if we compose two loops of the form  7! e21 and  7! e22; the lift of the
composite loop will be the composition of the lifts, where the lift of the second part
of the composite loop must be taken to start at 1: Thus, the lift of the composite
loop will go from 0 to 1 and then (shifting the lift of the second loop by 1) from 1
to 1 C 2: But any path from 0 to 1 C 2 in t is homotopic with endpoints ﬁxed to
the straight-line path  7! e2.1C2/: Thus, if we identify elements of 1.T / with
elements of ; the composition operator corresponds to addition in :
ut
We now state the ﬁrst main result of this section; the proof is given in Sect. 13.7.
Theorem 13.15. Every loop in K is homotopic to a loop in T:
The theorem does not mean that 1.K/ is isomorphic to 1.T /; since a loop
in T may be null homotopic in K even if it is not null homotopic in T: Indeed,
1.T / (which is isomorphic to ) is often very different from 1.K/ (which is, for
example, trivial when K D SU.n/). Nevertheless, the theorem gives us a useful
way to study 1.K/; because we understand 1.T /. Taking Theorem 13.15 and
Proposition 13.14 together, we see that every loop in K is homotopic to a loop of
the form  7! e2; with  2 : Thus, we are faced with a very concrete problem
to calculate 1.K/: We must only determine, for each  2 ; whether the loop
t 7! e2t is null homotopic in the compact group K: (By Proposition 13.14, such
a loop is null homotopic in the torus T only if  D 0:)
The condition for  7! e2 to be null homotopic in K turns out to be related
to the notion of coroots. Recall that if ˛ is a real root for t; then (Lemma 12.8),
the associated real coroot H˛ D 2˛= h˛; ˛i belongs to : Thus, any integer linear
combination of coroots also belongs to :
Deﬁnition 13.16. The coroot lattice, denoted I; is the set of all integer linear
combinations of real coroots H˛; ˛ 2 R:
We now state the second main result of this section; the proof is also in Sect. 13.7.
Theorem 13.17. For each  2 ; the loop  7! e2 is null homotopic in K if and
only if  belongs to the coroot lattice I:
If we combine this result with Theorem 13.15, we obtain the following descrip-
tion of the fundamental group of K:
Corollary 13.18. The fundamental group of K is isomorphic to the quotient group
=I; where the quotient is of commutative groups.
Proof. By Theorem 13.15, every loop in K is homotopic to a loop of the form  7!
e2; with  2 : Under this correspondence, composition of loops corresponds to
addition in : By Theorem 13.17, two loops of the form  7! e21 and  7! e22
are homotopic if and only if 1  2 belongs to I: Thus, 1.K/ may be identiﬁed
with the set of cosets of I in :
ut

380
13
Fundamental Groups of Compact Lie Groups
We now consider three examples. The ﬁrst two are familiar friends, the groups
SO.5/ and SU.3/: The third is the projective unitary group PSU.3/: In general,
PSU.n/ is the quotient of SU.n/ by the subgroup consisting of whichever multiples
of the identity have determinant 1 in U.n/: In the case n D 3; a matrix of the form
eiI has determinant 1 if and only if e3i D 1: Thus,
PSU.3/ D SU.3/=fI; e2i=3I; e4i=3Ig:
(13.4)
We can represent PSU.3/ as a matrix group by using the adjoint representation. It
is easy to check that the center of SU.3/ is the group being divided by on the right-
hand side of (13.4); thus, the image SU.3/ under the adjoint representation will be a
subgroup of GL.sl.3I C// isomorphic to PSU.3/: Since the center of the Lie algebra
su.3/ is trivial, the Lie algebra version of the adjoint representation is faithful; thus,
we may identify the Lie algebra of PSU.3/ with the Lie algebra of SU.3/:
Example 13.19. If K D SO.5/; then =I Š Z=2: If K D SU.3/; then =I is
trivial. Finally, if K D PSU.3/; then =I Š Z=3:
The veriﬁcation of the claims in Example 13.19 is left to the reader (Exercise 5).
Corollary 13.18 together with Example 5 give another way of computing the funda-
mental groups of SO.5/ (Z=2/ and SU.3/ (trivial), in addition to Proposition 13.10.
Figures 13.1 and 13.2 show  and I in the case of the groups SO.5/ and PSU.3/:
The black dots indicate points in I, whereas white dots indicate points in  that are
Fig. 13.1 The coroot lattice (black dots) and the kernel of the exponential mapping (black and
white dots) for the group SO.5/

13.4
The Fundamental Groups of K and T
381
2
1
Fig. 13.2 The coroot lattice (black dots) and the kernel of the exponential mapping (black and
white dots) for the group PSU.3/
not in I: In the case of SO.5/; it is easy to see that any two white dots differ by an
element of the coroot lattice, showing that there is exactly one nontrivial element of
=I: In the case of PSU.3/; note that the elements 1 and 2 D 21 are not in I; but
31 is in I; showing that Œ1 is an element of order 3 in =I: The reader may verify
that every element of  is either in I; differs from 1 by an element of I; or differs
from 2 by an element of I: The situation for SU.3/ is similar to that for PSU.3/;
the coroot lattice I does not change, but  is now equal to I; so that =I is trivial.
For now, the reader may regard the lines in the ﬁgures as merely decorative; these
lines will turn out to make up the "Stiefel diagram" for the relevant group. (See
Sect. 13.6.)
Corollary 13.20. If K is simply connected, then every algebraically integral
element is analytically integral.
Proof. In light of Theorem 13.17, K is simply connected if and only if I D : If
 2 t is algebraically integral, then h; H˛i 2 Z for all ˛; where H˛ D 2˛= h˛; ˛i
is the real coroot associated to ˛: It follows that h; i 2 Z for every element of I;
the set of integer linear combinations of coroots. Thus, if K is simply connected,
h; i 2 Z for every element of I D ; which means that  is analytically
integral.
ut
We may offer a completely different proof of Corollary 13.20, as follows. We
ﬁrst observe that if K is simply connected, then by Proposition 7.7, the complex
Lie algebra g WD kC is semisimple. Then let  be an algebraically integral element.
Since the sets of analytically integral and algebraically integral elements are both
invariant under the action of W; it is harmless to assume that  is dominant. Thus,

382
13
Fundamental Groups of Compact Lie Groups
by the results of Chapter 9, there is a ﬁnite-dimensional irreducible representation
.; V/ of g WD kC with highest weight : Since K is simply connected, Theorem 5.6
then tells us that there is an associated representation ... of K acting on V: Thus,
 is a weight for a representation of the group K; which implies (Proposition 12.5)
that  is analytically integral.
Although it is mathematically correct, the preceding argument may be considered
as "cheating," since it depends on the whole machinery of Verma modules (to
construct the representations of g) and on the Baker-Campbell-Hausdorff formula
(to prove Theorem 5.6). In the subsequent sections, we will use techniques similar
to those in the proof of the torus theorem to prove Theorem 13.17 and thus to give a
more direct (but not easy!) proof of Corollary 13.20.
Corollary 13.21. If K is simply connected, the element ı (half the sum of the real,
positive roots) is analytically integral.
Proof. According to Proposition 8.38 (translated into the language of real roots), the
element ı is algebraically integral. But by Corollary 13.20, if K is simply connected,
every algebraically integral element is analytically integral.
ut
Since it is easy to do so, we will immediately prove one direction of The-
orem 13.17, namely that if  is in the coroot lattice I, then 
7! e2 is
homotopically trivial in K:
Proof of Theorem 13.17, one direction. We assume at ﬁrst that  D H˛; a single
real coroot. Then by Corollary 7.20, there is a homomorphism  of su.2/ into k
such that  maps the element iH D diag.i; i/ in su.2/ to the real coroot H˛:
Since SU.2/ is simply connected, there exists a homomorphism ˆ W SU.2/ ! K
such that ˆ.eX/ D e.X/ for all X 2 su.2/:
Consider, then, the loop l in SU.2/ given by
l./ D e2iH D
 e2i
0
0
e2i

;
0    1:
Observe that ˆ.l.// D e2H˛; so that the image of l under ˆ is the relevant loop
in K: Since SU.2/ is simply connected, there is a family ls of loops connecting l to
a constant loop in SU.2/: Thus, the loops
 7! ˆ.ls.//
constitute a homotopy of our original loop to a point in K:
Now, if  is an integer linear combination of coroots, then by Proposition 13.14,
the loop  7! e2 is homotopic (in T and thus in K) to a composition of loops
of the form  7! e2H˛; for various coroots ˛: Since each of those loops is null
homotopic in K; so is the loop  7! e2:
ut

13.5
Regular Elements
383
13.5
Regular Elements
We will study the topology of K by writing every element x of K as
x D yty1;
with y 2 K and t 2 T: It is then convenient to write the variable t in the preceding
expression in exponential coordinates. Thus, we may consider the map ‰ W t 
.K=T / ! K given by
‰.H; Œy/ D ye2Hy1:
(13.5)
(We continue to scale the exponential map for T by a factor of 2:) The torus
theorem tells us that ‰ maps onto K: On the other hand, the behavior of ‰ is easiest
to understand when the differential ‰ is nonsingular.
We now introduce a notion of "regular"elements in K; we will see (Proposi-
tion 13.24) that ‰.H; Œy/ is invertible if and only if x WD ‰.H; Œy/ is regular. It
will turn out that the fundamental group of K is the same as the fundamental group
of the set of regular elements in K:
Deﬁnition 13.22. If x 2 K is contained in a unique maximal torus, x is regular;
if x is contained in two distinct maximal tori, x is singular. The set of regular
elements in K is denoted Kreg and the set of singular elements is denoted Ksing:
If, for example, t 2 T generates a dense subgroup of T; then the only maximal
torus containing yty1 is yTy1: Thus, for such a t; the element yty1 is regular.
As we will see, however, yty1 can be regular even if t does not generate a dense
subgroup of T:
Proposition 13.23. Suppose x 2 K has the form x D yty1 with t 2 T and y 2 K:
If there exists some X 2 k with X ... t such that
Adt.X/ D X;
then x is singular. If no such X exists, x is regular.
Proof. The condition of being regular is clearly invariant under conjugation; that is,
x is regular if and only if t is regular. Suppose now that there is some X ... t with
Adt.X/ D X: Then t commutes with eX for all  2 R: Applying Lemma 11.37
with S D feXg2R; there is a maximal torus S0 containing both t and feXg2R:
But then the Lie algebra s0 of S0 must contain X; which is not in t; which means
that S0 ¤ T: Thus, t (and therefore, also x) is singular.
In the other direction, if x (and therefore, also, t) is singular, then there is a
maximal torus S0 ¤ T containing t: But we cannot have S0  T; or else S0 would
not be maximal. Thus, there must be some X in the Lie algebra s0 of S0 that is not
in t: But since t 2 S0 and S0 is commutative, we have Adt.X/ D X:
ut

384
13
Fundamental Groups of Compact Lie Groups
Proposition 13.24. An element x D ye2H y1 is singular if and only if there is
some root ˛ for which
h˛; Hi 2 Z:
It follows that x D ye2Hy1 is singular if and only if ‰ is singular at the point
.H; Œy/:
Note that for each ﬁxed ˛ 2 R and n 2 Z; the set of H in t for which h˛; Hi D n
is a hyperplane (not necessarily through the origin).
Proof. By Proposition 13.23, x is singular if and only if Ade2H .X/ D X for some
X ... t: Now, the dimension of the eigenspace of Ade2H with eigenvalue 1 2 R is
the same whether we work over R or over C: The eigenvalues of Ade2H over C are
1 (on h) together with the numbers of the form
e2ih˛;Hi;
˛ 2 R;
(from the root space g˛). Thus, the dimension of the 1-eigenspace is greater than
dim h if and only if e2ih˛;Hi D 1 for some ˛; which holds if and only if h˛; Hi 2 Z:
Meanwhile, the map ‰ is just the map ˆ in Deﬁnition 11.25, composed with the
exponential map for T: Since T is commutative, the differential of the exponential
map for T is the identity at each point. Thus, using Proposition 11.27, we see that
‰.H; Œy/ is singular if and only if the restriction of Ade2H to t? (or, equivalently,
to .t?/C) has an eigenvalue of 1. Since the eigenvalues of Ade2H on .t?/C are the
numbers of the form e2ih˛;Hi; we see that ‰.H; Œy/ is singular if and only if
h˛; Hi 2 Z for some ˛:
ut
Deﬁnition 13.25. An element H of t is regular if for all ˛ 2 R; the quantity
h˛; Hi is not an integer. Otherwise, H is singular. The set of regular elements in t
is denoted treg:
A key issue in the proof of Theorem 13.17 is to understand the extent to which the
map ‰ in (13.5) fails to be injective. There are two obvious sources of noninjectivity
for ‰: The ﬁrst is the kernel of the exponential map; clearly if  2 ; then
‰.H C ; Œx/ D ‰.H; Œx/:
Meanwhile, if w 2 W and z 2 N.T / represents w; then
‰.w  H; Œxz1/ D xz1e2.wH/z1x1
D xe2Hx1
D ‰.H; Œx/;
since z1 represents w1: We now demonstrate that if we restrict ‰ to treg .K=T /,
these two sources account for all of the noninjectivity of ‰:

13.5
Regular Elements
385
Proposition 13.26. Suppose .H; Œx/ and .H 0; Œx0/ belong to treg  .K=T /: Then
‰.H; Œx/ D ‰.H 0; Œx0/ if and only if there exist some w D Œz in W and some
 2  such that
H 0 D w  H C 
Œx0 D Œxz1:
(13.6)
Here Œz denotes the coset containing z 2 N.T / in W D N.T /=T: Furthermore,
if the elements in (13.6) satisfy H 0 D H and Œx0 D Œx; then  D 0 and w is the
identity element of W:
Note that if z 2 N.T / and t 2 T; then
xtz1 D xz1.ztz1/;
where ztz1 2 T: Thus, Œxtz1 and Œxz1 are equal in K=T: That is to say, for
a ﬁxed z 2 N.T /; the map Œx 7! Œxz1 is a well-deﬁned map of K=T to itself.
A similar argument shows that this action depends only on the coset of z in N.T /=T:
Proof. If H 0 and x0 are as in (13.6), then by the calculations preceding the statement
of the proposition, we will have ‰.H 0; Œx0/ D ‰.H; Œx/: In the other direction, if
‰.H; Œx/ D ‰.H 0; Œx0/; then
xe2Hx1 D x0e2H 0.x0/1;
which means that
e2H D z1e2H 0z;
(13.7)
where z D .x0/1x:
Now, the relation (13.7) implies that e2H belongs to the torus z1Tz: Since H 2
treg; it follows from Proposition 13.24 that z1Tz D T; that is, that z 2 N.T /: Then,
if w D Œz; we have
e2H D e2w1H 0:
From this, we obtain e2.w1H 0H/ D I; which means that w1  H 0  H belongs
to : Since  is invariant under the action of W; the element  WD H 0  w  H also
belongs to ; and we ﬁnd that H 0 D w  H C  and x0 D xz1; as claimed.
Finally, if H 0 D H and Œx0 D Œx; then x0 and x belong to the same coset
in K=T; which means that z1 must be in T: Thus, w is the identity element in
W D N.T /=T: But once w D e; we see that H 0 D H only if  D 0:
ut
We now come to a key result that is essential to the proofs of Theorems 13.15
and 13.17.

386
13
Fundamental Groups of Compact Lie Groups
Theorem 13.27. The fundamental groups of K and Kreg are isomorphic. Specif-
ically, every loop in K is homotopic to a loop in Kreg and a loop in Kreg is null
homotopic in K only if it is null homotopic in Kreg:
To prove this result, we will ﬁrst show that the singular set in K is "small,"
meaning that it has codimension at least 3. (In the case K D SU.2/, for example,
the singular set is just fI; Ig; so that Ksing has dimension 0, whereas K had
dimension 3.) We will then argue as follows. Let n be the dimension of K and
suppose E and F are subsets of K of dimension k and l; respectively. If k C l < n;
then "generically" E and F will not intersect. If E and F do intersect, then (we will
show) it is possible to perturb F slightly so as to be disjoint from E: We ﬁrst apply
this result with E D Ksing and F being a loop in K: Then E has dimension at most
n  3 while F has dimension 1, so the loop F is homotopic to a loop that does not
intersect Ksing: We next apply this result with E D Ksing and F being a homotopy
of a loop l  Kreg: Then E still has dimension at most n3 while F has dimension
2 (the image of a square), so the homotopy can be deformed to a homotopy that
does not intersect Ksing: In the remainder of this section, we will ﬂesh out the above
argument.
Lemma 13.28. There exist ﬁnitely many smooth compact manifolds M1; : : : ; MN
together with smooth maps fj W Mj ! K such that (1) each Mj has dimension at
most dim K  3; and (2) each element of Ksing is in the image of fj for some j:
Proof. Since each root ˛ is analytically integral (Proposition 12.7), there exists a
map f˛ W T ! S1 such that
f˛.e2H / D e2ih˛;Hi
(13.8)
for all H 2 t: Clearly, f˛ is actually a homomorphism of T into S1: Let T˛ be the
kernel of f˛; so that T˛ is a closed subgroup of T: The Lie algebra of T˛ is the set
of H 2 t with h˛; Hi D 0; thus, T˛ has dimension one less than the dimension of
T: (Note that T˛ may not be connected.) For each H 2 t; we see from (13.8) that
e2H belongs to T˛ if and only if h˛; Hi is an integer. Thus, by the torus theorem
and Proposition 13.24, each singular element in K is conjugate to an element of T˛;
for some ˛:
Now ﬁx a root ˛ and let C.T˛/ denote the centralizer of T˛; that is, the set of
x 2 K such that x commutes with every element of T˛: Then C.T˛/ is a closed
subgroup of K; and the Lie algebra of C.T˛/ consists of those X 2 k such that
Adt.X/ D X for all t 2 T˛: Suppose now that X˛ belongs to the root space g˛ and
t D e2H belongs to T˛: Then
Ade2H .X˛/ D e2adH .X˛/ D e2ih˛;HiX˛ D X˛;
since f .e2H / D e2ih˛;Hi D 1; by assumption, and similarly with X˛ replaced by
Y˛ WD X
˛ : Thus, the Lie algebra of C.T˛/ will contain t and at least two additional
elements, X˛  Y˛ and i.X˛ C Y˛/; that are independent of each other and of t:
(Compare Corollary 7.20.) We conclude that the dimension of C.T˛/ is at least
dim T˛ C 2:

13.5
Regular Elements
387
Now, the map f˛ W T˛  K ! K given by
f˛.t; x/ D xtx1
descends to a map (still called f˛) of T˛  .K=C.T˛// into K: Furthermore, we
compute that
dim.T˛  .K=C.T˛/// D dim T˛ C dim K  dim C.T˛/
 dim T  1 C dim K  dim T  2
D dim K  3:
Since, as we have said, every singular element is conjugate to an element of some
T˛; we have proved the lemma, with the M 's being the manifolds T˛  .K=C.T˛//
and the f 's being the maps f˛:
ut
Lemma 13.29. Let M and N be compact manifolds and let D be a compact
manifold with boundary, with
dim M C dim D < dim N:
Let f W M ! N and g W D ! N be smooth maps. Suppose E is a closed subset
of D such that g.E/ is disjoint from f .M /: Then g is homotopic to a map g0 such
that g0 D g on E and such that g0.D/ is disjoint from f .M /:
Since g.E/ is already disjoint from f .M /; it is plausible that we can deform
g without changing its values on E to make the image disjoint from f .M /: Our
proof will make use of the following result: If X and Y are smooth manifolds with
dim X < dim Y and f W X ! Y is a smooth map, then the image of X under f is a
set of measure zero in Y: (Note that we do not assume f is injective.) This result is
a consequence of Sard's theorem; see, for example, Corollary 6.11 in [Lee]. We will
use this result to show that g can be moved locally off of f .M /; a ﬁnite number of
these local moves will then produce the desired map g0:
Proof.
Step 1: The local move. For x 2 D n E; let us choose a neighborhood U
of g.x/ diffeomorphic to Rn; where n D dim N: We may then deﬁne a map
h W f 1.U /  g1.U / ! U
by
h.m; x/ D f .m/  g.x/;
where the difference is computed in Rn Š U: By our assumption on the
dimensions, the image of h is a set of measure zero in U . Thus, in every
neighborhood of 0; we can ﬁnd some p that is not in the image of h:

388
13
Fundamental Groups of Compact Lie Groups
Suppose that W is any neighborhood of x in D such that NW is contained in
g1.U / and NW is disjoint from E: Then we can choose a smooth function  on
D such that  equals 1 on NW but such that  equals 0 both on E and on the
complement of g1.U /: Let us deﬁne a family of maps gs W D ! N by setting
gs.x/ D g.x/ C s.x/p;
0  s  1
for x 2 g1.U / and setting gs.x/ D g.x/ for x ... g1.U /:
When s D 1; and x 2 NW ; we have
f .m/  g1.x/ D f .m/  g.x/  p D h.m; x/  p:
Since h never takes the value p; we see that f .m/ does not equal g1.x/; thus,
g1. NW /  U is disjoint from f .M /: We conclude that g1 is a map homotopic to
g such that g1 D g on E and g1. NW / is disjoint from f .M /: Furthermore, since
p can be chosen to be as small as we want, we can make g1 uniformly as close
to g as we like.
Step 2: The global argument. Choose a neighborhood V of E in D such that
g. NV / is disjoint from f .M /; and let K be the complement of V in D: For each x
in K; we can ﬁnd a neighborhoodU  N of g.x/ such that U is diffeomorphicto
Rn: Now choose a neighborhoodW of x so that NW is contained in g1.U / but NW
is disjoint from E: Since K is compact, there is some ﬁnite collection x1; : : : ; xN
of points in K such that the associated open sets W1; : : : ; WN cover K: Thus,
each NWj is disjoint from E and is contained in a set of the form g1.Uj/; with
Uj  N diffeomorphic to Rn:
By the argument in Step 1, we can ﬁnd a map g1 homotopic to g such that
g1 D g on E and g1. NW1/ is disjoint from f .M /; but such that g is as close as
we like to g: Since g. NV / and f .M / are compact, the distance between g. NV / and
f .M / (with respect to an arbitrarily chosen Riemannian metric on N ) achieves
a positive minimum. Therefore, if we take g1 close enough to g; then g1. NV /
will still be disjoint from f .M /: We can similarly ensure that g1 still maps the
compact set NWj into Uj for j D 2; : : : ; N:
We may now perform a similar perturbation of g1 to a map g2 such that g2 D
g1 D g on E but such that g2. NW2/ is disjoint from f .M /: By making this
perturbation small enough, we can ensure that g2 has the same properties as
g1: First, g2. NV / is still disjoint from f .M /; second, g2. NW1/ is still disjoint from
f .M /; and third, g2. NWj / is still contained in Uj for j D 3; : : : ; N: Proceeding
on in this fashion, we eventually obtain a map gN homotopic to g such that
gN D g on E and such that gN . NV / and each gN . NWj/ are disjoint from f .M /:
Then g0 WD gN is the desired map.
ut
Proof of Theorem 13.27. In Lemma 13.28, it is harmless to assume that each Mj
has dimension dim K  3; since if any Mj has a lower dimension, we can take a
product of Mj with, say, a sphere dimension of an appropriate dimension, and then

13.6
The Stiefel Diagram
389
make fj independent of the sphere variable. Once this is done, we can let M be the
disjoint union of the Mj's and let f be the map equal to fj on Mj: That is to say,
the singular set is actually in the image under a smooth map f of single manifold
M of dimension dim K  3:
If l is any loop in K; it is not hard to prove that l is homotopic to smooth loop.
(See, for example, Theorem 6.26 in [Lee].) In fact, this claim can be proved by an
argument similar to the proof of Lemma 13.29; locally, any continuous map from
Rn to Rm can be approximated by smooth functions (even polynomials), and one
can then patch together these approximations, being careful at each stage not to
disrupt the smoothness achieved at the previous stage. We may thus assume that l
is smooth and apply Lemma 13.29 with D D S1; N D K; and E D ¿: Since
dim S1 C dim M < dim K; the lemma says that we can deform l until it does not
intersect f .M / 	 Ksing:
Next, suppose l is a loop in Kreg; which we can assume to be smooth. If l is
null homotopic in Kreg; it is certainly null homotopic in K: In the other direction,
suppose l is null homotopic in K: We think of the homotopy of l to a point as a map
g of a 2-disk D into K; where the boundary of D corresponds to the original loop
l and the center of D correspond to the point. After deforming g slightly—by the
same argument as in the previous paragraph—we may assume that g is smooth. We
now apply Lemma 13.29 with D equal to the 2-disk and E equal to the boundary
of D: The lemma tells us that we can deform g to a map g0 that agrees with g on
the boundary of D but such that g0.D/ is disjoint from f .M / 	 Ksing: Thus, g0 is
a homotopy of l to a point in Kreg:
ut
13.6
The Stiefel Diagram
In this section, we look more closely at the structure of the hyperplanes in t where
h˛; Hi D n, which appear in Proposition 13.24. The main result of the section is
Theorem 13.35, which constructs a cover of the regular set Kreg:
Deﬁnition 13.30. For each n 2 Z and ˛ 2 R; consider the hyperplane (not
necessarily through the origin) given by
L˛;n D fH 2 t jh˛; Hi D ng :
The union of all such hyperplanes is called the Stiefel diagram. A connected
component of the complement in t of the Stiefel diagram is called an alcove.
In light of Proposition 13.24 and Deﬁnition 13.25, the complement of the Stiefel
diagram is just the regular set treg in t: Figures 13.3, 13.4, and 13.5 show the Stiefel
diagrams for A2; B2; and G2; respectively. In each ﬁgure, the roots are indicated
with arrows, with the long roots being normalized to have length
p
2. The black
dots in the ﬁgure indicate the coroots and one alcove is shaded. Note that since

390
13
Fundamental Groups of Compact Lie Groups
Fig. 13.3 The Stiefel
diagram for A2; with the
roots normalized to have
length
p
2: The black dots
indicate the coroots
Fig. 13.4 The Stiefel diagram for B2; with the long roots normalized to have length
p
2: The
black dots indicate the coroots
h˛; H˛i D 2; if we start at the origin and travel in the ˛ direction, the coroot H˛
will be located on the second hyperplane orthogonal to ˛:
Let us check the correctness of, say, the diagram for G2: Suppose ˛ is a long root,
which we normalize so that h˛; ˛i D 2: Then ˛=2 belongs to the line L˛;1; so that
L˛;1 is the (unique) line orthogonal to ˛ passing through ˛=2: Similarly, L˛;n is the

13.6
The Stiefel Diagram
391
Fig. 13.5 The Stiefel diagram for G2; with the long roots normalized to have length
p
2: The
black dots indicate the coroots
line orthogonal to ˛ passing through n˛=2: Suppose, on the other hand, that ˛ is a
short root, which we must then normalize so that h˛; ˛i D 2=3: Then 3˛=2 belongs
to L˛;1, so that L˛;1 is the line orthogonal to ˛ passing through 3˛=2 and L˛;n is the
line orthogonal to ˛ passing through 3n˛=2: Meanwhile, for a long root ˛; we have
H˛ D 2˛=2 D ˛, whereas for a short root, we have H˛ D 2˛=.2=3/ D 3˛: These
results agree with what we see in Figure 13.5.
It will follow from Proposition 13.34 that every alcove is isometric to every other
alcove. Furthermore, if g WD kC is simple (i.e., if the root system R is irreducible),
each alcove is a "simplex," that is, a bounded region in a k-dimensional space
deﬁned by k C1 linear inequalities. Thus, for rank-two simple algebras, each alcove
is a triangle, as can be seen from Figures 13.3, 13.4, and 13.5. In general, the
structure of the alcoves for a simple algebra is described by an extended Dynkin
diagram; see Exercises 11, 12, and 13.
Proposition 13.31. The Stiefel diagram is invariant under the action of W and
under translations by elements of :
Proof. Since W permutes the roots, invariance of the Stiefel diagram is evident.
Meanwhile, if  is in the kernel  of the exponential map, the adjoint action of e2
on g is trivial. Thus, for X 2 g˛; we have
X D Ade2 .X/ D e2ih˛;iX;
which means that h˛; i is an integer, for all ˛ 2 R: From this we can easily see
that if H is in the Stiefel diagram, so H C :
ut

392
13
Fundamental Groups of Compact Lie Groups
Recall that an afﬁne transformation of a vector space is a transformation that
can be expressed as a combination of a linear transformation and a translation.
Proposition 13.32. Let  Ì W denote the set of afﬁne transformations of t that can
be expressed in the form
H 7! w  H C 
for some  2  and some w 2 W: Then  Ì W forms a group under composition,
with the group law given by
.; w/  .0; w0/ D . C w  0; ww0/:
The group  Ì W is a semidirect product of  and W; with  being the normal
factor.
Proof. We merely compute that
w  .w0  H C 0/ C  D .ww0/  H C  C w  0;
so that the composition of the afﬁne transformations associated to .; w/ and .0; w0/
is the afﬁne transformation associated to . C w  0; ww0/:
ut
We now introduce the analog of the Weyl group for the Stiefel diagram. Even
if a hyperplane V  t does not pass through the origin, we can still speak of the
reﬂection s about V; which is the unique afﬁne transformation of t such that
s.H C H 0/ D H  H 0
whenever H is in V and H 0 is orthogonal to V:
Deﬁnition 13.33. The extended Weyl group for K relative to t is the group of
afﬁne transformations of t generated by reﬂections about all the hyperplanes in the
Stiefel diagram of t:
We now establish some key properties of the extended Weyl group. Recall from
Deﬁnition 13.16 the notion of the coroot lattice I:
Proposition 13.34. 1. The extended Weyl group equals I Ì W; the semidirect
product of the ordinary Weyl group W and the coroot lattice I:
2. The extended Weyl group acts freely and transitively on the set of alcoves.
Note that since I   is invariant under the action of W; the extended Weyl
group I Ì W is a subgroup of the group  Ì W in Proposition 13.32. That is to say,
the group law in I Ì W is the same as in Proposition 13.32, although I Ì W will,
in general, be a proper subgroup of  Ì W:
Proof. For Point 1, let V˛; ˛ 2 R; be the hyperplane through the origin orthogonal
to ˛: Since h˛; H˛i D 2; we see that the hyperplane L˛;n in Deﬁnition 13.30 is the

13.6
The Stiefel Diagram
393
translate of V˛ by nH˛=2: We can then easily verify that the reﬂection s˛;n about
L˛;n is given by
s˛;n.H/ D s˛.H/ C nH˛;
(13.9)
where s˛ is the reﬂection about V˛. Thus, s˛;n is a combination of an element s˛ of
W and a translation by an element of I: It follows that the extended Weyl group
is contained in I Ì W: In the other direction, the extended Weyl group certainly
contains W; since the Stiefel diagram contains the hyperplanes through the origin
orthogonal to the roots. Furthermore, from (13.9), we see that the composition of
s˛;1 and s˛;0 is translation by H˛: Thus, the extended Weyl group contains all of
I Ì W:
For Point 2, we ﬁrst argue that the orbits of I Ì W in t do not have accumulation
points. For any H 2 t; the orbit of H is the set of all vectors of the form w  H C ,
with w 2 W and  2 I: Since W is ﬁnite and I contains only ﬁnitely many points in
each bounded region, the orbit of H also contains only ﬁnitely many points in each
bounded region. Once this observation has been made, we may now repeat, almost
word for word, the proof that the ordinary Weyl group acts freely and transitively
on the set of open Weyl chambers. (Replace the hyperplanes orthogonal to the roots
with the hyperplanes L˛;n and replace the open Weyl chambers with the alcoves.)
The only point where a change is necessary is in the proof of the transitivity of the
action (Proposition 8.23). To generalize that argument, we need to know that for
each H and H 0 in t; the orbit .I Ì W /  H 0 contains a point at minimal distance
from H: Although the extended Weyl group is inﬁnite, since each orbit contains
only ﬁnitely many points in each bounded region, the result still holds. The reader is
invited to work through the proofs of Propositions 8.23 and 8.27 with the ordinary
Weyl group replaced by the extended Weyl group, and verify that no other changes
are needed.
ut
Theorem 13.35. The map ‰
W
treg  .K=T /
!
Kreg is a covering map.
Furthermore, if A  t is any one alcove, then ‰ maps A  .K=T / onto Kreg and
‰ W A  .K=T / ! Kreg
is also a covering map.
Recall the deﬁnition of ‰ in (13.5). We will see in the next section that K=T
is simply connected. Since each alcove A is a convex set, A is contractible and
therefore simply connected. We will thus conclude that A  .K=T / is the universal
cover of Kreg:
Proof. Recall the map ˆ W T  .K=T / ! K given by ˆ.t; Œx/ D xtx1; and
let Treg D Kreg \ T: Since the exponential map for T is a local diffeomorphism, it
follows from Proposition 13.24 that ˆ is a local diffeomorphism on Treg  .K=T /:
Furthermore, by a trivial extension of Proposition 13.26, we have ˆ.t; Œx/ D
ˆ.t0; Œx0/ if and only if there is some z 2 N.T / such that t0 D ztz1 and x0 D xz1:

394
13
Fundamental Groups of Compact Lie Groups
Finally, if W D N.T /=T acts on Treg  .K=T / by
w  .t; Œx/ D .ztz1; Œxz1/
for each w D Œz in W; then this action is free on K=T and thus free on Treg.K=T /:
Fix some y in Kreg and pick some .t; Œx/ in Treg  .K=T / for which
ˆ.t; Œx/ D y. Then since W is ﬁnite and acts freely, we can easily ﬁnd a
neighborhood U of .t; Œx/ such that w  U is disjoint from U for all w ¤ e in
W: It follows that the sets w  U are pairwise disjoint and thus that ˆ is injective
on each such set. Now, since ˆ is a local diffeomorphism, ˆ.U / will be open in
Kreg: The preimage of V is then the disjoint union of the sets w  U and the local
diffeomorphism ˆ will map each w  U homeomorphically onto V: We conclude
that ˆ is a covering map of Treg  .K=T / onto Kreg:
Meanwhile, as shown in the proof of Proposition 13.14, the exponential map for
T is a covering map. Thus, the map
.H; Œx/ 7! .e2H ; Œx/
is a covering map from treg  .K=T / onto Treg  .K=T /: Since the composition of
covering maps is a covering map, we conclude that ‰ W treg  .K=T / onto Kreg is a
covering map, as claimed.
Finally, Proposition 13.34 shows that each point in treg can be moved into A by
the action of I ÌW   ÌW: Thus, ‰ actually maps A.K=T / onto Kreg: For each
y 2 Kreg; choose a neighborhood V of y so that ‰1.V / is a disjoint union of open
sets U˛ mapping homeomorphically onto V: By shrinking V if necessary, we can
assume V is connected, in which case the U˛'s will also be connected. Thus, each
U˛ is either entirely in A  .K=T / or disjoint from A  .K=T /: Thus, if we restrict
‰ to A  .K=T /; the preimage of V will now consist of some subset of the U˛'s,
each of which still maps homeomorphically onto V; showing that the restriction of
‰ to A  .K=T / is still a covering map.
ut
13.7
Proofs of the Main Theorems
We now have all the necessary tools to attack the proofs of our main results. Here is
an outline of our strategy in this section. We will ﬁrst show that the quotient K=T is
always simply connected. On the one hand, the simple connectivity of K=T leads
to a proof of Theorem 13.15, that every loop in K is homotopic to a loop in T: On
the other hand, the simple connectivity of K=T means that the set A  .K=T / in
Theorem 13.35 is actually the universal cover of Kreg: Thus, to determine 1.K/ Š
1.Kreg/; we merely need to determine how close to being injective the covering
map ‰ W A  .K=T / ! Kreg is.
Now, according to Proposition 13.26, the failure of injectivity for the full map
‰ W treg  .K=T / is due to the action of the group  Ì W: If we restrict ‰ to

13.7
Proofs of the Main Theorems
395
A  .K=T /, then by Proposition 13.34, we have eliminated the failure of injectivity
due to the subgroup I Ì W   Ì W: Thus, the (possible) failure of injectivity of
‰ on A  .K=T / will be measured by the extent to which I fails to be all of :
Proposition 13.36. The quotient manifold K=T is simply connected.
Proof. Let Œx./ be any loop in K=T: Let H be a regular element in T and consider
the loop l in Kreg given by
l./ D x./e2H x./1:
Now let t.s/ be any path in T connecting e2H to I; and consider the loops
ls./ WD x./t.s/x./1:
Clearly, ls is a homotopy of l in K to the constant loop at I:
Thus, for any loop Œx./ in K=T; the corresponding loop l./ in Kreg is null
homotopic in K. We now argue that Œx./ itself is null homotopic in K=T: As a ﬁrst
step, we use Theorem 13.27 to deform the homotopy ls to a homotopy l0
s shrinking
l to a point in Kreg: Now, the map ‰ is a covering map and the loop .H; Œx.//
is a lift of l to A  .K=T /: Thus, as a second step, we can lift the homotopy l0
s
to A  .K=T / to a homotopy l00
s shrinking .H; Œx.// to a point in A  .K=T /:
(See Proposition 13.3.) Finally, as a third step, we can project the homotopy l00
s from
A  .K=T / onto K=T to obtain a homotopy shrinking Œx./ to a point in K=T: ut
Proposition 13.37. Every loop in K is homotopic to a loop in T:
Proof. By Proposition 13.8, the group K is a ﬁber bundle with base K=T and ﬁber
T: Suppose now that l is a loop in K and that l0./ WD Œl./ is the corresponding
loop in K=T: Since K=T is simply connected, there is a homotopy l0
s shrinking l0
to a point in K=T: Furthermore, since K=T is connected, it is harmless to assume
that l0
s shrinks to the point ŒI in K=T: Now, ﬁber bundles are known to have the
homotopy lifting property (Proposition 4.48 in [Hat]), which in our case means that
there is a homotopy ls in K such that l0 D l and such that Œls./ D l0
s./ for all 
and s: Since l0
1 is a constant loop at ŒI; the loop l1 lies in T:
ut
Lemma 13.38. Suppose  2  but  ... I: Then there exist 0 in I and w in W such
that the afﬁne transformation
H 7! w  .H C  C 0/
(13.10)
maps A to itself but is not the identity map of A:
Proof. By Proposition 13.31, translation by  maps the alcove A to some other
alcove A0: Then by Proposition 13.34, there exists an element of the extended Weyl
group that maps A0 back to A: Thus, there exist 0 2 I and w 2 W such that
the map (13.10) maps A to itself. If this map were the identity, then translation by
 would be (the inverse of) some element of I Ì W; which would mean that 

396
13
Fundamental Groups of Compact Lie Groups
H
H
H
Fig. 13.6 The element  is in the kernel of the exponential mapping for PSU.3/; but is not in the
coroot lattice. If we apply a rotation by 2=3 to H C ; we obtain an element H 0 in the same
alcove as H
would have to be in I; contrary to our assumption. Thus, the map in (13.10) sends
A to itself and is not the identity map on t: But an afﬁne transformation is certainly
determined by its restriction to any nonempty open set, which means that the map
cannot be the identity on A:
ut
Suppose now that  2  but  ... I: Let us choose 0 2 I and w 2 W as in
Lemma 13.38. We may then choose H 2 A so that H 0 WD w  .H C  C 0/ lies in
A but is distinct from H: (See Figure 13.6 in the case of the group PSU.3/:) Let p
denote the path in A given by
p./ D H C .H 0  H/;
0    1:
Now choose x 2 N.T / representing w, let x./ be a path connecting I to x in K;
and deﬁne a path q in A  .K=T / by
q./ D .p./; Œx./1/:
(13.11)
Note that p.1/ ¤ p.0/ and thus, certainly, q.1/ ¤ q.0/:
Lemma 13.39. Let q./ be the path in A  .K=T / given by (13.11). Then the path
 7! ‰.q.//
is a loop in Kreg and this loop is homotopic in K to the loop
 7! e2:

13.7
Proofs of the Main Theorems
397
Proof. On the one hand, we have
‰.q.0// D e2H:
On the other hand, since x1 represents w1; we have
‰.q.1// D x1e2H 0x
D e2.HCC0/
D e2H;
since  2  and 0 2 I  : Thus, ‰ ı q is a loop in Kreg; as claimed.
Meanwhile,
‰.q.// D exp
˚
2 x./1p./x./

;
where x.0/1p.0/x.0/ D H and x.1/1p.1/x.1/ D H C  C 0: Since the vector
space k is simply connected, the path
 7! x./1p./x./
in k is homotopic with endpoints ﬁxed to the straight-line path connecting H to
H C  C 0, namely
 7! H C . C 0/:
Since the exponential map for K is continuous, we see that ‰ ı q is homotopic in
K to the loop
 7! e2H e2.C0/:
We may then continuously deform e2H to the identity, showing that ‰ ı q is
homotopic in K to the loop  7! e2.C0/:
Finally, in light of Proposition 13.14, the loop  7! e2.C0/ is homotopic to
the composition (in either order) of the loop  7! e2 and the loop  7! e20:
But since 0 belongs to I; we have already shown in Sect. 13.4 that this second loop
is null homotopic in K, showing that ‰ ı q is homotopic in K to  7! e2; as
claimed.
ut
It now remains only to assemble the previous results to ﬁnish the proof of
Theorem 13.17.

398
13
Fundamental Groups of Compact Lie Groups
Proof of Theorem 13.17, the other direction. We showed in Sect. 13.4 that if  2 I
the loop  7! e2 is null homotopic in K: In the other direction, suppose that 
belongs to  but not to I: We may then construct the path q in A.K=T / in (13.11).
Although this path is not a loop in A  .K=T /; the path ‰ ı q is a loop in Kreg; by
Lemma 13.39. Since q is a lift of ‰ ı q and q has distinct endpoints, Corollary 13.4
tells us that ‰ ı q is not null homotopic in Kreg: Meanwhile, since ‰ ı q homotopic
in K to  7! e2 and 1.Kreg/ is isomorphic to 1.K/ (Theorem 13.27), we
conclude that  7! e2 is not null homotopic in K:
ut
Summary. It is instructive to summarize the arguments in the proofs of
Theorems 13.15 and 13.17, as described in this section and the previous three
sections. We introduced the regular set and the singular set in K and we showed
that the singular set has codimension at least 3. Using this, we proved a key result,
that 1.Kreg/ is isomorphic to 1.K/: Next, we introduced the Stiefel diagram and
the local diffeomorphism
‰ W treg  .K=T / ! Kreg:
We found that the failure of injectivity of ‰ on treg  .K=T / is measured by the
action of the group  ÌW; and that the subgroup I ÌW of  ÌW acts transitively on
the alcoves. We concluded that if A is any alcove, the map ‰ W A  .K=T / ! Kreg
is a covering map.
We then demonstrated that K=T is simply connected. We did this by mapping
any loop Œx./ in K=T to a loop l in K and constructing a homotopy ls of l to a
point in K: Since 1.Kreg/ D 1.K/; we can deform the homotopy ls into Kreg: We
can then use the covering map ‰ to lift the homotopy ls to A  .K=T / and project
back onto K=T to obtain a homotopy of Œx./ to a point in K=T: After establishing
that K=T is simply connected, we proved our ﬁrst main result, that every loop in K
is homotopic to a loop in T; as follows. We start with a loop l in K; push it down to
K=T and then homotope it to the identity coset. We then lift this homotopy to K;
giving a homotopy of l into T:
Since 1.T / is easily calculated, we conclude that every loop in K is homotopic
to a loop in T of the form  7! e2; for some  2 : When  2 I; we showed
that  7! e2 is the image under a continuous homomorphism of a loop in the
simply connected group SU.2/; which shows that  7! e2 is null homotopic.
When  ... I; we started with some H in A; translated by ; and then mapped back
to some H 0 2 A by the action of I Ì W; where H is chosen so that H 0 ¤ H: We
then constructed a path q in A  .K=T / with distinct endpoints such that ‰ ı q is
a loop in Kreg and is homotopic in K to  7! e2: Since ‰ W A  .K=T / ! Kreg
is a covering map and q has distinct endpoints, ‰ ı q is homotopically nontrivial in
Kreg: Then since 1.Kreg/ D 1.K/; we concluded that  7! e2 is homotopically
nontrivial in K:

13.8
The Center of K
399
13.8
The Center of K
In this section, we analyze the center of K using the tools developed in the previous
sections. If T is any one ﬁxed maximal torus in K; Corollary 11.11 tells us that T
contains the center Z.K/ of K: We now give a criterion for an element t D e2H of
T to be in Z.K/:
Proposition 13.40. If H 2 t, then e2H belongs to Z.K/ if and only if
h˛; Hi 2 Z
for all ˛ 2 R:
Proof. By Exercise 17 in Chapter 3, an element x of K is in Z.K/ if and only if
Adx.X/ D X for all X 2 k; or, equivalently, if and only if Adx.X/ D X for all
X 2 g D kC: Now, if X 2 g˛; then
Ade2H .X/ D e2adH .X/ D e2h˛;HiX:
Thus, Ade2H acts as the identity on g˛ if and only if h˛; Hi is an integer. Since g is
the direct sum of tC (on which Ade2H certainly acts trivially) and the g˛'s, we see
that e2H belongs to Z.K/ if and only if h˛; Hi 2 Z for all ˛:
ut
Deﬁnition 13.41. Let ƒ  t denote the root lattice, that is, the set of all integer
linear combinations of roots. Let ƒ denote the dual of the root lattice, that is,
ƒ D f 2 H jh; i 2 Z; 8 2 ƒg :
Note that if  2 t has the property that h˛; i 2 Z for every root ˛; then certainly
h; i 2 Z whenever  is an integer combination of roots. Thus, Proposition 13.40
may be restated as saying that e2H 2 Z.K/ if and only if H 2 ƒ: Note also that
if e2H D I; then certainly e2H is in the center of K: Thus, the kernel  of the
exponential map must be contained in ƒ:
Proposition 13.42. The map
 7! e2;
 2 ƒ;
is a homomorphism of ƒ onto Z.K/ with kernel equal to : Thus,
Z.K/ Š ƒ=;
where ƒ is the dual of the root lattice and  is the kernel of the exponential.

400
13
Fundamental Groups of Compact Lie Groups
Proof. As we have noted, Corollary 11.11 implies that Z.K/  T: Since the
exponential map for T is surjective, Proposition 13.40 tells us that the map  7!
e2 maps ƒ onto Z.K/: This map is a homomorphism since t is commutative,
and the kernel of the map is   ƒ:
ut
Suppose, for example, that K D T: Then there are no roots, in which case the
dual of root lattice is all of t: In this case, we have
Z.K/ D Z.T / Š t= Š T:
On the other hand, if g is semisimple, both ƒ and  will be discrete subgroups of
t that span t; in which case, ƒ= will be ﬁnite.
Note that we have several different lattices inside t: Some of these "really" live
in t and only become subsets of t when we use the inner product to identify t
with t: Other lattices naturally live in t itself. The lattices that really live in t
are the root lattice, the lattice of analytically integral elements, and the lattice of
algebraically integral elements. Meanwhile, the lattices that naturally live in t are
the coroot lattice, the kernel of the exponential map, and the dual of the root lattice.
Note that there is a duality relationship between the lattices in t and the lattices in t:
An element is algebraically integral if and only if its inner product with each coroot
is an integer; thus, the lattice of algebraically integral elements and the coroot lattice
are dual to each other. Similarly, the lattice of analytically integral elements is dual
to the kernel of the exponential map. Finally, ƒ is, by deﬁnition, dual to the root
lattice ƒ:
The lattices in t are included in one another as follows:
.root lattice/  .analytically integral elements/
 .algebraically integral elements/:
(13.12)
The dual lattices in t are then included in one another in the reverse order:
.coroot lattice/  .kernel of exponential/
 .dual of root lattice/:
(13.13)
In light of Proposition 13.42 and Corollary 13.18, we have the following isomor-
phisms involving quotients of lattices in (13.13):
.kernel of exponential/=.coroot lattice/ Š 1.K/
and
.dual of root lattice/=.kernel of exponential/ Š Z.K/:

13.8
The Center of K
401
Corollary 13.43. Let ƒ denote the dual of the root lattice and let I denote the
coroot lattice. If K is simply connected, then
Z.K/ Š ƒ=I:
On the other hand, if Z.K/ is trivial, then
1.K/ Š ƒ=I:
Let us deﬁne the adjoint group associated to K to be the image of K under
the adjoint representation Ad W K ! GL.k/: (Since K is compact, the adjoint
group of K is compact and thus closed.) If K is semisimple, then the center of k
is trivial, which means that the Lie algebra version of the adjoint representation,
ad W k ! gl.k/ is faithful. Thus, if g is semisimple, the Lie algebra of the adjoint
group is isomorphic to the Lie algebra k of K itself. On the other hand, it is not hard
to check (still assuming that g is semisimple and thus that the Lie algebra of the
adjoint group is isomorphic to k) that the center of the adjoint group is trivial. Thus,
whenever g is semisimple, we can construct a new group K0 where the ﬁrst part of
the corollary applies:
1.K0/ Š ƒ=I:
Proof. If 1.K/ is trivial, the kernel  of the exponential map must equal the coroot
lattice I, which means that
Z.K/ Š ƒ= D ƒ=I:
Meanwhile, if Z.K/ is trivial, then the kernel  of the exponential must equal the
dual ƒ of the root lattice, which means that
1.K/ Š =I D ƒ=I;
as claimed.
ut
Example 13.44. If K D SO.4/; then the lattices ƒ; ; and I are as in Figure 13.7
and both 1.K/ and Z.K/ are isomorphic to Z=2: Explicitly, Z.SO.4// D fI; Ig:
Proof. If we compute as in Sect. 7.7.2, but adjusting for a factor of i to obtain the
real roots and coroots, we ﬁnd that the coroots are the matrices
0
BB@
0 a
a 0
0 b
b 0
1
CCA ;
(13.14)

402
13
Fundamental Groups of Compact Lie Groups
dual of rootlattice
kernel of exponential
corootlattice
Fig. 13.7 The lattices for SO.4/; with the coroots indicated by arrows
where a
D
˙1 and b
D
˙1: We identify the coroots with the vectors
.a; b/ D .˙1; ˙1/ in R2: The coroot lattice I will then consist of pairs .m; n/ 2 Z2
for which m C n is even. The kernel  of the exponential, meanwhile, is easily seen
to consist of all pairs .m; n/ 2 Z2: Finally, since the coroots have been normalized to
have length
p
2; the roots and coroots coincide. Thus, the dual ƒ of the root lattice
is the set of vectors .x; y/ having integer inner product with .1; 1/ and .1; 1/; that
is, such that x C y and x  y are integers. Thus, either x and y are both integers,
or x and y are both integer-plus-one-half. It is then easy to check that both =I and
ƒ= are isomorphic to Z=2:
If H is any coroot, then H=2 belongs to ƒ but not to : Thus, the unique
nontrivial element of Z.SO.4// may be computed as e2.H=2/. Direct calculation
with (13.14) then shows that this unique nontrivial element is I 2 SO.4/:
ut
Example 13.45. Suppose that K is a connected, compact matrix Lie group with Lie
algebra k, that t is a maximal commutative subalgebra of k; and that the root system
of k relative to t is isomorphic to G2: Then both 1.K/ and Z.K/ are trivial.
It turns out that for compact groups with root systems of type An; Bn; Cn and
Dn; none of them is simultaneously simply connected and center free. Among the
exceptional groups, however, the groups with root systems G2; F4; and E8 all have
this property.
Proof. As we can see from Figure 8.11, each of the fundamental weights for G2 is
a root. Thus, every algebraically integral element for G2 (i.e., every integer linear
combination of the fundamental weights) is in the root lattice. Thus, all three of
the lattices in (13.12) must be equal. By dualizing this observation, we see that all
three of the lattices in (13.13) must also be equal. Thus, both 1.K/ Š =I and
Z.K/ Š ƒ= are trivial.
ut

13.9
Exercises
403
13.9
Exercises
In Exercises 1-4, we consider the maximal commutative subalgebra t of the relevant
Lie algebra k given in Sects. 7.7.1-7.7.4. In each case, we identify the Cartan
subalgebra h D tC with Cn by the map given in those sections, except that we
adjust the map by a factor of i; so that t maps to Rn: We also consider the real roots
and coroots, which differ by a factor of i from the roots and coroots in Chapter 7.
1. For the group SU.n/; n  1; show that coroot lattice I consists of all integer
n-tuples .k1; : : : ; kn/ for which k1 C    C kn D 0: Show that the kernel  of
the exponential is the same as the coroot lattice.
2. For the group SO.2n/; n  2; show that the coroot lattice I is the set of integer
linear combinations of vectors of the form ˙ej ˙ ek; with j ¤ k: Conclude
that the coroot lattice consists of all integer n-tuples .k1; k2; : : : ; kn/ for which
k1 C    C kn is even. Show that the kernel  of the exponential consists of all
integer n-tuples and that =I Š Z=2:
3. For the group SO.2n C 1/; n  1; show that the coroot lattice I is the set of
integer linear combinations of vectors of the form ˙2ej and ˙ej ˙ ek; with
j ¤ k: Conclude that, as for SO.2n/; the coroot lattice consists of all integer
n-tuples .k1; k2; : : : ; kn/ for which k1 C  Ckn is even and the kernel  of the
exponential consists of all integer n-tuples.
4. For the group Sp.n/; n  1; show that the coroot lattice I is the set of integer
linear combinations of vectors of the form ˙ej and ˙ej ˙ ek; with j ¤ k:
Conclude that both the coroot lattice and the kernel  of the exponential consist
of all integer n-tuples.
5. Verify the claims in Example 13.19.
Hints: In the case of SO.5/; make use of the calculations in the proof of
Example 12.13. In the case of PSU.3/; note that X 2 psu.3/ D su.3/
exponentiates to the identity in PSU.3/ if and only if X exponentiates in SU.3/
to a constant multiple of the identity.
6. Using Theorems 13.15 and 13.17 and the results of Exercises 2 and 3, show
that every homotopically nontrivial loop in SO.n/; n  3; is homotopic to the
loop
 7!
0
BBBBB@
cos   sin 
sin 
cos 
1
:::
1
1
CCCCCA
;
0    2:
(This result also follows from the inductive argument using ﬁber bundles, as
discussed following the proof of Proposition 13.10.)
7. Let G be a connected matrix Lie group. Using the following outline, show
that 1.G/ is commutative. Let A./ and B./ be any two loops in G based at

404
13
Fundamental Groups of Compact Lie Groups
the identity. Construct two families of loops ˛s./ (deﬁned in terms of A./)
and ˇs./ (deﬁned in terms of B./) with the property that ˛0.t/ˇ0.t/ is the
concatenation of A with B and ˛1.t/ˇ1.t/ is the concatenation of B with A:
(Here the product of, say, ˛0.t/ˇ0.t/ is computed in the group G:)
8. Suppose G1 and G2 are connected matrix Lie groups with Lie algebras g1
and g2; respectively, and that ˆ W G1 ! G2 is a Lie group homomorphism.
Show that if the associated Lie algebra homomorphism  W g1 ! g2 is an
isomorphism, then ˆ is a covering map (Deﬁnition 13.1).
9. Show that Proposition 13.26 fails if we do not assume that H 0 and H are in treg:
10. Let E be a real Euclidean space. Suppose V  E is the hyperplane through the
origin orthogonal to a nonzero vector ˛: Now suppose L is the hyperplane (not
necessarily through the origin) obtained by translating V by c˛: Let s W V ! V
be the afﬁne transformation given by
s.v/ D v  2 h˛; vi
h˛; ˛i˛ C 2c˛:
Show that if v 2 L and d 2 R; then
s.v C d˛/ D v  d˛:
That is to say, s is the reﬂection about L:
11. Let RC be the set of positive roots associated to a particular base 
; and let
˛1; : : : ; ˛N be the positive roots.
(a) Show that for each alcove A; there are integers n1; : : : ; nN such that
A D
˚
H 2 t
ˇˇnj <
˝
˛j ; H
˛
< nj C 1; j D 1; : : : ; N

:
(13.15)
(b) If n1; : : : ; nN is any sequence of integers, show that if the set A in (13.15)
is nonempty, then this set is an alcove.
12. In this exercise, we assume g WD kC is simple. Let RC be the positive roots
associated to a particular base 
; and let C be the fundamental Weyl chamber
associated to RC: Let A be the alcove containing the "bottom" of C; that is,
such that all very small elements of C are in A: Let ˇ be the highest root, that
is, the highest weight for the adjoint representation of g; which is irreducible
by assumption.
(a) Show that A may be described as
A D
˚
H 2 t
ˇˇ0 < h˛; Hi < 1; 8˛ 2 RC 
:
(Compare Exercise 11.)

13.9
Exercises
405
A2
B2
G2
Fig. 13.8 The extended Dynkin diagrams associated to A2; B2; and G2
(b) Show that A may also be described as
A D fH 2 t jhˇ; Hi < 1; h˛; Hi > 0; 8˛ 2 
g :
13. If g D kC is simple, the alcove A in Exercise 12 is a simplex, that is, a bounded
region in t Š Rk deﬁned by k C 1 linear inequalities. By the same argument
as in the proof of Proposition 8.24, the extended Weyl group is generated by
the k C 1 reﬂections about the walls of A: The structure of A (and thus of
the extended Weyl group) can be captured in the extended Dynkin diagram,
deﬁned as follows. The diagram has k C 1 vertices, representing the elements
˛1; : : : ; ˛k and ˇ: We then deﬁne edges and arrows by the same rules as for
ordinary Dynkin diagrams (Sect. 8.6). (Note that ˇ is at an obtuse angle with
each element of 
:)
Verify the extended Dynkin diagrams for A2; B2; and G2 in Figure 13.8,
where in each diagram, the black dot indicates the "extra" vertex ˇ added to
the ordinary Dynkin diagram. (Refer to Figures 13.3, 13.4, and 13.5.)
14. Let K be a connected compact matrix Lie group with Lie algebra k and let t be
a maximal commutative subalgebra of k: Let A  t be an alcove and suppose
A has no nontrivial symmetries. (That is to say, suppose there is no nontrivial
element of the Euclidean group of t mapping A onto A:)
(a) Show that K is simply connected.
(b) Show that if K0 is any connected matrix Lie group whose Lie algebra is
isomorphic to k; then K0 is isomorphic to K:
Note: It is known that there exists a connected compact group K of rank 2
whose root system is G2: Since the each alcove for G2 is a triangle with 3
distinct edge lengths, Part (b) of the problem shows that any such group K
must be simply connected. (Compare Example 13.45.)
15. Consider the quotient space T=W; that is, the set of orbits of W acting on T:
Let A  t be any one alcove and let NA be the closure of A in t: Show that if K
is simply connected, then the map H 7! Œe2H ; where Œt denotes the W -orbit
of t; is a bijection between NA and T=W:
Hint: Imitate the proof of Proposition 8.29 with the Weyl group W replaced by
the extended Weyl group I Ì W and the chamber C replaced by the alcove A.

Appendix A
Linear Algebra Review
In this appendix, we review results from linear algebra that are used in the text.
The results quoted here are mostly standard, and the proofs are mostly omitted. For
more information, the reader is encouraged to consult such standard linear algebra
textbooks as [HK] or [Axl]. Throughout this appendix, we let Mn.C/ denote the
space of n  n matrices with entries in C:
A.1
Eigenvectors and Eigenvalues
For any A 2 Mn.C/; a nonzero vector v in Cn is called an eigenvector for A if there
is some complex number  such that
Av D v:
An eigenvalue for A is a complex number  for which there exists a nonzero v 2
Cn with Av D v: Thus,  is an eigenvalue for A if the equation Av D v or,
equivalently, the equation
.A  I/v D 0;
has a nonzero solution v: This happens precisely when A  I fails to be invertible,
which is precisely when det.A  I/ D 0: For any A 2 Mn.C/; the characteristic
polynomial p of A is given by
p./ D det.A  I/;
 2 C:
© Springer International Publishing Switzerland 2015
B. Hall, Lie Groups, Lie Algebras, and Representations, Graduate
Texts in Mathematics 222, DOI 10.1007/978-3-319-13467-3
407

408
A
Linear Algebra Review
This polynomial has degree n: In light of the preceding discussion, the eigenvalues
are precisely the zeros of the characteristic polynomial.
We can deﬁne, more generally, the notion of eigenvector and eigenvalue for any
linear operator on a vector space. If V is a ﬁnite-dimensional vector space over C (or
over any algebraically closed ﬁeld), every linear operator on V will have a least one
eigenvalue. If A is a linear operator on a vector space V and  is an eigenvalue for
A; the -eigenspace for A; denoted V; is the set of all vectors v 2 V (including the
zero vector) that satisfy Av D v: The -eigenspace for A is a subspace of V: The
dimension of this space is called the multiplicity of : (More precisely, this is the
"geometric multiplicity" of : In the ﬁnite-dimensional case, there is also a notion
of the "algebraic multiplicity" of ; which is the number of times that  occurs as a
root of the characteristic polynomial. The geometric multiplicity of  cannot exceed
the algebraic multiplicity).
Proposition A.1. Suppose that A is a linear operator on a vector space V and
v1; : : : ; vk are eigenvectors with distinct eigenvalues 1; : : : ; k: Then v1; : : : ; vk
are linearly independent.
Note that here V does not have to be ﬁnite dimensional.
Proposition A.2. Suppose that A and B are linear operators on a ﬁnite-
dimensional vector space V and suppose that AB D BA: Then for each eigenvalue
 of A; the operator B maps the -eigenspace of A into itself.
Proof. Let  be an eigenvalue of A and let V be the -eigenspace of A: Then let v
be an element of V and consider Bv: Since B commutes with A; we have
A.Bv/ D BAv D Bv,
showing that Bv is in V:
ut
A.2
Diagonalization
Two matrices A; B 2 Mn.C/ are said to be similar if there exists an invertible
matrix C such that
A D CBC 1:
The operation B ! CBC 1 is called conjugation of B by C: A matrix is said
to be diagonalizable if it is similar to a diagonal matrix. A matrix A 2 Mn.C/ is
diagonalizable if and only if there exist n linearly independent eigenvectors for A:
Speciﬁcally, if v1; : : : ; vn are linearly independent eigenvectors, let C be the matrix
whose kth column is vk: Then C is invertible and we will have

A.3
Generalized Eigenvectors and the SN Decomposition
409
A D C
0
B@
1
:::
n
1
CA C 1;
(A.1)
where 1; : : : ; n are the eigenvalues associated to the eigenvectors v1; : : : ; vn; in
that order. To verify (A.1), note that C maps the standard basis element ej to vj:
Thus, C 1 maps vj to ej; the diagonal matrix on the right-hand side of (A.1) then
maps ej to j ej; and C maps j ej to jvj : Thus, both sides of (A.1) map vj to
jvj ; for all j:
If A 2 Mn.C/ has n distinct eigenvalues (i.e., n distinct roots to the characteristic
polynomial), A is necessarily diagonalizable, by Proposition A.1. If the characteris-
tic polynomial of A has repeated roots, A may or may not be diagonalizable.
For A 2 Mn.C/; the adjoint of A; denoted A; is the conjugate-transpose of A;
.A/jk D Akj:
(A.2)
A matrix A is said to be self-adjoint (or Hermitian) if A D A: A matrix A is
said to be skew self-adjoint (or skew Hermitian) if A D A: A matrix is said to
be unitary if A D A1: More generally, A is said to be normal if A commutes
with A: If A is normal, A is necessarily diagonalizable, and, indeed, it is possible
to ﬁnd an orthonormal basis of eigenvectors for A: In such cases, the matrix C in
(A.1) may be taken to be unitary.
If A is self-adjoint, all of its eigenvalues are real. If A is real and self-adjoint
(or, equivalently, real and symmetric), the eigenvectors may be taken to be real as
well, which means that in this case, the matrix C may be taken to be orthogonal. If
A is skew, then its eigenvalues are imaginary. If A is unitary, then its eigenvalues
are complex numbers of absolute value 1.
We summarize the results of the previous paragraphs in the following.
Theorem A.3. Suppose that A 2 Mn.C/ has the property that AA D AA; (e.g.,
if A D A; A D A1; or A D A). Then A is diagonalizable and it is possible
to ﬁnd an orthonormal basis for Cn consisting of eigenvectors for A: If A D A;
all the eigenvalues of A are real; if A D A; all the eigenvalues of A are pure
imaginary; and if A D A1; all the eigenvalue of A have absolute value 1.
A.3
Generalized Eigenvectors and the SN Decomposition
Not all matrices are diagonalizable, even over C: If, for example,
A D
 1 1
0 1

;
(A.3)

410
A
Linear Algebra Review
then the only eigenvalue of A is 1, and every eigenvector with eigenvalue 1 is of the
form .c; 0/: Thus, we cannot ﬁnd two linearly independent eigenvectors for A: It is
not hard, however, to prove the following result.
Theorem A.4. Every matrix is similar to an upper triangular matrix. Every
nilpotent matrix is similar to an upper triangular matrix with zeros on the diagonal.
While Theorem A.4 is sufﬁcient for some purposes, we will in general need
something that comes a bit closer to a diagonal representation. If A 2 Mn.C/
does not have n linearly independent eigenvectors, we may consider the more
general concept of generalized eigenvectors. A nonzero vector v 2 Cn is called a
generalized eigenvector for A if there is some complex number  and some positive
integer k such that
.A  I/kv D 0:
(A.4)
If (A.4) holds for some v ¤ 0, then .AI/ cannot be invertible. Thus, the number
 must be an (ordinary) eigenvalue for A: However, for a ﬁxed eigenvalue ; there
may be generalized eigenvectors v that are not ordinary eigenvectors. In the case
of the matrix A in (A.3), for example, the vector .0; 1/ is a generalized eigenvector
with eigenvalue 1 (with k D 2).
It can be shown that every A 2 Mn.C/ has a basis of generalized eigenvectors.
For any matrix A and any eigenvalue  for A; let W be the generalized eigenspace
with eigenvalue :
W D fv 2 Cn ˇˇ.A  I/kv D 0 for some k g:
Then Cn decomposes as a direct sum of the W's, as  ranges over all the
eigenvalues of A: Furthermore, the subspace W is easily seen to be invariant
under the matrix A. Let A denote the restriction of A to the subspace W; and
let N D A  I; so that
A D I C N:
Then N is nilpotent; that is, N k
 D 0 for some positive integer k: We summarize
the preceding discussion in the following theorem.
Theorem A.5. Let A be an n  n complex matrix. Then there exists a basis for Cn
consisting of generalized eigenvectors for A: Furthermore, Cn is the direct sum of
the generalized eigenspaces W: Each W is invariant under A; and the restriction
of A to W is of the form I C N; where N is nilpotent.
The preceding result is the basis for the following decomposition.
Theorem A.6. Each A 2 Mn.C/ has a unique decomposition as A D S CN where
S is diagonalizable, N is nilpotent, and SN D NS:

A.5
The Trace
411
The expression A D S C N; with S and N as in the theorem, is called the
SN decomposition of A: The existence of an SN decomposition follows from the
previous theorem: We deﬁne S to be the operator equal to I on each generalized
eigenspace W of A and we set N to be the operator equal to N on each W. For
example, if A is the matrix in (A.3), then we have
S D
1 0
0 1

;
N D
0 1
0 0

:
A.4
The Jordan Canonical Form
The Jordan canonical form may be viewed as a reﬁnement of the SN decomposition,
based on a further analysis of the nilpotent matrices N in Theorem A.5.
Theorem A.7. Every A 2 Mn.C/ is similar to a block-diagonal matrix in which
each block is of the form
0
BBBB@
 1
 :::
::: 1

1
CCCCA
:
Two matrices A and B are similar if and only if they have precisely the same Jordan
blocks, up to reordering.
There may be several different Jordan blocks (possibly of different sizes) for the
same value of : In the case in which A is diagonalizable, each block is 1  1; in
which case, the ones above the diagonal do not appear. Note that each Jordan block
is, in particular, of the form I C N; where N is nilpotent.
A.5
The Trace
For A 2 Mn.C/, we deﬁne the trace of A to be the sum of the diagonal entries of A:
trace.A/ D
n
X
kD1
Akk:
Note that the trace is a linear function of A. For A; B 2 Mn.C/; we note that
trace.AB/ D
n
X
kD1
.AB/kk D
n
X
kD1
n
X
lD1
AklBlk:
(A.5)

412
A
Linear Algebra Review
If we similarly compute trace.BA/; we obtain the same sum with the labels for the
summation variables reversed. Thus,
trace.AB/ D trace.BA/:
(A.6)
If C is an invertible matrix and we apply (A.6) to the matrices CA and C 1; we have
trace.CAC 1/ D trace.C 1CA/ D trace.A/;
that is, similar matrices have the same trace.
More generally, if A is a linear operator on a ﬁnite-dimensional vector space V;
we can deﬁne the trace of A by picking a basis and deﬁning the trace of A to be the
trace of the matrix that represents A in that basis. The above calculations show that
the value of the trace of A is independent of the choice of basis.
A.6
Inner Products
Let h; i denote the standard inner product on Cn; deﬁned by
hx; yi D
n
X
j D1
xj yj ;
where we follow the convention of putting the complex-conjugate on the ﬁrst factor.
We have the following basic result relating the inner product to the adjoint of a
matrix, as deﬁned in (A.2).
Proposition A.8. For all A 2 Mn.C/; the adjoint A of A has the property that
hx; Ayi D hAx; yi
(A.7)
for all x; y 2 Cn:
Proof. We compute that
hx; Ayi D
n
X
j D1
xj
n
X
kD1
Ajkyk
D
n
X
j D1
n
X
kD1
Ajkxj yk
D
n
X
j D1
n
X
kD1
A
kjxj yk:
This last expression is just the inner product of Ax with y:
ut

A.6
Inner Products
413
We may generalize the notion of inner product as follows.
Deﬁnition A.9. If V is any vector space over C, an inner product on V is a map
that associates to any two vectors u and v in V a complex number hu; vi and that
has the following properties:
1. Conjugate symmetry: hv; ui D hu; vi for all u; v 2 V:
2. Linearity in the second factor: hu; v1 C av2i D hu; v1i C a hu; v2i ; for all
u; v1; v2 2 V and a 2 C:
3. Positivity: For all v 2 V; the quantity hv; vi is real and satisﬁes hv; vi  0; with
hv; vi D 0 only if v D 0:
Note that in light of the conjugate-symmetry and the linearity in the second
factor, an inner product must be conjugate-linear in the ﬁrst factor:
hv1 C av2; ui D hv1; ui C Na hv2; ui :
(Some authors deﬁne an inner product to be linear in the ﬁrst factor and conjugate
linear in the second factor.) An inner product on a real vector space is deﬁned in the
same way except that conjugate symmetry is replaced by symmetry (hv; ui D hu; vi)
and the constant a in Point 2 now takes only real values.
If V is a vector space with inner product, the norm of a vector v 2 V , denoted
kvk ; is deﬁned by
kvk D
p
hv; vi:
The positivity condition on the inner product guarantees that kvk is always a non-
negative real number and that kvk D 0 only if v D 0: If, for example, V D Mn.C/;
we may deﬁne the Hilbert-Schmidt inner product by the formula
hA; Bi D trace.AB/:
(A.8)
It is easy to see check that this expression is conjugate symmetric and linear in the
second factor. Furthermore, we may compute as in (A.5) that
trace.AA/ D
n
X
k;lD1
A
klAlk D
n
X
k;lD1
jAklj2  0;
and the sum is zero only if each entry of A is zero. The associated Hilbert-Schmidt
norm satisﬁes
kAk2 D
n
X
k;lD1
jAklj2 :

414
A
Linear Algebra Review
Suppose that V is a ﬁnite-dimensional vector space with inner product and that
W is a subspace of V: Then the orthogonal complement of W; denoted W ?; is
the set of all vectors v in V such that hw; vi D 0 for all w in W: The space V then
decomposes as the direct sum of W and W ?:
We now introduce the abstract notion of the adjoint of a matrix.
Proposition A.10. Let V be a ﬁnite-dimensional vector space with an inner
product h; i : If A is a linear map from V to V; there is a unique operator
A W V ! V such that
hu; Avi D hAu; vi
for all u; v 2 V: Furthermore, if W is a subspace of V that is invariant under A;
then W ? is invariant under A:
A.7
Dual Spaces
If V is a vector space over C; a linear functional on V is a linear map of V into
C: If v1; : : : ; vn is a basis for V; then for each set of constants a1; : : : ; an; there
is a unique linear functional  such that .vk/ D ak: If V is a ﬁnite-dimensional
complex vector space, then the dual space to V; denoted V ; is the set of all linear
functionals on V: The dual space is also a vector space and its dimension is the same
as that of V:
If W is a subspace of a vector space V; the annihilator subspace of W; denoted
W ^; is the set of all  in V  such that .w/ D 0 for all w in W: Then W ^ is a
subspace of V : If V is ﬁnite dimensional, then
dim W C dim W ^ D dim V
and the map W ! W ^ provides a one-to-one correspondence between subspaces
of V and subspaces of V :
In general, one should be careful to distinguish between a vector space and its
dual. Nevertheless, when V is ﬁnite dimensional and has an inner product, we can
produce an identiﬁcation between V and V :
Proposition A.11. Let V be a ﬁnite-dimensional inner product space and let  be
a linear functional on V: Then there exists a unique w 2 V such that
.v/ D hw; vi
for all v 2 V:
Recall that we follow the convention that inner products are linear in the second
factor, so that hw; vi is, indeed, linear in v:

A.8
Simultaneous Diagonalization
415
A.8
Simultaneous Diagonalization
We now extend the notion of eigenvectors and diagonalization to families of linear
operators.
Deﬁnition A.12. Let V be a vector space and let A be a collection of linear
operators on V: A nonzero vector v 2 V is a simultaneous eigenvector for A if
for all A 2 A; there exists a constant A such that Av D Av: The numbers A are
the simultaneous eigenvalues associated to v:
Consider, for example, the space D of all diagonal n  n matrices. Then for each
k D 1; : : : ; n; the standard basis element ek is a simultaneous eigenvector for D:
For each diagonal matrix A; the simultaneous eigenvalue associated to ek is the kth
diagonal entry of A:
Proposition A.13. If A is a commuting family of linear operators on a ﬁnite-
dimensional complex vector space, then A has at least one simultaneous eigen-
vector.
It is essential here that the elements of A commute; noncommuting families of
operators typically have no simultaneous eigenvectors.
In many cases, the collection A of operators on V is a subspace of End.V /;
the space of all linear operators from V to itself. In that case, if v is a simultaneous
eigenvector for A, the eigenvalues A for v depend linearly on A: After all, if A1v D
1v and A2v D 2v, then
.A1 C cA2/v D .1 C c2/v:
The preceding discussion leads to the following deﬁnition.
Deﬁnition A.14. Suppose that V is a vector space and A is a vector space of linear
operators on V: A weight for A is a linear functional  on A such that there exists
a nonzero vector v 2 V satisfying
Av D .A/v
for all A in A: For a ﬁxed weight ; the set of all vectors v 2 V satisfying Av D
.A/v for all A in A is called the weight space associated to the weight :
That is to say, a weight is a set of simultaneous eigenvalues for the operators in
A: If V is ﬁnite dimensional and the elements of A all commute with one another,
then there will exist at least one weight for A:
If A is ﬁnite dimensional and comes equipped with an inner product, it is
convenient to express the linear functional  in Deﬁnition A.14 as the inner product
of A with some vector, as in Proposition A.11. From this point of view, we deﬁne

416
A
Linear Algebra Review
a weight to be an element  of A (not A) such that there exists a nonzero v in V
with
Av D h; Ai v
for all A 2 A:
Deﬁnition A.15. Suppose that V is a ﬁnite-dimensional vector space and A is
some collection of linear operators on V: Then the elements of A are said to be
simultaneously diagonalizable if there exists a basis v1; : : : ; vn for V such that
each vk is a simultaneous eigenvector for A:
If A is a vector space of linear operators on V , then saying that the elements of A
are simultaneously diagonalizable is equivalent to saying that V can be decomposed
as a direct sum of weight spaces of A:
If a collection A of operators is simultaneously diagonalizable, then the elements
of A must commute, since they commute when applied to each vk: Conversely, if
each A 2 A is diagonalizable by itself and if the elements of A commute, then
(it can be shown), the elements of A are simultaneously diagonalizable. We record
these results in the following proposition.
Proposition A.16. If A is a commuting collection of linear operators on a ﬁnite-
dimensional vector space V and each A 2 A is diagonalizable, then the elements
of A are simultaneously diagonalizable.
We close this appendix with an analog of Proposition A.1 for simultaneous
eigenvectors.
Proposition A.17. Suppose V is a vector space and A is a vector space of linear
operators on V: Suppose 1; : : : ; m are distinct weights for A and v1; : : : ; vm are
elements of the corresponding weight spaces. If v1 C    C vm D 0; then vj D 0 for
all j D 1; : : : ; m: Furthermore, if v1 C    C vm is a weight vector with weight ,
then  D j for some j and vk D 0 for all k ¤ j:
Since this result is not quite standard, we provide a proof.
Proof. Assume ﬁrst that v1 C  Cvm D 0; with vj in the weight space with weight
j : If m D 1; then we have v1 D 0; as claimed. If m > 1; choose some A 2 A such
that 1.A/ ¤ 2.A/: If we then apply the operator A  2.A/I to v1 C    C vm;
we obtain
0 D
m
X
j D1
.j .A/  2.A//vj :
(A.9)
Now, the j D 2 term in (A.9) is zero, so that the sum actually contains at most m1
nonzero terms. Thus, by induction on m; we can assume that each term in (A.9) is
zero. In particular, .1.A/2.A//v1 D 0; which implies (by our choice of A) that

A.8
Simultaneous Diagonalization
417
v1 D 0: Once v1 is known to be zero, the original sum v1 C    C vm contains at
most m  1 nonzero terms. Thus, using induction on m again, we see that each term
in the sum is zero.
Assume now that v WD v1 C    C vm is a (nonzero) weight vector with some
weight ; and choose some j for which vj ¤ 0: Then for each A 2 A; we have
0 D Av  .A/v D
m
X
kD1
.k.A/  .A//vk:
Thus, by the ﬁrst part of the proposition, we must have .k.A/  .A//vk D 0 for
all j: Taking k D j; we conclude that j .A/  .A/ D 0: Since this result holds
for all A 2 A; we see that  D j : Finally, for any k ¤ j; we can choose A 2 A
so that k.A/ ¤ j .A/: With this value of A (and with  D j ), the fact that
.k.A/  j .A//vk D 0 forces vk to be zero.
ut

Appendix B
Differential Forms
In this section, we give a very brief outline of the theory of differential forms on
manifolds. Since this is all we require, we consider only top-degree forms, that k-
forms on k-dimensional manifolds. See Chapter 16 in [Lee] for more information.
We begin by considering forms at a single point, which is just a topic in linear
algebra.
Deﬁnition B.1. If V is a k-dimensional real vector space, a map ˛ W V k ! R is
said to be k-linear and alternating if (1) ˛.v1; : : : ; vk/ is linear with respect to vj
with each the other variables ﬁxed, and (2) ˛ changes sign whenever any two of the
variables are interchanged:
˛.v1; : : : ; vl; : : : ; vm; : : : vk/ D ˛.v1; : : : ; vm; : : : ; vl; : : : vk/:
It is a standard result in linear algebra (e.g., Theorem 2 in Section 5.3 of [HK])
that every k-dimensional real vector space admits a nonzero k-linear, alternating
form, and that any two such forms differ by multiplication by a constant. If T W
V ! V is a linear map and ˛ is a k-linear, alternating form on V; then for any
v1; : : : ; vk 2 V; we have
˛.Tv1; : : : ; Tvk/ D .det T /˛.v1; : : : ; vk/:
(B.1)
If v1; : : : ; vk and w1; : : : ; wk are two ordered bases for V; then there is a unique
invertible linear transformation T W V ! V such that Tvj D wj : We may divide the
collection of all ordered bases of V into two groups, where two ordered bases belong
to the same group if the linear map relating them has positive determinant and the
two bases belong to different groups if the linear map relating them has negative
determinant. An orientation of V is then a choice of one of the two groups of bases.
Once an orientation of V has been chosen, we say that a basis is positively oriented
if it belongs to the chosen group of bases. If ˛ is a nonzero k-linear, alternating form
© Springer International Publishing Switzerland 2015
B. Hall, Lie Groups, Lie Algebras, and Representations, Graduate
Texts in Mathematics 222, DOI 10.1007/978-3-319-13467-3
419

420
B
Differential Forms
on V; we can deﬁne an orientation of V by decreeing an ordered basis v1; : : : ; vk to
be positively oriented if ˛.v1; : : : ; vk/ > 0:
The following example of a k-linear, alternating form on Rk will help motivate
the notion of a k-form. For any vectors v1; : : : ; vk in Rk; deﬁne the parallelepiped
Pv1;:::;vk spanned by these vectors as follows:
Pv1;:::;vk D fc1v1 C    C ckvkj0  cl  1g :
(B.2)
(If k D 2; then a parallelepiped is just a parallelogram.) Let us use the orientation
on Rk in which the standard basis e1; : : : ; ek is positively oriented.
Example B.2. Deﬁne a map V W .Rk/k ! R by
V.v1; : : : ; vk/ D ˙Vol.Pv1;:::;vk/;
(B.3)
where we take a plus sign if v1; : : : ; vk is a positively oriented basis for Rk and a
minus sign if it is a negatively oriented basis. Then V is a k-linear, alternating form
on Rk:
Note that the volume of Pv1;:::;vk is zero if v1; : : : ; vk do not form a basis for
Rk, in which case, we do not have to worry about the sign on the right-hand side
of (B.3). Now, it is known that the volume of Pv1;:::;vk is equal to jdet T j ; where T
is the k  k matrix whose columns are the vectors v1; : : : ; vk: This claim is a very
special case of the change-of-variables theorem in multivariate calculus and can be
proved by expressing T as a product of elementary matrices. We can then see that
V.v1; : : : ; vk/ is equal to det T (without the absolute value signs). Meanwhile, it is a
standard result from linear algebra that the determinant of T is a k-linear, alternating
function of its column vectors v1; : : : ; vk:
We now turn to a discussion of top-degree forms on manifolds. If M is a k-
dimensional manifold (say, embedded into some RN ), we have the notion of the
tangent space to M at m; denoted TmM; which is a k-dimensional subspace of RN :
Deﬁnition B.3. Suppose M is a smoothly embedded, k-dimensional submanifold
of RN for some k; N: A k-form ˛ on M is a smoothly varying family ˛m of k-linear,
alternating maps on TmM; one for each m 2 M:
To be precise, let us say that a family ˛m of k-linear, alternating forms on each
TmM is "smoothly varying" if the following condition holds. Suppose X1; : : : ; Xk
are smooth RN -valued functions on RN with the property that for each m 2 M; the
vector Xj.m/ is tangent to M: Then the function ˛m.X1.m/; : : : ; Xk.m//; m 2 M;
should be a smooth function on M:
The "purpose in life" of a k-form ˛ on a k-dimensional manifold M is to
be integrated over regions in M: More precisely, we must assume that M is
orientable—meaning that it is possible to choose an orientation of each tangent
space TmM in a way that varies continuously with m—and that we have chosen
an orientation on M: Then if E is a "nice" subset of M (to be precise, a compact
k-dimensional submanifold with boundary), there is a notion of the integral of ˛

B
Differential Forms
421
F
M
v2
v1
Fig. B.1 The integral of ˛ over the small region F  M is approximately equal to ˛.v1; v2/
over E  M; denoted
Z
E
˛:
The value of
R
E ˛ may be thought of as assigning a sort of (possibly negative)
"volume" to the set E: If ˛ is a k-form on M and f W M ! R is a smooth
function, then f ˛ is also a k-form on M; which may also be integrated, using the
same orientation we used to integrate ˛.
We may gain an intuitive understanding of the notion of integration of k-forms
as follows. For any region E  M; we may think of chopping E up into very small
subregions F , each of which is shaped like a parallelepiped (as in (B.2)). More
speciﬁcally, each subregion will look like the parallelepiped spanned by tangent
vectors v1; : : : ; vk at some point m 2 E; which we can arrange to be positively
oriented. The idea is then that the integral of ˛ over each subregion should be
approximately ˛m.v1; : : : ; vk/. (See Figure B.1.) The integral of ˛ over all of E
should then be the sum of its integrals over the subregions.
If we think of
R
E ˛ as a sort of volume of the set E; then ˛m.v1; : : : ; vk/
represents the volume (possibly with a minus sign) of a small parallelepiped-shaped
subregion inside E: Example B.2 then makes it natural that we should require
˛m.v1; : : : ; vk/ to be k-linear and alternating.
We may give a more precise deﬁnition of the integral of a differential form as
follows. We choose a local coordinate system x1; : : : ; xk on our oriented manifold
M; deﬁned in some open set U: We then let @=@x1; : : : ; @=@xk denote the associated
basis for the tangent space at each point. (In coordinates, @=@xj is the unit vector
in the xj-direction.) We assume the coordinate system is "oriented," meaning that
@=@x1; : : : ; @=@xk is an oriented basis for the tangent space at each point in U:

422
B
Differential Forms
Deﬁnition B.4. Let ˛ be a k-form on a oriented k-dimensional manifold M and
suppose E  M is a compact subset of the domain U of fxj g: We then deﬁne
R
E ˛ as
Z
E
˛ D
Z
E
˛
 @
@x1
; : : : ; @
@xk

dx1 dx2    dxk;
(B.4)
where the integral on the right-hand side of (B.4) is an ordinary integral in Euclidean
space.
The integral on the right-hand side of (B.4) may be deﬁned as a Riemann integral
or using Lebesgue measure on Rk. A key point in the deﬁnition is to verify that the
value of
R
E ˛ is independent of the choice of coordinates. To this end, suppose fykg
is another oriented coordinate system whose domain includes E: Then by the chain
rule, we have
@f
@xl
D
X
m
@f
@ym
@ym
@xl
for any smooth function f: That is to say,
@
@xl
D
X
m
@ym
@xl
@
@ym
:
Thus, if T is the matrix whose entries are Tlm D @ym=@xl; we will have, by (B.1),
˛
 @
@x1
; : : : ; @
@xk

D .det T /
 @
@y1
; : : : ; @
@yk

:
On the other hand, the classical change of variables theorem says that
Z
E
f .x1; : : : ; xk/ dx1 dx2    dxk D
Z
E
f .y1; : : : ; yk/ J dy1 dy2    dyk;
where J is the determinant of the matrix f@xm=@ylg: (For example, in the k D 1
case, J is just dx=dy; which is obtained by writing dx D .dx=dy/ dy:) But by the
chain rule again, the matrix f@xm=@ylg is the inverse of the matrix f@ym=@xlg: Thus,
J is the reciprocal of det T; and we see that
Z
E
˛
 @
@x1
; : : : ; @
@xk

dx1 dx2    dxk
D
Z
E
˛
 @
@y1
; : : : ; @
@yk

dy1 dy2    dyk;
as claimed.

B
Differential Forms
423
Note that if we think of the integral in (B.4) as a Riemann integral, we
compute the integral by covering E with small k-dimensional "rectangles," and
these rectangles may be thought of as being "spanned" by multiples of the vectors
@=@x1; : : : ; @=@xk: In the Riemann integral, the integral of ˛ over each small
rectangle is being approximated by ˛.@=@x1; : : : ; @=@xk/ times the volume of the
rectangle, in agreement with preceding intuitive description of the integral.
If we wish to integrate a k-form ˛ over a general k-dimensional, compact subset
E of M; we use a partition of unity to write ˛ as a sum of forms ˛j , each of which
is supported in a small region in M: For each j; we choose a coordinate system
deﬁned on a set Uj containing the support of ˛j : We then integrate ˛j over E \ Uj
and sum over j:

Appendix C
Clebsch-Gordan Theory
and the Wigner-Eckart Theorem
C.1
Tensor Products of sl.2I C/ Representations
The irreducible representations of SU.2/ (or, equivalently, of sl.2I C/) were classi-
ﬁed in Sect. 4.6 and may be realized in spaces of homogeneous polynomials in two
complex variables as in Example 4.10. For each non-negative integer m; we have
an irreducible representation .m; Vm/ of sl.2I C/ of dimension m C 1; and every
irreducible representation of sl.2I C/ is isomorphic to one of these. We are using
here the mathematicians' labeling of the representations; in the physics literature,
the representations are labeled by the "spin" l WD m=2:
By the averaging method of Sect. 4.4, we can ﬁnd on each space Vm an inner
product that is invariant under the action of the compact group SU.2/: (In the case
of V1 Š C2; we can use the standard inner product on C2 and for any m; it is not hard
to describe such an inner product explicitly.) With respect to such an inner product,
the orthogonal complement of a subspace invariant under SU.2/ (or, equivalently,
under sl.2I C/) is again invariant under SU.2/. Since the element H D diag.1; 1/
of sl.2I C/ is in isu.2/; m.H/ will be self-adjoint with respect to this inner product.
Thus, eigenvectors of m.H/ with distinct eigenvectors must be orthogonal.
Recall from Sect. 4.3.2 the notion of the tensor product of representations of a
group or Lie algebra. We consider this in the case of the irreducible representations
of sl.2I C/: We regard the tensor product Vm ˝ Vn as a representation of sl.2I C/:
(Recall that it is also possible to view Vm ˝ Vn as a representation of sl.2I C/ ˚
sl.2I C/:) The action of sl.2I C/ on Vm ˝ Vn is given by
.m ˝ n/.X/ D m.X/ ˝ I C I ˝ n.X/:
(C.1)
We compute in the standard basis fX; Y; Hg for sl.2I C/: Once we have chosen
SU.2/-invariant inner products on Vm and Vn; there is a unique inner product
© Springer International Publishing Switzerland 2015
B. Hall, Lie Groups, Lie Algebras, and Representations, Graduate
Texts in Mathematics 222, DOI 10.1007/978-3-319-13467-3
425

426
C
Clebsch-Gordan Theory and the Wigner-Eckart Theorem
on Vm ˝ Vn with the property that hu1 ˝ v1; u2 ˝ v2i D hu1; u2i hv1; v2i : (This
assertion can be proved using the universal property of tensor products.) The inner
product on Vm ˝ Vn is also invariant under the action of SU.2/: We assume in the
rest of this section that an inner product of this sort has been chosen on each Vm˝Vn:
In general, Vm ˝ Vn will not be an irreducible representation of sl.2I C/; the
goal of this section is to describe how Vm ˝ Vn decomposes as a direct sum of
irreducible invariant subspaces. This decomposition is referred to as the Clebsch-
Gordan theory. Let us consider ﬁrst the case of V1 ˝ V1; where V1 D C2; the
standard representation of sl.2I C/: If fe1; e2g is the standard basis for C2; then
the vectors of the form ek ˝ el; 1  k; l  2; form a basis for C2 ˝ C2: Since
e1 and e2 are eigenvalues for 1.H/ with eigenvalues 1 and 1; respectively, then,
by (C.1), the basis elements for C2 ˝ C2 are eigenvectors for the action of H with
eigenvalues 2; 0; 0; and 2; respectively. Since 2 is the largest eigenvalue for H; the
corresponding eigenvector e1 ˝ e1 must be annihilated by X (i.e., by the operator
1.X/ ˝ I C I ˝ 1.X/).
If then we apply Y (i.e., by the operator 1.Y / ˝ I C I ˝ 1.Y /) repeatedly to
e1 ˝ e1; we obtain e1 ˝ e2 C e2 ˝ e1; then 2e2 ˝ e2; and then the zero vector. The
space spanned by these vectors is invariant under sl.2I C/ and irreducible, and is
isomorphic to the three-dimensional representation V2: The orthogonal complement
of this space in C2 ˝ C2, namely the span of e1 ˝ e2  e2 ˝ e1; is also invariant, and
sl.2I C/ acts trivially on this space. Thus,
C2 ˝ C2 D spanfe1 ˝ e1; e1 ˝ e2 C e2 ˝ e1; e2 ˝ e2g ˚ spanfe1 ˝ e2  e2 ˝ e1g:
We see, then, that the four-dimensional space V1 ˝ V1 is isomorphic, as an sl.2I C/
representation, to V2 ˚ V0:
Theorem C.1. Let m and n be non-negative integers with m  n: If we consider
Vm ˝ Vn as a representation of sl.2I C/; then
Vm ˝ Vn Š VmCn ˚ VmCn2 ˚    ˚ VmnC2 ˚ Vmn;
where Š denotes an isomorphism of sl.2I C/ representations.
Note that this theorem is consistent with the special case worked out earlier:
V1 ˝ V1 Š V2 ˚ V0: For applications to the Wigner-Eckart theorem, a key property
of the decomposition in Theorem C.1 is that it is multiplicity free. That is to say,
each irreducible representation that occurs in the decomposition of Vm ˝ Vn occurs
only once. This is a special feature of the representations of sl.2I C/; the analogous
statement does not hold for tensor products of representations of other Lie algebras.
Proof. Let us take a basis for each of the two spaces that is labeled by the
eigenvalues for H: That is to say, we choose a basis um; um2; : : : ; um for Vm and
vn; vn2; : : : ; vn for Vn; with m.H/uj D juj and n.H/vk D kvk: Then the
vectors of the form uj ˝ vk form a basis for Vm ˝ Vn; and we compute that
Œm.H/ ˝ I C I ˝ n.H/uj ˝ vk D .j C k/uj ˝ vk:

C.1
Tensor Products of sl.2I C/ Representations
427
Thus, each of our basis elements is an eigenvector for the action of H on Vm ˝ Vn:
The eigenvalues for the action of H range from m C n to .m C n/ in increments
of 2:
The eigenspace with eigenvalue m C n is one dimensional, spanned by um ˝ vn:
If n > 0; then the eigenspace with eigenvalue m C n  2 has dimension 2; spanned
by um2 ˝ vn and um ˝ vn2: Each time we decrease the eigenvalue of H by 2
we increase the dimension of the corresponding eigenspace by 1, until we reach the
eigenvalue m  n; which is spanned by the vectors
um2n ˝ vn; um2nC2 ˝ vn2; : : : ; um ˝ vn:
This space has dimension n C 1: As we continue to decrease the eigenvalue of H in
increments of 2, the dimensions remain constant until we reach eigenvalue n  m;
at which point the dimensions begin decreasing by 1 until we reach the eigenvalue
m  n; for which the corresponding eigenspace has dimension one, spanned by
um ˝vn: This pattern is illustrated by the following table, which lists, for the case
of V4 ˝ V2; each eigenvalue for H and a basis for the corresponding eigenspace.
Eigenvalue for H
Basis
6
u4 ˝ v2
4
u2 ˝ v2
u4 ˝ v0
2
u0 ˝ v2
u2 ˝ v0
u4 ˝ v2
0
u2 ˝ v2
u0 ˝ v0
u2 ˝ v2
2
u4 ˝ v2
u2 ˝ v0
u0 ˝ v2
4
u4 ˝ v0
u2 ˝ v2
6
u4 ˝ v2
Consider now the vector um˝vn; which is annihilated by X and is an eigenvector
for H with eigenvalue m C n: Applying Y repeatedly gives a chain of eigenvectors
for H with eigenvalues decreasing by 2 until they reach m  n: By the proof
of Theorem 4.32, the span W of these vectors is invariant under sl.2I C/ and
irreducible, isomorphic to VmCn: The orthogonal complement of W is also invariant.
Since W contains each of the eigenvalues of H with multiplicity one, each
eigenvalue for H in W ? will have its multiplicity lowered by 1. In particular, mCn
is not an eigenvalue for H in W ?; the largest remaining eigenvalue is m C n  2
and this eigenvalue has multiplicity one (unless n D 0). Thus, if we start with an
eigenvector for H in W ? with eigenvalue m C n  2; this will be annihilated by X
and will generate an irreducible invariant subspace isomorphic to VmCn2:
We now continue on in the same way, at each stage looking at the orthogonal
complement of the sum of all the invariant subspaces we have obtained in the
previous stages. Each step reduces the multiplicity of each H-eigenvalue by 1 and
thereby reduces the largest remaining H-eigenvalue by 2. This process will continue
until there is nothing left, which will occur after Vmn:
ut

428
C
Clebsch-Gordan Theory and the Wigner-Eckart Theorem
Theorem C.1 tells us, for example, that the 15-dimensional space V4 ˝ V2; we
decompose as the direct sum of a seven-dimensional invariant subspace isomorphic
to V6; a ﬁve-dimensional invariant subspace isomorphic to V4; and a three-
dimensional invariant subspace isomorphic to V2: By following the arguments in
the proof of the theorem, we could, in principle, compute these subspaces explicitly.
C.2
The Wigner-Eckart Theorem
Recall that the Lie algebras su.2/ and so.3/ are isomorphic. Speciﬁcally, we use the
bases fE1; E2; E3g for su.2/ and fF1; F2; F3g for so.3/ described in Example 3.27.
The unique linear map  W su.2/ ! so.3/ such that .Ek/ D Fk; k D 1; 2; 3;
is a Lie algebra isomorphism. Thus, the representations of so.3/ are in one-to-one
correspondence with the representations of su.2/; which, in turn, are in one-to-one
correspondence with the complex-linear representations of sl.2I C/: In particular,
the analysis of the decomposition of tensor products of sl.2I C/ representations in
the previous section applies also to so.3/ representations.
Suppose now that ... is a representation of SO.3/ acting on a ﬁnite-dimensional
vector space V: Let End.V / denote the space of endomorphisms of V (i.e., the space
of linear operators of V into itself). Then we can deﬁne an associated action of
SO.3/ on End.V / by the formula
R  C D ....R/C....R/1;
(C.2)
for all R 2 SO.3/ and C 2 End.V /: It is easy to check that this action constitutes
a representation of SO.3/:
Deﬁnition C.2. Let ....; V / be a representation of SO.3/: For any ordered triple
C WD .C1; C2; C3/ of operators on V and any vector v 2 R3; let v  C be the operator
v  C D
3
X
j D1
vj Cj:
(C.3)
The triple C is a vector operator if
.Rv/  C D ....R/.v  C/....R/1
(C.4)
for all R 2 SO.3/:
That is to say, the triple C is a vector operator if the map v 7! vC intertwines the
obvious action of SO.3/ on R3 with the action of SO.3/ on End.V / given in (C.2).
Note that if, say, R 2 SO.3/ maps e1 to e2; then (C.4) implies that
C2 D ....R/C1....R/1:
(C.5)
Equation (C.5) then says that C1 and C2 are "the same operator, up to rotation."

C.2
The Wigner-Eckart Theorem
429
Example C.3. Let V be the space of smooth functions on R3 and deﬁne an action
of SO.3/ on V by
.....R/f /.x/ D f .R1x/:
(C.6)
Deﬁne operators X D .X1; X2; X3/ on V by
.Xjf /.x/ D xj f .x/:
Then X is a vector operator.
Note that Xj is the operator of "multiplication by xj." The operators X1; X2 and
X3 are called the position operators in the physics literature.
Proof. For any v 2 R3 and R 2 SO.3/; we have
fŒ.Rv/  Xf g.x/ D ..Rv/  x/f .x/:
On the other hand, we compute that
Œ.v  X/....R/1f .x/ D .v  x/f .Rx/;
so that
Œ....R/.v  X/....R/1f .x/ D .v  .R1x//f .x/
D ..Rv/  x/f .x/;
as required for a vector operator.
ut
We are now ready for our ﬁrst version of the Wigner-Eckart theorem.
Theorem C.4. Let ....; V / be an irreducible ﬁnite-dimensional representation of
SO.3/; and let A and B be two vector operators on V , with A being nonzero. Then
there exists a constant c such that
B D cA:
The computational signiﬁcance of the theorem is as follows. For each irreducible
representation V; if we can ﬁnd one single vector operator A acting on V; then
the action of any other vector operator on V is completely determined by a single
constant c: There are two ingredients in the proof. The ﬁrst is Schur's lemma and the
second is Theorem C.1, which implies (as we will see shortly) that when End.V /
decomposes as a direct sum of irreducibles, the (complexiﬁcation of) the standard
representation of SO.3/ occurs at most once.

430
C
Clebsch-Gordan Theory and the Wigner-Eckart Theorem
Lemma C.5. Let ... be a ﬁnite-dimensional, irreducible representation of SO.3/
acting on a vector space V; and let SO.3/ act also on End.V / as in (C.2). Then
End.V / Š V ˝ V;
where Š denotes an isomorphism of SO.3/ representations.
Proof. For any ﬁnite-dimensional vector space V; there is, by Deﬁnition 4.13, a
unique linear map from ‰ W V ˝ V  ! End.V / such that for all v 2 V and
 2 V ; we have
‰.v ˝ /.w/ D .w/v:
By computing on a basis, it is easy to check that ‰ is an isomorphism of vector
spaces. If, in addition, V is a representation of SO.3/; then ‰ is an isomorphism of
representations, where SO.3/ acts on V  as in Sect. 4.3.3 and acts on End.V / as in
(C.2). (Compare Exercises 3 and 4 in Chapter 12.) Thus, End.V / Š V ˝ V :
Meanwhile, every irreducible representation of SO.3/ is isomorphic to its dual.
This can be seen either by noting that there is only one irreducible representation
in each dimension, or (more fundamentally) by noting that I is an element of the
Weyl group of the A1 root system. (Compare Exercise 10 in Chapter 10.) Thus,
actually, End.V / Š V ˝ V; as claimed.
ut
Proof of Theorem C.4. The action of SO.3/ on R3 is irreducible. Indeed, the
associated action of SO.3/ on C3 is irreducible; this is the unique irreducible
representation of SO.3/ of dimension 3. Now, the linear map v 7! vA extends to a
complex linear map from C3 into End.V /; and this extension is still an intertwining
map.
Meanwhile, End.V / Š V ˝ V; by the lemma, and V ˝ V decomposes as a
direct sum of irreducibles, as in Theorem C.1. In this decomposition, the three-
dimensional irreducible representation V2 of SO.3/ occurs exactly once, unless V
is trivial. Thus, by Schur's lemma, the map v 7! vA must be zero if V is trivial and
must map into the unique copy of C3 if V is nontrivial. Of course, the same holds
for the map v 7! v  B: Applying Schur's lemma a second time, we see that if A is
nonzero, B must be a multiple of A:
ut
We now turn to a more general form of the Wigner-Eckart theorem, in which
the space V on which the vector operators act is not assumed irreducible, or even
ﬁnite dimensional. Rather, the theorem describes how vector operators act relative
to a pair of irreducible invariant subspaces of V:
Theorem C.6 (Wigner-Eckart). Let V be an inner product space, possibly inﬁ-
nite dimensional. Suppose ... is a representation of SO.3/ acting on V in an
inner-product-preserving fashion. Let W1 and W2 be ﬁnite-dimensional, irreducible
subspaces of V: Suppose A and B are two vector operators on V and that
˝
w; Aj w0˛
is nonzero for some w 2 W1, w0 2 W2; and j 2 f1; 2; 3g: Then there exists a

C.2
The Wigner-Eckart Theorem
431
constant c such that
˝
w; Bj w0˛
D c
˝
w; Aj w0˛
for all w 2 W1, all w0 2 W2; and all j D 1; 2; 3:
In many applications, the space V is L2.R3/; the space of square-integrable
functions on R3; and where SO.3/ acts on L2.R3/ by the same formula as in (C.6).
The irreducible, SO.3/-invariant subspaces of L2.R3/ are described in Section 17.7
of [Hall]. The computational signiﬁcance of the theorem is similar to that of
Theorem C.4: For each pair of irreducible subspaces W1 and W2; the "matrix
entries" of any vector operator between W1 and W2 (i.e., the quantities
˝
w; Aj w0˛
with w 2 W1 and w0 2 W2) are the same, up to a constant. Indeed, these matrix
entries really depend only on the isomorphism class of W1 and W2: Thus, if one
can compute the matrix entries for some vector operator once and for all—for each
pair of irreducible representations of SO.3/—the matrix entries for any other vector
operator are then determined up to the calculation of a single constant.
Proof. Note that the operators Aj and Bj (or more generally, v  A and v  B;
for v 2 R3) do not necessarily map W2 into W1: On the other hand, taking the
inner product with an inner product of, say, Ajw0 with an element w of W1 has
the effect of projecting Aj w0 onto W1; since the inner product only depends on
the component of Ajw0 in W1: With this observation in mind, let P1 W V ! W1
be the orthogonal projection onto W1: (This operator exists even if V is not a
Hilbert space and can be constructed using an orthonormal basis for W1.) Let
Hom.W2; W1/ denote the space of linear operators from W2 to W1 and deﬁne a
linear map A W R3 ! Hom.W2; W1/ by
A.v/.w/ D P1.v  A/.w/
for all w 2 W2:
Now, since both W1 and W2 are invariant, if C belongs to Hom.W2; W1/; then so
does the operator
....R/C....R/1
(C.7)
for all R 2 SO.3/: Under the action (C.7), the space Hom.W2; W1/ becomes a
representation of SO.3/: We now claim that A is an intertwining map from R3 into
Hom.W2; W1/: To see this, note that since A is a vector operator, we have
A.Rv/.w/ D P1....R/.v  A/....R/1.w/:
(C.8)
But since W1 is invariant and the action of SO.3/ preserves the inner product, W ?
1 is
also invariant, in which case we can see that P1 commutes with ....R/: Thus, (C.8)
becomes

432
C
Clebsch-Gordan Theory and the Wigner-Eckart Theorem
A.Rv/.w/ D ....R/A.v/....R/1;
as claimed.
Now, by a simple modiﬁcation of the proof of Theorem C.4, we have
Hom.W2; W1/ Š W1 ˝ W 
2 Š W1 ˝ W2;
where Š denotes isomorphism of SO.3/ representations. By Theorem C.1, in the
decomposition of W1 ˝ W2; the three-dimensional irreducible representation C3 of
SO.3/ occurs at most once. If C3 does not occur, then A must be identically zero,
and similarly for the analogously deﬁned map B: If C3 does occur, both A and
B must map into the same irreducible subspace of Hom.W2; W1/; and, by Schur's
lemma, they must be equal up to a constant.
Finally, note that the orthogonal projection P1 is self-adjoint on V and is equal
to the identity on W1: Thus,
˝
w; P1.v  A/w0˛
D
˝
P1w; .v  A/w0˛
D
˝
w; .v  A/w0˛
;
and similarly with A replaced by B: Thus, since B D cA; we have
˝
w; .v  B/w0˛
D c
˝
w; .v  A/w0˛
for all v 2 R3: Specializing to v D ej, j D 1; 2; 3; gives the claimed result.
ut
C.3
More on Vector Operators
We now look a bit more closely at the notion of vector operator. We consider ﬁrst the
Lie algebra counterpart to Deﬁnition C.2. We use the basis fF1; F2; F3g for so.3/
from Example 3.27. For j; k; l 2 f1; 2; 3g; deﬁne "jkl as follows:
"klm D
8
<
:
0 if any two of j; k; l are equal
1 if .j; k; l/ is a cyclic permutation of .1; 2; 3/
1 if .j; k; l/ is an non-cyclic permutation of .1; 2; 3/:
Thus, for example, "112 D 0 and "132 D 1: The commutation relations among
F1; F2; and F3 may be written as
ŒFj ; Fk D
3
X
lD1
"jklFl:
Proposition C.7. Let ....; V / be a ﬁnite-dimensional representation of SO.3/ and
let  be the associated representation of so.3/: Then a triple C D .C1; C2; C3/ of

C.3
More on Vector Operators
433
operators is a vector operator if and only if
.Xv/  C D .X/.v  C/  .v  C/.X/
(C.9)
for all X 2 so.3/: This condition, in turn, holds if and only if C1; C2; and C3 satisfy
Œ.Fj /; Ck D
3
X
lD1
"jklCl:
(C.10)
In physics terminology, the operators .Fj / are (up to a factor of i„; where „ is
Planck's constant) the angular momentum operators. See Section 17.3 of [Hall] for
more information.
Proof. If SO.3/ acts on End.V / as R  C D ....R/C....R/1; the associated action
of so.3/ on End.V / is X  C D .X/C  C.X/: The condition (C.9) is just the
assertion that the map v 7! v  C is an intertwining map between the action of so.3/
on R3 and its action on End.V /: Since SO.3/ is connected, it is easy to see that this
condition is equivalent to the intertwining property in Deﬁnition C.2.
Meanwhile, (C.9) will hold if and only if it holds for X D Fj and v D ek;
for all j; k D 1; 2; 3: Now, direct calculation with the matrices F1; F2; and F3 in
Example 3.27 shows that Fj ek D P3
lD1 "jklel: Putting X D Fj and v D ek in (C.9)
gives
3
X
lD1
"jklCl D Œ.Fj /; Ck;
as claimed.
ut
There is one last aspect of vector operators that should be mentioned. In quantum
physics, it is expected that the vector space of states should carry an action of the
rotation group SO.3/: This action may not, however, be an ordinary representation,
but rather a projective representation. This means that the action is allowed to
be ill deﬁned up to a constant. The reason for allowing this ﬂexibility is that in
quantum mechanics, two vectors that differ by a constant are considered the same
physical state. (See Section 16.7.3 of [Hall] for more information on projective
representations.) In particular, the space of states for a "spin one-half" particle
carries a projective representation of SO.3/ that does not come from an ordinary
representation of SO.3/:
Suppose, for example, that V carries an action of the group SU.2/; rather than
SO.3/: Suppose, also, that the action of the element I 2 SU.2/ on V is either
as I or as I: If the action of I 2 SU.2/ on V is as I; then as in the proof of
Proposition 4.35, the representation will descend to a representation of SO.3/ Š
SU.2/=fI; Ig on V: Even if the action of I 2 SU.2/ on V is as I, we can still
construct a representation of SO.3/ that is well deﬁned up to a constant; that is, V

434
C
Clebsch-Gordan Theory and the Wigner-Eckart Theorem
still carries a projective representation of SO.3/: Furthermore, the associated action
of I 2 SU.2/ on End.V / will satisfy
.I/  C D .I/C.I/1 D C:
Thus, the action of SU.2/ on End.V / still descends to an (ordinary) action of
SO.3/: We can, therefore, still deﬁne vector operators in the setting of projective
representations of SO.3/; and the proof of the Wigner-Eckart theorem goes through
with only minor changes.

Appendix D
Completeness of Characters
In this appendix, we sketch a proof of the completeness of characters (Theorem
12.18) for an arbitrary compact Lie group, not assumed to be isomorphic to a matrix
group. The proof requires some functional analytic results, notably the spectral
theorem for compact self-adjoint operators. The needed results from functional
analysis may be found, for example, in Chapter II of [Kna1].
As in the proof for matrix groups in Chapter 12, we ﬁrst prove that general
functions can be approximated by matrix entries of irreducible representations
and then specialize to the case of class functions. We will begin by showing that
any ﬁnite-dimensional, translation-invariant space of functions on K decomposes
in terms of matrix entries. We will then construct such spaces of functions as
eigenspaces of certain convolution operators.
We consider the normalized left-invariant volume form ˛ on K: If we translate
˛ on the right by some x 2 K; the resulting form ˛x is easily seen to be, again, a
left-invariant volume form, which must agree with ˛ up to a constant. On the other
hand, ˛x is still normalized, so it must actually agree with ˛: Similarly, the pullback
of ˛ by the map x 7! x1 is easily seen to be left-invariant and normalized and
thus coincides with ˛: Thus, ˛ is invariant under both left and right translations and
under inversions.
Now, integration of a smooth function f against ˛ satisﬁes
ˇˇˇˇ
Z
K
f ˛
ˇˇˇˇ  sup
K
jf j :
Meanwhile, by the Stone-Weierstrass theorem (Theorem 7.33 in [Rud1]), every
continuous function on K can be uniformly approximated by smooth functions.
Thus, the map f
7!
R
K f ˛ extends by continuity from smooth functions to
continuous functions, and if f is non-negative,
R
K f ˛ will be non-negative. It then
© Springer International Publishing Switzerland 2015
B. Hall, Lie Groups, Lie Algebras, and Representations, Graduate
Texts in Mathematics 222, DOI 10.1007/978-3-319-13467-3
435

436
D
Completeness of Characters
follows from the Riesz representation theorem that there is a unique measure  on
the Borel 	-algebra in K such that
Z
K
f ˛ D
Z
K
f .x/ d.x/
for all continuous functions f on K: (See Theorems 2.14 and 2.18 in [Rud2]). Since
˛ is normalized and invariant under left and right translations and inversions, the
same is true of . We refer to  as the (bi-invariant, normalized) Haar measure
on K: We consider the Hilbert space L2.K/; the space of (equivalence classes of
almost-everywhere-equal) square-integrable functions on K with respect to :
We make use of the left translation and right translation operators, given by
.Lxf /.y/ D f .x1y/
.Rxf /.y/ D f .xy/:
Both L and R constitute representations of K acting on L2.K/: A subspace V 
L2.K/ is right invariant, left invariant, or bi-invariant if it is invariant under left
translations, right translations, or both left and right translations.
Proposition D.1. Suppose V  L2.K/ is a ﬁnite-dimensional, bi-invariant sub-
space and that each element of V is continuous. Then each element of V can
be expressed as a ﬁnite linear combination of matrix entries for irreducible
representations of K:
Saying that an element f of K is continuous means, more precisely, that the
equivalence class f has a (necessarily unique) continuous representative.
Proof. By complete reducibility, we may decompose V into subspaces Vj that are
ﬁnite-dimensional and irreducible under the right action of K: Since the elements
of Vj are continuous, "evaluation at the identity" is a well-deﬁned linear functional
on the ﬁnite-dimensional space Vj : Thus, there exists an element j of Vj such that
f .e/ D
˝
j ; f
˛
for all f 2 Vj: It follows that for all f 2 Vj ; we have
f .x/ D .Rxf /.e/
D
˝
j ; Rxf
˛
D trace.Rx
ˇˇj
˛
hf j/;
where
ˇˇj
˛
hf j is the operator mapping g 2 Vj to hf; gi j : Thus, each f 2 Vj is
a matrix entry of the irreducible representation .R; Vj/ of K and each f 2 V is a
linear combination of such matrix entries.
ut

D
Completeness of Characters
437
Deﬁnition D.2. If f and g are in L2.K/; the convolution of f and g is the function
f  g on K given by
.f  g/.x/ D
Z
K
f .xy1/g.y/ d.y/:
(D.1)
A key property of convolution is that convolution on the left commutes with
translation on the right, and vice versa. That is to say,
.Lxf /  g D Lx.f  g/
(D.2)
and
f  .Rxg/ D Rx.f  g/:
(D.3)
Intuitively, f  g can be viewed as a combination of right-translates of f; weighted
by the function g: Thus, say, (D.2) boils down to the fact that right translation
commutes with left translation, which is just a different way of stating that
multiplication on K is associative. Rigorously, both (D.2) and (D.3) follow easily
from the deﬁnition of convolution.
Using the Cauchy-Schwarz inequality and the invariance of  under translation
and inversion, we see that
j.f  g/.x/j  kf kL2.K/ kgkL2.K/
(D.4)
for all x 2 K: If f and g are continuous, then (since K is compact) f is
automatically uniformly continuous, from which it follows that f g is continuous.
For any f and g in L2.K/; we can approximate f and g in L2.K/ by continuous
functions and show, with the help of (D.4), that f  g is continuous. We may also
"move the norm inside the integral" in (D.1) to obtain the inequality
kf  gkL2.K/  kf kL2.K/ kgkL1.K/ :
(D.5)
Unlike convolution on the real line, convolution on a noncommutative group is,
in general, noncommutative. Nevertheless, we have the following result.
Proposition D.3. If f 2 L2.K/ is a class function, then for all g 2 L2.K/; we
have
f  g D g  f:
Proof. If we make the change of variable z D y1x; so that y D xz1 and y1 D
zx1 we ﬁnd that
.f  g/.x/ D
Z
K
f .xzx1/g.xz1/ d.z/:

438
D
Completeness of Characters
Since f is a class function, this expression reduces to
.f  g/.x/ D
Z
K
g.xz1/f .z/ d.z/ D .g  f /.x/;
as claimed.
ut
We now introduce the properties of operators that will feature in our version of
the spectral theorem.
Deﬁnition D.4. Let H be a Hilbert space and A a bounded linear operator on H:
Then A is self-adjoint if
hu; Avi D hAu; vi
for all u and v in H; and A is compact if for every bounded set E  H; the image
of E under A has compact closure in H:
Here compactness is understood to be relative to the norm topology on H: If H is
inﬁnite dimensional, the closed unit ball in H is not compact in the norm topology
and thus, for example, the identity operator on H is not compact.
Proposition D.5. If  2 L2.K/ is real-valued and invariant under x 7! x1; the
convolution operator C given by
C.f / D   f
is self-adjoint and compact.
Proof. The operator C is an integral operator with integral kernel k.x; y/ D
.xy1/: Now, an integral operator is self-adjoint precisely if its kernel satisﬁes
k.x; y/ D k.y; x/: In the case of C, this relation holds because
.xy1/ D .yx1/;
as a consequence of our assumptions on : Meanwhile, since  is square integrable
over K and K has ﬁnite measure, the function k.x; y/ D .xy1/ is square
integrable over K  K: It follows that C is a Hilbert-Schmidt operator, and
therefore compact. (See Theorem 2.4 in Chapter II of [Kna1].)
ut
Since K is compact, we can construct an inner product on the Lie algebra k of K
that is invariant under the adjoint action of K: Thinking of k as the tangent space to
K at the identity, we may then extend this inner product to an inner product on every
other tangent space by using (equivalently) either left or right translations. Thus, we
obtain a bi-invariant Riemannian metric on K, which we use in the following result.
Proposition D.6. Let B".I/ denote the ball of radius " about I 2 K: There exists
a sequence hni of non-negative class functions on K such that (1) supp.n/ 

D
Completeness of Characters
439
B1=n.I/; (2) n.x1/ D n.x/ for all x 2 K; and (3) R
K n.x/ d.x/ D 1: If hni
is any such sequence, then
lim
n!1 kf  n  f kL2.K/ ! 0
for all f 2 L2.K/:
We may think of the functions n in the proposition as approximating a "ı-
function" at the identity on K:
Proof. Since the metric on K is bi-invariant, each B".I/ is invariant under the
adjoint action of K: Thus, if  n is any non-negativefunction with support in B1=n.I/
that integrates to 1, we may deﬁne
n.x/ D
Z
K
 n.yxy1/ d.y/;
and n will be a class function, still supported in B1=n.I/ and still integrating to 1.
We may then deﬁne
n.x/ D 1
2.n.x/ C n.x1//
and n will have the required properties. (Note that d.x1; I/ D d.I; x/ by the left
invariance of the metric.)
Suppose g is continuous—and thus uniformly continuous—on K: Then if n is
large enough, we will have jg.y/  g.x/j < " whenever d.y; x/ < 1=n: Now, since
 is normalized, we have
.n  g/.x/  g.x/ D
Z
K
n.xy1/.g.y/  g.x// d.y/
and so, for large n;
j.n  g/.x/  g.x/j 
Z
K
n.xy1/ jg.y/  g.x/j d.y/
 "
Z
K
n.xy1/ d.y/
D ":
We conclude that n  g converges uniformly—and thus, also, in L2.K/—to g.

440
D
Completeness of Characters
For any f 2 L2.K/ is arbitrary, we choose a continuous function g close to f
in L2.K/ and observe that
kn  f  f kL2.K/
 kn  f  n  gkL2.K/ C kn  g  gkL2.K/ C kg  f kL2.K/
 knkL1.K/ kf  gkL2.K/ C kn  g  gkL2.K/ C kg  f kL2.K/ :
where in the second inequality, we have used (D.5) and Proposition D.3. Since n
is non-negative and integrates to 1, knkL1.K/ D 1 for all n: Thus, if we take g with
kf  gk < "=3 and then choose N so that kn  g  gk < "=3 for n  N; we see
that kn  f  f k < " for n  N:
ut
We now appeal to a general functional analytic result, the spectral theorem for
compact self-adjoint operators.
Theorem D.7 (Spectral Theorem for Compact Self-adjoint Operators). Sup-
pose H is an inﬁnite-dimensional, separable Hilbert space and A is a compact,
self-adjoint operator on H: Then A has an orthonormal basis of eigenvectors with
real eigenvalues that tend to zero.
For a proof, see Section II.2 of [Kna1]. Since the eigenvalues tend to zero, a ﬁxed
nonzero number can occur only ﬁnitely many times as an eigenvalue; that is, each
eigenspace with a nonzero eigenvalue is ﬁnite dimensional.
Theorem D.8. If K is any compact Lie group, the space of matrix entries is dense
in L2.K/:
Proof. Let us say that a function f 2 L2.K/ is K-ﬁnite if there exists a ﬁnite-
dimensional space of continuous functions on K that contains f and is invariant
under both left and right translation. In light of Proposition D.1, it sufﬁces to show
that the space of K-ﬁnite functions is dense in L2.K/:
To prove this claim, suppose g 2 L2.K/ is orthogonal to every K-ﬁnite function
f: If hni is as in Proposition D.6, then n  g converges to g in L2.K/: Since n
is a class function, Proposition D.3 and the identities (D.2) and (D.3) tell us that the
convolution operator Cn commutes with both left and right translations. Thus, the
eigenspaces of Cn are invariant under both left and right translations. Furthermore,
since n  f is continuous for any f
2 L2.K/; the eigenvectors of Cn with
nonzero eigenvalues must be continuous. Finally, since Cn is compact and self-
adjoint, the eigenspaces for Cn with nonzero eigenvalues are ﬁnite-dimensional.
Thus, eigenvectors for Cn with nonzero eigenvalues are K-ﬁnite.
We conclude that g must be orthogonal to all the eigenvectors of Cn with
nonzero eigenvalues. Thus, by the spectral theorem, g must actually be in the
eigenspace for Cn with eigenvalue 0; that is, n  g D 0 for all n: Letting n tend to
inﬁnity, we conclude that g is the zero function.
ut

D
Completeness of Characters
441
We may now prove (a generalization of) Theorem 12.18, without assuming ahead
of time that K is a matrix group.
Corollary D.9. If f is a square-integrable class function on K and f is orthogonal
to the character of every ﬁnite-dimensional, irreducible representation of K; then f
is zero almost everywhere.
Proof. By Theorem D.8, we can ﬁnd a sequence gn converging in L2.K/ to f;
where each gn is a linear combination of matrix entries. Since f is a class function,
the L2 distance between f .x/ and gn.yxy1/ is independent of y: Thus, if we
deﬁne fn by
fn.x/ D
Z
K
gn.yxy1/ d.y/;
the sequence fn will also converge to f in L2.K/: But by Lemma 12.20, each fn is
a linear combination of characters of irreducible representations. Thus, f must be
orthogonal to each fn; and we conclude that
kf k2 D hf; f i D lim
n!1 hf; fni D 0;
from which the claimed result follows.
ut

References
[Axl] Axler, S.: Linear Algebra Done Right, 2nd edn. Undergraduate Texts in Mathematics.
Springer, New York (1997)
[Baez] Baez, J.C.: The octonions. Bull. Am. Math. Soc. (N.S.) 39, 145-205 (2002); errata Bull.
Am. Math. Soc. (N.S.) 42, 213 (2005)
[BBCV] Baldoni, M.W., Beck, M., Cochet, C., Vergne, M.: Volume computation for polytopes
and partition functions for classical root systems. Discret. Comput. Geom. 35, 551-595
(2006)
[BF] Bonﬁglioli, A., Fulci, R.: Topics in Noncommutative Algebra: The Theorem of Camp-
bell, Baker, Hausdorff and Dynkin. Springer, Berlin (2012)
[BtD] Bröcker, T., tom Dieck, T.: Representations of Compact Lie Groups. Graduate Texts in
Mathematics, vol. 98. Springer, New York (1985)
[CT] Cagliero, L., Tirao, P.: A closed formula for weight multiplicities of representations of
Sp2.C/. Manuscripta Math. 115, 417-426 (2004)
[Cap] Capparelli, S.: Computation of the Kostant partition function. (Italian) Boll. Unione Mat.
Ital. Sez. B Artic. Ric. Mat. 6(8), 89-110 (2003)
[DK] Duistermaat, J., Kolk, J.: Lie Groups. Universitext. Springer, New York (2000)
[Got] Gotô, M.: Faithful representations of Lie groups II. Nagoya Math. J. 1, 91-107 (1950)
[Hall] Hall, B.C.: Quantum Theory for Mathematicians. Graduate Texts in Mathematics, vol.
267. Springer, New York (2013)
[Has] Hassani, S.: Mathematical Physics: A Modern Introduction to its Foundations, 2nd edn.
Springer, Heidelberg (2013)
[Hat] Hatcher, A.: Algebraic Topology. Cambridge University Press, Cambridge (2002). A
free (and legal!) electronic version of the text is available from the author's web page at
www.math.cornell.edu/~hatcher/AT/AT.pdf
[HK] Hoffman, K., Kunze, R.: Linear Algebra, 2nd edn. Prentice-Hall, Englewood Cliffs
(1971)
[Hum] Humphreys, J.: Introduction to Lie Algebras and Representation Theory. Second print-
ing, revised. Graduate Texts in Mathematics, vol. 9. Springer, New York/Berlin (1978)
[Jac] Jacobson, N.: Exceptional Lie Algebras. Lecture Notes in Pure and Applied Mathemat-
ics, vol. 1. Marcel Dekker, New York (1971)
[Kna2] Knapp, A.W.: Lie Groups Beyond an Introduction, 2nd edn. Progress in Mathematics,
vol. 140. Birkhäuser, Boston (2002)
[Kna1] Knapp, A.W.: Advanced Real Analysis. Birkhäuser, Boston (2005)
[Lee] Lee, J.: Introduction to Smooth Manifolds. 2nd edn. Graduate Texts in Mathematics, vol.
218. Springer, New York (2013)
© Springer International Publishing Switzerland 2015
B. Hall, Lie Groups, Lie Algebras, and Representations, Graduate
Texts in Mathematics 222, DOI 10.1007/978-3-319-13467-3
443

444
References
[Mill] Miller, W.: Symmetry Groups and Their Applications. Academic, New York (1972)
[Poin1] Poincaré, H.: Sur les groupes continus. Comptes rendus de l'Acad. des Sciences 128,
1065-1069 (1899)
[Poin2] Poincaré, H.: Sur les groupes continus. Camb. Philos. Trans. 18, 220-255 (1900)
[Pugh] Pugh, C.C.: Real Mathematical Analysis. Springer, New York (2010)
[Ross] Rossmann, W.: Lie Groups. An Introduction Through Linear Groups. Oxford Graduate
Texts in Mathematics, vol. 5. Oxford University Press, Oxford (2002)
[Rud1] Rudin, W.: Principles of Mathematical Analysis, 3rd edn. International Series in Pure
and Applied Mathematics. McGraw-Hill, New York-Auckland-Düsseldorf (1976)
[Rud2] Rudin, W.: Real and Complex Analysis, 3rd edn. McGraw-Hill, New York (1987)
[Run] Runde, V.: A Taste of Topology. Universitext. Springer, New York (2008)
[Tuy] Tuynman, G.M.: The derivation of the exponential map of matrices. Am. Math. Mon.
102, 818-819 (1995)
[Var] Varadarajan, V.S.: Lie Groups, Lie Algebras, and Their Representations. Reprint of the
1974 edn. Graduate Texts in Mathematics, vol. 102. Springer, New York (1984)
[Tar] Tarski, J.: Partition function for certain simple Lie algebras. J. Math. Phys. 4, 569-574
(1963)

Index
A
A2 root system, 145, 157, 201
A3 root system, 228
An root system, 189, 232
abelian, see commutative
AdA, 63
adjoint
group, 401
map, 63
of a matrix, 409
representation, 51
adX, 51, 64
afﬁne transformation, 392
alcove, 389, 393, 404
algebraically integral element, see integral
element, algebraic
analytic subgroup, see connected Lie subgroup
analytically integral element, see integral
element, analytic
angles in root systems, 199
angular momentum, 96
averaging method, 92
B
B2 root system, 201
B3 root system, 228
Bn root system, 191, 234
Baker-Campbell-Hausdorff formula, 109, 113
base of a root system, 206
basepoint, 371
bilinear form
skew symmetric, 9
symmetric, 8
bracket, 56
C
C3 root system, 228
Cn root system, 192, 234
Campbell-Hausdorff formula, see Baker-
Campbell-Hausdorff formula
canonical form, see Jordan canonical form
Cartan subalgebra, 154, 174
Casimir element, 269, 298
center
discrete subgroup of, 28
of a compact group, 314, 399
of a Lie algebra, 51, 94
of a matrix Lie group, 94
centralizer, 335
chamber, Weyl, see Weyl chamber
character of a representation, 275, 351, 441
characteristic polynomial, 407
class function, 331, 339
classical groups and Lie algebras, 188
classiﬁcation
of root systems, 236
of simple Lie algebras, 236
Clebsch-Gordan theory, 89, 426
closed subgroup, 4, 137
commutative
Lie algebra, 49, 57, 73
matrix Lie group, 57, 73
commutative group
representations of, 95
commutator, 56
commutator ideal, 54
compact group
complete reducibility of, 92
fundamental group of, 373
compact operator, 438
© Springer International Publishing Switzerland 2015
B. Hall, Lie Groups, Lie Algebras, and Representations, Graduate
Texts in Mathematics 222, DOI 10.1007/978-3-319-13467-3
445

446
Index
compact real form, 169
compact symplectic group, 10, 12
fundamental group of, 376
compactness, 16
complete reducibility, 90, 273
complex
Lie algebra, 49, 57
matrix Lie group, 57
complexiﬁcation of a Lie algebra, 65
conjugate linear, 413
connected Lie subgroup, 129
connectedness, 17, 71
contragredient representation, see dual
representation
convergence of a sequence of matrices,
4
convex hull, 159, 224, 265
convolution, 437
coroot, 179, 204
real, 333
coroot lattice, 379
cover, 372
covering group, see universal cover
covering map, 372, 393
cross product, 49, 74
D
Dn root system, 190, 233
ıjk, 6
ı, see half the sum of the positive roots
dense subgroup, 310
derivation, 51
derivative of exponential mapping, 114
derived series, 54
diag./, 156
diagonalization, 408
differential form, 93, 317, 419
direct product of matrix Lie groups, 74,
88
direct sum
of Lie algebras, 52
of representations, 84
of root systems, 198
discrete subgroup, 28, 308, 339
dominant, 147, 219, 344
dual
of root lattice, 399
representation, 89, 165
root system, 204
space, 89, 414
Dynkin diagram, 216, 235, 236
extended, 405
E
eigenspace, 408
eigenvalue, 407
eigenvector, 407
Ejk, 189
Euclidean group, 10
exponential
of a locally nilpotent operator, 260
of a matrix, 31
exponential map, 67
surjectivity of, 314
extended Dynkin diagram, see Dynkin
diagram, extended
extended Weyl group, 392
F
faithful, 77
ﬁber bundle, 374
Freudenthal's formula, 293
fundamental
representations, 152
weights, 219
Weyl chamber, 210
fundamental group, 371
of classical groups, 373, 377
of SO.3/, 21, 24
G
, see kernel of exponential map
G2 root system, 201, 208, 220, 402, 405
g˛, 176
general linear group, 4, 58
generalized eigenvector, 410
generalized orthogonal group, 8, 59
group versus Lie algebra homomorphisms, 119
H
half the sum of the positive roots, 220, 357,
366, 382
Hausdorff property, 321
Heisenberg group, 11, 110
higher, 146, 222
highest weight, 146, 243
highest weight cyclic representation, 148, 244
Hilbert-Schmidt
inner product, 188, 194
norm, 32, 46
homomorphism
of Lie algebras, 51, 60
of matrix Lie groups, 22, 60, 72
homotopic, 371

Index
447
homotopy group, 373
hyperplane, 198, 206, 207
I
ideal, 51, 53, 249, 254
identity component of a matrix Lie group, 17,
56
indecomposable positive root, 207
inhomogeneous Lorentz group, see Poincaré
group
inner product, 412
integral element, 144, 147, 218, 242
algebraic, 344, 381
analytic, 344, 346, 381
intertwining map, 78
invariant subspace, 78
irreducible
representation, 78
root system, 199
isomorphism
of Lie algebras, 51
of matrix Lie groups, 22
of representations, 78
of root systems, 199
J
joint eigenvector, see simultaneous eigenvector
Jordan canonical form, 411
K
kernel
of a Lie group homomorphism, 63
of the exponential map, 344, 378
Killing form, 194
Kostant multiplicity formula, 291
Kostant partition function, 288
Kreg, 383
L
length ratios in root systems, 199
Lie algebra
general, 49
of a matrix Lie group, 55
Lie group, 25
Lie product formula, 40
lift of a map, 372
linear functional, 414
local homomorphism, 119
locally nilpotent operator, 260
logarithm of a matrix, 36
long exact sequence of homotopy groups, 374
loop, 371
Lorentz group, 8
lower, 146, 222
M
manifold, 25, 70
mapping degree, 315, 329
matrix entry of a representation, 355, 366, 440
matrix Lie group, 4
maximal commutative subalgebra, 175, 313
maximal torus, 312
Mn.C/, 4
module, see representation
morphism, see intertwining map
multiplicity, 144, 343
N
negative root, 206
nilpotent
Lie algebra, 54
matrix, 47
operator, 410
nonmatrix Lie group, 103
nontrivial
ideal, 53
invariant subspace, 78
norm of a matrix, 31
normalizer, 313
N.T /, see normalizer
null homotopic, 371
O
one-parameter subgroup, 41, 56
orientation, 315
orthogonal, 191
complement, 414
matrix, 7
orthogonal group, 7, 58, 190
fundamental group of, 375
orthogonality
of characters, 352
of exponentials, 347
orthonormal basis, 409
P
path connected, 17
physicists' convention, 57
Poincaré-Birkhoff-Witt theorem, 250
Poincaré group, 11

448
Index
polar decomposition, 42, 127
polynomials
action of SU.2/ on, 82
action of SU.3/ on, 166
positive root, 206
positive simple root, 145, 206
product rule, 46
Q
quotient manifold, 321, 374
R
rank, 176, 198
real roots and coroots, 333
real weight, 343
reﬂection, 198, 334, 392, 404
regular element, 383, 384
regular value, 315
representation
of a Lie algebra, 77
of a matrix Lie group, 77
unitary, 80
root, 144, 176
lattice, 399
real, 333
space, 176
string, 238
system, 184, 197
vector, 144, 176
beer, 228
rotation, 8
RP 3, 21, 375
RP n, 19
S
S1, 12
Sard's theorem, 316
Schur's lemma, 94
Schwarz inequality, 32, 46
self-adjoint
matrix, 409
operator, 438
semidirect product, 392
semisimple, 169
simple connectedness, 18, 119, 372
simple Lie algebra, 53
simple root, see positive simple root
simply connected, 122
simultaneous
diagonalization, 416
eigenvector, 415
singular element, 383, 384
singular value, 315
skew self-adjoint, 409
sl.2I C/, 53, 83, 96
SL.2I R/, 127
sl.3I C/
representations of, 141
Weyl group of, 154
slice lemma, 322
smooth manifold, 25
SN decomposition, 34, 411
SO.3/
fundamental group of, 19
Lie algebra of, 62
representations of, 101
universal cover of, 126
solvable Lie algebra, 54, 55
special linear group, 6, 58
special orthogonal group, 7, 58
special unitary group, 6
spectral theorem, 440
spin, 101
Sp.n/, see compact symplectic group
square root
of a positive matrix, 43
uniqueness of, 41
standard representation, 81
Stiefel diagram, 389
strictly dominant, 219
structure constants, 52
SU.2/
Lie algebra of, 62
relationship to SO.3/, 22, 101, 126
representations of, 82, 96, 101
simple connectedness of, 19
SU.3/
representations of, 141
Weyl group of, 154
subalgebra, 50
symplectic group, 9, 59, 192
T
tangent space at the identity, 56, 71
tensor algebra, 248
tensor product
of representations, 87, 106, 425
of vector spaces, 85
theorem of the highest weight, 146, 243, 345
torus, 308
torus theorem, 314, 326
trace of a matrix, 411
trivial representation, 81
Trotter product formula, 40

Index
449
U
unipotent matrix, 47
unitarian trick, 92
unitary group, 6, 58
fundamental group of, 376
universal cover, 126, 393
of SL.2I R/, 127
of SO.3/, 126
universal enveloping algebra, 246
universal property of tensor products, 86
upper central series, 54
upper triangular matrix, 410
V
Vandermonde determinant, 304
vector operator, 428, 432
Verma module, 244, 254, 294
volume form, 325
W
weight diagram, 158, 266
weight of a representation, 144, 242,
343
Weyl
chamber, 210, 212
character formula, 275, 357
denominator, 277, 288, 289
dimension formula, 281
group, 154, 198, 203, 212, 313,
333
integral formula, 330, 358
Weyl-alternating function, 283
Z
Zometool system, 228
Z.T /, see centralizer


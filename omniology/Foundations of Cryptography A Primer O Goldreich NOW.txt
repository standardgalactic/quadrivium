Foundations of
Cryptography
– A Primer
Oded Goldreich
Department of Computer Science
Weizmann Institute of Science
Rehovot Israel
oded.goldreich@weizmann.ac.il
Boston – Delft

Foundations and Trends
R⃝in
Theoretical Computer Science
Published, sold and distributed by:
now Publishers Inc.
PO Box 1024
Hanover, MA 02339
USA
Tel. +1 781 871 0245
www.nowpublishers.com
sales@nowpublishers.com
Outside North America:
now Publishers Inc.
PO Box 179
2600 AD Delft
The Netherlands
Tel. +31-6-51115274
A Cataloging-in-Publication record is available from the Library of Congress
Printed on acid-free paper
ISBN: 1-933019-02-6; ISSNs: Paper version 1551-305X; Electronic
version 1551-3068
c⃝2005 O. Goldreich
All rights reserved. No part of this publication may be reproduced,
stored in a retrieval system, or transmitted in any form or by any
means, mechanical, photocopying, recording or otherwise, without prior
written permission of the publishers.
now Publishers Inc. has an exclusive license to publish this mate-
rial worldwide. Permission to use this content must be obtained from
the copyright license holder. Please apply to now Publishers, PO Box
179, 2600 AD Delft, The Netherlands, www.nowpublishers.com; e-mail:
sales@nowpublishers.com

Contents
1
Introduction and Preliminaries
1
1.1
Introduction
1
1.2
Preliminaries
7
I
Basic Tools
10
2
Computational Diﬃculty and One-way Functions
13
2.1
One-way functions
14
2.2
Hard-core predicates
18
3
Pseudorandomness
23
3.1
Computational indistinguishability
24
3.2
Pseudorandom generators
26
3.3
Pseudorandom functions
29
4
Zero-Knowledge
33
v

vi
Contents
4.1
The simulation paradigm
33
4.2
The actual deﬁnition
35
4.3
Zero-knowledge proofs for all NP-assertions and their applications
37
4.4
Variants and issues
43
II
Basic Applications
56
5
Encryption Schemes
59
5.1
Deﬁnitions
63
5.2
Constructions
67
5.3
Beyond eavesdropping security
71
6
Signature and Message Authentication Schemes
75
6.1
Deﬁnitions
77
6.2
Constructions
79
6.3
Public-key infrastructure
82
7
General Cryptographic Protocols
85
7.1
The deﬁnitional approach and some models
87
7.2
Some known results
96
7.3
Construction paradigms and two simple protocols
99
7.4
Concurrent execution of protocols
108
7.5
Concluding remarks
112
References
117

1
Introduction and Preliminaries
It is possible to build a cabin with no foundations,
but not a lasting building.
Eng. Isidor Goldreich (1906–1995)
1.1
Introduction
The vast expansion and rigorous treatment of cryptography is one of
the major achievements of theoretical computer science. In particular,
concepts such as computational indistinguishability, pseudorandomness
and zero-knowledge interactive proofs were introduced, classical notions
such as secure encryption and unforgeable signatures were placed on
sound grounds, and new (unexpected) directions and connections were
uncovered. Indeed, modern cryptography is strongly linked to complex-
ity theory (in contrast to “classical” cryptography which is strongly
related to information theory).
Modern cryptography is concerned with the construction of infor-
mation systems that are robust against malicious attempts to make
these systems deviate from their prescribed functionality. The pre-
scribed functionality may be the private and authenticated communi-
1

2
Introduction and Preliminaries
cation of information through the Internet, the holding of tamper-proof
and secret electronic voting, or conducting any “fault-resilient” multi-
party computation. Indeed, the scope of modern cryptography is very
broad, and it stands in contrast to “classical” cryptography (which has
focused on the single problem of enabling secret communication over
insecure communication media).
The design of cryptographic systems is a very diﬃcult task. One
cannot rely on intuitions regarding the “typical” state of the environ-
ment in which the system operates. For sure, the adversary attacking the
system will try to manipulate the environment into “untypical” states.
Nor can one be content with counter-measures designed to withstand
speciﬁc attacks, since the adversary (which acts after the design of the
system is completed) will try to attack the schemes in ways that are
diﬀerent from the ones the designer had envisioned. The validity of the
above assertions seems self-evident, but still some people hope that in
practice ignoring these tautologies will not result in actual damage.
Experience shows that these hopes rarely come true; cryptographic
schemes based on make-believe are broken, typically sooner than later.
In view of the foregoing, we believe that it makes little sense to make
assumptions regarding the speciﬁc strategy that the adversary may use.
The only assumptions that can be justiﬁed refer to the computational
abilities of the adversary. Furthermore, the design of cryptographic sys-
tems has to be based on ﬁrm foundations; whereas ad-hoc approaches
and heuristics are a very dangerous way to go. A heuristic may make
sense when the designer has a very good idea regarding the environ-
ment in which a scheme is to operate, yet a cryptographic scheme has
to operate in a maliciously selected environment which typically tran-
scends the designer’s view.
This primer is aimed at presenting the foundations for cryptography.
The foundations of cryptography are the paradigms, approaches and
techniques used to conceptualize, deﬁne and provide solutions to nat-
ural “security concerns”. We will present some of these paradigms,
approaches and techniques as well as some of the fundamental results
obtained using them. Our emphasis is on the clariﬁcation of funda-
mental concepts and on demonstrating the feasibility of solving several
central cryptographic problems.

1.1. Introduction
3
Solving a cryptographic problem (or addressing a security concern)
is a two-stage process consisting of a deﬁnitional stage and a construc-
tional stage. First, in the deﬁnitional stage, the functionality underlying
the natural concern is to be identiﬁed, and an adequate cryptographic
problem has to be deﬁned. Trying to list all undesired situations is
infeasible and prone to error. Instead, one should deﬁne the function-
ality in terms of operation in an imaginary ideal model, and require a
candidate solution to emulate this operation in the real, clearly deﬁned,
model (which speciﬁes the adversary’s abilities). Once the deﬁnitional
stage is completed, one proceeds to construct a system that satisﬁes
the deﬁnition. Such a construction may use some simpler tools, and its
security is proved relying on the features of these tools. In practice, of
course, such a scheme may need to satisfy also some speciﬁc eﬃciency
requirements.
This primer focuses on several archetypical cryptographic problems
(e.g., encryption and signature schemes) and on several central tools
(e.g., computational diﬃculty, pseudorandomness, and zero-knowledge
proofs). For each of these problems (resp., tools), we start by presenting
the natural concern underlying it (resp., its intuitive objective), then
deﬁne the problem (resp., tool), and ﬁnally demonstrate that the prob-
lem may be solved (resp., the tool can be constructed). In the latter
step, our focus is on demonstrating the feasibility of solving the prob-
lem, not on providing a practical solution. As a secondary concern, we
typically discuss the level of practicality (or impracticality) of the given
(or known) solution.
Computational diﬃculty
The aforementioned tools and applications (e.g., secure encryption)
exist only if some sort of computational hardness exists. Speciﬁcally,
all these problems and tools require (either explicitly or implicitly) the
ability to generate instances of hard problems. Such ability is captured
in the deﬁnition of one-way functions. Thus, one-way functions are the
very minimum needed for doing most natural tasks of cryptography. (It
turns out, as we shall see, that this necessary condition is “essentially”
suﬃcient; that is, the existence of one-way functions (or augmentations

4
Introduction and Preliminaries
and extensions of this assumption) suﬃces for doing most of crypto-
graphy.)
Our current state of understanding of eﬃcient computation does
not allow us to prove that one-way functions exist. In particular, if
P = NP then no one-way functions exist. Furthermore, the existence
of one-way functions implies that NP is not contained in BPP ⊇P
(not even “on the average”). Thus, proving that one-way functions
exist is not easier than proving that P ̸= NP; in fact, the former task
seems signiﬁcantly harder than the latter. Hence, we have no choice (at
this stage of history) but to assume that one-way functions exist. As
justiﬁcation to this assumption we may only oﬀer the combined beliefs
of hundreds (or thousands) of researchers. Furthermore, these beliefs
concern a simply stated assumption, and their validity follows from
several widely believed conjectures which are central to various ﬁelds
(e.g., the conjectured intractability of integer factorization is central to
computational number theory).
Since we need assumptions anyhow, why not just assume what we
want (i.e., the existence of a solution to some natural cryptographic
problem)? Well, ﬁrst we need to know what we want: as stated above,
we must ﬁrst clarify what exactly we want; that is, go through the
typically complex deﬁnitional stage. But once this stage is completed,
can we just assume that the deﬁnition derived can be met? Not really:
once a deﬁnition is derived, how can we know that it can be met at
all? The way to demonstrate that a deﬁnition is viable (and that the
corresponding intuitive security concern can be satisﬁed at all) is to
construct a solution based on a better understood assumption (i.e.,
one that is more common and widely believed). For example, look-
ing at the deﬁnition of zero-knowledge proofs, it is not a-priori clear
that such proofs exist at all (in a non-trivial sense). The non-triviality
of the notion was ﬁrst demonstrated by presenting a zero-knowledge
proof system for statements, regarding Quadratic Residuosity, which
are believed to be hard to verify (without extra information). Further-
more, contrary to prior beliefs, it was later shown that the existence
of one-way functions implies that any NP-statement can be proved in
zero-knowledge. Thus, facts that were not known to hold at all (and
even believed to be false), were shown to hold by reduction to widely

1.1. Introduction
5
believed assumptions (without which most of modern cryptography col-
lapses anyhow). To summarize, not all assumptions are equal, and so
reducing a complex, new and doubtful assumption to a widely-believed
simple (or even merely simpler) assumption is of great value. Further-
more, reducing the solution of a new task to the assumed security of
a well-known primitive typically means providing a construction that,
using the known primitive, solves the new task. This means that we do
not only know (or assume) that the new task is solvable but we also
have a solution based on a primitive that, being well-known, typically
has several candidate implementations.
Prerequisites and structure
Our aim is to present the basic concepts, techniques and results in
cryptography. As stated above, our emphasis is on the clariﬁcation of
fundamental concepts and the relationship among them. This is done
in a way independent of the particularities of some popular number
theoretic examples. These particular examples played a central role in
the development of the ﬁeld and still oﬀer the most practical imple-
mentations of all cryptographic primitives, but this does not mean
that the presentation has to be linked to them. On the contrary, we
believe that concepts are best clariﬁed when presented at an abstract
level, decoupled from speciﬁc implementations. Thus, the most relevant
background for this primer is provided by basic knowledge of algorithms
(including randomized ones), computability and elementary probability
theory.
The primer is organized in two main parts, which are preceded by
preliminaries (regarding eﬃcient and feasible computations). The two
parts are Part I – Basic Tools and Part II – Basic Applications. The basic
tools consist of computational diﬃculty (one-way functions), pseudo-
randomness and zero-knowledge proofs. These basic tools are used for
the basic applications, which in turn consist of Encryption Schemes,
Signature Schemes, and General Cryptographic Protocols.
In order to give some feeling of the ﬂavor of the area, we have
included in this primer a few proof sketches, which some readers may
ﬁnd too terse. We stress that following these proof sketches is not

6
Introduction and Preliminaries
1: Introduction and Preliminaries
Part I: Basic Tools
2: Computational Diﬃculty (One-Way Functions)
3: Pseudorandomness
4: Zero-Knowledge
Part II: Basic Applications
5: Encryption Schemes
6: Signature and Message Authentication Schemes
7: General Cryptographic Protocols
Fig. 1.1 Organization of this primer
essential to understanding the rest of the material. In general, later
sections may refer to deﬁnitions and results in prior sections, but not
to the constructions and proofs that support these results. It may be
even possible to understand later sections without reading any prior
section, but we believe that the order we chose should be preferred
because it proceeds from the simplest notions to the most complex
ones.
Suggestions for further reading
This primer is a brief summary of the author’s two-volume work on
the subject (65; 67). Furthermore, Part I corresponds to (65), whereas
Part II corresponds to (67). Needless to say, the reader is referred to
these textbooks for further detail.
Two of the topics reviewed by this primer are zero-knowledge proofs
(which are probabilistic) and pseudorandom generators (and func-
tions). A wider perspective on probabilistic proof systems and pseudo-
randomness is provided in (62, Sections 2–3).
Current research on the foundations of cryptography appears in gen-
eral computer science conferences (e.g., FOCS and STOC), in crypto-
graphy conferences (e.g., Crypto and EuroCrypt) as well as in the newly
established Theory of Cryptography Conference (TCC).

1.2. Preliminaries
7
Practice.
The aim of this primer is to introduce the reader to the
theoretical foundations of cryptography. As argued above, such founda-
tions are necessary for sound practice of cryptography. Indeed, practice
requires more than theoretical foundations, whereas the current primer
makes no attempt to provide anything beyond the latter. However,
given a sound foundation, one can learn and evaluate various prac-
tical suggestions that appear elsewhere (e.g., in (97)). On the other
hand, lack of sound foundations results in inability to critically eval-
uate practical suggestions, which in turn leads to unsound decisions.
Nothing could be more harmful to the design of schemes that need to
withstand adversarial attacks than misconceptions about such attacks.
Non-cryptographic references:
Some “non-cryptographic” works
were referenced for sake of wider perspective. Examples include (4; 5;
6; 7; 55; 69; 78; 96; 118).
1.2
Preliminaries
Modern cryptography, as surveyed here, is concerned with the con-
struction of eﬃcient schemes for which it is infeasible to violate the
security feature. Thus, we need a notion of eﬃcient computations as
well as a notion of infeasible ones. The computations of the legitimate
users of the scheme ought be eﬃcient, whereas violating the security
features (by an adversary) ought to be infeasible. We stress that we do
not identify feasible computations with eﬃcient ones, but rather view
the former notion as potentially more liberal.
Eﬃcient computations and infeasible ones
Eﬃcient computations are commonly modeled by computations that are
polynomial-time in the security parameter. The polynomial bounding
the running-time of the legitimate user’s strategy is ﬁxed and typically
explicit (and small). Indeed, our aim is to have a notion of eﬃciency
that is as strict as possible (or, equivalently, develop strategies that are
as eﬃcient as possible). Here (i.e., when referring to the complexity
of the legitimate users) we are in the same situation as in any algo-
rithmic setting. Things are diﬀerent when referring to our assumptions

8
Introduction and Preliminaries
regarding the computational resources of the adversary, where we refer
to the notion of feasible that we wish to be as wide as possible. A com-
mon approach is to postulate that feasible computations are polynomial-
time too, but here the polynomial is not a-priori speciﬁed (and is to
be thought of as arbitrarily large). In other words, the adversary is
restricted to the class of polynomial-time computations and anything
beyond this is considered to be infeasible.
Although many deﬁnitions explicitly refer to the convention of asso-
ciating feasible computations with polynomial-time ones, this conven-
tion is inessential to any of the results known in the area. In all cases,
a more general statement can be made by referring to a general notion
of feasibility, which should be preserved under standard algorithmic
composition, yielding theories that refer to adversaries of running-time
bounded by any speciﬁc super-polynomial function (or class of func-
tions). Still, for sake of concreteness and clarity, we shall use the former
convention in our formal deﬁnitions (but our motivational discussions
will refer to an unspeciﬁed notion of feasibility that covers at least
eﬃcient computations).
Randomized (or probabilistic) computations
Randomized computations play a central role in cryptography. One
fundamental reason for this fact is that randomness is essential for
the existence (or rather the generation) of secrets. Thus, we must allow
the legitimate users to employ randomized computations, and certainly
(since randomization is feasible) we must consider also adversaries that
employ randomized computations. This brings up the issue of success
probability: typically, we require that legitimate users succeed (in ful-
ﬁlling their legitimate goals) with probability 1 (or negligibly close to
this), whereas adversaries succeed (in violating the security features)
with negligible probability. Thus, the notion of a negligible probability
plays an important role in our exposition. One requirement of the deﬁ-
nition of negligible probability is to provide a robust notion of rareness:
A rare event should occur rarely even if we repeat the experiment for a
feasible number of times. That is, in case we consider any polynomial-
time computation to be feasible, a function µ:N→N is called negligible

1.2. Preliminaries
9
if 1 −(1 −µ(n))p(n) < 0.01 for every polynomial p and suﬃciently big
n (i.e., µ is negligible if for every positive polynomial p′ the function
µ(·) is upper-bounded by 1/p′(·)). However, if we consider the function
T(n) to provide our notion of infeasible computation then functions
bounded above by 1/T(n) are considered negligible (in n).
We will also refer to the notion of noticeable probability. Here the
requirement is that events that occur with noticeable probability, will
occur almost surely (i.e., except with negligible probability) if we repeat
the experiment for a polynomial number of times. Thus, a function
ν : N →N is called noticeable if for some positive polynomial p′ the
function ν(·) is lower-bounded by 1/p′(·).

Part I
Basic Tools
10

11
In this part we survey three basic tools used in modern cryptography.
The most basic tool is computational diﬃculty, which in turn is cap-
tured by the notion of one-way functions. Next, we survey the notion
of computational indistinguishability, which underlies the theory of
pseudorandomness as well as much of the rest of cryptography. In par-
ticular, pseudorandom generators and functions are important tools
that will be used in later sections. Finally, we survey zero-knowledge
proofs, and their use in the design of cryptographic protocols. For more
details regarding the contents of the current part, see our textbook (65).


2
Computational Diﬃculty and One-way Functions
Modern cryptography is concerned with the construction of systems
that are easy to operate (properly) but hard to foil. Thus, a complexity
gap (between the ease of proper usage and the diﬃculty of deviating
from the prescribed functionality) lies at the heart of modern crypto-
graphy. However, gaps as required for modern cryptography are not
known to exist; they are only widely believed to exist. Indeed, almost
all of modern cryptography rises or falls with the question of whether
one-way functions exist. We mention that the existence of one-way
functions implies that NP contains search problems that are hard to
solve on the average, which in turn implies that NP is not contained
in BPP (i.e., a worst-case complexity conjecture).
Loosely speaking, one-way functions are functions that are easy to
evaluate but hard (on the average) to invert. Such functions can be
thought of as an eﬃcient way of generating “puzzles” that are infeasible
to solve (i.e., the puzzle is a random image of the function and a solution
is a corresponding preimage). Furthermore, the person generating the
puzzle knows a solution to it and can eﬃciently verify the validity of
(possibly other) solutions to the puzzle. Thus, one-way functions have,
by deﬁnition, a clear cryptographic ﬂavor (i.e., they manifest a gap
between the ease of one task and the diﬃculty of a related one).
13

14
Computational Diﬃculty and One-way Functions
x
f(x)
easy
HARD
Fig. 2.1 One-way functions – an illustration.
2.1
One-way functions
One-way functions are functions that are eﬃciently computable but
infeasible to invert (in an average-case sense). That is, a function f :
{0, 1}∗→{0, 1}∗is called one-way if there is an eﬃcient algorithm
that on input x outputs f(x), whereas any feasible algorithm that tries
to ﬁnd a preimage of f(x) under f may succeed only with negligible
probability (where the probability is taken uniformly over the choices
of x and the algorithm’s coin tosses). Associating feasible computations
with probabilistic polynomial-time algorithms, we obtain the following
deﬁnition.
Deﬁnition 2.1. (one-way functions): A function f : {0, 1}∗→{0, 1}∗
is called one-way if the following two conditions hold:
(1) easy to evaluate: There exist a polynomial-time algorithm A
such that A(x) = f(x) for every x ∈{0, 1}∗.
(2) hard to invert: For every probabilistic polynomial-time algo-
rithm A′, every polynomial p, and all suﬃciently large n,
Pr[A′(f(x), 1n) ∈f −1(f(x))] <
1
p(n)

2.1. One-way functions
15
where the probability is taken uniformly over all the possible
choices of x ∈{0, 1}n and all the possible outcomes of the
internal coin tosses of algorithm A′.
Algorithm A′ is given the auxiliary input 1n so to allow it to run in
time polynomial in the length of x, which is important in case f drasti-
cally shrinks its input (e.g., |f(x)| = O(log |x|)). Typically, f is length
preserving, in which case the auxiliary input 1n is redundant. Note
that A′ is not required to output a speciﬁc preimage of f(x); any pre-
image (i.e., element in the set f −1(f(x))) will do. (Indeed, in case f is
1-1, the string x is the only preimage of f(x) under f; but in general
there may be other preimages.) It is required that algorithm A′ fails (to
ﬁnd a preimage) with overwhelming probability, when the probability
is also taken over the input distribution. That is, f is “typically” hard
to invert, not merely hard to invert in some (“rare”) cases.
Some of the most popular candidates for one-way functions are
based on the conjectured intractability of computational problems in
number theory. One such conjecture is that it is infeasible to factor
large integers. Consequently, the function that takes as input two (equal
length) primes and outputs their product is widely believed to be a one-
way function. Furthermore, factoring such a composite is infeasible if
and only if squaring modulo such a composite is a one-way function
(see (109)). For certain composites (i.e., products of two primes that
are both congruent to 3 mod 4), the latter function induces a permuta-
tion over the set of quadratic residues modulo this composite. A related
permutation, which is widely believed to be one-way, is the RSA func-
tion (112): x →xe mod N, where N = P · Q is a composite as above, e
is relatively prime to (P −1)·(Q−1), and x ∈{0, ..., N −1}. The latter
examples (as well as other popular suggestions) are better captured by
the following formulation of a collection of one-way functions (which is
indeed related to Deﬁnition 2.1):
Deﬁnition 2.2. (collections of one-way functions): A collection of
functions, {fi : Di →{0, 1}∗}i∈I, is called one-way if there exists three
probabilistic polynomial-time algorithms, I, D and F, so that the fol-
lowing two conditions hold:

16
Computational Diﬃculty and One-way Functions
(1) easy to sample and compute: On input 1n, the output of
(the index selection) algorithm I is distributed over the set
I ∩{0, 1}n (i.e., is an n-bit long index of some function).
On input (an index of a function) i ∈I, the output of (the
domain sampling) algorithm D is distributed over the set Di
(i.e., over the domain of the function). On input i ∈I and
x ∈Di, (the evaluation) algorithm F always outputs fi(x).
(2) hard to invert:1 For every probabilistic polynomial-time algo-
rithm, A′, every positive polynomial p(·), and all suﬃciently
large n’s
Pr

A′(i, fi(x))∈f −1
i
(fi(x))

<
1
p(n)
where i ←I(1n) and x ←D(i).
The collection is said to be a collection of permutations if each of the
fi’s is a permutation over the corresponding Di, and D(i) is almost
uniformly distributed in Di.
For example, in the case of the RSA, fN,e : DN,e →DN,e satisﬁes
fN,e(x) = xe mod N, where DN,e = {0, ..., N −1}. Deﬁnition 2.2 is
also a good starting point for the deﬁnition of a trapdoor permuta-
tion.2 Loosely speaking, the latter is a collection of one-way permuta-
tions augmented with an eﬃcient algorithm that allows for inverting
the permutation when given adequate auxiliary information (called a
trapdoor).
Deﬁnition 2.3. (trapdoor permutations): A collection of permutations
as in Deﬁnition 2.2 is called a trapdoor permutation if there are two aux-
iliary probabilistic polynomial-time algorithms I′ and F −1 such that
(1) the distribution I′(1n) ranges over pairs of strings so that the ﬁrst
1 Note that this condition refers to the distributions I(1n) and D(i), which are merely
required to range over I ∩{0, 1}n and Di, respectively. (Typically, the distributions I(1n)
and D(i) are (almost) uniform over I ∩{0, 1}n and Di, respectively.)
2 Indeed, a more adequate term would be a collection of trapdoor permutations, but the
shorter (and less precise) term is the commonly used one.

2.1. One-way functions
17
string is distributed as in I(1n), and (2) for every (i, t) in the range of
I′(1n) and every x ∈Di it holds that F −1(t, fi(x)) = x. (That is, t is a
trapdoor that allows to invert fi.)
For example, in the case of the RSA, fN,e can be inverted by raising to
the power d (modulo N = P ·Q), where d is the multiplicative inverse of
e modulo (P −1)·(Q−1). Indeed, in this case, the trapdoor information
is (N, d).
Strong versus weak one-way functions
Recall that the above deﬁnitions require that any feasible algorithm
succeeds in inverting the function with negligible probability. A weaker
notion only requires that any feasible algorithm fails to invert the
function with noticeable probability. It turns out that the existence of
such weak one-way functions implies the existence of strong one-way
functions (as deﬁned above). The construction itself is straightforward:
one just parses the argument to the new function into suﬃciently many
blocks, and applies the weak one-way function on the individual blocks.
We warn that the hardness of inverting the resulting function is not
established by mere “combinatorics” (i.e., considering the relative vol-
ume of St in Ut, for S ⊂U, where S represents the set of “easy to invert”
images). Speciﬁcally, one may not assume that the potential inverting
algorithm works independently on each block. Indeed this assumption
seems reasonable, but we should not assume that the adversary behaves
in a reasonable way (unless we can actually prove that it gains nothing
by behaving in other ways, which seem unreasonable to us).
The hardness of inverting the resulting function is proved via a
so-called “reducibility argument” (which is used to prove all condi-
tional results in the area). Speciﬁcally, we show that any algorithm that
inverts the resulting function F with non-negligible success probability
can be used to construct an algorithm that inverts the original function
f with success probability that violates the hypothesis (regarding f). In
other words, we reduce the task of “strongly inverting” f (i.e., violating
its weak one-wayness) to the task of “weakly inverting” F (i.e., violating
its strong one-wayness). We hint that, on input y = f(x), the reduction

18
Computational Diﬃculty and One-way Functions
invokes the F-inverter (polynomially) many times, each time feeding it
with a sequence of random f-images that contains y at a random loca-
tion. (Indeed such a sequence corresponds to a random image of F.)
The analysis of this reduction, presented in (65, Sec. 2.3), demonstrates
that dealing with computational diﬃculty is much more involved than
the analogous combinatorial question. An alternative demonstration of
the diﬃculty of reasoning about computational diﬃculty (in compar-
ison to an analogous purely probabilistic situation) is provided in the
proof of Theorem 2.5.
2.2
Hard-core predicates
Loosely speaking, saying that a function f is one-way implies that given
y (in the range of f) it is infeasible to ﬁnd a preimage of y under f.
This does not mean that it is infeasible to ﬁnd out partial informa-
tion about the preimage(s) of y under f. Speciﬁcally it may be easy
to retrieve half of the bits of the preimage (e.g., given a one-way func-
tion f consider the function g deﬁned by g(x, r) def
= (f(x), r), for every
|x| = |r|). As will become clear in subsequent sections, hiding partial
information (about the function’s preimage) plays an important role
in more advanced constructs (e.g., secure encryption). Thus, we will
ﬁrst show how to transform any one-way function into a one-way func-
tion that hides speciﬁc partial information about its preimage, where
this partial information is easy to compute from the preimage itself.
This partial information can be considered as a “hard core” of the dif-
ﬁculty of inverting f. Loosely speaking, a polynomial-time computable
(Boolean) predicate b, is called a hard-core of a function f if no feasible
algorithm, given f(x), can guess b(x) with success probability that is
non-negligibly better than one half.
Deﬁnition 2.4. (hard-core predicates (31)): A polynomial-time com-
putable predicate b : {0, 1}∗→{0, 1} is called a hard-core of a function
f if for every probabilistic polynomial-time algorithm A′, every positive

2.2. Hard-core predicates
19
polynomial p(·), and all suﬃciently large n’s
Pr
A′(f(x))=b(x)
 < 1
2 +
1
p(n)
where the probability is taken uniformly over all the possible choices
of x ∈{0, 1}n and all the possible outcomes of the internal coin tosses
of algorithm A′.
Note that for every b : {0, 1}∗→{0, 1} and f : {0, 1}∗→{0, 1}∗,
there exist obvious algorithms that guess b(x) from f(x) with success
probability at least one half (e.g., the algorithm that, obliviously of its
input, outputs a uniformly chosen bit). Also, if b is a hard-core predicate
(for any function) then it follows that b is almost unbiased (i.e., for a
uniformly chosen x, the diﬀerence |Pr[b(x)=0] −Pr[b(x)=1]| must be
a negligible function in n). Finally, if b is a hard-core of a 1-1 function
f that is polynomial-time computable then f is a one-way function.
Theorem 2.5. ((72), see simpler proof in (65, Sec. 2.5.2)): For any
one-way function f, the inner-product mod 2 of x and r is a hard-core
of f ′(x, r) = (f(x), r).
The proof is by a so-called “reducibility argument” (which is used to
prove all conditional results in the area). Speciﬁcally, we reduce the task
of inverting f to the task of predicting the hard-core of f ′, while mak-
ing sure that the reduction (when applied to input distributed as in the
inverting task) generates a distribution as in the deﬁnition of the pre-
dicting task. Thus, a contradiction to the claim that b is a hard-core of f ′
yields a contradiction to the hypothesis that f is hard to invert. We stress
that this argument is far more complex than analyzing the correspond-
ing “probabilistic” situation (i.e., the distribution of the inner-product
mod 2 of X and r, conditioned on a uniformly selected r ∈{0, 1}n, where
X is a random variable with super-logarithmic min-entropy, which rep-
resents the “eﬀective” knowledge of x, when given f(x)).3
3 The min-entropy of X is deﬁned as minv{log2(1/Pr[X = v])}; that is, if X has min-entropy
m then maxv{Pr[X = v]} = 2−m. The Leftover Hashing Lemma (120; 27; 87) implies that,
in this case, Pr[b(X, Un) = 1|Un] = 1
2 ±2−Ω(m), where Un denotes the uniform distribution
over {0, 1}n, and b(u, v) denotes the inner-product mod 2 of u and v.

20
Computational Diﬃculty and One-way Functions
Proof sketch:
The actual proof refers to an arbitrary algorithm B
that, when given (f(x), r), tries to guess b(x, r). Suppose that this
algorithm succeeds with probability 1
2+ϵ, where the probability is taken
over the random choices of x and r (as well as the internal coin tosses
of B). By an averaging argument, we ﬁrst identify a ϵ/2 fraction of the
possible coin tosses of B such that using any of these coin sequences B
succeeds with probability at least 1
2 + ϵ/2. Similarly, we can identify a
ϵ/4 fraction of the x’s such that B succeeds (in guessing b(x, r)) with
probability at least 1
2 + ϵ/4, where now the probability is taken only
over the r’s. We will show how to use B in order to invert f, on input
f(x), provided that x is in the good set (which has density ϵ/4).
As a warm-up, suppose for a moment that, for the aforemen-
tioned x’s, algorithm B succeeds with probability p > 3
4 + 1/poly(|x|)
(rather than at least
1
2 + ϵ/4). In this case, retrieving x from f(x)
is quite easy: To retrieve the ith bit of x, denoted xi, we ﬁrst ran-
domly select r ∈{0, 1}|x|, and obtain B(f(x), r) and B(f(x), r ⊕ei),
where ei = 0i−110|x|−i and v ⊕u denotes the addition mod 2 of the
binary vectors v and u. Note that if both B(f(x), r) = b(x, r) and
B(f(x), r⊕ei) = b(x, r⊕ei) indeed hold, then B(f(x), r)⊕B(f(x), r⊕ei)
equals b(x, r) ⊕b(x, r ⊕ei) = b(x, ei) = xi. The probability that both
B(f(x), r)=b(x, r) and B(f(x), r ⊕ei)=b(x, r ⊕ei) hold, for a random
r, is at least 1 −2 · (1 −p) > 1
2 +
1
poly(|x|). Hence, repeating the above
procedure suﬃciently many times (using independent random choices
of such r’s) and ruling by majority, we retrieve xi with very high prob-
ability. Similarly, we can retrieve all the bits of x, and hence invert f
on f(x). However, the entire analysis was conducted under (the unjus-
tiﬁable) assumption that p > 3
4 +
1
poly(|x|), whereas we only know that
p > 1
2 + ϵ
4 (for ϵ > 1/poly(|x|)).
The problem with the above procedure is that it doubles the orig-
inal error probability of algorithm B on inputs of the form (f(x), ·).
Under the unrealistic assumption (made above), that B’s average error
on such inputs is non-negligibly smaller than 1
4, the “error-doubling”
phenomenon raises no problems. However, in general (and even in the
special case where B’s error is exactly 1
4) the above procedure is unlikely
to invert f. Note that the average error probability of B (for a ﬁxed

2.2. Hard-core predicates
21
f(x), when the average is taken over a random r) cannot be decreased
by repeating B several times (e.g., for every x, it may be that B always
answer correctly on three quarters of the pairs (f(x), r), and always err
on the remaining quarter). What is required is an alternative way of
using the algorithm B, a way that does not double the original error
probability of B.
The key idea is to generate the r’s in a way that allows to apply
algorithm B only once per each r (and i), instead of twice. Speciﬁcally,
we will use algorithm B to obtain a “guess” for b(x, r ⊕ei), and obtain
b(x, r) in a diﬀerent way (which does not use B). The good news is
that the error probability is no longer doubled, since we only use B to
get a “guess” of b(x, r ⊕ei). The bad news is that we still need to know
b(x, r), and it is not clear how we can know b(x, r) without applying
B. The answer is that we can guess b(x, r) by ourselves. This is ﬁne if
we only need to guess b(x, r) for one r (or logarithmically in |x| many
r’s), but the problem is that we need to know (and hence guess) the
value of b(x, r) for polynomially many r’s. The obvious way of guessing
these b(x, r)’s yields an exponentially small success probability. Instead,
we generate these polynomially many r’s such that, on one hand they
are “suﬃciently random” whereas, on the other hand, we can guess all
the b(x, r)’s with noticeable success probability.4 Speciﬁcally, generat-
ing the r’s in a speciﬁc pairwise independent manner will satisfy both
(seemingly contradictory) requirements. We stress that in case we are
successful (in our guesses for all the b(x, r)’s), we can retrieve x with
high probability. Hence, we retrieve x with noticeable probability.
A word about the way in which the pairwise independent r’s are gen-
erated (and the corresponding b(x, r)’s are guessed) is indeed in place.
To generate m = poly(|x|) many r’s, we uniformly (and independently)
select ℓdef
= log2(m + 1) strings in {0, 1}|x|. Let us denote these strings
by s1, ..., sℓ. We then guess b(x, s1) through b(x, sℓ). Let us denote
these guesses, which are uniformly (and independently) chosen in {0, 1},
by σ1 through σℓ. Hence, the probability that all our guesses for the
b(x, si)’s are correct is 2−ℓ=
1
poly(|x|). The diﬀerent r’s correspond to the
diﬀerent non-empty subsets of {1, 2, ..., ℓ}. Speciﬁcally, for every such
4 Alternatively, we can try all polynomially many possible guesses.

22
Computational Diﬃculty and One-way Functions
subset J, we let rJ def
= ⊕j∈Jsj. The reader can easily verify that the rJ’s
are pairwise independent and each is uniformly distributed in {0, 1}|x|.
The key observation is that b(x, rJ) = b(x, ⊕j∈Jsj) = ⊕j∈Jb(x, sj).
Hence, our guess for b(x, rJ) is ⊕j∈Jσj, and with noticeable probabil-
ity all our guesses are correct.
2

3
Pseudorandomness
In practice “pseudorandom” sequences are often used instead of truly
random sequences. The underlying belief is that if an (eﬃcient) appli-
cation performs well when using a truly random sequence then it will
perform essentially as well when using a “pseudorandom” sequence.
However, this belief is not supported by ad-hoc notions of “pseudoran-
domness” such as passing the statistical tests in (92) or having large
linear-complexity (as in (83)). In contrast, the above belief is an easy
corollary of deﬁning pseudorandom distributions as ones that are com-
putationally indistinguishable from uniform distributions.
Loosely speaking, pseudorandom generators are eﬃcient procedures
for creating long “random-looking” sequences based on few truly ran-
dom bits (i.e., a short random seed). The relevance of such constructs
to cryptography is in the ability of legitimate users that share short
random seeds to create large objects that look random to any feasible
adversary (which does not know the said seed).
23

24
Pseudorandomness
3.1
Computational indistinguishability
Indistinguishable things are identical
(or should be considered as identical).
The Principle of Identity of Indiscernibles
G.W. Leibniz (1646–1714)
(Leibniz admits that counterexamples to this principle are conceivable,
but will not occur in real life because God is much too benevolent.)
A central notion in modern cryptography is that of “eﬀective sim-
ilarity” (introduced by Goldwasser, Micali and Yao (80; 123)). The
underlying thesis is that we do not care whether or not objects are
equal, all we care about is whether or not a diﬀerence between the
objects can be observed by a feasible computation. In case the answer
is negative, the two objects are equivalent as far as any practical appli-
cation is concerned. Indeed, in the sequel we will often interchange
such (computationally indistinguishable) objects. Let X = {Xn}n∈N
and Y = {Yn}n∈N be probability ensembles such that each Xn and
Yn is a distribution that ranges over strings of length n (or polyno-
mial in n). We say that X and Y are computationally indistinguishable
if for every feasible algorithm A the diﬀerence dA(n) def
= |Pr[A(Xn) =
1] −Pr[A(Yn)=1]| is a negligible function in n. That is:
Deﬁnition 3.1. (computational indistinguishability (80; 123)): We
say that X = {Xn}n∈N and Y = {Yn}n∈N are computationally indistin-
guishable if for every probabilistic polynomial-time algorithm D every
polynomial p, and all suﬃciently large n,
|Pr[D(Xn)=1] −Pr[D(Yn)=1]| <
1
p(n)
where the probabilities are taken over the relevant distribution (i.e.,
either Xn or Yn) and over the internal coin tosses of algorithm D.
That is, we can think of D as somebody who wishes to distinguish two
distributions (based on a sample given to it), and think of 1 as D’s

3.1. Computational indistinguishability
25
verdict that the sample was drawn according to the ﬁrst distribution.
Saying that the two distributions are computationally indistinguishable
means that if D is a feasible procedure then its verdict is not really
meaningful (because the verdict is almost as often 1 when the input is
drawn from the ﬁrst distribution as when the input is drawn from the
second distribution).
Indistinguishability by multiple samples
We comment that, for “eﬃciently constructible” distributions, indis-
tinguishability by a single sample (as deﬁned above) implies indistin-
guishability by multiple samples (see (65, Sec. 3.2.3)). The proof of
this fact provides a simple demonstration of a central proof technique,
known as a hybrid argument, which we brieﬂy present next.
To prove that a sequence of independently drawn samples of one dis-
tribution is indistinguishable from a sequence of independently drawn
samples from the other distribution, we consider hybrid sequences such
that the ith hybrid consists of i samples taken from the ﬁrst distribution
and the rest taken from the second distribution. The “homogeneous”
sequences (which we wish to prove to be computational indistinguish-
able) are the extreme hybrids (i.e., the ﬁrst and last hybrids consid-
ered above). The key observation is that distinguishing the extreme
hybrids (towards the contradiction hypothesis) yields a procedure for
distinguishing single samples of the two distributions (contradicting the
hypothesis that the two distributions are indistinguishable by a single
sample). Speciﬁcally, if D distinguishes the extreme hybrids, then it
also distinguishes a random pair of neighboring hybrids (i.e., D distin-
guishes the ith hybrid from the i + 1st hybrid, for a randomly selected
i). Using D, we obtain a distinguisher D′ of single samples: Given a
single sample, D′ selects i at random, generates i samples from the
ﬁrst distribution and the rest from the second, and invokes D with
the corresponding sequence, while placing the input sample in location
i+1 of the sequence. We stress that although the original distinguisher
D (arising from the contradiction hypothesis) was only “supposed to
work” for the extreme hybrids, we can consider D’s performance on

26
Pseudorandomness
any distribution that we please, and draw adequate conclusions (as we
have done).
Gen
seed
output  sequence
a  truly random  sequence
?
Fig. 3.1 Pseudorandom generators – an illustration.
3.2
Pseudorandom generators
Loosely speaking, a pseudorandom generator is an eﬃcient (determinis-
tic) algorithm that on input of a short random seed outputs a (typically
much) longer sequence that is computationally indistinguishable from a
uniformly chosen sequence. Pseudorandom generators were introduced
by Blum, Micali and Yao (31; 123), and are formally deﬁned as follows.
Deﬁnition 3.2. (pseudorandom generator (31; 123)): Let ℓ: N →N
satisfy ℓ(n) > n, for all n ∈N. A pseudorandom generator, with stretch
function ℓ, is a (deterministic) polynomial-time algorithm G satisfying
the following:
(1) For every s ∈{0, 1}∗, it holds that |G(s)| = ℓ(|s|).
(2) {G(Un)}n∈N and {Uℓ(n)}n∈N are computationally indistin-
guishable, where Um denotes the uniform distribution over
{0, 1}m.
Indeed, the probability ensemble {G(Un)}n∈N is called pseudorandom.
Thus, pseudorandom sequences can replace truly random sequences
not only in “standard” algorithmic applications but also in crypto-
graphic ones. That is, any cryptographic application that is secure
when the legitimate parties use truly random sequences, is also secure

3.2. Pseudorandom generators
27
when the legitimate parties use pseudorandom sequences. The beneﬁt
in such a substitution (of random sequences by pseudorandom ones) is
that the latter sequences can be eﬃciently generated using much less
true randomness. Furthermore, in an interactive setting, it is possible
to eliminate all random steps from the on-line execution of a program,
by replacing them with the generation of pseudorandom bits based on
a random seed selected and ﬁxed oﬀ-line (or at set-up time).
Various cryptographic applications of pseudorandom generators will
be presented in the sequel, but ﬁrst let us show a construction of
pseudorandom generators based on the simpler notion of a one-way
function. Using Theorem 2.5, we may actually assume that such a func-
tion is accompanied by a hard-core predicate. We start with a simple
construction that suﬃces for the case of 1-1 (and length-preserving)
functions.
Theorem 3.3. ((31; 123), see (65, Sec. 3.4)): Let f be a 1-1 function
that is length-preserving and eﬃciently computable, and b be a hard-
core predicate of f. Then G(s) = b(s) · b(f(s)) · · · b(f ℓ(|s|)−1(s)) is a
pseudorandom generator (with stretch function ℓ), where f i+1(x) def
=
f(f i(x)) and f 0(x) def
= x.
As a concrete example, consider the permutation1 x →x2 mod N,
where N is the product of two primes each congruent to 3
(mod 4),
and x is a quadratic residue modulo N. Then, we have GN(s) =
lsb(s) · lsb(s2 mod N) · · · lsb(s2ℓ(|s|)−1 mod N), where lsb(x) is the least
signiﬁcant bit of x (which is a hard-core of the modular squaring func-
tion (3)).
Proof sketch of Theorem 3.3:
We use the fundamental fact that
asserts that the following two conditions are equivalent:
1 It is a well-known fact (cf., (65, Apdx. A.2.4)) that, for such N’s, the mapping x →
x2 mod N is a permutation over the set of quadratic residues modulo N.

28
Pseudorandomness
(1) The distribution X (in our case {G(Un)}n∈N) is pseudoran-
dom (i.e., is computationally indistinguishable from a uni-
form distribution (on {Uℓ(n)}n∈N)).
(2) The distribution X is unpredictable in polynomial-time; that
is, no feasible algorithm, given a preﬁx of the sequence, can
guess its next bit with a non-negligible advantage over 1
2.
Clearly, pseudorandomness implies polynomial-time unpredictabil-
ity (i.e., polynomial-time predictability violates pseudorandomness).
The converse is shown using a hybrid argument, which refers to
hybrids consisting of a preﬁx of X followed by truly random bits
(i.e., a suﬃx of the uniform distribution). Thus, we focus on prov-
ing that G′(Un) is polynomial-time unpredictable, where G′(s) =
b(f ℓ(|s|)−1(s)) · · · b(f(s)) · b(s) is the reverse of G(s).
Suppose towards the contradiction that, for some j < ℓdef
= ℓ(n),
given the j-bit long preﬁx of G′(Un) an algorithm A′ can predict the
j + 1st bit of G′(Un). That is, given b(f ℓ−1(s)) · · · b(f ℓ−j(s)), algo-
rithm A′ predicts b(f ℓ−(j+1)(s)), where s is uniformly distributed in
{0, 1}n. Then, for x uniformly distributed in {0, 1}n, given y = f(x)
one can predict b(x) by invoking A′ on input b(f j−1(y)) · · · b(y) =
b(f j(x)) · · · b(f(x)), which in turn is polynomial-time computable from
y = f(x). In the analysis, we use the hypothesis that f induces a per-
mutation over {0, 1}n, and associate x with f ℓ−(j+1)(s).
2
We mention that the existence of a pseudorandom generator with
any stretch function (including the very minimal stretch function
ℓ(n) = n + 1) implies the existence of pseudorandom generators
for any desired stretch function. The construction is similar to the
one presented in Theorem 3.3. That is, for a pseudorandom gener-
ator G1, let F(x) (resp., B(x)) denote the ﬁrst |x| bits of G1(x)
(resp., the |x| + 1st bit of G1(x)), and consider G(s) = B(s) ·
B(F(s)) · · · B(F ℓ(|s|)−1(s)), where ℓis the desired stretch. Although F is
not necessarily 1-1, it can be shown that G is a pseudorandom generator
(65, Sec. 3.3.2).
We conclude this section by mentioning that pseudorandom gen-
erators can be constructed from any one-way function (rather than
merely from one-way permutations, as above). On the other hand, the

3.3. Pseudorandom functions
29
existence of one-way functions is a necessary condition to the existence
of pseudorandom generators. That is:
Theorem 3.4. (85): Pseudorandom generators exist if and only if one-
way functions exist.
The necessary condition is easy to establish. Given a pseudorandom
generator G that stretches by a factor of two, consider the func-
tion f(x) = G(x) (or, to obtain a length preserving-function, let
f(x, y) = G(x), where |x| = |y|). An algorithm that inverts f with
non-negligible success probability (on the distribution f(Un) = G(Un))
yields a distinguisher of {G(Un)}n∈N from {U2n}n∈N, because the prob-
ability that U2n is an image of f is negligible.
3.3
Pseudorandom functions
Pseudorandom generators provide a way to eﬃciently generate long
pseudorandom sequences from short random seeds. Pseudorandom
functions, introduced and constructed by Goldreich, Goldwasser and
Micali (68), are even more powerful: they provide eﬃcient direct access
to bits of a huge pseudorandom sequence (which is not feasible to
scan bit-by-bit). More precisely, a pseudorandom function is an eﬃcient
(deterministic) algorithm that given an n-bit seed, s, and an n-bit argu-
ment, x, returns an n-bit string, denoted fs(x), so that it is infeasible
to distinguish the values of fs, for a uniformly chosen s ∈{0, 1}n, from
the values of a truly random function F : {0, 1}n →{0, 1}n. That is,
the (feasible) testing procedure is given oracle access to the function
(but not its explicit description), and cannot distinguish the case it is
given oracle access to a pseudorandom function from the case it is given
oracle access to a truly random function.
One key feature of the above deﬁnition is that pseudorandom func-
tions can be generated and shared by merely generating and sharing
their seed; that is, a “random looking” function fs : {0, 1}n →{0, 1}n,
is determined by its n-bit seed s. Parties wishing to share a “random

30
Pseudorandomness
looking” function fs (determining 2n-many values), merely need to gen-
erate and share among themselves the n-bit seed s. (For example, one
party may randomly select the seed s, and communicate it, via a secure
channel, to all other parties.) Sharing a pseudorandom function allows
parties to determine (by themselves and without any further commu-
nication) random-looking values depending on their current views of
the environment (which need not be known a priori). To appreciate the
potential of this tool, one should realize that sharing a pseudorandom
function is essentially as good as being able to agree, on the ﬂy, on the
association of random values to (on-line) given values, where the latter
are taken from a huge set of possible values. We stress that this agree-
ment is achieved without communication and synchronization: When-
ever some party needs to associate a random value to a given value,
v ∈{0, 1}n, it will associate to v the (same) random value rv ∈{0, 1}n
(by setting rv = fs(v), where fs is a pseudorandom function agreed
upon beforehand).
Theorem 3.5. ((68), see (65, Sec. 3.6.2)): Pseudorandom functions
can be constructed using any pseudorandom generator.
Proof sketch: Let G be a pseudorandom generator that stretches its
seed by a factor of two (i.e., ℓ(n) = 2n), and let G0(s) (resp., G1(s))
denote the ﬁrst (resp., last) |s| bits in G(s). Deﬁne
Gσ|s|···σ2σ1(s) def
= Gσ|s|(· · · Gσ2(Gσ1(s)) · · ·).
We consider the function ensemble {fs : {0, 1}|s| →{0, 1}|s|}s∈{0,1}∗,
where fs(x) def
= Gx(s). Pictorially, the function fs is deﬁned by n-step
walks down a full binary tree of depth n having labels at the vertices.
The root of the tree, hereafter referred to as the level 0 vertex of the
tree, is labeled by the string s. If an internal vertex is labeled r then
its left child is labeled G0(r) whereas its right child is labeled G1(r).
The value of fs(x) is the string residing in the leaf reachable from the
root by a path corresponding to the string x.
We claim that the function ensemble {fs}s∈{0,1}∗, deﬁned above, is
pseudorandom. The proof uses the hybrid technique: The ith hybrid,

3.3. Pseudorandom functions
31
Hi
n, is a function ensemble consisting of 22i·n functions {0, 1}n →
{0, 1}n, each deﬁned by 2i random n-bit strings, denoted s
=
⟨sβ⟩β∈{0,1}i. The value of such function hs at x = αβ, where |β| = i, is
Gα(sβ). (Pictorially, the function hs is deﬁned by placing the strings in
s in the corresponding vertices of level i, and labeling vertices of lower
levels using the very rule used in the deﬁnition of fs.) The extreme
hybrids correspond to our indistinguishability claim (i.e., H0
n ≡fUn
and Hn
n is a truly random function), and neighboring hybrids can be
related to our indistinguishability hypothesis (speciﬁcally, to the indis-
tinguishability of G(Un) and U2n under multiple samples).
2
Useful variants (and generalizations) of the notion of pseudorandom
functions include Boolean pseudorandom functions that are deﬁned for
all strings (i.e., fs : {0, 1}∗→{0, 1}) and pseudorandom functions
that are deﬁned for other domains and ranges (i.e., fs : {0, 1}d(|s|) →
{0, 1}r(|s|), for arbitrary polynomially bounded functions d, r : N →
N). Various transformations between these variants are known (cf. (65,
Sec. 3.6.4) and (67, Apdx. C.2)).
Applications and a generic methodology.
Pseudorandom func-
tions are a very useful cryptographic tool: One may ﬁrst design a
cryptographic scheme assuming that the legitimate users have black-
box access to a random function, and next implement the random func-
tion using a pseudorandom function. The usefulness of this tool stems
from the fact that having (black-box) access to a random function gives
the legitimate parties a potential advantage over the adversary (which
does not have free access to this function).2 The security of the resulting
implementation (which uses a pseudorandom function) is established
in two steps: First one proves the security of an idealized scheme that
uses a truly random function, and next one argues that the actual
implementation (which uses a pseudorandom function) is secure (as
otherwise one obtains an eﬃcient oracle machine that distinguishes a
pseudorandom function from a truly random one).
2 The aforementioned methodology is sound provided that the adversary does not get the
description of the pseudorandom function (i.e., the seed) in use, but has only (possibly
limited) oracle access to it. This is diﬀerent from the so-called Random Oracle Methodology
formulated in (22) and criticized in (38).


4
Zero-Knowledge
Zero-knowledge proofs, introduced by Goldwasser, Micali and Rack-
oﬀ(81), provide a powerful tool for the design of cryptographic pro-
tocols. Loosely speaking, zero-knowledge proofs are proofs that yield
nothing beyond the validity of the assertion. That is, a veriﬁer obtain-
ing such a proof only gains conviction in the validity of the assertion
(as if it was told by a trusted party that the assertion holds). This is
formulated by saying that anything that is feasibly computable from
a zero-knowledge proof is also feasibly computable from the (valid)
assertion itself. The latter formulation follows the simulation paradigm,
which is discussed next.
4.1
The simulation paradigm
A key question regarding the modeling of security concerns is how
to express the intuitive requirement that an adversary “gains nothing
substantial” by deviating from the prescribed behavior of an honest
user. Our approach is that the adversary gains nothing if whatever it
can obtain by unrestricted adversarial behavior can also be obtained
within essentially the same computational eﬀort by a benign behav-
33

34
Zero-Knowledge
                                                                        







?
!
?
!
??
 !
X
X  is  true!
Fig. 4.1 Zero-knowledge proofs – an illustration.
ior. The deﬁnition of the “benign behavior” captures what we want
to achieve in terms of security, and is speciﬁc to the security concern
to be addressed. For example, in the previous paragraph, we said that
a proof is zero-knowledge if it yields nothing (to the adversarial veri-
ﬁer) beyond the validity of the assertion (i.e., the benign behavior is any
computation that is based (only) on the assertion itself, while assuming
that the latter is valid). Other examples are discussed in Sections 5.1
and 7.1.
A notable property of the aforementioned simulation paradigm, as
well as of the entire approach surveyed here, is that this approach is
overly liberal with respect to its view of the abilities of the adversary
as well as to what might constitute a gain for the adversary. Thus,
the approach may be considered overly cautious, because it prohibits
also “non-harmful” gains of some “far-fetched” adversaries. We warn
against this impression. Firstly, there is nothing more dangerous in
cryptography than to consider “reasonable” adversaries (a notion which
is almost a contradiction in terms): typically, the adversaries will try
exactly what the system designer has discarded as “far-fetched”. Sec-
ondly, it seems impossible to come up with deﬁnitions of security that
distinguish “breaking the scheme in a harmful way” from “breaking it in
a non-harmful way”: what is harmful is application-dependent, whereas
a good deﬁnition of security ought to be application-independent (as
otherwise using the scheme in any new application will require a full
re-evaluation of its security). Furthermore, even with respect to a spe-

4.2. The actual deﬁnition
35
ciﬁc application, it is typically very hard to classify the set of “harmful
breakings”.
4.2
The actual deﬁnition
A proof is whatever convinces me.
Shimon Even (1935–2004)
Before deﬁning zero-knowledge proofs, we have to deﬁne proofs. The
standard notion of a static (i.e., non-interactive) proof will not do,
because static zero-knowledge proofs exist only for sets that are easy
to decide (i.e, are in BPP) (76), whereas we are interested in zero-
knowledge proofs for arbitrary NP-sets. Instead, we use the notion of
an interactive proof (introduced exactly for that reason in (81)). That
is, here a proof is a (multi-round) randomized protocol for two parties,
called veriﬁer and prover, in which the prover wishes to convince the
veriﬁer of the validity of a given assertion. Such an interactive proof
should allow the prover to convince the veriﬁer of the validity of any
true assertion (i.e., completeness), whereas no prover strategy may fool
the veriﬁer to accept false assertions (i.e., soundness). Both the com-
pleteness and soundness conditions should hold with high probability
(i.e., a negligible error probability is allowed). The prescribed veriﬁer
strategy is required to be eﬃcient. No such requirement is made with
respect to the prover strategy; yet we will be interested in “relatively
eﬃcient” prover strategies (see below).1
1 We stress that the relative eﬃciency of the prover strategy refers to the strategy employed
in order to prove valid assertions; that is, relative eﬃciency of the prover strategy is a
strengthening of the completeness condition (which is indeed required for cryptographic
applications). This should not be confused with the relaxation (i.e., weakening) of the
soundness condition that restricts its scope to feasible adversarial prover strategies (rather
than to all possible prover strategies). The resulting notion of “computational soundness”
is discussed in Section 4.4.1, and indeed suﬃces in most cryptographic applications. Still,
we believe that it is simpler to present the material in terms of interactive proofs (rather
than in terms of computationally sound proofs).

36
Zero-Knowledge
Zero-knowledge is a property of some prover strategies. More gen-
erally, we consider interactive machines that yield no knowledge while
interacting with an arbitrary feasible adversary on a common input
taken from a predetermined set (in our case the set of valid assertions).
A strategy A is zero-knowledge on (inputs from) the set S if, for every
feasible strategy B∗, there exists a feasible computation C∗such that
the following two probability ensembles are computationally indistin-
guishable2:
(1) {(A, B∗)(x)}x∈S
def
= the output of B∗after interacting with
A on common input x ∈S; and
(2) {C∗(x)}x∈S
def
= the output of C∗on input x ∈S.
We stress that the ﬁrst ensemble represents an actual execution of
an interactive protocol, whereas the second ensemble represents the
computation of a stand-alone procedure (called the “simulator”), which
does not interact with anybody.
The above deﬁnition does not account for auxiliary information that
an adversary B∗may have prior to entering the interaction. Account-
ing for such auxiliary information is essential for using zero-knowledge
proofs as subprotocols inside larger protocols (see (71; 76)). This is
taken care of by a stricter notion called auxiliary-input zero-knowledge.
Deﬁnition 4.1. (zero-knowledge (81), revisited (76)): A strategy A is
auxiliary-input zero-knowledge on inputs from S if, for every probabilistic
2 Here we refer to a natural extension of Deﬁnition 3.1: Rather than referring to ensembles
indexed by N, we refer to ensembles indexed by a set S ⊆{0, 1}∗. Typically, for an ensem-
ble {Zα}α∈S, it holds that Zα ranges over strings of length that is polynomially-related
to the length of α. We say that {Xα}α∈S and {Yα}α∈S are computationally indistinguish-
able if for every probabilistic polynomial-time algorithm D every polynomial p, and all
suﬃciently long α ∈S,
|Pr[D(α, Xα)=1] −Pr[D(α, Yα)=1]| <
1
p(|α|)
where the probabilities are taken over the relevant distribution (i.e., either Xα or Yα) and
over the internal coin tosses of algorithm D.

TEAM FLY
4.3. Zero-knowledge proofs for all NP-assertions and their applications
37
polynomial-time strategy B∗and every polynomial p, there exists a
probabilistic polynomial-time algorithm C∗such that the following two
probability ensembles are computationally indistinguishable:
(1) {(A, B∗(z))(x)}x∈S , z∈{0,1}p(|x|)
def
= the output of B∗when
having auxiliary-input z and interacting with A on common
input x ∈S; and
(2) {C∗(x, z)}x∈S , z∈{0,1}p(|x|)
def
= the output of C∗on inputs x ∈
S and z ∈{0, 1}p(|x|).
Almost all known zero-knowledge proofs are in fact auxiliary-input
zero-knowledge. As hinted above, auxiliary-input zero-knowledge is pre-
served under sequential composition (76). A simulator for the multiple-
session protocol can be constructed by iteratively invoking the single-
session simulator that refers to the residual strategy of the adversarial
veriﬁer in the given session (while feeding this simulator with the tran-
script of previous sessions). Indeed, the residual single-session veriﬁer
gets the transcript of the previous sessions as part of its auxiliary input
(i.e., z in Deﬁnition 4.1). (For details, see (65, Sec. 4.3.4).)
4.3
Zero-knowledge proofs for all NP-assertions and their
applications
A question avoided so far is whether zero-knowledge proofs exist at
all. Clearly, every set in P (or rather in BPP) has a “trivial” zero-
knowledge proof (in which the veriﬁer determines membership by
itself); however, what we seek is zero-knowledge proofs for statements
that the veriﬁer cannot decide by itself.
Assuming the existence of “commitment schemes” (see below),
which in turn exist if one-way functions exist (101; 85), there exist
(auxiliary-input) zero-knowledge proofs of membership in any NP-set
(i.e., sets having eﬃciently veriﬁable static proofs of membership).
These zero-knowledge proofs, ﬁrst constructed by Goldreich, Micali and
Wigderson (75) (and depicted in Figure 4.2), have the following impor-
tant property: the prescribed prover strategy is eﬃcient, provided it is

38
Zero-Knowledge
given as auxiliary-input an NP-witness to the assertion (to be proved).3
That is:
Theorem 4.2. ((75), using (85; 101)): If one-way functions exist then
every set S ∈NP has a zero-knowledge interactive proof. Furthermore,
the prescribed prover strategy can be implemented in probabilistic
polynomial-time, provided it is given as auxiliary-input an NP-witness
for membership of the common input in S.
Theorem 4.2 makes zero-knowledge a very powerful tool in the design of
cryptographic schemes and protocols (see below). We comment that the
intractability assumption used in Theorem 4.2 seems essential; see (105;
122).
Analyzing the protocol of Figure 4.2.
Let us consider a sin-
gle execution of the main loop (and rely on the preservation of
zero-knowledge under sequential composition). Clearly, the prescribed
prover is implemented in probabilistic polynomial-time, and always
convinces the veriﬁer (provided that it is given a valid 3-coloring of
the common input graph). In case the graph is not 3-colorable then, no
matter how the prover behaves, the veriﬁer will reject with probability
at least 1/|E| (because at least one of the edges must be improperly col-
ored by the prover). We stress that the veriﬁer selects uniformly which
edge to inspect after the prover has committed to the colors of all ver-
tices. Thus, Figure 4.2 depicts an interactive proof system for Graph
3-Colorability (with error probability exp(−t)). As the reader might
have guessed, the zero-knowledge property is the hardest to establish,
3 The auxiliary-input given to the prescribed prover (in order to allow for an eﬃcient imple-
mentation of its strategy) is not to be confused with the auxiliary-input that is given to
malicious veriﬁers (in the deﬁnition of auxiliary-input zero-knowledge). The former is typ-
ically an NP-witness for the common input, which is available to the user that invokes the
prover strategy (cf. the generic application discussed below). In contrast, the auxiliary-
input that is given to malicious veriﬁers models arbitrary partial information that may be
available to the adversary.

4.3. Zero-knowledge proofs for all NP-assertions and their applications
39
Commitment schemes are digital analogs of sealed envelopes (or, better, locked boxes).
Sending a commitment means sending a string that binds the sender to a unique value
without revealing this value to the receiver (as when getting a locked box). Decommitting
to the value means sending some auxiliary information that allows the receiver to read
the uniquely committed value (as when sending the key to the lock).
Common Input: A graph G(V, E). Suppose that V ≡{1, ..., n} for n
def
= |V |.
Auxiliary Input (to the prover): A 3-coloring φ : V →{1, 2, 3}.
The following 4 steps are repeated t · |E| many times so to obtain soundness error
exp(−t).
Prover’s ﬁrst step (P1): Select uniformly a permutation π over {1, 2, 3}. For i = 1
to n, send the veriﬁer a commitment to the value π(φ(i)).
Veriﬁer’s ﬁrst step (V1): Select uniformly an edge e ∈E and send it to the prover.
Prover’s second step (P2): Upon receiving e = (i, j) ∈E, decommit to the i-th and
j-th values sent in Step (P1).
Veriﬁer’s second step (V2): Check whether or not the decommitted values are dif-
ferent elements of {1, 2, 3} and whether or not they match the commitments
received in Step (P1).
Fig. 4.2 The zero-knowledge proof of Graph 3-Colorability (of (75)). Zero-knowledge proofs
for other NP-sets can be obtained using the standard reductions.
and we will conﬁne ourselves to presenting an adequate simulator. We
start with three simplifying conventions (which are useful in general):
(1) Without loss of generality, we may assume that the cheat-
ing veriﬁer strategy is implemented by a deterministic
polynomial-time algorithm with an auxiliary input. This is
justiﬁed by ﬁxing any outcome of the veriﬁer’s coins (as part
of the auxiliary input), and observing that our (uniform)
simulation of the various (residual) deterministic strategies
yields a simulation of the original probabilistic strategy.
(2) Without loss of generality, it suﬃces to consider cheating
veriﬁers that (only) output their view of the interaction (i.e.,
their input, coin tosses, and the messages they received). In
other words, it suﬃces to simulate the view of the cheat-
ing veriﬁer rather than its output (which is the result of a
polynomial-time post-processing of the view).
(3) Without loss of generality, it suﬃces to construct a “weak
simulator” that produces an output with some noticeable

40
Zero-Knowledge
probability, provided that (conditioned on producing out-
put) the output is computationally indistinguishable from
the desired distribution (i.e., the view of the cheating veriﬁer
in a real interaction). This is the case because, by repeatedly
invoking this weak simulator (polynomially) many times, we
may obtain a simulator that fails to produce an output with
negligible probability. Finally, letting the simulator produce
an arbitrary output rather than failing, we obtain a simulator
that never fails (as required by the deﬁnition), while skewing
the output distribution by at most a negligible amount.
Our simulator starts by selecting uniformly and independently a ran-
dom color (i.e., element of {1, 2, 3}) for each vertex, and feeding the
veriﬁer strategy with random commitments to these random colors.
Indeed, the simulator feeds the veriﬁer with a distribution that is very
diﬀerent from the distribution that the veriﬁer sees in a real interaction
with the prover. However, being computationally-restricted the veriﬁer
cannot tell these distributions apart (or else we obtain a contradiction
to the security of the commitment scheme in use). Now, if the veriﬁer
asks to inspect an edge that is properly colored then the simulator per-
forms the proper decommitment action and outputs the transcript of
this interaction. Otherwise, the simulator halts proclaiming failure. We
claim that failure occurs with probability approximately 1/3 (or else
we obtain a contradiction to the security of the commitment scheme
in use). Furthermore, based on the same hypothesis (but via a more
complex proof (cf. (65, Sec. 4.4.2.3))), conditioned on not failing, the
output of the simulator is computationally indistinguishable from the
veriﬁer’s view of the real interaction.
Commitment schemes.
Loosely speaking, commitment schemes
are two-stage (two-party) protocols allowing for one party to commit
itself (at the ﬁrst stage) to a value while keeping the value secret. In
a (second) latter stage, the commitment is “opened” and it is guaran-
teed that the “opening” can yield only a single value determined in the
committing phase. Thus, the (ﬁrst stage of the) commitment scheme
is both binding and hiding. A simple (uni-directional communication)

4.3. Zero-knowledge proofs for all NP-assertions and their applications
41
commitment scheme can be constructed based on any one-way 1-1 func-
tion f (with a corresponding hard-core b). To commit to a bit σ, the
sender uniformly selects s ∈{0, 1}n, and sends the pair (f(s), b(s)⊕σ).
Note that this is both binding and hiding. An alternative construction,
which can be based on any one-way function, uses a pseudorandom gen-
erator G that stretches its seed by a factor of three (cf. Theorem 3.4).
A commitment is established, via two-way communication, as follows
(cf. (101)): The receiver selects uniformly r ∈{0, 1}3n and sends it to
the sender, which selects uniformly s ∈{0, 1}n and sends r ⊕G(s) if
it wishes to commit to the value one and G(s) if it wishes to commit
to zero. To see that this is binding, observe that there are at most 22n
“bad” values r that satisfy G(s0) = r⊕G(s1) for some pair (s0, s1), and
with overwhelmingly high probability the receiver will not pick one of
these bad values. The hiding property follows by the pseudorandomness
of G.
Zero-knowledge proofs for other NP-sets.
By using the stan-
dard Karp-reductions to 3-Colorability, the protocol of Figure 4.2 can
be used for constructing zero-knowledge proofs for any set in NP. We
comment that this is probably the ﬁrst time that an NP-completeness
result was used in a “positive” way (i.e., in order to construct something
rather than in order to derive a hardness result).4
Eﬃciency considerations.
The protocol in Figure 4.2 calls for
invoking some constant-round protocol for a non-constant number of
times (and its analysis relies on the preservation of zero-knowledge
under sequential composition). At ﬁrst glance, it seems that one can
derive a constant-round zero-knowledge proof system (of negligible
soundness error) by performing these invocations in parallel (rather
than sequentially). Unfortunately, as indicated in (71), it is not clear
that the resulting interactive proof is zero-knowledge. Still, under stan-
dard intractability assumptions (e.g., the intractability of factoring),
constant-round zero-knowledge proofs (of negligible soundness error)
4 Subsequent positive uses of completeness results have appeared in the context of inter-
active proofs (96; 118), probabilistically checkable proofs (6; 55; 5; 4), “hardness versus
randomness trade-oﬀs” (7), and statistical zero-knowledge (115).

42
Zero-Knowledge
do exist for every set in NP (cf. (70)). We comment that the number
of rounds in a protocol is commonly considered the most important
eﬃciency criterion (or complexity measure), and typically one desires
to have it be a constant.
A generic application.
As mentioned above, Theorem 4.2 makes
zero-knowledge a very powerful tool in the design of cryptographic
schemes and protocols. This wide applicability is due to two important
aspects regarding Theorem 4.2: Firstly, Theorem 4.2 provides a zero-
knowledge proof for every NP-set, and secondly the prescribed prover
can be implemented in probabilistic polynomial-time when given an
adequate NP-witness. We now turn to a typical application of zero-
knowledge proofs. In a typical cryptographic setting, a user U has a
secret and is supposed to take some action depending on its secret. The
question is how can other users verify that U indeed took the correct
action (as determined by U’s secret and publicly known information).
Indeed, if U discloses its secret then anybody can verify that U took
the correct action. However, U does not want to reveal its secret. Using
zero-knowledge proofs we can satisfy both conﬂicting requirements (i.e.,
having other users verify that U took the correct action without vio-
lating U’s interest in not revealing its secret). That is, U can prove
in zero-knowledge that it took the correct action. Note that U’s claim
to having taken the correct action is an NP-assertion (since U’s legal
action is determined as a polynomial-time function of its secret and
the public information), and that U has an NP-witness to its valid-
ity (i.e., the secret is an NP-witness to the claim that the action ﬁts
the public information). Thus, by Theorem 4.2, it is possible for U
to eﬃciently prove the correctness of its action without yielding any-
thing about its secret. Consequently, it is fair to ask U to prove (in
zero-knowledge) that it behaves properly, and so to force U to behave
properly. Indeed, “forcing proper behavior” is the canonical application
of zero-knowledge proofs (see (74; 63)).
This paradigm (i.e., “forcing proper behavior” via zero-knowledge
proofs), which in turn is based on the fact that zero-knowledge proofs
can be constructed for any NP-set, has been utilized in numerous diﬀer-

4.4. Variants and issues
43
ent settings. Indeed, this paradigm is the basis for the wide applicability
of zero-knowledge protocols in cryptography.
Zero-knowledge proofs for all IP.
For the sake of elegancy, we
mention that under the same assumption used in the case of NP,
it holds that any set that has an interactive proof also has a zero-
knowledge interactive proof (cf. (88; 24)).
4.4
Variants and issues
In this section we consider numerous variants on the notion of zero-
knowledge and the underlying model of interactive proofs. These
include computational soundness (cf. Section 4.4.1), black-box simula-
tion and other variants of zero-knowledge (cf. Section 4.4.2), as well as
notions such as proofs of knowledge, non-interactive zero-knowledge,
and witness indistinguishable proofs (cf. Section 4.4.3). We conclude
this section by reviewing relatively recent results regarding the com-
position of zero-knowledge protocols and the power of non-black-box
simulation (cf. Section 4.4.4).
4.4.1
Computational soundness
A fundamental variant on the notion of interactive proofs was intro-
duced by Brassard, Chaum and Cr´epeau (33), who relaxed the sound-
ness condition so that it only refers to feasible ways of trying to fool
the veriﬁer (rather than to all possible ways). Speciﬁcally, the sound-
ness condition was replaced by a computational soundness condition
that asserts that it is infeasible to fool the veriﬁer into accepting false
statements. We warn that although the computational-soundness error
can always be reduced by sequential repetitions, it is not true that this
error can always be reduced by parallel repetitions (cf. (21)).
Protocols that satisfy the computational-soundness condition are
called arguments.5 We mention that argument systems may be more
eﬃcient than interactive proofs (see (90) vs. (69; 78)) as well as provide
stronger zero-knowledge guarantees (see (33) vs. (59; 2)). Speciﬁcally,
5 A related notion (not discussed here) is that of CS-proofs, introduced by Micali (99).

44
Zero-Knowledge
perfect zero-knowledge arguments for NP can be constructed based on
some reasonable assumptions (33), where perfect zero-knowledge means
that the simulator’s output is distributed identically to the veriﬁer’s
view in the real interaction (see discussion in Section 4.4.2). Note that
stronger security for the prover (as provided by perfect zero-knowledge)
comes at the cost of weaker security for the veriﬁer (as provided by
computational soundness). The answer to the question of whether or
not this trade-oﬀis worthwhile seems to be application dependent,
and one should also take into account the availability and complexity
of the corresponding protocols. (However, as stated in Footnote 1, we
believe that a presentation in terms of proofs should be preferred for
expositional purposes.)
4.4.2
Deﬁnitional variations
We consider several deﬁnitional issues regarding the notion of zero-
knowledge (as deﬁned in Deﬁnition 4.1).
Universal and black-box simulation.
Further strengthening of
Deﬁnition 4.1 is obtained by requiring the existence of a universal sim-
ulator, denoted C, that is given the program of the veriﬁer (i.e., B∗)
as an auxiliary-input; that is, in terms of Deﬁnition 4.1, one should
replace C∗(x, z) by C(x, z, ⟨B∗⟩), where ⟨B∗⟩denotes the description of
the program of B∗(which may depend on x and on z). That is, we eﬀec-
tively restrict the simulation by requiring that it be a uniform (feasible)
function of the veriﬁer’s program (rather than arbitrarily depend on it).
This restriction is very natural, because it seems hard to envision an
alternative way of establishing the zero-knowledge property of a given
protocol. Taking another step, one may argue that since it seems infeas-
ible to reverse-engineer programs, the simulator may as well just use
the veriﬁer strategy as an oracle (or as a “black-box”). This reasoning
gave rise to the notion of black-box simulation, which was introduced
and advocated in (71) and further studied in numerous works (see,
e.g., (40)). The belief was that inherent limitations regarding black-
box simulation represent inherent limitations of zero-knowledge itself.
For example, it was believed that the fact that the parallel version

4.4. Variants and issues
45
of the interactive proof of Figure 4.2 cannot be simulated in a black-
box manner (unless NP is contained in BPP (71)) implies that this
version is not zero-knowledge (as per Deﬁnition 4.1 itself). However,
the (underlying) belief that any zero-knowledge protocol can be sim-
ulated in a black-box manner was refuted recently by Barak (8). For
further discussion, see Section 4.4.4.
Honest veriﬁer versus general cheating veriﬁer.
Deﬁnition 4.1
refers to all feasible veriﬁer strategies, which is most natural (in the
cryptographic setting) because zero-knowledge is supposed to cap-
ture the robustness of the prover under any feasible (i.e., adversarial)
attempt to gain something by interacting with it. A weaker and still
interesting notion of zero-knowledge refers to what can be gained by
an “honest veriﬁer” (or rather a semi-honest veriﬁer)6 that interacts
with the prover as directed, with the exception that it may maintain
(and output) a record of the entire interaction (i.e., even if directed
to erase all records of the interaction). Although such a weaker notion
is not satisfactory for standard cryptographic applications, it yields a
fascinating notion from a conceptual as well as a complexity-theoretic
point of view. Furthermore, as shown in (77; 122), every proof system
that is zero-knowledge with respect to the honest-veriﬁer can be trans-
formed into a standard zero-knowledge proof (without using intractabil-
ity assumptions and in the case of “public-coin” proofs this is done
without signiﬁcantly increasing the prover’s computational eﬀort).
Statistical versus Computational Zero-Knowledge.
Recall that
Deﬁnition 4.1 postulates that for every probability ensemble of one type
(i.e., representing the veriﬁer’s output after interaction with the prover)
there exists a “similar” ensemble of a second type (i.e., representing
the simulator’s output). One key parameter is the interpretation of
“similarity”. Three interpretations, yielding diﬀerent notions of zero-
6 The term “honest veriﬁer” is more appealing when considering an alternative (equivalent)
formulation of Deﬁnition 4.1. In the alternative deﬁnition (see (65, Sec. 4.3.1.3)), the
simulator is “only” required to generate the veriﬁer’s view of the real interaction, where
the veriﬁer’s view includes its (common and auxiliary) inputs, the outcome of its coin
tosses, and all messages it has received.

46
Zero-Knowledge
knowledge, have been commonly considered in the literature (cf., (81;
59)):
(1) Perfect Zero-Knowledge requires that the two probability
ensembles be identical.7
(2) Statistical Zero-Knowledge requires that these probability
ensembles be statistically close (i.e., the variation distance
between them is negligible).
(3) Computational (or rather general) Zero-Knowledge requires
that these probability ensembles be computationally indis-
tinguishable.
Indeed, Computational Zero-Knowledge is the most liberal notion, and
is the notion considered in Deﬁnition 4.1. We note that the class of prob-
lems having statistical zero-knowledge proofs contains several prob-
lems that are considered intractable. The interested reader is referred
to (121).
Strict versus expected probabilistic polynomial-time.
So far,
we did not specify what we exactly mean by the term probabilistic
polynomial-time. Two common interpretations are:
(1) Strict probabilistic polynomial-time. That is, there exist a
(polynomial in the length of the input) bound on the number
of steps in each possible run of the machine, regardless of the
outcome of its coin tosses.
(2) Expected
probabilistic
polynomial-time.
The
standard
approach is to look at the running-time as a random variable
and bound its expectation (by a polynomial in the length
of the input). As observed by Levin (cf. (65, Sec. 4.3.1.6)
and (12)), this deﬁnitional approach is quite problematic
(e.g., it is not model-independent and is not closed under
algorithmic composition), and an alternative treatment of
this random variable is preferable.
7 The actual deﬁnition of Perfect Zero-Knowledge allows the simulator to fail (while out-
putting a special symbol) with negligible probability, and the output distribution of the
simulator is conditioned on its not failing.

4.4. Variants and issues
47
Consequently, the notion of expected polynomial-time raises a variety
of conceptual and technical problems. For that reason, whenever possi-
ble, one should prefer the more robust (and restricted) notion of strict
(probabilistic) polynomial-time. Thus, with the exception of constant-
round zero-knowledge protocols, whenever we talked of a probabilistic
polynomial-time veriﬁer (resp., simulator) we mean one in the strict
sense. In contrast, with the exception of (8; 12), all results regarding
constant-round zero-knowledge protocols refer to a strict polynomial-
time veriﬁer and an expected polynomial-time simulator, which is
indeed a small cheat. For further discussion, the reader is referred
to (12).
4.4.3
Related notions: POK, NIZK, and WI
We brieﬂy discuss the notions of proofs of knowledge (POK), non-
interactive zero-knowledge (NIZK), and witness indistinguishable
proofs (WI).
Proofs
of
Knowledge.
Loosely speaking, proofs of knowledge
(cf. (81)) are interactive proofs in which the prover asserts “knowl-
edge” of some object (e.g., a 3-coloring of a graph), and not merely its
existence (e.g., the existence of a 3-coloring of the graph, which in turn
is equivalent to the assertion that the graph is 3-colorable). Before clar-
ifying what we mean by saying that a machine knows something, we
point out that “proofs of knowledge”, and in particular zero-knowledge
“proofs of knowledge”, have many applications to the design of crypto-
graphic schemes and cryptographic protocols. One famous application
of zero-knowledge proofs of knowledge is to the construction of identi-
ﬁcation schemes (e.g., the Fiat–Shamir scheme (58)).
What does it mean to say that a machine knows something? Any
standard dictionary suggests several meanings for the verb to know,
which are typically phrased with reference to awareness, a notion which
is certainly inapplicable in the context of machines. We must look for a
behavioristic interpretation of the verb to know. Indeed, it is reasonable
to link knowledge with ability to do something (e.g., the ability to write
down whatever one knows). Hence, we will say that a machine knows

48
Zero-Knowledge
a string α if it can output the string α. But this seems total non-sense
too: a machine has a well-deﬁned output – either the output equals α
or it does not. So what can be meant by saying that a machine can
do something? Loosely speaking, it may mean that the machine can
be easily modiﬁed so that it does whatever is claimed. More precisely,
it may mean that there exists an eﬃcient machine that, using the
original machine as a black-box (or given its code as an input), outputs
whatever is claimed.
So much for deﬁning the “knowledge of machines”. Yet, whatever
a machine knows or does not know is “its own business”. What can
be of interest and reference to the outside is whatever can be deduced
about the knowledge of a machine by interacting with it. Hence, we
are interested in proofs of knowledge (rather than in mere knowledge).
For sake of simplicity let us consider a concrete question: how can a
machine prove that it knows a 3-coloring of a graph? An obvious way is
just to send the 3-coloring to the veriﬁer. Yet, we claim that applying
the protocol in Figure 4.2 (i.e., the zero-knowledge proof system for 3-
Colorability) is an alternative way of proving knowledge of a 3-coloring
of the graph.
The deﬁnition of a veriﬁer of knowledge of 3-coloring refers to any
possible prover strategy. It requires the existence of an eﬃcient uni-
versal way of “extracting” a 3-coloring of a given graph by using any
prover strategy that convinces the verify to accept the graph (with
noticeable probability). Surely, we should not expect much of prover
strategies that convince the veriﬁer to accept the graph with negligible
probability. However, a robust deﬁnition should allow a smooth passage
from noticeable to negligible, and should allow to establish the intuitive
zero-knowledge property of a party that sends some information that
the other party has proved it knows.
Loosely speaking, we may say that an interactive machine, V , con-
stitutes a veriﬁer for knowledge of 3-coloring if, for any prover strategy
P, the complexity of extracting a 3-coloring of G when using machine
P as a “black box”8 is inversely proportional to the probability that
the veriﬁer is convinced by P (to accept the graph G). Namely, the
8 Indeed, one may also consider non-black-box extractors as done in (12).

4.4. Variants and issues
49
extraction of the 3-coloring is done by an oracle machine, called an
extractor, that is given access to a function specifying the behavior P
(i.e., the messages it sends in response to particular messages it may
receive). We require that the (expected) running time of the extrac-
tor, on input G and access to an oracle specifying P’s messages, be
inversely related (by a factor polynomial in |G|) to the probability
that P convinces V to accept G. In the case that P always convinces
V to accept G, the extractor runs in expected polynomial-time. The
same holds in case P convinces V to accept with noticeable probability.
On the other hand, in case P never convinces V to accept, essentially
nothing is required of the extractor. (We stress that the latter special
cases do not suﬃce for a satisfactory deﬁnition; see discussion in (65,
Sec. 4.7.1).)
Non-Interactive Zero-Knowledge.
The model of non-interactive
zero-knowledge proof systems, introduced in (29), consists of three enti-
ties: a prover, a veriﬁer and a uniformly selected reference string (which
can be thought of as being selected by a trusted third party). Both
the veriﬁer and prover can read the reference string, and each can
toss additional coins. The interaction consists of a single message sent
from the prover to the veriﬁer, who then is left with the ﬁnal decision
(whether to accept or not). The (basic) zero-knowledge requirement
refers to a simulator that outputs pairs that should be computation-
ally indistinguishable from the distribution (of pairs consisting of a
uniformly selected reference string and a random prover message) seen
in the real model.9 Non-interactive zero-knowledge proof systems have
numerous applications (e.g., to the construction of public-key encryp-
tion and signature schemes, where the reference string may be incorpo-
rated in the public-key). Several diﬀerent deﬁnitions of non-interactive
zero-knowledge proofs were considered in the literature.
9 Note that the veriﬁer does not eﬀect the distribution seen in the real model, and so the
basic deﬁnition of zero-knowledge does not refer to it. The veriﬁer (or rather a process of
adaptively selecting assertions to be proved) will be referred to in the adaptive variants of
the deﬁnition.

50
Zero-Knowledge
• In the basic deﬁnition, one considers proving a single asser-
tion of a-priori bounded length, where this length may be
smaller than the length of the reference string.
• A natural extension, required in many applications, is the
ability to prove multiple assertions of varying length, where
the total length of these assertions may exceed the length of
the reference string (as long as the total length is polyno-
mial in the length of the reference string). This deﬁnition is
sometimes referred to as the unbounded deﬁnition, because
the total length of the assertions to be proved is not a-priori
bounded.
• Other natural extensions refer to the preservation of secu-
rity (i.e., both soundness and zero-knowledge) when the
assertions to be proved are selected adaptively (based on
the reference string and possibly even based on previous
proofs).
• Finally, we mention the notion of simulation-soundness,
which is related to non-malleability. This extension, which
mixes the zero-knowledge and soundness conditions, refers
to the soundness of proofs presented by an adversary after
it obtains proofs of assertions of its own choice (with respect
to the same reference string). This notion is important in
applications of non-interactive zero-knowledge proofs to the
construction of public-key encryption schemes secure against
chosen ciphertext attacks (see (67, Sec. 5.4.4.4)).
Constructing non-interactive zero-knowledge proofs seems more diﬃ-
cult than constructing interactive zero-knowledge proofs. Still, based
on standard intractability assumptions (e.g., intractability of factor-
ing), it is known how to construct a non-interactive zero-knowledge
proof (even in the adaptive and non-malleable sense) for any NP-set
(cf. (56; 116)).
Witness Indistinguishability and the FLS-Technique.
The
notion of witness indistinguishability was suggested in (57) as a
meaningful relaxation of zero-knowledge. Loosely speaking, for any

4.4. Variants and issues
51
NP-relation R, a proof (or argument) system for the corresponding
NP-set is called witness indistinguishable if no feasible veriﬁer may dis-
tinguish the case in which the prover uses one NP-witness to x (i.e., w1
such that (x, w1) ∈R) from the case in which the prover is using a dif-
ferent NP-witness to the same input x (i.e., w2 such that (x, w2) ∈R).
Clearly, any zero-knowledge protocol is witness indistinguishable, but
the converse does not necessarily hold. Furthermore, it seems that
witness indistinguishable protocols are easier to construct than zero-
knowledge ones. Another advantage of witness indistinguishable proto-
cols is that they are closed under arbitrary concurrent composition (57),
whereas in general zero-knowledge protocols are not closed even under
parallel composition (71). Witness indistinguishable protocols turned
out to be an important tool in the construction of more complex proto-
cols, as is demonstrated next.
Feige, Lapidot and Shamir (56) introduced a technique for con-
structing zero-knowledge proofs (and arguments) based on witness
indistinguishable proofs (resp., arguments). Following is a sketchy
description of a special case of their technique, often referred to as
the FLS-technique, which has been used in numerous works. On com-
mon input x ∈L, where L is the NP-set deﬁned by the witness relation
R, the following two steps are performed:
(1) The parties generate an instance x′ for an auxiliary NP-
set L′, where L′ is deﬁned by a witness relation R′. The
generation protocol in use must satisfy the following two
conditions:
(a) If the veriﬁer follows its prescribed strategy then no
matter which strategy is used by the prover, with high
probability, the protocol’s outcome is a no-instance
of L′.
(b) Loosely speaking, there exists an eﬃcient (non-
interactive) procedure for producing a (random)
transcript of the generation protocol such that the
corresponding protocol’s outcome is a yes-instance of
L′ and yet the produced transcript is computationally
indistinguishable from the transcript of a real exe-

52
Zero-Knowledge
cution of the protocol. Furthermore, this procedure
also outputs an NP-witness for the yes-instance that
appears as the protocol’s outcome.
For example, L′ may consist of all possible outcomes of a
pseudorandom generator that stretches its seed by a factor of
two, and the generation protocol may consist of the two par-
ties iteratively invoking a “coin tossing” protocol to obtain
a random string. Note that the outcome of a real execution
will be an almost uniformly distributed string, which is most
likely a no-instance of L′, whereas it is easy to generate a
(random) transcript corresponding to any desired outcome
(provided that the parties use an adequate coin tossing
protocol).
(2) The parties execute a witness indistinguishable proof for
the NP-set L′′ deﬁned by the witness relation R′′
=
{((α, α′), (β, β′)) : (α, β)∈R∨(α′, β′)∈R′}. The sub-protocol
is such that the corresponding prover can be implemented
in probabilistic polynomial-time given any NP-witness for
(α, α′) ∈L′′. The sub-protocol is invoked on common input
(x, x′), where x′ is the outcome of Step 1, and the sub-prover
is invoked with the corresponding NP-witness as auxiliary
input (i.e., with (w, 0), where w is the NP-witness for x (given
to the main prover)).
The soundness of the above protocol follows by Property (a) of the
generation protocol (i.e., with high probability x′ ̸∈L′, and so x ∈L
follows by the soundness of the protocol used in Step 2). To demonstrate
the zero-knowledge property, we ﬁrst generate a simulated transcript of
Step 1 (with outcome x′ ∈L′) along with an adequate NP-witness (i.e.,
w′ such that (x′, w′) ∈R′), and then emulate Step 2 by feeding the sub-
prover strategy with the NP-witness (0, w′). Combining Property (b) of
the generation protocol and the witness indistinguishability property
of the protocol used in Step 2, the simulation is indistinguishable from
the real execution.

4.4. Variants and issues
53
4.4.4
Two basic problems: composition and black-box
simulation
We conclude this section by considering two basic problems regarding
zero-knowledge, which actually arise also with respect to the security
of other cryptographic primitives.
Composition of protocols.
The ﬁrst question refers to the preser-
vation of security (i.e., zero-knowledge in our case) under various
types of composition operations. These composition operations rep-
resent independent executions of a protocol that are attacked by an
adversary (which coordinates its actions in the various executions). The
preservation of security under such compositions (which involve only
executions of the same protocol) is a ﬁrst step towards the study of the
security of the protocol when executed together with other protocols
(see further discussion in Section 7.4). Turning back to zero-knowledge,
we recall the main facts regarding sequential, parallel and concurrent
execution of (arbitrary and/or speciﬁc) zero-knowledge protocols:
Sequential composition: As stated above, zero-knowledge (with
respect to auxiliary inputs) is closed under sequential composi-
tion.
Parallel composition: In general, zero-knowledge is not closed under
parallel composition (71). Yet, some zero-knowledge proofs (for
NP) preserve their security when many copies are executed in
parallel. Furthermore, some of these protocol use a constant
number of rounds (cf. (66)).
Concurrent composition: One may view parallel composition as
concurrent composition in a model of strict synchronity. This
leads us to consider more general models of concurrent compo-
sition. We distinguish between a model of full asynchronicity
and a model naturally limited asynchronicity.
• In the full asynchronous model, some zero-knowledge
proofs (for NP) preserve their security when many
copies are executed concurrently (cf. (111; 91; 107)), but
such a result is not known for constant-round protocols.

54
Zero-Knowledge
• In contrast, constant-round zero-knowledge proofs (for
NP) are known (cf. (53; 66)) in a model of limited asyn-
chronicity, where each party holds a local clock such
that the relative clock rates are bounded by an a-priori
known constant and the protocols may employ time-
driven operations (i.e., time-out incoming messages and
delay out-going messages).
The study of zero-knowledge in the concurrent setting provides a good
test case for the study of concurrent security of general protocols. In
particular, the results in (71; 40) point out inherent limitations of the
“standard proof methods” (used to establish zero-knowledge) when
applied to the concurrent setting, where (71) treats the synchronous
case and (40) uncovers much stronger limitations for the asynchronous
case. By “standard proof methods” we refer to the establishment of
zero-knowledge via a single simulator that obtains only oracle (or
“black-box”) access to the adversary procedure.
Black-box proofs of security.
The second basic question regarding
zero-knowledge refers to the usage of the adversary’s program within
the proof of security (i.e., demonstration of the zero-knowledge prop-
erty). For 15 years, all known proofs of security used the adversary’s
program as a black-box (i.e., a universal simulator was presented using
the adversary’s program as an oracle). Furthermore, it was believed
that there was no advantage in having access to the code of the adver-
sary’s program (cf. (71)). Consequently it was conjectured that negative
results regarding black-box simulation represent an inherent limitation
of zero-knowledge. This belief has been refuted recently by Barak (8)
who constructed a zero-knowledge argument (for NP) that has impor-
tant properties that are impossible to achieve by black-box simulation
(unless NP ⊆BPP). For example, this zero-knowledge argument uses
a constant number of rounds and preserves its security when an a-priori
ﬁxed (polynomial) number of copies are executed concurrently.10
10 This result falls short of achieving a fully concurrent zero-knowledge argument, because
the number of concurrent copies must be ﬁxed before the protocol is presented. Speciﬁ-
cally, the protocol uses messages that are longer than the allowed number of concurrent

4.4. Variants and issues
55
Barak’s results (cf. (8) and also (9)) call for the re-evaluation of
many common beliefs. Most concretely, they say that results regard-
ing black-box simulators do not reﬂect inherent limitations of zero-
knowledge (but rather an inherent limitation of a natural way of demon-
strating the zero-knowledge property). Most abstractly, they say that
there are meaningful ways of using a program other than merely invok-
ing it as a black-box. Does this mean that a method was found to
“reverse engineer” programs or to “understand” them? We believe that
the answer is negative. Barak (8) is using the adversary’s program in
a signiﬁcant way (i.e., more signiﬁcant than just invoking it), without
“understanding” it.
The key idea underlying Barak’s protocol (8) is to have the prover
prove that either the original NP-assertion is valid or that he (i.e.,
the prover) “knows the veriﬁer’s residual strategy” (in the sense that
it can predict the next veriﬁer message). Indeed, in a real interaction
(with the honest veriﬁer), it is infeasible for the prover to predict the
next veriﬁer message, and so computational-soundness of the protocol
follows. However, a simulator that is given the code of the veriﬁer’s
strategy (and not merely oracle access to that code), can produce a valid
proof of the disjunction by properly executing the sub-protocol using its
knowledge of an NP-witness for the second disjunctive. The simulation
is computationally indistinguishable from the real execution, provided
that one cannot distinguish an execution of the sub-protocol in which
one NP-witness (i.e., an NP-witness for the original assertion) is used
from an execution in which the second NP-witness (i.e., an NP-witness
for the auxiliary assertion) is used. That is, the sub-protocol should be a
witness indistinguishable argument system, and the entire construction
uses the FLS technique (described in Section 4.4.3). We warn the reader
that the actual implementation of the above idea requires overcoming
several technical diﬃculties (cf. (8; 11)).
copies. However, even preservation of security under an a priori bounded number of
executions goes beyond the impossibility results of (71; 40) (which refers to black-box
simulations).

Part II
Basic Applications
56

57
Encryption and signature schemes are the most basic applications of
cryptography. Their main utility is in providing secret and reliable
communication over insecure communication media. Loosely speaking,
encryption schemes are used to ensure the secrecy (or privacy) of the
actual information being communicated, whereas signature schemes are
used to ensure its reliability (or authenticity). In this part we survey
these basic applications as well as the construction of general secure
cryptographic protocols. For more details regarding the contents of the
current part, see our recent textbook (67).


5
Encryption Schemes
The problem of providing secret communication over insecure media is
the traditional and most basic problem of cryptography. The setting of
this problem consists of two parties communicating through a channel
that is possibly tapped by an adversary. The parties wish to exchange
information with each other, but keep the “wire-tapper” as ignorant
as possible regarding the contents of this information. The canonical
solution to the above problem is obtained by the use of encryption
schemes. Loosely speaking, an encryption scheme is a protocol allow-
ing these parties to communicate secretly with each other. Typically,
the encryption scheme consists of a pair of algorithms. One algorithm,
called encryption, is applied by the sender (i.e., the party sending a mes-
sage), while the other algorithm, called decryption, is applied by the
receiver. Hence, in order to send a message, the sender ﬁrst applies
the encryption algorithm to the message, and sends the result, called
the ciphertext, over the channel. Upon receiving a ciphertext, the other
party (i.e., the receiver) applies the decryption algorithm to it, and
retrieves the original message (called the plaintext).
In order for the above scheme to provide secret communication, the
communicating parties (at least the receiver) must know something
59

60
Encryption Schemes
that is not known to the wire-tapper. (Otherwise, the wire-tapper can
decrypt the ciphertext exactly as done by the receiver.) This extra
knowledge may take the form of the decryption algorithm itself, or some
parameters and/or auxiliary inputs used by the decryption algorithm.
We call this extra knowledge the decryption-key. Note that, without loss
of generality, we may assume that the decryption algorithm is known
to the wire-tapper, and that the decryption algorithm operates on two
inputs: a ciphertext and a decryption-key. We stress that the existence
of a decryption-key, not known to the wire-tapper, is merely a necessary
condition for secret communication. The above description implicitly
presupposes the existence of an eﬃcient algorithm for generating (ran-
dom) keys.
Evaluating the “security” of an encryption scheme is a very tricky
business. A preliminary task is to understand what is “security” (i.e., to
properly deﬁne what is meant by this intuitive term). Two approaches
to deﬁning security are known. The ﬁrst (“classical”) approach, intro-
duced by Shannon (119), is information theoretic. It is concerned with
the “information” about the plaintext that is “present” in the cipher-
text . Loosely speaking, if the ciphertext contains information about
the plaintext then the encryption scheme is considered insecure. It has
been shown that such high (i.e., “perfect”) level of security can be
achieved only if the key in use is at least as long as the total amount
of information sent via the encryption scheme (119). This fact (i.e.,
that the key has to be longer than the information exchanged using
it) is indeed a drastic limitation on the applicability of such (perfectly-
secure) encryption schemes.
The second (“modern”) approach, followed in the current text, is
based on computational complexity. This approach is based on the the-
sis that it does not matter whether the ciphertext contains information
about the plaintext, but rather whether this information can be eﬃ-
ciently extracted. In other words, instead of asking whether it is possible
for the wire-tapper to extract speciﬁc information, we ask whether it is
feasible for the wire-tapper to extract this information. It turns out that
the new (i.e., “computational complexity”) approach can oﬀer security
even when the key is much shorter than the total length of the messages
sent via the encryption scheme.

61
D
X
plaintext
Receiver’s  protected  region
Sender’s  protected  region
X
E
plaintext
ADVERSARY
ciphertext
e
e
e
d
The key-pair (e, d) is generated by the receiver, who posts the
encryption-key e on a public media, while keeping the decryption-
key d secret.
Fig. 5.1 Public-key encryption schemes – an illustration.
The computational complexity approach enables the introduction of
concepts and primitives that cannot exist under the information theo-
retic approach. A typical example is the concept of public-key encryp-
tion schemes, introduced by Diﬃe and Hellman (49). Recall that in the
above discussion we concentrated on the decryption algorithm and its
key. It can be shown that the encryption algorithm must get, in addition
to the message, an auxiliary input that depends on the decryption-key.
This auxiliary input is called the encryption-key. Traditional encryp-
tion schemes, and in particular all the encryption schemes used in the
millennia until the 1980s, operate with an encryption-key that equals
the decryption-key. Hence, the wire-tapper in these schemes must be
ignorant of the encryption-key, and consequently the key distribution
problem arises; that is, how can two parties wishing to communicate
over an insecure channel agree on a secret encryption/decryption key.
(The traditional solution is to exchange the key through an alterna-
tive channel that is secure though (much) more expensive to use.)
The computational complexity approach allows the introduction of
encryption schemes in which the encryption-key may be given to the
wire-tapper without compromising the security of the scheme. Clearly,
the decryption-key in such schemes is diﬀerent from the encryption-

62
Encryption Schemes
key, and furthermore infeasible to compute from the encryption-key.
Such encryption schemes, called public-key schemes, have the advan-
tage of trivially resolving the key distribution problem (because the
encryption-key can be publicized). That is, once some Party X gener-
ates a pair of keys and publicizes the encryption-key, any party can
send encrypted messages to Party X so that Party X can retrieve the
actual information (i.e., the plaintext), whereas nobody else can learn
anything about the plaintext.
D
K
X
plaintext
Receiver’s  protected  region
Sender’s  protected  region
X
E
K
plaintext
ADVERSARY
ciphertext
The key K is known to both receiver and sender, but is unknown
to the adversary. For example, the receiver may generate K at
random and pass it to the sender via a perfectly-private sec-
ondary channel (not shown here).
Fig. 5.2 Private-key encryption schemes – an illustration.
In contrast to public-key schemes, traditional encryption schemes in
which the encryption-key equals the description-key are called private-
key schemes, because in these schemes the encryption-key must be kept
secret (rather than be public as in public-key encryption schemes). We
note that a full speciﬁcation of either schemes requires the speciﬁca-
tion of the way in which keys are generated; that is, a (randomized)
key-generation algorithm that, given a security parameter, produces a
(random) pair of corresponding encryption/decryption keys (which are
identical in the case of private-key schemes).
Thus, both private-key and public-key encryption schemes consist
of three eﬃcient algorithms: a key generation algorithm denoted G, an

5.1. Deﬁnitions
63
encryption algorithm denoted E, and a decryption algorithm denoted
D. For every pair of encryption and decryption keys (e, d) generated
by G, and for every plaintext x, it holds that Dd(Ee(x)) = x, where
Ee(x) def
= E(e, x) and Dd(y) def
= D(d, y). The diﬀerence between the two
types of encryption schemes is reﬂected in the deﬁnition of security: the
security of a public-key encryption scheme should hold also when the
adversary is given the encryption-key, whereas this is not required for a
private-key encryption scheme. Below we focus on the public-key case
(and the private-key case can be obtained by omitting the encryption-
key from the sequence of inputs given to the adversary).
5.1
Deﬁnitions
A good disguise should not reveal the person’s height.
ShaﬁGoldwasser and Silvio Micali, 1982
For simplicity, we ﬁrst consider the encryption of a single message
(which, for further simplicity, is assumed to be of length n).1 As implied
by the above discussion, a public-key encryption scheme is said to be
secure if it is infeasible to gain any information about the plaintext by
looking at the ciphertext (and the encryption-key). That is, whatever
information about the plaintext one may compute from the cipher-
text and some a-priori information, can be essentially computed as
eﬃciently from the a-priori information alone. This fundamental deﬁ-
nition of security (called semantic security) turns out to be equivalent
to saying that, for any two messages, it is infeasible to distinguish
the encryption of the ﬁrst message from the encryption of the second
message, even when given the encryption-key. Both deﬁnitions were
introduced by Goldwasser and Micali (80).
Deﬁnition 5.1. (semantic security (following (80), revisited (64))):
A public-key encryption scheme (G, E, D) is semantically secure if for
1 In the case of public-key schemes no generality is lost by these simplifying assumptions,
but in the case of private-key schemes one should consider the encryption of polynomially-
many messages (as we do below).

64
Encryption Schemes
every probabilistic polynomial-time algorithm, A, there exists a prob-
abilistic polynomial-time algorithm B so that for every two functions
f, h : {0, 1}∗→{0, 1}∗such that |h(x)| = poly(|x|), and all probabil-
ity ensembles {Xn}n∈N, where Xn is a random variable ranging over
{0, 1}n, it holds that
Pr[A(e, Ee(x), h(x))=f(x)] < Pr[B(1n, h(x))=f(x)] + µ(n).
where the plaintext x is distributed according to Xn, the encryption-
key e is distributed according to G(1n), and µ is a negligible function.
That is, it is feasible to predict f(x) from h(x) as successfully as it is
to predict f(x) from h(x) and (e, Ee(x)), which means that nothing
is gained by obtaining (e, Ee(x)). Note that no computational restric-
tions are made regarding the functions h and f. We stress that the
above deﬁnition (as well as the next one) refers to public-key encryp-
tion schemes, and in the case of private-key schemes algorithm A is not
given the encryption-key e.
A good disguise should not allow a mother
to distinguish her own children.
ShaﬁGoldwasser and Silvio Micali, 1982
The following technical interpretation of security states that it is infeas-
ible to distinguish the encryptions of two plaintext s (of the same
length).
Deﬁnition 5.2. (indistinguishability of encryptions (following (80))):
A public-key encryption scheme (G, E, D) has indistinguishable encryp-
tions if for every probabilistic polynomial-time algorithm, A, and all
sequences of triples, (xn, yn, zn)n∈N, where |xn| = |yn| = n and
|zn| = poly(n),
|Pr[A(e, Ee(xn), zn)=1] −Pr[A(e, Ee(yn), zn)=1]| = µ(n).
Again, e is distributed according to G(1n), and µ is a negligible func-
tion.

5.1. Deﬁnitions
65
In particular, zn may equal (xn, yn). Thus, it is infeasible to distinguish
the encryptions of any two ﬁxed messages (such as the all-zero message
and the all-ones message).
Deﬁnition 5.1 is more appealing in most settings where encryption is
considered the end goal. Deﬁnition 5.2 is used to establish the security
of candidate encryption schemes as well as to analyze their application
as modules inside larger cryptographic protocols. Thus, their equiva-
lence is of major importance.
Equivalence of Deﬁnitions 5.1 and 5.2 – proof ideas.
Intu-
itively, indistinguishability of encryptions (i.e., of the encryptions of
xn and yn) is a special case of semantic security; speciﬁcally, it
corresponds to the case that Xn is uniform over {xn, yn}, f indi-
cates one of the plaintext s and h does not distinguish them (i.e.,
f(w) = 1 iﬀw = xn and h(xn) = h(yn) = zn, where zn is
as in Deﬁnition 5.2). The other direction is proved by considering
the algorithm B that, on input (1n, v) where v = h(x), generates
(e, d) ←G(1n) and outputs A(e, Ee(1n), v), where A is as in Deﬁni-
tion 5.1. Indistinguishability of encryptions is used to prove that B
performs as well as A (i.e., for every h, f and {Xn}n∈N, it holds that
Pr[B(1n, h(Xn)) = f(Xn)] = Pr[A(e, Ee(1n), h(Xn)) = f(Xn)] approxi-
mately equals Pr[A(e, Ee(Xn), h(Xn))=f(Xn)]).
Probabilistic encryption:
It is easy to see that a secure public-
key encryption scheme must employ a probabilistic (i.e., randomized)
encryption algorithm. Otherwise, given the encryption-key as (addi-
tional) input, it is easy to distinguish the encryption of the all-zero
message from the encryption of the all-ones message.2 This explains
the association of the aforementioned robust security deﬁnitions and
probabilistic encryption, an association that goes back to the title of
the pioneering work of Goldwasser and Micali (80).
2 The same holds for (stateless) private-key encryption schemes, when considering the secu-
rity of encrypting several messages (rather than a single message as done above). For
example, if one uses a deterministic encryption algorithm then the adversary can distin-
guish two encryptions of the same message from the encryptions of a pair of diﬀerent
messages.

66
Encryption Schemes
Further discussion:
We stress that (the equivalent) Deﬁnitions 5.1
and 5.2 go way beyond saying that it is infeasible to recover the plain-
text from the ciphertext . The latter statement is indeed a minimal
requirement from a secure encryption scheme, but is far from being
a suﬃcient requirement. Typically, encryption schemes are used in
applications where even obtaining partial information on the plain-
text may endanger the security of the application. When designing
an application-independent encryption scheme, we do not know which
partial information endangers the application and which does not. Fur-
thermore, even if one wants to design an encryption scheme tailored to
a speciﬁc application, it is rare (to say the least) that one has a precise
characterization of all possible partial information that endanger this
application. Thus, we need to require that it is infeasible to obtain any
information about the plaintext from the ciphertext . Furthermore, in
most applications the plaintext may not be uniformly distributed and
some a-priori information regarding it is available to the adversary. We
require that the secrecy of all partial information is preserved also in
such a case. That is, even in presence of a-priori information on the
plaintext , it is infeasible to obtain any (new) information about the
plaintext from the ciphertext (beyond what is feasible to obtain from
the a-priori information on the plaintext ). The deﬁnition of semantic
security postulates all of this. The equivalent deﬁnition of indistin-
guishability of encryptions is useful in demonstrating the security of
candidate constructions as well as for arguing about their eﬀect as part
of larger protocols.
Security of multiple messages:
Deﬁnitions 5.1 and 5.2 refer to the
security of an encryption scheme that is used to encrypt a single plain-
text (per generated key). Since the plaintext may be longer than the
key,3 these deﬁnitions are already non-trivial, and an encryption scheme
satisfying them (even in the private-key model) implies the existence
of one-way functions. Still, in many cases, it is desirable to encrypt
3 Recall that for sake of simplicity we have considered only messages of length n, but the
general deﬁnitions refer to messages of arbitrary (polynomial in n) length. We comment
that, in the general form of Deﬁnition 5.1, one should provide the length of the message
as an auxiliary input to both algorithms (A and B).

5.2. Constructions
67
many plaintext s using the same encryption-key. Loosely speaking, an
encryption scheme is secure in the multiple-messages setting if analo-
gous deﬁnitions (to Deﬁnitions 5.1 and 5.2) hold when polynomially-
many plaintext s are encrypted using the same encryption-key (cf. (67,
Sec. 5.2.4)). It is easy to see that in the public-key model, security in the
single-message setting implies security in the multiple-messages setting.
We stress that this is not necessarily true for the private-key model.
5.2
Constructions
It is common practice to use “pseudorandom generators” as a basis for
private-key encryption schemes. We stress that this is a very dangerous
practice when the “pseudorandom generator” is easy to predict (such as
the linear congruential generator or some modiﬁcations of it that output
a constant fraction of the bits of each resulting number). However,
this common practice becomes sound provided one uses pseudorandom
generators (as deﬁned in Section 3.2). An alternative and more ﬂexible
construction follows.
Private-key encryption scheme based on pseudorandom func-
tions:
We present a simple construction that uses pseudorandom
functions as deﬁned in Section 3.3. The key generation algorithm con-
sists of selecting a seed, denoted s, for a (pseudorandom) function,
denoted fs. To encrypt a message x ∈{0, 1}n (using key s), the encryp-
tion algorithm uniformly selects a string r ∈{0, 1}n and produces the
ciphertext (r, x⊕fs(r)), where ⊕denotes the exclusive-or of bit strings.
To decrypt the ciphertext (r, y) (using key s), the decryption algo-
rithm just computes y ⊕fs(r). The proof of security of this encryption
scheme consists of two steps (suggested as a general methodology in
Section 3.3):
(1) Prove that an idealized version of the scheme, in which one
uses a uniformly selected function F :{0, 1}n →{0, 1}n, rather
than the pseudorandom function fs, is secure.
(2) Conclude that the real scheme (as presented above) is secure
(because, otherwise one could distinguish a pseudorandom
function from a truly random one).

68
Encryption Schemes
Note that we could have gotten rid of the randomization (in the encryp-
tion process) if we had allowed the encryption algorithm to be history-
dependent (e.g., use a counter in the role of r). This can be done pro-
vided that either only one party uses the key for encryption (and main-
tains a counter) or that all parties that encrypt, using the same key,
coordinate their actions (i.e., maintain a joint state (e.g., counter)).
Indeed, when using a private-key encryption scheme, a common situa-
tion is that the same key is only used for communication between two
speciﬁc parties, which update a joint counter during their communi-
cation. Furthermore, if the encryption scheme is used for fifo com-
munication between the parties and both parties can reliably maintain
the counter value, then there is no need (for the sender) to send the
counter value. (The resulting scheme is related to “stream ciphers” that
are commonly used in practice.)
We comment that the use of a counter (or any other state) in the
encryption process is not reasonable in the case of public-key encryp-
tion schemes, because it is incompatible with the canonical usage of
such schemes (i.e., allowing all parties to send encrypted messages to
the “owner of the encryption-key” without engaging in any type of fur-
ther coordination or communication). Furthermore, as discussed before,
probabilistic encryption is essential for a secure public-key encryption
scheme even in the case of encrypting a single message (unlike in the
case of private-key schemes). Following Goldwasser and Micali (80), we
now demonstrate the use of probabilistic encryption in the construction
of a public-key encryption scheme.
Public-key encryption scheme based on trapdoor permuta-
tions:
We present two constructions that employ a collection of trap-
door permutations, as deﬁned in Deﬁnition 2.3. Let {fi : Di →Di}i be
such a collection, and let b be a corresponding hard-core predicate. The
key generation algorithm consists of selecting a permutation fi along
with a corresponding trapdoor t, and outputting (i, t) as the key-pair.
To encrypt a (single) bit σ (using the encryption-key i), the encryp-
tion algorithm uniformly selects r ∈Di, and produces the ciphertext
(fi(r), σ ⊕b(r)). To decrypt the ciphertext (y, τ) (using the decryption-
key t), the decryption algorithm computes τ ⊕b(f −1
i
(y)) (using the

5.2. Constructions
69
trapdoor t of fi). Clearly, (σ ⊕b(r)) ⊕b(f −1
i
(fi(r))) = σ. Indistin-
guishability of encryptions can be easily proved using the fact that b is
a hard-core of fi. We comment that the above scheme is quite waste-
ful in bandwidth; however, the paradigm underlying its construction
(i.e., applying the trapdoor permutation to a randomized version of the
plaintext rather than to the actual plaintext ) is valuable in practice.
A more eﬃcient construction of a public-key encryption scheme, which
uses the same key-generation algorithm, follows. To encrypt an ℓ-bit
long string x (using the encryption-key i), the encryption algorithm
uniformly selects r ∈Di, computes y ←b(r) · b(fi(r)) · · · b(f ℓ−1
i
(r))
and produces the ciphertext (f ℓ
i (r), x ⊕y). To decrypt the cipher-
text (u, v) (using the decryption-key t), the decryption algorithm ﬁrst
recovers r = f −ℓ
i
(u) (using the trapdoor t of fi), and then obtains
v ⊕b(r) · b(fi(r)) · · · b(f ℓ−1
i
(r)). Note the similarity to the construction
in Theorem 3.3, and the fact that the proof can be extended to estab-
lish the computational indistinguishability of (b(r) · · · b(f ℓ−1
i
(r)), f ℓ
i (r))
and (r′, f ℓ
i (r)), for random and independent r ∈Di and r′ ∈{0, 1}ℓ.
Indistinguishability of encryptions follows, and thus the aforementioned
scheme is secure.
Concrete implementations of the aforementioned public-key
encryption schemes:
For the ﬁrst scheme, we are going to use the
RSA scheme (112) as a trapdoor permutation (rather than using it
directly as an encryption scheme).4 The RSA scheme has an instance-
generating algorithm that randomly selects two primes, p and q, com-
putes their product N = p · q, and selects at random a pair of integers
(e, d) such that e · d ≡1
(mod φ(N)), where φ(N) def
= (p −1) · (q −1).
(The “plain RSA” operations are raising to power e or d modulo N.)
We construct a public-key encryption scheme as follows: The key-
generation algorithm is identical to the instance-generator algorithm of
RSA, and the encryption-key is set to (N, e) (resp., the decryption-key
is set to (N, d)), just as in “plain RSA”. To encrypt a single bit σ (using
4 Recall that RSA itself is not semantically secure, because it employs a deterministic
encryption algorithm. The scheme presented here can be viewed as a “randomized version”
of RSA.

70
Encryption Schemes
Key-generation on security parameter n:
(1) Select at random two n-bit primes, P and Q, each congruent to
3 mod 4.
(2) Compute
dP
=
((P + 1)/4)ℓ(n) mod P −1,
dQ
=
((Q +
1)/4)ℓ(n) mod Q −1, cP
=
Q · (Q−1 mod P ), and cQ
=
P ·
(P −1 mod Q).
The
output
key-pair
is
(N, T),
where
N
=
P Q
and
T
=
(P, Q, N, cP , dP , cQ, dQ).
(Note: for every s, it holds that (s2)(P +1)/4
≡
s
(mod P ), and so
(s2ℓ(n))dP ≡s
(mod P ). Thus, raising to the dP -th power modulo P is
equivalent to taking the 2ℓ-th root modulo P . To recover roots modulo N, we
use the Chinese Remainder Theorem with the corresponding coeﬃcients cP
and cQ.)
Encryption of message x ∈{0, 1}ℓ(n) using the encryption-key N:
(1) Uniformly select s0 ∈{1, ..., N}.
(2) For i = 1, .., ℓ(n) + 1, compute si ←s2
i−1 mod N and σi = lsb(si).
The ciphertext is (sℓ(n)+1, y), where y = x ⊕σ1σ2 · · · σℓ(n).
(Note: s1 plays the role played by r in the general scheme.)
Decryption of
the
ciphertext
(r, y)
using
the
encryption-key
T
=
(P, Q, N, cP , dP , cQ, dQ):
(1) Let s′ ←rdP mod P, and s′′ ←rdQ mod Q.
(2) Let s1 ←cP · s′ + cQ · s′′ mod N.
(3) For i = 1, .., ℓ(n), compute σi = lsb(si) and si+1 ←s2
i mod N.
The plaintext is y ⊕σ1σ2 · · · σℓ(n).
Note: lsb is a hard-core of the modular squaring function (3).
Fig. 5.3 The Blum–Goldwasser Public-Key Encryption Scheme (30). For simplicity we
assume that ℓ, which is polynomially bounded (e.g., ℓ(n) = n), is known at key-generation
time.
the encryption-key (N, e)), the encryption algorithm uniformly selects
an element, r, in the set of residues mod N, and produces the ciphertext
(re mod N, σ ⊕lsb(r)), where lsb(r) denotes the least signiﬁcant bit of
r (which is a hard-core of the RSA function (3)). To decrypt the cipher-
text (y, τ) (using the decryption-key (N, d)), the decryption algorithm
just computes τ ⊕lsb(yd mod N). Turning to the second scheme, we
assume the intractability of factoring large integers, and use squaring
modulo a composite as a trapdoor permutation over the corresponding
quadratic residues (while using composites that are the product of two

5.3. Beyond eavesdropping security
71
primes, each congruent to 3 modulo 4). The resulting secure public-key
encryption scheme, depicted in Figure 5.3, has eﬃciency comparable to
that of (plain) RSA. We comment that special properties of modular
squaring were only used (in Figure 5.3) to speed-up the computation
of f −ℓ
i
(i.e., rather than iteratively extracting modular square roots ℓ
times, we extracted the modular 2ℓ-th root).
5.3
Beyond eavesdropping security
Our treatment so far has referred only to a “passive” attack in which
the adversary merely eavesdrops on the line over which ciphertext s
are being sent. Stronger types of attacks, culminating in the so-called
Chosen Ciphertext Attack, may be possible in various applications.
Speciﬁcally, in some settings it is feasible for the adversary to make
the sender encrypt a message of the adversary’s choice, and in some
settings the adversary may even make the receiver decrypt a cipher-
text of the adversary’s choice. This gives rise to chosen plaintext attacks
and to chosen ciphertext attacks, respectively, which are not covered by
the security deﬁnitions considered in previous sections. In this section
we brieﬂy discuss such “active” attacks, focusing on chosen ciphertext
attacks (of the stronger type known as “a posteriori” or “CCA2”).
Loosely speaking, in a chosen ciphertext attack, the adversary may
obtain the decryptions of ciphertext s of its choice, and is deemed suc-
cessful if it learns something regarding the plaintext that corresponds
to some diﬀerent ciphertext (see (89; 19) and (67, Sec. 5.4.4)). That is,
the adversary is given oracle access to the decryption function corre-
sponding to the decryption-key in use (and, in the case of private-key
schemes, it is also given oracle access to the corresponding encryption
function). The adversary is allowed to query the decryption oracle on
any ciphertext except for the “test ciphertext ” (i.e., the very ciphertext
for which it tries to learn something about the corresponding plain-
text). It may also make queries that do not correspond to legitimate
ciphertext s, and the answer will be accordingly (i.e., a special “failure”
symbol). Furthermore, the adversary may eﬀect the selection of the test
ciphertext (by specifying a distribution from which the corresponding
plaintext is to be drawn).

72
Encryption Schemes
Private-key and public-key encryption schemes secure against cho-
sen ciphertext attacks can be constructed under (almost) the same
assumptions that suﬃce for the construction of the corresponding pas-
sive schemes. Speciﬁcally:
Theorem 5.3. (folklore, see (67, Sec. 5.4.4.3)): Assuming the exis-
tence of one-way functions, there exist private-key encryption schemes
that are secure against chosen ciphertext attack.
Theorem 5.4. ((104; 50), using (29; 56), see (67, Sec. 5.4.4.4)):
Assuming the existence of enhanced5 trapdoor permutations, there exist
public-key encryption schemes that are secure against chosen ciphertext
attack.
Both theorems are proved by constructing encryption schemes in which
the adversary’s gain from a chosen ciphertext attack is eliminated by
making it infeasible (for the adversary) to obtain any useful knowl-
edge via such an attack. In the case of private-key schemes (i.e.,
Theorem 5.3), this is achieved by making it infeasible (for the adver-
sary) to produce legitimate ciphertext s (other than those explic-
itly given to it, in response to its request to encrypt plaintext s of
its choice). This, in turn, is achieved by augmenting the ciphertext
with an “authentication tag” that is hard to generate without knowl-
edge of the encryption-key; that is, we use a message-authentication
scheme (as deﬁned in Section 6). In the case of public-key schemes
(i.e., Theorem 5.4), the adversary can certainly generate ciphertext s
by itself, and the aim is to to make it infeasible (for the adversary) to
produce legitimate ciphertext s without “knowing” the corresponding
plaintext . This, in turn, will be achieved by augmenting the plain-
text with a non-interactive zero-knowledge “proof of knowledge” of the
corresponding plaintext .
5 Loosely speaking, the enhancement refers to the hardness condition of Deﬁnition 2.2, and
requires that it be hard to recover f−1
i
(y) also when given the coins used to sample y
(rather than merely y itself). See (67, Apdx. C.1).

5.3. Beyond eavesdropping security
73
Security against chosen ciphertext attack is related to the notion of
non-malleability of the encryption scheme (cf. (50)). Loosely speaking,
in a non-malleable encryption scheme it is infeasible for an adversary,
given a ciphertext , to produce a valid ciphertext for a related plain-
text (e.g., given a ciphertext of a plaintext 1x, for an unknown x, it
is infeasible to produce a ciphertext to the plaintext 0x). For further
discussion see (50; 19; 89).


6
Signature and Message Authentication Schemes
Both signature schemes and message authentication schemes are meth-
ods for “validating” data; that is, verifying that the data was approved
by a certain party (or set of parties). The diﬀerence between signa-
ture schemes and message authentication schemes is that signatures
should be “universally veriﬁable”, whereas authentication tags are only
required to be veriﬁable by parties that are also able to generate them.
Signature Schemes:
The need to discuss “digital signatures” (49;
108) has arisen with the introduction of computer communication to
the business environment (in which parties need to commit them-
selves to proposals and/or declarations that they make). Discussions
of “unforgeable signatures” did take place also in previous centuries,
but the objects of discussion were handwritten signatures (and not dig-
ital ones), and the discussion was not perceived as related to “crypto-
graphy”. Loosely speaking, a scheme for unforgeable signatures should
satisfy the following:
• each user can eﬃciently produce its own signature on docu-
ments of its choice;
75

76
Signature and Message Authentication Schemes
• every user can eﬃciently verify whether a given string is a
signature of another (speciﬁc) user on a speciﬁc document;
but
• it is infeasible to produce signatures of other users to docu-
ments they did not sign.
We note that the formulation of unforgeable digital signatures provides
also a clear statement of the essential ingredients of handwritten sig-
natures. The ingredients are each person’s ability to sign for itself, a
universally agreed veriﬁcation procedure, and the belief (or assertion)
that it is infeasible (or at least hard) to forge signatures (i.e., produce
some other person’s signatures to documents that were not signed by it
such that these “unauthentic” signatures are accepted by the veriﬁca-
tion procedure). It is not clear to what extent handwritten signatures
meet these requirements. In contrast, our discussion of digital signa-
tures provides precise statements concerning the extent to which digi-
tal signatures meet the above requirements. Furthermore, unforgeable
digital signature schemes can be constructed based on some reasonable
computational assumptions (i.e., the existence of one-way functions).
Message authentication schemes:
Message authentication is a
task related to the setting considered for encryption schemes; that
is, communication over an insecure channel. This time, we consider
an active adversary that is monitoring the channel and may alter the
messages sent on it. The parties communicating through this insecure
channel wish to authenticate the messages they send so that their coun-
terpart can tell an original message (sent by the sender) from a modiﬁed
one (i.e., modiﬁed by the adversary). Loosely speaking, a scheme for
message authentication should satisfy the following:
• each of the communicating parties can eﬃciently produce an
authentication tag to any message of its choice;
• each of the communicating parties can eﬃciently verify
whether a given string is an authentication tag of a given
message; but

6.1. Deﬁnitions
77
• it is infeasible for an external adversary (i.e., a party other
than the communicating parties) to produce authentication
tags to messages not sent by the communicating parties.
Note that in contrast to the speciﬁcation of signature schemes we do not
require universal veriﬁcation: only the designated receiver is required
to be able to verify the authentication tags. Furthermore, we do not
require that the receiver cannot produce authentication tags by itself
(i.e., we only require that external parties cannot do so). Thus, message
authentication schemes cannot convince a third party that the sender
has indeed sent the information (rather than the receiver having gener-
ated it by itself). In contrast, signatures can be used to convince third
parties: in fact, a signature to a document is typically sent to a second
party so that in the future this party may (by merely presenting the
signed document) convince third parties that the document was indeed
generated (or sent or approved) by the signer.
6.1
Deﬁnitions
Formally speaking, both signature schemes and message authentica-
tion schemes consist of three eﬃcient algorithms: key generation, sign-
ing and veriﬁcation. As in the case of encryption schemes, the key-
generation algorithm is used to generate a pair of corresponding keys,
one is used for signing and the other is used for veriﬁcation. The dif-
ference between the two types of schemes is reﬂected in the deﬁnition
of security. In the case of signature schemes, the adversary is given the
veriﬁcation-key, whereas in the case of message authentication schemes
the veriﬁcation-key (which may equal the signing-key) is not given to
the adversary. Thus, schemes for message authentication can be viewed
as a private-key version of signature schemes. This diﬀerence yields dif-
ferent functionalities (even more than in the case of encryption): In the
typical use of a signature scheme, each user generates a pair of sign-
ing and veriﬁcation keys, publicizes the veriﬁcation-key and keeps the
signing-key secret. Subsequently, each user may sign documents using
its own signing-key, and these signatures are universally veriﬁable with
respect to its public veriﬁcation-key. In contrast, message authentica-
tion schemes are typically used to authenticate information sent among

78
Signature and Message Authentication Schemes
a set of mutually trusting parties that agree on a secret key, which is
being used both to produce and verify authentication-tags. (Indeed,
it is assumed that the mutually trusting parties have generated the
key together or have exchanged the key in a secure way, prior to the
communication of information that needs to be authenticated.)
We focus on the deﬁnition of secure signature schemes. Following
Goldwasser, Micali and Rivest (82), we consider very powerful attacks
on the signature scheme as well as a very liberal notion of breaking it.
Speciﬁcally, the attacker is allowed to obtain signatures to any message
of its choice. One may argue that in many applications such a general
attack is not possible (because messages to be signed must have a spe-
ciﬁc format). Yet, our view is that it is impossible to deﬁne a general
(i.e., application-independent) notion of admissible messages, and thus
a general/robust deﬁnition of an attack seems to have to be formulated
as suggested here. (Note that, at worst, our approach is overly cau-
tious.) Likewise, the adversary is said to be successful if it can produce
a valid signature to any message for which it has not asked for a signa-
ture during its attack. Again, this refers to the ability to form signatures
to possibly “nonsensical” messages as a breaking of the scheme. Yet,
again, we see no way to have a general (i.e., application-independent)
notion of “meaningful” messages (so that only forging signatures to
them will be considered a breaking of the scheme).
Deﬁnition 6.1. (secure signature schemes – a sketch): A chosen mes-
sage attack is a process that, on input a veriﬁcation-key, can obtain
signatures (relative to the corresponding signing-key) to messages of
its choice. Such an attack is said to succeed (in existential forgery) if it
outputs a valid signature to a message for which it has not requested a
signature during the attack. A signature scheme is secure (or unforge-
able) if every feasible chosen message attack succeeds with at most
negligible probability, where the probability is taken over the initial
choice of the key-pair as well as over the adversary’s actions.
We stress that plain RSA (alike plain versions of Rabin’s scheme (109)
and the DSS (1)) is not secure under the above deﬁnition. However, it

6.2. Constructions
79
may be secure if the message is “randomized” before RSA (or the other
schemes) is applied.
6.2
Constructions
Secure message authentication schemes can be constructed using
pseudorandom functions (68). Speciﬁcally, the key-generation algo-
rithm consists of selecting a seed s ∈{0, 1}n for such a function,
denoted fs :{0, 1}∗→{0, 1}n, and the (only valid) tag of message x with
respect to the key s is fs(x). As in the case of our private-key encryp-
tion scheme, the proof of security of the current message authentication
scheme consists of two steps:
(1) Prove that an idealized version of the scheme, in which one
uses a uniformly selected function F :{0, 1}∗→{0, 1}n, rather
than the pseudorandom function fs, is secure (i.e., unforge-
able).
(2) Conclude that the real scheme (as presented above) is secure
(because, otherwise one could distinguish a pseudorandom
function from a truly random one).
Note that the aforementioned message authentication scheme makes
an “extensive use of pseudorandom functions” (i.e., the pseudorandom
function is applied directly to the message, which requires a general-
ized notion of pseudorandom functions (cf. Section 3.3)). More eﬃcient
schemes may be obtained either based on a more restricted use of a
pseudorandom function (cf., e.g., (17)) or based on other cryptographic
primitives (cf., e.g., (93)).
Constructing secure signature schemes seems more diﬃcult than
constructing message authentication schemes. Nevertheless, secure sig-
nature schemes can be constructed based on any one-way function.
Furthermore:
Theorem 6.2. ((103; 114), see (67, Sec. 6.4)): The following three
conditions are equivalent.

80
Signature and Message Authentication Schemes
(1) One-way functions exist.
(2) Secure signature schemes exist.
(3) Secure message authentication schemes exist.
We stress that, unlike in the case of public-key encryption schemes,
the construction of signature schemes (which may be viewed as a
public-key analogue of message authentication) does not use a trapdoor
property.
How to construct secure signature schemes
Three central paradigms used in the construction of secure signature
schemes are the “refreshing” of the “eﬀective” signing-key, the usage
of an “authentication tree”, and the “hashing paradigm” (all to be
discussed in the sequel). In addition to being used in the proof of
Theorem 6.2, all three paradigms are also of independent interest.
The
refreshing
paradigm.
Introduced in (82), the refreshing
paradigm is aimed at limiting the potential dangers of chosen message
attacks. This is achieved by signing the actual document using a newly
(randomly) generated instance of the signature scheme, and authenti-
cating (the veriﬁcation-key of) this random instance with respect to the
ﬁxed public-key. That is, consider a basic signature scheme (G, S, V )
used as follows. Suppose that the user U has generated a key-pair,
(s, v) ←G(1n), and has placed the veriﬁcation-key v on a public-ﬁle.
When a party asks U to sign some document α, the user U generates
a new (“fresh”) key-pair, (s′, v′) ←G(1n), signs v′ using the origi-
nal signing-key s, signs α using the new signing-key s′, and presents
(Ss(v′), v′, Ss′(α)) as a signature to α. An alleged signature, (β1, v′, β2),
is veriﬁed by checking whether both Vv(v′, β1) = 1 and Vv′(α, β2) = 1
hold. Intuitively, the gain in terms of security is that a full-ﬂedged cho-
sen message attack cannot be launched on a ﬁxed instance of (G, S, V )
(i.e., on the ﬁxed veriﬁcation-key that resides in the public-ﬁle and is
known to the attacker). All that an attacker may obtain (via a chosen
message attack on the new scheme) is signatures, relative to the origi-
nal signing-key s of (G, S, V ), to random strings (distributed according
to G(1n)) as well as additional signatures that are each relative to a
random and independently distributed signing-key.

6.2. Constructions
81
Authentication
trees.
The security beneﬁts of the refreshing
paradigm are increased when combining it with the use of authen-
tication trees, as introduced in (98). The idea is to use the pub-
lic veriﬁcation-key in order to authenticate several (e.g., two) fresh
instances of the signature scheme, use each of these instances to authen-
ticate several additional fresh instances, and so on. We obtain a tree of
fresh instances of the basic signature scheme, where each internal node
authenticates its children. We can now use the leaves of this tree in order
to sign actual documents, where each leaf is used at most once. Thus,
a signature to an actual document consists of (1) a signature to this
document authenticated with respect to the veriﬁcation-key associated
with some leaf, and (2) a sequence of veriﬁcation-keys associated with
the nodes along the path from the root to this leaf, where each such
veriﬁcation-key is authenticated with respect to the veriﬁcation-key of
its parent. We stress that (by suitable implementation to be discussed
below) each instance of the signature scheme is used to sign at most one
string (i.e., a single sequence of veriﬁcation-keys if the instance resides
in an internal node, and an actual document if the instance resides in a
leaf). Thus, it suﬃces to use a signature scheme that is secure as long
as it is used to legitimately sign a single string. Such signature schemes,
called one-time signature schemes and introduced in (108), are easier to
construct than standard signature schemes, especially if one only wishes
to sign strings that are signiﬁcantly shorter than the signing-key (resp.,
than the veriﬁcation-key). For example, using a one-way function f, we
may let the signing-key consist of a sequence of n pairs of strings, let the
corresponding veriﬁcation-key consist of the corresponding sequence of
images of f, and sign an n-bit long message by revealing the adequate
pre-images.1
The hashing paradigm.
Note, however, that in the aforementioned
authentication-tree, the instances of the signature scheme (associated
with internal nodes) are used to sign a pair of veriﬁcation-keys. Thus,
1 That is, the signing-key consist of a sequence ((s0
1, s1
1), ..., (s0
n, s1
n)) ∈{0, 1}2n2, the cor-
responding veriﬁcation-key is (f(s0
1), f(s1
1)), ..., (f(s0
n), f(s1
n))), and the signature of the
message σ1 · · · σn is (sσ1
1 , ..., sσn
n ).

82
Signature and Message Authentication Schemes
we need a one-time signature scheme that can be used for signing mes-
sages that are longer than the veriﬁcation-key. Here is where the hashing
paradigm comes into play. This paradigm refers to the common practice
of signing documents via a two-stage process: First the actual document
is hashed to a (relatively) short bit string, and next the basic signa-
ture scheme is applied to the resulting string. This practice (as well
as other usages of the hashing paradigm) is sound provided that the
hashing function belongs to a family of collision-free hashing functions
(i.e., loosely speaking, given a random hash function in the family, it
is infeasible to ﬁnd two diﬀerent strings that are hashed by this func-
tion to the same value; cf. (47)). (A variant of the hashing paradigm
uses the weaker notion of a family of Universal One-Way Hash Func-
tions (cf. (103)), which in turn can be constructed using any one-way
function (103; 114).)
Implementation details.
In order to implement the aforementioned
(full-ﬂedged) signature scheme one needs to store in (secure) memory
all the instances of the basic (one-time) signature scheme that are gen-
erated throughout the entire signing process (which refers to numerous
documents). This can be done by extending the model so to allow for
memory-dependent signature schemes. Alternatively, we note that all
that we need to store are the random-coins used for generating each
of these instances, and the former can be determined by a pseudoran-
dom function (applied to the name of the corresponding vertex in the
tree). Indeed, the seed of this pseudorandom function will be part of
the signing-key of the resulting (full-ﬂedged) signature scheme.
6.3
Public-key infrastructure
The standard use of public-key encryption schemes (resp., signature
schemes) in real-life communication requires a mechanism for provid-
ing the sender (resp., signature veriﬁer) with the receiver’s authentic
encryption-key (resp., signer’s authentic veriﬁcation-key). Speciﬁcally,
this problem arises in large-scale systems, where typically the sender
(resp., veriﬁer) does not have a local record of the receiver’s encryption-
key (resp., signer’s veriﬁcation-key), and so must obtain this key in a

6.3. Public-key infrastructure
83
“reliable” way (i.e., typically, certiﬁed by some trusted authority). In
most theoretical works, one assumes that the keys are posted on and
can be retrieved from a public-ﬁle that is maintained by a trusted party
(which makes sure that each user can post only keys bearing its own
identity). In practice, maintaining such a public-ﬁle is a major problem,
and mechanisms that implement this abstraction are typically referred
to by the generic term “public-key infrastructure (PKI)”. For a discus-
sion of the practical problems regarding PKI deployment see, e.g., (97,
Section 13).


TEAM FLY
7
General Cryptographic Protocols
The design of secure protocols that implement arbitrary desired func-
tionalities is a major part of modern cryptography. Taking the opposite
perspective, the design of any cryptographic scheme may be viewed as
the design of a secure protocol for implementing a suitable functional-
ity. Still, we believe that it makes sense to diﬀerentiate between basic
cryptographic primitives (which involve little interaction) like encryp-
tion and signature schemes on one hand, and general cryptographic
protocols on the other hand.
We survey general results concerning secure multi-party computa-
tions, where the two-party case is an important special case. In a nut-
shell, these results assert that one can construct protocols for securely
computing any desirable multi-party functionality. Indeed, what is
striking about these results is their generality, and we believe that the
wonder is not diminished by the (various alternative) conditions under
which these results hold.
Our focus on the general study of secure multi-party computation
(rather than on protocols for solving speciﬁc problems) is natural in
the context of the theoretical treatment of the subject matter. We wish
to highlight the importance of this general study to practice. Firstly,
85

86
General Cryptographic Protocols
this study clariﬁes fundamental issues regarding security in a multi-
party environment. Secondly, it draws the lines between what is possible
in principle and what is not. Thirdly, it develops general techniques
for designing secure protocols. And last, sometimes, it may even yield
schemes (or modules) that may be incorporated in practical systems.
REAL   MODEL
IDEAL   MODEL
Fig. 7.1 Secure protocols emulate a trusted party – an illustration.
A general framework for casting (m-party) cryptographic (protocol)
problems consists of specifying a random process1 that maps m inputs
to m outputs. The inputs to the process are to be thought of as the local
inputs of m parties, and the m outputs are their corresponding (desired)
local outputs. The random process describes the desired functionality.
That is, if the m parties were to trust each other (or trust some external
party), then they could each send their local input to the trusted party,
who would compute the outcome of the process and send to each party
the corresponding output. A pivotal question in the area of crypto-
graphic protocols is to what extent can this (imaginary) trusted party
be “emulated” by the mutually distrustful parties themselves.
1 That is, we consider the secure evaluation of randomized functionalities, rather than
“only” the secure evaluation of functions. Speciﬁcally, we consider an arbitrary (ran-
domized) process F that on input (x1, ..., xm), ﬁrst selects at random (depending only
on ℓ
def
= m
i=1 |xi|) an m-ary function f, and then outputs the m-tuple f(x1, ..., xm) =
(f1(x1, ..., xm), ..., fm(x1, ..., xm)). In other words, F (x1, ..., xm) = F ′(r, x1, ..., xm), where
r is uniformly selected in {0, 1}ℓ′ (with ℓ′ = poly(ℓ)), and F ′ is a function mapping (m+1)-
long sequences to m-long sequences.

7.1. The deﬁnitional approach and some models
87
The results surveyed below describe a variety of models in which
such an “emulation” is possible. The models vary by the underlying
assumptions regarding the communication channels, numerous para-
meters relating to the extent of adversarial behavior, and the desired
level of emulation of the trusted party (i.e., level of “security”).
Organization:
Section 7.1 provides a rather comprehensive survey of
the various deﬁnitions used in the area of secure multi-party computa-
tion, whereas Section 7.2 similarly surveys the known results. However,
some readers may prefer to ﬁrst consider one concrete case of the def-
initional approach, as provided in Section 7.1.2, and proceed directly
to see some constructions. Indeed, a few constructions are sketched in
Section 7.3. All the above refers to the security of stand-alone exe-
cutions, and the preservation of security in an environment in which
many executions of many protocols are being attacked is considered in
Section 7.4.
7.1
The deﬁnitional approach and some models
Before describing the aforementioned results, we further discuss the
notion of “emulating a trusted party”, which underlies the deﬁnitional
approach to secure multi-party computation (as initiated and developed
in (73; 100; 13; 14; 35; 36)). The approach can be traced back to the
deﬁnition of zero-knowledge (cf. (81)), and even to the deﬁnition of
secure encryption (cf. (64), rephrasing (80)). The underlying paradigm
(called the simulation paradigm (cf. Section 4.1)) is that a scheme is
secure if whatever a feasible adversary can obtain after attacking it, is
also feasibly attainable “from scratch”. In the case of zero-knowledge
this amounts to saying that whatever a (feasible) veriﬁer can obtain
after interacting with the prover on a prescribed valid assertion, can be
(feasibly) computed from the assertion itself. In the case of multi-party
computation we compare the eﬀect of adversaries that participate in
the execution of the actual protocol to the eﬀect of adversaries that
participate in an imaginary execution of a trivial (ideal) protocol for
computing the desired functionality with the help of a trusted party. If
whatever the adversaries can feasibly obtain in the former real setting

88
General Cryptographic Protocols
can also be feasibly obtained in the latter ideal setting then the protocol
“emulates the ideal setting” (i.e., “emulates a trusted party”), and so
is deemed secure. This basic approach can be applied in a variety of
models, and is used to deﬁne the goals of security in these models.2 We
ﬁrst discuss some of the parameters used in deﬁning various models,
and next demonstrate the application of this approach in two important
models. For further details, see (36) or (67, Sec. 7.2 and 7.5.1).
7.1.1
Some parameters used in deﬁning security models
The following parameters are described in terms of the actual (or real)
computation. In some cases, the corresponding deﬁnition of security is
obtained by imposing some restrictions or provisions on the ideal model.
For example, in the case of two-party computation (see below), secure
computation is possible only if premature termination is not consid-
ered a breach of security. In that case, the suitable security deﬁnition
is obtained (via the simulation paradigm) by allowing (an analogue
of) premature termination in the ideal model. In all cases, the desired
notion of security is deﬁned by requiring that for any adequate adver-
sary in the real model, there exist a corresponding adversary in the
corresponding ideal model that obtains essentially the same impact (as
the real-model adversary).
The communication channels: The
parameters
of
the
model
include questions like whether or not the channels may be
tapped by an adversary, whether or not they are tamper-free,
and questions referring to the network behavior (in the case of
multi-party protocols).
2 A few technical comments are in place. Firstly, we assume that the inputs of all parties are
of the same length. We comment that as long as the lengths of the inputs are polynomially
related, the above convention can be enforced by padding. On the other hand, some length
restriction is essential for the security results, because in general it is impossible to hide
all information regarding the length of the inputs to a protocol. Secondly, we assume
that the desired functionality is computable in probabilistic polynomial-time, because we
wish the secure protocol to run in probabilistic polynomial-time (and a protocol cannot
be more eﬃcient than the corresponding centralized algorithm). Clearly, the results can
be extended to functionalities that are computable within any given (time-constructible)
time bound, using adequate padding.

7.1. The deﬁnitional approach and some models
89
Wire-tapping versus the private-channel model: The standard
assumption in cryptography is that the adversary may tap all
communication channels (between honest parties). In contrast,
one may postulate that the adversary cannot obtain messages
sent between a pair of honest parties, yielding the so-called
private-channel model (cf. (25; 42)). The latter postulate may
be justiﬁed in some settings. Furthermore, it may be viewed as a
useful abstraction that provides a clean model for the study and
development of secure protocols. In this respect, it is important
to mention that, in a variety of settings of the other parameters,
private channels can be easily emulated by ordinary “tapped
channels”.
Broadcast channel: In the multi-party context, one may postu-
late the existence of a broadcast channel (cf. (110)), and the
motivation and justiﬁcations are as in the case of the private-
channel model.
The tamper-free assumption: The standard assumption in the
area is that the adversary cannot modify, duplicate, or gener-
ate messages sent over the communication channels (between
honest parties). Again, this assumption can be justiﬁed in some
settings and can be emulated in others (cf., (18; 34)).
Network behavior: Most works in the area assume that com-
munication is synchronous and that point-to-point channels
exist between every pair of processors (i.e., a complete net-
work). However, one may also consider asynchronous communi-
cation (cf. (23)) and arbitrary networks of point-to-point chan-
nels (cf. (51)).
Set-up assumptions: Unless stated diﬀerently, we make no set-up
assumptions (except for the obvious assumption that all par-
ties have identical copies of the protocol’s program). How-
ever, in some cases it is assumed that each party knows a
veriﬁcation-key corresponding to each of the other parties (or
that a public-key infrastructure is available). Another assump-
tion, made more rarely, is that all parties have access to some
common (trusted) random string.

90
General Cryptographic Protocols
Computational limitations: Typically, we consider computation-
ally-bounded adversaries (e.g., probabilistic polynomial-time
adversaries). However, the private-channel model allows for
the (meaningful) consideration of computationally-unbounded
adversaries.
We stress that, also in the case of computationally-unbounded
adversaries, security should be deﬁned by requiring that for
every real adversary, whatever the adversary can compute after
participating in the execution of the actual protocol is com-
putable within comparable time by an imaginary adversary par-
ticipating in an imaginary execution of the trivial ideal proto-
col (for computing the desired functionality with the help of
a trusted party). That is, although no computational restric-
tions are made on the real-model adversary, it is required that
the ideal-model adversary that obtains the same impact does
so within comparable time (i.e., within time that is polyno-
mially related to the running time of the real-model adver-
sary being simulated). Thus, any construction proven secure in
the computationally-unbounded adversary model is (trivially)
secure with respect to computationally-bounded adversaries.
Restricted adversarial behavior: The parameters of the model
include questions like whether or not the adversary is “adap-
tive” and “active” (where these terms are discussed next).
Adaptive versus non-adaptive: The most general type of an
adversary considered in the literature is one that may corrupt
parties to the protocol while the execution goes on, and does so
based on partial information it has gathered so far (cf., (37)).
A somewhat more restricted model, which seems adequate in
many settings, postulates that the set of dishonest parties is
ﬁxed (arbitrarily) before the execution starts (but this set is,
of course, not known to the honest parties). The latter model
is called non-adaptive as opposed to the adaptive adversary
discussed ﬁrst. Although the adaptive model is stronger, the
author believes that the non-adaptive model provides a reason-
able level of security in many applications.

7.1. The deﬁnitional approach and some models
91
Active versus passive: An orthogonal parameter of restriction
refers to whether a dishonest party takes active steps to dis-
rupt the execution of the protocol (i.e., sends messages that
diﬀer from those speciﬁed by the protocol), or merely gath-
ers information (which it may latter share with the other dis-
honest parties). The latter adversary has been given a variety
of names such as semi-honest, passive, and honest-but-curious.
This restricted model may be justiﬁed in certain settings, and
certainly provides a useful methodological locus (cf., (75; 74; 63)
and Section 7.3). Below we refer to the adversary of the unre-
stricted model as to active; another commonly used name is
malicious.
Restricted notions of security: One important example is the will-
ingness to tolerate “unfair” protocols in which the execution
can be suspended (at any time) by a dishonest party, provided
that it is detected doing so. We stress that in case the execu-
tion is suspended, the dishonest party does not obtain more
information than it could have obtained when not suspending
the execution. (What may happen is that the honest parties will
not obtain their desired outputs, but rather will detect that the
execution was suspended.) We stress that the motivation to this
restricted model is the impossibility of obtaining general secure
two-party computation in the unrestricted model.
Upper bounds on the number of dishonest parties: In
some
models, secure multi-party computation is possible only if a
majority of the parties is honest (cf., (25; 44)). Sometimes even
a special majority (e.g., 2/3) is required. General “(resilient)
adversarial-structures” have been considered too (cf. (86)).
Mobile adversary: In most works, once a party is said to be dishon-
est it remains so throughout the execution. More generally, one
may consider transient adversarial behavior (e.g., an adversary
seizes control of some site and later withdraws from it). This
model, introduced in (106), allows to construct protocols that
remain secure even in case the adversary may seize control of
all sites during the execution (but never control concurrently,

92
General Cryptographic Protocols
say, more than 10% of the sites). We comment that schemes
secure in this model were later termed “proactive” (cf., (39)).
7.1.2
Example: Multi-party protocols with honest majority
Here we consider an active, non-adaptive, computationally-bounded
adversary, and do not assume the existence of private channels. Our
aim is to deﬁne multi-party protocols that remain secure provided that
the honest parties are in majority. (The reason for requiring a honest
majority will be discussed at the end of this subsection.)
Consider any multi-party protocol. We ﬁrst observe that each party
may change its local input before even entering the execution of the
protocol. However, this is unavoidable also when the parties utilize a
trusted party. Consequently, such an eﬀect of the adversary on the
real execution (i.e., modiﬁcation of its own input prior to entering the
actual execution) is not considered a breach of security. In general,
whatever cannot be avoided when the parties utilize a trusted party,
is not considered a breach of security. We wish secure protocols (in
the real model) to suﬀer only from whatever is unavoidable also when
the parties utilize a trusted party. Thus, the basic paradigm underlying
the deﬁnitions of secure multi-party computations amounts to requiring
that the only situations that may occur in the real execution of a secure
protocol are those that can also occur in a corresponding ideal model
(where the parties may employ a trusted party). In other words, the
“eﬀective malfunctioning” of parties in secure protocols is restricted to
what is postulated in the corresponding ideal model.
When deﬁning secure multi-party protocols with honest majority,
we need to pin-point what cannot be avoided in the ideal model (i.e.,
when the parties utilize a trusted party). This is easy, because the ideal
model is very simple. Since we are interested in executions in which the
majority of parties are honest, we consider an ideal model in which any
minority group (of the parties) may collude as follows:
(1) Firstly this dishonest minority shares its original inputs and
decides together on replaced inputs to be sent to the trusted
party. (The other parties send their respective original inputs
to the trusted party.)

7.1. The deﬁnitional approach and some models
93
(2) Upon receiving inputs from all parties, the trusted party
determines the corresponding outputs and sends them to the
corresponding parties. (We stress that the information sent
between the honest parties and the trusted party is not seen
by the dishonest colluding minority.)
(3) Upon receiving the output-message from the trusted party,
each honest party outputs it locally, whereas the dishonest
colluding minority may determine their outputs based on all
they know (i.e., their initial inputs and their received out-
puts).
Note that the above behavior of the minority group is unavoidable in
any execution of any protocol (even in presence of trusted parties).
This is the reason that the ideal model was deﬁned as above. Now,
a secure multi-party computation with honest majority is required to
emulate this ideal model. That is, the eﬀect of any feasible adversary
that controls a minority of the parties in a real execution of the actual
protocol, can be essentially simulated by a (diﬀerent) feasible adversary
that controls the corresponding parties in the ideal model. That is:
Deﬁnition 7.1. (secure protocols – a sketch): Let f be an m-ary func-
tionality and Π be an m-party protocol operating in the real model.
• For a real-model adversary A, controlling some minority of
the parties (and tapping all communication channels), and
an m-sequence x, we denote by realΠ,A(x) the sequence of
m outputs resulting from the execution of Π on input x under
attack of the adversary A.
• For an ideal-model adversary A′, controlling some minority of
the parties, and an m-sequence x, we denote by idealf,A′(x)
the sequence of m outputs resulting from the ideal process
described above, on input x under attack of the adversary
A′.
We say that Π securely implements f with honest majority if for every
feasible real-model adversary A, controlling some minority of the par-

94
General Cryptographic Protocols
ties, there exists a feasible ideal-model adversary A′, controlling the
same parties, so that the probability ensembles {realΠ,A(x)}x and
{idealf,A′(x)}x are computationally indistinguishable (as in Foot-
note 2).
Thus, security means that the eﬀect of each minority group in a real
execution of a secure protocol is “essentially restricted” to replacing its
own local inputs (independently of the local inputs of the majority
parties) before the protocol starts, and replacing its own local outputs
(depending only on its local inputs and outputs) after the protocol
terminates. (We stress that in the real execution the minority parties
do obtain additional pieces of information; yet in a secure protocol they
gain nothing from these additional pieces of information, because they
can actually reproduce those by themselves.)
The fact that Deﬁnition 7.1 refers to a model without private chan-
nels is due to the fact that our (sketchy) deﬁnition of the real-model
adversary allowed it to tap the channels, which in turn eﬀects the
set of possible ensembles {realΠ,A(x)}x. When deﬁning security in
the private-channel model, the real-model adversary is not allowed to
tap channels between honest parties, and this again eﬀects the pos-
sible ensembles {realΠ,A(x)}x. On the other hand, when we wish to
deﬁne security with respect to passive adversaries, both the scope of
the real-model adversaries and the scope of the ideal-model adversaries
changes. In the real-model execution, all parties follow the protocol but
the adversary may alter the output of the dishonest parties arbitrarily
depending on all their intermediate internal states (during the execu-
tion). In the corresponding ideal-model, the adversary is not allowed
to modify the inputs of dishonest parties (in Step 1), but is allowed to
modify their outputs (in Step 3).
We comment that a deﬁnition analogous to Deﬁnition 7.1 can be
presented also in case the dishonest parties are not in minority. In fact,
such a deﬁnition seems more natural, but the problem is that such a
deﬁnition cannot be satisﬁed. That is, most natural functionalities do
not have a protocol for computing them securely in case at least half of
the parties are dishonest and employ an adequate adversarial strategy.
This follows from an impossibility result regarding two-party computa-

7.1. The deﬁnitional approach and some models
95
tion, which essentially asserts that there is no way to prevent a party
from prematurely suspending the execution (46). On the other hand,
secure multi-party computation with dishonest majority is possible if
premature suspension of the execution is not considered a breach of
security (cf. Section 7.1.3).
7.1.3
Another example: Two-party protocols allowing abort
In light of the last paragraph, we now consider multi-party computa-
tions in which premature suspension of the execution is not considered
a breach of security. For concreteness, we focus here on the special case
of two-party computations.3
Intuitively, in any two-party protocol, each party may suspend the
execution at any point in time, and furthermore it may do so as soon
as it learns the desired output. Thus, in case the output of each parties
depends on both inputs, it is always possible for one of the parties to
obtain the desired output while preventing the other party from fully
determining its own output. The same phenomenon occurs even in case
the two parties just wish to generate a common random value. Thus,
when considering active adversaries in the two-party setting, we do
not consider such premature suspension of the execution a breach of
security. Consequently, we consider an ideal model where each of the
two parties may “shut-down” the trusted (third) party at any point
in time. In particular, this may happen after the trusted party has
supplied the outcome of the computation to one party but before it
has supplied it to the other. That is, an execution in the ideal model
proceeds as follows:
(1) Each party sends its input to the trusted party, where the
dishonest party may replace its input or send no input at all
(which can be treated as sending a default value).
(2) Upon receiving inputs from both parties, the trusted party
determines the corresponding outputs, and sends the ﬁrst
output to the ﬁrst party.
3 As in Section 7.1.2, we consider a non-adaptive, active, computationally-bounded adver-
sary.

96
General Cryptographic Protocols
(3) In case the ﬁrst party is dishonest, it may instruct the trusted
party to halt, otherwise it always instructs the trusted party
to proceed. If instructed to proceed, the trusted party sends
the second output to the second party.
(4) Upon receiving the output-message from the trusted party,
the honest party outputs it locally, whereas the dishonest
party may determine its output based on all it knows (i.e.,
its initial input and its received output).
A secure two-party computation allowing abort is required to emulate this
ideal model. That is, as in Deﬁnition 7.1, security is deﬁned by requir-
ing that for every feasible real-model adversary A, there exists a fea-
sible ideal-model adversary A′, controlling the same party, so that the
probability ensembles representing the corresponding (real and ideal)
executions are computationally indistinguishable. This means that each
party’s “eﬀective malfunctioning” in a secure protocol is restricted to
supplying an initial input of its choice and aborting the computation
at any point in time. (Needless to say, the choice of the initial input of
each party may not depend on the input of the other party.)
We mention that an alternative way of dealing with the problem of
premature suspension of execution (i.e., abort) is to restrict our atten-
tion to single-output functionalities; that is, functionalities in which only
one party is supposed to obtain an output. The deﬁnition of secure com-
putation of such functionalities can be made identical to Deﬁnition 7.1,
with the exception that no restriction is made on the set of dishonest
parties (and in particular one may consider a single dishonest party in
the case of two-party protocols). For further details, see (67, Sec. 7.2.3).
7.2
Some known results
We next list some of the models for which general secure multi-party
computation is known to be attainable (i.e., models in which one can
construct secure multi-party protocols for computing any desired func-
tionality). We mention that the ﬁrst results of this type were obtained
by Goldreich, Micali, Wigderson and Yao (75; 124; 74).

7.2. Some known results
97
• Assuming the existence of enhanced4 trapdoor permutations,
secure multi-party computation is possible in the following
models (cf. (75; 124; 74) and details in (63; 67)):
(1) Passive adversary, for any number of dishonest par-
ties (cf. (67, Sec. 7.3)).
(2) Active adversary that may control only a minority of
the parties (cf. (67, Sec. 7.5.4)).
(3) Active adversary, for any number of bad parties, pro-
vided that suspension of execution is not consid-
ered a violation of security (i.e., as discussed in Sec-
tion 7.1.3). (See (67, Sec. 7.4 and 7.5.5).)
In all these cases, the adversary is computationally-bounded
and non-adaptive. On the other hand, the adversary may tap
the communication lines between honest parties (i.e., we do
not assume “private channels” here). The results for active
adversaries assume a broadcast channel. Indeed, the latter
can be implemented (while tolerating any number of bad
parties) using a signature scheme and assuming a public-key
infrastructure (or that each party knows the veriﬁcation-key
corresponding to each of the other parties).
• Making
no
computational
assumptions
and
allowing
computationally-unbounded adversaries, but assuming pri-
vate channels, secure multi-party computation is possible in
the following models (cf. (25; 42)):
(1) Passive adversary that may control only a minority
of the parties.
(2) Active adversary that may control only less than one
third of the parties.5
In both cases the adversary may be adaptive (cf. (25; 37)).
• Secure multi-party computation is possible against an active,
adaptive and mobile adversary that may control a small con-
stant fraction of the parties at any point in time (106).
4 See Footnote 5.
5 Fault-tolerance can be increased to a regular minority if a broadcast channel exists (110).

98
General Cryptographic Protocols
This result makes no computational assumptions, allows
computationally-unbounded adversaries, but assumes pri-
vate channels.
• Assuming the existence of enhanced trapdoor permutations,
secure multi-party computation is possible in a model
allowing an active and adaptive computationally-bounded
adversary that may control only less than one third of the
parties (37; 48). We stress that this result does not assume
“private channels”.
Results for asynchronous communication and arbitrary networks of
point-to-point channels were presented in (23; 26) and (51), respec-
tively.
Note that the implementation of a broadcast channel can be
cast as a cryptographic protocol problem (i.e., for the functionality
(v, λ, ..., λ) →(v, v, ..., v), where λ denotes the empty string). Thus,
it is not surprising that the results regarding active adversaries either
assume the existence of such a channel or require a setting in which
the latter can be implemented.
Secure reactive computation:
All the above results (easily) extend
to a reactive model of computation in which each party interacts with a
high-level process (or application). The high-level process supplies each
party with a sequence of inputs, one at a time, and expect to receive
corresponding outputs from the parties. That is, a reactive system goes
through (a possibly unbounded number of) iterations of the following
type:
• Parties are given inputs for the current iteration.
• Depending on the current inputs, the parties are supposed
to compute outputs for the current iteration. That is, the
outputs in iteration j are determined by the inputs of the
jth iteration.
A more general formulation allows the outputs of each iteration to
depend also on a global state, which is possibly updated in each itera-
tion. The global state may include all inputs and outputs of previous

7.3. Construction paradigms and two simple protocols
99
iterations, and may only be partially known to individual parties. (In
a secure reactive computation such a global state may be maintained
by all parties in a “secret sharing” manner.) For further discussion,
see (67, Sec. 7.7.1).
Eﬃciency
considerations:
One
important
eﬃciency
measure
regarding protocols is the number of communication rounds in their
execution. The results mentioned above were originally obtained using
protocols that use an unbounded number of rounds. In some cases, sub-
sequent works obtained secure constant-round protocols: for example,
in the case of multi-party computations with honest majority (cf. (15))
and in the case of two-party computations allowing abort (cf. (94)).
Other important eﬃciency considerations include the total number of
bits sent in the execution of a protocol, and the local computation time.
The (communication and computation) complexities of the protocols
establishing the above results are related to the computational com-
plexity of the computation, but alternative relations (e.g., where the
complexities of the secure protocols are related to the (insecure) com-
munication complexity of the computation) may be possible (cf. (102)).
Theory versus practice (or general versus speciﬁc):
This
primer is focused on presenting general notions and general feasibil-
ity results. Needless to say, practical solutions to speciﬁc problems
(e.g., voting (84), secure payment systems (16), and threshold crypto-
systems (60)) are typically derived by speciﬁc constructions (and not by
applying general results of the abovementioned type). Still, the (above-
mentioned) general results are of great importance to practice because
they characterize a wide class of security problems that are solvable
in principle, and provide techniques that may be useful also towards
constructing reasonable solutions to speciﬁc problems.
7.3
Construction paradigms and two simple protocols
We brieﬂy sketch a couple of paradigms used in the construction of
secure multi-party protocols. We focus on the construction of secure
protocols for the model of computationally-bounded and non-adaptive

100
General Cryptographic Protocols
adversaries (75; 124; 74). These constructions proceed in two steps (see
details in (63; 67)). First a secure protocol is presented for the model
of passive adversaries (for any number of dishonest parties), and next
such a protocol is “compiled” into a protocol that is secure in one of
the two models of active adversaries (i.e., either in a model allowing
the adversary to control only a minority of the parties or in a model
in which premature suspension of the execution is not considered a
violation of security). These two steps are presented in the following
two corresponding subsections, in which we also present two relatively
simple protocols for two speciﬁc tasks, which are used extensively in
the general protocols.
Recall that in the model of passive adversaries, all parties follow
the prescribed protocol, but at termination the adversary may alter
the outputs of the dishonest parties depending on all their intermediate
internal states (during the execution). Below, we refer to protocols that
are secure in the model of passive (resp., active) adversaries by the term
passively-secure (resp., actively-secure).
7.3.1
Passively-secure computation with shares
For any m ≥2, suppose that m parties, each having a private input,
wish to obtain the value of a predetermined m-argument function
evaluated at their sequence of inputs. Below, we outline a passively-
secure protocol for achieving this goal. We mention that the design
of passively-secure multi-party protocol for any functionality (allowing
diﬀerent outputs to diﬀerent parties as well as handling also random-
ized computations) reduces easily to the aforementioned task.
We assume that the parties hold a circuit for computing the value
of the function on inputs of the adequate length, and that the circuit
contains only and and not gates. The key idea is to have each party
“secretly share” its input with everybody else, and “secretly transform”
shares of the input wires of the circuit into shares of the output wires
of the circuit, thus obtaining shares of the outputs (which allows for
the reconstruction of the actual outputs). The value of each wire in the
circuit is shared in a way such that all shares yield the value, whereas
lacking even one of the shares keeps the value totally undetermined.

7.3. Construction paradigms and two simple protocols
101
That is, we use a simple secret sharing scheme (cf. (117)) such that a
bit b is shared by a random sequence of m bits that sum-up to b mod
2. First, each party shares each of its input bits with all parties (by
secretly sending each party a random value and setting its own share
accordingly). Next, all parties jointly scan the circuit from its input
wires to the output wires, processing each gate as follows:
• When encountering a gate, the parties already hold shares of
the values of the wires entering the gate, and their aim is to
obtain shares of the value of the wires exiting the gate.
• For a not-gate this is easy: the ﬁrst party just ﬂips the value
of its share, and all other parties maintain their shares.
• Since an and-gate corresponds to multiplication modulo 2,
the parties need to securely compute the following random-
ized functionality (in which the xi’s denote shares of one
entry-wire, the yi’s denote shares of the second entry-wire,
the zi’s denote shares of the exit-wire, and the shares indexed
by i belongs to Party i):
((x1, y1), ..., (xm, ym))
→
(z1, ..., zm)
(7.1)
where m
i=1 zi = (m
i=1 xi) · (m
i=1 yi).
(7.2)
That is, the zi’s are random subject to Eq. (7.2).
Finally, the parties send their shares of each circuit-output wire to the
designated party, which reconstructs the value of the corresponding bit.
Thus, the parties have propagated shares of the input wires into shares
of the output wires, by repeatedly conducting privately-secure compu-
tation of the m-ary functionality of Eq. (7.1) & (7.2). That is, securely
evaluating the entire (arbitrary) circuit “reduces” to securely conduct-
ing a speciﬁc (very simple) multi-party computation. But things get
even simpler: the key observation is that
 m

i=1
xi

·
 m

i=1
yi

=
m

i=1
xiyi +

1≤i<j≤m
(xiyj + xjyi)
(7.3)
Thus, the m-ary functionality of Eq. (7.1) & (7.2) can be computed as
follows (where all arithmetic operations are mod 2):

102
General Cryptographic Protocols
(1) Each Party i locally computes zi,i
def
= xiyi.
(2) Next, each pair of parties (i.e., Parties i and j) securely
compute random shares of xiyj + yixj. That is, Parties i
and j (holding (xi, yi) and (xj, yj), respectively), need to
securely compute the randomized two-party functionality
((xi, yi), (xj, yj)) →(zi,j, zj,i), where the z’s are random sub-
ject to zi,j + zj,i = xiyj + yixj. Equivalently, Party j uni-
formly selects zj,i ∈{0, 1}, and Parties i and j securely com-
pute the deterministic functionality ((xi, yi), (xj, yj, zj,i)) →
(zj,i + xiyj + yixj, λ), where λ denotes the empty string.
The latter simple two-party computation can be securely
implemented using a 1-out-of-4 Oblivious Transfer (cf. (79)
and (67, Sec. 7.3.3)), which in turn can be implemented using
enhanced trapdoor permutations (see below). Loosely speak-
ing, a 1-out-of-k Oblivious Transfer is a protocol enabling one
party to obtain one of k secrets held by another party, with-
out the second party learning which secret was obtained by
the ﬁrst party. That is, we refer to the two-party functionality
(i, (s1, ..., sk)) →(si, λ).
(7.4)
Note that any function f : [k] × {0, 1}∗→{0, 1}∗can
be privately-computed by invoking a 1-out-of-k Oblivious
Transfer on inputs i and (f(1, y), ..., f(k, y)), where i (resp.,
y) is the initial input of the ﬁrst (resp., second) party.
(3) Finally, for every i = 1, ..., m, summing-up all the zi,j’s yields
the desired share of Party i.
The above construction is analogous to a construction that was brieﬂy
described in (74). A detailed description and full proofs appear in (63;
67).
We mention that an analogous construction has been subsequently
used in the private channel model and withstands computationally
unbounded active (resp., passive) adversaries that control less than
one third (resp., a minority) of the parties (25). The basic idea is to
use a more sophisticated secret sharing scheme; speciﬁcally, via a low
degree polynomial (117). That is, the Boolean circuit is viewed as an

7.3. Construction paradigms and two simple protocols
103
arithmetic circuit over a ﬁnite ﬁeld having more than m elements, and
a secret element s of the ﬁeld is shared by selecting uniformly a polyno-
mial of degree d = ⌊(m −1)/3⌋(resp., degree d = ⌊(m −1)/2⌋) having
a free-term equal to s, and handing each party the value of this poly-
nomial evaluated at a diﬀerent (ﬁxed) point (e.g., party i is given the
value at point i). Addition is emulated by (local) point-wise addition
of the (secret sharing) polynomials representing the two inputs (using
the fact that for polynomials p and q, and any ﬁeld element e (and
in particular e = 0, 1, ..., m), it holds that p(e) + q(e) = (p + q)(e)).
The emulation of multiplication is more involved and requires interac-
tion (because the product of polynomials yields a polynomial of higher
degree, and thus the polynomial representing the output cannot be the
product of the polynomials representing the two inputs). Indeed, the
aim of the interaction is to turn the shares of the product polynomial
into shares of a degree d polynomial that has the same free-term as the
product polynomial (which is of degree 2d). This can be done using the
fact that the coeﬃcients of a polynomial are a linear combination of
its values at suﬃciently many arguments (and the other way around),
and the fact that one can privately-compute any linear combination (of
secret values). For details see (25; 61).
A passively-secure 1-out-of-k Oblivious Transfer.
Using a col-
lection of enhanced trapdoor permutations, {fα : Dα →Dα}α∈I,
we outline a passively-secure implementation of the functionality of
Eq. (7.4). The implementation originates in (54) (and a full description
is provided in (67, Sec. 7.3.2)).
Inputs: The sender has input (σ1, σ2, ..., σk) ∈{0, 1}k, the receiver has
input i ∈{1, 2, ..., k}.
Step S1: The sender selects at random a permutation fα along with a
corresponding trapdoor, denoted t, and sends the permutation
fα (i.e., its index α) to the receiver.
Step R1: The
receiver
uniformly
and
independently
selects
x1, ..., xk ∈Dα, sets yi = fα(xi) and yj = xj for every
j ̸= i, and sends (y1, y2, ..., yk) to the sender.

104
General Cryptographic Protocols
Thus, the receiver knows f −1
α (yi) = xi, but cannot predict
b(f −1
α (yj)) for any j ̸= i. Of course, the last assertion presumes
that the receiver follows the protocol (i.e., is semi-honest).
Step S2: Upon receiving (y1, y2, ..., yk), using the inverting-with-
trapdoor algorithm and the trapdoor t, the sender computes
zj = f −1
α (yj), for every j ∈{1, ..., k}. It sends the k-tuple
(σ1 ⊕b(z1), σ2 ⊕b(z2), ..., σk ⊕b(zk)) to the receiver.
Step R2: Upon receiving (c1, c2, ..., ck), the receiver locally outputs
ci ⊕b(xi).
We ﬁrst observe that the above protocol correctly computes 1-out-of-k
Oblivious Transfer; that is, the receiver’s local output (i.e., ci ⊕b(xi))
indeed equals (σi ⊕b(f −1
α (fα(xi)))) ⊕b(xi) = σi. Next, we oﬀer some
intuition as to why the above protocol constitutes a privately-secure
implementation of 1-out-of-k Oblivious Transfer. Intuitively, the sender
gets no information from the execution because, for any possible value
of i, the sender sees the same distribution; speciﬁcally, a sequence of k
uniformly and independently distributed elements of Dα. (Indeed, the
key observation is that applying fα to a uniformly distributed element
of Dα yields a uniformly distributed element of Dα.) Intuitively, the
receiver gains no computational knowledge from the execution because,
for j ̸= i, the only information that the receiver has regarding σj is
the triplet (α, xj, σj ⊕b(f −1
α (xj))), where xj is uniformly distributed
in Dα, and from this information it is infeasible to predict σj better
than by a random guess. The latter intuition presumes that sampling
Dα is trivial (i.e., that there is an easily computable correspondence
between the coins used for sampling and the resulting sample), whereas
in general the coins used for sampling may be hard to compute from the
corresponding outcome (which is the reason that an enhanced hardness
assumption is used in the general analysis of the the above protocol).
(See (67, Sec. 7.3.2) for a detailed proof of security.)
7.3.2
Compilation of passively-secure protocols into actively-
secure ones
We show how to transform any passively-secure protocol into a corre-
sponding actively-secure protocol. The communication model in both

7.3. Construction paradigms and two simple protocols
105
protocols consists of a single broadcast channel. Note that the messages
of the original protocol may be assumed to be sent over a broadcast
channel, because the adversary may see them anyhow (by tapping the
point-to-point channels), and because a broadcast channel is trivially
implementable in the case of passive adversaries. As for the resulting
actively-secure protocol, the broadcast channel it uses can be imple-
mented via an (authenticated) Byzantine Agreement protocol (52; 95),
thus providing an emulation of this model on the standard point-to-
point model (in which a broadcast channel does not exist). We men-
tion that authenticated Byzantine Agreement is typically implemented
using a signature scheme (and assuming that each party knows the
veriﬁcation-key corresponding to each of the other parties).
Turning to the transformation itself, the main idea is to use zero-
knowledge proofs (as described in Section 4.3) in order to force parties
to behave in a way that is consistent with the (passively-secure) pro-
tocol. Actually, we need to conﬁne each party to a unique consistent
behavior (i.e., according to some ﬁxed local input and a sequence of coin
tosses), and to guarantee that a party cannot ﬁx its input (and/or its
coins) in a way that depends on the inputs of honest parties. Thus, some
preliminary steps have to be taken before the step-by-step emulation
of the original protocol may start. Speciﬁcally, the compiled protocol
(which like the original protocol is executed over a broadcast channel)
proceeds as follows:
(1) Committing to the local input: Prior to the emulation of the
original protocol, each party commits to its input (using
a commitment scheme (101)). In addition, using a zero-
knowledge proof-of-knowledge (81; 20; 75), each party also
proves that it knows its own input; that is, that it can
decommit to the commitment it sent. (These zero-knowledge
proofs-of-knowledge are conducted sequentially to prevent
dishonest parties from setting their inputs in a way that
depends on inputs of honest parties; a more round-eﬃcient
method was presented in (45).)
(2) Generation of local random tapes: Next, all parties jointly
generate a sequence of random bits for each party such

106
General Cryptographic Protocols
that only this party knows the outcome of the random
sequence generated for it, but everybody gets a commitment
to this outcome. These sequences will be used as the random-
inputs (i.e., sequence of coin tosses) for the original protocol.
Each bit in the random-sequence generated for Party X is
determined as the exclusive-or of the outcomes of instances
of an (augmented) coin-tossing protocol (cf. (28) and (67,
Sec. 7.4.3.5)) that Party X plays with each of the other par-
ties. The latter protocol provides the other parties with a
commitment to the outcome obtained by Party X.
(3) Eﬀective prevention of premature termination: In addition,
when compiling (the passively-secure protocol to an actively-
secure protocol) for the model that allows the adversary to
control only a minority of the parties, each party shares its
input and random-input with all other parties using a “Ver-
iﬁable Secret Sharing” (VSS) protocol (cf. (43) and (67,
Sec. 7.5.5.1)). Loosely speaking, a VSS protocol allows a
secret to be shared in a way that enables each participant
to verify that the share it got ﬁts the publicly posted infor-
mation, which includes (on top of the commitments posted
in Steps 1 and 2) commitments to all shares. The use of VSS
guarantees that if Party X prematurely suspends the exe-
cution, then the honest parties can together reconstruct all
Party X’s secrets and carry on the execution while playing
its role. This step eﬀectively prevents premature termina-
tion, and is not needed in a model that does not consider
premature termination a breach of security.
(4) Step-by-step emulation of the original protocol: After all the
above steps were completed, we turn to the main step in
which the new protocol emulates the original one. In each
step, each party augments the message determined by the
original protocol with a zero-knowledge proof that asserts
that the message was indeed computed correctly. Recall that
the next message (as determined by the original protocol)
is a function of the sender’s own input, its random-input,
and the messages it has received so far (where the latter are

7.3. Construction paradigms and two simple protocols
107
known to everybody because they were sent over a broadcast
channel). Furthermore, the sender’s input is determined by
its commitment (as sent in Step 1), and its random-input
is similarly determined (in Step 2). Thus, the next message
(as determined by the original protocol) is a function of pub-
licly known strings (i.e., the said commitments as well as the
other messages sent over the broadcast channel). Moreover,
the assertion that the next message was indeed computed
correctly is an NP-assertion, and the sender knows a corre-
sponding NP-witness (i.e., its own input and random-input as
well as the corresponding decommitment information). Thus,
the sender can prove in zero-knowledge (to each of the other
parties) that the message it is sending was indeed computed
according to the original protocol.
The above compilation was ﬁrst outlined in (75; 74). A detailed
description and full proofs appear in (63; 67).
A secure coin-tossing protocol.
Using a commitment scheme
(see Section 4.3), we outline a secure (ordinary as opposed to aug-
mented) coin-tossing protocol, which originates in (28).
Step C1: Party 1 uniformly selects σ ∈{0, 1} and sends Party 2 a
commitment, denoted c, to σ.
Step C2: Party 2 uniformly selects σ′ ∈{0, 1}, and sends σ′ to
Party 1.
Step C3: Party 1 outputs the value σ ⊕σ′, and sends σ along with
the decommitment information, denoted d, to Party 2.
Step C4: Party 2 checks whether or not (σ, d) ﬁt the commitment c it
has obtained in Step 1. It outputs σ ⊕σ′ if the check is satisﬁed
and halts with output ⊥otherwise (indicating that Party 1 has
essentially aborted the protocol prematurely).
Outputs: Party 1 always outputs b def
= σ ⊕σ′, whereas Party 2 either
outputs b or ⊥.
Intuitively, Steps C1–C2 may be viewed as “tossing a coin into the
well”. At this point (i.e., after Step C2) the value of the coin is deter-

108
General Cryptographic Protocols
mined (essentially as a random value), but only one party (i.e., Party 1)
“can see” (i.e., knows) this value. Clearly, if both parties are honest
then they both output the same uniformly chosen bit, recovered in
Steps C3 and C4, respectively. Intuitively, each party can guarantee
that the outcome is uniformly distributed, and Party 1 can cause pre-
mature termination by improper execution of Step 3. Formally, we have
to show how the eﬀect of every real-model adversary can be simulated
by an adequate ideal-model adversary (which is allowed premature ter-
mination). This is done in (67, Sec. 7.4.3.1).
7.4
Concurrent execution of protocols
The deﬁnitions and results surveyed so far refer to a setting in which,
at each time, only a single execution of a cryptographic protocol takes
place (or only one execution may be controlled by the adversary).
In contrast, in many distributed settings (e.g., the Internet), many
executions are taking place concurrently (and several of them may
be controlled by the same adversary). Furthermore, it is undesirable
(and sometimes even impossible) to coordinate these executions (so to
eﬀectively enforce a single-execution setting). Still, the deﬁnitions and
results obtained in the single-execution setting serve as a good starting
point for the study of security in the setting of concurrent executions.
As in the case of stand-alone security, the notion of zero-knowledge
provides a good test case for the study of concurrent security. Indeed,
in order to demonstrate the security issues arising from concurrent
execution of protocols, we consider the concurrent execution of zero-
knowledge protocols. Speciﬁcally, we consider a party P holding a ran-
dom (or rather pseudorandom) function f : {0, 1}2n →{0, 1}n, and
willing to participate in the following protocol (with respect to secu-
rity parameter n).6 The other party, called A for adversary, is supposed
to send P a binary value v ∈{1, 2} specifying which of the following
cases to execute:
6 In fact, assuming that P shares a pseudorandom function f with his friends (as explained
in Section 3.3), the above protocol is an abstraction of a natural “mutual identiﬁcation”
protocol. (The example is adapted from (71).)

7.4. Concurrent execution of protocols
109
For v = 1: Party P uniformly selects α ∈{0, 1}n, and sends it to
A, which is supposed to reply with a pair of n-bit long strings,
denoted (β, γ). Party P checks whether or not f(αβ) = γ. In
case equality holds, P sends A some secret information (e.g.,
the secret-key corresponding to P’s public-key).
For v = 2: Party A is supposed to uniformly select α ∈{0, 1}n, and
sends it to P, which selects uniformly β ∈{0, 1}n, and replies
with the pair (β, f(αβ)).
Observe that P’s strategy (in each case) is zero-knowledge (even w.r.t
auxiliary-inputs as deﬁned in Deﬁnition 4.1): Intuitively, if the adver-
sary A chooses the case v = 1, then it is infeasible for A to guess a
passing pair (β, γ) with respect to a random α selected by P. Thus,
except with negligible probability (when it may get secret informa-
tion), A does not obtain anything from the interaction. On the other
hand, if the adversary A chooses the case v = 2, then it obtains a pair
that is indistinguishable from a uniformly selected pair of n-bit long
strings (because β is selected uniformly by P, and for any α the value
f(αβ) looks random to A). In contrast, if the adversary A can conduct
two concurrent executions with P, then it may learn the desired secret
information: In one session, A sends v = 1 while in the other it sends
v = 2. Upon receiving P’s message, denoted α, in the ﬁrst session,
A sends it as its own message in the second session, obtaining a pair
(β, f(αβ)) from P’s execution of the second session. Now, A sends the
pair (β, f(αβ)) to the ﬁrst session of P, this pair passes the check, and
so A obtains the desired secret.
An attack of the above type is called a relay attack: During such an
attack the adversary just invokes two executions of the protocol and
relays messages between them (without any modiﬁcation). However,
in general, the adversary in a concurrent setting is not restricted to
relay attacks. For example, consider a minor modiﬁcation to the above
protocol so that, in case v = 2, party P replies with (say) the pair
(β, f(αβ)), where α = α⊕1|α|, rather than with (β, f(αβ)). The modi-
ﬁed strategy P is zero-knowledge and it also withstands a relay attack,
but it can be “abused” easily by a more general concurrent attack.

110
General Cryptographic Protocols
The above example is merely the tip of an iceberg, but it suﬃces for
introducing the main lesson: an adversary attacking several concurrent
executions of the same protocol may be able to cause more damage than
by attacking a single execution (or several sequential executions) of the
same protocol. One may say that a protocol is concurrently secure if
whatever the adversary may obtain by invoking and controlling parties
in real concurrent executions of the protocol is also obtainable by a
corresponding adversary that controls corresponding parties making
concurrent functionality calls to a trusted party (in a corresponding
ideal model).7 More generally, one may consider concurrent executions
of many sessions of several protocols, and say that a set of protocols is
concurrently secure if whatever the adversary may obtain by invoking
and controlling such real concurrent executions is also obtainable by
a corresponding adversary that invokes and controls concurrent calls
to a trusted party (in a corresponding ideal model). Consequently, a
protocol is said to be secure with respect to concurrent compositions if
adding this protocol to any set of concurrently secure protocols yields
a set of concurrently secure protocols.
A much more appealing approach was recently suggested by
Canetti (34). Loosely speaking, Canetti suggests to consider a protocol
to be secure (called environmentally-secure (or Universally Compos-
able secure (34))) only if it remains secure when executed within any
(feasible) environment. Following the simulation paradigm, we get the
following deﬁnition:
Deﬁnition 7.2. (environmentally-secure protocols (34) – a rough
sketch): Let f be an m-ary functionality and Π be an m-party pro-
tocol, and consider the following real and ideal models.
7 One speciﬁc concern (in such a concurrent setting) is the ability of the adversary to “non-
trivially correlate the outputs” of concurrent executions. This ability, called malleability,
was ﬁrst investigated by Dolev, Dwork and Naor (50). We comment that providing a
general deﬁnition of what “correlated outputs” means seems very challenging (if at all
possible). Indeed the focus of (50) is on several important special cases such as encryption
and commitment schemes.

7.4. Concurrent execution of protocols
111
In the real model the adversary controls some of the parties in an exe-
cution of Π and all parties can communicate with an arbitrary
probabilistic polynomial-time process, which is called an envi-
ronment (and possibly represents other executions of various
protocols that are taking place concurrently). Honest parties
only communicate with the environment before the execution
starts and when it ends; they merely obtain their inputs from
the environment and pass their outputs to it. In contrast, dis-
honest parties may communicate freely with the environment,
concurrently to the entire execution of Π.
In the ideal model the (simulating) adversary controls the same par-
ties, which use an ideal (trusted-party) that behaves accord-
ing to the functionality f (as in Section 7.1.2). All parties
can communicate with the (same) environment (as in the real
model). Indeed, the dishonest parties may communicate exten-
sively with the environment before and after their single com-
munication with the trusted party.
We say that Π is an environmentally-secure protocol for computing f if
for every probabilistic polynomial-time adversary A in the real model
there exists a probabilistic polynomial-time adversary A′ controlling the
same parties in the ideal model such that no probabilistic polynomial-
time environment can distinguish the case in which it is accessed by
the parties in the real execution from the case it is accessed by parties
in the ideal model.
As hinted above, the environment may account for other executions
of various protocols that are taking place concurrently to the main exe-
cution being considered. The deﬁnition requires that such environments
cannot distinguish the real execution from an ideal one. This means
that anything that the real adversary (i.e., operating in the real model)
gains from the execution and some environment, can be also obtained
by an adversary operating in the ideal model and having access to the
same environment. Indeed, Canetti proves that environmentally-secure
protocols are secure with respect to concurrent compositions (34).
It is known is that environmentally-secure protocols for any func-
tionality can be constructed for settings in which more than two-thirds

112
General Cryptographic Protocols
of the active parties are honest (34). This holds unconditionally for the
private channel model, and under standard assumptions (e.g., allowing
the construction of public-key encryption schemes) for the standard
model (i.e., without private channel). The immediate consequence of
this result is that general environmentally-secure multi-party computa-
tion is possible, provided that more than two-thirds of the parties are
honest.
In contrast, general environmentally-secure two-party computation
is not possible (in the standard sense).8 Still, one can salvage general
environmentally-secure two-party computation in the following reason-
able model: Consider a network that contains servers that are willing
to participate (as “helpers”, possibly for a payment) in computations
initiated by a set of (two or more) users. Now, suppose that two users
wishing to conduct a secure computation can agree on a set of servers so
that each user believes that more than two-thirds of the servers (in this
set) are honest. Then, with the active participation of this set of servers,
the two users can compute any functionality in an environmentally-
secure manner.
Other reasonable models where general environmentally-secure two-
party computation is possible include the common random-string
(CRS) model (41) and variants of the public-key infrastructure (PKI)
model (10). In the CRS model, all parties have access to a univer-
sal random string (of length related to the security parameter). We
stress that the entity trusted to post this universal random string is
not required to take part in any execution of any protocol, and that all
executions of all protocols may use the same universal random string.
The PKI models considered in (10) require that each party deposits a
public-key with a trusted center, while proving knowledge of a corre-
sponding private-key. This proof may be conducted in zero-knowledge
during special epochs in which no other activity takes place.
7.5
Concluding remarks
In Sections 7.1–7.2 we have mentioned a host of deﬁnitions of secu-
rity and constructions for multi-party protocols (especially for the case
8 Of course, some speciﬁc two-party computations do have environmentally-secure protocols.
See (34) for several important examples (e.g., key exchange).

7.5. Concluding remarks
113
of more than two parties). Furthermore, some of these deﬁnitions are
not comparable to others (i.e., they neither imply the others nor are
implies by them), and there seems to be no single deﬁnition that may
be crowned as the central one.
For example, in Sections 7.1.2 and 7.1.3, we have presented two
alternative deﬁnitions of “secure multi-party protocols”, one requiring
an honest majority and the other allowing abort. These deﬁnitions are
incomparable and there is no generic reason to prefer one over the other.
Actually, as mentioned in Section 7.1.2, one could formulate a natural
deﬁnition that implies both deﬁnitions (i.e., waiving the bound on the
number of dishonest parties in Deﬁnition 7.1). Indeed, the resulting
deﬁnition is free of the annoying restrictions that were introduced in
each of the two aforementioned deﬁnitions; the “only” problem with
the resulting deﬁnition is that it cannot be satisﬁed (in general). Thus,
for the ﬁrst time in this primer, we have reached a situation in which a
natural (and general) deﬁnition cannot be satisﬁed, and we are forced
to choose between two weaker alternatives, where each of these alter-
natives carries fundamental disadvantages.
In general, Section 7 carries a stronger ﬂavor of compromise (i.e.,
recognizing inherent limitations and settling for a restricted meaningful
goal) than previous sections. In contrast to the impression given in other
parts of this primer, it is now obvious that we cannot get all that we
may want (see Section 7.4). Instead, we should study the alternatives,
and go for the one that best suits our real needs.
Indeed, as stated in Section 1.1, the fact that we can deﬁne a crypto-
graphic goal does not mean that we can satisfy it as deﬁned. In case
we cannot satisfy the initial deﬁnition, we should search for relaxations
that can be satisﬁed. These relaxations should be deﬁned in a clear
manner so that it would be obvious what they achieve (and what they
fail to achieve). Doing so will allow a sound choice of the relaxation to
be used in a speciﬁc application. This seems to be a good point to end
the current primer.
A good compromise is one in which
the most important interests
of all parties are satisﬁed.

114
General Cryptographic Protocols
Adv. Klara Goldreich-Ingwer (1912–2004)

Acknowledgments
115

116
General Cryptographic Protocols
I wish to thank Minh-Huyen Nguyen for carefully reading this
manuscript and pointing out various diﬃculties and errors. I also wish
to thank Madhu Sudan and an anonymous referee for their comments.

References
[1] “National institute for standards and technology,” 1991.
Digital Signature
Standard (dss). Federal Register Vol. 56, No.169.
[2] W. Aiello and J. H˚astad, “Perfect zero-knowledge languages can be recog-
nized in two rounds,” in 28th IEEE Symposium on Foundations of Computer
Science, pp. 439–448, 1987.
[3] W. Alexi, B. Chor, O. Goldreich, and C. Schnorr, “Rsa/rabin functions: cer-
tain parts are as hard as the whole,” SIAM Journal on Computing, pp. 194–
209, 1988.
[4] S. Arora, C. Lund, R. Motwani, M. Sudan, and M. Szegedy, “Proof veriﬁcation
and intractability of approximation problems,” Journal of the ACM, vol. 17,
pp. 501–555, 1998. Preliminary version in 33rd FOCS, 1992.
[5] S. Arora and S. Safra, “Probabilistic checkable proofs: a new characterization
of np,” Journal of the ACM, vol. 45, pp. 70–122, 1998. Preliminary version in
33rd FOCS, 1992.
[6] L. Babai, L. Fortnow, L. Levin, and M. Szegedy, “Checking computations in
polylogarithmic time,” in 23rd ACM Symposium on the Theory of Computing,
pp. 21–31, 1991.
[7] L. Babai, L. Fortnow, N. Nisan, and A. Wigderson, “BPP has subexponential
time simulations unless exptime has publishable proofs,” Complexity Theory,
vol. 3, pp. 307–318, 1993.
[8] B. Barak, “How to go beyond the black-box simulation barrier,” in 42nd IEEE
Symposium on Foundations of Computer Science, pp. 106–115, 2001.
[9] B. Barak, “Constant-round coin-tossing with a man in the middle or realizing
the shared random string model,” in 43th IEEE Symposium on Foundations
of Computer Science, pp. 345–355, 2002.
117

118
References
[10] B. Barak, R. Canetti, and J. Nielsen, “Universally composable protocols with
relaxed set-up assumptions,” in 45th IEEE Symposium on Foundations of
Computer Science, pp. 186–195, 2004.
[11] B. Barak and O. Goldreich, “17th ieee conference on computational complex-
ity,” in Universal arguments and their applications, pp. 194–203, 2002.
[12] B. Barak and Y. Lindell, “Strict polynomial-time in simulation and extrac-
tion,” SIAM Journal on Computing, vol. 33(4), pp. 783–818, 2004.
[13] D. Beaver, Foundations of secure interactive computing. Vol. 576, Springer-
Verlag, 1991. Crypto91, Lecture Notes in Computer Science.
[14] D. Beaver, “Secure multi-party protocols and zero-knowledge proof systems
tolerating a faulty minority,” Journal of Cryptology, vol. 4, pp. 75–122, 1991.
[15] D. Beaver, S. Micali, and P. Rogaway, “The round complexity of secure pro-
tocols,” in 22nd ACM Symposium on the Theory of Computing, pp. 503–513,
1990. See details in (113).
[16] M. Bellare, “Electronic commerce and electronic payments,” Webpage of a
course. http://www-cse.ucsd.edu/users/mihir/cse291-00/.
[17] M. Bellare, R. Canetti, and R. Krawczyk, Keying hash functions for mes-
sage authentication. Vol. 1109, Springer, 1996. Crypto96 Lecture Notes in
Computer Science.
[18] M. Bellare, R. Canetti, and R. Krawczyk, “A modular approach to the design
and analysis of authentication and key-exchange protocols,” in 30th ACM
Symposium on the Theory of Computing, pp. 419–428, 1998.
[19] M. Bellare, A. Desai, D. Pointcheval, and P. Rogaway, Relations among
notions of security for public-key encryption schemes.
Vol. 1462, Springer,
1998. Crypto98 Lecture Notes in Computer Science.
[20] M. Bellare and O. Goldreich, On deﬁning proofs of knowledge.
Vol. 740,
Springer-Verlag, 1992. Crypto92 Lecture Notes in Computer Science.
[21] M. Bellare, R. Impagliazzo, and M. Naor, “Does parallel repetition lower
the error in computationally sound protocols?,” in 38th IEEE Symposium
on Foundations of Computer Science, pp. 374–383, 1997.
[22] M. Bellare and P. Rogaway, “Random oracles are practical: a paradigm for
designing eﬃcient protocols,” in 1st Conf. on Computer and Communications
Security, pp. 62–73, 1993.
[23] M. Ben-Or, R. Canetti, and O. Goldreich, “Asynchronous secure computa-
tion,” in 25th ACM Symposium on the Theory of Computing. See details in
(35).
[24] M. Ben-Or, O. Goldreich, S. Goldwasser, J. H˚astad, J. Kilian, S. Micali,
and P. Rogaway, Everything provable is probable in zero-knowledge. Vol. 403,
Springer-Verlag, 1990. Crypto88 Lecture Notes in Computer Science.
[25] M. Ben-Or, S. Goldwasser, and A. Wigderson, “Completeness theorems
for non-cryptographic fault-tolerant distributed computation,” in 20th ACM
Symposium on the Theory of Computing, pp. 1–10, 1988.
[26] M. Ben-Or, B. Kelmer, and T. Rabin, “Asynchronous secure computations
with optimal resilience,” in 13th ACM Symposium on Principles of Distributed
Computing, pp. 183–192, 1994.

References
119
[27] C. Bennett, G. Brassard, and J. Robert, “Privacy ampliﬁcation by public
discussion,” SIAM Journal on Computing, vol. 17, pp. 210–229, 1998. Prelim-
inary version in Crypto85, titled “How to reduce your enemy’s information”.
[28] M. Blum, “Coin ﬂipping by phone,” IEEE Sprig COMPCOM, pp. 133–137,
1982. See also SIGACT News, Vol. 15, No. 1, 1983.
[29] M. Blum, B. Feldman, and T. Micali, “Non-interactive zero-knowledge proof
systems,” in 20th ACM Symposium on Principles of Distributed Computing,
pp. 103–112, 1988. See (32).
[30] M. Blum and S. Goldwasser, An eﬃcient probabilistic public-key encryption
scheme which hides all partial information. Vol. 196, Springer-Verlag, 1985.
Crypto84 Lecture Notes in Computer Science.
[31] M. Blum and S. Micali, “How to generate cryptographically strong sequences
of pseudo-random bits,” SIAM Journal on Computing, vol. 13, pp. 850–864,
1984. Preliminary version in 23rd FOCS, 1982.
[32] M. Blum, A. D. Santis, S. Micali, and G. Persiano, “Non-interactive zero-
knowledge proof systems,” SIAM Journal on Computing, vol. 20(6), pp. 1084–
1118, 1991. (Considered the journal version of (29).
[33] G. Brassard, D. Chaum, and C. Cr´epeau, “Minimum disclosure proofs of
knowledge,” Journal of Computer and System Science, vol. 37(2), pp. 156–
189, 1988.
Preliminary version by Brassard and Cr´epeau in 27th FOCS,
1986.
[34] R. Canetti, “Universally composable security: a new paradigm for crypto-
graphic protocols,” in 42nd IEEE Symposium on Foundations of Computer
Science, pp. 136–145. Full version (with diﬀerent title) is available from Cryp-
tology ePrint Archive, Report 2000/067.
[35] R. Canetti, Studies in secure multi-party computation and applications. PhD
thesis, Weizmann Institute of Science, Rehovot, Israel, June 1995. Available
from http://www.wisdom.weizmann.ac.il/ oded/PS/ran-phd.ps.
[36] R. Canetti, “Security and composition of multi-party cryptographic proto-
cols,” Journal of Cryptology, vol. 13(1), pp. 143–202, 2000.
[37] R. Canetti, U. Feige, O. Goldreich, and M. Naor, “Adaptively secure multi-
party computation,” in 28th ACM Symposium on the Theory of Computing,
pp. 639–648, 1996.
[38] R. Canetti, O. Goldreich, and S. Halevi, “The random oracle methodology,
revisited,” in 30th ACM Symposium on the Theory of Computing, pp. 209–218,
1998.
[39] R. Canetti and A. Herzberg, Maintaining security in the presence of transient
faults. Vol. 839, Springer-Verlag, 1994. Crypto94 Lecture Notes in Computer
Science.
[40] R. Canetti, J. Kilian, E. Petrank, and A. Rosen, “Black-box concurrent zero-
knowledge requires ˜Ω(log n) rounds,” in 33rd ACM Symposium on the Theory
of Computing, pp. 494–503, 2002.
[41] R. Canetti, Y. Lindell, R. Ostrovsky, and A. Sahai, “Universally composable
two-party and multi-party secure computation,” in 34th ACM Symposium on
the Theory of Computing, pp. 494–503, 2002.

120
References
[42] D. Chaum, C. Cr´epeau, and I. Damg˚ard, “Multi-party unconditionally secure
protocols,” in 20th ACM Symposium on Principles of Distributed Computing,
pp. 260–268, 1987.
[43] B. Chor, S. Goldwasser, S. Micali, and B. Awerbuch, “Veriﬁable secret sharing
and achieving simultaneity in the presence of faults,” in 20th ACM Symposium
on the Theory of Computing, pp. 11–19, 1988.
[44] B. Chor and E. Kushilevitz, “A zero-one law for boolean privacy,” SIAM
Journal of Discrete Mathematics, vol. 4, pp. 36–47, 1991.
[45] B. Chor and M. Rabin, “Achieving independence in logarithmic number of
rounds,” in 6th ACM Symposium on Principles of Distributed Computing,
pp. 260–268, 1987.
[46] R. Cleve, “Limits on the security of coin ﬂips when half the processors are
faulty,” in 18th ACM Symposium on the Theory of Computing, pp. 364–369,
1986.
[47] I. Damg˚ard, Collision free hash functions and public key signature schemes.
Vol. 304, Springer-Verlag, 1988. EuroCryp87 Lecture Notes in Computer Sci-
ence.
[48] I. Damgard and J. Nielsen, Improved non-committing encryption schemes
based on general complexity assumption.
Vol. 1880, Springer-Verlag, 2000.
Crypto00 Lecture Notes in Computer Science.
[49] W. Diﬃe and M. Hellmann, “New directions in cryptography,” IEEE Trans.
on Info. Theory, pp. 644–654, 1976. IT-22.
[50] D. Dolev, C. Dwork, and M. Naor, “Non-malleable cryptography,” SIAM Jour-
nal on Computing, vol. 30, no. 2, pp. 391–437, 2000. Preliminary version in
23rd STOC, 1991.
[51] D. Dolev, C. Dwork, O. Waarts, and M. Yung, “Perfectly secure message
transmission,” Journal of the ACM, vol. 40(1), pp. 17–47, 1993.
[52] D. Dolev and H. Strong, “Authenticated algorithms for byzantine agreement,”
SIAM Journal on Computing, vol. 12, pp. 656–666, 1983.
[53] C. Dwork, M. Naor, and A. Sahai, “Concurrent zero-knowledge,” in 30th ACM
Symposium on the Theory of Computing, pp. 409–418, 1998.
[54] S. Even, O. Goldreich, and A. Lempel, “A randomized protocol for signing
contracts,” Communications of the ACM, vol. 28, no. 6, pp. 637–647, 1985.
[55] U. Feige, S. Goldwasser, L. Lov´asz, S. Safra, and M. Szegedy, “Approximating
clique is almost np-complete,” Journal of the ACM, vol. 43, pp. 268–292, 1996.
Preliminary version in 32nd FOCS, 1991.
[56] U. Feige, D. Lapidot, and A. Shamir, “Multiple non-interactive zero-knowledge
proofs under general assumptions,” SIAM Journal on Computing, vol. 29(1),
pp. 1–28, 1999.
[57] U. Feige and A. Shamir, “Witness indistinguishability and witness hiding pro-
tocols,” in 22nd ACM Symposium on the Theory of Computing, pp. 416–426,
1990.
[58] A. Fiat and A. Shamir, How to prove yourself: practical solution to identi-
ﬁcation and signature problems. Vol. 263, Springer-Verlag, 1987. Crypto86
Lecture Notes in Computer Science.

References
121
[59] L. Fortnow, “The complexity of perfect zero-knowledge,” in 19th ACM Sym-
posium on the Theory of Computing, pp. 204–209, 1987.
[60] P. Gemmell, An introduction to threshold cryptography. Vol. 2(3), RSA Lab,
1997. CryptoBytes.
[61] R. Gennaro, M. Rabin, and T. Rabin, “Simpliﬁed vss and fast-track mul-
tiparty computations with applications to threshold cryptography,” in 17th
ACM Symposium on Principles of Distributed Computing, pp. 101–112, 1998.
[62] O. Goldreich, Modern Cryptography, Probabilistic Proofs and Pseudoran-
domness. Vol. 17 of Algorithms and Combinatorics series, Springer, 1998.
[63] O. Goldreich, “Secure multi-party computation,” 1998. Working Draft, Avail-
able from http://www.wisdom.weizmann.ac.il/ oded/pp.html.
[64] O. Goldreich, “A uniform complexity treatment of encryption and zero-
knowledge,” Journal of Cryptology, vol. 6(1), pp. 21–53, 1998.
[65] O. Goldreich, Foundations of Cryptography – Basic Tools. Cambridge Uni-
versity Press, 2001.
[66] O. Goldreich, “Concurrent zero-knowledge with timing, revisited,” in 34th
ACM Symposium on the Theory of Computing, pp. 332–340, 2002.
[67] O. Goldreich, Foundations of Cryptography – Basic Applications. Cambridge
University Press, 2004.
[68] O. Goldreich, S. Goldwasser, and S. Micali, “How to construct random func-
tions,” Journal of the ACM, vol. 33(4), pp. 792–807, 1986.
[69] O. Goldreich and J. H˚astad, “On the complexity of interactive proofs with
bounded communication,” IPL, vol. 67(4), pp. 205–214, 1998.
[70] O. Goldreich and A. Kahan, “How to construct constant-round zero-knowledge
proof systems for np,” Journal of Cryptology, vol. 9(2), pp. 167–189, 1996.
[71] O. Goldreich and H. Krawczyk, “On the composition of zero-knowledge proof
systems,” SIAM Journal on Computing, vol. 25(1), pp. 169–192, 1996.
[72] O. Goldreich and L. Levin, “Hard-core predicates for any one-way function,”
in 21st ACM Symposium on the Theory of Computing, pp. 25–32, 1989.
[73] O. Goldreich and L. Levin, Fair computation of general functions in presence
of immoral majority. Vol. 537, Springer-Verlag, 1991. Crypto90 Lecture Notes
in Computer Science.
[74] O. Goldreich, S. Micali, and A. Wigderson, “How to play any mental game
– a completeness theorem for protocols with honest majority,” in 19th ACM
Symposium on the Theory of Computing, pp. 218–229, 1987. See details in
(63).
[75] O. Goldreich, S. Micali, and A. Wigderson, “Proofs that yield nothing but
their validity or all languages in np have zero-knowledge proof systems,” Jour-
nal of the ACM, vol. 38(1), pp. 691–729, 1991. Preliminary version in 27th
FOCS, 1986.
[76] O. Goldreich and Y. Oren, “Deﬁnitions and properties of zero-knowledge proof
systems,” Journal of Cryptology, vol. 7(1), pp. 1–32, 1994.
[77] O. Goldreich, A. Sahai, and S. Vadhan, “Honest-veriﬁer statistical zero-
knowledge equals general statistical zero-knowledge,” in 30th ACM Sympo-
sium on the Theory of Computing, pp. 399–408, 1998.

122
References
[78] O. Goldreich, S. Vadhan, and A. Wigderson, “On interactive proofs with a
laconic provers,” Computational Complexity, vol. 11, pp. 1–53, 2002.
[79] O. Goldreich and R. Vainish, How to solve any protocol problem – an eﬃciency
improvement.
Vol. 293, Springer-Verlag, 1988.
Crypto87 Lecture Notes in
Computer Science.
[80] S. Goldwasser and S. Micali, “Probabilistic encryption,” Journal of Computer
and System Science, vol. 28(2), pp. 270–299, 1984. Preliminary version in 14th
STOC, 1982.
[81] S. Goldwasser, S. Micali, and C. Rackoﬀ, “The knowledge complexity of inter-
active proof systems,” SIAM Journal on Computing, vol. 18, pp. 186–208,
1989. Preliminary version in 17th STOC, 1985.
[82] S. Goldwasser, S. Micali, and R. Rivest, “A digital signature scheme secure
against adaptive chosen-message attacks,” SIAM Journal on Computing,
pp. 281–308, 1988.
[83] S. Golomb, Shift Register Sequences. Aegean Park Press, revised edition ed.,
1982. Holden-Dat, 1967.
[84] R.
Greenstadt,
“Electronic
voting
bibliography,”
2000.
http://theory.lcs.mit.edu/ cis/voting/greenstadt-voting-bibligraphy.html.
[85] J. H˚astad, R. Impagliazzo, L. Levin, and M. Luby, “A pseudorandom gener-
ator from any one-way function,” SIAM Journal on Computing, vol. 28(4),
pp. 1364–1396, 1999.
[86] M. Hirt and U. Maurer, “Complete characterization of adversaries tolerable in
secure multi-party computation,” Journal of Cryptology, vol. 13(1), pp. 31–60,
2000.
[87] R. Impagliazzo, L. Levin, and M. Luby, “Pseudorandom generation from one-
way functions,” in 21st ACM Symposium on the Theory of Computing, pp. 12–
24, 1989.
[88] R. Impagliazzo and M. Yung, Direct zero-knowledge computations. Vol. 293,
Springer-Verlag, 1987. Crypto87 Lecture Notes in Computer Science.
[89] J. Katz and M. Yung, “Complete characterization of security notions for prob-
abilistic private-key encryption,” in 32nd ACM Symposium on the Theory of
Computing, pp. 245–254, 2000.
[90] J. Kilian, “A note on eﬃcient zero-knowledge proofs and arguments,” in 24th
ACM Symposium on the Theory of Computing, pp. 723–732, 1992.
[91] J. Kilian and E. Petrank, “Concurrent and resettable zero-knowledge in poly-
logarithmic rounds,” in 33rd ACM Symposium on the Theory of Computing,
pp. 560–569, 2001.
[92] D. Knuth, The Art of Computer Programming. Vol. 2, Addison-Wesley Pub-
lishing Company Inc, ﬁrst edition ed., 1969.
[93] H. Krawczyk, LFSR-based hashing and authentication.
Vol. 839, Springer-
Verlag, 1994. Crypto94 Lecture Notes in Computer Science.
[94] Y. Lindell, Parallel coin-tossing and constant-round secure two-party compu-
tation. Vol. 2139, Springer-Verlag, 2001. Crypto01 Lecture Notes in Computer
Science.
[95] Y. Lindell, A. Lysyanskaya, and T. Rabin, “On the composition of authen-
ticated byzantine agreement,” in 34th ACM Symposium on the Theory of
Computing, pp. 514–523, 2002.

References
123
[96] C. Lund, L. Fortnow, A. Karloﬀ, and N. Nisan, “Algebraic methods for inter-
active proof systems,” Journal of the ACM, vol. 39(4), pp. 859–868, 1992.
Preliminary version in 31st FOCS, 1990.
[97] A. Menezes, P. van Oorschot, and S. Vanstone, Handbook of Applied Crypto-
graphy. CRC Press, 1996.
[98] R. Merkle, “Protocols for public key cryptosystems,” in Proc. of the 1980
Symposium on Security and Privacy, 1980.
[99] S. Micali, “Computationally sound proofs,” SIAM Journal on Computing,
vol. 30(4), pp. 1253–1298, 2000. Preliminary version in 35th FOCS, 1994.
[100] S. Micali and P. Rogaway, Secure computation.
Vol. 576, Springer-Verlag,
1991. Crypto91 Lecture Notes in Computer Science. Elaborated working draft
available from the authors.
[101] M. Naor, “Bit commitment using pseudorandom generators,” Journal of Cryp-
tology, vol. 4, pp. 151–158, 1991.
[102] M. Naor and K. Nissin, “Communication preserving protocols for secure func-
tion evaluation,” in 33rd ACM Symposium on the Theory of Computing,
pp. 590–599, 2001.
[103] M. Naor and M. Yung, “Universal one-way hash functions and their crypto-
graphic application,” in 21st ACM Symposium on the Theory of Computing,
pp. 33–43, 1989.
[104] M. Naor and M. Yung, “Public-key cryptosystems provably secure against
chosen ciphertext attacks,” in 22nd ACM Symposium on the Theory of Com-
puting, pp. 427–437, 1990.
[105] R. Ostrovsky and A. Wigderson, “One-way functions are essential for non-
trivial zero-knowledge,” in 2nd Israel Symp. on Theory of Computing and
Systems, pp. 3–17, 1993. IEEE Comp. Soc. Press.
[106] R. Ostrovsky and M. Yung, “how to withstand mobile virus attacks,” in 10th
ACM Symposium on Principles of Distributed Computing, pp. 51–59, 1991.
[107] M. Prabhakaran, A. Rosen, and A. Sahai, “Concurrent zero-knowledge proofs
in logarithmic number of rounds,” in 43rd IEEE Symposium on Foundations
of Computer Science, pp. 366–375, 2002.
[108] M. Rabin.
Academic Press, 1977.
Foundations of Secure Computation
(R.A. DeMillo et al, eds).
[109] M. Rabin, “Digitalized signatures and public key functions as intractable as
factoring,” 1979. MIT/LCS/TR-212.
[110] T. Rabin and M. Ben-Or, “Veriﬁable secret sharing and multi-party protocols
with honest majority,” in 21st ACM Symposium on the Theory of Computing,
pp. 73–85, 1989.
[111] R. Richardson and J. Kilian, On the concurrent composition of zero-knowledge
proofs. Vol. 1592, Springer, 1999. EuroCrypt99 Lecture Notes in Computer
Science.
[112] R. Rivest, A. Shamir, and L. Adleman, “A method for obtaining digital sig-
natures and public key cryptosystems,” Communications of the ACM, vol. 21,
pp. 120–126, 1978.
[113] P. Rogaway, The round complexity of secure protocols. PhD thesis, MIT, 1991.
Available from http://www.cs.ucdavis.edu/∼rogaway/papers.

124
References
[114] J. Rompel, “One-way functions are necessary and suﬃcient for secure signa-
tures,” in 22nd ACM Symposium on the Theory of Computing, pp. 387–394,
1990.
[115] A. Sahai and S. Vadhan, “A complete promise problem for statistical zero-
knowledge,” Journal of the ACM, vol. 50(2), pp. 1–54, 2003.
[116] A. D. Santis, G. D. Crescenzo, R. Ostrovsky, G. Persiano, and A. Sahai, Robust
non-interactive zero-knowledge. Vol. 2139, Springer-Verlag, 2001. Crypto01
Lecture Notes in Computer Science.
[117] A. Shamir, “How to share a secret,” Journal of the ACM, vol. 22, pp. 612–613,
1979.
[118] A. Shamir, “Ip =pspace,” Journal of the ACM, vol. 39(4), pp. 869–877, 1992.
Preliminary version in 31st FOCS, 1990.
[119] C. Shannon, “Communication theory of secrecy systems,” Bell System Tech-
nical Journal, vol. 28, pp. 656–715, 1983.
[120] M. Sipser, “A complexity theoretic approach to randomness,” in 15th ACM
Symposium on the Theory of Computing, pp. 330–335, 1983.
[121] S.
Vadhan,
A
Study
of
Statistical
Zero-Knowledge
Proofs.
PhD
thesis,
Department
of
Mathematics,
MIT,
1999.
Available
from
http://www.eecs.harvard.edu/ salil/papers/phdthesis-abs.html.
[122] S. Vadhan, “An unconditional study of computational zero knowledge,” in
45th IEEE Symposium on Foundations of Computer Science, pp. 176–185,
2004.
[123] A. Yao, “Theory and application of trapdoor functions,” in 23rd IEEE Sym-
posium on Foundations of Computer Science, pp. 80–91, 1982.
[124] A. Yao, “How to generate and exchange secrets,” in 27th IEEE Symposium
on Foundations of Computer Science, pp. 162–167, 1986.


DIGITAL SIGNAL AND IMAGE PROCESSING SERIES 


Digital Signal and Image Processing using MATLAB® 


Revised and Updated 2nd Edition 
Digital Signal and Image 
Processing using MATLAB® 
iSlE 
Volume 1 
Fundamentals 
Gerard Blanchet 
Maurice Charbit 
WILEY 

First edition published 2006 in Great Britain and the United States by ISTE Ltd and John Wiley & Sons, 
Inc. © ISTE Ltd 2006 
This edition published 2014 in Great Britain and the United States by ISTE Ltd and John Wiley & Sons, 
Inc. 
Apart trom any fair dealing for the purposes of research or private study, or criticism or review, as 
permitted under the Copyright, Designs and Patents Act 1988, this publication may only be reproduced, 
stored or transmitted, in any form or by any means, with the prior permission in writing ofthe publishers, 
or in the case of reprographic reproduction in accordance with the terms and licenses issued by the 
CLA. Enquiries concerning reproduction outside these terms should be sent to the publishers at the 
undermentioned address: 
ISTE Ltd 
27-37 St George's Road 
London SW19 4EU 
UK 
www.iste.co.uk 
© ISTE Ltd 2014 
John Wiley & Sons, Inc. 
III River Street 
Hoboken, NJ 07030 
USA 
www.wiley.com 
The rights of Gerard Blanchet and Maurice Charbit to be identified as the authors of this work have been 
asserted by them in accordance with the Copyright, Designs and Patents Act 1988. 
Library of Congress Control Number: 2014942418 
British Library Cataloguing-in-Publication Data 
A CIP record for this book is available trom the British Library 
ISBN 978-1-84821-640-2 
MA TLAB®is a trademark of The Math Works, Inc. and is used with permission. The MathWorks does not 
warrant the accuracy of the text or exercises in this book. This book's use or discussion of MA TLAB® 
software does not constitute endorsement or sponsorship by The Math Works of a particular pedagogical 
approach or use of the MA TLAB® software. 
J;;S 
FSC 
www.fsc.org 
MIX 
Paper from 
responsible sources 
FSC" C013604 
Printed and bound in Great Britain by CPI Group (UK) Ltd., Croydon, Surrey CRO 4YY 

Contents 
Foreword 
Notations and Abbreviations 
Introduction to MATLAB 
1 
Variables and constants . 
2 
3 
4 
5 
6 
7 
1.1 
Vectors and matrices. 
1.2 
Predefined matrices . 
1.3 
1.4 
1.5 
Constants and initialization 
Multidimensional arrays 
Cells and structures 
Operations and functions . 
2.1 
Matrix operations . 
2.2 
Pointwise operations 
2.3 
Mathematical functions 
2.4 
Matrix functions . . . . 
2.5 
2.6 
Searching elements using min, max, find, etc. functions 
Other useful functions . . . . . . . . . 
Programming structures . . . . . . . . . . . . 
3.1 
Logical operators on boolean variables 
3.2 
Program loops ... . 
3.3 
Functions... .. . . 
Graphically displaying results 
4.1 
2D display. . . . . . . 
4.2 
3D display ...... . 
4.3 
Notes on plotting a curve 
4.4 
Animations.. .. .... 
Converting numbers to character strings 
Input/ output . .. . ... . .. . .. . . 
Program writing . . ... . .. . .. . . 
7.1 
Developing and testing performances . 
7.2 
Various functions ... 
7.3 
Using other languages ........ . 
11 
15 
19 
22 
22 
25 
26 
26 
27 
29 
29 
31 
32 
34 
34 
36 
37 
37 
38 
39 
39 
39 
43 
44 
45 
47 
47 
48 
48 
49 
51 

6 Digital Signal and Image Processing using MATLAB® 
Part I Deterministic Signals 
Chapter 1 
Signal Fundamentals 
1.1 
The concept of signal. . . . . . . . . . . 
1.1.1 
A few signals . . . . . . . . . . . 
1.1.2 
Spectral representation of signals 
1.2 
The concept of system 
1.3 
Summary .. . .. .. .. .... .. . . 
Chapter 2 
Discrete Time Signals and Sampling 
2.1 
Fundamentals of sampling .. 
2.1.1 
The Poisson formula ... . 
2.1.2 
Perfect reconstruction .. . 
2.1.3 
Digital-to-analog conversion 
2.2 
Plotting a signal as a function of time 
2.3 
Spectral representation ........ . 
2.3.1 
Discrete-time Fourier transform (DTFT) . 
2.3.2 
Discrete Fourier transform (DFT) 
2.3.3 
Fast Fourier transform . . . . . . . 
Chapter 3 
Spectral Observation 
3.1 
Spectral accuracy and resolution . . . . . . . 
3.1.1 
Observation of a complex exponential 
3.1.2 
Plotting accuracy of the DTFT ... . 
3.1.3 
Frequency resolution ......... . 
3.1.4 
Effects of windowing on the resolution 
3.2 
Short term Fourier transform .. . 
3.3 
Summing up . .... ...... . 
3.4 
Application examples and exercises 
3.4.1 
Amplitude modulations 
3.4.2 
Frequency modulation 
Chapter 4 
Linear Filters 
4.1 
Definitions and properties 
4.2 
The z-transform ..... 
4.2.1 
Definition and properties 
4.2.2 
A few examples ..... . 
4.3 
Transforms and linear filtering 
. 
4.4 
Difference equations and rational TF filters 
4.4.1 
Stability considerations ...... . 
4.4.2 
FIR and IIR filters ......... . 
4.4.3 
Causal solution and initial conditions 
4.4.4 
Calculating the responses . 
4.4.5 
Stability and the Jury test .. .. .. . 
53 
55 
55 
56 
57 
60 
62 
65 
66 
66 
68 
79 
80 
82 
82 
86 
91 
95 
95 
95 
98 
98 
101 
104 
108 
110 
110 
112 
115 
115 
120 
121 
122 
123 
125 
128 
129 
130 
133 
134 

4.5 
Connection between gain and poles/zeros 
4.6 
Minimum phase filters .... 
4.6.1 
All-pass filters 
Contents 7 
135 
144 
145 
4.6.2 
Minimum phase filters 
146 
4.7 
Filter design methods .... 
149 
4.7.1 
Going from the continuous-time filter to the discrete-time 
filter . . . . . . . . . . . . . . . . . . . . . . 
149 
4.7.2 
FIR filter design using the window method 
4.7.3 
IIR filter design ..... . 
4.8 
Oversampling and undersampling 
4.8.1 
Oversampling. 
4.8.2 
Undersampling ... .. . 
Chapter 5 
An Introduction to Image Processing 
5.1 
Introduction . . .. . ....... . 
5.1.1 
Image display, color palette . . .. . 
153 
164 
167 
167 
171 
175 
175 
175 
5.1.2 
Importing images. . . . . . . . . . . 
179 
5.1.3 
Arithmetical and logical operations. 
181 
5.2 
Color spaces. . . . . 
183 
5.2.1 
RGB coding. . 
187 
5.2.2 
HSV coding . . 
188 
5.2.3 
CMYK coding 
189 
5.2.4 
How to extract the RGB information from an image 
191 
5.2.5 
Converting from color to grayscale 
191 
5.3 
Geometric transformations of an image. 
192 
5.3.1 
The typical transformations 
192 
5.3.2 
Image registration ... 
5.4 Frequential content of an image 
5.5 
Linear filtering . . . . . . . 
5.6 
Other operations on images 
5.6.1 
Undersampling .. 
5.6.2 
Oversampling ... 
5.6.3 
Contour detection 
5.6.4 
Median filtering .. 
5.6.5 
Image binarization 
5.6.6 
Modifying the contrast of an image. 
5.6.7 
Morphological filtering of binary images 
5.7 JPEG lossy compression ......... . 
5.7.1 
Basic algorithm ........ . .. . 
5.7.2 
Writing the compression function .. 
5.7.3 
Writing the decompression function 
195 
198 
204 
213 
213 
215 
217 
221 
222 
227 
231 
233 
234 
235 
238 

8 Digital Signal and Image Processing using MATLAB® 
Part II Random Signals 
241 
Chapter 6 
Random Variables 
243 
6.1 
Random phenomena in signal processing 
243 
6.2 
Basic concepts of random variables . . . 
244 
6.3 
Common probability distributions 
253 
6.3.1 
Uniform probability distribution on (a, b) 
253 
6.3.2 
Real Gaussian random variable . . . . . . 
254 
6.3.3 
Complex Gaussian random variable 
255 
6.3.4 
Generating the common probability distributions 
256 
6.3.5 
Estimating the probability density 
259 
6.3.6 
Gaussian random vectors .. .. 
260 
6.4 
Generating an r.v. with any type of p.d. 
262 
6.5 
Uniform quantization. . . . . . . . . . . 
268 
Chapter 7 
Random Processes 
7.1 
Introduction ........ . 
7.2 
Wide-sense stationary processes ........... . 
7.2.1 
Definitions and properties of WSS processes. 
7.2.2 
Spectral representation of a WSS process 
7.2.3 
Sampling a WSS process ....... . 
7.3 
Estimating the covariance .......... . 
7.4 
Filtering formulae for WSS random processes 
7.5 
MA, AR and ARMA time series ...... . 
7.5.1 
Q order MA (Moving Average) process. 
7.5.2 
P order AR (AutoRegressive) Process 
7.5.3 
ARMA (P, Q) process ........ . 
Chapter 8 
Spectra Estimation 
8.1 
Non-parametric estimation of the psd ...... . . 
8.l.1 
Estimation from the auto covariance function 
8.l.2 
Estimation based on the periodogram 
8.2 
AR estimation .. . ...... . .. . .... .. . 
8.2.1 
AR parameters . . ... . .. . .. . ... . 
8.2.2 
Estimating the spectrum of an AR process 
8.3 
Estimating the amplitudes and the frequencies . 
8.3.1 
The case of a single complex exponential. 
8.3.2 
Real harmonic mixtures . .. . 
8.3.3 
Complex harmonic mixtures . . 
8.4 
Periodograms and the resolution limit 
Chapter 9 
The Least Squares Method 
9.1 
The projection theorem . 
9.2 
The least squares method ... . . . . 
271 
271 
272 
273 
276 
285 
288 
296 
302 
302 
304 
311 
313 
313 
313 
317 
325 
325 
329 
330 
330 
332 
334 
336 
349 
349 
353 

9.2.1 
Formulating the problem 
9.2.2 
The linear model . ... . 
9.2.3 
The least squares estimator 
9.2.4 
Identifying the impulse response of a channel 
9.3 
Linear predictions of the WSS processes .. 
9.3.1 
Yule-Walker equations . . .. . .. . 
9.3.2 
Predicting a WSS harmonic process 
9.3.3 
Predicting a causal AR-P process. 
9.4 Wiener filtering .. .. . .. .. . .. . . 
9.4.1 
Finite impulse response solution 
9.4.2 
Gradient algorithm ... . .. . . 
9.4.3 
Wiener equalization ...... . 
9.5 
The LMS (least mean square) algorithm 
9.5.1 
The constant step algorithm .. 
9.5.2 
The normalized LMS algorithm 
9.5.3 
Echo canceling . . . . . . .. . 
Part III Appendices 
Chapter 10 Hints and Solutions 
H1 
Signal fundamentals . . . . . 
H2 
Discrete time signals and sampling 
H3 
Spectral observation . . . . . . . . 
H4 
Linear filters . .. . .. . ... . . 
H5 
An Introduction to image processing 
H6 
Random variables . 
H7 
Random processes . ... 
H8 
Spectra estimation . . . . 
H9 
The least squares method 
Chapter 11 Appendix 
A1 
Fourier transform . . . . . . . . 
A2 
Discrete time Fourier transform 
A3 
Discrete Fourier transform . 
A4 
z-Transform ........ . 
Bibliography 
Index 
Contents 9 
353 
354 
355 
360 
362 
362 
364 
365 
366 
368 
369 
377 
379 
379 
388 
391 
397 
399 
399 
399 
405 
415 
437 
460 
466 
472 
475 
479 
479 
480 
481 
482 
485 
489 


Foreword 
Simulation is an essential tool in any field related to engineering techniques, 
whether it is used for teaching purposes or in research and development. 
When teaching technical subjects, lab works play an important role, as im-
portant as exercise sessions in helping students assimilate theory. The recent 
introduction of simulation tools has created a new way to work, halfway be-
tween exercise sessions and lab works. This is particularly the case for digital 
signal processing, for which the use of the MATLAB® language, or its clones, 
has become inevitable. Easy to learn and to use, it makes it possible to quickly 
illustrate a concept after introducing it in a course. 
As for research and development, obtaining and displaying results often 
means using simulation programs based on a precise "experimental protocol", 
as it would be done for actual experiments in chemistry or physics. 
These characteristics have led us, in a first step, to try to build a set of exer-
cises with solutions relying for the most part on simulation; we then attempted 
to design an introductory course on Digital Signal and Image Processing (DSIP) 
mostly based on such exercises. Although this solution cannot replace the tra-
ditional combination of lectures and lab works, we do wonder if it isn't just as 
effective when associated with exercise sessions and a few lectures. There is of 
course no end in sight to the debate on educational methods, and the amount 
of experiments being conducted in universities and engineering schools shows 
the tremendous diversity of ideas in the matter. 
Basic concepts of DSIP 
The recent technical evolutions, along with their successions of technological 
feats and price drops have allowed systems based on micro-controllers and 
microprocessors to dominate the field of signal and image processing, at the 
expense of analog processing. Reduced to its simplest form, signal processing 
amounts to manipulating data gathered by sampling analog signals. Digital 
Signal and Image Processing, or DSIP, can therefore be defined as the art of 
working with sequences of numbers. 

12 Digital Signal and Image Processing using MATLAB® 
The sampling theorem 
The sampling theorem is usually the first element found in a DSIP course, be-
cause it justifies the operation by which a continuous time signal is replaced 
by a discrete sequence of values. It states that a signal can be perfectly recon-
structed from the sequence of its samples if the sampling frequency is greater 
than a fundamental limit called the Nyquist frequency. If this is not the case, 
it results in an undesired effect called spectrum aliasing. 
Numerical Sequences and DTFT 
The Discrete Time Fourier Transform, or DTFT, introduced together with 
the sampling theorem, characterizes the spectral content of digital sequences. 
The analogy between the DTFT and the continuous time Fourier transform is 
considered, with a detailed description of its properties: linearity, translation, 
modulation, convolution, the Parseval relation, the Gibbs phenomenon, ripples 
caused by windowing, etc. 
In practice, signals are only observed for a finite period of time. This 
"time truncation" creates ripples in the spectrum and makes it more difficult 
to the separate two close frequencies in the presence of noise. This leads to 
the concept of frequency resolution. The DTFT is a simple way of separating 
two frequencies, but only if the observation time is greater than the inverse of 
the difference between the two frequencies. The frequency resolution will allow 
us to introduce the reader to weighting windows. However, a more complete 
explanation of the concept of resolution can only be made if noise disturbing 
the signal is taken into account, which is why it will be studied further when 
random processes are considered. 
The Discrete Fourier Transform, or DFT is the tool used for a numerical 
computation of the DTFT. Because this calculation involves a finite number 
of frequency values, the problem of precision has to be considered. There are 
a few differences in properties between the DFT and the DTFT, particularly 
regarding the indexing of temporal sequences that are processed modulo N. 
Some examples of this are the calculation of the DTFT and the DFT of a 
sinusoid, or the relation between discrete convolution and the DFT. At this 
point, the fast algorithm calculation of the DFT, also called FFT (Fast Fourier 
Transform), will be described in detail. 
Filtering and Elements of Filter Design 
Linear filtering was originally used to extract relevant signals from noise. The 
basic tools will be introduced: the discrete convolution, the impulse response, 
the frequency response, the z-transform. We will then focus on the fundamen-
tal relation between linear filtering with rational transfer functions and linear 
constant-coefficient recursive equations. 

Foreword 13 
Filter design is described based on a few detailed examples, particularly the 
window method and the bilinear transform. The concepts of over-sampling and 
under-sampling are then introduced, some applications of which are frequency 
change and the reduction of quantization noise. 
An introduction to images 
Image processing is described in its own separate chapter. Many of the con-
cepts used in signal processing are also used in image processing. However 
images have particular characteristics that require specific processing. The 
computation time is usually much longer for images than it is for signals. It is 
nevertheless possible to conduct image processing with MATLAB®. This theme 
will be discussed using examples on 2D filtering, contour detection, and other 
types of processing in cases where the 2D nature of the images does not make 
them too different from a ID signal. This chapter will also be the opportunity 
to discuss image compression and entropic coding. 
Random Processes 
Up until now, the signals used as observation models have been described by 
functions that depend on a finite number of well known parameters and on 
simple known basic functions: the sine function, the unit step function, the 
impulse function, etc. This type of signal is said to be deterministic. 
There are other situations where deterministic functions cannot provide us 
with a relevant apprehension of the variability of the phenomena. Signals must 
then be described by characteristics of a probabilistic nature. This requires 
the use of random processes, which are time-indexed sequences of random vari-
ables. Wide sense stationary processes, or WSSP, are an important category of 
random processes. The study of these processes is mainly based on the essen-
tial concept of power spectral density, or psd. The psd is the analog for WSSP 
of the square module of the Fourier transform for deterministic signals. The 
formulas for the linear filtering of WSSP are then laid down. Thus, we infer 
that WSSPs can also be described as the linear filtering of a white noise. This 
result leads to a large class of stationary processes: the AR process, the MA 
process, and the ARMA process. 
Spectral Estimation 
One of the main problems DSIP is concerned with is evaluating the psd of 
WSSPs. In the case of continuous spectra, it can be solved by using non-
parametric approaches (smooth periodograms, average periodograms, etc.) or 
parametric methods based on linear models (AR, MA, ARMA). 

14 
Digital Signal and Image Processing using MATLAB® 
The least squares 
This chapter discusses the use of the least squares method for solving problems. 
This method is used in a number of problems, in fields such as spectral anal-
ysis, modelling, linear prediction, communications, etc. We will discuss such 
methods as the gradient and LMS algorithms and the Wiener equalization. 
As a Conclusion 
One of the issues raised by many of those who use signal processing has to 
do with the artificial aspect introduced by simulation. For example, we use 
sampling frequencies equal to 1, and therefore frequencies with no dimension. 
There is a risk that the student may lose touch with the physical aspect of 
the phenomena and, because of that, fail to acquire the intuition of these 
phenomena. That is why we have tried, at least in the first chapters, to give 
exercises that used values with physical units: seconds, Hz, etc. 
This work discusses important properties and theorems, but its objective 
is not to be a book on mathematics. Its only claim, and it is certainly an 
excessive one, is to show how interesting signal and image processing can be, 
by providing examples and exercises we chose because they were not too trivial. 
All of the subjects discussed far from cover the extent of knowledge required 
in this field. However they seem to us to be a solid foundation for an engineer 
who would happen to deal with DSIP problems. 

Notations and 
Abbreviations 
o 
empty set 
2::k,n 
2::k 2::n 
rectT(t) 
sinc(x) 
l (x E A) 
(a, b] 
t5(t) 
Re(z) 
Im(z) 
i or j 
x(t) ;=: XU) 
(x*y)(t) 
{
I 
when 
o otherwise 
sin( 7fX) 
1TX 
{
I 
when x E A 
o otherwise 
{x: a < x :::; b} 
It I < T/2 
(indicator function of A) 
{
Dirac distribution when t E IR 
Kronecker symbol when t E Z 
real part of z 
imaginary part of z 
V-I 
Fourier transform 
continuous time convolution 
1. x(u)y(t - u)du 
discrete time convolution 
L
x(u)y(t - u) 
uEZ 

16 
Digital Signal and Image Processing using MATLAB® 
x or ;K 
I N 
A * 
AT 
AH 
A -I 
JP'{X E A} 
IE {X} 
Xc = X -IE {X} 
var(X) = IE{ IXcl}2 
IE {XIY} 
ADC 
ADPCM 
AMI 
AR 
ARMA 
BER 
bps 
cdf 
CF 
CZT 
DAC 
DCT 
d.e.jde 
DFT 
DTFT 
DTMF 
dsp 
e.s.d.jesd 
FIR 
FFT 
FT 
vector x 
(N x N)-dimension identity matrix 
complex conjugate of A 
transpose of A 
transpose-conjugate of A 
inverse matrix of A 
probability that X E A 
expectation value of X 
zero-mean random variable 
variance of X 
conditional expectation of X given Y 
Analog to Digital Converter 
Adaptive Differential PCM 
Alternate Mark Inversion 
Autoregressive 
AR and MA 
Bit Error Rate 
bits per second 
cumulative distribution function 
Clipping Factor 
Causal z-Transform 
Digital to Analog Converter 
Discrete Cosine Transform 
difference equation 
Discrete Fourier Transform 
Discrete Time Fourier Transform 
Dual Tone Multi-Frequency 
digital signal processing/processor 
energy spectral density 
Finite Impulse Response 
Fast Fourier Transform 
continuous time Fourier Transform 

HDB 
IDFT 
i.i.d.jiid 
IIR 
lSI 
LDA 
lms 
MA 
MAC 
OTF 
PAM 
PCA 
p.d./pd 
ppi 
p.s.d./psd 
PSF 
PSK 
QAM 
rls 
rms 
r.p./rp 
SNR 
r.v.jrv 
STFT 
TF 
WSS 
ZOH 
ZT 
Notations and Abbreviations 17 
High Density Bipolar 
Inverse Discrete Fourier Transform 
independent and identically distributed 
Infinite Impulse Response 
InterSymbol Interference 
Linear Discriminant Analysis 
least mean squares 
Moving Average 
Multiplication ACcumulation 
Optical Transfer Function 
Pulse Amplitude Modulation 
Principal Component Analysis 
probability distribution 
points per inch 
Power Spectral Density 
Point Spread Function 
Phase Shift Keying 
Quadrature Amplitude Modulation 
recursive least squares 
root mean square 
random process 
Signal to Noise Ratio 
random variable 
Short Term Fourier Transform 
Transfer Function 
Wide (Weak) Sense Stationary (second order) process 
Zero-Order Hold 
z-Transform 


Introduction to MATLAB 
In this book the name MATLAB® (short for Matrix Laboratory) will refer to: 
the program launched by using the command matlab in Dos or Unix 
environments, or by clicking on its icon in a graphic environment such as 
xU, Windows, MacOS, etc. 
or the language defined by a vocabulary and syntax rules. 
MATLAB® is an interpreter, that is to say a program that remains in 
the computer's memory once it is launched. MATLAB® displays several sub-
windows (Figure 1) one of which is a command window used to type commands 
(instructions, functions names, program names), which are then directly "in-
terpreted ". 
--
-
--
4. MATlAB R2011b 
_" 
x 
File 
Edit 
Debug 
Desktop 
Window 
Help 
D Name .... 
Min 
Ma 
Administriltcur 
0.7071 
0.7 
III 
Publk 
0.7854 
0.7 
III 
uscr6 
» 
theto!l'"'t)i / 1 ; 
co~ 
(theto!l) 
0 . 7071 
A » 
Figure 1 - The MATLAB® command window on MS- Windows 
MATLAB® can be used in two ways: by direct execution of the commands 
typed in the command window or by the execution of programs. When working 

20 Digital Signal and Image Processing using MATLAB® 
on large-sized projects, we use several programs and/or functions, which should 
be organized by location on the disc. Examples of these three situations are as 
follows: 
1. Direct execution by typing demands directly in the command window: as 
an example type x=log(2) at the MATLAB® prompt in the command 
window. The answer is shown in Figure 2. 
('\ n ('\ 
MATlAB 7.12.0 (R20 11a) 
~ 0 el 
d' ... ~ 
.., 
c:'" .. 00 ~ 
~
I 
[{Users/ blanchet/Docur:.:J D Ii!ll 
~ Shortcuts (fI How to Add 
(fI What', New 
~ Q) New to MArLAR? Watch this Video see Demos or read Getting Started. 
x 
;£ 
»x=log(2) 
~ 
~command 
::l 
X = 
u 
0.6931 
01( 
result 
.» 
A 
Figure 2 - The result log(2) is given by the interpreter 
Executed commands can be recalled by pressing the t arrow to scroll 
through all previously typed commands, or a sequence of letters followed 
by t. All commands beginning with these letters then scroll. 
2. Execution of a program: a program regroups commands in a text file 
also called script. The user gives it a name, with the extension m. The 
"built-in" editor can be used, which is highly recommended, or otherwise 
an external editor can be used to create such files. If a program is named 
progl.m, just type progl in the command window to start the execution. 
MATLAB® looks for the file in the current directory. If it is not found 
there, it looks in other paths known. This list can be obtained by typing 
path. 
The typed type progl enables the contents of the file to be viewed. 
EXAMPLE 1 (Using the built-in editor) The integrated editor is de-
signed to type the programs, and can be activated by selecting New 
script in the file menu or by typing edit in the command window. 
Type the two program lines of Figure 3. 
Once the program has been typed and saved under the name examplel.m, 
the type examplel and examplel commands give: 

Introduction to MATLAB 21 
nn 
~ 
+§l 
r§l 
~ 
El 
It-El 
~ 1~- ----a-~~[~I~,~
2~ ,~3~
]~ ,~ b~~- a----
-'
D 
2 -
c~[ 
1,-1] 
Iln 2 
Col 6 
#. 
Figure 3 - Typing a program with the built-in editor 
» t ype example 1 
a=[1,2 , 3] , b=a 
c=[1,-1] 
» example1 
a = 
1 
2 
3 
b 
1 
2 
3 
c = 
1 
-1 
The execution of a program can also be launched by selecting the ~ icon 
in the edit window or by using the F5 control key. 
3. Development of a project: this requires the creation of several programs 
and/or functions (see paragraph 3.3) . This implies in principle the cre-
ation of a directory in which all of these elements, and their associated 
data files will be stored. This is called the working directory. Dur-
ing each new session of work on the project, this directory in MATLAB® 
should be indicated. This can be achieved by selecting Current Folder in 
the subwindow. Once this is done, pwd can be typed to verify (Figure 4). 
The definition of the directory path can also be done by selecting the 
item set path ... in the menu file. The path can also be defined in 
the command prompt window (by clicking on the icon with " ... " in the 
top-right corner of the window), or by using the command addpath. 
The functions path, addpath, rmpath, genpath, pathtool a savepath 
can be used for managing the access path to the various directories used 

22 Digital Signal and Image Processing using MATLAB® 
n () ("I 
MATLAB 7.12.0 (R2011a) 
! 0 el 
d! ... Ia ., (" .. OJ @l 
@ 
I/ Users/blanchet/ Docum.nlS/ MATIAB 
~ Shortcuts (tJ How to Add [tI What's New 
x i' It- 0 
Current Folder 
!Itl ~ Users ~ blanchet ~ Documents ~ MATLAB 
II N;l~
'" 
calculTFO.ai 
t:I calculTFD.m 
t) essaLm 
€I hyst2.m 
€I mafonc.m 
€I myfunc.m 
€I myfunc2.m 
t:I myprog.m 
.JJ. myprog.m-
t) myprog2.m 
~ myprog2.m-
€I thinning.m 
Command Window 
New to MATLAB?Watch this ~ 
see ~ 
or read ~ 
» 
pwd 
ans = 
l users / blanchet/ Oocuments /MATLAB 
» 
userpath 
ans = 
luser s/blanchet/oocuments/MATLAB : 
~D_'U _iIS -, 
_____________________________ 
A~ 
~» 
"
Start 
Figure 4 - Example of a current directory which is the default directory under Mac 
OS 
by MATLAB® (see documentation, online help, or type help path). 
To avoid repeating this manipulation at each new work session, a 
startup. m file can be created in the startup directory. When using linux 
or Mac OS, the latter is obtained by the command userpath (Figure 4). 
With Windows, the userpath command can also be used, unless the 
Start in field in the Properties dialog box window is already initial-
ized with an access path. 
1 
Variables and constants 
1.1 Vectors and matrices 
The MATLAB® language is dedicated to matrix calculations and was optimized 
in this perspective. The variables handled as a priority are real or complex 
matrices. A scalar is a 1 x 1 matrix, a column vector is a matrix with only one 
column, and a line vector a matrix with only one line. 
The notation (€ x c) indicates that the considered variable has € lines and 
c columns. 
EXAMPLE 2 (Assignment of a real matrix) Type a= [1 2 3; 4,5,6] at the 
MATLAB® prompt in the command window: 
» a=[1 2 3;4,5,6] 
a = 
1 
4 
2 
5 
3 
6 

Introduction to MATLAB 23 
» size(a) 
ans 
2 
3 
The size (a) instruction gives the number of rows and columns of a. 
Values are assigned to the elements of a matrix by using brackets. A space, 
or a comma, is a separator, and takes you to the next column, while the semi-
colon takes you to the next line. Elements are indexed starting from 1. The 
first index is the line number, the second one is the column number, etc. In 
our example, a(l,l)=l and a(2,1)=4. The assignment a=[l 2;3 4 5] will 
of course lead to an error message, since the number of columns is different for 
the first and second lines. 
Character strings can also be assigned to the elements of a matrix. However, 
the string length must be compatible with the structure of the matrix. For 
example, N= [' paul' ; , john'] would be correct, whereas N= [' paul' ; 'peter'] 
would cause an error. 
When the vector's components form a sequence of values separated by reg-
ular intervals, it is easier to use what is called an "implicit" loop of the type 
(indB: step: indE). This expression refers to a list of values starting at indB 
and going up to indE by increments of step. Values cannot go beyond indE. 
The increment value step can be omitted if it is equal to 1. 
EXAMPLE 3 (Implicit enumeration) Type 
a=(0:1:10) 
or 
a=(0:10). 
MATLAB® returns: 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
EXAMPLE 4 (Incremented implicit enumeration) Type a= (0: 4: 10). 
MATLAB® returns: 
048 
The last element of a vector is indicated by the reserved word end. In the 
previous example, a(end) indicates that its value is 8. 
There are two other suites of value constructs: linspace and logspace. 
Thus x=linspace(-10,15,100) returns a vector line of 100 numerical values 
ranging from -10 to +15. x=logspace(-1,2,100) gives 100 values between 
10-1 and 102 "on a logarithmic scale". 
It is possible to extend the size of a matrix. The interpreter takes care of 
available space by dynamically allocating memory space during the execution 
of the typed instruction. 

24 
Digital Signal and Image Processing using MATLAB® 
EXAMPLE 5 (Extension of matrix) Type the following commands one after 
the other: 
» a=[l 2 3; 4 5 6] 
a = 
» 
a = 
» 
a = 
1 
4 
a=[a a] 
1 
4 
a=[a;a] 
1 
4 
1 
4 
COMMENTS: 
2 
5 
2 
5 
2 
5 
2 
5 
3 
6 
3 
6 
3 
6 
3 
6 
1 
4 
1 
4 
1 
4 
2 
5 
2 
5 
2 
5 
3 
6 
3 
6 
3 
6 
- when defining variables and objects, the language takes into account 
whether letters are capital or lowercase; 
- typing " ;" at the end of a command line prevents the program from 
displaying the results of an operation; 
- several instructions can be written on the same line (using the previous 
example, a=[l 2 3; 4 5 6], a=[a;a], a=[a;a] could be typed). In-
structions are separated by commas, in this case, or semicolons depending 
whether or not a display of the results is required by the user; 
- the display format can be modified by using the format command. Ex-
ecuting format long, for example, changes the number of displayed sig-
nificant digits from 5 to 15; 
- the user must bear in mind that MATLAB® dedicates memory space 
every time a variable is used for the first time. All of the variables used 
during a work session are stored in the computer's memory, which means 
it is necessary to free space from time to time so as not to get the OUT 
OF MEMORY error message (see the clear command in the documentation 
or type help Clear). 

Introduction to MATLAB 
25 
1.2 Predefined matrices 
The following commands are used to obtain certain particular matrices: 
- ones (L, C) returns a matrix with L lines and C columns containing noth-
ing but ones. ones (1, N) returns a line vector made up of Nones; 
- zeros (L, C) returns a matrix with L lines and C columns containing 
nothing but zeros; 
- eye (L, C) returns the L x C matrix with ones on the diagonal and zeroes 
everywhere else. eye (1, N) , for example, will return a line vector with 
one "I" followed by N - 1 "0"; 
- randn (L, C) returns a matrix with L lines and C columns containing a 
centered gaussian distributed sample with a variance equal to 1; 
- rand (L, C) returns a matrix with L lines and C columns containing a 
sample uniformly distributed on the interval (0,1); 
- aside from the usual matrices, such as Hilbert, Hadamard, Vander-
monde, etc., a large number of predefined matrices are available using 
the gallery function. For a list of these matrices, type help gallery. 
The reshape function is used to change the size of a matrix, for example, 
to go from a (2 x 6) matrix to a (3 x 4) matrix (refer to documentation, or type 
hel p reshape). This change of size can also be done directly, as shown in the 
following example: 
EXAMPLE 6 (Changing the size of a matrix) Type: 
» a=[(1 :6);(7:12)]; 
» c=zeros(3,4); 
» cC: )=a; 
% 2*6 matrix 
% predimensioning 
% column by column filling-out 
which would be the equivalent of c=reshape (a, 3,4) . The zeros (3,4) com-
mand initializes the choice of size for the matrix c. The purpose of the next 
instruction c ( : ) =a is to fill out the matrix c, column by column, with the se-
quence of 12 values taken from a column by column. a and c must have the 
exact same number of elements. 
The symbol % indicates that what follows in the line is a comment and will 
therefore not be executed. 
EXAMPLE 7 (Predefined matrices) The instructions: 
II 
» x=[ones(1,5) ;-ones(1,5)]; 
» y=zeros(1,10); y(:)=x 
return a line vector containing 10 alternate 1 and -1. As we will see, the same 
thing can be done with (-1). ~[O:9] 
(see section 2.2). 

26 Digital Signal and Image Processing using MATLAB® 
1.3 Constants and initialization 
The constants pi, i , j are predefined: pi=3. 14159265358979 . . . , i 
j 
A. Keep in mind that executing the instruction pi=4 makes pi lose its 
predefined value. It is recommended not to use pi, i and j as variables in a 
program. Writing 1i and 1j will be preferred to i and j , respectively. 
eps, realmin and realmax are other constants provided for limit 
test purposes. 
Their values are respectively: 
2.220446049250313e- 16 , 
2.225073858507201e-308 and l. 797693134862316e+308 . realmin and realmax 
correspond to the extreme values that can be obtained in the "floating-point 
double precision" coding of the IEEE-754 standard. 
The constants lnf (00) and NaN (Not-a-Number) can also be used in the 
calculations. It should be noted that NaN is not strictly speaking a constant 
but an indication on the impossibility of making a calculation. TYpe % 
to 
see the result. 
EXAMPLE 8 (Special constants) lnf or NaN can be used as any other 
constant in operations between matrices. 
» a=[O Inf NaN]; b=[O -1/0 3]; a+b 
ans 
o 
NaN 
NaN 
1.4 Multidimensional arrays 
Multidimensional arrays (not supported by some old versions) are an extension 
of the normal two-dimensional matrix. One way to create such an array is to 
start with a 2-dimension matrix that already exists and to extend it. Type: 
» a=[1:3;4:6] 
a = 
1 
4 
2 
5 
3 
6 
» a(:,: ,2)=zeros(2,3), % or a(:,: ,2)=0 
a(: , : ,1) 
1 
4 
a(: , : ,2) 
o 
o 
2 
5 
o 
o 
3 
6 
o 
o 

Introduction to MATLAB 
27 
The repmat and cat functions are provided in order to build multidimen-
sional arrays. 
1.5 Cells and structures 
In the most recent versions of MATLAB@, there are two groups of data that 
are more elaborate than scalar arrays and character string arrays: the first one 
is called a cell and the second a structure. 
EXAMPLE 9 (Cell definition) In an array of cells, the elements can be of any 
nature, numerical value, character string, array, etc. Type: 
» langcell={'MATLAB' ,[6.5;2.3] ,2002}; 
» langcell (2) 
ans 
[2x1 double] 
» langcell {2} 
ans 
6.5000 
2.3000 
» langcell{2}(1) 
ans 
6.5000 
langcell is made up of three elements: the first one is a character string, 
the second one is a column vector, and the third one is a scalar. This example 
shows the difference in syntax between an array and a cell, a left brace ({) 
and a right brace (}) being used instead of a left square bracket ([) and a 
right square bracket (]). As for the content, langcell (2) refers to the vector 
[6.5000; 2 . 3] , langcell {2} to the content of this vector, and langcell {2} (1) 
to the numerical value 6.5. 
A structure is defined by the struct instruction. The following exam-
ple defines a structure, called langstruc, comprising three fields: Language, 
VerSion, and Year. 
EXAMPLE 10 (Defining a structure) The instruction struct assigns the 
character string MATLAB to the first field, the character string 6.5 to the second 
field, and the numerical value 2002 to the third field: 

28 
Digital Signal and Image Processing using MATLAB® 
»langstruc=struct('Language', 'MATLAB', ... 
'Version', '6.5', 'Year' ,2002); 
»langstruc.Year 
ans = 
2002 
» 
The second instruction displays the content of langstruc. Year, which is 
2002. A 1 x 1 dimension structure is organized in the same way as an n x 1 
dimension array of cells, where n is the number of fields of the structure. Cells 
can therefore be compared to structures with unnamed fields. 
EXAMPLE 11 (Defining a structure) We 
define 
a 
structure 
named 
langstruc, comprised of two sets. 
Each set contains all three fields 
Language, Version, and Year. Fields are initialized, respectively, with the 
sequences of two character strings MATLAB and C, of the two values 6.5 and 
15.1, and of the two values 2002 and 2003: 
» 
langstruc=struct('Langage' ,{{'MATLAB', 'C'}}, ... 
'Version' ,[6.5;15.1], 'Year' ,[2002;2003]); 
» 
langstruc 
langstruc = 
Language: {'MATLAB' 
'C'} 
Version: [2x1 double] 
Year: [2x1 double] 
» 
langstruc.Langage{1} 
ans = 
MATLAB 
» 
langstruc.Language(1) 
ans 
'MATLAB' 
» 
These objects can be handled using certain functions: 
isstruct, 
fieldnames, setfield, rmfield, cellfun, celldisp, num2cell, cel12mat, 
cel12struct, struct2cell, etc. 
EXAMPLE 12 (Defining a structure from cells) An example of a conver-
sion is as follows: 
II 
» clear all 
» 
langcell={'MATLAB' ,[6.5;2.3] ,2002}; 

» mfld={ ' Langage', ' Version', 'Year'} ; 
» cel12struct (langcell ,mfld ,2) 
ans 
Langage : ' MATLAB' 
Vers i on : [2x1 double] 
Year : 2002 
Introduction to MATLAB 
29 
The third parameter (2) that is part of the instruction ce1l2struct indi-
cates the dimension of langcell that needs to be taken into account to define 
the number of fields. Here, for example, size (langcell, 2) means that the 
number of fields is 3. 
2 
Operations and functions 
2.1 
Matrix operations 
The main matrix operations are the following: 
- the (+, x) operations, sum and multiplication of two matrices. If a = [aij ] 
and b = [bij], and if the dimensions are correct, then a + b = [aij + bij] 
et a x b = [Lk aikbkj]. 
EXAMPLE 13 (Multiplication of m atrices) Type the following com-
mands: 
» a= [1 2; 3 4] * [5 ; 6] 
a = 
17 
39 
» size (a) 
ans 
2 
1 
Multiplication is done in the algebraic sense of the term. The matrice 
[1 2; 3 4] corresponds to a linear application and the vector a is the 
transformed vector. 
The command size (a) returns "2 1" giving us the number of lines and 
the number of columns of a; 

30 Digital Signal and Image Processing using MATLAB® 
the backs lash provides the solution to the linear problem A x = b in the 
form x=A \ b. If A is a full-rank square matrix, this amounts to multiplying 
on the left by the inverse matrix. Otherwise, the solution is given in the 
least squares senseI. 
EXAMPLE 14 (Solving a linear system) Type: 
»A=[1 2 ;2 3]; b=[1;1]; % Full rank square matrix 
x = 
-1 
1 
»x=A\b 
x = 
-1 
1 
% Solution using the inverse 
% Solution using the system resolution 
the operation AlB amounts to performing the operation B' \ A ' ; 
the operation ~ carries out the exponentiation of the argument, which 
can be a fractional scalar, positive or negative, or a matrix; 
the apostrophe ' is used for the transpose-conjugate or trans conjugate. 
As a reminder, if the (N x N) matrix A is the conjugate-transpose of B , 
then A = B H and we have [aij] = [bji] for 1 :S i,j :S N. 
EXAMPLE 15 (A few operations) Type the following commands: 
» 
X=[2 0;1 3]; X-2 % (= X * X) 
ans 
4 
0 
5 
9 
» r.5 
ans 
1.4142 
0 
0.3178 
1.7321 
» a=(0:3); b=(0:3); c=b*a' 
c = 
1 The problem of solving a linear system in the least-squares sense plays a crucial role in 
signal processing. This will be discussed further in Chapter 9. 

Introduction to MATLAB 
31 
14 
» 
d=b' *a 
d = 
0 
0 
0 
0 
0 
1 
2 
3 
0 
2 
4 
6 
0 
3 
6 
9 
The vectors a and b are real (4 x 1) line vectors. The scalar c is therefore 
equal to the scalar product of the vectors a and b. On the other hand, d is a 
"multiplication table"-type (4 x 4) matrix. 
2.2 Pointwise operations 
The operations ". x", ".;" and "
. ~" 
work term by term. The phrase pointwise 
operations is also used. For example, if A = [aij ] and B = [bij] are two matrices 
of the same dimension, A . * B returns the matrix [aij bij ]. 
EXAMPLE 16 (Pointwise operations) Type the following commands and 
check the result: 
»clear % Free data memory space 
» a=(1:3)' * (1:4); b=(5:7)' * (1:2:7); 
»c=a.*b; 
» d=a ./ b; 
» e = a .-( .5); 
» a,b,c,d,e 
In this sequence of instructions: 
- a and b are two matrices with 3 lines and 4 columns. They are obtained 
by multiplying a dimension 3 column vector and a dimension 4 line vector; 
- c is a matrix that has Cij = aij x bij as its generic element; 
-
d is a matrix whose element dij = aij jbij ; 
- e is a matrix whose element eij = ..;a:;;. 
EXAMPLE 17 (Alternating sequence) 
(-1) . - [0 : 9] leads to a sequence of alternating 1 and -1. 
COMMENTS: 
- in term by term operations, matrices must have the same dimensions; 

32 Digital Signal and Image Processing using MATLAB® 
- while the operation' (apostrophe) transconjugates a matrix, the opera-
tion .' (period-apostrophe) transposes without conjugating; 
- the conj function conjugates each element of a matrix; one obtains [aij]. 
EXAMPLE 18 (Transposition and transconjugation) Type: 
» a=[1+j 
a = 
1.0000 
3.0000 
» 
a ' 
ans = 
1.0000 
2 .0000 
» a. , 
ans = 
1.0000 
2.0000 
» 
conj(a) 
ans = 
2 ;3 4] 
+ 1.0000i 
- 1.0000i 
+ 1.0000i 
2.0000 
4.0000 
3. 0000 
4 .0000 
3. 0000 
4.0000 
1.0000 - 1 .0000i 
2 .0000 
3.0000 
4.0000 
The bsxfun function (Binary Singleton Expansion Function) can also be 
used to perform operations point to point. Type help bsxfun. 
EXAMPLE 19 (Pointwise operations with bsxfun) Type 
the 
following 
commands and check the result: 
»clear % free data memory space 
» 
a=(1 :3)' * (1:4); b=(5 :7)' * (1 :2 :7) ; 
»c =a .*b; 
» 
d=bsxfun(@times,a,b) ; 
» c ,d 
2.3 Mathematical functions 
Certain functions handle matrices only as an array of values. This is the case 
for functions such as: abs, sqrt, exp, cos, sin, log, tan, acos, aSin, atan, 
etc. 

Introduction to MATLAB 33 
EXAMPLE 20 (Exponentia l function) Type: 
» T=1024; tims=(0:T- 1) ; %===== three frequencies 
» fq =[ . 01 .013 .014]; 
» %===== complex Signal 
» sig = exp(2*1j*pi*tims'*fq); 
» %===== displaying of the real part of the complex exponential 
» % 
that is to say cos(2*pi*0 .01*n), for fq= .01 , .013 , .014 
» plot(tims, r eal(sig( : ,1))) 
tirns is a (1 x 1024) line vector and therefore tirns' *fq is a (1024 x 3) 
matrix. You can see this for yourself by typing, at the end of the previous 
program, the command whos: 
» "hos 
Name 
Size 
Elements 
Bytes 
Density 
Complex 
T 
1 by 1 
1 
8 
Full 
No 
fq 
1 by 3 
3 
24 
Full 
No 
sig 
1024 by 3 
3072 
49152 
Full 
Yes 
tims 
1 by 1024 
1024 
8192 
Full 
No 
The instruction sig 
exp(2*lj*pi*tirns'*fq); applies the exponen-
tial function to each of the elements of the matrix 2*lj*pi*tirns' *fq. 
The result is the (1024 x 3) matrix sig. 
In the last instruction 
plot(tirns,real(sig(: ,1))) , sig(: ,1) refers to the first column of the ma-
trix sig. The tirns and real (sig(: ,1)) correspond to the respective abscissa 
and ordinate of the requested plot. 
Built-in functions 
There is a large number of library functions that can be called using the 
MATLAB® language. Some are provided with the MATLAB® interpreter, 
while others have to be paid for, as part of extra modules. 
Among the functions available in the basic version, the user will for example 
find mathematical functions such as the exponential exp, the logarithm log, 
the usual trigonometric functions, etc. or functions that have more to do 
with signals and images such as the Fourier transform fit , the 2D convolution 
conv2, etc. 
Some of these functions are written in the MATLAB® language, while others 
are written in machine language, for reasons of execution speed. 
EXAMPLE 21 ("Source programs" and compiled progra m s) Type: 
type cornpan. MATLAB® displays the text of the cornpan function (if the 
version permits), which can be found in one of the folders of your hard-drive. 
However, if the instruction t ype fft is executed, MATLAB® returns: 
??? Built-in function 

34 Digital Signal and Image Processing using MATLAB® 
meaning that this function is compiled and that its source code cannot be 
accessed. 
In the most recent versions of MATLAB@, many functions that used to be 
written as" .m" programs were rewritten and now appear as "Built-in". 
2.4 Matrix functions 
As we have seen, the exp (A) command calculates the exponential of each ele-
ment of the matrix A . This operation must not be confused with the matrix 
exponential. The letter "m" at the end of the functions expm(A), logm(A), 
sqrtm(A) indicates that we are dealing with matrix functions. For example, 
eA is defined by: 
A 
A k 
eA = I + - + ... + -
+ ... 
I! 
k! 
and is obtained with the function expm (A) . 
There is also a function called funm that can be used to calculate any 
function of a matrix. Type help funm. 
2.5 Searching elements using min, max, find, etc. functions 
1. The min, max, median functions provide the requested - minimum, max-
imum, median - value and the index in which it has been found in the 
matrix. 
EXAMPLE 22 (min, max, median functions) Type: 
» a=[1 2;34], median(a), median(a,1), median(a,2) 
a = 
ans 
ans 
ans 
1 
2 
3 
4 
2 
3 
2 
3 
1.5000 
3.5000 

Introduction to MATLAB 35 
median(a) or median(a,1) provide the median value in each of the 
columns, median (a, 2) the median value along each of the rows. 
» max(a) , max(a,[],l), min(a,[],2) 
ans 
ans 
ans 
3 
4 
3 
4 
1 
3 
max (a) gives the maximum in each of the columns. max (a, [] ,1) (equiv-
alent to max (a)) and min (a, [] ,2) give the maximum and the minimum 
along the dimensions 1 and 2, respectively. 
2. The find function provides the indices for which the given condition is 
met. 
EXAMPLE 23 (Search function) Taper: 
» a=rand(3,3), find(a>.5), [1,e]=find(a>.5); [I,e] 
a = 
ans 
ans 
0.9649 
0.1576 
0.9706 
1 
3 
4 
6 
9 
1 
1 
3 
1 
1 
2 
3 
2 
3 
3 
0.9572 
0.4854 
0.8003 
0.1419 
0.4218 
0.9157 
rand(3,3) provides a (3 x 3) table obtained by the "random" drawing of 
numbers in the interval [0, 1]. The first form of find gives the linear index, 

36 Digital Signal and Image Processing using MATLAB® 
while the second provides pairs of indices of items (C( i), c( i) )that meet the 
condition (Matlab provides the function ind2sub to convert between the 
linear index and the index in each dimension). Type idx=find(a>. 5) ; 
[1,c]=ind2sub(size(a),idx). 
EXAMPLE 24 (Some other functions) The 
following 
instructions 
build the pairs (m, n), m =f. n, obtained from the four figures 1 to 4. 
» A=ones(4,4); B=triu(A,l) 
B = 
o 
o 
o 
o 
1 
o 
o 
o 
1 
1 
o 
o 
1 
1 
1 
o 
» 
[nl,nc]=find(B==i); [nl,nc] 
ans 
1 
2 
1 
3 
2 
3 
1 
4 
2 
4 
3 
4 
triu(A,1) provides the strictly upper triangular part of A. 
2.6 Other useful functions 
The eig function returns the eigenvalues and the eigenvectors of a matrix. The 
poly function returns the characteristic polynomial associated to a matrix, or 
a polynomial whose roots are a given vector. The roots function returns the 
roots of a polynomial. 
EXAMPLE 25 (A few functions - 1) Type: 
» a=[lll]; 
» rr=roots(a) 
rr = 
-0.5000 + 0.8660i 
-0.5000 - 0.8660i 
» poly(rr) 

Introduction to MATLAB 37 
ans 
1.0000 
1.0000 
1.0000 
In this example, the values of the complex roots of the polynomial a(x) = 
x 2 + X + 1 are obtained with roots (a) . 
EXAMPLE 26 (A few functions - 2) Type: 
» a=[l 2;1 1]; 
» poly(a) 
ans 
1.0000 
-2.0000 
-1.0000 
» 
[vp,md]=eig(a) 
vp = 
0.8165 
-0.8165 
0 .5774 
0 .5774 
md = 
2 .4142 
0 
0 
-0.4142 
» roots(poly(a)) 
ans = 
2.4142 
-0.4142 
In this example, a is a (2 x 2) matrix, its characteristic polynomial poly(a) 
is equal to det("\I - a) = ,,\2 - 2,,\ - 1. The eigenvectors of a are given by vp. 
md is the diagonal matrix bearing the eigenvalues on its diagonal, which are 
also the roots of the characteristic polynomial. 
3 
Programming structures 
A program execution can be modified using the Boolean variable's tests and 
iterated using loops. 
3.1 
Logical operators on Boolean variables 
The logical operators AND (symbol &), OR (symbol I), and NOT (symbol -) 
operate on Boolean quantities. The "false" Boolean value is coded as 0 and 

38 Digital Signal and Image Processing using MATLAB® 
"true" as a non-zero value. The Boolean constants true and false can be 
used. x=true, x=1 , x=O==O give the same result. 
Boolean quantities can be used in structures such as "if ... elseif ... 
else .. . end", "switch 
or "while ... end". 
case ... case ... otherwise ... end" 
EXAMPLE 27 (Logical functions) Type: 
» x=1; 
» if x==O, 
A=[12]; 
else 
A=[21]; 
end 
»A 
A = 
2 
1 
isnan, isinf, isfinite, isstr, ischar, etc. are Boolean functions used 
for testing the type or the state of the variables. 
3.2 Program loops 
The for . .. end program structure works as a calculation loop. 
EXAMPLE 28 (Program loops) Type: 
» A=[1 .5; .5 .25]; 
» M=eye(2,2); % Unit Matrix 
» for k=1:5 
M = M * A; 
% Calculation of the consecutive powers of A 
end 
The loop can be written in a single line with for k=1: 5; M = M * A; end. 
As is the case for many interpreters, loops tend to deteriorate calculation 
performances considerably. The user is therefore advised not to use them, by 
replacing them with matrix functions when possible. 
EXAMPLE 29 (Avoiding loops) Type: 
» N=100; a=randn(N,N); 
» tic, for k=1:N, for m=1:N, b(k,m)=a(k,m)-2; end, end, toe 
Elapsed time is 0.001374 seconds. 
» tic, c=a .- 2; toe 
Elapsed time is 0.000052 seconds. 

Introduction to MATLAB 
39 
tic "starts the clock", toe stops it and displays the time duration since 
tic. The instruction e=a . ~ 2 returns a matrix e identical to the matrix b. 
However its execution is much faster. 
3.3 Functions 
In the same way that MATLAB® puts functions at our disposal, it is possible 
to also develop functions to fit our needs. The structure of a function is as 
follows: 
function [outpl, outp2 , . . . ]=myfunc(inpl, inp2, .. . ) 
% Comments 
% inpl = . . . 
if nargin<2 , imp2=10 ; end 
return 
outpi, outp2, etc. are variables calculated by the myfune function. inpi, 
inp2, etc. are input parameters. Comment lines - starting with % - following 
the constructs of the declaration function assist the function and can be initi-
ated with help myfune. The nargin and nargout functions are used to test 
the number of input parameters and output, respectively. The error function 
allows an error message to be displayed, stopping the program execution: 
functi on [outp]=myfunc(inpl,inp2) 
% Comments 
% 
if nargin<2, inp2=4; end 
if nargin<l, 
error('input parameter missing'); 
end 
outp=(inpl+inp2)/2 ; 
end 
4 
Graphically displaying results 
4.1 
2D display 
Display windows are chosen using the command figure en) , where n is a win-
dow number (integer 2: 1). Inside the active window, the plot command can 
be used to graphically display results: 
- if x and y are two real vectors of the same length, the plot ex, y) displays 
the graph of y as a function of x; 

40 Digital Signal and Image Processing using MATLAB® 
- if x and yare two real matrices of the same size, the plot (x, y) command 
displays the first column of y as a function of the first column of x, the 
second column of y as a function of the second column of x, and so on 
until there are no columns left. Each line has its own color; 
- if x is a real vector with a length of N, and y 
is a size (N x K) real 
matrix, the plot (x, y) command displays the K graphs corresponding 
to the K columns of y as a function of x; 
- if x is a complex vector, the plot (x) displays the graph of the imaginary 
part of x as a function of the real part of x (see example 30). There 
may be problems if some components of the vectors are real. Therefore, 
preferably use plot (real (x) ,imag (x)) instead of plot (x); 
- if the command subplot (3,2,4) or subplot (324) is added before the 
plot command, the graph is divided in six "sub-windows" organized in 
three lines of two columns each, and the display is done in sub-window 
number 4. 
EXAMPLE 30 (Drawing of a circle) Type the following instructions: 
» clear; 
» z=exp(2*pi*1j*[O:100]/100); 
» subplot(121); plot(z); axis([-1.2 1.2 -1.2 1.2]); 
» subplot(122); 
» plot(real(z),imag(z»; axis('square ' ); 
The axis ( [-1.2 1.2 -1.2 1.2]) command forces particular values on the 
minima and the maxima of the x- and y-coordinates. The axis ( I square ') 
command forces the display to appear in a square (same units in abscissa and 
ordinate). 
The zoom command is used to zoom in on a particular part of the graph. 
Windows and graphs are objects whose properties can be consulted and 
modified using get and set. These properties can also be accessed from the 
pull-down menus, meaning that the user does not have to reprogram them. See 
exercise 2.8, page 93 which describes another way to go about this. Following 
the instructions of the example 30, type set (gca, 
I color I , 
I yellow '). The 
color property color, corresponding to the color of the background of the 
graphic object - sub-window 2 - is set to yellow. 
EXAMPLE 31 (Drawing an ellipse) 
The exponents T and H refer to the transposition and the transposition-
conjugation respectively. If the matrix is real, then of course Y H = yT. 
In its matrix form, the equation of an ellipse is: 
(X - Xo)HE(X - Xo) = c 

Introduction to MATLAB 
41 
where c is a positive constant, X 0 is a dimension 2 vector characterizing the 
center of the ellipse, and E is a 2 x 2 positive matrix, meaning that, for any 
complex vector Y , y H EY is a positive number. A simple way of obtaining a 
positive matrix is to take any real matrix G and to calculate G T G . 
By diagonalizing E , we get E = P DpH where D is a diagonal matrix 
with all its diagonal elements positive, and P a unitary matrix, that is to say 
such that P pH = pH P = I where I refers the identity matrix. 
Let us assume F = PD 1j2 p H. 
Incidentally, we have F = F H and 
F H F = E . 
F is called the square root of E . 
Starting with this, we 
have (X - XO) H E(X - Xo) = (X - XO) H F H F (X - Xo) . By assuming 
Y = F(X - X o), we get yHy = c which is the equation of a circle of cen-
ter 0 and radius ,;c in a set of orthonormal coordinates. This leads us to a 
calculation procedure of N points of the ellipse characterized by X 0, E and c: 
1. Calculate Y = ,;c[cose sine] for e from 0 to 27l' in steps of 27l'/N. 
2. Make the variable change X = Xo + F -1y. 
Starting with this procedure, we now write a function with the ellipse's 
center defined by any vector XO, the positive matrix E, and the constant c as 
input parameters. 
HINTS: type: 
function ellipse(XO, E, c) 
%%===================================================% 
%% Drawing an ellipse 
% 
%% SYNOPSIS: ELLIPSE(XO, E, c) 
% 
%% 
XO 
Coordinates of the ellipse's center (2x1) % 
%% 
E = A posi ti ve (2x2) matrix 
% 
%% 
c 
= Scale Factor 
% 
%%===================================================% 
N=100; theta = (O:N) * (2*pi) .j N ; 
Y = sqrt(c)*[cos(theta);sin(theta)]; 
Fm1=inv(sqrtm(E)); 
X = diag(XO)*ones(2,N+1)+Fm1*Y; 
plot(X(1,:),X(2,:)); set(gca, 'DataAspectRatio', [1 1 1]) 
return 
Test this function2 with the following program, choosing several 
values of c and: 
[
1.3628 0.7566] 
Xo = [0 
0] and E = 0.7566 0.5166 
Test program: 
2 Save this function under t he name ellipse.m. It will be used later on. 

42 Digital Signal and Image Processing using MATLAB® 
%===== testellipse .m ===== 
XO=[O 0]; 
E=[1.3628 .7566; .7566 .5166]; 
c=1; 
ellipse(XO,E,c) 
We displayed in Figure 5 the results obtained for c = {l, 2, 3, 4, 5}. 
1 0,-----~------~------~------,_----~------~ 
5 
o ----------" 
-5 ---------- " 
-10 
- 6 
-4 
- 2 
o 
2 
4 
6 
Figure 5 - Drawing of several ellipses 
EXAMPLE 32 (Plotting a Bode diagram) Consider the transfer function 
G(8): 
G 8 
_ 
2 8 + 4 
( ) -
8 2 + 38 + 2 
Type (see Figure 6): 
%===== testbode.m 
w=logspace(-1,1,100); % values between . 1 and 10 rad/s 
%===== Laplace transf . ==> s=jw 
s=1i*w; vnum=2*s+4; vden=s.*s+3*s+2; 
% polyval() may be used 
% vnum=polyval([2 4] ,s); vden=polyval([1 32),s); 
%===== complex gain 
gcplx=vnum . / vden; 
%===== displaying the Bode diagram 
subplot (211) , semilogx(w,20*log10(abs(gcplx))), % gain 
grid 
mphi=180*atan2(imag(gcplx),real(gcplx))/pi; 
subplot (212) , semilogx(w ,mphi) % phase 
grid 

Introduction to MATLAB 43 
10 ,-----~--~~~~~~----~--~--,-~~~ 
5 
---------~----
~---------.-----~---~--.--I--
~
-
I -~-
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
o ---------,--
----, 
, 
, 
____ ' ___ L __ L __ ' _' __ '_ -.! _ 
, 
, 
, 
, 
, 
, 
I 
I 
I 
I 
I 
-----.---,--'-.,-,-
-5 ---------r----
-10 
-
-
-
-1- -
- 1-
-
.. - -i-"-
I 
I 
I 
I 
I 
I 
I 
I 
-20 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
_______ __ 1 _____ J ____ , __ _ 1 __ .1 _ J_ 
L J _________ .1 _____ L ___ L __ 1. __ 1 __ L _ 1_ J_ 
I 
I 
I 
I 
I 
I 
" 
I 
I 
I 
I 
I 
I 
I 
I 
-40 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
- - - - - - - - -,- - - - -..,- - - ,- - - ,- - T - ,-, - r..,- - - - - - -
-
- - - - -r - - - r - - T - -,- - r - ,- ,-
-60 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
---------1-----""1----1--- 1--1"-"'1-;-1-""1---------1"-----1----1---
I 
I 
I 
I 
I 
I 
I 
I 
I 
-80 
I 
I 
I 
I 
I 
I 
I 
I 
I 
-100L-____ ~
' __ ~ ' __ ~
' ~~~~ 
____ ~
' __ ~' ~
' ~~~~ 
10- 1 
10° 
Figure 6 - Bode diagram of s2~~~
2 
The semilogx function is almost identical to the plot function, plot except 
that the ordinate is plotted in logarithmic scale. 
4.2 3D display 
Functions given by z = f(x, y) 
When there is a type z = f(x, y) function, the meshgrid function enables all 
the coordinates to be set on the grid. The mesh or surf function performs the 
plot of a surface. The contour function gives a representation by contour lines. 
EXAMPLE 33 (Drawing a 3D Gaussian) Taper: 
%===== mdraw2D.m ===== 
x=(-2: .1:2); y=(-2: .1:2); 
[X,Y]=meshgrid(x,y); Z=exp(-(X.-2+Y.-2)/2); 
figure(l); mesh(X,Y,Z); grid on 
print -depsc mdraw2Dtest.eps; 
hhm=figure(2); surf(X,Y,Z); grid on 
The print -depsc filename function exports a figure with the "encapsu-
lated color postscript" format. 

44 Digital Signal and Image Processing using MATLAB® 
0.8 
0.6 
0.2 
o 
2 
,-, --
Figure 7 - Drawing a Gaussian with surf 
Surfaces given by points M(x(t),y(t),z(t)) 
2 
The plot3 function allows general 3D curves to be plotted, under the path or 
point form. 
EXAMPLE 34 (Plotting a portion of a sphere) Type: 
%===== mysphere.m 
theta=(O:O.l:pi); phi=(-pi/4:0.1:pi/2); 
Nt=length(theta); Np=length(phi); 
x=cos(phi')*cos(theta); y=cos(phi')*sin(theta); 
z=sin(phi')*ones(l,Nt); 
plot3(x,y,z); 
The sphere, ellipsoid and cylinder functions allow standard surfaces to 
be plotted. 
4.3 Notes on plotting a curve 
When plotting parametric curves in maps, it is useful to use the full capacity of 
MATLAB® to manipulate the complexes. Consider for example a Bezier curve 
(cubic spline) such as those used by Adobe Illustrator®. The equation for such 
a curve is given by: 
where Po , Pl , P2 and P3 are control points, Po and P3 are end points of the 
curve and Pl and P2 are sometimes called handles. 

1 
0.8 
0.6 
0.4 
0.2 
o 
-0.2 
-0.4 
-0.6 
-0.8 
1 
Introduction to MATLAB 
45 
0.8 
0.6 
Figure 8 - Plotting a portion of a sphere 
EXAMPLE 35 (Plotting a Bezier curve) The mybezier. m program plots 
the Bezier curve, which is defined by the four points Po , Pl , P2 and P3 . 
%===== mybezier.m 
clear all 
Npts=100; t=(O:1/(Npts-1):1); 
PO=O; P1=.5+1j; P2=2-2*1j; P3=1.5; 
plot([PO P1 P2 P3], '0'), hold on, 
plot([PO P1 P2 P3], '-') 
t2=t.*t; t3=t2.*t; 
Omt=1-t; Omt2=Omt .* Omt; Omt3=Omt2 .* Omt; 
Pt=PO*t3 +3 * P1 * t2 .* Omt +3 * P2 * t .* Omt2 + P3 * Omt3; 
plot(real(Pt),imag(Pt)), grid, hold off 
4.4 Animations 
EXAMPLE 36 (Progressive plotting, using delete) 
The myanim. m program builds the animation by plotting the path segments 
and by displaying the extremity of the segment (a circle), identifying the object 
by the handle hh, for a length of time set by the pause function. Once this 
appropriate time has passed, the circle object is "destroyed" by delete (hh) . 
II 
~~::;= 
a~ianim 
. m 
trc=true; 

46 Digital Signal and Image Processing using MATLAB® 
, 
0.5 
-
-"'j 
-
-1-
-
-1-
-
1-
-
-"'j -
-
-1-
-
-
1-
-
-
I -
-"'j - --
- 0.5 
- 1 
--~---:---:---~--~---:---
I -
-1.5 
- 2L-~--~~ 
__ ~ __ ~~ __ ~~ __ ~~ 
o 0.2 0.4 0.6 0.8 
1.2 1.4 1.6 
Figure 9 - A Bezier curve 
fO=1/2; Ts= .05; t=(0:Ts:2); Lt=length(t); 
y=sin(2*pi*fO*t); 
subplot(211), plot(t,y,'o' ,t,y), grid 
subplot (212) 
mycol=get(gca, 'color'); 
plot([t(1) t(end)] ,[-1 1], 'color' ,mycol), hold on 
for k=2:Lt 
2 
if trc plot([t(k-1),t(k)] ,[y(k-1),y(k)],'-'); end 
hh=plot(t(k),y(k), '0'); 
pause(1/10), delete(hh) 
end 
There are functions to carry-out this type of animation (see comet and 
comet3). 
EXAMPL E 37 (Animated plot, using erasemode) 
The myanim2 program plots a Lissajous curve, in which the phase of the second 
"input" yt varies. 
%===== myanim2.m 
clear all 
fO=1/2; Ts= .02; tmax=2; K=2; 
t=(O:Ts:tmax); 
phi=[0:pi/39:K*pi]; Lt=length(phi); 
M=3; xt=sin(2*M*pi*fO*t); 
P=1; yt=sin(2*P*pi*fO*t+phi(1)); 
hh=plot(xt,yt, '-' ,'erasemode', 'xor'); grid 
axis([-1 1 -1 1]) 
for k=2:Lt 

Introduction to MATLAB 47 
yt=sin(2*P*pi*fO*t+phi(k)); 
set(hh, 'ydata' ,yt); 
pause 0/15) 
end 
5 
Converting numbers to character strings 
As an example, let us consider the text (x, y, 'text') command. It allows the 
user to add text to a graph, placed at coordinates (x, y). To add a numerical 
value to the text command, it must first be converted to a character string. 
This can be done with the num2str command. 
EXAMPLE 38 (Numbers and character strings) Type: 
II 
» fe=10' 
» 
valfe~num2str(fe) 
The sprintf command can also be used to build a character string. In fact, 
it is used by num2str. 
EXAMPLE 39 (Creating a character string) Type: 
II 
» fq=[10.5 20.566]; 
» valf=sprintf('F1 = %+15.2f, F2 = %4.2e', fq(1),fq(2)) 
The expression sprintf ( ... ) leads to a character string obtained by con-
verting the numerical value to the format specified by format. For example, 
the %10.4f format converts the given value with 4 decimal points. For more 
information, it is recommended to read the printf function's description in C 
language. 
The functions str2num and hex2num should also be looked into. 
6 
Input / output 
MATLAB® makes it possible to perform input-output operations from the 
keyboard, on the screen (as it was explained in the previous paragraph with 
sprintf) or on files. Here are the main functions: 
- input, ginput, 
for keyboard acquisition; 
- disp, sprintf, 
to display on the screen; 
- gtext, plot, grid, title, ... 
to display in a graph; 

48 
Digital Signal and Image Processing using MATLAB® 
- load, save to load or save parts of the variables in a file, or all of them, 
in a format specific to MATLAB®. By default, files have the extension 
.mat. 
It is recommended that the MATLAB® version is checked using the back-
up data. If the version is different, it is necessary to specify the format 
when you save the file (save); 
- fopen , fread , fwrite for input-output with formatting. 
EXAMPL E 40 (Input/output in a file) Type: 
» clear ; x=[1:100]; 
» fid=fopen('tryl .dat', ' w'); 
» fwrite(fid,x, 'short') ; 
% Writing 
» fclose(fid); 
» fid=fopen('tryl .dat', 'r' ); 
» y=fread(fid , 'short') ; 
% Reading 
» fclose(fid); 
This program creates the tryl . dat file of 16 bit integers, then reads its 
content in variable y. 
7 
Program writing 
MATLAB® offers several facilities for writing and developing programs. Some 
of which are provided below. 
7.1 
Developing and testing performances 
Breakpoints and "debug" mode 
It is possible to "define" breakpoints in the editing window (red disk in the 
left column of Figure 10). The execution of programs stops there, allowing the 
contents of variables to be viewed. This can be done in the command window, 
in which the prompt key becames K», or the edit window by positioning the 
cursor of the mouse over the name of the variable. 
D evelopment tools 
Besides the functions tie and toe (see example 29) that can be used to test 
execution times, there are several other tools to test performance and the form 
of the programs. 
1. mlint <prog_name>: analyses the script prog_name.m and provides in-
dications for improving the syntaxe, the readability and maintainability. 

I 
Introduction to MATLAB 49 
n n n Editor - /Users/blanchet/ECOLE/REDACTIONS/TNSWiley1/progs/cOO/mysphere.m 
.:. ll.il .. 
• N.. 
_.0£.. 
.. 
.. 
nnn 
MATLAB 7.12.0 (R2011a) 
~ 
1 
%-
-- sphere.m 
~ n eI 
M. 
~ 
~ '" 
(" • 
~ 
~ 
~ ~ 
» 
2 -
theta=(O:O.l:pi); 
3 0 ¢ 
Nt=length(theta); ~ Shortcuts [fI How to Add [fI What', New 
4 -
X=cOS(phi' )*COS(t Q) New to MATLAB? Watch this Video. see Demos. or read Getti 
5 -
z=sin(phi' )*ones( 
6 -
plot3(x,y,z); 
f'£ K» 
2 usages of "Nt" found 
.. Start I Stopped in debugger 
hi 
Figure 10 - Defining a breakpoint 
2. depfun <prog_name>: gives a list of all the scripts and functions of which 
prog_name . m needs to be executed. 
3. the cells (cells in the editor) : these are defined as a subset of code lines 
that can be executed to test effects. These sub-sets are simply separated 
by lines starting with %% (Figure 11). 
4. the performance test for the myanim2 is given by the function profile: 
» profile on 
» myanim2 
» profile viewer 
» profile off 
It is obtained by the profile viewer command. The obtained informa-
tion is shown in 12. 
5. local processing of errors: 
TRY 
instructions 
CATCH ME 
processing instructions if error between TRY and CATCH 
END 
7.2 Various functions 
executing strings: the eval function executes the instruction given by a 
string. 

50 Digital Signal and Image Processing using MATLAB® 
1M. 0 Editor - /Users/blanchet/ECOLE/ SCOlARlTE/AA FORMATIONCONTINUE/AAAREDACTlONS/LATEXpolyl.e!.;:::-' 
File 
Edit 
Text 
Go 
Cell 
Tools 
Debug 
Desktop 
Window 
Help 
x " .. ~O 
§ 
el • 
j(, 
~ 
~ .., t'" 
~ 
EJ . " .. • 
f<~ 
~ . ~ ~ D 
" CQJD 
~~ 
~ 
- ~ 
+ 
rul x 
%~ 
%% 
O. 
o This file uses Cell Mode . For information, see the ra~
i d code iteration video, the publishing video, or ~
. 
x 
1 
2 
3-
4-
5 -
6 -
7 -
B-
9 -
10 
11-
12 -
13 -
14 -
15 -
~ 
~ " 
%===== myanim2.m 
%% 
clear all 
fO=1/2 ; Ts= . 05 ; tmax=2 ; 
t=(O : Ts : tmax) ; I 
phi=10 :pi/29 : 2*p i] ; Lt=length(phi) ; 
-
y=sin(2*pi*fO*t+phi (1 » ; ymi n=mi n(y) ; ymax=max(y) ; 
hh;:-plot(t , y , , -
I , t , y , 'or' , 'erasemode' , 'xor' ) , g rid 
-
axi s ([O tmax ymin ymax ] ) 
%% r 
k=2 : Lt 
y=s in(2*pi *fO*t +phi (k » ; 
set(hh , 'ydata' , y) ; 
pause(1/10) 
end 
x 
myanim.m 
x 
mybezier.m 
x 
smokem.m 
( x 
myanlm2 .m 
) 
x 
simuproc.m 
~ 
I script 
I Ln 5 
Col 16 
/f. 
Figure 11 - Defining cells for testing purposes 
EXAMPLE 41 (Creating files) The testeval. m program creates five 
files in the MATLAB® format (this format depends on the version, there-
fore it may be necessary to specify the version of MATLAB® that these 
files will be used with). The name of the file is built in the program loop: 
%===== testeval.m 
for k=i : 5 
end 
fname=['datfile ' ,num2str (k ), I .dat ' ] ; 
s a ve (fname, I k ' ) 
calls to the operating system: the operating system commands can be 
requested in the command window by using the exclamation point: 
II » 
! cp datfile1. dat datf.mat 
copies the file datfilel .dat under the name datf . mat in the current 
directory when the operating system is Unix; 

Introduction to MATLAB 51 
lenD 
Profiler 
J 
File 
Edit 
Debug 
Desktop 
Window 
Help 
.. 
~ . .. t;i 
~ '" 
, 
~ I Start Profil ing I Run this code: [ 
El • 
Profile time: 58 sec 
Profile Summary 
Generated 19- Mar- 2010 11:13:22 using cpu time. 
Function Name 
Calls Total Time Self Time* Total Time Plot 
(dark band - self time) 
myanim2 
1 
34.288 s 
33.036 s 
newplot 
1 
1.059 s 
0.000 s 
I 
cia 
1 
1.059 s 
0.096 s 
I 
newlllot>ObserveAxesNextPlot 
1 
1.059 s 
0.000 s 
I 
gra~h
i cs lQrivate l clo 
1 
0.963 s 
0.385 s 
I 
setdiff 
2 
0.578 s 
0.289 s 
I 
axis 
1 
0.193 s 
0.096 s 
unique 
2 
0.193 s 
0.193 s 
I 
ismember 
1 
0.096 s 
0.096 s 
axis>LocSetLimits 
1 
0.096 s 
0.096 s 
gill! 
1 
Os 
0.000 s 
g.ct 
3 
Os 
0.000 s 
newplot>ObserveFigureNextPlot 1 
Os 
0.000 s 
gra~h
i (slpr
i vatelclo>find 
kids 
1 
Os 
0.000 s 
findall 
1 
Os 
0.000 s 
axis>allAxes 
1 
Os 
0.000 s 
ishold 
1 
Os 
0.000 s 
Self rime is the time spent in a function excluding the time spent in its child functi ons. 
Self time also includes overhead resulting from the process of profiling. 
. . 
#. 
Figu re 12 - Measuring the performances 
7.3 U sing other languages 
MATLAB® authorizes the use of procedures written in an evolved language 
such as C, Pascal or Fortran. These programs belong to the type called MEX. 
Throughout the rest of this book, we will use only predefined functions and 
those we are going to build in the MATLAB® language. 
MATLAB® also makes it possible to create programs with a graphic inter-
face, combining buttons, pull-down menus, scrolling windows, etc. Using these 
possibilities is a good way of building "press-a-button" demonstrations or lab 
works that help to emphasize certain properties. The demonstrations included 
with MATLAB® are an excellent source of documentation for creating such 
programs. 


Part I 
Deterministic Signals 


Chapter 1 
Signal Fundamentals 
Although this work is mainly focused on discrete-time signals, a discussion of 
continuous-time signals cannot be avoided, for at least two reasons: 
- the first reason is that the quantities we will be using - taken from nu-
meric sequences - are taken from continuous-time signal sampling. What 
is meant is that the numeric value of a signal, such as speech, or an 
electroencephalogram reading, etc., is measured at regular intervals; 
- the second reason is that for some developments, we will have to use math-
ematical tools such as Fourier series or Fourier transjorms of continuous-
time signals. 
The objective is not an extensive display of the knowledge needed in the 
field of deterministic signal processing. Many other books have already done 
that quite well. We will merely give the main definitions and properties useful 
to further developments. We will also take the opportunity to mention systems 
in a somewhat restricted meaning, this word referring to what are called jilters. 
1.1 The concept of signal 
A deterministic continuous-time signal is defined as a function of the real time 
variable t : 
Signal = function x(t), t E lR 
The space made up of these functions is completed by the Dirac pulse 
distribution, or J (t) function. Actually a distribution (a linear junctional), this 
object can be handled just like a function without any particular problems in 
the exercises we will be dealing with. 
The following functions spaces are considered: 

56 Digital Signal and Image Processing using MATLAB® 
- L1 (IR) is the vector space of sum mabie functions such that fIR Ix(t) Idt < 
+00; 
- L1 (a, b) is the vector space (vector sub-space of L1 (IR)) off'unctions such 
that f: Ix(t)ldt < +00; 
- L2 (IR) is the vector space of finite energy functions such that fIR Ix( t ji2 dt < 
+00; 
- L2 (a, b) is the vector space (vector sub-space of L2(IR)) of functions such 
that f: Ix(tji2dt < +00; 
- the set of "finite power" functions characterized by: 
1jT/2 
lim -
Ix(tWdt < +00 
T-++oo T 
-T/2 
L 2 (0, T) has the structure of what is called a Hilbert space structure with 
respect to the scalar product f x(t)y*(t)dt, a property that is often used for 
decomposing functions, for example in the case of Fourier series. 
In the course of our work, we will need to deal with a particular type of 
signal, in sets that have already been defined, taken from IR+. 
Definition 1.1 (Causal and anticausal signals) Signals x(t) such that 
x(t) = 0 for t < 0 are said to be causal. Signals x(t) such that x(t) = 0 
for t 2: 0 are said to be anticausal. 
1.1.1 
A few signals 
We will often be using particular functions characteristic of typical behaviors. 
Here are some important examples: 
- the unit step function or Heaviside function is defined by: 
u(t) = l(t E (0, +ooD 
(1.1 ) 
Its value at the origin, t = 0, is arbitrary. Most of the time, it is chosen 
equal to 1/2. The unit step can be used to show causality: x(t) is causal 
if x(t) = x(t)u(t); 
- the sign function is defined using the unit step by signet) = 2u(t) - 1; 
- the gate or rectangle function is defined by: 
rectT(t) = l(t E (-T/2, T/2» = u(t + T/2) - u(t - T/2) 
(1.2) 
It will be used to express the fact that a signal is observed over a finite 
time horizon, with a duration of T. The phrases rectangular windowing 
and rectangular truncation of x(t) are also used: XT(t) = x(t)rectT(t); 

Chapter 1 - Signal Fundamentals 57 
- the pulse, or Dirac function, has the following properties which serve the 
purpose of calculation rules: 
1. fIR b(t)dt = 1 and fIR b(t)x(t)dt = x(O). 
2. x(t) = fIR x(u)b(t-u)du = (x*b)(t) (* is the convolution operation). 
3. x(t)b(t - to) = x(to)b(t - to). 
4. (x(u)*b(u-to))(t) = (x*b)(t-to) = x(t -to). 
5. b(at) = b(t)/Ial for a"l O. 
6. Vt, f ~oo b(u)du = l(t E (0, +00)) = u(t) and therefore du(t)/dt = 
b(t). This result makes it possible to define the derivative of a 
function with a jump discontinuity at a time to. 
Let x(t) = 
xo(t) + au(t - to) where xo(t) is assumed to be differentiable. We 
have dx(t)/dt = dxo(t)/dt + ab(t - to); 
- the sine function is defined by: 
x(t) = Xo sin(wot + ¢) = Xo sin(27r fot + ¢) 
(1.3) 
where Xo is the peak amplitude of the signal, Wo its angular frequency (in 
radians/s), ¢ its phase at the origin, fo = wo/27r its frequency (in Hz) 
and T = 1/ fo its period; 
- the complex exponential function is defined by: 
x( t) = Xo exp(2j7r fot + j¢) 
(1.4) 
- the sine cardinal is defined by sinc(t) = sin(7rt)/7rt. It is equal to 0 
for all integers except t = 0 (hence its name). We have fIR sinc(t)dt = 1, 
fIR sinc(u)sinc(u-t)du = sinc(t) and the following orthogonality property, 
for n E N: 
r . (). ( 
)d 
{I with n = 0 
JIR smc usmc u - n u = 
0 with n"l 0 
1.1.2 Spectral representation of signals 
Fourier series 
A periodic signal with a period of T = 1/ fo may be decomposed as a sum of 
complex exponentials, a sum we will refer to as Fourier series1 : 
(1.5) 
1 We will only be using the complex exponential decomposition, since it easily leads to the 
one with the sine and cosine functions. 

58 Digital Signal and Image Processing using MATLAB® 
fa = l i T is called fundamental frequency, and its multiples are called har-
monic frequencies. A few comments should be made: 
- a signal with a bounded support on (h, t2) is also expandable in a Fourier 
series, but the series converges to the periodized function outside of the 
(t1 ' t2) interval; 
- expression 1.5 indicates that X k is the k-th component of x(t) in the 
orthonormal basis of the complex exponentials {T-1/2e2j7rkJot} kE d', in the 
Hilbert space L2(0, T); 
- XM(t) = "Li!-M Xk e2j7rkt/T is the best length M approximation of x(t) 
in the sense of the least squares. Sometimes ~ can be used instead of 
~ to indicate that the Fourier series convergence is ensured and is not 
uniform, which results in: 
r IX(t) - f Xk e2j7rkt/T I2 dt --t ° 
when M --t +00 
(1.6) 
J(T) 
k=- M 
- when x (t) is continuous, XM (t) converges uniformly to x (t) for any t, 
when M --t +00; 
- if x(t) shows first order discontinuities, XM(t) will converge to the half-
sum of the left and right limits of x(t). Finally, XM(t) can show some 
non-evanescent oscillations in the neighborhoods of all discontinuities. 
This phenomenon is referred to as the Gibbs phenomenon; 
- we have Parseval's relation: 
(1. 7) 
Because the first member of 1. 7 is by definition the signal's power, the 
sequence {IXkI2} can be interpreted as the power distribution along the 
frequency axis. It is also called power spectral density, or psd. 
Fourier transform 
The spectral contents X(f) of the function x (t) ELl (IR) n L 2 (IR) can be repre-
sented by an integral that uses complex exponentials, an integral we will call 
Fourier transform: 
(1.8) 

Chapter 1 - Signal Fundamentals 59 
IX(j)1 is called spectrum of x(t). The Fourier transform's main properties 
are summarized in Appendix AI. 
The convolution property 11.1 leads to Parseval's formula: 
(1.9) 
Because the left member of 1.9 is, by definition, the signal's energy, IX(jW 
can be interpreted as the energy distribution along the frequency axis. It is 
also called energy spectral density, or esd. 
More generally, we have: 
~ x(t)y*(t)dt = ~ X(j)Y*(j)df 
(1.10) 
EXAMPLE 1.1 (Analytical signal) 
Let x(t) be a continuous time real signal. The analytical signal associated 
with x(t) is the signal z(t) that has Z(j) = 2U(j)X(j) as its Fourier transform, 
where X(j) is the Fourier transform of x(t) and U(j) is the function equal to 
1 if f > 0 and 0 if f < O. U(O) is chosen equal to 1/ 2. 
Using the properties of the continuous-time Fourier transform, show that 
the real part of z(t) is equal to x(t) , and determine its imaginary part called 
the Hilbert transform of x(t). 
HINTS: let: 
p(t) = Re(z(t)) = (z(t) + z* (t)) / 2 
Using the Fourier transforms, we get: 
P(j) = (Z(j) + Z*( - J)) / 2 = U(j)X(j) + U( - J)X*( - J) 
Because x(t) is real, X(j) = X *( - J) , and therefore, P(j) = X(j), 
which means p(t) = x(t). As a conclusion, Re(z(t)) = x(t). 
Likewise, let: 
q(t) = Im(z(t)) = (z(t) - z* (t)) / 2j 
Using the Fourier transforms, we get: 
Q(j) 
(Z(j) - Z*( - J)) / 2j = -j(U(j)X(j) - U( - J)X*( - J)) 
-j(U(j) - U( - f))X(j) 
Because U(j) -
U( - J) 
is the sign(j) function, Q(j) 
-jsign(j)X(j). This equation can be interpreted as filtering (see 

60 Digital Signal and Image Processing using MATLAB® 
paragraph 1.2) with the complex gain filter -jsign(J). Its gain 
is equal to 1, meaning that the Fourier transforms of the output 
and input have the same modulus, IQ(J)I = IX(J)I. In the liter-
ature, the transformation that associates the output signal y(t) of 
the complex gain filter -jsign(J) with the real signal x(t) is called 
the Hilbert transform. 
As a conclusion, the analytical signal associated with the real signal 
x(t) is written (Figure 1.1): 
z(t) = x(t) + jx(t) 
where x(t) refers to the Hilbert transform of x(t) . 
1-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
~ 
~ 
z(t) = x(t) + jX(t) 
Hilbert 
\(yf-7--a-na-l--:;~ical 
signal 
transform 
~ 
l -jSign(f) ~~ 
x(t) 
x(t) 
(real) 
Figure 1.1 - Analytical signal construction 
1.2 The concept of system 
A system transforms the signal x(t) and delivers a signal y(t), the result of this 
alteration. We will refer to this transformation as y(t) = .9" [x(u) , tJ, and x(t) 
and y(t) will be called the input and the output of the system respectively. 
Filters 
A filter with x(t) as the input and y(t) as the output is a system defined by: 
y(t) = ~ x(u)h(t - u)du = ~ x(t - u)h(u)du 
(1.11) 
The existence of the integral has to do with how the set !Z of considered 
signals x(t) is chosen. Among the sets that have practical interest, two of them 
playa fundamental role: the signals that have a Fourier transform and those 
made up of a linear mix of complex exponentials. 
Certain conditions have to be met: 
- first , in the case of !Z sets that show some practical interest, such a 
system is linear: .9"[alxl(u) +a2x2 (u) , tJ = al.9"[xl(U), tJ +a2.9"[x2(U), tJ; 

Chapter 1 - Signal Fundamentals 61 
- second, it is time-invariant: g [ax(u), t - to] = g[ax(u - to), t] . Another 
way of expressing it is to say that the output is independent of the time 
origin. 
EXAMPLE 1.2 (Counterexample) 
The system defined by y(t) = J ~ x(u)du is linear but is time-dependent. 
HINTS: the output corresponding to the signal x(t - to): 
y(t) = J~ x(u - to)du = J ~~oto 
x(v)dv 
is different from: 
y(t - to) = J~
-to 
x(u)du 
which is the output at time t - to when x(t) is used as the input 
signal. 
Impulse response 
The h(t) function found in 1.11 is called the filter's impulse response. The 
output y(t), convolution product of x(t) and h(t) , is denoted y(t) = (x * h)(t). 
A causal system is a system that depends only on the current and previous 
inputs. This means that a filter is causal if h(t) = 0 for t < O. 
Frequency response 
Let us first consider the case of x(t) signals that have a Fourier transform X(J). 
Using the convolution product's property leads us to: 
Y(J) = X(J)H(J) 
The H(J) function is called the filter's frequency response or complex gain. 
Let us now take a look at signals x(t) that are a linear mix of complex 
exponentials. Because of the linearity property, all we have to do is calculate 
the output with x(t) = exp(2j'rrfot) as the input . We get: 
y(t) = k. exp(2j'rrfo(t - u))h(u)du = H(Jo) exp(2j7rfot) 
Therefore, the complex output signal H(Jo) exp(2j7r fot) corresponds to the 
complex exponential exp(2j7rfot). In this case, complex exponentials are called 
the eigenfunctions of the filters, the eigenvalue beeing H(Jo). 

62 Digital Signal and Image Processing using MATLAB® 
Stability 
A system is said to be BlBD stable if for any Bounded Input x(t) the Output 
y(t) is Bounded, that is Ix(t)1 < A =? ly(t)1 < B. Stability is an essential 
system property. 
A filter is ElBO stable if and only if: 
l
lh(U)ldU < +00 
1.3 Summary 
The following table contains some definitions and properties that will be used 
throughout the next lessons. The properties corresponding to the discrete time 
are also shown. It must be noted that the Laplace transform is given in its 
bilateral form. It is most often seen in the form ft
ClO x(t)e-stdt in the control 
field. The same applies for the z transform and its related form L ~:a 
x(n)z-n. 
Continuous time 
Discrete time 
Fourier transform 
Discrete time Fourier transform 
X(f) = 1 
x(t)e- 2jrrJtdt 
X(f) = L
x(n)e- 2jrrnJ 
nEZ 
x(t) = 1 
X(f)e2jrrJtdj 
11/ 2 
x(n) = 
X(f)e2jrrnf dj 
- 1/2 
Fourier series 
Discrete Fourier transform 
X(k) = ..!.IT x(t)e- 2jrrkt/T dt 
N- 1 
X(k) = L 
x(n)e- 2jrrkn/N 
T 
0 
n = O 
x(t) ~ L
X(k)e2jrrkt/T 
N- 1 
x(n) = ~ L 
X(k)e2jrrnk/N 
kEZ 
k=O 
Linear filter (t E JR) 
Linear filter (n E Z) 
(x * h)(t) +-'t X(f)H(f) 
(x * h)(n) +-'t X(f)H(f) 
BIBO stability <=} llh(t)'dt < +00 
BIBO stability <=} L 
Ih(n)1 < +00 
nEZ 

Chapter 1 - Signal Fundamentals 63 
Continuous time 
Discrete time 
Bilateral Laplace transform 
z-Transform 
X(s) = 1 
x (t)e- stdt 
X( z) = L
x(n)z- n 
n EZ 
1 1
C
+
JOO 
x(n) = 2~7r i X(z)zn- 1dz 
x (t) = ~ 
X(s) estds 
J7r 
C - joo 
Filter (t E JR) 
Filter (n E 2:) 
(x * h)(t) f--t X(s)H(s) 
(x * h)(n) f--t X( z)H(z) 
BIBO stability ? 
imaginary axis be-
BIBO stability? unit circle belongs to 
longs to the domain of convergence of 
the domain of convergence of H(z). 
H(s) . 


Chapter 2 
Discrete Time Signals and 
Sampling 
Signal processing consists of handling data in order to extract information 
considered relevant, or to modify them so as to give them useful properties: 
extracting, for example, information on a plane's speed or distance from a 
RADAR signal, making an old and decayed sound recording clearer, synthe-
sizing a sentence on an answering machine, transmitting information through 
a communication channel, etc. 
The processing is called digital if it deals with a discrete sequence of values 
{Xl, X2, . . . }. There are two types of scenarios: either the observation is already 
a sequence of numbers, as is the case for example for economic data, either the 
observed phenomenon is "continuous-time" , and the signal's value x(t) must 
then be measured at regular intervals. 
This second scenario has tremendous practical applications. This is why an 
entire paragraph of this chapter is devoted to the operation called sampling. 
The acquisition chain is described in Figure 2.1. 
Continuous-Time 
Signal 
Supply Voltage 
(References) 
Ts I (Sampling 
t Period) 
Acquisition, 
Measure 
Discrete-Time 
Signal (Sequence) 
{xs(n) = x(nTs)} 
Figure 2.1 - Digital signal acquisition 
The essential part of the acquisition device is usually the analog-to-digital 

66 Digital Signal and Image Processing using MATLAB® 
converter, or ADC, which samples the value of the input voltage at regular 
intervals - every Ts seconds - and provides a coded representation at the output. 
To be absolutely correct, this coded value is not exactly equal to the value 
of x(nTs). However, in the course of this chapter, we will assume that xs(n) = 
x (nTs). The sequence of these numerical values will be referred to as the digital 
signal, or more plainly as the signal. 
Ts is called the sampling period and Fs = 11Ts the sampling frequency. We 
will discuss later the problems caused by the gap between the actual value and 
the coded value, which is called quantization noise. 
Obviously, the sampling frequency must be high enough "in order not to 
lose too much information" - a concept we will discuss later on - from the orig-
inal signal, and there is a connection between this frequency and the sampled 
signal's "frequential content". Anybody who conducts experiments knows this 
"graph plotting principle": when the signal's value changes quickly (presence 
of "high frequencies"), "many" points have to be plotted (it would actually be 
preferable to use the phrase high point density) , whereas when the signal's value 
changes slowly (presence of low frequencies), less points need to be plotted. 
To sum up, the signal sampling must be done in such a way that the nu-
merical sequence {xs(n)} alone is enough to reconstruct the continuous-time 
signal. The sampling theorem specifies the conditions that need to be met for 
perfect reconstruction to be possible. 
2.1 
Fundamentals of sampling 
Let x(t) be a continuous signal, with X(F) its Fourier transform, which will 
also be called the spectrum. The sample sequence measured at the frequency 
Fs = 11Ts is denoted by xs(n) = x(nTs). 
Definition 2.1 When X(F) ~ 0 for FE (B 1,B2 ) and X(F) = 0 everywhere 
else, x( t) is said to be (Bl' B 2 ) band-limited. If x( t) is real, its Fourier transform 
has a property called hermitian symmetry, meaning that X (F) = X* ( - F), and 
the frequency band's expression is (-B , +B). A common misuse of language 
consists of referring to the signal as a B-band signal. 
2.1.1 The Poisson formula 
All the relative sampling properties are based on the Poisson summation for-
mula which gives the relation between X(F) and the values of x(t) at sampling 
times nTs . 
Lemma 2.1 (Poisson formula) Let x(t) be a signal, and X(F) its Fourier 

Chapter 2 - Discrete Time Signals and Sampling 67 
IX(F)I 
Bl Signal B2 
band 
IX(F)I 
------=~
==l=====
~~ 
F 
-B Signal +B 
band 
Figure 2.2 - (B 1, B 2 ) band-limited complex signal and (-B ,+B) band-limited real 
signal 
transform. Then for any Ts: 
1 
+00 
+00 
- L X(F - kFs) = L x(nTs) exp( -2j7rnFTs) 
Ts k=-oo 
n=-oo 
where the left member is assumed to be a continuous function of F. 
HINTS: the left member of equation 2.1 will be written cy(F). By 
construction, cy(F) is periodic with period Fs = 11Ts. Therefore, 
cy(F) can be expanded in a Fourier series that can be expressed as 
cy(F) s~. 
L~
:-oo 
cne- 2j7rnF/Fs where: 
where we have assumed that u = F - kFs in order to go from the 
first-to-last line to the last line. By referring to property 1.8, which 
gives us the inverse Fourier transform, we infer that Cn = x(nTs), 
thus demonstrating formula 2.1. 
(2.1) 
We did not go into the detail of all the hypotheses necessary to justify the 
previous calculations. We will assume that these calculations are valid. Other 
mathematics books written on the Fourier transform can be looked up for a 
more rigorous approach. 
We will use the following definition for the discrete-time Fourier transform. 

68 Digital Signal and Image Processing using MATLAB® 
Definition 2.2 (DTFT) The sum L.
~:-oo 
x(nTs) exp( -2j7rnFTs) is called 
the Discrete-Time Fourier Transform (DTFT) of the sequence {x(nTs)}. 
We will see another completely equivalent expression of it (definition 2.4, 
expression 2.22), but more frequently used in the case of numerical sequences. 
The DTFT is also called the spectrum of the signal {x( nTs)}. 
COMMENTS: The right side of equation (2.1) can also be written as: 
+00 
+00 
L x (nTs) exp( -2j7rnFTs) or L Xn exp( -2j7rnf) 
n= -oo 
n= -cx') 
by using f = FTs = F / Fs and Xn = x( nTs). It is assumed that f is the 
standard frequency (with respect to the sampling frequency) . 
The left side can be written as: 
1 
+00 
1 
+00 
- L X(F - kFs) = - L X((f - k)Fs) or 
Ts 
Ts 
k=-oo 
k= -oo 
where XN(f) is the Fourier transform at "standard time" (with respect to the 
sampling period). In the X(F) expression by using t = uTS) the following can 
be obtained: 
X(F) 
where XN (U) = x(uTs). This can be expressed using the Poisson formula: 
+00 
+00 
L Xn exp( -2j7rnf) = L XN(f - k) 
(2.2) 
n = - oo 
k = - oo 
It is often forgotten to specify that in (2.2) XN(f) and not X(f) is exam-
ined, and the expression is written as X(f - k). This, however, is not very 
serious, as the spectrum amplitude is rarely examined. 
2.1.2 Perfect reconstruction 
It is often specified at the time of sampling that this operation should be 
performed "without losing too much information contained in the continuous-
time signal". Another way of saying this is by making it clear that it should 
be possible to reconstruct x (t), at every time t, using the sampling sequence 

Chapter 2 - Discrete Time Signals and Sampling 69 
xs (n) = x (nTs). For this reconstruction a "scheme" defined by the expression 
(2.3) is used: 
+00 
y(t) = L x(nTs)h(t - nTs) 
(2.3) 
n = - CX) 
where h(t) is called a reconstruction function. Notice that 2.3 is linear with 
respect to x (nTs). In order to reach this objective, two questions have to be 
answered: 
1. is there a class of signals x (t) large enough for y(t) to be identical to x (t)? 
2. if that is the case, what is the expression of h(t)? 
The answers to these questions are provided by the sampling theorem 2.1. 
Theorem 2.1 (Sampling theorem) 
Let x (t) be a (Bl ' B 2) band-limited signal, real or complex, and let {x(nTs)} be 
its sample sequence, then there are two possible cases: 
1. if Fs = l / Ts is such that Fs ~ B2 -
B l , then x (t) can be perfectly 
reconstructed from its samples x(nTs) using the expression: 
+00 
x(t) 
L x (nTs)h(Bl ,B2)(t - nTs) 
(2.4) 
n = -oo 
where the FT of the reconstruction function h(B 1 ,B2) (t) is: 
(2.5) 
2. if Fs = l / Ts < B 2 - B l , perfect reconstruction turns out to be impossible 
because of the "spectrum aliasing " phenomenon. 
We now go back to the sampling theorem. By using the fact that the Fourier 
transform of h(t-nTs) is H(F) e- 2j7rnFTs, the Fourier transform of y(t), defined 
by 2.3, can be written: 
+00 
+00 
Y(F) 
L x (nTs) x H(F) e- 2j7rnFTs = H(F) L x (nTs)e- 2j7rnFTs 
n = - CX) 
H~~) 
~ 
X(F - kFs) 
k = - oo 
(2.6) 
Therefore, if Fs ~ B2 - B l , the different contributions X(F - kFs) do 
not overlap, and by simply assuming H (B 1 ,B2) (F) = Ts1(F E (Bl ' B 2», Y(F) 

70 Digital Signal and Image Processing using MATLAB® 
1 n=+oo 
T L X(F-nFs) 
~----1f----
s_n--,~ - 00 tran si tion band 
__ -L __ ~~~~~ 
__ -U~~~ 
__ ~-L 
__ ~~~F 
': ~ B 
+B: 
-Fs/ 2 
Fs/2 
Figure 2.3 - Real signal reconstruction 
coincides exactly with X(F). Figure 2.3 illustrates this case for a real signal. 
In this case, Bl = -B and B2 = B. 
Except if specified otherwise, we will assume from now on that x(t) is real. 
The sufficient reconstruction condition can be written as follows: 
(2.7) 
The limit frequency 2B is called the Nyquist frequency. Still in the same 
case, the Fourier transform of a possible reconstruction function is HB(F) = 
Tsrect2B(F), and therefore: 
hB(t) = sin(27rBt) 
7rFst 
(2.8) 
It should be noted that the filter HB(F) = Tsrect2B(F) is not the only 
possible filter. If Fs is assumed to be strictly greater than 2B, then we can 
choose a filter with larger transition bands (see Figure 2.3) , making it easier to 
design. 
When there is no possible doubt, we will not indicate the dependence on 
B, and simply write h(t) instead of hB(t). 
Anti-aliasing filter 
The reconstruction formula 2.3, is, according to the Poisson's formula 2.1, as-
sociated with the periodization of the spectrum X(F) with the period Fs. It 
follows that, for Fs < 2B, the different non-zero parts of the spectrum over-
lap, making perfect reconstruction impossible. The overlapping phenomenon 
is called spectrum aliasing. 
Figure 2.4 illustrates the spectrum aliasing phenomenon for a real signal 
whose frequential content is of the "low-pass" type, implicitly meaning that it 
"fills up" the band (-Fs / 2, +Fs / 2). 

Chapter 2 - Discrete Time Signals and Sampling 71 
Except in some particular cases (see example 2.1 and modulations), we will 
assume that spectrum signals are of this type, or that they can be modified to 
fit this description. 
Figure 2.4 - The aliasing phenomenon 
For a real signal, showing aliasing means that the frequencies beyond the 
frequency Fs/2 can be "brought back" to the (-Fs/2,+Fs/2) band. 
In practice, the following cases will occur: 
1. the sampling frequency is imposed: if, knowing how the data is used, the 
aliasing phenomenon is considered to "cause damage", the appropriate 
procedure for sampling a real signal requires the use of low-pass filter-
ing called anti-aliasing filtering which eliminates the components of the 
frequencies higher than Fs/2; 
2. the sampling frequency is not imposed: in this case, it can be chosen 
high enough so that the aliased components of the signal do not alter 
the expected results. If this is not possible, Fs is set, and the situation 
becomes the same as in the first case. 
Speech signals are a good example. If they are sampled at 8,000 Hz, an ex-
tremely common value, high enough to make the person speaking recognizable 
and understandable, and if no anti-aliasing filtering is done, the reconstructed 
signal contains a "hissing" noise. This alone justifies the use of an anti-aliasing 
filter. The irretrievable loss of high frequency components is actually better 
than the presence of aliasing. 
Figure 2.5 illustrates the case of a "low-pass", pre filtered , real signal to 
prevent aliasing. 
In general, it is important to understand that anti-aliasing filtering must 
be done in the band that is considered essential (useful band) to the unaliased 
signal reconstruction. The low-pass filtering mentioned here corresponds to a 
low-pass sampled signal. 
The following general rule can be stated: 

72 Digital Signal and Image Processing using MATLAB® 
__ ~ __ ~ __ LL~~ 
__ -+~ 
__ ~ __ ~~ ______ ~
F 
-Eo 
Figure 2.5 - Absence of aliasing after [-Eo, +Eol filtering [-Eo, +Eol 
The sampling operation of a signal at the frequency Fs must be preceded 
by an anti-aliasing filtering with a gain equal to 1 and with a width of 
Fs in the useful band. 
The following example illustrates the case of a real band-pass signal, there-
fore (B = [-Fmax , -Fmin]U[Fmin, Fmax]) band-limited. If this was not the case, 
an anti-aliasing filtering in the useful band B would be necessary. 
EXAMPLE 2.1 (Sampling of a narrowband signal) 
Let x(t) be a (Fmin' Fmax) "band-limited" real signal. If (Fmax+Fmin)/2» 
(F max - F min), the signal is called a narrowband signal. Determine the sampling 
frequencies that allow the perfect reconstruction of x (t). 
HINTS: the application of formula 2.7 leads to Fs > 2Fmax. We will 
now show that it is still possible to conduct a slower sampling. In 
order to do this, let us consider the Fourier transform of the signal 
y(t) = z:.
~:-oo 
x(nTs)h(t - nTs) given by expression 2.6: 
H(F) +00 
Y(F) = -
L X(F - kFs) 
Ts 
k = -oo 
This leads us to the conclusion that, in order for X(F) to coincide 
with Y(F), the two following conditions have to be met: 
1. the periodized function z:. X(F - kFs) shows no aliasing (Fig-
ure 2.6); 
2. the function H(F) = Ts for F min < IFI < Fmax and 0 other-
wise (see Figure 2.6). 

Chapter 2 - Discrete Time Signals and Sampling 73 
Fs 
IX(F)I 
,-, 
1
/ 
\, 
: 
\ F 
-Fmax -Fmin 
Figure 2.6 - Narrowband signal (Fmin , Fmax ) 
The non-aliasing condition (1) is met for frequencies for which both 
the following inequalities are true: 
kFs - Fmin < Fmin 
and 
(k + l)Fs - Fmax > Fmax 
which is equivalent to the condition: 
2Fmax 
F 
2Fmin 
k+1 < s<-k-
(2.9) 
where k is an integer such that k ::; ko where ko is the integer part 
of Fmin/(Fmax - Fmin). For k = 0, We encounter once again the 
Nyquist frequency 2Fmax , but if ko > 0, we get possible sampling 
frequencies that are smaller than 2Fmax. Condition (2) leads to the 
following reconstruction function: 
sin ( 7r 6.Ft ) 
h(t) = Ts 
cos(27rFot) 
7rt 
with 6.F = Fmax - Fmin and Fo = (Fmax + Fmin)/ 2. 
Causal approximation of the reconstruction formula 
In order to calculate x(t) at time t, expression 2.4 requires that all the future 
samples beyond t (absence of causality) and up until infinity be known. How-
ever, because h(t) decreases like lit, it is possible to approximate x(t) by using 
a finite number of samples before and after t. A delay is therefore necessary 
for practical reconstruction. For t E (mTso (m + l)Ts) and a high enough value 
of L, this can be written as follows: 
m + L 
x(t) ~ L x(kTs)h(t - kTs) 
(2.10) 
k=m-L 
Of course, this expression only allows the calculation of x(t), in the interval 
(mTs , (m + l)Ts), if x((m + L)Ts) is known. Reconstruction can therefore be 
accomplished by tolerating a delay LTs. 

74 Digital Signal and Image Processing using MATLAB® 
We will see in Chapter 4 an implementation based on the insertion of zeros 
followed by a filtering. Polynomial interpolations are other methods which can 
be used. 
Spectrum aliasing and ambiguity 
Let us consider the continuous-time sine signal: 
x(t) = cos(27rFot) with Fa = 350 Hz 
(2.11) 
sampled at a frequency of Fs = 800 Hz. The sample sequence can be written 
as follows: 
xs(n) = x(n/Fs) = cos(27rfon) with fa = Fo/Fs 
Let us also consider the continuous-time sine signal of frequency FI = Fs -
Fa: 
y(t) = cos(27rFlt) with FI = 1,150 Hz 
sampled at the same frequency Fs = 800 Hz. The sample sequence is: 
ys(n) = y(n/Fs) = cos(27rhn) with h = FdFs 
Using FI = Fa + Fs, we get h = fa + 1. Replacing in y(n) leads us to: 
ys(n) = cos(27r(Jo + l)n) = cos(27rfon) = xs(n) 
This result shows that the use of samples taken at a frequency of Fs alone 
is not enough to be able to tell signal x(t) from y(t). Therefore reconstruction 
will lead to the same signals, whether it is done from samples xs(n) or ys(n). 
In the case of the signal x(t), the result is accurate, but it is false for y(t): we 
started with a frequency of FI = 1,150 Hz and ended up with a signal frequency 
of Fa = 350 Hz. 
For a given signal, for any integer k, it is not possible to distinguish Fa from 
FI = Fa + kFs, k E Z, which is called the image frequency of Fa relative to Fs. 
This is the ambiguity due to the spectrum aliasing phenomenon (or generally 
speaking to the Poisson formula). 
EXAMPLE 2.2 (Ambiguity) 
In the previous example, we now consider FI = 450 Hz. Write a program 
illustrating this case. The continuous-time signal will be visualized over a 
period of 5 ms, as well as the samples xs(n) and ys(n). 
HINTS: type the program: 

Chapter 2 - Discrete Time Signals and Sampling 75 
%===== aliasexple.m 
Fs=800; Te=l/Fs; FO=350; Fl=Fs-FO; 
tmax=.005; mtm=[O:tmax/l00:tmax); 
xt=cos(2*pi*FO*mtm); yt=cos(2*pi*Fl*mtm); 
plot(mtm,[xt' yt')); grid; hold on; 
nlmax=floor(tmax/Te); 
mtm=[O :nlmax)*Te; % sampling times 
xenO=cos(2*pi*FO*mtm); plot (mtm , xenO , '0') ; 
xenl=cos(2*pi*Fl*mtm); 
plot(mtm,xenl,'x') ; hold off 
o samples from x(t) 
x samples from yet) 
Figure 2.7 - An illustration of aliasing 
We obtain the same result ys(n) = xs(n). In the case of x(t) 
sin(27rFot) we obtain ys(n) = -xs(n). 
Listen to the spectrum aliasing 
We will now perform two simple experiments that will allow us to "hear" the 
spectrum aliasing phenomenon. 
The first one simply consists of recording speech at a frequency of 8 kHz, 
then to take one out of every two samples, and to listen to the signal obtained 
at a frequency of 4 kHz. Type the following program: 
%===== speechalias.m 
%===== .mat file containing the speech signal x 
load speechsig 
Fs=8000 ; N=length(x) ; xr=x(1 :2:N) ; 
soundsc(x,Fs); pause; soundsc(xr,Fs/2) 
"Hissing" noises can be heard in the restored signal. We will come back to 
this example in exercise 4.14 and give the proper method for undersampling a 
signal while avoiding aliasing. 

76 Digital Signal and Image Processing using MATLAB® 
In the second example, we create a digital signal from the sampling of a 
signal defined by its continuous-time expression. Instead of working the way 
the sampling theorem tells us to, we are going to cause spectrum aliasing. 
Consider the continuous-time signal given by 2.12: 
x(t) = Acos(8(t)) with t E IR 
(2.12) 
The time dependent function defined by: 
(2.13) 
is called the instantaneous frequency. If x(t) is a Fo frequency sine signal, the 
instantaneous frequency is equal to Fo. In general, x(t) is said to be frequency 
modulated. Unfortunately, there is no simple expression for the spectrum of 
x(t). 
However we can suspect that, for the most part, the energy can be 
located in the frequency band scanned by the function Fi(t). 
Consider, for example, the case of an instantaneous frequency that varies 
linearly with time, which can be written: 
where A is expressed in Hz/s. By observing the signal over long enough periods 
of time, between instants 0 and T, the frequency should vary linearly between 
Fo and Fl = Fo + AT. We will now determine the expression of x(t). By 
integrating Fi(t) (2.13), and by assuming that Fi(O) = 0, we get: 
8(t) = 27rFot + 7rAt2 
The following program creates the samples taken at a frequency of Fs 
8,000 Hz of the signal x(t), for a period of T = 2 s, with Fo = 1,000 Hz and 
for a value of A that we will change, so as to sweep frequency ranges of varying 
widths: 
%===== modulfreq.m 
lambda=1000; 
% parameter (1000 ou 2000) 
Fs=8000; 
% sampling Freq. 
FO=1000; 
% initialization Freq. 
T=2; 
% observation time 
it=(0:Fs*T-1)/Fs; % time Vector 
theta=2*pi*FO*it+pi*lambda*(it .-2); 
x=cos(theta); 
soundsc(x,Fs) 
% result 
The soundsc(x, Fe) function reconstructs a continuous-time signal from 
samples x at a sampling frequency Fs, and sends it to the calculator's audio 
output. 

Chapter 2 - Discrete Time Signals and Sampling 77 
First listen to the signal obtained for A = 1,000 Hz/s, as it has been defined 
in the example. You can hear a sound going from a low-pitched frequency to 
a high-pitched frequency, because the instantaneous frequency varies linearly 
from Fo = 1,000 Hz to Fl = 3,000 Hz. 
Now listen to a signal for A = 2,000 Hz/s. This time, a low-pitched sound 
can be heard, "rising" to a higher frequency, and finally going back down to 
a low frequency. This result is rather unexpected, since the instantaneous fre-
quency varies linearly from Fo = 1,000 Hz to Fl = 5,000 Hz. This is simply 
the consequence of the spectrum aliasing phenomenon. Because the sampling 
frequency is equal to 8,000 Hz, the frequencies beyond Fs /2 = 4,000 Hz are 
aliased in the (0 Hz - 4,000 Hz) band. This means that, during reconstruction 
(see paragraph 2.1.3), when the instantaneous frequency varies between 4,000 
Hz and 5,000 Hz, the soundsc (x, Fe) function sees the signal as a frequency 
varying from 4,000 Hz to 3,000 Hz. 
Interpolation and visual impressions 
As we are now going to see, a sampling frequency equal to or slightly greater 
than the Nyquist frequency leads to a continuous-time signal that cannot be 
clearly identified simply by looking at it. This means that the eye, or more 
precisely the brain, is a rather poor interpolator. 
To observe this effect, consider a sine function with a frequency of 80 Hz 
and a first sampling frequency of Fsl = 200 samples per second. This sampling 
frequency is greater than the Nyquist frequency, equal only to 160 Hz, and 
therefore is high enough to reconstruct the sine function. Now consider the 
same signal sampled at a frequency of Fs2 = 1,500 samples per second. The 
following program creates and plots the sequences of values corresponding to 
these two sampling frequencies, over a period of 60ms: 
%===== sine80.m 
fO=80; 
% sine Freq. 
obsdur=0.06; 
% observation Time 
Fsl=200; Fs2=1500; 
% sampling Freq. 
nl=round(obsdur*Fsl); n2=round(obsdur*Fs2); 
tpsl=[O:nl-l]/Fsl; tps2=[0:n2-1]/Fs2; 
sl=3*sin(2*pi*fO*tpsl); s2=3*sin(2*pi*fO*tps2); 
subplot(211); plot(tpsl,sl, 'x'); grid 
subplot(212); plot(tps2,s2, 'x'); grid 
The resulting plot is shown on Figure 2.8. 
As you can see, the continuous-time sine function is not recognizable from 
the top figure, corresponding to the 200 Hz sampling. On the other hand, the 
bottom figure, corresponding to the 1,500 Hz sampling, gives a very good visual 
impression of a sine function. 

78 Digital Signal and Image Processing using MATLAB® 
=1 t
'- *Xx
}I
x
l ~FE
j 
o 
0.01 
0.02 
0.03 
0.04 
0.05 
0.06 s 
3 ~*,--~--~~~--
__ ~------~~----~~---, 
2 x- -_x_ ---- ~ ---x- --x--1------x - - ~ -------- -~:
- -~ --------:-x---x _ ---
1 
x
' 
'x
' 
x, x 
' 
x 
o :::::)(::: 1 :: :
::::
~ 1::::: x:::: j ~ ::::: -: -:---x--:::: t ::::: x::-
x __ : _)< 
'x
' 
-----x 
x 
-1 
x : x ------><;-
----
~ -x-
-- x --'-----x ---x: 
x 
=~
--x 
:x--~-
~
-x- xx
:--x 
o 
0.01 
0.02 
0.03 
0.04 
0.05 
0.06 s 
Figure 2.8 - Drawing of the 80 Hz sine function, sampled at a frequency of 200 Hz 
(top graph) and at a frequency of 1,500 Hz (bottom graph) 
It should be pointed out that if the sampling frequency is chosen to be 
much greater than the number of pixels on the screen, the dots on the graph are 
displayed as an almost "continuous-time" trajectory. An interpolation function 
can then be used to build the trajectory. 
Exercise 2.1 (An illustration of the sampling theorem) (see p. 399) 
Consider the function x(t) = sin(27rFot), sampled at a frequency of Fs: 
1. what signal results from perfect reconstruction for Fo = 200 Hz and 
Fs = 500 Hz? 
2. a 200 Hz sine function is sampled at a frequency of Fs = 250 Hz. What 
signal is obtained by using the ideal formula for perfect reconstruction? 
3. write a program: 
- displaying a 200 Hz sine function, 
- displaying 10 of its samples taken at the frequency FSl 
- displaying the reconstructed signal (expression 2.4). The reconstruc-
tion will be performed using the filter function in the following 
way: 
xti = filter(hn,l,xtr) 
where hn is the sample sequence h(nTs) of h(t) (expression 2.8) and 
xtr the sample sequence of the sine function completed with zeros, 
- and checking the accuracy of the results for questions 1 and 2. 

Chapter 2 - Discrete Time Signals and Sampling 79 
2.1.3 Digital-to-analog conversion 
Reconstructing a continuous-time analog signal from a numerical sequence is 
done by using a Digital-to-Analog Converter, or DAC. The DAC blocks the 
value of x( nTs) during the time interval (nTS) (n + 1 )Ts) where Ts = 1 I Fs. The 
converter is called a Zero-Order Hold (ZOH). 
xo(t), the ZOR's output signal, is shaped like a "staircase". Its expression 
is: 
xo(t) = L x(nTs) ho(t - nTs) = L x(nTs)l(t E (nTs, nTs + Ts)) 
n 
n 
Compared to the original signal x(t), the signal xo(t) has some of its power 
in high frequencies due to the presence of steep transitions. The frequential 
study clearly shows this behavior: the Poisson formula 2.1 gives us the following 
expression for the Fourier transform of xo(t): 
with: 
+00 
Xo(F) = Ho(F) L X(F - niTs) 
n= -oo 
sin(7rFTs) 
. FT 
. FT 
Ho(F) = 
e- J7r 
s = sinc(FTs)e-J7r 
s 
0.8 
0.6 
0.4 
0.2 
7rFTs 
side lobes 
OL-~~~~~-L~~L-~~-L~~~~~~~~~ 
-50 
-40 
-30 
-20 
-10 -40 4 
10 
20 
30 
40 
kHz 
1 r---~--~--~--~---r 
__ --~--~--~--~---' 
I 
I 
I 
' 
0.8 
-----
~ ------~------~------~---
i 
: 
' 
0.6 ------------ , ----- , -----r -
" 
, 
\ 
I 
I 
I 
I 
~ ----r------r-----
~ ------T------
" 
I 
I 
, 
, 
I-
-- r ----- , 
0.4 
30 
40 
kHz 
Figure 2.9 - Spectrum modulus at the ZOH's output 
The shape of IXo(F)1 (Figure 2.9) shows two kinds of distortion when com-
paring the original signal x(t) with the reconstructed signal xo(t): 
1. the first one is due to the presence of the term Isinc(FTs)I which deforms 
the original spectrum in the band (-FsI2, FsI2); 

80 Digital Signal and Image Processing using MATLAB® 
2. the second one has to do with the spectrum's periodization and the pres-
ence of side lobes for the function Isinc(fTs) I beyond Fs/2, and particu-
larly in the band (Fs/2, Fs) corresponding to the first side lobe. 
For example, in the case of an "audio application" sampled at a frequency 
of Fs = 8,000 Hz, these components appear between 4,000 Hz and 8,000 
Hz and are perfectly audible. One possible solution is to apply a low-pass 
filter to the ZOH's output. 
In general, the greater the sampling frequency (compared to the band of the 
signal x(t)), the weaker these distortions will be. This is why for some devices, 
the ZOH is preceded by an interpolation operation. This processing technique 
is explained in section 4.8. 
2.2 Plotting a signal as a function of time 
The sampling theorem makes it possible to go from a continuous-time signal 
to a sequence of values obtained by using a filter with a gain equal to 1 in the 
band (-Fs/2,Fs/2), followed by a sampling procedure at a frequency of Fs. 
From now on, and except if specified otherwise, we will only be considering 
discrete-time signals, that is to say sequences of values, that we will study 
plotted as functions of time and frequency. These two kinds of plotting, which 
are equivalent by definition, are nevertheless both useful when interpreting the 
phenomena we are dealing with. 
Digital signals 
The first model, called the temporal model, for a digital signal, is made up of 
the values of its samples. As is the case for continuous-time, the support of 
these sequences can be limited to N+. 
Definition 2.3 (Causal and anticausal signals) The causal signals x(n) 
are such that x(n) = 0 for n < o. If all the elements of the sequence are 
equal to zero for n ;::: 0, the sequence is said to be anticausal. 
In the same way, some "basic" signals have to be considered to come up 
with an ideal model for certain of the observed signals. This is the case for 
example for a sine voltage or for very short pulses used to characterize the 
behaviour of certain "systems". Here is an (incomplete) list of some of these 
signals: 
- the unit pulse, or unit impulse, defined by: 
i5 n = {I for n = 0 
( ) 
0 otherwise 
(2.14) 

Chapter 2 - Discrete Time Signals and Sampling 81 
- the unit step defined by: 
{
I 
for n > 0 
u( n) = 
0 other;ise 
- the sign function defined by: 
sign(n) = 2u(n) - 1 = { ~i 
for n;::: 0 
for 
n < 0 
- the gate function or rectangle function defined, for N ;::: 0, by: 
(2.15) 
(2.16) 
rectN(n) = u(n) _ u(n _ N) = {01 for n E. {O,'" ,N -I} (2.17) 
otherWIse 
- the sine function defined by: 
x(n) = xosin(27rfon+¢) 
- the complex exponential defined by: 
x( n) = Xo exp(2j7r fon) 
- the truncated sine function defined by: 
x(n) = Xo sin(27rfon + ¢) x rectN(n) 
- the truncated complex exponential defined by: 
(2.18) 
(2.19) 
(2.20) 
x(n) = Xo exp(2j7r fon) x rectN(n) 
(2.21) 
A discrete-time signal will be referred to as either the set {x( n)} of its 
values, or by its generic element x(n) or x n , depending on the context. 
EXAMPLE 2.3 (Basic signals) 
Write a program designed to create and plot basic signals. 
HINTS: the program basicfct.m plots a few basic signals, which 
are shown in Figure 2.10. 
%===== basicfct.m 
N=20; mtime=[0:N-1); 
impuls=eye(l,N); 
% unit pulse 
untstep=ones(l,N); 
% unit step 
fO=.l; fsin=sin(2*pi*fO*mtime); % sinusoid 
P=3; tps2=[-N:N); 
% 2P+1 sample recto 
rectP=[zeros(l,N-P) ones(1,2*P+1) zeros(l,N-P)); 
subplot(221); plot(mtime,impuls, 'x'); grid 
subplot(222); plot (mtime,untstep, 'x'); grid 
subplot(223); plot(mtime,fsin, 'x'); grid 
subplot(224); plot(tps2,rectP, 'x'); grid 

82 Digital Signal and Image Processing using MATLAB® 
1~--~--~--~---' 
0.8 ------r------r------r------
, 
, 
, 
0.6 
------
~ ------
~ ------
}
-
0.4 
0.2 
, 
, 
, 
, 
, 
, 
------~------~------~------
, 
, 
, 
, 
, 
, 
o 0'*"***'5 "*"'**10**'-15**"*"20 
1 ,.....,,,,.....--,,...,--------,--: -,,,'"',",.----;:----, 
, 
, 
, 
0.5 
<'-- - !<- ~ - - - - - - ~ <'-- - !<- ~ - - - - --
, 
, 
, 
o ------* -----* -----*--
-0.5 
-1 
, xx ' 
, xx 
o 
5 
10 
15 
20 
2 .--...,--------,---~----, 
1.5 
0.5 
------c------c------.------
, 
, 
, 
, 
, 
, 
, 
, 
, 
o O'-----~--~--~----' 
5 
10 
15 
20 
1 .--...,----~~~~----, 
0.8 
0.6 
0.4 
0.2 
------~------~------~------
, 
, 
, 
, 
, 
, 
, 
, 
, 
------,------,------,------
, 
, 
, 
, 
, 
, 
------~------~------~------
, 
, 
, 
, 
, 
, 
, 
, 
, 
------r------r------r------
, 
, 
, 
, 
, 
, 
~2~0~~-~10~~0~~10~~20· 
Figure 2.10 - Basic functions 
2.3 Spectral representation 
The main goal in the spectral study of a signal is to find out how to decompose 
this signal as a sum of sines. To evaluate the importance of the exp(27r fan) 
component with the frequency fa in the x(n) signal, an idea would be to cal-
culate: 
QUo) = L x(n) exp( -27rfon) 
nEZ 
which can be interpreted as a quantity that measures how similar the sequences 
{x( n)} and {exp(27r fan)} are. This sum can be seen as the scalar product 
between the sequence {x(n)} and the sequence {exp(27rfon)}. It is exactly 
what the Discrete- Time Fourier Transform (definition 2.2) does, as well, in 
fact, as the Fourier transform for continuous-time functions. 
2.3.1 Discrete-time Fourier transform (DTFT) 
The sampling period Ts appears in the DTFT's expression in definition 2.4. 
Definition 2.4 (DTFT) The discrete-time Fourier transform of a sequence 
{x(n)} is the function of f E IR, periodic with period 1, defined by: 
+00 
XU) = L x(n) exp( -2j7rnf) 
(2.22) 
n= -CX) 
As you can see, we need only impose FTs = f and replace x(nTs) by x(n) 
to go from 2.1 to 2.221. 
1 X(F) , which refers to the FT in 2.1 must not be confused with XU), the DTFT. 

Chapter 2 - Discrete Time Signals and Sampling 83 
Definition 2.4 calls for a few comments: it can be proven (see ref. 
[9]) 
that, if {x( n)} is summable CLn Ix( n) I < +00) , the series (2.22) converges uni-
formly to a continuous function X(J). However, if {x(n)} is square sum mabie 
(Ln Ix(n)12 < +00) without having a sum mabie modulus, then the series con-
verges in quadratic mean. There can be no uniform convergence. 
Because of its periodicity, the DTFT is plotted on an interval of length 1, 
most often the intervals (-1/2, + 1/2) or (0,1). 
EXAMPLE 2.4 (DTFT of the rectangle function) 
Let rect N (n) be the signal given by 2.17. Its DTFT is: 
N- l 
X(J) 
L e-2j7rnj = 1 + ... + e- 2j7r(N - 1lj 
(2.23) 
n = O 
{ 
N 
for f = 0 mod 1 
1- e-2j7rNj = e_ j7r(N_ 1ljsin(N7rj) 
1 -
e - 2j7rj 
sin(7rj) 
for f i= 0 mod 1 
The e - j7r(N-l)j is of modulus 1, and only has influence on the phase of 
X (J) (Figure 2.11). 
9 
8 
7 
6 
~ 
main lobes , ~~~ 
__ cc3->~_ 
~ 
____ .J _______ I _______ J. _______ I ____ _ 
. 
. 
5 -
----
~ --
____ l _______ ~ --
. 
. 
_____ I _______ L _____ _ 
-
--------------
4 
---- ~ -------'-------.: -------:- ----- - !. - - - - - - -:- - - - - - - ~ - - - - --
3 __ ; _ 
side lobes __:_ 
_ _
: __
~ _ 
2 
. N=lO 
o L-~_L_L~~~~_L~~L_~J__L~~L_~~~~ 
o 
0.2 
0.4 
0.6 
0.8 
1.2 
1.4 
1.6 
1.8 
Figure 2.11 - Modulus of the DTFT of the rectangle signal for N = 10 
Isin(N7rf)/ sin(7rf)1 shows one main lobe, with a width of 2/ N and side 
lobes with a width l/N. We will often deal with this signal again, particularly 
when observing a signal assumed to be of infinite duration, over a finite number 
N of values, since it amounts to multiplying it by a rectangle with a duration 
of N. 
Starting off from X(J), how can we go back to x(n)? One possible answer 
is given in the following result. 

84 Digital Signal and Image Processing using MATLAB® 
Theorem 2.2 (Inverse DTFT) If X(J) is a periodic function with period 
1, and if Jo
1 lX(J)I2df < +00, then X(J) = L nx(n)e-2j7rnf, where the x(n) 
coefficients are given by: 
/
1/2 
x(n) = 
X(J)e2j7rnf df 
-1/2 
(2.24) 
Relation between the FT and the DTFT 
First let us once again consider the reconstruction formula of a real signal x(t) 
from its samples xs(n) : 
(2.25) 
Fs refers to the sampling frequency and B to the bandwidth of the signal 
x(t). We will assume Fs ~ 2B. The frequency, expressed in Hz, will be denoted 
by F, the normalized frequency (no dimension) by f and the sampling period 
by Ts = llFs. 
In practice it is often needed to find the Fourier transform using the DTFT 
of xs(n), the frequency Fs and the band B. We get: 
+00 
X(F) = l(F E (-B, B)) L X(F - kFs) 
k = - oo 
The Poisson formula 2.1 leads us to: 
+00 
X(F) 
Tsl(F E (-B,B)) L xs(n)e2j7rnF/Fs 
n = - CX) 
where Xs(J) refers to the DTFT of xs(n). What should be remembered is that 
the FT of x(t) is obtained: 
- by calculating the DTFT of xs(n); 
- by dividing the amplitude by Fs; 
- by multiplying the frequency axis by Fs; 
- and by limiting the frequency band to the interval (-B, B). 
Conversely, the DTFT of xs(n) is obtained: 
- by calculating the FT of x(t) ; 
- by multiplying the amplitude by Fs; 

Chapter 2 - Discrete Time Signals and Sampling 85 
- by dividing the frequency axis by Fs; 
- and by periodizing with period 1. 
+00 
X s(J) = Fs L X ((J - k)Fs) 
(2.26) 
k=-oo 
The value of B is often omitted, and implicitly B = Fs / 2. For example, 
the MATLAB® function soundsc(x,Fs) , produces the signal in the band 
(-Fs /2 , Fs / 2) using the sequence x and the value Fs for the sampling frequency. 
The discrete-time Fourier transform's main properties are summarized in 
Appendix A2. 
As in the continuous-time case, we have the Parseval's formula: 
+00 
1/2 
L Ix(nW = J IX(JWdf 
n = -oo 
-1/2 
(2.27) 
and the conservation of the dot product: 
+00 
1/2 
L x(n)y*(n) = J X(J)Y*(J)df 
n = -oo 
-1/2 
(2.28) 
Because the left member of 2.27 is, by definition, the signal's energy, IX (J) 12 
represents the energy's distribution along the frequency axis. It is therefore 
called the energy spectral density (esd), or spectrum. In the literature, this last 
word is associated with the function IX (J) I. If X (J) is included, this adds up 
to three definitions for the same word. But in practice, this is not important, 
as the context is often enough to clear up any ambiguity. It should be pointed 
out that the two expressions IX(J)I and IX(JW become proportional if the 
decibel scale is used, by imposing: 
(2.29) 
EXAMPLE 2.5 (Inverse DTFT of a rectangle) 
Let X(J) be a periodic function with period 1 and X(J) = 1(J E (-b,b)) 
with 0 < b < 1/2: 
1. determine the sequence {x(n)} that has X(J) as its DTFT; 
2. using this result, find the sequence y(n) that has Y(J) = (X(J - fa) + 
X(J + fo))/2 as its DTFT. 

86 Digital Signal and Image Processing using MATLAB® 
HINTS: 
1. By using relation 2.24, we get: 
x(n) = lb e2j7rnj df = _1_ [e2j7rnjlb = sin(2'mb) 
-b 
2J7rn 
-b 
7rn 
{x(n)} is a non-causal sequence consisting of an infinity of 
terms; 
2. because of the linearity and modulation properties: 
e2j7rnjo + e-2j7rnjo 
y(n) = x(n) 
2 
= x(n) cos(27r fon) 
The sequence y(n) also has an infinity of non-zero values. 
Exercise 2.2 (Time domain hermitian symmetry) (see p. 400) 
Consider a signal x( n) such that x( n) = x* ( -n). Notice that x(O) is real: 
1. show that its DTFT X (j) is real; 
2. determine the expression of the DTFT Y(j) of the sequence defined by: 
{ 
x(n) 
for 
n > 0 
y(n) = 
x(0)j2 for 
n = 0 
o 
otherwise 
Using Y*(j), find the relation between X(j) and Y(j). 
2.3.2 Discrete Fourier transform (DFT) 
Definition of the discrete Fourier transform 
A computer calculation of the DTFT, based on the values of the samples x(n), 
imposes an infinite workload, because the sequence is made up of an infinity of 
terms, and because the frequency f varies continuously on the interval (0,1). 
This is why, digitally speaking, the DTFT does not stand a chance against the 
Discrete Fourier Transform, or DFT. The DFT calculation is limited to a finite 
number of values of n, and a finite number of values of f. 
The digital use of the DFT has acquired an enormous and undisputed prac-
tical importance with the discovery of a fast calculation method known as the 
Fast Fourier Transform, or FFT. The algorithm for the FFT can be found in 
paragraph 2.3.3. 
Consider the finite sequence {x(O), ... , x(P - I)}. Using definition 2.22, 
its DTFT is expressed X(j) = L~
~ ~ x(n)e-2j7rnj where f E (0,1). In order 
to obtain the values of X(j) using a calculator, only a finite number N of 

Chapter 2 - Discrete Time Signals and Sampling 87 
values for f are taken. The first idea that comes to mind is to take N values, 
uniformly spaced-out between 0 and 1, meaning that f = kiN with k E {O, 
... , N - I}. This gives us the N values: 
P-l 
X(kIN) = L x(n)e-2j7rnk/N, k E {O, .... , N - I} 
(2.30) 
n=O 
In this expression, P and N play two very different roles: N is the number 
of points used to calculate the DTFT, and P is the number of observed points 
of the temporal sequence. As we will see later on, N influences the precision 
of the plotting of X(f), whereas P is related to what is called the frequency 
resolution. 
In practice, P and N are chosen so that N ~ P. We then impose: 
x(n) = { ~(n) 
fornE{O, . . . ,P-l} 
for n E {P, . . . , N - I} 
Obviously: 
P-l 
N-l 
X(kIN) = L x(n)e- 2j7rnk/N = L x(n)e-2j7rnk/N 
n=O 
n=O 
Because the sequence x(n) is completed with (N - P) zeros, an operation 
called zero-padding, in the end we have as many points for the sequence x(n) 
as we do for X (k I N). Choosing to take as many points for both the temporal 
sequence and the frequential sequence does not restrict in any way the concepts 
we are trying to explain. This leads to the definition of the discrete Fourier 
transform. 
Definition 2.5 Let {x( n)} aN-length sequence. Its discrete Fourier transform 
or DFT is defined by: 
N-l 
X(k) = L x(n)WRfk, 
k E (0,1, ... N - 1) 
(2.31 ) 
n=O 
where WN = e-2j7r/N 
(2.32) 
is an N -th root of unity, that is to say such that wfj = 1. The inverse formula, 
leading from the sequence {X(k)} to the sequence {x(n)}, is: 
N -l 
x(n) = ~ L X(k)WNnk 
(2.33) 
k=O 
To show 2.33, you need to calculate its second member by replacing X(k) 
by 2.31 and using the following equality: 
1 N -l 
{ 
1 
g(n) = N L e2j7rkn/N = 
0 
k=O 
for n = 0 mod N 
otherwise 
(2.34) 

88 Digital Signal and Image Processing using MATLAB® 
With MATLAB@, the fft function uses the fast calculation algorithm for 
the DFT. This is the proper syntax: 
xf=fft(xt,N) 
The resulting N-length sequence xf is the DFT of the P-length (P :::; N) 
sequence xt (2.30). If parameter N is missing, it is chosen equal to P. Although 
the function fft allows the calculation of the values of the DFT for any number 
N of frequency points, N should be taken equal to a power of 2 to reduce the 
computation time2 . 
Exercise 2.3 (Comparing computation speeds) (see p. 401) 
Write a program that compares the respective speeds of the direct calcula-
tion using the expression L: x(n) exp( -2j7rnf) and the FFT calculation. Look 
into the use of the functions tic, toc, etime, etc. for purposes of measuring 
computation times. 
Use of the DFT to plot and study the properties of the DTFT 
As it was said before, the DFT is used to digitally determine the values of the 
DTFT. The more precise the plotting of the DFT is, the higher the number of 
frequency points has to be. 
Exercise 2.4 (Spectrum of the triangle function) (see p. 402) 
Consider the triangle function defined by sig= [1: P P-1: -1: 0] . This function 
is real. 
Using the FFT, digitally verify hermitian symmetry properties by 
plotting: 
1. the modulus and the phase of its DTFT for P = 10, 
2. and the imaginary and real parts of the DTFT. 
EXAMPLE 2.6 (Time delay properties) 
Let {x(n)} be a zero signal outside the {-no, ... , nd interval where no 
and nl are two positive integers, and let y(n) be defined by y(n) = x(n - no), 
obtained by a time-shift of no samples: 
1. determine the DTFT of {x(n)}, expressing it as a function of the DTFT 
of {y(n)}; 
2. write a program that checks the previous result for no = 5. In order to 
do this, set {x(n)} equal to 1 between -5 and 5, and y(n) = x(n-5). To 
digitally evaluate the DTFT over 256 frequency points regularly spaced-
out in the (0,1) interval, the fft function is used. 
2The N=nextpo,,2 (P) function returns the closest power of 2 greater than P. 

Chapter 2 - Discrete Time Signals and Sampling 89 
HINTS : 
1. We have: 
nl 
nl +no 
X(J) 
L x(n)e- 2j7rnf = e2j7rnof L x(k - no)e-2j7rkf 
n= -no 
k=O 
n l +nO 
e2j7rnof L y(k)e-2j7rkf = e2j7rnofY(J) 
k=O 
2. X(J) = sin(57f"j) / sin(7rJ). This means that to get the DTFT 
of { x (n)} , all you have to do is calculate the DTFT of {y(n)} 
and multiply it by elOj7rf . 
The following program can be used to verify this: 
%===== shiftf.m 
Lfft=256; 
% length equal to a power of two 
f=(O:Lfft-1)/Lfft; % normalized freq. 
nO=5; n1=5; yt=ones(n1+nO+1,1); 
Yf=fft(yt,Lfft); 
% DFT of yen) 
Xf=Yf .* exp(2*j*pi*5*f'); 
subplot(211); plot(real(Xf)); grid 
%==== imago part roughly zero (temporal symmetry) 
subplot(212); plot(imag(Xf)); grid 
Properties of the DFT 
The properties of the DFT show strong similarities with those of the DTFT. 
However, there is an essential difference. In the formulas associated with the 
DFT, all the index calculations are done modulo N. The discrete Fourier 
transform's main properties are summarized in Appendix A3. 
Exercise 2.5 (Circular convolution of the rectangular signal) (see 
p. 402) 
Consider the rectangular signal x(n) = 1 (n E {O, ··· , 7}) . Compare and 
explain the effects of the following commands (if it is the function used to 
obtain the inverse DFT): 
II x=ones (1,8); xs=fft (x); xs=xs . * xs; ifft (xs) 
and: 
II x=ones (1,8); xs=fft (x, 16); xs=xs . * xs; ifft (xs) 
Exercise 2.6 (Delay) (see p. 403) 
Because of the time shift property, in order to get the L points DFT of a signal 

90 Digital Signal and Image Processing using MATLAB® 
that has non-zero values between -no and nl, the sequence's DFT must be 
calculated on N points and then the delay has to be taken into account, by mul-
tiplying the result, term-by-term, by the complex exponential exp(2j7rnok/L) , 
where k E {O, ... , L -I}. This exercise introduces a different method to achieve 
the same result. 
Let x(n) be a signal equal to zero for n outside the set of indices {-no, ... , 
nd, where no and nl are positive, and let y(n) be the signal defined by: 
for n E {O, . . . ,nl} 
y(n) = 
0 
for n E {nl + 1, ... ,L - no - I} 
{ 
x(n) 
x(n - L) 
for n E {L - no, ... , L - I} 
with L > no + nl. One way of seeing it is to imagine the values of x(n) with 
negative indices being translated to the right by L points. 
1. calculate the DTFT of y( n) on L points. Conclude; 
2. let x(n) be a signal equal to 1 between -5 and 5, apply the previous result 
to a program designed to calculate the DTFT of x( n) on 256 points. 
The point of exercise 2.6 is to explain that, in order to determine the DFT 
of a sequence x(n) with a length of N, for L points, with L > N, you need to 
calculate the DFT of the sequence: 
y(n mod L) = x(n) 
meaning the sequence whose indices are calculated modulo L. 
EXAMPLE 2.7 (Calculating the IDFT using the DFT) 
Let X(k) be the DFT of x(n), and let y(k) = jX*(k) be the sequence 
resulting from the permutation of the imaginary parts and the real parts of 
X(k). In other words, y(k) = XI(k) + jXR(k), where XR(k) and XI(k) refer 
to the real and imaginary parts of X(k) respectively. 
Calculate the DFT of y(k). Use the result to determine a method for 
calculating the inverse DFT of a sequence using a direct DFT function with 
the real and imaginary parts as its input, and the real and imaginary parts of 
the IDFT as its output. 
HINTS: applying the definition of the DFT to the sequence y(k), 
we get: 
N-l 
N-l 
Y(n) 
L y(k)e-2j7rnk/N = j L X*(k)e-2j7rnk/N 
k=O 
k=O 
j (~ 
X(k)"j>nk/N) , 
j (Nx(n))' = N((xI (n)) + jxR(n)) 

Chapter 2 - Discrete Time Signals and Sampling 91 
This means that the use of the DFT function on the sequence 
jX*(k) leads to the reconstruction of the original sequence x(n) 
(multiplied by the factor N) with its real and imaginary parts 
switched. 
Let us now assume that we have at our disposal a direct DFT that 
has two arrays as its input, one for the real part, and the other 
for the imaginary part of the signal we wish to transform, and that 
has two arrays as its output, one for the real part, and the other 
for the imaginary part of the transform, according to the following 
synopsis: 
(XR,XI)= dft(xR,xI) 
To go from this function to the inverse DFT, all we have to do is set 
the transform as the input, by switching the roles of the real and 
imaginary parts. The resulting output is the inverse DFT except 
for a factor l iN. This can be expressed as follows: 
(xI,xR)= dft(XI,XR) 
In MATLAB®, the fit function, used to directly calculate the DFT, 
has an array of complex numbers as its argument, which means that 
it is not possible to apply the previous result. MATLAB®'s ifft 
function, in order to calculate the inverse DFT from the direct DFT, 
uses the conjugation property: 
(
N -l 
) * 
x(n) = ~ t; X*(k)e-2j7rkn/N 
The inverse DFT is the conjugate of the conjugate's direct DFT. 
This can be written x=conj (fft (conj (X))) IN where fft is the 
function calculating the DFT (see next paragraph). 
2.3.3 
Fast Fourier transform 
The fast Fourier transform, or FFT, first published in 1965 by J. W. Cooley 
and J. W. Tuckey [8], is a fast DFT calculation technique. The basic algorithm, 
many versions of which can be found, calculates a number of points N, equal to 
a power of 2, and the time saved compared with a direct calculation is roughly: 
gain = N I log2(N) 
To get a better idea, if N = 1,024, the FFT is about 100 times faster than 
the direct calculation based on the definition of the DFT. 

92 Digital Signal and Image Processing using MATLAB® 
Consider the simple case N = 8. Using the notation W N = exp( - 2jn / N), 
the DFT can be expressed as the sum of a term related to even rank indices 
and of a term related to odd rank indices: 
(x(O) + x(2)Wlk + x(4)Wr + x(8)W~k) 
+W; (x(l) + x(3)Wr + x(5)Wr + X( 7)W
~k) 
(x(O) + x(2)W; + x(4)Wr + x(8)Wfk) 
+W; (x(l) + x(3)W; + x(5)Wr + x(7)Wfk) 
(2.35) 
A length 8 DFT is thus replaced by two length 4 DFTs. By iterating the 
process, the DFT's length is divided by two at every step. It takes 10 steps to 
go from a length 1024 DFT to length 2 DFTs. 
Evaluating the number of operations 
As it can be seen in expression 2.35, a length 8 FFT was replaced, in the first 
step, by two length 4 FFTs. We have to include 8 complex multiplication-
addition operations (called MA C operations3 ). This result can easily be gener-
alized for a length N DFT, where N equals a power of 2: if CN is the number of 
MAC operations for the Nth step, CN = 2CN / 2 +N leads us to the complexity: 
CN = N x log2(N) 
We also have to include an index calculation phase needed to access the 
data. The indices of the terms Xn appear in an order corresponding to the 
inverted binary code of n, as it is indicated in the following table. This is 
called the bit reverse access. 
Rank 
Binary Coding 
Reversal 
Element 
0 
000 
000 
0 
1 
001 
100 
1 
5 
101 
101 
5 
6 
110 
011 
3 
7 
111 
111 
7 
COMMENTS: 
- first level processing is limited to adding and subtracting. The second 
level could also be dealt with in this particular way. 
In most of the FFT calculation programs, using these simplifications al-
lows you to save a little time; 
3The acronym MAC is in reference to the Multiplication-ACcumulation operation that 
can be found in the s = s + aibi algorithm, used to calculate a sum of products 2:i aibi. 

Chapter 2 - Discrete Time Signals and Sampling 93 
- processors designed for signal processing have a particular addressing 
mode, exempting them from actually calculating the indices. The ad-
dressing mode is called bit reverse addressing. 
Exercise 2.7 (FFTs of real sequences) (see p. 403) 
Consider the real sequence x(n), with n E {O, . . . , N - I}. Let X(k) be its 
DFT. The complex sequence y(n) will be defined by y(n) = x(2n) + jx(2n+ 1). 
Let Ak and Bk be the DFTs of the sequences x(2n) and x(2n + 1) respectively. 
By linearity, y(n) has the sequence Y(k) = A(k) + jB(k) as its DFT (notice 
that A(k) and B(k) may be complex. Therefore A(k) and B(k) are not the 
real and imaginary parts of Y(k) respectively): 
1. by noticing that x(2n) is the real part of y( n) and is therefore equal to 
(y(n) + y*(n))/2, express A(k) using the term Y(k). Do the same for 
B(k); 
2. find a method similar to the decomposition given by expression 2.35 to 
show that X(k) can be expressed as a function of A(k) and B(k). Using 
this result, write an algorithm that calculates the DFT of a real length 
N sequence based on a complex length N /2 FFT algorithm; 
3. compare the complexities of the previous algorithm and the complex 
length N FFT algorithm. 
Exercise 2.8 (Using the FFT) (see p. 405) 
What is the purpose of the following program: 
II 
plot (fft ([0 1],128)) 
set(gca, 'AspectRatio' ,[1 1]); % or: axis('square') 


Chapter 3 
Spectral Observation 
The purpose of this chapter is to introduce the reader to the two following 
fundamental concepts: 
- the accuracy of the frequency measurement when the DFT is used to 
evaluate a signal's DTFT. As we will see, this accuracy depends on the 
number of points used to calculate the DFT; 
- the spectral resolution, which is the ability to discern two distinct frequen-
cies contained in the same signal. It depends on the observation time and 
on the weighting windows applied to the signal. 
3.1 
Spectral accuracy and resolution 
3.1.1 
Observation of a complex exponential 
To illustrate the DFT's use in signal spectrum observation, we will begin with 
a simple example. 
EXAMPLE 3.1 (Sampling a complex exponential) 
Consider the sequence resulting from the sampling of a complex exponential 
e2j7rFot at a frequency of Fs = l/Ts. If we set fa = Fo/Fs and assume it to be 
< 1/2, we get x(n) = e2j7rJon. 
1. determine the DTFT's expression for the sequence {x( n) = exp(2j7r fan)} 
where fa = 7/32 and n E {O, ... , 31}; 
2. using this result, find the DTFT's values at the points of frequency f = 
k/32, for k E {a, ... ,31}; 
3. using the fft command, display the modulus of the DFT of {x(n)}; 

96 Digital Signal and Image Processing using MATLAB® 
4. now let fa = 0.2. Display the modulus of the DFT of {x(n)} . How do 
you explain the result? 
HINTS : 
1. Starting off with definition 2.22 of the DTFT, we get: 
N- l 
X(f) 
L e2j7rfone- 2j7rfn 
n=a 
si~(N7r(f 
- fa)) e - j7r (N - l)(fo - f) 
(3.1) 
sm(7r(f - fa)) 
Because a finite duration sequence is all we have at our dis-
posal, the signal's DTFT shows ripples (ratio of the sines). 
IX(f)1 is plotted in Figure 3.1, illustrating this phenomenon. 
I 
I 
I 
, 
I 
I 
I 
------,------,------ r ------,------,------r------r ----,------T------
DTFf 
N = 32
--+---~--
~ --~--
~
--
~
--
~
--
~~ 
30 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
25 ------,------ ~ ------ r - - - - - -:- - - - - - ~ - - - - - - ~ - - - - --: 
I 
I 
I 
I 
I 
I 
I 
j 
I 
I 
I 
I 
j 
I 
I 
, 
20 
------
, ------~------
~ ------:------~------r------' 
I 
j 
I 
I 
I 
I 
j 
I 
I 
I 
I 
I 
I 
I 
15 ____________ ~ 
_____ : ______ ~ 
I 
, 
" 
, 
" 
, 
" 
10 
------
, ------~------r------:------~------r-----
~ 
----~------T------
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
5 
------------
~ 
----- r 
-----
: ------
~ 
----- ! --
: 
lobes
: 
: 
1/N=1/32 
Figure 3.1 - Modulus of the DTFT of the complex exponential fa 
N = 32 
This was achieved with the following program: 
%===== resol1 .rn 
7/ 32 with 
N=32; 
% number of points of the signal 
fO=7/32; 
% sine Frequency 
npts=512 ; 
% number of points of the frequency 
freqrnin=-0.5; freqrnax=0.5; 
pas=(freqrnax-freqrnin)/npts; 
f=[freqrnin:pas:freqrnax-pas] ; freqM=f-fO; 
%===== direct calculion of the DTFT 
fctM=sin(N*freqM*pi) .1 sin(freqM*pi) ; 
plot(f ,abs(fctM»; grid 
hold on; plot([fO fO] ,[0 35]) ; hold off 

Chapter 3 - Spectral Observation 97 
in which expression 3.1 is directly used; 
2. because the DFT corresponds to a sampling of the DTFT at 
frequency points k/ N, its values are usually different from zero, 
except if fa is an exact multiple of 1/ N, which is the case for 
fa = 7/32. The values of f are given by 0, 1/32, ... , 31/32. 
We then get X(k) = 32 if k = 7/32 and 0 otherwise. Type: 
%===== resol2.m 
N=32; L=32; freq=(0:L-1)/L; 
fO=7/32; xt=exp(2*j*pi*fO*(0:N-1»; 
xf=fft(xt,L); 
% calculation with the DFT 
plot(freq,abs(xf), 'x'); 
%===== the DTFT calculated by FFT is superposed 
L=512; freq=(O:L-l)/L; xf=fft(xt,L); 
hold on ; plot(freq,abs(xf), ': '); grid; hold off 
This leads to the graph in Figure 3.2. 
DFT:x 
35 ,-------~--------------------~------~--_. 
32 
____________ ; ,~ 
30 
," 
, 
25 
-----
~ ------
j \ ----
~ ------L-----;------i------~-----
~ ------i------
~~ :::::r::::ll;;('j' j.LL LL 
I 
I 
I 
, 
I 
I 
I 
I 
I 
I 
I 
I 
I 
! 
I 
I 
I 
I 
10 
-----
~ ----
~ - f --
\ ~ --
· ------~-----~------f------~-----
" ------f------
5 
______ : ____ !~
~ __ ~:~
_ ~ - ____ -:------J------~------~-----
~ ______ 1 _____ _ 
:- :'.: ::: :: ::!, -
: 
: 
: 
: 
' 
: 
o 1\/\ /",w \,' 1: 1: 
~~: :'/ \,'~~~
~
,....\ 
"', ,~
:
'" 
I 
... L ...... 
.J 
... 
~\.,
.J 
~ 
" '-I 
o 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
7/32 
Figure 3.2 - DFT of the complex exponential when fo is a multiple of 1/ N 
A "peak" is observed (this is actually the only non-zero value 
of X(k)), with an amplitude of 32 at a frequency of 7/32, 
and all the other frequency points have a zero spectrum. This 
result, which seems to agree with what would be expected of 
a infinite duration complex exponential with only one peak, is 
rather exceptional; 
3. Figure 3.3 results from imposing fa = 0.2, and as it clearly 
shows, it is quite different from a single "peak". An explanation 
of this can be found in paragraph 3.1.4 which deals with the 
subject of windowing. 

98 Digital Signal and Image Processing using MATLAB® 
35 DFT: x 
I 
32 30 ______ : ______ ,f, ______ : ______ : _____ _ ; __ ____ : ___ ___ : _____ _ ; ______ : _____ _ 
25i ./ D"t ,
, ; , ..... . 
20 
-----~-----
i 
i -----f------~-----
i ------f------
~ -----i------;------
: 
I 
I 
:
: 
: 
: :  
15 ------: 
:;\( --+-----: 
-----;------------i--
;-
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
10 
-----~-----
~ 
- ~ ----f------~-----
~ ------f------
~ -----i------;------
I 
I 
I 
I 
I 
I 
I 
5 
: 
,,: 
: ,)( :: 
: 
:: 
------
: xt~-
-::: x
: ~ -----:------: ------: ------:------:------: ------
o }<}',/\j ~ 
~ 
~ \: \) ,/~,x".x,,¥.,~,~ 
x ~.~,~ 
x x: X,)(vX.ix.,x.,x.,k x,)( , 
o 
0.1 
0.3 
0.4 1 10.5 
0.6 
0.7 
0.8 
0.9 
1 
10=0.2 
4+ 
IIL=IIN 
Figure 3.3 - DFT of the complex exponential when fa is not a multiple of 1/ N 
3.1.2 Plotting accuracy of the DTFT 
As we have just seen, the DFT is all we have at our disposal to plot the DTFT, 
or rather its modulus. The previous example is a good illustration of a number 
of important properties, the first of which is: 
For those frequencies that are not a multiple of 1/ L, where L is the 
number of calculated DFT points, a pure sine appears in the form of 
several non-zero values. The value with the highest modulus is close to 
the actual frequency. 
It should be noted that the gap between the frequency fa and the frequency 
associated to the maximum of the L = 32 values of the DFT's modulus is, in 
the worst case, equal to 1/ L. This leads us to the following rule: 
If L refers to the number of DFT calculation points, the frequency accu-
racy is equal to 1/ L. For signals sampled at a frequency of Fs (in Hz), 
this leads to a accuracy of Fs/ L Hz. 
3.1.3 Frequency resolution 
Accuracy must not be confused with the ability to distinguish (or separate) two 
close frequencies in a signal. One possible definition of the frequency resolu-
tion is the minimum difference between the two sine frequencies with different 
amplitudes, necessary to "observe" an attenuation greater than 3 dB between 
their two maximums. 

Chapter 3 - Spectral Observation 
99 
---- -----------t --3 dB 
-------
----
Figure 3.4 - Separation of frequencies 
As seen previously (Figure 2.11), limiting ourselves to handling a number 
of values no greater than N causes lobes to appear in the sine spectrum. The 
main lobe's width is equal to 2/N. This means that if x(n) contains two sines 
the frequencies of which are separated by less than 1/ N, their two main lobes 
will be so close that it will be difficult to distinguish them by observing the 
spectrum. This is even more true when their amplitudes are very far apart. 
Resolution and noise 
As we will see in Chapter 8, there is no point in talking about frequency reso-
lution in the absence of noise. Consider observations made without noise and 
assume we have 100 measurement points of the signal x(n) = Al cos(27rhn + 
¢d + A2 cos(27r12n + ¢2). To determine, from the values of x(n), the two fre-
quencies, the two amplitudes and the two phases, we have to solve the following 
system of six equations with six unknowns: 
! 
Al + A2 
Al cos(27rh + ¢d + A2 cos(27r12 + ¢2) 
~I cos(107r h + ¢d + A2 cos(107r 12 + ¢2) 
x(O) 
x(l) 
x(5) 
The 94 remaining values must be consistent with the result! It should be 
noted that the precision of the result is limited only by the calculator's preci-
sion, and that no conditions have to be met regarding the difference between 
hand 12. And there's no point in using the DTFT calculation! 
However, if there is some noise, the observed values of x(n) are "riddled 
with errors". The statistical estimation theory tells us that it is better to use 
all of the values, calculating some sort of a mean value. This is precisely what 
the DTFT does. Separating hand 12 now depends on the difference between 
hand 12, but also on the desired signal-to-noise ratio. 
If Fs = l /Ts refers to the sampling frequency, we have: 
The frequency resolution R is expressed in Hz. Its has the same order 
of magnitude as Fs / N, which is also the inverse of the total observation 
time T = NTs . 

100 Digital Signal and Image Processing using MATLAB® 
Without additional information, frequency differences of less than Fsi N = 
l iT should not be interpreted when studying a spectrum! In the literature, 
the quantity R = FslN = liT is called the Fourier limit. 
As an example, type the following program: 
%===== resolfreq.m 
N=32; L=128; freq=(O:L-l)/L; 
%===== first frequency 
fO=.2; xtO=exp(2*j*pi*fO*(0:N-l)); xfO=fft(xtO,L); 
%===== Second frequence = 0.23 
fl=.23; xtl=exp(2*j*pi*fl*(0:N-l)); xfl=fft(xtl,L); 
subplot(311); plot(freq,abs([xfO' xfl' (xfO+xfl)'])); 
grid 
%===== second frequency = 0.22 
fl=.22; xtl=exp(2*j*pi*fl*(0:N-l)); xfl=fft(xtl,L); 
subplot(312); plot(freq,abs([xfO' xfl' (xfO+xf1)'])); 
grid 
%===== third frequency = 0.21 
fl=.21; xtl=exp(2*j*pi*fl*(0:N-l)); xfl=fft(xtl,L); 
subplot(313); plot(freq,abs([xfO' xfl' (xfO+xf1)'])); 
grid 
Figure 3.5 shows the modulus of the DTFT for the sum of two sines, for 
three frequency pairs. In the first case, fa = 0.2 and h = 0.23, the presence of 
two sines can be shown, whereas it is impossible in the other cases. 
: [.LjlI
···········
· ······!···
fO !~ 02 y J,--'O.21
1 
2g -----::-:.- :,~;; 
.::"'.~.,: --o f .~ -~ ---:------; ------f ------:------~ - -----i ------
o 
0.1 
fo fl 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1 
: ELl£
nnnnLmLn
f,
l~02 L j, ~ 0221 
I 
I 
I 
I 
I 
I 
I 
I 
o 
0 , : ,..' .. :'-i \:\0;" "'" 
~ 
: 
: 
: 
: 
: 
o 
0.1 
fo fl 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1 
: E£:MW:
. ..
,
.
h~
02
, j,~023
1 
I 
I 
I 
I 
I 
I 
I 
I 
o .-.. 
I 
: ~ ".-,;/-.. : 
\ /·'I~.::' 
• I: • • ~ 
A k. A: 
: 
: 
: 
: C 
A 
( 
o 
0.1 
fo fl 0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1 
Figure 3.5 - Frequency resolution: the closer the two frequencies are, the harder it 
is to distinguish their peaks 
The R x T product plays the role of a merit factor when using the DTFT 
to search for frequencies. For a given resolution R , choosing T so as to have 
R x T 2: 3 usually allows an easy separation of the frequencies. 

Chapter 3 - Spectral Observation 101 
Exercise 3.1 (Studying the resolution) (see p. 405) 
Consider the signal x(n), sum of two real sines with a frequency of fa and 
h = fa + b.f and amplitudes of aa > 0 and al > 0 respectively. 
1. using gN(J) = sin(N7rj)/sin(7rj), give an expression of X(J); 
2. let N fa » 1, N h » 1 and Nlb.fl » 1. Use these inequalities to show 
that IX(J)I has two maximums close to the two frequencies fa and h; 
3. write a program that displays the signal's spectrum for a given a = ad aa 
dB ratio, and a given phase shift. Change the difference b.f from 1/ N to 
2/ N for N = 32 and fa = 0.2. Without changing any other parameters, 
compare the two resolutions corresponding to <I> = 0 and <I> = 7r /2. 
3.1.4 Effects of windowing on the resolution 
Rectangular windows 
Limiting the number of samples N of a signal can be interpreted as the term-by-
term multiplication of the signal by the sequence wN(n) = 1(n E {O, . . . , N -
I} ). This sequence is called a rectangular window. The same signal was called 
a "rectangle" in the previous chapter. 
From a spectral perspective, this multiplication, or weighting, is equivalent 
to convoluting the DTFT of x(n) with the DTFT WN(J) of the sequence 
wN(n). This can be written as follows: 
{x(n) x w(n)} -t (X * WN )(J) 
where WN(J) is expressed (formula 2.23): 
WN(J) = si~(N7rj) 
e-j7r(N- 1l! 
sm(7r j) 
The effect of this convolution operation is to cause unwanted ripples to 
appear in the spectrum. 
The concept of windows 
Generally speaking, a window is a sequence of coefficients used to weight a 
signal. A relatively detailed study of the windows used for signal processing 
can be found in [16]. Usually, when the frequency resolution is improved: 
- the main lobe grows narrower; 
- and the side lobes become smaller. 

102 Digital Signal and Image Processing using MATLAB® 
Unfortunately, reducing the height of the side lobes always means widening 
the main lobe. A compromise must therefore be made between these effects. In 
the following exercise, which illustrates these properties, we will only be using 
the Hamming window, one of the most commonly used windows. Its expression 
is: 
Wh(n) = { ~.54 
- 0.46coselZr
n
) 
when n E {O, ... , N - 1} 
otherwise 
Exercise 3.2 (Effect of the Hamming windowing) (see p. 406) 
(3.2) 
Consider a length N = 32 sample of a complex exponential x(n) with a fre-
quency of fa = 0.2 and an amplitude of A = 1. Each sample is multiplied by 
chwh(n), where wh(n) refers to the Hamming window and Ch is a constant we 
have to determine. 
1. calculate, for any window, the constant Ch such that the maximum am-
plitude of the DFT of the windowed signal at fa is equal to A; 
2. write a program that displays the DTFT of x(n) for the rectangular 
windowing and the Hamming windowing; 
3. for both windows, check the width of the main lobe and the height of 
the side lobe (the lobe's height will be expressed in dB compared to the 
height of the main lobe); 
4. we want to distinguish, in a signal sampled at 1,000 Hz, two sines of the 
same amplitude. Use the previous plot to find an order of magnitude for 
the resolution of the two windows that were studied; 
5. we want to distinguish, in a signal sampled at 1,000 Hz, two sines with 
an amplitude ratio now worth 25 dB. Find an order of magnitude for the 
windows that were studied. 
In practice, the frequency resolution for sines of the same amplitude is 
roughly equal to liN when using a rectangular window. When the amplitude 
ratio is no longer equal to 1, the resolution depends on which analysis window 
is chosen. Exercise 3.2 shows that the Hamming window leads to a resolution 
that is not as good as the one obtained with the rectangular window, for an 
amplitude ratio of 0 dB, but this phenomenon is reversed for an amplitude 
ratio of 25 dB. 
A few windows 
The following table gives a few characteristics for the most commonly used win-
dows (see Figure 3.6). b. is the main lobe's width and AdS is the attenuation, 
in dB, of the first side lobe, compared to the main lobe's height. The results 

Chapter 3 - Spectral Observation 103 
of this table can be found using a MATLAB® program of the type (see Figure 
3.6): 
%===== onewin.m 
% 
Blackman 
N=10; w=0 .42-0.5*cos(2*pi*(0:N-l)/N)+0.08*cos (4*pi*(0 :N-l)/N); 
w=w/sum(w); 
%===== gain in 0 equal to 1 
ws=fft(w,1024); 
plot((0:1023)/1024,20*log10(abs(ws))) 
set(gca, ' xlim' , [0 .5], 'ylim', [-100 0]); grid 
%===== to measure freq . , click once on each max. 
[xm , ym] =ginput (2) 
Type 
I Expression for n E {O, ... , N - I} 
Rectangular 
l (n E {O, ... ,N - I}) 
Triangular 
N -width Triangle 
Hann 
21m 
0.5 - 0.5 cos( N ) 
Hamming 
21m 
0.54 - 0.46 cos( N ) 
Blackman 
21m 
41fn 
0.42 - 0.5 cos( N ) + 0.08 cos( N ) 
2/N 
- 13 dB 
4/N 
- 25 dB 
4/N 
-31 dB 
4/N 
-41 dB 
6/N 
- 61.5 dB 
o r- ~ ~~~--~--~--~--~--~'-
T-
--~--' 
- 10 
" 
' 
, ------!------0 ------! ------!---- ~~~~~~ti~~ 
----
-20 ------'------"------"----- ------ . ------"------,----
--.------
: 
: 
:: 
:: indB 
I 
- 30 
------:------~------~------:----
-
~ ------t------~---
-~------f------
: 
: 
: :  
: :  
I 
: 
- 40 ------,- -----" ------" ------,- -----. -----" ------,- --- -,- -----" ------
I 
I 
I 
I 
I 
I 
I 
I 
I 
-50 -----_: ----- ~ ----- ~ ----_: 
I 
_____ : _____ ~ ______ ~ __ 
: 
: 
: 
I 
- 60 ------,------"------"------,------ . ---- -"------,---- -,------"------
I 
I 
I 
I 
I 
I 
I 
- 70 
-80 
- 90 
I 
I 
I 
I 
I 
------,------,------r------,------ , ------
I 
I 
I 
I 
-
100
~--
~
' --~
' --~
' --
~
'----~--~--~--4_--~--~ 
o 
10( 
0.05 
0.1 
0.5 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
half-width of the main lobe 
~
I 
Figure 3.6 - The Blackman window parameters 

104 Digital Signal and Image Processing using MATLAB® 
Periodic and symmetrical window 
Consider, for n E {O, .. . , N - 1}, the two following expressions of the Hamming 
window: 
( 2Kn) 
( 2Kn ) 
wp(n) = 0.54 - 0.46 cos Nand ws(n) = 0.54 - 0.46 cos 
N _ 1 
The first one, indexed with a P, is periodic with period N, that is to say 
wp(O) = wp(N). It is used, among other things, as a weighting window for the 
spectral analysis, of length N portions of a signal. The second one, indexed with 
an S, is symmetrical in the sense that ws(O) = ws(N -1), ws(l) = ws(N - 2), 
etc. As we will see, it is particularly used as a weighting window in the case of 
length N FIR filter design (see paragraph 4.7). 
If you have the MATLAB® signal toolbox at your disposal, type help 
hamming. Depending on what version you own, you mayor may not have 
the choice between periodic windows and symmetrical windows. 
3.2 
Short term Fourier transform 
The Fourier transform "compares" the signals to the eternal exponentials by 
calculating a mean on the time axis. It is therefore better suited for the study 
of phenomena that vary little in time than it is for brief, transitory phenom-
ena. This does not mean, however, that information is lost, because the Fourier 
transform is bijective under the conditions expressed in the introduction chap-
ter. Consider, for example, the signal x(t) made up of two consecutive portions 
of sines with durations of Tl and T2 and frequencies of h = 0.1 and 12 = 0.2 
(Figure 3.7). This signal can be created by the following program: 
%===== twosin1.m 
T1=512; T2=256; 
tps1=[0:T1-1]; tps2=[0:T2-1]; 
f1=0.1; x1=sin(2*pi*f1*tps1); 
f2=0.2; x2=sin(2*pi*f2*tps2); 
x=[x1 x2]; plot(tps,x); grid, 
set(gca, 'xlim' ,[384 576]) 
% respective durations 
tps=[tps1 T1+tps2]; 
% plotting of the 2 sinusoids 
The Fourier transform X(f) of the complete signal "contains" the informa-
tion regarding the order in which the two sines appear. However this informa-
tion's interpretation is difficult, because it is found, not very explicitly, in the 
transform's phase. Therefore, by limiting ourselves to the visualization of the 
modulus of X(f), there is no way for us to know that h comes before 12. This 
can be illustrated by typing the following program: 
II 
%===== specct1.m 
% Plotting of the modulus and phase of the signal x 

Chapter 3 - Spectral Observation 
105 
0.8 
0.4 
--+ 
.:- -_.: 
- - - . ~ 
- --:; 
o - -- - - -- -I: 
-0.4 
-0.8 
-1 
400 
420 
440 
- - , 
-
-:. 
-- ~ ~-
I: 
: 
-- ;-
-- -:-
460 
480 
500 
520 
Figure 3.7- Two portions of sines 
% defined in program twosin1.m 
Lfft=1024; freq=(0 :Lfft-1)/Lfft ; 
xf=fft(x,Lfft); xfa=abs(xf); xfph=angle(xf); 
subplot(211); plot(freq,xfa); grid 
set(gca, 'xlim' , [0 0.5],'ylim' ,[0 max(xfa)]); 
subplot(212); plot(freq,xfph); grid 
set(gca, 'xlim' , [0 0.5],'ylim' ,[-pi pi]); 
Jz=0.2 
1 -
-
-
540 
560 s 
Figure 3.8 shows two peaks at frequencies 0.1 and 0.2, but it does not tell 
us which one comes first. 
200 
150 
100 
I 
I 
I 
I 
I 
I 
I 
------1------ ------r------------,------r------ r -----'------T------
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
______ 1 ___________ L 
___________ .J 
_____ 1 
_____ 1 ______ --' 
_____ 1 __ 
------1------
------~------
-----~------~------
I ------~------~------
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
50 ------:------
------~-----
-----~------}------
~ -----~------+------
I 
I 
I 
I 
I 
I 
: 
..;...... 
: . 
....,~
:: 
:: 
00 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
3 ~=~-===-=-~= 
2 
1 
o 
-1 
-2 
-3 o 
0.05 1,=0.1 0.15 Jz=0.2 0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
Figure 3.8 - Modulus and phase for the two portions of sines 

106 Digital Signal and Image Processing using MATLAB® 
Type: 
%===== specct2.m 
% xl and x2 are defined in twosinl.m 
xinv=[x2 xl]; xinvf=abs(fft(xinv,Lfft)); 
plot(freq,xinvf); grid 
set(gca, 'xlim' , [0 0.5],'ylim' ,[0 max(xinvf)]); 
The resulting spectrum is almost identical to the previous one. However, 
if the time interval is "cut up" in Nsi sub-intervals with a duration of Pi, 
and if Fourier transforms are performed on each of these sub-intervals, the 
information concerning the order of the frequencies becomes clear. This leads 
us to the concept of short term Fourier transform, or STFT. At the end of the 
previous program, type: 
%===== specct3.m 
% Tl ,T2, x defined in twosinl.m 
nfft=1024; freq=[O:nfft-l]/nfft; 
%===== 
nsi=8 ; npt=fix((Tl+T2)/nsi); 
xs=zeros(npt,nsi); xs( :)=x(l:npt*nsi); 
xsf=abs(fft(xs ,nfft)) ; xsf=xsf(1:nfft/2,:); 
mtime=[O:npt:npt*nsi- l] ; 
subplot(211); imagesc(mtime,freq(1:nfft/2) ,xsf); 
set(gca, 'xlim' , [0 700]) 
%===== 
nsi=32; npt=fix((Tl+T2)/nsi); 
xs=zeros(npt,nsi); xs( :)=x(l:npt*nsi); 
xsf=abs(fft(xs ,nfft)); xsf=xsf(1:nfft/2,:); 
mtime=[O:npt:npt*nsi- l] ; 
subplot(212) ; imagesc(mtime,freq(1:nfft/2) ,xsf); 
set(gca, 'xlim' , [0 700]) 
The spectra, which can be displayed using the imagesc command (see Chap-
ter 5), are represented in Figure 3.9 for two values of N si . 
surf or mesh can also be used for 3D graphs. 
It is clear that by following the time axis, the STFT tells us the order of the 
frequencies used in the signal. By comparing the two figures, we notice that 
the smaller the number of points K i in a sub-interval: 
- the easier it is to locate the position on the time axis of Tl = 512, corre-
sponding to the frequency change; 
- the harder it is to locate the positions 0.1 and 0.2 on the frequency axis 
because of the width of the main lobes. 
Let Pi be the number of points in an interval. The following comments can 
be made: 

01 
02 
03 
04 
05 
Chapter 3 - Spectral Observation 107 
~
-
~ 
-
~ 
-
-
-
-
I 
I 
I 
I 
I 
I 
I 
I 
~ .~~~~~ 
03 
~ 
. 1-
_
_ 1_ 
-
- -
04 
0.5 
Figure 3.9 - Spectrum for Ns i = 8 (above) and Ns i = 32 (below) 
- because the DTFT calculates a "mean" of Pi values, choosing a high 
value of Pi causes an intense smoothing of the signal's fluctuations in 
time. This means that the time transitions cannot be located precisely; 
- on the other hand, a high value of Pi gives every DTFT more calculation 
points. Therefore the width of the lobes (roughly equal to 1/ Pi) decreases 
and the frequency peaks appear more clearly. 
Given the sampling frequency Fs = l /Ts, the frequency resolution is roughly 
equal to RF = Fs/ Pi, while the time resolution is roughly equal to RT = PiTs, 
meaning that the product of the two remains roughly equal to 1. 
When using the short term Fourier transform (STFT), improving time 
resolution decreases frequency resolution. 
Exercise 3.3 (Short t erm Fourier transform) (see p. 408) 
The incTF1 .mat and incTF2 .mat we are going to use are supposed to come 
from the sampling at Fs = 1,000 Hz of the sum of a certain number of frequen-
tial components with different durations. To build these files, execute the two 
following programs: 
%===== genel.m 
T=0 .35; Fs=1000; NT=fix(Fs*T) ; tp=(O:NT-l) ; xt=zeros(l,NT); 
fq=[113 247 327 413]/Fs ; org=fix([O 0 0.030 0 . 150]*Fs); 
dur=fix([0.350 0.050 0 .200 0.200]*Fs); amp=[l 1.7 1.9 1.8]; 
for ii=1 :4 
end 
xc=amp(ii)*cos(2*pi*fq(ii) *tp) ; 
ti=org(ii)+l;tf=org(ii)+dur(ii); 
xt(ti :tf)=xt(ti :tf)+xc(ti :tf); 
save incTFl xt Fs 

108 Digital Signal and Image Processing using MATLAB® 
%===== gene2.m 
T=0.35; Fs=1000; NT=fix(Fs*T); tp=(O:NT-l)/Fs; 
fO=250; fm=3; beta=6; 
phi=fO*tp+beta*sin(2*pi*fm*tp); xt=cos(2*pi*phi); 
save incTF2 xt Fs 
1. write a short term Fourier transform function. It will be named: 
[spec,tps]=stft(xt,Lb,ovlp,Lfft,win) 
where: 
- xt is the signal, 
-
Lb the length (number of samples) of the blocks, 
- ovlp the number of overlapping samples, 
- Lfft the length of the FFT, 
- win the window type, 
-
spec the complex spectrogram, 
- tps the normalized time; 
2. load one of the two signals and display its chronogram; 
3. write a program designed to perform a time/frequency analysis of the 
signal. The time interval will be cut up in sub-intervals of the same 
length, with an overlapping coefficient of 50%. The contour function 
will be used for displaying the results. 
Exercise 3.4 (Visualizing the aliasing with the STFT) (see p. 409) 
Consider the signal 2.12 created in the paragraph on page 75 and illustrating the 
aliasing phenomenon by listening to the created signal. Visualize the evolution 
of this signal's spectrum using the mesh function, or in a 2D graph by using the 
contour function. The values T = 2 and ,\ = 2,000 will be taken. In order to 
achieve this, the signal will be cut up in "slices" with a length of 100 samples, 
on which FFTs will be performed. The blocks should not overlap. A Hamming 
window can be used before performing the FFT. 
3.3 Summing up 
The following exercise illustrates on one hand the sampling effects with the 
possible presence of aliasing and on the other hand the effects of truncation 
with the presence of ripples. 

Chapter 3 - Spectral Observation 109 
Exercise 3.5 (Effects of sampling and windowing) (see p. 409) 
Consider the continuous-time signal x(t) = exp( -t/to) l(t E (0, +oo[) with 
to > O. Its Fourier transform is called X(F). 
1. determine the expression of its Fourier transform X (F) . Use it to find the 
value, as an expression ofto, of the frequency corresponding to IX (0) 1/ y'2; 
2. x(t) is sampled at the frequency Fs = l /Ts . Let xs(n) = x(nTs) be its 
sample sequence and Xs(f) the DTFT of xs(n) . Using formula 2.26, find 
Xs(f) as an expression of X(F). What can you notice? 
3. the DTFT is evaluated using only the first M samples xs(O) to xs(M -1). 
What is the resulting effect on the signal's spectrum? 
4. write a program that gives you Figure 3.10, illustrating the different sig-
nal spectra, continuous-time, discrete-time, and windowed discrete-time 
(to = 1/ 0.7, M = 10, Fs = 2 Hz and Lfft= 256). Check the results for 
the period of the ripples. 
0.8 
0.6 
0.4 
0.2 
(a) 
2 .-~--~--~_, 
(b) : DTFT: 
l.5 ---. /
---
0.5 
o LLJ...l...L...L..L.L.L.:=<OOOOoOO<)O-l 
0 '---~-""::"":'"----~--' 
o 
5 
lO(s) 
-2 
2 (Hz) 
1.5 ,----------, 
(e) : 
0.8 
_____ ~--_-~ 
__ 
--_
L -----~----
, 
, 
, 
, 
I 
I 
I 
I 
--r----'----- T-----r----
0.6 
(d) 
DFT 
--------\ 
, 
, 
0.4 
.----~-----
~ -----~----
, 
, 
0.5 
0.2 I .---j-----;----+----
o '--'-'--'--'--~
' -~--
' --' 
o 
2 
4 
6 
8 (s) 
-2 
0 
2 (Hz) 
Figure 3.10 - Effects of sampling and windowing on the signal's spectrum: (a) 
original signal with its samples, (b) FT et DTFT with aliasing, (c) truncated signal, 
(d) effects of tuncation (ripples) and values of the DFT. 

110 Digital Signal and Image Processing using MATLAB® 
3.4 Application examples and exercises 
3.4.1 Amplitude modulations 
Exercise 3.6 (Amplitude modulation) (see p. 411) 
Consider a B band, continuous-time real signal m(t), that is to say a signal 
whose Fourier transform M(F) is equal to zero for IFI > B. Let Fa be a 
frequency such that Fa > B (for broadcasting, the order of magnitude for Fol B 
is 100). We call amplitude modulation (AM)l the operation that generates: 
x(t) = (1 + km(t)) cos(27rFot) 
Fa is called the carrier frequency. k refers to a positive constant called the 
modulation index, and is chosen so as to have Ikm(t)1 < 1. When Ikm(t)1 > 1, 
there is what is called overmodulation. 
1. give the expression of the FT X(F) of x(t) as a function of k, M(F) and 
Fa. How wide is the band occupied by X (F) around Fa? 
2. to perform a spectral analysis of the signal x(t), it has to be sam-
pled at Fs = 500 kHz. 
We will assume that Fa = 50 and that 
m(t) = cos(27rFlt) + 1.8cos(27rF2t) + 0.9cos(27rF3t) where FI = 2,310 
Hz, F2 = 3,750 Hz and F3 = 4,960 Hz. Let k = 1/2 . Write a program 
that generates x(t) for a duration of 2 ms. Make sure the chronograms 
for m(t) and x(t) show no overmodulation; 
3. give the number of samples that have to be processed in order to distin-
guish the two frequencies contained in the signal; 
4. how long must the FFT be if we want a precision of 100 Hz? 
5. write a program that draws the modulated signal's spectrum. 
Exercise 3.7 (Carrierless Double Side-Band) (see p. 413) 
Consider the B band real signal m(t). The carrierless amplitude modulation, 
or Double Side Band Suppressed Carrier (DSBSC), represented in Figure 3.11, 
is described by the expression x(t) = m(t) cos(27rFot). 
m(t) 
-. ® 
-. x(t) = m(t) cos(27rFat) 
cos(27r Fat) t 
oscillator 
Figure 3.11 - Carrierless Double Side-Band 
IThis modulation is called Double Side-Band (DSB) modulation as opposed to the Single 
Side-Band (SSB) modulation [34]. 

Chapter 3 - Spectral Observation 
111 
1. determine the expression of the amplitude spectrum of the modulated 
signal x( t); 
2. the signal x(t) is modulated a second time by the local oscillator 
2cos(27rFat + ¢). The result is the signal y(t) = 2x(t)cos(27rFat + ¢). 
Determine, as a function of M(F), Fa and ¢, the expression of the spec-
trum of y(t). Use this result to determine a method for reconstructing 
the message m(t) from the signal y(t). 
Why must we have ¢ = O? This operation is called synchronous demod-
ulation when ¢ = 0; 
3. consider m(t) = cos(27rFlt) + 1.8cos(27rF2t) +0.9cos(27rF3t) where Fl = 
2,310 Hz, F2 = 3,750 Hz and F3 = 4,960 Hz. Set Fa = 50 kHz and 
Fs = 500 kHz as your display frequencies. Write a program that plots 
the original, modulated and demodulated signals, as well as their spectra. 
Exercise 3.8 (Stereophonic signal) (see p. 414) 
Some frequency modulated broadcasting are sent stereophonically. This means 
that the received signal makes it possible to reconstruct both the left and right 
signals. This is achieved by sending the composite signal: 
c(t) = (l(t) + r(t)) + (l(t) - r(t)) cos(27rFat) 
where l(t) and r(t) refer to the left and right signals respectively. Notice that 
the signal (l( t) - r( t)) is transmitted as carrierless double side-band modulation 
(see exercise 3.7). For broadcasting, the signals l(t) and r(t) are band-pass 
signals centered in Fa = 38 kHz with a bandwidth of 30 kHz. 
1. determine the spectrum's expression for the signal c(t). Draw a quick 
sketch of its graph; 
2. show that c(t) makes it possible to reconstruct the signal on a monophonic 
set; 
3. write a program that displays c(t) , 2g(t) and 2d(t) for a sampling fre-
quency of Fs = 1 MHz, where: 
- the signal l(t) is the sum of 5 sines with the amplitudes 0.7, 1.5, 
1.9,2.8 and 3.7, and with the frequencies 380 Hz, 957 Hz, 1,164 Hz, 
1,587 Hz and 1,953 Hz respectively, 
- the signal r(t) is the sum of 5 sines with the amplitudes 0.3,1.5, 
2.7, 1.7 and 2.3, and with the frequencies 347 Hz, 523 Hz, 1,367 Hz, 
2,465 Hz and 3,888 Hz respectively. 
Use this to find a method for separately reconstructing, by sampling, the 
signals l(t) and r(t). 

112 
Digital Signal and Image Processing using MATLAB® 
3.4.2 Frequency modulation 
Let m(t) be a B band real signal. The name frequency modulation at the carrier 
frequency Fa » B refers to the operation that generates the signal: 
x(t) = AcoS(27TFat + <t>(t)) 
where the instantaneous frequency Fi(t) defined by: 
F(t) = F,a + ~ d<t>(t) 
2 
27T 
dt 
(3.3) 
is related to m(t) by: 
Fi(t) = Fa + t::.F x m(t) 
(3.4) 
This leads us to <t> (t) = 27T t::.F J~ m( u )du. For commercial broadcasting, 
Fa » B, since B = 15 kHz and Fa belongs to the 88 MHz to 108 MHz band. 
We can rewrite x(t) as: 
x(t) = AcoS(27TFat + <t>(t)) = Re {Ae2j7rFot+j<I>(t) } = Re {a(t) e2j7rFot } 
where a(t) = Aej<I>(t). If Fa » B, it can be shown [34] that, to obtain the 
spectrum of x(t), all you have to do is determine the spectrum of a(t), and to 
translate it, after dividing by 2, around the frequencies ±Fa. 
Let us now see the particular case of a sine message m(t) = cos(27TFmt). 
In this case, the instantaneous frequency Fi(t) varies between Fa - t::.F and 
Fa + t::.F. This is why t::.F is called the frequency deviation. 
Let j3 = t::.F / B. j3 is the modulation indei2. 
It can be shown that the periodic function a(t) = Aexp(j<t>(t)) 
A exp(jj3 sin(27TFmt)) has, as its Fourier series expansion: 
+00 
a(t) = A L I n (j3) exp(2j7TnFmt ) 
n = -oo 
where I n (j3) refers to the Bessel function of the first kind of order n. Its 
spectrum shows peaks spaced-out at intervals of Fm. This means that the x(t) 
spectrum also shows peaks spaced-out at intervals of Fm around ±Fa (Figure 
3.12). 
Figure 3.12 was obtained using the modfm2. m program. This program plots 
the spectrum of a signal modulated in frequency by a sine with a frequency of 
Fm = 5 kHz and for a carrier frequency Fa = 2 MHz. Type: 
2The modulation index plays a fundamental role in communications. In particular, it can 
be shown the performances of the frequency modulation in the presence of noise increase like 
/32 

Chapter 3 - Spectral Observation 113 
, 
, 
, 
------,------ , ------,-----
, 
, 
, 
------------- 1 ------1-----
, , , 
I 
" 
I 
- - - - - ~ - - - - - - ,- - - - - - ,- - - - - - ~ - - - - - - ~ - - - --
_____ J______ 
_ ____ J ______ J ____ _ 
Figure 3.12 - Spectrum of a frequency modulated sine signal with a modulation 
index (3 = 2.4 (in this case, Fo peak is missing) 
%===== modfm2.m 
Fs=1.0e7; 
% sampling freq. for the simulation 
npts=20000; mtime=(O:npts-l)/Fs; 
FO=2.0e6; 
% carrier freq. 
Fm=5000; 
% signal freq. 
disp('Carrier frequency: '); 
disp(sprintf('\t FO = %d MHz' ,FO/le6)); 
disp('Message frequency: '); 
disp(sprintf('\t Fm = %d kHz' ,Fm/l000)); 
disp('Instantaneous frequency: '); 
disp(sprintf('\t fi(t)/(2 pi) = FO + Deltaf*sin(2*pi*Fm*t) ')); 
%===== frequency deviation 
disp('Choose the frequency deviation (kHz): '); 
Df=input(sprintf('\t Deltaf (kHz) = ')); 
Df=Df*1000; 
% deviation in Hz 
beta=Df/Fm ; 
% modulation index 
theta=2*pi*FO*mtime+beta*cos(2*pi*Fm*mtime); x=cos(theta); 
Lfft=32*1024; fq=Fs*(O:Lfft-l)/Lfft; 
xf=abs(fft(x,Lfft)); plot(fq,xf); ax=axis; grid 
axis([max([O FO-2*Df]) min([Fs/2 FO+2*Df]) ax(3) ax(4)]) 
The program asks for the value of the frequency deviation (input ... ). By 
giving, for example, the value 2, 4*Fm/l000 (in the program this corresponds 
to Deltaf=12) , the result is that the peak at Fo is erased because in this case, 
(3 = 2.4 and Jo(2.4) :::::: O. 


Chapter 4 
Linear Filters 
When building a model to describe the behavior of some of the most commonly 
used systems, we often rely on the superposition principle. It amounts to 
assuming linearity (the use of Kirchoff's laws are an example) . Usually, time 
invariance is also assumed. It consists of saying that, on the time scales that 
are used, the characteristics of these systems remain unchanged. 
Linear filters are defined in the following section by these two characteris-
tics. Because of their importance in the field of signal processing, this chapter 
presents their main properties, as well as a few design methods. 
4.1 
Definitions and properties 
Definition 4.1 (Linear filter) A discrete-time linear filter1 is a system 
whose output sequence results from the input sequence {x( n)} according to 
the expression: 
+00 
+00 
yen) = (x * h)(n) = L x(k)h(n - k) = L h(k)x(n - k) 
(4.1) 
k= -oo 
k= -oo 
where the sequence {h( n)} that characterizes the filter is called the impulse 
response. The (x * h) operation is called convolution (Figure 4.1). 
For example, the processing defined by yen) = ~x(n) 
+ ~x(n-1) 
is therefore 
a linear filtering. The sequence {hen)} is defined by h(O) = ~, 
h(l) = ~ and 
hen) = 0 for any value of n =1= {O, I}. 
For commonly used classes of signals, expression 4.1 is perfectly well defined, 
and satisfies the linearity property. We will now prove the time invariance 
property. In order to do this, we will assume that the output sequence yen) 
1 Most of the time, we will just write filter instead of linear filter. 

116 Digital Signal and Image Processing using MATLAB® 
input 
( 
)0 I linear filter 1'\ _ 
output 
{x(n)}----'r\-, ~. 
{h(n)} r-;---{y(n)} 
convolution 
Figure 4.1 - Discrete-time linear filter 
corresponds to the input signal x( n), and we must determine what output 
signal v(n) corresponds to the input signal u(n) = x(n - no). We can write: 
+00 
+00 
v(n) 
L u(k)h(n - k) = L x(k - no)h(n - k) 
k = - oo 
k = -oo 
+00 
L x(p)h((n - no) - p) = yen - no) 
p = -oo 
However, if there is an no delay for the input, there is also a time delay 
of no for the output. It should be noted that linear systems as simple as the 
following two: 
+00 
yen) = x(n) cos(2n fon) 
or 
yen) = L x(k)h(n - k) 
k = O 
do not possess the time invariance property. 
Throughout the rest of this chapter, the two important concepts of causality 
and BIBO stability will often be referred to. 
Definition 4.2 (Causality) A system is said to be causal when its output y( n) 
at time n depends only on the current and previous values of the sequences x( n) 
and yen): 
yen) = F( {x(p)} , {y(p')}) with p,p' ::; n 
Definition 4.3 (Bounded input - bounded output stability (BIBO)) 
A system is said to be BIBO stable2 if, for any bounded input, the output 
remains bounded: 
Vn , Ix(n)1 < A ~ ly(n)1 < B 
We have the following two results: 
2From now on, when there is no possible confusion, we will just write "stable" instead of 
"BIBO stable". 

Chapter 4 - Linear Filters 117 
Theorem 4.1 A filter is causal if and only if its impulse response {h( n)} is 
such that: 
h(n)=Owhenn<O 
HINTS: because of 4.1, y(n) depends only on {x(n), x(n - 1), . . . , 
x(n - k) ... } for k ;:: 0, if and only if h(k) = 0 for k < O. If the 
terms h( n) are equal to zero for n ;:: M, the filter memory is finite, 
and its value is M. 
(4.2) 
An operation like y(n) = ~x(n+ 
1) + ~x(n) 
+ ix(n-1), characterized by the 
impulse response {h(-l) = 6' h(O) = 2' h(l) = i}' is not causal. However, 
this processing can be performed in delayed mode and not in real-time. It 
is also possible to "delay" the impulse response, if it is evanescent or cancels 
beyond a certain range for n < O. In our case, this leads to filtering by the 
impulse response {h(O) = i , h(l) = ~, h(2) = n . 
Theorem 4.2 A linear filter is BIBO stable if and only if its impulse response 
{h( n)} verifies: 
LnEZ Ih(n)1 < +00 
HINTS: let us first assume that Lk Ih(k)1 = M < +00. To any 
bounded input, that is to say such that Ix(n)1 < A, corresponds a 
signal y(n) that verifies: 
ly(n)1 ~ L
kEZ Ih(k)llx (n - k)1 < AM 
and which is therefore bounded itself, meaning that the filter is 
ElBO stable. 
Conversely, we assume that the filter is BIBO stable. We will use 
proof by contradiction. Let us assume that Lk Ih(k)1 = 00. The 
question is "can we find at least one bounded input yielding a non-
bounded output?". All we have to do is take x(n) = sign(h( -n)) as 
our bounded input, resulting at the time n = 0 in an infinite output 
y(O) = Lk Ih(k)1 = 00. 
(4.3) 
The properties describing causality and stability are best examined sepa-
rately. Stability can very well be achieved using a non-causal filter examined 
later (see section 4.4.1). 
A first use of MATLAB@'s filter function 
In MATLAB@, the filtering operation is performed by a built-in function called 
filter. This function provides us with a causal implementation of the filtering 

118 Digital Signal and Image Processing using MATLAB® 
operation, so in order to "filter" the input sequence {x(l), . . . , x(N)} by the 
impulse response filter of finite length {h(l), . .. , h(L)} (the phrase used will 
be "finite impulse response filter", or FIR filter), we need to type: 
y = filter(h,l,x); 
The output sequence {y(l), ... , y(N)} has the same length as the input 
sequence. To calculate it, the (L - 1) values preceding x(l) must first be 
known. filter can accept a fourth argument, used to specify the initial state, 
the linear combination of the initial values (see function filtic in the Signal 
Processing Toolbox). If this argument is left blank, the filter considers that 
all values are equal to zero, and we have: 
y(1) 
h(l)x(l) 
y(2) 
h(l )x(2) + h(2)x(1) 
y(L) 
h(l)x(L) + ... + h(L)x(l) 
y(n) 
h(l)x(n) + ... + h(L)x(n - L + 1) 
y(N) 
h(l)x(N) + ... + h(L)x(N - L + 1) 
Note that because the impulse response has a finite length, it necessarily 
verifies 4.3, and therefore the filter is BIBO stable. 
EXAMPLE 4.1 (Smoothing filter) We are going to filter the sequence [0: 6] 
with the use of the impulse response filter [1 1]. Type: 
%===== exfiltint.m 
clear; x=[O:6]; h=[l 1]; 
y=filter(h,l,x) 
The resulting sequence is: 
1 
3 
5 
7 
9 
11 
EXAMPLE 4.2 (Smoothing filtering of a random sequence) 
We will now filter a random sequence x=rand(50, 1) using a filter with the 
impulse response: 
h = 
[~ 
~ 
1 
4 
1 
4 
~] 
This filter calculates a weighted mean of five consecutive samples. The 
result is expected to be less "turbulent" than the signal we started with: 

Chapter 4 - Linear Filters 119 
%===== exfiltrand.m 
clear; x=rand(50,1); 
h=[1/8 1/4 1/4 1/4 1/8]; 
y=filter(h,l,x); 
% input signal x 
plot([x y]); grid 
% impulse response 
% output signal y 
The result is shown in Figure 4.2. 
Figure 4.2 - Filtering a random sequence with the smoothing impulse response filter 
{12221}/8 
Definition 4.4 (Step response of a filter) 
The step response of a filter is this filter's output when the unit step (u(n) = 
1 when n 2: 0 and 0 otherwise) is fed into the input. 
The step response's 
expression is: 
n 
yen) = L h(k) 
k = -oo 
In the case of a causal filter, yen) = 2:
~ = o h(k) for n 2: 0 and 0 otherwise. 
EXAMPLE 4.3 (Step response of an FIR filter) 
Consider the impulse response filter hen) = Aan for 0:::; n :::; 15 and 0 otherwise. 
Its step response is denoted yen) . Determine the expression of yen). Use this 
to find the expression of A that leads to 1 for n 2: 15. Write a program that 
calculates, with the iil ter function, the 30 first samples of the step response. 
HINTS : we have, for n :::; 15: 
n 
1 _ a n + 1 
yen) = L h(k) = A-1 -_-a-
k = O 

120 Digital Signal and Image Processing using MATLAB® 
For n ~ 15, y(n) = A(1 - a 16)/ (1 - a) . In order to have y(n) = 1 
for n ~ 15, A has to be set such that A = (1 - a)/(l - a 16 ) . The 
higher the value of lal, the slower the step response will close in on 
the final value 1, reaching it at the time n = 15. 
Type the following program: 
%===== stepresp.m 
% Impulse responses 
N=16; a=[1/2 3/4 7/8]; Nct=length(a); 
a=ones(N,l)*a; hh=(O:N-l)'*ones(l,Nct); 
h=a .- hh; sigm=sum(h); 
%===== the impulse responses are normalized in order 
% 
to compare the rise times 
hO=h(:,l)/sigm(l); hl=h(:,2)/sigm(2); h2=h(: ,3)/sigm(3); 
Lrep=30; 
% response's length 
tps=[O:Lrep-l]; x=ones(Lrep,l); 
%===== responses with null initial conditions 
y=[filter(hO,l,x) filter(hl,l,x) filter(h2,1,x)]; 
plot(tps,y, '-' ,tps,y, '0'); set(gca,'YLim' ,[0 1.1]); grid 
The results are given in Figure 4.3. 
0.8 
0.6 
---------- -1- -
0.4 
----- 1 ---------- 1 -
0.2 
--------- T 
--------- -r -
OL-----~----~------~----~----~----~ 
o 
5 
10 
15 
25 
30 
Figure 4.3 - Step responses for a = 1/ 2,3/4, 7/8 
4.2 The z-transform 
An important tool used in discrete-time linear-filtering is the z-transform for 
which we will give the definition, the main properties and a description of 
how it is used in a filtering context. Most of the properties mentioned in this 
paragraph are given without proof. Some of them will have no direct use for 
what follows; however, it is useful simply to know they exist. 

Chapter 4 - Linear Filters 121 
4.2.1 
Definition and properties 
Definition 4.5 (z-transform) The z-transform (ZT) of the sequence {x(n)} 
is the function X z(z)3 of the complex variable z defined by: 
+00 
X z(z) = L x(n)z-n 
( 4.4) 
n = -oo 
for values of z taken inside a ring described by {z E <C : Rl < Izl < Rd, 
assumed to be non-empty, and called the convergence area or domain of con-
vergence (Figure 4.4). 
The values of z for which Xz(z) is equal to zero are called zeros, and the 
values of z for which Xz(z) diverges are called poles. 
convergence 
area 
Imag 
(z) - plane 
Real 
Figure 4.4 - Convergence area 
The properties enumerated in Appendix A4 provide calculation methods 
pertaining to the sequence {x(n)} or to the function Xz(z). 
What should be remembered is that the analytical expression of X z (z) 
does not characterize the sequence {x(n)}. What does characterize the latter 
is the pair comprising the function X z(z) and a convergence area. 
For rational transfer function filters, the convergence areas are rings delim-
ited by poles. To each ring corresponds a single result. The domain Izl > R 
that extends to infinity gives a causal result, the domain Izl < R containing the 
origin, gives an anti-causal result, whereas the other domains Rl < Izl < R2 
give a bilateral result. The area that contains the unit circle gives a stable 
result. 
3When there is no possible confusion, we will use X (z) instead of X z (z). 

122 Digital Signal and Image Processing using MATLAB® 
4.2.2 
A few examples 
The following results can be proved as an exercise, in particular by using the 
fundamental formula: 
1 
2 
n 
I I 
-- = 1 + u + u + ... + u +. .. with u < 1 
1-u 
(4.5) 
- unit impulse: 
6(n) = g when n = 0 
th 
. 
-t 6.(z) = 1, Vz 
o erWlse 
(4.6) 
- unit step: 
1 
u(n) = l(n 2: 0) -t U(z ) = 1- Z- 1 ' with Izl > 1 
(4.7) 
- ramp: 
Z- 1 
r(n) = nl(n 2: 0) -t R(z ) = (1- Z- 1)2 ' with Izl > 1 
(4.8) 
- causal exponential: 
ec(n) = anl(n 2: 0) -t Ec(z) = 
1 -1 ' with Izl > lal 
(4.9) 
1- az 
- anti-causal exponential: 
1 
ea(n) = -an l(n ::; -1) -t Ea(z) = 1 _ az- 1 ' with Izl < lal (4.10) 
We are going to prove 4.8 and 4.10. Because of 11.11 and 4.7, we have: 
dU(z) 
Z- 1 
R(z ) = - z ~ = - (1- Z- 1) 2 
which is expression 4.8. 
For 4.10 we have: 
- 1 
+00 
-
L 
anz- n = - La-PzP + 1 
with 
la- 1z l < 1 
n = -oo 
p= o 
1 _ 
1 
1 
1-a- 1z 
1-az- 1 
Note that Ec(z ) and Ea(z) have the same analytical expressions. We can 
tell them apart by their convergence areas. 

Chapter 4 - Linear Filters 123 
The z-transform and the DTFT 
When the unit circle belongs to the convergence area, the DTFT exists. In the 
complex plane, the unit circle can be represented by z = e2j7r/ where f varies 
from 0 to 1. In that case: 
XAe 2j7r/) = L x(n)e- 2j7rn/ 
n 
is also the DTFT of x(n) which we denoted by X(f). The unit circle can 
therefore be scaled, in values of f, from f = 0 for z = 1, to f = 1/2 for z = -1, 
including f = 1/4 for z = j (Figure 4.5). 
f = 1/4(mod 1) 
M(z = e27fjf ) 
f = 1/2(mod_1-<):)-__ 
-+""-_-'--..l-o.:f~= 
O(mod 1) 
f = -1/4(mod 1) 
Figure 4.5 - Unit circle 
To be less specific, if Izl = 1: 
f = arg(z) 
271' 
4.3 Transforms and linear filtering 
(4.11) 
Consider again the previous linear filtering. We will refer to the transforms of 
the sequences {x(n)}, {yen)} and {hen)} as Xz(z), Yz(z) and Hz(z) respec-
tively, and to their DTFTs as X(f), Y(f) and H(f) (we will assume that all 
these functions exist, and in particular that the convergence areas of Xz(z), 
Yz(z) and H z(z) contain the unit circle) . We then have: 
Property 4.1 (Filtering relations for finite energy signals) 
Consider a BIBO stable linear filter with a BIBO stable impulse response 
{hen)}, meaning that 2:k Ih(k)1 < +00. In this case, for finite energy sig-
nals {x(n)}, that is to say such that 2:k Ix(kW < +00, we have the following 
input-output formulas: 
{ 
Yz(z) = HAz)XAz) 
yen) = (x * h)(n) ---+ 
Y(f) = H(f)X(f) 
(4.12) 

124 Digital Signal and Image Processing using MATLAB® 
Hz(z) is called the filter's transfer function (TF) and: 
+= 
H(J) = Hz (e2j7rf ) = L h(n)e-2j7rfn 
(4.13) 
n = -oo 
is called the complex gain or frequency response. Remember relation 2.24: 
1
1/2 
h(n) = 
H(J)e2j7rfndf 
-1/2 
(4.14) 
Let H(J) = G(J)ej¢(f). G(J) = IH(J) I is called the filter gain, and ¢(J) = 
arg(H(J)) is its phase. 
Property 4.2 For a linear filter whose impulse response h( n) is real, we have 
Hz(z) = H;(z*) and H(J) = H*( - 1). In this case, the gain G(J) = IH(J)I 
and the real part of H(J) are even functions. Its phase ¢(J) = arg(H(J)) and 
its imaginary part are odd functions: 
G(J) = G( - 1) 
and ¢(J) = -¢( - 1) 
In this case we can restrict the graphical representation of G(J) to f E 
(0,1/2). 
Property 4.3 (Harmonic response of a linear filter) 
Let {h( n)} be the BIBO stable impulse response of a filter, and let {x( n) = 
exp(2j7r fon)} be the input signal. The expression of the output signal is: 
y(n) = H(Jo) x(n) 
where H(J) = L:n h(n)e-2j7rnf is the DTFT of {h(n)}. 
This can easily be shown by writing: 
y(n) 
L h(k)x(n - k) = L h(k) exp(2j7rfo(n - k)) 
k 
k 
exp(2j7r fon) L h(k) exp( -2j7r fok) 
k 
Complex exponentials are called the eigenfunctions of linear filters. 
In the particular case of a real filter, that is in the case where h( n) is real, 
with the input signal x(n) = cos(27rfon) , we obtain, by using the expression 
x(n) = (exp(2j7rfon) +exp(-2j7rfon))/2 and the linearity property: 
1 
. 
1 
. 
y(n) = 2H(Jo) exp(2J7rfon) + 2H( - fo) exp( -2J7rfon) 

Chapter 4 - Linear Filters 125 
Because h(n) is real, H(f) = H*( - j) and therefore: 
y(n) 
~H(fo) 
exp(2j7ffon) + ~H
* (fo) 
exp( -2j7ffon) 
G(fo) cos(27f fon + ¢(fo)) 
The output cosine has the same frequency as the input cosine, but its am-
plitude is multiplied by G(fo) and its phase is shifted by ¢(fo). 
Consider again the example of the impulse response filter [1 1]. Its transfer 
function is, for any z: 
Its complex gain can be expressed: 
H(f) = Hz (e2j7rj ) = 1 + e-2j7rj = 2e-j7rj cos(7fj) 
Its gain is therefore G(f) = 21 cos(7ff) 1 and its phase is: 
¢(f) = -7ff if f E (-1/2, 1/2) 
You can check that G(f) = G( - j) and that ¢(f) = -¢( - j). These char-
acteristics can be directly plotted using the following program: 
The phase plot may show "jumps". To avoid this the unwrap function is 
used: unwrap: phase=unwrap (angle (gaincplxe)) *180/pi. 
4.4 Difference equations and rational TF filters 
Consider this difference equation (d. e.): 
Yn + alYn-l + ... + apYn-P = boxn + b1xn-l + ... + bQxn-Q 
(4.15) 
We will assume that the x( n) are known (n E Z) and that we want to 
calculate the y(n). Assuming that the z-transforms of x(n) and y(n) exist, 
consider the z-transforms of the two sides of equation 4.15. Using the delay 
property, we get: 
which leads us to: 
(4.16) 
The system relating the sequence y(n) to the sequence x(n) is therefore a 
linear filter with the transfer function Hz(z). This proves that linear recursive 

126 Digital Signal and Image Processing using MATLAB® 
equations with constant coefficients perform a linear filtering with rational 
fractions (ratios of two polynomials) as their transfer functions. 
But what convergence area should we choose? This choice is related to the 
way we calculate the solution to the difference equation. 
EXAMPLE 4.4 (First order difference equation) Consider the example: 
y(n) + ay(n - 1) = x(n - 1) 
If no further information is given, this equation cannot be solved. We have 
to indicate the type of solution that we want - causal or non-causal - and initial 
conditions. For these hypotheses we get: 
1. causal solution: a value is set at the origin, for example y(O), and y(n) is 
calculated based on y(n - 1). We write the successive expressions: 
y(1) 
-ay(O) + x(O) 
y(2) 
a2y(0) - ax(O) + x(l) 
2. anticausal solution: a value is set at the origin, for example y(O), and 
y( n - 1) is calculated based on y( n). We write the successive expressions: 
y( -1) 
y( -2) 
-y(O)/a + x( -l)/a 
y(0)/a2 - x( -1)/a2 + x( -2)/a 
y( -p) 
(-l)Py(O)/aP + (-1)p-1 X ( -l)/aP + ... + x( -p)/a 
In an equivalent way, we can define the convergence area of the z-transform 
of the sequence {y( n)} we wish to determine. Depending on whether z is set 
such that Izl > ex or Izl < ex, where ex has to be determined, we get a causal 
solution or an anti-causal solution, respectively (see properties 4.9 and 4.10). 
EXAMPLE 4.5 (Counterexample: homogeneous first order d.e.) 
Consider the difference equation: 
Yn -
e2j7rfoYn_1 = 0 
( 4.17) 
By changing over to the z-transform of the two sides, we get Yz (z)(l -
e2j7rfo Z-l) = 0, from which we infer Yz(z) = 0 and hence Yn = O. However, 
we can directly check, using the difference equation, that, for any A, the signal 
Yn = Ae2j7r fon is solution to (4.17). The fact that this solution cannot be found 
by changing over to the z-transform of (4.17) is precisely due to the fact that 
Yn does not have a z-transform. 

Chapter 4 - Linear Filters 127 
More generally, we have the following property: 
Property 4.4 Consider the harmonic signal defined by: 
p 
x(n) = L CXke2j7rikn 
k= l 
(4.18) 
where {CXk} is a sequence of P complex variables and {h} a sequence of P 
frequencies. 
1. there exist b1, ... , b p such that: 
x(n) + b1x(n - 1) + ... + bpx(n - P) = 0 
(4.19) 
2. the polynomial B(z) = zP + b1zP- 1 + ... + bp has its P roots on the unit 
circle. 
Conversely, if x(n) verifies 4.19 and if B(z) has all its simple roots located 
on the unit circle, then x(n) is of the type 4.18 where the CXk are P complex 
random variables. 
HINTS: 
- to prove this, let Zk = e2j7r/k and B(z) = n [ = l (z - Zk) . With these 
notations we have on one hand x(n) = L:[ = l CXkzk and on the other 
hand the fact that the P degree polynomial B(z) is such that B(Zk) = O. 
The polynomial B(z) can also be developed as B(z) = zP + b1zP- 1 + 
... + b p. We will now show that the obtained coefficients bk are such that 
x(n) + b1x(n - 1) + ... + bpx(n - P) = O. This is because we have: 
x(n) + b1x(n - 1) + ... + bpx(n - P) 
p 
p 
p 
= L CXkzk + b1 L CXkZ~-
l + ... + bp L CXkZ~-P 
k= l 
k= l 
k= l 
P 
P 
= L CXkZ~-P 
(zf: + hzk'-l + ... + bp) = L CXkZ~-P 
B(Zk) 
k= l 
k= l 
Therefore, the variable x(n) + hx(n - 1) + ... + bpx(n - P) is a linear 
combination of P variables CXk, each of them multiplied by z~-p 
B(Zk) 
which is equal to O. This proves it is null. 
- conversely, let us assume that x(n) verifies 4.19. In that case, because 
the set of solutions is a P dimension subspace, we only need to find P 
solutions. We are going to find the ones of the type x( n) = cxe2j7r in, 

128 Digital Signal and Image Processing using MATLAB® 
where ex and f are to be determined. By replacing this solution type in 
4.19, and by assuming that bo = 1, we get, after simplifying by ex, the 
equation: 
P 
P 
L bke2j7r!(n-k) = e2j7r!(n-P) L bke2j7r!ke2j7r!(n-P) B(e2j7r!) = 0 
k=O 
k=O 
But by hypothesis, B(z) has P distinct roots on the unit circle. This 
gives us the P solutions we were looking for. 
4.4.1 
Stability considerations 
We know, first that the possible convergence areas are delimited by poles, 
and second that stability is ensured so long as the unit circle belongs to the 
convergence area. Hence, a stable solution exists if and only if there are no 
poles on the unit circle. 
Theorem 4.3 (Stable solution) A system whose input x(n) and output yen) 
obey the recursive equation: 
is a stable filter if and only if: 
A(z) = 1 + alz- 1 + ... + apz- P =F 0 when Izl = 1 
( 4.20) 
In this case, the impulse response is the sequence {h( k)} of coefficients of the 
Fourier expansion of the rational function H(f) = B(e2j7r!)jA(e2j7r!), where 
B(z) is defined by 4.16, and we have: 
yen) = 2:t:-oo h(k)x(n - k) 
Causality 
When faced with the recursive equation 4.15, we can always solve it "causally", 
by writing: 
Yn = boxn + b1xn-l + ... + bQxn_Q - (alYn-l + ... + apYn-P) 
Calculating yen) requires that we know all the values of x(k) and y(k) 
for k :::; n. Obviously, this can also be inferred from the transfer function's 
expression. Hz(z), given by 4.16 as a function of z (and not of Z-l) , is such that 
the numerator and denominator polynomials are of the same degree, equal to 
max(P, Q). Therefore, because of the properties 11.5, a causal sequence {h(n)} 
exists with HAz) as its z-transform. This solution corresponds, for HAz), to 
the convergence area {z E C : Izl > maxk(lpkl)}. Here the Pk are used to denote 

Chapter 4 - Linear Filters 129 
the poles of HAz), that is the roots of A(z). However, the implementation of 
this solution is not necessarily stable. It is stable if and only if the convergence 
area contains the unit circle. This gives the following fundamental result: 
Theorem 4.4 (Stable and causal solution) 
The system whose input x(n) and output y(n) obey the recursive equation: 
Yn + alYn-l + ... + apYn-p = boxn + b1xn-l + ... + bQxn-Q 
is a causal and stable linear filter if and only if all of the poles of the transfer 
function H z(z) = B(z)/A(z) have a modulus that is strictly less than 1, that is 
to say if: 
A(z) = 1 + alz- 1 + ... + apz- P i= 0 when Izl 2: 1 
In that case, the impulse response is such that hk = 0 for k < 0 and: 
y(n) = x(n) + h1x(n - 1) + ... + hkx(n - k) + ... 
Note the importance of the words causal and stable when expressing this 
property. It is quite possible for the system described by the recursive equation 
4.15 to have, because of theorem 4.3, a stable solution (the convergence area 
of Hz(z) contains the unit circle) that is not causal. 
4.4.2 FIR and IIR filters 
When the polynomial A(z) is only a constant, equation 4.15 can be written: 
y(n) = box(n) + b1x(n - 1) + ... + bQx(n - Q) 
which can be seen, according to 4.1, as the convolution of x(n) with an impulse 
response that has a finite number of non-zero values. The filter is then called 
a Finite Impulse Response filter, or FIR filter. A direct consequence is the 
stability of this type of filter, because of the absence of poles. 
When the polynomial A(z) is not a constant, and if the rational function 
HAz) is irreducible, the filter is called an Infinite Impulse Response filter, IIR 
filter, or recursive filter. The use of this term is justified by the fact that the 
value y(n) is calculated not only using the sampled input values x(n), x(n - l), 
etc. but also the output ones y(n - 1), y(n - 2), etc. 
The expressions "Infinite Impulse Response" and "recursive" are not equiv-
alent. For example, the following filter H z(z) has a transfer function that can 
be expressed in two different ways: 
1- o;Pz-P 
H z(z) = 1 + O;z-l + ... + O;P-lz-P+l = ----:;-
1- O;Z- l 
This filter is fundamentally FIR. However, its implementation can be re-
cursive or non-recursive: 
either y(n) 
or y(n) 
O;y(n - 1) + x(n) - O;P x(n - P) (recursive) 
= x(n) + O;x(n - 1) + ... + O;P- lx(n - P + 1) (non-recursive) 

130 Digital Signal and Image Processing using MATLAB® 
Filtering implementation using MATLAB® 
The filter function, which we have already used for a finite impulse response 
filter, provides what is called the causal solution 
y(n) 
box(n) + b1x(n - 1) + ... + bQx(n - Q) 
- (aly(n - 1) + ... + apy(n - P)) 
to the recursive equation 4.15. All you have to do is type: 
y = filter(B,A,x); 
where A= [1 
a1 ... 
aP] and B= [bO 
b1 ... 
bQ]. 
The output sequence has the same length as the input sequence. The first 
term x (1) of the sequence x represents the oldest element. Theoretically, the 
calculation requires the Q past values of {x(n)} and the P past values of {y(n)}. 
Without any further explanation, these values are considered to be equal to 
zero. 
When the size of the signal to be filtered becomes too significant, processing 
in blocks is carried out. However, at each iteration the state left by the filtering 
of the previous block must be considered. This is the reason for having a fourth 
input parameter and a second output parameter for the filter function: filter: 
II [Y,Zf] = filter(B,A,X,Zi) 
In practice, knowing the number, Nb of blocks to be processed, a loop can 
be created: 
xn = fread(fid ,Lb, ... ); % first block 
[yn,Z]=filter(B,A,xn); % Z=current state 
for k=2:Nb-l 
end 
xn = fread(fid,Lb, ... ); 
[yn,Z] = filter(B,A,xn,Z); 
When using the filter function, because it provides the causal solution 
to the recursive equation, and because a filter must always be stable, it 
is imperative that there be no pole with a modulus greater than or equal 
to 1. Hence we must have A(z) i- 0 for Izi ;::: 1. 
4.4.3 
Causal solution and initial conditions 
Consider this example of a recursive equation to which we want to find a causal 
solution: 
y(n + 2) + aly(n + 1) + a2y(n) = x(n + 1) with n ;::: 0 
(4.21 ) 

Chapter 4 - Linear Filters 131 
We can write the successive expressions for n = 0, n = 1, etc. and multiply 
by 1, Z- l , Z-2, etc.: 
1 
x 
y(2) + aly(1) + a2Y(0) = x(l) 
Z- l 
x 
y(3) + aly(2) + a2y(1) = x(2) 
Z-2 
x 
y(4) + aly(3) + a2y(2) = x(3) 
By summing these relations, we get: 
Hence the ZT of the output is: 
Yz(z) = 
z 
Xz(z) + Z2y(0) + zy(l) + zy(O) - zx(O) 
z2 + alz + a2 
z2 + alz + a2 
( 4.22) 
This relation shows two terms. The first one corresponds to the "forced" 
part, or the particular solution to equation 4.21. The second term is the "free" 
part corresponding to the solution to the homogeneous equation which provides 
the solutions corresponding to the initial conditions. 
Generally speaking, if we have: 
y(n) + aly(n - 1) + .. . + aNy(n - N) = box(n) + ... + bMx(n - M) (4.23) 
with n 2: M where ak and bk are constant coefficients, applying the ZT to 4.23, 
taking into account the initial conditions, leads to expression 4.24: 
bo + b1z- 1 + .. . + bMz- M 
1 + alz-1 + ... + aNz-N Xz(z) 
Po(z) 
+----:---'---'--------:-;-
1 + alz- 1 + ... + aNz-N 
where Po(z) corresponds to the initial conditions. The quantity: 
bo + b1z-1 + ... + bMz-M 
Gz(z) = 1 + alz-1 + ... + aNz-N 
(4.24) 
is the transfer function (TF). As you can see, the transfer function coincides 
with Yz(z)/ Xz(z) when the initial conditions are equal to zero. 
It can be noted that, because of expression 4.24, simplifying, in the transfer 
function, an unstable pole of the system by an "unstable" zero (outside the unit 
disk) of another transfer function does not stabilize the system. There is no 

132 Digital Signal and Image Processing using MATLAB® 
reason for applying this simplification to the free part. Consider the following 
example with lal > 1: 
X ( ) 
(z - a)Bo(z) 
z z ---+ 
Az(z) 
There is indeed a simplification in the transfer function. However, if you 
consider the free parts of the two systems with the initial conditions polyno-
mials, Po(z) and Pb(z) respectively, the resulting free part is: 
Po(z) Pb(z) 
which remains unstable. 
These considerations can be related to structural concepts regarding filters, 
particularly their observability and controllability, which we will not discuss 
in this book. Do remember, however, that the transfer function is not enough 
to characterize the behaviour of a system defined by the recursive equation, 
because it provides us only with an input-output relation without enough in-
formation to lead to a model for the system. 
Given the method used to solve 4.21, we can consider that finding a causal 
solution requires the use of a slightly modified z-transform, called the unilateral 
z -transform. 
Definition 4.6 (Causal z-transform) 
The causal z-transform (CZT) of the sequence {x(n)} is the quantity: 
Xzc(z) = I:
~:a 
x(n)z-n 
for values of z such that R < 14 
( 4.25) 
This is the definition of the z-transform accepted by control engineers who 
are used to dealing with causal systems, and who take close interest in transient 
states. Among the properties of the CZT, the lead property is fundamentally 
different for the two transforms: 
Property 4.5 (Time advance) 
x(n + P) ~ zP Xz(z) - x(O) zP - x(l)ZP-l - . .. x(P - l)z 
( 4.26) 
The convergence area is unchanged. 
Applying this property to a recursive equation y(n + P) + aly(n + P -1) + 
... + apy( n) = box( n + P) + .. . to which we are trying to find a causal solution 
provides the free part of the answer. Using the x(n + P) ~ zP Xz(z) property 
only leads to the stationary solution (the one that "started at" n = -(0). 

Chapter 4 - Linear Filters 133 
4.4.4 Calculating the responses 
Evaluating the impulse response 
The filter function can be used to directly plot the impulse response of a 
rational transfer function filter from the coefficients of the recursive equation. 
Consider as an example the causal and stable filter whose transfer function is: 
Hz(z) = 1 + hz-
1 + b2z-2 
1 + alz-1 + a2z-2 
with b1 = 0.7, b2 = 0.6, al = 1.5, a2 = 0.9 and for which you can check that 
the two poles are complex, and that each one has a modulus smaller than 1. 
To plot its causal impulse response, type: 
%===== repimpuls.m 
N=60; 
% number of calculation points 
b=[l 0.7 0.6]; 
% numerator coefficients 
a=[l -1.5 0.9]; % denominator coefficients 
h=filter(b,a,eye(l,N»; 
stem(h) 
The input signal is a 1 followed by several ° (eye (1, N)). Obviously this 
method is not appropriate if the filter, assumed to be stable, is not causal. 
Evaluating a complex gain 
In order to calculate the complex gain Hz (e2j7r f) of a rational transfer function 
filter over L points at the frequencies f = kj L with k E {O, ... , L -1}, we need 
to calculate the quantities: 
{ 
1 + ale-2j7rkjL + ... + ape-2j7rPkjL 
bo + b1 e-2j7rkj L + ... + bQe-2j7rQk j L 
kE{0,···,L-1} 
Notice that they represent the DFTs of the sequences {1, aI , ... , ap} and 
{bo, b1 , ... , bQ } respectively, over L points. This leads to the following proce-
dure: 
The calculation of the complex gain of a filter with Hz(z) = B(z)jA(z ) 
as its transfer function over L points is performed by: 
H = fft(B,L) .J fft(A,L); 
where B and A represent the sequences of the numerator and denominator 
coefficients respectively, in decreasing powers of z. 
Exercise 4.1 (The rectangular impulse response filter) (see p. 415) 
Consider an impulse response h(n) = 1jM when n E {O, . . . ,M -1} and 
h( n) = ° 
otherwise: 

134 Digital Signal and Image Processing using MATLAB® 
1. what can be the purpose of such a filter? 
2. calculate the gain and the phase of this filter. What can you say about 
the latter? 
3. use MATLAB® to plot the gain, the phase, the index response of the filter 
h(n) for several values of M; 
4. two filters with an impulse response h(n) are arranged in a cascade. What 
is the gain of the resulting filter? 
4.4.5 Stability and the Jury test 
Examining a filter's stability requires you to know the poles of the transfer 
function Hz(z). Using the Jury, or Jury-Lee, test spares you the explicit cal-
culation of their value. It is applied to the denominator A(z) of Hz(z), which 
we will express as: 
We now set up the following 2n - 1 line array: 
ao 
al 
a2 
ai 
an 
an- l 
an- 2 
an- i 
bo 
b1 
bn- 1 
bn- 1 
bn- 2 
bo 
Co 
Cl 
Cn-2 
Cn-3 
go 
ql 
q2 
q2 
ql 
qo 
ro 
r l 
Lines 3 and 4 are filled out based on the first two lines using bi = aOai -
anan- i, then lines 5 and 6 are filled out using Ci = bobi - bn-l bn-i-l, etc. The 
necessary and sufficient condition for stability is the following set of conditions: 
ao> lan l 
Ibol > Ibn- 11 
Iqol > Iq2 1 
Irol > hi 
and all of them have to be satisfied simultaneously. In practice a 2n - 3 line 
array must be constructed such that its final values are qo ql q2 , and the 

conditions can then be expressed: 
A(l) > 0, 
{ 
A( -1) < ° 
A(-l) > ° 
when n odd 
when n even 
Chapter 4 - Linear Filters 135 
ao> lanl 
Ibol > Ibn-II 
Exercise 4.2 (Purely recursive first order filter) (see p. 417) 
Consider the purely recursive first order filter with the transfer function: 
1 
Hz(z) = 1 _ az-I 
1. this filter is assumed to be stable and causal. Determine the convergence 
area; 
2. by writing Hz(z) as the power series L kEIZ h(k)z-k, find the impulse 
response h( n); 
3. give the expressions of the complex gain, of the gain, and of the phase; 
4. using definition 4.4, determine the expression of the unit step response 
as a function of a. Determine, as a function of a, the value of A such that 
the unit step response tends to 1 when n tends to infinity; 
5. write a program using MATLAB® that calculates, with the use of the 
filter function, the 30 first samples of the index response for a = 
-2/3,a = 1/2, a = 3/4 and a = 7/8. What can you notice? 
4.5 
Connection between gain and poles/zeros 
The positions of the poles and zeros in the complex plane can easily be used 
to determine the shape of the gain. First of all, we need to study the transfer 
function of the purely recursive second order filter. 
Purely recursive second order filter 
Consider the filter whose transfer function coefficients are real: 
1 
We assume that ai - 4a§ < 0, meaning that the two poles are complex 
conjugates. They are denoted by PI = pej8 and P2 = pe-j8 . We also assume 
that PIP2 = p2 = a2 < 1 (the product of the roots is equal to a2). Under these 
conditions, the poles are inside the unit circle. 

136 Digital Signal and Image Processing using MATLAB® 
Figure 4.7 shows the positions of the poles and the unit circle. The filter 
is therefore stable and causal, and its transfer function is Hz(z), which will be 
associated with the convergence area Izl > p. We will now calculate its impulse 
response. We need to rewrite the transfer function: 
1 
Hz (z) = -,-( 1---Pl-Z---'-1 )--'(-1---p-2-Z---'l""") 
with Izl > p. By applying formula 4.5 to the two fractions in Z-l of the right 
side and by isolating the coefficient of z-n, we get for n 2: 0: 
pn+l _ pnH 
sin((n + 1)e) 
h ( n) = 
1 
2 
= pn _'-'--;-:-:--'---'-
Pl-P2 
sin(B) 
We plotted in Figure 4.6 the impulse response of this filter obtained with 
the program repimp2 . m, for p = 0.96 and B = 7r / 12: 
%===== repimp2.m 
N=60; 
% number of calculation points 
tps=[O:N-1]; 
% time vector 
theta=pi/12; rho=.96; % parameters 
%===== direct calculation of the response 
rep=rho .- tps .* sin((tps+1)*theta) / sin(theta); 
stem(tps,rep) ; grid 
The impulse response is a "damped sine". 
4 ,-----~----------------------------------_, 
3 
---,.---------- .. ----------...,-----------1----------- 1-----------
, 
" 
, 
" 
, 
, 
, 
, 
, 
, 
-r---------- "T -----------,-----------I----------- I-----------
2 
, 
, 
---------1----------- 1-----------
-1 
- - - - - - - - - - ~- - - - - - - - - - -:- - - - - - - - - - -:-- - - - - - - - --
, 
, 
, 
, 
-2 L-____ ~ __ ~U_~ 
____ ~ ______ ~ ______ ~ ____ ~ 
o 
10 
20 
30 
40 
50 
60 
Figure 4.6 - Impulse response of a second order filter whose two conjugate complex 
poles have a modulus p = 0.96 and a phase e = 7r / 12 
The farther away the poles are from the unit circle, the faster the impulse 
response will decrease to zero (it decreases like pn). The filter "forgets" the 
past faster as p is closer to O. We can therefore define a time after which the 

Chapter 4 - Linear Filters 137 
memory is "almost completely" gone, by calculating the index nR such that 
pnR can be considered to be negligible, meaning that pnR becomes < c. 
Let TJ = log(c), we have: 
TJ 
nR=--
log(p) 
( 4.27) 
This result is common to many situations: we can consider that an IIR 
filter which, in theory, has an infinite memory of the past, has an "almost 
finite" memory that becomes shorter as the poles move away from the unit 
circle. This duration corresponds to the time the filter spends "forgetting" the 
initial conditions, and to get close enough to asymptotic behavior. 
Let us now study the gain using the position of the poles. First, notice that 
the gain: 
C(f) = 1Hz (e2j7ri )I = 
1 
MP I X MP2 
where M represents a point on the unit circle with the affix e2j7r i, and PI and 
P2 the poles with PI and P2 as their respective affixes. We then see (Figure 
4.7) that as MPI decreases, C(f) increases. Consider, as an example, the case 
of a denominator: 
A(z) = 1 - O.8z- I + 0.4z - 2 
The purpoles. m program gives the position of the poles on the complex 
plane (Figure 4.7). 
%===== purpoles.m 
mycircle=exp(2*pi*lj*[O:100]!100); 
plot(mycircle); hold on 
plot(roots([l -.8 .4]),'x'); hold off; grid 
axis ( , square' ) 
The gain plot is represented in Figure 4.8. As an exercise, you can check 
that, if: 
-1 < -al (1 + a2) < 1 
4a2 
the gain shows a maximum at the frequency: 
( 4.28) 
The frequency fR is called the resonant frequency. The value C(fR) of the 
gain at resonant frequency is: 

138 Digital Signal and Image Processing using MATLAB® 
1 
0.8 
0.6 
0.4 
0.2 
0 
-0.2 
-0.4 
-0.6 
-0.8 
-1 
-1 
-0.5 
0 
0.5 
Figure 4.7 - Position of the poles on the complex plane for a purely recursive second 
order filter with al = -0.8 and a2 = 0.4. We can graphically evaluate the gain as the 
inverse of the product MP1 x MP2 
______ I ______ ~------~----
_I ______ ~ ______ ~ ______ I ______ ~ ______ ~ _____ _ 
I 
I 
I 
I 
I 
j 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
------1------,------,------,-- ---,------r------r-----'------ T------
I 
I 
I 
I 
I 
j 
I 
I 
I 
I 
I 
I 
I 
______ I ______ ~ ______ L ______ , ____________ L ______ I ______ ~ ______ ~ _____ _ 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
, 
, 
-
-
-
-
-
- I 
-
-
-
-
-
, 
-
-
-
-
- r 
- - - - - , - - - - - , - -
- T 
, 
, 
, 
, 
______ 1 _____ -1 
_____ 1 
_____ L _ 
I 
I 
I 
I 
I 
I 
I 
------,------,------,------,------,------r------ ,------, 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
o 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
Figure 4.8 - Gain of a purely recursive second order filter with al 
a2 = 0.4. Notice the resonant frequency as well as the overvoltage 
-0.8 and 
Let the input signal be x(n) = E cos(27r fRn) . According to property 4.3, the 
output signal's expression is y(n) = C(fR) E cos(27r f Rn + CPo). If C(fR) is much 
greater than 1, the amplitude can reach catastrophic values (like the ones, due 
to the wind, that caused the suspender cables on the Tacoma bridge to snap, 
in November 1940, only four months after its inauguration). 
Exercise 4.3 (Purely recursive second order) (see p. 418) 
Consider a purely recursive second order filter whose transfer function has real 
coefficients al and a2: 

Chapter 4 - Linear Filters 139 
1. sketch the gain in decibels of a second order cell whose poles are given 
by p = pej ¢ for p = 0.9 and for different values of ¢; 
2. do the same thing for different values of p, ¢ remaining constant; 
3. study the stability, in the sense of "bounded input - bounded output", as 
a function of al and a2 by applying the Jury test presented on page 134. 
General second order filter 
We now add two complex conjugate zeros to the purely recursive second order 
filter, to see how the frequency response is changed. The transfer function can 
be expressed as: 
By limiting ourselves to the case, which is actually very frequent, of zeros 
chosen on the unit circle, we get a gain equal to zero at the frequency fa = a / 27r 
where a refers to the argument of Zl. Such a gain is represented in Figure 
4.9. The value of a verifies 2cos(a ) = -1.1, which leads us to the gain's 
cancelling frequency fa = a / 27r ~ 0.3427. Choosing to put the zero outside of 
the bandpass reduced the gain's value in the frequency band where the gain 
was already small. 
--- r 
5 
-----~------
. -----
l- ______ I ______ -I ______ l. ______ I ______ -1 ______ l. _____ _ 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
-
-
-
-
- -1- -
-
-
-
-
,
-
-
-
-
-
-
-
-
-
-
-
- 1-
-
-
-
- -., -
-
-
-
-
-
T - - - - - -1- -
-
-
-
- -,- -
-
-
-
-
T -
-
-
- --
4 ____ __ : ______ .1 ______ ~ _____ 
1 ______ ~ ____ __ l. ______ : ______ ~ ______ J. _____ _ 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
- - - - - -1- - - - - - ., - - - - - - r - - - - -
- 1-
-
-
-
- -., -
-
-
-
-
- r - - - - - -1- - - - - - ,- - - - - - T - - - - --
3 ____ __ : ______ -I ______ ~ _ _ _ _ 
_ 1 ______ ~ ______ l. ____ __ : ______ ~ ______ J. _____ _ 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
------I------ , ------r------ -----,------ r ------r-----'------ r ------
2 
-----
~ 
-----
~ -
---
~ 
-----
~ -----
~ 
I 
I 
I 
I 
I 
------I------ , ------r------I----
-,------ r ------I------,------ T------
1 
1 
1 
1 
1 
____ __ 1 ______ J ______ L ____ __ 1_ _ _ _ _ _ 
_ ____ L ______ 1 ______ ..1 ______ L _____ _ 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
------: 
-----
:
------------
~
-
-----
: -----~-::.:
.-
~-
:':':':f=~+ 
OL-__ ~ __ ~ __ ~ __ ~ __ ~ __ ~~~~~ 
__ ~ __ ~ 
o 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
Figure 4.9 - Gain of a second order filter with two zeros on the unit circle: a l = 
- 0.8, a2 = 0.4. b1 = 1.1, b2 = bo = 1. Canceling frequency of the gain fo ~ 0.3427 
EXAM PLE 4.6 (Resonance and rise time) 
Consider a purely recursive second order filter with the following transfer func-
tion: 

140 Digital Signal and Image Processing using MATLAB® 
1. the poles are assumed to be complex with a given modulus p. Using 4.28, 
determine al and a2 such that the resonance frequency is fR = 0.1; 
2. derive the value of the gain G R at the resonant frequency; 
3. let the input signal be a sine signal x(n) = G[/ sin(27rfRn). Determine 
the output signal's expression; 
4. let the input signal be a "causal sine": 
Write a program that plots the output signal y(n) for values of p from 
0.99 to 0.999. What happens when n tends to infinity? 
5. what connection is there between the convergence time and the position 
of the poles? 
HINTS: 
1. because the poles are complex a2 = p2. Using 4.28: 
4a2 
al = --- cos(27rfR) 
1 + a2 
( 4.29) 
2. the gain is given by GR = 11 + ale-27rj / R + a2e-47rj/RI-l; 
3. because of theorem 4.3, 
if the input signal x(n) 
G[/ sin(27rfon), the output signal's expression is y(n) 
sin(27rfRn + ¢R) where ¢R = arg(Hz(e2j7r/ R )); 
4. Type: 
%===== cteres.m 
elf; clear all; figure(l) 
rho=[O.98:0.001:0.999]; fR=O.l; 
expfR=exp(-2j*pi*fR); 
a2=rho.-2; al=-4*a2*cos(2*pi*fR) .j (1+a2); 
HRml=1+expfR*al+expfR-2 *a2; 
GRml=abs(HRml); phase=angle(HRml); 
%===== input signal 
N=4000; mtime=(O:N-l); x=sin(2*pi*fR*mtime); 
for k=l:length(rho) 
end 
AA=[l al(k) a2(k)]; 
set(gca, 'ylim', [-1.1 1.1], 'xlim', [0 30]) 
xe=x*GRml(k); y=filter(l,AA,xe); % filtering 
plot(mtime,y); grid; 
title(sprintf('rho=%5.3f' ,rho(k))); 
set(gca, 'ylim', [-1.1 1.1]) 
pause(O.1) 

Chapter 4 - Linear Filters 141 
Notice that when n increases, the filter's output ends up tend-
ing to the sine y(n) with the amplitude 1. Everything works as 
if, after a while, the filter has "forgotten" the initial conditions; 
5. it should also be noted that the closer the pole gets to the 
unit circle, the longer it takes the filter to reach its asymptotic 
behavior. As we saw on page 137, expression 4.27 makes it 
possible to evaluate the rise time. 
Generally speaking, as the amplitude of the resonance peaks in-
creases, the time constant increases. As a consequence, a small 
amplitude input can lead to a high amplitude output so long as the 
energy is provided at the right frequency. 
Exercise 4.4 (Suppressing a sinusoidal component) (see p. 420) 
The rejection problem discussed in this exercise can be solved by using the 
location of the poles and zeros in the complex plane. We wish to suppress the 
frequential component f = fo. The first idea that comes to mind is to place a 
zero on the unit circle at the frequency f = fo . Because we want a real transfer 
function, we also need the conjugate zero. The numerator can be expressed as: 
Nz(z) = (1- e2j7rjoz-l)(1_ e-2j7rjoz-l) 
If we restrict ourselves to this transfer function, the gain is too far from 1 
for other frequencies than fo. This is why a pole is placed close to each zero. 
Here we are going to impose p e2j7r j o and p e-2j7rjo with p < 1 and ~ 1. Thus, 
when z = e 2j7rj is far away from the "pole-zero" pairs, the gain is roughly equal 
to 1. We have MP ~ MZ and MP ~ Mz (Figure 4.10). 
1 
Figure 4.10 - Graphic interpretation of the gain 
Consider now the second order filter with the transfer function: 
H 
-H 1-2 cos(¢)Z-1+ Z- 2 
z ( z) -
0 --,----.,.:..-.,.:.---:--..."----.,,. 
1 - 2pcOS(¢)Z-1 + p2Z-2 
( 4.30) 

142 Digital Signal and Image Processing using MATLAB® 
where p < 1, p:::::; 1 and Ho such that H(1) = 1: 
1. write a program that plots the frequency response of this filter; 
2. by making the approximation 271" f :::::; ¢ in the neighborhood of the reso-
nant frequency, determine the expression of the frequency interval's width 
for which the attenuation is higher than 3 dB (decibels); 
3. download ([x, Fs] = wavread ( I phrase. way ') ;) an audio file sampled 
at 8000 Hz. Add to the signal a sinusoidal component at 500 Hz. 
Perform the filtering of the signal by using the filter function with 
the filter Hz(z) previously defined. Check that the 500 Hz peak was 
suppressed by looking at the spectra of the original signal and of the 
processed signal; 
4. instead of the filter defined by 4.30, let us consider the filter the transfer 
function of which is the following: 
Hz(z) = ~ (1 + p2) - 4PCOSipZ-l + (1 + p2)Z-2 
2 
1-2pcosipZ-l+p2Z-2 
(a) verify that the gain is equal to 1 for f = 0 and f = 1/2, 
(b) verify that, if the zeros are e±jIJ: 
e _ 2pcosip 
cos -
(1 + p2) 
(4.31) 
( 4.32) 
(c) write a program that draws the poles and zeros of 4.30 and 4.31 in 
the complex plane for a few values of p, 
(d) write a program that draws the gain of 4.31 for the same values of 
p. 
A description of the gain of a filter 
In many practical cases, we have to describe a filter based on its frequency 
behavior. The frequency band is often partitioned in three "zones" (see Figure 
4.9): 
- the passband is the frequency band where the gain's values belong to the 
interval (1 - Op, 1 + op), where op ~ 0 is the passband ripple level; 
- the stopband is the frequency band where the gain's values are less than 
oa, where oa ~ 0 is the maximum allowed value for the ripples in the 
stopband; 
- the transition band is the area where the filter is "moving" between the 
stopbands and passbands. 

Chapter 4 - Linear Filters 143 
Figure 4.11 illustrates these points for a passband filter with characteris-
tics of mediocre quality, whereas Figure 4.12 illustrates the case of a lowpass 
Butterworth filter (see paragraph 4.7.3). 
transition 
band 
stopband 
I 
(dB) 
t 
o 
-
-10 
-20 
-30 
-40 
-50 
-60 
- - - - ,--- - - -,- - - --
-70 
-80 
-90 t..:..:.J=..:..:..::.:=t:..::..:..:..:.o.=::..:..:.;:..:..:..:~ 
o 
0.1 
0.2 0.3 
0.4 0.5 
0.5 
-0.5 
-1 
-1 
Figure 4.11 - Specification constraints and positions of the poles and zeros for a 
passband filter 
Summing up the temporal and spectral aspects of filtering 
You will find in this paragraph a certain number of properties that must be 
kept in mind: 
- the impulse response of an FIR filter has a finite length; 
- the impulse response of an IIR filter is the sum of "damped sinusoids" 
(when there are complex poles) that decrease exponentially, decreasing 
faster as the poles are closer to the unit circle. The decrease duration, or 
time constant, has the same order of magnitude as 1/ log (PM ) where PM 
denotes the modulus of the pole with the highest modulus; 
- a pole very close to the unit circle means an important gain at the reso-
nant frequency, which is roughly equal to arg(p) /2K; 
- the angular part of the complex plane where the poles can be found 
corresponds to the bandwidth. Depending on where the poles are, the 
filter can be low-pass, high-pass, or band pass. These names mean that 

144 Digital Signal and Image Processing using MATLAB® 
0.9 
0.8 
0.7 
0.6 
0.5 
0.4 -----:-
0.3 -----,-
0.2 
0.1 
o 
0.1 
, 
, 
, 
---1-----f----l-----
, 
, 
, 
, 
, 
, 
---~-----r----'-----
, 
, 
, 
, 
, 
, 
, 
, 
, 
---
~
-----
~
----
~
--
, 
, 
, 
, 
, 
, 
, 
, 
, 
---f-----~----~-----
, 
, 
, 
, 
, 
, 
Figure 4.12 - Specification constraints and positions of the poles and zeros for a 
low-pass Butterworth filter (paragraph 4.7.3) 
the filter allows low or high frequencies, or a given band of frequency 
components respectively, in the input signals to pass; 
- the higher the number of poles, the more the bandwidth ripples can be 
attenuated. A simplified way of seeing it is to imagine that as z travels 
along the unit circle, the overvoltages associated with each pole do not 
have the time to dampen; 
- the angular part of the complex plane where the zeros can be found 
correspond to the stopband; 
- the higher the number of zeros, the more the stopband ripples can be 
attenuated; 
- in the case where there are zeros on the unit circle, the gain is equal to 
zero at the corresponding frequencies; 
- a low-pass filter reduces the high frequency components making the tran-
sitions in the temporal domain smoother. 
4.6 Minimum phase filters 
Minimum phase filters have some optimality properties, particularly in terms 
of response time. Before studying these properties the reader must first be 
introduced to the concept of all-pass filters. 

Chapter 4 - Linear Filters 145 
4.6.1 
All-pass filters 
D efinition 4.7 (All-pass filter) An all-pass filter is a stable filter with a gain 
equal to 1. 
Theorem 4.5 Let {bd be a sequence of N complex values with their moduli 
smaller than 1. This means that the filter whose transfer function is: 
N 
z-1 _ b* 
N 
1 - b* z 
Pz(z) = II 
b :1 = II 
; with Izl > max Ibkl 
1 -
kZ 
Z -
k 
k 
k=1 
k=1 
is an all-pass, stable, causal filter and verifies: 
{
<I if Izl > 1 
IPz(z)1 
= 1 if Izl = 1 
> 1 if Izl < 1 
( 4.33) 
All you have to do is check it for the term Pk(z) = (1 - b'kz)/(z - bk). First, 
by taking z = e2j7rj , we get: 
1 - b*e2j7rj 
. 
e-2j7rj -
b* 
k 
= e2J7rj 
k 
e2j7rj - bk 
e2j7rj - bk 
the modulus of which is 1 (the modulus of the ratio of two complex conjugate 
numbers is equal to 1). For Izl < 1, notice that IPk(O)1 = 1/1bk l > 1. This 
means we necessarily have IPk(z) 1 > 1 for Izl < 1, otherwise this would contra-
dict the maximum theorem [6] for holomorphic functions. When Izl > 1, the 
same argument is used after noticing that 1 
Pk (1 I z*) 1 = 1 I 1 
Pk (z) I· 
Moreover, because the poles are strictly inside the unit circle, the filter is 
stable and causal. 
How are the zeros and poles of Hz(z) placed? Again, all we have to do 
is limit ourselves to the term (z-1 - b'k)/(l - bkz-1). The pole is in Pk = bk 
and the zero is in Zk = 1/b'k. The moduli of the complex values Pk and Zk 
are therefore the inverse of one another, and their phases are the same. In the 
complex plane, the two points are therefore the transforms of one another by 
the inversion centered in 0 with a ratio of 1 (Figure 4.13). 
Exercise 4.5 (All-pass filter, properties of the maximum) (see 
page 423) 
Prove geometrically, using the properties of the inversion, the property stated 
in theorem 4.5. 
Exercise 4.6 (All-pass filter) (see p. 424) 
Consider an all-pass filter. The input and output sequences will be denoted by 
x(n) and y(n) respectively: 

146 Digital Signal and Image Processing using MATLAB® 
Figure 4.13 - Positions of poles and zeros in an all-pass filter 
2. show that an all-pass causal filter verifies, for any N , ~ : = -oo Ix(nW ;:: 
~ : = -oo 
ly(n)12. 
Theorem 4.6 Let Hz(z) be the rational transfer function of a stable and causal 
filter. If we transform, by the inversion centered in 0 with a ratio of 1, the 
position of any zero, we get a stable, causal filter with the same gain. 
If we denote by Zl a zero of Hz(z ) and consider the transfer function: 
Z-l -
z* 
Fz(z) = Hz(z) 
_:\ 
1 -
ZlZ 
Because the poles have not moved, the filter whose transfer function is Fz(z) 
is stable and causal. However, the numerator has changed, but according to 
4.5: 
To put it simply, if IZll > 1, we made the zero go from outside the unit 
circle to inside it, and without changing the filter's gain. If we assume that 
Hz(z) has Q zeros, then there are 2Q ways of placing them, either outside or 
inside the unit circle. All the resulting filters have the same gain, but different 
phases. 
One of them, the filter with all its zeros inside the unit circle, plays an 
important role. It is called the minimum phase filter. 
4.6.2 Minimum phase filters 
Definition 4.8 (Minimum phase) A stable and causal filter is called a min-
imum phase filter if all the zeros of its transfer function, assumed to be rational, 

Chapter 4 - Linear Filters 147 
are strictly inside the unit circle. Notice that the inverse filter is also a stable, 
causal and minimum phase filter. 
Often when designing a rational filter with Q zeros and P poles, the only 
information given is the frequency gain. It is implicit that the filter is stable, 
causal and minimum phase, making the solution unique. In this case there is a 
a relationship between the gain and the phase, and that can be derived using 
the Kramers-Kronig relations. 
Exercise 4.7 (Minimum phase filter) (see p. 425) 
If the numerator of the rational function B(z)/A(z) has Q zeros, there are 2Q 
ways of placing them, either inside or outside the unit circle, without chang-
ing the modulus of B(e2j7rl)/A(e2j7rl). If A(z) =F 0 for Izl ~ 1, the impulse 
responses associated to all these rational functions are causal and stable. In 
this exercise, the index m refers to the causal, stable, minimum phase filter 
(all the zeros are inside the unit circle), whereas the absence of an index refers 
to one of the (2Q - 1) other causal and stable filters G(f) with the same gain 
IG(f)1 = IGm(f)I· The responses of the filters Gm(f) and G(f) to the signal 
x(n) are denoted Ym(n) and y(n) respectively. 
1. Show that if x( n) is causal, then IYm (0) I ~ IY(O) I· 
2. Show that L: ~ = -oo 
IYm(nW ~ L: ~ = -oo 
ly(nW· 
The results of exercise 4.7 show that, among all the causal and stable fil-
ters with the same gain, the minimum phase filter is the one with the fastest 
response. 
Definition 4.9 (Phase and group delay) 
The phase delay at fo is the quantity (its dimension is time) given by: 
T¢(fo) = -~ 
iJ>(f) I 
27f 
f 
1=10 
The group delay in fo is defined by: 
Tg(fo) = -~ 
diJ>(f) I 
27f 
df 
1=10 
Among all the filters with the same gain, the minimum phase filter is the 
one with the lowest group delay. 
As an example we will demonstrate the group delay property: since the 
zero b = Ible2j7r1b contributes the factor 1 - bz- 1 to the transfer function, the 
corresponding phase contribution is ¢b(f) = arg(l - be- 2j7r1 ). Then ¢b(f) 
contributes the following to the group delay: 
1 d¢b(f) 
-----
27f 
df 
( 4.34) 

148 Digital Signal and Image Processing using MATLAB® 
The denominator and fb are invariant by reflecting the zero b outside of the 
unit circle. However, by reflecting b outside of the unit circle, the magnitude 
of Ibl in the numerator of (4.34) is increased. Thus, having b inside the unit 
circle minimizes the group delay contributed by the factor (1 - bz- 1 ). We can 
extend this result to the general case of more than one zero since the phase of 
the multiplicative factors of the form (1 - biZ-I) is additive. 
The following example gives an explanation for the names group delay and 
phase delay. 
EXAMPLE 4.7 (Group delay, phase delay) 
Consider the complex signal x(n) = m(n) exp(2jn"fon) . x(n) can be seen as a 
sine with the frequency fo and its amplitude modulated by m(n). This signal 
is the input for a complex gain filter Ho exp(jiJ>(f)) (Figure 4.14). 
We assume that the frequency fo is greater than the bandwidth B of the 
signal m(n). Hence X(f) fills up a very narrow B frequency band around fo. 
lt is then justified to approximate iJ> by its first order series expansion around 
fo· 
x(n) = m(n) exp (2j7rfo)n) I 
I y(n) = 
--------~>o 
Hoexp(j<J!(f)). 
>0 
t 
A 
f 
Hom(n -
7 g ) exp (2j7rfo(n - 7</» ) 
m 
>0 
Figure 4.14 - Filtering by Ho exp(j<J!(f)) 
By assuming that 7 g is an integer (or the closest integer), determine the 
output signal's expression as a function of m(n), Ho, 7g , 7¢ and fo. 
HINTS: with iJ>(f) ~ iJ>(fo) + (f - fo)iJ>'(fo) and definition 4.9, we 
have: 
H(f) 
= Hoexp(jiJ>(f)) ~ Hoexp(jiJ>(fo) + j(f - fo)iJ>'(fo)) 
= Ho exp(-2j7rf07¢)exp(-2j7rTg (f - fo)) 
We also have X(f) = M(f - fo), hence: 
Y(f) = Ho exp(-2j'rrf07¢) exp(-2j7r7g (f - fo))M(f - fo) 
According 
to 
the 
delay 
property, 
the 
term T(f) 
exp( -2j7rTg f)M(f) is the DTFT of the sequence m(n - 7g ). 

Chapter 4 - Linear Filters 149 
Therefore, 
the term T(j -
fo) 
is the DTFT of the se-
quence m( n -
7g ) exp(2j'rr fon). 
If we multiply by the term 
Ho exp( -2j7r f07</» which is independent from f , we get: 
y(n) = Ho exp( -2j7r f07</»m(n - 7g ) exp(2j7r fon) 
= Hom(n - 7g ) exp(2j7r fo(n - 7</») 
In the end, we have: 
m(n) exp(2j7rfon) -t Hom(n - 7g ) exp(2j7rfo(n - 7</») 
The envelope is, on the whole, delayed by 7 g , hence the name group 
delay, and the phase of the carrier is shifted by 7</>. This result can 
easily be extended to the signal x(n) = m(n)cos(27rfon). All we 
need to do is decompose the cosine as two exponentials, one around 
- fo, and the other around + fo, 
4.7 Filter design methods 
The methods explained in this paragraph make it relatively easy to design 
the most common filters. We will only be using the window method and the 
methods taken from "discrete-time" to "continuous-time" transformations. 
The first paragraph shows the relation between the gain of a given 
continuous-time filter and the gain of the digital filter that implements it. 
4.7.1 
Going from the continuous-time filter to the 
discrete-time filter 
Consider a filter whose impulse response is h(t) with the continuous-time input 
signal x(t). The output signal is denoted by y(t). The Fourier transforms of 
x(t), y(t) and h(t) are denoted by X(F), Y(F) and H(F) respectively. 
H(F) 
input x(t) 
continuous-time 
~ 
system 
T ~ 
di,,:::timo 
system 
yin) 
Figure 4.15 - Comparing the outputs at sampling times 

150 Digital Signal and Image Processing using MATLAB® 
Consider H(F). We are going to try and find a discrete-time filter with 
an impulse response hs(n), which would have the samples ys(n) = y(nT) of 
the signal y(t) as its output when it has the samples xs(n) = x(nT) as its 
input. In order to do this, we are going to calculate on one hand the DTFT 
of the output samples of the digital filter's output, and on the other hand the 
DTFT associated with the output samples of the filter. By making these two 
expressions equal, we obtain a relation between the two filters. 
The discrete-time filter output samples we are trying to determine will be 
denoted ys(n). Using obvious notations, the DTFT of ys(n) is given by: 
(4.35) 
Because, by definition, Hs(J) is periodic with period 1, the function ii(J), 
defined on (-1/2, +1/2) by ii(J) = Hs(J)l(J E [-1/2, 1/2]), is such that: 
Hs(J) = L ii(J - k) 
k 
To put it more graphically, ii(J) represents the truncated pattern of the 
function Hs(J) in the (-1/2,1/2) band. Using formula 2.26 (see page 85), 
which gives us Xs(J) as a function of X(F), 4.35 can also be written: 
Ys(J) = LH(J-k) x ~LX((J-k)Fs) 
( 4.36) 
k 
k 
If we now assume that x(t) is (-Fs/2,+Fs/2) band limited, which hap-
pens in practice when an anti-aliasing filter is used before the analog-to-digital 
converter, X(F) = LkX(F - kFs)l(F E (-Fs/2,+Fs/2)). In this case, the 
truncated pattern of Lk X(F - kFs ), in the (-Fs /2, +Fs /2) band, coincides 
with X(F) and expression 4.36 can also be written: 
Ys(J) = ~ Lii(J - k)X((J - k)Fs) 
( 4.37) 
k 
As for continuous-time, we have Y(F) 
H(F)X(F), and therefore, by 
using once again formula 2.26, the DTFT of the sequence ys(n) = y(nT) can 
be expressed: 
Ys(J) = ~ LY((J - k)Fs) = ~ LH((J - k)Fs)X((J - k)Fs) 
(4.38) 
k 
k 
In order for 4.37 and 4.38 to coincide, we need: 
ii(J) = H(JFs)l(J E (-1/2,1/2)) 
Hence the method for constructing the gain Hs(J) from H(F): 

Chapter 4 - Linear Filters 151 
- H(F) is truncated at the interval (-Fs/2, +Fs/2); 
- the frequency scale is normalized by dividing by Fs; 
- the resulting function is periodized4 with period 1. 
Once the function Hs(f) has been determined, creating and implementing 
it requires certain techniques, some of which are given in this chapter (see 
exercise 4.10). Example 4.8 shows an application for which the filtering is 
applied directly to the frequency. 
EXAMPLE 4.8 (Analytical signal) 
As a reminder (see example 1.1), the analytical signal z(t) associated with the 
continuous-time real signal x(t), is obtained by filtering x(t) using the filter 
with 2U(F) as its complex gain, where U(F) is the unit-step function, equal 
to 1 if F > 0 and 0 if F < O. 
We also saw in the same example that x(t) was the real part of z(t) and 
that the Hilbert transform of x(t) was defined as the imaginary part of z(t): 
1. using Z(F) = 2U(F)X(F) as the frequency's expression, find the 
discrete-time filtering that creates the samples of z(t) by working with 
the samples of the signal x(t); 
2. we want to perform the frequency filtering Z(F) = 2U(F)X(F), using 
the DFT. What problems are we going to be faced with? 
3. write a program that calculates the analytical signal of the real signal 
x(n) resulting from the sampling of x(t) . Name this program siganal.m; 
4. Use the function created for plotting the impulse response of the Hilbert 
transform response, by typing siganal( [zeros (32,1) ; eye (1,32)] ) ; 
5. record a speech signal, sampled at Fs = 8,000 Hz. Using the previous 
function, calculate its Hilbert transform. Visualize the signal, then listen 
to it. 
HINTS: 
1. the analytical signal is obtained by applying the filter with the 
gain H(F) = 2U(F) to the real signal x(t) . Because we are 
working with the sampled signals, the filter's gain is, in the 
interval (-1/2, + 1/2): 
{
o when 
Hs(f) = 
1 when 
2 when 
-1/2 < f < 0 
f E {O, 1/2} 
0 < f < 1/2 
4If the resulting function shows jumps such that X(Jo) = a-
and X(Jril = a+, the 
condition X(Jo) = (a- + a+)/2 is set for continuity reasons. 

152 Digital Signal and Image Processing using MATLAB® 
The rest of the function Hs(J) is obtained by periodizing the 
expression above with period 1; 
2. in the expression Ys(J) = Hs(J)Xs(J), substituting the DFTs 
for DTFTs leads to replacing a linear convolution with a cir-
cular convolution. In practice, if the signal block on which the 
DFT is calculated is much greater than the duration of the 
filter's impulse response, the resulting error is small, except at 
the beginning and at the end of the block. This implementa-
tion can therefore be used since the impulse response of the 
analytical filter decreases like l/n; 
3. we are going to perform an L length DFT on the considered 
block signal. As x( n) is the real part of the analytical signal 
z(n), as in the continuous case, we have to verify H2U(k) + 
H2U( -k mod L) = 2 for k = 0 to L - 1. We then multiply 
by 2 the portion of the DFT that goes from the indices 2 to 
L/2 (positive frequencies), by 0 the portion of the DFT that 
goes from the indices L/2 + 2 to L (negative frequencies), and 
finally by 1 the terms for the indices 1 (zero frequency) and 
L/2 + 1 (frequency 1/2). Type: 
function sa=siganal(x) 
%!====================================================! 
%! Calculating the analytical signal of a real signal 
%! SYNOPSIS: sa=SIGNANAL(x) 
%! 
x = real signal 
%! 
sa = analytical signal associated with x 
%!====================================================! 
x=x(:); N=length(x); 
xf=fft(real(x)); 
if rem(N,2)==O 
twoUf=[l; 2*ones(N/2-1,1); 1; zeros(N/2-1,1)); 
else 
twoUf=[l; 2*ones((N-1)/2,1); zeros((N-1)/2,1)); 
end 
saf=xf .* twoUf; sa=ifft(saf); 
return 
4. we can now check the l/n decrease of the Hilbert transform 
filter's impulse response. Type: 
%===== riHilbert.m 
clear; L=32; 
%===== impulse response of the analytical filter 
riA=siganal([zeros(L,l);eye(L,l))); 
%===== impulse response of the Hilbert filter 
riH=imag(riA); 
hyperbola=zeros(L,l); 

Chapter 4 - Linear Filters 153 
hyperbola(2:2:L+l)=(2/pi) ./ (1:2:L); 
hyperbola=[O;-hyperbola(L:-l:2);hyperbola] ; 
stem(riH, 'x'); hold on; plot(hyperbola,'r: '); hold off 
The imaginary part of the result is assumed to be equal to the 
Hilbert transform. A direct calculation of the inverse DTFT 
of -jsign(f) leads us to 2/mr if n is odd, and 0 otherwise; 
5. Type: 
%===== hilphrase.m 
clear; load phrase 
yhilb=imag(siganal(y»; 
subplot(211), plot(y); grid 
subplot(212), plot(yhilb); grid 
soundsc(yhilb,8000); 
By looking at the graph of the signal resulting from the Hilbert 
transform, you can notice important modifications compared with 
the original signal, even though the signal remains perfectly clear 
when listened to. This is sometimes explained by saying that the 
human ear is mainly sensitive to the Fourier transform's modulus 
rather than to its phase, and it just so happens that the Hilbert 
transform filter's gain is equal to 1, hence the input and output 
Fourier transforms have the same modulus. 
4.7.2 FIR filter design using the window method 
The window method allows us to design finite impulse response filters, based 
on an ideal frequency response. This implementation always leads to unwanted 
ripples in the frequency response. Furthermore, the calculation time during a 
filtering operation, expressed as a number of MAC operations, is usually much 
greater than for an equivalent IIR structure. 
Its main advantage is that the calculation of the coefficients is simple. 
Other, more complex algorithms use optimization criteria or separate settings 
for passband and stopband ripples. 
We will start with an example. 
Linear phase FIR filter 
Most of the time, the window method is used to satisfy the linear phase con-
dition, which merely corresponds to a delay (see property 11.2 of the TFTD). 
For "audio" applications, this property is often required to make sure the fil-
tered signal maintains a certain level of quality. It is related to the coefficient 
symmetry property. To understand this, consider the impulse response {h( n)} 

154 Digital Signal and Image Processing using MATLAB® 
such that h(n) = h(P - n) for n E {O, . . . , P} and h(n) = 0 otherwise. We 
then have: 
H(J) 
h(O) + h(l)e-2j7rj + ... 
+h(P - l)e-2j7r(P-1lj + h(P)e-2j7rPj 
2e-j7rPj(h(O)cos(7rPf) + h(l)cos(7r(P - 2)f) + ... ) 
This expression shows that the phase of H(J) is iJ?(J) = 7r - P7r for iJ?(J) = 
- P7r f depending on the sign of the real term between parentheses. The filter 
is then called a linear-phase filter. It can easily be checked that taking h(n) = 
-h(P - n) leads to a similar result. 
In practice, in order to satisfy the symmetry properties, there are two possi-
bilities, depending on whether the impulse response we are trying to determine 
has an odd or an even number of coefficients. 
We will illustrate this with a simple example: consider a filter we want to 
create, the gain of which is represented in Figure 4.16. 
H(f) 
-+--~--~----~--~f 
-112 
-114 
114 
112 
Figure 4.16 - Example: half-band filter 
This filter is called a half-band filter (note that H(J)e-7rjj also obeys this 
property). 
1. If N is odd, the coefficients are given by: 
h(n) = 
H(J)e2j7rnj df = 
e2j7rnj df = sm n7r 
/
1/2 
/ 1/4 
. ( 
/2) 
-1/2 
-1/4 
n7r 
The moduli of the terms of this infinite length sequence decrease with n. 
Keeping only N terms introduces an error in the filter's output signal. In 
our example with N = 15, there is some justification for keeping only the 
terms for the indices from n = -7 to n = +7. Hence this filter is non-
causal. When performing real-time filtering, the impulse response has to 
be delayed by 7 samples to ensure causality. This leads to an equivalent 
output delay of (N - 1)/ 2 samples. 
Causal implementation gives the finite impulse response: 

Chapter 4 - Linear Filters 155 
n 
0 
1 
2 
3 
4 
5 
6 
7 
h(n) 
1 
0 
1 
0 
1 
0 
1 
1 
-
7'IT 
5 'IT 
-
3'IT 
'IT 
2 
n 
8 
9 
10 
11 
12 
13 
14 
h(n) 
1 
0 
1 
0 
1 
0 
1 
'IT 
3'IT 
5 'IT 
7'IT 
The filter's complex gain is: 
Its phase is linear, and given by if>(f) = ,6. - 147r f, where ,6. equals 0 or 
7r, depending on the sign of the term between parentheses. 
2. If N is odd (see Figure 4.17), the coefficients are given by: 
h(n) 
/1/2 [H(f)e - j'ITjj e2j'ITnj df = /1/4 e- j'ITj e2j'ITnj df 
- 1/2 
- 1/4 
sin((2n - 1)7r/4) 
(2n-l)7r/2 
f 1 I 
f 1 
-4 -3 -2 -1 0 1 2 
3 4 
-3 -2 -1 0 1 2 
3 4 
Figure 4.17 - Half-band filter: comparison of the impulse responses in the odd case 
(a) and in the even case (b) 
With N = 6, the indices n from -2 to +3 are kept. The causal imple-
mentation consists of designing the finite impulse response filter: 
n 
o 
1 
2 
v'2 
v'2 
v'2 
-
5'IT 
3'IT 
'IT 
h(n) 
3 
v'2 
'IT 

156 Digital Signal and Image Processing using MATLAB® 
This linear phase filter has the complex gain: 
H(f) 
_e- 5)"j -- cos(5n}) + - cos(3nf) + cos(7rf) 
2)2 
. 
( 
1 
1 
) 
7r 
5 
3 
Type: 
%===== evenodd.m 
Lfft=512; freq=[0:Lfft-1]' 1 Lfft; 
%===== odd case 
N=11; K=floor((N-1)/2); idx=(-K:K); 
hodd=sin(idx*pi/2) .1 idx 1 pi; 
hodd(K+1)=.5; hodd=hodd/sum(hodd); Hif=abs(fft(hodd,Lfft)); 
%===== even case 
N=12; K=N/2; idx=2*(-K+1:K)-1; 
heven=2*sin(idx*pi/4) .1 idx 1 pi; heven=heven/sum(heven); 
Hpf=abs(fft(heven,Lfft)); 
%===== drawing the gains and theoretical frequency response 
plot (freq,Hif, '-' ,freq,Hpf, 'r') 
hold on; plot([O 0.25 .25 .5] ,[1 1 0 0],' : ') ; hold off 
set(gca, 'XLim' ,[0 1/2]); grid 
Algorithm 
To sum up, the window method comprises the following steps: 
Steps: 
1. consider the complex gain H(f) we want to implement and the 
number N of the filter's coefficients; 
2. the coefficients h( n) are determined by: 
and we then calculate N values symmetrically spaced-out around 
n = 0; 
3. if needed, the resulting sequence is multiplied, term-by-term, by a 
sequence w(n) called a weighting window. 
Figure 4.18 shows a comparison of the answers calculated by the evenodd. m 
program. 

Chapter 4 - Linear Filters 157 
1.4
'---~--~--~--~--~--~--~--~--~--' 
, 
, 
, 
-----
------1------1-------1------
, 
, 
, 
, 
, 
, 
, 
, 
, 
, 
, 
, 
----- -----l------r-----l------
, 
, 
, 
, 
, 
, 
, 
, 
, 
0.4 
------1 ----- 1 ------1 -----
----- - 1 ' -----
-----
~ ------
:------
~ --
I 
\ 
I 
I 
I 
I
' 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
0.2 
-----~------~-----~------
-----~--
--
-----~------~-----~------
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
O L---~--~--~--~--~--~--~~~==~~ 
o 
0.05 
0.1 
0.l5 0.2 
0.25 
0.3 
0.35 
0.4 
0.45 0.5 
Figure 4.18 - Half-band filter: frequency response for N = 11 and N = 12 
Type of filters obtained with weighting windows 
Up until now, we have designed two types of filters for which the frequency 
response H(f ) was A or A e- j7r! . We also could have considered a complex 
gain A sign (f) or A sign(f)e-j7r! . A can be calculated by "standardizing" the 
filter, generally ensuring H(O) = 1, if it is possible, or L Ih(nW = 1. 
If we assume that we have made the impulse responses {h(n)} causal, we 
end up with four possibilities, four types, depending on the characteristics of 
the impulse responses: 
1. type I: N odd and h(n) = h(N - 1 - n); 
2. type II: N even and h(n) = h(N - 1 - n); 
3. type III: N odd and h(n) = -h(N - 1 - n); 
4. type IV: N even and h(n) = -h(N - 1 - n). 
We will denote by P = L N /2 J the integer part of N /2: 
1. in the first, by a direct calculation: 
P-l 
N-l 
H(f) 
L h(n)e- 27rjn! + h(P)e- 27rjP! + L h(n)e-27rjn! 
n = O 
n = P+l 
P-l 
P-l 
L h(n)e-27rjn! + h(P)e- 27rjP! + L h(m)e-27rj(N-l-m)! 
n = O 
m = O 
( 
P-l 
) 
e-7rj (N - l)! 
2 ~ h(n) cos7fj(N - 1- 2n) + h(P) 
e- 7rj (N - l)! HI (f) 

158 Digital Signal and Image Processing using MATLAB® 
HI(f) is the resulting filter when the coefficients h(n) are chosen sym-
metrically about n = 0 (hence before making the sequence causal). For 
a low-pass, as we have already seen, the result is: 
h(n) = lfe Ae27rjnf df = A sin(27mfc) 
- fe 
n7r 
2. in the second case: 
P-l 
N-l 
H(f) 
L h(n)e-27rjnf + L h(n)e-27rjnf 
n=O 
n=P 
( 
P-l 
) 
e-7rj(N-l)f 
2 ~ h(n)cos7rf(N -1- 2n) 
e-7rj(N-l)f HlI(f) 
For a low-pass, we get: 
3. in the third case h(P) = 0 and: 
P-l 
N-l 
H(f) 
L h(n)e-27rjnf + h(P)e-27rjPf + L h(n)e-27rjnf 
n=O 
n=P+l 
e- 7rj(N-l)f (2j ; 
h(n)sin7rf(N -1- 2n)) 
e-7rj(N-l)f HIlI (f) 
For a low-pass (defined by IH(f)1 = 1), we get: 
h(n) = lfe jA sign(f)e27rjnf df = A cos(27rnfc) - 1 
-fe 
n7r 
H(f) shows a "discontinuity" at the origin (gain = 0 for f = 0); 

Chapter 4 - Linear Filters 159 
4. in the fourth case: 
P-l 
N-l 
H (f) 
~ 
h(n)e-2~jnf 
+ ~ 
h (n)e-2~jnf 
n=O 
n=P 
P-l 
N-l-P 
~ 
h (n)e-2~jnf 
+ ~ 
h(N - 1 _ m)e-2~j(N-l-m)f 
n=O 
m=O 
( 
P-l 
) 
e-~j(N-l)f 
2j ]; h(n)siwTrf(N - 1 - 2n) 
e-~j(N-l)f 
H1V(f ) 
For a low-pass, we get: 
h(n) = f
fe j A sign(f)e-j~f 
e2~jnf 
df = 2A cos(27rnfc - 7rfc) - 1 
-fe 
2n7r -
7r 
H(f) shows a "discontinuity" at the origin as for the previous case. 
When designing a low-pass filter, it is actually preferable to choose types I 
and II. However, if the frequency response has to be asymmetrical, types III 
and IV can be used. As an example, type: 
%===== lowpass2 .m 
% normalization sum(h- 2(n»=1 if norml=true 
clear; nfft=1024; freq=[O:nfft-l] '/nfft; 
norml=true; 
fc=1/8; 
N=8; Nt=2*N; 
nodd=[-N:N]' ; 
% low-pass filter [-fc,+fc] 
% 2N+l coefficients 
neven=[-N+l:N] '; 
% 2N coefficients 
%===== type I (odd) 
hnI=sin(2*pi*nodd*fc) ./ nodd / pi; 
hnICN+l)=2*fc ; 
if norml, hnI=hnI/sum(hnI'*hnI); end 
hnIs=fft(hnI,nfft); hrIs=abs(hnIs) ; 
%===== type II (even) ===== 
hnII=sin(2*pi*neven*fc-(pi*fc» 
. / (neven-(1/2» 
/ pi; 
if norml, hnII=hnII/sum(hnII'*hnII); end 
hnIIs=fft(hnII,nfft); hrIIs=abs(hnIIs); 
%===== type III (odd) ===== 
hnIII=(cos(2*pi*nodd*fc)-1) ./ nodd / pi; 
hnIII(N+l)=O; 
if norml, hnIII=hnIII/sum(hnIII'*hnIII); end 
hnIIIs=fft(hnIII ,nfft); hrIIIs=abs(hnIIIs); 

160 
Digital Signal and Image Processing using MATLAB® 
%===== type IV (even) ===== 
hnIV=2*(cos(2*pi*neven*fc-pi*fc)-1) ./ (2*neven-l) / pi; 
if norml, hnIV=hnIV/sum(hnIV'*hnIV); end 
hnIVs=fft(hnIV,nfft); hrIVs=abs(hnIVs); 
subplot(211); 
plot (nodd,hnI , '-b' ,nodd,hnI, 'ob'); hold on, 
plot (neven,hnII, ':b' ,neven,hnII, 'ob'); 
plot (nodd,hnIII, '-r' ,nodd,hnIII, 'or'); 
plot (neven,hnIV, ':r' ,neven,hnIV, 'or'); 
hold off, grid 
subplot(212); plot(freq, ChrIs hrIIs hrIIIs hrIVs]); 
set(gca, 'xlim' ,[0 .5]); grid; 
0.4 
0.45 
0.5 
4.75 
4.65 
______ I _________ J _________ l ____ _ 
0.06 
0.065 
om 
Figure 4.19 - Impulse responses and gains for the four types of low-pass filters 
In cases III and IV a specific low pass filter is used mainly because the gain 
is offset when f = 0: "the continuous component is eliminated" (Figure 4.19). 
To sum up, if we choose the gain A(f), the impulse response calculations 
are done by: 
1. type I: h(n) = J!fc A(f)e27rjnj df; 
2. type II: h(n) = J!fc A(f)e- j7rj e27rjnj df; 
3. type III: h(n) = J!fc jA(f) sign(f)e27rjnj df; 
4. type IV: h(n) = J!fc jA(f) sign(f)e-j7rj e27rjnj df· 

Chapter 4 - Linear Filters 161 
Weighting window 
Keeping only a finite number of terms of the impulse response {h( n)} amounts 
to multiplying it by an N width rectangle, and therefore to convoluting H(j) 
with the function: 
This weighting function, called the rectangular weighting window, causes 
ripples in the bandpass and the stopband, and widens the transition band, 
hence the idea of applying other windows so as to modify these properties. 
Among the most common ones, we can mention the Bartlett, Hamming, Hann, 
and Kaiser windows [27], etc. Let us examine the Hamming window defined 
by: 
WH(n) = 0.54 - 0.46 cos (~~n1) 
, for n E {O, . . . ,N -I} 
(4.39) 
We plotted in Figure 4.20 the gain of an ideal half-band filter as well as 
the gains of the filters obtained by the window method, with the rectangular 
window and the Hamming window respectively. 
0.8 
0.6 
0.4 
0.2 
transition 
.---~----~--~----~ 
-~--~----~--~----~ 
ba':ld 
i---'::::::..--+--....;::;.-==-S::--~"<---l
-------
,-------
, -------:-------f-------
passband 
-------:-- Hamming -:-------~-------
/ 
_ vvindo\\i 
-: 
-
; 
stopband 
OL-~~--~--~--~~~~~~--~~~~~~~~ 
o 
0.05 
0.1 
0.15 
0.2 
0.4 
0.45 
0.5 
rectangular 
window 
Figure 4.20 - Gain of a 15th order filter for a rectangular window and a Hamming 
window 
Notice that reducing the ripples in the passband and in the stopband causes 
the transition band to widen, in other words the gain decreases more slowly 
around the frequency f = 1/4. 
Figure 4.20 is obtained using the program: 
%===== rifham.m 
Lfft=1024; freq=[O:Lfft-l] I 
/ Lfft; N=15; K=(N-l)/2; 
%===== rectangular window 

162 Digital Signal and Image Processing using MATLAB® 
h=sin((1:K)*pi/2) ./ ((1:K))/pi; h=[h(K:-1:1) 1/2 h]; 
h=h/sum(h); Hf=abs(fft(h,Lfft)); 
%===== Hamming window 
ham=.54-.46*cos(2*pi*(0:N-1)/(N-1)); hh=h .* ham; 
hh=hh/sum(hh) ; Hfh=abs(fft(hh ,Lfft)); 
plot(freq,[Hf;Hfh]); axis([O 1/2 0 1.4]); grid 
%===== drawing the theoretical frequency response 
hold on; plot([O 0.25 .25 .5] ,[1 1 0 0],': I); hold off 
The method gives rise to the following comments: 
- the integral we wish to calculate requires the analytical expression of 
H(f); 
- the resulting filter is stable (by definition, since it has no poles); 
- because the impulse response is symmetrical, the transfer function has 
zeros on both sides of the unit circle. Therefore, it is not a minimum 
phase filter; 
- its phase can be linear (piecewise) , unlike the IIR; 
- the ripples do not have a constant amplitude; 
- as we saw with the low-pass filter, the passband and stopband ripples are 
the same; 
- transition bands are widened because of the width of the chosen window's 
main lobe; 
- unwanted ripples are due to side-lobes. 
NOTE: the window method is mainly used because it can be used to provide 
linear phase filters. This property has to do with the symmetry of the coeffi-
cients: it is important for the weighting window to preserve this symmetry. In 
particular, the two extreme values, for n = 0 and n = N - 1, are equal (see 
expression 4.39). Bear in mind that the windows used to weight the signals do 
not have this symmetry, but are periodic with period N. 
Exercise 4.8 (Window method: low-pass filter) (see p. 425) 
We want to design an ideal low-pass filter, the gain of which is represented in 
Figure 4.21: 
1. determine the expression of h( n) in the cases where the length is chosen 
even, and where it is chosen odd; 
2. write the rif (N, fO) function, which determines the filter's coefficients 
based on the length N and the cancelling frequency fa. Write this func-
tion by implementing a Hamming window. 

, 
-112 
Chapter 4 - Linear Filters 163 
, f 
112 
Figure 4.21 - Ideal low-pass 
Using the angle function, use a program to check that the phase is piecewise 
linear. 
Exercise 4.9 (Window method: band-pass filter) (see p. 426) 
Let h( n) be the impulse response of a filter: 
1. what is the complex gain of the filter 2h(n) cos(27rnfo)? How can a band-
pass filter be designed using a low-pass filter? 
2. write a program that uses the fir function to design a band-pass filter 
centered at 0.2 with a width of 0.1. 
Exercise 4.10 (Window method: derivative filter) (see p. 427) 
Let x(t) be a continuous-time signal and let X(F) be its Fourier transform: 
1. show that the derivative of the Fourier transform of x(t) has the expres-
sion 2j7rFX(F). Using a procedure similar to the one found on page 149, 
determine the complex gain of a discrete-time derivative linear filter; 
2. using this result and the window method, find the coefficients of the 
FIR filter that approximates a derivative filter for an odd number of 
coefficients; 
3. the resulting filter is not causal. Give a causal solution to the problem. 
What is the consequence on the operation performed? 
4. write the derivative filter as a MATLAB® function and test it on different 
types of signals, in particular on a signal of the type sin(27r fon). 
Remez filters 
The Remez filtres are optimal for the examined waves. They are based on a 
mathematical assumption that the best polynomial approximation, measured in 
terms of maximum difference, of a continuous function is given by a polynomial 
satisfying the conditions of Chebyshev'S equioscillation theorem equioscillation 
theorem. The obtained filters are called equiripple FIR filters. 
MATLAB® in the signal toolbox provides the function firpm that uses the 
Parks-Mac Clellan algorithm to synthesize such filters. 

164 Digital Signal and Image Processing using MATLAB® 
4.7.3 IIR filter design 
A rather common method of recursive filter design is based on the continuous-
time/discrete-time change. Starting off with a "continuous-time" filter, the 
characteristics of which are known, all that needs to be done is to "discretize" 
that filter. This isn't the only way to proceed of course. Other design methods 
can be found in more specialized books, in particular methods that do not 
require changing over to continuous-time (notably the Remez method). 
A few characteristics of the analog filters used 
1. The Butterworth filters have a continuous-time transfer function H (s) 
(Laplace transform of their impulse response) resulting from a "normal-
ized" gain function that can be expressed: 
where Sk = j7r (2~;:;-1 
+ ~). 
The situation changes depending on whether 
n is odd or even: 
n/2 
1 
H even = II -::------,-,----,----,----,---
k= l 82 + 28cos((2k -1)7r/2n) + 1 
(n- l)/2 
1 
H 
__ 1_ II 
odd -
8 + 1 
k = l 
8 2 + 28cos(k7r/n) + 1 
Butterworth filters show no ripples in their passband and stopband. 
The unstandardized versions can be obtained using 8 -t 8/WO: 
n/2 
2 
Heven = II 
Wo 
k= l 8 2 + 28Wo COS( (2k - 1)7r /2n) + W5 
(n-l)/2 
H 
-~ 
II 
odd -
8 + Wo 
k= l 
omega5 
82 + 28Wo cos(k7r In) + w5 
2. Type 1 Chebyshev filters have the gain IHf(fW = (1 + c:2T;(f))-1 with: 
T x _ { cos(narccos(x)) 
n( ) -
cosh(nargcosh(x) 
for 
for 
Ixl < 1 
Ixl > 1 
And hence for k E {l, . . . ,n}, Pk = -sinh(ex)sin((2k -
1)7r/2n) + 
j cosh(ex) cos((2k - 1)7r/2n) with ex = argsinh(c
1 )/n. 

Chapter 4 - Linear Filters 165 
3. Type 2 Chebyshev filters have the following gain: 
where fa is the frequency where the stopband begins. And therefore: 
27f faAk 
. 27f faBk 
Pk = A2 
2 - J A2 
2 for k E {I, ... ,n} 
k + Bk 
k + Bk 
. h{ A k =-sinh(a)sin((2k-1)7f/2n) 
WIt 
Bk = cosh(a) cos((2k - 1)7f /2n) 
a = argcosh(A;;:-I)/n, whereAa is the imposed passband amplitude. The 
zeros are placed on the imaginary axis: 
2j7f fa 
Z k = ---,--,---''--'--:----,-----,-
cos((2k -1)7f/2n) 
Chebyshev filters show: 
- either a passband ripple but none in the stopband (type 1 filters); 
- either a stopband ripple but none in the passband (type 2 filters). 
The passband ripple is equal to 1/v1 + E2. The Chebyshev filter have 
better attenuation characteristics than the Butterworth filters. The band-
pass ripple, the attenuation and the integer n all need to be known in 
order to calculate these filters. 
4. Gauer filters or elliptic filters are optimal in terms of the transition band, 
and have the following gain: 
2 
1 
IG(f)1 = 1+E2R~(f,L) 
where Rn is a rational Chebyshev approximation, and where L character-
izes the attenuation. The Cauer filters show ripples in the bandpass and 
in the stopband. The band pass ripple is equal to 1/v'f+"E2. The band-
pass ripple and the minimum attenuation in the stopband are needed to 
be able to calculate these filters,which are defined by arrays. 

166 Digital Signal and Image Processing using MATLAB® 
U sing the bilinear transform 
Using the bilinear transform is justified by the calculation of an integral with 
the trapezoid method. We will write Xn = x(nT) to denote the function's 
values at points nT. If Sn is the value of the integral from 0 to nT, Sn obeys 
the recursive equation: 
T Xn + Xn-l 
Sn=Sn-l+ 
2 
B(z) = S(z) = '£ 1 + z-l 
X(z) 
2 1 - Z-l 
B (z) is an approximation of the integral operator, which is expressed 1 I S in 
the Laplace transform. The bilinear transform method consists of replacing S 
with II B(z) in the expression of the continuous-time filter's transfer function. 
[9, 2J. If the sampling is done fast enough, the frequency distortion caused by 
this transformation is negligible. When this is not the case, there are methods 
for compensating this distortion (to a certain extent). 
Exercise 4.11 (Butterworth filter) (see p. 429) 
We wish to perform an IIR filter design based on the Butterworth filter: 
1. write a program designed to calculate the coefficients of the Butterworth 
filter's denominator, for a given value of n; 
2. write the program that provides the frequency response of the Butter-
worth filter for a few orders (for example, for n from 2 to 6). The gain, 
in decibels, will be chosen as the y-coordinate, and IOglO(W) as the x-
coordinate (consider using the function logspace) , where W is the angu-
lar frequency in radl S (MATLAB® uses the function polyval to carry 
out this calculation); 
3. using the bilinear transform to obtain the digital filter: 
(a) consider the polynomial G(x) = ao + alX + ... + aNxN. Write its 
development as a recursive relation based on the Horner polynomial 
representation5 , 
(b) the variable change x = B(z)IA(z) is made, where A(z) and B(z) 
are polynomials. Write the previous relation for the numerator and 
denominator polynomials, 
( c) write the program that performs the bilinear transform of a polyno-
mial; 
4. compare the obtained discrete and continuous spectra. 
5 A polynomial xn + alXn - 1 + ... + an can be developed by writing xC . .. (x(x + all + 
a2) ... ) + an. This is known as the Horner polynomial representation or the Horner scheme. 

Chapter 4 - Linear Filters 167 
Using the DFT 
A method that would seem reasonable would be, for filter design, to start with 
values of the DTFT H(J) (actually the values of the DFT), and to try to find 
the original {h(n)} using the IDFT. As we are going to see in exercise 4.12, it 
actually is not such a good idea. 
Exercise 4.12 (Temporal aliasing and DFT) (see p. 433) 
Consider H (J), the complex gain of a filter with the impulse response {h( n)}. 
Give the expression of the filter's impulse response {h(n)}, calculated based 
on the DFT H(k/N) as a function of the h(n). Compare the results for the 
window method and this method, on a low-pass filter. Compare the resulting 
gains. 
4.8 Oversampling and undersampling 
The oversampling and undersampling operations play an important role in 
digital signal processing. 
Oversampling by an integer factor M consists of performing an interpolation 
on the sequence x(n) by calculating M - 1 intermediate values between two 
consecutive points. 
Undersampling by an integer factor M consists of calculating, based on a 
sequence sampled at the frequency Fs , the values of that same sequence as if it 
had been sampled at Fs/M. Undersampling does not mean simply taking one 
out of every M samples of the original sequence. 
A typical application of oversampling and undersampling is the frequency 
change. In order to go from 42 kHz to 48 kHz, for example, you can start by 
oversampling by a factor of 8, and then undersample by a factor of 7. 
4.8.1 
Oversampling 
We are going to start with an example. Let x(n) be a sequence with Xz(z) 
as its z-transform. Consider the sequence y(n) = x(n/4) for n = 0 mod 4 and 
y( n) = 0 if n -I- 0 mod 4. This operation, called the expansion operation, inserts 
three 0 in between the terms of the sequence x(n) . Notice that the sequence 
y(n) contains 4 times more samples than the sequence x(n), and theoretically, 
should be interpolated at 4 times the sampling frequency Fs associated to x(n). 
We will now determine the expression of the z-transform of the sequence 
y(n) as a function of x(n). With obvious notations, we have: 
Yz(z) = L
x(n)z-4n = X z(z4) 
k 

16S Digital Signal and Image Processing using MATLAB® 
With z = e2j7rj the DTFT can be expressed as Y(f) = X(4f). The interval 
(-1/ 2,1/2) contains the spectrum of x(n) replicated four times. We say that 
there are images in the spectrum (see Figure 4.22). 
This means that if you consider the samples y(n) corresponding to a 
continuous-time signal sampled at the frequency F~ 
= 4Fs, the spectrum is 
made up of these images in the (-2Fs, +2Fs) band (see p. S4) . Inserting zeros 
where real values should be has added high frequency components correspond-
ing to the brutal transitions introduced in the temporal sequence (Figure 4.22). 
~
(f) 
. 
--',-
L: 
\ 
I 
I 
I 
\ 
I 
I 
I 
\, 
: 
:
: 
...... 
I 
I 
I 
--' .. 
.
. ~ 
-.f 
-1/2+1/2 
+1 
Y(f) 
- 112 
-118 
118 
Figure 4.22 - Effects of oversampling 
If we have a sequence y( n) and wish to reconstruct the intermediate samples 
without causing the distortion due to the images, we simply need, after the 
expansion operation, to perform a gain F~/ Fs = 4 filtering in the (-l/S, l/S) 
band, in order to suppress the high frequencies found in the (-1/2, -l/S) and 
(+ l /S, + 1/ 2) bands. The resulting signal, after being reconstructed at the 
frequency 4Fs, is located in the (-4Fs/S, +4Fs/S ) = (-Fs/ 2,+Fs/ 2) band, 
with 4 times the number of samples. 
All these results can easily be generalized. If we insert (M - 1) zeros in 
between the elements of the sequence x( n) , the resulting sequence's z-transform 
has the expression: 
(4.40) 
In order to properly oversample, this expansion has to be followed by a low-
pass filter in the (-1/2M, 1/2M) band, corresponding, after reconstruction at 
the frequency F~ = M Fs, to the original (-Fs/ 2, Fs/ 2) band with M times the 
number of samples. What should be remembered of all this is written below: 
In order to overs ample a signal x(n) by a factor M, one method consists 
of inserting (M - 1) zeros in between the values of the signal, and then 
to perform an M gain filtering in the (-1/2M,+1/ 2M) band. 

Chapter 4 - Linear Filters 169 
This method is not the only possibility available for interpolating a signal. 
Polynomial interpolations are also widely used. The method based on the 
insertion of zeros followed by filtering is however quite effective in the case of 
real-time processing. 
Exercise 4.13 (Interpolation) (see p. 434) 
1. Write a function that interpolates by a factor of M; 
2. apply this function to the x=rand (1,40); sequence. 
EXAMPLE 4.9 (Expansion and frequency translation) 
Starting off with a speech signal the spectrum of which is placed, for the positive 
frequencies, in the (0 Hz - 4,000 Hz) band, we are going to listen to the signal 
resulting from the following operations (Figure 4.23) : 
1. The frequency scale is expanded by a factor of 5/4. Mathematically 
speaking, this means that if Sx(F) refers to the original signal's spectrum, 
the modified signal's spectrum is Sy (F) = Sx (5F /4). Therefore, the 
spectrum can be found, for the positive frequencies, in the (0 Hz - 5,000 
Hz) band. How is this achieved? 
2. The signal's spectrum is shifted by Fa = 1 kHz toward the positive fre-
quencies. This means that if S;}; (F) refers to the part of the signal's 
spectrum found in the positive frequencies of the original signal, then the 
part belonging to the positive frequencies of the modified signal's spec-
trum is S:(F) = S;};(F-Fa). Hence the spectrum is now, for the positive 
frequencies, in the (1, 4) kHz band. How is this achieved? 
Figure 4.23 - Expansion and frequency translation 
Write a program using MATLAB® that performs these two operations. Lis-
ten to the obtained signals and compare the results. 

170 Digital Signal and Image Processing using MATLAB® 
HINTS: 
1. According to the time scale expansion/compression property 
(see Chapter 1), all that is needed to expand the spectrum is 
to take the signal sampled at the frequency Fs = 8,000 Hz, 
and listen to it at the reconstruction frequency % Fs Hz. Type: 
soundsc(x, 10000); 
In order to compare with the signal obtained in the next 
question, you can also construct the signal interpolated by 
a factor of 2 corresponding to the sampling frequency F~ 
= 
2Fs = 16,000 Hz and listen to the result at the frequency 
% 
F~ = 20,000; 
2. the original signal, sampled at Fs = 8,000 Hz, is in the (-4, 
+4) kHz band. Because of the frequency translation, the de-
sired signal is in the (-5, +5) kHz band. Hence interpolation 
must first be performed in order to have a sampling frequency 
at least equal to 10,000 Hz. To make the interpolation op-
erations simpler, we will set F~ = 16,000 Hz, by using the 
interpM function with an interpolation factor of 2. 
The spectrum has then to be shifted by 1 kHz. 
This can 
be done by multiplying the signal by the function e2j7r1000t 
sampled at the frequency F~ 
= 16,000 Hz. 
The (+ 1, +5) kHz band then has to be filtered. To this pur-
pose, a low-pass filter is implemented in the (-2, +2) kHz band 
and shifted in frequency by 3 kHz. This result is achieved using 
the rif function in the (-b, b) band where b = 126000000 followed 
by a multiplication of the filter coefficients by e2j7rn3000/16000 . 
Type the program: 
%===== frqshift .m 
close all; clear all 
Fs=8000; load phrase.mat; % or [sn,Fs]=wavread('phrase.wav'); 
Fep=2*Fs; xi2=interM(sn,2,100); 
FO=1000; xi2trans=xi2.*exp(2*j*pi*(0:length(xi2)-1)'*FO/Fep); 
%===== low-pass filter 
Lh=201; h=rif(Lh,Fs/4/Fep); 
%===== band-pass filter centered on Fc 
Fc=Fs/4+FO; htrans=h . * exp(2*j*pi*(0:Lh-1)*Fc/Fep); 
xi2transfiltre=filter(htrans,1,xi2trans); 
xtrans=real(xi2transfiltre); 
%===== listening (sound or soundsc depending on the version) 
soundsc(sn,Fs); disp('Press a key'); pause 
soundsc(sn,5*Fs/4) ; disp('Press a key') ; pause 
soundsc(xtrans ,Fep); 

Chapter 4 Linear Filters 171 
Digital-to-analog conversion 
In paragraph 2.1.3, we saw that distortions appear during the signal's recon-
struction simply by using a ZOH. In the case of audio frequency applications 
(speech, music, etc.), a simple way of avoiding this is to place before the ZOH 
an overs ampler with a high enough factor M. 
This is because oversampling spreads further apart the periodized compo-
nents of the signal's spectrum. Furthermore, concerning the ZOH, working M 
times faster "widens" the sine cardinal lobes. 
Figure 4.24 shows what the spectrum looks like for the output signal of the 
ZOH, for M = 5 and for a sampling frequency of Fs = l/T = 8 kHz. The 
result should be compared to the one in Figure 2.9 of paragraph 2.1.3. 
1,-~~--~--~--~~~~~--~--~--~~-, 
0.8 
0.6 
0.4 
0.2 
OL-~~--~--~--~~~--~--~--~~~~ 
-50 
-40 
-30 
-20 
-10 
0 4 
10 
20 
30 
40 
kHz 
1.---~--~--~--~--~ 
__ --~--~--~--~---, 
0.8 -----+-----;------f-----+---
I0.
----f------
i------~------j------
0.6 ------: -----, ----- r -----:--
, 
, 
0.4 
------:------~------~------:----
I 
I 
I 
I 
---r------ r -----'------T------
I 
I 
I 
I 
, 
" 
0.2 
------
: ------
~ ------
~ ------
: --
I 
I 
I 
I 
--- y ------------ , ------ T -
o 
r-- :/"I
: 
: 
: 
: 
: 
~
: ,-
-50 
-40 
-30 
-20 
-10 
o 4 
10 
20 
30 
40 
kHz 
Figure 4.24 - Output spectrum of a ZOH preceded by an oversampling with a factor 
ofM = 5 
The part found in the (-Fs / 2, Fs /2) undergoes a slight distortion, since 
sinc(fT / M) stays close to 1. Beyond Fs /2, the first term due to the periodiza-
tion of the spectrum can be found around 40 kHz, outside of the audible band. 
This method is often used by the boards installed in our computers to avoid 
having to use an analog high-quality low-pass filter: the signal is oversampled 
by a factor M so as to have M x Fs greater than 40 kHz, and the obtained 
values are maintained constant, at the same processing rate. 
4.8.2 Undersampling 
Let {x(n)} be a sequence with X(z) as its z-transform. Consider the sequence 
{y( n) = x( 4n)} obtained by keeping only one out of every 4 samples of the 
sequence {x( n)}. The operation that takes us from {x( n)} to {y( n)} is called 
a factor M = 4 decimation. 

172 Digital Signal and Image Processing using MATLAB® 
By using the identity 2.34, the expression of the z-transform of {y(n)} is: 
+00 
+00 
( 1 3 
. 
) 
n~oo 
x(4n)z-n = p~oo 
x(p) 
4" {; e2J7rkp/4 
Z-p/4 
~ t, CI=oo x(p) (zl/4e-2j7rk/4) -p) 
3 
~ L X z (zl/4e-2j7rk/4) 
k=O 
Notice that you must not write p = 4n and then p E Z. The resulting 
inexact expression would be Yz(z) = X z(Zl/4). By using the DTFT, that is to 
say by choosing z = e2j7rj and by recalling the notation X(f) = Xz(e2j7rj), we 
get, for M = 4: 
3 
Y(f) = ~ LX (f ~ k) 
k=O 
( 4.41) 
This expression shows that inside the interval (-1/2,1/2), Y(f) is the alge-
braic sum of the four contributions, shifted by 1/4. To obtain the signals of the 
continuous-time signals (frequencies expressed in Hz), f has to be multiplied 
by the sampling frequency as we have already said. The y(n) are the signal 
samples taken at the frequency Fs /4, hence the expression of Y(f) shows a 
spectrum aliasing effect. Notice, by the way, the difference with the expansion 
operation that creates "images" in the signal's spectrum. 
___ ---r--~u(j) 
: obtained result 
; expected res,ult 
f 
~----r---r---r-r-+-+-T---~--~---r---r--~ 
f 
1/8 
1/2 
Figure 4.25 - Effects of undersampling 
However, according to the sampling theorem, the under sampled signal's 
spectrum is the one referred to as X 2(f) in Figure 4.25. Therefore, to under-
sample the signal x(n), a gain 1 filtering (see p. 84 with the ratio F~/ Fs = l/M 

Chapter 4 - Linear Filters 173 
already included in 4.41) has to be performed in the (-1/8, + 1/8) band before 
the decimation operation, so as to avoid spectrum aliasing. 
These results can be generalized. If y( n) refers to the sequence obtained by 
taking one out of every M values of the sequence x(n), the expression of its 
z-transform is: 
( 4.42) 
In order to undersample a signal x(n) by a factor M, one possible method 
is to perform a gain 1 filtering in the (- 1/(2M),+1/(2M)) band, fol-
lowed by a decimation operation of lout of every M values. 
Exercise 4.14 (Undersampling) (see p. 435) 
1. Write a function for undersampling by a factor M; 
2. record a speech signal at 8,000 Hz: 
- create a new signal by taking one out of every 2 samples without 
any particular processing. Listen to the result; 
- perform a "proper" under sampling by using the previous function 
with M = 2. Listen to the result. 
Figure 4.26 sums up the M factor oversampling and undersampling oper-
ations. It should be noted that all of these operations, including the filter-
ing, are performed in discrete-time, and on no occasion did we change to the 
continuous-time signal! 
a) oversampling 
--ffi---- ~ M+ r--
. 
0 -I/WM' 
expansIon 
L...-______ -' 
b) undersampling 
en M;h 
de 
'" 
co 
~I 
0-
~ ..s 
-I/;M--~M' 
cimation 
[ill---
Figure 4.26 - Oversampling and undersampling operations 
Exercise 4.15 (Paralleled undersampling and oversampling) (see 
p. 
436) 
The filtering operation necessary to the under sampling can be performed 

174 Digital Signal and Image Processing using MATLAB® 
M times faster by M parallel filters. The output at the time nM has the 
expression: 
y(nM) 
+00 
L h(k)x(nM - k) 
k= -oo 
M-l +00 
L L h(mM + r)x((n - m)M - r) 
r = O m = -oo 
y(nM) appears as the sum of M filterings with the impulse responses 
{hr(m) = h(mM + r)}mEZ. The filter input is the sequence . .. x(r - M), 
x(r), x(r + M), x(r + 2M) ... , obtained from x(n) by a delay of r followed by a 
decimation. Notice that hr is called "the r-th M-polyphase component of h": 
1. give the processing architecture; 
2. write a paralleled under sampler simulation program; 
3. write a paralleled oversampler simulation program. 

Chapter 5 
An Introduction to Image 
Processing 
This chapter provides the reader with a few elements on image processing 
with MATLAB@, which comes equipped with 2D (two dimension) functions, 
necessary when working in this field, a field not too different from 1D signal 
processing. 
This chapter is merely an introduction. The reader can benefit from read-
ing [12], a rather extensive overview of what is done with images, both still 
and animated. Some important problems, related to sampling, rectangular, 
hexagonal or of another kind, to perception, to content aspects in terms of 
objects, etc., will not be discussed here. The only thing we will be dealing with 
is handling two dimension arrays. We will also explain how to program some 
of the functions contained in the "image" toolbox. 
Examples in this chapter are illustrated by figures that cannot perfectly ren-
der the phenomena we are trying to underline. The printing process, whether 
monochrome or not, adds its own imperfections (quantization, weaving, num-
ber of colors, color transcription, etc.) when rendering images. In fact , every 
part of the digital processing chain, from the data recording device to the 
printer, has a role that will not be covered in this book. 
5.1 
Introduction 
5.1.1 Image display, color palette 
From now on, an image will be considered as a set of pixels (the contraction 
of picture element), associated with a rectangular grid of the original image 
(Figure 5.1). 
In MATLAB@, there are several ways to display an image: 

176 Digital Signal and Image Processing using MATLAB® 
Figure 5.1 - Each point of the original image has an 8-bit coded "gray-level': Each 
pixel appears as a gray square 
- either directly with an (N x M x 3) or (N x M x 4) array depending on 
the color model: RGB (Red, Green and Blue), CMYK (Cyan, Magenta, 
Yellow and blacK) , HSL (Hue, Saturation and Lightness), CIE Lab 
("Commission Internationale de l'Eclairage": L is for luminance, and a 
and b are color component coordinates), etc. 
In the following example, an image in JPEG format is imported with the 
use of the imread function as a 3 dimension 800 x 580 x 3 array, the 3 
indicating that there are three RGB color planes. Notice that the data 
type used is the 8-bit unsigned integer: 
» xx=imread('elido72.jpg', 'jpeg'); 
» whos 
Name 
Size 
ans 
1x94 
xx 
800x580x3 
Bytes Class 
188 
char array 
1392000 uint8 array 
- either by using a 2D (short for 2 dimension) array and a color palette. 
This is the display mode we will be using; it is called an indexed repre-
sentation. 
Let A = [a(i,j)], with 1 :::; i :::; Nand 1 :::; j :::; M, be an N x M array. The 
number a( i, j), placed in line i and column j , indicates the color of the point 
with coordinates (i, j) in the image after it has been "sampled" and "quantified". 
The line index i represents the horizontal position, and the column index j 
represents the vertical position. The point with the coordinates (1,1) is placed 
in the top-left corner (see Figure 5.2). 
EXAMPLE 5.1 (Pixelizing an image) Type: 
II 
» 
imagel=[32 0 48;0 16 0]; 
» image(imagel); colormap('gray') 

Chapter 5 - An Introduction to Image Processing 177 
The image displayed is comprised of 6 points, or logical pixels, and the one 
associated with image 1 C1, 1) is the one in the top-left corner (Figure 5.2). 
Figure 5.2 - Six logical pixels: notice the integer x- and y-coordinate corresponding 
to the "center" of each pixel 
Notice that an element of the array with the index (i,j) can be associated 
with several physical pixels of the display window. In fact, there is no reason for 
the number of values of the matrix of elements a( i, j) to be equal to the number 
of physical pixels of the display window. Hence, a point with the coordinates 
(i, j) can be represented by several physical pixels, just as a physical pixel can 
be used to represent several points with the coordinates (i, j). From now on, 
when we use the word pixel, we mean a logical pixel, that is to say elements 
identified by the pair (i, j). 
If we want to display a figure and preserve its real size (one screen 
pixel corresponding to one image pixel), we will be using the properties 
units, Position, AspeetRatio, etc. (these parameters can change from one 
MATLAB® version to the next). In example 5.1, a real-size display is achieved 
by typing: 
II» set(gca, 'units', 'pixels', 'Pos', [20 20 fliplr(size(imagel»]) 
where the property Pos, or Position, is given in the form [left bottom width 
height]. The reading of relevant phase plot properties (called CurrentAxes) 
can be done using get (gea) or et (gef , 'CurrentAxes ' ) . 
In the indexed representation, a( i, j) indexes a color array called the palette 
(Figure 5.3). The color palette is a (P x 3) array where each line is used to 
code a color according to its Red, Green and Blue components (RGB) using a 
real number between 0 and l. 
This representation is convenient since most bitmap editing programs can 
provide an image description in three planes, each one of them corresponding 
to a primary color R, G or B, encoded as an integer between 0 and 2n -
1 
(n-bit encoding) (see section 5.2). The images we will be considering will be 
"in levels of gray". MATLAB® has a "gray" palette that can be activated using 
the eolormap ( , gray') instruction. 
Type eolormap(' gray') then eolormap. You get a (64 x 3) array with 
three identical columns of values between 0 and 1: 

178 Digital Signal and Image Processing using MATLAB® 
A =[a(iJ)] 
~L----r---r-~L--
m 
Line i --r--------r'--7-----7'<:-
Palette 
Pll Pl2 PI3 
PZl 
P22 P23 
Color of 
the pixel 
(ixj) 
Figure 5.3 - Connection between the image array and the palette 
ans 
0 
0 
0 
0.0159 
0.0159 
0.0159 
0.0317 
0.0317 
0.0317 
0.0476 
0.0476 
0.0476 
0.9841 
0.9841 
0.9841 
1.0000 
1.0000 
1.0000 
COMMENTS: 
- in example 5.1 the zero values of the image1 array are redefined as 1 and 
therefore index the color (0,0,0), which is black (Figure 5.2); 
- help for the commands image, imagesc and colormap should particularly 
be looked into; 
- the "gray" palette is constructed linearly. Each column is of the type 
[0: 1/63: 1] I (1/63 :::: 0.0159). This does not quite correspond to the 
perception we have of brightness. The visual response is roughly pro-
portional to the logarithm of the intensity (Fechner- Weber law), hence 
the progression of the levels of gray should correspond to this law. In 
practice, the palette's linear conformation makes our work much easier 
since palette index lines and gray levels are related by an affine relation; 
- other palettes come standard in the basic version of MATLAB® to make 
the user's work easier. Use the help color command to learn more 
about them. Also, nothing stops you from defining your own palettes. 
For example, to get a display with 256 levels of gray, all you need to do 
is create a cmap array as follows: 
II 
cmap= [0: 255] I *ones (1,3) /255; 
colormap(cmap); 

Chapter 5 - An Introduction to Image Processing 179 
5.1.2 Importing images 
If you don't have an image you can perform tests on in MATLAB®, you can 
always create one based on raw format images (no header) using image pro-
cessing software. At the same time, you can save the palette, if that is possible. 
The following function allows you to read and/or create a file that can be used 
directly by MATLAB®. The image that was chosen is an image universally 
used by "image processors" to compare results obtained for different imple-
mentations. It is referred to as lena. We assume that the data is stored as 
unsigned 8-bit coded integers. 
Figure 5.4 - Test image 
function pixc=raw2matf(fnameI , Nrow , Ncol,Tr ,Fc , fnameO) 
%!==========================================================! 
%! Reading a raw image file 
%! SYNOPSIS: pixc=RAW2MATF(fnameI,Nrow,Ncol ,Tr,Fc,fnameO) 
%! 
fnameI 
raw file ([ .raw]) 
%! 
Nrow ,Ncol 
%! 
Tr 
%! 
Fc 
%! 
fnameO 
image dimensions 
when 'T' : transposing the image 
when 'F': creating the file fnameO (.mat) 
resulting file ([.mat]) 
%!==========================================================! 
if nargin<6 , fnameO='fictrav. mat' ; end 
if nargin<5 , Fc='N'; end 
if nargin<4, Tr='N'; end 
%===== raw image 
nFS=findstr(fnameI,'. '); 
if isempty(nFS), 
NFE=[fnameI , '.raw'] ; 

180 
Digital Signal and Image Processing using MATLAB® 
else 
NFE=fnameI; fnameI=fnameI(1:nFS-1); 
end 
fid=fopen(NFE, 'r'); [pixc,Npix]=fread(fid, 'uchar'); 
if (Npix -= Nrow*Ncol) 
sprintf('Dimension error: %d*%d -= %d' ,Nrow,Ncol,Npix) 
return 
end 
pixc=reshape(pixc,Nrow,Ncol); if Tr=='T', pixc=pixc'; end 
fclose(fid); 
%===== creating the .MAT file 
if Fc=='F', 
sprintf ('Creating the file %s' ,fnameO) 
eval(['save ' fnameO ' pixc']) 
end 
return 
The image can be loaded and displayed (Figure 5.4) by the following pro-
gram: 
%===== tstraw2mat.m 
pixc=raw2matf('lena50' ,256,256,'T'); 
%===== palette construction 
cmap=([255:-1:0] '/255)*[1 1 1]; 
%===== displaying with the new palette 
imagesc(pixc); colormap(cmap); axis('image') 
In this program, the palette is defined, but it can also be saved in the image 
processing application and stored in the .mat file. 
COMMENTS: 
recent versions of MATLAB® allow you to directly load and save images 
in formats such as "bmp" (bit map), "tiff" (Tagged Image File Format), 
"jpeg" (Joint Photographic Expert Group), "pcx" (Personal Computer 
Exchange) , etc. using the imread and imwri te functions; 
notice that when a palette is used, the image function works with an 
array of integer values (the non-integer values are rounded) between 1 
and M. The values above M are constrained to M, and those below 1 
are constrained to 1. Type at the end of the previous program: 
p256=pixc+256; subplot(121); image(p256); axis('image') 
pO=pixc-256; subplot(122); image(pO); axis('image') 
colormap(cmap) 
You should see a white square and a black square; 
it is usually preferable to use the imagesc function (suffix sc as in scale) 
which displays a version with the same scale as the original image: the 
values are changed to fit between 1 and size (colormap, 1) ; 

Chapter 5 - An Introduction to Image Processing 181 
the image's color levels can have values such that it becomes difficult to 
display the image because of a few extreme values. The use of image or 
imagesc may not be satisfactory. The following function allows you to 
improve the display by modifying the color distribution: 
function mydisp(pixr,cmap,stdpar,style) 
%!==============================================! 
%! Displaying with gray level control 
%! SYNOPSIS : MYDISP(pixr, cmap, stdpar, style) 
%! 
pixr 
image 
%! 
cmap 
palette 
%! 
stdpar 
controls the min and max indices 
%! 
style 
see AXIS function 
%!==============================================! 
if nargin<2, 
sprintf('Error on arguments'); 
return 
end 
if nargin<4, style='image'; end 
if nargin<3, stdpar=3; end 
if (stdpar <= 0 I stdpar >10), stdpar=l; end 
moy=mean(mean(pixr)); stdp=stdpar*std(std(pixr)); 
minp=moy-stdp; maxp=moy+stdp ; 
idx=l+(pixr-minp)*(size(cmap,l)-l)/(maxp-minp); 
colormap(cmap); image(idx); axis(style) 
return 
when using scanners or digital cameras, the standard sampling values, 
in "dots per inch" (dpi), are (300 x 600)1, (600 x 1,200),(1,600 x 3,200), 
(2,700 x 2,700) , etc., and for quantification, 8, 10, 12, etc. bits for each 
of the primary colors. 
5.1.3 Arithmetical and logical operations 
Because images in MATLAB® are matrices, the usual operations can be directly 
applied to them. In particular, arithmetic and logical operations between im-
ages, pixel by pixel, can be performed from the array values they are associated 
with. 
Thus, the sum of two images pix1 and pix2 of the same size can be written 
pix1 + pix2, or just as the square root of pix can be written sqrt (pix) . You 
only have to make sure that the obtained values are consistent with the color 
palette, or you can use the functions imagesc or mydisp. 
The logical operations have to be performed sur des "representations en-
tieres". for "whole representations". The codes associated with pixels, indices or 
1 Meaning 300 dots per inch in one direction, and 600 dots per inch in the other. 

182 Digital Signal and Image Processing using MATLAB® 
values must be first converted to this format using functions unint8, unint16, 
etc. It must be ensured that the ranges are compatible with this representa-
tion. Here is an example: consider the two images in Figure 5.5 - we are going 
to perform the AND function between the figure on the left and the figure on 
the right. 
Figure 5.5 - Logical operation AND 
The result is shown in Figure 5.6: the black areas of the image on the right 
in Figure 5.5, which are encoded as byte 00000000, force the corresponding 
areas of the resulting image to be black. This is because if xxxx xxxx is the 
value associated with a pixel from the first image, the logical AND of xxxx xxxx 
and 0000 0000 is 0000 0000. The white areas of the image on the right are 
encoded as byte 11111111, leaving untouched the values of the corresponding 
pixels of Lena. This is because the logical AND of xxxx xxxx with 1111 1111 
is xxx x xxxx. Finally, the areas of the image on the right which are encoded 
as yyyy yyyy lead to a pixel value with some bits unchanged, and others set 
to o. 
Figure 5.6 - Result of the logical AND 
The ANDlog function performs the logical AND operation we have just de-
scribed: 

Chapter 5 - An Introduction to Image Processing 183 
function pixr=ANDlog2(pix1,pix2) 
%!===============================================! 
%! Logical AND between two images 
%! SYNOPSIS: pixr=ANDLOG(pix1,pix2) (UINT8) 
%! 
pix1 
first image (gray palette) 
%! 
pix2 = second image (gray palette) (UINT8) 
%! 
pixr = image result (UINT8) 
%!===============================================! 
if (nargin<2), error('Parameters missing'); end 
N1=size(pix1) ; 
if (N1 -= size(pix2)), 
error('Matrix dimensions are not appropriate') 
end 
pixr=bitand(pix1,pix2); 
return 
The program testlogic. m which uses the ANDlog fonction, leads to Fig-
ure 5.6: 
%===== testlogic.m 
clear all 
load lena25; 
subplot(131); imagesc(pixc); 
colormap(cmap); axis('image'); 
load testlog1; 
subplot(132); imagesc(pixtl); 
axis (' image' ) 
%===== logical operation 
% loading and displaying 
% the first image 
% loading and displaying 
% the second image 
pixr = ANDlog(uint8(pixc),uint8(pixtl)); 
subplot(133); imagesc(pixr); axis('image'); 
Exercise 5.1 (Logical functions) (see p. 437) 
1. Write a function that uses the four basic logical operators AND, OR, 
EOR and NOT, as well as the comparison operators. Use the bitand, 
bitor, bitxor, bitcmp functions to implement it. It is assumed that the 
images in grayscale are indexed with an index between 0 and 255. The 
index is then converted to UINT8 "8-bit unsigned integer" format before 
processing. 
2. Write a test program for the logical operator NOT, as well as for the 
logical operator that is true when am :::; bm , where am and bm are the 
bits corresponding to two bytes we wish to compare. 
5.2 
Color spaces 
There is considerable literature - [25], [24], [14], etc. - on aspects related to 
representations of color images. To simply specify a color space (indexed-color, 

184 Digital Signal and Image Processing using MATLAB® 
RGB, CMYK, Lab color, etc.) in image processing software it is necessary to 
have a few ideas of these spaces [28] [11] . 
The perception of color is, in all accounts, a complex process. When asked 
to describe the color of a light source, the answers involve three parameters: 
hue, for example "it pulls on the yellow with a hint of green", brightness of the 
source, which is relative to the ambient brightness and saturation, for example 
"this is a deep red". 
The creation of a color representation system, in regards to color space, is 
built on these types of parameters. It is influenced by the following findings: 
- The eye is made up of photoreceptors: the three types of cones, involved 
in the perception of color and rods, mainly sensitive to intensity. It is 
said that humans have a tri-color perception. Each cone is characterized 
by a sensitivity function called spectral power distribution (SPD) function 
of the wavelength .\. The three SPDs are denoted by L('\) (long) , M('\) 
(medium) and S(.\) (short) (Figure 5.7). 
80 
60 
40 
20 
450 
500 
550 
600 
650 
700 (A) 
Figure 5.7- The normalized sensitivity functions (RLab-D65) 
In fact, these sensitivity functions are not the same from one individual 
to another and are only one element among many that are involved in vi-
sion. The perception of color depends indeed on many elements, ambient 
brightness, contrast, phenomena of vision adaptation, etc. 
- In 1931, the CIE (Compagnie Internationale de l'Eclairage) standardized 
three weighting functions for modeling, by the application of the SPD of 
a light source, the process of color perception in a human being. 
These three functions are designated by the Color Matching Functions 
(CMF) and denoted by x, y and z (Figure 5.8 obtained using the 
chromaticity.m) program. 

Chapter 5 - An Introduction to Image Processing 185 
%===== chromaticity.m 
% A Guided Tour of Color Space, C. Poynton 
% (A Technical Introduction to Digital Video 
% Wiley & Sons, New York, 1996) 
vallambda=false; 
[xb,yb,zb,lambda]=ciecmf(); 
N=length(xb); 
%===== CIE color matching functions 
figure(l); plot (lambda, [xb' ,yb' ,zb']), grid 
%===== (x,y) chromaticity diagram 
tsm=[xb;yb;zb]; stsm=sum(tsm); 
x=xb ./ stsm; y=yb ./ stsm; 
figure(2), plot([x x(l)],[y y(l)], '-r'), grid on 
hold on, plot(x,y, '0') 
%===== D65 CIE illuminant 
xD65=.3128; yD65=.3290; plot(xD65,yD65, 'x'); 
text(xD65+.01,yD65, 'D65') 
if vallambda 
for k=l:N, text(x(k)+.01,y(k),num2str(lambda(k»), end 
end 
hold off 
function [xb,yb,zb,lambda]=ciecmf() 
xb=[.0143 .0435 .1344 .2839 .3483 .3362 .2908 .1954 .. . 
. 0956 .0320 .0049 .0093 .0633 .1655 .2904 .4334 .. . 
. 5945 .7621 .9163 1.0263 1.0622 1.0026 .8544 .6424 ... 
. 4479 .2835 .1649 .0874 .0468 .0227 .0114]; 
yb=[.0004 .0012 .0040 .0116 .0230 .0380 .0600 .0910 
.1390 .2080 .3230 .5030 .7100 .8620 .9540 .9950 
.9950 .9520 .8700 .7570 .6310 .5030 .3810 .2650 
.1750 .1070 .0610 .0320 .0170 .0082 .0041]; 
zb=[.0679 .2074 .6456 1.3856 1.7471 1.7721 1.6692 ... 
1.2876 .8130 .4652 .2720 .1582 .0782 .0422 .0203 .. . 
. 0087 .0039 .0021 .0017 .0011 .0008 .0003 .0002 .. . 
o 0 0 0 0 0 0 0]; 
lambda=[400:10:700]; 
return 
Knowledge of the CMF allows a system of color representation to be built; 
- the representation system defined by the CIE uses three components, X, 
Y and Z , where Y represents the sensitivity of humans to brightness. 
If p(>.) represents the SPD of a light source based on the wavelength >., 
its brightness is equal to J p(>.)y(>.)d>.. With X = J p(>.)x(>')d>' and 
Z = J p(>')z(>.)d>., triplet (X, Y, Z) is called "XYZ-tristimulus". 
The luminosity is usually set relative to a "reference white" by a number 
between 1 and 100 instead of a candelas per square meter. 

186 Digital Signal and Image Processing using MATLAB® 
l.6 
1.2 
0.8 
0.4 
0 
400 
450 
500 
550 
600 
650 
700 (nm) 
(Wavelength) 
Figure 5.8 - ClE color matching functions 
The function giving X , Y and Z from L, M and S is linear. 
Many color systems are built from components X, Y and Z; 
- in addition to brightness, the CIE defines the two other components of 
chromaticity by: 
X 
Y 
x = X + Y + Z and y = X + Y + Z 
(5.1) 
A color can therefore be described by the triplet (Y, x, y). It is denoted by 
CIE- Yxy model. There are several similar representations derived from 
the latter: CIE- Yuv, CIE- Yu 'v' (1960), CIE-Y*u*v* (1976), CIE-Y*a*b* 
(1931), all of which aim to standardize the perception when considering 
the differences between points located at equal distances in the diagram. 
The 
chromaticity 
diagram 
(Figure 
5.9) 
is 
obtained 
by 
the 
chromat i ci t y. m program. 
The linear combination aXYZ1 + /3XYZ2 of two XYZ-tristimuli gives a 
linear relationship between x and y (straight line 6.a f3 of Figure 5.9): 
- a simple way to build a color is to leave the so-called three "primary" 
colors and to perform a linear combination of the SPD. Color additive 
synthesis, is often spoken about. The RGB (Red, Green, Blue) system is 
widely used in image reproduction systems, video projectors for example 
with three beams R, G and B, or to encode color images. The reproduc-
tion of an image on a screen or computer monitor requires a reference for 
the "white" to be defined. The CIE has done this digitally by defining 
"whites" according to the lighting conditions within their chromaticity 
(for example the D65 point in Figure 5.9). White can also be defined 

Chapter 5 - An Introduction to Image Processing 187 
(y) 0.9 ,---.....-----,------,---...-----r----,------,-------, 
520 : 
0.8 ---
---!...---
__ .!. _______ 1 _______ -' ________ ' ________ ' ________ , _______ _ 
: 
'
540: (wavel
~ ngth)' 
, 
: 
I 
I 
I 
I 
------1--------.1.------
_______ -1 _______ -1 ________ 
1 ________ 1- ______ _ 
I 
I 
I 
I 
0.7 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
------r-------T--------r-------
--------,-------- I--------r--------
0.6 
I 
I 
I 
I 
I 
: 
: 
: :  
: ~ Q(3 
I 
I 
I 
I 
------r--- - ---T--- - ---,--- - ---,------- ---------------
-------
0.5 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
______ L _______ ~ _______ J _______ J _______ ~
____ 
__ 
_ ______ L ______ _ 
I 
I 
I 
I 
I 
I 
0.4 
I 
I 
I 
I 
" 
, 
" 
, 
0.3 
" 
, 
0.2 
0.1 
, 
, 
, 
-,--------,--------,--------,--------
, 
, 
, 
, 
, 
, 
o ~--
~--~~----~--~
' ----~--~----~
' --~ 
o 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 (x) 
Figure 5.9 - (x,y) chromaticity diagram. The "ClE D65 illuminant" corresponds 
to a dayligth reference for the western and northern europe. 
from the light emission of a black body worn at a given temperature 
(expressed in degrees Kelvin). A system of encoding RGB is therefore 
specified by the chromaticity of its three primary colors and the reference 
white. 
What follows is three of the most commonly used systems: the RGB (Red, 
Green, Blue) , system the HSV system (Hue, Saturation, Value) , also known as 
HSB (Hue, Saturation, Brightness) , and the CMYK (Cyan, Magenta, Yellow, 
and K for Black) system. MATLAB® uses RGB to specify the colors of the 
objects graphics or images, indexed or not. 
5.2.1 
RGB coding 
The RGB system allows color to be defined by mixing the three primary colors, 
red, green and blue. The values of the three components are often coded on 
8 bits between 0 and 255. So the triplet (255, 255, 255) codes white, (0, 0, 0) 
black, (255, 0, 0) a pure red, (100, 100, 100) a gray, etc. 
In MATLAB®, each component can be coded with "floating-point" number 
between 0 and 1 or with a 8 or 16 bit integers. 

188 Digital Signal and Image Processing using MATLAB® 
EXAMPLE 5.2 (Coding a color with 8 bit numbers) Taper: 
» myimage=zeros(1,8,3,'uint8'); 
» myimage(:, :,1)=[255 255 255 255 0 0 0 0]; % red plane 
» myimage(:, :,2)=[0 0 255 255 255 255 0 0]; % green plane 
» myimage(:, :,3)=[0 255 0 255 0 255 0 255]; % blue plane 
» image(myimage), axis('image') 
Obviously the results obtained depend on the characteristics of the visual-
ization system. 
RGB coding is implemented in many digital devices: input devices - color 
scanners, digital photo equipment, camcorders, etc. - or output devices -
screens, printers, etc. However, for image processing, it is often more convenient 
to use a HSV representation. 
5.2.2 HSV coding 
The HSV model was introduced in 1978 by A. R. Smith [32] . It is a nonlinear 
transformation of the RGB color space. It is better than the RGB system 
to specify a tint. In fact, there are three representation systems best suited 
to the human perception of color: HSL (Hue, Saturation and Lightness), HSI 
(Hue, Saturation and Intensity) and HSV. Only the last representation will be 
addressed. HSV defines a color with three components: 
1. the tint, or hue, H describes the perceived color, such as red, blue, or 
yellow, and uses values from 0 to 360. Thus, 0 is red, 45 is a shade of 
orange and 55 is a shade of yellow; 
2. the saturation S, or the color intensity, ranges from 0 to 100%. 0 indicates 
"no color" and 100 indicates an intense color; 
3. the value V, or color brightness, ranges from 0 to 100%. 0 is always black. 
According to saturation, 100 can be white or can mean more or less color 
saturation. 
The MATLAB® functions hsv2rgb and rgb2hsv are used to perform con-
versions between color spaces. The testHSV. m program gives all the colors 
obtained with saturation and brightness equal to 1. Type: 
%===== testHSV.m 
h=(0:2:360)'; nbr=length(h); HSVcm=zeros(nbr,3); 
HSVcm(: ,1)=h/360; 
% H 
HSVcm(:,2)=ones(nbr,1)*1; % S 
HSVcm(:,3)=ones(nbr,1)*1; % V 
imr=20; myimage=zeros(imr,nbr,3); 
RGBcm=hsv2rgb(HSVcm); 
myimage(:,: ,1)=ones(imr,1) * RGBcm(:,l)'; % red plane 

Chapter 5 - An Introduction to Image Processing 189 
myimage(:,: ,2)=ones(imr,1) * RGBcm(:,2)'; % green plane 
myimage(:, : ,3)=ones(imr,1) * RGBcm(:,3)'; % blue plane 
image(myimage), axis('image') 
Other systems such as CIELAB [20, 19] (CIE L*a*b*, 1976 and CIE 15.2 
publication, 1986) or CIECAM02 [13] (used in Microsoft Windows Vista) are 
better suited to the separation of the primary colors. 
5.2.3 CMYK coding 
The CMYK coding is widely used when printing images to reproduce a wide 
color spectrum from the three base colors (cyan, magenta and yellow) to which 
black is added. The CMYK model works by partially or entirely hiding certain 
colors on a typically white background. Cyan, magenta, and yellow are the 
three primary colors in Subtractive synthesis, contrary to the red, green and 
blue (RGB) which use additive synthesis. 
Black is used to obtain grays, which would be more difficult to obtain by 
mixing the three primary colors. They may, however, be added as an extra 
color to black to accentuate the shades of gray (for example, a brown or orange 
ink). This method is used, in particular, with black and white photography. It 
is also possible to add a color (usually of cyan) in order to print a more intense 
black. 
Conversion RG B to CMYK 
Color values are meant to be real numbers between 0 and 1. The proposed 
conversion works from a RGB model to a CMYK model. In this case the CMYK 
color uses the most black (K) possible with the least amount of color (CMY) 
possible. Thus, the #808080 color (gray) will be converted to (0,0,0,0.5) and 
not (0.5,0.5,0.5,0). 
The RGB to CMY conversion is described by: 
(5.2) 
The CMY to CMYK conversion is described by: 
K = min{C', M' , Y'} 
(5.3) 
if K = 1 then [C, M, Y, K] = [0,0,0,1] 
e~ lrl rfj1 
(5.4) 

190 Digital Signal and Image Processing using MATLAB® 
function CMYK = rgb2cmykm(rgb) 
%!========================================! 
%! SYNOPSIS: CMYK = rgb2cmyk(rgb) 
%! rgb 
: (1,3)-matrix -> (R,G,B) 
%! CMYK 
: (1,4)-matrix -> (C,M,Y,K) 
%!========================================! 
cmy=[1-rgb(1),1-rgb(2),1-rgb(3)] ; 
k=min(cmy); 
if(k==l), CMYK=[O,O,O,l]; return; end 
CMYK=[(cmy(1)-k)/(1-k),(cmy(2)-k)/(1-k),(cmy(3)-k)/(1-k),k]; 
return 
Conversion of CMYK to RG B 
The CMY is used as an intermediate coding to perform the conversion between 
RGB and CMKY. It then converts the CMY value to RGB. 
[~:] 
[~] 
[
C(I- K) + K] 
M(I-K)+K 
Y(I- K) + K 
[
1- CIt] [(1- C)(I- K)] 
I-M 
= 
(I-M)(I-K) 
l-Y' 
(I-Y)(I-K) 
function RGB = cmyk2rgbm(cmyk) 
%!=========================================! 
%! SYNOPSIS: RGB = cmyk2rvb(CMYK) 
%! CMYK 
: (1,4)-matrix -> (C,M,Y,K) 
%! RGB 
: (1,3)-matrix -> (R,V,B) 
%!=========================================! 
cmy=[cmyk(1)*(1-cmyk(4))+cmyk(4), .. . 
cmyk(2)*(1-cmyk(4))+cmyk(4), .. . 
cmyk(3)*(1-cmyk(4))+cmyk(4)] ; 
RGB=[1-cmy(1),1-cmy(2),1-cmy(3)] ; 
return 
N OTE: the two rgb2cmykm and cmyk2rgbm conversion functions are an un-
satisfactory practice. The basic relationship "1-RGB" is indeed only approx-
imative. It originates from the fact that, for example, blue is achieved by 
simply removing red, green, then yellow. If so, all the yellow from the white 
source should be filtered, then blue should be obtained. In reality, the three 
filters, magenta, cyan and yellow absorb light across the spectrum. There are 
interactions between the three when filtering. 
The otained color space (gamut) remains less "extended" than that given 
by the RGB model, the transformation is not linear, and the manipulation of 
colors is even less intuitive than the RGB model. In practice, these conversions 
are based on tables or polynomial transformations. 

Chapter 5 - An Introduction to Image Processing 191 
5.2.4 How to extract the RGB information from an image 
The RGB information of a digital image can be extracted in two ways, depend-
ing on whether the image is indexed or not: 
so 
100 
150 
200 
250 
1. an indexed image (n x m), as previously shown, is an array of indexes 
"pointing-out" to a color map (colormap function). The latter is, in 
MATLAB@, a (64 x 3) matrix (default size). This allows each of the 
n x m triplets to be revealed in the table to therefore retrieve the RGB 
components; 
2. non-indexed images (n x m) ("structured images") are composed of three 
(n x m) arrays giving the values of the red, green and blue components. 
The following program retrieves the three R, G and B arrays: 
%===== draWlIlo.m 
myimg=imread('morisse.jpg', 'JPEG'); 
size (myimg) 
nbcl=64; mcol=[O:nbcl-1] '/(nbcl-1); 
mypal=zeros(nbcl,3,3); mypal(:,1,1)=mcol; 
mypal(:,2,2)=mcol; mypal(: ,3,3)=mcol; 
for k=1:3 
end 
figure(k), imagesc(myimg(:,:,k)) 
colormap(mypal(:,:,k)); axis('image') 
50 
100 
150 
200 
250 
SO 
100 
150 
200 
250 
50 
100 
150 
200 
250 
Figure 5.10 - The R, G and B components 
5.2.5 
Converting from color to grayscale 
Color to grayscale conversion is based on the perception of luminance. The 
C.I.E proposes two formulas to characterize this information: 
1. in its 709 recommendation, concerning "real" or natural colors: 
Y = gray = 0.2125 x red + 0.7154 x green + 0.0721 x blue 

192 Digital Signal and Image Processing using MATLAB® 
2. in its 601 recommendation for colors with gamma correction (for example, 
images seen on a video screen): 
Y = gray = 0.299 x red + 0.587 x green + 0.114 x blue 
The weights used to calculate luminosity are related to the monitor's phos-
phorus. These weights are explainable by the fact that for equal amounts of 
color, the eye is most sensitive to green, then red, then blue. This means that 
for equal "amounts" of green and blue light, the green will look much brighter 
than the blue. 
5.3 Geometric transformations of an image 
5.3.1 The typical transformations 
The simple geometric transformations, such as translations, rotations and tor-
sions are problematic because of the "integer" nature of the pixels' position in 
an image (Figure 5.11). 
o 
Column 
(m,n) 
y 
y 
00: : 
00: 
--r--l------------
---------
~ \ 
'F 
'\ ". :C" 
.. , 
"-
-+ -r-~
~ :- ~ ~~' -- : r -- - --- - - --- - -~- ~x
1=t=~~'~'~~'~'~==
+I'. ~x 
\ 
\:~ ':'" -'\ 
-----1 --
--,..--i------------
: : 
Line 
\,A, 
\\ \\ \, 
, 
' 
, 
' 
, 
' 
Figure 5.11 - Rotations of an image 
EXAMPLE 5.3 (Rotation of an image) 
We wish to rotate an image. To make things simpler, we will be using an 
indexed image in levels of gray: 
1. the rotation matrix has the expression: 
M = [cose 
sine 
- Sine] 
cose 

Chapter 5 - An Introduction to Image Processing 193 
2. we create an array for the pixel coordinates (change from the line and 
column numbers over to the x, y coordinates). Applying the rotation 
to every point provides, after rounding the resulting value, and changing 
back to the line, column representation, leads us to the final image: 
%===== geomtransf.m 
% Geometric transformations / rotation 
fmt='jpeg'; fn='imageGG.jpg'; pixc=imread(fn,fmt); 
%===== 
subplot(121), imagesc(pixc); Spix=size(pixc); 
Nr=Spix(l); Nc=Spix(2); 
mycmap=[0:1/255:1] '*[1 1 1]; 
%=== gray colormap 
colormap(mycmap); set(gca, 'DataAspectRatio', [1 1 1]) 
set(gca, 'units', 'pixels' ,'pos', [4040 fliplr(Spix)]) 
axis off, % turns off labels, marks ... 
%===== rotation center 
xor=(1+Nc)/2; yor=-(1+Nr)/2; 
%===== tbidx=indices (columnwise) 
tbx=ones(Nr,l)*[l:Nc]; tby=[l:Nr] '*ones(l,Nc); 
tbidx=[reshape(tby,l,Nr*Nc);reshape(tbx,l,Nr*Nc)] ; 
idtb=tbidx(1,:)+(tbidx(2,:)-1)*Nr; %=== linear indices 
%===== tbcoord=coordinates pixels/rotation center 
tbcoord=[tbidx(2,:)-xor ;-tbidx(1,:)-yor]; 
%===== rotation 
theta=25; thet=theta*pi/180; 
MRot=[cos(thet) -sin(thet);sin(thet) cos(thet)]; 
tbv=round(MRot*tbcoord); 
xmin=min(tbv(l, :)); xmax=max(tbv(l,:)); ncol=xmax-xmin+1; 
ymin=min(tbv(2, :)); ymax=max(tbv(2,:)); nlig=ymax-ymin+1; 
%===== index reconstruction 
tbidxR=[-tbv(2, : )-ymin+1;tbv(1,:)-xmin+1]; 
pixcR=zeros(nlig,ncol); pixcR2=pixcR-1; 
idtbR=tbidxR(l, :)+(tbidxR(2, :)-l)*nlig; 
pixcR(idtbR)=pixc(idtb); pixcR2(idtbR)=pixc(idtb); 
save pixcR2 pixcR2 thet mycmap Nr Nc; %=== 
%===== displaying the result 
subplot(122), imagesc(pixcR); colormap(mycmap) 
set(gca, 'DataAspectRatio', [1 1 1]); 
set(gca, 'units', 'pixels', ... 
'pos', [70+Spix(2) 40 fliplr(size(pixcR))]) 
axis off 
%===== saving the image for median filtering 
pxRmn=min(min(pixcR)); pxRmx=max(max(pixcR)); 
pixcRn=255*(pixcR-pxRmn)/(pxRmx-pxRmn)+1; 
imwrite(pixcRn,mycmap, 'imageGGR.bmp', 'bmp') 
Notice the use of the imread and imwrite functions, making it possible 
to read and save images in a given format, "jpeg" in this example. 

194 Digital Signal and Image Processing using MATLAB® 
Rotating two neighboring pixels can result, after rounding, in identical co-
ordinates. This leads us to the conclusion that there are "holes" in the target 
image. These are clearly visible in the image resulting from the rotation (Fig-
ure 5.12). We will see in exercise 5.11 how to deal with these isolated points. It 
is also possible to process the pixels with identical coordinates using a weighted 
mean of the source pixels. 
Figure 5.12 - Flaws due to the rotation 
Generally speaking, affine transformations are represented with expression 
5.5: 
(5.5) 
x and yare the coordinates of the source S, X and Y those of the target image 
T tx and ty define the translation applied to the image. 
Likewise, the word torsion (see exercise 5.3) is used when the relation be-
tween S and T is of the type 5.6: 
[~l 
[ ~;:; 1 m 
and X ~ ~,Y 
~ ~ 
(5.6) 
These two types of transformations pose a problem for interpola-
tion(paragraph 5.6.2) and/or under sampling (paragraph 5.6.1) which we will 
discuss later. 
There is no rule that says you have to use an xOy axis system instead of a 
"line, column" coordinate system (LC coordinates) . In the case of a rotation, 
it allows the transformation matrix to preserve its usual form. In general, the 
transformation matrix can be identified immediately in LC coordinates. 

Chapter 5 - An Introduction to Image Processing 195 
Exercise 5.2 (Plane transformation) (see p. 439) 
Describing a transformation can be done in an interactive way using a simple 
shape. Here we are going to use the triangle to define the affine transformation 
we will apply to the image: 
1. write a linear transformation function of an n x m pixel image, knowing 
that the (2 x 2) transformation matrix is described in an xOy system; 
2. write a program asking the user to define two triangles interactively, which 
then calculates the (3 x 3) affine transformation matrix used to go from 
one triangle to the other; 
3. apply the transformation to an image. 
Exercise 5.3 (Transformation of a rectangular selection) (see p. 441) 
Many image processing applications allow you to deform a rectangular-shaped 
selection by having an effect on each corner of the selection. Consider expression 
5.6 of the "torsion". For a corner with the coordinates Xk, Yk, the coordinates 
X k and Yk after modifications can be expressed: 
X k = Uk = aXk + bYk + tx 
Tk 
eXk + jYk + 1 
Yk = Vk = CXk + dYk + ty 
Tk 
eXk + jYk + 1 
{ 
Xk(exk + jYk + 1) = aXk + bYk + tx 
Yk(exk + jYk + 1) = CXk + dYk + ty 
(5.7) 
If applied to all four corners, these expressions make it possible to determine 
the eight coefficients of the transformation matrix: 
1. using 5.7, determine the linear system needed to find the transformation 
matrix; 
2. apply this transformation to an image by assuming that the rectangular 
selection is applied to the whole image. 
5.3.2 Image registration 
Many applications - biometrics, identification number recognition, handwriting 
recognition, etc. - require that an image be forced to fit a certain size before 
undergoing whatever processing is needed. A method called the Procrustes 
method is often used to perform this operation. 
The idea is to start with a simplifed model based on characteristics points. 
Thus, for a face, we can choose a model such as the one illustrated in Figure 

196 Digital Signal and Image Processing using MATLAB® 
5.13. In the case of a hand, you can either choose points on the outline of the 
hand, or points on the outline of each finger. For a license plate, the natural 
choice would be the four corners, etc. 
Figure 5.13 - Triangle-based model 
Once we have a reference model A (the pattern on the left in Figure 5.14), 
we can start searching for a transformation that drags the characteristic points 
of the figure B to be analyzed (the pattern on the right in Figure 5.14) over 
onto the points of the reference model, according to a criterion used to evaluate 
the distance between two sets of points. 
Figure 5.14 - Characteristic points and Delaunay triangulation: the pattern on the 
left serves as a reference, the set of points on the right corresponds to the figure we 
wish to align 
The fact that the two sets of points A and B must correspond exactly adds 
a difficulty. When the points are provided by the automatic image analysis, A 
and B do not necessarily have the same number of points, meaning that some 
manual corrections may turn out to be unavoidable. 
Let A E jRrxs and B E jRrxs be two r x s matrices. In our case, the matrix 

Chapter 5 - An Introduction to Image Processing 197 
size is (N,2) or (2, N), where N is the number of charateristic points. We are 
trying to determine the (r x r) matrix Q, solution to the problem, for which 
we define a constraint: 
(5.8) 
The matrices A and B are centered. If their size is (N,2): 
M=r~~ 
~~1 
X N 
YN 
they are centered by typing M - ones (N, 2) *mean(M). 
For a matrix M, the Frobenius norm is defined by IIMII} = Tr{MMT }. 
We have: 
IIA-QBII } 
Tr{AAT} - 2Tr{QBAT} + (T2Tr{BBT} 
Tr{AAT} - 2(TTr{PBAT } + (T2Tr{BBT} 
(5.9) 
where Q = (Tp where P is a unitary matrix. Hence, for a given (T, the mini-
mization problem amounts to the maximization problem of Tr{ P BAT } under 
the constraint p Tp = I s. 
The matrix BAT is an r x r square matrix. Its singular value decomposition 
can be written as follows: 
where U and Y are unitary. This leads us to: 
If we assume Z = y T PU , and because D is diagonal, we have: 
Tr{PBAT} = Tr{ZD} = L.i Zii d ii 
where the Zii and d ii are the diagonal terms of Z and D respectively. But, 
because Z is a unitary matrix, IZijl < 1 for i,j any pair. Indeed using ZT Z = I, 
for all j we have L.i IZij 12 = 1. This means that, for any matrix P : 
Tr{PBAT} ::; L.i dii 
The upper bound L.i dii , which is independent of P , can be reached if we 
let Z = y T PU = I , that is to say: 
P = VUT 
(5.10) 
which is unitary. Hence (5.10) is the solution we were looking for. To sum up, 
after starting with A and B E lR.rxs , we calculate, one after the other: 

198 Digital Signal and Image Processing using MATLAB® 
1. C = BAT; 
2. the singular value decomposition: C = UDVT ; 
3. P=VUT ; 
4. notice that minimizing 5.9 in regard to (J" leads to: 
(J" = Tr{PBAT} ~ Q = Tr{PBAT} P 
Tr{BBT } 
Tr{BBT } 
Notice that if A = B, Q = I . 
The counterpart to the problem posed by expression (5.8) is determining 
the r x r unitary matrix R , solution to the problem: 
(5.11) 
Of course, its solution can be inferred from the previous one if you notice 
that 5.11 is equivalent to: 
{ 
minR IIAT - R TBTIIF 
R TR= (32I s 
the solution of which is R = (3YWT where BT A = YD'WT . 
(5.12) 
Notice in the example above that if one of the dimensions is always equal 
to 2, for example: 
A = [Xl 
X 2 
YI 
Y2 
. . . X s ] and B = [Xl X2 
... 
Ys 
YI 
Y2 
.•. 
X s ] 
... Ys 
the algorithm's development is true for any r x s pair. In particular, the pixels 
of two images we wish compare can be directly used. 
5.4 Frequential content of an image 
Just as it was done for 1D discrete-time signals, we are going to define the 
Fourier transform X(v, /.1) of an image, referred to as the 2D-DTFT. 
Definition 5.1 (2D-DTFT) Let x(k, €) be a two index sequence. The 2D-
DTFT is the function of v and of /.1 defined by: 
+00 
+00 
X(/.1, v) = L L x(k, €) e - 271") (k"Hv) 
(5.13) 
k = -oo £= - 00 
Because of its definition, X (/.1, v) is periodic with period 1 for the two vari-
ables /.1 and v. 

Chapter 5 - An Introduction to Image Processing 199 
100 
200 
300 
400 
500 
600 
100 200 300 400 500 
100 
200 
300 
400 
500 
600 
600 
100 200 300 400 500 
0 
200 
400 
Figure 5.15 - Applying the multiplications on the right and on the left in the example 
of Figure 5.14· In the bottom-right, the size of the transformation matrix is (2 x 2). 
In the bottom-left, the size of the matrix is (N x N) where N is the number of points 
in the m esh 
In practice, the images processed have a finite size K x L and we have: 
K - 1 £ - 1 
X(j.L ,v) = L 
L
x(k, £)e- 27rj(kI-'H v) 
(5.14) 
k = O £=0 
In this case, X (j.L, v) poses no existence problems, since the values of x ( k, £) 
are bounded and the sequence is finite. 
The inverse formula leading to x( k, £) from X (j.L, v) is: 
1
1/2 1 1/2 
x (k,£) = 
X(j.L,v) e27rj (kI-'H v)dj.Ldv 
- 1/2 - 1/2 
(5.15) 
Property 5.1 (2D convolution) 2D convolution is the name of the oper-
ation that associates the two sequences x (k, £) and y(k, £) with the sequence 

200 Digital Signal and Image Processing using MATLAB® 
z(k, e): 
+00 
00 
z(k,e) = (x*y)(k,e) = L L x(i,j)y(k-i,e-j) 
(5.16) 
i= -ooj= -oo 
The 2D-DTFT of the 2D convolution of x with y is the product of the 
respective 2-DTFTs. In other words: 
(5.17) 
Just as for 1D, the problem of the numerical calculation of the 2D-DTFT 
leads to the introduction of the 2D-DFT, which corresponds to the 2D-DTFT's 
expression calculated in points regularly spread-out over the (0,1) x (0, 1) block. 
Without being at all specific, and as for 1D, the number of points before and 
after the transformation can be considered the same, by completing with zeros 
if necessary. This leads to the following definition. 
Definition 5.2 (2D Discrete Fourier Transform (2D-DFT)) 
The 2D discrete Fourier transform, or 2D-DFT, of the finite sequence {x(k,e)} , 
with k E {O, ... , M - I} and IE {O, ... , N - I}, is the sequence defined, for 
mE {O, . .. , M -I} and n E {O, .. . , N -I}, by: 
X(m,n) = l
l ~ x(k,e)exp {-27rj (~ 
+ ~ ) } 
k= O £=0 
(5.18) 
If we change the expression of X (m, n) to: 
(5.19) 
for each value of e, the 1D-DFT of the sequence x(k, e) for the variable k 
appears in the parenthesis. With MATLAB@, the N FFTs corresponding to 
expression 5.19 are calculated by applying the fft function to the (M x N) 
array x. The 2D-DFT is then achieved simply by performing another FFT on 
the resulting transpose array. 
To sum up, the 2D-DFT is obtained by doing: 
fft (fft (x) . ') . I 
The fft2 function, available in the basic version of MATLAB@, performs 
the same operation. As was the case with 1D signals, typing fft2 ex, M, N) 
completes, if necessary, the array x with zeros so as to have an M x N array. 
Again, as it was the case for 1D signals, it is often preferable to display the 
spatial frequencies with values between ° 
and 1, or between ° 
and 1/ 2. This is 
what we did in example 5.4. 

Chapter 5 - An Introduction to Image Processing 201 
EXAMPLE 5.4 (2D-DTFT of a square block) 
The following program calculates the 2D-DTFT of a square block and displays 
its modulus. The result is shown in Figures 5.16 and 5.17: 
%===== tstfftblock.m 
block=zeros(8, 8); delta=4; 
block(1:delta,1:delta)=ones(delta,delta); 
set(gcf, 'color ' , [1 1 1]) 
subplot(131); imagesc(block); colormap('gray'); 
axis('image'); set(gca,' xcolor' ,[0 0 0], ' ycolor' ,[0 0 0]) 
%===== spectral content 
M=32; N=32 ; blockFqs=fft2(block,M,N); 
%===== normalized spatial frequencies 
mu=(0:M-1)/M;nu=(0:N- 1)/N; 
subplot(132); contour(nu,mu,abs(blockFqs),20); 
axis('square'); set(gca, 'xlim' ,[0 .5], 'ylim', [0 .5]) 
set(gca, ' xcolor' ,[0 0 0] ,'ycolor', [0 0 0]) 
subplot(133); imagesc(nu,mu,abs(blockFqs» 
axis('square'); set(gca, 'xlim' , [0 .5], 'ylim', [0 .5]) 
set(gca, 'xcolor' , [0 0 0] , 'ycolor', [0 0 0]) 
0.5 
0.5 
2 
O'Q) 
0.4 
4 
0.3 
0.3 
I 
:~ _1CU 
0.2 
6 
0.1 
8 
o 
0.2 
0.4 
0.2 
0.4 
Figure 5.16 - 2D-FFT applied to the rectangular block by restricting the frequencies 
to ([0, 1/2] x [0, 1/2]) 
The lobes are similar to the ones obtained for the discrete-time sine cardinal 
(Figure 5.17). 
The properties of the 2D-DFT are similar to those of the 1D-DFT: 
Property 5.2 (Inverse 2D-DFT) The 2D-inverse-DFT of X(m, n) has the 
expression: 
1 
M-l N-l 
{( km 
J!n )} 
x(k,J!) = MN f o]; X(m,n)exp 2nj 
M + N 
where k E {O, . . . , M - 1} and J! E {O, . . . , N - 1}, 

202 Digital Signal and Image Processing using MATLAB® 
20 
15 
10 
5 
01 
Figure 5.17 - 2D-FFT applied to the rectangular block with the frequencies belonging 
to ([0, IJ x [0,1]) 
This result is obtained by using the relation: 
g(k,J!) 
1 
M-l N-l 
{ 
(km 
J!n )} 
M N fa ~ exp 27rj 
M + N 
{
I 
if k = 0 mod M and J! = 0 mod N 
o otherwise 
Property 5.3 (Circular convolution (2D-DFT» 
Let x(k,J!) and y(k,J!) be two images with the same finite size M x N. Let 
X(m, n) and Y(m, n) be their respective 2D-DFTs calculated over MxN points. 
Then the inverse 2D-DFT of Z(m, n) = X(m, n)Y(m, n) has the following 
expression, for k E {O, . . . , M - I} and J! E {O, . .. , N - I}: 
M-l N -l 
z(k,J!) = L 
Lx(u,v)Y«k-u)modM,(J!-v)modN) 
u=o v=o 
where the indices of yare calculated modulo M and modulo N respectively. 
Property 5.4 (Real image and hermitian symmetry (2D-DFT» 
If the image x(k, J!) is real then its 2D-DFT is such that: 
X(m,n) = X*(-m mod M, -n mod N) 
where the first indices2 are calculated modulo M and the second indices modulo 
N. 
Thus, X(O, O) = X*(O,O) which is therefore real. If M = 8 and N = 16, 
X(4,3) = X*(8 - 4, 16 - 3) = X*(4, 13). 
2Bear in mind that the array indices start at 1 and not O. 

Chapter 5 - An Introduction to Image Processing 
203 
EXAMPLE 5.5 (2D-DFT of a checkerboard) 
The following program calculates the 2D-DFT of a checkerboard the horizontal 
frequency of which is fOx=O. 2 and the vertical frequency fOy=O. 3, and displays 
its modulus (Figure 5.18). The resulting graph shows lobes at the spatial 
frequencies (0.2, 0.3) and (1 - 0.2, 1 - 0.3), since the image is real. You can 
try other values of fOx and fOy. 
%===== tstfftmo .m 
%===== checkerboard 
clear ; mside=8; bloc=zeros(mside ,mside) ; 
fOx=0.2; fOy=0.3; 
dom=fOx*(0:mside-1) '*ones(1,mside)+fOy*ones(mside,1)*(0 :mside-1); 
chkbd=cos(2*pi*dom)+1 ; 
set(gcf, 'color ' , [1 1 1]) 
subplot(121); imagesc(chkbd); 
colormap('gray'); axis('image') 
set(gca, 'xcolor' ,[0 0 0] ,'ycolor', [0 0 0]) 
%===== spectral content 
M=128; N=128; chkbdFqs=fft2(chkbd,M , N); 
mu=(0:M-1)/M;nu=(0:N-1)/N; 
subplot (122) ; 
contour(nu,mu,abs(chkbdFqs),20); %imagesc(nu,mu,abs(chkbdFqs»; 
set(gca, 'xcolor' ,[0 0 0], 'ycolor', [0 0 0]) 
axis('square'); grid 
2 
4 
6 
8 
Figure 5.18 - 2D-FF T applied to a checkerboard 
If x(k,£) is separable, that is if x(k,£) = xl (k)X2(£)' the 2D-DFT can be 
expressed as the product of two 1D-DFTs (meaning that X (m, n) is separable). 
Thus, we can write: 
X(m,n) 
The calculation then becomes quite simpler. 

204 Digital Signal and Image Processing using MATLAB® 
5.5 Linear filtering 
The filter2 function, used for 2D filtering, is available in the basic version 
of MATLAB®. This function uses the 2D-convolution function, the command 
line of which is, in MATLAB®, c=conv2 Ca, b). The 2D-convolution is a built-in 
Junction. 
Definition 5.3 2D-linear filtering is the operation that associates the image 
x(k, £) with the image y(k, £) defined by: 
+00 
+00 
y(k, £) = (x * h)(k, £) = L L x(k - m, £ - n)h(m, n) 
(5.20) 
m = -CX) n = -CX) 
The two index sequence h(k, £), characteristic oj the filter, is called the Point 
Spread Function, or PSF. 
As was the case for 1D-filtering, the operation denoted by "*" in expression 
5.20 is linear and space-invariant, and the sequence h(k, £) is the equivalent 
of the impulse response for the one dimension case. Once again, property 5.1 
leads to a simple expression of the filtering operation in the frequential range. 
This gives us the following property. 
Property 5.5 Consider a 2D-linear filter with the PSF h(k, £) . H(/-L,v) de-
notes the 2D-DTFT oj its PSF. It is called the optical transfer function, or 
OTF. Because oj property 5.1 we have: 
Y(/-L, v) = H(/-L, v)X(/-L, v) 
whereX(/-L,v) andY(/-L,v) reJerto the2-DTFTs oJx(k,£) andy(k,£) respec-
tively. 
Thus, the identity filter has the PSF h(k, £) = o(k)o(£), where o(k) is equal 
to 1 if k = 0 and 0 otherwise. Its OTF is equal to 1 for any frequency pair 
(/-L, v). This filter leaves the input image untouched. Because of property 5.5, 
we can also say that the identity filter passes all frequencies. 
In MATLAB®, unlike the filterCb,a,x) function for 1D use, which allows 
the user to design an infinite impulse response filter using the input coefficients 
a, the filter2CB,x) function performs only the 2D equivalent of a finite im-
pulse response filtering, the expression of which is: 
M2 
N2 
y(k,£)=(x*h)(k,£)= L L x(k -m,£-n)h(m,n) 
(5.21) 
m = M, n= N, 
The filter2 function has an additional parameter that allows the user 
to set how the side effects should be taken into account: 
I same I to have an 

Chapter 5 - An Introduction to Image Processing 
205 
output image with the same size as the input image (this is the default option), 
'valid' to keep only the part of the image unaffected by the side effect (the 
resulting image is smaller than the original), and 'full' to keep all of the 
points, including the ones resulting from the filter's impulse response (this 
leads to an image larger than the original). 
The concept of stability is essential, as it was with 1D signals. It states that 
to any bounded input corresponds a bounded output. Because we will only be 
considering filters characterized by expression 5.21 and similar to the 1D FIR 
filters, the stability condition will always be met from now on. 
On the other hand, the concept of causality, although fundamental when 
it comes to signals, has very little significance in the case of images. This 
is because there is no reason for the quantity calculated for the coordinates 
(k,€) to be dependent only on the points placed "before" (k,€), that is to say 
(k - m ,€ - n), where m and n are positive. In 2D processing, all of the points 
around (k, €) can contribute to the calculated value. 
EXAMPLE 5.6 (Circular filter) 
Consider what is called the circular filter, h(k, l), defined in the program: 
%===== smooth1.m 
h =[0 0 1 1 1 0 0; 
0 1 1 1 1 1 O· , 
1 1 1 1 1 1 1 ; 
1 1 1 1 1 1 l ' , 
1 1 1 1 1 1 1 ; 
0 1 1 1 1 1 O· , 
o 0 1 1 1 0 0]; 
h = h / sum(sum(h»; 
load wenmanu; subplot(121); imagesc(pixc); 
colormap(cmap); axis('image'); 
set(gca, 'units', 'pixels' ,'DataAspectRatio' , [1 1 1]) 
pixr=filter2(h ,pixc) ; subplot(122); imagesc(pixr) ; 
set(gca, 'units', 'pixels' , 'DataAspectRatio' , [1 1 1]) 
axis (' image' ) 
This program smooths the image 5.19. 
As was the case with 1D smoothing filters, this filter tends to "erase" high 
frequencies, particularly the ones contained in the contours, and therefore pro-
duces a blurred image (Figure 5.19) compared to the original image. 
D efinition 5.4 A filter is said to be separable when its PSF is such that: 
(5.22) 
In the case of a finite PSF, if h is the matrix with h( k , €) as its elements, 
and if hx and hy are the vectors with the respective components hx(k) and 

206 Digital Signal and Image Processing using MATLAB® 
50 
1 00 
1 50 
200 
250 
300 
350 
50 
1 00 
1 50 
200 
250 
300 
350 
Figure 5.19 - Smoothing of an image test using the circular filter 
hy(R), relation 5.22 is equivalent to: 
h = h x h ~ 
(5.23) 
We are going to show that a separable 2D filtering can be performed by 
combining two consecutive 1D filters. This is how it works: 
K2 
£ 2 
(x * h) (m, n) 
L L x(m - k,n - R)h(k,R) 
k = K, £=£, 
Bear in mind that if the filter function is used for a separable 2D filtering, 
you must take into account the fact that filter implements a causal design 
(exercise 5.4). 
Exercise 5.4 (The rectangular filter) (see p. 442) 
Consider the rectangular filter defined by: 
1 
1 
1 
1 
1 
1 
h =~ 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 [11111] 
25 
1 
1 
1 
1 
1 
25 
1 
1 
1 
1 
1 
1 
1 
Write a program that: 

Chapter 5 - An Introduction to Image Processing 207 
1. performs the filtering of the test image; 
2. performs the same filtering using two separate ID filterings. 
Exercise 5.5 (The conical filter) (see p. 443) 
The conical filter is defined by: 
0 0 
1 0 0 
h =~ 
0 
2 2 2 0 
1 2 5 2 
1 
25 
0 
2 2 2 0 
0 0 
1 0 0 
Apply the conical filter to the test image. 
D efinition 5.5 (The Gaussia n smoothing filter) The generating element 
of the Gaussian smoothing filter's PSF is: 
1 
(k2 + Ji
2
) 
h(k, Ji) = --2 exp -
2 
27r0" 
20" 
This filter is separable, since we can write h(k,Ji) = hx(k)hy(Ji). 
The smaller the 0" parameter is, the more the filter behaves like an identity 
filter, that is to say that it passes all the frequencies of the plane. The gain 
filter can of course be modified by multiplying it by a constant. 
Exercise 5.6 (The Gaussian smoothing filter) (see p. 443) 
1. Write a MATLAB® function that calculates the PSF of Gaussian smooth-
ing filter using 0" . Consider using the meshgrid function; 
2. apply the Gaussian filter to the test image. 
2D-DFT frequency filtering 
Starting with the circular convolution property 5.3, it is possible to consider 
performing a filtering by simply multiplying the 2D-DFT of an image by the 
2D-DFT of the filter's PSF. Of course, just like in ID, this process must take 
into account the circular convolution property. 
Consider the PSF h( k, Ji) of a K x L filter (number of non-zero coefficients), 
and an M x N image x(k,Ji) . We will assume M > K and N > L. H(m ,n) 
and X(m , n) refer to the 2D-DFTs of the PSF and of the image respectively. 
Both are calculated for M x N points. According to property 5.3, the inverse 

208 Digital Signal and Image Processing using MATLAB® 
2D-DFT of the product H(m, n)X(m, n) can be written, for k E {O, . . . , M -I} 
and € E {O, . . . , N - I}: 
K-IL-l 
y(k,€) = L L h(u,v)x(k - u mod M ,€ - v mod N) 
u=o v=o 
For k ~ K - 1, (k - u mod M) = k - u: there is no index "aliasing" when 
we sum u from 0 to (K -1). This also true for € ~ L-1 , (€-v mod N) = €-v . 
In this case, the calculated points do correspond to those of the convolution 
associated with the filtering. However, for k < K - 1 and/or € < N - 1, there 
is an index "aliasing" which leads to an incorrect result. One way of avoiding 
this phenomenon is by completing the image with K zeros along the horizontal 
axis, and L zeros along the vertical axis. 
Derivative operations 
To display the variations of a 2D function, the concept of derivative can be 
used, as it was in 1D. The difference is that in 2D, the derivative comprises 
two components corresponding to the two directions of the plane. Thus, to 
perform a 2D-derivative, you can use a first derivative filter along the horizontal 
direction, and a second one along the vertical direction. Base derivative filters 
are used for PSF matrices (5.24): 
(5.24) 
The filters that derive in one direction and smooth in the other direction 
are generally preferred. Prewitt and Sobel filters are good examples of this. 
Definition 5.6 (Prewitt derivative filter) 
The PSF of a Prewitt derivative filter along the vertical direction is given by: 
~ ~ q: ~ =:] 
~ m 
[1 
0 -11 
and the PSF of a Prewitt derivative filter along the vertical direction by: 
hh = ~ [ ~ 
~ 
~ ] 
~ [ ~] 
[1 
1 
1] 
-1 -1 -1 
-1 
These two filters are therefore separable. Decomposing h v clearly shows: 
- a smoothing function along one direction, smoothing corresponding to 
the vector [1 
1 
1] / v'3; 

Chapter 5 - An Introduction to Image Processing 209 
- a derivative function in the other direction, corresponding to the vector 
[1 
0 -1] / V3. Remember that the 1D causal filter defined by this 
vector associates the input u(n) with the output v(n) = (u(n) - u(n-
2)) / V3, which can be seen as the derivative. 
Definition 5.7 (Sobel derivative filter) 
The PSF of a Sobel fitler along the vertical axis is given by: 
(5.25) 
The PSF of a Sobel fitler along the vertical axis is given by: 
(5.26) 
Sobel filters are separable. Notice that they perform a derivative along one 
axis, and a smoothing operation along the perpendicular axis. 
Applying formula 5.23 leads to a 2D filter that derives in both directions 
without any smoothing: 
-1] = a [ ~ 
-1 
where a is a normalization coefficient. 
o 
o 
o Tl 
Starting off in 1D, we can also design a 2D filter that approximates the sec-
ond derivative, by convoluting the impulse response filter [-1 1], which is an 
approximation of the first derivative, with itself. If you type conv( [-1 1], [-1 
1]) in MATLAB@, the result is [1 -2 1] . By combining the two directions, 
we get a 2D filter that performs a second derivative in both directions, defined 
by: 
-2 
[
0 
_114 
O~l 
1] = a 
~ 
(5.27) 
where a is a normalization coefficient. 
Generally speaking, it is of course possible to design other filters using one 
dimension design methods, and then inferring a 2D separable filter with formula 
5.23. 

210 Digital Signal and Image Processing using MATLAB® 
Exercise 5.7 (The Sobel derivative filter) (see p. 444) 
1. Apply the Sobel filters 5.25 and 5.26 to the test image; 
2. apply the filter 5.27 to the same image. 
3. by using a similar method to the window method, calculate a derivative 
filter; 
4. same question for a second derivative filter. 
Definition 5.8 (Gaussian derivative filter) 
Consider the function defined as the difference between two Gaussians, also 
called a Difference of gaussians mask, or DoG mask: 
g(k,£) = _1_ exp (_ k
2 + £2 ) __ 
1_ exp (_ k
2 + £2 ) 
27rO"r 
20"r 
27r0"~ 
20"~ 
with 0"2 = r0"1 and r between 1.4 et 1.8. The Gaussian derivative filter is the 
filter with the following PSF: 
h(k, £) = g(k, £) - L L g(k, £) 
k 
£ 
implying that "£k "££ h(k, £) = O. 
The imposed condition, "£k "££ h(k, £) = 0, is related to the fact that a 
derivative filter has a gain equal to 0 at the frequency O. This result is similar 
to the one obtained in 1D in exercise 4.10. 
The graph of a Gaussian derivative filter PSF is shaped like the one in 
Figure 5.20. 
EXAMPLE 5.7 (Gaussian derivative filter) 
1. Write a MATLAB® function that calculates the PSF of a Gaussian deriva-
tive filter. Write it so that "£k "£z h(k, l) = 0; 
2. apply this filter to the test image for three values of 0"1 = {1, 2, 3} and 
for r = 1.4. Save the results in three different files (use the functions 
sprintf and eval to change in a program the name of the saved file). 
HINT: 
1. Type: 
function hd=dergauss(sigma) 
%!=================================! 
%! Gaussian derivative filter 
! 
%! SYNOPSIS: hd = DERGAUSS(sigma) 

Chapter 5 - An Introduction to Image Processing 211 
0.1 
0.08 
0.06 
0.04 
0.02 
°:0_ 
-0.02 __ : x ~~~~~~--lL 
-
---
-
~ -
--
, --
-0.04 
___ __ ___ _ -----r- _-:: -
Figure 5.20 - Graph shape of a Gaussian derivative filter 's PSF 
%! 
sigma = Standard deviation 
%! 
hd 
= filter with (N*N)-PSF 
%!=================================! 
rho=[-sigma*3:sigma*3]; N=length(rho); 
rp=1.4; s2=2*sigma-2; s22=s2*rp*rp; 
idx= ([1:N]-(N+1)/2)' * ones(1,N); idy=idx'; 
idxa=[1:N]' * ones(1,N); idya=idxa'; 
%===== 
inds(1,:)=reshape(idx,1,N*N); inda(1,:)=reshape(idxa,1,N*N); 
inds(2,:)=reshape(idy,1,N*N); inda(2,:)=reshape(idya,1,N*N); 
rho2=sum(inds .* inds); rho=sqrt(rho2); 
for k=1 :N*N 
g1=(1/sigma)*exp(-rho2(k) / s2); 
g2=(1/sigma/rp)*exp(-rho2(k) / s22); 
hd(inda(2,k),inda(1,k))=g1-g2; 
end 
hd=hd-sum(sum(hd))/N/N; 
return 
2. Applying the filter to the test image (Figure 5.21): 
%===== tstdergauss.m 
%===== loading the image 
clear; load lena; subplot(221); imagesc(pixc+1); 
colormap(cmap); axis('image') 
set(gca, 'Xcolor' ,[0 0 O],'Ycolor', [0 0 0]) 
%===== gaussian derivative filter 

212 Digital Signal and Image Processing using MATLAB® 
for k=1:3 
end 
hd=dergauss (k) ; 
pixr=round(filter2(hd,pixc»; 
subplot(2,2,k+1); imagesc(pixr); axis('image') 
set(gca,'Xcolor', [0 0 0], 'Ycolor' ,[0 0 0]) 
set(gcf, 'Color' ,[1 1 1]) 
0"=2 
0"=3 
Figure 5.21 - Gaussian derivative for 0"1 = {I, 2, 3} and 0"2 = 1.40"1 
The programs we have just described are used in the contour detection 
program. 
Definition 5.9 (Gaussian derivative-smoothing filter) 
Consider the rotation of angle e that changes the point with coordinates (u, v) 
according to the expression: 
[U(k,R)] = [cos(e) 
v(k,R) 
sinCe) 
- SinCe)] [k] 
cos(e) 
R 
(5.28) 
(5.29) 

Chapter 5 - An Introduction to Image Processing 
213 
and the derivative function: 
h (k f!) = _ v(k, f!) 
(_ v(k, f!)2) 
2, 
3 IF exp 
2 2 
172 VL-7r 
172 
(5.30) 
The Gaussian derivative-smoothing filter is the filter that performs a Gaus-
sian smoothing filtering (function hI) along the direction () E (0, 27r) and a 
Gaussian derivative filtering (Junction h2 ) along the perpendicular direction. 
The expression of its PSF's generating element is: 
h(k,f!) = hl(k,f!)h2(k,f!) - LLhl(k,f!)h2(k,f!) 
k 
C 
which verifies Lk Lc h(k, f!) = O. 
Exercise 5.8 (Gaussian derivative-smoothing filter) (see p. 447) 
1. Write a MATLAB® function that calculates the PSF of a Gaussian 
derivative-smoothing filter; 
2. apply this filter to the test image. 
5.6 Other operations on images 
5.6.1 
Undersampling 
As it was the case with one dimension signals, the undersampling has to meet 
some conditions to avoid the aliasing phenomenon. Remember that aliasing 
occurs when the sampling rate is too slow compared to the frequencies found 
in the image, and causes frequential artifacts to appear. In an image, high 
frequencies correspond to important variations in color and/or brightness con-
centrated on small surfaces. Take the example of the images represented in 
Figure 5.22. They were obtained with the following program: 
%===== aliasingtrains.m 
close all; clear all 
load trainsV4, sxxl=size(xxl); 
xxlse=xxl(1:5:sxxl(1),1:5:sxxl(2)); % undersampling 
Iwpass=ones(5,5)/25; 
yyl=filter2(lwpass,xxl); 
% filtering before 
yylse=yyl(1:5:sxxl(1),1:5:sxxl(2)); % under-sampling 
subplot(221); imagesc(xxl); colormap('gray'), axis('image') 
subplot(222); imagesc(xxlse); colormap('gray'), axis('image') 
subplot(223); imagesc(yyl); colormap('gray'), axis('image') 
subplot(224); imagesc(yylse); colormap('gray'), axis('image') 
set(gcf, 'color', 'w') 

214 Digital Signal and Image Processing using MATLAB® 
100 ...... Hi ~~ 
f!II!
!iii!!-
~~
. 
200 
300 
400 
500 
200 
300 
400 
500 
200 
200 
400 
600 
400 
600 
50 
100 
20 ~ W~ ~ ~NP':; 
40 
60 
80 
100 
50 
100 
150 
150 
Figure 5.22 - Effects of spectrum aliasing. In the top-left corner, the original image 
(512 x 768). In the top-right corner, the same image undersampled by a factor of 5. 
In the bottom-left corner, the image filtered by a smoothing filter over a 5 x 5 square. 
In the bottom-right corner, the image filtered and undersampled by a factor of 5 
In the top-left corner, you can see the original image, a 512 x 768 array. 
This image contains "high frequencies" , particularly around the electric cables 
and the tracks, where the shapes are in some places less than a few pixels 
wide. In the image represented in the top-right corner, obtained by taking 1 
out of 5 pixels horizontally and vertically, you can clearly see major and erratic 
variations in some areas of the image, due to aliasing. 
Just like in 1D, a low-pass filtering must be performed before the under-
sampling. This can be done with a simple filter, calculating the mean over 
5 x 5 cells. This operation is performed by filter2(lwpass,xxl) , which uses 
the filter2 function. The resulting image is shown in the bottom-left corner. 
The filter causes a slight "blur". The image in the bottom-right corner shows 
the previous image after the undersampling operation. Most of the artifacts 
are gone. The tracks in particular show less unwanted fluctuations. 

Chapter 5 - An Introduction to Image Processing 
215 
5.6.2 Oversampling 
As for the oversampling of 1D signals, the interpolation operation can be per-
formed by the insertion of "zeros" (the zero's significance is not the same here) 
followed by a low-pass filter. In the following example, we isolated the part 
of the original image containing the clock, on the platform to the left. This 
portion of the image is shown on the left-hand side of Figure 5.23. In order 
to improve the image rendering, we oversampled by a factor of 4, horizontally 
and vertically. The low-pass filter is a separable filter with a PSF of the type 
sin exl ex, to which a Hamming window is applied in order to reduce the ripples 
in the resulting image. The following program was used to obtain the image 
on the right of Figure 5.23: 
%===== oversamp2ds.m 
%===== over-sampling ratio 
clear; Mx=4; My=4; cmap='gray'; 
load trainsV4; ima=xxl; 
% the file "trains" --> xxi 
%===== zooming in on the clock 
pixc=ima(180:210,80 :120); 
[Lig ,Col]=size(pixc); 
%===== low-pass filter PSF (Lfft>N) 
N=30; [X,Y]=meshgrid(-N:l:N, -N:l:N); X=X+eps ; Y=Y+eps; 
FEP=Mx*My*(sin(pi * X/Mx) ./ X) . * (sin(pi * Y/My) . / Y) ; 
%===== Hamming window 
W = (0.54 - 0 .46*cos(2*pi*(X+N)/(2*N))) . . . 
. * (0 .54 - 0.46*cos(2*pi*(Y+N)/(2*N))) ; 
FEP=FEP . * W; 
%===== expansion and filtering 
pixcz=zeros(Mx*Lig,My*Col); 
pixcz(l:Mx:Mx*Lig,l :My :My*Col)=pixc; 
pixcSE=filter2(FEP,pixcz); 
%===== displaying the result 
subplot(121) ; imagesc(pixc); axis('image'); colormap(cmap); 
subplot(122); imagesc(pixcSE); axis('image'); colormap(cmap); 
set(gcf, 'Color ' , [1 1 1]) 
These techniques, taken directly from signal processing, are not the only 
ones used. The bibliography shows some sources of information for bilinear 
interpolations, cubic interpolations, etc. 
For example, the bilinear interpolation consists of constructing "intermedi-
ate" pixels P from four pixels Poo , Pal, PlO and Pu : 
P = Poo(1 - tx)( l - ty) + Pal (1 - tx)ty + PlOtx (1 - ty) + Putxty 
by making the values of the parameters tx and ty vary from 0 to 1. 
EXAMPLE 5.8 (Bilinear interpolation) The bilintrimg function performs 
the bilinear interpolation of an image with (n x m) pixels: 

216 Digital Signal and Image Processing using MATLAB® 
Figure 5.23 - Image on the left: zoom-in on the clock in the original image from 
Figure 5.22. Image on the right: oversampling by a factor of 4 in both directions 
function pixcR=bilintrimg(pixc,Rintx,Rinty) 
%!==============================================! 
%! Bilinear interpolation of an image 
! 
%! SYNOPSIS: pixcR=BILINTRIMG(pixc,Rintx,Rinty) 
%! 
pixc 
image (nl*nc) pixels 
%! 
Rintx = interpolation rate (x) 
%! 
Rinty = interpolation rate (y) 
%!==============================================! 
Spix=size(pixc); Nr=Spix(1); Nc=Spix(2); 
txt=[0:Rintx-1]/Rintx; ty=[0:Rinty-1] '/Rinty; 
nrow=(Nr-1)*Rinty+1; ncol=(Nc-1)*Rintx+1; 
pixcR=zeros(nrow+Rinty,ncol+Rintx); 
MOO=(1-ty)*(1-txt); M01=(1-ty)*txt; 
M10=ty*(1-txt); M11=ty*txt; 
pixc=[pixc zeros(Nr,1);zeros(1,Nc+1)]; 
for kl=1:Nr 
for kc=1:Nc 
end 
tl=(kl-1)*Rinty+[1:Rinty]; tc=(kc-1)*Rintx+[1:Rintx]; 
PC=pixc(kl,kc)*MOO+pixc(kl,kc+1)*M01+ . .. 
pixc(kl+1,kc)*M10+pixc(kl+1,kc+1)*M11; 
pixcR(tl,tc)=PC; 
end 
pixcR=pixcR(1:nrow,1:ncol); 
return 
The program creates interpolated points based on the four points A(O, 0,1), 
B(l, 0, 0), C(O, 1,0.5) and D(l, 1,0.5) with interpolation ratios equal to 4 (Fig-
ure 5.24) . 
%===== tstbilinr.m 
pixc=[1 0;.5 .5]; % A, B, C, D 
pixcR=bilintrimg(pixc,4,4); 

Chapter 5 - An Introduction to Image Processing 
217 
[X,Y]=meshgrid((O:4)/4 , (O:4)/4) ; 
Xr=reshape(X,25,1) ; Yr=reshape(Y , 25 , 1); 
pixcRr=reshape(pixcR,25,1); 
mesh(X,Y,pixcR), view(20,48) 
0.8 
0.6 
0.4 
0.2 
o 
o 
Figure 5.24 - Applying the bilinear interpolation 
5.6.3 
Contour detection 
Contour detection is a common application of image processing. A simple 
method is to start by extracting the portions of the image with a significant 
gradient. This can be done with a derivative filter. We then need to define a 
boolean information, for each pixel, stating whether or not the pixel belongs 
to a contour. This can be done simply by comparing the obtained results to a 
threshold. The following program uses an image obtained by differentiation in 
example 5.7 (gaussian derivative filter): 
%===== thresholdg .m 
%===== file loading 
clear; load lenabool2; % after differentiation 
subplot(121); mydisp (pixr,cmap); 
%===== threshold with manual choice for alpha 
alpha=O; 
decal=round((max(max (pixr))+min(min(pixr)))/2); 
subplot(122) ; imagesc(-sign(pixr+decal+alpha)) ; 
axis ( ' image I ) 
The result of the thresholding is represented in Figure 5.25. 

218 Digital Signal and Image Processing using MATLAB® 
Figure 5.25 - Results of the thresholding after derivative filtering 
Combining a Gaussian low-pass filter with first order horizontal and vertical 
derivatives, such as in the previous example, is a very common method for 
contour detection. J. Canny [5] showed that this method is very similar to 
applying a filter that optimizes a criterion related to precision and stability. 
It can be wiser to search for local maxima - peaks - in the result of the 
derivative. 
Exercise 5.9 (Contours using Sobel filtering) (see p. 448) 
1. Apply the Sobel filters 5.25 and 5.26 to the test image; 
2. using the resulting pixels Pv(k, l) and Ph(k, l), construct the image of the 
pixels Jp~(k,l) 
+p~(k
, l). 
By defining an appropriate threshold value, 
extract the contours of the image. 
The function fminsearch implementing the NeIder-Mead algorithm can be 
used for the seaching of extrema. 
Hough method 
For shape recognition, it may be useful to detect the presence of basic shapes, 
such as circles, ellipses, straight lines, etc. The Hough method is one of the 
most common. 
Consider for example the case of line detection in an image previously pro-
cessed so as to outline the contours. For straight contour detection, we then 
use sets of lines where each straight line is defined by the pair of paramaters 
(p, B) (Figure 5.26). p refers to the distance to the origin and B the angle to 
the direction perpendicular to the line. 
In each point of the contour, a set of concurrent lines is built, with the 
parameters p and B (see Figure 5.26). Figure 5.27 shows that two sets of lines 

Chapter 5 - An Introduction to Image Processing 219 
Image space 
e 
Parameter space 
90 
o 
p 
Set 2 
Figure 5.26 - Setting the parameters for the set of lines 
on a portion of a line share a common point, or to be less specific, in the same 
neighborhood, on the parameters. 
The accumulation, resulting from all the filters associated with this portion 
of a line, leads to a maximum in the neighborhood of this point. We then 
proceed to partitioning the parameter space, so as to obtain a quantization 
grid, then we count the points inside each box of the grid. The resulting values 
are then used for different kinds of processing. 
e 
r 
o 
Accumulation 
Figure 5.27 - Using sets of lines 
EXAMPLE 5.9 (Implementing the Hough method) 
Consider an image (Figure 5.29, image on the left), assumed to have been 

220 Digital Signal and Image Processing using MATLAB® 
obtained by contour extraction. The following program performs a search for 
straight lines 
%===== hough.m 
load hough; [nlig,ncol]=size(pixc); 
figure(l); subplot(121); colormap(cmap); 
imagesc(pixc); axis('image') 
%===== contour extraction 
indx=find(pixc==O) ; Nidx=length(indx); 
%===== 
Nt=60; thetad=[O: 180/Nt: 180] ; theta=thetad*pi/180; 
thet=thetad(l:Nt); tbl=zeros(Nt,Nt); 
figure (2) 
%===== for each point and each value 
% 
of theta, rho is computed 
for k=l: Nidx 
nc=floor((indx(k)-l)/nlig)+l; nl=indx(k)-(nc-1)*nlig; 
for m=l:Nt, rho(m)=nc*cos(theta(m»+nl*sin(theta(m»; end 
tbl(: ,k)=rho' ; 
plot(rho,thet); hold on 
end 
rhomax=sqrt(nlig*nlig+ncol*ncol); 
set(gca, 'Xlim' ,[0 rhomax]); grid; hold off 
%===== result (visual examination) 
figure(l); subplot(122); imagesc(pixc); colormap(cmap); 
axis('image'); hold on 
plot([O 54.5*cos(70*pi/180)], [0 54.5*sin(70*pi/180)]) 
hold off 
The observation of the resulting set (Figure 5.28) provides us with a direc-
tion. 
The perpendicular direction is indicated in Figure 5.29 (the image on the 
right) . An automatic search requires searching for zones with a high point 
density, hence the idea to use a 2D histogram to extract the (p, e) positions of 
the maxima. 
One example of an application for this type of processing is the search for 
the writing line directions in a handwritten text. The method described here 
is known as the Hough method [18], or Hough transform method, and allows the 
extraction of directions, one of which still has to be chosen. The presence of a 
high point density along a line can help find them. 
The Hough method can also be used to search for other shapes. The idea 
is the same. For example, for circular shapes, the parameter {r, e, R} can be 
used, where the pair (r, e) refers to the polar coordinates of the circle's center 
and R is its radius. 

Chapter 5 - An Introduction to Image Processing 
221 
Figure 5.28 - Zoom-in on the set of lines: we find the values p = 54.5 and () = 70
0 
Figure 5.29 - Search for a straight line 
5.6.4 Median filtering 
Compared to other non-linear filters, median filtering is both simple and ef-
ficient. Just like a linear low-pass filtering, it smooths the image and can 
therefore eliminate certain of the image's imperfections. However, unlike a lin-
ear low-pass filter, which inevitably adds a blur around the contours, it better 
preserves the sharp variations of the image. 
Definition 5.10 (Median filter) 
Let {a( k, e)} be an image. The median filter associates the mean value m( k, e) 
with the point with coordinates (k, e), in the (M x N) rectangular window, 

222 Digital Signal and Image Processing using MATLAB® 
centered on (k,f!). If we assume Nand M to be odd, and ifu(n) denotes the 
sorted sequence (u(n) ~ u(n - 1») obtained from the array [a(i ,j)J (M x N) 
where i E {(k - (M - 1)/2, ... , k + (M - 1)/2} and j E {( f! - (N - 1)/2, ... , 
f! - (N + 1)/2)}, we have: 
m(k,f!) = u((MN + 1)/2) 
Exercise 5.10 (Median filtering) (see p. 448) 
Apply this program to the test image: 
%===== snowing.m 
load lena 
dims = size(pixc); 
msnow = (randn(dims»-2); 
pixcmsnow = pixc .* msnow; 
imagesc(pixcmsnow); axis('image') 
This causes white points to randomly riddle the image, a bit like snow. 
Compare the effect of a Gaussian smoothing filter with the effect of a median 
filtering on the "snowy" image. 
Exercise 5.11 (Processing the result of a rotation) (see p. 449) 
Use the saved image in example 5.3: 
1. perform a 3 x 3 median filtering on the resulting image. Try several 
rotation angles; 
2. perform a processing of the "missing" points by calculating a mean on 
the surrounding pixels. 
There are many possible methods for processing an image after it has under-
gone geometric transformations: interpolations, morphological filtering (para-
graph 5.6.7), median filterings, etc. or other methods adapted to the case in 
question. There are no absolute rules in the field. 
5.6.5 Image binarization 
Image binarization consists of intensifying the contrast until complete satu-
ration is reached. Black and white are the only two levels kept after this 
operation. It is used in particular for Optical Character Recognition, or OCR. 
The technique described below is based on a method suggested by N. Otsu in 
1979 [26J. It requires the calculation of an histogram first, followed by a sep-
aration in two categories, ~ and C, associated with the two colors. It is quite 
simple to adapt this method to a greater number of categories: 

Chapter 5 - An Introduction to Image Processing 
223 
1. The histogram calculation consists of initializing with zeros an array H = 
[h(k)] with P = 256 entries. These entries correspond to P levels of gray. 
The entire image is covered, and for each pixel (n, m) with a level of gray 
k, the entry for h(k) in the array H is incremented. The histogram is 
normalized by dividing H by the number N of pixels in the image. The 
h(k) can then be interpreted as estimated values for the probabilities of 
finding the 256 levels of gray in the image. 
2. Separating the image pixels in two categories can be done by directly 
comparing levels of gray with a threshold value defined by observing the 
previous histogram. 
This very simple method can give disappointing results. There are two main 
drawbacks. First, isolated pixels can belong to an area and not be part of that 
area's category. In particular, this can lead to highly contrasted textures. The 
second drawback concerns images showing the shadow of certain objects. It 
is not always a good thing to have them belong to the same category as the 
object they came from, whatever the lighting may be. 
Consider for example the original image in Figure 5.31. The following pro-
gram first draws the histogram for the 256 levels (Figure 5.30), then uses it to 
calculate threshold values. Based on these values, the program displays two bi-
narization examples. The results, for two threshold values, are shown in Figure 
5.31: 
%===== binar1.m 
load elido72 
[nlig ncol]=size(pixc); nbpix=prod(size(pixc)); 
%===== global histogram 
histog=zeros(1,256); pixc3=zeros(nlig*ncol,1); pixc3(:)=pixc; 
histog=hist(pixc3,256)/nlig/ncol; 
figure(1); plot([O:255] ,histog); grid 
%===== thresholds based on a visual examination 
% 
of the histogram 
figure(2); subplot(131); 
imagesc(pixc); axis('image'); colormap(cmap); 
%===== threshold 1 
pixc2=zeros(nlig,ncol); 
seuil=152; idxy=find(pixc>seuil); 
pixc2(idxy)=255*ones(size(idxy)); subplot(132); imagesc(pixc2); 
axis('image'); colormap(cmap) 
%===== threshold 2 
pixc2=zeros(nlig,ncol); 
seuil=90; idxy=find(pixc>seuil); 
pixc2(idxy)=255*ones(size(idxy)); subplot(133); imagesc(pixc2); 
axis('image'); colormap(cmap) 
save histog histog 

224 Digital Signal and Image Processing using MATLAB® 
Threshol~ 
2 
Threshold 1 : 
0.01 
, 
, 
----
-----~----------
~ 
---------~-----------I-----------
, 
" 
, 
" 
, 
" 
, 
" 
, 
" 
-
-- -----T---------- , ----
"---,-----------1-----------
0.008 
, 
" 
, 
" 
, 
" 
, 
" 
0.006 
0.004 
0.002 
o ~~_L~ 
__ ~_L~ 
__ ~~~ 
__ ~_L~ 
__ ~~ 
____ ~ 
o 
50 
100 
150 
200 
250 
300 
Figure 5.30 - Histogram and global thresholds 
Figure 5.31 - Binarization of the image above for two threshold values 
Automatic threshold calulation: the Otsu method 
We will now see how to make the choice of the threshold automatic. In order to 
do this, we will write h(k) to refer to the percentage of values from the image 
that are equal to k, where k E {O, ... , 255}, as it was calculated in the previous 
histogram. h(k) provides an estimation for the probability of level k. 
Let s be the threshold. s defines two categories of values: category C1 for 
values below s, and category Cs for values above s. The method suggested by 
Otsu [26] simply consists of choosing, as the threshold value s, the integer for 
which the quadratic error is minimal between the observed random value k and 
its corresponding value f.l(K) in one of the two categories. 
This method is merely the particular case for 1 bit of the N scalar quan-
tification problem, the solution of which is known as the Lloyd-Max solution. 
Let f.ll and f.ls be the number of pixels in the categories C1 and Cs respec-

Chapter 5 - An Introduction to Image Processing 
225 
tively. The expression of the criterion we wish to minimize, with respect to s, 
f.lI and f.ls is: 
8-1 
P-1 
L(S,f.lI,f.lS) = L(k-f.lI)2h(k) + L(k-f.lS)2h(k) 
(5.31 ) 
k=O 
k=8 
Minimizing this ratio as a function of f.lI and f.ls can be achieved by zeroing 
the partial derivatives of L(s, f.lI, f.ls) with respect to f.lI and f.ls· 
In the case of f.lI for example, this leads to: 
8L(s,f.lI,f.lS) = -2~(k-f.lI)h(k) 
=0 
8f.lI 
k=O 
the solution of which is: 
(5.32) 
Likewise, we have: 
(5.33) 
Notice that there is an obvious interpretation for f.lI and f.ls: they are 
the respective means of each category. By replacing these two expressions of 
L(s, f.lI , f.lS) in 5.31, we get an expression J(s), dependent only on s, which 
needs to be minimal. The solution cannot be obtained analytically, but the 
numerical solution can be found by calculating J(s) for the 256 possible values 
of s. 
There are two equivalent expressions of J (s) that are best adapted for the 
numerical calculation. Let: 
8-1 
P-1 
PI(S) = L h(k) and Ps(s) = L 
h(k) 
(5.34) 
k=O 
k=8 
such that PI(S) + Ps(s) = 1. The minimizing of J(s) with respect to s is 
equivalent to maximizing: 
with respect to s. This is because: 
J(s) 
2:~
: ~ k2h(k) - 2 2:~
:
~ kf.lI(S)h(k) + 2:~
: ~ f.lY(s)h(k) + 
2:[':} k2h(k) - 2 2:[':} kf.ls(s)h(k) + 2:[':} f.l~(s)h(k) 
2:[':01 k2h(k) - (f.lI(s)PI(s) + f.l~(s)Ps(s)) 
(5.35) 

226 Digital Signal and Image Processing using MATLAB® 
Hence, because the first term is independent of s, minimizing J (s) is equiv-
alent to maximizing G(s) = /-lHS)PI(S) + /-l~(s)Ps(s). 
The maximizing of G(s) found in 5.35 is equivalent to the maximizing of: 
(5.36) 
Notice that the quantity: 
PI (s)/-lJ(s) + PS(S)/-lS(S) = L
:~Ol 
k2h(k) 
(see expressions 5.32, 5.33 and 5.34) is independent of s. We can calculate 
its square and substract this square value from G( s) without changing the 
maximizing with respect to s. We get: 
H(s) 
G(s) - (PI(S)/-lI(S) + Ps(s)/-lS(S»2 
/-l7(s)PJ(s) + /-l ~(s)Ps(s) 
- /-l7(s)pl(s) -
/-l ~(s)P§(s) 
-2P1 (s )Ps ( s )/-lI( s )/-ls( s) 
/-l7(s)PJ(s)(1 - PJ(s» + /-l~(s)Ps(s)(l 
- Ps(s» 
-2P1 (s )Ps( s )/-lI( s )/-ls( s) 
If we use PJ(s) + Ps(s) = 1, we get the expected result 5.36. 
Exercise 5.12 (Application of the Otsu method) (see p. 452) 
1. Using MATLAB@, write the function that calculates the threshold ob-
tained by maximizing expression 5.36; 
2. apply this function to a test image. 
Figure 5.32 gives the result obtained by calculating the threshold with the Otsu 
method. 
Figure 5.32 - Binarization for the threshold calculated with the Otsu method 

Chapter 5 - An Introduction to Image Processing 227 
5.6.6 Modifying the contrast of an image 
Differences in contrast were classified by J. Itten [21] into seven different types: 
"saturation", "light and dark", "extension", "complements", "hue", "warm and 
cool", "hue-primary" and "simultaneous". These refer to differences perceived 
between objects of color or brightness. Images in "grayscale" will now be 
examined and also the methods used to improve transitions between different 
areas of grayscale. 
Global changes 
The first and simplest modification consists of reallocating the different colors 
that appear in the brightness histogram. Thus, if the lighter or darker areas 
are not represented, it is easy to redistribute the other brightness settings on 
all other available codes. 
Consider the image in Figure 5.33. This image, in 8-bit grayscale, is a little 
too contrasted. This means that as a whole the luminosity of the histogram is 
focusing on a relatively narrow brightness spectrum (Figure 5.34). 
Figure 5.33 - Details of the image to be modified 
The histogram reveals that the extreme gray values, lower than h min and 
higher han h max , are not used. Other gray values (hmin ::::; p(x, y) ::::; h max ) are 
then distributed between 1 and 256 (fonction hlinmod). 
function pixm=hlinmod(pixc,hmn,hmx) 
%!==========================================! 
%! SYNOPSIS: pixm=HLINMOD(pixc,hmn,hmx) 

228 Digital Signal and Image Processing using MATLAB® 
xl04 
3 .-----~------~----~------~----~ 
2.5 
2 
1.5 
1 
0.5 
OL......O_-
o 
50 
100 
150 
200 
250 
Figure 5.34 - Histogram of the image to be modified 
%! pixc 
image array (8-bit, gray) 
%! hmn, hmx = limits for modifying contrast 
%!==========================================! 
[nr,nc]=size(pixc); pixm=ones(nr,nc); 
idxm=find(pixc<hmn);idxM=find(pixc>hmx); 
pixm (idxm) =hmn; pixm(idxM)=hmx; 
pixm=ones(nr,nc)+(pixc-hmn)*(256-1)/(hmx-hmn); 
return 
The hmn ou hmx parameters can be chosen manually, from the histogram, 
or automatically by choosing a minimum threshold for the number of represen-
tatives in the extreme classes. 
Figure 5.35 - Details of the image 
The lighter or darker areas appear symmetrically in the histogram, and 

Chapter 5 - An Introduction to Image Processing 229 
can be a distribution different at both ends of the code (fonction hadaptmod 
function for an adaptive modification). 
function pixm=hadaptmod(pixc,medpx,hrnn,hrnx) 
%!==============================================! 
%! SYNOPSIS: pixm=HADAPTMOD(pixc,medpx,hmn,hrnx) 
%! Adapted enhancement of contrast 
%! pixc 
= image array (8-bit, gray) 
%! medpx = median(pixc) 
%! hrnin, hrnax = limits for modifying contrast 
%!==============================================! 
[nr,nc]=size(pixc); pixm=ones(nr,nc); 
thetal=(l-medpx)/(hmn-medpx); 
theta2=(256-medpx)/(hrnx-medpx); 
Ct=ones (nr,nc) *medpx; 
idxl=find(pixc<=medpx); idx2=find(pixc>medpx); 
pixm(idxl)=Ct(idxl)+thetal*(pixc(idxl)-Ct(idxl)); 
idx=find(pixm<l); pixm(idx)=l; 
pixm(idx2)=Ct(idx2)+theta2*(pixc(idx2)-Ct(idx2)); 
idx=find(pixm>256); pixm(idx)=256; 
return 
Figure 5.36 - Modified image 
Local modifications 
The presence of very light or very dark areas in an image restricts other areas 
making them set at lower code levels. Image processing software offers the 

230 Digital Signal and Image Processing using MATLAB® 
possibility of globally changing this distribution interactively (applying distri-
bution curves to the histogram). Nevertheless, this remains a global solution 
like the previous ones. 
In practice, avoid modifying areas too much at the expense of others. Try 
to make local changes of the contrast. The principle of this is explained in 
Figure 5.37. 
Figure 5.37 - An image to be modified 
A low-pass filter (Gaussian low-pass filter) was applied to the image in 
Figure 5.37. The transitions are diminished. Then the difference between the 
original and the filtered image is used to produce the final image (Figure 5.38). 
• 
• 
• 
Figure 5.38 - Results with enhancement on the transitions 
The effect of these operations can be viewed by looking at the gray levels 
on one of the manipulated image lines. Each transition is best underlined: the 
lighter side is darkened while the darker side is lightened (Figure 5.39). 

Chapter 5 - An Introduction to Image Processing 231 
200 ,----~--~--~--_. 
PIp (x, y) 
150 - ---- ---- ---------------
150 
100 r=- ---- ---- -'-------:---:..:.:-
100 
50 ------ ------
------~------
50 
o ~~ _ __.l::==:±::::''____J 
0'-------'----'----'------' 
o 
200 
400 
600 
800 
0 
200 
400 
600 
800 
50,,--~-.~--~---. 
250 ,----~--~--~---. 
200-1~-
' ------
'-
-
o 
~~~ \ ---- ,--
------,-
: ~
: 
50 -
----~----
-
------~------
, 
, 
o ------~------
-----~
T
---
, 
, 
- 50 O'--------:--'-c------'-c----:-'-,----:-' 
- 50 '----__ 
-L.,-__ 
---'-,-__ -'-__ -:-' 
800 
0 
200 
400 
600 
800 
p(x,y) + (p(x,y) - Plp(X,y)) 
Figure 5.39 - Modification effect 
Exercise 5.13 (Local contrast changes) (see p. 454) 
Applying the method of local modification to the image in Figure 5.33, write 
a program: 
1. which displays the original image, 
2. which displays the image that has had the local changes applied, 
3. and finally, the end result of the adaptive modification. 
In the case of color images, direct application of this principle to each map 
may cause the appearance of undesired colors. This is the reason why it is 
advisable to work with codings that show a component of brightness. This is 
the map that was then applied to the method. 
5.6.7 Morphological filtering of binary images 
A morphological filtering is a filtering that uses min and max operations. This 
can be symbolized as follows: 
Pk ,l = F(E(P)) 
(5.37) 
where P is an image, E(P) a portion of the image extracted using a window 
E, and F is a logical operation applied to pixels isolated by the window E. 
The following function, called erosion, illustrates the process 5.37 applied to a 
binary image when the min operation amounts to a logical AND: 

232 Digital Signal and Image Processing using MATLAB® 
function ppx=erosion(block,mtool) 
%!===================================================! 
%! SYNOPSIS: ppx=EROSION(block,mtool) 
%! 
block = data block of the same size as the tool 
%! 
mtool = matrix of the tool shape (boolean) 
%! 
Example: [0 1 0;1 1 1;0 1 0] defines a cross. 
%! 
ppx 
= resulting pixel value 
%!===================================================! 
[nr,nc]=size(block); L=nr*nc; 
bb8=uint8(block); mm8=uint8(mtool*255); 
bm=bitand(bb8,mm8); 
ppx=bm(1) ; 
for k=2:L, ppx=bitand(ppx,bm(k)); end 
ppx=double(ppx); 
end 
In this function, the windowing matrix associated with the operator B is 
referred to as the structuring element. It consists of a boolean matrix. A "I" 
indicates a pixel that needs to be taken into account by the logical function 
processing. Hence the processing can be symbolized by: 
Pk ,l = 
n 
Pn ,m 
{n,m:bn.= = l} 
The program exerosion. m illustrates the erosion function call: 
%===== exerosion.m 
% processing using uint8 data 
clear 
mdisp=true; NbLevel=256; 
cmap=flipud([0:NbLevel-1] '/NbLevel)*[1 1 1]; 
load exerosion, [nlO,ncO]=size(pixc); 
if mdisp 
end 
subplot(131); imagesc(pixc); axis('image') 
colormap(cmap); 
%===== defining the tool 
mtool=ones(3,3); 
[nr,nc]=size(mtool); 
Nrowu=fix(nr/2); Nrowb=Nrowu; 
Ncoll=fix(nc/2); Ncolr=Ncoll; 
if rem(nr,2)==0, Nrowu=Nrowu-1; end 
if rem(nc,2)==0, Ncoll=Ncoll-1; end 
%===== the image must be coded between 0 and NbLevel-1 
pixc=[ones(nlO,Ncoll) pixc ones(nlO ,Ncolr)]; 
pixc=[ones(Nrowu,ncO+nc-1);pixc;ones(Nrowb,ncO+nc-1)]; 
ppxe=zeros(nlO+nr-1,ncO+nc-1); ppxd=ppxe; 
%===== 
for nl=Nrowu+1:nlO+Nrowu 

end 
Chapter 5 - An Introduction to Image Processing 233 
for nc=Ncoll+1:ncO+Ncolr 
blk=pixc(nl-Nrowu:nl+Nrowb,nc-Ncoll:nc+Ncolr); 
end 
ppxe(nl,nc)=erosion(blk,mtool); 
%===== 
ppxd(nl,nc)=dilation(blk,mtool); 
%===== 
if mdisp 
end 
subplot (132) ; 
imagesc(ppxe(Nrowu+1:nlO+Nrowu,Ncoll+1:ncO+Ncoll)); 
axis ( I image I ) 
subplot (133) ; 
imagesc(ppxd(Nrowu+1:nlO+Nrowu,Ncoll+1:ncO+Ncoll)); 
axis ( I image I ) 
In this program, the structuring element is a (3 x 3) square. Its execution is 
particularly slow. MATLAB® is not well suited for this type of processing com-
prising many loops. The best method would once again be to write a dedicated 
" . mex" function. The image toolbox, of course, provides such functions. 
If, in the erosion.m function, the AND (n) function is replaced with an 
OR (U) function, the result is a dilation function. This means we are dealing 
with the implementation of the max function for binary images. 
function ppx=dilation(block,mtool) 
%!===================================================! 
%! SYNOPSIS: ppx=DILATION(block,mtool) 
! 
%! 
block = data block of the same size as the tool 
%! 
mtool = matrix of the tool shape (boolean) 
%! 
Example: [0 1 0;1 1 1;0 1 0] defines a cross. 
%! 
ppx 
= resulting pixel value (double) 
%!===================================================! 
[nr,nc]=size(block); L=nr*nc; 
bb8=uint8(block); mm8=uint8(mtool*255); 
bm=bitand(bb8,mm8); 
ppx=bm(1) ; 
for k=2:L, ppx=bitor(ppx,bm(k)); end 
ppx=double(ppx); 
end 
Figure 5.40 illustrates the respective effects of erosion and dilation. In the 
case of erosion, any pattern not covered by the window disappears. The con-
tours of the objects in the foreground are "eroded". Dilation, on the contrary, 
emphasizes the image's details by "increasing" their size. 
5.7 JPEG lossy compression 
The JPEG format (Joint Photographic Experts Group) for coding image files is 
widely used because of the compression rates it can achieve without significant 

234 Digital Signal and Image Processing using MATLAB® 
20 
40 
60 
80 
100 
120 
Figure 5.40 - Effects of erosion and dilation: original, eroded and dilated images 
from left to right respectively 
quality loss. We are going to construct the functions of this coding, without 
trying, however, to construct the final binary flux. 
The idea behind this coding has to do with the use of the discrete cosine 
transform, or DCT. 
5.7.1 
Basic algorithm 
The JPEG compression (lossy compression) algorithm can be very briefly 
summed up as follows [15]: 
- the image is divided in blocks of 8 by 8 pixels, to which a DCT is applied 
(the blocks are read line by line, from top to bottom and from left to 
right). The basic process implies that the levels associated with each 
pixel are 8 bit coded. To make things simpler, we will assume that the 
images we are going to process are given in "levels of gray"; 
- the 64 coefficients of the DCT are quantified (rounded); 
- the "mean value" (DCT value at the frequency 0) is subtracted from the 
same term of the next block; 
- the 63 other terms are read in "zigzags" (Figure 5.41); 
- the sequence of the obtained values is coded (Huffman entropic coding); 
- each non-zero coefficient is coded by the number of zeros preceding it, 
the number of bits needed for its coding, and its value. The coding rules 
are imposed by the [33] standard. 
We assume that we will keep the floating-point representation coding. We 
will not try to optimize the size of the coded DCTs. 

Chapter 5 - An Introduction to Image Processing 
235 
P(iJ) -+-r'-'---c~,* 
Figure 5.41 - Reading of the DCT coefficients 
5.7.2 Writing the compression function 
Writing the DCT calculation and quantification functions 
Consider an (8 x 8) array of pixels p(x,y) (x,y E {O, .. . , 7}), the DCT's ex-
pression for u, v E {O, . . . ,7}: 
7 
7 
P( 
) 
1 C( )C( )"" ( 
) 
(2x + l)7ru 
(2y + l)7rv (5.38) 
u,v = 4" 
U 
v ~ ~ p x, y cos 
16 
cos 
16 
x=Oy=O 
with 0(0) = 
~ and O(k) = 1 for k = 1 . . . 7. 
Once the coefficients are 
obtained, the array is weighted and quantified: 
P(U, v) 
Pq(u,v) =round Q b( 
) 
ta u,v 
where Qtab is a quantification table for chrominance included with the standard 
as an example. It is supposed [35] to provide good results for the type of coding 
performed here and for most images commonly dealt with. 
The following initialization function returns the Qtab table as well as the 
indices used for the "zigzag" reading of the DCT array: 
function [Qtab,zig,zag]=initctes 
%!===============================================! 
%! Init. of the constants for the JPEG algorithm ! 
%!===============================================! 
global mNORM mY V mUX 
mUX = cos([0:7] '*(2*[0:7]+1)*pi/16); 
mY V = cos((2*[0:7] '+1)*[0:7]*pi/16); 
mNORM = [1/2 ones(1,7)/sqrt(2); ones(7,1)/sqrt(2) ones(7,7)]/4; 
Qtab=[16 11 10 16 24 40 51 61; 
12 12 14 19 26 58 60 55; 
14 13 16 24 40 57 69 56; 
14 17 22 29 51 87 80 62; 

236 Digital Signal and Image Processing using MATLAB® 
18 22 37 56 68 109 103 77; 
24 35 55 64 81 104 113 92; 
49 64 78 87 103 121 120 101; 
72 92 95 98 112 100 103 99]; 
zig=[l 9 2 3 10 17 25 18 .. . 
11 4 5 12 19 26 33 41 . . . 
34 27 20 13 6 7 14 21 .. . 
28 35 42 49 57 50 43 36 .. . 
29 22 15 8 16 23 30 37 . . . 
44 51 58 59 52 45 38 31 .. . 
24 32 39 46 53 60 61 54 .. . 
47 40 48 55 62 63 56 64]; 
zag=zigC64:-1:1); 
return 
Exercise 5.14 (Writing basic functions) (see p. 454) 
1. Write the calculation function of the DCT using the vectors mNORM, mY V 
and mUX, which will be declared global mNORM mY V mUX and initialized 
with the initctes.m function; 
2. write the quantification function; 
3. check the processing using the following data [1] . 
The data we are working with are coded on one byte, a value between 0 
and 255, that you need to bring back between -128 and 127: 
%===== dataex.m 
pix=[139 144 149 153 155 155 155 155; 
144 151 153 156 159 156 156 156; 
150 155 160 163 158 156 156 156; 
159 161 162 160 160 159 159 159; 
159 160 161 162 162 155 155 155; 
161 161 161 161 160 157 157 157; 
162 162 161 163 162 157 157 157; 
162 162 161 161 163 158 158 158] ; 
The result has to be: 
15 
0 
-1 
0 
0 
0 
0 
0 
-2 
-1 
0 
0 
0 
0 
0 
0 
-1 
-1 
0 
0 
0 
0 
0 
0 
-1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
COMMENT: the quantification table is usually associated with a quality 
factor Fq. The previous table corresponds to Fq = 50 %. The following function 
can be used to generate tables for other values of Fq : 

Chapter 5 - An Introduction to Image Processing 
237 
function [Qtab]=TabQuantif(Fq) 
%!======================================! 
%! SYNOPSIS: [Qtab]=TABQUANTIF(Fq) 
%! 
Fq 
= quality factor (0 to 100) ! 
%! 
Qtab = weighting table 
%!======================================! 
if nargin<1 , Fq=50 ; end 
%===== table for a quality factor = 50 
Qtab=[16 11 10 16 24 40 51 61 
12 12 14 19 26 58 60 55 
14 13 16 24 40 57 69 56 
14 17 22 29 51 87 80 62 
18 22 37 56 68 109 103 77 
24 35 55 64 81 104 113 92 
49 64 78 87 103 121 120 101 
72 92 95 98 112 100 103 99]; 
%===== 
if (Fq<50) 
scal = 5000/Fq; else scal = 200 - Fq*2; 
end 
Qtabnew=floor(((Qtab.*s cal)+50)./100); 
idz=find(Qtabnew<=O) ; Qtabnew(idz)=ones(size(idz)); 
idz=find(Qtabnew>255); Qtabnew(idz)=255*ones(size(idz)) ; 
Qtab=Qtabnew ; 
return 
Exercise 5.15 (Writing the compressed frame) (see p. 457) 
1. Using the functions written in exercise 5.14, write the function that cre-
ates the compressed frame for one block. The starting mean value is 
assumed to be zero. For the previous exercise, the result , with some 
comments, should be: 
%==== Test block 
5 
number of bits used to code the difference 
15 
diff . with the previous block's mean 
(0 in this case) 
1, 2,-2 
1 zero before the -2 , which is 2 bit 
0,1 , -1 
no zero before the -1 which is 1 bit 
0,1 , -1 
idem 
0,1 , -1 
idem 
2,1 , -1 
2 zeros before the -1 which is 1 bit 
0,1 ,-1 
no zero before the -1 which is 1 bit 
0,0 
there is nothing left but zeros 
coded 
coded 
coded 
coded 
Do not calculate, for now, the number of bits needed for coding each of 
the DCT's coefficients. We will assume it is the same for all of them, and 
that its value is 17. 

238 Digital Signal and Image Processing using MATLAB® 
Save the compressed data to the file unbloccode. dat, but after having 
added at the beginning of the file the number of line blocks and of column 
blocks as follows: 
fid=fopen('unbloccode.dat', 'w'); 
fwrite(fid,nby, 'integer*1'); 
fwrite(fid,nbx, 'integer*1'); 
% Writing the compressed data 
for k=1: ... 
fwrite(fid, ... , 'integer*1'); 
end 
fclose(fid); 
2. apply the obtained program to the test image. Save the compressed data 
to the file imgtstcode. dat. 
5.7.3 Writing the decompression function 
Inverse DCT 
The inverse DCT, referred to as the ICDT, is given by 5.39: 
7 
7 
( 
) _ ~ '" '" P( 
)C( )C() 
(2x + l)7ru 
(2y + l)7rv (5.39) 
P x, Y -
4 ~ ~ 
U,v 
U 
v cos 
16 
cos 
16 
u=Ov=O 
Exercise 5.16 (Decompression) (see p. 458) 
1. Using the Qtab table given on page 235, write the "dequantization func-
tion" of the DCT coefficients; 
2. write the inverse DCT function; 
3. test the "decompression" operation by applying it to the previously used 
test block which is coded as follows: 
II 
A=[1, 1,17,15,1,17,-2,0,17,-1,0,17,-1,0, 
17,-1,2,17,-1,0,17,-1,0,0] 
The first two terms indicate that there is only one block; 
4. apply the program to the file imgtstcode. dat obtained in exercise 5.15 
(the result obtained with the test image that was chosen is shown in 
Figure 5.42). 

Chapter 5 - An Introduction to Image Processing 239 
Figure 5.42 - Comparing original images with images obtained by coding and de-
coding for a quality factor of ~ 30% 


Part II 
Random Signals 


Chapter 6 
Random Variables 
6.1 
Random phenomena in signal processing 
In many practical circumstances, the phenomena observed show important vari-
ations, when in fact the relevant information itself has not changed. Thus, if 
you record the signals obtained when pronouncing several times the sound 
"A", a simple observation will tell you that all the recordings are different even 
though they all sound basically the same. It is neither easy nor relevant to 
try to find a deterministic equation to describe the evolution in time of such a 
phenomenon. The model used to describe this variability is based on the con-
cept of random variables (r.v.), defined by the Probability Theory. A common 
misuse of language consists of saying that this is a random phenomenon. 
Another example of a random phenomenon is the background noise heard 
with radio reception. It seems difficult to describe this noise without using sta-
tistical characteristics. In the complete chain of communications, an example 
we will come back to later, every device, as well as the transmission medium, 
causes background noise. But with such a system, this is not the only "source 
of randomness". From the receiver's point of view, the message itself must also 
be considered random. 
In fact, any device or physical phenomenon has a random part to it. De-
ciding how important that random part is to the system is the only factor in 
determining what type of model is used to describe the phenomena. If the 
amount of information that we cannot have knowledge of is negligible, then we 
will choose a deterministic approach. The evolutionary models this leads us to 
are comprised of differential or recursive equations, analytical expressions, etc. 
In the opposite case, we need to use probabilistic models to try to represent 
the variability of the observed signal with a time-indexed sequence of random 
variables. Each of the random variable describes the uncertainties related to 
the phenomenon at a given time. A family of random variables is called a ran-

244 Digital Signal and Image Processing using MATLAB® 
dom process. They will be studied in Chapter 7, but we will first give a quick 
overview of the main properties of random variables. 
6.2 
Basic concepts of random variables 
Without describing in detail the formalism used by the Probability Theory, we 
will simply remind the reader that a random variable is an application that 
associates a numerical value with each possible outcome of a trial. 
A familiar image is the presence of values between 1 and 6 in a trial con-
sisting of rolls of a dice. However, to be comfortable enough with probabilistic 
tools, we need to go beyond this simple definition. This is why we advise the 
reader to consult some of the many books with authority on the subject [10, 3j. 
From a practical point of view, it is often enough to distinguish two situ-
ations, that is whether the set of possible values of the random experiment is 
discrete or continuous. The number of people waiting in a line is an example 
of the discrete situation: the only possible values are zero or positive integers. 
Whereas taking down the speeds of vehicles on a road is an example of a contin-
uous random variable: this time, the possible values are real numbers between 
o and 65 mph. 
Definition 6.1 (Discrete random variable) 
A random variable X is said to be discrete if the set of its possible values is, at 
the most, countable. If {ao, . . . ,an , . .. }, where n E N, is the set of its values, 
the probability distribution (p.d.) of X is characterized by the sequence: 
px(n) = Pr(X = an) 
(6.1) 
representing the probability that X is equal to the element an. These values are 
such that 0:::; px(n) :::; 1 and Ln20 px (n) = l. 
This leads us to the probability for the random variable X to belong to the 
intervalja, bj . It is given by: 
Pr(X Eja, b]) = L n20 px (n) 1(an Eja, b]) 
The function defined for x E lR. by: 
Fx(x) 
Pr(X :::; x) = L{
n:an~X} 
px(n) 
L n20 px (n) 1(an Ej- oo,x]) 
(6.2) 
is called the cumulative distribution function (cd!) of the random variable X. It 
is a monotonic increasing function, and verifies Fx( -(0) = 0 and Fx( +(0) = l. 
Its graph resembles that of a staircase function (see Figure 6.1), the jumps of 
which are located at x-coordinates an and have an amplitude of p x (n). 

Chapter 6 - Random Variables 
245 
px(n) 
--------~------
__ ~----_+----~--------~x 
Gn 
Figure 6.1 - Cumulative distribution function 
Definition 6.2 (Two discrete random variables) 
Let X 
and Y 
be two discrete random variables, 
with possible values 
{ao, . . . , an, ... } and {bo, . .. ,bk, . .. } respectively. 
The joint probability dis-
tribution is characterized by the sequence of positive values: 
pXy(n, k) = Pr(X = an, Y = bk) 
(6.3) 
with 0 :S PXy(n, k) :S 1 and 2:n20 2:k2o pxy(n, k) = l. 
Pr(X = an) Y = bk) represents the probability to simultaneously have X = 
an and Y = bk . This definition can easily be extended to the case of a finite 
number of random variables. 
Property 6.1 (Marginal probability distribution) Let X 
and Y 
be 
two discrete random variables, with possible values {ao, ... , an, ... } and 
{bo, ... , bk, ... } respectively, and with their joint probability distribution char-
acterized by pxy(n, k). We have: 
+00 
px(n) 
Pr(X = an) = L
Pxy(n, k) 
(6.4) 
k=O 
+00 
py(k) 
Pr(Y = bk) = LPxy(n, k) 
n=O 
px(n) and py(k) denote the marginal probability distribution of X and Y 
respectively. 
Definition 6.3 (Continuous random variable) 
A random variable is said to be continuous if its values belong to lR. and if, for 
any real numbers a and b, the probability that X belongs to the intervalja, bj 
is: 
Pr(X Eja, b]) = lb Px(x )dx 
(6.5) 

246 Digital Signal and Image Processing using MATLAB® 
where px(x) is a function that must be positive or equal to zero such that 
r ~:: 
Px(x)dx = 1. Px(x) is called the probability density function (pdf) of X. 
The function defined for any x E IR by: 
Fx(x) = Pr(X ~ x) = [ Xoo Px(u)du 
(6.6) 
is called the cumulative distribution function (cdfJ of the random variable X. It 
is a monotonic increasing function and it verifies Fx (-00) = 0 and Fx (+00) = 
1. Notice that Px(x) also represents the derivative of Fx(x) with respect to x. 
Definition 6.4 (Two continuous random variables) 
Let X and Y be two random variables with possible values in IR x IR. They are 
said to be continuous if, for any domain ~ of IR2, the probability that the pair 
(X, Y) belongs to ~ is given by: 
Pr((X, Y) E ~) = J i PXy(x, y)dxdy 
(6.7) 
where the function PXy(x, y) ~ 0, and is such that: 
J 1.2 PXy(x, y)dxdy = 1 
PXy(x, y) is called the joint probability density function of the pair (X, Y). 
Property 6.2 (Marginal probability distributions) Let X and Y be two 
continuous random variables with a joint probability distribution characterized 
by p Xy (x, y). 
The probability distributions of X and Y have the following 
marginal probability density functions: 
Px(x) 1
+00 
-00 PXy(x, y)dy 
(6.8) 
py(y) 
1
+00 
-00 PXy(x, y)dx 
The marginal probability density functions of X and Yare referred to as 
Px(x) and py(y). 
An example involving two real random variables (X, Y) is the case of a 
complex random variable Z = X + jY. 
It is also possible to have a mixed situation, where one of the two variables 
is discrete and the other is continuous. This leads to the following: 

Chapter 6 - Random Variables 247 
Definition 6.5 (Mixed random variables) 
Let X be a discrete random variable with possible values {ao, ... , an, . . . } and 
Y a continuous random variable with possible values in lR. For any value an, 
and for any real numbers a and b, the probability: 
Pr(X = an, Y Eja, b]) = lb pXy(n, y)dy 
(6.9) 
where the function pXy(n, y), with n E {O, ... , k, . . . } and y E JR., is ::::: 0 and 
verifies L n:::o i lR Pxy(n, y)dy = 1. 
Definition 6.6 (Two independent random variables) 
Two random variables X and Yare said to be independent if and only if 
their joint probability distribution is the product of the marginal probability 
distributions. This can be expressed (Jor the previous cases only): 
- For two discrete random variables: 
pxy(n, k) = px(n)py(k) 
- For two continuous random variables: 
PXy(X, y) = Px(x)py(y) 
- For two mixed random variables: 
pxy(n, y) = px(n)py(y) 
where the marginal probability distributions are obtained with formulae 6.4 and 
6.8. 
We wish to insist on the fact that, knowingpxy(x,y), we can tell whether 
or not X and Yare independent. To do this, we need to calculate the marginal 
probability distributions and to check that PXy(x, y) = px(x)py(y). If that is 
the case, then X and Yare independent. 
The following definition is more general. 
Definition 6.7 (Independent random variables) The random variables 
(Xl, ... , Xn) are jointly independent if and only if their joint probability dis-
tribution is the product of their marginal probability distributions. This can be 
expressed: 
(6.10) 
where the marginal probability distributions are obtained as integrals with respect 
to (n - 1) variables, calculated from Px,x2 ... x n (Xl, X2, "" xn) . 

248 Digital Signal and Image Processing using MATLAB® 
For example, the marginal probability distribution of Xl has the expression: 
px, (xd = J ... J 
px,x2 .. x n (Xl, X2,··· ,xn)dx2 ... dXn 
"-v-"" 
jRn- l 
In practice, the following result is a simple method for determining whether 
or not random variables are independent: 
If Px,x2 ... x n (Xl, X2, ... ,xn) is a product of n positive functions of the 
type h(xdh(x2) ... fn(xn), then the variables are independent. 
It should be noted that if n random variables are independent of one an-
other, it does not necessarily mean that they are jointly independent. 
Definition 6.8 (Mathematical expectation) 
Let X be a random variable and f (x) a function. The mathematical expectation 
of f(X) (respectively f(X, Y)) is the value, denoted by lE {f(X)} (respectively 
lE {f(X, Y)}), defined: 
- for a discrete random variable, by: 
lE {f(X)} = L f(an)px(n) 
n2:0 
- for a continuous random variable, by: 
lE {f(X)} = ~ f(x)Px(x)dx 
- for two discrete random variables, by: 
lE {f(X, Y)} = L L f(an, bk)pxy(n, k) 
n2:0k2:0 
- for two continuous random variables, by: 
lE {f(X, Y)} = ~~ 
f( x, y)PXy(x, y)dxdy 
Property 6.3 If {Xl, X 2, . . . , Xn} are jointly independent, then for any 
integrable functions h, 12, ... , fn: 
(6.11) 

Chapter 6 - Random Variables 249 
Definition 6.9 (Characteristic function) 
The characteristic function of the probability distribution of the random vari-
ables Xl, ... , Xn is the function of (UI' ... ,un) E lR,n defined by: 
¢ X""Xn (UI , ... ,un) = IE { eju,x, + .. +junXn } = IE {IT ejukXk } 
k = l 
(6.12) 
Because lejuX I = 1, the characteristic function exists and is continuous even 
if the moments do not exist. The Cauchy probability distribution, for example, 
the probability density function of which is Px (x) = I/1f(1 +x2 ), has no moment 
and has the characteristic function e- 1ul . Notice that l¢x, ... Xn(UI, ... ,un) 1 :::; 
¢x(O, ... ,0) = 1. 
Theorem 6.1 (Fundamental) (Xl"'" Xn) are independent if and only if 
for any point (UI ' U2 , ... , un) oflR,n: 
n 
¢x,.Xn(UI, ... ,Un) = II ¢Xk(Uk) 
k=l 
Notice that the characteristic function ¢Xk(Uk) of the marginal probability 
distribution of X k can be directly calculated using 6.12. We have ¢Xk(Uk) = 
IE {ejUkXk} = ¢x,,,,xn (0, . .. , 0, Uk, 0, ... , 0). 
EXAMPLE 6.1 (First calculations) 
Let X be a random variable with possible values in {O, I} with Pr(X = 0) = 
Po ::::: 0, Pr(X = 1) = PI ::::: ° and Po + PI = 1. Calculate IE {X}, IE {X2}, 
IE {cos(1fX)} and ¢x(u). 
HINTS: we get: 
IE {X} = ° 
x Po + 1 X PI = PI and IE { X2} = 02 X Po + 12 X PI = PI 
then: 
IE {cos( 1f X)} = cos(O) x Po + cos( 1f) X PI = Po - PI 
and finally: 
Definition 6.10 (n-th order moment) 
The n-th order moment is the mathematical expectation of the function f(x) = 
xn . 

250 Digital Signal and Image Processing using MATLAB® 
Definition 6.11 (Mean, variance) 
The mean of the random variable X is defined as the first order moment, that 
is to say lE {X} . If the mean is equal to zero, the random variable is said to be 
centered. The variance of the random variable X is the quantity defined by: 
The variance is always positive, and its square root is called the standard 
deviation. 
The standard deviation can be interpreted as a measure of the random 
variable's fluctuations around its mean: the higher it is, the more the values of 
X are spread out around lE {X}. 
Property 6.4 (Chebyshev inequality) 
Let X be a random variable, with lE {X} as its mean and var(X) as its variance. 
Then for any 5> 0: 
Pr(IX-lE{X}I2:5) 
< 
Pr (lE {X} - 5 ::; X ::; lE {X} + 5) 
var(X) 
52 
(6.13) 
Inequality 6.13 means that the probability for X to deviate from its mean 
by ±5 decreases when the variance decreases. 
As an exercise, we are going to show that, for any constants a and b: 
lE {aX + b} 
var(aX + b) 
alE{X}+b 
a2var(X) 
(6.14) 
(6.15) 
6.14 is a direct consequence of the integral's linearity. We assume that Y = 
aX + b, then var(Y) = lE {(Y - lE {y})2}. By replacing lE {Y} = alE {X} + b, 
we get var(Y) = lE {a2(X - lE {X} )2} = a2var(X). 
A generalization of these two results to random vectors (their components 
are random variables) will be given by property 6.6. 
Definition 6.12 (Covariance, correlation) 
Let (X, Y) 1 be two random variables. The covariance of X and Y is the quantity 
defined by: 
cov(X, Y) 
lE {(X - lE {X} )(y* -lE {Y*})} 
lE {XY*} - lE {X} lE {Y*} 
(6.16) 
1 Except in some particular cases, the random variables considered from now on will be 
real. However, the definitions involving the mean and the covariance can be generalized with 
no exceptions to complex variables by conjugating the second variable. This is indicated by 
a star (*) in the case of scalars and by the exponent H in the case of vectors. 

Chapter 6 - Random Variables 251 
X and Yare said to be uncorrelated if cov(X, Y) = 0 that is to say if 
lE {XY*} = lE {X} lE {Y*}. The correlation coefficient is the quantity defined 
by: 
p(X Y) = 
cov(X, Y) 
, 
Jvar(X) Jvar(Y) 
(6.17) 
Applying the Schwartz inequality gives us -1 ::; p(X, Y) ::; 1. 
Definition 6.13 (Mean vector and covariance matrix) 
Let {Xl, . . . , Xn} be n random variables with the respective means lE {Xi} . The 
mean vector is the n dimension vector with the means lE {Xi} as its components. 
The n x n covariance matrix C is the matrix with the generating element 
Gij = COV(Xi' X j ) for 1 ::; i ::; nand 1 ::; j ::; n. 
Matrix notation: if we write 
to refer to the random vector with the random variable X k as its k-th compo-
nent, the mean-vector can be expressed: 
[
lE {Xdj 
lE{X} = 
: 
lE {Xn } 
and the covariance matrix: 
C = lE {(X -lE {X} )(X -lE {X} )H} = lE {XXH} -lE {X} lE {X}H (6.18) 
Notice that the diagonal elements of a covariance matrix represent the re-
spective variances of the n random variables. They are therefore positive. If 
the n random variables are un correlated, their covariance matrix is diagonal. 
Property 6.5 (Positivity of the covariance matrix) 
Any covariance matrix is positive, meaning that for any vector a, we have 
aHCa 2': o. 
To obtain this result, consider for any sequence of complex values 
{al , . . . , aN} the random variable Y = L:
~= l ak(Xk - lE {Xd) . We have, of 
course, lE {1Y12} 2': 0 (because it is the mathematical expectation of a positive 

252 Digital Signal and Image Processing using MATLAB® 
random variable). We will now express lE {1Y12}. We get: 
lE {~ak(Xk 
- lE {Xd) %;1 a:n(Xm - lE {Xm})* } 
N 
N 
L L aklE{(Xk -lE{Xk})(Xm -lE{Xm})*}a:n 
k= l m = l 
N 
N 
L L akck,ma:n = aH Ca 2: 0 
k= l m = l 
Property 6.6 (Linear transformation of a random vector) 
Let {Xl, ... ,Xn } be n random variables with lE{X} as their mean vector and 
C x as their covariance matrix, and let {Yl , ... , Yq} be q random variables 
obtained by the linear transformation: 
where A is a matrix and b is a non-random vector with the adequate sizes. We 
then have: 
lE {Y} 
C y 
A lE {X} + b 
ACxAH 
The first expression is a direct consequence of the integral's linearity. It 
leads to Y -lE {Y} = A(X -lE {X}). We now lay down the second expression: 
C y 
lE {(Y -lE {Y} )(Y -lE {Y} )H} 
A lE {(X -lE{X})(X -lE{X})H}AH = ACxAH 
Definition 6.14 (White sequence) Let {Xl, ... , Xn} be a set ofn random 
variables. 
They are said to form a white sequence if var(Xi ) = (J"2 and if 
COV(Xi' X j ) = 0 for i i= j. Hence their covariance matrix can be expressed: 
C = (J"2In 
where I n is the n x n identity matrix. 
Property 6.7 (Independence =} non-correlation) 
The random variables {Xl, ... , Xn} are independent, then un correlated, and 
hence their covariance matrix is diagonal. Usually the converse statement is 
false. 

Chapter 6 - Random Variables 253 
6.3 Common probability distributions 
6.3.1 
Uniform probability distribution on (a, b) 
Definition 6.15 A random variable X is said to be uniformly distributed on 
(a, b) with b > a if its probability density function has the expression: 
Px(x) = (b _ a)-ll(x E (a, b)) = { 
(b - a)-l 
for x E (a, b) 
o 
otherwise 
(6.19) 
Notice that the set of all possible values of X is reduced to the interval 
(a, b) , and that the probability of X belonging to an interval (c, d) C (a, b) is 
equal to (d - c)/(b - a), and is therefore proportional to the interval's length. 
Its probability density function is shown in Figure 6.2. It is constant in the 
interval ( a, b). 
b-a 
r
_~X(X) 
-
'---1 ----, 
a 
b 
'» x 
Figure 6.2 - Probability density of the uniform probability distribution 
You can check as an exercise that its mean, its second-order moment and 
its variance are given respectively by: 
lE {X} = Ib _ x_ dx = a + b 
a b - a 
2 
lE { X2} = 
_X_ dx = a + a + 
I
b 
2 
2 
b 
b2 
a b - a 
3 
An example of quantities that can be described by a uniformly distributed 
random variable is the errors that are made when, in calculations, numbers are 
rounded to D decimal places. When a large number of operations is performed, 
it can be assumed that the errors behave like random variables uniformly dis-
tributed between the values _lO-D /2 and lO-D /2. We will see on page 268 
how this random variable model is used to describe uniform quantization noise. 

254 Digital Signal and Image Processing using MATLAB® 
6.3.2 Real Gaussian random variable 
Definition 6.16 A random variable X is said to be Gaussian, or normal, if 
all its values belong to lR and if its characteristic function (see definition 6. g) 
has the expression: 
where m is a real parameter and a is a positive parameter. If a i- 0, it can be 
shown that the probability distribution has a probability density function with 
the expression: 
1 
((x-m)2) 
px(x) = aV2ii exp -
2a2 
(6.20) 
We also check that its mean is equal to m and its variance to a 2 . 
In physics, many phenomena, which are the combination of a multitude 
of microscopic effects, are distributed on the macroscopic scale according to 
a Gaussian probability distribution: this is the case of background noise in 
receptors. The Gaussian nature of these phenomena is a consequence of the 
central limit theorem [10]. Given {X(k)} is a sample of N i.i.d. random vari-
ables (/-1 = lE {X(k)} , a 2 = var(X(k))), a version of the central limit theorem 
states that the law of the arithmetic mean (L
~= l X (k)) / N converges, when 
N -+ +00, to a Gaussian law (/-1, ( 2 ). 
Figure 6.3 shows the shape of the probability density function for the Gaus-
sian random variable. 
px(x) 
0.2 
, 
0.16 
I 
I 
I 
I 
--------,---------~---------
, ---
-----T----- ---r--------1---------
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
:: 
::: 
________ J _________ ~ ________
_________ 1 _________________ J ________ _ 
I 
I 
I 
I 
I 
I 
0.12 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
:: 
::: 
________ ~---------L-----
___ I _________ + _________ ~--
_____ , ________ _ 
I 
I 
I 
I 
I 
I 
0.08 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
--------
~
---------
r
-
----------------
T
---------
~
------
-
~
-
0.04 
I 
I 
I 
I 
, 
" 
, 
" 
o~----=
' ~--_+----~------~
' ----~
' ----~--~~---
X 
m-a 
m 
m+a 
m -'- 2a 
4a 
m+2a 
Figure 6.3 - Probability density function of the Gaussian random variable and 95% 
confidence interval 

Chapter 6 - Random Variables 255 
It can be checked numerically that more than 99.7% of the values belong 
to the interval (m - 30", m + 30"). The interval can then be called a 99.7% 
confidence interval. This leads us to a practical rule called the 3-sigma rule 
for which the probability of "falling outside" this interval is less than 0.3%. 
If we restrict ourselves to a 95.45% confidence interval, we have to choose 
the interval (m - 20", m + 20"). The probabilities corresponding to the given 
confidence intervals can be verified using the function erf (x) which calculates: 
2 
r/V'i 
2 
erf(x/J2) = y7r 10 
e-t dt 
and gives lP' { -XO" < X < xO" } in the Gaussian case 
%===== confintrv.m 
x=[.5: .25:1.5]; cintv=erf(x/sqrt(2)); 
subplot (211) 
plct(x,cintv, '-' ,x,cintv,'c'), grid 
x=[1.5: .1:3.5]; cintv=erf(x/sqrt(2)); 
subplct(212) 
plct(x,cintv, '-' ,x,cintv, 'c'), grid 
The following table gives several values currently used for confidence inter-
vals: 
interval 
% 
interval 
% 
(m-O",m+O") 
68.3) 
(m - 2.580", m + 2.580") 
99 
(m - 1.960", m + 1.960") 
95 
(m - 30", m + 30") 
99.7 
(m - 20", m + 20") 
95.45 
6.3.3 
Complex Gaussian random variable 
In some problems, and particularly in the field of communications, the complex 
notation X = U + jV is used, where U and V refer to two real, Gaussian, 
centered, independent random variables with the same variance 0"2/2. Because 
of independence (definition 6.7), the joint probability distribution of the pair 
(U, V) has the following probability density: 
Puv(u, v) 
1 
(u
2
) 
Pu(u)Pv(v) = 0"y7r exp - 0"2 
1 
( V2 ) 
x --exp --
0"y7r 
0"2 
If we notice that Ixl 2 = u2 + v2 , and if we introduce the notation Px(x) = 
Puv (u, v), we can also write: 
1 
(IXI2) 
px(x) = -
exp --
Jr0"2 
0"2 
(6.21) 

256 Digital Signal and Image Processing using MATLAB® 
Expression 6.21 is called the probability density of a complex Gaussian 
random variable. The word circular is sometimes added as a reminder that the 
isodensity contours are the circles u2 + v2 = constant. 
Note that: 
lE{(U + jV)(U + jV)} = 0 
lE {XX*} = lE {(U + jV)(U - jV)} 
lE {U2} + lE {V2} = a2 
Expression 6.21 deserves a few words of warnings: 
- the argument x is complex; 
- if you compare expressions 6.21 and 6.20 of the probability density of a 
real Gaussian random variable, you will notice the disappearance of the 
factors 2 and of the square root in a 2 . 
6.3.4 Generating the common probability distributions 
MATLAB® has basically two random number generators (other generators are 
provided in the signal toolbox): 
1. the first one generates probability distributions uniformly distributed on 
(0,1) (see definition 6.15). This generator is called by the rand command; 
2. the second is a centered Gaussian probability distribution (see definition 
6.16), with a variance equal to 1. This generator is called by the randn 
command. 
We will assume that when we use these generators several times, the re-
sulting samples correspond to independent random variables. This is why the 
array randn(4, 1000) can be considered as an experiment with a trial length of 
1,000 on 4 independent, centered, Gaussian random variables with a variance 
equal to 1. 
COMMENT: prior to version 4 of MATLAB® the rand ( , un if orm ') or 
rand ( 'normal') initializes the random generator either for a probability dis-
tribution uniformly-distributed on the interval (0,1) or for centered, Gaussian 
probability distribution, with a variance equal to 1. Running the command 
rand(k,c) would then construct a matrix with k lines and C columns made up 
of random numbers of the corresponding probability distribution. The type of 
the currently used distributions could be known with the rand(' dist') com-
mand. In the new versions of MATLAB®, the uniform generator is called with 
the rand command, and the Gaussian generator with the randn command. 
This is the only notation we will be using. 

Chapter 6 - Random Variables 
257 
EXAMPLE 6.2 (Uniform probability distribution on (a, b)) 
We wish to obtain a random variable X with a uniform probability distribution 
on (a, b) , using the MATLAB® function rand which returns a random variable 
with a uniform probability distribution on (0,1). As an exercise, you can show 
that the random variable X = (b - a)U + a, where U is a uniform variable on 
(0, 1), is uniform on ( a, b). The un if ab . m function given below generates N 
values uniformly distributed on (a, b): 
function X=unifab(a,b,N) 
%!==================================================! 
%! Generating a r.v. uniformly distributed on (a,b) 
%! SYNOPSIS: X=UNIFAB(a,b,N) 
%! 
a,b 
interval 
%! 
N 
= number of samples 
%! 
X 
= sequence of samples 
%!==================================================! 
U=randCi,N) ; 
X=(b-a)*U+a; 
return 
In order to obtain 1,000 sample values of a random variable, uniform on 
(-7r, +7r) , type: x=unifab(-pi,pi,1000);. A sample of this type is repre-
sented in Figure 6.4. 
o 
100 
200 
300 
400 
500 
600 
700 
800 
900 
1,000 
Figure 6.4 - 1,000 trials of a random variable uniformly distributed on [-7r, +7r] 
Now type hist (X). The result is a diagram called a histogram, representing 
an estimation of the probability density shown in Figure 6.2. 
EXAMPLE 6.3 (Gaussian variable (m, a 2 )) 
Let Y be a centered Gaussian random variable with a variance of 1. If we 
apply relations 6.14 and 6.15, we can easily verify that the mean of the random 

258 Digital Signal and Image Processing using MATLAB® 
variable X = aY + m is m and that its variance is 172 . We will see, paragraph 
6.3.6, that a direct consequence of the general definition of a Gaussian vector 
is that its Gaussian nature is unchanged by linear transformation. To get a 
5,000 value sample of a Gaussian probability distribution with a mean equal 
to 4 and a variance equal to 7, type: 
%===== histogld.m 
clear; N=5000; m=4; sigma2=7; 
X=sqrt(sigma2)*randn(1,N)+m; 
figure(l); plot(X,'. ') 
lk=O.5; delta=3*sqrt(sigma2); 
[nn,xx]=hist(X,(-delta+m:lk:delta+m)); 
pxchap=nn/(N*lk); figure(2); bar(xx,pxchap) 
CG=1/sqrt(2*pi*sigma2); px=CG*exp(-(xx-m) .-2/(2*sigma2)); 
hold on; plot(xx,px, '0'); hold off; grid 
Notice the use of formula 6.23 in the command pxchap=nn/ (N*lk) to esti-
mate the probability density using the results returned by the hist function. 
The trials are shown in Figure 6.5. The theoretical probability densities (0) 
and the estimated ones (bar chart) are show in Figure 6.6. 
14 ,---____ ~----------~--------~------~---. 
12 -------:-~ ----: ------:.------:------~ --.---.-: -----.-i------~
-
.- ----: --.----
10 -----; -:--.-. --- ~ ---. -.- ~ ------:------ ~ ----_. ~ . --,. -:- ----- ~ -.-----i -.-; - -.-
, 
• 
I 
• 
1 •• 
_ 
e l •• ' 
I I. 
I 
I 
I 
• 
tit.. 
I 
• 
• 
8 --_ .... --:. - ! ~ -.. ~ ---.- , -.Ii -.-.; -. ~
: \
-
! - - ~ -: .!ii; - !.T e - :"-. -.! :; -.-.-••. - - ~- . - - J __ .j e.! _'!I __ .. 
6 :··:·II·f..':···
~ . ;: :-. 
'~
: .. ~,.:.
: ... :-::-:. 
~ ·,t.:f
: : ' ... :~ 
..... 
-;'- -.: ...... {- - ~ ; .. -.... 
~ . -:-1-1- -;--.. - (
-, - ~ , -... ~ ".-" •• - T •• : ...... t.J:~ 
,--.. --• .. 'i~t r, --- ..,. .. 
4 ''':' :.="'. "i>:' ~"""\>',:!;,;\:;~':.)o" 
:':'£ ... '." :;, -:.: .. :" " .... 
~ .... - -,.-,- - .... - , . --, -.. ~ ....... - - -. - ( -If': -'1- . - - ..... ' -,.-,.;a-.. -.-I-.C" -4" "' --''' -e- "~"''''
.!
- -. 
.~; 
·.1:'- -•• /~ 
~~,."t-, 
..... :-.t-r._:~ 
;~, ... ~.!"I 
•• '.::~' 
•• ~ 
~:~ 
••• ;'.:--.:::. 
2 "'.--, - ~ -.--_ ... $l. - -..... t" - ....... "1--• .,--..,-.-"--.,. _.--•.••.. --;1-: ... . ...,f.- - "I-.-.'!T ...... - ., 
••• 1-. ,~ 
•• : :.,\
, r··~ 
.~ ... :" ;. ; .... ::; ... ,.,.: .;" :i::·: ••• -•• :.::;.: .... ;:. 
"._~ 
o . -- - ~
: •.• , :]
~: 
~ - -:: .:'~=;.'?
: ] ~.:~
- _ 
.. J: .. _.} __ ,..~-:;
_ ;~;~;~
_ ! _ 
-2 -.; -
: • 
:
" 
" 
- ~ .~ 
-4 
.. 
o 
100 
200 
300 
400 
500 
600 
700 
800 
900 
1000 
Figure 6.5 - Gaussian probability distribution trials (m = 4, 0"2 = 7) 
EXAMPLE 6.4 (Complex Gaussian random variable) 
To generate a N = 1,000 value sample of a centered, complex, Gaussian random 
variable (see expression 6.21), with a variance of 5, type the following program: 
%===== gcompl.m 
N=1000; varX=5 ; 
U = sqrt(varX/2) * randn(l,N) 
V = sqrt(varX/2) * randn(l,N) 
X = U + j * V ; 
std(X)-2 

Chapter 6 - Random Variables 259 
0.16 
0.14 
0
: 
: 
: 
O
---
: --------;-------~--------, 
, 
0.12 
o 
, 
, 
_ I ______ --~-------~--------
, 
, 
, 
, 
0.1 
----- T -------1--
0.08 
______ --1 __ 
0.06 
0.04 
0.02 
0 -4 
-2 
0 
2 
4 
6 
Figure 6.6 - Histogram of the Gaussian probability distribution (m = 4,0-2 = 7) 
gcompl .m displays the estimation of the standard deviation using the ex-
pression: 
V 1 
1 
N 
std(X) = 
N _ 1 ~~ = l (X(k) - m)2 with m = N L X(k) 
k = l 
6.3.5 
Estimating the probability density 
A theorem inaccurately called the "law of large numbers" states that the proba-
bility for a random variable X to belong to an interval ~ can be approximated, 
if N is large enough, in the following way: 
- consider N independent random variables with the same probability dis-
tribution as X; an experiment is conducted, leading to the trials Xl, ... , 
XN; 
- the number n of values in these trials that belong to ~ is determined; 
- the approximation used for Pr(X E ~) 
is n/N. 
The quantity n/ N is called the empirical frequency. This result can be 
used to estimate the probability density of px(x) at the point X of the random 
variable X which is assumed to be continuous. By definition, we have: 
Pr(X E ~) = i px(u)du 
If ~ is a closed interval chosen small enough around the point x, we have 
px(u) :::::: px(x) and the second member is approximately equal to Px(x) xi! 
where i! refers to the length of~
. 
This leads us to a practical formula for 
estimating p x (x): 
n 
Px(x):::::: -Ni! 
(6.22) 

260 Digital Signal and Image Processing using MATLAB® 
where n is the number of observed points inside the closed interval~. 
Faced 
with a sample of N values, the procedure for estimating the probability density 
in P points can be summed up as follows: 
Steps: 
1. The interval containing the observed values is partitioned in P sub-
intervals h , ... ,Ip with the respective lengths Ji l , ... ,Jip , located around 
the points Xl,"" Xp. Usually, the sub-intervals are chosen so that they 
all have the same length, and so that Xk is placed in the middle of the 
sub-interval h, except possibly for the first and last intervals. 
2. The probability density at the point Xk is estimated by: 
where nk is the number of points in the interval h. 
'Ldlx(Xk)Jik = 1. 
(6.23) 
Note that 
The choice of the value of P is a complex problem. What we can say is that 
P has to be large enough for the probability density to be properly estimated 
but also small enough for the number of points in each interval to remain large. 
P = N I / 3 , for example, would be suitable, since it tends to infinity when N 
tends to infinity, and N / P also tends to infinity when N tends to infinity. 
The MATLAB® function hist implements partly this procedure. The usual 
syntax is [ndel ta xO] =hist eX). In this case, hist automatically chooses ten 
values regularly spread out between the minimum and the maximum of the 
sample X with intervals of the same length. 
The sequence ndel ta returns the number of points of X placed around 
each element of the sequence xO. Thus a list of values for xO can be set as a 
parameter. Expression 6.23 is then used to find an estimation of the probability 
density. 
6.3.6 Gaussian random vectors 
Definition 6.17 (Gaussian vector) Xl , . .. , Xn are said to be n jointly 
Gaussian variables, or that the length n vector [Xl 
Xn]T is Gaussian, 
if any linear combination of its components, that is to say Y = aT X for any 
a = [al 
anjT, is a Gaussian random variable. 
Theorem 6.2 (Probability distribution of a Gaussian vector) 
It can be shown that the probability distribution of a length n Gaussian vector, 

Chapter 6 - Random Variables 261 
with a length n mean vector m and an (n x n) covariance matrix C has the 
characteristic function: 
¢X(Ul, ... ,un) = exp (jmTu - ~uTCu) 
(6.24) 
where u = (Ul, ... ,un)T E ]Etn . Let x = (Xl, ... ,xn)T. Ifdet(C) =F 0, the 
probability distribution's density has the expression: 
1 
( 
1 
T 
- 1 
) 
PX(Xl, ... ,Xn) = 
exp --2(x-m) C 
(x-m) 
(2n)n/2vdet(C) 
(6.25) 
Theorem 6.3 (Gaussian case: non-correlation =} independence) 
If n jointly Gaussian variables are uncorrelated, then they are independent. 
This is because if we replace C = (72 I in expression 6.25, p x (Xl, .. . ,Xn ) = 
PX, (xd· .. PXn (xn ), hence, according to 6.10, the variables are independent. 
Theorem 6.4 (Linear transformation of a Gaussian vector) 
Let [Xl 
xnf be a Gaussian vector with a mean vector mx and a co-
variance matrix C x. The random vector Y = AX + b , where A and bare 
a matrix and a vector respectively, with the ad hoc length, is Gaussian and we 
have: 
my = Amx + b
and C y = ACxAT 
In other words, the Gaussian nature of a vector is untouched by linear 
transformations. 
This result is a consequence of definition 6.17 and of property 6.6. 
Exercise 6.1 (Confidence ellipse) (see p. 460) 
1. PX(Xl, X2) denotes the probability density of a length 2 random Gaussian 
vector, with the mean m and the covariance matrix C , and let: 
where b.(s) is the set of points in the plane defined by: 
the borderline of which in ]Et2 is the ellipse centered on m with the equa-
tion (x - m)TC-l(x - m) = s. 

262 Digital Signal and Image Processing using MATLAB® 
We will now determine the relation between sand a. The ellipse E is 
called the 100a % confidence ellipse of the variable X. 
By making the variable change Y 
= C - l / 2 (X - rn) , show that s 
-2Iog(1- a); 
[ 
]
T 
[ 2.3659 
2. we assume rn = 0 0 ,C = 
-0.3787 -0.3787] 
0.6427 
and a = 0.95. 
Write a program: 
- that generates a length 2 Gaussian sample of N = 200 values, with a 
mean rn and a covariance matrix C , using a centered, white sample 
obtained with y=randn (2, N), 
- that displays the points with the plane coordinates x, as well as the 
ellipse with the equation (x-rn)TC-l(x-rn) = s (use the ellipse 
function, given in example 31, where s = -2Iog(1- a)), 
- that counts the number of points outside the ellipse and compares 
it to the value (1 - a)N. Think of using the find function for the 
condition YI + Y~ > s. 
6.4 Generating an r.v. with any type of p.d. 
Since the only generators MATLAB® provides are rand and randn, you may 
wonder whether it is possible to infer the function that can generate random 
variables with any type of probability distributions. The answer is yes, and one 
solution is given by the inversion of the cumulative distribution function [7]. 
Generating a discrete random variable 
Let X be a discrete random variable, a sample of which we wish to generate. 
Let { ao, al, . .. , an, ... } be the set of its values, px(n) = Pr(X = an) its 
probability distribution and Fx(x) = Pr(X ::; x) its cumulative distribution 
function. Figure 6.7 shows the graph shape of the function Fx(x). Its value in 
x = ak is expressed Fx(ak) = L:~
= opx(n). 
Now consider a random variable U, uniformly-distributed on (0,1), and 
let Y be the random variable obtained from U by inversion of the cumulative 

Chapter 6 - Random Variables 263 
u -----:-- -------,- ----.;-----. 
x 
Figure 6.7 - Cumulative distribution function of a discrete random variable 
distribution function, which we write: 
[O,px(O)[ 
then Y = ao 
[Px(O),px(O) + Px(l)[ then 
Y = al 
HUE 
(6.26) 
[Fx(ak-d, Fx(ak)[ 
then Y = ak 
We will now show that the probability distribution of the obtained random 
variable Y is the very variable X we were looking for. Indeed, if we use the 
fact that U is uniform, we can successively write: 
The variable Y constructed with procedure 6.26 obeys the expected probability 
distribution. 
EXAMPLE 6.5 (Uniform discrete random variable) 
Generate a set of N values obeying a uniform discrete probability distribution 
on {O, ... , N - I}, meaning that Pr(X = k) = liN for 0 ::; k ::; N - 1. Draw 
the histogram of its values. 
HINTS: because the cumulative distribution function is such that 
Fx(k) = kiN for k E {O, . . . , N - I}, expression 6.26 provides us 
with, if U E [kiN, (k + 1)IN [, the value X = kiN, meaning that X 
is simply the integer part of NU. Type: 

264 
Digital Signal and Image Processing using MATLAB® 
%===== histounif.m 
clear; clf; nbp=3000; N=10; U=rand(l,nbp); X=ceil(N*U); 
px=hist(X,(l:N)); bar(px/nbp); grid 
As we expected, the obtained graph matches the uniform probabil-
ity distribution Pr(X = k) = 1/10. 
Exercise 6.2 (Poisson distribution) (see p. 461) 
The random variable X, with possible values in N, has a Poisson distribution 
when: 
(6.27) 
where a refers to a positive quantity called the distribution parameter: 
1. determine the mean and the variance of X; 
2. determine, for kEN, the recurrence relation that gives px(k) as a func-
tion of Px(k - 1), as well as the one that gives Fx(k) = Pr(X ::; k); 
3. using 6.26, write a program that generates a Poisson random variable with 
a parameter a = 5, using a random variable U uniformly-distributed on 
(0, 1). 
4. using the hist function, check the result. 
Theorem 6.5 (Variable change formula) 
Let x = f (u) be a bijective and differentiable function, and let U be a random 
variable, with the probability density Pu (u). Then the random variable X = 
feU) has the following probability density: 
I 
du I 
pu(u) 
pu(g(x)) 
Px(x) = Pu(u) dx = 
I ~~ I = If'(g(x))1 
(6.28) 
where l' (u) = dx / du refers to the derivative of f (u) and where u = g( x) refers 
to the inverse function of x = feu), that is to say such that g(j(u)) = u. 
Generating a continuous random variable 
We now apply 6.28 in the particular case where U is a random variable uni-
formly distributed on (0,1) the probability density of which has the expression 
Pu(u) = l(u E (0,1)). The probability distribution for the random variable 
X = feU) then has the probability density: 
px(x) = I 
~~ Il{9(X) E (0, I)} 
(6.29) 

Chapter 6 - Random Variables 
265 
where u = g(x) represents the inverse of x = f(u). For g(x), we will choose the 
function F(x), where F(x) is precisely the cumulative distribution function of 
the random variable we want to generate a sample of. We have ~~ 
= F'(x) ~ 0. 
By replacing it in 6.29 and by noticing that F(x) E [0,1], we get Px(x) = F'(x), 
meaning that the probability distribution of X has the probability density 
F'(x), which is the probability density of the expected distribution. 
We can use this result to our advantage, to generate, using the uniform 
generator on (0,1), a sample of the random variable for a given probability 
density Px(x). Here is the algorithm: 
Steps: 
1. Determine the function u = r::.ooPx(t)dt. 
2. Determine its inverse x = g(u). 
3. If U is a sample uniformly distributed on (0,1), then X = g(U) is a 
sample whose distribution has the probability distribution px(x). 
EXAMPLE 6.6 (Exponential distribution) 
A random variable has an exponential distribution if its values belong to lR.+ 
and its probability density has the expression: 
px(x) = >.exp(->,x)l(x E [O,+ooD 
(6.30) 
where the parameter >. ~ 0: 
1. determine the mean and the variance of X; 
2. using the cumulative probability distribution of X, determine a function 
X = g(U) such that X has an exponential distribution with the param-
eter >. when U has a uniform probability distribution on (0,1); 
3. check the result using the hist function. 
HINTS: 
1. an integration by parts leads to E {X} = >. fo+oo xe-)..xdx = 
1/ >.. Likewise, E{X2} = 2/>.2 =} var(X) = 1/ >.2; 
2. the cumulative distribution function of X has the expres-
sion u = fox >'e-)"tdt = 1 - e-)..X. If we "inverse" it, we get 
x = -log(l - u)/ >.. Hence the variable X = -log(l - U)/ >. 
has an exponential distribution if U has a uniform probability 
distribution on (0,1). Because U and (1 - U) have the same 
distribution, we can also simply choose X = -log(U) / >.; 

266 Digital Signal and Image Processing using MATLAB® 
3. type: 
%===== explaw.m 
clear; N=3000; lambda=2; U=rand(N,l); X=-log(U)/lambda; 
meanX=l/lambda; lk=meanX/l0; 
maxx=max(X); pointsx=(O:lk:maxx); 
[nn,xx]=hist(X,pointsx); bar(xx,nn/(N*lk»; hold on 
%===== theoretical exponential distribution 
pth=lambda*exp(-lambda*pointsx); plot (pointsx,pth, 'r') 
hold off; grid 
In the program, the step lk, used to estimate the probability den-
sity, is determined from the mean. 
Exercise 6.3 (Rayleigh distribution) (see p. 462) 
X has a Rayleigh distribution if its probability density has the following ex-
pression: 
x 
(
X2 ) 
Px(x) = (72 exp - 2(72 
l(x E [0, +oo[) 
(6.31) 
Check that lE {X} = (7 j1ff2. Knowing that U has a uniform probability 
distribution on (0,1): 
1. determine the function X = g(U) such that X has a Rayleigh distribu-
tion; 
2. use the hist to check the result. 
Exercise 6.4 (Bernoulli distribution) (see p. 463) 
B is said to have a Bernoulli distribution (the kind of distribution you get if you 
flip a coin several times in a row) with a parameter p, if B is a random variable 
with only two possible values, 0 and 1, with the probabilities Pr(X = 1) = P 
and Pr(X = 0) = 1 - p respectively. Considering N independent Bernoulli 
variables B n , let us assume: 
1. determine, as a function of p, the mean and the variance of Bn; 
2. determine, as a function of p, lE {BkBn} for k i- n. Remember that 
if random variables U and V are independent, they are uncorrelated, 
meaning that lE {UV} = lE {U} lE {V}; 
3. determine as a function of p and N the mean m and the variance (72 of 
the random variable S; 

Chapter 6 - Random Variables 267 
4. we assume that if N is large enough, the probability for S to be located 
in the interval (m - 20", m + 20") is greater than 99.5% (see section 6.3.2 
and the confidence interval). This means we can say that S provides an 
estimation of m with the relative precision Er = 20" /m. Determine the 
expression of Er as a function of p and of N. Use this result to show that 
for Er = 10%, and for small values of p, an approximate value of N is 
given by N ~ 400/p; 
5. write a program that calculates the length N of a Bernoulli sequence with 
the parameter p = 0.1 so that the empirical mean S is an approximation 
of p with an accuracy of Er . 
The Bernoulli distribution can be used, among other applications, as a 
random sequence of bits used to simulate a digital communications system, or 
as it is explained in example 6.7, as a model for describing errors. 
EXAMPLE 6.7 (Error probability estimation) 
Consider a random experiment where a sequence of values is received with an 
error probability p. We are going to try and estimate p using a sequence of 
N observations. In order to do this, a given sequence of length N is sent and 
compared to the received sequence. The question we are faced with is "what 
must be the value of N to estimate the error probability with an accuracy of 
1O%?". 
The model used for describing the error sequence is a sequence of random 
variables Bn such that Bn = 0 if the values in the n-th position are identical 
in the original sequence and the erroneous sequence and Bn = 1 if they are 
different. This way, the random variable: 
1 
N 
S= NLBn 
n = l 
(6.32) 
gives an estimation of the probability error p. If we assume that the random 
variables Bn are independent, we can use the results obtained in exercise 6.4. 
To estimate p with an accuracy Er roughly equal to 10% and a 95% confidence 
interval, we need to start with a sequence with a length of N ~ 400/p. At 95 
% this corresponds to the interval (m - 20", m + 20") (see table in section 6.3.2). 
This can be reduced to: 
2 
40"2 
4pq 
4 
400 
E =-=--~-=?N~-
r 
m2 
Nm2 
Np 
p 
If we restrict ourselves to a 70% confidence interval, the same accuracy Er = 0.1 
is achieved for N ~ 100/p (the interval is (m-O", m+O")). This value is often the 
one used in practice to calculate the length of a test. Note that Np represents 
approximately the number of errors. This leads us to adopting the following 
rule: 

268 Digital Signal and Image Processing using MATLAB® 
To estimate an error probability with a precision of 10% and a confidence 
level of 70%, you need to see a hundred errors "go by". 
Hence the 
estimation must be performed on a sequence with a length roughly equal 
to 100/ p. 
If the order of magnitude wanted for p is 10-5, you need N = 10,000,000. 
Such a length can require a long simulation time, even with a fast computer. 
This is a very common problem in the field of digital communications. 
6.5 Uniform quantization 
Quantization provides a non-trivial example using the concepts explained in 
this chapter. The practical implications of the results are fundamental. In 
this paragraph, we will only discuss the very simple case of the uniform scalar 
quantization. 
The uniform quantization operation on N bits consists of dividing the inter-
val (-A, +A) in 2N sub-intervals of the same length q = 2A/ 2N. q is called the 
quantization step. When the quantization operation is performed, each sample 
X is associated with the N -bit coded number of the interval it belongs to. 
When the signal is reconstructed, this number is replaced by the median value 
of the interval. If X denotes the sample we want to quantize and Y denotes 
the reconstructed value, we have: 
Y = kq+q/ 2 when kq:::; X < (k+ l)q 
Usually ADCs use a two's complement binary coding for the quantized 
samples. The code takes values between _2N-l and +2N- 1 - 1 where N is 
the number of bits used for coding. The conversion law can be described by 
Figure 6.8. 
-A 
:> : q :<: 
6 : 
9 
10 
~-1
: 
~ 
Q : 
,.........., 
6 
Q 
A 
x 
Figure 6.8 - Quantization and two 's complement binary coding with 3 bits 
Therefore, there is a gap between the "true" value and the reconstructed 
value. If c = X - Y refers to this gap, called the quantization noise, we can 

Chapter 6 - Random Variables 269 
Figure 6.9 - Uniform quantization 
write X = Y + c. The quantization operation is the equivalent of adding a 
noise with the power lE { c2 } . 
Although it is possible to determine the probability distribution of c using 
that of X, it is often sufficient to just assume that c is a uniform random 
variable on (-q/2 , +q/2). This means that lE {c} = 0 and that: 
j
+q/2 
1 
q2 
lE {c2 } = 
c2-dc = -2 
- q/2 
q 
1 
(6.33) 
The uniformity hypothesis implies the absence of clipping, meaning that 
none of the values we wish to quantize are located outside the range (-A, +A). 
Otherwise, c can assume much greater values than q/ 2. It just needs to be made 
sure that the amplitudes of X higher than A have a negligible probability. In 
the case where X is a centered random variable with a variance 0"2, we usually 
choose: 
A=FcO" 
where Fc is called Clipping Factor ( CF) . Thus, if X is Gaussian, by adopting 
the "3 sigma" rule (page 254), corresponding to a 99% confidence level, Fc = 3. 
For a speech signal, the Gaussian hypothesis usually works poorly, and the 
value of Fc is rather chosen roughly equal to 4. 
Exercise 6.5 (Signal-to-quantization noise ratio) (see p. 464) 
Consider the uniform quantization of an observation X described with a cen-
tered random variable with a variance 0"2. We wish to find the expression for 
the level of noise quantization as a function of the number of bits used for 
coding. 
1. The signal-to-quantization noise ratio (SNR) is defined by: 
(
lE {X2}) 
SNR = lOloglo 
lE {c2 } 
Show that in the absence of clipping, that is if A = FcO" with Fe high 
enough, the SNR's expression as a function of the number N of bits used 
for coding the samples is: 
(6.34) 

270 Digital Signal and Image Processing using MATLAB® 
2. Write a function performing the two's complement binary coding of a 
sequence. 
3. Write a program to check the hypothesis of uniform quantization noise 
distribution. Perform a simulation to measure the SNR for values of N 
from 1 to 7. 
What should be remembered from formula 6.34 of exercise 6.5 is the fol-
lowing rule, called the 6 dB per bit rule: 
In uniform quantization, the signal-to-quantization noise ratio is en-
hanced by 6 dB every time 1 bit is added to the quantizer. 

Chapter 7 
Random Processes 
7.1 Introduction 
At the beginning of Chapter 6, we pointed out that the concept of random 
variable was needed to describe with a model the variability of certain phe-
nomena said to be random. Speech signal observed at a microphone's output is 
an example. There is no use to try and describe it with a deterministic expres-
sion such as x(t) = A cos(27rfot) , which is relevant however when describing 
electrical voltage, hence the idea of using random variables for describing the 
phenomenon at every instant. This leads us to the following definition. 
D efinition 7.1 A random process is a set of time-indexed random variables 
X(t) defined in the same probability space. If the possible values for t belong to 
JR., the process is called a continuous-time random process. If the possible values 
for t belong to Z, then we are dealing with a discrete-time random process1 . 
The definition implies that a random process associates a real value called 
a realization with every instant t and every outcome w. A random process can 
therefore be interpreted as two different perspectives (Figure 7.1): 
1. either as a set of functions of time, also called trajectories, each one 
associated with an outcome; 
2. or as a set of random variables, each one associated with a given time. 
In MATLAB@, the randn function makes it possible to simulate the tra-
jectories of a zero-mean gaussian random process with a variance of 1. In the 
following program, x is a matrix with 4 columns and 100 lines: 
1 We will often use n , k, e, m ... to denote time for a discrete-time random process. 

272 Digital Signal and Image Processing using MATLAB® 
1 
X(t,wo) 
X(t,Wl) 
, 
, 
----9---------1--------- ---------
, 
, 
X(t,W2)~~~~~~~
~ __ ~~~
~_4
~_4~~~~~~--
- 1 
o 
10 
20 
to=30 
40 
50 
Figure 7.1 - Trajectories of a random process: at t = to, there is a random variable 
X(to) and for each Wk , a trajectory can be obtained. 
%===== trajl.m 
x=randn(100,4) ; 
for k=1:4 
subplot(2,2,k); plot(x(:,k)); grid 
end 
x can be considered as the representation of four trajectories for the same 
random process, for an observation time of 100 points (Figure 7.2). 
Figure 7.2 - Examples of random process trajectories 
7.2 Wide-sense stationary processes 
In some processings involving random processes, particularly in linear filtering, 
the signals are assumed to be stationary, and only the first and second order 

Chapter 7 - Random Processes 273 
moments are taken into account. The two trajectories represented in Figure 
7.3 illustrate these concepts. We might say, after observing them, that: 
- the behavior of the signals with time show a certain permanence. Their 
properties do not depend on the time origin; 
- the mean seems to be equal to zero; 
- most of the power is located around roughly 110 Hz, because 11 oscilla-
tions are observed over a duration of 0.1 s, hence about 110 oscillations 
per second. 
~J~ 
-l.5 
' 
, 
, 
, 
, 
, 
, 
, 
, 
o 
0.02 0.04 0.06 
0.08 
0.1 
0.12 0.14 0.16 
0.18 
0.2 s 
1~
. ~ 
I 
I 
I 
I 
I 
I 
I 
I 
I 
0.5 
-
:--;--
~
-:-
~ -
N -- -:-~-;-
-o~~~1Y~ 
I 
I 
I 
I 
I 
I 
I 
I 
I 
-1 
' 
, 
, 
, 
, 
, 
, 
, 
, 
o 
0.02 0.04 0.06 
0.08 
0.1 
0.12 0.14 0.16 
0.18 
0.2 s 
Figure 7.3 - The two representations give an idea of the meaning of the word "sta-
tionary" 
You will find here the mathematical definitions associated with these con-
cepts for random processes, and such a process is referred to as a Wide sense 
stationary random process, or WSS. 
7.2.1 
Definitions and properties of WSS processes 
Definition 7.2 (Mean) The mean of a random process is the mathematical 
expectation of the random variable X(t). This quantity, which depends on the 
time t, is a deterministic function of t, which will be denoted by: 
mx(t) = lE {X(t)} 
(7.1) 
Definition 7.3 (Autocovariance function) Let X(t) be a random process. 
The autocovariance function is the function of tl and t2 defined by: 
Rxx(t1 , t2) = lE {Xc(tdX;(t2)} 
(7.2) 
where X c (t) = X (t) - lE { X (t)} refers to the centered process. 

274 Digital Signal and Image Processing using MATLAB® 
Definition 7.4 (Autocorrelation function) Let X(t) be a random process. 
The autocorrelation function is the function of tl and t2 defined by: 
(7.3) 
where Xc(t) = X(t) -
IE: {X(t)} refers to the centered process. The Schwartz 
inequality tells us that for any random variables Xl and X 2: 
(7.4) 
Inequality 7.4 implies that: 
(7.5) 
A simple calculation shows that: 
(7.6) 
Definition 7.5 (Covariance function) The covariance function of two dis-
tinct processes X (t) and Y (t) is defined by: 
Of course, in the case of a real process, it is not useful to have the con-
jugation (*) appear in expressions 7.6 and 7.7. The auto covariance function 
is always a deterministic function of tl and t 2 . From now on, if there is no 
possibility of confusion, we will omit the index xx when writing something of 
the type Rxx(h, t2). 
In the general case, the auto covariance function Rxx(h, t2) depends sep-
arately on hand t2. In the particular case where it only depends on h - t2, 
the time origin does not determine the level of covariance. This implies that 
the trajectories of the process have an almost eternal permanence. The term 
stationary is associated with this property. This concept is absolutely funda-
mental in signal processing. It leads to the following definition. 
Definition 7.6 (Second order stationary random process) 
A random process is said to be "wide sense second order stationary" (WSS) , 
or simply "second order stationary", if it obeys the following properties2: 
- the mean IE: {X(t)} = m is independent oft; 
-
IE: {IX(t)12} < +00; 
2In the case of continuous-time processes, we add that the autocovariance function is 
continuous at the origin. 

Chapter 7 Random Processes 275 
- the autocovariance function IE: {Xc(tl)X~(t2)} 
= R(T) depends on the 
time difference T = h - t2· 
We have, according to relation 7.7, IE: {X(tdX*(t2)} = IE: {Xc(tdX~(t2)} 
+ 
mX(tl)m'X(t2). If we let t2 = t and h = t + T we get: 
IE: {X(t + T)X*(t)} = R(T) + Iml 2 
which depends only on T. 
EXAMPLE 7.1 (Complex harmonic process) 
Consider the complex random process defined by: 
p 
X(t) = L ake2j7r/kt 
k= l 
(7.8) 
where Uk} refers to a deterministic sequence of P frequencies and {ak} to 
a sequence of P zero-mean, uncorrelated complex random variables with the 
respective variances aL Such a process is called a harmonic process. It does 
not matter here whether t is an integer or a real number. 
First, we calculate the mean of X(t). Because the expectation of the sum 
is the sum of the expectations, we get IE: {X(t)} = O. Hence the process is 
zero-mean. Its auto covariance function has the expression: 
p 
p 
IE: {X(t + T)X*(t)} = L L IE: {aka~} 
e2j7r/k(t+r)e-2j7r!nt 
k= ln= l 
Because by hypothesis, IE: {lakI2} = a~ and IE:{aka~} 
= 0 for k i- n, we 
get: 
p 
IE: {X(t + T)X*(t)} = L a~e2j7r/kr 
k= l 
IE: {X(t + T)X*(t)} depends only on T. This process is therefore WSS. 
EXAMPLE 7.2 (Real harmonic process) 
Consider the random process: 
p 
X(t) = L Ak cos(211" fkt + <Pk) 
k= l 
(7.9) 
(7.10) 
where {ik} refers to a deterministic sequence of P frequencies, {Ad to a se-
quence of P independent, zero-mean, real random variables, with the respective 
variances a~ and {<Pk} to a sequence of P uniform random variables on (0,211"), 

276 Digital Signal and Image Processing using MATLAB® 
independent of one another and of A k . A calculation similar to the previous 
one leads to lE {X (t n = o. We now calculate the auto covariance function. We 
get: 
p 
p 
lE {X(t + T)X* (tn = L L lE {AkAn cos(27r fk(t + T) + <Pk) cos(27r fnt + <Pnn 
k=ln=l 
p 
p 
= L L lE {AkAn} lE {cos(27r fk(t + T) + <Pk) cos(27r fn t + <Pnn 
k=ln=l 
p 
= L lE {An lE {cos(27r fk(t + T) + <Pk) cos(27r fkt + <Pk)} 
k=l 
where we used the non correlation of the Ak, then the independence of Ak and 
of <Pk. Next, we get: 
1 p 
lE {X(t + T)X* (tn = "2 L (T~lE 
{cos(27r fkT) + cos(27r fk(2t + T) + 2<Pkn 
k=l 
1 p 
= "2 L (T~ [cos(27r fkT) + lE {cos(27r ik (2t + T) + 2<Pkn 1 
k=l 
1 P 
= "2 L (T~ cos(27r ikT) 
(7.11) 
k=l 
where we used the hypothesis according to which <Pk is uniform in (0,27r) and 
therefore that: 
lE{cos(27rfk(2t + T) + 2<Pkn = r
27r 
cos(27rik(2t + T) + 2¢)~ 
d¢ = 0 
10 
27r 
As a conclusion, the autocovariance function of a real harmonic process 
depends only on T and therefore the process is WSS. 
COMMENTS: if the auto covariance function (expressions 7.9 and 7.11) of a 
real or complex harmonic process is the sum of periodic functions, then the 
variables X(t + T) and X(t) remain correlated, even for large time differences. 
This is called a memory effect, and it lasts indefinitely. 
Unlike harmonic processes, there are WSS processes for which the auto-
covariance function R(T) tends to 0 when T tends to infinity. This can be 
interpreted as a memory loss of the process occurring with time. 
7.2.2 
Spectral representation of a WSS process 
Definition 7.7 (Power spectral density) 
Let X (t) be a WSS process with the autocovariance function R( T). The Fourier 

Chapter 7 - Random Processes 277 
transform of R( T) is called the power spectral density (or psd), or the spectrum. 
For continuous-time WSS random processes, the psd therefore has the expres-
sion: 
(7.12) 
and fo r discrete-time WSS random processes: 
+00 
S(J) = L R(k)e-2j7rjk 
(7.13) 
k= -oo 
Power is defined as: 
(7.14) 
The power's square root is also called the root mean square. We will often 
be dealing with zero-mean processes. In such cases, the power is equal to the 
auto covariance function's value at the origin. 
By inversion of the FT, we have, for continuous-time WSS random pro-
cesses: 
/
+00 
R( T) = 
-
(X) S(J)e2j7r jT df with T E lR 
(7.15) 
and for discrete-time WSS random processes: 
/
+ 1/ 2 
R(k) = 
S(J)e2j7rJkdf with k E Z 
-1/2 
(7.16) 
Note that because of 7.15 and 7.16, R(O) is merely the integral of S(J). 
COMMENTS: the rationale for the definition of the psd can be made by 
considering a "piece" of the trajectory xN (n,w) for -N < n < N and the 
estimation of its power by: 
by application of the Parseval relation. 

278 Digital Signal and Image Processing using MATLAB® 
The expression IXN (f, w W / (2N + 1) in the integral is a spectral density 
power. The estimate on the path set is made by taking the expectation: 
psd 
JE {2~
+l
IX N(f,w)12
} 
1 
"""" R(n - k)e-27rjj(n-k) 
2N + 1 
L.JL.J 
k,nE[- N ,+N] 
+N 
) 
"" R(£)e-27rjj£ (1 _ 21£1 
L.J 
2N + 1 
£= -N 
f R(£)e-27rjj£ (1 - 2~1~ 1) 1(£ E 
{-N, ... , N}) 
£= - 00 
+00 
L R(£)e-27rjj£ f(£, N) 
£= - 00 
(7.17) 
If we assume that R( k) is summable, we can calculate the limit of JE {.} by 
swapping the limit and the sum sign (dominated convergence theorem [10]). 
By noticing that f(£,N) tends to 1, we infer that JE{.} tends to S(f) when N 
tends to infinity. 
The following result can be proven [4]: 
Theorem 7.1 (Positivity of the psd) Let X(t) be a WSS random process, 
and let S(f) be its psd. We have: 
S(f) ~ 0 
(7.18) 
In the particular case of a real process, the psd is an even function, that is 
S(f) = S( - f). 
It should be noted that the positive nature of S(f) is directly related to the 
positive nature of the covariance (see property 6.5). 
Property 7.1 Let X(t) be a WSS random process. We have: 
1. hermitian symmetry: R( T) = R* (-T). Therefore, we only need to evalu-
ate R(T) for T ~ 0; 
2. positivity property: for any N, for any sequence of times {to, ... , tN-d, 
and for any sequence of complex values {ao, ... , aN-d: 
N -l N -l 
aHa = L L aka-::nR(tk -tm ) ~ 0 
k = O m = O 

Chapter 7 - Random Processes 279 
where R is the N x N covariance matrix constructed from the covariance 
function R(T) of the process, where a = [an 
aN_d
T and where 
the exponent H indicates a transpose-conjugation (see property 6.5); 
3. if the process has a mean different from zero, a "peak" with an amplitude 
Iml2 is attached to the spectrum at the origin (f = 0). This peak at the 
origin, which simply indicates the presence of a non-zero mean, is called 
the continuous component of a process. 
Using the Dirac distribution 
amounts to choosing the Fourier transform of the second order moment 
as the definition of the spectrum. Indeed, we have: 
E {X(t + T)X*(t)} = R(T) + Iml 2 
which has S(f) + ImI 25(f) as its Fourier transform; 
4. the complex and real harmonic processes, expressions 7.8 and 7.10, have 
periodic autocovariance functions, expressions 7. 9 and 7.11. Therefore, 
it does not exactly have a Fourier transform. However, we can find a 
meaning to the psd using Fourier series: 
- in the case of a complex process: 
p 
S(f) = L a~5(f 
+ fk) 
k=l 
- and in the case of a real process: 
1 PIP 
S(f) = 4 L a~5(f 
+ fk) + 4 L a~5(f 
- fk) 
k=l 
k=l 
(7.19) 
(7.20) 
The psd comprises peaks that indicate the presence in the signal of sinu-
soidal components with uncorrelated amplitudes; 
5. as it was the case with the deterministic description, the spectrum repre-
sents the distribution (or localization) of the power along the frequency 
axis. The power is given by: 
{ 
P = R(O) + Iml 2 = I:: S(f)df + Iml 2 
(continuous-time) 
1
+1/2 
(7.21) 
P = R(O) + Iml 2 = 
S(f)df + Iml 2 
(discrete-time) 
- 1/2 
COMMENT: the fact that the psd of an observed process contains peaks 
can be used in some synchronization systems to retrieve, using a very narrow 
band-pass filtering, a harmonic component with the same phase as a particular 
component of the observed process. 
We admit without proof the following result: [4] 

280 Digital Signal and Image Processing using MATLAB® 
Property 7.2 (Characterization of positivity) Consider a sequence f(k) 
with k E Z such that r(k) = f*( -k) and L k If(k) 1 < +00. This sequence is 
the covariance sequence of a WSS process if and only if, for all f: 
+00 
S(f) = L f(T)e - 2j7rjk ~ 0 
k = - oo 
EXAMPLE 7.3 Consider the real sequence r(k) = 1 x l (k = 0) +a x l (k = ±1). 
Determine the condition on a such that the sequence is the covariance sequence 
of a WSS process. 
HINTS: obviously we have, for any a, Lk If(k)1 < +00. Using 
property 7.2, f(k) is a sequence of covariance if and only if: 
S(f) = 1 + 2a cos(27r f) ~ 0 
The condition S(f) ~ 0 is equivalent to lal < 1/2. Notice that 
a represents the correlation coefficient p(l). Hence, after 7.5, we 
already knew that Ip(l)1 ::; 1. The fact that the sequence is a 
covariance sequence of a WSS process imposes a stronger condition. 
Studying methods that make it possible to estimate the spectrum of a sec-
ond order stationary random process is an important field in signal processing. 
We will discuss this later on. 
Positive Toeplitz matrix 
Consider a WSS discrete-time random process X(n). We are going to determine 
the covariance matrix at any K consecutive times of the process. If we start 
at the times {n , n + 1, ... , n + K - 1}3, the K x K covariance matrix has the 
expression: 
R 
Xc(n + 1) 
{ r 
X,(n) 1 
lE 
Xc(n +:K _ 1) 
[X
~ (n) 
X~(n 
+ 1) 
x; (n + K - 1)] } 
r 
R(O) 
R(-l) 
R(- K+ 1) 1 
R(l) 
R(O) 
R(-l) 
R(-l) 
R(K - 1) 
R(l) 
R(O) 
3Most of the time, we will write the time sequence from left to right by increasing times. 

Notice that R = RH and that because of the stationarity of the process, 
the matrix R is such that the lines parallel to the main diagonal are comprised 
of equal terms. This type of matrix is called a Toeplitz matrix. 
The MATLAB® function toepli tz (V) allows you to construct, from the 
vector V = [V(O) 
V(K -1)f, the square hermitian Toeplitz matrix 
the first line of which is [V(O) 
V(l) 
V(K - 1)]. 
Definition 7.8 (Gaussian random process) 
A random process X(t) is Gaussian if, for any k, and for any time sequence 
{h, ... , td, the vector [X(td 
X(tk)] is Gaussian. 
For the definition of a Gaussian vector, see 6.17. 
Definition 7.9 (White noise) Discrete-time white noise is the name given 
to a WSS, zero-mean, random process X (n), the covariance function of which 
can be written: 
R(k) = lE {X(n+ )X(n)} = { ~o 
when 
k = 0 
when 
k =1= 0 
Because of formula 7.13, which provides us with the spectrum, the power 
spectral density is constant, and has the expression: 
S(f) = Ro 
(7.22) 
In the continuous-time case, a definition similar to 7.9 poses a problem, 
because it leads to a random process of infinite power (the integral fIR S(f)df 
diverges) and the autocovariance function can only be defined in the distribu-
tions context. Thus, we have: 
S(f) = Ro ~ R(T) = Rob(T) 
where b (T) now refers to the Dirac distribution. We must say however, that 
in most practical cases, the calculations performed with the Dirac distribution 
lead to results that coincide with those obtained by starting off with a B band 
noise (which does not lead to an infinite power problem), and then making B 
tend to infinity. 
The word white comes from the analogy made with white light, for which 
the power is uniformly distributed among all the optical frequencies. 

282 Digital Signal and Image Processing using MATLAB® 
White noise is the archetype of models used in practice for describing noise. 
In communications systems, for example, it describes every kind of noise caused 
by thermal phenomena in the transmission chain, from the emitter to the re-
ceiver. The rounding and quantization noises that occur in a digital processing 
system are another example. 
Although a process often is both Gaussian and white, particularly in ther-
mal noise models, there is no implication between these two properties. Thus, 
a random process can be white without being Gaussian or Gaussian without 
being white. 
EXAMPLE 7.4 (Trajectory of a noisy sine) 
Write a program that displays a sequence of 30 samples taken at the frequency 
Fs = 1,000 Hz, from a signal X(t) = s(t) + B(t), sum of a sine s(t) with a 
frequency of Fo = 80 Hz and of a zero-mean, Gaussian, white noise B(t). The 
power of B(t) is chosen so as to have a signal-to-noise ratio equal to 15 dB, 
knowing that the signal s(t) has an amplitude of 3. 
HINTS: having a signal-to-noise ratio equal to 15 dB means that 
the ratio rp of the signal's power to the noise's power is such that 
10 loglO(rp) = 15, and therefore rp = 101.5. Because the power 
of a sine with an amplitude A is equal to P = A 2/2, the noise 
variance has to be (J2 = A 2/2rp. Since A = 3, this leads to (J = 
3/ )2 X 10l.5 . 
To obtain a trajectory, type: 
%===== sin80br.m 
N=30; FO=80; Fs=1000; Ts=1/Fs; tps=(O:N-1)*Ts; 
sigma=3/sqrt(2*10 - 1.5); 
%===== SNR 
s=3*eos(2 * pi * FO * tps); 
tpse=[tps(1):Ts/10:tps(end)] ; 
se=3*eos(2 * pi * FO * tpse); 
x=s + sigma*randn(1,length(tps)); 
plot (tpse, se, I -
I , tps, s, I X I , tps, x, '0 ' ); grid 
The result is shown in Figure 7.4. A cross indicates a sample with-
out noise, and a circle indicates a noisy sample. 
EXAMPLE 7.5 (Linear transformation of a WSS process) 
Let W(n) be a zero-mean, WSS, discrete-time random process. Rww(k) refers 
to the autocovariance function and W = [W(n) 
W(n + N - l)f to 
the vector obtained from N consecutive values of W(n): 
1. write, as a function of Rww(k), the expression of the covariance matrix 
of W (definition 6.13); 

Chapter 7 - Random Processes 283 
o samples of the noisy signal x 
x samples of the signal s 
4 ,-----~------~------~------~------~----__. 
3 
2 
1 
o 
-1 
-2 
- 3 
________ L _________ __ L __ _ 
- -
-
-
- -
-
-
- -, -
o 
-4 L-__________________________________________ ~ 
o 
0.005 
0.Q1 
0.Q15 
0.02 
0.025 
0.03 (s) 
Figure 7.4 - Sine with a frequency of 80 Hz, corrupted by white noise with a signal-
to-noise ratio SNR = 15 dB 
2. given an (N x N) matrix M , let X = MW. Determine the expres-
sion of the covariance matrix of the vector X. What is the probability 
distribution for X when W(n) is a Gaussian process? 
3. use this result to find a method for obtaining a sequence of N values with 
a given covariance, when W(n) is white, zero-mean and with a variance 
equal to 1; 
4. use this result to find a method for obtaining a sequence of N values of 
a white process with a variance equal to 1, when X(n) has Rxx(k) as its 
au to covariance function. This is called whitening the process X (n). 
HINTS: 
1. because W(n) is zero-mean, the covariance matrix is given by: 
Rw 
lE {WWH} = [lE{W(n+£)W*(n+k)} ] 
[Rww(n + £ - n - k)] = [R(£ - k)] 
where £, k E {O, ... , N - I}, which leads us to: 
[ 
Rww(O) 
Rw = 
Rww(~ 
_ 1) 
RWW(~N 
+ 1)] 
Rww(O) 
2. if we start off with X = MW , the covariance matrix has the 
expression: 

284 Digital Signal and Image Processing using MATLAB® 
If W(n) is Gaussian, the sample X is Gaussian itself, since 
Gaussian nature is unchanged by linear transformation (see 
theorem 6.4). Its mean is zero and its covariance matrix is 
R x , meaning that the probability density has the expression: 
X 
-
ex 
--x R 
x 
1 
( 
1 
H 
-1 ) 
Px( ) -
(21l')N/2Jdet(Rx) 
p 
2 
x 
where x = [xn 
3. If the process W(n) is white, zero-mean and has a variance 
of 1, its covariance matrix Rw = I N, where I N refers to the 
N x N identity matrix. Given the matrix R x, how should 
M be chosen for the sample X = MW , where W is white, 
to have the covariance Rx? All we need to do is choose M 
such that MMH = Rx. M is called a square root of Rx . 
Just like in the case of scalars, a positive matrix has several 
square roots4. With MATLAB®, the sqrtm(R) function, with 
R positive, calculates the square root of R the eigenvalues of 
which have nonnegative real part. 
4. Bear in mind that Rx = MRwMH. If we want X(n) to be 
white with a variance of 1, Rx has to be equal to the identity 
matrix. This can be achieved by choosing M as the inverse of 
the square root of R w , which is obtained in MATLAB® using 
the command inv(sqrtm(Rw)). 
If X(n) is Gaussian, then the sample W is Gaussian and 
white. If this is the case, we know (see theorem 6.3) that 
non-correlation implies independence. Therefore, the obtained 
sequence is comprised of independent variables. 
This provides us with the following important result: 
Colored noise can be changed into white noise by multiplying the sample 
by the inverse of the square root of the "colored" process's covariance 
matrix. This is called whitening the signal. Furthermore, if the original 
samples are Gaussian, the processed samples are Gaussian and indepen-
dent. 
4With scalars, if r 2: 0, the equation mm* = r has the solution "fmej </> where if; is an 
arbitrary real number. "fm is called the positive square root, and the number u = ej </> is 
such that uu* = 1. Likewise, the matrix equation MMH = R, where R 2: 0, has an infinite 
number of solutions of the type ..fRu where U is any unitary matrix that obeys UU H = I. 

Chapter 7 - Random Processes 285 
7.2.3 
Sampling a WSS process 
Consider a real, zero-mean, WSS random process X(t), t E IR, with the psd: 
where R(T) = E{X(t+T)X(t)} represents its auto covariance function. Here, 
F is a frequency expressed in Hz, and (t, T) are times expressed in seconds. We 
assume that X(t) is B-band limited, meaning that S(F) = 0 for IFI > B. 
The signal X(t) is sampled at a frequency of Fs = l /Ts. Its samples are 
denoted by Xs(n) = X(nTs). It can be shown [36] that if Fs ;:: 2B, the 
process can be reconstructed, as a limit in quadratic mean, from its samples 
according to the reconstruction formula 2.25 proved in the deterministic case, 
the expression of which is recalled below: 
(7.23) 
In the case where Fs < 2B, perfect reconstruction is impossible because of 
aliasing. 
As it was the case with deterministic signals, when a continuous-time WSS 
random process is sampled, the sampling operation at a frequency Fs must be 
preceded by anti-aliasing filtering with a gain of 1 in the (-Fs /2, Fs /2) band 
to avoid aliasing. 
We are now going to determine the relation between the psd S(F) of the 
continuous-time random process X(t) and the psd Ss(J) of the random process 
sampled at Xs(n) = X(n/Fs). Because of definition 7.13: 
Ss(J) = L
E {Xs(n + k)Xs(n)} e-2j7rkj 
k 
If we use the fact that Xs(n) = X(n/ Fs), we get: 
Replacing in Ss(J) leads us to: 
Ss(J) = L R(k/ Fs)e-2j7rkFTs 
k 
where we have assumed F = f Fs and FsTs = 1. If we apply the identity given 
by the Poisson formula 2.1 to the second member, we get: 
(7.24) 
n 

286 Digital Signal and Image Processing using MATLAB® 
Finally, we find the psd's expression for the process X(t): 
(7.25) 
To sum up, it should be remembered that the psd of X(t) is obtained from 
the psd of X s(n) by: 
- multiplying the amplitude by 1/ Fs; 
- multiplying the frequency axis by Fs; 
- and by limiting the frequency band to the interval (-B , B). 
Conversely, the psd of Xs(n) is obtained from the psd of X(t) by: 
- multiplying the amplitude by Fs; 
- dividing the frequency axis by Fs; 
- and by periodizing with the period 1. 
To illustrate this, we are going to apply these formulae to the problem of the 
signal-to-quantization noise ratio when the signal is oversampled at frequency 
higher than the Nyquist frequency. 
Quantization noise and oversampling 
When oversampling a band-limited signal without quantizing it, we know, from 
the sampling theorem, that it is useless to oversample (compared with the 
Nyquist frequency). This changes completely when the sampling is followed by 
a quantization operation, because the quantization operation can be interpreted 
as the addition of noise. Let us see the consequences of oversampling in terms 
of signal-to-noise ratio. 
Consider the B band, zero-mean, WSS, real random process X(t). This 
signal is sampled at the frequency Fs 2: 2B, the sequence of obtained signals 
is denoted by {Xs (n)}. These samples are then uniformly quantized, with a 
quantization step q (see paragraph 6.5). Let X~(n) 
be the samples that are 
quantized. We are going to reconstruct a signal denoted by XQ(t) , from the 
quantized samples X~(n), 
using expression 7.23, then try to evaluate the power 
of the "error" between the original signal X(t) and the signal XQ(t) obtained 
from the quantized samples. 
Starting off with 7.23, we can write successively: 
n 

Chapter 7 - Random Processes 287 
where hB(t) is given by 2.25. Let us define E(n) with X~(n) 
= Xs(n) + E(n). 
We get: 
n 
n 
By hypothesis, Fs ~ 2B. Hence the first term is exactly equal to the signal 
X (t) and therefore: 
n 
v 
= BQ(t) 
The signal BQ(t) represents the error between the original signal and the 
reconstructed signal: it is called the quantization noise. Notice that its expres-
sion is obtained, from the process E(n), precisely by using the reconstruction 
formula 7.23. Therefore, according to expression 7.25, we can determine the 
psd of BQ(t) from the psd of E(n), and from there, determine its power. 
By referring to the hypotheses on E(n) made in paragraph 6.5, we know 
that E(n) is a zero-mean random process with a variance of q2/12 and such 
that IE {E(n)E(k)} = 0 for n =F k. Hence the psd of E(n) is given for any f by: 
q2 
Se:(f) = 12 
Using formula 7.25, the psd of BQ(t) is then given by: 
q2 1 
SB(F) = --l(F E (-B, B)) 
12 Fs 
The quantization noise's power is obtained by integrating SB(F): 
1
+B 
q22B 
PB = 
SB(F)dF = --F 
-B 
12 
s 
(7.26) 
(7.27) 
T---~-+-~~--~--~-~~-r-~-~f 
: -B 
0 
B 
: 
-Fs 
-Fs/2 
Fs/2 
Fs 
Figure 7.5 - Calculating the signal-to-noise ratio 

288 Digital Signal and Image Processing using MATLAB® 
Using formula 6.34, we end up, in the case of uniform quantization with 
oversampling, with the following expression of the signal-to-noise ratio (Figure 
7.5): 
(7.28) 
where N refers to the number of bits of the quantizer and Fe to the clipping 
factor. A 3 dB gain occurs every time the sampling frequency is doubled. This 
result calls for a few comments: 
1. according to the sampling theorem, oversampling is useless without the 
quantization operation. All the information useful to reconstructing the 
signal without errors is contained in the samples taken at Fs = 2B; 
2. formula 7.28 was obtained by assuming that the quantization noise is 
white (the sequence s(n) is uncorrelated). If this happens to be false, 
the psd SB(F), given by expression 7.26, has a different shape (sharper 
peaks). This means that the quantization noise's power is no longer given 
by expression 7.27 and the gain can then be much less than 3 dB. This 
is the case when the oversampling factor becomes too high, because the 
non-correlation error hypothesis is not quite established anymore. Hence 
there cannot be an infinite iteration of the 3 dB gain by doubling the 
sampling frequency; 
3. there is no point in interpolating (interpolating is not oversampling) the 
already quantized discrete-time sequence in the hope of obtaining samples 
that would have been produced when oversampling a continuous-time sig-
nal. The errors introduced by the quantization process are permanently 
added, and the reconstructed samples are noised in the same way. 
7.3 Estimating the covariance 
In practice, the covariance functions are not known, and we are faced with 
the problem of estimating them. As we have already said, a random process 
can be seen as great number of trajectories corresponding to a great number of 
realizations of the identically repeated experiment. However, in many practical 
cases, we have at our disposal only one process trajectory. It then becomes clear 
that the stationary process category, for which the moments can be estimated 
by calculating a "temporal mean" on only one trajectory, will have an important 
practical role. 
Ergodicity is related to this concept. However, we will not 
give its general definition here. We will only say that a WSS random process 
X(n) with the mean m = JE {X(n)} and the auto covariance function R(k) = 
JE {Xe(n + k)X~(n)}, 
is ergodic if its mean and its auto covariance function can 
be obtained as the convergence in probability, when N tends to infinity, of a 

Chapter 7 - Random Processes 289 
temporal mean calculated for only one trajectory. This can be expressed, when 
N tends to infinity: 
N-I 
mN = ~ L X(n) ----+ m 
n =O 
(7.29) 
For the covariance, this leads to: 
N-k-l 
RN(k) = ~ L (X(n + k) - mN )(X*(n) - mrv)) ----+ R(k) 
(7.30) 
n=O 
We know, from the law of large numbers [10], that, for a sequence of inde-
pendent random variables, with the same mean m and the same finite variance, 
which is a particular case of a WSS process, the empirical mean: 
1 N - I 
N L X(n) 
n=O 
converges in probability to m. The question is "does this result apply to a 
larger class of random processes than just the sequences of independent random 
variables, such as for example the WSS random processes?" The answer is yes 
[29] , but it is not fundamentally useful for what we are going to do to go into 
it any further. We will simply assume that, for the WSS processes we will 
be considering, the conditions are in fact met. We can then use expressions 
7.29 and 7.30 to estimate the mean and the auto covariance function of a WSS 
process from the observation of N samples. 
Notice that if the mean of X(n) is m, then we can write that X(n) = m + 
B(n), where B(n) is a zero-mean process. If we start off with this, formula 7.30 
for estimating covariance consists of estimating m first, then of subtracting this 
estimation to X(n), and finally of estimating the covariance of B(n). Generally 
speaking, we have to consider X(n) = s(n; e)+B(n) where B(n) is a zero-mean 
random process and s(n; e) represents a deterministic signal that depends on a 
parameter e we have to determine. In this context, s(n; e) is sometimes referred 
to as the trend term. This trend can be either affine, polynomial or periodic. 
For the latter, the trend is said to be seasonal. In conclusion, estimating the 
covariance is achieved on the process B(n), which is obtained in the following 
way: 
- if a non-zero mean is observed, center the process by calculating B(n) = 
X(n) - -b L
~:OI 
X(k); 
- if an affine trend is observed, of the kind s(n; e) = al + a2n (here e = 
(al,a2)), use the program written in exercise 7.1, that allows you to 
estimate the pair (aI , a2) and then to obtain the residue B(n) ; 

290 Digital Signal and Image Processing using MATLAB® 
- if, finally, a seasonal trend is observed, of the kind s(n; e) = a + 
bcos(27r fan - ¢) (here e = (a, b, ¢) and fa is known) , use the program 
written in example 7.6. 
Suppressing a mean 
With MATLAB@, the estimated mean * 
~ ~= l X(k) is obtained with the 
mean command. To obtain the zero-mean process, all you need to do is type 
xc=x-mean ex). You can also type meanx=sum ex) IN, where x is assumed to be 
a length N column vector, then xc=x-meanx. 
Exercise 7.1 (Suppressing an affine trend) (see p. 466) 
Consider a discrete-time random process X(n) = al + a2n + B(n) where the 
noise B(n) is a centered WSS random process. What happens is that in the 
absence of noise, we get a line of equation Y(n) = al + a2n, whereas in the 
presence of noise, we get a scatter plot, more or less spread out around this 
line. The problem will be to find the line that best fits the scattered points, 
the meaning of which will soon become clear. 
To do this, we start with the observation of X(n) over a time interval 
{O, ... , N -I}, and we assume that B(n) is a white, Gaussian, random process 
with an unknown variance a;. al and a2 are the two unknown parameters we 
are going to determine: 
1. give the expression of the probability density Px (xo, ... ,XN-l; al, a2, a;) 
of the random vector [X (0) 
X (N - 1)] as a function of the pa-
rameters al, a2 and a;. p x is called the likelihood of the observed data 
knowing the parameters; 
2. Gauss had the idea of choosing the values of al, a2 and a; such that 
Px(xo, ... , XN-l; al, a2, a;) would be maximum. al, a2 and a; are called 
the maximum likelihood estimators. We choose the following notations: 
a = [~~] 
, X = 
: 
and W = 
: 
[ 
X(O) 1 
r~ 
X(N - 1) 
i 
o 
1 
N-1 
Determine, as a function of X and W, the expression of a that maximizes 
the likelihood; 
3. write a function that eliminates the affine trend and only keeps the sta-
tionary part zero-mean. Test this function. 
COMMENTS: 

Chapter 7 - Random Processes 291 
- what exercise 7.1 teaches us is that in the case of a Gaussian hypoth-
esis, the maximum likelihood estimator coincides with the least square 
estimator; 
- the method explained here can easily be applied to any polynomial trend 
of the kind X(n) = ao + a1n + ... + aNnN + B(n). 
EXAMPLE 7.6 (Suppressing a seasonal trend) 
In many fields, such as meteorology, economics and biology, certain behaviors 
show a periodicity related to natural phenomena that are periodic themselves: 
the rotation of the Earth around the Sun, the rotation of Earth on its axis, 
etc. These behaviors are said to show a seasonal trend. A model can then be 
used to describe them as a sum of deterministic components representing this 
trend and of a zero-mean random process B(n), representing the variability of 
the phenomenon, which can be written: 
X(n) = a + bcos(27rfon - ¢) + B(n) 
In this context, the frequency fo is assumed to be known. Studying the 
process B(n) requires a preprocessing to eliminate the seasonal trend. In order 
to do this, we estimate a, b, and ¢, then subtract a + b cos(27r fon - ¢) to X (n) 
to obtain an estimation of the residual process B(n). A criterion often used 
for estimation is the one called the least squares criterion (see the following 
comment in exercise 7.1). Beginning with the observation of X(n) for n E 
{a, ... ,N -1}, we are trying to find the values of a, band ¢ that minimize the 
RMS deviation: 
N-1 
J(a , b, ¢) = L (X(n) - (a + bcos(27rfon _ ¢»)2 
n = O 
between X (n) and the expected seasonal evolution: 
1. determine the expressions of a, band ¢ that minimize J (a, b, ¢); 
2. write a function that eliminates the seasonal trend, leaving only the sta-
tionary part zero-mean. 
HINTS: 
1. let (31 = b cos ¢ and (32 = b sin ¢. We have: 
N- 1 
L (X(n) - (a + bcos(27rfon - ¢»)2 
n = O 
N-1 
L (X(n) - a - (31 cos(27rfon) - (32 sin(27rfon»2 
n = O 
If we successively set to zero the derivatives with respect to a, 
(31 and (32, we get: 

292 Digital Signal and Image Processing using MATLAB® 
- with respect to a: 
N -1 
L (X(n) - a - /31 cos(27rfon) - /32 sin(27rfon)) = 0 
n=O 
- with respect to /31 : 
N-1 
N- 1 
L X(n) cos(27rfon) - a L cos(27rfon) 
n =O 
n =O 
N-1 
N-1 
- /31 L cos2(27rfon) - /32 L sin(27rfon) cos(27rfon) = 0 
n=O 
n=O 
- with respect to /32: 
Let: 
u 
X 
C 
S 
N-1 
N-1 
L X(n)sin(27rfon) - a L sin(27rfon) 
n =O 
n=O 
N-1 
N-1 
-/31 L cos(27rfon)sin(27rfon) - /32 L sin2(27rfon) = 0 
n=O 
[1 
... 
[X(O) 
[cos(27r fo x 0) 
[sin(27r fo x 0) 
n=O 
cos(27rfo x (N - l))]T 
sin(27rfo x (N _l))]T 
With these notations, we can group the three previous deriva-
tives together to write a single matrix equation: 
which can also be written: 
(7.31) 
If M refers to the 3 x 3 matrix found in the left-hand side of 
equation 7.31, and if we assume M to be invertible: 
(7.32) 

Chapter 7 Random Processes 293 
In the case where foN » 1, it can easily be checked that 
we successively have U T C 
~ 
0, U T S ~ 
0, ST C 
~ 
0, 
C T C ~ 
N /2 and ST S ~ 
N /2. 
This means that M 
~ 
diag(N, N /2, N /2) for which we infer the following approxi-
mate expressions: 
a 
~ 
1 N- l 
N L X(n) 
n = O 
2 N- l 
N L X(n) cos(27rfon) 
n = O 
and fh 
~ 
2 N- l 
N L X(n)sin(27rfon) 
n = O 
2. save the function trendseason. m: 
function dx=trendseason(x,fO) 
%!=================================! 
%! Suppressing a seasonal trend 
%! SYNOPSIS : dx=TRENDSEASON(x,fO) 
%! 
x 
= Input sequence 
%! 
fO = Seasonal frequency 
%! 
dx = Residue 
%!=================================! 
x=x( : ) ; N=length(x) ; 
U=ones (N ,1) ; 
C=cos(2*pi*fO*(0:N-1)'); S=sin(2*pi*fO*(0:N-1)'); 
M=[ N U'*C U' *S ; C'*U C'*C C'*S ; S'*U S'*C S'*S]; 
theta=inv(M)*[U' ;C' ;S']*x; 
dx=x-[U C S]*theta; 
return 
Test the tendseason function by executing the following pro-
gram: 
%===== testtrendseason.m 
N=100; B=randn(N,l); a=4; fO=O .Ol; phi=pi/6; 
tseason=3*cos(2*pi*fO*(0:N-1) '-phi); 
X=a+tseason+B ; Res=trendseason(X ,fO) ; 
subplot(311); plot(B); grid ; set(gca, 'ylim' , [-44]) 
subplot(312); plot (X); grid ; 
subplot(313); plot(Res); grid ; set(gca , 'ylim' ,[-44]) 
Because foN = 1, the terms that do not belong to the diagonal of 
matrix M (see expression 7.32) are not negligible. You can check 
that the approximated formulas provide results with noticeable dif-
ferences. 

294 Digital Signal and Image Processing using MATLAB® 
Estimating covariance 
From now on, we will assume, except if specified otherwise, that the observa-
tion sequence has been previously processed in order to remove the mean and 
the possible tendencies. This means, according to expression 7.30, that the 
estimation of the autocovariance function from {X(O) , ... , X(N - I)} is given, 
for k E {O, ... , K - I}, by: 
~ 
1 N-k-l 
1 N-l 
Rxx(k) = N L X(n + k)X*(n) = N L X(m)X*(m - k) 
(7.33) 
n = O 
m = k 
Likewise, an estimation of the covariance function between two random 
processes X(n) and Y(n), both assumed to be WSS and zero-mean, is given 
for k E {O, ... , K - I} by: 
~ 
1 N-k-l 
1 N-l 
Ryx(k) = N L Y(n + k)X*(n) = N L Y(m)X*(m - k) 
(7.34) 
n = O 
m = k 
From a theoretical point of view, it can be shown, as we said at the be-
ginning of this paragraph, that for a very large category of WSS processes, 
the estimators given by expressions 7.33 and 7.34 converge, when N tends to 
infinity, to the true covariance [4]. 
In practice, k must however remain much smaller than the number N of 
observations. A practical rule is to choose k less than N /10. 
Positivity of the estimated covariance matrix 
Consider the first K values Rxx(O) , ... , Rxx(K -1) obtained with expression 
7.33. To construct an estimation of the covariance matrix of a WSS process, 
you only need the hermitian Toeplitz matrix, for which the elements of the first 
column are precisely Rxx(O), .. . , Rxx(K - 1). The matrix can be written: 
R= 
Rxx(O) 
Rxx(l) 
Rxx(K -1) 
Rxx( -1) 
Rxx(O) 
A simple calculation shows that: 
~ 
1 
H 
R=-D D 
N 
Rxx(-K + 1) 
Rxx( -1) 
Rxx(O) 
(7.35) 
(7.36) 

Chapter 7 - Random Processes 295 
with D H = 
X*(O) 
X*(l) 
X*(N - 1) 
0 
0 
0 
X*(O) 
X*(l) 
X*(N - 1) 
(7.37) 
0 
0 
0 
X*(O) 
X*(l) 
X*(N - 1) 
The fact that R can be written as D H D / N guarantees that R is both 
hermitian and positive. This is because for any vector a , we can write that: 
aHRa = ~ aH (DHD) a = ~ (Da)H(Da) = ~ vHv 
N 
N 
N 
where we let v = Da. We can conclude by noticing that the scalar v H v is 
the sum of the square moduli of the components of v, and is therefore positive. 
Obviously, in practice, you do not construct D to then calculate D H D . You 
calculate Rxx(k) for k = 0, .. . ,K - 1 with formula 7.33, then you use the 
toepli tz function of MATLAB® to store the obtained values in a matrix of 
the type 7.35. 
~ 
In the literature, this method for calculating R is called the correlation 
method. What we see is that, in a way, everything is as if we had padded the 
observed sequence on the left and on the right with (K - 1) zeros. Its major 
drawback is therefore to add false data, zeros to be precise, on both sides of 
the observed data. This is why when the length N of the sample is small, it 
is usually discarded, to the benefit of the covariance method which consists of 
choosing as the covariance matrix: 
with: 
R = _l_ D HD 
N -K 
r 
X *(K - 1) 
X *(K - 2) 
X'(O) 
X*(K) 
X'(K - 1) 
X *(l) 
X'(N - 1) 1 
X '(N - 2) 
X'(N - K) 
(7.38) 
The resulting covariance matrix remains, of course, positive, but it loses its 
Toeplitz structure necessary to certain fast inversion algorithms. 
There are two other methods for constructing D , padding with zeros either 
on the left or on the right. For example: 
r 
X*(K - 1) 
D H = 
X*(K - 2) 
X*(O) 
X*(K - 1) 
X*(l) 
X*(N - 1) 
o 
X*(N - 1) 
o 

296 Digital Signal and Image Processing using MATLAB® 
COMMENT: in the correlation calculation, each of the terms can be inter-
preted as a convolution, hence the idea to use the DFT for calculating the 
sequence of the covariance. We know that with a convolution in the time 
domain corresponds a product in the frequency domain, and the estimated 
auto covariance function can be seen precisely as the convolution of x(n) with 
x*( -n) , with which X(f)X*(f) = IX(fW corresponds by Fourier transform. 
However, bear in mind that in the DFT context, we know that the associated 
convolution is circular. 
7.4 Filtering formulae for WSS random pro-
cesses 
Filtering formula for the psd 
Let X(t) be a WSS random process, with the auto covariance function RXX(T) 
and the psd Sxx(f) fed into the input of a linear filter with the impulse response 
h(t) and the complex gain H(f). 
linear filter 
X(t) ~I 
h(t), H(j) ~ 
Y(t) 
Figure 7.6 - Linear filter 
We assume that h(t) is summable (BlBO stable filter). In the continuous-
time case, this can be expressed: 
llh(t)ldt < +00 
and in the discrete-time case5 : 
L Ih(t)1 < +00 
tEZ 
It can be proven [4] that the output random process Y(t) is WSS itself. Its 
mean is given by: 
my = mxH(O) 
(7.39) 
Therefore, it is zero-mean if the input signal is zero-mean. Its psd is given 
by the following expression: 
(7.40) 
5In this paragraph, t and T belong either to IR or to Z depending on whether the considered 
process is continuous-time or discrete-time 

Chapter 7 - Random Processes 297 
Notice that if we decide to use the distribution formalism, formula 7.40 can 
still be applied when the process is harmonic. Consider for example as the input 
signal of the filter the real harmonic process X(t) = "Lf= l Ak cos(27ffkt + <Pk), 
the psd of which is given according to 7.20 by: 
1 PIP 
Sxx(J) = 4 L a ~6(J 
+ h) + 4 L a~6(J 
- h) 
k= l 
k= l 
A direct calculation of the output signal Y(t) leads to: 
P 
P 
Y(t) = L AkH( - fk)e-2j7riktHk + L AkH(h)e2j7rikHih 
k= l 
k= l 
If we use 7.20, we get, for the psd of Y(t): 
P 
1" 2 
2 
4 ~ aklH ( - fk)1 6(J + fk) 
k= l 
(7.41) 
which can be identified with Syy(J) = IH(J)12Sxx(J) if we use the identity 
H(J)6(J - fa) = H(Jo)6(J - fa), where 6(J) refers to the Dirac distribution. 
Starting off with 7.41, we end up with the following expression of the autoco-
variance function: 
1 P
I
P 
RYY(T) = 4 L a~IH( 
- fkWe-2j7rikT + 4 L a~IH(hWe2j7rikT 
k= l 
k = l 
If the filter is real, H(-fk) = H*(h) and: 
P 
1" 2 
2 
RYY(T) = 2 ~akIH(Jk)1 
cos(27ffkT) 
k= l 
In the case where the input process is a white noise, the psd Sxx(J) is 
constant. It may be useful to write the Parseval formula again: 
{ 
r~: 
Ih(t)1 2dt = r~ : IH(J)1 2df 
(continuous time) 
"L-:
=~oo 
Ih(t)12 = J ~ 11 f22
IH(J)12df 
(discrete time) 
(7.42) 
which allows you to calculate the filter's output power by integrating the im-
pulse response either in time or in frequency. 
The output auto covariance function's expression is not as simple as the 
psd's. If we restrict ourselves to the discrete-time case, we have: 
+00 ( +00 
) 
RYY(T) = m~
oo 
n ~ oo h(n)h*(n - m) 
RXX(T - m) 
(7.43) 

298 Digital Signal and Image Processing using MATLAB® 
Filtering formulae for the interspectrum 
If the previous stationarity hypotheses are made, it can be proven that 
the processes X(t) and Y(t) have stationary covariance, meaning that 
IE {Yc(t + T)X;(t)} = RYX(T) (the index c refers to zero-mean processes) only 
depends on the time gap T. Once again this formula has an simpler expression 
in frequency. We have: 
Syx(f) = H(f)Sxx(f) 
(7.44) 
where Syx(f), which is called the interspectrum, refers to the Fourier transform 
of R YX (T). Note that this function has none of the psd's remarkable properties. 
In particular, there is no reason why it should be positive, or even real. If we 
restrict ourselves to the case of discrete-time random processes, we infer: 
+00 
RYX(T) = IE {Yc(t + T)X;(t)} = L h(m)Rxx(T - m) 
(7.45) 
m = -(X) 
Notice that if X(t) is white with a variance of 1, that is RXX(T) = 6(T), 
formula 7.45 can be simplified, and leads to: 
The impulse response coincides with the output/input covariance. This 
result can be used for estimating a filter's impulse response (see exercise 7.5). 
Exercise 7.2 (Smoothing filtering of noise) (see p. 467) 
Consider the filter h(n) = 1/8 for 0 :::; n :::; 7 and 0 otherwise. A white, 
zero-mean random process with a variance of 1 is fed into the input: 
1. determine the gain IH(fW of the filter; 
2. use this result to find the output process's spectrum and the output 
power; 
3. use this result to find the form of the output autocovariance function. 
Determine after which value k the output auto covariance function is null; 
4. write a program that simulates the filtering over 2,000 points of data, that 
evaluates the output auto covariance function using formula 7.33, and uses 
this to find the spectrum by an FFT calculation over 512 points; 
5. compare with the theoretical results. 

Chapter 7 - Random Processes 299 
Generating a random signal using white noise 
We are often faced with the problem of simulating the trajectory of a WSS 
process with a given spectrum. The functions randn and filter make it 
possible to construct such a trajectory. 
The randn function generates samples of Gaussian white noise with a vari-
ance of 1. Its spectrum is therefore constant, and equal to 1 in the (-1/2, +1/2) 
band. The WSS process filtering formula 7.40 shows that we can obtain the 
trajectory of the process Y (n) with a given spectrum, by properly filtering 
W(n). We have: 
Syy(J) = IH(JW x Sww(J) = IH(JW 
Usually, the complex gain H(J) = Hz(e 2j7rf ) is that of a filter whose 
transfer function is a rational function that can be written Hz(z) 
BAz)/AAz). This means we have to use the filter function with the com-
mand y=f il ter (b ,a, w), where a and b refer to the denominator and numer-
ator polynomials of the transfer function. 
When used like this, the filter function starts with zero initial conditions, 
creating at the beginning of the trajectory a transient part that does not cor-
respond exactly to the intended trajectory. One way of partly avoiding this is 
to spread out the first P values of the obtained signal, when the choice of P 
is directly related to the duration of the transient state of the filter's impulse 
response. For the numerator of the transfer function, this duration is simply 
the number of coefficients. For the denominator, we know that this value is 
related to the position of the poles with respect to the unit circle. A simple and 
practical rule consists of considering the modulus Pmax of the most resonant 
pole, that is the one closest to the unit circle, and to choose P such that P~ax 
is negligible compared with the root-mean-square of Y(n) . Remember that for 
a pole with the modulus p, the transient state decreases like pn (see impulse 
response of a filter on page 143). 
EXAMPLE 7.7 (Generating a random signal) 
Consider the process x(n) obtained as the output of the filter with the transfer 
function Hz(z) = 1/(1 + az- 1 ), where a is a real number with its modulus less 
than 1, the white noise W(n) with a variance of 1 being fed to its input: 
1. determine its psd's expression; 
2. write a program that generates a trajectory for the process X (n). 
HINTS: 

300 Digital Signal and Image Processing using MATLAB® 
1. the psd's expression is given by formula 7.40: 
2. type: 
Sxx(f) 
%===== ar1.m 
IH(f)1
2 
= (1 + ae2j7rf)~1 
+ ae-2j7rf ) 
1 
1 + 2a cos(27r f) + a2 
a=O.9; N=1000; W=randn(N,1); 
X=filter(1, [1 a] ,W); plot(X); grid 
Note that 0.9100 = 2.7 X 10-5 is negligible. Hence we can con-
sider that after the hundredth sample the obtained signal almost 
represents the trajectory of a stationary random process. 
Exercise 7.3 (Generating a band limited process) (see p. 468) 
Write a program that generates T = 1,000 samples of a real random process 
sampled at a frequency of 10,000 Hz, the psd of which is constant in the (-1,000 
Hz - 1,000 Hz) band and null beyond it. Its power is assumed to be 2 Watts. 
Exercise 7.4 (Pre-emphasis and de-emphasis) (see p. 469) 
When transmitting a signal through a channel subjected to noise, the signal-
to-noise ratio can be enhanced by adding to the emitter a filter Hp(f) , called 
the pre-emphasis filter, and to the receiver the inverse filter Hd(f) = 1/ Hp(f), 
called the de-emphasis filter. The choice of Hp(f) depends on the spectral 
properties of the signal and of the noise. 
Consider the real, zero-mean, second order stationary discrete-time process 
X(n), we will assume that its psd Sx(f) is known. This signal is corrupted by 
a zero-mean, WSS, additive noise B(n) for which the psd SB(f) is also assumed 
to be known (Figure 7.7). This type of situation is encountered in transmission 
channels of communication systems, but also in any processing that adds noise 
to the signal, such as the quantization operation 
tB(n) 
X(~I 
HpU) h~r-I 
H-d-U-)-=-l-/H-p-U---') ~n) 
+ W(n) 
Figure 7.7 - Pre-emphasis and de-emphasis system 
The output signal has the expression X(n) + W(n) where W(n) refers to 
the noise obtained by filtering the noise B(n). The goal is to determine Hp(f) 
(and hence Hd(f) = 1/ Hp(f) also) so as to minimize the power of W(n). But 
a constraint has to be imposed, since we can make the power of W(n) as small 

Chapter 7 - Random Processes 301 
as we want it to be: we only have to multiply Hp(j) by a very large factor A 
and to divide Hd(j) by that same factor A, thus dividing the power of W(n) 
by A2 while leaving X(n) untouched. This is why we are going to compare 
the SNRs obtained with and without the pre-emphasis/de-emphasis system. Po 
denotes the output power of the filter Hp(j): 
1. determine as a function of Hd(j) and SB(j) the expression of the power 
of the signal W(n). Use this result to find the expression of the signal-
to-noise ratio PPD of the system represented in Figure 7.7; 
2. determine as a function of Hp(j) and Sx(j) the expression of Po. Use this 
result to find the expression of the SNR P = Pol E(IB(n)l2) for a process 
that does not use the suggested pre-emphasis/de-emphasis system; 
3. let 9 = PPD/ p. The factor 9 can be interpreted as a gain: the higher it 
is, the better the suggested system. Determine the expression of g; 
4. using the Schwarz inequality, determine the expression of the filter Hp(j) 
for which 9 is maximum. 
Exercise 7.5 (Estimation of an FIR filter's impulse response) (see p. 
471) 
As you may remember, the formula 7.45 giving the output/input covariance 
Ryx(k) is: 
Ryx(k) = E {Yc(n + k)X;(n)} = h(k) * Rxx(k) 
(7.46) 
In the case where X (n) is a white process with the psd 0-2 , the expression 
can be simplified, leading to Ryx(k) = 0-2h(k) . In this exercise, we are going 
to use this result to estimate the impulse response of an FIR filter: 
1. determine again the expression we found relating the output/input co-
variance function Ryx(k) to the input auto covariance function Rxx(k), 
for a linear filter with a finite impulse response of length L, assumed to 
be known. Show that the vector h = [h(O) 
dots 
h(L -1)r is the 
solution to a matrix expression of the type Rh = r where R is a matrix 
constructed from Rxx(k) and r is a vector constructed from Ryx(k); 
2. write a program that performs an estimation of h based on the values of 
X and Y generated by the following program: 
%===== generepimp.m 
tps=(-16:1.2:15); h=sin(tps*(pi/5.8)) ./ tps*(pi/5.8); 
num=[0.3 0.4 -0.2 0.1]; den=[1 -0.8 +0.5]; 
x=filter(num,den,randn(1,300)); y=filter(h,1,x); 
This method, called the method of moments should be compared with 
the least squares method (see Chapter 9). 

302 Digital Signal and Image Processing using MATLAB® 
7.5 MA, AR and ARMA time series 
The search for models to describe random processes is at the core of signal 
processing, and the applications cover most of the applied fields. In this section, 
we will discuss models originating from the linear filtering of a white noise, and 
only discrete-time processes will be considered. 
7.5.1 
Q order MA (Moving Average) process 
Definition 7.10 (MA-Q process) An MA-Q process, MA for Moving Aver-
age, is the random process defined by: 
X(n) = Wen) + h Wen -1) + ... + bQW(n - Q) 
(7.47) 
where Wen) refers to a centered, second order stationary, white random process 
with a variance of a 2 and {b1 , ... ,bQ} is a sequence of Q coefficients. 
The process constructed in this manner turns out to be the mean weighted 
by the sequence {I, b1 , ... ,bo} of the last (Q + 1) input values. Everything 
happens as if this weighting sequence was applied to the input signal, which is 
why the process is called Moving Average. The process X(n) can also be seen 
as the output of a linear filter the impulse response of which is the sequence 
{I, b1 ... ,bQ}. Therefore, this FIR filter has the following transfer function: 
B(z) = 1 + hz- 1 + ... + bQz-Q 
(7.48) 
It has no poles, hence it is stable. 
The filter function can be used to obtain the trajectory for such a process. 
The following program generates 300 samples of an MA-2 process where b1 = 
1.5, b2 = -1.2 and a 2 = 1: 
%===== trajrna.rn 
B=[l 1.5 -1.2]; w=randn(1,300); 
x=iilter(B,l,w); plot(x); grid 
Relations between the model's parameters and the covariances for 
an MA-Q 
We are going to determine the relations between the model's parameters and 
the covariance of the process X(n), starting with the example of an MA-2 
process associated to the equation: 
X(n) = Wen) + h Wen -1) + b2W(n - 2) 
where Wen) is a white, centered, WSS random process with the variance a 2 . 
First, we have E {X (n)} = O. Hence the process is centered. Let us determine 

Chapter 7 - Random Processes 303 
the expression of R( k) = lE {X (n + k )X* (n)} as a function of b1 , b2 and a2 . 
Using linearity and the fact that W(n) is white, we successively get: 
R(k) = lE {X(n + k)X*(n)} = 0 for k::;-3 
R( - 2) = lE {X(n - 2)X*(n)} = a 2b2 
R( -1) = lE {X(n - l)X*(n)} = a 2 (bi + b1b2) 
R(O) = lE{X(n)X*(n)} = a 2(1 + Ibl l2 + Ib212) 
R(l) = lE {X(n + l)X*(n)} = a 2 (b 1 + b2bi) 
R(2) = lE {X(n + 2)X*(n)} = a 2b2 
R(k) = 0 for k ~ 3 
(7.49) 
This result can be generalized to any MA-Q process for which the autoco-
variance function has the expression (if we note that): 
{ 
, LQ- I'I b 
b' 
for 
0::; k::; Q 
a 
j = O 
Hlkl j 
R(k) = 
a 2 :zQ-1kl b* 
b 
for 
- Q ::; k ::; 0 
(7.50) 
) = 0 
)+Ikl) 
0 
for Ikl > Q 
We check that R(k) = R*( - k). The sequence of covariances of an MA-Q 
has 2Q + 1 non-zero terms. 
Notice, finally, that the system of equations we wish to solve is not linear 
with respect to the parameters bj . 
Obviously, equation 7.50 can be used for estimating the model's (Q + 1) 
parameters bj , by substituting the autocovariance coefficients R(k) with their 
estimates given by 7.33. Unfortunately the system of equations is of the second 
degree with respect to the parameters bj . This is why when estimating an MA, 
even a short one, it is usually preferable to approximate it with a long AR 
because, as we are going to see, the system is now linear. 
Spectrum of an MA-Q 
According to equation 7.47, an MA-Q process can be seen as the output of a 
filter the input of which is a white noise with the psd a 2 . Hence formula 7.40 
can be applied and leads, for the psd, to: 
(7.51 ) 
COMMENT: 7.51 shows that knowing 8(1), which is equivalent, by defini-
tion, to knowing the covariance coefficients, only allows us to determine the 
modulus of B(e2j7r1 ) . Because of theorem 4.6, we know that the roots of B(z) 
can be inside as well as outside the unit circle without it changing the value of 
IB(e2j7rf)l. Therefore, if we start with the covariance coefficients of the psd, or 

304 Digital Signal and Image Processing using MATLAB® 
in practice with their estimates, we have 2Q solutions for the polynomial B(z), 
all of them leading to the same spectrum S(f). If this is all we know, there is 
no reason why one of them should be chosen rather than another. However, if 
we have reason to believe, in a particular problem, that B(z) has all its roots 
inside the unit circle, that is if B(z) is minimum phase (definition 4.8), then 
B(z) can be identified. Unfortunately, in digital communications, this is never 
the case. Completely identifying B(z) requires the use of what is called higher 
order statistics, or HOS (higher implicitly means higher than 2). This rules out 
the Gaussian case for good, since in that case, the HOS are statistical functions 
of the second order. 
7.5.2 
P order AR (AutoRegressive) Process 
Consider the recursive equation: 
X(n) + alX(n - 1) + ... + apX(n - P) = W(n) 
(7.52) 
where W(n) refers to a white, centered, WSS random process with the variance 
(J2, and where {aI, ... , ap} is a sequence of coefficients. If we let: 
(7.53) 
then the signal X(n) can be seen as the output of the all-pole filter with the 
transfer function Hz(z) and the process W(n) as its input (Figure 7.8). 
W(n.:....) -----3,~~ 
X(n) 
~ 
~ 
Figure 7.8 - Generating an AR process 
It can be shown [4] that the recursive equation 7.52 has a single solution 
X(n), second order stationary if and only if the denominator polynomial 7.53 
is different from 0 for Izl = 1 (no poles on the unit circle). This results leads 
us to adopting the following definition: 
Definition 7.11 (AR process) A P order autoregressive process, or AR-P, 
is the only WSS process to the equation: 
X(n) + alX(n - 1) + ... + apX(n - P) = W(n) 
(7.54) 
where W (n) refers to a centered, second order stationary, white random process 
with a variance of (J2 and where the polynomial: 
A(z) = 1 + alz- l + ... + apz- P 
=1= 0 for Izl = 1 
(7.55) 

Chapter 7 - Random Processes 305 
The expression of this solution is: 
+00 
X(n) = L hkW(n - k) 
(7.56) 
k= -oo 
where hk is the sequence of the Fourier series expansion coefficients of the 
function H(J) = 1/A(e2j7rf ). 
In the case where A(z) -I- 0 for Izl 2: 1, the poles of Hz(z) are strictly inside 
the unit circle, hk = 0 for k < 0 and X (n) can be causally expressed as a 
function of W(n): 
X(n) = W(n) + hlW(n -1) + ... + hkW(n - k) + ... 
(7.57) 
Notice that the stationary solution to equation 7.54 is the same as the 
stable solution we obtained in the case of deterministic signals (see theorem 
4.3, page 128). Finally, remember that if W(n) is Gaussian, then X(n) itself 
is Gaussian since Gaussian nature is unchanged by linear transformations (see 
theorem 6.4). 
Spectrum of an AR-P 
Since an AR-P can be interpreted as the output of a filter fed with a white noise 
with the psd 0'2, formula 7.40 can be used to determine the psd's expression. 
This leads us to: 
0'2 
S(J) = 1 
2' f 
2' Pfl2 
1 + ale- J7r + ... + ape- J7r 
(7.58) 
As an example, let us plot the trajectory of an AR-2 process associated 
with the polynomial A(z) = 1 + alz- 1 + a2z-2, the two poles of which are 
conjugated, imposing that ai - 4a2 < O. Let p be the modulus of the two 
conjugated poles and ±¢; their respective phases. Starting off with p and ¢;, 
and by calculating the product and the sum of the roots, we get a2 = p2 and 
al = -2p cos(¢;) . The program trajAR.m displays a trajectory of this process. 
%===== trajar.m 
sigma=2; phi=20*pi/180; rho=O.9; 
a1=-2*rho*cos(phi); a2=rho*rho; 
w=randn(1,300); x=filter(sigma,[1 a1 a2],w); plot(x); grid 
EXAMPLE 7.8 (Noised sine function versus AR-2) 
Write a program: 
- that displays a sequence of N = 100 samples taken at the frequency Fs = 
1,000 Hz, from a signal Y(t) = s(t)+B(t), sum of a sine s(t) = sin(27fFat) 
with a frequency of Fa = 100 Hz and of a zero-mean, Gaussian, white 
noise B(t) with variance of O'~ = 0.04; 

306 Digital Signal and Image Processing using MATLAB® 
- that displays a sequence of N = 100 samples taken at the frequency 
Fs = 1,000 Hz, from an AR-2, solution of the equation X(n) + a1X(n -
1) + a2X(n - 1) = W(n), where W(n) is a white, centered, WSS process 
with the variance O"i:v and with al and a2 such that the filter has a 
resonance at Fo = 100 Hz (see equation 4.28) . Using 7.59, derive the 
value of O"i:v such that X (n) and Y(n) have the same power. 
Compare the results. 
HINTS: type: 
o 
- 1 
o 
- 1 
%===== sinusversusar2 .m 
N=100; FO=100; FS=1000 ; tps=(O :N-l)/FS ; sigmaB2=O .04; 
y=sin(2*pi*FO*tps)+sqrt(sigmaB2)*randn(1,N); 
%===== 
a2=O.98; al=-4*a2*cos(2*pi*FO/FS)/(1+a2); a=[l al a2]; 
%===== X and Y have the same power 
RO=1/2 +sigmaB2; Rl=-RO*al/(1+a2); R2=-al*Rl-a2*RO; 
%===== 
sigmaW=sqrt(RO+al*Rl+a2*R2); w=sigmaW*randn(1 ,N+l000); 
x=filter(l,a,w); x=x(1001:1ength(x)); 
subplot(211); plot(tps, y); grid 
subplot(212); plot(tps, x); grid 
2T 
3T 
4T 
5T 
6T 
7T 
8T 
9T 
101' 
Figure 7.9 - Noised sine function versus AR-2 process 
Figure 7.9 shows that the graph (a) presents irregularities but, how-
ever large the errors, periodogram (see paragraph 8.1.2) analysis is 
applicable to such curve, and, given a sufficient number of periods, 
should yield a close approximation to the period. On the other 
hand, there are not abrupt variations in the graph (b), but the 
amplitude varies within wide limits, and the phase is continually 

Chapter 7 - Random Processes 307 
shifting. Increasing the magnitude of W(n) simply increases the 
amplitude: the graph remains smooth. 
Relations between the model's parameters and the covariances for 
an AR-P 
Property 7.3 For a causal AR-P process, the relation between the model's 
parameters and the covariances R(k), with R(k) = R*( -k), are given by: 
R(O) 
R( -1) 
R( -P) 
R(l) 
R(O) 
(7.59) 
R(-l) 
R(P) 
R(l) 
R(O) 
and for k > P by: 
p 
R(k) = - L aiR(k - i) 
(7.60) 
i = l 
Equations 7.59 are called normal equations or Yule-Walker equations. We 
will see later on that they are directly related to the problem of linear predic-
tion. More precisely, we will show that: 
p 
X(n) = - L akX(n - k) 
k = l 
represents the best linear estimation in the least squares sense, of X (n) based 
on its past and that the prediction error, defined by: 
c(n) = X(n) - X(n) 
(7.61 ) 
is therefore equal to W (n). 
To establish relations 7.59 and 7.60, we start by multiplying the two mem-
bers of the recurrence relation 7.54 by X*(n - k), then if we consider its math-
ematical expectation, we get: 
E {(X(n) + ... + apx(n - P))X*(n - k)} = E {W(n)X*(n - k)} 
For k 2: 1, the second member is equal to zero, since on one hand, X(n - k) 
only depends on W (n - k), W (n - k - 1) . . . because of the stationary solution's 
causality, and on the other hand, W(n) is white. If we use the stationarity of 
X(n), and if we let R(k) = E {X(n + k)X*(n)}, then for any k 2: 1: 
R(k) + alR(k - 1) + ... + apR(k - P) = 0 
(7.62) 

308 Digital Signal and Image Processing using MATLAB® 
We get the same relation as 7.60. Furthermore, if we multiply the two 
conjugate members of the recurrence relation 7.54 by Wen) and if we consider 
the expectation, we obtain: 
lE {(X*(n) + alX*(n - 1) + ... + apX*(n - P))W(n)} = lE {IW(n)12} = a2 
The first member is reduced to lE {X*(n)W(n)} because of the causality of 
X(n) as a function of Wen) and because Wen) is white. As a consequence, we 
have lE {X*(n)W(n)} = a 2 . Replacing Wen) with X(n) + alX(n - 1) + ... + 
apX(n - P) leads us to: 
R(O) + a1R( -1) + ... + apR( -P) = a 2 
(7.63) 
If we stack 7.63 and the P relations we obtained from 7.62 for k = 1, 2, ... , 
P, we get 7.59. 
You can recognize the (P + 1)-th order covariance matrix of the process X (n) 
in expression 7.59. This matrix is hermitian in the general case, R(-j) = R*(j), 
and is symmetrical if the process X (n) is real. 
An important result states that, because the covariance matrix is a positive 
Toeplitz matrix, the solution to equation 7.59 is such that the polynomial 
A(z) = 1 + alz-1 + ... + apz- P has all its roots inside the unit circle. This 
result is still true if the covariances are replaced with their estimates, so long 
as the matrix remains a positive Toeplitz matrix, as it is the case for the 
correlation method. The fact that A(z) has all its roots inside the unit circle 
ensures that the filter with the transfer function 1jA(z) has a causal and stable 
representation. This filter is used for creating X(n) from Wen). We will see 
an application for it in speech processing. 
Conversely, if we know the sequence {aI, ... , ap} et a2, equation 7.59 allows 
us to calculate the covariance coefficients R(k). The equations we have to solve 
are linear. Thus, in the case of a real random process for which R(k) = R( -k), 
we get the expression: 
ap 
o 
o 
1 
(7.64) 
We have a system of (P + 1) linear equations with the unknowns R(O), 
... , R(P). Once R(O), ... ,R(P) have been calculated, 7.60 can be used to 
calculate the values of R(k) beyond P, and the hermitian symmetry can be 
used for k < o. 
EXAMPLE 7.9 (First order AR model) 
Consider the first order, real AR process, solution of the equation X(n) + 

Chapter 7 - Random Processes 309 
a1X(n - 1) = W(n) where a1 is real with its modulus strictly less than 1, and 
W(n) is a white, centered, WSS process with the variance a2 : 
1. write the Yule-Walker equations; 
2. use them to find the covariances as a function of a1 et a 2 ; 
3. use this result to find a1 and a2 as a function R(O) of R(l). 
HINTS: 
1. the Yule-Walker equations are, for P = 1: 
(7.65) 
where we used R(l) = R( -1). And for k > 1, equation 7.60 
leads to R(k) = -a1R(k - 1); 
2. by solving equations 7.65 with respect to R(O) and R(l), we 
get: 
2 
R(O) = 1 ~ a2 
1 
Using the recurrence relation 7.60 then, for k < -1, noticing 
that R(k) = R( -k), leads us to: 
(7.66) 
3. we can also find a1 and a2 from R(O) and R(l): 
R(l) 
R2(1) 
and a 2 = R(O) ---
a1 = - R(O) 
R(O) 
(7.67) 
These expressions can be used to estimate the parameters a1 
and a 2 from a sequence of N observations. All we need to 
do is replace the covariance coefficients with their respective 
estimates (equation 7.33), and we get: 
0; = _ 2:::02 X(n + l)X(n) and &2 = 2:::01 X2(n) (1 _ 0;2) 
1 
", N-1 X2( ) 
N 
1 
Un= O 
n 
EXAMPLE 7.10 (Generating an exact Gaussian AR) 
We have already seen how to generate a sequence of random processes with 
given spectra by filtering a white sequence. We discussed the problem regarding 
the transient state. In the case of a Gaussian AR process, we will see that an 
exact process trajectory can be generated. This can be used to initialize the 
previous method. Write a program: 

310 Digital Signal and Image Processing using MATLAB® 
- that picks M poles at random inside the unit circle and then use the 
MATLAB® function poly to calculate the coefficients of the polynomial 
using these poles and their conjugates; 
- that calculates, using equations 7.64, the covariances of an AR process 
defined by A(z) and (]'2; 
- that generates 2M samples of the AR process. Use a method similar to 
that of example 7.5 where we generated a colored noise using a white 
noise and the square root of its covariance matrix; 
- that constructs a trajectory with the length T of this process from the 2M 
previous samples. Use the iil tric function6 to calculate, as a function 
of the 2M previous samples, the initial state that must be given to the 
filter function: 
- that plots the sequence of covariance; 
- that plots in 2D the couples {Xi,Xi+l} . 
H INT S : type: 
%===== exactar.m 
% Polynomial generation 
sigma2=2 ; M=5 ; rho=O.45*rand(1,M)+O.5; phi=pi*rand(1,M)/4 ; 
rac=rho . * exp(j*phi); rac=[rac conj(rac)]; 
coeff=real(poly (rac)) ; LgAR=length(coeff)-l; 
%===== calculation of the exact covariances 
cl=[coeff zeros(l,LgAR+l)]; c2=[zeros(1,LgAR) coeff] ; 
Al=toeplitz([coeff(LgAR+l) ;zeros(LgAR,l)], . . . 
coeff(LgAR+l :-l :l)) ; 
Al=Al( : ,LgAR+l :-l : l) ; 
A2=toeplitz(coeff, [coeff(l); zeros(LgAR,l)]) ; 
rx=(Al+A2)\[sigma2; zeros(LgAR,l)]; rx(1)=rx(1)*2 ; 
Rcov=toeplitz(rx); 
%===== square root of the covariance matrix 
MatM=sqrtm(Rcov(l :LgAR,l:LgAR)); 
%===== generating the first Ncoeff-l values of X(k) 
WO=randn(LgAR,l); XO=MatM*WO; ZO=filtricII(l,coeff,O,XO); 
T=200; W=sqrt(sigma2)*randn(T,1); 
Xf=filter(l,coeff,W , ZO); LXf=length(Xf) ; 
Xf=Xf-mean(Xf); R=2*LgAR; covX=zeros(R,l); 
for rr=l :R, covX(rr)=Xf(rr:T) ' *Xf(l:T-rr+l)/T; end 
subplot(221) ; mycirc=exp (2*j*pi*(O:100)/100); 
6the signal toolbox provides the filti c function 

Chapter 7 - Random Processes 311 
plot(real(mycirc),imag(mycirc)); 
hold on; plot (real(rac),imag(rac), 'X'); 
hold off; axis('square ' ); grid 
subplot(222); plot(Xf); grid 
subplot(223); stem(covX/covX(l)); 
subplot(224); plot(Xf(1:LXf-l),Xf(2:LXf), I. I); grid 
A high correlation between consecutive samples corresponds to 
poles near the unit circle and gives a 2D-plotting in which points 
are gathered around a straight line. 
7.5.3 ARMA (P, Q) process 
ARMA processes are obtained using an AR structure and an MA structure in 
series. The process is the solution to the recursive equation: 
X(n) + alX(n - 1) + .. . + apX(n - P) 
= W(n) + b1 W(n - 1) + ... + bQ W(n - Q) 
(7.68) 
where W(n) refers to second order stationary, centered, white random process 
with the variance a 2 and where {aI, . . . , ap} and {h, .. . , bQ } are two sequences 
of coefficients. Let: 
(7.69) 
It can be proven that equation 7.68 has a single, second-order stationary 
solution X(n) if and only if the denominator's roots, that is the poles of the 
transfer function Hz(z), have a modulus different from 1. In the case where 
A (z) i- 0 for I z I ~ 1, the poles of Hz (z) are strictly inside the unit circle, X (n) 
can be causally expressed as a function of W (n). 
By definition, an ARM A (P, Q) process is the stationary solution to the 
recursive equation of the type 7.68. 
Spectrum of an ARMA-(P, Q) 
Because an ARMA process can be interpreted as the output of a filter with a 
white noise with the psd a 2 as its input, we can use formula 7.40 to determine 
an expression of the psd. This leads us to: 
(7.70) 

312 Digital Signal and Image Processing using MATLAB® 
Comments 
Using an ARMA process as a model for describing an observation amounts to 
assuming that its spectrum is a rational function. This is why it can seem 
restrictive to assume that the second order parameters of an observation only 
depend on a finite number of parameters. From the engineer's perspective, the 
great "universal" nature of this model is due to the fact that rational transfer 
functions make it possible to approximate a very large number of functions, and 
quite naturally playa role in electric, electronic, or even mechanical devices, 
often by way of a constant coefficient linear recursive equation. 
Furthermore, any ARM A of MA process can be approximated with an AR 
order of a high enough order. This result is fundamental for practical applica-
tions, since if among these three models the wrong one is chosen, a reasonable 
accuracy can still be achieved by taking a high enough order. However, it is 
easily conceivable that a process for which the spectrum has "deep valleys" will 
require less parameters if the MA model is used to represent it rather than an 
AR model. And conversely for "high peaks". 
Finally, remember that estimating the coefficients of an MA model usually 
is not simple, since the relations (7.50) between the model's coefficients and 
the covariances are not linear, whereas they are for an AR model. This is why 
we will only be estimating the parameters of an AR model (the linear relation 
between the ak and R(k) is given by equation (7.59)). 

Chapter 8 
Spectra Estimation 
The object of this chapter is mainly a discussion of the power spectral density's 
(psd) estimation. In this field, it is customary to separate two cases: 
- when the statistical properties of the observation depend on a finite, and 
usually small number of parameters, the model is said to be parametric. 
To be more precise, it means that knowing the few useful parameters 
is enough to find the exact probability distribution of the observation. 
We have already encountered an example of the parametric model: the 
AR-P model in the case of a white Gaussian input. Knowing the (P + 
1) parameters {aI, . . . , ap, 0"2} is enough to determine the probability 
distribution. Without the Gaussian hypothesis, and although it is no 
longer possible to write the probability distribution of the observation 
precisely, it is still possible to estimate some useful quantities, such as its 
spectrum, given a finite number of parameters: this is sometimes called 
a semiparametric model; 
- otherwise, the model is said to be non-parametric. In the first paragraph 
of this chapter we will study a situation in which the only hypothesis is 
that the process is WSS. Knowing the spectrum requires the estimation 
of an infinity of parameters, that is the set of covariance coefficients. 
8.1 
Non-parametric estimation of the psd 
8.1.1 
Estimation from the autocovariance function 
We have seen that expressions 7.33 can be used to estimate R(k) from a series 
of N observations X(1), .. . , X(N). Using the resulting estimate of the auto-
covariance function and the definition 7.13 of the psd, an estimate of the latter 

314 Digital Signal and Image Processing using MATLAB® 
can be obtained by: 
K-l 
S(f) = L 
R(k)e-2j7rjk 
(8.1) 
k= -(K-l) 
For a real WSS random process, we can write 8.1 as follows: 
K-l 
S(f) = R(O) + 2 L R(k) cos(27rkf) 
(8.2) 
k= l 
since R(k) = R( -k). 
COMMENTS: 
- even if we have at our disposal the actual values of the auto covariance 
function (assumed to be with infinite support), the fact of restricting, in 
the calculation of S(f), the sequence R(k) to k E {-(K -1), ... ,(K -In 
amounts to multiplying {R(kn by the rectangular window wT(k) = l(k E 
{ - (K - 1), ... , (K - I)} ). This operation causes unwanted ripples, as we 
saw in Chapters 3 and 4. Because the lobes can have positive or negative 
values, this can result in negative values for the psd estimate. Losing the 
positive nature (see property 7.1) of the psd is not advisable. To avoid 
such a phenomenon, the triangular window, or Bartlett window can be 
used instead of the rectangular window, the expression of which is: 
( 
Ikl) 
wb(k) = 
1 -
K 
l(k E {-(K - 1), . . . , (K - In) 
(8.3) 
It ensures the positive nature of the result, because the triangle func-
tion is obtained by convolution of the rectangular window with itself. 
The DTFT of the sequence wb(k) is therefore the function sin2 (-7r(2K -
l)f) j sin2 ( 7r f), which is always positive. Another commonly used window 
is the Hamming window, the expression of which, as you may remember, 
is: 
wh(k) = [0.54 + 0.46 cos (K7r~ 1)] l(k E 
{-(K-1), . . . ,(K-1)}) (8.4) 
- another important element is the choice of the number K of estimated 
covariance points compared with the number N of observations. Consider 
the case where N -+ 00 (large samples). If K remains constant, the 
covariance coefficients (expression 7.30) become more and more precise, 
but the windowing effect remains. Hence the idea of increasing K as N 
increases, but much "slower" than N. For example, we can take K = ANa 

Chapter 8 - Spectra Estimation 315 
with A >:::: 1/10 and ex < 1. 
That way, when N tends to infinity, a 
number K, that tends to infinity, of covariance points are calculated while 
ensuring that the number of values used for estimating each covariance 
point also tends to infinity. 
The following function estimates the spectrum by DFT of K covariance 
coefficients estimated from N observations: 
function sf=covtodsp(x,K,wintype,Lfft) 
%!====================================================! 
%! Estimating the spectrum from the covariances 
! 
%! SYNOPSIS: sf=COVTODSP(x,K,wintype,Lfft) 
%! 
%! 
%! 
x 
input sequence 
K 
= number of estimated covariances 
wintype = 'r', 'h' ou 'b' 
%! 
for 'rectangular', 'hamming' or 'bartlett' 
%! 
Lfft 
= FFT size 
%! 
sf 
= PSD 
%!====================================================! 
if nargin<4, Lfft=1024; end 
if nargin<3, wintype='r'; end 
if nargin<2, error('Parameters missing'); end 
N=length(x); x=x(:); rx=zeros(l,K); x=x-ones(l,N)*x/N; 
if Lfft<K, Lfft=pow2(floor(log2(K))+1); end 
%===== estimating the K covariances 
for ii=l:K, rx(ii)=x(l:N-ii+l)'*x(ii:N); end 
rx=rx/N; 
%===== windowing 
if wintype (1) == 'b', 
% Bartlett 
rx=rx .* (K:-l:1)/K; % Hamming 
elseif wintype(l) == 'h' 
rx=rx .* (O.54+0.46*cos(pi*(O:K-l)/K)); 
end 
%===== using the hermitian symmetry property 
rx(1)=rx(1)/2; sf=fft(rx,Lfft); sf=2*real(sf); 
return 
The program is designed to process complex signals. In this case, the spec-
trum is of course real and positive, but no longer has the even symmetry any-
more. Any of the rectangular, Hamming or Bartlett, windows may be used 
by assigning to fentype one of the three values 
I r I, 
I h I or 
I b I . Calculat-
ing the psd uses the hermitian symmetry property of the covariance function: 
remember (see exercise 2.2 in Chapter 2) that to calculate the DFT of the 
sequence, completed by hermitian symmetry, all you have to do is take the 
real part multiplied by 2 of the monolateral sequence after having divided the 
first element by 2. The behaviors for a Bartlett window are illustrated by the 
spectra represented in dB in Figures 8.1 and 8.2 for two values of the number 

316 Digital Signal and Image Processing using MATLAB® 
of covariance estimates, K = 25 and K = 150. They are obtained using the 
program: 
%===== testcovtodsp.m 
N=500; Lfft=1024; freq=(0 :Lfft-1)/Lfft; 
%===== complex process generated by the all-pole filtering 
% 
of a white noise 
a=[l -2 .4788 3.0905 -2.0646 0.6856]; 
sw=[l;O*j]; Pw=sw'*sw; 
w=randn(N,2)*sw; x=filter(l,a,w); 
%===== K and the window type can be modified 
K=25; sest=covtodsp(x,K, 'b' ,Lfft); 
sestlog=10*log10(sest); 
sth =10*log10(Pw * absC1 .1 fft(a,Lfft» 
. -2); 
plot(freq,sth,freq,sestlog, 'r'); grid 
set(gca, 'xlim' , [0 1/2]) ; % x is real 
The real signal x is generated by a white noise filtered with N = 500 values. 
The theoretical spectrum is given by the filtering formula, equation 7.40. 
25 (dB) ,-----------------------, 
,", 
N=500 K=25 
20 ----- -~- / 
- "'., ---- ~ , ,:,; 
-- -~- -----; ------: ------,-- ----: ------• ------
15 
10 
I 
I 
I 
I 
j 
I 
I 
I 
I 
;r -:- - - - - - .., - - - - - - ~ - - - '\ -:- - - - - - ..., - - - - - - ~ - - - - - - 1-
-
-
-
-
-
~ -
-
-
-
-
-
.,. -
-
-
-
-
-
: 
: 
\ 
I 
I 
: 
-----,' 
----- r ----------- -, 
, 
" 
5 
------~------
~ ------t------~-
\
~ , - - ------r-----
o 
' 
, 
, 
'
" 
, 
'-------:------;------
------:------;------:------i----T'-,J------r-
-5 
' -----,',----- ,-
------ , 
I 
.. " ..... 
: 
-10 
I 
I 
I 
I 
I "  
-----:------ f ------
------;- -----, ------: ------;- -----, ---- - ~~~~~ 
- ~odel 
, 
-15 ------:- ----- .1 ______ ~ ______ : ______ J ______ ~ ______ ':> .. ;: ::: __ ~ ______ 1 _____ _ 
I 
I 
I 
I 
~ 
_ ,_ 
-20 ~--
~
'----~--~
' ----
~
' ----~--
~ '----~--~
' ----~~~ 
o 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
Figure 8.1 - Non-parametric estimation of an AR-4 spectrum based on K = 25 
estimates of the covariance coefficients. The length of the sample is N = 500. The 
window used is the Bartlett window. The dashed line is the theoretical psd. 
As you can see, when the number of estimates of the covariance coefficients 
increases, the estimated spectrum's fluctuations are "closer" to the theoretical 
spectrum, but the fluctuations have higher amplitudes. You can easily check 
with the program that the use of the rectangular or Hamming windows does 
not ensure that the estimated psd is positive (if not, the program returns an er-
ror message when the command plot (freq, sth, freq, sest, 'r') is executed, 
since sest contains complex numbers because of the fact that the logarithm 
of a negative number is a complex number). A complex signal can be used by 
changing for example sw= [1; 0* j] to sw= [1; 3*j] and suppressing the last line. 

Chapter 8 - Spectra Estimation 317 
25(dB)~----------------------------------~ 
20 
15 
10 
5 ----- , 
----- , -
o -----, ------------,-
-5 
------:------
~ ------l_-----
, 
, 
, 
, 
, 
, 
, 
------
I -------------
~
-
-10 
, 
, 
-15 _____ J 
_____ , ______ ,
_ 
_ ___ , ______ ,
_ 
_ __ J __ 
~ ~
: - -------
-20L---~--~--~--~--~--~--~--~--~~~ 
o 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
Figure 8.2 - Non-parametric estimation of an AR-4 spectrum based on K = 150 
estimates of the covariance coefficients. The length of the sample is N = 500. The 
window used is the Bartlett window. 
8.1.2 Estimation based on the periodogram 
Rather than to use the DTFT of the covariances to estimate the psd, an intu-
itive idea would be to start off with the Fourier transform of a trajectory, or 
of a portion of a trajectory and to calculate its square modulus. This leads to 
the following definition of the periodogram. 
Definition 8.1 (Periodogram) Let X(n) be a centered W88 random process. 
A periodogram is the random function of f E (0,1) defined by: 
(8.5) 
You would think that IN(J) might be a good estimator of the psd 8(J) 
(definition 7.7) of the process X(n) assumed to be WSS, but in fact not at all. 
Although the mean lE {IN(J)} tends to the "true" value S(J) when N tends to 
infinity, the square deviation lE {IIN(J) - S(JW} does not tend to zero when 
N tends to infinity. 
As can be seen (expression 7.17), the mathematical expectation of I N (J) 
tends to 8(J). Hence the periodogram is an asymptotic unbiased estimator of 
the psd: for N high enough, IN(J) fluctuates around the "true" value 8(J). 
However, it can be proven, and we will assume so, that the amplitude of 
the fluctuations, that is lE {IIN(J) - 8(J)12}, does not tend to 0 when N tends 
to infinity. To be more precise, what can be shown is that, under very general 
hypotheses, this quantity can have the same order of magnitude as the value 
we wish to estimate. 

318 Digital Signal and Image Processing using MATLAB® 
The periodogram fluctuates around the true psd. Even if N is very high, 
the amplitude of the fluctuations still has the same order of magnitude 
as the psd we wish to estimate. 
The following program illustrates this behavior: 
%===== fluctperio.m 
Lfft=1024; fq=(O:Lfft-l)/Lfft ; 
w=randn(1,1000); b=[l 1.2 0.9]; a=[l -1.1 0.92]; 
PSDth= 20*log10(abs(fft(b,Lfft) ./ fft(a,Lfft»); 
x=filter(b,a,w); 
%===== process 
lxt=[100 200 500 1000]; 
for ii=1:1ength(lxt) 
end 
xt=x(1:1xt(ii»; 
per=20*log10(abs(fft(xt,Lfft»)-10*log10(lxt(ii»; 
subplot(2,2,ii); plot(fq,per,fq,PSDth, 'w'); grid 
axis([O .5 -30 40]); 
title(sprintf('N = %d' ,lxt(ii»); 
The samples of the random process X(n) are obtained by filtering white 
noise with a variance of 1. Based on the transfer function of the filter Hz(z) = 
(1 + 1.2z- 1 + O.9z- 2 )/(1 - 1.1z - 1 + O.92z - 2 ), the expression of the theoretical 
psd of the process X (n) is: 
Figure 8.3 shows the periodograms for four values of N as well as the the-
oretical psd. The periodograms fluctuate around the exact psd and the ampli-
tude of these fluctuations does not seem to decrease when N increases. 
40 dB 
20 
0 
-20 
0 
0.1 0.2 0.3 
0.4 0.5 
0 
0.1 
0.2 
0.3 
0.4 0.5 
40 
20 
0 
-20 
Figure 8.3 - Fluctuations of the periodogram for several values of N 

Chapter 8 - Spectra Estimation 319 
Therefore, there is no point in directly using the periodogram for estimat-
ing the spectrum. However, in practice, every good estimator of the psd is 
constructed from the periodogram. We are going to explain two methods: the 
smoothed periodogram and the averaged periodogram. 
Frequency smoothed periodogram 
The periodograms in Figure 8.3 show that the values obtained in several neigh-
boring frequency points fluctuate around the actual value: some are very close, 
whereas others are very far, hence the idea of calculating a frequency mean. To 
be more precise, consider an integer M and a sequence WM,N(k) of weighting 
coefficients such that: 
1. for any k, WM,N(k) = Wl\I,N(-k) and WM,N(k)::::: 0; 
2. L lkl::;M WM,N(k) = 1; 
3. L lkl::;M Wk,N(k) -+ 0 when N -+ 00; 
4. [L lkl::;M k2WM,N(k)] /N 2 -+ 0 when N -+ 00 . 
If a relation of the type: 
M = N cx with ex < 1 
(8.6) 
is chosen, then the above conditions are met. This is particularly the case for 
the rectangular window defined by: 
1 
WM,N(k) = 2M + 11(k E {-M, . .. , M}) 
and of the triangular window, or Bartlett window, defined by: 
1 ( 
Ikl) 
WM,N(k) = M 
1- M 
l(kE{-M+l, ... ,M-l}) 
In the case of a rectangular window, condition 3 can be expressed: 
1 
1 
L (2M + 1)2 = 2M + 1 
Ikl::;M 
If M tends to infinity with N, then this condition 3 is satisfied. Condition 
4 can be expressed: 

320 Digital Signal and Image Processing using MATLAB® 
The condition is satisfied if M / N -+ 0 at infinity, which is the case with the 
conditions set forth. 
Then we calculate the periodogram of the sequence {X(O), ... , X(N - I)} 
at the points of frequencies k / N . Finally, the spectrum is estimated according 
to the expression: 
(8.7) 
To deal with the side effects, remember that IN(f) is periodic with period 
equal to l. 
Obviously, expression 8.7 leads to a decrease in the variance but on the 
other hand adds bias. A detailed study of the properties of various window 
and of the compromise between bias and variance can be found in [4]. We will 
only be considering the triangular window. 
EXAMPL E 8.1 (Smoothed periodogram) Write a function that smooths 
the periodogram using either a rectangular window or a triangular window, 
with a length of 2m + 1 and with the sum of its coefficients equal to l. 
Making the choice of L = N avoids withdrawing the empirical mean since 
2:,::01 /-iC2j7rkn/N = 0 for all k i- 0 mod N . To make the choice of the win-
dow's length automatic, you can take M = N 2/ 5 where N is the length of the 
signal. 
H INTS: type: 
function [sf,frq]=smperio(x ,M,window) 
%!================================================! 
%! Smoothed periodogram 
! 
%! SYNOPSIS : [sf,frq]=SMPERIO(x,M, window) 
%! 
x 
= input sequence 
%! 
M such that the window length is 2*M+l 
%! 
window 
'r' for rectangular 
%! 
't' for triangular 
%! 
frq 
frequencies for the estimated PSD 
%! 
sf 
spectrum 
%!================================================! 
if nargin<3, window='t'; end 
x=x(:); N=length(x); 
if nargin<2, M=N- (2/5); end 
sf=zeros(N-2*M+l,1); 
if window=='t' 
Wf=[(l:M+l) (M:-l:l)]/(M+l) - 2; 
else 
Wf=ones(1,2*M+l)/(2*M+l); 
end 
Periodogram=abs(fft(x)) . - 2/N; % Periodogram 
frq=(M+l :N-M-l)/N; 
sf=filter(Wf,l,Periodogram) ; sf=sf(2*M+2 :N) ; 

Chapter 8 - Spectra Estimation 321 
Averaged periodogram 
This method, suggested by P. Welch [37], consists of cutting up the signal in 
blocks and to calculate a mean of the different periodograms obtained for each 
block. To be more explicit, the algorithm is as follows: 
1. the sample {X (n n of length N is divided in L blocks of the same 
length K with overlapping; 
2. a weighting window is applied to each of the L blocks. The result-
ing sequences are denoted by {Xe(kn with C = (0 : L - 1) and 
k = (0: K - 1); 
3. the L periodograms are calculated, as well as their mean: 
(8.8) 
This algorithm calls for a few comments: 
1. numerically, 8.8 is calculated on a finite number of values of f = m/ M 
and m E {O, . . . ,M - I} using the fft function of MATLAB®; 
2. it can be shown, with relatively general conditions, that SN(f) is a spec-
trum estimator the variance of which tends to 0 when N tends to infinity. 
One condition in particular is that L also has to tend to infinity, but not 
as fast as N. To make the choice of L automatic, you can take for example 
L = N 1/ 3 or L = N 2/ 5 ; 
3. in practice, the variance is reduced by choosing a large value for L. How-
ever, for a given value of N, increasing the number L of periodograms 
amounts to reducing the number of points K = N / L of each analysis win-
dow and at the same time to reducing the frequency resolution. You may 
remember that this is because when a signal is observed over a duration 
of K, you cannot "see" frequency differences of less than 1/ K. The con-
sequence is that the various periodograms show a discrepancy with the 
actual spectrum: this is called a bias. The conclusion is that choosing L 
is a compromise between bias and variance; 
4. the absence of weighting in expression 8.8 amounts to multiplying the 
samples X(k) of the signal by the rectangular window: 
1 
wr(k) = v'K1(k E {O, ... , (K -In) 

322 Digital Signal and Image Processing using MATLAB® 
the energy of which is L:k w;(k) = 1. This causes unwanted ripples to 
appear, related to the side lobes of its DTFT, hence the idea of using a 
different window to reduce these ripples. The consequence, of course, is a 
decrease in the resolution related to the main lobe. Generally speaking, 
the results are the same as those already found when studying the spectral 
analysis of deterministic signals, or filter design: 
- the wider the main lobe is, the more the details of the spectrum will 
be rubbed out, 
- the higher the second lobes are, the stronger the induced ripples 
will be. This is particularly noticeable in the areas of the spectrum 
showing few variations. 
The Hamming window is very often used: 
wh(k) = ex (0.54 - 0.46 cos(27rk/ K)) l(k E {O, ... , (K - I)}) 
w here ex is chosen such that L: k w~ (k) = 1; 
5. multiplying some of the samples by very small weighting coefficients 
(about 0.08 for the smallest values of the Hamming window) gives these 
samples a very unimportant role in the calculation. This is why P. D. 
Welch (1967) had the idea of choosing the intervals so they would over-
lap. The most commonly used overlap factor is 50%. For example, if 
N = 1,000 and K = 200, we get the following intervals: 
I Xl . ' " ,200 II X201 . · · · , X400 II X401, '" 
, X60 0 II X60 1 , '" 
, X soo II XSOl,' " , XIOOO I 
I XIOl,···,X300 II X301"",X500 II X50l>···,X700 II X70l>···,XgOO I 
Each sub-interval is then weighted by the appropriate Hamming window. 
The nine periodograms, and then the mean are calculated; 
6. averaging the periodograms does not completely eliminate the fluctua-
tions. To be more specific the reference [22] gives a {3% confidence in-
terval involving what is called the x-square distribution. If the latter is 
approximated by a Gaussian distribution, we infer that the spectrum has 
a {3% chance of being in the interval: 
. h 
V2erfinv({3) 
WIt 
"Y = 
VL 
(8.9) 
where Sw(J) is the periodogram averaged using the Welch method (50% 
overlap). The erfinv({3) function is the inverse function of the error func-
tion, which can be called in MATLAB® using the command erfinv. 
For {3 = 95% and L = 30, we get, in decibels, the confidence interval 
(Sw(J) - 1.33 , Sw(J) + 1.92). 

Chapter 8 - Spectra Estimation 323 
Exercise 8.1 (Estimating the spectrum using the Welch method) 
(see p. 472) 
1. write a MATLAB® function that estimates the spectrum using the Welch 
method, with as the input the signal to be analyzed, the type of window 
(rectangular or Hamming) , and the number of points of the spectra. The 
function description will then be: 
%!=====================================================! 
%! SYNOPSIS: [sf ,gamma] =WELCH(x, lnwin , wtype ,Lfft ,beta) 
%! 
x 
Input sequence 
%! 
lnwin = cnalysis window length 
%! 
wtype 
cindow: h(ham) or r(rect) 
%! 
Lfft 
= FFT length 
%! 
beta = confidence parameter 
%! 
gamma = confidence interval (100*beta%) 
% ! 
sf 
= spectrum 
%!=====================================================! 
2. using the filter function, generate a signal corresponding to a given 
spectrum. Use the previous function to estimate the psd of this test sig-
nal. Compare the result with the spectrum obtained with the smperio.m 
function of example 8.1. Compare it with the theoretical spectrum. 
Exercise 8.2 (Spectrum of a binary signal) (see p. 473) 
Write a MATLAB® function that associates with each term of the sequence an 
of independent random variables with possible values -lor 1, either the signal 
g(n) or the signal -g(n) respectively. The signal g(n) is a rectangular impulse 
made up of a sequence of Tl = 10 values equal to A = 5, followed by a sequence 
of T2 = 25 values equal to O. Figure 8.4 shows this signal for a sequence of 
seven successive values of an. This signal could originate from the sampling of 
a "computer" signal used for transmitting a sequence of bits. 
Use the welch. m function to estimate the spectrum of the signal associated 
with a sequence of 1024 values. Compare with the theoretical spectrum the 
expression of which is given by: 
(8.10) 
where T = Tl + T2 is the duration of the sample and G(f) is the DTFT of 
the sequence g(n). S(f) is obtained either by applying the fit function to the 
sequence that defines g(n) , or by using expression 8.10. 
EXAMPLE 8.2 (Estimating the psd of the quantization noise) 
Write a program that estimates, using the welch function designed in exercise 

324 Digital Signal and Image Processing using MATLAB® 
6,------------------------------------------. 
4 
2 
o 
- 2 
, 
, 
------- r-------------r ------
-4 ------------,------------- r ---" 
-6L-
------~------~--------~------~------~ 
50 
100 
150 
200 
Figure 8.4 - Binary signal. The positive impulse corresponds to the bit 1 and the 
negative impulse to the bit O. Each impulse is comprised of a constant amplitude for 
a duration of 10 followed by a zero amplitude for a duration of 25 
8.1, the psd of the quantization noise for several values of the quantization step. 
Remember that, using the notations of Chapter 6, paragraph 6.5, under the 
hypothesis that the quantization is uniform and that the quantization noise is 
white, the quantization noise's psd is equal to q2/12 on the signal's entire band. 
Compare the theoretical and estimated values of the quantization noise's psds 
using a speech signal. 
HINTS: type: 
s 
%===== psdQ.m 
load phrase; Ac=max(y); 
Lfft=1024; Fe=8000; fq=Fe*(O:Lfft-l)/Lfft; 
for M=4:7 
%===== number of bits 
end 
q=2*Ac/2-M; 
%===== quantization step 
yQ=round(y/q)*q; eQ=y-yQ; 
[sf gamma]=welch(eQ,Lfft, 'rect' ,Lfft,0.95); 
plot(fq,10*log10(sf)); hold on 
sth=10*log10(q*q/12); plot([O Fe] , [sth sth], '-. ') 
hold off; set(gca, 'xlim' ,[0 Fe/2]); grid 
The results are shown in Figure 8.5. Notice that the theoretical 
values (horizontal full line) are in agreement with the estimates. 
Exercise 8.3 (Spectral observation and oversampling) (see p. 474) 
Use the welch function designed in exercise 8.1 to estimate the spectra: 
1. using filtering, create a signal X (n) with the length 1024. A crude ap-
proximation could make this signal a model for audio-frequency signal. 
You can use the rif function (see exercise 4.8) with a small number of 
coefficients to generate this signal. Display the resulting signal's spec-
trum; 

Chapter 8 - Spectra Estimation 
325 
145
- r-~--
-----------------
-----------
- -----------
, ---------' 
140-
~~
----~--~----7---~----~----
~
M =4 
135-
130-
115~--~----~--~----~----~--~----~--~ 
o 
500 
1000 
1500 
2000 
2500 
3000 
3500 
4000 
Figure 8.5 - psd of the quantization noise (in dB) as a function of the frequency 
(in Hz) for various values of the quantization step. The test signal is a speech signal 
sampled at 8,000 Hz. The continuous lines represent the theoretical value of the psd 
under the hypothesis that the quantization noise is white 
2. perform the expansion operation corresponding to an oversampling by a 
factor of 8 (see paragraph 4.8.1) . Display the resulting signal's spectrum; 
3. perform the oversampling operation necessary to the filtering operation. 
You can use the rif function with a high number of coefficients. Display 
the resulting signal's spectrum. 
8.2 AR estimation 
8.2.1 AR parameters 
The object of this section is to study methods for estimating the coefficients of 
an AR-model from the observed data X(O) , . .. , X(N - 1), and correlatively 
for estimating the dsp of this sample. 
Least squares method 
The idea is to minimize, with respect to a = [al 
]
T 
h 
. 
. 
ap 
, t e cntenon: 
( 
)
2 
N-l 
P 
]; X(n) - j; ajX(n - j) 
(8.11) 

326 Digital Signal and Image Processing using MATLAB® 
Canceling the derivatives w.r.t. {aj} gives: 
a = (D HD )- l DHx 
where x = [X(P) ... X(N - l)V and (see equation 7.38): 
r 
X*(P -1) 
X*(P - 2) 
DH = 
. 
X*(O) 
X*(P) 
X*(P - 1) 
X*(l) 
X*(N - 2) 1 
X*(N - 3) 
X*(N - P - 1) 
(8.12) 
The resulting matrix D H D is not Toeplitz due to end-effects. This might 
produce a polynomial A(z) with zeroes outside the unit circle. 
The Yule-Walker m ethod 
The Yule-Walker equations 7.59 provide us with a relation between the AR 
model's parameters and its covariance coefficients. This means they make it 
possible to estimate the parameters of an AR model by replacing the covari-
ances with their estimates, provided for example by the equations 7.33. 
The [a, sigma2] =xtoa(X, P) function given below estimates the sequence a 
of the P coefficients of an AR model as well as the power sigma2 of the white 
noise input from a sample X and the model's assumed order P : 
function [a,sigma2]=xtoa(x,P) 
%!====================================================! 
%! XTOA estimates the (P+1) parameters of an AR model 
%! SYNOPSIS : [a sigma2]=XTOA(x,P) 
%! 
x 
signal 
%! 
P 
order of the model 
%! 
a 
[1 a 1 
-
..... a P] 
%! 
sigma2 
variance of the input white noise 
%!====================================================! 
N=length(x) ; x=x( :); x=x-mean(x); 
for kk=1 :P+1 
rconj(kk)=x(kk:N)'*x(1:N-kk+1)/N; 
end 
Rc=toeplitz(rconj) ; vaux=Rc\eye (P+1 , 1); 
a=vaux/vaux(l); sigma2=1/vaux(1); 
return 
In this program, the quantities rconj (kk) provide estimations for the co-
variances R* (k) . We then know that the estimate Rc of the covariance matrix, 
provided by the toepli tz (rconj) function, is a positive matrix. 
To test the xtoa function, type the following program: 

Chapter 8 - Spectra Estimation 327 
%===== testxtoa.m 
trueCoef=[1 -1.3 0.8]; P=length(trueCoef)-1; sw=sqrt(1); 
Lrun=100; listN=(500:500:4000); 
19N=length(listN); perf=zeros(lgN,1); 
%===== 
for k=1:lgN 
N=listN(k); 
for ell=1:Lrun 
end 
w=sw*randn(N,1); x=filter(1,trueCoef,w); 
[aest s2est]=xtoa(x,P); 
%====== performance for the estimation of trueCoef(2) 
eQ=(aest(2)-trueCoef(2))*(aest(2)-trueCoef(2)) , ; 
perf(k)=perf(k)+eQ; 
end 
perf=perf/Lrun; 
plot(listN,perf); hold; plot(listN,perf, 'ro'); hold; grid 
In this program, we consider the AR-2 process defined by X(n) -1.3X(n-
1) +0.8X(n - 2) = W(n) where W(n) is a white gaussian noise with a variance 
of 1. L = 100 runs are performed identically, and we will take the mean over L 
runs of the square deviation between the estimated value of a parameter and its 
true value as an indication of the estimation error. The experiment is repeated 
for several values N of the sample's length. Notice that the obtained graph 
shows that the deviation decreases when N increases. It can be shown that 
this deviation has the same asymptotic behavior as l/VN. The program can 
be modified to evaluate the estimation deviations on another of the model's 
parameters, or to change the model's order. 
The Yule-Walker algorithm solves the least squares minimization by 
padding data with P zeroes on each side of the sequence (see equations 7.37 
and 7.38), which is equivalent to add false data to the observed sequence. The 
main advantages are the following: the associated estimates define stationary 
processes and the estimates may be easily computed using the Levinson recur-
sion. 
EXAMPLE 8.3 (Sunspot periodicity) 
The sun's magnetic field, and its interactions with the movements of plasma, 
cause small, temporarily active regions called sunspots to appear on the surface. 
The intriguing part is how their number follows a cycle. The number of these 
sunspots has been recorded every month for over two centuries. The resulting 
values are available for research, and can be downloaded off the internet. The 
graph in Figure 8.6 shows the monthly data gathered from January 1750 to 
December 1999. As you can see, there are 23 lobes of roughly equal widths, 
over a period of 250 years. A tendency over a longer period is also visible, but 
is more difficult to analyze. 
In 1898, Schuster defined the periodogram as a method to discover the 

328 Digital Signal and Image Processing using MATLAB® 
frequencies of the "hidden harmonics" in a signal and was the first to use the 
periodogram to analyzing sunspot activity [31]. Later in 1927, the sunspot 
data have first been studied by Yule [39] with an AR model (see example 7.8). 
Write a program to analyzing sunspot activity using periodogram and AR-2 
estimation. Notice that it is necessary to de noise the signal before processing 
it. Use the function rif.m as a lowpass filter. 
300 
250 
200 
150 
100 
50 
0 
1750 
1800 
1850 
1900 
1950 
2000 
Figure 8.6 - Number of sunspots recorded monthly during the period from January 
1750 to December 1999, plotted against time 
HINTS: type the program (Figure 8.7) : 
%===== analsunspots.m 
clear; load tachsol; y=tachsol; N=length(y); 
Fs=12; tps=(O :N-1)/Fs; % 12 samples per a year 
[AA yc]=tendoff(y); Lyc=length(yc); % detrend 
h=rif(30,1/40); yc=filter(h,l,yc); 
%===== periodogram 
Lfft=4096; fq=Fs*(O:Lfft-1)/Lfft; 
perioy=abs(fft(yc,Lfft)).-2/N; 
[periomax indperiomax]=max(perioy); 
fO=Fs*(indperiomax-1)/Lfft; mperiod=l/fO 
%===== covariance 
P=2; rr=zeros(P+1,1); 
for kk=1:P+1, rr(kk)=yc(kk:N)'*yc(1:N-kk+1)/N; end 
Rc=toeplitz(rr(1:P+1)); vaux=Rc\eye(P+1,1); 
%===== AR model 
aAR=vaux/vaux(l); sigma2=1/vaux(1); 
SAR=sigma2 ./ (abs(fft(aAR,Lfft)) .-2); 
[SARmax indSARmax]=max(SAR); 
fO_AR=Fs*(indSARmax-1)/Lfft; mperiodAR=l/fO_AR 
%===== figures 

Chapter 8 - Spectra Estimation 329 
subplot(221); plot(yc(2:Lyc),y(1:N-l),'x'); grid 
subplot(222); 
plot3(yc(3:Lyc),yc(2:Lyc-l),y(1:N-2), 'x'); grid 
subplot(212); plot(fq,perioy, 'g'); 
set(gca, 'xlim', [0 Fs/30]); grid 
hold on; plot(fO,periomax, 'or'); 
plot(fq,SAR,'r'); set(gca, 'xlim' ,[0 Fs/30]); hold off 
300,-----,---, -----, 
, 
, 
250 -------- ~ ---------;- -x-xx- ---
200 ---__ -- _ ~ _ -__ _ >x_x 
I 
150 
100 
50 
o 
-100 
o 
100 
200 
1.------------, 
0.5 
o 
-0.5 '----~-~-~--' 
o 
200 
400 
600 
800 
, 
, 
, 
-----
1 ------ , ------ 1 ------
o 
0.1 
0.2 
0.3 
0.4 
Figure 8.7- Sunspots: DTFT calculated over 4,096 frequency points 
Note that an estimation of the affine trend with function tendoff 
leads to A(2) ~ 0, as we can expect from to the slope from Figure 
8.6, which is almost equal to zero. However, the mean is large since 
the signal's values are positive. 
The processing, for the period, leads to a value slightly above 11 
years. The previous program can be modified so as to estimate the 
period over sub-intervals of the set of data. This is a method that 
can be used for studying possible fluctuations. 
8.2.2 Estimating the spectrum of an AR process 
The spectrum can be estimated using the formula: 
(8.13) 

330 Digital Signal and Image Processing using MATLAB® 
This formula is obtained by replacing the model's parameters with the es-
timated parameters in expression 7.58. 
This spectral estimation method is sometimes called the high-resolution 
method. Notice that it is not affected, unlike the periodogram method, by the 
2/N limitation related to the time truncation. Once the parameters have been 
measured, the spectrum is known with an "infinite" resolution. This may seem 
surprising, but is simply due to the fact that a priori information is added 
when we say that the signal is a P order AR process. 
An important obstacle to the AR identification however is the noticeable 
loss of resolution in the case of noised observations. The signal is then of the 
type Y(n) = X(n)+B(n) where X(n) is an AR process and B(n) is a noise. It 
can be shown that for small signal-to-noise ratios, the periodogram's resolution 
is improved. [22] gives the following order of magnitude: if the signal-to-noise 
ratio, expressed in decibels, is less than (3210g10 N - 24), the periodogram's 
resolution is better than that of the AR method. For example, for N = 100, 
the formula indicates that if SNR < 40 dB, it is wiser to use the periodogram. 
8.3 Estimating the amplitudes and the frequen-
CIes 
As we have seen for both the deterministic and the random cases, a signal 
composed of a sum of sines shows "peaks" in its spectrum. The object of 
this chapter is to study methods for estimating their frequencies and their 
amplitudes when the signal is corrupted by noise. 
In this section, random processes will be denoted by lowercase letters so as 
to reserve capital letters for Fourier transforms. 
8.3.1 
The case of a single complex exponential 
Consider an observation x(n) = s(n; 8) + b(n) where s(n; 8) = (Xle2j7rj,n is 
a complex harmonic signal, where b(n) is a white, centered, WSS, complex 
random signal, and where 8 refers to the parameters ((Xl, h). We are going to 
try to estimate the complex parameter (Xl and the parameter h, which belongs 
to (0,1), based on a sequence of N noised observations. 
In practice, the noise b( n) is used to take into account the measurement 
errors, but also the possibility that we are not quite sure of the model used for 
the signal s( n; 8). This occurs when we have a priori information at our disposal 
on the wanted signal, for example with an active radar, where s(n) represents 
the signal emitted then sent back by the target. It is also the case with speech 
when some of the noises originating from the vocal cords are described as a 
sum of sines. 

Chapter 8 - Spectra Estimation 
331 
The least squares method, the general presentation of which is given in 
Chapter 9, consists of calculating the values of al and h that minimize the 
square deviation between the observed values, that is x(n), and the expected 
values, that is s(n; 8). When the noise is assumed to be Gaussian, the obtained 
values are those that maximize the probability density. In that case, the method 
is called the maximum likelihood method. 
By stacking a sequence of N successive values of the model for the signal 
s( n; 8) = al e2j7rn!t, for n from 0 to N - 1, we get the vector expression 8(8) = 
ale(fd with: 
{ 
8(8) = [s(O) 
e(fd = [1 
e2j7r!t 
s(N-1)] 
T 
T 
e2j7r!t(N- lJ] 
Notice that the expression 8(8) = ale(h) is linear with respect to al 
whereas it is not with respect to h. 
The square deviation between the observation and the model is given by: 
N-l 
L Ix(n) - s(n; 8)1 2 = (x - ale)H (x - ale) 
n=O 
(xH - aieH)(x - ale) 
where x = [x(O) 
x(N -1)f represents the sequence of N observations. 
The expression is similar to the one we encountered in example 7.6, on sup-
pressing seasonal trends. The minimization of J (aI, h) with respect to al and 
h is performed first by setting to zero the derivative with respect to aI, then 
by replacing the result in J(al,h). We get: 
8J 
H 
H 
H 
-;::;-- = 2e (x - ale) = 0 ¢} e x = ale e 
Val 
Noticing that e H e = N for any h leads us to: 
1 
1 N -l 
. 
al = -
eHx = -
'"""' x(n)e- 2J7r!t n 
N 
N~ 
n=O 
(8.14) 
By replacing this expression of al in the expression of J, we get a new 
expression dependent only on h: 
J,(J') ~ x"x - a,e"x ~ x H x - ~ 1% 
x(n)e-,j'h"i' 
and that we have to minimize with respect to h E (0, 1). Because the first 
term x H x does not depend on h, the problem is equivalent to determining the 

332 Digital Signal and Image Processing using MATLAB® 
value of il E (0,1) that maximizes the expression: 
K(J,) ~ ~ I%:;: x(n)e ';.J,nl' 
(8.15) 
There is no simple analytical solution to this problem. It will be denoted 
by: 
I
N-1 
12 
h = arg max ~ L x(n)e- 2j7r f,n 
f,E(O,l) N 
n=O 
However, an approximation of the solution can be obtained digitally by 
performing a tightened sampling of the interval (0,1). Once this value is cal-
culated, we get the numerical complex amplitude using expression 8.14: 
1 N-1 
. ~ 
0:1 = N L x(n)e- 2J7r f,n 
n=O 
The fact that we used expression 8.15 to estimate a frequency is not in the 
least surprising, because it contains the expression of the DTFT of the sequence 
{x(O), .. . , x(N - 1)} or to be more precise, its square modulus, which is the 
expression of the periodogram (definition 8.5). 
To estimate, in the least squares sense, the frequency of one complex 
exponential corrupted by white noise, all we have to do is calculate the 
observation periodogram and find the frequency for which it reaches its 
maximum. 
8.3.2 
Real harmonic mixtures 
We now consider a real signal, sum of P sinusoidal components of the type: 
p 
s(n) 
LAkCos(27rikn+ ¢k) 
k=l 
p 
P 
L ak cos(27r ikn) + L 
bk sin(27r ikn) 
(8.16) 
k=l 
k=l 
where A k, ik and ¢k represent the parameters we wish to estimate based on a 
sequence of N observations. The frequencies il, .. . , ip are all assumed to be 
different. 
Notice that we go from the pair (Ak' ¢k) E IR+ x (0, 27r) to the pair (ak' bk) E 
IR x IR using the bijection: 
{ 
ak = Ak COS(¢k) 
{ Ak = va~ 
+ b~ 
{==} 
(8.17) 
bk = -Aksin(¢k) 
¢k = -arctan(bk/ak) 

Chapter 8 - Spectra Estimation 
333 
From now on, we will assume that P is known, and that there are more 
observations than there are parameters to estimate. Let f = (h,···, f p), 
s = [s(O) 
.. . 
s(N - l)f, and: 
A(f) = [C(f) S(f)] 
(8.18) 
where: 
1 
1 
C(f) = 
cos(27rkh) 
cos(27rkfp) 
cos(27r(N - l)fd 
cos(27r(N -l)fp) 
0 
0 
S(f) = 
sin(27rkfd 
sin(27rkfp) 
sin(27r(N - l)fd 
sin(27r(N - l)fp) 
A(f) is a (N x 2P) matrix. If we use these notations, and stack the N 
equations of s(n) for n from 0 to N - 1, we get the expression: 
s = A(f)d 
(8.19) 
where the Q = 2P sized vector 
is the amplitude vector we wish to estimate. The expression of the square 
deviation is still given by: 
N- l 
J(d,f) 
L Ix(n) - s(n)1
2 = (x - A(f)df(x - A(f)d) 
n=O 
(x T - dT A(ff)(x - A(f)d) 
(8.20) 
where x = [x(O) 
x(N - l)f. To solve the problem, we are going to 
proceed as we did previously by setting to zero the partial derivatives of J with 
respect to each of the components of d. We have: 
aJ 
T 
-
= a (x - A(f)d) = 0 
adj 
J 
j E {1, ... , Q} 

334 Digital Signal and Image Processing using MATLAB® 
where aj represents the j-th column of A. If we group together the equations 
in matrix form, we get: 
A(ff (x - A(f)d) = 0 {o} A(ff A(f)d = A(ff x 
Because the values of iI, ... , ip are assumed to be all different, the matrix 
A(f)T A(f) is invertible. This leads to the expression of d, dependent on iI, 
... , ip, that leads to the maximum: 
d = [A(ff A(f)r 1 A(ff x 
(8.21 ) 
If we then replace this value of d in J, the resulting expression is a function 
of iI, ... , ip that we have to maximize: 
(xT - dT A(ff)(x - A(f)d) 
x T x - x T A(f)[A(ff A(f)]-l A(ff x 
where we have used the fact that the matrix [A(f)T A(f)]-l is identical to its 
transpose. Because the first term is not dependent on the frequencies f we are 
trying to determine, the minimization is equivalent to the maximization of: 
T 
[ 
T 
]-1 
T 
K(iI , ··· , ip) = x A(f) A(f) A(f) 
A(f) x 
(8.22) 
The expression of K contains the frequencies iI, ... , ip, but is not linear 
with respect to these frequencies. Just as before, its maximization does not 
lead to a simple analytical formula. We will simply write: 
f = arg 
max 
x T A(f) [A(ff A(f)r
1 A(ff x 
JE(O,l) x ... x (0,1) 
We still have the possibility of a numerical calculation, but the problem 
quickly becomes overwhelmin~ 
because we have to find the maximum of a 
function of P variables. Once f has been calculated, the amplitudes are found 
by replacing its value in 8.21. We get: 
d = [A(ff A(f)] -1 A(ff x 
(8.23) 
8.3.3 Complex harmonic mixtures 
The complex case is dealt with in exactly the same way as the real case. Con-
sider: 
p 
s( n) = L CYk exp(2j7r ikn) 
(8.24) 
k = l 
where the CYk are a sequence of P complex amplitudes and the ik are a sequence 
of P frequencies assumed to be all different, and belonging to the interval (0,1). 

Chapter 8 - Spectra Estimation 335 
The square deviation between the observation and the model has the ex-
pression: 
If we let f = (fl,···, ip) and: 
r 
e2j~h 
E(f) = 
e2j7r(~
- 1)h 
J can be written: 
J(a1' ... ' ap, fl,···, ip) 
1 
1 
e 2j7r/2 
e 2j7r fp 
1 
e 2j7r(N - 1)/2 
e 2j7r(N - 1)fp 
Ix - E(f)aI 2 
(x - E(f)a)H (x - E(f)a) 
(xH - aH E(f)H)(X - E(f)a) 
(8.25) 
(8.26) 
where the exponent H indicates a transpose-conjugation. First, we minimize 
with respect to a = [a1, .. . , apjT. If we set to zero the partial derivatives, we 
get: 
E(f)H (x - E(f)a) = 0 {:} E(f)H X = E(f)H E(f)a 
You can check for yourself that if the P frequencies are different, the matrix 
E(f)H E(f) is invertible. This means that: 
(8.27) 
and that the minimum's expression is: 
We still have to minimize this quantity with respect to the set of frequencies 
f . Because the first term x H x does not depend on f , this is equivalent to 
maximizing the second term: 
H 
[ 
H 
]-1 
H 
K(fl, ··· ,i p) =x E(f) E(f) E(f) 
E(f) x 
(8.28) 
which implies the difficulties we mentioned earlier, and that will be studied in 
detail in the following paragraph. 

336 Digital Signal and Image Processing using MATLAB® 
8.4 Periodograms and the resolution limit 
When P is greater than 1, it becomes difficult to maximize the function 
K(h, ... ,jp) with respect to the frequencies h, ... , jp, whether in the real 
case with expression 8.22, or in the complex case with expression 8.28, because 
it is a function of several variables, and usually has several local maxima. 
However, if the differences between the frequencies are greater than 2/ N, the 
method whereby the P maxima of the periodogram are determined is a quite 
efficient method. This makes the calculations much simpler since the multi-
variable maximization problem is changed into a single-variable maximization 
problem. This is what we are going to see now by numerically studying the 
case where P = 2. 
Presence of several maxima for k 
Let us reconsider, for P = 2, the expression of the function K(h , h) defined 
by 8.26. The matrix E has the expression: 
This means that: 
e-2j7rj, 
e-2j7rh 
e-2j7r(N-l)j, 
e-2j7r(N-l)h 
H 
[ 
1 
PN(12 - h) 
E E = N 
p'N(12 - h) 
1 
where: 
(j) _ j7rf(N-l) sin(7rNf) 
PN 
- e 
N sin(7r f) 
By replacing this result in expression 8.28, we get: 
~ [XNUd XN(12)] 
x 
[ 
1 
PN(12 - jd] -1 [XNUd ] 
p'N(12 - h) 
1 
XN(12) 
where: 
N-l 
XNU) = L x(n)e-2j7rfn 
n=O 
(8.29) 
(8.30) 
is simply the DTFT of the sequence x( n). We are going to perform a numerical 
study by considering the signal x(n) = s(n) + b(n) where: 

Chapter 8 - Spectra Estimation 
337 
with a~ = 1.5, ag = 1, fP = 0.12 and fg = 0.61 and where b(n) is a white, 
centered, Gaussian noise. There are N = 10 sample values. The following 
program generates the signal x(n), plots the surface K(h ,h) defined by 8.28 
as well as the periodogram of x( n) defined by: 
Type: 
%===== Grnle.rn 
clear; T=10; f01=.12; f02=.61; tps=[0:T-1]'; 
%===== signal 
s=1.5*exp(2*lj*pi*f01*tps)+exp(2*lj*pi*f02*tps); 
SNR=15; sigrna2= (s'*s/T)/(10 -(SNR/10»; 
%===== noised signal 
xb=s+sqrt(sigrna2)*randn(T,1); 
Lf=70; f1=(0:Lf-1)/Lf; f2=f1; 
mm=exp(2*j*pi*tps*f1); 
%===== or: [X, Y]=rneshgrid(f1,tps); mm=exp(2*j*pi*(X.*Y»; 
yy=zeros(Lf,Lf); 
for k1=1:Lf 
end 
for k2=1:k1-1 
end 
E=[mm(: ,k1) mm(: ,k2)]; 
yy(k1,k2)=abs(xb' * E * pinv(E) 
* xb); 
subplot(121); rnesh(f1,f2,yy); view([115 35]) 
subplot(122); plot(f1,abs(fft(xb,Lf»); grid 
(8.31 ) 
The results, obtained for a signal-to-noise ratio equal to 15 dB are shown 
in Figure 8.8. Because K(h , h) = K*(h, h) , we restricted the representation 
to the half-plane delimited by the bisector of the first qu~rant. 
]'he function 
shows a global maximum in M the coordinates of which, hand 12, are almost 
equal to the two real values. But this function also has local maxima such as 
m, making it difficult to find the global maximum using a numerical technique. 
The graph at the bottom of Figure 8.8 shows that the obtained values h 
and 12 are almost equal to the x-coordinates of the two highest maxima of the 
periodogram of x(n). As we are going to show, this property has to do with 
the fact that the frequency difference is such that IfP - fglN = 4.5 » 1. This 
is a fundamental result, because it justifies the use of the periodogram for esti-
mating frequency sequences. Note that there is an essential difference between 
the search for the global maximum of the multivariable function K(h, h) and 
the search for the two highest maxima of the single-variable function IN(f) . 

338 Digital Signal and Image Processing using MATLAB® 
30 
30 
M 
25 
25 
20 
20 
15 
15 
10 
10 
5 
0 
5 
0 
0 
1 
0 
o 
12~1 
8 
--. ------- ---,- -------------- c - - -. - - -. - - - - - - - ,- - - - - - - - - - - - - --
, 
, 
, 
4 
- -------
:
---------~ 
-----
---:---------
, 
, 
0
' 
, 
o 
0.25 
0.5 
0.75 
Figure 8.8 - Frequency estimation: the figures correspond to the function K (/1 , h) 
given by equation 8.30. The bottom figure shows the periodogram (equation 8.31) for 
N = 10 and a signal-to-noise ratio of 15 dB 
R esolution limit of Fourier 
We are going to show that, when the frequencies contained in the signal s(n) 
are such that: 
1 
min Ifo - fO I » -
{ .. '-4. .} 
, 
J 
N 
~
, J
; ~ r
J 
(8.32) 
then the global maximum of K(h, .. . , fp) is located at a point whose coordi-
nates are almost equal to the periodogram's P highest maxima. First, let us 
write once more expression 8.28: 
If condition 8.32 is met, then according to 8.26, the diagonal elements of 
E H E are equal to N and the non-diagonal elements have the expression: 
PN(f0 _ f O ) = ej7r(N
- l)(f~
- f ;',,
) sin(1f-N(fZ - f;;")) 
k 
m 
N sin( 7r(fZ -
f~)) 
Therefore, they quickly tend to 0 when (fZ - f;;")N becomes large. Hence 
we can write, using the notation E = [el 
ep] in the form of column 

Chapter 8 - Spectra Estimation 339 
vectors: 
K(h ,··· ,fp) 
~ 
l(H 
H 
H 
H) 
N x ele1X+"'+X epepx 
But according to 8.31, -f:txHejefx = IN(fJ), and therefore: 
This function's maximum is obtained by separately maximizing IN(f) for 
each variable, hence the maximum of K(h, .. . , fp) is obtained by using the 
periodogram as an univariate function. This is why the 2/N limit condition on 
the use of the periodogram is called t£e fundamental resolution limit of Fourier. 
Finally, the estimates, denoted by fk' are used to simplify formula 8.27 and 
lead to the following estimates for the complex amplitudes: 
(8.33) 
We can also prove that, under condition 8.32 set by the resolution limit of 
Fourier, the result is similar in the case of real harmonic signals. Based on 
8.18, we first show that: 
S(f)] c:::: N [I p 
Op] 
2 
Op 
I p 
As a consequence, the frequencies are provided by the periodogram's max-
imum in the (0,1/2) band (because of the hermitian symmetry). As for the 
amplitudes, all we have to do is replace A(f)T A(f) ~ (N/2)I in 8.23, then 
use relations 8.17 and 8.19. We get: 
As a conclusion, the amplitudes ak and -bk are twice the real and imaginary 
parts respectively of the complex quantities O;k given by expression 8.33. 
Consider the case of a signal containing only one real sine with the frequency 
h = 0.1, that is to say two complex exponentials with the frequencies hand 
- h. If we apply the previous result, the periodogram is effective so long as the 
difference in frequency is such that 2hN » 1. Let us assume that N = 100, 
meaning that 2hN = 20. The following program implements the frequency 
estimation based on the periodogram: 

340 Digital Signal and Image Processing using MATLAB® 
%===== estlsinreel.m 
clear; fl=O.l; Lfft=4*1024; N=100; Al=2; phil=pi/3; 
SNR=30; sigmab=Al*10-(-SNR/20); 
xt=Al*cos(2*pi*fl*(0:N-l)+phil)+sigmab*randn(1,N); 
al=Al*cos(phil); bl=-Al*sin(phil); 
xf=fft(xt,Lfft)/N; [amp flind]=max(abs(xf(1:Lfft/2))); 
flest=(flind-l)/Lfft; 
alest=2*real(xf(flind)); blest=-2*imag(xf(flind)) 
[fl flest], [al alest], [bl blest], [Al sqrt (alest-2+blest-2)] 
Notice that the frequency value is given by the x-coordinate of the maximum 
of IN(J) on the interval (0,1). The maximum can be found using the FFT-
based computation of IN(J) for f = k/ Land k E {O, ... , L - I}. The 
frequency estimate's accuracy improves as the number L of FFT calculation 
points increases. This accuracy has direct consequences on the amplitudes of 
the sine and cosine components, and particularly on the measurement of the 
phase, as you can see from the two displayed values. 
The performances are "enhanced" when N increases 
We will show that, on average, the higher N is, the better the periodogram's 
components stand out in the noise. Consider once again the case where P = l. 
In the presence of noise, the signal has the expression: 
x(n) = ale2j7rhn + b(n) 
where n E {O, ... , N - I} and where b(n) is assumed to be white with the 
variance (J"2. We are going to determine the periodogram's expression. Let: 
and: 
With these notations, the periodogram can be written: 
1 IN-l 
. 12 
2 
N ~ x(n)e-2J7rnJ 
= IvlJVa1PN(J - h) + BN(J)I 
N aip7v(J - h) 
+2Re{ vIJV alPN(J - h)B"N(J)} + IBN(J)1 2 
(8.34) 
Consider the expectation of IN(J). The first term is deterministic. The 
second term of 8.34 has an expectation equal to zero because b( n) is centered. 

Chapter 8 - Spectra Estimation 341 
The expectation of the third term is expressed: 
N-l N -l 
E {IBN(f)12} = ~ L L E {b(n)b*(n') } e-2j7rjne2j7rjn' = (72 
n = O n' = O 
where we have used the fact that E {b(n)b*(n')} = (721(n = n'). Therefore: 
E {IN(f)} = Naip~(f 
- 1d + (72 
As a conclusion, E{IN(f)} is comprised of two terms: the first one, related 
to the wanted signal, shows a maximum in h that increases with N. The 
second one, related to the noise, is independent of N and equal to (72 . When 
N increases, that part corresponding to the wanted signal tends to stand out 
in the noise around h. This can be checked by using the previous program. 
Theoretically, when N is multiplied by 2, there is a 3 dB gain on the emergence 
of the peak. This result can be generalized to the case of a signal containing P 
complex exponentials. 
To sum up, if the frequency differences are much greater than 2/ N, de-
termining the frequencies is equivalent to studying the periodogram, which 
shows: 
- peaks around the real frequencies fP, ... , 1~, 
the heights of which in-
crease proportionally to N; 
- and farther away from these frequencies, a basically "constant" level for 
the power of the noise. 
Remember that using windows other than the rectangular windows makes 
it possible to reduce the height of the side lobes and therefore to help the low 
amplitude components stand out better, but at the cost of a worse frequency 
separation. 
A program for the search of the P maxima 
As we just saw, the P frequencies contained in a noised signal can be esti-
mated by choosing the periodogram's P maxima. However, this is difficult to 
implement because the periodogram usually has local maxima that must not 
be taken into account. To solve this problem, we are going to use the fact that 
the periodogram of a sum of P sines, in the absence of noise, and under the 
Fourier condition (condition 8.32) of P lobes with the width ,6,1 around the 
frequencies. The value of ,6,1 essentially depends on the weighting window. 
EXAMPLE 8.4 (Finding the maxima) 
Consider the length N = 25 sample of the signal x(n) = sen) + ben) where ben) 
refers to a Gaussian, additive, white noise of unknown power (72 . We know that 
sen) is the sum of three real sines with unknown frequencies and amplitudes 

342 Digital Signal and Image Processing using MATLAB® 
and that the differences in frequency are much greater than the resolution limit 
of Fourier, which in this case is equal to 2/25 = O.OS. 
Write a program that estimates the three frequencies by calculating the 
periodogram over L = 256 frequency points. Remember that the choice of L is 
related to the frequency accuracy: with L = 256, for example, the calculation 
points on the spectrum are separated by 1/256 >:::: 0.004. The method mentioned 
previously consists of finding the first maximum then to eliminate the points in 
the spectrum around this maximum in a range of 6.f = ±a/N. The choice of 
the value for a is based on the type of weighting window. For the rectangular 
window for example, the value of a is slightly higher than 1. The procedure is 
repeated until the P maxima are found. 
HINTS: 
%===== Pmaxsin.m 
clear; clg 
P=3; A=[2 1.5 1]; F=[0.1;0.23;0.3]; N=25; Lfft=256; 
deltaf=round(Lfft/N); 
s=A*cos(2*pi*F*(0:N-l»; 
sigma2=0.5;x=s+sqrt(sigma2)*randn(1,N); 
%===== signal 
subplot(221); plot(x); axis([O N -5 5]); grid 
text(16,4, 'Time') 
%===== spectrum 
xf=abs (fft (x, Lfft» 
. -2; xf=xf (1: Lfft/2) /max (xf) ; 
subplot(222); plot«0:Lfft/2-1)/Lfft,xf); 
set(gca, 'xlim', [0 0.5], 'ylim' ,[0 1]); grid 
text(.25, .75, 'Frequencies') 
subplot(212); grid 
%===== looking for the frequencies 
for ii=l:P 
end 
[mm im]=max(xf); fs=(im-l)/Lfft; 
ul=max(1,im-deltaf);u2=min(Lfft/2,im+deltaf); 
nb=u2-ul+l; xf(ul:u2)=zeros(1,nb); 
hold on; plot(fs,mm, '0'); hold off 
axis([O 0.5 0 1]) 
text(fs+O .Ol,mm,sprintf('% .3g' ,fs» 
Figure S.9 shows the temporal form of the signal and its spectrum. 
At first, the periodicities are difficult to distinguish in the signal's 
representation as a function of time. The spectrum, on the other 
hand, clearly shows the location of the three frequencies. This calls 
for a comment: a more relevant study of the temporal signal consists 
of interpolating the signal (this is allowed because it is in agreement 
with the sampling theorem) so as to obtain a time discretization 
sufficient to estimate the periodicity. 

Chapter 8 - Spectra Estimation 343 
The previous program also allows you to check the efficiency loss 
when the differences in frequency are too small. This is achieved 
by choosing values of b.f with a modulus close to liN. 
5 
1 
0.8 
__ fr~gu
_e~cies
_ 
0 
0.6 
-5 0 
5 
10 
1 
0.8 
0.6 
0.4 
0.2 
0 0 
0.05 
0.1 
0.15 
0.35 
0.45 
0.5 
Figure 8.9 - Top-left graph: the signal as a function of time. Top-right graph: the 
signal's spectrum. Bottom graph: location of the 3 maxima obtained with the program. 
The signal-to-noise ratio is equal to 10 dB and the real frequencies are equal to 0.1, 
0.23 and 0.3 
EXAMPLE 8.5 (Analysis of a musical note) 
Figure 8.10 shows the signal created by a piano note, a "C2" (264 Hz), sampled 
at the frequency 24 kHz. The signal's shape, as well as the physical phenomena 
involved when the chords vibrate, lead us to describing the signal as a sum of 
sines. 
o 
1000 2000 3000 4000 5000 6000 7000 8000 9000 
Figure 8.10 - Note played by a piano 

344 Digital Signal and Image Processing using MATLAB® 
The spectrum in Figure 8.11, for a 600 samples portion of the signal, shows 
peaks with frequencies that are approximatively the multiples of a fundamental 
frequency corresponding to the note that is played. The spectral envelope, the 
virtual line that passes through the maxima of the peaks, is characteristic of 
the instrument's timbre. 
50,------------------------------------------, 
40 
30 ·-----
20 
10 
o 
-10 
------ t -------------~-------------~-------------I-----------
, 
, 
, 
, 
, 
, 
- 20 
-------------
~ -------------
~ -------------~----------
--,-----------
- 30 
______________ L _____________ L _____________ L _________ _ 
, 
, 
, 
-40 ~------~--------~------~--------~----~ 
o 
500 
1,000 
1,500 
2,000 
Hz 
Figure 8.11 - Spectrum of a 600 sample portion of the signal shown in Figure 8.10 
We are first going to analyze the signal in such a way as to extract the 
main frequential components, then, based on these components, synthetize a 
signal. The point of this process is to have a small number of parameters (the 
duration, the frequency, the timbre) that can then be modified, to create a 
sound. After having recorded a sound created by a musical instrument such as 
a piano, a guitar, etc. write a program: 
- that cuts up the signal in windows covering a duration of a few periods; 
- that extracts the amplitudes and the frequencies of the Pmost important 
components. You can use the Hamming window, and with a method 
similar to the one used in the previous program, spread out a certain 
number of points located on either sideof the maxima. The value of P 
can be chosen based on an a priori study or through an automatic process 
using the 3-sigma rule (page 347); 
- that creates a signal from the extracted amplitudes and frequencies. Dis-
continuities appear in the trajectory when the created portions are placed 
one after the other, and these discontinuities are distinctly audible. This 
problem can be solved by cutting up the signal in windows with an cx% 
overlap. When the signal is created, the calculated window is multiplied 
by a triangular or trapezoidal window then added with an overlap of cx% 
to the previous window (Figure 8.12). 

Chapter 8 - Spectra Estimation 345 
overlapping windows 
/ 
~ 
•" '<> 
I 
I 
I 
I 
I 
, 
, 
I 
I 
I 
I 
, 
" 
, 
: 
W N l l 8
' 
:
' 
I 
I 
I 
I 
I 
I 
I 
I 
Figure 8.12 - Reconstruction with overlapping blocks 
This "Overlap-Add" technique ensures a satisfactory continuity of the 
total trajectory. 
HINTS: the following program analyzes and synthetizes a piano 
note. The duration of a window was set to 350 samples, which 
corresponds for this note to a little over 4 periods. The window 
must be chosen long enough, but not too long to maintain a good 
stationarity. A long window is particularly badly suited for the 
attack and the release of a note. 
%===== ananote .m 
clear, figure(1) 
load piano; Fe=24000; N=length(piano); 
%===== splitting in blocks 
lbloc=350; nbblocs=fix(N/lbloc); 
pianoF=piano(1:nbblocs*lbloc); 
xsyn=zeros(nbblocs*lbloc,1); tpsbloc=(O:lbloc-1)/Fe; 
%===== windows 
fenH=O.54-0.46*cos(2*pi*(O:lbloc-1)'/(lbloc-1)); 
fenH=fenH*lbloc/sum(fenH); 
% Normalization 
fenT=2*[(O:lbloc/2-1)'; (lbloc/2-1:-1:0)']/lbloc; 
%===== Parameters of the spectral analysis 
P=12; Lfft=4096; deltaf=2*round(Lfft/lbloc); 
fq=Fe*(O:Lfft/2-1)/Lfft; 
%===== processing 
for jj=O:2*nbblocs-2 
jj1=(lbloc/2)*jj+1; jj2=jj1+lbloc-1; 
x=pianoF(jj 1: jj2) . * fenH; x=x-mean(x); 
fs=zeros(1,P); mm=zeros(P,1); 
%===== spectrum 
xf=fft(x,Lfft); xf=xf(1:Lfft/2)/lbloc; xfvar=xf; 
xfvar(1:deltaf)=zeros(1,deltaf); 
%===== analysis 
for ii=1:P 
[bid im]=max(abs(xfvar)); 
fs(ii)=(im-1)/Lfft; mm(ii,1)=xfvar(im); 

346 Digital Signal and Image Processing using MATLAB® 
end 
u1=max(1,im-deltaf);u2=min(Lfft/2,im+deltaf); 
nb=u2-u1+1; xfvar(u1:u2)=zeros(1,nb); 
%===== synthesis 
xsyn_f=2*real(exp(2*j*pi*(0:lbloc-1)'*fs)*mm); 
%===== overlap-add 
xsyn(jj1:jj2)=xsyn(jj1:jj2)+ xsyn_f . * fenT; 
subplot (211) ; 
plot(tpsbloc,pianoF(jj1:jj2),' : ' ,tpsbloc,xsyn_f); grid 
%===== drawing the spectra 
subplot(212),plot(fq,20*10g10(abs(xf))); 
set(gca,'ylim' ,[-70 0]) 
hold on; plot (fs*Fe,20*10g10(abs(mm)), 'or'); hold off; 
grid; pause 
end 
ti=(0:nbblocs*lbloc-1); 
%===== displaying the reconstructed signal 
figure(2); plot(ti,pianoF, 'b' ,ti,xsyn,'r'); grid 
The diagram at the bottom of Figure 8.13 shows the spectra of a 
signal portion and of the estimated frequencies and amplitudes. The 
graph above it shows the analyzed (dashed line) and synthetized 
signal (full line). ~
-
I 
o 
0.005 
0.Ql 
0.Ql5 
----------1-----------
----------1-----------
---------- 1--
---------- 1--
---------- 1--
5,000 
6,000 
Figure 8.13 - Frequency and amplitude estimates of the harmonic part for a window 
of 350 samples extracted from the signal shown in Figure 8.10. 
Top figure: the 
original signal (dashed line) and the synthetized signal (full line). Bottom figure: the 
periodogram. The dots ( '0') indicate frequency and amplitude estimates 
We wish to determine sinusoidal frequencies, but in most problems, the 
number of sines is unknown. We can mention the case of the number of signif-
icant frequential components in a music signal (see example 8.5) or the case of 
the RADAR where P represents the number of targets that are being tracked. 

Chapter 8 - Spectra Estimation 347 
Unfortunately, estimating P is a difficult problem, and the reader can find more 
detailed information in the literature [22]. We will now introduce a heuristic 
method that has the advantage of being simple. It is based on the comment 
made on page 341 explaining that the periodogram's level is basically equal to 
the noise levels at the frequencies other than the sine frequencies. We are also 
going to estimate the value of 0'2 and use it to estimate the number of sines: 
- let us assume that the number of sines is less than a set value Pm ax , 
given a priori. Its value depends on the practical information available 
concerning the system being studied; 
- the periodogram is computed over L FFT points, and the values located in 
a D.-wide interval around each of the P max maxima. This makes it possible 
to eliminate the sinusoidal contributions. The choice of D. depends on the 
type of window used and the number of FFT points. It can be adjusted 
depending on the situation. Usually the number of lobes is chosen to be 
an integer; 
- based on the remaining values of the periodogram, 0'2 is estimated; 
- finally, only the P maxima greater than a certain threshold are kept. The 
choice of this threshold can be made according to the 3-sigma rule. In 
terms of power, this leads the values of the periodogram smaller than 90'2 
to be considered as noise, and the rest as part of the signal. 
%===== nbsin.m 
%===== definition of the signal 
A=[2 1.5 1]; F=[O.1;O.23;O.3]; N=100; Lfft=128; 
deltap=3*round(Lfft/N); 
% value to adjust 
% typically 3*Lfft/N 
%===== generation of samples 
s=A*cos(2*pi*F*(O:N-l)); sigma2=O.4; 
Pmax=6; 
%==== number of sines 
mm=zeros(l,Pmax); x=s+sqrt(sigma2)*randn(1,N); 
xf=abs(fft(x,Lfft)) . - 2 / N ; 
xfplus=xf(2:Lfft/2); 
for ii=l:Pmax 
[mm(ii) im]=max(xfplus); 
ul=max(l,im-deltap); u2=min(Lfft/2,im+deltap); 
nb=u2-ul +1; 
%===== set to 0 close to the maxima 
xfplus(ul:u2)=zeros(1,nb); 
end 
nbz=length(find(xfplus==O)); 
%===== mean of the values of the periodogram 
sigma2est=sum(xfplus)/((Lfft/2)-nbz); 
seuil=9*sigma2est; 
P=length(find(mm>seuil)); disp(sprintf('%2i sines' ,P)) 


Chapter 9 
The Least Squares Method 
In this chapter, we are going to present a series of techniques based on mini-
mizing mean square criteria to solve linear problems. But first we are going to 
state a fundamental theorem, called the projection theorem. It was mentioned 
more or less explicitly in the affine trend suppression problem, or when we 
estimated the amplitudes of a harmonic signal's components. We will see that 
it has major applications both in a deterministic or random context. 
9.1 
The projection theorem 
The projection theorem is presented in mathematical form. However, readers 
that are not used to this formalism should not be worried, since the result 
expressed by relation 9.1 is quite intuitive, as it is shown in Figure 9.1. 
Definition 9.1 (Hilbert space) Let 1i be a vector space with a dot product 
(x , y) for any two of its elements: 
- the norm of an element x of 1i is the positive number defined by Ilxll = 
J(x,X); 
- x and yare said to be orthogonal, which is denoted by x .1 y , if (x, y) = 0; 
- the distance between two elements x and y of 1i is the positive number 
defined byd(x,y) = Ilx-yll = V(x-y,x-y). 
1i is said to be a Hilbert space if it is a complete metric space, that is if any 
Cauchy sequence converges in 1i. 
The following examples are fundamental to signal processing applications: 

350 Digital Signal and Image Processing using MATLAB® 
-
deterministic case: the space g2(Z) of square sum mabIe complex se-
quences, that is the sequences such that L k IXkl2 < +00, with the scalar 
product: 
(x,y) = L XkYk 
k 
is a Hilbert space. Finite sequences of length N are an example of a 
Hilbert space. The set of these sequences can also be seen as the space 
eN of vectors with N complex components. 
-
deterministic case: the space L2(0, T) of square sum mabIe complex func-
tions, that is the functions such that JOT Ix (tWdt < +00, with the scalar 
product: 
(x, y) = faT x*(t)y(t)dt 
is a Hilbert space. 
-
random case: the space L2(0" F, JP') comprising the zero-mean random 
variables defined on a same probability space (0" F , JP'), and square 
summable, that is such that IE {lxI2} < +00, with the scalar product: 
(x , y) = IE {x*y} 
is a Hilbert space. 
Theorem 9.1 (Projection theorem) Let}{ be a Hilbert space, C a subspace 
of}{, and let y be any element of}{: 
- there exists a single element So E C such that: 
'II S E C, 
Ily - sol12 ::; Ily - sII2 ~ d(y, so) ::; d(y , s) 
- this element So verifies: 
'li s E C, 
y -
So ..1 S 
(9.1) 
- the minimal difference has the expression: 
E2 = (y - so , y - so) = (y - so,y) 
(9.2) 

Chapter 9 - The Least Squares Method 351 
"Y 
" " 
" 
--- __ ,: : ...----------IIY - So 11\ 
/
' 
~ "I 
\ 
L_C=---___ -_"": _-_-_-_-_-_-
~_:'~_: _ '_~-_o- _- _- -_-_--_-_-,> 
Figure 9.1 - The projection theorem 
Relation 9.1, also sometimes called the orthogonality principle, is what is 
used in practice for determining So . So is called the orthogonal projection of y 
onto C. Figure 9.1 illustrates this property. 
We will now examine the applications of the projection theorem for the 
previously described deterministic and random signals: 
- deterministic case: H is the space eN of the length N vectors with com-
plex components, with the scalar product and the associated norm: 
N 
(x, y) = L xkYk 
and Ilxll = lL~
= l IXkl2 
k= l 
Let us choose for C the sub-space generated by the linear combination of P 
vectors Xl, x 2 , ... , x P of H , with P < N. Let X = [X l 
x 2 
x P ] 
be the (N x P) matrix constructed from the N components of its P 
vectors. Any vector of C can then be written: 
P 
S = LhkXk = Xh 
k= l 
where h is a vector with P complex components. 
Let y be another vector of H. The closest element So to y that belongs 
to C is such that y -
So is orthogonal to the P vectors that generate C. 
Because So belongs to C, it can be written Xh. In order to determine 
it, we need only express the fact that y - X h is orthogonal to the P 
columns of X . In matrix form: 
Knowing y and X leads to the coefficient vector h that allows us to find 
the projection So we are trying to determine. For example, if XH X is 

352 Digital Signal and Image Processing using MATLAB® 
invertible we get: 
So = X(XH X) - l XHy 
(9.3) 
This result can be applied to the resolution of linear systems of equations, 
or to the parametric approximation of a sequence of values (see example 
9.1). 
-
random case: H is the space of square sum mabIe zero-mean random 
variables, defined on the same probability space, with the scalar product 
and the associated norm: 
(x, y) = lE {x*y} 
and Ilxll = JlE {lxI 2 } 
Let us assume that C is the sub-space generated by the linear combina-
tions of P random variables Xl, ... , Xp de H, with P < N. Any random 
variable of C can then be written: 
3 = L,f=l hkxk 
Let y be another random variable of H. 
30, the element closest to y 
belonging to C, is such that (y -
30) is orthogonal to the P random 
variables that generate C. Therefore we are trying to determine 3 such 
that, for any n E {I, ... , P}: 
lE {(y -
3)X~} 
= lE {(y - L,f=l hkXk ) X~
} = 0 
¢::::::} L,f=l hklE{XkX~} 
= lE{yx~} 
(9.4) 
Relation 9.4 then makes it possible to determine the coefficients of hk by 
knowing the values of lE
{yx
~ } and lE
{ XkX~
}, 
We saw, on page 363, an 
application of this result to the linear prediction problem. In this case, y 
is the value of the process at the time n, and the Xj are the values of that 
same process at the previous P times. We will see another application of 
this result on page 366 when we discuss the Wiener filter. 
COMMENT: the fact that the norm H is associated with a scalar product 
plays a fundamental role in the demonstration of the theorem. If we choose 
other norms, such as the norm 11.llp in the case of the space reP: 
IIxllp = (L,f=l IXj IP) lip 
which is not associated with a scalar product for p =1= 2, the theorem does 
not apply. This is why, independently from its physical significance, which is 
not always relevant, it is often preferable to use a quadratic approximation 
criterion since the projection theorem provides a simple analytical solution to 
the optimization problem. 

Chapter 9 - The Least Squares Method 353 
9.2 The least squares method 
C. F. Gauss came up with the idea of the "least squares" method and used it 
to study the movement of the planetsl . Based on a sequence of a planet's N 
positions (xl(n),x2(n)), n E [L.N] in its orbit plane (Figure 9.2), the problem 
was to estimate the parameters characterizing the general equation of an ellipse: 
aXI + bx~ + CXlX2 + dXl + eX2 - 1 = O. 
Figure 9.2 - Originally, the least squares method was created to study the movement 
of the planets 
Because of measurement errors, the observed values did not all perfectly 
belong to a single ellipse. Gauss's idea was to choose the values of the param-
eters that would characterize a "mean" ellipse. Mathematically speaking, the 
goal is to minimize the sum of the square deviations of the points observed on 
the ellipse to be determined, which amounts to choosing the values of a, b c, d 
and e that minimize: 
N L (axi(n) + bx~(n) 
+ cXl(n)x2(n) + dXl(n) + eX2(n) _1)2 
n = l 
This is actually a rather optimistic objective, since it means we have to 
assume that if the model works, we wouldn't be wrong, "on average". As for 
the solution, it is of a simple form, which is not surprising since setting the 
derivatives with respect to the various unknowns to zero leads to first degree 
equations. 
9.2.1 
Formulating the problem 
Consider a sequence of scalar observations y( n), presented as the sum of a 
signal s(n; ho) the type of which is known and of an additive noise w(n). The 
1 In 1801, Gauss calculated the orbit of the asteroid Ceres by observing it for 41 days. 
At the moment where Ceres became hidden from view because of the Sun's light, Gauss 
managed to predict where the asteroid would reappear. 

354 Digital Signal and Image Processing using MATLAB® 
vectorial parameter ho represents the parameter we wish to estimate. We have: 
y(n) = s(n; ho) + w(n) 
The noise w(n) encompasses both the measurement noise and the modeling 
noise, the latter expressing a lack of a priori knowledge of the theoretical model 
s(n; ho). 
Based on the N observations {y(1), .. . , y(N)}, the least squares estimator 
is the value of h that minimizes the square deviation: 
N 
J(h) = L Iy(n) - s(n;hW = (y - S(hO))H(y - s(ho)) 
(9.5) 
n = l 
where y = [y(1), ... , y(N)]T and s(ho) = [s(1 ; ho), . . . , s(N; hO)]T. We already 
encountered this type of problem in paragraph 8.3. The goal was to estimate the 
frequency and the amplitude of the complex exponential s(n; ho) = oPe2j7rf~n 
corrupted by noise. In that case, the least squares method consisted of deter-
mining h = (h, a) such that: 
N L I y(n) - ae2j7rhnl2 
n = l 
would be minimum. We saw that the solution suggested in paragraph 8.3 did 
not have a simple analytical form for h because the expression of s(n; h) is 
not linear with respect to h. However, when the model is linear, the problem 
can easily be solved. 
9.2.2 The linear model 
Let us now assume that the signal s( n; h) is expressed linearly as a function 
of the unknown parameter ho = [h(O) 
h(P -1)f according to the ex-
pression: 
s(n; ho) = xn,lh(O) + ... + xn,ph(P - 1) 
The known quantities Xn,j are sometimes referred to as regressors. There-
fore the observation has the expression y(n) = xn,lh(O) + ... + xn,ph(P -1) + 
w(n) where w(n) refers to the noise. By stacking N successive observations, 
we get the matrix expression: 
XI,P ] [ h(O) 1 [W(1)] 
x~,p 
h(P:- 1) + w(~) 
which can be written, using obvious notations: 
Y =Xho+w 
(9.6) 

Chapter 9 - The Least Squares Method 
355 
where ho = [h(O) 
h(P -1)f refers to the actual value and where X is 
the (N x P) matrix with Xn,j as its generating element. 
9.2.3 The least squares estimator 
Starting with N observations y = [y(l) 
y(N)f, the least squares esti-
mator is the argument h that minimizes the square deviation: 
J(h) = (y - Xh)H (y - Xh) 
(9.7) 
First consider the case of a noiseless observation expressed as y = Xho. 
To estimate ho, we need as many equations as there are unknowns, hence we 
choose N = P. If we assume that the square matrix X is invertible, the 
solution can be written simply as: 
Notice that in this case, the square deviation, expression 9.7, is precisely 
equal to O. Therefore, in the absence of noise, the observation of P of the 
signal's values is enough to estimate ho , and to do so without making an error. 
In the presence of noise, this is no longer the case: there usually is no vector 
h that simultaneously verifies the N equations y = X h , that is to say such 
that J(h) = o. At best, we can hope to find a vector h that minimizes J(h). 
But then why would we choose N » P? The answer is given by property 9.1 
on page 357, which states that the least squares estimator's variance usually 
decreases as N increases. Although the least squares method was not originally 
meant for probabilistic purposes, this result justifies its use in the presence of 
additive noise. 
We now go back to the minimization problem, to consider the case where 
the number N of equations is greater than the number P of unknowns: the 
system is said to be over-determined. The fact that there is no vector h such 
that X h is exactly equal to the observation vector y is equivalent to saying 
that the length N vector y does not belong to the vector subspace generated 
by the P column vectors of X. This vector subspace is called the image of X 
and is denoted by Image(X). The least squares method consists of finding the 
vector of Image(X) closest to y in terms of square distance, hence the solution 
is given by the projection theorem. 
Solving 
Using the same notations as in the projection theorem, }{ is the space eN of 
length N vectors and C = Image(X) is the subspace generated by the P column 
vectors of X. Any vector of C can be written X h where h is any length P 
vector. The orthogonality principle states that the vector Xh E C we wish to 

356 Digital Signal and Image Processing using MATLAB® 
determine is such that (y - Xh) is orthogonal to the columns of X. This can 
be written in matrix form as follows: 
(9.8) 
Figure 9.3 shows a representation of Image(X), as well as of the orthogonal 
projection of y onto Image(X). 
Y 
x 
h 
@ 
i :...---IIY - so il \, 
, , 
, 
'so 
Figure 9.3 - Image of X 
Remember that the orthogonal projection of y onto C is unique. This does 
not mean, however, that h is unique. We have the following results: 
- if the rank of X is P, in other words if X is a full rank matrix, then 
the P column vectors of X are independent, and the (P x P) matrix 
X H X is invertible, and the solution we are trying to determine has the 
expression: 
(9.9) 
In the particular case where P = N, the matrix X is square, and 
(XH X) -l XH = X - 1 . We end up with the usual inverse of a linear 
system of P equations with P unknowns. 
- if the rank of X is such that R < P, there are P - R independent length 
P vectors, referred to as u, such that Xu = O. The space N(X), the 
dimension of which is generated by these vectors, is called the kernel of 
X. 
In that case, the orthogonal projection is always unique. However, the 
vector h is no longer unique: let h be a vector that verifies 9.8, that is to 
say such that XH Xh = XH y. Then for any u E N(X), XH X(h+u) = 
X H X h + 0 = X H Y and hence the vector 9 = h + u also verifies the 
equation XH Xg = XHy. Therefore, there is an infinite number of 
solutions to equation 9.8. They are all completely defined, except for an 
additive vector belonging to the kernel of X. Among all these vectors, 

Chapter 9 - The Least Squares Method 357 
one of them has the minimum norm, the one orthogonal to the kernel of 
X. In terms of square deviation values, the solutions are of course all 
equivalent. 
The conclusion is that there is always at least one solution to the equation 
X H X h = X H y, called the least squares estimator, and we will denote it with: 
(9.10) 
X # is called the pseudo-inverse of X. There are two ways of calculating 
the pseudo-inverse with MATLAB®: 
- the function pinv, which is used as follows: 
II 
h=pinv ex) *y 
- the \ operator, which is used as follows: 
II 
h= X \ y 
These two computations only provide the same result if X is a full rank 
matrix (R = P). Otherwise, piny returns among all possible solutions the 
minimum norm solution, whereas \ returns the solution that has at most R 
non-zero components. Of course, these two solutions lead to the same square 
deviation, and without any further constraint, neither is better than the other. 
If X is a full rank matrix, we can always use the expression: 
(9.11) 
which then leads to h = (XH X)-l XH y . However, the computation with 
MATLAB® can be numerically less accurate than with the other two methods. 
Property 9.1 (Variance of the least squares estimator) Consider 
the 
model y = X ho + w where w is assumed to be a white zero-mean noise with the 
covariance a 2I. XHX is assumed to be invertible, and h = (XHX)-lXHy 
denotes the least squares estimator. Then h is an unbiased estimator of ho 
and the cumulated variance of all the components of h is given by: 
HINTS: by replacing y = Xho + win h = (XHX)-lXHy, we 
get: 
(9.13) 
(9.12) 

358 Digital Signal and Image Processing using MATLAB® 
If we change over to the expectation on both sides and use the 
fact that lE { w} = 0, we have lE {h} = ho, hence the least squares 
estimator is unbiased. Using 9.13, we can also write: 
If we change over to the expectation of the two members, we infer 
that the covariance matrix of h defined by: 
(9.14) 
has the expression: 
where we have used the fact that lE {ww H } = a 2 I. Finally, we get: 
Notice that a~ is the sum of the variances of the P components of 
the estimator h. 
In many practical problems, the sequence of the regressors involved with 
the matrix X is chosen so that the matrix X H X / N is almost equal to a 
matrix of the type s2 I p where I p is the (P x P) identity matrix. In that case, 
(XH X) -l ~ Ip/Ns 2 and Tr((XH X) -l) ~ P/Ns2 . By replacing this in 9.12, 
we get: 
a 2 P 
a~~-­
S2 N 
This result shows that the smaller the variance is, the higher the number N 
of observations, the higher the ratio p = S2 / a 2 and the smaller the dimension 
of the parameter we are trying to identify. 
Finally, note that expression 9.9 of the least squares estimator has a linear 
form with respect to the observation y. Furthermore, it can be proven [27J 
that it is, among all the unbiased linear estimators, the one with the minimum 
covariance in the case of white noise. It is called the BLUE (Best Linear 
Unbiased Estimator). 
In the property 9.1, if w is assumed to be a zero-mean noise with the 
covariance a 2C, it is easy to prove by whitening with C - 1/ 2 that: 
(9.15) 
is the best linear unbiased estimator (BLUE) and the covariance matrix of h 
defined by 9.14 has the expression: 
C h = a 2(XH C- 1 X)-l 

Chapter 9 - The Least Squares Method 359 
Weighted least squares 
Expression 9.9 can be generalized by considering the scalar product defined in 
reP by the expression: 
(9.16) 
where W refers to a positive matrix called a weighting matrix. W is typically 
used to take into account the fact that the measurement noise is not white 
or that the observations are not quite "stationary". In this last case, the first 
components of y should probably be given less importance. The classic example 
is to take: 
o 
o 
W= 
o 
(9.17) 
o 
o 
o 
1 
where the real value A E (0,1) is called the forg et factor. If A is close to 0, the 
"past" values of yare assigned a very small weight. 
If we once more apply the orthogonality principle, but use the scalar product 
defined by expression 9.16, equation 9.8 can be rewritten as follows: 
XHW(y - Xh) = 0 
If we assume that X H W X is invertible, we infer the expression of the 
weighted least squares estimator: 
(9.18) 
This leads to the same expression as 9.9 when choosing W = I N . 
We are now going to see a few examples that use a least squares estimator. 
In these examples, we chose A = 1. This study can of course be completed by 
considering a forget factor A < 1. 
EXAMPLE 9.1 (Polynomial approximation) 
We are going to try to approximate the function sin(x) by a (P - 1) degree 
polynomial on the range (-7r, +7r). Let ho, ... , h p - l be the P coefficients of 
this polynomial. To achieve the approximation, we have decided to minimize 
the square deviation between the polynomial we wish to determine and the 
function sin( x) for a set of values of x from -7r to +7r by steps of b.. Let {Xl, 
... , XN } be the set of these values and Yn = sin(xn) the corresponding values. 
COMMENT: in practice, the pairs of numerical values such as (Xl, yd, ... , 
(XN,YN) are obtained through a series of measures, and we then try to find 
a polynomial approximation that gives Y as a function of x over the complete 
range that was chosen. 

360 Digital Signal and Image Processing using MATLAB® 
1. by writing down the values Sn of the polynomial we are trying to deter-
mine at the points Xl, . . . , X N, show that the least squares optimization 
problem can be interpreted as the orthogonal projection of the vector 
[Yl 
YN] T onto a space generated by P vectors constructed from 
Xl, ... , XN. Give the least squares solution; 
2. write a program that performs the approximation. Check the quality of 
the results on the (-'iT, +'iT) grid by steps of ~ = 0.2. 
HINTS: 
1. let Xl, . . . , XN be the values used to perform the approximation 
and Yl, ... , YN the ones corresponding to the sine function. 
We wish to approximate the Yn by an expression of the type: 
Sn = ho + hlxn + ... + hp_1X~-1 
If we stack the Sn we get a vector generated by the P column 
vectors ej = [xi 
x~ 
x~( 
where j E {O, 1, ... , P - I}. 
Hence the problem is equivalent to finding the best approxi-
mation for y = [Yl , ... ,YN] T belonging to the space generated 
by the ej. Using the projection theorem, we find that h ver-
ifies XT Xh = X T y where X is the (N x P) matrix defined 
by X = [eo 
ep-l]. It can be proved that if Xi -=J Xj for 
any pair (i,j) with i -=J j, then X is a full rank matrix; 
2. the following programs returns Figure 9.4: 
%===== approxsin.m 
% Approximation of a sine function 
% with a (p-l) degree polynomial 
x=(-pi: .2:pi)l; N=length(x); 
y=sin(x); 
% function to be approximated 
pml=14; 
% polynomial degree 
X=zeros (N ,pml) ; 
%===== matrix X 
for ii=i:pmi, X(:,ii)=x .-(ii-i); end 
%===== solving the system 
h=X \ y; 
%===== verification 
x=(-pi: .Ol:pi) '; N=length(x); y=sin(x); X=zeros(N,pml); 
for ii=i:pml, X(:,ii)=x .-(ii-i); end 
s=X*h; plot(x,y-s); grid 
9.2.4 Identifying the impulse response of a channel 
The objective in a number of problems is to find a model for describing the 
communication channel as a simple FIR filter. Let {h(O), ... , h(P-1)} be its 

Chapter 9 - The Least Squares Method 361 
(10-9) 
4 ,---~--------~----------~--~----~---, 
2 
o 
-2 
-4 
-6 
-------- I--------t-------- -1 --------I-------- ... -------
--------
, 
" 
, 
" , 
, 
-- 1--
-
.,- -
-
-
-
-
-
-
-
- 1-
-
-
-
-
-
-
- r - - - - - - - ., - - - - - - - -,- - - - - - - -
T -
-
-
-
-
-
-
-, 
-
-
-
-
- --
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
- - - - - - - - ,- - - - - - - - r - - - - - - - , - - - - - - - -,- - - - - - - - T -
-
-
-
-
-
-
-. 
-
-
-
-
- --
-
-
-
-
-
-
-
j 
-
-
-
-
-
-
- , 
-
-
-
-
-
-
- T 
, , 
-8 
-i--------:--------1-------- -; --------:--------+--------: -------
I 
I 
I 
I 
I 
-1 0 
~ ------- _ 1-
-
-
-
-
-
-
- ~ -
-
-
-
-
-
- -j - - - - - - - -:- - - - - - - - ~ - - - - - - - -: - - - - - --
I 
I 
I 
I 
I 
-12 
_______ I ________ L 
______ ~ ________ : _______ i 
_______ : __ 
~
, 
" 
-14 L-__ -+ 
•• ~
' ____ ~ __ ~ ____ ~ ____ ~
' ____ ~ __ ~
' ~ __ ~ 
-4 
-3 
-2 
-1 
0 
2 
3 Jr 
4 
Figure 9.4 - Error resulting from the polynomial approximation 
impulse response. In order to achieve certain processes, the h(k) have to be 
estimated. This is called, in this context, channel identification. 
The easiest method consists of sending a known signal, called a training 
signal, and to observe the channel's output. In practice, if the channel varies 
with time, we have to repeat this operation by periodically sending the training 
signal. The channel output observation can be written:indexsymbol!channel 
identification 
y(n) = h(O)x(n) + ... + h(P - l)x(n - P + 1) + w(n) 
where w(n) represents an additive noise superimposed onto the transmitted 
signal (Figure 9.5). 
x(n) 
y(n) 
Figure 9.5 - Channel identification 
If we write the observation sequence in matrix form for n from P to N +P-1, 

362 Digital Signal and Image Processing using MATLAB® 
we get: 
[ 
y(P) 1 
[ 
x(P) 
x(l) ][ h(O) 
1 
y(N +:P -1) 
x (N +:P - 1) 
x (N) 
h(P - 1) 
[ 
w(P) 
1 
+ 
: 
=Xh+w 
w(N+P-1) 
With the same matrix notations as before, the vector h that minimizes the 
difference (y - Xh)H(y - Xh ) is given by expression 9.9 rewritten below: 
(9.19) 
where we have assumed X to be a full rank matrix. 
COMMENT: you may be wondering whether some input sequences are 
better than others, and the answer is yes. If you consider the sequence x(n) as 
the realization of a WSS process, chosen in a very vast class of WSS processes, 
it can be shown that optimal performances are achieved when the process 
is white. This is why in the practical problems of filter identification, the 
learning sequences used are as often as possible chosen "similar" to white noise. 
Equation 9.19 should be compared with the result from exercise 7.5. If we 
multiply and divide by N, we can also write: 
(9.20) 
an expression where R is written as an input autocovariance matrix and r as 
an output/input covariance vector of the filter. We will encounter later on an 
expression similar to 9.20 when we discuss the Wiener filtering. 
9.3 Linear predictions of the WSS processes 
9.3.1 
Yule-Walker equations 
The Yule-Walker equations, laid down on page 307, relate the parameters (aI, 
... , ap, (}"2) of an AR process defined by the recursive equation 7.54 to its 
covariance coefficients. As we are going to see, these equations are also the 
ones that relate the covariance coefficients of any WSS random process to the 
linear prediction coefficients. 

Chapter 9 - The Least Squares Method 363 
Consider a zero-mean, WSS random process x( n). The general expression 
of the process's linear predictor x( n) at the time n, constructed from its N past 
values, is of the type: 
N 
x(n) = a1x(n - 1) + ... + aNx(n - N) = L aix(n - i) 
(9.21) 
i = l 
Theorem 9.1 states that the coefficients al, ... , aN that minimize the mean 
square error lE {Ix( n) - x( n) 12}, between the actual value x( n) and the predicted 
value x( n) are such that the error: 
e(n) = x(n) - x(n) 
(9.22) 
is orthogonal to any x(n - k) with 1 :::; k :::; N. This can be written: 
lE ((x(n) - x(n))x*(n - k)} = 0 for 1:::; k :::; N 
(9.23) 
Using the expectation's linearity, we get: 
lE {x(n)x*(n - k)} -lE {x(n)x*(n - k)} = 0 
By replacing x(n) by its expression 9.21, then by again using the expecta-
tion's linearity, we get: 
N 
lE {x(n)x*(n - k)} - L ailE {x(n - i)x*(n - k)} = 0 
i = l 
Because the process is stationary, lE {x( n )x* (n - k)} = R( k), and: 
N 
R(k) - L aiR(k - i) = 0 for 1 :::; k :::; N 
(9.24) 
i = l 
The minimum mean square error is given by 9.2 which here has the expres-
sion: 
P 
E;2 = lE {le(n) 12} = lE {(x(n) - x(n))x*(n)} = R(O) - L aiR( -i) (9.25) 
i = l 
Stacking 9.25 and the N equations 9.24 in matrix form leads us to: 
R(O) 
R( -1) 
R( -N) 
R(1) 
R(O) 
(9.26) 
R(-1) 
R(N) 
R(1) 
R(O) 

364 Digital Signal and Image Processing using MATLAB® 
These equations are called the Yule- Walker equations, or the normal equa-
tions, and are the same as the equations 7.59 laid down in page 307. They 
allow us to calculate the prediction coefficients and of the minimum prediction 
mean-square error using the covariance matrix of a WSS process. In practice, 
the exact covariances can be replaced by their estimates according to expression 
7.33. 
We are now going to see how these prediction coefficients can be expressed 
in the particular cases of a harmonic process and of an AR process. 
9.3.2 Predicting a WSS harmonic process 
As a result of property 4.4, we can suppose that O;k are P zero-mean uncorre-
lated complex variables, with the respective variances (J~ 
and !k P frequencies. 
We have to check that any linear combination of the solutions - of the 
type 2:k O;k exp(2j7r fkn) - is a WSS process. All we need to do is choose P 
zero-mean uncorrelated complex variables, with any variances, as the O;k. 
After 
an 
obvious 
notation 
change, 
the 
recursive 
equation 
x(n) + b1x(n -
1) + ... + bpx(n -
P) 
0 can be rewritten as 
x(n) = f31x(n - 1) + ... + f3px(n - P). 
As you can see, a process can be exactly predicted from its past. This 
is what is meant by the term "almost deterministic", used for such random 
processes. 
The recursive equation x(n) + b1x(n - 1) + ... + bpx(n - P) = 0 should be 
compared with equation 7.54 defining a P-order AR random process but the 
second member of which would be null and the poles of which would be on the 
unit circle. 
The previous results, obtained for a complex harmonic process, are still true 
for a process x( n) that is the sum of P real sines of the type: 
p 
x(n) = L Ak cos(27r fk n + <Pk) 
k=l 
where {<pd refers to a sequence of independent uniform random variables on 
(0,27r) independent of the A k . We can rewrite: 
P 
2P 
x(n) = L ~k 
(eFhe2j7r!kn + e-jihe-2j7r!kn) = L O;k(~ 
k=l 
k=l 
where the (k are 2P values of the type e±2j7r /k and where the O;k are of the type 
Ak exp(±j<pk) and are zero-mean uncorrelated r.v. with variances (J~. 
Hence 
x(n) appears as the sum of 2P complex exponentials the frequencies of which 
come in pairs of a positive and a negative value. Because of property 4.4, x(n) 
obeys a recursive equation of the type x(n)+ b1x(n-1)+ ·· ·+b2Px(n-2P) = 0 

Chapter 9 - The Least Squares Method 365 
where the 2P degree polynomial has all of its roots located on the unit circle, 
come in pairs of complex conjugates. Therefore B(z) has real coefficients. 
9.3.3 Predicting a causal AR-P process 
Property 9.2 Let x(n) be a P order real AR process defined by equation 7.54 
the expression of which is recalled here: 
x(n) + alx(n - 1) + ... + apx(n - P) = w(n) 
with A(z) i- 0 for Izl 2: 1 and where w(n) is a WSS white process with the 
variance 0"2. Then for any N 2: P , the prediction coefficients aI, . . . , aN to 
the N -th order are given by: 
ai = { -ai 
for 
1:::; i :::; P 
o 
for P < i:::; N 
and the minimum prediction mean square error is equal to 0"2. 
You can see this by rewriting equation 7.54 as follows: 
x(n) = -al x(n - 1) - ... - apx(n - P) + w(n) = x(n) + w(n) 
(9.27) 
where we have defined: 
x(n) = -alx(n - 1) - ... - apx(n - P) 
(9.28) 
For any N 2: P, x(n) coincides with the best linear estimation given by 
9.21. Indeed, for any k 2: 1: 
lE {w( n )x* (n - k)} = lE {w( n)} lE {x* (n - k)} = 0 
(9.29) 
This result simply expresses the fact that x(n - k) is a function only of 
w(n - k), w(n - k - 1), etc. since the solution is causal and w(n) is white. 
Therefore x(n - k) and w(n) are not correlated. We infer 9.29 from this result. 
By replacing, according to 9.27, w(n) by x(n) - x(n) in 9.29, we get: 
lE {(x*(n) - x*(n))x(n - k)} = 0 for Vk 2: 1 
This relation is identical to 9.23. It proves that x( n) is the best linear 
prediction of x(n), in terms of the minimum mean square error, calculated 
on any past with a duration higher than P. Notice that it only uses its last 
P values. In other words, for a P-order AR, the orthogonal projection on the 
entire past coincides with the orthogonal projection on the last P instants. Still 
another way of saying it is that for an AR model, the prediction coefficients 
coincide with the model's parameters. This result is usually false for any WSS 

366 Digital Signal and Image Processing using MATLAB® 
process, for example an MA. This should not stop us, however, from looking 
into the prediction of these processes. 
Furthermore, the white input process w(n) appears as the prediction error 
and its variance as the minimum mean square error. 
We also should mention that in the field of statistics, the name linear re-
gression is also used for referring to linear prediction. Hence the name autore-
gressive given to these processes. 
9.4 Wiener filtering 
Consider the diagram in Figure 9.6. 
y(n) 
~'(n) 
~(n) 
~g(n) 
+ 
x(n) 
t -
Figure 9.6 - The Wiener filter 
Let y(n) be the observation signal and x(n) the desired signal. x(n) and 
y(n) are assumed to be zero-mean, WSS, real random processes with stationary 
covariances. Stationarity implies: 
{ 
lE {x(n + k)x(n)} = Rxx(k) 
lE {y(n + k)y(n)} = Ryy(k) 
lE {x(n + k)y(n)} = Rxy(k) 
All of these sequences are assumed to be known. 
We are going to determine the linear filter with the impulse response g(n) 
that minimizes the positive quantity: 
(9.30) 
where: 
x(n) = 'Lg(k)y(n - k) 
k 
The projection theorem gives us the solution. If we use the same notations 
as in paragraph 9.1, H is the space of square summable random variables and 
C is the subspace generated by linear combinations of elements of the sequence 
{y(n)} written x(n) = L-k g(k)y(n - k). 
The filter that minimizes the square deviation between x(n) and x(n) is 
such that the difference x( n) - x( n) is orthogonal to any element belonging to 
the space C. Hence, in terms of orthogonality in the Hilbert space of square 
summable random variables, we have for any k: 
lE {[x(n) - x(n)]y(n - k)} = lE {[x(n) - L-m g(k)y(n - m)]y(n - k)} = 0 

Chapter 9 - The Least Squares Method 367 
If we develop and use the stationarity hypotheses of y(n) we get: 
Rxy(p) = Lg(k)Ryy(p - k) 
(9.31 ) 
k 
This equation is called the Wiener equation and the solution is called the 
Wiener filter. It is in agreement with the expression 9.20 we saw in the case of 
channel identification. 
Unconstrained solution in C 
Let us first consider the case where C is the set of linear combinations of all 
the variables of the sequence {y(n)}: the summation in equation 9.31 is then 
performed from -00 to +00. Hence we have a convolution of g(k) with Ryy(k). 
To solve equation 9.31, we can then use the DTFT. If we denote by Syy(f) 
and Sxy(f) the respective DTFTs of Ryy(k) and Rxy(k), and if we use the 
convolution properties, we get: 
G(f) = Sxy(f) 
Syy(f) 
(9.32) 
Expression 9.32 is usually in the form of a rational function with poles 
inside and outside the unit circle. The stable solution is then bilateral. We 
can, however, extract by truncation a causal approximation if we tolerate a 
certain delay. 
Imposing time constraints 
The following three cases show some practical interest, but unfortunately they 
cannot be solved as easily as expression 9.32: 
- C is the set of linear combinations of y(k) with k :::; n up to the present 
time n. In this case: 
n 
+00 
x(n) = 
L 
g(n - k)y(k) = L 
g(m)y(n - m) 
k = -oo 
m = O 
This is called filtering; 
- C is the set of linear combinations of y(k) with k :::; (n - p), P > 0, up to 
the past time (n - p). In this case: 
n-p 
+00 
x(n) = L 
g(n - k)y(k) = Lg(k)y(n - k) 
k = -oo 
k = p 
This is called prediction; 

368 Digital Signal and Image Processing using MATLAB® 
- C is the set of linear combinations of y(k) with k :::; (n + p), P > 0, up to 
the future time (n + p). In this case: 
n+p 
+00 
x(n) = L g(n - k)y(k) = L g(k)y(n - k) 
k = -oo 
k = -p 
This is called smoothing. 
In the case of filtering, C is the set of linear combinations of all the variables 
y(n) for n E {-oo, ... , O} and equation 9.31 can be written: 
00 
Rxy (p) = L g(k)Ryy(p - k) 
k = O 
Because the summation starts at 0, it cannot be solved simply by calculating 
the DTFT of the two sides. Performing the calculation without taking any 
precautions does lead to a causal solution but one that is not stable. The right 
solution was found by Wiener. Its expression goes beyond the aim of this book, 
but can be found in [30]. 
9.4.1 
Finite impulse response solution 
Imagine that we are trying to find as a solution for g(n) an FIR filter with a 
length N = A + C, denoted by: 
g(-A) , . . . ,g(-l) ,g(O), . . . ,g(C -1) 
A is used here to take into account a certain delay. Using equation 9.31 
leads us to the expression: 
g( -A)Ryy(p + A) + ... + g( -l)Ryy(p + 1) + 
g(O)Ryy(p) + g(l)Ryy(p - 1) + ... + g(C - l)Ryy(p - C + 1) 
If we stack the N = A + C expressions of Rxy (p) for P from 0 to N - 1, we 
get the matrix expression: 
Rg=r 
where: 
[ 
Ryy(O) 
R = 
Ryy(~ 
-1) 
Ryy( - N + 1) 1 
Ryy(O) 
and r = 
(9.33) 
Rxy(C - 1) 

Chapter 9 - The Least Squares Method 369 
If R is invertible, the solution then has the expression: 
(9.34) 
It should be compared to expression 9.20. 
Generally speaking, you can see that finding the solution requires the inver-
sion of the matrix R. It can however be avoided by implementing the gradient 
algorithm. 
9.4.2 
Gradient algorithm 
A general and simple idea for calculating a minimum with respect to 9 of the 
function J(g) is to calculate g(n) at the n-th step using the value of g(n - 1) 
at the (n - l)-th step, going in the direction opposite to that of the gradient. 
This can be written as follows: 
IL 8J(g) I 
g(n) = g(n - 1) - - --
2 
8g 
g= g n- l 
where IL is a positive scalar called the gradient step. 
(9.35) 
If the function J(g) is regular enough and if IL is small enough, g(n) should 
converge and hence the resulting convergence value obeys equation 9.35. There-
fore, it sets the gradient to zero, which makes it a good candidate for the search 
for a minimum. 
Let us apply this result to equation 9.30. If we use the fact that g(n) has 
a finite length, we get: 
J(g) 
lE {(x(n) - gT y(n))(x(n) - yT(n)g)} 
Rxx(O) - 2gT r + gT Rg 
the gradient of which with respect to 9 has the expression: 
8J(g) = -2(r - Rg) 
8g 
(9.36) 
By replacing this expression, calculated at the point 9 = g(n -1), in equa-
tion 9.35, we get the following recursive equation: 
g(n) = g(n - 1) -IL (Rg(n - 1) - r) 
(9.37) 
called the gradient algorithm. 

370 Digital Signal and Image Processing using MATLAB® 
Study of equation 9.37 
We are going to determine the expression of the solution g(n) to equation 9.37 
as a function of the initial value g(O) and of the matrix A = 1 - pR. In order 
to do this, we need to write the recurrent equations for the first n values: 
g(n) 
g(n - 1) 
g(l) 
(1 - p,R)g(n - 1) + W 
(1 - p,R)g(n - 2) + w 
(1 - p,R)g(O) + W 
x 
1 
x 
(1 - p,R) 
If both sides are multiplied on the left by the indicated quantities, we get: 
g(n) 
p,(1 + ... + A n-1)r + Ang(O) 
p,(1 - A n)(1 - A)-lr + A ng(O) 
(1 - An)R-1r + A ng(O) 
(9.38) 
where we have imposed A = (1 -p,R) and used the identity (1 + .. ·+A n-l)(I_ 
A) = 1 - An. 
A classic result states that if the eigenvalues of A have a modulus smaller 
than 1, the matrix An tends to the zero matrix when n tends to infinity. But 
since any eigenvector of R associated with the eigenvalue Ai is an eigenvector 
of A associated with the eigenvalue (1 - AiP,) , the stability condition can be 
written -1 < 1 - AiP, < 1. Because Ai is positive (eigenvalue of a covariance 
matrix) , 0 < p, < 2/ Ai. Therefore, if p, is such that: 
2 
o < p, < -----,----,-
maxi(Ai) 
then the algorithm converges and the convergence solution, denoted by gCX), 
obeys the recurrent equation. This means that gCX) = gCX) + p,(RgCX) - r) , which 
leads to RgCX) = r , which is precisely expression 9.33, Rgw = r , leading to the 
Wiener filter gw. 
The misadjustment is defined by the expression: 
.6.n = J(g(n)) - J min 
(9.39) 
where J min refers to the criterion value with the optimal filter. First we must 
determine the criterion at the n-th step. We have: 
J(g(n)) 
Rxx(O) - 2gT(n)r + gT(n)Rg(n) 
Rxx(O) - 2gT(n)Rgw + gT(n) Rg(n) 
The criterion value with the optimal filter gw is given by: 
J min = Rxx(O) - 2rT gw + g; Rgw = Rxx(O) - g; Rgw 

Chapter 9 - The Least Squares Method 371 
This means that the misadjustment has the expression: 
6.n = (g(n) - gwf R(g(n) - gw) 
Therefore, when n -t +00, the speed at which the misadjustment decreases 
to 0 is related to the distribution of the eigenvalues of R. 
Simulations 
The following program uses simulation to evaluate the performances of the 
gradient algorithm. For Rand 9 such that r = Rg, the square deviation 
is plotted against the number of iterations. The program shows that about 
a thousand iterations are necessary to obtain gw with a good accuracy. The 
gradient algorithm is slow: 
%===== simulgraddeter.m 
%===== eigenvalues 
lambda=[1 0.01 0.0001]; nvp=length(lambda); 
R=diag(lambda); gw=ones(nvp,1); rxy=R*gw; 
%===== 
initializations 
lambda=[1 0.01 0.0001]; nlambda=lengthClambda); 
R=diag(lambda); gw=ones(nlambda,1); rxy=R*gw; 
%===== 
mu=1/2; N=100000; 
d2g=zeros(N,1); gn=zeros(nlambda,1); 
for n=1:N 
end 
gn=gn+mu*(rxy-R*gn); 
d2gCn)=(gn-gw)'*Cgn-gw); 
semilogx(d2g); grid 
Figure 9.7 shows the evolution of the square deviation over the course of 
N = 100,000 iterations. Notice that the graph shows three different parts corre-
sponding to the three consecutive decompositions of the modes, corresponding 
themselves to the three eigenvalues of R , from the smallest to the largest. This 
is a very general result. The determinating factor in the decreasing speed of 
the modes associated with each of the eigenvalues is the product /-lAi. 
The speed increases as /-lAi gets closer to 2 (from below). Hence, for the 
highest eigenvalue, the choice of /-l = 1 is the right one; however, for the smallest 
eigenvalue 0.0001, a much greater value of /-l would be needed to accelerate 
the decrease. Unfortunately, we can not change the value of /-l, otherwise the 
algorithm might diverge. 
The conclusion is that the number of iterations leading to the solution is 
related to the smallest eigenvalue, and that the choice of the gradient step for 
convergence is associated with the highest eigenvalue. 
A more subtle method would be to work separately on each of the three 
eigenvectors of the covariance matrix with three different steps better adapted 

372 Digital Signal and Image Processing using MATLAB® 
Figure 9.7 - Evolution of the square deviation between the actual value of gw and 
the obtained value, as a function of the number of iteration steps of the gradient 
algorithm 
to the three eigenvalues. This can be expressed as follows: 
[
ILl 
g(n) = g(n - 1) -
~ 
o 
IL2 o 
~l 
(Rg(n-1)-r) 
IL3 
In fact, as we are going to prove, the right matrix is the inverse of the 
covariance matrix. To understand this, consider the following algorithm: 
= 
_ [8
2J(9)] - 1 8J(g) I 
g n -l 
8 2 
8 
9 
9 
g = g n - l 
(9.40) 
in which ILl was replaced with the inverse of the Hessian of J(g). A simple 
calculation shows that J(g) is equal to 2R. Therefore, at the n-th step and for 
any g(n - 1): 
g(n) = g(n - 1) - R - l ( -r + Rg(n - 1)) = R -lr = gw 
(9.41) 
Hence the algorithm reaches its minimum in one step, meaning that the 
right matrix is not ILl, with IL having the same order of magnitude as the 
inverse of the eigenvalues of R , but precisely R- l . The major drawback is 
that this algorithm loses its essential quality, that is to say its simplicity. 
Comments 
The criterion J(g) is shaped like a bowl, the bottom of which corresponds to 
the Wiener solution we wish to reach. As IL increases, J (g) takes bigger steps 
towards the bottom of the bowl. However, as it gets closer to the minimum, it 

Chapter 9 - The Least Squares Method 373 
is better to have a small value of j.t if we want to get as close as possible. To 
give you an idea, a possible value for j.t is 2/(maxi(Ai) + mini(Ai)). 
The following program plots, against j.t , for a given number n of iterations 
the square deviation in dB between the actual value g = R -1r and the value 
given by expression 9.38 (for g(O) = 0): 
(9.42) 
You can see in Figure 9.8 that, for the values that were chosen, there is a 
minimum deviation in the neighborhood of the value 2/(maxi(Ai)+mini(Ai)) = 
1/(1 + 0.1) indicated previously: 
%===== gradmu1.m 
%===== (lambda * mu) must be < 2 
lambda=[1 0.1]; nvp=length(lambda); 
R=diag(lambda); gw=ones(nvp,1); 
rxy=R*gw; 
%===== 
N=50; mu=(1:0.02:1.99); L=length(mu); 
desj=zeros(L,1); 
% J(g) estimated 
for ii=1:L 
end 
mui=mu(ii) ; 
A=eye(nvp)-(eye(nvp)-mui*R)-N; 
gn=A*inv(R)*rxy; 
desj(ii)=10*log10((gn-gw)'*(gn-gw)); 
plot(mu,desj); grid 
%===== a possible value 
muO=2/ C1 +0 . 1) ; 
A=eye(nvp)-(eye(nvp)-muO*R)-N; 
gn=A*inv(R)*rxy; 
desjO=10*log10((gn-gw)'*(gn-gw)); 
hold on; plot(muO,desjO, '0'); hold off 
Influence of the eigenvalues of R on the performances 
We are now going to numerically study how the distribution of the eigenvalues 
of R influences the shape of the trajectories of g(n). We will assume that the 
Wiener solution is the filter with two coefficients gw(O) = 2 and gw(1) = 6. 
This is possible, for a given matrix, if we choose r = Rgw ' 
We saw that convergence was related to the eigenvalues of R. Remember 
that R is a covariance matrix (its size here is 2 x 2). Therefore, it has two 
positive eigenvalues. We are going to study changes in performance when 
the ratio p of its two eigenvalues varies. If we consider again the criterion's 
expression: 
J(go ,gd = Rxx(O) - 2[go 
gljr + [gO 
gljR [~~] 

374 Digital Signal and Image Processing using MATLAB® 
O,---~--~------~--~--~--~--~--~-, 
-10 
------
1 ------1------
~-
-----1------1------~------~-----~------
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
-20 
I 
I 
I 
I 
I 
I 
------------ , 
----- 1 
----- 1 
, 
, 
, 
, 
- 30 
, 
, 
-40 
I 
I 
I 
I 
I 
I 
I 
I 
------,------,------ r- -----,------,------r------r-----,------
-50 
- 60 
I 
I 
I 
I 
I 
I 
------,------l------r------~-----~----
I 
I 
I 
I 
I 
-70 
I 
I 
I 
I 
I 
I 
I 
I 
------,------1------ r ------,-----
, 
, 
- 80 
______ 1 ______ -.1 
_____ 1 
- 90L---~--~--~--~--~--~--~--~--~~ 
1 
1.1 
1.2 
1.3 
1.4 
1.5 
1.6 
1.7 
1.8 
1.9 
2 f1 
Figure 9.8 - Square deviation in dB plotted against p,. The '0' indicates the deviation 
obtained for the value 2/(maxi(Ai) + mini(Ai)) 
where R is positive. This is a second degree polynomial in (gO , gd that gener-
ates a paraboloid, the sections of which are ellipses. The eigendecomposition 
of R is as follows: 
where U is a 2 x 2 unitary matrix, the column vectors of which are the direc-
tions corresponding to the major and minor axes of the ellipse, and 0"5 and O"r 
are quantities that measure the ellipse's eccentricity. Let p = O"U0"5 and let 
0"5 + O"r = 1. This condition means that the power y(n) is constrained to be 
constant, because R is the covariance matrix of y(n), which is assumed to be 
stationary, and therefore the trace (sum of the diagonal elements) of R is equal 
to 2Ryy(O). But since the trace of a matrix is independent of the basis chosen 
for its decomposition, the trace is also equal to 0"5 + O"r = 1. This means we 
can make 0"5 and O"r parameters of our problem as functions of p, under the 
constraint of a constant power, using the two expressions: 
2 
1 
2 
P 
0"0 = -- and 0"1 = --
l+p 
l+p 
We start with a known initial value g(l) , and as p varies, observe the 
shape of the trajectory described by g(n) , which is calculated at each step 
n of the algorithm. This trajectory tends to the point with coordinates gw 
corresponding to the Wiener solution. The initial value can vary. 
We used the following program to carry out the numerical study: 
II 
%===== graddet22. m 
clear; N=70; 
% number of iterations 

Chapter 9 - The Least Squares Method 375 
gw=[2;6]; 
% Wiener filter 
ginit=[1 ;5]; 
% initialization 
rho=(2:5); lrho=length(rho); 
gn=zeros(2,N); gnc=zeros(N,lrho); 
mu=O.l; 
%===== unitary matrix 
theta=-3*pi!7; 
% step 
VP= [cos(theta) -sin(theta);sin(theta) cos(theta)]; 
%===== 
for ii=l:lrho 
rhoi=rho(ii); 
vpl=l/(l+rhoi); vp2=rhoi*vpl; % sigma_1-2 and sigma_2-2 
R=VP * diag([vpl vp2])*VP'; 
% Covariance matrix 
rxy=R*gw; 
gn ( : , 1) =gini t ; 
%==== Descent 
for n=2:N 
gn(:,n)=gn( : ,n-l)+mu*(rxy-R*gn(:,n-l)); 
end 
gnc(: ,ii)=gn'*[1;j]; 
end 
%===== 
plot([l j]*gw, 'ro'); grid 
hold on; plot(gnc); hold off 
The results are shown in Figure 9.9. You can see that whatever the initial 
value, the higher the eigenvalue ratio, the slower the convergence to the Wiener 
solution. 
The program graddet23 . m draws the ellipses associated with the criterion 
for each calculation point along the trajectory. The result is Figure 9.10: 
%===== graddet23.m 
clear; N=70; 
gw= [2; 6] ; 
ginit=[1;5] ; 
mu=O.2; 
%===== unitary matrix 
theta=-3*pi/7; 
% number of iterations 
% solution of the Wiener filter 
% initialization 
% step 
VP= [cos(theta) -sin(theta);sin(theta) cos(theta)]; 
rho=2; vpl=l/(l+rho); vp2=rho*vpl; 
R=VP * diag([vpl vp2])*VP'; 
rxy=R*gw; 
gn( :, l)=ginit; c=(ginit-gw)'*R*(ginit-gw); 
ellipse(gw,R,c); hold on 
plot(ginit(1),ginit(2),'yx') 
%===== descent 
for n=2:N 
gn( : ,n)=gn(: ,n-l)+mu*(rxy-R*gn(:,n-l)); 
c=(gn(: ,n)-gw)'*R*(gn( :, n)-gw) ; 

376 Digital Signal and Image Processing using MATLAB® 
g2
.-~~--~------~--------~------~-------. 
7.5 
------------ + 
-----------
~
--
7 
------------ T 
-----------
~
--
6.5 ____________ -1 
6 ------------,--
----------- , --
5.5 
--------------
5 
4.5 ------------ , ------------r -
4 ~------~------~--------~----~~------~ 
1 
1.5 
2 
2.5 
3 
3.5 g, 
Figure 9.9 - Gradient algorithm: evolution trajectories of the pairs (go(n),gl(n)) 
as functions of the ratios p E {2, 3, 4, 5} of the two eigenvalues of the matrix R , 
at a constant power, for four different initialization points. 
The Wiener filter is 
gw = {2, 6} 
ellipse(gw,R,c); 
plot(gn(1,n),gn(2,n),'yx') 
end 
gnc=gn'*[1;j] ; 
plot (gnc) 
%===== 
plot([1 j]*gw, 'yo'); grid 
plot([gw(1)-VP(1,1) gw(1)+VP(1,1)] , [gw(2)-VP(2,1) gw(2)+VP(2,1)]) 
plot([gw(1)-VP(1,2) gw(1)+VP(1,2)] , [gw(2)-VP(2,2) gw(2)+VP(2,2)]) 
hold off 
The vector g(n) - g(n - 1) can be interpreted as the vector tangent to the 
trajectory. But since we have: 
g(n) - g(n - 1) = p,(r - Rg(n - 1)) = p,R(g(X) - g(n - 1)) 
(9.43) 
when n -+ 00, the tangent vector tends to merge with the eigenvector of R 
associated with the smallest eigenvalue. This is what you can see in Figure 
9.10 which shows the two directions corresponding to the eigenvectors of the 
matrix R. 

Chapter 9 - The Least Squares Method 377 
6.05 
6 
5.95 
5.9 
5.85 
3.5 
Figure 9.10 - Gradient algorithm: trajectory with its elliptical isocriterion contours 
J(g(n) ) 
9.4.3 Wiener equalization 
Consider the diagram in Figure 9.11. The filter h(n) and the auto covariance 
sequences of x( n) and b( n) respectively are assumed to be known. x( n) and 
b( n) are also assumed to be centered and uncorrelated with each other. 
We wish to determine the best filter g( n) for finding the signal x( n) based on 
the observation of y(n). This operation is called an equalization. The resulting 
signal x(n) is not quite the same as x(n) but also depends on a certain residue 
of terms associated with the other values of the input sequence: this residue is 
called interference. 
yen) ~ 
x<j 
~ 
Figure 9.11 - Equalization by Wiener filtering 
In the absence of noise, the equalizing filter has of course the impulse re-
sponse g(n) with (g * h)(n) = o(n) and the complex gain G(J) = 1/ H(J). 
Because g( n) forces the interference to zero, the filter g( n) is called a zero 
forcing (ZF) filter. 
In the presence of noise, the zero forcing filter is not the one that minimizes 

378 Digital Signal and Image Processing using MATLAB® 
the variance of the error between the sequence x( n) and the equalizer output 
sequence x(n) . The optimum filter is the Wiener filter. Without any time 
constraints, and based on formula 9.32 expressing the solution in the frequency 
domain, we have to calculate Sxy(f) and Syy(f) as functions of Sbb(f) , Sxx(f) 
and H(f). According to the figure, and because of formulae 7.40 and 7.44 and 
the fact that x(n) and b(n) are assumed to be uncorrelated hence Sxb(f) = 0), 
we get: 
Replacing these expressions in 9.32, leads us to: 
H*(f) 
G(f) = IH(f)12 + p(f) 
Sbb(f) 
where p(f) = Sxx (f) 
(9.44) 
Of course, the result is equivalent to the Zero Forcing filter 1/ H(f) for a 
noise equal to zero. 
As we have already said, the main problem with expression 9.44 is that the 
stable solution usually is not causal. This is why we will only be considering 
the case, which has important practical applications, where the solution to 
the problem is approximated by an FIR filter with a sufficient delay. We 
will continue to use the improper notation g(n) to refer to the filter's impulse 
response. 
The FIR filter g(n) can then be obtained from expression 9.33. This means 
we first have to determine the expressions of Ryy (k) and Rxy (k) as functions 
of Rxx(k), Rbb(k) and h(k). We get: 
E {y(n + k)y(n)} 
E {(v(n + k) + b(n + k))(v(n) + b(n))} 
Rvv(k) + Rbb(k) 
where v(n) refers to the filter's output and where we have used the fact that 
b(n) and v(n) are not correlated. According to formula 7.43 which gives the 
auto covariance function of a linear filter's output, we have: 
Rvv(k) = Rxx(k) * (h(k) * h*( -k)) 
Meaning that: 
Ryy(k) = Rxx(k) * h(k) * h*( -k) + Rbb(k) 
Likewise, we have: 
E {x(n + k)y(n)} = E {x(n + k)(v(n) + b(n))} 
E {x(n + k)v(n)} = Rxv(k) 

Chapter 9 - The Least Squares Method 379 
Formula 7.45 gives us the intercorrelation between the input and the output 
of a filter. Using it leads to: 
To sum up, the expressions of Ryy (k) and Rxy (k) as functions of h( k), 
Rxx(k) and Rbb(k) are: 
Rxx(k) * h(k) * h* (-k) + Rbb(k) 
h( -k) * Rxx(k) 
At this point, we can construct the matrix R and the vector r and use 
formula 9.34 to find the coefficients of the filter g( n). 
Let us assume, for example, that h(k) = ° for k tf. {O, ... , L - I}, that 
the signal x(n) is white and has the power a;, and that the noise is white, 
with the power a;. This means that Ryy(k) = a;h(k) * h*( -k) + a;o(k) and 
Rxy(k) = a;h( -k). We can then determine the expressions of R and r involved 
in 9.33 and 9.34: 
{ 
R=a;Ch+a;I 
r = a;h 
where C h is an N x N Toeplitz matrix constructed from the sequence h(n) * 
h*( -n) and where his: 
h T = [y 
,,-h(_O) ___ 
h_(L--.;,--_l )_0_._ 
.. -,,~ 1 
9.5 The LMS (least mean square) algorithm 
9.5.1 
The constant step algorithm 
We now come back to equation 9.33, which gives us the Wiener filter. In theory, 
if we perfectly knew the auto covariance matrix R and the intercovariance vector 
r , equation 9.33 would give us the solution to the problem in the form 9 = 
R-1r. The solution could then be obtained numerically, using the gradient 
algorithm seen previously and defined by the recursive equation 9.37 rewritten 
here: 
g(n) = g(n - 1) + fJ (r - Rg(n - 1)) 
(9.45) 
For many practical problems, the autocovariance matrix R and the covari-
ance vector r are not known, and have to be estimated from the data observed. 
Also, the stationarity hypotheses are never completely verified in the long run, 

380 Digital Signal and Image Processing using MATLAB® 
meaning a single estimation at the beginning of the process is not enough. 
These quantities have to be re-estimated regularly. The LMS algorithm re-
turns a simple adaptive solution to the problem. 
Let y(n) = [y(n) 
y(n - 1) 
y(n - P + l)V be the vector con-
structed from the last P observations, and let: 
e(n) 
= 
x(n) - x(n) 
x(n) - [y(n) 
y(n - 1) 
= x(n) - yT(n)g(n - 1) 
y(n-P+1)] 
gn-l(P-1) 
(9.46) 
be the difference between the value x(n) observed at the time n and the value 
x(n) calculated from the observation of y(n), with the use of the coefficients 
g(n - 1) obtained at the previous time. Be aware that e(n) is different from 
the difference found in expression 9.30 and involving the desired Wiener filter 
g. With these notations, we have at the (n - l)-th step: 
r - Rg(n - 1) 
lE {x(n)y(n)} -lE {y(n)y(nf} g(n - 1) 
lE {y(n)e(n)} 
If we replace the previous expression in 9.45, we get: 
g(n) = g(n - 1) + p,lE {y(n)e(n)} 
(9.47) 
An idea we owe to Widrow [38] is to replace lE {y(n)e(n)} in equation 9.47 
with its "instantaneous" value y(n)e(n), leading to the following two equations 
that make up the stochastic gradient algorithm, also called simply the LMB 
algorithm: 
Initial value: g(O) = 0, 
Repeat: 
I 
e(n) = x(n) - yT(n)g(n - 1) 
g(n) = g(n - 1) + p,y(n)e(n) 
(9.48) 
You can see in Figure 9.12 that the algorithm can be interpreted in a simple 
way: if e( n) is null, then we can reasonably consider that the coefficients have 
the right value, in which case they are left unchanged. Otherwise, the correction 
made to g(n - 1) by the term p,y(n)e(n) increases with e(n) . 
The idea of replacing the mathematical expectations with the instantaneous 
values is sometimes associated with other criteria, such as the least squares 

Chapter 9 - The Least Squares Method 381 
y(n) 
~A(n) 
~(n) 
~g(n) 
+ 
x(n) 
t -
Figure 9.12 - A reproduction of Figure 9.6 
criterion, and leads to uneven results. As we did with expression 9.35, we start 
with the expression J(g) of the criterion to minimize with respect to g, then 
we determine the expression of its gradient with respect to g , and finally, to 
obtain a minimum, we use the recursive equation: 
f.L 8J(g) I 
g(n) = g(n - 1) - - --
2 
8g 
g = g(n- l ) 
(9.49) 
If there are now unknown mathematical expectations in expression 9.49, we 
can replace these expectations with instantaneous quantities obtained by the 
plain and simple subtraction of the mathematical expectation, the same way 
we did with the LMS algorithm. This leads us to a stochastic gradient type 
algorithm. However, the results do not always meet expectations, whereas in 
the case of the quadratic criterion, the results are quite good, as we are going 
to see in some examples. 
But in any case, we have to ask ourselves the following questions: 
- are there initial values g(O) and values of f.L that ensure convergence (the 
exact definition will have to be given later on) since g(n) is random? 
- if there is convergence, does g( n) lead to the global minimum of J(g) or 
to some possible unwanted local minima? 
- in the case of the existence of several minima, is there a practical condition 
on the choice of the initial value that ensures convergence to the global 
minimum? 
These are difficult problems to solve because removing the mathematical 
expectation from the recursive equation makes the analysis very complex. A 
certain number of answers can be found in the literature, but they use hypothe-
ses that usually are not well verified in practice. An in-depth approach can be 
found, for example, in [17] , [23]. 
In practice, the following procedure is often used for setting the value of f.L: 
f.L is progressively increased until the algorithm diverges, then decreased by at 
least 10%. Once the value of f.L is set, the results are presented by displaying 
the evolution, in dB, of the square of the instantaneous square deviation p( n) = 
e2(n). However, because the shape of p(n) is often quite chaotic, it is a good 
idea to smooth p( n) by calculating the mean q( n) of N consecutive values. This 

382 Digital Signal and Image Processing using MATLAB® 
can be done by using the qn=filter(h,l,pn) function with h=ones(N,1)/N. 
The following expression can also be used: 
q(n) = (1- a)q(n - 1) + ap(n) 
where a is a forget factor that can be chosen equal to 0.1. The closer a gets 
to 0, the smoother the fluctuations of pen) become. 
Unlike in the deterministic situation, there appears, on average a misadjust-
ment at the convergence (see definition 9.39) causing oscillations around the 
minimum. What should be remembered is that the misadjustment decreases 
when f.L decreases and when the number of coefficients of 9 decreases. As f.L gets 
smaller, the rise time decreases and the tracking capability declines. Therefore 
we must find a compromise that takes these two requirements into account. 
The following examples as well as paragraph 9.5.3 deal with these behaviors. 
EXAMPLE 9.2 (Suppression of a single tone jammer) 
Consider a signal sen) corrupted by a single tone jammer. The observed sig-
nal can be written x(n) = sen) + ben) where ben) = Acos(27fibn + ¢) refers 
to the jammer, the frequency ib of which is assumed to be known, while its 
amplitude A and its phase ¢ are unknown. ben) can also be expressed as 
ben) = gc cos(27f ibn) + g8 sin(27f ibn) where gc and g8 are the unknown param-
eters we have to estimate. 
The problem of extracting ben) from the observed signal x(n) is identical 
in every way to the one we encountered in example 7.6 on suppressing a trend. 
The solution suggested here uses the LMS algorithm, hence it has the advantage 
of being adaptive: 
1. with the help of the diagram on Figure 9.13, prove that the problem is 
equivalent to finding a two input filter. 
+ 
x(n) -----------3~ 
Figure 9.13 - Two input Wiener filter 
Determine the Wiener solution of this filter that minimizes the least 

Chapter 9 - The Least Squares Method 383 
squares criterion: 
n 
Use this result to find the adaptation equations of the associated LMS 
algorithm; 
2. in order to test the tracking capability (adaptivity) of the LMS, we now 
assume that the jammer is amplitude modulated according to the expres-
sion: 
(9.50) 
where the amplitude A, the constant k and the frequency im are un-
known. We assume as our hypothesis that this signal can be approxi-
mated by a sequence of time translated sines of the type: 
p 
b( n) '2:: L[gc(k) cos(27r ib( n - k)) + g8 (k) sin(27r ib( n - k))] 
k= l 
where the sequences {gc(k)} and {g8(k)} are unknown quantities we are 
going to obtain by minimizing the square deviation between x( n) and 
b( n). Show that the problem is also equivalent to finding a two input 
filter. Give this filter's impulse responses. Write a program, using the 
LMS algorithm, that eliminates such a jammer for a speech signal; 
3. write a program that generates a ib = 1,000 Hz jammer modulated at 
im = 20 Hz according to expression 9.50. Let A = 1 and k = 0.5. After 
applying this jammer to a speech signal. Implement the LMS algorithm 
to try and eliminate it. Choose P empirically by listening to the result. 
HINTS: 
1. the signal observed is x(n) = s(n) + b(n) where the expression 
of the jammer b( n) is: 
(9.51 ) 
Equation 9.51 can be interpreted as a FIR filtering of two in-
put signals Yl(n) = cos(27ribn) and Y2(n) = sin(27ribn). The 
coefficients of the filter are gc and g8 respectively. If you re-
fer to Figure 9.12, you will see that the two-dimension signal 
(cos(27r ibn), sin(27r ibn)) represents the signal y( n) shown in 
the figure, and the (two-dimension) filter's two coefficients gc 

384 Digital Signal and Image Processing using MATLAB® 
and gs. As for the signal e( n), it represents the denoised sig-
nal. This is shown in Figure 9.13. We can then determine 
the two coefficients gc and gs that minimize the square de-
viation between b(n) and x(n). s(n) therefore behaves as the 
unwanted signal when estimating the two quantities. The LMS 
algorithm, which estimates gc and gs, is described by the two 
equations: 
{ 
e(n) = x(n) - [cos(21ffbn) sin(21ffbn )] [~ :~~:::: m 
[gc(n)] = [gc(n -1)] + e(n) [C~S(21ffbn)] 
gs(n) 
gs(n - 1) 
J-L 
sm(21ffbn) 
2. to take into account the modulation phenomenon, or other 
aspects of the jammer, let us assume that the jammer's ex-
pression is a sum of P delayed sines of the kind: 
p 
b(n) = L[gc(k) cos(21ffb(n - k)) + gs(k) sin(21ffb(n - k))] 
k = l 
This expression can be seen as the sum of the filtering of 
Yl(n) = cos(21ffbn) by the filter gc(n) and of the filtering of 
Y2(n) = cos(21ffbn) by the filter gs(n). If we choose to mini-
mize the least squares criterion J(g) , then the LMS algorithm 
is associated with: 
{ 
e(n) = x(n) - gT(n - 1) 
g(n) = g(n - 1) + J-Le(n) 
where: 
C(n) 
S(n) 
C(n) 
S(n) 
g(n) = [[gc(O) 
gc(P - 1)] [gs(O) 
gs(P - l)]r 
C(n) = [cos(21ffbn) 
cos(21ffb(n - P + 1))r 
S(n) = [sin(21ffbn) 
sin(21ffb(n - P + l))r 
The signal e(n) represents the denoised signal; 
3. load the previously recorded speech signal s and execute: 
%===== scrambexple.m 
load phrase 
N=length(sn); sn=sn/max(abs(sn)); 
%===== modulation (A=l for a pure jammer) 
Fb=1000; Fe=8000; fb=Fb/Fe; Fm=20; fm=Fm/Fe 

Chapter 9 - The Least Squares Method 385 
A=1; k=O.5; 
b=A*(1+k*cos(2*pi*fm*(1:N) ')) .* cos(2*pi*fb*(1:N)'); 
%===== jammed signal 
x=sn+b; 
%===== LMS implementation 
mu=O.01; P=30; gch=zeros(2*P,1); 
%===== schap is the reconstructed signal 
schap=zeros(N,1); 
for n=P:N 
end 
gY=[cos(2*pi*fb*(n:-1:n-P+1)'); ... 
sin(2*pi*fb*(n:-1:n-P+1)')]; 
enO=x(n)-gch'*gY; gch=gch+mu*enO*gY; 
schap(n)=enO; 
subplot(311); plot(sn); grid; subplot(312); plot(x); grid 
subplot(313); plot(schap); set(gca,'ylim' ,[-1 1]); grid 
EXAMPLE 9.3 (Linear prediction) 
Prove that the problem of the linear prediction coefficient calculation can be 
solved by an LMS algorithm. Apply this result to the estimation of an AR-
2 process's coefficients. Plot the evolution of the square deviation for several 
trials by choosing values of f.L that ensure "convergence" to a small and relatively 
constant value. 
HINTS: linear prediction consists of estimating the value at the 
time n based on the last P values, according to the expression: 
x(n) = gox(n - 1) + ... + gp-lx(n - P) 
The diagram in Figure 9.12 shows that the sequence x(n - 1), ... , 
x(n - P) acts as the observation y(n), whereas x(n) acts as the 
reference signal x( n) in the diagram. This means we have to replace 
the vector y(n) by x(n - 1) = [x(n - 1) 
x(n - P)t in the 
equations 9.48 of the LMS algorithm. We get: 
e(n) = x(n) -
[go(n - 1) 
go(n) 
go(n - 1) 
+ f.Le(n)x(n - 1) 

386 Digital Signal and Image Processing using MATLAB® 
Notice that the Wiener solution provided by the equation Rg = r , 
an equation where r is the vector with the following P components: 
r(k) = IE {x(n)x(n - k)} , k E (1, ... , P) 
and R is the matrix with the generating element: 
IE {x(n)x(n - k)} ,k E (0, ... ,P - 1) 
are simply the Yule-Walker equations 7.59. We saw that, for an AR 
process with its poles inside the unit circle, the linear prediction 
coefficients coincide with the model's coefficients. 
The estARlms . m program generates an AR-2 and estimates its co-
efficients using the LMS algorithm: 
%===== estARlms .M 
%===== generation of the AR-2 signal 
a=[1 -1.6 0 .8]' ; P=length(a)-1; 
N=4000 ; w=randn(N,1) ; 
% sigma2w=1 
x=filter(1,a,w) ; 
% Signal 
%===== estimation of the eigenvalues 
rO=x' * x/N; r1=x(2 :N)'*x(1:N-1)/N; 
r2=x(3 :N)'*x(1:N-2)/N; 
% or D2=[x' 0 ; 0 x ']; 
Rx=toeplitz([rO r1]); 
% and Rx=D2*D2'/N; 
lambda=eig(Rx); muMax=2/max(lambda) % Estimated max. 
%===== LMS implementation 
mu=0 .005; gest=zeros(P,1) ; err=[] ; 
for n=P+1 :N 
y=x(n-1 :-1:n-P); enO=x (n)-gest'*y; 
err=[err ; enO] ; gest=gest + mu*enO*y; 
end 
aest=[1;-gest]; 
%===== Yule-Walker equation 
aYule=[1 ;-inv (Rx) * [r1 ;r2]]; 
%===== displaying the results 
[a aest aYule], Jlim=std(err) - 2 
sigma2w_estime=rO + aYule(2)*r1 + aYule(3)*r2 
The program estimates the eigenvalues of the covariance matrix 
to find the maximum value of /-ld = 2/(maxi Ai) that ensures the 
convergence of the deterministic gradient algorithm. If we now 
compare the convergences of the stochastic gradient algorithm, we 
notice that /-ld is much greater than /-l. At the convergence, the 
estimation slightly oscillates around the solution to the equation 
Rg = r. As we have said, the fact that we eliminated the expec-
tations in the gradient algorithm introduces a misadjustment. The 

Chapter 9 - The Least Squares Method 387 
program gives the value of the instantaneous square deviation e(n) 
(variable err) as well as its variance denoted by Jlim in the pro-
gram. This value should be compared with the theoretical value 
J rnin , which in this case is equal to (J~ = 1. 
Exercise 9.1 (LMS algorithm: channel identification) (see p. 475) 
Consider the channel identification problem. A training signal x(n) is sent, 
producing the output v(n) = ho(n) * x(n) . Let us assume that the unknown 
filter ho (n) is an FIR filter with P coefficients. In the presence of noise, the 
observation y(n) has the expression y(n) = v(n) + b(n) where the noise b(n) is 
assumed to be white. 
In practice, when the length of the filter ho(n) is unknown, a large value 
can be chosen in order to be sure that it is overestimated. In this case, we 
will consider that this length is known, and we are going to determine a P 
coefficient filter such that fJ( n) = (h * x) (n) is the one that best looks like y( n) 
(see Figure 9.14). 
Figure 9.14 - Identification of an ho channel 
In mathematical terms, we have to find the h( n) that minimizes the square 
deviation between y(n) and: 
fJ(n) = h(O)x(n) + ... + h(P - l)x(n - P + 1) 
Minimizing the square deviation leads to the Wiener solution given by 
Rg = r, where r is the vector whose P components have the expressions 
lE {y(n)x(n - k)} for k from 0 to P - 1 and R is the matrix with the generat-
ing term lE {x(n)x(n - k)} for k from 0 to P - 1. 
We choose to use the LMS algorithm to estimate ho(n), rather than to solve 
this equation. Write a program that simultaneously performs: 
- input and output signal simulation: the input signal x(n) is a white noise 
with a power equal to 1. The output signal is y(n) = h(n) *x(n) + b(n). 
h( n) is the finite impulse response that simulates the channel. Let h= [1 
0.6 0.3] I. 
b(n) is a white noise. Determine its variance based on the value of the 
signal-to-noise ratio between b(n) and x(n), expressed in dB; 

388 Digital Signal and Image Processing using MATLAB® 
- implementation of the LMS algorithm to identify ho(n) ; 
- presentation of the results: display the value in dB of the estimation 
error's power. Because this sequence of values is very chaotic, smooth it 
by calculating a mean over a hundred values, using the filter function 
or a forget factor. 
Study the convergence speed and the misadjustement for various values of 
f-L. What happens if f-L is too high? Compare with the value f-L = 2/ maxi Ai . 
9.5.2 The normalized LMS algorithm 
As a reminder, here is the structure of the LMS algorithm in its standard form: 
g(n) = g(n - 1) + f-Ly(n) [x(n) - yT(n)g(n - 1)] 
This version of the algorithm is not well adapted to the observation signal's 
power variations. This is because, as we have said, f-L has to be chosen according 
to the inverse of the highest eigenvalues of R . Remember that the sum of these 
eigenvalues precisely represents the power of y(n). Hence, in the practical 
situation of signals that are not stationary and a power that varies, we have to 
adjust f-L as time goes by. If we do not, the algorithm might diverge. Hence the 
idea of taking as the step's expression: 
A 
f-L(n) = Py(n) 
The step is then said to be normalized. We still have to find a simple way 
of estimating Py (n) as time goes by. In order to do so, we can use one of the 
following simple expressions: 
- Py (n) = yT (n )y( n) / P where P is the length of the filter we wish to 
estimate, 
- or a recursive expression with a forget factor 0 < a < 1 of the type: 
This leads to what is called the normalized LMB algorithm: 
initial value: g(O) = 0, Repeat: 
e(n) = x(n) - yT(n)g(n - 1) 
Py(n) = (1- a)Py(n - 1) + ay; 
A 
f-L(n) = Py(n) + E 
g(n) = g(n - 1) + f-L(n)y(n)e(n) 
(9.52) 

Chapter 9 - The Least Squares Method 389 
The positive quantity E is only used to prevent f-L(n) from becoming too 
high, particularly in the case of a rather long drop in the power of y( n) . 
You can do exercise 9.1 over again replacing the standard LMS algorithm 
with the normalized one. You should focus on the cases of sharp power transi-
tions to compare the tracking capabilities of the two algorithms. The normal-
ized LMS often comes out with the better results. 
The following program compares how the standard and normalized LMS 
algorithms perform in an equalization program. This is done by generating a 
signal with a power that varies, and then to consecutively filter it with two 
different filters. The equalization is performed with a slightly too long FIR 
filter, since it has a length of P = 6 coefficients. The values of muS and muN are 
set by trial and error, choosing the a that ensured stability: 
%===== COMPLMS.M 
%===== signal generation 
clear; N=1000; xO=randn(l,N); xl= .2*randn(1,N); 
x=[xO' ;xl ' ;xO']; subplot(411) ; plot(x); grid 
set(gca, 'ylim' , [-6 6]) 
%===== filtering and addition of noise 
hO=[l 0 .7] ; vO=filter(hO , l,xO); 
hl=[l 0.3]; vl=filter(hl,l,xl); 
v=[vO' ;vl' ;vO']; N=length(v); 
SNR=30; Px=x'*x/N ; sigma=sqrt(Px)*10-(-SNR/20) ; 
b=sigma*randn(N,l) ; y=v+b; 
subplot(412); plot(y); grid 
set(gca, 'ylim' , [-6 6]) 
%===== equalization using a P coefficient FIR filter 
P=6 ; 
HestS=zeros(P,l); 
HestN=zeros(P,l); 
enS=zeros(N-P+l,l) ; 
enN=zeros(N-P+l,l); 
%===== implementing 
muS=0 .06 ; 
muN=O .08; 
%===== forget factor 
alpha=O.05; 
% standard LMS 
% normalized LMS 
% error for the standard LMS 
% error for the normalized LMS 
the LMSs 
% standard LMS step 
% normalized LMS step 
%==== pyn : initial value for Py(n) 
pyn=y(l:P-l)'*y(l :P-l)/P; 
%===== algorithms 
for n=P:N 
%===== standard 
enOS=x(n)-HestS'*y (n :-l :n-P+l); 
HestS=HestS+muS*enOS*y(n:-l:n-P+l); % update 
enS (n-P+l)=enOS; 
%===== normalized 
enON=x(n)-HestN'*y (n :-l :n-P+l) ; 
% two expressions for the estimation of Py 

390 Digital Signal and Image Processing using MATLAB® 
pyn=y(n:-l:n-P+l)'*y(n:-l:n-P+l)/P; 
% pyn=(l-alpha)*pyn+alpha*y(n)*y(n); % with forget factor 
HestN = HestN+muN*enON*y(n:-l:n-P+l)/pyn; % update 
enN(n-P+l)=enON; 
end 
%===== displaying the results 
en2N=enN .-2; en2S=enS .-2; 
moy=100; hmoy=ones(l,moy)/moy; 
en2moyS=filter(hmoy,1,en2S(1:N-P+l)); 
en2moyN=filter(hmoy,1,en2N(1:N-P+l)); 
endBS=10 * log10(en2moyS(moy:N-P+l)); 
endBN=10 * log10(en2moyN(moy:N-P+l)); 
subplot(212); plot(endBS); grid 
ylabel('P_e (dB)') 
hold on; plot (endBN, 'r'); hold off 
The results are shown in Figure 9.15. 
o square dev~ation 
(dB) , 
, 
, 
, 
, 
, 
, 
-10 
----------
~ ----------
~ 
, 
, 
, 
, 
, 
, 
-20 
-30 ----------f-------/ j 
normalized LMS 
: 
-40L-
~--~--~--~----~------~----~----~ 
o 
500 
1,000 
1,500 
2,000 
2,500 
3,000 
Figure 9.15 - Comparing the tracking capabilities of the standard LMS and the 
normalized LMS 
You can see that when there is a loss of stationarity, the standardized LMS 
algorithm has the better response. In the stationary parts, however, it behaves, 
in this example, basically in the same way as the standard LMS algorithm. You 
can try this with several values of the signal-to-noise ratio, or use a signal speech 
instead of the signal x. 
Comments 
The standard and normalized LMS algorithms are different from any other 
adaptive algorithm because of how easy they are to implement. The gradient 
step is the only parameter that has to be set. This setting often requires a 
number of trials to ensure both the convergence of the algorithm (small value 
of f-l) and a good enough tracking capability (large value of f-l). 
Performances greatly depend on the covariance matrix of the observation, 
and more particularly on how far apart the eigenvalues are from each other. The 

Chapter 9 - The Least Squares Method 391 
highest eigenvalue is related to the final error obtained at the convergence and 
the lowest eigenvalue is related to the total convergence time of the algorithm. 
Notice, finally, that one of the hypotheses assumed for the determination of 
the Wiener filter states that the channel is time-invariant. Yet, what is expected 
of the adaptive algorithm is not so much to reach a stationary solution that 
may not exist so much as to follow a system that varies slowly. In this case, 
the LMS is an acceptable solution. 
9.5.3 
Echo canceling 
Echo canceling is an important example of the practical use of the LMS algo-
rithm. 
In some situations such as with a "handsfree" device on a cellular phone, a 
speaker is located close to a microphone (Figure 9.16). The signal emitted by 
the speaker is transmitted along an acoustic path, dependent on where the user 
is, and reaches the microphone, creating an unwanted echo. In practice, a filter 
with an unknown impulse response, sometimes with hundreds of coefficients, 
can be used as a model to describe the acoustic path. 
s(n) I"~ ___ o---
~x--,--(n-,----) 
~ 
/ 
~ u(n) 
s(n) 
Channe~, 
y(n) 
Figure 9.16 - The echo cancelation principle 
Let y(n) be the speaker's output signal, u(n) the resulting echo in front of 
the microphone, s(n) the microphone input signal we wish to send, and x(n) 
the microphone output signal observed. We can write: 
x(n) = s(n) + u(n) = s(n) + h(n) *y(n) 
where h( n) is the impulse response that serves as the model for the acoustic 
path from the speaker's output to the microphone's input. The difficulty in 
echo cancellation is to estimate s(n) based on two observed sequences x(n) and 
y(n). This is done by assuming that: 
- the sequences s(n) and y(n) are centered and uncorrelated; 
- the estimation of s(n) is linear, of the type s(n) = x(n) - gTy(n) , where 
the filter g , the length of which is P, is determined by minimizing: 
K(g) = E {I s(n) - s(n) 12} 

392 Digital Signal and Image Processing using MATLAB® 
We are going to prove that, in this case, echo cancellation is equivalent to 
determining the Wiener filter according to the diagram in Figure 9.6. We have: 
K(g) 
I[ {Is(n) - s(nW} = I[ {(Is(n) - (x(n) - gTy(n)W} 
I[ {ls(n)1 2 } - 21[{s(n)x(n)}-
21[ {s(n)yT (n)} 9 + I[ {I x(n) _ gT y(n) 12} 
where the second term is null because s(n) and x(n) are uncorrelated and 
centered. Hence, the minimization of K(g) with respect to 9 is equivalent to 
that of: 
J(g) = I[ {I x(n) _ gT y(n) 12} 
which does not involve s(n). Therefore, the LMS algorithm can be used to 
estimate 9 based on the sequences x(n) and y(n) , then to substract the signal 
gT y(n) from x(n) to obtain an estimate of the signal s(n). 
Absence of vocal activity 
We are first going to consider the situation where there is no vocal activity, that 
is to say when s( n) = 0 and perform the following simulation using MATLAB®: 
the signal y( n) is a white noise and the signal that represents x( n) is created 
by filtering y( n) with the finite impulse response filter [1 
0.3 
-0.1 0.2]. 
Let y(n) be the filtered signal. The following program implements the LMS 
algorithm: 
%===== echocance11.m 
clear; N=4000; alpha=0.2; 
yn=randn(N,1); 
% reference 
hh=[1 0.3 -0.1 0.2]; xn=filter(hh,1,yn); % echo 
%===== LMS implementation 
mu=0.05; P=20; gn=zeros(P,1); 
en=zeros(N,1); 
for n=P:N 
end 
enO=xn(n)-gn'*yn(n:-1:n-P+1); 
gn=gn+mu*enO*yn(n:-1:n-P+1); 
en(n)=(1-alpha) *en(n-1)+alpha*abs (enO)-2; 
%===== displaying the results 
plot(10*log10(en(P+1:N»); 
grid; set(gca, 'xlim', [0 3000]) 
%===== 
%plot(20*log10(abs(fft(hest,1024»» 
Figure 9.17 shows the values in decibels of the error squared, integrated 
with a forget factor equal to 0.2, plotted against the number of steps of the 
algorithm. The values f.l = 0.02 and f.l = 0.05 lead to convergence. 

Chapter 9 - The Least Squares Method 393 
o £------------------------------------------, 
-50 
- 100 
-150 
-200 
- 250 
- 300 
- 350
L-----~------~----~------~----~----~ 
o 
500 
1,000 
1,500 
2,000 
2,500 
3,000 
Figure 9.17 - LMS algorithm: evolution of the square deviation, in dB, as a function 
of the number of steps of the algorithm, for the two values f-L = 0.02 and f-L = 0.05 of 
the step. The echo is a white noise filtered by the filter with the finite impulse response 
{I; 0.3; - 0.1; 0.2} 
The results are particularly good: because the signal s( n) is null, the output 
signal is also null, or at least should be if the calculations were completely 
accurate. Notice that if the signal y( n) is no longer stationary or if the acoustic 
transfer varies with time, it would be better to use the normalized step LMS 
algorithm. 
Presence of vocal activity 
In the situation where there is vocal activity in front of the microphone, the 
signal s(n) created by the user of the microphone behaves as a signal added to 
the signal x(n) . This makes it more difficult to adapt the algorithm, especially 
given the fact that s(n) is relatively powerful compared to the signal x(n) . The 
following program conducts trials with the signal s(n) as the speech signal. The 
echo signal u( n) is still a filtered white noise. In this case, the error signal, when 
the echo is perfectly cancelled, should be a rather accurate copy of the signal 
that entered the microphone. Figure 9.17 shows the results: the cancellation 
is satisfactory after about 200 samples: 
%===== echocancel2.m 
clear; itest=true; 
load phrase 
% signal (sn) 
sn=sn(:); N=length(sn); mm=max(abs(sn»; 
sn=sn/mm; 
% normalization 
yn=randn(N,1); % reference 
hh=[1 0.3 -0.1 0.2]; echo=filter(hh,1,yn); xn=sn+echo; 
subplot(311); plot(sn); grid; set(gca, 'xlim', [3500 6800]) 

394 Digital Signal and Image Processing using MATLAB® 
subplot(312); plot(xn); grid; set(gca, 'xlim', [3500 6800]) 
%===== implementing the LMS 
mu=0.01; P=20; gn=zeros(P,1); 
en=zeros(N,1); % denoised signal 
for n=P:N 
end 
enO=xn(n)-gn'*yn(n:-1:n-P+1); 
gn=gn+mu*enO*yn(n:-1:n-P+1); 
en(n)=enO; 
%===== displaying the results 
subplot(313); plot(en); grid 
set(gca, 'xlim' ,[35006800]) 
%===== audio tests 
if itest 
end 
soundsc(sn,8000) %=== original signal 
soundsc(xn,8000) %=== signal with echo 
sound(en,8000) 
%=== signal after echo cancelling 
3,500 
4,000 
4,500 
5,000 
5,500 
6,000 
6,500 
o 
,.!,,~\A.."'VfA
: 
: 
~II.~"~" 
1 ~ 
, 
, 
, 
'L ,l ~
' ~I 
' j 
=~ ----
~( - ~j~~r:?
--
-~~ -} ---------j ----u 
V_ - ~ - - - - ~ ----± --_1_ ~ - ~'~t ---
3,500 
4,000 
4,500 
5,000 
5,500 
6,000 
6,500 
Figure 9.18 - LMS algorithm: top graph: speech signal without the echo; middle 
graph: speech signal after adding the echo, which is a white noise filtered by the filter 
with the finite impulse response {I; 0.3; -0.1; 0.2}; bottom graph: processed signal 
The program can also be used to test the tracking capability by varying 
either the filter or the power of the signal y(n). 
Double talking 
Finally, we can test the LMS algorithm by choosing speech signals for s(n) and 
y(n). In this case, adapting the algorithm is more difficult, because the signal 
y( n) is no longer stationary and has spectral properties that are close to those 

Chapter 9 - The Least Squares Method 395 
of s(n). It is then better to use both the normalized LMS algorithm and a 
vocal activity detector in front of the microphone. When the latter detects a 
signal from the user, the adaptation of the algorithm is blocked, and the echo 
keeps on being cancelled with the coefficients used until then. After making a 
few settings, the algorithm provides results that are quite satisfactory. 


Part III 
Appendices 


Chapter 10 
Hints and Solutions 
HI 
Signal fundamentals 
H2 Discrete time signals and sampling 
H2.1 (An illustration of the sampling theorem) (see p. 78) 
1. Because Fs = 500 Hz is greater than twice the signal's frequency (that 
is, 2 x 200 Hz), the sampling makes it possible to perfectly reconstruct 
the signal. Hence we end up with the same sine at the 200 Hz frequency; 
2. because Fs = 250 Hz is smaller than twice the signal's frequency, the 
sampling introduces aliasing. The ±Fs shifts in the spectrum (corre-
sponding to n = ±1 in formula 2.6) contribute to the frequency with 
- 250 + 200 = 50 Hz. Since the spectrum is symmetrical, everything hap-
pens as if the 200 Hz frequency were "aliased" by symmetry about the 
frequency Fs /2 = 125 Hz. The result of the reconstruction is a sine with 
the frequency 50 Hz (Figure H2.1); 
3. type: 
%===== csamp12 .m 
Ds=.1; 
% signal length 
FO=200; 
% frequency of the sine function 
Fs=input(ISampling frequency in Hz (FO=200 Hz) = I); 
Ts=1/Fs; Ne=Ds/Ts+1; % number of samples 
K=40; 
% interpolation fonction for displaying 
Tc=Ts/K; Nc=Ds/Tc+1; % nb points of the "continuous" signal 
%===== 
tpc=[O:Nc-i] *Tc ; xtc=cos(2*pi*tpc*FO); % "continuous" signal 
tpe=[O:Ne-1]*Ts; xte=cos(2*pi*tpe*FO); % samples 

400 Digital Signal and Image Processing using MATLAB® 
l.5 
1 
0.5 
0 
- 0.5 
-1 
- 1.5 
subplot(211) ; plot(tpc,xtc, '- ' ,tpe,xte, '0'); 
%===== interpolation function 
ht=sin(pi*Fs*tpc) . / tpc /Fs / pi; ht(1)=1; 
Ni=200; 
% reconstruction filter 
hti=[ht(Ni:-1 :2) ht(1 :Ni)]; 
% (length 2*Ni-1) 
subplot(212); plot([-Ni+1:Ni-1]*Tc,hti) ; grid 
%===== reconstructed signal 
xtr=zeros(1,Nc); xtr(1:K:Nc)=xte; 
xti=filter(hti,1,[xtr zeros(1,Ni-1)]); 
Lxti=length(xti); 
xti=xti(Ni:Lxti); 
% delay of the filter 
subplot(211) ; hold on; plot(tpc ,xti, '-r') ; hold off 
grid; set(gca,'xLim', [.03 .06]); 
% zoom in 
Samples 
0.03 
0.035 
0.04 
0.045 
0.05 
0.055 
0.06 s 
0.5 
o 
Reconstruction : 
function 
-------~--------:--------~---
, 
, 
, 
, 
, 
, 
, 
, 
, 
, 
, 
, 
, 
, 
, 
, 
~
-
---- r 
----~-------t-------~--------
, 
, 
, 
, 
, 
, 
, 
, 
, 
, 
, 
, 
, 
, 
, 
, 
- 1--
---- ,
-
- 0.5 L-__ ~ ____ ~ ____ ~ __ ~~ 
__ ~ ____ ~ ____ ~ __ ~ 
- 0.02 -0.015 - 0.01 - 0.005 
o 
0.005 
0.01 
0.015 
0.02 s 
Figure H2.1 - Sampling and reconstruction 
H2.2 (Time domain hermitian symmetry) (see p. 86) 
1. If we take the conjugate complex of X (f) and use x( n) = x* (-n) : 
+00 
+00 
X *(f) 
L x*(n)e2j7rnf = L x( -n )e2j7rnf 
n = -oo 
n = -oo 
+00 
L x(n)e-2j7rnf = X(f) 
n = -oo 

Chapter 10 - Hints and Solutions 401 
If, furthermore, x( n) is real, then we know that X (J) = X * (- f), hence 
X(J) = X( - f) = X *(J) . The conclusion is that X(J) is real and even; 
2. the DTFT of {y(n)} has the expression Y(J) = I:~
~ x(n)e-2jTmj + 
x(0) j 2. If we take the conjugate and use the fact that x* (n) = x( -n) , 
then we have: 
+ 00 
+00 
Y*(J) 
L x* (n) e2j 7rnj + x* (0) j 2 = L x( _n)e2j 7rnj + x(0) j 2 
n = l 
n = l 
-00 
L x(k)e-2j7rkj + x(0) j 2 
k = - l 
Therefore, Y*(J) + Y(J) = 2Re{Y(J)} = X(J). This leads us to the 
following method for calculating the DTFT of a sequence x(n): 
(a) only the elements of x(n) with non-negative indices are considered 
(n 2: 0) , 
(b) the value x(O) is divided by 2, 
(c) the DTFT of the resulting sequence is calculated, 
(d) the DTFT of the real part is calculated, then multiplied by 2. 
H2.3 (Comparing computation speeds) (see p. 88) Type: 
%===== compare.m 
clear; x=randn(1,1024); P=100; 
tbcd=[); tbcf=[); 
% array for durations 
for k=7: 10, 
npts=2-k; 
% number of frequency points 
freq=[0:npts-1)/npts; n=[0:npts-1)'; 
y=x(1:npts); 
% same number of samples 
to = clock; 
% direct computation ===== 
for m=1:npts 
fr=freq(m); caldir(m)=y * exp(-2*pi*j*fr*n); 
end 
te=etime(clock,tO); tbcd=[tbcd te); 
to = clock; 
% computing with the FFT ===== 
for m=1:P 
% we repeat it to get 
calfft=fft(y,npts); % a significant duration 
end 
te=etime(clock,tO); tbcf=[tbcf te); 
end 
format long; [tbcd' tbcf'/P (tbcd' ./tbcf')*P); 
format short 

402 Digital Signal and Image Processing using MATLAB® 
H2.4 (Spectrum of the triangle function) (see p. 88) Type: 
%===== cspectri.m 
nfft=512; freq=[0:nfft-1]!nfft; 
sig=[1:10 9:-1:0]; 
figure(l), plot([0:19],sig,[0:19] ,sig, 'x') 
sigspec=fft(sig,nfft); 
figure (2) 
subplot(411); plot(freq,abs(sigspec)); grid; 
subplot(412); plot(freq,angle(sigspec)); grid; 
subplot(223); plot(freq,real(sigspec)); grid; 
subplot(224); plot(freq,imag(sigspec)); grid; 
We have to check, using Figure H2.2, that the DTFT obeys the hermitian 
symmetry property. Because of the periodicity with period 1, it means that 
the graphs are symmetrical about the frequency f = 1/2. The modulus and 
the real part are even, whereas the phase and the imaginary part are odd. The 
unwrap function can be used to plot the phase. 
ill 
! 
....---....
' 
0.8 
0.9 
5 i
PN 
-;~ 
o 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1 
100 
100 ,--::---;---------, 
Real 
Imaginary 
part 
50 
part 
50 ---------------
0 
A 
-,,-
0 
~~ 
~ 
-50 
------------ r -
-50 
-100 
0 
0.5 
0 
0.5 
Figure H2.2 - Spectral features of the triangle function. Above, the even modulus 
and the odd phase. Below, the even real part and the odd imaginary part 
H2.5 (Circular convolution of the rectangular signal) (see p. 89) 
The difference comes from the fact that, in the second case, the sequence x is 
padded with 8 zeros and the resulting convolution is linear. 

Chapter 10 - Hints and Solutions 403 
H2.6 (Delay) (see p. 89) 
1. Let us calculate the DTFT over L points, that is to say the DFT: 
n1 
L1 
Y(k/ L) 
L x(n)e- 2j'7mk/L + L 
x(n - L)e-2j7rnk/L 
n=O 
n=L-no 
n1 
-1 
L x(n)e-2j7rnk/L + L x(p)e-2j7r(p+L)k/L = X(k/ L) 
n=O 
p=- no 
where we have defined p = n - L and where we have used the property 
C
2j7rk = 1. To obtain the DTFT over L points of a signal that assumes 
non-zero values between the indices -no and n1, we have to shift by L the 
negative index values and calculate the DFT of the resulting sequence; 
2. type the program: 
H2.7 (FFTs of real sequences) (see p. 93) 
1. Let A(k) be the DFT of x(2n). Because x(2n) = (y(n) + y*(n))/2 and 
because the DFT of y* (n) is equal to y * (-k mod N), we have: 
A(k) = ~ (Y(k) + Y*( -k mod N)) 
(10.1) 
Likewise, if B(k) refers to the DFT of x(2n+ 1), and because x(2n+ 1) = 
(y(n) - y*(n))/2j, we have: 
B(k) = -% (Y(k) - Y*( -k mod N)) 
(10.2) 
The two relations 10.1 and 10.2 allow us to directly calculate A(k) and 
B(k) from Y(k).We will now see how X(k) is obtained from A(k) and 
B(k); 
2. because A(k) = L : ~ ~-l 
x(2n)WR;72 et B(k) = L : ~ ~-l 
x(2n + 1)WR;72' 
we get for k E {O, ... , N - 1}: 
X(k) 
C~' 
X(2n)w~nk) 
+ wi, C~' 
x(2n + l)W~,nk) 
A(k mod N/2) + W fVB(k mod N/2) 

404 Digital Signal and Image Processing using MATLAB® 
This last part can be written as follows: 
AD 
+ Bo 
AD 
+ Bo 
Al 
+ Wj.,Bl 
Al 
+ Wj.,B1 
A N/ 2 - 1 + 
N/2-1 
A N/ 2 - 1 + 
N/2-1 
WN 
B N/ 2- 1 
WN 
B N/2- 1 
=} 
AD 
Bo 
AD 
+ W!:/2 Bo 
Al 
Wj.,Bl 
A N/ 2- 1 + W N-1 B 
N 
N/2-1 
A N/2- 1 
N/2- 1 
WN 
B N/ 2- 1 
This tells us how to calculate the DFT of the real, length N sequence 
x( n) using the DFT of the complex, length N / 2 sequence y( n): 
(a) we define the sequence y(n) = x (2n) + j x (2n + 1) and calculate its 
N/2 order FFT Y(k), 
(b) we calculate A(k) and B(k) using relations 10.1 and 10.2 respec-
tively, 
(c) we calculate X(k) = A(k) + WJ/B(k); 
3. the previous algorithm is comprised of (N / 2) log2(N /2) operations for 
the computation of the complex, length N / 2 FFT, and also of N 
multiplication-additions for the computation of X(k) . As a consequence, 
the total computation is roughly N/2 log2(N/2) + N operations. This 
number should be compared with the computation load involved when 
using the length N algorithm, which is N log2(N) . For N = 1,024 we get 
5,632 in the first case, whereas we get 10,240 in the second; 
4. simulation program (Figure H2.3): 
%===== cfftreal .m 
N=64; mtime=[0:N-1]; fO=.23; freq=[0:N-1]/N; 
x=sin(2*pi*fO*mtime); x=[x zeros(1,rem(N,2»]; 
Nx=size(x,2); xspec=fft(x); xspeca=abs(xspec); 
%===== approximating the DTFT 
atftd=fft(x,1024); 
plot([0:1023]/1024,abs(atftd), 'r') ; grid; hold on 
%===== result of the direct calculation 
plot(freq,xspeca); 
set(gca, 'xlim', [0 .5], 'ylim' ,[0 max(abs(atftd»]) 
x2n=x(1:2:Nx-1); x2np1=x(2:2:Nx); Ny=Nx/2; 
y = x2n + j*x2np1; yspec=fft(y); % 
inds=[1 Ny :-1:2]; 
% conjugation 
yspecs(1,:)=conj(yspec(inds»; 
% in time 

Chapter 10 - Hints and Solutions 405 
Ak = (yspec+yspecs)/2; Bk = j*(yspecs-yspec)/2; 
Wn=exp(-2*j*pi*[O:Nx/2-1]/Nx); Wn=[Wn -Wn]; 
yk = [Ak Ak] + [Bk Bk] .* Wn; 
%===== result 
plot(freq,abs(yk), 'or') ; hold off 
30 
------
: ------~------
~ ------:---
~~
-~------~------~-----~------+------
I 
I 
I 
I 
,I 
I 
, 
I 
I 
I 
: 
: DTFr 
:
:
: 
: 
: 
: 
: 
25 ------:------ ~ ------ ~ --~
~ - ~ ------ ~ ------:- ----- ~- ----- ~ ------
I 
I 
• 
I 
I 
I 
, 
I 
I 
I 
• 
I 
I 
I 
I 
I 
I 
" 
I 
I 
I 
20 ----- - 1-
-
-
-
-
-
~ -
-
-
-
-
-
... -
-
-
-
-
-1- -
- , 
-
~ -
-
-
-
-
-
~ -
-
-
-
- -:- -
-
-
-
- ~-
-
-
-
-
-
~ -
-
-
- --
I 
I 
I 
' 
I 
I 
I 
I 
I 
I 
I 
I 
'
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
15 -----_:_Result of the -----: 
-: ~ ------ ~ ------:-------:------+-
: fast computation 
: 
:::: 
10 
------------
~ 
-----:--
------
~ 
5 ------------ , 
Figure H2.3 - Comparing the results 
H2.8 (Using the FFT) (see p. 93) 
The program draws a unit circle, since the calculation of the FFT of [0 1] leads 
to the values: 
X(k) = e-27rjk / 128 , with k = 0 ... 127 
H3 Spectral observation 
H3.1 (Study of the resolution) (see p. 101) 
1. The expression of X (f) is: 
2. let us assume that fo » liN, h » l i N and Ifo - h i » liN. Because 
gN(f) is a quickly decreasing function for If I » liN, the four terms 
involved in the expression of X (f) are never simultaneously null. Hence, 
for f belonging to a neighborhood of fo: 
IX(f)1 shows a maximum in fo, the amplitude of which is roughly Nao; 

406 Digital Signal and Image Processing using MATLAB® 
3. type the program: 
%===== cresol.m 
N=32; 
% signal length 
L=1024; 
% number of frequency points 
fO = 0.2; 
mtime=(O:N-1)' ; 
freq= (0: L-1) /L; 
% column vector for time 
phi=input('Relative phase between the two sines (degrees): '); 
phi=phi*pi/180; 
adb= input('Amplitude ratio (dB): '); 
a = 10 -(adb/20); 
deltaf=(1/N:1/(5*N):3/N) ; % frequency deviations 
nbdf=length(deltaf); f1=fO+deltaf; 
x1=cos(2*pi*mtime*fO); x1f=20*log10(abs(fft(x1,L))); 
%===== for each f1 
for k=1:nbdf 
end 
x2=a * cos(2*pi*mtime*f1(k)+phi); 
x2f=20*log10(abs(fft(x2,L))); 
subplot(211); plot(freq, [x1f x2f]); grid 
axis([O 0.5 -20 40]); 
x=x1+x2; xf=20*log10(abs(fft(x,L))); 
subplot(212); plot(freq,xf); grid; 
axis([O 0.5 -20 40]); 
title(sprintf('delta_f = %1.2g x 1/N' ,deltaf(k)*N)) 
disp('Press a key to proceed. ') 
pause 
As you can see, the resolution is highly dependent on the relative phase 
between the two sines. 
H3.2 (Effect of the Hamming windowing) (see p. 102) 
1. Calculation of the normalization coefficient Ch: let Xh(f) be the spectrum 
of the windowed signal. We have: 
N-l 
Xh(f) = A L ChWh(n) exp(2j7rfon) exp(-2j7rfn) 
n=O 
At point fo, we have Xh(fo) = A L,
~:01 
chwh(n), a quantity we wish to 
have equal to A. Therefore, Ch = 1/ L,
~:01 
wh(n); 
2. type: 
II %===== ceffham.m 

Chapter 10 - Hints and Solutions 407 
N=32; L=1024; freq=(O:L-l)/L; 
s=exp(2*i*pi*0.2*(0:N-l)); 
whamm = 0.54 - 0.46 * cos(2*pi*(0:N-l)/N); 
cr = 1 / N; ch = 1 / sum(whamm); 
%===== windowing (rectangle or hamming windows) 
sr=cr * s; sh = ch * s .* whamm; 
srf=fft(sr,L); shf = fft(sh,L); 
srfdb=20 * log10(abs(srf)); shfdb=20 * log10(abs(shf)); 
subplot(211); plot(freq,shfdb); grid 
[xyl xy2]=ginput(2); 
sprintf('Ratio=%d dB' ,abs(xyl(2)-xy2(2))) 
subplot(212); plot(freq,srfdb); grid 
[xyl xy2]=ginput(2); 
sprintf('Ratio=%d dB' ,abs(xyl(2)-xy2(2))) 
3. Figure H3.1 shows the DTFT of the Hamming window. The width of the 
main lobe is measured, as well as the attenuation between the first side 
lobe and the main lobe (look up how to use the ginput function): 
window 
width 
attenuation 
rectangular 
2/N 
-13 dB 
Hamming 
4/N 
- 40 dB 
OdB,---~~~--~--~--~--~----~--~--~--, 
o 
0.1 
02 
03 
OA 
05 
Oh 
O~ 
O~ 
09 
OdB.---~--~~~----~--~--~--~--~----~--, 
I 
I 
I 
I 
I 
I 
I 
---,------,-------,------- r ------ r- -----T------,------
I 
I 
I 
I 
I 
I 
I 
I 
I 
-25 dB 
------T---
--1------1------~-------,-------
r
------
r
------i------
1------
------f--
---~---
--~------~-------:-------
~ ------
~ ------f------~------
Figure H3.1 - Comparing the rectangular window (above) and the Hamming window 
(below) 
4. in the case where the two sines have the same amplitudes, the resolution 
is directly related to the width of the main lobe, because if the two sines 
are distant enough from each other, then main lobes will not be too close, 
and they can easily be distinguish (see exercise 3.1). 
For N = 32 and Fs = 1,000 Hz, the resolutions obtained for 1.5 times 
the width of the main lobe are: 

408 Digital Signal and Image Processing using MATLAB® 
window 
width 
resolution(Hz) 
rectangular 
l /N 
1,500/32 ~ 47 Hz 
Hamming 
2/N 
3,000/32 ~ 94 Hz 
5. in the case where the two sines have different amplitudes, the resolution 
is related to the width of the main lobe, but also to the height of the side 
lobe. For example, if we wish to distinguish a possible ratio of 25 dB, 
we have to go beyond the 6-th side lobe in the case of the rectangular 
window, whereas we only need to go beyond the first main lobe in the case 
of the Hamming window (see Figure H3.1). In this case, the Hamming 
window therefore allows a resolution of l.5 x 2,000/32 ~ 94 Hz, which 
is better than the one obtained with a rectangular window, which is 
6 x 1,000/32 ~ 188 Hz. 
H3.3 (Short term Fourier transform) (see p. 107) 
l. The analysis function is as follows: 
function [spec,normtm]=stft(xt,Lb,ovlp,Lfft,win) 
%!===================================================! 
%! Short Term Fourier Transform 
%! SYNOPSIS: [spec ,normtm]=STFT(xt ,Lb,ovlp,Lfft ,win) 
%! 
xt 
signal 
%! 
Lb 
block size 
%! 
ovlp 
overlap length 
%! 
Lfft 
FFT length 
%! 
win 
window type 
%! 
spec 
spectrogram 
%! 
normtm 
time vector (normalized) 
! 
%!===================================================! 
if nargin<5, win='rect'; end 
if nargin<4, Lfft=128; end 
xt=xt(:); Nx=length(xt); 
switch lower(win) 
end 
case 'hamm' , 
wn=.54- .46*cos(2*pi*[O :Lb-l] 'iLb) ; 
case 'hann' 
wn= .5-.5*cos(2*pi*[O:Lb-l] 'iLb); 
case 'rect' 
wn=ones (Lb, 1) ; 
blkS=(Lb-ovlp); nbwin=floor(NxiblkS); 
spec=zeros(Lfft,nbwin-l); 
%===== calculating the spec matrix 
for k=l:nbwin-l 
bblk=(k-l)*ovlp+l; 
xxw=xt(bblk:bblk+Lb-l) .* wn; 

Chapter 10 - Hints and Solutions 409 
II 
spec( : , k) =fft (xxw , Lfft) ; 
end 
normtm=[0:nbwin-2]*blkS; 
2. the following program returns the spectrogram for a chosen block size: 
%===== cstft.m 
% 
uses genel.m or gene2.m 
% 
overlap = 50% 
Tt=length(xt); Lfft=128; frq=Fs * (O:Lfft-l)/Lfft; 
frqs2=frq(Lfft/2:-l:l); 
disp(sprintf('Number of samples: %.Of' ,Tt)); 
tbl=input('Block size (0: return)='); 
while (tbl -= 0) 
end 
[spec,tps]=stft(xt,tbl,floor(tbl/2)); 
mtime=tps/Fs; 
xreshf=abs(spec); xreshf=flipud(xreshf(1:Lfft/2,:)); 
contour(mtime,frqs2,xreshf); grid; %or imagesc(xreshf) 
tbl=input('Block size (0: return)='); 
H3.4 (Visualizing the aliasing with the STFT) (see p. 108) 
%===== modulfreq2 .m 
lambda=2000; Fs=8000; FO=1000; T=2; 
nfft=128; Lbloc=100; freq=[0:nfft/2-l]*Fs/nfft; 
%===== signal 
it=(O:Fs*T-l)/Fs; 
theta=2*pi*FO*it+pi*lambda*(it .-2); 
x=cos(theta'); Lx=length(x); 
nblocs=floor(Lx/Lbloc); x=x(l:nblocs*Lbloc); 
x=reshape(x,Lbloc,nblocs); 
%===== windowing 
w=.54-.46*cos(2*pi*[0:Lbloc-l] '/(Lbloc-l)); 
w=w * ones(l,nblocs); 
B=x.*w; A=abs(fft(B,nfft)); 
%===== displaying between 0 and Fs/2 
mesh([l:nblocs] ,freq,A(1:nfft/2,:)) 
view ( [-20 40]) 
H3.5 (Effects of sampling and windowing) (see p. 109) 
1. The signal x(t) has the following Fourier transform: 

410 Digital Signal and Image Processing using MATLAB® 
Figure H3.2 - Time-frequency representation 
Notice that IX(FW goes from the value t6 in F = 0 to the value t6/2 
in Fc = 1/(27fto). The frequency Fc which corresponds to a ratio of 2, 
hence 10log1o (2) = 3 dB, is called the 3 dB cut-off frequency; 
2. Xs(f) is obtained by periodizing Fs then normalizing the frequency scale 
by dividing by Fs. The fact that X(F) has an infinite band causes alias-
ing. However this aliasing decreases as Fs » Fc; 
3. this is equivalent to mutiplying the signal xs(n) by a width M rectangular 
window, hence to convolute Xs(f) with the function given by expression 
3.1. This results in oscillations with pseudo-period 1/ M; 
4. type: 
%===== ceffsamp.m 
clear; clf 
%===== continuous time 
to=1/0.7; Fa=100; M=10; 
%===== discrete time 
Fs=2; Ts=l/Fs; Ntrq=5; 
%===== continuous time signal 
tpsa=(O:M*Fa-l)/Fa; xa=exp(-tpsa/tO); 
subplot(221); plot(tpsa,xa); set(gca, 'xgrid', 'on') 
%===== FT 
frqsa=(-100:0.1:100)/Fa; 
XFa= abs(tO .1 (1+2*j*pi*tO*frqsa)); 

Chapter 10 - Hints and Solutions 
411 
subplot(222); 
plot (frqsa,XFa, '-' ,frqsa+Fs,XFa, 'g:' ,frqsa-Fs,XFa, 'g: '); 
%===== discrete time signal 
xe=xa(l:Ts*Fa:M*Fa); tpse=tpsa(l:Ts*Fa:M*Fa); 
subplot(221); hold on; stem(tpse,xe); hold off 
%===== DTFT 
Lfft=1024; frqstrq=Fs*(0:Lfft-1)/Lfft-Fs/2; 
XFs=abs(fft(xe,Lfft))/Fs; XFs=fftshift(XFs); 
subplot(222); hold on; 
plot (frqstrq,XFs, 'r' ,frqstrq+Fs,XFs, 'r' ,frqstrq-Fs,XFs, 'r'); 
plot ([Fs Fs], [1.2 0],' : ' ); plot ([ -Fs -Fs] , [1.2 0],' : ' ) 
set(gca, 'xlim', [-3 3]); hold off; 
%===== truncated signal 
xatrq=xe(l:Ntrq); tpstrq=tpse(l:Ntrq); 
subplot(223); stem(tpstrq,xatrq); 
set(gca, 'xlim', [0 tpsa(M*Fa)], 'xgrid' ,'on') 
%===== DTFT 
frqstrq=Fs*(0:Lfft-1)/Lfft-Fs/2; 
XFtrq=abs(fft(xatrq,Lfft))/Fs; XFtrq=fftshift(XFtrq); 
subplot(224); plot(frqstrq,XFtrq,'r' , ... 
frqstrq+Fs,XFtrq, 'r' ,frqstrq-Fs,XFtrq, 'r'); 
set(gca, 'xlim', [-3 3]); 
%==== TFD 
LfftTFD=8; frqstrqTFD=Fs*(0:LfftTFD-1)/LfftTFD-Fs/2; 
XFtrqTFD=abs(fft(xatrq,LfftTFD))/Fs; 
XFtrqTFD=fftshift(XFtrqTFD); 
hold on, stem(frqstrqTFD,XFtrqTFD); 
plot ([Fs Fs], [1.2 0],' : ' , [-Fs -Fs] , [1.2 0],':'), hold off 
H3.6 (Amplitude modulation) (see p. 110) 
1. We have x(t) = cos(27fFat) + km(t) cos(27fFat). If we replace cos(27fFat) 
with [exp(2j7fFat) +exp( -2j7fFat)]/2 and take the Fourier transform, we 
get: 
2X(F) 
J(F - Fa) + J(F + Fa) + kM(F) * (J(F - Fa) + J(F + Fa)) 
J(F - Fa) + J(F + Fa) + kM(F - Fa) + kM(F + Fa) 
In any case, the spectrum of x(t) contains two peaks at the frequencies 
±Fa, as well as the spectrum of m(t) shifted by ±Fa. If the width of 
m(t) is B, meaning that its spectrum is non-zero between -B and +B, 
then the spectrum of x(t) occupies a 2B band around Fa. Because the 
spectrum of m(t) obeys hermitian symmetry (real signal) , the spectrum 
of X (F) has the same property around Fa. Hence we can restrict the 
representation of X(F) to the frequencies beyond Fa; 

412 Digital Signal and Image Processing using MATLAB® 
2. the spectrum is comprised of seven peaks in the positive frequencies (the 
negative frequencies are obtained using hermitian symmetry): 
- 50 kHz (carrier); 
- 47,870 Hz and 52,130 Hz originating from the component at 2,130 
Hz; 
- 46,250 Hz and 53,750 Hz originating from the component at 3,750 
Hz; 
- 45,040 Hz and 54,960 Hz originating from the component at 4,960 
Hz. 
3. the program cmodam.m allows you to obtain Figure H3.3 for m(t) and 
x(t) . 
2 .---~--~----~--~--~----~--~--~----~--. 
o L-__ ~ __ ~ ____ ~ __ ~ __ ~ ____ ~ __ ~ __ ~ ____ ~~ 
o 
0.2 
0.4 
0.6 
0.8 
l.2 
1.4 
l.6 
l.8 
2 
2 .---~--~----~--~--~----~--~--~----~--. 
o 
-1 
-2 ~ __ ~ __ ~ ____ ~ __ ~ ____ ~ __ ~ ____ ~ __ ~ ____ L-__ 
~ 
Figure H3.3 - Amplitude Modulation. Above: signal m( t). Below: modulated signal 
The absence of overmodulation is characterized by the fact that (1 + 
km(t)) never becomes negative. Notice that (1 + km(t)) is therefore the 
the upper envelope of x(t). This is an essential practical result, as it 
allows us to perform the demodulation operation in a very simple way: a 
full-wave rectifier is used, followed by an RC filter in order to detect the 
envelope. If B « 1/ RC « Fo, the output signal will follow the envelope. 
The development of radio communications was based on this very simple 
technique; 
4. here, the two closest peaks are 1,000 Hz apart, or in normalized frequen-
cies, 1,000/500,000 apart. Hence, in order to distinguish them using the 
DTFT, we need a number of points much greater than 500. To be able to 
have an outright separation we will choose N = 1,000, which corresponds 
to 2 ms of signal; 

Chapter 10 - Hints and Solutions 413 
5. in normalized frequencies, 100 Hz correspond to 1/5,000. Hence we need 
an FFT size greater than 5,000; 
6. type the following program: 
%===== cmodam.m 
Fs=500000; durat=2/1000; N=Fs*durat; td=(O:N-l); t=td/Fs; 
FO=50000; fOr=FO/Fs; 
% mod. frequency 
Fm=[2130;3750;4960]; fmr=Fm/Fs; Am=[1,1.8,0.9]; 
k=1/2; ac=l+k*Am * cos(2*pi*fmr*td); 
xt=ac .* cos(2*pi*fOr*td); 
subplot(311); plot(t,ac); grid 
subplot(312); plot(t,xt); grid 
L=8192; freq=[O:L-l]/L*Fs; % real frequency 
subplot(313); plot(freq,abs(fft(xt,L))); grid 
set(gca, 'xLim', [40000 60000]) 
Figure H3.4 shows the spectrum obtained in agreement with the theoret-
ical spectrum. ~ 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
-----1------1----
, -----
1-
-----
----1----
-------------
--
---
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
o 
0.2 
0.4 
0.6 
0.8 
1 
1.2 
1.4 
1.6 
1.8 
2 s 
~
"
'.
"
. 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
, 
, 
, 
I
i
, 
I 
I 
I 
I 
: 
i 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
" 
, 
o 
0.2 
0.4 
0.6 
0,8 
1.2 
1.4 
1.6 
1.8 
2 s 
F :F Fo+F2 
: 
. o+~'
·· t
··· ~ T io~~
: ······ 
40,000 
60,000 Hz 
Figure H3.4 - Spectrum of a double side band modulation 
H3.7 (Carrierless double side-band) (see p. 110) 
1. If M(F) refers to the Fourier transform of m(t), then that of the mod-
ulated signal x(t) = m(t) cos(27rFot) is X(F) = (M(F + Fo) + M(F -
Fo)) /2. The spectrum of x( t) is comprised, around Fo, of the spectrum 
M(F - Fo)/2, which has a width of 2B; 

414 Digital Signal and Image Processing using MATLAB® 
2. if we multiply the signal x(t) by 2cos(27rFat + ¢), we get y(t) = 
2m(t) cos(27rFat) cos(27rFat + ¢) which can also be written y(t) = 
m(t) cos( ¢) +m(t) cos( 47r Fat + ¢). The signal y(t) therefore has a low fre-
quency component m(t), multiplied by cos(¢), and a (modulation type) 
high frequency around 2Fa with a width 2B. If we then use a low-pass 
filter, such as the one shown in Figure H3.5, the signal m(t) cos(¢) is 
reconstructed. 
It is important to have ¢ = 0, because if ¢ =1= 0, the useful signal is 
attenuated. In the presence of noise, a mere amplification is not sufficient 
to compensate this attenuation. 
3. type: 
x(t) ~~I 
~ ~ m(t) 
cos(27r Fot) i :ocal oscillator 
Figure H3.5 - Synchronous demodulator 
%===== cmodcdsb.m 
Fs=500000; durat=2/1000; N=Fs*durat; td=(0:N-1); t=td/Fs; 
FO=50000; fOr=FO/Fs; 
% mod. frequency 
Fm=[2130 ; 3750 ; 4960]; fmr=Fm/Fs; Am=[1 1.8 0.9]; 
mt=Am * cos(2*pi*fmr*td); xt=mt .* cos(2*pi*fOr*td); 
subplot(311); plot(t,mt); grid 
subplot(312); plot(t,xt); grid 
L=8192; freq=[O:L-1]/L*Fs; % real frequency 
subplot(313); plot(freq,abs(fft(xt,L))); grid 
set(gca, 'xLim', [40000 60000]) 
H3.8 (Stereophonic signal) (see p. 111) 
1. Spectrum of c(t) (Figure H3.6): 
C(F) 
1 
(G(F) + D(F)) + 2(G(F + Fa) + D(F + Fa)) 
1 
+2(G(F - Fa) + D(F - Fa)) 
P 
Fa 
P 
Fa 
+- 5(F + -) + - 5(F - -) 
222 
2 

Chapter 10 - Hints and Solutions 415 
19 
---f--+-+--'t--'--+--"''"i4--t-'--~ 
F (KHz) 
Figure H3.6 - Spectrum of the stereophonic signal used for FM radio broadcasting 
2. for a monophonic set, all we need to do is filter the signal c(t) in the 
(-15,+15) kHz band to reconstruct the signal g(t) +d(t). This is what 
determined the choice of the composite signal's shape. People who owned 
a monophonic set had to be able to still listen to it without having to 
buy a new set; 
3. type the program: 
%===== estereO.ffi 
fa=1000000; fO=38000/fa; 
fl=[380 957 1164 1587 1953] '/fa; 
Al=[0.7 1.5 1.9 2.8 3.7]; 
fr=[347 523 1367 2465 3888] '/fa; 
Ar=[0.3 1.5 2.7 1.7 2.3]; 
T=1000; t=(O:T-l); 
%===== left and right signals 
g=Al*eos(2*pi*fr*t); d=Ar*sin(2*pi*fl*t); 
e=(g+d)+(g-d) . * eos(2*pi*fO*t); 
plot(t' ,[e' 2*g' 2*d']); grid 
We need to sample the signal c(t) at the frequency of 76 kHz. The odd 
times correspond to the left signal, and the even times to the right signal 
(Figure H3.7). Obviously, a slight delay causes crosstalk, meaning that 
a small part of the right signal is mixed up with the left signal, and vice 
versa. 
H4 Linear filters 
H4.1 (Rectangular impulse response filter) (see page 133) 
1. This filter calculates the mean of the last M values of the input signal. 
This operation smooths the signal and eliminates the rapid fluctuations 
corresponding to the high frequencies. This is a low-pass filter with a 
cut-off frequency dependent on the value of M ; 

416 Digital Signal and Image Processing using MATLAB® 
20 
15 
10 
5 
o 
-5 
-10 
-15 L-__ ~ __ ~ __ ~ __ ~ __ ~ ____ ~ __ ~ __ ~ __ ~ __ ~ 
o 
100 
200 
300 
400 
500 
600 
700 
800 
900 
1,000 
Figure H3.7 - Composite stereophonic signal c( t): the upper and lower envelopes 
represent the left and right signals 
2. the complex gain is: 
H(J) = e- j (M-l)7r!sin(M7rf) 
sin(7r f) 
and the phase is piecewise linear with the slope -(M - 1)7r: 
cp(J) = -(M - 1)7rf + e(J)7r 
where e(J) is equal ±1 depending on whether s i~~1!f
{) 
is positive or 
negative; 
3. these results are gathered in Figure H4.1 where the phase was represented 
using the angle function which brings it back between -7r and 7r; 
::~ 
o 
05 
1 
0.08 : ~ ~~~~~~~ 
_ : t 
~r _e_s_~~~~~
_ : : : : 
0.04 
------ 1 -----
,
-
o ------~------
i
------
~
------
o 
5 
10 
15 
20 
2 ~hase 
--~I\-:f\~ 
o --
.---
~ - t\
- t\+
- kr~
~~ 
-2 ;\f~?{~[~?
:::::---
o 
0.5 
1 ,------,--------:-7'>-¥*'''*l<E-¥***'*o 
0.8 
0.4 
----- ~ ---- step response 
00 
5 
10 
15 
20 
Figure H4.1 - Gain, phase, impulse response and step response 
Figure H4.1 was obtained using the program: 

Chapter 10 - Hints and Solutions 417 
%===== rectfilter.m 
T=20; mtime=(O:T-1); Lfft=1024; fq=(O:Lfft-1)/Lfft; 
M=10; h=ones(1,M)/M; 
Hf=fft(h,Lfft); 
% frequency response 
Gf=abs(Hf); Phif=angle(Hf); 
d=eye(1,T); yd=filter(h,1,d); 
% impulse response 
u=ones(1,T); yu=filter(h,1,u); % step response 
subplot(221); plot(fq,Gf); grid 
subplot(222); plot(fq,Phif); grid 
subplot(223); plot(mtime,yd, '-' ,mtime,yd, 'x'); grid 
subplot(224); plot(mtime,yu, '-' ,mtime,yu, 'x'); grid 
4. the filter resulting from cascading the two previous filters has as its im-
pulse response the triangle function (h * h) (n). The corresponding gain 
is the square H2 (f) of the previous gain. 
H4.2 (Purely recursive first order) (see page 135) 
1. Because the filter is causal, the convergence area is of the type Izl > lal ; 
2. if we perform the series expansion of Hz(z), and according to the defini-
tion of the z-transform, we have: 
H z(z) = 1 + az- + a z- + ... = 
a z-
=} 
1 
2 
2 
L 
k 
k 
{h(n) = an for n ~ 0 
o otherwise 
kEN 
The BIBO stability condition is satisfied since: 
L Ih(k)1 = L lal
k < 00 =} lal < 1 
kEZ 
kEN 
3. the complex gain, the gain and the phase are: 
Hz(e27rj/) = H(f) = 
1 2/ ' gain = IH(f)I , phase = arg(H(f)) 
1 - ae- 7rJ 
4. we saw that the impulse response of the filter had the expression h(n) = 
)..an for n ~ 0 and 0 otherwise. Therefore, the index response can be 
written: 
n 
1 _ an+1 
y(n) = ).. "'"' ak = )..---
L 
I-a 
k=O 
If we choose).. = 1 - a, then y(n) = 1 - an+1 tends to 1 when n tends to 
infinity, with a decreasing speed as lal get closer to 1; 

418 Digital Signal and Image Processing using MATLAB® 
5. the plots H4.2 were obtained using the following program: 
%===== steprespAR1.m 
N=30; mtime=(0:N-1); a=[-2/3 1/2 3/4 7/8]; 
Na=length(a); mstep=ones(N,1); y=zeros(N,Na); 
for ii=1:Na 
y(: ,ii)=filter(1-a(ii) ,[1 -a(ii)],mstep); 
end 
plot (mt ime ,y, '-' ,mt ime , y , ' 0 ' ) ; 
set(gca, 'xlim', [0 N-1]); set(gca,'ylim' ,[0 1.8]); grid 
As you can see, as lal gets closer to 1, the output signal slowly converges to 
its limit value 1. This "rise time" can be evaluated from the index beyond 
which the difference with the value 1 is considered to be negligible. To 
be more precise, we can write that if 0 < a < 1, y(n) = l_e(n+l)Jog(a) = 
1- e-(n+l)/T where T = 1/log(l/a) > O. 
If -1 < a < 0, y(n) = 1 - (_I)n+lc(n+l)/T where T = 1/ log(I/lal) > O. 
Therefore, whether a E (-1, 1) is positive or negative, the index response 
has an "exponential" shape, the time constant of which is given by T = 
1/ log(I/lal). The closer lal gets to 1, the larger T becomes. 
1.6 
--------- , --------- r 
--------, -------------------,--
1.4 
---------" ----------" ---------, ----------,----------, ----------
, 
, 
, 
, 
1.2 
---
-----~----------~---------
, ----------
I ----------
T ----------
1 
0.8 
0.6 
0.4 
0.2 
o o 
, 
, 
, 
, 
I 
I 
I 
I 
-------~---------
1
----------
I ----------
T
----------
--------- 1--
, 
, 
_______ ~ __________ L _________ ~ __________ I __________ ~ _________ _ 
I 
I 
I 
I 
I 
, 
, 
, 
, 
5 
10 
15 
20 
25 
30 
Figure H4.2 - Step response of the filter H(z) = 1/1-az- 1 , for a = -2/3, a = 1/2, 
a = 3/4 and a = 7/8 
H4.3 (Purely recursive second order) (see page 138) 
1. the transfer function is given by: 
1 
Therefore, al = - 2Re(pd and a2 = IPlI 2 . The variations as functions of 
the phases of the poles is given by the program: 

Chapter 10 - Hints and Solutions 419 
%===== car21.m 
% Gain as a function of the phase with a constant modulus 
Lfft=1024; freq=(0 :Lfft-1)/Lfft; modp=0 .9; 
theta=(20 : 10 :80) ; theta=theta * pi / 180 ; 
nbph=length(theta) ; 
a1=-2*modp*cos(theta) ; a2=modp - 2 * ones(1,nbph); 
AA=[ones(1,nbph) ; a1 ; a2] ; 
Df=fft(AA, Lfft); Hf=-20 * log10(abs(Df)); 
plot(freq(1 :Lfft/2),Hf(1:Lfft/2, :)); grid 
dB ,------------------------------------------, 
20 : 
30 : 
: 
: 
, 
: 
, 
: 
, 
20 -- ---:--
-- , 40 -50 ------:----- + ----+-----:-------:------ +------
15 
: 
• 
: 60 76 80
: 
: 
: 
: 
: 
10 
5 
o 
- 5 
- 10 ------:------
, ------;------:------
, ------;----
~--
,-~--- ~- ~~~ 
I 
I 
I 
I 
- 15L---~
' --~----~
' --~
' ----~--~
' --~----~--~--~ 
o 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
Figure H4.3 - Gains of a second order filter as functions of the phases of the poles 
2. type (Figure H4.4): 
%===== car22.m 
%==== Gain as a function of the modulus 
Lfft=1024; freq=(0 :Lfft-1)/Lfft ; 
modp=[0 . 1:0.2:0 .9 .95 .98]; 
% a few moduli 
theta=30 * pi / 180; 
nbph=length(modp); 
a1=- 2 * modp * cos(theta); a2=modp .- 2 ; 
AA=[ones(1,nbph); a1; a2]; 
Df=fft(AA, Lfft); Hf=-20 * log10(abs(Df)); 
plot(freq(1 :Lfft/2),Hf(1:Lfft/2, :)); grid 
3. the Jury test leads to: (i) D(l) > 0 =? 1 + al + a2 > 0, (ii) n even 
=? D( - 1) > 0 =? 1 - al + a2 > 0 and (iii) lao l > lanl =? 1 > la21 . In 
the plane (aI, a2), these conditions delimit a triangle called the stability 
triangle. 

420 Digital Signal and Image Processing using MATLAB® 
dB 
0.95 : 
30 ------ 1 -
20 
I 
I 
I 
I 
I 
I 
I 
------- , ------- ,------- 1 -------,-------,-------,-------
, 
, 
, 
10 
------, ------ , ------ ,--
0.3 
0 
0.1 
-10 
o 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
Figure H4.4 - Gains of a second order filter as functions of the moduli of the poles 
H4.4 (Suppressing a sinusoidal component) (see page 141) 
1. Type: 
%===== csup50hz1.m 
%===== Frequency response of the rejection filter 
nfft=256; freq=[O:nfft-1] / nfft; 
phi=pi/4; ro=.9; 
num=[ 1 -2*cos(phi) 1]; den=[ 1 -2*ro*cos(phi) ro*ro]; 
k=sum(den)/sum(num); num=k*num; % normalization 
snum=fft(num,nfft); sden=fft(den,nfft); 
spec=snum ./ sden; plot(freq,abs(spec)); grid 
1 ~
-
: 
, 
, 
: 
, 
: '-ir-
0.8 ------\- - -- -~ --- ---i------, ------~ ------i------~ ------, -----f----- _. 
I 
I 
I 
I 
I 
I 
I 
I 
I 
, 
" 
----- 1-
----------- -t 
----------- -1 
-
,--
0.6 
, 
, 
, 
, 
, 
---- , ------------------- , ------------ ,
-
-,--
0.4 
, 
" 
__ __ L _____ _ 1 ______ oJ. ______ L ______ 1 ______ .J ______ L ___ _ 
, 
, 
0.2 
, 
, 
, 
, 
o '--_--'_-'--______ --" _____ ' ___ ----''--_----' 
o 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
Figure H4.5 - Gain of the rejection filter 
2. when the frequency f is very different from ¢/27r, the modulus is roughly 
equal to 1 and the phase is roughly equal to O. 

Chapter 10 - Hints and Solutions 421 
In a neighborhood of ¢ (Figure H4.6) , we have: 
IH(f)1 
MZ 
l e2j7rf- eN I I 2(1-cos(27rf- ¢)) 
11/2 
MP~ 
e2j7rf-pej¢ = 1+p2-2pcos(27rf- ¢) 
1
27rf - 27rfo I 
1-p 
And therefore: 
1 
1 
IH(2j7rf) I < 
Ii> ~ 6.F ~ 
1i>(1- p)Fs 
y 2 
27ry 2 
2n(fo-./) 
Figure H4.6 - Construction of the rejection filter's frequency response 
3. type: 
%===== rejec500hz.m 
[x,Fs]=wavread('phrase.wav'); 
N=length(x); x=x(:)/max(abs(x»; mtime=(O:N-1), ; 
Fb=500; % Hz 
mnoise=sin(2*pi*Fb*mtime/Fs); 
xnoisy=x+mnoise; 
%====== rho is very close to 1. The transient part 
% 
of the output is very long 
rho=O.999; theta=2*pi*Fb/Fs; cost=cos(theta); 
cosphi=cost*(1+rho*rho)/rho/2; 
num1=O.5*[1+rho*rho -4*rho*cosphi l+rho*rho]; 
den1=[1 -2*rho*cosphi rho*rho]; 
%===== 

422 Digital Signal and Image Processing using MATLAB® 
xdenoised=filter(num1,den1,xnoisy); 
soundsc(xnoisy,Fs) 
disp('Press a key'); pause 
soundsc(xdenoised,Fs) 
subplot(211); plot(xnoisy); subplot(212); plot(xdenoised) 
2,----,----,----,----,----,----,----,----,----, 
signal with the sinusoidal cOJTIpon(~nt 
o 
-I 
_2L-__ ~ __ ~ ____ L-__ ~ __ ~ ____ ~ __ ~ __ _L __ ~ 
0.5 
o 
-0.5 
_
I~ __ ~ __ ~ ____ L-__ 
~ __ ~ ____ ~ __ ~ __ _L __ ~ 
Figure H4.7 - Effect of a value p R:j 1 on the filter's response 
4. expression 4.32 is verified by writing that the numerator of 4.31 has the 
same zeros as 4.30, that is: 
4p cos cp = 2 cos (j 
1 + p2 
The program rejection.m draws the complex gains and the poles and 
zeros of both transfer functions: 
%===== rejection .m 
nfft=1024; freq=[0:nfft-1]/nfft; 
rho=[.8: .02: .99]; rh02=rho.*rho; Lrho=length(rho); 
theta=pi/4; cost=cos(theta); 
figure(1); plot(fft([O 1] ,128)); grid; axis('square') 
numO=[1 -2*cost 1]; crnumO=roots(numO); 
hold on; plot(real(crnumO),imag(crnumO), '0'); 
figure(2); grid on; hold on 
figure(3); grid on; hold on 
for k=1:Lrho 
den1=[1 -2*rho(k)*cos(theta) rh02(k)] ; 

Chapter 10 - Hints and Solutions 423 
crden1=roots(den1); plot(real(crden1),imag(crden1), 'xr'); 
num1=numO*sum(den1)/sum(numO) ; 
num2=(1+rho2(k))*[1 -2*cost 1]/2; 
den2=[1 -cost*(1+rho2(k)) rho2(k)]; 
crden2=roots(den2); plot(real(crden2),imag(crden2), 'x'); 
num1s=fft(num1,nfft); num2s=fft(num2,nfft); 
den1s=fft(den1,nfft); den2s=fft(den2,nfft); 
figure(2), plot(freq,abs(num1s./den1s)) 
figure(3), plot(freq,abs(num2s./den2s)) 
figure (1) 
end 
hold off 
H4.5 (All-pass filter, properties of the maximum) (see page 145) 
Let bk = pej<p. The transformation defined by: 
P ( ) _ 1 - b'kz _ 1 - p2 
b* 
k
Z 
-
-----
k 
Z - bk 
Z - bk 
transforms the unit circle into itself. 
Figure H4.8 - Construction of Pk(z) 
Let M (z) be a point inside the unit circle r. z-bk translates r to rl. Mis 
transformed into Ml . The inversion 1/ (z - bk) takes M2, the transformation 
of Ml , outside the circle r 2, transformation of r l , because the inversion center 
remains inside r l (Ibkl < 1). The homothety and the translation that follows 

424 Digital Signal and Image Processing using MATLAB® 
maintain the successive transformations of M outside of the transformed circles. 
In the end, the point Pk(z) is therefore such that IPk(z)1 > 1. 
Conversely, if M (z) is outside f, Pk(z) will be inside f, hence IPk(z)1 < 1. 
H4.6 (All-pass filter) (see page 145) 
1. If we use the Parseval formula 2.27, then the filtering formula Y(f) = 
H(f)X(f) , we have: 
+00 
11/2 
1 1/2 
L ly(nW = 
IY(JWdj = 
IH(fWIX(fWdj 
n = - oo 
-1/2 
-1/2 
+00 
+00 
=? L ly(nW = L Ix(nW 
n =-CXJ 
n =-(XJ 
where we used the fact that IH(f)1 = 1; 
2. let: 
{ 
x ( n) if n ::; N 
x 
n = 
N ( 
) 
0 otherwise 
and let YN(n) be the filter's output signal. According to the result from 
the previous question: 
N 
+00 
+00 
L Ix(nW = L IXN(nW = L IYN(nW 
n=-oo 
n =-oo 
n =-(X) 
Because the filter is causal, YN(n) only depends on the values of xN(k) 
for k ::; n and therefore YN(n) = y(n) for n ::; N. This means that: 
+00 
N 
+00 
N 
L IYN(nW = L ly(nW + L IYN(nW ~ L ly(nW 
n = - oo 
n = - oo 
n = N+1 
n = - oo 
'---.,v.....----' 
2:0 

Chapter 10 - Hints and Solutions 425 
H4.7 (Minimum phase filter) (see page 147) 
1. Because the signal x(n) is causal and because the filters GAz) and Gzm(z) 
are causal, the signals y(n) and Ym(n) are causal. Hence formula 11.12 
can be applied, and we have: 
{ 
y(O) = limlzl--Hoo YAz) = limlzl-HOO GAz)Xz(z) 
Ym(O) = limlzl--HOO Yzm(z) = limlzl--HOO Gzm(z)Xz(z) 
But the relation between the transfer function Gzm(z) of the minimum 
phase filter and the transfer function of one of the filters with the same 
gain is of the type: 
where lal < 1. So if we impose Izl ---+ +00, we get: 
This result, shown for only one of the zeros, can of course be generalized 
to all of the zeros. 
As a conclusion, the impulse response of the minimum phase filter is, from 
the very first value, more "intense" than any other filter with the same 
gain. Simply put, the minimum phase filter has a "quicker response"; 
2. any filter Gz(z) can be seen as the series cascade of the minimum phase 
filter Gzm(z) and of an all-pass filter of the type: 
where lail < 1. Because all the poles of Hz(z) are inside the unit circle, 
the stable solution of Hz(z) is causal. If we apply the result from question 
2 of exercise 4.6, we can prove the expected result. 
Therefore, among all the systems that have a frequency response with 
the same modulus, the minimum phase system is the one that transmits 
the most energy over the shortest period of time. 
H4.8 (Window method: low-pass filter) (see page 162) 
1. For N odd, we have: 
h(n) = l
fo e2j7rnf dJ = sin(27rnJo) 
-fo 
7rn 

426 Digital Signal and Image Processing using MATLAB® 
and for N even: 
2. type: 
h(n) = l fo ej'Trf e2j'Trnf df = sin(27r(n + 1/2)fo) 
-fa 
7r(n + 1/2) 
function h=rif(N,fO) 
%!========================================================! 
%! FIR synthesis using the window method (Hamming window) 
%! SYNOPSIS: h=RIF(N,fO) 
%! 
h 
= length N impulse response 
%! 
N = filter order 
%! 
fO = normalized cut-off frequency 
%!========================================================! 
P=fix(N/2); ham=0.54-0.46*cos(2*pi*(0:P-1)/(N-1)); 
if (rem(N,2)==0) 
% N even 
d=((-P:-1)+.5)*pi; h=sin(2*d*fO) ./ d; 
h=h .* ham ; h=[h h(P:-1:1)); 
else 
% N odd 
end 
d=(-P:-1)*pi; h=sin(2*d*fO) ./ d; 
h=h .* ham ; h=[h 2*fO h(P:-1:1)); 
return 
H4.9 (Window method: band-pass filter) (see page 163) 
1. The DTFT of the sequence 2h(n) cos(27rnfo) 
h(n)e2j'Trnfo + 
h(n)e-2j'Trn fo can be written as H(j - fo) + H(j + fo). Hence, if H(j) 
is the complex gain of a low-pass filter, the result is a filter the complex 
gain of which is centered around ±fo, therefore a band-pass filter; 
2. type: 
%===== bpfilter.m 
%===== band-pass filter 
Lfft=1024; fq=(0:Lfft-1)/Lfft; fO=0.2; fb=0.1; 
N=input('length: '); 
P=fix(N/2); R=rem(N,2); h=rif(N,fb/2); 
if (R==O), D=(-P:P-1)+1/2; else, D=(-P:P); end 
%===== modulation 
g=2*h .* cos(2*pi*fO*D); gf=fft(g,Lfft); 
agf=abs(gf); phigf=angle(gf); 
figure(1); plot(fq(1:Lfft/2),agf(1:Lfft/2)); grid 
figure(2); plot(fq(1:Lfft/2),phigf(1:Lfft/2)); grid 
Notice that if we want to maintain a linear phase, we have to multiply 
by 2cos(27rnfo) if N is odd and by 2cos(27r(n + 1/2)fo) if N is even. 
Figure H4.9 shows the gabarit obtained for N = 80. 

Chapter 10 - Hints and Solutions 427 
0.8 
0.6 
0.4 
0.2 
OL-~--~~~--~--~~~~--~~~~ 
o 0.05 0.1 0.15 0.2 0.i5
o -i)j 0.35 0.4 0.45 0.5 
Figure H4.9 - Spectrum of the band-pass filter 
H4.10 (Window method: derivative filter) (see page 163) 
1. The Fourier transform of dx(t)/dt is 2j'irFX(F) (section AI). This ex-
pression corresponds to the filtering of x(t) by a filter with the complex 
gain Ha(F) = 2j'irF. This expression calls for a comment: in the case of 
(-Fs/2, +Fs/2) band-limited real signals, the method given on page 149 
provides, for the digital filter, a complex gain equal to H(f) = 2j'ir Fsi 
in the band (-1/2,1/2); 
2. the previous results lead us to the Fourier series expansion coefficients of 
H(f), which have the expression: 
h(n) = Fs 11/2 2j'irie2j7rnj di 
-1/2 
In theory, a derivative filter is such that the output dimension is the same 
as the input dimension divided by seconds, or in other words multiplied 
by Hertz. This explains how the term Fs appears in the digital filter's 
impulse response. When all the calculations are done, we get: 
{ 
0 
for n = 0 
h(n) = 
cos ('irn) 
Fs 
for 
n i- 0 
n 
(10.3) 
The sequence is truncated between -N and +N, and the result is mul-
tiplied by the weighting function; 
3. for N = 12, the filter's output must have the expression: 
u(n) = h( -12)x(n + 12) + ... + h(O)x(n) + ... + h(12)x(n - 12) 

428 Digital Signal and Image Processing using MATLAB® 
This is not a causal solution: it requires the 12 future input values to be 
known in order to calculate the output u(n). A causal realization consists 
of taking: 
y(n) = h( -12)x(n) + ... + h(O)x(n - 12) + ... + h(12)x(n - 24) 
hence y(n) = u(n - 12). This solution causes a 12 sample delay; 
4. the derivative function is: 
function [y,hder]=deriv(N,x,Fs) 
%!===========================================! 
%! Digital derivation (Hamming window) 
! 
%! SYNOPSIS: [y,hder]=DERIV(N,x,Fs) 
%! 
N 
(2N+1) coefficient FIR filter 
%! 
x 
signal 
%! 
Fs 
sampling frequency (default : 1) 
%! 
y 
result 
%! 
hder 
impulse response of the filter 
%!===========================================! 
if nargin<3, Fs=l; end; 
if nargin<2, error('Parameters are missing. '); return; end 
hder=cos(pi*(l:N)) ./ (l:N); hder=[-fliplr(hder) 0 hder]; 
%===== Hamming window 
hder=Fs * hder .* (O.54+0.46*cos((-N:N)*pi/N)); 
y=filter(hder,l,x); 
return 
The following program tests the derivative filter on the function xa(t) = 
sin(27r Fot). 
Notice the shift due to the causal design, as well as the 
transient state due to the choice of the initial conditions. Type: 
%===== cdersin.m 
N=12; Fs=4000; nfft=512; freq=(O:nfft-1)/nfft*Fs; 
FO=300; T=100 ; t=(O :T-1)/Fs; 
%===== original 
x=sin(2*pi*FO*t); 
subplot(321); plot(t,x); grid 
axis([O (T-1)/Fs -1 .2 1.2]); title('x(t)') 
%===== theoretical derivative 
xp=2*pi*FO*cos(2*pi*FO*t); 
% result 
ordm=1.2*2*pi*FO; 
subplot(322); plot(t,xp); grid 
axis([O (T-1)/Fs -ordm ordm]); title('x' '(t)') 
%===== digital derivative 
[y hder]=deriv(N,x); y=Fs*y; 
subplot(323) ; plot(t,y) ; grid 

Chapter 10 - Hints and Solutions 429 
axis([O (T-l)/Fs -ordm ordm]); title('y(t)') 
%===== delay due to the filter 
subplot(324); plot(t,xp, 'b' ,t-N/Fs,y); grid 
axis([O (T-l)/Fs -ordm ordm]); title('y(t-N/Fs) ') 
%===== gain of the derivative filter 
hders=fft(Fs*hder,nfft); 
subplot (313) , plot(freq,abs(hders) ,[0 Fs/2] , [0 Fs*pi]) ; 
axtemp=axis; axis([O Fs/2 axtemp(3 :4)]); grid 
This program also provides the gain of the derivative filter and of the 
obtained filter. Try both the rectangular window and the Hamming win-
dow. Check that the amplitude ratio of x (t) to y(t) is equal to the value 
of the gain in Fa = 300 Hz. 
The following program tests the derivative filter on a periodic square 
signal (Figure H4.1O): 
%===== cderhor.m 
clear; N=30; Fs=100; 
xT=[ones(1,50) zeros(1,50)]; x=[xT xT xT xT]; 
T=length(x); t=(O:T-l)/Fs; 
[y hder]=deriv(N,x); y=y*Fs; 
%===== original signal 
subplot(311); plot(t,x); 
axis([O T/Fs -1.5 1.5]); grid 
%===== impulse response of the filter 
subplot(312) ; 
plot([0:2*N] ,hder, '-' ,[0:2*N],hder,'o'); grid 
%===== result 
subplot(313) ; plot(t-N/Fs,y) ; 
axis([O T/Fs -Fs Fs]); grid 
H4.11 (Butterworth filter) (see page 166) 
1. Type: 
function den=calcbutter(n,omegO) 
%!========================================! 
%! SYNOPSIS: den=CALCBUTTER(n) 
! 
%! n 
= denominator degree 
%! den = denominator in descending powers 
%!========================================! 
if nargin<2, omegO=l; end 
nr=rem(n,2); nb=(n-nr)/2; 
den=[eye(l,nr)/omegO 1]; 
for k=l:nb 
alp=pi*(2*k-l+nr)/n/2; 
den2=[1/omegO/omegO 2*cos(alp)/omegO 1]; 

430 Digital Signal and Image Processing using MATLAB® 
~ 
, 
, 
, 
~ :nnllunuguuFlunnj 
-------
--------
--------
-------
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
o 
0.5 
1.5 
2 
2.5 
3 
3.5 
4 
100,-----,------,----~,-----,-----_,----_, 
50 
---------- c 
---------- 1--
o 
-50 
impulse response of the derivative filter 
-IOOL-----~------~----~----~~----~----~ 
o 
10 
20 
30 
40 
50 
60 
100,---~----~--~----~----~--_,----~--~ 
-- 1 
------- ,--
50 
O~~~~~
.~
~~~~~I~~~~~~:~~~~~--~~-I 
-50 -------"----
. -
--. - - - - - --.- - --
-IOOL---~----~--~----~----~·--~----~--~ 
o 
0.5 
1.5 
2 
2.5 
3 
3.5 
Figure H4.10 - Differentiation of a periodic square signal 
II 
den=conv(den,den2) ; 
end 
return 
Type: 
2. type: 
%===== cbutterl .m 
n=input('Butterworth filter order: '); 
den=calcbutter(n); 
den 
% displaying the result 
%===== cbutter2.m 
Nordre=6; mmax=40; omegO=10; 
omeg=logspace(log10(omegO/l0) ,log10(10*omegO) ,mmax); 
s=lj*omeg; g=zeros(mmax,Nordre-l); 
for iord=2:Nordre 
end 
den=calcbutter(iord,omegO) 
gain=l ./ polyval(den,s); 
g( : ,iord-l)=gain.'; 
ag=abs(g); agdB=20*log10(ag); 
subplot(211) , semilogx(omeg,ag,'.' ,omeg,ag); grid ; 
set(gca, 'xlim', [.4 ,3]*omegO), title('Gain') 
4 

Chapter 10 - Hints and Solutions 431 
set(gca, 'Xtick' ,[.5 .6 .7 .8 . 9 1 23]*omegO) 
subplot(212), semilogx(omeg,agdB , ' . ' ,omeg,agdB); grid ; 
set(gca, 'xlim', [ .4 , 2]*omegO), title('dB gain') 
set(gca, 'Xtick' ,[ . 5 .6 .7 .8 .9 1 2 3]*omegO) 
3. bilinear transformation program: 
(a) Horner representation of the polynomial g(x): 
{
go = an 
gk(X) = aN-k + gk- 1(X)X for 
k = 1 : N 
(b) rational function variable change: x = B(z )/A(z). If we define 
gk = Nk/ Dk, we get for k = 1, ... , N: 
No(z) = an and Do(z) = 1 
k = 1 
N ' { Dk(Z) = Dk- 1(Z)A(z) 
,... 
. 
Nk(z ) = aN- kDk(Z) + Nk- 1(Z)B(z) 
( c) for the bilinear transform (in our case choose T = 1): 
2 1 -
Z- l 
4. type1 : 
X = -...,------:-
T 1 + Z- 1 
function [B ,A]=nbilin(pol,Ts) 
%!==================================================! 
%! Bilinear transform of a polynomial 
! 
%! SYNOPSIS : [B,A]=NBILIN(pol , Ts) 
%! 
pol = polynomial (decreasing powers of s ) 
%! 
= aO s-n+a1 s - (n-i) + .. . +aN 
%! 
Ts 
= sampling period 
%! 
B,A = numerator and denominator of the result 
%!==================================================! 
if nargin<2, Ts=l; end 
NX=[1 -1]*2/Ts; DX=[1 1]; 
nP=length(pol); PP=zeros(nP,l); PP(:)=pol ; 
B=pol(1); A=[l] ; 
for k=2:nP 
A=conv (A,DX); B=conv (B,NX) + pol(k)*A ; 
end 
return 
buttertest .m uses the functions calcbutter and nbilin (Figures H4.11 
and H4.12): 
IThe "signal processing toolbox" uses the bilinear function, the "robust control toolbox" 
uses the bilin function and the "control system toolbox" uses the c2d function. 

432 Digital Signal and Image Processing using MATLAB® 
%===== buttertest.m 
Nordre=6; mmax=40; omegO=10; 
alpha=15; % ratio ws/wO 
Fs=alpha*omegO/2/pi; Ts=l/Fs; 
%===== 
omeg=linspace(omegO/10,omegO*10,mmax); s=lj*omeg; 
den=calcbutter(Nordre,omegO); gain=l . / polyval(den,s); 
gdB=20*10g10(abs(gain)); 
%===== 
figure(l) , plot(omeg,gdB,'.' ,0meg,gdB); grid on; hold on 
%===== 
nfft=1024; freq=[O:nfft-1]/nfft*Fs; omegd=2*pi*freq; 
[dend,numd]=nbilin(den,Ts); 
gaind=fft(numd,nfft)./fft(dend,nfft); 
gdBd=20*10g10(abs(gaind)); 
idx=(1:7*nfft/16); plot (omegd(idx) ,gdBd(idx) , 'r') 
mylim=get(gca, 'ylim'); 
plot([pi*Fs pi*Fs] ,mylim), hold off 
%===== poles and zeros 
figure (2) , plot(exp(2*pi*j*[O:100]/100)) , hold on 
rden=roots(dend); rnum=roots(numd); 
plot(real(rden),imag(rden), 'x') 
plot(real(rnum),imag(rnum), '0') 
grid, set(gca,'DataAspectRatio' ,[1 1 1]), hold off 
(dB),--,-----------------------,-------, 
o 
- 20 
-40 
- 60 
- 80 
-100 
-120 
-140 
..... , . ••• . > .• l ti"TtiT 
•••• ] •••• ' •• · 
•• > • ••• • • 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
--------r-----'------r-----'------
I 
I 
I 
I 
I 
I 
I 
I 
, 
, 
, 
I 
I 
I 
I 
I 
I 
I 
I 
::::::[:::::J::::
~
- : ~ :: -:::[:::--j
::::::~:::::~::::
--
discrete time : 
------~-----~------~-----~------
-----~------~-----~------
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
-160 
----- -~ ----- ~- ---- -~ ----- ~- ---- -~ -- -- ~- ---- -~ ----- ~- -----
I 
I 
I 
I 
I 
I 
I 
I 
W 
I 
I 
I 
I 
I 
I 
I 
I 
-180 '---__ 
0 -~
''----~
' -~
'--~
' -~
' --~
' 
---7-~
' __ 
''------' 
o 
10 
20 
30 
40 
50 
60 
70
, 80 
90 
100 (rad/s) 
ws/2 
Figure H4.11 - Continuous and discrete time frequency response 

Chapter 10 - Hints and Solutions 433 
0.8 ------------ ---------
--~-----------
" ------------~--
---------
, 
, 
, 
, 
0.6 ------------ ----
-------i-------------
, ------------~-------
----
, 
, 
, 
, 
, 
, 
0.4 ------------ -
----------I-------------
I ------------~----------
-
: 
: 
x 
, 
, 
0.2 
------------1 ------------:-------------
: ------------~-----
~ ------
" 
, 
~ 
~
: 
: 
x 
o --------~--
--i---------:------------
~ ------------~------------
" 
: 
' 
: 
x 
, 
, 
-0.2 
--------- --: ------------:-------------
: ------------~-----
~
------
0.005 o 
-0.005 
c-::-::-=nc:'-'::'::-:--::-::l
: 
: 
x 
-------1-------------1------------,---------- -
--
~ ----
----
~
--
I 
I 
, 
, 
, 
, 
, 
, 
---1--- 0 0- -- ' --
, 
, 
--
~ --
G -
-
-(}
-
~ --
-------:-------------'------------:------- ----
___ : ___ 0 .0. _- 1--: 
: 
----
--I-------------
I ------------~--
________ _ 
--
~
----
----
~
--
I 
I 
-- -:- - - - ~ - - - - f --
-1.01 -1 -0.99 
-0.5 
o 
0.5 
Figure H4.12 - Poles and zeros 
H4.12 (Temporal aliasing and DFT) (see page 167) 
Let us calculate the original hen) of {H(k/N)}: 
hen) 
00 
N-l 
~L 
'\"' 
. (= -n)k 
hem) L.J e- 27rJ -
N-
m = -oo 
k=O 
The last sum is different from zero, and equal to N when m = n[N]: 
00 
hen) = L hen + rN), with n E [0, N - 1] 
T =-(X) 
This sum expresses a temporal aliasing phenomenon, which is negligible if 
the number of points chosen for "sampling" H(f) is large. For example, in 
the case of a low-pass filter, the coefficients behave like l/n and the temporal 
aliasing grows fainter as N increases: 
%===== tempalias.m 
%===== window method 
nfft=1024; freq=[O:nfft-l]/nfft; 
Ng=128; Npts=Ng/2; n=[-Npts:Npts-l]; tps=n+Npts; 

434 Digital Signal and Image Processing using MATLAB® 
pit=((-Npts:Npts-1)+.5)*pi; 
fc=.1; hn=sin(2*pit*fc) .1 pit; hns=fft(hn,nfft); 
hnf=hn .* ( .54-.46*cos(pi*tps/Npts)); hnfs=fft(hnf,nfft); 
subplot(221) ; stem(n,hnf) ; grid 
Mx=max(hnf)*1.5; Mn=min(hnf)*1.2; 
set(gca, 'ylim' , [Mn Mx]) 
subplot(212); plot(freq,abs(hnfs), '-' ,freq,abs(hns), '-g'); grid 
%===== direct method 
Ngt=128 ; frt=[0:Ngt-1]/Ngt; 
fcd=fix (fc*Ngt)+1; Nz=Ngt-2*fcd; 
Gfk=[ones (1,fcd) zeros(1,Nz+1) ones(1,fcd-1)] ; 
subplot(212) ; hold on; plot(frt ,Gfk , 'or'); hold off ; 
set(gca, 'xlim' , [0 .5]) 
%set(gca, ' xlim ' , [ .5 1.5]*fc, ' ylim' ,[-0.05 1.3]) 
%===== calculating hnt 
hnt=real(ifft(Gfk)) ; hnt=[hnt(Ngt/2+1:Ngt) hnt(1 :Ngt/2)]; 
subplot(222) ; stem([- Ngt/2:Ngt/2-1] , hnt) ; grid 
set(gca, 'ylim' , [Mn Mx]) 
%===== windowing the hnt 
hntf=hnt .* ( .54- .46*cos(2*pi*[0 :Ngt-1]/(Ngt-1))); 
%===== verification with the spectrum 
nfft=8192; freq=[0:nfft-1]/nfft; 
hnts=fft(hnt,nfft) ; hntfs=fft(hntf ,nfft) ; 
subplot(212); hold on ; 
plot (freq,abs(hnts) , 'b' ,freq,abs(hntfs), 'r') ; hold off; 
set(gca, 'xlim' , [fc*. 8 fc*1 .2]) 
H4.13 (Interpolation) (see page 169) 
1. The interM . m function interpolates by a factor M. To approximate the 
ideal low-pass filter with the band (-I / (2M), +1/ (2M)) and the gain 1, 
we used the window method for the computation of the length 81 FIR 
filter. The window used is a Hamming window: 
function y=interM(x, M,Nf) 
%!===============================! 
%! Interpolation function 
%! SYNOPSIS: y=INTERM(x ,M,Nf) 
%! 
x 
= input sequence 
%! 
M 
interpolation ratio 
%! 
Nf 
2Nf+1 coeffts filter 
%! 
y 
output sequence 
%!===============================! 
if nargin<3, Nf=40; end 
%===== low-pass filter 
theta=pi*[1 :Nf]; h=sin(theta/M) .1 (theta) ; 
%===== Hamming window 
h=h .* 
( .54 + .46*cos(theta/Nf ) ) ; 

Chapter 10 - Hints and Solutions 435 
h=[fliplr(h) l/M h); h=h/sum(h)*M; 
%===== insertion of zeros 
xO=zeros(length(x)*M+Nf,l); xO(l:M:length(x)*M)=x; 
y =filter(h,l,xO); y=y(Nf+l:length(y)); 
return 
2. an application example: 
%===== intermex.m 
x=rand (1 ,40) ; 
M=4; y=interM(x,M); 
plot(y); hold on; plot(y,'xr'); 
plot([l:M:length(y)) ,x, '0'); hold off, grid 
H4.14 (Undersampling) (see page 173) 
1. The deeM.m function undersamples by a factor M. To approximate the 
ideal low-pass filter with the band (-1 / (2M) , +1/ (2M)) and the gain 1, 
we used the window method for the computation of a FIR filter: 
function y=decM(x,M,Nf) 
%!===============================! 
%! Decimation function 
%! SYNOPSIS: y=DECM(x,M,Nf) 
%! 
x 
= input sequence 
%! 
M 
decimation ratio 
%! 
Nf 
2Nf+l coeffts filter 
%! 
y 
output sequence 
%!===============================! 
if nargin<3, Nf=20; end 
theta=(l:Nf)*pi; h=sin(theta/M) ./ (theta/M); 
h=h .* (0.54 + 0.46 * cos(theta/Nf)); % Hamming window 
h=[fliplr(h) 1 h)/M; 
xO=zeros(length(x)+Nf,l); xO(l:length(x))=x; 
y= filter(h, 1, xO); 
y=y(Nf+l:M:length(y)); 
% decimation 
return 
2. undersampling a speech signal: 
%===== decMspeech.m 
load phrase 
Lsn=length(sn); 
soundsc(sn,8000) 
%===== one out of every 2 samples 
sn2=sn(1:2:Lsn); soundsc(sn2,4000) 
% original signal 
I sn2se=decM(sn,2); soundsc(sn2se,4000) % undersampling with M=2 

436 Digital Signal and Image Processing using MATLAB® 
H4.15 (Paralleled undersampling and oversampling) (see page 173) 
1. Figure H4.13 shows the design structure of the factor M oversampler: 
the signal we wish to undersample is "broken up" into M delayed and 
undersampled signals that are filtered in parallel by M filters. If the ideal 
low-pass filter is approximated by a length L = £M filter, each filter in 
the diagram H4.13 has a length £; 
{x(n)} 
~ 
{x(n-mM)} 
-.1 
h(nM) ~r 
{yen)} 
~[EJ 
{x(n-mM-l)} ~ I h(nM+l) h 
+ 
¢,l 
[EJ {x(n-mM-M+l)} I 
~ 1 
~ 
'fM 
')0 h(nM+M-l) 
Q9 
Figure H4.13 - Polyphase architecture of undersampling 
2. for M = 4 and for a length 8 filter, let us write the output signal yen). 
In n = 0, we have yeO) = h(0)x(4n). For n = 1, y(4) = (h(0)x(4) + 
h(4)x(0)) + h(l)x(3) + h(2)x(2) + h(3)x(I). And for n > 1, we get: 
y(4n) 
(h(0)x(4n) + h(4)x(4n - 4)) 
+ (h(1)x(4n - 1) + h(5)x(4n - 5)) 
+ (h(2)x( 4n - 2) + h(6)x( 4n - 6)) 
+ (h(3)x( 4n - 3) + h(7)x( 4n - 7)) 
Therefore, y(4n) is the sum of 4 filterings involving the sequences {x(O), 
x(4), ... }, {O, x(3), x(7), .. . }, {O, x(2), x(6), ... } and {O, x(I), x(5), 
. . . }. This computation is performed in the following program: 
%===== decpara.m 
clear; M=4; 
N=1500; L=16; % Nand M must be multiple of M 
x=randn(N,1); h=(1:L); 
%===== direct undersampling -> yu 
y=filter(h,1,x); yu=y(1:M:N); 
%===== parallelized undersampling -> yp 
yp=zeros(N/M,1); 
for k=1:M 
auxx=x(k+1:M:N); lx=length(auxx); auxh=h(M-k+1:M:end); 
yp(1:lx)=yp(1:lx)+filter(auxh,1,auxx); 
end 
max(abs(yu(M+1:lx)-yp(M:lx-1))) 

3. type: 
%===== overpara.m 
clear all 
M=4; N=150; L=16; 
Chapter 10 - Hints and Solutions 437 
x=randn(N,l); h=(l:L); 
xo=zeros(N*M,l); xo(l:M:end)=x; 
%===== direct oversampling -> yo 
yo=filter(h,l,xo); yp=zeros(N*M,l); 
%===== parallelized oversampling -> yp 
for k=l:M 
auxh=h(k:M:end); 
yp(k:M:N*M)=yp(k:M:N*M)+filter(auxh,l,x); 
end 
max(abs(yo(M:end)-yp(M:end))) 
H5 An introduction to image processing 
H5.1 (Logical functions) (see p. 183) 
1. Logical functions: 
function pixr=FoncLog(pix1,ffl,pix2) 
%!==================================================! 
%! Logical operation 
! 
%! SYNOPSIS: pixr=FONCLOG(pix1,ffl,pix2) 
%! 
pix1 
first image (gray levels) (UINT8) 
%! 
ffl 
logical function '&', '1', 'xor', '-' 
%! 
I == I, 
I < I, 
I> I, 
I <= I, 
I >= I, 
1 .... = I 
%! 
pix2 
second image (gray levels) (UINT8) 
%! 
pixr 
resulting image (UINT8) 
%! Uses: bitan, bitor, bitxor, bitcmp 
%! Constraint: size(colormap,1)<=256 
%!==================================================! 
N1=size (pix!) ; 
ffl=strtrim(ffl); 
if (nargin<3), 
ffl='-' ; 
else 
if (N1 -= size(pix2)), 
error('Matrix dimensions have to match! ') 
end 
end 
if isempty(findstr('&I«=»==-=xor' ,ffl)), 
error('Logical expression not correct') 
end 
ff=strvcat(ffl, 'AAA'); ffl=ff(l ,:) ; 

438 Digital Signal and Image Processing using MATLAB® 
oplog=strvcat ( , &' , , I ' , 'xor' , , - , , ... 
I == I , I < I , I> I , 
I <= I , I >= I , I _= ') ; 
nbc=size(oplog); 
%===== 
oplog2=repmat(' ',1,nbc(1)*nbc(2)); 
oplog2 ( : ) =oplog' ; 
kO=ceil(strfind(oplog2,ffl)!nbc(2)); 
switch kO 
case 1 
pixr=bitand(pixl,pix2); 
case 2 
pixr=bitor(pixl,pix2); 
case 3 
pixr=bitxor(pixl,pix2); 
case 4 
pixr=bitcmp(pixl); 
case 5 
pixr=bitxor(pixl,pix2); 
case 6 % < (pl* p2) 
pixr=bitand(bitcmp(pixl),pix2); 
case 7 % > (pi p2*) 
pixr=bitand(pixl,bitcmp(pix2)); 
case 8 % <= (pl* + p2) 
pixr=bitor(bitcmp(pixl),pix2); 
case 9 % >= (pi + p2*) 
pixr=bitor(pixl,bitcmp(pix2)); 
case 10 % -= 
pixr=bitcmp(bitxor(pixl,pix2)); 
otherwise 
end 
return 
2. test program for the FoncLog function: 
%===== testlogic2.m 
load lena25; 
% loading and displaying 
subplot(221); imagesc(pixc); 
% the first image 
colormap(cmap); axis('image'); 
set(gca, 'units', 'pixels'); title('Original') 
subplot(222); 
imagesc(pixtl); axis('image'); title('Mask') 
%===== logical operators >= and -
pixr = FoncLog(uint8(pixc), '>=' ,uint8(pixtl)); 
subplot(223); imagesc(pixr); axis('image'); 
title('Result for the >=') 
pixr = FoncLog(uint8(pixc), '-' ,uint8(pixtl)); 
subplot(224); imagesc(pixr); axis('image'); 
title('Result for the -') 

Chapter 10 - Hints and Solutions 439 
H5.2 (Plane transformation) (see p. 195) 
1. Linear transformation: 
function pixcR=lintrimg(pixc,M) 
%!==================================! 
%! Linear transform of an image 
%! SYNOPSIS: pixcR=LINTRIMG(pixc,M) 
%! 
pixc 
image (nl*nc) pixels 
%! 
M = transform matrix 
%! 
pixcR = result 
%!==================================! 
Spix=size(pixc); Nl=Spix(l); Nc=Spix(2); 
%===== pixel coordinates 
tbx=ones(Nl,l)*[l:Nc]; tby=[l:Nl] '*ones(l,Nc); 
nlg=reshape(tby,l,Nl*Nc); ncl=reshape(tbx,l,Nl*Nc); 
tbidx=[nlg;ncl] ; 
idtb=nlg+(ncl-l)*Nl; 
% linear indices 
%===== transformation 
tbxy=[ncl;Nl-nlg]; 
% coordinates 
tbv=round(M*tbxy); 
xmin=min(tbv(l, :)); xmax=max(tbv(l,:)); ncol=xmax-xmin+l; 
ymin=min(tbv(2, :)); ymax=max(tbv(2,:)); nlig=ymax-ymin+l; 
pixcR=zeros(nlig,ncol); 
tbidxR=[nlig-(tbv(2, :)-ymin) ;tbv(1, :)-xmin+l]; 
idtbR=tbidxR(l, :)+(tbidxR(2, :)-l)*nlig ; % linear indices 
pixcR(idtbR)=pixc(idtb); 
return 
2. defining the transformation (Figure H5.1), type: 
%===== transftri.m 
% From one triangle to another using (x,y)-coordinates 
hl=figure; set(hl, 'color', [1 1 0]) 
set(gca, 'xlim', [0 100], 'ylim' ,[0 100],'NextPlot', 'add'); 
grid; trixy=[]; 
for k=1:3 
pt=ginput(l); trixy=[trixy;pt]; plot(pt(1),pt(2), 'ob') 
text(pt(1)-1,pt(2)+3 , int2str(k)) 
end 
plot([trixy(: ,1);trixy(1 , 1)] ,[trixy(:,2);trixy(1,2)]) 
triXY=[] ; 
for k=1:3 
end 
pt=ginput(l); triXY=[triXY;pt]; plot(pt(1),pt(2), 'or') 
text(pt(1)-1,pt(2)+3,int2str(k)) 
plot([triXY(: ,1);triXY(1,1)] , [triXY(:,2);triXY(1,2)] , 'r') 
%===== transformation matrix 
Mxy=[trixy [1;1 ;1]]' ; MXY=[triXY [1 ;1;1]]'; M=MXY*inv(Mxy) 

440 Digital Signal and Image Processing using MATLAB® 
80 
70 
""'" 
~
" ~ F:T:j
Z1r
~Z
~ 
30 
____ : ____ : ____ :_ 3 
1--0---
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
____ ~ ____ ' 
___ L 
__ ~ 
___ L _ 
" 
, 
20 
" 
, 
10 
" 
, 
o L--L~L-~~ 
__ ~~ 
__ ~~ 
__ L_~ 
o 
10 20 30 40 50 60 70 80 90 100 
Figure H5.1 - Defining the transformation 
3. applying the transformation (Figure H5.2) can lead to many "missing 
points" that have to be processed. In the example we chose, a simple 
median filtering is enough, but this is rarely the case: 
%===== wentog.m 
A = imread('wentog.jpg'); % (589x440x3) 
Ad=double(A); 
%==== RVB --) gray 
C=[O.299;O.587;O.114] ; 
Ag=Ad(:,: ,1)*C(1)+Ad(:,: ,2)*C(2)+Ad(:,: ,3)*C(3); 
Ag=Ag/255; 
subplot (211) 
imagesc(Ag); axis('image'); colormap(cmap) 
%==== transformation 
pixcR=lintrimg(Ag,M(1:2,1:2)); 
subplot(223); imagesc(pixcR); 
colormap(cmap); axis('image') 
%==== median filter 
imageS = median2D(pixcR,3,3); 
subplot(224); imagesc(imageS); 
colormap(cmap); axis('image') 

Chapter 10 - Hints and Solutions 441 
Figure H5.2 - Applying the transformation 
H5.3 (Transformation of a rectangular selection) (see p. 195) 
We can write: 
o 0 
1 
o 
o 
0 
0 
-XIXI 
Xl 
YI 
1 
- XIYI 
a 
b 
tx 
c 
d 
ty 
e 
f 
By solving this system, we can determine the transformation matrix: 
%===== recttransf.m 
% Transformation of a rectangle 
fmt='jpeg'; fn='christine.jpg'; pixc=imread(fn,fmt); 
figure(1); imagesc(pixc); set(gcf, 'color', [1 1 1]) 
tbcolor=[O:1/255:1] '*[1 1 1]; colormap(tbcolor); 
set(gca, 'DataAspectRatio', [1 1 1], 'units', 'pixels') 
Spix=size(pixc); NI=Spix(1); Nc=Spix(2); 
xor=40; yor=40; 
set(gca, 'Position', [xor yor Nc NI], 'NextPlot', 'add'); 
xy=[1 1;Nc 1;Nc NI ; 1 NI]; XY=[] ; 
plot([xy(:,1);xy(1,1)],[xy(:,2);xy(1,2)], 'r') 
for k=1:4 
end 
pt=ginput(1); XY=[XY;pt]; plot(pt(1),pt(2), 'oy') 
text(pt(1),pt(2),int2str(k)) 
plot([XY(:,1);XY(1,1)],[XY(:,2);XY(1,2)], 'y') 
set(gca, 'Position', [xor yor Nc NI], 'NextPlot', 'replace'); 
%===== calculating the transformation matrix 

442 Digital Signal and Image Processing using MATLAB® 
Mt= [] ; 
for k=1:4 
11=[xy(k,:) 1 zeros(1,3) -xy(k,l)*XY(k,l) -xy(k,2)*XY(k,1)]; 
12=[zeros(1,3) xy(k,:) 1 -xy(k,1)*XY(k,2) -xy(k,2)*XY(k,2)]; 
Mt=[Mt;11;12]; 
end 
XX=zeros(8,1); XX(:)=XY'; cof=Mt\XX; 
coeff=zeros(3,3); coeff(:)=[cof;l]; coeff=coeff'; 
%===== application 
xc=[l:Nc]; yc=[l:NI]'; 
tbx=ones(NI,l)*xc; tby=yc*ones(l,Nc); 
tbxy1=[reshape(tbx,1,NI*Nc);reshape(tby,1,NI*Nc);ones(1,NI*Nc)]; 
idtb=(tbxy1(1,:)-1)*NI+tbxy1(2,:); % linear index 
tbv=coeff*tbxy1; 
tbv(l,:)=round(tbv(l, :)./tbv(3,:)); 
tbv(2,:)=round(tbv(2, :)./tbv(3,:)); 
xmin=min(tbv(l,:)); xmax=max(tbv(l,:)); ncol=xmax-xmin+1; 
ymin=min(tbv(2,:)); ymax=max(tbv(2,:)); nlig=ymax-ymin+1; 
tbidxR=[tbv(1,:)-xmin+1;tbv(2,:)-ymin+1]; 
pixcR=255*ones(nlig,ncol); 
idtbR=(tbidxR(1,:)-1)*nlig+tbidxR(2,:); 
pixcR(idtbR)=pixc(idtb); 
%===== displaying the result 
figure(2); imagesc(pixcR); set(gcf, 'color' ,[1 1 1]) 
colormap(tbcolor); set(gca, 'DataAspectRatio', [1 1 1]) 
set(gca, 'units', 'pixels' ,'Position' , [xor yor ncol nlig]); 
50 
100 
150 
200 
250 
300 
20 40 60 80 100120 
Figure H5.3 - Applying the torsion 
H5.4 (Rectangular filter) (see p. 206) 
%===== lenarect.m 
load lena; dims=size(pixc); 
figure(l); imagesc(pixc+1); colormap(cmap); axis('image') 
%===== rectangular filter 
h=ones(5,1) * ones(1,5); 
h=h/25; pixr=round(filter2(h,pixc)); 

Chapter 10 - Hints and Solutions 443 
figure(2); imagesc(pixr+1); colormap(cmap); axis('image') 
resul=zeros(dims(1)+2,dims(2)+2); resu12=resul; 
%===== implementing the filtering with two filters 
pixc==[pixc ones(dims(1),2); ones(2,dims(2)+2)]; 
for 1=1:dims(2)+2, 
resul(: ,1)=filter(ones(5,1)/5,1,pixc(: ,1)); 
end 
for k=1:dims(1)+2, 
resu12(k, :)=filter(ones(1,5)/5,1,resul(k, : )); 
end 
%===== 
resul=round(resu12(3:dims(1)+2, 3:dims(2)+2)); 
figure(3); imagesc(resul+1); colormap(cmap); axis('image') 
H5.5 (Conical filter) (see p. 207) 
%===== lenacone.m 
%===== loading the original image 
load lena; figure(1); imagesc(pixc+1); 
colormap(cmap); axis('image') 
%===== conical filter 
h=[O= 0 1 0 0;0 2 2 2 0; ... 
1 2 5 2 1;0 2 2 2 0;0 0 1 0 0]; 
h=h/sum(sum(h)); pixr=round(filter2(h,pixc)); 
figure(2); colormap(cmap); 
imagesc(pixr+1); axis('image') 
H5.6 (Gaussian smoothing filter) (see p. 207) 
1. Function: 
function hg=moygauss(sigma) 
%!==============================! 
%! 
Gaussian filter 
%! SYNOPSIS : hg=MOYGAUSS(sigma) 
%! 
sigma = standard deviation 
%! 
hg 
= gaussian filter 
%!==============================! 
xx=-sigma*5:sigma*5; 
g=exp(-xx.*xx / (2*sigma*sigma)); % gaussian 
%===== keeping only significant values 
hg=g(g>max(g)*.005); hg=hg/sum(hg(:)); 
hg=hg' * hg; 
return 
2. applying it to the test image (Figure H5.4): 
II 
%===== tstmoygauss. m 
load lena; subplot(222) ; imagesc(pixc+1); 

444 Digital Signal and Image Processing using MATLAB® 
colormap(cmap); axis('image') 
set(gca, 'Xcolor' ,[0 0 0] , 'Ycolor' ,[0 0 0]) 
%===== Gaussian filter with sigma=4 
h=moygauss(4); subplot(221); imagesc(h); 
axis('image'); set(gca, 'Xcolor' ,[0 0 0], 'Ycolor', [0 0 0]) 
pixr=round(filter2(h,pixc)); 
subplot(223); colormap(cmap); imagesc(pixr); axis('image') 
set(gca, 'Xcolor' ,[0 0 0] ,'Ycolor' ,[0 0 0]) 
%===== Gaussian filter with sigma=2 
h=moygauss(2); pixr=round(filter2(h,pixc)); 
subplot(224); colormap(cmap); imagesc(pixr); axis('image') 
set(gca, 'Xcolor' ,[0 0 0] ,'Ycolor' ,[0 0 0]) 
set(gcf, 'Color' ,[1 1 1]) 
Figure H5.4 - Effect of the Gaussian smoothing filter for two values of (J 
H5.7 (Sobel derivative filter) (see p. 209) 
1. The Sobel filter: 
%===== lenasobel .m 
load lena 
set(gcf, 'color' ,[1 1 1]) 
Spix=size(pixc); Nl=Spix(l); Nc=Spix(2); 
colormap(cmap); NbLevel=size(cmap,l) ; 

50 
100 
150 
Chapter 10 - Hints and Solutions 445 
subplot(131); imagesc(pixc); axis('image') 
%===== Sobel filter 
hx=[1 0 -1;2 0 -2;1 0 -1]/4; hy=hx'; 
%===== x-filtering 
pixrx=filter2(hx,pixc); 
subplot(132); imagesc(pixrx); axis('image') 
%===== y-filtering 
pixry=filter2(hy,pixc); 
subplot(133); imagesc(pixry); axis('image') 
50 
50 }.c'/!P:: 
- .~:1'~ 
100 
100 
.; .......... . 
.--
.-
L----'!!"'" 
150 
150 \ -,. 
50 
100 
150 
50 
100 
150 
50 
100 
150 
Figure H5.5 - Differentiation with respect to the x and y axes 
2. second derivative filter (Figure H5.6): 
%===== lenadersec.m 
load lena 
subplot(121) ; imagesc(pixc); 
colormap(cmap); axis('image') 
%===== second derivative filter 
h=[O 1 0;1 -4 1;0 1 0]; 
pixr=(round(filter2(h,pixc))); 
subplot(122); imagesc(pixr); axis('image'); 
Figure H5.6 - Second derivative 
3. derivative filter design (Figure H5.7): 

446 Digital Signal and Image Processing using MATLAB® 
%===== derivsynth.m 
load lena; 
set(gcf, 'Color' ,[1 1 1]) 
subplot(231); imagesc(pixc); axis('image'); 
colormap(cmap); 
set(gca, 'Xcolor' ,[0 0 0] , 'Ycolor' ,[0 0 0] , . . . 
'XTick', [], 'YTick', [J) 
%===== derivative filter 
N=5; hx=cos (pi* (1: N)) . / (1: N); hx= [-fliplr (hx) 0 hx]; 
hx=hx . * (.54 -
.46*cos(2*pi*(0 :2*N)/(2*N))) ; 
hdp=hx' * hx; 
% derivative along Ox and Oy 
pixr=filter2(hdp,pixc); 
%===== 
subplot(232) ; imagesc(pixr); axis('image'); 
set(gca, 'Xcolor' ,[0 0 0] , 'Ycolor' ,[0 0 0] , . . . 
'XTick', [], 'YTick', [J) 
subplot(235); image(pixr); axis('image'); 
set(gca, 'Xcolor' ,[0 0 0] , 'Ycolor' ,[0 0 0] , . . . 
'XTick' , [], 'YTick', [J) 
%===== second derivative filter 
N=3; hx=2*cos(pi*(1:N)) ./ (1:N) ./ (1:N); 
hx=[fliplr(hx) pi*pi/3 hx] ; 
hx=hx . * (.54 -
.46*cos(2*pi*(0 :2*N)/(2*N))) ; 
hds=hx' * hx ; 
% derivation along both axes 
pixrd=filter2(hds,pixc) ; 
%===== 
subplot(233) ; imagesc(pixrd) ; axis('image') ; 
set(gca, 'Xcolor' ,[0 0 0] , 'Ycolor' ,[0 0 0] , .. . 
'XTick', [], 'YTick', [J) 
subplot(236); image(pixrd); axis('image'); 
set(gca, 'Xcolor' ,[0 0 0] , 'Ycolor' ,[0 0 0] , . . . 
'XTick', [], 'YTick', [J) 
Figure H5.7 - Derivative filter design 

Chapter 10 - Hints and Solutions 447 
H5.8 (Gaussian derivative-smoothing filter) (see p. 213) 
1. Type: 
function hd=dermoygauss(n1,n2,sigma1,sigma2,theta) 
%!=======================================================! 
%! Gaussian derivative-smoothing filter 
%! SYNOPSIS: hd = DERMOYGAUSS(n1,n2,sigma1,sigma2,theta) 
%! 
n1,n2 
filter dimensions 
%! 
sigma1 
dtandard deviation in the x direction 
%! 
sigma2 
standard deviation in the y direction 
%! 
theta 
rotation angle 
%! 
hd 
filter PSF 
%!=======================================================! 
den1=sigma1*sqrt(2*pi); den2=sigma2-3*sqrt(2*pi); 
s12=2*sigmal-2; s22=2*sigma2-2; 
ct=cos(theta); st=sin(theta); 
m1=(n1+1)/2; m2=(n2+1)/2; hd=zeros(n1,n2); 
for k=1:n1 
end 
xc=k-m1 ; 
for ell=1:n2 
end 
yc=ell-m2; u=xc*ct-yc*st; 
v=xc*st+yc*ct; 
h1=exp(-u- 2/s12)/den1; h2=-v*exp(-v- 2/s22)/den2; 
hd(k,ell)=h1*h2; 
hd=hd / sqrt(sum(sum(abs(hd).*abs(hd)))); 
return 
2. applying the filter to the test image (Figure H5.8): 
%===== tstdermoygauss .m 
clear; load lena; colormap(cmap); 
subplot(223); imagesc(pixc); axis('image') 
set(gca, 'Xcolor' ,[0 0 O] , 'Ycolor', [0 0 0], 'XTick', . . . 
[] , , YTick' , [] ) 
%===== gaussian derivative filter along y 
hd=dermoygauss(20,20,3,2 ,pi/4) ; 
subplot(221); mesh(hd); view(-30,20); 
set(gca, 'Xcolor' ,[0 0 O] , 'Ycolor', [0 0 0]) 
%===== 
subplot(222); imagesc(l-hd); axis('image') 
set(gca, 'Xcolor' ,[0 0 O],'Ycolor', [0 0 0], 'XTick', . . . 
[] , , YTick' , [] ) 
%===== filtering 
pixr=round(filter2(hd ,pixc)) ; 
subplot(224); imagesc(pixr); axis('image') 
set(gca, 'Xcolor' ,[0 0 O] , 'Ycolor', [0 0 0], 'XTick', . . . 
[] , , YTick' , [] ) 
set(gcf, 'Color' ,[1 1 1]) 

448 Digital Signal and Image Processing using MATLAB® 
20 
10 
20 
/ 
-;/ 
, 
,/' 
) 
:/ / / 
( 
~ / 
I 
' 
Figure H5.8 - Applying the filter to the Gaussian derivative-smoothing filter 
H5.9 (Contours using Sobel filtering) (see p. 218) 
%===== contoursobel.m 
load lena; 
subplot(121); colormap(cmap); NbLevel=size(cmap,1); 
imagesc(pixc); axis('image') 
%===== Sobel filter 
hx=[1 0 -1 ;2 0 -2;1 0 -1]/4; hy=hx'; 
%===== x- and y-filtering 
pixrx=filter2(hx,pixc); pixry=filter2(hy,pixc); 
%===== processing the contours 
pixr=sqrt(pixrx.*pixrx+pixry.*pixry); 
spx=size(pixr); pixres=ones(spx); 
threshold=25; [idl]=find(pixr>threshold); 
pixres(idl)=NbLevel*ones(size(idl)); 
subplot(122); imagesc(pixres); axis('image') 
H5.10 (Median filtering) (see p. 222) 
Type: 
II 
function imageO=median2D (imageI ,M, N) 
%!============================================! 

Chapter 10 - Hints and Solutions 449 
%! Median filter 
%! SYNOPSIS: imageO=MEDIAN2D(imageI,M,N) 
%! 
%! 
%! 
%! 
image I 
M 
N 
imageD 
image to be filtered 
x dimension (odd) of the window 
y dimension (odd) of the window 
filtered image 
%!============================================! 
Mo2=(M-l)/2; No2=(N-l)/2; 
dims=size(imageI); imageD = zeros(dims); 
Aw=zeros(l,M*N); 
for k=(Mo2+1):(dims(1)-Mo2), 
for 1=(No2+1) : (dims(2)-No2), 
pixbloc=imageI(k-Mo2:k+Mo2,1-No2:l+No2); 
Aw ( : ) =pixbloc; 
end 
end 
return 
imageO(k,l)=median(Aw); % median value 
The following program uses the median2D function as well as the 
moygauss. m function (created in exercise 5.6) on the "snowy" test image. 
Notice in Figure H5.9 that the median filtering is better at preserving the 
contours and eliminating the snow than the Gaussian-smoothing filtering: 
%===== lenamedian.m 
load lena; dims=size(pixc); 
subplot(221); imagesc(pixc); 
colormap(cmap); axis('image') 
%===== noise (snow) 
snow=(randn(dims»-2); pixcsnow=pixc .* snow; 
%===== median filtering 
M=3; N=3; 
pixfilmedp = median2Dp(pixcsnow,M,N); 
%===== gaussian filter 
hd = moygauss (2) ; 
% Try 2 then 4 
pixfilgauss=filter2(hd,pixcsnow); 
subplot(222); imagesc(pixcsnow); axis('image') 
subplot(223); imagesc(pixfilmedp); axis('image') 
subplot(224); imagesc(pixfilgauss); axis('image') 
H5.11 (Processing the result of a rotation) (see p. 222) 
1. Type: 
%===== geomproc.m 
% Processing of the rotated image 
pixc=imread('imageGGR.bmp', 'bmp') ; 
imageS = median2D(pixc,3 ,3); 

450 Digital Signal and Image Processing using MATLAB® 
Figure H5.9 - Suppression of the snow by gaussian-smoothing filtering (image on 
the left) and by median filtering (image on the right) 
tbcolor=[O :1/255:1] '*[1 1 1] ; 
% gray colormap 
imagesc(imageS); colormap(tbcolor) 
set(gca, 'DataAspectRatio', [1 1 1]); 
imwrite(imageS,tbcolor, 'imageGGRn .bmp', 'bmp') 
2. the mean is calculated over the four adjacent pixels (left, right, above 
and below) . Type: 
%===== geomproc2 .m 
% Processing of the rotated image 
%===== transformed image , original rotation angle 
% and dimension 
load pixcR2; % image, angle and dims of the original image 
spix=size(pixcR2); nl=spix(1); nc=spix(2); 
imagesc(pixcR2); colormap(tbcolor) 
set(gca, 'DataAspectRatio', [1 1 1]); 
tbx=ones(nl,1)*[1:nc]; tby=[1:nl] '*ones(1,nc); 
numR=reshape(tby,1,nl*nc); 
numC=reshape(tbx,1,nl*nc); 
%===== testing the pixels of the result 
nbool=tstinrect(numR,numC,thet,NI,Nc,nl ,nc) ; 
idxrect=find(nbool); Lir=length(idxrect); 
idxm1=find(pixcR2(idxrect)==-1) ; Lim1=length(idxm1); 
pp=pixcR2; 
%===== processing 
for k=1:Lim1 
nn=O ; sp=O; ptc=idxrect(idxm1(k»; 
kkl=ptc-nl; kkr=ptc+nl; 
if kkl>=1 & pp(kkl)--1, sp=sp+pp(kkl); nn=nn+1; end 
if kkr<=Lir & pp(kkr) - -1, sp=sp+pp(kkr); nn=nn+1; end 
kku=ptc-1; kkd=ptc+1; 

Chapter 10 - Hints and Solutions 451 
end 
if mod(kku,nl)-=1 & pp(kku)--1, 
sp=sp+pp(kku); nn=nn+1; 
end 
if mod(kkd,nl)-=O & pp(kkd)--1, 
sp=sp+pp(kkd); nn=nn+1; 
end 
pp(idxrect(idxm1(k)))=sp/nn; 
figure(3); imagesc(pp); colormap(tbcolor); 
set(gca, 'DataAspectRatio', [1 1 1]); 
The previous program uses a function (tstinrect) that makes it possible 
to detect the points of the resulting rectangle that belong to the original 
rectangle. Notice that we can end up with more points than in the original 
rectangle (Figure H5.10). The overlapping of the processed pixels is not 
carried out: 
Original 
image 
Result for a 
45° rotation 
m~ 
Result for a 
30° rotation 
Figure H5.10 - Effects of the rotation: the 'x's indicate the pixels of the resulting 
rectangle that were found to have belonged to the original rectangle subjected to the 
rotation 
function nbool=tstinrect(nR,nC,thet,NI,Nc,nl,nc) 
%!===================================================! 
%! SYNOPSIS: nbool=TSTINRECT(nR,nC,thet,NI,Nc,nl,nc) 
%! 
nR,nC 
%! 
%! 
thet 
%! 
NI,Nc 
%! 
points to be tested (vectors of the row 
and column numbers) 
rotation angle 
number of rows and columns of the 
original image 
%! 
nl,nc 
number of rows and columns of the 
%! 
tranf ormed image 
%! 
nbool 
result of the test 
%!===================================================! 
nx=lengthCnR); nR=reshape(nR,1,nx); 
nC=reshape(nC,1,nx); 
xor=(1+nc)/2; yor=(1+nl)/2; 
Xor=(1+Nc)/2 ; Yor=(1+NI)/2; 

452 Digital Signal and Image Processing using MATLAB® 
MRotM=[cos(thet) sin(thet);-sin(thet) cos(thet)); 
V=[nC-xor;yor-nR) ; 
VmR=MRotM*V; 
Cl=(VmR(1,:) < 
C2= (VmR(2, :) < 
nbool=Cl & C2; 
% reverse rotation 
Xor-0.5) & (VmR(l,:) > -Xor+0.5); 
Yor-0 .5) & (VmR(2,:) > -Yor+0.5); 
return 
Figure H5.11 - Result of the mean calculation operation on the transformed image 
H5.12 (Application of the Otsu method) (see p. 226) 
1. Threshold calculation function: 
function [threshold,Hs)=otsu(pixc) 
%!======================================================! 
%! Binarization with the Otsu method 
! 
%! SYNOPSIS: [threshold,Hs)=OTSU(pixc) 
%! 
pixc 
image with 256 gray levels 
%! 
threshold = calculated optimal threshold 
%! 
Hs 
= criterium to be maximized for s=0:255 
%!======================================================! 
%===== histogram of the image 
[nlig ncol)=size(pixc) ; 
Lhist=256; histog=zeros(Lhist,l); 
for k=l:Lhist 
histog(k)=length(find(pixc==k-l)); 
end 
histog=histog/nlig/ncol; 
%===== calculating the criterium 
Pinf=O; Psup=l; sinf=O; muinf=O ; 
ssup=(O :Lhist-l)*histog ; 
%===== calculating the criterium for the values of S 

Chapter 10 - Hints and Solutions 453 
% Hs = Pinf*Psup*(muinf-musup)*(muinf-musup) 
Hs=zeros(l,Lhist); 
for S=O:Lhist-2 
%===== distributions 
Pinf=Pinf+histog(S+l); Psup=l-Pinf; 
%===== local means 
sinf=sinf+S*histog(S+l); ssup=ssup-S*histog(S+l); 
muinf=sinf/Pinf; musup=ssup/Psup; 
Hs(S+l)=Pinf*Psup*(muinf-musup)*(muinf-musup); 
end 
threshold=find(Hs==max(Hs)); % threshold 
return 
2. use of the previous function (Figure H5.12): 
%===== binarOtsu .m 
load('elido2.mat'); 
nlig=size(pixc,l); ncol=size(pixc,2); 
figure(l); colormap(cmap); 
subplot(121) ; imagesc(pixc); axis('image') 
%===== 
thresholdOtsu=otsu(pixc); 
pixc2=zeros(nlig,ncol); pixc2=255*(pixc>thresholdOtsu) ; 
subplot(122) ; imagesc(pixc2); 
axis('image'); colormap(cmap) 
}:
I} ~!()g!~~
_ ~ ____________ ; ____ <;~I<::~J~!~st 
_ 
~~~<:~~()!st 
_ (f!1
_<:~~2 _ _ ____ :_ 
I 
I 
I 
I 
I 
128 : 
o 
50 
100 
150 
200 
250 
Variance cI~viation 
____ : ___ CaJculatl?d threshold (Otsu) __ 
____________ L ____ _
______ ~ _____ _ 
_____ J __________
_ 
~ ____________ ~ _ 
, 
, 
, 
, 
_ _ _ _ _ _ _ L ___ _________ J. _____ _ 
_____ J ____________ ~ ______ _ 
, 
, 
132 : 
' 
o 
50 
100 
150 
200 
250 
Figure H5 .12 - Results of the threshold calculation 

454 Digital Signal and Image Processing using MATLAB® 
H5.13 (Local contrast modifications) (see p. 231) 
%===== munsharpwl.m 
clear 
pixc=imread('nenupharg.jpg'); pixc=double(pixc); 
%===== 
[nr,nc]=size(pixc); mrect=[30,30,nc,nr]; 
figure(1); image(pixc), axis('image') 
set(gca, 'units', 'pix', 'position' ,mrect) 
mmap=(O:1/255:1) '*[1 1 1]; colormap(mmap) 
%===== gaussian filter 
sigma=2; hg=moygauss(sigma); 
pixcf=filter2(hg,pixc); 
%===== 
pixcc=2*pixc-pixcf; 
vM=max(max(pixcc)); vm=min(min(pixcc)); 
pixcc=1+(pixcc-vm)*255/(vM-vm); 
figure(2); image(pixcc), axis('image') 
set(gca, 'units', 'pix', 'position' ,mrect) 
colormap(mmap) 
%===== 
px=zeros(1,nr*nc); px(:)=pixcc; medpx=median(px); 
nh=64; ml=.04; [hmin,hmax]=hlimits(pixcc,nh,ml); 
pixa=hadaptmod(pixcc,medpx,hmin,hmax); 
figure(3); image(pixa), axis('image') 
set(gca, 'units', 'pix', 'position' ,[30,30,nc,nr]) 
colormap(mmap) 
% for saving 
% pixa=uint8(pixa); imwrite(pixa, 'nenupharusa.png', 'png'); 
H5.14 (Writing basic functions) (see p. 236) 
1. The DCT given by expression 5.38 can be written: 
F(u, v) 
7 
7 
1 C( )C( )"""" ( 
) 
(2x + 1 )7rU 
(2y + 1 )7rV 
4" 
U 
v L.J L.Jp x, Y cos 
16 
cos 
16 
x =Oy=o 
7 
7 
1c( )C()"" 
(2x+1)7ru"" ( 
) 
(2y+1)7rv 
4" 
U 
v L.J cos 
16 
L.Jp x, Y cos 
16 
x=O 
y=O 
with C(O) = ~ and C(k) = 1 for k = 1 ... 7. 
The second sum corresponds to the matrix product: 
(2y + l)7rv 
q(x,v) = [P(x, y)] x [ct(y, v)] with ct(y,v) = cos 
16 

Chapter 10 - Hints and Solutions 455 
Figure H5.13 - Result of the local enhancement 
The first sum corresponds to: 
(2x + l )7ru 
P(u,v) = [cs(u, x)] x [q(x, v)] with cs(u,x) = cos 
16 
The matrix N = [C(u)C(v)]/4 is a weighting matrix, and we have: 
where" . *" denotes term-by-term multiplication: 
function coeff=DCTp(pixc) 
%!====================================================! 
%! Calculating the DCT coefficients of an (8*8) block 
%! SYNOPSIS : coeff=DCTP(pixc) 
%! 
pixc 
= (8*8) block 
%! 
coeff = DCT coefficients (8*8) 
! 
%!====================================================! 
%! 
Uses the global variables mNORM , mY V , mUX: 
%! 
mNORM = [1/2 ones (1,7) /sqrt (2); . . . 
%! 
ones(7 ,1)/sqrt(2) ones(7 ,7)]/4; 

456 Digital Signal and Image Processing using MATLAB® 
%! 
mY V 
= cos((2*[0:7] '+1)*[0:7]*pi/16); 
%! 
mUX 
= cos([0:7] '*(2*[0:7]+1)*pi/16); 
%!====================================================! 
global mNORM mY V mUX 
coeff = mNORM . * 
(mUX * pixc * mYV); 
return 
2. quantization function: 
3. 
function matq=quant(dctcof, Qtab) 
%!==================================================! 
%! Quantization and rounding of the DCT coefficient 
%! SYNOPSIS: matq=QUANT(dctcof, Qtab) 
%! 
dctcof = DCT coefficients 
%! 
Qtab 
= weighting matrix 
%!==================================================! 
matq = round(dctcof ./ Qtab); 
return 
verification: 
%===== corrig1.m 
clear 
global mNORM mY V mUX 
[Qtab, zig, zag]=initctes; 
pix=[139 144 149 153 155 155 155 155; 
144 151 153 156 159 156 156 156; 
150 155 160 163 158 156 156 156; 
159 161 162 160 160 159 159 159; 
159 160 161 162 162 155 155 155; 
161 161 161 161 160 157 157 157; 
162 162 161 163 162 157 157 157; 
162 162 161 161 163 158 158 158] ; 
coeff=DCTp(pix-128); coeffQ=quant(coeff, Qtab) ; 
valmoy=O; 
tramec=code1 (coeffQ ,valmoy ,zag) 
where the codel.m function given below creates a frame without any 
entropic coding: 
function [framec,valm]=code1(coeffQ,meanval,zig) 
%!=======================================================! 
%! Calculating a frame without entropic coding 
! 
%! SYNOPSIS: [framec ,valm] =CODE1 (coeffQ ,meanval ,zig) 
%! 
coeffQ 
quantized DCT coefficients 
%! 
meanval 
mean value of the previous block 
%! 
zig 
zigzag indices for reading 
%! 
framec 
coded frame 
(bits numbers are set to 0) 

Chapter 10 - Hints and Solutions 457 
%! 
valm 
= mean used for the next block's coding 
%!=======================================================! 
bb=O; 
if (coeffQ(1,1)==0), coeffQ(1,1)=eps; bb=1; end 
frame=coeffQ(zig); 
% coeffQ read in zigzag 
%===== indices of the non-zero terms 
idz=find(frame); lidz=length(idz) ; 
nbzer=diff(idz)-1; lnbz=length(nbzer); 
valm=coeffQ(1,1); framec=[17 valm-meanval]; 
for ik=1:lnbz 
framec=[framec nbzer(ik) 17 frame(idz(ik+1»] ; 
end 
framec=[framec 0 0]; 
if bb, framec(2)=-meanval; end 
return 
H5.15 (Writing the compressed frame) (see p. 237) 
1. Type: 
%===== unbloccode.m 
clear 
global mNORM mY V mUX 
[Qtab, zig, zag]=initctes; 
%===== the test image 
pix=[139 144 149 153 155 155 155 155; 
144 151 153 156 159 156 156 156; 
150 155 160 163 15S 156 156 156; 
159 161 162 160 160 159 159 159; 
159 160 161 162 162 155 155 155; 
161 161 161 161 160 157 157 157; 
162 162 161 163 162 157 157 157; 
162 162 161 161 163 15S 15S 15S]; 
pixc=[pix pix pix ones(S,S)*255]; [nl,nc]=size(pixc); 
%===== analysis and coding of the block 
nbx=floor(nc/S); nby=floor(nl/S); 
fid=fopen('unbloccode.dat', 'w'); 
fwrite(fid,nby, 'intS'); fwrite(fid,nbx, 'intS'); 
nbbits=O; meanval=O; 
for indy=1:nby 
for indx=1 :nbx 
end 
idx=indx*S; idy=indy*S; 
pix=pixc(idy-7:idy,idx-7 :idx); 
cofpix=DCTp(pix-12S); coeffQ=quant(cofpix, Qtab); 
[tramec,valm]=code1(coeffQ,meanval,zag); 
fwrite(fid,tramec, 'intS'); 
meanval=valm; 
end 
fclose(fid); 

458 Digital Signal and Image Processing using MATLAB® 
2. type: 
%===== imgtstcode.m 
clear 
global mNORM mY V mUX 
[Qtab, zig, zag]=initctes; 
load wendyg 
figure(1); imagesc(pixc); colormap(cmap); axis('image') 
[nl,nc]=size(pixc); 
%===== analysis and coding of each block 
nbx=floor(nc/S) ; nby=floor(nl/S); 
fid=fopen('imgtstcode.mat', 'w'); 
fwrite(fid,nby, 'intS'); 
fwrite(fid,nbx, 'intS'); 
nbbits=O; meanval=O; 
for indy=1:nby 
idy=indy*S; 
for indx=1 :nbx 
% coefficients 
% header, number of rows 
% number of columns 
idx=indx*S; pix=pixc(idy-7:idy,idx-7:idx); 
cofpix=DCTp(pix-12S); coeffQ=quant(cofpix, Qtab); 
[framec,valm]=code1(coeffQ,meanval,zig); 
fwrite(fid,framec, 'intS'); 
% Writing the frame 
meanval=valm; 
end 
end 
fclose(fid); 
return 
H5.16 (Decompression) (see p. 238) 
1. Type: 
2. type: 
function dctcof=unquant(matq, Qtab) 
%!========================================! 
%! Dequantization of the DCT coefficients 
%! SYNOPSIS: dctcof=UNQUANT(matq, Qtab) 
%! 
Qtab = quantization matrix 
%! 
matq = quantized DCT coefficients 
%!========================================! 
dctcof = matq .* Qtab; 
return 
function pixels=iDCTp(Pdct) 
%!============================================! 
%! Calculating the inverse DCT 
%! SYNOPSIS: pixels=IDCTP(Pdct) 

Chapter 10 - Hints and Solutions 459 
%! 
Pdct 
= DCT coefficients (S*S) 
%! 
pixels = (S*S) block 
%!============================================! 
%! Uses the global variables mNORM, mY V , mUX. ! 
%!============================================! 
global mNORM mY V mUX 
pixels = mY V * (mNORM .* Pdct) * mUX; 
return 
3. test sequence obtained for the decoding: 
framec 
17 
15 
1 
17 
-2 
0 
17 
-1 
0 
17 
-1 
0 
17 
-1 
2 
17 
-1 
0 
17 
-1 
0 
0 
valm = 
15 
framec 
17 
0 
1 
17 
-2 
0 
17 
-1 
0 
17 
-1 
0 
17 
-1 
2 
17 
-1 
0 
17 
-1 
0 
0 
valm = 
15 
framec 
17 
0 
1 
17 
-2 
0 
17 
-1 
0 
17 
-1 
0 
17 
-1 
2 
17 
-1 
0 
17 
-1 
0 
0 
valm = 
15 
framec 
17 
49 
0 
0 
valm = 
64 
4. type: 
%===== imgtstdecode .m 
global mNORM mY V mUX 
[Qtab, zig, zag]=initctes; 
%===== file containing the codes 
fid=fopen('imgtstcode.mat', 'r'); 
nby=fread(fid,1, 'intS'); 
% number of row blocks 
nbx=fread(fid,1, 'intS'); 
% number of column blocks 
framec=fread(fid, 'intS'); 
% data 
fclose(fid); 
pixcr=zeros(S*nby,S*nbx); pixO=O; idx=1; 
%===== for each block . .. 
for indy=1 :nby 
idS=indy*S; 
for indx=1 :nbx 
coeffQ=zeros(1,64); kz=1; 

460 Digital Signal and Image Processing using MATLAB® 
end 
end 
idx=idx+1; a=framec(idx); 
a=a+pixO; coeffQ(zig(kz))=a; pixO=a; 
idx=idx+1; kz=kz+1; 
%===== reading a frame associated to a block 
while (framec(idx)-=O I framec(idx+1)-=0), 
end 
if framec(idx)-=O, kz=kz+framec(idx); end 
coeffQ(zig(kz))=framec(idx+2) ; 
kz=kz+1; idx=idx+3 ; 
idx=idx+2 ; 
coeffQ=reshape(coeffQ,S, S); 
coeff=unquant(coeffQ, Qtab); 
pix=fix(iDCTp(coeff))+12S ; 
pixcr(idS-7 :idS,indx*S-7 :indx*S)=pix ; 
%===== comparison of the images 
load wendyg 
subplot(121) ; imagesc(pixc') ; axis('image') ; colormap(cmap) ; 
set(gca, 'Xcolor' ,[0 0 0] , 'Ycolor' ,[0 0 0]) 
subplot(122); imagesc(pixcr'); axis('image'); colormap(cmap); 
set(gca, 'Xcolor' ,[0 0 0] , 'Ycolor' ,[0 0 0]) 
set(gcf, 'Color' ,[1 1 1]) 
return 
H6 Random variables 
H6.1 (Confidence ellipse ) (see p. 261) 
1. By using the indicated variable change Y = C - 1/ 2 (X - m ), then by 
changing over to the polar coordinates, we successively get: 
hence s = -2 log(1 - a); 
2. type: 
II %===== cellconf . m 

Chapter 10 - Hints and Solutions 461 
N=200; alpha=0.95; 
s=-2*log(1-alpha); Nth=fix((l-alpha)*N); 
C=[2.3659 -0.3787;-0.3787 0.6427]; 
y=randn(2,N); x=sqrtm(C)*y; 
ellipse([O 0] ,inv(C),s); hold; 
plot(x(1,:),x(2,:), '+'); hold; axis([-6 6 -6 6]); 
normy = ones(1,2) * (y .* y); nb=length(find(normy>s»; 
text(2,4,sprintf('Nth=%g 
N=%g' ,Nth,nb» 
H6.2 (Poisson distribution) (see p. 264) 
1. The mean is given by: 
+00 
E{X} = L
e-akak jk! = a 
k=O 
and the variance by var(X) = E {X2} - E2{X}. Because we have: 
then E {X2} = E {X(X - I)} + E {X} = a2 + a. Therefore, var(X) = a; 
2. Fx(k) = Pr(X ::; k) can be written Fx(k) = Fx(k - 1) + Px(k) and 
to avoid calculating the factorial function, the following recursive form is 
used for calculating p x (k): 
px(k) = apx(k - l)jk 
3. in order to generate a Poisson variable with the parameter a, we can use 
the following method: 
(a) generate the sequence U by typing U = randCi ,N). The maximum 
value is denoted by Umax , 
(b) construct, as a function of a, the table of the cumulative probabilities 
Fx(k) = Pr(X ::; k) smaller than Umax , 
(c) determine, for each U(k), the highest integer X(k) such that 
Fx(X(k)) ::; U(k). We can use find«X>=a)&(X<b)) which ex-
tracts the indices of the values of x between a and b. 
%===== cpoisson.m 
clear; a=5; N=2000; U=rand(l,N); X=zeros(l,N); 
Umax=max(U); p(l)=exp(-a); % P(X=O) 

462 Digital Signal and Image Processing using MATLAB® 
FX(1)=p(1); nmax=1; 
%===== cdf 
while ((FX(nmax)<Umax)&(nmax<N)) 
end 
p(nmax+1)=a * p(nmax) / nmax; % iterative calc. 
FX(nmax+1)=FX(nmax) + p(nmax+1); nmax=nmax+1; 
%===== generation of X 
for ii=1:nmax-1 
end 
ind=find((U>=FX(ii))&(U<FX(ii+1))); 
X(ind)=ii * ones(1,length(ind)); 
%===== extreme values 
ind=find(U<FX(1)); X(ind)=zeros(1,length(ind)); 
ind=find(U>=FX(nmax)); lind=length(ind); 
X(ind)=nmax * ones(1,length(ind)); 
%===== verification 
[pest xO]=hist(X,(O:nmax)); stem(xO,pest/N) 
hold; plot((O:nmax-1),p,(O:nmax-1),p, 'x'); hold 
H6.3 (Rayleigh distribution) (see p. 266) 
1. We get: 
l
x t 
(t2 ) 
( X2 ) 
U = 
-
exp - -
dt = 1 - exp --
o cy2 
2cy2 
2cy2 
If we solve for x, we get x = cyJ -2Iog(1 - u). Hence, if U has a uniform 
distribution on (0,1), X = cyJ-2Iog(1- U) has a Rayleigh distribution. 
Because U and 1- U have the same distribution, it is equivalent to taking 
X = cyJ-2Iog(U) instead; 
2. type the program: 
%===== crayleigh.m 
clear; N=3000; 
U=rand(1,N); X=sqrt(-2*log(U)); 
lk=(max(X)-min(X))/20; maxx=max(X); 
pointsx=(O:lk:maxx); [nn xx]=hist(X,pointsx); 
%===== estimating the pdf 
pest=nn/(N*lk); bar(pointsx,pest) 
%===== theoretical Rayleigh pdf 
pth=pointsx .* exp(-pointsx .* pointsx /2); 
hold on; plot (pointsx,pth, 'or'); hold off; grid 
Figure H6.1 shows the theoretical graph for the probability density of a 
Rayleigh distribution and the histogram of the values obtained by the 
generator. 

Chapter 10 - Hints and Solutions 463 
0.7 ,-------,---------,-----,---------,---------,-------, 
0.6 ----------"--
---------" --------- " ----------
~
-
0.5 ---------- "-----
0.4 ---------- "--- 0 
0.3 ----------"--
, 
, 
, 
Q _---- ; -----------:-----------:-----------
, 
, 
, 
, 
, 
, 
o 
.--
~ ----------~-----------I-----_-----, 
, 
, 
, 
, 
, 
---------
~ -----------
I --
, 
, 
, 
, 
~:~ :::::::::::-
-9 -~ ---
:::
i ::::::::::
t
::::::::: 
o~ __ ~Ra
' ~I~~~~~~~r1A~I~IA
b: ~I,n~~~~~QA
: ____ ~ 
-1 
0 
2 
3 
4 
5 
Figure H6.1 - Rayleigh distribution: probability density (0) and histogram of the 
simulated length 3000 sample 
H6.4 (Bernoulli distribution) (see p. 266) 
1. E {Bn} = 1 x Pr(Bn = 1) + 0 x Pr(Bn = 0) = p. Likewise, E {B
~ } = P 
and therefore var(Bn) = p - p2 = p(l - p); 
2. because the random variables are assumed to be independent, they 
are necessarily uncorrelated, and we have, for k -I- n, E {BkBn} = 
E {Bk} E {Bn} =p2 ; 
3. because of linearity, E {S} = E {Bn} = p. In order to determine the 
variance, we first have to calculate the second order moment E { S2}: 
1 
N 
N 
N2 LL E {BnBd 
n = lk= l 
1 
N 
1 
N2 L
E {B~} 
+ N2 L
E{BnBd 
n = l 
n#k 
1 
N(N - 1) 2 
2 
p(l - p) 
N P+ 
N2 
p =p + 
N 
In the end, m = E {S} = p and (J"2 = var(S) = E {S2} - E2{S} = pq/N; 
4. we have successively E; = 4(J"2/m2 = 4(1 - p)/Np c:::' 4/Np if we assume 
that p« 1, meaning that N :::::: 4/(pE;). For Er = 0.1, we get N = 400/p; 
5. the inequalities 6.26 show that, in order to determine the random variable 
B that has a Bernoulli distribution with a parameter p, from a random 
variable U uniformly distributed on (0,1), we have to choose B = 0 if 
0::; U < (1 - p) and 1 otherwise. This result is obtained in the following 
program with the instruction B= (U>q) where q = 1 - p: 

464 Digital Signal and Image Processing using MATLAB® 
%===== cbernou.m 
p=O.1; q=1-p; 
er=.1; N=fix(q/(p*er*er)); 
U=rand(1 ,N); B=(U>q) ; 
mean (B) 
H6.5 (Signal-to-quantization noise ratio) (see p. 269) 
1. Since all the amplitudes are assumed to belong to ( - F a, + F a ), the quan-
tization step is given by q = 2F a / 2N . Because IE {c2 } = q2/12, we have: 
Thus, we get the following practical rule: the signal-to-noise ratio is in-
creased by 6 dB for every additional coding bit; 
2. type: 
function [rcval,code]=bincoding(x,N,A) 
%!=========================================! 
%! Two's complement binary coding 
%! SYNOPSIS: [rcval,code]=bincoding(x,N,A) 
%! 
x 
%! 
N 
%! 
A 
%! 
rcval 
%! 
code 
sequence to be coded 
2-N codes (default 4) 
conversion between -A and A 
reconstructed value 
two's complement binary code 
%! 
-2-(N-1)<=code<=2-(N-1)-1! 
%!=========================================! 
if nargin<3, A=1; end 
if nargin<2, N=4; end 
if nargin<1, error('Parameters missing . . . '); end 
q=A/2- (N-1); code=floor(x/q) ; 
%===== Clipping 
cm=-2- (N-1); idm=find(code<cm); code(idm)=cm*ones(size(idm)) ; 
cM=2-(N-1)-1; idM=find(code>cM); code(idM)=cM*ones(size(idM)); 
I 
rcval=code*q+q/2; 
return 
Notice the use of the floor function to directly calculate the reconsructed 
values yt corresponding to the values of the unclipped signal xt . Of 
course, some values of the signal xt are greater than Ac , and are therefore 
clipped. Their number must be negligible for the chosen value of F; 
3. the following program displays the histogram of the error signal and both 
the theoretical and the measured ratios (S/ B )dB plotted against N (Fig-
ure H6.2) . The peak value Ac = F,jP is obtained from an estimation P 
of the signal's power and from an a priori peak factor of 3.5: 

Chapter 10 - Hints and Solutions 465 
%===== cssurbq.m 
clear; T=2000; xt=randn(l,T); 
%===== RLF 
Fc==3.5; Cc=10*log10(3!(Fc*Fc»; 
%===== Ac from the estimated power 
px=xt*xt'!T; Ac=Fc*sqrt(px); 
%===== indices for which there is saturation 
indmax=find(xt>=Ac); lindmax=length(indmax); 
indmin=find(xt<=-Ac); lindmin=length(indmin); 
figure (1) ; 
%===== 
for Nb=1 :7 
end 
[yt,code]=bincoding(xt,Nb,Ac); ninter=2-Nb; 
q=2*Ac!ninter; 
%==== quantization step 
errq=xt-yt; 
%==== quantization error 
%===== histogram 
hist(errq,(-5:5)*q!5); grid; pause 
pe=errq*errq'!T; ssb(Nb)=10*log10(px!pe); 
%===== theoretical values 
ssbth(Nb)=6*Nb+Cc; 
figure(2); plot([1:7] ,ssbth,' :y' ,[1:7] ,ssb, 'Xy'); grid 
40 ,------------------------------------------, 
35 
----------0----------0----------
" ----------"-----------~--------
I 
I 
I 
I 
I 
I 
I 
I 
I 
30 
----------:----------:----------
" ----------~----------' 
----------
I 
I 
I 
I 
I 
I 
I 
I 
I 
25 
----------
~ ----------
~ ---------, ---------
~ -
--------:--
20 
15 
10 
5 
" 
, 
" 
, 
o ----------~----------~----------
~ ----------~----------
-i-----------
I 
I 
I 
I 
-5 L-___ 
~
' ---~
' ---~----
'~--~
'~-----' 
1 
2 
3 
4 
5 
6 
7 
Figure H6.2 - Both the theoretical and the measured "Signal-to-noise" ratios (SNR) 
of a uniform quantization plotted against the number of coding bits. The "x "s are 
obtained from a sample with the size 2, 000 
By examining the histograms, we can check the uniform distribution of 
the quantization error hypothesis. As you can also see, the power of the 
error is close to q2/12. 

466 Digital Signal and Image Processing using MATLAB® 
H7 Random processes 
H7.1 (Suppressing a n affine trend) (see p. 290) 
1. {B (n)} is a sequence of N Gaussian, centered random variables, uncor-
related, and is therefore independent. Hence, we have: 
Because B(n) = X(n) - al - a2n, the probability distribution of (X(O), 
... , X(N - 1)) has the density: 
2. maxlmlzmg px(xo, ... ,xN- l;al,a2,a;) with respect to al and a2 is 
equivalent to minimizing the sum found in the exponential, that is to 
say J(al, a2) = L;
~:ol(Xn 
- al - a2n)2. By differentiating J(al , a2) with 
respect to al and a2, and setting the derivatives to zero, we get the system 
of two equations: 
which can be written, with the suggested notations, W TWa = W T X. 
Because W TW is invertible, the solution is a = (W TW )-l W T X , 
which happens to be the solution for the least squares estimator (page 
353). This is not surprising since maximizing the probability density in 
the Gaussian case is equivalent to minimizing the square deviation; 
3. the following function eliminates the affine trend: 
function [A ,dx]=tendoff(x) 
%!====================================! 
%! Suppression of an affine trend 
! 
%! SYNOPSIS : [A,dx]=TENDOFF (x) 
%! 
x 
= input sequence 
%! 
A = affine regression coeffts 
%! 
dx = residual 
! 
%!====================================! 
x=x(:) ; N=length(x) ; w=[ones(N,l) (O:N-l ) ']; 
A=w\ x; dx=x- w*A; 
r eturn 

Chapter 10 - Hints and Solutions 467 
H7.2 (Smoothing filtering of noise) (see p. 298) 
1. Starting with the definition, we get: 
+00 
7 
1 . ( 
) 12 
H(J) = L h(n)e-2j7rjn = L ~ e-2j7rjn =? IH(J)12 = 
S1~ 87rf 
8 
8sm(7rf) 
n=-oo 
n=O 
2. because the input process is white with a variance of 1, Sxx(J) = 1 and 
therefore Syy(J) = IH(J)12; 
3. the output power can simply be written: 
1
1/2 
11/2 
7 
P = 
Syy(J)df = 
IH(JWdf = L Ih(nW = 1/8 
-1/2 
-1/2 
n=O 
4. the output autocovariance function is obtained as the inverse Fourier 
transform of Syy (J) . This is also the convolution of a rectangle of width 
Lh = 8 with itself, which results in the triangle with the support (- (Lh -
1), Lh - 1). Theoretically, Ryy(k) is therefore null for Ikl ;:: Lh; 
5. the following program generates a trajectory for the signal y(t) and esti-
mates 10 points of the auto covariance function. We have to check that 
the estimated function is negligible beyond Lh = 8: 
%===== cmeannoise.m 
T=2000 ; N=10 ; 
% number of covariance coeffts 
%===== generating the signal using a rectangular FIR 
Lh=8; h=ones(l,Lh)/Lh; w=randn(l ,T); x=filter(h,l,w); 
%===== estimation of the autocovariance 
xi=[x zeros(l,N-l)); 
D=toepl(xi,[xi(l) zeros(l,N-l))); R=D*xi'/T; 
%===== drawing the autocovariance 
tau=(-(N-l) : (N-l)) ; Rs=[R(N:-l:2) ; R); 
subplot(211) ; plot(tau,Rs, '0') ; grid 
%===== drawing the spectrum 
Lfft=512; freq=(O:Lfft-l)/Lfft; 
Sf=10*10g10(abs(fft(R ,Lfft))); 
subplot(212) ; plot(freq,Sf) ; grid 
Figure H7.1 shows the N = 10 point estimate of the auto covariance 
function. The triangular shape is visible for 0 :::; Ikl :::; 7. Beyond that, 
the values are almost equal to zero, as predicted by the theoretical results. 
We have to check that the estimated value in k = 0, which represents the 
power of Y(n), is in agreement with the theoretical value of 1/8. 

468 Digital Signal and Image Processing using MATLAB® 
0.12 
0.1 
0.08 
0.06 
0.04 
------r------T------,------,------
------ I------- r ------
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
------r------T------,------,--- ------ ---I------- r------
, 
, 
, 
, 
, 
, 
, 
, 
, 
------ r ------ T ------ 1
-
, 
, 
, 
-8 
-6 
-4 
-2 
o 
2 
4 
6 
Figure H7 .1 - Triangular autocovariance function 
8 
10 
-4 ---
--
T ------
+ ------~-------:-------:-------:-------~-----
-
i- ------
f
--
---
I 
I 
I 
I 
I 
I 
I 
I 
I 
-6 -----
~ ------
~ ------~------~-------i-------~------~------
~ ------
~ ------
I 
I 
I 
I 
I 
I 
I 
I 
- 8 ------ r-
----
T ------~------~-------i-------~------~------
r ----
-T------
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
-10 ------ i- ------ +--
---~------~-------:-------~------~---, 
-- t------ t------
I 
I 
I 
I 
I 
, 
, 
, 
, 
, 
, 
-12 
~ ---- -,- ---- :-
, 
, 
, 
, 
-14L------------------
' --------~
' 
~--------------~ 
o 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.8 
0.9 
Figure H7.2 - Spectrum estimate 
The spectrum, calculated from the FFT estimation of the auto covariance 
is represented in Figures H7.2 for f E (0,1/2). It corresponds to the 
theoretical spectrum IH(fW. The width of its main lobe is equal to 
0.25. 
H7.3 (Generating a band limited process) (see p. 300) 
To shift the frequencies back to the (-1/2,+1/2) band, we have to divide the 
frequency scale expressed in Hz by the sampling frequency Fs = 10,000 Hz. 
Hence the spectrum band for which the process is different from zero is the 
(-0.1, +0.1) band. Its psd is shown in Figure H7.3. Because the power is equal 
to the integral of the psd, P = 0.20: and therefore 0: = 10. 
The following method can be used to obtain a trajectory: 
- with the use of the randn function, construct a random sequence the 
spectral density of which is equal to 1 in the (-1/2,+1/2) band; 
- multiply this sequence by VIO, to obtain a power equal to 10; 

Chapter 10 - Hints and Solutions 469 
r--4---, a 
----~---+--~---+---+--~--~~f 
o 
0.1 
(1,000 Hz) 
0.5 
(5,000 Hz) 
Figure H7.3 - Power spectral density of the low-pass process 
- apply a filter with a gain 1 to the (-0.1, +0.1) band. To generate the 
coefficients of this filter, use the rif function (page 426). 
Type: 
%===== cpablim.m 
Fs=10000; Fc=1000; fcr=Fc/Fs; T=1000; 
w=sqrt(10)*randn(1,T); B=rif(50,fcr); x=filter(B,l,w); 
subplot(211); plot(w); grid; 
subplot(212); plot(x); grid 
The trajectory of x is less "chaotic" than that of w. This phenomenon is 
analogous to the one observed for deterministic signals: reducing the frequency 
band leads to slower time fluctuations. 
H7.4 (Pre-emphasis and de-emphasis) (see p. 300) 
1. If we use the filtering formula, we get the psd of the output signal W(n) 
of the filter with B(n) as its input. This leads to the expression of the 
power: 
Because these two cascaded filters have no effect on the signal X(n) 
(Figure H7.4), the power of the output signal's useful part is given by 
J
+ 1/2 
-1/2 Sx(f)df· 
tB(n) 
X(~I 
HpU) h~'-I 
H-d-U-)-=-l-/H-p-U--') ~n) 
+ W(n) 
Figure H7.4 - The pre-emphasis and de-emphasis system 

470 Digital Signal and Image Processing using MATLAB® 
Hence the signal-to-noise ratio has the expression: 
2. using the filtering formula, we get the psd of the output signal of the filter 
Hp(f). This leads us to the expression of the power: 
A system without pre-emphasis/de-emphasis amounts to the addition of 
noise, with the power J~11
/22 SB(f)dj, to the useful signal. For an accurate 
comparison of the systems with and without pre-emphasis, we have to 
consider that the power available to the system without pre-emphasis is 
equal to Po. Therefore, the signal-to-noise ratio has the expression: 
3. using g = PPD/ p: 
4. using the Schwarz inequality, we get a lower-bound for the denominator, 
then the fact that Hp(f)Hd(f) = 1 leads us to: 
(10.4) 
The two sides are equal if and only if IHp(f)1 2Sx(f) is proportional to 
IHd(f)1 2SB(f). Finally, we infer that the maximum of the gain g is 
obtained for: 
SB(f) 
Sx(f) 

Chapter 10 - Hints and Solutions 471 
The corresponding gain is given by the right-hand side of equation 10.4 
which is always greater than 1 according to the Schwarz inequality. As a 
conclusion, we started out with the expressions of the spectra and ended 
up with improved performances. 
In many cases, the spectra of the signal and of the noise are such that a 
low-pass filter provides a good approximation of the de-emphasis filter. 
Therefore, a high-pass filter is used on the pre-emphasis side. To get 
a better idea, imagine that the treble level is "raised", hence the name 
pre-emphasis. 
Frequency modulation radio transmissions use this technique. It is also 
found in speech coders. In that case, the pre-emphasis filter used is very 
simple: it is a high-pass FIR filter with the two coefficient impulse re-
sponse h = [1 
-0.9375]. 
H7.5 (Estimation of an FIR filter's impulse response) (see p. 301) 
1. We saw in expression 7.46 that Ryx(k) = h(k)*Rxx(k). Because {h(k)} 
has a finite length L: 
Ryx(k) = h(O)Rxx(k) + ... + h(L - l)Rxx(k - L + 1) 
2. if we stack the L equations for k from 0 to(L - 1), we get the matrix 
expression TyX = R xxh with: 
1 
h = [h(O) 
h(l) 
h(L -1)r 
TyX ~ [E{Y(n)X(n)) ... E {Y(n)X(n- L+ I))]" 
Rxx= [lE{X(n-i)X(n-j)}] 
.. 
= [Rxx(j-i)] 
.. 
1-::;'t,)-::;'£ 
1-::;'t,)-::;'£ 
Therefore, h = RX~T 
yx. Because Rxx is a Toeplitz matrix, a rapid AQ5 
algorithm, the Levinson algorithm, exists, which provides a fast tech-
nique for computing the inverse of Rxx. Here we will only be using the 
MATLAB® function inv; 
3. to estimate Rxx(k) and Ryx(k) based on a sequence of N observations, 
we first have to center X(n) and Y(n): 
1 
N 
1 
N 
X c(n) = X(n) - N LX(j) and Yc(n) = Y(n) - N LY(j) 
j = 1 
j = 1 

472 Digital Signal and Image Processing using MATLAB® 
then we use: 
Type: 
A 
1 N-k 
Rxx(k) = N L Xc(j + k)Xc(j ) 
j = 1 
N-k 
A
I" 
and Ryx (k) = N ~ Yc(j + k)Xc (j) 
j = 1 
%===== crepimp .m 
N=26; Tx=length(x); 
for jj=l:N 
end 
rx(jj)=[x zeros(1 , jj-l)]*[zerosCjj-l,l) ; x']; 
ryx(jj)=[y zeros(l,jj-l)]*[zeros(jj-l,l) ; x']; 
Rxx=toepl(rx); hest=inv(Rxx) * ryx'; 
plot((O:N-l) ,hest, 'x', (O:N-1) ,h, '0') 
H8 Spectra estimation 
HB.l (Spectrum estimation using the Welch m ethod) (see p. 323) 
1. welch function: 
function [sf,gamma]=welch(x,lnwin,wtype ,Lfft ,beta) 
%!=====================================================! 
%! SYNOPSIS : [sf,gamma]=WELCH(x,lnwin,wtype,Lfft,beta) 
%! 
x 
Input sequence 
%! 
lnwin = analysis window length 
%! 
wtype 
cindow : h(ham) or r(rect) 
%! 
Lfft 
= FFT length 
%! 
beta = confidence parameter 
%! 
gamma = confidence interval (100*beta%) 
% ! 
sf 
= spectrum 
%!=====================================================! 
x=x( :); N=length(x); sf=zeros(Lfft , l); 
%===== suppression of the trend 
[abid x]=tendoff(x); 
Ks2=fix(lnwin/2); lnwin=2*Ks2; nbblocks=fix(N/Ks2)-1; 
if (wtype(l)=='h') 
fen=O.54-0.46*cos(2*pi*(O :lnwin-l)/lnwin); 
elseif (wtype(l)=='r ' ) 
fen=ones(l , lnwin) ; 

Chapter 10 - Hints and Solutions 473 
else 
disp('Unknown window'); return 
end 
wfen=(1/sqrt(fen*fen'»*fen' ; 
for tt=1:nbblocks 
end 
ti=(tt-1)*Ks2+1;tf=ti+lnwin-1 ; pt=x(ti:tf) .* wfen ; 
apf=abs(fft(pt,Lfft» 
.-2; sf=sf+apf; 
sf=sf/nbblocks; gamma=sqrt(2)*erfinv(beta)/sqrt(nbblocks); 
return 
2. the welch function is tested and compared with the smperio function in 
the following program: 
%===== ctestwelch.m 
clear all 
T=1000; blocana=64; Lfft=1024; freq=(0:Lfft-1)/Lfft; 
%===== signal generation 
w=randn(1,T); x=filter([1 .5 .02 .01],1,w); 
xfth=20*log10(abs(fft([1; .5; .02; .01] ,Lfft»); 
%===== periodogram 
ws=fft(x,Lfft); dsp=(1/T)*abs(ws).-2; 
wsf=10*log10(abs(dsp»; 
%===== averaged Periodogram 
[xfw gamma]=welch(x,blocana, 'ham' ,Lfft,0.95); 
%===== smoothed Periodogram 
[xfp freqp]=smperio(x,16,'t'); 
xfw=10*log10(xfw); xfp=10*log10(xfp); 
df1=-10*log10(1+gamma); df2=-10*log10(1-gamma); 
plot(freq,xfw,freq,xfw+df1, 'g- . ' , ... 
freq,xfw+df2, 'g-.' ,freq,xfth); 
set(gca, 'xlim', [0 0.5]); 
hold on; plot(freqp,xfp, 'r'); 
plot (freq,wsf, 'r'); hold off, grid 
H8.2 (Spectrum of a binary signal) (see p. 323) 
Type: 
%===== specbin.m 
clear; Ib=1024; A=5; T1=10; T2=25; T=T1+T2; 
gn=[A*ones(T1,1);zeros(T2,1)]; an=sign(rand(1,lb)-1/2); 
yn=gn*an; yn=reshape(yn,1,T*lb); 
%===== signal 
figure(1),plot(yn(1:7*T»; grid 
set(gca, 'ylim' ,1.2*[-A A], 'xlim' ,[1 7*T]) 
%===== spectra 
Lfft=1024; fq=(0:Lfft-1)/Lfft; tbloc=128; 
spectheor=abs(fft(gn,Lfft» 
.-2 /T; 

474 Digital Signal and Image Processing using MATLAB® 
[syf gamma]=welch(yn,tbloc, 'ham' ,Lfft,95); 
figure(2); subplot(212); plot(fq,10*log10(syf)); grid 
set(gca, 'xlim' ,[0 1/2] ,'ylim' ,[-40 20]) 
subplot(211); plot(fq,10*log10(spectheor), 'r'); grid 
set(gca, 'xlim' ,[0 1/2] ,'ylim' ,[-40 20]) 
%===== theoretical spectrum 
fq1=(1:Lfft-1)/Lfft; 
absGf=A*abs(sin(pi*fq1*T1) ./ sin(pi*fq1)); 
spectheor2=[A*T1 absGf] .-2/T; 
Both the theoretical and the estimated spectra are represented in Figure 
H8.1. We can also use the smperio function (example 8.1), by typing: 
[syfp, freqp]=smperio(yn); 
figure(2); subplot(212); hold on; 
plot(freqp,10*log10(syfp), 'g'); hold off 
With the values chosen for the size of the windows, the results are noticeably 
better with the smperio function. Note, however, that the Welch method 
calculates several (559) length 1,024 FFTs whereas the smoothed periodogram 
method calculates only one FFT with a length at least equal to 35,840 in our 
example. 
=~~ 
-40 
' 
" 
, 
, 
, 
o 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
20~ 
10 
-----~--
---~------
~ ------~-----~------~------~-----~------:------
-lg :::::j:::::-
j : -::::
t ::--
: - ~ ::
--:
; :--
::
-t-
::::
- l::::
-j:-
::::
~ ::::
-: 
=~g o 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
Figure HS.l - Signal spectra: above, the theoretical spectrum, below, the estimated 
spectrum 
HS.3 (Spectral observation and oversampling) (see p. 324) 
Type (Figure H8.2): 
%===== specoversmp.m 
M=8; Npts=1024; Lfft=256; freq=(0:Lfft-1)/Lfft; 
%===== signal generation 

Chapter 10 - Hints and Solutions 475 
NB=8; hn=rif(NB,1/4); xSig=filter(hn,l,randn(l,Npts)); 
xsigs=welch(xsig,16, 'ham' ,Lfft,0.95); 
subplot(311); plot(freq,xsigs); axis([O .5 ° max(xsigs)]); 
%===== insertion of zeros 
y=zeros(M,Npts); y(l,:)=xsig; yr=zeros(l,Npts*M); yr(:) = y; 
yrs=welch(yr,128, 'ham' ,Lfft,0.95); 
subplot(312); plot(freq,yrs); axis([O .5 ° max(yrs)]); 
%===== lowpass filtering 
NPB=50; unsur2M=1/(M*2); hinter=rif(NPB,unsur2M); 
y=filter(hinter,l,yr); 
ys=welch(y,128, 'ham' ,Lfft,0.95) ; 
subplot(313); plot(freq,ys); axis([O .5 ° maxeys)]) ; 
o 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
after insertion (Mis) 
r
-\ ---:------j-<J\------:-j--
- ~ - \ ---
; ------
V-=
-\
-----
; -- -fl 
m j
---- *
-- ~
----:- =
- I =
i--- L
--- *
----i L
-
1 
o 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
after lowpass filtering (Mf..) 
------------ 1 ------------ 1 -------------1 ----- 1 
_____ L ______ I _____ ~ ______ L _____ L _____ J 
_____ L 
I 
-----
o 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
Figure HB.2 - Spectra of the generated signals 
Notice that the window size used for the estimation of the spectrum is 8 
times smaller in the first case than it is in the next two, in order to have the 
same resolution for the three spectra. 
H9 The least squares method 
H9.1 (LMS algorithm: channel identification) (see p. 387) 
The following program allows us to test the LMS algorithm: 
II 
%===== identlms. m 
%===== channel identification 
h=[l 0.6 0 .3]'; P=length(h); % theoretical channel 

476 Digital Signal and Image Processing using MATLAB® 
N=4000; 
% number of steps 
%===== signal generation 
x=randn(N,l); v=filter(h,l,x); Pv=v'*v/N; 
SNR=20; b=sqrt(Pv*10-(-SNR/l0))*randn(N,1); 
y=v+b; 
% noisy observation 
%===== LMS algorithm 
mu=0.002; 
hest=zeros(P,l); en=zeros(N-P+l,l); 
for n=P:N 
enO=y(n) - hest'*x(n:-l:n-P+l); 
hest=hest + mu*enO*x(n:-l:n-P+l); 
en(n-P+l)=enO; 
end 
%===== smoothing of the error over 200 points 
en2=en .-2; moy=200; hmoy=ones(l,moy)/moy; 
en2moy=filter(hmoy,1,en2(1:N-P+l)); 
endb=10 .* log10(en2moy(moy:N-P+l)); 
plot(endb); grid 
[h hest] 
The results are shown in Figure H9.1. As you can see, the final difference 
when the algorithm converges is related to the signal-to-noise ratio. 
O~--~----~--~----~--~----~--~---. 
-2 
I 
I 
I 
I 
I 
I 
I 
-----
1
--------,--------r-------l--------~-------
T
-------~--------
I 
I 
I 
I 
I 
-4 ---
--
~ --------i--------~-------~--------~-------
~ -------~
--------
I 
I 
I 
I 
I 
-6 
-------
~ --------:--------~-------~--------~-------
+ -------~--------
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
-------1 
_______ 1 
______ -""' __ 
-8 
, 
, 
, 
, 
-10 
, 
, 
-----, ------- , ------ , -------, 
-------, --
-12 
: ..... ~~~~~? 
. ~!3. ....... : ........•....... _ ....... . 
I 
I 
I 
I 
, 
I 
I 
I 
-14 
-16 
___ L _______ ~ ________ 
~ _______ L _______ ~ _______ _ 
I 
I 
I 
I 
I 
I 
I 
I 
-18 
-20L---~----~--~----~--~----~--~--~ 
o 
500 
1,000 
1,500 
2,000 
2,500 
3,000 
3,500 
4,000 
Figure H9.1 - LMS algorithm: the error signal plotted against the number of iter· 
ations of the algorithm (square deviation in dB smoothed over 200 points) 
As we saw for the deterministic gradient, the higher the gradient step /-l is, 
the higher the algorithm's descent speed. This is shown in Figure H9.2. 
When /-l increases, the error decreases faster to the final plateau. We can also 
show that the higher /-l is, the higher the final plateau for the difference. This 
is called misadjustment. This behavior is the same for any adaptive algorithm: 
convergence speed comes at the cost of a higher misadjustment. However, this 
effect is rather faint in this case. 

Chapter 10 
Hints and Solutions 477 
5 ,--------------------------------------------, 
o 
______ --1 __ 
-5 
_____ I ________ ~ _______ ~ ________ ~ _______ ~ _______ __l ______
_ 
_ 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
" 
, 
" 
, 
" 
, 
______ --1 ________ 1 
______ --1 __ 
-10 
" 
, 
" 
, 
" 
, 
" 
, 
" 
, 
-15 
, 
, 
-20~--~----~--~----~----~--~----~--~ 
o 
500 
1,000 
1,500 
2,000 
2,500 
3,000 
3,500 
4,000 
Figure H9.2 - LMS algorithm: error signal plotted against the number of iterations 
of the algorithm for different values of It 
When comparing algorithms through simulation, the parameters are set so 
as to reach the same misadjustment level (convergence plateau) for both, then 
the slopes of the error plots, which are characteristic of the convergence speed, 
are compared. 
The theoretical maximum value of the deterministic gradient step is J-Lmax = 
2/ maxi(Ai). In our example, x(n) is a white noise with the variance 1, and 
therefore the matrix R = [ 2. We then have J-Lmax = 2. As the simulations show, 
this value is much too high to ensure the convergence of the LMS algorithm. 
In practice, this is always the case, and J-L is found experimentally, based on 
the observed data, by determining the value of J-L that ensures convergence and 
then by reducing this value by 10%. 
The LMS is adaptive: this means that it can adapt to the possible fluctua-
tions the filter's coefficients could be subjected to. This goes beyond the strict 
context of linear filters, since the time invariance property is no longer obeyed. 
The tracking capability of the LMS algorithm can be measured by adding a 
break in the model and observing how the algorithm is able to track this break. 
Performances crucially depend on the choice of J-L. 


Chapter 11 
Appendix 
Al Fourier transform 
Property 11.1 The main properties of the DFT are listed below: 
- X (J) is bounded, continuous, tends towards 0 at infinity and belongs to 
L2 (JR); 
- the Fourier transform is linear; 
- expansion/compression of time: 
the Fourier transform of x(at) is 
I;IX(J/a); 
- delay: the Fourier transform of x(t - to) is X(J) e-2j7rfto; 
- modulation: the Fourier transform of x(t)e2j7rfot is X(J - fo); 
- conjugation: the Fourier transform of x*(t) is X*( - 1). Therefore, if 
the signal x( t) is real, X (J) = X* (-1). This property is said to be of 
hermitian symmetry; 
- if the signal x( t) is real and even, X (J) is real and even; 
- if the signal is purely imaginary and odd, X (J) is purely imaginary and 
odd; 
- the convolution product, written (x * y)(t) , is defined by: 
/
+00 
/ +00 
(x *y)(t) = - 00 x(u)y(t - u)du = - 00 x(t - u)y(u)du 
(11.1) 
and has X(J)Y(J) as its Fourier transform; 

480 Digital Signal and Image Processing using MATLAB® 
- likewise, the Fourier transform of x(t)y(t) is (X * Y)(f) ; 
- if x(t) is m times continuously differentiable and if its derivatives are 
summable up to the m -th order, then the Fourier transform of the m -th 
derivative x(m)(t) is (2j7rf)mX(f); 
- iftmx(t) is summable, then the Fourier transform of (- 2j7rt)mx(t) is the 
m-th derivative x(m)(f) . 
A2 Discrete time Fourier transform 
Property 11.2 Let X(f) and Y(f) be the DTFTs of the sequences {x(n)} and 
{y(n)} respectively. The DTFT has the following properties: 
1. linearity: ax(n) + by(n) ---+ aX(f) + bY(f); 
2. time-shift: 
x(n - no) ---+ X(f)e-2j7rnof 
(11.2) 
3. modulation: x(n)e2j7rfon ---+ X(f - fa); 
4· time reversal: x( - n) ---+ X ( - f); 
5. conjugation: x* (n) ---+ X* ( - f); 
6. real sequence: x( n) real ---+ X (f) = X* (- f) . X (f) has a property called 
hermitian symmetry. Particularly, IX (f) I and the real part Re(X (f)) are 
even functions. Its phase arg(X(f)) and its imaginary part Im(X(f)) 
are odd functions. In this case, the plotting of X (f) can be limited to the 
interval f E (0,1/2). 
7. convolution: the convolution product or convolution defined by: 
+00 
+00 
x(n) *y(n) = L x(k)y(n - k) = L x(n - k)y(k) 
k= -oo 
k= -oo 
has the product X(f)Y(f) as its DTFT. 
As an exercise, we will now demonstrate part 6 of properties 11.2. Starting 
off with the definition 2.22, we get, after conjugating and changing the sign: 
+00 
X*(-f) = L x*(n)exp(-2j7rnf) 
which is still equal to X(f) since x*(n) = x(n). 

Appendix 481 
A3 Discrete Fourier transform 
Property 11.3 The main properties of the DFT are listed below: 
1. linearity: ax(n) + by(n) ~ aX(k) + bY(k),-
2. time-shift: x((n - p) mod N) ~ X(k)e-2j7rpk/N,-
3. time reversal: x( ( -n) mod N) ~ X (( - k) mod N),-
4. conjugation: x*(n) ~ X*(( -k) mod N),-
5. real sequence: x(n) real ~ X(k) = X*((-k) mod N),-
6. circular convolution: the sequence Z(k) = X(k)Y(k) has: 
N-l 
z(n) = L x(p)y((n - p) mod N) 
(11.3 ) 
p=O 
as its inverse DFT. 
7. Parseval formula: 
N-l 
N-l 
L Ix(n)12 = ~ L IX(kW 
n=O 
k=O 
(11.4) 
o As an example, consider the case of time reversal (item 3) for N = 4. The 
sequence y(n) = x( -n) refers to: 
{y(O),y(1),y(2),y(3)} = {x(O),x(3),x(2),x(1)} 
Its DFT is: 
{Y(O) , Y(l) , Y(2) , Y(3)} = {X(O),X(3),X(2),X(1)} 
o To prove expression 11.3 of item 6, we will calculate the inverse DFT of 
Z(k) = X(k)Y(k), using 2.33. We get: 
N -l 
z(n) = ~L 
X(k)Y(k)e2j7rnk/N 
k=O 
~ ~ (% x(al,-Xj,"'/N %;: Y(~l,-Xj'
P k/N ) e'j,nk/N 
% 
%;: x("ly(~l 
(~ ~ ,-,jK(aW -n)k/N) 
The equality 2.34 leads us to the expected result. 
o To prove item 7, all you have to do is set y(n) = x*( -n) in 11.3, then calculate 
z(n) for n = O. 

482 Digital Signal and Image Processing using MATLAB® 
A4 
z -Transform 
P rop erty 11.4 Let XI(z) and X 2(z) be the z-transforms of the sequences 
{Xl (n)} and {X2 (n)} respectively, and VI and V 2 their convergence areas. We 
have the following properties: 
1. linearity: 
(11.5 ) 
2. time delay: 
x(n - k) --+ z-k X(z) 
(11.6) 
The convergence area is unchanged; 
3. time reversal: 
x(- n) --+ X(l/z) 
with 
(11. 7) 
4. reality and symmetry: if the sequence {x( n)} is real, then: 
X(z) = X*(z*) 
(11.8) 
The convergence area is unchanged; 
5. convolution: 
+00 
(Xl *x2)(n) = L XI(n - k)X2(k) --+ X I(Z)X2(z) 
(11.9) 
k = -oo 
6. Parseval relation: 
+00 
L Ix(nW = ~ i X(z)X*(l/z*) dz 
n = - oo 
2J7r 
(r) 
Z 
(11.10) 
where the integration contour (f) is inside the convergence areas of X (z) 
and of X*(l/z*) . Note that the unit circle is inside the convergence area; 

Appendix 483 
7. any function XA z), holomorphic inside the ring Rl < Izl < R2, is ex-
pandable in a unique power series X z(z ) = 2:nEZ x (n) z-n with: 
x(n) = ~ i X( z )zn-1 dz 
2Pf (r) 
where (r) refers to a Cauchy contour [9j inside the ring Rl < Izl < R2. 
This integral can be calculated with the use of Cauchy 's integral formula 
(also known as the "residue method"); 
8. the sum of the power series X z(z ) = 2:nEZ x (n) z-n is a holomorphic 
function in the convergence area Rl < Izl < R2 and its derivative can be 
obtained term-by-term: 
( ) 
dXz(z) 
nx n --+ - z 
dz 
(11.11) 
The convergence area is unchanged. 
The results below show that the convergence area's shape is related to the 
properties of the sequence {x( n)} . 
P roperty 11.5 We have the following: 
1. x ( n) is such that 2:n Ix( n) 1 < +00 if and only if the unit circle belongs 
to the convergence area; 
2. the signal is causal if and only if the convergence area of its z-transform 
verifies {z E <C: Izl > Rd (meaning also that R2 = +00), and we have: 
x(O) = lim X z(z ) 
z-++oo 
(11.12) 
3. the signal is anti-causal if and only if the convergence area of its z -
transform verifies {z E <C : Izl < R2} (meaning also that Rl = 0),. 
4. if X z(z ) is a rational function B(z )jA(z ), where the degrees of B(z ) and 
A(z ) are equal to Q and P respectively, the zeros are the Q roots of the 
polynomial B(z ), and the poles are the P roots of the polynomial A(z ); 
5. if XA z ) = B(z )jA(z ), the possible convergence areas are the non empty 
rings containing no poles and delimited by two poles. Mathematically 
speaking, a convergence area can be expressed: 
where Pj and Pk are two distinct roots of A(z) defined so that A(z ) i- 0 
for any z such that Ipj 1 < Izl < IPk I· 
As a consequence, when using 
B(z )jA(z ), there are several possible converging areas, each one corre-
sponding to a different sequence {x ( n)},. 

484 Digital Signal and Image Processing using MATLAB® 
6. if Xz(z) = B(z)/ A(z) , a necessary condition for the sequence {x(n)} to be 
causal is that the degree (with respect to z) of the numerator is less than, 
or equal to the degree (still with z as the variable) of the denominator. 
All we have to do then, in order to completely characterize the causal 
sequence, is to choose, as the convergence area: 
{z E C : I z I > Ip M I} 
where PM denotes the pole of Xz(z) with the highest modulus. 
As a 
counter-example, you can check that no causal sequence corresponds to 
the function Xz(z) = (z - 1)2/(z + 1); 
7. if Xz(z) = B(z)/A(z), and if the original corresponding sequence is real, 
then the roots of A(z) and B(z) are either real, or come in pairs of 
conjugate complex numbers. 

Bibliography 
[1] M. F. Barnsley and L. P. Hurd. Fractal Image Compression. AK Peters, 
Ltd., 1993. 
[2] G. Blanchet and J . Prado. Elements d'Automatique. Collection Peda-
gogique de Telecommunication. Ellipses, 1994. 
[3] P. Bremaud. Introduction aux Probabilites. Springer Verlag, 1988. 
[4] P. Brockwell and R. Davies. Time Series: Theory and Methods. Springer 
Verlag, 1990. 
[5] J. Canny. "Computational Approach to Edge Detection". IEEE Trans. 
Pattern Anal. Machine Intell., pages 679- 698, November 1986. 
[6] H. Cartan. 
Theorie Elementaire des Fonctions Analytiques de une ou 
plusieurs Variables Complexes. Hermann, Paris, 1975. 
[7] M. Charbit. Elements de Theorie du Signal: Signaux Aleatoires. Collection 
Pedagogique de Telecommunication. Ellipses, 1996. 
[8] J.W. Cooley and J.W. Tuckey. "An Algorithm for the Machine Calculation 
of Complex Fourier Series". Math. of Comp. , 19:297-301, April 1965. 
[9] J.P. Delmas. 
Elements de Theorie du Signal: Signaux Deterministes. 
Collection Pedagogique de Telecommunication. Ellipses, 1995. 
[10] J.P. Delmas. Introduction aux Probabilites. Ellipses, 2000. 
[11] LeRoy E. DeMarsh and Edward J. Giorgianni. Color science for imaging 
systems. Physics Today, September 1989. 
[12] Al Bovik (editor). Handbook of Image fj Video Processing. Academic 
Press, 2000. 
[13] Mark D. Fairchild, M. R. Luo, and R. W. G. Hunt. 
A Revision of 
CIECAM97s for Practical Applications. Color Research and Applications. 
Wiley Interscience, August 2000. 

486 Digital Signal and Image Processing using MATLAB 
® 
[14] C. Fernandez-Maloigne, F. Robert-Inacio, and L. Macaire. 
Couleur 
numerique : Acquisition, perception, codage et rendu. Number ISBN-
13: 978-2746225558 in IC2 signal et image. Hermes Science Lavoisier 
June 2012. 
[15] J. P. Guillois. Techniques de Compression des Images. Hermes, 1996. 
[16] F.J. Harris. "On the Use of Windows for Harmonic Analysis with the 
Discrete Fourier Transform". Proc. IEEE, 66:51-83, January 1978. 
[17] S. Haykin. Adaptive Filter Theory. Prentice Hall, Englewood Cliffs, NJ, 
USA, 2nd edition, 1991. 
[18] P. Hough. Method for recognizing complex patterns, 1962. US Patent 
3069654. 
[19] R. S. Hunter. "Accuracy, preCISIOn, and stability of new photo-electric 
color-difference meter". JOSA, Proceedings of the Thirty- Third Annual 
Meeting of the Optical Society of America, December 1948. 
[20] R. S. Hunter. "Photoelectric color-difference meter". JOSA, Proceedings 
of the Winter Meeting of the Optical Society of America, July 1948. 
[21] Johannes Itten. The Art of Color: the subjective experience and objective 
rationale of color. A VNR book. Wiley, 1974. 
[22] S. M. Kay. Modern Spectral Estimation, Theory and Application. Prentice 
Hall, Englewood Cliffs, 1988. 
[23] O. Macchi. Adaptive Processing: The Least Mean Squares Approach with 
Applications in Transmission. John Wiley & Sons, Inc., 1995. 
[24] D. Malacara. Color Vision and Colorimetry: Theory and Applications. 
Press Monographs. Society of Photo Optical, 2002. 
[25] N. Ohta and A. Robertson. Colorimetry: Fundamentals and Applications. 
The Wiley-IS&T Series in Imaging Science and Technology. Wiley, 2006. 
[26] N. Otsu. "A Threshold Selection Method from Gray-Level Histograms". 
IEEE Trans. on Syst. Man and Cyber., 1:62-69, 1979. 
[27] B. Porat. A Course in Digital Signal Processing. John Wiley & Sons, Inc., 
1997. 
[28] C. Poynton. A technical Introduction to Digital Video. John Wiley and 
Sons, 1996. 
[29] M. Rosenblatt. Stationary Processes and Random Fields. Birkhauser, 
1985. 

Bibliography 487 
[30] L. L. Scharf. Statistical Signal Processing: Detection, Estimation, and 
Time Series Analysis. Addison Wesley, 1991. 
[31] A. Schuster. "On the Periodicities of Sunspots". Philosophical Transactions 
of the Royal Society of London, 206, Ser.A:60- 100, April 1906. 
[32] A. R. Smith. 
"Color gamut transform pairs". 
Computer Graphics, 
12(3): 12-19, August 1978. 
[33] Union Internationale des Telecommunications, CCITT. Technologie de 
l'information - Compression numerique et codage des images fixes - de 
nature photographique - Prescriptions et lignes directrices, 1981. 
[34] D. Ventre. Communications Analogiques. Collection Pedagogique de Te-
lecommunication. Ellipses, 1991. 
[35] C. K. Wallace. "The JPEC Still Picture Compression Standard". Com-
munications of the ACM, April 1991. 
[36] E. Wang. Stochastic Processes in Information and Dynamical Systems. 
Mac Craw Hill, 1971. 
[37] P. D. Welch. "The Use of Fast Fourier Transforms for the Estimation of 
Power Spectra: A Method Based on Time Averaging Over Short Modified 
Periodograms". IEEE Trans. Audio Electroacoust., AU-15, June 1967. 
[38] B. Widrow and S. Stearns. Adaptive Signal Processing. Prentice Hall, 
1985. 
[39] C. U. Yule. "On a Method of Investigating Periodicities in Disturbed 
Series, with special reference to Wolfer's Sunspot Numbers". Philosophical 
Transactions of the Royal Society of London, 226, Ser.A:267-298, April 
1927. 


Index 
3 dB cut-off frequency, 412 
\ operator, 359 
2D-DFT,202 
2D-DTFT, 201, 202 
2D-IDTFT, 201 
3-sigma rule, 256, 349 
6 dB per bit rule, 272 
ADC, 68 
addition (of matrices), 31 
addressing 
bit reverse, 95 
affine, 292 
trend (suppressing), 468 
algorithm 
LMS, 382 
deterministic gradient, 372 
gradient, 371 
gradient (convergence condition), 
372 
LMS, 390 
stochastic gradient, 390 
aliasing, 72 
temporal, 169, 435 
all-pass (filter) , 148 
all-passmultiply-def, 147 
all-pole, 306 
alpha layer, 178 
AM, 112 
ambiguity, 76 
amplitude modulation, 112 
analog-to-digital converter (ADC), 68 
analytical signal, 61 
AND (logical), 184 
anti-aliasing, 74, 287 
anticausal 
sequence, 58 
signal, 58, 485 
anti causality 
sequence, 82 
AR, 306, 329, 366, 387 
AR process, 387 
AR1, 311 
ARMA, 313 
autocorrelation, 276 
autocovariance, 275 
estimation, 315 
hermitian symmetry, 280 
positive nature, 296 
positivity, 280 
Toeplitz matrix, 283 
autoregressive, 306, 329, 366 
band 
limited WSS r.p., 287 
pass, 163 
stop, 163 
transition, 163 
useful, 73 
band-limited, 302 
signal, 68 
band-pass 
filter, 146 
bandwidth, 146 
bar chart, 260 
Bessel, 114 
best 
linear 
unbiased 
estimator 
(BLUE),360 

490 Digital Signal and Image Processing using MATLAB® 
bias, 360 
bias and variance (periodogram), 323 
BlBO (Bounded Input, Bounded Out-
put),64 
bilinear transform, 168 
binarization, 224 
binary 
signal, 325 
bit reverse, 94 
addressing, 95 
BLUE, 360 
blur effect, 207 
Bode 
diagrams, 44 
plots, 44 
Butterworth filter, 166, 431 
Canny, 220 
carrier frequency, 112 
carrierless amplitude modulation, 112 
causal 
sequence, 58 
signal, 58, 485 
z-transform, 485 
causality, 75, 88, 118, 157 
sequence, 82 
transfer function, 130 
cdf, 246, 248 
cells, 29 
centered 
process, 276 
random variable, 252 
CF,271 
characteristic function, 251 
marginal probability distribution, 
251 
Chebyshev 
filter, 167 
inequality, 252 
chromaticity, 188 
CIE Lab, 178 
clipping, 271 
factor, 271 
CMF,186 
CMY, 192 
CMYK, 178, 191 
color matching functions (CMF), 186 
colormap, 179 
communications channel, 363 
complex exponential, 83 
complex exponential (truncated) , 83 
confidence 
ellipse, 263, 462 
interval, 256 
continuous component, 281 
contour detection, 219 
contrast 
adapted modification, 231 
enhancement, 233, 456 
linear modification, 229 
local modification, 232 
convergence 
L 2 , 60 
area, 123, 485 
condition (gradient), 372 
DTFT,85 
least squares, 60 
conversion 
analog-to-digital, 68 
digital-to-analog, 81, 173 
functions, 49 
convolution, 117 
circular, 483 
DTFT, 482 
FT,481 
z-transform, 484 
correlation, 252 
coefficients, 252 
deterministic, 84 
method,297 
covariance, 252 
matrix, 281 
estimation, 290 
method,297 
stationary, 300 
cumulative distribution function, 264 

CZT, 134 
d.e. (difference equation), 127 
DAC, 81 
DCT, 236, 237 
de-emphasis, 302, 473 
decibel, 87 
decimation, 174, 176 
delay 
FT,481 
z-transform, 484 
density (probability), 248 
derivative, 165 
DFT, 88, 89 
DFT delay, 92 
difference equation, 127 
digital processing, 67 
digital-to-analog converter, 81 
dilation, 235 
Dirac distribution, 283 
discrete cosine transform, 236, 237 
discrete Fourier transform, 88 
distribution 
Bernoulli, 464 
Poisson, 463 
Rayleigh, 464 
DoG mask, 212 
domain of convergence, 123 
dot product (DTFT), 87 
double side-band 
modulation, 112 
suppressed carrier, 112 
DSB modulation, 112 
DSBSC, 112 
DTFT, 70, 84 
hermitian symmetry, 403 
properties of, 482 
echo canceling, 393 
eigenfunctions, 63, 127 
encapsulated postscript, 45 
energy spectral density, 61 , 87 
envelope, 346 
detector, 414 
Index 491 
eps (encapsulated postscript), 45 
equalization, 379, 391 
Wiener, 380 
equation 
normal, 310, 365 
Yule-Walker, 310, 365 
ergodicity, 290 
erosion, 233 
error 
prediction, 309 
esd, 61, 87 
estimator 
unbiased, 360 
expansion, 170 
factor 
furget, 361, 384, 390 
quality (JPEG) , 238 
fast Fourier transform, 88, 93 
FFT, 88 
number of operations of the, 94 
real sequences, 95 
filter 
Gaussian 
derivative-smoothing, 
214 
median, 224 
anti-aliasing, 74 
band-pass, 146 
circular, 207 
conical, 209 
finite impulse response, 120 
FIR,120 
gaussian, 209 
gaussian (contrast), 232 
gaussian derivative, 212 
high-pass, 146 
identity (2D) , 206 
low-pass, 146 
median, 450 
memory, 119 
perfect reconstruction 71 
Prewitt, 210 
' 
rectangular, 208 

492 Digital Signal and Image Processing using MATLAB® 
second derivative, 211 
separable, 207 
Sobel, 211, 446 
stability (FIR), 164 
type, 160 
Wiener, 369, 372 
finite energy function, 58 
finite impulse response, 131 
finite power function, 58 
FIR, 131, 155 
FIR filter 
linear response, 156 
format 
eps, 45 
Fourier 
DFT, 88 
discrete-time transform, 70 
DTFT, 70, 84 
limit, 102 
series, 59 
transform, 60 
frequency 
carrier, 112, 114 
deviation, 114 
empirical, 261 
fundamental, 60 
harmonic, 60 
image, 76 
instantaneous, 114 
resolution, 89, 100, 324, 338 
resonance, 141 
resonant, 140 
response, 126 
function 
\, 359 
dilation, 235 
erosion, 233 
fminsearch, 220 
imread, 178 
abs , 34 
acos, 34 
addpath, 23 
AND log, 184 
as in, 34 
atan, 34 
autocorrelation, 276 
autocovariance, 275 
axis, 42 
Bessel, 114 
bilintrimg, 217 
bincoding, 466 
calcbuttercalcbutter .m, 431 
cat, 29 
characteristic, 251 
clear, 26 
cmyk2rgbm, 192 
codel , 458 
complex exponential, 59 
contour, 45 
conv2, 206 
cos, 34 
covariance, 300 
covariance (of two processes), 276 
covtodsp, 317 
cumulative distribution, 246, 248, 
264, 267 
cylinder, 46 
DCTp, 457 
decM, 437 
definition, 41 
delete , 47 
dergauss, 212 
deriv, 430 
dermoygauss , 449 
dilation, 235 
Dirac, 59 
disp, 49 
eig, 38 
ellipse, 43, 264 
ellipsoid, 46 
end, 25 
erasemode, 48 
erosion, 233 
error, 41 
etime, 90 
eval,214 

exp, 34 
expm, 36 
eye, 27 
fit , 90 
figure , 41 
filter, 120 
filter2 , 206 
filtricII , 312 
find, 38 
finite energy, 58 
finite power, 58 
FoncLog, 439 
fopen, 50 
for , 40 
format , 26 
fread, 50 
funm, 36 
fwrite , 50 
gallery, 27 
gate, 58, 83, 136 
genpath, 23 
get, 42 
ginput, 49, 409 
grid, 49 
hadaptmod, 231 
Heaviside, 58 
hex2num, 49 
hist, 262 
hlinmod, 229 
hough, 222 
hsv2rgb, 190 
iDCTp, 460 
if,40 
imagesc, 108 
imread, 182, 195 
imwri te, 182, 195 
ind2sub, 38 
initctes, 237 
input, 49 
interM, 436 
ischar, 40 
isfinite, 40 
iSinf, 40 
isnan, 40 
isstr, 40 
linspace, 25 
lintrimg, 441 
load, 50 
log, 34 
logm, 36 
logspace, 25 
max, 36 
mean, 292 
median, 36 
median2D, 450 
mesh, 45, 108 
MEX, 53 
min, 36 
moygauss, 445 
mydisp, 183 
nargin, 41 
nargout , 41 
nbilin, 433 
nextpow2, 90 
num2str, 49 
ones, 27 
otsu, 454 
path, 23 
pathtool, 23 
pinY, 359 
plot, 41, 49 
plot3, 46 
poly, 38 
print, 45 
Index 493 
probability density, 248 
pulse, 59 
quant , 458 
rand, 27, 258 
randn, 27, 258, 273 
raw2matf , 181 
reconstruction, 71, 72 
rectangle, 58, 87 
repmat , 29 
reshape, 27 
rgb2cmykm, 191 
rgb2hsv, 190 

494 Digital Signal and Image Processing using MATLAB® 
rif , 165, 428 
rmpath, 23 
roots, 38 
save, 50 
savepath, 23 
semilogx, 45 
set, 42 
siganal.m, 154 
sign, 58 
sin, 34 
sine, 59 
sine cardinal, 59 
sphere, 46 
sprintf, 49, 214 
sqrt, 34 
sqrtm, 36, 286 
stft .m, 410 
str2num, 49 
struct, 29 
subplot, 41 
summable, 58 
surf, 45, 108 
switch, 40 
TabQuantif, 238 
tan, 34 
tendofi,468 
tic, 90 
title, 49 
toc, 90 
transfer, 126 
trends eason, 295 
triangle, 90 
tstinrect, 453 
unifab, 259 
unquant , 460 
unwrap, 127, 404 
userpath, 24 
welch, 474 
while, 40 
xtoa, 328 
zeros, 27 
zoom, 42 
fundamental frequency, 60 
gain 
complex, 126 
of a filter, 126 
gaussian, 286 
process (whitening), 286 
white noise, 284 
generating a trajectory, 301 
Gibbs (phenomenon) , 60 
gray 
levels, 180 
group delay, 149 
half-band 
filter, 156 
Hamming, 316 
window, 104, 163 
harmonic frequency, 60 
Heaviside function, 58 
hermitian symmetry, 88, 280 
DTFT,482 
FT,481 
high-pass 
filter, 146 
higher order, 306 
Hilbert 
space, 351 
transform, 61, 153 
histogram, 259 
of an image, 225 
Horner scheme, 169 
HOS, 306 
Hough (method), 220 
HSL, 178 
HSV, 190 
identification 
channel, 363, 389 
IDFT, 89, 92 
IIR, 131 
image, 177 
oversampling, 170 
registration, 197 
impulse 
response, 63, 303 

impulse response, 117 
calculation, 135 
indexed image representation, 178 
inequality 
Schwarz, 276 
infinite impulse response, 131 
initial conditions, 134, 301 
initial state, 120 
instantaneous 
frequency, 78 
interference, 379 
intersection, 184 
interspectrum, 300 
invariance, 117 
inverse DFT, 89, 92 
joint probability, 247 
JPEG, 236 
Jury criterion, 136, 421 
kernel of a matrix, 358 
learning sequence, 363, 389 
least squares, 293, 356, 468 
levels of gray, 180 
likelihood, 292 
limit (Fourier), 102 
linear 
phase, 156 
prediction, 354, 365, 368 
regression, 368 
linear filter 
all-pass, 147, 148 
anti-aliasing, 287 
Butterworth, 166, 168 
Cauer, 168 
causality, 63, 157 
Chebyshev, 167 
complex gain, 63 
de-emphasis, 302 
derivative, 165 
eigenfunctions, 63 
elli ptic, 168 
FIR, 131 
first order, 137 
frequency response, 63 
gain, 126 
half-band, 156 
IIR, 131 
Index 495 
impulse response, 63 
input/ output covariance, 300 
input/ output interspectrum, 300 
low-pass, 165 
minimum phase, 149 
phase, 126 
pre-emphasis, 302 
purely recursive, 137, 141 
recursive, 131 
rejector, 143 
second order, 141 
smoothing, 300 
stability, 64 
transient, 301 
Wiener, 368, 372 
WSS process, 298 
linear prediction, 387 
linear system resolution (\ ), 32 
linear transformation 
gaussian, 286 
whitening, 286 
lineari ty, 117 
z-transform, 484 
LMS, 382 
convergence condition, 390 
gradient step, 390 
misadjustement, 373 
misadjustment, 372 
normalized, 390 
lobe, 85 
main, 104 
side, 104 
local oscillator, 113 
logical operation, 184 
low-pass 
filter, 146, 165 
MA, 304 

496 Digital Signal and Image Processing using MATLAB® 
marginal probability distribution 
characteristic function, 251 
matrix 
covariance, 310 
covariance, 253, 281 
positive (square root), 286 
weighting, 361 
maximum likelihood, 292, 333 
mean, 252 
vector, 253 
merit factor, 102 
method 
Canny, 220 
Hough,220 
Otsu, 224 
correlation, 297 
covariance, 297 
least squares, 333 
of moments, 303 
procrustes, 197 
Welch, 323, 324, 474 
window, 155- 157 
minimum 
norm, 359 
phase, 149, 164, 306 
misadjustment, 372, 373 
model 
autoregressive, 368 
parametric, 315 
semiparametric, 315 
modulation 
amplitude, 112 
carrierless amplitude, 113 
frequency, 78, 113, 114 
FT,481 
index, 112, 114 
modulo N (DFT), 483 
moving average, 304 
multiplication (of matrices), 31 
noise 
quantization, 289 
background, 256 
measurement, 356 
model, 356 
quantization, 68, 270, 290, 326, 
466 
white, 283 
norm, 351 
frobenius, 199 
normalized 
frequency, 70 
time, 70 
OCR,224 
operator 
Backslash, 359 
optical 
characters recognition, 224 
transfer function, 206 
orthogonality, 351, 352 
OTF,206 
Otsu, 224 
outcome, 273 
overlap-add, 347 
overmodulation, 112 
oversampling, 169, 290, 326 
parallel 
oversampling, 176 
undersampling, 176 
Parseval 
formula, 60, 61, 87, 299 
formula (DFT), 483 
formula (ZT) , 484 
passband, 145, 163 
perfect reconstruction, 71 
periodic/symmetrical window, 106 
periodogram, 319, 334, 339 
averaged, 323 
frequency smoothed, 321 
phase 
of a filter, 126 
phase delay, 149 
pixels, 177 
point spread function, 206 
Poisson 

formula, 69 
poles (transfer function), 485 
polyphases components, 176 
positivity, 280, 296 
power, 279 
spectral density, 60, 279 
spectral distribution, 281 
pre-emphasis, 302, 473 
prediction, 309 
error, 368 
Wiener, 369 
probability 
density, 248 
distribution, 246 
joint, 247 
probability distribution 
Bernoulli, 268 
complex Gaussian, 257, 260 
complex normal, 257 
exponential, 267 
Gaussian, 256 
normal,256 
Poisson, 266 
Rayleigh, 268 
process 
random, 246 
procrustes (method) , 197 
program 
aliasingtrains.m, 215 
oversamp2Ds.m, 217 
aliasexple.m, 76 
analsunspots.m, 330 
ananote .m, 347 
approxsin.m, 362 
arl.m, 302 
basicfct. m, 83 
binarl.m, 225 
binarOtsu.m, 455 
bpfilter.m, 428 
buttertest.m, 433 
car21.m, 420 
car22.m, 421 
cbernou.m, 465 
cbutterl.m, 432 
cbutter2.m, 432 
cdelayl . m, 405 
cderhor.m, 431 
Cdersin.m, 430 
ceffham. m, 408 
ceffsamp.m, 412 
cellconf.m, 462 
cfftreal.m, 406 
chromaticity.m, 186 
ciecmf.m, 187 
cmeannoise.m, 469 
Cmle .m, 339 
cmodam.m, 415 
cmodcdsb.m, 416 
compare.m, 403 
complms.m, 391 
confintrv.m, 257 
contoursobel.m, 450 
corrigl.m, 458 
cpablim. m, 471 
cpoisson.m, 463 
crayleigh.m, 464 
crepimp.m, 474 
cresol.m, 408 
csamp12.m, 401 
cspectri.m, 404 
cssurbq.m, 466 
cstereo.m, 417 
cstft. m, 411 
csup50hzl.m, 422 
cteres.m, 142 
ctestwelch.m, 475 
dataex.m, 238 
decMspeech.m, 437 
decpara.m, 438 
derivsynth.m, 447 
drawmo.m, 193 
echocancell.m, 394 
echocance12.m, 395 
estlsinreel.m, 341 
estARlms.m, 388 
evenodd.m, 158 
Index 497 

498 Digital Signal and Image Processing using MATLAB® 
exactAR.m, 312 
exerosion.m, 234 
exfiltint.m, 120 
exfiltrand.m, 121 
explaw.m, 268 
fluctperio.m, 320 
frqshift. m, 172 
gainphase.m, 127 
gcompl.m, 260 
gene 1 . m, 109 
gene2 . m, 110 
generepimp.m, 303 
geomproc .m, 451 
geomproc2.m, 452 
geomtransf.m, 195 
graddet22.m, 376 
graddet23.m, 377 
gradmul . m, 375 
hilphrase .m, 155 
histoGld .m, 260 
histoUnif .m, 265 
identlms.m, 477 
imgtstcode.m, 460 
imgtstdecode .m, 461 
interMex.m, 437 
lenacone .m, 445 
lenadersec.m, 447 
lenamedian.m, 451 
lenarect .m, 444 
lenasobel.m, 446 
loops, 40 
lowpass2. m, 161 
mdraw2D .m,45 
modfm2.m, 114 
modulfreq.m,78 
modulfreq2 . m, 411 
munsharpwl.m, 456 
myanim.m, 47 
myanim2.m, 48 
mybezier.m, 47 
mysphere.m,46 
nbsin.m, 349 
onewin.m, 105 
overpara.m, 439 
Pmaxsin.m, 344 
psdQ.m, 326 
purpoles.m, 139 
rectfilter.m, 418 
recttransf .m, 443 
rejec500Hz.m, 423 
rejection .m, 424 
repimp2.m, 138 
repimpuls . m, 135 
resol1.m,98 
reso12.m,99 
resolfreq . m, 102 
rifham.m, 163 
riHilbert .m, 154 
scrambexple.m, 386 
shiftf . m, 91 
simulgraddeter.m, 373 
sin80br.m, 284 
sine80.m,79 
sinusversusAR2.m, 308 
smoothl.m, 207 
smperio.m, 322 
snowing.m, 224 
specbin.m, 475 
specct1.m, 106 
specct2.m, 108 
specct3 . m, 108 
specoversmp.m, 476 
speechalias.m,77 
startup.m, 24 
stepresp.m, 122 
steprespAR1.m, 420 
tempalias.m, 435 
testbode.m,44 
testcovtodsp.m, 318 
testellipse.m, 43 
testeval.m,52 
testHSV.m, 190 
testlogic .m, 185 
testlogic2.m, 440 
testtrendseason.m, 295 
testxtoa.m, 328 

thresholdg.m, 219 
traj 1. m, 273 
trajAR.m, 307 
trajMA.m, 304 
transftri .m, 441 
tstbilinr .m, 218 
tstdergauss.m, 213 
tstdermoygauss.m, 449 
tstfftblock.m, 203 
tstfftMo.m, 205 
tstmoygauss.m, 445 
tstraw2mat.m, 182 
twosini.m, 106 
unbloccode.m, 459 
wentog.m, 442 
writing, 22 
projection 
orthogonal, 352 
theorem, 352, 368 
psd, 60, 279 
pseudo-inverse, 359 
PSF, 206 
quantization, 180, 290 
6 dB per bit, 466 
and oversampling, 288 
noise, 326 
uniform scalar, 270 
radio communications, 414 
random 
harmonic, 281 
process, 246 
signal, 245 
variable, 245 
vector, 252 
random process 
harmonic, 277 
almost deterministic, 366 
AR, 306, 329, 366 
AR-1 , 311 
ARMA, 313 
centered, 276 
ergodic, 291 
filtering, 298 
gaussian, 286, 307 
MA,304 
outcome, 273 
realization, 273 
trajectory, 273 
Index 499 
wide-sense stationary (WSS), 276 
random value 
mean, 252 
variance, 252 
random variable 
complex Gaussian, 257, 260 
Gaussian, 256 
independence, 249 
standard deviation, 252 
real sequences (FFT), 95 
reconstruction function, 71 
rectangle, 85 
function, 83 
rectangular windowing, 58 
registration (image), 197 
regressor, 356 
rejector, 143 
replica, 170 
resolution 
frequency, 89 
resolution x time product, 102 
resolution (frequency), 100 
resolution limit of Fourier, 341 
resonant frequency, 146 
RGB, 178, 179, 189 
ripple factor, 145 
ripples, 103, 163 
rise time, 420 
root mean square, 279 
rotation (image), 194 
sampling, 180 
random signal, 287 
deterministic signal, 68 
frequency, 68 
period, 68 
scalar product, 351 

500 Digital Signal and Image Processing using MATLAB® 
Schwarz 
inequality, 276 
script, 22 
seasonal, 293 
second order, 141 
filter, 141 
separable (filter), 207 
sequence (learning), 389 
short term Fourier transform, 108, 109 
sign function, 83 
signal 
analog (reconstruction), 81 
analytical, 61, 153 
anticausal, 82 
band-limited, 302 
causal, 82 
digital, 68 
low-pass, 73 
narrowband, 74 
rectangle, 85 
stereophonic, 113 
signal-to-noise ratio, 271, 284, 302, 
303, 332, 339, 389 
signal-to-noise ratio (quantization), 
466 
simulation, 326 
sine 
cardinal, 59, 173 
function, 83 
truncated, 83 
single side-band modulation, 112 
smoothing, 207, 300 
Wiener, 370 
space 
L 1,58 
L 2 , 58 
spectral distribution of power, 281 
spectral peak, 281 
spectrum, 61, 87, 279 
aliasing, 71, 72,174 
peak, 281 
ripples in the, 103 
spectrum (continuous time), 68 
spectrum (discrete time), 70 
speech 
coding, 473 
expansion and frequency tranla-
tion, 171 
square root 
positive matrix, 286 
SSB modulation, 112 
stability 
BlBO, 64, 118, 119 
Toeplitz, 310 
transfer function, 130 
triangle, 421 
startup, 24 
stationary random processes, 275 
statistics 
step 
higher order, 306 
quantization, 270 
gradient, 371 
response, 121 
STFT,109 
STFT (short term FT), 108 
stochastic gradient, 382, 383 
stopband, 163 
structures, 29 
summable function, 58 
sunspots, 329 
superposition principle, 117 
suppressing 
a mean, 291 
a seasonal trend, 291 
a trend, 291 
synchronous demodulation, 113 
test sequence (length), 269 
TF (transfer function), 126 
theorem 
time 
projection, 352, 368 
advance (z-transform), 134 
constant, 141 
delay (z-transform), 484 

reversal (DFT), 483 
time delay, 90 
torsion, 196, 443 
trajectory, 273 
transconjugate of a matrix, 32 
transfer function, 126 
transform 
bilinear, 168 
causal z-transform, 134 
discrete Fourier, 88, 89 
Hilbert, 61, 153 
unilateral z-transform, 134 
z-transform, 123 
transformation 
affine, 196, 197, 441 
torsion, 196, 197, 443 
transient state, 301 
transition band, 145 
transpose-conjugate of a matrix, 32 
trend 
affine, 292 
seasonal, 293 
triangle 
stability, 421 
triangle function, 90 
truncated sine, 83 
undersampling, 174 
uniform quantization 
signal-to-noise ratio, 272 
unit 
impulse, 82, 283 
pulse, 82 
step, 58, 83 
weighting, 103 
Welch,323 
white 
Dirac distribution, 283 
gaussian, 284 
noise, 283 
random sequence, 254 
unit impulse, 283 
whitening, 286 
gaussian process, 286 
Wiener 
equalization, 380 
equation, 369 
filter, 354 
linear filter, 372 
window, 103 
Index 501 
Hamming, 104, 163, 316 
method, 155 
periodic/symmetrical, 106 
rectangular, 103 
symmetry, 165 
triangular (Bartlett), 316, 322 
weighting, 159 
XYZ-tristimulus, 187 
Yule-Walker, 388 
equations, 310, 365 
zero forcing, 379 
zero-order hold, 81, 173 
zero-padding, 89 
zeros (transfer function), 485 
ZOH,81 


Other titles from 
i 1( 
... -
..... 
, 1101--
III 
Digital Signal and Image Processing 
2014 
AUGER Fran<;:ois 
Signal Processing with Free Software: Practical Experiments 
FANETHerve 
Medical Imaging Based on Magnetic Fields and Ultrasounds 
MOUKADEM Ali, OULD Abdeslam Djaffar, DIETERLEN Alain 
Time-Frequency Domainfor Segmentation and Classification of Non-
stationary Signals: The Stockwell Transform Applied on Bio-signals and 
Electric Signals 
NDAGTJTMANA Fabien 
Signal Integrity: From High Speed to Radiofrequency Applications 
TUPIN Florence, INGLADA Jordi, NICOLAS Jean-Marie 
Remote Sensing Imagery 
VLADEANU Calin, EL ASSAD SafWan 
Nonlinear Digital Encoders for Data Communications 

2013 
GOY AERT Gerard, NADTF Mohamed 
Co-Clustering 
JA Y Emmanuelle, Duv AUT Patrick, DAROLLES Serge 
Multi-factor Models and Signal Processing Techniques: Application to 
Quantitative Finance 
LUCAS Laurent, Loscos CeIine, REMION Yannick 
3D Video: From Capture to Diffusion 
MOREAU Eric, ADAU Tulay 
Blind Identification and Separation of Complex-valued Signals 
PERRIN Vincent 
MRJ Techniques 
WAGNER Kevin, DOROSLOV ACKI Milos 
Proportionate-type Normalized Least Mean Square Algorithms 
FERNANDEZ Christine, MACAIRE Ludovic, ROBERT-INACIO Frederique 
Digital Color Imaging 
FERNANDEZ Christine, MACAIRE Ludovic, ROBERT-INACIO Frederique 
Digital Color: Acquisition, Perception, Coding and Rendering 
NATT-Au Amine, FOURNIER Regis 
Signal and Image Processingfor Biometrics 
OUAHABl Abdeljalil 
Signal and Image Multiresolution Analysis 
2011 
CASTANIE Francis 
Digital Spectral Analysis: Parametric, Non-parametric and Advanced 
Methods 
DESCOMBES Xavier 
Stochastic Geometry for Image Analysis 

FANETHerve 
Photon-based Medical Imagery 
MOREAU Nicolas 
Tools for Signal Compression 
2010 
NAJMAN Laurent, TALBOT Hugues 
Mathematical Morphology 
2009 
BERTEIN Jean-Claude, CESCHI Roger 
Discrete Stochastic Processes and Optimal Filtering / 2nd edition 
CHANUSSOT Jocelyn et al. 
Multivariate Image Processing 
DHOME Michel 
Visual Perception through Video Imagery 
GOY AERT Gerard 
Data Analysis 
GRANGEAT Pierre 
Tomography 
MOHAMAD-DJAFARl Ali 
Inverse Problems in Vision and 3D Tomography 
SIARRY Patrick 
Optimisation in Signal and Image Processing 
2008 
AERY Patrice et al. 
Scaling, Fractals and Wavelets 
GARELLO Rene 
Two-dimensional Signal Analysis 
HLA W A TSCH Franz et al. 
Time-Frequency Analysis 

IDlER Jerome 
Bayesian Approach to Inverse Problems 
MAITRE Henri 
Processing of Synthetic Aperture Radar (SAR) Images 
MAITRE Henri 
Image Processing 
NAlT-ALI Amine, CAVARO-MENARD Christine 
Compression of Biomedical Images and Signals 
NAJIM Mohamed 
Modeling, Estimation and Optimal Filtration in Signal Processing 
QUTNQUTS Andre 
Digital Signal Processing Using Matlab 
2007 
BERTETN Jean-Claude, CESCHI Roger 
Discrete Stochastic Processes and Optimal Filtering 
BLOCH Isabelle 
Information Fusion in Signal and Image Processing 
GLA VIEUX Alain 
Channel Coding in Communication Networks 
OPPENHEIM Georges et al. 
Wavelets and their Applications 
2006 
BLANCHET Gerard, CHARBlT Maurice 
Digital Signal and Image Processing using MATLAB 
CAST ANTE Francis 
Spectral Analysis 
NAJIM Mohamed 
Digital Filters Designfor Signal and Image Processing 

